# 五、网络挖掘、数据库和大数据

本章菜单上有以下食谱:

*   模拟网络浏览
*   刮网
*   处理非 ASCII 文本和 HTML 实体
*   实现关联表
*   设置数据库迁移脚本
*   向现有表中添加表列
*   创建表后添加索引
*   设置测试 web 服务器
*   用事实表和维度表实现星型模式
*   利用 HDFS
*   设置火花
*   使用 Spark 对数据进行聚类

# 简介

这一章侧重于数学，但更侧重于技术主题。科技可以为数据分析师提供很多东西。数据库已经存在了一段时间，但是大多数人熟悉的关系数据库可以追溯到 20 世纪 70 年代。埃德加·科德提出了许多想法，这些想法后来导致了关系模型和 SQL 的创建。从那以后，关系数据库一直是一项主导技术。在 20 世纪 80 年代，面向对象的编程语言引起了范式的转变，并不幸与关系数据库不匹配。

面向对象编程语言支持继承等概念，而关系数据库和 SQL 不支持这些概念(当然也有一些例外)。Python 生态系统有几个试图解决这种不匹配问题的对象关系映射 ( **ORM** )框架。这是不可能的，也没有必要全部覆盖，所以我选择了 SQLAlchemy 作为这里的食谱。我们还将把数据库模式迁移视为一个常见的热门话题，尤其是对于生产系统。

大数据是你可能听说过的流行语之一。Hadoop 和 Spark 可能听起来也很熟悉。我们将在本章中研究这些框架。如果您使用我的 Docker 映像，您将很不幸地在其中找不到 Selenium、Hadoop 和 Spark，因为为了节省空间，我决定不包含它们。

另一个重要的技术发展是万维网，也称为互联网。互联网是最终的数据来源；然而，以易于分析的形式获得这些数据有时是一个相当大的挑战。作为最后的资源，我们可能不得不抓取网页。成功没有保证，因为网站所有者可以在不警告我们的情况下更改内容。这是由你来保持网页抓取食谱的最新代码。

# 模拟网页浏览

公司网站通常由团队或部门使用专门的工具和模板制作。很多内容是动态生成的，由很大一部分 JavaScript 和 CSS 组成。这意味着即使我们下载了内容，我们仍然至少要评估 JavaScript 代码。我们可以通过 Python 程序做到这一点的一种方法是使用 **【硒】**应用编程接口。Selenium 的主要目的实际上是测试网站，但没有什么能阻止我们用它来刮网站。

我们将刮除本书代码包中的`test_widget.ipynb`文件，而不是刮除网站。为了模拟浏览这个网页，我们在`test_simulating_browsing.py`中提供了一个单元测试类。如果你想知道，这不是测试 IPython 笔记本的推荐方法。

出于历史原因，我更喜欢使用 XPath 来查找 HTML 元素。XPath 是一种查询语言，也适用于 HTML。这不是唯一的方法，您还可以使用 CSS 选择器、标签名或 id。为了找到正确的 XPath 表达式，你可以为你最喜欢的浏览器安装一个相关的插件，或者在谷歌浏览器中，你可以检查一个元素的 XPath。

## 做好准备

使用以下命令安装硒:

```py
$ pip install selenium

```

我用 Selenium 2.47.1 测试了代码。

## 怎么做…

以下步骤向您展示了如何使用我制作的 IPython 小部件模拟网页浏览。该配方的代码在本书代码包的`test_simulating_browsing.py`文件中:

1.  第一步是运行以下内容:

    ```py
    $ ipython notebook

    ```

2.  进口情况如下:

    ```py
    from selenium import webdriver
    import time
    import unittest
    import dautil as dl

    NAP_SECS = 10
    ```

3.  定义以下函数，创建一个火狐浏览器实例:

    ```py
    class SeleniumTest(unittest.TestCase):
        def setUp(self):
            self.logger = dl.log_api.conf_logger(__name__)
            self.browser = webdriver.Firefox()
    ```

4.  定义测试完成后要清理的以下功能:

    ```py
        def tearDown(self):
            self.browser.quit()
    ```

5.  以下功能点击小部件标签(我们必须等待用户界面响应):

    ```py
    def wait_and_click(self, toggle, text):
            xpath = "//a[@data-toggle='{0}' and contains(text(), '{1}')]"
            xpath = xpath.format(toggle, text)
            elem = dl.web.wait_browser(self.browser, xpath)
            elem.click()
    ```

6.  定义以下函数，该函数执行测试，包括评估笔记本单元格并单击 IPython 小部件中的几个选项卡(我们使用端口 8888):

    ```py
        def test_widget(self):
            self.browser.implicitly_wait(NAP_SECS)
            self.browser.get('http://localhost:8888/notebooks/test_widget.ipynb')

            try:
                # Cell menu
                xpath = '//*[@id="menus"]/div/div/ul/li[5]/a'
                link = dl.web.wait_browser(self.browser, xpath)
                link.click()
                time.sleep(1)

                # Run all
                xpath = '//*[@id="run_all_cells"]/a'
                link = dl.web.wait_browser(self.browser, xpath)
                link.click()
                time.sleep(1)

                self.wait_and_click('tab', 'Figure')
                self.wait_and_click('collapse', 'figure.figsize')
            except Exception:
                self.logger.warning('Error while waiting to click', exc_info=True)
                self.browser.quit()

            time.sleep(NAP_SECS)
            self.browser.save_screenshot('widgets_screenshot.png')

    if __name__ == "__main__":
        unittest.main()
    ```

以下截图由代码创建:

![How to do it…](img/B04223_05_01.jpg)

## 另见

*   Selenium 文档位于[https://Selenium-python . readed docs . org/en/latest/installation . html](https://selenium-python.readthedocs.org/en/latest/installation.html)(2015 年 9 月检索)
*   维基百科中关于 XPath 的页面位于[https://en.wikipedia.org/wiki/XPath](https://en.wikipedia.org/wiki/XPath)(检索于 2015 年 9 月)

# 刮网

我们知道搜索引擎发出被称为机器人的自主程序，在互联网上寻找信息。通常，这导致创建类似于电话簿或字典的巨大索引。Python 3 用户目前的情况(2015 年 9 月)在刮网方面并不理想。大多数框架只支持 Python 2。然而，圭多·范罗森斯，**一生的仁慈独裁者** ( **BDFL** )刚刚在 GitHub 上贡献了一个使用 AsyncIO API 的爬虫。向 BDFL 致敬！

为了保存抓取的网址，我分叉了存储库并做了一些小的修改。我也让爬虫提前退出了。这些变化不是很优雅，但这是我在有限的时间内所能做的。无论如何，我不能指望比 BDFL 本人做得更好。

一旦我们有了网页链接列表，我们将从 Selenium 加载这些网页(参考*模拟网页浏览*食谱)。我选择了 PhantomJS，一款无头浏览器，应该比 Firefox 占用空间更小。虽然这不是绝对必要的，但我认为有时下载你正在抓取的网页是有意义的，因为你可以在本地测试抓取。您还可以更改下载的 HTML 中的链接，以指向本地文件。这与*设置测试网络服务器*配方有关。抓取的一个常见用例是创建一个用于语言分析的文本语料库。这是我们在这个食谱中的目标。

## 做好准备

按照*模拟网页浏览*食谱所述安装硒。我在这个食谱中使用了 PhantomJS，但这并不是硬性要求。您可以使用 Selenium 支持的任何其他浏览器。我的修改在[https://github.com/ivanidris/500lines/releases](https://github.com/ivanidris/500lines/releases)的 0.0.1 标签下(2015 年 9 月检索)。下载其中一个源档案并将其解压缩。导航至`crawler`目录及其`code`子目录。

使用以下命令启动(可选步骤)爬虫(我以 CNN 为例):

```py
$ python crawl.py edition.cnn.com

```

## 怎么做…

您可以在本书的代码包中使用带有链接的 CSV 文件，也可以按照我在上一节中解释的那样自己制作。以下过程描述了如何创建新闻文章的文本语料库(参见本书代码包中的`download_html.py`文件):

1.  进口情况如下:

    ```py
    import dautil as dl
    import csv
    import os
    from selenium import webdriver
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.common.by import By
    import urllib.parse as urlparse
    import urllib.request as urlrequest
    ```

2.  定义以下全局常数:

    ```py
    LOGGER = dl.log_api.conf_logger('download_html')
    DRIVER = webdriver.PhantomJS()
    NAP_SECONDS = 10
    ```

3.  定义以下函数从一个 HTML 页面中提取文本并保存:

    ```py
    def write_text(fname):
        elems = []

        try:
            DRIVER.get(dl.web.path2url(fname))

            elems = WebDriverWait(DRIVER, NAP_SECONDS).until(
                EC.presence_of_all_elements_located((By.XPATH, '//p'))
            )

            LOGGER.info('Elems', elems)

            with open(fname.replace('.html', '_phantomjs.html'), 'w') as pjs_file:
                LOGGER.warning('Writing to %s', pjs_file.name)
                pjs_file.write(DRIVER.page_source)

        except Exception:
            LOGGER.error("Error processing HTML", exc_info=True)

        new_name = fname.replace('html', 'txt')

        if not os.path.exists(new_name):
            with open(new_name, 'w') as txt_file:
                LOGGER.warning('Writing to %s', txt_file.name)

                lines = [e.text for e in elems]
                LOGGER.info('lines', lines)
                txt_file.write(' \n'.join(lines))
    ```

4.  定义如下`main()`函数，读取带有链接的 CSV 文件，调用前面步骤中的函数:

    ```py
    def main():
        filedir = os.path.join(dl.data.get_data_dir(), 'edition.cnn.com')

        with open('saved_urls.csv') as csvfile:
            reader = csv.reader(csvfile)

            for line in reader:
                timestamp, count, basename, url = line
                fname = '_'.join([count, basename])
                fname = os.path.join(filedir, fname)

                if not os.path.exists(fname):
                    dl.data.download(url, fname)

                write_text(fname)

    if __name__ == '__main__':
        DRIVER.implicitly_wait(NAP_SECONDS)
        main()
        DRIVER.quit()
    ```

# 处理非 ASCII 文本和 HTML 实体

HTML 不像来自数据库查询或 Pandas 的数据那样结构化。你可能会被用正则表达式或字符串函数操纵 HTML。然而，这种方法只在有限的情况下有效。您最好使用专门的 Python 库来处理 HTML。在这个食谱中，我们将使用 lxml 库的`clean_html()`功能。这个函数从一个网页中剥离所有的 JavaScript 和 CSS。

**美国信息交换标准码** ( **ASCII** )在 2007 年底之前一直是互联网上的主导编码标准，UTF-8 (8 位 Unicode)占据首位。ASCII 仅限于英文字母，不支持不同语言的字母。Unicode 对字母有更广泛的支持。但是，我们有时需要将自己限制在 ASCII 中，所以这个食谱为您提供了一个如何忽略非 ASCII 字符的示例。

## 做好准备

使用 pip 或 conda 安装 lxml，如下所示:

```py
$ pip install lxml
$ conda install lxml

```

我用 Anaconda 的 lxml 3.4.2 测试了代码。

## 怎么做…

代码在本书的代码包中的`processing_html.py`文件中，分为以下步骤:

1.  进口情况如下:

    ```py
    from lxml.html.clean import clean_html
    from difflib import Differ
    import unicodedata
    import dautil as dl

    PRINT = dl.log_api.Printer()
    ```

2.  定义以下函数来区分两个文件:

    ```py
    def diff_files(text, cleaned):
        d = Differ()
        diff = list(d.compare(text.splitlines(keepends=True),
                              cleaned.splitlines(keepends=True)))
        PRINT.print(diff)
    ```

3.  以下代码块打开一个 HTML 文件，对其进行清理，并将清理后的文件与原始文件进行比较:

    ```py
    with open('460_cc_phantomjs.html') as html_file:
        text = html_file.read()
        cleaned = clean_html(text)
        diff_files(text, cleaned)
        PRINT.print(dl.web.find_hrefs(cleaned))
    ```

4.  下面的代码片段演示了对非 ASCII 文本的处理:

    ```py
    bulgarian = 'Питон is Bulgarian for Python'
    PRINT.print('Bulgarian', bulgarian)
    PRINT.print('Bulgarian ignored', unicodedata.normalize('NFKD', bulgarian).encode('ascii', 'ignore'))
    ```

最终结果请参考下面的截图(为了简洁起见，我省略了一些输出):

![How to do it…](img/B04223_05_02.jpg)

## 另见

*   lxml 文档是位于[http://lxml.de/index.html](http://lxml.de/index.html)的(2015 年 9 月检索)

# 实现关联表

关联表充当数据库表之间的桥梁，数据库表具有多对多的关系。该表包含与它所连接的表的主键相链接的外键。

在这个食谱中，我们将把网页和网页内的链接联系起来。一个页面有很多链接，链接可以在很多页面中。我们将只关注其他网站的链接，但这不是必需的。如果您试图在本地机器上复制一个网站进行测试或分析，那么您还需要存储图像和 JavaScript 链接。看看下面的关系模式图:

![Implementing association tables](img/B04223_05_03.jpg)

## 做好准备

我用 Anaconda 安装了 SQLAlchemy 0.9.9，如下所示:

```py
$ conda install sqlalchemy

```

如果愿意，也可以使用以下命令安装 SQLAlchemy:

```py
$ pip install sqlalchemy

```

## 怎么做…

本书代码包中`impl_association.py`文件的以下代码实现了关联表模式:

1.  进口情况如下:

    ```py
    from sqlalchemy import create_engine
    from sqlalchemy import Column
    from sqlalchemy import ForeignKey
    from sqlalchemy import Integer
    from sqlalchemy import String
    from sqlalchemy import Table
    from sqlalchemy.orm import backref
    from sqlalchemy.orm import relationship
    from sqlalchemy.ext.declarative import declarative_base
    from sqlalchemy.orm import sessionmaker
    from sqlalchemy.exc import IntegrityError
    import dautil as dl
    import os

    Base = declarative_base()
    ```

2.  定义以下类来表示网页:

    ```py
    class Page(Base):
        __tablename__ = 'pages'
        id = Column(Integer, primary_key=True)
        filename = Column(String, nullable=False, unique=True)
        links = relationship('Link', secondary='page_links')

        def __repr__(self):
            return "Id=%d filename=%s" %(self.id, self.filename)
    ```

3.  定义以下类来表示网络链接:

    ```py
    class Link(Base):
        __tablename__ = 'links'
        id = Column(Integer, primary_key=True)
        url = Column(String, nullable=False, unique=True)

        def __repr__(self):
            return "Id=%d url=%s" %(self.id, self.url)
    ```

4.  定义以下类来表示页面和链接之间的关联:

    ```py
    class PageLink(Base):
        __tablename__ = 'page_links'
        page_id = Column(Integer, ForeignKey('pages.id'), primary_key=True)
        link_id = Column(Integer, ForeignKey('links.id'), primary_key=True)
        page = relationship('Page', backref=backref('link_assoc'))
        link = relationship('Link', backref=backref('page_assoc'))

        def __repr__(self):
            return "page_id=%s link_id=%s" %(self.page_id, self.link_id)
    ```

5.  定义以下功能来浏览 HTML 文件并更新相关表格:

    ```py
    def process_file(fname, session):
        with open(fname) as html_file:
            text = html_file.read()

            if dl.db.count_where(session, Page.filename, fname):
                # Cowardly refusing to continue
                return

            page = Page(filename=fname)
            hrefs = dl.web.find_hrefs(text)

            for href in set(hrefs):
                # Only saving http links
                if href.startswith('http'):
                    if dl.db.count_where(session, Link.url, href):
                        continue

                    link = Link(url=href)
                    session.add(PageLink(page=page, link=link))

            session.commit()
    ```

6.  定义以下函数来填充数据库:

    ```py
    def populate():
        dir = dl.data.get_data_dir()
        path = os.path.join(dir, 'crawled_pages.db')
        engine = create_engine('sqlite:///' + path)
        DBSession = sessionmaker(bind=engine)
        Base.metadata.create_all(engine)
        session = DBSession()

        files  = ['460_cc_phantomjs.html', '468_live_phantomjs.html']

        for file in files:
            process_file(file, session)

        return session
    ```

7.  下面的代码片段使用了我们定义的函数和类:

    ```py
    if __name__ == "__main__":
        session = populate()
        printer = dl.log_api.Printer(nelems=3)
        pages = session.query(Page).all()
        printer.print('Pages', pages)

        links = session.query(Link).all()
        printer.print('Links', links)

        page_links = session.query(PageLink).all()
        printer.print('PageLinks', page_links)
    ```

最终结果参见下面的截图:

![How to do it…](img/B04223_05_04.jpg)

# 设置数据库迁移脚本

你在编程课上学到的第一件事是，没有人能第一时间把复杂的程序做对。软件随着时间的推移而发展，我们希望最好的。自动化测试中的自动化有助于确保我们的程序随着时间的推移而改进。然而，当谈到进化数据库模式时，自动化似乎并不那么明显。尤其是在大型企业中，数据库模式是数据库管理员和专家的领域。当然，还有与更改模式相关的安全和操作问题，在生产数据库中更是如此。在任何情况下，您都可以在本地测试环境中实现数据库迁移，并为生产团队记录建议的更改。

我们将使用 Alembic 来演示如何着手设置迁移脚本。在我看来，Alembic 是这项工作的合适工具，尽管它截至 2015 年 9 月仍处于测试阶段。

## 做好准备

使用以下命令安装 Alembic:

```py
$ pip install alembic

```

Alembic 依赖于 SQLAlchemy，如果需要，它会自动安装 SQLAlchemy。你现在应该有`alembic`命令在你的路径上。这一章我用了 Alembic 0.8.2。

## 怎么做…

以下步骤描述了如何设置一个迁移步骤。当我们在目录中运行 Alembic 初始化脚本时，它会创建一个`alembic`目录和一个名为`alembic.ini`的配置文件:

1.  导航到适当的目录并初始化迁移项目，如下所示:

    ```py
    $ alembic init alembic

    ```

2.  根据需要编辑`alembic.ini`文件。例如，更改`sqlalchemy.url`属性以指向正确的数据库。

## 另见

*   相关的 Alembic 文档位于[https://alembic.readthedocs.org/en/latest/tutorial.html](https://alembic.readthedocs.org/en/latest/tutorial.html)(2015 年 9 月检索)

# 向现有表格添加表格列

如果我们使用对象关系映射器(ORM)，比如 SQLAlchemy，我们将类映射到表，将类属性映射到表列。通常，由于新的业务需求，我们需要添加一个表列和相应的类属性。我们可能需要在添加后立即填充该列。

如果我们处理生产数据库，那么您可能没有直接访问权限。幸运的是，我们可以用 Alembic 生成 SQL，数据库管理员可以查看它。

## 做好准备

参考*设置数据库迁移脚本*配方。

## 怎么做…

Alembic 有自己的版本控制系统，需要额外的表。它还用生成的 Python 代码文件在`alembic`目录下创建一个`versions`目录。我们需要在这些文件中指定迁移所需的更改类型:

1.  创建新版本，如下所示:

    ```py
    $ alembic revision -m "Add a column"

    ```

2.  打开生成的 Python 文件(例如`27218d73000_add_a_column.py`)。用下面的代码替换其中的两个函数，增加`link_type`字符串列:

    ```py
    def upgrade():
        # MODIFIED Ivan Idris
        op.add_column('links', sa.Column('link_type', sa.String(20)))

    def downgrade():
        # MODIFIED Ivan Idris
        op.drop_column('links', 'link_type')
    ```

3.  生成 SQL，如下所示:

    ```py
    $ alembic upgrade head --sql

    ```

有关最终结果，请参考以下屏幕截图:

![How to do it…](img/B04223_05_05.jpg)

# 创建表后添加索引

指数是计算中的一个通用概念。这本书还有一个索引，可以更快地查找，将概念与页码匹配起来。索引占用空间；就这本书而言，几页。数据库索引还有一个额外的缺点，由于更新索引的额外开销，它们会使插入和更新变得更慢。通常，主键和外键会自动获得索引，但这取决于数据库实现。

添加索引不能掉以轻心，最好在咨询数据库管理员后进行。Alembic 的索引添加功能类似于我们在*中看到的功能，将一个表列添加到现有的表*配方中。

## 做好准备

参考*设置数据库迁移脚本*配方。

## 怎么做…

这个配方和*在现有的表格*配方上增加一个表格列有一些重叠，所以我就不重复所有的细节了:

1.  创建新版本，如下所示:

    ```py
    $ alembic revision -m "Add indices"

    ```

2.  打开生成的 Python 文件(例如`21579ecccd8_add_indices.py`)并修改代码，使其具有以下功能，这些功能负责添加索引:

    ```py
    def upgrade():
        # MODIFIED Ivan Idris
        op.create_index('idx_links_url', 'links', ['url'])
        op.create_index('idx_pages_filename', 'pages', ['filename'])

    def downgrade():
        # MODIFIED Ivan Idris
        op.drop_index('idx_links_url')
        op.drop_index('idx_pages_filename')
    ```

3.  生成 SQL，如下所示:

    ```py
    $ alembic upgrade head --sql

    ```

最终结果参见下面的截图:

![How to do it…](img/B04223_05_06.jpg)

## 它是如何工作的…

`create_index()`函数添加给定索引名、表和表列列表的索引。`drop_index()`函数则相反，移除给定索引名的索引。

## 另见

*   关于数据库索引的维基百科页面位于[https://en.wikipedia.org/wiki/Database_index](https://en.wikipedia.org/wiki/Database_index)(检索于 2015 年 9 月)

# 设置测试网络服务器

在[第 1 章](01.html "Chapter 1. Laying the Foundation for Reproducible Data Analysis")、*为可再现数据分析奠定基础*中，我们讨论了为什么单元测试是一个好主意。纯粹主义者会告诉你，你只需要单元测试。然而，普遍的共识是，更高级别的测试也是有用的。

显然，这本书是关于数据分析的，而不是关于 web 开发的。不过，通过网站或网络服务分享你的结果或数据是一个常见的要求。当你挖掘网络或者做其他与网络相关的事情时，经常需要重现某些用例，比如登录表单。正如您对成熟语言的期望，Python 有许多优秀的网络框架。我选择了 Flask，这是一个简单的 Pythonic 网络框架，因为它看起来很容易设置，但是你应该使用你自己的判断，因为我不知道你的要求是什么。

## 做好准备

我用 Anaconda 的 Flask 0.10.1 测试了代码。用`conda`或`pip`安装烧瓶，如下所示:

```py
$ conda install flask
$ pip install flask

```

## 怎么做…

在这个食谱中，我们将设置一个带有登录表单的安全页面，您可以使用它进行测试。代码由`app.py` Python 文件和`templates`目录下的 HTML 文件组成(我就不详细讨论 HTML 了):

1.  进口情况如下:

    ```py
    from flask import Flask
    from flask import render_template
    from flask import request
    from flask import redirect
    from flask import url_for

    app = Flask(__name__)
    ```

2.  定义以下功能来处理主页请求:

    ```py
    @app.route('/')
    def home():
        return "Test Site"
    ```

3.  定义以下功能来处理登录尝试:

    ```py
    @app.route('/secure', methods=['GET', 'POST'])
    def login():
        error = None
        if request.method == 'POST':
            if request.form['username'] != 'admin' or\
                    request.form['password'] != 'admin':
                error = 'Invalid password or user name.'
            else:
                return redirect(url_for('home'))
        return render_template('admin.html', error=error)
    ```

4.  以下块运行服务器(不要将`debug=True`用于面向公众的网站):

    ```py
    if __name__ == '__main__':
        app.run(debug=True)
    ```

5.  运行`$ python app.py`，在`http://127.0.0.1:5000/`和`http://127.0.0.1:5000/secure`打开网页浏览器。

有关最终结果，请参考以下屏幕截图:

![How to do it…](img/B04223_05_07.jpg)

# 用事实表和维度表实现星型模式

**星型模式**是一种便于报告的数据库模式。星型模式适用于事件的处理，如网站访问、广告点击或金融交易。事件信息(指标，如温度或购买金额)存储在事实表中，事实表链接到小得多的维度表。星型模式是非规范化的，这将完整性检查的责任放在了应用程序代码上。因此，我们应该只以受控的方式写入数据库。如果使用 SQLAlchemy 进行大容量插入，应该选择核心应用编程接口而不是 ORM 应用编程接口，或者使用直接的 SQL。你可以在[http://docs.sqlalchemy.org/en/rel_1_0/faq/performance.html](http://docs.sqlalchemy.org/en/rel_1_0/faq/performance.html)阅读更多原因(2015 年 9 月检索)。

时间是报告中的一个常见维度。例如，我们可以在维度表中存储每日天气测量的日期。对于数据中的每个日期，我们可以保存日期、年份、月份和一年中的某一天。我们可以在处理事件之前预先填充这个表，然后根据需要添加新的日期。如果我们假设只需要维护数据库一个世纪，我们甚至不需要向时间维度表中添加新记录。在这种情况下，我们将在时间维度表中预先填充我们想要支持的范围内的所有可能日期。如果我们正在处理二进制或分类变量，预先填充维度表也是可能的。

在本食谱中，我们将为[中描述的直接](http://blog.minethatdata.com/2008/03/minethatdata-e-mail-analytics-and-data.html)营销数据实施星型模式。数据在一个直接营销活动的 CSV 文件中。为了简单起见，我们将忽略一些列。作为一个指标，我们将带采购金额的`spend`列。对于维度，我选择了渠道(电话、网络或多渠道)、邮政编码(农村、郊区或城市)和细分市场(男性、女性或没有电子邮件)。请参考以下实体关系图:

![Implementing a star schema with fact and dimension tables](img/B04223_05_08.jpg)

## 怎么做…

以下代码下载数据，加载到数据库中，然后查询数据库(参考本书代码包中的`star_schema.py`文件):

1.  进口情况如下:

    ```py
    from sqlalchemy import Column
    from sqlalchemy import create_engine
    from sqlalchemy.ext.declarative import declarative_base
    from sqlalchemy import ForeignKey
    from sqlalchemy import Integer
    from sqlalchemy import String
    from sqlalchemy.orm import sessionmaker
    from sqlalchemy import func
    import dautil as dl
    from tabulate import tabulate
    import sqlite3
    import os
    from joblib import Memory

    Base = declarative_base()
    memory = Memory(cachedir='.')
    ```

2.  定义以下类来表示邮政编码维度:

    ```py
    class DimZipCode(Base):
        __tablename__ = 'dim_zip_code'
        id = Column(Integer, primary_key=True)
        # Urban, Suburban, or Rural.
        zip_code = Column(String(8), nullable=False, unique=True)
    ```

3.  定义以下类来表示线段尺寸:

    ```py
    class DimSegment(Base):
        __tablename__ = 'dim_segment'
        id = Column(Integer, primary_key=True)
        # Mens E-Mail, Womens E-Mail or No E-Mail
        segment = Column(String(14), nullable=False, unique=True)
    ```

4.  定义以下类来表示通道维度:

    ```py
    class DimChannel(Base):
        __tablename__ = 'dim_channel'
        id = Column(Integer, primary_key=True)
        channel = Column(String)
    ```

5.  定义以下类来表示事实表:

    ```py
    class FactSales(Base):
        __tablename__ = 'fact_sales'
        id = Column(Integer, primary_key=True)
        zip_code_id = Column(Integer, ForeignKey('dim_zip_code.id'),
                             primary_key=True)
        segment_id = Column(Integer, ForeignKey('dim_segment.id'),
                            primary_key=True)
        channel_id = Column(Integer, ForeignKey('dim_channel.id'),
                            primary_key=True)

        # Storing amount as cents
        spend = Column(Integer)

        def __repr__(self):
            return "zip_code_id={0} channel_id={1} segment_id={2}".format(
                self.zip_code_id, self.channel_id, self.segment_id)
    ```

6.  定义以下功能创建 SQLAlchemy 会话:

    ```py
    def create_session(dbname):
        engine = create_engine('sqlite:///{}'.format(dbname))
        DBSession = sessionmaker(bind=engine)
        Base.metadata.create_all(engine)

        return DBSession()
    ```

7.  定义以下功能来填充段维度表:

    ```py
    def populate_dim_segment(session):
        options = ['Mens E-Mail', 'Womens E-Mail', 'No E-Mail']

        for option in options:
            if not dl.db.count_where(session, DimSegment.segment, option):
                session.add(DimSegment(segment=option))

        session.commit()
    ```

8.  定义以下函数来填充邮政编码维度表:

    ```py
    def populate_dim_zip_code(session):
        # Note the interesting spelling
        options = ['Urban', 'Surburban', 'Rural']

        for option in options:
            if not dl.db.count_where(session, DimZipCode.zip_code, option):
                session.add(DimZipCode(zip_code=option))

        session.commit()
    ```

9.  定义以下函数来填充通道维度表:

    ```py
    def populate_dim_channels(session):
        options = ['Phone', 'Web', 'Multichannel']

        for option in options:
            if not dl.db.count_where(session, DimChannel.channel, option):
                session.add(DimChannel(channel=option))

        session.commit()
    ```

10.  定义以下函数来填充事实表(出于性能原因，它使用直接的 SQL:

    ```py
    def load(csv_rows, session, dbname):
        channels = dl.db.map_to_id(session, DimChannel.channel)
        segments = dl.db.map_to_id(session, DimSegment.segment)
        zip_codes = dl.db.map_to_id(session, DimZipCode.zip_code)
        conn = sqlite3.connect(dbname)
        c = conn.cursor()
        logger = dl.log_api.conf_logger(__name__)

        for i, row in enumerate(csv_rows):
            channel_id = channels[row['channel']]
            segment_id = segments[row['segment']]
            zip_code_id = zip_codes[row['zip_code']]
            spend = dl.data.centify(row['spend'])

            insert = "INSERT INTO fact_sales (id, segment_id,\
                zip_code_id, channel_id, spend) VALUES({id}, \
                {sid}, {zid}, {cid}, {spend})"
            c.execute(insert.format(id=i, sid=segment_id,
                                    zid=zip_code_id, cid=channel_id,     spend=spend))

            if i % 1000 == 0:
                logger.info("Progress %s/64000", i)
                conn.commit()

        conn.commit()
        c.close()
        conn.close()
    ```

11.  定义以下函数下载并解析数据:

    ```py
    @memory.cache
    def get_and_parse():
        out = dl.data.get_direct_marketing_csv()
        return dl.data.read_csv(out)
    ```

12.  以下模块使用我们定义的函数和类【T3:

    ```py
    if __name__ == "__main__":
        dbname = os.path.join(dl.data.get_data_dir(), 'marketing.db')
        session = create_session(dbname)
        populate_dim_segment(session)
        populate_dim_zip_code(session)
        populate_dim_channels(session)

        if session.query(FactSales).count() < 64000:
            load(get_and_parse(), session, dbname)

        fsum = func.sum(FactSales.spend)
        query = session.query(DimSegment.segment, DimChannel.channel,
                              DimZipCode.zip_code, fsum)
        dim_cols = (DimSegment.segment, DimChannel.channel, DimZipCode.zip_code)
        dim_entities = [dl.db.entity_from_column(col) for col in dim_cols]
        spend_totals = query.join(FactSales,
                                  *dim_entities)\
                            .group_by(*dim_cols).order_by(fsum.desc()).all()
        print(tabulate(spend_totals, tablefmt='psql',
                       headers=['Segment', 'Channel', 'Zip Code', 'Spend']))
    ```

有关最终结果(以美分为单位的支出金额)，请参考以下屏幕截图:

![How to do it…](img/B04223_05_09.jpg)

## 另见

*   https://en.wikipedia.org/wiki/Star_schema 的星模式维基百科页面(2015 年 9 月检索)

# 使用 HDFS

**Hadoop 分布式文件系统** ( **HDFS** )是大数据 Hadoop 框架的存储组件。HDFS 是一个分布式文件系统，它在多个系统上传播数据，其灵感来自于谷歌用于其搜索引擎的谷歌文件系统。HDFS 需要一个 **Java 运行时环境** ( **JRE** )，它使用一个`NameNode`服务器来跟踪文件。系统还会复制数据，这样丢失几个节点不会导致数据丢失。HDFS 的典型用例是处理大型只读文件。Apache Spark 也包含在本章中，它也可以使用 HDFS。

## 做好准备

安装 Hadoop 和一个 JRE。因为这些不是 Python 框架，所以您必须检查什么程序适合您的操作系统。这个食谱我用的是 Hadoop 2.7.1 和 Java 1.7.0_60。这可能是一个复杂的过程，但网上有许多资源可以帮助您解决特定系统的故障。

## 怎么做…

我们可以用在你的 Hadoop 安装中找到的几个 XML 文件来配置 HDFS。本节中的一些步骤仅作为示例，您应该根据您的操作系统、环境和个人偏好来实施它们:

1.  编辑`core-site.xml`文件，使其具有以下内容(注释省略):

    ```py
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://localhost:8020</value>
        </property>
    </configuration>
    ```

2.  编辑`hdfs-site.xml`文件，使其具有以下内容(注释省略)，将每个文件的复制设置为仅 1，以在本地运行 HDFS:

    ```py
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
    </configuration>
    ```

3.  如有必要，在您的系统上启用远程登录来 SSH 到本地主机并生成密钥(Windows 用户可以使用 putty):

    ```py
    $ ssh-keygen -t dsa -f ~/.ssh/id_dsa
    $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys

    ```

4.  从 Hadoop 目录的根目录格式化文件系统:

    ```py
    $ bin/hdfs namenode –format

    ```

5.  启动 NameNode 服务器，如下(相反命令为`$ sbin/stop-dfs.sh` ):

    ```py
    $ sbin/start-dfs.sh

    ```

6.  使用以下命令在 HDFS 创建目录:

    ```py
    $ hadoop fs -mkdir direct_marketing

    ```

7.  或者，如果您想使用火花配方中的`direct_marketing.csv`文件，您需要将其复制到 HDFS，如下所示:

    ```py
    $ hadoop fs -copyFromLocal <path to file>/direct_marketing.csv direct_marketing

    ```

## 另见

*   位于[的](https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html) HDFS 用户指南

# 设置火花

**Apache Spark** 是 Hadoop 生态系统中的一个项目(参考*使用 HDFS* 配方)，据称其性能优于 Hadoop 的 MapReduce。Spark 尽可能将数据加载到内存中，对机器学习有很好的支持。在*用 Spark* 配方聚类数据中，我们将通过 Spark 应用机器学习算法。

Spark 可以独立工作，但它被设计为使用 HDFS 与 Hadoop 一起工作。**弹性分布式数据集** ( **RDDs** )是 Spark 的中心结构，代表分布式数据。Spark 对 Scala(一种 JVM 语言)有很好的支持，对 Python 的支持有些滞后。例如，pyspark API 中对流的支持有点滞后。Spark 也有 DataFrames 的概念，但是它不是通过 pandas 实现的，而是通过 Spark 实现的。

## 做好准备

从[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)的下载页面下载星火(2015 年 9 月检索)。我下载了星火 1.5.0 的`spark-1.5.0-bin-hadoop2.6.tgz`档案。

将归档文件解压缩到适当的目录中。

## 怎么做…

以下步骤通过几个可选步骤说明了 Spark 的基本设置:

1.  如果要使用不同于系统 Python 的 Python 版本，请通过操作系统的 GUI 或 CLI 设置`PYSPARK_PYTHON`环境变量，如下所示:

    ```py
    $ export PYSPARK_PYTHON=/path/to/anaconda/bin/python

    ```

2.  设置`SPARK_HOME`环境变量，如下所示:

    ```py
    $ export SPARK_HOME=<path/to/spark/>spark-1.5.0-bin-hadoop2.6

    ```

3.  将`python`目录添加到您的`PYTHONPATH`环境变量中，如下所示:

    ```py
    $ export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

    ```

4.  将`py4j`的 ZIP 添加到您的`PYTHONPATH`环境变量中，如下所示:

    ```py
    $ export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH

    ```

5.  如果 Spark 的日志记录过于冗长，将`$SPARK_HOME/conf`目录中的`log4j.properties.template`文件复制到`log4j.properties`并将 INFO 级别更改为 WARN。

## 另见

官方星火网站在[http://spark.apache.org/](http://spark.apache.org/)(2015 年 9 月检索)

# 用 Spark 聚类数据

在上一个食谱*设置火花*中，我们介绍了火花的基本设置。如果你按照*使用 HDFS* 食谱，你可以选择提供来自 Hadoop 的数据。在这种情况下，您需要以这种方式指定文件的网址，`hdfs://hdfs-host:port/path/direct_marketing.csv`。

我们将使用与我们在*中使用的相同的数据，用事实和维度表*实现星型模式。但是，这次我们将使用`spend`、`history`和`recency`列。第一列对应直销活动后的最近购买金额，第二列对应历史购买金额，第三列对应最近几个月的购买金额。数据描述见[http://blog . minethatdata . com/2008/03/minethatdata-e-mail-analytics-and-data . html](http://blog.minethatdata.com/2008/03/minethatdata-e-mail-analytics-and-data.html)(2015 年 9 月检索)。我们将应用流行的 K-means 机器学习算法对数据进行聚类。[第九章](09.html "Chapter 9. Ensemble Learning and Dimensionality Reduction")、*集成学习和降维*，更关注机器学习算法。K-means 算法试图为给定数量的聚类的数据集找到最佳聚类。我们应该要么知道这个数字，要么通过反复试验找到它。在本食谱中，我通过**集内平方和误差** ( **WSSSE** )也称为**集内平方和** ( **WCSS** )来评估聚类。该度量计算每个点和其指定簇之间距离的平方误差之和。您可以在[第 10 章](10.html "Chapter 10. Evaluating Classifiers, Regressors, and Clusters")、*评估分类器、回归器和聚类*中了解更多关于评估指标的信息。

## 做好准备

按照*设置火花*配方中的说明进行操作。

## 怎么做…

该配方的代码在本书代码包的`clustering_spark.py`文件中:

1.  进口情况如下:

    ```py
    from pyspark.mllib.clustering import KMeans
    from pyspark import SparkContext
    import dautil as dl
    import csv
    import matplotlib.pyplot as plt
    import matplotlib as mpl
    from matplotlib.colors import Normalize
    ```

2.  定义以下函数计算误差:

    ```py
    def error(point, clusters):
        center = clusters.centers[clusters.predict(point)]

        return dl.stats.wssse(point, center)
    ```

3.  读取并解析数据，如下所示:

    ```py
    sc = SparkContext()
    csv_file = dl.data.get_direct_marketing_csv()
    lines = sc.textFile(csv_file)
    header = lines.first().split(',')
    cols_set = set(['recency', 'history', 'spend'])
    select_cols = [i for i, col in enumerate(header) if col in cols_set]
    ```

4.  设置以下 rdd:

    ```py
    header_rdd = lines.filter(lambda l: 'recency' in l)
    noheader_rdd = lines.subtract(header_rdd)
    temp = noheader_rdd.map(lambda v: list(csv.reader([v]))[0])\
                       .map(lambda p: (int(p[select_cols[0]]),
                            dl.data.centify(p[select_cols[1]]),
                            dl.data.centify(p[select_cols[2]])))

    # spend > 0
    temp = temp.filter(lambda x: x[2] > 0)
    ```

5.  用 k-means 算法对数据进行聚类:

    ```py
    points = []
    clusters = None

    for i in range(2, 28):
        clusters = KMeans.train(temp, i, maxIterations=10,
                                runs=10, initializationMode="random")

        val = temp.map(lambda point: error(point, clusters))\
                  .reduce(lambda x, y: x + y)
        points.append((i, val))
    ```

6.  绘制集群，如下所示:

    ```py
    dl.options.mimic_seaborn()
    fig, [ax, ax2] = plt.subplots(2, 1)
    ax.set_title('k-means Clusters')
    ax.set_xlabel('Number of clusters')
    ax.set_ylabel('WSSSE')
    dl.plotting.plot_points(ax, points)

    collected = temp.collect()
    recency, history, spend = zip(*collected)
    indices = [clusters.predict(c) for c in collected]
    ax2.set_title('Clusters for spend, history and recency')
    ax2.set_xlabel('history (cents)')
    ax2.set_ylabel('spend (cents)')
    markers = dl.plotting.map_markers(indices)
    colors = dl.plotting.sample_hex_cmap(name='hot', ncolors=len(set(recency)))

    for h, s, r, m in zip(history, spend, recency, markers):
        ax2.scatter(h, s, s=20 + r, marker=m, c=colors[r-1])

    cma = mpl.colors.ListedColormap(colors, name='from_list', N=None)
    norm = Normalize(min(recency), max(recency))
    msm = mpl.cm.ScalarMappable(cmap=cma, norm=norm)
    msm.set_array([])
    fig.colorbar(msm, label='Recency')

    for i, center in enumerate(clusters.clusterCenters):
        recency, history, spend = center
        ax2.text(history, spend, str(i))

    plt.tight_layout()
    plt.show()
    ```

最终结果见以下截图(图中数字对应聚类中心):

![How to do it…](img/B04223_05_10.jpg)

## 它是如何工作的…

k 均值聚类将数据点分配给 k 个聚类。聚类问题不是直接可以解决的，但是我们可以应用启发式算法，获得一个可接受的结果。k 均值算法在两个步骤之间迭代，不包括 k 均值的初始化(通常是随机的):

*   给每个数据点分配一个 WCSS 均值最低的聚类
*   重新计算聚类中心作为聚类点坐标的平均值

当集群分配变得稳定时，算法停止。

## 还有更多…

Spark 1.5.0 增加了对流 K 均值的实验支持。由于这些新特性的实验性质，我决定不详细讨论它们。我在本书的代码包`streaming_clustering.py`文件中添加了以下示例代码:

```py
import dautil as dl
from pyspark.mllib.clustering import StreamingKMeansModel
from pyspark import SparkContext

csv_file = dl.data.get_direct_marketing_csv()
csv_rows = dl.data.read_csv(csv_file)

stkm = StreamingKMeansModel(28 * [[0., 0., 0.]], 28 * [1.])
sc = SparkContext()

for row in csv_rows:
    spend = dl.data.centify(row['spend'])

    if spend > 0:
        history = dl.data.centify(row['history'])
        data = sc.parallelize([[int(row['recency']),
                               history, spend]])
        stkm = stkm.update(data, 0., 'points')

print(stkm.centers)
```

## 另见

*   *用 Python 构建机器学习系统*、威利·里歇特和路易斯·佩德罗科埃略(2013)
*   维基百科关于 K-means 聚类的页面在 https://en.wikipedia.org/wiki/K-means_clustering(2015 年 9 月检索)