# 一、说明这一切是怎么回事

*这本书是关于 Hadoop 的，这是一个用于大规模数据处理的开源框架。 在我们详细介绍这项技术及其在后面章节中的用法之前，花一点时间探索导致 Hadoop 的创建及其巨大成功的趋势是很重要的。*

*Hadoop 不是在真空中创建的；相反，它的存在是因为创建和使用的数据量呈爆炸式增长，而且这种数据洪流不仅出现在大型跨国公司身上，还出现在小型初创公司身上。 与此同时，其他趋势也改变了软件和系统的部署方式，与更传统的基础设施一起使用云资源，甚至优先使用云资源。*

本章将探讨其中的一些趋势，并详细解释 Hadoop 试图解决的具体问题以及影响其设计的驱动因素。

在本章的其余部分中，我们将：

*   了解大数据革命
*   了解 Hadoop 是什么以及它如何从数据中提取价值
*   了解云计算并了解亚马逊 Web 服务提供了什么
*   看看大数据处理和云计算的结合有多强大
*   了解本书其余部分涵盖的主题的概述

所以让我们开始吧！

# 大数据处理

环顾一下我们今天拥有的技术，很容易得出这样的结论：一切都是关于数据的。 作为消费者，我们对富媒体的胃口越来越大，无论是我们看的电影，还是我们创建和上传的图片和视频。 我们还经常不假思索地在网络上留下一条数据的踪迹，当我们执行日常生活中的行为时。

不仅生成的数据量在增加，而且增长速度也在加快。 从电子邮件到 Facebook 帖子，从购买历史到网络链接，到处都有不断增长的大型数据集。 挑战在于从这些数据中提取最有价值的方面；有时这意味着特定的数据元素，而在其他时候，重点是识别数据片段之间的趋势和关系。

在幕后发生了一种微妙的变化，这一切都是关于以越来越有意义的方式使用数据。 大公司已经意识到数据的价值已经有一段时间了，并一直在利用它来改善他们向客户(即我们)提供的服务。 考虑一下谷歌如何展示与我们的网络冲浪相关的广告，或者亚马逊或 Netflix 如何推荐通常与我们的品味和兴趣非常匹配的新产品或标题。

## 数据的值

如果大规模数据处理不能带来可观的投资回报或竞争优势，这些公司就不会投资。 大数据有几个主要方面值得重视：

*   有些问题只有在被问到足够大的数据集时才会给出价值。 在没有其他因素的情况下，根据另一个人的喜好推荐一部电影不太可能非常准确。 将人数增加到 100 人，机会略有增加。 利用 1000 万其他人的观看历史记录，发现可用于给出相关建议的模式的机会大大提高。
*   与以前的解决方案相比，大数据工具通常能够以更大的规模和更低的成本处理数据。 因此，通常可以执行以前昂贵得令人望而却步的数据处理任务。
*   大规模数据处理的成本不仅仅是财务费用；延迟也是一个关键因素。 一个系统可能能够处理抛给它的尽可能多的数据，但是如果平均处理时间是以周为单位来衡量的，那么它可能就没有什么用处了。 大数据工具允许在控制处理时间的同时增加数据量，通常是通过将增加的数据量与额外的硬件相匹配来实现的。
*   以前关于数据库应该是什么样子或者它的数据应该如何构造的假设可能需要重新考虑，以满足最大的数据问题的需要。
*   与上述几点相结合，足够大的数据集和灵活的工具可以回答以前想象不到的问题。

## 从历史上看，面向少数人而不是多数人

上一节讨论的例子通常以大型搜索引擎和在线公司的创新形式出现。 这延续了一种更为古老的趋势，即处理大型数据集是一项昂贵而复杂的任务，中小型组织负担不起。

同样，更广泛的数据挖掘方法已经存在很长时间了，但在最大的公司和政府机构之外，从来没有真正成为一种实用的工具。

这种情况可能令人遗憾，但大多数较小的组织并不处于劣势，因为它们很少能够获得需要这种投资的大量数据。

然而，数据的增长不再局限于大型公司；许多中小型公司-更不用说一些个人-发现自己收集的数据越来越多，他们怀疑这些数据可能有一些他们想要释放的价值。

在理解如何实现这一点之前，重要的是要了解一些更广泛的历史趋势，它们为今天的 Hadoop 等系统奠定了基础。

### 经典数据处理系统

大数据挖掘系统稀少且昂贵的根本原因是，扩展一个系统以处理大型数据集非常困难；正如我们将看到的，它传统上受限于可以内置到一台计算机中的处理能力。

但是，随着数据大小的增加，有两种扩展系统的主要方法，通常称为向上扩展**-**和向外扩展。

#### _

在大多数企业中，数据处理通常是在价格高得惊人的大型计算机上执行的。 随着数据大小的增长，方法是移动到更大的服务器或存储阵列。 通过一个有效的架构--即使是今天，正如我们将在本章后面描述的那样--这种硬件的成本可以很容易地以数十万美元或数百万美元来衡量。

简单向上扩展的优势在于，架构不会因为增长而发生重大变化。 虽然使用了较大的组件，但基本关系(例如，个数据库服务器和存储阵列)保持不变。 对于商业数据库引擎等应用，软件处理利用可用硬件的复杂性，但从理论上讲，通过将相同的软件迁移到越来越大的服务器上可以实现更大的规模。 但请注意，将软件迁移到越来越多的处理器上的难度绝不是微不足道的；此外，单个主机的大小也有实际限制，因此在某些情况下，向上扩展不能再进一步扩展。

任何规模的单一架构的承诺也是不切实际的。 设计一个纵向扩展系统来处理 1TB、100TB 和 1PB 等大小的数据集，在概念上可能会应用相同组件的更大版本，但随着规模的增加，其连接的复杂性可能会从廉价商品到定制硬件有所不同。

#### 横向扩展的早期方法

横向扩展方法将处理分散到越来越多的机器上，而不是将系统扩展到越来越大的硬件上。 如果数据集翻了一番，只需使用两台服务器，而不是一台双倍大小的服务器。 如果再翻一番，就移到四台主机上。

这种方法的明显好处是，购买成本仍然比扩大规模低得多。 当人们试图购买更大的机器时，服务器硬件成本往往会急剧增加，虽然一台主机的价格可能为 5000 美元，但一台处理能力是其十倍的主机的价格可能是其一百倍。 缺点是，我们需要制定策略，将我们的数据处理分散到一组服务器上，而历史上用于此目的的工具已被证明是复杂的。

因此，部署横向扩展解决方案需要大量的工程工作；系统开发人员通常需要手工制作用于数据分区和重组的机制，更不用说跨群集安排工作和处理单个机器故障的逻辑了。

### 限制因素

在大型企业、政府和学术界之外，这些扩大规模和横向扩展的传统方法并未被广泛采用。 购买成本往往很高，开发和管理系统的努力也是如此。 仅这些因素就让许多小企业望而却步。 此外，随着时间的推移，这些方法本身也有几个明显的弱点：

*   随着横向扩展系统变得越来越大，或者随着纵向扩展系统处理多个 CPU，系统中并发的复杂性带来的困难变得非常严重。 有效利用多个主机或 CPU 是一项非常困难的任务，实施必要的策略以在所需工作负载的整个执行过程中保持效率可能需要付出巨大的努力。
*   硬件的进步--通常以摩尔定律的形式表述--已经开始突显系统能力的差异。 CPU 能力的增长速度远远快于网络或磁盘速度；CPU 周期曾经是系统中最有价值的资源，但今天，这种情况已经不再存在。 与 20 年前相比，现代 CPU 可能能够执行数百万倍的操作，而内存和硬盘速度只增加了数千倍甚至数百倍。 构建一个 CPU 能力如此强大的现代系统是相当容易的，以至于存储系统根本不能以足够快的速度向其提供数据，从而使 CPU 保持忙碌。

## 一种不同的方法

从前面的场景来看，有许多技术已经被成功地用于减轻将数据处理系统扩展到大数据所需的大规模的痛苦。

### 所有道路都会导致横向扩展

正如刚才所暗示的，采取扩大规模的方法进行扩展并不是一种无限制的策略。 可以从主流硬件供应商那里购买的单个服务器的大小是有限制的，即使是更多的小众玩家也不能提供任意大的服务器。 在某一时刻，工作负载将增加到超出单个整体纵向扩展服务器的容量，那又如何呢？ 不幸的是，最好的方法是使用两台大型服务器，而不是一台。 然后，稍后，三，四，以此类推。 或者，换句话说，纵向扩展架构的自然趋势是-在极端情况下-在组合中添加横向扩展策略。 虽然这提供了这两种方法的一些优点，但它也增加了成本和缺点；这种混合体系结构需要两者，而不是非常昂贵的硬件或需要手动开发跨集群逻辑。

由于这种最终趋势和纵向扩展体系结构的总体成本概况，它们很少用于大数据处理领域，而横向扩展体系结构是事实上的标准。

### 提示

如果您的问题空间涉及具有强大内部交叉引用和事务完整性需求的数据工作负载，大型纵向扩展关系数据库仍然可能是一个很好的选择。

### 不共享任何内容

任何有孩子的人都会花相当多的时间教小孩子分享是件好事。 这一原则不适用于数据处理系统，而且这一思想既适用于数据，也适用于硬件。

尤其是横向扩展体系结构的概念性视图显示了各个主机，每个主机处理整个数据集的一个子集，以产生其最终结果的一部分。 现实很少如此直截了当。 相反，主机之间可能需要相互通信，或者多个主机可能需要某些数据。 这些额外的依赖关系会给系统带来两方面的负面影响：瓶颈和增加的故障风险。

如果系统中的每个计算都需要一条数据或单个服务器，则在相互竞争的客户端访问公共数据或主机时，存在争用和延迟的可能性。 例如，如果在具有 25 台主机的系统中，只有一台主机必须由所有其他主机访问，则系统整体性能将受该关键主机的功能限制。

更糟糕的是，如果保存关键数据的这个“热”服务器或存储系统出现故障，整个工作负载将崩溃成堆。 早期的集群解决方案经常显示出这种风险；即使工作负载是跨服务器场处理的，它们通常使用共享存储系统来保存所有数据。

系统的各个组件应该尽可能独立，而不是共享资源，无论其他组件是否被复杂的工作所束缚或正在经历故障，每个组件都可以继续工作。

### 预计失败

在前面的原则中隐含的意思是，将尽可能独立地投入更多的硬件来解决这个问题。 这只有在系统构建时预期单个组件会出现故障(通常是有规律的且时间不方便)时才能实现。

### 备注

您会经常听到“五个九”这样的术语(指的是 99.999%的正常运行时间或可用性)。 虽然这绝对是同类中最好的可用性，但重要的是要认识到，由许多此类设备组成的系统的总体可靠性可能会有很大差异，这取决于系统是否能够容忍单个组件故障。

假设一台服务器具有 99%的可靠性，并且系统需要五台这样的主机才能运行。 系统可用性为 0.99*0.99*0.99*0.99*0.99，相当于 95%的可用性。 但如果单个服务器的评级只有 95%，系统可靠性就会下降到只有 76%。

相反，如果您构建的系统在任何给定时间只需要五台主机中的一台正常工作，则系统可用性将达到五个九的范围。 考虑与每个组件的关键程度相关的系统正常运行时间有助于将重点放在系统可用性可能达到的水平上。

### 提示

如果 99%的可用性这样的数字对您来说有点抽象，请考虑一下在给定时间段内意味着多少停机时间。 例如，99%的可用性相当于每年的停机时间略高于 3.5 天或每月停机 7 小时。 听起来还能达到 99%吗？

这种拥抱失败的方式往往是大数据系统最难让新手充分领会的方面之一。 这也是该方法与纵向扩展架构的最大不同之处。 大型纵向扩展服务器成本高的主要原因之一是用于减轻组件故障影响的工作量。 即使是低端服务器也可能有冗余电源，但在一个大的铁盒中，您会看到 CPU 安装在卡上，这些卡跨多个底板连接到内存和存储系统组。 大型钢铁供应商经常走极端来展示他们的系统有多么有弹性，他们做了一切事情，从服务器运行时拔出部分服务器，到实际向服务器开枪。 但是，如果系统的构建方式不是将每一次失败都视为一场需要缓解的危机，而是将其降为无关紧要的，那么就会出现一个非常不同的架构。

### 智能软件，哑巴硬件

如果我们希望看到硬件集群以尽可能灵活的方式使用，为多个并行工作流提供托管，答案是将智能推向软件，而不是硬件。

在此模型中，硬件被视为一组资源，将硬件分配给特定工作负载的责任交给软件层。 这允许硬件是通用的，因此获得起来既容易又便宜，并且有效使用硬件的功能转移到软件上，而软件是关于有效执行该任务的知识所在。

### 移动处理，而不是数据

假设您有一个非常大的数据集，比如说 1000TB(即 1PB)，并且您需要对数据集中的每个数据执行一组四个操作。 让我们看看实现系统来解决这个问题的不同方式。

传统的大型纵向扩展解决方案将看到一台巨型服务器连接到同样令人印象深刻的存储系统，几乎可以肯定地使用光纤通道等技术来最大化存储带宽。 系统将执行该任务，但会受到 I/O 限制；即使是高端存储交换机也会限制将数据传送到主机的速度。

或者，以前集群技术的处理方法可能会看到一个由 1,000 台机器组成的集群，每台机器都有 1TB 的数据，分为四个象限，每个象限负责执行其中一个操作。 然后，集群管理软件将协调数据在集群中的移动，以确保每一块都接受所有四个处理步骤。 由于每条数据可以在其所在的主机上执行一个步骤，因此它将需要将数据流式传输到其他三个象限，因此我们实际上消耗了 3 PB 的网络带宽来执行处理。

请记住，处理能力的增长速度快于网络或磁盘技术，那么这些真的是解决问题的最佳方法吗？ 最近的经验表明答案是否定的，另一种方法是避免移动数据，而是移动处理。 使用刚才提到的集群，但不要将其划分为象限；相反，让 1000 个节点中的每个节点对本地保存的数据执行所有四个处理阶段。 如果幸运的话，您只需从磁盘流式传输数据一次，而通过网络传输的只有程序二进制文件和状态报告，这两者与实际数据集相比都相形见绌。

如果 1,000 个节点的群集听起来大得离谱，请考虑一下大数据解决方案所使用的一些现代服务器外形规格。 它们看到的是单个主机，每个主机中有多达 12 个 1 TB 或 2 TB 的磁盘。 因为现代处理器有多个核心，所以可以构建一个具有 1 PB 存储空间的 50 节点集群，同时仍然有一个 CPU 核心专门处理来自每个单独磁盘的数据流。

### 构建应用，而不是基础设施

在考虑上一节中的场景时，很多人都会关注数据移动和处理的问题。 但是，任何曾经构建过这样的系统的人都会知道，作业调度、错误处理和协调等不太明显的元素才是真正的魔力所在。

如果我们必须实现用于确定在哪里执行处理、执行处理并将所有子结果合并到整体结果中的机制，我们就不会从旧模型中获得太多好处。 在那里，我们需要显式地管理数据分区；我们只是在交换一个难题和另一个难题。

这涉及到最新的趋势，我们将在这里重点介绍：一个透明地处理大部分集群机制并允许开发人员从业务问题的角度进行思考的系统。 框架提供了定义良好的接口，这些接口抽象了所有这些复杂性-智能软件-在此基础上可以构建特定于业务领域的应用，从而提供了开发人员和系统效率的最佳组合。

## Hadoop

深思熟虑(或怀疑)的读者了解到前面的方法都是 Hadoop 的关键方面时，不会感到惊讶。 但是我们仍然没有真正回答 Hadoop 到底是什么的问题。

### 谢谢，谷歌

这一切都始于谷歌，它在 2003 年和 2004 年发布了两篇描述谷歌技术的学术论文：**Google 文件系统**(**gfs**)([http://research.google.com/archive/gfs.html](http://research.google.com/archive/gfs.html))和 MapReduce([http://research.google.com/archive/mapreduce.html](http://research.google.com/archive/mapreduce.html))。 这两者共同提供了一个以高效方式大规模处理数据的平台。

### 谢谢，道格

与此同时，Doug Cutting 正在开发 Nutch 开源网络搜索引擎。 他一直在研究系统中的元素，这些元素在 Google GFS 和 MapReduce 论文发表后引起了强烈共鸣。 Doug 开始了这些 Google 系统的实现工作，Hadoop 很快就诞生了，最初是 Lucene 的一个子项目，很快就成为了 Apache 开源基金会中自己的顶级项目。 因此，Hadoop 的核心是一个开源平台，它同时提供 MapReduce 和 GFS 技术的实现，并允许跨低成本商用硬件集群处理非常大的数据集。

### 谢谢，雅虎

雅虎在 2006 年聘请了 Doug Cutting，并很快成为 Hadoop 项目最著名的支持者之一。 除了经常宣传一些世界上最大的 Hadoop 部署外，雅虎还允许 Doug 和其他工程师在受雇期间为 Hadoop 做出贡献；雅虎还贡献了一些内部开发的 Hadoop 改进和扩展。 虽然道格现在已经转向 Cloudera(另一家支持 Hadoop 社区的知名初创公司)，雅虎 Hadoop 团队的大部分成员也被剥离出来，成立了一家名为 Hortonworks 的初创公司，但雅虎仍然是 Hadoop 的主要贡献者。

### Hadoop 的部分内容

顶层 Hadoop 项目有许多组件子项目，我们将在本书中讨论其中几个，但主要的两个是**Hadoop 分布式文件系统**(**HDFS**)和 MapReduce。 这些都是 Google 自己的 GFS 和 MapReduce 的直接实现。 我们将对两者进行更详细的讨论，但目前，最好将 HDFS 和 MapReduce 视为一对互补但截然不同的技术。

**HDFS**是一个文件系统，它可以通过跨主机群集向外扩展来存储非常大的数据集。 它具有特定的设计和性能特征；尤其是，它针对吞吐量而不是延迟进行了优化，并且通过复制而不是冗余来实现高可用性。

**MapReduce**是一种数据处理范例，它指定数据将如何从其两个阶段(称为映射和还原)输入和输出，然后将其应用于任意大的数据集。 MapReduce 与 HDFS 紧密集成，确保 MapReduce 任务尽可能直接在保存所需数据的 HDFS 节点上运行。

### 通用构建块

HDFS 和 MapReduce 都展示了上一节中描述的几个体系结构原则。 特别是：

*   两者都设计为在商用(即中低规格)服务器集群上运行
*   两者都通过添加更多服务器来扩展容量(横向扩展)
*   两者都有识别和解决故障的机制
*   两者都透明地提供许多服务，使用户能够专注于手头的问题
*   两者都有一个体系结构，其中软件群集位于物理服务器上，并控制系统执行的所有方面

### HDFS

HDFS 是一个不同于您以前可能遇到的大多数文件系统的文件系统。 它不是兼容 POSIX 的文件系统，这基本上意味着它不能提供与常规文件系统相同的保证。 它也是分布式文件系统，这意味着它将存储分布在多个节点上；在一些历史技术中，缺乏这样高效的分布式文件系统是一个限制因素。 主要功能包括：

*   HDFS 以块为单位存储文件，通常大小至少为 64MB，远远大于大多数文件系统中 4-32KB 的大小。
*   HDFS 针对延迟吞吐量进行了优化；它在流式传输大文件的读取请求方面非常高效，但在许多小文件的寻道请求方面效率很低。
*   HDFS 针对通常为一次写入和多次读取类型的工作负载进行了优化。
*   每个存储节点都运行一个称为 DataNode 的进程，该进程管理该主机上的数据块，这些数据块由在单独主机上运行的主 NameNode 进程协调。
*   HDFS 使用复制，而不是通过在磁盘阵列中设置物理冗余或类似策略来处理磁盘故障。 组成文件的每个数据块都存储在群集内的多个节点上，HDFS NameNode 会持续监视每个 DataNode 发送的报告，以确保故障没有使任何数据块低于所需的复制系数。 如果确实发生这种情况，它会计划在群集中添加另一个拷贝。

### MapReduce

虽然 MapReduce 作为一种相对较新的技术，它建立在数学和计算机科学的许多基础工作的基础上，特别是寻求表达运算的方法，然后这些运算将应用于一组数据中的每个元素。 实际上，称为`map`和`reduce`的各个函数概念直接来自函数式编程语言，它们被应用于输入数据列表。

另一个关键的基本概念是“分而治之”，即将一个问题分解成多个单独的子任务。 当子任务并行执行时，这种方法会变得更加强大；在理想情况下，1000 分钟的任务可以在 1 分钟内被 1000 个并行子任务处理。

**MapReduce**是一个基于这些原则的处理范例；它提供了一系列从源数据集到结果数据集的转换。 在最简单的情况下，输入数据被馈送到`map`函数，结果临时数据被馈送到`reduce`函数。 开发人员只定义数据转换；Hadoop 的 MapReduce 作业管理如何将这些转换并行应用到集群中的数据的过程。 尽管潜在的想法可能并不新颖，但 Hadoop 的一个主要优势在于它如何将这些原则整合到一个可访问和精心设计的平台中。

与传统的关系型数据库不同，传统的关系型数据库需要具有定义良好的模式的结构化数据，MapReduce 和 Hadoop 在半结构化或非结构化数据上工作得最好。 与符合严格模式的数据不同，要求将数据作为一系列键值对提供给`map`函数。 `map`函数的输出是一组其他键值对，`reduce`函数执行聚合以收集最终结果集。

Hadoop 为`map`和`reduce`函数提供了标准规范(即接口)，这些函数的实现通常称为**映射器**和**减法器**。 典型的 MapReduce 作业将由许多映射器和减速器组成，其中几个非常简单，这并不少见。 开发人员专注于表示源和结果数据集之间的转换，Hadoop 框架管理作业执行、并行化和协调的所有方面。

最后一点可能是 Hadoop 最重要的方面。 该平台负责跨数据执行处理的各个方面。 在用户定义了作业的关键标准之后，其他所有事情都将由系统负责。 重要的是，从数据大小的角度来看，相同的 MapReduce 作业可以应用于托管在任何大小的集群上的任何大小的数据集。 如果数据大小为 1 GB，并且位于单个主机上，Hadoop 将相应地安排处理。 即使数据是 1PB 大小，并且托管在 1000 台机器上，它仍然会这样做，决定如何最好地利用所有主机来最有效地执行工作。 从用户的角度来看，数据和集群的实际大小是透明的，除了影响处理作业所需的时间外，它们不会改变用户与 Hadoop 交互的方式。

### 更好的结合在一起

我们可以欣赏 HDFS 和 MapReduce 各自的优点，但当它们组合在一起时，功能会更强大。 HDFS 可以在没有 MapReduce 的情况下使用，因为它本质上是一个大规模的数据存储平台。 尽管 MapReduce 可以从非 HDFS 源读取数据，但其处理性质与 HDFS 非常一致，因此将两者结合使用是迄今为止最常见的用例。

在执行 MapReduce 作业时，Hadoop 需要决定在哪里执行代码以最有效地处理数据集。 如果 MapReduce 群集主机都从单个存储主机或阵列提取其数据，这在很大程度上无关紧要，因为存储系统是共享资源，会导致争用。 但是，如果存储系统是 HDFS，则它允许 MapReduce 在保存感兴趣的数据的节点上执行数据处理，这建立在移动数据处理的成本低于数据本身的原则上。

Hadoop 最常见的部署模型是将 HDFS 和 MapReduce 集群部署在同一组服务器上。 每个包含数据的主机和管理数据的 HDFS 组件还托管一个 MapReduce 组件，该组件可以调度和执行数据处理。 当作业提交到 Hadoop 时，它可以尽可能多地使用优化过程来计划数据所在的主机上的数据，从而最大限度地减少网络流量并最大限度地提高性能。

回想一下我们前面的示例，即如何处理分布在 1000 台服务器上的 1PB 数据上的四步任务。 MapReduce 模型将(以某种简化和理想化的方式)对 HDFS 中驻留的主机上的每段数据执行`map`函数中的处理，然后重用`reduce`函数中的集群，将各个结果收集到最终结果集中。

Hadoop 的部分挑战在于将整个问题分解为`map`和`reduce`函数的最佳组合。 只有当四阶段处理链可以依次独立地应用于每个数据元素时，上述方法才有效。 正如我们将在后面章节中看到的，答案有时是使用多个 MapReduce 作业，其中一个作业的输出是下一个作业的输入。

### 公共架构

如前所述，HDFS 和 MapReduce 都是显示共同特征的软件集群：

*   每个节点都遵循一个体系结构，其中一个工作节点集群由一个特殊的主/协调器节点管理
*   在每种情况下，主服务器(HDFS 的 NameNode 和 MapReduce 的 JobTracker)通过移动数据块或重新调度失败的工作来监视群集的运行状况并处理故障
*   每台服务器(HDFS 的 DataNode 和 MapReduce 的 TaskTracker)上的进程负责在物理主机上执行工作，接收来自 NameNode 或 JobTracker 的指令，并向其报告运行状况/进度状态

作为次要术语，我们通常使用术语**主机**或**服务器**来指代托管 Hadoop 各种组件的物理硬件。 术语**节点**将指包括群集一部分的软件组件。

### 它有什么好处，有什么不好

与任何工具一样，了解 Hadoop 何时适合所讨论的问题非常重要。 本书的大部分内容将基于前面关于处理大数据量的广泛概述来强调其优势，但在不是最佳选择的早期阶段也要开始欣赏它，这一点很重要。

在 Hadoop 中做出的体系结构选择使其成为今天灵活且可伸缩的数据处理平台。 但是，与大多数架构或设计选择一样，必须理解一些后果。 其中最主要的一点是 Hadoop 是一个批处理系统。 当您在大型数据集上执行作业时，框架将一直搅动，直到最终结果准备就绪。 有了大型集群，即使是庞大的数据集也可以相对快速地生成答案，但事实仍然是，生成答案的速度不够快，无法为不耐烦的用户提供服务。 因此，Hadoop 本身并不能很好地适用于低延迟查询，比如在网站、实时系统或类似问题域中接收到的查询。

当 Hadoop 在大型数据集上运行作业时，设置作业、确定在每个节点上运行哪些任务以及所需的所有其他内务管理活动的开销只占总执行时间的一小部分。 但是，对于小数据集上的作业，存在执行开销，这意味着即使是简单的 MapReduce 作业也可能至少需要 10 秒。

### 备注

更广泛的 Hadoop 家族的另一个成员是**HBase**，它是另一项 Google 技术的开源实现。 这在 Hadoop 之上提供了一个(非关系)数据库，该数据库使用各种方法来支持低延迟查询。

但谷歌(Google)和雅虎(Yahoo)不都是这种计算方法的最坚定支持者吗？它们不都是关于响应时间至关重要的网站吗？ 答案是肯定的，它突出了如何将 Hadoop 整合到任何组织或活动中，或者如何将其与其他技术结合使用以发挥各自优势的一个重要方面。 在一篇论文([http://research.google.com/archive/googlecluster.html](http://research.google.com/archive/googlecluster.html))中，谷歌概述了他们当时是如何利用 MapReduce 的；在网络爬虫检索到更新的网页数据后，MapReduce 处理了庞大的数据集，并由此产生了一批 MySQL 服务器用于服务最终用户搜索请求的 Web 索引。

# 使用 Amazon Web 服务的云计算

我们将在本书中探讨的另一个技术领域是云计算，它是由 Amazon Web Services 提供的几个产品的形式。 但首先，我们需要消除围绕云计算的一些炒作和热词。

## 云层太多

云计算已经成为一个过度使用的术语，可以说，过度使用云计算可能会让它变得毫无意义。 因此，在这本书中，当我们使用这个术语时，让我们明确我们的意思-并关心什么。 这主要有两个方面：一种新的架构选项和一种不同的成本方法。

## 第三条路

我们已经讨论过将纵向扩展和横向扩展作为扩展数据处理系统的选项。 但到目前为止，我们的讨论都理所当然地认为，实现这两种选择的物理硬件将由进行系统开发的组织购买、拥有、托管和管理。 我们关心的云计算增加了第三种方法：将您的应用放到云中，让提供商处理可伸缩性问题。

当然，事情并不总是那么简单。 但对于许多云服务来说，这种模式确实是一场革命性的变革。 您可以根据一些已发布的指导方针或接口开发软件，然后将其部署到云平台上，并允许它根据需求扩展服务，当然这是有成本的。 但考虑到制造扩展系统通常涉及的成本，这往往是一个令人信服的命题。

## 不同类型的成本

云计算的这种方法也改变了系统硬件的支付方式。 通过分流基础设施成本，所有用户都能从云提供商实现的规模经济中受益，因为云提供商将其平台构建到能够承载数千或数百万个客户端的规模。 作为用户，您不仅可以让其他人担心诸如扩展等棘手的工程问题，而且您还可以根据需要为容量付费，而不必根据可能的最大工作负载调整系统的大小。 相反，您可以获得灵活性的好处，并根据工作负载需求使用更多或更少的资源。

有一个例子可以帮助说明这一点。 许多公司的财务部门都会在月底运行工作量，以生成税收和薪资数据，而且通常在年末会出现规模大得多的数据处理。 如果让您设计这样的系统，您会购买多少硬件？ 如果您只购买了足够处理日常工作的东西，系统可能会在月底遇到困难，在年终处理结束时可能会遇到真正的麻烦。 如果您针对月末工作负载进行扩展，则系统在一年中的大部分时间都会有空闲容量，并且可能在执行年终处理时仍会遇到问题。 如果您针对年终工作负载进行调整，则系统将有大量容量在今年剩余时间处于闲置状态。 考虑到硬件的购买成本，再加上托管和运行成本-服务器的用电量可能占其生命周期成本的很大一部分-基本上你是在浪费大量的钱。

云计算的按需服务特性使您可以在很小的硬件占用空间上启动应用，然后随着时间的推移进行扩展和缩减。 在按使用付费模式下，您的成本随利用率而变化，您有能力处理工作负载，而无需购买足够的硬件来应对高峰。

这个模型的一个更微妙的方面是，这极大地降低了组织推出在线服务的入门成本。 我们都知道，一个新的热门服务如果不能满足需求，出现性能问题，就很难恢复势头和用户兴趣。 例如，在 2000 年，一个希望成功发布的组织需要在发布当天准备好足够的容量，以满足他们希望但不确定预期的用户流量的巨大激增。 考虑到地理位置的成本，在一次产品发布上花费数百万美元是很容易的。

今天，有了云计算，最初的基础设施成本可能会低至每月几十美元或数百美元，而这只会在流量需求时增加。

## AWS-亚马逊提供的按需基础设施

**Amazon Web Services**(**AWS**)是由 Amazon 提供的一组此类云计算服务。 在本书中，我们将使用其中的几项服务。

### 弹性计算云(EC2)

亚马逊的**弹性计算云**(**EC2**)位于[http://aws.amazon.com/ec2/](http://aws.amazon.com/ec2/)，基本上是随需应变服务器。 注册 AWS 和 EC2 后，只需信用卡详细信息即可访问专用虚拟机，在我们的服务器上轻松运行各种操作系统，包括 Windows 和许多 Linux 版本。

需要更多服务器吗？ 开始更多。 需要更强大的服务器吗？ 更改为所提供的较高规格(和成本)类型之一。 除此之外，EC2 还提供一整套免费服务，包括负载均衡器、静态 IP 地址、高性能附加虚拟磁盘驱动器等。

### 简单存储服务(S3)

亚马逊的**简单存储服务**(**S3**)，在[http://aws.amazon.com/s3/](http://aws.amazon.com/s3/)找到，是一个提供简单键/值存储模型的存储服务。 使用 Web、命令行或编程界面创建对象(可以是从文本文件到图像再到 MP3 的所有对象)，您可以基于分层模型存储和检索数据。 您可以在此模型中创建包含对象的存储桶。 每个存储桶都有一个唯一的标识符，并且在每个存储桶中，每个对象都是唯一命名的。 这一简单的策略实现了一项极其强大的服务，亚马逊对此完全负责(除了数据的可靠性和可用性之外，还负责服务扩展)。

### 弹性 MapReduce(EMR)

亚马逊的**Elastic MapReduce**(**EMR**)位于[http://aws.amazon.com/elasticmapreduce/](http://aws.amazon.com/elasticmapreduce/)，基本上是云中的 Hadoop，构建在 EC2 和 S3 之上。 同样，使用多个界面(Web 控制台、CLI 或 API)中的任何一个，Hadoop 工作流都是使用所需 Hadoop 主机数量和源数据位置等属性定义的。 提供了实现 MapReduce 作业的 Hadoop 代码，并按下了虚拟 Go 按钮。

在其最令人印象深刻的模式下，EMR 可以从 S3 提取源数据，在它在 EC2 上创建的 Hadoop 集群上对其进行处理，将结果推送回 S3，并终止 Hadoop 集群和托管它的 EC2 虚拟机。 当然，这些服务中的每一项都有成本(通常基于每 GB 存储和服务器时间使用量)，但无需专用硬件即可访问如此强大的数据处理功能的能力是非常强大的。

## 这本书涵盖了哪些内容

在这本书中，我们将学习如何编写 MapReduce 程序来执行一些重要的数据处理，以及如何在本地管理和 AWS 托管的 Hadoop 集群上运行这些程序。

我们不仅将 Hadoop 视为执行 MapReduce 处理的引擎，还将探索 Hadoop 功能如何适应组织的基础设施和系统的其余部分。 我们将介绍一些集成的共同点，比如在 Hadoop 和关系数据库之间获取数据，以及如何使 Hadoop 看起来更像这样的关系数据库。

### 一种双重方式

在本书中，我们的讨论不会局限于 Amazon EC2 上托管的 EMR 或 Hadoop；除了展示如何通过 EMR 将处理推到云中之外，我们还将讨论本地 Hadoop 集群(在 Ubuntu Linux 上)的构建和管理。

这有两个原因：首先，尽管 EMR 使 Hadoop 更容易访问，但该技术的某些方面只有在手动管理集群时才会变得明显。 虽然也可以在更手动的模式下使用 EMR，但我们通常会使用本地集群进行此类探索。 其次，虽然这不一定是非此即彼的决定，但许多组织混合使用内部和云托管功能，有时是因为担心过度依赖单个外部提供商，但实际上，在本地容量上进行开发和小规模测试，然后将其按生产规模部署到云中通常比较方便。

在后面的一些章节中，我们将讨论与 Hadoop 集成的其他产品，我们将只给出本地集群的示例，因为无论部署在哪里，这些产品的工作方式都没有区别。

# 摘要

在本章中，我们学到了很多关于大数据、Hadoop 和云计算的知识。

具体地说，我们介绍了大数据的出现，以及数据处理方法和系统架构的变化如何使几乎所有以前昂贵的组织技术都可以使用。

我们还回顾了 Hadoop 的历史，以及它如何在众多趋势的基础上提供可扩展到海量的灵活而强大的数据处理平台。 我们还研究了云计算如何提供另一种系统架构方法，这种方法交换了高额的前期成本和对现收现用模式的直接物理责任，并依赖云提供商进行硬件配置、管理和扩展。 我们还了解了什么是 Amazon Web Services，以及其 Elastic MapReduce 服务如何利用其他 AWS 服务在云中提供 Hadoop。

我们还讨论了本书的目的及其在本地管理和 AWS 托管的 Hadoop 集群上的探索方法。

既然我们已经介绍了基础知识，并且知道了这项技术的来源和它的好处，我们就需要动手并让它运行起来，这就是我们将在[第 2 章](02.html "Chapter 2. Getting Hadoop Up and Running")，*让 Hadoop 启动并运行*中要做的事情。