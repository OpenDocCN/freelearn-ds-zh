# 十、使用 Flume 收集数据

*在前两章中，我们看到了 Have 和 Sqoop 如何为 Hadoop 提供关系数据库接口，并允许它与“真实”数据库交换数据。 虽然这是一个非常常见的用例，但当然，我们可能希望将许多不同类型的数据源引入 Hadoop。*

在本章中，我们将介绍：

*   Hadoop 中常用处理的数据概述
*   将这些数据放入 Hadoop 的简单方法
*   Apache Flume 如何让这项任务变得容易得多
*   简单到复杂的 Flume 设置的常用图案
*   无论采用何种技术都需要考虑的常见问题，如数据生命周期

# 关于 AWS 的说明

本章讨论的 AW 比本书中的任何其他 AW 都要少。 事实上，在这一节之后我们甚至不会提到它。 亚马逊没有类似于 Flume 的服务，所以我们也没有特定于 AWS 的产品可以探索。 另一方面，在使用 Flume 时，无论是在本地主机上还是在 EC2 虚拟实例上，它的工作方式都完全相同。 因此，本章的其余部分不假定执行这些示例的环境；它们将在每个环境上执行相同的操作。

# 数据数据无处不在...

在讨论 Hadoop 与其他系统的集成时，很容易将其视为一对一模式。 数据来自一个系统，在 Hadoop 中处理，然后传递到第三个系统。

事情可能在第一天就是这样，但实际情况往往是一系列相互协作的组件，数据流在它们之间来回传递。 我们如何以可维护的方式构建这个复杂的网络是本章的重点。

## 数据类型

为了便于讨论，我们将数据分为两大类：

*   **网络流量**、，其中数据由系统生成并通过网络连接发送
*   **文件数据**，其中数据是由系统生成的，并写入文件系统上某处的文件

除了检索数据的方式外，我们不假设这些数据类别有任何不同。

## 让网络流量进入 Hadoop

当我们说网络数据时，我们指的是通过 HTTP 连接从 Web 服务器检索的信息、客户端应用拉取的数据库内容或通过数据总线发送的消息。 在每种情况下，数据都由客户端应用检索，该应用要么通过网络拉取数据，要么侦听数据到达。

### 备注

在以下几个示例中，我们将使用`curl`实用程序检索或发送网络数据。 确保它已安装在您的系统上，如果没有，请安装它。

# 该采取行动了-将 Web 服务器数据导入 Hadoop

让我们来看看我们如何简化地将数据从 Web 服务器复制到 HDFS。

1.  将 NameNode Web 界面的文本检索到本地文件：

    ```scala
    $ curl localhost:50070 > web.txt

    ```

2.  Check the file size:

    ```scala
    $ ls -ldh web.txt 

    ```

    您将收到以下响应：

    ```scala
    -rw-r--r-- 1 hadoop hadoop 246 Aug 19 08:53 web.txt

    ```

3.  将文件复制到 HDFS：

    ```scala
    $ hadoop fs -put web.txt web.txt

    ```

4.  Check the file on HDFS:

    ```scala
    $ hadoop fs -ls 

    ```

    您将收到以下响应：

    ```scala
    Found 1 items
    -rw-r--r--   1 hadoop supergroup        246 2012-08-19 08:53 /user/hadoop/web.txt

    ```

## *刚刚发生了什么？*

这里有应该没有什么令人惊讶的地方。 我们使用`curl`实用程序从托管 NameNode Web 界面的嵌入式 Web 服务器检索网页，并将其保存到本地文件。 我们检查文件大小，将其复制到 HDFS，并验证文件是否已成功传输。

这里要注意的不是一系列操作--毕竟这只是我们自[第 2 章](02.html "Chapter 2. Getting Hadoop Up and Running")、*启动并运行*以来一直使用的`hadoop fs`命令的另一种用法--而是我们应该讨论使用的模式。

虽然我们想要的数据位于 Web 服务器中，并且可以通过 HTTP 协议进行访问，但开箱即用的 Hadoop 工具非常基于文件，并且不具备对此类远程信息源的任何内在支持。 这就是我们需要在将网络数据传输到 HDFS 之前将其复制到文件中的原因。

当然，我们可以通过[第 3 章](03.html "Chapter 3. Understanding MapReduce")，*编写 MapReduce 作业*中提到的编程接口将数据直接写入 HDFS，这样做效果很好。 然而，这需要我们开始为需要从中检索数据的每个不同的网络源编写自定义客户端。

## 来个围棋英雄

以编程方式检索数据并将其写入 HDFS 是一项非常强大的功能，值得探索一下。 非常流行的 HTTPJava 库是**Apache****HTTPClient**，位于[http://hc.apache.org/httpcomponents-client-ga/index.html](http://hc.apache.org/httpcomponents-client-ga/index.html)的**HTTP Components**项目中。

使用 HTTPClient 和 Java HDFS 接口像以前一样检索网页并将其写入 HDFS。

## 将文件导入 Hadoop

我们前面的示例展示了将基于文件的数据导入 Hadoop 的最简单方法，以及标准命令行工具或编程 API 的使用。 这里没有什么可讨论的，因为这是我们在整本书中一直在讨论的一个主题。

## 隐藏问题

尽管前面的方法都很好，但有几个原因说明它们可能不适合生产使用。

### 将网络数据保存在网络上

我们将通过网络访问的数据复制到文件，然后再将其放到 HDFS 上的模型会对性能产生影响。 由于往返到磁盘(系统中最慢的部分)会增加延迟。 对于在一次调用中检索的大量数据来说，这可能不是问题-尽管磁盘空间可能会成为一个问题-但对于高速检索的少量数据，这可能会成为一个真正的问题。

### Hadoop 依赖项

对于基于文件的方法，前面提到的模型中隐含着这样一点：我们可以访问文件的点必须能够访问 Hadoop 安装，并且必须配置为知道集群的位置。 这可能会在系统中增加额外的依赖关系；这可能会迫使我们将 Hadoop 添加到真正需要对其一无所知的主机。 我们可以通过使用 SFTP 等工具将文件检索到支持 Hadoop 的计算机，并从那里复制到 HDFS 来缓解此问题。

### 可靠性

请注意，在前面的方法中完全缺少错误处理。 我们使用的工具没有内置的重试机制，这意味着我们需要围绕每个数据检索包装一定程度的错误检测和重试逻辑。

### 重新创建控制盘

最后一点可能涉及到这些特殊方法的最大问题；很容易得到十几个不同的命令行工具和脚本字符串，每个字符串都执行非常相似的任务。 随着时间的推移，重复工作和更困难的错误跟踪方面的潜在成本可能会很大。

### 通用框架方法

在这一点上，任何有企业计算经验的人都会认为，这听起来像是使用某种类型的公共集成框架最好解决的问题。 这是完全正确的，并且确实是诸如**Enterprise Application Integration**(**EAI**)等领域所熟知的一般产品类型。

不过，我们需要的是一个框架，它能够识别 Hadoop，并且可以轻松地与 Hadoop(以及相关项目)集成，而不需要花费大量精力编写自定义适配器。 我们可以创建我们自己的，但是让我们看一下**Apache Flume**，它提供了我们需要的大部分内容。

# Apache Flume 简介

Flume 在[Hadoop](http://flume.apache.org)找到，是另一个与 http://flume.apache.org 紧密集成的 Apache 项目，我们将在本章的剩余部分对其进行探讨。

在我们解释 Flume 可以做什么之前，让我们先弄清楚它不能做什么。 Flume 被描述为用于检索和分发日志的系统，这意味着面向行的文本数据。 它不是一个通用的数据分发平台；尤其是，不要指望使用它来检索或移动二进制数据。

但是，由于在 Hadoop 中处理的绝大多数数据都符合这一描述，因此 Flume 很可能会满足您的许多数据检索需求。

### 备注

Flume 也不是像我们在[第 5 章](05.html "Chapter 5. Advanced MapReduce Techniques")、*高级 MapReduce 技术*中使用的**Avro**那样的通用数据序列化框架，也不是**Thrift**和**Protocol Buffers**等类似技术。 正如我们将看到的，Flume 对数据格式进行了假设，并且没有提供序列化这些格式以外的数据的方法。

Flume 提供了从多个源检索数据、将其传递到远程位置(可能是扇出或管道模型中的多个位置)，然后将其传递到各种目的地的机制。 虽然它确实有一个允许开发自定义源和目标的编程 API，但是这个基础产品对许多最常见的场景都有内置支持。 让我们把它安装好，然后看一看。

## 关于版本化的说明

最近，Flume 经历了一些重大变化。 原始 Flume(现已重命名为**Flume OG**用于原始生成)将被**Flume NG**(**下一代**)取代。 虽然总体原则和功能非常相似，但实现方式却大相径庭。

因为 Flume NG 是未来的，我们将在本书中介绍它。 不过，在一段时间内，它将缺少更成熟的 Flume OG 的几个功能，所以如果您发现 Flume NG 没有满足的特定要求，那么可能值得看看 Flume OG。

# 行动时间-安装和配置 Flume

让我们下载并安装 Flume。

1.  从[http://flume.apache.org/](http://flume.apache.org/)检索最新的 Flume NG 二进制文件，并将其下载并保存到本地文件系统。
2.  将文件移动到所需位置并解压缩：

    ```scala
    $ mv apache-flume-1.2.0-bin.tar.gz /opt
    $ tar -xzf /opt/apache-flume-1.2.0-bin.tar.gz

    ```

3.  创建指向安装的符号链接：

    ```scala
    $ ln -s /opt/apache-flume-1.2.0 /opt/flume

    ```

4.  定义`FLUME_HOME`环境变量：

    ```scala
    Export FLUME_HOME=/opt/flume

    ```

5.  将 Flume`bin`目录添加到您的路径：

    ```scala
    Export PATH=${FLUME_HOME}/bin:${PATH}

    ```

6.  验证是否设置了`JAVA_HOME`：

    ```scala
    Echo ${JAVA_HOME}

    ```

7.  验证 Hadoop 库是否位于类路径中：

    ```scala
    $ echo ${CLASSPATH}

    ```

8.  创建将用作 Flume`conf`目录的目录：

    ```scala
    $ mkdir /home/hadoop/flume/conf

    ```

9.  将所需文件复制到`conf`目录：

    ```scala
    $ cp /opt/flume/conf/log4j.properties /home/hadoop/flume/conf
    $ cp /opt/flume/conf/flume-env.sh.sample /home/hadoop/flume/conf/flume-env.sh

    ```

10.  编辑`flume-env.sh`并设置`JAVA_HOME`。

## *刚刚发生了什么？*

Flume 安装非常简单，并且与我们之前安装的工具具有类似的先决条件。

首先，我们检索了最新版本的 Flume NG(任何 1.2.x 或更高版本都可以)，并将其保存到本地文件系统。 我们将其移动到所需位置，将其解压缩，并创建了指向该位置的方便符号链接。

我们需要定义`FLUME_HOME`环境变量，并将安装目录中的`bin`目录添加到类路径中。 如前所述，这可以直接在命令行上完成，也可以在方便的脚本中完成。

Flume 要求定义`JAVA_HOME`，我们证实了这一点。 它还需要 Hadoop 库，所以我们检查 Hadoop 类是否在类路径中。

虽然最后几个步骤将用于生产，但并不是严格意义上的演示所必需的。 Flume 查找配置目录，其中包含定义默认日志记录属性和环境设置变量(如`JAVA_HOME`)的文件。 我们发现 Flume 在正确设置此目录时的性能是最可预测的，因此我们现在就这样做了，不需要在很长时间内更改它。

我们假设`/home/hadoop/flume`是存储 Flume 配置和其他文件的工作目录；根据您的系统的需要更改此目录。

## 使用 Flume 捕获网络数据

现在我们已经安装了 Flume，让我们使用它来捕获一些网络数据。

# 在日志文件中捕获网络流量的操作时间

在第一个实例中，让我们使用一个简单的 Flume 配置，该配置将把网络数据捕获到主 Flume 日志文件中。

1.  在您的 Flume 工作目录中创建以下文件`agent1.conf`：

    ```scala
    agent1.sources = netsource
    agent1.sinks = logsink
    agent1.channels = memorychannel

    agent1.sources.netsource.type = netcat
    agent1.sources.netsource.bind = localhost
    agent1.sources.netsource.port = 3000

    agent1.sinks.logsink.type = logger

    agent1.channels.memorychannel.type = memory
    agent1.channels.memorychannel.capacity = 1000
    agent1.channels.memorychannel.transactionCapacity = 100

    agent1.sources.netsource.channels = memorychannel
    agent1.sinks.logsink.channel = memorychannel
    ```

2.  Start a Flume agent:

    ```scala
    $ flume-ng agent --conf conf --conf-file 10a.conf  --name agent1 

    ```

    上述命令的输出如下图所示：

    ![Time for action – capturing network traffic in a log file](img/7300_10_01.jpg)

3.  在另一个窗口中，打开到本地主机上端口 3000 的 telnet 连接，然后键入一些文本：

    ```scala
    $ curl telnet://localhost:3000
    Hello
    OK
    Flume!
    OK

    ```

4.  使用*Ctrl*+*C*关闭卷曲连接。
5.  Look at the Flume log file:

    ```scala
    $ tail flume.log

    ```

    您将收到以下响应：

    ```scala
    2012-08-19 00:37:32,702 INFO sink.LoggerSink: Event: { headers:{} body: 68 65 6C 6C 6F                                  Hello }
    2012-08-19 00:37:32,702 INFO sink.LoggerSink: Event: { headers:{} body: 6D 65                                           Flume }

    ```

## *刚刚发生了什么？*

首先，我们在 Flume 工作目录中创建了一个 Flume 配置文件。 我们稍后将更详细地讨论这一点，但现在，请考虑 Flume 通过名为**source**的组件接收数据，并将其写入名为**Sink**的目的地。

在本例中，我们创建了一个**Netcat**源，它在端口上监听网络连接。 您可以看到，我们将其配置为绑定到本地计算机上的端口 3000。

配置的是类型`logger`，毫不奇怪，它会将其输出写入日志文件。 配置文件的其余部分定义名为`agent1`的**代理**，它使用此源和接收器。

然后，我们使用`flume-ng`二进制文件启动 Flume 代理。 这是我们将用来启动所有 Flume 进程的工具。 请注意，我们为此命令提供了几个选项：

*   `agent`参数告诉 Flume 启动代理，该代理是数据移动中涉及的正在运行的 Flume 进程的通用名称
*   如前所述，`conf`目录
*   我们将要启动的进程的特定配置文件
*   配置文件中代理的名称

代理将启动，且屏幕上不会显示进一步输出。 (显然，我们将在生产环境中的后台运行该过程。)

在另一个窗口中，我们使用`curl`实用程序打开到本地计算机端口 3000 的 telnet 连接。 打开此类会话的传统方式当然是 telnet 程序本身，但许多 Linux 发行版默认安装了 curl；几乎没有一个使用较旧的`telnet`实用程序。

我们在每行键入一个单词并按*Enter*，然后使用*Ctrl*+*C*命令终止会话。 最后，我们查看正在写入 Flume 工作目录的`flume.log`文件，并看到我们键入的每个单词都有一个条目。

# 将操作记录到控制台的时间

查看日志文件并不总是很方便，特别是在我们已经打开代理屏幕的情况下。 让我们修改代理，使其也将事件记录到屏幕上。

1.  Restart the Flume agent with an additional argument:

    ```scala
    $ flume-ng agent --conf conf --conf-file 10a.conf --name agent1 -Dflume.root.logger=INFO,console

    ```

    您将收到以下响应：

    ```scala
    Info: Sourcing environment configuration script /home/hadoop/flume/conf/flume-env.sh
    …
    org.apache.flume.node.Application --conf-file 10a.conf --name agent1
    2012-08-19 00:41:45,462 (main) [INFO - org.apache.flume.lifecycle.LifecycleSupervisor.start(LifecycleSupervisor.java:67)] Starting lifec

    ```

2.  在另一个窗口中，通过 cURL：

    ```scala
    $ curl telnet://localhost:3000

    ```

    连接到服务器
3.  Type in `Hello` and `Flume` on separate lines, hit *Ctrl* + *C*, and then check the agent window:

    ![Time for action – logging to the console](img/7300_10_02.jpg)

## *刚刚发生了什么？*

我们添加了这个示例，因为它在调试或创建新流时变得非常有用。

如上例所示，默认情况下，Flume 会将其日志写入文件系统上的一个文件。 更确切地说，这是在我们的`conf`目录中的 log4j 属性文件中指定的默认行为。 有时，我们希望获得更即时的反馈，而无需不断查看日志文件或更改属性文件。

通过在命令行上显式设置变量`flume.root.logger`，我们可以覆盖默认的记录器配置，并将输出直接发送到代理窗口。 记录器是标准的 log4j，因此支持`DEBUG`和`INFO`等普通日志级别。

## 将网络数据写入日志文件

Flume 将其接收的数据写入日志文件的默认日志接收器行为有一些限制，特别是当我们想要在其他应用中使用捕获的数据时。 通过配置不同类型的接收器，我们可以将数据写入更多可使用的数据文件。

# 执行操作的时间-将命令的输出捕获到平面文件

让我们用的方法来演示这一点，同时演示一种新的源。

1.  在 Flume 工作目录中创建以下文件`agent2.conf`：

    ```scala
    agent2.sources = execsource
    agent2.sinks = filesink
    agent2.channels = filechannel

    agent2.sources.execsource.type = exec
    agent2.sources.execsource.command = cat /home/hadoop/message

    agent2.sinks.filesink.type = FILE_ROLL
    agent2.sinks.filesink.sink.directory = /home/hadoop/flume/files
    agent2.sinks.filesink.sink.rollInterval = 0

    agent2.channels.filechannel.type = file
    agent2.channels.filechannel.checkpointDir = /home/hadoop/flume/fc/checkpoint
    agent2.channels.filechannel.dataDirs = /home/hadoop/flume/fc/data

    agent2.sources.execsource.channels = filechannel
    agent2.sinks.filesink.channel = filechannel
    ```

2.  在主目录中创建一个简单的测试文件：

    ```scala
    $ echo "Hello again Flume!" > /home/hadoop/message

    ```

3.  启动代理：

    ```scala
    $ flume-ng agent --conf conf --conf-file agent2.conf --name agent2

    ```

4.  In another window, check file sink output directory:

    ```scala
    $ ls files
    $ cat files/*

    ```

    上述命令的输出如下图所示：

    ![Time for action – capturing the output of a command to a flat file](img/7300_10_03.jpg)

## *刚刚发生了什么？*

前面的示例遵循与前面类似的模式。 我们为 Flume 代理创建了配置文件，运行了代理，然后确认它已经捕获了我们预期的数据。

这次我们使用了 EXEC 源和`file_roll`接收器。 顾名思义，前者在主机上执行命令并捕获其输出作为 Flume 代理的输入。 尽管在前一种情况下，该命令只执行一次，但这只是为了说明目的。 更常见的用法是使用产生持续数据流的命令。 请注意，可以将 EXEC 接收器配置为在命令终止时重新启动该命令。

代理的输出被写入配置文件中指定的文件。 默认情况下，Flume 每 30 秒轮换(滚动)一个新文件；我们禁用此功能是为了更容易跟踪单个文件中发生的事情。

我们看到该文件确实包含指定的`exec`命令的输出。

### 日志与文件

为什么 Flume 既有日志接收器又有文件接收器，这可能不是很明显。 从概念上讲，两者做的是一样的事情，那么有什么不同呢？

实际上，记录器接收器更像是一个调试工具。 它不仅记录源捕获的信息，还添加了大量附加元数据和事件。 然而，文件接收器记录的输入数据与接收到的数据完全一样，没有任何更改-尽管如果需要的话，这是可能的，正如我们稍后将看到的那样。

在大多数情况下，您会希望文件接收器捕获输入数据，但根据您的需要，日志也可能在非生产情况下有用。

# 执行操作的时间-在本地平面文件中捕获远程文件

让我们展示另一个将数据捕获到文件接收器的示例。 这一次，我们将使用另一个允许它从远程客户端接收数据的 Flume 功能。

1.  在 Flume 工作目录中创建以下文件`agent3.conf`：

    ```scala
    agent3.sources = avrosource
    agent3.sinks = filesink
    agent3.channels = jdbcchannel

    agent3.sources.avrosource.type = avro
    agent3.sources.avrosource.bind = localhost
    agent3.sources.avrosource.port = 4000
    agent3.sources.avrosource.threads = 5

    agent3.sinks.filesink.type = FILE_ROLL
    agent3.sinks.filesink.sink.directory = /home/hadoop/flume/files
    agent3.sinks.filesink.sink.rollInterval = 0

    agent3.channels.jdbcchannel.type = jdbc

    agent3.sources.avrosource.channels = jdbcchannel
    agent3.sinks.filesink.channel = jdbcchannel
    ```

2.  将新测试文件创建为`/home/hadoop/message2`：

    ```scala
    Hello from Avro!
    ```

3.  启动 Flume 代理：

    ```scala
    $ flume-ng agent –conf conf –conf-file agent3.conf –name agent3 

    ```

4.  在另一个窗口中，使用 Flume Avro 客户端将文件发送到代理：

    ```scala
    $ flume-ng avro-client -H localhost -p 4000 -F /home/hadoop/message

    ```

5.  As before, check the file in the configured output directory:

    ```scala
    $ cat files/*

    ```

    上述命令的输出如以下截图所示：

    ![Time for action – capturing a remote file in a local flat file](img/7300_10_04.jpg)

## *刚刚发生了什么？*

与前面一样，我们创建了一个新的配置文件，这一次为代理使用了 avro 源。 回想一下[第 5 章](05.html "Chapter 5. Advanced MapReduce Techniques")，*高级 MapReduce 技术*，Avro 是一个数据序列化框架；也就是说，它管理数据在网络上从一个点到另一个点的打包和传输。 与 Netcat 源类似，Avro 源需要指定其网络设置的配置参数。 在本例中，它将侦听本地计算机上的端口 4000。 代理配置为像以前一样使用文件接收器，我们照常启动它。

Flume 既有 Avro 源，也有独立的 Avro 客户端。 后者可用于读取文件并将其发送到网络上任何位置的 Avro 源。 在我们的示例中，我们只使用本地机器，但请注意，avro 客户端需要它应该将文件发送到的 avro 源的显式主机名和端口。 因此，这不是一个限制；Avro 客户端可以将文件发送到网络上任何位置的监听 Flume Avro 源。

Avro 客户端读取文件，将其发送到代理，并将其写入文件接收器。 我们通过确认文件内容是否如预期的那样位于文件接收器位置来检查此行为。

## 源、汇和通道

我们在前面的示例中有意使用了各种源、汇和通道，只是为了说明如何混合和匹配它们。 然而，我们还没有对它们进行过详细的探索，特别是渠道。 现在让我们更深入地挖掘一下。

### 来源

我们查看了三个来源：Netcat、exec 和 Avro。 Flume NG 还支持序列生成器源(主要用于测试)以及读取**syslogd**数据的源的 TCP 和 UDP 变体。 每个源都配置在一个代理中，在接收到足够的数据以生成 Flume 事件后，它会将这个新创建的事件发送到源所连接的通道。 尽管源可能具有与其如何读取数据、转换事件和处理故障情况相关的逻辑，但源不知道如何存储事件。 源负责将事件传递到配置的通道，而事件处理的所有其他方面对源是不可见的。

### Flume

除了我们之前使用的记录器和文件滚动槽，Flume 还支持 HDFS、HBase(两种类型)、Avro(用于代理链接)、NULL(用于测试)和 IRC(用于 Internet 中继聊天服务)的接收器。 接收器在概念上类似于源，但情况相反。

接收器等待从配置的通道接收事件，它对其内部工作原理一无所知。 接收时，接收器将事件的输出处理到其特定目标，管理有关超时、重试和轮换的所有问题。

### 通道

那么连接信源和信宿的神秘通道是什么呢？ 正如前面的名称和配置条目所暗示的那样，它们是管理事件交付的通信和保留机制。

当我们定义一个源和一个接收器时，它们读取和写入数据的方式可能会有很大的不同。 例如，EXEC 源接收数据的速度可能比文件卷接收器写入数据的速度快得多，或者源可能有需要暂停写入的时间(例如，当滚动到新文件或处理系统 I/O 拥塞时)。 因此，通道需要在源和宿之间缓冲数据，以允许数据尽可能高效地流经代理。 这就是为什么我们的配置文件的通道配置部分包含容量等元素的原因。

**存储器**通道最容易理解，因为事件被从源存储器读取到存储器中，并在接收器能够接收它们时传递到接收器。 但是，如果代理进程在进程中途死亡(无论是由于软件还是硬件故障)，则内存通道中当前的所有事件都将永远丢失。

我们还使用的**文件**和**JDBC**通道提供事件的持久存储，以防止此类丢失。 从源读取事件后，文件通道将内容写入文件系统上的文件，该文件仅在成功传递到接收器后才会删除。 类似地，JDBC 通道使用嵌入式 Derby 数据库以可恢复的方式存储事件。

这是经典的性能与可靠性之间的权衡。 内存通道速度最快，但有数据丢失的风险。 文件和 JDBC 通道通常要慢得多，但可以有效地向接收器提供有保证的传输。 您选择哪个频道取决于应用的性质和每个事件的值。

### 备注

不要过于担心这种取舍；在现实世界中，答案通常是显而易见的。 此外，一定要仔细检查使用的信源和信宿的可靠性。 如果这些都是不可靠的，您无论如何都会丢弃事件，那么您会从持久通道中获益很多吗？

### 或者自己滚

不要觉得被现有的源、汇和渠道集合所限制。 Flume 提供了一个接口来定义您自己的实现。 此外，Flume OG 中存在的一些组件尚未整合到 Flume NG 中，但可能会在未来出现。

## 了解 Flume 配置文件

现在我们已经讨论了个源、接收器和通道，让我们更详细地看看前面的一个配置文件：

```scala
agent1.sources = netsource
agent1.sinks = logsink
agent1.channels = memorychannel
```

这些第一行命名代理，并定义与其关联的源、汇和通道。 每行可以有多个值；这些值以空格分隔：

```scala
agent1.sources.netsource.type = netcat
agent1.sources.netsource.bind = localhost
agent1.sources.netsource.port = 3000
```

这些行指定源的配置。 由于我们使用的是 Netcat 源，因此配置值指定它应该如何绑定到网络。 每种类型的源都有自己的配置变量。

```scala
agent1.sinks.logsink.type = logger
```

这指定要使用的接收器是通过命令行或 log4j 属性文件进一步配置的记录器接收器。

```scala
agent1.channels.memorychannel.type = memory
agent1.channels.memorychannel.capacity = 1000
agent1.channels.memorychannel.transactionCapacity = 100
These lines specify the channel to be used and then add the type specific configuration values.  In this case we are using the memory channel and we specify its capacity but – since it is non-persistent – no external storage mechanism.
agent1.sources.netsource.channels = memorychannel
agent1.sinks.logsink.channel = memorychannel
```

最后的行配置用于信源和信宿的通道。 虽然我们对不同的代理使用了不同的配置文件，但我们可以将所有元素放在单个配置文件中，因为各个代理名称提供了必要的分隔。 然而，这可能会产生一个非常冗长的文件，当您刚刚学习 Flume 时，它可能会有点吓人。 我们也可以在给定的代理中有多个流，例如，我们可以将前面的两个示例组合到单个配置文件和代理中。

## 来个围棋英雄

就这么做吧！ 在包含以下内容的单个复合代理中创建一个配置文件，该文件指定前面示例中前面的`agent1`和`agent2`的功能：

*   Netcat 源及其关联的记录器接收器
*   EXEC 源及其关联的文件接收器
*   两个内存通道，分别用于前面提到的源/宿对

为了让您开始，下面是组件定义的外观：

```scala
agentx.sources = netsource execsource
agentx.sinks = logsink filesink
agentx.channels = memorychannel1 memorychannel2
```

## 这一切都是关于事件的

在我们尝试另一个例子之前，让我们再讨论一个定义。 究竟什么是事件？

请记住，Flume 是明确基于日志文件的，因此在大多数情况下，一个事件等同于一行文本，后跟一个换行符。 这就是我们在使用过的源和汇上看到的行为。

然而，情况并不总是如此，例如，UDP syslogd 源将接收到的每个数据包视为通过系统传递的单个事件。 然而，在使用这些接收器和源时，这些事件定义是不可更改的，例如，在读取文件时，我们别无选择，只能使用基于行的事件。

# 将网络流量写入 HDFS 的操作时间

在一本关于 Hadoop 的书中关于 Flume 的讨论到目前为止还没有实际使用过 Hadoop。 让我们通过 Flume 将数据写入 HDFS 来解决这个问题。

1.  在 Flume 工作目录中创建以下文件`agent4.conf`：

    ```scala
    agent4.sources = netsource
    agent4.sinks = hdfssink
    agent4.channels = memorychannel

    agent4.sources.netsource.type = netcat
    agent4.sources.netsource.bind = localhost
    agent4.sources.netsource.port = 3000

    agent4.sinks.hdfssink.type = hdfs
    agent4.sinks.hdfssink.hdfs.path = /flume
    agent4.sinks.hdfssink.hdfs.filePrefix = log
    agent4.sinks.hdfssink.hdfs.rollInterval = 0
    agent4.sinks.hdfssink.hdfs.rollCount = 3
    agent4.sinks.hdfssink.hdfs.fileType = DataStream

    agent4.channels.memorychannel.type = memory
    agent4.channels.memorychannel.capacity = 1000
    agent4.channels.memorychannel.transactionCapacity = 100

    agent4.sources.netsource.channels = memorychannel
    agent4.sinks.hdfssink.channel = memorychannel
    ```

2.  启动代理：

    ```scala
    $ flume-ng agent –conf conf –conf-file agent4.conf –name agent4 

    ```

3.  在另一个窗口中，打开 telnet 连接并向 Flume 发送七个事件：

    ```scala
    $ curl telnet://localhost:3000

    ```

4.  Check the contents of the directory specified in the Flume configuration file and then examine the file contents:

    ```scala
    $ hadoop fs -ls /flume
    $ hadoop fs –cat "/flume/*"

    ```

    前面命令的输出可以在下面的屏幕截图中显示：

    ![Time for action – writing network traffic onto HDFS](img/7300_10_05.jpg)

## *刚刚发生了什么？*

这一次，我们将 Netcat 源与 HDFS 接收器配对。 从配置文件中可以看出，我们需要指定文件的位置、任何文件前缀以及从一个文件滚动到另一个文件的策略等方面。 在本例中，我们在`/flume`目录中指定了文件，每个文件都以`log-`开头，并且每个文件中最多有三个条目(显然，这样的低值仅用于测试)。

启动代理后，我们再次使用 cURL 向 Flume 发送 7 个单字事件。 然后，我们使用 Hadoop 命令行实用程序查看目录内容，并验证我们的输入数据是否正在写入 HDFS。

请注意，第三个 HDFS 文件的扩展名为`.tmp`。 请记住，我们为每个文件指定了三个条目，但只输入了七个值。 因此，我们填满了两个文件，开始了另一个文件。 Flume 为当前正在写入的文件赋予了`.tmp`扩展名，这使得在指定要通过 MapReduce 作业处理哪些文件时，很容易区分已完成的文件和正在进行的文件。

# 动作添加时间戳时间

我们在前面提到过，有一些机制可以让文件数据以稍微复杂的方式写入。 让我们做一些非常常见的事情，并使用动态创建的时间戳将我们的数据写入到一个目录中。

1.  将以下配置文件创建为`agent5.conf`：

    ```scala
    agent5.sources = netsource
    agent5.sinks = hdfssink
    agent5.channels = memorychannel

    agent5.sources.netsource.type = netcat
    agent5.sources.netsource.bind = localhost
    agent5.sources.netsource.port = 3000
    agent5.sources.netsource.interceptors = ts

    agent5.sources.netsource.interceptors.ts.type = org.apache.flume.interceptor.TimestampInterceptor$Builder

    agent5.sinks.hdfssink.type = hdfs
    agent5.sinks.hdfssink.hdfs.path = /flume-%Y-%m-%d
    agent5.sinks.hdfssink.hdfs.filePrefix = log-
    agent5.sinks.hdfssink.hdfs.rollInterval = 0
    agent5.sinks.hdfssink.hdfs.rollCount = 3
    agent5.sinks.hdfssink.hdfs.fileType = DataStream

    agent5.channels.memorychannel.type = memory
    agent5.channels.memorychannel.capacity = 1000
    agent5.channels.memorychannel.transactionCapacity = 100

    agent5.sources.netsource.channels = memorychannel
    agent5.sinks.hdfssink.channel = memorychannel
    ```

2.  启动代理：

    ```scala
    $ flume-ng agent –conf conf –conf-file agent5.conf –name agent5

    ```

3.  在另一个窗口中，打开 telnet 会话并向 Flume 发送七个事件：

    ```scala
    $ curl telnet://localhost:3000

    ```

4.  Check the directory name on HDFS and the files within it:

    ```scala
    $ hadoop fs -ls /

    ```

    上述代码的输出可以在下面的屏幕截图中显示：

    ![Time for action – adding timestamps](img/7300_10_06.jpg)

## *刚刚发生了什么？*

我们对前面的配置文件进行了一些更改。 我们向 Netcat 源代码添加了`interceptor`规范，并将其实现类指定为`TimestampInterceptor`。

Flume 拦截器是可以在事件从源传递到通道之前操作和修改事件的插件。 大多数拦截器要么向事件添加元数据(如本例所示)，要么根据特定条件删除事件。 除了几个内置的拦截器之外，自然还有一种用于用户定义的拦截器的机制。

我们在这里使用了**时间戳**拦截器，它将读取事件时的 Unix 时间戳添加到事件元数据中。 这允许我们扩展要将事件写入其中的 HDFS 路径的定义。

虽然以前我们只是将所有事件写入`/flume`目录，但现在我们将路径指定为`/flume-%Y-%m-%d`。 在运行代理并将一些数据发送到 Flume 之后，我们查看了 HDFS，发现这些变量已经展开，为目录提供了年/月/日后缀。

HDFS 接收器支持许多其他变量，如源的主机名和允许精确分区到秒级别的附加时间变量。

这里的实用程序很简单；这种简单的机制可以提供自动分区，使数据管理变得更容易，但也为 MapReduce 作业提供了更简单的数据接口，而不是将所有事件写入一个随时间而变得庞大的单个目录中。 例如，如果您的大多数 MapReduce 作业都处理每小时一次的数据，那么让 Flume 将传入事件划分到每小时一次的目录中将使您的工作变得容易得多。

准确地说，通过 Flume 的事件添加了完整的 Unix 时间戳，即精确到最接近的秒。 在我们的示例中，我们在目录规范中仅使用与日期相关的变量，如果需要每小时或更细粒度的目录分区，则将使用与时间相关的变量。

### 备注

这里假设处理点的时间戳足以满足您的需要。 如果正在对文件进行批处理，然后将其送入 Flume，则文件内容的时间戳可能来自前一小时的时间戳，而不是它们被处理时的时间戳。 在这种情况下，您可以编写一个自定义拦截器来根据文件内容设置时间戳头。

## 到 Sqoop 或到 Flume...

一个明显的问题是，如果我们想要将关系数据库中的数据导出到 HDFS 上，那么 Sqoop 或 Flume 哪一个最合适。 我们已经看到了 Sqoop 如何执行这样的导出，我们可以使用 Flume 执行类似的操作，可以使用自定义源，甚至只需将对`mysql`命令的调用包装在 EXEC 源中即可。

一个很好的经验法则是查看数据类型，并询问它是日志数据还是其他更复杂的数据。

Flume 在很大程度上是为了处理日志数据而创建的，它在这方面非常出色。 但在大多数情况下，Flume 网络负责将事件从源传送到汇点，而不会对日志数据本身进行任何真正的转换。 如果您在多个关系数据库中有日志数据，那么 Flume 可能是一个很好的选择，尽管我会质疑使用数据库存储日志记录的长期可伸缩性。

非日志数据可能需要只有 Sqoop 才能执行的数据操作。 我们在上一章中使用 Sqoop 执行的许多转换，比如指定要检索的列的子集，实际上使用 Flume 是不可能的。 如果您正在处理需要单独字段处理的结构化数据，那么 Flume 本身也很可能不是理想的工具。 如果您想要直接集成 Hive，那么 Sqoop 是您唯一的选择。

当然，请记住，这些工具还可以在更复杂的工作流程中协同工作。 事件可以通过 Flume 收集到 HDFS 上，通过 MapReduce 进行处理，然后通过 Sqoop 导出到关系数据库中。

# 行动时间-多级 Flume 网络

让我们把前面提到的几个部分放在一起，看看一个 Flume 代理如何使用另一个代理作为它的接收器。

1.  将以下文件创建为`agent6.conf`：

    ```scala
    agent6.sources = avrosource
    agent6.sinks = avrosink
    agent6.channels = memorychannel

    agent6.sources.avrosource.type = avro
    agent6.sources.avrosource.bind = localhost
    agent6.sources.avrosource.port = 2000
    agent6.sources.avrosource.threads = 5

    agent6.sinks.avrosink.type = avro
    agent6.sinks.avrosink.hostname = localhost
    agent6.sinks.avrosink.port = 4000

    agent6.channels.memorychannel.type = memory
    agent6.channels.memorychannel.capacity = 1000
    agent6.channels.memorychannel.transactionCapacity = 100

    agent6.sources.avrosource.channels = memorychannel
    agent6.sinks.avrosink.channel = memorychannel
    ```

2.  启动按照前面创建的`agent3.conf`文件配置的代理，即使用 avro 源和文件接收器：

    ```scala
    $ flume-ng client –conf conf –conf-file agent3.conf agent3 

    ```

3.  在第二个窗口中，启动另一个代理；该代理配置有前面的文件：

    ```scala
    $ flume-ng client –conf conf –conf-file agent6.conf agent6

    ```

4.  在第三个窗口中，使用 Avro 客户端将文件发送到每个 Flume 代理：

    ```scala
    $ flume-ng avro-client –H localhost –p 4000 –F /home/hadoop/message
    $ flume-ng avro-client -H localhost -p 2000 -F /home/hadoop/message2

    ```

5.  Check the output directory for files and examine the file present:

    ![Time for action – multi level Flume networks](img/7300_10_07.jpg)

## *刚刚发生了什么？*

首先，我们定义了一个具有 Avro 信源和 Avro 信宿的新代理。 我们以前没有使用过这个接收器；这个接收器不是将事件写入本地位置或 HDFS，而是使用 avro 将事件发送到远程源。

我们先启动这个新代理的一个实例，然后再启动前面使用的`agent3`的一个实例。 回想一下，该代理有一个 avro 源和一个文件滚动接收器。 我们将第一个代理中的 Avro 接收器配置为指向第二个代理中的 Avro 接收器的主机和端口，并通过这样做来构建数据路由链。

在两个代理都运行的情况下，我们然后使用 avro 客户端向每个代理发送一个文件，并确认它们都出现在配置为`agent3`接收器目标的文件位置。

这不仅仅是技术能力本身。 此功能是允许 Flume 构建任意复杂的分布式事件收集网络的构建块。 不是每个代理的一个副本，而是每种类型的多个代理将事件馈送到链中的下一个链接，该链接充当事件聚合点。

# 对多个接收器进行动作写入的时间

我们需要最后一项功能来构建这样的网络，即可以写入多个接收器的代理。 让我们创建一个。

1.  将以下配置文件创建为`agent7.conf`：

    ```scala
    agent7.sources = netsource
    agent7.sinks = hdfssink filesink
    agent7.channels = memorychannel1 memorychannel2

    agent7.sources.netsource.type = netcat
    agent7.sources.netsource.bind = localhost
    agent7.sources.netsource.port = 3000
    agent7.sources.netsource.interceptors = ts

    agent7.sources.netsource.interceptors.ts.type = org.apache.flume.interceptor.TimestampInterceptor$Builder

    agent7.sinks.hdfssink.type = hdfs
    agent7.sinks.hdfssink.hdfs.path = /flume-%Y-%m-%d
    agent7.sinks.hdfssink.hdfs.filePrefix = log
    agent7.sinks.hdfssink.hdfs.rollInterval = 0
    agent7.sinks.hdfssink.hdfs.rollCount = 3
    agent7.sinks.hdfssink.hdfs.fileType = DataStream

    agent7.sinks.filesink.type = FILE_ROLL
    agent7.sinks.filesink.sink.directory = /home/hadoop/flume/files
    agent7.sinks.filesink.sink.rollInterval = 0

    agent7.channels.memorychannel1.type = memory
    agent7.channels.memorychannel1.capacity = 1000
    agent7.channels.memorychannel1.transactionCapacity = 100

    agent7.channels.memorychannel2.type = memory
    agent7.channels.memorychannel2.capacity = 1000
    agent7.channels.memorychannel2.transactionCapacity = 100

    agent7.sources.netsource.channels = memorychannel1 memorychannel2
    agent7.sinks.hdfssink.channel = memorychannel1
    agent7.sinks.filesink.channel = memorychannel2

    agent7.sources.netsource.selector.type = replicating
    ```

2.  启动代理：

    ```scala
    $ flume-ng agent –conf conf –conf-file agent7.conf –name agent7 

    ```

3.  Open a telnet session and send an event to Flume:

    ```scala
    $ curl telnet://localhost:3000

    ```

    您将收到以下响应：

    ```scala
    Replicating!
    Check the contents of the HDFS and file sinks:
    $ cat files/*
    $ hdfs fs –cat "/flume-*/*"

    ```

    上述命令的输出如下图所示：

    ![Time for action – writing to multiple sinks](img/7300_10_08.jpg)

## *刚刚发生了什么？*

我们创建了一个配置文件，其中包含单个 Netcat 源文件以及该文件和 HDFS 接收器。 我们配置了单独的内存通道，将源连接到两个接收器。

然后，我们将源选择器类型设置为`replicating`，这意味着事件将被发送到所有已配置的通道。

在正常启动代理并将事件发送到源之后，我们确认该事件确实写入了文件系统和 HDFS 接收器。

## 选择器复制和多路复用

源选择器有两种模式，复制(如我们在这里看到的)和多路复用。 **多路复用**源选择器将根据指定报头字段的值使用逻辑来确定事件应该发送到哪个通道。

## 处理接收器故障

由于接收器是个输出目的地，因此预计接收器可能会随着时间的推移出现故障或变得无响应。 与任何输入/输出设备一样，接收器可能已饱和、空间不足或脱机。

正如 Flume 将选择器与源关联以允许我们刚才看到的复制和多路复用行为一样，它还支持接收器处理器的概念。

定义了两个信宿处理器，即**故障转移**信宿处理器和**负载平衡**信宿处理器。

接收器处理器将接收器视为在一个组内，并根据它们的类型，在事件到达时做出不同的反应。 负载平衡接收器处理器使用循环或随机算法向接收器发送事件，每次发送一个事件，以选择下一步使用哪个接收器。 如果接收器出现故障，则会在另一个接收器上重试该事件，但出现故障的接收器仍保留在池中。

相反，故障转移接收器将接收器视为优先级列表，并且仅在其上面的接收器失败时才尝试较低优先级的接收器。 出现故障的接收器将从列表中删除，并且仅在随着后续故障而增加的冷静期后才会重试。

## 有一个围棋英雄-处理 Flume 故障

设置具有三个已配置 HDFS 接收器的 Flume 配置，每个接收器都写入 HDFS 上的不同位置。 使用负载平衡器接收器处理器确认事件已写入每个接收器，然后使用故障转移接收器处理器显示优先顺序。

您是否可以强制代理选择优先级最高的处理器以外的处理器？

## 接下来，世界

我们现在已经介绍了 Flume 的大部分关键功能。 正如前面提到的，Flume 是一个框架，应该仔细考虑这一点；Flume 的部署模型比我们看到的任何其他产品都灵活得多。

它通过相对较小的一组功能实现其灵活性；通过通道将源连接到汇点，以及允许多代理或多通道配置的多种变体。 这看起来可能不是很多，但请考虑一下，可以组合这些构造块来创建如下系统，其中多个 Web 服务器场将其日志馈送到中央 Hadoop 群集：

*   每个场中的每个节点都运行一个代理，依次拉取每个本地日志文件。
*   这些日志文件被发送到一个高度可用的聚合点，每个场中的一个聚合点还执行一些处理并向事件添加附加元数据，将事件分类为三种类型的记录。
*   然后，这些一级聚合器将事件发送到访问 Hadoop 集群的一系列代理之一。 聚合器提供多个接入点，事件类型 1 和 2 被发送到第一个接入点，事件类型 3 被发送到第二个接入点。
*   在最终聚合器中，它们将事件类型 1 和 2 写入 HDFS 上的不同位置，类型 2 也写入本地文件系统。 事件类型 3 直接写入 HBase。

如此简单的原语可以组合起来构建这样复杂的系统，这真是令人惊讶！

## 有一个围棋英雄-下一个，世界

作为一种思维实验，尝试完成前面的场景，并确定在流程中的每个步骤都需要什么样的 Flume 设置。

# 更大的图景

重要的是要认识到，从一个点到另一个点“简单地”获取数据很少是您考虑数据的范围。 最近，像**数据生命周期管理**这样的术语被广泛使用是有原因的。 让我们简要地看一下一些需要考虑的事情，最好是在数据泛滥整个系统之前。

## 数据生命周期

就数据生命周期而言，要问的主要问题是，您从数据存储中获得的价值将在多长时间内大于存储成本。 永久保存数据似乎很有吸引力，但随着时间的推移，保存越来越多数据的成本将会增加。 这些成本不仅仅是财务上的；许多系统的性能随着数据量的增加而下降。

这个问题不是--或者至少不应该是--由技术因素决定的。 相反，你需要企业的价值和成本成为驱动因素。 有时数据很快就变得一文不值，有时由于竞争或法律原因，企业无法将其删除。 确定位置并采取相应的行动。

当然，请记住，在保留或删除数据之间不是一个二元决策；您还可以跨存储层迁移数据，这些存储层的成本和性能会随着时间的推移而降低。

## 暂存数据

在这个过程的另一边，通常值得考虑如何将数据提供给处理平台(如 MapReduce)。 对于多个数据源，您通常最不希望的就是将所有数据都放在一个巨大的卷上。

正如我们在前面看到的，Flume 能够参数化它在 HDFS 上写入的位置，这是一个很好的工具来帮助解决这个问题。 但是，通常将此初始下载点视为在处理之前写入数据的临时中转区是很有用的。 在处理之后，可以将其移动到长期目录结构中。

## 排程

在流程中的许多点上，我们已经讨论过，有一种隐含的需求，即需要外部任务来做一些事情。 如前所述，我们希望 MapReduce 在 Flume 将文件写入 HDFS 后对其进行处理，但是该任务是如何调度的呢？ 或者，我们如何管理源主机上的后处理、旧数据的存档或删除，甚至是日志文件的删除？

其中一些任务(例如后者)可能由 Linux 上的**logrotate**等现有系统管理，但其他任务可能需要构建。 像**cron**这样显而易见的工具可能已经足够好了，但是随着系统复杂性的增加，您可能需要研究更复杂的调度系统。 在下一章中，我们将简要介绍这样一个与 Hadoop 紧密集成的系统。

# 摘要

本章讨论了如何跨网络检索数据并使其可在 Hadoop 中处理的问题。 正如我们所看到的，这实际上是一个更普遍的挑战，尽管我们可能会使用特定于 Hadoop 的工具，如 Flume，但这些原则并不是唯一的。 特别是，我们概述了我们可能想要写入 Hadoop 的数据类型，通常将其归类为网络数据或文件数据。 我们探索了使用现有命令行工具进行此类检索的一些方法。 虽然功能强大，但这些方法缺乏复杂性，不适合扩展到更复杂的场景中。

我们将 Flume 视为定义和管理数据(特别是来自日志文件)路由和交付的灵活框架，并了解了 Flume 体系结构，该体系结构可以看到数据到达源，通过通道进行处理，然后写入接收器。

然后，我们探索了 Flume 的许多功能，例如如何使用不同类型的源、汇和通道。 我们了解了如何将简单的构建块组合成非常复杂的系统，最后介绍了一些关于数据管理的更一般的想法。

这就是本书的主要内容。 在下一章中，我们将勾勒出一些可能令人感兴趣的其他项目，并重点介绍一些让社区参与和获得支持的方法。