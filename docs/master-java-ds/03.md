

# 三、探索性数据分析

在前一章中，我们讨论了数据处理，这是将数据转换成可用于分析的形式的重要步骤。在本章中，我们在清理和查看数据之后进行下一个逻辑步骤。这一步被称为**探索性数据分析** ( **EDA** )，它由汇总数据和创建可视化组成。

在本章中，我们将讨论以下主题:

*   使用 Apache Commons Math 和 Joinery 进行汇总统计
*   Java 和 JVM 中 EDA 的交互式 shells

在本章结束时，你将知道如何计算汇总统计数据和用 Joinery 创建简单的图表。



# Java 中的探索性数据分析

探索性数据分析是指获取数据集并从中提取最重要的信息，这样就有可能了解数据的样子。这包括两个主要部分:总结和可视化。

汇总步骤对于理解数据非常有帮助。对于数值变量，在这一步我们计算最重要的样本统计数据:

*   极值(最小值和最大值)
*   平均值或样本平均值
*   标准差，描述了数据的分布

我们通常会考虑其他统计数据，如中位数和四分位数(25%和 75%)。

正如我们在前一章已经看到的，Java 提供了一套很好的数据准备工具。相同的工具集可以用于 EDA，尤其是创建摘要。



# 搜索引擎数据集

在这一章中，我们将使用我们正在运行的例子——构建一个搜索引擎。在[第二章](a005c42b-d837-402c-9bba-971b440268b5.xhtml)、*数据处理工具箱*中，我们从搜索引擎返回的 HTML 页面中提取了一些数据。这个数据集包括一些数字特征，比如标题的长度和内容的长度。

为了存储这些功能，我们创建了以下类:

```java
public class RankedPage { 
    private String url; 
    private int position; 
    private int page; 
    private int titleLength; 
    private int bodyContentLength; 
    private boolean queryInTitle; 
    private int numberOfHeaders; 
    private int numberOfLinks; 
    // setters, getters are omitted 
}

```

看看这些信息对搜索引擎是否有用是很有趣的。例如，给定一个 URL，我们可能想知道它是否可能出现在引擎输出的第一页。通过 EDA 查看数据将有助于我们了解这是否可行。

此外，真实世界的数据很少是干净的。我们将使用 EDA 来尝试发现一些奇怪或有问题的观察结果。

让我们开始吧。我们将数据保存为 JSON 格式，现在我们可以使用 streams 和 Jackson 读回它:

```java
Path path = Paths.get("./data/ranked-pages.json"); 
try (Stream<String> lines = Files.lines(path)) { 
    return lines.map(line -> parseJson(line)).collect(Collectors.toList());
}

```

这是返回一系列`RankedPage`对象的函数体。我们从`ranked-page.json`文件中读取它们。然后我们使用`parseJson`函数将 JSON 转换成 Java 类:

```java
JSON.std.beanFrom(RankedPage.class, line);

```

看完数据，我们就可以分析了。通常，分析的第一步是查看汇总统计数据，我们可以使用 Apache Commons Math 来实现这一点。



# 阿帕奇公共数学

一旦我们读取了数据，我们就可以计算统计数据。正如我们前面提到的，我们通常对诸如最小值、最大值、平均值、标准偏差等汇总感兴趣。我们可以使用 Apache Commons 数学库。我们把它包含在`pom.xml`里吧:

```java
<dependency>
  <groupId>org.apache.commons</groupId> 
  <artifactId>commons-math3</artifactId> 
  <version>3.6.1</version> 
</dependency>

```

有一个用于计算汇总的`SummaryStatistics`类。让我们用它来计算一些关于我们抓取的页面的正文内容长度分布的统计数据:

```java
SummaryStatistics statistics = new SummaryStatistics(); data.stream().mapToDouble(RankedPage::getBodyContentLength)
    .forEach(statistics::addValue); 
System.out.println(statistics.getSummary());

```

这里，我们创建了`SummaryStatistics`对象，并添加了所有的正文内容长度。之后，我们可以调用一个`getSummary`方法来一次获得所有的汇总统计数据。这将打印以下内容:

```java
StatisticalSummaryValues: 
n: 4067 
min: 0.0 
max: 8675779.0 
mean: 14332.239242685007 
std dev: 144877.54551111493 
variance: 2.0989503193325176E10 
sum: 5.8289217E7

```

`DescriptiveStatistics`方法是这个库中另一个有用的类。它允许获得更多的值，如中位数和百分位数，以及百分位数；更好地展示数据的样子:

```java
double[] dataArray = data.stream()
        .mapToDouble(RankedPage::getBodyContentLength)
        .toArray();
DescriptiveStatistics desc = new DescriptiveStatistics(dataArray); 
System.out.printf("min: %9.1f%n", desc.getMin()); 
System.out.printf("p05: %9.1f%n", desc.getPercentile(5)); 
System.out.printf("p25: %9.1f%n", desc.getPercentile(25)); System.out.printf("p50: %9.1f%n", desc.getPercentile(50)); System.out.printf("p75: %9.1f%n", desc.getPercentile(75)); System.out.printf("p95: %9.1f%n", desc.getPercentile(95)); System.out.printf("max: %9.1f%n", desc.getMax());

```

这将产生以下输出:

```java
min: 0.0 
p05: 527.6 
p25: 3381.0 
p50: 6612.0 
p75: 11996.0 
p95: 31668.4 
max: 8675779.0

```

从输出中，我们可以注意到最小长度为零，这很奇怪；最有可能的是，存在数据处理问题。此外，最大值非常高，这表明存在异常值。稍后，从我们的分析中排除这些值是有意义的。

内容长度为零的页面可能是爬行错误。我们来看看这几页的比例:

```java
double proportion = data.stream()
    .mapToInt(p -> p.getBodyContentLength() == 0 ? 1 : 0)
    .average().getAsDouble(); 
System.*out*.printf("proportion of zero content length: %.5f%n", proportion);

```

我们看到没有多少页面是零长度的，所以删除它们是相当安全的。

稍后，在下一章中，我们将尝试预测一个 URL 是否来自第一个搜索页面结果。如果一些特征对于每个页面具有不同的值，那么机器学习模型将能够看到这种差异，并将其用于更准确的预测。让我们看看不同页面的内容长度值是否相似。

为此，我们可以按页面对 URL 进行分组，并计算平均内容长度。正如我们已经知道的，Java 流可以用来做到这一点:

```java
Map<Integer, List<RankedPage>> byPage = data.stream() 
    .filter(p -> p.getBodyContentLength() != 0)  
    .collect(Collectors.groupingBy(RankedPage::getPage));

```

请注意，我们为空白页面添加了一个过滤器，因此它们不会出现在组中。现在，我们可以使用组来计算平均值:

```java
List<DescriptiveStatistics> stats = byPage.entrySet().stream()   
    .sorted(Map.Entry.comparingByKey()) 
    .map(e -> calculate(e.getValue(), RankedPage::getBodyContentLength)) 
    .collect(Collectors.toList());

```

这里，`calculate`是一个函数，它接受一个集合，计算每个元素上提供的函数(在本例中使用`getBodyContentLength`，并从中创建一个`DescriptiveStatistics`对象:

```java
private static DescriptiveStatistics calculate(List<RankedPage> data, 
            ToDoubleFunction<RankedPage> getter) {
    double[] dataArray = data.stream().mapToDouble(getter).toArray(); 
    return new DescriptiveStatistics(dataArray); 
}

```

现在，在列表中，您将拥有每个组的描述性统计数据(在本例中为页面)。然后，我们可以用任何我们想要的方式展示它们。考虑下面的例子:

```java
Map<String, Function<DescriptiveStatistics, Double>> functions = new LinkedHashMap<>();
functions.put("min", d -> d.getMin()); 
functions.put("p05", d -> d.getPercentile(5)); 
functions.put("p25", d -> d.getPercentile(25)); 
functions.put("p50", d -> d.getPercentile(50)); 
functions.put("p75", d -> d.getPercentile(75)); 
functions.put("p95", d -> d.getPercentile(95)); 
functions.put("max", d -> d.getMax()); 
System.out.print("page"); 

for (Integer page : byPage.keySet()) {
    System.out.printf("%9d ", page); 
}
System.out.println();

for (Entry<String, Function<DescriptiveStatistics, Double>> pair : functions.entrySet()) { 
    System.out.print(pair.getKey()); 
    Function<DescriptiveStatistics, Double> function = pair.getValue();    
    System.out.print(" "); 
    for (DescriptiveStatistics ds : stats) {
        System.out.printf("%9.1f ", function.apply(ds)); 
    }
    System.out.println(); 
}

```

这会产生以下输出:

```java
page 0 1 2 
min 5.0 1.0 5.0 
p05 1046.8 900.6 713.8 
p25 3706.0 3556.0 3363.0 
p50 7457.0 6882.0 6383.0 
p75 13117.0 12067.0 11309.8 
p95 42420.6 30557.2 27397.0 
max 390583.0 8675779.0 1998233.0

```

输出表明，内容长度的分布在来自搜索引擎结果的不同页面的 URL 之间是不同的。因此，在预测给定 URL 的搜索页码时，这可能很有用。



# 细木工制品

您可能会注意到，我们刚刚编写的代码非常冗长。当然，可以把它放在一个 helper 函数中，在需要时调用它，但是还有另一种更简洁的方法来计算这些统计数据——使用 joinery 及其数据框架。

在 Joinery 中，`DataFrame`对象有一个名为`describe`的方法，它创建一个包含汇总统计信息的新数据框架:

```java
List<RankedPage> pages = Data.readRankedPages(); 
DataFrame<Object> df = BeanToJoinery.convert(pages, RankedPage.class); 
df = df.retain("bodyContentLength", "titleLength", "numberOfHeaders"); DataFrame<Object> describe = df.describe(); 
System.out.println(describe.toString());

```

在前面的代码中，`Data.readRankedPages`是一个 helper 方法，它读取 JSON 数据并将其转换为一组 Java 对象，`BeanToJoinery.convert`是一个 helper 类，它将一组 Java 对象转换为一个`DataFrame`。

然后，我们只保留三列，删除其他所有内容。以下是输出:

```java
body   contentLength   numberOfHeaders  titleLength
count  4067.00000000   4067.00000000    4067.00000000
mean   14332.23924269  25.25325793      46.17334645
std    144877.5455111  32.13788062      27.72939822
var    20989503193.32  1032.84337047    768.91952552
max    8675779.000000  742.00000000     584.00000000
min    0.00000000      0.00000000       0.00000000

```

我们还可以查看不同组的平均值，例如，不同页面的平均值。为此，我们可以使用`groupBy`方法:

```java
DataFrame<Object> meanPerPage = df.groupBy("page").mean()
    .drop("position") 
    .sortBy("page")
    .transpose(); 
System.out.println(meanPerPage);

```

除了在`groupBy`之后应用 mean 之外，我们还删除了一个列位置，因为我们已经知道位置对于不同页面是不同的。此外，我们在最后应用转置操作；这是一个技巧，当有许多列时，使输出适合一个屏幕。这会产生以下输出:

```java
page 0 1 2 
bodyContentLength 12577 18703 11286 
numberOfHeaders 30 23 21 
numberOfLinks 276 219 202 
queryInTitle 0 0 0 
titleLength 46 46 45

```

我们可以看到，一些变量的平均值差异很大。对于其他变量，如`queryInTitle`，似乎没有任何区别。但是，请记住这是一个布尔变量，因此平均值介于 0 和 1 之间。出于某种原因，Joinery 决定不在这里显示小数部分。

现在，我们知道如何在 Java 中计算一些简单的汇总统计数据，但是要做到这一点，我们首先需要编写一些代码，编译它，然后运行它。这不是最方便的过程，有更好的交互方式，即避免编译并立即得到结果。接下来，我们将看到如何在 Java 中实现它。



# Java 中的交互式探索性数据分析

Java 是一种静态类型的编程语言，用 Java 编写的代码需要编译。虽然 Java 适合开发复杂的数据科学应用程序，但它使得交互式地探索数据变得更加困难；每次，我们都需要重新编译源代码，重新运行分析脚本来查看结果。这意味着，如果我们需要读取一些数据，我们将不得不一遍又一遍地这样做。如果数据集很大，程序需要更多的时间来启动。

因此很难与数据交互，这使得在 Java 中进行 EDA 比在其他语言中更困难。特别是**读取-评估-打印循环** ( **REPL** )这个交互 shell，对于做 EDA 来说是相当重要的一个特性。

不幸的是，Java 8 没有 REPL，但是有几个替代方案:

*   其他交互式 JVM 语言，如 JavaScript、Groovy 或 Scala
*   带有 jshell 的 Java 9
*   完全不同的平台，如 Python 或 R

在这一章中，我们将着眼于前两个选项——JVM 语言和 Java 9 的 REPL。



# JVM 语言

你大概知道，Java 平台不仅是 Java 编程语言，而且 **Java 虚拟机** (JVM)可以运行其他 JVM 语言的代码。有很多运行在 JVM 上的语言都有 REPL，比如 JavaScript、Scala、Groovy、JRuby 和 Jython。还有很多。所有这些语言都可以访问任何用 Java 编写的代码，而且它们有交互式控制台。

例如，Groovy 与 Java 非常相似，在 Java 8 之前，几乎所有用 Java 编写的代码都可以在 Groovy 中运行。但是，对于 Java 8 来说，情况就不再是这样了。Groovy 不支持 lambda 表达式和函数接口的新 Java 语法，所以我们不能在那里运行本书的大部分代码。

Scala 是另一种流行的函数式 JVM 语言，但是它的语法与 Java 非常不同。对于数据处理来说，它是一种非常强大和富有表现力的语言，它有一个漂亮的交互式外壳，并且有许多用于进行数据分析和数据科学的库。

此外，有几个 JavaScript 实现可用于 JVM。其中一个是 Nashorn，它自带 Java 8 开箱即用；没有必要将它作为一个独立的依赖项包含进来。Joinery 还带有一个内置的交互式控制台，它利用了 JavaScript，在本章的后面，我们将看到如何使用它。虽然所有这些语言都很好，但它们超出了本书的范围。你可以从这些书中了解更多:

*   *动作麻利*，*迪克·科尼格，曼宁*
*   *Scala 数据分析食谱，* *阿伦·马尼瓦南，帕克特出版社*



# 交互式 Java

说 Java 是一种 100%非交互式语言是不公平的；有一些扩展直接为 Java 提供了 REPL 环境。

一个这样的环境是看起来完全像 Java 的脚本语言(BeanShell)。但是，它太旧了，并且不支持新的 Java 8 语法，所以对于进行交互式数据分析来说，它不是很有用。

更有趣的是 Java 9，它附带了一个名为 JShell 的集成 REPL，支持 tab 上的自动补全、Java 流和 lambda 表达式的 Java 8 语法。在撰写本文时，Java 9 只作为早期访问版本提供。你可以从 https://jdk9.java.net/download/.下载

启动 shell 很容易:

```java
$ jshell

```

但是通常你想访问一些库，因此它们需要在类路径中。通常，我们使用 Maven 来管理依赖项，所以我们可以运行下面的代码将在`pom`文件中指定的所有`jar`库复制到我们选择的目录中:

```java
mvn dependency:copy-dependencies -DoutputDirectory=lib 
mvn compile

```

完成后，我们可以像这样运行 shell:

```java
jshell -cp lib/*:target/classes

```

如果您在 Windows 上，请用分号替换冒号:

```java
jshell -cp lib/*;target/classes

```

然而，我们的实验表明，不幸的是，JShell 还很原始，有时会崩溃。在撰写本文时，计划在 2017 年 3 月底发布。现在，我们不会更详细地讨论 JShell，但是本章前半部分的所有代码都应该可以在这个控制台上运行，不需要额外的配置。此外，我们应该能够立即看到输出。

到目前为止，我们已经使用 Joinery 几次了，它也支持执行简单的 EDA。接下来，我们将看看如何用细木工板壳进行分析。



# 细木工外壳

细木工已经多次被证明对数据处理和简单的 EDA 很有用。它有一个交互式的外壳，你可以立即得到答案。

如果数据已经是 CSV 格式，那么可以从系统控制台调用 Joinery shell:

```java
$ java joinery.DataFrame shell

```

你可以在[https://github.com/cardillo/joinery.](https://github.com/cardillo/joinery)看到例子，所以如果你的数据已经在 CSV 中，你就可以开始了，只需按照那里的指示。

在本书中，当数据帧不是 CSV 格式时，我们将看一个更复杂的例子。在我们的例子中，数据是 JSON 格式的，而 Joinery shell 不支持这种格式，所以我们需要先做一些预处理。

我们能做的是在 Java 代码中创建一个 DataFrame 对象，然后创建交互式 shell 并将 DataFrame 传递到那里。让我们看看如何能做到这一点。

但是在我们这样做之前，我们需要添加一些依赖项来使之成为可能。第一，Joinery shell 使用 JavaScript，但不使用 JVM 附带的 Nashorn engin 相反，它使用的是 Mozilla 的名为 **Rhino** 的引擎。因此，我们需要将它包含到我们的`pom`:

```java
<dependency> 
  <groupId>rhino</groupId> 
  <artifactId>js</artifactId>  
  <version>1.7R2</version> 
</dependency>

```

第二，它依赖于一个特殊的自动补全库`jline`。我们也来补充一下:

```java
<dependency> 
  <groupId>jline</groupId> 
  <artifactId>jline</artifactId> 
  <version>2.14.2</version> 
</dependency>

```

使用 Maven 给了你很大的灵活性；它更简单，不需要您手动下载所有的库，并从源代码构建 Joinery 来执行 shell。所以，我们让 Maven 来处理。

现在我们可以使用它了:

```java
List<RankedPage> pages = Data.readRankedPages(); 
DataFrame<Object> dataFrame = BeanToJoinery.convert(pages, 'RankedPage.class); 
Shell.repl(Arrays.asList(dataFrame));

```

让我们将这段代码保存到一个`chapter03.JoineryShell`类中。之后，我们可以用下面的 Maven 命令运行它:

```java
mvn exec:java -Dexec.mainClass="chapter03.JoineryShell"

```

这将把我们带到细木工外壳:

```java
# DataFrames for Java -- null, 1.7-8e3c8cf
# Java HotSpot(TM) 64-Bit Server VM, Oracle Corporation, 1.8.0_91
# Rhino 1.7 release 2 2009 03 22
>

```

我们在 Java 中传递给 shell 对象的所有数据帧都可以在 Shell 中的 Frames 变量中找到。所以，为了得到`DataFrame`，我们可以这样做:

```java
> var df = frames[0]

```

要查看`DataFrame`的内容，只需写下它的名字:

```java
> df

```

你会看到前几排`DataFrame`。请注意，自动完成功能按预期工作:

```java
> df.<tab>

```

您将看到选项列表。

我们可以使用这个 shell 调用数据帧上的相同方法，就像我们在普通的 Java 应用程序中使用的方法一样。例如，您可以按如下方式计算平均值:

```java
> df.mean().transpose()

```

我们将看到以下输出:

```java
bodyContentLength 14332.23924269
numberOfHeaders 25.25325793
numberOfLinks 231.16867470
page 1.03221047
position 18.76518318
queryInTitle 0.59822965
titleLength 46.17334645

```

或者。我们可以执行相同的`groupBy`示例:

```java
> df.drop('position').groupBy('page').mean().sortBy('page').transpose()

```

这将产生以下输出:

```java
page 0 1 2
bodyContentLength 12577 18703 11286
numberOfHeaders 30 23 21
numberOfLinks 276 219 202
queryInTitle 0 0 0
titleLength 46 46 45

```

最后，用细木工也可以创造一些简单的情节。为此，我们需要使用一个额外的库。对于绘图，细木工使用`xchart`。让我们把它包括进来:

```java
<dependency> 
  <groupId>com.xeiam.xchart</groupId> 
  <artifactId>xchart</artifactId> 
  <version>2.5.1</version> 
</dependency>

```

并再次运行控制台。现在我们可以使用`plot`函数:

```java
> df.retain('titleLength').plot(PlotType.SCATTER)

```

我们会看到这个:

![](img/image_03_001.png)

这里，我们看到一个标题长度超过 550 个字符的异常值。让我们把 200 以上的都去掉，再看一下图片。另外，记住有一些零长度的内容页面，所以我们也可以把它们过滤掉。

为了只保留那些满足某些条件的行，我们使用了`select`方法。它采用一个函数，应用于每一行；如果函数返回`true`，则保留该行。

我们可以这样使用它:

```java
> df.retain('titleLength') 
 .select(function(list) { return list.get(0) <= 200; }) 
 .select(function(list) { return list.get(0) > 0;}) 
 .plot(PlotType.SCATTER)

```

前面代码中的换行符是为了提高可读性而添加的，但是它们在控制台中不起作用，所以不要使用它们。

现在，我们有了一个更清晰的画面:

![](img/image_03_002.png)

遗憾的是，joinery 的绘图能力相当有限，使用`xchart`制作图形需要很大的努力。

正如我们已经知道的，在细木工中，很容易计算不同组之间的统计数据；我们只需要使用`groupBy`方法。然而，不可能容易地使用这种方法来绘制数据，以便容易地比较每组的分布。

还有其他工具也可以用于 EDA:

*   用 Java 编写的 Weka 是一个用于执行数据挖掘的库。它有一个用于执行 EDA 的 GUI 界面。
*   另一个 Java 库 Smile 有一个 Scala shell 和一个 smile-plot 库，用于创建可视化效果。

不幸的是，Java 通常不是执行 EDA 的理想选择，有其他更适合的动态语言。例如，R 和 Python 对于这个任务来说是理想的，但是介绍它们超出了本书的范围。你可以从以下书籍中了解更多信息:

*   通过 R，Gergely Daroczi 掌握数据分析
*   Python 机器学习，塞巴斯蒂安·拉什卡



# 摘要

在这一章中，我们讨论了探索性数据分析，简称 EDA。我们讨论了如何用 Java 进行 EDA，包括创建摘要和简单的可视化。

在本章中，我们使用了我们的搜索引擎示例，并分析了我们之前收集的数据。我们的分析表明，对于来自搜索引擎结果的不同页面的 URL，一些变量的分布看起来是不同的。这表明，可以利用这些差异来建立一个模型，预测 URL 是否来自第一页。

在下一章，我们将看看如何做到这一点，并讨论监督机器学习算法，如分类和回归。