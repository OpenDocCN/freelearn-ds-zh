# 九、设计数据可视化解决方案

一旦我们拥有了 Hadoop 生态系统中的数据并对其进行了处理，下一个合乎逻辑的步骤就是构建驱动业务决策的分析。

在本章中，我们将学习以下主题:

*   数据可视化
*   Apache 德鲁伊
*   Apache superset(Apache super set)

# 数据可视化

数据可视化是通过图形方式理解原始数据中各种实体之间关系的过程。这是一项非常强大的技术，因为它使最终用户能够以非常简单的形式获得消息，而无需了解底层数据。

数据可视化在大数据洞察的可视化交流中发挥着非常重要的作用。这既是一门艺术，也是一门科学，在理解数据方面需要一些努力；同时，我们也需要对目标受众有所了解。

到目前为止，我们已经看到任何类型的数据都可以存储在 **Hadoop 文件系统** ( **HDFS** )中。为了将复杂的数据结构转换成可视化的形式，我们需要了解用于表示数据的标准技术。

在数据可视化中，信息以图形的形式传达给最终用户，这些图形可以是 1D、2D、三维甚至更高维度。这完全取决于我们试图传达的意思。

让我们看一下用于向用户传达视觉信息的标准图形:

*   条形图/柱形图
*   折线图/面积图
*   圆形分格统计图表
*   雷达图
*   散点图/气泡图
*   标签云
*   泡泡图

# 条形图/柱形图

这是数据的 2D 图形表示，其中数据点显示为垂直/水平条。每个条代表一个数据点。当数据点不涉及时间维度时，这些点的显示顺序可能不会有任何不同。当我们处理用于表示条形图的时间序列数据时，我们通常遵循沿 X(水平)轴显示的时间顺序。

让我们看一下由四个数据点生成的示例图表。数据代表每个用户拥有的数量:

![](img/9a5e0f63-0700-430c-873e-dbeeb2c09d0b.png)

**解读**:图中既有行和列的文本数据，也有视觉效果。如果仔细观察，文本数据的大小更小，只有四条记录。但是视觉图形在不了解任何数据的情况下直接传达了信息。

图表传达的信息是:

*   西塔比所有人都有钱
*   吉塔的钱最少

其他解释也是可能的。它们留给读者。

# 折线图/面积图

这也是典型的 2D 图表，其中每个数据点都表示为画布上的一个点，并且属于同一数据集的所有这些点都用一条线连接起来。当从水平/垂直轴到直线的区域被完全覆盖时，该图表成为面积图。

同一图形中可以有多条线，表示同一实体的多个数据系列。

让我们根据与之前相同的数据来看一下这个面积图的示例:

![](img/b9849529-fcc2-488a-8c2d-02fadaba8876.png)

这些是图表的属性:

*   *x* 轴有所有人的列表
*   *y* 轴表示从 **0** 到 **100** 的量
*   图表上四个地方画有点，对应于表格形式的值
*   点是用直线连接的
*   该区域填充在线下方，使其成为面积图

# 圆形分格统计图表

这也是一个 2D 图表，绘制成一个圆圈中的多个扇区。当我们想要突出所有数据点的相对重要性时，此图表非常有用。

让我们看一下使用与之前相同的数据集绘制的示例图表，以便更好地理解它:

![](img/ffa15300-a402-41d1-9baf-d4caae0188a9.png)

如你所见，很容易理解使用这张图表的每个人所拥有的金额的相对重要性。

得出的结论与前面的图表相似。但是图是一个简单的圆，这里没有多个维度给用户增加负担。

# 雷达图

这也是一个 2D 图形，其中数据轴是等距扇区的边缘(像饼图的边缘)。当我们想要了解每个数据点的相对重要性的多维度时，此图非常有用。

为了更好地理解这个图表，让我们看一下这个样本数据和图表:

![](img/436900d2-ca59-4ff0-a579-c8a69ab9b5a6.png)

数据由八列组成:

*   **第一列**:所有用户列表
*   **第二栏至第八栏**:一周中的天数以及当天每人拥有的美元

我们想画一张图表，向我们展示以下内容:

*   每天总美元数
*   每个人每天拥有的美元

我们已经在雷达图中绘制了所有这些信息，其中轴是扇区(天)，上限为 **400** 的最大值。每个用户的价值被一个接一个地画出来，这样我们就知道总价值，而不是相对价值(这类似于面积叠加)。

# 散点图/气泡图

散点图可以是多维图形。当我们沿着轴渲染画布上对应于数值的每个数据点时，这是一个更容易理解的图形。该图有助于理解轴上每个点的相对重要性。

气泡图是散点图的变体，画布上的点将数值显示为大气泡(表示其重要性)。

让我们用这个例子来看看这两个图形:

![](img/15f06c51-ebce-4863-935b-8485c89f33e4.png)

左侧的图形是气泡图，右侧的图形是散点图。

让我们看看生成的数据和图表。

输入数据:

*   由五行组成，而我们在列中有**销售**和**产品数量**

用气泡图:

*   *y* 轴显示产品数量
*   *x* 轴只是位置轴，并不反映输入数据的值
*   画布上的每个点都显示了与产品数量相对应的销售额

使用散点图:

*   *y* 轴显示完成的销售额
*   *x* 轴显示销售的产品
*   画布上的每个点显示输入中的每一行

# 其他图表

还有许多其他类型的图形可能在本节中没有涉及，但值得在[https://d3js.org](https://d3js.org)网站上探索。这将使您了解如何表示数据，以便向用户传达非常好的信息。

# Hadoop 中的实用数据可视化

Hadoop 拥有丰富的数据源和应用生态系统，可以帮助我们构建丰富的可视化。在接下来的章节中，我们将了解两个这样的应用:

*   Apache 德鲁伊
*   Apache superset(Apache super set)

我们还将学习如何使用 Apache 超集处理关系数据库(如 MySQL)中的数据。

# Apache 德鲁伊

Apache Druid 是一个分布式、高性能的柱状商店。其官网为 [https://druid.io](https://druid.io) 。

Druid 允许我们存储实时和历史数据，本质上是时间序列。它还提供了快速的数据聚合和灵活的数据探索。该架构支持以千兆字节大小存储数万亿个数据点。

为了更好的了解德鲁伊建筑，请参考[http://static.druid.io/docs/druid.pdf](http://static.druid.io/docs/druid.pdf)的这份白皮书。

# 德鲁伊组件

让我们快速了解一下德鲁伊集群的不同组件:

| **组件** | **描述** |
| 德鲁伊经纪人 | 这些节点知道数据在集群中的位置。这些节点由应用/客户端联系，以获取德鲁伊内部的数据。 |
| 德鲁伊协调员 | 这些节点管理历史节点上的数据(加载、删除和负载平衡数据)。 |
| 德鲁伊霸主 | 该组件负责接受任务并返回任务的状态。 |
| 德鲁伊路由器 | 当数据量在万亿字节或更高范围时，需要这些节点。这些节点将请求路由到代理。 |
| 德鲁伊历史 | 这些节点存储不可变的段，是德鲁伊集群的骨干。它们服务于加载段、删除段，并服务于对段请求的查询。 |

# 其他所需组件

下表列出了其他几个必需的组件:

| **组件** | **描述** |
| 动物园管理员 | Apache Zookeeper 是一个高度可靠的分布式协调服务 |
| 元数据存储 | MySQL 和 PostgreSQL 是流行的关系数据库系统，用于跟踪所有部门、主管、任务和配置 |

# Apache Druid 安装

Apache Druid 可以独立安装，也可以作为 Hadoop 集群的一部分安装。在本节中，我们将看到如何通过 Apache Ambari 安装 Druid。

# adservice

首先，我们调用 Hadoop 集群中服务列表下方的操作下拉列表。

屏幕如下所示:

![](img/56f10c5f-97bf-40c1-b098-83a1f671e5ee.png)

# 选择德鲁伊和超集

在这个设置中，我们将同时安装德鲁伊和超集。超集是可视化应用，我们将在下一步中了解它。

选择屏幕如下所示:

![](img/f0086078-bf64-45a7-af90-00f00093b19f.png)

选择两种服务后，单击下一步。

# 服务器上的服务放置

在这一步中，我们将可以选择安装应用的服务器。为此，我选择了节点 3。您可以选择任何想要的节点。

屏幕看起来像这样:

![](img/e573a6f3-105c-45a7-a56b-b903cd89c4a9.png)

更改完成后，单击下一步。

# 选择奴隶和客户

在这里，我们可以选择为安装的组件选择需要从属节点和客户端的节点。我留下了已经为我选择的选项:

![](img/23b2b7f5-6540-421e-8a95-2a3a0a3dcf83.png)

# 服务配置

在这一步中，我们需要为德鲁伊和超集应用使用的元数据存储选择数据库、用户名和密码。随意选择默认的。我给了 MySQL 作为他们两个的后端存储。

屏幕如下所示:

![](img/72b59851-5564-4c73-8143-b34a002282ee.png)

一旦更改看起来不错，点击屏幕底部的“下一步”按钮。

# 服务安装

在此步骤中，应用将自动安装，状态将在计划结束时显示。

安装完成后，单击下一步。对当前屏幕的更改如下所示:

![](img/1d03751d-b4f0-4879-bbc8-ef7ba50c9a27.png)

# 安装摘要

一旦一切顺利完成，我们会看到一个已经完成的总结。完成后，单击完成:

![](img/02790068-b2da-4efd-a4b5-2afc1dfdb055.png)

# 将样本数据摄入 Druid

一旦我们的 Hadoop 集群中运行了所有与 Druid 相关的应用，我们就需要一个样本数据集，为了运行一些分析任务，我们必须加载该数据集。

让我们看看如何加载样本数据。从网上下载德鲁伊档案:

```sh
[druid@node-3 ~$ curl -O http://static.druid.io/artifacts/releases/druid-0.12.0-bin.tar.gz
% Total % Received % Xferd Average Speed Time Time Time Current
                               Dload Upload Total Spent Left Speed
100 222M 100 222M 0 0 1500k 0 0:02:32 0:02:32 --:--:-- 594k
```

提取档案:

```sh
[druid@node-3 ~$ tar -xzf druid-0.12.0-bin.tar.gz
```

将示例维基百科数据复制到 Hadoop:

```sh
[druid@node-3 ~]$ cd druid-0.12.0
[druid@node-3 ~/druid-0.12.0]$ hadoop fs -mkdir /user/druid/quickstart
[druid@node-3 ~/druid-0.12.0]$ hadoop fs -put quickstart/wikiticker-2015-09-12-sampled.json.gz /user/druid/quickstart/
```

提交导入请求:

```sh
[druid@node-3 druid-0.12.0]$ curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/wikiticker-index.json localhost:8090/druid/indexer/v1/task;echo
{"task":"index_hadoop_wikiticker_2018-03-16T04:54:38.979Z"}
```

这一步之后，德鲁伊会自动将数据导入到德鲁伊集群中，进度可以在霸王控制台看到。

该界面可通过`http://<overlord-ip>:8090/console.html`访问。屏幕如下所示:

![](img/8df1b469-ba70-4646-a81f-bfe2cce4267c.png)

一旦摄取完成，我们将看到作业状态为成功。

In case of `FAILED` imports, please make sure that the backend that is configured to store the Metadata for the Druid cluster is up and running. Even though Druid works well with the OpenJDK installation, I have faced a problem with a few classes not being available at runtime. In order to overcome this, I have had to use Oracle Java version 1.8 to run all Druid applications.

现在我们准备开始使用德鲁伊来完成可视化任务。

# MySQL 数据库

Apache 超集还允许我们读取关系数据库管理系统(如 MySQL)中的数据。我们还将在本节中创建一个示例数据库，稍后我们可以将它与超集一起使用来创建可视化。

# 样本数据库

雇员数据库是一个标准数据集，其中包含示例组织及其雇员、工资和部门数据。我们将看看如何为我们的任务设置它。

本节假设 MySQL 数据库已经配置并运行。

# 下载样本数据集

在任何可以访问 MySQL 数据库的服务器上，使用以下命令从 GitHub 下载示例数据集:

```sh
[user@master ~]$ sudo yum install git -y
```

```sh
[user@master ~]$ git clone https://github.com/datacharmer/test_db
Cloning into 'test_db'...
remote: Counting objects: 98, done.
remote: Total 98 (delta 0), reused 0 (delta 0), pack-reused 98
Unpacking objects: 100% (98/98), done.
```

# 将数据复制到 MySQL

在这一步中，我们将把文件中的数据内容导入到 MySQL 数据库中:

```sh
[user@master test_db]$ mysql -u root < employees.sql
INFO
CREATING DATABASE STRUCTURE
INFO
storage engine: InnoDB
INFO
LOADING departments
INFO
LOADING employees
INFO
LOADING dept_emp
INFO
LOADING dept_manager
INFO
LOADING titles
INFO
LOADING salaries
data_load_time_diff
NULL

```

# 验证表的完整性

这是一个重要的步骤，只是为了确保我们导入的所有数据都正确地存储在数据库中。验证发生时会显示完整性检查的摘要:

```sh
[user@master test_db]$ mysql -u root -t < test_employees_sha.sql
+----------------------+
| INFO                 |
+----------------------+
| TESTING INSTALLATION |
+----------------------+
+--------------+------------------+------------------------------------------+
| table_name   | expected_records | expected_crc                             |
+--------------+------------------+------------------------------------------+
| employees    | 300024 | 4d4aa689914d8fd41db7e45c2168e7dcb9697359 |
| departments  |  9 | 4b315afa0e35ca6649df897b958345bcb3d2b764 |
| dept_manager |               24 | 9687a7d6f93ca8847388a42a6d8d93982a841c6c |
| dept_emp     | 331603 | d95ab9fe07df0865f592574b3b33b9c741d9fd1b |
| titles       | 443308 | d12d5f746b88f07e69b9e36675b6067abb01b60e |
| salaries     | 2844047 | b5a1785c27d75e33a4173aaa22ccf41ebd7d4a9f |
+--------------+------------------+------------------------------------------+
+--------------+------------------+------------------------------------------+
| table_name   | found_records    | found_crc                        |
+--------------+------------------+------------------------------------------+
| employees    | 300024 | 4d4aa689914d8fd41db7e45c2168e7dcb9697359 |
| departments  |  9 | 4b315afa0e35ca6649df897b958345bcb3d2b764 |
| dept_manager |               24 | 9687a7d6f93ca8847388a42a6d8d93982a841c6c |
| dept_emp     | 331603 | d95ab9fe07df0865f592574b3b33b9c741d9fd1b |
| titles       | 443308 | d12d5f746b88f07e69b9e36675b6067abb01b60e |
| salaries     | 2844047 | b5a1785c27d75e33a4173aaa22ccf41ebd7d4a9f |
+--------------+------------------+------------------------------------------+
```

```sh
+--------------+---------------+-----------+
| table_name   | records_match | crc_match |
+--------------+---------------+-----------+
| employees    | OK | ok        |
| departments  | OK | ok        |
| dept_manager | OK            | ok |
| dept_emp     | OK | ok        |
| titles       | OK | ok        |
| salaries     | OK | ok        |
+--------------+---------------+-----------+
+------------------+
| computation_time |
+------------------+
| 00:00:11         |
+------------------+
+---------+--------+
| summary | result |
+---------+--------+
| CRC     | OK |
| count   | OK |
+---------+--------+
```

现在数据被正确加载到名为**员工**的 MySQL 数据库中。

# 单一标准化表格

在数据仓库中，与许多小型相关表相比，标准化表是一种标准做法。让我们创建一个包含员工、工资、部门

```sh
MariaDB [employees]> create table employee_norm as select e.emp_no, e.birth_date, CONCAT_WS(' ', e.first_name, e.last_name) full_name , e.gender, e.hire_date, s.salary, s.from_date, s.to_date, d.dept_name, t.title from employees e, salaries s, departments d, dept_emp de, titles t where e.emp_no = t.emp_no and e.emp_no = s.emp_no and d.dept_no = de.dept_no and e.emp_no = de.emp_no and s.to_date < de.to_date and s.to_date < t.to_date order by emp_no, s.from_date;
Query OK, 3721923 rows affected (1 min 7.14 sec)
Records: 3721923  Duplicates: 0  Warnings: 0

MariaDB [employees]> select * from employee_norm limit 1\G
*************************** 1\. row ***************************
    emp_no: 10001
birth_date: 1953-09-02
 full_name: Georgi Facello
    gender: M
 hire_date: 1986-06-26
    salary: 60117
 from_date: 1986-06-26
   to_date: 1987-06-26
 dept_name: Development
     title: Senior Engineer
1 row in set (0.00 sec)

MariaDB [employees]> 
```

一旦我们有了规范化的数据，我们将看到如何使用这个表中的数据来生成丰富的可视化。

# Apache superset(Apache super set)

超集是一个现代化的企业级商业智能应用。这个应用的重要特性是，我们可以直接从浏览器运行所有分析。没有必要为此安装任何特殊软件。

如果你还记得，我们已经在前面的章节中安装了超集和德鲁伊。现在我们需要学习如何使用超集来构建丰富的可视化。

# 访问超集应用

在网络浏览器中打开`http://<SERVER-IP>:9088/`。如果一切运行正常，我们将看到如下登录屏幕:

![](img/cb4e68e7-e99a-4f3d-956f-ac74a506fe67.png)

输入`admin`作为安装过程中选择的用户名和密码。

# 超集仪表板

仪表板是超集应用的重要部分。他们让我们以图形形式展示分析计算的结果。仪表板是从切片创建的，而切片又是从超集应用中配置的各种数据源构建的。

成功登录后，不会有任何自动为我们创建的仪表盘。我们将看到一个空白的仪表板列表，如下所示:

![](img/1cfcf69f-da70-4dc1-a4ff-b74154a0cfdc.png)

为了构建仪表板，我们首先需要配置数据源。因此，让我们点击顶部导航中的源菜单，然后点击刷新德鲁伊元数据:

![](img/2ca5bc92-fdeb-48ec-90cb-1e1b41350655.png)

完成这一步后，我们将进入数据源页面，一个新的数据源将自动出现在这里。还记得我们之前把这个数据集上传到德鲁伊吗？

![](img/186e1e1f-b1d8-4348-9b89-d65bc8e27621.png)

现在我们可以点击数据源名称(绿色)，这将把我们带到数据源探索页面:

![](img/721f4f3e-09d5-4153-aa95-0e04aec73401.png)

正如我们所看到的，这个页面被分成多个部分。

*   **左侧界面**:
    *   **数据源和图表类型**:在这个栏目中，我们可以选择我们需要使用的数据源，也可以选择右边我们想要看到的图形类型。
    *   **时间**:这是我们可以将数据源的数据限制在给定时间范围内的栏目。初学者往往会对该列出错，因为他们在右侧看不到任何数据。因此，选择一个开始时间值(为了获得更好的结果，建议选择 100 年前这样的相对值)。
    *   **分组依据**:该列用于根据输入数据的维度对数据进行分组。
    *   **其他选项**:分组依据下面还有其他选项，我们将在接下来的步骤中探讨。
*   **右侧界面**:
    *   这个用户界面包含我们在左侧选择的选项的结果。

# 理解维基百科编辑数据

在我们开始构建可视化之前。让我们更仔细地看看我们摄入到 Druid 中的数据，以及我们可以从这些数据中呈现什么类型的图形:

| **公制/尺寸** | **数据类型** | **描述** |
| `delta` | `LONG` | 以数字形式表示的变化 |
| `deleted` | `LONG` | 从文章中删除了数字形式的数据 |
| `added` | `LONG` | 添加数据，以数字形式测量 |
| `isMinor` | `STRING` | 布尔值，指示这是否是次要编辑 |
| `page` | `STRING` | 维基百科中发生变化的页面 |
| `isRobot` | `STRING` | 变化是由机器人(不是人类，而是某种形式的程序)完成的吗 |
| `channel` | `STRING` | 维基百科频道发生了变化 |
| `regionName` | `STRING` | 进行更改的地理区域名称 |
| `cityName` | `STRING` | 进行更改的城市名称 |
| `countryIsoCode` | `STRING` | 进行变更的国家的国际标准化组织代码 |
| `user` | `STRING` | 做出更改的维基百科用户或 IP 地址 |
| `countryName` | `STRING` | 进行更改的国家的名称 |
| `isAnonymous` | `STRING` | 更改是否由匿名用户完成(未登录状态)？ |
| `regionIsoCode` | `STRING` | 进行变更的地理区域的国际标准化组织代码 |
| `metroCode` | `STRING` | 这类似于美国的邮政编码(见[http://www . nlsinfo . org/usersvc/NLSY97/NLSY97 rnd 9 geocodecodebooksupplement/GATT 101 . html](http://www.nlsinfo.org/usersvc/NLSY97/NLSY97Rnd9geocodeCodebookSupplement/gatt101.html)) |
| `namespace` | `STRING` | 维基百科文章/页面命名空间 |
| `comment` | `STRING` | 为此更改添加的注释 |
| `isNew` | `STRING` | `true`如果这是新的一页(见[https://en.wikipedia.org/wiki/Wikipedia:Glossary#N](https://en.wikipedia.org/wiki/Wikipedia:Glossary#N) |
| `isUnpatrolled` | `STRING` | `true`如果变更不是巡逻变更(见[https://en.wikipedia.org/wiki/Wikipedia:New_pages_patrol](https://en.wikipedia.org/wiki/Wikipedia:New_pages_patrol) |

因此，我们列出了数据的所有属性。让我们看一下示例一，以更好地理解我们正在谈论的内容:

```sh
{
  "time": "2015-09-12T00:47:21.578Z",
  "channel": "#en.wikipedia",
  "cityName": null,
  "comment": "Copying assessment table to wiki",
  "countryIsoCode": null,
  "countryName": null,
  "isAnonymous": false,
  "isMinor": false,
  "isNew": false,
  "isRobot": true,
  "isUnpatrolled": false,
  "metroCode": null,
  "namespace": "User",
  "page": "User:WP 1.0 bot/Tables/Project/Pubs",
  "regionIsoCode": null,
  "regionName": null,
  "user": "WP 1.0 bot",
  "delta": 121,
  "added": 121,
  "deleted": 0
}
```

一旦我们对数据维度有了一些了解，我们就需要看看我们可以从这些数据中回答什么类型的问题。这些问题是我们容易获得的见解。稍后，我们可以用最适合我们的图形形式来表示这些。

让我们看看从这些数据中我们可以回答的一些问题。

**一维洞见**:

*   哪些城市是产生变化的地方？
*   哪些页面被更改了？
*   哪些国家做出了改变？
*   创建了多少新页面？

**沿尺寸**计数:

*   每个城市有多少变化？
*   哪些是做出改变的顶级城市？
*   哪些是促成这些变化的顶级用户？
*   经常更改的名称空间有哪些？

**多维洞察**:

*   所有国家在上午 9 点到 10 点之间发生了多少变化？
*   机器人编辑的挂钟时间是什么时候？
*   哪个国家的变化起源最多，是在什么时候被机器人瞄准的？

看起来很有趣，对吧？为什么我们不尝试使用 Apache 超集来创建一个具有这些见解的仪表板呢？

为此，我们需要在超集应用中遵循这个简单的工作流程:

1.  数据来源:
    *   从支持的数据库中定义新的数据源
    *   刷新 Apache 德鲁伊数据源
2.  创建切片
3.  使用切片制作仪表板

如果我们回想一下，我们已经在前面的章节中完成了*步骤 1* 。所以，我们可以马上进入第二步和第三步。

# 使用维基百科数据创建超集切片

让我们看看使用超集应用中的切片功能可以生成什么类型的图形。

# 唯一用户数

在此切片中，我们将看到如何生成图形来查找对数据集中的编辑做出贡献的唯一用户。

首先，我们需要从顶部导航进入切片页面。之后，屏幕看起来是这样的:

![](img/178cfd0e-a1c9-4260-80b8-b8e5e6b76a58.png)

在此页面中，单击加号图标(+)添加新切片:

![](img/35366e2f-997f-4366-84b8-3755677d28fc.png)

之后，我们会看到系统中配置的数据源列表。我们必须点击数据源名称:

![](img/9667e719-7831-48fc-9d87-4667320f76de.png)

在我们点击 wikiticker 之后，我们会进入可视化页面，在那里我们定义想要呈现为图形的维度。

对于当前的用例，让我们从用户界面中选择以下选项:

| **用户界面位置** | **图形** | **解释** |
| 补充报道 | ![](img/dfd162c5-4692-4ee4-865a-b17419505a33.png) | 选择**数据源**作为【德鲁伊-安巴里】。【维基百科】和图形类型为大数字*。*在时间部分，选择自 5 年前开始的值，并将其余值保留为默认值。在公制部分。从自动完成中选择 COUNT(DISTINCT user_unique)。在子标题部分，添加屏幕上显示的唯一用户数。之后，点击顶部的查询按钮。 |
| 图形输出 | ![](img/800cad03-8066-4307-9252-a685b878591e.png) | 我们在这张图中看到了查询的结果。 |
| 保存切片 | ![](img/798a6ed3-139e-4199-9f06-951316752e77.png) | 点击顶部的另存为按钮会显示一个类似这样的弹出窗口，我们需要在其中添加相应的值。将切片保存为`Unique Users`并将其添加到名为`My Dashboard 1`的新仪表板中。 |

听起来很简单，对吧？我们先不要急着看仪表盘。在接下来的部分中，让我们根据数据创建更多分析。

# 美国顶级地区的词云

在这一节中，我们将学习如何为美国顶级地区构建一个单词云，这些地区为我们在 Druid 中的数据源中的维基百科编辑做出了贡献。我们可以继续编辑上一节中的相同切片，或者转到一个空白切片，如前一节所述。

让我们专注于生成单词云时需要选择的值:

| **用户界面位置** | **图形** | **解释** |
| 补充报道 | ![](img/0ad3f384-ff61-47a6-aa8e-db2a4eae0ef6.png) | 选择数据源为[德鲁伊-安巴里]。[维基百科]图形类型为 Word Cloud。在时间部分，选择自 5 年前的值，并将其余值保留为默认值。 |
|  | ![](img/4c1ceee1-5973-450a-972b-4988d1920fc8.png) | 在“系列”部分。从下拉列表中选择地区名称。在度量中，选择计数(*)，这是总编辑计数。 |
|  | ![](img/c8882e1f-fd08-4647-b59d-91b4b4463747.png) | 在“过滤器”部分，选择“国家/地区代码”；它应该在美国。添加另一个过滤器以仅选择有效区域(跳过空代码)。如图所示，添加这些值。 |
| 图形输出 | ![](img/92a36d97-f891-40f6-b609-7846c7ede83b.png) | 点击查询后，我们看到这个美丽的字云。 |
| 保存切片 | ![](img/690e8348-71ec-4532-9bf3-586060cd9bbf.png) | 点击顶部的另存为按钮会显示一个类似这样的弹出窗口，我们需要在其中添加相应的值。将切片另存为`Word Cloud - Top US Regions`并将其添加到名为`My Dashboard 1` *的新仪表板中。* |

单词云的意义在于我们可以根据单词的相对大小来看到最上面的单词。当我们想要看到相对意义的单词较少时，这种可视化是有帮助的。

让我们尝试从数据中生成另一个图形。

# 太阳爆发图——十大城市

在本节中，我们将了解到一种不同类型的图表，这种图表我们在本章中还没有见过。但是首先，让我们提出用例。

我们希望在所有三个级别找到每个频道、城市名称和命名空间的唯一用户；也就是说，图形应该能够向我们展示:

*   每个频道的唯一用户
*   每个频道/城市名称的唯一用户
*   每个频道/城市名称/命名空间的唯一用户

为了显示这种层次数据，我们可以使用太阳爆发图。

让我们看看需要选择什么类型的值来呈现这种类型的图表:

| **用户界面位置** | **图形** | **解释** |
| 补充报道 | ![](img/0329cb06-5945-445b-8893-b5e9f2f69255.png) | 选择数据源为[德鲁伊-安巴里]。[维基百科]图形类型为 Sunburst。在时间部分，选择自 5 年前的值，并将其余值保留为默认值。 |
|  | ![](img/c4970db4-b189-470b-9e03-fa0488452a60.png) | 在层次部分，从下拉列表中选择`channel`、`cityName`和`namespace`。在主要指标和次要指标中，选择计数(DISTINCT user_unique)，这是用户总数。 |
|  | ![](img/11a6a4bc-0a09-4e22-b301-2d32bab56027.png) | 在**过滤器**部分，选择城市名称，并使用正则表达式匹配添加非空条件 |
|  | ![](img/e6b4b21e-6e88-43fa-a7b8-d49c731e309e.png) | 点击顶部的“另存为”按钮将显示如下弹出窗口。我们需要在这里添加相应的值。将切片另存为`Sunburst - Top 10 Cities`并将其添加到名为`My Dashboard 1` *的新仪表板中。* |
| 图形输出 | ![](img/fa393586-c43c-442b-8b17-0a69a61a1835.png) | 点击查询后，我们看到这个美丽的图形。 |

如我们所见，图中有三个同心环:

*   最内环是`channel`维度
*   中环显示`cityName`尺寸
*   最外环是`namespace`维度

当我们悬停在最内环上时，我们可以看到它是如何扩展到最外环的。同样的事情也发生在其他戒指上。

当我们想要对数据进行漏斗分析时，这种图形非常有用。让我们在下一节中看看另一种类型的分析。

# 通过定向强制布局的前 50 个通道和名称空间

**定向力布局** ( **DFL** )是一个点与点相互连接的网络布局。由于是力布局，我们可以看到`d3.js`应用物理引擎时，屏幕上的点在移动。

在这个网络图中，我们希望通过唯一的用户数度量来了解名称空间和通道之间的连通性。由于这是一个网络图，我们将看到节点在不同的路径上重复。

让我们看看如何得出这个图表:

| **用户界面位置** | **图形** | **解释** |
| 补充报道 | ![](img/d0bacbf9-fbe8-4b9a-868d-afb3fead3ed0.png) | 选择数据源为[德鲁伊-安巴里]。[维基百科]和图形类型为定向力布局*。*在时间部分，选择自 5 年前开始的值，并将其余值保留为默认值。 |
|  | ![](img/e5b3cdcf-eebf-45d9-a9b0-89d3ed63f1e8.png) | 在源/目标部分，从下拉菜单中选择`channel`和`namespace`。在度量部分，选择计数(DISTINCT user_unique)，这是用户总数。我们将世界排名限制在 50 名，这样我们就只能看到前 50 名*。* |
|  | ![](img/540474de-0c35-4f68-b0f7-3004dfc034c1.png) | 点击顶部的另存为按钮会显示一个类似这样的弹出窗口，我们需要在其中添加相应的值。将切片保存为`DFL - Top 50 Channels & Namespaces`。将其添加到名为`My Dashboard 1`的新仪表板中。 |
| 图形输出 | ![](img/a2c1210d-ffd7-48ac-896f-a7f1ed5dc218.png) | 点击查询后，我们看到这个美丽的图形。 |

请随意拖动图形中的节点，以了解更多关于它们如何相互连接的信息。节点的大小表示唯一的用户数及其细分(类似于阳光爆发图)。

让我们在下一节花一些时间学习另一个可视化和业务用例。

# 前 25 名国家/渠道分布

现在我们将学习桑基图，这是一种类似瀑布的方式来表示数据之间的分解和互连。在这种情况下，我们希望了解在唯一用户指标方面，通道名称和国家名称维度是如何关联的:

| **用户界面位置** | **图形** | **解释** |
| 补充报道 | ![](img/b7bc7cbf-4167-49e8-b6e4-758a9813b6e1.png) | 选择**数据源**作为【德鲁伊-安巴里】。【维基百科】和作为桑基的**图形类型**。在**时间**部分，选择 5 年前的“自”值，其余值默认。 |
|  | ![](img/c11d0b06-6547-4392-b8d3-a1aa54e98eb0.png) | 在**源/目标**部分，从下拉菜单中选择`channel`和`countryName`。在指标中，选择总编辑计数(*)。将行限制保持在 25；所以我们只会看到前 25 项*。* |
|  | ![](img/1eaf35b3-357d-4876-a6b9-cb61601ac660.png) | 在**过滤器**部分，选择国家名称并启用正则表达式过滤器，以便只选择那些具有有效国家名称的记录。 |
|  | ![](img/42a3d617-7a02-4545-8ef9-674f23fce62b.png) | 点击顶部的**另存为**按钮将显示一个弹出窗口。我们需要在这里添加相应的值。将切片另存为`Top 25 Countries/Channels Distribution`并将其添加到名为`My Dashboard 1`的新仪表板中。 |
| 图形输出 | ![](img/0efcc190-641e-49dc-946c-0bddf1cba3fe.png) | 点击查询后，我们看到这个美丽的图形。 |

至此，我们可以生成的所有分析列表都已完成。现在在下一节中，我们将看到如何在仪表板中使用它(这是我们最初的目标)。

# 创建维基百科编辑来自切片的仪表板

到目前为止，我们已经看到了如何在 Apache 超集应用中为维基百科编辑存储在 Apache Druid 数据库中的数据创建切片。现在是时候让我们看看如何创建一个仪表板，以便我们可以与业务团队或任何其他我们想要分享见解的团队共享它。

在这个过程中，第一步是单击顶部导航栏上的仪表板菜单。这将带我们进入添加新仪表板页面，我们需要填写以下详细信息。

| **元素** | **描述** | **值** |
| 标题 | 这是我们要创建的仪表板的名称 | 我的仪表板 1 |
| 鼻涕虫 | 仪表板的简称 | dash1 |
| 部分 | 我们要添加到仪表板的切片列表。 | 

1.  Sunburst-Top 10 Cities
2.  DFL-Top 50 Channels & Namespace
3.  Top 25 countries/channels contribution
4.  Cloud-the top region of the United States
5.  Unique user

 |
| 其他字段 | 我们可以将其他字段留空，因为它们不是创建仪表板所必需的 |  |

这是本页的图片:

![](img/91059ba0-71e3-47ab-adf9-6752f0d7b4a4.png)

一旦更改看起来不错，点击屏幕底部的保存按钮。

这将带我们进入下一步，我们可以看到仪表板已成功创建:

![](img/33f7bb5a-fc17-4a85-977e-bc84f3c69250.png)

我们可以在仪表板列表中看到我的仪表板 1。要访问此仪表板，请单击它，我们将进入仪表板屏幕:

![](img/4f258abd-6893-4f4d-9e83-bfa0e4cab0eb.png)

如我们所见，我们有一种非常强大的方式来表示所有原始数据。这肯定会对最终用户产生影响，确保信息得到传达。

到目前为止，我们已经学习了如何从存储在 Apache Druid 柱状数据库中的数据创建切片和仪表板。在下一节中，我们将看到如何连接到关系数据库管理系统，并根据这些数据生成切片和仪表板。

# 带有 RDBMS 的 Apache 超集

Apache 超集是使用 Python 编程语言构建的，并且支持许多关系数据库，因为它使用 SQLAlchemy 作为数据库驱动程序。这些驱动程序的安装超出了本节的范围。但是，安装这些应该非常容易。大多数时候，操作系统供应商会为我们打包它们。所以，我们不用担心手动安装这些。

# 支持的数据库

以下是 Apache 超集支持的一些数据库:

| **数据库名称** | **Python 包名** | **驾驶员 URI 前缀** | **详情** |
| 关系型数据库 | `mysqlclient` | `mysql://` | 甲骨文数据库 |
| 一种数据库系统 | `psycopg2` | `postgresql+psycopg2://` | 世界上最先进的开源数据库 |
| 很快 | `pyhive` | `presto://` | 开源分布式查询引擎 |
| 神谕 | `cx_Oracle` | `oracle://` | 甲骨文公司创建的多模型数据库管理系统。 |
| Sqlite |  | `sqlite://` | 快速、可扩展的嵌入式数据库库 |
| 红移 | `sqlalchemy-redshift` | `postgresql+psycopg2://` | 亚马逊红移是建立在 PostgreSQL 上的柱状数据库 |
| MSSQL | `pymssql` | `mssql://` | 微软 SQL 服务器 |
| 黑斑羚 | `impyla` | `impala://` | Apache Impala 是运行在 Hadoop 上的大规模并行处理 SQL 引擎 |
| SparkSQL | `pyhive` | `jdbc+hive://` | Apache Spark 模块，用于在 Spark 程序中编写 SQL。 |
| 青梅 | `psycopg2` | `postgresql+psycopg2://` | Greenplum 是一个先进的、功能齐全的开源数据平台 |
| 雅典娜(智慧与技艺的女神) | `PyAthenaJDBC` | `awsathena+jdbc://` | 亚马逊雅典娜是无服务器交互式查询服务 |
| 垂直的 | `sqlalchemy-vertica-python` | `vertica+vertica_python://` | Vertica 是大数据分析软件 |
| 钟罩 | `sqlalchemy-clickhouse` | `clickhouse://` | 开源分布式柱状数据存储 |

Portions of the above table is extracted from the official documentation of Apache Superset ([https://superset.incubator.apache.org/installation.html#database-dependencies](https://superset.incubator.apache.org/installation.html#database-dependencies))

# 了解员工数据库

如果您还记得，在前面的部分中，我们已经导入了一个名为 Employees 的示例数据库，并将其加载到 MySQL 数据库中。我们将深入研究这个示例数据存储，以便了解我们可以从中生成哪些类型的分析。

# 雇员表

`employees`表包含员工的详细信息(随机生成的数据)，具有以下属性

| **栏** | **数据类型** | **描述** |
| `emp_no` | `INTEGER` | 员工编号 |
| `birth_date` | `DATE` | 员工出生日期 |
| `first_name` | `STRING` | 员工的名字 |
| `last_name` | `STRING` | 员工的姓氏 |
| `gender` | `STRING` | 员工性别，男性为男，女性为女 |
| `hire_date` | `STRING` | 员工的最新加入日期 |

# 部门表

`departments`表由组织中每个部门的基本细节组成。这张表进一步说明了这一点:

| **表列** | **数据类型** | **描述** |
| `dept_no` | `STRING` | 部门编号 |
| `dept_name` | `STRING` | 部门名称 |

# 部门经理表

`dept_manager`表记录了员工担任给定部门经理的情况。更多详情见下表:

| **表列** | 数据类型 | **描述** |
| `emp_no` | `INT` | 担任该部门经理的员工 ID |
| `dept_no` | `STRING` | 部门标识 |
| `from_date` | `DATE` | 员工担任该部门经理的起始日期。 |
| `to_date` | `DATE` | 员工担任该部门经理的截止日期。 |

# 部门员工表

`dept_emp`表由显示每个员工属于一个部门的时间的所有记录组成。

| **表列** | **数据类型** | **描述** |
| `emp_no` | `INT` | 员工编号 |
| `dept_no` | `STRING` | 部门标识 |
| `from_date` | `DATE` | 员工所属部门的起始日期 |
| `to_date` | `DATE` | 该部门员工的最后日期 |

# 标题表

**标题**表包含从给定日期到结束日期的所有员工角色。更多细节如下所示:

| **表列** | **数据类型** | **描述** |
| `emp_no` | `INT` | 员工 Id |
| `title` | `STRING` | 员工的指定 |
| `from_date` | `DATE` | 员工担任此角色的开始日期 |
| `to_date` | `DATE` | 员工履行此职责的最后日期 |

# 薪金表

`salaries`表由给定员工的薪资历史记录组成。下表解释了更多详细信息:

| **表列** | **数据类型** | **描述** |
| `emp_no` | `INT` | 员工 Id |
| `salary` | `INT` | 员工工资 |
| `from_date` | `DATE` | 计算薪资的起始日 |
| `to_date` | `DATE` | 计算薪资的最后一天。 |

# 标准化员工表

`employee_norm`表由员工、工资、部门、`dept_emp`和职称表的数据组成。让我们详细看看这张表:

| **表列** | **数据类型** | **描述** |
| `emp_no` | `INT` | 员工编号 |
| `birth_date` | `DATE` | 员工出生日期 |
| `full_name` | `STRING` | 员工全名 |
| `gender` | `STRING` | 员工的性别 |
| `hire_date` | `DATE` | 员工加入日期 |
| `salary` | `INT` | 该期间员工的工资 |
| `from_date` | `DATE` | 薪资期间开始 |
| `to_date` | `DATE` | 薪资期间结束 |
| `dept_name` | `STRING` | 员工在此薪资期间工作的部门 |
| `title` | `STRING` | 在此期间指定员工 |

了解了员工数据库中的各种表格后，我们现在对目前掌握的数据有了一些了解。现在，下一个任务是找出我们可以从这些数据中生成什么类型的分析。我们将在下一节中了解这一点。

# 员工数据库的超集切片

一旦我们对存储在 MySQL 数据库中的数据类型有了一些基本的了解。我们现在将看到我们可以从这些数据中回答什么类型的问题。

**一维洞见:**

*   组织中有多少员工？
*   组织中所有员工的工资总额是多少？
*   有多少个部门？

**多维洞察**

*   每年支付的工资总额是多少？
*   每个部门的总工资是多少？
*   谁是每年收入最高的员工？

如果我们按照这些思路思考，我们应该能够回答关于数据的非常重要的问题，并且应该能够生成漂亮的图形。

让我们举几个例子来说明在接下来的部分中我们可以生成什么类型的可视化。

# 注册 MySQL 数据库/表

在我们开始为员工表生成切片之前，我们应该首先注册它。注册过程包括以下步骤。

通过单击顶部导航栏中“源”菜单中的“数据库”下拉列表打开数据库，如下所示:

![](img/3ddfbcc9-a012-48e3-b287-5ba19ec18e8c.png)

之后，我们需要点击页面上的加号(+)图标:

![](img/cac387f0-9a8d-4256-ac05-21b6ec2616b1.png)

这将把我们带到一个页面，在那里我们可以注册新的数据库。屏幕如下所示:

![](img/32fa6696-267f-43c4-af46-86e6b86b2745.png)

我们将填写如下所示的详细信息。

| **字段名** | **值** | **描述** |  |
| 数据库ˌ资料库 | `employees` | 我们要注册的数据库的名称。(在 MySQL 数据库中输入与其相同的名称) |  |
| SQL anywhere uri | `mysql+pymysql://superset:superset@master:3306/employees` | URI 可以编程访问这个数据库。这将包括协议/驱动程序、用户名、密码、主机名和数据库名 |  |
| 其他字段 |  | 将它们保留为默认值 |  |

在此之后，点击保存按钮，这将使用 Apache 超集保存数据库详细信息。我们将进入表格列表页面，如下所示:

![](img/96399df1-4b77-4313-84ed-669d011ddb85.png)

如我们所见，我们已经在 MySQL 后端注册了员工数据库。

在下一步中，我们需要从顶部菜单中选择表格:

![](img/d624d6e5-a390-494e-9a63-9213dff650b5.png)

因为我们没有注册任何表，所以我们将看到一个像这样的空页面:

![](img/55176447-b183-47af-b93c-c208d6ed065f.png)

为了注册一个新的表，我们必须点击用户界面中的加号(图标)，这将我们带到以下页面:

![](img/a54b67c3-3d25-4ca9-8727-71335bf1a2d2.png)

输入如下所示的字段值，完成后单击保存:

| **字段名** | **值** | **描述** |
| 表名 | `employee_norm` | 我们要注册的表的名称。 |
| 数据库ˌ资料库 | `employees` | 选择已经向超集注册的数据库。 |

现在我们可以看到该表已成功注册，如下图屏幕所示:

![](img/81790eb0-b29e-4aa1-9927-35e994db7b02.png)

超集的一个重要特性是，它将根据数据类型自动选择我们可以对表的列执行的不同类型的操作。这决定了我们在用户界面的其余部分显示哪些类型的维度和指标。

为了选择这些选项，我们需要通过单击编辑图标来编辑表格，我们将看到此页面:

![](img/793aa239-22f3-4b32-8810-9b5bd6c9e791.png)

正如我们所看到的，Apache 超集已经自动识别了每个字段的数据类型，它还为我们提供了一个选项来为各种活动选择这些维度。下表列出了这些活动:

| **活动** | **描述** |
| 可分组 | 如果选中该复选框，则该字段可用作分组操作的一部分(在 SQL 中为`GROUP BY`)。 |
| 可滤过的 | 如果选中该复选框，则该字段可用作条件操作(`WHERE`子句)的一部分。 |
| 区分计数 | 如果选中该复选框，则该字段可用作字段计数(`DISTINCT`)操作的一部分。 |
| 总和 | 如果选择了复选框，则该字段可用作`SUM()`功能的一部分。 |
| 最小/最大 | 指示该字段可用作查找最小值和最大值的一部分。 |
| 是暂时的 | 指示该字段是时间维度。 |

如上所示进行更改，然后单击保存按钮。

现在，我们准备在接下来的步骤中开始创建切片和仪表板。

# 切片和仪表板创建

正如我们在前面几节中看到的，为了创建仪表板，我们首先需要创建切片。在本节中，我们将学习创建几个切片。

# 部门薪资拆分

在这一部分中，我们将学习如何创建可视化，显示每个部门的薪资细分百分比:

| **用户界面位置** | **图形** | **描述** |
| 补充报道 | ![](img/767c4c8e-67d2-4cb9-a705-2d344b10d236.png) | **数据源&图表类型**:选择【员工】。**数据源**为【员工 _ 定额】，图表类型为“分布-国家职业类别 3 -饼图”在**时间**部分，选择出生日期作为时间栏，选择 100 年前作为**自**起栏。在**指标**部分，从下拉列表中选择 sum_salary 作为值，部门名称作为**分组依据。** |
| 图形输出 | ![](img/665462dd-38a7-469b-a8bd-ecfb036cec2e.png) | 点击“查询”按钮将会显示这张喜欢的图表。用部门薪资分手这个名字保存。 |

就像上一节一样，看看在没有任何编程知识的情况下创建好看的图形有多容易。

在下一节中，我们将从同一员工数据库中了解另一种类型的图形。

# 薪酬多样性

这是一个重要的图表，在这里我们确定了在整个组织历史中，不同性别之间的工资差异。这里我们用平均工资作为分析的基础。

| **用户界面位置** | **图形** | **描述** |
| 补充报道 | ![](img/87174669-baa2-45d8-8df5-c407e6a5d63e.png) | **Datasource & Chart Type**: select [employees].[employee_norm] as the datasource and Time Series - line chart as chart type 在时间部分，选择出生日期作为时间列& 100 年前作为自列。在指标部分，选择平均工资作为指标，性别作为`Group By`。 |
| 输出 | ![](img/c15c7256-324e-4b28-aea1-0c5935ffec73.png) | 显示每个性别每年平均工资的图形。用标题**工资差异**保存 |

从图中我们可以看出，男女之间的工资差距很大，而且非常接近。同期平均工资也有类似的增长。

在下一节中，我们将学习生成另一种类型的图形，这将使我们对数据有不同的见解。

# 每年每个角色的薪资变化

这是一个重要的统计数据，我们想知道组织中不同职位的工资变化有多大。

| **用户界面位置** | **图形** | **描述** |
| 补充报道 | ![](img/77e45d70-7738-40c3-9598-52ab46387de3.png) | Datasource & Chart Type: select [employees].[employee_norm] as the datasource and Time Series - Percentage Change as chart type
In the Time section, Select from_date as **Time** column , Year as **Time Granularity** & 100 years ago as **Since** column.在**指标**部分，选择 sum_salary 作为**指标**，标题为**组** By。 |
| 输出 | ![](img/e78d9c2b-5723-4430-9238-abfc62336bc1.png) | 点击查询，我们会得到下面的图形。用名称**每年每个角色的薪资变化**保存。 |

从这张图中我们可以发现，很少有角色在组织内的总薪酬中有非常大的差异。

到目前为止，我们已经创建了三个切片，我们将使用到目前为止创建的切片创建一个新的仪表板。

# 仪表板创建

在这一步中，我们将通过转到仪表板页面并单击添加仪表板图标(如前几节所示)来创建新的仪表板。

我们将看到下面的屏幕，在这里我们选择到目前为止创建的三个切片，然后单击保存:

![](img/66ae3349-1abb-46f8-a9c4-81e993e1b2ef.png)

仪表板保存成功后，我们可以看到它是这样的:

![](img/b410d0cb-ad1e-4d4b-a76e-f519e88db039.png)

正如我们所看到的，仪表板是以简单的方式表达大量数据的非常强大的方式。

# 摘要

在本章中，我们学习了数据可视化，以及它如何帮助用户在不了解底层数据的情况下接收所需的消息。然后，我们看到了以图形方式可视化数据的不同方法。

我们浏览了用于可视化数据的 Hadoop 应用，如 Apache Druid 和 Apache 超集，并学习了如何将它们与 MySQL 等关系数据库一起使用。我们还看到了一个示例数据库，以帮助我们更好地理解应用。

在下一章中，我们将学习如何在云上构建我们的 Hadoop 集群。