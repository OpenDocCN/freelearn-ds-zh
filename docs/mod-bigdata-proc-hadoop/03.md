# 三、Hadoop 设计考虑

大数据不一定意味着大数据。如果数据集很小，分析它非常容易。我们可以将其加载到 Excel 电子表格中，并进行所需的计算。但是，随着数据量越来越大，我们必须找到其他方法来处理它。我们可能必须将其加载到一个 RDMBS 表中，并运行一个 SQL 查询来查找给定结构的趋势和模式。此外，如果数据集格式更改为类似电子邮件的格式，那么加载到关系数据库管理系统将成为一个巨大的挑战。更复杂的是，如果数据速度变得像实时一样，用传统的基于关系数据库管理系统的工具分析给定的数据集几乎是不可能的。在现代世界中，术语*大数据*可以用五个最著名的 *V* s 来表示，以下是每个 *V* 的简单解释。

![](img/21dfce7d-6161-4729-94cb-0e9c871736e3.png)

在本章中，我们将涵盖以下主题:

*   数据结构原理
*   安装 Hadoop 集群
*   探索 Hadoop 架构
*   引入 Yarn
*   Hadoop 集群组合
*   Hadoop 文件格式

# 理解数据结构原理

让我们来看看一些重要的数据架构原则:

*   **数据是企业的资产**:数据具有可衡量的价值。它为企业提供了一些真正的价值。在现代，数据被视为真金白银。
*   **数据在企业范围内共享**:数据只被捕获一次，然后被多次使用和分析。多个用户针对不同的用例和需求访问相同的数据。
*   **数据治理**:治理数据，保证数据质量。
*   **数据管理**:需要对数据进行管理，以达到企业目标。
*   **数据访问**:所有用户都要有数据访问权限。
*   **数据安全**:数据要妥善保护。
*   **数据定义**:数据的每个属性都需要在企业范围内统一定义。

既然我们已经了解了大数据的基础知识及其原理，让我们开始一些实际行动吧。

# 安装 Hadoop 集群

安装 Hadoop 集群需要执行以下步骤。在撰写本书时，Hadoop 2 . 7 . 3 版本是一个稳定的版本。我们会安装它。

1.  使用以下命令检查 Java 版本:

```sh
Java -version
Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)
You need to have Java 1.6 onwards 
```

2.  借助以下命令，在所有服务器上创建一个 Hadoop 用户帐户，包括所有名称节点和数据节点:

```sh
useradd hadoop
passwd hadoop1 
```

假设我们有四个服务器，我们必须使用所有四个服务器创建一个 Hadoop 集群。这四个服务器的 IPs 如下:`192.168.11.1`、`192.168.11.2`、`192.168.11.3`、`192.168.11.4`。在这四个服务器中，我们将首先使用一个服务器作为主服务器(名称节点)，其余所有服务器都将作为从服务器(数据节点)。

3.  在名称节点和数据节点这两个服务器上，使用以下命令更改`/etc/hosts`文件:

```sh
vi /etc/hosts--   
```

4.  然后将以下内容添加到所有服务器上的所有文件中:

```sh
NameNode 192.168.11.1
DataNode1 192.168.11.2
DataNode2 192.168.11.3
DataNode3 192.168.11.4 
```

5.  现在，在名称节点和数据节点上设置 SSH:

```sh
su - hadoop
ssh-keygen -t rsa
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@namenode
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@datanode1
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@datanode2
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@datanode3
chmod 0600 ~/.ssh/authorized_keys
exit
```

6.  在名称节点和所有数据节点上下载并安装 Hadoop:

```sh
mkdir /opt/hadoop
cd /opt/hadoop
wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
tar -xvf hadoop-2.7.3.tar.gz 
mv Hadoop-2.7.3 hadoop
chown -R hadoop /opt/hadoop
cd /opt/hadoop/Hadoop
```

# 在名称节点上配置 Hadoop

登录到名称节点:

```sh
cd /opt/Hadoop/conf

vi core-site.xml  
```

使用这些值查找并更改以下属性:

| **文件名** | **物业名称** | **属性值** |
| `core-site.xml` | `fs.default.name` | `hdfs://namenode:9000/` |
|  | `dfs.permissions` | `False` |
| `hdfs-site.xml` | `dfs.data.dir` | `/opt/hadoop/hadoop/dfs/namenode/data` |
|  | `dfs.name.dir` | `/opt/hadoop/hadoop/dfs/namenode` |
|  | `dfs.replication` | `1` |
| `mapred-site.xml` | `mapred.job.tracker` | `namenode:9001` |

```sh
    vi masters
    namenode

    vi slaves
    datanode1
    datanode2
    datanode3

```

# 格式名称模式

以下代码用于格式化名称节点:

```sh
 cd /opt/Hadoop/Hadoop/bin

    hadoop -namenode  -format 
```

# 启动所有服务

我们用以下代码行启动所有服务:

```sh
    ./start-all.sh

```

有关如何设置 Hadoop 单节点和多节点集群的详细信息，请使用以下链接:[https://Hadoop . Apache . org/docs/r 2 . 7 . 0/Hadoop-project-dist/Hadoop-common/ClusterSetup . html](https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/ClusterSetup.html)。

# 探索 HDFS 建筑

HDFS 建筑基于主模式和从模式。名称节点是主节点，所有数据节点都是从属节点。以下是关于这两个节点需要注意的一些要点。

# 定义名称节点

名称节点是 Hadoop 集群中所有数据节点的主节点。它只存储以树的形式存储的文件和目录的元数据。重要的一点是，除了元数据，NameNode 从不存储任何其他数据。名称节点跟踪以块的形式写入数据节点的所有数据。默认块大小为 256 兆字节(可配置)。没有名称节点，就无法读取数据节点文件系统上的数据。元数据使用两个文件本地存储在名称节点上——文件系统名称空间映像文件、FSImage 和编辑日志。FSImage 是从 NameNode 编辑日志开始的文件系统的快照—自 NameNode 启动以来文件系统的所有更改，当 NameNode 启动时，它读取 FSImage 文件并编辑日志文件。所有的事务(编辑)都被合并到 FSImage 文件中。将 FSImage 文件写入磁盘，并创建一个新的空编辑日志文件来记录所有编辑。由于名称节点不经常重新启动，编辑日志文件变得非常大，难以管理。重新启动名称节点时，需要很长时间才能重新启动，因为所有编辑都需要应用到 FSImage 文件。在名称节点崩溃的情况下，编辑日志文件中的所有元数据都不会写入 FSImage 文件，并且会丢失。

# 辅助名称节点

辅助名称节点的名称令人困惑。它不充当名称节点。它的主要功能是从名称节点获取文件系统更改，并定期将其合并到名称节点 FSImage。将编辑日志文件更改写入 FSImage 称为**提交**。定期提交有助于减少名称节点的启动时间。辅助名称节点也称为提交节点。

# 名称节点安全模式

这是 HDFS 群集的只读模式。不允许客户端对文件系统或块进行任何修改。在启动过程中，NameNode 自动以安全模式启动，对 FSImage 应用编辑，自动禁用安全模式，并以正常模式重新启动。

# 数据节点

数据节点是 Hadoop 集群的主力。它们的主要功能是以块的形式存储和检索数据。他们总是以心跳的形式向名称节点传达他们的状态。这就是名称节点跟踪任何数据节点的方式，无论它们是活的还是死的。数据节点保留已知数据块和复制因子的三个副本。数据节点与其他数据节点通信，以复制数据块来维护数据复制。

# 数据复制

HDFS 体系结构支持将非常大的文件放置在集群中的机器上。每个文件都存储为一系列块。为了确保容错，每个数据块将被复制三次到三台不同的机器上。它被称为复制因子，可以在群集级别或单个文件级别进行更改。它是一个命名节点，负责做出与数据块复制相关的所有决策。名称节点从每个数据节点获取心跳和块报告。心跳确保数据节点是活动的。块报告包含数据节点上所有块的列表。

# 机架感知

HDFS 数据块放置将通过将一个数据块副本放置在不同的机架上，使用机架感知来实现容错，如下图所示:

![](img/aba679b2-4019-46ed-aea6-8d2e45428bae.png)

让我们详细了解一下图:

*   第一个副本与发起请求的数据节点放在同一机架上，例如机架 1 和数据节点 1
*   第二个副本放在另一个机架的任何数据节点上，例如，机架 2，数据节点 2
*   第三个副本放置在同一机架的任何数据节点上，例如机架 2、数据节点 3

自定义机架拓扑脚本包含选择适当数据节点的算法，可以使用 Unix 外壳、Java 或 Python 开发。可通过更改`Core-site.xml`文件中的`topology.script.file.name`参数在集群上激活。

# HDFS WebUI

下表显示了 HDFS 网络界面中的服务:

| **服务** | **协议** | **港口** | **URL** |
| WebUI 名称节点 | 超文本传送协议 | `50070` | `http://namenode:50070/` |
| 数据节点网络界面 | 超文本传送协议 | `50075` | `http://datanode:50075/` |
| 辅助名称节点 | 超文本传送协议 | `50090` | `http://Snamenode:50090/` |

# 引入 Yarn

另一个资源协商者 ( **Yarn**)分离资源管理、调度和处理组件。它有助于实现集群资源的 100%资源利用率。Yarn 基于 Hadoop 调度策略管理集群的中央处理器和内存。Yarn 支持任何类型的应用，并不仅限于 MapReduce。它支持用任何类型的语言编写的应用，前提是可以在 Hadoop 集群上安装二进制文件。

# Yarn 结构

让我们在接下来的章节中详细了解 Yarn 的架构。

# 资源管理程序

资源管理器负责跟踪集群中的资源并调度应用。资源管理器有两个主要组件:调度器和应用管理器。

# 节点管理器

节点管理器负责启动和管理节点上的容器。容器执行应用主服务器指定的任务。它充当资源管理器的从属。每个节点管理器跟踪其从属节点上的可用数据处理资源，并定期向资源管理器发送报告。Hadoop 集群中的处理资源消耗在称为**容器**的字节大小的块中。

# Yarn 的结构

您可以执行以下步骤来配置 Yarn:

1.  启动 Hadoop 名称节点、辅助名称节点和数据节点
2.  年龄〔t0〕。

Find corresponding XML files based on your Hadoop installation.

3.  在`YARN_CONF_DIR`的定义下增加以下内容:

```sh
export HADOOP_CONF_DIR="${HADOOP_CONF_DIR:-$YARN_HOME/etc/hadoop}"
export HADOOP_COMMON_HOME="${HADOOP_COMMON_HOME:-$YARN_HOME}"
export HADOOP_HDFS_HOME="${HADOOP_HDFS_HOME:-$YARN_HOME}"  
```

4.  年龄〔t0〕:

```sh
<?xml version="1.0"?>
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce.shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
</configuration> 
```

5.  年龄〔t0〕:

```sh
<?xml version="1.0"?>
<?xml-stylesheet href="configuration.xsl"?>
<configuration>
  <property>
    <name>mapreduce.framework.name </name>
    <value>yarn</value>
  </property>
</configuration>  
```

6.  启动 Yarn 服务:

```sh
yarn resourcemanager
yarn nodemanager 
```

# 配置 HDFS 高可用性

让我们看看 Hadoop 随着时间的推移带来的变化。

# 在 Hadoop 1.x 期间

Hadoop 1.x 从单个 NameNode 的架构开始。所有用于向该单一名称节点发送数据块报告的数据节点。架构中有一个辅助名称节点，但它的唯一职责是合并对 FSImage 的所有编辑。通过这种架构，名称节点成为了**单点故障** ( **SPOF** )。由于它拥有 Hadoop 集群所有数据节点的所有元数据，因此在 NameNode 崩溃的情况下，Hadoop 集群在下次重启 NameNode 修复之前不可用。如果名称节点无法恢复，则所有数据节点中的所有数据都将完全丢失。如果关闭名称节点进行计划维护，HDFS 将无法正常使用。因此，有必要通过频繁备份名称节点文件系统来保护现有的名称节点，以最大限度地减少数据丢失。

# 在 Hadoop 2.x 和更高版本中

为了克服 HDFS **高可用性** ( **HA** )问题，让 NameNode 成为 SPOF，架构发生了变化。新的体系结构提供了在同一个群集中运行两个冗余名称节点的主动/被动配置和一个热备盘。这允许在机器崩溃的情况下快速故障切换到新的名称节点，或者出于计划维护的目的由管理员发起的正常故障切换。HDFS 高可用性提供了以下两种体系结构选项:

*   使用共享存储
*   使用仲裁日志管理器

# 使用 NFS 的 HDFS 高可用性集群

下图描述了 HDFS 高可用性集群使用 NFS 作为名称节点体系结构所需的共享存储:

![](img/6b9bba22-b44f-4c0c-91b9-00fc64bd9794.png)

# 重要的架构点

关于使用共享存储体系结构的 HDFS 高可用性，需要记住以下几点:

*   在集群中，有两个独立的机器:活动状态名称节点和备用状态名称节点。
*   在任何给定的时间点，只有一个名称节点处于活动状态，另一个处于待机状态。
*   活动的名称节点管理来自集群中所有客户端数据节点的请求，而备用节点仍然是从属节点。
*   所有数据节点都以这样的方式进行配置，即它们向活动和备用名称节点发送其数据块报告和心跳。
*   备用名称节点保持其状态与活动名称节点同步。
*   活动节点和备用节点都可以访问共享存储设备上的文件系统(例如，从 NAS 装载的 NFS)
*   当客户端对文件系统进行任何更改时，活动的名称节点会对网络共享目录中的编辑日志文件进行相应的更改(编辑)。
*   备用名称节点对其自己的名称空间进行所有相应的更改。这样，它与活动的名称节点保持同步。
*   在活动名称节点不可用的情况下，备用名称节点会确保吸收共享网络目录中的所有更改(编辑)，并将其自身提升为活动名称节点。
*   Hadoop 管理员应该对共享存储应用隔离方法，以避免在给定时间使两个名称节点都处于活动状态的情况。在发生故障切换时，隔离方法会切断对先前活动名称节点的访问，以便对共享存储进行任何更改，从而确保顺利故障切换到备用名称节点。之后，备用名称节点成为活动名称节点。

# 高可用性名称的配置具有共享存储的节点

在`hdfs-site.xml`中添加以下属性:

| **属性** | **值** |
| `dfs.nameservices` | `cluster_name` |
| `dfs.ha.namenodes.cluster_name` | `NN1`、`NN2` |
| `dfs.namenode.rpc-address.cluster_name.NN1` | `machine1:8020` |
| `dfs.namenode.rpc-address.cluster_name.NN2` | `machine2:8020` |
| `dfs.namenode.http-address.cluster_name.NN1` | `machine1:50070` |
| `dfs.namenode.http-address.cluster_name.NN2` | `machine2:50070` |
| `dfs.namenode.shared.edits.dir` | `file:///mnt/filer1/dfs/ha-name-dir-shared` |
| `dfs.client.failover.proxy.provider.cluster_name` | `org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider` |
| `dfs.ha.fencing.methods` | `sshfence` |
| `dfs.ha.fencing.ssh.private-key-files` | `/home/myuser/.ssh/id_rsa` |
| `dfs.ha.fencing.methods` | `sshfence([[username][:port]])` |
| `dfs.ha.fencing.ssh.connect-timeout` | `30000` |

将以下属性添加到`core-site.xml`:

| **属性** | **值** |
| `fs.defaultFS` | `hdfs://cluster_name` |

# 使用仲裁日志管理器的 HDFS 高可用性集群

下图描述了**仲裁日志管理器** ( **QJM** )体系结构，用于在活动和备用名称节点之间共享编辑日志:

![](img/d19fd8c6-61cf-47dc-b86b-a68d32a19151.png)

# 重要的架构点

关于使用 QJM 架构的 HDFS 高可用性，需要记住以下几点:

*   在集群中，有两个独立的机器—活动状态名称节点和备用状态名称节点。
*   在任何时间点，名称节点中恰好有一个处于活动状态，另一个处于待机状态。
*   活动的名称节点管理来自集群中所有客户端数据节点的请求，而备用节点仍然是从属节点。
*   所有数据节点都以这样的方式进行配置，即它们向活动和备用名称节点发送其数据块报告和心跳。
*   活动节点和备用节点都通过与一组名为**日志节点** ( **JNs** )的独立守护进程通信来保持彼此同步。
*   当客户端进行任何文件系统更改时，活动的名称节点会将修改记录持久地记录到大多数这些 JNs 中。
*   备用节点通过与 JNs 通信，立即将这些更改应用到自己的名称空间。
*   在活动名称节点不可用的情况下，备用名称节点会确保吸收来自 JNs 的所有更改(编辑)，并将自己提升为活动名称节点。
*   为了避免在给定时间使两个名称节点都处于活动状态的情况，JNs 一次只允许一个名称节点作为编写器。这允许新的活动名称节点安全地进行故障转移。

# 带 QJM 的高可用性名称节点的配置

将以下属性添加到`hdfs-site.xml`:

| **属性** | **值** |
| `dfs.nameservices` | `cluster_name` |
| `dfs.ha.namenodes.cluster_name` | `NN1`、`NN2` |
| `dfs.namenode.rpc-address.cluster_name.NN1` | `machine1:8020` |
| `dfs.namenode.rpc-address.cluster_name.NN2` | `machine2:8020` |
| `dfs.namenode.http-address.cluster_name.NN1` | `machine1:50070` |
| `dfs.namenode.http-address.cluster_name.NN2` | `machine2:50070` |
| `dfs.namenode.shared.edits.dir` | `qjournal://node1:8485;node2:8485;node3:8485/cluster_name` |
| `dfs.client.failover.proxy.provider.cluster_name` | `org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider` |
| `dfs.ha.fencing.methods` | `sshfence` |
| `dfs.ha.fencing.ssh.private-key-files` | `/home/myuser/.ssh/id_rsa` |
| `dfs.ha.fencing.methods` | `sshfence([[username][:port]])` |
| `dfs.ha.fencing.ssh.connect-timeout` | `30000` |

将以下属性添加到`core-site.xml`:

| **属性** | **值** |
| `fs.defaultFS` | `hdfs://cluster_name` |
| `dfs.journalnode.edits.dir` | `/path/to/journal/node/local/datat` |

# 自动故障转移

非常重要的是要知道，上述两种体系结构仅支持手动故障转移。为了实现自动故障转移，我们必须引入两个额外的组件 ZooKeeper quorum，以及**ZKFailoverController**(**ZKFC**)流程和更多的配置更改。

# 重要的架构点

*   活动和备用的每个名称节点都运行 ZKFC 进程。
*   名称节点的状态由 ZKFC 监控和管理。
*   ZKFC 定期 pings 其本地名称节点，以确保名称节点是活动的。如果它没有恢复 ping，它会将该名称节点标记为不健康。
*   健康的名称节点持有一个特殊的锁。如果名称节点变得不健康，该锁将被自动删除。
*   如果本地名称节点是健康的，并且 ZKFC 看到锁当前没有被任何其他名称节点持有，它将尝试获取锁。如果它成功获得了锁，那么它就赢得了选举。现在，该名称节点负责运行故障转移，以使其本地名称节点处于活动状态。

# 配置自动故障转移

将以下属性添加到`hdfs-site.xml`以配置自动故障转移:

| **属性** | **值** |
| `dfs.ha.automatic-failover.enabled` | `true` |
| `ha.zookeeper.quorum` | `zk1:2181`、`zk2:2181`、`zk3:2181` |

# Hadoop 集群组合

众所周知，Hadoop 集群由主服务器和从服务器组成:主节点——管理基础设施，从节点——分布式数据存储和数据处理。边缘节点不是 Hadoop 集群的一部分。该机器用于与 Hadoop 集群交互。用户没有任何权限直接登录到任何主节点和数据节点，但是他们可以登录到边缘节点来运行 Hadoop 集群上的任何作业。EdgeNode 上没有存储任何应用数据。数据始终存储在 Hadoop 集群的数据节点上。根据在 Hadoop 集群上运行作业的用户数量，可以有多个 EdgeNode。如果有足够的硬件可用，最好将每个主节点和数据节点托管在单独的机器上。但是，在典型的 Hadoop 集群中，有三个主节点。

请注意，假设我们将 HBase 用作群集中的 NoSQL 数据存储。

# 典型的 Hadoop 集群

Hadoop 集群组合如下所示:

![](img/d657d3de-3c51-464f-a724-3d74f6ff3430.png)

以下是一些需要考虑的硬件规格:

*   名称节点和备用名称节点。
*   内存需求取决于要创建的文件和数据块副本的数量。通常，建议命名节点至少使用 64 GB - 96 GB 内存。
*   名称节点需要可靠的存储来托管 FSImage 和编辑日志。建议这些主节点至少有 4 TB - 6 TB 的 SAS 存储。为命名节点提供 RAID 5 - 6 存储是一个好主意。如果群集是高可用性群集，则规划您的 Hadoop 群集时，应在主节点上配置 JNs。

就处理器而言，建议至少有 2 个运行在 2 千兆赫的四核处理器，以处理主节点的消息流量。

*   数据节点/从属节点每个节点应该至少有 64 GB 内存。建议通常每个 Hadoop 守护进程需要 2 GB - 3 GB 内存，如 DataNode、节点管理器 ZooKeeper 等；5 GB 用于操作系统和其他服务；每个 MapReduce 任务 5 GB - 8 GB。
*   数据节点可能有商品存储，至少有 8 TB - 10 TB 磁盘存储，配备 7，200 转/分的 SATA 驱动器。硬盘配置应该在**只是一堆磁盘** ( **JBOD** )。
*   建议所有数据节点至少有 8 个处理器，即 2.5 千兆赫内核和 24 核处理器。
*   建议每个机架内有 1gb 到 10gb 的网络连接。对于所有从节点，建议使用 1 GB 网络带宽，对于主节点，建议使用 10 GB 带宽。
*   如果您计划将来扩展您的 Hadoop 集群，您也可以添加额外的机器。

请阅读以下来自 Hortonworks 和 Cloudera 的文章，以获取更多参考:

*   [http://docs . hortonworks . com/HDPDocuments/HDP1/HDP-1 . 3 . 3/bk _ cluster-planning-guide/content/suction . html](http://docs.hortonworks.com/HDPDocuments/HDP1/HDP-1.3.3/bk_cluster-planning-guide/content/conclusion.html)
*   [http://blog . cloud era . com/blog/2013/08/如何为新 hadoop 集群选择合适的硬件/](http://blog.cloudera.com/blog/2013/08/how-to-select-the-right-hardware-for-your-new-hadoop-cluster/)

# Hadoop 部署的最佳实践

以下是 Hadoop 部署应遵循的一些最佳实践:

*   **从小处着手**:和其他软件项目一样，一个实现 Hadoop 也涉及风险和成本。设置一个四节点的小型 Hadoop 集群总是更好。这个小集群可以设置为**概念验证** ( **概念验证**)。在使用任何 Hadoop 组件之前，都可以将其添加到现有的 Hadoop POC 集群中作为**技术证明** ( **POT** )。它允许基础设施和开发团队了解大数据项目需求。在成功完成 POC 和 POT 后，可以向现有集群添加额外的节点。
*   **Hadoop 集群监控**:要了解集群的健康状况，需要对 NameNode 和所有数据节点进行适当的监控。它有助于在节点出现问题时采取纠正措施。如果某项服务出现故障，及时采取行动有助于避免将来出现大问题。设置 Gangalia 和 Nagios 是配置警报和监控的流行选择。在 Hortonworks 集群、Ambari 监控和 Cloudera 集群的情况下，Cloudera (CDH)管理器监控可以很容易设置。
*   **自动化部署**:使用 Puppet 或 Chef 等工具对于 Hadoop 部署至关重要。使用自动化工具而不是手动部署来部署 Hadoop 集群变得超级容易和高效。使用可用的工具/组件重视数据分析和数据处理。优先使用 Hive 或 Pig 脚本来解决问题，而不是编写繁重的自定义 MapReduce 代码。目标应该是少开发多分析。
*   **高可用性的实施**:在决定高可用性基础设施和架构时，应仔细考虑需求和数据增长的任何增加。如果出现任何故障或崩溃，系统应该能够自我恢复或故障转移到另一个数据中心/站点。
*   **安全性**:需要通过创建用户和组，并将用户映射到组来保护数据。设置适当的权限和强制使用强密码应该可以锁定每个用户组。
*   **数据保护**:在将敏感数据移动到 Hadoop 集群之前，识别敏感数据至关重要。了解隐私政策和政府法规对于更好地识别和缓解合规暴露风险非常重要。

# Hadoop 文件格式

在 Hadoop 中，有许多可用的文件格式。用户可以根据用例选择任何格式。每种格式在存储和性能方面都有特殊的功能。让我们详细讨论每种文件格式。

# 文本/CSV 文件

文本和 CSV 文件在 Hadoop 数据处理算法中非常常见。文件中的每一行都被视为新记录。通常，每行以 *n* 字符结束。这些文件不支持列标题。因此，在处理时，总是需要额外的一行代码来删除列标题。CSV 文件通常使用 GZIP 编解码器压缩，因为它们不支持块级压缩；这增加了更多的加工成本。不用说，它们不支持模式演化。

# JSON

JSON 格式在所有现代编程语言中都变得非常流行。这些文件是集合名称/值对。JSON 格式通常用于数据交换应用，它被视为对象、记录、结构或数组。这些文件是文本文件，支持模式演化。从 JSON 文件中添加或删除属性非常容易。与文本/CSV 文件一样，JSON 文件不支持块级压缩。

# 序列文件

序列文件是由二进制键/值对组成的平面文件。它们在 MapReduce([https://wiki.apache.org/hadoop/MapReduce](https://wiki.apache.org/hadoop/MapReduce))中被广泛用作输入/输出格式。它们主要用于一系列 MapReduce 作业中的中间数据存储。序列文件很适合作为小文件的容器。如果 HDFS 有太多的小文件，可以将它们打包成一个序列文件，以提高文件处理效率。序列文件有三种格式:未压缩、记录压缩和块压缩键/值记录。序列文件支持块级压缩，但不支持模式演化。

# 欧罗欧欧欧罗欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧

Avro 是 Hadoop 社区中广泛使用的文件类型。它之所以受欢迎，是因为它有助于模式进化。它包含二进制格式的序列化数据。Avro 文件是可拆分的，支持块压缩。它包含数据和元数据。它使用一个单独的 JSON 文件来定义模式格式。当 Avro 数据存储在一个文件中时，它的模式也随之存储，这样文件以后可以被任何程序处理。如果读取数据的程序期望不同的模式，这很容易解决，因为两种模式都存在。

# 镶木地板

拼花地板以扁平柱状格式存储嵌套数据结构。在存储和性能方面，拼花比任何行级文件格式都更高效。Parquet 以面向列的方式存储二进制数据。在拼花格式中，新的列被添加到结构的末尾。Cloudera 主要支持 Impala 实现的这种格式，但最近变得非常流行。这种格式对于 SQL 查询很好，因为只有选择性的列被读取以降低输入输出成本，所以 SQL 查询从具有许多列的宽表中读取特定的列。

# 妖魔

ORC 文件是优化的记录列文件格式，是 RC 文件的扩展版本。这些非常适合压缩，并且最适合 Hive 读取、写入和处理数据以减少访问时间和存储空间时的 Hive SQL 性能。这些文件不支持真正的模式演化。它们主要由 Hortonworks 支持，不适合 Impala SQL 处理。

# 哪种文件格式更好？

答案是:这取决于你的用例。通常，选择文件格式的标准基于查询读取和查询写入性能。此外，这取决于您使用的 Hadoop 发行版。ORC 文件格式对于使用 Hortonworks 发行版的 Hive 和 Tez 来说是最好的，对于 Cloudera Impala 实现来说，建议使用拼花文件。对于涉及模式演化的用例，Avro 文件是最适合的。如果要使用 Sqoop 从 RDBMS 导入数据，文本/CSV 文件格式是更好的选择。对于存储地图中间输出，序列文件是最终选择。

# 摘要

在本章中，主要目的是了解各种 Hadoop 设计替代方案。说到 Hadoop 集群及其在典型生产环境中部署的最佳实践，我们学到了很多。我们从对 Hadoop 的基本了解开始，然后开始 Hadoop 配置、安装和 HDFS 架构。我们还了解了实现 HDFS 高可用性的各种技术。我们还研究了 Yarn 架构。最后，我们研究了各种文件格式，以及如何根据您的用例选择一种格式。

在下一章中，我们将看到如何将数据摄取到新创建的 Hadoop 集群中。