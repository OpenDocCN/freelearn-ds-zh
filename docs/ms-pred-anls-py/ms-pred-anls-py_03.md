# 第三章. 在噪声中寻找模式 – 聚类和无监督学习

关于数据集的一个自然问题是它是否包含组。例如，如果我们检查由随时间变化的股票价格波动组成的金融市场数据，是否存在下跌和上升模式相似的股票组？同样，对于来自电子商务业务的客户交易集，我们可能会问是否存在由相似购买活动模式区分的用户账户组？通过使用本章中描述的方法识别相关项目的组，我们可以将数据视为一组通用模式，而不仅仅是单个点。这些模式可以帮助在预测建模项目的初期进行高级总结，或者作为持续报告我们正在建模的数据形状的一种方式。产生的分组本身可以成为洞察，或者可以作为我们在后续章节中讨论的模型的起点。例如，数据点所属的组可以成为此观察的特征，为其个别值添加额外的信息。此外，我们可以在这些组内潜在地计算统计量（如均值和标准差），这些统计量可能比单个条目作为模型特征更稳健。

与我们在后续章节中将要讨论的方法相比，分组或*聚类*算法被称为**无监督学习**，这意味着我们没有用于确定算法最佳参数的响应值，例如销售价格或点击率。相反，我们仅使用特征来识别相似数据点，作为次级分析可能会问我们识别的聚类是否在其响应中共享一个共同的模式（从而表明该聚类在寻找与我们感兴趣的输出相关的组时是有用的）。

找到这些组或*聚类*的任务在算法之间有一些共同的成分，这些成分有所不同。一个是数据集中项目之间的距离或相似性的概念，这将使我们能够定量比较它们。第二个是我们希望识别的组数；这可以通过领域知识初始指定，或者通过运行具有不同聚类数量的算法来确定。然后，我们可以通过统计方法，如检查算法确定的组内的数值方差，或通过视觉检查，来识别描述数据集的最佳聚类数量。在本章中，我们将深入探讨：

+   如何对数据进行归一化以用于聚类算法并计算分类数据和数值数据的相似度测量

+   如何通过检查平方误差函数来使用 k 均值聚类识别最佳聚类数量

+   如何使用层次聚类来识别不同尺度的聚类

+   使用亲和传播自动识别数据集中聚类的数量

+   如何使用谱方法聚类具有非线性关系的点

# 相似性和距离度量

对任何新的数据集进行聚类的第一步是决定如何比较项目之间的相似性（或差异性）。有时选择是由我们最感兴趣的相似性类型决定的，在其他情况下，它受到数据集特性的限制。在以下章节中，我们将说明几种用于数值、分类、时间序列和基于集合的数据的距离类型——虽然这个列表不是详尽的，但它应该涵盖了你在商业分析中遇到的大多数常见用例。我们还将介绍在运行聚类算法之前可能需要的归一化方法。

## 数值距离度量

让我们从探索`wine.data`文件中的数据开始。它包含一组描述不同种类葡萄酒特性的化学测量值，以及分配给葡萄酒的质量水平（I-III）（Forina, M.，等. *PARVUS An Extendible Package for Data Exploration*. 分类和相关性（1988））。在 iPython 笔记本中打开文件，并使用以下命令查看前几行：

```py
>>> df = pd.read_csv("wine.data",header=None)
>>> df.head()

```

![数值距离度量](img/B04881_03_01.jpg)

注意到在这个数据集中我们没有列描述，这使得数据难以理解，因为我们不知道特征是什么。我们需要从数据集描述文件`wine.names`中解析列名，该文件除了列名外还包含有关数据集的附加信息。以下代码生成一个正则表达式，该表达式将匹配列名（使用一个模式，其中数字后面跟着一个括号，括号后面跟着列名，正如你在文件中列名列表中看到的那样）：

```py
>>> import re
>>> expr = re.compile('.*[0-9]+\)\s?(\w+).*')

```

然后，我们创建一个数组，其中第一个元素是葡萄酒的类别标签（它是否属于质量类别 I-III）。

```py
>>> header_names = ['Class']

```

迭代文件中的行，我们提取那些与我们的正则表达式匹配的行：

```py
>>> df_header = open("wine.names")
>>> for l in df_header.readlines():
 if len(expr.findall(l.strip()))!=0:
 header_names.append(expr.findall(l.strip())[0])
>>> df_header.close()

```

然后，我们将此列表分配给数据框的列属性，该属性包含列名：

```py
>>> df.columns = header_names

```

现在我们已经附加了列名，我们可以使用`df.describe()`方法查看数据集的摘要：

![数值距离度量](img/B04881_03_02.jpg)

在对数据进行一些清理之后，我们如何根据每行的信息计算基于葡萄酒的相似度测量？一个选项是将每款葡萄酒视为一个由其维度（除了 `Class` 之外的所有列）指定的十三维空间中的点。由于生成的空间有十三维，我们无法直接使用散点图来可视化数据点以查看它们是否接近，但我们可以使用欧几里得距离公式来计算距离，该公式简单地是两点之间直线段的长度。这个长度的公式可以用于点在二维图或更复杂的空间（如本例所示）中，公式如下：

![数值距离度量](img/B04881_03_04.jpg)

在这里，*x* 和 *y* 是数据集的行，*n* 是列数。欧几里得距离公式的一个重要方面是，尺度与其他列差异很大的列可能会主导计算的整体结果。在我们的例子中，描述每款葡萄酒中镁含量的值是描述酒精含量或灰分百分比的特性的 `~100x` 倍。

如果我们要计算这些数据点之间的距离，它将主要取决于镁浓度（因为在这个尺度上的微小差异会压倒性地决定距离计算的值），而不是其任何其他属性。虽然这有时可能是可取的（例如，如果数值最大的列是我们最关心的相似性判断的列），但在大多数应用中，我们不倾向于一个特征而忽略另一个，并希望对所有列给予相同的权重。为了获得这些点之间公平的距离比较，我们需要对列进行归一化，使它们落入相同的数值范围（具有相似的极大值和极小值）。我们可以使用 scikit-learn 中的 `scale()` 函数和以下命令来实现这一点，该命令使用我们之前构建的 `header_names` 数组来访问所有列，但不是类标签（数组的第一个元素）：

```py
>>> from sklearn import preprocessing
>>> df_normalized = pd.DataFrame(preprocessing.scale(df[header_names[1:]]))
>>> df_normalized.columns = header_names[1:]
>>> df_normalized.describe()

```

![数值距离度量](img/B04881_03_03.jpg)

此函数将从每一列的均值中减去每个元素，然后将每个点除以该列的标准差。这种归一化方法将每个列中的数据中心化到均值为 0，方差为 1，对于正态分布的数据，这会导致标准正态分布。此外，请注意，`scale()` 函数返回一个 `numpy array`，这就是为什么我们必须在输出上调用 `dataframe` 来使用 pandas 函数 `describe()`。

现在我们已经对数据进行归一化，我们可以使用以下命令计算行之间的欧几里得距离：

```py
>>> import sklearn.metrics.pairwise as pairwise
>>> distances = pairwise.euclidean_distances(df_normalized)

```

您可以通过以下命令验证该命令生成一个 178 x 178 的方阵（原始数据集的行数）：

```py
>>> distances.shape

```

我们现在已将 178 行 13 列的数据集转换成一个方阵，给出了这些行之间的距离。换句话说，这个矩阵中的第 i 行第 j 列表示我们数据集中第 i 行和第 j 行之间的欧几里得距离。这个“距离矩阵”是我们将在下一节中用于聚类输入的输入。

如果想了解点相对于彼此的分布情况，可以使用给定的距离度量，我们可以使用**多维尺度分析**（**MDS**）——（Borg, Ingwer, 和 Patrick JF Groenen. 现代多维尺度分析：理论与应用. Springer Science & Business Media, 2005；Kruskal, Joseph B. 《非度量多维尺度分析：一种数值方法》。Psychometrika 29.2 (1964): 115-129；Kruskal, Joseph B. 《通过优化非度量假设的拟合优度进行多维尺度分析》。Psychometrika 29.1 (1964): 1-27）来创建可视化。MDS 试图找到一组低维坐标（在这里，我们将使用两个维度），以最好地表示数据集原始、更高维度的点之间的距离（在这里，是我们从 13 个维度计算出的成对欧几里得距离）。MDS 根据应变函数找到最优的 2 维坐标：

![数值距离度量](img/B04881_03_09.jpg)

`D`代表我们计算点之间的距离。通过**奇异值分解**（**SVD**）找到最小化此函数的二维坐标，我们将在第六章“文字与像素 - 处理非结构化数据”中更详细地讨论它。从 MDS 获得坐标后，我们可以使用`wine`类来着色图中的点，从而绘制结果。请注意，坐标本身没有解释（实际上，由于算法中的数值随机性，它们每次运行算法时都可能改变）。相反，我们感兴趣的是点的相对位置：

```py
>>> from sklearn.manifold import MDS
>>> mds_coords = MDS().fit_transform(distances)
>>> pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0,color=df.Class[:],colormap='Reds')

```

![数值距离度量](img/B04881_03_08.jpg)

考虑到我们可以以多种方式计算数据点之间的距离，欧几里得距离在这里是一个好的选择吗？从多维尺度图上直观来看，我们可以看到基于我们用来计算距离的特征，类别之间存在分离，因此从概念上讲，这似乎是一个合理的选项。然而，这个决定也取决于我们试图比较的内容。如果我们对检测具有相似绝对属性的葡萄酒感兴趣，那么这是一个好的度量标准。然而，如果我们对葡萄酒的绝对组成不太感兴趣，而是想知道其变量是否在不同酒精含量的葡萄酒中遵循相似的趋势，那么情况就不同了。在这种情况下，我们不会对值的绝对差异感兴趣，而是对列之间的*相关性*感兴趣。这种比较在时间序列分析中很常见，我们将在下一部分讨论。

## 相关性相似度指标和时间序列

对于时间序列数据，我们通常关心的是系列之间的模式是否在时间上表现出相同的变异，而不是它们在价值上的绝对差异。例如，如果我们比较股票，我们可能希望识别出随着时间的推移价格上下波动模式相似的股票组合。与这种增减模式相比，绝对价格不太重要。让我们通过查看**道琼斯工业平均指数**（**DJIA**）股票价格随时间的变化来举例（Brown, Michael Scott, Michael J. Pelosi, and Henry Dirska. *Dynamic-radius species-conserving genetic algorithm for the financial forecasting of Dow Jones index stocks.* Machine Learning and Data Mining in Pattern Recognition. Springer Berlin Heidelberg, 2013\. 27-41.）。首先，导入数据并检查前几行：

```py
>>> df = pd.read_csv("dow_jones_index/dow_jones_index.data")
>>> df.head()

```

![相关性相似度指标和时间序列](img/B04881_03_10.jpg)

这份数据包含了 30 只股票的每日股价（为期 6 个月）。由于所有数值（价格）都在相同的尺度上，我们不会像处理葡萄酒维度那样对这份数据进行归一化。

我们注意到关于这份数据的两个问题。首先，每周的收盘价（我们将用它来计算相关性）被表示为一个字符串。其次，日期的格式对于绘图来说是不正确的。我们将处理这两个列以解决这个问题，分别将这些列转换为`float`和`datetime`对象，使用以下命令。

为了将收盘价转换为数字，我们应用一个匿名函数，该函数接受所有字符，但不包括美元符号，并将其转换为浮点数。

```py
>>> df.close = df.close.apply( lambda x: float(x[1:]))

```

为了转换日期，我们在日期列的每一行上使用一个匿名函数，将字符串分割成年、月和日元素，并将它们转换为整数以形成一个用于`datetime`对象的元组输入：

```py
>>> import datetime
>>> df.date = df.date.apply( lambda x: datetime.\
 datetime(int(x.split('/')[2]),int(x.split('/')[0]),int(x.split('/')[1])))

```

通过这种转换，我们现在可以创建一个交叉表（正如我们在第二章，*使用 Python 进行探索性数据分析和可视化*中所述），将每周的收盘价作为列，个别股票作为行，使用以下命令：

```py
>>> df_pivot = df.pivot('stock','date','close').reset_index()
>>> df_pivot.head()

```

![相关相似度指标和时间序列](img/B04881_03_11.jpg)

如我们所见，我们只需要计算行之间的相关性的列，从第 2 列开始，因为前两列是索引和股票代码。现在，让我们通过选择每行的数据框的第二列到末尾列，计算成对的相关性距离指标，并使用 MDS 进行可视化，就像之前一样：

```py
>>> import numpy as np
>>> correlations = np.corrcoef(np.float64(np.array(df_pivot)[:,2:]))
>>> mds_coords = MDS().fit_transform(correlations)
>>> pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0)

```

![相关相似度指标和时间序列](img/B04881_03_12.jpg)

需要注意的是，我们在这里计算的皮尔逊相关系数是衡量这些时间序列之间线性相关性的指标。换句话说，它捕捉了相对于另一个价格的趋势的线性增加（或减少），但并不一定能捕捉非线性趋势（如抛物线或 S 形模式）。我们可以通过查看皮尔逊相关系数的公式来看到这一点，该公式如下：

![相关相似度指标和时间序列](img/B04881_03_17.jpg)

这里 μ 和 σ 分别代表序列 *a* 和 *b* 的平均值和标准差。这个值从 1（高度相关）到 -1（反向相关），0 代表没有相关性（例如球形点云）。你可能认出这个方程的分子是 **协方差**，它是衡量两个数据集，*a* 和 *b*，如何同步变化的度量。你可以通过考虑分子在两个数据集中的对应点都高于或低于它们的平均值时达到最大值来理解这一点。然而，这个是否准确地捕捉了数据中的相似性取决于尺度。在数据在最大值和最小值之间以相同间隔分布，并且连续值之间差异大致相同的情况下，它很好地捕捉了这种模式。然而，考虑一个数据呈指数分布的情况，最小值和最大值之间有数量级差异，连续数据点的差异也很大。在这种情况下，皮尔逊相关系数将仅由序列中的最大值在数值上主导，这可能或可能不代表数据的整体相似性。这种数值敏感性也出现在分母中，它代表两个数据集标准差的乘积。相关性的值在两个数据集的变化大致由它们各自变化的乘积解释时达到最大；没有剩余的变异在数据集中没有被它们各自的标准差解释。通过提取这个集合中前两只股票的数据并绘制它们的成对值，我们看到这个线性假设对于比较数据点似乎是有效的：

```py
>>> df_pivot.iloc[0:2].transpose().iloc[2:].plot(kind='scatter',x=0,y=1)

```

![相关性相似度指标和时间序列](img/B04881_03_13.jpg)

除了验证这些股票具有大致的线性相关性之外，此命令还引入了一些在 pandas 中你可能觉得有用的新函数。第一个是 `iloc`，它允许你从数据框中选择索引行。第二个是 `transpose`，它反转行和列。在这里，我们选择了前两行，进行了转置，然后选择了第二个之后的所有行（价格）（因为第一个是索引，第二个是 *股票代码* 符号）。

尽管在这个例子中我们看到了这种趋势，我们可能会想象价格之间可能存在非线性趋势。在这些情况下，可能更好的是测量价格本身的线性相关性，而不是一个股票的高价格是否与另一个股票的高价格一致。换句话说，按价格对市场日进行排名应该相同，即使价格是非线性相关的。我们也可以使用 SciPy 计算这种排名相关性，也称为 Spearman's Rho，以下公式：

![相关性相似度指标和时间序列](img/B04881_03_20.jpg)

### 注意

注意，这个公式假设排名是不同的（没有平局）；在出现平局的情况下，我们可以使用数据集的排名而不是原始值来计算皮尔逊相关系数。

其中 *n* 是两个集合 *a* 和 *b* 中每个集合的数据点数量，*d* 是每对数据点 *ai* 和 *bi* 之间排名的差异。因为我们只比较数据的排名，而不是它们的实际值，所以这个度量可以捕捉到两个数据集之间的变化，即使它们的数值在广泛的范围内变化。让我们看看使用斯皮尔曼相关系数指标绘制结果是否会在从 MDS 计算的股票的成对距离中产生任何差异，使用以下命令：

```py
>>> import scipy.stats
>>> correlations2 = scipy.stats.spearmanr(np.float64(np.array(df_pivot)[:,1:]))
>>> mds_coords = MDS().fit_transform(correlations2.correlation)
>>> pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0)

```

![相关相似度指标和时间序列](img/B04881_03_14.jpg)

基于坐标轴 *x* 和 *y* 的斯皮尔曼相关系数距离似乎比皮尔逊距离更接近，这表明从排名相关的角度来看，时间序列更相似。

尽管它们在关于两个数据集如何数值分布的假设上有所不同，皮尔逊和斯皮尔曼相关系数都要求两个集合长度相同。这通常是一个合理的假设，并且在我们考虑的大多数例子中都是正确的。然而，对于我们要比较长度不等的时间序列的情况，我们可以使用**动态时间规整**（**DTW**）。从概念上讲，DTW 的想法是通过允许我们在任一数据集中打开间隙，使其与第二个数据集大小相同，来将一个时间序列“扭曲”以与第二个时间序列对齐。算法需要解决的是两个系列中最相似点的位置，以便可以在适当的位置放置间隙。在最简单的实现中，DTW 由以下步骤组成（见以下图表）：

![相关相似度指标和时间序列](img/B04881_03_15.jpg)

1.  对于长度为 *n* 的数据集 *a* 和长度为 *m* 的数据集 *b*，构建一个大小为 *n* 乘 *m* 的矩阵。

1.  将这个矩阵的顶部行和最左边的列都设置为无穷大（如图所示）。

1.  对于集合 *a* 中的每个点 *i* 和集合 *b* 中的每个点 *j*，使用成本函数比较它们的相似性。将这个成本函数与元素 (*i-1, j-1*)、(*i-1, j*) 和 (*j-1, i*) 中的最小值相加——即从矩阵中向上和向左移动、向左或向上移动）。这些在概念上代表了在其中一个系列中打开间隙的成本，与在两个系列中对齐相同元素的成本（如图中上方中间所示）。

1.  在步骤 2 结束时，我们将追踪到对齐两个系列的最小成本路径，DTW 距离将由矩阵的底部角落 (*n.m*) 表示（如图所示）。

此算法的一个负面方面是第 3 步涉及到为序列*a*和*b*的每个元素计算一个值。对于大型时间序列或大型数据集，这可能计算上难以承受。虽然关于算法改进的全面讨论超出了我们当前示例的范围，但我们建议感兴趣的读者参考 FastDTW（我们将在示例中使用）和 SparseDTW 作为改进的例子，这些改进可以通过更少的计算进行评估（Al-Naymat, Ghazi, Sanjay Chawla, 和 Javid Taheri. *Sparsedtw: 一种加快动态时间规整的新方法*。第八届澳大利亚和新西兰数据挖掘会议-第 101 卷。澳大利亚计算机协会，2009 年。Salvador, Stan, 和 Philip Chan. *向线性时间和空间中的准确动态时间规整迈进*。智能数据分析 11.5（2007 年）：561-580.）。

我们可以使用 FastDTW 算法来比较股票数据，并再次使用 MDS 绘制结果。首先，我们将成对比较每一对股票，并将它们的 DTW 距离记录在一个矩阵中：

```py
>>> from fastdtw import fastdtw
>>> dtw_matrix = np.zeros(shape=(df_pivot.shape[0],df_pivot.shape[0]))
…  for i in np.arange(0,df_pivot.shape[0]):
…     for j in np.arange(i+1,df_pivot.shape[0]):
 …      dtw_matrix[i,j] = fastdtw(df_pivot.iloc[i,2:],df_pivot.iloc[j,2:])[0]

```

### 注意

此功能位于 fastdtw 库中，您可以使用 pip 或`easy_install`进行安装。

为了计算效率（因为`i`和`j`之间的距离等于股票`j`和`i`之间的距离），我们只计算这个矩阵的上三角。然后我们将转置（例如，下三角）添加到这个结果中，以获得完整的距离矩阵。

```py
>>> dtw_matrix+=dtw_matrix.transpose()

```

最后，我们可以再次使用 MDS 来绘制结果：

```py
>>> mds_coords = MDS().fit_transform(dtw_matrix)
>>> pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0) 

```

![相关相似度指标和时间序列](img/B04881_03_16.jpg)

与 Pearson 相关和秩相关的坐标分布相比，DTW 距离似乎跨越了更广泛的范围，捕捉到股票价格时间序列之间更细微的差异。

现在我们已经研究了数值和时间序列数据，作为一个最后的例子，让我们来考察计算分类数据集的相似度测量。

## 分类数据的相似度指标

文本代表一类分类数据：例如，我们可能使用一个向量来表示提交给学术会议的一组论文中给定关键词的存在或不存在，正如我们的示例数据集（Moran, Kelly H., Byron C. Wallace, 和 Carla E. Brodley. *通过社区来源的约束进行聚类以发现更好的 AAAI 关键词*。第二十八届 AAAI 人工智能会议。2014 年。）所示。如果我们打开数据，我们会看到关键词被表示为一列中的字符串，我们需要将其转换为二进制向量：

```py
>>> df2 = pd.read_csv("Papers.csv",sep=",")
>>> df2.head()

```

![分类数据的相似度指标](img/B04881_03_18.jpg)

虽然第六章，“文字和像素 – 处理非结构化数据”，我们将检查特殊函数来完成从文本到向量的转换，为了说明目的，我们现在将亲自编写代码。我们需要收集所有独特的关键词，并为每个关键词分配一个唯一的索引，为每个关键词生成一个新的列名`'keyword_n'`：

```py
>>> keywords_mapping = {}
>>> keyword_index = 0

>>> for k in df2.keywords:
…    k = k.split('\n')
…    for kw in k:
…        if keywords_mapping.get(kw,None) is None:
…           keywords_mapping[kw]='keyword_'+str(keyword_index)
…           keyword_index+=1

```

然后，我们使用这个关键字到列名的映射生成一组新的列，在每个行中，如果该文章的关键词中包含该关键字，则在该行设置 1：

```py
>>>for (k,v) in keywords_mapping.items():
…        df2[v] = df2.keywords.map( lambda x: 1 if k in x.split('\n') else 0 ) Image_B04881_03_18.png

```

这些列将被附加到现有列的右侧，我们可以使用`iloc`命令选择这些二进制指示符，就像之前一样：

```py
>>> df2.head().iloc[:,6:]

```

![分类数据的相似性度量](img/B04881_03_19.jpg)

在这种情况下，可以计算文章之间的欧几里得距离，但由于每个坐标只能是 0 或 1，它并不能提供我们想要的连续距离分布（由于只有有限种加法和减法 1 和 0 的方式，我们将会得到很多重复值）。

我们可以使用哪些类型的相似性度量来代替？一个是 Jaccard 系数：

![分类数据的相似性度量](img/B04881_03_32.jpg)

这是相交项的数量（在我们的例子中，*a*和*b*都设置为*1*的位置）除以并集（*a*或*b*中任一设置为 1 的总位置数）。然而，如果文章的关键词数量差异很大，这个度量可能会存在偏差，因为更大的词集有更大的概率与另一篇文章相似。如果我们担心这种偏差，可以使用余弦相似度，它测量向量之间的角度，并且对每个向量的元素数量敏感：

![分类数据的相似性度量](img/B04881_03_34.jpg)

其中：

![分类数据的相似性度量](img/B04881_03_35.jpg)

我们还可以使用汉明距离（Hamming, Richard W. *Error detecting and error correcting codes*. Bell System technical journal 29.2 (1950): 147-160.），它简单地计算两个集合的元素是否相同：

![分类数据的相似性度量](img/B04881_03_37.jpg)

显然，如果我们主要寻找匹配和不匹配，这个度量将是最优的。它也像 Jaccard 系数一样，对每个集合中的项目数量敏感，因为简单地增加元素数量会增加距离的上限。与汉明距离相似的是曼哈顿距离，它不需要数据是二进制的：

![分类数据的相似性度量](img/B04881_03_38.jpg)

以曼哈顿距离为例，我们可以再次使用 MDS 来绘制以下命令中关键词空间中文档的排列：

```py
>>> distances = pairwise.pairwise_distances(np.float64(np.array(df2)[:,6:]),metric='manhattan')
>>> mds_coords = MDS().fit_transform(distances)
pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0)

```

![分类数据的相似性度量](img/B04881_03_23.jpg)

我们看到许多论文组，它们提出通过简单比较常见关键词可以提供一种区分文章的方法。

下面的图表总结了我们讨论的不同距离方法，以及在选择特定问题时选择一种方法而不是另一种方法的决策过程。虽然它并不详尽，但我们希望它能为你自己的聚类应用提供一个起点。

![分类数据的相似性度量](img/B04881_03_24.jpg)

### 小贴士

**旁白：归一化分类数据**

正如你可能已经注意到的，我们不会像使用`scale()`函数处理数值数据那样对分类数据进行归一化。原因有两点。首先，对于分类数据，我们通常处理的是范围在`[0,1]`之间的数据，因此数据集中某一列包含的极端较大值压倒距离度量的问题最小化了。其次，`scale()`函数的概念是数据列中的数据是有偏的，我们通过减去均值来消除这种偏差。对于分类数据，'均值'的含义不太明确，因为数据只能取 0 或 1 的值，而均值可能在两者之间（例如，0.25）。减去这个值没有意义，因为它会使数据元素不再是二进制指示器。

**旁白：混合距离度量**

在本章迄今为止考虑的例子中，我们处理的数据可能被描述为数值型、时间序列型或分类型。然而，我们很容易找到这种情况并不成立的例子。例如，我们可能有一个包含随时间变化的股票价值数据集，其中还包含有关股票所属行业和公司规模及收入的分类信息。在这种情况下，选择一个能够充分处理所有这些特征的单一距离度量将变得困难。相反，我们可以为这三组特征（时间序列、分类和数值）中的每一组计算不同的距离度量，并将它们混合（例如，通过取平均值、总和或乘积）。由于这些距离可能覆盖非常不同的数值范围，我们可能需要对这些距离进行归一化，例如使用上面讨论的`scale()`函数将每个距离度量转换为具有均值 0、标准差 1 的分布，然后再将它们组合起来。

现在我们有了一些比较数据集中项目相似性的方法，让我们开始实现一些聚类流程。

## K-means 聚类

K-means 聚类是经典的划分聚类算法。其思想相对简单：标题中的**k**代表我们希望识别的簇的数量，在运行算法之前我们需要决定这个数量，而**means**表示算法试图将数据点分配到它们最接近簇平均值的簇中。因此，一个给定的数据点会在 k 个不同的均值中选择，以便被分配到最合适的簇中。该算法最简单版本的基本步骤如下（MacKay, David. *一个推理任务示例：聚类*. 信息理论、推理和学习算法（2003 年）：284-292）：

1.  选择一个期望的组数 *k*。

    分配 k 个簇中心；这些可以简单地是数据集中的 k 个随机点，这被称为 Forgy 方法（Hamerly, Greg, 和 Charles Elkan. *替代 k-means 算法以找到更好的聚类*. 第十一届国际信息与知识管理会议论文集. ACM, 2002 年）。或者，我们可以将一个随机簇分配给每个数据点，并计算分配给同一簇的 k 个中心的平均数据点，这种方法称为随机划分（Hamerly, Greg, 和 Charles Elkan. *替代 k-means 算法以找到更好的聚类*. 第十一届国际信息与知识管理会议论文集. ACM, 2002 年）。也可能有更复杂的方法，我们很快就会看到。

1.  根据某些相似度度量（例如平方欧几里得距离），将任何剩余的数据点分配到最近的簇中。

1.  通过计算分配给每个 *k* 个组的点的平均值来重新计算每个组的中心。请注意，在此点，中心可能不再代表单个数据点的位置，而是该组中所有点的加权质心。

1.  重复 3 和 4，直到没有点改变簇分配或达到最大迭代次数。

    ### 小贴士

    **K-means++**

    在上述步骤 2 中算法的初始化中，存在两个潜在问题。如果我们简单地选择随机点作为聚类中心，它们可能不会在数据中优化分布（尤其是如果聚类的大小不等）。k 个点可能实际上不会最终落在数据中的 k 个聚类中（例如，如图下顶部中间面板所示，多个随机点可能位于数据集最大的聚类中），这意味着算法可能不会收敛到“正确”的解，或者可能需要很长时间才能做到。同样，随机划分方法倾向于将所有中心放置在数据点最大质量附近（见下图中顶部右侧面板），因为任何随机点集都将被最大的聚类主导。为了改进参数的初始选择，我们可以使用 2007 年提出的 k++初始化（Arthur, David, 和 Sergei Vassilvitskii. "k-means++: The advantages of careful seeding." Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 2007.）。在这个算法中，我们随机选择一个初始数据点作为第一个聚类的中心。然后我们计算每个其他数据点到所选数据点的平方距离，并选择下一个聚类中心，其概率与这个距离成比例。随后，我们通过计算给定数据点到先前选定的中心的平方距离来选择剩余的聚类。因此，这种初始化将以更高的概率选择远离任何先前选择点的点，并在空间中更均匀地分布初始中心。这个算法是 scikit-learn 中默认使用的算法。

![K-means 聚类](img/B04881_03_25.jpg)

Kmeans++聚类。 (顶部，左侧)：具有三个大小不等聚类的示例数据。 (顶部，中间)：随机选择聚类中心倾向于最大的基础聚类中的点。 (顶部，右侧)：随机划分导致所有三个随机聚类的质心接近图表底部。 (底部面板)：Kmeans++导致在数据集中均匀选择三个聚类中心。

让我们思考一下为什么这有效；即使我们以随机分组中心开始，一旦我们将点分配到这些组，中心就会被拉向我们的数据集中观察的平均位置。更新后的中心更接近这个平均值。经过多次迭代后，每个组的中心将由随机选择起始点附近的平均数据点的位置主导。如果起始点选择得不好，它将被拉向这个平均值，而可能被错误分配到这个组的点将逐渐被重新分配。在这个过程中，通常最小化的整体值通常是平方误差之和（当我们使用欧几里得距离度量时），由以下公式给出：

![K-means 聚类](img/B04881_03_44.jpg)

其中 D 是欧几里得距离，c 是分配给点的簇的中心。这个值有时也被称为惯性。如果我们稍微思考一下，我们可以看到这会产生这样的效果：该算法最适合由圆形（或更高维度的球体）组成的数据；当点在球形云中均匀地远离簇时，簇的整体 SSE 最小化。相比之下，非均匀形状（如椭圆）往往会具有更高的 SSE 值，算法将通过将数据分成两个簇来优化，即使从视觉上看，它们似乎可以用一个簇很好地表示。这一事实强化了为什么标准化通常是有益的（因为 0 均值，1 标准差标准化试图近似所有维度的正态分布形状，从而形成数据圆或球体），以及在判断簇的质量时，数据可视化在除了数值统计之外的重要作用。

考虑到这一最小化标准对步骤 3 的影响也很重要。SSE 等于簇点与其质心之间的欧几里得距离的平方和。因此，使用平方欧几里得距离作为比较的度量，我们保证了簇分配也在优化最小化标准。我们可以使用其他距离度量，但这样就不能保证这一点。如果我们使用曼哈顿或汉明距离，我们可以将最小化标准改为到簇中心的距离之和，我们称之为 k-中位数，因为优化这个统计量的值是簇的中位数（Jain, Anil K. 和 Richard C. Dubes. 数据聚类算法。Prentice-Hall, Inc.，1988 年）。或者，我们可以使用任意距离度量，例如 k-medoids 算法（见下文）。

显然，这种方法将对我们初始选择的组中心的选择敏感，因此我们通常会多次运行算法，并使用最佳结果。

让我们看看一个例子：在笔记本中输入以下命令以读取样本数据集。

```py
>>> df = pd.read_csv('kmeans.txt',sep='\t')
>>> df.plot(kind='scatter',x='x_coord',y='y_coord')

```

![K-means 聚类](img/B04881_03_26.jpg)

通过视觉检查，这个数据集明显包含多个簇。让我们尝试使用*k=5*进行聚类。

```py
>>> from sklearn.cluster import KMeans
>>> kmeans_clusters = KMeans(5).fit_predict(X=np.array(df)[:,1:])
>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=kmeans_clusters)

```

![K-means 聚类](img/B04881_03_27.jpg)

你会注意到我们使用切片操作符 '`[]`' 来索引从输入数据框创建的 `numpy` 数组，并选择所有行以及第一个之后的列（第一个包含标签，因此我们不需要它，因为它不是用于聚类的数据的一部分）。我们使用在 scikit-learn 和 PySpark 中许多算法都会熟悉的模式来调用 KMeans 模型：我们用参数（在这里是 5，这是簇的数量）创建模型对象（KMeans），并调用 'fit_predict' 来校准模型参数并将模型应用于输入数据。在这里，应用模型会生成簇中心，而在我们将在 第四章、*通过模型连接点 – 回归方法* 和 第五章、*将数据放在合适的位置 – 分类方法和分析* 中讨论的回归或分类模型中，'predict' 将分别对每个数据点产生估计的连续响应或类标签。我们也可以简单地调用 KMeans 的 `fit` 方法，这将简单地返回一个描述簇中心和拟合模型产生的统计信息的对象，例如我们上面描述的惯性度量。

这个簇的数量适合数据吗？我们可以通过使用几个 `k` 的值进行聚类并绘制每个的惯性来探索这个问题。在 Python 中，我们可以使用以下命令。

```py
>>> inertias = []
>>> ks = np.arange(1,20)
>>> for k in ks:
 …      inertias.append(KMeans(k).fit(X=np.array(df)[:,1:]).inertia_)
>>> results = pd.DataFrame({"num_clusters": ks, "sum_distance": inertias})

```

回想一下，惯性被定义为簇中点到其分配的簇中心的平方距离之和，这是我们试图在 k-means 中优化的目标。通过在每个簇编号 *k* 处可视化这个惯性值，我们可以对最适合数据的簇数量有一个感觉：

```py
>>> results.plot(kind='scatter', x='num_clusters', y='sum_distance')

```

![K-means 聚类](img/B04881_03_28.jpg)

我们注意到在五个簇的标记处有一个 *肘部*，幸运的是，这是我们最初选择的值。这个肘部表明，在五个簇之后，我们增加更多簇时惯性并没有显著减少，这表明在 k=5 时我们已经捕捉到了数据中的重要组结构。

这个练习也说明了某些问题：正如您从图中可以看到的，我们的某些聚类可能是由看似重叠的段形成的，形成一个十字形状。这是一个单独的聚类还是两个混合的聚类？遗憾的是，如果没有在我们的聚类模型中指定聚类应遵循的形状，结果将完全由距离度量驱动，而不是您可能通过视觉注意到的模式。这强调了可视化结果并使用领域专家检查它们以判断获得的聚类是否有意义的重要性。在没有领域专家的情况下，我们还可以查看获得的聚类是否包含所有用已知分配标记的点——如果一个高比例的聚类富含单一标签，这表明聚类不仅概念质量良好，而且最小化了我们的距离度量。

我们还可以尝试使用一种方法来自动计算数据集的最佳聚类数量。

# 亲和传播 – 自动选择聚类数量

k-means 算法的一个弱点是我们需要事先定义预期在数据中找到的聚类数量。当我们不确定合适的选项时，我们可能需要运行多次迭代来找到一个合理的值。相比之下，亲和传播算法（Frey, Brendan J. 和 Delbert Dueck. *通过数据点间传递消息进行聚类*. science 315.5814 (2007): 972-976）能够从数据集中自动找到聚类数量。该算法以相似性矩阵（S）作为输入（例如，可能是欧几里得距离的逆矩阵——因此，在 S 中，更接近的点具有更大的值），并在初始化一个包含所有零值的责任和可用性矩阵后执行以下步骤。它计算一个数据点 k 作为另一个数据点 i 的聚类中心的责任。这通过两个数据点之间的相似性数值表示。由于所有可用性都从零开始，在第一轮中，我们只需从 i 中减去任何其他点（k'）的最高相似性。因此，当点 k 比任何其他点与 i 更相似时，就会产生一个高的责任分数。

![亲和传播 – 自动选择聚类数量](img/B04881_03_52.jpg)

其中*i*是我们试图找到的聚类中心的数据点，*k*是可能被分配给数据点*i*的潜在聚类中心，s 是它们的相似性，a 是下面描述的'可用性'。在下一步中，算法计算数据点*k*作为数据点*i*的聚类中心时的可用性，这表示 k 作为*i*的聚类中心是否合适，通过判断它是否也是其他点的中心。被许多其他点选择为具有高责任度的点具有高可用性，如公式所示：

![亲和传播 – 自动选择簇数量](img/B04881_03_53.jpg)

其中*r*是上面给出的责任。如果*i=k*，则此公式为：

![亲和传播 – 自动选择簇数量](img/B04881_03_54.jpg)

这些步骤有时被称为**消息传递**，因为它们代表了两个数据点之间关于一个数据点作为另一个数据点簇中心的相对概率的信息交换。查看步骤 1 和 2，你可以看到随着算法的进行，许多数据点的责任将降低到负数（因为我们不仅减去了其他数据点的最高相似度，还减去了这些点的可用性得分，只留下少数几个正数来确定簇中心。在算法结束时（一旦责任和可用性不再以可观的数值变化），每个数据点都将指向另一个数据点作为簇中心，这意味着簇的数量会自动从数据中确定。这种方法的优势在于我们不需要事先知道簇的数量，但与其他方法相比，其扩展性并不好，因为在最简单的实现中，我们需要一个 n-by-n 的相似度矩阵作为输入。如果我们将此算法应用于之前的数据集，我们会看到它检测到的簇数量远多于我们的肘图所暗示的，因为运行以下命令给出簇数量为 309。

```py
>>> affinity_p_clusters = sklearn.cluster.AffinityPropagation().fit_predict(X=np.array(df)[:,1:]) 
>>> len(np.unique(affinity_p_clusters))

```

然而，如果我们查看每个簇中数据点的数量直方图，使用以下命令：

```py
>>> pd.DataFrame(affinity_p_clusters).plot(kind='hist',bins=np.unique(affinity_p_clusters))

```

我们可以看到只有少数簇很大，而许多点被标识为属于它们自己的组：

![亲和传播 – 自动选择簇数量](img/B04881_03_29.jpg)

在 K-Means 失败的地方：聚类同心圆

到目前为止，我们的数据已经很好地使用 k-means 或亲和传播等变体进行了聚类。这个算法可能在哪些情况下表现不佳？让我们通过加载我们的第二个示例数据集并使用以下命令进行绘图来举一个例子：

```py
>>> df = pd.read_csv("kmeans2.txt",sep="\t")
>>> df.plot(x='x_coord',y='y_coord',kind='scatter')

```

![亲和传播 – 自动选择簇数量](img/B04881_03_30.jpg)

仅凭肉眼，你可以清楚地看到有两个组：两个嵌套在一起的圆。然而，如果我们尝试对此数据进行 k-means 聚类，我们会得到一个不满意的结果，正如你从以下命令的运行和结果图中可以看到：

```py
>>> kmeans_clusters = KMeans(2).fit_predict(X=np.array(df)[:,1:])
>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=kmeans_clusters)

```

![亲和传播 – 自动选择簇数量](img/B04881_03_31.jpg)

在这种情况下，算法无法识别数据中的两个自然聚类——因为数据中心环在许多点上的距离与外环相同，随机分配的聚类中心（更有可能落在外环的某个地方）对于最近的聚类是一个数学上合理的选择。这个例子表明，在某些情况下，我们可能需要改变我们的策略，并使用概念上不同的算法。也许我们的目标——平方误差（惯性）是不正确的，例如。在这种情况下，我们可能会尝试使用 k-medoids。

# k-medoids

正如我们之前所描述的，k-means（中位数）算法最适合特定的距离度量，分别是平方欧几里得距离和曼哈顿距离，因为这些距离度量等同于这些算法试图最小化的统计量的最优值（如总平方距离或总距离）。在可能具有其他距离度量（如相关性）的情况下，我们也可以使用 k-medoid 方法（Theodoridis, Sergios, and Konstantinos Koutroumbas. *Pattern recognition.* (2003).），它包括以下步骤：

1.  选择*k*个初始点作为初始聚类中心。

1.  通过任何距离度量计算每个数据点的最近聚类中心，并将其分配给该聚类。

1.  对于每个点和每个聚类中心，将聚类中心与点交换，并计算使用此交换在整个聚类成员中到聚类中心的总体距离的减少。如果它没有改进，则撤销。对所有点重复步骤 3。

这显然不是一个穷举搜索（因为我们没有重复步骤 1），但它的优点是，最优性标准不是一个特定的优化函数，而是通过灵活的距离度量来提高聚类的紧凑性。k-medoids 能否改善我们同心圆的聚类？让我们尝试使用以下命令运行并绘制结果：

```py
>>> from pyclust import KMedoids
>>> kmedoids_clusters = KMedoids(2).fit_predict(np.array(df)[:,1:])
>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=kmedoids_clusters)

```

### 注意

注意，k-medoids 不包括在 sci-kit learn 中，所以您需要使用`easy_install`或`pip`安装 pyclust 库。

![k-medoids](img/B04881_03_33.jpg)

与 k-means 相比，改进不大，所以我们可能需要完全改变我们的聚类算法。也许我们不应该在单阶段生成数据点之间的相似性，而应该检查相似性和聚类的层次度量，这正是我们将要检查的层次聚类算法的目标。

# 层次聚类

与将数据集划分为单个组的算法（如 k-means）不同，**聚合**或**层次**聚类技术首先将每个数据点视为其自己的聚类，并从底部向上将它们合并成更大的组（Maimon, Oded, 和 Lior Rokach 编著 *数据挖掘与知识发现手册*。第 2 卷。纽约：Springer，2005）。这一想法的经典应用是在进化中的系统发育树，其中共同的祖先连接了单个生物体。确实，这些方法将数据组织成树状图，称为**树状图**，以可视化数据如何按顺序合并成更大的组。

聚合算法的基本步骤如下（如图所示）：

1.  从每个点在其自己的聚类开始。

1.  使用距离度量比较每对数据点。这可以是上述讨论的任何方法。

1.  使用连接标准合并数据点（在第一阶段）或聚类（在后续阶段），其中连接由如下函数表示：

    +   两组点之间的最大距离（也称为完全连接），或最小距离（也称为单连接）。

    +   两组点之间的平均距离，也称为**未加权配对组平均法**（**UPGMA**）。每个组中的点也可以加权，以给出加权平均值，或 WUPGMA。

    +   中心点（质量中心）之间的差异，或 UPGMC。

    +   两组点之间的欧几里得距离的平方，或 Ward 的方法（Ward Jr, Joe H. *通过优化目标函数进行层次分组*。*美国统计协会杂志* 58.301 (1963): 236-244）。

    +   重复步骤 2-3，直到只剩下一个包含所有数据点的单一聚类。注意，在第一轮之后，聚类的第一阶段成为一个新的点，与其他所有聚类进行比较，并且随着算法的进行，每个阶段的聚类形成会越来越大。在这个过程中，我们将构建一个树状图，因为我们按顺序将前一步骤中的聚类合并在一起。

    ![聚合聚类](img/B04881_03_40.jpg)

    聚合聚类：从上到下，从数据集（左）通过顺序合并最近的点构建树形结构（右）的示例。

注意，我们也可以反向运行此过程，从一个初始数据集开始，将其拆分为单个点，这也会构建一个树状图。在两种情况下，我们都可以通过选择树的截止深度并将点分配给它们在截止深度以下分配的最大聚类来找到多个分辨率的聚类。这个深度通常使用步骤 3 中给出的连接分数来计算，使我们能够在概念上选择一个合适的组间距离来考虑聚类（随着我们向上移动树，要么相对接近要么相对远离）。

## 聚类分析中的不足

层次聚类算法与 k-means 算法有许多相同的成分；我们选择一个聚类数（这将决定我们如何切割聚类生成的树——在最极端的情况下，所有点都成为单个聚类的成员）和一个相似性度量。我们还需要为第 3 步选择一个**链接度量**，它决定了合并树中单个分支的规则。层次聚类能否在 k-means 失败的地方成功？尝试这种方法在圆形数据上似乎不然，如下面的命令结果所示：

```py
>>> from sklearn.cluster import AgglomerativeClustering
>>> agglomerative_clusters = AgglomerativeClustering(2,linkage='ward').fit_predict(X=np.array(df)[:,1:])
>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=agglomerative_clusters)

```

![聚类失败的地方](img/B04881_03_41.jpg)

为了正确地将内圈和外圈分组，我们可以尝试通过连通性来修改我们对相似性的理解，连通性是一个从图分析中借用的概念，其中一组节点通过边连接，连通性指的是两个节点之间是否共享一条边。在这里，我们通过阈值化可以相互视为相似点的点的数量，而不是测量每对点之间的连续距离度量，从而在点对之间构建一个图。这可能会减少我们在同心圆数据上的困难，因为如果我们设置一个非常小的值（比如说 10 个附近的点），从中心到外环的均匀距离就不再成问题，因为中心点总是彼此比外围点更近。为了构建这种基于连通性的相似性，我们可以取一个距离矩阵，例如我们之前已经计算过的那些，并对其进行阈值化，以某个我们认为点之间相连的相似性值，得到一个由 0 和 1 组成的二进制矩阵。这种表示图中节点之间边存在与否的矩阵也被称为**邻接矩阵**。我们可以通过检查成对相似度分数的分布或基于先验知识来选择这个值。然后，我们可以将这个矩阵作为参数提供给我们的层次聚类程序，提供在比较数据点时要考虑的点邻域，这为聚类提供了初始结构。我们可以看到，在运行以下命令后，这会对算法的结果产生巨大影响。注意，当我们生成邻接矩阵 L 时，我们可能会得到一个非对称矩阵，因为我们为数据中的每个成员阈值化了最相似的十个点。这可能导致两个点不是彼此最近的，导致在邻接矩阵的上三角或下三角中只表示一个边。为了生成聚类算法的对称输入，我们取矩阵 L 及其转置的平均值，这实际上在两点之间添加了双向边。

```py
>>> from sklearn.cluster import AgglomerativeClustering
>>> from sklearn.neighbors import kneighbors_graph
>>> L = kneighbors_graph(np.array(df)[:,1:], n_neighbors=10, include_self=False)
>>> L = 0.5 * (L + L.T)
>>> agglomerative_clusters = AgglomerativeClustering(n_clusters=2,connectivity=L,linkage='average').fit_predict(X=np.array(df)[:,1:])
>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=agglomerative_clusters)

```

现在，正如你所看到的，这个算法可以正确地识别和分离两个聚类：

有趣的是，构建这个邻域图并将其划分为子图（将整个图划分为一组节点和边，这些节点和边主要相互连接，而不是与其他网络元素连接），这与在转换后的距离矩阵上执行 k-means 聚类等价，这种方法被称为谱聚类（Von Luxburg, Ulrike. *谱聚类的教程.* 统计与计算 17.4 (2007): 395-416）。这里的转换是将我们之前计算的欧几里得距离 D 转换为核分数——由以下高斯核给出：

![聚合聚类失败的地方](img/B04881_03_70.jpg)

在每对点 i 和 j 之间，用带宽 γ 代替我们在构建邻域之前所做的硬阈值。使用从所有点 i 和 j 计算出的成对核矩阵 K，然后我们可以构建一个图的拉普拉斯矩阵，它由以下公式给出：

![聚合聚类失败的地方](img/B04881_03_71.jpg)

在这里，*I* 是单位矩阵（对角线上的元素为 1，其他地方为 0），而 *D* 是对角矩阵，其元素为：

![聚合聚类失败的地方](img/B04881_03_72.jpg)

为每个点 *i* 提供邻居的数量。本质上，通过计算 L，我们现在将数据集表示为一系列通过边（这个矩阵的元素）连接的节点（点），这些边的值已经被归一化，使得每个节点的所有边的总和为 1。由于高斯核分数是连续的，在这个归一化过程中，将给定点与所有其他点之间的成对距离划分为一个概率分布，其中距离（边）的总和为 1。

你可能还记得，从线性代数中，矩阵 A 的特征向量 v 是这样的向量，如果我们用矩阵乘以特征向量 v，我们会得到与用常数 λ 乘以向量相同的结果：![聚合聚类失败的地方](img/B04881_03_73.jpg)。因此，这里的矩阵代表对向量的某种操作。例如，单位矩阵给出特征值 1，因为用单位矩阵乘以 v 得到的是 v 本身。我们也可以有如下矩阵：

![聚合聚类失败的地方](img/B04881_03_74.jpg)

它将与之相乘的向量的值加倍，表明矩阵在向量上执行了“拉伸”操作。从这个角度来看，较大的特征值对应于向量更大的拉伸，而特征向量给出了拉伸发生的方向。这很有用，因为它给出了矩阵操作作用的主要轴。在我们的例子中，如果我们取具有最大特征值的两个特征向量（本质上，矩阵表示向量最大变换的方向），我们正在提取矩阵中两个最大的变化轴。当我们讨论第六章中的*主成分*时，我们将更详细地回到这个概念，*文字和像素 - 处理非结构化数据*，但简而言之，如果我们在这两个特征向量上运行`run-kmeans`（这种方法被称为**谱聚类**，因为聚类的矩阵的特征值被称为矩阵的谱），我们得到的结果与之前使用邻域的聚合聚类方法非常相似，正如我们从以下命令的执行中可以看到：

```py
>>> spectral_clusters = sklearn.cluster.SpectralClustering(2).fit_predict(np.array(df)[:,1:])
>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=spectral_clusters)

```

![聚类失败的地方](img/B04881_03_42.jpg)

我们可以成功地捕捉到这个非线性分离边界，因为我们已经将点表示在成对距离最大的空间中，这是数据中内圈和外圈之间的差异：

上述示例应该已经给了你许多可以用来解决聚类问题的方法，并且作为一个经验法则指南，以下图表说明了选择它们之间的决策过程：

![聚类失败的地方](img/B04881_03_46.jpg)

在我们探索聚类的最后部分，让我们看看一个利用 Spark Streaming 和 k-means 的示例应用，这将允许我们随着接收到的数据逐步更新我们的聚类。

# Spark 中的流式聚类

到目前为止，我们主要展示了用于即席探索性分析的示例。在构建分析应用时，我们需要开始将这些内容放入一个更健壮的框架中。作为一个例子，我们将演示使用 PySpark 的流式聚类管道的使用。这个应用有可能扩展到非常大的数据集，我们将以这种方式组合分析的部分，使其在数据格式错误的情况下具有容错性。

由于我们将在接下来的章节中使用与 PySpark 类似的示例，让我们回顾一下此类应用中我们需要的关键组成部分，其中一些我们在第二章中已经看到，*Python 中的探索性数据分析和可视化*。我们将在本书中创建的大多数 PySpark 作业都由以下步骤组成：

1.  构建一个 Spark 上下文。该上下文包含有关应用程序名称以及内存和任务数量等参数的信息。

1.  Spark 上下文可以用来构建二级上下文对象，例如我们将在此示例中使用的流上下文。此上下文对象包含特定于特定类型任务的参数，例如流数据集，并继承我们在基本 Spark 上下文中先前初始化的所有信息。

1.  构建一个数据集，在 Spark 中表示为**弹性分布式数据集**（**RDD**）。从程序的角度来看，我们可以像操作例如 pandas 数据框一样操作这个 RDD，但实际上在分析过程中它可能被并行化到多台机器上。我们可能在从源文件读取后或从 Hadoop 等并行文件系统中读取数据后并行化数据。理想情况下，我们不希望因为一行数据错误而导致整个作业失败，因此我们希望在错误处理机制中放置一个将提醒我们解析行失败而不会阻塞整个作业的机制。

1.  我们经常需要将我们的输入数据集转换为 RDD 的子类，称为**标记 RDD**。标记 RDD 包含一个标签（例如，本章中我们研究过的聚类算法的聚类标签）和一组特征。对于我们的聚类问题，我们将在预测时执行此转换（因为我们通常不知道聚类的时间），但对于我们在第四章、“用模型连接点——回归方法”和第五章、“将数据放在合适的位置——分类方法和分析”中将要查看的回归和分类模型，标签用作拟合模型的一部分。

1.  我们经常希望有一种方法将我们的建模输出保存下来，以便由下游应用程序使用，无论是在磁盘上还是在数据库中，我们可以稍后通过历史记录查询索引模型。

让我们使用 Python 笔记本查看这些组件。假设我们已经在系统上安装了 Spark，我们将首先导入所需的依赖项：

```py
>>> from pyspark import SparkContext
>>> from pyspark.streaming import StreamingContext

```

然后，我们可以测试启动`SparkContext`：

```py
>>> sc = SparkContext( 'local', 'streaming-kmeans')

```

请记住，第一个参数提供了我们的 Spark 主机的 URL，即协调 Spark 作业执行并将任务分配给集群中工作机的机器。在这种情况下，我们将本地运行它，因此将此参数指定为`localhost`，但否则这可能是我们集群中远程机器的 URL。第二个参数只是我们为应用程序指定的名称。在上下文运行后，我们还可以使用以下方式生成流上下文，该上下文包含有关我们的流应用程序的信息：

```py
>>> ssc = StreamingContext(sc, 10)

```

第一个参数简单地是作为`StreamingContext`父类的`SparkContext`：第二个是我们将检查流数据源新数据的频率（以秒为单位）。如果我们期望数据定期到达，我们可以将其设置得更低，或者如果预期新数据不太频繁地可用，我们可以将其设置得更高。

现在我们有了`StreamingContext`，我们可以添加数据源。假设现在我们将有两个训练数据源（可能是历史数据）。我们希望作业在给出一条错误数据时不会死亡，因此我们使用一个提供这种灵活性的`Parser`类：

```py
>>> class Parser():
 def __init__(self,type='train',delimiter=',',num_elements=5, job_uuid=''):
 self.type=type
 self.delimiter=delimiter
 self.num_elements=num_elements
 self.job_uuid=job_uuid

 def parse(self,l):
 try:
 line = l.split(self.delimiter) 
 if self.type=='train':
 category = float(line[0])
 feature_vector = Vectors.dense(line[1:])
 return LabeledPoint(category, feature_vector)
 elif self.type=='test':
 category = -1
 feature_vector = Vectors.dense(line)
 return LabeledPoint(category, feature_vector)
 else:
 # log exceptions
 f = open('/errors_events/{0}.txt'.format(self.job_uuid),'a')
 f.write('Unknown type: {0}'.format(self.type))
 f.close()
 except:
 # log errors
 f = open('/error_events/{0}.txt'.format(self.job_uuid),'a')
 f.write('Error parsing line: {0}'.format)
 f.close() 

```

我们将错误行记录到以我们的作业 ID 命名的文件中，这样如果需要的话，我们可以稍后定位它们。然后我们可以使用这个解析器来训练和评估模型。为了训练模型，我们将具有三列（标签和要聚类的数据）的文件移动到训练目录中。我们还可以向测试数据目录添加只有两列的文件，仅包含坐标特征：

```py
>>> num_features = 2
num_clusters = 3

training_parser = Parser('train',',',num_features+1,job_uuid)
test_parser = Parser('test',',',num_features,job_uuid)

trainingData = ssc.textFileStream("/training_data").\
 map(lambda x: training_parser.parse(x)).map(lambda x: x.features)
testData = ssc.textFileStream("/test_data").\
 map(lambda x: test_parser.parse(x)).map(lambda x: x.features)
streaming_clustering = StreamingKMeans(k=num_clusters, decayFactor=1.0).\
 setRandomCenters(num_features,0,0)
streaming_clustering.trainOn(trainingData)
streaming_clustering.predictOn(testData).\
 pprint()
ssc.start() 

```

参数中的衰减因子给出了结合当前聚类中心和旧聚类中心的配方。对于参数 1.0，我们使用新旧数据之间的相等权重，而在另一个极端，即 0 时，我们只使用新数据。如果我们在任何时候停止模型，我们可以使用`lastestModel()`函数来检查它：

```py
>>>  streaming_clustering.latestModel().clusterCenters

```

我们也可以使用`predict()`函数在适当大小的向量上预测：

```py
>> streaming_clustering.latestModel().predict([ … ])

```

# 摘要

在本节中，我们学习了如何在数据集中识别相似项的组，这是一种探索性分析，我们可能会在解读新数据集时频繁使用它作为第一步。我们探讨了计算数据点之间相似性的不同方法，并描述了这些指标可能最适合应用的数据类型。我们考察了两种聚类算法：一种是从单个组开始将数据分割成更小组件的划分聚类算法，另一种是每个数据点最初都是其自己的聚类的聚合方法。使用多个数据集，我们展示了这些算法表现好坏的例子，以及一些优化它们的方法。我们还看到了我们的第一个（小型）数据处理管道，这是一个使用流数据的 PySpark 聚类应用。
