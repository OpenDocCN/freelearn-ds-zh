# 第六章：无监督机器学习算法

本章是关于无监督机器学习算法的。本章以介绍无监督学习技术开始。然后，我们将学习两种聚类算法：k 均值聚类和层次聚类算法。接下来的部分将介绍一种降维算法，当我们有大量输入变量时可能会很有效。接下来的部分展示了无监督学习如何用于异常检测。最后，我们将看看最强大的无监督学习技术之一，关联规则挖掘。本节还解释了从关联规则挖掘中发现的模式如何代表跨交易中各种数据元素之间的有趣关系，这可以帮助我们进行基于数据的决策。

在本章结束时，读者应该能够理解无监督学习如何用于解决一些现实世界的问题。读者将了解目前用于无监督学习的基本算法和方法论。

在本章中，我们将涵盖以下主题：

+   无监督学习

+   聚类算法

+   降维

+   异常检测算法

+   关联规则挖掘

# 介绍无监督学习

无监督学习的最简单定义是，它是通过发现和利用数据的固有模式来为非结构化数据提供某种结构的过程。如果数据不是由某种随机过程产生的，它在其多维问题空间中的数据元素之间将具有一些模式。无监督学习算法通过发现这些模式并利用它们来为数据集提供一些结构。这个概念在下图中显示：

![](img/32f60651-b8ff-4ec0-96fa-7b7145366467.png)

请注意，无监督学习通过发现现有模式的新特征来添加结构。

# 数据挖掘生命周期中的无监督学习

理解无监督学习的作用，首先要看数据挖掘过程的整体生命周期。有不同的方法论将数据挖掘过程的生命周期划分为不同的独立阶段，称为**阶段**。目前，有两种流行的表示数据挖掘生命周期的方式：

+   **CRISP-DM**（**跨行业标准数据挖掘过程**）生命周期

+   **SEMMA**（**样本、探索、修改、建模、访问**）数据挖掘过程

CRISP-DM 是由一些数据挖掘者联合开发的，他们来自包括克莱斯勒和**SPSS**（**社会科学统计软件包**）在内的各种公司。SEMMA 是由**SAS**（**统计分析系统**）提出的。让我们看看这两种数据挖掘生命周期的表示之一，CRISP-DM，并尝试理解无监督学习在数据挖掘生命周期中的位置。请注意，SEMMA 在其生命周期内有一些类似的阶段。

如果我们看 CRISP-DM 生命周期，可以看到它包括六个不同的阶段，如下图所示：

![](img/ff5d5601-03c9-465c-92dd-b830b77cd130.png)

让我们逐个了解每个阶段：

+   **阶段 1：业务理解**：这是收集需求的阶段，涉及从业务角度深入全面地理解问题。根据**机器学习**（**ML**）的要求定义问题的范围，并适当地重新表述它是这个阶段的重要部分。例如，对于二元分类问题，有时将需求用可以证明或拒绝的假设来表述是有帮助的。本阶段还涉及记录将在下游阶段 4 中训练的机器学习模型的期望。例如，对于分类问题，我们需要记录可以部署到生产中的模型的最低可接受准确性。

CRISP-DM 生命周期的第一阶段是业务理解，重点是需要做什么，而不是如何做。

+   **第二阶段：数据理解**：这是关于理解可用于数据挖掘的数据。在这个阶段，我们将找出是否有适合解决问题的正确数据集。在确定数据集之后，我们需要了解数据的质量和结构。我们需要找出可以从数据中提取的模式，这些模式可能会引导我们获得重要的见解。我们还将尝试找到可以根据第一阶段收集的要求用作标签（或目标变量）的正确特征。无监督学习算法可以在实现第二阶段目标方面发挥强大作用。无监督算法可以用于以下目的：

+   在数据集中发现模式

+   通过分析发现的模式来理解数据集的结构

+   识别或推导目标变量

+   **第三阶段：数据准备**：这是为我们将在第四阶段训练的 ML 模型准备数据的阶段。可用的标记数据被分成两个不相等的部分。较大的部分称为**训练数据**，用于在第四阶段训练模型。较小的部分称为**测试数据**，在第五阶段用于模型评估。在这个阶段，无监督机器学习算法可以用作准备数据的工具，例如，它们可以用于将非结构化数据转换为结构化数据，提供可以帮助训练模型的额外维度。

+   **第四阶段：建模**：这是我们使用监督学习来制定已发现模式的阶段。我们需要根据所选的监督学习算法的要求成功准备数据。这也是确定将用作标签的特定特征的阶段。在第三阶段，我们将数据分为测试集和训练集。在这个阶段，我们形成数学公式来表示我们感兴趣的模式中的关系。这是通过使用第三阶段创建的训练数据来训练模型完成的。如前所述，最终的数学公式将取决于我们选择的算法。

+   **第五阶段：评估**：这个阶段是关于使用第三阶段的测试数据测试新训练的模型。如果评估符合第一阶段设定的期望，那么我们需要再次迭代所有前面的阶段，从第一阶段开始。这在前面的图像中有所说明。

+   **第六阶段：部署**：如果评估符合或超过第五阶段描述的期望，那么训练好的模型将被部署到生产环境中，并开始为我们在第一阶段定义的问题提供解决方案。

CRISP-DM 生命周期的第二阶段（数据理解）和第三阶段（数据准备）都是关于理解数据并为训练模型做准备。这些阶段涉及数据处理。一些组织为这个数据工程阶段雇佣专家。

很明显，提出问题的解决方案的过程完全是数据驱动的。结合监督和无监督机器学习用于制定可行的解决方案。本章重点介绍解决方案的无监督学习部分。

数据工程包括第二阶段和第三阶段，是机器学习中最耗时的部分。它可能占据典型 ML 项目时间和资源的 70%。无监督学习算法在数据工程中可以发挥重要作用。

以下各节提供了有关无监督算法的更多细节。

# 无监督学习的当前研究趋势

多年来，对机器学习算法的研究更多地集中在监督学习技术上。由于监督学习技术可以直接用于推断，因此它们在时间、成本和准确性方面的优势相对容易衡量。无监督机器学习算法的潜力最近才被认识到。由于无监督学习不受指导，因此它不太依赖假设，并且可能在任何维度上收敛解决方案。尽管更难控制无监督学习算法的范围和处理要求，但它们有更多潜力发现隐藏的模式。研究人员还在努力将无监督机器学习技术与监督学习技术相结合，以设计新的强大算法。

# 实际例子

目前，无监督学习用于更好地理解数据并为其提供更多结构，例如，它用于市场细分、欺诈检测和市场篮分析（稍后在本章中讨论）。让我们看几个例子。

# 语音分类

无监督学习可以用于对语音文件中的个别声音进行分类。它利用了每个人的声音具有独特的特征这一事实，从而创建可能可分离的音频模式。这些模式可以用于语音识别，例如，谷歌在其 Google Home 设备中使用这种技术来训练它们区分不同人的声音。一旦训练完成，Google Home 可以个性化地为每个用户提供响应。

例如，假设我们有一段录制的三个人互相交谈半个小时的对话。使用无监督学习算法，我们可以识别数据集中不同人的声音。请注意，通过无监督学习，我们为给定的非结构化数据集添加了结构。这种结构为我们的问题空间提供了额外有用的维度，可以用于获取见解并为我们选择的机器学习算法准备数据。以下图表显示了无监督学习用于语音识别的情况：

![](img/71ed2127-a466-4865-b70e-6511b810bb98.png)

请注意，在这种情况下，无监督学习建议我们添加一个具有三个不同级别的新特征。

# 文档分类

无监督机器学习算法也可以应用于非结构化文本数据的存储库，例如，如果我们有一组 PDF 文档的数据集，那么无监督学习可以用于以下目的：

+   发现数据集中的各种主题

+   将每个 PDF 文档与发现的主题之一关联起来

无监督学习用于文档分类的情况如下图所示。这是另一个例子，我们在非结构化数据中添加了更多的结构：

![](img/bafdb93e-edf1-4f72-a99e-dfd51fe936b8.png)图 6.4：使用无监督学习进行文档分类

请注意，在这种情况下，无监督学习建议我们添加一个具有五个不同级别的新特征。

# 理解聚类算法

在无监督学习中使用的最简单和最强大的技术之一是基于通过聚类算法将相似模式分组在一起。它用于理解与我们试图解决的问题相关的数据的特定方面。聚类算法寻找数据项中的自然分组。由于该组不是基于任何目标或假设，因此被归类为无监督学习技术。

各种聚类算法创建的分组是基于在问题空间中找到各种数据点之间的相似性。确定数据点之间的相似性的最佳方法将因问题而异，并且将取决于我们正在处理的问题的性质。让我们看看可以用来计算各种数据点之间相似性的各种方法。

# 量化相似性

聚类算法创建的分组的可靠性是基于这样一个假设：我们能够准确量化问题空间中各种数据点之间的相似性或接近程度。这是通过使用各种距离度量来实现的。以下是用于量化相似性的三种最流行的方法：

+   欧几里得距离度量

+   曼哈顿距离度量

+   余弦距离度量

让我们更详细地看看这些距离度量。

# 欧几里得距离

不同点之间的距离可以量化两个数据点之间的相似性，并且广泛用于无监督机器学习技术，如聚类。欧几里得距离是最常见和简单的距离度量。它通过测量多维空间中两个数据点之间的最短距离来计算。例如，让我们考虑二维空间中的两点**A(1,1)**和**B(4,4)**，如下图所示：

![](img/7a13e022-8b59-44d6-a2de-f009d99da231.png)

要计算**A**和**B**之间的距离——即*d(A,B)*，我们可以使用以下毕达哥拉斯公式：

![](img/b816650c-9590-4145-850e-b428499fbb7c.png)

请注意，此计算是针对二维问题空间的。对于*n*维问题空间，我们可以计算两点**A**和**B**之间的距离如下：

![](img/ab5364d1-0bdf-420c-bbfe-443875a74304.png)

# 曼哈顿距离

在许多情况下，使用欧几里得距离度量来测量两点之间的最短距离将无法真正代表两点之间的相似性或接近程度——例如，如果两个数据点代表地图上的位置，则使用地面交通工具（如汽车或出租车）从点 A 到点 B 的实际距离将大于欧几里得距离计算出的距离。对于这类情况，我们使用曼哈顿距离，它标记了两点之间的最长路线，并更好地反映了在繁忙城市中可以前往的源点和目的地点之间的接近程度。曼哈顿和欧几里得距离度量之间的比较如下图所示：

![](img/2c74e092-8cbb-4082-84c4-f3adbdfc47e3.png)

曼哈顿距离始终大于或等于相应的欧几里得距离。

# 余弦距离

欧几里得和曼哈顿距离度量在高维空间中表现不佳。在高维问题空间中，余弦距离更准确地反映了多维问题空间中两个数据点之间的接近程度。余弦距离度量是通过测量由两个连接到参考点的点所创建的余弦角来计算的。如果数据点接近，则角度将很窄，而不管它们具有的维度如何。另一方面，如果它们相距很远，那么角度将很大：

![](img/d5d492a0-e445-4361-80c4-7a92031aebd7.png)文本数据几乎可以被视为高维空间。由于余弦距离度量在高维空间中表现非常好，因此在处理文本数据时是一个不错的选择。

请注意，在前面的图中，**A(2,5)**和**B(4.4)**之间的角的余弦是余弦距离。这些点之间的参考点是原点——即**X(0,0)**。但实际上，问题空间中的任何点都可以充当参考数据点，并且不一定是原点。

# K 均值聚类算法

k-means 聚类算法的名称来自于它试图创建*k*个聚类，通过计算均值来找到数据点之间的接近程度。它使用了一个相对简单的聚类方法，但由于其可扩展性和速度而仍然受欢迎。从算法上讲，k-means 聚类使用了一个迭代逻辑，将聚类的中心移动到它们所属的分组的最具代表性的数据点。

重要的是要注意，k-means 算法缺乏聚类所需的非常基本的功能之一。这个缺失的功能是，对于给定的数据集，k-means 算法无法确定最合适的聚类数。最合适的聚类数*k*取决于特定数据集中自然分组的数量。这种省略背后的哲学是尽可能简化算法，最大限度地提高其性能。这种精益简洁的设计使 k-means 适用于更大的数据集。假设将使用外部机制来计算*k*。确定*k*的最佳方法将取决于我们试图解决的问题。在某些情况下，*k*直接由聚类问题的上下文指定，例如，如果我们想将一类数据科学学生分成两个聚类，一个由具有数据科学技能的学生组成，另一个由具有编程技能的学生组成，那么*k*将为 2。在其他一些问题中，*k*的值可能不明显。在这种情况下，将不得不使用迭代的试错程序或基于启发式的算法来估计给定数据集的最合适的聚类数。

# k-means 聚类的逻辑

本节描述了 k-means 聚类算法的逻辑。让我们逐一看一下。

# 初始化

为了对它们进行分组，k-means 算法使用距离度量来找到数据点之间的相似性或接近程度。在使用 k-means 算法之前，需要选择最合适的距离度量。默认情况下，将使用欧氏距离度量。此外，如果数据集中有异常值，则需要制定机制来确定要识别和删除数据集的异常值的标准。 

# k-means 算法的步骤

k-means 聚类算法涉及的步骤如下：

| 步骤 1 | 我们选择聚类的数量*k*。 |
| --- | --- |
| 步骤 2 | 在数据点中，我们随机选择*k*个点作为聚类中心。 |
| 步骤 3 | 基于所选的距离度量，我们迭代地计算问题空间中每个点到*k*个聚类中心的距离。根据数据集的大小，这可能是一个耗时的步骤，例如，如果聚类中有 10,000 个点，*k*=3，这意味着需要计算 30,000 个距离。 |
| 步骤 4 | 我们将问题空间中的每个数据点分配给最近的聚类中心。 |
| 步骤 5 | 现在我们问题空间中的每个数据点都有一个分配的聚类中心。但我们还没有完成，因为初始聚类中心的选择是基于随机选择的。我们需要验证当前随机选择的聚类中心实际上是每个聚类的重心。我们通过计算每个*k*聚类的组成数据点的平均值来重新计算聚类中心。这一步解释了为什么这个算法被称为 k-means。 |
| 步骤 6 | 如果在步骤 5 中聚类中心发生了变化，这意味着我们需要重新计算每个数据点的聚类分配。为此，我们将回到步骤 3 重复这个计算密集的步骤。如果聚类中心没有发生变化，或者我们的预定停止条件（例如，最大迭代次数）已经满足，那么我们就完成了。 |

下图显示了在二维问题空间中运行 k-means 算法的结果：

![](img/70ce3a57-73b1-4c76-97fe-fdbcc956e36d.png)（a）聚类前的数据点；（b）运行 k 均值聚类算法后的结果集群

请注意，在运行 k 均值后创建的两个结果集群在这种情况下有很好的区分度。

# 停止条件

对于 k 均值算法，默认的停止条件是在第 5 步中不再移动集群中心。但是与许多其他算法一样，k 均值算法可能需要很长时间才能收敛，特别是在处理高维问题空间中的大型数据集时。我们可以明确定义停止条件，而不是等待算法收敛，如下所示：

+   通过指定最大执行时间：

+   **停止条件**：*如果 t>t[max]*，其中*t*是当前执行时间，*t[max]*是我们为算法设置的最大执行时间。

+   通过指定最大迭代次数：

+   **停止条件**：*如果 m>m[max]*，其中*m*是当前迭代次数，*m[max]*是我们为算法设置的最大迭代次数。

# 编写 k 均值算法

让我们看看如何在 Python 中编写 k 均值算法：

1.  首先，让我们导入编写 k 均值算法所需的软件包。请注意，我们正在导入`sklearn`软件包进行 k 均值聚类：

```py
from sklearn import cluster import pandas as pd
import numpy as np
```

1.  要使用 k 均值聚类，让我们在二维问题空间中创建 20 个数据点，这些数据点将用于 k 均值聚类：

```py
dataset = pd.DataFrame({
    'x': [11, 21, 28, 17, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 62, 70, 72, 10],
    'y': [39, 36, 30, 52, 53, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 18, 7, 24, 10]
})
```

1.  让我们有两个集群（*k*=2），然后通过调用`fit`函数创建集群：

```py
myKmeans = cluster.KMeans(n_clusters=2)
myKmeans.fit(dataset)
```

1.  让我们创建一个名为`centroid`的变量，它是一个包含形成的集群中心位置的数组。在我们的情况下，*k*=2，数组的大小将为 2。让我们还创建另一个名为`label`的变量，表示每个数据点分配给两个集群中的一个。由于有 20 个数据点，这个数组的大小将为 20：

```py
centroids = myKmeans.cluster_centers_
labels = myKmeans.labels_
```

1.  现在让我们打印这两个数组，`centroids`和`labels`：

![](img/1bd564b9-9201-40d0-8cb3-4c7109677fdd.png)

请注意，第一个数组显示了每个数据点与集群的分配，第二个数组显示了两个集群中心。

1.  让我们使用`matplotlib`绘制并查看这些集群：

![](img/6cc48dd3-517d-4311-a93e-5f3e206d7f1f.png)

请注意，图中的较大点是由 k 均值算法确定的中心点。

# k 均值聚类的局限性

k 均值算法旨在成为一种简单快速的算法。由于其设计上的故意简单性，它具有以下限制：

+   k 均值聚类的最大限制是初始集群数量必须预先确定。

+   集群中心的初始分配是随机的。这意味着每次运行算法时，可能会得到略有不同的集群。

+   每个数据点只分配给一个集群。

+   k 均值聚类对异常值敏感。

# 层次聚类

k 均值聚类使用自上而下的方法，因为我们从最重要的数据点开始算法，即集群中心。还有一种聚类的替代方法，即不是从顶部开始，而是从底部开始算法。在这种情况下，底部是问题空间中的每个单独数据点。解决方案是在向上移向集群中心的过程中不断将相似的数据点分组在一起。这种替代的自下而上方法由层次聚类算法使用，并在本节中讨论。

# 层次聚类的步骤

层次聚类涉及以下步骤：

1.  我们在问题空间中为每个数据点创建一个单独的集群。如果我们的问题空间包含 100 个数据点，那么它将从 100 个集群开始。

1.  我们只将彼此最接近的点分组。

1.  我们检查停止条件；如果停止条件尚未满足，则重复步骤 2。

生成的集群结构称为**树状图**。

在树状图中，垂直线的高度决定了物品的接近程度，如下图所示：

![](img/5557743e-1874-453f-b274-47d2f3ca06c7.png)

请注意，停止条件显示为上图中的虚线。

# 编写一个分层聚类算法

让我们学习如何在 Python 中编写一个分层算法：

1.  我们将首先从`sklearn.cluster`库中导入`AgglomerativeClustering`，以及`pandas`和`numpy`包：

```py
from sklearn.cluster import AgglomerativeClustering import pandas as pd
import numpy as np
```

1.  然后我们将在二维问题空间中创建 20 个数据点：

```py
dataset = pd.DataFrame({
    'x': [11, 21, 28, 17, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 62, 70, 72, 10],
    'y': [39, 36, 30, 52, 53, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 18, 7, 24, 10]
})

```

1.  然后我们通过指定超参数来创建分层集群。我们使用`fit_predict`函数来实际处理算法：

```py
cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward') 
cluster.fit_predict(dataset) 
```

1.  现在让我们看一下每个数据点与创建的两个簇的关联：

![](img/055dc165-fddc-4091-b73f-259376a09a82.png)

您可以看到分层和 k 均值算法的集群分配非常相似。

# 评估聚类

良好质量的聚类的目标是属于不同簇的数据点应该是可区分的。这意味着以下内容：

+   属于同一簇的数据点应尽可能相似。

+   属于不同簇的数据点应尽可能不同。

人类直觉可以用来通过可视化集群结果来评估集群结果，但也有数学方法可以量化集群的质量。轮廓分析是一种比较 k 均值算法创建的集群中的紧密度和分离度的技术。轮廓绘制了一个图，显示了特定集群中每个点与相邻集群中其他点的接近程度。它将与每个集群关联的数字范围为[-0, 1]。以下表显示了此范围中的数字表示什么：

| **范围** | 意义 | 描述 |
| --- | --- | --- |
| 0.71–1.0 | 优秀 | 这意味着 k 均值聚类导致的组在相当程度上是可区分的。 |
| 0.51–0.70 | 合理 | 这意味着 k 均值聚类导致的组在某种程度上是可区分的。 |
| 0.26–0.50 | 弱 | 这意味着 k 均值聚类导致了分组，但不应依赖分组的质量。 |
| <0.25 | 未找到任何聚类 | 使用选择的参数和使用的数据，无法使用 k 均值聚类创建分组。 |

请注意，问题空间中的每个簇将获得一个单独的分数。

# 聚类的应用

聚类用于我们需要在数据集中发现潜在模式的地方。

在政府使用案例中，聚类可用于以下目的：

+   犯罪热点分析

+   人口社会分析

在市场研究中，聚类可用于以下目的：

+   市场细分

+   定向广告

+   客户分类

主成分分析（PCA）也用于通常探索数据并从实时数据中去除噪音，例如股票市场交易。

# 降维

我们数据中的每个特征对应于问题空间中的一个维度。将特征的数量最小化以使问题空间更简单称为降维。可以通过以下两种方式之一来完成：

+   特征选择：选择在我们试图解决的问题的上下文中重要的一组特征

+   特征聚合：使用以下算法之一组合两个或多个特征以减少维度：

+   PCA：线性无监督 ML 算法

+   线性判别分析（LDA）：线性监督 ML 算法

+   核主成分分析：一种非线性算法

让我们更深入地了解一种流行的降维算法，即 PCA。

# 主成分分析

PCA 是一种无监督的机器学习技术，可以使用线性变换来降低维度。在下图中，我们可以看到两个主成分**PC1**和**PC2**，它们显示了数据点的分布形状。PC1 和 PC2 可以用适当的系数来总结数据点：

![](img/2a037f1f-58b8-4a7b-9a9b-8ad91808dfcb.png)

让我们考虑以下代码：

```py
from sklearn.decomposition import PCA
iris = pd.read_csv('iris.csv')
X = iris.drop('Species', axis=1)
pca = PCA(n_components=4)
pca.fit(X)
```

现在让我们打印我们的 PCA 模型的系数：

![](img/07d52f65-eb9d-4f86-b9c0-01ab5f62c25b.png)

请注意，原始的 DataFrame 有四个特征，`Sepal.Length`、`Sepal.Width`、`Petal.Length`和`Petal.Width`。前面的 DataFrame 指定了四个主成分 PC1、PC2、PC3 和 PC4 的系数，例如，第一行指定了可以用来替换原始四个变量的 PC1 的系数。

根据这些系数，我们可以计算我们输入 DataFrame X 的 PCA 组件：

```py
pca_df=(pd.DataFrame(pca.components_,columns=X.columns))

# Let us calculate PC1 using coefficients that are generated
X['PC1'] = X['Sepal.Length']* pca_df['Sepal.Length'][0] + X['Sepal.Width']* pca_df['Sepal.Width'][0]+ X['Petal.Length']* pca_df['Petal.Length'][0]+X['Petal.Width']* pca_df['Petal.Width'][0]

# Let us calculate PC2
X['PC2'] = X['Sepal.Length']* pca_df['Sepal.Length'][1] + X['Sepal.Width']* pca_df['Sepal.Width'][1]+ X['Petal.Length']* pca_df['Petal.Length'][1]+X['Petal.Width']* pca_df['Petal.Width'][1]

#Let us calculate PC3
X['PC3'] = X['Sepal.Length']* pca_df['Sepal.Length'][2] + X['Sepal.Width']* pca_df['Sepal.Width'][2]+ X['Petal.Length']* pca_df['Petal.Length'][2]+X['Petal.Width']* pca_df['Petal.Width'][2]

# Let us calculate PC4
X['PC4'] = X['Sepal.Length']* pca_df['Sepal.Length'][3] + X['Sepal.Width']* pca_df['Sepal.Width'][3]+ X['Petal.Length']* pca_df['Petal.Length'][3]+X['Petal.Width']* pca_df['Petal.Width'][3]

```

现在让我们在计算 PCA 组件后打印 X：

![](img/3d1712cf-5e6f-407c-81c9-b2c66a593b73.png)

现在让我们打印方差比率，并尝试理解使用 PCA 的影响：

![](img/808e5134-ae27-41c7-82fb-59d557f5eccd.png)

方差比率表示如下：

+   如果我们选择用 PC1 替换原始的四个特征，那么我们将能够捕获大约 92.3%的原始变量的方差。我们通过不捕获原始四个特征 100%的方差来引入一些近似。

+   如果我们选择用 PC1 和 PC2 替换原始的四个特征，那么我们将捕获额外的 5.3%的原始变量的方差。

+   如果我们选择用 PC1、PC2 和 PC3 替换原始的四个特征，那么我们现在将捕获原始变量进一步的 0.017%的方差。

+   如果我们选择用四个主成分替换原始的四个特征，那么我们将捕获原始变量的 100%的方差（92.4 + 0.053 + 0.017 + 0.005），但用四个主成分替换四个原始特征是没有意义的，因为我们没有减少维度，也没有取得任何成果。

# PCA 的局限性

PCA 的局限性如下：

+   PCA 只能用于连续变量，对于类别变量无关。

+   在聚合时，PCA 近似了组件变量；它以准确性为代价简化了维度的问题。在使用 PCA 之前，应该仔细研究这种权衡。

# 关联规则挖掘

特定数据集中的模式是需要被发现、理解和挖掘的宝藏。有一组重要的算法试图专注于给定数据集中的模式分析。在这类算法中，较受欢迎的算法之一称为**关联规则挖掘**算法，它为我们提供了以下功能：

+   衡量模式频率的能力

+   建立模式之间*因果*关系的能力。

+   通过将它们的准确性与随机猜测进行比较，量化模式的有用性

# 使用示例

当我们试图调查数据集中不同变量之间的因果关系时，使用关联规则挖掘。以下是它可以帮助回答的示例问题：

+   哪些湿度、云层覆盖和温度值可能导致明天下雨？

+   什么类型的保险索赔可能表明欺诈？

+   哪些药物的组合可能会导致患者并发症？

# 市场篮分析

在本书中，推荐引擎在第八章“神经网络算法”中进行了讨论。篮子分析是学习推荐的一种简单方法。在篮子分析中，我们的数据只包含有关哪些物品一起购买的信息。它没有任何关于用户或用户是否喜欢个别物品的信息。请注意，获取这些数据要比获取评级数据容易得多。

例如，当我们在沃尔玛购物时，就会产生这种数据，而不需要任何特殊技术来获取数据。这些数据在一段时间内收集起来，被称为**交易数据**。当将关联规则分析应用于便利店、超市和快餐连锁店中使用的购物车的交易数据集时，就称为**市场篮子分析**。它衡量了一组物品一起购买的条件概率，有助于回答以下问题：

+   货架上物品的最佳摆放位置是什么？

+   物品在营销目录中应该如何出现？

+   基于用户的购买模式，应该推荐什么？

由于市场篮子分析可以估计物品之间的关系，因此它经常用于大众市场零售，如超市、便利店、药店和快餐连锁店。市场篮子分析的优势在于其结果几乎是不言自明的，这意味着它们很容易被业务用户理解。

让我们来看一个典型的超市。商店中所有可用的唯一物品可以用一个集合![](img/ce31fde4-ba41-4fa9-bb71-4c63b428aec9.png)={item[1]，item[2]，...，item[m]}来表示。因此，如果那家超市销售 500 种不同的物品，那么![](img/cd637cd5-f4fc-4481-8df6-6dc079d692cb.png)将是一个大小为 500 的集合。

人们会从这家商店购买物品。每当有人购买物品并在柜台付款时，它就会被添加到一个特定交易中的物品集合中，称为**项目集**。在一段时间内，交易被分组在一个由![](img/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png)表示的集合中，其中![](img/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png)={t[1],t[2],...,t[n]}。

让我们来看一下只包含四个交易的简单交易数据。这些交易总结在下表中：

| t1 | 球门，护腕 |
| --- | --- |
| t2 | 球棒，球门，护腕，头盔 |
| t3 | 头盔，球 |
| t4 | 球棒、护腕、头盔 |

让我们更详细地看一下这个例子：

![](img/ce31fde4-ba41-4fa9-bb71-4c63b428aec9.png)={球棒，球门，护腕，头盔，球}，它代表了商店中所有可用的唯一物品。

让我们考虑来自![](img/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png)的一个交易 t3。请注意，t3 中购买的物品可以用 itemset[t3]={头盔，球}表示，这表明顾客购买了两件物品。由于这个 itemset 中有两件物品，因此 itemset[t5]的大小被称为两。 

# 关联规则

关联规则通过数学方式描述了各种交易中涉及的物品之间的关系。它通过研究形式为*X*⇒*Y*的两个项目集之间的关系来实现这一点，其中*X*⊂![](img/4a0f1bae-4e77-4c84-86a7-6691729c3e57.png)，*Y*⊂![](img/1735cf94-543b-47a9-b5b8-24cec3a6055c.png)。此外，*X*和*Y*是不重叠的项目集；这意味着![](img/25efda0c-53d6-46dc-b473-443ee7bc0dfa.png)。

关联规则可以用以下形式描述：

{头盔，球}⇒{自行车}

在这里，{头盔，球}是*X*，{球}是*Y*。

# 规则类型

运行关联分析算法通常会从交易数据集中生成大量规则。其中大部分是无用的。为了挑选出可以提供有用信息的规则，我们可以将它们分类为以下三种类型之一：

+   琐碎

+   莫名其妙

+   可操作

让我们更详细地看看每种类型。

# 琐碎的规则

在生成的大量规则中，许多派生的规则将是无用的，因为它们总结了关于业务的常识。它们被称为琐碎规则。即使琐碎规则的置信度很高，它们仍然是无用的，不能用于任何数据驱动的决策。我们可以安全地忽略所有琐碎规则。

以下是琐碎规则的例子：

+   任何从高楼跳下的人都有可能死亡。

+   更努力工作会导致考试成绩更好。

+   随着温度下降，取暖器的销量会增加

+   在高速公路上超速驾驶会增加事故的可能性。

# 不可解释规则

在运行关联规则算法后生成的规则中，那些没有明显解释的规则是最难使用的。请注意，规则只有在能帮助我们发现和理解预期最终会导致某种行动的新模式时才有用。如果不是这种情况，我们无法解释事件*X*导致事件*Y*的原因，那么它就是一个不可解释的规则，因为它只是一个最终探索两个无关和独立事件之间毫无意义关系的数学公式。

以下是不可解释规则的例子：

+   穿红衬衫的人在考试中得分更高。

+   绿色自行车更容易被盗。

+   购买泡菜的人最终也会购买尿布。

# 可操作规则

可操作规则是我们正在寻找的黄金规则。它们被业务理解并引发见解。当呈现给熟悉业务领域的观众时，它们可以帮助我们发现事件可能的原因，例如，可操作规则可能根据当前的购买模式建议产品在商店中的最佳摆放位置。它们还可能建议将哪些商品放在一起，以最大化它们一起销售的机会。

以下是可操作规则及其相应的行动的例子：

+   **规则 1：**向用户的社交媒体账户展示广告会增加销售的可能性。

**可操作项目：**建议产品的替代广告方式

+   **规则 2：**创建更多的价格点会增加销售的可能性。

**可操作项目：**一个商品可能在促销中进行广告，而另一个商品的价格可能会上涨。

# 排名规则

关联规则有三种衡量方式：

+   物品的支持（频率）

+   置信度

+   提升

让我们更详细地看看它们。

# 支持

支持度量是一个数字，用来量化我们在数据集中寻找的模式有多频繁。首先计算我们感兴趣的模式出现的次数，然后将其除以所有交易的总数来计算。

让我们看看特定*itemset[a]*的以下公式：

*numItemset[a] =包含 itemset[a]的交易数*

*num[total] =交易总数*

![](img/b03e04f2-449b-4843-ba17-698bb6ffe74c.png)仅通过支持，我们就可以了解到模式发生的罕见程度。低支持意味着我们在寻找一种罕见事件。

例如，如果*itemset[a] = {头盔，球}*在六次交易中出现了两次，那么支持（itemset[a]）= 2/6 = 0.33。

# 置信度

置信度是一个数字，通过计算条件概率来量化我们可以将左侧（*X*）与右侧（*Y*）关联的强度。它计算了事件*X*发生的情况下，事件*Y*会发生的概率。

从数学上讲，考虑规则*X* ⇒ *Y*。

这条规则的置信度表示为 confidence(*X* ⇒ *Y*)，并按以下方式测量：

![](img/74b2952b-14dc-4f9e-b791-77fff35337da.png)

让我们举个例子。考虑以下规则：

{头盔，球} ⇒ {球门}

这条规则的置信度由以下公式计算：

![](img/1d0a3ece-4655-4222-b625-51cc092e2c38.png)

这意味着如果有人的篮子里有{头盔，球}，那么他们还有球门的概率是 0.5 或 50%。

# 提升

估计规则质量的另一种方法是通过计算提升。提升返回一个数字，量化了规则在预测结果方面相对于仅假设等式右侧的结果的改进程度。如果*X*和*Y*项集是独立的，那么提升的计算如下：

![](img/01fae583-e035-4d40-b73d-32212d50c0e2.png)

# 关联分析算法

在本节中，我们将探讨以下两种可用于关联分析的算法：

+   **Apriori 算法**：由 Agrawal, R.和 Srikant 于 1994 年提出。

+   **FP-growth 算法**：由 Han 等人于 2001 年提出的改进建议。

让我们看看这些算法各自的情况。

# Apriori 算法

Apriori 算法是一种迭代和多阶段的算法，用于生成关联规则。它基于生成和测试的方法。

在执行 apriori 算法之前，我们需要定义两个变量：support[threshold]和 Confidence[threshold]。

该算法包括以下两个阶段：

+   **候选生成阶段**：它生成包含所有高于 support[threshold]的项集的候选项集。

+   **过滤阶段**：它过滤掉所有低于预期 confidence[threshold]的规则。

过滤后，得到的规则就是答案。

# Apriori 算法的局限性

Apriori 算法中的主要瓶颈是第 1 阶段候选规则的生成，例如，![](img/1486d41a-07b6-45b2-bab4-71873083bfd2.png) = {item  [1]  , item  [2]  , . . . , item  [m]  } 可以产生 2^m 个可能的项集。由于其多阶段设计，它首先生成这些项集，然后努力找到频繁项集。这个限制是一个巨大的性能瓶颈，使得 apriori 算法不适用于更大的项。

# FP-growth 算法

**频繁模式增长**（**FP-growth**）算法是对 apriori 算法的改进。它首先展示频繁交易 FP 树，这是一个有序树。它包括两个步骤：

+   填充 FP 树

+   挖掘频繁模式

让我们一步一步地看这些步骤。

# 填充 FP 树

让我们考虑下表中显示的交易数据。让我们首先将其表示为稀疏矩阵：

| **ID** | **球棒** | **球门** | **防护板** | **头盔** | **球** |
| --- | --- | --- | --- | --- | --- |
| 1 | 0 | 1 | 1 | 0 | 0 |
| 2 | 1 | 1 | 1 | 1 | 0 |
| 3 | 0 | 0 | 0 | 1 | 1 |
| 4 | 1 | 0 | 1 | 1 | 0 |

让我们计算每个项的频率，并按频率降序排序：

| **项** | **频率** |
| --- | --- |
| 防护板 | 3 |
| 头盔 | 3 |
| 球棒 | 2 |
| 球门 | 2 |
| 球 | 1 |

现在让我们根据频率重新排列基于交易的数据：

| **ID** | **原始项** | **重新排序的项** |
| --- | --- | --- |
| t1 | 防护板，球门 | 防护板，球门 |
| t2 | 球棒，球门，防护板，头盔 | 头盔，防护板，球门，球棒 |
| t3 | 头盔，球 | 头盔，球 |
| t4 | 球棒，防护板，头盔 | 头盔，防护板，球棒 |

要构建 FP 树，让我们从 FP 树的第一个分支开始。FP 树以**Null**作为根开始。为了构建树，我们可以用一个节点表示每个项，如下图所示（这里显示了 t[1]的树表示）。请注意，每个节点的标签都是项的名称，冒号后面附加了其频率。还要注意**pads**项的频率为 1：

![](img/a1831fc4-21ae-4229-8d09-c125d2451a45.png)

使用相同的模式，让我们绘制所有四个交易，得到完整的 FP 树。FP 树有四个叶节点，每个节点代表与四个交易相关的项集。请注意，我们需要计算每个项的频率，并在多次使用时增加它-例如，将 t[2]添加到 FP 树时，**头盔** 的频率增加到了两次。类似地，当添加 t[4]时，它再次增加到了三次。结果树如下图所示：

![](img/81575675-cbb3-418b-b2ba-e58240c27b40.png)

请注意，前面图中生成的 FP 树是有序树。

# 挖掘频繁模式

FP-growth 树的第二阶段涉及从 FP 树中挖掘频繁模式。通过创建一个有序树，意图是创建一个高效的数据结构，可以轻松导航以搜索频繁模式。

我们从叶节点（即末端节点）开始向上移动-例如，让我们从叶节点项之一 **球棒** 开始。然后我们需要计算 **球棒** 的条件模式基。通过指定从叶节点项到顶部的所有路径来计算条件模式基。**球棒** 的条件模式基如下：

| 球门: 1 | 护腕: 1 | 头盔: 1 |
| --- | --- | --- |
| 护腕: 1 | 头盔: 1 |  |

**球棒** 的 **频繁模式** 如下：

*{球门, 护腕, 头盔} : 球棒*

*{护腕,头盔} : 球棒*

# 使用 FP-growth 的代码

让我们看看如何使用 Python 中的 FP-growth 算法生成关联规则。为此，我们将使用 `pyfpgrowth` 软件包。首先，如果我们以前从未使用过 `pyfpgrowth`，让我们首先安装它：

```py
!pip install pyfpgrowth
```

然后，让我们导入实现此算法所需的软件包：

```py
import pandas as pd
import numpy as np
import pyfpgrowth as fp
```

现在我们将创建以 `transactionSet` 形式的输入数据：

```py
dict1 = {
 'id':[0,1,2,3],
 'items':[["wickets","pads"],
 ["bat","wickets","pads","helmet"],
 ["helmet","pad"],
 ["bat","pads","helmet"]]

}
transactionSet = pd.DataFrame(dict1)
```

一旦生成了输入数据，我们将生成基于我们传递给 `find_frequent_patterns()` 的参数的模式。请注意，传递给此函数的第二个参数是最小支持度，在本例中为 1：

```py
patterns = fp.find_frequent_patterns(transactionSet['items'],1)
```

模式已生成。现在让我们打印模式。模式列出了项的组合及其支持：

![](img/1c898244-8bbf-4f3f-acd7-0d71e554569c.png)

现在让我们生成规则：

![](img/a253eee2-98ff-43df-b71b-315b8921f0d6.png)

每个规则都有左侧和右侧，由冒号（:）分隔。它还为我们提供了输入数据集中每个规则的支持。

# 实际应用-将相似的推文进行聚类

无监督机器学习算法也可以实时应用于将相似的推文进行聚类。它们将执行以下操作：

+   步骤 1- **主题建模：** 从给定的一组推文中发现各种主题

+   步骤 2- **聚类：** 将每个推文与发现的主题之一关联起来

这种无监督学习的应用如下图所示：

![](img/8a9a0326-d16f-4c14-9707-a123d24850bb.png)请注意，此示例需要实时处理输入数据。

让我们逐一看看这些步骤。

# 主题建模

主题建模是发现一组文档中的概念的过程，这些概念可以用来区分它们。在推文的背景下，这是关于找出一组推文可以被分成哪些最合适的主题。潜在狄利克雷分配是一种用于主题建模的流行算法。因为每条推文都是一个短的 144 个字符的文档，通常涉及一个非常特定的主题，我们可以为主题建模目的编写一个更简单的算法。该算法描述如下：

1.  对推文进行标记化处理。

1.  预处理数据。删除停用词、数字、符号并进行词干处理

1.  为推文创建一个术语-文档矩阵（TDM）。选择在唯一推文中出现最频繁的前 200 个词。

1.  选择直接或间接代表概念或主题的前 10 个单词。例如时尚、纽约、编程、事故。这 10 个单词现在是我们成功发现的主题，并将成为 tweets 的聚类中心。

让我们继续下一步，即聚类

# 聚类

一旦我们发现了主题，我们将选择它们作为聚类的中心。然后我们可以运行 k-means 聚类算法，将每个 tweet 分配到其中一个聚类中心。

因此，这是一个实际的例子，说明一组 tweets 如何被聚类成发现的主题。

# 异常检测算法

*异常* 的词典定义是与众不同、异常、奇特或不容易分类的东西。它是偏离常规规则的。在数据科学的背景下，异常是偏离预期模式很多的数据点。寻找这样的数据点的技术被称为异常检测技术。

现在让我们看看异常检测算法的一些应用：

+   信用卡欺诈

+   在 **磁共振成像（MRI）** 扫描中发现恶性肿瘤

+   集群中的故障预防

+   考试中的冒名顶替

+   高速公路上的事故

在接下来的章节中，我们将看到各种异常检测技术。

# 使用聚类

诸如 k-means 的聚类算法可以用来将相似的数据点分组在一起。可以定义一个阈值，任何超出该阈值的点都可以被分类为异常。这种方法的问题在于，由于异常数据点的存在，k-means 聚类创建的分组本身可能会存在偏差，并可能影响方法的实用性和准确性。

# 使用基于密度的异常检测

基于密度的方法试图找到密集的邻域。**k-最近邻**（**KNN**）算法可以用于此目的。远离发现的密集邻域的异常被标记为异常。

# 使用支持向量机

**支持向量机**（**SVM**）算法可以用来学习数据点的边界。任何超出这些发现的边界的点都被识别为异常。

# 总结

在本章中，我们看了各种无监督的机器学习技术。我们看了尝试减少我们试图解决的问题的维度的情况，以及不同的方法。我们还研究了无监督机器学习技术在哪些情况下非常有帮助，包括市场篮分析和异常检测。

在下一章中，我们将看看各种监督学习技术。我们将从线性回归开始，然后我们将看看更复杂的监督机器学习技术，如基于决策树的算法、SVM 和 XGBoast。我们还将研究朴素贝叶斯算法，它最适合于非结构化的文本数据。
