# 二、编写 Hadoop MapReduce 程序

在前一章中，我们学习了如何设置 R 和 Hadoop 开发环境。因为我们对执行大数据分析感兴趣，所以我们需要学习 Hadoop 来使用 Hadoop MapReduce 执行操作。在本章中，我们将讨论什么是 MapReduce，为什么有必要，如何通过 Apache Hadoop 开发 MapReduce 程序，等等。

在本章中，我们将介绍:

*   了解 MapReduce 的基础知识
*   介绍 Hadoop MapReduce
*   了解 Hadoop MapReduce 基础知识
*   编写 Hadoop MapReduce 示例
*   理解几种可能的 MapReduce 定义来解决业务问题
*   学习用 R 写 Hadoop MapReduce 的不同方法

# 了解 MapReduce 的基础知识

如果一个人没有集群或者使用**消息传递接口** ( **MPI** ) ，理解 MapReduce 的基础可能是一个长期的解决方案。然而，一个更现实的用例是当数据不适合一个磁盘，但是适合一个**分布式文件系统** ( **DFS** )或者已经存在于 Hadoop 相关软件上。

此外，MapReduce 是一种以分布式方式工作的编程模型，但它并不是唯一这样做的模型。描述其他编程模型可能很有启发性，例如 MPI 和**批量同步并行** ( **BSP** )。用 R 等工具和几种机器学习技术处理大数据需要高配置的机器，但这不是永久的解决方案。因此，分布式处理是处理这些数据的关键。这种分布式计算可以用 MapReduce 编程模型来实现。

MapReduce 是回答大数据问题的工具。从逻辑上讲，要处理数据，我们需要并行处理，这意味着在大计算上进行处理；它可以通过计算机集群或增加机器的配置来获得。使用计算机集群是处理大容量数据的理想方式。

在我们讨论更多关于并行处理的 MapReduce 之前，我们将讨论谷歌 MapReduce 研究和 2004 年由 Jeffrey Dean 和 Sanjay Ghemawat 撰写的白皮书。他们引入了 MapReduce 作为大型集群上的简化数据处理软件。MapReduce 实现运行在拥有商用硬件的大型集群上。这个数据处理平台更容易让程序员进行各种操作。该系统处理输入数据，在计算机网络上分发数据，并行处理数据，最后将其输出合并成一个文件，供以后聚合。这在成本方面非常有帮助，对于处理集群中的大型数据集也是一个省时的系统。此外，它将有效地利用计算机资源对大量数据进行分析。谷歌已经获得了 MapReduce 的专利。

对于 MapReduce，程序员只需要将应用设计/迁移到两个阶段:Map 和 Reduce。他们只需设计用于处理键值对的 Map 函数来生成一组中间键值对，并设计 Reduce 函数来合并所有中间键。“地图”和“缩小”功能都维护地图缩小工作流。完成后或地图输出可用时，减少功能将开始执行代码。

它们的执行顺序如下:

![Understanding the basics of MapReduce](img/3282OS_02_00.jpg)

MapReduce 假设地图是独立的，并将并行执行它们。MapReduce 算法的关键方面是，如果每个 Map 和 Reduce 都独立于网络中所有其他正在进行的 Map 和 Reduce，则操作将在不同的键和数据列表上并行运行。

分布式文件系统将数据的多个副本分布在不同的机器上。这提供了可靠性和容错能力。如果有一个文件副本的机器崩溃，将从另一个复制的数据源提供相同的数据。

MapReduce 守护程序的主节点将负责 MapReduce 作业的所有职责，例如作业的执行、Mappers、Reducers、Combiners 和 Partitioners 的调度、单个作业任务的成功和失败的监控，以及最终批处理作业的完成。

Apache Hadoop 通过在 Hadoop 分布式文件系统上存储的数据附近的服务器上运行 Hadoop MapReduce 作业，以并行方式处理分布式数据。

使用 MapReduce 的公司包括:

*   **亚马逊**:这是一家面向大数据分析的在线电商和云 web 服务提供商
*   **易贝**:这是一个通过描述查找文章的电商门户
*   **谷歌**:这是一个网络搜索引擎，用于查找与特定主题相关的网页
*   **LinkedIn** :这是一个专业的大数据存储和生成个性化推荐的社交网站
*   **trobit**:这是一个垂直搜索引擎，用来寻找符合给定描述的工作
*   **推特**:这是一个寻找消息的社交网站

除此之外，还有许多其他品牌正在使用 Hadoop 进行大数据分析。

# 介绍 Hadoop MapReduce

基本上，MapReduce 模型可以用多种语言实现，但除此之外，Hadoop MapReduce 是一个流行的 Java 框架，适合容易编写的应用。它以可靠和容错的方式在大型商用硬件集群(数千个节点)上并行处理大量数据(多字节数据集)。这个 MapReduce 范例分为两个阶段，Map 和 Reduce，主要处理数据的键值对。地图和减少任务在集群中顺序运行，地图阶段的输出成为减少阶段的输入。

不能更新 MapReduce 中的所有数据输入元素。如果映射任务的输入`(key, value)`对发生变化，则不会反映在输入文件中。映射器输出将通过管道传输到适当的缩减器，该缩减器以键属性作为输入进行分组。在 Hadoop MapReduce 算法和 Hadoop 集群的帮助下，这个顺序数据过程将以并行方式进行。

MapReduce 程序将列表格式的输入数据集转换成也是列表格式的输出数据。这个逻辑列表转换过程在映射和缩减阶段通常会重复两次。我们也可以通过固定映射器和缩减器的数量来处理这些重复。在下一节中，将基于旧的 MapReduce 应用编程接口描述 MapReduce 概念。

## 列出 Hadoop MapReduce 实体

以下是负责对大数据执行分析的 Hadoop 组件:

*   **客户端**:这个初始化作业
*   **作业跟踪器**:这个监控作业
*   **任务跟踪器**:这个执行任务
*   **HDFS** :此存储输入输出数据

## 了解 Hadoop MapReduce 场景

Hadoop MapReduce 数据处理的四个主要阶段如下:

*   将数据加载到 HDFS
*   地图阶段的执行
*   洗牌和排序
*   减少阶段的执行

### 将数据加载到 HDFS

输入的数据集需要上传到 Hadoop 目录的中，以便 MapReduce 节点使用。然后， **Hadoop 分布式文件系统** ( **HDFS** )将输入数据集划分为数据分割，并通过考虑容错的复制因素将其存储到集群中的数据节点。所有数据分割将由任务跟踪器以并行方式处理地图和减少任务。

此外，还有一些使用 Hadoop 组件在 HDFS 获取数据集的替代方法:

*   **Sqoop** :这是一个开源工具，旨在高效地在 Apache Hadoop 和结构化关系数据库之间传输大量数据。假设您的应用已经配置了 MySQL 数据库，并且您希望使用相同的数据来执行数据分析，建议使用 Sqoop 将数据集导入 HDFS。此外，在数据分析过程完成后，输出可以导出到 MySQL 数据库。
*   **Flume** :这个是一个分布式的、可靠的、可用的服务，用于高效地收集、聚合和移动大量的日志数据到 HDFS。Flume 能够读取大多数来源的数据，例如日志文件、sys 日志和 Unix 进程的标准输出。

使用前面的数据收集和移动框架可以使数据传输过程对于数据分析的 MapReduce 应用非常容易。

### 执行地图阶段

执行客户端应用会启动 Hadoop MapReduce 进程。然后，映射阶段复制作业资源(未归档的类文件)并将其存储到 HDFS，并请求作业跟踪器执行该作业。作业跟踪器初始化作业，检索输入，分割信息，并为每个作业创建一个映射任务。

作业跟踪器将调用任务跟踪器在分配的输入数据子集上运行映射任务。映射任务将该输入分割数据读取为提供给映射器方法的输入`(key, value)`对，然后映射器方法产生中间的`(key, value)`对。每个输入`(key, value)`对至少有一个输出。

![Executing the Map phase](img/3282OS_02_01.jpg)

映射输入列表的单个元素

生成(键、值)对的列表，使得键属性将被重复多次。因此，它的关键属性将在 Reduce 中重新用于聚合 MapReduce 中的值。就格式而言，Mapper 输出格式值和 Reducer 输入值必须相同。

完成此映射操作后，任务跟踪器将把结果保存在其缓冲存储和本地磁盘空间中(如果输出数据大小超过阈值)。

例如，假设我们有一个`Map`函数，将输入文本转换为小写。这将把输入字符串列表转换成小写字符串列表。

### 类型

**键和值**:在 MapReduce 中，每个值都有其标识符，该标识符被认为是键。映射器接收的键值对取决于作业配置文件中指定的输入数据类型。

### 洗牌和排序

为了优化 MapReduce 程序，这个中间阶段非常重要。

一旦从映射阶段输出的映射器可用，这个中间阶段将被自动调用。映射阶段完成后，仅当分区器存在时，映射器端的分区器才会对所有发出的中间(键、值)对进行分区。分区器的输出将根据映射器端的键属性进行排序。排序操作的输出存储在映射器节点“任务跟踪器”的缓冲存储器中。

组合器通常是减速器本身。所以通过压缩，不是 **Gzip** 或者类似的压缩，而是地图输出数据的节点上的 Reducer。然后，合并器返回的数据被混洗并发送到精简节点。为了加快映射器输出到任务跟踪器的减速器插槽的数据传输，您需要使用`Combiner`功能压缩该输出。默认情况下，映射器输出将存储到缓冲内存中，如果输出大小大于阈值，它将存储到本地磁盘中。该输出数据将通过 **超文本传输协议** ( **HTTP** )获得。

### 减少阶段执行

一旦映射器输出可用，减速器节点中的任务跟踪器将检索可用的分区映射的输出数据，它们将被组合在一起并合并成一个大文件，然后用`Reducer`方法将其分配给一个进程。最后，这将在数据提供给`Reducer`方法之前进行整理。

`Reducer`方法从输入`(key, list (value))`接收输入值列表，并基于自定义逻辑对它们进行聚合，并产生输出`(key, value)`对。

![Reducing phase execution](img/3282OS_02_02.jpg)

将输入值减少到一个总值作为输出

减少阶段的`Reducer`方法的输出将按照 MapReduce 作业配置类指定的格式直接写入 HDFS。

## 了解 MapReduce 的局限性

让我们看看 Hadoop MapReduce 的一些局限性:

*   众所周知，MapReduce 框架很难用于不那么简单的转换逻辑，例如实时流、图形处理和消息传递。
*   与使用索引数据创建的数据库相比，分布式、未索引数据的数据查询效率较低。但是，如果生成了数据的索引，则需要在删除或添加数据时对其进行维护。
*   我们不能将缩减任务并行化为地图任务来减少整体处理时间，因为在地图任务的输出可用之前，缩减任务不会启动。(减速器的输入完全依赖于映射器的输出。)此外，我们无法控制地图和缩小任务的执行顺序。但是有时，基于应用逻辑，当数据收集在地图任务完成后立即开始时，我们肯定可以在实例上为缩减任务配置一个缓慢的开始。
*   长期运行的缩减任务无法完成，因为它们的资源利用率很低，如果缩减任务花费太多时间来完成并且失败，或者如果没有其他可用的缩减槽来重新计划它(这可以通过 Yarn 来解决)。

## 了解 Hadoop 解决问题的能力

因为这本书是面向分析师的，所以提供分析性的例子可能是相关的；例如，如果读者有一个类似于前面描述的问题，Hadoop 可能会有用。Hadoop 不是所有大数据问题的通用解决方案；当大数据需要分成小块并分布在需要并行处理的服务器上时，这是一种很好的技术。这节省了对庞大数据集执行分析的时间和成本。

如果我们能够为问题设计地图和减少阶段，就有可能用地图减少来解决。通常，Hadoop 提供计算能力来处理不适合机器内存的数据。(R 用户在处理大数据时大多发现错误消息，看到如下消息:无法分配大小为 2.5 GB 的向量。)

## 了解 Hadoop 编程中使用的不同 Java 概念

有一些经典的 Java 概念让 Hadoop 更具交互性。它们是如下:

*   **远程过程调用**:这是一种进程间通信，允许计算机程序在另一个地址空间(通常在共享网络上的另一台计算机上)执行一个子例程或过程，而无需程序员为这种远程交互明确编码。也就是说，无论子程序是在执行程序的本地还是远程，程序员编写的代码本质上都是相同的。
*   **序列化/反序列化**:有了序列化，一个 **Java 虚拟机** ( **JVM** )就可以把对象的状态写出到某个流中，这样我们基本上就可以读取所有的成员，把把它们的状态写出到一个流、磁盘等等。默认机制是二进制格式，因此比文本格式更紧凑。通过这种方式，机器可以通过网络发送数据。反序列化则相反，用于通过网络接收数据对象。
*   **Java 泛型**:这个允许一个类型或方法对各种类型的对象进行操作，同时提供编译时类型安全，使 Java 成为一种完全静态类型化的语言。
*   **Java 集合**:这个框架是一组类和接口，用单个 Java 对象处理各种类型的数据集合。
*   **Java 并发**:这个是为了支持并发编程而设计的，所有的执行都发生在线程的上下文中。它主要用于将计算进程实现为单个操作系统进程中的一组线程。
*   **普通旧 Java 对象** ( **POJO** ):这些其实是普通的 JavaBeans。POJO 暂时用于设置为以及来检索数据对象的值。

# 了解 Hadoop MapReduce 基础知识

为了正确理解 Hadoop MapReduce 的基本原理，我们将:

*   理解 MapReduce 对象
*   了解如何在 MapReduce 中决定地图的数量
*   了解如何在 MapReduce 中决定减少的数量
*   理解 MapReduce 数据流
*   仔细看看 Hadoop MapReduce 术语

## 理解 MapReduce 对象

正如我们所知， Hadoop 中的 MapReduce 操作主要由三个对象来执行:Mapper、Reduce 和 Driver。

*   **映射器**:这是为 MapReduce 的 Map 阶段设计的，它通过携带输入文件并将其拆分成若干块来启动 MapReduce 操作。对于每个片段，它将发出一个键值数据对作为输出值。
*   **减速器**:这个是为 MapReduce 作业的 Reduce 阶段设计的；它从 Mapper 输出中接受基于键的分组数据，通过聚合逻辑对其进行缩减，并为该组值发出`(key, value)`对。
*   **驱动程序**:这个是驱动 MapReduce 进程的主文件。它在从客户端应用获得带有参数的请求后开始执行 MapReduce 任务。驱动程序文件负责构建作业的配置，并将其提交给 Hadoop 集群。驱动程序代码将包含从命令行接受参数的`main()`方法。该程序将接受 Hadoop MapReduce 作业的输入和输出目录。驱动程序是定义作业配置详细信息的主要文件，如作业名称、作业输入格式、作业输出格式以及映射器、组合器、分区器和缩减器类。MapReduce 通过调用 Driver 类的这个`main()`函数进行初始化。

不是每一个问题都可以用单个 Map 和单个 Reduce 程序解决，但是用单个 Map 和单个 Reduce 任务解决不了的就更少了。有时，还需要设计具有多个地图和缩小任务的地图缩小作业。当我们需要在单个作业中一起执行数据操作时，例如数据提取、数据清理和数据合并，我们可以设计这种类型的作业。通过为单个作业编写多个映射器和缩减器任务，可以解决许多问题。在多个映射和缩减任务的情况下，将顺序调用的映射缩减步骤是映射 1 后接缩减 1，映射 2 后接缩减 2，依此类推。

当我们需要编写一个具有多个 Map 和 Reduce 任务的 MapReduce 作业时，我们必须编写多个 MapReduce 应用驱动程序来顺序运行它们。

在提交地图缩减作业时，我们可以提供许多地图任务，并将根据映射器输入的输出和 Hadoop 集群容量创建许多缩减器。此外，请注意，设置映射器和缩减器的数量不是强制性的。

## 决定 MapReduce 中地图的数量

地图的号是，通常由输入数据的大小和数据分割块的大小来定义，数据分割块的大小由 HDFS 文件/数据分割的大小来计算。因此，如果我们有一个 5 TB 的 HDFS 数据文件和一个 128 MB 的块大小，文件中将有 40，960 个映射。但是有时，由于推测性执行，创建的映射器的数量会超过这个计数。当输入是一个文件时，这是正确的，尽管它完全依赖于`InputFormat`类。

在 Hadoop MapReduce 处理中，当分配的映射器或缩减器需要很长时间才能完成时，作业的结果会有延迟。如果您想避免这种情况，Hadoop 中的推测执行可以在不同的节点上运行同一个 Map 或 Reduce 任务的多个副本，并且可以使用第一个完成的节点的结果。从带有`setNumMapTasks(int)`方法的 Hadoop API 中，我们可以了解 Mappers 的数量。

## 决定 MapReduce 中的 Reduce 数量

减速器的数量是根据映射器的输入创建的。但是，如果在 MapReduce 中硬编码 Reduce 的数量，集群中有多少节点并不重要。它将按照配置中的指定执行。

此外，我们可以在运行时使用命令提示符`-D mapred.reduce.tasks`中的 MapReduce 命令设置减速器的数量，以及您想要的数量。可通过`conf.setNumReduceTasks(int)`编程设置。

## 理解 MapReduce 数据流

现在我们已经看到了使一个基本的 MapReduce 作业成为可能的组件，我们将在更高的层次上区分一切是如何协同工作的。从下图中，我们将理解 Hadoop 集群中多个节点的 MapReduce 数据流:

![Understanding MapReduce dataflow](img/3282OS_02_03.jpg)

MapReduce 数据流

Hadoop MapReduce 可用的两个应用编程接口是:新的(Hadoop 1.x 和 2.x)和旧的 Hadoop (0.20)。Yarn 是下一代 Hadoop MapReduce 和新的 Apache Hadoop 子项目，已为 Hadoop 资源管理发布。

Hadoop 数据处理包括几项任务，有助于实现输入数据集的最终输出。这些任务如下:

1.  在 HDFS 预加载数据。
2.  通过调用驱动程序运行 MapReduce。
3.  映射器读取输入数据，这将导致映射器自定义逻辑的数据执行分离，并生成中间键值对
4.  执行合并器和洗牌阶段，以优化整个 Hadoop MapReduce 过程。
5.  对中间键值对进行排序并将其提供给缩减阶段。然后执行减少阶段。Reducer 获取这些分区的键值对，并基于 Reducer 逻辑对它们进行聚合。
6.  最终输出数据存储在 HDFS。

在这里，可以为多个数据操作定义映射和缩减任务，如下所示:

*   数据析取
*   数据加载
*   数据分段
*   数据清理
*   数据转换
*   数据集成

我们将在本章的下一部分更详细地探讨 MapReduce 任务。

## 仔细看看 Hadoop MapReduce 术语

在这一部分，我们将看到关于 Hadoop MapReduce 数据流的更多细节，包括几个 MapReduce 术语及其 Java 类细节。在上一节的 MapReduce 数据流图中，多个节点通过网络连接，使用 Hadoop 设置执行分布式处理。地图和缩减阶段的后续属性对获得最终输出起着重要作用。

地图阶段的属性如下:

*   `InputFiles` 术语是指已创建/提取用于商业分析的输入原始数据集，这些数据集已存储在 HDFS。这些输入文件非常大，有几种类型。
*   `InputFormat` 是一个 Java 类，通过获取每行偏移量的文本和内容来处理输入文件。它定义了如何分割和读取输入数据文件。我们可以设置与地图和缩小阶段相关的输入格式的几种输入类型，如`TextInputFormat`、`KeyValueInputFormat`和`SequenceFileInputFormat`。
*   `InputSplits` 类用于设置数据拆分的大小。
*   `RecordReader` 是一个 Java 类，它有几种方法通过在数据分割中迭代来检索键和值。此外，它还包括获取当前进度状态的其他方法。
*   为地图阶段创建`Mapper` 实例。`Mapper`类接受输入`(key, value)`对(由 RecordReader 生成)，并通过在`Map()`方法中执行用户定义的代码来产生中间的`(key, value)`对。`Map()`方式主要取两个输入参数:键和值；剩下的是`OutputCollector`和`Reporter. OutputCollector`。他们将提供中间的键值对，以减少工作的阶段。报告程序定期向作业跟踪器提供当前作业的状态。作业跟踪器将在作业结束时聚合它们，以便以后检索。

减少阶段的属性如下:

*   在完成映射阶段后，基于`hash`函数中的关键属性相似性考虑，对生成的中间`(key, value)`对进行划分。所以，每个地图任务可能会发出`(key, value)`对来划分；同一个键的所有值总是一起减少，而不考虑哪个映射器是它的原点。地图阶段完成后，地图缩减作业将自动完成这种分区和洗牌。没有必要单独给他们打电话。此外，我们可以根据 MapReduce 作业的要求显式覆盖它们的逻辑代码。
*   在完成分区和洗牌之后，初始化缩减任务之前，Hadoop MapReduce 作业基于关键属性值对中间的`(key, value)`对进行排序。
*   `Reduce`实例是为减少阶段创建的。它是用户提供的执行减少任务的代码部分。`Reducer`类的一个`Reduce()`方法主要带两个参数，以及`OutputCollector`和`Reporter`，与`Map()`功能相同。它们是`OutputCollector`和`Reporter`对象。“映射”和“缩减”中的`OutputCollector`具有相同的功能，但是在“缩减”阶段，`OutputCollector`提供输出到下一个“映射”阶段(在多个“映射”和“缩减”作业组合的情况下)，或者根据需求将其报告为作业的最终输出。除此之外，`Reporter`定期向作业跟踪器报告正在运行的任务的当前状态。
*   最后，在`OutputFormat`中，生成的输出(键、值)对被提供给`OutputCollector`参数，然后被写入由`OutputFormat`控制的`OutputFiles`。它控制地图缩减驱动程序中定义的`OutputFiles`格式的设置。格式将从`TextOutputFormat`、`SequenceFileOutputFileFormat`或`NullOutputFormat`中选择。
*   `OutputFormat`使用的工厂`RecordWriter`以适当的格式写入输出数据。
*   输出文件是 MapReduce 作业完成后`RecordWriter`写入 HDFS 的输出数据。

为了高效地运行这个 MapReduce 作业，我们需要了解一些 Hadoop shell 命令来执行管理任务。请参考下表:

<colgroup><col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

外壳命令

 | 

用法和代码示例

 |
| --- | --- |
| 

```r
cat

```

 | 要将源路径复制到`stdout`:

```r
Hadoop fs -cat URI [URI …]

```

 |
| 

```r
chmod

```

 | 要更改文件的权限:

```r
Hadoop fs -chmod [-R] <MODE[,MODE]... &#124; OCTALMODE> URI [URI …]

```

 |
| 

```r
copyFromLocal

```

 | 要将文件从本地存储复制到 HDFS:

```r
Hadoop fs –copyFromLocal<localsrc> URI

```

 |
| 

```r
copyToLocal

```

 | 要将文件从 HDFS 复制到本地存储，请执行以下操作:

```r
Hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>

```

 |
| 

```r
cp

```

 | 要将文件从源位置复制到 HDFS 的目标位置，请执行以下操作:

```r
Hadoop fs -cp URI [URI …] <dest>

```

 |
| 

```r
du

```

 | 要显示文件的总长度:

```r
Hadoop fs -du URI [URI …]

```

 |
| 

```r
dus

```

 | 要显示文件长度的摘要:

```r
Hadoop fs -dus<args>

```

 |
| 

```r
get

```

 | 要将文件复制到本地文件系统:

```r
Hadoop fs -get [-ignorecrc] [-crc] <src><localdst>

```

 |
| 

```r
ls

```

 | 要列出 HDFS 当前目录中的所有文件:

```r
Hadoop fs –ls<args>

```

 |
| 

```r
mkdir

```

 | 要在 HDFS 创建目录:

```r
Hadoop fs –mkdir<paths>

```

 |
| 

```r
lv

```

 | 要将文件从源移动到目标:

```r
Hadoop fs -mv URI [URI …] <dest>

```

 |
| 

```r
rmr

```

 | 要从当前目录中删除文件:

```r
Hadoop fs -rmr URI [URI …]

```

 |
| 

```r
setrep

```

 | 要更改文件的复制因子:

```r
Hadoop fs -setrep [-R] <path>

```

 |
| 

```r
tail

```

 | 将文件的最后一千字节显示到`stdout`:

```r
Hadoop fs -tail [-f] URI

```

 |

# 写一个 Hadoop MapReduce 的例子

现在，我们将通过学习一个非常常见且简单的字数示例来推进 MapReduce。这个例子的目标是计算每个单词在提供的文档中出现的次数。这些文档可以被认为是 MapReduce 文件的输入。

在这个例子中，我们已经有了一组文本文件——我们想要识别文件中存在的所有唯一单词的频率。我们将通过设计 Hadoop MapReduce 阶段来实现这一点。

在本节中，我们将看到更多关于使用 Hadoop MapReduce 的旧应用编程接口进行 Hadoop MapReduce 编程的信息。这里我们假设读者已经按照[第 1 章](1.html "Chapter 1. Getting Ready to Use R and Hadoop")、*准备使用 R 和 Hadoop* 中的描述设置了 Hadoop 环境。另外，请记住，我们不会使用 R 来计算单词；这里将只使用 Hadoop。

基本上，Hadoop MapReduce 有三个主要对象:映射器、缩减器和驱动程序。它们可以用三个 Java 类开发；它们是`Map`类、`Reduce`类和`Driver`类，其中`Map`类表示 Map 阶段，`Reducer`类表示 Reduce 阶段，`Driver`类表示用`main()`方法初始化 Hadoop MapReduce 程序的类。

在 Hadoop MapReduce 基础的上一节中，我们已经讨论了什么是映射器、减速器和驱动程序。现在，我们将学习如何定义它们，并用 Java 为它们编程。在接下来的章节中，我们将学习如何结合使用 R 和 Hadoop 做更多的事情。

### 类型

有许多语言和框架用于构建 MapReduce，但每种语言和框架都有不同的优势。有多种因素可以通过修改在 MapReduce 上提供高延迟。参考 Cloudera 在[http://blog.cloudera.com/blog/2009/05/10-mapreduce-tips/](http://blog.cloudera.com/blog/2009/05/10-mapreduce-tips/)发布的文章 *10 MapReduce 提示* 。

为了让 MapReduce 开发更容易，使用配置了 **Maven** 的 **Eclipse** ，支持旧的 MapReduce API。

## 了解运行 MapReduce 作业的步骤

让我们看看用 Hadoop 运行 MapReduce 作业的步骤:

1.  在准备 Java 类的最初步骤中，我们需要您根据我们业务问题的定义开发一个 Hadoop MapReduce 程序。在这个的例子中，我们考虑了一个字数问题。所以，我们为 MapReduce 程序开发了三个 Java 类；它们是`Map.java`、`Reduce.java`和`WordCount.java`，用于计算单词在提供的文本文件中的出现频率。
    *   `Map.java`:这是字数映射器的 Map 类。

        ```r
        // Defining package of the class
        package com.PACKT.chapter1;

        // Importing java libraries 
        import java.io.*;
        importjava.util.*;
        import org.apache.hadoop.io.*;
        import org.apache.hadoop.mapred.*;

        // Defining the Map class
        public class Map extends MapReduceBase implements
                 Mapper<LongWritable, 
                        Text, 
                        Text, 
                        IntWritable>{

        //Defining the map method – for processing the data with // problem specific logic
        public void map(LongWritable key,
                        Text value,
                        OutputCollector<Text,
                        IntWritable> output,
                        Reporter reporter) 
                        throws IOException {

        // For breaking the string to tokens and convert them to lowercase
        StringTokenizer st = new StringTokenizer(value.toString().toLowerCase());

        // For every string tokens
        while(st.hasMoreTokens()) {

        // Emitting the (key,value) pair with value 1.
        output.collect(new Text(st.nextToken()), 
                       new IntWritable(1));
                }

            }

        }
        ```

    *   `Reduce.java`:这个是字数缩减器的缩减类。

        ```r
        // Defining package of the class
        package com.PACKT.chapter1;

        // Importing java libraries
        import java.io.*;
        importjava.util.*;
        import org.apache.hadoop.io.*;
        importorg.apache.hadoop.mapred.*;

        // Defining the Reduce class 
        public class Reduce extends MapReduceBase implements
                  Reducer<Text,
                          IntWritable,
                          Text,
                          IntWritable> {

        // Defining the reduce method for aggregating the //generated output of Map phase
        public void reduce(Text key,
                           Iterator<IntWritable> values,
                           OutputCollector<Text,IntWritable>
                           output, 
                           Reporter reporter) throws IOException {

        // Setting initial counter value as 0
        int count = 0;

        // For every element with similar key attribute, increment its counter value by adding 1.
        while(values.hasNext()) {
        count += values.next().get();
                }

        // Emitting the (key,value) pair
        output.collect(key, new IntWritable(count));
            }
        }
        ```

    *   `WordCount.java`:这是 Hadoop MapReduce 驱动主文件中驱动的任务。

        ```r
        //Defining package of the class
        package com.PACKT.chapter1;

        // Importing java libraries
        import java.io.*;
        importorg.apache.hadoop.fs.*;
        import org.apache.hadoop.io.*;
        importorg.apache.hadoop.mapred.*;
        importorg.apache.hadoop.util.*;
        importorg.apache.hadoop.conf.*;

        //Defining wordcount class for job configuration 
          // information
        public class WordCount extends Configured implements Tool{

        publicint run(String[] args) throws IOException{
        JobConfconf = new JobConf(WordCount.class);
        conf.setJobName("wordcount");

        //For defining the output key format
        conf.setOutputKeyClass(Text.class);

        //For defining the output value format
        conf.setOutputValueClass(IntWritable.class);

        // For defining the Mapper class implementation
        conf.setMapperClass(Map.class);

        // For defining the Reducer class implementation
        conf.setReducerClass(Reduce.class);

        // For defining the type of input format 
        conf.setInputFormat(TextInputFormat.class);

        // For defining the type of output format
        conf.setOutputFormat(TextOutputFormat.class);

        // For defining the command line argument sequence for // input dataset path
        FileInputFormat.setInputPaths(conf, new Path(args[0]));

        // For defining the command line argument sequence for // output dataset path
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));

        // For submitting the configuration object
        JobClient.runJob(conf);

        return 0;
            }

        // Defining the main() method to start the execution of // the MapReduce program
        public static void main(String[] args) throws Exception {
          intexitCode = ToolRunner.run(new WordCount(), args);
          System.exit(exitCode); } }
        ```

2.  编译 Java 类。

    ```r
    // create a folder for storing the compiled classes
    hduser@ubuntu:~/Desktop/PacktPub$ mkdir classes

    // compile the java class files with classpath
    hduser@ubuntu:~/Desktop/PacktPub$ javac -classpath /usr/local/hadoop/hadoop-core-1.1.0.jar:/usr/local/hadoop/lib/commons-cli-1.2.jar -d classes *.java

    ```

3.  从编译的类中创建一个`.jar`文件。

    ```r
    hduser@ubuntu:~/Desktop/PacktPub$ cd classes/

    // create jar of developed java classes
    hduser@ubuntu:~/Desktop/PacktPub/classes$ jar -cvf wordcount.jar com

    ```

4.  启动 Hadoop 守护程序。

    ```r
    // Go to Hadoop home Directory
    hduser@ubuntu:~$ cd $HADOOP_HOME

    // Start Hadoop Cluster
    hduser@ubuntu:/usr/local/hadoop$ bin/start-all.sh

    ```

5.  检查所有正在运行的守护程序。

    ```r
    // Ensure all daemons are running properly 
    hduser@ubuntu:/usr/local/hadoop$ jps

    ```

6.  创建 HDFS 目录`/wordcount/input/`。

    ```r
    // Create Hadoop directory for storing the input dataset
    hduser@ubuntu:/usr/local/hadoop$ bin/Hadoop fs -mkdir /wordcount/input

    ```

7.  提取要在字数统计示例中使用的输入数据集。由于我们需要通过字数统计示例来处理文本文件，因此我们将使用 Hadoop 发行版提供的文本文件(`CHANGES.txt`、`LICENSE.txt`、`NOTICE.txt`和`README.txt`)将其复制到 Hadoop 目录中。我们可以在这个 MapReduce 算法中使用来自互联网输入的其他文本数据集，而不是使用现成的文本文件。我们也可以从互联网上提取数据来处理它们，但是这里我们使用的是现成的输入文件。
8.  将所有文本文件复制到 HDFS。

    ```r
    // To copying the text files from machine's local
     // directory in to Hadoop directory

    hduser@ubuntu:/usr/local/hadoop$ bin/hadoopfs -copyFromLocal $HADOOP_HOME/*.txt /wordcount/input/

    ```

9.  使用以下命令运行 Hadoop MapReduce 作业:

    ```r
    // Command for running the Hadoop job by specifying jar, main class, input directory and output directory.

    hduser@ubuntu:/usr/local/hadoop$ bin/hadoop jar wordcount.jar com.PACKT.chapter1.WordCount /wordcount/input/ /wordcount/output/

    ```

10.  This is how the final output will look.

    ```r
    // To read the generated output from HDFS directory

    hduser@ubuntu:/usr/local/hadoop$ bin/hadoopfs -cat /wordcount/output/part-00000

    ```

    ### 类型

    在 MapReduce 阶段，您需要监控作业和节点。使用以下内容监控网络浏览器中的 MapReduce 作业:

    *   本地主机:50070:名称节点网络接口(适用于 HDFS)
    *   `localhost:50030` : JobTracker Web 界面(针对 MapReduce 层)
    *   `localhost:50060`:taskstracker Web 界面(针对 MapReduce 层)

### 学习监控和调试 Hadoop MapReduce 作业

在本节中，我们将学习如何在没有任何命令的情况下监控和调试 Hadoop MapReduce 作业。

这是使用 Hadoop MapReduce 管理用户界面最简单的方法之一。我们可以通过浏览器输入网址`http://localhost:50030`(作业跟踪器守护程序的网络用户界面)来访问它。这将显示 Hadoop MapReduce 作业的日志信息，如下图所示:

![Learning to monitor and debug a Hadoop MapReduce job](img/3282OS_02_04.jpg)

映射/减少管理

在这里我们可以查看正在运行的作业的信息和状态，一个作业的 Map 和 Reduce 任务的状态，过去已完成的作业以及 Map 和 Reduce 任务失败的失败作业。此外，我们可以通过单击失败地图的超链接或失败作业的缩减任务来调试地图缩减作业。这将在作业运行时在标准输出上产生一条错误消息。

### 探索 HDFS 数据

在本节中，我们将看到如何在不运行任何 **Bash** 命令的情况下探索 HDFS 目录。名称节点守护程序的网络用户界面提供了这样的功能。我们只需要在`http://localhost:50070`找到它。

![Exploring HDFS data](img/3282OS_02_05.jpg)

名称节点管理

该用户界面使我们能够获得集群摘要(内存状态)、名称节点日志以及集群中活节点和死节点的信息。此外，这允许我们探索为存储 Hadoop MapReduce 作业的输入和输出数据而创建的 Hadoop 目录。

## 理解几种可能的 MapReduce 定义来解决业务问题

到目前为止，我们已经了解了什么是 MapReduce 以及如何对其进行编码。现在，我们将看到一些用于业务分析的常见 MapReduce 问题定义。任何了解 Hadoop 的 MapReduce 的读者都可以通过修改字数的 MapReduce 示例来轻松编码和解决这些问题定义。主要的变化将是数据解析和数据操作背后的逻辑。主要工作将需要在数据收集、数据清理和数据存储方面。

*   **服务器 web 日志处理**:通过这个 MapReduce 定义，我们可以进行 web 日志分析。web 服务器的日志提供了有关 web 请求的信息，如请求页面的 URL、日期、时间和协议。由此，我们可以从 web 服务器日志中识别出我们网站的峰值负载时间，并根据网站的流量对我们的 web 服务器配置进行缩放。因此，识别夜间无流量将有助于我们通过缩小服务器规模来节省资金。此外，还有许多业务案例可以通过 web 日志服务器分析来解决。
*   **带有网站统计的 Web 分析**:网站统计可以提供更详细的关于访问者元数据的信息，例如来源、活动、访问者类型、访问者位置、搜索关键字、请求的页面 URL、浏览器和花费在页面上的总时间。谷歌分析是最受欢迎的免费网站服务提供商之一。通过分析所有这些信息，我们可以了解访问者在网站上的行为。通过描述性分析，我们可以根据访问者对网页或其他网络属性的上瘾程度来识别它们的重要性。对于一个电子商务网站，我们可以根据访问总数、页面浏览量和访问者在页面上花费的时间来识别受欢迎的产品。此外，预测分析可以在网络数据上实现，以预测业务。
*   **搜索引擎**:假设我们有一大堆文档，想要在文档中搜索一个特定的关键词，使用 Hadoop MapReduce 的倒排索引将帮助我们找到关键词，这样我们就可以构建一个大数据的搜索引擎。
*   **股市分析**:假设我们已经收集了很长一段时间的股市数据(大数据)，现在想要识别模式，并对下一个时间段进行预测。这需要训练所有历史数据集。然后，我们可以使用几个带有 Hadoop MapReduce 的机器学习库来计算上述时间段的股市变化频率。

此外，有太多可能的 MapReduce 应用可以用来提高业务成本。

# 学习用 R 写 Hadoop MapReduce 的不同方法

我们知道，使用 MapReduce 进行 Hadoop 大数据处理对统计人员、网络分析师和产品经理来说是一件大事，他们过去使用 R 工具进行分析，因为使用 Hadoop 将分析迁移到 MapReduce 需要 MapReduce 的补充编程知识。此外，我们知道 R 是一个不断普及的工具；有很多正在开发的用于与 R 集成的包/库，所以为了开发一个能够以 R 的日志和 Hadoop 的计算能力运行的 MapReduce 算法或程序，我们需要 R 和 Hadoop 的中间件。rhadop、RHIPE 和 Hadoop streaming 是帮助在 r 内开发和执行 Hadoop MapReduce 的中间件，在这最后一节，我们将讨论 rhadop、RHIPE 和介绍 Hadoop streaming，从后面的章节开始，我们将纯粹用这些包开发 MapReduce。

## 学习 RHadoop

RHadoop 是一个伟大的 R 开源软件框架，用于通过 R 函数在 Hadoop 平台上执行数据分析。RHadoop 由 **Revolution Analytics** 开发，该公司是基于统计计算开源 R 项目的领先商业软件和服务提供商。RHadoop 项目有三个不同的 R 包:`rhdfs`、`rmr`和`rhbase`。所有这些包都在 Cloudera Hadoop 发行版 CDH3、CDH4 和 R 2.15.0 上实现和测试。此外，这些都是用革命分析的 4.3、5.0 和 6.0 版本测试的。

这三个不同的 R 包是基于 Hadoop 的两个主要功能 HDFS 和 MapReduce 设计的:

*   `rhdfs`:这是一个 R 包，提供所有 Hadoop HDFS 对 R 的访问，所有分布式文件都可以用 R 函数管理。
*   `rmr`:这是一个 R 包，为 R 提供 Hadoop MapReduce 接口，借助这个包，可以轻松开发 Mapper 和 Reduce。
*   `rhbase`:这个是一个 R 包，通过 R 在 HBase 分布式数据库处理数据。

## 学习 RHIPE

**R 和 Hadoop 集成编程环境** ( **RHIPE** )是一个免费的开源项目。RHIPE 广泛用于执行带有 **D & R** 分析的大数据分析。D & R 分析用于分割巨大的数据，在分布式网络上并行处理产生中间输出，最后将所有这些中间输出重组为一个集合。RHIPE 旨在 Hadoop 平台上对 R 中的复杂大数据进行 D & R 分析。RHIPE 是由 Saptarshi Joy Guha(Mozilla Corporation 的数据分析师)和她的团队开发的，作为她在普渡统计部门博士论文的一部分。

## 学习 Hadoop 流

Hadoop 流是 Hadoop 发行版附带的实用程序。该实用程序允许您使用任何可执行文件或脚本作为映射器和/或缩减器来创建和运行映射缩减作业。R、Python、Ruby、Bash、Perl 等都支持这一点。我们将使用带有 bash 脚本的 R 语言。

此外，还有一个名为`HadoopStreaming`的 R 包，它是为了在 R 脚本的帮助下对 Hadoop 集群进行数据分析而开发的，这是一个使用 R 进行 Hadoop 流的接口。此外，它还允许在没有 Hadoop 的情况下运行 MapReduce 任务。这个包是由传感网络公司的首席科学家大卫·罗森伯格(T2)开发的。他在机器学习和统计建模方面有专长。

# 总结

在这一章中，我们已经看到了什么是 Hadoop MapReduce，以及如何开发和运行它。在下一章中，我们将学习如何安装 RHIPE 和 RHadoop，并通过示例开发 MapReduce 及其可用的函数库。