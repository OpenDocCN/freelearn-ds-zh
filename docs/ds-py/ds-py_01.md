# 第一章 编程与数据科学——一种新工具集

> “数据是宝贵的，它将比系统本身存在得更久。”

– *蒂姆·伯纳斯-李*，*万维网的发明人*

([`en.wikipedia.org/wiki/Tim_Berners-Lee`](https://en.wikipedia.org/wiki/Tim_Berners-Lee))

在这一章节的开始，我将尝试回答一些基础问题，希望这些问题能为本书的后续内容提供背景和清晰度：

+   什么是数据科学以及为什么它正在兴起

+   为什么数据科学将长久存在

+   为什么开发人员需要参与数据科学

结合我作为开发人员和最近的数据科学实践者的经验，我将讨论一个具体的数据管道项目，以及从这个项目中衍生出的一种数据科学策略，这个策略由三大支柱组成：数据、服务和工具。最后，我将介绍 Jupyter Notebooks，它是我在本书中提出的解决方案的核心。

# 什么是数据科学

如果你在网上搜索“数据科学”的定义，你一定会找到很多不同的解释。这反映了数据科学对不同的人意味着不同的东西。对于数据科学家究竟做什么以及他们必须接受什么样的训练，实际上并没有达成一致意见；这完全取决于他们要完成的任务，例如数据收集与清洗、数据可视化等。

现在，我会尝试使用一个普遍且希望得到共识的定义：*数据科学指的是分析大量数据，以提取知识和见解，从而做出可操作的决策*。不过这仍然有些模糊；人们可以问，我们到底在谈论什么样的知识、见解和可操作的决策？

为了引导话题，让我们将数据科学的领域范围缩小到三个领域：

+   **描述性分析**：数据科学与信息检索和数据收集技术相关，目的是重建过去的事件，以识别模式并发现有助于理解发生了什么以及是什么导致其发生的见解。一个例子是查看销售数据和按地区划分的人口统计信息，以便对客户偏好进行分类。这部分需要熟悉统计学和数据可视化技术。

+   **预测性分析**：数据科学是一种预测当前正在发生或将来会发生某些事件的可能性的方法。在这种情况下，数据科学家会查看过去的数据，找出解释变量，并构建可以应用于其他数据点的统计模型，以预测其结果。例如，实时预测信用卡交易是否存在欺诈行为。这部分通常与机器学习领域相关。

+   **规范性分析**：在这种情况下，数据科学被看作是一种做出更好决策的方法，或者我应该说是基于数据的决策。这个想法是，查看多个选项，并使用模拟技术量化并最大化结果，例如，通过优化供应链来最小化运营成本。

从本质上讲，描述性数据科学回答了*什么*（数据告诉我什么），预测性数据科学回答了*为什么*（数据为什么以某种方式表现），而规范性数据科学回答了*如何*（我们如何将数据优化以实现特定目标）。

# 数据科学会一直存在吗？

让我们从一开始就直截了当地说：我坚信答案是肯定的。

然而，情况并非总是如此。几年前，当我第一次听说数据科学作为一个概念时，我最初以为这只是另一个营销术语，用来描述行业中已经存在的活动：**商业智能**（**BI**）。作为一名开发者和架构师，主要解决复杂的系统集成问题，我很容易说服自己不需要直接参与数据科学项目，尽管显然它们的数量在不断增加，原因是开发者传统上将数据管道视为可以通过明确定义的 API 访问的黑箱。然而，在过去的十年里，我们看到数据科学在学术界和工业界的兴趣呈指数级增长，直到它变得非常清楚，这种模型将无法持续下去。

随着数据分析在公司运营过程中扮演着越来越重要的角色，开发者的角色也扩展到了更接近算法的部分，并构建能够在生产环境中运行它们的基础设施。数据科学已成为新的*淘金热*的另一个证据就是数据科学家职位的非凡增长，这些职位连续两年在 Glassdoor 上排名第一（[`www.prnewswire.com/news-releases/glassdoor-reveals-the-50-best-jobs-in-america-for-2017-300395188.html`](https://www.prnewswire.com/news-releases/glassdoor-reveals-the-50-best-jobs-in-america-for-2017-300395188.html)），并且在 Indeed 上经常是雇主发布最多的职位。猎头也在 LinkedIn 和其他社交媒体平台上活跃，向任何拥有数据科学技能资料的人发送大量招聘信息。

所有这些新技术背后投资的主要原因之一，是希望它们能为企业带来显著的改进和更高的效率。然而，尽管这是一个正在发展的领域，今天企业中的数据科学仍然局限于实验，而没有像大家预期的那样成为核心活动。这使得许多人开始怀疑，数据科学是否只是一个短暂的潮流，最终会消退，成为又一个技术泡沫，最终破灭，留下许多人被抛在后头。

这些都是很好的观点，但我很快意识到这不仅仅是一个昙花一现的潮流；我所领导的越来越多的项目开始将数据分析融入到核心产品功能中。最终，当 IBM Watson 问答系统在*危险边缘*游戏中战胜了两位经验丰富的冠军时，我深信数据科学、云计算、大数据和**人工智能**（**AI**）将会长久存在，并最终改变我们对计算机科学的理解。

# 为什么数据科学在崛起？

数据科学的迅速崛起涉及多个因素。

首先，收集的数据量正在以指数级的速度增长。根据 IBM 营销云的最新市场研究（[`www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=WRL12345GBEN`](https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=WRL12345GBEN)），每天大约会产生 2.5 万亿字节的数据（为了让你更清楚它有多大，这相当于 25 亿亿字节），但这些数据中只有极小的一部分会被分析，导致错失了大量的机会。

其次，我们正处于一场几年前开始的认知革命中；几乎所有行业都在纷纷加入 AI 的浪潮，这其中包括**自然语言处理**（**NLP**）和机器学习。尽管这些领域已经存在了很长时间，但最近它们获得了重新的关注，以至于现在它们已经成为大学中最受欢迎的课程之一，并且占据了开源活动的大部分份额。显然，如果企业想要生存下去，它们需要变得更加敏捷，更快地行动，并转型为数字化企业。而随着决策时间的缩短，几乎接近实时，它们必须完全依赖数据。再加上 AI 算法需要高质量（并且大量的）数据才能正常工作，我们可以开始理解数据科学家所扮演的关键角色。

第三，随着云技术的进步和**平台即服务**（**PaaS**）的发展，访问海量计算引擎和存储变得前所未有的简单且廉价。曾经只有大型企业才能承担的大数据工作负载，现在对小型组织或任何拥有信用卡的个人都可用；这反过来又推动了各领域创新的增长。

正因如此，我毫不怀疑，类似于人工智能革命，数据科学将长期存在，而且它的增长将持续很长时间。但我们也不能忽视数据科学尚未充分发挥其潜力，并且未能产生预期的结果，特别是在帮助公司转型为数据驱动型组织方面。最常见的挑战是实现下一步，即将数据科学和分析转化为核心业务活动，最终推动清晰、智能的“赌命”决策。

# 这与开发者有什么关系？

这是一个非常重要的问题，我们将在接下来的章节中花费大量时间探讨。让我先回顾一下我的职业生涯；我大部分时间作为开发者，回溯到 20 多年前，参与了计算机科学的多个方面。

我开始通过构建各种工具来帮助软件国际化，自动化将用户界面翻译成多种语言的过程。随后，我在 Eclipse 中开发了一个 LotusScript（Lotus Notes 的脚本语言）编辑器，该编辑器可以直接与底层编译器进行交互。这个编辑器提供了第一流的开发功能，例如内容提示（提供建议）、实时语法错误报告等等。接着，我花了几年时间为 Lotus Domino 服务器构建基于 Java EE 和 OSGI 的中间件组件（[`www.osgi.org`](https://www.osgi.org)）。在此期间，我带领团队通过将其引入当时可用的最新技术来现代化 Lotus Domino 编程模型。我对软件开发的各个方面都很熟悉，包括前端、中间件、后端数据层、工具等；我可以说是一个全栈开发者。

直到我看到 IBM Watson 问答系统的演示，2011 年它在《危险边缘！》节目中击败了长期冠军 Brad Rutter 和 Ken Jennings。哇！这真是一个突破性进展，一个能够回答自然语言问题的计算机程序。我非常感兴趣，在做了一些研究，拜访了几个参与该项目的研究人员，并了解了构建此系统所用的技术，如 NLP、机器学习和通用数据科学后，我意识到，如果将这项技术应用到商业的其他领域，它将有巨大的潜力。

几个月后，我有机会加入 IBM 新成立的 Watson 部门，领导一个工具团队，任务是为 Watson 系统构建数据摄取和准确性分析功能。我们的一个重要需求是确保工具对客户易于使用，这也是为什么回头看，将这个责任交给一个开发团队是正确的决定。从我的角度来看，接手这个职位既具挑战性又充满收获。我离开了一个熟悉的世界，在那里我擅长设计基于常见模式的架构，实施前端、中间件或后端软件组件，进入了一个主要聚焦于大数据工作的世界：获取数据、清洗数据、分析数据、可视化数据并构建模型。我花了前六个月像从火管中喝水一样，阅读和学习自然语言处理（NLP）、机器学习、信息检索和统计数据科学，至少足以能够参与我所构建的功能。

正是在那个时候，我与研究团队合作将这些算法推向市场，我意识到开发人员和数据科学家需要更好地协作。传统的方法是让数据科学家单独解决复杂的数据问题，然后将结果“抛给”开发人员，由他们来实现这些结果。但考虑到数据处理量不断呈指数级增长，以及市场所需时间日益缩短，这种做法是不可持续的，且无法扩展。

相反，他们的角色需要转变为作为一个团队共同工作，这意味着数据科学家必须像软件开发人员一样思考和工作，反之亦然。事实上，这在理论上看起来非常好：一方面，数据科学家将从成熟的软件开发方法论中受益，例如敏捷开发——其快速迭代和频繁反馈的方式——同时也能从严谨的软件开发生命周期中获益，带来符合企业需求的合规性，例如安全性、代码审查、源代码控制等。另一方面，开发人员将开始以全新的方式思考数据：将其视为用于发现洞察的分析，而不仅仅是具有查询和**CRUD**（即**创建**、**读取**、**更新**、**删除**）API 的持久层。

# 将这些概念付诸实践

在担任 Watson 核心工具首席架构师并为 Watson 问答系统构建自助工具四年后，我加入了 IBM Watson 数据平台组织的开发者倡导团队，后者的使命是创建一个平台，将数据和认知服务的产品组合带到 IBM 公共云。我们的任务相当简单：赢得开发者的心，并帮助他们在数据和 AI 项目中取得成功。

这项工作有多个维度：教育、宣传和激进主义。前两者比较直接，但激进主义的概念与这次讨论相关，并且值得详细解释。顾名思义，激进主义是关于在需要改变的地方带来变革。对于我们 15 位开发者倡导者的团队来说，这意味着要站在开发者的角度思考他们在处理数据时的体验——无论他们是刚刚开始，还是已经在应用高级算法——感受他们的痛点并识别应该解决的空白。为此，我们构建并开源了多个带有真实场景案例的数据管道示例。

至少，这些项目需要满足三个基本要求：

+   用作输入的原始数据必须是公开可用的

+   提供清晰的指令，确保数据管道可以在合理时间内部署到云端

+   开发者应该能够将该项目作为类似场景的起点，即代码必须具备高度的可定制性和可重用性

从这些练习中获得的经验和洞察是无价的：

+   理解哪些数据科学工具最适合每项任务

+   最佳实践框架和语言

+   部署和操作分析的最佳实践架构

指导我们选择的指标有很多：准确性、可扩展性、代码重用性，但最重要的是，改善数据科学家与开发者之间的协作。

# 深入探讨一个具体的例子

最初，我们希望构建一个数据管道，通过对包含特定标签的推文进行情感分析，提取 Twitter 中的洞察，并将结果部署到实时仪表板上。这个应用程序对我们来说是一个完美的起点，因为数据科学分析并不复杂，且该应用程序涵盖了许多现实场景的方面：

+   高流量、高吞吐量的流式数据

+   通过情感分析 NLP 进行数据增强

+   基本数据聚合

+   数据可视化

+   部署到实时仪表板

为了尝试这个方案，第一次实现是一个简单的 Python 应用程序，使用了 tweepy 库（Python 的官方 Twitter 库：[`pypi.python.org/pypi/tweepy`](https://pypi.python.org/pypi/tweepy)）来连接 Twitter，获取一系列的推文流，以及 textblob 库（用于基本 NLP 的简单 Python 库：[`pypi.python.org/pypi/textblob`](https://pypi.python.org/pypi/textblob)）进行情感分析的丰富。

然后，结果被保存到 JSON 文件中进行分析。这个原型是启动项目并快速实验的一个绝佳方式，但经过几轮迭代后，我们迅速意识到，需要认真构建一个满足企业需求的架构。

# 数据管道蓝图

从高层次来看，数据管道可以通过以下通用蓝图来描述：

![Data pipeline blueprint](img/B09699_01_01.jpg)

数据管道工作流

数据管道的主要目标是将数据科学分析结果转化为可操作的（即*提供直接的业务价值*）输出，并且在可扩展、可重复的过程中实现高度自动化。分析的例子可以是一个推荐引擎，用于激励消费者购买更多产品，例如，亚马逊的推荐列表，或者一个展示**关键绩效指标**（**KPI**）的仪表板，帮助 CEO 为公司的未来决策提供依据。

构建数据管道的过程中涉及多个角色：

+   **数据工程师**：他们负责设计和运营信息系统。换句话说，数据工程师负责与数据源对接，获取原始数据，然后进行处理（有些人称之为数据清洗），直到数据准备好进行分析。在亚马逊推荐系统的例子中，他们会实现一个流处理管道，捕捉并聚合来自电商系统的特定消费者交易事件，并将这些数据存储到数据仓库中。

+   **数据科学家**：他们分析数据并构建提取洞察的分析模型。在亚马逊推荐系统的例子中，他们可能会使用 Jupyter Notebook 连接到数据仓库，加载数据集，并使用例如协同过滤算法来构建推荐引擎（[`en.wikipedia.org/wiki/Collaborative_filtering`](https://en.wikipedia.org/wiki/Collaborative_filtering)）。

+   **开发人员**：他们负责将分析功能转化为面向业务用户（如业务分析师、高层管理人员、最终用户等）的应用程序。在亚马逊推荐系统中，开发人员将在用户完成购买后或通过定期电子邮件呈现推荐的产品列表。

+   **业务用户**：指的是所有使用数据科学分析输出的用户，例如，业务分析师分析仪表板以监控业务健康状况，或最终用户使用一个提供推荐的应用程序来决定下一步购买什么。

### 注意

在现实生活中，通常同一个人可能会扮演多个角色；这意味着一个人在与数据管道互动时可能有多个不同的需求。

正如前面的图示所示，构建数据科学管道是一个迭代过程，并遵循一个明确定义的流程：

1.  **获取数据**：此步骤包括从各种来源获取原始数据：结构化（关系数据库、记录系统等）或非结构化（网页、报告等）：

    +   **数据清洗**：检查数据完整性，填补缺失数据，修复不正确的数据，以及数据预处理

    +   **数据准备**：丰富数据，检测/移除异常值，并应用业务规则

1.  **分析**：此步骤结合了描述性（理解数据）和规范性（构建模型）活动：

    +   **探索**：发现统计特性，例如集中趋势、标准差、分布，以及变量识别，如单变量和双变量分析、变量之间的相关性等。

    +   **可视化**：这个步骤对于正确分析数据和形成假设至关重要。可视化工具应该提供合理的互动性，以便帮助理解数据。

    +   **构建模型**：应用推断统计学来形成假设，例如为模型选择特征。这个步骤通常需要专家领域知识，并且有很大的解释空间。

1.  **部署**：将分析阶段的结果转化为可操作的实际应用：

    +   **沟通**：生成报告和仪表板，清晰地传达分析结果，以供业务用户（如高层管理人员、业务分析师等）使用

    +   **发现**：设定一个聚焦于发现新见解和商业机会的业务目标，这些见解和机会可能会带来新的收入来源

    +   **实施**：为最终用户创建应用程序

1.  **测试**：这个活动应该贯穿于每一步，但在这里我们讨论的是创建一个来自实际应用的反馈循环：

    +   创建衡量模型准确性的指标

    +   优化模型，例如获取更多数据、寻找新特征等

# 成为数据科学家需要哪些技能？

在行业中，现实情况是，数据科学如此新兴，以至于公司还没有为其制定出明确的职业发展路径。那么，如何才能获得数据科学家的职位呢？需要多少年的经验？需要具备哪些技能？数学、统计学、机器学习、信息技术、计算机科学，还有什么？

其实，答案可能是“稍微懂一点儿各个领域”再加上一项关键技能：领域特定的专业知识。

目前有一种争论，关于是否在没有深入了解数据意义的情况下，将通用的数据科学技术应用于任何数据集，能否实现期望的商业结果。许多公司倾向于确保数据科学家具有足够的领域专业知识，其理由是，没有这一点，你可能会在任何步骤中无意间引入偏差，比如在数据清洗阶段填补空缺时，或在特征选择过程中，最终构建的模型可能非常适合某个数据集，但依然没有实际价值。想象一下，一个没有化学背景的数据科学家，正在为一家制药公司研究不需要的分子相互作用，帮助其开发新药。或许这也是为什么我们看到越来越多专门针对某一领域的统计学课程，例如生物学的生物统计学，或是供应链分析，专注于分析与供应链相关的运营管理等。

总结来说，一名数据科学家在理论上应该对以下领域有所熟练掌握：

+   数据工程 / 信息检索

+   计算机科学

+   数学与统计学

+   机器学习

+   数据可视化

+   商业智能

+   领域专业知识

### 注意事项

如果你正在考虑获取这些技能，但没有时间参加传统的课堂学习，我强烈推荐使用在线课程。

我特别推荐这门课程：[`www.coursera.org/`](https://www.coursera.org/): [`www.coursera.org/learn/data-science-course`](https://www.coursera.org/learn/data-science-course)。

Drew 的 Conway 维恩图提供了一个关于什么是数据科学以及为什么数据科学家有点像独角兽的优秀可视化图示：

![成为数据科学家需要什么样的技能？](img/B09699_01_02.jpg)

Drew 的 Conway 数据科学维恩图

到目前为止，我希望你已经明白，符合上述描述的完美数据科学家更像是例外而非常态，而且通常这个角色涉及多个身份。没错，我想表达的观点是，*数据科学是一项团队运动*，这个观点将在本书中反复出现。

# IBM Watson DeepQA

有一个项目很好地说明了数据科学是一项团队运动的观点，那就是 IBM DeepQA 研究项目。这个项目最初是 IBM 发起的一个重大挑战，目的是构建一个能够回答自然语言问题的人工智能系统，依据预定的领域知识。**问答（QA）**系统应该足够好，能够与《危险边缘！》这档受欢迎的电视游戏节目中的人类选手竞争。

众所周知，这个名为 IBM Watson 的系统在 2011 年赢得了比赛，战胜了两位最有经验的*危险边缘！*冠军：Ken Jennings 和 Brad Rutter。以下照片来自 2011 年 2 月播出的实际比赛：

![IBM Watson DeepQA](img/B09699_01_03.jpg)

IBM Watson 与 Ken Jennings 和 Brad Rutter 在《危险边缘！》上的对决！

来源：https://upload.wikimedia.org/wikipedia/e

正是在与构建 IBM Watson QA 计算机系统的研究团队互动时，我才得以更深入地了解 DeepQA 项目架构，并亲身体验到许多数据科学领域实际上是如何被应用的。

以下图示描述了 DeepQA 数据管道的高层次架构：

![IBM Watson DeepQA](img/B09699_01_04.jpg)

Watson DeepQA 架构图

来源：https://researcher.watson.ibm.com/researcher/files/us-mi

如上图所示，回答问题的数据管道由以下高层次步骤组成：

1.  **问题与主题分析（自然语言处理）**：此步骤使用一个深度解析组件，检测构成问题的单词之间的依赖关系和层次结构。目标是更深入地理解问题，并提取出基本属性，例如：

    +   **重点**：这个问题到底是在问什么？

    +   **词汇答案类型（**LAT**）**：期望答案的类型，例如一个人，一个地方等等。在候选答案的评分过程中，这些信息非常重要，因为它为与 LAT 不匹配的答案提供了早期过滤。

    +   **命名实体解析**：将一个实体解析为标准化的名称，例如，将“Big Apple”解析为“New York”。

    +   **指代解析**：将代词与问题中的先前术语连接起来，例如，在句子“1715 年 9 月 1 日，路易十四在这座城市去世，**他**建造了这座宏伟的宫殿”中，代词“他”指代的是路易十四。

    +   **关系检测**：检测问题中的关系，例如，“她在 1954 年与乔·迪马吉奥离婚”，其中的关系是“乔·迪马吉奥娶了 X”。这类关系（主语->谓语->宾语）可以用于查询三元组存储，并得到高质量的候选答案。

    +   **问题分类**：将问题映射到*Jeopardy!*中预定义的类型之一，例如，事实类问题、多项选择题、谜题等。

1.  **初步搜索与假设生成（信息检索）**：这一步骤主要依赖于问题分析步骤的结果，构建一组适应不同答案来源的查询。答案来源的一些例子包括各种全文搜索引擎，如 Indri（[`www.lemurproject.org/indri.php`](https://www.lemurproject.org/indri.php)）和 Apache Lucene/Solr（[`lucene.apache.org/solr`](http://lucene.apache.org/solr)），面向文档和标题的搜索（Wikipedia），三元组存储等等。搜索结果随后用于生成候选答案。例如，面向标题的结果将直接作为候选答案，而文档搜索则需要对段落进行更详细的分析（再次使用 NLP 技术）以提取可能的候选答案。

1.  **假设与证据评分（NLP 和信息检索）**：对于每个候选答案，进行另一次搜索以寻找更多支持证据，并使用不同的评分技术。此步骤还充当筛选测试，某些不符合步骤 1 中计算的 LAT 的候选答案将被淘汰。此步骤的输出是一组与找到的支持证据对应的机器学习特征。这些特征将作为输入，传递给一组机器学习模型，用于对候选答案进行评分。

1.  **最终合并与评分（机器学习）**：在最后一步中，系统识别相同答案的变体并将它们合并。它还使用机器学习模型，根据步骤 3 中生成的特征，选择由各自得分排名的最佳答案。这些机器学习模型已在一组代表性问题上进行训练，使用正确答案和预先摄取的文档语料库进行对比。

在继续讨论数据科学和人工智能如何改变计算机科学领域时，我认为有必要看看最先进的技术。IBM Watson 是这些旗舰项目之一，它为我们自它击败 Ken Jennings 和 Brad Rutter 参加*Jeopardy!*（危险游戏）以来的进步铺平了道路。

# 回到我们的 Twitter 话题情感分析项目

我们构建的快速数据管道原型使我们对数据有了更好的理解，但接下来我们需要设计一个更强大的架构，并使我们的应用程序准备好进入企业级。我们的主要目标仍然是积累构建数据分析的经验，而不是在数据工程部分花费太多时间。这就是为什么我们尽可能利用开源工具和框架的原因：

+   **Apache Kafka** ([`kafka.apache.org`](https://kafka.apache.org)): 这是一个可扩展的流处理平台，用于以可靠和容错的方式处理大量的推文。

+   **Apache Spark** ([`spark.apache.org`](https://spark.apache.org)): 这是一个内存计算集群框架。Spark 提供了一个编程接口，抽象了并行计算的复杂性。

+   **Jupyter Notebooks** ([`jupyter.org`](http://jupyter.org)): 这些基于 Web 的交互式文档（Notebooks）允许用户远程连接到计算环境（内核），以创建先进的数据分析。Jupyter 内核支持多种编程语言（Python、R、Java/Scala 等）以及多个计算框架（Apache Spark、Hadoop 等）。

对于情感分析部分，我们决定用 Watson Tone Analyzer 服务 ([`www.ibm.com/watson/services/tone-analyzer`](https://www.ibm.com/watson/services/tone-analyzer)) 替换我们使用 textblob Python 库编写的代码。Watson Tone Analyzer 是一个基于云的 REST 服务，提供包括情感、语言和社交语气检测在内的高级情感分析。尽管 Tone Analyzer 不是开源的，但 IBM 云上提供了一个可供开发和试用的免费版本 ([`www.ibm.com/cloud`](https://www.ibm.com/cloud))。

我们的架构现在如下所示：

![回到我们的 Twitter 话题情感分析项目](img/B09699_01_05.jpg)

Twitter 情感分析数据管道架构

在上述图中，我们可以将工作流程分解为以下几个步骤：

1.  产生一系列推文并将其发布到 Kafka 主题中，Kafka 主题可以看作是将事件聚集在一起的通道。接收组件可以订阅该主题/通道以消费这些事件。

1.  通过情感、语言和社交语气得分来丰富推文：使用 Spark Streaming 订阅来自组件**1**的 Kafka 主题，并将文本发送到 Watson Tone Analyzer 服务。生成的语气得分将被添加到数据中，供下游进一步处理。此组件是使用 Scala 实现的，为了方便起见，它是通过 Jupyter Scala Notebook 运行的。

1.  数据分析和探索：对于这一部分，我们决定使用 Python Notebook，因为 Python 提供了更具吸引力的库生态系统，尤其是在数据可视化方面。

1.  将结果发布回 Kafka。

1.  实现一个实时仪表板作为 Node.js 应用。

在三人团队的协作下，我们花了大约 8 周的时间，成功将仪表板与实时 Twitter 情感数据对接。这个看似较长的时间有多个原因：

+   一些框架和服务，比如 Kafka 和 Spark Streaming，对我们来说是全新的，我们必须学习如何使用它们的 API。

+   仪表板前端是作为一个独立的 Node.js 应用构建的，使用了 Mozaïk 框架（[`github.com/plouc/mozaik`](https://github.com/plouc/mozaik)），它使得构建强大的实时仪表板变得简单。然而，我们发现代码中有一些局限性，这迫使我们深入实现，编写补丁，因此对整体进度造成了一定延迟。

结果展示在以下截图中：

![返回我们的 Twitter 标签情感分析项目](img/B09699_01_06.jpg)

Twitter 情感分析实时仪表板

# 从构建我们第一个企业级数据管道中学到的经验教训

利用开源框架、库和工具，的确帮助我们在实现数据管道时提高了效率。例如，Kafka 和 Spark 部署相对简单，使用也非常方便，而且当我们遇到瓶颈时，我们可以随时依赖开发者社区的帮助，例如通过使用像[`stackoverflow.com`](https://stackoverflow.com)这样的问题解答网站。

使用基于云的托管服务进行情感分析步骤，比如 IBM Watson Tone Analyzer（[`www.ibm.com/watson/services/tone-analyzer`](https://www.ibm.com/watson/services/tone-analyzer)），也是一个积极的做法。它让我们可以抽象出训练和部署模型的复杂性，使得整个步骤更可靠，显然比我们自己实现要准确得多。

它也非常容易集成，因为我们只需要发送一个 REST 请求（也称为 HTTP 请求，关于 REST 架构的更多信息，请参考[`en.wikipedia.org/wiki/Representational_state_transfer`](https://en.wikipedia.org/wiki/Representational_state_transfer)）就能获得我们的答案。现在，大多数现代 Web 服务都遵循 REST 架构，然而，我们仍然需要了解每个 API 的规范，这通常需要很长时间才能正确实现。通过使用 SDK 库，这一步通常会变得更简单，SDK 库通常是免费的，并且支持大多数流行的编程语言，如 Python、R、Java 和 Node.js。SDK 库通过抽象出生成 REST 请求的代码，提供了更高级的程序化访问服务。SDK 通常会提供一个类来表示服务，每个方法封装一个 REST API，并处理用户认证和其他请求头。

在工具方面，我们对 Jupyter Notebooks 印象深刻，它提供了许多出色的功能，例如协作和完全交互性（我们稍后会更详细地介绍 Notebooks）。

当然，并非一切都很顺利，我们在一些关键领域遇到了困难：

+   在选择编程语言方面，对于一些关键任务，如数据丰富和数据分析，我们最终选择了 Scala 和 Python，尽管团队经验不多，主要是因为它们在数据科学家中非常受欢迎，而且我们也希望学习它们。

+   创建数据探索的可视化图表耗费了太多时间。使用可视化库（如 Matplotlib 或 Bokeh）编写一个简单的图表需要写大量的代码。这反过来又拖慢了我们对快速实验的需求。

+   将分析功能操作化为实时仪表板的过程非常困难，难以扩展。如前所述，我们需要编写一个完整的独立 Node.js 应用程序，该应用程序从 Kafka 消费数据，并且需要作为云平台应用程序（[`www.cloudfoundry.org`](https://www.cloudfoundry.org)）部署在 IBM Cloud 上。可以理解的是，这个任务第一次完成时需要相当长的时间，但我们还发现，更新它也很困难。写入 Kafka 的分析功能需要与仪表板应用程序上的更改同步。

# 数据科学战略

如果数据科学要继续增长并成为核心业务活动，公司必须找到一种方法，在整个组织的所有层面上扩展它，并克服我们之前讨论的所有困难挑战。为了实现这一目标，我们确定了三大重要支柱，架构师在规划数据科学战略时应重点关注这三点：数据、服务和工具：

![数据科学战略](img/B09699_01_07.jpg)

大规模数据科学的三大支柱

+   **数据是你最宝贵的资源**：你需要一个合适的数据策略，确保数据科学家可以轻松访问所需的精心策划的内容。通过对数据进行适当的分类、设定合适的治理政策，并使元数据可搜索，将减少数据科学家获取数据并请求使用权限的时间。这不仅能提高他们的生产力，还能提升他们的工作满意度，因为他们会有更多的时间从事实际的数据科学工作。

    *制定一个数据策略，使数据科学家能够轻松访问与其相关的高质量数据，能够提高生产力和士气，并最终带来更高的成功率。*

+   **服务**：每个规划数据科学的架构师都应该考虑采用**面向服务架构**（**SOA**）。与传统的单体应用程序不同，后者将所有功能捆绑在一起进行单一部署，面向服务的系统将功能拆解为若干服务，每个服务只负责做少数几件事，但做得非常出色，且具备高性能和可扩展性。这些系统可以独立部署和维护，相互之间没有依赖，从而为整个应用基础设施提供可扩展性和可靠性。例如，你可以有一个运行算法创建深度学习模型的服务，另一个则负责持久化模型并允许应用程序运行它来对客户数据进行预测，等等。

    优势显而易见：高度的可重用性、更容易的维护、更短的上市时间、可扩展性等等。此外，这种方法还非常适合融入云策略，当你的工作负载超出现有容量时，它能够提供增长路径。你还需要优先考虑开源技术，并尽可能标准化开放协议。

    *将流程拆分为更小的功能模块，能够为系统注入可扩展性、可靠性和重复性。*

+   **工具确实很重要！** 没有合适的工具，一些任务将变得极其难以完成（至少这是我用来解释自己为什么修不好家里东西的理由）。然而，你也需要保持工具的简单性、标准化，并合理地集成它们，以便即便是技术水平较低的用户也能使用（即使我得到了正确的工具，我也不确定自己能否完成家务修理任务，除非它足够简单）。一旦你降低了这些工具的学习曲线，非数据科学家的用户将会感到更加舒适使用它们。

    *简化工具的使用有助于打破信息孤岛，并增强数据科学、工程和业务团队之间的协作。*

# Jupyter Notebooks 是我们策略的核心

本质上，Notebooks 是由可编辑单元格组成的网页文档，允许你与后端引擎进行交互式命令操作。正如其名称所示，我们可以将其视为纸质便签的数字版，用于记录实验的笔记和结果。这个概念既强大又简单：用户可以在自己选择的编程语言中输入代码（大多数 Notebooks 实现支持多种语言，如 Python、Scala、R 等），运行单元并在单元下方的输出区域互动式地查看结果，这些结果会成为文档的一部分。结果可以是任何类型：文本、HTML 和图像，非常适合数据图表化。这就像是在传统的 **REPL**（**读取-评估-打印-循环**）程序上使用增强版，因为 Notebook 可以连接到强大的计算引擎（例如 Apache Spark（[`spark.apache.org`](https://spark.apache.org)）或 Python Dask（[`dask.pydata.org`](https://dask.pydata.org)）集群），如果需要，可以进行大数据实验。

在 Notebooks 中，任何在单元格中创建的类、函数或变量都可以在下方的单元格中看到，允许你逐步编写复杂的分析，迭代地测试假设并在进入下一个阶段之前解决问题。此外，用户还可以使用流行的 Markdown 语言编写富文本，或者使用 LaTeX（[`www.latex-project.org/`](https://www.latex-project.org/)）编写数学表达式，来描述他们的实验供其他人阅读。

下图展示了一个示例 Jupyter Notebook 的部分内容，其中包含一个 Markdown 单元，解释实验的内容，一个用 Python 编写的代码单元用于创建 3D 图表，以及实际的 3D 图表结果：

![Jupyter Notebooks 在我们战略中的核心地位](img/B09699_01_08.jpg)

样本 Jupyter Notebook

## 为什么 Notebooks 如此受欢迎？

在过去的几年里，Notebooks 作为数据科学相关活动的首选工具，迎来了爆炸式的增长。解释这一现象的原因有很多，但我认为主要原因是其多功能性，使得它不仅是数据科学家不可或缺的工具，也是参与构建数据管道的大多数角色，包括业务分析师和开发人员的必备工具。

对于数据科学家来说，Notebooks 非常适合迭代实验，因为它使得他们能够快速加载、探索和可视化数据。Notebooks 也是一个出色的协作工具；它们可以导出为 JSON 文件，并轻松地在团队之间共享，允许实验在需要时能够精确重复和调试。此外，由于 Notebooks 还是网页应用，它们可以轻松集成到基于云的多用户环境中，从而提供更好的协作体验。

这些环境还可以通过使用像 Apache Spark 这样的框架，将笔记本与机器集群连接，从而按需提供大规模计算资源。这些基于云的笔记本服务器的需求正在迅速增长，因此，我们看到越来越多的**SaaS**（即**软件即服务**）解决方案，不仅包括商业产品，如 IBM 数据科学体验（[`datascience.ibm.com`](https://datascience.ibm.com)）或 DataBricks（[`databricks.com/try-databricks`](https://databricks.com/try-databricks)），还包括开源解决方案，如 JupyterHub（[`jupyterhub.readthedocs.io/en/latest`](https://jupyterhub.readthedocs.io/en/latest)）。

对于商业分析师，笔记本可以作为演示工具，在大多数情况下，凭借其对 Markdown 的支持，提供足够的功能来取代传统的 PowerPoint。生成的图表和表格可以直接用于有效地传达复杂分析的结果；不再需要复制粘贴，并且算法的更改会自动反映在最终演示中。例如，某些笔记本实现（如 Jupyter）提供了将单元格布局自动转换为幻灯片的功能，使得整个体验更加无缝。

### 注意

作为参考，以下是在 Jupyter 笔记本中制作这些幻灯片的步骤：

+   使用**视图** | **单元工具栏** | **幻灯片放映**，首先通过选择**幻灯片**、**子幻灯片**、**片段**、**跳过**或**笔记**来标注每个单元格。

+   使用 `nbconvert jupyter` 命令将笔记本转换为 Reveal.js 驱动的 HTML 幻灯片：

+   可选地，你可以启动一个 web 应用服务器，以便在线访问这些幻灯片：

```py

jupyter nbconvert <pathtonotebook.ipynb> --to slides
 jupyter nbconvert <pathtonotebook.ipynb> --to slides –post serve

```

对于开发人员来说，情况要复杂得多。一方面，开发人员喜欢 REPL 编程，而笔记本提供了交互式 REPL 的所有优势，此外，它还可以连接到远程后端。由于它是在浏览器中运行的，结果可以包含图形，而且由于它们可以被保存，整个笔记本或其部分内容可以在不同的场景中重复使用。因此，对于开发人员来说，只要你选择的语言可用，笔记本提供了一种很好的方式来尝试和测试不同的内容，比如微调算法或集成新的 API。另一方面，尽管开发人员最终负责将分析结果转化为满足客户需求的应用程序，但他们对笔记本的采纳率很低，尤其是在数据科学活动中，这些活动本可以补充数据科学家的工作。

为了改善软件开发生命周期并缩短价值实现的时间，开发者需要开始使用与数据科学家相同的工具、编程语言和框架，包括 Python 及其丰富的库生态系统，以及已经成为数据科学重要工具的 Notebooks。尽管开发者必须在数据科学的理论和概念上与数据科学家站在一起并迅速掌握，但根据我的经验，我强烈推荐使用**MOOCs**（大规模在线开放课程），如 Coursera（[`www.coursera.org`](https://www.coursera.org)）或 EdX（[`www.edx.org`](http://www.edx.org)），这些平台提供各种各样的课程，适合各个层次的学习者。

然而，尽管 Notebook 功能强大，但它们主要是为数据科学家设计的，开发者需要面临陡峭的学习曲线。它们还缺乏开发者所需的应用开发功能，这对开发者至关重要。正如我们在*Twitter 话题标签情感分析*项目中所看到的，基于 Notebook 中创建的分析构建应用程序或仪表板可能非常困难，需要一种架构，而这种架构不仅难以实现，而且对基础设施的负担较重。

正是为了解决这些问题，我决定创建 PixieDust（[`github.com/ibm-watson-data-lab/pixiedust`](https://github.com/ibm-watson-data-lab/pixiedust)）库并将其开源。如我们在接下来的章节中将看到的，PixieDust 的主要目标是通过提供简单的 API 来加载和可视化数据，从而降低新用户（无论是数据科学家还是开发者）的*入门成本*。PixieDust 还提供了一个开发者框架，带有 API，方便开发者构建可以直接在 Notebook 中运行并部署为 Web 应用程序的应用、工具和仪表板。

# 总结

在本章中，我从开发者的角度分享了我对数据科学的看法，讨论了为什么我认为数据科学与人工智能（AI）和云计算（Cloud）一起，具有定义下一代计算时代的潜力。我还讨论了在数据科学充分实现其潜力之前，必须解决的许多问题。尽管本书并不打算提供一个解决所有这些问题的“魔法食谱”，但它确实尝试回答一个既困难又关键的问题，那就是如何实现数据科学的民主化，特别是如何*弥合数据科学家和开发者之间的鸿沟*。

在接下来的几章中，我们将深入探讨 PixieDust 开源库，了解它如何帮助 Jupyter Notebook 用户在处理数据时提高效率。我们还将深入研究 PixieApp 应用开发框架，该框架使开发者能够利用 Notebook 中实现的分析来构建应用程序和仪表板。

在接下来的章节中，我们将深入探讨许多示例，展示数据科学家和开发者如何有效地协作，构建端到端的数据管道，迭代分析，并将其在更短的时间内部署到最终用户。这些示例应用将涵盖多个行业用例，如图像识别、社交媒体和金融数据分析，其中包括数据科学用例，如描述性分析、机器学习、自然语言处理和*流数据*。

我们不会深入讨论示例应用中涵盖的所有算法背后的理论（这超出了本书的范围，且需要一本书来讲解），而是将重点强调如何利用开源生态系统快速完成手头的任务（如模型构建、可视化等），并将结果转化为应用程序和仪表板。

### 注意

提供的示例应用主要使用 Python 编写，并附带完整的源代码。代码经过广泛测试，已经准备好可以在您的项目中重复使用和定制。
