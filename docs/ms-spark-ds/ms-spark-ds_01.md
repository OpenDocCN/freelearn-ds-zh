# 第一章：大数据科学生态系统

作为一名数据科学家，您无疑对处理文件和处理大量数据非常熟悉。然而，正如您所同意的，除了简单分析单一类型的数据之外，需要一种组织和编目数据的方法，以便有效地管理数据。事实上，这是一名优秀数据科学家的基石。随着数据量和复杂性的增加，一种一致而坚固的方法可以决定泛化的成功和过度拟合的失败之间的差异！

本章是介绍一种在大规模数据上取得成功的方法和生态系统。它侧重于数据科学工具和技术。它介绍了环境以及如何适当配置，但也解释了一些与整体数据架构相关的非功能性考虑。虽然在这个阶段几乎没有实际的数据科学，但它为本书的其余部分的成功铺平了道路。

在本章中，我们将涵盖以下主题：

+   数据管理责任

+   数据架构

+   伴侣工具

# 介绍大数据生态系统

数据管理尤为重要，特别是当数据处于不断变化或定期产生和更新的状态时。在这些情况下需要的是一种存储、结构化和审计数据的方式，允许对模型和结果进行持续处理和改进。

在这里，我们描述了如何最好地持有和组织您的数据，以便与 Apache Spark 和相关工具在数据架构的背景下进行整合。

## 数据管理

即使在中期，您只打算在家里玩一点数据；然而，如果没有适当的数据管理，往往会导致努力升级到您很容易迷失方向并且会发生错误的程度。花时间考虑数据的组织，特别是其摄入，是至关重要的。没有比等待长时间运行的分析完成，整理结果并生成报告，然后发现您使用了错误版本的数据，或者数据不完整，缺少字段，甚至更糟糕的是您删除了结果更糟糕的事情了！

坏消息是，尽管它很重要，但数据管理是一个在商业和非商业企业中一直被忽视的领域，几乎没有现成的解决方案。好消息是，使用本章描述的基本构建模块进行出色的数据科学要容易得多。

## 数据管理责任

当我们考虑数据时，很容易忽视我们需要考虑的范围的真正程度。事实上，大多数数据“新手”以这种方式考虑范围：

1.  获取数据

1.  将数据放在某个地方（任何地方）

1.  使用数据

1.  扔掉数据

实际上，还有许多其他考虑因素，我们有责任确定哪些适用于特定的工作。以下数据管理构建模块有助于回答或跟踪有关数据的一些重要问题：

+   文件完整性

+   数据文件是否完整？

+   你怎么知道的？

+   它是否属于一组？

+   数据文件是否正确？

+   在传输过程中是否被篡改？

+   数据完整性

+   数据是否符合预期？

+   所有字段都存在吗？

+   是否有足够的元数据？

+   数据质量是否足够？

+   是否有任何数据漂移？

+   调度

+   数据是否定期传输？

+   数据多久到达一次？

+   数据是否按时接收？

+   你能证明数据是何时接收的吗？

+   它需要确认吗？

+   模式管理

+   数据是结构化的还是非结构化的？

+   数据应该如何解释？

+   是否可以推断出模式？

+   数据是否随时间改变？

+   模式是否可以从上一个版本演变？

+   版本管理

+   数据的版本是多少？

+   版本是否正确？

+   如何处理不同版本的数据？

+   您如何知道自己使用的是哪个版本？

+   安全

+   数据是否敏感？

+   它包含个人可识别信息（PII）吗？

+   它包含个人健康信息（PHI）吗？

+   它包含支付卡信息（PCI）吗？

+   我应该如何保护数据？

+   谁有权读取/写入数据？

+   是否需要匿名化/清理/混淆/加密？

+   处置

+   我们如何处理数据？

+   我们何时处置数据？

如果经过所有这些之后，你仍然不确定，在你继续编写使用`gawk`和`crontab`命令的 bash 脚本之前，继续阅读，你很快就会发现有一种更快、更灵活、更安全的方法，可以让你从小处着手，逐步创建商业级的摄取管道！

## 合适的工具来完成工作

Apache Spark 是可扩展数据处理的新兴事实标准。在撰写本书时，它是最活跃的**Apache 软件基金会**（**ASF**）项目，并且有丰富多样的伴随工具可用。每天都会出现新项目，其中许多项目在功能上有重叠。因此，需要时间来了解它们的功能并决定是否适合使用。不幸的是，这方面没有快速的方法。通常，必须根据具体情况做出特定的权衡；很少有一刀切的解决方案。因此，鼓励读者探索可用的工具并明智地选择！

本书介绍了各种技术，希望能为读者提供一些更有用和实用的技术的入门，以至于他们可以开始在自己的项目中利用它们。此外，我们希望展示，如果代码编写得当，即使决定被证明是错误的，也可以通过巧妙地使用**应用程序接口**（**API**）（或 Spark Scala 中的高阶函数）来交换技术。

# 总体架构

让我们从数据架构的高层介绍开始：它们的作用是什么，为什么它们有用，何时应该使用它们，以及 Apache Spark 如何适应其中。

![总体架构](img/image_01_001.jpg)

在最一般的情况下，现代数据架构具有四个基本特征：

+   数据摄取

+   数据湖

+   数据科学

+   数据访问

现在让我们介绍每一个，这样我们可以在后面的章节中更详细地讨论。

## 数据摄取

传统上，数据是根据严格的规则摄取，并根据预定的模式进行格式化。这个过程被称为**提取、转换、加载**（**ETL**），仍然是一个非常常见的做法，得到了大量商业工具以及一些开源产品的支持。

![数据摄取](img/image_01_002.jpg)

ETL 方法倾向于进行前期检查，以确保数据质量和模式一致，以简化后续的在线分析处理。它特别适用于处理具有特定特征集的数据，即与经典实体关系模型相关的数据。然而，并不适用于所有情况。

在大数据革命期间，对结构化、半结构化和非结构化数据的需求出现了象征性的爆炸，导致需要处理具有不同特征集的系统的创建。这些被定义为“4V：容量、多样性、速度和准确性”[`www.ibmbigdatahub.com/infographic/four-vs-big-data`](http://www.ibmbigdatahub.com/infographic/four-vs-big-data)。传统的 ETL 方法在这种新负担下陷入困境，因为它们处理大量数据需要太长时间，或者在面对变化时过于僵化，于是出现了一种不同的方法。进入“读时模式”范式。在这里，数据以其原始形式（或至少非常接近）被摄入，规范化、验证等细节在分析处理时进行。

这通常被称为“提取加载转换”（ELT），是对传统方法的参考：

![数据摄入](img/image_01_003.jpg)

这种方法重视及时交付数据，延迟详细处理直到绝对需要。这样，数据科学家可以立即访问数据，使用一系列传统方法不可用的技术寻找洞见。

尽管我们在这里只提供了一个高层概述，但这种方法非常重要，因此在整本书中，我们将通过实施各种读时模式算法来进一步探讨。我们将假定数据摄入采用 ELT 方法，也就是说我们鼓励用户根据自己的方便加载数据。这可以是每隔*n*分钟、夜间或在低使用率时进行。然后可以通过运行离线批处理作业再次由用户自行决定检查数据的完整性、质量等等。

## 数据湖

数据湖是一个方便、无处不在的数据存储。它很有用，因为它提供了许多关键的好处，主要包括：

+   可靠的存储

+   可扩展的数据处理能力

让我们简要地看一下每一个。

### 可靠的存储

数据湖有许多可靠的底层存储实现，包括 Hadoop 分布式文件系统（HDFS）、MapR-FS 和 Amazon AWS S3。

在整本书中，HDFS 将被假定为存储实现。此外，在本书中，作者使用部署在 Hortonworks HDP 环境中运行的 YARN 的分布式 Spark 设置。因此，除非另有说明，否则 HDFS 是使用的技术。如果您对这些技术不熟悉，它们将在本章后面进一步讨论。

无论如何，值得知道的是，Spark 可以本地引用 HDFS 位置，通过前缀`file://`访问本地文件位置，并通过前缀`s3a://`引用 S3 位置。

### 可扩展的数据处理能力

显然，Apache Spark 将是我们首选的数据处理平台。此外，正如您可能记得的那样，Spark 允许用户在其首选环境中执行代码，无论是本地、独立、YARN 还是 Mesos，都可以通过配置适当的集群管理器来实现；在`masterURL`中。顺便说一句，这可以在以下三个位置之一完成：

+   在发出`spark-submit`命令时使用`--master`选项

+   在`conf/spark-defaults.conf`文件中添加`spark.master`属性

+   在`SparkConf`对象上调用`setMaster`方法

如果您不熟悉 HDFS，或者没有访问集群的权限，那么可以使用本地文件系统运行本地 Spark 实例，这对于测试很有用。但是要注意，有时只有在集群上执行时才会出现不良行为。因此，如果您对 Spark 很认真，值得投资于分布式集群管理器，为什么不尝试 Spark 独立集群模式，或者亚马逊 AWS EMR？例如，亚马逊提供了许多负担得起的云计算路径，您可以探索[`aws.amazon.com/ec2/spot/`](https://aws.amazon.com/ec2/spot/)上的抢购实例的想法。

## 数据科学平台

数据科学平台提供了服务和 API，使得有效的数据科学得以进行，包括探索性数据分析、机器学习模型的创建和完善、图像和音频处理、自然语言处理和文本情感分析。

这是 Spark 真正擅长的领域，也是本书剩下部分的主要重点，利用强大的本地机器学习库、无与伦比的并行图处理能力和强大的社区。Spark 为数据科学提供了真正可扩展的机会。

剩下的章节将深入探讨这些领域，包括第六章，“抓取基于链接的外部数据”，第七章，“构建社区”，和第八章，“构建推荐系统”。

## 数据访问

数据湖中的数据最常由数据工程师和科学家使用 Hadoop 生态系统工具访问，比如 Apache Spark、Pig、Hive、Impala 或 Drill。然而，有时其他用户，甚至其他系统，需要访问数据，而常规工具要么太技术化，要么无法满足用户对实时延迟的苛刻期望。

在这些情况下，数据通常需要被复制到数据仓库或索引存储中，以便可以暴露给更传统的方法，比如报告或仪表盘。这个过程通常涉及创建索引和重组数据以实现低延迟访问，被称为数据出口。

幸运的是，Apache Spark 有各种适配器和连接器，可以连接传统数据库、BI 工具以及可视化和报告软件。本书将介绍其中许多。

# 数据技术

当 Hadoop 刚开始时，Hadoop 这个词指的是 HDFS 和 MapReduce 处理范式的组合，因为这是原始论文的概要[`research.google.com/archive/mapreduce.html`](http://research.google.com/archive/mapreduce.html)。自那时起，出现了大量的技术来补充 Hadoop，随着 Apache YARN 的发展，我们现在看到其他处理范式的出现，比如 Spark。

现在，Hadoop 通常被用作整个大数据软件堆栈的俗语，因此在这一点上，为本书定义该堆栈的范围是明智的。本书将在整本书中访问的一系列技术的典型数据架构如下所述：

![数据技术](img/B05261_01_04-1.jpg)

这些技术之间的关系是一个复杂的话题，因为它们之间存在复杂的相互依赖关系，例如，Spark 依赖于 GeoMesa，而 GeoMesa 又依赖于 Accumulo，Accumulo 又依赖于 Zookeeper 和 HDFS！因此，为了管理这些关系，有一些可用的平台，比如 Cloudera 或 Hortonworks HDP [`hortonworks.com/products/sandbox/`](http://hortonworks.com/products/sandbox/)。这些平台提供了集中的用户界面和集中的配置。平台的选择取决于读者，然而，不建议最初安装一些技术，然后转移到受管理的平台，因为遇到的版本问题会非常复杂。因此，通常更容易从一个干净的机器开始，并在前期做出决定。

我们在本书中使用的所有软件都是与平台无关的，因此适用于前面描述的一般架构。它可以独立安装，并且在单个或多个服务器环境中使用相对简单，而不需要使用受管理的产品。

## Apache Spark 的作用

在许多方面，Apache Spark 是将这些组件联系在一起的粘合剂。它越来越多地代表了软件堆栈的中心。它与各种组件集成，但没有一个是硬连接的。事实上，甚至底层存储机制都可以被替换。将这个特性与利用不同处理框架的能力相结合，意味着最初的 Hadoop 技术有效地成为组件，而不是一个庞大的框架。我们的架构的逻辑图如下所示：

![Apache Spark 的作用](img/image_01_005.jpg)

随着 Spark 的发展和广泛的行业认可，许多最初的 Hadoop 实现已经被重构为 Spark。因此，为了给这个画面增加更多的复杂性，通常有几种可能的方法来以编程方式利用任何特定的组件；尤其是根据 API 是否从最初的 Hadoop Java 实现中移植出来的命令式和声明式版本。在接下来的章节中，我们尽量保持对 Spark 精神的忠实。

# 伴随工具

现在我们已经建立了一个要使用的技术堆栈，让我们描述每个组件，并解释它们在 Spark 环境中的用处。本书的这一部分旨在作为参考而不是直接阅读。如果您熟悉大多数技术，那么您可以刷新您的知识并继续阅读下一节，第二章，*数据采集*。

## Apache HDFS

**Hadoop 分布式文件系统**（**HDFS**）是一个带有内置冗余的分布式文件系统。它默认优化为在三个或更多节点上工作（尽管一个节点也可以正常工作，且限制可以增加），这提供了存储数据的能力。因此，文件不仅被分割成多个块，而且这些块的三个副本在任何时候都存在。这巧妙地提供了数据冗余（如果一个丢失了，其他两个仍然存在），同时也提供了*数据局部性*。当对 HDFS 运行分布式作业时，系统不仅会尝试收集作业输入所需的所有块，还会尝试仅使用与运行该作业的服务器物理接近的块；因此，它有能力减少网络带宽，只使用其本地存储上的块，或者那些接近自身的节点上的块。实际上，这是通过将 HDFS 物理磁盘分配给节点，并将节点分配给机架来实现的；块是以节点本地、机架本地和集群本地的方式写入的。所有对 HDFS 的指令都通过一个名为**NameNode**的中央服务器传递，因此这提供了一个可能的单点故障；有各种方法可以提供 NameNode 的冗余。

此外，在多租户 HDFS 场景中，许多进程同时访问同一文件时，通过使用多个块也可以实现负载平衡；例如，如果一个文件占用一个块，这个块被复制三次，因此可能可以同时从三个不同的物理位置读取。尽管这可能看起来不是一个很大的优势，在数百或数千个节点的集群上，网络 IO 通常是运行作业的最大限制因素--作者在多千节点集群上确实经历过作业不得不等待数小时才能完成的情况，纯粹是因为网络带宽由于大量其他线程调用数据而达到最大值。

如果您正在运行笔记本电脑，需要将数据存储在本地，或者希望使用您已经拥有的硬件，那么 HDFS 是一个不错的选择。

### 优势

使用 HDFS 的优势如下：

+   **冗余**：块的可配置复制提供了对节点和磁盘故障的容忍

+   **负载平衡**：块复制意味着相同的数据可以从不同的物理位置访问

+   **数据本地性**：分析尝试访问最接近的相关物理块，减少网络 IO。

+   **数据平衡**：有一个算法可以在数据块变得过于集中或碎片化时重新平衡数据块。

+   **灵活的存储**：如果需要更多空间，可以添加更多磁盘和节点；尽管这不是一个热过程，但集群将需要停机来添加这些资源

+   **额外成本**：没有第三方成本涉及

+   **数据加密**：隐式加密（打开时）

### 缺点

以下是缺点：

+   NameNode 提供了一个中心故障点；为了减轻这一点，有辅助和高可用性选项可用

+   集群需要基本的管理和可能一些硬件工作

### 安装

要使用 HDFS，我们应该决定是以本地、伪分布式还是完全分布式的方式运行 Hadoop；对于单个服务器，伪分布式对于分析是有用的，因为分析应该可以直接从这台机器转移到任何 Hadoop 集群。无论如何，我们应该安装 Hadoop，至少包括以下组件：

+   NameNode

+   辅助 NameNode（或高可用性 NameNode）

+   DataNode

Hadoop 可以通过[`hadoop.apache.org/releases.html`](http://hadoop.apache.org/releases.html)进行安装。

Spark 需要知道 Hadoop 配置的位置，特别是以下文件：`hdfs-site.xml`，`core-site.xml`。然后在 Spark 配置中设置配置参数`HADOOP_CONF_DIR`。

然后 HDFS 将以本地方式可用，因此在 Spark 中可以简单地使用`/user/local/dir/text.txt`来访问文件`hdfs://user/local/dir/text.txt`。

## 亚马逊 S3

S3 将所有与并行性、存储限制和安全性相关的问题都抽象化，允许非常大规模的并行读/写操作，并提供了极低的成本和极好的服务级别协议（SLA）。如果您需要快速启动、无法在本地存储数据，或者不知道未来的存储需求是什么，这是完美的选择。需要注意的是，`s3n`和`S3a`采用对象存储模型，而不是文件存储，因此存在一些妥协：

+   最终一致性是指一个应用程序所做的更改（创建、更新和删除）在一段时间内不可见，尽管大多数 AWS 区域现在支持写后读一致性。

+   `s3n`和`s3a`利用了非原子重命名和删除操作；因此，重命名或删除大型目录需要与条目数量成比例的时间。然而，在此期间，目标文件可能对其他进程可见，直到最终一致性得到解决。

S3 可以通过命令行工具（`s3cmd`）通过网页和大多数流行语言的 API 访问；它通过基本配置与 Hadoop 和 Spark 进行本地集成。

### 优势

以下是优势：

+   无限的存储容量

+   无硬件考虑

+   可用加密（用户存储的密钥）

+   99.9%的可用性

+   冗余

### 缺点

以下是缺点：

+   存储和传输数据的成本

+   没有数据局部性

+   最终一致性

+   相对较高的延迟

### 安装

您可以创建一个 AWS 账户：[`aws.amazon.com/free/`](https://aws.amazon.com/free/)。通过这个账户，您将可以访问 S3，并且只需要创建一些凭据。

当前的 S3 标准是`s3a`；要通过 Spark 使用它需要对 Spark 配置进行一些更改：

```scala
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem 
spark.hadoop.fs.s3a.access.key=MyAccessKeyID 
spark.hadoop.fs.s3a.secret.key=MySecretKey

```

如果使用 HDP，您可能还需要：

```scala
spark.driver.extraClassPath=${HADOOP_HOME}/extlib/hadoop-aws-currentversion.jar:${HADOOP_HOME}/ext/aws-java-sdk-1.7.4.jar

```

然后，所有 S3 文件都可以在 Spark 中使用前缀`s3a://`来访问 S3 对象引用：

```scala
val rdd = spark.sparkContext.textFile("s3a://user/dir/text.txt") 

```

我们也可以内联使用 AWS 凭据，假设我们已经设置了`spark.hadoop.fs.s3a.impl`：

```scala
spark.sparkContext.textFile("s3a://AccessID:SecretKey@user/dir/file") 

```

然而，这种方法不会接受键中的斜杠字符`/`。这通常可以通过从 AWS 获取另一个键来解决（直到没有斜杠出现为止）。

我们也可以通过 AWS 账户中的 S3 选项卡下的 Web 界面浏览对象。

## Apache Kafka

Apache Kafka 是一个分布式的、用 Scala 编写的消息代理，可在 Apache 软件基金会许可下使用。该项目旨在提供一个统一的、高吞吐量、低延迟的平台，用于处理实时数据源。其结果本质上是一个大规模可扩展的发布-订阅消息队列，对于企业基础设施处理流式数据非常有价值。

### 优势

以下是优势：

+   发布-订阅消息

+   容错

+   保证交付

+   故障时重播消息

+   高度可扩展的共享无架构

+   支持背压

+   低延迟

+   良好的 Spark-streaming 集成

+   客户端实现简单

### 缺点

以下是缺点：

+   至少一次语义-由于缺乏事务管理器，无法提供精确一次性消息传递

+   需要 Zookeeper 进行操作

### 安装

由于 Kafka 是一个发布-订阅工具，其目的是管理消息（发布者）并将其定向到相关的端点（订阅者）。这是通过经过 Kafka 实现时安装的代理来完成的。Kafka 可以通过 Hortonworks HDP 平台获得，也可以独立安装，链接如下[`kafka.apache.org/downloads.html`](http://kafka.apache.org/downloads.html)。

Kafka 使用 Zookeeper 来管理领导选举（因为 Kafka 可以分布式，从而实现冗余），在前面的链接中找到的快速入门指南可以用于设置单节点 Zookeeper 实例，并提供客户端和消费者来发布和订阅主题，这提供了消息处理的机制。

## Apache Parquet

自 Hadoop 诞生以来，基于列的格式（而不是基于行）的想法得到了越来越多的支持。Parquet 已经开发出来，以利用压缩、高效的列式数据表示，并且设计时考虑了复杂的嵌套数据结构；它借鉴了 Apache Dremel 论文中讨论的算法。Parquet 允许在每一列上指定压缩方案，并且为添加更多编码做好了未来的准备。它还被设计为在整个 Hadoop 生态系统中提供兼容性，并且像 Avro 一样，将数据模式与数据本身一起存储。

### 优势

以下是优势：

+   列式存储

+   高度存储效率

+   每列压缩

+   支持谓词下推

+   支持列剪枝

+   与其他格式兼容，例如 Avro

+   高效读取，设计用于部分数据检索

### 缺点

以下是缺点：

+   不适合随机访问

+   写入可能需要大量计算

### 安装

Parquet 在 Spark 中是原生可用的，并且可以直接访问如下：

```scala
val ds = Seq(1, 2, 3, 4, 5).toDS 
ds.write.parquet("/data/numbers.parquet") 
val fromParquet = spark.read.parquet("/data/numbers.parquet")
```

## Apache Avro

Apache Avro 最初是为 Hadoop 开发的数据序列化框架。它使用 JSON 定义数据类型和协议（尽管还有另一种 IDL），并以紧凑的二进制格式序列化数据。Avro 既提供了持久数据的序列化格式，又提供了 Hadoop 节点之间通信的传输格式，以及客户端程序与 Hadoop 服务之间的通信格式。另一个有用的功能是它能够将数据模式与数据本身一起存储，因此任何 Avro 文件都可以在不需要引用外部源的情况下读取。此外，Avro 支持模式演变，因此旧模式版本编写的 Avro 文件可以与新模式版本兼容。

### 优点

以下是优点：

+   模式演变

+   节省磁盘空间

+   支持 JSON 和 IDL 中的模式

+   支持多种语言

+   支持压缩

### 缺点

以下是缺点：

+   需要模式才能读写数据

+   序列化计算量大

### 安装

由于本书中使用 Scala、Spark 和 Maven 环境，因此可以导入 Avro 如下：

```scala
<dependency>   
   <groupId>org.apache.avro</groupId>   
   <artifactId>avro</artifactId>   
   <version>1.7.7</version> 
</dependency> 

```

然后就是创建模式并生成 Scala 代码，使用模式将数据写入 Avro。这在第三章 *输入格式和模式*中有详细说明。

## Apache NiFi

Apache NiFi 起源于美国国家安全局（NSA），并于 2014 年作为其技术转移计划的一部分发布为开源项目。NiFi 能够在简单的用户界面中生成可扩展的数据路由和转换有向图。它还支持数据溯源，各种预构建的处理器以及快速高效地构建新处理器的能力。它包括优先级设置、可调的交付容忍度和反压功能，允许用户根据特定要求调整处理器和管道，甚至允许在运行时修改流程。所有这些都使其成为一个非常灵活的工具，可以构建从一次性文件下载数据流到企业级 ETL 管道的所有内容。通常使用 NiFi 构建管道和下载文件比编写快速的 bash 脚本甚至更快，加上用于此目的的功能丰富的处理器，这使其成为一个引人注目的选择。

### 优点

以下是优点：

+   广泛的处理器范围

+   集线器和辐射结构

+   图形用户界面（GUI）

+   可扩展

+   简化并行处理

+   简化线程处理

+   允许运行时修改

+   通过集群实现冗余

### 缺点

以下是缺点：

+   没有横切错误处理程序

+   表达语言只有部分实现

+   流文件版本管理不足

### 安装

Apache NiFi 可以与 Hortonworks 一起安装，称为 Hortonworks Dataflow。它也可以作为 Apache 的独立安装程序使用，[`nifi.apache.org/`](https://nifi.apache.org/)。在第二章 *数据采集*中有关 NiFi 的介绍。 

## Apache YARN

YARN 是 Hadoop 2.0 的主要组件，它基本上允许 Hadoop 插入处理范式，而不仅仅限于原始的 MapReduce。YARN 由三个主要组件组成：资源管理器、节点管理器和应用程序管理器。本书不涉及深入研究 YARN；主要要理解的是，如果我们运行 Hadoop 集群，那么我们的 Spark 作业可以在客户端模式下使用 YARN 执行，如下所示：

```scala
spark-submit --class package.Class /  
             --master yarn / 
             --deploy-mode client [options] <app jar> [app options] 

```

### 优点

以下是优点：

+   支持 Spark

+   支持优先级调度

+   支持数据本地性

+   作业历史存档

+   与 HDP 一起开箱即用

### 缺点

以下是缺点：

+   没有 CPU 资源控制

+   不支持数据谱系

### 安装

YARN 作为 Hadoop 的一部分安装；这可以是 Hortonworks HDP，Apache Hadoop，或其他供应商之一。无论如何，我们应该至少安装带有以下组件的 Hadoop：

+   资源管理器

+   NodeManager（1 个或更多）

为了确保 Spark 可以使用 YARN，它只需要知道`yarn-site.xml`的位置，这是通过在 Spark 配置中使用`YARN_CONF_DIR`参数设置的。

## Apache Lucene

Lucene 是一个最初用 Java 构建的索引和搜索库工具，但现在已经移植到其他几种语言，包括 Python。 Lucene 在其时间内产生了许多子项目，包括 Mahout，Nutch 和 Tika。这些现在已成为自己的顶级 Apache 项目，而 Solr 最近作为子项目加入。Lucene 具有全面的功能，但尤其以在问答搜索引擎和信息检索系统中的使用而闻名。

### 优点

以下是优点：

+   高效的全文搜索

+   可扩展的

+   多语言支持

+   出色的开箱即用功能

### 缺点

缺点是数据库通常更适合关系操作。

### 安装

如果您希望了解更多并直接与库交互，可以从[`lucene.apache.org/`](https://lucene.apache.org/)下载 Lucene。

在使用 Lucene 时，我们只需要在项目中包含`lucene-core-<version>.jar`。例如，使用 Maven 时：

```scala
<dependency> 
    <groupId>org.apache.lucene</groupId> 
    <artifactId>lucene-core</artifactId> 
    <version>6.1.0</version> 
</dependency> 

```

## Kibana

Kibana 是一个分析和可视化平台，还提供图表和流数据汇总。它使用 Elasticsearch 作为其数据源（反过来使用 Lucene），因此可以利用规模上非常强大的搜索和索引功能。Kibana 可以以许多不同的方式可视化数据，包括条形图，直方图和地图。我们在本章末尾简要提到了 Kibana，并且在本书中将广泛使用它。

### 优点

以下是优点：

+   在规模上可视化数据

+   直观的界面，快速开发仪表板

### 缺点

以下是缺点：

+   只与 Elasticsearch 集成

+   Kibana 发布与特定的 Elasticsearch 版本绑定

### 安装

Kibana 可以作为独立的部分轻松安装，因为它有自己的 Web 服务器。它可以从[`www.elastic.co/downloads/kibana`](https://www.elastic.co/downloads/kibana)下载。由于 Kibana 需要 Elasticsearch，因此还需要安装 Elasticsearch；有关更多信息，请参见前面的链接。Kibana 配置在`config/kibana.yml`中处理，如果安装了独立版本的 Elasticsearch，则不需要进行任何更改，它将立即运行！

## Elasticsearch

Elasticsearch 是一个基于 Lucene（见前文）的基于 Web 的搜索引擎。它提供了一个分布式的，多租户的，无模式的 JSON 文档全文搜索引擎。它是用 Java 构建的，但由于其 HTTP Web 界面，可以从任何语言中利用。这使得它特别适用于要通过网页显示的交易和/或数据密集型指令。

### 优点

优点如下：

+   分布式

+   无模式

+   HTTP 接口

### 缺点

缺点如下

+   无法执行分布式事务

+   缺乏前端工具

### 安装

Elasticsearch 可以从[`www.elastic.co/downloads/elasticsearch`](https://www.elastic.co/downloads/elasticsearch)下载。为了提供对 Rest API 的访问，我们可以导入 Maven 依赖项：

```scala
<dependency> 
    <groupId>org.elasticsearch</groupId> 
    <artifactId>elasticsearch-spark_2.10</artifactId> 
    <version>2.2.0-m1</version> 
</dependency> 

```

还有一个很好的工具可以帮助管理 Elasticsearch 内容。在[`chrome.google.com/webstore/category/extensions`](https://chrome.google.com/webstore/category/extensions)搜索 Chrome 扩展名 Sense。也可以在[`www.elastic.co/guide/en/sense/current/installing.html`](https://www.elastic.co/guide/en/sense/current/installing.html)找到更多解释。或者，它也适用于 Kibana，网址为[`www.elastic.co/guide/en/sense/current/installing.html`](https://www.elastic.co/guide/en/sense/current/installing.html)。

## Accumulo

Accumulo 是基于 Google 的 Bigtable 设计的 NoSQL 数据库，最初由美国国家安全局开发，随后于 2011 年发布给 Apache 社区。Accumulo 为我们提供了通常的大数据优势，如批量加载和并行读取，但还具有一些额外的功能；迭代器，用于高效的服务器和客户端预计算，数据聚合，最重要的是单元级安全。Accumulo 的安全方面使其在企业使用中非常有用，因为它在多租户环境中实现了灵活的安全性。Accumulo 由 Apache Zookeeper 提供支持，与 Kafka 一样，并且利用了 Apache Thrift，[`thrift.apache.org/`](https://thrift.apache.org/)，它实现了跨语言的远程过程调用（RPC）功能。

### 优点

优点如下：

+   Google Bigtable 的纯实现

+   单元级安全

+   可扩展的

+   冗余

+   为服务器端计算提供迭代器

### 缺点

缺点如下：

+   Zookeeper 在 DevOps 中并不普遍受欢迎

+   并不总是批量关系操作的最有效选择

### 安装

Accumulo 可以作为 Hortonworks HDP 发布的一部分安装，也可以作为独立实例从[`accumulo.apache.org/`](https://accumulo.apache.org/)安装。然后应根据安装文档进行配置，在撰写本文时为[`accumulo.apache.org/1.7/accumulo_user_manual#_installation`](https://accumulo.apache.org/1.7/accumulo_user_manual#_installation)。

在第七章中，*构建社区*，我们演示了 Accumulo 与 Spark 的使用，以及一些更高级的功能，如`迭代器`和`输入格式`。我们还展示了如何在 Elasticsearch 和 Accumulo 之间处理数据。

# 总结

在本章中，我们介绍了数据架构的概念，并解释了如何将责任分组为能力，以帮助管理数据的整个生命周期。我们解释了所有数据处理都需要一定程度的尽职调查，无论是由公司规定还是其他方式，没有这一点，分析及其结果很快就会变得无效。

在确定了我们的数据架构范围后，我们已经详细介绍了各个组件及其各自的优缺点，并解释了我们的选择是基于集体经验的。事实上，在选择组件时总是有选择的，他们各自的特性在做出任何承诺之前都应该仔细考虑。

在下一章中，我们将深入探讨如何获取和捕获数据。我们将建议如何将数据带入平台，并讨论与数据处理和处理相关的方面。
