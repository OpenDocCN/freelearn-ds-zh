# 四、数据访问组件——Hive 和 PIG

Hadoop 通常可以容纳万亿字节或千兆字节的数据进行处理；因此，数据访问在任何项目或产品中都是一个极其重要的方面，尤其是在 Hadoop 中。当我们处理大数据来处理数据时，我们将不得不执行一些临时处理来获得对数据和设计策略的见解。Hadoop 的基本处理层是 MapReduce，正如我们前面讨论的，它是一个大规模并行处理框架，具有可伸缩性、速度快、适应性强和容错性。

我们将详细了解 MapReduce 编程的一些限制和一些编程抽象层，如 Hive 和 Pig，它们可以使用用户友好的语言执行 MapReduce，以实现更快的开发和管理。当涉及到轻松地做一些特别的分析和一些不太复杂的分析时，Hive 和 PIG 是相当有用和方便的。

# 在 Hadoop 上需要一个数据处理工具

MapReduce 是对大数据进行处理的关键，但是理解、设计、编码和优化都很复杂。MapReduce 的学习曲线很高，需要掌握良好的编程技巧。通常大数据用户来自编程、数据库管理员、脚本、分析师、数据科学、数据管理人员等不同背景，并非所有用户都能适应 MapReduce 的编程模式。因此，我们对 Hadoop 的数据访问组件有不同的抽象。

数据访问组件对开发人员非常有用，因为他们可能不需要详细学习 MapReduce 编程，并且仍然可以在界面中利用 MapReduce 框架，在该界面中，他们可以更加舒适，并有助于更快的开发和更好的代码管理。抽象可以帮助快速地对数据进行临时处理，并专注于业务逻辑。

Hadoop 生态系统中广泛使用的两个数据访问组件是:

*   PIG
*   储备

让我们通过一些例子来详细讨论其中的每一个。

# PIG

Pig 是一个在 MapReduce 之上有 Pig 拉丁语言抽象包装的组件。PIG 是雅虎开发的！大约在 2006 年，作为一个开源项目被贡献给了 Apache。Pig Latin 是一种数据流语言，对于过程语言开发人员或用户来说更合适。Pig 可以帮助管理流中的数据，这对于数据流过程、ETL(提取转换加载)或 ELT(提取加载转换)过程特别数据分析是理想的。

Pig 可以以更简单的方式用于结构化和半结构化数据分析。Pig 的开发基于一种理念，即 pig 可以吃任何东西，住在任何地方，可以被用户轻松控制和修改，快速处理数据很重要。

## PIG 数据类型

Pig 有原始数据类型的集合，也有复杂的数据类型。Pig 关系运算符的输入和输出使用以下数据类型指定:

*   **原语** : int、long、float、double、chararray 和 bytearray
*   **Map**: Map is an associative array data type that stores a chararray key and its associated value. The data type of a value in a map can be a complex type. If the type of the value cannot be determined, Pig defaults to the bytearray data type. The key and value association is specified as the # symbol. The key values within a map have to be unique.

    语法:`[key#value, key1#value1…]`

*   **Tuple**: A tuple data type is a collection of data values. They are of fixed length and ordered. Tuple is similar to a record in a SQL table, without restrictions on the column types. Each data value is called a field. Ordering of values offers the capability to randomly access a value within a tuple.

    语法:`(value1, value2, value3…)`

*   **Bag**: A bag data type is a container for tuples and other bags. They are unordered, that is, a tuple or a bag within a bag cannot be accessed randomly. There are no constraints on the structure of the tuples contained in a bag. Duplicate tuples or bags are allowed within a bag.

    语法:`{(tuple1), (tuple2)…}`

Pig 允许嵌套复杂的数据结构，您可以将元组嵌套在元组、包和映射中。PIG 拉丁语语句处理关系，可以认为是:

*   关系(类似于数据库表)是一个包
*   包是元组的集合
*   元组(类似于数据库行)是一组有序的字段
*   字段是一段数据

## PIG 的建筑

Pig 数据流架构是分层的，用于将 Pig 拉丁语语句转换为 MapReduce 步骤。编译和执行 Pig 脚本有三个主要阶段，如下所示:

*   逻辑计划
*   物理计划
*   MapReduce 计划

### 逻辑计划

在逻辑计划中，对 Pig 语句进行语法错误解析，并验证输入文件和输入数据结构。然后准备一个逻辑计划，一个操作符作为节点的 DAG(有向无环图)，以及作为边的数据流。基于内置规则的优化发生在这个阶段。逻辑计划与操作员一一对应。

### 物理计划

在这一阶段，每个操作者被转换成执行的物理形式。对于 MapReduce 平台，除了少数，大多数操作人员都与物理计划有一一对应关系。除了逻辑运算符，还有一些物理运算符。它们如下:

*   局部重排
*   全局重排
*   包装

像`GROUP`、`COGROUP`或`JOIN`这样的逻辑运算符被翻译成一系列的`LR`、`GR`和`P`运算符。`LR`操作符对应于混洗准备阶段，在该阶段，基于密钥进行分区。`GR`对应于地图和减少任务之间的实际洗牌。`P`操作符是减少侧的分割操作符。

### 地图缩减计划

Pig 编译的最后阶段是将物理计划编译成实际的 MapReduce 作业。只要物理计划中存在`LR`、`GR`和`P`序列，就需要减少任务。编译器也会尽可能寻找机会放入 Combiners。上图中物理计划的 MapReduce 计划有两个 MapReduce 作业，一个对应于逻辑计划中的`JOIN`，另一个对应于`GROUP`。对应于`GROUP`操作符的 MapReduce 任务也有一个组合器。必须注意的是`GROUP`操作发生在地图任务中。

## PIG 模式

用户可以通过两种模式运行 Pig:

*   **本地模式**:通过访问一台机器，所有文件都使用本地主机和文件系统安装和运行。
*   **MapReduce 模式**:这是默认模式，需要访问 Hadoop 集群。

在 Pig 中有三种执行模式:

*   **交互模式或咕哝模式**
*   **批处理模式或脚本模式**
*   **嵌入模式**:以 Python 或 JavaScript 等宿主语言嵌入 Pig 命令并运行程序

这些执行模式既可以在本地模式下执行，也可以在地图缩减模式下执行。

## 咕噜贝

咕噜是 PIG 的交互外壳。可用于交互输入 PIG 拉丁，为用户提供与 HDFS 交互的外壳。

对于本地模式:

使用`-x`标志指定本地模式:

```sh
$ pig –x local

```

对于 MapReduce 模式:

将`HADOOP_CONF_DIR`置于`PIG_CLASSPATH`上，将小 PIG 指向远程集群。

### 类型

`HADOOP_CONF_DIR`是包含`hadoop-site.xml`、`hdfs-site.xml`和`mapred-site.xml`文件的目录。

示例:`$ export PIG_CLASSPATH=<path_to_hadoop_conf_dir>`

这里给出的是:

```sh
$ pig
grunt>

```

### 输入数据

我们将使用`movies_data.csv`文件作为探索 PIG 的数据集。输入文件包含以下字段和示例数据:

<colgroup><col> <col> <col> <col> <col></colgroup> 
| 

身份证明

 | 

名字

 | 

年

 | 

评级

 | 

持续时间(秒)

 |
| --- | --- | --- | --- | --- |
| Forty thousand one hundred and forty-six | 奥斯卡的绿洲:耍鸡高手枪蜥蜴:爱情的渴望力量 | Two thousand and eleven |   | One thousand six hundred and one |
| Forty thousand one hundred and forty-seven | 变形金刚:救援机器人:第一季:恐龙机器人的回归 | Two thousand and eleven |   | One thousand three hundred and twenty-four |
| Forty thousand one hundred and forty-eight | 浮游生物入侵:温克尔行动、鳕鱼行动、硬壳行动 | Two thousand and twelve |   | One thousand two hundred and sixty-two |
| Forty thousand one hundred and forty-nine | 变形金刚:救援机器人:第一季:深度麻烦 | Two thousand and eleven |   | One thousand three hundred and twenty-four |
| Forty thousand one hundred and fifty | 预告片:揭开面 Yarn | Two thousand and twelve | Three point six | sixty-nine |
| Forty thousand one hundred and fifty-one | 预告片:痛苦 | Two thousand and twelve | Three point six | fifty-two |
| Forty thousand one hundred and fifty-two | 托德和《纯粹邪恶之书》 | Two thousand and ten | Three point nine |   |
| Forty thousand one hundred and fifty-three | 预告片:纸牌屋 | Two thousand and twelve | Three point seven | One hundred and forty-eight |

### 加载数据

对于在 Pig 中加载数据的，我们使用`LOAD`命令并将其映射到关系的别名(如本例中的电影)，该别名可以从文件系统或 HDFS 读取数据，并将其加载到 Pig 中进行处理。Pig 中有不同的存储处理程序，通过提及`USING`和存储处理程序功能来处理不同类型的记录；一些常用的存储处理函数是:

*   PigStorage 用于带有可指定分隔符的结构化文本文件，是默认的存储处理程序
*   用于处理来自糖化血红蛋白表的数据的糖化血红蛋白存储
*   BinStorage，用于二进制和机器可读格式
*   JSONStorage，用于处理 JSON 数据和应该指定的模式
*   用于 UTF-8 的非结构化数据的文本加载器

如果我们没有默认提到任何处理程序，默认使用 PigStorage，PigStorage 和 TextStorage 支持压缩文件`gzip`和`bzip`。

示例:

```sh
grunt> movies = LOAD '/user/biadmin/shiva/movies_data.csv' USING PigStorage(',') as (id,name,year,rating,duration);

```

我们可以使用模式为字段分配类型:

```sh
A = LOAD 'data' AS (name, age, gpa); // name, age, gpa default to bytearrays
A = LOAD 'data' AS (name:chararray, age:int, gpa:float); // name is now a String (chararray), age is integer and gpa is float

```

### 垃圾场

转储命令对于交互式查看关系中存储的值并将输出写入控制台非常有用。转储不会保存数据:

示例:

```sh
grunt> DUMP movies;
INFO  [JobControl] org.apache.hadoop.mapreduce.lib.input.FileInputFormat     - Total input paths to process : 1
INFO  [main] org.apache.hadoop.mapreduce.lib.input.FileInputFormat     - Total input paths to process : 1
(1,The Nightmare Before Christmas,1993,3.9,4568)
(2,The Mummy,1932,3.5,4388)
(3,Orphans of the Storm,1921,3.2,9062)
(4,The Object of Beauty,1991,2.8,6150)
(5,Night Tide,1963,2.8,5126)
(6,One Magic Christmas,1985,3.8,5333)
(7,Muriel's Wedding,1994,3.5,6323)

```

### 店铺

存储命令用于写入或继续数据。Pig 只有在遇到`DUMP`或`STORE`时才开始作业。我们也可以在`STORE`中使用`LOAD`中提到的处理器。

示例:

```sh
grunt> STORE movies INTO '/temp' USING PigStorage(','); //This will write contents of movies to HDFS in /temp location

```

#### FOREACH 生成

`FOREACH`操作用于在关系的每个记录中应用列级表达式。甚至允许关系中的某些列是相当强大的，我们可以在`FOREACH`中使用 UDF 作为表达。

示例:

```sh
grunt> movie_duration = FOREACH movies GENERATE name, (double)(duration/60);

```

### 过滤器

Filter 是用来获取符合表达式条件的行。

示例:

```sh
grunt> movies_greater_than_four = FILTER movies BY (float)rating>4.0;
grunt> DUMP movies_greater_than_four;

```

我们可以对过滤器和布尔运算符(与、或、非)使用多个条件:

```sh
grunt> movies_greater_than_four_and_2012 = FILTER movies BY (float)rating>4.0 AND year > 2012;
grunt> DUMP movies_greater_than_four_and_2012;
INFO  [JobControl] org.apache.hadoop.mapreduce.lib.input.FileInputFormat     - Total input paths to process : 1
WARN  [main] org.apache.pig.data.SchemaTupleBackend     - SchemaTupleBackend has already been initialized
INFO  [main] org.apache.hadoop.mapreduce.lib.input.FileInputFormat     - Total input paths to process : 1
(22148,House of Cards: Season 1,2013,4.4,)
(22403,House of Cards,2013,4.4,)
(37138,Orange Is the New Black: Season 1,2013,4.5,)
(37141,Orange Is the New Black,2013,4.5,)
(37174,The Following: Season 1,2013,4.1,)
(37239,The Following,2013,4.1,)
(37318,The Carrie Diaries,2013,4.3,)
(37320,The Carrie Diaries: Season 1,2013,4.3,)
(37589,Safe Haven,2013,4.2,6936)

```

### 分组依据

`Group By`命令用于用键创建记录组。`Group By`关系用于处理分组数据的聚合函数。

分组依据的语法如下:

`alias = GROUP alias { ALL | BY expression} [, alias ALL | BY expression …] [PARALLEL n];`

例如:

*   至`Group By`(员工在销售团队的起始年)

    ```sh
    grunt> grouped_by_year = group movies by year;

    ```

*   或者`Group By`多个字段:

    ```sh
    B = GROUP A BY (age, employeesince);

    ```

### 极限

`Limit`命令限制了一个关系中输出元组的数量，但是元组返回可以在命令的不同执行中改变。对于特定的元组，我们必须使用`ORDER`和`LIMIT`，这将返回元组的有序集。

示例:

```sh
grunt> movies_limit_10 = LIMIT movies 10;
grunt> DUMP movies_limit_10;
INFO  [JobControl] org.apache.hadoop.mapreduce.lib.input.FileInputFormat     - Total input paths to process : 1
INFO  [JobControl] org.apache.hadoop.mapreduce.lib.input.FileInputFormat     - Total input paths to process : 1
WARN  [main] org.apache.pig.data.SchemaTupleBackend     - SchemaTupleBackend has already been initialized
INFO  [main] org.apache.hadoop.mapreduce.lib.input.FileInputFormat     - Total input paths to process : 1
(1,The Nightmare Before Christmas,1993,3.9,4568)
(2,The Mummy,1932,3.5,4388)
(3,Orphans of the Storm,1921,3.2,9062)
(4,The Object of Beauty,1991,2.8,6150)
(5,Night Tide,1963,2.8,5126)
(6,One Magic Christmas,1985,3.8,5333)
(7,Muriel's Wedding,1994,3.5,6323)
(8,Mother's Boys,1994,3.4,5733)
(9,Nosferatu: Original Version,1929,3.5,5651)
(10,Nick of Time,1995,3.4,5333)

```

### 聚合

Pig 提供了一堆聚合功能，比如:

*   AVG
*   数数
*   计数 _ 星
*   总和
*   马克斯(男子名ˌ等于 Maximilian)
*   部

示例:

```sh
grunt> count_by_year = FOREACH grouped_by_year GENERATE group, COUNT(movies);
grunt> DUMP count_by_year;
INFO  [JobControl] org.apache.hadoop.mapreduce.lib.input.FileInputFormat     - Total input paths to process : 1
INFO  [main] org.apache.hadoop.mapreduce.lib.input.FileInputFormat     - Total input paths to process : 1
(1913,3)
(1914,20)
.
.
 (2009,4451)
(2010,5107)
(2011,5511)
(2012,4339)
(2013,981)
(2014,1)

```

### 合群

`Cogroup`是群的概括。它不是基于一个键收集一个输入的记录，而是基于一个键收集 *n* 个输入的记录。结果是一个记录，每个输入都有一个键和一个包。每个包包含该输入中具有给定值的所有记录:

```sh
$ cat > owners.csv
adam,cat
adam,dog
alex,fish
alice,cat
steve,dog

$ cat > pets.csv
nemo,fish
fido,dog
rex,dog
paws,cat
wiskers,cat

grunt> owners = LOAD 'owners.csv'
>>     USING PigStorage(',')
>>     AS (owner:chararray,animal:chararray);

grunt> pets = LOAD 'pets.csv'
>>     USING PigStorage(',')
>>     AS (name:chararray,animal:chararray);

grunt> grouped = COGROUP owners BY animal, pets by animal;
grunt> DUMP grouped;

```

这将根据动物栏对每个表格进行分组。对于每种动物，它将从两个表中创建一个匹配行的包。对于这个例子，我们得到如下表所示的结果:

<colgroup><col> <col> <col></colgroup> 
| 

组

 | 

业主

 | 

宠物

 |
| --- | --- | --- |
| 猫 | {(亚当，猫)，(爱丽丝，猫)} | 小猫咪小猫咪小猫咪 |
| 狗 | {(亚当，狗)，(史蒂夫，狗)} | {(菲多，狗)，(雷克斯，狗)} |
| 鱼 | {(亚历克斯，鱼)} | {(尼莫，鱼)} |

### 描述

`DESCRIBE`命令给出了一个关系的模式，如下所示:

```sh
grunt> Describe grouped;
grouped: {group: chararray,owners: {(owner: chararray,animal: chararray)},pets: {(name: chararray,animal: chararray)}}

```

### 解释

关系上的`EXPLAIN`命令显示了 Pig 脚本将如何执行。它显示了关系的逻辑计划、物理计划和 MapReduce 计划。我们可以使用`EXPLAIN`命令来研究已经进入计划的优化。此命令可用于进一步优化脚本:

```sh
grunt> explain grouped;
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
grouped: (Name: LOStore Schema: group#107:chararray,owners#108:bag{#118:tuple(owner#94:chararray,animal#95:chararray)},pets#110:bag{#119:tuple(name#96:chararray,animal#97:chararray)})
|
|---grouped: (Name: LOCogroup Schema: group#107:chararray,owners#108:bag{#118:tuple(owner#94:chararray,animal#95:chararray)},pets#110:bag{#119:tuple(name#96:chararray,animal#97:chararray)})
 |   |
 |   animal:(Name: Project Type: chararray Uid: 95 Input: 0 Column: 1)
 |   |
 |   animal:(Name: Project Type: chararray Uid: 97 Input: 1 Column: 1)
 |
 |---owners: (Name: LOForEach Schema: owner#94:chararray,animal#95:chararray)
 |   |   |
 |   |   (Name: LOGenerate[false,false] Schema: owner#94:chararray,animal#95:chararray)ColumnPrune:InputUids=[95, 94]ColumnPrune:OutputUids=[95, 94]
 |   |   |   |
 |   |   |   (Name: Cast Type: chararray Uid: 94)
 |   |   |   |
 |   |   |   |---owner:(Name: Project Type: bytearray Uid: 94 Input: 0 Column: (*))
 |   |   |   |
 |   |   |   (Name: Cast Type: chararray Uid: 95)
 |   |   |   |
 |   |   |   |---animal:(Name: Project Type: bytearray Uid: 95 Input: 1 Column: (*))
 |   |   |
 |   |   |---(Name: LOInnerLoad[0] Schema: owner#94:bytearray)
 |   |   |
 |   |   |---(Name: LOInnerLoad[1] Schema: animal#95:bytearray)
 |   |
 |   |---owners: (Name: LOLoad Schema: owner#94:bytearray,animal#95:bytearray)RequiredFields:null
 |
 |---pets: (Name: LOForEach Schema: name#96:chararray,animal#97:chararray)
 |   |
 |   (Name: LOGenerate[false,false] Schema: name#96:chararray,animal#97:chararray)ColumnPrune:InputUids=[96, 97]ColumnPrune:OutputUids=[96, 97]
 |   |   |
 |   |   (Name: Cast Type: chararray Uid: 96)
 |   |   |
 |   |   |---name:(Name: Project Type: bytearray Uid: 96 Input: 0 Column: (*))
 |   |   |
 |   |   (Name: Cast Type: chararray Uid: 97)
 |   |   |
 |   |   |---animal:(Name: Project Type: bytearray Uid: 97 Input: 1 Column: (*))
 |   |
 |   |---(Name: LOInnerLoad[0] Schema: name#96:bytearray)
 |   |
 |   |---(Name: LOInnerLoad[1] Schema: animal#97:bytearray)
 |
 |---pets: (Name: LOLoad Schema: name#96:bytearray,animal#97:bytearray)RequiredFields:null

#-----------------------------------------------
# Physical Plan:
#-----------------------------------------------
grouped: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-76
|
|---grouped: Package[tuple]{chararray} - scope-71
 |
 |---grouped: Global Rearrange[tuple] - scope-70
 |
 |---grouped: Local Rearrange[tuple]{chararray}(false) - scope-72
 |   |   |
 |   |   Project[chararray][1] - scope-73
 |   |
 |   |---owners: New For Each(false,false)[bag] - scope-61
 |       |   |
 |       |   Cast[chararray] - scope-56
 |       |   |
 |       |   |---Project[bytearray][0] - scope-55
 |       |   |
 |       |   Cast[chararray] - scope-59
 |       |   |
 |       |   |---Project[bytearray][1] - scope-58
 |       |
 |       |---owners: Load(file:///home/opt/pig/bin/owners.csv:PigStorage(',')) - scope-54
 |
 |---grouped: Local Rearrange[tuple]{chararray}(false) - scope-74
 |   |
 |   Project[chararray][1] - scope-75
 |
 |---pets: New For Each(false,false)[bag] - scope-69
 |   |
 |   Cast[chararray] - scope-64
 |   |
 |   |---Project[bytearray][0] - scope-63
 |   |
 |   Cast[chararray] - scope-67
 |   |
 |   |---Project[bytearray][1] - scope-66
 |
 |---pets: Load(file:///home/opt/pig/bin/pets.csv:PigStorage(',')) - scope-62

#--------------------------------------------------
# Map Reduce Plan
#--------------------------------------------------
MapReduce node scope-79
Map Plan
Union[tuple] - scope-80
|
|---grouped: Local Rearrange[tuple]{chararray}(false) - scope-72
|   |   |
|   |   Project[chararray][1] - scope-73
|   |
|   |---owners: New For Each(false,false)[bag] - scope-61
|       |   |
|       |   Cast[chararray] - scope-56
|       |   |
|       |   |---Project[bytearray][0] - scope-55
|       |   |
|       |   Cast[chararray] - scope-59
|       |   |
|       |   |---Project[bytearray][1] - scope-58
|       |
|       |---owners: Load(file:///home/opt/pig/bin/owners.csv:PigStorage(',')) - scope-54
|
|---grouped: Local Rearrange[tuple]{chararray}(false) - scope-74
 |   |
 |   Project[chararray][1] - scope-75
 |
 |---pets: New For Each(false,false)[bag] - scope-69
 |   |
 |   Cast[chararray] - scope-64
 |   |
 |   |---Project[bytearray][0] - scope-63
 |   |
 |   Cast[chararray] - scope-67
 |   |
 |   |---Project[bytearray][1] - scope-66
 |
 |---pets: Load(file:///home/opt/pig/bin/pets.csv:PigStorage(',')) - scope-62--------
Reduce Plan
grouped: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-76
|
|---grouped: Package[tuple]{chararray} - scope-71--------
Global sort: false
----------------

```

### 说明

`ILLUSTRATE`命令也许是最重要的发展援助。关系上的`ILLUSTRATE`对数据进行采样，并对其应用查询。这可以在调试过程中节省大量时间。样本明显小于数据，使得代码、测试和调试周期非常快。在许多情况下，`JOIN`或`FILTER`操作员可能不会对数据样本产生任何输出。在这种情况下，`ILLUSTRATE`制造通过这些运算符的记录，并将它们插入样本数据集中:

```sh
grunt> illustrate grouped;
-----------------------------------------------------------
| owners     | owner:chararray     | animal:chararray     |
-----------------------------------------------------------
|            | steve               | dog                  |
|            | adam                | dog                  |
-----------------------------------------------------------
--------------------------------------------------------
| pets     | name:chararray     | animal:chararray     |
--------------------------------------------------------
|          | fido               | dog                  |
|          | rex                | dog                  |
--------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| grouped     | group:chararray     | owners:bag{:tuple(owner:chararray,animal:chararray)}                 | pets:bag{:tuple(name:chararray,animal:chararray)}                 |
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
|             | dog                 | {(steve, dog), (adam, dog)}                                          | {(fido, dog), (rex, dog)}                                         |
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

```

Pig 广泛用于数据流和 ETL，因此像 Pig 拉丁语言这样的脚本有助于轻松设计流。

# 鼠标

Hive 用类似 SQL 的包装器在 Hadoop 中提供了一个数据仓库环境，并且还翻译 MapReduce 作业中的 SQL 命令进行处理。Hive 中的 SQL 命令被称为 HiveQL，不支持 SQL 92 方言，不应该假设支持所有的关键字，因为整体思路是隐藏 MapReduce 编程的复杂性，对数据进行分析。

Hive 还可以充当与其他系统的分析接口，因为大多数系统与 Hive 集成良好。Hive 不能用于处理事务，因为它不提供行级更新和实时查询。

## Hive 建筑

Hive 架构有不同的组件，例如:

*   **驱动程序**:驱动程序管理 HiveQL 语句在 Hive 中移动时的生命周期，还维护会话统计的会话句柄。
*   **Metastore** : Metastore 存储系统目录和关于表、列、分区等的元数据。
*   **查询编译器**:它把 HiveQL 编译成一个优化的地图/缩减任务的 DAG。
*   **执行引擎**:以适当的依赖顺序执行编译器产生的任务。执行引擎与底层 Hadoop 实例交互。
*   **HiveServer2** :提供节俭接口和 JDBC/ODBC 服务器，提供 Hive 与其他应用集成的方式，支持多客户端并发和身份验证。
*   客户端组件，如命令行界面、网络用户界面和驱动程序。驱动程序是由供应商提供的 JDBC/ODBC 驱动程序和其他合适的驱动程序。

HiveQL 的流程描述如下:

*   HiveQL 语句可以从命令行界面、网络用户界面或使用诸如节俭、ODBC 或 JDBC 等接口的外部客户端提交。
*   驱动程序首先将查询传递给编译器，在编译器中，它使用存储在 Metastore 中的元数据，通过典型的解析、类型检查和语义分析阶段。
*   编译器生成一个逻辑计划，然后通过一个简单的基于规则的优化器进行优化。最后，以 MapReduce 任务和 HDFS 任务的 DAG 的形式生成优化计划。然后，执行引擎通过使用 Hadoop 按照依赖关系的顺序执行这些任务。

让我们检查元存储、查询编译器和执行引擎的更多细节。

### Metastore

Metastore 存储关于表、分区、模式、列、类型等的所有细节。它充当 Hive 的系统目录。可以从不同编程语言的客户端调用，详情可以使用节俭查询。Metastore 对于 Hive 非常关键，没有它就无法检索结构设计细节，也无法访问数据。因此，Metastore 会定期备份。

Metastore 可能会成为 Hive 中的瓶颈，因此建议使用本地 JDBC 数据库(如 MySQL)来隔离 JVM 进程。Hive 确保作业的映射器和缩减器不会直接访问 Metastore 相反，它通过一个 xml 计划传递，该计划由编译器生成，并包含运行时所需的信息。

### 查询编译器

查询编译器使用 Metastore 存储的元数据处理 HiveQL 语句，生成执行计划。查询编译器执行以下步骤:

*   **解析**:查询编译器解析语句。
*   **类型检查和语义分析**:在这个阶段，编译器使用元数据来检查语句的表达式和语义中的类型兼容性。在检查被验证并且没有发现错误之后，编译器为语句构建一个逻辑计划。
*   **优化**:编译器优化逻辑计划，并创建一个 DAG，将一个链的结果传递给下一个链，并尝试通过对逻辑步骤应用不同的规则(如果可能)来优化计划。

### 执行引擎

执行引擎执行优化后的计划。它一步一步地执行计划，为计划中的每一个任务考虑要完成的从属任务。任务的结果存储在一个临时位置，在最后一步，数据被移动到所需的位置。

## 数据类型和模式

Hive 支持`TINYINT`、`SMALLINT`、`INT`、`BIGINT`、`FLOAT`、`DOUBLE`、`DECIMAL`等所有原始数值数据类型。除了这些原语数据类型，Hive 还支持字符串类型，如`CHAR`、`VARCHAR`和`STRING`数据类型。与 SQL 一样，存在`TIMESTAMP`、`DATE`等时间指标数据类型。也有`BOOLEAN`和`BINARY`杂类。

许多复杂的数据类型也可用。复杂类型可以由其他基本类型或复杂类型组成。可用的复杂类型有:

*   **STRUCT**: These are groupings of data elements similar to a C-struct. The dot notation is used to dereference elements within a struct. A field within column C defined as a `STRUCT {x INT, y STRING}` can be accessed as `A.x` or `A.y`.

    语法:`STRUCT<field_name : data_type>`

*   **MAP**: These are key value data types. Providing the key within square braces can help access a value. A value of a map column M that maps from key x to value y can be accessed by M[x].There is no restriction on the type stored by the value, though the key needs to be of a primitive type.

    语法:`MAP<primitive_type, data_type>`

*   **ARRAY**: These are lists that can be randomly accessed through their position. The syntax to access an array element is the same as a map. But what goes into the square braces is a zero-based index of the element.

    语法:`ARRAY<data_type>`

*   **UNION**: There is a union type available in Hive. It can hold an element of one of the data types specified in the union.

    语法:`UNIONTYPE<data_type1, data_type2…>`

## 安装蜂箱

Hive 可以通过下载并解包一个 tarball 来安装，也可以下载源代码，使用 Maven(0.13 及更高版本)或 Ant(0.12 及更低版本)来构建 Hive。

Hive 安装过程有以下要求:

*   Java 1.7(首选)或 Java 1.6
*   Hadoop 2。 *x* (首选)或 1。 *x* 。Hive 版本高达 0.13，但它也支持 0.20。 *x* 或 0.23。 *x*
*   Hive 通常用于 Linux 和 Windows 环境中的生产

首先，从一个 Apache 下载镜像下载最新稳定的 Hive 版本(请参见 Hive Releases)。

接下来，你需要打开油球。这将导致创建一个名为 hive-x.y.z 的子目录(其中 x.y.z 是发行号):

```sh
$ tar -xzvf hive-x.y.z.tar.gz

```

设置环境变量`HIVE_HOME`指向安装目录:

```sh
 $ cd hive-x.y.z
 $ export HIVE_HOME={{pwd}}

```

最后，将`$HIVE_HOME/bin`添加到您的`Path`中:

```sh
 $ export PATH=$HIVE_HOME/bin:$PATH

```

## 启动 Hive 外壳

对于使用 Hive shell，我们应该遵循以下步骤:

1.  用户必须先创建`/tmp`和`/user/hive/warehouse`，并在 HDFS 将其设置为`chmod g+w`，然后才能在 Hive 中创建表格。执行该设置的命令如下:

    ```sh
     $HADOOP_HOME/bin$ ./hadoop dfs -mkdir /tmp
     $HADOOP_HOME/bin$ ./hadoop dfs -mkdir /user/hive/warehouse
     $HADOOP_HOME/bin$ ./hadoop dfs -chmod g+w /tmp
     $HADOOP_HOME/bin$ ./hadoop dfs -chmod g+w /user/hive/warehouse

    ```

2.  要从外壳使用 Hive 命令行界面(`cli`)，请使用以下脚本:

    ```sh
     $HIVE_HOME/bin$ ./hive

    ```

## HiveQL

HiveQL 拥有广泛的 Hive 内置运算符，Hive 内置函数，Hive 内置聚合函数，UDF 和 UDAF 为用户自定义函数。

### 数据定义语言操作

让我们从 DDL 操作命令开始，这些命令是:

*   `Create database`:使用`Create database`命令在 Hive 中创建数据库。示例:

    ```sh
    hive> Create database shiva;
    OK
    Time taken: 0.764 seconds

    ```

*   `Show database`:使用`Show database`命令列出 Hive 中所有现有的数据库。示例:

    ```sh
    hive> show databases;

    OK
    default
    shiva
    Time taken: 4.458 seconds, Fetched: 2 row(s)

    ```

*   `Use database`:使用`Use database`命令为会话选择一个数据库。示例:

    ```sh
    hive> use shiva;

    ```

*   `Create table`:使用`Create table`创建一个 Hive 表。在 create table 命令中，我们可以指定一个表是托管表还是外部表，如果它需要分区、分时段和 Hive 表中的其他重要功能的话。一个简单的“创建表格”选项的例子是:

    ```sh
    hive> Create table person (name STRING , add STRING);

    ```

`Create table`命令有许多选项，我们将在接下来给出的创建表格部分中看到。前面的命令是 Hive 中最简单的表格创建形式。

`Create`命令针对具体情况有很多选项，其要求是:

```sh
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name
[(col_name data_type [COMMENT col_comment], ...)]
[COMMENT table_comment]
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
[CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
[ROW FORMAT row_format] [STORED AS file_format]
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
[AS select_statement]

```

*   **创建表**:这个命令用给定的表名和下面解释的选项创建一个表:
    *   **如果不存在**:如果同名的表或视图已经存在，该命令用于跳过错误。
    *   **EXTERNAL 关键字**:正如我们前面讨论的，这个命令允许我们创建一个表，我们必须为此提供一个 LOCATION。
    *   **ROW FORMAT** :我们可以在创建表的时候使用自定义 SerDe 或者原生 SerDe。如果未指定行格式或指定了行格式分隔，则使用本机 SerDe。您可以使用 DECLARED 子句读取分隔文件。
    *   **STORED AS** :如果数据需要以纯文本文件的形式存储，我们可以使用 TEXTFILE。如果数据需要压缩，请使用存储为序列文件。
    *   **PARTITIONED BY** :可以使用 PARTITIONED BY 子句创建分区表。
    *   **CLUSTERED BY** :此外，可以使用列对表或分区进行分桶，并且可以通过 SORT BY 列在该桶内对数据进行排序。这可以提高某些类型查询的性能。
    *   **TBLPROPERTIES** :该子句允许您使用自己的元数据键/值对来标记表定义。
*   **显示表格**:`Show tables`命令用于列出数据库中的所有表格:

    ```sh
    hive>Show tables;
    OK
    person
    Time taken: 0.057 seconds, Fetched: 1 row(s)

    hive>Show tables '.*n';-- List all the table end that end with s.

    OK
    person
    Time taken: 0.057 seconds, Fetched: 1 row(s)

    ```

*   **描述表**:使用`describe table`命令获取关于表和列的有用信息。

    ```sh
    hive> describe person;
    OK
    name                    string                  None
    add                     string                  None
    Time taken: 0.181 seconds, Fetched: 2 row(s)

    ```

*   **更改表**:使用`Alter table`命令更改表元数据，添加分区或桶。

    ```sh
    hive> Alter table person ADD COLUMNS (PNO  INT);
    OK
    Time taken: 0.334 seconds

    ```

*   **删除表**:使用`drop table`命令从 Hive 元数据中删除表；如果表是 Hive 管理的，那么这个命令也将删除数据，如果它是外部的，那么只有 Hive 元数据被删除。

    ```sh
    hive>drop table person;

    ```

### DML(数据操作语言)操作

现在，让我们看看 DML 的操作命令:

**加载数据**:Hive 中的文件可以从本地加载，也可以从 HDFS 加载；默认情况下，Hive 将在 HDFS 显示。

我们使用的输入数据是简单的个人数据，有`Name`、`add`和`pno`；这个数据的例子是这样的:

<colgroup><col> <col> <col></colgroup> 
| 

名字

 | 

增加

 | 

进行性核性眼肌麻痹

 |
| --- | --- | --- |
| 艾尔文·琼纳 | 尼斯大道 678-8957 号 | one |
| 贾斯帕·罗伯逊 | 公元前 8336 年。 | Two |
| 迪尔德丽·富尔顿 | 页:1。街上 | three |
| 希拉里·克雷格 | AP #198-3439 id 关闭。 | four |
| 布雷泽·卡尔 | 普鲁斯路 283-9985 号 | five |

请看下面的命令:

```sh
hive>LOAD DATA INPATH 'hdfs://localhost:9000/user/hive/shiva/PersonData.csv' OVERWRITE INTO TABLE person;

Loading data to table shiva.person
OK
Time taken: 0.721 seconds

```

前面的命令将数据从 HDFS 文件/目录加载到表中，从 HDFS 加载数据的过程将导致文件/目录的移动。

对于本地数据加载，请使用以下代码:

```sh
hive>LOAD DATA LOCAL INPATH './examples/shiva/file1.txt' OVERWRITE INTO TABLE person;

```

我们还可以用 PARTITION 加载数据:

```sh
hive>LOAD DATA LOCAL INPATH './examples/ shiva /file2.txt'
 OVERWRITE INTO TABLE person PARTITION (date='26-02-2014');

```

### SQL 操作

在 Hive 中查询数据可以按照以下部分进行:

**选择** : `SELECT`是 SQL 中的投影运算符。用于此功能的子句有:

*   `SELECT`扫描由`FROM`子句指定的表格
*   `WHERE`给出过滤什么的条件
*   `GROUP BY`给出列列表，然后指定如何聚合记录
*   `CLUSTER BY`、`DISTRIBUTE BY`和`SORT BY`指定排序顺序和算法
*   `LIMIT`指定要检索的记录数量:

    ```sh
    SELECT [ALL | DISTINCT] select_expr, select_expr,
    FROM table_reference
    [WHERE where_condition]
    [GROUP BY col_list]
    [HAVING having_condition]
    [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list]]
    [LIMIT number];

    ```

示例:

```sh
hive>select * from person where name = 'Alvin Joyner';

Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201503051113_2664, Tracking URL = http://machine76.bigdatadomain.com:50030/jobdetails.jsp?jobid=job_201503051113_2664
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2015-03-24 14:52:54,541 Stage-1 map = 0%,  reduce = 0%
2015-03-24 14:52:58,570 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.57 sec
2015-03-24 14:52:59,579 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.57 sec
MapReduce Total cumulative CPU time: 2 seconds 570 msec
Ended Job = job_201503051113_2664
MapReduce Jobs Launched:
Job 0: Map: 1   Cumulative CPU: 2.57 sec   HDFS Read: 4502 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 570 msec
OK
Time taken: 12.53 seconds

```

#### 连接

HiveQL 支持以下类型的连接:

*   加入
*   左外连接
*   右外连接
*   完全外部连接

HiveQL 中只支持`equi join`；无法执行非相等条件连接。HiveQL 中的默认连接选项是等价连接，而 SQL 中的默认连接是内部连接；《也在场》中的一个句法差异是，我们不得不提到左`OUTER JOIN`和`RIGHT OUTER JOIN`，而在`SQL LEFT JOIN`和`RIGHT JOIN`的作品中。

HiveQL 被转换成 MapReduce 作业，因此我们必须在设计查询时牢记 MapReduce 范例。根据解析器和优化计划，连接作为 Mapside 连接或减少侧连接执行，因此经验法则是尽早连接较小的表，以避免大量数据传输或处理，并在最后连接较大的表。这背后的原因是，在关节的每个 MapReduce 阶段，最后一个表都是通过减速器精简的；而其他的被缓冲。

示例:

```sh
Hive> SELECT a.val1, a.val2, b.val, c.val
 > FROM a
 > JOIN b ON (a.key = b.key)
 > LEFT OUTER JOIN c ON (a.key = c.key);

```

如 Hive wiki 中所述，不支持以下条件:

*   联合后跟一个映射连接
*   横向视图，后跟地图连接
*   减少接收器(分组依据/连接/排序依据/群集依据/分发依据)，然后是映射连接
*   地图连接后跟联合
*   映射连接后跟连接
*   MapJoin 后跟 MapJoin

#### 聚合

HiveQL 支持聚合，也允许同时进行多个聚合。可能的聚合器有:

*   `count(*)`、`count(expr)`、`count(DISTINCT expr[, expr_.])`
*   `sum(col)`、`sum(DISTINCT col)`
*   `avg(col)`、`avg(DISTINCT col)`
*   `min(col)`
*   `max(col)`

示例:

```sh
hive> SELECT a, sum(b) FROM t1
 > GROUP BY a;

```

Hive 还支持`Group By`的地图端聚合，以提高性能，但需要更多内存。如果我们将`hive.map.aggr`设置为真(默认值为假)，那么 Hive 将直接在地图任务中进行一级聚合。

```sh
hive> set hive.map.aggr=true;
hive> SELECT COUNT(*) FROM table2;

```

### 内置功能

Hive 有许多内置功能，其一些广泛使用的功能有:

*   `concat(string A, string B,...)`
*   `substr(string A, int start)`
*   `round(double a)`
*   `upper(string A), lower(string A)`
*   `trim(string A)`
*   `to_date(string timestamp)`
*   `year(string date), month(string date), day(string date)`

### 自定义 UDF(用户定义函数)

我们可以创建自己的自定义 UDF 函数，并在 Hive 查询中使用它。Hive 为用户定义的函数提供了一个接口，在这里可以用 Java 编写和部署自定义函数，这些函数可以在 HiveQL 中作为函数使用。定制 UDF 要执行的步骤如下:

1.  创建一个新的 Java 类，用一个或多个名为 evaluate:

    ```sh
    import org.apache.hadoop.hive.ql.exec.UDF;
    import org.apache.hadoop.io.Text;
    public class LowerUDF extends UDF
    {
      public Text evaluate(final Text s)
      {
        if (s == null) { return null; }
        return new Text(s.toString().toLowerCase());
      }
    }
    ```

    的方法扩展 UDF
2.  现在，编译函数，制作 jar。
3.  Deploy jars for user-defined functions:

    ```sh
    hive> add jar my_jar.jar;

    ```

    在类路径中添加了`my_jar.jar`

4.  一旦 Hive 开始在类路径中使用您的 jars，最后一步就是注册您的函数:

    ```sh
    create temporary function my_lowerUDF as 'Lower';

    ```

5.  现在，你可以开始使用它了。

    ```sh
    hive> select my_lowerUDF(title), sum(freq) from titles group by my_lowerUDF(title);

    ```

## 管理表格–外部与托管

Hive 可以灵活地只管理元数据或元数据以及数据。在 Hive 中，两种类型的数据管理是:

*   **托管**:元数据连同数据将由 Hive 管理。
*   **外部**:Hive 只存储和管理元数据。

如果我们想让 Hive 管理表的生命周期，就应该使用 Hive 中的托管表，如果是临时表，就应该使用数据。

使用外部表格的优势是:

*   我们可以使用一个自定义的位置，如 HBase、Cassandra 等。
*   数据可以由另一个系统处理，可以避免锁定，同时处理和提高性能
*   在 DROP 表命令中，只有元数据将被删除，数据不会被删除。

## ser

使用 Hadoop 的重要好处之一是它可以灵活地存储，并提供接口来处理半结构化和非结构化数据。Hive 也可以用于处理这些数据；Hive 这样做是因为它复杂的数据类型和 SerDe 属性。SerDe 是一个 serializer 和 Serializer 接口，可以允许对 Java 对象中的字符串或二进制数据进行编组和解组，Hive 可以使用这些数据在表中进行读写。Hive 有一些内置的 SerDe 库，比如 Avro、ORC、RegEx、节俭、Parquet 和 CSV 它还有一个像亚马逊提供的 JSON SerDe 那样的第三方 SerDe。

我们也可以写我们的自定义 SerDe。为了编写定制的 SerDe 类，我们必须重写一些方法:

*   `public void initialize (Configuration conf, Properties tbl) throws SerDeException`:方法`initialize()`只调用一次，我们可以从表属性中获取和设置一些常用的信息，比如列类型和名称。
*   `public Writable serialize (Object obj, ObjectInspector oi) throws SerDeException`:`serialize()`方法要有序列化的逻辑，取一个代表一行数据的 Java 对象，生成一个可序列化的可写接口对象。
*   `public Class<? extends Writable> getSerializedClass ()`:`getSerializedClass()`返回序列化对象的返回类型类。
*   `public Object deserialize (Writable blob) throws SerDeException`:`deserialize()`应该有反序列化逻辑。
*   `public ObjectInspector getObjectInspector () throws SerDeException`:`ObjectInspectors`是用于描述和检查复杂类型层次结构的 Hive 对象。
*   `public SerDeStats getSerDeStats()`:它们覆盖支持一些统计。

让我们看一下实现自定义服务的代码:

```sh
public class CustomSerDe implements SerDe {

 private StructTypeInfo rowTypeInfo;
 private ObjectInspector rowOI;
 private List<String> colNames;
 Object[] outputFields;
 Text outputRowText;
 private List<Object> row = new ArrayList<Object>();

 @Override
 public void initialize(Configuration conf, Properties tbl)throws SerDeException {
   // Get a list of the table's column names.
   String colNamesStr = tbl.getProperty(Constants.LIST_COLUMNS);
   colNames = Arrays.asList(colNamesStr.split(","));

   // Get a list of TypeInfos for the columns. This list lines up with
   // the list of column names.
   String colTypesStr = tbl.getProperty(Constants.LIST_COLUMN_TYPES);
   List<TypeInfo> colTypes = TypeInfoUtils.getTypeInfosFromTypeString(colTypesStr);
   rowTypeInfo = (StructTypeInfo) TypeInfoFactory.getStructTypeInfo(colNames, colTypes);
   rowOI = TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(rowTypeInfo);
 }

 @Override
 public Object deserialize(Writable blob) throws SerDeException {
   row.clear();
   // Implement the logic of Deserialization
   return row;
 }

 @Override
 public ObjectInspector getObjectInspector() throws SerDeException {
   return rowOI;
 }

 @Override
 public SerDeStats getSerDeStats() {
   return null;
 }

 @Override
 public Class<? extends Writable> getSerializedClass() {
   return Text.class;
 }

 @Override
 public Writable serialize(Object obj, ObjectInspector oi)
     throws SerDeException {
   // Implement Logic of Serialization
   return outputRowText;
 }
}
```

我们必须创建类的 jar 文件，并将其放入 Hive 服务器中。然后，我们可以在创建表时使用 SerDe，如下面的代码所示:

```sh
CREATE EXTERNAL TABLE IF NOT EXISTS my_table (field1 string, field2 int, field3 string, field4 double)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2\. CustomSerDe' LOCATION '/path-to/my_table/';
```

## 分区

Hive 支持数据分区，可用于水平分布数据。以为例，如果我们有一个大型事务表，该表频繁地查询一年或一系列月，那么我们可以在创建表时用`PARTITIONED BY (year INT, month INT)`对该表进行分区。

Hive 通过创建子目录作为分区字段的结构来管理数据，例如:

`/DB/Table/Year/Month/.`

`/db/table/2014/11/.`

`/db/table/2014/12/.`

`/db/table/2015/1/.`

`/db/table/2015/2/.`

分区适用于托管表和外部表，建议用于非常大的表，这样可以限制要处理的文件，并为提高性能提供巨大优势。

分区应该小心进行，因为它可能有以下缺点:

*   如果没有正确选择分区列，那么它可能会不均匀地划分数据，查询执行将不会得到优化。
*   如果分区层次级别变高，则递归扫描目录将比完全数据扫描更昂贵。

## 颠簸

我们刚刚讨论了分区可以不均匀分布数据的事实，但是通常不太可能得到均匀分布。但是，我们可以使用 bucket 来实现几乎均匀的分布式数据处理。Bucketing 将一个数据值放入一个存储桶中，因此同一存储桶中可以有相同的值记录，并且一个存储桶可以有多组值。Bucketing 提供了对多个文件的控制，因为我们在使用`create table`时必须提到 buckets 的数量而使用`CLUSTERED BY (month) INTO #noofBuckets BUCKETS`。

为了数据的均匀分布，我们应该设置`hive.enforce.bucketing = true`。桶是辅助地图端连接的理想选择，因为桶中存在相同的值数据，合并排序将更快、更有效。它可以使用分区，也可以不使用分区。

# 总结

在本章中，我们探讨了 MapReduce 编程的两个包装器——Pig 和 Hive。

MapReduce 非常强大，但却是一条非常复杂的高学习曲线。困难的部分是管理 MapReduce 程序以及开发和优化所花费的时间。为了在 MapReduce 中更容易和更快地开发，我们有抽象层，例如 Pig，它是 MapReduce 之上的 Pig 拉丁语过程语言的包装器，以及 HiveQL 包装器，它是一个类似于 SQL 的 HiveQl 包装器。

数据流程模型中使用了 Pig，因为它使用 DAG 模型将 Pig 拉丁语言转换为 MapReduce 作业。Pig 在三个计划中进行转换，即从逻辑到物理到 MapReduce，其中每个计划翻译语句并产生一个优化的执行计划。Pig 还有交互分析数据的咕噜模式。Pig 有非常有用的命令来过滤、分组、聚合、共组等等，它还支持用户定义的函数。

Hive 被那些在类似 SQL 的开发中更舒服的用户使用，因为它有 HiveQL。配置单元体系结构包含驱动程序、元存储、查询编译器、执行引擎和 HiveServer。HiveQL 有一个详尽的内置函数和命令列表来分析数据。Hive 有许多内置函数，也支持用户定义的函数。

在下一章中，我们将介绍 Hadoop 中最重要的组件之一。它是非关系分布式数据库，具有高吞吐量和高性能；我们称之为 HBase。