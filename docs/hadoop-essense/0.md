# 零、前言

Hadoop 是一个非常吸引人和有趣的项目，各种组织和机构都对它表现出了极大的兴趣和贡献。Hadoop 已经走过了很长的路，从一个批处理系统变成了一个数据湖，并在各种 Hadoop 生态系统组件的帮助下，在低延迟下进行大容量流分析，特别是 Yarn。这一进展是实质性的，并使 Hadoop 成为一个强大的系统，可以设计为存储、转换、批处理、分析或流和实时处理系统。

Hadoop 项目作为一个数据湖可以分为多个阶段，如数据摄取、数据存储、数据访问、数据处理和数据管理。对于每个阶段，我们都有不同的子项目，它们是工具、实用程序或框架来帮助和加速这个过程。Hadoop 生态系统组件经过了测试、配置和验证，要自行构建类似的实用程序，需要花费大量的时间和精力。Hadoop 框架的核心是复杂的开发和优化。加速和简化这一过程的明智方法是利用不同的非常有用的 Hadoop 生态系统组件，这样我们就可以更加专注于应用流设计和与其他系统的集成。

随着 Hadoop 中许多有用的子项目和 Hadoop 生态系统中的其他工具的出现，出现的问题是何时以及如何有效地使用哪个工具。这本书旨在完成何时以及如何使用各种生态系统组件的拼图，并让您充分了解 Hadoop 生态系统实用程序以及应该使用它们的案例和场景。

# 这本书涵盖了什么

[第 1 章](1.html "Chapter 1. Introduction to Big Data and Hadoop")、*大数据与 Hadoop 介绍*，涵盖了大数据与 Hadoop 的概述，以及具有 Hadoop 优势和特点的不同用例模式。

[第二章](2.html "Chapter 2. Hadoop Ecosystem")、 *Hadoop 生态系统*，探讨 Hadoop 项目开发的不同阶段或层次，以及每一层可以使用的一些组件。

[第三章](3.html "Chapter 3. Pillars of Hadoop – HDFS, MapReduce, and YARN")、*Hadoop 的支柱——HDFS、MapReduce 和 Yarn*，是关于 Hadoop 的三个关键基础组件，分别是 HDFS、MapReduce 和 Yarn。

[第 4 章](4.html "Chapter 4. Data Access Components – Hive and Pig")、*数据访问组件–Hive 和 Pig* 涵盖了数据访问组件 Hive 和 Pig，它们分别是 MapReduce 框架之上的类 SQL 和 Pig 拉丁过程语言的抽象层。

[第五章](5.html "Chapter 5. Storage Component – HBase")、*存储组件–HBase*，详细介绍了 NoSQL 组件数据库 HBA se。

[第 6 章](6.html "Chapter 6. Data Ingestion in Hadoop – Sqoop and Flume")、*Hadoop 中的数据摄取–Sqoop 和 Flume* ，涵盖了数据摄取库工具 SQOOP 和 Flume。

[第 7 章](7.html "Chapter 7. Streaming and Real-time Analysis – Storm and Spark")、*流和实时分析–Storm 和 Spark*，是关于建立在 Yarn 之上的流和实时框架 Storm 和 Spark。

# 这本书你需要什么

这本书的先决条件是很好地理解 Java 编程和分布式计算的基础知识，这对理解 Hadoop 及其生态系统组件非常有帮助和兴趣。

### 注

代码和语法已经在 Hadoop 2.4.1 和其他兼容的生态系统组件版本中进行了测试，但在较新的版本中可能会有所不同。

# 这本书是给谁的

如果你是一个对学习如何使用 Hadoop 框架解决实际问题感兴趣的系统或应用开发人员，那么这本书非常适合你。这本书也是为 Hadoop 专业人士准备的，他们希望找到解决他们在 Hadoop 项目中遇到的不同挑战的方法。它假定您熟悉分布式存储和分布式应用。

# 惯例

在这本书里，你会发现许多区分不同种类信息的文本样式。以下是这些风格的一些例子和对它们的意义的解释。

文本中的码字、数据库表名、文件夹名、文件名、文件扩展名、路径名、伪 URL、用户输入和 Twitter 句柄如下所示:“我们可以通过使用`include`指令来包含其他上下文。”

代码块设置如下:

```sh
public static class MyPartitioner extends   org.apache.hadoop.mapreduce.Partitioner<Text,Text>

{
  @Override
  public int getPartition(Text key, Text value, int numPartitions)
  {
   int count =Integer.parseInt(line[1]);
   if(count<=3)
    return 0;
   else
    return 1;
  }
}

And in Driver class
job.setPartitionerClass(MyPartitioner.class);
```

任何命令行输入或输出都编写如下:

```sh
hadoop fs -put /home/shiva/Samplefile.txt  /user/shiva/dir3/

```

### 注

警告或重要提示会出现在这样的框中。

### 类型

提示和技巧是这样出现的。

# 读者反馈

我们随时欢迎读者的反馈。让我们知道你对这本书的看法——你喜欢或不喜欢什么。读者反馈对我们来说很重要，因为它有助于我们开发出你真正能从中获益的标题。

要给我们发送一般反馈，只需发送电子邮件`<[feedback@packtpub.com](mailto:feedback@packtpub.com)>`，并在您的邮件主题中提及书名。

如果你对某个主题有专业知识，并且对写作或投稿感兴趣，请参见我们位于[www.packtpub.com/authors](http://www.packtpub.com/authors)的作者指南。

# 客户支持

现在，您已经自豪地拥有了一本书，我们有许多东西可以帮助您从购买中获得最大收益。

## 下载示例代码

您可以从您在[http://www.packtpub.com](http://www.packtpub.com)的账户下载您购买的所有 Packt Publishing 书籍的示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便将文件直接通过电子邮件发送给您。

## 勘误表

尽管我们尽了最大努力来确保我们内容的准确性，但错误还是会发生。如果你在我们的某本书里发现了错误——可能是文本或代码中的错误——如果你能向我们报告，我们将不胜感激。通过这样做，你可以让其他读者免受挫折，并帮助我们改进这本书的后续版本。如果您发现任何勘误表，请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，选择您的书籍，点击**勘误表提交表**链接，并输入您的勘误表的详细信息。一旦您的勘误表得到验证，您的提交将被接受，勘误表将上传到我们的网站或添加到该标题勘误表部分下的任何现有勘误表列表中。

要查看之前提交的勘误表，请前往[https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)并在搜索栏中输入图书名称。所需信息将出现在**勘误表**部分。

## 盗版

互联网上版权材料的盗版是所有媒体的一个持续问题。在 Packt，我们非常重视版权和许可证的保护。如果您在互联网上遇到任何形式的我们作品的非法拷贝，请立即向我们提供位置地址或网站名称，以便我们寻求补救。

请通过`<[copyright@packtpub.com](mailto:copyright@packtpub.com)>`联系我们，获取疑似盗版资料的链接。

我们感谢您在保护我们的作者方面的帮助，以及我们为您带来有价值内容的能力。

## 问题

如果您对本书的任何方面有问题，可以在`<[questions@packtpub.com](mailto:questions@packtpub.com)>`联系我们，我们将尽最大努力解决问题。