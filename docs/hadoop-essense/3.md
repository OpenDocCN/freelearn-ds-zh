# 三、Hadoop 的支柱——HDFS、MapReduce 和 Yarn

我们在最后两章讨论了大数据、Hadoop 和 Hadoop 生态系统。现在，让我们讨论更多关于 **Hadoop 架构**的技术方面。Hadoop 架构极其灵活、可扩展和容错。Hadoop 成功的关键是它的架构允许数据按原样加载并以分布式方式存储，没有数据丢失，也不需要预处理。

我们知道 Hadoop 是分布式计算，是一个并行处理环境。Hadoop 架构可以分为两部分:存储和处理。Hadoop 中的存储由 **Hadoop 分布式文件系统**(**HDFS**)**处理，处理由**T7】MapReduce 处理，如下图所示:

![Pillars of Hadoop – HDFS, MapReduce, and YARN](img/image00201.jpeg)

在本章中，我们将介绍 HDFS 概念、体系结构、一些关键特性、读写过程是如何发生的，以及一些示例。MapReduce 是 Hadoop 的核心，我们将涵盖架构、序列化数据类型、MapReduce 步骤或流程、各种文件格式以及编写 MapReduce 程序的示例。在这之后，我们将来到在 Hadoop 中最有前途的 Yarn，许多应用已经采用了 Yarn，这提升了 Hadoop 的能力。

# HDFS

HDFS 是 Hadoop 中的默认存储文件系统，它是分布式的，设计相当简单，并且具有极高的可扩展性、灵活性和高容错能力。HDFS 架构具有主从模式，因此可以更好地管理和利用从节点。HDFS 甚至可以在商用硬件上运行，并且该体系结构接受一些节点可以关闭，并且仍然需要恢复和处理数据。HDFS 具有自愈过程和推测执行，这使得系统具有容错性，并且可以灵活地添加/移除节点，增加了可扩展性和可靠性。HDFS 被设计成最适合 MapReduce 编程。HDFS 的一个关键假设是*移动计算比移动数据*便宜。

## HDFS 的特色

HDFS 的重要特征如下:

*   **可扩展性** : HDFS 可扩展到千兆字节甚至更多。HDFS 足够灵活，可以添加或删除节点，这可以实现可扩展性。
*   **可靠性和容错** : HDFS 将数据复制到一个可配置的参数中，这使得获得高可靠性的灵活性和增加系统的容错性，因为数据将存储在多个节点中，即使少数节点停机，也可以从其他可用节点访问数据。
*   **数据一致性** : HDFS 有 **WORM** ( **写一次，读多次**)模型，简化了数据一致性，给出了高吞吐量。
*   **硬件故障恢复** : HDFS 假设集群中的一些节点可能会出现故障，并且具有良好的故障恢复流程，这使得 HDFS 即使在商用硬件中也能运行。HDFS 有故障转移流程，可以恢复数据和处理硬件故障恢复。
*   **可移植性** : HDFS 在不同的硬件和软件上都是可移植的。
*   **计算更接近数据** : HDFS 将计算过程移向数据，而不是将数据拉出来进行计算，这要快得多，因为数据是分布式的，非常适合 MapReduce 过程。

## HDFS 建筑

HDFS 是由守护进程管理的，守护进程如下:

*   **名称节点**:主进程
*   **数据节点**:从进程
*   **检查点名称节点或辅助名称节点**:检查点进程
*   **备份节点**:备份名称节点

HDFS 架构如下图所示:

![HDFS architecture](img/image00202.jpeg)

### 姓氏

NameNode 是 HDFS 的主进程守护服务器，负责协调 Hadoop 中所有与存储相关的操作，包括 HDFS 的读写操作。名称节点管理文件系统名称空间。名称节点保存所有文件块之上的元数据，并且其中数据块的所有节点都存在于集群中。名称节点不存储任何数据。名称节点缓存数据并将元数据存储在内存中，以便更快地访问，因此它需要一个具有高内存的系统，否则名称节点会成为集群处理的瓶颈。

名称节点在 HDFS 是一个非常关键的过程，并且是单点故障，但是 HDFS 可以配置为 HDFS 高可用性，这允许两个名称节点，其中一个在某个时间点只能处于活动状态，另一个将处于待机状态。备用名称节点将获得更新和数据节点状态，这使得备用名称节点可以在名称节点的活动节点出现故障时接管和恢复。

名称节点维护以下两个元数据文件:

*   **Fsimage 文件**:保存整个文件系统名称空间，包括块到文件和文件系统属性的映射
*   **编辑日志文件**:这个保存文件系统元数据发生的每一个变化

当 NameNode 启动时，它从磁盘读取`FsImage`和`EditLog`文件，将`EditLog`中的所有事务合并到`FsImage`中，并将这个新版本刷新到磁盘上的新`FsImage`中。然后，它可以截断旧的`EditLog`，因为它的事务已经应用于持久的`FsImage`。

### 数据节点

数据节点保存 HDFS 的实际数据，还负责创建、删除和复制数据块，由名称节点分配。数据节点向名称节点发送消息，这些消息被称为周期间隔内的心跳。如果数据节点未能发送心跳消息，则名称节点会将其标记为死节点。如果数据节点中的文件数据小于复制因子，则名称节点会将文件数据复制到其他数据节点。

![DataNode](img/image00203.jpeg)

图片来源:[http://yoyoclouds . files . WordPress . com/2011/12/Hadoop _ arch . png](http://yoyoclouds.files.wordpress.com/2011/12/hadoop_arch.png)。

### 检查点名称节点或辅助名称节点

检查点名称节点，早期称为辅助名称节点，是一个经常合并`FsImage`和`EditLog`文件的数据检查点的节点，在任何名称节点出现故障时可用于名称节点。检查点名称节点收集并存储最新的检查点。存储后，它合并元数据中的更改，使其可用于名称节点。检查点名称节点通常必须是一个单独的节点，并且它需要与名称节点类似的配置机器，因为内存需求与名称节点相同。

### 备份节点

BackupNode 与 Checkpoint NameNode 类似，但它将`FsImage`的更新副本保存在 RAM 内存中，并始终与 NameNode 同步。备份节点的内存需求与命名节点相同。在高可用性方面，备份节点可以配置为热备用节点，动物园管理员协调将备份节点作为故障转移名称节点。

## HDFS 的数据存储

在 HDFS，文件被分成块，存储在多个数据节点中，它们的元数据存储在名称节点中。为了理解 HDFS 是如何工作的，我们需要了解一些参数以及为什么使用它。参数如下:

*   **区块**:文件分为多个区块。在 HDFS，块是可配置的参数，我们可以在这里设置值，文件将按块大小划分:在 2.2.0 之前的版本中，默认块大小为 64 MB，在 Hadoop 2.2.0 之后的版本中，默认块大小为 128 MB。数据块大小较高，可以最大限度地减少磁盘寻道时间的成本(速度较慢)，利用传输速率(可能较高)，并减少文件在名称节点中的元数据大小。
*   **复制**:之前分割的每个文件块都存储在多个数据节点中，我们可以配置复制因子的数量。默认值为`3`。复制因素是实现容错的关键。复制因子的数量越多，系统的容错性就越高，并且会占用文件保存的大量时间，还会增加 NameNode 中的元数据。我们要平衡复制因素，不能太高也不能太低。

### 读取管道

HDFS 读取过程如下图所示:

![Read pipeline](img/image00204.jpeg)

HDFS 读取过程包括以下六个步骤:

1.  使用 Hadoop 客户端应用编程接口的**分布式文件系统**对象的客户端调用`open()`，启动读取请求。
2.  **分布式文件系统**与**名称节点**连接。**名称节点**标识要读取的文件的块位置以及块所在的**数据节点**。**命名节点**然后按照距离客户端最近的数据节点的顺序发送**数据节点列表**。
3.  **分布式文件系统**然后创建 **FSDataInputStream** 对象，该对象反过来包装一个 **DFSInputStream** ，该 DFSInputStream 可以连接到所选的**数据节点**，并获取块，然后返回到客户端。客户端通过调用 **FSDataInputStream** 的`read()`发起转账。
4.  **FSDataInputStream** 反复调用`read()`方法获取块数据。
5.  当到达数据块的末尾时， **DFSInputStream** 关闭来自数据节点的连接，并为下一个数据块识别最佳数据节点。
6.  当客户端读取完毕后，会在 **FSDataInputStream** 上调用`close()`关闭连接。

### 写流水线

HDFS 写流水线流程总结如下图:

![Write pipeline](img/image00205.jpeg)

HDFS 写流水线流程描述如下七个步骤:

1.  客户端使用 Hadoop 客户端应用编程接口的**分布式文件系统**对象，调用`create()`，启动写请求。
2.  **分布式文件系统**与**名称节点**连接。**名称节点**启动新文件创建，并在元数据中创建新记录，并启动类型为 **FSDataOutputStream** 的输出流，该输出流包装 **DFSOutputStream** 并将其返回给客户端。在启动文件创建之前，**名称节点**检查文件是否已经存在，客户端是否有权限创建新文件，如果任何条件为真，则向客户端抛出一个 IOException。
3.  客户端使用 **FSDataOutputStream** 对象写入数据，并调用`write()`方法。 **FSDataOutputStream** 对象是 DFSOutputStream，处理与数据节点和**名称节点**的通信。
4.  DFSOutputStream 将文件分割成块，并用**名称节点**进行协调，以识别**数据节点**和副本数据节点。复制因子的数量将是标识的数据节点的数量。数据将分组发送到一个**数据节点**，该**数据节点**将相同的分组发送到第二个**数据节点**，第二个**数据节点**将发送到第三个，以此类推，直到确定数据节点的数量。
5.  当所有数据包被接收和写入时，数据节点向发送方**数据节点**向客户端发送确认数据包。DFSOutputStream 在内部维护一个队列，以检查数据包是否被**数据节点**成功写入。如果没有收到确认或者**数据节点**在写入时失败，DFSOutputStream 也会进行处理。
6.  如果所有数据包都已成功写入，客户端将关闭流。
7.  如果该过程完成，则**分布式文件系统**对象通知**名称节点**该状态。

HDFS 有一些重要的概念，使架构容错和高度可用。

## 机架感知

HDFS 是容错的，这可以通过跨节点配置机架感知来增强。在大型 Hadoop 集群系统中，数据节点将跨多个机架，可在 HDFS 进行配置，以识别数据节点的机架信息。在最简单的形式中，可以通过使用一个脚本使 HDFS 知道机架，该脚本可以为节点的 IP 地址返回机架地址。要设置试管架映射脚本，请在`conf/hadoop-site.xml`中指定键`topology.script.file.name`，它必须是可执行脚本或程序，应提供运行命令以返回试管架标识。

Hadoop 中的机架标识是分层的，看起来像路径名。默认情况下，每个节点的机架标识为/default-rack。您可以将节点的机架号设置为任意路径，例如`/foo/bar-rack`。更靠左的路径元素在树的上方。因此，大型安装的合理结构可能是`/top-switch-name/rack-name`。Hadoop 机架标识将用于查找副本放置的远近节点。

### 机架意识在 HDFS 的优势

机架感知可用于在整个机架出现故障时防止数据丢失，并在读取文件时识别存在数据块的最近节点。为了实现高效的机架感知，一个节点不能有同一个数据块的两个副本，在一个机架中，一个数据块最多可以存在于两个节点中。用于数据块复制的机架数量应始终少于数据块副本的总数。

考虑以下情况:

*   **写入块**:创建新块时，第一个副本放在本地节点上，第二个副本放在不同的机架上，第三个副本放在本地机架的不同节点上
*   **读取数据块**:对于读取请求，与正常读取过程一样，NameNode 按照距离客户端较近的数据节点的顺序发送数据节点列表，因此优先选择同一机架的数据节点

为了验证数据块是否损坏，HDFS 进行了块扫描。每个数据节点都会检查其中存在的数据块，并使用在数据块创建过程中生成的存储校验和进行验证。在 HDFS 客户端读取数据块并且数据节点收到结果通知后，也会验证校验和。Block Scanner 计划运行三周，也可以进行配置。

如果发现数据块损坏，将通知名称节点，名称节点会将数据节点中的数据块标记为损坏，并启动数据块的复制，一旦创建了一个良好的拷贝并通过校验和验证，该数据节点中的数据块将被删除。

## HDFS 联邦

我们已经讨论过名称节点与数据节点紧密耦合，是 Hadoop 1 中的 SPOF T2(T3)单点故障 T4。 *x* 。让我们试着理解 HDFS 1.0 的局限性来理解 HDFS 联邦的必要性。

### HDFS 1.0 的局限性

以下是一些限制:

*   **文件数量的限制**:尽管 HDFS 可以有成百上千个节点，但是由于 NameNode 将元数据保存在内存中，因此可以存储的文件数量会受到限制，这取决于分配给 NameNode 的映射堆内存。这种限制是由单个名称节点引起的。
*   **单一命名空间**:由于单一命名空间，NameNode 无法委派任何工作负载，可能成为瓶颈。
*   **SPOF** : NameNode 是单点故障，因为它很关键，NameNode 可能会有太多的工作负载。
*   **不能运行非 MapReduce 应用** : HDFS 只设计运行 MapReduce 进程的应用或者基于 MapReduce 框架的应用。

名称节点只有一个名称空间，与数据节点紧密耦合，因为所有的请求都必须与名称节点协调才能获得块的位置，因此它可能成为瓶颈。名称节点必须高度可用，否则请求将无法得到服务。HDFS 联盟是一项功能，它使 Hadoop 能够拥有独立的多个命名空间，克服了我们讨论的限制。让我们看看下面的图片:

![Limitations of HDFS 1.0](img/image00206.jpeg)

通过多个独立的名称空间层次结构，名称节点的责任在多个名称空间之间共享，这些名称空间是联合的，但共享集群的数据节点。由于名称节点的联合，一些请求可以在名称节点之间获得负载平衡。联合名称节点在多租户架构中工作，以提供隔离。

### HDFS 联邦的利益

读取和写入过程由于多个名称节点而更快，避免了单个名称节点过程中的瓶颈。

水平可伸缩性是由 HDFS 联邦实现的，它有一个巨大的优势，即它是一个高度可用的进程，也可以充当负载平衡器。

## HDFS 港口

在 Hadoop 生态系统中，组件有不同的端口，通信通过各自的端口进行。通常，端口号会很难记住。

默认的 HDFS web UI 端口汇总于[的 Hortonworks 文档中。](http://docs.hortonworks.com/HDPDocuments/HDP1/HDP-1.2.0/bk_reference/content/reference_chap2_1.html)

<colgroup><col> <col> <col> <col> <col> <col> <col></colgroup> 
| 

服务

 | 

服务器

 | 

使用的默认端口

 | 

草案

 | 

描述

 | 

需要最终用户访问吗？

 | 

配置参数

 |
| --- | --- | --- | --- | --- | --- | --- |
| WebUI 名称节点 | 主节点(名称节点和任何备份名称节点) | Fifty thousand and seventy | 超文本传送协议（Hyper Text Transport Protocol 的缩写） | 查看 HDFS 的当前状态，探索文件系统 | 是(通常是管理员、开发/支持团队) | dfs.http.address |
| Fifty thousand four hundred and seventy | https | 安全 http 服务 | dfs.https.address |
| 名称节点元数据服务 | 主节点(名称节点和任何备份名称节点) | 8020/9000 | 工业程序控制（ industrial process control 的缩写） | 文件系统元数据操作 | 是(所有直接需要与 HDFS 互动的客户) | 嵌入在由 fs.default.name 指定的 URI 中 |
| 数据节点 | 所有从节点 | Fifty thousand and seventy-five | 超文本传送协议（Hyper Text Transport Protocol 的缩写） | 访问状态、日志等。 | 是(通常是管理员、开发/支持团队) | dfs.datanode.http.address |
| Fifty thousand four hundred and seventy-five | https | 安全 http 服务 | dfs.datanode.https.address |
| Fifty thousand and ten |   | 数据传送 |   | dfs.datanode.address |
| Fifty thousand and twenty | 工业程序控制（ industrial process control 的缩写） | 元数据操作 | 不 | dfs.datanode.ipc.address |
| 检查站名称节点或者辅助名称节点 | 辅助名称节点和任何备份辅助名称节点 | Fifty thousand and ninety | 超文本传送协议（Hyper Text Transport Protocol 的缩写） | 名称节点元数据的检查点 | 不 | dfs.secondary.http.address |

## HDFS 命令道

Hadoop 命令行环境类似于 Linux。Hadoop 文件系统(fs)提供各种外壳命令来执行文件操作，例如复制文件、查看文件内容、更改文件所有权、更改权限、创建目录等。

Hadoop fs shell 命令的语法如下:

```sh
hadoop fs <args>

```

1.  在 HDFS 的给定路径上创建一个目录:
    *   用法:

        ```sh
        hadoop fs -mkdir <paths>

        ```

    *   示例:

        ```sh
        hadoop fs -mkdir /user/shiva/dir1 /user/shiva/dir2

        ```

2.  列出目录的内容:
    *   用法:

        ```sh
        hadoop fs -ls <args>

        ```

    *   示例:

        ```sh
        hadoop fs -ls /user/shiva

        ```

3.  在 HDFS 存档和获取文件:
    *   用法(放):

        ```sh
        hadoop fs -put <localsrc> ... <HDFS_dest_Path>

        ```

    *   示例:

        ```sh
        hadoop fs -put /home/shiva/Samplefile.txt  /user/shiva/dir3/

        ```

    *   用法(获取):

        ```sh
        hadoop fs -get <hdfs_src> <localdst>

        ```

    *   示例:

        ```sh
        hadoop fs -get /user/shiva/dir3/Samplefile.txt /home/

        ```

4.  参见文件内容:
    *   用法:

        ```sh
        hadoop fs -cat <path[filename]>

        ```

    *   示例:

        ```sh
        hadoop fs -cat /user/shiva/dir1/abc.txt

        ```

5.  将文件从源复制到目标:
    *   用法:

        ```sh
        hadoop fs -cp <source> <dest>

        ```

    *   示例:

        ```sh
        hadoop fs -cp /user/shiva/dir1/abc.txt /user/shiva/dir2

        ```

6.  将文件从/复制到本地文件系统到 HDFS:
    *   copyFromLocal 的用法:

        ```sh
        hadoop fs -copyFromLocal <localsrc> URI

        ```

    *   示例:

        ```sh
        hadoop fs -copyFromLocal /home/shiva/abc.txt  /user/shiva/abc.txt

        ```

    *   copyToLocal 的用法

        ```sh
        hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>

        ```

7.  将文件从源移动到目标:
    *   用法:

        ```sh
        hadoop fs -mv <src> <dest>

        ```

    *   示例:

        ```sh
        hadoop fs -mv /user/shiva/dir1/abc.txt /user/shiva/dir2

        ```

8.  删除 HDFS 的文件或目录:
    *   用法:

        ```sh
        hadoop fs -rm <arg>

        ```

    *   示例:

        ```sh
        hadoop fs -rm /user/shiva/dir1/abc.txt

        ```

    *   删除递归版本的用法:

        ```sh
        hadoop fs -rmr <arg>

        ```

    *   示例:

        ```sh
        hadoop fs -rmr /user/shiva/

        ```

9.  显示文件的最后几行:
    *   用法:

        ```sh
        hadoop fs -tail <path[filename]>

        ```

    *   示例:

        ```sh
        hadoop fs -tail /user/shiva/dir1/abc.txt

        ```

# MapReduce

MapReduce 是一个大规模并行处理框架，可以处理分布式环境中更快、可扩展和容错的数据。与 HDFS 类似，Hadoop MapReduce 甚至可以在商用硬件中执行，并假设节点随时可能出现故障，但仍能处理作业。MapReduce 可以并行处理大量数据，方法是将一个任务分成独立的子任务。MapReduce 也有主从架构。

输入和输出，甚至是 MapReduce 作业中的中间输出，都是以<`Key`、`Value` >对的形式。键和值必须是可序列化的，并且不使用 Java 序列化包，但是必须有一个接口，该接口必须实现，并且可以有效地序列化，因为数据处理必须从一个节点移动到另一个节点。键必须是实现可写可比接口的类，这是对键进行排序所必需的，值必须是实现可写接口的类。

## MapReduce 架构

MapReduce 架构有以下两个守护进程:

*   作业跟踪器:主流程
*   任务跟踪器:从进程

### 工作跟踪者

JobTracker 是主协调器守护进程，负责协调和完成 Hadoop 中的一个 MapReduce 作业。JobTracker 的主要功能是资源管理、跟踪资源可用性和任务流程周期。作业跟踪器识别任务跟踪器来执行某些任务，并监控任务的进度和状态。JobTracker 是 MapReduce 进程的单点故障。

### 任务跟踪器

任务跟踪器是执行作业跟踪器分配的任务的从守护进程。任务跟踪器定期向作业跟踪器发送心跳消息，以通知空闲插槽，并将任务的状态发送给作业跟踪器，并检查是否有任何任务必须执行。

## 序列化数据类型

MapReduce 中的序列化极其重要，因为数据和中间数据必须在非常大的规模上从一个 TaskTracker 移动到另一个 task tracker。Java 序列化没有优化，因为即使对于较小的值，对象序列化程序也会有较大的大小，这可能是 Hadoop 性能的瓶颈，因为 Hadoop 处理需要大量数据传输。因此，Hadoop 不使用 Java 序列化包，而是使用可写接口。

对于序列化，Hadoop 使用以下两个接口:

*   可写接口(对于值)
*   可写可比较接口(用于按键)

### 可写界面

可写接口用于序列化和反序列化的值。实现可写接口的类有`ArrayWritable`、`BooleanWritable`、`ByteWritable`、`DoubleWritable`、`FloatWritable`、`IntWritable`、`LongWritable`、`MapWritable`、`NullWritable`、`ObjectWritable`、`ShortWritable`、`TupleWritable`、`VIntWritable`和`VLongWritable`。

我们可以创建自己的自定义可写类，可以在 MapReduce 中使用。为了创建类，我们必须实现可写类，并实现以下两种方法:

*   `void write (DataOutput out)`:这将序列化对象
*   `void readFields (DataInput in)`:读取输入流并将其转换为对象

### 可写可比界面

WritableComparable 用于键，它是从可写接口继承而来的，实现了一个可比较的接口来提供值对象的比较。一些实现是`BooleanWritable`、`BytesWritable`、`ByteWritable`、`DoubleWritable`、`FloatWritable`、`IntWritable`、`LongWritable`、`NullWritable`、`ShortWritable`、`Text`、`VIntWritable`和`VLongWritable`。

我们可以创建自己的自定义可写可比较类，可以在 MapReduce 中使用。为了创建一个类，我们必须实现 WritableComparable 类，并实现以下三种方法:

*   `void write (DataPutput out)`:这将序列化对象
*   `void readFields (DataInput in)`:读取输入流并将其转换为对象
*   `Int compareTo (Object obj)`:比较排序关键字所需的值

## MapReduce 示例

MapReduce 最初很难理解，所以我们将尝试用一个简单的例子来理解它。假设我们有一个文件，其中包含一些单词，并且该文件在 HDFS 被划分为多个块，我们必须计算一个单词在该文件中出现的次数。我们将逐步使用 MapReduce 功能来实现这个结果。整个过程如下图所示:

![The MapReduce example](img/image00207.jpeg)

### 类型

**下载示例代码**

您可以从您在[http://www.packtpub.com](http://www.packtpub.com)的账户下载您购买的所有 Packt Publishing 书籍的示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便将文件直接通过电子邮件发送给您。

以下为上图描述:

1.  Each block will be processed. Each line in the block will be sent as an input to the process. This process is called as **Mapper**.

    映射器解析该行，获取一个单词并为每个单词设置`<<word>, 1>`。在本例中，一行**苹果橙芒果**的 Mapper 输出将是<苹果、1 >、<橙子、1 >和<芒果、1 >。所有地图工具的关键字为单词，数值为 1。

2.  The next phase is where the output of Mapper, which has the same key will be consolidated. So key with Apple, Orange, Mango, and others will be consolidated, and values will be appended as a list, in this case

    `<Apple, List<1, 1, 1, 1>>, <Grapes, List<1>>, <Mango, List<1, 1>>, and so on`。

    Mappers 生成的关键字将被比较和排序。这一步叫做洗牌和排序。密钥和值列表将被发送到密钥排序序列中的下一步。

3.  下一阶段将获得`<key, List<>>`作为输入，并将只计算列表中 1 的数量，并将计数值设置为输出。这个步骤被称为**减速器**，例如，某些步骤的输出如下:
    *   `<Apple, List<1, 1, 1, 1>> will be <Apple, 4>`
    *   `<Grapes, List<1>> will be < Grapes, 1>`
    *   `<Mango, List<1, 1>> will be <Mango, 2>`
4.  减速器输出将合并到一个文件中，并作为最终输出保存在 HDFS。

## MapReduce 过程

MapReduce 框架有多个步骤和流程或任务。对于程序员来说，MapReduce 抽象了其中的大部分，在许多情况下，只需要关心两个过程；映射和简化为了协调这个过程，必须编写一个驱动类程序。在驱动程序类中，我们可以从输入、映射器类、减少器类、输出和执行映射减少作业所需的其他参数中设置运行映射减少作业的各种参数。

MapReduce 作业复杂，涉及多个步骤；有些步骤是由 Hadoop 以默认行为执行的，如果需要可以被覆盖。以下是在 MapReduce 中按顺序执行的强制步骤:

1.  制图人
2.  无序播放和排序
3.  还原剂

下图解释了上述过程:

![The MapReduce process](img/image00208.jpeg)

图片来源:来自雅虎的 Hadoop 教程！

### 文件夹

在 MapReduce 中，并行性将由 Mapper 实现，其中 Mapper 功能将出现在 TaskTracker 中，TaskTracker 将处理一个 Mapper。映射器代码应该有一个逻辑，它可以独立于其他块数据。映射器逻辑应该利用算法中所有可能的并行步骤。映射器的输入是在映射器进程必须运行的特定输入格式类型和文件的驱动程序中设置的。Mapper 的输出将是一个地图`<key, value>`、`key`和`value`在 Mapper 输出中设置的地图不保存在 HDFS，但是在操作系统空间路径中创建了一个中间文件，并且该文件被读取，并且进行洗牌和排序。

### 洗牌和排序

Shuffle 和 sort 是 MapReduce 中 Mapper 和 Reduce 之间的中间步骤，由 Hadoop 处理，如果需要可以覆盖。无序播放过程通过对映射器输出的键值进行分组来聚合所有映射器输出，该值将被追加到值列表中。因此，Shuffle 输出格式将是一个地图`<key, List<list of values>>`。映射器输出的关键字将被合并和排序。映射器输出将使用排序键序列发送到减速器，默认为`HashPartitioner`，这将以排序序列的减速器数量序列的循环方式发送映射器结果。

### 减速器

在 MapReduce 中，Reducer 是聚合器进程，其中经过洗牌和排序后的数据被发送到 Reducer，在这里我们有`<key, List<list of values >>`，Reducer 将在值列表中进行处理。每把钥匙都可以送到不同的减速器。Reduce 可以设置该值，该值将合并到 MapReduce 作业的最终输出中，该值将作为最终输出保存在 HDFS。

## 投机性执行

正如我们所讨论的，MapReduce 作业被分解为多个 Mapper 和 Reduce 进程，以及一些中间任务，这样一个作业可以产生数百或数千个任务，而一些任务或节点可能需要很长时间才能完成一个任务。Hadoop 监控并检测任务何时运行速度慢于预期，如果该节点有执行任务速度慢的历史，那么它会在另一个节点启动相同的任务作为备份，这被称为任务的推测执行。Hadoop 不会试图修复或诊断节点或进程，因为进程不会给出错误，但它很慢，并且由于硬件降级、软件错误配置、网络拥塞等原因，可能会出现缓慢。对于推测性执行，作业跟踪器在所有任务启动后监控任务，并识别执行缓慢的任务，监控其他正在运行的任务。一旦执行缓慢的任务被标记，作业跟踪器在不同的节点启动任务，并获取先完成的任务的结果，杀死其他任务并记录情况。如果一个节点始终落后，那么作业跟踪器对该节点的偏好就会降低。

可以启用或禁用推测执行，默认情况下，它是打开的，因为它是一个有用的过程。推测性执行必须监视每个任务，在某些情况下会影响性能和资源。在任务(尤其是缩减器)由于特定缩减器上的数据偏斜而可能获得数百万个值的作业中，不建议进行推测执行，因为这将比其他任务花费更长的时间，并且启动另一个任务也无济于事。另一种情况可能是一个 Sqoop 流程，其中一个任务导入数据，如果它花费的时间超过通常的时间，它可以在另一个节点启动相同的任务，并将导入相同的数据，这将导致重复的记录。

## 文件格式

文件格式控制 MapReduce 程序中的输入输出。一些文件格式可以被认为是数据结构。Hadoop 提供了一些实现的文件格式，我们也可以编写自己的自定义文件格式。我们将在下一节中看到它们。

### 输入格式

映射器过程步骤提供了并行性，为了更快地处理，映射器必须进行优化设计。为了独立执行数据，映射器的输入数据被分成称为输入分割的块。InputSplit 可以被认为是输入数据的一部分，在这里数据可以被独立处理。映射器对输入的数据进行处理。MapReduce 作业的输入必须被定义为实现 InputFormat 接口的类，有时需要 RecordReader 在拆分之间读取数据，以从输入数据文件中识别独立的数据块。

Hadoop 已经有了不同类型的 InputFormat，用于解释各种类型的输入数据和从输入文件中读取数据。InputFormat 将输入文件拆分为输入到地图任务的片段。实现的输入格式类的例子如下:

*   TextInputFormat 用于逐行读取文本文件
*   SequenceFileInputFormat 用于读取二进制文件格式
*   DBInputFormat 子类是一个可以用来从 SQL 数据库中读取数据的类
*   CombineFileInputFormat 是 FileInputFormat 类的抽象子类，可用于将多个文件合并为一个拆分

我们可以通过实现输入格式接口或扩展任何实现输入格式的类来创建自己的自定义输入格式类。扩展类是首选，因为编写的许多函数都是可重用的，这有助于代码的可维护性和可重用性。该类必须重写以下两个函数:

*   `public RecordReader createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException`
*   `protected boolean isSplitable(JobContext context, Path file)`

### 录音机

InputFormat 拆分数据，但是拆分并不总是整齐地结束在一行的末尾。记录读取器可以允许数据，即使该行跨越了分割的末尾，否则可能会丢失可能已经跨越了输入分割边界的记录。

下图解释了 RecordReader 的概念，其中块大小为 128 MB，拆分大小为 50 MB:

![RecordReader](img/image00209.jpeg)

我们可以看到不同块之间存在数据分割的重叠。分割 1 和 2 可以从**块 1** 读取，但是对于分割 3，记录读取器必须从 101 兆字节本地读取到 128 兆字节，从 129 兆字节到 150 兆字节必须从**块 2** 远程读取，合并的数据将作为输入发送到映射器。

### 输出格式

OutputFormat 实现类负责写一个 MapReduce 作业的输出和结果，它给出了你想要如何高效地写记录以优化结果的控制，并且可以用来写数据的格式以便与其他系统互操作。默认的输出格式是文本输出格式(我们将它用作我们的字数示例的输出)，它是由行分隔和制表符分隔的键值对。TextOutputFormat 可以在许多用例中使用，但不能以优化或有效的方式使用，因为它会浪费空间，并会使输出大小变大，从而提高资源利用率。因此，我们可以重用 Hadoop 提供的一些输出格式，甚至可以编写自定义的输出格式。

一些广泛使用的输出格式如下:

1.  所有输出格式的文件输出格式(实现接口输出格式)基类
    *   MapFileOutputFormat
    *   SequenceFileOutputFormat

        SequenceFileAsBinaryOutputFormat

    *   文本输出格式
    *   MultipleOutputFormat

        multiplatextoutputformat

        多重序列输出格式

2.  SequenceOutputFormat 可用于对象的二进制表示，它会将其压缩并作为输出写入。OutputFormats 使用 RecordWriter 的实现来实际写入数据。

### 记录作者

RecordWriter 接口提供了更多的控制，可以按照我们想要的方式写入数据。RecordWriter 将输入作为键值对，可以转换要写入的数据的格式。

RecordWriter 是一个抽象类，它有两个要实现的方法，如下所示:

```sh
abstract void write(K key, V value) Writes a key/value pair.
abstract void close(TaskAttemptContext context) Close this RecordWriter to future operations.
```

默认的记录写入程序是行记录写入程序。

## 编写 MapReduce 程序

一个 MapReduce 作业类将有以下三个强制类或任务:

*   **Mapper** :在 Mapper 类中，编写实际的独立步骤，并行化运行在独立的子任务中
*   **Reducer** :在 Reducer 类中，进行映射器输出的聚合，并将输出写入 HDFS
*   **驱动程序**:在驱动程序类中，我们可以从输入、映射器类、缩减器类、输出以及 MapReduce 作业执行所需的其他参数中设置运行 MapReduce 作业的各种参数

我们已经看到了一个简单的字数示例的逻辑来说明 MapReduce 是如何工作的。现在，我们将看到如何在 Java MapReduce 程序中对其进行编码。

### 映射器代码

映射器类必须扩展

```sh
org.apache.hadoop.mapreduce.Mapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT>.
```

以下是映射器类代码的片段:

```sh
// And override 
public void map(Object key, Text value, Context context) throws IOException, InterruptedException
public static class WordCountMapper extends Mapper<Object, Text, Text, IntWritable> {

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
      StringTokenizer token = new StringTokenizer(value.toString());
      while (token.hasMoreTokens()) {
        word.set(token.nextToken());
        context.write(word, one);
      }
    }
  }
```

在映射函数中，输入值(苹果、橘子、芒果)必须被标记化，标记化的单词将被写成映射键，值为 1。请注意，值 1 是可写的。

### 减速器代码

减速器类必须扩展

```sh
org.apache.hadoop.mapreduce.Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT>.
```

以下是单词 WordCountReducer 的代码:

```sh
 // and override reduce function
protected void reduce(KEYIN key, Iterable<VALUEIN> values,
org.apache.hadoop.mapreduce.Reducer.Context context) 
throws IOException, InterruptedException
public static class WordCountReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable count = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      count.set(sum);
      context.write(key, count);
    }
  }
```

Reducer 输入将是`<word, List<1,1,…>>`对于字数来说 Reducer 必须对值列表求和并写入值。减速器输出将作为关键字，值作为计数。

### 驱动程序代码

MapReduce 中的驱动代码大部分是参数刚刚变化的锅炉板代码，可能需要设置一些辅助类，如下图所示:

```sh
public static void main(String[] args) throws Exception {
  Configuration conf = new Configuration();
  Job job = Job.getInstance(conf, "word count");
  job.setJarByClass(WordCount.class);
  job.setMapperClass(WordCountMapper.class);
  job.setReducerClass(WordCountReducer.class);
  job.setOutputKeyClass(Text.class);
  job.setOutputValueClass(IntWritable.class);
  FileInputFormat.addInputPath(job, new Path(args[0]));
  FileOutputFormat.setOutputPath(job, new Path(args[1]));
  System.exit(job.waitForCompletion(true) ? 0 : 1);
}
```

驱动程序代码必须创建`Configuration`对象的实例，该实例用于获取`Job`类的实例。在`Job`类中，我们可以设置以下内容:

*   MapperClass
*   减速器
*   OutputKeyClass
*   OutputValueClass
*   输入格式
*   输出格式
*   JarByClass

字数统计的整个程序如下:

```sh
import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class WordCountMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
      StringTokenizer token = new StringTokenizer(value.toString());
      while (token.hasMoreTokens()) {
        word.set(token.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class WordCountReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable count = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      count.set(sum);
      context.write(key, count);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(WordCountMapper.class);
    job.setReducerClass(WordCountReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
```

编译`WordCount.java`并创建一个罐子。

运行应用:

```sh
$ bin/hadoop jar wc.jar WordCount /input /user/shiva/wordcount/output

```

输出:

```sh
$ bin/hdfs dfs -cat /user/shiva/wordcount/output/part-r-00000

```

## 辅助步骤

除了和映射器、混洗和排序以及缩减器，在 MapReduce 中还有其他可以设置的辅助步骤，或者可以覆盖默认实现来处理 MapReduce 作业。以下是我们将讨论的一些过程:

*   组合器
*   瓜分者ˌ分割者

下图中讨论了上述几点:

![Auxiliary steps](img/image00210.jpeg)

### 合并

组合器是节点本地减速器。组合器用于减少 Mapper 设置的键值数量，我们可以减少为洗牌而发送的数据数量。许多程序将 Reducer 作为组合器类，如果需要，可以有不同于 Reducer 的实现。组合器是使用`job.setCombinerClass(CombinerClassName)`为作业指定的。

组合器应该具有与映射器的输出类型相同的输入/输出键和值类型。组合器只能用于可交换的`(a.b = b.a)`和可关联的`{a. (b.c) = (a.b).c}`函数。

在 WordCount 示例中，我们可以使用一个组合器，它可以与 Reducer 类相同，并将提高作业的性能。

组合器不会总是由作业跟踪器处理。如果映射器中的数据溢出，那么组合器肯定会被调用。

### 分区

分割器负责将特定的键值对发送到特定的减压器。`HashPartitioner`是默认的 Partitioner，它根据 reduce 的数量(如果指定的话)对记录的键进行散列，以循环方式确定记录属于哪个分区，或者分区的数量等于作业的 reduce 任务的数量。有时需要分区来控制映射器中的键值对移动到特定的缩减器。分区对我们要运行的作业的整体性能有直接的影响。

#### 自定义分割器

假设我们想根据标记出现的次数对字数的输出进行排序。假设我们的工作将由两个减速器处理，如下所示:

减速器的设置数量:我们可以通过`job.setNumReduceTasks(#NoOfReducucer)`来指定。

如果我们在不使用任何用户定义的分区器的情况下运行作业，我们将获得如下输出:

<colgroup><col> <col> <col> <col></colgroup> 
| 

数数

 | 

单词

 | 

数数

 | 

单词

 |
| --- | --- | --- | --- |
| one | 这 | Two | a |
| three | 是 | four | 因为 |
| six | 如同 | five | 关于 |
| 减速器 1 | 减速器 2 |

这是部分排序，是`HashPartitioner`的默认行为。如果我们使用正确的分区函数，我们可以将小于或等于 3 的计数发送给一个减速器，而将更高的计数发送给另一个减速器，我们必须将`setNumReduceTasks`设置为`2`。我们将得到发生次数的总顺序。

输出如下所示:

<colgroup><col> <col> <col> <col></colgroup> 
| 

数数

 | 

单词

 | 

数数

 | 

单词

 |
| --- | --- | --- | --- |
| one | 这 | four | 因为 |
| Two | A | five | 如同 |
| three | 是 | six | 关于 |
| 减速器 1 | 减速器 2 |

我们来看看如何才能编写一个自定义的`Partitioner`类，如下图所示:

```sh
public static class MyPartitioner extends   org.apache.hadoop.mapreduce.Partitioner<Text,Text>

{
  @Override
  public int getPartition(Text key, Text value, int numPartitions)
  {
   int count =Integer.parseInt(line[1]);
   if(count<=3)
    return 0;
   else
    return 1;
  }
}

And in Driver class
job.setPartitionerClass(MyPartitioner.class);
```

# Yarn

Yarn 是**又一个资源协商者**，下一代计算和集群管理技术。Yarn 提供了一个在 Hadoop 中构建/运行多个分布式应用的平台。2012 年在 Hadoop 2.0 版本中发布了 Yarn，标志着 Hadoop 架构的重大改变。Yarn 花了大约 5 年时间在一个开放的社区发展。

我们讨论了作业跟踪器是 MapReduce 的单点故障，考虑到 Hadoop 被设计为即使在商品服务器上也能运行，作业跟踪器很有可能会失败。JobTracker 有两个重要的功能:资源管理，以及作业调度和监控。

Yarn 委托和分割责任到单独的守护程序，并实现更好的性能和容错。由于 Yarn，Hadoop 只能作为批处理工作，现在可以被设计成处理交互式和实时处理系统。这是一个巨大的优势，因为许多系统、机器、传感器和其他来源会不断产生大量的数据流，而 Yarn 可以处理这些数据，如下图所示:

![YARN](img/image00211.jpeg)

图片来源:[http://hortonworks.com/wp-content/uploads/2013/05/yarn.png](http://hortonworks.com/wp-content/uploads/2013/05/yarn.png)

## Yarn 架构

与 MapReduce 1 相比，YARN 架构具有极高的可扩展性、容错性和更快的数据处理速度。 *x* 。Yarn 侧重于集群中资源的高可用性和利用率。Yarn 架构有以下三个组成部分:

*   **资源管理器** ( **RM** )
*   **节点管理器** ( **NM** )
*   **应用大师** ( **AM** )

下图说明了 Yarn 的结构:

![YARN architecture](img/image00212.jpeg)

图片来源:[http://Hadoop . Apache . org/docs/current/Hadoop-SHARE/Hadoop-SHARE-site/SHARE . html](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)。

### 资源管理器

在 Yarn 中，ResourceManager 是负责系统中应用间资源管理的主进程管理器。ResourceManager 有一个调度器，它只将资源分配给应用，以及 ResourceManager 从提供内存、磁盘、CPU、网络等信息的容器中获得的资源可用性。

### 节点管理器

在 Yarn 中，节点管理器存在于所有节点中，负责容器、认证、监控资源使用情况，并将信息报告给资源管理器。与任务跟踪器类似，节点管理器向资源管理器发送心跳。

### ApplicationMaster

ApplicationMaster 存在于每个应用中，负责管理运行在 Yarn 内的应用的每个实例。ApplicationMaster 与 ResourceManager 协调资源的协商，并与 NodeManager 协调监控容器的执行和资源消耗，如 CPU、内存的资源分配等。

## 由 Yarn 驱动的应用

在下面是一些应用，它们已经调整了 Yarn 来利用其功能并实现高可用性:

*   **Apache·吉拉夫**:图形处理
*   **Apache 哈马**:高级分析
*   **Apache Hadoop MapReduce** :批处理
*   **Apache Tez** :Hive 顶部的交互/批处理
*   **Apache S4** :流处理
*   **Apache Samza** :流处理
*   **ApacheStorm**:流处理
*   **Apache Spark** :实时迭代处理
*   **霍亚**:Hbase on SHARE

# 总结

在这一章中，我们详细讨论了 HDFS、MapReduce 和 Yarn。

HDFS 具有高度的可扩展性、容错性、可靠性和可移植性，甚至可以在商用硬件上工作。HDFS 体系结构有四个守护进程，分别是名称节点、数据节点、检查点名称节点和备份节点。HDFS 面临许多复杂的设计挑战，这些挑战由不同的技术管理，如复制、心跳、数据块概念、机架感知和数据块扫描程序，HDFS 联邦使 HDFS 具有高可用性和容错性。

Hadoop MapReduce 还具有高度的可扩展性、容错性，甚至可以在商用硬件中工作。MapReduce 架构在节点中有一个主任务跟踪器和多个工作任务跟踪器进程。MapReduce 作业分为多步进程，包括映射器、洗牌器、排序器、缩减器以及辅助合并器和分割器。MapReduce 作业需要大量的数据传输，为此 Hadoop 使用了可写和可写的可比接口。MapReduce 文件格式具有输入格式接口、记录读取器、输出格式和记录写入器，以提高处理速度和效率。

Yarn 是一个分布式资源管理器，用于在 Hadoop 之上管理和运行不同的应用，并为 MapReduce 框架提供了急需的增强，这可以使 Hadoop 更加可用、可扩展和可集成。Yarn 架构有以下组件:资源管理器、节点管理器和应用主控器。许多应用都建立在 are 之上，这使得 Hadoop 更加稳定，并且可以与其他应用集成。

在下一章中，我们将介绍 Hadoop 生态系统中使用的数据访问组件技术，如 Hive 和 Pig，因为它有助于简化编程模型，并使其更快、更易维护。