# 四、基于 Python 和 Hadoop 的科学计算和大数据分析

在本章中，我们将介绍 Python 以及使用 Hadoop 和 Python 包分析大数据。我们将看到一个基本的 Python 安装，打开一个 Jupyter 笔记本，并通过一些例子进行工作。

简而言之，本章将涵盖以下主题:

*   安装:
    *   下载并安装 Python
    *   下载并安装 Anaconda
    *   安装 Jupyter 笔记本
*   数据分析

# 装置

在本节中，我们将了解使用 Python 解释器安装和设置 Jupyter Notebook 以执行数据分析所涉及的步骤。

# 安装标准 Python

用你的网络浏览器去[http://www.python.org/download/](http://www.python.org/download/)的 Python 下载页面。Python 在 Windows、macOS 和 Linux 上受支持，您会发现不同的安装:

*   Python 在[https://www.python.org/downloads/windows/](https://www.python.org/downloads/windows/)为 Windows 发布
*   Python 在[https://www.python.org/downloads/mac-osx/](https://www.python.org/downloads/mac-osx/)发布了 macOS X
*   Python 源代码版本(Linux 和 Unix)在[https://www.python.org/downloads/source/](https://www.python.org/downloads/source/)

单击下载页面时，您将看到以下屏幕:

![](img/30ce0526-b290-453c-8a56-a0d3b7037b35.png)

如果您点击一个特定的版本，如 3.6.5，那么您将进入一个不同的页面，如下图所示:

![](img/e6eec0e0-088f-46d8-ab57-e52d5f43632a.png)

您可以阅读发行说明，然后通过向下滚动页面继续下载 Python 版本，如下图所示:

![](img/93f7897c-ff7e-4465-8261-79f2b752fa5f.png)

单击适合您的操作系统的正确版本并下载安装程序。下载完成后，在您的计算机上安装 Python。

# 安装蟒蛇

标准的 Python 安装有局限性，所以你必须安装 Jupyter、其他包、`pip`等等，才能让安装生产为你做好准备。Anaconda 是一个强调科学的一体化安装程序:它包括 Python、标准库和许多有用的第三方库。

使用浏览器，输入网址[https://www.anaconda.com/download/](https://www.anaconda.com/download/)–这将带您进入 Anaconda 下载页面，如下图所示:

![](img/02f71c2d-a0c7-4d4f-bd97-c8943c791841.png)

下载适合您平台的 Anaconda 版本，然后按照网页[https://docs.anaconda.com/anaconda/install/](https://docs.anaconda.com/anaconda/install/)上的说明进行安装。

安装完成后，您应该可以打开 Anaconda Navigator(在 Windows 上，这是在“开始”菜单中，在 Mac 上，您可以简单地搜索)。

On Linux, typically you have to use the command line to launch Jupyter Notebook.

例如，在苹果电脑上，出现了如下截图所示的 Anaconda 导航器:

![](img/9069b0f4-7068-4050-8ec0-a4e8cc2b4bfe.png)

如果您使用的是 Anaconda Navigator，只需点击 Jupyter 笔记本启动按钮即可启动 Jupyter，如下图截图所示:

![](img/4a43f53e-e5f9-4f83-95b7-7095607597f7.png)

# 使用 Conda

到目前为止，Conda 命令行是成功设置 Python 安装最有用、最易于使用的工具。Conda 支持多个可以共存的环境，因此您可以设置 Python 2.7 环境和 Python 3.6 环境。如果你对深度学习感兴趣，你可以将 TensorFlow 设置为一个独立的环境，等等。

You can download and install `conda` by browsing to the URL [https://conda.io/docs/user-guide/install/index.html](https://conda.io/docs/user-guide/install/index.html).

下面的截图是 Conda 安装页面:

![](img/bdb2a1b7-b41f-4be1-a61b-f617ffe73dbd.png)

从链接下载 Conda 后，按照说明在您的机器上安装 Conda，如下图所示:

![](img/b8652aad-c472-4bb9-9be8-d3902a4b8365.png)

在命令行上输入`conda list`会显示所有安装的软件包。这将帮助您了解安装了哪些版本的软件包:

![](img/9507c094-073e-4387-b0a6-e31c6508b81e.png)

使用`conda`安装包装很容易。就像`conda install <package name>`一样简单。

例如，键入:

```scala
conda install scikit-learn
```

![](img/4f97f5be-fc0a-4312-bcdb-c1f3acc3e7cc.png)

更重要的是，`conda install Jupyter`安装 Jupyter 笔记本，需要很多其他的包:

![](img/b6fa2c9b-dee9-4aaa-9a13-fc0625f1d78e.png)

让我们尝试另一个重要的包:

```scala
conda install pandas
```

![](img/5bd0c9ca-c68a-4289-bb93-c212f9ac3189.png)

其他重要的包有:

```scala
conda install scikit-learn
conda install matplotlib
conda install seaborn
```

除了`conda`安装，我们还需要安装软件包来访问 HDFS (Hadoop)和打开文件(拼花格式):

```scala
pip install hdfs
pip install pyarrow
```

Jupyter 笔记本配置可以通过运行如下命令来生成:

```scala
[root@4b726275a804 /]# jupyter notebook --generate-config
 Writing default config to: /root/.jupyter/jupyter_notebook_config.py
```

Jupyter 需要身份验证，默认情况下这是一个令牌。但是，如果您想要创建基于密码的身份验证，那么只需运行下面代码中显示的命令来设置密码:

```scala
[root@4b726275a804 /]# jupyter notebook password
 Enter password:
 Verify password:
 [NotebookPasswordApp] Wrote hashed password to /root/.jupyter/jupyter_notebook_config.json
```

现在，我们已经准备好启动笔记本，因此键入以下命令:

```scala
jupyter notebook --allow-root --no-browser --ip=* --port=8888
```

以下是运行上述命令时的控制台:

![](img/bd649387-abaf-469f-a94a-ff60c55b114a.png)

当您打开浏览器并输入`localhost:8888`时，浏览器将打开登录屏幕，然后您必须输入前面步骤中设置的密码:

![](img/3fca26d1-6983-4890-b252-a61b233c6923.png)

一旦提供了密码，Jupyter 笔记本门户就会打开，显示任何现有的笔记本。在这种情况下，我们没有以前的笔记本，所以下一步是创建一个。单击新建，然后为您的新笔记本选择 Python 2:

![](img/806450ca-7e2e-48be-9854-61b9d70f979c.png)

下面是一个新的笔记本，您现在可以在其中键入一些测试代码，如下图所示:

![](img/b7808e7c-c867-4e85-b2d3-686df7ea10dc.png)

现在我们已经安装了 Python 和 Jupyter Notebook，我们准备使用 Notebooks 和 Python 语言进行数据分析。在下一节中，我们将深入研究可以进行的不同类型的数据分析。

# 数据分析

从随书提供的链接下载`OnlineRetail.csv`。然后，您可以使用熊猫加载文件。

以下是使用 Pandas 读取本地文件的简单方法:

```scala
import pandas as pd
path = '/Users/sridharalla/Documents/OnlineRetail.csv'
df = pd.read_csv(path)
```

然而，由于我们是在 Hadoop 集群中分析数据，我们应该使用`hdfs`而不是本地系统。以下是如何将`hdfs`文件加载到`pandas`数据帧的示例:

```scala
import pandas as pd
from hdfs import InsecureClient
client_hdfs = InsecureClient('http://localhost:9870')
with client_hdfs.read('/user/normal/OnlineRetail.csv', encoding = 'utf-8') as reader:
 df = pd.read_csv(reader,index_col=0)
```

下面是下面一行代码的作用:

```scala
df.head(3)
```

您将获得以下结果:

![](img/31b579e6-d88e-4726-a156-ed3853a712ac.png)

基本上，它显示了数据框中的前三个条目。

我们现在可以用数据做实验。输入以下内容:

```scala
len(df)
```

这将输出以下内容:

```scala
65499
```

这仅仅意味着数据帧的长度或大小。它告诉我们整个文件中有`65,499`个条目。

现在这样做:

```scala
df2 = df.loc[df.UnitPrice > 3.0]
df2.head(3)
```

我们定义了一个名为`df2`的新数据框，并将其设置为单价大于 3 的原始数据框中的所有条目。

然后，我们告诉它显示前三个条目，如下图所示:

![](img/8ca66c67-43cf-4af4-9a26-79c1d07a2f15.png)

以下代码行选择单价高于`3.0`的指数，并将其描述设置为`Miscellaneous`。然后显示前三项:

```scala
df.loc[df.UnitPrice > 3.0, ['Description']] = 'Miscellaneous'
df.head(3)
```

这就是结果:

![](img/0f9a596c-bab0-4a94-8f52-a6815b8bb659.png)

如您所见，条目 2(索引为 1)的描述被更改为`Miscellaneous`，因为它的单价是 3.39 美元(正如我们之前指定的，这已经超过了 3 美元)。

代码行输出索引为 2 的数据:

```scala
df.loc[2]
```

输出如下:

![](img/f46f43ed-70af-4a3b-a9d4-6e4fe1dfaf53.png)

最后，我们可以创建一个**数量**列的图，如下代码所示:

```scala
df['Quantity'].plot()
```

![](img/b22d7f83-65a7-41c7-a55a-76488459f8c3.png)

还有很多功能需要探索。

这里有一个使用`.append()`函数的例子。

我们定义了一个新的`df`对象`df3`，并将其设置为等于`df`的前 10 行加上`df`的第 200–209 行。换句话说，我们将第 200-209 行追加到`df`的第 0-9 行:

```scala
df3 = df[0:10].append(df[200:210])
df3
```

这是结果输出:

![](img/feb26dd8-0b08-4738-a3d6-789010411d68.png)

现在，假设您只关心几列，即**库存代码**、**数量**、**发票日期**和**单价**。我们可以定义一个新的`DataFrame`对象，只包含数据中的那些列:

```scala
df4 = pd.DataFrame(df, columns=['StockCode', 'Quantity', 'InvoiceDate', 'UnitPrice']
df4.head(3)
```

这是以下结果:

![](img/b601b350-9f68-440d-a983-ffcf21991c56.png)

熊猫提供了不同的方式来组合数据。更具体地说，我们可以**合并**、**连接**、**加入**，以及**追加**。我们已经介绍了 append，所以现在我们来看看连接数据。

看看这个代码块:

```scala
d1 = df[0:10]
d2 = df[10:20]

d3 = pd.concat([d1, d2])
d3
```

基本上，我们将`d1`设置为一个包含`df`前 10 个指数的`DataFrame`对象。然后，我们将`d2`设置为`df`的下十个指数。最后，我们将`d3`设置为`d1`和`d2`的串联。这是它们连接后的结果:

![](img/58c5d329-23cd-4d9b-8699-7c48be6df688.png)

我们可以做得更多。我们可以指定按键，这样可以更容易区分`d1`和`d2`。看看下面的代码行:

```scala
d3 = pd.concat([d1, d2], keys=['d1', 'd2'])
```

![](img/227a1e13-6b43-4fca-93b2-41a25d3064db.png)

如您所见，区分这两个数据集要容易得多。我们可以随意调用这些键，甚至像 *x* 和 *y* 这样的简单键也可以。如果我们有三个数据集`d1`、`d2`和一些`d3`，我们可以说键是( *x* 、 *y* 、 *z* )，这样我们就可以区分所有三个数据集。

现在，我们继续讨论不同列的连接。默认情况下，`concat()`功能使用**外部**连接。这意味着它组合了所有的列。想想 A 和 B 两组，其中 A 组包含所有属于`d1`的列名，B 组包含所有属于`d2`的列名。如果我们使用前面使用的代码行连接`d1`和`d2`，我们将看到的列由 A 和 b 的并集表示

我们也可以指定要使用**内部**连接，用 A 和 b 的交集表示，看看下面几行代码:

```scala
d4 = pd.DataFrame(df, columns=['InvoiceNo', 'StockCode', 'Description'])[0:10]
d5 = pd.DataFrame(df, columns=['StockCode', 'Description', 'Quantity'])[0:10]

pd.concat([d4, d5])
```

![](img/1855b249-fe0b-453b-9051-93f227404d7f.png)

如您所见，它使用了所有的列标签。

请记住，默认情况下，`concat()`使用外部连接。所以，说`pd.concat([d4, d5])`和说:

```scala
pd.concat([d4, d5], join='outer')
```

现在，我们使用内部连接。保持其他一切不变，但改变对`concat()`函数的调用。请看下面一行代码:

```scala
pd.concat([d4, d5], join='inner')
```

现在应该输出:

![](img/1b50f632-7f40-436f-a297-392e8dd4ee76.png)

可以看到，这次我们只有`d4`和`d5`共有的列标签。同样，我们可以添加键，以便更容易区分表中的两个数据集。

**合并**稍微复杂一些。这一次，您可以在外部联接、内部联接、左侧联接和右侧联接之间进行选择，还可以选择要合并的列。

让我们继续修改我们最初对`d4`和`d5`的定义:

```scala
d4 = pd.DataFrame(df, columns=['InvoiceNo', 'StockCode', 'Description'])[0:11]
d5 = pd.DataFrame(df, columns=['StockCode', 'Description', 'Quantity'])[10:20]
```

你在`d4`定义的末尾看到的括号意味着我们将按照定义获取该特定`DataFrame`的前 11 个元素。`d5 `定义末尾的括号表示我们将元素 10 到 20 放入`d5`，而不是整个元素。

值得注意的是，他们将有一个重叠的元素，这将很快发挥作用。

首先从`merge`功能开始。让我们对`d4`和`d5`进行左连接合并:

```scala
pd.merge(d4, d5, how='left')
```

![](img/4b585925-1254-4042-b2ab-dcf4edc15d77.png)

这样做是使用了对`d4`和`d5`中左侧数据框的所有列，并在此基础上添加了`d5`的列。如您所见，由于我们定义`d5`包含元素 10 到 20，因此没有从索引 0 到 10 的数量值。然而，由于元素 11 同时在`d5`和`d4`中，我们在**数量**下看到了该元素的数据值。

同样，我们可以对右连接做同样的事情:

```scala
pd.merge(d4, d5, how='right')
```

![](img/6b6977ed-f0a4-4468-ab07-70694743aa68.png)

现在，它使用`d5`的列标签以及`d5`的数据(从元素 10 到 20)。如您所见，索引 0 处的数据是与`d4`共享的，因此它在这个特定的表中完成。这是因为元素编号 11(索引 10)与`d5`(索引 10)的第一个元素重叠。

现在我们做内部连接:

```scala
pd.merge(d4, d5, how='inner')
```

![](img/4f1a390b-effd-4be0-a3e0-bbf948c13135.png)

内部连接意味着它只包含两个数据框共有的元素。在这种情况下，显示的元素是元素编号 11，索引 10 在`df`中。因为它存在于`d4`和`d5`中，所以它既有**发票号**的数据，也有**数量**的数据(因为**发票号**的数据在`d4`中，而**数量**的数据在`d5`中)。

现在，我们将进行外部连接:

```scala
pd.merge(d4, d5, how='outer')
```

![](img/f54b5de6-783e-465f-85c0-af88c19afe2f.png)

如您所见，外部连接意味着它包括所有列(列在`d4`和`d5`中的并集)。

任何不存在的数据值都被标记为 NaN。例如`d5`中没有标注 **InvoiceNo** 的列，所以那里所有的数据值都显示为 NaN。

现在，让我们谈谈加入一个专栏。我们可以在函数调用中引入一个新参数`on=`。以下是**股票代码**栏的合并示例:

```scala
pd.merge(d4, d5, on='StockCode', how='left')
```

![](img/11311ac5-8d7e-47c5-b334-ebef330306e1.png)

该图类似于我们使用左连接合并`d4`和`d5`时生成的表格。但是，例外的是由于**说明**是`d4`和`d5`共有的一列，所以增加了两者，但分别用 **_x** 和 **_y** 来区分。

正如你在最后一个条目中看到的，它被`d4`和`d5`共享，所以 **Description_x** 和 **Description_y** 是相同的。

请记住，我们只能输入两个数据框共有的列名。所以，我们可以做**股票代码**或者**描述**来合并。

如果我们合并到**描述**上，看起来就是这样:

```scala
pd.merge(d4, d5, on='Description', how='left')
```

![](img/7ccac09e-7b20-4cb3-b606-86b759ac5ecf.png)

再次，通过添加 **_x** 和 **_y** 分别表示`d4`和`d5`来区分它们共享的列。

我们实际上可以传入一个列名列表，而不是一个列名。所以，现在我们有:

```scala
pd.merge(d4, d5, on=['StockCode', 'Description'], how='left')
```

![](img/24949e0d-12ae-41ce-846c-ccc75ffee33d.png)

然而，在这种情况下，我们可以看到，这是同一个表:

```scala
pd.merge(d4, d5, how='left')
```

这是因为在这种特殊情况下，我们传入的列表包含了他们共享的所有列名。如果他们共享三列，而我们只传入两列，情况就不是这样了。

为了说明这一点，假设这样:

```scala
d4 = pd.DataFrame(df, columns=['InvoiceNo', 'StockCode', 'Description', 'UnitPrice'])[0:11]
d5 = pd.DataFrame(df, columns=['StockCode', 'Description', 'Quantity', 'UnitPrice'])[10:20]
```

现在，让我们再试一次:

```scala
pd.merge(d4, d5, on=['StockCode', 'Description'], how='left')
```

所以，现在我们的桌子看起来像:

![](img/e14a3df9-447d-4d82-b464-21b6631fcbec.png)

我们还可以指定希望所有的列都存在，即使是共享的列。

考虑一下:

```scala
pd.merge(d4, d5, left_index = True, right_index=True, how='outer')
```

您可以指定所需的任何连接类型，它仍会显示所有列。但是，在本例中，它将使用外部联接:

![](img/2ad2f15b-6eb3-4e6b-b349-bc6164d4ae52.png)

现在，我们可以进入`join()`功能。需要注意的一点是，如果两个数据框共享一个列名，它将不允许我们连接它们。所以，以下是不允许的:

```scala
d4 = pd.DataFrame(df, columns=['StockCode', 'Description', 'UnitPrice'])[0:11]
d5 = pd.DataFrame(df, columns=[ 'Description', 'Quantity', 'InvoiceNo'])[10:20]
d4.join(d5)
```

否则，会导致错误。

现在，看看下面几行代码:

```scala
d4 = pd.DataFrame(df, columns=['StockCode', 'UnitPrice'])[0:11]
d5 = pd.DataFrame(df, columns=[ 'Description', 'Quantity'])[10:20]
d4.join(d5)
```

这将产生这个表:

![](img/643568a3-eb68-4a98-8224-7d5c3e828913.png)

所以取`d4 `表，从`d5`添加列和对应的数据。由于`d5`没有从指数 0 到 9 的描述或数量数据，它们都显示为 NaN。由于`d5`和`d4`都共享索引 10 的数据，因此该元素的所有数据都显示在相应的列中。

我们也可以反过来加入他们:

```scala
d4 = pd.DataFrame(df, columns=['StockCode', 'UnitPrice'])[0:11]
d5 = pd.DataFrame(df, columns=[ 'Description', 'Quantity'])[10:20]
d5.join(d4)
```

![](img/e1355760-d1f0-48be-b355-0a66028562cb.png)

这是同样的逻辑，除了`d4`的列和相应的数据被添加到`d5`的表中。

接下来，我们可以使用`combine_first()`来组合数据。

请看下面的代码:

```scala
d6 = pd.DataFrame.copy(df)[0:5]
d7 = pd.DataFrame.copy(df)[2:8]

d6.loc[3, ['Quantity']] = 110
d6.loc[4, ['Quantity']] = 110

d7.loc[3, ['Quantity']] = 210
d7.loc[4, ['Quantity']] = 210
pd.concat([d6, d7], keys=['d6', 'd7'])
```

在`pd.DataFrame`之后添加的`.copy`确保我们复制了原始的`df`，而不是编辑原始的`df`本身。这样，`d6`将指数`3`和`4`的数量改为`110`应该不会影响`d7`，反之亦然。请记住，如果您传入要选择的列列表，这将不起作用，因此您不能有类似以下的内容:

```scala
pd.DataFrame(df, columns=['Quantity', 'UnitPrice'])
```

运行前面的代码后，这就是结果表:

![](img/ba319e80-727f-4747-a333-5a362e8e80ee.png)

注意`d6`和`d7`都有共同的元素，即索引为 2 到 4 的元素。

现在，看看这段代码:

```scala
d6.combine_first(d7)
```

![](img/ff8ebbb2-ec30-4810-8f27-dd567031f616.png)

这样做是把`d7`的数据和`d6`的数据结合起来，但是优先选择`d6`。请记住，我们在`d6`中将指数 3 和 4 的数量设置为`110`。如您所见，`d6`的数据保存在两个数据集有共同索引的地方。现在看看这一行代码:

```scala
d7.combine_first(d6)
```

![](img/09a12ab6-5115-4abb-b5e9-ac4c10c99dbb.png)

现在你会看到，当两个元素有共同的索引时(在索引 3 和 4 处)，保留`d7 `的数据。

您也可以使用`value_counts()`获得选择类别中每个值的出现次数。看看这段代码:

```scala
pd.value_counts(df['Country'])
```

![](img/cb0d1388-8804-4deb-9155-d4b673e4e1ee.png)

在合并过程中需要考虑的一件事是，您可能会遇到重复的数据值。要解决这些问题，请使用`.drop_duplicates()`。

考虑一下:

```scala
d1 = pd.DataFrame(df, columns = ['InvoiceNo', 'StockCode', 'Description'])[0:100]
d2 = pd.DataFrame(df, columns = ['Description', 'InvoiceDate', 'Quantity'])[0:100]

pd.merge(d1, d2)
```

![](img/355647e0-7991-4d72-9d98-4652b0fc8da4.png)

如果我们一直滚动到底部:

![](img/12615988-fbbf-4905-aec4-b830e7f72b2d.png)

如您所见，有许多重复的数据条目。要全部移除，我们可以使用`drop_duplicates()`。此外，我们可以指定可以使用哪些列数据来确定哪些条目是要删除的重复条目。例如，我们可以使用`StockCode`删除所有重复的条目，假设每个项目都有一个唯一的股票代码。我们还可以假设每个项目都有一个唯一的描述，并以这种方式删除项目。现在看看这段代码:

```scala
d1 = pd.DataFrame(df, columns = ['InvoiceNo', 'StockCode', 'Description'])[0:100]
d2 = pd.DataFrame(df, columns = ['Description', 'InvoiceDate', 'Quantity'])[0:100]

pd.merge(d1, d2).drop_duplicates(['StockCode'])
```

![](img/d55b41b2-163a-4fa4-9327-0d9267d81845.png)

如果我们滚动到底部:

![](img/c4196f9e-025c-46d5-9eeb-85069ff900ee.png)

您将看到许多重复条目被删除。我们也可以通过`Description`、`StockCode`或`Description`，它会产生同样的结果。

你会注意到指数到处都是。我们可以用`reset_index()`来修复。请看下面的代码:

```scala
d1 = pd.DataFrame(df, columns = ['InvoiceNo', 'StockCode', 'Description'])[0:100]
d2 = pd.DataFrame(df, columns = ['Description', 'InvoiceDate', 'Quantity'])[0:100]

d3 = pd.merge(d1, d2).drop_duplicates(['StockCode'])
d3.reset_index()
```

这就是它的样子:

![](img/d13e6b18-6cbd-4c16-91e0-d3b91dfd3137.png)

显然，这不是你想要的。是的，它重置了索引，但是它添加了旧索引作为列。有一个简单的方法，那就是引入一个新的参数。现在，看看这段代码:

```scala
d3.reset_index(drop=True)
```

![](img/669aae63-614d-4129-9dd9-24864b327480.png)

好多了。默认情况下，`drop=False`，所以如果不希望旧索引作为新列添加到数据中，那么记得设置`drop=True`。

你可能还记得之前的`.plot()`功能。您可以使用它来帮助可视化数据帧，尤其是在数据帧很大的情况下。

这里有一个涉及单个列的例子:

```scala
d8 = pd.DataFrame(df, columns=['Quantity'])[0:100]
d8.plot()
```

这里，只选择前 100 个元素，以使图形不那么拥挤，并更好地说明示例。

现在，你将拥有:

![](img/ce49e5ea-0bdc-4009-a0f6-fbfd0f6f08fc.png)

现在，假设您希望显示多个列。请看以下内容:

```scala
d8 = pd.DataFrame(df, columns=['Quantity', 'UnitPrice'])[0:100]
d8.plot()
```

![](img/6f978d13-5b63-413d-876a-91b9bb1befcc.png)

请记住，它不会绘制**描述**等定性数据列，只会绘制**数量**和**单价**等可以绘制的东西。

# 摘要

在本章中，我们已经讨论了 Python 以及如何使用 Python 使用 Jupyter Notebook 执行数据分析。我们还研究了使用 Python 可以完成的几种不同的操作。

在下一章中，我们将研究另一种流行的分析语言 R，以及如何使用 R 来执行数据分析。