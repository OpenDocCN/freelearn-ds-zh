# 五、基于 R 和 Hadoop 的统计大数据计算

本章介绍了 R 以及如何使用 R 使用 Hadoop 对大数据进行统计计算。我们将看到从工作站上的开源 R 到像 Revolution R Enterprise 这样的并行化商业产品的各种选择，以及介于两者之间的许多其他选择。在这两个极端之间是一系列具有独特能力的选项:扩展数据、性能、功能和易用性。因此，正确的选择取决于您的数据大小、预算、技能、耐心和治理限制。

在本章中，我们将总结使用纯开源 r 的替代方案及其一些优势。此外，我们将描述通过结合开源和商业技术来实现更大规模、速度、稳定性和易开发性的选项。

简而言之，本章将涵盖以下主题:

*   将 R 与 Hadoop 集成介绍
*   R 与 Hadoop 的集成方法
*   用 R 进行数据分析

# 介绍

这一章是为了帮助目前对 Hadoop 不熟悉的 R 用户理解和选择要评估的解决方案而写的。和大多数开源的东西一样，首先考虑的当然是货币。不是一直都是吗？好消息是有多种免费的替代方案，并且在各种开源项目中正在开发额外的功能。

我们通常会看到使用完全开源的堆栈构建 R 和 Hadoop 集成的四个选项:

*   在工作站上安装 R 并连接到 Hadoop 中的数据
*   在共享服务器上安装 R 并连接到 Hadoop
*   利用旋转打开
*   使用 RMR2 在 MapReduce 内部执行 R

让我们在以下几节中详细介绍每个选项。

# 在工作站上安装 R 并连接到 Hadoop 中的数据

这种基线方法的最大优势是简单和成本。免费的。端到端免费。生活中还有什么？通过以开源形式提供的包 Revolution，包括`rhdfs`和`rhbase`，R 用户可以直接从 Hadoop 中的`hdfs`文件系统和`hbase`数据库子系统中获取数据。这两个连接器都是 Revolution 创建和维护的 RHadoop 包的一部分，是首选。

还有其他选择。RHive 包直接从 R 执行 Hive 的 HQL(类似 SQL 的查询语言)，并提供从 Hive 检索元数据的功能，如数据库名、表名、列名等。尤其是`rhive`包的优势在于，它的数据操作需要将一些工作下推到 Hadoop 中，避免了数据移动和大速度提升的并行操作。类似的下推也可以通过`rhbase`实现。然而，两者都不是特别丰富的环境，复杂的分析问题总是会暴露出一些能力差距。

除了有限的下推能力，R 最擅长处理从`hdfs`、`hbase`或`hive`采集的适度数据；这样，当前的 R 用户就可以快速上手 Hadoop。

# 在共享服务器上安装 R 并连接到 Hadoop

一旦你厌倦了笔记本电脑上的内存障碍，显而易见的下一条路就是共享服务器。有了今天的技术，你只需花几千美元就可以装备一台强大的服务器，并在几个用户之间轻松共享。当使用具有 256 GB 或 512 GB 内存的窗口或 Linux 时，R 可以用来分析高达数百千兆字节的文件，尽管没有您希望的那么快。

与选项一一样，共享服务器上的 R 也可以利用`rhbase`和`rhive`包的下推功能来实现并行性并避免数据移动。然而，与工作站一样，`rhive`和`rhbase`的下推功能有限。

当然，虽然大量内存可以防止可怕的内存耗尽，但它对计算性能影响不大，并且依赖于分享在幼儿园学到的(或者可能没有学到的)技能。出于这些原因，可以认为共享服务器是工作站上 R 的一个很好的补充，但不是完全的替代品。

# 利用旋转打开

用 R 发行版**Revolution R Open**(**RRO**)取代 R 的 CRAN 下载，性能进一步提升。RRO 和 R 本身一样，是开源的，100% R，免费下载。它使用英特尔数学内核库加速数学计算，并且 100%兼容 CRAN 和其他存储库中的算法，如 BioConductor。不需要对 R 脚本进行任何更改，对于大量使用某些数学和线性代数原语的脚本，MKL 库提供的加速从可以忽略到一个数量级不等。如果你用语言做数学运算，你可以预期 RRO 可以让你的平均成绩翻倍。与选项一和选项二一样，RRO 可以与`rhdfs`等连接器一起使用，它可以通过`rhbase`和`rhive`将工作连接并下推到 Hadoop 中。

# 使用 RMR2 在 MapReduce 内部执行 R

一旦你发现你的问题集太大，或者你的耐心在工作站或服务器上被耗尽，并且`rhbase`和`rhive`下推的限制阻碍了进展，你就准备好在 Hadoop 内部运行 R 了。

开源 RHadoop 项目包括`rhdfs`、`rhbase`、`plyrmr`，还有一个名为`rmr2`的包，可以让 R 用户使用 R 函数构建 Hadoop MapReduce 操作。使用映射器，R 函数被应用于组成一个`hdfs`文件、`hbase`表或其他数据集的所有数据块；结果可以发送到一个减速器，也是一个 R 函数，用于聚合或分析。所有的工作都是在 Hadoop 内部进行的，但内置于 R 中。让我们明确一点:将 R 函数应用于每个`hdfs`文件段是加速计算的好方法。但在很大程度上，避免移动数据才是真正强调性能的原因。为此，`rmr2`对 Hadoop 节点上的数据应用 R 函数，而不是将数据移动到 R 所在的位置。

虽然`rmr2`给出了本质上无限的能力，但作为数据科学家或统计学家，你的想法很快就会转向在大数据集上用 R 计算整个算法。以这种方式使用`rmr2`使 R 程序员的开发变得复杂，因为他或她必须编写所需算法的整个逻辑或修改现有的 CRAN 算法。然后，他/她必须验证算法是否准确，是否反映了预期的数学结果，并为诸如数据缺失等各种情况编写代码。

`rmr2`需要你自己编码来管理并行化。对于数据转换操作、聚合等来说，这可能是微不足道的，如果您试图在大型数据集上训练预测模型或构建分类器，这可能是相当繁琐的。虽然`rmr2`可能比其他方法更繁琐，但这并不是站不住脚的，大多数 R 程序员会发现`rmr2`比求助于基于 Java 的 Hadoop mappers 和 reducers 开发要容易得多。虽然有些乏味，但它:

*   是完全开源的
*   有助于并行化计算以处理更大的数据集
*   跳过痛苦的数据移动
*   被广泛使用，所以你会发现有帮助
*   是免费的

`rmr2`不是该类别中的唯一选项；一个类似的名为`rhipe`的包也在那里，并提供类似的功能。`rhipe`在[https://www.rhipe.com/download-confirmation/](https://www.rhipe.com/download-confirmation/)有描述，可从 GitHub 下载。

# 纯开源选项的总结和展望

在 Hadoop 中使用 R 的基于开源的选项的范围正在扩大。例如，Apache Spark 社区正在通过可预见的名称 SparkR 快速改进 R 集成。如今，SparkR 提供了从 R 到 Spark 的访问，就像今天`rmr2`和`rhipe`为 Hadoop MapReduce 所做的那样。

我们预计，在未来，SparkR 团队将增加对 Spark 的 MLlib 机器学习算法库的支持，直接从 r 提供执行。可用性日期尚未广泛发布。

也许最令人兴奋的观察是，R 已经成为平台厂商的**桌赌注**。我们在 Cloudera、Hortonworks、MapR 和其他公司的合作伙伴，以及数据库供应商和其他公司，都敏锐地意识到了 R 在庞大且不断增长的数据科学社区中的主导地位，以及 R 作为从构建在 Hadoop 之上的新兴数据存储库中获取见解和价值的一种手段的重要性。

在随后的文章中，我将回顾通过将范围从仅开源解决方案扩展到像 Hadoop 的 Revolution R Enterprise 这样的解决方案，为 R 用户创造更高性能、简单性、可移植性和可扩展性的选项。

r 是一个惊人的数据科学编程工具，可以对模型进行统计数据分析，并将分析结果转换成彩色图形。毫无疑问，R 是统计学家、数据科学家、数据分析师和数据架构师最喜欢的编程工具，但在处理大型数据集时，它却有所欠缺。R 编程语言的一个主要缺点是所有对象都被加载到单机的主内存中。以千兆字节为单位的大型数据集无法加载到内存中；这也是 Hadoop 与 R 集成是理想解决方案的时候。为了适应 R 编程语言的内存、单机限制，数据科学家不得不将他们的数据分析限制在大数据集的数据样本上。当处理大数据时，R 编程语言的这种局限性是一个主要障碍。由于 R 的可伸缩性不是很高，核心 R 引擎只能处理有限的数据。

相反，像 Hadoop 这样的分布式处理框架对于大型数据集(petabyte 范围)上的复杂操作和任务是可扩展的，但不具备强大的统计分析能力。由于 Hadoop 是一个流行的大数据处理框架，将 R 与 Hadoop 集成是下一个合乎逻辑的步骤。在 Hadoop 上使用 R 将提供一个高度可扩展的数据分析平台，该平台可以根据数据集的大小进行扩展。将 Hadoop 与 R 集成可以让数据科学家在大型数据集上并行运行 R，因为 R 语言中的数据科学库都不能在大于内存的数据集上工作。借助 R 和 Hadoop 的大数据分析与商品硬件集群在垂直扩展方面提供的成本价值回报相竞争。

# R 与 Hadoop 的集成方法

使用 Hadoop 的数据分析师或数据科学家可能有他们用于数据处理的 R 包或 R 脚本。为了在 Hadoop 中使用这些 R 脚本或 R 包，他们需要用 Java 编程语言或任何其他实现 Hadoop MapReduce 的语言重写这些 R 脚本。这是一个繁重的过程，可能会导致不必要的错误。为了将 Hadoop 与 R 编程语言集成，我们需要使用一个已经为 R 编写的软件，数据存储在 Hadoop 的分布式存储中。有许多使用 R 语言来执行大型计算的解决方案，但所有这些解决方案都要求在将数据分发到计算节点之前将其加载到内存中。这不是大型数据集的理想解决方案。以下是一些常用的将 Hadoop 与 R 集成的方法，以最大限度地利用 R 对大型数据集的分析能力。

# RHADOOP–在工作站上安装 R 并连接到 HADOOP 中的数据

最常用的将 R 编程语言与 Hadoop 集成的开源分析解决方案是 **RHadoop** 。由 Revolution analytics 开发的 RHadoop 允许用户直接从 HBase 数据库子系统和 HDFS 文件系统中摄取数据。RHadoop 包是在 Hadoop 上使用 R 的最佳解决方案，因为它简单且具有成本优势。RHadoop 是五个不同包的集合，允许 Hadoop 用户使用 R 编程语言管理和分析数据。RHadoop 包与开源 Hadoop 兼容，也与流行的 Hadoop 发行版 Cloudera、Hortonworks 和 MapR 兼容:

*   `rhbase`:`rhbase`包使用一个节俭服务器为 R 内的 HBase 提供数据库管理功能。此包需要安装在将运行 R 客户端的节点上。使用`rhbase`，数据工程师和数据科学家可以从 r
*   `rhdfs`:`rhdfs`包为 R 程序员提供了与 Hadoop 分布式文件系统的连接，以便他们读取、写入或修改存储在 Hadoop HDFS 中的数据。
*   `plyrmr`:这个包支持对 Hadoop 管理的大数据集进行数据操作。`plyrmr` ( `plyr`用于 MapReduce)提供流行包中存在的数据操作操作，如`reshape2`和`plyr`。这个包依赖 Hadoop MapReduce 来执行操作，但抽象了大部分 MapReduce 细节。
*   `ravro`:这个包允许用户从本地和 HDFS 文件系统读写 Avro 文件。
*   `rmr2`(在 Hadoop MapReduce 内部执行 R):使用这个包，R 程序员可以对一个 Hadoop 集群中存储的数据进行统计分析。使用`rmr2`将 R 与 Hadoop 集成可能是一个麻烦的过程，但是许多 R 程序员发现使用`rmr2`比依赖基于 Java 的 Hadoop 映射器和减压器要容易得多。`rmr2`可能有点乏味，但它消除了数据移动，并有助于处理大型数据集的并行计算。

# RHIPE–在 Hadoop MapReduce 中执行 R

**R 和 Hadoop 集成编程环境(RHIPE)** 是一个 R 库，允许用户在 R 编程语言内运行 Hadoop MapReduce 作业。R 程序员只需要编写 R Map 和 R Reduce 函数，RHIPE 库会进行传递，调用相应的 Hadoop Map 和 Hadoop Reduce 任务。RHIPE 使用协议缓冲编码方案来传输映射和减少输入。与其他并行 R 包相比，使用 RHIPE 的优势在于它与 Hadoop 集成良好，并提供了一种在机器集群中使用 HDFS 的数据分发方案，该方案提供了容错能力并优化了处理器的使用。

# R 和 Hadoop 流

Hadoop Streaming API 允许用户使用任何可执行脚本运行 Hadoop MapReduce 作业，该脚本从标准输入中读取数据，并将数据作为映射器或缩减器写入标准输出。因此，Hadoop 流应用编程接口可以在映射或缩减阶段与 R 编程脚本一起使用。这种集成 R 和 Hadoop 的方法不需要任何客户端集成，因为流作业是通过 Hadoop 命令行启动的。提交的 MapReduce 作业将通过 UNIX 标准流和序列化进行数据转换，以确保 Java 投诉输入到 Hadoop，而与程序员提供的输入脚本的语言无关。

你认为 R 与 Hadoop 集成的最佳方式是什么？

# RHIVE–在工作站上安装 R 并连接到 Hadoop 中的数据

如果您希望从 R 接口启动 Hive 查询，那么 r Hive 是一个定位包，具有从 Apache Hive 中检索元数据(如数据库名、列名和表名)的功能。RHIVE 通过用 R 语言函数扩展 HiveQL，为存储在 Hadoop 中的数据提供了丰富的 R 编程语言可用的统计库和算法。RHIVE 函数允许用户将 R 统计学习模型应用于已经使用 Apache Hive 编目的 Hadoop 集群中存储的数据。使用 RHIVE 进行 Hadoop R 集成的优势在于，由于数据操作被下推到 Hadoop 中，因此可以并行化操作，避免数据移动。

# ORCH–面向 Hadoop 的甲骨文连接器

ORCH 可用于非 Oracle Hadoop 集群或任何其他 Oracle 大数据设备。Mappers 和 Reduce 是用 R 编写的，MapReduce 作业是通过高级接口从 R 环境中执行的。有了 ORCH for R Hadoop 集成，R 程序员不必学习一门新的编程语言(如 Java)就能了解 Hadoop 环境的细节，如 Hadoop 集群硬件或软件。ORCH 连接器还允许用户通过相同的函数调用在本地测试 MapReduce 程序的能力，这要在它们部署到 Hadoop 集群之前很久。

使用 R 和 Hadoop 执行大数据分析的开源选项的数量在不断增加，但对于简单的 Hadoop MapReduce 作业，R 和 Hadoop Streaming 仍然被证明是最佳解决方案。R 和 Hadoop 的结合是从事大数据工作的专业人员的必备工具包，可结合您所需的性能、可扩展性和灵活性创建快速预测分析。

大多数 Hadoop 用户声称，使用 R 的优势在于其用于统计和数据可视化的详尽的数据科学库列表。然而，R 中的数据科学库本质上是非分布式的，这使得数据检索成为一件耗时的事情。这是 R 编程语言的一个内在限制，但是如果我们忽略它，那么 R 和 Hadoop 一起可以让大数据分析成为一种享受！

# 数据分析

r 允许我们进行各种各样的数据分析。我们用 Python 中的`pandas`所做的一切，我们也可以用 R 来做。

看看下面的代码:

```scala
df = read.csv(file=file.choose(), header=T, fill=T, sep=",", stringsAsFactors=F)
```

`file.choose()`表示将有一个新窗口，允许您选择要打开的数据文件。`header=T`表示会读取表头。`fill=T`表示它将为任何未定义或缺失的数据值填写 NaN。最后，`sep=","`表示知道如何区分`.csv`文件中不同的数据值。在这种情况下，它们都用逗号隔开。`stringsAsFactors`告诉它把所有的字符串值都当成字符串，而不是因子。这允许我们稍后替换数据中的值。

现在，你应该看到这个:

![](img/5129bc79-f680-4b72-804a-a4c7c6d57da4.png)

Figure: Screenshot of output you will obtain

按*进入*。如果您在 Windows 上，应该会看到类似这样的内容:

![](img/e7bf5619-7bc7-4284-9979-84423484bac0.png)

无论操作系统如何，您都应该会看到一个窗口，允许您选择文件。接下来，您应该会看到:

![](img/7f9a1b41-7d33-46c8-b22b-9a2d766a8029.png)

如果你向右看，你会看到一个名为`df`的新字段。如果你点击它，你可以看到它的内容:

![](img/7d6f7d49-5226-4089-a9fe-dcd706ebe109.png)

现在，我们已经创建了一个数据框架，我们可以开始一些分析。

我们可以获得一些关于行数和列数的信息，以及数据框的长度和列名。请看下面几行代码及其各自的输出:

```scala
> is.data.frame(df)
[1] TRUE
```

```scala
> ncol(df)
[1] 8
```

```scala
> length(df)
[1] 8
```

```scala
> nrow(df)
[1] 27080
```

```scala
> names(df)
[1] "InvoiceNo" "StockCode" "Description" "Quantity" "InvoiceDate"
    "UnitPrice" "CustomerID" "Country"
```

```scala
> colnames(df)
[1] "InvoiceNo" "StockCode" "Description" "Quantity" "InvoiceDate"
    "UnitPrice" "CustomerID" "Country"
```

现在，我们可以继续创建数据子集。看看这段代码:

```scala
d1 = df[1:3]
```

这就是它的结果:

![](img/ddff25d9-3c29-4f22-95b9-78fa2a71693d.png)

所以基本上，我们选择了第 1、2、3 列作为`d1`的数据集。除了我们想要的列之外，我们还可以选择我们想要的行。让我们重新定义`d1`:

```scala
d1 = df[1:10, c(1:3)]
```

![](img/a889acbc-7342-4512-8f68-045c6e249f35.png)

我们还可以访问数据框的单个列。看看这个:

```scala
v1 = df[[3]]
```

这会将整列数据分配给`v1`。现在，让我们进入`v1`的前五个要素:

```scala
v1[1:5]
```

![](img/41221842-c4e9-4262-83e1-22c4e21b958b.png)

我们也可以这样做:

```scala
v2 = df$Description
v2[1:5]
```

![](img/fef8b9d1-8724-4a11-88fb-7d8250d061c9.png)

假设我们知道一个特定的数据值，我们甚至可以访问每个单独的行。这里，我们使用股票代码:

```scala
d1[d1$StockCode == "85123A", ]
```

![](img/4428abc5-db3a-4253-b661-df1ed28ede06.png)

我们可以访问我们想要的特定行:

```scala
d1 = df[1:10, c(1:8)]
d1[2, c(1:8)]
```

![](img/cd439564-438d-47ab-9711-7198a01f8e02.png)

类似于 Python 中的`.head()`函数，r 中有一个`head()`函数，看看这段代码:

```scala
head(df)
```

![](img/9d8e0718-1322-4c4c-9873-fb2dc42d11ab.png)

我们可以添加另一个参数来选择要显示的行数。假设我们要显示前`10`行。下面是代码:

```scala
head(df, 10)
```

![](img/0b1e0833-a8b9-4b1b-8420-0f9d0989f313.png)

我们可以有一个负数作为第二个参数。请看以下内容:

```scala
head(d1, -2)
```

![](img/224dba61-6742-43d8-8976-b4fc4f2a3006.png)

同样，我们可以使用`tail()`显示最后的 *n* 行。请看以下内容:

```scala
tail(d1, 4)
```

![](img/93c37f48-a466-4ffb-bea3-540dc265e4bc.png)

我们也可以有一个负数作为第二个参数，就像`head()`一样。看看这一行代码:

```scala
tail(d1, -2)
```

这会显示 *nrow(d1) + n* 行，其中 *n* 是传递到`tail()`函数的参数:

![](img/5403442c-9241-4774-b3d4-d7b3aeaf2a56.png)

我们可以对一个栏目做一些基本的统计分析。但是，我们必须先转换数据。我们可以做`min()`、`max()`、`mean()`等等。看看这个:

```scala
min(as.numeric(df$UnitPrice))
[1] 0
min(df$UnitPrice)
[1] 0
```

`as.numeric()`表示任何字符串形式的数据值都将被转换为数字。在这种情况下，它们都不是字符串值，否则您会在`0`中看到`min(df$UnitPrice)`结果:

```scala
max(df$UnitPrice)
[1] 16888.02
```

```scala
mean(df$UnitPrice)
[1] 5.857586
```

```scala
median(df$UnitPrice)
[1] 2.51
```

```scala
quantile(df$UnitPrice)
```

![](img/a8acfe69-1886-4867-a0da-624def38b5b8.png)

我们可以在这里添加另一个参数来自定义我们想要的百分比值:

```scala
quantile(df$UnitPrice, c(0, .1, .5, .9)
```

![](img/cdfadca2-1f2d-47a5-b73a-63f2107ee344.png)

```scala
sd(df$UnitPrice)
```

![](img/c1e348ab-ddc3-422a-b5b0-75bfadbaf28b.png)

这告诉我们`df$UnitPrice`的标准差。我们还可以找到方差:

```scala
var(df$UnitPrice)
```

![](img/0c63685c-3327-49b6-9f23-62971df6facc.png)

```scala
range(df$UnitPrice)
```

![](img/a2ff26ae-537d-4fbe-8746-7e67e1e1bd2a.png)

我们还可以得到一个五位数的总结，它告诉我们最小值、第一个分位数、中间值(也是 50%标记)、第三个分位数(75%标记)和最大值:

```scala
fivenum(df$UnitPrice)
```

![](img/beb0a1bd-61e4-4d63-8694-0757f57ddcbb.png)

我们还可以绘制一列选择。看看这个:

```scala
plot(df$UnitPrice)
```

![](img/0c585bf3-1e98-4854-837e-d8055786ac4a.png)

我们可以有不同类型的情节。我们可以引入另一个参数来指定我们想要的绘图类型。请看下面几行代码及其结果图:

```scala
plot(df$UnitPrice, type="p")
```

![](img/0c585bf3-1e98-4854-837e-d8055786ac4a.png)

如你所见，它和我们之前看到的图是一样的。但是，图表有点拥挤，所以我们使用一个较小的范围:

```scala
d1 = df[0:30, c(1:8)]
plot(d1$UnitPrice)
```

![](img/fb6f6450-181c-4a87-a66f-7f2231394235.png)

让我们更简单地重新定义`d1`使其只有`UnitPrice`列:

```scala
d1 = d1$UnitPrice
plot(d1, type="p")
```

该图应该与前一个图相同。

现在，让我们继续:

```scala
plot(d1, type="l")
```

![](img/59f48f7a-b510-42a6-a182-058dc406da5d.png)

这是`d1`的线图:

```scala
plot(d1, type="b")
```

![](img/2f6b392b-1bab-4c22-bb1d-68fa9b3b03f4.png)

这是`d1`的组合线图和点图。然而，它们并不相互重叠:

```scala
plot(d1, type="c")
```

![](img/e3a637d9-5590-4518-a26c-d7aaf8036098.png)

该图仅是我们之前看到的`type="b"`组合图中的线条图:

```scala
plot(d1, type="o")
```

![](img/7e61f33d-af09-4248-863e-668fff00b8c3.png)

这是`d1`的一个过奖图。这意味着线图和点图相互重叠:

```scala
plot(d1, type="h")
```

![](img/3de80e77-ede4-40ed-aedd-2e05eb50979f.png)

这是`d1`的直方图:

```scala
plot(d1, type="s")
```

![](img/8ab0ab49-01b4-4216-85c0-42ad2b2c5502.png)

这是一个阶梯图:

```scala
plot(d1, type="S")
```

![](img/413f6517-29aa-4456-9650-2b66a2482863.png)

两个图的区别在于第一步图，其中`type="s"`是图先水平后垂直的地方。第二步图有`type="S"`，先垂直移动后水平移动。通过看图表可以看出这种差异。

我们还可以使用其他参数，例如:

```scala
#Note: these are parameters, not individual lines of code.

#The title of the graph
main="Title" 

#Subtitle for the graph
sub="title"

#Label for the x-axis
xlab="X Axis"

#Label for the y-axis
ylab="Y Axis"

#The aspect ratio between y and x.
asp=1
```

现在举个例子:

```scala
plot(d1, type="h", main="Graph of Unit Prices vs Index", sub ="First 30 Rows", xlab = "Row Index", ylab="Prices", asp=1.4)
```

![](img/e9f6203c-532f-40b7-afe7-a17531edaf46.png)

要将两个不同的数据帧添加在一起，我们使用`rbind()`。

请看下面的代码:

```scala
d2 = df[0:10, c(1:8)]
d3 = df[21:30, c(1:8)]
d4 = rbind(d2, d3)
```

这是`d2`:

![](img/b9cc7c4c-59fa-4c80-8908-643521e7df9c.png)

这是`d3`:

![](img/e83a8200-d4b1-436d-87fb-15b02a2777b1.png)

现在，这是`d4`:

![](img/42f86184-4b3a-40ec-ad4f-80bdf0ede1fc.png)

需要注意的一点是，所有传入`rbind()`的数据帧必须有相同的列。顺序不重要。

我们也可以合并两个数据帧。

看看这段代码:

```scala
d2 = df[0:11, c("InvoiceNo", "StockCode", "Description")]
d3 = df[11:20, c("StockCode", "Description", "Quantity")]
d4 = merge(d2, d3)
```

这是`d2`:

![](img/8998cbdc-e40c-4219-9de8-2d323a076192.png)

这是`d3`:

![](img/e1676c1e-97a4-4c13-9b47-3ecd1d9f1006.png)

这是`d4`:

![](img/e91dbdab-b16d-4f99-8034-ff6d926a9093.png)

所以默认情况下，`merge()`使用内部连接。

现在，让我们看看外部连接:

```scala
d4 = merge(d2, d3, all=T)
```

![](img/7f6b1060-fa1f-4d4e-a7e2-d750f5635a97.png)

这是左外连接:

```scala
d4 = merge(d2, d3, all.x=T)
```

![](img/4cfe1607-fd3f-4c5d-abc9-40a3dde62397.png)

这是右外连接:

```scala
d4 = merge(d2, d3, all.y=T)
```

![](img/5bbca044-7da7-476d-8076-f661ce2d51f0.png)

最后，交叉连接:

```scala
d4 = merge(d2, d3, by=NULL)
```

![](img/0a97f61d-0125-462e-92bd-77b35c14f28e.png)

就像在熊猫中一样，我们可以用`by=`来指定两个数据项之间的一个`.x`和`.y`，而不是`_x`和`_y`。请看以下内容:

```scala
d4 = merge(d2, d3, by="StockCode", all=T)
```

这是`StockCode`列上的外部连接。

这就是结果:

![](img/9bdf2d92-cbe3-4f5b-bea7-5a27d936e62f.png)

我们可以随时记录下所有的命令，以防万一。执行以下代码保存命令日志:

```scala
savehistory(file="logname.Rhistory")
```

要加载历史记录:

```scala
loadhistory(file="logname.Rhistory")
```

如果您想查看您的历史记录，只需执行以下操作:

```scala
history()
```

![](img/c9c0dd44-7d2d-40d4-a123-36dd24b96ae7.png)

我们可以检查数据，看看是否有空白数据。看看代码:

```scala
colSums(is.na(df))
```

![](img/a3733edb-0075-4d82-b012-1731998281dd.png)

现在，让我们再重复一遍。回想一下，当我们合并两个数据帧时，有些数据值是 NaN:

```scala
d2 = df[0:11, c("InvoiceNo", "StockCode", "Description")]
d3 = df[11:20, c("StockCode", "Description", "Quantity")]
```

现在，让我们对它们进行外部合并:

```scala
d4 = merge(d2, d3, all=T)
```

现在，让我们试试这一行代码:

```scala
colSums(is.na(d4))
```

![](img/7e2d15d2-895f-4d53-81a6-321c84f0d701.png)

我们也可以替换数据中的值。

现在，假设您想将价格大于 3 的每件商品的描述更改为`"Miscellaneous"`。看看这个示例代码:

```scala
d1 = df[0:30, c(1:8)]
```

![](img/e6550294-0b5e-4577-a03f-9702337d35c9.png)

现在，看看这个:

```scala
d1[d1$UnitPrice > 3, "Description"] <- "Miscellaneous"
```

![](img/ab40229c-c499-465d-90c0-82a2de39bfa6.png)

现在我们看到单价大于三的东西都有`"Miscellaneous"`的描述。

我们可以使用除`>`以外的其他运算符，也可以替换其他列中的值。

这里还有一个例子。

假设发票号为`536365`的每一项实际上都来自`United States`。

现在，由于它们都共享相同的发票号和发票日期，我们可以使用其中任何一个来选择所需的行:

```scala
d1[d1$InvoiceNo == 536365, "Country"] = "United States"
```

![](img/f920d771-0a45-48de-8ea6-48d9f0d80304.png)

注意这次我们用`=`代替了`<-`。在这种情况下，它们都在分配一些东西，所以任何一个都可以使用。

# 摘要

在本章中，我们讨论了如何使用 R 来执行数据分析。我们还描述了集成 R 和 Hadoop 的不同选项。

在下一章中，我们将了解 Apache Spark，以及如何基于批处理模型将其用于大数据分析。