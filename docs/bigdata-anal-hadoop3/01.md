# 一、Hadoop 简介

本章向读者介绍了 Hadoop 的世界和 Hadoop 的核心组件，即 **Hadoop 分布式文件系统** ( **HDFS** )和 MapReduce。我们将从介绍 Hadoop 3 版本中的变化和新功能开始。特别是，我们将讨论 HDFS 和**的新特性，以及对客户端应用的更改。此外，我们还将在本地安装一个 Hadoop 集群，并展示新功能，如**擦除编码** ( **EC** )和时间轴服务。快速说明一下，[第 10 章](10.html)、*可视化大数据*向您展示了如何在 AWS 中创建 Hadoop 集群。**

简而言之，本章将涵盖以下主题:

*   HDFS

    *   高可用性
    *   数据节点内平衡器
    *   欧盟委员会（European Commission）
    *   端口映射
*   MapReduce
    *   任务级优化
*   故事
    *   机会集装箱
    *   时间轴服务 v.2
    *   码头集装箱化
*   其他变化
*   Hadoop 3.1 的安装
    *   HDFS
    *   故事
    *   欧盟委员会（European Commission）
    *   时间轴服务 v.2

# Hadoop 分布式文件系统

HDFS 是一个用 Java 实现的基于软件的文件系统，它位于本地文件系统之上。HDFS 背后的主要概念是，它将文件划分为块(通常为 128 MB)，而不是将文件作为一个整体来处理。这允许许多功能，例如分发、复制、故障恢复，更重要的是使用多台机器对数据块进行分布式处理。数据块大小可以是 64 MB、128 MB、256 MB 或 512 MB，无论哪种大小都可以。对于具有 128 兆字节块的 1 GB 文件，将有 1024 兆字节/128 兆字节等于 8 个块。如果考虑三倍的复制因子，则为 24 个数据块。HDFS 提供具有容错和故障恢复的分布式存储系统。HDFS 有两个主要组成部分:名为“T1”的节点和“T2”数据节点“T3”。名称节点包含文件系统所有内容的所有元数据:文件名、文件权限和每个文件的每个块的位置，因此它是 HDFS 最重要的机器。数据节点连接到名称节点，并将数据块存储在 HDFS。他们依赖名称节点来获取文件系统中所有关于内容的元数据信息。如果名称节点没有任何信息，数据节点将无法向任何想要读取/写入 HDFS 的客户端提供信息。

名称节点和数据节点进程可以在一台机器上运行；但是，一般来说，HDFS 集群由运行名称节点进程的专用服务器和运行数据节点进程的数千台机器组成。为了能够访问存储在名称节点中的内容信息，它将整个元数据结构存储在内存中。它通过跟踪数据块的复制因子，确保不会因机器故障而导致数据丢失。由于这是单点故障，为了降低因名称节点故障而导致数据丢失的风险，可以使用辅助名称节点来生成主名称节点内存结构的快照。

数据节点具有很大的存储容量，与名称节点不同，如果数据节点出现故障，HDFS 将继续正常运行。当数据节点出现故障时，名称节点会自动处理故障数据节点中所有数据块的复制，并确保复制得到备份。由于名称节点知道复制块的所有位置，因此连接到群集的任何客户端都能够很少甚至没有中断地继续进行。

In order to make sure that each block meets the minimum required replication factor, the NameNode replicates the lost blocks.

下图描述了文件到名称节点中的块的映射，以及块及其副本在数据节点中的存储:

![](img/f6a72fd8-99f5-487f-9450-99eaae570aeb.png)

如上图所示，自 Hadoop 开始以来，名称节点一直是单点故障。

# 高可用性

在 Hadoop 1.x 和 Hadoop 2.x 中，名称节点的丢失都会导致集群崩溃。在 Hadoop 1.x 中，没有简单的恢复方法，而 Hadoop 2.x 引入了高可用性(主动-被动设置)来帮助从名称节点故障中恢复。

下图显示了高可用性的工作原理:

![](img/327702fa-757a-46a2-bea3-ce7580840dcd.png)

在 Hadoop 3.x 中，您可以有两个被动名称节点和一个主动节点，以及五个**日志节点**，以帮助从灾难性故障中恢复:

*   **名称节点机器**:运行活动和备用名称节点的机器。它们应该具有彼此等效的硬件，以及在非高可用性集群中使用的硬件。

*   **日志节点机器**:运行日志节点的机器。JournalNode 守护程序相对较轻，因此这些守护程序可以合理地与其他 Hadoop 守护程序放在一起，例如 NameNodes、**作业跟踪器**或 Yarn**资源管理器**。

# 数据节点内平衡器

HDFS 有一种方法可以在数据节点之间平衡数据块，但在具有多个硬盘的同一数据节点内部却没有这种平衡。因此，12 轴数据节点可能会有失衡的物理磁盘。但是为什么这对性能有影响呢？嗯，由于磁盘不平衡，数据节点级别的数据块可能与其他数据节点相同，但由于磁盘不平衡，读/写会有偏差。因此，Hadoop 3.x 引入了节点内平衡器来平衡每个数据节点内的物理磁盘，以减少数据的偏斜。

这增加了群集上运行的任何进程执行的读写操作，例如**映射器**或**缩减器**。

# 擦除编码

自从 Hadoop 诞生以来，HDFS 一直是它的基本组件。在 Hadoop 1.x 和 Hadoop 2.x 中，典型的 HDFS 安装使用三倍的复制因子。

与默认的复制因子 3 相比，EC 可能是 HDFS 近年来最大的变化，通过将复制因子从 3 降低到约 1.4，从根本上使许多数据集的容量翻倍。现在让我们了解一下电子商务是怎么回事。

电子商务是一种数据保护方法，其中数据被分解成片段，扩展，用冗余数据段编码，并存储在一组不同的位置或存储器中。如果在此过程中的某个时刻，数据因损坏而丢失，则可以使用存储在其他地方的信息来重建数据。虽然电子商务的中央处理器更密集，但这大大减少了可靠存储大量数据所需的存储空间(HDFS)。HDFS 使用复制来提供可靠的存储，这很昂贵，通常需要存储三份数据拷贝，因此会造成 200%的存储空间开销。

# 端口号

在 Hadoop 3.x 中，各种服务的许多端口都已更改。

以前，多个 Hadoop 服务的默认端口在 Linux 临时端口范围内(32768–61000)。这表明在启动时，服务有时会由于冲突而无法绑定到端口和另一个应用。

这些冲突端口已移出短暂范围，影响了名称节点、辅助名称节点、数据节点和 KMS。

更改如下:

*   **名称节点端口** : 50470 → 9871、50070 → 9870 和 8020 → 9820
*   **辅助名称节点端口** : 50091 → 9869 和 50090 → 9868
*   **数据节点端口** : *5* 0020 → 9867、50010 → 9866、50475 → 9865 和 50075 → 9864

# MapReduce 框架

理解这个概念的一个简单方法是，想象你和你的朋友想把成堆的水果分类放入盒子里。为此，你要给每个人分配一个任务，让他们检查一篮子生水果(全部混在一起)，然后把水果分成不同的盒子。然后每个人做同样的任务，用这篮子水果把水果分成不同的种类。最后，你会从你所有的朋友那里得到很多盒水果。然后，你可以分配一个小组，把同一种水果放在一个盒子里，称重，密封盒子进行运输。展示 MapReduce 框架工作原理的一个经典示例是字数统计示例。以下是处理输入数据的各个阶段，首先将输入拆分到多个工作节点，然后最终生成输出，即字数统计:

![](img/7bc7cdd9-81ef-489e-9585-c360814df7ca.png)

MapReduce 框架由一个资源管理器和多个节点管理器组成(通常，节点管理器与 HDFS 的数据节点共存)。

# 任务级本机优化

MapReduce 增加了对地图输出收集器的本机实现的支持。这种新的支持可以使性能提高大约 30%或更多，尤其是对于洗牌密集型作业。

原生库将通过`Pnative`自动构建。用户可以通过设置`mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred`逐个工作地选择新的收集器。
`nativetask.NativeMapOutputCollectorDelegator`在他们的岗位配置中。

基本思想是能够添加一个`NativeMapOutputCollector`，以便处理映射器发出的键/值对。因此`sort`、`spill`和`IFile`序列化都可以在本机代码中完成。初步测试(在 Xeon E5410、jdk6u24 上)显示了以下有希望的结果:

*   `sort`比 Java 快 3-10 倍左右(仅支持二进制字符串比较)
*   `IFile`序列化速度比 Java 快 3 倍左右:每秒约 500 MB。如果使用 CRC32C 硬件，在每秒 1 GB 或更高的范围内，事情会变得更快
*   合并代码尚未完成，因此测试使用了足够的`io.sort.mb`来防止中间溢出

# 故事

当应用想要运行时，客户端启动 ApplicationMaster，然后 application master 与 ResourceManager 协商，以容器的形式获取集群中的资源。容器表示在单个节点上分配的用于运行任务和进程的 CPU(核心)和内存。容器由节点管理器监督，由资源管理器调度。

容器示例:

*   一个内核和 4 GB 内存
*   两个内核和 6 GB 内存
*   四个内核和 20 GB 内存

一些容器被指定为映射器，另一些被指定为缩减器；所有这些都由 ApplicationMaster 与 ResourceManager 协同工作。这个框架叫做**Yarn**:

![](img/9060b3fa-b525-4979-b94b-1cdb80c3d6c0.png)

使用 Yarn，几个不同的应用可以请求和执行容器上的任务，共享集群资源非常好。然而，随着集群规模的增长以及应用和需求的变化，资源利用的效率会随着时间的推移而降低。

# 机会集装箱

机会容器可以被传输到节点管理器，即使它们在特定时间的执行不能立即开始，不像 Yarn 容器，当且仅当存在未分配的资源时，Yarn 容器才在节点中被调度。

在这些类型的场景中，机会容器将在节点管理器中排队，直到所需的资源可用。这些容器的最终目标是提高集群资源利用率，进而提高任务吞吐量。

# 容器执行的类型

容器有两种类型，如下所示:

*   **保证容器**:这些容器对应现有的 Yarn 容器。它们由容量调度程序分配。当且仅当有资源可用于立即开始执行它们时，它们才被传输到节点。
*   **机会容器**:与保证容器不同，在这种情况下，我们不能保证一旦它们被分派到一个节点，就会有资源可用来开始它们的执行。相反，它们将在节点管理器中排队，直到资源变得可用。

# Yarn 时间线服务 v.2

Yarn 时间线服务 v.2 解决了以下两大挑战:

*   增强时间轴服务的可扩展性和可靠性
*   通过引入流和聚合来提高可用性

# 增强可扩展性和可靠性

版本 2 采用了更具可扩展性的分布式写入器架构和后端存储，与 v.1 相反，v . 1 没有在小集群之外扩展，因为它使用了写入器/读取器架构和后端存储的单个实例。

由于 Apache HBase 甚至可以很好地扩展到更大的集群，并继续保持良好的读写响应时间，v.2 更喜欢选择它作为主要后端存储。

# 可用性改进

很多时候，用户更感兴趣的是在流级别获得的信息，或者是 Yarn 应用的逻辑组。因此，启动一系列的 Yarn 应用来完成一个逻辑工作流会更方便。

为了实现这一点，v.2 支持流的概念，并在流级别聚合度量。

# 体系结构

Yarn 时间线服务 v.2 使用一组收集器(写入器)将数据写入后端
存储器。收集器是分布式的，并且与它们专用的应用主设备位于同一位置。除了资源管理器时间线收集器之外，属于该应用的所有数据都被发送到应用层
时间线收集器。

对于给定的应用，应用主可以将应用的数据写入位于同一位置的时间线收集器(这是本版本中的一个 NM 辅助服务)。此外，运行应用容器的其他节点的
节点管理器也将
数据写入运行应用主节点的节点上的时间线收集器。

资源管理器还维护自己的时间线收集器。它只发出 Yarn 一般的
生命周期事件，以保持合理的写入量。

时间轴读取器是独立于时间轴收集器的独立守护程序，它们
专门用于通过 REST API 提供查询:

![](img/7569b93e-eea4-4d18-a6d5-f1c177807245.png)

下图从高层次说明了设计:

![](img/9d6ef8f3-cef8-492b-ac70-64ab0b5eb2d3.png)

# 其他变化

Hadoop 3 还有其他变化，主要是为了更容易维护和操作。特别是，命令行工具已经进行了改进，以更好地满足运营团队的需求。

# 最低要求的 Java 版本

所有 Hadoop JARs 现在都编译成运行时版本的 Java 8。因此，仍在使用 Java 7 或更低版本的用户必须升级到 Java 8。

# Shell 脚本重写

Hadoop 外壳脚本已经被重写，以修复许多长期存在的错误，并包含一些新功能。

发行说明中记录了不兼容的更改。你可以在 https://issues.apache.org/jira/browse/HADOOP-9902 找到他们。

更多详细信息可在[https://Hadoop . Apache . org/docs/r 3 . 0 . 0/Hadoop-project-dist/Hadoop-common/Unixshellguide . html](https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-common/UnixShellGuide.html)上查阅。出现在[https://Hadoop . Apache . org/docs/r 3 . 0 . 0/Hadoop-project-dist/Hadoop-common/unixshellapi . html](https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-common/UnixShellAPI.html)上的文档将吸引超级用户，因为它描述了大多数新功能，尤其是那些与可扩展性相关的功能。

# 阴影客户端 JARs

新增`hadoop-client-api``hadoop-client-runtime`神器，参考[https://issues.apache.org/jira/browse/HADOOP-11804](https://issues.apache.org/jira/browse/HADOOP-11804)。这些工件将 Hadoop 的依赖关系隐藏在一个 JAR 中。因此，它避免了将 Hadoop 的依赖关系泄漏到应用的类路径中。

Hadoop 现在还支持与微软 Azure 数据湖和阿里云对象存储系统集成，作为 Hadoop 兼容文件系统的替代方案。

# 安装 Hadoop 3

在本节中，我们将了解如何在本地机器上安装单节点 Hadoop 3 集群。为此，我们将遵循在[https://Hadoop . Apache . org/docs/current/Hadoop-project-dist/Hadoop-common/single cluster . html](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html)中给出的文档。

本文档详细描述了如何安装和配置单节点 Hadoop 设置，以便使用 Hadoop MapReduce 和 HDFS 快速执行简单操作。

# 先决条件

必须安装 Java 8 才能运行 Hadoop。如果你的机器上没有 Java 8，那么你可以下载安装 Java 8:[https://www.java.com/en/download/](https://www.java.com/en/download/)。

当您在浏览器中打开下载链接时，以下内容将出现在您的屏幕上:

![](img/3af27dce-05d8-4238-889c-8611b94aa102.png)

# 下载

使用以下链接下载 Hadoop 3.1 版本:[http://Apache . spinellistions . com/Hadoop/common/Hadoop-3 . 1 . 0/](http://apache.spinellicreations.com/hadoop/common/hadoop-3.1.0/)。

以下截图是在浏览器中打开下载链接时显示的页面:

![](img/82d8f312-6bf6-46b7-8f75-6dcca34eef39.png)

当您在浏览器中获得此页面时，只需将`hadoop-3.1.0.tar.gz`文件下载到您的本地机器上。

# 装置

执行以下步骤在您的计算机上安装单节点 Hadoop 集群:

1.  使用以下命令提取下载的文件:

```scala
tar -xvzf hadoop-3.1.0.tar.gz
```

2.  提取 Hadoop 二进制文件后，只需运行以下命令来测试 Hadoop 二进制文件，并确保二进制文件在我们的本地机器上工作:

```scala
cd hadoop-3.1.0

mkdir input

cp etc/hadoop/*.xml input

bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar grep input output 'dfs[a-z.]+'

cat output/*
```

如果一切按预期运行，您将看到一个显示一些输出的输出目录，这表明示例命令是有效的。

A typical error at this point will be missing Java. You might want to check and see if you have Java installed on your machine and the `JAVA_HOME` environment variable set correctly.

# 设置无密码 ssh

现在，通过运行一个简单的命令，检查是否可以在没有密码的情况下将`ssh`切换到`localhost`，如下所示:

```scala
$ ssh localhost
```

如果没有密码就无法从`ssh`切换到`localhost`，请执行以下命令:

```scala
$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
$ chmod 0600 ~/.ssh/authorized_keys
```

# 设置名称节点

对配置文件`etc/hadoop/core-site.xml`进行以下更改:

```scala
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

对配置文件`etc/hadoop/hdfs-site.xml`进行以下更改:

```scala
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
        <name>dfs.name.dir</name>
        <value><YOURDIRECTORY>/hadoop-3.1.0/dfs/name</value>
    </property>
</configuration>
```

# 从 HDFS 开始

按照所示步骤启动 HDFS(名称节点和数据节点):

1.  格式化文件系统:

```scala
$ ./bin/hdfs namenode -format
```

2.  启动名称节点守护程序和数据节点守护程序:

```scala
$ ./sbin/start-dfs.sh
```

Hadoop 守护程序日志输出被写入`$HADOOP_LOG_DIR`目录(默认为`$HADOOP_HOME/logs`)。

3.  浏览名称节点的网页界面；默认情况下，它在`http://localhost:9870/`可用。
4.  创建执行 MapReduce 作业所需的 HDFS 目录:

```scala
$ ./bin/hdfs dfs -mkdir /user 
$ ./bin/hdfs dfs -mkdir /user/<username>
```

5.  完成后，使用以下命令停止守护程序:

```scala
$ ./sbin/stop-dfs.sh
```

6.  打开浏览器查看你本地的 Hadoop，可以在浏览器中作为`http://localhost:9870/`启动。以下是 HDFS 安装的样子:

![](img/235149c3-d713-469c-9062-d9a636e57bf2.png)

7.  单击数据节点选项卡会显示节点，如下图所示:

![](img/38d12898-4191-47d9-859c-845ef3f9c98e.png)

Figure: Screenshot showing the nodes in the Datanodes tab

8.  单击日志将显示集群中的各种日志，如下图所示:

![](img/98bedc90-92bb-4b59-8d5f-6b3bd469c081.png)

9.  如下图所示，您还可以查看集群组件的各种 JVM 指标:

![](img/3856bcdf-f5bd-49d3-9387-5f1350eeda47.png)

10.  如下图所示，您也可以检查配置。这是查看整个配置和所有默认设置的好地方:

![](img/dfc1251c-5569-4e38-b212-42503579cf81.png)

11.  您还可以浏览新安装的集群的文件系统，如下图所示:

![](img/67d6ae3e-693a-45af-a5b2-ff07e37d3ea2.png)

Figure: Screenshot showing the Browse Directory and how you can browse the filesystem in you newly installed cluster

在这一点上，我们都应该能够看到和使用一个基本的 HDFS 集群。但这只是一个包含一些目录和文件的 HDFS 文件系统。我们还需要一个作业/任务调度服务来实际使用集群来满足计算需求，而不仅仅是存储。

# 设置 Yarn 服务

在本节中，我们将设置一个 Yarn 服务，并启动运行和操作 Yarn 集群所需的组件:

1.  启动资源管理器守护程序和节点管理器守护程序:

```scala
$ sbin/start-yarn.sh
```

2.  浏览资源管理器的网页界面；默认情况下，它在:`http://localhost:8088/`可用

3.  运行 MapReduce 作业

4.  完成后，使用以下命令停止守护程序:

```scala
$ sbin/stop-yarn.sh
```

以下是 Yarn 资源管理器，您可以通过将网址`http://localhost:8088/`放入浏览器来查看:

![](img/98257c4c-6e34-4f0c-93c5-0fc5dffb0df5.png)

Figure: Screenshot of YARN ResouceManager

下面是一个视图，显示了集群中的资源队列以及所有正在运行的应用。这也是您可以查看和监控正在运行的作业的地方:

![](img/c840d338-0f8e-40c4-ac05-fdf3e7a60cd5.png)

Figure: Screenshot of queues of resources in the cluster

此时，我们应该能够看到运行 Hadoop 3.1.0 的本地集群中正在运行的 SHART 服务。接下来，我们将看看 Hadoop 3.x 中的一些新功能。

# 擦除编码

EC 是 Hadoop 3.x 中的一个关键变化，与早期版本相比，HDFS 利用率
效率有了显著提高，在早期版本中，例如 3 的复制因子导致了
各种数据的宝贵集群文件系统的巨大浪费，无论这些数据对手头的任务有多重要。

可以使用策略设置电子商务，并将策略分配给 HDFS 的目录。为此，HDFS 提供了一个 ec 子命令来执行与 EC 相关的管理命令:

```scala
hdfs ec [generic options]
    [-setPolicy -path <path> [-policy <policyName>] [-replicate]]
    [-getPolicy -path <path>]
    [-unsetPolicy -path <path>]
    [-listPolicies]
    [-addPolicies -policyFile <file>]
    [-listCodecs]
    [-enablePolicy -policy <policyName>]
    [-disablePolicy -policy <policyName>]
    [-help [cmd ...]]
```

以下是每个命令的详细信息:

*   `[-setPolicy -path <path> [-policy <policyName>] [-replicate]]`:在指定路径的目录上设置 EC 策略。
    *   `path`:HDFS 的一个目录。这是一个强制参数。设置策略只影响新创建的文件，不影响现有文件。
    *   `policyName`:用于该
        目录下文件的欧共体政策。如果设置了`dfs.namenode.ec.system.default.policy`配置，该参数可以省略。路径的电子商务策略将在配置中使用默认值进行设置。
    *   `-replicate`:对目录应用特殊的复制策略，强制目录采用 3x 复制方案。
    *   `-replicate and -policy <policyName>`:这些是可选参数。不能同时指定它们。
*   `[-getPolicy -path <path>]`:获取指定路径的文件或目录
    的 EC 策略的详细信息。
*   `[-unsetPolicy -path <path>]`:取消对目录中`setPolicy`的前一次调用所设置的 EC 策略。如果目录从祖先目录继承了 EC 策略，`unsetPolicy`是一个禁止操作。在没有显式策略设置的目录上取消设置策略不会返回错误。
*   `[-listPolicies]`:列出所有(启用、禁用和删除)在 HDFS 注册的欧共体政策
    。只有启用的策略适合与`setPolicy`命令一起使用。
*   `[-addPolicies -policyFile <file>]`:添加欧共体政策列表。示例策略文件请参考
    `etc/hadoop/user_ec_policies.xml.template`。
    最大单元大小在属性
    `dfs.namenode.ec.policies.max.cellsize`中定义，默认值为 4 MB。
    目前 HDFS 允许用户总共添加 64 个策略，添加的策略 ID 在 64 到 127 之间。如果已经添加了 64 个策略，添加策略将失败。
*   `[-listCodecs]`:获取系统中支持的 EC 编解码器和编码器列表。
    编码器是编解码器的一种实现。一个编解码器可以有不同的实现，因此有不同的编码器。编解码器的编码器列在秋季
    倒序中。
*   `[-removePolicy -policy <policyName>]`:删除欧共体政策
*   `[-enablePolicy -policy <policyName>]`:启用欧共体政策
*   `[-disablePolicy -policy <policyName>]`:禁用欧共体政策

通过使用`-listPolicies`，您可以列出当前在您的集群
中设置的所有 EC 策略，以及这些策略的状态，无论它们是`ENABLED`还是`DISABLED`:

![](img/d9f5b65b-d252-4d6e-aa44-a1c7da6e4906.png)

Lets test out EC in our cluster. First we will create directories in the HDFS shown as follows:

```scala
./bin/hdfs dfs -mkdir /user/normal
./bin/hdfs dfs -mkdir /user/ec
```

创建两个目录后，您可以在任何路径上设置策略:

```scala
./bin/hdfs ec -setPolicy -path /user/ec -policy RS-6-3-1024k
Set RS-6-3-1024k erasure coding policy on /user/ec
```

现在将任何内容复制到`/user/ec`文件夹中都会落入新设置的`policy`中。

键入如下所示的命令进行测试:

```scala
./bin/hdfs dfs -copyFromLocal ~/Documents/OnlineRetail.csv /user/ec
```

下面的截图显示了复制的结果，正如预期的那样，系统会抱怨，因为我们的本地系统上没有足够的集群来实现 EC。但这应该能让我们了解需要什么，以及它会是什么样子:

![](img/de9f2ac3-cc3e-4041-b707-1996c93ade54.png)

# 数据节点内平衡器

虽然 HDFS 一直有一个很好的功能，即在集群中的数据节点之间平衡数据，但这通常会导致数据节点内的磁盘倾斜。例如，如果您有四个磁盘，两个磁盘可能会占用大部分数据，另外两个磁盘可能利用率不足。考虑到物理磁盘(比如 7，200 或 10，000 rpm)的读/写速度较慢，这种数据倾斜会导致性能不佳。使用节点内平衡器，我们可以在磁盘之间重新平衡数据。

运行以下示例中显示的命令，在数据节点上调用磁盘平衡:

```scala
./bin/hdfs diskbalancer -plan 10.0.0.103
```

以下是磁盘平衡器命令的输出:

![](img/787ea679-5262-4c92-a4e8-8447c890ed5a.png)

# 安装 Yarn 时间线服务 v.2

如*Yarn 时间线服务 v.2* 部分*所述，* v.2 始终选择 Apache HbSe 作为主要后备存储，因为 Apache HbSe 甚至可以很好地扩展到更大的集群，并继续保持良好的读写响应时间。

为时间轴服务 v.2 准备存储需要执行几个步骤:

1.  设置 HBase 集群
2.  启用协处理器
3.  为时间轴服务 v.2 创建模式

以下各节将更详细地解释每个步骤。

# 设置 HBase 群集

第一步包括选择一个 Apache HBase 集群用作存储集群。时间轴服务 v.2 支持的 Apache HBase 版本是 1.2.6。1.0.x 版本不再与时间轴服务 v.2 一起工作。HBase 的更高版本尚未与时间轴服务一起测试。

# HBase 的简单部署

如果您想要为 Apache HBase 集群创建一个简单的部署配置文件，其中数据
加载量很小，但数据需要跨节点来来往往保持不变，那么您可以
考虑*独立 HBase* 而不是 *HDFS* 部署模式。

[http://mirror.cogentco.com/pub/apache/hbase/1.2.6/](http://mirror.cogentco.com/pub/apache/hbase/1.2.6/)

以下截图是 HBase 1.2.6 的下载链接:

![](img/e483c898-ef32-449e-958d-eb6fb5b3fd76.png)

下载`hbase-1.2.6-bin.tar.gz`到你的本地机器。然后提取 HBase
二进制文件:

```scala
tar -xvzf hbase-1.2.6-bin.tar.gz
```

以下是提取的糖化血红蛋白的内容:

![](img/b2a3451f-7775-48ce-b4be-dab766e13e46.png)

这是独立的 HBase 设置的一个有用的变体，它让所有的 HBase 守护程序在一个 JVM 中运行，但是它不是持久保存在本地文件系统中，而是持久保存在 HDFS 实例中。向复制数据的 HDFS 进行写入可确保数据在节点来来往往时保持不变。要配置这个独立变体，编辑您的`hbasesite.xml`设置`hbase.rootdir`指向您的 HDFS 实例中的一个目录，然后将`hbase.cluster.distributed`设置为`false`。

下面是我们安装的本地集群的`hbase-site.xml`和`hdfs`端口`9000`作为一个属性。如果不考虑这一点，将不会安装 HBase 集群。

```scala
<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://localhost:9000/hbase</value>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>false</value>
    </property>
</configuration>
```

下一步是启动 HBase。我们将通过使用`start-hbase.sh`脚本来做到这一点:

```scala
./bin/start-hbase.sh
```

下面的截图显示了我们刚刚安装的 HBase 集群:

![](img/2d9583e6-2731-4393-8559-5bb959b42d74.png)

以下屏幕截图显示了显示各种组件版本的 HBase 群集设置的更多属性:

![](img/099c6f3a-1144-403b-b231-67353e9581a0.png)

Figure: Screenshot of attributes of the HBase cluster setup and the versions of different components

一旦准备好使用 Apache HBase 集群，请执行以下部分中的步骤。

# 启用协处理器

在这个版本中，协处理器是动态加载的。

将时间轴服务`.jar`复制到 HDFS，在那里 HBase 可以加载它。它是在模式创建器中创建 flowrun 表所必需的。默认的 HDFS 位置是`/hbase/coprocessor`。

例如:

```scala
hadoop fs -mkdir /hbase/coprocessor hadoop fs -put hadoop-yarn-server-timelineservice-hbase-3.0.0-alpha1-SNAPSHOT.jar /hbase/coprocessor/hadoop-yarn-server-timelineservice.jar
```

为了将罐子放在 HDFS 的不同位置，还存在一个名为`yarn.timeline-service.hbase.coprocessor.jar.hdfs.location`的 Yarn 配置设置，如下所示:

```scala
<property>
  <name>yarn.timeline-service.hbase.coprocessor.jar.hdfs.location</name>
  <value>/custom/hdfs/path/jarName</value>
</property>
```

使用模式创建工具创建时间线服务模式。为了实现这一点，我们还需要确保所有的 JARs 都被正确找到:

```scala
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/Users/sridharalla/hbase-1.2.6/lib/:/Users/sridharalla/hadoop-3.1.0/share/hadoop/yarn/timelineservice/
```

更正类路径后，我们可以使用一个简单的命令创建 HBase 模式/表，如下所示:

```scala
./bin/hadoop org.apache.hadoop.yarn.server.timelineservice.storage.TimelineSchemaCreator -create -skipExistingTable
```

以下是根据前面的命令创建的 HBase 架构:

![](img/0f39e3bf-a2db-417d-bfc4-d7edc06b78c8.png)

# 启用时间轴服务 v.2

以下是启动时间轴服务 v.2 的基本配置:

```scala
<property>
  <name>yarn.timeline-service.version</name>
  <value>2.0f</value>
</property>

<property>
  <name>yarn.timeline-service.enabled</name>
  <value>true</value>
</property>

<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle,timeline_collector</value>
</property>

<property>
  <name>yarn.nodemanager.aux-services.timeline_collector.class</name>
  <value>org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService</value>
</property>

<property>
  <description> This setting indicates if the yarn system metrics is published by RM and NM by on the timeline service. </description>
  <name>yarn.system-metrics-publisher.enabled</name>
  <value>true</value>
</property>

<property>
  <description>This setting is to indicate if the yarn container events are published by RM to the timeline service or not. This configuration is for ATS V2\. </description>
  <name>yarn.rm.system-metrics-publisher.emit-container-events</name>
  <value>true</value>
</property>
```

此外，将`hbase-site.xml`配置文件添加到客户端 Hadoop 集群配置中，以便它可以将数据写入您正在使用的 Apache HBase 集群，或者将`yarn.timeline-service.hbase.configuration.file`设置为指向`hbase-site.xml`的文件 URL，用于将数据写入 HBase 的相同目的，例如:

```scala
<property>
  <description>This is an Optional URL to an hbase-site.xml configuration file. It is to be used to connect to the timeline-service hbase cluster. If it is empty or not specified, the HBase configuration will be loaded from the classpath. Else, they will override those from the ones present on the classpath. </description>
  <name>yarn.timeline-service.hbase.configuration.file</name>
  <value>file:/etc/hbase/hbase-site.xml</value>
</property>
```

# 运行时间轴服务 v.2

重新启动资源管理器和节点管理器以获取新配置。收集器以嵌入式方式在资源管理器和节点管理器中启动。

时间轴服务读取器是一个独立的 Yarn 守护程序，可以使用以下语法启动:

```scala
$ yarn-daemon.sh start timelinereader
```

# 启用 MapReduce 写入时间线服务 v.2

要将 MapReduce 框架数据写入时间轴服务 v.2，请在`mapred-site.xml`中启用以下配置:

```scala
<property>
  <name>mapreduce.job.emit-timeline-data</name>
  <value>true</value>
</property>
```

时间线服务仍在发展中，所以您应该尝试它，只是为了测试功能，而不是在生产中，并等待更广泛采用的版本，它应该很快就会推出。

# 摘要

在本章中，我们讨论了 Hadoop 3.x 中的新功能，以及它如何提高 Hadoop 2.x 的可靠性和性能。我们还介绍了在本地机器上安装独立的 Hadoop 集群。

在下一章中，我们将一窥大数据分析的世界。