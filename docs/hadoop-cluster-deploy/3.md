# 三、配置 Hadoop 生态系统

Hadoop 是一个强大的分布式数据处理系统。我们在上一章中配置的集群是一个现成的系统，但是如果您开始在这个配置中为任何实际应用使用 Hadoop，您将很快发现 MapReduce 提供了一种非常低级的方法来访问和处理数据。你需要自己解决很多事情。您需要决定如何从外部来源导出数据，并以最有效的方式将其上传到 Hadoop 中。您将需要弄清楚以什么格式存储数据，并编写 Java 代码来实现 MapReduce 范例中的数据处理。Hadoop 生态系统包括许多辅助项目，这些项目是为了解决加载、处理和提取数据的不同方面而创建的。在本章中，我们将讨论几个流行且重要的 Hadoop 生态系统项目的设置和配置:

*   用于从外部数据源提取数据的 Sqoop
*   高级、类似于 SQL 的数据访问配置单元
*   用于实时数据处理的 Impala

还有许多与 Hadoop 相关的项目，但我们将重点关注那些能立即提高 Hadoop 集群对最终用户可用性的项目。

# 托管 Hadoop 生态系统

通常，额外的 Hadoop 组件不会托管在集群本身中。这些项目大多作为 HDFS 和 MapReduce 的客户端，可以在不同的服务器上执行。这些服务器被标记为 Hadoop 客户端，如[第 1 章](1.html "Chapter 1. Setting Up Hadoop Cluster – from Hardware to Distribution")、*设置 Hadoop 集群-从硬件到分发*的集群图所示。在物理上和网络级别上将 Hadoop 节点与客户端分开的主要原因是安全性。Hadoop 客户端服务器应该由组织内的不同人员访问。如果您决定在与 Hadoop 相同的服务器上运行客户端，您将不得不投入大量精力为每个用户提供适当的访问级别。从逻辑和物理上分离这些实例简化了任务。Hadoop 客户端通常部署在虚拟机上，因为资源要求不高。

### 注

请记住，读写 HDFS 数据的客户端需要能够访问命名节点以及群集中的所有数据节点。

如果您正在运行一个小型集群或者的用户数量有限，并且不太关心安全问题，您可以将大多数客户端程序托管在与名称节点或数据节点相同的节点上。

# Sqoop

Sqoop 是一个工具，它连接了两个世界:关系数据库和 Hadoop。将关系数据库中的数据导入 Hadoop，反之亦然，这是一项非常常见的任务。对于许多组织来说，Hadoop 是一个数据集成平台。让我们假设您的网站生成了大量的应用日志文件。这些日志包含关于用户如何与您的应用交互的非常有价值的信息，但是您可以在日志中捕获的唯一用户信息是用户名。另一方面，您的关系数据库中有非常详细的用户配置文件数据。能够轻松地将这些数据导出到 Hadoop 中，并对不同类别的用户如何与网站交互进行一些分析是极其重要的。另一方面，复杂的长时间运行的 MapReduce 作业产生的结果可能相对较小。通常，这些数据被加载到关系数据库管理系统中，用于实时数据访问或与商业智能工具集成。

## 安装和配置 Sqoop

Sqoop 是一个命令行工具，尽管未来的版本将提供客户端-服务器类型的访问。Sqoop 几乎总是部署在 Hadoop 集群之外的服务器上。

Sqoop 需要能够访问所有 Hadoop 节点，以及您计划使用的关系数据库。只对几个选定的 Sqoop 节点提供这种级别的访问更容易。

使用 CDH 存储库很容易安装 Sqoop:

```sh
# yum install sqoop

```

如果您在尚未安装 CDH 软件包的客户端服务器上安装它，您将需要安装几个依赖项，包括 Hadoop-client 和 Hadoop-MapReduce 软件包。

Sqoop 包附带了几个配置文件:

```sh
# ls -lh /etc/sqoop/conf/
-rwxr-xr-x 1 root root 1.4K Apr 22 20:38 sqoop-env-template.sh
-rwxr-xr-x 1 root root 5.0K Apr 22 20:38 sqoop-site-template.xml
-rwxr-xr-x 1 root root 5.0K Apr 22 20:38 sqoop-site.xml

```

模板文件提供了一个选项列表，您可以使用它来优化 Sqoop。幸运的是，如果你使用 CDH 分布，你将不需要在那里改变很多东西。主 Sqoop 配置文件`sqoop-site.xml`带有一组很好的默认值，Sqoop 可以开箱即用。有几个与 Metastore 配置相关的选项，您可能想要更改。我们稍后将讨论它们。

Sqoop 依靠 JDBC 驱动程序将连接到不同的数据库服务器。由于许可差异，这些驱动程序不随 Sqoop 一起提供，必须单独安装。为了演示 Sqoop 的基本导入/导出功能，我们将使用一个 MySQL 数据库作为源。可以从[mysql.com](http://mysql.com)下载 MySQL JDBC 驱动。驱动程序以`.jar`文件的形式出现，需要复制到`/usr/lib/sqoop/lib`下，由 Sqoop 自动拾取。

## Sqoop 导入示例

对于下面的例子，我们将使用 MySQL“`world`”数据库，该数据库可以在[http://dev.mysql.com/doc/index-other.html](http://dev.mysql.com/doc/index-other.html)找到。要执行这些示例，您需要启动并运行 MySQL 服务器，将`world`数据库加载到其中，为 Sqoop 创建一个用户，并确保运行 Sqoop 的机器可以访问远程(或本地)MySQL 服务器。

要将表从 MySQL 导入 Hadoop，可以运行以下命令:

```sh
# sqoop import --connect jdbc:mysql://mysql-db.server.example.com/world --table Country --username sqoop_user –P
Enter password:

```

在这种情况下，传递给 Sqoop 的选项非常简单。我们需要指定 JDBC 连接字符串、我们想要导入的表(本例中为`Country`)以及我们为此目的创建的 MySQL 用户。您也可以使用`--driver` 选项指定使用哪个 JDBC 驱动程序。请记住，JDBC 连接字符串格式因数据库而异，因此请务必参考数据库文档。

了解 Sqoop 如何执行导入非常重要。Sqoop 分析您试图导入的表，并生成要在 Hadoop 集群上执行的 Java MapReduce 代码。每个地图任务将连接到 MySQL 数据库，使用`SELECT`语句读取一部分数据，并将其写入 HDFS。Sqoop 试图利用 Hadoop 提供的并行性来加快导入过程。这对于性能来说非常好，但是需要记住这种方法的一些含义。最重要的影响之一是，导入任务是从几个集群服务器运行的，并且从数据库的角度来看是完全独立的。这意味着无法保证事务一致性。您可以使用`--num-mappers` 选项控制并行任务的数量，默认为 4。

### 注

使用`--num-mappers`选项时要小心。来自多个 Hadoop 节点的并发连接太多，很容易使数据库过载。

让我们看看导入的文件。默认情况下，Sqoop 将在运行作业的用户的 HDFS 主目录中创建新目录。新的目录名将与正在导入的表名相匹配。您可以使用`--warehouse-dir`选项控制目标目录。如果您在导入后检查您的 HDFS 主目录，您将会看到以下内容(为简洁起见，输出被删除):

```sh
# hdfs dfs -ls /user/alice/Country
Found 6 items
0 2013-06-05 20:56 /user/alice/Country/_SUCCESS
0 2013-06-05 20:54 /user/alice/Country/_logs
9919 2013-06-05 20:56 /user/alice/Country/part-m-00000
6990 2013-06-05 20:56 /user/alice/Country/part-m-00001
7069 2013-06-05 20:56 /user/alice/Country/part-m-00002
7512 2013-06-05 20:56 /user/alice/Country/part-m-00003

```

从我们在[第 2 章](2.html "Chapter 2. Installing and Configuring Hadoop")、*安装和配置 Hadoop* 中运行的 Wordcount MapReduce 程序中，您应该对`_SUCCESS`和`_logs`文件很熟悉。这非常清楚地表明，每个 Sqoop 导入实际上都是一个 MapReduce 作业。您还会注意到有四个结果文件。这是因为本次导入使用了 4 个独立的地图任务。他们每个人都生成了一个单独的纯文本文件。您现在可以使用`hdfs`命令检查这些文件，对它们运行 MapReduce 作业，等等。

## Sqoop 导出示例

为了演示将数据加载到 MySQL(或任何其他关系数据库)中的相反过程，创建一个示例 CSV 文件并将其保存到 HDFS `/user/alice/tomysql`目录中:

```sh
# hdfs dfs -cat /user/alice/tomysql/result

```

要将数据导出到 MySQL 表中，请运行以下命令:

```sh
# sqoop export --connect jdbc:mysql://mysql-db.server.example.com/test --table hadoop_results --username sqoop_user -P --export-dir /user/root/tomysql/

```

这与的`import`命令没有太大区别。在这里，我们需要用`--test`选项指定哪个 MySQL 表将用于数据加载，以及从哪里用`--export-dir`读取数据。作业完成后，您可以使用简单的`SELECT` 语句验证数据是否正确加载。

这些例子只展示了 Sqoop 可以执行的非常基本的操作。Sqoop 比所展示的更加灵活和强大。它允许使用自定义条件控制从关系数据库导入哪些数据。Sqoop 可以执行增量导入，跟踪已经导入了哪些行。您可以指定要使用的文件格式和压缩类型等等。关于 Sqoop 的详细文档可以在[http://sqoop.apache.org/docs/1.4.3/SqoopUserGuide.html](http://sqoop.apache.org/docs/1.4.3/SqoopUserGuide.html)找到。

### 注

新版本的 Sqoop 2 将解决当前版本的几个限制。这包括以客户机-服务器模式运行、提供可重用的数据库连接、拥有适当的权限模型等等。到目前为止，Sqoop 2 还没有一个稳定的版本。

# 鼠标

如果你想从[第二章](2.html "Chapter 2. Installing and Configuring Hadoop")、*安装和配置 Hadoop* 中探究 WordCount MapReduce 作业示例的源代码，或者试图自己编写一些代码，你现在应该已经意识到这是 Hadoop 中处理数据的一种非常低级的方式。事实上，如果编写 MapReduce 作业是 Hadoop 中访问数据的唯一方式，那么它的可用性将非常有限。

Hive 就是为了解决这个特殊的问题而设计的。事实证明，大量处理数据过滤、聚合和分组的 MapReduce 代码可以自动生成。因此，可以设计一种高级数据处理语言，然后将其编译成本机 Java MapReduce 代码。实际上，没有必要为此设计新的语言。SQL 已经成为关系数据库中处理数据的事实标准。对于 Hive 开发人员来说，解决方案是显而易见的:采用一种 SQL 方言，构建 Hive 作为从 SQL 到 MapReduce 的编译器。Hive 提供的语言叫做 **HiveQL** 。值得一提的是，像 Pig 这样的项目走了不同的道路，他们没有重新使用 SQL 作为高级语言，而是想出了自己的新语言。

在 Hadoop 集群上使用 SQL 的能力极大地提高了其可用性，并扩大了可以使用集群数据的用户群体。它还允许将使用 SQL 接口的应用与 Hadoop 集成在一起。这使得 Hive 成为 Hadoop 生态系统中的必备组件之一。

## Hive 建筑

本质上，Hive 执行以下任务:获取一个 HiveQL 查询，将其编译成一个 MapReduce 任务，将该任务提交给集群，并将结果呈现给用户。为了将 HiveQL 编译成 MapReduce 代码，Hive 必须理解被查询数据的结构。Hive 将数据组织到熟悉的关系表中，这些关系表有定义类型的列。另一方面，Hive 对存储在 Hadoop 集群中的数据进行操作，Hadoop 不会对其数据强加任何模式，这一点您已经知道。这意味着 Hive 必须提供一个额外的层来存储其表的元数据。Hive 通过 **Metastore** 服务实现这一层。

Metastore 是一个双组件服务。第一个组件是一个服务器，它接受来自 Hive 客户端的连接，并向它们提供所需表的信息。第二个是关系数据库，Metastore 使用它将元数据保存在磁盘上。为此，Hive Metastore 支持几个数据库，如 MySQL、PostgreSQL 和 Oracle。MySQL 是 Metastore 后端的流行选择，我们将在集群中使用它。Metastore 可以在不同的模式下运行。您可以使用嵌入在 Hive 客户端中的 Metastore。

您可以将 Metastore 作为本地服务运行，并将几个 Hive 客户端连接到它。或者，您可以运行远程集群范围的服务，该服务可以由来自多台机器的多个客户端使用。所有选项都有用例，但是对于生产 Hadoop 集群，运行远程 Metastore 是最常见的选择。这样，您可以轻松地在多个客户端之间共享 Hive 元数据，同时将客户端与后端关系数据库隔离开来。为了实现高可用性，您还可以运行多个 Metastore 服务器。

到目前为止，当我们提到 Hive 客户端时，我们实际上指的是 Hive 命令行界面。类似于 SQL 的接口允许 Hive 支持使用 JDBC 和 ODBC 驱动程序的应用。这使得 Hive 查询引擎和数据可供各种第三方应用使用。这些应用不直接与 Metastore 通信，而是依赖于一个 **Hive Server** ，它提供查询编译和 Metastore 访问服务。

如果您再次参考[第 1 章](1.html "Chapter 1. Setting Up Hadoop Cluster – from Hardware to Distribution")、*中的 Hadoop 集群图设置 Hadoop 集群–从硬件到分发*，您会看到我们已经为 Hive Metastore 和其他服务分配了一个单独的服务器。我们将坚持这个计划，在单独的机器上安装 Hive Metastore、Hive Server 和 Metastore 后端数据库。这是可选的，您可以选择将这些服务器与其他 Hadoop 组件(如作业跟踪器或名称节点)共存。Hive 的硬件资源要求一般，但取决于客户端的数量和您计划托管的 Hive 表。

## 安装 Hive 元存储

在我们安装 Hive Metastore 之前，我们需要确保有一个后端数据库启动并运行。对于我们的集群，我们将为此目的使用 MySQL 数据库。我们将在运行 Hive Metastore 的同一台服务器上运行 MySQL 数据库，但是后端数据库通常位于 Hadoop 集群之外。安装和配置 MySQL 超出了本书的范围。MySQL 安装详情可以参考[http://dev.mysql.com/doc/refman/5.5/en/installing.html](http:// http://dev.mysql.com/doc/refman/5.5/en/installing.html)。值得一提的是，丢失 Hive 元数据会有很大的影响，因为您需要重新创建所有的表定义。为 Metastore 后端数据库设置适当的高可用性和备份非常重要。

安装并配置完 MySQL 后，您需要为 Hive Metastore 创建一个 MySQL 数据库和一个用户。

如果您已经在 Hive Metastore 服务器上配置了 CDH 存储库，如[第 2 章](2.html "Chapter 2. Installing and Configuring Hadoop")、*安装和配置 Hadoop* 中所述，您现在可以简单地使用以下`yum`命令来安装 Metastore:

```sh
# yum install hive-metastore

```

配置单元配置类似于其他 Hadoop 组件。配置文件，包括带有所有选项的模板文件及其描述，位于`/etc/hive/conf`下。有许多配置选项可用，但我们将重点关注核心选项，以快速启动和运行 Hive。所有选项都应添加到`hive-site.xml`文件中。

### 注

您需要将 Hadoop 集群节点中的`hdfs-site.xml`和`mapred-site.xml`文件复制到所有与 Hive 相关的服务器上的`/etc/Hadoop/conf`目录中。否则，Hive 将无法与 HDFS 通信并启动 MapReduce 作业。

首先，我们需要指定后端使用哪个数据库驱动程序。在我们的例子中，我们需要添加以下属性:

```sh
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
```

几个选项为后端数据库提供凭证，并且是不言自明的，因此我们将它们包含在一个块中，如下所示:

```sh
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://localhost/hive_meta </value>

<name>javax.jdo.option.ConnectionUserName</name>
<value>hivemetauser</value>

<name>javax.jdo.option.ConnectionPassword</name>
<value>secret</value>
```

数据库连接字符串具有简单的 JDBC 格式，并且 MySQL 用户和密码应该与 MySQL 安装期间创建的匹配。

如前所述，您可以运行几个 Metastore 服务，使用相同的数据库后端来实现高可用性。您需要在`hive.metastore.uris`变量中为所有这些元存储指定连接字符串。在我们的例子中，只有一个 Metastore:

```sh
<name>hive.metastore.uris</name>
<value>thrift://hive.hadoop.test.com:9083</value>
```

在前面的代码中，`hive.hadoop.test.com`是运行 Metastore 的机器的主机名，`9083`是正在监听的默认端口。您可以通过修改`/etc/default/hive-metastore`文件中的`PORT`变量来更改端口。确保该端口对所有 Hive 客户端可用。

最后，我们需要指定 Hive 将存储表数据的位置。在 Hive 中，每个数据库和表都由 HDFS 的一个目录和一组文件来表示。您可以使用`hive.metastore.warehouse.dir`变量指定这些文件的默认位置:

```sh
<name>hive.metastore.warehouse.dir</name>
<value>/warehouse</value>
```

要启动 Metastore 服务，我们需要采取以下步骤:

1.  需要在 HDFS 创建具有适当权限的 Hive 仓库目录。以`hdfs`用户身份运行以下命令:

    ```sh
    # hdfs dfs -mkdir /warehouse
    # hdfs dfs -chmod a+w /warehouse

    ```

2.  请注意，我们允许所有用户写入`/warehouse`目录。这对于允许多个用户使用 Hive 非常重要。另一种方法是将所有 Hive 用户分配到一个组，并将`/warehouse`所有权更改为该组。
3.  下载并安装 MySQL JDBC 驱动程序。由于许可证差异，它没有随 Hive 包一起发货，所以您需要从[http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/)下载。打开档案后，使用以下命令将`.jar`文件复制到 Hive `lib`目录:

    ```sh
    # cp mysql-connector-java-5.1.25-bin.jar /usr/lib/hive/lib/

    ```

4.  使用以下命令启动 Hive Metastore】

始终检查错误日志文件以确认服务已经正确启动。默认情况下，可以在`/var/log/hive`目录中找到 Hive 日志文件。

## 安装配置单元客户端

在本节中，我们将安装 Hive CLI 并将其链接到 Hive Metastore。Hive 客户端可以安装在集群内的任何机器上，但在大多数情况下，它们安装在专用的 Gateway 服务器上。通常，作为 Hadoop 管理员，您需要向组织内的多个用户提供 Hive 客户端访问。将这些用户隔离在网关机器上可以更容易地控制和管理这些权限。

要安装配置单元客户端，请运行以下命令:

```sh
# yum install hive

```

就像在 Hive Metastore 的情况下一样，您需要将 Hadoop 配置文件复制到客户端机器，以提供对 HDFS 和 MapReduce 的 Hive 访问。

Hive 客户端使用与 Metastore 相同的配置文件`hive-site.xml`。但是，您不需要将 Metastore 配置文件复制到客户端。Metastore 的整个思想是封装对后端数据库的访问。Hive 客户端不需要访问我们为 Metastore 配置的 MySQL 数据库。只有 Hive Metastore 需要在其配置文件中指定 MySQL 凭据。

相反，您将使用单独的 Hive 客户端配置文件。幸运的是，让 Hive 启动并运行只需要几个核心选项。

首先，您需要指定您将使用远程 Metastore。将以下选项添加到您的 Hive 客户端服务器上的`hive-site.xml`中:

```sh
<name>hive.metastore.local</name>
<value>false</value>
```

接下来，您需要将 Hive 客户端指向 Hive Metastore 的位置:

```sh
<name>hive.metastore.uris</name>
<value>thrift://hive.hadoop.test.com:9083</value>
```

这两个选项足以让 Hive 客户端运行。要启动配置单元命令行界面，请运行以下`hive`命令:

```sh
# hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/root/hive_job_log_88a7e774-cdb5-4d6c-b1eb-77ad36207a91_1397164153.txt
hive>

```

您可以在屏幕上看到一些关于正在打印的 Hive 日志文件的信息。日志文件的级别和位置可以在`hive-log4j.properties`文件中调整。

有关创建 Hive 表、编写 HiveQL 查询等的详细信息，您可以参考以下网站的 Hive 文档:[https://cwiki.apache.org/confluence/display/Hive/Tutorial](https://cwiki.apache.org/confluence/display/Hive/Tutorial)

## 安装配置单元服务器

此时，您应该已经安装并配置了 Hive Metastore 和 Hive 客户端。如果您只打算使用 Hive CLI，这个设置就足够了，但是如果您需要使用 Hive JDBC 或 ODBC 驱动程序提供对应用的访问，则需要安装和配置 Hive Server。这类应用可能包括高级网络界面，如 Beeline 和 BI 工具。这也将允许您从 Python、Perl 和其他语言的程序中访问 Hive。

我们将在用于 Hive Metastore 的物理机器上安装 Hive 服务器，尽管这不是必需的。您可以在集群中的任何其他计算机上安装配置单元服务器。

CDH 存储库中目前有两个版本的 Hive Server:Hive Server 1 和 HiveServer2。我们将安装 HiveServer2，因为它提供并发客户端支持、额外的安全选项和一些其他改进。HiveServer2 是所有新开发发生的地方，为了兼容性，HiveServer1 包含在存储库中。

要安装 HiveServer2，请运行以下命令:

```sh
# yum install hive-server2

```

如果您查看包内容，您会注意到它只提供服务器启动/关闭的脚本。所有必需的二进制文件都已经安装了 Hive 和 Hive Metastore 包。

有几个选项，您需要将其添加到`/etc/hive/conf/hive-site.xml`文件中，以获得 HiveServer2 支持。首先，我们需要支持并发客户端:

```sh
<name>hive.support.concurrency</name>
<value>true</value>
```

HiveServer2 使用 ZooKeeper 集群来管理表锁，这是正确的并发管理所必需的。我们将把它指向 ZooKeeper 服务器，我们已经在[第 2 章](2.html "Chapter 2. Installing and Configuring Hadoop")、*为 NameNode HA 安装和配置了 Hadoop* :

```sh
<name>hive.zookeeper.quorum</name>
<value>nn1.hadoop.test.com:2181,nn2.hadoop.test.com:2181,jt1.hadoop.test.com:2181</value>
```

因为我们在与 Hive Metastore 相同的机器上安装 HiveServer2，所以我们使用相同的配置文件。如果您决定在单独的机器上安装 HiveServer2，您需要使用`hive.metastore.uris`选项将其指向 Metastore。

默认情况下，HiveServer2 将使用端口`10000`，但您可以通过更改`/etc/defaults/hive-server2`文件中的`HIVE_PORT`选项来控制这一点。

现在，您可以通过运行以下命令来启动 HiveServer2:

```sh
# service hive-server2 start

```

# 黑斑羚

黑斑羚是 Hadoop 生态系统的新成员。它的测试版于 2012 年首次推出，第一个稳定版本于 2013 年 6 月发布。即使 Impala 是一个新项目，仍然有很多需要改进的地方，但它试图实现的目标的意义使其在本书中值得一提。Impala 的目标非常远大——为 Hadoop 带来实时查询。Hive 使得在 Hadoop 中使用类似 SQL 的语言来查询数据成为可能，但是在性能方面仍然受到 MapReduce 框架的限制。值得一提的是，像 Stinger([http://hortonworks.com/labs/stinger/](http://hortonworks.com/labs/stinger/))这样的项目致力于显著提高 Hive 的性能，但它仍在开发中。

Impala 绕过 MapReduce，直接在 HDFS 对数据进行操作，以实现显著的性能提升。Impala 大部分是用 C++写的。它使用内存缓冲区来缓存数据，通常操作起来更像并行关系数据库。黑斑羚被设计成与 Hive 兼容。它使用相同的 HiveQL 语言，甚至利用现有的 Hive Metastore 来获取关于表和列的信息。如果您的集群中有现有的 Hive 表，您将能够使用 Impala 查询它们，而无需任何更改。

## 黑斑羚建筑

黑斑羚由几个部分组成。有一个 **Impala 服务器**进程，应该运行在集群中的每个数据节点上。它的职责是处理来自客户端的查询，将数据读写到 HDFS，执行连接和聚合任务，等等。由于查询在许多服务器上并行执行，接收用户请求的守护程序成为查询协调器，并同步该特定查询的所有其他节点的工作。

由于 Hadoop 集群中的数据节点或单个 Impala 守护程序可能会不时停机，因此需要持续跟踪集群中所有守护程序的状态。为此，黑斑羚使用了一个 **黑斑羚州立商店**。如果一些 Impala 服务器没有向状态存储报告，它们会被标记为死亡，并被排除在进一步的查询尝试之外。Impala 服务器可以在没有状态存储的情况下运行，但是如果集群中有故障节点，响应可能会延迟。

此外，Impala 使用了我们在*安装 Hive Metastore* 部分为 Hive 配置的相同 Metastore 服务。Impala 同时提供了命令行界面和 JDBC/ODBC 访问。

Impala 服务没有额外的硬件要求。Impala 服务器总是运行在数据节点上，因为它们依赖于本地数据访问。Impala 状态存储可以与现有的 Hadoop 服务(如 JobTracker 或备用名称节点)位于同一位置。

## 安装黑斑羚州立商店

对于我们的集群，我们将在 JobTracker 节点上安装状态存储。Impala 不包含在标准的 CDH 存储库中，因此您需要通过将[http://archive . cloudera . com/impala/red hat/6/x86 _ 64/impala/cloudera-impala . repo](http://archive.cloudera.com/impala/redhat/6/x86_64/impala/cloudera-impala.repo)文件复制到除 NameNodes 之外的所有 Hadoop 节点上的`/etc/yum.repos.d/`目录中来添加一个新的存储库。

### 注

黑斑羚储存库的链接可能会改变。请参考以下网站的 Cloudera 文档了解最新信息:[http://www . Cloudera . com/content/Cloudera-content/Cloudera-docs/Impala/latest/Installing-and-use-Impala/ciiu _ prereqs . html](http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_prereqs.html)

完成后，您可以通过运行以下`yum`命令来安装 Impala 状态存储:

```sh
# yum install impala-state-store

```

状态存储包仅包含启动脚本，impala 二进制文件将作为依赖 Impala 包的一部分安装。

状态存储不需要太多配置。Impala 不提取现有的 Hadoop 配置文件，而是依赖 HDFS 进行数据访问。这意味着您需要手动将`core-site.xml`和`hdfs-site.xml`文件从您的 Hadoop 配置目录复制到`/etc/imala/conf`目录。

您可以通过编辑`/etc/default/impala`文件来调整状态存储端口和日志目录。默认情况下，状态存储使用端口`24000`。我们将保持现状。

要启动状态存储，运行以下命令:

```sh
# service impala-state-store start

```

您需要检查`/var/log/impala`中的日志文件，以确保服务已经正确启动。

## 安装黑斑羚服务器

要在数据节点上安装 Impala 服务器，运行以下命令:

```sh
# yum install impala-server

```

这将安装 Impala 二进制文件，以及服务器启动停止脚本。在启动 Impala 服务器之前，需要对数据节点配置进行一些额外的调整。

黑斑羚使用一种叫做短路读数的 HDFS 特征。它允许 Impala 绕过标准的数据节点级别，并能够直接从本地存储中读取文件块。要启用该功能，需要在`hdfs-site.xml`文件中添加以下选项:

```sh
<name>dfs.client.read.shortcircuit</name>
<value>true</value>

<name>dfs.domain.socket.path</name>
<value>/var/run/hadoop-hdfs/dn._PORT</value>
```

像 Impala 这样的本地 HDFS 客户端将使用套接字路径与数据节点通信。您需要保留`dfs.domain.socket.path`的值，如这里所示。Hadoop 将用数据节点端口号替换它。

此外，Impala 将尝试独立于数据节点跟踪文件块的位置。以前，名称节点服务只能提供哪个块驻留在哪个服务器上的信息。Impala 更进一步，跟踪这个数据节点的哪些文件块存储在哪个磁盘上。为此，在 CDH 4.1 中引入了一个新的 API。要启用它，您需要在`hdfs-site.xml`文件中添加以下选项:

```sh
<name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
<value>true</value>

<name>dfs.client.file-block-storage-locations.timeout</name>
<value>3000</value>
```

您需要在所有数据节点上应用上述更改，并重新启动数据节点守护程序，然后才能继续进行 Impala 配置。

我们需要做的下一件事是将 Impala 服务器指向状态存储，这是我们之前在 JobTracker 上配置的。为此，更改`/etc/default/impala`配置文件中的以下变量:

```sh
IMPALA_STATE_STORE_HOST=jt1.hadoop.test.com
IMPALA_STATE_STORE_PORT=24000
```

Impala 是一个非常消耗内存的过程。目前，它必须在内存中完全执行表连接等操作。如果没有足够的可用内存，查询将被中止。此外，Impala 需要缓存关于现有表、文件和块位置的元数据。要控制 Impala 内存分配的上限，请将`–mem_limit=X`选项添加到`/etc/default/impala`中的`IMPALA_SERVER_ARGS`变量。`X`将是服务器上可用物理内存的百分比。

要为 MapReduce 任务和 Impala 查询正确分配资源，您需要知道您的集群工作负载概要是什么。如果您知道将有一个 MapReduce 作业与 Impala 查询同时运行，您可能需要将 Impala 内存限制为可用 RAM 的 40%，甚至更少。请做好准备，在这种情况下，某些 Impala 查询将无法完成。如果你计划使用 Impala 作为主要的数据处理工具，你可以分配 70-80%的内存给它。

Impala 服务器以及 Impala 状态存储不重用现有的 Hadoop 配置文件。在启动 Impala 服务器之前，需要将`core-site.xml`和`hdfs-site.xml`文件复制到`/etc/impala/conf`目录中。

要启动英帕拉服务器，运行以下命令:

```sh
# service impala-server start

```

要在命令行中使用 Impala，您需要安装 Impala 命令行界面:

```sh
# yum install impala-shell

```

要启动 Impala shell，请执行以下命令:

```sh
# imapla-shell

```

要连接到 Impala 服务器，在 Impala shell 中运行以下命令:

```sh
> connect dn1.hadoop.test.com;
Connected to dn1.hadoop.test.com:21000
Server version: impalad version 1.0.1 RELEASE (build df844fb967cec8740f08dfb8b21962bc053527ef)

```

### 注

请记住，目前 Impala 不处理自动元数据更新。例如，如果您已经使用 Hive 创建了一个新表，您将需要在 Impala shell 中运行一个`REFRESH`命令来查看更改。

现在，您的一个数据节点上运行着一个功能齐全的 Impala 服务器。要完成安装，您需要将此设置传播到所有数据节点。每个 Impala 服务器都可以接受来自 shell 或应用的客户端请求。接收请求的服务器成为查询协调器，因此在集群中的节点之间平均分配请求是一个好主意。

Impala 仍处于活跃的开发阶段。请务必查看每个新版本的文档，因为 Impala 配置和一般行为的许多方面在未来可能会发生变化。

# 总结

在本章中，我们学习了安装三个主要的 Hadoop 生态系统项目。Sqoop、Hive 和 Impala 显著提高了集群的可用性。还有很多 Hadoop 生态系统项目，我们无法在本书中涵盖。它们中的大多数都可以在 CDH 存储库中找到，因此您可以在需要时安装它们。在下一章中，我们将介绍 Hadoop 安全选项。公司将其最关键和最敏感的数据托付给 Hadoop，因此了解如何确保这些数据的安全非常重要。