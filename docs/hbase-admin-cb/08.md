# 八、基本性能调整

在本章中，我们将介绍：

*   设置 Hadoop 以分散磁盘 I/O
*   使用网络拓扑脚本使 Hadoop 机架感知
*   使用 `noatime`和 `nodiratime`挂载磁盘
*   将 `vm.swappiness`设置为 0 以避免交换
*   Java GC 和 HBase 堆设置
*   使用压缩
*   管理压缩
*   管理区域拆分

# 简介

性能是 HBase 集群行为最有趣的特征之一。 这对管理员来说是一项具有挑战性的操作，因为性能调优不仅需要深入了解 HBase，还需要深入了解 Hadoop、**Java 虚拟机垃圾收集(JVM GC)**以及操作系统的重要调优参数。

典型的 HBase 集群结构如下图所示：

![Introduction](img/7140_08_01.jpg)

集群中有几个组件-ZooKeeper 集群、HBase 主节点、区域服务器、**Hadoop 分布式文件系统(HDFS)**和 HBase 客户端。

ZooKeeper 集群充当整个 HBase 集群的协调服务，处理主服务器选择、根区域服务器查找、节点注册等。 主节点不执行繁重的任务。 它的工作包括区域分配和故障转移、日志拆分和负载均衡。 区域服务器保存实际区域；它们处理对托管区域的 I/O 请求，将内存中的数据存储(MemStore)刷新到 HDFS，以及拆分和压缩区域。 HDFS 是 HBase 存储其数据文件(StoreFile)和预写日志(WAL)的地方。 我们通常有一个 HBase 区域服务器与 HDFS DataNode 运行在同一台机器上，但这不是强制性的。

HBase 客户端提供访问 HBase 集群的 API。 要与集群通信，客户端需要找到持有特定行键范围的区域服务器；这称为区域查找。 HBase 有两个系统表来支持区域查找- `-ROOT-`表和 `.META`表。 桌子。

`-ROOT-`表用于引用 `.META`中的区域。 表，而 `.META`。 该表包含对所有用户区域的引用。 首先，客户端查询 ZooKeeper 以找到 `-ROOT-`表位置(部署它的区域服务器)；然后查询 `-ROOT-`表，然后查询 `.META`。 表中，查找包含特定区域的区域服务器。 客户端还缓存区域位置，以避免查询 ZooKeeper、 `-ROOT-`和 `.META`。 每次都有桌子。

有了这些背景知识，我们将在本章描述如何调优 HBase 以获得更好的性能。

除了 HBase 本身，其他调优点包括 Hadoop 配置、JVM 垃圾收集设置和操作系统内核参数。 这些与调整 HBase 本身一样重要。 在本章中，我们还将介绍调整这些配置的方法。

# 设置 Hadoop 以分散磁盘 I/O

现代服务器通常具有多个磁盘设备以提供大存储容量。 这些磁盘通常按照出厂设置配置为 RAID 阵列。 这对许多情况都有好处，但对 Hadoop 不好。

Hadoop 从节点在其本地磁盘上存储 HDFS 数据块和 MapReduce 临时文件。 这些本地磁盘操作得益于使用多个独立磁盘来分散磁盘 I/O。

在本指南中，我们将介绍如何设置 Hadoop 以使用多个磁盘来分散其磁盘 I/O。

## 做好准备

我们假设每个 DataNode 节点都有多个磁盘。 这些磁盘采用**JBOD(就是一堆磁盘)**或 RAID0 配置。 假设磁盘挂载在 `/mnt/d0, /mnt/d1`、...、 `/mnt/dn`，启动 HDFS 的用户对每个挂载点都有写权限。

## 怎么做……

要将 Hadoop 设置为分散磁盘 I/O，请按照以下说明操作：

1.  在每个 DataNode 节点上，在每个磁盘上为 HDFS 创建目录以存储其数据块：

    ```scala
    hadoop$ mkdir -p /mnt/d0/dfs/data
    hadoop$ mkdir -p /mnt/d1/dfs/data
    ...
    hadoop$ mkdir -p /mnt/dn/dfs/data

    ```

2.  将以下代码添加到 HDFS 配置文件(hdfs-site.xml)：

    ```scala
    hadoop@master1$ vi $HADOOP_HOME/conf/hdfs-site.xml
    <property>
    <name>dfs.data.dir</name>
    <value>/mnt/d0/dfs/data,/mnt/d1/dfs/data,...,/mnt/dn/dfs/data </value>
    </property>

    ```

3.  在群集中同步修改后的 `hdfs-site.xml`文件：

    ```scala
    hadoop@master1$ for slave in `cat $HADOOP_HOME/conf/slaves`
    do
    rsync -avz $HADOOP_HOME/conf/ $slave:$HADOOP_HOME/conf/
    done

    ```

4.  重新启动 HDFS：

    ```scala
    hadoop@master1$ $HADOOP_HOME/bin/stop-dfs.sh
    hadoop@master1$ $HADOOP_HOME/bin/start-dfs.sh

    ```

## 它是如何工作的.

我们推荐将 JBOD 或 RAID0 用于 DataNode 磁盘，因为您不需要 RAID 的冗余，因为 HDFS 使用节点之间的复制来确保其数据冗余。 因此，单个磁盘发生故障时不会丢失数据。

选择 JBOD 还是 RAID0？ 从理论上讲，JBOD 配置比 RAID 配置具有更好的性能。 这是因为，在 RAID 配置中，您必须等待阵列中最慢的磁盘完成，然后才能完成整个写入操作，这使得平均 I/O 时间相当于最慢的磁盘的 I/O 时间。 在 JBOD 配置中，速度较快的磁盘上的操作将独立于速度较慢的磁盘完成，这使得平均 I/O 时间比速度最慢的磁盘快。 然而，企业级 RAID 卡可能会带来很大的不同。 在决定使用哪种配置之前，您可能希望对 JBOD 和 RAID0 配置进行基准测试。

对于 JBOD 和 RAID0 配置，您将在不同的路径挂载磁盘。 这里的关键点是将 `dfs.data.dir`属性设置为每个磁盘上创建的所有目录。 `dfs.data.dir`属性指定 DataNode 应将其本地块存储在何处。 通过将其设置为逗号分隔的多个目录，DataNode 以循环方式跨所有磁盘存储数据块。 这会使 Hadoop 有效地将磁盘 I/O 分散到所有磁盘。

### 提示

**警告**

不要在 `dfs.data.dir`属性值的目录路径之间留空，否则它不会按预期工作。

您需要在整个群集中同步更改并重新启动 HDFS 才能应用这些更改。

## 还有更多...

如果您运行 MapReduce，因为 MapReduce 将其临时文件存储在 TaskTracker 的本地文件系统上，您可能还希望设置 MapReduce 以扩展其磁盘 I/O：

1.  在每个 TaskTracker 节点上，在每个磁盘上为 MapReduce 创建目录以存储其中间数据文件：

    ```scala
    hadoop$ mkdir -p /mnt/d0/mapred/local
    hadoop$ mkdir -p /mnt/d1/mapred/local
    ...
    hadoop$ mkdir -p /mnt/dn/mapred/local

    ```

2.  将以下内容添加到 MapReduce 的配置文件(mapred-site.xml)：

    ```scala
    hadoop@master1$ vi $HADOOP_HOME/conf/mapred-site.xml
    <property>
    <name>mapred.local.dir</name>
    <value>/mnt/d0/mapred/local,/mnt/d1/mapred/local,...,/mnt/dn/mapred/local </value>
    </property>

    ```

3.  在群集中同步修改后的 `mapred-site.xml`文件，然后重新启动 MapReduce。

MapReduce 在执行过程中会在 TaskTracker 的本地磁盘上生成大量临时文件。 与 HDFS 一样，在不同磁盘上设置多个目录有助于显著分散 MapReduce 磁盘 I/O。

# 使用网络拓扑脚本实现 Hadoop 机架感知

Hadoop 有“机架感知”的概念。 管理员可以定义群集中每个 DataNode 的机架。 使 Hadoop 机架感知极其重要，因为：

*   机架感知可防止数据丢失
*   机架感知可提高网络性能

在本食谱中，我们将介绍如何使 Hadoop 机架感知，以及为什么它很重要。

## 做好准备

您需要知道每个从节点所属的机架。 以启动 Hadoop 的用户身份登录到主节点。

## 怎么做……

以下步骤介绍如何使 Hadoop 机架感知：

1.  Create a `topology.sh` script and store it under the Hadoop configuration directory. Change the path for `topology.data`, in line 3, to fit your environment:

    ```scala
    hadoop@master1$ vi $HADOOP_HOME/conf/topology.sh
    while [ $# -gt 0 ] ; do
    nodeArg=$1
    exec< /usr/local/hadoop/current/conf/topology.data
    result=""
    while read line ; do
    ar=( $line )
    if [ "${ar[0]}" = "$nodeArg" ] ; then
    result="${ar[1]}"
    fi
    done
    shift
    if [ -z "$result" ] ; then
    echo -n "/default/rack "
    else
    echo -n "$result "
    fi
    done

    ```

    别忘了设置脚本文件的 EXECUTE 权限：

    ```scala
    hadoop@master1$ chmod +x $HADOOP_HOME/conf/topology.sh

    ```

2.  创建一个 `topology.data`文件，如以下代码片段所示；更改 IP 地址和机架以适合您的环境：

    ```scala
    hadoop@master1$ vi $HADOOP_HOME/conf/topology.data
    10.161.30.108 /dc1/rack1
    10.166.221.198 /dc1/rack2
    10.160.19.149 /dc1/rack3

    ```

3.  将以下内容添加到 Hadoop 核心配置文件(`core-site.xml`)：

    ```scala
    hadoop@master1$ vi $HADOOP_HOME/conf/core-site.xml
    <property>
    <name>topology.script.file.name</name>
    <value>/usr/local/hadoop/current/conf/topology.sh</value>
    </property>

    ```

4.  跨群集中同步修改后的文件，然后重新启动 HDFS 和 MapReduce。
5.  确保 HDFS 现在是机架感知的。 如果一切正常，您应该能够在 NameNode 日志文件中找到类似以下代码段的内容：

    ```scala
    2012-03-10 13:43:17,284 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /dc1/rack3/10.160.19.149:50010
    2012-03-10 13:43:17,297 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /dc1/rack1/10.161.30.108:50010
    2012-03-10 13:43:17,429 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /dc1/rack2/10.166.221.198:50010

    ```

6.  确保 MapReduce 现在是机架感知的。 如果一切正常，您应该能够在 JobTracker 日志文件中找到类似以下代码段的内容：

    ```scala
    2012-03-10 13:50:38,341 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /dc1/rack3/ip-10-160-19-149.us-west-1.compute.internal
    2012-03-10 13:50:38,485 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /dc1/rack1/ip-10-161-30-108.us-west-1.compute.internal
    2012-03-10 13:50:38,569 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /dc1/rack2/ip-10-166-221-198.us-west-1.compute.internal

    ```

## 它是如何工作的.

下图显示了 Hadoop 机架感知的概念：

![How it works...](img/7140_08_02.jpg)

HDFS 文件的每个数据块将被复制到多个 DataNode，以防止因一台机器故障而丢失所有数据副本。 但是，如果数据的所有副本恰好在同一机架中的 DataNode 上复制，并且该机架出现故障，则所有数据副本都将丢失。 因此，为了避免这种情况，NameNode 需要知道网络拓扑，以便使用该信息进行智能数据复制。

如上图所示，在默认复制系数为 3 的情况下，两个数据副本将放置在同一机架中的计算机上，另一个数据副本将放置在不同机架中的计算机上。 这确保了单个机架故障不会导致丢失所有数据副本。

通常，与不同机架中的两台机器相比，同一机架中的两台机器之间具有更高的带宽和更低的延迟。 有了网络拓扑信息，Hadoop 能够通过从适当的 DataNode 读取数据来最大化网络性能。 如果本地机器上有数据可用，Hadoop 将从其中读取数据。 如果不是，Hadoop 将尝试从同一机架中的机器读取数据，如果两者都不可用，则将从不同机架中的机器读取数据。

在步骤 1 中，我们创建一个 `topology.sh`脚本。 该脚本将 DNS 名称作为参数，并返回网络拓扑(Rack)名称作为输出。 DNS 名称到网络拓扑的映射由步骤 2 中创建的 `topology.data`文件提供。如果在 `topology.data`文件中未找到条目，脚本将返回 `/default/rack`作为默认机架名称。

### 备注

请注意，我们在 `topology.data`文件中使用的是 IP 地址，而不是主机名。 有一个已知缺陷，即 Hadoop 无法正确处理以字母“a”到“f”开头的主机名。 有关更多详细信息，请查看 HADOOP-6682：[https://issues.apache.org/jira/browse/HADOOP-6682](http://https://issues.apache.org/jira/browse/HADOOP-6682)。

在步骤 3 中，我们设置 `core-site.xml`中的 `topology.script.file.name`属性，告诉 Hadoop 调用 `topology.sh`将 DNS 名称解析为网络拓扑名称。

重新启动 Hadoop 后，如步骤 5 和 6 中的日志所示，HDFS 和 MapReduce 会将正确的机架名称作为前缀添加到从节点的 DNS 名称中。 这表明 HDFS 和 MapReduce 机架感知与上述设置配合良好。

# 带记号和记号的安装盘

如果您纯粹为 Hadoop 挂载磁盘，并且使用 ext3 或 ext4，或者 XFS 文件系统，我们建议您使用 `noatime`和 `nodiratime`属性挂载磁盘。

如果将磁盘挂载为 `noatime`，则在文件系统上读取文件时不会更新访问时间戳。 在属性为 `nodiratime`的情况下，挂载磁盘不会更新文件系统上的目录信息节点访问时间。 由于不再有用于更新访问时间戳的磁盘 I/O，这加快了文件系统的读取速度。

在本指南中，我们将介绍为什么推荐 Hadoop 使用 `noatime`和 `nodiratime`选项，以及如何使用 `noatime`和 `nodiratime`挂载磁盘。

## 做好准备

您需要在从节点上拥有 root 权限。 我们假设您有两个仅用于 Hadoop 的磁盘-/dev/xvdc 和 `/dev/xvdd`。 这两个磁盘分别安装在 `/mnt/is1`和 `/mnt/is2`。 此外，我们还假设您使用的是 ext3 文件系统。

## 怎么做……

要使用 `noatime`和 `nodiratime`挂载磁盘，请在集群中的每个从节点上执行以下指令：

1.  将以下内容添加到 `/etc/fstab`文件：

    ```scala
    $ sudo vi /etc/fstab
    /dev/xvdc /mnt/is1 ext3 defaults,noatime,nodiratime 0 0
    /dev/xvdd /mnt/is2 ext3 defaults,noatime,nodiratime 0 0

    ```

2.  卸载磁盘并再次装载它们以应用更改：

    ```scala
    $ sudo umount /dev/xvdc
    $ sudo umount /dev/xvdd
    $ sudo mount /dev/xvdc
    $ sudo mount /dev/xvdd

    ```

3.  检查是否已应用装载选项：

    ```scala
    $ mount
    /dev/xvdc on /mnt/is1 type ext3 (rw,noatime,nodiratime)
    /dev/xvdd on /mnt/is2 type ext3 (rw,noatime,nodiratime)

    ```

## 它是如何工作的.

由于 Hadoop(HDFS)使用 NameNode 管理其文件系统的元数据(Inode)，因此 Hadoop 保存的任何访问时间信息都独立于单个块的 `atime`属性。 因此，DataNode 的本地文件系统中的访问时间戳在这里没有任何意义。 这就是为什么我们建议您使用 `noatime`和 `nodiratime`挂载磁盘，如果这些磁盘仅用于 Hadoop。 使用 `noatime`和 `nodiratime`挂载磁盘可在每次访问本地文件时节省写入 I/O。

这些选项在 `/etc/fstab`文件中设置。 要应用更改，请不要忘记再次卸载并挂载磁盘。

启用这些选项后，HDFS 读取的性能有望提高。 由于 HBase 将数据存储在 HDFS 上，因此 HBase 的读取性能也有望提高。

## 还有更多...

另一种优化方法是降低 ext3 或 ext4 文件系统保留块的百分比。 默认情况下，某些文件系统块保留供特权进程使用。 这是为了避免用户进程为了继续工作而填满系统守护进程所需的磁盘空间的情况。 这对于托管操作系统的磁盘非常重要，但对于仅由 Hadoop 使用的磁盘用处较小。

通常，这些仅用于 Hadoop 的磁盘具有非常大的存储空间。 降低保留块的百分比可以向 HDFS 群集添加相当多的存储容量。 通常，保留块的默认百分比为 5%。 可以降到 1%。

### 提示

**警告：**

不要减少托管操作系统的磁盘上的保留块。

为此，请在集群中每个从节点的每个磁盘上运行以下命令：

```scala
$ sudo tune2fs -m 1 /dev/xvdc
tune2fs 1.41.12 (17-May-2010)
Setting reserved blocks percentage to 1% (1100915 blocks)

```

# 将 vm.swappness 设置为 0 以避免交换

Linux 将一段时间内未访问的内存页移动到交换空间，即使有足够的空闲内存可用。 这就是所谓的换出。 另一方面，将交换出的数据从交换空间读出到内存称为换入。 交换在许多情况下是必要的，但由于**Java 虚拟机(JVM)**在交换下表现不佳，如果交换，HBase 可能会遇到麻烦。 ZooKeeper 会话到期是交换可能导致的典型问题。

在本食谱中，我们将介绍如何调优 Linux `vm.swappiness`参数以避免交换。

## 做好准备

确保您在群集中的节点上具有 root 权限。

怎么做……。 要调优 Linux 参数以避免交换，请在集群中的每个节点上调用以下命令：

1.  执行以下命令将 `vm.swappiness`设置为 `0:`

    ```scala
    root# sysctl -w vm.swappiness=0
    vm.swappiness = 0

    ```

    *   此更改将一直持续到服务器下一次重新启动。
2.  将以下内容添加到 `/etc/sysctl.conf`文件中，以便在系统启动时启用该设置：

    ```scala
    root# echo "vm.swappiness = 0" >> /etc/sysctl.conf

    ```

## 它是如何工作的.

`vm.swappiness`参数可用于定义内存页面交换到磁盘的积极程度。 它接受 `0`到 `100—a`之间的任何值。较低的值意味着内核不太可能交换应用，而较高的值将使内核更频繁地换出应用。 默认值为 `60`。

我们在步骤 1 中将 `vm.swappiness`设置为 `0`，这将导致内核尽可能长时间地避免将进程换出物理内存。 这对 HBase 有好处，因为 HBase 进程消耗大量内存。 较高的 `vm.swappiness`值将使 HBase 交换很多，并遇到非常慢的垃圾收集。 随着 ZooKeeper 会话超时，这可能会导致 RegionServer 进程被终止。 我们建议您将其设置为 `0`或任何其他较小的数字(例如， `10)`)，并观察交换状态。

请注意，由 `sysctl`命令设置的值仅在服务器下一次重新启动之前有效。 您需要在 `/etc/sysctl.conf`文件中设置 `vm.swappiness`，以便在系统重新启动时启用该设置。

## 另请参阅

*   *更改内核设置*配方，[第 1 章](01.html "Chapter 1. Setting Up HBase Cluster")，*设置 HBase 群集*

# Java GC 和 HBase 堆设置

由于 HBase 在 JVM 中运行，JVM**垃圾收集(GC)**设置对于 HBase 平稳、高性能运行非常重要。 除了配置 HBase 堆设置的一般指导原则之外，让 HBase 进程输出其 GC 日志，然后根据 GC 日志的输出调优 JVM 设置也很重要。

在本食谱中，我们将描述最重要的 HBase JVM 堆设置，以及如何启用和理解 GC 日志记录。 我们还将介绍一些针对 HBase 调优 Java GC 设置的一般指导原则。

## 做好准备

登录您的 HBase 区域服务器。

## 怎么做……

以下是推荐的 Java GC 和 HBase 堆设置：

1.  通过编辑 `hbase-env.sh`文件为 HBase 提供足够的堆大小。 例如，以下代码片段为 HBase 配置 8000 MB 的堆大小：

    ```scala
    $ vi $HBASE_HOME/conf/hbase-env.sh
    export HBASE_HEAPSIZE=8000

    ```

2.  使用以下命令启用 GC 日志记录：

    ```scala
    $ vi $HBASE_HOME/conf/hbase-env.sh
    export HBASE_OPTS="$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/usr/local/hbase/logs/gc-hbase.log"

    ```

3.  添加以下代码以早于默认值启动**并发标记扫描 GC(CMS)**：

    ```scala
    $ vi $HBASE_HOME/conf/hbase-env.sh
    export HBASE_OPTS= "$HBASE_OPTS -XX:CMSInitiatingOccupancyFraction=60"

    ```

4.  跨群集中同步更改并重新启动 HBase。
5.  Check that the GC logs were output to the specified log file (/usr/local/hbase/logs/gc-hbase.log).

    GC 日志如下所示：

![How to do it...](img/7140_08_03.jpg)

## 它是如何工作的.

在步骤 1 中，我们配置 HBase 堆内存大小。 默认情况下，HBase 使用 1 GB 的堆大小，这对于现代机器来说太低了。 4 GB 以上的堆大小对 HBase 是有好处的，而我们的建议是 8 GB 或更大，但在 16 GB 以下。

在步骤 2 中，我们启用 JVM 日志记录。 使用该设置，您将获得区域服务器的 JVM 日志，类似于我们在步骤 5 中显示的内容。理解日志输出需要有关 JVM 内存分配和垃圾收集的基本知识。 以下是 JVM 分代垃圾收集系统的示意图：

![How it works...](img/7140_08_04.jpg)

有三个堆世代：**Perm**(或永久)世代、**老世代**(或永久)世代和**年轻世代**。 年轻一代部分由三个独立的空间组成：**伊甸园**空间和两个幸存者空间**S0**和**S1**。

通常，对象分配在年轻一代的**Eden**空间中。 如果分配失败(**Eden**已满)，所有 Java 线程都会暂停，并调用年轻一代 GC(Minor GC)。 年轻一代(**Eden**和**S0**空间)中的所有幸存对象都被复制到**S1**空间。 如果**s1**空间已满，则会将对象复制(升级)到旧版本。 升级失败时收集老一代(主要/完整 GC)。 永久世代和老世代通常聚集在一起。 永久生成用于保存对象的类和方法定义。

回到步骤 5 中的示例，上述选项的次要 GC 输出将以以下形式生成：

```scala
<timestamp>: [GC [<collector>: <starting occupancy1> -> <ending occupancy1>, <pause time1> secs] <starting occupancy3> -> <ending occupancy3>, <pause time3> secs] [Times: <user time> <system time>, <real time>]

```

在此输出中：

*   `<timestamp>`是发生 GC 的时间，相对于应用启动。
*   `<collector>`是次要集合中使用的收集器的内部名称。
*   `<starting occupancy1>`是收藏前年轻一代的占有率。
*   `<ending occupancy1>`是收集后年轻一代的占有率。
*   `<pause time1>`是次要收集的暂停时间(秒)。
*   `<starting occupancy3>`是集合之前整个堆的占用率。
*   `<ending occupancy3>`是集合之后整个堆的占用率。
*   `<pause time3>`是整个垃圾收集的暂停时间。 这将包括一次大型收藏的时间。
*   `[Time:]`说明 GC 收集所花费的时间、用户时间、系统时间和实时。

步骤 5 中输出的第一行指示一个较小的 GC，这会使 JVM 暂停 0.0764200 秒。 它将年轻一代的空间从 14.8MB 减少到 1.6MB。

接下来，我们将看到 CMS GC 日志。 HBase 使用 CMS GC 作为老一代的默认垃圾收集器。

CMS GC 执行以下步骤：

1.  首标
2.  并发阅卷
3.  评论 / 话语 / 注意 / 言辞
4.  并发扫描

CMS 仅在初始标记和备注阶段暂停应用的线程。 在并发标记和清理阶段，CMS 线程与应用的线程一起运行。

示例中的第二行表示 CMS 初始标记花费了 0.0100050 秒，而并发标记花费了 6.496 秒。 请注意，这是一个并发标记；Java 没有暂停。

在以 `1441.435: [GC[YG occupancy:`开始的行处有一个停顿...]。 在前面 GC 日志的屏幕截图中。 此处的停顿时间是 0.0413960 秒以重新标记堆。 在那之后，你可以看到清扫开始了。 Cms 扫描花了 3.446 秒，但是堆大小在这里没有太大变化(它一直占用大约 150MB)。

这里的调谐点是将所有这些停顿保持在较低的水平。 为了保持较低的暂停时间，您可能需要通过-XX：NewSize 和 `-XX:MaxNewSize`JVM 标志调整年轻一代空间的大小，以便将它们设置为相对较小的值(例如，高达几百 MB)。 如果服务器的 CPU 能力更强，我们建议您通过设置 `-XX:+UseParNewGC`选项来使用并行新收集器。 您可能还希望通过 `-XX:ParallelGCThreads`JVM 标志调优年轻一代的并行 GC 线程数。

我们建议将上述设置添加到 `HBASE_REGIONSERVER_OPTS`变量，而不是 `hbase-env.sh`文件中的 `HBASE_OPTS`变量。 `HBASE_REGIONSERVER_OPTS`变量只影响区域服务器进程，这很好，因为 HBase 主服务器既不处理繁重的任务，也不参与数据进程。

对于老一辈人来说，并发收集(CMS)一般不会加速，但可以更早开始。 当旧一代中分配的空间百分比超过阈值时，CMS 开始运行。 此阈值由收集器自动计算。 对于某些情况，特别是在加载期间，如果 CMS 启动太晚，HBase 可能会运行完整的垃圾回收。 为了避免这种情况，我们建议设置 `-XX:CMSInitiatingOccupancyFraction`JVM 标志，以明确指定 CMS 的启动百分比，就像我们在步骤 3 中所做的那样。从 60%或 70%开始是一种良好的做法。 当对老一代使用 CMS 时，默认的年轻一代 GC 将被设置为并行新收集器。

## 还有更多...

如果您使用的是 0.92 之前的 HBase 版本，请考虑启用 MemStore-Local 分配缓冲区，以防止在繁重的写入负载下出现老式堆碎片：

```scala
$ vi $HBASE_HOME/conf/hbase-site.xml
<property>
<name>hbase.hregion.memstore.mslab.enabled</name>
<value>true</value>
</property>

```

此功能在 HBase 0.92 中默认启用。

## 另请参阅

*   *设置 Ganglia 以监视 HBase 群集*配方，请参见[第 5 章](05.html "Chapter 5. Monitoring and Diagnosis")，*监视和诊断*

# 使用压缩

HBase 最重要的特性之一是使用数据压缩。 这很重要，因为：

*   压缩减少了写入 HDFS/从 HDFS 读取的字节数
*   节省磁盘使用量
*   提高从远程服务器获取数据时的网络带宽效率

HBase 支持 GZip 和 LZO 编解码器。 我们建议使用 LZO 压缩算法，因为它数据解压速度快，CPU 使用率低。 由于系统首选较好的压缩比，您应该考虑 GZip。

不幸的是，由于许可证问题，HBase 不能与 LZO 一起发布。 HBase 是 Apache 许可的，而 LZO 是 GPL 许可的。 因此，我们需要自己安装 LZO。 我们将使用 hadoop-lzo 库，它为 Hadoop 带来了可拆分的 LZO 压缩。

在本指南中，我们将介绍如何安装 LZO 以及如何配置 HBase 以使用 LZO 压缩。

## 做好准备

确保要在其上构建 Hadoop-lzo 的计算机上安装了 Java。

从源代码构建 Hadoop-lzo 需要 Apache Ant。 通过运行以下命令安装 Ant：

```scala
$ sudo apt-get -y install ant

```

群集中的所有节点都需要安装本机 LZO 库。 您可以使用以下命令进行安装：

```scala
$ sudo apt-get -y install liblzo2-dev

```

## 怎么做……

我们将使用 Hadoop-lzo 库将 LZO 压缩支持添加到 HBase：

1.  从[https://github.com/toddlipcon/hadoop-lzo](http://https://github.com/toddlipcon/hadoop-lzo)获取最新的 hadoop-lzo 源代码。
2.  从源代码构建本机和 Java Hadoop-lzo 库。 根据您的操作系统，您应该选择构建 32 位或 64 位二进制文件。 例如，要构建 32 位二进制文件，请运行以下命令：

    ```scala
    $ export JAVA_HOME="/usr/local/jdk1.6"
    $ export CFLAGS="-m32"
    $ export CXXFLAGS="-m32"
    $ cd hadoop-lzo
    $ ant compile-native
    $ ant jar

    ```

    *   这些命令将创建 hadoop-lzo/build/ative 目录和 hadoop-lzo/build/hadoop-lzo-x.y.z.jar 文件。 要构建 64 位二进制文件，只需将 CFLAGS 和 CXXFLAGS 的值更改为-m64 即可。
3.  将构建的库复制到主节点上的 `$HBASE_HOME/lib`和 `$HBASE_HOME/lib/native`目录：

    ```scala
    hadoop@master1$ cp hadoop-lzo/build/hadoop-lzo-x.y.z.jar $HBASE_HOME/lib
    hadoop@master1$ mkdir $HBASE_HOME/lib/native/Linux-i386-32
    hadoop@master1$ cp hadoop-lzo/build/native/Linux-i386-32/lib/* $HBASE_HOME/lib/native/Linux-i386-32/

    ```

    *   对于 64 位操作系统，将 linux-i386-32(在上一步中)更改为 linux-amd64-64。
4.  将 `hbase.regionserver.codecs`的配置添加到您的 `hbase-site.xml`文件：

    ```scala
    hadoop@master1$ vi $HBASE_HOME/conf/hbase-site.xml
    <property>
    <name>hbase.regionserver.codecs</name>
    <value>lzo,gz</value>
    </property>

    ```

5.  在群集中同步 `$HBASE_HOME/conf`和 `$HBASE_HOME/lib`目录。
6.  HBase 附带了一个测试压缩设置是否正确的工具。 使用此工具测试群集的每个节点上的 LZO 设置。 如果一切配置正确，您将得到 `SUCCESS`输出：

    ```scala
    hadoop@client1$ $HBASE_HOME/bin/hbase org.apache.hadoop.hbase.util.CompressionTest /tmp/lzotest lzo
    12/03/11 11:01:08 INFO hfile.CacheConfig: Allocating LruBlockCache with maximum size 249.6m
    12/03/11 11:01:08 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
    12/03/11 11:01:08 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev Unknown build revision]
    12/03/11 11:01:08 INFO compress.CodecPool: Got brand-new compressor
    12/03/11 11:01:18 INFO compress.CodecPool: Got brand-new decompressor
    SUCCESS

    ```

7.  通过创建带有 LZO 压缩的表来测试配置，并在 HBase Shell 中进行验证：

    ```scala
    $ hbase> create 't1', {NAME => 'cf1', COMPRESSION => 'LZO'}
    $ hbase> describe 't1'
    DESCRIPTION ENABLED
    {NAME => 't1', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', true
    REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'LZO',
    MIN_VERSIONS => '0', TTL => '2147483647', BLOCKSIZE => '65536', IN
    _MEMORY => 'false', BLOCKCACHE => 'true'}]}
    1 row(s) in 0.0790 seconds

    ```

## 它是如何工作的.

我们构建了 Hadoop-lzo Java 和本地库，并将它们分别安装在 `$HBASE_HOME/lib`和 `$HBASE_HOME/lib/native`目录下。 通过添加 LZO 压缩支持，HBase StoreFiles(HFiles)将在写入数据块时使用 LZO 压缩。 HBase 使用本机 LZO 库执行压缩，而本机库由 HBase 通过我们构建的 Hadoop-lzo Java 库加载。

为了避免在启动节点时丢失或安装错误的编解码器，我们将 LZO 添加到 `hbase-site.xml`文件的 `hbase.regionserver.codecs`设置中。 如果 LZO 安装不正确，此设置将导致区域服务器启动失败。 如果您看到诸如“无法加载本机 GPL 库”之类的日志，则说明 LZO 安装有问题。 为了修复它，请确保安装了本机 LZO 库并正确配置了路径。

压缩算法是以每个列族为基础指定的。 如步骤 7 所示，我们创建了一个表 `t1`，其中包含一个使用 LZO 压缩的列族 `cf1`。

虽然 LZO 增加了读取时间的损失，因为数据块在读取时可能会被解压缩，但作为实时压缩库，LZO 的速度已经足够快了。 我们建议使用 LZO 作为生产 HBase 中的默认压缩算法。

## 还有更多...

另一个压缩选项是使用最近发布的 Snappy 压缩库和 Hadoop Snappy 集成。 由于设置与我们之前所做的基本相同，因此我们将跳过细节。 查看以下 URL，了解如何将快速压缩添加到 HBase：

[http：//hbase.apache.org/book.html#snappy.compression](http://hbase.apache.org/book.html#snappy.compression)

# 管理压缩

HBase 表具有以下物理存储结构：

![Managing compactions](img/7140_08_05.jpg)

它由多个区域组成。 虽然一个区域可能有多个商店，但每个商店都有一个柱族。 编辑首先写入宿主区域存储区的内存空间，称为 MemStore。 当 MemStore 的大小达到阈值时，它会刷新到 HDFS 上的 StoreFiles。

随着数据量的增加，HDFS 上可能会有很多 StoreFiles，这不利于其性能。 因此，HBase 会自动选取几个较小的 StoreFiles，并将它们重写为较大的 StoreFiles。 这一过程被称为次要压实。 对于某些情况，或者当由配置的时间间隔触发时(默认情况下为每天一次)，主要压缩会自动运行。 重大压缩将丢弃已删除或过期的单元格，并将 Store 中的所有 StoreFiles 重写为单个 StoreFile；这通常会提高性能。

但是，由于主要压缩会重写存储区的所有数据，因此在此过程中可能会发生大量磁盘 I/O 和网络流量。 这在重载系统上是不可接受的。 您可能希望在较低的系统加载时间运行它。

在本食谱中，我们将介绍如何关闭此自动主要压缩功能，并手动运行它。

## 做好准备

以启动群集的用户身份登录到您的 HBase 主服务器。

## 怎么做……

以下步骤介绍如何禁用自动主要压缩：

1.  将以下内容添加到 `hbase-site.xml`文件：

    ```scala
    $ vi $HBASE_HOME/conf/hbase-site.xml
    <property>
    <name>hbase.hregion.majorcompaction</name>
    <value>0</value>
    </property>

    ```

2.  跨群集中同步更改并重新启动 HBase。
3.  使用前述设置，将禁用自动主要压缩；您现在需要显式运行它。
4.  要通过 HBase Shell 在特定区域手动运行主要压缩，请运行以下命令：

    ```scala
    $ echo "major_compact 'hly_temp,,1327118470453.5ef67f6d2a792fb0bd737863dc00b6a7.'" | $HBASE_HOME/bin/hbase shell
    HBase Shell; enter 'help<RETURN>' for list of supported commands.
    Type "exit<RETURN>" to leave the HBase Shell
    Version 0.92.0, r1231986, Tue Jan 17 02:30:24 UTC 2012
    major_compact 'hly_temp,,1327118470453.5ef67f6d2a792fb0bd737863dc00b6a7.'
    0 row(s) in 1.7070 seconds

    ```

## 它是如何工作的.

`hbase.hregion.majorcompaction`属性指定一个区域中所有 StoreFiles 的主要压缩之间的时间间隔(以毫秒为单位)。 默认值为 `86400000`，表示一天一次。 我们在步骤 1 中将其设置为 `0`，以禁用自动主要压缩。 这将防止在高负载期间运行主要压缩，例如，当 MapReduce 作业在 HBase 集群上运行时。

另一方面，为了提高性能，需要进行较大的压实。 在步骤 4 中，我们展示了如何通过 HBase Shell 在特定区域手动触发重大压缩的示例。 在本例中，我们向 `major_compact`命令传递了一个区域名称，以便仅在单个区域上调用主要压缩。 通过将表名传递给命令，还可以对表的所有区域运行主要压缩。 `major_compact`命令将指定的表或区域排队以进行主要压缩；这将由托管它们的区域服务器在后台执行。

正如我们前面提到的，您可能只想在较低的加载时间内手动执行主要压缩。 这可以通过从 cron 作业调用 `major_compact`命令轻松完成。

## 还有更多...

调用主要压缩的另一种方法是使用 `org.apache.hadoop.hbase.client.HBaseAdmin`类提供的 `majorCompact`API。 在 Java 中很容易调用此 API，因此您可以从 Java 管理复杂的主要压缩调度。

# 管理区域拆分

通常，HBase 表从单个区域开始。 但是，随着数据的不断增长和区域达到其配置的最大大小，它会自动分为两部分，以便它们可以处理更多数据。 下图显示了 HBase 区域拆分：

![Managing a region split](img/7140_08_06.jpg)

这是 HBase 区域拆分的默认行为。 这种机制在许多情况下都工作得很好，但也有遇到问题的情况，例如分裂/压实风暴问题。

由于数据分布和增长大致一致，最终表中的所有区域都需要同时拆分。 拆分后，将立即对子区域运行压缩，以将其数据重写到单独的文件中。 这会导致大量的磁盘 I/O 和网络流量。

为了避免这种情况，您可以关闭自动拆分并手动调用它。 由于您可以控制何时调用拆分，因此有助于分散 I/O 负载。 另一个优点是，手动拆分可以让您更好地控制区域，从而帮助您跟踪和修复与区域相关的问题。

在本菜谱中，我们将介绍如何关闭自动区域拆分并手动调用它。

## 做好准备

以启动群集的用户身份登录到您的 HBase 主服务器。

## 怎么做……

要关闭自动区域拆分并手动调用它，请执行以下步骤：

1.  将以下内容添加到 `hbase-site.xml`文件：

    ```scala
    $ vi $HBASE_HOME/conf/hbase-site.xml
    <property>
    <name>hbase.hregion.max.filesize</name>
    <value>107374182400</value>
    </property>

    ```

2.  跨群集中同步更改并重新启动 HBase。
3.  使用上述设置，在区域大小达到配置的 100 GB 阈值之前，不会发生区域拆分。 您需要在选定的区域显式触发它。
4.  要运行通过 HBase Shell 拆分的区域，请使用以下命令：

    ```scala
    $ echo "split 'hly_temp,,1327118470453.5ef67f6d2a792fb0bd737863dc00b6a7.'" | $HBASE_HOME/bin/hbase shell
    HBase Shell; enter 'help<RETURN>' for list of supported commands.
    Type "exit<RETURN>" to leave the HBase Shell
    Version 0.92.0, r1231986, Tue Jan 17 02:30:24 UTC 2012
    split 'hly_temp,,1327118470453.5ef67f6d2a792fb0bd737863dc00b6a7.'
    0 row(s) in 1.6810 seconds

    ```

## 它是如何工作的.

`hbase.hregion.max.filesize`属性以*字节*指定最大区域大小。 默认情况下，该值为 1 GB(对于 HBase 0.92 之前的版本，为 256MB)，这意味着当一个区域超过此大小时，它将被一分为二。 在步骤 1 中，我们将此最大区域大小设置为 100 GB，这是一个非常高的数字。

由于在区域达到 100 GB 上限之前不会发生拆分，因此我们需要显式调用它。 在步骤 4 中，我们使用 `split`命令通过 HBase Shell 调用指定区域的拆分。

别忘了分割大区域。 区域是 HBase 中数据分发和平衡的基本单位。 区域应该分割成合适的大小，并在较低的加载时间。

另一方面，太多的分裂是不好的。 区域服务器上的区域太多会降低其性能。

您可能还希望在手动拆分区域后触发主要压缩和平衡。

## 还有更多...

我们之前设置的设置会导致整个群集的默认最大区域大小为 100 GB。 除了更改整个簇之外，还可以在创建表格时以列族为基础指定 `MAX_FILESIZE`属性：

```scala
$ hbase> create 't1', {NAME => 'cf1', MAX_FILESIZE => '107374182400'}

```

与主要压缩一样，您也可以使用 `org.apache.hadoop.hbase.client.HBaseAdmin`Java 类提供的 `split`API。

## 另请参阅

*   *使用自己的算法预制区域*配方，[第 9 章](09.html "Chapter 9. Advanced Configurations and Tuning")，*高级配置和调整*