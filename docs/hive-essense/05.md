# 五、数据操作

操纵数据的能力在大数据分析中至关重要。 操作数据是交换、移动、排序、转换和修改数据的过程。 此技术可用于许多情况，如清理数据、搜索模式、创建趋势等。 HQL 提供各种语句、关键字、运算符和函数来执行数据操作。

在本章中，我们将介绍以下主题：

*   使用`LOAD`、`INSERT`、`IMPORT`和`EXPORT`进行数据交换
*   数据排序
*   功能
*   事务和锁

# 与负载进行数据交换

为了移动数据，配置单元使用`LOAD`语句。 *Move*这里表示原始数据被移动到目标表/分区，不再存在于原始位置。 `LOAD`语句中的`LOCAL`关键字指定文件在客户端主机上的位置。 如果未指定`LOCAL`关键字，则默认情况下，将从`INPATH`之后指定的完整统一资源标识符(URI)(大多数情况下为`hdfs path`)或`hdfs-site.xml`中定义的`fs.default.name`属性的值加载文件。 `INPATH`之后的路径可以是相对路径，也可以是绝对路径。 该路径指向要加载的文件或文件夹(指文件夹中的所有文件)，但指定的路径中不允许有子文件夹。 如果将数据加载到分区表中，则必须指定分区列。 关键字`OVERWRITE`用于决定是否替换目标表/分区中的现有数据。以下是如何将数据从本地或 HDFS 文件移动到表或分区的示例：

1.  在内部或外部的表中加载本地数据。 LOAD 语句不可重复，因为要加载的文件已被移动：

```sql
 > LOAD DATA LOCAL INPATH
 > '/home/dayongd/Downloads/employee_hr.txt'
 > OVERWRITE INTO TABLE employee_hr;
 No rows affected (0.436 seconds)
```

2.  将本地数据加载到分区中：

```sql
 > LOAD DATA LOCAL INPATH
 > '/home/dayongd/Downloads/employee.txt'
 > OVERWRITE INTO TABLE employee_partitioned
 > PARTITION (year=2018, month=12);
 No rows affected (0.772 seconds)
```

3.  使用 URI 将数据从 HDFS 加载到表中：

```sql
      -- Use default fs path
 > LOAD DATA INPATH
 > '/tmp/hivedemo/data/employee.txt'
 > INTO TABLE employee; -- Without OVERWRITE, it appends data
 No rows affected (0.453 seconds)

      -- Use full URI
 > LOAD DATA INPATH
 > 'hdfs://localhost:9000/tmp/hivedemo/data/employee.txt'
 > OVERWRITE INTO TABLE employee;
 No rows affected (0.297 seconds)
```

# 与 INSERT 进行数据交换

要从表/分区中提取数据，可以使用关键字`INSERT`。 与其他关系数据库一样，配置单元支持通过从另一个表中选择数据来将数据插入到表中。 这是一种非常常见的 ETL(数据仓库中用于提取、转换和加载的术语)模式，用于从另一个表或数据集中填充现有的或新的表。 HQL 的`INSERT`语句与关系数据库的`INSERT`具有相同的语法。 然而，HQL 通过支持数据覆盖、多次插入、动态分区插入以及将数据插入到文件中，改进了它的`INSERT`语句。 以下是 HQL 中的几个`INSERT`语句示例：

1.  以下是`SELECT`语句中的常规`INSERT`：

```sql
-- Check the target table, which is empty.
 > SELECT name, work_place FROM employee;
 +-------------+-------------------+
 |employee.name|employee.work_place|
 +-------------+-------------------+
 +-------------+-------------------+
 No rows selected (0.115 seconds)

      -- Populate data from query while "INTO" will append data
 > INSERT INTO TABLE employee SELECT * FROM ctas_employee;
 No rows affected (31.701 seconds)

      -- Verify the data loaded
 > SELECT name, work_place FROM employee;
 +-------------+----------------------+
 |employee.name|  employee.work_place |
 +-------------+----------------------+
 | Michael     |["Montreal","Toronto"]|
 | Will        |["Montreal"]          |
 | Shelley     |["New York"]          |
 | Lucy        |["Vancouver"]         |
 +-------------+----------------------+
 4 rows selected (0.12 seconds)
```

2.  插入具有指定列的表。 对于未指定的列，填充`NULL`。 然而，目前有两个限制。 首先，它只适用于`INSERT INTO`，而不适用于`INSERT OVERWRITE`。 其次，未指定的列必须是主要的数组数据类型(如不支持数组)。 同样的限制也适用于以下`INSERT INTO ... VALUES`语句：

```sql
 > CREATE TABLE emp_simple( -- Create a test table only has 
primary types
 > name string,
 > work_place string
 > );
 No rows affected (1.479 seconds)

 > INSERT INTO TABLE emp_simple (name) -- Specify which columns to 
      insert
 > SELECT name FROM employee WHERE name = 'Will';
 No rows affected (30.701 seconds)

 > INSERT INTO TABLE emp_simple VALUES -- Insert constant values
 > ('Michael', 'Toronto'),('Lucy', 'Montreal');
 No rows affected (18.045 seconds)

 > SELECT * FROM emp_simple; -- Verify the data loaded
 +---------+------------+
 | name    | work_place |
 +---------+------------+
 | Will    | NULL       | -- NULL when column is not specified
 | Michael | Toronto    |
 | Lucy    | Montreal   |
 +---------+------------+
 3 rows selected (0.263 seconds)
```

3.  从 CTE 语句插入数据：

```sql
 > WITH a as (
 > SELECT * FROM ctas_employee 
 > )
 > FROM a 
 > INSERT OVERWRITE TABLE employee
 > SELECT *;
 No rows affected (30.1 seconds)
```

4.  通过只扫描源表一次来运行多次插入，以获得更好的性能：

```sql
 > FROM ctas_employee
 > INSERT OVERWRITE TABLE employee
 > SELECT *
 > INSERT OVERWRITE TABLE employee_internal
 > SELECT * 
 > INSERT OVERWRITE TABLE employee_partitioned 
 > PARTITION (year=2018, month=9) -- Insert to static partition
 > SELECT *
 > ; 
 No rows affected (27.919 seconds)
```

The `INSERT OVERWRITE` statement will replace the data in the target table/partition, while `INSERT INTO` will append data.

在将数据插入分区时，我们需要指定分区列。 与指定静态分区值不同，配置单元还支持动态提供分区值。 当需要从数据值动态填充分区时，动态分区非常有用。 默认情况下禁用动态分区，因为不小心插入动态分区可能会意外创建许多分区。*我们必须设置以下属性才能启用动态分区：

```sql
> SET hive.exec.dynamic.partition=true;
No rows affected (0.002 seconds)
```

默认情况下，用户必须至少指定一个静态分区列。 这是为了避免意外覆盖分区。 要禁用此限制，我们可以在插入到动态分区之前将分区模式从默认的`strict`模式设置为`nonstrict`，如下所示：

```sql
> SET hive.exec.dynamic.partition.mode=nonstrict;
No rows affected (0.002 seconds) 
-- Partition year, month are determined from data
> INSERT INTO TABLE employee_partitioned
> PARTITION(year, month)
> SELECT name, array('Toronto') as work_place,
> named_struct("gender","Male","age",30) as gender_age,
> map("Python",90) as skills_score,
> map("R&D",array('Developer')) as depart_title, 
> year(start_date) as year, month(start_date) as month
> FROM employee_hr eh
> WHERE eh.employee_id = 102;
No rows affected (29.024 seconds)
```

Complex type constructors are used in the preceding example to create a constant value of a complex data type.

`INSERT`还支持将数据写入文件，与`LOAD`相比，这是相反的操作。 它通常用于将数据从`SELECT`语句提取到本地/hdfs 目录中的文件。 但是，它只支持`OVERWRITE`关键字，这意味着我们只能覆盖而不能向数据文件追加数据。 默认情况下，在导出的文件中，列由 C*Trl*+*A*分隔，行由换行符分隔。 列分隔符、行分隔符和集合分隔符也可以像在表创建语句中那样被覆盖。 以下是使用`INSERT OVERWRITE ... directory`语句将数据导出到文件的几个示例：

1.  我们可以使用默认行分隔符插入到本地文件：

```sql
 > INSERT OVERWRITE LOCAL DIRECTORY '/tmp/output1'
 > SELECT * FROM employee;
 No rows affected (30.859 seconds)
```

Many partial files could be created by reducers when doing an insert into a directory. To merge them into one file, we can use the HDFS merge command: `hdfs dfs –getmerge <exported_hdfs_folder> <local_folder>`.

2.  使用指定的行分隔符插入到本地文件中：

```sql
 > INSERT OVERWRITE LOCAL DIRECTORY '/tmp/output2'
 > ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
 > SELECT * FROM employee;
 No rows affected (31.937 seconds)

      -- Verify the separator
 $vi /tmp/output2/000000_0
 Michael,Montreal^BToronto,Male^B30,DB^C80,
      Product^CDeveloper^DLead
 Will,Montreal,Male^B35,Perl^C85,Product^CLead^BTest^CLead
 Shelley,New York,Female^B27,Python^C80,Test^CLead^BCOE^CArchitect
 Lucy,Vancouver,Female^B57,Sales^C89^BHR^C94,Sales^CLead
```

3.  使用 MULTI-INSERT 语句从同一表导出数据：

```sql
 > FROM employee
 > INSERT OVERWRITE DIRECTORY '/user/dayongd/output3'
 > SELECT *
 > INSERT OVERWRITE DIRECTORY '/user/dayongd/output4'
 > SELECT name ;
 No rows affected (25.4 seconds)
```

Combined HQL and HDFS shell commands, we can extract data to local or remote files with both append and overwrite supported. The `hive -e quoted_hql_string` or `hive -f <hql_filename>` commands can execute a HQL query or query file. Linux's redirect operators and piping can be used with these commands to redirect result sets. The following are a few examples:

*   **附加到本地文件**：**和`$hive -e 'select * from employee' >> test`**
*   **覆盖本地文件**：**和`$hive -e 'select * from employee' > test`**
*   **附加到 HDFS 文件**：**和`$hive -e 'select * from employee'|hdfs dfs -appendToFile - /tmp/test1 `**
*   **覆盖 HDFS 文件**：**`$hive -e 'select * from employee'|hdfs dfs -put -f - /tmp/test2`**

# 与[EX|IM]端口进行数据交换

在进行数据迁移或版本部署时，我们可能需要在不同的环境或群集之间移动数据。 在 HQL 中，`EXPORT`和`IMPORT`语句可用于在不同环境或集群中的 HDF 之间移动数据。 `EXPORT`语句从表或分区中导出数据和元数据。 元数据在名为`_metadata`的文件中导出。 数据导出到名为 data 的子目录中，如下所示：

```sql
> EXPORT TABLE employee TO '/tmp/output5';
No rows affected (0.19 seconds)

> dfs -ls -R /tmp/output5/;
+--------------------------------+
|           DFS Output           |
+--------------------------------+
| ... /tmp/output5/_metadata     |
| ... /tmp/output5/data          |
| ... /tmp/output5/data/000000_0 |
+--------------------------------+
3 rows selected (0.014 seconds)
```

For `EXPORT`, the database name can be used before the table name without any syntax error, but the database is useless and ignored by the `IMPORT` statement.

导出后，我们可以手动或使用命令`hadoop distcp <srcurl> <desturl>`将导出的文件复制到其他群集。 然后，我们可以通过以下方式导入数据：

1.  将数据导入到新表中。 如果该表存在，它将抛出一个错误：

```sql
 > IMPORT TABLE FROM '/tmp/output5'; -- By default, use exported 
      name
 Error: Error while compiling statement: FAILED: SemanticException 
      [Error 10119]: Table exists and contains data files 
      (state=42000,code=10119)

 > IMPORT TABLE empolyee_imported -- Specify a table imported
 > FROM '/tmp/output5';
 No rows affected (0.788 seconds)
```

2.  将数据导入外部表，其中`LOCATION`属性是可选的**：**

```sql
 > IMPORT EXTERNAL TABLE empolyee_imported_external
 > FROM '/tmp/output5'
 > LOCATION '/tmp/output6';
 No rows affected (0.256 seconds)
```

3.  导出和导入分区：

```sql
 > EXPORT TABLE employee_partitioned partition
 > (year=2018, month=12) TO '/tmp/output7';
 No rows affected (0.247 seconds)

 > IMPORT TABLE employee_partitioned_imported
 > FROM '/tmp/output7';
 No rows affected (0.14 seconds)
```

# 数据排序

处理数据的另一个方面是对数据进行适当的排序，以便清楚地识别重要事实，例如前*N*值、最大值、最小值等。 HQL 支持以下关键字进行数据排序：

1.  **`ORDER BY [ASC|DESC]`**：类似于 SQL 语句`ORDER BY`。 使用`ORDER BY`*时，*会在每个减速器的所有输出上维护排序顺序。 它只使用一个减法器执行全局排序，因此返回结果需要更长的时间。 `ORDER BY`之后的方向说明符可以是`ASC`，表示上升(从低到高)，也可以是`DESC`，表示下降(从高到低)。 如果不提供方向说明符，则使用缺省升序。从 v2.1.0 开始，`ORDER BY`语句支持指定`NULL`值的排序方向，如`NULL FIRST`或`NULL LAST`。 默认情况下，`NULL`在`ASC`方向保持在第一位，在`DESC`方向保持在最后一位：

```sql
 > SELECT name FROM employee ORDER BY name DESC; -- By columns
 +----------+
 |   name   |
 +----------+
 | Will     |
 | Shelley  |
 | Michael  |
 | Lucy     |
 +----------+
 4 rows selected (24.057 seconds)

 > SELECT name 
 > FROM employee -- Order by expression
 > ORDER BY CASE WHEN name = 'Will' THEN 0 ELSE 1 END DESC;
 +----------+
 |   name   |
 +----------+
 | Lucy     |
 | Shelley  |
 | Michael  |
 | Will     |
 +----------+
 4 rows selected (25.057 seconds)

 > SELECT * FROM emp_simple ORDER BY work_place NULL LAST;
 +---------+------------+
 | name    | work_place |
 +---------+------------+
 | Lucy    | Montreal   |
 | Michael | Toronto    |
 | Will    | NULL       | -- NULL stays at the last
 +---------+------------+
 3 rows selected (0.263 seconds)
```

Using `LIMIT`with `ORDER BY`is strongly recommended. When the `hive.mapred.mode = strict`property is set(the default value for` hive.mapred.mode` is nonstrict in `Hive v1.*` and strict in `Hive v2.*`), it throws exceptions when using `ORDER BY` without `LIMIT`*. *

2.  `SORT BY [ASC|DESC]`：它指定使用哪些列对减速器输入记录进行排序。 这意味着在将数据发送到减速器之前，分类工作尚未完成。 `SORT BY`语句不会执行全局排序(但`ORDER BY`会执行)，并且只确保数据在每个归约器中进行本地排序。 如果`SORT BY`仅与一个减速器(`setmapred.reduce.tasks=1`)排序，则等于`ORDER BY`*，*，如下例所示。 大多数情况下，`SORT BY`本身没有用处，但与下面介绍的`DISTRIBUTE BY`*、*一起使用：

```sql
 > SET mapred.reduce.tasks = 2; -- Sort by with more than 1 reducer No rows affected (0.001 seconds)

 > SELECT name FROM employee SORT BY name DESC;
 +---------+
 |   name  |
 +---------+
 | Shelley | -- Once result is collected to client, it is 
     | Michael | order-less
 | Lucy    |
 | Will    |
 +---------+
 4 rows selected (54.386 seconds)

 > SET mapred.reduce.tasks = 1; -- Sort by one reducer
 No rows affected (0.002 seconds)

 > SELECT name FROM employee SORT BY name DESC;
 +----------+
 |   name   |
 +----------+
 | Will     | -- Same result to ORDER BY
 | Shelley  |
 | Michael  |
 | Lucy     |
 +----------+
 4 rows selected (46.03 seconds)
```

3.  `DISTRIBUTE BY`：它与`GROUP BY`非常相似(在[第 6 章](06.html)*，数据聚合和采样*)***和***中介绍，当映射器决定它可以将输出传递到哪个减速器时。 与`GROUP BY`*相比，*`DISTRIBUTE BY`不会处理数据聚合，例如`count(*)`，而只是指示数据的去向*。* 在这种情况下，通常使用*`DISTRIBUTE BY`按指定的列重新组织文件中的数据。 例如，*，*我们可能需要在一个新的`UNION`结果集之后使用*和*`DISTRIBUTE BY`来以更高的粒度重新组织数据。当与`SORT BY`*和*一起使用对指定组内的数据进行排序时，在一个查询中，可以在`SORT BY`之前使用`DISTRIBUTE BY`。 此外，`DISTRIBUTE BY`*和*之后的列必须出现在选择列列表中，如下所示：*

```sql
      -- Error when not specify distributed column employee_id in 
      select
 > SELECT name FROM employee_hr DISTRIBUTE BY employee_id; 
 Error: Error while compiling statement: FAILED: SemanticException 
      [Error 10004]: Line 1:44 Invalid table alias or column reference 
      'employee_id': (possible column names are: name) 

 > SELECT name, employee_id FROM employee_hr DISTRIBUTE BY 
      employee_id; 
 +----------+--------------+
 |   name   | employee_id  |
 +----------+--------------+
 | Lucy     | 103          |
 | Steven   | 102          |
 | Will     | 101          |
 | Michael  | 100          |
 +----------+--------------+
 4 rows selected (38.92 seconds)

      -- Used with SORT BY to order name started on the same day
 > SELECT name, start_date
 > FROM employee_hr
 > DISTRIBUTE BY start_date SORT BY name;
 +----------+--------------+
 |   name   |  start_date  |
 +----------+--------------+
 | Lucy     | 2010-01-03   |
 | Michael  | 2014-01-29   |
 | Steven   | 2012-11-03   |
 | Will     | 2013-10-02   |
 +----------+--------------+
 4 rows selected (38.01 seconds)
```

4.  `CLUSTER BY`：它是一个快捷运算符，可用于对同一组列执行`DISTRIBUTE BY`和`SORT BY`操作。 `CLUSTER BY`语句还不允许您指定`ASC`或`DESC`。 与全局排序的`ORDER BY`相比，`CLUSTER BY`语句对每个分布式组中的数据进行排序：

```sql
 > SELECT name, employee_id FROM employee_hr CLUSTER BY name;
 +----------+--------------+
 |   name   | employee_id  |
 +----------+--------------+
 | Lucy     | 103          |
 | Michael  | 100          |
 | Steven   | 102          |
 | Will     | 101          |
 +----------+--------------+
 4 rows selected (39.791 seconds)
```

When we have to do a global sort, we can do `CLUSTER BY` first and then `ORDER BY`*.* In this way, we can fully utilize all the available reducers ahead of `ORDER BY` and have better performance, for example: `SELECT * FROM (SELECT * FROM employee CLUSTER BY name) base ORDER BY name;`.

总而言之，这些排序关键字之间的区别如下图所示：

![](img/c0b1d764-fded-4d53-92e6-f03588921402.png)

HQL sorting keywords difference

# 功能

为了进一步操作数据，我们还可以使用 HQL 中的运算符、表达式、公式和函数来转换数据。 配置单元维基([https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF))提供了所有支持的表达式和函数的规范，因此除了本章中的几个重要提示外，我们不想在此重复所有这些规范。

HIVE 定义了关系运算符、算术运算符、逻辑运算符、复杂类型运算符和复杂类型运算符构造函数。 对于关系运算符、算术运算符和逻辑运算符，它们类似于 SQL/Java 中的标准运算符。 对于复杂数据类型上的运算符，我们已经在[第 3 章](03.html)、*数据定义和描述*的*了解配置单元数据类型*一节中介绍了它们，以及本章前面介绍的将数据插入动态分区的示例。 HQL 中的功能分类如下：

*   **数学函数**：主要用于进行数学计算，如`rand(...)`、`pi(...)`
*   **集合函数**：它们用于查找复杂类型的大小、键和值，如`size(...)`
*   **类型转换函数**：主要是将一种类型转换为另一种类型的`cast(...)`函数和`binary(...)`函数
*   **日期函数**：它们用于执行与日期相关的计算，如`year(...)`和`month(...)`
*   **条件函数**：它们用于检查具有定义的返回值的特定条件，如`coalesce(...)`、`if(...)`和 Case When Then`else`End
*   **字符串函数**：它们用于执行与字符串相关的操作，如`upper(...)`和`trim(...)`
*   **聚合函数**：它们用于执行聚合(将在下一章中介绍)，例如`sum(...)`和*和*`count(*)`
*   **表格生成函数**：这些函数将单个输入行转换为多个输出行，如`explode(...)`和`json_tuple(...)`
*   **定制函数**：这些函数由 Java 作为扩展创建，并在[章](08.html)*，可扩展性考虑事项*中介绍

要列出所有运算符、内置函数和用户定义函数，可以使用以下`SHOW FUNCTIONS`命令。 有关特定函数的更多详细信息，我们可以使用`DESC [EXTENDED] function_name`，如下所示：

```sql
> SHOW FUNCTIONS; -- List all functions
> DESCRIBE FUNCTION <function_name>; -- Detail for the function
> DESCRIBE FUNCTION EXTENDED <function_name>; -- More details
```

以下是结合示例使用 HQL 函数的一些提示和最佳实践。

# 集合的函数提示

`size(...)`函数用于计算`MAP`、`ARRAY`或嵌套的`MAP/ARRAY`的集合大小。 如果集合为`NULL`，则返回`-1`，如果集合为空，则返回`0`，如下所示：

```sql
> SELECT 
> SIZE(work_place) as array_size,
> SIZE(skills_score) as map_size,
> SIZE(depart_title) as complex_size,
> SIZE(depart_title["Product"]) as nest_size
> FROM employee;
+-------------+-----------+---------------+------------+
| array_size  | map_size  | complex_size  | nest_size  |
+-------------+-----------+---------------+------------+
| 2           | 1         | 1             | 2          |
| 1           | 1         | 2             | 1          |
| 1           | 1         | 2             | -1         |
| 1           | 2         | 1             | -1         |
+-------------+-----------+---------------+------------+
4 rows selected (0.062 seconds)

> SELECT size(null), size(array(null)), size(array());
+-----+-----+-----+
| _c0 | _c1 | _c2 |
+-----+-----+-----+
| -1  |  1  |  0  |
+-----+-----+-----+
1 row selected (11.453 seconds)
```

`array_contains(...)`函数检查数组是否包含一些值，并返回`TRUE`或`FALSE`。 函数的作用是：按升序对数组进行排序。 这些选项可按如下方式使用：

```sql
> SELECT 
> array_contains(work_place, 'Toronto') as is_Toronto,
> sort_array(work_place) as sorted_array
> FROM employee;
+-------------+-------------------------+
| is_toronto  |      sorted_array       |
+-------------+-------------------------+
| true        | ["Montreal","Toronto"]  |
| false       | ["Montreal"]            |
| false       | ["New York"]            |
| false       | ["Vancouver"]           |
+-------------+-------------------------+
4 rows selected (0.059 seconds)
```

# 日期和字符串的函数提示

函数的作用是：从日期中删除小时、分钟和秒。 当我们需要检查 Date/Time 类型列的值是否在数据范围内(例如 2014-11-01 和 2014-11-31 之间的`to_date(update_datetime)`)时，这很有用。 `to_date(...)`的用法如下：

```sql
> SELECT TO_DATE(FROM_UNIXTIME(UNIX_TIMESTAMP())) as currentdate;
+---------------+
| currentdate   |
+---------------+
| 2018-05-15    |
+---------------+
1 row selected (0.153 seconds)
```

函数的作用是：颠倒字符串中每个字母的顺序。 函数的作用是：使用指定的标记器对字符串进行标记化。 下面是使用这两个参数从路径中获取文件名的示例：

```sql
> SELECT
> reverse(split(reverse('/home/user/employee.txt'),'/')[0])
> as linux_file_name;
+------------------+
| linux_file_name  |
+------------------+
| employee.txt     |
+------------------+
1 row selected (0.1 seconds)
```

`explode(...)`将数组或映射中的每个元素输出为单独的行，而`collect_set(...)`和`collect_list(...)`则相反，它们返回每个组的元素集/列表。 `collect_set(...)`语句将从结果中删除重复项，但`collect_list(...)`不会：

```sql
> SELECT 
> collect_set(gender_age.gender) as gender_set,
> collect_list(gender_age.gender) as gender_list
> FROM employee;
+-------------------+-----------------------------------+
| gender_set        | gender_list                       |
+-------------------+-----------------------------------+
| ["Male","Female"] | ["Male","Male","Female","Female"] |
+-------------------+-----------------------------------+
1 row selected (24.488 seconds)
```

# 虚拟列函数

虚拟列是 HQL 中的特殊功能。 现在，有两个虚拟列：`INPUT__FILE__NAME`和`BLOCK__OFFSET__INSIDE__FILE`。 函数`INPUT__FILE__NAME`显示映射器任务的输入文件名。`BLOCK__OFFSET__INSIDE__FILE`函数显示当前全局文件位置或当前块的文件偏移量(如果文件被压缩)。 以下是使用虚拟列找出数据在 HDFS 中的物理位置的示例，特别是对于存储桶表和分区表：

```sql
> SELECT 
> INPUT__FILE__NAME,BLOCK__OFFSET__INSIDE__FILE as OFFSIDE
> FROM employee;
+-----------------------------------------------------------------------+
| input__file__name                                           | offside |
+-----------------------------------------------------------------------+
| hdfs://localhost:9000/user/hive/warehouse/employee/000000_0 | 0       |
| hdfs://localhost:9000/user/hive/warehouse/employee/000000_0 | 62      |
| hdfs://localhost:9000/user/hive/warehouse/employee/000000_0 | 115     |
| hdfs://localhost:9000/user/hive/warehouse/employee/000000_0 | 176     |
+-------------------------------------------------------------+---------+
4 rows selected (0.47 seconds)
```

# 事务和锁

**ACID**属性(原子性、一致性、数据隔离性和持久性)是人们期待已久的配置单元特性，它为关系数据库奠定了基础；从配置单元 0.14.0 版开始就可以使用了。 配置单元中的完全 ACID 支持是通过行级事务和锁实现的。这使得配置单元能够处理并发读写、数据清理、数据修改、复杂的 ETL/SCD(慢变维度)、流数据摄取、批量数据合并等用例。 在本节中，我们将更详细地介绍它们。

# 交易记录

目前，HQL 中的所有事务都是自动提交的，不像关系数据库那样支持**、**`BEGIN`、`COMMIT`和`ROLLBACK`。 此外，启用了事务功能的表必须是具有`ORC`文件格式的桶表。 要启用事务支持，必须在`hive-site.xml`或直线连接字符串中适当设置以下配置参数：

```sql
> SET hive.support.concurrency = true;
> SET hive.enforce.bucketing = true;
> SET hive.exec.dynamic.partition.mode = nonstrict;
> SET hive.txn.manager = 
> org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
> SET hive.compactor.initiator.on = true;
> SET hive.compactor.worker.threads = 1;
```

When a transaction is enabled, each transaction-related operation, such as `INSERT`, `UPDATE`, and `DELETE`, stores data in delta files. At read time, the reader merges the base and delta files, applying any updates and deletes. Both base and delta directory names contain the transaction IDs. Occasionally, these changes need to be merged into the base files by compactors, which are background processes in the `metastore`, for better performance and smaller file size. To see a list of all tables/partitions currently being compacted or scheduled for compaction, use the `SHOW COMPACTIONS` statement.

然后，创建一个在表属性中启用了事务的表，并填充数据：

```sql
> CREATE TABLE employee_trans (
> emp_id int,
> name string,
> start_date date,
> quit_date date,
> quit_flag string
> ) 
> CLUSTERED BY (emp_id) INTO 2 BUCKETS STORED as ORC
> TBLPROPERTIES ('transactional'='true'); -- Also need to set this No rows affected (2.216 seconds)

> INSERT INTO TABLE employee_trans VALUES 
> (100, 'Michael', '2017-02-01', null, 'N'),
> (101, 'Will', '2017-03-01', null, 'N'),
> (102, 'Steven', '2018-01-01', null, 'N'),
> (104, 'Lucy', '2017-10-01', null, 'N');
No rows affected (48.216 seconds)
```

对于启用了事务的表，我们可以对数据执行`UPDATE`、`DELETE`和`MERGE`操作。

# UPDATE 语句

`UPDATE`语句用于在满足特定条件时更新表中的一列或多列。 在这里，更新后的列不能对列或存储桶列进行分区。 用于更新的值应该是表达式或常量，而不是子查询：

```sql
> UPDATE employee_trans 
> SET quite_date = current_date, quit_flag = 'Y'
> WHERE emp_id = 104;
No rows affected (39.745 seconds)

> SELECT
> quit_date, quit_flag 
> FROM employee_trans 
> WHERE emp_id = 104; -- Verify the update
+-------------+-----------+
| quit_date   | quit_flag |
+-------------+-----------+
| 2018-04-20  | Y         |
+-------------+-----------+
1 row selected (0.325 seconds)
```

# DELETE 语句

`DELETE`语句用于在满足特定条件时从表中删除一行或多行，如下所示：

```sql
> DELETE FROM employee_trans WHERE emp_id = 104;
No rows affected (42.298 seconds)

-- Verify the result, deleted
> SELECT name FROM employee_trans WHERE emp_id = 104;
+------+
| name |
+------+
+------+
No rows selected (0.33 seconds)
```

# MERGE 语句

从配置单元 2.2 开始提供的`MERGE`语句用于根据与源表或查询匹配或不匹配的`JOIN`条件，对目标表执行`UPDATE`、`DELETE`或`INSERT`操作。 标准语法如下：

```sql
MERGE INTO <target_table> as Target USING <source_query/table> as Source
ON <join_condition between two tables>
WHEN MATCHED [AND <boolean expression>] THEN UPDATE SET <set clause list>
WHEN MATCHED [AND <boolean expression>] THEN DELETE
WHEN NOT MATCHED [AND <boolean expression>] THEN INSERT VALUES <value list>
```

当前对`MERGE INTO`语句的限制如下：

*   可能存在一个、两个或三个`WHEN`子句
*   每种类型的`UPDATE`/`DELETE`/`INSERT`最多只能使用一种
*   `WHEN NOT MATCHED`必须是最后一个子句，并且只支持`INSERT VALUES <value_list>`
*   `WHEN MATCHED`仅支持`UPDATE`或`DELETE`
*   如果同时存在`UPDATE`和`DELETE`子句，则语句中的第一个子句必须包含`[AND <boolean expression>]`

以下是在 HQL 中合并数据的示例：

```sql
-- Create another table as merge source > CREATE TABLE employee_update (
> emp_id int,
> name string,
> start_date date,
> quit_date date,
> quit_flag string
> );
No rows affected (0.127 seconds) -- Populate data > INSERT INTO TABLE employee_update VALUES 
> (100, 'Michael', '2017-02-01', '2018-01-01', 'Y'), -- People quit
> (102, 'Steven', '2018-01-02', null, 'N'), -- People has start_date update
> (105, 'Lily', '2018-04-01', null, 'N'); -- People newly started No rows affected (19.832 seconds) -- Do a data merge from employee_update to employee_trans > MERGE INTO employee_trans as tar USING employee_update as src
> ON tar.emp_id = src.emp_id
> WHEN MATCHED and src.quit_flag <> 'Y' THEN UPDATE SET start_date src.start_date
> WHEN MATCHED and src.quit_flag = 'Y' THEN DELETE
> WHEN NOT MATCHED THEN INSERT VALUES (src.emp_id, src.name, src.start_date, src.quit_date, src.quit_flag);
No rows affected (174.357 seconds)

SELECT * FROM employee_trans; -- Verify the result, Michael is deleted
+--------+--------+------------+-----------+-----------+
| emp_id | name   |start_date  | quit_date | quit_flag |
+--------+--------+------------+-----------+-----------+ 
| 102    | Steven | 2018-01-02 | NULL      | N         | -- Update
| 101    | Will   | 2017-03-01 | NULL      | N         |
| 105    | Lily   | 2018-04-01 | NULL      | N         | -- Insert
+--------+--------+------------+-----------+-----------+
3 rows selected (0.356 seconds)
```

在 HQL 中，可以使用`SHOW TRANSACTIONS`语句来显示系统中当前打开和中止的事务。 当我们运行前面的查询时，我们可以打开另一个配置单元连接并发出以下语句来查看当前事务：

```sql
> SHOW TRANSACTIONS;
+-----+-----+-------------+-----------------+-------+----------+
|txnid|state|startedtime  |lastheartbeattime|user   |host      |
+-----+-----+-------------+-----------------+-------+----------+
|2    |OPEN |1524183790000|1524183790000    |vagrant|vagrant.vm|
+-----+-----+-------------+-----------------+-------+----------+
2 rows selected (0.063 seconds)
```

The `ABORT TRANSACTIONS transaction_id` statement has been used to kill a transaction with a specified ID since Hive v2.1.0.

# 锁 / 扣住 / 过船闸 / 隐藏

锁可确保数据隔离，如酸原理所述。 从 v0.7.0 开始，hive 就支持并发访问和锁定机制，并在 v0.13.0 中更新为新的锁管理器。 提供了两种类型的锁，如下所示：

*   **共享锁：**也称为`S`锁，允许并发共享。 这是在读取表/分区时获取的。
*   **排他锁**：也称为`X`锁。 这是针对修改表/分区的所有其他操作获取的。

For partition tables, only a shared lock is acquired if the change is only applicable to the newly-created partitions. An exclusive lock is acquired on the table if the change is applicable to all partitions. In addition, an exclusive lock on the table globally affects all partitions. For more information regarding locks, see [https://cwiki.apache.org/confluence/display/Hive/Locking](https://cwiki.apache.org/confluence/display/Hive/Locking).

要启用锁定，请确保在配置单元会话或`hive-site.xml`中设置了这两个属性(请参阅上面的*事务、*节*和*)：

*   `hive.support.concurrency`=`true`
*   `hive.txn.manager`=`org.apache.hadoop.hive.ql.lockmgr.DbTxnManager`

任何查询在被允许执行相应的锁允许操作之前都必须获得适当的锁。 当查询为`SELECT`时，它将获得一个`S`锁。 同一个表上的并发`SELECT`语句将获得多个`S`锁并并行运行。 当查询为`INSERT`时，它将获得一个`X`锁。 并发的`INSERT`语句将只获得一个`X`锁，因此一个`INSERT`语句必须等待另一个`INSERT`释放锁。 此外，一个表只能有一个`X`锁。 当尝试获取`X`锁时，表上不应该有其他锁，否则需要`X`锁的操作(如`INSERT`、`ALTER`)必须等待并重试(表`hive.lock.sleep.between.retries`属性控制重试时间)。

通过使用新的锁管理器`DbTxnManager`，只能从查询隐式获取/释放锁。 要查看表上的锁，请使用`SHOW LOCKS`/`SHOW LOCKS``table_name`语句：

```sql
-- Show all locks when running merge into above > SHOW LOCKS;
+--------+----------+-----------------+------------+------------+
| lockid | database | table           | lock_state | lock_type  |
+--------+----------+-----------------+------------+------------+
| 19.1   | default  | employee_update | ACQUIRED   | SHARED_READ|
| 19.2   | default  | employee_trans  | ACQUIRED   |SHARED_WRITE|
+--------+----------+-----------------+-------------------------+
3 rows selected (0.059 seconds)
```

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们介绍了如何使用`LOAD`、`INSERT`、`IMPORT`和`EXPORT`关键字在表和文件之间交换数据。 然后，我们介绍了不同的数据排序和排序选项。 我们还介绍了一些使用函数的常用提示。 最后，我们概述了行级事务、`DELETE`、`UPDATE`、`MERGE`和锁。 读完本章后，我们应该能够使用 HQL 导入或导出数据。 我们应该有使用不同类型的数据排序关键字、函数和事务语句的经验。

在下一章中，我们将介绍在 HQL 中执行数据聚合和采样的不同方式。*