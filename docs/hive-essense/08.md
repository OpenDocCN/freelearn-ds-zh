# 八、可扩展性注意事项

尽管 Hive 提供了许多内置功能，但在特殊情况下，用户可能需要超出所提供的功能。 在这种情况下，我们可以在三个主要方面扩展 Hive 的功能：

*   **用户定义函数**(**UDF**)：这提供了一种使用外部函数(主要是用 Java 编写的)扩展功能的方法，该函数可以在 HQL 中求值
*   **HPL/SQL**：这为 HQL 提供过程语言编程支持
*   **流**：这将用户自己的定制程序插入到数据流中
*   **serDe**：这代表序列化和反序列化，并提供了一种使用定制文件格式序列化或反序列化数据的方法

在本章中，我们将更详细地讨论它们中的每一个。

# 用户定义函数

用户定义函数提供了一种在 HQL 查询期间使用用户自己的应用/业务逻辑处理列值的方法。 例如，用户定义的函数可以使用外部机器学习库执行特征清理、验证来自其他服务的用户访问、将多个值合并为一个或多个、执行特殊的数据编码或加密以及常规 HQL 运算符和函数范围之外的其他操作。 配置单元定义了以下三种类型的用户定义函数，它们是可扩展的：

*   `UDF`：代表用户自定义函数，按行操作，一行输出一个结果，比如大多数内置的数学函数和字符串函数。
*   `UDAF`：代表用户定义的聚合函数，按行或按组操作，结果输出整个表一行或每组一行，如：`max(...)`和`count(...)`内置函数。

*   `UDTF`：它代表用户定义的表格生成函数，该函数也是按行操作的，但是会产生多个行/表，比如`explode(...)`函数。 `UDTF`可以在`SELECT`或`LATERAL VIEW`语句之后使用。

Although all In functions in HQL are implemented in Java, UDF can also be implemented in any JVM-compatible language, such as Scala. In this book, we only focus on writing user-defined functions in Java.

在接下来的几节中，我们将开始更详细地查看每种用户定义函数的 Java 代码模板。

# 自定义项代码模板

规则`UDF`的代码模板如下：

```sql
package com.packtpub.hive.essentials.hiveudf;

import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType; 
import org.apache.hadoop.io.Text;
*// Other libraries my needed*

*// These information is show by "desc function <function_name>"*
@Description(
 name = "udf_name",
 value = "_FUNC_(arg1, ... argN) - description for the function.",
 extended = "decription with more details, such as syntax, examples."
)
@UDFType(deterministic = true, stateful = false)

public class udf_name extends UDF { 
     // *evaluate() is the only necessary function to overwrite*
     public Text evaluate(){
         /*
          * *H****ere** to impelement core function logic*
          */
          return "return the udf result"; 
     } 
     // *override is supported*
     public String evaluate(<Type_arg1> arg1,..., <Type_argN> argN){
          /*
           * *Do something here*
           */
          return "return the udf result"; 
     } 
}
```

在前面的模板中，包定义和导入应该是不言而喻的。 除了前三个必需库之外，我们还可以导入任何需要的库。 `@Description`注释是特定于配置单元的有用注释，可提供函数和用法。 在`value`属性中定义的信息将显示在`DESC FUNCTION`语句中。 在`extended`属性中定义的信息将显示在`DESCRIBE FUNCTION EXTENDED`语句中。 `@UDFType`注释指定函数的预期行为。 确定性`UDF`(`deterministic = true`)是在传递相同参数时始终给出相同结果的函数，例如`length(...)`和`max(...)`。 另一方面，非确定性(`deterministic = false`)`UDF`可以为同一组参数返回不同的结果，例如，`unix_timestamp()`，它返回默认时区的当前时间戳。 Stateful(`stateful = true`)属性允许函数跨行保留一些静态变量，例如为表行分配序列号的`row_number()`。

所有`UDF`都应该从`org.apache.hadoop.hive.ql.exec.UDF`类扩展，因此`UDF`子类必须实现`evaluate()`方法，该方法也可以重写用于不同的目的。 在此方法中，我们可以使用 Java、Hadoop 和配置单元库和数据类型实现预期的功能逻辑和异常处理。

# UDAF 代码模板

在本节中，我们将介绍`UDAF`代码模板，它是从`org.apache.hadoop.hive.ql.exec.UDAF`*和*类扩展而来的。 代码模板如下：

```sql
package com.packtpub.hive.essentials.hiveudaf;

import org.apache.hadoop.hive.ql.exec.UDAF;
import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;

@Description(
 name = "udaf_name",
 value = "_FUNC_(arg1, arg2, ... argN) - description for the function",
 extended = "description with more details, such as syntax, examples."
)
@UDFType(deterministic = false, stateful = true)

public final class udaf_name extends UDAF {
  /**
   * *The internal state of an aggregation function.* *
   * *Note that this is only needed if the internal state* * *cannot be represented by a primitive type.* *
   * *The internal state can contain fields with types like* * *ArrayList<String> and HashMap<String,Double> if needed.*
   */
  public static class UDAFState {
    private <Type_state1> state1;
    private <Type_stateN> stateN;
  }

  /**
   * *The actual class for doing the aggregation. Hive will* * *automatically look for all internal classes of the UDAF* * *that implements UDAFEvaluator.*
   */
  public static class UDAFExampleAvgEvaluator implements UDAFEvaluator {

    UDAFState state;

    public UDAFExampleAvgEvaluator() {
      super();
      state = new UDAFState();
      init();
    }

    /**
     * *Reset the state of the aggregation.*
     */
    public void init() {
      /*
       * *Examples for initializing state.*
       */
      state.state1 = 0;
      state.stateN = 0;
    }

    /**
     * *Iterate through one row of original data.* *
     * *The number and type of arguments need to be the same as we* * *call this UDAF from the Hive command line.* ** *This function should always return true.*
     */
    public boolean iterate(<Type_arg1> arg1,..., <Type_argN> argN){
      /*
       * *Add logic here for how to do aggregation if there is* * *a new value to be aggregated.*
       */
      return true;
    }

    /**
     * *Called on the mapper side on different data nodes.* * *Terminate a partial aggregation and return the state.* * *If the state is a primitive, just return primitive Java* * *classes like Integer or String.*
     */
    public UDAFState terminatePartial() {
      /*
       * *Check and return a partial result in expectations.*
       */
      return state;
    }

    /**
     * *Merge with a partial aggregation.* *
     * *This function should always have a single argument,* * *which has the same type as the return value of* * *terminatePartial().*
     */
    public boolean merge(UDAFState o) {
      /*
       * *Define operations how to merge the result calculated* * *from all data nodes.*
       */
      return true;
    }

    /**
     * *Terminates the aggregation and returns the final result.*
     */
    public long terminate() {
      /*
       * *Check and return final result in expectations.*
       */
      return state.stateN;
    }
  }
}
```

UDAF 必须是包含一个或多个实现`org.apache.hadoop.hive.ql.exec.UDAFEvaluator`的嵌套静态类的`org.apache.hadoop.hive.ql.exec.UDAF`的子类。 确保实现`UDAFEvaluator`的内部类定义为 public。 否则，配置单元将无法使用反射并确定`UDAFEvaluator`实现。 我们还应该实现前面已经描述过的五个必需函数`init()`、`iterate()`、`terminatePartial()`、`merge()`和`terminate()`。

还可以通过扩展`GenericUDF`和`GenericUDAFEvaluator`类来实现`UDF`和`UDAF`，以避免使用 Java 反射以获得更好的性能。 此外，泛型函数支持复杂数据类型(如`MAP`、`ARRAY`和`STRUCT`)作为参数，而`UDF`和`UDAF`函数不支持。 有关`GenericUDAF`的更多信息，请参考[https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy](https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy)上的 Hive 维基。

# UDTF 代码模板

要实现`UDTF`，只有一个方法从`org.apache.hadoop.hive.ql.exec.GenericUDTF`扩展而来。 没有普通的`UDTF`类。 我们需要实现三个方法：`initialize()`、`process()`和`close()`。 `UDTF`将调用`initialize()`方法，该方法返回函数输出的信息，如数据类型和输出数量。 然后，调用`process()`方法来执行带参数的核心函数逻辑并转发结果。 最后，如果需要，`close()`方法将进行适当的清理。 `UDTF`的代码模板如下：

```sql
package com.packtpub.hive.essentials.hiveudtf;

import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

@Description(
 name = "udtf_name",
 value = "_FUNC_(arg1, arg2, ... argN) - description for the function",
 extended = "description with more detail, such as syntax, examples."
)
public class udtf_name extends GenericUDTF {
  private PrimitiveObjectInspector stringOI = null;
  /**
   * *This method will be called exactly once per instance.* * *It performs any custom initialization logic we need.* * *It is also responsible for verifying the input types and* * *specifying the output types.*
   */
  @Override
  public StructObjectInspector initialize(ObjectInspector[] args) 
  throws UDFArgumentException {

    // *Check number of arguments.*
    if (args.length != 1) {
      throw new UDFArgumentException(
      "The UDTF should take exactly one argument");
    }
    /*
     * *Check that the input ObjectInspector[] array contains a* * *single PrimitiveObjectInspector of the Primitive type,* * *such as String.*
     */
    if (args[0].getCategory() != ObjectInspector.Category.PRIMITIVE
        && 
       ((PrimitiveObjectInspector) args[0]).getPrimitiveCategory() 
        != 
        PrimitiveObjectInspector.PrimitiveCategory.STRING) {
        throw new UDFArgumentException(
        "The UDTF should take a string as a parameter");
    }

    stringOI = (PrimitiveObjectInspector) args[0];
    /*
     * *Define the expected output for this function, including*
     * *each alias and types for the aliases.*
     */
    List<String> fieldNames = new ArrayList<String>(2);
    List<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(2);
    fieldNames.add("alias1");
    fieldNames.add("alias2");
    fieldOIs.add(PrimitiveObjectInspectorFactory.
                 javaStringObjectInspector);
    fieldOIs.add(PrimitiveObjectInspectorFactory.
                 javaIntObjectInspector);
    //*Set up the output schema.*
    return ObjectInspectorFactory.
    getStandardStructObjectInspector(fieldNames, fieldOIs);
  }

  /**
   * *This method is called once per input row and generates* * *output. The "forward" method is used (instead of* * *"return") in order to specify the output from the function.*
   */
  @Override
  public void process(Object[] record) throws HiveException {
   /*
    * *We may need to convert the object to a primitive type*
    * *before implementing customized logic.*
    */
    final String recStr = (String) stringOI.
    getPrimitiveJavaObject(record[0]);

    //*Emit newly created structs after applying customized logic.*
    forward(new Object[] {recStr, Integer.valueOf(1)});
  }

  /**
   * *This method is for any cleanup that is necessary before* * *returning from the UDTF. Since the output stream has* * *already been closed at this point, this method cannot* * *emit more rows.*
   */
  @Override
  public void close() throws HiveException {
    //*Do nothing*.
  }
}
```

# 开发和部署

我们将通过一个示例完成整个开发和部署步骤。 让我们创建一个名为`toUpper`的简单函数，通过以下开发和部署步骤将字符串转换为大写：

1.  下载并安装 Java IDE，如 Eclipse 或 IntelliJ IDEA。
2.  启动 IDE 并创建一个 Java 项目
3.  右键单击项目以选择构建路径|配置构建路径|添加外部 Jars 选项。 它将打开一个新窗口。 导航到包含配置单元和 Hadoop 库的目录。 然后，选择并添加我们需要导入的所有 JAR 文件。 我们还可以使用 maven([http://maven.apache.org/](http://maven.apache.org/))自动解析库依赖关系；本书的示例代码中给出了适当的`pom.xml`文件，以便作为 Maven 项目导入。
4.  在 IDE 中，根据前面提到的 UDF 模板创建以下`ToUpper.java`文件：

```sql
      package hive.essentials.hiveudf;

      import org.apache.hadoop.hive.ql.exec.UDF;
      import org.apache.hadoop.io.Text;

      class ToUpper extends UDF {
        public Text evaluate(Text input) {
          if(input == null) return null;
          return new Text(input.toString().toUpperCase());
        }
      }
```

5.  使用`hiveudf-1.0.jar`编译并构建项目 JAR 文件。
6.  使用`hdfs dfs -put hiveudf-1.0.jar /app/hive/function/`命令将 JAR 文件上传到 HDFS。
7.  将该函数创建为仅在当前会话中有效的临时函数。 从配置单元 v0.13.0 开始，我们还可以创建永久函数，该函数永久注册到元存储区，并且可以在所有查询和会话中引用：

```sql
 > CREATE TEMPORARY FUNCTION tmptoUpper 
 > as 'com.packtpub.hive.essentials.hiveudf.toupper';
 > USING JAR 'hdfs:///app/hive/function/hiveudf-1.0.jar';

 > CREATE FUNCTION toUpper -- Create permanent function
 > as 'hive.essentials.hiveudf.ToUpper' 
 > USING JAR 'hdfs:///app/hive/function/hiveudf-1.0.jar';
```

8.  验证并检查功能：

```sql
 > SHOW FUNCTIONS ToUpper;
 > DESCRIBE FUNCTION ToUpper;
 > DESCRIBE FUNCTION EXTENDED ToUpper;
 +----------------------------------------------------+
 | tab_name                                           |
 +----------------------------------------------------+
 | toUpper(value) - Returns upper case of value.      |
 | Synonyms: default.toupper                          |
 | Example:                                           |
 | > SELECT toUpper('will');                          |
 | WILL                                               |
 | Function class:hive.essentials.hiveudf.ToUpper     |
 | Function type:PERSISTENT                           |
 | Resource:hdfs:///app/hive/function/hiveudf-1.0.jar |
 +----------------------------------------------------+
```

9.  重新加载并使用 HQL 中的函数：

```sql
 > RELOAD FUNCTION; -- Reload all invisible functions if needed

 > SELECT 
 > name, toUpper(name) as cap_name, tmptoUpper(name) as cname 
 > FROM employee;
 +---------+----------+----------+
 | name    | cap_name | c_name   |
 +---------+----------+----------+
 | Michael | MICHAEL  | MICHAEL  |
 | Will    | WILL     | WILL     |
 | Shelley | SHELLEY  | SHELLEY  |
 | Lucy    | LUCY     | LUCY     |
 +---------+----------+----------+
 4 rows selected (0.363 seconds)
```

10.  在需要时删除该函数：

```sql
      > DROP TEMPORARY FUNCTION IF EXISTS tmptoUpper;
 > DROP FUNCTION IF EXISTS toUpper;
```

# HPL/SQL

从 HifeV2.0.0 开始，Hadoop 过程语言 SQL(HPL/SQL)(http://www.hplsql.org/))可用于在 Hieve 中提供存储过程编程。 HPL/SQL 支持配置单元、Spark SQL 和 Impala，并与 Oracle、DB2、MySQL 和 TSQL 标准兼容。 它的好处之一是使现有的数据库存储过程迁移到配置单元变得简单而高效。 使用 HPL/SQL 不需要 Java 技能就可以实现前面提到的通过 UDF 实现的功能。 与 UDF 相比，HPL/SQL 的性能稍慢一些，在生产应用中仍然是新事物。

以下是创建存储存储过程的示例。 HPL/SQL 支持创建`Function`和`Procedure`：

```sql
$ cat getEmpCnt.pl
CREATE PROCEDURE getCount()
BEGIN
DECLARE cnt INT = 0;
SELECT COUNT(*) INTO cnt FROM employee;
PRINT 'Users cnt: ' || cnt;
END;

call getCount(); -- Call a procedure
```

为了运行过程，我们需要通过提供`hiveserver2`连接 URL 在`hplsql-site.xml`中设置数据库连接，如下所示。 之后，HPL/SQL 可以使用默认连接提交过程语句或文件：

SQL`hplsql`命令与带`-f`选项的`hive`命令位于同一文件夹中，如下所示：

```sql
$ cat /opt/hive2/conf/hplsql-site.xml

 <configuration>
 <property>
 <name>hplsql.conn.default</name>
 <value>hive2conn</value>
 </property>
 <property>
 <name>hplsql.conn.hive2conn</name>
 <value>org.apache.hive.jdbc.HiveDriver;jdbc:hive2://localhost:10500</value>
 </property>
 </configuration>
```

然后，我们可以呼叫 HPL：

```sql
$cd /opt/hive2/bin
$ ./hplsql -f getEmpCnt.pl
SLF4J: Class path contains multiple SLF4J bindings.
...
Open connection: jdbc:hive2://localhost:10500 (1.02 sec)
Starting query
Query executed successfully (569 ms)
Users cnt: 4
```

# 流

HIVE 还可以利用 Hadoop 中的流功能以另一种方式转换数据。 流 API 打开通往外部进程(如脚本)的 I/O 管道。 然后，该过程从标准输入读取数据，并通过标准输出将结果写出。 在 HQL 中，我们可以直接使用`TRANSFORM`子句来嵌入用命令、shell 脚本、Java 或其他编程语言编写的映射器和削减器脚本。 尽管流在进程之间使用序列化/反序列化带来了开销，但它为非 Java 开发人员提供了一种简单的编码模式。 `TRANSFORM`子句的语法如下：

```sql
FROM (
    FROM src
    SELECT TRANSFORM '(' expression (',' expression)* ')'
    (inRowFormat)?
    USING 'map_user_script'
    (AS colName (',' colName)*)?
    (outRowFormat)? (outRecordReader)?
    (CLUSTER BY?|DISTRIBUTE BY? SORT BY?) src_alias
 )
 SELECT TRANSFORM '(' expression (',' expression)* ')'
 (inRowFormat)?
 USING 'reduce_user_script'
 (AS colName (',' colName)*)?
 (outRowFormat)? (outRecordReader)? 
```

默认情况下，用户脚本的`INPUT`值如下：

*   转换为`STRING`值的列
*   由制表符分隔
*   `NULL`转换为文本字符串的`N`值(区分`NULL`值和空字符串)

默认情况下，用户脚本的`OUTPUT`值如下：

*   视为制表符分隔的`STRING`列
*   *N*将重新解释为`NULL`
*   生成的`STRING`列将转换为 TABLE 声明中指定的数据类型

可以用`ROW FORMAT`覆盖这些默认值。 使用 Python 脚本`upper.py`进行流式传输的示例如下：

```sql
$cat upper.py
#!/usr/bin/env python
'''
This is a script to upper all cases
'''
import sys

def main():
    try:
        for line in sys.stdin:
          n = line.strip()
          print n.upper()
    except:
        return None
if __name__ == "__main__":main()
```

通过以正常方式运行脚本来测试该脚本，如下所示：

```sql
$ echo "Will" | python upper.py
$ WILL
```

使用 HQL 调用脚本：

```sql
 > ADD FILE /tmp/upper.py;
 > SELECT 
 > TRANSFORM (name,work_place[0])
 > USING 'python upper.py' as (CAP_NAME,CAP_PLACE)
 > FROM employee;
 +-----------+------------+
 | cap_name  | cap_place  |
 +-----------+------------+
 | MICHAEL   | MONTREAL   |
 | WILL      | MONTREAL   |
 | SHELLEY   | NEW YORK   |
 | LUCY      | VANCOUVER  |
 | STEVEN    | NULL       |
 +-----------+------------+
 5 rows selected (30.101 seconds)
```

从配置单元 v0.13.0 开始配置基于 SQL 标准的授权时，不允许使用`TRANSFORM`命令。

# 谢尔德先生

SerDe 代表序列化和反序列化。 它是用于处理记录并将其映射到配置单元表格中的列数据类型的技术。 要解释使用 SerDe 的场景，我们需要首先了解配置单元是如何读写数据的。

读取数据的过程如下。

1.  数据从 HDFS 读取。
2.  数据由`INPUTFORMAT`实现处理，该实现定义输入数据拆分和键/值记录。 在配置单元中，我们可以使用`CREATE TABLE ... STORED AS <FILE_FORMAT>`(参见[第 9 章](07.html)*，性能注意事项*)来指定它从哪个`INPUTFORMAT`读取。
3.  调用 SerDe 中定义的 Java`Deserializer`类将数据格式化为映射到表中的列和数据类型的记录。

作为读取数据的示例，我们可以使用 JSON SerDe 从 HDFS 读取`TEXTFILE`格式的数据，并将 JSON 属性和值的每一行转换为配置单元表中具有正确模式的行。

写入数据的流程如下：

1.  要写入的数据(如使用`INSERT`语句)由 SerDe 中定义的`Serializer`类转换为`OUTPUTFORMAT`类可以读取的格式。

2.  数据由`OUTPUTFORMAT`实现处理，该实现创建`RecordWriter`对象。 与`INPUTFORMAT`实现类似，`OUTPUTFORMAT`实现的指定方式与它写入数据的表的方式相同。
3.  数据被写入到表中(保存在 HDFS 中的数据)。

以写入数据为例，我们可以使用 JSON SerDe 将数据行-列写入配置单元表，JSON SerDe 将数据转换为保存到 HDFS 的 JSON 文本字符串。

支持的常用 SerDe(`org.apache.hadoop.hive.serde2`)列表如下：

*   `LazySimpleSerDe`：与`TEXTFILE`格式一起使用的默认内置 SerDe(`org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`)。 其实现方式如下：

```sql
 > CREATE TABLE test_serde_lz 
 > STORED as TEXTFILE as
 > SELECT name from employee;
 No rows affected (32.665 seconds)
```

*   `ColumnarSerDe`：这是与`RCFILE`和`ORC`格式一起使用的内置 SerDe。 它的使用方法如下：

```sql
 > CREATE TABLE test_serde_rc
 > STORED as RCFILE as
 > SELECT name from employee;
 No rows affected (27.187 seconds)

 > CREATE TABLE test_serde_orc
 > STORED as ORC as
 > SELECT name from employee;
 No rows affected (24.087 seconds)
```

*   `RegexSerDe`：这是 SerDe 中用于解析文本文件的内置 Java 正则表达式。 它的使用方法如下：

```sql
 > CREATE TABLE test_serde_rex(
 > name string,
 > gender string,
 > age string.
 > )
 > ROW FORMAT SERDE
 > 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
 > WITH SERDEPROPERTIES(
 >   'input.regex' = '([^,]*),([^,]*),([^,]*)',
 >   'output.format.string' = '%1$s %2$s %3$s'
 > )
 > STORED AS TEXTFILE;
 No rows affected (0.266 seconds)
```

*   `HBaseSerDe`：这是内置的 SerDe，使配置单元能够与 HBase 集成。 通过利用该 SerDe 查询和插入数据，我们可以将配置单元表映射到现有的 HBase 表。 在运行以下查询之前，请确保 HBase 守护进程正在运行。 有关更多详细信息，请参阅[第 1o 章](10.html)和*使用其他工具*：

```sql
 > CREATE TABLE test_serde_hb(
 > id string,
 > name string,
 > gender string,
 > age string
 > )
 > ROW FORMAT SERDE
 > 'org.apache.hadoop.hive.hbase.HBaseSerDe'
 > STORED BY
 > 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
 > WITH SERDEPROPERTIES (
 > "hbase.columns.mapping"=
 > ":key,info:name,info:gender,info:age"
 > )
 > TBLPROPERTIES("hbase.table.name" = "test_serde");
 No rows affected (0.387 seconds)
```

*   `AvroSerDe`：这是内置的 SerDe，可以读取和写入 Hive 表中的 AVRO(参见[http://avro.apache.org/](http://avro.apache.org/))数据。 AVRO 是一个远程过程调用和数据序列化框架。 从配置单元 v0.14.0 开始，只需通过三种方式将文件格式指定为`AVRO`，即可创建 Avro 支持的表格：

```sql
 > CREATE TABLE test_serde_avro( -- Specify schema directly
 > name string,
 > gender string,
 > age string
 > )
 > STORED as AVRO;
 No rows affected (0.31 seconds)

 > CREATE TABLE test_serde_avro2 -- Specify schema from properties
 > STORED as AVRO
 > TBLPROPERTIES (
 >   'avro.schema.literal'='{
 >    "type":"record",
 >    "name":"user",
 >    "fields":[ 
 >    {"name":"name", "type":"string"}, 
 >    {"name":"gender", "type":"string", "aliases":["gender"]}, 
 >    {"name":"age", "type":"string", "default":"null"}
 >    ]
 >   }'
 > );
 No rows affected (0.41 seconds)

      -- Using schema file directly as follows is a more flexiable way
 > CREATE TABLE test_serde_avro3 -- Specify schema from schema 
      file
 > STORED as AVRO
 > TBLPROPERTIES (
 > 'avro.schema.url'='/tmp/schema/test_avro_schema.avsc'
 > );
 No rows affected (0.21 seconds)

      -- Check the schema file
 $ cat /tmp/schema/test_avro_schema.avsc
 {
 "type" : "record",
 "name" : "test",
 "fields" : [
 {"name":"name", "type":"string"}, 
 {"name":"gender", "type":"string", "aliases":["gender"]}, 
 {"name":"age", "type":"string", "default":"null"}
 ]
 }
```

*   `ParquetHiveSerDe`：这是内置 SerDe(`parquet.hive.serde.ParquetHiveSerDe`)，支持从配置单元 v0.13.0 开始读取和写入拼花数据格式。 它的使用方法如下：

```sql
 CREATE TABLE test_serde_parquet
 > STORED as PARQUET as 
 > SELECT name from employee;
 No rows affected (34.079 seconds)
```

*   `OpenCSVSerDe`：这是读写 CSV 数据的串口。 从配置单元 v0.14.0 开始，它是内置的 SerDe。 `OpenCSVSerDe`比支持转义和引号规范等支持的内置行分隔符更强大。 它的使用方法如下：

```sql
 > CREATE TABLE test_serde_csv(
 > name string,
 > gender string,
 > age string
 >)
 > ROW FORMAT SERDE
 > 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
 > WITH SERDEPROPERTIES (
 >   "separatorChar" = "\t",
 >   "quoteChar" = "'",
 >   "escapeChar" = "\\"
 > ) 
 > STORED AS TEXTFILE;
```

*   `JSONSerDe`：从配置单元 v0.12.0 开始，JSON SerDe 可用于使用配置单元读取和写入 JSON 数据记录：

```sql
 > CREATE TABLE test_serde_js(
 > name string,
 > gender string,
 > age string
 > )
 > ROW FORMAT SERDE 
 > 'org.apache.hive.hcatalog.data.JsonSerDe'
 > STORED AS TEXTFILE;
 No rows affected (0.245 seconds)
```

HIVE 还允许用户定义自定义 SerDe(如果所有这些都不适用于他们的数据格式)。 有关自定义 SerDe 的更多信息，请参阅位于[https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-HowtoWriteYourOwnSerDe](https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-HowtoWriteYourOwnSerDe)的配置单元维基。

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们介绍了扩展 Hive 功能的四个主要方面。 我们还介绍了三种用户定义函数，以及它们的编码模板和部署步骤，以指导编码和部署过程。 然后介绍了 HPL/SQL，它在 HQL 中加入了过程性语言编程。 此外，我们还讨论了流以插入您自己的代码，这些代码不必是 Java 代码。 在本章的最后，我们讨论了在读写数据时可用于解析不同格式的数据文件的 SerDe。 读完本章后，您应该能够编写基本的 UDF 和 HPL/SQL，将代码插入到流中，并在配置单元中使用可用的 SerDe。

在下一章中，我们将讨论安全注意事项。