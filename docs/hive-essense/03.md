# 三、数据定义和描述

本章介绍了配置单元中描述数据的基本数据类型、数据定义语言和模式。 它还介绍了通过使用内部或外部表、分区、存储桶和视图正确有效地描述数据的最佳实践。 在本章中，我们将介绍以下主题：

*   了解数据类型
*   数据类型转换
*   数据定义语言
*   数据库
*   桌子 / 列表 / 表格 / 平面
*   隔断
*   桶 / 一桶的量 / 戽斗 / 铲斗
*   观点

# 了解数据类型

配置单元数据类型分为两种类型：原始类型和复杂类型。 `String`和`Int`是最有用的原语类型，大多数 HQL 函数都支持它们。 基元类型的详细信息如下：

可以包含一组任何类型的字段。 复杂类型允许嵌套类型。 复杂类型的详细信息为

| **基元类型** | **说明** | **示例** |
| --- | --- | --- |
| `TINYINT` | 它有 1 个字节，从`-128`到`127`。 后缀是`Y`。 它被用作一个小范围的数字。 | `10Y` |
| `SMALLINT` | 它有 2 个字节，来自`-32,768 to 32,767`。 后缀是`S`。 它用作常规的描述性数字。 | `10S` |
| `INT` | 它有 4 个字节，来自`-2,147,483,648 to 2,147,483,647`。 | `10` |
| `BIGINT` | 它有 8 个字节，从`-9,223,372,036,854,775,808`到`9,223,372,036,854,775,807`。 后缀是`L`。 | `100L` |
| `FLOAT` | 这是一个从`1.40129846432481707e<sup>-45</sup>`到`3.40282346638528860e<sup>+38</sup>`(正数或负数)的 4 字节单精度浮点数。 目前还不支持科学记数法。 它存储非常接近的数值近似值。 | `1.2345679` |
| `DOUBLE` | 这是一个从`4.94065645841246544e<sup>-324d</sup>`到`1.79769313486231570e<sup>+308d</sup>`(正数或负数)的 8 字节双精度浮点数。 目前还不支持科学记数法。 它存储非常接近的数值近似值。 | `1.2345678901234567` |
| `BINARY` | 这是在配置单元 0.8.0 中引入的，仅支持`CAST`到`STRING`，反之亦然。 | `1011` |
| `BOOLEAN` | 这是`TRUE`或`FALSE`值。 | `TRUE` |
| `STRING` | 这包括用单引号(`'`)或双引号(`"`)表示的字符。 配置单元在字符串中使用 C 样式转义。 最大尺寸约为 2G。 | `'Books'`或`"Books"` |
| `CHAR` | 从配置单元 0.13.0 开始提供此功能。 在配置单元 0.14.0 之后，大多数 UDF 将适用于此类型。 最大长度固定为`255`。 | `'US'`或`"US"` |
| `VARCHAR` | 从配置单元 0.12.0 开始提供此功能。 在配置单元 0.14.0 之后，大多数 UDF 将适用于此类型。 最大长度固定为`65,355`。 如果转换/分配给`varchar`值的字符串值超过指定的长度，该字符串将被自动截断。 | `'Books'`或`"Books"` |
| `DATE` | 它以`YYYY-MM-DD`的格式描述特定的年、月和日。 从配置单元 0.12.0 开始提供。 日期范围是从`0000-01-01`到`9999-12-31`。 | `2013-01-01` |
| `TIMESTAMP` | 它以`YYYY-MM-DD HH:MM:SS[.fff...]`的格式描述特定的年、月、日、小时、分钟、秒和毫秒。 从配置单元 0.8.0 开始提供。 | `2013-01-01 12:00:01.345` |

配置单元有三种主要的复杂类型：`ARRAY`、`MAP`和`STRUCT`。 这些数据类型构建在原始数据类型之上。 `ARRAY`和`MAP`与 Java 中的类似。 `STRUCT`是一种记录类型，可以包含一组任何类型的字段。 复杂类型允许嵌套类型。 复杂类型的详细信息如下：

| **复杂类型** | **说明** | **示例** |
| --- | --- | --- |
| `ARRAY` | 这是相同类型的项目列表，例如[`val1`、`val2`等]。 您可以使用`array_name[index]`访问该值，例如，`fruit[0]="apple"`。 索引从 0 开始。 | `"apple","orange","mango"` |
| `MAP` | 这是一组键-值对，例如`{key1, val1, key2, val2, and so on}`。 您可以使用`map_name[key]`访问该值，例如，`fruit[1]="apple"`。 | `{1: "apple",2: "orange"}` |
| `STRUCT` | 这是任何类型字段的用户定义结构，例如{val1、val2、val3 等}。 默认情况下，`STRUCT`*和*字段名称将为 col1、col2，依此类推。 您可以使用`structs_name.column_name`访问该值，例如，`fruit.col1=1`。 | `{1, "apple"}` |
| `NAMED STRUCT` | 这是用户定义的任意数量的类型化字段的结构，例如`{name1, val1, name2, val2, and so on}`。 您可以使用`structs_name.column_name`访问该值，例如，`fruit.apple="gala"`。 | `{"apple":"gala","weight kg":1}` |
| `UNION` | 这是一个恰好具有任意一种指定数据类型的结构。 从配置单元 0.7.0 开始提供。 它并不常用。 | `{2:["apple","orange"]}` |

For `MAP`, the type of  keys and values are unified. However, `STRUCT` is more flexible. `STRUCT` is more like a table, whereas `MAP` is more like an `ARRAY` with a customized index.

以下是所有常用数据类型的简短练习。 `CREATE`、`LOAD`和`SELECT`语句的详细信息将在后面的章节中介绍。 让我们来看看这个练习：

1.  *数据准备如下：

```sql
 $vi employee.txt
 Michael|Montreal,Toronto|Male,30|DB:80|Product:Developer^DLead
 Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead
 Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect
 Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead
```

2.  使用 JDBC：URL 登录至直线：

```sql
      $beeline -u "jdbc:hive2://localhost:10000/default"
```

3.  使用各种数据类型创建表格(`>`表示直线交互模式)：

```sql
 > CREATE TABLE employee (
 >   name STRING,
 >   work_place ARRAY<STRING>,
 >   gender_age STRUCT<gender:STRING,age:INT>,
 >   skills_score MAP<STRING,INT>,
 >   depart_title MAP<STRING,ARRAY<STRING>>
 > )
 > ROW FORMAT DELIMITED
 > FIELDS TERMINATED BY '|'
 > COLLECTION ITEMS TERMINATED BY ','
 > MAP KEYS TERMINATED BY ':'
      > STORED AS TEXTFILE;
 No rows affected (0.149 seconds) 
```

4.  验证表是否已创建：

```sql
 > !table employee
 +---------+------------+------------+--------------+---------+
 |TABLE_CAT|TABLE_SCHEMA| TABLE_NAME |  TABLE_TYPE  | REMARKS |
 +---------+------------+------------+--------------+---------+
 |         |default     | employee   | MANAGED_TABLE|         |
 +---------+------------+------------+--------------+---------+

 > !column employee
 -------------+-------------+-------------+-------------------+
 | TABLE_SCHEM | TABLE_NAME  | COLUMN_NAME | TYPE_NAME        |
 +-------------+-------------+-------------+------------------+
 | default     | employee    | name        | STRING           |
 | default     | employee    | work_place  | array<string>    |
      | default     | employee    | gender_age  |
 struct<gender:string,age:int>|
 | default     | employee    | skills_score| map<string,int>  |
 | default     | employee    | depart_title| 
 map<string,array<string>>   |
 +-------------+-------------+-------------+------------------+
```

5.  将数据加载到表中：

```sql
 > LOAD DATA INPATH '/tmp/hivedemo/data/employee.txt' 
 > OVERWRITE INTO TABLE employee;
 No rows affected (1.023 seconds)
```

6.  查询表中的整个数组和每个数组元素：

```sql
 > SELECT work_place FROM employee;
 +----------------------+
 |      work_place      |
 +----------------------+
 | [Montreal, Toronto]  |
 | [Montreal]           |
 | [New York]           |
 | [Vancouver]          |
 +----------------------+
 4 rows selected (27.231 seconds) 
 > SELECT 
 > work_place[0] as col_1, work_place[1] as col_2, 
      > work_place[2] as col_3 
 > FROM employee;
 +------------+----------+--------+
 |   col_1    |  col_2   | col_3  |
 +------------+----------+--------+
 | Montreal   | Toronto  |        |
 | Montreal   |          |        |
 | New York   |          |        |
 | Vancouver  |          |        |
 ------------+----------+---------+
 4 rows selected (24.689 seconds)
```

7.  查询表中的整个结构和每个结构属性：

```sql
 > SELECT gender_age FROM employee;
 +------------------+
 |    gender_age    |
 +------------------+
 | [Male, 30]       |
 | [Male, 35]       |
 | [Female, 27]     |
 | [Female, 57]     |
 +------------------+
 4 rows selected (28.91 seconds)

 > SELECT gender_age.gender, gender_age.age FROM employee;
 +------------+------+
 |   gender   | age  |
 +------------+------+
 | Male       | 30   |
 | Male       | 35   |
 | Female     | 27   |
 | Female     | 57   |
 +------------+------+
      4 rows selected (26.663 seconds)
```

8.  查询表中的整个地图和每个地图元素：

```sql
 > SELECT skills_score FROM employee;
 +--------------------+
 |    skills_score    |
 +--------------------+
 | {DB=80}            |
 | {Perl=85}          |
 | {Python=80}        |
 | {Sales=89, HR=94}  |
 +--------------------+
 4 rows selected (32.659 seconds)

 > SELECT 
 > name, skills_score['DB'] as DB, skills_score['Perl'] as Perl,
      > skills_score['Python'] as Python, 
      > skills_score['Sales'] as Sales,
 > skills_score['HR'] as HR
 > FROM employee;
 +----------+-----+-------+---------+--------+-----+
 |   name   | db  | perl  | python  | sales  | hr  |
 +----------+-----+-------+---------+--------+-----+
 | Michael  | 80  |       |         |        |     |
 | Will     |     | 85    |         |        |     |
 | Shelley  |     |       | 80      |        |     |
 | Lucy     |     |       |         | 89     | 94  |
 +----------+-----+-------+---------+--------+-----+
 4 rows selected (24.669 seconds)
```

Note that the column name shown in the result or in the hive statement is not case sensitive. It is always shown in lowercase letters.

9.  查询表中的复合类型：

```sql
 > SELECT depart_title FROM employee;
 +---------------------------------+
 |          depart_title           |
 +---------------------------------+
 | {Product=[Developer, Lead]}     |
 | {Test=[Lead], Product=[Lead]}   |
 | {Test=[Lead], COE=[Architect]}  |
 | {Sales=[Lead]}                  |
 +---------------------------------+
 4 rows selected (30.583 seconds)

 > SELECT 
 > name, depart_title['Product'] as Product, depart_title['Test'] 
      as Test,
 > depart_title['COE'] as COE, depart_title['Sales'] as Sales
 > FROM employee;
 +--------+--------------------+---------+-------------+------+
 |   name |      product       |  test   |     coe     |sales |
 +--------+--------------------+---------+-------------+------+
 | Michael| [Developer, Lead]  |         |             |      |
 | Will   | [Lead]             | [Lead]  |             |      |
 | Shelley|                    | [Lead]  | [Architect] |      |
 | Lucy   |                    |         |             |[Lead]|
 +--------+--------------------+---------+-------------+------+
 4 rows selected (26.641 seconds)

 > SELECT
 > name, depart_title['Product'][0] as product_col0, 
 > depart_title['Test'][0] as test_col0 
 > FROM employee;
 +----------+---------------+------------+
 |   name   | product_col0  | test_col0  |
 +----------+---------------+------------+
 | Michael  | Developer     |            |
 | Will     | Lead          | Lead       |
 | Shelley  |               | Lead       |
 | Lucy     |               |            |
 +----------+---------------+------------+
 4 rows selected (26.659 seconds)
```

The default delimiters in table-creation DDL are as follows:

*   **行分隔符**：可配合*Ctrl*+*A*或<sup>^</sup>A 使用(建表时使用*\001 和*)
*   **收款项目分隔符**：可配合*Ctrl*+*B*或<sup>^</sup>B(*\002*)使用
*   **映射键分隔符**：可与*Ctrl*+*C*或<sup>^</sup>C(*\003*)配合使用

If the delimiter is overridden during the table creation, it only works when used in the flat structure. This is still a limitation in Hive described in Apache Jira Hive-365 ([https://issues.apache.org/jira/browse/HIVE-365](https://issues.apache.org/jira/browse/HIVE-365)). For nested types, the level of nesting determines the delimiter. Using `ARRAY` of `ARRAY` as an example, the delimiters for the outer `ARRAY`, as expected, are *Ctrl* + *B *characters, but the inner `ARRAY` delimiter becomes *Ctrl* + *C *characters, which is the next delimiter in the list. In the preceding example, the `depart_title` column, which is a `MAP` of `ARRAY`, the `MAP` key delimiter is *Ctrl* + *C*, and the `ARRAY` delimiter is *Ctrl* + *D*.

# 数据类型转换

与 SQL 类似，HQL 同时支持隐式和显式类型转换。 从窄类型到宽类型的基元类型转换称为隐式转换。 但是，不允许反向转换。 所有整数数值类型`FLOAT`和`STRING`都可以隐式转换为`DOUBLE`，而`TINYINT`、`SMALLINT`和`INT`都可以转换为`FLOAT`。`BOOLEAN`类型不能转换为任何其他类型。 有一个数据类型交叉表，描述了每两个类型之间允许的隐式转换，可以在[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types)中找到。 显式类型转换使用带有`CAST(value as TYPE)`语法的`CAST`函数。 例如，`CAST('100' as INT)`会将`100`字符串转换为`100`整数值。 如果强制转换失败，如`CAST('INT' as INT)`，则该函数返回`NULL`。

此外，`BINARY`类型只能先转换为`STRING`，然后根据需要从`STRING`转换为其他类型。

# 数据定义

HIVE 的**数据定义语言**(**DDL**)是 HQL 语句的子集，这些语句通过创建、删除或更改模式对象(如数据库、表、视图、分区和存储桶)来描述配置单元数据结构。 大多数 DDL 语句都以`CREATE`、`DROP`或`ALTER`关键字开头。 HQL DDL 的语法与 SQL DDL 非常相似。 在下一节中，我们将重点介绍 HQL DDL 的细节。

HQL uses `*--*` before a single line of characters as comments, and it does not support multiline comments until v2.3.0\. After v2.3.0, we can use bracketed single or multiline comments between `/*` and `*/`.

# 数据库

配置单元中的数据库描述了用于类似目的或属于相同组的表的集合。 如果未指定数据库，则使用文件`default`数据库，并将 HDFS*和*中的文件`/user/hive/warehouse`用作其根目录。 此路径可通过`hive-site.xml`中的`hive.metastore.warehouse.dir`属性进行配置。每当创建新数据库时，配置单元都会在`/user/hive/warehouse`下为每个数据库创建一个新目录。 例如，`myhivebook`数据库位于`/user/hive/datawarehouse/myhivebook.db`。 此外，`DATABASE`还有一个别名`**SCHEMA**`*，*，表示它们在 HQL 中是相同的。以下是数据库操作的主要 DDL：

1.  如果数据库/架构不存在，请创建该数据库/架构：

```sql
 > CREATE DATABASE myhivebook;
 > CREATE SCHEMA IF NOT EXISTS myhivebook; 
```

2.  使用位置、注释和元数据信息创建数据库：

```sql
 > CREATE DATABASE IF NOT EXISTS myhivebook
 > COMMENT 'hive database demo'
 > LOCATION '/hdfs/directory'
 > WITH DBPROPERTIES ('creator'='dayongd','date'='2018-05-01');

      -- To show the DDL use show create database since v2.1.0
 > SHOW CREATE DATABASE default;
 +------------------------------------------------+
 | createdb_stmt                                  |
 +------------------------------------------------+
 | CREATE DATABASE `default`                      |
 | COMMENT                                        |
 | 'Default Hive database'                        |
 | LOCATION                                       |
 | 'hdfs://localhost:9000/user/hive/warehouse'    |
 +------------------------------------------------+
```

3.  使用通配符显示和描述数据库：

```sql
 > SHOW DATABASES;
 +----------------+
 | database_name  |
 +----------------+
 | default        |
 +----------------+
 1 row selected (1.7 seconds)

 > SHOW DATABASES LIKE 'my.*';

 > DESCRIBE DATABASE default;
 +-------+----------------------+--------------------------------+
      |db_name|      comment         |                   location |
 +-------+----------------------+--------------------------------+
 |default|Default Hive database | hdfs://localhost:9000
                                       /user/hive/warehouse           |
 +-------+----------------------+--------------------------------+
 1 row selected (1.352 seconds)
```

4.  切换为使用一个数据库或直接使用数据库名称限定表名：

```sql
 > USE myhivebook;
 > --SELECT * FROM myhivebook.table_name;
```

5.  显示当前数据库：

```sql
 > SELECT current_database();
 +----------+
 | _c0      |
 +----------+
 | default  |
 +----------+
 1 row selected (0.218 seconds)
```

6.  删除数据库：

```sql
 > DROP DATABASE IF EXISTS myhivebook;--failed when database is 
      not empty
 > DROP DATABASE IF EXISTS myhivebook CASCADE;--drop database and 
      tables
```

Hive databases/tables are directories/subdirectories in HDFS. In order to remove the database directory, we need to remove the subdirectories (for tables) first. By default, the database cannot be dropped if it is not empty, unless the CASCADE option is specified. With this option*,* it drops all tables in the database automatically before dropping the database.

7.  更改数据库属性。 `ALTER DATABASE`语句只能应用于数据库上的`dbproperties`、`owner`和`location`。 其他数据库属性不能更改：

```sql
 > ALTER DATABASE myhivebook SET DBPROPERTIES ('edited-
      by'='Dayong');
 > ALTER DATABASE myhivebook SET OWNER user dayongd;
 > ALTER DATABASE myhivebook SET LOCATION '/tmp/data/myhivebook';
```

Since Hive v2.2.1, the `ALTER DATABASE ... SET LOCATION` statement can be used to modify the database's location, but it does not move all existing tables/partitions in the current database directory to the newly specified location. It only changes the location for newly added tables after the database is altered. This behavior is analogous to how changing a table-directory does not move existing partitions to a different location. The `SHOW` and `DESC` (or `DESCRIBE`) statements in Hive are used to show the definition for most of the objects, such as tables and partitions. The `SHOW` statement supports a wide range of Hive objects, such as tables, tables' properties, table DDL, index, partitions, columns, functions, locks, roles, configurations, transactions, and compactions. The `DESC` statement supports a small range of Hive objects, such as databases, tables, views, columns, and partitions. However, the `DESC` statement is able to provide more detailed information combined with the `EXTENDED` or `FORMATTED` keywords. In this book, there is no dedicated section to introduce `SHOW`and `DESC`**.** Instead, we introduce them in line with other HQL through the remaining chapters.

# 桌子 / 列表 / 表格 / 平面

配置单元中的表的概念与关系数据库中的表非常相似。 每个表都映射到一个目录，在 HDFS 中，该目录默认位于`/user/hive/warehouse`目录下。 例如，为`employee`表格创建了`/user/hive/warehouse/employee`。 表中的所有数据都存储在此配置单元用户可管理的目录中(完全权限)。 这类表称为内部表或托管表。 当数据已经存储在 HDFS 中时，可以创建一个外部表来描述数据。 之所以称为`external`，是因为外部表中的数据是在`LOCATION`属性中指定的，而不是默认的仓库目录。 在内部表中保存数据时，表完全管理其中的数据。 当内部表被删除时，它的数据将被一起删除。 但是，删除外部表时，不会删除数据。 将外部表用于源只读数据或将处理后的数据共享给数据消费者是相当常见的，给出了定制的 HDFS 位置。另一方面，内部表在数据处理过程中经常被用作中间表，因为在 HQL 的支持下，它是相当强大和灵活的。

# 表创建

以下是用于创建内部和外部表的主要 DDL：

1.  显示`employee.txt`的数据文件内容：

```sql
 $ vi /home/hadoop/employee.txt
 Michael|Montreal,Toronto|Male,30|DB:80|Product:Developer^DLead
 Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead
 Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect
 Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead
```

2.  创建内部表并加载数据：

```sql
 > CREATE TABLE IF NOT EXISTS employee_internal (
 > name STRING COMMENT 'this is optinal column comments',
 > work_place ARRAY<STRING>,-- table column names are NOT case 
      sensitive
 > gender_age STRUCT<gender:STRING,age:INT>, 
 > skills_score MAP<STRING,INT>, -- columns names are lower case
 > depart_title MAP<STRING,ARRAY<STRING>>-- No "," for the last 
      column
 > )
```

```sql

 > COMMENT 'This is an internal table'-- This is optional table 
      comments
 > ROW FORMAT DELIMITED
 > FIELDS TERMINATED BY '|'     -- Symbol to seperate columns
 > COLLECTION ITEMS TERMINATED BY ','-- Seperate collection elements
 > MAP KEYS TERMINATED BY ':'  -- Symbol to seperate keys and values
 > STORED as TEXTFILE;         -- Table file format
 No rows affected (0.149 seconds)

 > LOAD DATA INPATH '/tmp/hivedemo/data/employee.txt'
 > OVERWRITE INTO TABLE employee_internal;
```

If the folder path does not exist in the `LOCATION`property, Hive will create that folder. If there is another folder inside it, Hive will NOT report errors when creating the table but querying the table.

3.  创建外部表并加载数据：

```sql
 > CREATE EXTERNAL TABLE employee_external ( -- Use EXTERNAL keywords
 > name string,
 > work_place ARRAY<string>,
 > gender_age STRUCT<gender:string,age:int>,
 > skills_score MAP<string,int>,
 > depart_title MAP<STRING,ARRAY<STRING>>
 > )
 > ROW FORMAT DELIMITED
 > FIELDS TERMINATED BY '|'
 > COLLECTION ITEMS TERMINATED BY ','
 > MAP KEYS TERMINATED BY ':'
 > STORED as TEXTFILE
 > LOCATION '/user/dayongd/employee'; -- Specify data folder location
 No rows affected (1.332 seconds)

 > LOAD DATA INPATH '/tmp/hivedemo/data/employee.txt'
 > OVERWRITE INTO TABLE employee_external;
```

Since v2.1.0, Hive supports primary and foreign key constraints. However, these constraints are not validated, so the upstream system needs to ensure data integrity before it's loaded into Hive. The Hive constraints may benefit some SQL tools to generate more efficient queries with them, but they are not used very often. 

HIVE 还支持创建临时表。临时表只对当前用户会话可见。 它会在会话结束时自动删除。 临时表的数据通常存储在用户的临时目录中，如`/tmp/hive-<username>`。 因此，当您的临时表中有敏感数据时，请确保文件夹配置正确或安全可靠。如果临时表与永久表同名，则会选择临时表，而不是永久表。 临时表不支持分区和索引。 以下是创建临时表的三种方式：

```sql
> CREATE TEMPORARY TABLE IF NOT EXISTS tmp_emp1 (
> name string,
> work_place ARRAY<string>,
> gender_age STRUCT<gender:string,age:int>,
> skills_score MAP<string,int>,
> depart_title MAP<STRING,ARRAY<STRING>>
> ); 
No rows affected (0.122 seconds)

> CREATE TEMPORARY TABLE tmp_emp2 as SELECT * FROM tmp_emp1;

> CREATE TEMPORARY TABLE tmp_emp3 like tmp_emp1;
```

还可以在一条称为 Create-Table-as-Select(**CTAS**)的语句中使用查询结果创建和填充表。 在填充所有查询结果之前，CTAS 创建的表对其他用户不可见。 CTA 有以下限制：

*   创建的表不能是分区表
*   创建的表不能是外部表
*   创建的表不能是列表分类表

尽管`SELECT *`语句本身不会触发任何 Yarn 作业，但 CCTAS`SELECT *`语句始终会触发 Yarn 作业来填充数据。

CTA 还可以与**CTE**一起使用，**CTE**表示**C**ommon**T**able**E**xpression。CTE 是一个临时结果集，派生自一个在`WITH`子句中指定的简单 SELECT 查询，后跟一个`SELECT`语句或一个`INSERT`语句来构建结果集。 CTE 仅在单个语句的执行范围内定义。 一个或多个 CTE 可以嵌套或链接的方式与关键字一起使用，如`SELECT`、`INSERT`、`CREATE TABLE AS SELECT`或`CREATE VIEW AS SELECT`语句。 使用 HQL 的 CTE 使查询比编写复杂的嵌套查询更简洁、更清晰。

以下是使用 CTAS 和 CTE 创建表的示例：

1.  使用 CTAS 创建表格：

```sql
 > CREATE TABLE ctas_employee as SELECT * FROM employee_external;
 No rows affected (1.562 seconds)
```

2.  创建同时包含 CTAS 和 CTE 的表格：

```sql
 > CREATE TABLE cte_employee as
 > WITH r1 as (
 > SELECT name FROM r2 WHERE name = 'Michael'
 > ),
 > r2 as (
 > SELECT name FROM employee WHERE gender_age.gender= 'Male'
 > ),
 > r3 as (
 > SELECT name FROM employee WHERE gender_age.gender= 'Female'
 > )
 > SELECT * FROM r1 
 > UNION ALL 
 > SELECT * FROM r3;
 No rows affected (61.852 seconds)

 > SELECT * FROM cte_employee;
 +----------------------------+
 | cte_employee.name          |
 +----------------------------+
 | Michael                    |
 | Shelley                    |
 | Lucy                       |
 +----------------------------+
 3 rows selected (0.091 seconds)
```

3.  使用 CTAS 通过从另一个表复制架构来创建空表。 它为空，因为 WHERE 条件为 FALSE：

```sql
 > CREATE TABLE empty_ctas_employee as
 > SELECT * FROM employee_internal WHERE 1=2;
 No rows affected (213.356 seconds) 
```

4.  另一方面，我们还可以使用`CREATE TABLE LIKE`创建一个空表。 这是一种更快的复制表模式的方法，因为它不会触发任何作业，而只复制元数据：

```sql
 > CREATE TABLE empty_like_employee LIKE employee_internal;
 No rows affected (0.115 seconds)
```

# 表说明

由于我们大部分时间都在处理表，因此有几个有用的表信息显示命令，如下所示：

1.  显示具有正则表达式筛选器的表：

```sql
 > SHOW TABLES; -- Show all tables in database
 > SHOW TABLES '*sam*'; -- Show tables name contains "sam"
 > SHOW TABLES '*sam|lily*'; -- Show tables name contains "sam" or
      "lily"
```

2.  列出与给定正则表达式匹配的所有表的详细表信息：

```sql
 >SHOW TABLE EXTENDED LIKE 'employee_int*';
 OK
 tableName:employee_internal
 owner:dayongd
 location:hdfs://localhost/user/hive/warehouse/employee_internal
      inputformat:org.apache.hadoop.mapred.TextInputFormat
      outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyText
      OutputFormatcolumns:struct columns { i32 num}      partitioned:false
 partitionColumns:
 totalNumberFiles:0
 totalFileSize:0
 maxFileSize:0
 minFileSize:0
 lastAccessTime:0
 lastUpdateTime:1274517075221
```

3.  以两种方式显示表列信息：

```sql
 > SHOW COLUMNS IN employee_internal;
 +---------------+
 |     field     |
 +---------------+
 | name          |
 | work_place    |
 | gender_age    |
 | skills_score  |
 | depart_title  |
 +---------------+
 5 rows selected (0.101 seconds)

 > DESC employee_internal;
 +--------------+-------------------------------+---------+
 | col_name     | data_type                     | comment |
 +--------------+-------------------------------+---------+
 | name         | string                        |         |
 | work_place   | array<string>                 |         |
 | gender_age   | struct<gender:string,age:int> |         |
 | skills_score | map<string,int>               |         |
 | depart_title | map<string,array<string>>     |         |
 +---------------+------------------------------+---------+
      5 rows selected (0.127 seconds)
```

4.  显示指定表的 CREATE-TABLE DDL 语句：

```sql
 > SHOW CREATE TABLE employee_internal;
 +--------------------------------------------------------------+
 |                      createtab_stmt                          |
 +--------------------------------------------------------------+
 | CREATE TABLE `employee_internal`(                            |
 | `name` string,                                               |
 | `work_place` array<string>,                                  |
 | `gender_age` struct<gender:string,age:int>,                  |
 | `skills_score` map<string,int>,                              |
 | `depart_title` map<string,array<string>>)                    |
 | COMMENT 'this is an internal table'                          |
 | ROW FORMAT SERDE                                             |
 | 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'         |
 | WITH SERDEPROPERTIES (                                       |
 | 'colelction.delim'=',',                                      |
 | 'field.delim'='|',                                           |
 | 'mapkey.delim'=':',                                          |
 | 'serialization.format'='|')                                  |
 | STORED as INPUTFORMAT                                        |
 | 'org.apache.hadoop.mapred.TextInputFormat'                   |
 | OUTPUTFORMAT                                                 |
 | 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' |
 | LOCATION                                                     |
 | 'hdfs://localhost:9000/user/hive/warehouse/employee_internal'|
 | TBLPROPERTIES (                                              |
 | 'transient_lastDdlTime'='1523108773')                        |
 +--------------------------------------------------------------+
 22 rows selected (0.191 seconds)
```

5.  显示指定表的表属性：

```sql
 > SHOW TBLPROPERTIES employee_internal;
 +-----------------------+---------------------------+
 | prpt_name             | prpt_value                |
 +-----------------------+---------------------------+
 | comment               | this is an internal table |
 | numFiles              | 1                         |
 | numRows               | 0                         |
 | rawDataSize           | 0                         |
 | totalSize             | 227                       |
 | transient_lastDdlTime | 1523108773                |
 +-----------------------+---------------------------+
 6 rows selected (0.095 seconds)
```

# 餐桌清洁

有时，我们可能需要清理表格，要么只删除记录，要么只删除表格和记录。 HQL 中有两个语句用于执行此类清理。 一个是`DROP TABLE`语句，另一个是`TRUNCATE TABLE`语句。 如果配置了垃圾桶设置，则内部表上的 DROP-TABLE 语句将完全删除该表，并将数据移动到当前用户目录中的`.trash`。 外部表上的 DROP-TABLE 语句只会删除表定义，但会保留数据：

```sql
> DROP TABLE IF EXISTS empty_ctas_employee;
No rows affected (0.283 seconds)
```

另一方面，TRUNCATE TABLE 语句只从表中删除数据。 该表仍然存在，但为空。 注意，TRUNCATE TABLE 只能应用于内部表：

```sql
> TRUNCATE TABLE cte_employee;-- Only apply to internal tables
No rows affected (0.093 seconds)

> SELECT name FROM cte_employee;--Other hand, the truncate t
-- Not data left, but empty table exists
+--------------------+
| cte_employee.name  |
+--------------------+
+--------------------+
No rows selected (0.059 seconds)
```

# 表格更改

一旦创建了表，我们仍然可以修改它的元数据，比如添加新列和更改列的数据类型。 在 HQL 中，我们使用`ALTER`命令修改表的元数据。 但是，ALTER TABLE 无法相应地更新数据。 我们应该手动确保实际数据符合元数据定义，否则查询将不会返回任何预期的结果。

以下是在 HQL 中更改表格的示例：

1.  使用`ALTER`语句重命名表。 这通常用作数据备份：

```sql
 > ALTER TABLE cte_employee RENAME TO cte_employee_backup;
 No rows affected (0.237 seconds)
```

2.  使用`TBLPROPERTIES`更改表属性：

```sql
 > ALTER TABLE c_employee SET TBLPROPERTIES 
      ('comment'='New comments');
 No rows affected (0.239 seconds)
```

3.  使用`SERDEPROPERTIES`*：*更改表的行格式和 SerDe(SerDe 在[章 8](08.html)*，可扩展性注意事项*中介绍)

```sql
 > ALTER TABLE employee_internal SET SERDEPROPERTIES 
      ('field.delim' = '$');
 No rows affected (0.148 seconds)
```

4.  使用`FILEFORMAT`更改表的文件格式：

```sql
 > ALTER TABLE c_employee SET FILEFORMAT RCFILE;
 No rows affected (0.235 seconds) 
```

5.  使用`LOCATION`更改表的位置，即 HDFS 的完整 URI：

```sql
 > ALTER TABLE c_employee SET LOCATION 
      'hdfs://localhost:9000/tmp/employee'; 
 No rows affected (0.169 seconds)
```

6.  启用/禁用表的数据保护；`NO_DROP`或`OFFLINE`。`NO_DROP`可防止表被删除，而`OFFLINE`**或**可防止查询表中的数据(非元数据)：

```sql
 > ALTER TABLE c_employee ENABLE NO_DROP; 
 > ALTER TABLE c_employee DISABLE NO_DROP; 
 > ALTER TABLE c_employee ENABLE OFFLINE;
 > ALTER TABLE c_employee DISABLE OFFLINE;
```

7.  在`RCFile`或`ORC`表中启用串联(如果它有许多小文件)：

```sql
 > ALTER TABLE c_employee SET FILEFORMAT ORC; -- Convert to ORC
 No rows affected (0.160 seconds)

 > ALTER TABLE c_employee CONCATENATE;
 No rows affected (0.165 seconds)
```

Since v0.8.0, `RCFile` is added to support fast block-level merging of small `RCFiles` using the `CONCATENATE` option. Since v0.14.0, ORC file is added to support the fast stripe-level merging of small ORC files using the `CONCATENATE` option. Other file formats are not supported yet. `RCfiles` merge at the block level, while `ORC` files merge at the stripe level, thereby avoiding the overhead of decompressing and decoding the data. 

8.  更改列的数据类型、位置(使用`AFTER`或`FIRST`)和注释：

```sql
 > DESC employee_internal; -- Check column type before alter
 +----------------+-------------------------------+----------+
 |    col_name    |          data_type            | comment  |
 +----------------+-------------------------------+----------+
 | employee_name  | string                        |          |
 | work_place     | array<string>                 |          |
 | gender_age     | struct<gender:string,age:int> |          |
 | skills_score   | map<string,int>               |          |
 | depart_title   | map<string,array<string>>     |          |
 +----------------+-------------------------------+----------+
 5 rows selected (0.119 seconds)

 > ALTER TABLE employee_internal 
 > CHANGE name employee_name string AFTER gender_age;
 No rows affected (0.23 seconds)

 > DESC employee_internal; -- Verify type and order changes above
 +----------------+-------------------------------+----------+
 |    col_name    |          data_type            | comment  |
 +----------------+-------------------------------+----------+
 | work_place     | array<string>                 |          |
 | gender_age     | struct<gender:string,age:int> |          |
 | employee_name  | string                        |          |
 | skills_score   | map<string,int>               |          |
 | depart_title   | map<string,array<string>>     |          |
 +----------------+-------------------------------+----------+
 5 rows selected (0.214 seconds)

 > ALTER TABLE employee_internal 
 > CHANGE employee_name name string COMMENT 'updated' FIRST;
 No rows affected (0.238 seconds)

 > DESC employee_internal; -- Verify changes by FRIST keywords
 +---------------+-------------------------------+----------+
 |   col_name    |          data_type            | comment  |
 +---------------+-------------------------------+----------+
 | name          | string                        | updated  |
 | work_place    | array<string>                 |          |
 | gender_age    | struct<gender:string,age:int> |          |
 | skills_score  | map<string,int>               |          |
 | depart_title  | map<string,array<string>>     |          |
 +---------------+-------------------------------+----------+
 5 rows selected (0.119 seconds)
```

9.  向表中添加新列：

```sql
 > ALTER TABLE c_employee ADD COLUMNS (work string);
 No rows affected (0.184 seconds)

 > DESC c_employee; 
 +-----------+------------+----------+
 | col_name  | data_type  | comment  |
 +-----------+------------+----------+
 | name      | string     |          |
 | work      | string     |          |
 +-----------+------------+----------+
 2 rows selected (0.115 seconds)
```

10.  使用指定的新列替换表中的所有列：

```sql
 > ALTER TABLE c_employee REPLACE COLUMNS (name string);
 No rows affected (0.132 seconds)

 > DESC c_employee; -- Verify the changes
 +-----------+------------+----------+
 | col_name  | data_type  | comment  |
 +-----------+------------+----------+
 | name      | string     |          |
 +-----------+------------+----------+
 1 row selected (0.129 seconds)
```

# 隔断

默认情况下，一个简单的 HQL 查询扫描整个表。 这会降低查询大表时的性能。 这个问题可以通过创建分区来解决，这些分区与 RDBMS 中的分区非常相似。 在配置单元中，每个分区对应于一个预定义的分区列，该列映射到 HDFS 中表目录中的子目录。 当查询表时，只读取表中所需的数据分区(目录)，因此大大减少了查询的 I/O 和时间。 使用分区是提高配置单元性能的一种非常简单有效的方法。

以下是在 HQL 中创建分区的示例：

```sql
> CREATE TABLE employee_partitioned (
> name STRING,
> work_place ARRAY<STRING>,
> gender_age STRUCT<gender:STRING,age:INT>,
> skills_score MAP<STRING,INT>,
> depart_title MAP<STRING,ARRAY<STRING>> -- This is regular column > )
> PARTITIONED BY (year INT, month INT) -- Use lower case partition column > ROW FORMAT DELIMITED
> FIELDS TERMINATED BY '|'
> COLLECTION ITEMS TERMINATED BY ','
> MAP KEYS TERMINATED BY ':';
No rows affected (0.293 seconds)

> DESC employee_partitioned; 
-- Partition columns are listed twice
+-------------------------+-------------------------------+---------+
|         col_name        |         data_type             | comment |
+-------------------------+-------------------------------+---------+
| name                    | string                        |         |
| work_place              | array<string>                 |         |
| gender_age              | struct<gender:string,age:int> |         |
| skills_score            | map<string,int>               |         |
| depart_title            | map<string,array<string>>     |         |
| year                    | int                           |         |
| month                   | int                           |         |
|                         | NULL                          |   NULL  |
| # Partition Information | NULL                          |   NULL  |
| # col_name              | data_type                     | comment |
|                         | NULL                          |   NULL  |
| year                    | int                           |         |
| month                   | int                           |         |
+-------------------------+-------------------------------+---------+
13 rows selected (0.38 seconds)

> SHOW PARTITIONS employee_partitioned; -- Check partitions +------------+
| partition  |
+------------+
+------------+
 No rows selected (0.177 seconds)
```

从前面的结果可以看出，该分区没有自动启用。 我们必须使用`ALTER TABLE ADD PARTITION`语句将静态分区添加到表中。 在这里，静态表示分区是手动添加的。 此命令更改表的元数据，但不加载数据。 如果分区位置中不存在数据，则查询不会返回任何结果。 要删除分区元数据，请使用`ALTER TABLE ... DROP PARTITION`语句。 对于外部表，`ALTER`不更改数据，但元数据，DROP PARTITION 不会删除分区内的数据。 为了删除数据，我们可以使用`hdfs dfs -rm`命令从 HDFS 中删除外部表的数据。 对于内部表，`ALTER TABLE ... DROP PARTITION`将同时删除分区和数据。 以下是对分区表进行常见操作的更多示例：

1.  执行分区操作，如添加、删除和重命名分区：

```sql
 > ALTER TABLE employee_partitioned ADD -- Add multiple static 
      partitions > PARTITION (year=2018, month=11) PARTITION (year=2018, 
      month=12);
      No rows affected (0.248 seconds)      

 > SHOW PARTITIONS employee_partitioned;
        +---------------------+
        |      partition      |
        +---------------------+
        | year=2018/month=11  |
        | year=2018/month=12  |
        +---------------------+
      2 rows selected (0.108 seconds)      -- Drop partition with PURGE at the end will remove completely      -- Drop partition will NOT remove data for external table
      -- Drop partition will remove data with partition for internal table > ALTER TABLE employee_partitioned 
 > DROP IF EXISTS PARTITION (year=2018, month=11);      
 > SHOW PARTITIONS employee_partitioned;
        +---------------------+
        |      partition      |
        +---------------------+
        | year=2018/month=12  |
        +---------------------+
        1 row selected (0.107 seconds)

 > ALTER TABLE employee_partitioned 
 > DROP IF EXISTS PARTITION (year=2017); -- Drop all partitions in 
      2017

 > ALTER TABLE employee_partitioned 
 > DROP IF EXISTS PARTITION (month=9); -- Drop all month is 9

 > ALTER TABLE employee_partitioned -- Rename exisiting partition
      values
 > PARTITION (year=2018, month=12) 
 > RENAME TO PARTITION (year=2018,month=10);
 No rows affected (0.274 seconds)

 > SHOW PARTITIONS employee_partitioned;
 +---------------------+ 
 | partition           | 
 +---------------------+
 | year=2018/month=10  |
 +---------------------+
 2 rows selected (0.274 seconds)

      -- Below is failed 
      -- Because all partition columns should be specified for partition 
      rename
 > --ALTER TABLE employee_partitioned PARTITION (year=2018) 
 > --RENAME TO PARTITION (year=2017);
```

2.  创建分区后，将数据加载到表分区：

```sql
 > LOAD DATA INPATH '/tmp/hivedemo/data/employee.txt'
 > OVERWRITE INTO TABLE employee_partitioned 
 > PARTITION (year=2018, month=12);
 No rows affected (0.96 seconds)

 > SELECT name, year, month FROM employee_partitioned; -- Verify data 
      loaded
 +----------+-------+--------+
 |   name   | year  | month  |
 +----------+-------+--------+
 | Michael  | 2018  | 12     |
 | Will     | 2018  | 12     |
 | Shelley  | 2018  | 12     |
 | Lucy     | 2018  | 12     |
 +----------+-------+--------+
 4 rows selected (37.451 seconds)
```

为了避免手动添加静态分区，动态分区插入(或多分区插入)设计用于在扫描输入表时动态确定应该添加和填充哪些分区。 此部分在[第 5 章](05.html)、*数据操作*中的`INSERT`语句中进行了更详细的介绍。 要在分区中填充数据，我们可以使用`LOAD`或`INSERT`语句。 该语句仅加载指定分区列表中的数据。

虽然分区列映射到目录名而不是数据，但我们可以像 HQL 中的常规列一样查询或选择它们，以缩小结果集。

The use case for static and dynamic partition is quite different. Static partition is often used for an external table containing data newly landed in HDFS. In this case, it often uses the date, such as `yyyyMMdd`, as the partition column. Whenever the data of the new day arrives, we add the day-specific static partition (by script) to the table, and then the newly arrived data is queryable from the table immediately. For dynamic partition, it is often being used for data transformation between internal tables with partition columns derived from data itself; see [Chapter 5](05.html), *Data Manipulation*.

3.  从分区中删除数据。 注意，删除数据不会删除分区信息。 为了进行完整的数据清理，我们可以在删除数据后删除步骤 1 中描述的分区：

```sql
      -- For internal table, we use truncate
 > TRUNCATE TABLE employee_partitioned PARTITION 
      (year=2018,month=12);

      -- For external table, we have to use hdfs command
 > dfs -rm -r -f /user/dayongd/employee_partitioned;
```

4.  向分区表添加常规列。 注意，我们需要`CANNOT`添加新列作为分区列。 在分区表中添加/删除列时有两个选项：`CASCADE`和`RESTRICT`。 常用的 _`CASCADE`选项将相同的更改级联到表中的所有分区。 但是，`RESTRICT`*和*是默认设置，将列更改限制为仅对表元数据进行更改，这意味着更改将仅应用于新分区，而不会应用于现有分区：

```sql
 > ALTER TABLE employee_partitioned ADD COLUMNS (work string) 
      CASCADE;
```

5.  我们可以更改现有的分区列数据类型：

```sql
 > ALTER TABLE employee_partitioned PARTITION COLUMN(year string);
 No rows affected (0.274 seconds)

 > DESC employee_partitioned; -- Verify the changes
 +-------------------------+-------------------------------+---------+
 |         col_name        |         data_type             | comment |
 +-------------------------+-------------------------------+---------+
 | name                    | string                        |         |
 | work_place              | array<string>                 |         |
 | gender_age              | struct<gender:string,age:int> |         |
 | skills_score            | map<string,int>               |         |
 | depart_title            | map<string,array<string>>     |         |
 | work                    | string                        |         |
 | year                    | int                           |         |
 | month                   | int                           |         |
 |                         | NULL                          |   NULL  |
 | # Partition Information | NULL                          |   NULL  |
 | # col_name              | data_type                     | comment |
 |                         | NULL                          |   NULL  |
 | year                    | string                        |         |
 | month                   | int                           |         |
 +-------------------------+-------------------------------+---------+
 13 rows selected (0.38 seconds)
```

Right now, we can only change the partition column data type. We cannot add/remove a column from partition columns. If we have to change the partition design, we must back up and recreate the table, and then migrate the data. In addition, we are `NOT` able to change a non-partition table to a partition table directly.

6.  在文件格式、位置、保护和串联方面更改分区的其他属性与更改 TABLE 语句的语法相同：

```sql
 > ALTER TABLE employee_partitioned PARTITION (year=2018) 
 > SET FILEFORMAT ORC;
 > ALTER TABLE employee_partitioned PARTITION (year=2018) 
 > SET LOCATION '/tmp/data';
 > ALTER TABLE employee_partitioned PARTITION (year=2018) ENABLE 
      NO_DROP;
 > ALTER TABLE employee_partitioned PARTITION (year=2018) ENABLE 
      OFFLINE;
 > ALTER TABLE employee_partitioned PARTITION (year=2018) DISABLE 
      NO_DROP;
 > ALTER TABLE employee_partitioned PARTITION (year=2018) DISABLE 
      OFFLINE;
 > ALTER TABLE employee_partitioned PARTITION (year=2018) CONCATENATE;
```

# 桶 / 一桶的量 / 戽斗 / 铲斗

除了分区之外，存储桶是将数据集聚集到更易于管理的部分以优化查询性能的另一种技术。 与分区不同，存储桶对应 HDFS 中的文件段。 例如，上一节中的`employee_partitioned`表使用`year`和`month`作为顶级分区。 如果进一步请求使用`employee_id`作为第三级分区，则会创建许多分区目录。 例如，我们可以使用`employee_id`作为存储桶列来存储`employee_partitioned`表。 此列的值将按用户定义的存储桶数进行散列。 具有相同`employee_id`的记录将始终存储在同一存储桶(文件段)中。 存储桶列由`CLUSTERED BY`关键字定义。 它与分区列有很大不同，因为分区列指的是目录，而存储桶列必须是实际的表数据列。 通过使用存储桶，HQL 查询可以轻松高效地执行采样(参见[第 6 章](06.html)、*数据聚合和采样*)、存储桶端连接和映射端连接(参见[第 4 章](04.html)、*数据关联和范围*)。 创建存储桶表的示例如下所示：

```sql
--Prepare table employee_id and its dataset to populate bucket table
> CREATE TABLE employee_id (
> name STRING,
> employee_id INT, 
> work_place ARRAY<STRING>,
> gender_age STRUCT<gender:STRING,age:INT>,
> skills_score MAP<STRING,INT>,
> depart_title MAP<STRING,ARRAY<STRING>>
> )
> ROW FORMAT DELIMITED
> FIELDS TERMINATED BY '|'
> COLLECTION ITEMS TERMINATED BY ','
> MAP KEYS TERMINATED BY ':';
No rows affected (0.101 seconds)

> LOAD DATA INPATH 
> '/tmp/hivedemo/data/employee_id.txt' 
> OVERWRITE INTO TABLE employee_id
No rows affected (0.112 seconds)

--Create the buckets table > CREATE TABLE employee_id_buckets (
> name STRING,
> employee_id INT,  -- Use this table column as bucket column later
> work_place ARRAY<STRING>,
> gender_age STRUCT<gender:string,age:int>,
> skills_score MAP<string,int>,
> depart_title MAP<string,ARRAY<string >>
> )
> CLUSTERED BY (employee_id) INTO 2 BUCKETS -- Support more columns > ROW FORMAT DELIMITED
> FIELDS TERMINATED BY '|'
> COLLECTION ITEMS TERMINATED BY ','
> MAP KEYS TERMINATED BY ':';
No rows affected (0.104 seconds)
```

To define the proper number of buckets, we should avoid having too much or too little data in each bucket. A better choice is somewhere near two blocks of data, such as 512 MB of data in each bucket. As a best practice, use 2<sup>N</sup> as the number of buckets.

分组与数据加载过程密切相关。 要将数据正确加载到时段表中，我们需要将最大减数设置为与创建表中指定的时段数相同的时段数(例如，2)，或者启用强制分组(推荐)，如下所示：

```sql
> set map.reduce.tasks = 2;
No rows affected (0.026 seconds)

> set hive.enforce.bucketing = true; -- This is recommended No rows affected (0.002 seconds)
```

要将数据填充到存储桶表中，我们不能使用`LOAD DATA`语句，因为它不会根据元数据验证数据。 相反，应该始终使用`INSERT`来填充桶表：

```sql
> INSERT OVERWRITE TABLE employee_id_buckets SELECT * FROM employee_id;
No rows affected (75.468 seconds)

-- Verify the buckets in the HDFS from shell $hdfs dfs -ls /user/hive/warehouse/employee_id_buckets
Found 2 items
-rwxrwxrwx   1 hive hive        900 2018-07-02 10:54 
/user/hive/warehouse/employee_id_buckets/000000_0
-rwxrwxrwx   1 hive hive        582 2018-07-02 10:54 
/user/hive/warehouse/employee_id_buckets/000001_0
```

# 观点

视图是逻辑数据结构，可用于通过隐藏复杂性(如联接、子查询和筛选器)来简化查询。 之所以称其为逻辑，是因为视图仅在`metastore`中定义，没有在 HDFS 中占用空间。 与关系数据库中的不同，HQL 中的视图不存储数据或实现。 创建视图后，其架构将立即冻结。 对基础表的后续更改(例如，添加列)不会反映在视图的架构中。 如果基础表被删除或更改，则后续查询无效视图的尝试将失败。 此外，视图是只读的，不能用作`LOAD`/`INSERT`/`ALTER`语句的目标。

以下是视图创建语句的示例：

```sql
> CREATE VIEW IF NOT EXISTS employee_skills
> AS
> SELECT 
> name, skills_score['DB'] as DB,
> skills_score['Perl'] as Perl, 
> skills_score['Python'] as Python,
> skills_score['Sales'] as Sales, 
> skills_score['HR'] as HR 
> FROM employee;
No rows affected (0.253 seconds)
```

创建视图时，不会触发 Yarn 作业，因为这只是元数据更改。 但是，该作业将在查询视图时触发。 要检查视图定义，可以使用`SHOW`语句。 在修改视图定义时，我们可以使用`ALTER VIEW`语句。 以下是显示、检查和修改视图的一些示例：

1.  仅显示数据库中的视图。 这是在配置单元 v2.2.0 中引入的。 我们可以在早期版本的配置单元中使用`SHOW TABLES`语句：

```sql
 > SHOW VIEWS;
 > SHOW VIEWS 'employee_*';
 No rows affected (0.19 seconds)
```

2.  显示视图的定义：

```sql
 > DESC FORMATTED employee_skills;
 > SHOW CREATE TABLE employee_skills; -- this is recommended
 No rows affected (0.19 seconds)
```

3.  更改视图的属性：

```sql
 > ALTER VIEW employee_skills SET TBLPROPERTIES ('comment'='A 
      view');
 No rows affected (0.19 seconds)
```

4.  重新定义视图：

```sql
 > ALTER VIEW employee_skills as SELECT * from employee;
 No rows affected (0.17 seconds)
```

5.  删除视图：

```sql
 > DROP VIEW employee_skills;
 No rows affected (0.156 seconds)
```

在 HQL 中有一个特殊的视图，称为`LateralView`。 通常与配置单元中的自定义制表函数配合使用，如`explode()`*、*用于数据归一化或处理 JSON 数据。`LateralView`首先将制表函数应用于数据，然后将函数的输入和输出连接在一起。 请参阅以下示例：

```sql
> SELECT name, workplace FROM employee_internal 
> LATERAL VIEW explode(work_place) wp as workplace;
+---------+-----------+
| name    | workplace |
+---------+-----------+
| Michael | Montreal  |
| Michael | Toronto   |
| Will    | Montreal  |
| Shelley | Montreal  |
| Lucy    | Vancouver |
+---------+-----------+
5 rows selected (6.693 seconds)
```

通过在`LATERAL VIEW`后添加`OUTER`*和*，即使表格生成函数的输出是`NULL`，我们也可以确保生成结果：

```sql
> SELECT name, workplace FROM employee_internal
> LATERAL VIEW explode(split(null, ',')) wp as workplace;
+-------+------------+
| name  | workplace  |
+-------+------------+
+-------+------------+
No rows selected (5.499 seconds)

> SELECT name, workplace FROM employee_internal
> LATERAL VIEW OUTER explode(split(null, ',')) wp as workplace;
+---------+-----------+
| name    | workplace |
+---------+-----------+
| Michael | NULL      |
| Michael | NULL      |
| Will    | NULL      |
| Shelley | NULL      |
| Lucy    | NULL      |
+---------+-----------+
5 rows selected (5.745 seconds)
```

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们学习了如何在配置单元中定义和使用各种数据类型。 我们了解了如何创建、更改和删除表、分区和视图。 我们还介绍了如何使用外部表、内部表、分区、存储桶和视图。

在下一章中，我们将深入探讨 Hive 中查询数据的细节。