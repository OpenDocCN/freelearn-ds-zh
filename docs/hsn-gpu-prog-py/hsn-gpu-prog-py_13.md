# 第十三章：评估

# 第一章，为什么要进行 GPU 编程？

1.  前两个`for`循环遍历每个像素，它们的输出彼此不变；因此我们可以在这两个`for`循环上并行化。第三个`for`循环计算特定像素的最终值，这是固有的递归。

1.  阿姆达尔定律没有考虑在 GPU 和主机之间传输内存所需的时间。

1.  512 x 512 等于 262,144 像素。这意味着第一个 GPU 一次只能计算一半像素的输出，而第二个 GPU 可以一次计算所有像素；这意味着第二个 GPU 在这里将比第一个 GPU 快大约两倍。第三个 GPU 有足够的核心来一次计算所有像素，但正如我们在问题 1 中看到的那样，额外的核心在这里对我们没有用。因此，对于这个问题，第二个和第三个 GPU 的速度将是相同的。

1.  将某个代码段通用地指定为与阿姆达尔定律相关的并行化存在一个问题，即假设这段代码的计算时间在处理器数量*N*非常大时接近 0。正如我们从上一个问题中看到的，情况并非如此。

1.  首先，一致使用*时间*可能很麻烦，并且可能无法找出程序的瓶颈。其次，分析器可以告诉您从 Python 的角度来看，所有代码的确切计算时间，因此您可以确定某些库函数或操作系统的后台活动是否有问题，而不是您的代码。

# 第二章，设置 GPU 编程环境

1.  不，CUDA 只支持 Nvidia GPU，不支持 Intel HD 或 AMD Radeon

1.  本书只使用 Python 2.7 示例

1.  设备管理器

1.  lspci

1.  `free`

1.  `.run`

# 第三章，开始使用 PyCUDA

1.  是的。

1.  主机/设备之间的内存传输和编译时间。

1.  你可以，但这将取决于你的 GPU 和 CPU 设置。

1.  使用 C `?`运算符来执行点对点和减少操作。

1.  如果`gpuarray`对象超出范围，其析构函数将被调用，这将自动在 GPU 上释放它所代表的内存。

1.  `ReductionKernel`可能执行多余的操作，这可能取决于底层 GPU 代码的结构。*中性元素*将确保没有值因这些多余的操作而改变。

1.  我们应该将`neutral`设置为带符号 32 位整数的最小可能值。

# 第四章，内核，线程，块和网格

1.  试试看。

1.  并非所有线程都同时在 GPU 上运行。就像 CPU 在操作系统中在任务之间切换一样，GPU 的各个核心在内核的不同线程之间切换。

1.  O(n/640 log n)，即 O(n log n)。

1.  试试看。

1.  实际上，CUDA 中没有内部网格级同步，只有块级（使用`__syncthreads`）。我们必须将单个块以上的任何内容与主机同步。

1.  天真：129 次加法运算。高效工作：62 次加法运算。

1.  同样，如果我们需要在大量块的网格上进行同步，我们不能使用`__syncthreads`。如果我们在主机上进行同步，我们还可以在每次迭代中启动更少的线程，从而为其他操作释放更多资源。

1.  在天真的并行求和的情况下，我们可能只会处理一小部分数据点，这些数据点应该等于或少于 GPU 核心的总数，这些核心可能适合于块的最大大小（1032）；由于单个块可以在内部同步，我们应该这样做。只有当数据点的数量远远大于 GPU 上可用核心的数量时，我们才应该使用高效的工作算法。

# 第五章，流，事件，上下文和并发性

1.  两者的性能都会提高；随着线程数量的增加，GPU 在两种情况下都达到了峰值利用率，减少了使用流所获得的收益。

1.  是的，你可以异步启动任意数量的内核，并使用`cudaDeviceSynchronize`进行同步。

1.  打开你的文本编辑器，试试看！

1.  高标准差意味着 GPU 的使用不均匀，有时会过载 GPU，有时会过度利用它。低标准差意味着所有启动的操作通常都运行顺利。

1.  i. 主机通常可以处理的并发线程远远少于 GPU。ii. 每个线程都需要自己的 CUDA 上下文。GPU 可能会因为上下文过多而不堪重负，因为每个上下文都有自己的内存空间，并且必须处理自己加载的可执行代码。

# 第六章，调试和分析 CUDA 代码

1.  在 CUDA 中，内存分配会自动同步。

1.  `lockstep`属性仅在大小为 32 或更小的单个块中有效。在这里，两个块将在没有任何`lockstep`的情况下正确分歧。

1.  在这里也会发生同样的事情。这个 64 线程块实际上会被分成两个 32 线程的 warp。

1.  Nvprof 可以计时单个内核启动、GPU 利用率和流使用；任何主机端的分析器只会看到 CUDA 主机函数的启动。

1.  Printf 通常更容易用于规模较小、内联内核相对较短的项目。如果你编写了一个非常复杂的 CUDA 内核，有数千行代码，那么你可能会想使用 IDE 逐行步进和调试你的内核。

1.  这告诉 CUDA 我们要使用哪个 GPU。

1.  `cudaDeviceSynchronize`将确保相互依赖的内核启动和内存复制确实是同步的，并且它们不会在所有必要的操作完成之前启动。

# 第七章，使用 Scikit-CUDA 库

1.  SBLAH 以 S 开头，因此该函数使用 32 位实数浮点数。ZBLEH 以 Z 开头，这意味着它使用 128 位复数浮点数。

1.  提示：设置`trans = cublas._CUBLAS_OP['T']`

1.  提示：使用 Scikit-CUDA 包装器进行点积，`skcuda.cublas.cublasSdot`

1.  提示：建立在上一个问题的答案基础上。

1.  你可以将 cuBLAS 操作放在一个 CUDA 流中，并使用该流的事件对象来精确测量 GPU 上的计算时间。

1.  由于输入看起来对 cuFFT 来说很复杂，它将计算所有值作为 NumPy。

1.  暗边是由于图像周围的零缓冲造成的。这可以通过在边缘上*镜像*图像来缓解，而不是使用零缓冲。

# 第八章，CUDA 设备函数库和 Thrust

1.  试试看。(它实际上比你想象的更准确。)

1.  一个应用：高斯分布可以用于向样本添加“白噪声”以增加机器学习中的数据集。

1.  不，因为它们来自不同的种子，如果我们将它们连接在一起，这些列表可能会有很强的相关性。如果我们打算将它们连接在一起，我们应该使用相同种子的子序列。

1.  试试看。

1.  提示：记住矩阵乘法可以被看作是一系列矩阵-向量乘法，而矩阵-向量乘法可以被看作是一系列点积。

1.  `Operator()`用于定义实际函数。

# 第九章，实现深度神经网络

1.  一个问题可能是我们没有对训练输入进行归一化。另一个可能是训练速率太大。

1.  使用较小的训练速率，一组权重可能会收敛得非常缓慢，或者根本不会收敛。

1.  大的训练速率可能导致一组权重过度拟合特定的批处理值或该训练集。此外，它可能导致数值溢出/下溢，就像第一个问题一样。

1.  Sigmoid。

1.  Softmax。

1.  更多更新。

# 第十章，使用已编译的 GPU 代码

1.  只有 EXE 文件将包含主机函数，但 PTX 和 EXE 都将包含 GPU 代码。

1.  `cuCtxDestory`。

1.  `printf`带有任意输入参数。(尝试查找`printf`原型。)

1.  使用 Ctypes 的`c_void_p`对象。

1.  这将允许我们使用原始名称从 Ctypes 链接到函数。

1.  CUDA 会自动同步设备内存分配和设备/主机之间的内存复制。

# 第十一章，CUDA 性能优化

1.  `atomicExch`是线程安全的事实并不保证所有线程将同时执行此函数（因为在网格中不同的块可以在不同的时间执行）。

1.  大小为 100 的块将在多个 warp 上执行，除非我们使用`__syncthreads`，否则块内部将不会同步。因此，`atomicExch`可能会被多次调用。

1.  由于 warp 默认以锁步方式执行，并且大小为 32 或更小的块将使用单个 warp 执行，因此`__syncthreads`是不必要的。

1.  我们在 warp 内使用天真的并行求和，但除此之外，我们使用`atomicAdd`进行了许多求和，就像我们使用串行求和一样。虽然 CUDA 自动并行化了许多这些`atomicAdd`调用，但我们可以通过实现高效的并行求和来减少所需的`atomicAdd`调用总数。

1.  肯定是`sum_ker`。很明显 PyCUDA 的求和没有使用与我们相同的硬件技巧，因为我们在较小的数组上表现更好，但通过扩大尺寸，PyCUDA 版本更好的唯一解释是它执行的加法操作更少。

# 第十二章，下一步去哪里

1.  两个例子：DNA 分析和物理模拟。

1.  两个例子：OpenACC，Numba。

1.  TPU 仅用于机器学习操作，缺少呈现图形所需的组件。

1.  以太网。
