# 十二、将这一切结合在一起

虽然我们已经展示了使用 Java 支持数据科学任务的许多方面，但是仍然需要以集成的方式组合和使用这些技术。孤立地使用这些技术是一回事，以连贯的方式使用它们是另一回事。在本章中，我们将为您提供这些技术的额外经验，以及如何将它们结合使用的见解。

具体来说，我们将创建一个基于控制台的应用程序，分析与用户定义的主题相关的 tweets。使用基于控制台的应用程序使我们能够专注于特定于数据科学的技术，并避免选择可能与我们无关的特定 GUI 技术。它提供了一个公共基础，如果需要，可以从这个基础上创建 GUI 实现。

该应用程序执行并演示了以下高级任务:

*   数据采集
*   Data cleaning, including:

    *   删除停用词
    *   Cleaning the text

        *   情感分析
        *   基础数据统计收集
        *   结果显示

这些步骤中的许多步骤可以使用多种类型的分析。我们将展示更相关的方法，并适当提及其他可能性。我们将尽可能使用 Java 8 的特性。

# 定义我们应用的目的和范围

该应用程序将提示用户输入一组选择标准，包括主题和子主题区域，以及要处理的 tweets 数量。执行的分析将简单地计算和显示一个主题和子主题的正面和负面推文的数量。我们使用了通用的情感分析模型，这将影响情感分析的质量。但是，可以添加其他模型和更多分析。

我们将使用 Java 8 流来构建 tweet 数据的处理。它是一串`TweetHandler`对象，我们将很快描述。

我们在这个应用程序中使用了几个类。它们总结如下:

*   这个类保存原始的 tweet 文本和处理所需的特定字段，包括实际的 tweet、用户名和类似的属性。
*   `TwitterStream`:用于获取应用程序的数据。使用特定的类将数据的获取与处理分开。该类拥有几个控制如何获取数据的字段。
*   `ApplicationDriver`:这包含了`main`方法、用户提示和控制分析的`TweetHandler`流。

这些类中的每一个都将在后面的章节中详细介绍。然而，我们将在接下来的`ApplicationDriver`中概述分析过程以及用户如何与应用程序交互。

# 了解应用程序的架构

每个应用程序都有自己独特的结构或架构。这种架构为应用程序提供了总体的组织或框架。对于这个应用程序，我们在`ApplicationDriver`类中使用 Java 8 流来组合这三个类。这个类由三个方法组成:

*   `ApplicationDriver`:包含应用程序的用户输入
*   `performAnalysis`:执行分析
*   `main`:创建`ApplicationDriver`实例

接下来显示了类结构。三个实例变量用于控制处理:

```java
public class ApplicationDriver { 
    private String topic; 
    private String subTopic; 
    private int numberOfTweets; 

    public ApplicationDriver() { ... } 
    public void performAnalysis() { ...     } 

    public static void main(String[] args) { 
        new ApplicationDriver(); 
    } 
} 

```

接下来是`ApplicationDriver`构造函数。创建一个`Scanner`实例并构建情感分析模型:

```java
public ApplicationDriver() { 
    Scanner scanner = new Scanner(System.in); 
    TweetHandler swt = new TweetHandler(); 
    swt.buildSentimentAnalysisModel(); 
    ... 
} 

```

该方法的其余部分提示用户输入，然后调用`performAnalysis`方法:

```java
out.println("Welcome to the Tweet Analysis Application"); 
out.print("Enter a topic: "); 
this.topic = scanner.nextLine(); 
out.print("Enter a sub-topic: "); 
this.subTopic = scanner.nextLine().toLowerCase(); 
out.print("Enter number of tweets: "); 
this.numberOfTweets = scanner.nextInt(); 
performAnalysis(); 

```

`performAnalysis`方法使用从`TwitterStream`实例获得的 Java 8 `Stream`实例。`TwitterStream`类构造函数使用 tweets 的数量和`topic`作为输入。该类在*使用 Twitter 的数据采集*部分讨论:

```java
public void performAnalysis() { 
Stream<TweetHandler> stream = new TwitterStream( 
    this.numberOfTweets, this.topic).stream(); 
    ... 
} 

```

该流使用一系列的`map`、`filter`和一个`forEach`方法来执行处理。`map`方法修改流的元素。`filter`方法从流中移除元素。`forEach`方法将终止流并生成输出。

流的各个方法按顺序执行。当从公共 Twitter 流获取 Twitter 信息时，Twitter 信息以 JSON 文档的形式到达，我们首先对其进行处理。这允许我们提取相关的 tweet 信息，并将数据设置到`TweetHandler`实例的字段中。接下来，推文的文本被转换成小写。只处理英语推文，并且只处理包含子主题的推文。然后处理推文。最后一步计算统计数据:

```java
stream 
        .map(s -> s.processJSON()) 
        .map(s -> s.toLowerCase()) 
        .filter(s -> s.isEnglish()) 
        .map(s -> s.removeStopWords()) 
        .filter(s -> s.containsCharacter(this.subTopic)) 
        .map(s -> s.performSentimentAnalysis()) 
        .forEach((TweetHandler s) -> { 
            s.computeStats(); 
            out.println(s); 
        }); 

```

然后显示处理结果:

```java
out.println(); 
out.println("Positive Reviews: " 
        + TweetHandler.getNumberOfPositiveReviews()); 
out.println("Negative Reviews: " 
        + TweetHandler.getNumberOfNegativeReviews()); 

```

我们在周一晚上的一场足球比赛中测试了我们的应用程序，使用的主题是#MNF。 *#* 符号被称为**标签**，用于对推文进行分类。通过选择一个流行的推文类别，我们确保了我们将有大量的推文数据来处理。为了简单起见，我们选择了足球副题。对于这个例子，我们还选择只分析 50 条推文。以下是我们的提示、输入和输出的简短示例:

```java
Building Sentiment Model
Welcome to the Tweet Analysis Application
Enter a topic: #MNF
Enter a sub-topic: football
Enter number of tweets: 50
Creating Twitter Stream
51 messages processed!
Text: rt @ bleacherreport : touchdown , broncos ! c . j . anderson punches ! lead , 7 - 6 # mnf # denvshou 
Date: Mon Oct 24 20:28:20 CDT 2016
Category: neg
...
Text: i cannot emphasize enough how big td drive . @ broncos offense . needed confidence booster & amp ; just got . # mnf # denvshou 
Date: Mon Oct 24 20:28:52 CDT 2016
Category: pos
Text: least touchdown game . # mnf 
Date: Mon Oct 24 20:28:52 CDT 2016
Category: neg
Positive Reviews: 13
Negative Reviews: 27

```

我们打印出每条推文的文本，以及时间戳和类别。请注意，推文的文本并不总是有意义的。这可能是因为 Twitter 数据的缩写性质，但部分原因是因为该文本已被清理，停用词已被删除。我们仍然应该看到我们的主题，`#MNF`，尽管由于我们的文本清理，它将是小写的。最后，我们打印出分为正面和负面的推文总数。

推文的分类是通过`performSentimentAnalysis`方法完成的。注意，使用情感分析的分类过程并不总是精确的。下面的推文提到了一名丹佛野马队球员触地得分。根据个人对该团队的个人感受，这条推文可以被解释为积极或消极的，但我们的模型将其归类为积极的:

```java
Text: cj anderson td run @ broncos . broncos now lead 7 - 6 . # mnf 
Date: Mon Oct 24 20:28:42 CDT 2016
Category: pos

```

此外，一些推文可能有中性语气，如下图所示，但仍可分为正面或负面。下面这条推文是一个热门体育新闻推特账号的转发，`@bleacherreport`:

```java
Text: rt @ bleacherreport : touchdown , broncos ! c . j . anderson punches ! lead , 7 - 6 # mnf # denvshou 
Date: Mon Oct 24 20:28:37 CDT 2016
Category: neg

```

这条推文被归类为负面，但也许可以被认为是中立的。推文的内容只是提供了一场足球比赛的比分信息。这是积极的还是消极的事件将取决于一个人可能支持哪个队。当我们检查分析的整套 tweet 数据时，我们注意到同一条`@bleacherreport` tweet 被转发了很多次，每次都被归类为负面。当我们考虑到我们可能有大量分类不当的推文时，这可能会扭曲我们的分析。使用不正确的数据会降低结果的准确性。

根据分析的目的，一种选择可能是排除新闻媒体或其他受欢迎的 Twitter 用户的推文。此外，我们可以排除带有 *RT* 的推文，这是一个缩写，表示该推文是另一个用户的转发。

在执行这种类型的分析时，还需要考虑其他问题，包括所使用的子主题。如果我们要分析一个星球大战角色的受欢迎程度，那么我们需要小心我们使用的名字。例如，当选择诸如韩 Solo 的角色名称时，推文可能会使用别名。汉·索洛的别名包括维克·德雷戈、里斯多、杰诺斯·伊达尼安、索洛·贾萨尔、神枪手和乔贝克·琼恩，仅举几个例子([T1)。可能会用演员的名字来代替实际的角色，在韩索罗的情况下是哈里森·福特。我们也可以考虑演员的昵称，比如 Harry 代表 Harrison。](http://starwars.wikia.com/wiki/Category:Han_Solo_aliases)

# 使用 Twitter 获取数据

Twitter API 与 HBC 的 HTTP 客户端结合使用来获取推文，如前面第二章、*数据采集*的[处理 Twitter 部分所述。这个过程包括使用默认访问级别的公共流 API 来提取当前在 Twitter 上流动的公共 tweets 的样本。我们将根据用户选择的关键字来提炼数据。](part0029.xhtml#aid-RL0A2 "Chapter 2. Data Acquisition")

首先，我们声明了`TwitterStream`类。它由两个实例变量(`numberOfTweets`和`topic`)、两个构造函数和一个`stream`方法组成。`numberOfTweets`变量包含要选择和处理的推文数量，而`topic`允许用户搜索与特定主题相关的推文。我们已经设置了默认的构造函数来提取与`Star Wars`相关的`100` tweets:

```java
public class TwitterStream { 
    private int numberOfTweets; 
    private String topic; 

    public TwitterStream() { 
        this(100, "Stars Wars"); 
    } 

    public TwitterStream(int numberOfTweets, String topic) { ... } 

} 

```

我们的`TwitterStream`类的核心是`stream`方法。我们首先使用创建 Twitter 应用程序时 Twitter 提供的信息执行身份验证。然后我们创建一个`BlockingQueue`对象来保存我们的流数据。在本例中，我们将设置默认容量`1000`。我们在`trackTerms`方法中使用`topic`变量来指定我们要搜索的推文类型。最后，我们指定我们的`endpoint`并关闭失速警告:

```java
String myKey = "mySecretKey"; 
String mySecret = "mySecret"; 
String myToken = "myToKen"; 
String myAccess = "myAccess"; 

out.println("Creating Twitter Stream"); 
BlockingQueue<String> statusQueue = new  
LinkedBlockingQueue<>(1000); 
StatusesFilterEndpoint endpoint = new StatusesFilterEndpoint(); 
endpoint.trackTerms(Lists.newArrayList("twitterapi", this.topic)); 
endpoint.stallWarnings(false); 

```

现在我们可以使用`OAuth1`创建一个`Authentication`对象，这是`OAuth`类的一个变种。这允许我们构建我们的连接客户端并完成 HTTP 连接:

```java
Authentication twitterAuth = new OAuth1(myKey, mySecret, myToken,
  myAccess); 

BasicClient twitterClient = new ClientBuilder() 
        .name("Twitter client") 
        .hosts(Constants.STREAM_HOST) 
        .endpoint(endpoint) 
        .authentication(twitterAuth) 
        .processor(new StringDelimitedProcessor(statusQueue)) 
        .build(); 

twitterClient.connect(); 

```

接下来，我们创建两个数组列表，`list`保存我们的`TweetHandler`对象，`twitterList`保存从 Twitter 流出的 JSON 数据。我们将在下一节讨论`TweetHandler`对象。我们使用`drainTo`方法代替[第 2 章](part0029.xhtml#aid-RL0A2 "Chapter 2. Data Acquisition")、*数据采集*中演示的`poll`方法，因为它对于大量数据更有效:

```java
List<TweetHandler> list = new ArrayList(); 
List<String> twitterList = new ArrayList(); 

```

接下来，我们遍历检索到的消息。我们调用`take`方法从`BlockingQueue`实例中移除每个字符串消息。然后，我们使用该消息创建一个新的`TweetHandler`对象，并将其放入我们的`list`中。在我们处理完所有消息并且 for 循环完成之后，我们停止 HTTP 客户端，显示消息的数量，并返回我们的`TweetHandler`对象流:

```java
statusQueue.drainTo(twitterList); 
for(int i=0; i<numberOfTweets; i++) { 
    String message; 
    try { 
        message = statusQueue.take(); 
        list.add(new TweetHandler(message)); 
    } catch (InterruptedException ex) { 
        ex.printStackTrace(); 
    } 
} 
twitterClient.stop(); 
out.printf("%d messages processed!\n",     
    twitterClient.getStatsTracker().getNumMessages()); 

return list.stream(); 
} 

```

我们现在准备清理和分析我们的数据。

# 了解 TweetHandler 类

`TweetHandler`类保存关于特定 tweet 的信息。它获取原始的 JSON tweet，并提取与应用程序需求相关的部分。它还拥有处理 tweet 文本的方法，比如将文本转换成小写，删除不相关的 tweet。该类的第一部分如下所示:

```java
public class TweetHandler { 
    private String jsonText; 
    private String text; 
    private Date date; 
    private String language; 
    private String category; 
    private String userName; 
    ... 
    public TweetHandler processJSON() { ... } 
    public TweetHandler toLowerCase(){ ... } 
    public TweetHandler removeStopWords(){ ... }     
    public boolean isEnglish(){ ... }     
    public boolean containsCharacter(String character) { ... }        
    public void computeStats(){ ... } 
    public void buildSentimentAnalysisModel{ ... } 
    public TweetHandler performSentimentAnalysis(){ ... } 
} 

```

实例变量显示了从 tweet 中检索并处理的数据类型，如下所示:

*   `jsonText`:原始 JSON 文本
*   `text`:处理后的 tweet 的文本
*   `date`:推文的日期
*   推文的语言
*   `category`:推文分类，正面还是负面
*   `userName`:Twitter 用户的名字

该类还使用了其他几个实例变量。以下内容用于创建和使用情感分析模型。分类器静态变量指的是模型:

```java
private static String[] labels = {"neg", "pos"}; 
private static int nGramSize = 8; 
private static DynamicLMClassifier<NGramProcessLM>  
    classifier = DynamicLMClassifier.createNGramProcess( 
        labels, nGramSize); 

```

默认构造函数用于提供一个实例来构建情感模型。单参数构造函数使用原始 JSON 文本创建一个`TweetHandler`对象:

```java
    public TweetHandler() { 
        this.jsonText = ""; 
    } 

    public TweetHandler(String jsonText) { 
        this.jsonText = jsonText; 
    } 

```

其余的方法将在下面的章节中讨论。

## 为情感分析模型提取数据

在[第九章](part0068.xhtml#aid-20R681 "Chapter 9. Text Analysis")、*文本分析*中，我们使用 DL4J 进行了情感分析。在这个例子中，我们将使用 LingPipe 作为前面方法的替代方法。因为我们想要对 Twitter 数据进行分类，所以我们选择了一个包含预先分类的推文的数据集，可以在[http://thinknook . com/WP-content/uploads/2012/09/perspective-Analysis-dataset . zip](http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip)获得。在继续我们的应用程序开发之前，我们必须完成一个一次性的过程，将这些数据提取为我们的模型可以使用的格式。

这个数据集存在于一个大的`.csv`文件中，每行有一条 tweet 和分类。推文被分为`0`(负面)或`1`(正面)。以下是该数据文件中一行的示例:

```java
95,0,Sentiment140, - Longest night ever.. ugh!    http://tumblr.com/xwp1yxhi6 

```

第一个元素代表一个惟一的 ID 号，它是原始数据集的一部分，我们将用它作为文件名。第二个元素是分类，第三个是数据集标签(在本项目中被忽略)，最后一个元素是实际的 tweet 文本。在将这些数据用于我们的 LingPipe 模型之前，我们必须将每条 tweet 写入一个单独的文件。为此，我们创建了三个字符串变量。根据每条推文的分类，`filename`变量将被赋予`pos`或`neg`，并将用于写操作。我们还使用`file`变量保存单个 tweet 文件的名称，使用`text`变量保存单个 tweet 文本。接下来，我们使用`readAllLines`方法和`Paths`类的`get`方法将数据存储在`List`对象中。我们还需要指定字符集`StandardCharsets.ISO_8859_1`:

```java
try { 
    String filename; 
    String file; 
    String text; 
    List<String> lines = Files.readAllLines( 
Paths.get("\\path-to-file\\SentimentAnalysisDataset.csv"),  
StandardCharsets.ISO_8859_1); 
    ... 

} catch (IOException ex) { 
    // Handle exceptions 
} 

```

现在我们可以循环遍历我们的列表，并使用`split`方法将我们的`.csv`数据存储在一个字符串数组中。我们将位置`1`的元素转换为整数，并确定它是否是一个`1`。用`1`分类的推文被认为是正面推文，我们将`filename`设置为`pos`。所有其他推文将`filename`设置为`neg`。我们从位置`0`的元素中提取输出文件名，从元素`3`中提取文本。出于本项目的目的，我们忽略位置`2`的标签。最后，我们写出我们的数据:

```java
for(String s : lines) { 
    String[] oneLine = s.split(","); 
    if(Integer.parseInt(oneLine[1])==1) { 
        filename = "pos"; 
    } else { 
        filename = "neg"; 
    } 
    file = oneLine[0]+".txt"; 
    text = oneLine[3]; 
    Files.write(Paths.get( 
        path-to-file\\txt_sentoken"+filename+""+file), 
        text.getBytes()); 
} 

```

注意，我们在`txt_sentoken`目录中创建了`neg`和`pos`目录。当我们读取文件来构建模型时，这个位置很重要。

## 建立情感模型

现在我们已经准备好构建我们的模型了。我们遍历包含`pos`和`neg`的`labels`数组，并为每个标签创建一个新的`Classification`对象。然后我们使用这个标签创建一个新文件，并使用`listFiles`方法创建一个文件名数组。接下来，我们将使用一个`for`循环遍历这些文件名:

```java
public void buildSentimentAnalysisModel() { 
    out.println("Building Sentiment Model"); 

    File trainingDir = new File("\\path to file\\txt_sentoken"); 
    for (int i = 0; i < labels.length; i++) { 
        Classification classification =  
            new Classification(labels[i]); 
        File file = new File(trainingDir, labels[i]); 
        File[] trainingFiles = file.listFiles(); 
        ... 
    } 
} 

```

在`for`循环中，我们提取 tweet 数据并将其存储在我们的字符串`review`中。然后我们使用`review`和`classification`创建一个新的`Classified`对象。最后我们可以调用`handle`方法来分类这个特殊的文本:

```java
for (int j = 0; j < trainingFiles.length; j++) { 
    try { 
        String review = Files.readFromFile(trainingFiles[j],  
            "ISO-8859-1"); 
        Classified<CharSequence> classified = new  
            Classified<>(review, classification); 
        classifier.handle(classified); 
    } catch (IOException ex) { 
        // Handle exceptions 
    } 
 } 

```

对于上一节中讨论的数据集，此过程可能需要相当长的时间。然而，我们认为这种时间权衡是值得的，这种训练数据使分析质量成为可能。

## 处理 JSON 输入

Twitter 数据是使用 JSON 格式检索的。我们将使用 Twitter 4j([http://twitter4j.org](http://twitter4j.org))提取 tweet 的相关部分，并存储在`TweetHandler`类的相应字段中。

`TweetHandler`类的`processJSON`方法执行实际的数据提取。基于 JSON 文本创建了一个`JSONObject`的实例。该类拥有几种从对象中提取特定类型数据的方法。我们使用`getString`方法来获得我们需要的字段。

接下来显示了`processJSON`方法的开始，我们从获取`JSONObject`实例开始，我们将使用它来提取 tweet 的相关部分:

```java
public TweetHandler processJSON() { 
    try { 
        JSONObject jsonObject = new JSONObject(this.jsonText); 
        ... 
    } catch (JSONException ex) { 
        // Handle exceptions 
    } 
    return this; 
} 

```

首先，我们提取推文的文本，如下所示:

```java
this.text = jsonObject.getString("text"); 

```

接下来，我们提取推文的日期。我们使用`SimpleDateFormat`类将日期字符串转换成一个`Date`对象。它的构造函数被传递一个指定日期字符串格式的字符串。我们使用了字符串`"EEE MMM d HH:mm:ss Z yyyy"`，其组成部分将在下面详述。字符串元素的顺序对应于 JSON 实体中的顺序:

*   `EEE`:用三个字符指定的星期几
*   `MMM`:月份，三位字符
*   `d`:一个月中的某一天
*   `HH:mm:ss`:时、分、秒
*   `Z`:时区
*   `yyyy`:年份

代码如下:

```java
SimpleDateFormat sdf = new SimpleDateFormat( 
    "EEE MMM d HH:mm:ss Z yyyy"); 
try { 
    this.date = sdf.parse(jsonObject.getString("created_at")); 
} catch (ParseException ex) { 
    // Handle exceptions 
} 

```

剩余的字段将被提取，如下所示。我们必须提取一个中间 JSON 对象来提取`name`字段:

```java
this.language = jsonObject.getString("lang"); 
JSONObject user = jsonObject.getJSONObject("user"); 
this.userName = user.getString("name"); 

```

获取并提取了文本后，我们现在准备执行清理数据的重要任务。

## 清理数据以改善我们的结果

数据清理是大多数数据科学问题的关键步骤。没有正确清理的数据可能会有错误，如拼写错误、日期等元素的不一致表示以及无关的单词。

我们可以对 Twitter 数据应用许多数据清理选项。对于这个应用程序，我们执行简单的清理。另外，我们会过滤掉某些推文。

文本到小写字母的转换很容易实现，如下所示:

```java
    public TweetHandler toLowerCase() { 
        this.text = this.text.toLowerCase().trim(); 
        return this; 
    } 

```

这个过程的一部分是删除某些不需要的推文。例如，下面的代码说明了如何检测推文是否是英文的，以及它是否包含用户感兴趣的子主题。Java 8 流中的`filter`方法使用了`boolean`返回值，它执行实际的删除:

```java
    public boolean isEnglish() { 
        return this.language.equalsIgnoreCase("en"); 
    } 

    public boolean containsCharacter(String character) { 
        return this.text.contains(character); 
    } 

```

许多其他清理操作可以很容易地添加到该过程中，如删除前导和尾随空白，替换制表符，以及验证日期和电子邮件地址。

## 删除停用词

停用词是那些无助于理解或处理数据的词。典型的停用词包括 0、and、a 和 or。当它们对数据处理没有贡献时，它们可以被移除以简化处理并使其更有效。

有几种去除停用词的技巧，在[第 9 章](part0068.xhtml#aid-20R681 "Chapter 9. Text Analysis")、*文本分析*中讨论。对于这个应用程序，我们将使用 LingPipe([http://alias-i.com/lingpipe/](http://alias-i.com/lingpipe/))来删除停止字。我们使用`EnglishStopTokenizerFactory` 类来获得基于`IndoEuropeanTokenizerFactory`实例的停用词模型:

```java
public TweetHandler removeStopWords() { 
    TokenizerFactory tokenizerFactory 
            = IndoEuropeanTokenizerFactory.INSTANCE; 
    tokenizerFactory =  
        new EnglishStopTokenizerFactory(tokenizerFactory); 
    ... 
    return this; 
} 

```

提取一系列不包含停用词的标记，并使用一个`StringBuilder`实例创建一个字符串来替换原始文本:

```java
Tokenizer tokens = tokenizerFactory.tokenizer( 
        this.text.toCharArray(), 0, this.text.length()); 
StringBuilder buffer = new StringBuilder(); 
for (String word : tokens) { 
    buffer.append(word + " "); 
} 
this.text = buffer.toString(); 

```

我们使用的 LingPipe 模型可能不是最适合所有推文的。此外，有人提出，从推文中删除停用词可能不会有成效([http://oro.open.ac.uk/40666/](http://oro.open.ac.uk/40666/))。可以将选择各种停用词以及是否应该删除停用词的选项添加到流过程中。

## 执行情感分析

现在，我们可以使用本章“构建情感模型”一节中构建的模型来执行情感分析。我们通过将清理后的文本传递给`classify`方法来创建一个新的`Classification`对象。然后我们使用`bestCategory`方法将我们的文本分为正面或负面。最后，我们将`category`设置为结果，并返回`TweetHandler`对象:

```java
public TweetHandler performSentimentAnalysis() { 
    Classification classification =  
        classifier.classify(this.text); 
    String bestCategory = classification.bestCategory(); 
    this.category = bestCategory; 
    return this; 
} 

```

我们现在准备分析我们的应用程序的结果。

## 分析结果

这个应用程序中执行的分析相当简单。一旦推文被分类为正面或负面，总数就会被计算出来。为此，我们使用了两个静态变量:

```java
    private static int numberOfPositiveReviews = 0; 
    private static int numberOfNegativeReviews = 0; 

```

从 Java 8 流中调用`computeStats`方法，并增加适当的变量:

```java
public void computeStats() { 
    if(this.category.equalsIgnoreCase("pos")) { 
        numberOfPositiveReviews++; 
    } else { 
        numberOfNegativeReviews++; 
    } 
} 

```

两种`static`方法提供对评论数量的访问:

```java
public static int getNumberOfPositiveReviews() { 
    return numberOfPositiveReviews; 
} 

public static int getNumberOfNegativeReviews() { 
    return numberOfNegativeReviews; 
} 

```

此外，还提供了一个简单的`toString`方法来显示基本的 tweet 信息:

```java
public String toString() { 
    return "\nText: " + this.text 
            + "\nDate: " + this.date 
            + "\nCategory: " + this.category; 
} 

```

可以根据需要添加更复杂的分析。这个应用程序的目的是演示一种组合各种数据处理任务的技术。



# 其他可选配件

可以对应用程序进行许多改进。其中许多是用户偏好，其他的与改进应用程序的结果有关。GUI 界面在许多情况下都很有用。在用户选项中，我们可能希望添加对以下内容的支持:

*   显示个人推文
*   允许空的子主题
*   处理其他 tweet 字段
*   提供主题或子主题列表，供用户选择
*   生成附加统计数据和支持图表

关于过程结果的改进，应考虑以下几点:

*   纠正拼写错误的用户条目
*   删除标点符号周围的空格
*   使用替代的停用字词删除技术
*   使用替代的情感分析技术

这些增强的细节取决于所使用的 GUI 界面以及应用程序的目的和范围。



# 总结

本章旨在说明如何将各种数据科学任务集成到应用程序中。我们选择了一个处理推文的应用程序，因为它是一个流行的社交媒体，允许我们应用前面章节中讨论的许多技术。

使用了一个简单的基于控制台的界面，以避免特定但可能不相关的 GUI 细节扰乱讨论。应用程序提示用户输入 Twitter 主题、子主题和要处理的 tweets 数量。分析包括确定推文的情绪，以及关于推文积极或消极性质的简单统计。

这个过程的第一步是建立一个情感模型。我们使用 LingPipe 类来构建模型并执行分析。使用了 Java 8 流，它支持流畅的编程风格，可以轻松地添加和删除各个处理步骤。

一旦流被创建，JSON 原始文本被处理并用于初始化一个`TweetHandler`类。这个类的实例随后被修改，包括将文本转换成小写，删除非英语 tweet，删除停用词，以及只选择包含子主题的 tweet。然后进行情感分析，接着进行统计数据的计算。

数据科学是一个广泛的主题，利用了大量的统计和计算机科学主题。在本书中，我们简要介绍了这些主题，以及 Java 是如何支持它们的。