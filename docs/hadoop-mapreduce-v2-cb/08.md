# 八、搜索和索引

在本章中，我们将介绍以下食谱：

*   使用 Hadoop MapReduce 生成倒排索引
*   使用 Apache Nutch 进行域内 Web 爬行
*   使用 Apache Solr 对 Web 文档进行索引和搜索
*   将 Apache HBase 配置为 Apache Nutch 的后端数据存储
*   使用 Hadoop/HBase 群集使用 Apache Nutch 进行全 Web 爬行
*   用于索引和搜索的 ElasticSearch
*   为爬行网页生成内链接图

# 简介

MapReduce 框架非常适合大规模搜索和索引应用。 事实上，谷歌提出了最初的 MapReduce 框架，专门用来简化网络搜索涉及的各种操作。 Apache Hadoop 项目也是作为**Apache Nutch**搜索引擎的子项目开始的，然后作为单独的顶级项目衍生出来。

网络搜索由获取、索引、排序和检索组成。 考虑到数据量非常大，所有这些操作都需要可伸缩。 此外，检索的延迟也应该很低。 通常，获取是通过 Web 爬行执行的，其中爬行器获取获取队列中的一组页面，从获取的页面中提取链接，将提取的链接添加回获取队列，并重复此过程多次。 索引以快速高效的方式解析、组织和存储获取的数据，以便于查询和检索。 搜索引擎基于诸如 PageRank 之类的算法对文档进行离线排名，并基于查询参数对结果进行实时排名。

在本章中，我们将介绍几个可用于 Apache Hadoop 来执行大规模搜索和索引的工具。

### 提示

**示例代码**

本书的示例代码文件位于 gihub 的[https://github.com/thilg/hcb-v2](https://github.com/thilg/hcb-v2)。 `chapter8`文件夹代码库包含本章的示例代码。

可以通过在代码库的`chapter8`文件夹中发出`gradle build`命令来编译和构建示例代码。 Eclipse IDE 的项目文件可以通过在代码库的主文件夹中运行`gradle eclipse`命令来生成。 IntelliJ IDEA IDE 的项目文件可以通过在代码存储库的主文件夹中运行`gradle idea`命令来生成。

# 使用 Hadoop MapReduce 生成倒排索引

Simple 文本搜索系统依靠倒排索引来查找包含给定词或术语的文档集。 在本食谱中，我们实现了一个简单的倒排索引构建应用，该应用计算文档中的术语列表、包含每个术语的文档集以及每个文档中的术语频率。 从倒排索引检索结果可以像返回包含给定项的文档集一样简单，也可以涉及复杂得多的操作，例如返回基于特定排名排序的文档集。

## 做好准备

您必须配置并安装 Apache Hadoopv2 才能遵循本指南。 编译和构建源代码需要 Gradle。

## 怎么做……

在以下步骤中，我们将使用 MapReduce 程序为文本数据集构建倒排索引：

1.  Create a directory in HDFS and upload a text dataset. This dataset should consist of one or more text files.

    ```scala
    $ hdfs dfs -mkdir input_dir
    $ hdfs dfs -put *.txt input_dir

    ```

    ### 备注

    您可以按照[http://www.gutenberg.org/wiki/Gutenberg:Information_About_Robot_Access_to_our_Pages](http://www.gutenberg.org/wiki/Gutenberg:Information_About_Robot_Access_to_our_Pages)给出的说明下载古腾堡计划书籍的文本版本。 请确保提供下载请求的`filetypes`查询参数为`txt`。 解压缩下载的文件。 您可以使用解压缩的文本文件作为此食谱的文本数据集。

2.  通过从源资料库的`chapter 8`文件夹运行`gradle build`命令，编译源。
3.  使用以下命令运行倒排索引 MapReduce 作业。 提供在步骤 2 中上载输入数据的 HDFS 目录作为第一个参数，并提供 HDFS 路径以存储输出作为第二个参数：

    ```scala
    $ hadoop jar hcb-c8-samples.jar \
     chapter8.invertindex.TextOutInvertedIndexMapReduce \
     input_dir output_dir

    ```

4.  通过运行以下命令检查输出目录中的结果。 此程序的输出将由术语后跟逗号分隔的文件名和频率列表组成：

    ```scala
    $ hdfs dfs -cat output_dir/*
    ARE three.txt:1,one.txt:1,four.txt:1,two.txt:1,
    AS three.txt:2,one.txt:2,four.txt:2,two.txt:2,
    AUGUSTA three.txt:1,
    About three.txt:1,two.txt:1,
    Abroad three.txt:2,

    ```

5.  为了更清楚地理解算法，我们在步骤 3 中使用了文本输出倒排索引 MapReduce 程序。 `chapter8`存储库的源文件夹中的`chapter8/invertindex/InvertedIndexMapReduce.java`MapReduce 程序使用 Hadoop`SequenceFiles`和`MapWritable`类输出倒排索引。 该索引对机器处理更友好，存储效率更高。 您可以通过将步骤 3 中的命令替换为以下命令来运行此版本的程序：

    ```scala
    $ hadoop jar hcb-c8-samples.jar \
     chapter8.invertindex.InvertedIndexMapReduce \
     input_dir seq_output_dir

    ```

## 它是如何工作的.

Map 函数接收输入文档的一大块作为输入，并为每个单词输出术语和`<docid,1>`对。 在 Map 函数中，我们首先替换输入文本值中的所有非字母数字字符，然后再对其进行标记，如下所示：

```scala
public void map(Object key, Text value, ……… {
  String valString = value.toString().replaceAll("[^a-zA-Z0-9]+"," ");
  StringTokenizer itr = new StringTokenizer(valString);
   StringTokenizer(value.toString());

  FileSplit fileSplit = (FileSplit) context.getInputSplit();
  String fileName = fileSplit.getPath().getName();
  while (itr.hasMoreTokens()) {
    term.set(itr.nextToken());
    docFrequency.set(fileName, 1);
    context.write(term, docFrequency);
  }
}
```

我们使用`MapContext`的`getInputSplit()`方法来获取对分配给当前 Map 任务的`InputSplit`的引用。 此计算的`InputSplits`类是`FileSplit`的实例，这是因为使用了基于`FileInputFormat`的`InputFormat`。 然后，我们使用`FileSplit`的`getPath()`方法获取包含当前拆分的文件的路径，并从中提取文件名。 在构建倒排索引时，我们使用这个提取的文件名作为文档 ID。

`Reduce`函数接收包含术语(键)作为输入的所有文档的 ID 和频率。 然后，`Reduce`函数将该术语和文档 ID 列表以及该术语在每个文档中出现的次数作为输出输出：

```scala
public void reduce(Text key, Iterable<TermFrequencyWritable> values,Context context) …………{

  HashMap<Text, IntWritable> map = new HashMap<Text, IntWritable>();
  for (TermFrequencyWritable val : values) {
    Text docID = new Text(val.getDocumentID());
    int freq = val.getFreq().get();
    if (map.get(docID) != null) {
      map.put(docID, new IntWritable(map.get(docID).get() + freq));
    } else {
      map.put(docID, new IntWritable(freq));
    }
  }
  MapWritable outputMap = new MapWritable();
  outputMap.putAll(map);
  context.write(key, outputMap);
}
```

在前面的模型中，我们为每个单词输出一条记录，在 Map 任务和 Reduce 任务之间生成大量中间数据。 我们使用以下合并器聚合 Map 任务发出的术语，从而减少需要在 Map 和 Reduce 任务之间传输的中间数据量：

```scala
public void reduce(Text key, Iterable<TermFrequencyWritable> values …… {
  int count = 0;
  String id = "";
  for (TermFrequencyWritable val : values) {
    count++;
    if (count == 1) {
      id = val.getDocumentID().toString();
    }
  }
  TermFrequencyWritable writable = new TermFrequencyWritable();
  writable.set(id, count);
  context.write(key, writable);
}
```

在驱动程序中，我们设置了 Mapper、Reducer 和 Combiner 类。 此外，当我们为 Map 任务和 Reduce 任务使用不同的值类型时，我们同时指定了 Output Value 和 MapOutput Value 属性。

```scala
…
job.setMapperClass(IndexingMapper.class);
job.setReducerClass(IndexingReducer.class);
job.setCombinerClass(IndexingCombiner.class);
…
job.setMapOutputValueClass(TermFrequencyWritable.class);
job.setOutputValueClass(MapWritable.class);
job.setOutputFormatClass(SequenceFileOutputFormat.class);
```

## 还有更多...

我们可以通过执行诸如过滤停用词、用词干替换词、存储更多关于词的上下文的信息等优化来改进该索引程序，从而使索引成为一个复杂得多的问题。 幸运的是，有几个开源索引框架可以用于索引目的。 本章后面的食谱将探索使用 Apache Solr 和 Elasticsearch 进行索引，它们基于 Apache Lucene 索引引擎。

下一节将介绍如何使用`MapFileOutputFormat`以索引随机访问的方式存储`InvertedIndex`。

### 输出随机可访问索引 InvertedIndex

Apache Hadoop 支持一种名为**MapFile**的文件格式，可用于将索引存储到 SequenceFiles 中存储的数据中。 当我们需要随机访问存储在大型 SequenceFile 中的记录时，MapFile 非常有用。 您可以使用**MapFileOutputFormat**格式来输出 MapFiles，它将由一个包含实际数据的 SequenceFile 和另一个包含 SequenceFile 索引的文件组成。

`chapter8`源文件夹中的`chapter8/invertindex/MapFileOutInvertedIndexMR.java`MapReduce 程序利用 MapFiles 将二级索引存储到倒排索引中。 您可以使用以下命令执行该程序。 第三个参数(`sample_lookup_term`)应该是输入数据集中的单词：

```scala
$ hadoop jar hcb-c8-samples.jar \
 chapter8.invertindex.MapFileOutInvertedIndexMR \
 input_dir indexed_output_dir sample_lookup_term

```

如果您选中`indexed_output_dir`，您将能够看到名为`part-r-xxxxx`的文件夹，每个文件夹包含一个`data`和一个`index`文件。 我们可以将这些索引加载到 MapFileOutputFormat，并对数据执行随机查找。 在`MapFileOutInvertedIndexMR.java`程序中给出了一个使用此方法的简单查找示例，如下所示：

```scala
MapFile.Reader[] indexReaders = MapFileOutputFormat.getReaders(new Path(args[1]), getConf());
MapWritable value = new MapWritable();
Text lookupKey = new Text(args[2]);
// Performing the lookup for the values if the lookupKey
Writable map = MapFileOutputFormat.getEntry(indexReaders, new HashPartitioner<Text, MapWritable>(), lookupKey, value);
```

为了使用此功能，您需要通过设置以下属性来确保禁止 Hadoop 在`output`文件夹中写入 `_SUCCESS`文件。 使用 MapFileOutputFormat 查找索引中的值时，`_SUCCESS`文件的存在可能会导致错误：

```scala
job.getConfiguration().setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);
```

## 另请参阅

*   在[第 10 章](10.html "Chapter 10. Mass Text Data Processing")，*海量文本数据处理*中，*为文本数据*配方创建 TF 和 TF-IDF 矢量。

# 使用 Apache Nutch 进行域内网络爬行

**网络爬行**是访问和下载 Internet 上的所有或部分网页的过程。 虽然爬行和实现简单爬行器的概念听起来很简单，但构建一个成熟的爬行器需要大量的工作。 一个成熟的爬行器需要是分布式的，必须遵守最佳实践，如不使服务器过载并遵守`robots.txt`、执行定期爬行、确定要爬行的页面的优先顺序、识别多种格式的文档等。 Apache Nutch 是一个开源搜索引擎，它提供了一个高度可伸缩的爬行器。 Apache Nutch 提供了礼貌、健壮性和可伸缩性等特性。

在本食谱中，我们将在独立模式下使用 Apache Nutch 进行小规模的域内 Web 爬行。 几乎所有的 Nutch 命令都是作为 Hadoop MapReduce 应用实现的，正如您在执行本菜谱的步骤 10 到 18 时会注意到的那样。 Nutch Standalone 在本地模式下使用 Hadoop 执行这些应用。

本食谱以[http://wiki.apache.org/nutch/NutchTutorial](http://wiki.apache.org/nutch/NutchTutorial)给出的说明为基础。

## 做好准备

设置`JAVA_HOME`环境变量。 安装 Apache Ant 并将其添加到`PATH`环境变量。

## 怎么做……

以下步骤显示了如何在独立模式下使用 Apache Nutch 进行小规模 Web 爬网：

1.  Apache Nutch 独立模式使用 HyperSQL 数据库作为默认数据存储。 从[http://sourceforge.net/projects/hsqldb/](http://sourceforge.net/projects/hsqldb/)下载 HyperSQL。 解压缩发行版并转到数据目录：

    ```scala
    $ cd hsqldb-2.3.2/hsqldb

    ```

2.  使用以下命令启动 HyperSQL 数据库。 以下数据库使用`data/nutchdb.*`作为数据库文件，并使用`nutchdb`作为数据库别名。 在步骤 7：

    ```scala
    $ java -cp lib/hsqldb.jar \
    org.hsqldb.server.Server \
    --database.0 file:data/nutchdb \
    --dbname.0 nutchtest
    ......
    [Server@79616c7]: Database [index=0, id=0, db=file:data/nutchdb, alias=nutchdb] opened sucessfully in 523 ms.
    ......

    ```

    中，我们将在`gora.sqlstore.jdbc.url`属性中使用此数据库别名
3.  从[http://nutch.apache.org/](http://nutch.apache.org/)下载 Apache Nutch 2.2.1 并解压缩。
4.  Go to the extracted directory, which we will refer as `NUTCH_HOME`. Change the `gora-core` dependency version to 0.2.1 and uncomment the `gora-sql` dependency by modifying the `Gora artifacts` section of the `ivy`/`ivy.xml` file as follows:

    ```scala
    <!--================-->
    <!-- Gora artifacts -->
    <!--================-->
    <dependency org="org.apache.gora" name="gora-core" rev="0.2.1" conf="*->default"/>

    <dependency org="org.apache.gora" name="gora-sql" rev="0.1.1-incubating" conf="*->default" />
    ```

    ### 备注

    您还可以通过更新`conf/gora.properties`文件的`Default SqlStore properties`部分中的必要数据库配置，将 MySQL 数据库用作 Nutch 独立模式 Web 爬网的后端数据库。 您还必须取消注释`ivy`/`ivy.xml`文件的`Gora artifacts`部分中的`mysql-connector-java`依赖项。

5.  使用以下命令构建 Apache Nutch：

    ```scala
    $ ant runtime

    ```

6.  确保在`NUTCH_HOME/runtime/local/conf/gora.properties`文件中包含以下内容。 提供步骤 2 中使用的数据库别名：

    ```scala
    ###############################
    # Default SqlStore properties #
    ###############################
    gora.sqlstore.jdbc.driver=org.hsqldb.jdbc.JDBCDriver
    gora.sqlstore.jdbc.url=jdbc:hsqldb:hsql://localhost/nutchtest
    gora.sqlstore.jdbc.user=sa
    ```

7.  转到`runtime/local`目录并运行`bin/nutch`命令以验证 Nutch 安装。 成功的安装将打印出如下的 Nutch 命令列表：

    ```scala
    $ cd runtime/local
    $ bin/nutch 
    Usage: nutch COMMAND
    where COMMAND is one of:…..

    ```

8.  将以下内容添加到`NUTCH_HOME/runtime/local/conf/nutch-site.xml`。 您可以为指定`http.agent.name`：

    ```scala
    <configuration>
    <property>
      <name>storage.data.store.class</name>
      <value>org.apache.gora.sql.store.SqlStore</value>
    </property>
    <property>
      <name>http.agent.name</name>
      <value>NutchCrawler</value>
    </property>
    <property>
      <name>http.robots.agents</name>
      <value>NutchCrawler,*</value>
    </property>
    </configuration>
    ```

    的值的任何名称
9.  您可以通过编辑位于`NUTCH_HOME/runtime/local/conf/`的`regex-urlfiler.txt`文件来限制要爬网的域名。 例如，为了将域限制为[http://apache.org](http://apache.org)，请在`NUTCH_HOME/runtime/local/conf/regex-urlfilter.txt`处替换以下行：

    ```scala
    # accept anything else
    +.
    ```

10.  使用以下正则表达式：

    ```scala
    +^http://([a-z0-9]*\.)*apache.org/
    ```

11.  创建名为`urls`的目录，并在该目录内创建名为`seed.txt`的文件。 将您的种子 URL 添加到此文件。 种子 URL 用于开始爬网，并且将是最先爬网的页面。 在下面的示例中，我们使用[http://apache.org](http://apache.org)作为种子 URL：

    ```scala
    $ mkdir urls
    $ echo http://apache.org/ > urls/seed.txt

    ```

12.  使用以下命令将种子 URL 注入 Nutch 数据库：

    ```scala
    $ bin/nutch inject urls/
    InjectorJob: starting
    InjectorJob: urlDir: urls
    ……
    Injector: finished

    ```

13.  使用以下命令验证种子是否注入到 Nutch 数据库。 此命令打印的`TOTAL urls`应与您的`seed.txt`文件中的 URL 数量相匹配。 您还可以在后面的周期中使用以下命令来了解数据库中的网页条目数量：

    ```scala
    $ bin/nutch readdb  -stats
    WebTable statistics start
    Statistics for WebTable: 
    min score:  1.0
    ....
    TOTAL urls:  1

    ```

14.  使用以下命令从注入的种子 URL 生成获取列表。 这将准备在爬行的第一个周期中要获取的网页列表。 生成将为当前生成的获取列表分配一个批处理 ID，该获取列表可在后续命令中使用：

    ```scala
    $ bin/nutch generate –topN 1
    GeneratorJob: Selecting best-scoring urls due for fetch.
    GeneratorJob: starting
    GeneratorJob: filtering: true
    GeneratorJob: done
    GeneratorJob: generated batch id: 1350617353-1356796157

    ```

15.  使用以下命令获取在步骤 12 中准备的页面列表。此步骤执行网页的实际获取。 参数`–all`用于通知 Nutch 获取所有生成的批次：

    ```scala
    $ bin/nutch fetch -all
    FetcherJob: starting
    FetcherJob: fetching all
    FetcherJob: threads: 10
    ......

    fetching http://apache.org/
    ......

    -activeThreads=0
    FetcherJob: done

    ```

16.  使用下面的命令从抓取的网页中解析并提取有用的数据，如页面的文本内容、页面的元数据、从抓取的页面链接的页面集合等。 我们将从获取的页面链接的一组页面称为该特定获取的页面的外部链接。 外部链接数据将用于发现要获取的新页面，以及使用链接分析算法(如 PageRank：

    ```scala
    $ bin/nutch parse -all
    ParserJob: starting
    ......
    ParserJob: success

    ```

    )对页面进行排名
17.  执行以下命令，使用上一步中提取的数据更新 Nutch 数据库。 此步骤包括更新获取的页面的内容以及添加通过获取的页面中包含的链接发现的页面的新条目。

    ```scala
    $ bin/nutch updatedb
    DbUpdaterJob: starting
    ……
    DbUpdaterJob: done

    ```

18.  执行以下命令，使用先前获取的数据中的信息生成新的获取列表。 `topN`参数限制为下一个提取周期生成的 URL 数量：

    ```scala
    $ bin/nutch generate -topN 100
    GeneratorJob: Selecting best-scoring urls due for fetch.
    GeneratorJob: starting
    ......
    GeneratorJob: done
    GeneratorJob: generated batch id: 1350618261-1660124671

    ```

19.  获取新列表，对其进行解析，然后更新数据库。

    ```scala
    $ bin/nutch fetch –all
    ......
    $ bin/nutch parse -all 
    ......
    $ bin/nutch updatedb
    ......

    ```

20.  重复步骤 17 和 18，直到从起始 URL 获得所需页数或所需深度。

## 另请参阅

*   *使用 Hadoop/HBase 集群的 Apache Nutch 进行的*全网爬行*和使用 Apache Solr*食谱索引和搜索 Web 文档的*。*
*   有关使用 HyperSQL 的更多信息，请参考[http://www.hsqldb.org/doc/2.0/guide/index.html](http://www.hsqldb.org/doc/2.0/guide/index.html)。

# 使用 Apache Solr 索引和搜索 Web 文档

**Apache Solr**是一个开源搜索平台，是**Apache Lucene**项目的一部分。 它支持强大的全文搜索、分面搜索、动态集群、数据库集成、丰富的文档(例如 Word 和 PDF)处理和地理空间搜索。 在本食谱中，我们将为 Apache Nutch 抓取的网页编制索引，以供 Apache Solr 使用，并使用 Apache Solr 搜索这些网页。

## 做好准备

1.  按照使用 Apache Nutch 配方的*域内 Web 爬网，使用 Apache Nutch 爬网一组网页*
2.  Solr 4.8 及更高版本需要 JDK 1.7

## 怎么做……

以下步骤显示如何索引和搜索已爬网的网页数据集：

1.  从[http://lucene.apache.org/solr/](http://lucene.apache.org/solr/)下载并提取 Apache Solr。 对于本章中的示例，我们使用 Apache Solr 4.10.3。 从这里开始，我们将提取的目录称为`$SOLR_HOME`。
2.  使用位于`$NUTCH_HOME/runtime/local/conf/`下的 `schema.solr4.xml`文件替换位于`$SOLR_HOME/examples/solr/collection1/conf/`下的`schema.xml`文件，如下所示：

    ```scala
    $ cp $NUTCH_HOME/conf/schema-solr4.xml \
     $SOLR_HOME/example/solr/collection1/conf/schema.xml

    ```

3.  将以下配置添加到`<fields>`标记下的`$SOLR_HOME/examples/solr/collection1/conf/schema.xml`：

    ```scala
    <fields>
      <field name="_version_" type="long" indexed="true" stored="true"/>
    ……
    </fields>
    ```

4.  通过从`$SOLR_HOME/`下的`example`目录执行以下命令启动 Solr：

    ```scala
    $ java -jar start.jar

    ```

5.  转到 URL`http://localhost:8983/solr`以验证 Apache Solr 安装。
6.  通过从`$NUTCH_HOME/runtime/local`目录发出以下命令，将使用 Apache Nutch 获取的数据索引到 Apache Solr 中。 此命令通过 Solr Web 服务接口将 Nutch 抓取的数据推送到 Solr 中：

    ```scala
    $ bin/nutch solrindex http://127.0.0.1:8983/solr/ -reindex 

    ```

7.  Go to Apache Solr search UI at `http://localhost:8983/solr/#/collection1/query`. Enter a search term in the **q** textbox and click on **Execute Query**, as shown in the following screenshot:

    ![How to do it...](img/5471OS_08_01.jpg)

8.  您还可以使用 HTTP get 请求直接发出搜索查询。 将`http://localhost:8983/solr/collection1/select?q=hadoop&start=5&rows=5&wt=xml`URL 粘贴到您的浏览器。

## 它是如何工作的.

Apache Solr 是使用 Apache Lucene 文本搜索库构建的。 Apache Solr 在 Apache Lucene 之上添加了许多功能，并提供了一个开箱即用的文本搜索 Web 应用。 前面的步骤部署 Apache Solr，并将 Nutch 抓取的数据导入到部署的 Solr 实例中。

我们计划使用 Solr 索引和搜索的文档的元数据需要通过 Solr`schema.xml`文件指定。 Solr 模式文件应该定义文档中的数据字段，以及 Solr 应该如何处理这些数据字段。 我们使用随 Nutch(`$NUTCH_HOME/conf/schema-solr4.xml`)提供的模式文件，该文件定义了 Nutch 抓取的网页的模式，作为本食谱的 Solr 模式文件。 有关 Solr 模式文件的更多信息可以在[http://wiki.apache.org/solr/SchemaXml](http://wiki.apache.org/solr/SchemaXml)中找到。

## 另请参阅

*   用于索引和搜索配方的*弹性搜索。*
*   有关使用 Apache Solr 的更多信息，请按照[http://lucene.apache.org/solr/tutorial.html](http://lucene.apache.org/solr/tutorial.html)上提供的教程进行操作。
*   SolrCloud 为 Apache Solr 提供分布式索引和搜索功能。 有关 solrcloud 的更多信息，请访问[https://cwiki.apache.org/confluence/display/solr/Getting+Started+with+SolrCloud](https://cwiki.apache.org/confluence/display/solr/Getting+Started+with+SolrCloud)。

# 将 Apache HBase 配置为 Apache Nutch 的后端数据存储

Apache Nutch 集成了 Apache Gora 以添加对不同后端数据存储的支持。 在本食谱中，我们将 Apache HBase 配置为 Apache Nutch 的后端数据存储。 同样，可以通过 Gora 插入数据存储，如 RDBMS 数据库、Cassandra 和其他。

本食谱以[http://wiki.apache.org/nutch/Nutch2Tutorial](http://wiki.apache.org/nutch/Nutch2Tutorial)给出的说明为基础。

### 备注

从 Apache Nutch 2.2.1 版本开始，Nutch 项目还没有正式迁移到 Hadoop 2.x，整个网络爬行仍然依赖 Hadoop 1.x。 但是，可以使用 Hadoop 2.x 集群执行 Nutch 作业，该集群利用 Hadoop 的向后兼容性特性。

Nutch HBaseStore 集成进一步依赖于 HBase 0.90.6，而 HBase 0.90.6 不支持 Hadoop2。因此，此配方仅适用于 Hadoop 1.x 集群。 我们期待着一个完全支持 Hadoop2.x 的新 Nutch 版本。

## 做好准备

1.  安装 ApacheAnt 并将其添加到`PATH`环境变量。

## 怎么做……

以下步骤说明如何将 Apache HBase 本地模式配置为 Apache Nutch 的后端数据存储，以存储已爬网的数据：

1.  安装 Apache HBase。 Apache Nutch 2.2.1 和 Apache Gora 0.3 推荐使用 HBase 0.90.6 版本。
2.  Create two directories to store the HDFS data and Zookeeper data. Add the following to the `hbase-site.xml` file under `$HBASE_HOME/conf/` replacing the values with the paths to the two directories. Start HBase:

    ```scala
    <configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>file:///u/software/hbase-0.90.6/hbase-data</value>
      </property>
    <property>
        <name>hbase.zookeeper.property.dataDir</name>
        <value>file:///u/software/hbase-0.90.6/zookeeper-data</value>
      </property>
    </configuration>
    ```

    ### 提示

    在继续之前，请使用 HBase Shell 测试您的 HBase 安装。

3.  如果您没有为本章前面的食谱下载 Apache Nutch，请从[http://nutch.apache.org](http://nutch.apache.org)下载 Nutch 并解压缩。
4.  将以下内容添加到`$NUTCH_HOME/conf/`下的`nutch-site.xml` 文件中：

    ```scala
    <property>
     <name>storage.data.store.class</name>
     <value>org.apache.gora.hbase.store.HBaseStore</value>
     <description>Default class for storing data</description>
    </property>
    <property>
     <name>http.agent.name</name>
     <value>NutchCrawler</value>
    </property>
    <property>
      <name>http.robots.agents</name>
      <value>NutchCrawler,*</value>
    </property>
    ```

5.  取消注释`$NUTCH_HOME/ivy/`下`ivy.xml` 文件的`Gora artifacts`部分中后面的。 恢复您对早期配方中的 ivy/ivy.xml 文件所做的更改，并确保`gora-core`依赖项版本为 0.3。 此外，请确保注释`gora-sql`依赖项：

    ```scala
    <dependency org="org.apache.gora" name="gora-hbase" rev="0.3" conf="*->default" />
    ```

6.  将以下内容添加到`$NUTCH_HOME/conf/`下的`gora.properties`文件中，以将 HBase 存储设置为默认的 GORA 数据存储：

    ```scala
    gora.datastore.default=org.apache.gora.hbase.store.HBaseStore
    ```

7.  在`$NUTCH_HOME`目录中执行以下命令，以 HBase 作为后端数据存储构建 Apache Nutch：

    ```scala
    $ ant clean
    $ ant runtime

    ```

8.  按照使用 Apache Solr 配方的*域内 Web 爬网步骤 9 到 19 进行操作。*
9.  启动 HBase shell 并发出以下命令以查看获取的数据：

    ```scala
    $ hbase shell
    HBase Shell; enter 'help<RETURN>' for list of supported commands.
    Type "exit<RETURN>" to leave the HBase Shell
    Version 0.90.6, r1295128, Wed Feb 29 14:29:21 UTC 2012
    hbase(main):001:0> list
    TABLE 
    webpage 
    1 row(s) in 0.4970 seconds

    hbase(main):002:0> count 'webpage'
    Current count: 1000, row: org.apache.bval:http/release-management.html 
    Current count: 2000, row: org.apache.james:http/jspf/index.html 
    Current count: 3000, row: org.apache.sqoop:http/team-list.html 
    Current count: 4000, row: org.onesocialweb:http/ 
    4065 row(s) in 1.2870 seconds

    hbase(main):005:0> scan 'webpage',{STARTROW => 'org.apache.nutch:http/', LIMIT=>10}
    ROW                                   COLUMN+CELL
     org.apache.nutch:http/               column=f:bas, timestamp=1350800142780, value=http://nutch.apache.org/
     org.apache.nutch:http/               column=f:cnt, timestamp=1350800142780, value=<....
    ......
    10 row(s) in 0.5160 seconds

    ```

10.  遵循*索引**中的步骤，使用 Apache Solr*配方搜索 Web 文档，并使用 Apache Solr 搜索获取的数据。

## 它是如何工作的.

以上步骤使用 Apache HBase 作为存储后端配置并运行 Apache Nutch。 配置后，Nutch 将获取的网页数据和其他元数据存储在 HBase 表中。 在本食谱中，我们使用独立的 HBase 部署。 然而，正如*使用 Hadoop/HBase 集群的 Apache Nutch 进行全网爬行*食谱所示，Nutch 也可以与分布式 HBase 部署一起使用。 使用 HBase 作为后端数据存储为 Nutch 爬行提供了更高的可伸缩性和性能。

## 另请参阅

*   [第 7 章](07.html "Chapter 7. Hadoop Ecosystem II – Pig, HBase, Mahout, and Sqoop")、*Hadoop 生态系统 II 中的 HBase 食谱--Pig、HBase、Mahout 和 Sqoop*。
*   关于将 Cassandra 或 MYSQL 配置为 nutch 的存储后端的说明，请参考[http://techvineyard.blogspot.com/2010/12/build-nutch-20.html](http://techvineyard.blogspot.com/2010/12/build-nutch-20.html)。

# 使用 Hadoop/HBase 群集使用 Apache Nutch 进行全 Web 爬行

通过利用 MapReduce 集群的强大功能，可以高效地爬行大量的个 Web 文档。

### 备注

从 Apache Nutch 2.2.1 版本开始，Nutch 项目还没有正式迁移到 Hadoop 2.x，整个网络爬行仍然依赖 Hadoop 1.x。 但是，可以使用 Hadoop 2.x 集群执行 Nutch 作业，该集群利用 Hadoop 的向后兼容性特性。

Nutch HBaseStore 集成进一步依赖于 HBase 0.90.6，而 HBase 0.90.6 不支持 Hadoop2。因此，此配方仅适用于 Hadoop 1.x 集群。 我们期待着一个完全支持 Hadoop2.x 的新 Nutch 版本。

## 做好准备

我们假设您已经部署了 Hadoop 1.x 和 HBase 集群。

## 怎么做……

以下步骤说明如何将 Apache Nutch 与 Hadoop MapReduce 集群和 HBase 数据存储配合使用，以执行大规模 Web 爬行：

1.  确保可以从命令行访问`hadoop`命令。 如果没有，请将`$HADOOP_HOME/bin`目录添加到计算机的`PATH`环境变量中，如下所示：

    ```scala
    $ export PATH=$PATH:$HADOOP_HOME/bin/

    ```

2.  按照*将 Apache HBase 配置为 Apache Nutch*的后端数据存储食谱中的步骤 3 到 7 进行操作。 如果您已经按照该食谱操作，则可以跳过此步骤。
3.  在 HDFS 中创建一个目录以上载种子 URL。

    ```scala
    $ hadoop dfs -mkdir urls

    ```

4.  Create a text file with the seed URLs for the crawl. Upload the seed URLs file to the directory created in the preceding step.

    ```scala
    $ hadoop dfs -put seed.txt urls

    ```

    ### 备注

    您可以使用 Open Directory 项目 RDF 转储([http://rdf.dmoz.org/](http://rdf.dmoz.org/))来创建种子 URL。 Nutch 提供了一个实用程序类，用于从提取的 DMOZ RDF 数据中选择 URL 子集，如`bin/nutch org.apache.nutch.tools.DmozParser content.rdf.u8 -subset 5000 > dmoz/urls`。

5.  从`$NUTCH_HOME/runtime/deploy`发出下面的命令，将种子 URL 注入到 Nutch 数据库，并生成初始获取列表：

    ```scala
    $ bin/nutch inject urls
    $ bin/nutch generate

    ```

6.  从`$NUTCH_HOME/runtime/deploy`发出以下命令：

    ```scala
    $ bin/nutch fetch -all
    14/10/22 03:56:39 INFO fetcher.FetcherJob: FetcherJob: starting
    14/10/22 03:56:39 INFO fetcher.FetcherJob: FetcherJob: fetching all
    ......

    $ bin/nutch parse -all
    14/10/22 03:48:51 INFO parse.ParserJob: ParserJob: starting
    ......

    14/10/22 03:50:44 INFO parse.ParserJob: ParserJob: success

    $ bin/nutch updatedb
    14/10/22 03:53:10 INFO crawl.DbUpdaterJob: DbUpdaterJob: starting
    ....
    14/10/22 03:53:50 INFO crawl.DbUpdaterJob: DbUpdaterJob: done

    $ bin/nutch generate -topN 10
    14/10/22 03:51:09 INFO crawl.GeneratorJob: GeneratorJob: Selecting best-scoring urls due for fetch.
    14/10/22 03:51:09 INFO crawl.GeneratorJob: GeneratorJob: starting
    ....
    14/10/22 03:51:46 INFO crawl.GeneratorJob: GeneratorJob: done
    14/10/22 03:51:46 INFO crawl.GeneratorJob: GeneratorJob: generated batch id: 1350892269-603479705

    ```

7.  根据需要重复步骤 6 中的命令，以爬网所需的页数或深度。
8.  遵循*使用 Apache Solr*索引和搜索 Web 文档的食谱，使用 Apache Solr 为获取的数据编制索引。

## 它是如何工作的.

我们在本配方中使用的所有 Nutch 操作，包括获取和解析，都是作为 MapReduce 程序实现的。 这些 MapReduce 程序利用 Hadoop 集群以分布式方式执行 Nutch 操作，并使用 HBase 跨 HDFS 集群存储数据。 您可以通过 Hadoop 集群的监控 UI 监控这些 MapReduce 计算。

Apache Nutch Ant Build 创建一个 Hadoop 作业文件，其中包含`$NUTCH_HOME/runtime/`下`deploy`文件夹中的所有依赖项。 `bin/nutch`脚本使用此作业文件将 MapReduce 计算提交给 Hadoop 集群。

## 另请参阅

*   使用 Apache Nutch 配方的*域内 Web 爬行。*

# 用于索引和搜索的 Elasticsearch

Elasticsearch([http://www.elasticsearch.org/](http://www.elasticsearch.org/))是一个 Apache2.0 许可的开源搜索解决方案，构建在 Apache Lucene 的之上。 ElasticSearch 是一个分布式、多租户和面向文档的搜索引擎。 ElasticSearch 通过将索引分解为碎片并跨集群中的节点分布碎片来支持分布式部署。 Elasticsearch 和 Apache Solr 都使用 Apache Lucene 作为核心搜索引擎，而 Elasticsearch 的目标是提供比 Apache Solr 更适合云环境的可伸缩性和分布式解决方案。

## 做好准备

安装 Apache Nutch 并按照*域内 Web 爬网使用 Apache Nutch*或使用 Hadoop/HBase 群集配方使用 Apache Nutch 爬网*爬网一些网页。 确保 Nutch 的后端 HBase(或 HyperSQL)数据存储仍然可用。*

## 怎么做……

以下步骤说明如何使用 Elasticsearch 索引和搜索 Nutch 搜索到的数据：

1.  下载并从[http://www.elasticsearch.org/download/](http://www.elasticsearch.org/download/)解压弹性搜索。
2.  转到解压的 Elasticsearch 目录，执行以下命令在前台启动 Elasticsearch 服务器：

    ```scala
    $ bin/elasticsearch

    ```

3.  在新控制台中运行以下命令以验证您的安装：

    ```scala
    > curl localhost:9200
    {
     "status" : 200,
     "name" : "Talisman",
     "cluster_name" : "elasticsearch",
     "version" : {
     "number" : "1.4.2",
     ……
     "lucene_version" : "4.10.2"
     },
     "tagline" : "You Know, for Search"
    }

    ```

4.  转到`$NUTCH_HOME/runtime/deploy`(如果您在本地模式下运行 Nutch，则转到`$NUTCH_HOME/runtime/local`)目录。 执行以下命令，将 Nutch 抓取的数据索引到 Elasticsearch 服务器：

    ```scala
    $ bin/nutch elasticindex elasticsearch -all 
    14/11/01 06:11:07 INFO elastic.ElasticIndexerJob: Starting
    …...

    ```

5.  发出以下命令执行搜索：

    ```scala
    $ curl -XGET 'http://localhost:9200/_search?q=hadoop'
    ....
    {"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":36,"max_score":0.44754887,"hits":[{"_index":"index","_type":"doc","_id": 100 30551  100  30551 "org.apache.hadoop:http/","_score":0.44754887, ....

    ```

## 它是如何工作的.

与 Apache Solr 类似，Elasticsearch 也是使用 Apache Lucene 文本搜索库构建的。 在前面的步骤中，我们将 Nutch 爬行的数据导出到 Elasticsearch 实例中，以用于索引和搜索。

您也可以将 Elasticsearch 安装为一项服务。 有关将 http://www.elasticsearch.org/guide/reference/setup/installation.html 作为服务安装的更多详细信息，请参阅[Elasticsearch](http://www.elasticsearch.org/guide/reference/setup/installation.html)。

我们使用 Nutch 的 ElasticIndex 作业将 Nutch 抓取的数据导入到 Elasticsearch 服务器。 弹性索引命令的用法如下：

```scala
bin/nutch  elasticindex  <elastic cluster name> \
 (<batchId> | -all | -reindex) [-crawlId <id>]

```

弹性集群名称恢复为默认值 ElasticSearch。 您可以通过编辑`config/`下的`elasticsearch.yml`文件中的`cluster.name`属性来更改群集名称。 群集名称用于自动发现，并且对于单个网络中的每个 Elasticsearch 部署都应该是唯一的。

## 另请参阅

*   *使用 Apache Solr*配方索引和搜索 Web 文档。

# 为抓取的网页生成内链接图

从其他页面到特定网页的个链接的数量，即内链接的数量，被广泛认为是衡量一个网页的受欢迎程度或重要性的一个很好的度量标准。 事实上，网页内链接的数量和这些链接来源的重要性已经成为大多数流行的链接分析算法(如 Google 引入的 PageRank)不可或缺的组成部分。

在本食谱中，我们将从 Apache Nutch 获取并存储在 Apache HBase 后端数据存储中的一组网页中提取内链接信息。 在我们的 MapReduce 程序中，我们首先检索存储在 Nutch HBase 数据库中的那组网页的外部链接信息，然后使用该信息计算这组网页的内部链接图。 所计算的内链接图将仅包含来自获取的 web 图的子集的链接信息。

## 做好准备

遵循*使用 Hadoop/HBase 群集*配方使用 Apache Nutch 进行全网爬行，或者*将 Apache HBase 配置为 Apache Nutch*配方的后端数据存储，并使用 Apache Nutch 将一组网页爬行到后端 HBase 数据存储。

## 怎么做……

以下步骤显示如何从存储在 Nutch HBase 数据存储中的网页中提取出链接图，以及如何使用提取出的出链图计算入链接图：

1.  启动 HBase 外壳：

    ```scala
    $ hbase shell

    ```

2.  创建名为`linkdata`的 HBase 表和名为`il`的列族。 退出 HBase 外壳：

    ```scala
    hbase(main):002:0> create 'linkdata','il'
    0 row(s) in 1.8360 seconds
    hbase(main):002:0> quit

    ```

3.  解压缩本章的源包，并通过从`chapter8`源目录执行`gradle build`来编译它。
4.  通过发出以下命令运行 Hadoop 程序：

    ```scala
    $ hadoop jar hcb-c8-samples.jar \
    chapter8.InLinkGraphExtractor

    ```

5.  启动 HBase shell 并使用以下命令扫描`linkdata`表，以检查 MapReduce 程序的输出：

    ```scala
    $ hbase shell
    hbase(main):005:0> scan 'linkdata',{COLUMNS=>'il',LIMIT=>10}
    ROW                            COLUMN+CELL 
    ....

    ```

## 它是如何工作的.

由于我们将使用 HBase 来读取输入以及写入输出，因此我们使用 HBase`TableMapper`和`TableReducer`帮助器类来实现 MapReduce 应用。 我们使用`TableMapReduceUtil`类中给出的实用程序方法配置`TableMapper`和`TableReducer`类。 对象`Scan`用于指定从 HBase 数据存储读取输入数据时映射器要使用的条件：

```scala
Configuration conf = HBaseConfiguration.create();
Job job = new Job(conf, "InLinkGraphExtractor");
job.setJarByClass(InLinkGraphExtractor.class);
Scan scan = new Scan();
scan.addFamily("ol".getBytes());
TableMapReduceUtil.initTableMapperJob("webpage", scan, ……);
TableMapReduceUtil.initTableReducerJob("linkdata",……);
```

MAP 实现接收 HBase 行作为输入记录。 在我们的实现中，每行对应一个获取的网页。 `Map`函数的输入键由网页 URL 组成，值由从该特定网页链接的网页组成。 `Map`函数为每个链接网页发出一条记录，其中`Map`输出记录的关键字是链接页面的 URL，`Map`输出记录的值是`Map`函数的输入键(当前处理网页的 URL)：

```scala
public void map(ImmutableBytesWritable row, Result values,……){
  List<KeyValue> results = values.list();      
  for (KeyValue keyValue : results) {
    ImmutableBytesWritable userKey = new     ImmutableBytesWritable(keyValue.getQualifier());
    try {
      context.write(userKey, row);
    } catch (InterruptedException e) {
      throw new IOException(e);
    }
  }
}
```

Reduce 实现接收网页 URL 作为关键字，并接收包含到该网页的链接(在关键字中提供)的网页列表作为值。 Reduce 函数将该数据存储到 HBase 表中：

```scala
public void reduce(ImmutableBytesWritable key,
  Iterable<ImmutableBytesWritable> values, ……{

Put put = new Put(key.get());
  for (ImmutableBytesWritable immutableBytesWritable :values)   {
    put.add(Bytes.toBytes("il"), Bytes.toBytes("link"),
            immutableBytesWritable.get());
  }
  context.write(key, put);
}
```

## 另请参阅

*   第 7 章，*Hadoop 生态系统 II 中的*在 HBase*配方上运行 MapReduce 作业-Pig、HBase、Mahout 和 Sqoop*。