# 六、Hadoop 生态系统——ApacheHive

在本章中，我们将介绍以下食谱：

*   Apache 配置单元入门
*   使用配置单元 CLI 创建数据库和表
*   使用 Apache 配置单元进行简单的 SQL 样式数据查询
*   使用配置单元查询结果创建和填充配置单元表和视图
*   在配置单元中使用不同的存储格式-使用 ORC 文件存储表数据
*   使用配置单元内置函数
*   配置单元批处理模式-使用查询文件
*   执行与配置单元的联接
*   创建分区配置单元表
*   编写配置单元用户定义函数(UDF)
*   HCatalog-对映射到配置单元表的数据执行 Java MapReduce 计算
*   HCatalog-将数据从 Java MapReduce 计算写入配置单元表

# 简介

Hadoop 有一系列项目，这些项目要么构建在 Hadoop 之上，要么与 Hadoop 紧密合作。 这些项目已经形成了一个专注于大规模数据处理的生态系统，通常，用户可以组合使用这些项目中的几个来解决他们的用例。 本章介绍 Apache Have，它在 HDFS 中存储的数据之上提供数据仓库功能。 [第 7 章](07.html "Chapter 7. Hadoop Ecosystem II – Pig, HBase, Mahout, and Sqoop")，*Hadoop 生态系统 II--Pig、HBase、Mahout 和 Sqoop*介绍了 Hadoop 生态系统中的其他几个关键项目。

Apache Have 提供了另一个高级语言层，用于使用 Hadoop 执行大规模数据分析。 HIVE 允许用户将 HDFS 中存储的数据映射到表格模型，并使用 HiveQL(类似 SQL 的语言层)对其进行处理，以使用 Hadoop 查询非常大的数据集。 HiveQL 可用于执行数据集的即席查询以及数据汇总和执行数据分析。 由于其类似 SQL 的语言，对于使用关系数据库进行数据仓库存储经验丰富的用户来说，hive 是一个自然而然的选择。

HIVE 将 HiveQL 查询转换为执行实际工作的一个或多个 MapReduce 计算。 配置单元允许我们使用表架构在现有数据集上定义结构。 但是，配置单元仅在读取和处理数据时才将此结构强加于数据(读取时的架构)。

HIVE 非常适合分析非常大的数据集，因为它可以利用 Hadoop 集群的并行性实现巨大的聚合吞吐量。 但是，由于运行 MapReduce 计算的延迟相对较高，所以配置单元不适合分析较小的数据集或交互式查询。 通过在底层使用 MapReduce 和 HDFS，HIVE 提供了良好的可扩展性和容错能力。 配置单元不支持数据的事务或行级更新。

本章还介绍了 HCatalog，它是配置单元的一个组件，为存储在 HDFS 中的数据提供元数据抽象层，使 Hadoop 生态系统的不同组件能够轻松处理这些数据。 HCatalog 抽象基于表格模型，并增加了 HDFS 数据的结构、位置和其他元数据信息。 使用 HCatalog，我们可以使用 Pig、Java MapReduce 等数据处理工具，而不必担心数据的结构、存储格式或存储位置。

### 提示

**示例代码**

本书的示例代码和数据文件可以在 giHub 上的[https://github.com/thilg/hcb-v2](https://github.com/thilg/hcb-v2)获得。 代码库的`chapter6`文件夹包含本章的示例代码。

可以通过在代码库的`chapter6`文件夹中发出`gradle build`命令来编译和构建示例代码。 Eclipse IDE 的项目文件可以通过在代码库的主文件夹中运行`gradle eclipse`命令来生成。 IntelliJ IDEA IDE 的项目文件可以通过在代码存储库的主文件夹中运行`gradle idea`命令来生成。

在本章中，我们使用 Book Crossing 数据集作为样本数据。 该数据集由 Cai-Nicolas Ziegler 编制，包含图书、用户和评分列表。 本章的源库的`Resources`文件夹包含数据集的示例。 您可以从[http://www2.informatik.uni-freiburg.de/~cziegler/BX/](http://www2.informatik.uni-freiburg.de/~cziegler/BX/)获取完整的数据集。

# Apache 配置单元入门

为了安装配置单元，我们建议您使用免费的商业 Hadoop 发行版，如[第 1 章](01.html "Chapter 1. Getting Started with Hadoop v2")，*《Hadoop v2 入门》*中所述。 另一种选择是使用 Apache bigtop 安装配置单元。 有关使用 Apache bigtop 发行版安装配置单元的步骤，请参阅[第 1 章](01.html "Chapter 1. Getting Started with Hadoop v2")*《Hadoop v2 入门》*中与 Bigtop 相关的食谱。

## 怎么做……

本节介绍如何开始使用配置单元。

1.  如果您已经安装了可用的配置单元，请通过在命令提示符下执行`hive`来启动配置单元**命令行界面**(**CLI**)，然后跳到步骤 4：

    ```scala
    $ hive

    ```

2.  在情况下，如果您没有正在运行的配置单元和 Hadoop 安装，下面几个步骤将指导您如何使用 MapReduce 本地模式安装配置单元。 建议仅用于学习和测试目的。 从[http://hive.apache.org/releases.html](http://hive.apache.org/releases.html)下载并解压最新配置单元版本：

    ```scala
    $ tar -zxvf hive-0.14.0.tar.gz 

    ```

3.  通过从`the` 提取的配置单元文件夹运行以下命令启动配置单元：

    ```scala
    $ cd hive-0.14.0
    $ bin/hive

    ```

4.  或者，您可以设置以下配置单元属性，以使配置单元在显示配置单元 CLI 中的查询结果时打印标题：

    ```scala
    hive> SET hive.cli.print.header=true;

    ```

5.  或者，在您的主目录中创建一个`.hiverc`文件。 每当您启动配置单元 CLI 时，配置单元 CLI 都会将其作为初始化脚本加载。 您可以设置任何自定义属性，例如在此文件中启用打印标题(步骤 4)。 此文件的其他常见用法包括切换到配置单元数据库以及注册您将经常使用的任何库和自定义 UDF(请参阅*编写配置单元用户定义函数*食谱以了解有关 UDF 的更多信息)。

## 另请参阅

有关配置单元提供的配置属性的完整列表，请参阅[https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties](https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties)。

# 使用配置单元 CLI 创建数据库和表

此配方将指导您完成命令，以使用配置单元 CLI 创建配置单元数据库和表。 配置单元表用于定义结构(架构)和其他元数据信息，例如 HDFS 中存储的数据集的位置和存储格式。 这些表定义支持使用配置单元查询语言进行数据处理和分析。 正如我们在引言中所讨论的，配置单元遵循一种“读取时架构”方法，它仅在读取和处理数据时采用这种结构。

## 做好准备

对于本食谱，您需要一个正常工作的 Hive 安装。

## 怎么做……

本节介绍如何创建配置单元表以及如何对配置单元表执行简单查询：

1.  通过运行以下命令启动配置单元 CLI：

    ```scala
    $ hive

    ```

2.  执行以下命令，为简介中提到的图书交叉数据集创建和使用配置单元数据库：

    ```scala
    hive> CREATE DATABASE bookcrossing;
    hive> USE bookcrossing;

    ```

3.  使用以下命令查看数据库的详细信息和文件系统的位置：

    ```scala
    hive> describe database bookcrossing;
    OK
    bookcrossing    hdfs://……/user/hive/warehouse/bookcrossing.db

    ```

4.  让我们通过在配置单元 CLI 中运行以下命令来创建一个表来映射用户信息数据。 将在图书交叉数据库中创建一个表：

    ```scala
    CREATE TABLE IF NOT EXISTS users 
     (user_id INT, 
     location STRING, 
     age INT) 
    COMMENT 'Book Crossing users cleaned' 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '\073' 
    STORED AS TEXTFILE;

    ```

5.  让我们使用`LOAD`命令将数据加载到表中。 `LOAD`命令将文件复制到 HDFS 中的配置单元仓库位置。 在此步骤中，HIVE 不执行任何数据验证或数据解析。 请注意，LOAD 命令中的`OVERWRITE`子句将覆盖并删除表中已有的所有旧数据：

    ```scala
    hive> LOAD DATA LOCAL INPATH 'BX-Users-prepro.txt' OVERWRITE INTO TABLE users;
    Copying data from file:/home/tgunarathne/Downloads/BX-Users-prepro.txt
    Copying file: file:/home/tgunarathne/Downloads/BX-Users-prepro.txt
    Loading data to table bookcrossing.users
    Deleted /user/hive/warehouse/bookcrossing.db/users
    Table bookcrossing.users stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 10388093, raw_data_size: 0]
    OK
    Time taken: 1.018 seconds

    ```

6.  现在，我们可以运行一个简单的查询来检查创建的表中的数据。 此时，配置单元使用表定义中定义的格式解析数据，并执行查询指定的处理：

    ```scala
    hive> SELECT * FROM users LIMIT 10;
    OK
    1  nyc, new york, usa  NULL
    2  stockton, california, usa  18
    3  moscow, yukon territory, russia  NULL
    ………
    10  albacete, wisconsin, spain  26
    Time taken: 0.225 seconds, Fetched: 10 row(s)

    ```

7.  使用以下命令查看配置单元表格的列：

    ```scala
    hive> describe users; 
    OK
    user_id               int                   None 
    location              string                None 
    age                   int                   None 
    Time taken: 0.317 seconds, Fetched: 3 row(s)

    ```

## 它是如何工作的.

当我们运行配置单元时，我们首先定义一个表结构，并将文件中的数据加载到配置单元表中。 值得注意的是，表定义必须与输入数据的结构相匹配。 任何类型不匹配都将导致个空值，并且任何未定义的列都将被配置单元截断。 `LOAD`命令将文件复制到配置单元仓库位置，不做任何更改，这些文件将由配置单元管理。 只有在配置单元读取数据进行处理时，才会对数据实施表架构：

```scala
CREATE TABLE IF NOT EXISTS users 
 (user_id INT, 
 location STRING, 
 age INT) 
COMMENT 'Book Crossing users cleaned' 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\073' 
STORED AS TEXTFILE;

```

请注意，配置单元表名和列名不区分大小写。 前面的表将在`bookcrossing`数据库中创建，因为我们在发出 CREATE TABLE 命令之前发出了`use bookcrossing`命令。 或者，您也可以使用数据库名称`bookcrossing.users`来限定表名。 `ROW FORMAT DELIMITED`指示配置单元将本机**SerDe**(序列化程序和反序列化程序类，由配置单元用来序列化和反序列化数据)与分隔字段一起使用。 在前表使用的数据集中，字段使用`;`字符分隔，该字符是使用`\073`指定的，因为它是配置单元中的保留字符。 最后，我们指示配置单元数据文件是文本文件。 有关配置单元支持的不同存储格式选项的更多信息，请参阅*在配置单元中使用不同的存储格式-使用 ORC 文件存储表数据*配方。

## 还有更多...

在本节中，我们将探讨配置单元数据类型、配置单元外部表、集合数据类型以及有关`describe`命令的更多信息。

### 配置单元数据类型

可用于定义表列的配置单元数据类型列表可在[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types)中找到。 其中包括简单数据类型，如 TINYINT(1 字节有符号整数)、INT(4 字节有符号整数)、BIGINT(8 字节有符号整数)、DOUBLE(8 字节双精度浮点)、TIMESTAMP、DATE、STRING、Boolean 以及其他几种。

HIVE 还支持几种复杂的集合数据类型，如数组和映射作为表列数据类型。 配置单元包含几个用于操作数组和映射的内置函数。 一个例子是`explode()`函数，它将数组或映射的项输出为单独的行。 有关如何使用配置单元函数的更多信息，请参阅*使用配置单元内置函数*配方。

### Hive 外部表

配置单元外部表允许我们将 HDFS 中的数据集映射到配置单元表，而不让配置单元管理数据集。 外部表的数据集不会移动到配置单元默认仓库位置。

此外，删除外部表不会导致删除基础数据集，而不是删除常规配置单元表，在常规配置单元表中，数据集将被删除。 当您想要防止意外删除数据时，这是一个有用的功能：

1.  将 bx-Books-prepro.txt 文件复制到 HDFS 中的目录：

    ```scala
    $ hdfs dfs -mkdir book-crossing
    $ hdfs dfs -mkdir book-crossing/books
    $ hdfs dfs -copyFromLocal BX-Books-prepro.txt book-crossing/books

    ```

2.  通过运行以下命令启动配置单元 CLI，然后使用图书交叉数据库：

    ```scala
    $ hive
    Hive> use bookcrossing;

    ```

3.  通过在配置单元 CLI 中运行以下命令，创建外部表以映射图书信息数据：

    ```scala
    CREATE EXTERNAL TABLE IF NOT EXISTS books
     (isbn INT, 
     title STRING, 
     author STRING, 
     year INT, 
     publisher STRING, 
     image_s STRING, 
     image_m STRING, 
     image_l STRING) 
     COMMENT 'Book crossing books list cleaned'
     ROW FORMAT DELIMITED
     FIELDS TERMINATED BY '\073' 
     STORED AS TEXTFILE
     LOCATION '/user/<username>/book-crossing/books';

    ```

4.  使用下面的查询检查新创建的表的数据：

    ```scala
    hive> select * from books limit 10;
    OK
    195153448  Classical Mythology  Mark P. O. Morford  2002  Oxford University Press  http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg  http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg  http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg

    ```

5.  使用以下命令删除该表：

    ```scala
    hive> drop table books;
    OK
    Time taken: 0.213 seconds

    ```

6.  使用以下命令检查 HDFS 中的数据文件。 即使表被删除，数据文件仍然存在：

    ```scala
    $ hdfs dfs -ls book-crossing/books
    Found 1 items
    -rw-r--r--   1 tgunarathne supergroup   73402860 2014-06-19 18:49 /user/tgunarathne/book-crossing/books/BX-Books-prepro.txt

    ```

### 使用 DESCRIBE FORMACTED 命令检查配置单元表元数据

您可以使用`describe`命令检查配置单元表的基本元数据。 `describe extended`命令将打印其他元数据信息，包括数据位置、输入格式、创建时间等。 `describe formatted`命令以更加用户友好的方式呈现此元数据信息：

```scala
hive> describe formatted users;
OK
# col_name              data_type             comment 
user_id               int                   None 
location              string                None 
age                   int                   None 

# Detailed Table Information 
Database:             bookcrossing 
Owner:                tgunarathne 
CreateTime:           Mon Jun 16 02:19:26 EDT 2014 
LastAccessTime:       UNKNOWN 
Protect Mode:         None 
Retention:            0 
Location:             hdfs://localhost:8020/user/hive/warehouse/bookcrossing.db/users 
Table Type:           MANAGED_TABLE 
Table Parameters: 
 comment               Book Crossing users cleaned
 numFiles              1 
 numPartitions         0 
 numRows               0 
 rawDataSize           0 
 totalSize             10388093 
 transient_lastDdlTime  1402900035 

# Storage Information 
SerDe Library:        org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe 
……
Time taken: 0.448 seconds, Fetched: 35 row(s)

```

# 使用 Apache 配置单元进行简单的 SQL 风格的数据查询

我们可以使用 HiveQL 查询已经映射到 hive 表的数据集，这类似于 SQL。 这些查询可以是简单的数据探索操作，如计数、`orderby, and group by`以及复杂的联接、汇总和分析操作。 在本食谱中，我们将探索简单的数据探索配置单元查询。 本章后面的食谱将介绍一些高级查询用例。

## 做好准备

安装配置单元并按照前面的步骤*使用配置单元 CLI*创建数据库和表。

## 怎么做……

本节演示如何使用配置单元执行简单的 SQL 样式查询。

1.  通过发出以下命令启动配置单元：

    ```scala
    $ hive

    ```

2.  在配置单元 CLI 中发出以下查询以检查年龄在 18 岁到 34 岁之间的用户。 配置单元在后台使用 MapReduce 作业执行此数据筛选操作：

    ```scala
    hive> SELECT user_id, location, age FROM users WHERE age>18 and age <34 limit 10; 
    Total MapReduce jobs = 1
    Launching Job 1 out of 1
    ……
    10  albacete, wisconsin, spain  26
    13  barcelona, barcelona, spain  26
    ….
    Time taken: 34.485 seconds, Fetched: 10 row(s)

    ```

3.  在配置单元 CLI 中发出以下查询，统计满足上述条件(即年龄在 18 到 34 岁之间)的用户总数。 配置单元将此查询转换为 MapReduce 计算以计算结果：

    ```scala
    hive> SELECT count(*) FROM users WHERE age>18 and age <34; 
    Total MapReduce jobs = 1
    Launching Job 1 out of 1
    …………
    2014-06-16 22:53:07,778 Stage-1 map = 100%,  reduce = 100%, 
    …………
    Job 0: Map: 1  Reduce: 1   Cumulative CPU: 5.09 sec   HDFS Read: 10388330 HDFS Write: 6 SUCCESS
    Total MapReduce CPU Time Spent: 5 seconds 90 msec
    OK
    74339
    Time taken: 53.671 seconds, Fetched: 1 row(s)

    ```

4.  以下查询统计按年龄分组的用户数：

    ```scala
    hive> SELECT  age, count(*) FROM users GROUP BY age; 
    Total MapReduce jobs = 1
    ………
    Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.8 sec   HDFS Read: 10388330 HDFS Write: 1099 SUCCESS
    Total MapReduce CPU Time Spent: 3 seconds 800 msec
    OK
    ….
    10  84
    11  121
    12  192
    13  885
    14  1961
    15  2376

    ```

5.  下面的查询按用户年龄计算用户数，并按用户数的降序对结果进行排序：

    ```scala
    hive> SELECT  age, count(*) as c FROM users GROUP BY age ORDER BY c DESC;
    Total MapReduce jobs = 2
    …..
    Job 0: Map: 1  Reduce: 1   Cumulative CPU: 5.8 sec   HDFS Read: 10388330 HDFS Write: 3443 SUCCESS
    Job 1: Map: 1  Reduce: 1   Cumulative CPU: 2.15 sec   HDFS Read: 3804 HDFS Write: 1099 SUCCESS
    Total MapReduce CPU Time Spent: 7 seconds 950 msec
    OK
    NULL  110885
    24  5683
    25  5614
    26  5547
    23  5450
    27  5373
    28  5346
    29  5289
    32  4778

    ```

## 它是如何工作的.

您可以使用`explain`命令查看配置单元查询的执行计划。 这对于识别大规模查询的瓶颈并对其进行优化非常有用。 以下是我们在前面的菜谱中使用的一个查询的执行计划。 如您所见，该查询产生了一个 MapReduce 计算，然后是一个数据输出阶段：

```scala
hive> EXPLAIN SELECT user_id, location, age FROM users WHERE age>18 and age <34 limit 10;
OK
ABSTRACT SYNTAX TREE:
 …

STAGE PLANS:
 Stage: Stage-1
 Map Reduce
 Alias -> Map Operator Tree:
 users 
 TableScan
 alias: users
 Filter Operator
 predicate:
 expr: ((age > 18) and (age < 34))
 type: boolean
 Select Operator
 expressions:
 expr: user_id
 type: int
 expr: location
 type: string
 expr: age
 type: int
 outputColumnNames: _col0, _col1, _col2
 Limit
 File Output Operator
 compressed: false
 GlobalTableId: 0
 table:
 input format: org.apache.hadoop.mapred.TextInputFormat
 output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

 Stage: Stage-0
 Fetch Operator
 limit: 10

```

## 还有更多...

HIVE 提供了几个用于对查询结果进行排序的运算符，这些运算符具有细微的差异和性能权衡：

*   **ORDER BY**：这个保证使用单个减法器对数据进行全局排序。 但是，对于任何非平凡数量的结果数据，使用单个减少器都会显著降低计算速度。
*   **排序依据**：这个保证每个 Reduce 任务输出的数据的本地排序。 但是，Reduce 任务将包含重叠的数据区域。
*   **CLUSTER BY**：这将分发数据以减少任务，避免任何范围重叠，并且每个减少任务将以排序的顺序输出数据。 这确保了数据的全局排序，即使结果将存储在多个文件中。

有关上述运算符区别的详细说明，请参阅[http://stackoverflow.com/questions/13715044/hive-cluster-by-vs-order-by-vs-sort-by](http://stackoverflow.com/questions/13715044/hive-cluster-by -vs-order-by-vs-sort-by)。

### 使用 Apache Tez 作为配置单元的执行引擎

TEZ 是一个新的执行框架，它构建在 Yarn 之上，提供了比 MapReduce 更低级别的 API(有向无环图)。 TEZ 比 MapReduce 更灵活、更强大。 TEZ 允许应用通过使用比 MapReduce 模式更具表现力的执行模式来提高性能。 配置单元支持 TEZ 执行引擎作为后台 MapReduce 计算的替代，在后台 MapReduce 计算中，配置单元会将配置单元查询转换为 TEZ 执行图，从而大大提高了性能。 您可以执行以下步骤：

*   通过设置以下配置单元属性，可以指示配置单元使用 TEZ 作为执行引擎：

    ```scala
    hive> set hive.execution.engine=tez;

    ```

*   您可以切换回 MapReduce 作为执行引擎，如下所示：

    ```scala
    hive> set hive.execution.engine=mr;

    ```

## 另请参阅

有关配置单元`select`语句支持的子句和功能的列表，请参阅[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select)。

# 使用配置单元查询结果创建和填充配置单元表和视图

配置单元允许我们通过创建新的配置单元表来保存配置单元查询的输出数据。 我们还可以将配置单元查询的结果数据插入到另一个现有的表中。

## 做好准备

安装配置单元并按照*使用配置单元 CLI*配方创建数据库和表。

## 怎么做……

以下步骤显示如何将配置单元查询的结果存储到新的配置单元表中：

1.  发出以下查询，将前面配方的步骤 3 的查询输出保存到名为`tmp_users`的表中：

    ```scala
    hive> CREATE TABLE tmp_users AS SELECT user_id, location, age FROM users WHERE age>18 and age <34;
    …
    Table bookcrossing.tmp_users stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2778948, raw_data_size: 0]
    74339 Rows loaded to hdfs://localhost:8020/tmp/hive-root/hive_2014-07-08_02-57-18_301_5868823709587194356/-ext-10000

    ```

2.  使用以下命令检查新创建的表的数据：

    ```scala
    hive> select * from tmp_users limit 10;
    OK
    10  albacete, wisconsin, spain  26
    13  barcelona, barcelona, spain  26
    18  rio de janeiro, rio de janeiro, brazil  25

    ```

3.  配置单元还允许我们将配置单元查询的结果插入到现有表中，如下所示。 发出以下查询以将以下查询的输出数据加载到`tmp_users`配置单元表：

    ```scala
    hive> INSERT INTO TABLE tmp_users SELECT user_id, location, age FROM users WHERE age>33 and age <51;
    Total MapReduce jobs = 3
    Launching Job 1 out of 3
    …….
    Loading data to table bookcrossing.tmp_users
    Table bookcrossing.tmp_users stats: [num_partitions: 0, num_files: 2, num_rows: 0, total_size: 4717819, raw_data_size: 0]
    52002 Rows loaded to tmp_users

    ```

4.  您还可以使用查询在现有表中创建视图，如下所示。 该视图可以用作用于查询的常规表，但视图的内容将仅按配置单元

    ```scala
    hive> CREATE VIEW tmp_users_view AS SELECT user_id, location, age FROM users WHERE age>18 and age <34;

    ```

    按需进行计算

# 在配置单元中使用不同的存储格式-使用 ORC 文件存储表数据

除了简单的文本文件外，配置单元还支持几种可用于存储表底层数据的其他二进制存储格式。 其中包括基于行的存储格式(如 Hadoop SequenceFiles 和 Avro 文件)以及基于列(列)的存储格式(如 ORC 文件和 Parquet)。

列存储格式逐列存储数据，其中列的所有值将一起存储，而不是在基于行的存储中以逐行的方式存储。 例如，如果我们将前一个配方中的`users`表存储在列数据库中，则所有用户 ID 将存储在一起，所有位置将存储在一起。 列存储提供了更好的数据压缩，因为很容易压缩存储在一起的相同类型的相似数据。 列存储还为配置单元查询提供了几个性能改进。 列存储允许处理引擎跳过从特定计算不需要的列加载数据，还可以更快地执行列级分析查询(例如，计算用户的最大年龄)。

在此配方中，我们将使用配置单元 CLI 配方创建数据库和表的`users`表中的数据存储到以 ORC 文件格式存储的配置单元表中。

## 做好准备

安装配置单元并按照*使用配置单元 CLI*配方创建数据库和表。

## 怎么做……

以下步骤显示如何创建使用 ORC 文件格式存储的配置单元表格：

1.  在配置单元 CLI 中执行以下查询以创建使用 ORC 文件格式存储的用户表：

    ```scala
    hive> USE bookcrossing;

    hive> CREATE TABLE IF NOT EXISTS users_orc 
     (user_id INT, 
     location STRING, 
     age INT) 
    COMMENT 'Book Crossing users table ORC format' 
    STORED AS ORC;

    ```

2.  执行以下命令将数据插入到新创建的表中。 我们必须使用之前创建的表填充数据，因为我们不能将文本文件直接加载到 ORC 文件或其他存储格式表：

    ```scala
    hive> INSERT INTO TABLE users_orc 
     SELECT * 
     FROM users;

    ```

3.  执行以下查询以检查`users_orc`表中的数据：

    ```scala
    Hive> select * from users_orc limit 10;

    ```

## 它是如何工作的.

以下命令中的`STORED AS ORC`短语通知配置单元此表的数据将使用 ORC 文件存储。 您可以使用`STORED AS PARQUET`使用拼图格式存储表格数据，使用 Avro 文件使用`STORED AS AVRO`存储数据：

```scala
CREATE TABLE IF NOT EXISTS users_orc 
 (user_id INT, 
 location STRING, 
 age INT) 
STORED AS ORC;

```

# 使用配置单元内置函数

HIVE 提供了许多内置函数来帮助我们处理和查询数据。 这些函数提供的一些功能包括字符串操作、日期操作、类型转换、条件运算符、数学函数等等。

## 做好准备

此配方假定已执行了较早的配方。 如果您尚未安装配置单元，请按照之前的配方进行安装。

## 怎么做……

本节演示如何使用 `parse_url` 配置单元函数解析 URL 的内容：

1.  通过运行以下命令启动配置单元 CLI：

    ```scala
    $ hive

    ```

2.  发出以下命令以获取与每本书关联的小图像的`FILE`部分：

    ```scala
    hive> select isbn, parse_url(image_s, 'FILE') from books limit 10; 
    Total MapReduce jobs = 1
    …..
    OK
    0195153448  /images/P/0195153448.01.THUMBZZZ.jpg
    0002005018  /images/P/0002005018.01.THUMBZZZ.jpg
    0060973129  /images/P/0060973129.01.THUMBZZZ.jpg
    ……
    Time taken: 17.183 seconds, Fetched: 10 row(s)

    ```

## 它是如何工作的.

为前面查询选择的每个数据记录调用`parse_url`函数：

```scala
parse_url(string urlString, string partToExtract)
```

`parse_url`函数解析由`urlString`参数提供的 URL，并支持 HOST、PATH、QUERY、REF、PROTOCOL、AUTHORITY、FILE 和 USERINFO 作为`partToExtract`参数。

## 还有更多...

您可以在配置单元 CLI 中发出以下命令，以查看配置单元安装支持的函数列表：

```scala
hive> show functions;

```

您可以使用配置单元 CLI 中的`describe <function_name>`和`describe extended <function_name>`命令访问每个功能的帮助和用法，如下所示。 例如：

```scala
hive> describe function extended parse_url;
OK
parse_url(url, partToExtract[, key]) - extracts a part from a URL
Parts: HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO
key specifies which query to extract
Example:
 > SELECT parse_url('http://facebook.com/path/p1.php?query=1', 'HOST') FROM src LIMIT 1;
 'facebook.com'
 > SELECT parse_url('http://facebook.com/path/p1.php?query=1', 'QUERY') FROM src LIMIT 1;
 'query=1'
 > SELECT parse_url('http://facebook.com/path/p1.php?query=1', 'QUERY', 'query') FROM src LIMIT 1;
 '1'

```

## 另请参阅

HIVE 提供了许多不同类别的函数，包括数学、日期操作、字符串操作等等。 有关配置单元提供的功能的完整列表，请参阅[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)。

有关编写自己的函数以用于配置单元查询的信息，请参阅*编写配置单元用户定义函数(UDF)*配方。

# 配置单元批处理模式-使用查询文件

除了配置单元交互式 CLI 之外，在中，配置单元还允许我们使用脚本文件以批处理模式执行查询。 在本食谱中，我们使用配置单元脚本文件创建图书交叉数据集的图书、用户和评级表，并将数据加载到新创建的表中。

## 怎么做……

本节演示如何使用配置单元脚本文件创建表和加载数据。 继续执行以下步骤：

1.  解压本章源库提供的数据包：

    ```scala
    $ tar –zxvf chapter6-bookcrossing-data.tar.gz

    ```

2.  在本章来源资料库的配置单元脚本文件夹中找到`create-book-crossing.hql`配置单元查询文件。 通过为`DATA_DIR`参数提供提取的数据包的位置，按如下方式执行此配置单元脚本文件。 请注意，执行以下脚本文件将覆盖图书交叉数据库的 USERS、BOOK 和 RASTING 表(如果这些表预先存在)中的任何现有数据：

    ```scala
    $ hive \
     -hiveconf DATA_DIR=…/hcb-v2/chapter6/data/ \
     -f create-book-crossing.hql 
    ……
    Copying data from file:……/hcb-v2/chapter6/data/BX-Books-Prepro.txt
    ……
    Table bookcrossing.books stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 73402860, raw_data_size: 0]
    OK
    ……
    OK
    Time taken: 0.685 seconds

    ```

3.  启动配置单元 CLI 并发出以下命令以检查由前面的脚本创建的表：

    ```scala
    $ hive
    hive> use bookcrossing; 
    ……
    hive> show tables; 
    OK
    books
    ratings
    users
    Time taken: 0.416 seconds, Fetched: 3 row(s)
    hive> select * from ratings limit 10;
    OK
    276725  034545104X  0
    276726  0155061224  5
    276727  0446520802  0

    ```

## 它是如何工作的.

`hive –f <filename>`选项以批处理模式执行给定文件中包含的 HiveQL 查询。 使用最新的配置单元版本，您甚至可以在 HDFS 中指定一个文件作为此命令的脚本文件。

Create-book-cross sing.hql 脚本包含用于创建 Book-Crossing 数据库以及创建数据并将数据加载到 USERS、BOOK 和 RATS 表的命令：

```scala
CREATE DATABASE IF NOT EXISTS bookcrossing;
USE bookcrossing;

CREATE TABLE IF NOT EXISTS books
  (isbn STRING, 
  title STRING, 
  author STRING, 
  year INT, 
  publisher STRING, 
  image_s STRING, 
  image_m STRING, 
  image_l STRING) 
COMMENT 'Book crossing books list cleaned'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\073' 
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '${hiveconf:DATA_DIR}/BX-Books-Prepro.txt' OVERWRITE INTO TABLE books;
```

调用配置单元命令时，可以使用`–hiveconf <property-name>=<property-value>`选项设置属性并将参数传递给配置单元脚本文件。 您可以使用`${hiveconf:<property-name>}`在脚本中引用这些属性。 在查询执行之前，配置单元查询内的此类属性用法将被该属性的值替代。 在当前的配方中可以看到一个这样的例子，我们使用`DATA_DIR`属性将数据文件的位置传递给配置单元脚本。 在脚本中，我们通过`${hiveconf:DATA_DIR}`使用此属性的值。

`–hiveconf`选项也可用于设置配置单元配置变量。

## 还有更多...

您可以使用`hive –e '<query>'`选项直接从命令行运行批处理模式配置单元查询。 以下是这种用法的示例：

```scala
$ hive -e 'select * from bookcrossing.users limit 10'

```

## 另请参阅

有关配置单元 https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli 支持的选项的更多信息，请参阅[CLI](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli)。

# 执行与配置单元的联接

本配方将指导您如何使用配置单元跨两个数据集执行联接。 第一个数据集是 Book-Crossing 数据库的图书详细信息数据集，第二个数据集是这些图书的审阅者评分。 这个食谱将使用 Hive 来寻找收视率超过 3 颗星的作者。

## 做好准备

遵循前面的*配置单元批处理模式-使用查询文件*配方。

## 怎么做……

本节演示如何使用配置单元执行联接。 继续执行以下步骤：

1.  启动配置单元 CLI 并使用图书交叉数据库：

    ```scala
    $ hive
    hive > USE bookcrossing;

    ```

2.  在使用查询文件配方引用前面的*配置单元批处理模式命令后，通过执行`create-book-crossing.hql`配置单元查询文件来创建图书和图书评级表。 使用以下命令验证`Book-Crossing`数据库中是否存在这些表：

    ```scala
    hive > SELECT * FROM books LIMIT 10;
    ….
    hive > SELECT * FROM RATINGS LIMIT 10;
    ….

    ```* 
3.  现在，我们可以使用配置单元的类似 SQL 的`join`命令连接这两个表：

    ```scala
    SELECT
     b.author AS author, 
     count(*) AS count 
    FROM 
     books b 
    JOIN
     ratings r 
    ON (b.isbn=r.isbn) and r.rating>3 
    GROUP BY b.author 
    ORDER BY count DESC 
    LIMIT 100;

    ```

4.  如果成功，它将把以下内容和连同结果一起打印到控制台：

    ```scala
    Total MapReduce jobs = 4
    ...
    2014-07-07 08:09:53  Starting to launch local task to process map join;  maximum memory = 1013645312
    ...
    Launching Job 2 out of 4
    ....
    Launching Job 3 out of 4
    ...
    2014-07-07 20:11:02,795 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.18 sec
    MapReduce Total cumulative CPU time: 8 seconds 180 msec
    Ended Job = job_1404665955853_0013
    Launching Job 4 out of 4
    ....
    Total MapReduce CPU Time Spent: 21 seconds 360 msec
    OK
    Stephen King  4564
    Nora Roberts  2909
    John Grisham  2505
    James Patterson  2334
    J. K. Rowling  1742
    ...
    Time taken: 116.424 seconds, Fetched: 100 row(s)

    ```

## 它是如何工作的.

执行时，配置单元首先将 JOIN 命令转换为组 MapReduce 计算。 这些 MapReduce 计算将首先根据给定的架构加载和解析这两个数据集。 然后，根据给定的连接条件使用 MapReduce 计算连接数据。

配置单元支持内部联接以及左、右和完全外部联接。 目前，配置单元只支持基于相等的条件作为联接条件。 HIVE 能够根据数据集的性质和大小执行多项优化，以优化联接的性能。

## 另请参阅

*   有关详细信息，请参阅[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins)。
*   第 5 章，*Analytics*的*使用 MapReduce*配方连接两个数据集说明了如何使用 MapReduce 实现连接操作。

# 创建分区配置单元表

本配方将展示如何使用分区表在配置单元中存储数据。 分区表允许我们存储按一个或多个数据列分区的数据集，以实现高效的查询。 实际数据将驻留在单独的目录中，这些目录的名称将构成分区列的值。 分区表可以通过在使用适当的`where`谓词时仅读取选择分区来减少配置单元必须处理的数据量，从而提高某些查询的性能。 一个常见的示例是存储按日期分区的事务性数据集(或其他带有时间戳的数据集，如 Web 日志)。 当配置单元表按日期分区时，我们可以查询属于单个日期或日期范围的数据，只读取属于这些日期的数据。 在未分区的表中，这将导致全表扫描，读取该表中的所有数据，当您将 TB 级的数据映射到配置单元表时效率可能会非常低。

## 做好准备

本配方假定已经执行了较早的*配置单元批处理模式-使用查询文件*配方。 如果您还没有安装 Hive，请按照该食谱进行安装。

## 怎么做……

本部分演示如何在配置单元中动态创建分区表。 继续执行以下步骤：

1.  启动配置单元 CLI。
2.  运行以下命令以启用配置单元中的动态分区创建：

    ```scala
    hive> set hive.exec.dynamic.partition=true; 
    hive> set hive.exec.dynamic.partition.mode=nonstrict;
    hive> set hive.exec.max.dynamic.partitions.pernode=2000; 

    ```

3.  执行以下查询以使用 SELECT 语句的结果创建新的分区表。 在本例中，我们使用图书的出版年份对表进行分区。 通常，年份和日期可以作为跨时间数据(例如，日志数据)的良好分区列。 向分区表动态插入数据时，分区列应该是 INSERT 语句中的最后一列：

    ```scala
    hive> INSERT INTO TABLE books_partitioned
     > partition (year)
     > SELECT 
     >   isbn,
     >   title,
     >   author,
     >   publisher,
     >   image_s,
     >   image_m,
     >   image_l,
     >   year
     > FROM books;
    Total MapReduce jobs = 3
    Launching Job 1 out of 3
    …… 
    Loading data to table bookcrossing.books_partitioned partition (year=null)
     Loading partition {year=1927}
     Loading partition {year=1941}
     Loading partition {year=1984}
    ……. 
    Partition bookcrossing.books_partitioned{year=1982} stats: [num_files: 1, num_rows: 0, total_size: 1067082, raw_data_size: 0]
    …

    ```

4.  执行以下查询。 由于使用了`year`分区列，该查询将只查看配置单元表 1982 年的数据分区中存储的数据。 如果不是分区，此查询将需要处理整个数据集的 MapReduce 计算：

    ```scala
    hive> select * from books_partitioned where year=1982 limit 10;
    OK
    156047624  All the King's Men  Robert Penn Warren  Harvest Books  http://images.amazon.com/images/P/0156047624.01.THUMBZZZ.jpg  http://images.amazon.com/images/P/0156047624.01.MZZZZZZZ.jpg  http://images.amazon.com/images/P/0156047624.01.LZZZZZZZ.jpg  1982

    ```

5.  退出配置单元 CLI 并在命令提示符下执行以下命令。 您可以看到配置单元创建的分区目录：

    ```scala
    $ hdfs dfs -ls /user/hive/warehouse/bookcrossing.db/books_partitioned
    Found 116 items
    drwxr-xr-x   - root hive          0 2014-07-08 20:24 /user/hive/warehouse/bookcrossing.db/books_partitioned/year=0
    drwxr-xr-x   - root hive          0 2014-07-08 20:24 /user/hive/warehouse/bookcrossing.db/books_partitioned/year=1376
    drwxr-xr-x   - root hive          0 2014-07-08 20:24 /user/hive/warehouse/bookcrossing.db/books_partitioned/year=1378
    ….

    ```

# 编写配置单元用户定义函数(UDF)

正如在*使用配置单元内置函数*配方中提到的，配置单元支持许多用于数据操作和分析的内置函数。 配置单元还允许我们编写自己的自定义函数，以便与配置单元查询一起使用。 这些函数称为用户定义函数，本菜谱将向您展示如何为配置单元编写一个简单的**用户定义函数**(**UDF**)。 配置单元 UDF 允许我们扩展配置单元的功能以满足我们定制的需求，而不必从头开始实现 Java MapReduce 程序。

## 做好准备

此配方假定已执行了较早的配方。 如果您尚未安装配置单元，请按照之前的配方进行安装。

确保您的系统中安装了 Apache Ant。

## 怎么做……

本节演示如何实现简单的配置单元 UDF。 执行以下步骤：

1.  使用本章源代码存储库中的 Gradle 构建文件构建用户定义的函数 JAR 文件：

    ```scala
    $ gradle build

    ```

2.  启动配置单元 CLI：

    ```scala
    $ hive
    hive > USE bookcrossing;

    ```

3.  使用步骤 1 中创建的 JAR 文件的完整路径将新创建的 JAR 文件添加到环境中：

    ```scala
    hive> ADD JAR /home/../ hcb-c6-samples.jar;

    ```

4.  使用以下命令定义配置单元内部的新 UDF：

    ```scala
    hive> CREATE TEMPORARY FUNCTION filename_from_url as 'chapter6.udf. ExtractFilenameFromURL';

    ```

5.  发出以下命令，使用我们新定义的 UDF 获取与每本书关联的小图像的文件名部分：

    ```scala
    hive> select isbn, filename_from_url(image_s, 'FILE') from books limit 10; 

    ```

## 它是如何工作的.

配置单元 UDF 应该扩展配置单元的 UDF 类，并实现`evaluate`方法来执行您需要执行的自定义计算。 需要使用适当的 Hadoop Writable 类型提供`evaluate`方法的输入和输出参数，该类型对应于您要处理并从 UDF 接收的配置单元数据类型：

```scala
public class ExtractFilenameFromURL extends UDF {
  public Text evaluate(Text input) throws MalformedURLException {
    URL url = new URL(input.toString());
    Text fileNameText = new Text(FilenameUtils.getName(url.getPath()));
    return fileNameText;
  }
}
```

我们可以使用如下所示的注释向 UDF 添加说明。 如果您从配置单元 CLI 向此 UDF 发出`describe`命令，则会发出这些命令：

```scala
@UDFType(deterministic = true)
@Description(
    name = "filename_from_url", 
    value = "Extracts and return the filename part of a URL.", 
    extended = "Extracts and return the filename part of a URL. "
        + "filename_from_url('http://test.org/temp/test.jpg?key=value') returns 'test.jpg'."
)
```

# HCatalog-对映射到配置单元表的数据执行 Java MapReduce 计算

HCatalog 是存储在 HDFS 中的文件的元数据抽象层，它使得不同的组件很容易处理存储在 HDFS 中的数据。 HCatalog 抽象基于表格表模型，为 HDFS 中存储的数据集增加了结构、位置、存储格式和其他元数据信息。 有了 HCatalog，我们就可以使用 Pig、Java MapReduce 等数据处理工具对 Hive 表进行读写操作，而不必担心数据的结构、存储格式或存储位置。 当您想要对使用二进制数据格式(如 ORCFiles)存储在配置单元中的数据集执行 Java MapReduce 作业或 Pig 脚本时，HCatalog 非常有用。 拓扑如下所示：

![HCatalog – performing Java MapReduce computations on data mapped to Hive tables](img/5471OS_06_01.jpg)

HCatalog 通过提供到配置单元元存储的接口来实现此功能，从而使其他应用能够利用配置单元表元数据信息。 我们可以使用 HCatalog 命令行界面(CLI)查询 HCatalog 中的表信息。 HCatalog CLI 基于配置单元 CLI，支持配置单元**数据定义语言**(**DDL**)语句，但需要运行 MapReduce 查询的语句除外。 HCatalog 还公开了一个名为 WebHCat 的 REST API。

在本食谱中，我们将通过利用 HCatalog 提供的元数据，在配置单元表中存储的数据之上使用 Java MapReduce 计算。 HCatalog 提供了 HCatInputFormat 类来从配置单元表格中检索数据。

## 做好准备

确保 HCatalog 与配置单元一起安装在您的系统中。

## 怎么做……

本节演示如何使用 MapReduce 计算处理配置单元表数据。 执行以下步骤：

1.  使用本章的查询文件配方按照配置单元批处理模式命令创建并填充我们将在本配方中使用的`bookcrossing.user`配置单元表。
2.  通过从源代码存储库的`chapter6`文件夹运行以下`gradle`命令，编译本章的示例源代码。

    ```scala
    $ gradle clean build uberjar
    ```

3.  使用以下命令运行 MapReduce 作业。 第一个参数是数据库名，第二个参数是输入表名称，第三个参数是输出路径。 此作业统计年龄在 18 岁到 34 岁之间的用户数，按年份分组：

    ```scala
    $ hadoop jar build/libs/hcb-c6-samples-uber.jar \
    chapter7.hcat.HCatReadMapReduce \
    bookcrossing users hcat_read_out 

    ```

4.  通过运行以下命令检查此计算的结果：

    ```scala
    $ hdfs dfs -cat hcat-read-out/part*

    ```

## 它是如何工作的.

您可以从本章源代码文件夹中的`chapter6/hcat/``HCatReadMapReduce.java`文件中找到本食谱的源代码。

`run()`函数中的以下几行将`HCatalogInputFormat`指定为计算的`InputFormat`，并使用输入的数据库名称和表名对其进行配置。

```scala
// Set HCatalog as the InputFormat
job.setInputFormatClass(HCatInputFormat.class);
HCatInputFormat.setInput(job, dbName, tableName);
```

`map()`函数从配置单元表格接收记录作为`HCatRecord`值，而`map()`输入键不包含任何有意义的数据。 HCatRecord 包含根据配置单元表的列结构解析的数据字段，我们可以在`map`函数中从`HCatRecord`中提取字段，如下所示：

```scala
public void map( WritableComparable key,HCatRecord value,…)
        throws IOException, InterruptedException {
  HCatSchema schema = HCatBaseInputFormat.getTableSchema(context.getConfiguration());
  // to avoid the "null" values in the age field 
  Object ageObject = value.get("age", schema);
  if (ageObject instanceof Integer) {
    int age = ((Integer) ageObject).intValue();
    // emit age and one for count
    context.write(new IntWritable(age), ONE);
    }
     }
  }
```

Hadoop 类路径中需要 HCatalogjar、hive jar 及其依赖项才能执行 HCatalog MapReduce 程序。 在调用 Hadoop jar 命令时，我们还需要通过在命令行使用`libjars`参数指定依赖库，从而为 Map 和 Reduce 任务提供这些 JAR。 同时解决 Hadoop 类路径和`libjars`需求的另一种方法是将所有依赖 JAR 打包到单个 FAT-JAR 中，并使用它提交 MapReduce 程序。

在此示例中，我们使用第二种方法，并使用 Gradle 构建创建一个 FAT-JAR(`hcb-c6-samples-uber.jar`)，如下所示：

```scala
task uberjar(type: Jar) {
  archiveName = "hcb-c6-samples-uber.jar"
  from files(sourceSets.main.output.classesDir)
  from {configurations.compile.collect {zipTree(it)}} {
      exclude "META-INF/*.SF"
      exclude "META-INF/*.DSA"
      exclude "META-INF/*.RSA"
  }
}
```

# HCatalog-将数据从 Java MapReduce 计算写入配置单元表

HCatalog 还允许我们使用`HCatOutputFormat`将数据从 Java MapReduce 计算写入配置单元表。 在本食谱中，我们将了解如何使用 Java MapReduce 计算将数据写入配置单元表。 此配方通过添加表写入功能扩展了前面*HCatalog 配方的计算-在映射到配置单元表格*的数据上执行 Java MapReduce 计算。

## 做好准备

确保 HCatalog 与配置单元一起安装在您的系统中。

## 怎么做……

本节演示如何使用 MapReduce 计算将数据写入配置单元表。 执行以下步骤：

1.  遵循配置单元批处理模式-使用本章的查询文件配方创建并填充我们将在本配方中使用的用户配置单元表。
2.  通过从源代码库的`chapter6`文件夹运行以下`gradle` 命令，编译本章的示例源代码：

    ```scala
    $ gradle clean build uberjar

    ```

3.  使用配置单元 CLI 创建配置单元表以存储计算结果。

    ```scala
    hive> create table hcat_out(age int, count int);

    ```

4.  使用以下命令运行 MapReduce 作业。 第一个参数是数据库名，第二个参数是输入表名称，第三个参数是输出表名称。 此作业统计按年份分组的 18 到 34 岁的用户数，并将结果写入我们在步骤 3：

    ```scala
    $ hadoop jar hcb-c6-samples-uber.jar \ 
    chapter6.hcat.HCatWriteMapReduce \
    bookcrossing users hcat_out

    ```

    中创建的`hcat_out`表
5.  通过在配置单元 CLI 中运行以下命令读取结果：

    ```scala
    hive> select * from bookcrossing.hcat_out limit 10;
    OK
    hcat_out.age    hcat_out.count
    19    3941
    20    4047
    21    4426
    22    4709
    23    5450
    24    5683

    ```

## 它是如何工作的.

您可以从`chapter6/src/chapter6/hcat/``HCatWriteMapReduce.java`文件中找到配方的来源。

除了我们在前面的*HCatalog-在映射到配置单元表格的数据上执行 Java MapReduce 计算*配方中讨论的配置之外，我们还指定`HCatalogOutputFormat` 作为`run()`函数中的计算的`OutputFormat`，如下所示。 我们还配置了输出数据库和表名称：

```scala
job.setOutputFormatClass(HCatOutputFormat.class);

HCatOutputFormat.setOutput(job,
    OutputJobInfo.create(dbName, outTableName, null));
```

将数据写入配置单元表格时，我们必须使用`DefaultHCatRecord`作为作业输出值：

```scala
job.setOutputKeyClass(WritableComparable.class);
job.setOutputValueClass(DefaultHCatRecord.class);
```

我们为输出表设置模式，如下所示：

```scala
HCatSchema schema = HCatOutputFormat.getTableSchema(job
                          .getConfiguration());
HCatOutputFormat.setSchema(job, schema);
```

`reduce()`函数将数据输出为`HCatRecord`值。 `HCatOutputFormat`忽略任何输出键：

```scala
public void reduce(IntWritable key, Iterable<IntWritable> values,
        Context context) … {
  if (key.get() < 34 & key.get() > 18) {
     int count = 0;
     for (IntWritable val : values) {
    count += val.get();
     }

      HCatRecord record = new DefaultHCatRecord(2);
     record.set(0, key.get());
     record.set(1, count);
     context.write(null, record);
  }
}
```