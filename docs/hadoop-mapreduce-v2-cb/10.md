# 十、海量文本数据处理

在本章中，我们将介绍以下主题：

*   使用 Hadoop Streaming 和 Python 进行数据预处理(提取、清理和格式转换)
*   使用 Hadoop 流消除重复数据
*   将大型数据集加载到 Apache HBase 数据存储-import 和 Bulkload
*   为文本数据创建 TF 和 TF-IDF 矢量
*   使用 Apache Mahout 对文本数据进行聚类
*   基于潜在狄利克雷分配(LDA)的主题发现
*   基于 Mahout 朴素贝叶斯分类器的文档分类

# 简介

Hadoop MapReduce 与支持项目集一起，使其成为处理大型文本数据集和执行**提取-转换-加载**(**ETL**)类型操作的理想框架。

在本章中，我们将探讨如何使用 Hadoop Streaming 执行数据提取、格式转换和重复数据删除等数据预处理操作。 我们还将使用 HBase 作为数据存储来存储数据，并探索以最小的开销将大量数据加载到 HBase 的机制。 最后，我们将研究如何使用 Apache Mahout 算法执行文本分析。

我们将对本章中的食谱使用以下示例数据集：

*   20 个新闻组数据集，位于[http://qwone.com/~jason/20Newsgroups](http://qwone.com/~jason/20Newsgroups)。 此数据集包含大约 20,000 个最初由 Ken Lang 收集的新闻组文档。

### 提示

**示例代码**

本书的示例代码文件位于 gihub 的[https://github.com/thilg/hcb-v2](https://github.com/thilg/hcb-v2)。 代码库的`chapter10`文件夹包含本章的示例代码。

# 使用 Hadoop Streaming 和 Python 进行数据预处理

数据预处理是数据分析中重要的，通常也是必需的组件。 当使用从多个不同来源生成的非结构化文本数据时，Data 预处理变得更加重要。 数据预处理步骤包括清理数据、从数据中提取重要特征、从数据集中删除重复项、转换数据格式等操作。

Hadoop MapReduce 提供了在处理海量数据集时并行执行这些任务的理想环境。 除了使用 Java MapReduce 程序或 Pig 脚本或 Have 脚本对数据进行预处理外，Hadoop 还包含一些其他工具和功能，这些工具和功能可用于执行这些数据预处理操作。 其中一个特性是`InputFormat`s，它为我们提供了通过实现自定义`InputFormat`来支持自定义数据格式的能力。另一个特性是 Hadoop 流支持，它允许我们使用我们最喜欢的脚本语言来执行实际的数据清理和提取，而 Hadoop 将并行计算到数百个计算和存储资源。

在本食谱中，我们将使用 Hadoop Streaming 和基于 Python 脚本的 Mapper 来执行数据提取和格式转换。

## 做好准备

*   检查 Hadoop 工作节点上是否已经安装了 Python。 如果没有，请在所有 Hadoop 工作节点上安装 Python。

## 怎么做……

以下步骤显示如何从 20news 数据集中清理和提取数据，并将数据存储为制表符分隔的文件：

1.  从[http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz](http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz)：

    ```scala
    $ wget http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz
    $ tar –xzf 20news-19997.tar.gz

    ```

    下载并解压 20news 数据集
2.  将提取的数据上传到 HDFS。 在中，为了节省计算时间和资源，您只能使用数据集的子集：

    ```scala
    $ hdfs dfs -mkdir 20news-all
    $ hdfs dfs –put  <extracted_folder> 20news-all

    ```

3.  解压本章的资源包并找到`MailPreProcessor.py`Python 脚本。
4.  在您的计算机中找到 Hadoop 安装的`hadoop-streaming.jar`JAR 文件。 使用该 JAR 运行以下 Hadoop 流命令。 /`usr/lib/hadoop-mapreduce/`是基于 bigtop 的 Hadoop 安装的`hadoop-streaming`jar 文件的位置：

    ```scala
    $ hadoop jar \
    /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -input 20news-all/*/* \
    -output 20news-cleaned \
    -mapper MailPreProcessor.py \
    -file MailPreProcessor.py

    ```

5.  使用以下命令检查结果：

    ```scala
    > hdfs dfs –cat 20news-cleaned/part-* | more

    ```

## 它是如何工作的.

Hadoop 使用默认的`TextInputFormat`作为上一次计算的输入规范。 使用`TextInputFormat`将为输入数据集中的每个文件生成一个 Map 任务，并为每行生成一条 Map 输入记录。 Hadoop 流通过标准输入向地图应用提供输入：

```scala
line =  sys.stdin.readline();
while line:
….
  if (doneHeaders):
    list.append( line )
  elif line.find( "Message-ID:" ) != -1:
    messageID = line[ len("Message-ID:"):]
  ….
  elif line == "":
    doneHeaders = True

   line = sys.stdin.readline();
```

前面的 Python 代码从标准输入读取输入行，直到它到达文件末尾。 我们解析新闻组文件的标题，直到遇到分隔标题和消息内容的空行。 消息内容将逐行读入列表：

```scala
value = ' '.join( list )
value = fromAddress + "\t" ……"\t" + value
print '%s\t%s' % (messageID, value)
```

前面的代码段将消息内容合并为单个字符串，并将流应用的输出值构造为一组以制表符分隔的所选头，后跟消息内容。 输出键值是从输入文件中提取的`Message-ID`头。 通过使用制表符来分隔键和值，将输出写入标准输出。

## 还有更多...

通过将`SequenceFileOutputFormat`指定为流计算的`OutputFormat` ，我们可以生成 Hadoop`SequenceFile`格式的上一次计算的输出：

```scala
$ hadoop jar \
/usr/lib/Hadoop-mapreduce/hadoop-streaming.jar \
-input 20news-all/*/* \
-output 20news-cleaned \
-mapper MailPreProcessor.py \
-file MailPreProcessor.py \
-outputformat \
 org.apache.hadoop.mapred.SequenceFileOutputFormat \
-file MailPreProcessor.py

```

在第一次传递输入数据之后，最好将数据存储为`SequenceFiles`(或其他 Hadoop 二进制文件格式，如 avro)，因为`SequenceFiles`占用的空间较少，并且支持压缩。 您可以使用`hdfs dfs -text <path_to_sequencefile>`将`SequenceFile`的内容输出为文本：

```scala
$ hdfs dfs –text 20news-seq/part-* | more

```

但是，要使前面的命令起作用，`SequenceFile`中使用的任何可写类都应该在 Hadoop 类路径中可用。

## 另请参阅

*   请参阅[第 4 章](04.html "Chapter 4. Developing Complex Hadoop MapReduce Applications")，*开发复杂 Hadoop MapReduce 应用的[章](04.html "Chapter 4. Developing Complex Hadoop MapReduce Applications")，*中的*将 Hadoop 与遗留应用结合使用-Hadoop Streaming*和*添加对新输入数据格式的支持-实现自定义 InputFormat*食谱。**

# 使用 Hadoop 流消除重复数据

通常，数据集包含需要消除的重复项，以确保结果的准确性。 在本食谱中，我们使用 Hadoop 删除 20news 数据集中的重复邮件记录。 这些重复记录是由于用户将同一消息交叉发布到多个新闻板造成的。

## 做好准备

*   确保您的 Hadoop 计算节点上安装了 Python。

## 怎么做……

以下步骤显示了如何从 20news 数据集中删除由于跨列表交叉发布而导致的重复邮件：

1.  从[http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz](http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz)：

    ```scala
    $ wget http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz
    $ tar –xzf 20news-19997.tar.gz

    ```

    下载并解压 20news 数据集
2.  将提取的数据上传到 HDFS。 为了节省计算时间和资源，您只能使用数据集的子集：

    ```scala
    $ hdfs dfs -mkdir 20news-all
    $ hdfs dfs –put  <extracted_folder> 20news-all

    ```

3.  我们将使用前面配方中的`MailPreProcessor.py`Python 脚本，使用 Hadoop 流进行*数据预处理，并使用 Python*作为映射器。 在本章的源代码存储库中找到`MailPreProcessorReduce.py`文件。
4.  执行以下命令：

    ```scala
    $ hadoop jar \
    /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -input 20news-all/*/* \
    -output 20news-dedup\
    -mapper MailPreProcessor.py \
    -reducer MailPreProcessorReduce.py \
    -file MailPreProcessor.py\
    -file MailPreProcessorReduce.py

    ```

5.  使用以下命令检查结果：

    ```scala
    $ hdfs dfs –cat 20news-dedup/part-00000 | more

    ```

## 它是如何工作的.

Mapper Python 脚本输出 MessageID 作为键。 我们使用 MessageID 来标识跨不同新闻组交叉发布的重复邮件。

Hadoop Streaming 通过标准输入将每个键组的 Reducer 输入记录逐行提供给 Streaming Reducer 应用。 但是，Hadoop 流没有区分新键值组的机制。 当 Hadoop 开始向进程提供新密钥的记录时，流减少器应用需要跟踪输入键以识别新组。 由于我们使用 MessageID 输出 Mapper 结果，因此 Reducer 输入将按 MessageID 分组。 每个 MessageID 包含多个值(也称为一条消息)的任何组都包含重复项。 在下面的脚本中，我们只使用记录组的第一个值(Message)，并丢弃其他值(即重复的消息)：

```scala
#!/usr/bin/env python
import sys;

currentKey = ""

for line in sys.stdin:
  line = line.strip()
  key, value = line.split('\t',1)
  if currentKey == key :
    continue
  print '%s\t%s' % (key, value)
```

## 另请参阅

*   在遗留应用中使用 Hadoop 的*-本章第 4 章、*开发复杂 Hadoop MapReduce 应用的 Hadoop Streaming*配方，以及本章的使用 Hadoop Streaming 和 Python*配方的*数据预处理。*

# 将大型数据集加载到 Apache HBase 数据存储-import 和 Bulkload

当以半结构化方式存储大规模数据时，Apache HBase data 存储非常有用，这样它就可以用于使用 Hadoop MapReduce 程序进行进一步处理，或者为客户端应用提供随机访问数据存储。 在本食谱中，我们将使用`importtsv`和`bulkload`工具将大型文本数据集导入 HBase。

## 做好准备

1.  在 Hadoop 群集中安装和部署 Apache HBase。
2.  确保您的 Hadoop 计算节点中安装了 Python。

## How to Do It…

以下步骤显示如何将 TSV(制表符分隔值)转换后的 20news 数据集加载到 HBase 表中：

1.  遵循*数据预处理(使用 Hadoop Streaming 和 Python*配方)来执行此配方的数据预处理。 我们假设该配方的以下步骤 4 的输出存储在名为“`20news-cleaned`”的 HDFS 文件夹中：

    ```scala
    $ hadoop jar \
     /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
     -input 20news-all/*/* \
     -output 20news-cleaned \
     -mapper MailPreProcessor.py \
     -file MailPreProcessor.py

    ```

2.  启动 HBase 外壳：

    ```scala
    $ hbase shell

    ```

3.  通过在 HBase shell 中执行以下命令创建名为 20news-data 的表。 旧版本的`importtsv`命令(在下一步中使用)只能处理单个列族。 因此，在创建 HBase 表时，我们仅使用单个列族：

    ```scala
    hbase(main):001:0> create '20news-data','h'

    ```

4.  执行以下命令以将预处理数据导入到前面创建的 HBase 表中：

    ```scala
    $ hbase \
     org.apache.hadoop.hbase.mapreduce.ImportTsv \
     -Dimporttsv.columns=HBASE_ROW_KEY,h:from,h:group,h:subj,h:msg \
     20news-data 20news-cleaned

    ```

5.  启动 HBase Shell 并使用 HBase Shell 的 COUNT 和 SCAN 命令验证表的内容：

    ```scala
    hbase(main):010:0> count '20news-data'
     12xxx row(s) in 0.0250 seconds

    hbase(main):010:0> scan '20news-data', {LIMIT => 10}
     ROW                                       COLUMN+CELL 
     <1993Apr29.103624.1383@cronkite.ocis.te column=h:c1,    timestamp=1354028803355, value= katop@astro.ocis.temple.edu (Chris Katopis)>
     <1993Apr29.103624.1383@cronkite.ocis.te column=h:c2,  timestamp=1354028803355, value= sci.electronics 
    ......

    ```

以下是使用`bulkload`功能将`20news`数据集加载到 HBase 表的步骤：

1.  按照步骤 1 到 3 操作，但使用不同的名称创建表：

    ```scala
    hbase(main):001:0> create '20news-bulk','h'

    ```

2.  使用以下命令生成 HBase`bulkload`数据文件：

    ```scala
    $ hbase \
     org.apache.hadoop.hbase.mapreduce.ImportTsv \
    -Dimporttsv.columns=HBASE_ROW_KEY,h:from,h:group,h:subj,h:msg\
    -Dimporttsv.bulk.output=hbaseloaddir \
    20news-bulk–source 20news-cleaned

    ```

3.  列出文件以验证是否生成了`bulkload`数据文件：

    ```scala
    $ hadoop fs -ls 20news-bulk-source
    ......
    drwxr-xr-x   - thilina supergroup          0 2014-04-27 10:06 /user/thilina/20news-bulk-source/h

    $ hadoop fs -ls 20news-bulk-source/h
    -rw-r--r--   1 thilina supergroup      19110 2014-04-27 10:06 /user/thilina/20news-bulk-source/h/4796511868534757870

    ```

4.  以下命令通过将输出文件移动到正确位置将数据加载到 HBase 表中：

    ```scala
    $ hbase \
     org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles \
     20news-bulk-source 20news-bulk
    ......
    14/04/27 10:10:00 INFO mapreduce.LoadIncrementalHFiles: Trying to load hfile=hdfs://127.0.0.1:9000/user/thilina/20news-bulk-source/h/4796511868534757870 first= <1993Apr29.103624.1383@cronkite.ocis.temple.edu>last= <stephens.736002130@ngis>
    ......

    ```

5.  启动 HBase Shell 和使用 HBase Shell 的`count`和`scan`命令验证表的内容：

    ```scala
    hbase(main):010:0> count '20news-bulk' 
    hbase(main):010:0> scan '20news-bulk', {LIMIT => 10}

    ```

## 它是如何工作的.

`MailPreProcessor.py`Python 脚本从新闻板消息中提取一组选定的数据字段，并将其作为制表符分隔的数据集输出：

```scala
value = fromAddress + "\t" + newsgroup 
+"\t" + subject +"\t" + value
print '%s\t%s' % (messageID, value)
```

我们使用`importtsv`工具将流式 MapReduce 计算生成的以制表符分隔的数据集导入 HBase。 `importtsv`工具要求数据除了分隔数据字段的制表符之外，不能有其他制表符。 因此，我们使用以下 Python 脚本片段删除输入数据中可能存在的任何制表符：

```scala
line = line.strip()
line = re.sub('\t',' ',line)
```

`importtsv`工具支持直接使用`Put`操作以及通过生成 HBase 内部`HFiles`将数据加载到 HBase 中。 以下命令使用`Put`操作直接将数据加载到 HBase。 我们生成的数据集在值中包含一个键和四个字段。 我们使用`-Dimporttsv.columns`参数将数据字段指定到数据集的表列名称映射。 此映射包括按照输入数据集中以制表符分隔的数据字段的顺序列出各自的表列名称：

```scala
$ hbase \
 org.apache.hadoop.hbase.mapreduce.ImportTsv \
 -Dimporttsv.columns=<data field to table column mappings> \ 
 <HBase tablename> <HDFS input directory>

```

我们可以使用命令后面的命令为数据集生成 HBase HFiles。 这些 HFile 无需通过 HBase API 即可直接加载到 HBase，从而减少了所需的 CPU 和网络资源量：

```scala
$ hbase \
 org.apache.hadoop.hbase.mapreduce.ImportTsv \
 -Dimporttsv.columns=<filed to column mappings> \ 
 -Dimporttsv.bulk.output=<path for hfile output> \
 <HBase tablename> <HDFS input directory>

```

只需将文件移动到正确的位置，就可以将这些生成的 HFile 加载到 HBase 表中。 此移动可通过使用`completebulkload`命令执行：

```scala
$ hbase \org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles \
 <HDFS path for hfiles> <table name>

```

## 还有更多...

您也可以使用`importtsv`工具，该工具具有带有其他数据字段分隔符字符的数据集，方法是指定‘-Dimport tsv.Separator’参数。 以下是使用逗号作为分隔符将逗号分隔的数据集导入到 HBase 表的示例：

```scala
$ hbase \
 org.apache.hadoop.hbase.mapreduce.ImportTsv \
 '-Dimporttsv.separator=,' \
 -Dimporttsv.columns=<data field to table column mappings> \ 
 <HBase tablename> <HDFS input directory>

```

在 MapReduce 作业控制台输出或 Hadoop 监控控制台中查找中的`Bad Lines`。 使用`Bad Lines`的原因之一是使用不需要的分隔符。 我们在数据清理步骤中使用的 Python 脚本删除了消息中的任何额外选项卡：

```scala
14/03/27 00:38:10 INFO mapred.JobClient:   ImportTsv
14/03/27 00:38:10 INFO mapred.JobClient:     Bad Lines=2

```

### 使用 HBase 进行重复数据消除

HBase 支持为每条记录存储多个版本的列值。 查询时，除非我们特别提到时间段，否则 HBase 会返回最新版本的值。 通过确保对重复值使用相同的 RowKey，可以使用 HBase 的此功能执行自动重复数据消除。 在我们的 20news 示例中，我们使用 MessageID 作为记录的 RowKey，确保重复的消息将显示为同一数据记录的不同版本。

HBase 允许我们配置每个列系列的最大或最小版本数。 将最大版本数设置为较低值将通过丢弃旧版本来减少数据使用量。 有关设置最大或最小版本数的详细信息，请参阅[http://hbase.apache.org/book/schema.versions.html](http://hbase.apache.org/book/schema.versions.html)。

## 另请参阅

*   在[第 7 章](07.html "Chapter 7. Hadoop Ecosystem II – Pig, HBase, Mahout, and Sqoop")，*Hadoop 生态系统 II 的 HBase*配方上运行 MapReduce 作业的*--Pig、HBase、Mahout 和 Sqoop*。
*   有关`ImportTsv`命令的更多信息，请参考[http://hbase.apache.org/book/ops_mgt.html#importtsv](http://hbase.apache.org/book/ops_mgt.html#importtsv)。

# 为文本数据创建 TF 和 TF-IDF 矢量

大多数文本分析数据挖掘算法对矢量数据进行操作。 我们可以使用向量空间模型将文本数据表示为一组向量。 例如，我们可以通过获取数据集中出现的所有术语的集合并为术语集中的每个术语分配索引来构建向量空间模型。 术语集中的项的数量是结果向量的维度，并且向量的每个维度对应于一个项。 对于每个文档，向量包含每个术语在分配给该特定术语的索引位置的出现次数。 这将使用每个文档中的个词频创建向量空间模型，这与我们在使用[第 8 章](08.html "Chapter 8. Searching and Indexing")、*搜索和索引*的 Hadoop MapReduce 配方生成倒排索引的*中执行的计算结果类似。*

向量可以如下所示：

![Creating TF and TF-IDF vectors for the text data](img/5471OS_10_01.jpg)

词频和结果文档向量

然而，使用前面的术语计数模型创建向量时，许多文档中频繁出现的术语(例如，the、is、a、are、was、who 等等)会被赋予很高的权重，尽管这些频繁出现的术语在定义文档含义方面的贡献非常小()。 **术语频率-逆文档频率**(**TF-IDF**)模型通过利用**倒置文档频率**(**IDF**)来缩放**术语频率**(**TF**)来解决该问题。 IDF 通常为，计算方法是：首先对出现该术语的文档数(Df)进行计数，将其倒置(1/df)，然后将其与文档数相乘，并使用所得值的对数进行归一化，大致如以下公式所示：

![Creating TF and TF-IDF vectors for the text data](img/5471OS_10_03.jpg)

在本食谱中，我们将使用 Apache Mahout 的内置实用工具从文本数据集创建 TF-IDF 向量。

## 做好准备

使用 Hadoop 发行版在您的计算机上安装 Apache Mahout，或手动安装最新版本的 Apache Mahout。

## How to Do It…

下面的步骤向您展示了如何从到构建 20news 数据集的矢量模型：

1.  从[http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz](http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz)：

    ```scala
    $ wget http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz
    $ tar –xzf 20news-19997.tar.gz

    ```

    下载并解压 20news 数据集
2.  将提取的数据上传到 HDFS。 为了节省计算时间和资源，您可以只使用数据集的一个子集：

    ```scala
    $ hdfs dfs -mkdir 20news-all
    $ hdfs dfs –put  <extracted_folder> 20news-all

    ```

3.  转到`MAHOUT_HOME`。 从上传的文本数据生成 Hadoop 序列文件：

    ```scala
    $ mahout seqdirectory -i 20news-all -o 20news-seq

    ```

4.  Generate TF and TF-IDF sparse vector models from the text data in the sequence files:

    ```scala
    $ mahout seq2sparse -i 20news-seq  -o 20news-vector 

    ```

    前面的命令启动一系列 MapReduce 计算。 等待以下计算完成：

    ![How to do it…](img/5471OS_10_02.jpg)

5.  使用以下命令检查输出目录。 `tfidf-vectors`文件夹包含 TF-IDF 模型向量，`tf-vectors`文件夹包含术语计数模型向量，`dictionary.file-0`包含术语到术语索引的映射：

    ```scala
    $ hdfs dfs -ls 20news-vector

    Found 7 items
    drwxr-xr-x   - u supergroup          0 2012-11-27 16:53 /user/u/20news-vector /df-count
    -rw-r--r--   1 u supergroup       7627 2012-11-27 16:51 /user/u/20news-vector/dictionary.file-0
    -rw-r--r--   1 u supergroup       8053 2012-11-27 16:53 /user/u/20news-vector/frequency.file-0
    drwxr-xr-x   - u supergroup          0 2012-11-27 16:52 /user/u/20news-vector/tf-vectors
    drwxr-xr-x   - u supergroup          0 2012-11-27 16:54 /user/u/20news-vector/tfidf-vectors
    drwxr-xr-x   - u supergroup          0 2012-11-27 16:50 /user/u/20news-vector/tokenized-documents
    drwxr-xr-x   - u supergroup          0 2012-11-27 16:51 /user/u/20news-vector/wordcount

    ```

6.  或者，可以使用以下命令将 TF-IDF 矢量转储为文本。 关键字是文件名，矢量内容的格式为`<term index>:<TF-IDF value>`：

    ```scala
    $ mahout seqdumper -i 20news-vector/tfidf-vectors/part-r-00000

    ……
    Key class: class org.apache.hadoop.io.Text Value Class: class org.apache.mahout.math.VectorWritable
    Key: /54492: Value: {225:3.374729871749878,400:1.5389964580535889,321:1.0,324:2.386294364929199,326:2.386294364929199,315:1.0,144:2.0986123085021973,11:1.0870113372802734,187:2.652313232421875,134:2.386294364929199,132:2.0986123085021973,......}
    ……

    ```

## …的工作原理

Hadoop SequenceFiles 将数据存储为二进制键-值对，并支持数据压缩。 Mahout 的`seqdirectory`命令将文本文件转换为 Hadoop SequenceFile，方法是使用文本文件的文件名作为键，将文本文件的内容用作值。 `seqdirectory`命令将所有文本内容存储在单个 SequenceFile 中。 但是，我们可以指定块大小来控制 HDFS 中 SequenceFile 数据块的实际存储。 以下是`seqdirectory`命令的一组选定选项：

```scala
mahout seqdirectory –i <HDFS path to text files> -o <HDFS output directory for sequence file> 
 -ow                   If present, overwrite the output directory 
 -chunk <chunk size>   In MegaBytes. Defaults to 64mb 
 -prefix <key prefix>  The prefix to be prepended to the key 

```

`seq2sparse`命令是一个 Apache Mahout 工具，支持从包含文本数据的 SequenceFiles 生成稀疏向量。 它支持生成 TF 和 TF-IDF 矢量模型。 此命令作为一系列 MapReduce 计算执行。 以下是为`seq2sparse`命令选择的一组选项：

```scala
mahout seq2sparse -i <HDFS path to the text sequence file> -o <HDFS output directory>
 -wt {tf|tfidf} 
 -chunk <max dictionary chunk size in mb to keep in memory> 
 --minSupport <minimum support>
 --minDF <minimum document frequency>
 --maxDFPercent <MAX PERCENTAGE OF DOCS FOR DF

```

`minSupport`命令是将单词视为特征的最低频率。 `minDF`是 Word 需要包含的最小文档数。 `maxDFPercent`是表达式的最大值(单词的文档频率/文档总数)，以便将该单词视为文档中的良好特征。 这有助于删除诸如停用词等高频特征。

您可以使用 Mahout`seqdumper`命令将使用 Mahout 可写数据类型的 SequenceFile 的内容转储为纯文本：

```scala
mahout seqdumper -i <HDFS path to the sequence file>
 -o <output directory>
 --count         Output only the number of key value pairs.
 --numItems      Max number of key value pairs to output
 --facets        Output the counts per key.

```

## 另请参阅

*   *使用 Hadoop MapReduce*配方(第 9 章，*分类、推荐和查找关系]的 Hadoop MapReduce*配方生成倒排索引。
*   有关从[https://cwiki.apache.org/confluence/display/MAHOUT/Creating+Vectors+from+Text](https://cwiki.apache.org/confluence/display/MAHOUT/Creating+Vectors+from+Text)处的文本数据创建矢量的信息，请参阅 Mahout 文档。

# 使用 Apache Mahout 对文本数据进行群集

聚类在数据挖掘计算中起着不可或缺的作用。 基于用例，使用数据项的一个或多个特征将数据集的相似项分组在一起。 文档聚类被用于许多文本挖掘操作，如文档组织、主题识别、信息表示等。 文档聚类与传统的数据聚类机制有许多相同的机制和算法。 然而，在确定用于聚类的特征以及构建表示文本文档的向量空间模型时，文档聚类有其独特的挑战。

第 7 章，*Hadoop 生态系统 II-Pig、HBase、Mahout 和 Sqoop*的*Running K-Means with Mahout*配方侧重于使用 Mahout KMeansClusters 来集群统计数据。 本书前一版的[章](08.html "Chapter 8. Searching and Indexing")、*分类、推荐和查找关系*中的*聚类亚马逊销售数据集*配方侧重于使用聚类来识别具有相似兴趣的客户。 这两个食谱总体上提供了对使用集群算法的更深入的理解。 本食谱重点介绍 Apache Mahout 中可用于文档集群的几种集群算法中的两种。

## 做好准备

*   使用 Hadoop 发行版在您的计算机上安装 Apache Mahout，或在您的计算机上手动安装最新的 Apache Mahout 版本。

## 怎么做……

以下步骤使用 Apache Mahout KmeansClusters 算法对 20news 数据集进行聚类：

1.  请参阅本章中的*为文本数据*配方创建 TF 和 TF-IDF 矢量，并为 20news 数据集生成 TF-IDF 矢量。 我们假设 TF-IDF 矢量位于 HDFS 的`20news-vector/tfidf-vectors`文件夹中。
2.  执行以下命令以运行 Mahout KMeansClusters 计算：

    ```scala
    $ mahout kmeans \
     --input 20news-vector/tfidf-vectors \
     --clusters 20news-seed/clusters
     --output 20news-km-clusters\
     --distanceMeasure \
    org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure-k 10 --maxIter 20 --clustering
    Execute the following command to convert the clusters to text:
    $ mahout clusterdump \
     -i 20news-km-clusters/clusters-*-final\
     -o 20news-clusters-dump \
     -d 20news-vector/dictionary.file-0 \
     -dt sequencefile \
     --pointsDir 20news-km-clusters/clusteredPoints

    $ cat 20news-clusters-dump

    ```

## 它是如何工作的.

下面的代码显示了 Mahout KMeans 算法的用法：

```scala
mahout kmeans 
 --input <tfidf vector input>
 --clusters <seed clusters>
 --output <HDFS path for output>
 --distanceMeasure <distance measure>-k <number of clusters>--maxIter <maximum number of iterations>--clustering

```

当为`--clusters`选项提供空的 HDFS 目录路径时，mahout 将生成随机种子群集。 Manhout 支持几种不同的距离计算方法，如欧几里得、余弦和曼哈顿。

以下是 Mahout`clusterdump`命令的用法：

```scala
mahout clusterdump -i <HDFS path to clusters>-o <local path for text output>
 -d <dictionary mapping for the vector data points>
 -dt <dictionary file type (sequencefile or text)>
 --pointsDir <directory containing the input vectors to       clusters mapping>

```

## 另请参阅

*   第 7 章，*Hadoop 生态系统 II 的[章](07.html "Chapter 7. Hadoop Ecosystem II – Pig, HBase, Mahout, and Sqoop")，*的*Running K-Means with Mahout*配方--Pig、HBase、Mahout 和 Sqoop*。*

# 基于潜在 Dirichlet 分配(LDA)的主题发现

我们可以使用**潜在 Dirichlet 分配**(**LDA**)将给定的词集聚集成主题，将一组文档聚集成主题的组合。 LDA 在基于上下文识别文档或单词的含义时非常有用，而不是完全依赖于单词的数量或确切的单词。 LDA 从原始文本匹配向语义分析迈进了一步。 LDA 可用于在诸如搜索引擎的系统中识别意图并解决歧义词。 LDA 的其他一些示例用例包括针对特定主题识别有影响力的推特用户，以及 Twahpicc([http://twahpic.cloudapp.net](http://twahpic.cloudapp.net))应用使用 LDA 识别推特上使用的主题。

LDA 使用 TF 向量空间模型，而不是 TF-IDF 模型，因为它需要考虑单词的共现和相关性。

## 做好准备

使用 Hadoop 发行版在您的计算机上安装 Apache Mahout，或手动安装最新版本的 Apache Mahout。

## How to Do It…

以下步骤向您展示了如何在 20news 数据集的子集上运行 Mahout LDA 算法：

1.  从[http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz](http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz)：

    ```scala
    $ wget http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz
    $ tar –xzf 20news-19997.tar.gz

    ```

    下载并解压 20news 数据集
2.  将提取的数据上传到 HDFS。 为了节省计算时间和资源，您可以只使用数据集的一个子集：

    ```scala
    $ hdfs dfs -mkdir 20news-all
    $ hdfs dfs –put  <extracted_folder> 20news-all

    ```

3.  从上传的文本数据生成序列文件：

    ```scala
    $ mahout seqdirectory -i 20news-all -o 20news-seq 

    ```

4.  从序列文件中的文本数据生成稀疏向量：

    ```scala
    $ mahout seq2sparse \
    –i 20news-seq  -o 20news-tf \
    -wt tf -a org.apache.lucene.analysis.WhitespaceAnalyzer

    ```

5.  将 TF 矢量从 SequenceFile<text vectorwritable="">转换为 SequenceFile<intwritable>：

    ```scala
    $ mahout rowid -i 20news-tf/tf-vectors -o 20news-tf-int

    ```</intwritable></text> 
6.  运行以下命令以执行 LDA 计算：

    ```scala
    $ mahout cvb \
    -i 20news-tf-int/matrix -o lda-out \
    -k 10  -x 20  \
    -dict 20news-tf/dictionary.file-0 \
    -dt lda-topics \
    -mt lda-topic-model

    ```

7.  转储并检查 LDA 计算的结果：

    ```scala
    $ mahout seqdumper -i lda-topics/part-m-00000

    Input Path: lda-topics5/part-m-00000
    Key class: class org.apache.hadoop.io.IntWritable Value Class: class org.apache.mahout.math.VectorWritable
    Key: 0: Value: {0:0.12492744375758073,1:0.03875953927132082,2:0.1228639250669511,3:0.15074522974495433,4:0.10512715697420276,5:0.10130565323653766,6:0.061169131590630275,7:0.14501579630233746,8:0.07872957132697946,9:0.07135655272850545}
    .....

    ```

8.  将输出向量与术语到术语索引的字典映射连接起来：

    ```scala
    $ mahout vectordump \
    -i lda-topics/part-m-00000 \
    --dictionary 20news-tf/dictionary.file-0 \
    --vectorSize 10  -dt sequencefile 

    ......

    {"Fluxgate:0.12492744375758073,&:0.03875953927132082,(140.220.1.1):0.1228639250669511,(Babak:0.15074522974495433,(Bill:0.10512715697420276,(Gerrit:0.10130565323653766,(Michael:0.061169131590630275,(Scott:0.14501579630233746,(Usenet:0.07872957132697946,(continued):0.07135655272850545}
    {"Fluxgate:0.13130952097888746,&:0.05207587369196414,(140.220.1.1):0.12533225607394424,(Babak:0.08607740024552457,(Bill:0.20218284543514245,(Gerrit:0.07318295757631627,(Michael:0.08766888242201039,(Scott:0.08858421220476514,(Usenet:0.09201906604666685,(continued):0.06156698532477829}
    .......

    ```

## …的工作原理

LDA 的 Mahout CVB 版本使用迭代 MapReduce 方法实现折叠变量贝叶斯推理算法：

```scala
mahout cvb \
-i 20news-tf-int/matrix \
-o lda-out -k 10  -x 20 \
-dict 20news-tf/dictionary.file-0 \
-dt lda-topics \
-mt lda-topic-model

```

`-i`参数提供输入路径，而`-o`参数提供存储输出的路径。 参数`-k`指定要学习的主题数，`–x`指定计算的最大迭代次数。 `-dict`参数指向包含术语到术语索引的映射的字典。 `–dt`参数中给出的路径存储训练主题分布。 `–mt`中给出的路径用作存储中间模型的临时位置。

通过调用`help`选项可以查询`cvb`命令的所有命令行选项，如下所示：

```scala
mahout  cvb  --help

```

将主题数量设置为非常小的值将显示极高级别的主题。 大量的主题会产生更多描述性的主题，但需要更长的处理时间。 可以使用`maxDFPercent`选项删除常用单词，从而加快处理速度。

## 另请参阅

*   *T.W.Teh，D.Newman 和 M.Well 提出的潜在 Dirichlet 分配的折叠变分贝叶斯推理算法*。 在 NIPS 中，2006 年第 19 卷，可在[http://www.gatsby.ucl.ac.uk/~ywteh/research/inference/nips2006.pdf](http://www.gatsby.ucl.ac.uk/~ywteh/research/inference/nips2006.pdf)找到。

# 基于 Mahout 朴素贝叶斯分类器的文档分类

分类将文档或数据项分配给一组已知的具有已知属性的类。 当我们需要将文档分配到一个或多个类别时，可以使用文档分类。 这是信息检索和图书馆学中的常见用例。

[第 9 章](09.html "Chapter 9. Classifications, Recommendations, and Finding Relationships")，*分类、建议和查找关系*中的*分类使用朴素贝叶斯分类器*配方提供了关于分类用例的更详细描述，还概述了使用朴素贝叶斯分类器算法。 本食谱重点介绍 Apache Mahout 中对文本文档的分类支持。

## 做好准备

*   使用 Hadoop 发行版在您的计算机上安装 Apache Mahout，或手动安装最新版本的 Apache Mahout。

## 怎么做……

以下个步骤使用 ApacheMahout 朴素贝叶斯算法对 20news 数据集进行聚类：

1.  请参阅本章中的*为文本数据*配方创建 TF 和 TF-IDF 矢量，并为 20news 数据集生成 TF-IDF 矢量。 我们假设 TF-IDF 矢量位于 HDFS 的`20news-vector/tfidf-vectors`文件夹中。
2.  将数据拆分为训练和测试数据集：

    ```scala
    $ mahout split \
     -i 20news-vectors/tfidf-vectors \
     --trainingOutput /20news-train-vectors \
     --testOutput /20news-test-vectors  \
     --randomSelectionPct 40 \
    --overwrite --sequenceFiles 

    ```

3.  训练朴素贝叶斯模型：

    ```scala
    $ mahout trainnb \
     -i 20news-train-vectors -el \
     -o  model \
     -li labelindex 

    ```

4.  测试数据集上的分类：

    ```scala
    $ mahout testnb \
     -i 20news-train-vectors \
     -m model \
     -l labelindex \
     -o 20news-testing 

    ```

## 它是如何工作的.

Mahout 的`split`命令可用于将数据集分割为训练数据集和测试数据集。 此命令适用于文本数据集以及 Hadoop SequenceFile 数据集。 以下是 Mahout`data-splitting`命令的用法。 您可以将`--help`选项与`split`命令一起使用，以打印出所有选项：

```scala
mahout split \
 -i <input data directory> \
 --trainingOutput <HDFS path to store the training dataset> \
 --testOutput <HDFS path to store the test dataset>  \
 --randomSelectionPct <percentage to be selected as test data> \ 
 --sequenceFiles 

```

`sequenceFiles`选项指定输入数据集在 Hadoop SequenceFiles 中。

以下是 Mahout naive Bayes 分类器训练命令的用法。 `--el`选项通知 Mahout 从输入数据集中提取标签：

```scala
mahout trainnb \
 -i <HDFS path to the training data set> \
 -el \
 -o <HDFS path to store the trained classifier model> \
 -li <Path to store the label index> \

```

以下是 Mahout naive Bayes 分类器测试命令的用法：

```scala
mahout testnb \
 -i <HDFS path to the test data set>
 -m <HDFS path to the classifier model>\
 -l <Path to the label index> \
 -o <path to store the test result>

```

## 另请参阅

*   使用[第 9 章](09.html "Chapter 9. Classifications, Recommendations, and Finding Relationships")、*分类、建议和查找关系的朴素贝叶斯分类器*配方的*分类*