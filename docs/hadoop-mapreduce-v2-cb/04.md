# 四、开发复杂的 Hadoop MapReduce 应用

在本章中，我们将介绍以下食谱：

*   选择合适的 Hadoop 数据类型
*   实现自定义 Hadoop 可写数据类型
*   实现自定义 Hadoop 密钥类型
*   从映射器发出不同值类型的数据
*   为您的输入数据格式选择合适的 Hadoop InputFormat
*   添加对新输入数据格式的支持-实现自定义 InputFormat
*   设置 MapReduce 计算结果的格式-使用 Hadoop OutputFormats
*   从 MapReduce 计算写入多个输出
*   Hadoop 中间数据分区
*   二次排序-排序可减少输入值
*   向 MapReduce 作业中的任务广播和分发共享资源-Hadoop DistributedCache
*   将 Hadoop 与旧式应用配合使用-Hadoop 流
*   在 MapReduce 作业之间添加依赖关系
*   用于报告自定义指标的 Hadoop 计数器

# 简介

本章将向您介绍几个高级 Hadoop MapReduce 功能，这些功能将帮助您开发高度自定义、高效的 MapReduce 应用。

![Introduction](img/5471OS_04_01.jpg)

上图描述了 Hadoop MapReduce 计算的典型流程。 InputFormat 从 HDFS 读取输入数据，并解析数据以创建`map`函数的键-值对输入。 InputFormat 还执行数据的逻辑分区，以创建计算的 Map 任务。 典型的 MapReduce 计算为每个输入 HDFS 数据块创建一个 Map 任务。 Hadoop 为每个生成的键-值对调用用户提供的`map`函数。 正如在[第 1 章](01.html "Chapter 1. Getting Started with Hadoop v2")，*Hadoop v2*入门中提到的，如果提供，可以使用`map`函数的输出数据调用可选的组合器步骤。

然后，**分区程序**步骤对 Map 任务的输出数据进行分区，以便将它们发送到相应的 Reduce 任务。 这种分区是使用 Map 任务输出键-值对的键字段执行的，其结果是分区的数量等于 Reduce 任务的数量。 每个 Reduce 任务从 Map 任务获取各自的输出数据分区(也称为**改组**)，并基于键字段对数据执行合并排序。 在调用`reduce`函数之前，Hadoop 还根据数据的关键字段将输入数据分组到 Reduce 函数。 Reduce 任务的输出键-值对将根据 OutputFormat 类指定的格式写入 HDFS。

在本章中，我们将详细探讨前面提到的 Hadoop MapReduce 计算高级流程的不同部分，并探索每个步骤可用的选项和定制。 首先，您将了解 Hadoop 提供的不同数据类型，以及为 Hadoop MapReduce 计算实现自定义数据类型的步骤。 然后，我们将介绍 Hadoop 提供的不同数据 InputFormats 和 OutputFormats。 接下来，我们将基本了解如何在 Hadoop 中添加对新数据格式的支持，以及从单个 MapReduce 计算中输出多个数据产品的机制。 我们还将探索 Map 输出数据分区，并使用该知识介绍`reduce`函数输入数据值的二次排序。

除了上面的之外，我们还将讨论其他高级 Hadoop 功能，比如使用**DistributedCache**分发数据，使用 Hadoop 流功能快速构建 Hadoop 计算的原型，使用 Hadoop 计数器报告计算的自定义指标，以及添加作业依赖项来管理简单的基于 DAG 的 Hadoop MapReduce 计算工作流。

### 备注

**示例代码和数据**

本书的示例代码文件位于 gihub 的[https://github.com/thilg/hcb-v2](https://github.com/thilg/hcb-v2)。 代码库的`chapter4`文件夹包含本章的示例源代码文件。

您可以从[http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html)下载日志处理示例的数据。 您可以在[http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html)中找到此数据结构的说明。 此数据集的一小部分可用于测试，位于`chapter4/resources`的代码存储库中提供了该数据集的一小部分。

可以通过在代码库的`chapter4`文件夹中发出`gradle build`命令来编译示例代码。 Eclipse IDE 的项目文件可以通过在代码库的主文件夹中运行`gradle eclipse`命令来生成。 IntelliJ IDEA IDE 的项目文件可以通过在代码存储库的主文件夹中运行`gradle idea`命令来生成。

# 选择合适的 Hadoop 数据类型

Hadoop 使用可写的基于接口的类作为 MapReduce 计算的数据类型。 这些数据类型在整个 MapReduce 计算流程中使用，从读取输入数据开始，在 Map 和 Reduce 任务之间传输中间数据，最后在写入输出数据时使用。 为输入、中间和输出数据选择适当的`Writable`数据类型会对 MapReduce 程序的性能和可编程性产生很大影响。

要用作 MapReduce 计算的`value`数据类型，数据类型必须实现`org.apache.hadoop.io.Writable`接口。 `Writable`接口定义在传输和存储数据时 Hadoop 应该如何序列化和反序列化这些值。 为了用作 MapReduce 计算的`key`数据类型，数据类型必须实现`org.apache.hadoop.io.WritableComparable<T>`接口。 除了`Writable`接口的功能之外，`WritableComparable`接口还定义了如何将此类型的键实例相互比较以进行排序。

### 备注

**Hadoop 的可写入性与 Java 的可序列化**

与使用通用 Java 的本机序列化框架相比，Hadoop 基于 Writable 的序列化框架为 MapReduce 程序提供了更高效、更定制的数据序列化和表示。 与 Java 的序列化相反，Hadoop 的 Writable 框架不会写入类型名，每个对象都希望序列化数据的所有客户端都知道序列化数据中使用的类型。 省略类型名称可以使序列化过程更快，并产生可由非 Java 客户端轻松解释的紧凑、随机访问的序列化数据格式。 Hadoop 基于 Writable 的序列化还能够通过重用`Writable`对象来减少对象创建开销，这在 Java 的本机序列化框架中是不可能的。

## 怎么做……

以下步骤显示如何配置 Hadoop MapReduce 应用的输入和输出数据类型：

1.  使用泛型变量

    ```scala
    public class SampleMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

      public void map(LongWritable key, Text value,
        Context context) … {
    ……  }
    }
    ```

    指定映射器的输入(键：`LongWritable`，值：`Text`)和输出(键：`Text`，值：`IntWritable`)键-值对的数据类型
2.  使用泛型变量指定 Reducer 的输入(键：`Text`，值：`IntWritable`)和输出(键：`Text`，值：`IntWritable`)键-值对的数据类型。 Reducer 的输入键-值对数据类型应该与 Mapper 的输出键-值对匹配。

    ```scala
    public class Reduce extends Reducer<Text, IntWritable, Text, IntWritable> {

      public void reduce(Text key,
        Iterable<IntWritable> values, Context context) {
      ……  }
    }
    ```

3.  使用`Job`对象指定 MapReduce 计算的输出数据类型，如以下代码片段所示。 这些数据类型将用作 Reducer 和 Mapper 的输出类型，除非您按照步骤 4 专门配置 Mapper 输出类型。

    ```scala
    Job job = new Job(..);
    ….
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    ```

4.  或者，当 Mapper 和 Reducer 的输出键-值对具有不同的数据类型时，可以使用以下步骤为映射器的输出键-值对配置不同的数据类型。

    ```scala
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(IntWritable.class);
    ```

## 还有更多...

Hadoop 提供了几种原始数据类型，如`IntWritable`、`LongWritable`、`BooleanWritable`、`FloatWritable`和`ByteWritable`，它们是各自 Java 原始数据类型的`Writable`版本。 我们既可以使用这些类型作为`key`类型，也可以使用这些类型作为`value`类型。

下面的是另外几种 Hadoop 内置数据类型，我们既可以用作`key`类型，也可以用作`value`类型：

*   `Text`：此存储 UTF8 文本
*   `BytesWritable`：这个存储一个字节序列
*   `VIntWritable`和`VLongWritable`：这些存储变量长度整数值和长整型值
*   `NullWritable`：此是零长度可写类型，可在不想使用`key`或`value`类型时使用

以下 Hadoop 内置集合数据类型只能用作`value`类型：

*   `ArrayWritable`：这存储了一个属于`Writable`类型的值数组。 要使用`ArrayWritable`类型作为 Reducer 输入的`value`类型，您需要创建`ArrayWritable`的子类，以指定存储在其中的`Writable`值的类型。

    ```scala
    public class LongArrayWritable extends ArrayWritable {
      public LongArrayWritable() {
        super(LongWritable.class);
      }
    }
    ```

*   `TwoDArrayWritable`：此存储属于相同`Writable`类型的值的矩阵。 要使用`TwoDArrayWritable`类型作为 Reducer 输入的`value`类型，您需要通过创建类似于`ArrayWritable`类型的`TwoDArrayWritable`类型的子类来指定存储值的类型。

    ```scala
    public class LongTwoDArrayWritable extends TwoDArrayWritable {
      public LongTwoDArrayWritable() {
        super(LongWritable.class);
      }
    }
    ```

*   `MapWritable`：这个存储键-值对的映射。 键和值应为`Writable`数据类型。 您可以使用`MapWritable`函数，如下所示。 但是，您应该意识到，由于包含了映射中存储的每个对象的类名，所以`MapWritable`的序列化增加了轻微的性能损失。

    ```scala
    MapWritable valueMap = new MapWritable();
    valueMap.put(new IntWritable(1),new Text("test"));
    ```

*   `SortedMapWritable`：它存储键-值对的排序映射。 键应该实现`WritableComparable`接口。 `SortedMapWritable`的用法类似于`MapWritable`函数。

## 另请参阅

*   实现自定义 Hadoop 可写数据类型配方的
**   实现自定义 Hadoop 密钥类型配方的*

 **# 实现自定义 Hadoop 可写数据类型

在某些用例中，内置数据类型可能都不符合您的需求，或者针对您的用例优化的自定义数据类型可能比 Hadoop 内置数据类型执行得更好。 在这种情况下，我们可以通过实现`org.apache.hadoop.io.Writable`接口来定义数据类型的序列化格式，从而轻松地编写自定义的`Writable`数据类型。 基于`Writable`接口的类型可以用作 Hadoop MapReduce 计算中的`value`类型。

在本食谱中，我们为 HTTP 服务器日志条目实现一个示例 Hadoop`Writable`数据类型。 对于本示例，我们认为日志条目由五个字段组成：请求主机、时间戳、请求 URL、响应大小和 HTTP 状态代码。 以下是示例日志条目：

```scala
192.168.0.2 - - [01/Jul/1995:00:00:01 -0400] "GET /history/apollo/ HTTP/1.0" 200 6245

```

您可以从[ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz](ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz)下载示例 HTTP 服务器日志数据集。

## 怎么做……

以下是为 HTTP 服务器日志条目实现自定义 Hadoop`Writable`数据类型的步骤：

1.  编写实现`org.apache.hadoop.io.Writable`接口的新`LogWritable`类：

    ```scala
    public class LogWritable implements Writable{

      private Text userIP, timestamp, request;
      private IntWritable responseSize, status;

      public LogWritable() {
        this.userIP = new Text();
        this.timestamp=  new Text();
        this.request = new Text();
        this.responseSize = new IntWritable();
        this.status = new IntWritable();
      }
      public void readFields(DataInput in) throws IOException {
        userIP.readFields(in);
        timestamp.readFields(in);
        request.readFields(in);
        responseSize.readFields(in);
        status.readFields(in);
      }

      public void write(DataOutput out) throws IOException {
        userIP.write(out);
        timestamp.write(out);
        request.write(out);
        responseSize.write(out);
        status.write(out);
      }

    ……… // getters and setters for the fields
    }
    ```

2.  在 MapReduce 计算中使用新的`LogWritable`类型作为`value`类型。 在下面的示例中，我们使用`LogWritable`类型作为 Map 输出值类型：

    ```scala
    public class LogProcessorMap extends Mapper<LongWritable,
    Text, Text, LogWritable> {
    ….
    }

    public class LogProcessorReduce extends Reducer<Text,
    LogWritable, Text, IntWritable> {

      public void reduce(Text key,
      Iterable<LogWritable> values, Context context) {
         ……  }
    }
    ```

3.  相应地配置作业的输出类型。

    ```scala
    Job job = ……
    ….
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(LogWritable.class);
    ```

## 它是如何工作的.

`Writable`接口由两个方法`readFields()`和`write()`组成。 在`readFields()`方法内部，我们反序列化输入数据并填充`Writable`对象的字段。

```scala
  public void readFields(DataInput in) throws IOException {
    userIP.readFields(in);
    timestamp.readFields(in);
    request.readFields(in);
    responseSize.readFields(in);
    status.readFields(in);
  }
```

在前面的示例中，我们使用`Writable`类型作为自定义`Writable`类型的字段，并使用字段的`readFields()`方法来反序列化来自`DataInput`对象的数据。 还可以将 Java 基元数据类型用作`Writable`类型的字段，并使用`DataInput`对象的相应读取方法从基础流中读取值，如以下代码片段所示：

```scala
int responseSize = in.readInt();
String userIP = in.readUTF();
```

在`write()`方法内部，我们将`Writable`对象的字段写入底层流。

```scala
  public void write(DataOutput out) throws IOException {
    userIP.write(out);
    timestamp.write(out);
    request.write(out);
    responseSize.write(out);
    status.write(out);
  }
```

如果您使用 Java 基元数据类型作为`Writable`对象的字段，则可以使用`DataOutput`对象的相应写入方法将值写入基础流，如下所示：

```scala
out.writeInt(responseSize);
out.writeUTF(userIP);
```

## 还有更多...

请在实现自定义`Writable`数据类型时注意以下问题：

*   如果要向自定义`Writable`类添加自定义构造函数，请确保保留默认的空构造函数。
*   `TextOutputFormat`使用`toString()`方法序列化`key`和`value`类型。 如果您使用`TextOutputFormat`来序列化自定义`Writable`类型的实例，请确保您的自定义`Writable`数据类型有一个有意义的`toString()`实现。
*   在读取输入数据时，Hadoop 可能会重复使用`Writable` 类的实例。 在`readFields()`方法中填充对象时，不应依赖对象的现有状态。

## 另请参阅

实现自定义 Hadoop 密钥类型配方的*。*

# 实现自定义 Hadoop 密钥类型

Hadoop MapReduce`key`类型的实例应该能够相互比较以进行排序。 为了在 MapReduce 计算中用作`key`类型，Hadoop`Writable`数据类型应该实现`org.apache.hadoop.io.WritableComparable<T>`接口。 `WritableComparable`接口扩展了`org.apache.hadoop.io.Writable`接口，并添加了`compareTo()`方法来执行比较。

在此配方中，我们修改了实现自定义 Hadoop Writable 数据类型配方的*的`LogWritable`数据类型，以实现`WritableComparable`接口。*

## 怎么做……

以下是为 HTTP 服务器日志条目实现自定义 Hadoop`WritableComparable`数据类型的步骤，该数据类型使用请求主机名和时间戳进行比较。

1.  修改`LogWritable`类以实现`org.apache.hadoop.io.WritableComparable`接口：

    ```scala
    public class LogWritable implements
      WritableComparable<LogWritable> {

      private Text userIP, timestamp, request;
      private IntWritable responseSize, status;

      public LogWritable() {
        this.userIP = new Text();
        this.timestamp=  new Text();
        this.request = new Text();
        this.responseSize = new IntWritable();
        this.status = new IntWritable();
      }

      public void readFields(DataInput in) throws IOException {
        userIP.readFields(in);
        timestamp.readFields(in);
        request.readFields(in);
        responseSize.readFields(in);
        status.readFields(in);
      }

      public void write(DataOutput out) throws IOException {
        userIP.write(out);
        timestamp.write(out);
        request.write(out);
        responseSize.write(out);
        status.write(out);
      }

      public int compareTo(LogWritable o) {
        if (userIP.compareTo(o.userIP)==0){
             return (timestamp.compareTo(o.timestamp));
        }else return (userIP.compareTo(o.userIP);
      }

      public boolean equals(Object o) {
        if (o instanceof LogWritable) {
             LogWritable other = (LogWritable) o;
             return userIP.equals(other.userIP) && timestamp.equals(other.timestamp);
        }
        return false;
      }

      public int hashCode()
      {
        Return userIP.hashCode();
      }
       ……… // getters and setters for the fields
    }
    ```

2.  在 MapReduce 计算中，可以将`LogWritable`类型用作`key`类型或`value`类型。 在下面的示例中，我们使用`LogWritable`类型作为映射输出`key`类型：

    ```scala
    public class LogProcessorMap extends Mapper<LongWritable,
    Text, LogWritable, IntWritable> {
    …
    }

    public class LogProcessorReduce extends Reducer<LogWritable,
    IntWritable, Text, IntWritable> {

    public void reduce(LogWritablekey,
    Iterable<IntWritable> values, Context context) {
         ……  }
    }
    ```

3.  相应地配置作业的输出类型。

    ```scala
    Job job = ……
    …
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    job.setMapOutputKeyClass(LogWritable.class);
    job.setMapOutputValueClass(IntWritable.class);
    ```

## 它是如何工作的.

除了`Writable`接口的`readFields()`和`write()`方法外，`WritableComparable`接口还引入了`compareTo()`方法。 如果此对象小于、等于或大于与其进行比较的对象，则`compareTo()`方法应返回负整数、零或正整数。 在`LogWritable`实现中，如果用户的 IP 地址和时间戳相同，我们认为对象相等。 如果对象不相等，我们将决定排序顺序，首先基于用户 IP 地址，然后基于时间戳。

```scala
  public int compareTo(LogWritable o) {
    if (userIP.compareTo(o.userIP)==0){
        return (timestamp.compareTo(o.timestamp));
    }else return (userIP.compareTo(o.userIP);
  }
```

Hadoop 使用`HashPartitioner`作为默认的分割器实现来计算中间数据到减少器的分布。 `HashPartitioner`要求 Key 对象的`hashCode()`方法满足以下两个属性：

*   跨不同的 JVM 实例提供相同的哈希值
*   提供哈希值的统一分布

因此，您必须为满足上述两个要求的自定义 Hadoop`key`类型实现稳定的`hashCode()`方法。 在`LogWritable`实现中，我们使用请求主机名/IP 地址的散列码作为`LogWritable`实例的散列码。 这确保中间`LogWritable`数据将根据请求的主机名/IP 地址进行分区。

```scala
  public int hashCode()
  {
    return userIP.hashCode();
  }
```

## 另请参阅

实现自定义 Hadoop 可写数据类型配方的*。*

# 从映射器发出不同值类型的数据

从映射器发出属于多个值类型的数据产品在执行 Reducer 端联接时非常有用，而且当我们需要避免使用多个 MapReduce 计算来汇总数据集中不同类型的属性时，也是非常有用的。 但是，Hadoop 减少器不允许多个输入值类型。 在这些场景中，我们可以使用`GenericWritable`类包装属于不同数据类型的多个`value`实例。

在这个配方中，我们重用 HTTP 服务器日志条目，分析实现自定义 Hadoop Writable 数据类型配方的*示例。 然而，在当前的配方中，我们没有使用自定义数据类型，而是从映射器输出多个值类型。 此示例汇总了从 Web 服务器向特定主机提供服务的总字节数，并输出特定主机请求的以制表符分隔的 URL 列表。 我们使用`IntWritable`从映射器输出字节数，使用`Text`输出请求 URL。*

## 怎么做……

以下步骤显示如何实现可以包装`IntWritable`或`Text`数据类型实例的 Hadoop`GenericWritable`数据类型：

1.  编写一个扩展`org.apache.hadoop.io.GenericWritable`类的类。 实现`getTypes()`方法以返回要使用的`Writable`类的数组。 如果要添加自定义构造函数，请确保还添加了无参数的默认构造函数。

    ```scala
    public class MultiValueWritable extends GenericWritable {

      private static Class[] CLASSES =  new Class[]{
        IntWritable.class,
        Text.class
      };

      public MultiValueWritable(){
      }

      public MultiValueWritable(Writable value){
        set(value);
      }

      protected Class[] getTypes() {
        return CLASSES;
      }
    }
    ```

2.  将`MultiValueWritable`设置为映射器的输出值类型。 用`MultiValueWritable`类的实例包装映射器的输出`Writable`值。

    ```scala
    public class LogProcessorMap extends
        Mapper<Object, Text, Text, MultiValueWritable> {
      private Text userHostText = new Text();
      private Text requestText = new Text();
      private IntWritable responseSize = new IntWritable();

      public void map(Object key, Text value,
                                  Context context)…{
        ……// parse the value (log entry) using a regex.
        userHostText.set(userHost);
        requestText.set(request);
        bytesWritable.set(responseSize);

        context.write(userHostText,
        new MultiValueWritable(requestText));
        context.write(userHostText,
        new MultiValueWritable(responseSize));
      }
    }
    ```

3.  将减速器输入值类型设置为`MultiValueWritable`。 实现`reduce()`方法以处理多个值类型。

    ```scala
    public class LogProcessorReduce extends
      Reducer<Text,MultiValueWritable,Text,Text> {
      private Text result = new Text();

      public void reduce(Text key, Iterable<MultiValueWritable>values, Context context)…{
      int sum = 0;
      StringBuilder requests = new StringBuilder();
      for (MultiValueWritable multiValueWritable : values) {
        Writable writable = multiValueWritable.get();
        if (writable instanceof IntWritable){
          sum += ((IntWritable)writable).get();
        }else{
          requests.append(((Text)writable).toString());
          requests.append("\t");
        }
      }
      result.set(sum + "\t"+requests);
      context.write(key, result);
      }
    }
    ```

4.  将`MultiValueWritable`设置为此计算的映射输出值类别：

    ```scala
        Job job = …
        job.setMapOutputValueClass(MultiValueWritable.class);
    ```

## 它是如何工作的.

`GenericWritable`实现应该扩展`org.apache.hadoop.io.GenericWritable`，并且应该通过从`getTypes()`方法返回一个`CLASSES`数组来指定一组要包装的`Writable`值类型。 `GenericWritable`实现使用指向该类数组的索引序列化和反序列化数据。

```scala
  private static Class[] CLASSES =  new Class[]{
    IntWritable.class,
    Text.class
  };

  protected Class[] getTypes() {
    return CLASSES;
  }
```

在映射器中，使用`GenericWritable`实现的实例包装每个值：

```scala
private Text requestText = new Text();
context.write(userHostText,new MultiValueWritable(requestText));
```

Reducer 实现必须手动处理不同的值类型。

```scala
if (writable instanceof IntWritable){
  sum += ((IntWritable)writable).get();
}else{
  requests.append(((Text)writable).toString());
  requests.append("\t");
}
```

## 还有更多...

`org.apache.hadoop.io.ObjectWritable`是另一个可用于实现与`GenericWritable`相同目标的类。 `ObjectWritable`类可以处理 Java 基元类型、字符串和数组，而不需要`Writable`包装器。 然而，Hadoop 通过在每个序列化条目中写入实例的类名来序列化`ObjectWritable`实例，这与基于`GenericWritable`类的实现相比效率低下。

## 另请参阅

实现自定义 Hadoop 可写数据类型配方的*。*

# 为您的输入数据格式选择合适的 Hadoop InputFormat

Hadoop 支持通过 InputFormat 处理许多不同格式和类型的数据。 Hadoop MapReduce 计算的 InputFormat 通过解析输入数据为映射器生成键-值对输入。 InputFormat 还执行将输入数据拆分成逻辑分区，实质上确定 MapReduce 计算的 Map 任务的数量，并间接决定 Map 任务的执行位置。 Hadoop 为每个逻辑数据分区生成一个 Map 任务，并使用逻辑拆分的键-值对作为输入调用相应的映射器。

## 怎么做……

以下步骤说明如何使用基于`FileInputFormat`的`KeyValueTextInputFormat`作为 Hadoop MapReduce 计算的 InputFormat：

1.  在本例中，我们将使用`Job`对象将 Hadoop MapReduce 计算的`KeyValueTextInputFormat`指定为 InputFormat，如下所示：

    ```scala
    Configuration conf = new Configuration();
    Job job = new Job(conf, "log-analysis");
    ……
    job.SetInputFormatClass(KeyValueTextInputFormat.class)
    ```

2.  设置作业的输入路径：

    ```scala
    FileInputFormat.setInputPaths(job, new Path(inputPath));
    ```

## 它是如何工作的.

`KeyValueTextInputFormat`是纯文本文件的一种输入格式，它为输入文本文件的每一行生成一个 key-value 记录。 使用分隔符将输入数据的每一行分成键(文本)和值(文本)对。 默认分隔符是制表符。 如果一行不包含分隔符，则整行将被视为键，值将为空。 我们可以通过设置作业的 Configuration 对象中的属性来指定自定义分隔符，如下所示，其中我们使用逗号字符作为键和值之间的分隔符。

```scala
conf.set("key.value.separator.in.input.line", ",");
```

`KeyValueTextInputFormat`基于`FileInputFormat`，它是基于文件的 InputFormats 的基类。 因此，我们使用`FileInputFormat`类的`setInputPaths()`方法指定 MapReduce 计算的输入路径。 当使用任何基于`FileInputFormat`类的 InputFormat 时，我们必须执行此步骤。

```scala
FileInputFormat.setInputPaths(job, new Path(inputPath));
```

通过提供逗号分隔的路径列表，我们可以为 MapReduce 计算提供多个 HDFS 输入路径。 您还可以使用`FileInputFormat`类的`addInputPath()`静态方法向计算中添加额外的输入路径。

```scala
public static void setInputPaths(JobConf conf,Path... inputPaths)
public static void addInputPath(JobConf conf, Path path)
```

## 还有更多...

确保 Mapper 输入数据类型与 MapReduce 计算使用的 InputFormat 生成的数据类型匹配。

以下是 Hadoop 为支持几种常见数据格式而提供的一些 InputFormat 实现：

*   `TextInputFormat`：此选项用于纯文本文件。 `TextInputFormat`为输入文本文件的每一行生成键值记录。 对于每一行，键(`LongWritable`)是文件中该行的字节偏移量，值(`Text`)是文本行。 `TextInputFormat`是 Hadoop 的默认 InputFormat。
*   `NLineInputFormat`：此选项用于纯文本文件。 `NLineInputFormat`将输入文件拆分为固定行数的逻辑拆分。 当我们希望 Map 任务接收固定数量的行作为输入时，可以使用`NLineInputFormat`。 类似于`TextInputFormat`类，为拆分中的每一行生成键(`LongWritable`)和值(`Text`)记录。 默认情况下，`NLineInputFormat`每行创建一个逻辑拆分(和一个映射任务)。 可以按如下方式指定每个拆分的行数(或每个映射任务的键值记录)。 `NLineInputFormat`为输入文本文件的每一行生成键值记录。

    ```scala
    NLineInputFormat.setNumLinesPerSplit(job,50);
    ```

*   `SequenceFileInputFormat`：用于 Hadoop SequenceFile 输入数据。 Hadoop SequenceFiles 将数据存储为二进制键-值对，并支持数据压缩。 当使用 SequenceFile 格式的上一次 MapReduce 计算结果作为 MapReduce 计算的输入时，`SequenceFileInputFormat`非常有用。 下面是它的子类：
    *   `SequenceFileAsBinaryInputFormat`：这是`SequenceInputFormat`类的子类，它以原始二进制格式表示键(`BytesWritable`)和值(`BytesWritable`)对。
    *   `SequenceFileAsTextInputFormat`：这是`SequenceInputFormat`类的子类，它将键(`Text`)和值(`Text`)对表示为字符串。
*   `DBInputFormat`：这支持从 SQL 表中读取 MapReduce 计算的输入数据。 `DBInputFormat`使用记录号作为关键字(`LongWritable`)，使用查询结果记录作为值(`DBWritable`)。

## 另请参阅

*添加了对新输入数据格式的支持-实现自定义 InputFormat*配方

# 添加对新输入数据格式的支持-实现自定义 InputFormat

Hadoop 使我们能够实现和指定 MapReduce 计算的自定义 InputFormat 实现。 我们可以实现自定义的 InputFormat 实现，以获得对输入数据的更多控制，并支持专有或特定于应用的输入数据文件格式作为 Hadoop MapReduce 计算的输入。 InputFormat 实现应该扩展`org.apache.hadoop.mapreduce.InputFormat<K,V>`抽象类，覆盖`createRecordReader()`和`getSplits()`方法。

在本食谱中，我们为 HTTP 日志文件实现一个 InputFormat 和一个 RecordReader。 此 InputFormat 将生成`LongWritable`个实例作为键，生成`LogWritable`个实例作为值。

## 怎么做……

以下是基于`FileInputFormat`类为 HTTP 服务器日志文件实现自定义 InputFormat 的步骤：

1.  `LogFileInputFormat`对 HDFS 文件中的数据进行操作。 因此，我们实现了扩展`FileInputFormat`类的`LogFileInputFormat`子类：

    ```scala
    public class LogFileInputFormat extends FileInputFormat<LongWritable, LogWritable>{

      public RecordReader<LongWritable, LogWritable>createRecordReader(InputSplit arg0,TaskAttemptContext arg1) throws …… {
        return new LogFileRecordReader();
      }

    }
    ```

2.  实现`LogFileRecordReader`类：

    ```scala
    public class LogFileRecordReader extends RecordReader<LongWritable, LogWritable>{

      LineRecordReader lineReader;
      LogWritable value;

      public void initialize(InputSplit inputSplit, TaskAttemptContext attempt)…{
        lineReader = new LineRecordReader();
        lineReader.initialize(inputSplit, attempt);
      }

      public boolean nextKeyValue() throws IOException, ..{
        if (!lineReader.nextKeyValue()){
          return false;
      }

        String line  =lineReader.getCurrentValue().toString();
        ……………//Extract the fields from 'line' using a regex

        value = new LogWritable(userIP, timestamp, request,
            status, bytes);
        return true;
      }

      public LongWritable getCurrentKey() throws..{
        return lineReader.getCurrentKey();
      }

      public LogWritable getCurrentValue() throws ..{
        return value;
      }

      public float getProgress() throws IOException ..{
        return lineReader.getProgress();
      }

      public void close() throws IOException {
        lineReader.close();
      }
    }
    ```

3.  使用`Job`对象将`LogFileInputFormat`指定为 MapReduce 计算的 InputFormat，如下所示。 使用底层的`FileInputFormat`指定计算的输入路径。

    ```scala
    Job job = ……
    ……
    job.setInputFormatClass(LogFileInputFormat.class);
    FileInputFormat.setInputPaths(job, new Path(inputPath));
    ```

4.  确保计算的映射器使用`LongWritable`作为输入`key`类型，使用`LogWritable`作为输入`value`类型：

    ```scala
    public class LogProcessorMap extendsMapper<LongWritable, LogWritable, Text, IntWritable>{
        public void map(LongWritable key, LogWritable value, Context context) throws ……{
        ………}
    }
    ```

## 它是如何工作的.

`LogFileInputFormat`扩展了`FileInputFormat`，它为基于 HDFS 文件的 InputFormat 提供了通用拆分机制。 我们覆盖`LogFileInputFormat`中的`createRecordReader()`方法，以提供自定义`RecordReader`实现`LogFileRecordReader`的一个实例。 或者，我们也可以覆盖`FileInputFormat`类的`isSplitable()`方法来控制输入文件是拆分到逻辑分区还是用作整个文件。

```scala
Public RecordReader<LongWritable, LogWritable>createRecordReader(InputSplit arg0,TaskAttemptContext arg1) throws …… {
    return new LogFileRecordReader();
}
```

`LogFileRecordReader`类扩展了`org.apache.hadoop.mapreduce.RecordReader<K,V>`抽象类，并在内部使用`LineRecordReader`来执行输入数据的基本解析。 `LineRecordReader`从输入数据读取文本行：

```scala
    lineReader = new LineRecordReader();
    lineReader.initialize(inputSplit, attempt);
```

我们在`nextKeyValue()`方法中对输入数据的日志条目执行自定义解析。 我们使用正则表达式从 HTTP 服务日志条目中提取字段，并使用这些字段填充`LogWritable`类的一个实例。

```scala
  public boolean nextKeyValue() throws IOException, ..{
    if (!lineReader.nextKeyValue())
      return false;

    String line = lineReader.getCurrentValue().toString();
    ……………//Extract the fields from 'line' using a regex

    value = new LogWritable(userIP, timestamp, request, status, bytes);
    return true;
  }
```

## 还有更多...

我们可以通过覆盖 InputFormat 类的`getSplits()`方法来执行输入数据的自定义拆分。 `getSplits()`方法应该返回`InputSplit`对象的列表。 `InputSplit`对象表示输入数据的逻辑分区，并将被分配给单个映射任务。 `InputSplit`类扩展了`InputSplit`抽象类，并且应该覆盖`getLocations()`和`getLength()`方法。 `getLength()`方法应提供拆分的长度，`getLocations()`方法应提供物理存储此拆分表示的数据的节点列表。 Hadoop 使用数据本地节点列表进行 Map 任务调度。 我们在前面示例中使用的`FileInputFormat`类使用`org.apache.hadoop.mapreduce.lib.input.FileSplit`作为`InputSplit`实现。

您也可以为非 HDFS 数据编写 InputFormat 实现。 `org.apache.hadoop.mapreduce.lib.db.DBInputFormat`是`InputFormat.DBInputFormat`支持从 SQL 表读取输入数据的一个示例。

## 另请参阅

*为您的输入数据格式选择合适的 Hadoop InputFormat*配方。

# 格式化 MapReduce 计算结果-使用 Hadoop OutputFormats

通常，MapReduce 计算的输出将被其他应用使用。 因此，以目标应用可以高效使用的格式存储 MapReduce 计算的结果非常重要。 在目标应用可以高效访问的位置存储和组织数据也很重要。 我们可以使用 Hadoop OutputFormat 接口来定义 MapReduce 计算的数据存储格式、数据存储位置和输出数据的组织。 OutputFormat 准备输出位置并提供`RecordWriter`实现来执行实际的数据序列化和存储。

Hadoop 使用`org.apache.hadoop.mapreduce.lib.output.TextOutputFormat<K,V>`抽象类作为 MapReduce 计算的默认 OutputFormat。 `TextOutputFormat`将输出数据的记录写入 HDFS 中的纯文本文件，对每个记录使用单独的行。 `TextOutputFormat`使用制表符分隔记录的键和值。 `TextOutputFormat`扩展了`FileOutputFormat`，它是所有基于文件的输出格式的基类。

## 怎么做……

以下步骤向您展示了如何使用基于`FileOutputFormat`的`SequenceFileOutputFormat`作为 Hadoop MapReduce 计算的 OutputFormat。

1.  在本例中，我们将使用`Job`对象将`org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat<K,V>`指定为 Hadoop MapReduce 计算的 OutputFormat，如下所示：

    ```scala
    Job job = ……
    ……
    job.setOutputFormatClass(SequenceFileOutputFormat.class)
    ```

2.  设置作业的输出路径：

    ```scala
    FileOutputFormat.setOutputPath(job, new Path(outputPath));
    ```

## 它是如何工作的.

`SequenceFileOutputFormat`将数据序列化到 Hadoop SequenceFiles。 Hadoop SequenceFiles 将数据存储为二进制键-值对，并支持数据压缩。 SequenceFiles 特别适用于存储非文本数据。 如果 MapReduce 计算的输出将作为另一个 Hadoop MapReduce 计算的输入，我们可以使用 SequenceFiles 来存储 MapReduce 计算的结果。

`SequenceFileOutputFormat`基于`FileOutputFormat`，它是基于文件的`OutputFormat`的基类。 因此，我们使用`FileOutputFormat`的`setOutputPath()`方法指定 MapReduce 计算的输出路径。 当使用任何基于`FileOutputFormat`的 OutputFormat 时，我们必须执行此步骤。

```scala
FileOutputFormat.setOutputPath(job, new Path(outputPath));
```

## 还有更多...

您可以实现自定义 OutputFormat 类，以专有或自定义数据格式写入 MapReduce 计算的输出，和/或通过扩展`org.apache.hadoop.mapreduce.OutputFormat<K,V>`抽象类将结果存储在 HDFS 以外的存储中。 如果您的 OutputFormat 实现将数据存储在文件系统中，您可以从`FileOutputFormat`类扩展以使您的工作更轻松。

# 从 MapReduce 计算写入多个输出

我们可以使用 Hadoop 的`MultipleOutputs`特性来发出 MapReduce 计算的多个输出。 当我们想要将不同的输出写入不同的文件时，以及除了作业的主输出之外还需要输出其他输出时，此功能非常有用。 `MultipleOutputs`功能还允许我们为每个输出指定不同的 OutputFormat。

## 怎么做……

以下步骤显示如何使用`MultipleOutputs`功能从 Hadoop MapReduce 计算输出两个不同的数据集：

1.  使用 Hadoop 驱动程序配置并命名多个输出：

    ```scala
    Job job = Job.getInstance(getConf(), "log-analysis");
    …
    FileOutputFormat.setOutputPath(job, new Path(outputPath));
    MultipleOutputs.addNamedOutput(job, "responsesizes", TextOutputFormat.class,Text.class, IntWritable.class);
    MultipleOutputs.addNamedOutput(job, "timestamps", TextOutputFormat.class,Text.class, Text.class);
    ```

2.  将数据写入来自`reduce`函数的不同命名输出：

    ```scala
    public class LogProcessorReduce …{
      private MultipleOutputs mos;

      protected void setup(Context context) .. {
        mos = new MultipleOutputs(context);
      }

      public void reduce(Text key, … {
        …
        mos.write("timestamps", key, val.getTimestamp());
        …
        mos.write("responsesizes", key, result);
      }
    }
    ```

3.  通过将以下内容添加到 Reduce 任务的`cleanup`函数来关闭所有打开的输出：

    ```scala
    @Override
      public void cleanup(Context context) throws IOException,
        InterruptedException {
          mos.close();
      }
    ```

4.  对于写入的每种输出类型，输出文件名将采用`namedoutput-r-xxxxx`格式。 对于当前示例，示例输出文件名为`responsesizes-r-00000`和`timestamps-r-00000`。

## 它是如何工作的.

我们首先使用`MultipleOutputs`类的以下静态方法将命名输出添加到驱动程序中的作业：

```scala
public static addNamedOutput(Job job, String namedOutput, Class<? extends OutputFormat> outputFormatClass, Class<?> keyClass, Class<?> valueClass)
```

然后，我们初始化 Reduce 任务的`setup`方法中的`MultipleOutputs`特性，如下所示：

```scala
protected void setup(Context context) .. {
  mos = new MultipleOutputs(context);
  }
```

我们可以使用在驱动程序中定义的名称，使用`MultipleOutputs`类的以下方法编写不同的输出：

```scala
public <K,V> void write (String namedOutput, K key, V value)
```

您可以使用`MultipleOutputs`的以下方法直接写入输出路径，而无需定义命名输出。 此输出将使用为作业定义的 OutputFormat 格式化输出。

```scala
public void write(KEYOUT key, VALUEOUT value,
  String baseOutputPath)
```

最后，我们确保使用`MultipleOutputs`类的 Close 方法关闭 Reduce 任务的`cleanup`方法的所有输出。 应该这样做，以避免丢失写入不同输出的任何数据。

```scala
public void close()
```

### 在单个 MapReduce 应用中使用多种输入数据类型和多种 Mapper 实现

我们可以使用 Hadoop 的`MultipleInputs`特性来运行具有多个输入路径的 MapReduce 作业，同时为每个路径指定不同的 InputFormat 和(可选)Mapper。 Hadoop 会将不同映射器的输出路由到 MapReduce 计算的 Single Reducer 实现的实例。 当我们想要处理具有相同含义但不同 InputFormats(逗号分隔的数据集和制表符分隔的数据集)的多个数据集时，具有不同 InputFormats 的多个输入非常有用。

我们可以使用`MutlipleInputs`类的以下`addInputPath`静态方法将输入路径和各自的 InputFormats 添加到 MapReduce 计算中：

```scala
Public static void addInputPath(Job job, Path path, Class<?extendsInputFormat>inputFormatClass)
```

以下是上述方法的用法示例：

```scala
MultipleInputs.addInputPath(job, path1, CSVInputFormat.class);
MultipleInputs.addInputPath(job, path1, TabInputFormat.class);
```

多输入功能能够指定不同的映射器实现，`InputFormats`在执行两个或多个数据集的减边联接时非常有用：

```scala
public static void addInputPath(JobConfconf,Path path,
     Class<?extendsInputFormat>inputFormatClass,
     Class<?extends Mapper>mapperClass)
```

下面的是通过不同的 InputFormats 和不同的 Mapper 实现使用多个输入的示例：

```scala
MultipleInputs.addInputPath(job, accessLogPath, TextInputFormat.class, AccessLogMapper.class);
MultipleInputs.addInputPath(job, userDataPath, TextInputFormat.class, UserDataMapper.class);
```

## 另请参阅

*增加了对新输入数据格式的支持-实现了自定义的 InputFormat*配方。

# Hadoop 中间数据分区

Hadoop MapReduce 跨计算的 Reduce 任务对*Map 任务生成的中间数据进行分区。 确保个 Reduce 任务负载均衡的适当分区函数对 MapReduce 计算的性能至关重要。 分区还可以用于将相关记录集组合在一起，以执行特定的 Reduce 任务，在这些任务中，您希望将某些输出处理或分组在一起。 本章*简介*部分中的图描述了分区在哪里适合整个 MapReduce 计算流程。*

Hadoop 根据中间数据的密钥空间对中间数据进行分区，并决定哪个 Reduce 任务将接收哪个中间记录。 分区的已排序键集及其值将是 Reduce 任务的输入。 在 Hadoop 中，分区总数应该等于 MapReduce 计算的 Reduce 任务数。 Hadoop 分割器应该扩展`org.apache.hadoop.mapreduce.Partitioner<KEY,VALUE>`抽象类。 Hadoop 使用`org.apache.hadoop.mapreduce.lib.partition.HashPartitioner`作为 MapReduce 计算的默认分区程序。 **HashPartiator**使用公式*key.hashcode()mod r*根据键的`hashcode()`对键进行分区，其中*r*是 Reduce 任务的数量。 下图说明了具有两个 Reduce 任务的计算的`HashPartitioner`：

![Hadoop intermediate data partitioning](img/5471OS_04_02.jpg)

可能会有这样的场景：我们的计算逻辑需要或可以使用应用的特定数据分区模式更好地实现。 在本食谱中，我们为 HTTP 日志处理应用实现了一个自定义分区程序，它根据键(IP 地址)的地理区域对键(IP 地址)进行分区。

## 怎么做……

以下步骤显示如何实现自定义分区程序，该程序根据请求 IP 地址或主机名的位置对中间数据进行分区：

1.  实现扩展`Partitioner`抽象类的`IPBasedPartitioner`类：

    ```scala
    public class IPBasedPartitioner extends Partitioner<Text, IntWritable>{

      public int getPartition(Text ipAddress,
        IntWritable value, int numPartitions) {
        String region = getGeoLocation(ipAddress);

        if (region!=null){
          return ((region.hashCode() & Integer.MAX_VALUE) % numPartitions);
        }
      return 0;
      }
    }
    ```

2.  设置`Job`对象中的`Partitioner`类参数：

    ```scala
    Job job = ……
    ……
    job.setPartitionerClass(IPBasedPartitioner.class);
    ```

## 它是如何工作的.

在前面的示例中，我们对中间数据执行分区，以便将来自相同地理区域的请求发送到相同的 Reducer 实例。 方法的作用是：返回给定 IP 地址的地理位置。 我们省略了`getGeoLocation()`方法的实现细节，因为它对于理解此示例不是必需的。 然后，我们获得地理位置的`hashCode()`方法，并执行模运算来选择请求的 Reducer 存储桶。

## 还有更多...

`TotalOrderPartitioner`和`KeyFieldPartitioner`是 Hadoop 提供的几个内置分区程序实现中的两个。

### Колибрипрограммирования

`TotalOrderPartitioner`扩展`org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner<K,V>`。 到还原器的输入记录集合按排序顺序，确保输入分区内的正确排序。 然而，Hadoop 默认分区策略(`HashPartitioner`)在对中间数据进行分区时不强制排序，并将键分散在分区之间。 在我们希望确保全局订单的用例中，我们可以使用`TotalOrderPartitioner`强制执行总订单，以减少 Reducer 任务中的输入记录。 `TotalOrderPartitioner`需要分区文件作为定义分区范围的输入。 `org.apache.hadoop.mapreduce.lib.partition.InputSampler`实用程序允许我们通过采样输入数据为`TotalOrderPartitioner`生成分区文件。 `TotalOrderPartitioner`用于 Hadoop TeraSort 基准测试。

```scala
Job job = ……
……
job.setPartitionerClass(TotalOrderPartitioner.class);
TotalOrderPartitioner.setPartitionFile(jobConf,partitionFile);
```

### KeyFieldBasedPartiator

`org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner<K,V>`抽象类可用于根据键的部分划分中间数据。 可以使用分隔符字符串将关键字拆分为一组字段。 我们可以指定分区时要考虑的字段集的索引。 我们还可以指定字段中字符的索引。

# 二次排序-排序减少输入值

MapReduce 框架根据键-值对中的键对 Reduce 输入数据进行排序，还根据键对数据进行分组。 Hadoop 以键的排序顺序为每个唯一键调用`reduce`函数，并将属于该键的值列表作为第二个参数。 但是，每个键的值列表不会按任何特定顺序排序。 在许多情况下，根据某些条件对每个键的 Reduce 输入值列表进行排序也会很有用。 这些示例包括在不迭代整个列表的情况下查找给定键的最大值或最小值、优化缩减端连接、识别重复的数据产品，等等。

我们可以使用 Hadoop 框架通过一种称为辅助排序的机制对约简输入值进行排序。 我们通过强制 Hadoop 框架使用键以及使用值中的几个指定字段对 Reduce 输入键-值对进行排序来实现这一点。 但是，Map 输出数据的分区和 Reduce 输入数据的分组仍然仅使用键执行。 这确保了 Reduce 输入数据按键分组和排序，而属于某个键的值列表也将按排序顺序排列。

## 怎么做……

以下步骤说明如何在 Hadoop MapReduce 计算中对 Reduce 输入值执行二次排序：

1.  首先，实现一个`WritableComparable`数据类型，该类型可以包含需要包含在排序顺序中的值中的实际键(`visitorAddress`)和字段(`responseSize`)。 此新复合类型的比较器应强制排序顺序，其中首先是实际键，然后是从此新类型中包含的值字段派生的排序条件。 我们使用此复合类型作为 Map Output 键类型。 或者，您也可以使用现有的`WritableComparable`类型作为 Map Output Key 类型，它包含实际键和值中的其他字段，方法是为该数据类型提供类似的实现以强制排序。

    ```scala
    public class SecondarySortWritable … {
      private String visitorAddress;
      private int responseSize;
      ………
      @Override
      public boolean equals(Object right) {
        if (right instanceof SecondarySortWritable) {
          SecondarySortWritable r = (SecondarySortWritable) right;
          return (r.visitorAddress.equals(visitorAddress) && (r.responseSize == responseSize));
        } else {
          return false;
        }
      }

      @Override
      public int compareTo(SecondarySortWritable o) {
        if (visitorAddress.equals(o.visitorAddress)) {
          return responseSize < o.responseSize ? -1 : 1;
        } else {
          return visitorAddress.compareTo(o.visitorAddress);
        }
      }
    }
    ```

2.  修改`map`和`reduce`函数以使用我们创建的复合键：

    ```scala
    public class LogProcessorMap extends Mapper<Object, LogWritable, SecondarySortWritable, LogWritable > {
      private SecondarySortWritable outKey ..

      public void map(Object key, …..{
        outKey.set(value.getUserIP().toString(), value.getResponseSize().get());
        context.write(outKey,value);
      }
    }

    public class LogProcessorReduce extends
      Reducer<SecondarySortWritable, LogWritable..{
      ……
      public void reduce(SecondarySortWritable key, Iterable<LogWritable> values {
        ……
      }
    }
    ```

3.  实现自定义分区程序，以便仅根据复合键

    ```scala
    public class SingleFieldPartitioner extends… {
      public int getPartition(SecondarySortWritable key, Writable value, int numPartitions) {
        return (int)(key.getVisitorAddress().hashCode() % numPartitions);
      }
    }
    ```

    中包含的实际键(`visitorAddress`)对映射输出数据进行分区
4.  实现自定义分组比较器，以便仅根据实际键(`visitorAddress`)对 Reduce 输入进行分组：

    ```scala
    public class GroupingComparator extends WritableComparator {
      public GroupingComparator() {
        super(SecondarySortWritable.class, true);
      }

      @Override
      public int compare(WritableComparable o1, WritableComparable o2) {
        SecondarySortWritable firstKey = (SecondarySortWritable) o1;
        SecondarySortWritable secondKey = (SecondarySortWritable) o2;
        return (firstKey.getVisitorAddress()).compareTo(secondKey.getVisitorAddress());
      }
    ```

5.  在驱动程序中配置分区程序`GroupingComparator`和映射输出键类型：

    ```scala
    Job job = Job.getInstance(getConf(), "log-analysis");
    ……
    job.setMapOutputKeyClass(SecondarySortWritable.class);
    ……

    // group and partition by the visitor address
    job.setPartitionerClass(SingleFieldPartitioner.class);
    job.setGroupingComparatorClass(GroupingComparator.class);
    ```

## 它是如何工作的.

我们首先实现了一个自定义的`WritableComparable`键类型，该类型将保存值的实际键和排序字段。 我们确保这个新复合键类型的排序顺序为实际键，后跟来自值的排序字段。 这将确保 Reduce 输入数据首先根据实际键排序，然后是值的给定字段。

然后，我们实现了一个自定义分区程序，它只根据来自新复合键的实际键字段对 Map 输出数据进行分区。 此步骤确保具有相同实际密钥的每个键-值对将由相同的 Reducer 处理。 最后，我们实现了一个分组比较器，在对减少的输入键-值对进行分组时，它将只考虑新键的实际键字段。 这确保每个`reduce`函数输入将是新的复合键以及属于实际键的值列表。 值列表将按照复合键的比较器中定义的排序顺序排列。

## 另请参阅

*增加了对新输入数据格式的支持-实现了自定义的 InputFormat*配方。

# 向 MapReduce 作业中的任务广播和分发共享资源-Hadoop DistributedCache

我们可以使用 Hadoop**DistributedCache**将基于文件的只读资源分发到 Map 和 Reduce 任务。 这些资源可以是映射器或缩减器执行计算所需的简单数据文件、存档或 JAR 文件。

## 怎么做……

以下步骤向您展示了如何将文件添加到 Hadoop DistributedCache，以及如何从 Map 和 Reduce 任务中检索该文件：

1.  将资源复制到 HDFS。 您还可以使用 HDFS 中已有的文件。

    ```scala
    $ hadoop fs –copyFromLocal ip2loc.dat ip2loc.dat

    ```

2.  将资源从您的驱动程序添加到 DistributedCache：

    ```scala
    Job job = Job.getInstance……
    ……
    job.addCacheFile(new URI("ip2loc.dat#ip2location"));
    ```

3.  检索 Mapper 或 Reducer 的`setup()`方法中的资源，并使用`map()`或`reduce()`函数中的数据：

    ```scala
    public class LogProcessorMap extends Mapper<Object, LogWritable, Text, IntWritable> {
      private IPLookup lookupTable;

      public void setup(Context context) throws IOException{

        File lookupDb = new File("ip2location");
        // Load the IP lookup table (a simple hashmap of ip
        // prefixes as keys and country names as values) to
        // memory
        lookupTable = IPLookup.LoadData(lookupDb);
      }

      public void map(…) {
         String country = lookupTable.getCountry(value.ipAddress);
         ……
      }
    }
    ```

## 它是如何工作的.

Hadoop 在执行作业的任何任务之前将添加到 DistributedCache 的文件复制到所有工作节点。 DistributedCache 在每个作业中只复制这些文件一次。 Hadoop 还支持通过将具有所需符号链接名称的片段添加到 URI 来创建指向计算工作目录中的 DistributedCache 文件的符号链接。 在以下示例中，我们使用`ip2location`作为指向 DistributedCache 中的`ip2loc.dat`文件的符号链接：

```scala
job.addCacheArchive(new URI("/data/ip2loc.dat#ip2location"));
```

我们在 Mapper 或 Reducer 的`setup()`方法中解析并加载 DistributedCache 中的数据。 可以使用提供的符号链接的名称从工作目录访问带有符号链接的文件。

```scala
private IPLookup lookup;
public void setup(Context context) throws IOException{

  File lookupDb = new File("ip2location");
  // Load the IP lookup table to memory
  lookup = IPLookup.LoadData(lookupDb);
}

public void map(…) {
  String location =lookup.getGeoLocation(value.ipAddress);
  ……
}
```

我们还可以使用`getLocalCacheFiles()`方法直接访问 DistributedCache 中的数据，而不使用符号链接：

```scala
URI[] cacheFiles = context.getCacheArchives();
```

### 备注

DistributedCache 在 Hadoop 本地模式下不工作。

## 还有更多...

以下节介绍如何使用 DistributedCache 分发压缩存档，如何使用命令行将资源添加到 DistributedCache，以及如何使用 DistributedCache 将资源添加到映射器和 Reducer 的类路径。

### 使用 DistributedCache 分发档案

我们也可以使用 DistributedCache 分发档案。 Hadoop 提取工作节点中的存档。 您还可以使用 URI 片段提供指向存档的符号链接。 在下一个示例中，我们使用`ip2locationdb`符号链接作为`ip2locationdb.tar.gz`存档。

考虑以下 MapReduce 驱动程序：

```scala
Job job = ……
job.addCacheArchive(new URI("/data/ip2locationdb.tar.gz#ip2locationdb"));
```

可以使用前面提供的符号链接从 Mapper 或 Reducer 的工作目录访问提取的归档目录：

请考虑以下映射器程序：

```scala
  public void setup(Context context) throws IOException{
    Configuration conf = context.getConfiguration();

    File lookupDbDir = new File("ip2locationdb");
    String[] children = lookupDbDir.list();

    …
  }
```

您还可以在 Mapper 或 Reducer 实现中使用以下方法直接访问未解压的 DistributedCache 存档文件：

```scala
URI[] cachePath;

public void setup(Context context) throws IOException{
  Configuration conf = context.getConfiguration();
  cachePath = context.getCacheArchives();
    …
}
```

### 从命令行向 DistributedCache 添加资源

Hadoop 支持使用命令行将文件或档案添加到 DistributedCache，前提是您的 MapReduce 驱动程序实现了`org.apache.hadoop.util.Tool`接口或利用了`org.apache.hadoop.util.GenericOptionsParser`。 文件可以使用`–files`命令行选项添加到 DistributedCache，而归档文件可以使用`–archives`命令行选项添加。 文件或存档可以位于 Hadoop 可访问的任何文件系统中，包括您的本地文件系统。

这些选项支持逗号分隔的路径列表和使用 URI 片段创建符号链接。

```scala
$ hadoop jar C4LogProcessor.jar LogProcessor-files ip2location.dat#ip2location  indir outdir
$ hadoop jar C4LogProcessor.jar LogProcessor-archives ip2locationdb.tar.gz#ip2locationdb indir outdir

```

### 使用 DistributedCache 将资源添加到类路径

您可以使用 DistributedCache 将 JAR 文件和其他依赖库分发到 Mapper 或 Reducer。 您可以在驱动程序中使用以下方法将 JAR 文件添加到运行 Mapper 或 Reducer 的 JVM 的类路径中：

```scala
public static void addFileToClassPath(Path file,Configuration conf,FileSystem fs)

public static void addArchiveToClassPath(Path archive,Configuration conf, FileSystem fs)
```

与我们在*从命令行*小节向 DistributedCache 添加资源中描述的`–files`和`–archives`命令行选项类似，我们还可以使用`–libjars`命令行选项将 JAR 文件添加到 MapReduce 计算的类路径中。 为了使`–libjars`命令行选项工作，MapReduce 驱动程序应该实现`Tool`接口或者应该利用`GenericOptionsParser`。

```scala
$ hadoop jar C4LogProcessor.jar LogProcessor-libjars ip2LocationResolver.jar  indir outdir

```

# 将 Hadoop 与旧式应用配合使用-Hadoop 流

Hadoop 流允许我们使用任何可执行文件或脚本作为 Hadoop MapReduce 作业的 Mapper 或 Reducer。 Hadoop Streaming 使我们能够使用 Linux shell 实用程序或脚本语言执行 MapReduce 计算的快速原型。 Hadoop 流还允许具有一定 Java 知识或没有 Java 知识的用户利用 Hadoop 处理存储在 HDFS 中的数据。

在本食谱中，我们使用 Python 为我们的 HTTP 日志处理应用实现了一个映射器，并使用了一个基于 Hadoop 聚集包的 Reducer。

## 怎么做……

以下是使用 Python 程序作为映射器处理 HTTP 服务器日志文件的步骤：

1.  编写`logProcessor.py`Python 脚本：

    ```scala
    #!/usr/bin/python
    import sys
    import re
    def main(argv):
      regex =re.compile('……')
      line = sys.stdin.readline()
      try:
        while line:
          fields = regex.match(line)
          if(fields!=None):
            print"LongValueSum:"+fields.group(1)+
              "\t"+fields.group(7)
          line =sys.stdin.readline()
      except"end of file":
        return None
    if __name__ == "__main__":
      main(sys.argv)
    ```

2.  从 Hadoop 安装目录使用以下命令执行 MapReduce 流计算：

    ```scala
    $ hadoop jar \
     $HADOOP_MAPREDUCE_HOME/hadoop-streaming-*.jar \
     -input indir \
     -output outdir \
     -mapper logProcessor.py \
     -reducer aggregate \
     -file logProcessor.py

    ```

## 它是如何工作的.

每个 Map 任务在 Worker 节点中将 Hadoop Streaming 可执行文件作为单独的进程启动。 映射器的输入记录(日志文件的条目或行，未拆分为键-值对)以行的形式提供给该进程的标准输入。 可执行文件应该读取并处理标准输入中的记录，直到到达文件末尾。

```scala
line = sys.stdin.readline()
try:
    while line:
      ………
      line =sys.stdin.readline()
except "end of file":
    return None
```

Hadoop Streaming 从进程的标准输出中收集可执行文件的输出。 Hadoop Streaming 将标准输出的每一行转换为键-值对，其中直到第一个制表符的文本被视为键，而该行的其余部分被视为值。 根据此约定，`logProcessor.py`python 脚本输出键-值对，如下所示：

```scala
If (fields!=None):
      print "LongValueSum:"+fields.group(1)+ "\t"+fields.group(7);
```

在我们的示例中，我们使用 Hadoop`aggregate`包进行计算的约简部分。 Hadoop`aggregate`包为简单的聚合操作(如求和、最大值、唯一值计数和直方图)提供了 Reducer 和 Composer 实现。 当与 Hadoop 流一起使用时，映射器输出必须将当前计算的聚合操作类型指定为输出键的前缀，在我们的示例中是`LongValueSum`。

Hadoop 流还支持使用`–file`选项将文件分发到工作节点。 我们可以使用此选项来分发流计算所需的可执行文件、脚本或任何其他辅助文件。 我们可以为一次计算指定多个`–file`选项。

```scala
$ hadoop jar …… \
 -mapper logProcessor.py \
 -reducer aggregate \
 -file logProcessor.py

```

## 还有更多...

我们可以指定 Java 类作为 Hadoop 流计算的 Mapper 和/或 Reducer 和/或组合器程序。 我们还可以为 Hadoop 流计算指定 InputFormat 和其他选项。

Hadoop Streaming 还允许我们使用 Linux shell 实用程序作为 Mapper 和 Reducer。 下面的示例显示了如何使用`grep`作为 Hadoop 流计算的映射器。

```scala
$ hadoop jar 
 $HADOOP_MAPREDUCE_HOME/hadoop-streaming-*.jar \
 –input indir \
 -output outdir \ 
 -mapper 'grep "wiki"'

```

Hadoop 流逐行向执行可执行文件的进程的标准输入提供每个键组的 Reducer 输入记录。 然而，Hadoop 流没有一种机制来区分它何时开始向进程提供新密钥的记录。 因此，Reducer 程序的脚本或可执行文件应该跟踪最后看到的输入记录的键，以便在键组之间进行划分。

有关 Hadoop 流的详细文档，请参阅[http://hadoop.apache.org/mapreduce/docs/stable1/streaming.html](http://hadoop.apache.org/mapreduce/docs/stable1/streaming.html)。

## 另请参阅

使用 Hadoop 流的*数据预处理和使用 Hadoop 流的 Python*和*重复数据消除*在[第 10 章](10.html "Chapter 10. Mass Text Data Processing")，*海量文本数据处理*中介绍。

# 在 MapReduce 作业之间添加依赖关系

通常我们需要以类似于工作流的方式执行多个 MapReduce 应用来实现我们的目标。 Hadoop`ControlledJob`和`JobControl`类通过指定 MapReduce 作业之间的依赖关系，提供了一种执行 MapReduce 作业的简单工作流图的机制。

在本配方中，我们先对 HTTP 服务器日志数据集执行`log-grep`MapReduce 计算，然后执行`log-analysis`MapReduce 计算。 `log-grep`计算根据正则表达式过滤输入数据。 `log-analysis`计算分析过滤后的数据。 因此，`log-analysis`计算依赖于`log-grep`计算。 我们使用`ControlledJob`类来表示这种依赖关系，并使用`JobControl`类来执行两个相关的 MapReduce 计算。

## 怎么做……

以下步骤说明如何将 MapReduce 计算添加为另一个 MapReduce 计算的依赖项：

1.  为第一个 MapReduce 作业创建`Configuration`和`Job`对象，并使用其他所需配置填充它们：

    ```scala
    Job job1 = ……
    job1.setJarByClass(RegexMapper.class);
    job1.setMapperClass(RegexMapper.class);
    FileInputFormat.setInputPaths(job1, new Path(inputPath));
    FileOutputFormat.setOutputPath(job1, new Path(intermedPath));
    ……
    ```

2.  为第二个 MapReduce 作业创建`Configuration`和`Job`对象，并使用必要的配置填充它们：

    ```scala
    Job job2 = ……
    job2.setJarByClass(LogProcessorMap.class);
    job2.setMapperClass(LogProcessorMap.class);
    job2.setReducerClass(LogProcessorReduce.class);
    FileOutputFormat.setOutputPath(job2, new Path(outputPath));
    ………
    ```

3.  将第一个作业的输出目录设置为第二个作业的输入目录：

    ```scala
    FileInputFormat.setInputPaths(job2, new Path(intermedPath +"/part*"));
    ```

4.  使用先前创建的`Job`对象创建`ControlledJob`对象：

    ```scala
    ControlledJob controlledJob1 = new ControlledJob(job1.getConfiguration());
    ControlledJob controlledJob2 = new ControlledJob(job2.getConfiguration());
    ```

5.  将第一个作业作为依赖项添加到第二个作业：

    ```scala
    controlledJob2.addDependingJob(controlledJob1);
    ```

6.  为这组作业创建`JobControl`对象，并将在步骤 4 中创建的`ControlledJob`对象添加到新创建的`JobControl`对象：

    ```scala
    JobControl jobControl = new  JobControl("JobControlDemoGroup");
    jobControl.addJob(controlledJob1);
    jobControl.addJob(controlledJob2);
    ```

7.  创建一个新线程来运行添加到`JobControl`对象的作业组。 启动线程并等待其完成：

    ```scala
        Thread jobControlThread = new Thread(jobControl);
        jobControlThread.start();
        while (!jobControl.allFinished()){
          Thread.sleep(500);
        }
        jobControl.stop();
    ```

## 它是如何工作的.

`ControlledJob`类封装 MapReduce 作业并跟踪作业的依赖关系。 具有相关作业的`ControlledJob`类只有在其所有相关作业都成功完成时才会准备好提交。 如果任何相关作业失败，则`ControlledJob`类将失败。

`JobControl`类封装了一组`ControlledJobs`及其依赖项。 `JobControl`跟踪封装的`ControlledJobs`的状态，并包含提交处于*就绪*状态的作业的线程。

如果要使用 MapReduce 作业的输出作为从属作业的输入，则必须手动设置从属作业的输入路径。 默认情况下，Hadoop 为每个 Reduce 任务名称生成一个带有`part`前缀的输出文件夹。 我们可以使用通配符将所有`part`前缀的子目录指定为相关作业的输入。

```scala
FileInputFormat.setInputPaths(job2, new Path(job1OutPath +"/part*"));
```

## 还有更多...

我们还可以使用`JobControl`类来执行和跟踪一组非依赖任务。

Apache**Oozie**是一个用于 Hadoop MapReduce 计算的工作流系统。 您可以使用 Oozie 执行 MapReduce 计算的**有向无环图**(**DAG**)。 您可以从该项目的主页[http://oozie.apache.org/](http://oozie.apache.org/)找到有关 Oozie 的更多信息。

Hadoop MapReduce API 的旧版本中提供了`ChainMapper`类，它允许我们在管道中的单个 Map 任务计算中执行 Mapper 类的管道。 `ChainReducer`为 Reduce 任务提供了类似的支持。 出于向后兼容的原因，该 API 仍然存在于 Hadoop2 中。

# 用于报告自定义指标的 Hadoop 计数器

Hadoop 使用一组计数器来聚合 MapReduce 计算的指标。 Hadoop 计数器有助于了解 MapReduce 程序的行为，并跟踪 MapReduce 计算的进度。 我们可以定义自定义计数器来跟踪 MapReduce 计算中特定于应用的指标。

## 怎么做……

以下步骤向您展示了如何定义自定义计数器来计算日志处理应用中损坏或损坏的记录数：

1.  使用`enum`：

    ```scala
      public static enum LOG_PROCESSOR_COUNTER {
         BAD_RECORDS
        };
    ```

    定义自定义计数器列表
2.  在映射器或还原器中递增计数器：

    ```scala
    context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS).increment(1);
    ```

3.  将以下内容添加到您的驱动程序中以访问计数器：

    ```scala
    Job job = new Job(getConf(), "log-analysis");
    ……
    Counters counters = job.getCounters();
    Counter badRecordsCounter = counters.findCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS);
    System.out.println("# of Bad Records:"+ badRecordsCounter.getValue());
    ```

4.  执行 Hadoop MapReduce 计算。 您还可以在管理控制台或命令行中查看计数器值。

    ```scala
    $ hadoop jar C4LogProcessor.jar \
     demo.LogProcessor in out 1
    ………
    12/07/29 23:59:01 INFO mapred.JobClient: Job complete: job_201207271742_0020
    12/07/29 23:59:01 INFO mapred.JobClient: Counters: 30
    12/07/29 23:59:01 INFO mapred.JobClient:   demo.LogProcessorMap$LOG_PROCESSOR_COUNTER
    12/07/29 23:59:01 INFO mapred.JobClient:   BAD_RECORDS=1406
    12/07/29 23:59:01 INFO mapred.JobClient:   Job Counters
    ………
    12/07/29 23:59:01 INFO mapred.JobClient:     Map output records=112349
    # of Bad Records :1406

    ```

## 它是如何工作的.

您必须使用`enum`定义您的自定义计数器。 `enum`中的计数器集合将形成一组计数器。 ApplicationMaster 聚合映射器和减少器报告的计数器值。**