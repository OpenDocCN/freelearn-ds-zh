# 七、数据分析应用示例

在本章中，我们希望让您熟悉典型的数据准备任务和分析技术，因为熟练准备、分组和重塑数据是成功数据分析的重要组成部分。

虽然准备数据似乎是一项平凡的任务——通常也是如此——但这是我们不能跳过的一步，尽管我们可以通过使用 Pandas 等工具来努力简化它。

为什么准备是必要的？因为大多数有用的数据将来自现实世界，并且会有缺陷、包含错误或者是零碎的。

数据准备之所以有用，还有更多原因:它让你与原材料密切接触。了解您的输入有助于您及早发现潜在的错误，并对结果建立信心。

以下是一些数据准备场景:

*   一个客户交给你三个文件，每个文件包含一个单一地质现象的时间序列数据，但是观察到的数据被记录在不同的时间间隔，并使用不同的分隔符
*   机器学习算法只能处理数字数据，但您的输入只包含文本标签
*   你拿到了一个新兴服务的网络服务器的原始日志，你的任务是根据现有的访问者行为，对增长策略提出建议

# 数据收集

用于数据收集的工具库非常庞大，虽然我们将重点讨论 Python，但我们也想提一些有用的工具。如果它们在您的系统上可用，并且您希望大量处理数据，那么它们值得学习。

有一组工具属于 UNIX 传统，它强调文本处理，因此在过去的四十年中，开发了许多高性能和经过战斗考验的工具来处理文本。常见的工具有:`sed`、`grep`、`awk`、`sort`、`uniq`、`tr`、`cut`、`tail`、`head`。它们做的是非常基础的事情，比如从文件中过滤掉行(`grep`)或列(`cut`)，替换文本(`sed`、`tr`)或只显示文件的一部分(`head`、`tail`)。

我们只想用一个例子来证明这些工具的力量。

假设你拿到了一个网络服务器的日志文件，你对 IP 地址的分布感兴趣。

日志文件的每一行都包含一个常用日志服务器格式的条目(您可以从[http://ita.ee.lbl.gov/html/contrib/EPA-HTTP.html](http://ita.ee.lbl.gov/html/contrib/EPA-HTTP.html)下载该数据集):

```py
$ cat epa-html.txt
wpbfl2-45.gate.net [29:23:56:12] "GET /Access/ HTTP/1.0" 200 2376ebaca.icsi.net [30:00:22:20] "GET /Info.html HTTP/1.0" 200 884

```

例如，我们想知道某些用户访问我们网站的频率。

我们只对第一列感兴趣，因为这是可以找到 IP 地址或主机名的地方。之后，我们需要统计每台主机出现的次数，最后以友好的方式显示结果。

`sort` | `uniq -c`节是我们这里的主力:它首先对数据进行排序，`uniq -c`将保存出现的次数以及值。`sort -nr | head -15` 是我们的格式化部分；我们按数字(`-n`)和反向(`-r`)排序，只保留前 15 个条目。

用管子把它们连在一起:

```py
$ cut -d ' ' -f 1 epa-http.txt | sort | uniq -c | sort -nr | head -15
294 sandy.rtptok1.epa.gov
292 e659229.boeing.com
266 wicdgserv.wic.epa.gov
263 keyhole.es.dupont.com
248 dwilson.pr.mcs.net
176 oea4.r8stw56.epa.gov
174 macip26.nacion.co.cr
172 dcimsd23.dcimsd.epa.gov
167 www-b1.proxy.aol.com
158 piweba3y.prodigy.com
152 wictrn13.dcwictrn.epa.gov
151 nntp1.reach.com
151 inetg1.arco.com
149 canto04.nmsu.edu
146 weisman.metrokc.gov

```

只需一个命令，我们就可以将顺序服务器日志转换为访问我们站点的最常见主机的有序列表。我们还看到，在我们的顶级用户中，我们的访问量似乎没有很大的差异。

还有更多小的有用的工具，以下只是其中的一小部分:

*   `csvkit`:这是一套使用表格文件格式之王 CSV 的工具
*   `jq`:这是一款轻量级灵活的命令行 JSON 处理器
*   `xmlstarlet`:这是一个工具，支持带 XPath 的 XML 查询，等等
*   `q`:这在文本文件上运行 SQL

在 UNIX 命令行结束的地方，轻量级语言接管了。您可能只能从文本中获得印象，但您的同事可能会更喜欢 matplotlib 生成的视觉表示，如图表或漂亮的图形。

Python 及其数据工具生态系统比命令行要通用得多，但对于首次探索和简单操作来说，命令行的有效性往往是无与伦比的。

## 清洁数据

大多数真实世界的数据会有一些缺陷，因此需要先经过一个清理步骤。我们从一个小文件开始。尽管该文件仅包含四行，但它将允许我们演示清理数据集的过程:

```py
$ cat small.csv
22,6.1
41,5.7
 18,5.3*
29,NA

```

请注意，该文件有一些问题。包含值的行都是逗号分隔的，但是我们有缺失(NA)和可能不干净(5.3*)的值。尽管如此，我们可以将该文件加载到数据框中:

```py
>>> import pandas as pd
>>> df = pd.read_csv("small.csv")
>>> df
 22   6.1
0  41   5.7
1  18  5.3*
2  29   NaN

```

Pandas 用第一排作为`header`，但这不是我们想要的:

```py
>>> df = pd.read_csv("small.csv", header=None)
>>> df
 0     1
0  22   6.1
1  41   5.7
2  18  5.3*
3  29   NaN

```

这样更好，但是我们希望提供自己的列名，而不是数值:

```py
>>> df = pd.read_csv("small.csv", names=["age", "height"])
>>> df
 age height
0   22    6.1
1   41    5.7
2   18   5.3*
3   29    NaN

```

`age`列看起来不错，因为 Pandas 已经推断出了想要的类型，但是`height`还不能解析成数值:

```py
>>> df.age.dtype
dtype('int64')
>>> df.height.dtype
dtype('O')

```

如果我们试图将`height`列强制转换为浮点值，Pandas 将报告一个异常:

```py
>>> df.height.astype('float')
ValueError: invalid literal for float(): 5.3*

```

我们可以将任何可解析的值作为一个浮点数使用，并使用`convert_objects`方法丢弃其余的值:

```py
>>> df.height.convert_objects(convert_numeric=True)
0    6.1
1    5.7
2    NaN
3    NaN
Name: height, dtype: float64

```

如果我们事先知道数据集中不需要的字符，我们可以用一个定制的转换器函数来扩充`read_csv`方法:

```py
>>> remove_stars = lambda s: s.replace("*", "")
>>> df = pd.read_csv("small.csv", names=["age", "height"],
 converters={"height": remove_stars})
>>> df
 age height
0   22    6.1
1   41    5.7
2   18    5.3
3   29     NA

```

现在我们终于可以让高度栏更有用一点了。我们可以为其分配更新的版本，该版本具有喜欢的类型:

```py
>>> df.height = df.height.convert_objects(convert_numeric=True)
>>> df
 age  height
0   22     6.1
1   41     5.7
2   18     5.3
3   29     NaN

```

如果我们只想保留完整的条目，我们可以删除任何包含未定义值的行:

```py
>>> df.dropna()
 age  height
0   22     6.1
1   41     5.7
2   18     5.3

```

我们可以使用默认高度，也许是一个固定值:

```py
>>> df.fillna(5.0)
 age  height
0   22     6.1
1   41     5.7
2   18     5.3
3   29     5.0

```

另一方面，我们也可以使用现有值的平均值:

```py
>>> df.fillna(df.height.mean())
 age  height
0   22     6.1
1   41     5.7
2   18     5.3
3   29     5.7

```

最后三个数据框是完整和正确的，这取决于您在处理缺失值时对正确的定义。特别是，这些列具有所请求的类型，并准备好进行进一步的分析。哪个数据帧最适合将取决于手头的任务。

## 过滤

即使我们有干净且可能正确的数据，我们可能只想使用它的一部分，或者我们可能想检查异常值。离群点是由于可变性或测量误差而远离其他观察的观察点。在这两种情况下，我们都希望减少数据集中的元素数量，使其与进一步的处理更加相关。

在这个的例子中，我们将尝试寻找潜在的异常值。我们将使用美国能源信息管理局记录的欧洲布伦特原油现货价格。原始 Excel 数据可从[http://www.eia.gov/dnav/pet/hist_xls/rbrted.xls](http://www.eia.gov/dnav/pet/hist_xls/rbrted.xls)获得(可在第二张工作表中找到)。我们稍微清理了一下数据(清理过程是本章末尾练习的一部分)，并将使用以下数据框，包含 1987 年至 2015 年的 7160 个条目:

```py
>>> df.head()
 date  price
0 1987-05-20  18.63
1 1987-05-21  18.45
2 1987-05-22  18.55
3 1987-05-25  18.60
4 1987-05-26  18.63
>>> df.tail()
 date  price
7155 2015-08-04  49.08
7156 2015-08-05  49.04
7157 2015-08-06  47.80
7158 2015-08-07  47.54
7159 2015-08-10  48.30

```

尽管许多人都知道油价——无论是从新闻还是加油站——但让我们暂时忘记我们所知道的一切。我们可以先问两个极端:

```py
>>> df[df.price==df.price.min()]
 date  price
2937 1998-12-10    9.1
>>> df[df.price==df.price.max()]
 date   price
5373 2008-07-03  143.95

```

另一种发现潜在异常值的方法是要求最偏离平均值的值。我们可以首先使用`np.abs`函数计算与平均值的偏差:

```py
>>> np.abs(df.price - df.price.mean())
0       26.17137
1       26.35137
...
7157     2.99863
7158     2.73863 
7159     3.49863

```

我们现在可以从标准偏差的倍数(我们选择 2.5)来比较这个偏差:

```py
>>> import numpy as np
>>> df[np.abs(df.price - df.price.mean()) > 2.5 * df.price.std()]
 date   price
5354 2008-06-06  132.81
5355 2008-06-09  134.43
5356 2008-06-10  135.24
5357 2008-06-11  134.52
5358 2008-06-12  132.11
5359 2008-06-13  134.29
5360 2008-06-16  133.90
5361 2008-06-17  131.27
5363 2008-06-19  131.84
5364 2008-06-20  134.28
5365 2008-06-23  134.54
5366 2008-06-24  135.37
5367 2008-06-25  131.59
5368 2008-06-26  136.82
5369 2008-06-27  139.38
5370 2008-06-30  138.40
5371 2008-07-01  140.67
5372 2008-07-02  141.24
5373 2008-07-03  143.95
5374 2008-07-07  139.62
5375 2008-07-08  134.15
5376 2008-07-09  133.91
5377 2008-07-10  135.81
5378 2008-07-11  143.68
5379 2008-07-14  142.43
5380 2008-07-15  136.02
5381 2008-07-16  133.31
5382 2008-07-17  134.16

```

我们看到 2008 年夏天的那几天一定很特别。果然，不难找到标题为*2007–08*石油冲击的原因和后果的文章和随笔。我们仅仅通过查看数据就发现了这些事件的踪迹。

我们可以每十年分别问一次上面的问题。我们首先让我们的数据框架看起来更像一个时间序列:

```py
>>> df.index = df.date
>>> del df["date"]
>>> df.head()
 price
date
1987-05-20  18.63
1987-05-21  18.45
1987-05-22  18.55
1987-05-25  18.60
1987-05-26  18.63

```

我们可以过滤掉八十年代:

```py
>>> decade = df["1980":"1989"]
>>> decade[np.abs(decade.price - decade.price.mean()) > 2.5 * decade.price.std()]
 price
date
1988-10-03  11.60
1988-10-04  11.65
1988-10-05  11.20
1988-10-06  11.30
1988-10-07  11.35

```

我们观察到在可获得的数据(1987-1989)中，1988 年的秋天显示出石油价格的轻微上涨。同样，在九十年代，我们看到我们有一个更大的偏差，在 1990 年秋天:

```py
>>> decade = df["1990":"1999"]
>>> decade[np.abs(decade.price - decade.price.mean()) > 5 * decade.price.std()]
 price
date
1990-09-24  40.75
1990-09-26  40.85
1990-09-27  41.45
1990-09-28  41.00
1990-10-09  40.90
1990-10-10  40.20
1990-10-11  41.15

```

过滤数据的用例还有很多。空间和时间是典型的单位:您可能希望按州或城市过滤人口普查数据，或者按季度过滤经济数据。可能性是无穷无尽的，将由你的项目驱动。

## 合并数据

的情况很常见:你有多个数据源，但是为了对内容做陈述，你宁愿把它们组合起来。幸运的是，Pandas 的连接和合并功能在组合、连接或对齐数据时，抽象掉了大部分痛苦。它也以高度优化的方式做到了这一点。

在两个数据帧具有相似形状的情况下，一个接一个地追加可能是有用的。也许`A`和`B`是产品，一个数据框包含商店中每种产品售出的商品数量:

```py
>>> df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
>>> df1
 A  B
0  1  4
1  2  5
2  3  6
>>> df2 = pd.DataFrame({'A': [4, 5, 6], 'B': [7, 8, 9]})
>>> df2
 A  B
0  4  7
1  5  8
2  6  9
>>> df1.append(df2)
 A  B
0  1  4
1  2  5
2  3  6
0  4  7
1  5  8
2  6  9

```

有时，我们不会关心原始数据帧的索引:

```py
>>> df1.append(df2, ignore_index=True)
 A  B
0  1  4
1  2  5
2  3  6
3  4  7
4  5  8
5  6  9

```

`pd.concat`函数提供了一种更灵活的组合对象的方法，它采用任意数量的序列、数据帧或面板作为输入。默认行为类似于追加:

```py
>>> pd.concat([df1, df2])
 A  B
0  1  4
1  2  5
2  3  6
0  4  7
1  5  8
2  6  9

```

默认的`concat`操作沿着行或索引追加两个帧，对应于轴 0。要沿着列连接，我们可以传入 axis 关键字参数:

```py
>>> pd.concat([df1, df2], axis=1)
 A  B  A  B
0  1  4  4  7
1  2  5  5  8
2  3  6  6  9

```

我们可以添加关键字来创建分层索引。

```py
>>> pd.concat([df1, df2], keys=['UK', 'DE'])
 A  B
UK 0  1  4
 1  2  5
 2  3  6
DE 0  4  7
 1  5  8
 2  6  9

```

如果您希望稍后引用数据框的某些部分，这可能会很有用。我们使用`ix`索引器:

```py
>>> df3 = pd.concat([df1, df2], keys=['UK', 'DE'])
>>> df3.ix["UK"]
 A  B
0  1  4
1  2  5
2  3  6

```

数据框类似于数据库表。因此，Pandas 在它们身上实现类似 SQL 的连接操作并不奇怪。令人惊讶的是，这些操作高度优化且速度极快:

```py
>>> import numpy as np
>>> df1 = pd.DataFrame({'key': ['A', 'B', 'C', 'D'],
 'value': range(4)})
>>> df1
 key  value
0   A      0
1   B      1
2   C      2
3   D      3
>>> df2 = pd.DataFrame({'key': ['B', 'D', 'D', 'E'], 'value': range(10, 14)})
>>> df2
 key  value
0   B     10
1   D     11
2   D     12
3   E     13

```

如果我们在`key`上合并，我们得到一个内部连接。这通过基于连接谓词组合原始数据帧的列值来创建新的数据帧，这里使用`key`属性:

```py
>>> df1.merge(df2, on='key')
 key  value_x  value_y
0   B        1       10
1   D        3       11
2   D        3       12

```

左、右和全连接可以由`how`参数指定:

```py
>>> df1.merge(df2, on='key', how='left')
 key  value_x  value_y
0   A        0      NaN
1   B        1       10
2   C        2      NaN
3   D        3       11
4   D        3       12
>>> df1.merge(df2, on='key', how='right')
 key  value_x  value_y
0   B        1       10
1   D        3       11
2   D        3       12
3   E      NaN       13
>>> df1.merge(df2, on='key', how='outer')
 key  value_x  value_y
0   A        0      NaN
1   B        1       10
2   C        2      NaN
3   D        3       11
4   D        3       12
5   E      NaN       13

```

合并方法可以用 how 参数指定。下表显示了与 SQL 相比较的方法:

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

合并方法

 | 

SQL 联接名

 | 

描述

 |
| --- | --- | --- |
| `left` | 左外连接 | 仅使用左侧框架中的按键。 |
| `right` | 右外连接 | 仅使用右侧框架中的关键点。 |
| `outer` | 完全外部连接 | 使用两个帧的联合密钥。 |
| `inner` | 内部连接 | 使用两个帧的关键点的交集。 |

## 重塑数据

我们看到了如何组合数据帧，但有时我们在单个数据结构中拥有所有正确的数据，但这种格式对于某些任务是不切实际的。我们再次从一些人工天气数据开始:

```py
>>> df
 date    city  value
0   2000-01-03  London      6
1   2000-01-04  London      3
2   2000-01-05  London      4
3   2000-01-03  Mexico      3
4   2000-01-04  Mexico      9
5   2000-01-05  Mexico      8
6   2000-01-03  Mumbai     12
7   2000-01-04  Mumbai      9
8   2000-01-05  Mumbai      8
9   2000-01-03   Tokyo      5
10  2000-01-04   Tokyo      5
11  2000-01-05   Tokyo      6

```

如果我们想计算每个城市的最高温度，我们可以将数据按城市分组，然后使用`max`函数:

```py
>>> df.groupby('city').max()
 date  value
city
London  2000-01-05      6
Mexico  2000-01-05      9
Mumbai  2000-01-05     12
Tokyo   2000-01-05      6

```

然而，如果我们每次都必须把数据整理成表格，我们可以更有效一点，首先创建一个重新整形的数据框架，把日期作为索引，城市作为列。

我们可以用`pivot`函数创建这样一个数据帧。参数是索引(我们使用日期)、列(我们使用城市)和值(存储在原始数据框的值列中):

```py
>>> pv = df.pivot("date", "city", "value")
>>> pv
city date         London  Mexico  Mumbai  Tokyo
2000-01-03       6       3      12      5
2000-01-04       3       9       9      5
2000-01-05       4       8       8      6

```

我们可以直接在这个新的数据帧上使用`max`功能:

```py
>>> pv.max()
city
London     6
Mexico     9
Mumbai    12
Tokyo      6
dtype: int64

```

有了更合适的形状，其他操作也变得更容易。例如，为了找到每天的最高温度，我们可以简单地提供一个额外的轴参数:

```py
>>> pv.max(axis=1)
date
2000-01-03    12
2000-01-04     9
2000-01-05     8
dtype: int64

```

# 数据聚合

作为最后一个主题，我们将研究如何获得聚合数据的精简视图。Pandas 自带很多内置的聚合功能。我们已经在[第三章](3.html "Chapter 3. Data Analysis with Pandas")、*Pandas 数据分析*中看到了`describe`功能。这也适用于部分数据。我们再次从一些人工数据开始，包括每个城市日照时数的测量值和日期:

```py
>>> df.head()
 country     city        date  hours
0  Germany  Hamburg  2015-06-01      8
1  Germany  Hamburg  2015-06-02     10
2  Germany  Hamburg  2015-06-03      9
3  Germany  Hamburg  2015-06-04      7
4  Germany  Hamburg  2015-06-05      3

```

要查看每个`city`的摘要，我们使用分组数据集上的`describe`功能:

```py
>>> df.groupby("city").describe()
 hours
city
Berlin     count  10.000000
 mean    6.000000
 std     3.741657
 min     0.000000
 25%     4.000000
 50%     6.000000
 75%     9.750000
 max    10.000000
Birmingham count  10.000000
 mean    5.100000
 std     2.078995
 min     2.000000
 25%     4.000000
 50%     5.500000
 75%     6.750000
 max     8.000000

```

在某些`data sets`上，按多个属性分组会很有用。通过输入两个列名，我们可以大致了解每个国家和日期的日照时间:

```py
>>> df.groupby(["country", "date"]).describe()
 hours country date
France  2015-06-01 count  5.000000
 mean   6.200000
 std    1.095445
 min    5.000000
 25%    5.000000
 50%    7.000000
 75%    7.000000
 max    7.000000
 2015-06-02 count  5.000000
 mean   3.600000
 std    3.577709
 min    0.000000
 25%    0.000000
 50%    4.000000
 75%    6.000000
 max    8.000000
UK      2015-06-07 std    3.872983
 min    0.000000
 25%    2.000000
 50%    6.000000
 75%    8.000000
 max    9.000000

```

我们也可以计算单个统计:

```py
>>> df.groupby("city").mean()
 hours
city
Berlin        6.0
Birmingham    5.1
Bordeax       4.7
Edinburgh     7.5
Frankfurt     5.8
Glasgow       4.8
Hamburg       5.5
Leipzig       5.0
London        4.8
Lyon          5.0
Manchester    5.2
Marseille     6.2
Munich        6.6
Nice          3.9
Paris         6.3

```

最后，我们可以用`agg`方法定义要应用于组的任何函数。上面本来可以这样用`agg`来写:

```py
>>> df.groupby("city").agg(np.mean)
hours
city
Berlin        6.0
Birmingham    5.1
Bordeax       4.7
Edinburgh     7.5
Frankfurt     5.8
Glasgow       4.8
...

```

但是任意函数都是可能的。作为最后一个例子，我们定义了一个`custom`函数，它接受一个序列对象的输入，并计算最小和最大元素之间的差值:

```py
>>> df.groupby("city").agg(lambda s: abs(min(s) - max(s)))
 hours
city
Berlin         10
Birmingham      6
Bordeax        10
Edinburgh       8
Frankfurt       9
Glasgow        10
Hamburg        10
Leipzig         9
London         10
Lyon            8
Manchester     10
Marseille      10
Munich          9
Nice           10
Paris           9

```

# 分组数据

数据探索期间的一个典型工作流程如下:

*   您可以找到一个用于对数据进行分组的标准。也许你有沿着大陆的每个国家的 GDP 数据，你想问关于大陆的问题。这些问题通常会导致一些函数应用——你可能想计算每个大陆的平均国内生产总值。最后，您希望将这些数据存储在新的数据结构中，以便进一步处理。
*   这里我们用一个更简单的例子。想象一些关于每天晴天小时数和城市的虚构天气数据:

    ```py
    >>> df
     date    city  value
    0   2000-01-03  London      6
    1   2000-01-04  London      3
    2   2000-01-05  London      4
    3   2000-01-03  Mexico      3
    4   2000-01-04  Mexico      9
    5   2000-01-05  Mexico      8
    6   2000-01-03  Mumbai     12
    7   2000-01-04  Mumbai      9
    8   2000-01-05  Mumbai      8
    9   2000-01-03   Tokyo      5
    10  2000-01-04   Tokyo      5
    11  2000-01-05   Tokyo      6

    ```

*   `groups`属性返回包含唯一组和相应值的字典作为轴标签:

    ```py
    >>> df.groupby("city").groups
    {'London': [0, 1, 2],
    'Mexico': [3, 4, 5],
    'Mumbai': [6, 7, 8],
    'Tokyo': [9, 10, 11]}

    ```

*   虽然的结果是一个 GroupBy 对象，而不是一个 DataFrame，但是我们可以使用通常的索引符号来引用列:

    ```py
    >>> grouped = df.groupby(["city", "value"])
    >>> grouped["value"].max()
    city
    London     6
    Mexico     9
    Mumbai    12
    Tokyo      6
    Name: value, dtype: int64
    >>> grouped["value"].sum()
    city
    London    13
    Mexico    20
    Mumbai    29
    Tokyo     16
    Name: value, dtype: int64

    ```

*   我们看到，根据我们的数据集，孟买似乎是一个阳光明媚的城市。实现上述目标的另一种更详细的方法是:

    ```py
    >>> df['value'].groupby(df['city']).sum()
    city
    London    13
    Mexico    20
    Mumbai    29
    Tokyo     16
    Name: value, dtype: 
    int64

    ```

# 总结

在这一章中，我们已经看到了处理数据帧的方法，从清理和过滤，到分组、聚合和整形。Pandas 使许多常见的操作变得非常容易，而更复杂的操作，如按多个属性进行旋转或分组，也经常可以表示为一行。清理和准备数据是数据探索和分析的重要组成部分。

下一章将简要介绍机器学习算法，该算法正在应用数据分析结果来做出决策或构建有用的产品。

**练习练习**

**练习 1:** 清洗:在关于过滤的部分，我们使用了欧洲布伦特原油现货价格，在网上可以找到一个 Excel 文档。拿着这个 Excel 电子表格，试着把它转换成一个 CSV 文档，准备和 Pandas 一起导入。

提示:有很多方法可以做到。我们使用了一个名为`xls2csv.py`的小工具，并且我们能够用一个辅助方法加载生成的 CSV 文件:

```py
import datetime
import pandas as pd
def convert_date(s):
    parts = s.replace("(", "").replace(")", "").split(",")
	if len(parts) < 6:
	return datetime.date(1970, 1, 1)
	return datetime.datetime(*[int(p) for p in parts])
	df = pd.read_csv("RBRTEd.csv", sep=',', names=["date", "price"], converters={"date": convert_date}).dropna()
```

拿一个对你的工作很重要的数据集来说，或者如果你手头没有，那就拿一个你感兴趣的并且可以在网上找到的数据集来说。提前问一两个关于数据的问题。然后使用清理、过滤、分组和绘图技术来回答你的问题。