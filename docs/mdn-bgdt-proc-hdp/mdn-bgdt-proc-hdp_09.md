# 第九章：设计数据可视化解决方案

一旦数据生活在 Hadoop 生态系统中，并且已经经过处理，下一步的逻辑步骤就是构建驱动业务决策的分析。

在本章中，我们将学习以下主题：

+   数据可视化

+   Apache Druid

+   Apache Superset

# 数据可视化

数据可视化是通过图形手段理解原始数据中各种实体之间关系的过程。这是一个非常强大的技术，因为它使最终用户能够以非常简单的方式获取信息，即使他们根本不了解底层数据。

数据可视化在从大数据中洞察的视觉沟通中扮演着非常重要的角色。它既是艺术也是科学，需要投入一些努力来理解数据；同时，我们还需要对目标受众有一定的了解。

到目前为止，我们已经看到任何类型的数据都可以存储在**Hadoop 文件系统**（**HDFS**）中。为了将复杂的数据结构转换为视觉形式，我们需要了解用于表示数据的标准技术。

在数据可视化中，信息以图形的形式传达给最终用户，这些图形可以是 1D、2D、3D，甚至更高维度。这完全取决于我们试图传达的意义。

让我们看看用于向用户传达视觉信息的标准图形：

+   柱状图/条形图

+   线形/面积图

+   饼图

+   雷达图

+   散点/气泡图

+   标签云

+   气泡图

# 柱状图/条形图

这是一个二维数据图形表示，其中数据点以垂直/水平条的形式显示。每个条代表一个数据点。当没有时间维度与数据点相关时，这些点显示的顺序可能没有差别。当我们处理用于表示条形图的时间序列数据时，我们通常遵循 X（水平）轴上的显示时间顺序。

让我们看看一个由四个数据点生成的示例图表。这些数据代表每个用户的金额：

![图片](img/9a5e0f63-0700-430c-873e-dbeeb2c09d0b.png)

**解释**：图表既有行和列中的文本数据，也有视觉元素。如果你仔细观察，文本数据的大小较小，只有四条记录。但视觉图形直接传达信息，而不需要了解任何关于数据的信息。

图表传达的信息是：

+   西塔的钱比所有人都要多

+   吉塔的钱最少

其他解释也是可能的。它们留给读者去思考。

# 线形/面积图

这通常也是一个二维图表，其中每个数据点都表示为画布上的一个点，所有属于同一数据集的点都通过线条连接。当水平/垂直轴的区域完全覆盖到线条时，此图表就变成了面积图。

同一个图表中可以有不止一条线，这表示同一实体的多个数据系列。

让我们看看基于之前相同数据的这个面积图的样本：

![数据图](img/b9849529-fcc2-488a-8c2d-02fadaba8876.png)

这些是图表的特性：

+   *x* 轴上有所有人的名单

+   *y* 轴表示从**0**到**100**的金额

+   图表上有四个点，对应于表格形式的值

+   点用直线连接

+   线条下方填充的区域使其成为一个面积图

# 饼图

这也是一个以圆形中的多个扇区形式绘制的二维图表。当我们要强调所有数据点的相对重要性时，这种图表很有用。

让我们看看之前相同数据集绘制的示例图表，以更好地理解它：

![图表](img/ffa15300-a402-41d1-9baf-d4caae0188a9.png)

如您所见，使用这个图表很容易理解每个人拥有的金额的相对重要性。

得出的结论与之前的图表相似。但图表是一个简单的圆圈，这里没有多个维度来增加用户的负担。

# 雷达图

这也是一个二维图形，其中数据轴是等距扇形的边缘（就像饼图的边缘）。当有多个维度，我们想要理解每个数据点的相对重要性时，这种图表很有用。

为了更好地理解这个图表，让我们看看这个样本数据和图形：

![图表](img/436900d2-ca59-4ff0-a579-c8a69ab9b5a6.png)

数据由八列组成：

+   **第一列**：所有用户的名单

+   **第二至第八列**：一周中的天数和每个人那天拥有的美元数

我们想要绘制一个图表，显示以下内容：

+   每天的总美元数

+   每个人每天拥有的美元数

我们已经将所有这些信息绘制在雷达图中，其中轴是扇区（天数），最大值为**400**。每个用户的值都是叠加绘制的，这样我们就可以知道总价值而不是相对价值（这与面积堆叠类似）。

# 散点/气泡图

散点图可以是一个多维图形。这是我们理解起来比较简单的一种图形，因为我们将在画布上渲染每个数据点，对应于轴上的数值。这种图表有助于理解轴上每个点的相对重要性。

气泡图是散点图的一种变体，画布上的点以大泡泡的形式显示值（以表示其重要性）。

让我们用这个例子来看看这两种图形：

![扇形图](img/15f06c51-ebce-4863-935b-8485c89f33e4.png)

左侧的图形是气泡图，右侧的是散点图。

让我们看看数据和生成的图表。

输入数据：

+   由五行组成，而我们在列中有**销售**和**产品数量**

使用气泡图：

+   *y* 轴显示产品的数量

+   *x* 轴仅表示位置，并不反映输入数据的值

+   画布上的每个点表示对应产品数量的销售

使用散点图：

+   *y* 轴显示完成的销售

+   *x* 轴显示销售的产品

+   画布上的每个点表示输入中的每一行

# 其他图表

在本节中未涵盖但值得在 [`d3js.org`](https://d3js.org) 网站上探索的其他许多图形类型。这将帮助您了解数据如何表示，以便向用户传达非常好的信息。

# Hadoop 中的实用数据可视化

Hadoop 拥有丰富的数据源和应用生态系统，帮助我们构建丰富的可视化。在接下来的章节中，我们将了解两个这样的应用：

+   Apache Druid

+   Apache Superset

我们还将学习如何使用 Apache Superset 与 RDBMS（如 MySQL）中的数据一起使用。

# Apache Druid

Apache Druid 是一个分布式、高性能的列式存储。其官方网站是 [`druid.io`](https://druid.io)。

Druid 允许我们存储实时和历史数据，这些数据本质上是时间序列的。它还提供了快速的数据聚合和灵活的数据探索。该架构支持在千兆字节大小上存储万亿数据点。

为了更了解 Druid 架构，请参阅此白皮书 [`static.druid.io/docs/druid.pdf`](http://static.druid.io/docs/druid.pdf)。

# 德鲁伊组件

让我们快速了解一下 Druid 集群的不同组件：

| **组件** | **描述** |
| --- | --- |
| 德鲁伊代理 | 这些节点知道数据在集群中的位置。这些节点被应用程序/客户端联系以获取 Druid 中的数据。 |
| 德鲁伊协调器 | 这些节点管理历史节点上的数据（它们加载、删除和负载均衡数据）。 |
| 德鲁伊统治者 | 此组件负责接受任务并返回任务的状态。 |
| 德鲁伊路由器 | 当数据量达到或超过太字节级别时，需要这些节点。这些节点将请求路由到代理。 |
| 德鲁伊历史 | 这些节点存储不可变段，是 Druid 集群的骨干。它们提供加载段、删除段，并在段请求上提供查询服务。 |

# 其他所需组件

下表展示了其他一些所需的组件：

| **组件**  | **描述** |
| --- | --- |
| Zookeeper | Apache Zookeeper 是一个高度可靠的分布式协调服务 |
| **组件** | **描述** |

# Apache Druid 安装

Apache Druid 可以以独立模式或作为 Hadoop 集群的一部分进行安装。在本节中，我们将了解如何通过 Apache Ambari 安装 Druid。

# 添加服务

首先，我们在 Hadoop 集群中服务列表下方调用操作下拉菜单。

屏幕看起来像这样：

![](img/56f10c5f-97bf-40c1-b098-83a1f671e5ee.png)

# 选择 Druid 和 Superset

在这个设置中，我们将同时安装 Druid 和 Superset。Superset 是我们将在下一步学习的数据可视化应用程序。

选择屏幕看起来像这样：

![](img/f0086078-bf64-45a7-af90-00f00093b19f.png)

在两个服务都选择完毕后，点击下一步。

# 服务器上的服务放置

在这一步，我们将有一个选择，可以选择应用程序需要安装的服务器。我已经选择了节点 3 来完成这个任务。你可以选择任何你希望的服务器。

屏幕看起来像这样：

![](img/e573a6f3-105c-45a7-a56b-b903cd89c4a9.png)

当更改完成后，点击下一步。

# 选择从属节点和客户端

在这里，我们有一个选择，可以选择安装组件所需的从属节点和客户端节点。我已经保留了为我预先选定的选项：

![](img/23b2b7f5-6540-421e-8a95-2a3a0a3dcf83.png)

# 服务配置

在这一步，我们需要选择 Druid 和 Superset 应用程序使用的元数据存储的数据库、用户名和密码。请随意选择默认值。我已经为它们两个选择了 MySQL 作为后端存储。

屏幕看起来像这样：

![](img/72b59851-5564-4c73-8143-b34a002282ee.png)

一旦更改看起来不错，点击屏幕底部的下一步按钮。

# 服务安装

在这一步，应用程序将自动安装，并在计划的末尾显示状态。

安装完成后，点击下一步。当前屏幕的更改看起来像这样：

![](img/1d03751d-b4f0-4879-bbc8-ef7ba50c9a27.png)

# 安装摘要

一切成功完成后，我们会看到所做工作的摘要。完成时点击完成：

![](img/02790068-b2da-4efd-a4b5-2afc1dfdb055.png)

# 将样本数据导入 Druid

一旦我们所有的 Druid 相关应用程序都在我们的 Hadoop 集群中运行，我们需要一个样本数据集，以便运行一些分析任务。

让我们看看如何加载数据。从互联网下载 Druid 存档：

```py
[druid@node-3 ~$ curl -O http://static.druid.io/artifacts/releases/druid-0.12.0-bin.tar.gz
% Total % Received % Xferd Average Speed Time Time Time Current
                               Dload Upload Total Spent Left Speed
100 222M 100 222M 0 0 1500k 0 0:02:32 0:02:32 --:--:-- 594k
```

解压存档：

```py
[druid@node-3 ~$ tar -xzf druid-0.12.0-bin.tar.gz
```

将样本维基百科数据复制到 Hadoop：

```py
[druid@node-3 ~]$ cd druid-0.12.0
[druid@node-3 ~/druid-0.12.0]$ hadoop fs -mkdir /user/druid/quickstart
[druid@node-3 ~/druid-0.12.0]$ hadoop fs -put quickstart/wikiticker-2015-09-12-sampled.json.gz /user/druid/quickstart/
```

提交导入请求：

```py
[druid@node-3 druid-0.12.0]$ curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/wikiticker-index.json localhost:8090/druid/indexer/v1/task;echo
{"task":"index_hadoop_wikiticker_2018-03-16T04:54:38.979Z"}
```

在这一步之后，Druid 将自动将数据导入 Druid 集群，进度可以在 Overlord 控制台中查看。

该界面可通过`http://<overlord-ip>:8090/console.html`访问。屏幕看起来像这样：

![](img/8df1b469-ba70-4646-a81f-bfe2cce4267c.png)

一旦导入完成，我们将看到作业状态为成功。

如果发生`FAILED`导入，请确保配置为存储 Druid 集群元数据的后端正在运行。

尽管 Druid 与 OpenJDK 安装配合良好，但我遇到了一些类在运行时不可用的问题。为了克服这个问题，我不得不使用 Oracle Java 版本 1.8 来运行所有 Druid 应用程序。

现在，我们已经准备好开始使用 Druid 来进行我们的可视化任务。

# MySQL 数据库

Apache Superset 还允许我们读取 RDBMS 系统中如 MySQL 存在的数据。我们将在本节中创建一个示例数据库，稍后我们可以使用 Superset 来创建可视化。

# 示例数据库

员工数据库是一个标准数据集，包含一个示例组织和他们的员工、薪资和部门数据。我们将看到如何为我们的任务设置它。

本节假设 MySQL 数据库已经配置并正在运行。

# 下载示例数据集

在任何可以访问 MySQL 数据库的服务器上，使用以下命令从 GitHub 下载示例数据集：

```py
[user@master ~]$ sudo yum install git -y
[user@master ~]$ git clone https://github.com/datacharmer/test_db
Cloning into 'test_db'...
remote: Counting objects: 98, done.
remote: Total 98 (delta 0), reused 0 (delta 0), pack-reused 98
Unpacking objects: 100% (98/98), done.
```

# 将数据复制到 MySQL

在这一步，我们将导入文件中的数据内容到 MySQL 数据库中：

```py
[user@master test_db]$ mysql -u root < employees.sql
INFO
CREATING DATABASE STRUCTURE
INFO
storage engine: InnoDB
INFO
LOADING departments
INFO
LOADING employees
INFO
LOADING dept_emp
INFO
LOADING dept_manager
INFO
LOADING titles
INFO
LOADING salaries
data_load_time_diff
NULL

```

# 验证表的一致性

这是一个重要的步骤，只是为了确保我们导入的所有数据都正确存储在数据库中。一致性检查的摘要如下所示，验证过程中会显示：

```py
[user@master test_db]$ mysql -u root -t < test_employees_sha.sql
+----------------------+
| INFO                 |
+----------------------+
| TESTING INSTALLATION |
+----------------------+
+--------------+------------------+------------------------------------------+
| table_name   | expected_records | expected_crc                             |
+--------------+------------------+------------------------------------------+
| employees    | 300024 | 4d4aa689914d8fd41db7e45c2168e7dcb9697359 |
| departments  |  9 | 4b315afa0e35ca6649df897b958345bcb3d2b764 |
| dept_manager |               24 | 9687a7d6f93ca8847388a42a6d8d93982a841c6c |
| dept_emp     | 331603 | d95ab9fe07df0865f592574b3b33b9c741d9fd1b |
| titles       | 443308 | d12d5f746b88f07e69b9e36675b6067abb01b60e |
| salaries     | 2844047 | b5a1785c27d75e33a4173aaa22ccf41ebd7d4a9f |
+--------------+------------------+------------------------------------------+
+--------------+------------------+------------------------------------------+
| table_name   | found_records    | found_crc                        |
+--------------+------------------+------------------------------------------+
| employees    | 300024 | 4d4aa689914d8fd41db7e45c2168e7dcb9697359 |
| departments  |  9 | 4b315afa0e35ca6649df897b958345bcb3d2b764 |
| dept_manager |               24 | 9687a7d6f93ca8847388a42a6d8d93982a841c6c |
| dept_emp     | 331603 | d95ab9fe07df0865f592574b3b33b9c741d9fd1b |
| titles       | 443308 | d12d5f746b88f07e69b9e36675b6067abb01b60e |
| salaries     | 2844047 | b5a1785c27d75e33a4173aaa22ccf41ebd7d4a9f |
+--------------+------------------+------------------------------------------+
+--------------+---------------+-----------+
| table_name   | records_match | crc_match |
+--------------+---------------+-----------+
| employees    | OK | ok        |
| departments  | OK | ok        |
| dept_manager | OK            | ok |
| dept_emp     | OK | ok        |
| titles       | OK | ok        |
| salaries     | OK | ok        |
+--------------+---------------+-----------+
+------------------+
| computation_time |
+------------------+
| 00:00:11         |
+------------------+
+---------+--------+
| summary | result |
+---------+--------+
| CRC     | OK |
| count   | OK |
+---------+--------+
```

现在数据已经正确加载到名为 **employees** 的 MySQL 数据库中。

# 单个标准化表

在数据仓库中，与许多小相关表相比，拥有标准化表是一种标准做法。让我们创建一个包含员工、薪资和部门详情的单个标准化表。

```py
MariaDB [employees]> create table employee_norm as select e.emp_no, e.birth_date, CONCAT_WS(' ', e.first_name, e.last_name) full_name , e.gender, e.hire_date, s.salary, s.from_date, s.to_date, d.dept_name, t.title from employees e, salaries s, departments d, dept_emp de, titles t where e.emp_no = t.emp_no and e.emp_no = s.emp_no and d.dept_no = de.dept_no and e.emp_no = de.emp_no and s.to_date < de.to_date and s.to_date < t.to_date order by emp_no, s.from_date;
Query OK, 3721923 rows affected (1 min 7.14 sec)
Records: 3721923  Duplicates: 0  Warnings: 0

MariaDB [employees]> select * from employee_norm limit 1\G
*************************** 1\. row ***************************
    emp_no: 10001
birth_date: 1953-09-02
 full_name: Georgi Facello
    gender: M
 hire_date: 1986-06-26
    salary: 60117
 from_date: 1986-06-26
   to_date: 1987-06-26
 dept_name: Development
     title: Senior Engineer
1 row in set (0.00 sec)

MariaDB [employees]> 
```

一旦我们标准化了数据，我们将展示如何使用此表中的数据来生成丰富的可视化。

# Apache Superset

Superset 是一个现代的企业级商业智能应用程序。这个应用程序的重要特性是我们可以直接从浏览器运行所有分析。为此不需要安装任何特殊软件。

如果你还记得，我们在前面的章节中已经安装了 Superset 和 Druid。现在我们需要学习如何使用 Superset 来构建丰富的可视化。

# 访问 Superset 应用程序

在你的网络浏览器中打开 `http://<SERVER-IP>:9088/`。如果一切运行正常，我们将看到一个登录界面，如下所示：

![](img/cb4e68e7-e99a-4f3d-956f-ac74a506fe67.png)

将用户名输入为 `admin`，密码为安装过程中选择的密码。

# Superset 仪表板

仪表板是 Superset 应用程序中的重要组成部分。它们让我们以图形形式展示分析计算的成果。仪表板是由切片创建的，而切片又是从 Superset 应用程序中配置的各种数据源构建的。

登录成功后，系统不会自动为我们创建任何仪表板。我们将看到一个空白的仪表板列表，如下所示：

![](img/1cfcf69f-da70-4dc1-a4ff-b74154a0cfdc.png)

为了构建仪表板，我们首先需要配置数据源。因此，让我们点击顶部导航中的“源”菜单，然后点击“刷新 Druid 元数据”：

![](img/2ca5bc92-fdeb-48ec-90cb-1e1b41350655.png)

在此步骤之后，我们将被带到数据源页面，并自动在此处出现一个新的数据源。记得我们之前已经将此数据集上传到 Druid 了吗？

![](img/186e1e1f-b1d8-4348-9b89-d65bc8e27621.png)

现在，我们可以点击数据源名称（绿色），这将带我们到数据源探索页面：

![](img/721f4f3e-09d5-4153-aa95-0e04aec73401.png)

如我们所见，此页面被分为多个部分。

+   **左侧 UI**：

    +   **数据源和图表类型**：在此列中，我们可以选择我们需要使用的数据源，以及我们希望在右侧看到的图形类型。

    +   **时间**：这是我们可以将数据源的数据限制在给定时间范围内的列。初学者往往会在这个列上犯错误，因为他们不会在右侧看到任何数据。所以，选择一个起始时间值（建议使用 100 年前这样的相对值以获得更好的结果）。

    +   **按组分组**：此列用于根据输入数据的维度对数据进行分组。

    +   **其他选项**：在“按组分组”下方还有其他选项，我们将在接下来的步骤中探讨。

+   **右侧 UI**：

    +   此 UI 包含我们在左侧选择的选项的结果。

# 理解维基百科编辑数据

在我们开始构建可视化之前。让我们更仔细地看看我们已导入 Druid 的数据以及我们可以从这些数据中渲染哪些类型的图形：

| **度量/维度** | **数据类型** | **描述** |
| --- | --- | --- |
| `delta` | `LONG` | 以数值形式表示的更改 |
| `deleted` | `LONG` | 文章中删除的数据，以数值形式表示 |
| `added` | `LONG` | 添加的数据，以数值形式表示 |
| `isMinor` | `STRING` | 布尔值，表示这是否是一个小编辑 |
| `page` | `STRING` | 发生更改的维基百科页面 |
| `isRobot` | `STRING` | 更改是否由机器人（不是人类但某种形式的程序）进行的 |
| `channel` | `STRING` | 发生更改的维基百科频道 |
| `regionName` | `STRING` | 进行更改的地理区域名称 |
| `cityName` | `STRING` | 进行更改的城市名称 |
| `countryIsoCode` | `STRING` | 进行更改的国家的 ISO 代码 |
| `user` | `STRING` | 进行更改的维基百科用户或 IP 地址 |
| `countryName` | `STRING` | 进行更改的国家名称 |
| `isAnonymous` | `STRING` | 更改是否由匿名用户（未登录状态）进行的 |
| `regionIsoCode` | `STRING` | 进行更改的地理区域的 ISO 代码 |
| `metroCode` | `STRING` | 这与美国邮政编码类似（见[`www.nlsinfo.org/usersvc/NLSY97/NLSY97Rnd9geocodeCodebookSupplement/gatt101.html`](http://www.nlsinfo.org/usersvc/NLSY97/NLSY97Rnd9geocodeCodebookSupplement/gatt101.html)） |
| `namespace` | `STRING` | 维基百科文章/页面命名空间 |
| `comment` | `STRING` | 为这次修改添加的注释 |
| `isNew` | `STRING` | 如果这是一个新页面则为`true`（见[`en.wikipedia.org/wiki/Wikipedia:Glossary#N`](https://en.wikipedia.org/wiki/Wikipedia:Glossary#N)） |
| `isUnpatrolled` | `STRING` | 如果修改不是受监控的，则为`true`（见[`en.wikipedia.org/wiki/Wikipedia:New_pages_patrol`](https://en.wikipedia.org/wiki/Wikipedia:New_pages_patrol)） |

因此，我们已经列出了数据的所有属性。让我们看看样本之一，以便更好地理解我们在谈论什么：

```py
{
  "time": "2015-09-12T00:47:21.578Z",
  "channel": "#en.wikipedia",
  "cityName": null,
  "comment": "Copying assessment table to wiki",
  "countryIsoCode": null,
  "countryName": null,
  "isAnonymous": false,
  "isMinor": false,
  "isNew": false,
  "isRobot": true,
  "isUnpatrolled": false,
  "metroCode": null,
  "namespace": "User",
  "page": "User:WP 1.0 bot/Tables/Project/Pubs",
  "regionIsoCode": null,
  "regionName": null,
  "user": "WP 1.0 bot",
  "delta": 121,
  "added": 121,
  "deleted": 0
}
```

一旦我们对数据维度有了了解，我们需要看看我们可以从这些数据中回答哪些类型的问题。这些问题是我们可以轻松获得的见解。稍后，我们可以用最适合我们的图形形式来表示这些见解。

让我们看看我们可以从这些数据中回答的一些问题。

**一维见解**：

+   哪些城市进行了修改？

+   哪些页面被修改了？

+   哪些国家进行了修改？

+   创建了多少个新页面？

**维度上的计数**：

+   从每个城市进行了多少次修改？

+   哪些是进行修改的前十大城市？

+   哪些是贡献修改的前十大用户？

+   哪些命名空间被频繁修改？

**多维见解**：

+   在所有国家，上午 9:00 到 10:00 之间有多少次修改？

+   机器人在何时进行编辑？

+   哪个国家有最多的机器人目标修改来源，以及何时？

看起来很有趣，对吧？我们为什么不尝试使用 Apache Superset 创建一个包含这些见解的仪表板呢？

为了做到这一点，我们需要在 Superset 应用程序中遵循这个简单的流程：

1.  数据源：

    +   从支持的数据库中定义新的数据源

    +   刷新 Apache Druid 数据源

1.  创建切片

1.  使用切片制作仪表板

如果我们回想一下，我们已经在之前的章节中完成了*步骤 1*。因此，我们可以直接进行第二步和第三步。

# 使用维基百科数据创建 Superset 切片

让我们看看我们可以使用 Superset 应用程序中的切片功能生成哪些类型的图形。

# 唯一用户计数

在这个切片中，我们看到如何生成图形以找到对数据集中的编辑做出贡献的唯一用户。

首先，我们需要从顶部导航转到切片页面。之后，屏幕看起来像这样：

![](img/178cfd0e-a1c9-4260-80b8-b8e5e6b76a58.png)

从这个页面，点击加号图标（+）添加一个新的切片：

![](img/35366e2f-997f-4366-84b8-3755677d28fc.png)

之后，我们看到系统配置的数据源列表。我们必须点击数据源名称：

![图片](img/9667e719-7831-48fc-9d87-4667320f76de.png)

点击 wikiticker 后，我们将被带到可视化页面，在那里我们定义想要渲染为图形的维度。

对于当前的使用场景，让我们从 UI 中选择以下选项：

| **UI 位置** | **图形** | **说明** |
| --- | --- | --- |
| 侧边栏 | ![图片](img/dfd162c5-4692-4ee4-865a-b17419505a33.png) | 将**数据源**选择为[druid-ambari].[wikiticker]，并将图形类型选择为“Big Number*”。在时间部分，选择“since”为 5 年前，并将其余值保留为默认值。在度量部分，从自动完成中选择 COUNT(DISTINCT user_unique)。在子标题部分，添加“Unique User Count”，这是在屏幕上显示的。之后，点击顶部的查询按钮。 |
| 图形输出 | ![图片](img/800cad03-8066-4307-9252-a685b878591e.png) | 我们在这个图形中看到查询的结果。 |
| 保存切片 | ![图片](img/798a6ed3-139e-4199-9f06-951316752e77.png) | 点击顶部的“另存为”按钮将显示一个弹出窗口，如上图所示，我们需要添加相应的值。将切片保存为`Unique Users`并将其添加到名为“我的仪表板 1”的新仪表板中。 |

听起来很简单，对吧？让我们不要急于查看仪表板。让我们在接下来的几节中创建更多来自数据的分析。

# 美国主要地区的词云

在本节中，我们将学习如何为在 Druid 数据源中为 Wikipedia 编辑做出贡献的美国主要地区构建词云。我们可以继续编辑上一节中的相同切片，或者像上一节中提到的，进入一个空白切片。

让我们集中选择生成词云所需的价值：

| **UI 位置** | **图形** | **说明** |
| --- | --- | --- |
| 侧边栏 | ![图片](img/0ad3f384-ff61-47a6-aa8e-db2a4eae0ef6.png) | 将数据源选择为[druid-ambari].[wikiticker]，并将图形类型选择为“Word Cloud”。在时间部分，选择“Since”为 5 年前，并将其余值保留为默认值。 |
|  | ![图片](img/4c1ceee1-5973-450a-972b-4988d1920fc8.png) | 在系列部分，从下拉菜单中选择 regionName。在度量中，选择 COUNT(*)，这是总编辑数。 |
|  | ![图片](img/c8882e1f-fd08-4647-b59d-91b4b4463747.png) | 在过滤器部分，选择 countryIsoCode；它应该是 US。添加另一个过滤器以仅选择有效区域（跳过 null 代码）。将值添加到图形中所示的位置。 |
| 图形输出 | ![图片](img/92a36d97-f891-40f6-b609-7846c7ede83b.png) | 点击查询后，我们看到这个漂亮的词云。 |
| 保存切片 | ![](img/690e8348-71ec-4532-9bf3-586060cd9bbf.png) | 点击顶部的“另存为”按钮将显示一个类似这样的弹出窗口，其中我们需要添加相应的值。将切片保存为`Word Cloud - Top US Regions`并将其添加到名为“我的仪表板 1”的新仪表板中*。* |

词云的重要性在于我们可以根据它们的相对大小看到顶级单词。这种类型的可视化在想要看到相对重要性的单词较少时非常有帮助。

让我们尝试从数据中生成另一个图形。

# 太阳花图 – 前 10 个城市

在本节中，我们将了解本章中尚未见过的另一种类型的图表。但首先，让我们提出用例。

我们想要找到每个渠道、城市名称和 namespaces 的所有三个级别的唯一用户；也就是说，图形应该能够显示给我们：

+   每个渠道的唯一用户

+   每个渠道/城市名称的唯一用户

+   每个渠道/城市名称/namespace 的唯一用户

为了显示这种层次结构数据，我们可以使用太阳花图。

让我们检查一下我们需要选择什么类型的值来渲染这种图表：

| **UI 位置** | **图形** | **说明** |
| --- | --- | --- |
| 侧边栏 | ![](img/0329cb06-5945-445b-8893-b5e9f2f69255.png) | 将数据源选择为[druid-ambari].[wikiticker]，图形类型选择为 Sunburst。在时间部分，选择“自 5 年前”的值，并将其余值保留为默认值。 |
|  | ![](img/c4970db4-b189-470b-9e03-fa0488452a60.png) | 在层次结构部分，从下拉菜单中选择`channel`、`cityName`和`namespace`。在主要指标和次要指标中，选择 COUNT(DISTINCT user_unique)，这是总用户数。 |
|  | ![](img/11a6a4bc-0a09-4e22-b301-2d32bab56027.png) | 在**过滤器**部分，选择`cityName`并使用正则表达式添加非空条件 |
|  | ![](img/e6b4b21e-6e88-43fa-a7b8-d49c731e309e.png) | 点击顶部的“另存为”按钮将显示一个类似这样的弹出窗口。我们需要在这里添加相应的值。将切片保存为`Sunburst - Top 10 Cities`并将其添加到名为“我的仪表板 1”的新仪表板中*。* |
| 图形输出 | ![](img/fa393586-c43c-442b-8b17-0a69a61a1835.png) | 点击查询后，我们看到这个漂亮的图形。 |

如我们所见，图形中有三个同心环：

+   最内层环是`channel`维度

+   中间环显示`cityName`维度

+   最外层环是`namespace`维度

当我们悬停在最内层环上时，我们可以看到它如何向外扩展到最外层圆圈。其他环也会发生同样的事情。

这种类型的图形在我们想要对我们的数据进行漏斗分析时非常有帮助。让我们在下一节中看看另一种分析类型。

# 通过有向力布局显示前 50 个渠道和 namespaces

**定向力布局**（**DFL**）是一种点与点相互连接的网络布局。由于它是一种力布局，我们可以看到点在屏幕上移动，因为`d3.js`应用了物理引擎。

在这个网络图中，我们想了解在唯一用户度量指标下，命名空间和频道之间的连接性。由于这是一个网络图，我们将看到节点在不同的路径中重复。

让我们看看我们如何到达这个图：

| **UI 位置** | **图形** | **说明** |
| --- | --- | --- |
| 侧边栏 | ![](img/d0bacbf9-fbe8-4b9a-868d-afb3fead3ed0.png) | 选择数据源为[druid-ambari].[wikiticker]，图形类型为定向力布局*. *在时间部分，选择 since 的值为 5 年前，并将其余值保留为默认值。 |
|  | ![](img/e5b3cdcf-eebf-45d9-a9b0-89d3ed63f1e8.png) | 在源/目标部分，从下拉菜单中选择`channel`和`namespace`。在度量部分，选择 COUNT(DISTINCT user_unique)，这是总用户数。我们将行限制设置为 50，这样我们只会看到前 50 项。*.* |
|  | ![](img/540474de-0c35-4f68-b0f7-3004dfc034c1.png) | 点击顶部的“另存为”按钮将显示一个类似这样的弹出窗口，其中我们需要添加相应的值。将切片保存为`DFL - Top 50 Channels & Namespaces`。将其添加到名为`My Dashboard 1`的新仪表板中。 |
| 图形输出 | ![](img/a2c1210d-ffd7-48ac-896f-a7f1ed5dc218.png) | 点击查询后，我们看到这个漂亮的图形。 |

随意拖动图形中的节点，以了解更多它们之间是如何相互连接的。节点的大小表示唯一用户数及其分解（类似于太阳风图）。

让我们在下一节花些时间学习另一个可视化和业务用例。

# 前 25 个国家/频道分布

现在我们将学习桑基图，这是一种类似于瀑布的方式，用于表示数据之间的分解和互联性。在这种情况下，我们想找出当涉及到唯一用户度量时，channelName 和 countryName 维度是如何相关的：

| **UI 位置** | **图形** | **说明** |
| --- | --- | --- |
| 侧边栏 | ![](img/b7bc7cbf-4167-49e8-b6e4-758a9813b6e1.png) | 选择**数据源**为[druid-ambari].[wikiticker]，**图形类型**为桑基图。在**时间**部分，选择 Since 的值为 5 年前，并将其余值保留为默认值。 |
|  | ![](img/c11d0b06-6547-4392-b8d3-a1aa54e98eb0.png) | 在**源/目标**部分，从下拉菜单中选择`channel`和`countryName`。在度量中，选择 COUNT(*)，这是总编辑数。将行限制保持在 25；这样我们只会看到前 25 项。*.* |
|  | ![](img/1eaf35b3-357d-4876-a6b9-cb61601ac660.png) | 在**过滤器**部分，选择 countryName 并启用正则表达式过滤器，以便只选择具有有效国家名称的记录。 |
|  | ![图片](img/42a3d617-7a02-4545-8ef9-674f23fce62b.png) | 点击顶部的“另存为”按钮将显示一个弹出窗口。我们需要在这里添加相应的值。将切片保存为“前 25 个国家/频道分布”并将其添加到名为“我的仪表板 1”的新仪表板中。 |
| 图形输出 | ![图片](img/0efcc190-641e-49dc-946c-0bddf1cba3fe.png) | 点击查询后，我们看到这个漂亮的图形。 |

这就完成了到目前为止我们可以生成的所有分析列表。现在在下一节中，我们将看到如何在仪表板中使用这些信息（这本来就是我们的目标）。

# 从切片创建维基百科编辑仪表板

到目前为止，我们已经看到了如何在 Apache Superset 应用程序中创建存储在 Apache Druid 数据库中的维基百科编辑数据的切片。现在是时候看看如何创建一个仪表板，这样我们就可以与业务团队或任何其他我们希望分享洞察力的团队共享它。

在这个过程中，第一步是点击顶部导航栏上的仪表板菜单。这将带我们到“添加新仪表板”页面，在那里我们需要填写以下详细信息。

| **元素** | **描述** | **值** |
| --- | --- | --- |
| 标题 | 我们想要创建的仪表板名称 | 我的仪表板 1 |
| 别名 | 仪表板的简短别名 | dash1 |
| 切片 | 我们想要添加到仪表板中的切片列表。 |

1.  太阳花图 - 前 10 大城市

1.  DFL - 前 50 个频道与命名空间

1.  前 25 个国家/频道贡献

1.  词云 - 前 25 个美国地区

1.  独立用户

|

| 其他字段 | 我们可以留其他字段为空，因为它们不是创建仪表板的必需项 |  |
| --- | --- | --- |

这是本页的图形：

![图片](img/91059ba0-71e3-47ab-adf9-6752f0d7b4a4.png)

当屏幕底部的更改看起来不错时，点击保存按钮。

这将带我们到下一步，我们可以看到仪表板已成功创建：

![图片](img/33f7bb5a-fc17-4a85-977e-bc84f3c69250.png)

我们可以在仪表板列表中看到“我的仪表板 1”。为了访问此仪表板，点击它，我们将被带到仪表板屏幕：

![图片](img/4f258abd-6893-4f4d-9e83-bfa0e4cab0eb.png)

如我们所见，我们有一种非常强大的方式来表示所有原始数据。这肯定会对最终用户产生影响，确保信息得到传达。

到目前为止，我们已经学习了如何从存储在 Apache Druid 列式数据库中的数据创建切片和仪表板。在下一节中，我们将看到如何连接到 RDBMS 并从该数据生成切片和仪表板。

# Apache Superset 与 RDBMS

Apache Superset 使用 Python 编程语言构建，并支持许多关系型数据库，因为它使用 SQLAlchemy 作为数据库驱动程序。这些驱动程序的安装超出了本节的范围。但是，安装它们应该非常简单。大多数情况下，操作系统供应商为我们打包它们。因此，我们不需要担心这些驱动程序的手动安装。

# 支持的数据库

这里列出了 Apache Superset 支持的一些数据库：

| **数据库名称** | **Python 包名称** | **驱动程序 URI 前缀** | **详细信息** |
| --- | --- | --- | --- |
| MySQL | `mysqlclient` | `mysql://` | Oracle MySQL 数据库 |
| PostgreSQL | `psycopg2` | `postgresql+psycopg2://` | 世界上最先进的开源数据库 |
| Presto | `pyhive` | `presto://` | 开源分布式查询引擎 |
| Oracle | `cx_Oracle` | `oracle://` | Oracle 公司创建的多模型数据库管理系统 |
| Sqlite |  | `sqlite://` | 快速、可扩展的嵌入式数据库库 |
| Redshift | `sqlalchemy-redshift` | `postgresql+psycopg2://` | 基于 PostgreSQL 构建的 Amazon Redshift 列式数据库 |
| MSSQL | `pymssql` | `mssql://` | Microsoft SQL Server |
| Impala | `impyla` | `impala://` | 在 Hadoop 上运行的 Apache Impala 大规模并行处理 SQL 引擎 |
| SparkSQL | `pyhive` | `jdbc+hive://` | Apache Spark 中的 SQL 编写模块 |
| Greenplum | `psycopg2` | `postgresql+psycopg2://` | 高级、功能齐全的开源数据平台 Greenplum |
| Athena | `PyAthenaJDBC` | `awsathena+jdbc://` | Amazon Athena 是无服务器交互式查询服务 |
| Vertica | `sqlalchemy-vertica-python` | `vertica+vertica_python://` | Vertica 是大数据分析软件 |
| ClickHouse | `sqlalchemy-clickhouse` | `clickhouse://` | 开源分布式、列式数据存储 |

上表的部分内容摘自 Apache Superset 的官方文档（[`superset.incubator.apache.org/installation.html#database-dependencies`](https://superset.incubator.apache.org/installation.html#database-dependencies)）

# 理解员工数据库

如果你还记得，在前面的章节中，我们已经导入了一个名为 Employees 的示例数据库，并将其加载到 MySQL 数据库中。我们将进一步深入研究这个示例数据存储，以便了解我们可以从这个数据存储中生成哪些类型的分析。

# 员工表

`employees` 表包含员工的详细信息（随机生成数据），以下属性如下：

| **列** | **数据类型** | **描述** |
| --- | --- | --- |
| `emp_no` | `INTEGER` | 员工编号 |
| `birth_date` | `DATE` | 员工出生日期 |
| `first_name` | `STRING` | 员工名字 |
| `last_name` | `STRING` | 员工姓氏 |
| `gender` | `STRING` | 员工性别，男性为 M，女性为 F |
| `hire_date` | `STRING` | 员工最新加入日期 |

# 部门表

`departments` 表包含组织中每个部门的基本详细信息。以下表格进一步解释了这一点：

| **表列** | **数据类型** | **描述** |
| --- | --- | --- |
| `dept_no` | `STRING` | 部门编号 |
| `dept_name` | `STRING` | 部门名称 |

# 部门经理表

`dept_manager` 表记录了担任特定部门经理的员工记录。更多详细信息请参阅此表：

| **表列** | **数据类型** | **描述** |
| --- | --- | --- |
| `emp_no` | `INT` | 担任此部门经理的员工 ID |
| `dept_no` | `STRING` | 部门 ID |
| `from_date` | `DATE` | 员工担任该部门经理的起始日期 |
| `to_date` | `DATE` | 员工担任该部门经理的结束日期 |

# 部门员工表

`dept_emp` 表包含所有记录，显示每位员工属于一个部门的时间长度。

| **表列** | **数据类型** | **描述** |
| --- | --- | --- |
| `emp_no` | `INT` | 员工 ID |
| `dept_no` | `STRING` | 部门 ID |
| `from_date` | `DATE` | 员工属于该部门的起始日期 |
| `to_date` | `DATE` | 员工在该部门的最后日期 |

# 职位表

`titles` 表包含从给定日期到结束日期的所有员工角色。更多详细信息如下所示：

| **表列** | **数据类型** | **描述** |
| --- | --- | --- |
| `emp_no` | `INT` | 员工 ID |
| `title` | `STRING` | 员工的职位 |
| `from_date` | `DATE` | 员工担任此角色的起始日期 |
| `to_date` | `DATE` | 员工扮演此角色的最后日期 |

# 薪资表

`salaries` 表包含特定员工的薪资历史。更多详细信息如下表所示：

| **表列** | **数据类型** | **描述** |
| --- | --- | --- |
| `emp_no` | `INT` | 员工 ID |
| `salary` | `INT` | 员工薪资 |
| `from_date` | `DATE` | 计算薪资的起始日 |
| `to_date` | `DATE` | 计算薪资的最后日期 |

# 标准化员工表

`employee_norm` 表包含来自员工、薪资、部门、`dept_emp` 和职位表的资料。让我们详细查看此表：

| **表列** | **数据类型** | **描述** |
| --- | --- | --- |
| `emp_no` | `INT` | 员工 ID |
| `birth_date` | `DATE` | 员工出生日期 |
| `full_name` | `STRING` | 员工全名 |
| `gender` | `STRING` | 员工性别 |
| `hire_date` | `DATE` | 员工加入日期 |
| `salary` | `INT` | 员工在该期间的薪资 |
| `from_date` | `DATE` | 薪资周期开始 |
| `to_date` | `DATE` | 薪资周期结束日期 |
| `dept_name` | `STRING` | 员工在薪资周期内工作的部门 |
| `title` | `STRING` | 此时间段内员工的职位 |

在了解了员工数据库中各种表的知识后，我们现在对已有的数据有了些了解。接下来的任务是找出我们可以从这些数据中生成哪些类型的分析。我们将在下一节中学习这一点。

# 员工数据库的 Superset 切片

一旦我们对存储在 MySQL 数据库中的数据类型有了基本的了解，我们现在将看到我们可以从这些数据中回答哪些类型的问题。

**一维洞察：**

+   组织中有多少名员工？

+   组织中为所有员工支付的总薪资是多少？

+   有多少个部门？

**多维洞察**

+   每年支付的总薪酬是多少？

+   每个部门的总薪酬是多少？

+   每年谁是最高薪酬的员工？

如果我们沿着这些思路思考，我们应该能够回答关于数据的一些非常重要的问题，并生成漂亮的图形。

让我们来看看在接下来的章节中我们可以生成哪些类型的可视化示例。

# 注册 MySQL 数据库/表格

在我们开始为员工表生成切片之前，我们首先应该注册它。注册过程包括以下步骤。

通过点击顶部导航栏中的“来源”菜单下的“数据库”下拉菜单来打开数据库，如下所示：

![图片](img/3ddfbcc9-a012-48e3-b287-5ba19ec18e8c.png)

在此之后，我们需要点击页面上的加号（+）图标：

![图片](img/cac387f0-9a8d-4256-ac05-21b6ec2616b1.png)

这将带我们到一个可以注册新数据库的页面。屏幕看起来如下所示：

![图片](img/32fa6696-267f-43c4-af46-86e6b86b2745.png)

我们将填写以下详细信息，如下所示。

| **字段名** | **值** | **描述** |  |
| --- | --- | --- | --- |
| 数据库 | `employees` | 我们想要注册的数据库名称。（输入与 MySQL 数据库中相同的名称） |  |
| SQLAlchemy URI | `mysql+pymysql://superset:superset@master:3306/employees` | 以编程方式访问此数据库的 URI。这将包括协议/驱动程序、用户名、密码、主机名和数据库名 |  |
| 其他字段 |  | 保持默认设置 |  |

在此之后，点击“保存”按钮，这将使用 Apache Superset 保存数据库详细信息。我们将被带到表格列表页面，如下所示：

![图片](img/96399df1-4b77-4313-84ed-669d011ddb85.png)

如我们所见，我们已将 MySQL 后端注册的员工数据库。

在下一步中，我们需要从顶部菜单中选择表格：

![图片](img/d624d6e5-a390-494e-9a63-9213dff650b5.png)

由于我们没有注册任何表格，我们将看到一个空页，如下所示：

![图片](img/55176447-b183-47af-b93c-c208d6ed065f.png)

为了注册一个新的表格，我们必须点击 UI 中的加号（图标），这将带我们到以下页面：

![图片](img/a54b67c3-3d25-4ca9-8727-71335bf1a2d2.png)

按照以下所示输入字段的值，完成后点击保存：

| **字段名** | **值** | **描述** |
| --- | --- | --- |
| 表名 | `employee_norm` | 我们想要注册的表格名称。 |
| 数据库 | `employees` | 选择已与 Superset 注册的数据库。 |

现在，我们可以看到表格已成功注册，如下面的屏幕截图所示：

![图片](img/81790eb0-b29e-4aa1-9927-35e994db7b02.png)

Superset 的重要特性之一是它会根据数据类型自动选择我们可以在表格列上执行的不同类型操作。这决定了我们在 UI 的其余部分显示哪些维度和度量。

为了选择这些选项，我们需要通过点击编辑图标来编辑表格，然后我们就会看到这个页面：

![图片](img/793aa239-22f3-4b32-8810-9b5bd6c9e791.png)

如我们所见，Apache Superset 自动识别了每个字段的数据类型，并且还为我们提供了选择这些维度用于各种活动的选项。这些活动在以下表格中列出：

| **活动** | **描述** |
| --- | --- |
| 可分组 | 如果复选框被选中，则该字段可以用作分组操作 (`GROUP BY` 在 SQL 中) 的一部分。 |
| 可筛选 | 如果复选框被选中，则该字段可以用作条件操作 (`WHERE` 子句) 的一部分。 |
| 计数唯一 | 如果复选框被选中，则该字段可以用作对字段进行计数 (`DISTINCT`) 操作的一部分。 |
| 总计 | 如果复选框被选中，则该字段可以用作 `SUM()` 函数的一部分。 |
| 最小/最大 | 表示该字段可以用作查找最小和最大值的操作的一部分。 |
| 是否时间维度 | 表示该字段是时间维度。 |

按照上述方式修改，然后点击保存按钮。

现在我们已经准备好在接下来的步骤中开始创建切片和仪表板。

# 切片和仪表板创建

正如我们在前面的章节中看到的，为了创建仪表板，我们首先需要创建切片。在本节中，我们将学习如何创建一些切片。

# 部门薪酬细分

在这个切片中，我们将学习如何创建一个可视化，它将显示每个部门的薪酬细分百分比：

| **UI 位置** | **图形** | **描述** |
| --- | --- | --- |
| 侧边栏 | ![图片](img/767c4c8e-67d2-4cb9-a705-2d344b10d236.png) | **数据源与图表类型**：选择 [employees].[employee_norm] 作为 **数据源**，分布 - NVD3 - 饼图作为图表类型。在 **时间** 部分，选择 birth_date 作为时间列，并选择 100 年前作为 **起始** 列。在 **度量** 部分，从下拉菜单中选择 sum_salary 作为值，并选择 dept_name 作为 **分组依据**。 |
| 图形输出 | ![图片](img/665462dd-38a7-469b-a8bd-ecfb036cec2e.png) | 点击查询按钮将渲染这个令人满意的图表。将其保存为“部门薪酬细分”。 |

就像在前面的章节中一样，看看创建一个看起来好的图形是多么容易，而且不需要任何编程知识。

在下一节中，我们将学习关于来自同一员工数据库的另一种类型的图形。

# 薪酬多样性

这是一个重要的图形，其中我们确定了组织历史中性别之间的薪酬多样性。在这里，我们以平均薪酬为基础进行分析。

| **UI 位置** | **图形** | **描述** |
| --- | --- | --- |
| 侧边栏 | ![图片](img/87174669-baa2-45d8-8df5-c407e6a5d63e.png) | **数据源与图表类型**：选择 [employees].[employee_norm] 作为数据源，时间序列 - 折线图作为图表类型。在时间部分，选择 birth_date 作为时间列，并选择 100 年前作为起始列。在度量部分，选择 avg_salary 作为度量，并选择 gender 作为 `分组依据`。 |
| 输出 | ![](img/c15c7256-324e-4b28-aea1-0c5935ffec73.png) | 展示每年按性别平均薪资的图形。以**薪资多样性**为标题保存此图形。 |

如我们从图形中看到的，薪资在性别之间分配均匀，并且非常接近。在此期间，平均薪资也有类似的增长。

在下一节中，我们将学习生成另一种类型的图形，它将为我们提供对数据的不同见解。

# 每年每个角色的薪资变化

这是一个重要的统计数据，我们想要找出在组织内部不同职位随年份变化的薪资变化情况。

| **UI 位置** | **图形** | **描述** |
| --- | --- | --- |
| 侧边栏 | ![](img/77e45d70-7738-40c3-9598-52ab46387de3.png) | 数据源与图表类型：选择[employees].[employee_norm]作为数据源，时间序列 - 百分比变化作为图表类型。在时间部分，选择 from_date 作为**时间**列，Year 作为**时间粒度**，100 years ago 作为**自**列。在**度量**部分，选择 sum_salary 作为**度量**，title 作为**分组**。 |
| 输出 | ![](img/e78d9c2b-5723-4430-9238-abfc62336bc1.png) | 点击查询，得到以下图形。以**每年每个角色的薪资变化**为名保存此图形。 |

从这个图形中，我们可以发现组织内部少数角色在总薪资方面存在很大的差异。

到目前为止，我们已经创建了三个切片，我们将使用迄今为止创建的切片创建一个新的仪表板。

# 仪表板创建

在这一步，我们将通过转到仪表板页面并点击添加仪表板图标（如前几节所示）来创建一个新的仪表板。

我们将看到以下屏幕，其中我们选择我们迄今为止创建的三个切片并点击保存：

![](img/66ae3349-1abb-46f8-a9c4-81e993e1b2ef.png)

一旦仪表板成功保存，我们可以看到如下所示：

![](img/b410d0cb-ad1e-4d4b-a76e-f519e88db039.png)

如我们所见，仪表板是一种非常强大的方式，可以以简单的方式表达大量数据。

# 摘要

在本章中，我们学习了数据可视化的概念以及它是如何帮助用户在没有任何底层数据知识的情况下接收所需信息的。然后，我们看到了不同的数据图形化展示方式。

我们了解了 Hadoop 应用，如 Apache Druid 和 Apache Superset，它们用于数据可视化，并学习了如何使用它们与如 MySQL 这样的关系数据库管理系统（RDBMS）。我们还看到了一个示例数据库，以帮助我们更好地理解应用。

在下一章中，我们将学习如何在云上构建我们的 Hadoop 集群。
