# 二、数据接收和输出模式

在前一章中，您已经了解了设计模式的高级概念，并看到了 Pig 是如何实现它们的。我们讨论了 Hadoop 和 Pig 的发展，传统系统的局限性，以及 Hadoop 和 Pig 如何与企业相关，以解决与大数据相关的具体问题。用一个现成的例子来解释 Pig 编程语言，说明了语言的特点。

我们将很快看到使用 Pig 设计模式在 Hadoop 中接收和输出各种数据的能力。本章的总体目标是作为 Hadoop 从业者的发射台，快速加载数据，尽快开始处理和分析数据，然后输出到其他系统，而不会陷入编写复杂 MapReduce 代码的迷宫。

首先，本章总结了大数据环境中常见的各种类型的数据及其来源。然后，我们讨论了几种将存储在各种源系统中的多结构数据导入 Hadoop 的数据入口设计模式。我们还将讨论将存储在 Hadoop 中的数据以本机格式导出到目标系统的数据导出设计模式。为了解释接收和输出模式，我们考虑了各种数据格式，如日志文件、图像、CSV、JSON 和 XML。用来说明这些模式的数据源是文件系统、大型机和 NoSQL 数据库。

# 数据接收和输出的上下文

**数据摄取** 是将数据输入系统进行后处理或存储的过程。这个过程包括连接到数据源，访问数据，然后将数据导入 Hadoop。导入意味着将外部来源的数据复制到 Hadoop 中，并将其存储在 HDFS。

**数据导出**是将数据处理后送出 Hadoop 环境的过程。输出数据的格式将与目标系统的格式相匹配。当下游系统使用数据来创建可视化、服务网络应用或网络服务或执行定制处理时，数据输出被执行。

随着 Hadoop 的出现，我们见证了以前所未有的规模快速高效地接收和输出数据的能力。企业正在根据其分析价值的需要，采用更新的示例来接收和输出数据。每一个输入企业的数据都有潜在的价值。这些提要主要由遗留企业数据、非结构化数据和外部数据组成。数据接收过程处理所有类型的提要，这些提要定期与现有的企业资产同步。数据导出处理限制出站数据，以满足集成下游系统的数据要求。

一旦数据在企业范围内，其对原始数据本身进行有意义分析的能力将得到增强，甚至在它转化为传统意义上更结构化的东西(即信息)之前。我们不再需要高度组织和结构化数据来从中收集见解。然而，随着新的老化算法的激增，任何类型或形式的数据都可以被分析。这最近导致了一种令人兴奋的商业模式，在这种模式下，企业突然发现自己正在从出于合规目的而存储多年的磁带中解压缩数据，以便发现隐藏的宝藏和价值。企业开始意识到，没有任何数据是可有可无或无用的，无论它看起来多么非结构化或无关紧要。他们已经开始争夺每一个可能给他们带来行业竞争优势的数据。

人们又对非结构化数据产生了兴趣:能否与企业现有的结构化数据源进行集成，这种集成能否通过预测性分析带来更好的业务结果。今天的数据科学家陶醉于异质性。探索非结构化数据是他们的新挑战，从随机分布中发现模式是新常态。在选定的云提供商上旋转几个 Hadoop 节点、导入数据和运行复杂的算法已经成为许多数据科学家的日常琐事。这使得处理来自许多不同来源的大量不同类型的数据变得相对简单。所有这些都强调了数据访问各种来源的重要性，这是最终价值的基础。本章我们将讨论*如何通过 PIG 的设计模式摄取和排出*，并给予*特殊的推动力*。

# 企业中的数据类型

下节详细介绍了以企业为中心的数据视图及其与大数据处理栈的关联，如下图所示:

![Types of data in the enterprise](img/5556OS_02_01.jpg)

企业中的数据多样性

以下是大量数据的各种类别的解释:

*   **遗留数据**:此数据类型包括来自所有遗留系统和应用的数据，包括在线或离线存储的结构化和半结构化数据格式。数据类型有许多用例——地震数据、飓风数据、人口普查数据、城市规划数据和社会经济数据。这些类型可以被吸收到 Hadoop 中，并与主数据相结合，以创建有趣且具有预测性的混搭。
*   **事务性(OLTP)数据**:来自事务性系统的数据传统上加载到数据仓库中。Hadoop 的出现解决了传统系统缺乏极致扩展性的问题。因此，事务性数据通常会被建模，以便在分析中不使用来自源系统的所有数据。Hadoop 可用于加载和分析整个 OLTP 数据，作为预处理或二次处理步骤。它还可以用来接收、输出和集成来自企业资源规划、大型机、供应链管理和客户关系管理系统的交易数据，以创建强大的数据产品。
*   **非结构化数据**:这种数据类型包括驻留在企业内容管理平台中的文档、笔记、备忘录和合同。这些企业系统旨在生产和存储内容，而无需大量的数据分析。Hadoop 通过发现基于上下文和用户定义的规则，为接收和处理内容提供了一个接口。内容处理的输出用于定义和设计分析，以探索使用语义技术来挖掘非结构化数据。
*   **视频**:很多企业已经开始使用基于视频的数据，获取安全监控、天气、媒体等相关用例的关键洞察。Hadoop 支持捕获视频中的组件，如内容、音频和相关元数据。在 Hadoop **企业数据仓库** ( **EDW** )中，将上下文化视频数据及关联元数据与结构化数据进行集成，进一步用先进的算法对数据进行处理。
*   **音频**:来自呼叫中心的数据包含了很多关于客户、比赛等类别的信息。虽然目前的数据仓库在处理和集成这种类型的数据方面有局限性，但 Hadoop 通过将其与仓库中现有的数据集成，无缝地吸收了这些数据。在 Hadoop 中，音频数据提取可以作为上下文数据和相关元数据进行处理和存储。
*   **影像**:静态的影像承载了大量的信息，对于政府机构(地理空间整合)、医疗(x 光和 CAT 扫描)等领域非常有用。在 Hadoop 中吸收这些数据，并将其与数据仓库集成，将通过分析洞察为大型企业带来好处，这将产生商业机会，而这些机会最初由于缺乏数据可用性或处理能力而不存在。
*   **数值/图案/图形**:此数据类型属于半结构化范畴。它包括地震数据、传感器数据、天气数据、股市数据、科学数据、射频识别、Hive 塔数据、车载计算机芯片、全球定位系统数据和流媒体视频。其他此类数据是出现的模式，或以周期性间隔重复列表的数字数据或图表。Hadoop 通过将结果与数据仓库相集成来帮助接收和处理这种类型的数据。这种处理将提供分析机会来执行相关性分析、聚类分析或贝叶斯类型分析，这将有助于识别收入泄漏、客户细分行为和业务风险建模中的机会。
*   **社交媒体数据** : 通常归类为脸书、LinkedIn 或 Twitter 数据，社交媒体数据不止这些渠道。这些数据可以从第三方聚合商处购买，如 DataSift、Gnip 和 Nielsen。在 Hadoop 中取这些类型的数据，并与结构化数据相结合，可以实现情感检测等各种社交网络分析应用。

在接下来的章节中，我们将研究大量用于处理上述数据类型的接收和输出模式。

# 多结构数据的接收输出方式

以下部分描述了接收非结构化数据(图像)和半结构化文本数据(Apache 日志和自定义日志)的具体设计模式。以下是该格式的简要概述:

*   **Apache 日志格式**:从该格式中提取信息是一个广泛使用的企业用例，并且是综合关联的。
*   **自定义日志格式**:此格式代表任何可以用正则表达式解析的日志。理解这种模式将有助于您将其扩展到许多其他需要编写定制加载器的类似用例中。
*   **图像格式**:这个是处理非文本数据的唯一模式，描述拍摄图像的模式可以调整，适用于任何类型的二进制数据。我们还将讨论图像输出模式，以说明使用 Pig 的可扩展性输出二进制数据的简单性。

## 摄入原木时的注意事项

日志存储取决于用例的特性。通常，在企业中，日志被存储、索引、处理并用于分析。MapReduce 的作用是索引和处理摄取的日志数据。处理后，必须存储在为日志索引实时查询提供读取性能的系统中。在本节中，我们将研究存储日志数据的各种选项，以提高实时读取性能:

*   一种选择是各种基于 SQL 的关系数据库。它们不适合为需要实时查询以获得洞察力的用例存储大量日志数据。
*   由于以下特点，NoSQL 数据库似乎是存储非结构化数据的好选择:
    *   文档(如 CouchDB 和 MongoDB)将数据存储在文档中，其中每个文档可以包含可变数量的字段或模式。在日志处理的情况下，模式通常是预先确定的，不会如此频繁地改变。因此，文档数据库可以用在模式灵活性(具有不同模式的日志)是主要标准的用例中。
    *   面向列的数据库，如 HBase 和 Cassandra，将密切相关的数据存储在列中，这是可扩展的。这些数据库非常适合分布式存储，并且注重性能。这些在读取操作和计算一组列时非常有效。然而，与此同时，这些数据库不如其他 NoSQL 数据库灵活。在存储数据之前，必须事先确定数据库结构。日志文件处理的最常见用例可以在面向列的数据库中实现。
    *   GraphLab 和 Secondary 等图形数据库是适用于日志文件处理的 Neo4j，因为日志不能表示为图形的节点或顶点。
    *   像 SimpleDB 这样的键值数据库存储可以由键访问的值。当数据库方案灵活且需要频繁访问数据时，键值数据库运行良好。理想情况下，这些数据库不适合在一段时间内模式没有明显变化的日志文件处理。

考虑到上述特点，最好的做法是选择柱状数据库的性能和分发能力，而不是键值模式和文档数据库对日志文件存储和处理的灵活性。帮助做出更好决策的另一个重要标准是选择具有良好读取性能而不是良好写入性能的柱状数据库，因为必须读取和聚合数百万个日志才能进行分析。

根据所有标准，企业已经成功实施了使用 HBase 作为首选数据库的日志分析平台。

### Apache 日志摄取模式

日志摄取模式描述了如何使用 pig Latin 将 Apache 日志摄取到 Hadoop 文件系统中，以在数据管道上进一步处理它们。

我们将讨论 Apache 日志与企业的相关性，并了解各种日志格式，每种格式的差异，以及日志结合 Pig 的用例。您还将了解 Pig 如何使摄取这些日志比在 MapReduce 中编程容易得多。

下面关于这个模式的实现级细节的讨论旨在让您熟悉重要的概念和适用的替代方案。一个示例代码片段用于从 Pig 语言的角度更好地理解模式，然后是使用模式的结果。

### 背景

Apache 服务器日志用于服务器运行状态的一般跟踪和监控。Apache 网络服务器创建 Apache 日志，并将它们存储在本地存储中。这些日志通过 Apache Flume 框架定期移动到 Hadoop 集群中，分布在 Cloudera 等主要 Hadoop 发行版中，以便存储在 Hadoop 文件系统中。

以下是 Flume 的简要介绍:

*   Flume 是一个分布式的、可靠的生产者-消费者系统，用于将大量日志(配置后自动)移动到 Hadoop 进行处理。
*   接收器代理运行在 web 服务器(生产者)上。
*   代理使用收集器(使用者)定期收集日志数据。
*   代理将日志数据推送到目标文件系统 HDFS。

下图描述了此体系结构的快照:

![Background](img/5556OS_02_02.jpg)

典型测井采集

### 动机

分析日志数据了解并跟踪任何应用或 web 服务的行为。它包含大量关于应用及其用户的信息，这些信息被聚合以发现模式、错误或次优用户体验，从而将*不可见的*日志数据转化为有用的性能洞察。这些见解以特定用例的形式在整个企业中使用，从产品支持到工程和营销，提供运营和商业智能。

一个计算机集群包含许多独立的服务器，每个服务器都有自己的日志工具。这使得服务器管理员很难分析整个集群的整体性能。将每台服务器的日志文件合并成一个日志文件对于获取有关群集性能的信息非常有用。组合日志文件可以直观地显示集群的性能，并在短时间内检测集群中的问题。然而，将集群的服务器日志存储几天将产生几千兆字节的数据集。分析如此大量的数据需要大量的处理能力和内存。像 Hadoop 这样的分布式系统最适合这种处理能力和内存。

日志大小可以增长到数百 GB。Hadoop 吸收这些文件进行进一步分析，并考虑各种维度，如时间、源地理和浏览器类型，以提取模式和重要信息。

### 用例

该设计模式可用于以下用例:

*   找到链接到网站的用户。
*   查找网站访问者和独特用户的数量。这可以跨空间和时间维度完成。
*   在时间和空间上找出峰值负载时间。
*   分析机器人和蠕虫的访问。
*   查找与站点性能相关的统计数据。
*   分析服务器的响应和请求，深刻理解 web 服务器问题的根源。
*   分析用户对网站的哪个页面或哪个部分更感兴趣。

### 模式实现

通过使用 piggybank 的`ApacheCommonLogLoader`和`ApacheCombinedLogLoader`类，在 Pig 中实现了 Apache 访问日志摄取模式。这些功能扩展了 PIG 的`LoadFunc`水平。

### 代码片段

以下示例中使用的两种不同类型的日志是名为`access_log_Jul95`的**通用日志格式**和名为`access.log`的**组合日志格式**。在企业设置中，这些日志是使用生成日志的 web 服务器上的 Flume 代理提取的。

下表描述了每种类型的组成属性:

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

属性

 | 

通用日志格式

 | 

组合日志格式

 |
| --- | --- | --- |
| 国际计算机的互联网地址 | 是 | 是 |
| 用户标识 | 是 | 是 |
| 请求时间 | 是 | 是 |
| 请求文本 | 是 | 是 |
| 状态代码 | 是 | 是 |
| 字节大小 | 是 | 是 |
| 参考文献 |   | 是 |
| HTTP 代理 |   | 是 |

#### Code of common log loader class

以下 Pig 脚本解释了如何使用`CommonLogLoader`类将`access_log_Jul95`日志文件摄取到 Pig 关系日志中:

```sh
/*
Register the piggybank jar file to be able to use the UDFs in it
*/
REGISTER '/usr/share/pig/contrib/piggybank/java/piggybank.jar';

/*
Assign the aliases ApacheCommonLogLoader and DayExtractor to piggybank's CommonLogLoader and DateExtractor UDFs
*/
DEFINE ApacheCommonLogLoader org.apache.pig.piggybank.storage.apachelog.CommonLogLoader();
DEFINE DayExtractor org.apache.pig.piggybank.evaluation.util.apachelogparser.DateExtractor('yyyy-MM-dd');

/*
Load the logs dataset using the alias ApacheCommonLogLoader into the relation logs
*/
logs = LOAD '/user/cloudera/pdp/datasets/logs/access_log_Jul95' USING ApacheCommonLogLoader
    AS (addr: chararray, logname: chararray, user: chararray, time: chararray,
    method: chararray, uri: chararray, proto: chararray,
    status: int, bytes: int);
/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Display the contents of the relation logs on the console
*/
DUMP logs;
```

#### Code of combined loader class

以下 PIG 脚本解释了如何使用`CombinedLogLoader`类将`access.log`文件摄取到 PIG 关系日志中:

```sh
/*
Register the piggybank jar file to be able to use the UDFs in it
*/
REGISTER '/usr/share/pig/contrib/piggybank/java/piggybank.jar';

/*
Load the logs dataset using piggybank's CombinedLogLoader into the relation logs
*/
logs = LOAD '/user/cloudera/pdp/datasets/logs/access.log'
  USING org.apache.pig.piggybank.storage.apachelog.CombinedLogLoader()
  AS (addr: chararray, logname: chararray, user: chararray, time: chararray,
    method: chararray, uri: chararray, proto: chararray,
    status: int, bytes: int,
    referer: chararray, useragent: chararray);
/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

-- Display the contents of the relation logs on the console
DUMP logs;
```

### 结果

使用这种模式的结果是，来自 Apache 日志文件的数据存储在一个包中。以下是用无效值存储 Pig 关系的几种方法:

*   如果日志文件包含无效数据，空值将存储在包中。
*   如果在`AS`子句之后的模式中没有定义数据类型，那么在包中所有的列都将默认为`bytearray`。Pig 稍后将根据数据使用的上下文执行转换。有时需要显式键入列，以减少分析时间错误。

### 附加信息

*   [http://pig .Apache。组织/文档/r 0 .11.0/API/org/Apache/pig/PIG 库/存储/Apache 日志/组合日志加载器。html](http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/piggybank/storage/apachelog/CombinedLogLoader.html)
*   [http://pig .Apache。组织/文档/r 0 .11.0/API/org/Apache/pig/PIG 库/存储/Apache 日志/通用日志加载器。html](http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/piggybank/storage/apachelog/CommonLogLoader.html)
*   [http://pig .Apache。组织/文档/r 0 .11.0/API/org/Apache/pig/pig bank/evaluation/util/Apache gpar ser/package-summary .html](http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/piggybank/evaluation/util/apachelogparser/package-summary.html)

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `chapter2/code/`
*   `chapter2/datasets/`

## 自定义日志摄取模式

用户自定义日志摄取模式描述如何使用 pig Latin 将任意类型的日志摄取到 Hadoop 文件系统中，以进一步处理数据管道上的日志。

我们将讨论定制日志在企业中的相关性，并了解这些日志是如何生成并传输到 Hadoop 集群的，以及日志与 Pig 相结合的用例。您还将了解 Pig 如何使摄取这些日志比在 MapReduce 中编程容易得多。

下面关于这个模式的实现级细节的讨论旨在让您熟悉重要的概念和适用的替代方案。一个示例代码片段用于从 Pig 语言的角度更好地理解模式，然后是使用模式的结果。

### 背景

大多数日志使用特定的约定来划分组件字段，类似于 CSV 文件，但也有我们遇到文本文件的情况，这些文件没有正确分隔(通过制表符或逗号)。这些日志在分析前需要清理。在到达 HDFS 之前，数据可以通过 Flume、Chukwa 或 Scribe 进行清理，并以 Pig 或 Hive 易于处理的格式进行存储，以便进行分析。如果数据以未清理的格式存储在 HDFS，您可以编写一个 Pig 脚本来清理数据，并将其加载到 Hive 或 HBase 中进行分析，以备后用。

### 动机

Pig 在处理非结构化数据方面的声誉源于其对具有部分或未知模式的数据的本地支持。加载数据时，可以选择指定模式，也可以指定加载后的模式。这与其他系统(如 Hive)形成鲜明对比，在 Hive 中，您必须在加载之前强制实施模式。

当文本数据尚未标准化和格式化时，使用此模式。

### 用例

以下是可以应用这种模式的一般用例:

*   接收任何没有明确定义模式的文本或日志文件。
*   获取文本或日志文件，并根据分析的适用性实验性地找出可以强加给它们的模式。

### 模式实现

利用 piggybank 的`TextLoader`功能，在 Pig 中实现了非结构化文本的摄入模式。这些函数继承了`LoadFunc`类。

Pig 有一个很好的特性，如果没有显式指定模式，它可以解释数据的类型。在这种情况下，字段被设置为默认的`bytearray`类型，然后根据下一条语句中数据的用法和上下文推断出正确的类型。

Piggybank 的`TextLoader`功能可以加载文本文件，将它们拆分成新的行，并将每一行加载到 Piggybank 元组中。如果模式是用`AS`子句指定的，那么每个元组都被认为是`chararray`。如果省略号子句没有指定模式，则结果元组将没有该模式。

同样，使用指定的正则表达式模式过滤行后，可以使用`MyRegexLoader`类加载文件内容。可以使用`MyRegExLoader`指定正则表达式格式。如果一个模式作为参数传递给它，这个函数返回一个匹配的正则表达式`chararray`。

### 代码片段

在这个用例中，我们说明了应用日志文件的摄取，并通过分析请求/响应模式来帮助识别 web 服务器中潜在的性能问题。我们将使用`sample_log.1`数据集来计算每个服务的平均响应时间。日志文件包含嵌入的事件日志以及 web 应用生成的 web 服务请求和响应信息。格式如下面的代码所示。这里，我们只对提取请求-响应对感兴趣，忽略了与 info、DEBUG 和 ERRORS 相关的事件信息:

```sh
/* other unstructured event logs related to INFO, DEBUG, and ERROR logs are depicted here */
Request <serviceName> <requestID> <Timestamp>
Response <serviceName> <requestID> <Timestamp>
/* other unstructured event logs related to INFO, DEBUG, and ERROR logs are depicted here */
```

下面的代码片段显示了使用`MyRegexLoader`加载与指定正则表达式匹配的行:

```sh
/*
Register the piggybank jar file to be able to use the UDFs in it
*/
REGISTER '/usr/share/pig/contrib/piggybank/java/piggybank.jar';

/*
Load the logs dataset using piggybank's MyRegExLoader into the relation logs.
MyRegexLoader loads only the lines that match the specified regex format
*/
logs = LOAD '/user/cloudera/pdp/datasets/logs/sample_log.1'
  USING org.apache.pig.piggybank.storage.MyRegExLoader(
    '(Request|Response)(\\s+\\w+)(\\s+\\d+)(\\s+\\d\\d/\\d\\d/\\d\\d\\s+\\d\\d:\\d\\d:\\d\\d:\\d\\d\\d\\s+CST)')
    AS (type:chararray, service_name:chararray, req_id:chararray, datetime:chararray);

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

-- Display the contents of the relation logs on the console
DUMP logs;
```

进一步处理将在提取的日志上完成，以计算每个服务的平均响应时间并识别潜在的性能问题。

以下代码片段显示了使用`TextLoader`加载自定义日志:

```sh
/*
Load the logs dataset using TextLoader into the relation logs
*/
logs = LOAD '/user/cloudera/pdp/datasets/logs/sample_log.1' USING TextLoader  AS (line:chararray);

/*
The lines matching the regular expression are stored in parsed_logs.
FILTER function filters the records that do not match the pattern
*/
parsed_logs = FILTER logs BY $0 MATCHES '(Request|Response)(\\s+\\w+)(\\s+\\d+)(\\s+\\d\\d/\\d\\d/\\d\\d\\s+\\d\\d:\\d\\d:\\d\\d:\\d\\d\\d\\s+CST)';

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

-- Display the contents of the relation parsed_logs on the console
DUMP parsed_logs;
```

### 结果

使用此模式的结果是，日志文件中的数据存储在包中。该袋用于后续分析步骤。

以下是用无效值存储 Pig 关系的几种方法:

*   如果日志文件包含无效数据，空值将存储在包中。
*   如果在`AS`子句之后的模式中没有定义数据类型，那么在包中所有的列都将默认为`bytearray`。Pig 稍后将根据使用数据的上下文执行转换。有时需要显式键入列，以减少分析时间错误。
    *   您可以在`AS`子句后定义数据类型，但是在为每列定义适当的数据类型时必须小心。当无法进行数据类型转换时，包中会存储空值，例如`chararray`类型强制转换为`int`类型，导致空值。但是`int`可以铸造成`chararray`。(例如`int 27`可以打成`chararray "27"`。)
*   您必须特别注意可能导致关系中出现空值的数据。像`COUNT`这样的关系运算符会忽略空值，但是`COUNT_STAR`函数不会忽略它，而是将空值视为有价值的值。

### 附加信息

*   [http://pig .Apache。组织/文档/r 0 .11.0/API/org/Apache/PIG 库/仓储/my regex 加载程序 html](http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/piggybank/storage/MyRegExLoader.html)
*   [http://pig.apache.org/docs/r0.11.1/func.html#textloader](http://pig.apache.org/docs/r0.11.1/func.html#textloader)

本节的完整代码和数据集可以在以下 GitHub 目录中找到:

*   `chapter2/code/`
*   `chapter2/datasets/`

## 图像输入和输出模式

本节设计模式描述了如何使用 Pig Latin 在 Hadoop 文件系统中捕获并输出一组图像，以进一步处理数据管道中的图像。

我们将讨论图像在企业中的相关性，并了解创建和存储图像的各种方法，收集要在 Hadoop 上处理的图像的最佳方式，以及将图像与 Pig 相结合的用例。您还将学习 Pig 如何使用用 Java 编写的自定义 UDF 轻松捕获这些图像。

下面关于这个模式的实现级细节的讨论旨在让您熟悉重要的概念和适用的替代方案。一个示例代码片段用于从 Pig 语言的角度更好地理解模式，然后是使用模式的结果。

### 背景

Hadoop 有许多处理结构化数据和非结构化文本数据的记录用例。然而，我们有证据表明，许多情况下使用 Hadoop 的真正力量来处理其他形式的非结构化数据，如图像、视频和声音文件。

在图像处理方面，Hadoop 在分析气象/军事/民用卫星拍摄的图像时，即当图像尺寸大、分辨率高，需要由一组服务器进行处理时，起到了关键作用。

Hadoop 为大图像或一组小图像提供了有效的存储机制。不同于在 RDBMS 中将图像存储为 BLOB 的过程，无法通过 SQL 进行大规模、有意义的分析，可以在 Hadoop 上编写特定的图像处理算法，可以处理单个图像和一组图像，并行进行高端的图像分析。

### 动机

这种模式适合在 Hadoop 中加载和处理大量的图像文件。将图像加载到数据管道中是由一个用 Java 编写的 UDF 完成的。

![Motivation](img/5556OS_02_03.jpg)

图像入口和出口

获取非结构化图像数据，并将其与提供上下文信息的结构化图像元数据(如标签、EXIF 信息和对象标签)相结合，导致社交媒体分析和其他领域(如安全、情报收集、天气预报和人脸识别)的更新和发展。这个想法可以扩展到更广泛的图像特征，使我们能够以革命性的方式检查和分析图像。

图像导入 Hadoop 后，原始图像的实际处理是一项复杂的任务，涉及原始像素级的多次计算。这些计算是由低级的 C 和 C++算法完成的。Hadoop 与这些算法集成，使用 Hadoop 流包装这些算法，并作为标准 Hadoop 作业工作。

图像输出模式试图展示一种简单的机制，其中作为序列文件存在于 HDFS 的二进制图像作为图像文件输出。这种模型的动机在于，基于 Hadoop 的图像处理算法可以对图像进行复杂的计算，以连接其组件来创建更大的图像。

Hadoop 通过将图像文件分组为少量的大文件而不是大量的小图像文件来有效地工作。使用大量小图像文件(其大小小于 64 MB 的 HDFS 块大小)可能会导致从磁盘读取大量数据并搜索名称节点，这可能会导致大量网络流量将这些文件从一个节点传输到另一个节点。这将导致无效的数据访问。这种设计模式探索了一种通过在 Hadoop 中将这些图像文件分组为序列文件来克服这种限制的方法。您可以扩展此模式来接收和处理其他类型的二进制文件，如声音和视频。

拍摄设计模式适用于作为图像文件的大语料库的一部分的图像，其中每个图像都是不同的，将它们组合在一起是不自然的。这种模式不适合在 Hadoop 集群的节点之间划分非常大的映像。上图显示了该架构的示意图。

### 用例

您可以考虑在以下用例中应用图像捕获模式作为预处理步骤:

*   捕获多个图像，以对每个图像应用各种类型的图像过滤器。
*   图像质量的批量增强
*   理解图像的内容，例如，应用基于人工智能的无监督计算机视觉算法从图像中提取道路、运河或建筑物等特征。
*   使用图像进行模式匹配，例如医学和地理信息系统图像。

您可以考虑在必须从图像中去除噪声的用例中应用图像导出模式。将原始图像加载到 Hadoop 中并进行处理以去除噪声；通过组合图像的多个块来创建新图像，并且返回被噪声过滤的图像。

### 模式实现

下一节介绍图像捕捉模式和图像输出模式的模式实现。

#### 影像门户的实现

使用自定义加载器函数在 Pig 中实现图像捕获模式，在 Java 中实现为 UDF 函数。这`ImageToSequenceFileUDF`将图像转换成序列文件。输入是图像目录的 HDFS 路径，输出是序列文件的路径。

序列文件将图像文件的内容存储为映射到文件名关键字的值。因为序列文件可以拆分，所以可以通过流式传输或使用 MapReduce 进行处理。MapReduce 在内部使用一个标签将文件划分成块大小的块，并独立操作。文件可以被许多编解码器压缩，块压缩用于最大限度地提高存储和检索效率。在这种情况下，将没有解压缩器来防止数据洗牌和带宽消耗。当序列文件中存储大量图像数据时，这将使 Hadoop 的可伸缩优势得以利用。

#### 图像导出的实现

Image 导出模式利用自定义存储功能在 Pig 中实现，在 Java 中实现为 UDF。这个`SequenceToImageStorage`类将序列文件转换成图像，并将其存储在磁盘上的指定位置。这个函数的输入是序列文件的路径。

### 代码片段

以下部分描述了图像捕获模式的代码，后面是图像退出。

#### 图像输入

为了解释这种模式是如何工作的，我们考虑一组存储在 HDFS Hadoop 文件系统可访问的文件夹中的图像文件。图像未经预处理；它们以原始格式(JPEG)存储。下面的代码有两个主要部分。首先是 Pig Latin 脚本，它加载包含 images 文件夹路径的文件，其次是用 Java 编写的自定义 UDF，它实际上在幕后工作，将一个图像或一组图像分解成序列文件。

##### PIG

以下是读取图像文件并将其转换为序列文件的 Pig 脚本:

```sh
/*
Register the custom loader imagelibrary.jar, it has UDFs to convert images to sequence file and sequence file to images
*/
REGISTER '/home/cloudera/pdp/jars/imagelibrary.jar';

/*
Load images_input file, it contains the path to images directory
*/
images_file_path = LOAD '/user/cloudera/pdp/datasets/images/images_input' AS (link:chararray);

/*
ImageToSequenceFileUDF converts multiple image files to a sequence file.
This ensures that there are no large number of small files on HDFS, instead multiple small images are converted into a single sequence file.
Another advantage of sequence file is that it is splittable.
The sequence file contains key value pairs, key is the image file name and value is the image binary data.
It returns the path of the sequence file.
*/
convert_to_seq = FOREACH images_file_path GENERATE com.mycustomudf.ImageToSequenceFileUDF();

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

-- Display the contents of the convert_to_seq on the console
DUMP convert_to_seq;
```

##### 图像到序列 UDF 片段

以下是`ImagetoSequenceFileUDF` 的 Java 代码片段，展示了从图像文件到序列文件的转换:

```sh
public static String createSequenceFile(Path inPutPath)
{
.
.

for(int i=0;i<status.length;i++)
  {
    //FSDataInputStream is opened at the given path
    dataInputStream = fileSystem.open(status[i].getPath());
    // extracting image name from the absolute path
    fileName = status[i].getPath().toString().
    substring(status[i].getPath().toString().
    lastIndexOf("/")+1);
    byte buffer[] = new byte[dataInputStream.available()];
    //buffer.remaining() bytes will be read into buffer.
    dataInputStream.read(buffer);
    /*Add a key/value pair. Key is the image filename and
      value is the BytesWritable object*/
    seqFileWriter.append(new Text(fileName),
    new BytesWritable(buffer));
    .
    .
  }
}
```

#### 图像退出

下一节描述图像退出的代码。

##### PIG

以下是将序列文件的内容输出到图像的 Pig 脚本:

```sh
/*
Register the custom jar, it has UDFs to convert images to sequence file and sequence file to images
*/
REGISTER '/home/cloudera/pdp/jars/imagelibrary.jar';

/*
Load images_input file, it contains the path to images directory
*/
images_file_path = LOAD '/user/cloudera/pdp/datasets/images/images_input' AS (link:chararray);

/*
ImageToSequenceFileUDF function converts multiple image files to a sequence file.
This ensures that there are no large number of small files on HDFS, instead multiple small images are converted into a single sequence file.
Another advantage of sequence file is that it is splittable.
The sequence file contains key value pairs, key is the image file name and value is the image binary data.
It returns the path of the sequence file.
*/
convert_to_seq = FOREACH images_file_path GENERATE com.mycustomudf.ImageToSequenceFileUDF();

/*
* Some processing logic goes here which is deliberately left out to improve readability.
* It is assumed that in-between the load and store steps, a user performs some image processing step such as stitching multiple image tiles together.
*/

/*
The custom UDF SequenceToImageStorage reads the sequence file and writes out images.
It reads each key/value pair and writes out the contents as images with keyname as the filename in the folder seq_to_img_output
*/
STORE convert_to_seq INTO '/user/cloudera/pdp/output/images/seq_to_img_output' USING com.mycustomudf.SequenceToImageStorage();
```

##### 序列对 UDF 进行成像

以下是自定义存储功能`SequenceToImageStorage`和的片段，用于读取序列文件并将内容写入图像文件:

```sh
@Override
public void putNext(Tuple tuples) throws IOException {
  .
  .
  // Do this for each key/value pair
  while (seqFilereader.next(key, value))
  {
    bufferString = value.toString().split(" ");
    buffer =new byte[bufferString.length];
    for(int i=0;i<bufferString.length;i++)
    {
    /*
      String parameter parsed as signed integer in the radix given by the second parameter
      */
      buffer[i] = (byte)
      Integer.parseInt(bufferString[i], 16);
    }
    /*
    output path of the image which is the path specified, key is the image name
    */
    outPutPath=new Path(location+"/"+key);
    // FSDataOutputStream will be created at the given Path.
    seqFileWriter = fileSystem.create(outPutPath);
    // All bytes in array are written to the output stream
    seqFileWriter.write(buffer);
  }
  .
  .
}
```

Hadoop API 的`SequenceFile.Reader`类用于读取序列文件，获取键值对。迭代键值对，然后为每个键值对创建一个带有键名的新文件，并将该值作为字节写入文件，从而生成多个图像文件。

### 结果

作为应用图像捕获模式的结果，图像语料库被 Java UDF 解析成序列文件。然后将每个序列文件分解为 RGB 值，并存储在 PIG 拉丁映射关系中。数据管道的下一阶段使用映射关系来进一步处理序列文件。

作为图像输出模式的结果，存储在 HDFS 的序列文件被转换成图像文件，使得上游图像显示系统可以使用这些图像。

### 附加信息

*   [http://pig .Apache。组织/文档/r 0 .11.0/API/org/Apache/pig/pig bank/storage/sequence fileloper .html](http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/piggybank/storage/SequenceFileLoader.html)
*   [http://pig。 Apache. Organization/document /r 0\. 11 .1 UDF。 Html # Load-Store-Function](http://pig.apache.org/docs/r0.11.1/udf.html#load-store-functions)

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `chapter2/code/`
*   `chapter2/datasets/`

# nosql 数据的导入和导出模式

本节描述了从两种类型的 NoSQL 数据中获取数据的模式。为了说明 Pig 随时支持 NoSQL 数据库的能力及其相关用例，我们选择了 MongoDB 等文档数据库和 HBase 等柱状数据库。

## MongoDB 出入口模式

MongoDB 导入导出模式描述了如何使用 Pig 拉丁语将 MongoDB 文档集合的内容存储在 Hadoop 文件系统(Pig 关系)中进行数据处理，然后将处理后的数据写回 MongoDB。

我们将讨论存储在 MongoDB 中的数据与企业之间的相关性，并了解访问 MongoDB 数据的各种方式、接收和输出的动机，以及 MongoDB 数据与 Pig 相结合的用例。您还将学习 Pig 如何比用 Java 编写的 MapReduce 代码更直观地接收和导出这些数据。

下面关于这个模式的实现级细节的讨论旨在让您熟悉重要的概念和适用的替代方案。示例代码片段可以从 Pig 语言的角度更好地理解模式和使用模式的结果。

### 背景

Godb 是一个 NoSQL 数据库，从头开始设计，以文档集合的形式存储数据，不同于 RDBMS 的行列。由于其广泛的索引功能和使用 JSON 与外部应用的集成，它具有很高的可扩展性，并使文档检索变得容易。MongoDB 非常灵活，可以处理可变模式。(集合中的每个文档不必具有相同的架构。)因为 MongoDB 将数据存储为文档，并且这些文档集合的几乎所有属性都被索引，所以作为处理实时查询的操作存储，它是一个非常有效的解决方案。与 Hadoop 不同，Hadoop 擅长离线批处理和聚合各种来源的数据。

### 动机

在典型的企业中，MongoDB 和 Hadoop 在某些场景下是集成的。在这些场景中，Hadoop 需要处理比 MongoDB 更极端的数据负载，以聚合数据并促进复杂的分析。

来自 MongoDB 的数据被吸收到 Hadoop 中，并用 MapReduce 作业进行处理。Hadoop 使用 Pig 数据管道将数据与来自其他企业来源的附加数据相结合，以开发多数据聚合解决方案。对 Pig 数据管道中的数据进行处理后，写回 MongoDB 进行具体的分析和查询。这确保了现有应用可以使用从 Hadoop 导出的数据来创建可视化或驱动其他系统。

在许多企业用例中，Hadoop 作为一个中央数据存储库，将数据与不同的数据存储集成在一起。在这种情况下，通过使用 MapReduce 作业，MongoDB 可以用作定期向 Hadoop 提供数据的数据源之一。一旦 MongoDB 数据被吸收到 Hadoop 中，合并后的较大数据集就会被处理，并可用于进一步查询它们。

MongoDB 也是**运营数据仓库** ( **ODS** )之一，连接其他数据仓库和数据仓库。获取分析见解包括从这些连接的数据源移动数据和执行 ETL。PIG 可以有效地用于这一目的。Hadoop 充当 ETL 中枢，从一个存储中提取数据，使用 MapReduce 作业执行各种转换，并将数据加载到另一个存储中。

### 注

应该注意的是，与已经加载/传输到 HDFS 的数据相比，直接从外部来源(MongoDB)获取的数据具有非常不同的操作性能特征。

### 用例

您可能要考虑在以下场景中使用数据导入导出模式，其中 MongoDB 和 Hadoop 的集成将获得丰厚的回报:

*   MongoDB 和 Hadoop 分别处理接近实时和批处理的不同工作负载。在您想要将负载卸载到 Hadoop 进行批处理的用例中，考虑使用门户设计模式，从而释放 MongoDB 中的资源。考虑使用导出设计模式，使 MongoDB 成为从 Hadoop 导出数据并支持实时查询操作的接收器。
*   MongoDB 本身有一个在 MongoDB 数据库上运行的 MapReduce 实现。然而，它比 Hadoop MapReduce 慢，因为它是用 JavaScript 实现的，而且它执行复杂分析的数据类型和库更少。考虑在需要将数据卸载到 Hadoop 的用例中使用门户设计模式，以利用 Hadoop 的库支持、机器学习、ETL 功能和处理规模。考虑使用导出设计模式将数据从 Hadoop 移动到 Mongo DB，并使用 MongoDB 的 MapReduce 实现。
*   MongoDB 支持很少的基本数据聚合函数来生成 SQL 风格的聚合，这就需要更高的学习曲线来理解聚合框架。如果您想使用 Hadoop 执行复杂的聚合任务，请考虑使用门户设计模式。
*   当存在大量非结构化数据并且需要 MongoDB 进行实时分析时，可以使用这种设计模式。在这种情况下，可以使用导入设计模式将 Hadoop 中摄取的原始数据创建一个结构，并使用导出设计模式将数据导出到 MongoDB 中，以促进 MongoDB 中的优化存储，从而进行实时查询和分析。

### 模式实现

下图显示了 MongoDB 连接器集成:

![Pattern implementation](img/5556OS_02_04.jpg)

MongoDB 连接器集成

您可以使用 Hadoop 的 MongoDB 连接器来集成 Hadoop 和 MongoDB。这个连接器有助于将数据从 MongoDB 转移到 Hadoop 生态系统，并允许通过其他编程语言(Hadoop 流)进行访问。连接器与 Pig 的集成如上图所示。

#### 进入实施

Pig 使用 `MongoLoader`函数将 MongoDB 数据加载到 Pig 的拉丁关系中。使用这个函数，数据直接从数据库加载。Pig 也可以使用`BSONLoader`功能读取 MongoDB 原生格式(BSON)。

`MongoLoader`函数可以在非模态模式下工作，无需指定字段名。在这种模式下，记录被解释为包含单个映射(文档)的元组。当您不知道 MongoDB 集合的模式时，这很有用。`MongoLoader`该功能也可以在模式模式下工作，在该模式下可以指定将 Pig 脚本中的字段与文档中的字段进行映射的字段名。

#### 出口变现

PIG 关系中的数据可以通过两种方式写入 MongoDB。第一种方法是使用`BSONStorage`函数在`.BSON`文件中存储一个关系，以后可以导入 MongoDB。这种方法的优点是以 MongoDB 的本地存储格式写入，具有高吞吐量。第二种方法使用 MongoDB 的包装器将连接到数据库，并使用`MongoStorage`函数将其直接写入数据库。该函数将在元组级别运行，并将它接收的每个元组存储到 MongoDB 中的相应文档中。在写之前，已经映射了 Pig 关系和 MongoDB 文档的模式。使用第二种方法将为您在记录或元组级别写入数据提供极大的灵活性，但它会影响输入/输出速度。

`MongoStorage`该函数还可以通过在构造函数中指定更新键来更新 MongoDB 中已有的文档集合。如果指定了更新关键字，则对应于该关键字的第一个文档(值)将被 Pig 元组的内容更新。

### 代码片段

在下面的示例代码中，我们考虑已经驻留在 MongoDB 中的`nasdaqDB.store_stock`数据的内容。数据集包括 20 世纪 70 年代至 2010 年纳斯达克数据；这包括各种公司的股票跟踪数据以及它们在特定一天的交易量数据中的表现。数据集按照跑马灯符号的字母顺序组织，并作为 JSON 对象存储在 MongoDB 中。

#### 进入码

下面的代码通过将 MongoDB 文档的字段映射到模式中指定的字段，执行连接到 MongoDB、建立连接、加载 MongoDB 本地文件、解析它以及仅检索在`MongoLoader`构造函数中指定的模式的任务。这个抽象是通过调用`MongoLoader`函数来实现的。

```sh
/*
Register the mongo jar files to be able to use MongoLoader UDF
*/
REGISTER '/home/cloudera/pdp/jars/mongo.jar';
REGISTER '/home/cloudera/pdp/jars/mongo-hadoop-pig.jar';

/*
Load the data using MongoLoader UDF, it connects to MongoDB, loads the native file and parses it to retrieve only the specified schema.
*/
stock_data = LOAD 'mongodb://slave1/nasdaqDB.store_stock' USING com.mongodb.hadoop.pig.MongoLoader('exchange:chararray, stock_symbol:chararray, date:chararray, stock_price_open:float, stock_price_high:float, stock_price_low:float, stock_price_close:float, stock_volume:long, stock_price_adj_close:chararray') AS (exchange,stock_symbol,date,stock_price_open,stock_price_high,stock_price_low,stock_price_close,stock_volume,stock_price_adj_close);

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Display the contents of the relation stock_data on the console
*/
DUMP stock_data;
```

#### 导出代码

以下代码描述了将`stock_data` pig 中存在的数据写入 MongoDB 文档集合:

```sh
/*
Register the mongo jar files and piggybank jar to be able to use the UDFs
*/
REGISTER '/home/cloudera/pdp/jars/mongo.jar';
REGISTER '/home/cloudera/pdp/jars/mongo_hadoop_pig.jar';
REGISTER '/usr/share/pig/contrib/piggybank/java/piggybank.jar';

/*
Assign the alias MongoStorage to MongoStorage class
*/
DEFINE MongoStorage com.mongodb.hadoop.pig.MongoStorage();

/*
Load the contents of files starting with NASDAQ_daily_prices_ into a Pig relation stock_data
*/
stock_data= LOAD '/user/cloudera/pdp/datasets/mongo/NASDAQ_daily_prices/NASDAQ_daily_prices_*' USING org.apache.pig.piggybank.storage.CSVLoader() as (exchange:chararray, stock_symbol:chararray, date:chararray, stock_price_open:chararray, stock_price_high:chararray, stock_price_low:chararray, stock_price_close:chararray, stock_volume:chararray, stock_price_adj_close:chararray);

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Store data to MongoDB by specifying the MongoStorage serializer.  The MongoDB URI nasdaqDB.store_stock is the document collection created to hold this data.
*/
STORE stock_data INTO 'mongodb://slave1/nasdaqDB.store_stock' using MongoStorage();
```

### 结果

作为在 MongoDB 文档集合上应用摄取设计模式的结果，MongoDB URI 指定的集合的内容被加载到`stock_data`PIG 关系中。同样，导出设计模式将`stock_data`PIG 关系的内容存储到`nasdaqDB.store.stock`蒙古文数据库文档集合中。

以下方式具体到`MongoLoader`的实现，可以用无效值存储 Pig 关系:

*   如果输入的 MongoDB 文档包含一个未在构造函数模式下映射的字段，`MongoLoader`函数将在 Pig 关系中存储该字段的空值。
*   如果 MongoDB 文档不包含构造函数模式中指定的字段，则关系的整行或元组将被设置为 null。
*   如果 MongoDB 文档字段和指定模式之间存在类型不匹配，则`MongoLoader`将在 Pig 关系中将该字段设置为空。

### 附加信息

*   [https://github。com/mong odb/mongo-Hadoop/blob/master/pig/自述哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟哟. MD〔t1〕t1〔t1〕](https://github.com/mongodb/mongo-hadoop/blob/master/pig/README.md)

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `chapter2/code/`
*   `chapter2/datasets/`

## 糖化血红蛋白的进出方式

HBase 的导入导出模式描述了如何使用 Pig 拉丁语将 HBase 表的内容摄取到 Pig 关系中，对数据进行进一步处理，再将处理后的数据导出到 HBase。

我们将讨论糖化血红蛋白与企业的相关性，了解糖化血红蛋白数据的各种内部存储和外部访问方式，以及糖化血红蛋白数据结合 Pig 的使用案例。您还将了解 Pig 如何通过提供现成的功能来更轻松地导入和导出 HBase 数据。

下面关于这个模式的实现级细节的讨论旨在让您熟悉重要的概念和适用的替代方案。样例代码片段可以从 Pig 语言的角度更好地理解模式，进而是使用模式的结果。

### 背景

HBase 是一个面向列的 NoSQL 数据库，其灵感来源于 Google 实现的 Big Table，专门设计用于以灵活的模式存储数据并实时访问。它对于包含数十亿列的数据是线性可伸缩的，并且具有数据压缩和快速访问的内存操作功能。

在 HDFS，糖化血红蛋白数据以定制的优化格式存储在内部，称为**索引存储文件**。HBase 使用 HDFS 来利用其存储和高可用性功能。由于 HDFS 无法存储数据来执行随机读写，因此 HBase 使用针对随机读写访问优化的二进制格式来克服 HDFS 的限制。在 HDFS 存储 HBase 索引存储文件，非常适合 MapReduce 在不从其他地方导入数据的情况下在上面工作。

逻辑上，HBase 将数据存储在一个嵌套的多维映射抽象中，该抽象具有排序的键值对和与键值相关联的时间戳。时间戳使最新版本的数据能够以有序的顺序存储，以便于搜索。HBase 实现了快速和慢速变化数据的概念，因此可以使用版本来相应地存储它们。多维嵌套映射中的数据是通过使用主键来检索的，主键在 HBase 中称为 rowkey，可用于取消引用所有嵌套数据。

多维映射有两个重要的嵌套结构(实现为映射)，称为列族和属于列族的列。列族的架构在存储生命周期中不能更改，而列族中列的架构可以有灵活的架构，每行可以更改一次。这种数据组织本质上适合存储不相关的适合实时访问的非结构化数据(因为一切都在地图中)。

### 动机

需要将 HBase 表摄取到 Pig 中，有助于用 MapReduce 框架对其进行批处理，达到用例目标。在 PIG 数据管道中处理糖化血红蛋白数据后，有时需要将其存储回糖化血红蛋白中，以便实时访问在糖化血红蛋白上运行的查询。正是在这样的背景下，HBase 数据的导入和导出模式显得尤为诱人。

HBase 中的数据通过 HBase Java 客户端的应用编程接口在本地访问，放入并获取数据。这个应用编程接口非常适合与需要实时查询能力的外部应用集成。但是，API 没有能力执行批处理数据处理来创建数据聚合和复杂的管道来生成分析意见。这种批处理能力伴随着低层次的抽象，如 MapReduce 或 Pig 的高级灵活性。

您可以编写 Java MapReduce 作业来访问存储在 HBase 中的数据并进行处理，但与必须编写 Java 代码来访问 HBase 中的数据相比，Pig 在简单性和简洁优化的代码方面得分很高。

通过 Pig 中的运算符访问存储在 HBase 中的数据，可以对 Pig 数据管道中的数据进行操作，并使用批处理对其进行转换。将 Pig 关系中的数据存储到 HBase 中，使 HBase 能够为应用提供实时查询数据的权利。

由于数据以索引存储文件的形式驻留在 HDFS，因此有必要告诉 Pig 如何以 Pig 能够理解和处理的方式将数据序列化和反序列化为 HBase 格式。Pig 需要清楚地知道如何在列族、HBase 抽象中的列和 Pig 的原生数据类型之间进行转换。该模式说明了如何使用`HBaseStorage` pig 功能完成向/从 HBase 摄取和导出数据的任务。

### 注

与 MongoDB 示例不同，这里我们读取了已经存储在 HDFS 的 HBase 文件。我们不连接到 HBase 服务器，也不通过网络从它们读取数据。

### 用例

以下是 Pig 从 HBase 接收和输出数据的一个用例:

*   使用摄取设计模式创建一个数据管道，以集成驻留在 HBase 中的实时数据来执行分析。
*   使用摄取设计模式访问 HBase 中的数据，以便在 Pig 中为下游系统执行高级数据聚合。Pig 可以作为一个 ETL 中枢来转换 HBase 中的数据，并将其与其他应用的数据集成在一起。
*   使用导出设计模式将 HDFS 现有平面文件的内容存储到一个 HBase 表中。对于实时查询，这种模式对于在 HBase 中存储复杂数据集成或转换管道的结果也很有用。

### 模式实现

下面一节介绍 HbA1c 摄入模式的模式实现，后面是 HbA1c 输出。

#### 进入实施

hbase 中的数据可以通过以下两种方式获取:

*   第一种选择是用并行读取的 MapReduce `EXPORT job`导出整个表，从而将表的内容导出到 HDFS 序列文件。反序列化程序可以用 Java 或 Pig 编写，以访问序列文件的内容，供以后操作。这个选项有点难以实现，因为我们必须从后端访问 HBase 的内容，然后反序列化文件。此外，这可以一次处理一个表并访问多个表；必须重叠的列表。
*   第二种选择是实现 HBase 进气设计模式，使用 Pig `HBaseStorage`的内置加载功能。这是一个连接到 HBase 表并将表的内容直接放入 Pig 关系的简单选项。Pig 负责将 HBase 类型映射到 Pig 类型的反序列化任务和并行导入的 MapReduce 作业。`HBaseStorage`还有一个优点，就是将数据加载到 Pig 关系中，使用所有列族或者只使用列族的列子集。因为列包含键值类型，所以它们可以按类型转换为 Pig 的映射类型。

#### 出口变现

Pig 使用`HBaseStorage`功能实现导出设计模式。除了使用`STORE`子句之外，该模式与摄取模式的实现非常相似。`STORE`子句向 Pig 编译器传达要从指定的 Pig 关系中提取什么数据，并将其序列化到参数中的 HBase 表中。

下图说明了入口和出口实施选项:

![The egress implementation](img/5556OS_02_05.jpg)

糖化血红蛋白酶与 PIG 的整合

### 代码片段

下面的代码示例使用了一个包含零售交易复合样本的数据集。包含 T0、T1、T2、T10、T3、T4、T5、T6 等属性。该数据已存储在 HBase 中，以说明此示例。糖化血红蛋白表`hbase://retail_transactions`通过 PIG 拉丁的`HBaseStorage`功能进入。

#### 进入码

下面的代码片段说明了摄入 PIG 的糖化血红蛋白数据之间的关系:

```sh
/*
Load data from HBase table retail_transactions, it contains the column families transaction_details, customer_details and product_details.
The : operator is used to access columns in a column family.
First parameter to HBaseStorage is the list of columns and the second parameter is the list of options
The option -loadkey true specifies the rowkey should be loaded as the first item in the tuple, -limit 500 specifies the number of rows to be read from the HBase table
*/
transactions = LOAD 'hbase://retail_transactions'
  USING org.apache.pig.backend.hadoop.hbase.HBaseStorage(
  'transaction_details:transaction_date customer_details:customer_id customer_details:age customer_details:residence_area product_details:product_subclass product_details:product_id product_details:amount product_details:asset product_details:sales_price', '-loadKey true -limit 500')
  AS (id: bytearray, transaction_date: chararray, customer_id: int, age: chararray, residence_area: chararray, product_subclass: int, product_id: long, amount: int, asset: int, sales_price: int);

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

-- Display the contents of the relation transactions on the console
DUMP transactions;
```

#### 导出代码

以下代码说明了在 HBase 表中存储 Pig 关系的内容:

```sh
/*
Load the transactions dataset using PigStorage into the relation transactions
*/
transactions = LOAD '/user/cloudera/pdp/datasets/hbase/transactions.csv' USING PigStorage( ',' ) AS (
    listing_id: chararray,
    transaction_date: chararray,
    customer_id: int,
    age: chararray,
    residence_area: chararray,
    product_subclass: int,
    product_id: long,
    amount: int,
    asset: int,
    sales_price: int);

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Use HBaseStorage to store data from the Pig relation transactions into a HBase table hbase://retail_transactions.
The individual contents of transactions are mapped to three column families transaction_details, product_details and customer_details.
*/
STORE transactions INTO 'hbase://retail_transactions' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('transaction_details:transaction_date customer_details:customer_id customer_details:age customer_details:residence_area product_details:product_subclass product_details:product_id product_details:amount product_details:asset product_details:sales_price');
```

### 结果

由于应用了糖化血红蛋白的数据摄入模式，糖化血红蛋白表中以列族和对应列表示的数据会以 PIG 的关系加载。在这个设计模式中，加载到 Pig 关系中的结果类型根据传递的参数而变化。如果用列族和列标识符(`CFName:CName`)指定一列，结果类型将是由标量值组成的元组。如果使用列族名和部分列名后跟星号(`CFName:CN*`)来指定列，则生成的列类型将是列描述符作为键的映射。

需要注意的是，在检索 HBase 中存储的时间序列或基于事件的数据时，不能使用 Pig 获取 HBase 值的时间戳信息。

作为应用 HBase 数据输出模式的结果，pig 关系中的数据存储在 HBase 表中，并映射到相应的列族和相应的列。

### 附加信息

*   [http://pig.apache.org/docs/r0.11.1/func.html#HBaseStorage](http://pig.apache.org/docs/r0.11.1/func.html#HBaseStorage)

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `chapter2/code/`
*   `chapter2/datasets/`

# 结构化数据的入口和出口模式

下面部分以 Hive 为例，作为我们可以获取结构化数据的来源之一，讨论不同的方法。选择 Hive 来说明结构化数据的摄入模式，因为它是企业中使用最广泛的数据接收器。此外，通过理解这种模式，您可以将其扩展到其他结构化数据。

## Hive 的接入方式

Hive 摄取模式描述了如何用 Pig Latin 将数据从 Hive 表摄取和排出到 Hadoop 文件系统，以便在数据管道上进一步处理。

我们将讨论 Hive 和企业之间的相关性，并了解内部存储的各种方式(RCFile、序列文件等)。)和 Hive 数据的外部访问(HQL 和 Pig/MapReduce)。您将探索将 Hive 数据与 Pig 相结合的用例。您还将了解 Pig 如何通过提供现成的函数来更容易地摄取 Hive 数据，然后了解 Hadoop 生态系统的一个组件 HCatalog 的作用，以简化 Pig 和 Hive 表之间的连接和访问机制。

下面关于这个模式的实现级细节的讨论旨在让您熟悉重要的概念和适用的替代方案。一个示例代码片段用于从 Pig 语言的角度更好地理解模式，然后是使用模式的结果。

### 背景

Hive 让熟悉 SQL 并在 RDBMS 工作过的程序员很容易开发出 Hadoop。Hive 使用 HDFS 作为数据的物理存储，这从逻辑角度给出了表级抽象。Hive 实现了自己的类似 SQL 的方言 HiveQL 来访问和操作数据。HiveQL 提供了 SELECT、GROUP 和 JOIN 等运算符，这些运算符在 Hadoop 集群上执行之前会转换为 MapReduce。

在企业中，Hive 已经被数据仓库、商业智能分析、仪表盘等用例广泛认可。所有这些用例在 Hive 中都有一个公共的数据线程，它已经被清理、正确标记、键入并整齐地组织在表中，因此任何特殊的查询或报告都可以毫不费力地生成。与这种情况形成对比的是，Pig 的用例必须处理来自不同来源的新生成的数据，这种情况令人困惑，没有相关的名称、类别或元数据来解释。因此，Pig 在研究数据本身时的相关性是创建一个快速原型，并在非模态数据的表面随机性中寻找意义。

在 HDFS，Hive 数据的存储是通过将 Hive 表的内容序列化为可以存储在 HDFS 的物理文件来实现的。在 Hive 的上下文中，HDFS 被用来提供高可用性、容错性以及在 Hive 的特定文件上运行 MapReduce 的功能。目前，Hive 支持四种不同的存储文件:纯文本文件、二进制序列文件、ORC 文件和 RC 文件。这些文件格式中的每一种都有自己相关的序列化和反序列化功能，它将存储在 Hive 中的数据的表级抽象转换为存储在 HDFS 的文件。

Hive 将有关物理文件内容的信息存储在 RDBMS 上实现的外部元数据存储中(默认情况下选择 Derby、MySQL)。这个元数据存储包含了所有的信息，比如表、模式、类型、物理文件映射等。每当用户执行数据操作时，他们将首先查询这个元存储来找到数据的位置，然后访问实际的数据。

### 动机

Hive 以随时可用的格式存储数据，以便进行特别分析和报告。数据摄取模式与 Hive 数据相关，Hive 数据被摄取并与 Pig 数据管道中新到达的数据整合；然后，对来自 Hive 和 Pig 的组合数据进行总结、归纳和转换，以便在高级分析模型中进一步使用。

数据导出模式适用于 Pig 数据管道中已经存在的数据，有一种方法可以直接存储在 Hive 表中。

外部 Hadoop 生态系统组件(如 Pig、HBase 或 MapReduce)可以通过知道在 HDFS 使用哪种存储格式(文本、RCFile 或序列文件)来存储 Hive 数据，以及元存储中的表和模式的元数据信息来访问 Hive 数据。

这个入口和出口设计模式描述了用 Pig 读写 Hive 数据的方法。

### 注

与 HBase 类似，我们已经在 HDFS 加载了 Hive 文件。这与 MongoDB 形成对比，在 MongoDB 中，我们直接从外部源而不是 HDFS 读取数据。

### 用例

Hive 摄入设计模式的主要用例是为 Pig 提供对 Hive 中存储的数据的访问。Pig 使用这些数据的原因如下:

*   它与其他非结构化来源相集成。
*   清理并转换组合数据。
*   使用 Pig 管道中其他数据源的组合进行聚合和汇总。

Hive 导出设计模式的主要用例是提供一种机制，用于在 Hive 表的 Pig 管道中存储转换数据。您可以考虑将这种设计模式用于以下目的:

*   数据在 Pig 数据管道中与外部数据集成，然后导出到 Hive。
*   将清理和转换后的数据从 Pig 数据管道导出到 Hive。
*   将聚合导出到 Hive 或其他下游系统，以便进一步处理或分析。

### 模式实现

以下部分描述了 Hive 进入模式的模式实现，随后是 Hive 退出:

#### 进入实施

以下是将 Hive 数据加载到 Pig 拉丁关系中的两种方式:

*   一种方法是显式指定反序列化程序从 Hive 中检索数据。例如`HiveColumnarLoader`是 Pig 的反序列化器，专用于使用 RCFile 格式加载或序列化到 Hive 中的数据。同样，我们可以使用 Piggybank 的`SequenceFileLoader`从 Hive 加载已经以`SequenceFile`格式存储的数据。这两个例子与文件的位置、用于存储它们的模式的格式、是否使用压缩等密切相关。
*   第二种方法是使用 HCatalog 的功能将 Hive 数据加载到 Pig 中。与前一点相比，这个过程有很多优点。HCatalog 提供了一种抽象的方式来查看文件的存储。它包装了来自 HDFS 的 metastore 和存储信息，为访问表提供了统一的视角。使用 HCatalog，您不再需要担心文件的存储位置、模式的格式或是否使用压缩。您只需要指定 HCatalog 加载器的表名，它就可以在幕后完成必要的管道工作，并绘制底层的存储格式、位置和模式。现在用户不知道表的位置、分区、模式、压缩类型和存储格式。HCatalog 通过表级抽象简化了这一点，并在幕后做了大量工作。

#### 出口变现

导出设计模式是使用 HCatalog 函数实现的，该函数将 Pig 关系中的数据存储到 Hive 表中。HCatalog 提供了一个 HCatStorer 接口，将 Pig 关系的内容存储到 HCatalog 管理的 Hive 表中。有关为什么 HCatalog 接口是加载和存储操作的最佳选择的更多信息，请参考上一节*入口实现*中的第二点。

下图说明了这些设计模式遵循的方法:

![The egress implementation](img/5556OS_02_06.jpg)

Hive 和 PIG 的结合

### 代码片段

下面的代码示例使用了一个零售交易数据集样本。包含 T0、T1、T2、T3、T4、T5、T6、T7、T8 等属性。这个数据已经存储在 Hive 中来说明这个例子。Hive 的本地存储 RCFile，包含此表的内容，用于解释直接访问；`HCatalogLoader`在下例中也有说明。

#### 进入码

下面的代码说明了从 Hive 获取数据。

##### 使用 RCFile 导入数据

下面的代码说明了`HiveColumnarLoader`从存储在 RCFile 中的 Hive 表加载数据的用法:

```sh
/*
Register the Piggybank jar file to be able to use the UDFs in it
*/
REGISTER '/usr/share/pig/contrib/piggybank/java/piggybank.jar';

-- Register Hive common and exec jars
REGISTER '/usr/lib/hive/lib/hive-common-0.11.0.1.3.0.0-107.jar';
REGISTER '/usr/lib/hive/lib/hive-exec-0.11.0.1.3.0.0-107.jar';

/*
Load retail_transactions_rc  RCfile and specify the names of the columns of the table and their types in the constructor of HiveColumnarLoader.
*/
transactions = LOAD '/apps/hive/warehouse/transactions_db.db/retail_transactions_rc' USING org.apache.pig.piggybank.storage.HiveColumnarLoader('transaction_no int,transaction_date string,cust_no int,amount double,category string,product string,city string,state string,spendby string');

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Display the contents of the relation transactions on the console
*/
DUMP transactions;
```

##### 使用 HCatalog 导入数据

下面的代码展示了如何使用 HCatalog 从 Hive 加载数据:

```sh
/*
Specify the table name as the input to the HCatLoader function provided by HCatalog.
This function abstracts the storage location, files type, schema from the user and takes only the table name as input
*/
transactions = LOAD 'transactions_db.retail_transactions' USING org.apache.hcatalog.pig.HCatLoader();

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Display the contents of the relation transactions on the console
*/
DUMP transactions;
```

#### 导出代码

下面的代码展示了如何使用 HCATTOR 将数据导出到 Hive:

```sh
-- Register piggybank and hcatalog-pig-adapter jars
REGISTER '/usr/share/pig/contrib/piggybank/java/piggybank.jar';
REGISTER '/usr/lib/hcatalog/share/hcatalog/hcatalog-pig-adapter.jar';

/*
Load the transactions dataset into the relation transactions
*/
transactions = LOAD '/user/cloudera/pdp/datasets/hive/retail_transactions.csv' USING org.apache.pig.piggybank.storage.CSVLoader() AS (transaction_no:int, transaction_date:chararray, cust_no:int, amount:double, category:chararray, product:chararray, city:chararray, state:chararray, spendby:chararray);

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Specify the Hive table name transactions_db.retail_transactions as the input to the HCatStorer function.
The contents of the relation transactions are stored into the Hive table.
*/
STORE transactions INTO 'transactions_db.retail_transactions' using org.apache.hcatalog.pig.HCatStorer();
```

### 结果

应用进气设计模式后，Hive 表中的数据以 Pig 关系加载，并准备进一步处理。使用`HCatLoader`时，正确解释 HCatalog 的数据类型如何映射到 Pig 类型很重要。除了 HCatalog 中映射到二进制的`bytearray`类型外，所有基本类型的 PIG 都映射到它们对应的 HCatalog 类型。在复杂数据类型中，HCatalog 的映射映射到 Pig 中的映射，HCatalog 的列表映射到 Pig 中的包，HCatalog 的`struct`映射到 Pig 中的元组。

应用导出设计模式，Pig 关系中的数据存储在 Hive 表中，以便在 Hive 中进行报告和具体分析。前段提到的`HCatLoader`的所有模式转换规则也适用于`HCatStorer`。`HCatStorer`类接受代表分区表键值对的字符串参数。如果要在分区表中存储 Pig 关系的内容，应该强制指定这个参数。

### 附加信息

*   [http://pig .Apache。组织/文档/r 0 .11.0/API/org/Apache/pig ybbank/storage/hivecolumnarloader .html](http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/piggybank/storage/HiveColumnarLoader.html)
*   [https://cwiki。 Apache. Organize/gather/show/hive /HCatalog+ load storage](https://cwiki.apache.org/confluence/display/Hive/HCatalog+LoadStore)

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `chapter2/code/`
*   `chapter2/datasets/`

# 半结构化数据的导入导出模式

本节描述半结构化数据的设计模式，如 XML、JSON 和大型机数据。我们选择了 XML 和 JSON，因为它们是互联网数据交换最流行的编码格式。大量数据被锁定在文档、期刊和内容管理系统中，这可能会从分析中受益。之所以选择大型机数据作为这个用例，主要是因为它在很多企业中是一个相对未被探索的领域，随着新模型的出现，它最终可能会被普及。

## 主机摄入模式

大型机摄取模式描述了如何使用 Pig Latin 将从大型机导出的数据摄取到 Hadoop 文件系统中，以便在数据管道上进一步处理。

我们将讨论处理存储在大型机中的数据与企业之间的相关性，并对大型机内部存储和访问数据的各种方式有更深入的了解。我们还将结合 Pig 讨论摄取的动机和大型机数据的用例。您还将学习 Pig 如何比使用用 Java 编写的 MapReduce 代码(使用 UDFs)更直观地获取数据。

下面关于这个模式的实现级细节的讨论旨在让您熟悉重要的概念和适用的替代方案。一个示例代码片段用于从 Pig 语言的角度更好地理解模式，然后是使用模式的结果。

### 背景

大型机似乎还有很长的路要走，很难想象一个没有这些老黄牛来处理这些事务几十年的世界。这就是这些机器的稳定性。即使在极高的数据吞吐量下，它们也能忠实地运行，一秒钟也不闪烁。难怪这些工程奇迹已经成为从飞机和汽车到金融服务和政府的商业支柱，多年来不断销售、跟踪、插入和更新这些实体的每一笔交易。

凭借 20 世纪 60 年代推动工业革命的经验，大型机随着时间的推移而发展，变得更加强大，并且能够处理它们设计的特定高吞吐量事务性工作负载。如今，他们使用定制的处理器和其他高端硬件来实施虚拟化、纵向扩展并展示事务完整性，尽管吞吐量极高。显然，大型机可以提供比任何其他体系结构更好的结果，具有极高的吞吐量、难以置信的高可用性、可靠性和顶级安全性。

### 动机

Hadoop 在从大型机卸载大量事务数据并对其进行批处理方面发挥着越来越重要的作用。这与通过将处理从昂贵的定制系统转移到包含 Hadoop 框架的商业硬件来提高大型机的事务吞吐量和批处理时间是一致的。同样，当大型机的批处理能力无法有效扩展时(在一个价位和性能范围内)，将处理工作卸载给 Hadoop 是有好处的，因为 Hadoop 可以以更好的性价比完成这项工作。如下图所示:

![Motivation](img/5556OS_02_07.jpg)

批处理和卸载到 Hadoop

### 用例

这个设计模式可以应用于解决以下用例:

*   将数据从大型机迁移到 Pig，以与其他系统的数据集成并创建高级分析。
*   将非关键批次工作负载卸载到 Pig，大幅释放大型机吞吐量。
*   在 Pig 中重写 COBOL 代码带来了可重用性、可维护性、简单性和紧凑性的优点。

### 模式实现

通过卸载，Hadoop 的相关处理意味着将 COBOL 编写的代码重写到 MapReduce，并从大型机传输数据。

COBOL 是*通用语言*，用于大型机访问 DB2 和其他数据库，执行批处理和处理在线事务。不适合在互联网上实现复杂的算法，可能有利于更新业务需求，如风险建模、预测分析等。

为了将 COBOL 代码迁移到 MapReduce，我们可以选择一些在大型机中实现的函数，这些函数可以遵循 mapper 和 Reduce 的结构。例如，遗留的 COBOL 代码可以对数十亿条记录进行排序，将它们与其他数据源进行合并和分组，并执行复杂的转换，这比 Pig 中的 COBOL 更有效。在 Pig 代码中加入 Java UDF 函数，可以对数据管道进行高级分析；这种结合可能会创造奇迹。因此，将代码迁移到 Pig Latin 以有效地执行特定的处理可以带来丰厚的回报。

迁移大型机数据有其自身的一系列挑战。通常，大型机在内部将各种类型的数据存储在 VSAM 文件、平面文件和数据库管理系统中。对于 Hadoop 来说，要访问这些数据，需要将其转换成它能理解的格式，然后通过文件传输机制物理传输到 Hadoop 集群。使用特定的实用程序(如 IDCAMS)，您可以将 VSAM 文件转换为平面文件供 Hadoop 使用。

每个大型数据库管理系统都有自己的专用工具，可以了解数据库管理系统的内部文件存储格式，并将其转换为平面文件。我们可能需要处理这些平面文件从大型机中的一个代码页到 Hadoop 集群的目标机器中的另一个代码页的转换。一般来说，从主机导出的平面文件是反规格化的 CSV 格式。如下图所示:

![Pattern implementation](img/5556OS_02_08.jpg)

大型机数据提取和摄取到 Hadoop 中

为了理解 CSV 格式每一列的物理布局和定义，使用了一个专用于大型机的字帖。在 Hadoop 中，字帖提供的信息被用作分析 CSV 和解码 CSV 文件含义的模式。因此，主机数据的摄取需要两个输入:一个是平面文件本身，另一个是字帖。

Pig 有内置的加载器，可以读取 CSV 文件，但是解析必须在 CSV 上用字帖的内容完成。因此，必须编写一个 Java UDF 或自定义加载器来实现这一点。

### 代码片段

下面的代码示例使用了一个数据集，该数据集包含从大型机到 CSV 文件的与车辆维修索赔相关的示例车辆保险索赔数据。VSAM 文件中数据元素的元数据和物理布局在字帖中定义。字帖包含`claim`、`policy`、`vehicle`、`customer,`、`garage details`等字段。下一节中的 Java 代码片段解析字帖，检索元数据，并使用它将数据加载到 CSV 文件中。

以下 Pig 脚本使用自定义加载器将从大型机提取的数据加载到 CSV 文件中。这里`VSAMLoader`使用字帖文件确定 CSV 文件的元数据并加载:

```sh
/*
Register custom UDF vsamloader.jar and  cb2java jar which is a dynamic COBOL copybook parser for Java
*/
REGISTER '/home/cloudera/pdp/jars/vsamloader.jar';
REGISTER '/home/cloudera/pdp/jars/cb2java0.3.1.jar';

/*
Load the contents of the automobile insurance claims dataset using custom UDF.
VSAMLoader uses the copybook file to parse the data and returns the schema to be used to load the data
*/
data = LOAD '/user/cloudera/pdp/datasets/vsam/automobile_insurance_claims_vsam.csv' USING com.mycustomloader.vsamloader.VSAMLoader();

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

-- Display the contents of the relation data on the console
DUMP data;

-- Display the schema of the relation data
DESCRIBE data;
```

以下是 VSAMLoader 的 Java 代码片段，这是一个自定义的加载器实现:

```sh
@Override
public ResourceSchema getSchema(String arg0, Job arg1) throws IOException {
  .
  .
  while (it.hasNext()) {
    Map.Entry pairs = (Map.Entry) it.next();
    //Get the next key/value pairs
    String key = (String) pairs.getKey();
    String value = (String) pairs.getValue();
    /*For Group and Alphanumeric types in copybook, return
    pig compliant type chararray*/
    if (value.toString()
    .equals("class net.sf.cb2java.copybook.Group")
    || value.toString().equals("class net.sf.cb2java.copybook.AlphaNumeric")){
       fieldSchemaList.add(new FieldSchema(key,
        org.apache.pig.data.DataType.CHARARRAY));
      }
    /*For Decimal type in copybook, return
    pig compliant type integer*/
    else if (value.toString()
    .equals("class net.sf.cb2java.copybook.Decimal")){
      fieldSchemaList.add(new FieldSchema(key,
        org.apache.pig.data.DataType.INTEGER));
    }
    // Else return default bytearray
    else
    {
       fieldSchemaList.add(new FieldSchema(key,
       org.apache.pig.data.DataType.BYTEARRAY));
    }
    }
  return new ResourceSchema(new Schema(fieldSchemaList));
}
```

在`vsamloader` JAR 实现的自定义加载器代码中，我们使用外部 API 解析字帖文件，得到所有的值。然后，我们从 Pig API 及其`getSchema()`方法中实现一个名为`LoadMetaData`的接口，它将返回我们通过解析字帖获得的模式。使用类型为`FieldSchema`的`ArrayList`类，它将最终在字帖文件中填写列名及其数据类型。这个`ArrayList`作为新模式返回，小 PIG 加载 VSAM 文件时会用到。

### 结果

将该模式应用于主机数据提取的结果是将平面文件中的数据加载到 Pig 拉丁关系中进行进一步处理。因为 Pig 中没有现成的函数来理解字帖格式，所以我们通过自定义加载器来扩展 Pig。必须注意在自定义加载器中正确映射模式，因为并非所有的 COBOL 数据类型都可以轻松映射到 Java 对应类型。例如，COBOL 对布尔值和日期类型的支持是有限的，所以我们必须实现一个特殊的转换来用 Java 处理它以获得准确的结果。有关更多信息，请参见下一节中的链接。

### 附加信息

*   [http://pig。 Apache. Organization/document /r 0\. 11 .1 UDF。 Html # Load-Store-Function](http://pig.apache.org/docs/r0.11.1/udf.html#load-store-functions)
*   [http://pic.dhe.ibm.com/infocenter/dmanager/v7r5/index.jsp?主题= % 2 fcom .国际商用机器公司.数据服务器.规则工作室% 2 内容% 2 FB 业务 _ 规则% 2 2F _ 公共库% 2 finfocenter _ Primary % 2 fps _ DS _ Rule _ designer 772 .html](http://pic.dhe.ibm.com/infocenter/dmanager/v7r5/index.jsp?topic=%2Fcom.ibm.dserver.rulestudio%2FContent%2FBusiness_Rules%2F_pubskel%2FInfocenter_Primary%2Fps_DS_Rule_Designer772.html)
*   [http://www .3480-3590-数据转换 com/article-reading-cobol-layout-1 .html](http://www.3480-3590-data-conversion.com/article-reading-cobol-layouts-1.html)

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `chapter2/code/`
*   `chapter2/datasets/`

## XML 接收输出模式

本节介绍如何使用 Pig Latin 将 XML 编码的文档或日志的内容导入和导出到 Hadoop 文件系统，以便在数据管道上进一步处理。

我们将讨论如何处理存储在 XML 中的数据与企业的相关性，了解 Pig 访问 XML 数据(原始 XML 和二进制)的各种方式。您将了解使用原始和二进制 XML 解析的优缺点，然后了解将 XML 数据与 Pig 相结合的动机和用例。您还将学习 Pig 如何比使用用 Java 编写的 MapReduce 代码更直观、更高效地获取数据。

下面关于这个模式的实现级细节的讨论旨在让您熟悉重要的概念和适用的替代方案。一个示例代码片段用于从 Pig 语言的角度更好地理解模式，然后是使用模式的结果。

### 背景

XML 是以直观的方式存储和传输数据的最广泛使用的协议之一，这使得人类和机器相对容易理解数据的含义。XML 是文本格式，不是二进制格式。它具有用相关元数据编码数据的特殊能力，并且可以自己阐明其含义。由于这一特性，XML 已经成为大多数互联网应用事实上的数据传输标准。XML 的通用性在于它不仅可以表示文档，还可以表示网络服务中任意的数据结构。今天，我们看到数以千计的基于 XML 的分类法、信息交换格式和文档格式——如微软 Office、SOAP、RSS、XHTML 和 ATOM——正在被广泛使用。从分析的角度来看，所有这些基于 XML 的数据存储和传输格式都包含了丰富的信息。

### 动机

使用 Hadoop 接收和输出 XML 本来就很复杂，在灵活性上也有一些妥协。复杂性来自于任意的嵌套，元数据本身所需要的空间可能是惊人的。尽管 XML 通过包含标签和其他可选字段为您提供了模拟现实世界的灵活性，并使用大量元数据信息对数据进行编码，但它显然会导致属性的深度嵌套，并使大量数据的计算更加复杂和耗时。这意味着将 XML 文档加载到计算机内存中是一项非常复杂的 CPU 密集型工作。

由于上述复杂性，Hadoop 提供了许多优势来更快地处理大型复杂的 XML 数据，并且通过接收、转换和导出 XML 供下游系统进一步使用，它可以使用更低成本的操作。要在 Hadoop 中处理 XML，您可能必须考虑处理 XML 数据的性质和上下文。

#### 摄取原始 XML 的动机

Hadoop 中摄取和处理 XML 的一个原因是当你事先不知道 XML 模式，在读取文件时想要理解它。该方法的要点如下:

*   XML 数据以其原始格式加载，在查询过程中找到其模式，并在发现后执行其转换。
*   这种方法本质上更具探索性；它提供了快速的初始加载，因为数据没有使用序列化以二进制格式进行清理或存储。
*   它支持更大的灵活性，因此可以使用多个模式来解析不同类型分析查询的 XML。
*   它适用于定义良好的格式，这可能会导致为每个查询解析 XML 数据，并对查询性能产生轻微影响。

#### 摄取二进制 XML 的动机

在 Hadoop 中摄取和处理 XML 的另一个原因是当您已经知道了 XML 的模式，并且想要对 XML 执行高性能查询时。该方法的要点如下:

*   XML 必须首先被解析，以二进制格式序列化到磁盘，跨节点拆分，压缩，并针对查询进行优化。
*   如果在加载过程中需要大量的清理和重新格式化，这种方法是可行的。
*   如果需要对生产工作负载执行重复查询，这是合适的。
*   如果加载时模式未知，则此方法不适合，因为以可查询格式加载、预处理和存储 XML 需要很长时间。

#### XML 输出的动机

Hadoop 可用于从 HDFS 的 CSV 和 Hive 表等结构化数据创建和输出 XML 文件。XML 文件也可以直接摄取到 Hadoop 中，这样就可以由 Pig 进行验证或转换，写回 XML，满足下游系统的交换格式。

### 用例

XML 摄取模式可用于以下用例，以解决以下问题:

*   它可用于从内容管理系统(如技术文档、参考手册和期刊)接收基于 XML 的文档数据。摄取是在使用 Lucene 创建搜索索引并对其执行分析之前完成的。
*   它可以用来接收包含 SOAP 和 EDXL-CAP 类型的消息文本的 XML 日志，并分析系统之间的请求和响应。例如，可以从网络故障管理系统中提取 XML 编码的消息，并对其进行分析，以了解或预测子系统未来的故障。

当 HDFS 的结构化数据(分离的平面文件或配置单元表)需要转换为可扩展标记语言进行处理和序列化时，可以使用可扩展标记语言导出模式，以便上游系统可以使用可扩展标记语言进一步处理数据。

### 模式实现

Pig 提供结构直接加载原 XML 文件，支持加载预处理后的 XML 文件。

#### XML 原始摄取的实现

pigybank 库提供`XMLLoader`功能访问 XML 文件的内容。`XMLLoader`该函数的参数是一个内部转换成单记录元组的 XML 标记名。该记录包含开始 XML 标记和结束 XML 标记中包含的文本。使用从 XML 文件返回的元组，您可能必须执行进一步的解析，以将记录级 XML 值分解为它们的组件值；一般使用正则表达式函数`REGEX_EXTRACT`进行展平投影。

#### XML 二进制摄取的实现

将 XML 转换为二进制格式进行拆分是一个两步的过程。

*   第一步，为了解析 XML 文档，可以将文件完全读入基于内存的数据结构，使用 DOM 等解析器可以随机访问数据的所有元素。或者，您可以连续访问文件的内容，并一次控制一个解析步骤。这可以通过 XML SAX 解析器来完成，并且执行速度稍慢。DOM 解析的优点是可以链接多个处理器，但是编程比较困难，而 SAX 解析的优点是容易编程和拆分。但是，另一方面，SAX 执行起来很慢，因为它是一种用于解析 XML 文档的串行访问机制。
*   第二步，将解析后的 XML 转换成 Hadoop 可以处理的可拆分二进制格式，Avro 是执行符合这些标准的序列化的最佳选择，这有助于将 XML 文档转换成字节流，并以磁盘格式存储。Avro 是专门为 Hadoop 设计的，使其成为一种高度可压缩和可分离的二进制格式，与序列文件非常相似。与序列文件不同，只能通过 Java API 访问，Avro 文件可以从 C、C++、C#、Ruby、Python 等其他语言访问。凭借其独特的互操作性格式，Avro 文件可以从一种语言编写的代码传输到另一种语言编写的不同代码，甚至可以从 C 语言等编译语言传输到 Pig 等脚本语言。
*   每个 Avro 文件都用元数据包装了 XML 文件的底层内容，元数据包含反序列化或读取内容所需的信息。Avro 将元数据与文件的实际内容一起存储在文件中，这使得其他程序更容易首先理解元数据，然后处理嵌入的数据。一般元数据以 JSON 格式存储，数据以二进制格式存储。
*   Avro 文件还包括一个标签，用于在集群的多个节点上划分数据。在访问序列化的 Avro 文件之前，您必须将 XML 文件准备成 Avro 格式，并使用 piggybank 的`AvroStorage`将其读入 Pig 拉丁语的关系。

下图描述了上述实现方面:

![The implementation of the XML binary ingestion](img/5556OS_02_09.jpg)

XML 数据的进入和退出

## 代码片段

下面的代码示例使用了一个包含 MedlinePlus 健康讨论的 XML 数据集。

### 【T0】XML 原始摄取代码

XML 数据由数据元素的标签组成，如主题标题、相关网站、主要语言、日期、词汇、摘要、成员资格等其他语言的相关健康主题和内容。下面的代码片段解析 XML 标记，并将内容作为关系加载到 Pig 拉丁脚本中:

```sh
-- Register piggybank jar
REGISTER '/home/cloudera/pig-0.11.0/contrib/piggybank/java/piggybank.jar';

/*
XMLLoader accesses the specified XML file and retrieves the record level value to be stored in the tuple data specified by the parameter to the XMLLoader.
*/
data = LOAD '/user/cloudera/pdp/datasets/xml/mplus_topics_2013-09-26.xml' USING org.apache.pig.piggybank.storage.XMLLoader('article');

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Print the contents of the relation data to the console
*/
DUMP data;
```

### 【T0】XML 二进制摄取代码

以下代码执行 XML 的二进制摄取:

```sh
-- Register piggybank jar
REGISTER '/usr/share/pig/contrib/piggybank/java/piggybank.jar';

-- Register Avro and JSON jar files
REGISTER '/home/cloudera/pdp/jars/avro-1.7.4.jar';
REGISTER '/home/cloudera/pdp/jars/json-simple-1.1.1.jar';

/*
Assign the alias AvroStorage to piggybank's AvroStorage UDF
*/
DEFINE AvroStorage org.apache.pig.piggybank.storage.avro.AvroStorage();

/*
Load the dataset using the alias AvroStorage into the relation health_topics
*/
health_topics = LOAD '/user/cloudera/pdp/datasets/xml/mplus-topics_2013-09-26.avro' USING AvroStorage;

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

-- Print the contents of the relation health_topics to the console
DUMP health_topics;
```

下图描述了将 XML 文件转换为 AVRO 所涉及的步骤:

![The XML binary ingestion code](img/5556OS_02_10.jpg)

XML 到 Avro 预处理

我们使用第三方工具从给定的 XML 文件生成 XSD，并使用 schemagen 生成 JAXB、Avro 绑定和 Avro 模式。在内部，schemagen 使用 JAXB 绑定编译器 XJC；；然后，它从 XSD 模式文件生成一个代码模型。然后执行 XJC 插件以 JSON 格式创建 Avro 模式。XJC 插件调用 Avro 的 Java 模式编译器来生成新的 Java 类，以便序列化到 Avro 和从 Avro 序列化。

### 【T0】XML 导出代码

以下是将 CSV 文件内容转换为 XML 格式的 Pig 脚本。

#### PIG

自定义存储功能 XMLStorage 用于完成 CSV 文件内容到 XML 格式的转换:

```sh
/*
Register custom UDF jar that has a custom storage function XMLStorage to store the data into XML file.
*/
REGISTER '/home/cloudera/pdp/jars/xmlgenerator.jar';

/*
Load the transactions dataset using PigStorage into the relation transactions
*/
transactions = LOAD '/user/cloudera/pdp/datasets/hbase/transactions.csv' USING PigStorage( ',' ) AS (
    listing_id: chararray,
    transaction_date: chararray,
    customer_id: int,
    age: chararray,
    residence_area: chararray,
    product_subclass: int,
    product_id: long,
    amount: int,
    asset: int,
    sales_price: int);

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Custom UDF XMLStorage generates the XML file and stores it in the xml folder
*/
STORE transactions INTO '/user/cloudera/pdp/output/xml' USING com.xmlgenerator.XMLStorage();
```

#### 【T0】XML 存储

以下是 XML 存储的代码:

```sh
protected void write(Tuple tuple)
  {
    // Retrieving all fieds from the schema
    ResourceFieldSchema[] fields = schema.getFields();

    //Retrieve values from tuple
    List<Object> values = tuple.getAll();

    /*creating xml element by using fields as element tag
    and tuple value as element value*/
    Element transactionElement =  
    xmlDoc.createElement(XMLStorage.elementName);
    for(int counter=0;counter<fields.length;counter++)
    {
      //Retrieving element value from value
      String columnValue = 
      String.valueOf(values.get(counter));
      //Creating element tag from fields
      Element columnName = 
      xmlDoc.createElement(fields[counter].getName().toString().trim());
      //Appending value to element tag
      columnName.appendChild
      (xmlDoc.createTextNode(columnValue));
      //Appending element to transaction element
      transactionElement.appendChild(columnName);
    }
    //Appending transaction element to root element
    rootElement.appendChild(transactionElement);
  }
```

`write`该方法以元组为输入，表示 CSV 文件中的一行。此方法为元组中的每个字段创建一个 XML 元素。对 CSV 中的所有行重复此过程，以生成一个 XML 文件。

### 结果

将此模式应用于 medline XML 文件的结果是将数据加载到 Pig Latin 关系中进行进一步处理。请确保 XML 文件格式正确，所有元素都有开始和结束标记，否则`XMLLoader`可能会返回无效值。

应用导出设计模式，关系事务中的数据被写入指定路径的 XML 文件中。

### 附加信息

*   [http://pig .Apache。组织/文档/r 0 .11.0/API/org/Apache/PIG 库/仓储/xmlloader .html](http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/piggybank/storage/XMLLoader.html)
*   [http://pig .Apache。组织/文档/r 0 .11 . 0/API/org/Apache/pig/pig bank/storage/avro/avrostorage .html](http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/piggybank/storage/avro/AvroStorage.html)

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `chapter2/code/`
*   `chapter2/datasets/`

# JSON 进入退出模式

JSON 摄取模式描述了如何从 Hadoop 文件系统中通过 Pig Latin 摄取并输出表示为 JSON 的数据，从而进一步处理数据管道中的数据。

我们将讨论如何处理 JSON 中存储的数据与企业的相关性，并了解可以使用 Pig 访问和存储 JSON 数据的各种方式(简单 JSON 和嵌套 JSON)。您将了解使用简单 JSON 和嵌套 JSON 解析的优缺点，了解将 JSON 数据与 Pig 相结合的动机和用例。您还将学习 Pig 如何比用 Java 编写的 MapReduce 代码更直观地摄取这些数据(通过使用象鸟等外部库)。

下面关于这个模式的实现级细节的讨论旨在让您熟悉重要的概念和替代方案(如果适用的话)。一个示例代码片段用于从 Pig 语言的角度更好地理解模式，然后是使用模式的结果。

## 背景

JSON 是构造文本的另一种方式。JSON 是一种数据交换格式，它以分层的方式描述数据，这样机器和人类都可以读取数据并对其进行操作。

JSON 以一种简单得多的方式表示数据，更像是一个键值对，其中的值可以是非常原始的，比如整数、字符串和数组。JSON 不支持像 XML 这样极其复杂和嵌套的数据类型。它不太冗长，只需要一个查找函数来检索值，因为数据存储在键值对中。这使得 JSON 非常紧凑，适合更有效地表示数据，不像 XML。在 XML 中，数据以复杂的嵌套方式由丰富的数据类型表示，这使得解析 XML 树变得非常复杂。在现实世界中，JSON 用于存储简单的数据类型，而 XML 用于建模数据类型的复杂性，这提供了一些功能，使您能够更好地表达数据的结构。

### 动机

JSON 作为最受欢迎的数据表示标准之一的崛起，很大程度上是由于社交网络公司的强劲崛起，如领英、推特和脸书。这些企业和许多其他需要与外部世界交换内部业务数据(如社交对话或任何占用空间较小的数据)的公司主要转向能够在不增加 XML 复杂性的情况下承载简单高效负载的 API。JSON 是这些 API 的首选格式，因为它的简单性和作为关键数据源的实现使得解析变得容易。

随着社交媒体的兴起，我们可以看到 NoSQL 数据库的出现，这使得 JSON 成为他们的支柱。许多这样的数据库，如 MongoDB、CouchDB 和 Riak，都使用 JSON 作为它们的主要存储格式。由于 JSON 的使用，这些数据库表现出极高的性能特点和横向扩展的能力。这些数据库是专门为互联网规模的应用而设计的，其中实时响应的需求是最重要的。

以非社交媒体为中心的企业也在加速传播。目前，JSON 用于存储具有多个标题和其他键值对的日志文件。JSON 格式的日志数据很好地表示了用户会话和用户活动，每个用户活动的信息都嵌套在会话信息下。这种 JSON 格式的数据嵌套在执行高级分析时提供了天然的优势。对于处理传感器数据的企业来说，JSON 也是一个很好的选择，传感器数据包含为不同测量收集的各种属性。

虽然 JSON 作为快速检索和更高效承载互联网有效载荷的首选存储格式表现良好，但在很多用例(如日志处理、传感器分析)中，JSON 中表示的数据不仅用于搜索，还广泛集成其他企业数据资产进行分析。这种集成意味着对 JSON 和其他结构化数据的组合进行批处理。下面几节讨论的接收设计模式描述了将 JSON 接收到数据管道中的方法。

批处理数据管道的输出有时可以概括为 JSON 表示的现成数据。这适用于使用 JSON 将批处理 JSON 输出馈送到 NoSQL 数据库的用例，以及 JSON 可以用作 web 服务的负载的用例。导出设计模式展示了我们如何使用 Pig 将数据管道中存储的数据转换为 JSON 格式。

### 用例

以下是 Pig 摄取和输出 JSON 数据的用例:

*   使用摄取设计模式将 JSON 数据集成到 Pig 关系中，以便数据管道中的组合数据可以用于分析。
*   使用摄取设计模式来消费来自 Twitter 和其他社交媒体来源的 JSON 应用编程接口，以执行高级分析，例如情感挖掘。
*   存储在 JSON 中的传感器数据是使用机器故障分析的捕获设计模式捕获的。
*   使用导出设计模式以 JSON 格式存储 HDFS 现有平面文件的内容。这种模式对于以 JSON 格式存储复杂数据集成或转换管道的结果以供下游系统访问也很有用。

### 模式实现

下面部分展示了出入口的实现。

#### 进入实施

JSON 可以通过`JSONLoader`函数加载到 Pig 关系中，将 JSON 文件的内容加载到地图中。`JSONLoader`该功能可以使用或不使用模式信息。

*   如果在`JSONLoader`中提供了模式信息，那么 Pig 和 JSON 数据类型之间的映射是直接的，并且遵循指定的模式。
*   当`JSONLoader`中没有提供模式信息时，Pig 关系的数据类型设置为默认`bytearray`，在执行周期的后期推断实际模式。要处理一个大的 JSON 文件并对其执行并行处理，您可能必须用每行一个 JSON 对象格式来格式化 JSON 文件。当您需要解析超过 HDFS 存储块大小的非常大的 JSON 文件时，以及当您可以控制 JSON 的格式以每行包含一个 JSON 对象时，这个先决条件是适用的。
*   当 JSON 文件不能格式化为每行包含一个 JSON 对象时，MapReduce 无法拆分 JSON 文件，因为 JSON 是嵌套的，在不同的级别使用相同的元素。JSON 格式与 XML 形成对比，后者有一个开始和结束标记来表示嵌套数据结构的边界。这个问题的解决方案在象鸟库`LZOJSONLoader`的实现中得到了解决，允许确定嵌套 JSON 文件的划分边界。大象库是一个开源的实用程序库，用于处理 JSON 和 Twitter 提供的其他格式。在[https://github.com/kevinweil/elephant-bird](https://github.com/kevinweil/elephant-bird)有售。

#### 出口变现

使用 `JsonStorage`功能，以 JSON 格式存储 PIG 关系的内容。Pig 关系的内容在输出中存储为单个 JSON 行。执行模式映射时，`JsonStorage`函数将 Pig 元组映射到 JSON 对象。同样，它将 Pig 映射到一个 JSON 对象，Pig 包对应于 JSON 数组。

下图描述了如何使用 Pig 摄取和排出 JSON 的总体思路:

![The egress implementation](img/5556OS_02_11.jpg)

JSON Hadoop 集成

### 代码片段

下面的代码示例使用了来自安然语料库的样本数据集，该数据集包含来自 150 个用户的电子邮件，平均每个用户有 757 条消息。数据集中的字段是 T0、T1、T2、T3、T4、T5、T6 和 T7。

#### 进入码

下面的部分展示了将 JSON 格式存储的数据摄入 Pig 关系的代码及其解释。

##### 简单 JSON 的代码

使用`JsonLoader`加载 JSON 文件的代码如下:

```sh
/*
Use JSONLoader UDF, it takes in the parameter of the JSON schema and loads the contents of the JSON file emails.json into a map enron_emails
*/
enron_emails = LOAD '/user/cloudera/pdp/datasets/json/emails.json' USING JsonLoader('body:chararray, from:chararray, tos:chararray, ccs:chararray, bccs:chararray, date:chararray, message_id:chararray, subject:chararray');

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Display the contents of the relation enron_emails on the console
*/
DUMP enron_emails;
```

重要的是注意到`JsonLoader`没有使用`AS`子句来提供模式。

##### 嵌套 JSON 的代码

加载嵌套 JSON 的 PIG 脚本如下图，我们用象鸟库来实现:

```sh
/*
Register elephant-bird and JSON jar files
*/
REGISTER '/home/cloudera/pdp/jars/elephant-bird-core-3.0.5.jar';
REGISTER '/home/cloudera/pdp/jars/elephant-bird-pig-3.0.5.jar';
REGISTER '/home/cloudera/pdp/jars/json-simple-1.1.1.jar';

/*
Use ElephantBird's JsonLoader for loading a nested JSON file
The parameter –nestedload denotes nested loading operation
*/
emails = LOAD '/user/cloudera/pdp/datasets/json/emails.json' USING com.twitter.elephantbird.pig.load.JsonLoader('-nestedLoad');

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Display the contents of the relation emails on the console
*/
DUMP emails;
```

#### 导出代码

以下部分展示了与 JSON 格式相关的 Pig 中存储的导出数据的代码及其说明:

```sh
/*
Load the JSON file using JsonLoader to the relation enron_emails
*/
enron_emails = LOAD '/user/cloudera/pdp/datasets/json/emails.json' USING JsonLoader('body:chararray, from:chararray, tos:chararray, ccs:chararray, bccs:chararray, date:chararray, message_id:chararray, subject:chararray');

/*
* Some processing logic goes here which is deliberately left out to improve readability
*/

/*
Use JsonStorage to store the contents of the relation to a json file
*/
STORE enron_emails into '/user/cloudera/pdp/output/json/output.json' USING JsonStorage();
```

### 结果

应用设计模式会导致 JSON 数据以 Pig 关系存储。如果字段解析不正确或找不到字段，`JsonLoader`将在 Pig 关系中存储空值。`JsonLoader`不关心构造函数中字段的顺序；您可以按任何顺序指定它们。`JsonLoader`只要字段名匹配，就可以正确解析。如果存在类型不匹配，则根据可行性进行`JsonLoader`自动类型转换。可以把`int`铸成`string`，但不能把`string`铸成`int`。作为最佳实践，您可以考虑使用没有模式定义的`JsonLoader`函数来理解 JSON 对象中所有键的顶层视图，并更好地理解数据。

应用导出设计模式将导致来自 Pig 关系的数据以 JSON 格式存储。`JsonStorage`功能使用缓冲技术以 JSON 格式存储。这种缓冲能力用于加载大容量数据，从而提高存储性能。您可以在`JsonStorage`构造函数中以千字节为单位指定固定大小的缓冲区。

### 附加信息

*   [http://pig.apache.org/docs/r0.11.1/func.html#jsonloadstore](http://pig.apache.org/docs/r0.11.1/func.html#jsonloadstore)
*   [https://github。com/kevinkil/③](https://github.com/kevinweil/elephant-bird)
*   [http://pig.apache.org/docs/r0.11.1/func.html#jsonloadstor](http://pig.apache.org/docs/r0.11.1/func.html#jsonloadstor)

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `chapter2/code/`
*   `chapter2/datasets/`

# 总结

在本章中，我们从了解企业环境中的数据类型开始，并讨论每种数据类型的相关性、它们在企业中的使用以及 Hadoop 如何处理这些数据。

在下一节中，我们从更仔细地查看特定类型的数据开始，并将入口和出口设计模式应用于它们。我们涵盖了非结构化、结构化和半结构化类别中最相关的数据类型。我们还试图突出高级数据类型(如图像和大型机)的设计模式，以考虑 Pig 的适应性和可扩展性。在本书展示的每个设计模式中，我们从背景细节开始，了解模式的上下文相关性，然后了解模式对特定数据类型的动机和适用性。通过线程讨论了代码和模式的实现，简化了设计模式的实现。我们讨论了为什么 Pig 更好，并讨论了解决设计模式中特定情况的各种选项。

在下一章中，我们将了解更多可以应用于各种数据格式的数据分析模式。下一章的目标是让你通过分析数据，熟练运用 Pig 【T0】来理解数据的内容、脉络、结构和条件。Pig 提供了一组丰富的原语来分析数据，您将学习在分析企业级数据时使用的适当设计模式。您还将学习扩展 Pig 的功能，以适应数据分析的更高级用途。