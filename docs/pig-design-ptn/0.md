# 零、前言

这本书是实现大数据分析能力的实用指南。它指导您的大数据技术专家完成准备数据、应用分析和从数据中创造价值的过程。所有这些都是通过在 Pig 中使用适当的设计模式来完成的。我们选择 Pig 来证明它有多有用，这可以从以下几点清楚地看出:

*   Pig 通过其简单的语言构造具有内在的可修改性，非常容易学习，其可扩展性和对结构化和非结构化大数据的适用性使其成为首选。
*   Pig 可以轻松快速地实现模式，并从任何大数据中明显的随机性中获得意义，这是值得称赞的。
*   本书指导系统架构师和开发人员更熟练地使用 Pig 创建复杂的分析解决方案。它通过让他们接触各种 Pig 设计模式、UDF、工具和最佳实践来做到这一点。

通过阅读这本书，你将实现以下目标:

*   通过跨平台执行数据移动、数据接收、分析、验证、转换、数据缩减和输出，简化创建复杂数据管道的流程；你也可以在这些设计模式中使用 Pig。
*   创建解决方案，使用模式对多结构未建模数据进行探索性分析，从而从中导出结构，并将数据移动到下游系统进行进一步分析。
*   解释 Pig 如何与 Hadoop 生态系统中的其他工具共存，使用设计模式创建大数据解决方案。

# 这本书包括什么？

[第一章](1.html "Chapter 1. Setting the Context for Design Patterns in Pig")*在 PIG*中设定了设计模式的脉络，为设计模式奠定了基础。Hadoop、MapReduce 及其生态系统组件逐渐暴露了 Pig、其数据流范式以及使 Pig 工作所需的语言结构和带有几个基本示例的概念。它为了解 Pig 最适合的各种工作负载以及 Pig 如何得分提供了背景。如果你有足够的动力去学习更多关于 PIG 的知识，这一章更多的是一个快速实用的参考，并指出更多的参考。

[第二章](2.html "Chapter 2. Data Ingest and Egress Patterns")、*数据摄取输出模式*说明了处理各种数据源的数据摄取输出设计模式。本章包括具体的例子，说明了与外部系统集成的技术，该技术发送多结构和结构化的数据，并使用 Hadoop 作为接收点。本章还讨论了将数据从 Hadoop 导出到外部系统的模式。为了解释这些接收和输出模式，我们考虑了几种文件系统，包括但不限于日志文件、JSON、XML、MongoDB、Cassandra、HBase 等常见的结构化数据源。阅读本章后，您将能够更好地对企业环境中与接收和输出相关的模式进行编程，并且您可以通过使用正确的 Pig 编程或编写自己的 UDF 程序来应用这些知识来构建这些模式。

[第三章](3.html "Chapter 3. Data Profiling Patterns")、*数据分析模式*重点介绍了适用于各种数据格式的数据分析模式，并在 Pig 中实现了这些模式。这些模式包括 Pig 的不同方法、分析数据和发现数据质量问题的基本和创新统计技术。您将学习如何使用 Pig 在您的企业环境中编程类似的模式，并编写自己的 UDF 来扩展这些模式。

[第四章](4.html "Chapter 4. Data Validation and Cleansing Patterns")、*数据校验与清理模式*是关于适用于各种数据格式的数据校验与清理模式。验证模式处理约束、正则表达式和其他统计技术。数据清理模式处理简单过滤器、布隆过滤器和其他统计技术，为应用转换准备数据。

[第五章](5.html "Chapter 5. Data Transformation Patterns")、*数据转换模式*涉及 Hadoop 中应用于各种数据类型的数据转换模式。阅读本章后，您将能够选择基本转换的正确模式，并了解广泛使用的概念，例如创建连接、汇总、聚合、立方体、汇总数据、泛化和属性构造，必要时使用 Pig 的编程构造和 UDF。

[第 6 章](6.html "Chapter 6. Understanding Data Reduction Patterns")*了解数据约简模式*并解释应用于已被摄取、清理和转换的数据的数据约简模式。阅读本章后，您将能够理解并使用模式进行降维、采样技术、宁滨、聚类和无关属性约简，从而为数据分析做好准备。本章讨论了使用 Pig 语言的各种技术，并扩展了 Pig 的功能，以提供复杂的数据简化用法。

[第七章](7.html "Chapter 7. Advanced Patterns and Future Work")、*高级模式及未来工作*涉及高级数据分析模式。这些模式涵盖了 Pig 语言的可扩展性，并用案例解释了与可执行代码集成的方法，映射了用 Java 编写的 reduce 代码，来自 PiggyBank 的 UDF 等来源。高级分析涵盖与自然语言处理、聚类、分类和文本索引相关的模式。

## 这本书的动机

写这本书的灵感来源于我以工作为生，即引领大数据的企业实践，参与基于大数据技术栈的解决方案的创新和交付。

作为这个角色的一部分，我参与了许多用例、解决方案架构的试点以及许多大数据解决方案的开发。根据我的经验，Pig 是一种灵感，对于想要快速测试一个用例并向业务展示价值的用户有很大的吸引力。我用 Pig 证明了快速盈利，解决了需要不太陡峭的学习曲线的问题。同时我发现企业使用 Pig 的书面知识在某些情况下是不存在的，如果有这样的知识会广泛传播。个人认为有必要有一本基于用例模式的知识参考书。通过这本书，我想分享我的经验和教训，并从模式的角度向你传达 Pig 在解决你常见问题方面的可用性和优势。

之所以选择写 Pig 的设计模式，还有一个原因是我对 Pig 语言着迷，它的简单性、通用性和可扩展性。我一直在企业环境中寻找一个可重复的 Pig 食谱模式，这启发我将其记录下来，以便更广泛地使用。我想通过贡献一个 Pig 的模式库来传播我在使用 Pig 时学到的最佳实践。我对在各种用例中使用 Pig 的无形可能性非常感兴趣。通过这本书，我计划进一步扩大它的应用范围，让 Pig 更快乐地工作。

这本书描述了学习 PIG 的实际和实用方面。它为大数据企业中的常见挑战提供了特定的可重用解决方案。它的目标是指导您快速将 Pig 的使用映射到您的问题上下文，并从设计模式的角度设计端到端的大数据系统。

在本书中，设计模式是一组逻辑连接的企业用例，因此它们可以被分解成易于遵循的离散解决方案，并且可以由 Pig 解决。这些设计模式解决了复杂数据管道的创建、进入、退出、转换、迭代处理、合并和海量数据分析等常见的企业问题。

这本书增强了您对特定设计模式的适用性做出更好决策的能力，并使用 Pig 来实现解决方案。

Pig 拉丁语一直是复杂数据管道、数据迭代处理和研究的首选语言。所有这些用例都包括连续的步骤，在这些步骤中，数据被获取、清理、转换并提供给上游系统。成功地创建一个复杂的管道来集成来自不同结构的多个数据平台的倾斜数据，是任何企业利用大数据并通过分析从中创造价值的基石。

这本书使您能够使用这些设计模式来简化 Pig 的使用，以创建复杂的数据管道，从多个数据源获取数据，清理、分析、验证、转换并最终呈现大量数据。

本书使用用 Java 编写的 Pig 和 UDF 的集成提供了深入的解释和代码示例。每一章都包含一组设计模式来提出和解决与企业用例相关的技术挑战。这些章节彼此相对独立，可以以任何顺序完成，因为它们是根据企业中一组常见步骤的特定设计模式设计的。例如，期待解决数据转换问题的读者可以直接访问[的第 5 章和*数据转换模式*，快速开始使用本章提到的代码和说明。这本书建议你使用这些模式来解决你遇到的相同或相似的问题。如果设计模式不适合特定的情况，创建自己的模式。](5.html "Chapter 5. Data Transformation Patterns")

这本书的目的不是作为一个完整的 Pig 编程指南，而是作为一本参考书，介绍将 Pig 应用于设计模式的思想。它还旨在使您能够创造性地使用设计模式，并使用它们构建有趣的混搭。

# 这本书你需要什么？

您将需要访问单个机器(虚拟机)或多节点 Hadoop 集群来执行本书中给出的 Pig 脚本。预计将配置运行 Pig 所需的工具。我们已经用 Pig 0.11.0 测试了本书中的示例，强烈建议您安装此版本。

本书中的 UDF 代码是用不同的语言编写的，比如 Java。因此，允许您使用熟悉的开发工具访问机器是明智的，例如 Eclipse。

建议在开发人员的机器上使用 Pig Pen (Eclipse 插件)来开发和调试 Pig 脚本。

The pen can be downloaded from [https://issues. Apache. org/jira/secure/attachment/12456988/org。 Apache. Pig. Pen _ 0\. 7 .5\. Can](https://issues.apache.org/jira/secure/attachment/12456988/org.apache.pig.pigpen_0.7.5.jar) download.

# 这本书是给谁的？

这本书是为已经熟悉 Pig 的有经验的开发者写的。他们期望从用例的角度，能够把企业遇到的数据接收、分析、清理、转换、输出问题联系起来。Pig 的这些超级用户会以这本书为参考，了解 Pig 设计模式对于解决他们的问题的意义。

Hadoop 和 Pig 知识对于您更好地掌握 Pig 设计模式的复杂性是强制性的。为了解决这个问题，[的第一章和*在 PIG*](1.html "Chapter 1. Setting the Context for Design Patterns in Pig") 中设置了设计模式的上下文，其中包含了介绍性的概念，并附有简单的例子。建议读者熟悉 Java 和 Python，以便更好地理解在许多章节中用作示例的 UDF。

# 约定

在这本书里，你会发现许多区分不同种类信息的文本样式。以下是这些风格及其含义的一些例子。

正文中的码字如下:“从这一点开始，我们将调用解包后的 Hadoop 目录`HADOOP_HOME`

用 Java 编写的 UDF 代码块设置如下:

```sh
package com.pigdesignpatterns.myudfs;

public class DeIdentifyUDF extends EvalFunc<String> {

  @Override
  public String exec(Tuple input){

          try {
                String plainText = (String)input.get(0);
                String encryptKey = (String)input.get(1);
                String str="";
                str = encrypt(plainText,encryptKey.getBytes());
                return str;
              }
              catch (NullPointerException npe) {
                warn(npe.toString(), PigWarning.UDF_WARNING_2);
                return null;
              } catch (StringIndexOutOfBoundsException npe) {
                warn(npe.toString(), PigWarning.UDF_WARNING_3);
                return null;
              } catch (ClassCastException e) {
                warn(e.toString(), PigWarning.UDF_WARNING_4);
                return null;
              }
```

PIG 脚本如下所示:

```sh
Users = load 'users' as (name, age); 
Fltrd = filter Users by
 age >= 18 and age <= 25; 
Pages = load 'pages' as (user, url); 
Jnd = join Fltrd by name, Pages by user; 
Grpd = group Jnd by url; 
Smmd = foreach Grpd generate group, 
COUNT(Jnd) as clicks; 
Srtd = order Smmd by clicks desc; 
Top5 = limit Srtd 5; 
store Top5 into 'top5sites'
```

任何命令行输入或输出都编写如下:

```sh
>tar -zxvf hadoop-1.x.x.tar.gz

```

**新名词**和**重要词汇**以粗体显示。您在屏幕上看到的单词，例如菜单或对话框中的单词，出现在以下文本中:“单击**下一步**按钮进入下一个屏幕。”

### 注

否则这样的盒子里会出现一个重要的警告。

### 型

技巧和技巧是这样出现的。

# 读者反馈

欢迎读者反馈。让我们知道你对这本书的看法——你喜欢或不喜欢什么。读者的反馈对我们开发标题非常重要，你可以从中真正获得最大的好处。

要给我们发送一般反馈，只需发送电子邮件至`<[feedback@packtpub.com](mailto:feedback@packtpub.com)>`并通过您消息的主题提及书名即可。

如果您对某个主题有专业知识，并且有兴趣写作或投稿，请参考我们在[www.packtpub.com/authors](http://www.packtpub.com/authors)的作者指南。

# 客户支持

现在，您很自豪拥有一本书，我们有很多东西可以帮助您从购买中获得最大收益。

## 下载示例代码

你可以下载你在[http://www.packtpub.com](http://www.packtpub.com)账户购买的所有 Packt 书籍的样本代码文件。如果你是在外地买的这本书，可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)注册，通过邮件直接把文件发给你。本书中的例子是通过为 Pig 0 . 11 . 0 版本编译来测试的。许多 Pig 脚本、UDF 和数据都可以从发行商的网站或 GitHub 上获得。

也可以使用[https://github.com/pradeep-pasupuleti/pig-design-patterns](https://github.com/pradeep-pasupuleti/pig-design-patterns)。

PIG 脚本的例子按照章节组织在各自的目录中。Java 和 Python 的 UDF 也是章节目录的一部分，并被组织在一个名为`src`的单独子目录中。所有数据集都在数据集目录中。包括自述文件，以帮助您建立 UDF 和理解数据文件的内容。

每个脚本都是在假设输入和输出都在 HDFS 路径上的情况下编写的。

### 第三方库

为了方便起见，使用了许多第三方库。它们包含在 Maven 依赖项中，因此使用这些库不需要额外的工作。下表列出了在整个代码示例中常用的库:

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

库名

 | 

形容

 | 

戒指

 |
| --- | --- | --- |
| 数据单元 | 数据单元是用户自定义函数的集合，用于处理 Hadoop 和 Pig 中的大规模数据，尤其是数据挖掘和统计。 | [http://search.maven.org/remotecontent?file 路径= com/LinkedIn/data fu/data fu/0 .0 .10/数据 fu-0 .0 .10 .罐子](http://search.maven.org/remotecontent?filepath=com/linkedin/datafu/datafu/0.0.10/datafu-0.0.10.jar) |
| Mungo -hadoop Akin | 这是 Hadoop 的一个插件，它提供了使用 MongoDB 作为输入源和/或输出源的能力。 | [http://repo 1 .梅文 org/maven 2/org/MongoDB/Mongo-Hadoop-core _ 1 .0 . 0/1 . 0 . 0-rc0/mongo-Hadoop-core _ 1 .0 .0-1 .0 .0-rc0 .罐子](http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop-core_1.0.0/1.0.0-rc0/mongo-hadoop-core_1.0.0-1.0.0-rc0.jar) |
| 蒙古-Hadoop-PIG | 这是为了从 MongoDB 数据库中加载记录，以便它们可以在 Pig 脚本中使用并写入 MongoDB 实例。 | [http://repo 1 .梅文 org/maven 2/org/MongoDB/Mongo-Hadoop-pig/1 .0 .0/mongo-Hadoop-pig-1 .0 .0 .罐子](http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop-pig/1.0.0/mongo-hadoop-pig-1.0.0.jar) |
| Mongo-Java-驱动程序 | 这是 MongoDB 的一个 Java 驱动程序。 | [http://repo 1。苏慕恩梅文 maven .-什么:1 云娥/2 .9 .0/Java mongo 云娥-2.9.0.jar](http://repo1.maven.org/maven2/org/mongodb/mongo-java-driver/2.9.0/mongo-java-driver-2.9.0.jar) |
| 大象 PIG | 这是 Twitter 的 PIG 装载功能开源库。 | [http://repo 1 .梅文 org/maven 2/com/Twitter/象鸟/象鸟-PIG/3 .0 .5/象鸟-PIG-3.0.5.jar](http://repo1.maven.org/maven2/com/twitter/elephantbird/elephant-bird-pig/3.0.5/elephant-bird-pig-3.0.5.jar) |
| 鸟芯 | 这是推特核心实用程序的集合。 | [http://repo 1 .梅文 org/maven 2/com/Twitter/象鸟/象鸟-PIG/3 .0 .5/象鸟-PIG-3.0.5.jar](http://repo1.maven.org/maven2/com/twitter/elephantbird/elephant-bird-pig/3.0.5/elephant-bird-pig-3.0.5.jar) |
| hctalog-清管器适配器 | 它包含从 Hcatalog 管理的表中访问数据的实用程序。 | [http://search.maven.org/remotecontent?file 路径= org/Apache/h 目录/h 目录-清管器-适配器/0 .11 .0 英里/小时目录-清管器-适配器-0。11 .0 .罐子](http://search.maven.org/remotecontent?filepath=org/apache/hcatalog/hcatalog-pig-adapter/0.11.0/hcatalog-pig-adapter-0.11.0.jar) |
| cb2java | 这个罐子优酷动态解析 cobol 字帖。 | [http://SourceForge .net/project/cb2 Java/file/latest/下载](http://sourceforge.net/projects/cb2java/files/latest/download) |
| 罗欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧欧 | 这是 Avro 核心组件库。 | [http://repo 1 .梅文 org/maven 2/org/Apache/4 月/4 月/1 .7 .4/4 月-1.7.4.jar](http://repo1.maven.org/maven2/org/apache/avro/avro/1.7.4/avro-1.7.4.jar) |
| json-simple | 这个库是一个用于 JSON 编码或解码 JSON 文本的 Java 工具包。 | [http://www .java2s .com/Code/JarDownload/JSON-simple/JSON-simple-1 .1 .1 .罐子。拉链](http://www.java2s.com/Code/JarDownload/json-simple/json-simple-1.1.1.jar.zip) |
| 公地数学 | 这个库包含很少的数学和统计成分。 | [http://repo 1。苏慕恩梅文 maven .文士/maven 2/org/Apache/commons/commons-math 3/3.2/commons-math 3-3.2。jar〔t1〕t1〔t1〕](http://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar) |

### 数据集

在本书中，您将使用这些数据集为示例提供一些多样性。可以在 https://github.com/pradeep-pasupuleti/pig-design-patterns 目录的 GitHub 存储库中获得所用确切数据的副本。只要相关，章节特定数据就存在于相同 GitHub 位置下的章节特定子目录中。

以下是数据集的主要分类，它们与本书中讨论的用例相关:

*   The logs dataset contains a month's worth of HTTP requests to the NASA Kennedy Space Center WWW server in Florida. These logs are in the format of Apache access logs.

    ### 注

    数据集从链接[ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz](ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz)和[ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz](ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz)下载。

    鸣谢:日志由肯尼迪航天中心的吉姆·杜穆林收集，萨斯喀彻温大学的马丁·朴正洙(`<[mfa126@cs.usask.ca](mailto:mfa126@cs.usask.ca)>`)和凯里·威廉姆森(`<[carey@cs.usask.ca](mailto:carey@cs.usask.ca)>`)提供。

*   自定义日志数据集包含由 web 应用以自定义日志格式生成的日志。事件日志中嵌入了 Web 服务请求和响应信息。这是一个专门为说明本书中的例子而创建的复合数据集。
*   The historical NASDAQ stock data from 1970 to 2010, including daily open, close, low, high, and trading volume figures. Data is organized alphabetically by ticker symbol.

    ### 注

    本数据集下载自链接 [http://www .信息黑猩猩. com/dataset/Nasdaq-exchange-daily-1970-2010-开盘-收盘-高-低-成交量/下载量/166853](http://www.infochimps.com/datasets/nasdaq-exchange-daily-1970-2010-open-close-high-low-and-volume/downloads/166853) 。

*   客户零售交易数据集包含购买产品类别的详细信息和客户人口统计信息。这是一个专门为说明本书中的例子而创建的复合数据集。
*   汽车保险索赔数据集由两个文件组成。`automobile_policy_master.csv`单据中包含车辆的价格和为此支付的保费。文件`automobile_insurance_claims.csv`包含了汽车保险的理赔数据，具体是车辆修理费的理赔。这是一个专门为说明本书中的例子而创建的复合数据集。
*   The MedlinePlus health topic XML files contain records of health topics. Each health topic record includes data elements associated with that topic.

    ### 注

    该数据集可从链接 [http://www .健康数据. gov/data/dataset/medlineplus-health-topic-XML-files-0](http://www.healthdata.gov/data/dataset/medlineplus-health-topic-xml-files-0)下载。

*   This dataset contains a large set of e-mail messages from the Enron corpus which has about 150 users with an average of 757 messages per user; the dataset is in AVRO format and we have converted it to JSON format for the purpose of this book.

    ### 注

    本数据集下载自[链接 S3 .亚马逊 com/rjurney _ public _ web/Hadoop/Enron .avro](https://s3.amazonaws.com/rjurney_public_web/hadoop/enron.avro) 。

*   电器制造数据集是为了本书的目的而创建的复合数据集。此数据集包含以下文件:
    *   `manufacturing_units.csv`:包含每个制造单元的信息。
    *   `products.csv`:包含产品的详细信息。
    *   `manufacturing_units_products.csv`:保存不同制造单位生产的产品的详细信息。
    *   `production.csv`:这里保存生产明细。
*   非结构化文本数据集包含一些来自维基百科的关于计算机科学和信息技术、大数据、医学、电话发明、停用词列表和词典词表的文章。
*   Outlook 联系人数据集是一个复合数据集，它是出于本书的目的通过导出 Outlook 联系人而创建的；它是一个带有属性联系人名称和标题的 CSV 文件。
*   The German credit dataset in CSV format classifies people as good or bad credit risks based on a set of attributes. There are 20 attributes (7 numerical and 13 categorical) with 1,000 instances.

    ### 注

    数据集可从链接[http://archive.ics.uci.edu/ml/machine 学习-数据库/statlog/German/German.data](http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data) 下载。

    鸣谢:数据来自 UCI 机器学习知识库(http://archive . ics . UCI . edu/ml/datasets/stat og+(德语+Credit+Data))，来源:汉堡大学统计与经济研究所教授汉斯·霍夫曼博士。

## 勘误表

虽然我们尽最大努力确保内容的准确性，但错误还是会发生。如果你在我们的一本书里发现一个错误——可能是文本或代码中的错误——如果你能向我们报告，我们将不胜感激。通过这样做，你可以让其他读者免受挫折，并帮助我们改进这本书的后续版本。如有勘误，请访问[http://www.packtpub.com/support](http://www.packtpub.com/support)，选择您的图书，点击**勘误**T4【提交 **表格**链接，输入您的勘误详情。一旦您的勘误表得到验证，您的提交将被接受，勘误表将被上传到我们的网站或添加到标题勘误表部分下的任何现有勘误表列表中。

## 盗版

互联网上版权材料的盗版是所有媒体的一个老大难问题。在 Packt，我们非常重视版权和许可证的保护。如果您在互联网上遇到任何形式的非法复制我们的作品，请立即向我们提供位置地址或网站名称，以便我们寻求补救措施。

请联系我们`<[copyright@packtpub.com](mailto:copyright@packtpub.com)>`获取疑似盗版资料的链接。

我们感谢您在保护我们的作者方面的帮助，以及我们为您带来有价值内容的能力。

## 问题

如果您对本书有任何疑问，可以在`<[questions@packtpub.com](mailto:questions@packtpub.com)>`联系我们，我们会尽力解决。