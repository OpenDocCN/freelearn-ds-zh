# 六、理解数据简化模式

在前一章中，我们研究了各种大数据转换技术，这些技术处理了将数据结构转换为分层表示的问题。这是为了利用 Hadoop 处理半结构化数据的能力。在分析数据之前，我们已经看到了标准化数据的重要性。然后，我们讨论使用连接来反规范化数据。并且 CUBE 和 ROLLUP 对数据执行多次聚合；这些聚合提供了数据的快照。在数据合成部分，我们讨论了各种数值和分类数据的合成技术。

在这一章中，我们将讨论使用主成分分析技术的降维设计模式，以及使用聚类、采样和直方图技术的降维设计模式。

# 数据简化–快速介绍

数据简化旨在获得数据的简化表示。它确保了数据的完整性，尽管缩减后的数据集在体积上比原始数据集小得多。

数据简化技术分为以下三类:

*   **降维**:这组数据降维技术处理的是减少分析问题中考虑的属性数量。他们通过检测和消除不相关的属性、相关但弱的属性或冗余属性来做到这一点。主成分分析和小波变换是降维技术的例子。
*   **数值约简**:这套数据约简技术通过用数据的稀疏表示代替原始数据集来约简数据。数据的稀疏子集采用参数法计算，如回归，其中模型用于估计数据，因此只有一个子集就足够了，而不是整个数据集。还有其他方法，如非参数方法，如聚类、采样和直方图，无需建模即可工作。
*   **压缩**:这组数据缩减技术使用算法来减少数据消耗的物理存储大小。通常，压缩在比属性或记录级别更高的粒度级别上执行。如果需要从压缩数据中检索原始数据而不丢失任何信息(这在存储字符串或数字数据时是必需的)，请使用无损压缩方案。相反，如果视频和声音文件需要解压缩以适应难以察觉的清晰度损失，则使用有损压缩技术。

下图说明了上述组中使用的不同技术:

![Data reduction – a quick introduction](img/5556OS_06_01.jpg)

数据简化技术-概述

# 大数据数据约简注意事项

在大数据问题中，数据约简技术必须被视为分析过程的一部分，而不是一个单独的过程。这将使您了解哪些类型的数据必须保留或删除，因为它们与建议的分析相关问题无关。

在典型的大数据分析环境中，数据通常是从多个来源获取和整合的。尽管使用整个数据集进行分析可能会获得隐藏的回报，这可能会产生更丰富、更好的见解，但成本有时会超过结果。正是在这种情况下，您可能不得不考虑减少数据量，而不会大大降低分析意见的有效性，这本质上是为了保护数据的完整性。

由于数据量巨大，对大数据执行任何类型的分析通常都会导致高昂的存储和检索成本。当数据很小时，数据约简过程的好处有时并不明显；当数据集开始变大时，它们开始变得明显。这些数据缩减过程是从存储和检索角度优化数据的第一步。重要的是要考虑数据约简的影响，这样在数据约简上花费的计算时间就不会超过或*消除*数据挖掘对约简后的数据集节省的时间。既然我们已经理解了数据缩减的概念，我们将在下面的小节中探索一些特定的设计模式。

# 降维-主成分分析设计模式

在这个设计模式中，我们会考虑使用**主成分分析** ( **主成分分析**)和**奇异值分解** ( **奇异值分解**)来实现降维，广泛用于探索性数据分析和预测创建。

## 背景

给定数据中的维度可以直观地理解为用来解释数据观测属性的所有属性的集合。降维意味着将高维数据转换成与数据的内在或潜在维度成比例的降维集合。这些潜在维度是描述数据集所需的最小属性数。因此，降维是一种理解数据隐藏结构的方法，用于缓解高维空间的诅咒等不必要的属性。

一般来说，降维有两种方式。一种是线性降维，如主成分分析和奇异值分解。二是非线性降维，以核主成分分析和多维标度为例。

在这种设计模式中，我们通过在 R 中实现 PCA 和在 Mahout 中实现 SVD 并将其与 Pig 集成来探索线性降维。

## 动机

我们先来概述一下 PCA。主成分分析(PCA)是一种线性降维技术，通过将数据集植入低维的子空间，对给定的数据集进行无监督的处理，这是通过构建原始数据的基于方差的表示来实现的。

主成分分析的基本原理是通过分析数据变化最大的方向或数据分布最广的方向来识别数据的隐藏结构。

直观地说，主成分可以认为是一条线，它穿过一组变化较大的数据点。如果让同一条线通过数据点，没有区别，说明数据是一样的，没有携带太多信息。在没有方差的情况下，数据点不被认为是整个数据集属性的代表，这些属性可以省略。

主成分分析包括寻找数据集的成对特征值和特征向量。给定的数据集被分解成多对特征向量和特征值。特征向量定义了单位向量或垂直于其他向量的数据方向。就是特征值数据在这个方向上的分布值。

在多维数据中，可以存在的特征值和特征向量的个数等于数据的维数。特征值最大的特征向量是主成分。

找出主成分后，按照特征值降序排序，这样第一个向量显示方差最高，第二个向量显示方差次高，以此类推。这些信息有助于发现以前没有被怀疑的隐藏模式，从而允许通常不会产生的解释。

由于数据现在按重要性降序排序，因此可以通过消除具有弱分量的属性或具有小数据方差的低重要性来减少数据大小。利用高值主成分，原始数据集可以构建一个良好的近似。

例如，考虑一个对 1 亿人的抽样选举调查，这些人被问了 150 个关于他们对选举相关问题的看法的问题。分析超过 150 个属性的 1 亿个答案是一项繁琐的任务。我们有一个 150 维的高维空间，从中产生 150 个特征值/向量。我们按照重要性降序排列特征值(例如，230，160，130，97，62，8，6，4，2，1 …最多 150 个维度)。从这些数值中，我们可以理解为可以有 150 个维度，但只有前 5 个维度有变化较大的数据。有了这个，我们可以把高维空间减少 150，在下一步的分析过程中考虑前五个特征值。

接下来我们来看看 SVD。奇异值分解与主成分分析密切相关**有时两个术语都用作奇异值分解，这是实现主成分分析的一种更通用的方法。奇异值分解是矩阵分析的一种形式，它产生高维矩阵的低维表示。它通过移除线性相关数据来减少数据。和主成分分析一样，奇异值分解也利用特征值进行降维。该方法是将来自几个相关向量的信息组合成正交基向量，并解释数据中的大部分方差。**

 **例如，如果您有两个属性，一个是冰淇淋的销量，另一个是温度，它们之间的相关性非常高，以至于第二个属性“温度”不会提供任何对分类任务有用的附加信息。奇异值分解得到的特征值决定了哪些属性信息最丰富，哪些属性不能使用。

Mahout 的**随机奇异值分解** ( **ssvd** ) 是基于分布式计算数学的奇异值分解。如果`pca`参数设置为真，SSVD 在主成分分析模式下运行；该算法计算输入列均值，然后用它计算主成分分析空间。

## 用例

可以考虑用这个模式进行数据约简和数据探索，作为聚类和多元回归的输入。

设计模式可以应用于稀疏和倾斜数据的有序和无序属性。它也可以用于图像。这种设计模式不能应用于复杂的非线性数据。

## 模式实现

以下步骤描述了使用 R 实现主成分分析:

*   脚本使用主成分分析技术进行降维。主成分分析包括寻找数据集的特征值和特征向量对。特征值最大的特征向量是主成分。按组件特征值的降序排序。
*   该脚本加载数据，并使用流调用 r 脚本。r 脚本对数据执行 PCA 并返回主成分。只能选择能够解释大部分变化的前几个主成分，从而降低了数据的维度。

### 主成分分析的局限性

虽然流允许你调用自己选择的可执行文件，但是会影响性能，在输入数据集很大的情况下，解决方案是不可伸缩的。为了克服这一点，我们提出了一种更好的利用 Mahout 进行降维的方法。它包含一组高度可扩展的机器学习库。

以下步骤描述了 SSVD 在 Mahout 上的实现:

*   以 CSV 格式读取输入数据集，以键/值对的形式准备一组数据点；关键字应该是唯一的，值应该由 *n* 个向量元组组成。
*   将之前的数据写入序列文件。按键可以是`WritableComparable`、`Long`或者`String`类型，数值应该是`VectorWritable`类型。
*   确定缩减空间的维度。
*   在 Mahout 上使用`rank`参数执行 SSVD(这指定了尺寸)，并将`pca`、`us`和`V`设置为真。当`pca`参数设置为真时，算法在主成分分析模式下运行，计算输入列均值，然后用它计算主成分分析空间。`USigma`文件夹包含缩小后的输出。

一般来说，降维应用于非常高维的数据集；然而，在我们的例子中，为了更好地解释，我们在较少维度的数据集上演示了这一点。

## 代码片段

为了说明该模型的工作原理，我们考虑存储在 **Hadoop 文件系统** ( **HDFS** )中的零售交易数据集。包含`Transaction ID`、`Transaction date`、`Customer ID`、`Product subclass`、`Phone No`、`Product ID`、`age`、`quantity`、`asset`、`Transaction Amount`、`Service Rating`、`Product Rating`、`Current Stock`等 20 个属性。对于这个模型，我们将使用主成分分析来降维。下面的代码片段是一个 Pig 脚本，它演示了通过 Pig 流实现这种模式:

```sh
/*
Assign an alias pcar to the streaming command
Use ship to send streaming binary files (R script in this use case) from the client node to the compute node
*/
DEFINE pcar '/home/cloudera/pdp/data_reduction/compute_pca.R' ship('/home/cloudera/pdp/data_reduction/compute_pca.R'); 

/*
Load the data set into the relation transactions
*/
transactions = LOAD '/user/cloudera/pdp/datasets/data_reduction/transactions_multi_dims.csv' USING  PigStorage(',') AS (transaction_id:long, transaction_date:chararray, customer_id:chararray, prod_subclass:chararray, phone_no:chararray, country_code:chararray, area:chararray, product_id:chararray, age:int, amt:int, asset:int, transaction_amount:double, service_rating:int, product_rating:int, curr_stock:int, payment_mode:int, reward_points:int, distance_to_store:int, prod_bin_age:int, cust_height:int);
/*
Extract the columns on which PCA has to be performed.
STREAM is used to send the data to the external script.
The result is stored in the relation princ_components
*/
selected_cols = FOREACH transactions GENERATE age AS age, amt AS amount, asset AS asset, transaction_amount AS transaction_amount, service_rating AS service_rating, product_rating AS product_rating, curr_stock AS current_stock, payment_mode AS payment_mode, reward_points AS reward_points, distance_to_store AS distance_to_store, prod_bin_age AS prod_bin_age, cust_height AS cust_height;
princ_components = STREAM selected_cols THROUGH pcar;

/*
The results are stored on the HDFS in the directory pca
*/
STORE princ_components INTO '/user/cloudera/pdp/output/data_reduction/pca';
```

下面是说明这种模式的实现的代码:

```sh
#! /usr/bin/env Rscript
options(warn=-1)

#Establish connection to stdin for reading the data
con <- file("stdin","r")

#Read the data as a data frame
data <- read.table(con, header=FALSE, col.names=c("age", "amt", "asset", "transaction_amount", "service_rating", "product_rating", "current_stock", "payment_mode", "reward_points", "distance_to_store", "prod_bin_age", "cust_height"))
attach(data)

#Calculate covariance and correlation to understand the variation between the independent variables
covariance=cov(data, method=c("pearson"))
correlation=cor(data, method=c("pearson"))

#Calculate the principal components
pcdat=princomp(data)
summary(pcdat)
pcadata=prcomp(data, scale = TRUE)
pcadata
```

下面的代码片段说明了使用 Mahout 的 SSVD 实现这个模式。以下是 shell 脚本的一个片段，其中包含执行 CSV 到序列转换器的命令:

```sh
#All the mahout jars have to be included in HADOOP_CLASSPATH before execution of this script. 
#Execute csvtosequenceconverter jar to convert the CSV file to sequence file.
hadoop jar csvtosequenceconverter.jar com.datareduction.CsvToSequenceConverter /user/cloudera/pdp/datasets/data_reduction/transactions_multi_dims_ssvd.csv /user/cloudera/pdp/output/data_reduction/ssvd/transactions.seq
```

以下是 Pig 脚本的代码片段，其中包含在 Mahout 上执行 SSVD 的命令:

```sh
/*
Register piggybank jar file
*/
REGISTER '/home/cloudera/pig-0.11.0/contrib/piggybank/java/piggybank.jar';

/*
*Ideally the following data pre-processing steps have to be generally performed on the actual data, we have deliberately omitted the implementation as these steps were covered in the respective chapters

*Data Ingestion to ingest data from the required sources

*Data Profiling by applying statistical techniques to profile data and find data quality issues

*Data Validation to validate the correctness of the data and cleanse it accordingly

*Data Transformation to apply transformations on the data.
*/

/*
Use sh command to execute shell commands.
Convert the files in a directory to sequence files
-i specifies the input path of the sequence file on HDFS
-o specifies the output directory on HDFS
-k specifies the rank, i.e the number of dimensions in the reduced space
-us set to true computes the product USigma
-V set to true computes V matrix
-pca set to true runs SSVD in pca mode
*/

sh /home/cloudera/mahout-distribution-0.8/bin/mahout ssvd -i /user/cloudera/pdp/output/data_reduction/ssvd/transactions.seq -o /user/cloudera/pdp/output/data_reduction/ssvd/reduced_dimensions -k 7 -us true -V true -U false -pca true -ow -t 1

/*
Use seqdumper to dump the output in text format.
-i specifies the HDFS path of the input file
*/
sh /home/cloudera/mahout-distribution-0.8/bin/mahout seqdumper -i /user/cloudera/pdp/output/data_reduction/ssvd/reduced_dimensions/V/v-m-00000
```

## 结果

以下是通过 Pig 流执行 R 脚本的结果片段。为了提高可读性，只显示结果的重要部分。

```sh
Importance of components:
                             Comp.1      Comp.2       Comp.3
Standard deviation     1415.7219657 548.8220571 463.15903326
Proportion of Variance    0.7895595   0.1186566   0.08450632
Cumulative Proportion     0.7895595   0.9082161   0.99272241
```

下图显示了结果的图形表示:

![Results](img/5556OS_06_02.jpg)

主成分分析输出

从累积的结果来看，我们可以用前三个组成部分来解释大部分的变化。因此，我们可以去掉其他组件，仍然解释大部分数据，从而实现数据约简。

以下是在 Mahout 上应用 SSVD 后获得的结果的代码片段:

```sh
Key: 0: Value: {0:6.78114976729216E-5,1:-2.1865954292525495E-4,2:-3.857078959222571E-5,3:9.172780131217343E-4,4:-0.0011674781643860148,5:-0.5403803571549012,6:0.38822546035077155}
Key: 1: Value: {0:4.514870142377153E-6,1:-1.2753047299542729E-5,2:0.002010945408634006,3:2.6983823401328314E-5,4:-9.598021198119562E-5,5:-0.015661212194480658,6:-0.00577713052974214}
Key: 2: Value: {0:0.0013835831436886054,1:3.643672803676861E-4,2:0.9999962672043754,3:-8.597640675661196E-4,4:-7.575051881399296E-4,5:2.058878196540628E-4,6:1.5620427291943194E-5}
.
.
Key: 11: Value: {0:5.861358116239576E-4,1:-0.001589570485260711,2:-2.451436184622473E-4,3:0.007553283166922416,4:-0.011038688645296836,5:0.822710349440101,6:0.060441819443160294}
```

`V`文件夹的内容显示了原始变量对每个主成分的贡献。结果是一个 12×7 的矩阵，因为在我们的原始数据集中有 12 个维度，根据 SSVD 秩参数，这些维度被减少到 7 个。

`USigma`文件夹包含缩小尺寸的输出。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter6/code/`
*   `Chapter6/datasets/`

关于 Mahout 实现 SSVD 的信息可以在以下链接中找到:

*   [https://cwiki。Apache。组织/合流/显示/MAHOUT/随机+奇点+值+分解](https://cwiki.apache.org/confluence/display/MAHOUT/Stochastic+Singular+Value+Decomposition)
*   [https://cwiki .Apache org/converge/download/attachments/27832158/SSVD-CLI .pdf？版本=18 &修改日期=1381347063000 & api=v2](https://cwiki.apache.org/confluence/download/attachments/27832158/SSVD-CLI.pdf?version=18&modificationDate=1381347063000&api=v2)
*   [http://en。 wikibooks。 Organization/Wiki/Data Mining Algorithm Dimension Reduction/Singular Value _ Decomposition](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition)

# 递减数值-直方图设计模式

*数值化简-直方图设计模式*探索 直方图技术在数据化简中的实现。

## 背景

直方图属于数据约简的数值约简范畴。它们是非参数数据约简方法，其中假设数据不适合预定义的模型或函数。

## 动机

直方图的工作原理是将整个数据划分成桶或组，存储每个桶的中心趋势。在内部，这类似于宁滨。直方图可以通过动态编程进行优化。直方图不同于条形图，因为它们代表连续的数据类别，而不是离散的类别。这意味着直方图中代表不同类别的列之间没有间隙。

直方图通过将大量连续属性分组来帮助减少数据的类别。表示大量属性可能会导致复杂的直方图，有太多的列来解释信息。因此，数据被分组到一个范围中，该范围代表属性值的连续范围。数据可以按以下方式分组:

*   **等宽分组技术**:在这个分组技术中，每个区间都是等宽的。
*   **等频(或等深)分组技术**:在等频分组技术中，以每个范围的频率恒定或每个范围包含相同数量的连续数据元素的方式创建范围。
*   **V-最优分组技术**:在这个分组技术中，我们考虑给定范围内所有可能的直方图，选择方差最小的直方图。
*   **Maxdiff 分组技术**:这种直方图分组技术根据每对相邻值之间的差异考虑将值分组到一个范围内。边界被定义在具有最大差异的每对相邻点之间。下图描述了根据 9-14 和 18-27 之间的最大差异分为三个范围的分类数据。

![Motivation](img/5556OS_06_07.jpg)

最大差异-图标

在前面提到的分组技术中，V-Optimal 和 MaxDiff 技术对于接近稀疏和密集的数据以及高度偏斜和均匀的数据更加准确和有效。这些直方图也可以通过使用多维直方图来处理多个属性，多维直方图可以捕捉属性之间的依赖关系。

## 用例

这种设计模式在以下情况下可以考虑:

*   当数据不适用于回归或对数线性模型等参数模型时
*   当数据是连续的而不是离散的时
*   当数据具有有序或无序的数字属性时
*   当数据倾斜或稀疏时

## 模式实现

脚本加载数据，并使用等宽分组将数据分成桶。`Transaction Amount`字段的数据被分组到桶中。它计算每个存储桶中的事务数量，并返回存储桶范围和计数作为输出。

这种模式产生了数据集的简化表示，其中事务量被划分为指定数量的桶，事务计数在这个范围内。这些数据被绘制成直方图。

## 代码片段

为了解释这个模型是如何工作的，我们考虑存储在 HDFS 的零售交易数据集。包含 T0、T1、T2、T3、T4、T5、T6、T7、T8、T9 等属性。对于这个模式，我们将在属性`Transaction Amount`上生成桶。下面的代码片段是一个 Pig 脚本，演示了这种模式的实现:

```sh
/*
Register the custom UDF
*/
REGISTER '/home/cloudera/pdp/jars/databucketgenerator.jar';

/*
Define the alias generateBuckets for the custom UDF, the number of buckets(20) is passed as a parameter
*/
DEFINE generateBuckets com.datareduction.GenerateBuckets('20');

/*
Load the dataset into the relation transactions
*/
transactions = LOAD '/user/cloudera/pdp/datasets/data_reduction/transactions.csv' USING  PigStorage(',') AS (transaction_id:long,transaction_date:chararray, cust_id:chararray, age:chararray, area:chararray, prod_subclass:int, prod_id:long, quantity:int, asset:int, transaction_amt:double, phone_no:chararray, country_code:chararray);

/*
Maximum value of transactions amount and the actual transaction amount are passed to generateBuckets UDF
The UDF calculates the bucket size by dividing maximum transaction amount by the number of buckets.
It finds out the range to which each value belongs to and returns the value along with the bucket range
*/
transaction_amt_grpd = GROUP transactions ALL;
transaction_amt_min_max = FOREACH transaction_amt_grpd GENERATE MAX(transactions.transaction_amt) AS max_transaction_amt,FLATTEN(transactions.transaction_amt) AS transaction_amt;
transaction_amt_buckets = FOREACH transaction_amt_min_max GENERATE generateBuckets(max_transaction_amt,transaction_amt) ;

/*
Calculate the count of values in each range
*/
transaction_amt_buckets_grpd = GROUP transaction_amt_buckets BY range;
transaction_amt_buckets_count = FOREACH transaction_amt_buckets_grpd GENERATE group, COUNT(transaction_amt_buckets);

/*
The results are stored on HDFS in the directory histogram.
*/
STORE transaction_amt_buckets_count INTO '/user/cloudera/pdp/output/data_reduction/histogram';
```

下面的代码片段是 Java UDF 代码，演示了这种模式的实现:

```sh
@Override
  public String exec(Tuple input) throws IOException {
    if (input == null || input.size() ==0)
      return null;
    try{
      //Extract the maximum transaction amount
      max = Double.parseDouble(input.get(0).toString());
      //Extract the value
      double rangeval = Double.parseDouble(input.get(1).toString());
      /*Calculate the bucket size by dividing maximum 
        transaction amount by the number of buckets.
      */
      setBucketSize();

      /*Set the bucket range by using the bucketSize and 
        noOfBuckets
      */
      setBucketRange();

      /*
      It finds out the range to which each value belongs 
      to and returns the value along with the bucket range
      */
      return getBucketRange(rangeval);
    } catch(Exception e){
      System.err.println("Failed to process input; error - " + e.getMessage());
      return null;
    }
```

## 结果

以下是将该模式应用于数据集的结果片段；第一列是`Transaction Amount`属性的时间范围，第二列是交易计数:

```sh
1-110        45795
110-220      50083
220-330      60440
330-440      40001
440-550      52802

```

下面是使用 gnuplot 绘制这些数据时生成的直方图。它以图形方式显示了交易金额期间和每个期间的交易数量。

![Results](img/5556OS_06_03.jpg)

输出直方图

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter6/code/`
*   `Chapter6/datasets/`

# 递减数值-抽样设计模式

这个设计模式探索了数据约简的采样技术的实现。

## 背景

采样 属于数据约简的数值约简范畴。它可以用作数据缩减技术，因为它使用小得多的子集来表示大量数据。

## 动机

采样本质上是一种数据约简方法，确定具有整体种群特征的种群的近似子集。抽样是一种选择数据子集以准确代表人口的通用方法。采样通过各种方法执行，这些方法定义子集的内容，并以不同的方式定位子集的候选项。

在大数据场景下，分析整个群体的成本(比如分类、优化)非常高；采样有助于降低成本，因为它减少了用于执行实际分析的数据空间，然后根据整体情况推断结果。准确性会略有下降，但这远远超过了减少时间和存储之间权衡的好处。

说到大数据，无论统计抽样技术应用在哪里，识别要分析的人都很重要。即使收集到的数据非常大，样本也可能只与人口中的一小部分相关，并不代表全部。在选择样本时，代表性起着至关重要的作用，因为它决定了抽样数据与总体的接近程度。

可以使用概率和非概率方法进行采样。下图显示了采样技术的概况:

![Motivation](img/5556OS_06_04.jpg)

抽样法

**概率抽样**方法使用随机抽样，总体中的每个元素都有已知的非零(大于零)机会被选入抽样子集中。概率抽样方法利用加权抽样得到总体的无偏样本。以下是一些概率抽样方法:

*   **简单随机抽样**:这是最基本的抽样类型。在这个抽样中，群体中的每个元素都有同等的机会被选入一个子集。样本是客观随机选取的。简单的随机抽样可以通过替换总体中的选定项目，以便它们可以被再次选择(带替换的抽样)或通过不替换总体中的选定项目(不带替换的抽样)来完成。随机抽样并不总是产生有代表性的样本，在非常大的数据集上执行这种操作是一种昂贵的操作。采用分层或聚类的方法对人群进行预抽样，可以提高随机抽样的代表性。
*   The following diagram illustrates the difference between the **Simple Random Sampling Without Replacement** (**SRSWOR**) and **Simple Random Sampling With Replacement** (**SRSWR**).

    ![Motivation](img/5556OS_06_05.jpg)

    斯沃弗 vs 斯沃弗

*   **分层抽样**:这个抽样技术是在我们已经知道种群包含很多独特的类别时使用的，用来将种群组织成子种群(地层)；然后可以从中选择一个样本。所选样本必须包含每个子群体的元素。这种抽样方法侧重于相关的子组，而忽略了不相关的子组。通过消除绝对随机性，增加了样本的代表性，这一点可以通过简单的随机抽样和从独立类别中选择项目来证明。当预先确定地层的独特类型时，分层抽样是一种更有效的抽样技术。分层有一个总的时间成本权衡，因为最初为相对同质的人识别独特的类别可能很无聊。
*   **非概率抽样**:这种抽样方法选择的是人群的一个子集，但并没有给人群中的某些元素同样的选择机会。在这种抽样中，不能准确确定选择元素的概率。元素的选择纯粹是基于对感兴趣的人的一些假设。非概率抽样得分太低，无法准确代表总体，因此无法将分析从样本外推至总体。非概率抽样方法包括协方差抽样、判断抽样和定额抽样。

## 用例

您可以考虑在以下场景中使用数字下采样设计模式:

*   当数据是连续的或离散的时
*   当数据的每个元素都有同等的机会被选择而不影响抽样的代表性时
*   当数据具有有序或无序的属性时

## 模式实现

这个设计模式在 Pig 中作为独立脚本实现。它使用`datafu`库，把 SRSWR 的实现看成是一对 UDF、`SimpleRandomSampleWithReplacementElect`和`SimpleRandomSampleWithReplacementVote`；他们为 SRSWR 实现了一个可扩展的算法。该算法包括投票和选举两个阶段。每个职位的候选人在投票阶段投票。在选举阶段，每个职位选举一名候选人。输出是一包采样数据。

该脚本使用 SRSWR 技术从交易数据集中选择 100，000 条记录的样本。

## 代码片段

为了说明 T10 模型的工作原理，我们考虑存储在 HDFS 的零售交易数据集。包含 T0、T1、T2、T3、T4、T5、T6、T7、T8、T9 等属性。在这种模式下，我们将对事务数据集执行 SRSWR。下面的代码片段是一个 Pig 脚本，演示了这种模式的实现:

```sh
/*
Register datafu and commons math jar files
*/
REGISTER '/home/cloudera/pdp/jars/datafu-1.2.0.jar';
REGISTER '/home/cloudera/pdp/jars/commons-math3-3.2.jar';

/*
Define aliases for the classes SimpleRandomSampleWithReplacementVote and SimpleRandomSampleWithReplacementElect
*/
DEFINE SRSWR_VOTE  datafu.pig.sampling.SimpleRandomSampleWithReplacementVote();
DEFINE SRSWR_ELECT datafu.pig.sampling.SimpleRandomSampleWithReplacementElect();

/*
Load the dataset into the relation transactions
*/
transactions= LOAD '/user/cloudera/pdp/datasets/data_reduction/transactions.csv' USING  PigStorage(',') AS (transaction_id:long,transaction_date:chararray, cust_id:chararray, age:int, area:chararray, prod_subclass:int, prod_id:long, quantity:int, asset:int, transaction_amt:double, phone_no:chararray, country_code:chararray);

/*
The input to Vote UDF is the bag of items, the desired sample size (100000 in our use case) and the actual population size.
  This UDF votes candidates for each position
*/
summary = FOREACH (GROUP transactions ALL) GENERATE COUNT(transactions) AS count;
candidates = FOREACH transactions GENERATE FLATTEN(SRSWR_VOTE(TOBAG(TOTUPLE(*)), 100000, summary.count));

/*
The Elect UDF elects one candidate for each position and returns a bag of sampled items stored in the relation sampled
*/
sampled = FOREACH (GROUP candidates BY position PARALLEL 10) GENERATE FLATTEN(SRSWR_ELECT(candidates));

/*
The results are stored on the HDFS in the directory sampling
*/
STORE sampled into '/user/cloudera/pdp/output/data_reduction/sampling';
```

## 结果

以下是采样交易数据后得到的结果片段。为了提高可读性，我们删除了一些列。

```sh
580493 … 1621624 … … … … 1 115 576 900-435-5791 U.S.A
193016 … 1808643 … … … … 1 119 735 9020138550 U.S.A
800748 … 199995 … … … … 1 28 1577 904-066-467q USA
```

结果是一个包含 100，000 条记录的文件，作为原始数据集的样本。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter6/code/`
*   `Chapter6/datasets/`

# 减量-集群设计模式

这种设计模式探索了聚类技术在数据约简中的实现。

## 背景

聚类 属于数据约简的数值约简范畴。聚类是一种非参数模型，它使用无监督学习在没有类别标签先验知识的情况下工作。

## 动机

聚类是解决数据分组问题的一种通用方法。这可以通过各种算法来实现，这些算法在定义什么进入一个组以及如何找到该组的候选人方面是不同的。聚类算法有 100 多种不同的实现方式，可以针对不同的目标解决各种问题。对于给定的问题，没有单一的规模适合所有的聚类算法；我们必须通过仔细的实验选择正确的。适用于特定数据模型的聚类算法并不总是适用于不同的模型。聚类广泛应用于机器学习、图像分析、模式识别和信息检索。

聚类的目标是基于一组启发式算法对数据集进行划分，并有效地减小其大小。集群在某种程度上类似于宁滨，因为它模仿了宁滨的分组方法；然而，区别在于聚类中分组的精确方式。

分区的实现方式是一个集群中的数据与同一个集群中的另一个数据相似，但与其他集群中的其他数据不同。这里，相似性被定义为数据彼此有多接近的度量。

K-means 是应用最广泛的聚类方法之一。用聚类分析的 k 种方法将观测值分成 k 个聚类；这里，每个观测值都属于平均值最近的聚类。这是一个迭代过程，只有当集群质心不再移动时，这个过程才会稳定。

可以通过测量每个聚类对象距聚类质心的直径或平均距离来确定聚类执行得如何的质量度量。

通过一家服装公司计划向市场发布新 t 恤的例子，我们可以直观地理解聚类减少数据量的必要性。如果公司不使用数据还原技术，最终会做出不同尺寸的 t 恤来迎合不同的人。为了防止这种情况，他们减少了数据。首先，他们记录了人们的身高和体重，绘制在图表上，并将其分为三类:小、中、大。

K-Means 方法使用身高和体重的数据集( *n* 个观测值)并将其划分为 *k* (即三个聚类)。对于每个聚类(小、中、大)，聚类中的数据点更接近聚类类别(即小身高、小体重的平均值)。K Means 为我们提供了三种最适合每个人的尺寸，从而有效降低了数据的复杂度；集群使我们能够自己替换实际数据，而不是处理实际数据。

### 注

我们已经考虑使用 Mahout 的 K-Means 实现；更多信息可从[https://mahout . Apache . org/users/clustering/k-means-clustering . html](https://mahout.apache.org/users/clustering/k-means-clustering.html)获取。

从大数据的角度来看，由于需要处理大量的数据，在选择聚类算法时需要考虑时间和质量的权衡。正在进行新的研究，以开发一种能够高效处理大数据的预聚类方法。然而，预聚类方法的结果是原始数据集的近似预划分，最终将通过传统方法(如 K-means)再次聚类。

## 用例

这种设计模式在以下情况下可以考虑:

*   当数据是连续的或离散的并且数据的类别标签事先未知时
*   当需要通过聚类对数据进行预处理并最终对大量数据进行分类时
*   当数据具有数字、有序或无序属性时
*   当数据是绝对的
*   当数据不偏斜、稀疏或模糊时

## 模式实现

设计模式在 Pig 和 Mahout 中实现。数据集被载入 Pig。要对其执行 K 均值聚类的年龄属性被转换成向量，并以 Mahout 可读格式存储。它将 Mahout 的 K 均值聚类应用于事务数据集的年龄属性。k 均值聚类将观测值划分为 *k* 个聚类，其中每个观测值都属于均值最近的聚类；只有当聚类质心不再移动时，该过程才是迭代和稳定的。

该模式产生数据集的简化表示，其中`age`属性被分成预定数量的簇。该信息可用于识别光顾该商店的顾客的年龄组。

## 代码片段

为了说明的工作原理，我们考虑存储在 HDFS 的零售交易数据集。包含 T0、T1、T2、T3、T4、T5、T6、T7、T8、T9 等属性。对于这种模式，我们将对`age`属性进行 K 均值聚类。下面的代码片段是一个 Pig 脚本，演示了这种模式的实现:

```sh
/*
Register the required jar files
*/
REGISTER '/home/cloudera/pdp/jars/elephant-bird-pig-4.3.jar';
REGISTER '/home/cloudera/pdp/jars/elephant-bird-core-4.3.jar';
REGISTER '/home/cloudera/pdp/jars/elephant-bird-mahout-4.3.jar';
REGISTER '/home/cloudera/pdp/jars/elephant-bird-hadoop-compat-4.3.jar';
REGISTER '/home/cloudera/mahout-distribution-0.7/lib/json-simple-1.1.jar';
REGISTER '/home/cloudera/mahout-distribution-0.7/lib/guava-r09.jar';
REGISTER '/home/cloudera/mahout-distribution-0.7/mahout-examples-0.7-job.jar'; 
REGISTER '/home/cloudera/pig-0.11.0/contrib/piggybank/java/piggybank.jar';

/*
Use declare to create aliases.
declare is a preprocessor statement and is processed before running the script
*/
%declare SEQFILE_LOADER 'com.twitter.elephantbird.pig.load.SequenceFileLoader';
%declare SEQFILE_STORAGE 'com.twitter.elephantbird.pig.store.SequenceFileStorage';
%declare VECTOR_CONVERTER 'com.twitter.elephantbird.pig.mahout.VectorWritableConverter';
%declare TEXT_CONVERTER 'com.twitter.elephantbird.pig.util.TextConverter';

/*
Load the dataset into the relation transactions
*/
transactions = LOAD '/user/cloudera/pdp/datasets/data_reduction/transactions.csv' USING  PigStorage(',') AS (id:long,transaction_date:chararray, cust_id:int, age:int, area:chararray, prod_subclass:int, prod_id:long, quantity:int, asset:int, transaction_amt:double, phone_no:chararray, country_code:chararray);

/*
Extract the columns on which clustering has to be performed
*/
age = FOREACH transactions GENERATE id AS tid, 1 AS index, age AS cust_age;

/*
Generate tuples from the parameters
*/
grpd = GROUP age BY tid;
vector_input = FOREACH grpd generate group, org.apache.pig.piggybank.evaluation.util.ToTuple(age.(index, cust_age));
/*
Use elephant bird functions to store the data into sequence file (mahout readable format)
cardinality represents the dimension of the vector.
*/
STORE vector_input INTO '/user/cloudera/pdp/output/data_reduction/kmeans_preproc' USING $SEQFILE_STORAGE (
 '-c $TEXT_CONVERTER', '-c $VECTOR_CONVERTER -- -cardinality 100'
);
```

下面的是一个 shell 脚本的片段，其中包含对 Mahout 执行 K-means 聚类的命令:

```sh
#All the mahout jars have to be included in classpath before execution of this script.
#Create the output directory on HDFS before executing VectorConverter
hadoop fs -mkdir /user/cloudera/pdp/output/data_reduction/kmeans_preproc_nv

#Execute vectorconverter jar to convert the input to named vectors
hadoop jar /home/cloudera/pdp/data_reduction/vectorconverter.jar com.datareduction.VectorConverter /user/cloudera/pdp/output/data_reduction/kmeans_preproc/ /user/cloudera/pdp/output/data_reduction/kmeans_preproc_nv/

#The below Mahout command shows the usage of kmeans. The algorithm takes the input vectors from the path specified in the -i argument, it chooses the initial clusters at random, -k argument specifies the number of clusters as 3, -x specified the maximum number of iterations as 15\. -dm specifies the distance measure to use i.e euclidean distance and a convergence threshold specified in -cd as 0.1
/home/cloudera/mahout-distribution-0.7/bin/mahout kmeans -i /user/cloudera/pdp/output/data_reduction/kmeans_preproc_nv/ -c kmeans-initial-clusters -k 3 -o /user/cloudera/pdp/output/data_reduction/kmeans_clusters -x 15 -ow -cl -dm org.apache.mahout.common.distance.EuclideanDistanceMeasure -cd 0.01

# Execute cluster dump command to print information about the cluster
/home/cloudera/mahout-distribution-0.7/bin/mahout clusterdump --input /user/cloudera/pdp/output/data_reduction/kmeans_clusters/clusters-4-final --pointsDir /user/cloudera/pdp/output/data_reduction/kmeans_clusters/clusteredPoints --output age_kmeans_clusters
```

## 结果

以下是在事务数据集上应用该模式的结果片段:

```sh
VL-817732{n=309263 c=[1:45.552] r=[1:4.175]}
  Weight : [props - optional]:  Point:
1.0: 1 = [1:48.000]
  1.0: 2 = [1:42.000]
  1.0: 3 = [1:42.000]
  1.0: 4 = [1:41.000]
VL-817735{n=418519 c=[1:32.653] r=[1:4.850]}
  Weight : [props - optional]:  Point:
  1.0: 5 = [1:24.000]
  1.0: 7 = [1:38.000]
  1.0: 12 = [1:34.000]
  1.0: 14 = [1:23.000]
VL-817738{n=89958 c=[1:65.198] r=[1:5.972]}
  Weight : [props - optional]:  Point:
  1.0: 6 = [1:66.000]
  1.0: 8 = [1:58.000]
  1.0: 16 = [1:62.000]
  1.0: 24 = [1:74.000]
```

`VL-XXXXXX`是收敛聚类的聚类标识，`c`是质心和向量，`n`是聚类的点数，`r`是半径和向量。根据 K-means 命令，数据被分为三个簇。当这个数据被可视化时，我们可以推断 41 和 55 之间的值被分组在组 1 下，20 和 39 被分组在组 2 下，56 和 74 被分组在组 3 下。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter6/code/`
*   `Chapter6/datasets/`

# 总结

在本章中，您学习了各种旨在获得数据简化表示的数据简化技术。我们探索了使用主成分分析技术降维和使用聚类、采样和直方图技术降维的设计模式。

在下一章中，您将探索用 Pig 模拟社交媒体数据的高级模型，并使用文本分类和其他相关技术来更好地理解上下文。我们也将了解未来 PIG 语将如何演变。**