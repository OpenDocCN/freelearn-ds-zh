# 三、数据分析模式

|   | *花时间看数据总是值得的。* |   |
|   | -证人，等等。 |

在前一章中，您研究了从 Hadoop 生态系统接收和输出不同类型数据的各种模式，以便开始分析过程的下一个逻辑步骤。在本章中，我们将了解与数据分析相关的最广泛使用的设计模式。本章将逐步诊断数据集是否有问题，最后将数据集转化为可用信息。

通过了解数据的内容、上下文、结构和条件，数据分析是深入了解 Hadoop 所吸收的数据的必要的第一步。

本章描述的数据分析设计模式在开始将数据清理为更有用的形式之前，收集 Hadoop 集群中关于数据属性的重要信息。在本章中，我们将查看以下部分:

*   理解大数据背景下数据分析的概念和意义。
*   数据分析中使用 Pig 的基本原理
*   理解数据类型推理设计模式
*   了解基本的统计分析设计模式。
*   理解模式匹配设计模式
*   理解字符串分析设计模式
*   理解非结构化文本分析的设计模式

# 大数据的数据分析

不良数据隐藏在 Hadoop 吸收的所有数据中，但随着大数据数量和类型的惊人增加，不良数据的影响也在不断扩大。处理丢失的记录、格式错误的值和格式错误的文件会增加浪费的时间。令我们沮丧的是，我们看到了即使我们拥有也无法使用的数据量，我们手头拥有但随后丢失的数据，以及与昨天不同的数据。在大数据分析项目中，通常会收到一个非常大的数据集，但是关于它来自哪里、如何收集、字段的含义等信息并不多。在许多情况下，数据自收集以来经历了许多手和许多转换，没有人真正知道这一切意味着什么。

分析是对后续步骤中数据质量和数据处理适用性的度量。只是说明数据有问题。数据分析首先是对数据进行短期爆发式分析，以确定其适用性，了解挑战，并在数据密集型工作的早期阶段做出是否外出的决定。数据分析活动从定性的角度为您提供关于 Hadoop 吸收哪些数据的关键见解，并在获得任何分析见解之前评估将数据与其他来源集成所涉及的风险。有时，分析过程在分析过程的不同阶段进行，以消除不良数据并改进分析本身。

数据分析通过帮助我们从业务角度而不是分析角度理解数据，在提高整体数据质量、可读性和可加工性方面发挥了重要作用。在大数据信息管理平台(如 Hadoop)中构建数据分析框架，可以保证数据质量不会影响报告、分析、预测等决策所需的关键业务需求的结果。

传统上，数据汇总分析是根据其预期用途进行的，其中使用数据的目的是预先定义的。对于大数据项目来说，数据可能不得不以一种意想不到的方式被使用，必须进行相应的分析来解决如何重用数据的问题。这是因为大多数大数据项目处理对定义不明确的数据的探索性分析，这种分析试图找出如何使用数据或重新调整数据用途。要做到这一点，必须明确规定各种质量措施，如**完整性**、**一致性**、**一致性**、**正确性**、**及时性**、**合理性**。

在大数据项目中，采集元数据确定数据质量。元数据包括以下内容:

*   数据质量属性
*   商业规则
*   起草
*   清洁程序
*   要素汇总和测量

大数据环境下数据质量的度量考虑以下因素:

*   数据源
*   数据类型
*   数据的有意和无意使用
*   将使用数据和结果来分析工件的用户组。

在上述各点中，数据类型在数据质量要求中起着至关重要的作用，如以下各点所述:

*   **结构化大数据分析:**在处理大量结构化数据的大数据项目中，企业可以重用现有的处理关系数据库的数据质量流程，前提是这些流程可以扩展以满足大规模需求。
*   **分析非结构化大数据:**社交媒体相关大数据项目聚焦质量问题，涉及从由俚语和缩写组成的非标准语言表达的句子中提取实体。社交媒体的分析价值可以通过与结构化交易数据关联来提取，因此社交网络上事件之间的关系可以映射到组织的内部数据，如供应链数据或客户人口统计数据。要执行此映射，必须分析非结构化文本以了解以下几点:
    *   如何提取对分析很重要的实体
    *   有多少数据拼错了？
    *   特定字段的通用缩写是什么？
    *   删除 stopword 的标准是什么？
    *   如何执行 stem？
    *   如何根据前面的单词理解单词的语境意义？

## 大数据分析维度

大数据分析跨多个维度进行，具体维度的选择通常取决于分析问题和时间/质量的平衡。有些维度重叠，有些维度根本不适用于这个问题。以下是衡量大数据质量的最重要方面:

*   **Completeness:** This dimension is a measure to know if you have all the data required to answer your queries. To evaluate if your dataset is complete, start by understanding the answers that you wish to seek from the data, and determine the fields needed and the percentage of complete records required to comfortably answer these questions. The following are a few of the ways completeness can be determined:
    *   如果我们知道主数据统计(记录、字段等的数量。)预先，完整性可以被确定为接收的记录与主数据记录的数量的比率。
    *   当主数据统计不可访问时，完整性通过**空值**的存在以下列方式来衡量:
        *   **属性完整性**:它处理特定属性中是否有空值。
        *   **元组完整性**:它处理元组中未知属性值的数量。
        *   **值完整性**:它处理半结构化 XML 数据中缺失的完整元素或属性。

    因为大数据分析的一个方面是问以前没有问过的问题，所以检查完整性维度就有了新的意义。在这种情况下，可以考虑执行一次迭代*，向前看*找到一系列你期望回答的问题，然后*，向后推理*找出回答这些问题需要什么样的数据。可以应用简单的记录计数机制来检查 Hadoop 集群中是否有总的期望记录。但是，对于跨越千兆字节的数据大小，此活动可能会很繁重，必须通过应用统计采样来执行。如果发现数据不完整，可以根据分析案例对丢失的记录进行修复、删除、标记或忽略。

*   **Correctness:** This dimension measures the accuracy of the data. To find out if the data is accurate, you have to know what comprises inaccurate data, and this purely depends on the business context. In cases where data should be unique, duplicate data is considered inaccurate. Calculating the number of duplicate elements in data that is spread across multiple systems is a nontrivial job. The following techniques can be used to find out the measure of potential inaccuracy of data:
    *   在包含离散值的数据集中，频率分布可以对数据的潜在不准确性给出有价值的见解。频率相对较低的值可能不正确。
    *   对于字符串，您可以创建字符串长度分布模式和低频标志模式作为潜在的疑点。同样，具有非典型长度的字符串可能会被标记为不正确。
    *   在连续属性的情况下，可以使用描述性统计数据(如最大值和最小值)将数据标记为不准确。

    对于大数据项目，建议确定精度需要哪些属性子集，知道应该有多少数据是准确的，并对数据进行采样以确定精度。

    ### 注

    需要注意的是，*大海捞针*的经典大数据问题中，有大量的分析值隐藏在不准确的数据中，可以认为是异常值。这些异常值不会被认为是不准确的，但是它们可以被标记并考虑用于用例的进一步分析，例如欺诈检测。

*   **相干性:**这个维度衡量数据相对于自身是否有意义，判断记录之间是否有一致的关联，遵循数据集的内在逻辑。数据集一致性的度量可以通过以下方法理解:
    *   **参照完整性**:这个保证了表之间的关系是一致的。在大数据环境中，引用完整性不能应用于存储在 NoSQL 数据库(如 HBase)中的数据，因为没有数据的关系表示。
    *   **值完整性**:这个保证一个表中的值是否和自己一致。通过将这些值与预定义的一组可能值(来自主数据)进行比较，可以发现不一致的数据。

## 大数据采样分析注意事项

大数据采样是了解数据质量，通过只分析一部分人群，而不是深入到整个人群。选择样本最重要的标准之一是它的代表性，它决定了样本子集和总体之间的相似性。为了获得准确的结果，代表性应该更高。采样尺度对子集的代表性精度也有相当大的影响。

对大量数据进行采样以进行特征分析平衡了成本和质量之间的权衡，因为所有人进行特征分析的成本和复杂性都很高。在大多数情况下，分析活动并不是作为一种全面分析整个数据的机制来设计的，而是一个从正确性、一致性和完整性等维度获得数据整体质量的首过分析/发现阶段。随着数据在管道中移动，分析活动将迭代进行，这将有助于细化数据。因此，用于分析目的的数据采样在大数据中起着非常重要的作用。

虽然采样被认为是分析所必需的，但在将采样技术应用于大数据时建议谨慎。由于实现的复杂性和不适用性，并非所有的数据类型和收集机制都需要采样；当几乎实时地从传感器获取数据时，这是有效的。同样，并不是所有的用例都需要采样，这在获取数据进行搜索、推荐系统和点击流分析时是有效的。在这种情况下，必须完全不取样地查看数据。在这些情况下，取样会引入一些偏差，降低结果的准确性。

选择合适的采样技术对轮廓的整体精度有影响。这些技术包括非概率抽样和概率抽样方法。通常，我们不考虑非概率方法来执行数据分析活动。我们只限于概率抽样方法，因为这种方法提高了精度，减少了代表性偏差。为了更好的了解采样技术，数据简化模式 中*数值简化-采样设计模式*详见[第六章*。*](6.html "Chapter 6. Understanding Data Reduction Patterns")

### PIG 的采样支持

Pig 有本地支持使用`SAMPLE`操作符采样。我们使用了`SAMPLE`运算符来解释它在剖面分析的上下文中是如何工作的，并使用了基本的统计剖面分析设计模式。`SAMPLE`算子通过概率算法帮助你从人群中随机选择样本。内部算法非常初级，有时不能代表被采样的整个数据集。算法内部采用**简单随机采样技术**。 `SAMPLE`算子正在进化以适应更深奥的采样算法。更多关于前方道路的信息可以在[https://issues.apache.org/jira/browse/PIG-1713](https://issues.apache.org/jira/browse/PIG-1713)找到。

在 Pig 中实现稳健采样方法的其他方法是利用 UDF 特征和 Pig 流对其进行扩展。

利用 Pig 的可扩展性，可以像 UDF 一样实现采样，但是使用起来非常复杂和费力，因为 UDF 最大的局限性就是只接受一个输入值，生成一个输出值。

也可以考虑使用 stream 实现采样，不受 UDFS 的限制。一个流可以接受任意数量的输入或发出任意数量的输出。r 语言有执行采样所需的函数，可以通过 Pig 流在 Pig 脚本中使用这些函数。这种方法的局限性在于，它通过将大部分数据保存在主存中来执行采样计算，而 R 必须安装在 Hadoop 集群的每个数据节点上，流才能工作。

LinkedIn 的 Pig 实用程序中的 **Datafu** 库已经发布了一些自己的采样实现。这个库现在是 Cloudera Hadoop 发行版的一部分。以下采样技术由 Datafu 实施:

*   **储层采样**:它利用内存中的储层生成给定大小的随机样本。
*   **Samplebykey** :它根据某个键从元组中生成一个随机的样本。采用内部 **分层随机抽样**技术。
*   **加权样本**:它通过赋权生成一个随机样本。

关于 Datafu 采样实施的更多信息,请访问 [http://LinkedIn .github .io/数据单元/文档/当前/数据单元/清管器/取样/包装-摘要 html](http://linkedin.github.io/datafu/docs/current/datafu/pig/sampling/package-summary.html) 。

# 数据分析中使用 Pig 的基本原理

在 Hadoop 环境中实现分析代码减少了对外部系统质量检查的依赖。下图描述了实施的高级概述:

![Rationale for using Pig in data profiling](img/5556OS_03_01.jpg)

对 PIG 进行分析。

以下是在 Hadoop 环境中使用 Pig 进行数据分析的优势:

*   在 Pig 中实现设计模式通过将分析代码直接移动到数据中来减少数据移动，从而提高性能并加速分析和开发过程。
*   通过在 Pig 中实现这种模式，数据质量工作和数据转换在同一个环境中进行。这减轻了每次在 Hadoop 中接收数据时执行重复数据质量检查的手动冗余工作。
*   Pig 擅长于运行前摄入的数据模式未知的情况；它的语言特性为数据科学家提供了在运行时破译正确模式和构建原型模型的灵活性。
*   Pig 固有的发现数据模式和采样的能力使得在 Hadoop 环境中实现概要分析代码非常有利。
*   Pig 易于使用，这使得编写定制的分析代码变得更加容易。
*   Pig 通过链接复杂的分析工作流实现了分析过程的自动化，这对于定期更新的数据集非常有用。

既然我们已经理解了数据概要分析的概念和使用 Pig 进行概要分析的基本原理，我们将在下面的小节中探索一些具体的设计模式。

# 数据类型推理模式

本节描述了数据类型推断设计模式，其中我们使用 Pig 脚本来捕获关于数据类型的重要信息。

## 背景

Hadoop 中的大部分数据都有一些关联的元数据，这是对其特性的描述。元数据包括关于字段类型、长度、约束和唯一性的重要信息。我们也可以知道一个字段是否是强制的。元数据还用于通过检查刻度、测量单位、标签含义等来解释值。理解数据集的预期结构有助于解释其含义、描述、语义和数据质量。这种对数据类型的分析有助于我们了解它们在语法上是否一致(不同的数据集具有相同的一致格式规范)以及在语义上是否一致(不同的数据集具有相同的值集)。

## 动机

这种设计模式的意图是从 Hadoop 中摄取的数据中推断数据类型元数据。这个模型帮助你找到`Type`元数据与实际数据的对比，看看它们是否不一致，对分析有没有什么深远的影响。扫描数据类型和属性值，并将其与记录的元数据进行比较。基于该扫描，提出了适当的数据类型和数据长度。

这种设计模式用于检查数据集的结构，对于数据集，很少或没有现有的元数据，或者有理由怀疑现有元数据的完整性或质量。模式的结果有助于发现、记录和组织关于数据集元数据的“基本事实”。在这里，数据汇总分析的结果用于增量获取与数据元素的结构、语义和使用相关联的知识库。

## 用例

当你不得不摄取大量的结构化数据并且缺乏关于数据集的文档化知识时，你可以使用的设计模式。如果您需要使用未记录的数据进行进一步分析，或者需要对领域业务术语、相关数据元素、它们的定义、使用的参考数据集以及数据集中属性的结构有更深入的了解，则可以使用这种设计模式。

## 模式实现

这个模式被实现为一个独立的 Pig 脚本，里面有一个 Java UDF。实现这种模式的核心概念是发现列中的主要数据类型。首先，检查列值，找出它们是类型`int`、`long`、`double`、`string`还是`boolean`。评估该值后，组合每种数据类型以找到频率。从这个分析中，我们可以找出哪个是占主导地位的(最频繁的)数据类型。

## 代码片段

为了说明的工作原理，我们考虑存储在 **Hadoop 分布式文件系统** ( **HDFS** )中的零售交易数据集。包括`Transaction ID`、`Transaction date`、`Customer ID`、`Phone Number`、`Product`、`Product subclass`、`Product ID`、`Sales Price`、`Country Code`等属性。对于这个模式，我们感兴趣的是属性`Customer ID`的值。

### PIG

下面是一个 Pig 脚本，演示了这种模式的实现:

```sh
/*
Register the datatypeinferer and custom storage jar files
*/
REGISTER '/home/cloudera/pdp/jars/datatypeinfererudf.jar';
REGISTER'/home/cloudera/pdp/jars/customdatatypeinfererstorage.jar';

/*
Load the transactions dataset into the relation transactions
*/
transactions = LOAD'/user/cloudera/pdp/datasets/data_profiling/transactions.csv'USING  PigStorage(',') AS (transaction_id:long,transaction_date:chararray, cust_id:chararray, age:chararray,area:chararray, prod_subclass:int, prod_id:long, amt:int,asset:int, sales_price:int, phone_no:chararray,country_code:chararray);

/*
Infer the data type of the field cust_id by invoking the DataTypeInfererUDF.
It returns a tuple with the inferred data type.
*/
data_types = FOREACH transactions GENERATEcom.profiler.DataTypeInfererUDF(cust_id) AS inferred_data_type;

/*
Compute the count of each data type, total count, percentage.
The data type with the highest count is considered as dominant data type
*/
grpd = GROUP data_types BY inferred_data_type;
inferred_type_count = FOREACH grpd GENERATE group ASinferred_type, COUNT(data_types) AS count;
grpd_inf_type_count_all = GROUP inferred_type_count ALL;
total_count = FOREACH grpd_inf_type_count_all GENERATESUM(inferred_type_count.count) AS tot_sum,MAX(inferred_type_count.count) AS max_val;
percentage = FOREACH inferred_type_count GENERATE inferred_type AStype, count AS total_cnt,CONCAT((Chararray)ROUND(count*100.0/total_count.tot_sum),'%') ASpercent,(count==total_count.max_val?'Dominant':'Other') ASinferred_dominant_other_datatype;
percentage_ord = ORDER percentage BYinferred_dominant_other_datatype ASC;

/*
CustomDatatypeInfererStorage UDF extends the StoreFunc. All the abstract methods have been overridden to implement logic that writes the contents of the relation into a file in a custom report like format.
The results are stored on the HDFS in the directory datatype_inferer
*/
STORE percentage_ord INTO'/user/cloudera/pdp/output/data_profiling/datatype_inferer'using com.profiler.CustomDatatypeInfererStorage('cust_id','chararray');
```

### Java UDF

以下是 Java UDF 的代码片段:

```sh
@Override
  public String exec(Tuple tuples) throws IOException {

    String value = (String) tuples.get(0);
    String inferredType = null;
    try {
/*if tuples.get(0) is null it returns null else invokes getDataType() method to infer the datatype
      */
      inferredType = value != null ? getDataType(value) : NULL;

    } catch (Exception e) {
      e.printStackTrace();
  }
    // returns inferred datatype of the input value
    return inferredType;
```

## 结果

以下是将设计模式应用于交易数据的结果:

```sh
Column Name :  cust_id
Defined Datatype :  chararray
Inferred Dominant Datatype(s):  int, Count: 817740 Percentage: 100% 
```

在之前的结果中，对输入数据列`cust_id`进行了评估，以检查该值是否准确反映了定义的数据类型。在摄取阶段，数据类型被定义为`chararray`。通过使用数据来推断设计模式，`cust_id`列中的值的数据类型被推断为整数。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter3/code/`
*   `Chapter3/datasets/`

# 基本统计分析模式

本节描述了基本的统计分析设计模式，其中我们使用 Pig 脚本来应用统计函数来捕获关于数据质量的重要信息。

## 背景

之前的设计模式描述了一种推断数据类型的方法。数据分析过程的下一个逻辑步骤是评估值的质量测量。这是通过应用统计方法收集和分析数据来实现的。这些统计数据提供了数据对特定分析问题的适用性的高级概述，并揭示了数据生命周期管理早期阶段的潜在问题。

## 动机

基本统计分析设计模式有助于创建数据质量元数据，包括平均值、中值、模式、最大值、最小值和标准差等基本统计数据。这些统计数据为您提供了整个数据领域的完整快照，随着时间的推移跟踪这些统计数据将有助于您深入了解 Hadoop 集群正在接收的新数据的特征。在将新数据引入 Hadoop 之前，可以检查新数据的基本统计信息，提前警告不一致的数据，帮助防止添加低质量的数据。

该设计模式试图解决以下总结要求:

*   范围分析方法扫描这些值，并确定数据是否要进行整体排序，还确定这些值是否被约束在一个定义明确的范围内。
*   您可以评估数据的**稀疏度**来找到未填充元素的百分比。
*   数据集的**基数**可以通过找出数据中出现的不同值的个数来分析。
*   可以评估**的唯一性，以确定分配给属性的每个值是否确实是排他的。**
*   **可以评估数据过载**以检查属性是否被用于各种目的。
*   格式评估可以通过将无法识别的数据解析为已定义的格式来完成。

## 用例

以下是可以应用基本统计概要设计模式的用例:

*   这种设计模式可以用来检测数据集中的异常，并通过对数据集中的值进行经验分析来发现意外行为。该模式检查记录数据的频率分布、方差、百分比及其与数据集的关系，以揭示潜在的缺陷数据值。
*   可以使用这种设计模式的一个常见用例是，将数据从仍在使用的遗留数据源吸收到 Hadoop 集群中。在大型机等遗留系统中，大型机程序员在数据创建期间设计快捷方式和代码，并为不再使用或理解的不同目的重新加载特定字段。当这些数据被吸收到 Hadoop 中时，基本的统计设计模式可以帮助发现这个问题。

## 模式实现

这个设计模式在 Pig 中实现为一个独立的脚本，内部使用一个宏来传递参数和检索答案。拉丁语有一组`Math`函数，可以直接应用于一列数据。首先将数据加载到 Pig 关系中，然后将该关系作为参数传递给`getProfile`宏。宏迭代关系并将`Math`函数应用于每一列。`getProfile`宏设计模块化，可以应用于各种数据集，更好地理解数据汇总。

## 代码片段

为了解释这个模型是如何工作的，我们考虑存储在 HDFS 的零售交易数据集。包括`Transaction ID`、`Transaction date`、`Customer ID`、`Phone Number`、`Product`、`Product subclass`、`Product ID`、`Sales Price`、`Country Code`等属性。对于这个模式，我们将分析属性`Sales Price`的值。

### PIG

下面是 Pig 脚本，说明了这个模式的实现:

```sh
/*
Register the datafu and custom storage jar files
*/
REGISTER '/home/cloudera/pdp/jars/datafu.jar';
REGISTER '/home/cloudera/pdp/jars/customprofilestorage.jar';

/*
Import macro defined in the file numerical_profiler_macro.pig
*/
IMPORT '/home/cloudera/pdp/data_profiling/numerical_profiler_macro.pig';

/*
Load the transactions dataset into the relation transactions
*/
transactions = LOAD'/user/cloudera/pdp/datasets/data_profiling/transactions.csv'USING  PigStorage(',') AS (transaction_id:long,transaction_date:datetime, cust_id:long, age:chararray,area:chararray, prod_subclass:int, prod_id:long, amt:int,asset:int, sales_price:int, phone_no:chararray,country_code:chararray);

/*
Use SAMPLE operator to pick a subset of the data, at most 20% of the data is returned as a sample
*/
sample_transactions = SAMPLE transactions 0.2;

/*
Invoke the macro getProfile with the parameters sample_transactions which contains a sample of the dataset and the column name on which the numerical profiling has to be done.
The macro performs numerical profiling on the sales_price column and returns various statistics like variance, standard deviation, row count, null count, distinct count and mode
*/
result =  getProfile(sample_transactions,'sales_price');

/*
CustomProfileStorage UDF extends the StoreFunc. All the abstract methods have been overridden to implement logic that writes the contents of the relation into a file in a custom report like format.
The results are stored on the HDFS in the directory numeric
*/
STORE result INTO'/user/cloudera/pdp/output/data_profiling/numeric' USINGcom.profiler.CustomProfileStorage();
```

### 宏

下面是一个 Pig 脚本，展示了 `getProfile`宏的实现:

```sh
/*
Define alias VAR for the function datafu.pig.stats.VAR
*/
DEFINE VAR datafu.pig.stats.VAR(); 

/*
Define the macro, specify the input parameters and the return value
*/
DEFINE getProfile(data,columnName) returns numerical_profile{

/*
Calculate the variance, standard deviation, row count, null count and distinct count for the column sales_price
*/
data_grpd = GROUP $data ALL;
numerical_stats = FOREACH data_grpd  
{
  variance = VAR($data.$columnName);
  stdDeviation = SQRT(variance);
  rowCount = COUNT_STAR($data.$columnName);
  nullCount = COUNT($data.$columnName);
  uniq = DISTINCT $data.$columnName;
  GENERATE 'Column Name','$columnName' AS colName,'Row Count',rowCount,'Null Count' , (rowCount - nullCount),'Distinct Count',COUNT(uniq),'Highest Value',MAX($data.$columnName) AS max_numerical_count,'Lowest Value',MIN($data.$columnName) ASmin_numerical_count, 'Total Value',SUM($data.$columnName) AStotal_numerical_count,'Mean Value', AVG($data.$columnName) ASavg_numerical_count,'Variance',variance AS 	variance,'StandardDeviation', stdDeviation AS stdDeviation,'Mode' asmodeName,'NONE' as modevalue;
}

/*
Compute the mode of the column sales_price
*/
groupd = GROUP $data BY $columnName;
groupd_count = FOREACH groupd GENERATE 'Mode' as modeName, groupAS mode_values, (long) COUNT($data) AS total;
groupd_count_all = GROUP groupd_count ALL;
frequency = FOREACH groupd_count_all GENERATEMAX(groupd_count.total) AS fq;
filterd = FILTER groupd_count BY (total== frequency.fq AND total>1AND mode_values IS NOT NULL);
mode  = GROUP filterd BY modeName;

/*
Join relations numerical stats and mode. Return these values
*/
$numerical_profile = JOIN numerical_stats BY modeName FULL,mode BY group;
};
```

## 结果

通过使用基本统计分析模型，获得以下结果:

```sh
Column Name: sales_price
Row Count: 163794
Null Count: 0
Distinct Count: 1446
Highest Value: 70589
Lowest Value: 1
Total Value: 21781793
Mean Value: 132.98285040966093
Variance: 183789.18332067598
Standard Deviation: 428.7064069041609
Mode: 99
```

前面的结果总结了数据属性、行计数、空计数和不同值的数量。我们也知道数据在中心趋势和偏差方面的关键特征。均值和模式是衡量中心趋势的几个指标；方差是理解数据偏差的一种方法。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter3/code/`
*   `Chapter3/datasets/`

# 模式匹配模式

这部分描述了模式匹配设计模式，其中我们使用 Pig 脚本来匹配数字和文本模式，以确定数据是否与自身相关，从而获得数据质量的度量。

## 背景

在企业的上下文中，检查数据的一致性是在数据被摄取并确定其完整性和正确性之后。给定属性的值可以有不同的形状和大小。对于需要手动输入的字段尤其如此，在这些字段中，值是根据用户的意愿输入的。假设代表电话号码字段的列是连贯的，可以说所有的值都代表有效的电话号码，因为它们匹配预期的格式、长度和数据类型(号码)，从而满足系统的预期。以不正确的格式虚报数据将导致不准确的分析。在大数据环境下，庞大的数据量会放大这种不准确性。

## 动机

从模式匹配的角度分析数据，衡量数据的一致性以及与预期模式匹配的数据量。该分析过程将这些值与一组预定义的可能值进行比较，以找出这些值是否与其自身一致。它抓住了数据的本质，并告诉您一个字段是完全数字的还是具有相同的长度。它还提供特定于数据格式的其他信息。评估是通过将无法识别的数据解析为定义的格式来完成的。基于模式分析和使用，识别数据的抽象类型以执行语义数据类型关联。在分析周期的早期阶段识别不匹配模式的百分比不准确性可以确保更好的数据清理并减少工作量。

## 用例

这个设计模式可以用来分析应该与特定模式匹配的数字或字符串数据。

## 模式实现

这个设计模式在 Pig 中作为独立脚本实现。该脚本试图通过分析存储在属性中的数据字符串来发现数据中的模式和常见记录类型。它生成几个与属性中的值匹配的模式，并报告每个候选模式后的数据百分比。该脚本主要执行以下任务:

*   从元组中发现模式；每一个的数量和百分比。
*   检查发现的模式，并将其分类为有效或无效。

## 代码片段

为了解释这个模型是如何工作的，我们考虑存储在 HDFS 的零售交易数据集。包含 T0、T1、T2、T3、T4、T5、T6、T7、T8 等属性。对于这个模式，我们感兴趣的是属性`Phone Number`的值。

### PIG

下面是一个 Pig 脚本，说明了这种模式的实现:

```sh
/*
Import macro defined in the file pattern_matching_macro.pig
*/
IMPORT '/home/cloudera/pdp/data_profiling/pattern_matching_macro.pig';

/*
Load the dataset transactions.csv into the relation transactions
*/
transactions = LOAD'/user/cloudera/pdp/datasets/data_profiling/transactions.csv'USING  PigStorage(',') AS (transaction_id:long,transaction_date:datetime, cust_id:long, age:chararray,area:chararray, prod_subclass:int, prod_id:long, amt:int,asset:int, sales_price:int, phone_no:chararray,country_code:chararray);

/*
Invoke the macro and pass the relation transactions and the column phone_no as parameters to it.
The pattern matching is performed on the column that is passed.
This macro returns the phone number pattern, its count and the percentage
*/
result = getPatterns(transactions, 'phone_no');

/*
Split the relation result into the relation valid_pattern if the phone number pattern matches any of the two regular expressions. The patterns that do not match any of the regex are stored into the relation invalid_patterns
*/
SPLIT result INTO valid_patterns IF (phone_number MATCHES'([0-9]{3}-[0-9]{3}-[0-9]{4})' or phone_number MATCHES'([0-9]{10})'), invalid_patterns OTHERWISE;

/*
The results are stored on the HDFS in the directories valid_patterns and invalid_patterns
*/
STORE valid_patterns INTO '/user/cloudera/pdp/output/data_profiling/pattern_matching/valid_patterns';
STORE invalid_patterns INTO '/user/cloudera/pdp/output/data_profiling/pattern_matching/invalid_patterns';
```

### 宏

以下是显示`getPatterns`宏实现的 PIG 脚本:

```sh
/*
Define the macro, specify the input parameters and the return value
*/
DEFINE getPatterns(data,phone_no) returns percentage{

/*
Iterate over each row of the phone_no column and transform each value by replacing all digits with 9 and all alphabets with a to form uniform patterns
*/
transactions_replaced = FOREACH $data  
{
  replace_digits = REPLACE($phone_no,'\\d','9');
  replace_alphabets = REPLACE(replace_digits,'[a-zA-Z]','a');
  replace_spaces = REPLACE(replace_alphabets,'\\s','');
  GENERATE replace_spaces AS phone_number_pattern;
}
/*
Group by phone_number_pattern and calculate count of each pattern
*/
grpd_ph_no_pattern = GROUP transactions_replaced BYphone_number_pattern;
phone_num_count = FOREACH grpd_ph_no_pattern GENERATE group asphone_num, COUNT(transactions_replaced.phone_number_pattern) ASphone_count;

/*
Compute the total count and percentage.
Return the relation percentage with the fields phone number pattern, count and the rounded percentage
*/
grpd_ph_no_cnt_all = GROUP phone_num_count ALL;
total_count = FOREACH grpd_ph_no_cnt_all GENERATESUM(phone_num_count.phone_count) AS tot_sum;
$percentage = FOREACH phone_num_count GENERATE phone_num asphone_number, phone_count as phone_number_count,CONCAT((Chararray)ROUND(phone_count*100.0/total_count.tot_sum),'%') as percent;
};
```

## 结果

以下是将设计模式应用于交易数据的结果。结果存储在文件夹`valid_patterns`和`invalid_patterns`中。

文件夹`valid_patterns`中的输出如下:

```sh
9999999999  490644    60%
999-999-9999  196257    24%
```

文件夹`invalid_patterns`中的输出如下:

```sh
99999         8177  1%
aaaaaaaaaa  40887  5%
999-999-999a  40888  5%
aaa-aaa-aaaa  40887  5%
```

先前的结果给出了我们数据集中所有电话号码模式的快照，它们的计数和百分比。利用这些数据，我们可以确定数据集中不准确数据的百分比，并在数据清理阶段采取必要的措施。因为 999-999-9999 格式的电话号码具有较高的相对频率，并且它是有效的模式，所以您可以导出一个规则，该规则要求该属性中的所有值都符合该模式。此规则可应用于数据清理阶段。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter3/code/`
*   `Chapter3/datasets/`

# 字符串分析模式

本节描述字符串分析的设计模式，其中我们使用文本数据的 Pig 脚本来学习重要的统计信息。

## 背景

大多数大数据实现处理嵌入在列中的文本数据。为了从这些列中获得洞察力，它们必须与其他企业结构化数据集成。这种设计模式说明了一些有助于理解文本数据质量的方法。

## 动机

文本的质量可以通过对属性值应用基本的统计技术来确定。在为目标系统选择合适的数据类型和大小时，找到字符串的长度是最重要的方面。您可以使用最大和最小字符串长度来确定 Hadoop 接收的数据是否满足给定的约束。当处理 petabyte 范围内的数据时，将字符数量限制得足够大可以通过减少不必要的存储空间来优化存储和计算。

使用字符串长度，还可以确定一列中每个字符串的不同长度，以及每个长度在表格中所代表的行数百分比。

例如，代表美国州代码的列的配置文件应该是两个字符，但是如果收集的配置文件显示的值不同于两个字符，则表明该列中的值不一致。

## 用例

这种模式可以应用于主要包含文本数据类型的数据列，以找出文本是否在定义的约束内。

## 模式实现

这个设计模式在 Pig 中实现为一个独立的脚本，它使用一个宏在内部检索概要文件。Pig Latin 有一组可以直接应用于一列数据的数学函数。首先将数据加载到 Pig 关系`transactions`中，然后将该关系作为参数传递给`getStringProfile`宏。宏迭代关系并将`Math`函数应用于每个值。`getStringProfile`宏被设计成模块化的，可以跨各种文本列应用，以更好地理解字符串数据的摘要。

## 代码片段

为了说明 T10 模型的工作原理，我们考虑存储在 HDFS 的零售交易数据集。包含 T0、T1、T2、T3、T4、T5、T6、T7、T8 等属性。对于这个模式，我们感兴趣的是属性`Country Code`的值。

### PIG

下面是一个 Pig 脚本，说明了这种模式的实现:

```sh
/*
Register the datafu and custom storage jar files
*/
REGISTER '/home/cloudera/pdp/jars/datafu.jar';
REGISTER '/home/cloudera/pdp/jars/customprofilestorage.jar';

/*
Import macro defined in the file string_profiler_macro.pig
*/
IMPORT'/home/cloudera/pdp/data_profiling/string_profiler_macro.pig';

/*
Load the transactions dataset into the relation transactions
*/
transactions = LOAD'/user/cloudera/pdp/datasets/data_profiling/transactions.csv'using PigStorage(',') as(transaction_id:long,transaction_date:datetime, cust_id:long,age:chararray, area:chararray, prod_subclass:int, prod_id:long,amt:int, asset:int, sales_price:int, phone_no:chararray,country_code:chararray);
/*
Invoke the macro getStringProfile with the parameters transactions and the column name on which the string profiling has to be done.
The macro performs string profiling on the country_code column and returns various statistics like row count, null count, total character count, word count, identifies distinct country codes in the dataset and calculates their count and percentage.
*/
result =  getStringProfile(transactions,'country_code');

/*
CustomProfileStorage UDF extends the StoreFunc. All the abstract methods have been overridden to implement logic that writes the contents of the relation into a file in a custom report like format.
The results are stored on the HDFS in the directory string
*/
STORE result INTO'/user/cloudera/pdp/output/data_profiling/string' USINGcom.profiler.CustomProfileStorage();
```

### 宏

以下是【T1】 Pig 脚本，展示了`getStringProfile`宏的实现:

```sh
/*
Define the macro, specify the input parameters and the return value
*/
DEFINE getStringProfile(data,columnName) returns string_profile{

/*
Calculate row count and null count on the column country_code
*/
data_grpd = GROUP $data ALL;
string_stats = FOREACH data_grpd  
{
  rowCount = COUNT_STAR($data.$columnName);
  nullCount = COUNT($data.$columnName);
  GENERATE 'Column Name','$columnName' AS colName,'Row Count',rowCount,'Null Count' ,(rowCount - nullCount),'Distinct Values' as dist,'NONE' as distvalue;
}

/*
Calculate total char count, max chars, min chars, avg chars on the column country_code
*/
size = FOREACH $data GENERATE SIZE($columnName) AS chars_count;
size_grpd_all = GROUP size ALL;
char_stats = FOREACH size_grpd_all GENERATE 'Total CharCount',SUM(size.chars_count) AS total_char_count,'Max Chars',MAX(size.chars_count) AS max_chars_count,'Min Chars',MIN(size.chars_count) AS min_chars_count,'Avg Chars',AVG(size.chars_count) AS avg_chars_count,'Distinct Values' asdist,'NONE' as distvalue;

/*
Calculate total word count, max words and min words on the column country_code
*/
words = FOREACH $data GENERATE FLATTEN(TOKENIZE($columnName)) ASword;
whitespace_filtrd_words = FILTER words BY word MATCHES '\\w+';
grouped_words = GROUP whitespace_filtrd_words BY word;
word_count = FOREACH grouped_words GENERATECOUNT(whitespace_filtrd_words) AS count, group AS word;
word_count_grpd_all = GROUP word_count ALL;
words_stats = FOREACH word_count_grpd_all GENERATE 'WordCount',SUM(word_count.count) AS total_word_count,'Max Words',MAX(word_count.count) AS max_count,'Min Words',MIN(word_count.count) AS min_count,'Distinct Values'as dist,'NONE' as distvalue;

/*
Identify distinct country codes and their count
*/
grpd_data = GROUP $data BY $columnName;
grpd_data_count = FOREACH grpd_data GENERATE group ascountry_code, COUNT($data.$columnName) AS country_count;

/*
Calculate the total sum of all the counts
*/
grpd_data_cnt_all = GROUP grpd_data_count ALL;
total_count = FOREACH grpd_data_cnt_all GENERATESUM(grpd_data_count.country_count) AS tot_sum;

/*
Calculate the percentage of the distinct country codes
*/
percentage = FOREACH grpd_data_count GENERATE country_code ascountry_code, 
country_count as country_code_cnt,ROUND(country_count*100.0/total_count.tot_sum) aspercent,'Distinct Values' as dist;

/*
Join string stats, char_stats, word_stats and the relation with distinct country codes, their count and the rounded percentage. Return these values
*/
percentage_grpd = GROUP percentage BY dist;
$string_profile = JOIN string_stats BY dist,char_stats BY dist ,words_stats BY dist, percentage_grpd BY group;
};
```

## 结果

使用字符串分析模式，可以获得以下结果:

```sh
Column Name: country_code
Row Count: 817740
Null Count: 0
Total Char Count: 5632733
Max Chars: 24
Min Chars: 2
Avg Chars: 6.888171056815125
Word Count: 999583
Max Words: 181817
Min Words: 90723

Distinct Values
country_code      Count    Percentage
US          181792    22%
U.S	        90687    11%
USA	        181782    22%
U.S.A        90733    11%
America        90929    11%
United States      91094    11%
United States of America  90723    11%
```

前面的结果总结了数据属性，如行数、空值数和字符总数。`Max chars`和`Min chars`计数可以通过检查数值的长度是否在范围内来验证数据质量。根据元数据，国家代码的有效值应为两个字符，但结果显示最大字符数为`24`，这意味着数据不准确。`Distinct values`部分的结果显示了数据集中不同的国家代码，以及它们的计数和百分比。利用这些结果，我们可以确定数据集中不准确数据的百分比，并在数据清理阶段采取必要的措施。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter3/code/`
*   `Chapter3/datasets/`

# 非结构化文本解析模式

本节描述了非结构化文本分析的设计模式，其中我们使用 Pig 脚本来学习自由格式文本数据的重要统计信息。

## 背景

文本挖掘基于 Hadoop 拍摄的非结构化数据，从无意义的数据块中提取有趣且非琐碎的有意义的模式。文本挖掘是一个跨学科的领域，它利用信息检索、数据挖掘、机器学习、统计学和计算语言学从文本中提取有意义的模式。通常，Hadoop 的并行处理能力用于处理大量文本数据、文档分类、推文聚类、本体构建、实体提取、情感分析等。

该模型讨论了一种利用文本预处理技术来确定文本数据质量的方法，如停止词去除、词干和 TF-IDF。

## 动机

非结构化文本本质上是不一致的，会导致分析不准确。文本中的不一致是因为表达一个想法的方式有很多。

文本预处理提高了数据质量，提高了分析的准确性，降低了文本挖掘过程的难度。以下是完成文本预处理的步骤:

*   文本预处理的第一步是将文本块转换成标签，以删除标点符号、连字符、括号等。，并且只保留有意义的关键字、缩写和首字母缩略词以供进一步处理。标记涉及文档一致性的度量，因为被消除的无意义标记的数量和数据相对于自身的不一致性之间存在线性比例关系。
*   去词是文本预处理的下一个逻辑步骤。此步骤包括删除不为文档提供任何含义或上下文的单词。这些单词叫做 *stopword* 。它们通常包含代词、冠词、介词等。在实际从原始文本中删除它们之前，将定义一个停用词列表。该列表可以包括特定于特定领域的其他单词。
*   词干步骤通过将一个单词移除或转换成它的基本单词(词干)，将单词的各种形式简化成它的词根形式。例如，同意、同意、不同意、同意和不同意(取决于特定的词干算法)被词干化为同意。这样做是为了使语料库中的所有标签一致。
*   词干处理完成后，通过计算词频，相对于出现频率为标签分配权重。此统计数据表示单词在文档中出现的次数。计算逆文档频率，知道单词在所有文档中出现的频率。这个统计数据决定了一个词在所有文档中是常见还是罕见。查找单词的频率和反向文档的频率对文本的质量有影响，因为这些统计数据会根据单词对文档或语料库的重要性来告诉您是否可以丢弃或使用它们。

## 用例

这种设计模式可以用于需要通过文本预处理技术了解非结构化文本语料库质量的情况。这种设计模式并不详尽，它涵盖了文本预处理的几个重要方面及其对数据分析的适用性。

## 模式实现

这个设计模式在 Pig 中作为独立脚本实现。内部使用`unstructuredtextprofiling` Java UDF 执行词干，生成词频和逆文档词频。该脚本执行右连接来移除停止字。Stopword 列表首先从外部文本文件加载到关系中，然后在外部连接中使用。

词干是使用`unstructuredtextprofiling` JAR 文件中实现的波特斯特梅尔算法完成的。

## 代码片段

为了演示这种模式是如何工作的，我们考虑维基百科的文本语料库，它存储在 HDFS 可以访问的文件夹中。这个样本语料库由与计算机科学和信息技术相关的维基页面组成。

### PIG

下面是一个 Pig 脚本，说明了这种模式的实现:

```sh
/*
Register custom text profiler jar
*/
REGISTER '/home/cloudera/pdp/jars/unstructuredtextprofiler.jar';

/*
Load stop words into the relation stop_words_list
*/
stop_words_list = LOAD'/user/cloudera/pdp/datasets/data_profiling/text/stopwords.txt'USING PigStorage();

/*
Tokenize the stopwords to extract the words
*/
stopwords = FOREACH stop_words_list GENERATEFLATTEN(TOKENIZE($0));

/*
Load the dataset into the relations doc1 and doc2.
Tokenize to extract the words for each of these documents
*/
doc1 = LOAD'/user/cloudera/pdp/datasets/data_profiling/text/computer_science.txt' AS (words:chararray);
docWords1 = FOREACH doc1 GENERATE 'computer_science.txt' ASdocumentId, FLATTEN(TOKENIZE(words)) AS word;
doc2 = LOAD'/user/cloudera/pdp/datasets/data_profiling/text/information_technology.txt' AS (words:chararray);
docWords2 = FOREACH doc2 GENERATE 'information_technology.txt' ASdocumentId, FLATTEN(TOKENIZE(words)) AS word;

/*
Combine the relations using the UNION operator
*/
combined_docs = UNION docWords1, docWords2;

/*
Perform pre-processing by doing the following
Convert the data into lowercase
Remove stopwords
Perform stemming by calling custom UDF. it uses porter stemmer algorithm to perform stemming
*/
lowercase_data = FOREACH combined_docs GENERATE documentId asdocumentId, FLATTEN(TOKENIZE(LOWER($1))) as word;
joind = JOIN stopwords BY $0 RIGHT OUTER, lowercase_data BY $1;
stop_words_removed = FILTER joind BY $0 IS NULL;
processed_data = FOREACH stop_words_removed GENERATE documentId asdocumentId, com.profiler.unstructuredtextprofiling.Stemmer($2)as word;

/*
Calculate word count per word/doc combination using the Group and FOREACH statement and the result is stored in word_count
*/
grpd_processed_data = GROUP processed_data BY (word, documentId);
word_count = FOREACH grpd_processed_data GENERATE group ASwordDoc,COUNT(processed_data) AS wordCount;

/*
Calculate Total word count per document using the Group and FOREACH statement and the result is stored in total_docs_wc
*/
grpd_wc = GROUP word_count BY wordDoc.documentId;
grpd_wc_all = GROUP grpd_wc ALL;
total_docs = FOREACH grpd_wc_all GENERATEFLATTEN(grpd_wc),COUNT(grpd_wc) AS totalDocs;
total_docs_wc = FOREACH total_docs GENERATEFLATTEN(word_count),SUM(word_count.wordCount) AS wordCountPerDoc,totalDocs;

/*
Calculate Total document count per word is using the Group and FOREACH statement and the result is stored in doc_count_per_word 
*/
grpd_total_docs_wc = GROUP total_docs_wc BY wordDoc.word;
doc_count_per_word = FOREACH grpd_total_docs_wc GENERATEFLATTEN(total_docs_wc),COUNT(total_docs_wc) AS docCountPerWord;

/*
Calculate tfidf by invoking custom Java UDF.
The overall relevancy of a document with respect to a term is computed and the resultant data is stored in gen_tfidf
*/
gen_tfidf = FOREACH doc_count_per_word GENERATE $0.word AS word,$0.documentId AS documentId,com.profiler.unstructuredtextprofiling.GenerateTFIDF(wordCount,wordCountPerDoc,totalDocs,docCountPerWord) AS tfidf;

/*
Order by relevancy
*/
orderd_tfidf = ORDER gen_tfidf BY word ASC, tfidf DESC;

/*
The results are stored on the HDFS in the directory tfidf
*/
STORE orderd_tfidf into'/user/cloudera/pdp/output/data_profiling/unstructured_text_profiling/tfidf';
```

### 爪哇 UDF 是炮泥。

以下是 Java UDF 的代码片段:

```sh
public String exec(Tuple input) throws IOException {
    //Few declarations go here
    Stemmer s = new Stemmer();
    //Code for exception handling goes here
    //Extract values from the input tuple
    String str = (String)input.get(0);

    /*
    Invoke the stem(str) method of the class Stemmer.
    It return the stemmed form of the word
    */
    return s.stem(str);
}         
```

### TF-IDF 一代的爪哇 UDF

以下是用于计算 TF-IDF 的 Java UDF 代码片段:

```sh
public class GenerateTFIDF extends EvalFunc<Double>{
  @Override
  /**
  *The pre-calculated wordCount, wordCountPerDoc, totalDocs and docCountPerWord are passed as parameters to this UDF.
  */
  public Double exec(Tuple input) throws IOException {
    /*
    Retrieve the values from the input tuple
    */
    long countOfWords = (Long) input.get(0);
    long countOfWordsPerDoc = (Long) input.get(1);
    long noOfDocs = (Long) input.get(2);
    long docCountPerWord = (Long) input.get(3);
    /*
    Compute the overall relevancy of a document with respect to a term. 
    */
    double tf = (countOfWords * 1.0) / countOfWordsPerDoc;
    double idf = Math.log((noOfDocs * 1.0) / docCountPerWord);
    return tf * idf;
  }
} 
```

## 结果

以下是将设计模式应用于 `computer_science`和`information_technology`维基文本语料库的结果:

```sh
associat  information_technology.txt  0.0015489322470613302
author  information_technology.txt  7.744661235306651E-4
automat  computer_science.txt  8.943834587870262E-4
avail  computer_science.txt  0.0
avail  information_technology.txt  0.0
babbag  computer_science.txt  8.943834587870262E-4
babbage'  computer_science.txt  0.0026831503763610786
base  information_technology.txt  0.0
base  computer_science.txt  0.0
base.  computer_science.txt  8.943834587870262E-4
basic  information_technology.txt  7.744661235306651E-4
complex.  computer_science.txt  8.943834587870262E-4
compon  information_technology.txt  0.0015489322470613302
compsci  computer_science.txt  8.943834587870262E-4
comput  computer_science.txt  0.0
comput  information_technology.txt  0.0
computation  computer_science.txt  8.943834587870262E-4
computation.  computer_science.txt  8.943834587870262E-4
distinguish  information_technology.txt  7.744661235306651E-4
distribut	computer_science.txt  0.0
distribut	information_technology.txt  0.0
divid  computer_science.txt  8.943834587870262E-4
division.  computer_science.txt  8.943834587870262E-4
document  information_technology.txt  7.744661235306651E-4
encompass  information_technology.txt  7.744661235306651E-4
engin  computer_science.txt  0.0035775338351481047
engine.[5]  computer_science.txt  8.943834587870262E-4
enigma  computer_science.txt  8.943834587870262E-4
enough  computer_science.txt  0.0017887669175740523
enterprise.[2]  information_technology.txt  7.744661235306651E-4
entertain  computer_science.txt  8.943834587870262E-4
```

原文经过停词和词干提取阶段，然后计算词频-逆文档频率。结果显示单词、它所属的文档和 TF-IDF。TF-IDF 较高的单词意味着它们与其出现的文档有很强的关系，而 TF-IDF 较低的单词被认为质量较低，可以忽略。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter3/code/`
*   `Chapter3/datasets/`

# 总结

本章以[第二章](2.html "Chapter 2. Data Ingest and Egress Patterns")、*数据接收输出模式*所学知识为基础。在本章中，我们将来自多个源系统的数据进行集成，并将其接收到 Hadoop 中。下一步是通过查看组件值来找到关于数据类型的线索。检查这些值，查看它们是否被曲解，它们的单位是否被曲解，或者单位的上下文是否被错误导出。这种调查机制将在数据类型推断模型中详细讨论。

在基本统计分析模式下，我们收集数值的统计信息，检查这些数值是否满足用例的质量预期，从而找到以下问题的答案:对于数值字段，所有数值都是数值吗？可枚举字段的所有值都属于正确的集合吗？字段是否满足范围约束？它们完整吗？，等等。

模式匹配设计模式探索了一些通过数据类型、数据长度和正则表达式模式来度量数字和文本列数据集的一致性的技术。下一个模式通过使用各种统计方法揭示了表示字符串值的列的质量度量。这将在字符串分析设计模式中详细解释。非结构化文本分析设计模式试图形式化文本预处理技术(如停止词移除、词干分析和 TF-IDF 计算)来理解非结构化文本的质量。

在下一章中，我们将重点关注可以应用于各种数据格式的数据验证和清理模式。阅读本章后，观众将能够使用约束检查和正则表达式匹配等技术选择正确的模式来验证数据的准确性和完整性。我们还将在下一章讨论数据清理技术，如过滤器和统计清理。