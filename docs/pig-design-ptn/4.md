# 四、数据验证和清理模式

在前一章中，您已经学习了与数据分析相关的各种模式。通过这些模式，您已经了解了获取 Hadoop 集群中数据的属性、内容、上下文、结构和条件等重要信息的不同方法。在开始将数据清理成更有用的形式之前，将这些数据分析模式应用于数据生命周期。

本章涵盖以下设计模式:

*   **约束验证和清理模式**:这说明数据是根据一组约束进行验证，检查是否有缺失值，这些值是否在业务规则规定的范围内，或者这些值是否符合参照完整性和唯一性约束。根据业务规则，要么删除无效记录，要么对无效数据应用适当的清理步骤。
*   **regex 验证和清理模式**:这演示了通过使用正则表达式和基于模式的记录过滤来验证数据，以特定模式或长度匹配数据来清理无效数据。
*   **损坏数据验证和清理模式**:这为从各种来源了解数据损坏设置了上下文。该模型详细解释了噪声和异常值对数据的影响以及检测和清除它们的方法。
*   **非结构化文本数据验证和清洗模式**:这展示了通过执行预处理步骤，如小写转换、停止词移除、词干移除、标点符号移除、额外空间移除、数字识别和拼写错误识别来验证和清洗非结构化数据语料库的方法。

# 大数据的数据验证和清理

数据验证和清理过程检测并删除数据中不正确的记录。数据验证和清理的过程确保在数据用于分析过程之前，可以很好地识别数据中的不一致性。然后替换、修改或删除不一致的数据，使其更加一致。

大多数数据验证和清理都是通过基于模式分析静态约束来执行的。组合约束检查模式将告诉我们缺失值、空值、模糊表示、外键约束等的存在。

从大数据的角度来看，数据验证和清理在获取价值方面发挥着越来越重要的作用。清理大数据时，需要考虑的最大权衡之一是**时间质量**权衡。给定无限的时间，我们可以提高坏数据的质量，但设计一个好的数据清理脚本的挑战是在时限内覆盖尽可能多的数据并成功清理。

在典型的大数据用例中，不同类型数据的集成会增加清理过程的复杂性。来自联邦系统的高维数据有自己的清理方法，而大量纵向数据的清理方法是不同的。流式数据可以是时间序列数据，可以通过实时清洗的方式代替批量清洗机制进行有效处理。必须使用文本预处理和清理方法来处理非结构化数据、描述性数据和 web 数据。

Hadoop 环境中的各种接触点可能会导致数据不良。以下要点概述了映射到这些接触点的常见清洁问题:

*   **数据采集导致的大数据清理问题**:数据不一致可能是数据采集方式错误导致的。这些方法可以从手动输入到社交媒体数据(使用非标准单词)、输入重复数据和测量误差。
*   **数据传递不当导致的大数据清理**:这可能是进入与 Hadoop 集成的上游系统后数据转换不当。数据传输不正确导致数据不良的原因包括聚合不正确、空值转换为默认值、缓冲区溢出和传输问题。
*   **数据存储问题导致的大数据清理问题**:数据不一致可能是数据物理和逻辑存储问题的结果。物理存储导致的数据不一致是罕见的原因。这种情况发生在数据长时间存储并趋于损坏时，称为**位损坏**。将数据存储在逻辑存储结构中会导致以下问题:元数据不足、实体关系中缺少链接、缺少时间戳等。
*   **数据集成带来的大数据清洗问题**:集成异构系统的数据对不良数据的贡献很大，通常会导致不一致字段相关的问题。不同的定义、不同的字段格式、不正确的时间同步和遗留数据的特征、错误的处理算法都会导致这种情况。

一般来说，验证和清理大数据的第一步是用各种数学技术探索数据，以检测数据中是否有任何不准确之处。这一初步探索是通过了解每个属性的数据类型和域、其上下文、可接受的值等来完成的。之后，核实实际数据，验证是否符合可接受的限值。这给出了不准确特征及其位置的初步估计。在验证阶段，我们可以通过指定预期约束来执行这一探索活动，以查找和过滤不符合预期约束的数据，然后在清理步骤中对数据采取所需的操作。

在我们探索数据并发现异常之后，一系列数据清理例程将迭代运行。这些数据清理例程参考主数据和相关业务规则来执行清理，并获得更高质量数据的最终结果。数据清理例程通过填充缺失值、平滑有噪声的数据、识别或移除异常值以及解决不一致来清理数据。在执行清洗之后，执行可选的控制步骤，其中评估结果，并且异常处理在清洗过程中未被校正的元组。

# 选择 PIG 进行验证和清洗。

在 Hadoop 环境中的 Pig 中实现验证和清理代码减少了时间和质量之间的权衡，以及将数据移动到外部系统以执行清理的需要。下图描述了实施的高级概述:

![Choosing Pig for validation and cleansing](img/5556OS_04_01.jpg)

验证并清洗 PIG。

以下是在 Hadoop 环境下使用 Pig 进行数据清理的优势:

*   由于验证和清洗是在同一环境中进行的，因此整体性能得到了提高。无需将数据传输到外部系统进行清理。
*   Pig 非常适合编写用于验证和清理脚本的代码，因为内置函数适合处理混乱的数据和探索性分析。
*   Pig 通过链接复杂的工作流实现了清洗过程的自动化，对于定期更新的数据集非常方便。

# 约束验证和清洁设计模式

约束验证和清理模式根据一套规则和技术处理验证数据，然后清理无效数据。

## 背景

约束告诉我们数据应该服从的属性。它们可以应用于整个数据库、表、列或整个模式。这些约束是在设计时创建的规则，用于防止数据被破坏并减少处理错误数据的开销。它们指定哪些值对数据有效。

约束，如空值检查和范围检查，可以用来知道在 Hadoop 中获得的数据是否有效。通常，Hadoop 中的约束验证和数据清理可以基于业务规则来执行，业务规则实际上决定了必须应用于特定数据子集的约束类型。

在给定列必须属于特定类型的情况下，将应用数据类型约束。当我们想要强制一个约束时，比如一个数字或日期应该落在一个指定的范围内，一个范围约束被应用。这些范围约束通常指定用于比较的最小值和最大值。约束强制执行硬验证规则，以确保一些重要字段不会保持为空，这本质上是检查空值或缺失值，并使用一系列方法来消除它们。成员约束强制执行数据值应该总是来自一组预定值的规则。

无效数据可能是从没有在 Hadoop 中实现软件约束的遗留系统接收数据的结果，也可能是从电子表格等来源接收数据的结果，在这些来源中，对用户选择在单元格中输入的内容设置约束相对困难。

## 动机

约束验证和清理设计模式实现了一个 Pig 脚本，它通过检查数据是否在特定、指定和强制的范围约束内来验证数据，然后进行清理。

有许多方法可以检查 Hadoop 中驻留的数据是否符合强制约束。最有用的方法之一是检查空值或缺失值。如果给定的一组数据中有缺失的值，了解这些缺失的值是否是数据质量不足的原因是很重要的，因为在很多情况下，数据中有缺失的值是可以的。

查找空值或缺失值相对简单，但是通过用适当的值填充它们来清除缺失值是一项复杂的任务，这通常取决于业务案例。

根据业务案例，空值可以被忽略或手动输入，作为清理过程的一部分，但这种方法是最不推荐的。对于分类变量，可以使用一个恒定的全局标签，如“XXXXX”来描述缺失的值。如果此标签不能与表中的其他现有值冲突，或者缺少的值可以由最频繁出现的值(模式)替换。根据数据分布，建议使用正态分布数据的平均值和偏斜分布数据的中值。平均值和中值的使用仅适用于数字数据类型。使用概率度量，如贝叶斯推理或决策树，可以比其他情况更准确地计算缺失值，但这是一种耗时的方法。

范围通过指定有效值的上限和下限来限制数据中可以使用的值。设计模式首先检查数据的有效性，并找出数据是否不在指定的范围内。通过过滤无效数据，按照业务规则清理无效数据，或者如果无效数据高于最大范围值，则用最大范围值替换无效值；相反，如果无效数据低于范围，无效值将被最小范围值替换。

唯一约束将值的存在限制为在整个表中唯一。这适用于重复值等于无效数据的主键。一个表可以有任意数量的唯一约束，主键被定义为其中之一。Hadoop 接收到数据后，我们可以使用这种设计模式进行验证，找出数据是否满足唯一约束，并通过删除重复项来清理。

## 用例

当您摄取大量结构化数据，并希望通过对照强制、范围和唯一约束验证数据来执行完整性检查，然后清理数据时，您可以使用这种设计模式。

## 模式实现

这种设计模式作为一个独立的脚本在 Pig 中实现。该脚本加载数据，并根据指定的约束对其进行验证。以下是模式如何实现的简要描述:

*   **强制约束**:【T2】脚本检查不符合强制约束的无效和缺失数据，并通过用中值替换缺失值来移除缺失数据。
*   **范围约束** : 脚本定义了一个范围约束，说明`claim_amount`列的有效值应该在下限和上限之间。该脚本验证数据并找到该范围之外的所有值。在清洗步骤中，过滤这些值；它们还可以根据预定义的业务规则更新为范围的最小值和最大值。
*   **唯一约束**:【T2】脚本执行检查以验证数据是否不同，然后通过删除重复值来清除数据。

## 代码片段

为了说明模式的工作原理，我们考虑一个存储在 HDFS 的汽车保险理赔数据集，它包含两个文件。`automobile_policy_master.csv`为主文件；它包含唯一的 ID、车辆详细信息、价格和支付的保费。主文件用于验证索赔文件中的数据。`automobile_insurance_claims.csv`文件包含汽车保险理赔数据，具体为车辆修理费理赔；包括`CLAIM_ID`、`POLICY_MASTER_ID`、`VEHICLE_DETAILS`和`CLAIM_DETAILS`等属性。对于该模式，我们将对`CLAIM_AMOUNT`、`POLICY_MASTER_ID`、`AGE`和`CITY`进行约束验证和清理，如下代码所示:

```sh
/*
Register Datafu and custom jar files
*/
REGISTER '/home/cloudera/pdp/jars/datatypevalidationudf.jar';
REGISTER  '/home/cloudera/pdp/jars/datafu.jar';

/*
Define aliases for Quantile UDF from Datafu and custom UDF DataTypeValidationUDF.
The parameters to Quantile constructor specify list of quantiles to compute
The parameter to the DataTypeValidationUDF constructor specifies the Data type that would be used for validation
*/
DEFINE Quantile datafu.pig.stats.Quantile('0.25','0.5','0.75'); 
DEFINEDataTypeValidationUDF com.validation.DataTypeValidationUDF('double');

/*
Load automobile insurance claims data set into the relation claims and policy master data set into the relation policy_master
*/
claims = LOAD'/user/cloudera/pdp/datasets/data_validation/automobile_insurance_claims.csv' USING  PigStorage(',') AS(claim_id:chararray, policy_master_id:chararray,registration_no:chararray, engine_no:chararray,chassis_no:chararray,customer_id:int,age:int,first_name:chararray,last_name:chararray,street:chararray,address:chararray,  city:chararray,  zip:long,gender:chararray, claim_date:chararray,garage_city:chararray,bill_no:long,claim_amount:chararray,garage_name:chararray,claim_status:chararray);
policy_master = LOAD'/user/cloudera/pdp/datasets/data_validation/automobile_policy_master.csv' USING  PigStorage(',') AS(policy_master_id:chararray, model:int, make:chararray,price:double, premium:float);

/*
Remove duplicate tuples from the relation claims to ensure that the data meets unique constraint
*/
claims_distinct = DISTINCT claims;

/*
Invoke the custom DataTypeValidationUDF with the parameter claim_amount.
The UDF returns the tuples where claim_amount does not match the specified data type (double), these values are considered as invalid.
Invalid values are stored in the relation invalid_claims_amt
*/
claim_distinct_claim_amount = FOREACH claims_distinct GENERATEclaim_amount AS claim_amount;
invalid_c_amount = FOREACH claim_distinct_claim_amount GENERATEDataTypeValidationUDF(claim_amount) AS claim_amount;
invalid_claims_amt = FILTER invalid_c_amount BY claim_amount ISNOT NULL;

/*
Filter invalid values from the relation claims_distinct and segregate the valid and invalid claim amount
*/
valid_invalid_claims_amount_join = JOIN invalid_claims_amt BY claim_amount RIGHT, claims_distinct BY claim_amount;
valid_claims_amount = FILTER valid_invalid_claims_amount_join BY$0 IS NULL;
invalid_claims_amount = FILTER valid_invalid_claims_amount_join BY$0 IS NOT NULL;

/*
For each invalid_claims_amount, generate all the values and specify the reason for considering these values as invalid
*/
invalid_datatype_claims = FOREACH invalid_claims_amount GENERATE$1 AS claim_id,$2 AS policy_master_id, $3 AS registration_no,$4 AS engine_no, $5 AS chassis_no,$6 AS customer_id,$7 AS age,$8 AS first_name,$9 AS last_name, $10 AS street, $11 AS address,$12 AS city, $13 AS zip, $14 AS gender, $15 AS claim_date,$16 AS garage_city,$17 AS bill_no, $18 AS claim_amount,$19 ASgarage_name, $20 AS claim_status,'Invalid Datatype forclaim_amount' AS reason;

valid_datatype_claims = FOREACH valid_claims_amount GENERATE $1 ASclaim_id,$2 AS policy_master_id, $3 AS registration_no,$4 AS engine_no, $5 AS chassis_no,$6 AS customer_id,$7 AS age,$8 AS first_name,$9 AS last_name, $10 AS street, $11 AS address,$12 AS city, $13 AS zip, $14 AS gender, $15 AS claim_date,$16 AS garage_city,$17 AS bill_no, $18 AS claim_amount,$19 AS garage_name, $20 AS claim_status;

/*
Compute quantiles using Datafu's Quantile UDF
*/
groupd = GROUP valid_datatype_claims ALL;
quantiles = FOREACH groupd {
  sorted = ORDER valid_datatype_claims BY age;
  GENERATE Quantile(sorted.age) AS quant;
}

/*
Check for occurrence of null values for the column Age which is a numerical field and for city which is a categorical field.
The nulls in age column are replaced with median and the nulls in city column are replaced with a constant string XXXXX.
*/
claims_replaced_nulls = FOREACH valid_datatype_claims GENERATE $0,$1 ,$2 , $3 ,$4 , $5 ,(int) ($6 is null ? FLOOR(quantiles.quant.quantile_0_5) : $6) AS age, $7, $8 ,$9 , $10 ,($11 is null ? 'XXXXX' : $11) AS city, $12, $13 , $14 , $15 ,$16 ,(double)$17 , $18 ,$19;

/*
Ensure Referential integrity by checking if the policy_master_id in the claims dataset is present in the master dataset.
The values in the claims dataset that do not find a match in the master dataset are considered as invalid values and are removed.
*/
referential_integrity_check = JOIN claims_replaced_nulls BYpolicy_master_id, policy_master BY policy_master_id;
referential_integrity_invalid_data = JOIN policy_master BYpolicy_master_id RIGHT, claims_replaced_nulls BYpolicy_master_id;
referential_check_invalid_claims = FILTERreferential_integrity_invalid_data BY $0 IS NULL;

/*
For each referential_check_invalid_claims, generate all the values and specify the reason for considering these values as invalid
*/
invalid_referential_claims = FOREACHreferential_check_invalid_claims GENERATE  $5 ,$6, $7, $8 ,$9 ,$10 , $11, $12, $13 , $14 , $15 , $16 ,$17 , $18 ,$19,$20,  $21 ,(chararray) $22 , $23 ,$24,'Referential check Failed for policy_master_id' AS reason;

/*
Perform Range validation by checking if the values in the claim_amount column are within a range of 7% to 65% of the price in the master dataset.
The values that fall outside the range are considered as invalid values and are removed.
*/
referential_integrity_valid_claims = FILTERreferential_integrity_check BY( claims_replaced_nulls::claim_amount>=(policy_master::price*7/100) ANDclaims_replaced_nulls::claim_amount<=(policy_master::price*65/100 ));
valid_claims = FOREACH referential_integrity_valid_claims GENERATE$0, $1 ,$2 , $3 ,$4 , $5 ,$6 , $7, $8 ,$9 , $10 , $11 , $12,$13 , $14 , $15 , $16 ,$17 , $18 ,$19;
invalid_range = FILTER referential_integrity_check BY( claims_replaced_nulls::claim_amount<=(policy_master::price*7/100) ORclaims_replaced_nulls::claim_amount>=(policy_master::price*65/100 ));

/*
For each invalid_range, generate all the values and specify the reason for considering these values as invalid
*/
invalid_claims_range = FOREACH invalid_range GENERATE $0, $1 ,$2 ,$3 ,$4 , $5 ,$6, $7, $8 ,$9 , $10 , $11, $12, $13 , $14 , $15 ,$16 ,(chararray)$17 , $18 ,$19,'claim_amount not within range' AS reason;

/*
Combine all the relations containing invalid values. 
*/
invalid_claims = UNIONinvalid_datatype_claims,invalid_referential_claims,invalid_claims_range;

/*
The results are stored on the HDFS in the directories valid_data and invalid_data
The values that are not meeting the constraints are written to a file in the folder invalid_data. This file has an additional column specifying the reason for elimination of the record, this can be used for further analysis.
*/
STORE valid_claims INTO'/user/cloudera/pdp/output/data_validation_cleansing/constraints_validation_cleansing/valid_data';
STORE invalid_claims INTO'/user/cloudera/pdp/output/data_validation_cleansing/constraints_validation_cleansing/invalid_data';
```

## 结果

以下为原始数据集的片段；为了提高可读性，我们删除了一些列。

```sh
claim_id,policy_master_id,cust_id,age,city,claim_date,claim_amount
A123B39,A213,39,34,Maryland,5/13/2012,147157
A123B39,A213,39,34,Maryland,5/13/2012,147157
A123B13,A224,13,,Minnesota,2/18/2012,8751.24
A123B70,A224,70,59,,4/2/2012,8751.24
A123B672,A285AC,672,52,Las Vegas,10/19/2012,7865.73
A123B726,A251ext,726,26,Las Vegas,4/6/2013,4400
A123B21,A214,21,41,Maryland,2/28/2009,1230000000
A123B40,A214,40,35,Austin,6/30/2009,29500  
A123B46,A220,46,32,Austin,12/29/2011,13986 Amount
A123B20,A213,20,42,Redmond,5/18/2013,147157 Price
A123B937,A213,937,35,Minnesota,9/27/2009,147157
```

以下是将此模式应用于数据集的结果:

*   有效数据

    ```sh
    A123B39,A213,39,34,Maryland,5/13/2012,147157
    A123B13,A224,13,35,Minnesota,2/18/2012,8751.24
    A123B70,A224,70,59,XXXXX,4/2/2012,8751.24
    A123B937,A213,937,35,Minnesota,9/27/2009,147157
    ```

*   无效数据

    ```sh
    A123B672,A285AC,672,52,Las Vegas,10/19/2012,7865.73,Referential check Failed for policy_master_id
    A123B726,A251ext,726,26,Las Vegas,4/6/2013,4400,Referential check Failed for policy_master_id
    A123B21,A214,21,41,Maryland,2/28/2009,1230000000,claim_amount not within range
    A123B40,A214,40,35,Austin,6/30/2009,29500,claim_amount not within range
    A123B46,A220,46,32,Austin,12/29/2011,13986 Amount,InvalidDatatype for claim_amount
    A123B20,A213,20,42,Redmond,5/18/2013,147157 Price,InvalidDatatype for claim_amount
    ```

获得的 T10 数据可以分为有效数据和无效数据。在之前的结果中，删除了带有`claim_id A123B39`的重复记录。对于有`claim_id A123B13`的记录，`age`的空值被`35`(中间值)代替，对于有`claim_id A123B70`的记录，`city`的空值被`XXXXX`代替。此外，有效数据还包含与列`claim_amount`和`policy_master_id`上的数据类型、范围和引用完整性约束相匹配的记录列表。无效数据被写入文件夹`invalid_data`中的文件。文件的最后一列提到了记录被认为无效的原因。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter4/code/`
*   `Chapter4/datasets/`

# 正则表达式验证和清洗设计模式

本设计模式处理使用正则表达式函数验证数据。regex 函数可用于验证数据以匹配特定长度或模式，并清除无效数据。

## 背景

这个设计模式讨论了如何使用正则表达式函数来识别和清除字段长度无效的数据。该模式还从数据中识别具有指定日期格式的所有值，并删除不符合指定格式的无效值。

## 动机

识别长度不正确的字符串数据是知道数据是否准确的最快方法之一。通常，我们需要这个字符串长度参数来判断数据，而不需要深入数据。当字符串的长度必须有上限时，这将非常有用，例如，美国州代码的长度上限通常为 2。

查找与日期模式匹配的所有字符串模式是最常见的数据转换之一。日期很容易用多种方式表达(日/月/YY、月/日/YY、年/月/日等)。).转换包括发现这些模式的出现，并将所有这些格式标准化为业务规则所需的统一日期格式。

## 用例

正则表达式用于要求字符串完全或部分匹配的情况。以下是执行**提取、转换和加载** ( **ETL** )时的一些常见用例:

*   **字符串长度和模式验证**:如果数据的结构是标准格式，并且数据匹配指定的长度，则使用正则表达式来验证数据。例如，如果一个字段以字母开头，后跟三个数字，它可以帮助验证数据。
*   **过滤与特定模式不匹配的字段**:如果你的业务案例需要你剔除与特定模式不匹配的数据，这个可以在清理阶段使用；例如，筛选日期与预定义格式不匹配的记录。
*   **将字符串拆分成标签**:非结构化文本可以使用正则表达式解析并拆分成标签。一个常见的例子是使用`\s`标记将文本拆分成单词，这意味着按空格拆分。另一个用途是使用模式来分割字符串以获得前缀或后缀。例如，从字符串“`100 dollars`”中提取`100`的值。
*   **提取数据匹配模式**:这个有一些用处。您可以从一个巨大的文件中提取一些文本匹配模式。文件预处理就是一个例子；您可以形成一个正则表达式，从一个巨大的日志中提取请求或响应模式，并进一步分析提取的数据。

## 模式实现

该设计模式在 Pig 中作为独立脚本实现。脚本加载数据，并根据正则表达式模式对其进行验证。

*   **字符串模式和长度**:脚本验证`policy_master_id`列中的值是否与预定义的长度和模式匹配。与图案或长度不匹配的值将被删除。
*   **日期格式**:脚本验证`claim_date`列的值是否与 MM/DD/YYYY 日期格式匹配；过滤日期格式无效的记录。

## 代码片段

为了说明该模型的工作原理，我们考虑一个存储在 HDFS 的汽车保险索赔数据集，它包含两个文件。`automobile_policy_master.csv`为主文件；它包含唯一的 ID、车辆详细信息、价格和支付的保费。主文件用于验证索赔文件中的数据。`automobile_insurance_claims.csv`文件包含汽车保险理赔数据，具体为车辆修理费理赔；包括`CLAIM_ID`、`POLICY_MASTER_ID`、`VEHICLE_DETAILS`和`CLAIM_DETAILS`等属性。对于这种模式，我们将对`POLICY_MASTER_ID`和`CLAIM_DATE`进行正则表达式验证和清理，如下代码所示:

```sh
/*
Load automobile insurance claims dataset into the relation claims
*/
claims = LOAD'/user/cloudera/pdp/datasets/data_validation/automobile_insurance_claims.csv' USING  PigStorage(',') AS(claim_id:chararray, policy_master_id:chararray,registration_no:chararray, engine_no:chararray,chassis_no:chararray,customer_id:int,age:int,first_name:chararray,last_name:chararray,street:chararray,address:chararray,city:chararray,zip:long,gender:chararray, claim_date:chararray,garage_city:chararray,bill_no:long,claim_amount:chararray,garage_name:chararray,claim_status:chararray);

/*
Validate the values in the column policy_master_id with a regular expression to match the pattern where the value should start with an alphabet followed by three digits.
The values that do not match the pattern or length are considered as invalid values and are removed.
*/
valid_policy_master_id = FILTER claims BY policy_master_id MATCHES'[aA-zZ][0-9]{3}';

/*
Invalid values are stored in the relation invalid_length
*/
invalid_policy_master_id = FILTER claims BY NOT(policy_master_id MATCHES '[aA-zZ][0-9]{3}');
invalid_length = FOREACH invalid_policy_master_id GENERATE $0,$1 ,$2 , $3 ,$4 , $5 ,$6 , $7, $8 ,$9 , $10 , $11, $12, $13 ,$14 , $15 , $16 ,$17 , $18 ,$19,'Invalid length or pattern for policy_master_id' AS reason;

/*
Validate the values in the column claim_date to match MM/DD/YYYY format, also validate the values given for MM and DD to fall within 01 to 12 for month and 01 to 31 for day
The values that do not match the pattern are considered as invalid values and are removed.
*/
valid_claims = FILTER valid_policy_master_id BY( claim_date MATCHES '^(0?[1-9]|1[0-2])[\\/](0?[1-9]|[12][0-9]|3[01])[\\/]\\d{4}$');

/*
Invalid values are stored in the relation invalid_date
*/
invalid_dates = FILTER valid_policy_master_id BY NOT( claim_date MATCHES '^(0?[1-9]|1[0-2])[\\/](0?[1-9]|[12][0-9]|3[01])[\\/]\\d{4}$');
invalid_date = FOREACH invalid_dates GENERATE $0, $1 ,$2 , $3 ,$4 , $5 ,$6 , $7, $8 ,$9 , $10 , $11, $12, $13 , $14 , $15 ,$16 ,$17 , $18 ,$19,'Invalid date format for claim_date' AS reason;

/*
Combine the relations that contain invalid values. 
*/
invalid_claims = UNION invalid_length,invalid_date;

/*
The results are stored on the HDFS in the directories valid_data and invalid_data
The invalid values are written to a file in the folder invalid_data. This file has an additional column specifying the reason for elimination of the record, this can be used for further analysis.
*/
STORE valid_claims INTO'/user/cloudera/pdp/output/data_validation_cleansing/regex_validation_cleansing/valid_data';
STORE invalid_claims INTO'/user/cloudera/pdp/output/data_validation_cleansing/regex_validation_cleansing/invalid_data';
```

## 结果

以下为原始数据集的片段；为了提高可读性，我们删除了一些列。

```sh
claim_id,policy_master_id,cust_id,age,city,claim_date,claim_amount
A123B1,A290,1,42,Minnesota,1/5/2011,8211
A123B672,A285AC,672,52,Las Vegas,10/19/2012,7865.73
A123B726,A251ext,726,26,Las Vegas,4/6/2013,4400
A123B2,A213,2,35,Redmond,1/22/2009,147157
A123B28,A221,28,19,Austin,6/37/2012,31930.2
A123B888,A247,888,49,Las Vegas,21/20/2012,873
A123B3,A214,3,23,Maryland,7/8/2011,8400
```

以下是将此模式应用于数据集的结果:

*   有效数据

    ```sh
    A123B1,A290,1,42,Minnesota,1/5/2011,8211
    A123B2,A213,2,35,Redmond,1/22/2009,147157
    A123B3,A214,3,23,Maryland,7/8/2011,8400
    ```

*   无效数据

    ```sh
    A123B672,A285AC,672,52,Las Vegas,10/19/2012,7865.73,Invalid length or pattern for policy_master_id
    A123B726,A251ext,726,26,Las Vegas,4/6/2013,4400,Invalid length or pattern for policy_master_id
    A123B28,A221,28,19,Austin,6/37/2012,31930.2,Invalid date format for claim_date
    A123B888,A247,888,49,Las Vegas,21/20/2012,873,Invalid date format for claim_date
    ```

如前所示，结果数据分为有效数据和无效数据。有效数据包含符合 regex 模式的记录列表，用于验证`policy_master_id`和`claim_date`。无效数据写入`invalid_data`文件夹中的文件；文件的最后一列提到了记录被认为无效的原因。我们选择过滤无效数据；但是，清理技术取决于业务案例，在这种情况下，无效数据可能必须转换为有效数据。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter4/code/`
*   `Chapter4/datasets/`

# 数据验证和损坏数据清理的设计模式

这个设计模式从被视为噪声或异常值的损坏数据的角度来讨论数据损坏。详细讨论了受损数据的识别和去除技术。

## 背景

本设计模式探索使用 Pig 来验证和清除数据集中的损坏数据。它试图从传感器到结构化数据的各种大数据源中设置数据损坏的背景。此设计模式从两个角度检测数据损坏，一个是噪声，另一个是异常值，如下表所示:

*   噪声可以定义为测量中的随机误差，它导致损坏的数据与正确的数据一起被吸收。误差与期望值相差不远。
*   异常值也是一种噪声，但误差值与期望值相差太远。异常值对分析的准确性有很大影响。它们通常是测量或记录误差。其中一些可以代表有趣的现象，从应用领域的角度来看非常重要，这意味着不应该消除所有的异常值。

受损数据可以表示为噪声或异常值，两者的主要区别在于与期望值的差异程度。噪声数据的变化程度小，其值更接近原始数据，而异常值的变化程度大，其值远离原始数据。为了说明下面这组数字中的例子，4 可以视为噪声，21 可以视为异常值。

A = [1，2，1，4，1，2，1，1，1，2，21，1，2，2]

数据损坏会增加执行分析所需的工作量，还会对数据挖掘分析的准确性产生不利影响。以下几点说明了适用于大数据的几种数据损坏来源:

*   **传感器数据中的数据损坏**:传感器数据已经成为我们数据世界大量大数据源中最大的数据源之一。这些传感器通常长时间产生大量数据，由于数据不准确，导致各种计算挑战。Hadoop 被广泛应用于纵向传感器数据中的模式挖掘，而这个过程中最大的挑战之一就是传感器数据的自然误差和不完整性。传感器的电池寿命不足，很多传感器可能长时间无法发送准确的数据，从而损坏数据。
*   **结构化数据中的数据损坏**:在结构化大数据的上下文中，任何数据，无论是数字数据还是分类数据，如果存储在 Hadoop 中的方式使其不能被为数据编写的任何数据处理例程读取或使用，都可以被视为损坏。

## 动机

通过应用适当的噪声和异常检测技术，可以验证和清除损坏的数据。用于此目的的常用技术有宁滨、回归和聚类，如下表所示:

*   **宁滨**:这个技术可以用来识别噪声和异常值。该技术也用于通过应用平滑函数来去除噪声数据。宁滨的工作原理是创建一组分类值，这些值被分成不同的容器。这些值除以等频率或等宽度。为了平滑数据(或去除噪声)，给定分区或框中的原始数据值被该分区的平均值或中值替换。在当前的设计模式下，我们将解释宁滨消除噪声的适用性。
*   **回归**:它是一种将数据值拟合到函数的技术。回归可用于通过识别回归函数并移除远离函数预测值的所有数据值来消除噪声。线性回归找到“最合适”的线性函数来拟合两个变量，这样一个变量可以用来预测另一个变量。线性回归类似于线性回归，涉及两个以上的变量，数据适用于多维曲面。
*   **聚类**:离群值分析可以通过聚类来执行，通过将相似的值分组在一起来找到在聚类之外并且可以被认为是离群值的值。一个集群包含的值与同一集群中的其他值相似，但与其他集群中的值不同。您可以将一组值视为一个组，以便在宏级别与其他组值进行比较。

寻找异常值的另一种方法是计算 **( **iqr** )的四分位数区间。在该方法中，首先根据这些值计算三个四分位数(Q1、Q2 和 Q3)。四分位数将值分成四个相等的组，每个组包含四分之一的数据。上下两列用三个四分位数计算，高于或低于这两列的任何值都被认为是异常值。边界是定义异常值范围的指南。在当前的设计模式中，我们使用这种方法来发现异常值。**

## 用例

您可以考虑使用这种设计模式，通过去除噪声和异常值来去除损坏的数据。这种设计模式将有助于理解如何将数据分类为噪声或异常值，然后将其移除。

## 模式实现

这个设计模式是使用第三方库`datafu.jar`作为独立的 Pig 脚本实现的。该脚本实现了识别和去除噪声和异常值。

宁滨技术识别并消除噪音。在宁滨，这些值被分类并分配到多个箱中。确定每个库的最小值和最大值，并将它们设置为库边界。每个箱值由最近的箱边界值代替。这种方法被称为通过面元边界平滑。为了识别异常值，我们使用标准盒图规则方法；它根据数据分布的上四分位数和下四分位数发现异常值。使用 *(Q1-C * iqd，Q3+C * iqd)* 计算数据分布的 Q1 和 Q3 及其四分位间距，给出数据应该落在的范围。这里，c 是一个值为 1.5 的常数。超出此范围的值被视为异常值。该脚本使用 Datafu 库来计算四分位数。

## 代码片段

为了说明这个模型的工作原理，我们考虑一个存储在 HDFS 的汽车保险理赔数据集，它包含两个文件。`automobile_policy_master.csv`为主文件；它包含唯一的 ID、车辆详细信息、价格和支付的保费。主文件用于验证索赔文件中的数据。`automobile_insurance_claims.csv`文件包含汽车保险理赔数据，具体为车辆修理费理赔；包括`CLAIM_ID`、`POLICY_MASTER_ID`、`VEHICLE_DETAILS`和`CLAIM_DETAILS`等属性。对于该模式，我们将在`CLAIM_AMOUNT`和`AGE`上验证和清理损坏的数据，如下代码所示:

```sh
/*
Register Datafu jar file
*/
REGISTER  '/home/cloudera/pdp/jars/datafu.jar';

/*
Define alias for the UDF quantile
The parameters specify list of quantiles to compute
*/
DEFINE Quantile datafu.pig.stats.Quantile('0.25','0.50','0.75'); 

/*
Load automobile insurance claims data set into the relation claims
*/
claims = LOAD'/user/cloudera/pdp/datasets/data_validation/automobile_insurance_claims.csv' USING  PigStorage(',') AS(claim_id:chararray, policy_master_id:chararray,registration_no:chararray, engine_no:chararray,chassis_no:chararray,customer_id:int,age:int,first_name:chararray,last_name:chararray,street:chararray,address:chararray,city:chararray,zip:long,gender:chararray, claim_date:chararray,garage_city:chararray,bill_no:long,claim_amount:double,garage_name:chararray,claim_status:chararray);

/*
Sort the relation claims by age
*/
claims_age_sorted = ORDER claims BY age ASC;

/*
Divide the data into equal frequency bins. 
Minimum and maximum values are identified for each bin and are set as bin boundaries.
Replace each bin value with the nearest bin boundary.
*/
bin_id_claims = FOREACH claims_age_sorted GENERATE(customer_id - 1) * 10 / (130- 1 + 1) AS bin_id, $0 ,$1 ,$2 ,$3 ,$4 ,$5 ,$6 ,$7 ,$8 ,$9 ,$10 ,$11 ,$12 ,$13 ,$14 ,$15 ,$16 ,$17 ,$18 ,$19 ;
group_by_id = GROUP bin_id_claims BY bin_id;
claims_bin_boundaries = FOREACH group_by_id
{
  bin_lower_bound=(int) MIN(bin_id_claims.age);
  bin_upper_bound = (int)MAX(bin_id_claims.age);
  GENERATE bin_lower_bound AS bin_lower_bound, bin_upper_bound ASbin_upper_bound, FLATTEN(bin_id_claims);
};
smoothing_by_bin_boundaries = FOREACH claims_bin_boundariesGENERATE $3 AS claim_id,$4 AS policy_master_id,$5 ASregistration_no,$6 AS engine_no,$7 AS chassis_no,$8 AS customer_id,( ( $9 - bin_lower_bound ) <=( bin_upper_bound - $9 ) ? bin_lower_bound : bin_upper_bound )AS age,$10 AS first_name,$11 AS last_name,$12 AS street,$13 AS address,$14 AS city,$15 AS zip,$16 AS gender,$17 AS claim_date,$18 AS garage_city,$19 AS bill_no,$20 AS claim_amount,$21 AS garage_name,$22 AS claim_status;

/*
Identify outliers present in the column claim_amount by calculating the quartiles, interquartile distance and the upper and lower fences.
The values that do not fall within this range are considered as outliers and are filtered out.
*/
groupd = GROUP smoothing_by_bin_boundaries ALL;
quantiles = FOREACH groupd { 
  sorted = ORDER smoothing_by_bin_boundaries BY claim_amount;
  GENERATE Quantile(sorted.claim_amount) AS quant;
}
valid_range = FOREACH quantiles GENERATE(quant.quantile_0_25 - 1.5 * (quant.quantile_0_75 - quant.quantile_0_25)) ,(quant.quantile_0_75 + 1.5 *(quant.quantile_0_75 - quant.quantile_0_25));
claims_filtered_outliers = FILTER smoothing_by_bin_boundaries BYclaim_amount>= valid_range.$0 AND claim_amount<= valid_range.$1;

/*
Store the invalid values in the relation invalid_claims
*/
invalid_claims_filter = FILTER smoothing_by_bin_boundaries BY claim_amount<= valid_range.$0 OR claim_amount>= valid_range.$1;
invalid_claims = FOREACH invalid_claims_filter GENERATE $0 ,$1 ,$2 ,$3 ,$4 ,$5 ,$6 ,$7 ,$8 ,$9 ,$10 ,$11 ,$12 ,$13 ,$14 ,$15 ,$16 ,$17 ,$18 ,$19,'claim_amount identified as Outlier' as reason;

/*
The results are stored on the HDFS in the directories valid_data and invalid_data
The invalid values are written to a file in the folder invalid_data. This file has an additional column specifying the reason for elimination of the record, this can be used for further analysis.
*/
STORE invalid_claims INTO'/user/cloudera/pdp/output/data_validation_cleansing/corrupt_data_validation_cleansing/invalid_data';
STORE claims_filtered_outliers INTO'/user/cloudera/pdp/output/data_validation_cleansing/corrupt_data_validation_cleansing/valid_data';
```

## 结果

以下为原始数据集的片段；为了提高可读性，我们删除了一些列。

```sh
claim_id,policy_master_id,cust_id,age,city,claim_date,claim_amount
A123B6,A217,6,42,Las Vegas,6/25/2010,-12495
A123B11,A222,11,21,,11/5/2012,293278.7,claim_amount identified asOutlier
A123B2,A213,2,42,Redmond,1/22/2009,147157,claim_amount identifiedas Outlier
A123B9,A220,9,21,Maryland,9/20/2011,13986
A123B4,A215,4,42,Austin,12/16/2011,35478
```

以下是将此模式应用于数据集的结果:

*   有效数据

    ```sh
    A123B6,A217,6,42,Las Vegas,6/25/2010,-12495
    A123B9,A220,9,21,Maryland,9/20/2011,13986
    A123B4,A215,4,42,Austin,12/16/2011,35478
    ```

*   无效数据

    ```sh
    A123B11,A222,11,21,,11/5/2012,293278.7,claim_amount identified as Outlier
    A123B2,A213,2,42,Redmond,1/22/2009,147157,claim_amount identified as Outlier
    ```

如前所示，结果数据分为有效数据和无效数据。有效数据有一个记录列表，其中`age`列的噪声被平滑。检测`claim_amount`栏的异常值，上下栏标为`-34929.0`、`70935.0`；不在此范围内的值被识别为异常值，并写入文件夹`invalid_data`中的文件。文件的最后一列显示了记录被认为无效的原因。过滤异常值，数据存储在`valid_data`文件夹中。删除离群值的前一个脚本；但是，这一决定可能会因业务规则而异。

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter4/code/`
*   `Chapter4/datasets/`

# 非结构化文本数据验证和清理的设计模式

非结构化文本验证和清理模式 n 演示了应用各种数据预处理技术清理非结构化数据的方法。

## 背景

用 Hadoop 处理海量非结构化数据是一项具有挑战性的任务，需要清理并做好处理准备。文本数据，包括文档、电子邮件、文本文件和聊天文件，在被 Hadoop 接收时，本质上是无组织的，没有定义的数据模型。

为了打开非结构化数据进行分析，我们必须引入类似于结构的东西。组织非结构化数据的基础是通过在数据存储中执行有计划和有控制的数据清理转换和流动，将其与企业中现有的结构化数据集成在一起进行操作和/或分析。非结构化数据的集成对于结果数据的有意义的查询和分析是必要的。

接收非结构化数据后的第一步是从文本数据中发现元数据，并以有利于进一步处理的方式进行组织，从而消除数据中的一些不规则和歧义。这种元数据的创建本身就是一个多步骤的迭代过程，采用各种数据解析、清洗和转换技术，从简单的实体提取和语义标注到使用人工智能算法的自然语言处理。

## 动机

该设计模式展示了一种通过执行预处理步骤来清理非结构化数据语料库的方法，如小写转换、停止词移除、词干、标点符号移除、额外空间移除、数字识别和拼写错误识别。

这个模型的动机是了解非结构化数据中的各种不一致性，并帮助识别和消除这些问题。

非结构化数据容易出现从完整性到不一致性的各种质量问题。以下是非结构化文本的常见清理步骤:

*   文本也可以用替代拼写来表示；例如，一个名字可以用不同的方式书写。如果拼写不同，但仍然指向同一个实体，则搜索该名称将不会得到结果。一方面，可以认为是拼写错误。集成和清理这些替代拼写来引用同一个实体可以减少歧义。从非结构化文本到结构化格式的有效转换需要我们考虑所有可选的拼写。
*   拼写错误也是很多不规范的原因，会影响分析的准确性。为了使数据可处理，我们必须识别拼错的单词并用正确的单词替换它们。
*   文本中的数字标识使我们能够选择所有的数字。根据业务环境，这些提取的号码可以包含在进一步的处理中或从进一步的处理中删除。数据清理也可以通过从文本中提取数字来执行；例如，如果文本由短语“1000”组成，则可以将其转换为 1000 来执行适当的分析。
*   使用正则表达式提取与特定模式匹配的数据可能是一种清理方法。例如，您可以通过指定模式从文本中提取日期。如果提取的日期不是标准格式(DD/MM/YY)，则标准化日期可能是按日期读取和索引非结构化数据的清理活动之一。

## 用例

考虑数据被 Hadoop 摄取后，这种设计模式可以通过删除拼写错误、标点符号等来清理非结构化数据。

## 模式实现

这个设计模式实现为一个独立的 Pig 脚本，内部使用右连接去除停止词。Stopword 列表首先从外部文本文件加载到关系中，然后在外部连接中使用。

`LOWER`功能用于将所有单词转换为小写。使用`REPLACE`功能匹配特定单词删除标点符号。同样，通过使用`REPLACE`匹配文本中所有数字的模式来删除数字。

拼错单词的代码使用布鲁姆过滤器，该过滤器最近在 0.10 版本中被包含为 UDF。

Bloom filter 是一种空间优化的数据结构，专门用于通过测试属于较小数据集的元素是否是较大数据集的成员，从较大的数据集中过滤出较小的数据集。Bloom filter 内部实现了一种巧妙的机制来存储每个元素，从而使用恒定的内存量，而不管元素的大小，从而实现彻底的空间优化。虽然布隆过滤器与其他结构相比具有巨大的空间优势，但过滤并不完全准确，因为可能存在误报的可能性。

Pig 通过调用`BuildBloom`来支持 Bloom filter，通过从 Pig 关系中存储的字典语料库加载的值列表中加载和训练 Bloom filter 来构建 Bloom filter。存储在分布式缓存中并内部传输到`Mapper`功能的经过训练的布隆过滤器用于通过使用`BLOOM` UDF 进行`FILTER`操作来对输入数据执行实际的过滤操作。在布隆过滤器消除所有拼写错误的单词后，过滤后的结果集将是拼写正确的单词。

## 代码片段

为了演示这个模型是如何工作的，我们考虑了存储在 HDFS 可访问的文件夹中的维基百科文本语料库。这个样本语料库由与计算机科学和信息技术相关的维基页面组成。一些拼错的单词被故意引入语料库，以展示该模型的功能。

```sh
/*
Define alias for the UDF BuildBloom.
The first parameter to BuildBloom constructor is the hashing technique to use, the second parameter specifies the number of distinct elements that would be placed in the filter and the third parameter is the acceptable rate of false positives.
*/
DEFINE BuildBloom BuildBloom('jenkins', '75000', '0.1');

/*
Load dictionary words
*/
dict_words1 = LOAD'/user/cloudera/pdp/datasets/data_validation/unstructured_text/dictionary_words1.csv' as (words:chararray); 
dict_words2 = LOAD'/user/cloudera/pdp/datasets/data_validation/unstructured_text/dictionary_words2.csv' as (words:chararray);

/*
Load stop words
*/
stop_words_list = LOAD'/user/cloudera/pdp/datasets/data_validation/unstructured_text/stopwords.txt' USING PigStorage();
stopwords = FOREACH stop_words_list GENERATEFLATTEN(TOKENIZE($0));

/*
Load the document corpus and tokenize to extract the words
*/
doc1 = LOAD'/user/cloudera/pdp/datasets/data_validation/unstructured_text/computer_science.txt' AS (words:chararray);
docWords1 = FOREACH doc1 GENERATE FLATTEN(TOKENIZE(words)) ASword;
doc2 = LOAD'/user/cloudera/pdp/datasets/data_validation/unstructured_text/information_technology.txt' AS (words:chararray);
docWords2 = FOREACH doc2 GENERATE FLATTEN(TOKENIZE(words)) ASword;

/*
Combine the contents of the relations docWords1 and docWords2
*/
combined_docs = UNION docWords1, docWords2;

/*
Convert to lowercase, remove stopwords, punctuations, spaces, numbers.
Replace nulls with the value "dummy string"
*/
lowercase_data = FOREACH combined_docs GENERATEFLATTEN(TOKENIZE(LOWER($0))) as word;
joind = JOIN stopwords BY $0 RIGHT OUTER, lowercase_data BY $0;
stop_words_removed = FILTER joind BY $0 IS NULL;
punctuation_removed = FOREACH stop_words_removed  
{
  replace_punct = REPLACE($1,'[\\p{Punct}]','');
  replace_space = REPLACE(replace_punct,'[\\s]','');
  replace_numbers = REPLACE(replace_space,'[\\d]','');
  GENERATE replace_numbers AS replaced_words;
}
replaced_nulls = FOREACH punctuation_removed GENERATE(SIZE($0) > 0 ? $0 : 'dummy string') as word;

/*
Remove duplicate words
*/
unique_words_corpus = DISTINCT replaced_nulls;

/*
Combine the two relations containing dictionary words
*/
dict_words = UNION dict_words1, dict_words2;

/*
BuildBloom builds a bloom filter that will be used in Bloom.
Bloom filter is built on the relation dict_words which contains all the dictionary words.
The resulting file dict_words_bloom is used in bloom filter by passing it to Bloom.
The call to bloom returns the words that are present in the dictionary, we select the words that are not present in the dictionary and classify them as misspelt words. The misspelt words are filtered from the original dataset and are stored in the folder invalid_data.
*/
dict_words_grpd = GROUP dict_words all;
dict_words_bloom = FOREACH dict_words_grpd GENERATEBuildBloom(dict_words.words);
STORE dict_words_bloom into 'dict_words_bloom';
DEFINE bloom Bloom('dict_words_bloom');
filterd = FILTER unique_words_corpus BY NOT(bloom($0));
joind = join filterd by $0, unique_words_corpus by $0;
joind_right = join filterd by $0 RIGHT, unique_words_corpus BY $0;
valid_words_filter = FILTER joind_right BY $0 IS NULL;
valid_words = FOREACH valid_words_filter GENERATE $1;
misspellings = FOREACH joind GENERATE $0 AS misspelt_word;

/*
The results are stored on the HDFS in the directories valid_data and invalid_data.
The misspelt words are written to a file in the folder invalid_data.
*/
STORE misspellings INTO'/user/cloudera/pdp/output/data_validation_cleansing/unstructured_data_validation_cleansing/invalid_data';
STORE valid_words INTO'/user/cloudera/pdp/output/data_validation_cleansing/unstructured_data_validation_cleansing/valid_data';
```

## 结果

以下单词被识别为拼写错误，并存储在文件夹`invalid_data`中。我们选择从原始数据集中过滤这些单词。但是，这取决于业务规则；如果业务规则要求拼错的单词必须用正确的拼写替换，则必须采取适当的步骤来纠正拼写。

```sh
sme
lemme
puttin
speling
wntedly
mistaces
servicesa
insertingg
missspellingss
telecommunications
```

## 附加信息

本节的完整代码和数据集位于以下 GitHub 目录中:

*   `Chapter4/code/`
*   `Chapter4/datasets/`

# 总结

在本章中，您学习了各种大数据验证和清理技术，这些技术用于检测和清理数据中不正确或不准确的记录。这些技术保证了不一致的数据在用于分析过程之前，可以通过按照一套规则对数据进行验证来识别，然后根据业务规则对不一致的数据进行替换、修改或删除，使其更加一致。在本章中，我们学习了上一章中关于数据分析的知识。

在下一章中，我们将重点介绍**数据转换模式**，它可以应用于多种数据格式。阅读本章后，读者将能够使用聚合、归纳和连接等技术选择正确的模式来转换数据。