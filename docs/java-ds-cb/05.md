

# 五、从数据中学习——第二部分

在本章中，我们将介绍以下配方:

*   使用 Java 机器学习库在数据上应用机器学习
*   数据集导入和导出
*   聚类和评估
*   分类
*   交叉验证和延期测试
*   特征评分
*   特征选择
*   使用斯坦福分类器对数据点进行分类
*   使用大量在线分析对数据点进行分类

# 简介

在[第 4 章](ch04.html "Chapter 4. Learning from Data - Part 1")、*从数据中学习-第 1 部分*中，我们使用 Weka 机器学习工作台进行不同的分类、聚类、关联规则挖掘、特征选择等。我们在那一章还提到，Weka 并不是唯一用 Java 编写的从数据中学习模式的工具。还有其他工具可以执行类似的任务。这类工具的例子包括但不限于 **Java 机器学习** ( **Java-ML** )库、**海量在线分析** ( **MOA** )和斯坦福机器学习库。

在这一章中，我们将关注这些其他工具的一点一滴，以对数据进行机器学习分析。



# 使用 Java 机器学习(Java-ML)库对数据应用机器学习

**Java 机器学习** ( **Java-ML** )库是标准机器学习算法的集合。与 Weka 不同，该库没有任何 GUI，因为它主要面向软件开发人员。Java-ML 的一个特别有利的特性是，它对每种类型的算法都有一个公共接口，因此，算法的实现相当容易和直接。对该库的支持是它的另一个关键特性，因为源代码有很好的文档记录，因此是可扩展的，并且有大量的代码示例和教程，可以使用该库完成各种机器学习任务。http://java-ml.sourceforge.net/[网站](http://java-ml.sourceforge.net/)有关于图书馆的所有细节。

在这个菜谱中，我们将使用这个库来完成以下任务:

*   数据集导入和导出
*   聚类和评估
*   分类
*   交叉验证和延期测试
*   特征评分
*   特征选择

## 准备就绪

为了执行此配方，我们需要以下内容:

1.  In this recipe, we will be using the **0.1.7** version of the library. Download this version from [https://sourceforge.net/projects/java-ml/files/java-ml/](https://sourceforge.net/projects/java-ml/files/java-ml/):![Getting ready](img/image_05_001.jpg)
2.  The file you downloaded is a compressed zip file. Extract the files to a directory. The directory structure looks like the following:![Getting ready](img/image_05_002.jpg)

    我们需要将`javaml-0.1.7.jar`文件作为一个外部 JAR 文件包含在 Eclipse 项目中，我们将使用它来实现菜谱。

3.  The directory also has a folder named `lib`. By opening the lib folder, we will see that it contains several other JAR files:![Getting ready](img/image_05_003.jpg)

    这些 JAR 文件是 Java-ML 的依赖项，因此也必须作为外部 JAR 文件包含在项目中:

    ![Getting ready](img/image_05_004.jpg)
4.  In our recipe, we will also be using a sample Iris dataset that is compatible with Java-ML's native file format. Iris and other data files, however, do not come with the library's distribution; they need to be downloaded from a different repository. To download the datasets, go to [http://java-l.sourceforge.net/content/databases](http://java-l.sourceforge.net/content/databases). Java-ML has two types of datasets: 111 small UCI datasets and 7 large UCI datasets. For your practice, it is highly recommended to download both types of datasets. For the recipe, click 111 small UCI datasets and you will be prompted for its download:![Getting ready](img/image_05_005.jpg)
5.  下载完成后，解压缩文件夹。您将看到在这个分布中有 111 个文件夹，每个文件夹代表一个数据集。找到 iris 数据集文件夹并将其打开。您将看到有两个数据文件和一个名称文件。在我们的食谱中，我们将使用`iris.data`文件。需要注意到该文件的路径，因为我们将在菜谱中使用该路径:

![Getting ready](img/image_05_006.jpg)

### 注意

如果您使用任何 UCI 数据集，您需要相应地注明并提供对原始作品的引用。详情可在[http://archive.ics.uci.edu/ml/citation_policy.html](http://archive.ics.uci.edu/ml/citation_policy.html)找到。

## 怎么做...

1.  创建一个名为`JavaMachineLearning`的类。我们将使用一个主方法来实现所有的机器学习任务。该方法将抛出一个`IOException` :

    ```java
            public class JavaMachineLearning { 
              public static void main(String[] args) throws IOException{ 

    ```

2.  First, we will be reading the iris dataset using Java-ML's `FileHandler` class's `loadDataset()` method:

    ```java
            Dataset data = FileHandler.loadDataset(new File("path to your 
            iris.data"), 4, ","); 

    ```

    该方法的参数是到`dataset`的路径、类属性的位置和分隔值的分隔符。可以用任何标准文本编辑器阅读`dataset`。属性的起始索引是 0，iris `dataset`的第五个属性是 class 属性。因此，第二个参数设置为 4。同样，在我们的例子中，数据值由逗号分隔。因此，第三个参数被设置为逗号。文件的内容被带到一个`Dataset`对象中。

3.  通过简单地将对象传递给`System.out.println()`方法:

    ```java
            System.out.println(data); 

    ```

    来打印`dataset`内容
4.  代码的部分输出如下:

    ```java
            [{[5.1, 3.5, 1.4, 0.2];Iris-setosa}, {[4.9, 3.0, 1.4, 
              0.2];Iris-setosa}, {[4.7, 3.2, 1.3, 0.2];Iris-setosa}, {[4.6,  
                 3.1, 1.5, 0.2];Iris-setosa}, {[5.0, 3.6, 1.4, 0.2];Iris-
                   setosa}, {[5.4, 3.9, 1.7, 0.4];Iris-setosa}, {[4.6, 3.4, 
                     1.4, 0.3];Iris-setosa}, {[5.0, 3.4, 1.5, 0.2];Iris-
                        setosa}, {[4.4, 2.9, 1.4, 0.2];Iris-setosa}, ...] 

    ```

5.  If at any point, you need to export your `dataset` from the .data format to a .txt format, Java-ML has a very simple way to accomplish it using its `exportDataset()` method of the `FileHandler` class. The method takes the data and the output file as its parameter. The following line of code creates a text file in the `C:/` drive with the contents of the iris `dataset`:

    ```java
            FileHandler.exportDataset(data, new File("c:/javaml-
              output.txt")); 

    ```

    上述代码生成的文本文件的部分输出如下:

    ```java
     Iris-setosa 5.1   3.5   1.4   0.2 
            Iris-setosa 4.9   3.0   1.4   0.2 
            Iris-setosa 4.7   3.2   1.3   0.2 
            Iris-setosa 4.6   3.1   1.5   0.2 
            ...................................

    ```

    ### 注意

    对于 Java-ML 生成的数据文件，有两点需要注意。首先，类值是第一个属性，其次，这些值不再像在。数据文件；相反，它们由制表符分隔。

6.  要读取上一步创建的数据文件，我们可以再次使用`loadDataset()`方法。但是这次参数的值会不同:

    ```java
            data = FileHandler.loadDataset(new File("c:/javaml-
              output.txt"), 0,"\t"); 

    ```

7.  If we print the data:

    ```java
            System.out.println(data); 

    ```

    然后，它将与我们在步骤 3 中看到的输出相同:

    ```java
            [{[5.1, 3.5, 1.4, 0.2];Iris-setosa}, {[4.9, 3.0, 1.4, 
              0.2];Iris-setosa}, {[4.7, 3.2, 1.3, 0.2];Iris-setosa}, {[4.6, 
                3.1, 1.5, 0.2];Iris-setosa}, {[5.0, 3.6, 1.4, 0.2];Iris-
                  setosa}, {[5.4, 3.9, 1.7, 0.4];Iris-setosa}, {[4.6, 3.4, 
                    1.4, 0.3];Iris-setosa}, {[5.0, 3.4, 1.5, 0.2];Iris-
                      setosa}, {[4.4, 2.9, 1.4, 0.2];Iris-setosa}, ...] 

    ```

8.  Java-ML 提供了非常简单的接口来应用聚类、显示聚类和评估聚类。我们将在菜谱中使用`KMeans`聚类。创建一个`KMeans`集群器:

    ```java
            Clusterer km = new KMeans(); 

    ```

9.  使用`cluster()`方法向集群器提供数据。结果将是数据点的多个聚类(或多个数据集)。将结果放入一个数组`Dataset` :

    ```java
            Dataset[] clusters = km.cluster(data); 

    ```

10.  If you want to see the data points in each cluster, use a for loop to iterate over the array of datasets:

    ```java
            for(Dataset cluster:clusters){ 
              System.out.println("Cluster: " + cluster); 
            } 

    ```

    该步骤中代码的部分输出如下:

    ```java
     Cluster: [{[6.3, 3.3, 6.0, 2.5];Iris-virginica}, {[7.1, 3.0, 
              5.9, 2.1];Iris-virginica}, ...] 
            Cluster: [{[5.5, 2.3, 4.0, 1.3];Iris-versicolor}, {[5.7, 2.8, 
               4.5, 1.3];Iris-versicolor}, ...] 
            Cluster: [{[5.1, 3.5, 1.4, 0.2];Iris-setosa}, {[4.9, 3.0, 1.4, 
               0.2];Iris-setosa}, ...] 
            Cluster: [{[7.0, 3.2, 4.7, 1.4];Iris-versicolor}, {[6.4, 3.2, 
               4.5, 1.5];Iris-versicolor}, ...]

    ```

    从输出中，我们可以看到 KMeans 算法从 iris 数据集创建了四个聚类。

11.  误差平方和是衡量聚类器性能的指标之一。我们将使用`ClusterEvaluation`类来测量聚类的误差:

    ```java
            ClusterEvaluation sse = new SumOfSquaredErrors(); 

    ```

12.  接下来，我们简单地将聚类发送给对象的 score 方法，以获得聚类的误差平方和:

    ```java
            double score = sse.score(clusters); 

    ```

13.  Print the error score:

    ```java
            System.out.println(score);  

    ```

    输出中将显示以下内容:

    ```java
     114.9465465309897 

    ```

    这是 iris 数据集的 KMeans 聚类的误差平方和。

14.  Java-ML 中的分类也非常容易，只需要几行代码。下面的代码创建了一个 **K 近邻** ( **KNN** )分类器。分类器将基于来自五个最近邻居的多数投票来预测看不见的数据点的标签。`buildClassifier()`方法用于训练一个分类器，该分类器将数据集(在我们的例子中是 iris)作为参数:

    ```java
            Classifier knn = new KNearestNeighbors(5); 
            knn.buildClassifier(data); 

    ```

15.  After a model is built, the recipe will then continue to evaluate the model. We will see two evaluation methods that can be accomplished using Java-ML:

    *   k 倍交叉验证和
    *   坚持测试

16.  对于 KNN 分类器的 k-fold 交叉验证，我们将使用分类器创建一个`CrossValidation`实例。`CrossValidation`类有一个名为`crossValidation()`的方法，它将数据集作为参数。该方法返回一个 map，它的第一个参数是 object，第二个参数是 evaluation metric:

    ```java
            CrossValidation cv = new CrossValidation(knn); 
            Map<Object, PerformanceMeasure> cvEvaluation = 
              cv.crossValidation(data); 

    ```

17.  Now that we have the cross-validation results, we can simply print them by using the following:

    ```java
            System.out.println(cvEvaluation); 

    ```

    这将显示每个类别的真阳性、假阳性、真阴性和假阴性:

    ```java
            {Iris-versicolor=[TP=47.0, FP=1.0, TN=99.0, FN=3.0], Iris-
              virginica=[TP=49.0, FP=3.0, TN=97.0, FN=1.0], Iris-setosa=
                [TP=50.0, FP=0.0, TN=100.0, FN=0.0]} 

    ```

18.  In order to do a held-out testing, we need to have a test dataset. Unfortunately, we do not have any test dataset for iris. Therefore, we will be using the same iris.data file (that was used to train our KNN classifier) as our test dataset. But note that in real life, you will have a test dataset with the exact number of attributes as in your training dataset, while the labels of the data points will be unknown.

    首先，我们加载测试数据集:

    ```java
            Dataset testData = FileHandler.loadDataset(new File("path to 
              your iris.data "), 4, ","); 

    ```

    然后，我们使用以下代码获得分类器对测试数据的性能:

    ```java
            Map<Object, PerformanceMeasure> testEvaluation = 
              EvaluateDataset.testDataset(knn, testData); 

    ```

19.  Then, we can simply print the results for each class by iterating over the map object:

    ```java
            for(Object classVariable:testEvaluation.keySet()){ 
              System.out.println(classVariable + " class has " + 
                testEvaluation.get(classVariable).getAccuracy()); 
            } 

    ```

    前面的代码将打印每个类的 KNN 分类器的精度:

    ```java
            Iris-versicolor class has 0.9666666666666667 
            Iris-virginica class has 0.9666666666666667 
            Iris-setosa class has 1.0 

    ```

20.  Feature scoring is a key aspect of machine learning to reduce dimensionality. In Java-ML, we will be implementing the following method that generates a score for a given attribute:

    ```java
            public double score(int attIndex); 

    ```

    首先，创建一个特征评分算法实例。在我们的配方中，我们将使用增益比算法:

    ```java
            GainRatio gainRatio = new GainRatio(); 

    ```

21.  接下来，将算法应用于数据:

    ```java
            gainRatio.build(data); 

    ```

22.  Finally, print the scores of each feature using a for loop and by iterating through sending the attribute index to the  `score()` method, one by one:

    ```java
            for (int i = 0; i < gainRatio.noAttributes(); i++){ 
              System.out.println(gainRatio.score(i)); 
            } 

    ```

    虹膜数据集的特征得分如下:

    ```java
     0.2560110727706682 
            0.1497001925156687 
            0.508659832906763 
            0.4861382158327255

    ```

23.  We can also rank features based on some feature-ranking algorithms. To do this, we will be implementing the `rank()` method of Java-ML that works in a similar way like the `score()` method--both take the index of an attribute:

    ```java
            public int rank(int attIndex); 

    ```

    创建一个特征排序算法实例。在我们的例子中，我们将依赖于 SVM 的基于递归消除特征方法的特征排序。构造函数的参数表示将被消除的排名最差的特征的百分比:

    ```java
            RecursiveFeatureEliminationSVM featureRank = new 
              RecursiveFeatureEliminationSVM(0.2);
    ```

    *   接下来，对数据集应用算法:

        ```java
                featureRank.build(data); 

        ```

    *   最后，使用 for 循环并通过将属性索引依次发送给`rank()`方法进行迭代来打印每个特性的排名:

        ```java
                for (int i = 0; i < featureRank.noAttributes(); i++){ 
                   System.out.println(featureRank.rank(i)); 
                } 

        ```

    *   iris 数据集的特征排序如下:

    ```java
     3 
            2 
            0 
            1

    ```

24.  而对于特征的评分和排序，我们得到单个特征的信息，当我们应用 Java-ML 的特征子集选择时，我们只得到从数据集中选择的特征子集。

首先，创建一个特征选择算法。在我们的配方中，我们将使用一种使用`greedy`方法的正向选择方法。在选择特征的过程中，我们需要一个距离度量，在我们的例子中是皮尔逊相关度量。构造函数的第一个参数代表子集中要选择的属性数:

```java
        GreedyForwardSelection featureSelection = new 
        GreedyForwardSelection(5, new PearsonCorrelationCoefficient()); 

```

*   然后，将算法应用于数据集:

    ```java
            featureSelection.build(data); 

    ```

*   最后，您可以轻松打印算法选择的特征:

```java
        System.out.println(featureSelection.selectedAttributes()); 

```

要素的输出子集如下:

```java
[0] 

```

食谱的完整代码如下:

```java
import java.io.File; 
import java.io.IOException; 
import java.util.Map; 
import net.sf.javaml.classification.Classifier; 
import net.sf.javaml.classification.KNearestNeighbors; 
import net.sf.javaml.classification.evaluation.CrossValidation; 
import net.sf.javaml.classification.evaluation.EvaluateDataset; 
import net.sf.javaml.classification.evaluation.PerformanceMeasure; 
import net.sf.javaml.clustering.Clusterer; 
import net.sf.javaml.clustering.KMeans; 
import net.sf.javaml.clustering.evaluation.ClusterEvaluation; 
import net.sf.javaml.clustering.evaluation.SumOfSquaredErrors; 
import net.sf.javaml.core.Dataset; 
import net.sf.javaml.distance.PearsonCorrelationCoefficient; 
import net.sf.javaml.featureselection.ranking.
  RecursiveFeatureEliminationSVM; 
import net.sf.javaml.featureselection.scoring.GainRatio; 
import net.sf.javaml.featureselection.subset.GreedyForwardSelection; 
import net.sf.javaml.tools.data.FileHandler; 

public class JavaMachineLearning { 
   public static void main(String[] args) throws IOException{ 
      Dataset data = FileHandler.loadDataset(new File("path to 
        iris.data"), 4, ","); 
      System.out.println(data); 
      FileHandler.exportDataset(data, new File("c:/javaml-
        output.txt")); 
      data = FileHandler.loadDataset(new File("c:/javaml-output.txt"), 
        0,"\t"); 
      System.out.println(data); 

      //Clustering 
      Clusterer km = new KMeans(); 
      Dataset[] clusters = km.cluster(data); 
      for(Dataset cluster:clusters){ 
         System.out.println("Cluster: " + cluster); 
      } 
      ClusterEvaluation sse= new SumOfSquaredErrors(); 
      double score = sse.score(clusters); 
      System.out.println(score); 

      //Classification 
      Classifier knn = new KNearestNeighbors(5); 
      knn.buildClassifier(data); 
      //Cross validation 
      CrossValidation cv = new CrossValidation(knn); 
      Map<Object, PerformanceMeasure> cvEvaluation = 
        cv.crossValidation(data); 
      System.out.println(cvEvaluation); 
      //Held-out testing 
      Dataset testData = FileHandler.loadDataset(new File("path to 
        iris.data"), 4, ","); 
      Map<Object, PerformanceMeasure> testEvaluation = 
            EvaluateDataset.testDataset(knn, testData); 
      for(Object classVariable:testEvaluation.keySet()){ 
         System.out.println(classVariable + " class has 
           "+testEvaluation.get(classVariable).getAccuracy()); 
      } 

      //Feature scoring 
      GainRatio gainRatio = new GainRatio(); 
      gainRatio.build(data); 
      for (int i = 0; i < gainRatio.noAttributes(); i++){ 
         System.out.println(gainRatio.score(i)); 
      } 

      //Feature ranking 
      RecursiveFeatureEliminationSVM featureRank = new 
        RecursiveFeatureEliminationSVM(0.2); 
      featureRank.build(data); 
      for (int i = 0; i < featureRank.noAttributes(); i++){ 
         System.out.println(featureRank.rank(i)); 
      } 

      //Feature subset selection 
      GreedyForwardSelection featureSelection = new 
        GreedyForwardSelection(5, new 
            PearsonCorrelationCoefficient()); 
      featureSelection.build(data); 
      System.out.println(featureSelection.selectedAttributes()); 
   } 
}

```



# 使用斯坦福分类器分类数据点

斯坦福分类器是斯坦福大学自然语言处理小组开发的机器学习分类器。软件用 Java 实现，作为它的分类器，软件使用最大熵。最大熵相当于多类逻辑回归模型，只是参数设置略有不同。使用斯坦福分类器的优势在于，软件中使用的技术与谷歌或亚马逊使用的基本技术相同。

## 准备就绪

在这个食谱中，我们将使用斯坦福分类器，根据它使用最大熵的学习来分类数据点。我们将使用 3.6.0 版本的软件。详情请参考[http://nlp.stanford.edu/software/classifier.html](http://nlp.stanford.edu/software/classifier.html)。要运行这个食谱的代码，您需要 Java 8。为了执行此配方，我们需要执行以下操作:

1.  Go to [http://nlp.stanford.edu/software/classifier.html](http://nlp.stanford.edu/software/classifier.html), and download version 3.6.0\. This is the latest version at the time of writing of this book. The software distribution is a compressed zip file:![Getting ready](img/image_05_007.jpg)
2.  Once downloaded, decompress the files. You will see a list of files and folders as follows:![Getting ready](img/image_05_008.jpg)

    `stanford-classifier-3.6.0.jar`文件需要包含在您的 Eclipse 项目中:

    ![Getting ready](img/image_05_009.jpg)

    该发行版还有一个名为 examples 的文件夹。我们将在食谱中使用这个文件的内容。示例文件夹包含两个数据集:奶酪疾病数据集和虹膜数据集。每个数据集包含三个相关联的文件:一个训练文件(带有。train 扩展名)、一个测试文件(带有。测试扩展名)，以及一个属性(带有`.prop`扩展名)。在我们的食谱中，我们将使用奶酪疾病数据集:

    ![Getting ready](img/image_05_010.jpg)

    如果打开`cheeseDisease.train`文件，内容如下:

    ```java
    2 Back Pain
    2 Dissociative Disorders
    2 Lipoma
    1 Blue Rathgore
    2 Gallstones
    1 Chevrotin des Aravis
    2 Pulmonary Embolism
    2 Gastroenteritis
    2 Ornithine Carbamoyltransferase Deficiency Disease ............

    ```

    第一列表示 1 或 2，是数据实例的类，而第二列是字符串值，是名称。类别 1 表示其后的名称是奶酪的名称，类别 2 表示其后的名称是疾病的名称。对数据集应用监督分类的目标是构建一个分类器，将奶酪名称与疾病名称分开。

    ### 注意

    数据集中的列由制表符分隔。这里，只有一个类列和一个预测列。这是训练分类器的最低要求。但是，您可以拥有任意数量的预测列并指定它们的角色。

3.  `Cheese2007.prop`文件是数据集的属性文件。您需要理解该文件的内容，因为我们的配方中的代码将联系该文件，以获得关于类特性、分类器要使用的特性类型、控制台上的显示格式、分类器的参数等等的必要信息。因此，我们现在将检查该文件的内容。

属性文件的前几行列出了特性选项。带#的行表示注释。这些行告诉分类器在学习过程中使用类特征(在训练文件中是第 1 列)。它还提供信息，例如分类器将使用 N 元语法特征，其中 N 元语法的最小长度为 1，最大长度为 4。分类器在计算 N-gram 和 10、20 和 30 的装箱长度时也将使用前缀和后缀:

```java
        # # Features 
        # useClassFeature=true
        1.useNGrams=true 
        1.usePrefixSuffixNGrams=true 
        1.maxNGramLeng=4 
        1.minNGramLeng=1 
        1.binnedLengths=10,20,30
```

下一个重要的行块是映射，其中属性文件向分类器传达评估的基础事实将是列 0，并且需要对列 1 进行预测:

```java
        # # Mapping 
        # goldAnswerColumn=0 
        displayedColumn=1
```

接下来，属性文件保存最大熵分类器的优化参数:

```java
       # 
       # Optimization 
       # intern=true 
        sigma=3 
        useQN=true 
        QNsize=15 
        tolerance=1e-4
```

最后，属性文件包含训练和测试文件路径的条目:

```java
       #  Training input
       # trainFile=./examples/cheeseDisease.train
       testFile=./examples/cheeseDisease.test
```

## 怎么做...

1.  对于食谱，我们将在项目中创建一个类。我们将只使用一种`main()`方法来演示分类。该方法将抛出一个异常:

    ```java
            public class StanfordClassifier { public static void 
              main(String[] args) throws Exception {
    ```

2.  斯坦福分类器是在`ColumnDataClassifier`类中实现的。通过为奶酪疾病数据集提供属性文件的路径来创建分类器:

    ```java
            ColumnDataClassifier columnDataClassifier = new 
              ColumnDataClassifier("examples/cheese2007.prop");
    ```

3.  接下来，使用训练数据构建分类器。`Classifier`类的泛型是`<String, String>`，因为第一列是类，第二列是奶酪/疾病的名称。注意，即使 class 列的标称值是 1 和 2，它们也被视为字符串:

    ```java
            Classifier<String,String> classifier =   
              columnDataClassifier.makeClassifier
                (columnDataClassifier.readTrainingExamples
                  ("examples/cheeseDisease.train"));
    ```

4.  Finally, iterate through each line of the test dataset. The test dataset is similar to the training dataset: the first column is the actual class and the second column is the name. A first few lines of the test dataset are as follows:

    2 鹦鹉热

    2 库欣综合征

    2 内斜视

    2 黄疸，新生儿

    2 胸腺瘤...............

## 它是如何工作的...

列数据分类器被应用到测试集的每一行，结果被发送到一个`Datum`对象。分类器预测`Datum`对象的类别，并在控制台上打印预测结果:

```java
for (String line : ObjectBank.getLineIterator("examples/cheeseDisease.test", "utf-8")) { Datum<String,String> d = columnDataClassifier.makeDatumFromLine(line); System.out.println(line + " ==> " + classifier.classOf(d)); }

```

控制台上的输出如下(输出被截断):

```java
2 Psittacosis ==> 2 2 Cushing Syndrome ==> 2 2 Esotropia ==> 2 2 Jaundice, Neonatal ==> 2 2 Thymoma ==> 2 1 Caerphilly ==> 1 2 Teratoma ==> 2 2 Phantom Limb ==> 1 2 Iron Overload ==> 1 ...............

```

第一列是实际类别，第二列是名称，`==>`符号右侧的值是分类器预测的类别。食谱的完整代码如下:

```java
import edu.stanford.nlp.classify.Classifier;
import edu.stanford.nlp.classify.ColumnDataClassifier; 
import edu.stanford.nlp.ling.Datum; 
import edu.stanford.nlp.objectbank.ObjectBank; 
public class StanfordClassifier { 
public static void main(String[] args) throws Exception {   ColumnDataClassifier columnDataClassifier = new ColumnDataClassifier("examples/cheese2007.prop"); Classifier<String,String> classifier = columnDataClassifier.makeClassifier(columnDataClassifier.readTrainingExamples("examples/cheeseDisease.train"));
 for (String line : ObjectBank.getLineIterator("examples/cheeseDisease.test", "utf-8")) { Datum<String,String> d = columnDataClassifier.makeDatumFromLine(line); System.out.println(line + " ==> " + classifier.classOf(d)); 
}
} 
}

```

该方法不演示斯坦福分类器模型的加载和保存。如果有兴趣，可以看看发行版中的`ClassifierDemo.java`文件。



# 使用海量在线分析(MOA)对数据点进行分类

海量在线分析或 MOA 与 Weka 相关，但它具有更大的可扩展性。它是一个著名的用于数据流挖掘的 Java 工作台。有了强大的社区，MOA 实现了分类、聚类、回归、概念漂移识别和推荐系统。MOA 的其他主要优势是其可被开发者扩展的能力以及与 Weka 进行双向交互的能力。

## 准备就绪

为了执行此配方，我们需要以下内容:

1.  MOA can be downloaded from [https://sourceforge.net/projects/moa-datastream/](https://sourceforge.net/projects/moa-datastream/), which eventually is accessible from the MOA *getting started* webpage at [http://moa.cms.waikato.ac.nz/getting-started/](http://moa.cms.waikato.ac.nz/getting-started/):![Getting ready](img/image_05_011.jpg)

    这会将一个名为`moa-release-2016.04.zip`的 zip 文件下载到您的系统中。把它保存在你喜欢的任何地方。

2.  Once downloaded, extract the files. You will see files and folders as follows:![Getting ready](img/image_05_012.jpg)
3.  您需要将`moa.jar`文件作为项目的外部库:

![Getting ready](img/image_05_013.jpg)

## 怎么做...

1.  首先，创建一个类和一个带两个参数的方法:第一个参数表示我们将要处理的实例数量，第二个参数表示我们是否要测试分类器:

    ```java
            public class MOA { public void run(int numInstances, boolean 
              isTesting){

    ```

2.  创建一个`HoeffdingTree`分类器:

    ```java
    Classifier learner = new HoeffdingTree();
    ```

    ### 注意

    恐鸟实现了以下分类器：朴素贝叶斯、赫夫丁树、赫夫丁选项树、赫夫丁自适应树、装袋、增压、使用阿德温的装袋、杠杆装袋、SGD、感知器、斯佩加索斯.

3.  接下来，创建一个随机径向基函数流。
4.  准备要使用的流:

    ```java
            stream.prepareForUse();
    ```

5.  设置对数据流头的引用。使用`getHeader()`方法可以找到数据流的头:

    ```java
            learner.setModelContext(stream.getHeader());
    ```

6.  然后，准备要使用的分类器:

    ```java
            learner.prepareForUse();
    ```

7.  声明两个变量，用于跟踪样本数和正确分类的样本数:

    ```java
            int numberSamplesCorrect = 0; int numberSamples = 0;
    ```

8.  声明另一个变量来跟踪分类所花费的时间:

    ```java
            long evaluateStartTime =  
              TimingUtils.getNanoCPUTimeOfCurrentThread();
    ```

9.  接下来，循环执行，直到流中有更多的实例，并且被分类的样本数没有达到实例总数。在循环中，获取流的每个实例的数据。然后，检查分类器是否正确地对实例进行了分类。如果是，将变量`numberSamplesCorrect`增加 1。仅当测试打开时才选中此项(通过此方法的第二个参数)。
10.  然后，通过增加样本数来转到下一个样本，并使用下一个训练实例来训练您的学员:

    ```java
          while (stream.hasMoreInstances() && numberSamples < numInstances)
           {
            Instance trainInst = stream.nextInstance().getData(); 
            if (isTesting)
             { 
              if (learner.correctlyClassifies(trainInst)){ 
                numberSamplesCorrect++; 
               }
             }
           numberSamples++; learner.trainOnInstance(trainInst); 
          }

    ```

11.  计算精度:

    ```java
           double accuracy = 100.0 * (double) numberSamplesCorrect/ 
             (double) numberSamples;
    ```

12.  此外，计算分类所需的时间:

    ```java
            double time = TimingUtils.nanoTimeToSeconds(TimingUtils.
              getNanoCPUTimeOfCurrentThread()- evaluateStartTime);

    ```

13.  最后，显示这些评估指标并关闭方法:

    ```java
            System.out.println(numberSamples + " instances processed with " 
              + accuracy + "% accuracy in "+time+" seconds."); }
    ```

14.  要执行该方法，可以有如下的`main()`方法。关闭您的班级:

```java
        public static void main(String[] args) throws IOException { MOA 
          exp = new MOA(); exp.run(1000000, true); } }
```

该配方的完整代码如下:

```java
import moa.classifiers.trees.HoeffdingTree;
import moa.classifiers.Classifier;
import moa.core.TimingUtils;
import moa.streams.generators.RandomRBFGenerator;
import com.yahoo.labs.samoa.instances.Instance;
import java.io.IOException;

public class MOA {
	public void run(int numInstances, boolean isTesting){
		Classifier learner = new HoeffdingTree();
		RandomRBFGenerator stream = new RandomRBFGenerator();
		stream.prepareForUse();

		learner.setModelContext(stream.getHeader());
		learner.prepareForUse();

		int numberSamplesCorrect = 0;
		int numberSamples = 0;
		long evaluateStartTime = Tim-
                  ingUtils.getNanoCPUTimeOfCurrentThread();
		while (stream.hasMoreInstances() && numberSamples < 
                  numIn-stances) {
		      Instance trainInst = 
                          stream.nextInstance().getData();
			if (isTesting) {
				if 
                        (learner.correctlyClassifies(trainInst)){
				numberSamplesCorrect++;
				}
			}
			numberSamples++;
			learner.trainOnInstance(trainInst);
		}
		double accuracy = 100.0 * (double) 
                  numberSamplesCorrect/ (double) numberSamples;
		double time = Tim-in-
                  gUtils.nanoTimeToSeconds(TimingUtils.
                   getNanoCPUTimeOfCurrentThread()- evaluateStartTime);
		System.out.println(numberSamples + " instances 
                 processed with " + accuracy + "% accuracy in "+time+" 
                  seconds.");
	}

	public static void main(String[] args) throws IOException {
		MOA exp = new MOA();
		exp.run(1000000, true);
	}
}

```

菜谱中代码的输出如下所示(输出可能因机器而异):

```java
 1000000 instances processed with 91.0458% accuracy in 6.769871032 seconds. 

```



# 使用木兰对多标记数据点进行分类

到目前为止，我们已经看到了多类分类，其目的是将一个数据实例分类到几个类中的一个。多标签数据实例是可以有多个类或标签的数据实例。到目前为止，我们使用的机器学习工具无法处理具有多个目标类这一特征的数据点。

为了对多标记数据点进行分类，我们将使用一个名为 Mulan 的开源 Java 库。Mulan 实现了各种分类、排序、特征选择和模型评估。由于木兰没有 GUI，使用它的唯一方法是通过命令行或使用它的 API。在这份食谱中，我们将把重点限制在使用两个不同分类器对多标记数据集进行分类和分类评估上。

## 准备就绪

为了执行此配方，我们需要以下内容:

1.  首先，下载花木兰。在我们的食谱中，我们将使用它的 1.5 版本。库的压缩文件可以在[https://SourceForge . net/projects/Mulan/files/Mulan-1-5/Mulan-1 . 5 . 0 . zip/download](https://sourceforge.net/projects/mulan/files/mulan-1-5/mulan-1.5.0.zip/download)找到，可以通过[http://mulan.sourceforge.net/download.html](http://mulan.sourceforge.net/download.html)访问。
2.  Unzip the compressed files. You will see a data folder as follows. Take a look inside the `dist` folder:![Getting ready](img/image_05_014.jpg)
3.  You will see three files there. Among the three files, there is another compressed file named `Mulan-1.5.0.zip`. Unzip the files:![Getting ready](img/image_05_015.jpg)
4.  This time, you will see three or four JAR files. From the four JAR files, we will be using the JAR files highlighted in the following image:![Getting ready](img/image_05_016.jpg)
5.  Add these three JAR files into your project as external libraries:![Getting ready](img/image_05_017.jpg)
6.  木兰分布的数据文件夹包含一个多标签数据集示例。在我们的食谱中，我们将使用`emotions.arff`文件和`emotions.xml`文件:

![Getting ready](img/image_05_018.jpg)

在开始我们的食谱之前，让我们先来看看木兰的数据集格式。Mulan 需要两个文件来指定多标签数据集。第一个是 ARFF 文件(看看[第四章](ch04.html "Chapter 4. Learning from Data - Part 1")，*从数据中学习——第一部分*)。标签应该被指定为具有两个值“0”和“1”的标称属性，其中前者表示标签不存在，后者表示标签存在。以下示例演示了数据集有三个数字要素，每个实例可以有五个类或标签。在`@data`部分，前三个值代表实际的特征值，然后我们有五个 0 或 1 表示类的存在或不存在:

```java
@relation MultiLabelExample

@attribute feature1 numeric
@attribute feature2 numeric
@attribute feature3 numeric
@attribute label1 {0, 1}
@attribute label2 {0, 1}
@attribute label3 {0, 1}
@attribute label4 {0, 1}
@attribute label5 {0, 1}

@data
2.3,5.6,1.4,0,1,1,0,0

```

另一方面，XML 文件指定标签以及它们之间的任何层次关系。以下示例是与上一示例相对应的 XML 文件:

```java
<labels >
<label name="label1"></label>
  <label name="label2"></label>
  <label name="label3"></label>
  <label name="label4"></label>
  <label name="label5"></label>
</labels>

```

更多详情，请参见[http://mulan.sourceforge.net/format.html](http://mulan.sourceforge.net/format.html)。

## 怎么做...

1.  创建一个类和一个方法。我们将在`main()`方法中编写所有的代码:

    ```java
            public class Mulan { public static void main(String[] args){
    ```

2.  创建一个数据集并将`emotions.arff`和`emotions.xml`文件读取到数据集:

    ```java
            MultiLabelInstances dataset = null; 
              try { 
                dataset = new  MultiLabelInstances("path to 
                   emotions.arff", 
                  "path to emotions.xml"); 
              } catch (InvalidDataFormatException e) { 
              }
    ```

3.  接下来，我们将创建一个`RAkEL`分类器和一个`MLkNN`分类器。请注意，`RAkEL`是一个元分类器，这意味着它可以有一个多标签学习器，它通常与`LabelPowerset`算法一起使用。`LabelPowerset`是一种基于变换的算法，可以将单标签分类器(在我们的例子中，J48)作为参数。`MLkNN`是一个自适应分类器，基于 k 近邻:

    ```java
            RAkEL learner1 = new RAkEL(new LabelPowerset(new J48())); 
                MLkNN 
              learner2 = new MLkNN();
    ```

4.  创建一个评估器来评估分类性能:

    ```java
            Evaluator eval = new Evaluator();
    ```

5.  因为我们将对两个分类器进行评估，所以我们需要声明一个可以有多个评估结果的变量:

    ```java
            MultipleEvaluation results;
    ```

6.  我们将进行 10 倍交叉验证评估。因此，声明一个你要创建的折叠数的变量:

    ```java
             int numFolds = 10;
    ```

7.  接下来，评估你的第一个学员并显示结果:

    ```java
             results = eval.crossValidate(learner1, dataset, numFolds); 
             System.out.println(results);
    ```

8.  最后，评估你的第二个学员并显示结果:

    ```java
             results = eval.crossValidate(learner2, dataset, numFolds);
             System.out.println(results);
    ```

9.  关闭方法和类:

```java
         } }
```

食谱的完整代码如下:

```java
import mulan.classifier.lazy.MLkNN;
import mulan.classifier.meta.RAkEL;
import mulan.classifier.transformation.LabelPowerset;
import mulan.data.InvalidDataFormatException;
import mulan.data.MultiLabelInstances;
import mulan.evaluation.Evaluator;
import mulan.evaluation.MultipleEvaluation;
import weka.classifiers.trees.J48;

public class Mulan {
	public static void main(String[] args){
		MultiLabelInstances dataset = null;
		try {
		   dataset = new MultiLabelInstances("path to emo-
                     tions.arff", "path to emotions.xml");
		} catch (InvalidDataFormatException e) {
		}
		RAkEL learner1 = new RAkEL(new LabelPowerset(new 
                  J48()));
		MLkNN learner2 = new MLkNN();
		Evaluator eval = new Evaluator();
		MultipleEvaluation results;
		int numFolds = 10;
		results = eval.crossValidate(learner1, dataset, num-
                  Folds);
		System.out.println(results);
		results = eval.crossValidate(learner2, dataset, num-
                  Folds);
		System.out.println(results);
	}
}

```

代码的输出将是您选择的两个学习者的表现:

```java
Fold 1/10
Fold 2/10
Fold 3/10
Fold 4/10
Fold 5/10
Fold 6/10
Fold 7/10
Fold 8/10
Fold 9/10
Fold 10/10
Hamming Loss: 0.2153±0.0251
Subset Accuracy: 0.2562±0.0481
Example-Based Precision: 0.6325±0.0547
Example-Based Recall: 0.6307±0.0560
Example-Based F Measure: 0.5990±0.0510
Example-Based Accuracy: 0.5153±0.0484
Example-Based Specificity: 0.8607±0.0213
........................................
Fold 1/10
Fold 2/10
Fold 3/10
Fold 4/10
Fold 5/10
Fold 6/10
Fold 7/10
Fold 8/10
Fold 9/10
Fold 10/10
Hamming Loss: 0.1951±0.0243
Subset Accuracy: 0.2831±0.0538
Example-Based Precision: 0.6883±0.0655
Example-Based Recall: 0.6050±0.0578
Example-Based F Measure: 0.6138±0.0527
Example-Based Accuracy: 0.5326±0.0515
Example-Based Specificity: 0.8994±0.0271
........................................ 

```