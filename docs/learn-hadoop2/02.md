# 二、存储

在上一章中概述完 Hadoop 之后，我们现在将开始更详细地研究它的各个组成部分。 在本章中，我们将从堆栈的概念底层开始：在 Hadoop 中存储数据的方法和机制。 我们将特别讨论以下主题：

*   描述**Hadoop 分布式文件系统**(**HDFS**)的体系结构
*   显示 Hadoop 2 中对 HDFS 进行了哪些增强
*   了解如何使用命令行工具和 Java API 访问 HDFS
*   简要描述 ZooKeeper-Hadoop 中的另一个(某种)文件系统
*   在 Hadoop 中存储数据以及可用的文件格式的调查注意事项

在[第 3 章](03.html "Chapter 3. Processing – MapReduce and Beyond")，*Processing-MapReduce 和 Beyond*中，我们将描述 Hadoop 如何提供允许处理数据的框架。

# HDFS 的内部工作原理

在[第 1 章](01.html "Chapter 1. Introduction")，*简介*中，我们对 HDFS 进行了非常高层次的概述；现在我们将更详细地探讨它。 正如那一章中提到的，HDFS 可以被视为一个文件系统，尽管它具有非常特定的性能特征和语义。 它由两个主服务器进程实现：**NameNode**和**DataNodes**，在主/从设置中配置。 如果将 NameNode 视为保存所有文件系统元数据，将 DataNode 视为保存实际文件系统数据(块)，则这是一个很好的起点。 放到 HDFS 上的每个文件都将被拆分成多个块，这些块可能驻留在许多 DataNode 上，而 NameNode 了解如何组合这些块来构造文件。

## 集群启动

假设我们有一个先前关闭的 HDFS 集群，然后检查启动行为，让我们探索这些节点的各种职责以及它们之间的通信。

### NameNode 启动

我们将首先考虑 NameNode 的启动(尽管对此没有实际的排序要求，我们这样做只是出于叙述原因)。 NameNode 实际上存储关于文件系统的两种类型的数据：

*   文件系统的结构，即目录名、文件名、位置和属性
*   构成文件系统上每个文件的数据块

此数据存储在 NameNode 启动时读取的文件中。 注意，NameNode 并不持久地存储存储在特定 DataNode 上的块的映射；我们将很快看到该信息是如何通信的。

因为 NameNode 依赖于文件系统的这种内存表示形式，所以与 DataNode 相比，它往往具有完全不同的硬件要求。 我们将在[第 10 章](10.html "Chapter 10. Running a Hadoop Cluster")，*运行 Hadoop 集群*中更详细地探讨硬件选择；目前，只需记住 NameNode 往往非常需要内存。 这在具有许多(数百万或更多)文件的非常大的集群上尤其如此，特别是当这些文件具有非常长的名称时。 NameNode 上的这种伸缩限制还带来了一个额外的 Hadoop2 特性，我们不会详细介绍它：NameNode 联合，多个 NameNode(或 NameNode HA 对)协同工作，为整个文件系统提供总体元数据。

NameNode 写入的主文件称为`fsimage`；这是整个集群中最重要的一段数据，因为如果没有它，将丢失如何将所有数据块重构为可用的文件系统的知识。 该文件被读取到内存中，并且将来对文件系统的所有修改都将应用于该文件系统的该内存中表示。 NameNode 不会在运行后应用新更改时写出`fsimage`的新版本；相反，它会写入另一个名为`edits`的文件，该文件是自写入上一个版本的`fsimage`以来所做更改的列表。

NameNode 启动过程首先读取`fsimage`文件，然后读取`edits`文件，并将存储在`edits`文件中的所有更改应用于`fsimage`的内存副本。 然后，它将最新版本的`fsimage`文件写入磁盘，并准备好接收客户端请求。

### 数据节点启动

当数据节点启动时，它们首先对它们保存副本的块进行编目。 通常，这些块将简单地写为本地 DataNode 文件系统上的个文件。 DataNode 将执行一些数据块一致性检查，然后向 NameNode 报告其具有有效拷贝的数据块的列表。 这就是 NameNode 构造其所需的最终映射的方式-通过了解哪些块存储在哪些 DataNode 上。 一旦 DataNode 将自身注册到 NameNode，就会在节点之间发送一系列持续的心跳请求，以允许 NameNode 检测已关闭、变得不可访问或新进入集群的 DataNode。

## 数据块复制

HDFS 将每个数据块复制到多个 DataNode 上；默认复制系数为 3，但可以在每个文件级别进行配置。 HDFS 还可以配置为能够确定给定的 DataNode 是否位于同一物理硬件机架中。 在给定智能块放置和集群拓扑知识的情况下，HDFS 将尝试将第二个副本放在不同的主机上，但与第一个和第三个副本放在与第一个和第三个副本相同的设备机架中，放在机架外的主机上。 通过这种方式，系统可以在多达整个机架的设备出现故障时幸存下来，并且每个数据块仍至少有一个活动副本。 正如我们将在[第 3 章](03.html "Chapter 3. Processing – MapReduce and Beyond")，*Processing-MapReduce 以及*之外看到的，有关块放置的知识还允许 Hadoop 将处理调度到尽可能接近每个块的副本，这可以极大地提高性能。

请记住，复制是一种恢复能力的策略，但不是一种备份机制；如果您在 HDFS 中控制了至关重要的数据，则需要考虑备份或其他可提供错误保护的方法，例如意外删除的文件，而复制无法防御这些错误。

当 NameNode 启动并从 DataNode 接收数据块报告时，它将保持安全模式，直到将可配置的数据块阈值(默认值为 99.9%)报告为实时。 在安全模式下，客户端不能对文件系统进行任何修改。

# 对 HDFS 文件系统的命令行访问

在 Hadoop 发行版中，有一个名为`hdfs`的命令行实用程序，它是从命令行与文件系统交互的主要方式。 在不带任何参数的情况下运行此命令，以查看各种可用的子命令。 不过，有很多种；有几种用于启动或停止各种 HDFS 组件。 `hdfs`命令的一般形式为：

```scala
hdfs <sub-command> <command> [arguments]

```

我们将在本书中使用的两个主要子命令是：

*   `dfs`：这是，用于一般文件系统访问和操作，包括读/写和访问文件和目录
*   `dfsadmin`：此用于文件系统的管理和维护。 不过，我们不会详细介绍此命令。 看一下`-report`命令，它列出了文件系统和所有 DataNode 的状态：

    ```scala
    $ hdfs dfsadmin -report
    ```

### 备注

请注意，`dfs`和`dfsadmin`命令也可以与主要的 Hadoop 命令行实用程序一起使用，例如`hadoop fs -ls /`。 这是 Hadoop 早期版本中的方法，但现在已弃用，取而代之的是`hdfs`命令。

## 探索 HDFS 文件系统

运行以下以获取`dfs`子命令提供的可用命令列表：

```scala
$ hdfs dfs

```

从前面命令的输出中可以看出，其中许多命令看起来与标准的 Unix 文件系统命令相似，并且毫不奇怪，它们的工作方式与预期一致。 在我们的测试 VM 中，我们有一个名为`cloudera`的用户帐户。 使用此用户，我们可以按如下方式列出文件系统的根目录：

```scala
$ hdfs dfs -ls /
Found 7 items
drwxr-xr-x   - hbase hbase               0 2014-04-04 15:18 /hbase
drwxr-xr-x   - hdfs  supergroup          0 2014-10-21 13:16 /jar
drwxr-xr-x   - hdfs  supergroup          0 2014-10-15 15:26 /schema
drwxr-xr-x   - solr  solr                0 2014-04-04 15:16 /solr
drwxrwxrwt   - hdfs  supergroup          0 2014-11-12 11:29 /tmp
drwxr-xr-x   - hdfs  supergroup          0 2014-07-13 09:05 /user
drwxr-xr-x   - hdfs  supergroup          0 2014-04-04 15:15 /var

```

输出非常类似于 Unix`ls`命令。 文件属性的工作原理与 Unix 文件系统上的`user`/`group`/`world`属性相同(如图所示，包括`t`粘性位)以及目录所有者、组和修改时间的详细信息。 组名和修改日期之间的列是大小；对于目录，此列为 0，但对于文件，将有一个值，我们将在下面的信息框后面的代码中看到：

### 备注

如果使用相对路径，则从用户的主目录获取这些路径。 如果没有主目录，我们可以使用以下命令创建它：

```scala
$ sudo -u hdfs hdfs dfs –mkdir /user/cloudera
$ sudo -u hdfs hdfs dfs –chown cloudera:cloudera /user/cloudera

```

`mkdir`和`chown`步骤需要超级用户权限(`sudo -u hdfs`)。

```scala
$ hdfs dfs -mkdir testdir
$ hdfs dfs -ls
Found 1 items
drwxr-xr-x   - cloudera cloudera     0 2014-11-13 11:21 testdir

```

然后，我们可以创建一个文件，将其复制到 HDFS，并直接从其在 HDFS 上的位置读取其内容，如下所示：

```scala
$ echo "Hello world" > testfile.txt
$ hdfs dfs -put testfile.txt testdir

```

请注意，有一个名为`-copyFromLocal`的较旧命令，其工作方式与`-put`相同；您可能会在较旧的在线文档中看到它。 现在，运行以下命令并检查输出：

```scala
$ hdfs dfs -ls testdir
Found 1 items
-rw-r--r--   3 cloudera cloudera         12 2014-11-13 11:21 testdir/testfile.txt

```

请注意文件属性和所有者之间的新列；这是文件的复制系数。 现在，最后，运行以下命令：

```scala
$ hdfs dfs -tail testdir/testfile.txt
Hello world

```

其余的`dfs`子命令非常直观；可以随意使用。 我们将在本章后面探讨快照和对 HDFS 的编程访问。

# 保护文件系统元数据

由于`fsimage`文件对文件系统非常关键，因此它的丢失是灾难性的故障。 在 Hadoop1 中，NameNode 是单点故障，最佳实践是将 NameNode 配置为同步写入`fsimage`并将文件编辑到本地存储以及远程文件系统(通常是 NFS)上的至少一个其他位置。 在 NameNode 出现故障的情况下，可以使用文件系统元数据的此最新副本启动替换 NameNode。 然而，该过程需要大量的人工干预，并将导致集群完全不可用的一段时间。

## 辅助 NameNode 无法拯救

在 Hadoop1 的所有组件中，命名最不幸的组件是二级 NameNode，这并不是没有道理的，许多人期望它是某种备份或备用 NameNode。 不是这样的；相反，二级 NameNode 只负责定期读取`fsimage`的最新版本，并编辑文件并创建应用了未完成编辑的新的最新`fsimage`。 在繁忙的集群上，此检查点可以通过减少 NameNode 在能够为客户端提供服务之前必须应用的编辑次数来显著加快 NameNode 的重启速度。

在 Hadoop 2 中，命名更加清晰；有检查点节点(执行以前由辅助 NameNode 执行的角色)和 Backup NameNodes(保留文件系统元数据的本地最新副本)，尽管将备份节点提升为主 NameNode 的过程仍然是一个多阶段的手动过程。

## Hadoop 2 NameNode HA

然而，在大多数生产 Hadoop 2 集群中，使用完全高可用性(HA)解决方案比使用依赖检查点和备份节点更有意义。 尝试将 NameNode HA 与检查点和备份节点机制结合使用实际上是错误的。

其核心思想是在主动/被动集群中配置一对 NameNode(目前不支持超过两个)。 一个 NameNode 充当为所有客户端请求提供服务的实时主节点，第二个 NameNode 仍然准备好在主节点出现故障时接管。 特别是，Hadoop 2 HDFS 通过两种机制启用此 HA：

*   为两个 NameNode 提供一致的文件系统视图
*   为客户端始终连接到主 NameNode 提供了一种方法

### 保持 HA NameNodes 同步

实际上有两种机制使活动 NameNode 和备用 NameNode 保持文件系统视图的一致性：使用**NFS**共享或**仲裁日志管理器**(**QJM**)。

在 NFS 情况下，对外部远程 NFS 文件共享有一个明显的要求-请注意，在 Hadoop1 中，对于文件系统元数据的第二个副本，使用 NFS 是最佳实践，因此许多集群已经有了一个。 如果高可用性是一个问题，但是应该记住，使 NFS 高度可用通常需要高端且昂贵的硬件。 在 Hadoop2 中，HA 使用 NFS；但是，NFS 位置成为文件系统元数据的主要位置。 当活动 NameNode 将所有文件系统更改写入 NFS 共享时，备用节点会检测到这些更改并相应地更新其文件系统元数据副本。

QJM 机制使用外部服务(日志管理器)而不是文件系统。 日志管理器集群是在该数量的主机上运行的奇数个服务(3、5 和 7 是最常见的)。 对文件系统的所有更改都提交给 QJM 服务，只有当大多数 QJM 节点提交更改时，更改才被视为已提交。 备用 NameNode 从 QJM 服务接收更改更新，并使用此信息使其文件系统元数据副本保持最新。

QJM 机制不需要额外的硬件，因为检查点节点是轻量级的，并且可以与其他服务共存。 该模型中也没有单点故障。 因此，QJM HA 通常是首选选项。

在任何一种情况下，无论是在基于 NFS 的 HA 中还是在基于 QJM 的 HA 中，DataNode 都会向这两个 NameNode 发送块状态报告，以确保这两个 NameNode 都具有块到 DataNode 映射的最新信息。 请记住，此块分配信息不保存在`fsimage`/编辑数据中。

## 客户端配置

HDFS 集群的客户端大多不知道 NameNode HA 正在被使用这一事实。 配置文件需要包括两个 NameNode 的详细信息，但用于确定哪个是活动 NameNode 以及何时切换到备用 NameNode 的机制完全封装在客户端库中。 但基本概念是，与 Hadoop 1 中的显式 NameNode 主机不同，Hadoop 2 中的 HDFS 标识了 NameNode 的名称服务 ID，其中为 HA 定义了多个单独的 NameNode(每个 NameNode 都有自己的 NameNode ID)。 请注意，名称服务 ID 的概念也由 NameNode 联邦使用，我们在前面简要提到了这一点。

## 故障转移的工作原理

故障转移可以是手动的，也可以是自动的。 手动故障转移需要管理员触发将备用 NameNode 升级到当前活动 NameNode 的交换机。 尽管自动故障转移对维护系统可用性的影响最大，但在某些情况下，这可能并不总是可取的。 触发手动故障切换只需要运行几个命令，因此，即使在此模式下，故障切换也比 Hadoop 1 或 Hadoop 2 备份节点的情况容易得多，后者转换到新的 NameNode 需要大量手动工作。

无论故障转移是手动触发还是自动触发，它都有两个主要阶段：确认以前的主服务器不再为请求提供服务，以及将备用服务器提升为主服务器。

故障转移中最大的风险是存在两个 NameNode 都在为请求提供服务的时间段。 在这种情况下，可能会对两个 NameNode 上的文件系统进行冲突更改，或者它们可能不同步。 即使在使用 QJM(它只接受来自单个客户端的连接)的情况下这应该是不可能的，但过时的信息可能会被提供给客户端，然后客户端可能会尝试根据这些陈旧的元数据做出不正确的决定。 当然，如果之前的主 NameNode 在某种程度上行为不正确，这尤其有可能，这就是为什么首先需要确定故障转移的原因。

为了确保任何时候只有一个 NameNode 处于活动状态，需要使用隔离机制来验证现有的 NameNode 主服务器是否已关闭。 最简单的包含机制将尝试 ssh 进入 NameNode 主机并主动终止进程，尽管也可以执行自定义脚本，因此该机制非常灵活。 在隔离成功且系统已确认以前的主 NameNode 现已失效并已释放所有所需资源之前，故障转移将不会继续。

一旦隔离成功，备用 NameNode 将成为主 NameNode，如果 NFS 用于 HA，则备用 NameNode 将开始写入 NFS 挂载的`fsimage`并编辑日志；如果这是 HA 机制，则备用 NameNode 将成为 QJM 的单个客户端。

在讨论自动故障转移之前，我们需要稍微介绍一下用于启用此功能的另一个 Apache 项目。

# Apache ZooKeeper-一种不同类型的文件系统

在 Hadoop 中，我们在讨论文件系统和数据存储时将主要讨论 HDFS。 但是，在几乎所有的 Hadoop2 安装中，还有另一个服务看起来有点像文件系统，但它提供了对分布式系统的正常运行至关重要的重要功能。 该服务是 Apache zooKeeper([HDFS](http://zookeeper.apache.org))，因为它是 http://zookeeper.apache.org HA 实现的关键部分，我们将在本章中介绍它。 然而，它也被多个其他 Hadoop 组件和相关项目使用，所以我们将在本书中多次涉及到它。

ZooKeeper 最初是 HBase 的一个子组件，用于启用该服务的几个操作功能。 当构建任何复杂的分布式系统时，几乎总是需要一系列活动，而且这些活动总是很难正确进行。 这些活动包括处理共享锁、检测组件故障以及支持一组协作服务中的领导者选举等。 ZooKeeper 是作为协调服务创建的，它将提供一系列基本操作，HBase 可以根据这些操作实现这些类型的操作关键特性。 请注意，ZooKeeper 还从[http://research.google.com/archive/chubby-osdi06.pdf](http://research.google.com/archive/chubby-osdi06.pdf)中描述的 Google Chubby 系统获得灵感。

ZooKeeper 以实例集群的形式运行，称为整体。 该集合提供了一种数据结构，它在某种程度上类似于文件系统。 结构中的每个位置都称为 Z 节点，可以像目录一样拥有子节点，也可以像文件一样拥有内容。 请注意，ZooKeeper 不适合存储非常大量的数据，默认情况下，Znode 中的最大数据量为 1MB。 在任何时间点，集合中的一台服务器都是主服务器，并做出有关客户端请求的所有决策。 围绕主控的责任有非常明确的规则，包括它必须确保只有在大多数合唱团成员提交更改时才提交请求，并且一旦提交，任何冲突的更改都会被拒绝。

您应该在 Cloudera 虚拟机中安装 ZooKeeper。 如果没有，请使用 Cloudera Manager 将其作为单个节点安装在主机上。 在生产系统中，ZooKeeper 具有关于绝对多数投票的非常特定的语义，因此有些逻辑只有在较大的集合中才有意义(3、5 或 7 个节点是最常见的大小)。

Cloudera VM 中有一个名为`zookeeper-client`的 ZooKeeper 命令行客户端；请注意，在普通的 ZooKeeper 发行版中，它被称为`zkCli.sh`。 如果不带参数运行它，它将连接到本地计算机上运行的 ZooKeeper 服务器。 在这里，您可以键入`help`来获取命令列表。

最感兴趣的命令将是`create`、`ls`和`get`。 顾名思义，它们创建一个 Znode，列出文件系统中特定位置的 ZNode，并获取存储在特定 Znode 的数据。 以下是一些用法示例。

*   创建无数据的 Z 节点：

    ```scala
    $ create /zk-test '' 

    ```

*   创建第一个 Znode 的子节点并在其中存储一些文本：

    ```scala
    $ create /zk-test/child1 'sampledata'

    ```

*   检索与特定 Znode 关联的数据：

    ```scala
    $ get /zk-test/child1 

    ```

客户端还可以在给定的 Znode 上注册观察器-如果有问题的 Znode 发生更改，无论是其数据还是子节点被修改，都会发出警报。

这听起来可能不是很有用，但是 ZNode 还可以创建为顺序节点和临时节点，这就是神奇之处所在。

## 使用顺序 ZNode 实现分布式锁

如果在 CLI 中使用`-s`选项创建了 Znode，则它将被创建为顺序节点。 ZooKeeper 将为提供的名称添加一个 10 位整数后缀，该整数保证是唯一的，并且大于同一 Znode 的任何其他连续的子节点。 我们可以使用此机制来创建分布式锁。 ZooKeeper 本身并不持有实际的锁；客户端需要了解 ZooKeeper 中的特定状态对于它们到相关应用锁的映射意味着什么。

如果我们在`/zk-lock`创建一个(非顺序的)Znode，那么任何希望持有锁的客户端都将创建一个顺序的子节点。 例如，在第一种情况下，`create -s /zk-lock/locknode`命令可能会创建节点`/zk-lock/locknode-0000000001`，并为后续调用增加整数后缀。 当客户端在锁下创建 Z 节点时，它将检查其顺序节点是否具有最低整数后缀。 如果有，则将其视为拥有锁。 如果不是，那么它将需要等待，直到持有锁的节点被删除。 客户端通常会监视具有下一个最低后缀的节点，然后在该节点被删除时收到警报，表明它现在持有锁。

## 使用短暂的 ZNode 实现群组成员资格和领导人选举

在整个会话过程中，任何 ZooKeeper 客户端都会向服务器发送心跳信号，表明它处于活动状态。 对于我们到目前为止已经讨论过的 ZNode，我们可以说它们是持久的，并且将跨会话存活。 然而，我们可以将 Znode 创建为短暂的，这意味着一旦创建它的客户机断开连接或被 ZooKeeper 服务器检测到死亡，它就会消失。 在 CLI 中，通过向 CREATE 命令添加`-e`标志来创建临时 Znode。

临时 ZNodes 是在分布式系统中实现组成员发现的一种很好的机制。 对于任何节点可能在没有通知的情况下发生故障、加入和离开的系统来说，知道哪些节点在任何时间点都是活动的通常是一项困难的任务。 在 ZooKeeper 中，我们可以让每个节点在 ZooKeeper 文件系统中的某个位置创建一个临时 Znode，从而为此类发现提供基础。 ZNode 可以保存有关服务节点的数据，如主机名、IP 地址、端口号等。 要获得活动节点的列表，我们可以简单地列出父组 Znode 的子节点。 由于临时节点的性质，我们可以确信在任何时候检索到的活动节点列表都是最新的。

如果我们让个服务节点创建 Znode 子节点，这些子节点不仅是短暂的，而且是连续的，那么我们还可以为需要在任何时候拥有单个主节点的服务构建领导人选举机制。 锁的机制与此相同；客户端服务节点创建顺序的和短暂的 Z 节点，然后检查它是否具有最低序列号。 如果是这样的话，那它就是主人了。 如果不是，则它将在下一个最低顺序节点上注册观察器，以便在它可能成为主节点时收到警报。

## Колибриобработает

`org.apache.zookeeper.ZooKeeper`类是访问 ZooKeeper 集合的主要编程客户端。 有关详细信息，请参阅 javadoc，但基本接口相对简单，与 CLI 中的命令明显一一对应。 例如：

*   `create`：等同于 CLI`create`
*   `getChildren`：等同于 CLI`ls`
*   `getData`：等同于 CLI`get`

## 积木

正如所见，ZooKeeper 提供了少量定义良好的操作，这些操作具有非常强的语义保证，可以构建到更高级别的服务中，例如我们前面讨论的锁、组成员和领导人选举。 最好将 ZooKeeper 看作是对分布式系统至关重要的精心设计和可靠功能的工具包，这些功能可以在其上构建，而不必担心其实现的复杂性。 不过，提供的 ZooKeeper 接口相当低级，并且出现了一些高级接口，它们提供了更多从低级原语到应用级逻辑的映射。 策展人项目([http://curator.apache.org/](http://curator.apache.org/))就是一个很好的例子。

ZooKeeper 在 Hadoop1 中使用得很少，但现在它非常普遍。 MapReduce 和 HDFS 都使用它来实现其 JobTracker 和 NameNode 组件的高可用性。 我们稍后将探讨的 HIVE 和 Impala 使用它在由多个并发作业访问的数据表上放置锁。 我们将在 Samza 的上下文中讨论的 Kafka 将 ZooKeeper 用于节点(Kafka 术语中的代理)、领导人选举和状态管理。

## 进一步阅读

我们没有详细描述 ZooKeeper，完全省略了一些方面，比如它将配额和访问控制列表应用于文件系统内的 ZNode 的能力，以及构建回调的机制。 我们在这里的目的是提供足够的细节，以便您对如何在本书中探讨的 Hadoop 服务中使用它有一些了解。 有关更多信息，请参阅项目主页。

# 自动 NameNode 故障转移

现在我们已经引入了 ZooKeeper，我们可以展示如何使用它来启用**自动 NameNode**故障转移。

Automatic NameNode Failover 向系统引入了两个新组件：**ZooKeeper Quorum**和在每个 NameNode 主机上运行的**ZooKeeper Failover Controller**(ZKFC)。 ZKFC 在 ZooKeeper 中创建一个短暂的 Znode，只要它检测到本地 NameNode 处于活动状态并正常工作，它就会一直持有该 Znode。 它通过不断向 NameNode 发送简单的健康检查请求来确定这一点，如果 NameNode 在短时间内未能正确响应，则 ZKFC 将假定 NameNode 已经失败。 如果 NameNode 机器崩溃或其他故障，ZooKeeper 中的 ZKFC 会话将关闭，短暂的 Znode 也将自动删除。

ZKFC 进程还在监视集群中其他 NameNode 的 ZNode。 如果备用 NameNode 主机上的 ZKFC 看到现有的主 Znode 消失，它将假定主 Znode 已出现故障，并将尝试故障转移。 它通过尝试获取 NameNode 的锁(通过 ZooKeeper 部分中描述的协议)来实现这一点，如果成功，它将通过前面描述的相同隔离/提升机制启动故障转移。

# HDFS 快照

我们在前面提到过，仅使用 HDFS 复制不是合适的备份策略。 在 Hadoop2 文件系统中，添加了快照，这为 HDFS 带来了另一个级别的数据保护。

文件系统快照在各种技术中已经使用了一段时间。 其基本思想是可以查看文件系统在特定时间点的确切状态。 这是通过在制作快照时获取文件系统元数据的副本并使其可供将来查看来实现的。

当对文件系统进行更改时，任何会影响快照的更改都会被特殊处理。 例如，如果存在于快照中的文件被删除，则即使它将从文件系统的当前状态中移除，其元数据仍将保留在快照中，并且与其数据相关联的块将保留在文件系统中，尽管不能通过除快照之外的任何系统视图来访问。

举个例子可以说明这一点。 假设您有一个包含以下文件的文件系统：

```scala
/data1 (5 blocks)
/data2 (10 blocks)
```

您拍摄快照，然后删除文件`/data2`。 如果查看文件系统的当前状态，则只有`/data1`可见。 如果检查快照，您将看到这两个文件。 在幕后，所有 15 个块仍然存在，但只有那些与未删除的文件`/data1`相关联的块是当前文件系统的一部分。 仅当快照本身被删除时，才会释放文件`/data2`的数据块-快照是只读视图。

Hadoop2 中的快照可以在整个文件系统级别上应用，也可以仅在特定路径上应用。 路径需要设置为快照表格，请注意，如果路径的任何子路径或父路径本身都是快照表格，则不能有路径快照表格。

让我们根据前面创建的目录来举一个简单的例子来说明快照的用法。 我们将要说明的命令需要以超级用户权限执行，而超级用户权限可以通过`sudo -u hdfs`获得。

首先，使用`hdfs`CLI 实用程序的`dfsadmin`子命令启用目录快照，如下所示：

```scala
$ sudo -u hdfs hdfs dfsadmin -allowSnapshot \
/user/cloudera/testdir
Allowing snapshot on testdir succeeded

```

现在，我们创建快照并对其进行检查；可以通过 snapshottable 目录的`.snapshot`子目录访问快照。 请注意，`.snapshot`目录在目录的正常列表中不可见。 下面是我们如何创建快照并对其进行检查：

```scala
$ sudo -u hdfs hdfs dfs -createSnapshot \
/user/cloudera/testdir sn1
Created snapshot /user/cloudera/testdir/.snapshot/sn1

$ sudo -u hdfs hdfs dfs -ls \
/user/cloudera/testdir/.snapshot/sn1

Found 1 items -rw-r--r--   1 cloudera cloudera         12 2014-11-13 11:21 /user/cloudera/testdir/.snapshot/sn1/testfile.txt

```

现在，我们从主目录中删除测试文件，并验证它现在是否为空：

```scala
$ sudo -u hdfs hdfs dfs -rm \
/user/cloudera/testdir/testfile.txt
14/11/13 13:13:51 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes. Moved: 'hdfs://localhost.localdomain:8020/user/cloudera/testdir/testfile.txt' to trash at: hdfs://localhost.localdomain:8020/user/hdfs/.Trash/Current
$ hdfs dfs -ls /user/cloudera/testdir
$

```

请注意提到的垃圾桶目录；默认情况下，HDFS 会将任何删除的文件复制到用户主目录中的`.Trash`目录中，这有助于防止手指滑倒。 这些文件可以通过`hdfs dfs -expunge`删除，或者默认情况下将在 7 天后自动清除。

现在，我们检查现在已删除的文件仍可用的快照：

```scala
$ hdfs dfs -ls testdir/.snapshot/sn1
Found 1 items drwxr-xr-x   - cloudera cloudera          0 2014-11-13 13:12 testdir/.snapshot/sn1
$ hdfs dfs -tail testdir/.snapshot/sn1/testfile.txt
Hello world

```

然后，我们可以删除快照，释放它持有的所有数据块，如下所示：

```scala
$ sudo -u hdfs hdfs dfs -deleteSnapshot \
/user/cloudera/testdir sn1 
$ hdfs dfs -ls testdir/.snapshot
$

```

可以看到，快照中的文件完全可供读取和复制，从而提供了对创建快照时文件系统的历史状态的访问。 每个目录最多可以有 65,535 个快照，HDFS 管理快照的方式对正常文件系统操作的影响非常高效。 它们是在任何可能产生负面影响的活动(例如尝试访问文件系统的应用的新版本)之前使用的一种很好的机制。 如果新软件损坏文件，则可以恢复目录的旧状态。 如果在一段时间的验证后软件被接受，则可以改为删除快照。

# Hadoop 文件系统

在之前，我们将 HDFS 称为*即*Hadoop 文件系统。 实际上，Hadoop 对文件系统有一个相当抽象的概念。 HDFS 只是`org.apache.hadoop.fs.FileSystem`Java 抽象类的几个实现之一。 可以在[https://hadoop.apache.org/docs/r2.5.0/api/org/apache/hadoop/fs/FileSystem.html](https://hadoop.apache.org/docs/r2.5.0/api/org/apache/hadoop/fs/FileSystem.html)中找到可用的文件系统列表。 下表总结了其中的一些文件系统，以及相应的 URI 方案和 Java 实现类。

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

档案系统

 | 

URI 方案

 | 

Java 实现

 |
| --- | --- | --- |
| 本地人 / 慢车 / 当地居民 / 本地新闻 | `file` | `org.apache.hadoop.fs.LocalFileSystem` |
| HDFS | `hdfs` | `org.apache.hadoop.hdfs.DistributedFileSystem` |
| S3(本地) | `s3n` | `org.apache.hadoop.fs.s3native.NativeS3FileSystem` |
| S3(基于数据块) | `s3` | `org.apache.hadoop.fs.s3.S3FileSystem` |

存在 S3 文件系统的两种实现。 Native-`s3n`-用于读写常规文件。 使用`s3n`存储的数据可由任何工具访问，反之亦然，可用于读取其他 S3 工具生成的数据。 `s3n`无法处理大于 5TB 的文件或重命名操作。

与 HDFS 非常类似，基于块的 S3 文件系统以块为单位存储文件，并要求 S3 存储桶专用于文件系统。 存储在 S3 文件系统中的文件可以大于 5 TB，但它们不能与其他 S3 工具互操作。 此外，基于块的 S3 支持重命名操作。

## Hadoop 接口

Hadoop 是用 Java 编写的，毫不奇怪，与系统的所有交互都是通过 Java API 进行的。 我们在前面的示例中通过`hdfs`命令使用的命令行界面是一个 Java 应用，它使用`FileSystem`类在可用的文件系统上执行输入/输出操作。

### Колибриобработается

由`org.apache.hadoop.fs`包提供的 Java API 公开了个 Apache Hadoop 文件系统。

`org.apache.hadoop.fs.FileSystem`是每个文件系统实现的抽象类，并提供与 Hadoop 中的数据交互的通用接口。 所有使用 HDFS 的代码都应该能够处理`FileSystem`对象。

### Libhdfs

Libhdfs 是一个 C 库，尽管它的名字是，但它可以用于访问任何 Hadoop 文件系统，而不仅仅是 HDFS。 它是使用 Java Native Interface(JNI)编写的，模拟 Java 文件系统类。

### 节俭

**Apache Thrift**([http://thrift.apache.org](http://thrift.apache.org))是一个框架，用于通过数据序列化和远程方法调用机制构建跨语言的软件。 在`contrib`中提供的 Hadoop Thrift API 将 Hadoop 文件系统公开为 Thrift 服务。 该接口使非 Java 代码能够轻松地访问存储在 Hadoop 文件系统中的数据。

除了上述接口之外，还有其他接口允许通过 HTTP 和 FTP(仅限 HDFS)以及 WebDAV 访问 Hadoop 文件系统。

# 管理和序列化数据

拥有文件系统固然不错，但我们还需要表示数据并将其存储在文件系统上的机制。 我们现在将探索其中的一些机制。

## 可写界面

对于我们开发人员来说，如果我们能够操作更高级别的数据类型，并让 Hadoop 负责将它们序列化为字节以写入文件系统并在从文件系统读取字节流时从字节流中重建所需的过程，这将是非常有用的。

`org.apache.hadoop.io package`包含 Writable 接口，该接口提供此机制，指定如下：

```scala
   public interface Writable
   {
   void write(DataOutput out) throws IOException ;
   void readFields(DataInput in) throws IOException ;
   }
```

此接口的主要用途是提供在通过网络传递数据或从磁盘读取和写入数据时对数据进行序列化和反序列化的机制。

当我们在后面的章节中探索 Hadoop 上的处理框架时，我们经常会看到要求数据参数是类型 Writable 的实例。 如果我们使用提供此接口的适当实现的数据结构，则 Hadoop 机制可以自动管理数据类型的序列化和反序列化，而不需要知道它表示什么或如何使用。

## 介绍包装器类

幸运的是，您不必从头开始构建您将使用的所有数据类型的可写变体。 Hadoop 提供了包装 Java 原语类型并实现 Writable 接口的类。 它们在`org.apache.hadoop.io`包中提供。

这些类在概念上类似于`java.lang`中的原始包装类，如 Integer 和 Long。 它们保存单个原始值，可以在构造时设置，也可以通过 setter 方法设置。 这些建议如下：

*   `BooleanWritable`
*   `ByteWritable`
*   `DoubleWritable`
*   `FloatWritable`
*   `IntWritable`
*   `LongWritable`
*   `VIntWritable`：可变长度整型
*   `VLongWritable`：可变长度长型
*   还有一个文本，它对`java.lang.String`进行换行。

## 数组包装类

Hadoop 还提供了一些基于集合的包装类。 这些类为其他 Writable 对象数组提供了可写包装器。 例如，实例可以保存`IntWritable`或`DoubleWritable`的数组，但不能保存原始 int 或 Float 类型的数组。 需要为所需的 Writable 类指定一个子类。 这些建议如下：

```scala
ArrayWritable
TwoDArrayWritable
```

## 可比较接口和可写可比较接口

当我们说包装类实现`Writable`时，我们有点不准确；它们实际上在`org.apache.hadoop.io`包中实现了一个名为`WritableComparable`的复合接口，该复合接口将`Writable`与标准的`java.lang.Comparable`接口结合起来：

```scala
   public interface WritableComparable extends Writable, Comparable
   {}
```

只有当我们在下一章探索 MapReduce 时，对`Comparable`的需求才会变得明显，但现在，只需记住包装器类提供了由 Hadoop 或其任何框架对其进行序列化和排序的机制。

# 存储数据

到目前为止，我们介绍了 HDFS 的体系结构，以及如何使用命令行工具和 Java API 以编程方式存储和检索数据。 在到目前为止看到的示例中，我们隐含地假设我们的数据存储为文本文件。 实际上，一些应用和数据集需要特殊的数据结构来保存文件内容。 多年来，创建文件格式既是为了满足 MapReduce 处理的要求(例如，我们希望数据是可拆分的)，也是为了满足对结构化和非结构化数据建模的需要。 目前，很多注意力都集中在更好地捕捉关系数据存储和建模的用例上。 在本章的剩余部分，我们将介绍 Hadoop 生态系统中可用的一些流行的文件格式选择。

## 序列化和容器

在讨论文件格式时，我们假设有两种情况，如下所示：

*   **序列化：**我们希望将在处理时生成和操作的数据结构编码为我们可以存储到文件中、传输并在稍后阶段检索并转换回以供进一步处理的格式
*   **容器**：一旦数据被序列化为文件，容器就提供了将多个文件组合在一起并添加附加元数据的方法

## 压缩

在处理数据时，文件压缩通常可以显著节省存储文件所需的空间，以及跨网络和从/到本地磁盘的数据 I/O。

概括地说，使用处理框架时，压缩可以在处理管道中的三个点发生：

*   要处理的输入文件
*   处理完成后产生的输出文件
*   管道内部生成的中间/临时文件

当我们在这些阶段中的任何一个阶段添加压缩时，我们就有机会大幅减少要读取或写入磁盘或通过网络的数据量。 这对于 MapReduce 这样的框架特别有用，例如，这些框架可以生成比输入或输出数据集更大的临时数据量。

Apache Hadoop 附带了许多压缩编解码器：gzip、bzip2、lzo、snappy-每个都有自己的折衷。 选择编解码器是经过深思熟虑的选择，应该既考虑正在处理的数据的类型，也考虑处理框架本身的性质。

除了一般的空间/时间权衡(其中最大的空间节省是以压缩和解压缩速度为代价(反之亦然))，我们还需要考虑存储在 HDFS 中的数据将由并行的分布式软件访问；其中一些软件还将增加其自身对文件格式的特殊要求。 例如，MapReduce 对可以拆分为有效子文件的文件最有效。

这可能会使决策复杂化，比如选择是否压缩以及在压缩时使用哪个编解码器，因为大多数压缩编解码器(如 gzip)不支持可拆分文件，而少数压缩编解码器(如 LZO)支持。

## 通用文件格式

第一类文件格式是那些通用的文件格式，可以应用于任何应用域，并且不对数据结构或访问模式进行任何假设。

*   **text**：在 HDFS 上存储数据的最简单方法是使用平面文件。 文本文件既可用于保存非结构化数据(网页或推文)，也可用于保存结构化数据(长度为行的 CSV 文件)。 文本文件是可拆分的，但需要考虑如何处理文件中多个元素(例如，行)之间的边界。
*   **SequenceFile**：SequenceFile 是由二进制键/值对组成的平面数据结构，引入该结构是为了满足基于 MapReduce 的处理的特定要求。 在 MapReduce 中，它仍然作为一种输入/输出格式被广泛使用。 正如我们将在[第 3 章](03.html "Chapter 3. Processing – MapReduce and Beyond")，*Processing-MapReduce 和 Beyond*中看到的，在内部，映射的临时输出使用 SequenceFile 存储。

SequenceFile 分别提供`Writer`、`Reader`和`Sorter`类来写入、读取和排序数据。

根据使用的压缩机制，可以区分 SequenceFile 的三种变体：

*   未压缩的键/值记录。
*   记录压缩的键/值记录。 只有‘value’被压缩。
*   阻止压缩的键/值记录。 键和值被收集在任意大小的块中，并分别压缩。

然而，在每种情况下，SequenceFile 都是可拆分的，这是它最大的优势之一。

## 面向列的数据格式

在关系数据库世界中，面向列的数据存储根据列组织和存储表；一般来说，每列的数据将存储在一起。 与大多数按行组织数据的关系型 DBMS 相比，这是一种显著不同的方法。 面向列的存储具有显著的性能优势；例如，如果查询只需要从包含数百列的非常宽的表中读取两列，则只访问所需的列数据文件。 传统的面向行的数据库必须读取需要数据的每一行的所有列。 这对在大量相似项上计算聚合函数的工作负载(例如数据仓库系统的典型 OLAP 工作负载)的影响最大。

在[第 7 章](07.html "Chapter 7. Hadoop and SQL")、*Hadoop 和 SQL*中，我们将看到 Hadoop 如何成为数据仓库世界的 SQL 后端，这要归功于 Apache Have 和 Cloudera Impala 等项目。 作为向该领域扩展的一部分，已经开发了许多文件格式来满足关系建模和数据仓库需求。

RCFile、ORC 和 Parquet 是针对这些用例开发的三种最先进的面向列的文件格式。

### RCFile

行列文件(RCFile)最初由 Facebook 开发，用作其 Hive 数据仓库系统的后端存储，该系统是第一个开源的主流 SQL-on-Hadoop 系统。

RCFile 的目标是提供以下功能：

*   快速数据加载
*   快速查询处理
*   高效的存储利用率
*   对动态工作负载的适应性

有关 RCFile 的更多信息，请参见[http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/abs11-4.html](http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/abs11-4.html)。

### 兽人

优化的行列文件格式(ORC)旨在将 RCFile 的性能与 Avro 的灵活性相结合。 它主要用于 Apache Have，最初由 Hortonworks 开发，以克服其他可用文件格式的感知限制。

更多详细信息可以在[http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html)上找到。

### 检察官办公室

Parquet 发现于[Cloudera](http://parquet.incubator.apache.org)，最初是 Cloudera、http://parquet.incubator.apache.org 和 Criteo 共同开发的，现在已捐赠给 Apache 软件基金会。 Parquet 的目标是为 Cloudera Impala 提供一种现代的、高性能的柱状文件格式。 与黑斑羚一样，镶木地板的灵感来自德雷梅尔的论文([http://research.google.com/pubs/pub36632.html](http://research.google.com/pubs/pub36632.html))。 它允许复杂的嵌套数据结构，并允许在每列级别上进行高效编码。

### _

Apache Avro([http://avro.apache.org](http://avro.apache.org))是一种面向模式的二进制数据序列化格式和文件容器。 在本书中，AVRO 将是我们首选的二进制数据格式。 它既是可拆分的，也是可压缩的，这使得它成为使用 MapReduce 等框架进行数据处理的有效格式。

然而，许多其他项目也有内置的特定 Avro 支持和集成，因此它的应用非常广泛。 当数据存储在 avro 文件中时，其架构(定义为 JSON 对象)与其一起存储。 文件可以稍后由第三方处理，而不需要事先知道数据是如何编码的。 这使得数据具有自描述性，并便于使用动态和脚本语言。 读取时模式模型还有助于提高 Avro 记录的存储效率，因为不需要对各个字段进行标记。

在后面的章节中，您将看到这些属性如何简化数据生命周期管理，并允许模式迁移等重要操作。

### 使用 Java API

现在，我们将演示如何使用 Java API 来解析 Avro 模式、读写 Avro 文件以及使用 Avro 的代码生成工具。 请注意，该格式本质上是独立于语言的；大多数语言都有 API，Java 创建的文件可以从任何其他语言无缝读取。

AVRO 模式被描述为 JSON 文档，并由`org.apache.avro.Schema`类表示。 为了演示用于操作 Avro 文档的 API，我们将向前看我们在[第 7 章](07.html "Chapter 7. Hadoop and SQL")、*Hadoop 和 SQL*中用于配置单元表的 Avro 规范。 可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch2/src/main/java/com/learninghadoop2/avro/AvroParse.java](https://github.com/learninghadoop2/book-examples/blob/master/ch2/src/main/java/com/learninghadoop2/avro/AvroParse.java)找到以下代码。

在下面的代码中，我们将使用 Avro Java API 创建一个包含 tweet 记录的 avro 文件，然后使用文件中的架构重新读取该文件，以提取存储记录的详细信息：

```scala
    public static void testGenericRecord() {
        try {
            Schema schema = new Schema.Parser()
   .parse(new File("tweets_avro.avsc"));
            GenericRecord tweet = new GenericData
   .Record(schema);

            tweet.put("text", "The generic tweet text");

            File file = new File("tweets.avro");
            DatumWriter<GenericRecord> datumWriter = 
               new GenericDatumWriter<>(schema);
            DataFileWriter<GenericRecord> fileWriter = 
               new DataFileWriter<>( datumWriter );

            fileWriter.create(schema, file);
            fileWriter.append(tweet);
            fileWriter.close();

            DatumReader<GenericRecord> datumReader = 
                new GenericDatumReader<>(schema);
            DataFileReader<GenericRecord> fileReader = 
                new DataFileReader(file, datumReader);
            GenericRecord genericTweet = null;

            while (fileReader.hasNext()) {
                genericTweet = (GenericRecord) fileReader
                    .next(genericTweet);

                for (Schema.Field field : 
                    genericTweet.getSchema().getFields()) {
                    Object val = genericTweet.get(field.name());

                    if (val != null) {
                        System.out.println(val);
                    }
                }

            }
        } catch (IOException ie) {
            System.out.println("Error parsing or writing file.");
        }
    }
```

位于[https://github.com/learninghadoop2/book-examples/blob/master/ch2/tweets_avro.avsc](https://github.com/learninghadoop2/book-examples/blob/master/ch2/tweets_avro.avsc)，处的`tweets_avro.avsc`模式描述具有多个字段的 tweet。 要创建这种类型的 avro 对象，我们首先要解析架构文件。 然后，我们使用 Avro 的`GenericRecord`概念构建符合此模式的 Avro 文档。 在本例中，我们只设置一个属性-tweet 文本本身。

要写入这个包含单个对象的 avro 文件，我们将使用 avro 的 I/O 功能。 要读取该文件，我们不需要从模式开始，因为我们可以从从文件读取的`GenericRecord`中提取该模式。 然后，我们遍历架构结构，并基于发现的字段动态处理文档。 这一功能尤其强大，因为它是客户端保持独立于 Avro 模式以及它如何随时间发展的关键推动因素。

但是，如果我们事先有了模式文件，我们就可以使用 Avro 代码生成来创建一个定制类，使操作 Avro 记录变得容易得多。 要生成代码，我们将使用`avro-tools.jar`中的 Compile 类，向其传递模式文件的名称和所需的输出目录：

```scala
$ java -jar /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/lib/avro/avro-tools.jar compile schema tweets_avro.avsc src/main/java

```

该类将被放置在基于模式中定义的任何命名空间的目录结构中。 由于我们在`com.learninghadoop2.avrotables`名称空间中创建了此模式，因此我们可以看到以下内容：

```scala
$ ls src/main/java/com/learninghadoop2/avrotables/tweets_avro.java

```

通过这个类，让我们回顾一下 Avro 对象的创建和读写操作，如下所示：

```scala
    public static void testGeneratedCode() {
        tweets_avro tweet = new tweets_avro();
        tweet.setText("The code generated tweet text");

        try {
            File file = new File("tweets.avro");
            DatumWriter<tweets_avro> datumWriter = 
                new SpecificDatumWriter<>(tweets_avro.class);
            DataFileWriter<tweets_avro> fileWriter = 
                new DataFileWriter<>(datumWriter);

            fileWriter.create(tweet.getSchema(), file);
            fileWriter.append(tweet);
            fileWriter.close();

            DatumReader<tweets_avro> datumReader = 
                new SpecificDatumReader<>(tweets_avro.class);
            DataFileReader<tweets_avro> fileReader = 
                new DataFileReader<>(file, datumReader);

            while (fileReader.hasNext()) {
                tweet = fileReader.next(tweet);
                System.out.println(tweet.getText());
            }
        } catch (IOException ie) {
            System.out.println("Error in parsing or writingfiles.");
        }
    }
```

因为我们使用了代码生成，所以我们现在将 avro`SpecificRecord`机制与生成的表示域模型中的对象的类一起使用。 因此，我们可以直接实例化对象并通过熟悉的 get/set 方法访问其属性。

编写文件类似于前面执行的操作，不同之处在于我们使用特定的类，并在需要时直接从 tweet 对象检索模式。 类似地，通过创建特定类的实例并使用 get/set 方法可以简化读取。

# 摘要

本章简要介绍了 Hadoop 集群上的存储。 我们特别介绍了以下内容：

*   Hadoop 中使用的主要文件系统 HDFS 的高级体系结构
*   HDFS 如何在幕后工作，尤其是其实现可靠性的方法
*   Hadoop 2 如何显著增加了 HDFS，特别是以 NameNode HA 和文件系统快照的形式
*   ZooKeeper 是什么，Hadoop 如何使用它来启用 NameNode 自动故障切换等功能
*   用于访问 HDFS 的命令行工具概述
*   Hadoop 中用于文件系统的 API 以及 HDFS 如何在代码级成为更灵活的文件系统抽象的一种实现
*   如何将数据序列化到 Hadoop 文件系统，以及核心类中提供的一些支持
*   Hadoop 中最常存储数据的各种文件格式及其一些特定使用情形

在下一章中，我们将详细介绍 Hadoop 如何提供可用于处理存储在其中的数据的处理框架。