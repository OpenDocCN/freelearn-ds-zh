# 九、让开发变得更容易

在本章中，我们将介绍如何根据用例和最终目标，使用构建在 JavaAPI 之上的大量抽象和框架来简化 Hadoop 中的应用开发。 我们将特别了解以下主题：

*   流 API 如何允许我们使用 Python 和 Ruby 等动态语言编写 MapReduce 作业
*   Apache Crunch 和 Kite Morphline 等框架如何允许我们使用更高级别的抽象来表示数据转换管道
*   Kite Data 是 Cloudera 开发的一个很有前途的框架，它如何为我们提供了应用设计模式和样板来简化 Hadoop 生态系统中不同组件的集成和互操作性的能力

# 选择框架

在前面的章中，我们了解了用于编写分布式应用的 MapReduce 和 Spark 编程 API。 虽然这些 API 非常强大和灵活，但它们具有一定程度的复杂性，可能需要大量的开发时间。

为了减少冗长，我们引入了 Pig 和 Have 框架，它们将特定于领域的语言 Pig 拉丁语和 Have QL 编译到许多 MapReduce 作业或 Spark DAG 中，有效地将 API 抽象出来。 这两种语言都可以使用 UDF 进行扩展，UDF 是将复杂逻辑映射到 Pig 和 Have 数据模型的一种方式。

当我们需要一定程度的灵活性和模块性时，事情可能会变得棘手。 根据用例和开发人员需求，Hadoop 生态系统提供了大量的 API、框架和库选择。 在本章中，我们将识别四类用户，并将其与以下相关工具进行匹配：

*   希望避免使用 Java 而倾向于使用动态语言编写 MapReduce 作业脚本的开发人员，或者使用未在 JVM 上实现的语言的开发人员。 典型的用例是前期分析和快速原型制作：Hadoop Streaming
*   Java 开发人员，需要集成 Hadoop 生态系统的组件，并且可以受益于代码化的设计模式和样板：Kite Data
*   希望使用熟悉的 API 编写模块化数据管道的 Java 开发人员：Apache Crunch
*   更愿意配置数据转换链的开发人员。 例如，一个想要在 ETL 管道中嵌入现有代码的数据工程师：Kite Morphines

# Hadoop 流

我们在前面已经提到过，MapReduce 程序不必用 Java 编写。 您可能想要或需要用另一种语言编写地图和减少任务，原因有几个。 也许您有现有的代码可以利用，或者需要使用第三方二进制文件-原因是多种多样且合理的。

Hadoop 提供了许多机制来帮助非 Java 开发，其中最主要的是 Hadoop 管道(提供本地 C++接口)和 Hadoop 流(允许任何使用标准输入和输出的程序用于映射和缩减任务)。 使用 MapReduce Java API，map 和 Reduce 任务都为包含任务功能的方法提供实现。 这些方法将输入作为方法参数接收到任务，然后通过`Context`对象输出结果。 这是一个清晰且类型安全的接口，但根据定义它是特定于 Java 的。

Hadoop 流媒体采用了一种不同的方法。 使用流，您可以编写一个映射任务，该任务从标准输入读取其输入，一次一行，并将其结果的输出提供给标准输出。 然后，Reduce 任务也执行同样的操作，同样只对其数据流使用标准输入和输出。

从标准输入和输出读取和写入的任何程序都可以在流中使用，比如编译的二进制文件、Unix shell 脚本或用动态语言(如 Python 或 Ruby)编写的程序。 流媒体的最大优势是，它可以让你尝试想法，并比使用 Java 更快地迭代它们。 您只需编写脚本并将它们作为参数传递到流 JAR 文件，而不是编译/JAR/提交循环。 特别是在对新数据集进行初步分析或尝试新想法时，这可以显著加快开发速度。

关于动态语言和静态语言的经典争论平衡了快速开发与运行时性能和类型检查的好处。 使用流式传输时，这些动态缺点也适用于。 因此，我们倾向于使用流式处理进行前期分析，使用 Java 实现将在生产集群上执行的作业。

## Python 中的流式字数统计

我们将通过使用 Python 重新实现我们熟悉的字数统计示例来演示 Hadoop 流。 首先，我们创建一个脚本作为我们的映射器。 它使用`for`循环使用标准输入中的 UTF-8 编码文本行，将其拆分成单词，并使用`print`函数将每个单词写入标准输出，如下所示：

```scala
#!/bin/env python
import sys

for line in sys.stdin:
    # skip empty lines
    if line == '\n':
        continue

    # preserve utf-8 encoding
    try:
        line = line.encode('utf-8')
    except UnicodeDecodeError:
        continue
    # newline characters can appear within the text
    line = line.replace('\n', '')

    # lowercase and tokenize
    line = line.lower().split()

    for term in line:
        if not term:
          continue
        try:
            print(
                u"%s" % (
                    term.decode('utf-8')))
        except UnicodeEncodeError:
            continue
```

减法器统计标准输入中每个单词的出现次数，并将输出作为最终值提供给标准输出，如下所示：

```scala
#!/bin/env python
import sys

count = 1
current = None

for word in sys.stdin:
    word = word.strip()

    if word == current:
        count += 1
    else:
        if current:
            print "%s\t%s" % (current.decode('utf-8'), count)
        current = word
        count = 1
if current == word:
    print "%s\t%s" % (current.decode('utf-8'), count)
```

### 备注

在这两种情况下，我们都隐式使用前面章节中讨论的 Hadoop 输入和输出格式。 它是处理源文件的`TextInputFormat`，并将每一行一次提供给映射脚本。 相反，`TextOutputFormat`将确保 Reduce 任务的输出也被正确地写为文本。

将`map.py`和`reduce.py`复制到 HDFS，然后使用前几章中的样本数据将脚本作为流作业执行，如下所示：

```scala
$ hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
-file map.py \
-mapper "python map.py" \
-file reduce.py \
-reducer "python reduce.py" \
-input sample.txt \
-output output.txt 

```

### 备注

推文采用`UTF-8`编码。 确保相应地设置了`PYTHONIOENCODING`，以便在 UNIX 终端中通过管道传输数据：

```scala
$ export PYTHONIOENCODING='UTF-8'

```

可以从命令行提示符执行相同的代码：

```scala
$ cat sample.txt | python map.py| python reduce.py > out.txt

```

映射器和减速器代码可在[https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/wc/python/map.py](https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/wc/python/map.py)中找到。

## 使用流式处理时作业的差异

在 Java 中，我们知道我们的`map()`方法将为每个输入键/值对调用一次，而我们的`reduce()`方法将为每个键及其值集调用。

对于流，我们不再有 map 或 Reduce 方法的概念；相反，我们编写了处理接收数据流的脚本。 这改变了我们编写减速器的方式。 在 Java 中，每个键的值分组是由 Hadoop 执行的；每次调用 Reduce 方法都会收到一个用制表符分隔的键及其所有值。 在流式处理中，Reduce 任务的每个实例每次都被赋予一个单独的未收集的值。

Hadoop 流确实会对键进行排序，例如，如果映射器发出以下数据：

```scala
First 1
Word 1
Word 1
A 1
First 1

```

流减少器将按以下顺序接收它：

```scala
A 1
First 1
First 1
Word 1
Word 1

```

Hadoop 仍然收集每个键的值，并确保每个键只传递给一个减法器。 换句话说，Reducer 获取多个键的所有值，并将它们分组在一起；但是，它们不会打包到 Reducer 的单独执行中，也就是每个键一个值，这与 Java API 不同。 由于 Hadoop 流使用`stdin`和`stdout`通道在任务之间交换数据，因此调试和错误消息不应打印到标准输出。 在下面的示例中，我们将使用 Python`logging`([https://docs.python.org/2/library/logging.html](https://docs.python.org/2/library/logging.html))包将警告语句记录到一个文件中。

## 查找文本中的重要单词

我们现在将实现一个度量**Term Frequency-Inverse Document Frequency**(**TF-IDF**)，它将帮助我们根据单词在一组文档(在我们的例子中是 tweet)中出现的频率来确定它们的重要性。

直观地说，如果一个词经常出现在文档中，它就很重要，应该给它一个高分。 然而，如果一个词出现在许多文档中，我们应该用较低的分数来惩罚它，因为它是一个常见的词，而且它的出现频率并不是本文所独有的。

因此，许多文档中出现的常见单词(如*The*和表示的*)将被缩小。 在一条推文中频繁出现的词语将被放大。 TF-IDF 的使用通常与其他度量和技术结合使用，包括停用词删除和文本分类。 请注意，此技术在处理较短的文档(如 tweet)时会有缺点。 在这种情况下，术语频率分量将趋向于变为 1。 相反，人们可以利用这一特性来检测离群值。*

我们将在示例中使用的 TF-IDF 的定义如下：

```scala
tf = # of times term appears in a document (raw frequency)
idf = 1+log(#  of documents / # documents with term in it)
tf-idf = tf * idf
```

我们将使用三个 MapReduce 作业在 Python 中实现该算法：

*   第一个计算词频
*   第二个计算文档频率(IDF 的分母)
*   第三个计算每条推文的 TF-IDF

### 计算词频

词频部分与字数统计示例非常相似。 主要区别在于，我们将使用多字段、制表符分隔的键来跟踪术语和文档 ID 的同时出现情况。 对于 JSON 格式的每个 tweet，映射器提取`id_str`和`text`字段，标记化`text`，并发出`term`、`doc_id`元组：

```scala
for tweet in sys.stdin:
    # skip empty lines
    if tweet == '\n':
        continue
    try:
        tweet = json.loads(tweet)
    except:
        logger.warn("Invalid input %s " % tweet)
        continue
    # In our example one tweet corresponds to one document.
    doc_id = tweet['id_str']
    if not doc_id:
        continue

    # preserve utf-8 encoding
    text = tweet['text'].encode('utf-8')
    # newline characters can appear within the text
    text = text.replace('\n', '')

    # lowercase and tokenize
    text = text.lower().split()

    for term in text:
        try:
            print(
                u"%s\t%s" % (
                    term.decode('utf-8'), doc_id.decode('utf-8'))
                )
        except UnicodeEncodeError:
            logger.warn("Invalid term %s " % term)
```

在减法器中，我们以制表符分隔的字符串形式发出文档中每个术语的频率：

```scala
freq = 1
cur_term, cur_doc_id = sys.stdin.readline().split()
for line in sys.stdin:
    line = line.strip()
    try:
        term, doc_id = line.split('\t')
    except:
        logger.warn("Invalid record %s " % line)

    # the key is a (doc_id, term) pair
    if (doc_id == cur_doc_id) and (term == cur_term):
        freq += 1

    else:
        print(
            u"%s\t%s\t%s" % (
                cur_term.decode('utf-8'), cur_doc_id.decode('utf-8'), freq))
        cur_doc_id = doc_id
        cur_term = term
        freq = 1

print(
    u"%s\t%s\t%s" % (
        cur_term.decode('utf-8'), cur_doc_id.decode('utf-8'), freq))
```

要使此实现正常工作，减速器输入按术语排序是至关重要的。 我们可以使用以下管道从命令行测试这两个脚本：

```scala
$ cat tweets.json  |  python map-tf.py  | sort -k1,2  | \
python reduce-tf.py

```

在命令行中，我们使用`sort`实用程序，而在 MapReduce 中，我们将使用`org.apache.hadoop.mapreduce.lib.KeyFieldBasedComparator`。 该比较器实现`sort`命令提供的功能子集。 特别地，可以使用`–k<position>`选项指定按字段排序。 要按术语(键的第一个字段)过滤，我们设置`-D mapreduce.text.key.comparator.options=-k1`：

```scala
/usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
-D map.output.key.field.separator=\t \
-D stream.num.map.output.key.fields=2 \
-Dmapreduce.output.key.comparator.class=\
org.apache.hadoop.mapreduce.lib.KeyFieldBasedComparator \
-D mapreduce.text.key.comparator.options=-k1,2 \
-input tweets.json \
-output /tmp/tf-out.tsv \
-file map-tf.py \
-mapper "python map-tf.py" \
-file reduce-tf.py \
-reducer "python reduce-tf.py" 

```

### 备注

我们在比较器选项中指定哪些字段属于键(用于混洗)。

映射器和减速器代码可在[https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/map-tf.py](https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/map-tf.py)中找到。

### 计算单据频次

计算文档频率的主逻辑在减法器中，而映射器只是一个标识函数，它加载并输送 TF 作业的输出(按术语排序)。 在缩减器中，对于每个术语，我们计算它在所有文档中出现的次数。 对于每个术语，我们保留(`term`，`doc_id`，`tf`)元组的缓冲区`key_cache`，当找到新的术语时，我们将缓冲区刷新为标准输出，以及累积的文档频率`df`：

```scala
# Cache the (term,doc_id, tf) tuple. 
key_cache = []

line = sys.stdin.readline().strip()
cur_term, cur_doc_id, cur_tf = line.split('\t')
cur_tf = int(cur_tf)
cur_df = 1

for line in sys.stdin:
    line = line.strip()

    try:
        term, doc_id, tf = line.strip().split('\t')
        tf = int(tf)
    except:
        logger.warn("Invalid record: %s " % line)
        continue

    # term is the only key for this input
    if (term == cur_term):
        # increment document frequency
        cur_df += 1

        key_cache.append(
            u"%s\t%s\t%s" % (term.decode('utf-8'), doc_id.decode('utf-8'), tf))

    else:
        for key in key_cache:
            print("%s\t%s" % (key, cur_df))

        print (
            u"%s\t%s\t%s\t%s" % (
                cur_term.decode('utf-8'),
                cur_doc_id.decode('utf-8'),
                cur_tf, cur_df)
            )

        # flush the cache
        key_cache = []
        cur_doc_id = doc_id
        cur_term = term
        cur_tf = tf
        cur_df = 1

for key in key_cache:
    print(u"%s\t%s" % (key.decode('utf-8'), cur_df))
print(
    u"%s\t%s\t%s\t%s\n" % (
        cur_term.decode('utf-8'),
        cur_doc_id.decode('utf-8'),
        cur_tf, cur_df))
```

我们可以通过以下命令行测试脚本：

```scala
$ cat /tmp/tf-out.tsv  |  python map-df.py  | python reduce-df.py > /tmp/df-out.tsv

```

我们可以通过以下方式测试 Hadoop 流上的脚本：

```scala
/usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
-D map.output.key.field.separator=\t \
-D stream.num.map.output.key.fields=3 \
-D mapreduce.output.key.comparator.class=\
org.apache.hadoop.mapreduce.lib.KeyFieldBasedComparator \
-D mapreduce.text.key.comparator.options=-k1 \
-input /tmp/tf-out.tsv/part-00000 \
-output /tmp/df-out.tsv \
-mapper org.apache.hadoop.mapred.lib.IdentityMapper \
-file reduce-df.py \
-reducer "python reduce-df.py"

```

在 Hadoop 上，我们使用`org.apache.hadoop.mapred.lib.IdentityMapper`，它提供与`map-df.py`脚本相同的逻辑。

映射器和减速器代码可在[https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/map-df.py](https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/map-df.py)中找到。

### 把它们放在一起-TF-IDF

要计算 TF-IDF，我们只需要一个使用上一步输出的映射器：

```scala
num_doc = sys.argv[1]

for line in sys.stdin:
    line = line.strip()

    try:
        term, doc_id, tf, df = line.split('\t')

        tf = float(tf)
        df = float(df)
        num_doc = float(num_doc)
    except:
        logger.warn("Invalid record %s" % line)

    # idf = num_doc / df
    tf_idf = tf * (1+math.log(num_doc / df))
    print("%s\t%s\t%s" % (term, doc_id, tf_idf))
```

集合中的文档数作为参数传递给`tf-idf.py`：

```scala
/usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
-D mapreduce.reduce.tasks=0 \
-input /tmp/df-out.tsv/part-00000 \
-output /tmp/tf-idf.out \
-file tf-idf.py \
-mapper "python tf-idf.py 15578"

```

要计算 tweet 总数，我们可以将`cat`和`wc`Unix 实用程序与 Hadoop 流结合使用：

```scala
/usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
-input tweets.json \
-output tweets.cnt \
-mapper /bin/cat \
-reducer /usr/bin/wc

```

映射器源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/tf-idf.py](https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/tf-idf.py)找到。

# 套件数据

Kite SDK([Hadoop](http://www.kitesdk.org))是类、命令行工具和示例的集合，旨在简化在 http://www.kitesdk.org 之上构建应用的过程。

在本节中，我们将了解 Kite 的子项目 Kite Data 如何简化与 Hadoop 数据仓库的几个组件的集成。 风筝示例可以在[https://github.com/kite-sdk/kite-examples](https://github.com/kite-sdk/kite-examples)找到。

在 Cloudera 的 QuickStart VM 上，可以在`/opt/cloudera/parcels/CDH/lib/kite/`找到 Kite Jars。

Kite 数据被组织在许多子项目中，我们将在下面的部分中描述其中一些子项目。

## _

顾名思义，核心是数据模块中提供的所有功能的构建块。 它的主要抽象是数据集和存储库。

`org.kitesdk.data.Dataset`接口用于表示一组不可变的数据：

```scala
@Immutable
public interface Dataset<E> extends RefinableView<E> {
  String getName();
  DatasetDescriptor getDescriptor();
  Dataset<E> getPartition(PartitionKey key, boolean autoCreate);
  void dropPartition(PartitionKey key);
  Iterable<Dataset<E>> getPartitions();
  URI getUri();
}
```

每个数据集由`org.kitesdk.data.DatasetDescriptor`接口的名称和实例标识，即数据集的结构描述，并提供其模式(`org.apache.avro.Schema`)和分区策略。

`Reader<E>`接口的实现用于从底层存储系统读取数据并产生类型为`E`的反序列化实体。 `newReader()`方法可用于获取给定数据集的适当实现：

```scala
public interface DatasetReader<E> extends Iterator<E>, Iterable<E>, Closeable {
  void open();

  boolean hasNext();

  E next();
    void remove();
    void close();
    boolean isOpen();
}
```

`DatasetReader`的实例将提供读取和迭代数据流的方法。 类似地，`org.kitesdk.data.DatasetWriter`提供了将数据流写入`Dataset`对象的接口：

```scala
public interface DatasetWriter<E> extends Flushable, Closeable {
  void open();
  void write(E entity);
  void flush();
  void close();
  boolean isOpen();
}
```

与阅读器一样，编写器也是一次性使用的对象。 它们序列化类型为`E`的实体的实例，并将它们写入底层存储系统。 编写器通常不会直接实例化；相反，可以通过`newWriter()`工厂方法创建适当的实现。 `DatasetWriter`的实现将持有资源，直到调用`close()`，并期望调用方在写入器不再使用时调用`finally`块中的`close()`。 最后，请注意`DatasetWriter`的实现通常不是线程安全的。 从多个线程访问编写器的行为未定义。

数据集的一个特殊情况是`View`接口，如下所示：

```scala
public interface View<E> {
   Dataset<E> getDataset();
   DatasetReader<E> newReader();
   DatasetWriter<E> newWriter();
   boolean includes(E entity);
   public boolean deleteAll();
}
```

视图携带现有数据集的键和分区的子集；它们在概念上类似于关系模型中的“视图”概念。

`View`界面可以从数据范围或键范围创建，也可以作为其他视图之间的联合创建。

## _Data HCatalog

Data HCatalog 是一个模块，可用于访问 HCatalog 存储库。 该模块的核心抽象是`org.kitesdk.data.hcatalog.HCatalogAbstractDatasetRepository`及其具体实现`org.kitesdk.data.hcatalog.HCatalogDatasetRepository`。

它们描述了使用 HCatalog 管理存储的元数据和 HDFS 的`DatasetRepository`，如下所示：

```scala
public class HCatalogDatasetRepository extends HCatalogAbstractDatasetRepository {
   HCatalogDatasetRepository(Configuration conf) {
    super(conf, new HCatalogManagedMetadataProvider(conf));
  }
   HCatalogDatasetRepository(Configuration conf, MetadataProvider provider) {
    super(conf, provider);
  }
   public <E> Dataset<E> create(String name, DatasetDescriptor descriptor) {
    getMetadataProvider().create(name, descriptor);
    return load(name);
  }
   public boolean delete(String name) {
    return getMetadataProvider().delete(name);
  }
   public static class Builder {
   …
  }
}
```

### 备注

从 Kite 0.17 开始，Data HCatalog 已弃用，取而代之的是新的 Data Have 模块。

数据目录的位置由`Hive/HCatalog`选择(所谓的“托管表”)，或者在创建该类的实例时通过在构造函数中提供文件系统和根目录(外部表)来指定。

## _

kite-data-模块通过`Dataset`接口公开配置单元模式。 从 Kite 0.17 开始，此包将取代 Data HCatalog。

## Колибрипрограмма数据映射还原

`org.kitesdk.data.mapreduce`包提供了接口，用于使用 MapReduce 对数据集进行读写操作。

## ==同步，由长者更正==

`org.kitesdk.data.spark`包提供了接口，用于使用 Apache Spark 对数据集进行读写操作。

## _

`org.kitesdk.data.crunch.CrunchDatasets`包是一个帮助器类，用于将数据集和视图公开为 Crunch`ReadableSource`或`Target`类：

```scala
public class CrunchDatasets {
public static <E> ReadableSource<E> asSource(View<E> view, Class<E> type) {
    return new DatasetSourceTarget<E>(view, type);
  }
public static <E> ReadableSource<E> asSource(URI uri, Class<E> type) {
    return new DatasetSourceTarget<E>(uri, type);
  }
public static <E> ReadableSource<E> asSource(String uri, Class<E> type) {
    return asSource(URI.create(uri), type);
  }

public static <E> Target asTarget(View<E> view) {
    return new DatasetTarget<E>(view);
  }
 public static Target asTarget(String uri) {
    return asTarget(URI.create(uri));
  }
public static Target asTarget(URI uri) {
    return new DatasetTarget<Object>(uri);
  }
}
```

# ♫T0\\ApacheCrunch

Apache Crunch([http://crunch.apache.org](http://crunch.apache.org))是一个 Java 和 Scala 库，用于创建 MapReduce 作业的管道。 它基于谷歌的 FlumeJava([http://dl.acm.org/citation.cfm?id=1806638](http://dl.acm.org/citation.cfm?id=1806638))论文和库。 该项目的目标是通过公开许多实现聚合、联接、过滤和排序记录等操作的模式，使熟悉 Java 编程语言的任何人都能尽可能简单地编写 MapReduce 作业。

与 Pig 等工具类似，Crunch 管道是通过组合不可变的分布式数据结构并在这些结构上运行所有处理操作来创建的；它们被表示和实现为用户定义的函数。 管道被编译成 MapReduce 作业的 DAG，其执行由库的规划者管理。 Crunch 允许我们编写迭代代码，并从映射和归约操作的角度抽象出思考的复杂性，同时避免了对诸如 Pig 拉丁语之类的特殊编程语言的需要。 此外，Crunch 提供了一个高度可定制的类型系统，允许我们处理和混合 Hadoop Writables、HBase 和 Avro 序列化对象。

FlumeJava 的主要假设是，MapReduce 对于几类问题来说是错误的抽象级别，这些问题的计算通常由多个链式作业组成。 出于性能原因，我们经常需要将逻辑上独立的操作(例如，过滤、投影、分组和其他转换)组合到单个物理 MapReduce 作业中。 这一方面对代码的可测试性也有影响。 虽然我们不会在本章中讨论这一方面，但我们鼓励读者通过参考 Crunch 的文档来深入了解它。

## 入门

QuickStart 虚拟机上已经安装了 Crash Jars。 默认情况下，JAR 位于`/opt/cloudera/parcels/CDH/lib/crunch`中。

或者，最近的 Crunch 库可以从[https://crunch.apache.org/download.html](https://crunch.apache.org/download.html)、从 Maven Central 或特定于 Cloudera 的存储库下载。

## 概念

压缩管道由两个抽象组成：`PCollection`和`PTable`。

`PCollection<T>`接口是类型为`T`的对象的分布式不可变集合。 `PTable<Key, Value>`接口是由`Key`类型的键和`Value`类型的值组成的分布式、不可变的哈希表(PCollection 的子接口)，它公开了使用键-值对的方法。

这两个抽象支持以下四个基元操作：

*   `parallelDo`：将用户定义函数`DoFn`应用于给定的`PCollection`，并返回新的`PCollection`
*   `union`：将两个或多个`PCollections`合并为单个虚拟`PCollection`
*   `groupByKey`：按键对`PTable`的元素进行排序和分组
*   `combineValues`：聚合来自`groupByKey`操作的值

[https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/HashtagCount.java](https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/HashtagCount.java)实现了一个 Crunch MapReduce 管道，该管道对出现的散列标签进行计数：

```scala
Pipeline pipeline = new MRPipeline(HashtagCount.class, getConf());

pipeline.enableDebug();

PCollection<String> lines = pipeline.readTextFile(args[0]);

PCollection<String> words = lines.parallelDo(new DoFn<String, String>() {
  public void process(String line, Emitter<String> emitter) {
    for (String word : line.split("\\s+")) {
        if (word.matches("(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)")) {
            emitter.emit(word);
        }
    }
  }
}, Writables.strings());

PTable<String, Long> counts = words.count();

pipeline.writeTextFile(counts, args[1]);
// Execute the pipeline as a MapReduce.
pipeline.done();
```

在本例中，我们首先创建一个`MRPipeline`管道，并使用它首先将用`stream.py -t`创建的`sample.txt`的内容读取到一个字符串集合中，其中该集合的每个元素代表一条 tweet。 我们使用`tweet.split("\\s+")`将每条 tweet 标记为单词，并发出与标签正则表达式匹配的每个单词，序列化为 Writable。 请注意，标记化和过滤操作由`parallelDo`调用创建的 MapReduce 作业并行执行。 我们创建了一个`PTable`，它将每个表示为字符串的 hashtag 与它在数据集中出现的次数关联起来。 最后，我们将`PTable`计数作为文本文件写入 HDFS。 使用`pipeline.done()`执行流水线。

要编译和执行管道，我们可以使用 Gradle 来管理所需的依赖项，如下所示：

```scala
$ ./gradlew jar
$ ./gradlew copyJars

```

将使用`copyJars`下载的 Crunch 和 Avro 依赖项添加到`LIBJARS`环境变量：

```scala
$ export CRUNCH_DEPS=build/libjars/crunch-example/lib
$ export LIBJARS=${LIBJARS},${CRUNCH_DEPS}/crunch-core-0.9.0-cdh5.0.3.jar,${CRUNCH_DEPS}/avro-1.7.5-cdh5.0.3.jar,${CRUNCH_DEPS}/avro-mapred-1.7.5-cdh5.0.3-hadoop2.jar

```

然后，在 Hadoop 上运行示例：

```scala
$ hadoop jar build/libs/crunch-example.jar \
com.learninghadoop2.crunch.HashtagCount \
tweets.json count-out \
-libjars $LIBJARS

```

## 数据序列化

框架的目标之一是使处理包含嵌套和重复数据结构(如协议缓冲区和 Thrift 记录)的复杂记录变得容易。

`org.apache.crunch.types.PType`接口定义了在 Crunch 管道中使用的数据类型与用于从 HDFS 读取数据/向 HDFS 写入数据的序列化和存储格式之间的映射。 每个`PCollection`都有一个关联的`PType`，它告诉 Crunch 如何读/写数据。

`org.apache.crunch.types.PTypeFamily`接口提供抽象工厂来实现共享相同序列化格式的`PType`实例。 目前，Crunch 支持两种类型的系列：一种基于 Writable 接口，另一种基于 Apache Avro。

### 备注

尽管 Crunch 允许在同一管道中混合和匹配使用`PType`的不同实例的`PCollection`接口，但每个`PCollection`接口的`PType`必须属于唯一的系列。 例如，不可能将键序列化为 Writable 并使用 avro 序列化其值的`PTable`。

这两个类型族都支持一组通用的原语类型(字符串、长整型、整型、浮点型、双精度型、布尔型和字节)，以及可以由其他`PTypes`构造的更复杂的`PType`接口。 其中包括其他`PType`的元组和集合。 一个特别重要、复杂的`PType`是`tableOf`，它确定`paralleDo`的返回类型是`PCollection`还是`PTable`。

可以通过继承和扩展 Avro 和 Writable 族的内置内容来创建新的`PTypes`。 这需要实现 Input`MapFn<S, T>`和 Output `MapFn<T, S>`类。 我们为`S`是原始类型而`T`是新类型的实例实现`PType`。

派生的`PTypes`可以在`PTypes`类中找到。 其中包括对协议缓冲区、Thrift 记录、Java Enums、BigInteger 和 UUID 的序列化支持。 我们在[章](06.html "Chapter 6. Data Analysis with Apache Pig")，*使用 Apache Pig 进行数据分析*中讨论的 Elephant Bird 库包含其他示例。

## 数据处理模式

`org.apache.crunch.lib`为常见的数据操作操作实现了许多设计模式。

### 聚合和排序

`org.apache.crunch.lib`提供的大多数数据处理模式依赖于`PTable`的`groupByKey`方法。该方法有三种不同的重载形式：

*   `groupByKey()`：让规划人员确定分区的数量
*   `groupByKey(int numPartitions)`：用于设置开发者指定的分区数量
*   `groupByKey(GroupingOptions options)`：允许我们指定用于混洗的自定义分区和比较器

`org.apache.crunch.GroupingOptions`类采用 Hadoop 的`Partitioner`和`RawComparator`类的实例来实现自定义分区和排序操作。

`groupByKey`方法返回`PGroupedTable`的实例，`PGroupedTable`是 Crunch 对分组表格的表示。 它对应于 MapReduce 作业的混洗阶段的输出，并允许将值与`combineValue`方法组合。

`org.apache.crunch.lib.Aggregate`包公开了对`PCollection`实例执行简单聚合(count、max、top 和 length)的方法。

Sort 提供了一个 API 来对其内容实现`Comparable`接口的`PCollection`和`PTable`实例进行排序。

默认情况下，Crunch 使用一个缩减器对数据进行排序。 可以通过将所需的分区数传递给`sort`方法来修改此行为。 `Sort.Order`方法用信号表示应该进行排序的顺序。

下面是如何为集合指定不同的排序选项：

```scala
public static <T> PCollection<T> sort(PCollection<T> collection)
public static <T> PCollection<T> sort(PCollection<T> collection, Sort.Order order)
public static <T> PCollection<T> sort(PCollection<T> collection, int numReducers,                                       Sort.Order order)
```

下面是如何为表指定不同的排序选项：

```scala
public static <K,V> PTable<K,V> sort(PTable<K,V> table)

public static <K,V> PTable<K,V> sort(PTable<K,V> table, Sort.Order key)
public static <K,V> PTable<K,V> sort(PTable<K,V> table, int numReducers, Sort.Order key)
```

最后，`sortPairs`使用`Sort.ColumnOrder`中指定的列顺序对`PCollection`对进行排序：

```scala
sortPairs(PCollection<Pair<U,V>> collection, Sort.ColumnOrder... columnOrders)
```

### 连接数据

`org.apache.crunch.lib.Join`包是基于公共密钥加入`PTables`的 API。 支持以下四种联接操作：

*   `fullJoin`
*   `join`(默认为`innerJoin`)
*   `leftJoin`
*   `rightJoin`

这些方法具有共同的返回类型和签名。 作为参考，我们将描述实现内部联接的常用`join`方法：

```scala
public static <K,U,V> PTable<K,Pair<U,V>> join(PTable<K,U> left, PTable<K,V> right)
```

`org.apache.crunch.lib.Join.JoinStrategy`包提供了定义自定义联接策略的接口。 Crunch 的默认策略(`defaultStrategy`)是连接数据减少端。

## 管道实施和执行

Crunch 伴随着管道接口的三个实现而来。 本章隐含使用的最旧的是`org.apache.crunch.impl.mr.MRPipeline`，它使用 Hadoop 的 MapReduce 作为其执行引擎。 `org.apache.crunch.impl.mem.MemPipeline`允许在内存中执行所有操作，而不执行到磁盘的序列化。 Crunch 0.10 引入了`org.apache.crunch.impl.spark.SparkPipeline`，它编译并运行 Apache Spark 的 DAG`PCollections`。

### SparkPippeline

使用 SparkPipeline，Crunch 将的大部分执行任务委托给 Spark，并执行相对较少的计划任务，以下例外情况除外：

*   多路输入
*   多路输出
*   数据序列化
*   检查点设置

在撰写本文时，SparkPipeline 仍在大力开发中，可能无法处理标准 MRPipeline 的所有用例。 Crunch 社区正在积极工作，以确保两种实现之间的完全兼容性。

### Pippeline

MemPipeline 在客户机上执行内存中的。 与 MRPipeline 不同，MemPipeline 不是显式创建的，而是通过调用静态方法`MemPipeline.getInstance()`引用的。 所有操作都在内存中，PTypes 的使用非常少。

## 压缩示例

现在我们将使用 Apache Crunch 以更模块化的方式重新实现到目前为止编写的一些 MapReduce 代码。

### 词语共现

在[第 3 章](03.html "Chapter 3. Processing – MapReduce and Beyond")，*Processing-MapReduce and Beyond*中，我们展示了一个 MapReduce 作业 BiGramCount，用于计算 tweet 中单词的共现次数。 同样的逻辑可以实现为`DoFn`。 使用 Crunch，我们可以使用复杂类型`Pair<String, String>`，而不是发出多字段键并在稍后阶段对其进行解析，如下所示：

```scala
class BiGram extends DoFn<String, Pair<String, String>> {
    @Override
    public void process(String tweet, 
Emitter<Pair<String, String>> emitter) {
        String[] words = tweet.split(" ") ;

        Text bigram = new Text();
        String prev = null;

        for (String s : words) {
          if (prev != null) {
              emitter.emit(Pair.of(prev, s));
            }       
            prev = s;
        }
    }   
}       
```

请注意，与 MapReduce 相比，`BiGram`Crunch 实现是一个独立的类，可以在任何其他代码库中轻松重用。 此示例的代码包含在[https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/DataPreparationPipeline.java](https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/DataPreparationPipeline.java)中。

### TF-IDF

我们可以使用`MRPipeline`实现 TF-IDF 作业链，如下所示：

```scala
public class CrunchTermFrequencyInvertedDocumentFrequency 
         extends Configured implements Tool, Serializable {

   private Long numDocs;

   @SuppressWarnings("deprecation")

   public static class TF {
        String term;
        String docId;
        int frequency;

        public TF() {}

        public TF(String term, 
               String docId, Integer frequency) {
           this.term = term;
           this.docId = docId;
           this.frequency = (int) frequency;

        }
   }

   public int run(String[] args) throws Exception {
       if(args.length != 2) {
         System.err.println();
         System.err.println("Usage: " + this.getClass().getName() + " [generic options] input output");

         return 1;
       }
       // Create an object to coordinate pipeline creation and execution.
       Pipeline pipeline = 
new MRPipeline(TermFrequencyInvertedDocumentFrequency.class, getConf());

       // enable debug options
       pipeline.enableDebug();

       // Reference a given text file as a collection of Strings.
       PCollection<String> tweets = pipeline.readTextFile(args[0]);
       numDocs = tweets.length().getValue();

       // We use Avro reflections to map the TF POJO to avsc 
       PTable<String, TF> tf = tweets.parallelDo(new TermFrequencyAvro(), Avros.tableOf(Avros.strings(), Avros.reflects(TF.class)));

       // Calculate DF
       PTable<String, Long> df = Aggregate.count(tf.parallelDo( new DocumentFrequencyString(), Avros.strings()));

       // Finally we calculate TF-IDF 
       PTable<String, Pair<TF, Long>> tfDf = Join.join(tf, df);
       PCollection<Tuple3<String, String, Double>> tfIdf = tfDf.parallelDo(new TermFrequencyInvertedDocumentFrequency(),
                Avros.triples(
                      Avros.strings(), 
                      Avros.strings(), 
                      Avros.doubles()));

       // Serialize as avro 
       tfIdf.write(To.avroFile(args[1]));

       // Execute the pipeline as a MapReduce.
       PipelineResult result = pipeline.done();
       return result.succeeded() ? 0 : 1;
   }
   …
}
```

与流式传输相比，我们在这里遵循的方法具有许多优势。 首先，我们不需要使用单独的脚本手动链接 MapReduce 作业。 这项任务是 Crunch 的主要目的。 其次，我们可以将度量的每个组件表示为不同的类，使其更容易在未来的应用中重用。

为了实现词频，我们创建了一个`DoFn`类，它接受 tweet 作为输入并发出`Pair<String, TF>`。 第一个元素是一个术语，第二个元素是将使用 avro 序列化的 POJO 类的实例。 `TF`部分包含三个变量： `term`、`documentId`和`frequency`。 在引用实现中，我们希望输入数据是我们反序列化和解析的 JSON 字符串。 我们还将标记化作为 Process 方法的一个子任务。

根据用例的不同，我们可以分别在和`DoFns`中抽象这两个操作，如下所示：

```scala
class TermFrequencyAvro extends DoFn<String,Pair<String, TF>> {
    public void process(String JSONTweet, 
Emitter<Pair <String, TF>> emitter) {
        Map<String, Integer> termCount = new HashMap<>();

        String tweet;
        String docId;

        JSONParser parser = new JSONParser();

        try {
            Object obj = parser.parse(JSONTweet);

            JSONObject jsonObject = (JSONObject) obj;

            tweet = (String) jsonObject.get("text");
            docId = (String) jsonObject.get("id_str");

            for (String term : tweet.split("\\s+")) {
                if (termCount.containsKey(term.toLowerCase())) {
                    termCount.put(term, 
termCount.get(term.toLowerCase()) + 1);
                } else {
                    termCount.put(term.toLowerCase(), 1);
                }
            }

            for (Entry<String, Integer> entry : termCount.entrySet()) {
                emitter.emit(Pair.of(entry.getKey(), new TF(entry.getKey(), docId, entry.getValue())));
            }
        } catch (ParseException e) {
            e.printStackTrace();
        }
    }
  }
}
```

文档频率很简单。 对于在项频率步骤中生成的每个`Pair<String, TF>`，我们发出项-该对的第一个元素。 我们汇总并计算得到的术语的`PCollection`，以获得文档频率，如下所示：

```scala
class DocumentFrequencyString extends DoFn<Pair<String, TF>, String> {
@Override
   public void process(Pair<String, TF> tfAvro,
      Emitter<String> emitter) {
      emitter.emit(tfAvro.first());
   }
}
```

最后，我们将共享密钥(Term)上的`PTable`TF 与`PTable`DF 连接起来，并将得到的`Pair<String, Pair<TF, Long>>`对象提供给`TermFrequencyInvertedDocumentFrequency`。

对于每个术语和文档，我们计算 TF-IDF 并返回`term`、`docIf`和`tfIdf`三元组：

```scala
   class TermFrequencyInvertedDocumentFrequency extends MapFn<Pair<String, Pair<TF, Long>>, Tuple3<String, String, Double> >  {      
      @Override
      public Tuple3<String, String, Double> map(
            Pair<String, Pair<TF, Long>> input) {

         Pair<TF, Long> tfDf = input.second();
         Long df = tfDf.second();

         TF tf = tfDf.first();
         double idf = 1.0+Math.log(numDocs / df);
         double tfIdf = idf * tf.frequency;

         return  Tuple3.of(tf.term, tf.docId, tfIdf);
      }

   }   
```

我们使用`MapFn`，因为我们将为每个输入输出一条记录。 本例的源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/CrunchTermFrequencyInvertedDocumentFrequency.java](https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/CrunchTermFrequencyInvertedDocumentFrequency.java)找到。

可以使用以下命令编译和执行该示例：

```scala
$ ./gradlew jar
$ ./gradlew copyJars

```

如果尚未完成，请将使用`copyJars`下载的 Crunch 和 Avro 依存关系添加到`LIBJARS`环境变量，如下所示：

```scala
$ export CRUNCH_DEPS=build/libjars/crunch-example/lib
$ export LIBJARS=${LIBJARS},${CRUNCH_DEPS}/crunch-core-0.9.0-cdh5.0.3.jar,${CRUNCH_DEPS}/avro-1.7.5-cdh5.0.3.jar,${CRUNCH_DEPS}/avro-mapred-1.7.5-cdh5.0.3-hadoop2.jar

```

此外，将`json-simple`JAR 添加到`LIBJARS`：

```scala
$ export LIBJARS=${LIBJARS},${CRUNCH_DEPS}/json-simple-1.1.1.jar

```

最后，将`CrunchTermFrequencyInvertedDocumentFrequency`作为 MapReduce 作业运行，如下所示：

```scala
$ hadoop jar build/libs/crunch-example.jar \
com.learninghadoop2.crunch.CrunchTermFrequencyInvertedDocumentFrequency  \
-libjars ${LIBJARS} \
tweets.json tweets.avro-out

```

## 风筝睡眠线

Kite Morphines 是一个数据转换库，灵感来自 Unix 管道，最初是作为 Cloudera 搜索的一部分开发的。 变形线是内存中的转换命令链，它依赖插件结构来利用异构数据源。 它使用声明性命令对记录执行 ETL 操作。 命令在配置文件中定义，该文件稍后将提供给驱动程序类。

其目标是通过提供一个允许开发人员用一系列配置设置替换编程的库，使将 ETL 逻辑嵌入到任何 Java 代码库中成为一项微不足道的任务。

### 概念

形态线是围绕两个抽象构建的：`Command`和`Record`。

记录是`org.kitesdk.morphline.api.Record`接口的实现：

```scala
public final class Record {  
  private ArrayListMultimap<String, Object> fields;  
…
    private Record(ArrayListMultimap<String, Object> fields) {…}
  public ListMultimap<String, Object> getFields() {…}
  public List get(String key) {…}
  public void put(String key, Object value) {…}
   …
}
```

记录是一组个命名字段，其中每个字段都有一个包含一个或多个值的列表。 `Record`是在 Google Guava 的`ListMultimap`和`ArrayListMultimap`类之上实现的。 请注意，值可以是任何 Java 对象，字段可以是多值的，并且两条记录不需要使用公共字段名。 记录可以包含`_attachment_body`字段，该字段可以是`java.io.InputStream`或字节数组。

命令实现`org.kitesdk.morphline.api.Command`接口：

```scala
public interface Command {
   void notify(Record notification);
   boolean process(Record record);
   Command getParent();
}
```

命令将记录转换为零个或多个记录。 命令可以调用为读写操作以及添加或删除字段提供的`Record`实例上的方法。

命令被链接在一起，在变形线的每一步，父命令将记录发送给子命令，子命令继而处理这些记录。 父母和孩子之间使用两个通信通道(平面)交换信息；通知通过控制平面发送，记录通过数据平面发送。 记录由`process()`方法处理，该方法返回一个布尔值以指示是否应该继续进行变形线。

命令不是直接实例化的，而是通过实现`org.kitesdk.morphline.api.CommandBuilder`接口来实例化的：

```scala
public interface CommandBuilder {
   Collection<String> getNames();
   Command build(Config config, 
      Command parent, 
      Command child, 
      MorphlineContext context);
}
```

`getNames`方法返回可用于调用命令的名称。 支持多个名称以允许向后兼容名称更改。 `build()`方法创建并返回一个以给定的变形线配置为根的命令。

`org.kitesdk.morphline.api.MorphlineContext`接口允许将附加参数传递给所有 Morphline 命令。

条形态线的数据模型是按照源-管-宿模式构建的，在这种模式下，从源捕获数据，通过多个处理步骤通过管道传输数据，然后将其输出传送到宿中。

### Morphline 命令

Kite Morphines 附带了许多默认命令，这些命令实现了常见序列化格式(纯文本、Avro、JSON)的数据转换。 当前可用的命令组织为变形线的子项目，包括：

*   `kite-morphlines-core-stdio`：将从二进制大型对象(BLOB)和文本读取数据
*   `kite-morphlines-core-stdlib`：包装用于数据操作和表示的 Java 数据类型
*   `kite-morphlines-avro`：是，用于序列化和反序列化 avro 格式的数据
*   `kite-morphlines-json`：将序列化和反序列化 JSON 格式的数据
*   `kite-morphlines-hadoop-core`：是否用于访问 HDFS
*   `kite-morphlines-hadoop-parquet-avro`：是，用于序列化和反序列化 Parquet 格式的数据
*   `kite-morphlines-hadoop-sequencefile`：用于序列化和反序列化 Sequencefile 格式的数据
*   `kite-morphlines-hadoop-rcfile`：使用序列化和反序列化 RC 文件格式的数据

所有可用命令的列表可在[http://kitesdk.org/docs/0.17.0/kite-morphlines/morphlinesReferenceGuide.html](http://kitesdk.org/docs/0.17.0/kite-morphlines/morphlinesReferenceGuide.html)中找到。

命令是通过在配置文件`morphline.conf`中声明一系列转换来定义的，然后由驱动程序编译并执行该配置文件。 例如，我们可以指定一个`read_tweets`变形行，它将加载存储为 JSON 数据的 tweet，使用 Jackson 序列化和反序列化它们，并通过组合`org.kitesdk.morphline`包中包含的默认`readJson`和`head`命令打印前 10 个，如下所示：

```scala
morphlines : [{
  id : read_tweets
  importCommands : ["org.kitesdk.morphline.**"]

  commands : [{
    readJson {
      outputClass : com.fasterxml.jackson.databind.JsonNode
    }}
    {
      head { 
      limit : 10
    }}
  ]
}]
```

现在，我们将展示如何从独立的 Java 程序和 MapReduce 执行此变形线。

`MorphlineDriver.java`显示了如何使用嵌入到主机系统中的库。 我们在 `main`方法中执行的第一步是加载 Morphline 的 JSON 配置，构建一个`MorphlineContext`对象，并将其编译成`Command`的实例，该实例充当 Morphline 的起始节点。 请注意，`Compiler.compile()`接受一个`finalChild`参数；在本例中，它是`RecordEmitter`。 我们使用`RecordEmitter`作为形态线的接收器，要么将记录打印到 stdout，要么将其存储到 HDFS 中。 在`MorphlineDriver`示例中，我们使用`org.kitesdk.morphline.base.Notifications`以事务的方式管理和监视 Morphline 生命周期。

调用`Notifications.notifyStartSession(morphline)`将在通过调用`Notifications.notifyBeginTransaction`定义的事务内启动转换链。 成功后，我们使用`Notifications.notifyShutdown(morphline)`终止管道。 在失败的情况下，我们回滚事务`Notifications.notifyRollbackTransaction(morphline)`，并将异常处理程序从变形行上下文传递到调用 Java 代码：

```scala
public class MorphlineDriver {
    private static final class RecordEmitter implements Command {
       private final Text line = new Text();

      @Override
      public Command getParent() {
         return null;
      }

      @Override
      public void notify(Record record) {

      }

      @Override
      public boolean process(Record record) {
         line.set(record.get("_attachment_body").toString());

         System.out.println(line);

         return true;
      }
       }  

   public static void main(String[] args) throws IOException {
       /* load a morphline conf and set it up */
       File morphlineFile = new File(args[0]);
       String morphlineId = args[1];
       MorphlineContext morphlineContext = new MorphlineContext.Builder().build();
       Command morphline = new Compiler().compile(morphlineFile, morphlineId, morphlineContext, new RecordEmitter());

       /* Prepare the morphline for execution
        * 
        * Notifications are sent through the communication channel  
        * */

       Notifications.notifyBeginTransaction(morphline);

       /* Note that we are using the local filesystem, not hdfs*/
       InputStream in = new BufferedInputStream(new FileInputStream(args[2]));

       /* fill in a record and pass  it over */
       Record record = new Record();
       record.put(Fields.ATTACHMENT_BODY, in); 

       try {

            Notifications.notifyStartSession(morphline);
            boolean success = morphline.process(record);
            if (!success) {
              System.out.println("Morphline failed to process record: " + record);
            }
        /* Commit the morphline */
       } catch (RuntimeException e) {
           Notifications.notifyRollbackTransaction(morphline);
           morphlineContext.getExceptionHandler().handleException(e, null);
         }
       finally {
            in.close();
        }

        /* shut it down */
        Notifications.notifyShutdown(morphline);     
    }
}
```

在本例中，我们将 JSON 格式的数据从本地文件系统加载到一个`InputStream`对象中，并使用它来初始化一个新的`Record`实例。 `RecordEmitter`类包含链的最后一个处理的记录实例，我们在该实例上提取`_attachment_body`并将其打印到标准输出。 `MorphlineDriver`的源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch9/kite/src/main/java/com/learninghadoop2/kite/morphlines/MorphlineDriver.java](https://github.com/learninghadoop2/book-examples/blob/master/ch9/kite/src/main/java/com/learninghadoop2/kite/morphlines/MorphlineDriver.java)中找到。

使用 MapReduce 作业中相同的变形线非常简单。 在 Mapper 的设置阶段，我们构建一个包含实例化逻辑的上下文，而 map 方法设置`Record`对象并触发处理逻辑，如下所示：

```scala
public static class ReadTweets
        extends Mapper<Object, Text, Text, NullWritable> {
    private final Record record = new Record();
    private Command morphline;

    @Override
    protected void setup(Context context)
            throws IOException, InterruptedException {
        File morphlineConf = new File(context.getConfiguration()
                .get(MORPHLINE_CONF));
        String morphlineId = context.getConfiguration()
                .get(MORPHLINE_ID);
        MorphlineContext morphlineContext = 
new MorphlineContext.Builder()
                .build();

        morphline = new org.kitesdk.morphline.base.Compiler()
                .compile(morphlineConf,
                        morphlineId,
                        morphlineContext,
                        new RecordEmitter(context));
    }

    public void map(Object key, Text value, Context context)
            throws IOException, InterruptedException {
        record.put(Fields.ATTACHMENT_BODY,
                new ByteArrayInputStream(
value.toString().getBytes("UTF8")));
        if (!morphline.process(record)) {
              System.out.println(
"Morphline failed to process record: " + record);
        }

        record.removeAll(Fields.ATTACHMENT_BODY);
    }
}
```

在 MapReduce 代码中，我们修改了`RecordEmitter`以从后处理的记录中提取`Fields`有效负载，并将其存储到上下文中。 这允许我们通过在 MapReduce 配置样板中指定`FileOutputFormat`将数据写入 HDFS：

```scala
private static final class RecordEmitter implements Command {
    private final Text line = new Text();
    private final Mapper.Context context;

    private RecordEmitter(Mapper.Context context) {
        this.context = context;
    }

    @Override
    public void notify(Record notification) {
    }

    @Override
    public Command getParent() {
        return null;
    }

    @Override
    public boolean process(Record record) {
        line.set(record.get(Fields.ATTACHMENT_BODY).toString());
        try {
            context.write(line, null);
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
        return true;
    }
}   
```

请注意，我们现在可以通过修改`morphline.conf`来更改处理管道行为并添加进一步的数据转换，而无需明确更改实例化和处理逻辑。 MapReduce 驱动程序源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch9/kite/src/main/java/com/learninghadoop2/kite/morphlines/MorphlineDriverMapReduce.java](https://github.com/learninghadoop2/book-examples/blob/master/ch9/kite/src/main/java/com/learninghadoop2/kite/morphlines/MorphlineDriverMapReduce.java)中找到。

这两个示例都可以使用以下命令从`ch9/kite/`编译：

```scala
$ ./gradlew jar
$ ./gradlew copyJar

```

我们将`runtime`依赖项添加到`LIBJARS`，如下所示

```scala
$ export KITE_DEPS=/home/cloudera/review/hadoop2book-private-reviews-gabriele-ch8/src/ch8/kite/build/libjars/kite-example/lib
export LIBJARS=${LIBJARS},${KITE_DEPS}/kite-morphlines-core-0.17.0.jar,${KITE_DEPS}/kite-morphlines-json-0.17.0.jar,${KITE_DEPS}/metrics-core-3.0.2.jar,${KITE_DEPS}/metrics-healthchecks-3.0.2.jar,${KITE_DEPS}/config-1.0.2.jar,${KITE_DEPS}/jackson-databind-2.3.1.jar,${KITE_DEPS}/jackson-core-2.3.1.jar,${KITE_DEPS}/jackson-annotations-2.3.0.jar

```

我们可以使用以下内容运行 MapReduce 驱动程序：

```scala
$ hadoop jar build/libs/kite-example.jar \
com.learninghadoop2.kite.morphlines.MorphlineDriverMapReduce \
-libjars ${LIBJARS} \
morphline.conf \
read_tweets \
tweets.json \
morphlines-out

```

可以使用以下命令执行 Java 独立驱动程序：

```scala
$ export CLASSPATH=${CLASSPATH}:${KITE_DEPS}/kite-morphlines-core-0.17.0.jar:${KITE_DEPS}/kite-morphlines-json-0.17.0.jar:${KITE_DEPS}/metrics-core-3.0.2.jar:${KITE_DEPS}/metrics-healthchecks-3.0.2.jar:${KITE_DEPS}/config-1.0.2.jar:${KITE_DEPS}/jackson-databind-2.3.1.jar:${KITE_DEPS}/jackson-core-2.3.1.jar:${KITE_DEPS}/jackson-annotations-2.3.0.jar:${KITE_DEPS}/slf4j-api-1.7.5.jar:${KITE_DEPS}/guava-11.0.2.jar:${KITE_DEPS}/hadoop-common-2.3.0-cdh5.0.3.jar
$ java -cp $CLASSPATH:./build/libs/kite-example.jar \
com.learninghadoop2.kite.morphlines.MorphlineDriver \
morphline.conf \
read_tweets tweets.json \
morphlines-out

```

# 摘要

在本章中，我们介绍了四个简化 Hadoop 开发的工具。 我们特别介绍了以下内容：

*   Hadoop Streaming 如何允许使用动态语言编写 MapReduce 作业
*   Kite Data 如何简化与异构数据源的接口
*   Apache Crunch 如何提供高级抽象来编写实现通用设计模式的 Spark 和 MapReduce 作业的管道
*   Morphline 如何允许我们声明命令和数据转换链，然后这些命令和数据转换可以嵌入到任何 Java 代码库中

在[第 10 章](10.html "Chapter 10. Running a Hadoop Cluster")，*运行 Hadoop2 集群*中，我们将把重点从软件开发领域转移到系统管理上。 我们将讨论如何设置、管理和扩展 Hadoop 集群，同时考虑到监控和安全性等方面。