# 五、使用 Spark 的迭代计算

在上一章中，我们了解了 Samza 如何在 Hadoop 中实现近乎实时的流数据处理。 这与传统的 MapReduce 批处理模型相去甚远，但仍然符合提供定义良好的接口以实现业务逻辑任务的模型。 在这一章中，我们将探讨 Apache Spark，它既可以被视为构建应用的框架，也可以被视为本身的处理框架。 不仅应用构建在 Spark 之上，Hadoop 生态系统中的整个组件也在重新实现，以使用 Spark 作为其底层处理框架。 我们将特别介绍以下主题：

*   Spark 是什么？它的核心系统如何在 Yarn 上运行
*   Spark 提供的数据模型，可实现高度可伸缩和高效的数据处理
*   其他 Spark 组件和相关项目的广度

需要注意的是，尽管 Spark 有自己的处理流数据的机制，但这只是 Spark 提供的一部分。 最好把它看作是一个更广泛的倡议。

# ==___ _

ApacheSpark([MapReduce](https://spark.apache.org/))是一个基于 https://spark.apache.org/的泛化的数据处理框架。 它最初是由加州大学伯克利分校的 AMP 实验室开发的([https://amplab.cs.berkeley.edu/](https://amplab.cs.berkeley.edu/))。 与 TEZ 一样，Spark 充当一个执行引擎，将数据转换建模为 DAG，并努力消除 MapReduce 的 I/O 开销，以便在规模上执行迭代计算。 虽然 Tez 的主要目标是为 Hadoop 上的 MapReduce 提供一个更快的执行引擎，但是 Spark 已经被设计为一个独立的框架和一个用于应用开发的 API。 该系统设计用于执行通用内存数据处理、流工作流以及交互和迭代计算。

Spark 是在 Scala 中实现的，Scala 是一种用于 Java VM 的静态类型编程语言，除了 Scala 本身之外，它还公开了 Java 和 Python 的本机编程接口。 请注意，尽管 Java 代码可以直接调用 Scala 接口，但类型系统的某些方面使得此类代码相当笨拙，因此我们使用本机 Java API。

Scala 附带了一个类似于 Ruby 和 Python 的交互式 shell；这允许用户从解释器交互运行 Spark 来查询任何数据集。

Scala 解释器的操作方式是为用户键入的每一行编译一个类，将其加载到 JVM 中，并在其上调用一个函数。 此类包括一个单例对象，该对象包含该行上的变量或函数，并在初始化方法中运行该行的代码。 除了其丰富的编程接口之外，Spark 正逐渐成为一个执行引擎，Hadoop 生态系统的流行工具(如 Pig 和 Have)被移植到框架中。

## 使用工作集的集群计算

Spark 的架构以**弹性分布式数据集**(**RDDS**)的概念为中心，这是一组 Scala 对象的只读集合，这些 Scala 对象被分区到一组可以持久存储在内存中的机器上。 这一抽象是在 2012 年的一篇研究论文*弹性分布式数据集：内存中集群计算的容错抽象*中提出的，可在[https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf](https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf)找到。

Spark 应用由一个驱动程序组成，该驱动程序在一群工作进程和长期进程上执行并行操作，这些进程可以通过调度作为并行任务运行的函数在内存中存储数据分区，如下图所示：

![Cluster computing with working sets](img/5518OS_05_01.jpg)

星火集群体系结构

流程通过 SparkContext 实例进行协调。 SparkContext 连接到资源管理器(比如 Yarn)，请求工作节点上的执行器，并发送要执行的任务。 执行器负责在本地运行任务和管理内存。

Spark 允许您使用称为共享变量的抽象在任务之间或任务与驱动程序之间共享变量。 Spark 支持两种类型的共享变量：广播变量和累加器，广播变量可用于在所有节点的内存中缓存值，累加器是附加变量，如计数器和总和。

### 弹性分布式数据集(RDDS)

RDD 存储在内存中，跨机器共享，并用于类似 MapReduce 的并行操作。 容错是通过*世系*的概念实现的：如果 RDD 的一个分区丢失，则 RDD 有足够的信息说明它是如何从其他 RDD 派生出来的，从而能够仅重建该分区。 RDD 可以通过四种方式构建：

*   通过从存储在 HDFS 中的文件读取数据
*   通过将 Scala 集合划分-并行化-将其划分为多个分区，然后将这些分区发送给工作者
*   通过使用并行运算符转换现有 RDD
*   通过更改现有 RDD 的持久性

当 RDDS 可以放入内存中并且可以跨操作缓存时，Spark 就会闪耀光芒。 API 公开了方法来持久化 RDDS，并允许几种持久化策略和存储级别，从而允许溢出到磁盘以及节省空间的二进制序列化。

### 操作

操作通过将函数传递给 Spark 来调用。 该系统根据函数式编程范例处理变量和副作用。 闭包可以引用创建它们的作用域中的变量。 操作的示例有`count`(返回数据集中的元素数量)和`save`(将数据集输出到存储)。 在 RDDS 上的其他并行操作包括：

*   `map`：将函数应用于数据集的每个元素
*   `filter`：根据用户提供的条件从数据集中选择元素
*   `reduce`：使用关联函数组合数据集元素
*   `collect`：将数据集的所有元素发送到驱动程序
*   `foreach`：通过用户提供的函数传递每个元素
*   `groupByKey`：按提供的键将项目分组在一起
*   `sortByKey`：按键对项目进行排序

## 部署

Spark 既可以在本地模式下运行，类似于 Hadoop 单节点设置，也可以在资源管理器上运行。 当前支持的资源管理器包括：

*   Spark 独立集群模式
*   纺 Yarn / 奇谈 / 闲聊
*   ApacheMesos

### Yarn 上的火花

为了在 Yarn 上部署 Spark，需要构建一个临时合并 JAR。 Spark 在 ResourceManager 中启动独立部署的集群的一个实例。 Cloudera 和 MapR 都将 Spark on Sink 作为其软件分发的一部分。 在撰写本文时，Spark 已经作为技术预览版提供给 Hortonworks 的 HDP([http://hortonworks.com/hadoop/spark/](http://hortonworks.com/hadoop/spark/))。

### EC2 上的火花

Spark 附带了位于`ec2`目录中的部署脚本`spark-ec2`。 此脚本自动在 EC2 实例集群上设置 Spark 和 HDFS。 要在 Amazon 云上启动 Spark 集群，请转到`ec2`目录并运行以下命令：

```scala
./spark-ec2 -k <keypair> -i <key-file> -s <num-slaves> launch <cluster-name>

```

这里，`<keypair>`是 EC2 密钥对的名称，`<key-file>`是密钥对的私钥文件，`<num-slaves>`是要启动的从节点数，`<cluster-name>`是要赋予集群的名称。 有关密钥对设置的更多详细信息，请参见[第 1 章](01.html "Chapter 1. Introduction")，*简介*，并通过转到集群调度程序的 Web UI(脚本完成后将打印其地址)来验证集群调度程序是否已启动并看到所有从属程序。

您可以通过形式为`s3n://<bucket>/path`的 URI 指定 S3 中的路径作为输入。 您还需要设置 Amazon 安全凭据，方法是在执行程序之前设置环境变量`AWS_ACCESS_KEY_ID`和`AWS_SECRET_ACCESS_KEY`，或者通过`SparkContext.hadoopConfiguration`。

## Spark 入门

Spark 二进制文件和源代码可以在项目网站[http://spark.apache.org/](http://spark.apache.org/)上找到。 以下部分中的示例使用 Cloudera CDH5.0 QuickStart VM 上从源代码构建的 Spark 1.1.0 进行了测试。

使用以下命令下载并解压缩`gzip`档案：

```scala
$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.1.0.tgz 
$ tar xvzf spark-1.1.0.tgz
$ cd spark-1.1.0

```

Spark 在 Scala2.10 上构建，并使用`sbt`([https://github.com/sbt/sbt](https://github.com/sbt/sbt))构建源代码核心和相关示例：

```scala
$ ./sbt/sbt -Dhadoop.version=2.2.0  -Pyarn  assembly

```

使用`-Dhadoop.version=2.2.0`和`-Pyarn`选项，我们指示`sbt`针对 Hadoop 版本 2.2.0 或更高版本进行构建，并启用 Yarn 支持。

使用以下命令在独立模式下启动 Spark：

```scala
$ ./sbin/start-all.sh 

```

该命令将在`spark://localhost:7077`启动一个本地主实例以及一个工作节点。

可以在`http://localhost:8080/`访问到主节点的 Web 界面，如以下屏幕截图所示：

![Getting started with Spark](img/5518OS_05_02.jpg)

主节点 Web 界面

Spark 可以通过`spark-shell`交互运行，`spark-shell`是 Scala shell 的修改版本。 作为第一个示例，我们将使用 Scala API 对我们在[第 3 章](03.html "Chapter 3. Processing – MapReduce and Beyond")、*处理-MapReduce 以及之后的*中使用的 Twitter 数据集进行字数统计。

通过运行以下命令启动交互式`spark-shell`会话：

```scala
$ ./bin/spark-shell

```

外壳程序实例化一个`SparkContext`对象`sc`，该对象负责处理到工作进程的驱动程序连接。 我们将在本章后面描述它的语义。

为简单起见，让我们创建一个示例文本数据集，该数据集每行包含一个状态更新：

```scala
$ stream.py -t -n 1000 > sample.txt

```

然后，将其复制到 HDFS：

```scala
$ hdfs dfs -put sample.txt /tmp

```

在`spark-shell`内，我们首先从样本数据创建一个`RDD - file`-：

```scala
val file = sc.textFile("/tmp/sample.txt")

```

然后，我们应用一系列转换来计算文件中出现的单词。 请注意，转换链`counts`的输出仍然是 RDD：

```scala
val counts = file.flatMap(line => line.split(" "))
.map(word => (word, 1))
.reduceByKey((m, n) => m + n)  
```

此转换链对应于我们熟悉的映射和缩减阶段。 在映射阶段，我们加载数据集的每一行(`flatMap`)，将每条 tweet 标记为一个单词序列，计算每个单词(`map`)的出现次数，并发出(`key, value`)对。 在 Reduce 阶段，我们按键(`word`)和总和值(`m, n`)分组以获得字数。

最后，我们将前个元素`counts.take(10)`打印到控制台：

```scala
counts.take(10).foreach(println)
```

## 编写和运行独立应用

Spark 允许使用三个 API 编写独立应用：Scala、Java 和 Python。

### _API 比例

Spark 驱动程序必须做的第一件事是创建一个`SparkContext`对象，该对象告诉 Spark 如何访问集群。 将类和隐式转换导入程序后，如下所示：

```scala
import org.apache.spark.SparkContext 
import org.apache.spark.SparkContext._
```

可以使用以下构造函数创建`SparkContext`对象：

```scala
new SparkContext(master, appName, [sparkHome]) 
```

也可以通过接受`SparkConf`对象的`SparkContext(conf)`创建它。

主参数是指定要连接到的集群 URI 的字符串(如`spark://localhost:7077`)或要在本地模式下运行的 `local`字符串。 `appName`术语是将在集群 Web 用户界面中显示的应用名称。

不可能覆盖默认的`SparkContext`类，也不可能在运行的 Spark shell 中创建新的类。 但是，可以使用`MASTER`环境变量指定上下文连接到哪个主服务器。 例如，要在四个内核上运行 `spark-shell`，请使用以下命令：

```scala
$ MASTER=local[4] ./bin/spark-shell 

```

### Колибриобработает

`org.apache.spark.api.java`包向 Java 公开了 Scala 版本中所有可用的 Spark 特性。 Java API 有一个`JavaSparkContext`类，它返回`org.apache.spark.api.java.JavaRDD`的实例，并且使用 Java 集合而不是 Scala 集合。

Java 和 Scala API 之间有几个主要区别：

*   Java7 不支持匿名函数或一级函数；因此，必须通过扩展`org.apache.spark.api.java.function.Function`、`Function2`和其他类来实现函数。 从 Spark Version 1.0 开始，API 已经进行了重构，以支持 Java8lambda 表达式。 在 Java8 中，函数类可以替换为内联表达式，这些表达式充当匿名函数的简写。
*   RDD 方法返回 Java 集合
*   键-值对在 Scala 中简称为(`key`，`value`)，由`scala.Tuple2`类表示。
*   为了维护类型安全，一些 RDD 和函数方法(如处理密钥对和 Double 的方法)被实现为专用的类。

### Java 中的字数统计

在`examples/src/main/java/org/apache/spark/examples/JavaWordCount.java`处的 Spark 源代码发行版中包含了一个 Java 中的字数计数示例。

首先，我们使用`JavaSparkContext`类创建上下文：

```scala
   JavaSparkContext sc = new JavaSparkContext(master, "JavaWordCount",
     System.getenv("SPARK_HOME"), JavaSparkContext.jarOfClass(JavaWordCount.class));

    JavaRDD<String> data = sc.textFile(infile, 1);
    JavaRDD<String> words = data.flatMap(new FlatMapFunction<String, String>() {
      @Override
      public Iterable<String> call(String s) {
        return Arrays.asList(s.split(" "));
      }
    });

    JavaPairRDD<String, Integer> ones = words.map(new PairFunction<String, String, Integer>() {
      @Override
      public Tuple2<String, Integer> call(String s) {
        return new Tuple2<String, Integer>(s, 1);
      }
    });

    JavaPairRDD<String, Integer> counts = ones.reduceByKey(new Function2<Integer, Integer, Integer>() {
      @Override
      public Integer call(Integer i1, Integer i2) {
        return i1 + i2;
      }
    });
```

然后，我们从 HDFS 位置`infile`构建 RDD。 在转换链的第一步中，我们对数据集中的每个 tweet 进行标记化，并返回一个单词列表。 我们使用`JavaPairRDD<String, Integer>`的实例来计算每个单词出现的次数。 最后，我们将 RDD 简化为一个新的`JavaPairRDD<String, Integer>`实例，该实例包含一个元组列表，每个表示一个单词及其在数据集中被发现的次数。

### Python API

PySpark 需要 Python 版本 2.6 或更高版本。 RDDS 支持与 Scala 对应的相同方法，但采用 Python 函数并返回 Python 集合类型。 Lambda 语法([https://docs.python.org/2/reference/expressions.html](https://docs.python.org/2/reference/expressions.html))用于将函数传递给 RDDS。

`pyspark`中的字数与其对应的 Scala 中的字数相对相似：

```scala
tweets = sc.textFile("/tmp/sample.txt")
counts = tweets.flatMap(lambda tweet: tweet.split(' ')) \
                  .map(lambda word: (word, 1)) \
                  .reduceByKey(lambda m,n:m+n)
```

`lambda`构造在运行时创建匿名函数。 `lambda tweet: tweet.split(' ')`创建一个函数，该函数接受字符串`tweet`作为输入，并输出由空格分隔的字符串列表。 Spark 的`flatMap`将此函数应用于`tweets`数据集的每一行。 在`map`阶段，对于每个`word`标记，`lambda word: (word, 1)`返回指示数据集中出现单词的`(word, 1)`个元组。 在`reduceByKey`中，我们按关键字对这些元组进行分组，然后将这些值相加，以获得与`lambda m,n:m+n`的字数。

# 星火生态系统

Apache Spark 支持许多工具，既可以作为库，也可以作为执行引擎。

## 火花流

SparkStreaming(位于[Scala](http://spark.apache.org/docs/latest/streaming-programming-guide.html))是 http://spark.apache.org/docs/latest/streaming-programming-guide.html API 的扩展，允许从 Kafka、Flume、Twitter、ZeroMQ 和 TCPSocket 等流获取数据。

Spark Streaming 接收实时输入数据流，并将数据分成批(任意大小的时间窗口)，然后由 Spark 核心引擎处理，以批量生成最终结果流。 这种高级抽象称为 DStream(`org.apache.spark.streaming.dstream.DStreams`)，并以 RDDS 序列的形式实现。 DStream 允许两种操作：*转换*和*输出操作*。 转换作用于一个或多个 DStream 以创建新的 DStream。 作为转换链的一部分，数据可以持久化到存储层(HDFS)或输出通道。 火花流允许在数据的滑动窗口上进行转换。 基于窗口的操作需要指定两个参数：窗口长度、窗口持续时间和滑动间隔，即执行基于窗口的操作的间隔。

## GraphX

GraphX(位于[Pregel](https://spark.apache.org/docs/latest/graphx-programming-guide.html))是一种用于图计算的 API，它公开了一组用于面向图的计算的运算符和算法，以及 https://spark.apache.org/docs/latest/graphx-programming-guide.html 的优化变体。

## 帖子主题：Re：Колибри

MLlib(位于[http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html))提供常见的**机器学习**(**ML**)功能，包括测试和数据生成器。 MLlib 目前支持四种类型的算法：二进制分类、回归、聚类和协作过滤。

## 火花 SQL

Spark SQL 派生自 Shark，Shark 是使用 Spark 作为执行引擎的 Hive 数据仓库系统的实现。 我们将在[第 7 章](07.html "Chapter 7. Hadoop and SQL")、*Hadoop 和 SQL*中讨论配置单元。 使用 Spark SQL，可以将类似 SQL 的查询与 Scala 或 Python 代码混合使用。 查询返回的结果集本身就是 RDDS，因此，它们可以由 Spark 核心方法或 MLlib 和 GraphX 操作。

# 使用 Apache Spark 处理数据

在本节中，我们将使用 Scala API 实现[第 3 章](03.html "Chapter 3. Processing – MapReduce and Beyond")、*Processing-MapReduce 以及*以外的示例。 我们将考虑批处理和实时处理场景。 我们将向您展示如何使用 Spark Streaming 来计算实时 Twitter 流的统计数据。

## 构建和运行示例

示例的 scala 源代码可以在[https://github.com/learninghadoop2/book-examples/tree/master/ch5](https://github.com/learninghadoop2/book-examples/tree/master/ch5)中找到。 我们将使用`sbt`来构建、管理和执行代码。

`build.sbt`文件控制代码基元数据和软件依赖关系；这些包括 Spark 链接到的 Scala 解释器的版本、用于解析隐式依赖关系的 Akka 包库的链接以及对 Spark 和 Hadoop 库的依赖关系。

所有示例的源代码都可以用以下命令编译：

```scala
$ sbt compile

```

或者，可以使用以下命令将其打包到 JAR 文件中：

```scala
$ sbt package

```

可以使用以下命令生成用于执行编译类的帮助器脚本：

```scala
$ sbt add-start-script-tasks
$ sbt start-script

```

可以按如下方式调用帮助器：

```scala
$ target/start <class name> <master> <param1> … <param n>

```

这里，`<master>`是主节点的 URI。 可以使用以下命令通过`sbt`调用交互式 Scala 会话：

```scala
$ sbt console

```

该控制台与 Spark 交互式 shell 不同；相反，它是执行代码的另一种方式。 为了在其中运行 Spark 代码，我们需要手动导入和实例化一个`SparkContext`对象。 本节中提供的所有示例都期望包含使用者密钥和机密以及访问令牌的`twitter4j.properties`文件位于调用`sbt`或`spark-shell`的同一目录中：

```scala
oauth.consumerKey=
oauth.consumerSecret=
oauth.accessToken=
oauth.accessTokenSecret=
```

### 在 Yarn 上运行示例

要在 Yarn 网格上运行示例，我们首先使用以下命令构建一个 JAR 文件：

```scala
$ sbt package

```

然后，我们使用`spark-submit`命令将其发送到资源管理器：

```scala
./bin/spark-submit --class application.to.execute --master yarn-cluster [options] target/scala-2.10/chapter-4_2.10-1.0.jar [<param1> … <param n>]

```

与独立模式不同，我们不需要指定`<master>`URI。 在 YAR 中，ResourceManager 是从集群配置中选择的。 有关在 Yarn 中发射火花的更多信息，请参阅[http://spark.apache.org/docs/latest/running-on-yarn.html](http://spark.apache.org/docs/latest/running-on-yarn.html)。

### 查找热门话题

与前面使用 Spark shell 的示例不同，我们将初始化`SparkContext`作为程序的一部分。 我们将三个参数传递给`SparkContext`构造函数：我们要使用的调度器类型、应用的名称和安装 Spark 的目录：

```scala
import org.apache.spark.SparkContext._
import org.apache.spark.SparkContext
import scala.util.matching.Regex

object HashtagCount {
  def main(args: Array[String]) {
[…]
  val sc = new SparkContext(master, 
"HashtagCount", 
System.getenv("SPARK_HOME"))

    val file = sc.textFile(inputFile)
    val pattern = new Regex("(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)")

    val counts = file.flatMap(line => 
      (pattern findAllIn line).toList)
        .map(word => (word, 1))
        .reduceByKey((m, n) => m + n)  

    counts.saveAsTextFile(outputPath)
  }
}
```

我们从 HDFS(InputFile)中存储的数据集创建初始 RDD，并应用与 Wordcount 示例类似的逻辑。

对于数据集中的每条 tweet，我们提取与 hashtag 模式`(pattern findAllIn line).toArray`匹配的字符串数组，并使用 map 操作符计算每个字符串的出现次数。 这将以元组列表的形式生成一个新的 RDD，格式为：

```scala
(word, 1), (word2, 1), (word, 1) 
```

最后，我们使用`reduceByKey()`方法将这个 RDD 的元素组合在一起。 我们使用`saveAsTextFile`将最后一步生成的 RDD 存储回 HDFS。

独立驱动程序的代码位于[https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/HashTagCount.scala](https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/HashTagCount.scala)。

### 为主题分配情感

本例的源代码位于[https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/HashTagSentiment.scala](https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/HashTagSentiment.scala)，代码如下：

```scala
import org.apache.spark.SparkContext._
import org.apache.spark.SparkContext
import scala.util.matching.Regex
import scala.io.Source

object HashtagSentiment {
  def main(args: Array[String]) {
   […]
    val sc = new SparkContext(master, 
"HashtagSentiment", 
System.getenv("SPARK_HOME"))

    val file = sc.textFile(inputFile)

    val positive = Source.fromFile(positiveWordsPath)
      .getLines
      .filterNot(_ startsWith ";")
      .toSet
    val negative = Source.fromFile(negativeWordsPath)
      .getLines
      .filterNot(_ startsWith ";")
      .toSet

    val pattern = new Regex("(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)")
    val counts = file.flatMap(line => (pattern findAllIn line).map({
    word => (word, sentimentScore(line, positive, negative)) 
    })).reduceByKey({ (m, n) => (m._1 + n._1, m._2 + n._2) })

    val sentiment = counts.map({hashtagScore =>
    val hashtag = hashtagScore._1
    val score = hashtagScore._2
    val normalizedScore = score._1 / score._2
    (hashtag, normalizedScore)
    })

    sentiment.saveAsTextFile(outputPath)
  }
}
```

首先，我们将正面和负面单词列表读取到 Scala`Set`对象中，并过滤掉注释(以`;`开头的字符串)。

当找到一个标签时，我们调用一个函数-`sentimentScore`-来估计该给定文本所表达的情感。 此函数实现的逻辑与我们在[第 3 章](03.html "Chapter 3. Processing – MapReduce and Beyond")、*处理-MapReduce 及之后*中使用的逻辑相同，用于估计 tweet 的情绪。 它将 tweet 的文本`str`以及正面和负面单词列表作为`Set[String]`对象作为输入参数。 返回值是正负分数与 tweet 中的字数之差。 在 Spark 中，我们将此返回值表示为一对`Double`和`Integer`对象：

```scala
def sentimentScore(str: String, positive: Set[String], 
         negative: Set[String]): (Double, Int) = {
   var positiveScore = 0; var negativeScore = 0;
    str.split("""\s+""").foreach { w =>
      if (positive.contains(w)) { positiveScore+=1; }
      if (negative.contains(w)) { negativeScore+=1; }
    } 
    ((positiveScore - negativeScore).toDouble, 
           str.split("""\s+""").length)
}
```

我们通过按键(Hashtag)聚合来减少映射输出。 在这个阶段，我们发出一个由标签、正负分数之差之和和每条 tweet 的字数组成的三元组。 我们使用额外的 MAP 步骤来归一化情感得分，并将得到的标签和情感对列表存储到 HDFS 中。

## 流上的数据处理

前面的示例可以很容易地调整为处理实时数据流。 在本节和下一节中，我们将使用`spark-streaming-twitter`对实时消防软管执行一些简单的分析任务：

```scala
  val window = 10
  val ssc = new StreamingContext(master, "TwitterStreamEcho", Seconds(window), System.getenv("SPARK_HOME"))

  val stream = TwitterUtils.createStream(ssc, auth)

  val tweets = stream.map(tweet => (tweet.getText()))
  tweets.print()

  ssc.start()
  ssc.awaitTermination()
}   
```

本例的 scala 源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/TwitterStreamEcho.scala](https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/TwitterStreamEcho.scala)中找到。

我们需要导入的两个关键包是：

```scala
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.twitter._
```

我们使用一个 10 秒的窗口在本地集群上初始化一个新的`StreamingContext ssc`，并使用该上下文创建我们打印其文本的 tweet 的`DStream`条。

在成功执行后，Twitter 的实时消防软管将以 10 秒为一批的数据在终端中回放。 请注意，计算将无限期继续，但可以通过按*Ctrl*+*C*随时中断。

`TwitterUtils`对象是`spark-streaming-twitter`附带的`Twitter4j`库([http://twitter4j.org/en/index.html](http://twitter4j.org/en/index.html))的包装器。 成功调用`TwitterUtils.createStream`将返回`Twitter4j`个对象(`TwitterInputDStream`)的 DStream。 在前面的示例中，我们使用`getText()`方法提取 tweet 文本；但是，请注意，`twitter4j`对象公开了完整的 Twitter API。 例如，我们可以使用以下调用打印用户流：

```scala
val users = stream.map(tweet => (tweet.getUser().getId(), tweet.getUser().getName()))
users.print()
```

### 状态管理

Spark 流提供了一个特别的 DStream 来保持 RDD 中每个键的状态，并提供了`updateStateByKey`方法来改变状态。

我们可以重用批处理示例的代码来分配和更新流上的情感分数：

```scala
object StreamingHashTagSentiment {
[…]

    val counts = text.flatMap(line => (pattern findAllIn line)
      .toList
      .map(word => (word, sentimentScore(line, positive, negative))))
      .reduceByKey({ (m, n) => (m._1 + n._1, m._2 + n._2) })

    val sentiment = counts.map({hashtagScore =>
        val hashtag = hashtagScore._1
        val score = hashtagScore._2
        val normalizedScore = score._1 / score._2
        (hashtag, normalizedScore)
    })

    val stateDstream = sentiment
         .updateStateByKey[Double](updateFunc)

    stateDstream.print

    ssc.checkpoint("/tmp/checkpoint")
    ssc.start()
}
```

通过调用`hashtagSentiment.updateStateByKey`创建状态 DStream。

`updateFunc`函数实现状态突变逻辑，它是一段时间内情绪得分的累积和：

```scala
    val updateFunc = (values: Seq[Double], state: Option[Double]) => {
      val currentScore = values.sum

      val previousScore = state.getOrElse(0.0)

      Some( (currentScore + previousScore) * decayFactor)
    }   
```

`decayFactor`是一个常量值，小于或等于零，我们用它来随着时间的推移按比例减少分数。 直观地说，如果标签不再流行，这将使它们褪色。 Spark Streaming 将有状态操作的中间数据写入 HDFS，因此我们需要使用`ssc.checkpoint`检查点数据流上下文。

本例的源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/StreamingHashTagSentiment.scala](https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/StreamingHashTagSentiment.scala)找到。

## 使用 Spark SQL 进行数据分析

SPARKSQL 可以简化表示和操作结构化数据的任务。 我们将 JSON 文件加载到临时表中，并通过混合 SQL 语句和 Scala 代码来计算简单的统计数据：

```scala
object SparkJson {
   […]
   val file = sc.textFile(inputFile)

   val sqlContext = new org.apache.spark.sql.SQLContext(sc)
   import sqlContext._

   val tweets = sqlContext.jsonFile(inFile)
   tweets.printSchema()

   // Register the SchemaRDD as a table
   tweets.registerTempTable("tweets")
   val text = sqlContext.sql("SELECT text, user.id FROM tweets")

   // Find the ten most popular hashtags
   val pattern = new Regex("(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)")

   val counts = text.flatMap(sqlRow => (pattern findAllIn sqlRow(0).toString).toList)
            .map(word => (word, 1))
            .reduceByKey( (m, n) => m+n)
   counts.registerTempTable("hashtag_frequency")

counts.printSchema

val top10 = sqlContext.sql("SELECT _1 as hashtag, _2 as frequency FROM hashtag_frequency order by frequency desc limit 10")

top10.foreach(println)
}
```

与前面的示例一样，我们实例化一个`SparkContext sc`并加载 JSON tweet 的数据集。 然后，我们基于现有的`sc`创建`org.apache.spark.sql.SQLContext`的实例。 `import sqlContext._`提供对`sqlContext`的所有函数和隐式约定的访问。 我们使用`sqlContext.jsonFile`加载 tweet 的 JSON 数据集。 得到的`tweets`对象是`SchemaRDD`的一个实例，它是 Spark SQL 引入的一种新的类型的 RDD。 `SchemaRDD`类在概念上类似于关系数据库中的表；它由`Row`对象和描述每个`Row`中内容的模式组成。 我们可以通过调用`tweets.printSchema()`来查看 tweet 的模式。 在我们能够使用 SQL 语句操作 tweet 之前，我们需要将`SchemaRDD`注册为`SQLContext`中的一个表。 然后，我们使用 SQL 查询提取 JSON tweet 的文本字段。 请注意，`sqlContext.sql`的输出再次是 RDD。 因此，我们可以使用 Spark 核心方法来操作它。 在我们的例子中，我们重用前面示例中使用的逻辑来提取 hashtag 并计算它们的出现次数。 最后，我们将结果 RDD 注册为表`hashtag_frequency`，并使用 SQL 查询按频率对标签进行排序。

此示例的源代码可在[https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/SparkJson.scala](https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/SparkJson.scala)中找到。

### 数据流上的 SQL

在写入时，不能从`StreamingContext`对象直接实例化`SQLContext`。 但是，可以通过为给定流中的每个 RDD 注册`SchemaRDD`来查询 DStream：

```scala
object SqlOnStream {
[…]

    val ssc = new StreamingContext(sc, Seconds(window))

    val gson = new Gson()

    val dstream = TwitterUtils
   .createStream(ssc, auth)
   .map(gson.toJson(_))

    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    import sqlContext._

   dstream.foreachRDD( rdd => {
      rdd.foreach(println)
        val jsonRDD = sqlContext.jsonRDD(rdd)
        jsonRDD.registerTempTable("tweets")
        jsonRDD.printSchema 

         sqlContext.sql(query)
    })

    ssc.checkpoint("/tmp/checkpoint")
    ssc.start() 
    ssc.awaitTermination() 
}
```

为了让两者协同工作，我们首先创建一个`SparkContext sc`，我们使用它来初始化 a`StreamingContext ssc`和 a`sqlContext`。 与前面的示例一样，我们使用`TwitterUtils.createStream`创建 DStream RDD`dstream`。 在本例中，我们使用 Google 的 gson JSON 解析器将每个`twitter4j`对象序列化为 JSON 字符串。 为了在流上执行 Spark SQL 查询，我们在`dstream.foreachRDD`循环中注册了一个`SchemaRDD jsonRDD`。 我们使用`sqlContext.jsonRDD`方法从一批 JSON tweet 创建 RDD。 此时，我们可以使用`sqlContext.sql`方法查询`SchemaRDD`。

本例的源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/SqlOnStream.scala](https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/SqlOnStream.scala)找到。

# Samza 和 Spark Streaming 的比较

比较 Samza 和 Spark Streaming 有助于确定各自最适用的领域。 正如希望在本书中所阐明的那样，这些技术非常具有互补性。 尽管 Spark Streaming 可能看起来与 Samza 竞争，但我们觉得这两款产品在某些领域都提供了令人信服的优势。

当输入数据确实是离散事件流，并且您希望构建对这种类型的输入进行操作的处理时，Samza 就会大放异彩。 在 Kafka 上运行的 Samza 作业的延迟可能在毫秒量级。 这提供了一个专注于单个消息的编程模型，更适合真正接近实时的处理应用。 尽管它缺乏构建协作作业拓扑的支持，但其简单的模型允许构建类似的结构，或许更重要的是，它很容易推理。 它的分区和伸缩模型也注重简单性，这再次使 Samza 应用非常易于理解，并使其在处理像实时数据这样本质复杂的事情时具有显著优势。

Spark 不仅仅是一款流媒体产品。 它支持从现有数据集构建分布式数据结构，并使用强大的原语来操作这些结构，这使它能够在更高的粒度级别处理大型数据集。 Spark 生态系统中的其他产品在这个通用的批处理核心上构建额外的接口或抽象。 这与 Samza 的消息流模型非常不同。

当我们查看 Spark Streaming 时，也演示了此批处理模型；它将消息流分割为一系列 RDD，而不是按消息处理模型。 使用快速执行引擎，这意味着延迟低至 1 秒([http://www.cs.berkeley.edu/~matei/papers/2012/hotcloud_spark_streaming.pdf](http://www.cs.berkeley.edu/~matei/papers/2012/hotcloud_spark_streaming.pdf))。 对于希望以这种方式分析流的工作负载，这将比 Samza 的每消息模型更合适，后者需要额外的逻辑来提供这种窗口。

# 摘要

本章探讨了 Spark，并向您展示了它如何将迭代处理添加为一个新的丰富框架，可以在其上构建应用。 我们特别强调了：

*   基于分布式数据结构的 Spark 处理模型及其如何实现高效的内存数据处理
*   更广泛的 Spark 生态系统，以及如何在其上构建多个附加项目以进一步专门化计算模型

在下一章中，我们将探索 Apache Pig 及其编程语言 Pig 拉丁语。 我们将看到这个工具如何通过抽象掉 MapReduce 和 Spark 的一些复杂性来极大地简化 Hadoop 的软件开发。