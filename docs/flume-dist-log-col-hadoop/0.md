# 零、前言

Hadoop 是一个很棒的开源工具，可以将大量非结构化数据筛选成可管理的数据，以便您的企业能够更好地洞察您的客户和需求。 它很便宜(大部分可以是免费的)，只要你的数据中心有空间和电力，它就可以水平扩展，并且可以处理你的传统数据仓库可能会被压垮的问题。 也就是说，一个鲜为人知的秘密是，您的 Hadoop 集群需要您向其提供数据；否则，您只会拥有一个非常昂贵的发热器。 一旦您度过了使用 Hadoop 的“玩耍”阶段，您很快就会发现，您将需要一个工具来自动将数据馈送到您的集群中。 在过去，你必须为这个问题想出一个解决方案，但仅此而已！ Flume 一开始是 Cloudera 的一个项目，当时他们的集成工程师不得不一遍又一遍地编写工具，让客户自动导入数据。 如今，该项目与 Apache Foundation 一起工作，正在积极开发中，并拥有多年来一直在其生产环境中使用它的用户。

在这本书中，我希望通过 Flume 的架构概述和快速入门指南来帮助您快速上手。 之后，我们将深入研究许多更有用的 Flume 组件的细节，包括用于持久化动态数据记录的非常重要的 File Channel，以及用于缓冲数据并将数据写入**HDFS**(**Hadoop 分布式文件系统**)的 HDFS Sink。 由于 Flume 附带了各种各样的模块，因此您需要开始使用的唯一工具很可能是配置文件的文本编辑器。

到本书结束时，您应该已经掌握了足够的知识来构建一个为 Hadoop 集群提供支持的高可用性、容错、流数据管道。

# 这本书涵盖了哪些内容

[第 1 章](1.html "Chapter 1. Overview and Architecture")，*概述和体系结构*向读者介绍了 Flume 以及它试图解决的问题空间(特别是关于 Hadoop)。 我们将在后面的章节中给出各种组件的体系结构概述。

[第 2 章](2.html "Chapter 2. Flume Quick Start")，*Flume Quick Start*帮助您快速启动和运行，包括下载 Flume、创建“Hello World”配置并运行它。

[第三章](3.html "Chapter 3. Channels")*通道*涵盖了大多数人将使用的两个主要通道以及每个通道的配置选项。

[第 4 章](4.html "Chapter 4. Sinks and Sink Processors")，*接收器和接收器处理器*详细介绍了如何使用 HDFS Flume 输出，包括压缩选项和格式化数据的选项。 还介绍了故障转移选项，以创建更强大的数据管道。

[第 5 章](5.html "Chapter 5. Sources and Channel Selectors")，*源和通道选择器*将介绍几种 Flume 输入机制及其配置选项。 涵盖了基于数据内容的不同通道之间的切换，允许创建复杂的数据流。

[第 6 章](6.html "Chapter 6. Interceptors, ETL, and Routing")，*拦截器、ETL 和路由*解释了如何转换飞行中的数据，以及如何从有效负载中提取信息以与通道选择器一起使用来做出路由决策。 通过使用 Avro 序列化以及使用 Flume 命令行作为独立的 Avro 客户端手动测试和导入数据，介绍了对 Flume 代理进行分层。

[第 7 章](7.html "Chapter 7. Monitoring Flume")，*监视 Flume*讨论了可用于内部和外部监视 Flume 的各种选项，包括 Monit、Nagios、Ganglia 和自定义挂钩。

[第 8 章](8.html "Chapter 8. There Is No Spoon – The Realities of Real-time Distributed Data Collection")，*没有勺子-实时分布式数据收集的实际情况*，是一组需要考虑的杂物，超出了仅配置和使用 Flume 的范围。

# 这本书你需要什么

您需要一台安装了 Java 虚拟机的计算机，因为 Flume 是用 Java 编写的。 如果您的计算机上没有 JAVA，可以从[http://java.com/](http://java.com/)下载。

您还需要 Internet 连接，才能下载 Flume 来运行快速入门示例。

本书介绍了 Apache Flume 1.3.0，包括一些反向移植到 Cloudera 的 Flume CDH4 发行版中的项目。

# 这本书是给谁看的

本书面向负责实现将数据从各种系统自动移动到 Hadoop 集群的人员。 如果您的工作是定期将数据加载到 Hadoop 中，那么这本书应该会帮助您将自己的代码编写得不再繁琐，也不再需要编写一个只要您在公司工作就会支持的自定义工具。

只需要具备 HDFS 的基本 Hadoop 知识。 如果您的需要需要，本文还介绍了一些自定义实现。 对于这种级别的实现，您需要知道如何用 Java 编程。

最后，您需要您最喜欢的文本编辑器，因为本书的大部分内容介绍了如何通过代理的文本配置文件配置各种 Flume 组件。

# 公约

在这本书中，你会发现许多区分不同信息的文本样式。 以下是这些风格的一些示例，并解释了它们的含义。

文本中的代码词如下所示：“我们可以通过使用`include`指令包括其他上下文。”

代码块设置如下：

```scala
agent.sinks.k1.hdfs.path=/logs/apache/access
agent.sinks.k1.hdfs.filePrefix=access
agent.sinks.k1.hdfs.fileSuffix=.log
```

当我们希望您注意代码块的特定部分时，相关行或项将以粗体显示：

```scala
agent.sources.s1.command=uptime
agent.sources.s1.restart=true
agent.sources.s1.restartThrottle=60000
```

任何命令行输入或输出都如下所示：

```scala
$ tar -zxf apache-flume-1.3.1.tar.gz
$ cd apache-flume-1.3.1

```

**新术语**和**重要单词**以粗体显示。

### 备注

警告或重要说明会出现在这样的框中。

### 提示

提示和技巧如下所示。

# 读者反馈

欢迎读者的反馈。 让我们知道你对这本书的看法-你喜欢什么或不喜欢什么。 读者反馈对于我们开发真正能让您获得最大收益的图书非常重要。

要向我们发送一般反馈，只需发送电子邮件至`<[feedback@packtpub.com](mailto:feedback@packtpub.com)>`，并通过消息主题提及书名。 如果有一个您擅长的主题，并且您有兴趣撰写或投稿一本书，请参阅我们位于[www.Packtpub.com/Authors](http://www.packtpub.com/authors)上的作者指南。

# 客户支持

现在您已经成为 Packt 图书的拥有者，我们有很多东西可以帮助您从购买中获得最大价值。

## 勘误表

虽然我们已经竭尽全力确保内容的准确性，但错误还是会发生。 如果您在我们的一本书中发现错误--可能是文本或代码中的错误--如果您能向我们报告，我们将不胜感激。 通过这样做，您可以将其他读者从挫折中解救出来，并帮助我们改进本书的后续版本。 如果您发现任何勘误表，请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，选择您的图书，单击**勘误表提交表**链接，然后输入勘误表的详细信息。 一旦您的勘误表被核实，您提交的勘误表将被接受，勘误表将被上传到我们的网站上，或添加到该标题勘误表部分下的任何现有勘误表列表中。 通过从[http://www.packtpub.com/support](http://www.packtpub.com/support)中选择您的书目，可以查看任何现有勘误表。

## 盗版

在互联网上盗版版权材料是所有媒体持续存在的问题。 在 Packt，我们非常重视版权和许可证的保护。 如果您在互联网上发现任何形式的非法复制我们的作品，请立即提供我们的位置、地址或网站名称，以便我们采取补救措施。

请拨打`<[copyright@packtpub.com](mailto:copyright@packtpub.com)>`与我们联系，并提供疑似盗版材料的链接。

我们感谢您在保护我们的作者方面的帮助，以及我们为您提供有价值内容的能力。

## 问题

如果您对本书的任何方面有任何问题，可以拨打`<[questions@packtpub.com](mailto:questions@packtpub.com)>`与我们联系，我们将尽最大努力解决。