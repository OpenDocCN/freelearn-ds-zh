# 四、Flume 和 Flume 处理器

到目前为止，您应该对 Flume 与 Flume 架构的位置有了很好的了解。 在本章中，我们将了解 Hadoop 中最常用的接收器，即 HDFS 接收器。 Flume 的总体架构支持许多其他 Flume，我们在本书中没有篇幅涵盖所有这些 Flume。 有些与 Flume 捆绑在一起，可以写入 HBase、IRC、ElasticSearch，正如我们在[第 2 章](2.html "Chapter 2. Flume Quick Start")、*Flume Quick Start*、log4j 和文件接收器中看到的那样。 Internet 上还有其他接收器，可用于将数据写入 MongoDB、Cassandra、RabbitMQ、Redis 以及您能想到的任何其他数据存储。 如果您找不到符合您需要的接收器，您可以通过扩展`org.apache.flume.sink.Abstractsink`类轻松地编写一个接收器。

# HDFS 接收器

HDFS 接收器的任务是在 HDFS 中连续打开一个文件，将数据流到其中，然后在某个时刻关闭该文件并启动一个新文件。 正如我们在[第 1 章](1.html "Chapter 1. Overview and Architecture")、*概述和体系结构*中所讨论的，文件轮换之间的时长必须与 HDFS 中关闭文件的速度相平衡，从而使数据可见以供处理。 正如我们已经讨论过的，输入大量小文件会使 MapReduce 作业效率低下。

要使用 HDFS 接收器，请将您命名的`sink`上的`type`参数设置为`hdfs`：

```scala
agent.sinks.k1.type=hdfs
```

这为名为`agent`的代理定义了名为`k1`的 HDFS 接收器。 您还需要指定一些其他必需参数，从要写入数据的 HDFS 中的`path`开始：

```scala
agent.sinks.k1.hdfs.path=/path/in/hdfs
```

与 Hadoop 中的大多数文件路径一样，可以通过三种不同的方式指定此 HDFS 路径，即绝对路径、使用服务器名称指定的绝对路径和相对路径。 这些都是等效的(假设您的 Flume Agent 以 Flume 用户身份运行)：

<colgroup><col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

类型 / 品种 / 象征 / 印刷文字

 | 

小径 / 路线 / 途径 / 道路

 |
| --- | --- |
| 绝对 | `/Users/flume/mydata` |
| 服务器名称为的绝对名称 | `hdfs://namenode/Users/flume/mydata` |
| 与ΔT0 抯相关 | `mydata` |

我更喜欢通过设置 Hadoop 的`core-site.xml`文件中的`fs.default.name`属性，使用有效的`hadoop`命令行来配置我正在安装 Flume 的任何服务器。 我不在 HDFS 用户目录中保存持久数据，但我更喜欢使用具有一些有意义的路径名(即`/logs/apache/access`)的绝对路径。 只有当目标是完全不同的 Hadoop 集群时，我才会专门指定名称节点。 这允许您将已经在一个环境中测试的配置移动到另一个环境中，而不会产生意外的后果，例如生产服务器将数据写入到临时 Hadoop 集群中，因为有人忘记在配置中编辑目标。 外部化环境细节是避免这种情况的最佳实践。

HDFS 接收器(实际上是任何接收器)的最后一个必需参数是它将从中执行 Take 操作的通道。 为此，使用要读取的通道名称设置`channel`参数：

```scala
agent.sinks.k1.channel=c1
```

这告诉`k1`接收器从`c1`通道读取事件。

以下是您可以根据默认值调整的几乎完整的配置参数列表：

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

钥匙 / 键 / 关键 / 主调

 | 

规定的

 | 

类型 / 品种 / 象征 / 印刷文字

 | 

不履行 / 拖欠 / 未到庭 / 不到场

 |
| --- | --- | --- | --- |
| `type` | 是 | `String` | HDFS |
| `channel` | 是 | `String` |   |
| `hdfs.path` | 是 | `String` |   |
| `hdfs.filePrefix` | 没有Колибрисистема | `String` | FlumeData |
| `hdfs.fileSuffix` | 没有Колибрисистема | `String` |   |
| `hdfs.maxOpenFiles` | 没有Колибрисистема | `long` | 5000 |
| `hdfs.round` | 没有Колибрисистема | `Boolean` | 与事实不符的 / 错的 / 貌似的 / 故意捏造的 |
| `hdfs.roundValue` | 没有Колибрисистема | `int` | 1. |
| `hdfs.roundUnit` | 没有Колибрисистема | `String`(`second`，`minute`，or`hour`) | 秒 / 片刻 / 第二名 / 瞬间 |
| `hdfs.timeZone` | 没有Колибрисистема | `String` | 地方时间，当地时间 |
| `hdfs.inUsePrefix` | 没有Колибрисистема | `String` | (仅限于 CDH4.2.0 或 Flume 1.4) |
| `hdfs.inUseSuffix` | 没有Колибрисистема | `String` | .tmp(仅限于 CDH4.2.0 或 Flume 1.4) |
| `hdfs.rollInterval` | 没有Колибрисистема | `long`(秒) | 30 秒(0=可用) |
| `hdfs.rollSize` | 没有Колибрисистема | `long`(字节) | 1024 字节(0=禁用) |
| `hdfs.rollCount` | 没有Колибрисистема | `long` | 10(0=禁用) |
| `hdfs.batchSize` | 没有Колибрисистема | `long` | 100 个 |
| `hdfs.codeC` | 没有Колибрисистема | `String` |   |

请记住始终检查您在[http://flume.apache.org/](http://flume.apache.org/,)上使用的版本的 Flume 用户指南，因为本书的发行和您实际使用的版本之间可能会发生变化。

## 路径和文件名

每次 Flume 在 HDFS 中的`hdfs.path`处启动要写入数据的新文件时，文件名由`hdfs.filePrefix`、句点字符、文件启动时的纪元时间戳以及由`hdfs.fileSuffix`属性(如果设置)指定的文件后缀(可选)组成。 例如，请参见以下代码行：

```scala
agent.sinks.k1.hdfs.path=/logs/apache/access
```

此行将生成一个文件，如`/logs/apache/access/FlumeData.1362945258`。

但是，请看一下以下配置：

```scala
agent.sinks.k1.hdfs.path=/logs/apache/access
agent.sinks.k1.hdfs.filePrefix=access
agent.sinks.k1.hdfs.fileSuffix=.log
```

在此配置中，您的文件名更像`/logs/apache/access/access.1362945258.log`。

随着时间的推移，`hdfs.path`目录将变得非常满，因此您需要在 PATH 中添加某种类型的时间元素，以将文件划分为子目录。 Flume 支持各种基于时间的转义序列，例如指定四位年份的`%Y`。 我喜欢使用年/月/日/小时形式的序列(因此它们从最旧到最新排序)，因此我经常使用以下形式作为路径：

```scala
agent.sinks.k1.hdfs.path=/logs/apache/access/%Y/%m/%D/%H
```

这说明我需要一条类似`/logs/apache/access/2013/03/10/18/`的路径。

有关基于时间的转义序列的完整列表，请参阅《Flume 用户指南》。

另一个方便的转义序列机制是能够在路径中使用 Flume 标头值。 例如，如果有一个标头的键为`logType`，我可以通过转义标头的键将 Apache 访问和错误日志分到不同的目录中，同时使用相同的通道，如下所示：

```scala
agent.sinks.k1.hdfs.path=/logs/apache/%{logType}/%Y/%m/%D/%H
```

这将导致访问日志转到`/logs/apache/access/2013/03/10/18/`，错误日志转到`/logs/apache/error/2013/03/10/18/`。 但是，如果我更喜欢同一目录路径中的两种日志类型，我可以在我的`hdfs.filePrefix`中使用`logType`，如下所示：

```scala
agent.sinks.k1.hdfs.path=/logs/apache/%Y/%m/%D/%H
agent.sinks.k1.hdfs.filePrefix=%{logType}
```

显然，Flume 可以一次写入多个文件。 属性`hdfs.maxOpenFiles`设置一次可以打开的数量上限，默认值为 5000。 如果超过此限制，则仍处于打开状态的最旧文件将被关闭。 请记住，每个打开的文件都会在操作系统级别和 HDFS(NameNode 和 DataNode 连接)上产生开销。

您可能会发现另一组有用的属性允许以小时、分钟或秒的粒度向下舍入事件时间，同时仍保留文件路径中的这些元素。 假设您有一个路径规范，如下所示：

```scala
agent.sinks.k1.hdfs.path=/logs/apache/%Y/%m/%D/%H%M
```

但在这种情况下，您每天只需要四个子目录(在每小时的 00、15、30 和 45 点，每个目录包含 15 分钟的数据)。 您可以通过设置以下值来完成此操作：

```scala
agent.sinks.k1.hdfs.round=true
agent.sinks.k1.hdfs.roundValue=15
agent.sinks.k1.hdfs.roundUnit=minute
```

这将导致 2013-03-10 的 01：15：00 到 01：29：59 之间的日志被写入`/logs/apache/2013/03/10/0115/`中包含的文件。 01：30：00 至 01：44：59 的日志将写入`/logs/apache/2013/03/10/0130/`中包含的文件中。

`hdfs.timeZone`属性用于指定要为转义序列解释时间的时区。 默认值为您的计算机的本地时间。 如果您的当地时间受到夏令时调整的影响，则在`%H == 02`时(秋季)会有两倍的数据，而在`%H == 02`时(春季)则没有数据。 我认为将时区引入计算机可以读取的内容是一个坏主意。 我相信时区只是人类关心的问题，计算机应该只在世界时进行交流。 出于这个原因，我在我的 Flume 代理上设置了此属性，以使时区问题不复存在：

```scala
-Duser.timezone=UTC
```

如果您不同意，您可以自由使用默认值(当地时间)，或者将`hdfs.timeZone`设置为您喜欢的任何值。 您传递的值用于对`java.util.Timezone.getTimeZone(…)`的调用，因此请检查 Javadoc 是否可以接受此处使用的值。

最后，在将文件写入 HDFS 的同时，添加了`.tmp`扩展名。 当文件关闭时，扩展名将被删除。 这允许您在 Flume 主动写入的目录上运行 MapReduce 作业时，轻松排除这些文件作为输入。 它还允许您通过查看 HDFS 中的目录列表来查看正在写入哪些文件。 由于您通常会为 MapReduce 作业中的输入指定一个目录(或者因为您使用的是配置单元)，因此临时文件通常会被错误地作为空输入或乱码输入。 Flume-1702 就是为了解决这个问题而创建的，并将在 Flume 1.4 中发布，但是如果您碰巧使用的是 Cloudera 的 CDH4.2.0 版本，那么更改会被反向移植到 Flume 1.3 中。 这引入了两个新属性来更改“in use”前缀和后缀。 为避免临时文件在关闭前被拾取，请将后缀设置为空白(而不是默认的`.tmp`)，并将前缀设置为圆点或下划线字符，如下所示：

```scala
agent.sinks.k1.hdfs.inUsePrefix=_
agent.sinks.k1.hdfs.inUseSuffix=
```

## 文件轮换

默认情况下，Flume 将每 30 秒、10 个事件或 1024 字节主动轮换写入文件。 这是通过分别设置`hdfs.rollInterval`、`hdfs.rollCount`和`hdfs.rollSize`属性来实现的。 可以将其中一个或多个设置为零以禁用该特定滚动机构。 例如，如果您只需要 1 分钟的基于时间的滚动，则可以按如下方式设置这些参数：

```scala
agent.sinks.k1.hdfs.rollInterval=60
agent.sinks.k1.hdfs.rollCount=0
agent.sinks.k1.hdfs.rollSize=0
```

如果您的输出包含任意数量的标题信息，则每个文件的 HDFS 大小可能会大于您的预期，因为`hdfs.rollSize`循环方案只计算事件正文长度。 显然，您可能不想同时禁用所有三种轮换机制，否则 HDFS 中的一个目录会出现文件溢出。

最后，一个相关参数是`hdfs.batchSize`。 这是接收器将从通道读取的每个事务的事件数。 如果您的通道中有大量数据，则可以通过将此值设置为高于默认值 100 来提高性能，从而降低每个事件的事务开销。

既然我们已经讨论了文件在 HDFS 中的管理和滚动方式，让我们来看看事件内容是如何写入的。

# 压缩编解码器

**编解码器**(编码器/解码器)用于使用各种压缩算法压缩和解压缩数据。 `gzip`、`bzip2`、`lzo`和`snappy`受 Flume 支持，不过您可能需要自己安装 LZO，尤其是在由于许可问题而使用 CDH 等发行版的情况下。

如果要为数据指定压缩，如果希望 HDFS 接收器写入压缩文件，请设置`hdfs.codeC`属性。 该属性还用作写入 HDFS 的文件的后缀。 对于示例，如果按如下方式指定编解码器，则所有写入的文件都将具有`.gzip`扩展名，因此在这种情况下不需要指定`hdfs.fileSuffix`属性：

```scala
agent.sinks.k1.hdfs.codeC=gzip
```

您选择使用哪种编解码器需要您进行一些研究。 有观点认为，使用`gzip`或`bzip2`可以获得更高的压缩比，但代价是压缩时间更长，特别是如果您的数据只写入一次，但将被读取数百次或数千次。 另一方面，使用`snappy`或`lzo`会导致较快的压缩性能，但会导致较低的压缩比。 请记住，文件的可拆分性，特别是在使用纯文本文件时，将极大地影响 MapReduce 作业的性能。 如果您不确定我在说什么，可以去拿一本*Hadoop 初学者指南*([http://amzn.to/14Dh6TA](http://amzn.to/14Dh6TA))或*Hadoop：权威指南*([http://amzn.to/16OsfIf](http://amzn.to/16OsfIf))。

# 事件序列化程序

事件序列化程序是将 Flume 事件转换为另一种输出格式的机制。 它的功能类似于 log4j 中的`Layout`类。 默认情况下，`text`序列化程序，它只输出 Flume 事件正文。 还有另一个函数`header_and_text`，它同时输出头和正文。 最后，还有一个`avro_event`序列化程序，可用于创建事件的 Avro 表示。 如果您自己编写，则可以使用实现的完全限定类名作为`serializer`属性值。

## 文本输出

如前所述，默认序列化程序是`text`序列化程序。 这将仅输出 Flume 事件正文，并丢弃标头。 每个事件都有一个新行字符附加器，除非您通过将`serializer.appendNewLine`属性设置为`false`来覆盖此默认行为。

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

钥匙 / 键 / 关键 / 主调

 | 

规定的

 | 

类型 / 品种 / 象征 / 印刷文字

 | 

不履行 / 拖欠 / 未到庭 / 不到场

 |
| --- | --- | --- | --- |
| `serializer` | 不 / 否决票 / 同 Noh | `String` | 文本 / 文本信息 / 课文 / 主题 |
| `serializer.appendNewLine` | 不 / 否决票 / 同 Noh | `boolean` | 真实的 / 符合实际情况的 / 精确的 / 忠实的 |

## 带标题的文本

`text_with_headers`序列化程序允许您保存而不是丢弃 Flume 事件标头。 输出格式由标题组成，后跟一个空格，然后是正文有效负载，最后以一个可选禁用的换行符结束。 以下是该序列化程序生成的一些示例输出：

```scala
{key1=value1, key2=value2} body text here
```

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

钥匙 / 键 / 关键 / 主调

 | 

规定的

 | 

类型 / 品种 / 象征 / 印刷文字

 | 

不履行 / 拖欠 / 未到庭 / 不到场

 |
| --- | --- | --- | --- |
| `serializer` | 不 / 否决票 / 同 Noh | `String` | 带标题的文本(_W) |
| `serializer.appendNewLine` | 不 / 否决票 / 同 Noh | `boolean` | 真实的 / 符合实际情况的 / 精确的 / 忠实的 |

## _>Apache Avro

Apache Avro 项目([Hadoop](http://avro.apache.org/))提供了一种在功能上类似于 Google 协议缓冲区的序列化格式，但由于容器基于`HadoopSequenceFiles`并具有一些 http://avro.apache.org/集成，因此对 Hadoop 更加友好。 格式还使用 JSON 进行自我描述，这是一种很好的长期数据存储格式，因为您的数据格式可能会随着时间的推移而演变。 如果你的数据有很多你想避免变成字符串的结构，只是为了在你的 MapReduce 作业中解析这些字符串，你应该去阅读更多关于 Avro 的内容，看看你是否想把它用作 HDFS 中的一种存储格式。

`avro_event`序列化程序基于 Flume 事件模式创建 Avro 数据。 它没有格式化参数，因为 avro 规定了数据的格式，而 Flume 事件的结构规定了使用的模式：

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

钥匙 / 键 / 关键 / 主调

 | 

规定的

 | 

类型 / 品种 / 象征 / 印刷文字

 | 

不履行 / 拖欠 / 未到庭 / 不到场

 |
| --- | --- | --- | --- |
| `serializer` | 不 / 否决票 / 同 Noh | `String` | 我会有 _ 个活动 |
| `serializer.compressionCodec` | 不 / 否决票 / 同 Noh | `String`(gzip、bzip2、lzo 或 snappy) |   |
| `serializer.syncIntervalBytes` | 不 / 否决票 / 同 Noh | `int`(字节) | 2048000(字节) |

如果您希望使用 Avro，但又希望使用与 Flume 事件模式不同的模式，则必须编写您自己的事件序列化程序。

如果希望在将数据写入 avro 容器之前对其进行压缩，则应将`serializer.compressionCodec`属性设置为已安装编解码器的文件扩展名。 `serializer.syncIntervalBytes`属性确定在将数据刷新到 HDFS 之前使用的数据缓冲区的大小，因此，此设置可能会影响使用编解码器时的压缩比。 下面是一个使用 4 MB 缓冲区对 Avro 数据进行快速压缩的示例：

```scala
agent.sinks.k1.serializer=avro_event
agent.sinks.k1.serializer.compressionCodec=snappy
agent.sinks.k1.serializer.syncIntervalBytes=4194304
agent.sinks.k1.hdfs.fileSuffix=.avro
```

要使 Avro 文件在 Avro MapReduce 作业中工作，它们必须以`.avro`结尾，否则将被忽略为输入。 因此，您需要显式设置`hdfs.fileSuffix`属性。 此外，您不会在 avro 文件上设置`hdfs.codeC`属性。

## 文件类型

默认情况下，HDFS 接收器将数据作为 Hadoop SequenceFiles 写入 HDFS。 这是一个常见的 Hadoop 包装器，由一个键和值字段组成，由二进制字段和记录分隔符分隔。 通常，计算机上的文本文件会假设每条记录都是换行符结束的。 那么，如果您的数据包含换行符，比如一些 XML，该怎么办呢？ 使用序列文件可以解决此问题，因为它使用不可打印的字符作为分隔符。 SequenceFiles 也是可拆分的，这使得在对数据(尤其是大文件)运行 MapReduce 作业时具有更好的局部性和并行性。

### _ 文件

使用 SequenceFile 文件类型时，需要指定希望如何在 SequenceFile 中的记录上写入键和值。 每条记录上的关键字将始终是包含当前时间戳的`LongWritable`，或者如果设置了时间戳事件头，则将改为使用。 默认情况下，该值的格式为与`byte[]`Flume Body 对应的`org.apache.hadoop.io.BytesWritable`：

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

钥匙 / 键 / 关键 / 主调

 | 

规定的

 | 

类型 / 品种 / 象征 / 印刷文字

 | 

不履行 / 拖欠 / 未到庭 / 不到场

 |
| --- | --- | --- | --- |
| `hdfs.fileType` | 不 / 否决票 / 同 Noh | `String` | SequenceFile |
| `hdfs.writeType` | 不 / 否决票 / 同 Noh | `String` | 可写入的 |

但是，如果希望将有效负载解释为`String`，则可以覆盖`hdfs.writeType`属性，以便将`org.apache.hadoop.io.Text`用作值字段：

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

钥匙 / 键 / 关键 / 主调

 | 

规定的

 | 

类型 / 品种 / 象征 / 印刷文字

 | 

不履行 / 拖欠 / 未到庭 / 不到场

 |
| --- | --- | --- | --- |
| `hdfs.fileType` | 不 / 否决票 / 同 Noh | `String` | SequenceFile |
| `hdfs.writeType` | 不 / 否决票 / 同 Noh | `String` | 文本 / 文本信息 / 课文 / 主题 |

### 数据流

如果因为数据没有自然键而不想输出 SequenceFile，则可以使用数据流仅输出未压缩的值。 只需覆盖`hdfs.fileType`属性：

```scala
agent.sinks.k1.hdfs.fileType=DataStream
```

这是您将在 Avro 序列化中使用的文件类型，因为任何压缩都应该在事件序列化程序中完成。 要序列化`gzip`压缩 Avro 文件，您需要设置以下属性：

```scala
agent.sinks.k1.serializer=avro_event
agent.sinks.k1.serializer.compressionCodec=gzip
agent.sinks.k1.hdfs.fileType=DataStream
agent.sinks.k1.hdfs.fileSuffix=.avro
```

### 压缩流

除了数据在写入时被压缩之外，`CompressedStream`类似于`DataStream`。 您可以将其视为对未压缩文件运行`gzip`实用程序，但只需一个步骤。 这与压缩的 avro 文件不同，压缩的 avro 文件的内容被压缩，然后写入未压缩的 avro 包装。

```scala
agent.sinks.k1.hdfs.fileType=CompressedStream
```

请记住，如果您决定使用`CompressedStream`，则只有某些压缩格式在 MapReduce 中是可拆分的。 压缩算法选择没有 Flume 配置，而是由核心 Hadoop 中的`zlib.compress.strategy`和`zlib.compress.level`属性指定的。

## 超时和工作人员

最后，有两个与超时相关的杂项属性和两个可以更改的工作池的其他属性：

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

钥匙 / 键 / 关键 / 主调

 | 

规定的

 | 

类型 / 品种 / 象征 / 印刷文字

 | 

不履行 / 拖欠 / 未到庭 / 不到场

 |
| --- | --- | --- | --- |
| `hdfs.callTimeout` | 不 / 否决票 / 同 Noh | `long`(毫秒) | 10000 |
| `hdfs.idleTimeout` | 不 / 否决票 / 同 Noh | `int`(秒) | 0(0=禁用) |
| `hdfs.threadsPoolSize` | 不 / 否决票 / 同 Noh | `int` | 10 个 |
| `hdfs.rollTimerPoolSize` | 不 / 否决票 / 同 Noh | `int` | 1. |

`hdfs.callTimeout`是 HDFS 接收器在放弃之前等待 HDFS 操作返回成功(或失败)的时间量。 如果您的 Hadoop 集群特别慢(例如，开发或虚拟集群)，您可能需要将此值设置得更高，以避免错误。 请记住，如果您不能保持高于通道输入速率的写入吞吐量，通道将会溢出。

`hdfs.idleTimeout`属性如果设置为非零值，则是 Flume 等待自动关闭空闲文件的时间。 我从未使用过它，因为`hdfs.fileRollInterval`在每个滚动周期处理文件的关闭，如果通道空闲，它将不会打开新文件。 此设置似乎是作为已讨论的大小、时间和事件计数机制的替代滚动机制而创建的。 您可能希望将尽可能多的数据写入文件，并仅在确实没有更多数据时才将其关闭。 在这种情况下，如果您还将`hdfs.rollInterval`、`hdfs.rollSize`和`hdfs.rollCount`全部设置为零，则可以使用`hdfs.idleTimeout`来完成此循环方案。

您可以设置的第一个调整工作进程数的属性是`hdfs.threadsPoolSize`，默认为 10。这是可以同时写入的最大文件数。 如果您使用事件头来确定文件路径和名称，您可能会一次打开 10 个以上的文件，但在过度增加此值时要小心，以免使 HDFS 不堪重负。

与工作池相关的最后一个属性是`hdfs.rollTimerPoolSize`。 这是处理由`hdfs.idleTimeout`属性设置的超时的工作进程数。 关闭文件的工作量非常小，因此不太可能从一个工作进程的默认值增加此值。 如果不使用基于`hdfs.idleTimeout`的旋转，则可以忽略`hdfs.rollTimerPoolSize`属性，因为它没有被使用。

# 汇流组

为了消除数据处理管道中的单点故障，Flume 能够使用负载平衡或故障转移将事件发送到不同的接收器。 为了做到这一点，我们需要引入一个新的概念，称为下沉群。 接收器组用于创建接收器的逻辑分组。 此分组的行为由称为接收器处理器的东西决定，它确定如何路由事件。

有一个默认接收器处理器，它包含一个接收器，只要您的接收器不属于任何接收器组，就会使用该接收器。 我们在[第 2 章](2.html "Chapter 2. Flume Quick Start")，*Flume Quick Start*中的 Hello World 示例使用了默认的接收器处理器。 单个接收器不需要特殊配置。

为了让 Flume 了解接收组，有一个名为`sinkgroups`的新顶级代理属性。 与源、通道和接收器类似，您可以使用代理名称作为该属性的前缀，如下所示：

```scala
agent.sinkgroups=sg1
```

这里，我们定义了对于名为`agent`的代理，有一个名为`sg1`的接收组。

对于每个命名接收器组，需要使用由空格分隔的接收器名称列表组成的`sinks`属性指定其包含的接收器：

```scala
agent.sinkgroups.sg1.sinks=k1,k2
```

这定义了接收器`k1`和`k2`是名为`agent`的代理的接收器组`sg1`的一部分。

接收器组通常与数据的分层移动结合使用，以绕过故障进行路由。 但是，它们也可以用于写入不同的 Hadoop 集群，因为即使是维护良好的集群也有定期维护。

## 负载均衡

继续前面的示例，假设您希望均匀地将流量负载平衡到`k1`和`k2`。 下表中列出了一些需要指定的其他属性：

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

钥匙 / 键 / 关键 / 主调

 | 

类型 / 品种 / 象征 / 印刷文字

 | 

不履行 / 拖欠 / 未到庭 / 不到场

 |
| --- | --- | --- |
| `processor.type` | `String` | 负载均衡 |
| `processor.selector` | `String`(ROUND_ROBIN，随机) | 轮询 |
| `processor.backoff` | `boolean` | 与事实不符的 / 错的 / 貌似的 / 故意捏造的 |

将`processor.type`设置为`load_balance`时，除非`processor.selector`属性另有指定，否则将使用循环选择。 可以将其设置为`round_robin`或`random`。 您还可以指定您自己的负载平衡选择器机制，我们在这里不会讨论这一点。 如果需要此自定义控件，请参考 Flume 文档。

`processor.backoff`属性指定在重试引发异常的接收器时是否应使用指数备份。 缺省值为`false`，这意味着在抛出异常之后，将基于循环或随机选择在下一次轮到接收器时再次尝试。 如果设置为`true`，则对于每次失败，等待时间从 1 秒开始增加一倍，限制为约 18 小时(2^16 秒)。

### 备注

在编写本文时，代码中`processor.backoff`的默认值为 false，但 Flume 文档显示为 true。 省去让自己头疼的麻烦，指定你想要什么，而不是依赖默认设置。

## 故障转移

如果您宁愿尝试一个接收器，如果该接收器失败，则尝试另一个接收器，则需要将`processor.type`设置为`failover`。 接下来，您需要设置其他属性来指定顺序，方法是：设置`processor.priority`属性，后跟接收器名称：

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

钥匙 / 键 / 关键 / 主调

 | 

类型 / 品种 / 象征 / 印刷文字

 | 

不履行 / 拖欠 / 未到庭 / 不到场

 |
| --- | --- | --- |
| `processor.type` | `String` | 故障切换 |
| `processor.priority.NAME` | `int` |   |
| `processor.maxpenality` | `int`(毫秒) | 30000 |

让我们看一下这个例子：

```scala
agent.sinkgroups.sg1.sinks=k1,k2,k3
agent.sinkgroups.sg1.processor.type=failover
agent.sinkgroups.sg1.processor.priority.k1=10
agent.sinkgroups.sg1.processor.priority.k2=20
agent.sinkgroups.sg1.processor.priority.k3=20
```

优先级较低的数字排在第一位，在平局的情况下，顺序是任意的。 你可以使用任何对你有意义的编号系统(一、五、十，随便什么)。 在本例中，将首先尝试接收器`k1`，如果抛出异常，则接下来将尝试`k2`或`k3`。 如果首先选择`k3`进行尝试，但失败，则 K2 仍会尝试。 如果接收器组中的所有接收器都失败，则回滚通道的事务。

最后，`processor.maxPenality`为组中失败的接收器设置指数回退的上限。 第一次失败后，将在一秒钟后才能再次使用。 每次后续故障都会使等待时间加倍，直到达到`processor.maxPenality`。

# 摘要

在本章中，我们深入讨论了 HDFS 接收器，即将流数据写入 HDFS 的 Flume 输出。 我们介绍了 Flume 如何根据 Flume 标头的时间或内容将数据划分到不同的 HDFS 路径中。 还讨论了几种文件滚动技术，包括：

*   时间旋转
*   事件计数轮换
*   大小旋转
*   仅空闲时旋转

讨论了压缩作为减少 HDFS 中存储需求的一种方法，并应在可能的情况下使用压缩。 除了节省存储空间外，读取压缩文件并解压缩到内存中通常比读取未压缩文件更快。 这将提高在此数据上运行的 MapReduce 作业的性能。 压缩数据的可分割性也是决定使用哪种压缩算法的一个因素。

引入事件序列化程序作为将 Flume 事件转换为外部存储格式的机制，包括以下内容：

*   文本(仅正文)
*   文本和页眉(页眉和正文)
*   AVRO 序列化(带可选压缩)

接下来，介绍各种文件格式，包括以下内容：

*   序列文件(Hadoop 密钥/值文件)
*   数据流(未压缩的数据文件，如 Avro 容器)
*   压缩数据流

最后，我们介绍了接收器组作为一种使用负载平衡或故障转移路径将事件路由到不同源的方法，这些路径可用于消除将数据路由到其目的地的单点故障。

在下一章中，我们将讨论各种输入机制(源)，它们将反馈回[第 3 章](3.html "Chapter 3. Channels")、*通道*中介绍的已配置通道。