- en: Special RDD Operations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特殊的 RDD 操作
- en: '"It''s supposed to be automatic, but actually you have to push this button."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"本来应该是自动的，但实际上你必须按这个按钮。"'
- en: '- John Brunner'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 约翰·布鲁纳'
- en: In this chapter, you learn how RDDs can be tailored to different needs, and
    how these RDDs provide new functionalities (and dangers!) Moreover, we investigate
    other useful objects that Spark provides, such as broadcast variables and accumulators.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何根据不同的需求调整 RDD，并且了解这些 RDD 提供的新功能（以及潜在的风险！）。此外，我们还将探讨 Spark 提供的其他有用对象，如广播变量和累加器。
- en: 'In a nutshell, the following topics will be covered throughout this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将涵盖以下主题：
- en: Types of RDDs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 的类型
- en: Aggregations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合
- en: Partitioning and shuffling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区和洗牌
- en: Broadcast variables
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广播变量
- en: Accumulators
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累加器
- en: Types of RDDs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD 的类型
- en: '**Resilient Distributed Datasets** (**RDDs**) are the fundamental object used
    in Apache Spark. RDDs are immutable collections representing datasets and have
    the inbuilt capability of reliability and failure recovery. By nature, RDDs create
    new RDDs upon any operation such as transformation or action. They also store
    the lineage, which is used to recover from failures. We have also seen in the
    previous chapter some details about how RDDs can be created and what kind of operations
    can be applied to RDDs.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集** (**RDDs**) 是 Apache Spark 中使用的基本对象。RDD 是不可变的集合，代表数据集，并具有内建的可靠性和故障恢复能力。RDD
    的特点是每次操作（如转换或动作）都会创建新的 RDD，并且它们还存储继承链，继承链用于故障恢复。在前一章中，我们已经看到了一些关于如何创建 RDD 以及可以应用于
    RDD 的操作类型的细节。'
- en: 'The following is a simply example of the RDD lineage:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的 RDD 继承示例：
- en: '![](img/00056.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpeg)'
- en: 'Let''s start looking at the simplest RDD again by creating a RDD from a sequence
    of numbers:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次从一个简单的 RDD 开始，通过创建一个由数字序列组成的 RDD：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding example shows RDD of integers and any operation done on the RDD
    results in another RDD. For example, if we multiply each element by `3`, the result
    is shown in the following snippet:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例展示了整数类型的 RDD，任何对该 RDD 执行的操作都会生成另一个 RDD。例如，如果我们将每个元素乘以 `3`，结果如下面的代码片段所示：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s do one more operation, adding `2` to each element and also print all
    three RDDs:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再做一个操作，将 `2` 加到每个元素上，并打印出所有三个 RDD：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'An interesting thing to look at is the lineage of each RDD using the `toDebugString`
    function:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的事情是使用 `toDebugString` 函数查看每个 RDD 的继承链：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following is the lineage shown in the Spark web UI:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在 Spark Web UI 中显示的继承链：
- en: '![](img/00064.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00064.jpeg)'
- en: RDD does not need to be the same datatype as the first RDD (integer). The following
    is a RDD which writes a different datatype of a tuple of (string, integer).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 不需要与第一个 RDD（整数类型）保持相同的数据类型。以下是一个 RDD，它写入了不同数据类型的元组（字符串，整数）。
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The following is a RDD of the `StatePopulation` file where each record is converted
    to `upperCase`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `StatePopulation` 文件的 RDD，其中每个记录都转换为 `upperCase`。
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following is a diagram of the preceding transformation:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前一个转换的图示：
- en: '![](img/00156.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00156.jpeg)'
- en: Pair RDD
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pair RDD
- en: Pair RDDs are RDDs consisting of key-value tuples which suits many use cases
    such as aggregation, sorting, and joining data. The keys and values can be simple
    types such as integers and strings or more complex types such as case classes,
    arrays, lists, and other types of collections. The key-value based extensible
    data model offers many advantages and is the fundamental concept behind the MapReduce
    paradigm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Pair RDD 是由键值对组成的 RDD，非常适合用于聚合、排序和连接数据等场景。键和值可以是简单的类型，如整数和字符串，或者更复杂的类型，如案例类、数组、列表以及其他类型的集合。基于键值的可扩展数据模型提供了许多优势，并且是
    MapReduce 范式的基本概念。
- en: Creating a `PairRDD` can be done easily by applying transformation to any RDD
    to convert the RDD to an RDD of key-value pairs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `PairRDD` 可以通过对任何 RDD 应用转换来轻松实现，将其转换为键值对的 RDD。
- en: Let's read the `statesPopulation.csv` into an RDD using the `SparkContext`,
    which is available as `sc`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `SparkContext` 将 `statesPopulation.csv` 读入 RDD，`SparkContext` 可用 `sc`
    来表示。
- en: 'The following is an example of a basic RDD of the state population and how
    `PairRDD` looks like for the same RDD splitting the records into tuples (pairs)
    of state and population:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个基本的州人口 RDD 示例，以及同一 RDD 拆分记录为州和人口的元组（对）后的 `PairRDD` 的样子：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following is a diagram of the preceding example showing how the RDD elements
    are converted to `(key - value)` pairs:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前一个示例的图示，展示了 RDD 元素如何转换为 `(key - value)` 对：
- en: '![](img/00341.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00341.jpeg)'
- en: DoubleRDD
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DoubleRDD
- en: DoubleRDD is an RDD consisting of a collection of double values. Due to this
    property, many statistical functions are available to use with the DoubleRDD.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DoubleRDD 是一个由双精度值集合构成的 RDD。由于这一特性，可以对 DoubleRDD 使用许多统计函数。
- en: 'The following are examples of DoubleRDD where we create an RDD from a sequence
    of double numbers:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 DoubleRDD 的示例，其中我们从一组双精度数字创建了一个 RDD：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is a diagram of the DoubleRDD and how you can run a `sum()` function
    on the DoubleRDD:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 DoubleRDD 的示意图，展示了如何在 DoubleRDD 上运行 `sum()` 函数：
- en: '![](img/00371.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00371.jpeg)'
- en: SequenceFileRDD
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SequenceFileRDD
- en: '`SequenceFileRDD` is created from a `SequenceFile` which is a format of files
    in the Hadoop File System. The `SequenceFile` can be compressed or uncompressed.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`SequenceFileRDD` 是从 `SequenceFile` 创建的，`SequenceFile` 是 Hadoop 文件系统中的一种文件格式。`SequenceFile`
    可以是压缩的或未压缩的。'
- en: Map Reduce processes can use SequenceFiles, which are pairs of Keys and Values.
    Key and Value are of Hadoop writable datatypes, such as Text, IntWritable, and
    so on.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Map Reduce 过程可以使用 SequenceFiles，SequenceFiles 是键和值的对。键和值是 Hadoop 可写数据类型，如 Text、IntWritable
    等。
- en: 'The following is an example of a `SequenceFileRDD`, which shows how we can
    write and read `SequenceFile`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 `SequenceFileRDD` 的示例，展示了如何写入和读取 `SequenceFile`：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is a diagram of **SequenceFileRDD** as seen in the preceding
    example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面示例中看到的 **SequenceFileRDD** 的示意图：
- en: '![](img/00013.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00013.jpeg)'
- en: CoGroupedRDD
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CoGroupedRDD
- en: '`CoGroupedRDD` is an RDD that cogroups its parents. Both parent RDDs have to
    be pairRDDs for this to work, as a cogroup essentially generates a pairRDD consisting
    of the common key and list of values from both parent RDDs. Take a look at the
    following code snippet:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`CoGroupedRDD` 是一个将其父 RDD 进行 cogroup 操作的 RDD。两个父 RDD 必须是 pairRDD 才能工作，因为 cogroup
    操作本质上会生成一个包含共同键和值列表的 pairRDD，值列表来自两个父 RDD。请看以下代码片段：'
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following is an example of a CoGroupedRDD where we create a cogroup of
    two pairRDDs, one having pairs of State, Population and the other having pairs
    of State, Year:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 CoGroupedRDD 的示例，我们创建了两个 pairRDD 的 cogroup，其中一个包含州和人口的值对，另一个包含州和年份的值对：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following is a diagram of the cogroup of **pairRDD** and **pairRDD2** by
    creating pairs of values for each key:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通过为每个键创建值对来对 **pairRDD** 和 **pairRDD2** 进行 cogroup 操作的示意图：
- en: '![](img/00179.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00179.jpeg)'
- en: ShuffledRDD
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ShuffledRDD
- en: '`ShuffledRDD` shuffles the RDD elements by key so as to accumulate values for
    the same key on the same executor to allow an aggregation or combining logic.
    A very good example is to look at what happens when `reduceByKey()` is called
    on a PairRDD:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`ShuffledRDD` 根据键对 RDD 元素进行洗牌，从而将相同键的值积累到同一执行器上，以便进行聚合或合并逻辑。一个很好的例子是查看在 PairRDD
    上调用 `reduceByKey()` 时发生的情况：'
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following is a `reduceByKey` operation on the `pairRDD` to aggregate the
    records by the State:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对 `pairRDD` 执行 `reduceByKey` 操作，以按州聚合记录的示例：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following diagram, is an illustration of the shuffling by Key to send the
    records of the same Key(State) to the same partitions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了根据键进行洗牌，将相同键（State）的记录发送到同一分区的过程：
- en: '![](img/00024.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00024.jpeg)'
- en: UnionRDD
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UnionRDD
- en: '`UnionRDD` is the result of a union operation of two RDDs. Union simply creates
    an RDD with elements from both RDDs as shown in the following code snippet:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`UnionRDD` 是两个 RDD 进行联合操作后的结果。联合操作仅仅是创建一个包含两个 RDD 中所有元素的新 RDD，如以下代码片段所示：'
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following diagram is an illustration of a union of two RDDs where the elements
    from both **RDD 1** and **RDD 2** are combined into a new RDD **UnionRDD**:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了两个 RDD 进行联合操作后，来自 **RDD 1** 和 **RDD 2** 的元素如何被合并到一个新的 RDD **UnionRDD**
    中：
- en: '![](img/00305.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00305.jpeg)'
- en: HadoopRDD
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HadoopRDD
- en: '`HadoopRDD` provides core functionality for reading data stored in HDFS using
    the MapReduce API from the Hadoop 1.x libraries. `HadoopRDD` is the default used
    and can be seen when loading data from any file system into an RDD:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`HadoopRDD` 提供了从 Hadoop 1.x 库中的 MapReduce API 读取存储在 HDFS 中的数据的核心功能。`HadoopRDD`
    是默认使用的，当从任何文件系统加载数据到 RDD 时，可以看到它：'
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When loading the state population records from the CSV, the underlying base
    RDD is actually `HadoopRDD` as in the following code snippet:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当从 CSV 文件加载州人口记录时，底层的基础 RDD 实际上是 `HadoopRDD`，如以下代码片段所示：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following diagram is an illustration of a **HadoopRDD** created by loading
    a textfile from the file system into an RDD:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了通过将文本文件从文件系统加载到 RDD 中创建 **HadoopRDD** 的示例：
- en: '![](img/00032.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00032.jpeg)'
- en: NewHadoopRDD
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NewHadoopRDD
- en: '`NewHadoopRDD` provides core functionality for reading data stored in HDFS,
    HBase tables, Amazon S3 using the new MapReduce API from Hadoop 2.x `libraries.NewHadoopRDD`
    can read from many different formats thus is used to interact with several external
    systems.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`NewHadoopRDD` 提供了读取存储在 HDFS、HBase 表、Amazon S3 中的数据的核心功能，使用的是来自 Hadoop 2.x
    的新 MapReduce API。`libraries.NewHadoopRDD` 可以读取多种不同格式的数据，因此它可以与多个外部系统进行交互。'
- en: Prior to `NewHadoopRDD`, `HadoopRDD` was the only available option which used
    the old MapReduce API from Hadoop 1.x
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `NewHadoopRDD` 之前，`HadoopRDD` 是唯一可用的选项，它使用的是 Hadoop 1.x 中的旧 MapReduce API。
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The simplest example is to use SparkContext''s `wholeTextFiles` function to
    create `WholeTextFileRDD`. Now, `WholeTextFileRDD` actually extends `NewHadoopRDD`
    as shown in the following code snippet:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的例子是使用 SparkContext 的 `wholeTextFiles` 函数来创建 `WholeTextFileRDD`。现在，`WholeTextFileRDD`
    实际上扩展了 `NewHadoopRDD`，如下所示的代码片段：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s look at another example where we will use the function `newAPIHadoopFile`
    using the `SparkContext`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看另一个例子，在这个例子中，我们将使用 `SparkContext` 中的 `newAPIHadoopFile` 函数：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Aggregations
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合
- en: Aggregation techniques allow you to combine the elements in the RDD in arbitrary
    ways to perform some computation. In fact, aggregation is the most important part
    of big data analytics. Without aggregation, we would not have any way to generate
    reports and analysis like *Top States by Population*, which seems to be a logical
    question asked when given a dataset of all State populations for the past 200
    years. Another simpler example is that of a need to just count the number of elements
    in the RDD, which asks the executors to count the number of elements in each partition
    and send to the Driver, which then adds the subsets to compute the total number
    of elements in the RDD.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合技术允许你以任意方式组合 RDD 中的元素来执行某些计算。事实上，聚合是大数据分析中最重要的部分。如果没有聚合，我们就无法生成报告和分析，例如 *按人口最多的州*，这似乎是给定过去
    200 年所有州人口数据集时提出的一个逻辑问题。另一个简单的例子是需要计算 RDD 中元素的数量，这要求执行器计算每个分区中的元素数量，并将其发送给 Driver，Driver
    再将这些子集相加，从而计算 RDD 中的元素总数。
- en: In this section, our primary focus is on the aggregation functions used to collect
    and combine data by key. As seen earlier in this chapter, a PairRDD is an RDD
    of (key - value) pairs where key and value are arbitrary and can be customized
    as per the use case.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们的主要焦点是聚合函数，这些函数用于通过键收集和合并数据。正如本章前面所看到的，PairRDD 是一个 (key - value) 对的 RDD，其中
    key 和 value 是任意的，可以根据具体应用场景进行自定义。
- en: In our example of state populations, a PairRDD could be the pairs of `<State,
    <Population, Year>>` which means `State` is taken as the key and the tuple `<Population,
    Year>` is considered the value. This way of breaking down the key and value can
    generate aggregations such as *Top Years by Population per State*. On the contrary,
    in case our aggregations are done around Year say *Top States by Population per
    Year*, we can use a `pairRDD` of pairs of `<Year, <State, Population>>`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们关于州人口的例子中，PairRDD 可以是 `<State, <Population, Year>>` 的对，这意味着 `State` 被作为键，`<Population,
    Year>` 的元组被作为值。这种将键和值分解的方式可以生成如 *按州的人口最多年份* 等聚合结果。相反，如果我们的聚合是围绕年份进行的，例如 *按年份的人口最多的州*，我们可以使用一个
    `pairRDD`，其中包含 `<Year, <State, Population>>` 的对。
- en: 'The following is the sample code to generate a `pairRDD` from the `StatePopulation`
    dataset both with `State` as the key as well as the `Year` as the key:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成 `pairRDD` 的示例代码，数据来源于 `StatePopulation` 数据集，既有以 `State` 作为键，也有以 `Year`
    作为键的情况：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we can generate a `pairRDD` using `State` as the key and a tuple of `<Year,
    Population>` as the value as shown in the following code snippet:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以生成一个 `pairRDD`，使用 `State` 作为键，`<Year, Population>` 的元组作为值，如以下代码片段所示：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As mentioned earlier, we can also generate a `PairRDD` using `Year` as the
    key and a tuple of `<State, Population>` as the value as shown in the following
    code snippet:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们还可以使用 `Year` 作为键，`<State, Population>` 的元组作为值，生成一个 `PairRDD`，如以下代码片段所示：
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will now look into how we can use the common aggregation functions on the
    `pairRDD` of `<State, <Year, Population>>`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探讨如何在 `<State, <Year, Population>>` 的 `pairRDD` 上使用常见的聚合函数：
- en: '`groupByKey`'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groupByKey`'
- en: '`reduceByKey`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduceByKey`'
- en: '`aggregateByKey`'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregateByKey`'
- en: '`combineByKey`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combineByKey`'
- en: groupByKey
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: groupByKey
- en: '`groupByKey` groups the values for each key in the RDD into a single sequence.
    `groupByKey` also allows controlling the partitioning of the resulting key-value
    pair RDD by passing a partitioner. By default, a `HashPartitioner` is used but
    a custom partitioner can be given as an argument. The ordering of elements within
    each group is not guaranteed, and may even differ each time the resulting RDD
    is evaluated.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`将RDD中每个键的所有值组合成一个单一的序列。`groupByKey`还允许通过传递分区器来控制生成的键值对RDD的分区。默认情况下，使用`HashPartitioner`，但可以作为参数传入自定义分区器。每个组内元素的顺序无法保证，甚至每次评估结果RDD时可能会不同。'
- en: '`groupByKey` is an expensive operation due to all the data shuffling needed.
    `reduceByKey` or `aggregateByKey` provide much better performance. We will look
    at this later in this section.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`是一个代价高昂的操作，因为需要大量的数据洗牌。`reduceByKey`或`aggregateByKey`提供了更好的性能。我们将在本节稍后讨论这一点。'
- en: '`groupByKey` can be invoked either using a custom partitioner or just using
    the default `HashPartitioner` as shown in the following code snippet:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`可以通过使用自定义分区器或直接使用默认的`HashPartitioner`来调用，如以下代码片段所示：'
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As currently implemented, `groupByKey` must be able to hold all the key-value
    pairs for any key in memory. If a key has too many values, it can result in an
    `OutOfMemoryError`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 按照当前的实现，`groupByKey`必须能够在内存中保存任何键的所有键值对。如果某个键有太多值，就可能导致`OutOfMemoryError`。
- en: '`groupByKey` works by sending all elements of the partitions to the partition
    based on the partitioner so that all pairs of (key - value) for the same key are
    collected in the same partition. Once this is done, the aggregation operation
    can be done easily.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`通过将所有分区的元素发送到基于分区器的分区中，从而将相同键的所有(key-value)对收集到同一个分区中。一旦完成，就可以轻松地进行聚合操作。'
- en: 'Shown here is an illustration of what happens when `groupByKey` is called:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是调用`groupByKey`时发生情况的示意图：
- en: '![](img/00036.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00036.jpeg)'
- en: reduceByKey
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: reduceByKey
- en: '`groupByKey` involves a lot of shuffling and `reduceByKey` tends to improve
    the performance by not sending all elements of the `PairRDD` using shuffles, rather
    using a local combiner to first do some basic aggregations locally and then send
    the resultant elements as in `groupByKey`. This greatly reduces the data transferred,
    as we don''t need to send everything over. `reduceBykey` works by merging the
    values for each key using an associative and commutative reduce function. Of course,
    first, this will'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`涉及大量的数据洗牌，而`reduceByKey`通过不使用洗牌将所有`PairRDD`的元素发送，而是使用本地的combiner先在本地做一些基本聚合，然后再像`groupByKey`那样发送结果元素。这样大大减少了数据传输量，因为我们不需要传输所有内容。`reduceBykey`通过使用结合性和交换性的reduce函数合并每个键的值来工作。当然，首先会...'
- en: also perform the merging locally on each mapper before sending results to a
    reducer.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个映射器上本地执行合并操作，然后将结果发送到归约器。
- en: If you are familiar with Hadoop MapReduce, this is very similar to a combiner
    in MapReduce programming.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉Hadoop MapReduce，这与MapReduce编程中的combiner非常相似。
- en: '`reduceByKey` can be invoked either using a custom partitioner or just using
    the default HashPartitioner as shown in the following code snippet:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`可以通过使用自定义分区器或直接使用默认的`HashPartitioner`来调用，如以下代码片段所示：'
- en: '[PRE24]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`reduceByKey` works by sending all elements of the partitions to the partition
    based on the `partitioner` so that all pairs of (key - value) for the same Key
    are collected in the same partition. But before the shuffle, local aggregation
    is also done reducing the data to be shuffled. Once this is done, the aggregation
    operation can be done easily in the final partition.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`通过根据`partitioner`将所有分区的元素发送到指定的分区，以便将相同键的所有(key-value)对收集到同一个分区。但在洗牌之前，首先会进行本地聚合，减少需要洗牌的数据量。一旦完成，最终分区中就可以轻松地进行聚合操作。'
- en: 'The following diagram is an illustration of what happens when `reduceBykey`
    is called:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了调用`reduceBykey`时发生的情况：
- en: '![](img/00039.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00039.jpeg)'
- en: aggregateByKey
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: aggregateByKey
- en: '`aggregateByKey` is quite similar to `reduceByKey`, except that `aggregateByKey`
    allows more flexibility and customization of how to aggregate within partitions
    and between partitions to allow much more sophisticated use cases such as generating
    a list of all `<Year, Population>` pairs as well as total population for each
    State in one function call.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregateByKey`与`reduceByKey`非常相似，唯一不同的是`aggregateByKey`在分区内和分区之间聚合时提供了更多的灵活性和定制性，允许处理更复杂的用例，例如在一次函数调用中生成所有`<Year,
    Population>`对以及每个州的总人口。'
- en: '`aggregateByKey` works by aggregating the values of each key, using given combine
    functions and a neutral initial/zero value.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregateByKey` 通过使用给定的合并函数和中立的初始/零值来聚合每个键的值。'
- en: 'This function can return a different result type, `U`, than the type of the
    values in this RDD `V`, which is the biggest difference. Thus, we need one operation
    for merging a `V` into a `U` and one operation for merging two `U`''s. The former
    operation is used for merging values within a partition, and the latter is used
    for merging values between partitions. To avoid memory allocation, both of these
    functions are allowed to modify and return their first argument instead of creating
    a new `U`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数可以返回不同的结果类型 `U`，而不是此 RDD 中值的类型 `V`，这是最大的区别。因此，我们需要一个操作将 `V` 合并成 `U`，另一个操作用于合并两个
    `U`。前者操作用于合并分区内的值，后者用于合并分区间的值。为了避免内存分配，允许这两个函数修改并返回其第一个参数，而不是创建一个新的 `U`：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`aggregateByKey` works by performing an aggregation within the partition operating
    on all elements of each partition and then applies another aggregation logic when
    combining the partitions themselves. Ultimately, all pairs of (key - value) for
    the same Key are collected in the same partition; however, the aggregation as
    to how it is done and the output generated is not fixed as in `groupByKey` and
    `reduceByKey`, but is more flexible and customizable when using `aggregateByKey`.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregateByKey` 通过在分区内执行聚合操作，作用于每个分区的所有元素，然后在合并分区时应用另一种聚合逻辑来工作。最终，所有相同 Key
    的 (key - value) 对都会收集到同一个分区中；然而，聚合的方式以及生成的输出不像 `groupByKey` 和 `reduceByKey` 那样固定，而是使用
    `aggregateByKey` 时更加灵活和可定制的。'
- en: 'The following diagram is an illustration of what happens when `aggregateByKey`
    is called. Instead of adding up the counts as in `groupByKey` and `reduceByKey`,
    here we are generating lists of values for each Key:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了调用 `aggregateByKey` 时发生的情况。与 `groupByKey` 和 `reduceByKey` 中将计数相加不同，在这里我们为每个
    Key 生成值的列表：
- en: '![](img/00043.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00043.jpeg)'
- en: combineByKey
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: combineByKey
- en: '`combineByKey` is very similar to `aggregateByKey`; in fact, `combineByKey`
    internally invokes `combineByKeyWithClassTag`, which is also invoked by `aggregateByKey`.
    As in `aggregateByKey`, the `combineByKey` also works by applying an operation
    within each partition and then between combiners.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey` 和 `aggregateByKey` 非常相似；实际上，`combineByKey` 内部调用了 `combineByKeyWithClassTag`，而
    `aggregateByKey` 也会调用它。和 `aggregateByKey` 一样，`combineByKey` 也是通过在每个分区内应用操作，然后在合并器之间进行操作来工作的。'
- en: '`combineByKey` turns an `RDD[K,V]` into an `RDD[K,C]`, where `C` is a list
    of Vs collected or combined under the name key `K`.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey` 将 `RDD[K,V]` 转换为 `RDD[K,C]`，其中 `C` 是在键 `K` 下收集或合并的 `V` 列表。'
- en: There are three functions expected when you call combineByKey.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `combineByKey` 时，期望有三个函数。
- en: '`createCombiner`, which turns a `V` into `C`, which is a one element list'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createCombiner` 将 `V` 转换为 `C`，其中 `C` 是一个包含一个元素的列表'
- en: '`mergeValue` to merge a `V` into a `C` by appending the `V` to the end of the
    list'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mergeValue` 用于将 `V` 合并为 `C`，通过将 `V` 附加到列表末尾'
- en: '`mergeCombiners` to combine two Cs into one'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mergeCombiners` 用于将两个 `C` 合并为一个'
- en: In `aggregateByKey`, the first argument is simply a zero value but in `combineByKey`,
    we provide the initial function which takes the current value as a parameter.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `aggregateByKey` 中，第一个参数只是一个零值，但在 `combineByKey` 中，我们提供了一个初始函数，该函数将当前值作为参数。
- en: '`combineByKey` can be invoked either using a custom partitioner or just using
    the default HashPartitioner as shown in the following code snippet:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey` 可以通过自定义分区器调用，也可以像以下代码片段那样使用默认的 HashPartitioner：'
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`combineByKey` works by performing an aggregation within the partition operating
    on all elements of each partition and then applies another aggregation logic when
    combining the partitions themselves. Ultimately, all pairs of (key - value) for
    the same Key are collected in the same partition however the aggregation as to
    how it is done and the output generated is not fixed as in `groupByKey` and `reduceByKey`,
    but is more flexible and customizable when using `combineByKey`.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey` 通过在分区内执行聚合操作，作用于每个分区的所有元素，然后在合并分区时应用另一种聚合逻辑来工作。最终，所有相同 Key 的
    (key - value) 对都将收集到同一个分区中，但聚合的方式以及生成的输出不像 `groupByKey` 和 `reduceByKey` 那样固定，而是更加灵活和可定制的。'
- en: 'The following diagram is an illustration of what happens when `combineBykey`
    is called:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了调用 `combineByKey` 时发生的情况：
- en: '![](img/00045.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00045.jpeg)'
- en: Comparison of groupByKey, reduceByKey, combineByKey, and aggregateByKey
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`groupByKey`、`reduceByKey`、`combineByKey` 和 `aggregateByKey` 的比较'
- en: Let's consider the example of StatePopulation RDD generating a `pairRDD` of
    `<State, <Year, Population>>`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个 StatePopulation RDD 的例子，它生成一个 `<State, <Year, Population>>` 的 `pairRDD`。
- en: '`groupByKey` as seen in the preceding section will do `HashPartitioning` of
    the `PairRDD` by generating a hashcode of the keys and then shuffling the data
    to collect the values for each key in the same partition. This obviously results
    in too much shuffling.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所见，`groupByKey` 会通过生成键的哈希码进行 `HashPartitioning`，然后洗牌数据，将每个键的值收集到同一个分区中。这显然会导致过多的洗牌。
- en: '`reduceByKey` improves upon `groupByKey` using a local combiner logic to minimize
    the data sent in a shuffle phase. The result will be the same as `groupByKey`,
    but will be much more performant.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey` 通过使用本地合并器逻辑改进了 `groupByKey`，从而减少了在洗牌阶段发送的数据量。结果与 `groupByKey`
    相同，但性能更好。'
- en: '`aggregateByKey` is very similar to `reduceByKey` in how it works but with
    one big difference, which makes it the most powerful one among the three. `aggregateBykey`
    does not need to operate on the same datatype and can do different aggregation
    within the partition and do a different aggregation between partitions.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregateByKey` 的工作方式与 `reduceByKey` 非常相似，但有一个重大区别，这使得它在三者中最为强大。`aggregateByKey`
    不需要在相同的数据类型上操作，并且可以在分区内进行不同的聚合，同时在分区之间也可以进行不同的聚合。'
- en: '`combineByKey` is very similar in performance to `aggregateByKey` except for
    the initial function to create the combiner.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey` 在性能上与 `aggregateByKey` 非常相似，除了用于创建合并器的初始函数不同。'
- en: The function to use depends on your use case but when in doubt just refer to
    this section on *Aggregation* to choose the right function for your use case.
    Also, pay close attention to the next section as *Partitioning and shuffling*
    are covered in that section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哪个函数取决于你的用例，但如果不确定，请参考本节的 *聚合* 部分，以选择适合你用例的正确函数。另外，密切关注下一部分，因为 *分区和洗牌* 会在其中讨论。
- en: The following is the code showing all four ways of calculating total population
    by state.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是显示四种按州计算总人口的方法的代码。
- en: '**Step 1\. Initialize the RDD:**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1\. 初始化 RDD：**'
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Step 2\. Convert to pair RDD:**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2\. 转换为 pair RDD：**'
- en: '[PRE28]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Step 3\. groupByKey - Grouping the values and then adding up populations:**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3\. groupByKey - 对值进行分组，然后加总人口数：**'
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Step 4\. reduceByKey - Reduce the values by key simply adding the populations:**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4\. reduceByKey - 简单地通过添加人口数来减少每个键的值：**'
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Step 5\. aggregateBykey - aggregate the populations under each key and adds
    them up:**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5\. aggregateByKey - 对每个键下的人口进行聚合并加总：**'
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Step 6\. combineByKey - combine within partitions and then merging combiners:**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 6\. combineByKey - 在分区内进行合并，然后合并合并器：**'
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you see, all four aggregations result in the same output. It's just how they
    work that is different.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，所有四种聚合方法都得到了相同的结果。只是它们的工作方式不同。
- en: Partitioning and shuffling
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区与洗牌
- en: We have seen how Apache Spark can handle distributed computing much better than
    Hadoop. We also saw the inner workings, mainly the fundamental data structure
    known as **Resilient Distributed Dataset** (**RDD**). RDDs are immutable collections
    representing datasets and have the inbuilt capability of reliability and failure
    recovery. RDDs operate on data not as a single blob of data, rather RDDs manage
    and operate data in partitions spread across the cluster. Hence, the concept of
    data partitioning is critical to the proper functioning of Apache Spark Jobs and
    can have a big effect on the performance as well as how the resources are utilized.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到 Apache Spark 如何比 Hadoop 更好地处理分布式计算。我们还了解了它的内部工作原理，主要是被称为 **弹性分布式数据集**（**RDD**）的基本数据结构。RDD
    是不可变的集合，表示数据集，具有内置的可靠性和故障恢复能力。RDD 操作数据时，并非作为单一的整体数据，而是以分区的方式在集群中管理和操作数据。因此，数据分区的概念对
    Apache Spark 作业的正常运行至关重要，并且会对性能以及资源利用产生重要影响。
- en: RDD consists of partitions of data and all operations are performed on the partitions
    of data in the RDD. Several operations like transformations are functions executed
    by an executor on the specific partition of data being operated on. However, not
    all operations can be done by just performing isolated operations on the partitions
    of data by the respective executors. Operations like aggregations (seen in the
    preceding section) require data to be moved across the cluster in a phase known
    as **shuffling**. In this section, we will look deeper into the concepts of partitioning
    and shuffling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 由数据的分区组成，所有操作都在 RDD 的数据分区上执行。像转换这样的操作是由执行器在特定数据分区上执行的函数。然而，并非所有操作都可以通过仅在相应执行器上对数据分区执行孤立操作来完成。像聚合（在前面的章节中提到的）这样的操作需要数据在集群中移动，这一过程称为
    **洗牌**。在本节中，我们将深入探讨分区和洗牌的概念。
- en: Let's start looking at a simple RDD of integers by executing the following code.
    Spark Context's `parallelize` function creates an RDD from the Sequence of integers.
    Then, using the `getNumPartitions()` function, we can get the number of partitions
    of this RDD.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行以下代码来查看一个简单的整数 RDD。Spark Context 的 `parallelize` 函数从整数序列创建一个 RDD。然后，使用
    `getNumPartitions()` 函数，我们可以获得这个 RDD 的分区数。
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The RDD can be visualized as shown in the following diagram, which shows the
    8 partitions in the RDD:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，可以将 RDD 可视化，图中展示了 RDD 中的 8 个分区：
- en: '![](img/00136.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00136.jpeg)'
- en: The number of partitions is important because this number directly influences
    the number of tasks that will be running RDD transformations. If the number of
    partitions is too small, then we will use only a few CPUs/cores on a lot of data
    thus having a slower performance and leaving the cluster underutilized. On the
    other hand, if the number of partitions is too large then you will use more resources
    than you actually need and in a multi-tenant environment could be causing starvation
    of resources for other Jobs being run by you or others in your team.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 分区数量非常重要，因为这个数量直接影响将运行 RDD 转换的任务数。如果分区数量太小，那么我们会在大量数据上仅使用少数 CPU/核心，导致性能变慢并使集群资源未得到充分利用。另一方面，如果分区数量太大，那么你将使用比实际需要更多的资源，并且在多租户环境中，可能会导致其他作业（无论是你自己还是团队中的其他人）资源不足。
- en: Partitioners
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区器
- en: Partitioning of RDDs is done by partitioners. Partitioners assign a partition
    index to the elements in the RDD. All elements in the same partition will have
    the same partition index.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 的分区是通过分区器来完成的。分区器为 RDD 中的元素分配一个分区索引。同一分区中的所有元素将具有相同的分区索引。
- en: Spark comes with two partitioners the `HashPartitioner` and the `RangePartitioner`.
    In addition to these, you can also implement a custom partitioner.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了两种分区器：`HashPartitioner` 和 `RangePartitioner`。除了这两种，你还可以实现一个自定义的分区器。
- en: HashPartitioner
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哈希分区器
- en: '`HashPartitioner` is the default partitioner in Spark and works by calculating
    a hash value for each key of the RDD elements. All the elements with the same
    hashcode end up in the same partition as shown in the following code snippet:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`HashPartitioner` 是 Spark 中的默认分区器，通过计算每个 RDD 元素键的哈希值来工作。所有具有相同哈希值的元素将被分配到相同的分区，如以下代码片段所示：'
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following is an example of the String `hashCode()` function and how we
    can generate `partitionIndex`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是字符串 `hashCode()` 函数的示例，以及我们如何生成 `partitionIndex`：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The default number of partitions is either from the Spark configuration parameter
    `spark.default.parallelism` or the number of cores in the cluster
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的分区数量要么来自 Spark 配置参数 `spark.default.parallelism`，要么来自集群中的核心数。
- en: 'The following diagram is an illustration of how hash partitioning works. We
    have an RDD with 3 elements **a**, **b**, and **e**. Using String hashcode we
    get the `partitionIndex` for each element based on the number of partitions set
    at 6:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了哈希分区是如何工作的。我们有一个包含 **a**、**b** 和 **e** 三个元素的 RDD。通过使用字符串的 hashcode，我们可以根据设置的
    6 个分区数量计算每个元素的 `partitionIndex`：
- en: '![](img/00140.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00140.jpeg)'
- en: RangePartitioner
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 范围分区器
- en: '`RangePartitioner` works by partitioning the RDD into roughly equal ranges.
    Since the range has to know the starting and ending keys for any partition, the
    RDD needs to be sorted first before a `RangePartitioner` can be used.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`RangePartitioner` 通过将 RDD 分成大致相等的区间来工作。由于区间需要知道每个分区的起始和结束键，因此在使用 `RangePartitioner`
    之前，RDD 需要先进行排序。'
- en: '`RangePartitioning` first needs reasonable boundaries for the partitions based
    on the RDD and then create a function from key K to the `partitionIndex` where
    the element belongs. Finally, we need to repartition the RDD, based on the `RangePartitioner`
    to distribute the RDD elements correctly as per the ranges we determined.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`RangePartitioning`首先需要根据RDD为分区设定合理的边界，然后创建一个从键K到`partitionIndex`（元素所在分区的索引）之间的函数。最后，我们需要根据`RangePartitioner`重新分区RDD，以根据我们确定的范围正确地分配RDD元素。'
- en: 'The following is an example of how we can use `RangePartitioning` of a `PairRDD`.
    We also can see how the partitions changed after we repartition the RDD using
    a `RangePartitioner`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用`PairRDD`的`RangePartitioning`的示例。我们还可以看到，在使用`RangePartitioner`对RDD重新分区后，分区如何发生变化：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following diagram is an illustration of the `RangePartitioner` as seen
    in the preceding example:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了前面示例中提到的`RangePartitioner`：
- en: '![](img/00143.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00143.jpeg)'
- en: Shuffling
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 洗牌（Shuffling）
- en: Whatever the partitioner used, many operations will cause a repartitioning of
    data across the partitions of an RDD. New partitions can be created or several
    partitions can be collapsed/coalesced. All the data movement necessary for the
    repartitioning is called **shuffling,** and this is an important concept to understand
    when writing a Spark Job. The shuffling can cause a lot of performance lag as
    the computations are no longer in memory on the same executor but rather the executors
    are exchanging data over the wire.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用何种分区器，许多操作都会导致数据在RDD的分区之间重新分配。新分区可以被创建，或者多个分区可以被合并。所有用于重新分区所需的数据移动过程都被称为**洗牌（Shuffling）**，这是编写Spark作业时需要理解的一个重要概念。洗牌可能会导致很大的性能延迟，因为计算不再保存在同一执行器的内存中，而是执行器通过网络交换数据。
- en: A good example is the example of `groupByKey()`, we saw earlier in the *Aggregations*
    section. Obviously, lot of data was flowing between executors to make sure all
    values for a key are collected onto the same executor to perform the `groupBy`
    operation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的例子是我们在*聚合*（Aggregations）部分中看到的`groupByKey()`示例。显然，为了确保所有相同键的值都收集到同一个执行器上以执行`groupBy`操作，大量的数据在执行器之间流动。
- en: Shuffling also determines the Spark Job execution process and influences how
    the Job is split into Stages. As we have seen in this chapter and the previous
    chapter, Spark holds a DAG of RDDs, which represent the lineage of the RDDs such
    that not only does Spark use the lineage to plan the execution of the job but
    also any loss of executors can be recovered from. When an RDD is undergoing a
    transformation, an attempt is made to make sure the operations are performed on
    the same node as the data. However, often we use join operations, reduce, group,
    or aggregate operations among others, which cause repartitioning intentionally
    or unintentionally. This shuffling in turn determines where a particular stage
    in the processing has ended and a new stage has begun.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 洗牌（Shuffling）还决定了Spark作业的执行过程，并影响作业如何被拆分成阶段（Stages）。正如我们在本章和上一章中所看到的，Spark持有一个RDD的有向无环图（DAG），该图表示了RDD的血统关系，Spark不仅利用这个血统来规划作业的执行，还可以从任何执行器丢失中恢复。当一个RDD正在进行转换时，系统会尽力确保操作在与数据相同的节点上执行。然而，我们经常使用连接操作（join）、聚合（reduce）、分组（group）或其他聚合操作，这些操作往往会有意或无意地导致重新分区。这个洗牌过程反过来决定了数据处理中的某一阶段何时结束以及新的阶段何时开始。
- en: 'The following diagram is an illustration of how a Spark Job is split into stages.
    This example shows a `pairRDD` being filtered, transformed using map before invoking
    `groupByKey` followed by one last transformation using `map()`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了Spark作业如何被拆分成多个阶段。这个示例展示了一个`pairRDD`在执行`groupByKey`之前，先经过过滤和使用map转换的过程，最后再通过`map()`进行一次转换：
- en: '![](img/00147.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00147.jpeg)'
- en: The more shuffling we have, the more stages occur in the job execution affecting
    the performance. There are two key aspects which are used by Spark Driver to determine
    the stages. This is done by defining two types of dependencies of the RDDs, the
    narrow dependencies and the wide dependencies.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的洗牌操作越多，作业执行过程中就会有更多的阶段，从而影响性能。Spark Driver使用两个关键方面来确定这些阶段。这是通过定义RDD的两种依赖关系类型来完成的，分别是窄依赖（narrow
    dependencies）和宽依赖（wide dependencies）。
- en: Narrow Dependencies
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窄依赖（Narrow Dependencies）
- en: When an RDD can be derived from another RDD using a simple one-to-one transformation
    such as a `filter()` function, `map()` function, `flatMap()` function, and so
    on, then the child RDD is said to depend on the parent RDD on a one-to-one basis.
    This dependency is known as narrow dependency as the data can be transformed on
    the same node as the one containing the original RDD/parent RDD partition without
    requiring any data transfer over the wire between other executors.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个 RDD 可以通过简单的一对一转换（如 `filter()` 函数、`map()` 函数、`flatMap()` 函数等）从另一个 RDD 派生时，子
    RDD 被认为是基于一对一关系依赖于父 RDD。这种依赖关系被称为窄依赖，因为数据可以在包含原始 RDD/父 RDD 分区的同一节点上进行转换，而不需要通过其他执行器之间的网络传输任何数据。
- en: Narrow dependencies are in the same stage of the job execution.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 窄依赖处于作业执行的同一阶段。
- en: 'The following diagram is an illustration of how a narrow dependency transforms
    one RDD to another RDD, applying one-to-one transformation on the RDD elements:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了窄依赖如何将一个 RDD 转换为另一个 RDD，并对 RDD 元素应用一对一转换：
- en: '![](img/00152.jpeg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00152.jpeg)'
- en: Wide Dependencies
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 宽依赖
- en: When an RDD can be derived from one or more RDDs by transferring data over the
    wire or exchanging data to repartition or redistribute the data using functions,
    such as `aggregateByKey`, `reduceByKey` and so on, then the child RDD is said
    to depend on the parent RDDs participating in a shuffle operation. This dependency
    is known as a Wide dependency as the data cannot be transformed on the same node
    as the one containing the original RDD/parent RDD partition thus requiring data
    transfer over the wire between other executors.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个 RDD 可以通过在网络上传输数据或交换数据来重新分区或重新分发数据（使用函数，如 `aggregateByKey`、`reduceByKey`
    等）从一个或多个 RDD 派生时，子 RDD 被认为依赖于参与 shuffle 操作的父 RDD。这种依赖关系被称为宽依赖，因为数据不能在包含原始 RDD/父
    RDD 分区的同一节点上进行转换，因此需要通过其他执行器之间的网络传输数据。
- en: Wide dependencies introduce new stages in the job execution.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 宽依赖会在作业执行过程中引入新的阶段。
- en: 'The following diagram is an illustration of how wide dependency transforms
    one RDD to another RDD shuffling data between executors:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了宽依赖如何将一个 RDD 转换为另一个 RDD，并在执行器之间进行数据交换：
- en: '![](img/00155.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00155.jpeg)'
- en: Broadcast variables
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播变量
- en: Broadcast variables are shared variables across all executors. Broadcast variables
    are created once in the Driver and then are read only on executors. While it is
    simple to understand simple datatypes broadcasted, such as an `Integer`, broadcast
    is much bigger than simple variables conceptually. Entire datasets can be broadcasted
    in a Spark cluster so that executors have access to the broadcasted data. All
    the tasks running within an executor all have access to the broadcast variables.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量是跨所有执行器共享的变量。广播变量在驱动程序中创建一次，然后在执行器中只读。虽然广播简单数据类型（如 `Integer`）是易于理解的，但广播的概念远不止于简单变量。整个数据集可以在
    Spark 集群中进行广播，以便执行器能够访问广播的数据。所有在执行器中运行的任务都可以访问广播变量。
- en: Broadcast uses various optimized methods to make the broadcasted data accessible
    to all executors. This is an important challenge to solve as if the size of the
    datasets broadcasted is significant, you cannot expect 100s or 1000s of executors
    to connect to the Driver and pull the dataset. Rather, the executors pull the
    data via HTTP connection and the more recent addition which is similar to BitTorrent
    where the dataset itself is distributed like a torrent amongst the cluster. This
    enables a much more scalable method to distribute the broadcasted variables to
    all executors rather than having each executor pull the data from the Driver one
    by one which can cause failures on the Driver when you have a lot of executors.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 广播使用各种优化方法使广播的数据对所有执行器可用。这是一个需要解决的重要挑战，因为如果广播的数据集的大小很大，你不能指望数百或数千个执行器连接到驱动程序并拉取数据集。相反，执行器通过
    HTTP 连接拉取数据，最新的方式类似于 BitTorrent，其中数据集像种子一样在集群中分发。这使得广播变量的分发方式更加可扩展，而不是让每个执行器逐个从驱动程序拉取数据，这可能会导致当执行器数量较多时，驱动程序出现故障。
- en: The driver can only broadcast the data it has and you cannot broadcast RDDs
    by using references. This is because only Driver knows how to interpret RDDs and
    executors only know the particular partitions of data they are handling.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序只能广播它所拥有的数据，不能通过引用广播 RDD。这是因为只有驱动程序知道如何解释 RDD，而执行器只知道它们所处理的特定数据分区。
- en: If you look deeper into how broadcast works, you will see that the mechanism
    works by first having the Driver divide the serialized object into small chunks
    and then stores those chunks in the BlockManager of the driver. When the code
    is serialized to be run on the executors, then each executor first attempts to
    fetch the object from its own internal BlockManager. If the broadcast variable
    was fetched before, it will find it and use it. However, if it does not exist,
    the executor then uses remote fetches to fetch the small chunks from the driver
    and/or other executors if available. Once it gets the chunks, it puts the chunks
    in its own BlockManager, ready for any other executors to fetch from. This prevents
    the driver from being the bottleneck in sending out multiple copies of the broadcast
    data (one per executor).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果深入了解广播的工作原理，会发现机制是首先由 Driver 将序列化对象分割成小块，然后将这些小块存储在 Driver 的 BlockManager
    中。当代码被序列化并在执行器上运行时，每个执行器首先尝试从自己内部的 BlockManager 获取对象。如果广播变量已经被获取，它会找到并使用该变量。如果不存在，执行器会通过远程获取来从
    Driver 和/或其他执行器拉取小块。一旦获取到小块，它会将这些小块存储到自己的 BlockManager 中，准备供其他执行器使用。这可以防止 Driver
    成为发送多个广播数据副本（每个执行器一个副本）的瓶颈。
- en: 'The following diagram is an illustration of how broadcast works in a Spark
    cluster:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示演示了广播在 Spark 集群中的工作原理：
- en: '![](img/00006.jpeg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00006.jpeg)'
- en: Broadcast variables can be both created and destroyed too. We will look into
    the creation and destruction of broadcast variables. There is also a way to remove
    broadcasted variables from memory which we will also look at.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量既可以创建，也可以销毁。我们将探讨广播变量的创建与销毁方法。此外，我们还将讨论如何从内存中移除广播变量。
- en: Creating broadcast variables
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建广播变量
- en: Creating a broadcast variable can be done using the Spark Context's `broadcast()`
    function on any data of any data type provided that the data/variable is serializable.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 创建广播变量可以使用 Spark Context 的 `broadcast()` 函数，适用于任何数据类型的可序列化数据/变量。
- en: 'Let''s look at how we can broadcast an Integer variable and then use the broadcast
    variable inside a transformation operation executed on the executors:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何广播一个 Integer 变量，并在执行器上执行的转换操作中使用该广播变量：
- en: '[PRE37]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Broadcast variables can also be created on more than just primitive data types
    as shown in the next example where we will broadcast a `HashMap` from the Driver.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量不仅可以在原始数据类型上创建，如下一个示例所示，我们将从 Driver 广播一个 `HashMap`。
- en: 'The following is a simple transformation of an integer RDD by multiplying each
    element with another integer by looking up the HashMap. The RDD of 1,2,3 is transformed
    to 1 X 2 , 2 X 3, 3 X 4 = 2,6,12 :'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的整数 RDD 转换示例，通过查找 HashMap，将每个元素与另一个整数相乘。RDD 1,2,3 被转换为 1 X 2 , 2 X 3,
    3 X 4 = 2,6,12：
- en: '[PRE38]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Cleaning broadcast variables
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理广播变量
- en: Broadcast variables do occupy memory on all executors and depending on the size
    of the data contained in the broadcasted variable, this could cause resource issues
    at some point. There is a way to remove broadcasted variables from the memory
    of all executors.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量会占用所有执行器的内存，且根据广播变量中包含的数据大小，这可能会在某个时刻导致资源问题。确实有方法可以从所有执行器的内存中移除广播变量。
- en: Calling `unpersist()` on a broadcast variable removed the data of the broadcast
    variable from the memory cache of all executors to free up resources. If the variable
    is used again, then the data is retransmitted to the executors in order for it
    to be used again. The Driver, however, holds onto the memory as if the Driver
    does not have the data, then broadcast variable is no longer valid.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对广播变量调用 `unpersist()` 会将广播变量的数据从所有执行器的内存缓存中移除，以释放资源。如果该变量再次被使用，数据会重新传输到执行器，以便再次使用。然而，Driver
    会保留这部分内存，因为如果 Driver 没有数据，广播变量就不再有效。
- en: We look at destroying broadcast variables next.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何销毁广播变量。
- en: The following is an example of how `unpersist()` can be invoked on a broadcast
    variable. After calling `unpersist` if we access the broadcast variable again,
    it works as usual but behind the scenes, the executors are pulling the data for
    the variable again.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在广播变量上调用 `unpersist()` 的示例。调用 `unpersist` 后，如果再次访问广播变量，它会像往常一样工作，但在幕后，执行器会重新获取该变量的数据。
- en: '[PRE39]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Destroying broadcast variables
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 销毁广播变量
- en: You can also destroy broadcast variables, completely removing them from all
    executors and the Driver too making them inaccessible. This can be quite helpful
    in managing the resources optimally across the cluster.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以销毁广播变量，完全从所有执行器和驱动程序中删除它们，使其无法访问。这在优化集群资源管理时非常有帮助。
- en: Calling `destroy()` on a broadcast variable destroys all data and metadata related
    to the specified broadcast variable. Once a broadcast variable has been destroyed,
    it cannot be used again and will have to be recreated all over again.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `destroy()` 方法销毁广播变量时，将删除与该广播变量相关的所有数据和元数据。广播变量一旦被销毁，就不能再使用，必须重新创建。
- en: 'The following is an example of destroying broadcast variables:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是销毁广播变量的示例：
- en: '[PRE40]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: If an attempt is made to use a destroyed broadcast variable, an exception is
    thrown
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尝试使用已销毁的广播变量，将抛出异常
- en: 'The following is an example of an attempt to reuse a destroyed broadcast variable:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是尝试重用已销毁的广播变量的示例：
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Thus, broadcast functionality can be use to greatly improve the flexibility
    and performance of Spark jobs.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，广播功能可以大大提高 Spark 作业的灵活性和性能。
- en: Accumulators
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 累加器
- en: Accumulators are shared variables across executors typically used to add counters
    to your Spark program. If you have a Spark program and would like to know errors
    or total records processed or both, you can do it in two ways. One way is to add
    extra logic to just count errors or total records, which becomes complicated when
    handling all possible computations. The other way is to leave the logic and code
    flow fairly intact and add Accumulators.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 累加器是跨执行器共享的变量，通常用于向 Spark 程序中添加计数器。如果你有一个 Spark 程序，并希望了解错误或总记录数，或者两者的数量，你可以通过两种方式来实现。一种方法是添加额外的逻辑来单独计数错误或总记录数，但当处理所有可能的计算时，这会变得复杂。另一种方法是保持逻辑和代码流程大体不变，直接添加累加器。
- en: Accumulators can only be updated by adding to the value.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 累加器只能通过累加值来更新。
- en: The following is an example of creating and using a long Accumulator using Spark
    Context and the `longAccumulator` function to initialize a newly created accumulator
    variable to zero. As the accumulator is used inside the map transformation, the
    Accumulator is incremented. At the end of the operation, the Accumulator holds
    a value of 351.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 Spark Context 创建和使用长整型累加器的示例，使用 `longAccumulator` 函数将新创建的累加器变量初始化为零。由于累加器在
    map 转换中使用，累加器的值会增加。操作结束时，累加器的值为 351。
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'There are inbuilt accumulators which can be used for many use cases:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多内置的累加器可以用于不同的使用场景：
- en: '`LongAccumulator`: for computing sum, count, and average of 64-bit integers'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LongAccumulator`：用于计算 64 位整数的总和、计数和平均值'
- en: '`DoubleAccumulator`: for computing sum, count, and averages for double precision
    floating numbers.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DoubleAccumulator`：用于计算双精度浮点数的总和、计数和平均值。'
- en: '`CollectionAccumulator[T]` : for collecting a list of elements'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CollectionAccumulator[T]`：用于收集一组元素'
- en: All the preceding Accumulators are built on top of the `AccumulatorV2` class.
    By following the same logic, we can potentially build very complex and customized
    Accumulators to use in our project.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前面的累加器都是建立在 `AccumulatorV2` 类的基础上的。通过遵循相同的逻辑，我们可以构建非常复杂和定制的累加器，以供项目中使用。
- en: 'We can build a custom accumulator by extending the `AccumulatorV2` class. The
    following is an example showing the necessary functions to implement. `AccumulatorV2[Int,
    Int]` shown in the following code means that the Input and Output are both of
    Integer type:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过扩展 `AccumulatorV2` 类来构建自定义累加器。以下是实现所需函数的示例。代码中的 `AccumulatorV2[Int, Int]`
    表示输入和输出都为整数类型：
- en: '[PRE43]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Next, we will look at a practical example of a custom accumulator. Again, we
    shall use the `statesPopulation` CSV file for this. Our goal is to accumulate
    the sum of year and sum of population in a custom accumulator.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一个自定义累加器的实际例子。我们将再次使用 `statesPopulation` CSV 文件作为示例。我们的目标是使用自定义累加器累计年份总和和人口总和。
- en: '**Step 1\. Import the package containing the AccumulatorV2 class:**'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 步：导入包含 AccumulatorV2 类的包：**'
- en: '[PRE44]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Step 2\. Case class to contain the Year and Population:**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 2 步：定义一个包含年份和人口的 Case 类：**'
- en: '[PRE45]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Step 3\. StateAccumulator class extends AccumulatorV2:**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3 步：StateAccumulator 类继承自 AccumulatorV2：**'
- en: '[PRE46]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 4\. Create a new StateAccumulator and register the same with SparkContext:**'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 4 步：创建一个新的 StateAccumulator，并在 SparkContext 中注册：**'
- en: '[PRE47]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '**Step 5\. Read the statesPopulation.csv as an RDD:**'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 5 步：将 statesPopulation.csv 读取为 RDD：**'
- en: '[PRE48]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '**Step 6\. Use the StateAccumulator:**'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 6 步：使用 StateAccumulator：**'
- en: '[PRE49]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '**Step 7\. Now, we can examine the value of the StateAccumulator:**'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 7 步**。现在，我们可以检查 StateAccumulator 的值：'
- en: '[PRE50]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In this section, we examined accumulators and how to build a custom accumulator.
    Thus, using the preceding illustrated example, you can create complex accumulators
    to meet your needs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究了累加器及如何构建自定义累加器。因此，通过前面示例的展示，你可以创建复杂的累加器来满足你的需求。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the many types of RDDs, such as `shuffledRDD`,
    `pairRDD`, `sequenceFileRDD`, `HadoopRDD`, and so on. We also looked at the three
    main types of aggregations, `groupByKey`, `reduceByKey`, and `aggregateByKey`.
    We looked into how partitioning works and why it is important to have a proper
    plan around partitioning to increase the performance. We also looked at shuffling
    and the concepts of narrow and wide dependencies which are basic tenets of how
    Spark jobs are broken into stages. Finally, we looked at the important concepts
    of broadcast variables and accumulators.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了多种类型的 RDD，例如 `shuffledRDD`、`pairRDD`、`sequenceFileRDD`、`HadoopRDD`
    等。我们还介绍了三种主要的聚合方式，`groupByKey`、`reduceByKey` 和 `aggregateByKey`。我们探讨了分区是如何工作的，并解释了为什么合理规划分区对于提升性能至关重要。我们还讨论了洗牌过程及其与狭义依赖和广义依赖的概念，这些是
    Spark 作业被划分为多个阶段的基本原则。最后，我们介绍了广播变量和累加器的相关概念。
- en: The true power of the flexibility of RDDs makes it easy to adapt to most use
    cases and perform the necessary operations to accomplish the goal.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 的灵活性是其真正的力量，它使得适应大多数用例并执行必要的操作以实现目标变得非常容易。
- en: In the next chapter, we will switch gears to the higher layer of abstraction
    added to the RDDs as part of the Tungsten initiative known as DataFrames and Spark
    SQL and how it all comes together in the [Chapter 8](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduce a Little Structure – Spark SQL*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转向 RDDs 之上，Tungsten 计划所增加的更高层次的抽象——DataFrames 和 Spark SQL，以及它们如何在[第
    8 章](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c)中汇聚，*引入一些结构 – Spark
    SQL*。
