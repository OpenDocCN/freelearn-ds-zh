- en: Chapter 7. Building Communities
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。建立社区
- en: With more and more people interacting together and communicating, exchanging
    information, or simply sharing a common interest in different topics, most data
    science use cases can be addressed using graph representations. Although very
    large graphs were, for a long time, only used by the Internet giants, government,
    and national security agencies, it is becoming more common place to work with
    large graphs containing millions of vertices. Hence, the main challenge of a data
    scientist will not necessarily be to detect communities and find influencers on
    graphs, but rather to do so in a fully distributed and efficient way in order
    to overcome the constraint of scale. This chapter progresses through building
    a graph example, at scale, using the persons we identified using NLP extraction
    described in [Chapter 6](ch06.xhtml "Chapter 6. Scraping Link-Based External Data"),
    *Scraping Link-Based External Data*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的人相互交流和沟通，交换信息，或者只是在不同主题上分享共同的兴趣，大多数数据科学用例都可以使用图形表示来解决。尽管很长一段时间以来，非常大的图仅被互联网巨头、政府和国家安全机构使用，但现在使用包含数百万个顶点的大图变得更加普遍。因此，数据科学家的主要挑战不一定是在图表上检测社区并找到影响者，而是以一种完全分布式和高效的方式来克服规模的限制。本章将通过使用我们在[第6章](ch06.xhtml
    "第6章。抓取基于链接的外部数据")中描述的NLP提取识别的人员来构建一个大规模的图表示例。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Use Spark to extract content from Elasticsearch, build a Graph of person entities
    and learn the benefits of using Accumulo as a secure graph database
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark从Elasticsearch中提取内容，构建人员实体的图表，并了解使用Accumulo作为安全图数据库的好处
- en: Write a community detection algorithm from A to Z using *GraphX* and triangle
    optimization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*GraphX*和三角形优化从A到Z编写社区检测算法
- en: Leverage Accumulo specific features, including cell-level security to observe
    the changes in communities, and iterators to provide server and client-side computation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用Accumulo特定功能，包括单元级安全性来观察社区的变化，并使用迭代器提供服务器和客户端计算
- en: This chapter being quite technical, we expect the reader to be already familiar
    with graph theory, message passing, and *Pregel* API. We also invite the reader
    to go through every white paper mentioned in the chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章节非常技术化，我们期望读者已经熟悉图论、消息传递和*Pregel* API。我们还邀请读者阅读本章中提到的每一篇白皮书。
- en: Building a graph of persons
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建人员图表
- en: We previously used NLP entity recognition to identify persons from an HTML raw
    text format. In this chapter, we move to a lower level by trying to infer relations
    between these entities and detect the possible communities surrounding them.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用了NLP实体识别来从HTML原始文本格式中识别人物。在本章中，我们将尝试推断这些实体之间的关系，并检测围绕它们的可能社区。
- en: Contact chaining
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联系链
- en: 'Within the context of news articles, we first need to ask ourselves a fundamental
    question. What defines a relation between two entities? The most elegant answer
    would probably be to study words using the Stanford NLP libraries described in
    [Chapter 6](ch06.xhtml "Chapter 6. Scraping Link-Based External Data"), *Scraping
    Link-Based External Data*. Given the following input sentence, which is taken
    from [http://www.ibtimes.co.uk/david-bowie-yoko-ono-says-starmans-death-has-left-big-empty-space-1545160](http://www.ibtimes.co.uk/david-bowie-yoko-ono-says-starmans-death-has-left-big-empty-space-1545160):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在新闻文章的背景下，我们首先需要问自己一个基本问题。什么定义了两个实体之间的关系？最优雅的答案可能是使用斯坦福NLP库中描述的单词来研究，详情请参阅[第6章](ch06.xhtml
    "第6章。抓取基于链接的外部数据")中描述的*抓取基于链接的外部数据*。给定以下输入句子，该句子取自[http://www.ibtimes.co.uk/david-bowie-yoko-ono-says-starmans-death-has-left-big-empty-space-1545160](http://www.ibtimes.co.uk/david-bowie-yoko-ono-says-starmans-death-has-left-big-empty-space-1545160)：
- en: '*"Yoko Ono said she and late husband John Lennon shared a close relationship
    with David Bowie"*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"Yoko Ono说她和已故丈夫约翰·列侬与大卫·鲍伊有着密切的关系"*'
- en: 'We could easily extract the syntactic tree, a structure that linguists use
    to model how sentences are grammatically built and where each element is reported
    with its type such as a noun (`NN`), a verb (`VR`), or a determiner (`DT`) and
    its relative position in the sentence:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松提取句法树，这是语言学家用来模拟句子语法结构的结构，其中每个元素都以其类型报告，例如名词（`NN`），动词（`VR`）或限定词（`DT`），以及其在句子中的相对位置。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A thorough study of each element, its type, its predecessors, and successors
    would help build a directed graph with edges being the true definitions of the
    relations that exist between all these three entities. An example of a graph built
    out of that sentence is reported as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个元素、其类型、其前驱和后继的彻底研究将有助于构建一个有向图，其中边是存在于所有这三个实体之间关系的真实定义。从这个句子构建的图的示例如下所示：
- en: '![Contact chaining](img/image_07_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![联系链](img/image_07_001.jpg)'
- en: 'Figure 1: Syntactic graph for David Bowie, Yoko Ono, and John Lennon'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：大卫·鲍伊、Yoko Ono和约翰·列侬的句法图
- en: 'Although it makes perfect sense (grammatically speaking), building a graph
    out of syntactic trees would require excessive amount of coding, would probably
    deserve a whole chapter on its own, and does not bring much added value since
    most of the relations we would build (in the context of news articles) would not
    be based on true facts taken from history books, but rather need to be put in
    their context. To illustrate this point we have two sentences which are taken
    from [http://www.digitalspy.com/music/news/a779577/paul-mccartney-pays-tribute-to-great-star-david-bowie-his-star-will-shine-in-the-sky-forever/](http://www.digitalspy.com/music/news/a779577/paul-mccartney-pays-tribute-to-great-star-david-bowie-his-star-will-shine-in-the-sky-forever/):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从语法上讲是完全合理的，但是构建一个句法树图需要大量的编码，可能需要一个完整的章节来讲解，并且并没有带来太多附加值，因为我们建立的大多数关系（在新闻文章的背景下）都不是基于历史书籍中的真实事实，而是需要放在它们的背景中。为了说明这一点，我们有两个句子，这些句子取自[http://www.digitalspy.com/music/news/a779577/paul-mccartney-pays-tribute-to-great-star-david-bowie-his-star-will-shine-in-the-sky-forever/](http://www.digitalspy.com/music/news/a779577/paul-mccartney-pays-tribute-to-great-star-david-bowie-his-star-will-shine-in-the-sky-forever/)：
- en: '*"Sir Paul McCartney described [David Bowie] as a great star"*'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“保罗·麦卡特尼爵士称[大卫·鲍伊]为一颗伟大的星星”*'
- en: '*"[Sir Paul McCartney] treasure[s] the moments they had together"*'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“[保罗·麦卡特尼爵士]珍视他们在一起的时刻”*'
- en: It would create the same grammatical link between vertices [Paul McCartney]
    and [David Bowie], while only the latter assumes a physical connection between
    them (they actually spent time together).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它将在[保罗·麦卡特尼]和[大卫·鲍伊]之间创建相同的语法链接，而只有后者假定它们之间存在物理联系（他们实际上在一起度过了一些时间）。
- en: Instead, we use a much faster approach by grouping names based on their positions
    within a text. Our Naive assumption is that most of the authors usually start
    mentioning the names of important people first, then write about secondary characters,
    and lastly about less important persons. Our contact chaining is therefore a simple
    nested loop across all the names in a given article, names being sorted from the
    most to the least important ones using their actual position. Because of its relative
    time complexity *O(n²)* this approach will only be valid for hundreds of records
    per article and will certainly be a limiting factor with text mentioning hundreds
    of thousands of different entities.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们使用了一种更快速的方法，即根据它们在文本中的位置对名称进行分组。我们的天真假设是，大多数作者通常首先提到重要人物的名字，然后写有关次要角色的内容，最后是不太重要的人物。因此，我们的联系链接是在给定文章中的所有名称上进行的简单嵌套循环，名称根据它们的实际位置从最重要的到最不重要的进行排序。由于其相对时间复杂度为*O(n²)*，这种方法只对每篇文章的记录数有效，对于提及数以千计不同实体的文本来说，它肯定会成为一个限制因素。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In our code repository, you will see an alternative: `Combinations`, which
    is a more generic solution that allows the specification of a variable `r`; this
    allows us to specify the number of entities that need to appear in each output
    combination, that is, 2 for this chapter but more in other contexts. Using `Combinations.buildTuples`
    is functionally equal to the `buildTuples` code given earlier.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码库中，您将看到另一种选择：`Combinations`，这是一个更通用的解决方案，允许指定一个变量`r`；这使我们能够指定每个输出组合中需要出现的实体数量，即本章为2，但在其他情境中可能更多。使用`Combinations.buildTuples`在功能上等同于之前给出的`buildTuples`代码。
- en: Extracting data from Elasticsearch
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Elasticsearch中提取数据
- en: 'Elasticsearch is a perfect tool for storing and indexing text content together
    with its metadata attributes, and was therefore a logical choice for our online
    data store using the text content we extracted in the previous chapter. As this
    section is more batch-process oriented, we get the data from Elasticsearch into
    our Spark cluster using the excellent Spark Elasticsearch API as shown in the
    following code:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch是一个存储和索引文本内容及其元数据属性的完美工具，因此它是我们在线数据存储的逻辑选择，使用我们在上一章中提取的文本内容。由于本节更加面向批处理，我们使用出色的Spark
    Elasticsearch API将数据从Elasticsearch获取到我们的Spark集群中，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Given an index type and name, a convenient way for interacting with the Elasticsearch
    API is using Spark DataFrame. Efficient enough in most use cases (a simple example
    is shown next), this might become a challenge when working on more complex and
    nested schemas:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 给定索引类型和名称，与Elasticsearch API 交互的一种便捷方式是使用Spark DataFrame。在大多数用例中效率足够高（下面显示了一个简单的例子），但在处理更复杂和嵌套的模式时可能会成为一个挑战：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In fact, the Elasticsearch API is not flexible enough to read nested structures
    and complex arrays. Using latest versions of Spark, one will quickly run into
    errors such as *"Field ''persons'' is backed by an array but the associated Spark
    Schema does not reflect this"*. With some experimentation, we can see that accessing
    nested and complex structures from Elasticsearch is usually much easier using
    a set of standard JSON parsers (such as `json4s` in the following code):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，Elasticsearch API 并不灵活，无法读取嵌套结构和复杂数组。使用最新版本的Spark，人们很快就会遇到诸如“'persons'字段由数组支持，但相关的Spark模式并不反映这一点”之类的错误。通过一些实验，我们可以看到，使用一组标准的JSON解析器（例如下面的`json4s`）通常更容易从Elasticsearch中访问嵌套和复杂的结构：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We query Elasticsearch using the implicit `esJsonRdd` function from a spark
    context:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用隐式的`esJsonRdd`函数从spark上下文查询Elasticsearch：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Using the `query` parameter, we can access all the data from Elasticsearch,
    a sample of it, or even all of the records matching a specific query. We can finally
    build our list of tuples using the simple contact chaining method explained earlier.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`query`参数，我们可以访问Elasticsearch中的所有数据，其中的一部分数据，或者甚至是与特定查询匹配的所有记录。最后，我们可以使用之前解释的简单联系链接方法来构建我们的元组列表。
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using the Accumulo database
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Accumulo数据库
- en: We have seen a method to read our `personRdd` object from Elasticsearch and
    this forms a simple and neat solution for our storage requirements. However, when
    writing commercial applications, we must always be mindful of security and, at
    the time of writing, Elasticsearch security is still in development; so it would
    be useful at this stage to introduce a storage mechanism with native security.
    This is an important consideration we are using GDELT data that is, of course,
    open source by definition. In a commercial environment, it is very common for
    datasets to be confidential or commercially sensitive in some way, and clients
    will often request details of how their data will be secured long before they
    discuss the data science aspect itself. It is the authors experience that many
    a commercial opportunity is lost due to the inability of solution providers to
    demonstrate a robust and secure data architecture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了从Elasticsearch读取`personRdd`对象的方法，这为我们的存储需求提供了一个简单而整洁的解决方案。然而，在编写商业应用程序时，我们必须始终牢记安全性，在撰写本文时，Elasticsearch安全性仍在开发中；因此，在这个阶段引入具有本地安全性的存储机制将是有用的。这是一个重要的考虑因素，因为我们使用的是GDELT数据，当然，根据定义，它是开源的。在商业环境中，数据集很常见地是机密的或在某种程度上具有商业敏感性，客户通常会在讨论数据科学方面之前要求了解他们的数据将如何得到安全保护。作者的经验是，许多商业机会由于解决方案提供者无法展示健壮和安全的数据架构而丧失。
- en: '**Accumulo** ([http://accumulo.apache.org](http://accumulo.apache.org)) is
    a NoSQL database based on Google''s Bigtable design ([http://research.google.com/archive/bigtable.html](http://research.google.com/archive/bigtable.html))
    and was originally developed by the United States National Security Agency, which
    was subsequently released to the Apache community in 2011\. Accumulo offers us
    the usual big data advantages such as bulk loading and parallel reading but also
    has some additional capabilities such as Iterators, for efficient server and client-side
    precomputation, data aggregation and, most importantly, cell-level security.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Accumulo** ([http://accumulo.apache.org](http://accumulo.apache.org)) 是一个基于Google的Bigtable设计（[http://research.google.com/archive/bigtable.html](http://research.google.com/archive/bigtable.html)）的NoSQL数据库，最初由美国国家安全局开发，后来在2011年释放给Apache社区。Accumulo为我们提供了通常的大数据优势，如批量加载和并行读取，但还具有一些额外的功能，如迭代器，用于高效的服务器和客户端预计算、数据聚合，最重要的是单元格级安全。'
- en: For our work in community detection, we will use Accumulo to take advantage
    specifically of its iterator and cell-level security features. First of all, we
    should set up an Accumulo instance and then load some data from Elasticsearch
    to Accumulo you can find the full code in our GitHub repository.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的社区检测工作中，我们将使用Accumulo来特别利用其迭代器和单元格级安全功能。首先，我们应该设置一个Accumulo实例，然后从Elasticsearch加载一些数据到Accumulo，你可以在我们的GitHub存储库中找到完整的代码。
- en: Setup Accumulo
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置Accumulo
- en: The steps required to install Accumulo are out of the scope of this book; there
    are several tutorials available on the Web. A vanilla installation with a root
    user is all that is required to continue with this chapter, although we need to
    pay particular attention to the initial security setup in the Accumulo configuration.
    Once you run the Accumulo shell successfully, you are ready to proceed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Accumulo所需的步骤超出了本书的范围；网上有几个教程可供参考。只需进行一个带有根用户的原始安装即可继续本章，尽管我们需要特别注意Accumulo配置中的初始安全设置。一旦成功运行Accumulo
    shell，您就可以继续进行。
- en: Use the following code as a guideline to creating users. The aim is to create
    several users with different security labels so that when we load the data, the
    users will have varying access to it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码作为创建用户的指南。目标是创建几个具有不同安全标签的用户，这样当我们加载数据时，用户将有不同的访问权限。
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Cell security
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单元格安全
- en: 'Accumulo protects its cells using tokens. Tokens are made up of labels; in
    our case, these are [`unclassified`], [`secret`], and [`topsecret`], but you can
    use any comma-delimited values. Accumulo rows are written with a `visibility`
    field (refer to the following code) that is simply a string representation of
    the labels required to access a row value. The `visibility` field can contain
    Boolean logic to combine different labels and also allows for basic precedence,
    for instance:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Accumulo使用令牌来保护其单元格。令牌由标签组成；在我们的情况下，这些是[`未分类`], [`机密`], 和 [`绝密`], 但你可以使用任何逗号分隔的值。Accumulo行是用`visibility`字段（参考下面的代码）编写的，它只是对访问行值所需的标签的字符串表示。`visibility`字段可以包含布尔逻辑来组合不同的标签，还允许基本的优先级，例如：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A user has to match at least the `visibility` field in order to be granted access,
    and must supply labels that are a subset of his token stored in Accumulo (or the
    query will be rejected). Any values that are not matched will simply not be returned
    in the user query, this is an important point because if there is some indication
    to the user that data is missing, it is often possible for the user to draw logical,
    correct (or often worse, incorrect) conclusions about the nature of the data,
    for example, in a contact chain of people, if some vertices are available to a
    user and some not, but the unavailable vertices are marked as such, then the user
    might be able to determine information about those missing entities based on the
    surrounding graph. For example, a government agency investigating organized crime
    may allow senior employees to view an entire graph, but junior employees to only
    view parts of it. Let's say some well-known persons are shown in the graph, and
    there is a blank entry for a vertex, then it might be straightforward to workout
    who the missing entity is; if this placeholder is absent altogether, then there
    is no obvious indication that the chain stretches any further, thus allowing the
    agency to control dissemination of information. The graph is still of use to analysts,
    however, who are oblivious to the link and can continue working on specific areas
    of the graph.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 用户必须至少匹配`visibility`字段才能获得访问权限，并且必须提供标签，这些标签是存储在Accumulo中的令牌的子集（否则查询将被拒绝）。任何不匹配的值在用户查询中将不会被返回，这是一个重要的观点，因为如果用户得知数据缺失，往往可以根据周围图的性质得出逻辑上正确（或者更糟糕的是错误）的结论，例如，在一个人的联系链中，如果一些顶点对用户可见而另一些不可见，但不可见的顶点被标记为不可见，那么用户可能能够根据周围的图确定有关这些缺失实体的信息。例如，调查有组织犯罪的政府机构可能允许高级员工查看整个图，但只允许初级员工查看其中的部分。假设图中显示了一些知名人物，并且一个顶点的条目为空白，那么可能很容易推断出缺失的实体是谁；如果这个占位符完全不存在，那么就没有明显的迹象表明链条延伸得更远，从而允许机构控制信息的传播。然而，对于对这些链接一无所知的分析人员来说，图仍然是有用的，并且可以继续在图的特定区域上工作。
- en: Iterators
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代器
- en: Iterators are a very important feature in Accumulo and provide a real-time processing
    framework, which leverages the power and parallelization of Accumulo, to produce
    modified versions of data at very low latency. We won't go into great detail here
    as the Accumulo documentation has plenty of examples, but we will use an iterator
    to keep a sum of the values for the same Accumulo row, that is, the number of
    times we have seen the same person pair; and this will be stored in that row value.
    This iterator will then appear to take effect whenever the table is scanned; we
    will also demonstrate how to invoke the same Iterator from the client side (for
    use when it has not been applied to the server).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代器是Accumulo中非常重要的特性，提供了一个实时处理框架，利用Accumulo的强大和并行能力，以非常低的延迟产生修改后的数据版本。我们不会在这里详细介绍，因为Accumulo文档中有很多例子，但我们将使用一个迭代器来保持相同Accumulo行的值的总和，也就是我们看到相同的人员对的次数；这将存储在该行值中。每当扫描表时，这个迭代器就会生效；我们还将演示如何从客户端调用相同的迭代器（当它尚未应用于服务器时）。
- en: Elasticsearch to Accumulo
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Elasticsearch到Accumulo
- en: Let's take advantage of Spark's ability to use Hadoop input and output formats,
    which leverage the native Elasticsearch and Accumulo libraries. It is worth noting
    that there are different routes that we could take here, the first is to use the
    Elasticsearch code given earlier to produce an array of string tuples and feed
    that into `AccumuloLoader` (found in the code repository); the second is to explore
    an alternative using additional Hadoop `InputFormat`; we can produce code that
    reads from Elasticsearch using `EsInputFormat` and writes to Accumulo using `AccumuloOutputFormat`
    class.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用Spark能够使用Hadoop输入和输出格式的能力，利用本地Elasticsearch和Accumulo库。值得注意的是，我们在这里可以采取不同的路线，第一种是使用之前提供的Elasticsearch代码生成一个字符串元组数组，并将其输入到`AccumuloLoader`（在代码库中找到）；第二种是探索另一种使用额外Hadoop
    `InputFormat` 的方法；我们可以编写代码，使用`EsInputFormat` 从Elasticsearch读取数据，并使用`AccumuloOutputFormat`
    类写入Accumulo。
- en: A graph data model in Accumulo
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Accumulo中的图数据模型
- en: 'Before delving into the code, it is worth describing the schema we will be
    using to store a graph of persons in Accumulo. Each source node (`person A`) will
    be stored as a row key, the association name (such as "is also known as") as a
    column family, the destination node (`person B`) as a column qualifier, and a
    default value of `1` as a column value (that will be aggregated thanks to our
    iterator). This is reported here in Figure 2:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码之前，值得描述一下我们将在Accumulo中使用的存储人员图的模式。每个源节点（`person A`）将被存储为行键，关联名称（如“也被称为”）作为列族，目标节点（`person
    B`）作为列限定符，以及默认值`1`作为列值（这将通过我们的迭代器进行聚合）。如图2所示：
- en: '![A graph data model in Accumulo](img/image_07_002.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Accumulo中的图数据模型](img/image_07_002.jpg)'
- en: 'Figure 2: Graph data model on Accumulo'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Accumulo上的图数据模型
- en: The main advantage of such a model is that given an input vertex (a person's
    name), one can quickly access all its known relationships through a simple GET
    query. The reader will surely appreciate the cell level security where we hide
    a particular edge triplet `[personA] <= [relationB] => [personD]` from most Accumulo
    users with no [`SECRET`] authorization granted.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的主要优势在于，给定一个输入顶点（一个人的名字），可以通过简单的GET查询快速访问所有已知的关系。读者肯定会欣赏单元级别的安全性，我们可以隐藏一个特定的边三元组`[personA]
    <= [relationB] => [personD]`，对大多数没有[`SECRET`]授权的Accumulo用户。
- en: The downside of such a model is that, compared to a graph database (such as
    Neo4J or OrientDB), traversing queries such as a depth first search would be terribly
    inefficient (we would need multiple recursive queries). We delegate any graph
    processing logic to GraphX later in this chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的缺点是，与图数据库（如Neo4J或OrientDB）相比，遍历查询（如深度优先搜索）将非常低效（我们需要多次递归查询）。我们将任何图处理逻辑委托给本章后面的GraphX。
- en: Hadoop input and output formats
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hadoop输入和输出格式
- en: We use the following maven dependency in order to build both our input/output
    formats and our Spark client. The version obviously depends on the distribution
    of Hadoop and Accumulo installed.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下maven依赖项来构建我们的输入/输出格式和我们的Spark客户端。版本显然取决于安装的Hadoop和Accumulo的发行版。
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We configure for reading from Elasticsearch through the `ESInputFormat` class.
    We extract a key-value pair RDD of `Text` and `MapWritable`, where the key contains
    the document ID and the value of all the JSON documents wrapped inside of a serializable
    HashMap:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过`ESInputFormat`类配置从Elasticsearch中读取。我们提取了一个`Text`和`MapWritable`的键值对RDD，其中键包含文档ID，值包含所有JSON文档的可序列化HashMap包装在内：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'An Accumulo `mutation` is similar to a `put` object in HBase, and contains
    the table''s coordinates such as row key, column family, column qualifier, column
    value, and visibility. This object is built as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Accumulo的`mutation`类似于HBase中的`put`对象，包含表的坐标，如行键，列族，列限定符，列值和可见性。该对象构建如下：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We use the aforementioned `buildTuples` method to calculate our person pairs
    and write them to Accumulo using the Hadoop `AccumuloOutputFormat`. Note that
    we can optionally apply a security label to each of our output rows using `ColumnVisibility`;
    refer to *Cell security*, which we saw earlier.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上述的`buildTuples`方法来计算我们的人员对，并使用Hadoop的`AccumuloOutputFormat`将它们写入Accumulo。请注意，我们可以选择为我们的输出行应用安全标签，使用`ColumnVisibility`；参考*Cell
    security*，我们之前看到过。
- en: 'We configure for writing to Accumulo. Our output RDD will be a key-value pair
    RDD of `Text` and `Mutation`, where the key contains the Accumulo table and the
    value the mutation to insert:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们配置用于写入Accumulo。我们的输出RDD将是一个`Text`和`Mutation`的键值对RDD，其中键包含Accumulo表，值包含要插入的mutation：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Reading from Accumulo
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Accumulo读取
- en: 'Now that we have our data in Accumulo, we can use the shell to inspect it (assuming
    we select a user that has enough privileges to see the data). Using the `scan`
    command in Accumulo shell, we can simulate a specific user and query, therefore
    validating the results of `io.gzet.community.accumulo.AccumuloReader`. When using
    the Scala version, we must ensure that the correct Authorization is used-it is
    passed into the read function via a `String`, an example might be `"secret,topsecret"`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据在Accumulo中，我们可以使用shell来检查它（假设我们选择了一个有足够权限查看数据的用户）。在Accumulo shell中使用`scan`命令，我们可以模拟特定用户和查询，从而验证`io.gzet.community.accumulo.AccumuloReader`的结果。在使用Scala版本时，我们必须确保使用正确的授权-它通过`String`传递到读取函数中，例如可能是`"secret,topsecret"`。
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This method of applying Hadoop input/output format utilizes `static` methods
    within the Java Accumulo library (`AbstractInputFormat` is subclassed by `InputFormatBase`,
    which is subclassed by `AccumuloInputFormat`). Spark users must pay particular
    attention to these utility methods that alter the Hadoop configuration via an
    instance of a `Job` object. This can be set as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种应用Hadoop输入/输出格式的方法利用了Java Accumulo库中的`static`方法（`AbstractInputFormat`是`InputFormatBase`的子类，`InputFormatBase`是`AccumuloInputFormat`的子类）。Spark用户必须特别注意这些实用方法，通过`Job`对象的实例来修改Hadoop配置。可以设置如下：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You will also notice the configuration of an Accumulo iterator:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会注意到配置了Accumulo迭代器：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can use client or server-side iterators and we have previously seen an example
    of server-side when configuring Accumulo via the shell. The key difference is
    that client-side Iterators are executed within the client JVM, as opposed to server-side,
    which leverage the power of the Accumulo tablet servers. A full explanation can
    be found in the Accumulo documentation. However, there are many reasons for selecting
    a client or server-side Iterator including choices over whether tablet server
    performance should be compromised, JVM memory usage, and so on. These decisions
    should be made when creating your Accumulo architecture. At the end of our `AccumuloReader`
    code, we can see the calling function that produces an RDD of `EdgeWritable`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用客户端或服务器端迭代器，之前我们已经在通过shell配置Accumulo时看到了一个服务器端的例子。关键区别在于客户端迭代器在客户端JVM中执行，而不是服务器端迭代器利用Accumulo表服务器的功能。在Accumulo文档中可以找到完整的解释。然而，选择客户端或服务器端迭代器的许多原因，包括是否应该牺牲表服务器性能，JVM内存使用等。这些决定应该在创建Accumulo架构时进行。在我们的`AccumuloReader`代码的末尾，我们可以看到产生`EdgeWritable`的RDD的调用函数：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: AccumuloGraphxInputFormat and EdgeWritable
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AccumuloGraphxInputFormat和EdgeWritable
- en: We have implemented our own Accumulo `InputFormat` to enable us to read Accumulo
    rows and automatically output our own Hadoop `Writable`; `EdgeWritable`. This
    provides for a convenience wrapper to hold our source vertex, our destination
    vertex, and the count as edge weight, which can then be used when building the
    graph. This is extremely useful as Accumulo uses the iterator discussed earlier
    to calculate the total count for each unique row, thereby removing the need to
    do this manually. As Accumulo is written in Java, our `InputFormat` uses Java
    to extend `InputFormatBase`, thus inheriting all of the Accumulo `InputFormat`
    default behavior, but outputting our choice of schema.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了自己的Accumulo `InputFormat`，使我们能够读取Accumulo行并自动输出我们自己的Hadoop `Writable`；`EdgeWritable`。这提供了一个方便的包装器，用于保存我们的源顶点，目标顶点和作为边权重的计数，这在构建图时可以使用。这非常有用，因为Accumulo使用前面讨论的迭代器来计算每个唯一行的总计数，从而无需手动执行此操作。由于Accumulo是用Java编写的，我们的`InputFormat`使用Java来扩展`InputFormatBase`，从而继承了所有Accumulo`InputFormat`的默认行为，但输出我们选择的模式。
- en: We are only interested in outputting `EdgeWritables`; therefore, we set all
    of the keys to be null (`NullWritable`) and the values to `EdgeWritable`, an additional
    advantage being that values in Hadoop only need to inherit from the `Writable`
    Interface (although we have inherited `WritableComparable` for completeness, and
    `EdgeWritable` can therefore be used as a key, if required).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只对输出`EdgeWritables`感兴趣；因此，我们将所有键设置为null（`NullWritable`），值设置为`EdgeWritable`，另一个优势是Hadoop中的值只需要继承自`Writable`接口（尽管我们为了完整性继承了`WritableComparable`，因此如果需要，`EdgeWritable`也可以用作键）。
- en: Building a graph
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建图
- en: 'Because GraphX uses long objects as an underlying type for storing vertices
    and edges, we first need to translate all of the persons we fetched from Accumulo
    into a unique set of IDs. We assume our list of unique persons does not fit in
    memory, or wouldn''t be efficient to do so anyway, so we simply build a distributed
    dictionary using the `zipWithIndex` function as shown in the following code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因为GraphX使用长对象作为存储顶点和边的基础类型，所以我们首先需要将从Accumulo获取的所有人员翻译成一组唯一的ID。我们假设我们的唯一人员列表不适合存储在内存中，或者无论如何都不高效，所以我们简单地使用`zipWithIndex`函数构建一个分布式字典，如下面的代码所示：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We create an edge RDD using two successive join operations onto our person tuples
    and finally build our weighted and directed graph of persons with vertices containing
    the person name, and edge attributes the frequency count of each tuple.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两次连续的连接操作来创建边RDD，最终构建包含人员名称的顶点和包含每个元组频率计数的边属性的加权有向图。
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Community detection algorithm
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 社区检测算法
- en: Community detection has become a popular field of research over the past few
    decades. Sadly, it did not move as fast as the digital world that a true data
    scientist lives in, with more and more data collected every second. As a result,
    most of the proposed solutions are simply not suitable for a big data environment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年里，社区检测已经成为研究的热门领域。遗憾的是，它没有像真正的数据科学家所处的数字世界一样快速发展，每秒都在收集更多的数据。因此，大多数提出的解决方案对于大数据环境来说根本不合适。
- en: Although a lot of algorithms suggest a new scalable way for detecting communities,
    none of them is actually meaning scalable in a sense of distributed algorithms
    and parallel computing.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多算法提出了一种新的可扩展的检测社区的方法，但实际上没有一种是在分布式算法和并行计算方面真正可扩展的。
- en: Louvain algorithm
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Louvain算法
- en: Louvain algorithm is probably the most popular and widely used algorithm for
    detecting communities on undirected weighted graphs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Louvain算法可能是检测无向加权图中社区最流行和广泛使用的算法。
- en: Note
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information about Louvain algorithm, refer to the publication: *Fast
    unfolding of communities in large networks. Vincent D. Blondel, Jean-Loup Guillaume,
    Renaud Lambiotte, Etienne Lefebvre. 2008*
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Louvain算法的更多信息，请参阅出版物：*大型网络中社区的快速展开。文森特D.布隆德，让-卢·吉约姆，勒诺·兰比奥特，艾蒂安·勒菲布尔。2008*
- en: 'The idea is to start with each vertex being the center of its own community.
    At each step, we look for community neighbors and check whether or not merging
    both communities together would result in any gain in the modularity values. Going
    through each vertex, we compress the graph so that all nodes being part of the
    same community become a unique community vertex, with all community internal edges
    becoming a self-edge with aggregated weights. We repeat this process until the
    modularity can no longer be optimized. The process is reported as follows in *Figure
    3*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是从每个顶点作为其自己社区的中心开始。在每一步中，我们寻找社区邻居，并检查合并这两个社区是否会导致模块化值的增益。通过每个顶点，我们压缩图形，使得所有属于同一个社区的节点成为一个唯一的社区顶点，所有社区内部边成为具有聚合权重的自边。我们重复这个过程，直到无法再优化模块化。该过程如*图3*所示：
- en: '![Louvain algorithm](img/B05261_07_03.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![Louvain算法](img/B05261_07_03.jpg)'
- en: 'Figure 3: Fast unfolding of communities in large networks-Vincent D. Blondel,
    Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre, 2008'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：大型网络中社区的快速展开-文森特D.布隆德尔，让-卢·吉约姆，勒诺·兰比奥特，艾蒂安·勒菲布尔，2008
- en: Because modularity will be updated any time a vertex changes, and because the
    change of each vertex will be driven by the global modularity update, vertices
    need to be processed in a serial order; making the modularity optimization a cut-off
    point to the nature of parallel computing. Recent studies have reported that the
    quality of results may decrease as the size of graph increases excessively so
    that modularity is unable to detect small and well-defined communities.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每当顶点改变时，模块化都会更新，而且每个顶点的改变都将由全局模块化更新驱动，所以顶点需要按顺序处理；这使得模块化优化成为并行计算性质的一个分界点。最近的研究报告称，随着图的规模过度增加，结果的质量可能会下降，以至于模块化无法检测到小而明确定义的社区。
- en: To the very best of our knowledge, the only distributed version of Louvain that
    is publicly available has been created by Sotera, a national security technology
    supplier ([https://github.com/Sotera/distributed-graph-analytics/tree/master/dga-graphx](https://github.com/Sotera/distributed-graph-analytics/tree/master/dga-graphx)).
    With different implementations on either MapReduce, Giraph, or GraphX, their idea
    is to make vertices choices simultaneously and update the graph state after each
    change. Because of the parallel nature, some of the vertex choices will be incorrect
    as they may not maximize a global modularity, but eventually become more and more
    consistent after repeated iterations.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，唯一公开可用的Louvain的分布式版本是由国家安全技术供应商Sotera创建的（[https://github.com/Sotera/distributed-graph-analytics/tree/master/dga-graphx](https://github.com/Sotera/distributed-graph-analytics/tree/master/dga-graphx)）。他们在MapReduce、Giraph或GraphX上有不同的实现，他们的想法是同时做出顶点选择，并在每次更改后更新图状态。由于并行性质，一些顶点选择可能是不正确的，因为它们可能无法最大化全局模块化，但在重复迭代后最终变得越来越一致。
- en: This (potentially) slightly less accurate, but definitely highly scalable, algorithm
    was worth investigating, but because there is no right or wrong solution to the
    community detection problem and because each data science use case is different,
    we decided to build our own distributed version of a different algorithm rather
    than describing an existing one. For convenience, we repackaged this distributed
    version of Louvain and made it available in our GitHub repository.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这种（可能）略微不准确，但绝对高度可扩展的算法值得研究，但由于社区检测问题没有对错解决方案，而且每个数据科学用例都不同，我们决定构建我们自己的分布式版本的不同算法，而不是描述现有的算法。为了方便起见，我们重新打包了这个分布式版本的Louvain，并在我们的GitHub存储库中提供了它。
- en: Weighted Community Clustering (WCC)
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加权社区聚类（WCC）
- en: By searching for some documentation material on graph algorithms, we came across
    a fantastic and recent white paper mentioning both scalability and parallel computing.
    We invite our readers to read this paper first before moving forward in the implementation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过搜索一些关于图算法的文档材料，我们偶然发现了一份关于可扩展性和并行计算的出色且最新的白皮书。我们邀请我们的读者在继续实施之前先阅读这篇论文。
- en: Note
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information about WCC algorithm, refer to the publication: *A. Prat-Perez,
    D. Dominguez-Sal, and J.-L. Larriba-Pey, "High quality, scalable and parallel
    community detection for large real graphs," in Proceedings of the 23rd International
    Conference on World Wide Web, ser. WWW ''14\. New York, NY, USA: ACM, 2014, pp.
    225-236*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '有关**WCC**算法的更多信息，请参阅以下出版物：*A. Prat-Perez, D. Dominguez-Sal, and J.-L. Larriba-Pey,
    "High quality, scalable and parallel community detection for large real graphs,"
    in Proceedings of the 23rd International Conference on World Wide Web, ser. WWW
    ''14\. New York, NY, USA: ACM, 2014, pp. 225-236*'
- en: Although no implementation could be found, and the authors are discrete about
    the technologies they were using, we were particularly interested by the heuristic
    used as a measure of the graph partitioning since detection can be done in parallel
    without having to re-compute a global metric such as the graph modularity.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管找不到任何实现，并且作者对他们使用的技术保持低调，但我们对作为图分区度量的启发式方法特别感兴趣，因为检测可以并行进行，而无需重新计算图模块度等全局度量。
- en: Description
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述
- en: 'Also interesting is the assumption they use, inspired from real-life social
    networks, as a quality measure for detecting communities. Because communities
    are groups of vertices that are tightly connected together and loosely connected
    with the rest of the graph, there should be a high concentration of triangles
    closed within each community. In other words, vertices that form part of a community
    should be closing many more triangles in their own community than they would be
    closing outside:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 同样有趣的是他们使用的假设，受到现实生活社交网络的启发，作为检测社区的质量度量。因为社区是紧密连接在一起并与图的其余部分松散连接的顶点组成的群体，所以每个社区内应该有大量的三角形。换句话说，组成社区的顶点应该在自己的社区内关闭的三角形数量要比在外部关闭的要多得多。
- en: '![Description](img/B05261_07_04.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![Description](img/B05261_07_04.jpg)'
- en: 'As per the preceding equation, the clustering coefficient (WCC) for a given
    vertex **x** in a community **C** will be maximized when **x** will be closing
    more triangles inside of its community than outside (communities will be well
    defined) and/or when the number of its neighbors where it does not close any triangle
    with will be minimal (all nodes are interconnected). As reported in the following
    equation, the **WCC** of a community **S** will be the average **WCC** of each
    of its vertices:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前述方程，给定顶点**x**在社区**C**中的聚类系数（**WCC**）将在**x**在其社区内部关闭的三角形数量多于外部时达到最大值（社区将被明确定义），和/或者当它与不关闭任何三角形的邻居数量最小时（所有节点相互连接）。如下方程所述，社区**S**的**WCC**将是其每个顶点的平均**WCC**：
- en: '![Description](img/B05261_07_05.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![Description](img/B05261_07_05.jpg)'
- en: 'Similarly, the **WCC** of a graph partition **P** will be the weighted average
    of each community''s WCC:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，图分区**P**的**WCC**将是每个社区**WCC**的加权平均值：
- en: '![Description](img/B05261_07_06.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![Description](img/B05261_07_06.jpg)'
- en: The algorithm consists of three different phases explained next. A preprocessing
    step that creates an initial set of communities, a community-back propagation
    to ensure initial communities are consistent, and finally an iterative algorithm
    that optimizes the global clustering coefficient value.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法包括三个不同的阶段，下面将对其进行解释。预处理步骤创建初始社区集，社区回传以确保初始社区一致，最后是一个迭代算法，优化全局聚类系数值。
- en: Preprocessing stage
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理阶段
- en: 'The first step is to define a graph structure with vertices containing all
    the variables we need to compute the WCC metrics locally, including the current
    community a vertex belongs to, the number of triangles each vertex is closing
    inside and outside of its communities, the number of nodes it shares triangles
    with and the current WCC metric. All these variables will be wrapped into a `VState`
    class:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义一个图结构，其中顶点包含我们需要在本地计算**WCC**指标的所有变量，包括顶点所属的当前社区，每个顶点在其社区内外关闭的三角形数量，它与其他节点共享三角形的数量以及当前**WCC**指标。所有这些变量将被封装到一个`VState`类中：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In order to compute the initial WCC, we first need to count the number of triangles
    any vertex is closing within its neighborhood. Counting the number of triangles
    usually consists of aggregating the neighbors IDs for each vertex, sending this
    list to each of its neighbors, and searching for common IDs in both vertex neighbors
    and vertex neighbors' neighbors. Given two connected vertices A and B, the intersection
    between A's and B's respective list of neighbors is the number of triangles vertex
    A closes with B, and the aggregation in A returns the total number of triangles
    vertex A is closing across the graph.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算初始**WCC**，我们首先需要计算任何顶点在其邻域内关闭的三角形数量。通常计算三角形的数量包括为每个顶点聚合邻居的ID，将此列表发送给每个邻居，并在顶点邻居和顶点邻居的邻居中搜索共同的ID。给定两个相连的顶点A和B，A的邻居列表和B的邻居列表的交集是顶点A与B关闭的三角形数量，而A中的聚合返回顶点A在整个图中关闭的三角形的总数。
- en: 'In large networks with highly connected vertices, sending a list of adjacent
    vertices to each neighbor can be time consuming and network intensive. In GraphX,
    the `triangleCount` function has been optimized so that for each edge, only the
    least important vertex (in term of degrees) will be sending its list to its adjacent
    nodes, hence minimizing the associated cost. This optimization requires the graph
    to be canonical (a source ID is lower than a destination ID) and partitioned.
    Using our graph of persons, this can be done as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有高度连接的顶点的大型网络中，向每个邻居发送相邻顶点的列表可能会耗时且网络密集。在GraphX中，`triangleCount`函数已经经过优化，因此对于每条边，只有最不重要的顶点（按度数而言）将向其相邻节点发送其列表，从而最小化相关成本。此优化要求图形是规范的（源ID小于目标ID）并且被分区。使用我们的人员图，可以按以下方式完成：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'A prerequisite of the WCC optimization is to remove edges that are not part
    of any triangle since they will not be contributing to communities. We therefore
    need to count the number of triangles, the degree of each vertex, the neighbor''s
    IDs, and we finally remove edges where the intersection of neighbor''s IDs is
    empty. Filtering out these edges can be done using the `subGraph` method that
    takes both a `filter` function for edges'' triplets and a `filter` function for
    vertices as input arguments:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: WCC优化的先决条件是删除不属于任何三角形的边，因为它们不会对社区做出贡献。因此，我们需要计算三角形的数量，每个顶点的度数，邻居的ID，最后删除邻居ID的交集为空的边。可以使用`subGraph`方法来过滤这些边，该方法接受边三元组的`filter`函数和顶点的`filter`函数作为输入参数：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Because we removed all edges that were not closing any triangle, the number
    of degrees for each vertex becomes the number of distinct vertices a given vertex
    is closing triangles with. Finally, we create our initial `VState` graph as follows,
    where each vertex becomes a center node of its own community:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们删除了没有闭合任何三角形的所有边，因此每个顶点的度数变成了给定顶点与三角形闭合的不同顶点的数量。最后，我们按照以下方式创建我们的初始`VState`图，其中每个顶点都成为其自己社区的中心节点：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Initial communities
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始社区
- en: 'The second step of this phase is to initialize communities using these initial
    WCC values. We define our initial set of communities as being consistent if and
    only if the following three requirements are all met:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段的第二步是使用这些初始WCC值初始化社区。我们定义我们的初始社区集合只有在满足以下三个要求时才是一致的：
- en: Any community must contain a single center node and border nodes, and all border
    vertices must be connected to the community center
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何社区必须包含单个中心节点和边界节点，并且所有边界顶点必须连接到社区中心
- en: Any community center must have the highest clustering coefficient in its community
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何社区中心必须具有其社区中最高的聚类系数
- en: A border vertex that is connected to two different centers (therefore two different
    communities according to rule 1) must be part of the community whose center has
    the highest clustering coefficient
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到两个不同中心（因此根据规则1属于两个不同社区）的边界顶点必须属于其中心具有最高聚类系数的社区
- en: Message passing
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消息传递
- en: In order to define our initial communities, each vertex needs to send information
    to its neighbors, including its ID, its clustering coefficient, its degrees, and
    the current community it belongs to. For convenience, we will send the main vertex
    attribute `VState` class as a message as it already contains all this information.
    Vertices will receive these messages from their neighborhood, will select the
    best one with the highest WCC score (within our `getBestCid` method), highest
    degree, highest ID, and will update their community accordingly.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义我们的初始社区，每个顶点都需要向其邻居发送信息，包括其ID，其聚类系数，其度数和它当前所属的社区。为方便起见，我们将发送主要顶点属性`VState`类作为消息，因为它已经包含了所有这些信息。顶点将从其邻域接收这些消息，将选择具有最高WCC分数（在我们的`getBestCid`方法中），最高度数，最高ID的最佳消息，并相应地更新其社区。
- en: 'This communication across vertices is a perfect use case for the `aggregateMessages`
    function, the equivalent of the map-reduce paradigm in GraphX. This function requires
    two functions to be implemented, one that sends a message from one vertex to its
    adjacent node, and one that aggregates multiple messages at the vertex level.
    This process is called *message passing* and is described as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 顶点之间的这种通信是`aggregateMessages`函数的一个完美用例，它相当于GraphX中的映射-减少范式。这个函数需要实现两个函数，一个是从一个顶点向其相邻节点发送消息，另一个是在顶点级别聚合多个消息。这个过程被称为*消息传递*，并且描述如下：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: An example of this community initialization process is reported in *Figure 4*.
    The left graph, with its nodes proportionally resized to reflect their true WCC
    coefficients, has been initialized with four different communities, **1**, **11**,
    **16**, and **21**.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 社区初始化过程的一个示例报告在*图4*中。左图的节点按比例调整大小以反映其真实的WCC系数，已经初始化为四个不同的社区，**1**，**11**，**16**和**21**。
- en: '![Message passing](img/image_07_007.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![消息传递](img/image_07_007.jpg)'
- en: 'Figure 4: WCC community initialization'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：WCC社区初始化
- en: Although one will surely appreciate that a single `aggregateMessages` function
    was returning relatively consistent communities, this initial partitioning violates
    the third of the rules we defined earlier. Some vertices (such as **2**, **3**,
    **4**, and **5**) belongs to a community whose center is not a center node (vertex
    **1** belongs to community **21**). This same issue is noticed for community **11**.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人们肯定会欣赏到一个`aggregateMessages`函数返回了相对一致的社区，但这种初始分区违反了我们之前定义的规则中的第三条。一些顶点（如**2**，**3**，**4**和**5**）属于一个中心不是中心节点的社区（顶点**1**属于社区**21**）。对于社区**11**也存在同样的问题。
- en: Community back propagation
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 社区回传
- en: In order to address this inconsistency and respect our third requirement, any
    vertex *x* must broadcast its updated community to all of its neighbors having
    a lower coefficient, as, according to our second rule, only these lower ranked
    vertices could potentially become a border node of *x*. Any further update will
    result in a new message to be passed to lower rank vertices, and so on, until
    no vertices will change community, at which point the third of our rules will
    be satisfied.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这种不一致性并满足我们的第三个要求，任何顶点*x*必须将其更新的社区广播给所有系数较低的邻居，因为根据我们的第二条规则，只有这些排名较低的顶点可能成为*x*的边界节点。任何进一步的更新都将导致向较低排名的顶点传递新消息，依此类推，直到没有顶点会改变社区，此时我们的第三条规则将得到满足。
- en: Since no global knowledge of the graph is required between iterations (such
    as counting the global WCC value), community updates can be extensively parallelized
    using the Pregel API of GraphX. Initially developed at Google, Pregel allows vertices
    to receive messages from previous iterations, send new messages to their neighborhood
    and modify their own state until no further messages could be sent.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于迭代之间不需要图的全局知识（例如计算全局WCC值），使用GraphX的Pregel API可以广泛并行化社区更新。Pregel最初由Google开发，允许顶点接收来自先前迭代的消息，向其邻域发送新消息，并修改自己的状态，直到不能再发送更多消息。
- en: Note
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information about *Pregel* algorithm, refer to the publication: *G.
    Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G.
    Czajkowski, "Pregel: A system for large-scale graph processing," in Proceedings
    of the 2010 ACM SIGMOD International Conference on Management of Data, ser. SIGMOD
    ''10\. New York, NY, USA: ACM, 2010, pp. 135-146\. [Online]. Available: [http://doi.acm.org/10.1145/1807167.1807184](http://doi.acm.org/10.1145/1807167.1807184)*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '有关*Pregel*算法的更多信息，请参阅以下出版物：*G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert,
    I. Horn, N. Leiser, and G. Czajkowski, "Pregel: A system for large-scale graph
    processing," in Proceedings of the 2010 ACM SIGMOD International Conference on
    Management of Data, ser. SIGMOD ''10\. New York, NY, USA: ACM, 2010, pp. 135-146\.
    [Online]. Available: [http://doi.acm.org/10.1145/1807167.1807184](http://doi.acm.org/10.1145/1807167.1807184)*'
- en: Similar to the `aggregateMessages` function mentioned earlier, we will send
    the vertex attribute `VState` as a message across vertices, with, as an initial
    message for Pregel super step, a new object initialized with default values (WCC
    of 0).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前提到的`aggregateMessages`函数类似，我们将顶点属性`VState`作为消息发送到顶点之间，作为Pregel超步的初始消息，使用默认值初始化的新对象（WCC为0）。
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When more than one message is received at the vertex level, we only keep the
    one with the highest clustering coefficient, and given the same coefficient, the
    one with the highest degree (and then the highest ID). We create an implicit ordering
    on `VState` for that purpose:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当在顶点级别接收到多个消息时，我们只保留具有最高聚类系数的消息，如果系数相同，则保留具有最高度数的消息（然后是最高ID）。我们为此目的在`VState`上创建了一个隐式排序：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Following the same principle as per recursive algorithms, we need to properly
    define a breaking clause at which point Pregel should stop sending and processing
    messages. This will be done within the send function that takes an edge triplet
    as an input and returns an iterator of messages. A vertex will send its `VState`
    attribute if and only if its community has changed over the previous iteration.
    In that case, the vertex will inform its lower ranked neighbors about its community
    update but will also send a signal to itself to acknowledge this successful broadcast.
    The latter is our breaking clause as it ensures no further message will be sent
    from that given node (unless its community gets updated in the further steps):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循递归算法的相同原则，我们需要适当地定义一个中断子句，Pregel应在该点停止发送和处理消息。这将在发送函数中完成，该函数以边三元组作为输入并返回消息的迭代器。如果顶点的社区在上一次迭代中发生了变化，顶点将发送其`VState`属性。在这种情况下，顶点将通知其排名较低的邻居其社区更新，但也会向自己发送信号以确认此成功广播。后者是我们的中断子句，因为它确保不会从给定节点发送更多消息（除非其社区在后续步骤中得到更新）：
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The last function to implement is the core function of the Pregel algorithm.
    Here we define the logic to be applied at the vertex level given the unique message
    we selected from the `mergeMsg` function. We identify four different possibilities
    of messages, each of them defined with the logic to be applied on the vertex status.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要实现的函数是Pregel算法的核心函数。在这里，我们定义了在顶点级别应用的逻辑，给定我们从`mergeMsg`函数中选择的唯一消息。我们确定了四种不同的消息可能性，每种消息都定义了应用于顶点状态的逻辑。
- en: If the message is the initial message sent from Pregel (vertex ID is not set,
    WCC is null), we do not update the vertex community ID.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果消息是从Pregel发送的初始消息（顶点ID未设置，WCC为空），我们不会更新顶点社区ID。
- en: If the message comes from the vertex itself, this is an acknowledgement from
    the `sendMsg` function, we set the vertex status to silent.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果消息来自顶点本身，这是来自`sendMsg`函数的确认，我们将顶点状态设置为静默。
- en: If the message (with higher WCC) comes from a center node of a community, we
    update the vertex attribute to be a border node of this new community.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果消息（带有更高的WCC）来自社区的中心节点，我们将更新顶点属性为这个新社区的边界节点。
- en: If the message (with higher WCC) comes from a border node of a community, this
    vertex becomes a center of its own community and will broadcast this update further
    to its lower-ranked network.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果消息（带有更高的WCC）来自社区的边界节点，这个顶点将成为自己社区的中心，并将进一步将此更新广播给其排名较低的网络。
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we chain all these three functions together using the `apply` function
    of the `Pregel` object. We set the maximum number of iterations to infinity as
    we rely on the breaking clause we defined using an acknowledgment type message:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`Pregel`对象的`apply`函数将这三个函数链接在一起。我们将迭代的最大次数设置为无穷大，因为我们依赖于我们使用确认类型消息定义的中断子句：
- en: '[PRE28]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Although the concept of Pregel is fascinating, its implementation is certainly
    not. As a reward to this tremendous effort, we display the resulting graph in
    *Figure 5* next. Vertices **1** and **11** are still part of community **21**
    which remains valid, but communities **1** and **11** have now been replaced with
    communities **15** and **5** respectively, vertices having the highest clustering
    coefficient, degree, or ID in their community, hence validating the third requirement:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Pregel的概念很迷人，但它的实现确实不是。作为对这一巨大努力的回报，我们在*图5*中展示了结果图。顶点**1**和**11**仍然属于社区**21**，这仍然有效，但社区**1**和**11**现在分别被替换为社区**15**和**5**，顶点具有最高的聚类系数、度或ID在其社区中，因此验证了第三个要求：
- en: '![Community back propagation](img/image_07_008.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![社区反向传播](img/image_07_008.jpg)'
- en: 'Figure 5: Community back propagation update'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：社区反向传播更新
- en: We used the Pregel API to create our initial set of communities with respect
    to the rules introduced earlier, but we are not set yet. The preceding figure
    definitely suggests some improvements that will be addressed in the following
    subsection. However, before moving forward, one can notice that no particular
    partitioning was used here. If we were to send multiple messages across communities'
    nodes, and if those vertices were located on different partitions (hence different
    executors), we would certainly not optimize the network traffic related to message
    passing. Different sorts of partitioning exist in GraphX, but none of them allow
    us to use a vertex attribute such as the community ID as a measure of the partitioning.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Pregel API根据之前介绍的规则创建了我们的初始社区集，但我们还没有完成。前面的图表明了一些改进，这些将在下一小节中讨论。然而，在继续之前，可以注意到这里没有使用特定的分区。如果我们要在社区节点之间发送多条消息，并且这些顶点位于不同的分区（因此位于不同的执行器），我们肯定不能优化与消息传递相关的网络流量。GraphX中存在不同类型的分区，但没有一种允许我们使用顶点属性（如社区ID）作为分区的度量。
- en: 'In the following simple function, we extract all graph triplets, build a hashcode
    out of a community tuple and repartition this edge RDD using the standard key
    value `HashPartitioner` class. We finally build a new graph out of this repartitioned
    set so that we guarantee all vertices connected from a community C1 to a community
    C2 will all belong to the same partition:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的简单函数中，我们提取所有的图三元组，根据社区元组构建一个哈希码，并使用标准的键值`HashPartitioner`类重新分区这个边RDD。最后，我们根据这个重新分区的集合构建一个新的图，以确保从社区C1连接到社区C2的所有顶点都属于同一个分区：
- en: '[PRE29]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: WCC iteration
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WCC迭代
- en: 'The purpose of this stage is to iteratively let all vertices choose between
    the following three options until the WCC value cannot be longer optimized, at
    which point our community detection algorithm would converge to its optimal graph
    structure:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段的目的是让所有顶点在以下三个选项之间进行迭代选择，直到WCC值不能再被优化为止，此时我们的社区检测算法将收敛到其最佳图结构：
- en: '**STAY**: To stay in its community'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**留下**：留在它的社区里'
- en: '**TRANSFER**: To move from its community and become part of its neighbor''s
    community'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转移**：从它的社区移动并成为它的邻居的一部分'
- en: '**REMOVE**: To leave its community and become part of its own community'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除**：离开它的社区，成为自己社区的一部分'
- en: For each vertex, the best movement is the one that maximizes the total WCC value.
    Similar to the Louvain approach, each movement depends on a global score to be
    computed, but the reason we turned to this algorithm is that this score can be
    approximated using a heuristic defined in *High quality, scalable and parallel
    community detection for large real graphs* from Arnau Prat-Pérez et. al. Because
    this heuristic does not require the computation of all internal triangles, vertices
    can all move simultaneously and this process can therefore be designed in a fully
    decentralized and highly scalable way.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个顶点，最佳移动是最大化总WCC值的移动。与Louvain方法类似，每个移动都取决于要计算的全局分数，但我们转向这个算法的原因是，这个分数可以使用Arnau
    Prat-Pérez等人在*用于大型实际图的高质量、可扩展和并行社区检测*中定义的启发式方法来近似。因为这个启发式方法不需要计算所有内部三角形，顶点可以同时移动，因此这个过程可以设计成完全分散和高度可扩展的。
- en: Gathering community statistics
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 收集社区统计信息
- en: 'In order to compute this heuristic, we first need to aggregate basic statistics
    at the community level such as the number of elements and the number of inbound
    and outbound links, both of them expressed as a simple word count function here.
    We combine them in memory as the number of communities will be considerably smaller
    than the number of vertices:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个启发式方法，我们首先需要在社区级别聚合基本统计数据，比如元素数量和入站和出站链接数量，这两者都可以用简单的词频函数来表示。我们将它们组合在内存中，因为社区的数量将远远小于顶点的数量：
- en: '[PRE30]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we collect both the number of vertices and the community statistics
    (including the community edge density) and broadcast the results to all of our
    Spark executors:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们收集顶点数量和社区统计信息（包括社区边缘密度），并将结果广播到我们所有的Spark执行器：
- en: '[PRE31]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Tip
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: It is important to understand the use of the `broadcast` method here. If the
    community statistics are used within a Spark transformation, this object will
    be sent to the executors for each record the latter has to process. We compute
    them once, broadcast the result to the executors' caches so that any closure can
    locally make use them, hence saving lots of unnecessary network transfer.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里理解`broadcast`方法的使用是很重要的。如果社区统计信息在Spark转换中使用，这个对象将被发送到执行器，以便后者处理每条记录。我们计算它们一次，将结果广播到执行器的缓存中，以便任何闭包可以在本地使用它们，从而节省大量不必要的网络传输。
- en: WCC Computation
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: WCC计算
- en: 'According to the set of equations defined earlier, each vertex must have access
    to the community statistics it belongs to and the number of triangles it closes
    with any vertex inside of its community. For that purpose, we collect neighbors
    via a simple message passing, but only on the vertices within the same community,
    thus limiting the network traffic:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前定义的一系列方程，每个顶点必须访问其所属的社区统计数据以及它与社区内任何顶点之间的三角形数量。为此，我们通过简单的消息传递来收集邻居，但只限于同一社区内的顶点，从而限制网络流量：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Similarly, we count the number of shared triangles using the following function.
    Note that we use the same optimization as per the default `triangleCount` method
    using the smallest set only to send messages to the largest one.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们使用以下函数来计算共享三角形的数量。请注意，我们使用与默认的`triangleCount`方法相同的优化，只使用最小集合向最大集合发送消息。
- en: '[PRE33]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We compute and update the new WCC score of each vertex as a function of the
    community neighborhood size and the number of community triangles. This equation
    is the one described earlier while introducing the WCC algorithm. We compute a
    score as a ratio of triangles closed inside versus outside of a community C given
    a vertex *x*:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算并更新每个顶点的新WCC分数，作为社区邻域大小和社区三角形数量的函数。这个方程就是之前介绍WCC算法时描述的方程。我们计算一个分数，作为社区C内外闭合的三角形的比率，给定一个顶点*x*：
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The global WCC value is a simple aggregation of each vertex WCC normalized
    with the number of elements in each community. This value must be broadcast to
    Spark executors too as it will be used inside of a Spark transformation:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 全球WCC值是每个顶点WCC的简单聚合，经过每个社区中元素数量的归一化。这个值也必须广播到Spark执行器中，因为它将在Spark转换中使用：
- en: '[PRE35]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: WCC iteration
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: WCC迭代
- en: 'Given the cost of inserting a vertex *x* into a community **C**, the costs
    of removing/transferring *x* from/to a community **C** can be expressed as a function
    of the former, and can be derived from three parameters **Θ[1]**, **Θ[2]**, and
    **Θ[3]**. This heuristic states that for each vertex *x*, a single computation
    is needed for each of its surrounding communities **C**, and can be done in parallel
    assuming we gathered all the community statistics in the first place:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到将顶点*x*插入到社区**C**的成本，从/向社区**C**移除/转移*x**的成本可以表示为前者的函数，并且可以从三个参数**Θ[1]**、**Θ[2]**和**Θ[3]**中导出。这个启发式规定，对于每个顶点*x*，需要对其周围的每个社区**C**进行一次计算，并且可以并行进行，假设我们首先收集了所有社区统计数据：
- en: '![WCC iteration](img/image_07_009.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![WCC迭代](img/image_07_009.jpg)'
- en: The computation of **Θ[1]**, **Θ[2]**, and **Θ[3]** will not be reported here
    (it is available on our GitHub), but depends on the community density, the external
    edges, and the number of elements, all of them available within our broadcasted
    set of `CommunityStats` objects defined earlier. Finally, it is worth mentioning
    that this computation has a linear time complexity.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**Θ[1]**、**Θ[2]**和**Θ[3]**的计算将不在此处报告（可在我们的GitHub上找到），但取决于社区密度、外部边缘和元素数量，所有这些都包含在我们之前定义的广播的`CommunityStats`对象集合中。最后值得一提的是，这个计算具有线性时间复杂度。'
- en: At each iteration, we will collect the different communities surrounding any
    vertex, and will aggregate the number of edges using the `mappend` aggregation
    from Scalaz that we introduced in [Chapter 6](ch06.xhtml "Chapter 6. Scraping
    Link-Based External Data"), *Scraping Link-Based External Data*. This helps us
    to limit the amount of code written and avoids using mutable objects.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们将收集任何顶点周围的不同社区，并使用我们在[第6章](ch06.xhtml "第6章。抓取基于链接的外部数据")中介绍的Scalaz的`mappend`聚合来聚合边的数量，*抓取基于链接的外部数据*。这有助于我们限制编写的代码量，并避免使用可变对象。
- en: '[PRE36]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Using the community statistics, the WCC value from the previous iteration, the
    number of vertices and the above edges count, we can now estimate the cost of
    inserting each vertex *x* into a surrounding community **C**. We find the local
    best movement for each vertex and for each of its surrounding communities, and
    finally apply the best one that maximizes the WCC value.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 利用社区统计数据、上一次迭代的WCC值、顶点数量和上述边的数量，我们现在可以估算将每个顶点*x*插入到周围社区**C**中的成本。我们找到每个顶点的最佳移动以及其周围社区的最佳移动，最终应用最大化WCC值的最佳移动。
- en: Finally, we call back the set of methods and functions defined earlier in order
    to update the new WCC value for each vertex, for each community, and then for
    the graph partition itself to see whether or not all these changes resulted in
    any WCC improvement. If the WCC value cannot be optimized any longer, the algorithm
    has converged to its optimal structure and we finally return a vertex RDD containing
    both the vertex ID and the final community ID this vertex belongs to.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们回调之前定义的一系列方法和函数，以更新每个顶点、每个社区的新WCC值，然后更新图分区本身，以查看所有这些变化是否导致了WCC的改善。如果WCC值无法再进行优化，算法就已经收敛到了最佳结构，最终我们返回一个包含顶点ID和该顶点所属的最终社区ID的顶点RDD。
- en: 'Our test community graph has been optimized (not without its fair share of
    effort) and reported as shown in *Figure 6*:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试社区图已经经过优化（虽然不是没有付出努力），并如*图6*所示报告：
- en: '![WCC iteration](img/image_07_010.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![WCC迭代](img/image_07_010.jpg)'
- en: 'Figure 6: WCC optimized communities'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：WCC优化的社区
- en: We observe all the changes we were expecting from the previous figures. Vertices
    **1** and **11** are now part of their expected communities, respectively **5**
    and **11**. We also note that vertex 16 has now been included within its community
    11.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到之前预期的所有变化。顶点**1**和**11**现在分别属于它们预期的社区，分别是**5**和**11**。我们还注意到顶点16现在已经包括在其社区11中。
- en: GDELT dataset
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GDELT数据集
- en: 'In order to validate our implementation, we use the GDELT dataset we analyzed
    in the previous chapter. We extracted all of the communities and spent some time
    looking at the person names to see whether or not our community clustering was
    consistent. The full picture of the communities is reported in *Figure 7* and
    has been realized using the Gephi software, where only the top few thousand connections
    have been imported:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的实现，我们使用了我们在上一章中分析过的GDELT数据集。我们提取了所有的社区，并花了一些时间查看人名，以确定我们的社区聚类是否一致。社区的完整图片报告在*图7*中，并且是使用Gephi软件实现的，只导入了前几千个连接。
- en: '![GDELT dataset](img/B05261_07_11-1.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![GDELT数据集](img/B05261_07_11-1.jpg)'
- en: 'Figure 7: Community detection on January 12'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：2021年1月12日的社区检测
- en: We first observe that most of the communities we detected are totally aligned
    with the ones we could eyeball on a force-directed layout, giving a good confidence
    level about the algorithm accuracy.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先观察到，我们检测到的大多数社区与我们在力导向布局中可以直观看到的社区完全一致，这给算法准确性带来了很高的信心水平。
- en: The Bowie effect
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鲍伊效应
- en: Any well-defined community has been properly identified, and the less obvious
    ones are the ones surrounding highly connected vertices such as David Bowie. The
    name David Bowie being heavily mentioned in GDELT articles alongside so many different
    persons that, on that day of January 12, 2016, it became too large to be part
    of its logical community (music industry) and formed a broader community impacting
    all its surrounding vertices. There is definitely an interesting pattern here
    as this community structure gives us clear insights about a potential breaking
    news article for a particular person on a particular day.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 任何明确定义的社区都已经被正确识别，而不太明显的社区是围绕着像大卫·鲍伊这样的高度连接的顶点而形成的。大卫·鲍伊这个名字在GDELT文章中被频繁提及，与许多不同的人一起，以至于在2016年1月12日，它变得太大，无法成为其逻辑社区（音乐行业）的一部分，并形成了一个更广泛的社区，影响了其周围的所有顶点。这里绝对存在一个有趣的模式，因为这种社区结构为我们提供了关于特定人物在特定日期可能成为突发新闻文章的明确见解。
- en: Looking at the David Bowie's closest communities in *Figure 8*, we observe the
    nodes to be highly interconnected because of what we will be calling the *Bowie
    effect*. In fact, there have been so many tributes paid from so many various communities
    that the number of triangles formed across different communities has been abnormally
    high. As a result, it brought different logical communities closer to each other,
    communities that were theoretically not meant to be, such as the *70s* rock star
    idols close enough to religious people.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 观察大卫·鲍伊在*图8*中最接近的社区，我们观察到节点之间高度相互连接，这是因为我们将其称为*鲍伊效应*。事实上，来自许多不同社区的许多致敬使得跨不同社区形成的三角形数量异常高。结果是，它将不同的逻辑社区彼此靠近，这些社区在理论上本不应该靠近，比如*70年代*的摇滚明星偶像与宗教人士之间的接近。
- en: The small world phenomenon, as defined in the 60s by Stanley Milgram, states
    that everyone is connected through a short number of acquaintances. Kevin Bacon,
    an American actor, even suggested he would be connected to every other actor by
    a maximum depth of 6 connections, also known as its *Bacon Number* ([https://oracleofbacon.org/](https://oracleofbacon.org/)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 小世界现象是由斯坦利·米尔格拉姆在60年代定义的，它指出每个人都通过少数熟人相连。美国演员凯文·贝肯甚至建议他与其他任何演员之间最多只能通过6个连接相连，也被称为*贝肯数*（[https://oracleofbacon.org/](https://oracleofbacon.org/)）。
- en: On that day, the *Kevin Bacon Number* of Pope Francis and Mick Jagger was only
    1 thanks to the Cardinal Gianfranco Ravasi who tweeted about David Bowie.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在那一天，教皇弗朗西斯和米克·贾格尔的*凯文·贝肯数*仅为1，这要归功于主教吉安弗兰科·拉瓦西在推特上提到了大卫·鲍伊。
- en: '![The Bowie effect](img/B05261_07_12-1.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![鲍伊效应](img/B05261_07-12-1.jpg)'
- en: 'Figure 8: Communities surrounding David Bowie, January 12'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：围绕大卫·鲍伊的社区，1月12日
- en: Although the Bowie's effect, by its nature of a breaking news article, is a
    true pattern on that particular graph structure, its effect could have been minimized
    using weighted edges based on names frequency count. Indeed, some random noise
    from the GDELT dataset could be enough to close critical triangles from two different
    communities and therefore bring them close to each other, no matter the weight
    of this critical edge. This limitation is common for all un-weighted algorithms
    and would require a preprocessing phase to reduce this unwanted noise.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管鲍伊效应，由于其作为突发新闻文章的性质，在特定的图结构上是一个真正的模式，但它的影响可以通过基于名称频率计数的加权边来最小化。事实上，来自GDELT数据集的一些随机噪音可能足以关闭来自两个不同社区的关键三角形，从而将它们彼此靠近，无论这个关键边的权重如何。这种限制对于所有非加权算法都是普遍存在的，并且需要一个预处理阶段来减少这种不需要的噪音。
- en: Smaller communities
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 较小的社区
- en: 'We can, however, observe some more defined communities here, such as the UK
    politicians Tony Blair, David Cameron, and Boris Johnson or the movie directors
    Christopher Nolan, Martin Scorsese, or Quentin Tarantino. Looking at a broader
    level, we can detect well-defined communities, such as tennis players, footballers,
    artists, or politicians of a specific country. As an undeniable proof of accuracy,
    we even detected Matt Leblanc, Courtney Cox, Matthew Perry, and Jennifer Anniston
    as being part of a same Friends community and Luke Skywalker, Anakin Skywalker,
    Chewbacca, and Emperor Palpatine as part of the Star Wars community and its recently
    lost actress, Carrie Fisher. An example of professional boxer''s communities is
    reported in *Figure 9*:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以观察到一些更明确定义的社区，比如英国政治家托尼·布莱尔、大卫·卡梅伦和鲍里斯·约翰逊，或者电影导演克里斯托弗·诺兰、马丁·斯科塞斯和昆汀·塔伦蒂诺。从更广泛的角度来看，我们可以检测到明确定义的社区，比如网球运动员、足球运动员、艺术家或特定国家的政治家。作为准确性的不容置疑的证据，我们甚至检测到马特·勒布朗、考特尼·考克斯、马修·佩里和詹妮弗·安妮斯顿作为同一个《老友记》社区的一部分，卢克·天行者、阿纳金·天行者、乔巴卡和帕尔帕廷皇帝作为《星球大战》社区的一部分，以及最近失去的女演员凯丽·费雪。职业拳击手社区的一个例子如*图9*所示：
- en: '![Smaller communities](img/B05261_07_13-1.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![较小的社区](img/B05261_07_13-1.jpg)'
- en: 'Figure 9: Professional boxer communities'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：职业拳击手社区
- en: Using Accumulo cell level security
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Accumulo单元格级安全
- en: 'We have previously discussed the nature of cell-level security in Accumulo.
    In the context of the graphs that we have produced here, the usefulness of security
    can be well simulated. If we configure Accumulo such that rows containing David
    Bowie are securely labeled differently to all other rows, then we can turn on
    and off the Bowie''s effect. Any Accumulo user with full access will see the complete
    graph provided earlier. If we then restrict that user to everything other than
    David Bowie (a simple change to the Authorization in `AccumuloReader`), then we
    see the following figure. This new graph is very interesting as it serves a number
    of purposes:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经讨论了Accumulo中单元级安全的性质。在这里我们生成的图的背景下，安全性的有用性可以很好地模拟。如果我们配置Accumulo，使得包含大卫·鲍伊的行与所有其他行标记不同的安全标签，那么我们可以打开和关闭鲍伊的效应。任何具有完全访问权限的Accumulo用户将看到之前提供的完整图。然后，如果我们将该用户限制在除了大卫·鲍伊之外的所有内容（在`AccumuloReader`中对授权进行简单更改），那么我们将看到以下图。这个新图非常有趣，因为它具有多种用途：
- en: It removes the noise created by the social media effect of David Bowie's death,
    thereby revealing the true communities involved
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它消除了大卫·鲍伊死亡的社交媒体效应所产生的噪音，从而揭示了真正涉及的社区
- en: It removes many of the false links between entities, thereby increasing their
    Bacon number and showing their true relationship
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它消除了实体之间的许多虚假链接，从而增加了它们的Bacon数，并显示了它们真正的关系
- en: It demonstrates that it is possible to remove a key figure in a graph and still
    retain a large amount of useful information, thereby demonstrating the point made
    earlier regarding the removal of key entities for security reasons (as discussed
    in *Cell security*)
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它证明了可以移除图中的一个关键人物，仍然保留大量有用信息，从而证明了之前关于出于安全原因移除关键实体的观点（如*单元安全*中讨论的）。
- en: It also has to be said, of course, that by removing an entity, we may also be
    removing key relationships between entities; that is, the contact chaining effect
    and this is a negative aspect when specifically trying to relate individual entities-overall,
    the communities, however, remain intact.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还必须说，通过移除一个实体，我们也可能移除实体之间的关键关系；也就是说，联系链效应，这在特定试图关联个体实体时是一个负面因素，然而，社区仍然保持完整。
- en: '![Using Accumulo cell level security](img/B05261_07_14.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![使用Accumulo单元级安全](img/B05261_07_14.jpg)'
- en: 'Figure 10: David Bowie''s communities with restricted access'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：大卫·鲍伊的受限访问社区
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have discussed and built a real-world implementation of graph communities
    leveraging the power of a secure and robust architecture. We have outlined the
    idea that there is no right or wrong solution in the community detection problem
    space, as it strongly depends on the use case. In a social network context, for
    example, where vertices are tightly connected together (an edge represents a true
    connection between two users), the edge weight does not really matter while the
    triangle approach probably does. In the telecommunication industry, one could
    be interested in the communities based on the frequency call of a given user A
    to a user B, hence turning to a weighted algorithm such as Louvain.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论并构建了一个利用安全和稳健架构的图社区的实际实现。我们已经概述了在社区检测问题空间中没有正确或错误的解决方案，因为它严重依赖于使用情况。例如，在社交网络环境中，其中顶点紧密连接在一起（一条边表示两个用户之间的真实连接），边的权重并不重要，而三角形方法可能更重要。在电信行业中，人们可能对基于给定用户A对用户B的频率呼叫的社区感兴趣，因此转向加权算法，如Louvain。
- en: We appreciate that building this community algorithm was far from an easy task,
    and perhaps stretches the goals of this book, but it involves all of the techniques
    of graph processing in Spark that makes GraphX a fascinating and extensible tool.
    We introduced the concepts of message passing, Pregel, graph partitioning, and
    variable broadcast, backed by a real-world implementation in Elasticsearch and
    Accumulo.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢构建这个社区算法远非易事，也许超出了本书的目标，但它涉及了Spark中图处理的所有技术，使GraphX成为一个迷人且可扩展的工具。我们介绍了消息传递、Pregel、图分区和变量广播的概念，支持了Elasticsearch和Accumulo中的实际实现。
- en: In the next chapter, we will apply the concepts of graph theory we learned here
    to the music industry, learning how to build a music recommendation engine using
    audio signal, Fourier transforms, and *PageRank* algorithm.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将应用我们在这里学到的图论概念到音乐行业，学习如何使用音频信号、傅立叶变换和*PageRank*算法构建音乐推荐引擎。
