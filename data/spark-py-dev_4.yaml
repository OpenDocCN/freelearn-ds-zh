- en: Chapter 4. Learning from Data Using Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。使用Spark从数据中学习
- en: As we have laid the foundation for data to be harvested in the previous chapter,
    we are now ready to learn from the data. Machine learning is about drawing insights
    from data. Our objective is to give an overview of the Spark **MLlib** (short
    for **Machine Learning library**) and apply the appropriate algorithms to our
    dataset in order to derive insights. From the Twitter dataset, we will be applying
    an unsupervised clustering algorithm in order to distinguish between Apache Spark-relevant
    tweets versus the rest. We have as initial input a mixed bag of tweets. We first
    need to preprocess the data in order to extract the relevant features, then apply
    the machine learning algorithm to our dataset, and finally evaluate the results
    and the performance of our model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们已经为数据的收集奠定了基础，现在我们准备从数据中学习。机器学习是关于从数据中获取见解。我们的目标是概述Spark MLlib（简称机器学习库）并将适当的算法应用于我们的数据集，以便得出见解。从Twitter数据集中，我们将应用无监督的聚类算法，以区分Apache
    Spark相关的推文和其他推文。我们首先需要预处理数据，以提取相关特征，然后将机器学习算法应用于我们的数据集，最后评估模型的结果和性能。
- en: 'In this chapter, we will cover the following points:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Providing an overview of the Spark MLlib module with its algorithms and the
    typical machine learning workflow.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供Spark MLlib模块及其算法以及典型的机器学习工作流程的概述。
- en: Preprocessing the Twitter harvested dataset to extract the relevant features,
    applying an unsupervised clustering algorithm to identify *Apache Spark*-relevant
    tweets. Then, evaluating the model and the results obtained.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理Twitter收集的数据集，以提取相关特征，应用无监督的聚类算法来识别Apache Spark相关的推文。然后，评估模型和获得的结果。
- en: Describing the Spark machine learning pipeline.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述Spark机器学习管道。
- en: Contextualizing Spark MLlib in the app architecture
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在应用架构中定位Spark MLlib
- en: Let's first contextualize the focus of this chapter on data-intensive app architecture.
    We will concentrate our attention on the analytics layer and more precisely machine
    learning. This will serve as a foundation for streaming apps as we want to apply
    the learning from the batch processing of data as inference rules for the streaming
    analysis.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将本章的重点放在数据密集型应用架构上。我们将集中精力放在分析层，更确切地说是机器学习上。这将为流应用提供基础，因为我们希望将从数据的批处理中学到的知识应用于流分析的推理规则。
- en: The following diagram sets the context of the chapter's focus, highlighting
    the machine learning module within the analytics layer while using tools for exploratory
    data analysis, Spark SQL, and Pandas.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表设置了本章重点的上下文，突出了分析层内的机器学习模块，同时使用了探索性数据分析、Spark SQL和Pandas工具。
- en: '![Contextualizing Spark MLlib in the app architecture](img/B03986_04_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![在应用架构中定位Spark MLlib](img/B03986_04_01.jpg)'
- en: Classifying Spark MLlib algorithms
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对Spark MLlib算法进行分类
- en: Spark MLlib is a rapidly evolving module of Spark with new algorithms added
    with each release of Spark.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib是Spark的一个快速发展模块，每次Spark发布都会添加新的算法。
- en: 'The following diagram provides a high-level overview of Spark MLlib algorithms
    grouped in the traditional broad machine learning techniques and following the
    categorical or continuous nature of the data:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表提供了Spark MLlib算法的高级概述，分为传统的广义机器学习技术和数据的分类或连续性特性：
- en: '![Classifying Spark MLlib algorithms](img/B03986_04_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![对Spark MLlib算法进行分类](img/B03986_04_02.jpg)'
- en: We categorize the Spark MLlib algorithms in two columns, categorical or continuous,
    depending on the type of data. We distinguish between data that is categorical
    or more qualitative in nature versus continuous data, which is quantitative in
    nature. An example of qualitative data is predicting the weather; given the atmospheric
    pressure, the temperature, and the presence and type of clouds, the weather will
    be sunny, dry, rainy, or overcast. These are discrete values. On the other hand,
    let's say we want to predict house prices, given the location, square meterage,
    and the number of beds; the real estate value can be predicted using linear regression.
    In this case, we are talking about continuous or quantitative values.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Spark MLlib算法分为两列，根据数据类型分为分类或连续。我们区分分类或更具有定性特征的数据与连续数据，后者是定量的。定性数据的一个例子是预测天气；给定大气压、温度和云的存在和类型，天气将是晴天、干燥、多雨或阴天。这些是离散值。另一方面，假设我们想要预测房价，给定位置、平方米和床的数量；可以使用线性回归来预测房地产价值。在这种情况下，我们谈论的是连续或定量值。
- en: The horizontal grouping reflects the types of machine learning method used.
    Unsupervised versus supervised machine learning techniques are dependent on whether
    the training data is labeled. In an unsupervised learning challenge, no labels
    are given to the learning algorithm. The goal is to find the hidden structure
    in its input. In the case of supervised learning, the data is labeled. The focus
    is on making predictions using regression if the data is continuous or classification
    if the data is categorical.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 水平分组反映了所使用的机器学习方法的类型。无监督与监督机器学习技术取决于训练数据是否带有标签。在无监督学习挑战中，学习算法没有标签。目标是找到输入中的隐藏结构。在监督学习的情况下，数据是有标签的。重点是使用回归进行预测，如果数据是连续的，或者使用分类，如果数据是分类的。
- en: An important category of machine learning is recommender systems, which leverage
    collaborative filtering techniques. The Amazon web store and Netflix have very
    powerful recommender systems powering their recommendations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个重要类别是推荐系统，它利用协同过滤技术。亚马逊网店和Netflix拥有非常强大的推荐系统来支持他们的推荐。
- en: '**Stochastic Gradient Descent** is one of the machine learning optimization
    techniques that is well suited for Spark distributed computation.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降是一种适合Spark分布式计算的机器学习优化技术之一。
- en: For processing large amounts of text, Spark offers crucial libraries for feature
    extraction and transformation such as **TF-IDF** (short for **Term Frequency –
    Inverse Document Frequency**), Word2Vec, standard scaler, and normalizer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于处理大量文本，Spark提供了关键的特征提取和转换库，如**TF-IDF**（**词项频率-逆文档频率**），Word2Vec，标准缩放器和归一化器。
- en: Supervised and unsupervised learning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督和无监督学习
- en: We delve more deeply here in to the traditional machine learning algorithms
    offered by Spark MLlib. We distinguish between supervised and unsupervised learning
    depending on whether the data is labeled. We distinguish between categorical or
    continuous depending on whether the data is discrete or continuous.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里更深入地探讨了Spark MLlib提供的传统机器学习算法。我们根据数据是否有标签来区分监督学习和无监督学习。我们根据数据是离散的还是连续的来区分分类或连续。
- en: 'The following diagram explains the Spark MLlib supervised and unsupervised
    machine learning algorithms and preprocessing techniques:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表解释了Spark MLlib监督和无监督机器学习算法以及预处理技术：
- en: '![Supervised and unsupervised learning](img/B03986_04_03.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![监督和无监督学习](img/B03986_04_03.jpg)'
- en: 'The following supervised and unsupervised MLlib algorithms and preprocessing
    techniques are currently available in Spark:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下监督和无监督的MLlib算法和预处理技术目前在Spark中可用：
- en: '**Clustering**: This is an unsupervised machine learning technique where the
    data is not labeled. The aim is to extract structure from the data:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：这是一种无监督的机器学习技术，其中数据没有标记。目的是从数据中提取结构：'
- en: '**K-Means**: This partitions the data in K distinct clusters'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K均值**：这将数据分区为K个不同的簇'
- en: '**Gaussian Mixture**: Clusters are assigned based on the maximum posterior
    probability of the component'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯混合**：根据组件的最大后验概率分配簇'
- en: '**Power Iteration Clustering (PIC)**: This groups vertices of a graph based
    on pairwise edge similarities'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幂迭代聚类（PIC）**：这基于图的顶点之间的成对边相似性进行分组'
- en: '**Latent Dirichlet Allocation** (**LDA**): This is used to group collections
    of text documents into topics'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）：这用于将文本文档集合分组成主题'
- en: '**Streaming K-Means**: This means clusters dynamically streaming data using
    a windowing function on the incoming data'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式K均值**：这意味着使用传入数据的窗口函数动态地对流式数据进行聚类'
- en: '**Dimensionality Reduction**: This aims to reduce the number of features under
    consideration. Essentially, this reduces noise in the data and focuses on the
    key features:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：这旨在减少考虑的特征数量。基本上，这减少了数据中的噪音，并专注于关键特征：'
- en: '**Singular Value Decomposition** (**SVD**): This breaks the matrix that contains
    the data into simpler meaningful pieces. It factorizes the initial matrix into
    three matrices.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）：这将包含数据的矩阵分解为更简单的有意义的部分。它将初始矩阵分解为三个矩阵。'
- en: '**Principal Component Analysis** (**PCA**): This approximates a high dimensional
    dataset with a low dimensional sub space.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）：这将高维数据集近似为低维子空间。'
- en: '**Regression and Classification**: Regression predicts output values using
    labeled training data, while Classification groups the results into classes. Classification
    has dependent variables that are categorical or unordered whilst Regression has
    dependent variables that are continuous and ordered:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归和分类**：回归使用标记的训练数据预测输出值，而分类将结果分组成类别。分类具有分类或无序的因变量，而回归具有连续和有序的因变量：'
- en: '**Linear Regression Models** (linear regression, logistic regression, and support
    vector machines): Linear regression algorithms can be expressed as convex optimization
    problems that aim to minimize an objective function based on a vector of weight
    variables. The objective function controls the complexity of the model through
    the regularized part of the function and the error of the model through the loss
    part of the function.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性回归模型**（线性回归，逻辑回归和支持向量机）：线性回归算法可以表示为旨在最小化基于权重变量向量的目标函数的凸优化问题。目标函数通过函数的正则化部分和损失函数控制模型的复杂性和模型的误差。'
- en: '**Naive Bayes**: This makes predictions based on the conditional probability
    distribution of a label given an observation. It assumes that features are mutually
    independent of each other.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**：这基于给定观察的标签的条件概率分布进行预测。它假设特征之间是相互独立的。'
- en: '**Decision Trees**: This performs recursive binary partitioning of the feature
    space. The information gain at the tree node level is maximized in order to determine
    the best split for the partition.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树**：这执行特征空间的递归二元分区。在树节点级别上最大化信息增益，以确定分区的最佳拆分。'
- en: '**Ensembles of trees** (Random Forests and Gradient-Boosted Trees): Tree ensemble
    algorithms combine base decision tree models in order to build a performant model.
    They are intuitive and very successful for classification and regression tasks.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树的集成**（随机森林和梯度提升树）：树集成算法将基本决策树模型组合在一起，以构建一个高性能的模型。它们对于分类和回归任务非常直观和成功。'
- en: '**Isotonic Regression**: This minimizes the mean squared error between given
    data and observed responses.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保序回归**：这最小化给定数据和观察响应之间的均方误差。'
- en: Additional learning algorithms
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附加学习算法
- en: 'Spark MLlib offers more algorithms than the supervised and unsupervised learning
    ones. We have broadly three more additional types of machine learning methods:
    recommender systems, optimization algorithms, and feature extraction.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib提供的算法比监督和无监督学习算法更多。我们还有三种额外类型的机器学习方法：推荐系统，优化算法和特征提取。
- en: '![Additional learning algorithms](img/B03986_04_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![附加学习算法](img/B03986_04_04.jpg)'
- en: 'The following additional MLlib algorithms are currently available in Spark:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下附加的MLlib算法目前在Spark中可用：
- en: '**Collaborative filtering**: This is the basis for recommender systems. It
    creates a user-item association matrix and aims to fill the gaps. Based on other
    users and items along with their ratings, it recommends an item that the target
    user has no ratings for. In distributed computing, one of the most successful
    algorithms is **ALS** (short for **Alternating Least Square**):'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤**：这是推荐系统的基础。它创建一个用户-项目关联矩阵，并旨在填补空白。基于其他用户和项目以及它们的评分，它推荐目标用户尚未评分的项目。在分布式计算中，最成功的算法之一是**ALS**（**交替最小二乘法**的缩写）：'
- en: '**Alternating Least Squares**: This matrix factorization technique incorporates
    implicit feedback, temporal effects, and confidence levels. It decomposes the
    large user item matrix into a lower dimensional user and item factors. It minimizes
    a quadratic loss function by fixing alternatively its factors.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交替最小二乘法**：这种矩阵分解技术结合了隐式反馈、时间效应和置信水平。它将大型用户项目矩阵分解为较低维度的用户和项目因子。它通过交替固定其因子来最小化二次损失函数。'
- en: '**Feature extraction and transformation**: These are essential techniques for
    large text document processing. It includes the following techniques:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取和转换**：这些是大型文本文档处理的基本技术。它包括以下技术：'
- en: '**Term Frequency**: Search engines use TF-IDF to score and rank document relevance
    in a vast corpus. It is also used in machine learning to determine the importance
    of a word in a document or corpus. Term frequency statistically determines the
    weight of a term relative to its frequency in the corpus. Term frequency on its
    own can be misleading as it overemphasizes words such as *the*, *of*, or *and*
    that give little information. Inverse Document Frequency provides the specificity
    or the measure of the amount of information, whether the term is rare or common
    across all documents in the corpus.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词频**：搜索引擎使用TF-IDF对大量语料库中的文档相关性进行评分和排名。它还用于机器学习，以确定文档或语料库中单词的重要性。词频统计上确定了术语相对于语料库中的频率的权重。单独的词频可能会产生误导，因为它过分强调了诸如*the*、*of*或*and*这样提供很少信息的词语。逆文档频率提供了特定性或术语在语料库中所有文档中是罕见还是常见的度量。'
- en: '**Word2Vec**: This includes two models, **Skip-Gram** and **Continuous Bag
    of Word**. The Skip-Gram predicts neighboring words given a word, based on sliding
    windows of words, while Continuous Bag of Words predicts the current word given
    the neighboring words.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Word2Vec**：这包括两种模型，**Skip-Gram**和**连续词袋**。Skip-Gram根据单词的滑动窗口预测给定单词的相邻单词，而连续词袋根据相邻单词预测当前单词。'
- en: '**Standard Scaler**: As part of preprocessing, the dataset must often be standardized
    by mean removal and variance scaling. We compute the mean and standard deviation
    on the training data and apply the same transformation to the test data.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准缩放器**：作为预处理的一部分，数据集通常必须通过均值去除和方差缩放进行标准化。我们计算训练数据的均值和标准差，并将相同的转换应用于测试数据。'
- en: '**Normalizer**: We scale the samples to have unit norm. It is useful for quadratic
    forms such as the dot product or kernel methods.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化器**：我们将样本缩放为单位范数。它对于二次形式（如点积或核方法）非常有用。'
- en: '**Feature selection**: This reduces the dimensionality of the vector space
    by selecting the most relevant features for the model.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：通过选择模型中最相关的特征来减少向量空间的维度。'
- en: '**Chi-Square Selector**: This is a statistical method to measure the independence
    of two events.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卡方选择器**：这是一种衡量两个事件独立性的统计方法。'
- en: '**Optimization**: These specific Spark MLlib optimization algorithms focus
    on various techniques of gradient descent. Spark provides very efficient implementation
    of gradient descent on a distributed cluster of machines. It looks for the local
    minima by iteratively going down the steepest descent. It is compute-intensive
    as it iterates through all the data available:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：这些特定的Spark MLlib优化算法专注于梯度下降的各种技术。Spark提供了非常高效的梯度下降实现，可以在分布式机器集群上进行。它通过迭代沿着最陡的下降方向寻找局部最小值。由于需要迭代处理所有可用数据，因此计算密集型：'
- en: '**Stochastic Gradient Descent**: We minimize an objective function that is
    the sum of differentiable functions. Stochastic Gradient Descent uses only a sample
    of the training data in order to update a parameter in a particular iteration.
    It is used for large-scale and sparse machine learning problems such as text classification.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**：我们最小化一个可微函数的总和。随机梯度下降仅使用训练数据的样本来更新特定迭代中的参数。它用于大规模和稀疏的机器学习问题，如文本分类。'
- en: '**Limited-memory BFGS** (**L-BFGS**): As the name says, L-BFGS uses limited
    memory and suits the distributed optimization algorithm implementation of Spark
    MLlib.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限内存BFGS**（**L-BFGS**）：顾名思义，L-BFGS使用有限内存，适用于Spark MLlib的分布式优化算法实现。'
- en: Spark MLlib data types
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark MLlib数据类型
- en: 'MLlib supports four essential data types: **local vector**, **labeled point**,
    **local matrix**, and **distributed matrix**. These data types are widely used
    in Spark MLlib algorithms:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib支持四种基本数据类型：**本地向量**、**标记点**、**本地矩阵**和**分布式矩阵**。这些数据类型在Spark MLlib算法中被广泛使用：
- en: '**Local vector**: This resides in a single machine. It can be dense or sparse:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地向量**：这存在于单个机器中。它可以是密集的或稀疏的：'
- en: Dense vector is a traditional array of doubles. An example of dense vector is
    `[5.0, 0.0, 1.0, 7.0]`.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集向量是传统的双精度数组。密集向量的一个示例是`[5.0, 0.0, 1.0, 7.0]`。
- en: Sparse vector uses integer indices and double values. So the sparse representation
    of the vector `[5.0, 0.0, 1.0, 7.0]` would be `(4, [0, 2, 3], [5.0, 1.0, 7.0])`,
    where represent the dimension of the vector.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏向量使用整数索引和双精度值。因此，向量`[5.0, 0.0, 1.0, 7.0]`的稀疏表示将是`(4, [0, 2, 3], [5.0, 1.0,
    7.0])`，其中表示向量的维度。
- en: 'Here''s an example of local vector in PySpark:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是PySpark中本地向量的示例：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Labeled point**. A labeled point is a dense or sparse vector with a label
    used in supervised learning. In the case of binary labels, 0.0 represents the
    negative label whilst 1.0 represents the positive value.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记点**。标记点是在监督学习中使用的带有标签的稠密或稀疏向量。在二元标签的情况下，0.0表示负标签，而1.0表示正值。'
- en: 'Here''s an example of a labeled point in PySpark:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是PySpark中标记点的一个示例：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Local Matrix**: This local matrix resides in a single machine with integer-type
    indices and values of type double.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地矩阵**：这个本地矩阵位于单个机器上，具有整数类型的索引和双精度类型的值。'
- en: 'Here''s an example of a local matrix in PySpark:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是PySpark中本地矩阵的一个示例：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Distributed Matrix**: Leveraging the distributed mature of the RDD, distributed
    matrices can be shared in a cluster of machines. We distinguish four distributed
    matrix types: `RowMatrix`, `IndexedRowMatrix`, `CoordinateMatrix`, and `BlockMatrix`:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式矩阵**：利用RDD的分布式特性，分布式矩阵可以在一组机器的集群中共享。我们区分四种分布式矩阵类型：`RowMatrix`、`IndexedRowMatrix`、`CoordinateMatrix`和`BlockMatrix`。'
- en: '`RowMatrix`: This takes an RDD of vectors and creates a distributed matrix
    of rows with meaningless indices, called `RowMatrix`, from the RDD of vectors.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RowMatrix`：这需要一个向量的RDD，并从向量的RDD创建一个带有无意义索引的行的分布式矩阵，称为`RowMatrix`。'
- en: '`IndexedRowMatrix`: In this case, row indices are meaningful. First, we create
    an RDD of indexed rows using the class `IndexedRow` and then create an `IndexedRowMatrix`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IndexedRowMatrix`：在这种情况下，行索引是有意义的。首先，我们使用`IndexedRow`类创建索引行的RDD，然后创建`IndexedRowMatrix`。'
- en: '`CoordinateMatrix`: This is useful to represent very large and very sparse
    matrices. `CoordinateMatrix` is created from RDDs of the `MatrixEntry` points,
    represented by a tuple of type (long, long, or float)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CoordinateMatrix`：这对于表示非常大和非常稀疏的矩阵很有用。`CoordinateMatrix`是从`MatrixEntry`点的RDD创建的，由(long,
    long, float)类型的元组表示。'
- en: '`BlockMatrix`: These are created from RDDs of sub-matrix blocks, where a sub-matrix
    block is `((blockRowIndex, blockColIndex), sub-matrix)`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BlockMatrix`：这些是从子矩阵块的RDD创建的，其中子矩阵块是`((blockRowIndex, blockColIndex), sub-matrix)`。'
- en: Machine learning workflows and data flows
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流程和数据流程
- en: Beyond algorithms, machine learning is also about processes. We will discuss
    the typical workflows and data flows of supervised and unsupervised machine learning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了算法，机器学习还涉及到流程。我们将讨论监督和无监督机器学习的典型工作流程和数据流程。
- en: Supervised machine learning workflows
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督机器学习工作流程
- en: In supervised machine learning, the input training dataset is labeled. One of
    the key data practices is to split input data into training and test sets, and
    validate the mode accordingly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督机器学习中，输入训练数据集是有标签的。一个关键的数据实践是将输入数据分为训练集和测试集，并相应地验证模型。
- en: 'We typically go through a six-step process flow in supervised learning:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们通常会经历一个六步的流程：
- en: '**Collect the data**: This step essentially ties in with the previous chapter
    and ensures we collect the right data with the right volume and granularity in
    order to enable the machine learning algorithm to provide reliable answers.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集数据**：这一步基本上与前一章相关，并确保我们收集正确数量和粒度的数据，以使机器学习算法能够提供可靠的答案。'
- en: '**Preprocess the data**: This step is about checking the data quality by sampling,
    filling in the missing values if any, scaling and normalizing the data. We also
    define the feature extraction process. Typically, in the case of large text-based
    datasets, we apply tokenization, stop words removal, stemming, and TF-IDF.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理数据**：这一步是关于通过抽样检查数据质量，填补缺失值（如果有的话），对数据进行缩放和归一化。我们还定义特征提取过程。通常，在大型基于文本的数据集的情况下，我们应用标记化、停用词去除、词干提取和TF-IDF。'
- en: In the case of supervised learning, we separate the input data into a training
    and test set. We can also implement various strategies of sampling and splitting
    the dataset for cross-validation purposes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们将输入数据分为训练集和测试集。我们还可以实施各种采样和数据集拆分策略，以进行交叉验证。
- en: '**Ready the data**: In this step, we get the data in the format or data type
    expected by the algorithms. In the case of Spark MLlib, this includes local vector,
    dense or sparse vectors, labeled points, local matrix, distributed matrix with
    row matrix, indexed row matrix, coordinate matrix, and block matrix.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准备数据**：在这一步中，我们将数据格式化或转换为算法所期望的格式或数据类型。在Spark MLlib中，这包括本地向量、稠密或稀疏向量、标记点、本地矩阵、带有行矩阵、索引行矩阵、坐标矩阵和块矩阵的分布式矩阵。'
- en: '**Model**: In this step, we apply the algorithms that are suitable for the
    problem at hand and get the results for evaluation of the most suitable algorithm
    in the evaluate step. We might have multiple algorithms suitable for the problem;
    their respective performance will be scored in the evaluate step to select the
    best preforming ones. We can implement an ensemble or combination of models in
    order to reach the best results.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：在这一步中，我们应用适合问题的算法，并获得评估步骤中最适合算法的结果。我们可能有多个适合问题的算法；它们在评估步骤中的性能将被评分以选择最佳的性能。我们可以实施模型的集成或组合，以达到最佳结果。'
- en: '**Optimize**: We may need to run a grid search for the optimal parameters of
    certain algorithms. These parameters are determined during training, and fine-tuned
    during the testing and production phase.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：我们可能需要对某些算法的最佳参数进行网格搜索。这些参数在训练期间确定，并在测试和生产阶段进行微调。'
- en: '**Evaluate**: We ultimately score the models and select the best one in terms
    of accuracy, performance, reliability, and scalability. We move the best performing
    model to test with the held out test data in order to ascertain the prediction
    accuracy of our model. Once satisfied with the fine-tuned model, we move it to
    production to process live data.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：最终我们对模型进行评分，并选择在准确性、性能、可靠性和可扩展性方面最好的模型。我们将最佳性能的模型移至测试集，以确定模型的预测准确性。一旦对经过微调的模型满意，我们将其移至生产环境以处理实时数据。'
- en: 'The supervised machine learning workflow and dataflow are represented in the
    following diagram:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习的工作流程和数据流程如下图所示：
- en: '![Supervised machine learning workflows](img/B03986_04_05.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![监督机器学习工作流程](img/B03986_04_05.jpg)'
- en: Unsupervised machine learning workflows
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督机器学习工作流
- en: As opposed to supervised learning, our initial data is not labeled in the case
    of unsupervised learning, which is most often the case in real life. We will extract
    the structure from the data by using clustering or dimensionality reduction algorithms.
    In the unsupervised learning case, we do not split the data into training and
    test, as we cannot make any prediction because the data is not labeled. We will
    train the data along six steps similar to those in supervised learning. Once the
    model is trained, we will evaluate the results and fine-tune the model and then
    release it for production.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习相反，在无监督学习的情况下，我们的初始数据没有标签，这在现实生活中是最常见的情况。我们将使用聚类或降维算法从数据中提取结构。在无监督学习的情况下，我们不会将数据分为训练和测试，因为我们无法进行任何预测，因为数据没有标签。我们将对数据进行六个步骤的训练，类似于监督学习。一旦模型训练完成，我们将评估结果并微调模型，然后将其投入生产。
- en: Unsupervised learning can be a preliminary step to supervised learning. Namely,
    we look at reducing the dimensionality of the data prior to attacking the learning
    phase.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习可以作为监督学习的初步步骤。换句话说，我们在进入学习阶段之前看一下如何降低数据的维度。
- en: 'The unsupervised machine learning workflows and dataflow are represented as
    follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习工作流和数据流如下所示：
- en: '![Unsupervised machine learning workflows](img/B03986_04_06.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![无监督机器学习工作流程](img/B03986_04_06.jpg)'
- en: Clustering the Twitter dataset
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对Twitter数据集进行聚类
- en: 'Let''s first get a feel for the data extracted from Twitter and get an understanding
    of the data structure in order to prepare and run it through the K-Means clustering
    algorithms. Our plan of attack uses the process and dataflow depicted earlier
    for unsupervised learning. The steps are as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从Twitter中提取的数据中了解一下，并了解数据结构，以便准备并通过K-Means聚类算法运行。我们的攻击计划使用了前面描述的无监督学习的过程和数据流。步骤如下：
- en: Combine all tweet files into a single dataframe.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有推文文件合并为单个数据框。
- en: Parse the tweets, remove stop words, extract emoticons, extract URL, and finally
    normalize the words (for example, mapping them to lowercase and removing punctuation
    and numbers).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析推文，删除停用词，提取表情符号，提取URL，最后规范化单词（例如，将它们映射为小写并删除标点和数字）。
- en: 'Feature extraction includes the following:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取包括以下内容：
- en: '**Tokenization**: This breaks down the parsed tweet text into individual words
    or tokens'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记化**：这将解析推文文本为单个单词或标记'
- en: '**TF-IDF**: This applies the TF-IDF algorithm to create feature vectors from
    the tokenized tweet texts'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TF-IDF**：这将应用TF-IDF算法从标记化的推文文本中创建特征向量'
- en: '**Hash TF-IDF**: This applies a hashing function to the token vectors'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**哈希TF-IDF**：这将对标记向量应用哈希函数'
- en: Run the K-Means clustering algorithm.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行K-Means聚类算法。
- en: 'Evaluate the results of the K-Means clustering:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估K-Means聚类的结果：
- en: Identify tweet membership to clusters
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别推文归属于聚类
- en: Perform dimensionality reduction to two dimensions with the Multi-Dimensional
    Scaling or the Principal Component Analysis algorithm
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多维缩放或主成分分析算法将维度降低到两个维度
- en: Plot the clusters
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制聚类
- en: 'Pipeline:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道：
- en: Fine-tune the number of relevant clusters K
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调相关聚类K的数量
- en: Measure the model cost
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量模型成本
- en: Select the optimal model
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最佳模型
- en: Applying Scikit-Learn on the Twitter dataset
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Twitter数据集上应用Scikit-Learn
- en: Python's own Scikit-Learn machine learning library is one of the most reliable,
    intuitive, and robust tools around. Let's run through a preprocessing and unsupervised
    learning using Pandas and Scikit-Learn. It is often beneficial to explore a sample
    of the data using Scikit-Learn before spinning off clusters with Spark MLlib.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Python自带的Scikit-Learn机器学习库是最可靠、直观和强大的工具之一。在使用Pandas和Scikit-Learn进行预处理和无监督学习之前，通常有利于使用Scikit-Learn探索数据的样本。在使用Spark
    MLlib分离聚类之前，我们经常使用Scikit-Learn探索数据的样本。
- en: 'We have a mixed bag of 7,540 tweets. It contains tweets related to Apache Spark,
    Python, the upcoming presidential election with Hillary Clinton and Donald Trump
    as protagonists, and some tweets related to fashion and music with Lady Gaga and
    Justin Bieber. We are running the K-Means clustering algorithm using Python Scikit-Learn
    on the Twitter dataset harvested. We first load the sample data into a Pandas
    dataframe:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一袋杂货的7540条推文。它包含与Apache Spark、Python、即将到来的总统选举以及Lady Gaga和Justin Bieber相关的时尚和音乐推文。我们正在使用Python
    Scikit-Learn对Twitter数据集进行K-Means聚类算法。我们首先将样本数据加载到Pandas数据框中：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We first perform a feature extraction from the tweets'' text. We apply a sparse
    vectorizer to the dataset using a TF-IDF vectorizer with 10,000 features and English
    stop words:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从推文文本中进行特征提取。我们使用具有10,000个特征和英语停用词的TF-IDF向量化器对数据集应用稀疏矢量化器：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As the dataset is now broken into a 7540 sample with vectors of 6,638 features,
    we are ready to feed this sparse matrix to the K-Means clustering algorithm. We
    will choose seven clusters and 100 maximum iterations initially:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集现在被分成了7540个样本，每个样本有6638个特征向量，我们准备将这个稀疏矩阵输入K-Means聚类算法。我们最初选择七个聚类和100次最大迭代：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The K-Means clustering algorithm converged after 18 iterations. We see in the
    following results the seven clusters with their respective key words. Clusters
    `0` and `6` are about music and fashion with Justin Bieber and Lady Gaga-related
    tweets. Clusters `1` and `5` are related to the U.S.A. presidential elections
    with Donald Trump-and Hilary Clinton-related tweets. Clusters `2` and `3` are
    the ones of interest to us as they are about Apache Spark and Python. Cluster
    `4` contains Thailand-related tweets:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means聚类算法在18次迭代后收敛。在以下结果中，我们看到了七个聚类及其各自的关键词。聚类`0`和`6`是关于贾斯汀·比伯和Lady Gaga相关推文的音乐和时尚。聚类`1`和`5`与美国总统选举有关，包括唐纳德·特朗普和希拉里·克林顿相关的推文。聚类`2`和`3`是我们感兴趣的，因为它们涉及Apache
    Spark和Python。聚类`4`包含泰国相关的推文：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will visualize the results by plotting the cluster. We have 7,540 samples
    with 6,638 features. It will be impossible to visualize that many dimensions.
    We will use the **Multi-Dimensional Scaling** (**MDS**) algorithm to bring down
    the multidimensional features of the clusters into two tractable dimensions to
    be able to picture them:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过绘制聚类来可视化结果。我们有7,540个样本，6,638个特征。不可能可视化那么多维度。我们将使用**多维缩放**（**MDS**）算法将聚类的多维特征降低到两个可处理的维度，以便能够将它们呈现出来：
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here''s a plot of Cluster `2`, *Big Data* and *Spark*, represented by blue
    dots along with Cluster `3`, *Spark* and *Python*, represented by red dots, and
    some sample tweets related to the respective clusters:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是聚类`2`的绘图，*大数据*和*Spark*用蓝色点表示，聚类`3`的*Spark*和*Python*用红色点表示，以及一些与各自聚类相关的示例推文：
- en: '![Applying Scikit-Learn on the Twitter dataset](img/B03986_04_07.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![在Twitter数据集上应用Scikit-Learn](img/B03986_04_07.jpg)'
- en: We have gained some good insights into the data with the exploration and processing
    done with Scikit-Learn. We will now focus our attention on Spark MLlib and take
    it for a ride on the Twitter dataset.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对Scikit-Learn进行探索和处理，我们对数据获得了一些有益的见解。现在我们将把注意力集中在Spark MLlib上，并在Twitter数据集上进行尝试。
- en: Preprocessing the dataset
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理数据集
- en: 'Now, we will focus on feature extraction and engineering in order to ready
    the data for the clustering algorithm run. We instantiate the Spark Context and
    read the Twitter dataset into a Spark dataframe. We will then successively tokenize
    the tweet text data, apply a hashing Term frequency algorithm to the tokens, and
    finally apply the Inverse Document Frequency algorithm and rescale the data. The
    code is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将专注于特征提取和工程，以准备数据进行聚类算法运行。我们实例化Spark上下文，并将Twitter数据集读入Spark数据框。然后我们将逐步对推文文本数据进行标记化，对标记应用哈希词频算法，最后应用逆文档频率算法并重新调整数据。代码如下：
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Running the clustering algorithm
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行聚类算法
- en: 'We will use the K-Means algorithm against the Twitter dataset. As an unlabeled
    and shuffled bag of tweets, we want to see if the *Apache Spark* tweets are grouped
    in a single cluster. From the previous steps, the TF-IDF sparse vector of features
    is converted into an RDD that will be the input to the Spark MLlib program. We
    initialize the K-Means model with 5 clusters, 10 iterations of 10 runs:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用K-Means算法对Twitter数据集进行处理。作为一个未标记和洗牌的推文包，我们想看看*Apache Spark*的推文是否被分组到一个单独的聚类中。从之前的步骤中，TF-IDF稀疏特征向量被转换为将成为Spark
    MLlib程序输入的RDD。我们用5个聚类、10次迭代和10次运行来初始化K-Means模型：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Evaluating the model and the results
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型和结果
- en: 'One way to fine-tune the clustering algorithm is by varying the number of clusters
    and verifying the output. Let''s check the clusters and get a feel for the clustering
    results so far:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 微调聚类算法的一种方法是改变聚类的数量并验证输出。让我们检查一下聚类，并对迄今为止的聚类结果有所了解：
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We map the `5` clusters with some sample tweets. Cluster `0` is about Spark.
    Cluster `1` is about Python. Cluster `2` is about Lady Gaga. Cluster `3` is about
    Thailand's Phuket News. Cluster `4` is about Donald Trump.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一些示例推文对`5`个聚类进行了映射。聚类`0`是关于Spark的。聚类`1`是关于Python的。聚类`2`是关于Lady Gaga的。聚类`3`是关于泰国普吉岛新闻的。聚类`4`是关于唐纳德·特朗普的。
- en: Building machine learning pipelines
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建机器学习管道
- en: We want to compose the feature extraction, preparatory activities, training,
    testing, and prediction activities while optimizing the best tuning parameter
    to get the best performing model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在优化最佳调整参数的同时，组合特征提取、准备活动、训练、测试和预测活动，以获得最佳性能模型。
- en: 'The following tweet captures perfectly in five lines of code a powerful machine
    learning Pipeline implemented in Spark MLlib:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下推文完美地捕捉了在Spark MLlib中实现的强大机器学习管道的五行代码：
- en: '![Building machine learning pipelines](img/B03986_04_08.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![构建机器学习管道](img/B03986_04_08.jpg)'
- en: The Spark ML pipeline is inspired by Python's Scikit-Learn and creates a succinct,
    declarative statement of the successive transformations to the data in order to
    quickly deliver a tunable model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML管道受Python的Scikit-Learn启发，并创建了一个简洁的、声明性的语句，用于对数据进行连续转换，以快速交付可调整的模型。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we got an overview of Spark MLlib's ever-expanding library
    of algorithms Spark MLlib. We discussed supervised and unsupervised learning,
    recommender systems, optimization, and feature extraction algorithms. We then
    put the harvested data from Twitter into the machine learning process, algorithms,
    and evaluation to derive insights from the data. We put the Twitter-harvested
    dataset through a Python Scikit-Learn and Spark MLlib K-means clustering in order
    to segregate the tweets relevant to *Apache Spark*. We also evaluated the performance
    of the model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了Spark MLlib不断扩展的算法库。我们讨论了监督学习和无监督学习、推荐系统、优化和特征提取算法。然后，我们将从Twitter中收集的数据放入机器学习过程、算法和评估中，以从数据中获取见解。我们通过Python
    Scikit-Learn和Spark MLlib的K-means聚类对Twitter收集的数据集进行了处理，以将与*Apache Spark*相关的推文分离出来。我们还评估了模型的性能。
- en: This gets us ready for the next chapter, which will cover Streaming Analytics
    using Spark. Let's jump right in.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们为下一章做好准备，下一章将涵盖使用Spark进行流式分析。让我们马上开始吧。
