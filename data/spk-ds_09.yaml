- en: Chapter 9. Visualizing Big Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章：可视化大数据
- en: Proper data visualization has solved many business problems in the past without
    much statistics or machine learning being involved. Even today, with so many technological
    advancements, applied statistics, and machine learning, proper visuals are the
    end deliverables for business users to consume information or the output of some
    analyses. Conveying the right information in the right format is something that
    data scientists yearn for, and an effective visual is worth a million words. Also,
    representing the models and the insights generated in a way that is easily consumable
    by the business is extremely important. Nonetheless, exploring big data visually
    is very cumbersome and challenging. Since Spark is designed for big data processing,
    it also supports big data visualization along with it. There are many tools and
    techniques that have been built on Spark for this purpose.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的数据可视化在过去解决了许多商业问题，而无需过多依赖统计学或机器学习。即使在今天，随着技术进步、应用统计学和机器学习的发展，合适的视觉呈现依然是商业用户获取信息或分析结果的最终交付物。传递正确的信息、以正确的格式展现，这是数据科学家所追求的，而一个有效的可视化比百万个词语还要有价值。此外，以易于商业用户理解的方式呈现模型和生成的洞察至关重要。尽管如此，以可视化方式探索大数据非常繁琐且具有挑战性。由于Spark是为大数据处理设计的，它也支持大数据的可视化。为了这个目的，基于Spark构建了许多工具和技术。
- en: The previous chapters outlined how to model structured and unstructured data
    and generate insights from it. In this chapter, we will look at data visualization
    from two broad perspectives-one is from a data scientist's perspective—where visualization
    is the basic need to explore and understand the data effectively, and the other
    is from a business user's perspective, where the visuals are end deliverables
    to the business and must be easily comprehendible. We will explore various data
    visualization tools such as *IPythonNotebook* and *Zeppelin* that can be used
    on Apache Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章概述了如何对结构化和非结构化数据建模并从中生成洞察。在本章中，我们将从两个广泛的角度来看待数据可视化——一个是数据科学家的角度，数据可视化是有效探索和理解数据的基本需求；另一个是商业用户的角度，视觉呈现是交付给商业用户的最终成果，必须易于理解。我们将探索多种数据可视化工具，如*IPythonNotebook*和*Zeppelin*，这些工具可以在Apache
    Spark上使用。
- en: 'As a prerequisite for this chapter, a basic understanding of SQL and programming
    in Python, Scala, or other such frameworks, is nice to have. The topics covered
    in this chapter are listed as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前提是，你应该对SQL以及Python、Scala或其他类似框架的编程有基本的了解。 本章涵盖的主题如下：
- en: Why visualize data?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要可视化数据？
- en: A data engineer's perspective
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程师的角度
- en: A data scientist's perspective
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家的角度
- en: A business user's perspective
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商业用户的角度
- en: Data visualization tools
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化工具
- en: IPython notebook
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPython notebook
- en: Apache Zeppelin
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Zeppelin
- en: Third-party tools
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三方工具
- en: Data visualization techniques
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化技巧
- en: Summarizing and visualizing
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结和可视化
- en: Subsetting and visualizing
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子集和可视化
- en: Sampling and visualizing
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽样和可视化
- en: Modeling and visualizing
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模和可视化
- en: Why visualize data?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要可视化数据？
- en: 'Data visualization deals with representing data in a visual form so as to enable
    people to understand the underlying patterns and trends. Geographical maps, the
    bar and line charts of the seventeenth century, are some examples of early data
    visualizations. Excel is perhaps a familiar data visualization tool that most
    of us have already used. All data analytics tools have been equipped with sophisticated,
    interactive data visualization dashboards. However, the recent surge in big data,
    streaming, and real-time analytics has been pushing the boundaries of these tools
    and they seem to be bursting at the seams. The idea is to make the visualizations
    look simple, accurate, and relevant while hiding away all the complexity. As per
    the business needs, any visualization solution should ideally have the following
    characteristics:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化是以可视化的形式呈现数据，以帮助人们理解数据背后的模式和趋势。地理地图、十七世纪的条形图和折线图是早期数据可视化的一些例子。Excel 可能是我们最熟悉的数据可视化工具，许多人已经使用过。所有的数据分析工具都配备了复杂的、互动的数据可视化仪表盘。然而，近年来大数据、流式数据和实时分析的激增推动了这些工具的边界，似乎已经达到了极限。目标是使可视化看起来简单、准确且相关，同时隐藏所有复杂性。根据商业需求，任何可视化解决方案理想情况下应该具备以下特点：
- en: Interactivity
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交互性
- en: Reproducibility
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重复性
- en: Control over the details
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制细节
- en: Apart from these, if the solution allows users to collaborate over the visuals
    or reports and share with each other, then that would make up an end-to-end visualization
    solution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，如果解决方案允许用户在可视化或报告上进行协作并相互共享，那么这将构成一个端到端的可视化解决方案。
- en: Big data visualization in particular poses its own challenges because we may
    end up with more data than pixels on the screen. Manipulating large data usually
    requires memory- and CPU-intensive processing and may have long latency. Add real-time
    or streaming data to the mix and the problem becomes even more challenging. Apache
    Spark is designed from the ground up just to tackle this latency by parallelizing
    CPU and memory usage. Before exploring the tools and techniques to visualize and
    work with big data, let's first understand the visualization needs of data engineers,
    data scientists, and business users.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是大数据可视化面临着自身的挑战，因为我们可能会遇到数据比屏幕上的像素还多的情况。处理大数据通常需要大量的内存和CPU处理，且可能存在较长的延迟。如果再加入实时或流数据，这个问题将变得更加复杂。Apache
    Spark从一开始就被设计用来通过并行化CPU和内存使用来解决这种延迟。在探索可视化和处理大数据的工具和技术之前，我们首先要了解数据工程师、数据科学家和业务用户的可视化需求。
- en: A data engineer's perspective
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据工程师的视角
- en: 'Data engineers play a crucial role in almost every data-driven requirement:
    sourcing data from different data sources, consolidating them, cleaning and preprocessing
    them, analyzing them, and then the final reporting with visuals and dashboards.
    Their activities can be broadly stated as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师在几乎所有数据驱动的需求中都发挥着至关重要的作用：从不同的数据源获取数据，整合数据，清洗和预处理数据，分析数据，最终通过可视化和仪表板进行报告。其活动可以大致总结如下：
- en: Visualize the data from different sources to be able to integrate and consolidate
    it to form a single data matrix
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化来自不同来源的数据，以便能够整合并合并它们，形成一个单一的数据矩阵
- en: Visualize and find various anomalies in the data, such as missing values, outliers
    and so on (this could be while scraping, sourcing, ETLing, and so on) and get
    those fixed
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化并发现数据中的各种异常，如缺失值、异常值等（这可能发生在抓取、数据源获取、ETL等过程中），并进行修复
- en: Advise the data scientists on the properties and characteristics of the dataset
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向数据科学家提供有关数据集的属性和特征的建议
- en: Explore various possible ways of visualizing the data and finalize the ones
    that are more informative and intuitive as per the business requirement
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索多种可能的方式来可视化数据，并根据业务需求最终确定那些更具信息性和直观性的方式
- en: Observe here that the data engineers not only play a key role in sourcing and
    preparing the data, but also take a call on the most suitable visualization outputs
    for the business users. They usually work very closely to the business as well
    to have a very clear understanding on the business requirement and the specific
    problem at hand.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据工程师不仅在数据源获取和准备过程中发挥关键作用，还负责决定最适合业务用户的可视化输出。他们通常与业务部门密切合作，以便对业务需求和当前具体问题有非常清晰的理解。
- en: A data scientist's perspective
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学家的视角
- en: A data scientist's need for visualizing data is different from that of data
    engineers. Please note that in some businesses, there are professionals who play
    a dual role of data engineers and data scientists.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家在可视化数据方面的需求与数据工程师不同。请注意，在某些业务中，可能有专业人员同时承担数据工程师和数据科学家的双重角色。
- en: 'Data scientists need to visualize data to be able to take the right decisions
    in performing statistical analysis and ensure proper execution of the analytics
    projects. They would like to slice and dice data in various possible ways to find
    hidden insights. Let''s take a look at some example requirements that a data scientist
    might have to visualize the data:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家需要可视化数据，以便在进行统计分析时做出正确的决策，并确保分析项目的顺利执行。他们希望以多种方式切片和切块数据，以发现隐藏的洞察。让我们看看数据科学家在可视化数据时可能需要的一些示例需求：
- en: See the data distribution of the individual variables
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看各个变量的数据分布
- en: Visualize outliers in the data
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化数据中的异常值
- en: Visualize the percentage of missing data in a dataset for all variables
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化数据集中所有变量的缺失数据百分比
- en: Plot the correlation matrix to find the correlated variables
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制相关性矩阵以查找相关的变量
- en: Plot the behavior of residuals after a regression
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制回归后的残差行为
- en: After a data cleaning or transformation activity, plot the variable again and
    see how it behaves
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据清洗或转换活动后，重新绘制变量图，并观察其表现
- en: Please note that some of the things just mentioned are quite similar to the
    case of data engineers. However, data scientists could have a more scientific/statistical
    intention behind such analyses. For example, data scientists may see an outlier
    from a different perspective and treat it statistically, but a data engineer might
    think of the various options that could have triggered this.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，刚才提到的一些内容与数据工程师的情况非常相似。然而，数据科学家可能在这些分析背后有更科学/统计的意图。例如，数据科学家可能会从不同的角度看待一个离群值并进行统计处理，而数据工程师则可能考虑导致这一现象的多种可能选项。
- en: A business user's perspective
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 业务用户的视角
- en: 'A business user''s perspective is completely different from that of data engineers
    or data scientists. Business users are usually the consumers of information! They
    would like to extract more and more information from the data, and for that, the
    correct visuals play a key role. Also, most business questions are more complex
    and causal these days. The old-school reports are no longer enough. Let''s look
    at some example queries that business users would like to extract from reports,
    visuals, and dashboards:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 业务用户的视角与数据工程师或数据科学家的视角完全不同。业务用户通常是信息的消费者！他们希望从数据中提取更多的信息，为此，正确的可视化工具至关重要。此外，如今大多数业务问题都更加复杂且具因果关系。传统的报告已经不再足够。我们来看一些业务用户希望从报告、可视化和仪表板中提取的示例查询：
- en: Who are the high-value customers in so-and-so region?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某个地区，谁是高价值客户？
- en: What are the common characteristics of these customers?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些客户的共同特征是什么？
- en: Predict whether a new customer would be high-value
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测一个新客户是否会是高价值客户
- en: Advertising in which media would give maximum ROI?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在哪个媒体上做广告能获得最大的投资回报率？
- en: What if I do not advertise in a newspaper?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我不在报纸上做广告会怎样？
- en: What are the factors influencing a customer's buying behavior?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 影响客户购买行为的因素有哪些？
- en: Data visualization tools
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化工具
- en: Out of the many different visualization options, choosing the right visual depends
    on specific requirements. Similarly, selecting a visualization tool depends on
    both the target audience and the business requirement.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多可视化选项中，选择合适的可视化工具取决于特定的需求。同样，选择可视化工具也取决于目标受众和业务需求。
- en: Data scientists or data engineers would prefer a more interactive console for
    quick and dirty analysis. The visuals they use are usually not intended for business
    users. They would like to dissect the data in every possible way to get more meaningful
    insights. So, they usually prefer a notebook-type interface that supports these
    activities. A notebook is an interactive computational environment where they
    can combine code chunks and plot data for explorations. There are notebooks such
    as **IPython**/**Jupyter** or **DataBricks**, to name a few available options.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家或数据工程师通常会偏好一个更为互动的控制台，用于快速且粗略的分析。他们使用的可视化工具通常不面向业务用户。数据科学家或数据工程师倾向于从各个角度解构数据，以获得更有意义的洞察。因此，他们通常会更喜欢支持这些活动的笔记本类型界面。笔记本是一个互动的计算环境，用户可以在其中结合代码块并绘制数据进行探索。像**IPython**/**Jupyter**或**DataBricks**等笔记本就是可用的选项之一。
- en: Business users would prefer a more intuitive and informative visual that they
    can share with each other or use to generate reports. They expect to receive the
    end result through visuals. There are hundreds and thousands of tools, including
    some popular ones such as **Tableau**, that businesses use; but quite often, developers
    have to custom-build specific types for some unique requirements and expose them
    through web applications. Microsoft's **PowerBI** and open source solutions such
    as **Zeppelin** are a few examples.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 业务用户更喜欢直观且信息丰富的可视化，这样他们可以相互分享或用来生成报告。他们期望通过可视化得到最终结果。市场上有成百上千种工具，包括一些流行工具，如**Tableau**，企业都在使用；但通常，开发人员必须为一些独特的需求定制特定类型，并通过Web应用程序展示它们。微软的**PowerBI**和开源解决方案如**Zeppelin**就是一些例子。
- en: IPython notebook
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IPython 笔记本
- en: The IPython/Jupyter notebook on top of Spark's **PySpark** API is an excellent
    combination for data scientists to explore and visualize the data. The notebook
    internally spins up a new instance of the PySpark kernel. There are other kernels
    available; for example, the Apache **Toree** kernel can be used to support Scala
    as well.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Spark的**PySpark** API之上的IPython/Jupyter笔记本是数据科学家探索和可视化数据的绝佳组合。笔记本内部启动了一个新的PySpark内核实例。还有其他内核可用；例如，Apache的**Toree**内核可以用来支持Scala。
- en: For many data scientists, it is the default choice because of its capability
    of integrating text, code, formula, and graphics in one JSON document file. The
    IPython notebook supports `matplotlib`, which is a 2D visualization library that
    can produce production-quality visuals. Generating plots, histograms, scatterplots,
    charts, and so on becomes easy and simple. It also supports the `seaborn` library,
    which is actually built upon matplotlib but is easy to use as it provides higher
    level abstraction and hides the underlying complexities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多数据科学家来说，这是默认选择，因为它能够将文本、代码、公式和图形集成在一个JSON文档文件中。IPython笔记本支持`matplotlib`，它是一个可以生成生产质量视觉效果的二维可视化库。生成图表、直方图、散点图、图形等变得既简单又容易。它还支持`seaborn`库，实际上这是建立在matplotlib基础上的，但它易于使用，因为它提供了更高级的抽象，隐藏了底层的复杂性。
- en: Apache Zeppelin
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Zeppelin
- en: Apache Zeppelin is built upon JVM and integrates well with Apache Spark. It
    is a browser-based or frontend-based open source tool that has its own notebook.
    It supports Scala, Python, R, SQL, and other graphical modules to serve as a visualization
    solution not only to business users but also to data scientists. In the following
    section on visualization techniques, we will take a look at how Zeppelin supports
    Apache Spark code to generate interesting visuals. You need to download Zeppelin
    ([https://zeppelin.apache.org/](https://zeppelin.apache.org/)) in order to try
    out the examples.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Zeppelin建立在JVM之上，并与Apache Spark良好集成。它是一个基于浏览器或前端的开源工具，拥有自己的笔记本。它支持Scala、Python、R、SQL及其他图形模块，作为一种可视化解决方案，不仅为业务用户，也为数据科学家服务。在接下来的可视化技术部分中，我们将看看Zeppelin如何支持Apache
    Spark代码来生成有趣的可视化效果。你需要下载Zeppelin（[https://zeppelin.apache.org/](https://zeppelin.apache.org/)）以尝试这些示例。
- en: Third-party tools
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三方工具
- en: There are many products that support Apache Spark as the underlying data processing
    engine and are built to fit in the organizational big data ecosystem. While leveraging
    the processing power of Spark, they provide the visualization interface that supports
    a variety of interactive visuals, and they also support collaboration. Tableau
    is one such example of a tool that leverages Spark.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多产品支持Apache Spark作为底层数据处理引擎，并且是为了适应组织的大数据生态系统而构建的。它们利用Spark的处理能力，提供支持各种交互式视觉效果的可视化界面，并且支持协作。Tableau就是一个利用Spark的工具示例。
- en: Data visualization techniques
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化技术
- en: Data visualization is at the center of every stage in the data analytics life
    cycle. It is especially important for exploratory analysis and for communicating
    results. In either case, the goal is to transform data into a format that's efficient
    for human consumption. The approach of delegating the transformation to client-side
    libraries does not scale to large datasets. The transformation has to happen on
    the server side, sending only the relevant data to the client for rendering. Most
    of the common transformations are available in Apache Spark out of the box. Let's
    have a closer look at these transformations.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化是数据分析生命周期每个阶段的核心。它对探索性分析和结果传达尤为重要。在这两种情况下，目标都是将数据转换成对人类处理高效的格式。将转换委托给客户端库的方法无法扩展到大数据集。转换必须在服务器端进行，只将相关数据发送到客户端进行渲染。Apache
    Spark开箱即用地提供了大多数常见的转换。让我们仔细看看这些转换。
- en: Summarizing and visualizing
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结和可视化
- en: '**Summarizing and visualizing** is a technique used by many **Business Intelligence**
    (**BI**) tools. Since summarization will be a concise dataset regardless of the
    size of the underlying dataset, the graphs look simple enough and easy to render.
    There are various ways to summarize the data such as aggregating, pivoting, and
    so on. If the rendering tool supports interactivity and has drill-down capabilities,
    the user gets to explore subsets of interest from the complete data. We will show
    how to do the summarization rapidly and interactively with Spark through the Zeppelin
    notebook.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结和可视化** 是许多**商业智能**（**BI**）工具使用的一种技术。由于总结无论底层数据集的大小如何，都会生成简明的数据集，因此图表看起来足够简单并且易于渲染。有多种方法可以总结数据，例如聚合、透视等。如果渲染工具支持交互性并且具备下钻功能，用户可以从完整数据中探索感兴趣的子集。我们将展示如何通过Zeppelin笔记本快速、互动地进行总结。'
- en: The following image shows the Zeppelin notebook with source code and a grouped
    bar chart. The dataset contains 24 observations with sales information of two
    products, **P1** and **P2**, for 12 months. The first cell contains code to read
    a text file and register data as a temporary table. This cell uses the default
    Spark interpreter using Scala. The second cell uses the SQL interpreter which
    is supported by out-of-the-box visualization options. You can switch the chart
    types by clicking on the right icon. Note that the visualization is similar for
    either Scala or Python or R interpreters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了 Zeppelin 笔记本，包含源代码和分组条形图。数据集包含 24 条观测数据，记录了两个产品 **P1** 和 **P2** 在 12
    个月中的销售信息。第一个单元格包含用于读取文本文件并将数据注册为临时表的代码。此单元格使用默认的 Spark 解释器 Scala。第二个单元格使用 SQL
    解释器，支持开箱即用的可视化选项。你可以通过点击右侧图标切换图表类型。请注意，Scala、Python 或 R 解释器的可视化效果是相似的。
- en: 'Summarization examples are as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总示例如下：
- en: 'The source code to read data and register as a SQL View:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于读取数据并注册为 SQL 视图的源代码：
- en: '**Scala (default)**:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Scala（默认）**：'
- en: '![Summarizing and visualizing](img/image_09_001.jpg)'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![总结和可视化](img/image_09_001.jpg)'
- en: '**PySpark**:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**PySpark**：'
- en: '![Summarizing and visualizing](img/image_09_002.jpg)'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![总结和可视化](img/image_09_002.jpg)'
- en: '**R**:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**R**：'
- en: '![Summarizing and visualizing](img/image_09_003.jpg)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![总结和可视化](img/image_09_003.jpg)'
- en: All three are reading the data file and registering as a temporary SQL view.
    Note that minor differences exist in the preceding three scripts. For example,
    we need to remove the header row for R and set the column names. The next step
    is to produce the visualization, which works from the `%sql` interpreter. The
    following first picture shows the script to produce the quarterly sales for each
    product. It also shows the chart types available out of the box, followed by the
    settings and their selection. You can collapse the settings after making selections.
    You can even make use of Zeppelin's in-built dynamic forms, say to accept a product
    during runtime. The second picture shows the actual output.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这三者都在读取数据文件并注册为临时 SQL 视图。请注意，前面的三个脚本中存在一些细微的差异。例如，我们需要为 R 移除表头行并设置列名。下一步是生成可视化，它是通过
    `%sql` 解释器工作的。下图显示了生成每个产品季度销售额的脚本。它还显示了现成的图表类型，以及设置和选择。做出选择后，你可以折叠设置。你甚至可以使用 Zeppelin
    内置的动态表单，例如在运行时接受产品输入。第二张图显示了实际输出。
- en: The script to produce quarterly sales for two products:![Summarizing and visualizing](img/image_09_004.jpg)
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于生成两个产品季度销售额的脚本：![总结和可视化](img/image_09_004.jpg)
- en: The output produced:![Summarizing and visualizing](img/image_09_005.jpg)
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 产生的输出：![总结和可视化](img/image_09_005.jpg)
- en: We have seen Zeppelin's inbuilt visualization in the preceding example. But
    we can use other plotting libraries as well. Our next example utilizes the PySpark
    interpreter with matplotlib in Zeppelin to draw a histogram. This example code
    computes bin intervals and bin counts using RDD's histogram function and brings
    in just this summarized data to the driver node. Frequency is provided as weights
    while plotting the bins to give the same visual understanding as a normal histogram
    but with very low data transfer.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的示例中已经看到了 Zeppelin 的内置可视化。但我们也可以使用其他绘图库。我们的下一个示例使用 PySpark 解释器与 matplotlib
    在 Zeppelin 中绘制直方图。此示例代码使用 RDD 的直方图函数计算桶间隔和桶计数，并仅将这些汇总数据传送到驱动节点。频率作为权重绘制桶，以提供与正常直方图相同的视觉效果，但数据传输非常低。
- en: 'The histogram examples are as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图示例如下：
- en: '![Summarizing and visualizing](img/image_09_006.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![总结和可视化](img/image_09_006.jpg)'
- en: 'This is the generated output (it may come as a separate window):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成的输出（它可能作为一个单独的窗口弹出）：
- en: '![Summarizing and visualizing](img/image_09_007.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![总结和可视化](img/image_09_007.jpg)'
- en: In the preceding example of preparing histograms, note that the bucket counts
    could be parameterized using the inbuilt dynamic forms support.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的直方图准备示例中，请注意，桶计数可以通过内置的动态表单支持进行参数化。
- en: Subsetting and visualizing
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子集化和可视化
- en: Sometimes, we may have a large dataset but we may be interested only in a subset
    of it. Divide and conquer is one approach where we explore a small portion of
    data at a time. Spark allows data subsetting using SQL-like filters and aggregates
    on row-column datasets as well as graph data. Let us perform SQL subsetting first,
    followed by a GraphX example.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能拥有一个大型数据集，但只对其中的一部分感兴趣。“分而治之”是一种方法，我们一次探索数据的一个小部分。Spark 允许使用类似 SQL 的过滤器和聚合对行列数据集以及图形数据进行数据子集化。让我们首先执行
    SQL 子集化，然后是 GraphX 示例。
- en: The following example takes bank data available with Zeppelin and extracts a
    few relevant columns of data related to managers only. It uses the `google visualization
    library` to plot a bubble chart. The data was read using PySpark. Data subsetting
    and visualization are carried out using R. Note that we can choose any of the
    interpreters to these tasks and the choice here was just arbitrary.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用 Zeppelin 提供的银行数据，并提取与管理者相关的几个相关数据列。它使用`google 可视化库`绘制气泡图。数据是使用 PySpark
    读取的。数据子集化和可视化是通过 R 完成的。请注意，我们可以选择任何解释器来执行这些任务，这里选择的是随意的。
- en: 'The data subsetting example using SQL is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SQL 进行数据子集化的示例如下：
- en: Read data and register the SQL view:![Subsetting and visualizing](img/image_09_008.jpg)
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据并注册 SQL 视图：![子集化与可视化](img/image_09_008.jpg)
- en: Subset managers' data and show a bubble plot:![Subsetting and visualizing](img/image_09_009.jpg)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 子集化管理者数据并显示气泡图：![子集化与可视化](img/image_09_009.jpg)
- en: 'The next example demonstrates some GraphX processing that uses data provided
    by the **Stanford Network Analysis Project** (**SNAP**). The script extracts a
    subgraph covering a given set of nodes. Here, each node represents a Facebook
    ID and an edge represents a connection between the two nodes (or people). Further,
    the script identifies direct connections for a given node (id: 144). These are
    the level 1 nodes. Then it identifies the direct connections to these *level 1
    nodes*, which form *level 2 nodes* to the given node. Even though a second-level
    contact may be connected to more than one first-level contact, it is shown only
    once thereby forming a connection tree without crisscrossing edges. Since the
    connection tree may have too many nodes, the script limits up to three connections
    at level 1 as well as level 2, thereby showing only 12 nodes under the given root
    node (one root + three level 1 nodes + three of each level 2 nodes).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例演示了一些使用 **斯坦福网络分析项目**（**SNAP**）提供的数据的 GraphX 处理。该脚本提取了一个包含给定节点集的子图。在这里，每个节点代表一个
    Facebook ID，一条边代表两个节点（或人）之间的连接。此外，脚本还识别了给定节点（ID：144）的直接连接。这些是一级节点。然后，它识别这些*一级节点*的直接连接，这些形成了给定节点的*二级节点*。即使一个二级联系人可能与多个一级联系人连接，它也只显示一次，从而形成没有交叉边的连接树。由于连接树可能包含太多节点，脚本将一级和二级连接限制为最多三个连接，从而在给定的根节点下仅显示12个节点（一个根节点
    + 三个一级节点 + 每个二级节点的三个连接）。
- en: '**Scala**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note the use of `z.put` and `z.get`. This is a mechanism to exchange data between
    cells/interpreters in Zeppelin.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意 `z.put` 和 `z.get` 的使用。这是 Zeppelin 中用于交换单元格/解释器之间数据的一种机制。
- en: Now that we have created a data frame with level 1 contacts and their direct
    contacts, we are all set to draw the tree. The following script uses the graph
    visualization library igraph and Spark R.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个包含一级联系人及其直接联系人的数据框架，我们准备好绘制树形图了。以下脚本使用了图形可视化库 igraph 和 Spark R。
- en: 'Extract nodes and edges. Plot the tree:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 提取节点和边。绘制树形图：
- en: '![Subsetting and visualizing](img/image_09_010.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![子集化与可视化](img/image_09_010.jpg)'
- en: 'The preceding script gets parent nodes from the nodes table, which are the
    parents of level 2 nodes as well as direct connections to the given head node.
    Ordered pairs of head nodes and level 1 nodes are created and assigned to `edges1`.
    The next step explodes the array of level 2 nodes to form one row per each array
    element. The data frame thus obtained is transposed and pasted to form edge pairs.
    Since paste converts data into strings, they are reconverted to numeric. These
    are the level 2 edges. The level 1 and level 2 edges are concatenated to form
    a single list of edges. These are fed to form the graph as shown next. Note that
    the smudge in `headNode` is 144, though not visible in the following figure:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述脚本从节点表中获取父节点，这些父节点是第 2 级节点的父节点，也是与给定头节点的直接连接。创建头节点与第 1 级节点的有序对，并将其分配给 `edges1`。接下来的步骤是展开第
    2 级节点数组，将每个数组元素形成一行。由此获得的数据框被转置并粘贴，形成边对。由于粘贴操作将数据转换为字符串，因此需要重新转换为数字。这些就是第 2 级边。第
    1 级和第 2 级边被合并，形成一个边的单一列表。这些边接着被用来形成图形，如下所示。请注意，`headNode` 中的模糊值为 144，虽然在下图中不可见：
- en: '![Subsetting and visualizing](img/image_09_011.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![子集选择与可视化](img/image_09_011.jpg)'
- en: Connection tree for the given node
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 给定节点的连接树
- en: Sampling and visualizing
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽样与可视化
- en: Sampling and visualizing has been used by statisticians for a long time. Through
    sampling techniques, we take a portion of the dataset and work on it. We will
    show how Spark supports different sampling techniques such as **random sampling**,
    **stratified sampling**, and **sampleByKey**, and so on. The following example
    is created using the Jupyter notebook, PySpark kernel, and `seaborn` library.
    The data file is the bank dataset provided by Zeppelin. The first plot shows the
    balance for each education category. The colors indicate marital status.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样和可视化技术已被统计学家使用了很长时间。通过抽样技术，我们从数据集中提取一部分数据并进行处理。我们将展示 Spark 如何支持不同的抽样技术，如**随机抽样**、**分层抽样**和**sampleByKey**等。以下示例是在
    Jupyter notebook 中创建的，使用了 PySpark 核心和 `seaborn` 库。数据文件是 Zeppelin 提供的银行数据集。第一个图展示了每个教育类别的余额，颜色表示婚姻状况。
- en: 'Read data and take a random sample of 5%:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据并随机抽取 5% 的样本：
- en: '![Sampling and visualizing](img/image_09_012.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![抽样与可视化](img/image_09_012.jpg)'
- en: 'Render data using `stripplot`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `stripplot` 渲染数据：
- en: '![Sampling and visualizing](img/image_09_013.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![抽样与可视化](img/image_09_013.jpg)'
- en: 'The preceding example showed a random sample of available data, which is much
    better than completely plotting the population. But if the levels in the categorical
    variable of interest (in this case, `education`) are too many, then this plot
    becomes hard to read. For example, if we want to plot the balance for job instead
    of `education`, there will be too many strips, making the picture look cluttered.
    Instead, we can take desired sample of desired categorical levels only and then
    examine the data. Note that this is different from subsetting because we will
    not be able to specify the sample ratio in normal subsetting using SQL `WHERE`
    clauses. We need to use `sampleByKey` for that, as shown next. The following example
    takes only two jobs and with specific sampling ratios:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例展示了随机抽样的可用数据，这比完全绘制整体数据要好得多。但是，如果感兴趣的分类变量（在本例中是 `education`）的级别过多，那么这个图表就会变得难以阅读。例如，如果我们想绘制工作类别的余额而不是
    `education`，将会有太多的条形，使得图像看起来凌乱不堪。相反，我们可以只抽取所需类别级别的样本，然后再进行数据分析。请注意，这与子集选择不同，因为在正常的
    SQL `WHERE` 子集选择中我们无法指定样本比例。我们需要使用 `sampleByKey` 来实现这一点，如下所示。以下示例只取了两种工作类别，并且有特定的抽样比例：
- en: '![Sampling and visualizing](img/image_09_014.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![抽样与可视化](img/image_09_014.jpg)'
- en: Stratified sampling
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 分层抽样
- en: Modeling and visualizing
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模与可视化
- en: 'Modeling and visualizing are possible with Spark''s **MLLib** and **ML** modules.
    Spark''s unified programming model and diverse programming interfaces enable combining
    these techniques into a single environment to get insights from the data. We have
    already covered most of the modeling techniques in the previous chapters. However,
    here are a few examples for your reference:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark 的**MLLib**和**ML**模块可以进行建模与可视化。Spark 的统一编程模型和多样化的编程接口使得将这些技术结合到一个环境中，从数据中获取洞察成为可能。我们已经在前几章中涵盖了大多数建模技术。然而，以下是一些供参考的示例：
- en: '**Clustering**: K-means, Gaussian Mixture Modeling'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：K-means、Gaussian 混合模型'
- en: '**Classification and regression**: Linear model, Decision tree, Naïve Bayes,
    SVM'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类与回归**：线性模型、决策树、朴素贝叶斯、支持向量机（SVM）'
- en: '**Dimensionality reduction**: Singular value decomposition, Principal component
    analysis'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：奇异值分解，主成分分析'
- en: '**Collaborative Filtering**'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤**'
- en: '**Statistical testing**: Correlations, Hypothesis testing'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计测试**：相关性，假设检验'
- en: 'The following example takes a model from the [Chapter 7](ch07.xhtml "Chapter 7. 
    Extending Spark with SparkR"), *Extending Spark with SparkR*, which tries to predict
    the students'' pass or fail results using a Naïve Bayes model. The idea is to
    make use of the out-of-the-box functionality provided by Zeppelin and inspect
    the model behavior. So, we load the data, perform data preparation, build the
    model, and run the predictions. Then we register the predictions as an SQL view
    so as to harness inbuilt visualization:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例选自[第7章](ch07.xhtml "第7章：使用SparkR扩展Spark")，*使用SparkR扩展Spark*，该示例试图使用朴素贝叶斯模型预测学生的合格或不合格结果。其思路是利用Zeppelin提供的开箱即用功能，检查模型的行为。因此，我们加载数据，进行数据准备，构建模型并运行预测。然后，我们将预测结果注册为SQL视图，以便利用内置的可视化功能：
- en: '[PRE1]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Modeling and visualizing](img/image_09_015.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![建模与可视化](img/image_09_015.jpg)'
- en: The next step is to write the desired SQL query and define the appropriate settings.
    Note the use of the UNION operator in SQL and the way the match column is defined.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是编写所需的SQL查询并定义适当的设置。请注意SQL中的UNION操作符的使用以及匹配列的定义方式。
- en: 'Define SQL to view model performance:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 定义SQL以查看模型表现：
- en: '![Modeling and visualizing](img/image_09_016.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![建模与可视化](img/image_09_016.jpg)'
- en: 'The following picture helps us understand where the model prediction deviates
    from the actual data. Such visualizations are helpful in taking business users''
    inputs since they do not require any prior knowledge of data science to comprehend:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片帮助我们理解模型预测与实际数据的偏差。这样的可视化对于获取业务用户的反馈非常有用，因为它们不需要任何数据科学的先验知识就能理解：
- en: '![Modeling and visualizing](img/image_09_017.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![建模与可视化](img/image_09_017.jpg)'
- en: Visualize model performance
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化模型表现
- en: We usually evaluate statistical models with error metrics, but visualizing them
    graphically instead of seeing the numbers makes them more intuitive because it
    is usually easier to understand a diagram than numbers in a table. For example,
    the preceding visualization can be easily understood by people outside the data
    science community as well.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用误差指标来评估统计模型，但通过图形化展示而不是查看数字使其更加直观，因为通常理解图表比理解表格中的数字更容易。例如，前面的可视化可以让非数据科学领域的人也容易理解。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored most of the widely used visualization tools and
    techniques supported on Spark in a big data setup. We explained some of the techniques
    with code snippets for better understanding of visualization needs at different
    stages of the data analytics life cycle. We also saw how business requirements
    are satisfied with proper visualization techniques by addressing the challenges
    of big data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了在大数据环境中Spark支持的最常用可视化工具和技术。我们通过代码片段解释了一些技术，以便更好地理解数据分析生命周期各个阶段的可视化需求。我们还展示了如何通过适当的可视化技术解决大数据的挑战，以满足业务需求。
- en: The next chapter is the culmination of all the concepts explained till now .
    We will walk through the Complete Data Analysis Life Cycle through an example
    dataset.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将是我们迄今为止解释的所有概念的总结。我们将通过一个示例数据集走完完整的数据分析生命周期。
- en: References
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '21 Essential Data Visualization Tools: [http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html](http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 21种必备数据可视化工具：[http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html](http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html)
- en: 'Apache Zeppelin notebook home page: [https://zeppelin.apache.org/](https://zeppelin.apache.org/)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Zeppelin notebook主页：[https://zeppelin.apache.org/](https://zeppelin.apache.org/)
- en: 'Jupyter notebook home page: [https://jupyter.org/](https://jupyter.org/)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter notebook主页：[https://jupyter.org/](https://jupyter.org/)
- en: 'Using IPython Notebook with Apache Spark: [http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/](http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用IPython Notebook与Apache Spark：[http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/](http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/)
- en: 'Apache Toree, which enables interactive workloads between applications and
    Spark cluster. Can be used with jupyter to run Scala code: [https://toree.incubator.apache.org/](https://toree.incubator.apache.org/)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Toree，支持应用程序与 Spark 集群之间的交互式工作负载。可以与 jupyter 一起使用来运行 Scala 代码：[https://toree.incubator.apache.org/](https://toree.incubator.apache.org/)
- en: 'GoogleVis package using R: [https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html](https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 R 的 GoogleVis 包：[https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html](https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html)
- en: 'GraphX Programming Guide: [http://spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GraphX 编程指南：[http://spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html)
- en: 'Going viral with R''s igraph package: [https://www.r-bloggers.com/going-viral-with-rs-igraph-package/](https://www.r-bloggers.com/going-viral-with-rs-igraph-package/)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 R 的 igraph 包进行病毒传播分析：[https://www.r-bloggers.com/going-viral-with-rs-igraph-package/](https://www.r-bloggers.com/going-viral-with-rs-igraph-package/)
- en: 'Plotting with categorical data: [https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial](https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分类数据绘图：[https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial](https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial)
- en: Data source citations
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据来源引用
- en: '**Bank data source (citation)**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**银行数据来源（引用）**'
- en: '[Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for
    Bank Direct Marketing: An Application of the CRISP-DM Methodology'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Moro et al., 2011] S. Moro, R. Laureano 和 P. Cortez. 使用数据挖掘进行银行直销营销：CRISP-DM
    方法的应用'
- en: In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling
    Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011\. EUROSIS
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 见 P. Novais 等人（编辑），《欧洲仿真与建模会议论文集 - ESM'2011》，第 117-121 页，葡萄牙吉马良斯，2011 年 10 月，EUROSIS
- en: Available at [pdf] [http://hdl.handle.net/1822/14838](http://hdl.handle.net/1822/14838)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可通过 [pdf] [http://hdl.handle.net/1822/14838](http://hdl.handle.net/1822/14838)
    获取
- en: '[bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt'
- en: '**Facebook data Source (citation)**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**Facebook 数据来源（引用）**'
- en: J. McAuley and J. Leskovec. Learning to Discover Social Circles in Ego Networks.
    NIPS, 2012.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. McAuley 和 J. Leskovec. 学习在自我网络中发现社交圈。NIPS, 2012.
