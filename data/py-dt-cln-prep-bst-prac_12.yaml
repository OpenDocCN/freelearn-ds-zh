- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Text Preprocessing in the Era of LLMs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模语言模型时代的文本预处理
- en: In the era of **Large Language Models** (**LLMs**), mastering text preprocessing
    is more crucial than ever. As LLMs grow in complexity and capability, the foundation
    of successful **Natural Language Processing** (**NLP**) tasks still lies in how
    well the text data is prepared. In this chapter, we will discuss text preprocessing,
    the foundation for any NLP Task. We will also explore essential preprocessing
    techniques, focusing on adapting them to maximize the potential of LLMs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在**大规模语言模型**（**LLMs**）时代，掌握文本预处理比以往任何时候都更加重要。随着LLMs在复杂性和能力上的不断提升，成功的**自然语言处理**（**NLP**）任务的基础依然在于文本数据的准备工作。在本章中，我们将讨论文本预处理，这是任何NLP任务的基础。我们还将探讨重要的预处理技术，并重点研究如何调整这些技术以最大化LLMs的潜力。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: Relearning text preprocessing in the era of LLMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大规模语言模型时代重新学习文本预处理
- en: Text cleaning techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本清洗技术
- en: Handling rare words and spelling variations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理稀有词汇和拼写变体
- en: Chunking
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词块划分
- en: Tokenization strategies
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词策略
- en: Turning tokens into embeddings
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将词元转化为嵌入
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The complete code for this chapter can be found in the following GitHub repository:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的完整代码可以在以下GitHub仓库中找到：
- en: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter12](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter12)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter12](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter12)'
- en: 'Let''s install the necessary libraries we will use in this chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装本章中将使用的必要库：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Relearning text preprocessing in the era of LLMs
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在大规模语言模型时代重新学习文本预处理
- en: '**Text preprocessing** involves the application of various techniques to raw
    textual data with the aim of cleaning, organizing, and transforming it into a
    format suitable for analysis or modeling. The primary goal is to enhance the quality
    of the data by addressing common challenges associated with unstructured text.
    This entails tasks such as cleaning irrelevant characters, handling variations,
    and preparing the data for downstream NLP tasks.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本预处理**是指对原始文本数据应用各种技术，目的是清理、组织并将其转化为适合分析或建模的格式。其主要目标是通过解决与非结构化文本相关的常见挑战来提高数据的质量。这包括清理无关字符、处理变体以及为后续的自然语言处理（NLP）任务准备数据等任务。'
- en: With the rapid advancements in LLMs, the landscape of NLP has evolved significantly.
    However, fundamental preprocessing techniques such as text cleaning and tokenization
    remain crucial, albeit with some shifts in approach and importance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大规模语言模型（LLMs）的快速发展，自然语言处理（NLP）的格局发生了显著变化。然而，基础的预处理技术，如文本清洗和分词，依然至关重要，尽管它们在方法和重要性上有所变化。
- en: Staring with text cleaning, while LLMs have shown remarkable robustness to noise
    in input text, clean data still yields better results and is especially important
    for fine-tuning tasks. Basic cleaning techniques such as removing HTML tags, handling
    special characters, and normalizing text are still relevant. However, more advanced
    techniques such as spelling correction may be less critical for LLMs, as they
    can often handle minor spelling errors. Domain-specific cleaning remains important,
    especially when dealing with specialized vocabulary or jargon.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本清洗开始，尽管大规模语言模型（LLMs）在处理输入文本噪声方面表现出显著的鲁棒性，但清洗后的数据仍然能带来更好的结果，尤其在微调任务中尤为重要。基础清洗技术，如去除HTML标签、处理特殊字符以及文本标准化，依然是相关的。然而，像拼写纠正这样的高级技术对于LLMs的必要性可能较低，因为它们通常能处理轻微的拼写错误。领域特定的清洗仍然非常重要，尤其是在处理专业词汇或术语时。
- en: Tokenization has evolved with the advent of subword tokenization methods used
    by most modern LLMs such as **Byte-Pair Encoding** (**BPE**) or WordPiece. Traditional
    word-level tokenization is less common in LLM contexts. Some traditional NLP preprocessing
    steps such as stopword removal, stemming, and lemmatization have become less critical.
    Stopword removal, which involves eliminating common words such as “and” or “the,”
    is less necessary because LLMs can understand their contextual importance and
    how they contribute to the meaning of a sentence. Similarly, stemming and lemmatization,
    which reduce words to their base forms (e.g., “running” to “run”), are less frequently
    used because LLMs can interpret different word forms accurately and understand
    their relationships within the text. This shift allows for a more nuanced understanding
    of language, capturing subtleties that rigid preprocessing might miss.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着子词标记化方法的出现，Tokenization也得到了发展，现代大多数LLM（大规模语言模型）都使用如**字节对编码**（**BPE**）或WordPiece等方法。传统的基于词的标记化在LLM的背景下不再常见。一些传统的NLP预处理步骤，如停用词去除、词干提取和词形还原，变得不那么重要。停用词去除，即去除常见词汇，如“and”或“the”，变得不那么必要，因为LLM能够理解这些词在上下文中的重要性以及它们如何贡献于句子的意义。类似地，词干提取和词形还原（如将“running”还原为“run”）也不常使用，因为LLM能够准确理解不同词形，并理解它们在文本中的关系。这一转变使得对语言的理解更加细致，能够捕捉到一些严格预处理可能遗漏的细微差别。
- en: 'The key message is that while LLMs can handle raw text impressively, preprocessing
    remains crucial in certain scenarios as it can improve model performance on specific
    tasks. Remember: **garbage in, garbage out**. Cleaning and standardizing text
    can also reduce the number of tokens processed by an LLM, potentially lowering
    computational costs. New approaches are emerging that blend traditional preprocessing
    with LLM capabilities, using LLMs themselves for data cleaning and preprocessing
    tasks.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的信息是，虽然LLM能够令人印象深刻地处理原始文本，但在某些情境下，预处理仍然至关重要，因为它可以提高模型在特定任务上的表现。记住：**垃圾进，垃圾出**。清洗和标准化文本也可以减少LLM处理的token数量，从而可能降低计算成本。新的方法正在出现，它们将传统的预处理与LLM的能力相结合，利用LLM本身来进行数据清洗和预处理任务。
- en: In conclusion, while LLMs have reduced the need for extensive preprocessing
    in many NLP tasks, understanding and judiciously applying these fundamental techniques
    remains valuable. In the following sections, we will focus on the text preprocessing
    techniques that remain relevant.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，尽管LLM在许多NLP任务中减少了对广泛预处理的需求，但理解并谨慎应用这些基础技术仍然具有价值。在接下来的章节中，我们将重点介绍仍然相关的文本预处理技术。
- en: Text cleaning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本清洗
- en: The primary goal of text cleaning is to transform unstructured textual information
    into a standardized and more manageable form. While cleaning text, several operations
    are commonly performed, such as the removal of HTML tags, special characters,
    and numerical values, as well as the standardization of letter cases and the handling
    of whitespaces and formatting issues. These operations collectively contribute
    to refining the quality of textual data and reducing its ambiguity. Let’s deep
    dive into these techniques.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 文本清洗的主要目标是将非结构化的文本信息转化为标准化且更易处理的形式。在清洗文本时，常见的操作包括去除HTML标签、特殊字符和数字值，标准化字母大小写，处理空格和格式问题。这些操作共同有助于提升文本数据的质量并减少其歧义性。让我们深入探讨这些技术。
- en: Removing HTML tags and special characters
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去除HTML标签和特殊字符
- en: HTML tags are often present due to the extraction of content from web pages.
    These tags, such as `<p>`, `<a>`, or `<div>`, carry *no semantic meaning* in the
    context of NLP and must be removed. The cleaning process involves the identification
    and stripping of HTML tags, leaving behind only the actual words.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: HTML标签通常会出现在从网页中提取内容的过程中。这些标签，如`<p>`、`<a>`或`<div>`，在NLP的上下文中*没有语义意义*，必须被移除。清洗过程包括识别并去除HTML标签，保留实际的文本内容。
- en: For this example, let’s consider a scenario where we have a dataset of user
    reviews for a product and want to prepare the text data for sentiment analysis.
    You can find the code for this section in the GitHub repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/1.text_cleaning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/1.text_cleaning.py).
    In this script, the data generation is also available for you, and you can follow
    the example step by step.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，让我们假设我们有一个产品的用户评论数据集，并希望为情感分析准备文本数据。你可以在GitHub代码库中找到这一部分的代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/1.text_cleaning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/1.text_cleaning.py)。在这个脚本中，数据生成也已提供，你可以一步一步跟着示例走。
- en: Important note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Throughout this chapter, we’ve included key code snippets to illustrate the
    most important concepts. However, to see the complete code, including the libraries
    used, and to run the full end-to-end examples, please visit the repository.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们包含了关键的代码片段，以说明最重要的概念。然而，要查看完整的代码，包括使用的库，并运行完整的端到端示例，请访问代码库。
- en: 'The first text preprocessing step that we will execute is the removal of HTML
    tags. Let’s have a look at the code step by step:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行的第一个文本预处理步骤是移除HTML标签。让我们一步步查看代码：
- en: 'Let’s import the libraries for this example:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们为这个示例导入所需的库：
- en: '[PRE1]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The sample user reviews are shown here:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里展示了示例用户评论：
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we create a function that uses `BeautifulSoup` to parse the HTML content
    and extract only the text, removing any HTML tags:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个使用`BeautifulSoup`来解析HTML内容并提取文本的函数，移除所有HTML标签：
- en: '[PRE3]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then we preprocess all the reviews:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对所有评论进行预处理：
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we get the preprocessed reviews as follows:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们得到以下的预处理评论：
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As we can see, all the HTML tags have been removed and the text is clean. We
    will continue enhancing this example by adding another common preprocessing step:
    handling the capitalization of text.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，所有HTML标签已经被移除，文本变得干净整洁。我们将继续通过添加另一个常见的预处理步骤来增强此示例：处理文本的大小写。
- en: Handling capitalization and letter case
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理大小写
- en: Text data often comes in various cases—uppercase, lowercase, or a mix of both.
    Inconsistent capitalization can lead to ambiguity in language processing tasks.
    Therefore, one common text-cleaning practice is to standardize the letter case
    throughout the corpus. This not only aids in maintaining consistency but also
    ensures that the model generalizes well across different cases.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据通常有各种大小写——大写、小写或两者的混合。不一致的大小写可能会导致语言处理任务中的歧义。因此，一种常见的文本清理做法是统一整个语料库中的字母大小写。这不仅有助于保持一致性，还能确保模型在不同大小写之间具有良好的泛化能力。
- en: 'Building on the previous example, we are going to expand the preprocessing
    function to add one extra step: that of letter standardization:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的示例，我们将扩展预处理函数，增加一个额外步骤：字母标准化：
- en: 'Let’s first remind ourselves what the reviews looked like after the removal
    of HTML tags from the previous preprocessing step:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先回顾一下在上一步移除HTML标签后的评论是怎样的：
- en: '[PRE6]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following function will convert all characters into lowercase:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下函数将把所有字符转换为小写字母：
- en: '[PRE7]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will expand the `preprocess_text` function we presented in the previous
    example to convert all characters in the text to lowercase, making the text case
    insensitive:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将扩展在前一个示例中介绍的`preprocess_text`函数，将文本中的所有字符转换为小写字母，使得文本对大小写不敏感：
- en: '[PRE8]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s print the preprocessed reviews:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印出预处理后的评论：
- en: '[PRE9]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The lower-cased reviews are presented here:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里展示了小写处理后的评论：
- en: '[PRE10]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Notice how all the letters have turned to lower case! Go ahead and update the
    capitalization function as follows to turn everything to upper case:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意所有字母都变成小写了！请继续更新大小写函数，按照以下方式将所有内容转换为大写：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The upper case reviews are presented here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了大写字母的评论：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In case you are wondering whether you should use lower or upper case, we’ve
    got you covered.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在犹豫是否应该使用小写或大写，我们已经为你准备好了答案。
- en: Lower or upper case?
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小写还是大写？
- en: The choice between using lowercase or uppercase text depends on the specific
    requirements of the NLP task. For instance, tasks such as sentiment analysis typically
    benefit from lowercasing, as it simplifies the text and reduces variability. Conversely,
    tasks such as **Named Entity Recognition** (**NER**) may require preserving case
    information to accurately identify and differentiate entities.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小写或大写文本的选择取决于NLP任务的具体要求。例如，情感分析等任务通常更适合小写处理，因为这能简化文本并减少变异性。相反，像**命名实体识别（NER）**这样的任务可能需要保留大小写信息，以便准确识别和区分实体。
- en: For example, in German, all nouns are capitalized, so maintaining the case is
    crucial for correct language representation. In contrast, English typically does
    not use capitalization to convey meaning, so lowercasing might be more appropriate
    for general text analysis.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在德语中，所有名词都需要大写，因此保持大小写对于正确的语言表现至关重要。相比之下，英语通常不使用大小写来表达意义，因此对于一般文本分析来说，转换为小写可能更为合适。
- en: When dealing with text data from user inputs, such as social media posts or
    reviews, it’s important to consider the role of case variations. For instance,
    a tweet may use mixed case for emphasis or tone, which could be relevant for sentiment
    analysis.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理来自用户输入的文本数据时，如社交媒体帖子或评论，考虑大小写变化的作用非常重要。例如，一条推文可能会使用混合大小写来强调或表达语气，这对于情感分析可能是相关的。
- en: Modern LLMs such as **Bidirectional Encoder Representations from Transformers**
    (**BERT**) and GPT-3 are trained on mixed-case text and handle both uppercase
    and lowercase effectively. These models utilize case information to enhance context
    and understanding. Their tokenizers are designed to manage case sensitivity inherently,
    processing text without needing explicit conversion.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现代大型语言模型（LLMs），如**双向编码器表示（BERT）**和GPT-3，都是在混合大小写文本上训练的，能够有效处理大写和小写。这些模型利用大小写信息来增强上下文理解。它们的分词器本身设计能处理大小写敏感性，无需显式转换。
- en: If your task requires distinguishing between different cases (e.g., recognizing
    proper nouns or acronyms), it is better to preserve the original casing. However,
    always consult the documentation and best practices for the specific model you
    are using. Some models might be optimized for lowercased input and could perform
    better if the text is converted to lowercase.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的任务需要区分不同的大小写（例如，识别专有名词或首字母缩略词），最好保留原始的大小写。然而，始终参考你所使用的模型的文档和最佳实践。有些模型可能已优化为适应小写输入，如果文本转换为小写，可能会表现得更好。
- en: The next step is to learn how we can deal with numerical values and symbols
    in the text.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是学习如何处理文本中的数字值和符号。
- en: Dealing with numerical values and symbols
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理数字值和符号
- en: Numerical values, symbols, and mathematical expressions may be present in text
    data but may not always contribute meaningfully to the context. Cleaning them
    involves deciding whether to retain, replace, or remove these elements based on
    the specific requirements of the task.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数字值、符号和数学表达式可能会出现在文本数据中，但并不总是对上下文产生有意义的贡献。清理它们需要根据任务的具体要求决定是保留、替换还是删除这些元素。
- en: For instance, in sentiment analysis, numerical values might be less relevant,
    and their presence could be distracting. In contrast, for tasks related to quantitative
    analysis or financial sentiment, preserving numerical information becomes crucial.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在情感分析中，数字值可能不太相关，它们的存在可能会分散注意力。相反，对于与定量分析或金融情感相关的任务，保留数字信息变得至关重要。
- en: 'Building on the previous example, we are going to remove all the numbers and
    symbols in the text:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例基础上，我们将删除文本中的所有数字和符号：
- en: 'Let’s review how the data looked like after the previous preprocessing step:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们回顾一下上一步预处理后的数据样貌：
- en: '[PRE13]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let’s add a function that removes all characters from the text *except
    alphabetic characters* *and spaces*:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们添加一个函数，删除文本中所有*除字母字符外* *和空格*以外的字符：
- en: '[PRE14]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Apply the text preprocessing pipeline:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用文本预处理流程：
- en: '[PRE15]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s have a look at the preprocessed reviews:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看预处理后的评论：
- en: '[PRE16]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, after this preprocessing step, all the punctuations and symbols
    have been removed from the text. The decision to retain, replace, or remove symbols
    and punctuation during text preprocessing depends on the specific goals of your
    NLP task and the characteristics of your dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在这个预处理步骤之后，文本中的所有标点符号和符号都已被移除。在文本预处理过程中，是否保留、替换或删除符号和标点符号，取决于你NLP任务的具体目标和数据集的特征。
- en: Retaining symbols and punctuation
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保留符号和标点符号
- en: With the advancements in LLMs, the approach to handling punctuation and symbols
    during text preprocessing has evolved significantly. Modern LLMs benefit from
    retaining punctuation and symbols due to their extensive training on diverse datasets.
    This retention helps these models understand context more accurately by capturing
    nuances such as emotions, emphasis, and sentence boundaries. For instance, punctuation
    marks such as exclamation points and question marks play a crucial role in sentiment
    analysis by conveying strong emotions, which improves the model’s performance.
    Similarly, in tasks such as text generation, punctuation maintains readability
    and structure, while in NER and translation, it aids in identifying proper nouns
    and sentence boundaries.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大规模语言模型（LLMs）的发展，处理标点符号和符号的预处理方法已经发生了显著变化。现代大规模语言模型通过保留标点符号和符号受益，因为它们在多样化数据集上的广泛训练帮助模型更准确地理解上下文。保留这些符号有助于模型捕捉情感、强调和句子边界等细微差别。例如，感叹号和问号等标点符号在情感分析中发挥着重要作用，通过传达强烈的情感，提升了模型的表现。同样，在文本生成任务中，标点符号维持了可读性和结构，而在命名实体识别（NER）和翻译中，它有助于识别专有名词和句子边界。
- en: On the other hand, there are scenarios where removing punctuation and symbols
    can be advantageous. Modern LLMs are robust enough to handle noisy data, but in
    certain applications, simplifying text by removing punctuation can streamline
    preprocessing and *reduce the number of unique tokens*. This approach is beneficial
    for tasks such as topic modeling and clustering, where the focus is on content
    rather than structural elements. For example, removing punctuation can help identify
    core topics by eliminating distractions from sentence structure, and in text classification,
    it can standardize input data when punctuation does not add significant value.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，有些情况下，移除标点符号和符号可能会带来优势。现代大规模语言模型（LLMs）足够强大，能够处理噪声数据，但在某些应用中，通过移除标点符号简化文本可以优化预处理并*减少独特标记的数量*。这种方法对于主题建模和聚类等任务尤为有用，因为这些任务更侧重于内容而非结构元素。例如，移除标点符号有助于通过消除句子结构中的干扰来识别核心主题，而在文本分类中，当标点符号没有提供显著价值时，它可以帮助标准化输入数据。
- en: Another approach is replacing punctuation and symbols with spaces or specific
    tokens, which helps in normalizing text while preserving some level of separation
    between tokens. This method can be particularly useful for custom tokenization
    strategies. In specialized NLP pipelines, replacing punctuation with specific
    tokens can retain important distinctions without adding unnecessary clutter to
    the text, facilitating more effective tokenization and preprocessing for downstream
    tasks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是用空格或特定标记替换标点符号和符号，这有助于在规范化文本时保持标记之间的某种分隔。这种方法对于自定义分词策略特别有用。在专业的自然语言处理（NLP）管道中，将标点符号替换为特定标记可以保留重要的区别，而不会给文本添加不必要的杂乱，从而促进更有效的分词和下游任务的预处理。
- en: 'Let’s see a quick example on how to remove or replace symbols and punctuation.
    You can find the code for this section at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/2.punctuation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/2.punctuation.py):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来看看如何移除或替换符号和标点符号。你可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/2.punctuation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/2.punctuation.py)找到本节的代码：
- en: 'Create the sample text:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建示例文本：
- en: '[PRE17]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Option 1: replace symbols and punctuation with spaces:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选项 1：用空格替换符号和标点符号：
- en: '[PRE18]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will print the following output:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印以下输出：
- en: '[PRE19]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Option 2: remove symbols and punctuation:'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选项 2：移除符号和标点符号：
- en: '[PRE20]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will print the following output:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印以下输出：
- en: '[PRE21]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Removing symbols and numbers is a crucial preprocessing step in text analysis
    that simplifies text by eliminating non-alphanumeric characters. The last thing
    we will discuss in this section is addressing whitespace issues to enhance text
    readability and ensure consistent formatting.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 移除符号和数字是文本分析中的一个重要预处理步骤，它通过消除非字母数字字符简化了文本。在本节的最后，我们将讨论解决空格问题，以提高文本的可读性并确保一致的格式。
- en: Addressing whitespace and formatting issues
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理空格和格式问题
- en: Whitespaces and formatting inconsistencies can be prevalent in text data, especially
    when it originates from diverse sources. Cleaning involves addressing issues such
    as multiple consecutive spaces, leading or trailing whitespaces, and variations
    in formatting styles. Regularization of whitespace ensures a standardized text
    representation, reducing the risk of misinterpretation by downstream models.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 空格和格式不一致在文本数据中是常见的，尤其是当数据来源多样时。清理过程涉及解决多个连续空格、前后空格以及格式样式差异等问题。空格的规范化确保了文本表示的一致性，减少了下游模型误解的风险。
- en: Addressing whitespace and formatting issues remains crucial in the world of
    LLMs. Although modern LLMs exhibit robustness to various formatting inconsistencies,
    managing whitespace and formatting effectively can still enhance model performance
    and ensure data consistency.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 解决空格和格式化问题在大语言模型（LLMs）的世界中依然至关重要。尽管现代LLMs对各种格式不一致表现出较强的鲁棒性，但有效管理空格和格式仍能提升模型表现并确保数据一致性。
- en: Standardizing whitespace and formatting creates a uniform dataset, which facilitates
    model training and analysis by minimizing noise and focusing attention on the
    content rather than formatting discrepancies. Enhanced readability, achieved through
    proper whitespace management, aids both human and machine learning interpretation
    by clearly delineating text elements. Furthermore, consistent whitespace handling
    is essential for accurate tokenization—a fundamental process in many NLP tasks—as
    it ensures precise identification and processing of words and phrases.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化空格和格式化可以创建统一的数据集，这通过最小化噪声并将注意力集中在内容上而非格式差异，有助于模型训练和分析。通过适当的空格管理提高可读性，有助于人类和机器学习的解读，清晰地划定文本元素。此外，一致的空格处理对于准确的分词非常重要——这是许多NLP任务中的基础过程——它确保了单词和短语的精确识别和处理。
- en: 'So, let’s go back to the review example and add another step in the pipeline
    to remove whitespaces:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们回到评论示例，并在流程中添加另一步骤来去除空格：
- en: 'Let’s start by addressing whitespace and formatting issues. This function removes
    extra spaces and ensures that there is only one space between words:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先从解决空格和格式化问题开始。此函数移除多余的空格，并确保单词之间只有一个空格：
- en: '[PRE22]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we’ll add this to our text preprocessing pipeline:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在文本预处理管道中添加这一步骤：
- en: '[PRE23]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s have a look at the reviews before applying the new step and focus on
    the whitespaces marked here:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在应用新步骤之前，先看一下评论，并集中注意力于这里标记的空格：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, let’s review the clean dataset, after having applied the whitespace
    removal:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在应用了空格移除之后，让我们检查清理后的数据集：
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let’s move from pure text cleaning to focusing on safeguarding the data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从纯文本清理过渡到专注于保护数据。
- en: Removing personally identifiable information
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去除个人身份信息
- en: When preprocessing text data, removing **Personally Identifiable Information**
    (**PII**) is crucial for maintaining privacy, ensuring compliance with regulations,
    and improving data quality. For instance, consider a dataset of user reviews that
    includes names, email addresses, and phone numbers. If this sensitive information
    is not anonymized or removed, it poses significant risks such as privacy violations
    and potential misuse. Regulations such as the **General Data Protection Regulation**
    (**GDPR**), **California Consumer Privacy Act** (**CCPA**), and **Health Insurance
    Portability and Accountability Act** (**HIPAA**) mandate that personal data must
    be handled carefully. Failing to remove PII can lead to legal penalties and loss
    of trust. Moreover, including identifiable details can introduce bias into machine
    learning models and compromise their generalization. Removing PII is essential
    for responsible AI development, as it allows for the creation and use of datasets
    that maintain individual privacy while still providing valuable insights for research
    and analysis
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理文本数据时，去除**个人身份信息**（**PII**）对于维护隐私、确保符合规定以及提高数据质量至关重要。例如，考虑一个包含用户名、电子邮件地址和电话号码的用户评论数据集。如果这些敏感信息未被匿名化或移除，将会带来诸如隐私侵犯和潜在滥用等重大风险。**通用数据保护条例**（**GDPR**）、**加利福尼亚消费者隐私法案**（**CCPA**）和**健康保险流动性与责任法案**（**HIPAA**）等法规要求对个人数据进行小心处理。未能移除PII可能会导致法律处罚和信任丧失。此外，包含可识别的细节可能会给机器学习模型引入偏差，影响其泛化能力。去除PII对于负责任的人工智能开发至关重要，因为这可以在保持个人隐私的同时创建和使用数据集，为研究和分析提供有价值的见解。
- en: 'The following code snippet demonstrates how to use the presidio-analyzer and
    presidio-anonymizer libraries to detect and anonymize PII. Let’s have a look at
    the code step by step. The full code can be accessed at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/3.pii_detection.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/3.pii_detection.py):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段演示了如何使用`presidio-analyzer`和`presidio-anonymizer`库来检测和匿名化PII（个人身份信息）。我们一步步来看一下代码。完整代码可以通过以下链接访问：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/3.pii_detection.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/3.pii_detection.py)：
- en: 'Let’s start by importing the required libraries for this example:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先导入本示例所需的库：
- en: '[PRE26]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We create a sample DataFrame with one column named `text` containing sentences
    with different types of PII (e.g., names, email addresses, and phone numbers):'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个示例DataFrame，其中有一列名为`text`，包含包含不同类型PII（例如，姓名、电子邮件地址和电话号码）的句子：
- en: '[PRE27]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We initialize `AnalyzerEngine` for *detecting* PII entities and `AnonymizerEngine`
    for *anonymizing* the detected PII entities:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化`AnalyzerEngine`来*检测*PII实体，并初始化`AnonymizerEngine`来*匿名化*检测到的PII实体：
- en: '[PRE28]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we’ll define an anonymization function that detects PII in the text and
    applies masking rules based on the entity type:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个匿名化函数，该函数在文本中检测PII并根据实体类型应用掩码规则：
- en: '[PRE29]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `anonymize_text` function is designed to protect sensitive information within
    a given text by anonymizing specific types of entities. It first analyzes the
    text to identify entities such as names (`PERSON`), email addresses (`EMAIL_ADDRESS`),
    and phone numbers (`PHONE_NUMBER`) using an analyzer. For each entity type, it
    then applies a masking operation to conceal part of the information. Specifically,
    it masks the last four characters of a person’s name, the last five characters
    of an email address, and the last six characters of a phone number. The function
    returns the text with these sensitive entities anonymized, ensuring that personal
    information is obscured while retaining the overall structure of the text.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`anonymize_text`函数旨在通过匿名化特定类型的实体来保护给定文本中的敏感信息。它首先分析文本，识别出姓名（`PERSON`）、电子邮件地址（`EMAIL_ADDRESS`）和电话号码（`PHONE_NUMBER`）等实体。对于每种实体类型，它应用掩码操作来隐藏部分信息。具体来说，它会掩盖人名的最后四个字符、电子邮件地址的最后五个字符和电话号码的最后六个字符。该函数返回匿名化后的文本，确保个人信息被隐藏，同时保留文本的整体结构。'
- en: 'Apply the anonymization function to the DataFrame:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将匿名化函数应用于DataFrame：
- en: '[PRE30]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Display the DataFrame:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示DataFrame：
- en: '[PRE31]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: By using these configurations, you can tailor the anonymization process to meet
    specific requirements and ensure that sensitive information is properly protected.
    This approach helps you comply with privacy regulations and protect sensitive
    information in your datasets.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这些配置，您可以根据特定需求定制匿名化过程，确保敏感信息得到适当保护。这种方法有助于您遵守隐私法规，并保护数据集中的敏感信息。
- en: While removing PII is essential for protecting privacy and ensuring data compliance,
    another critical aspect of text preprocessing is handling rare words and spelling
    variations.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然删除PII对于保护隐私和确保数据合规性至关重要，但文本预处理的另一个关键方面是处理稀有词汇和拼写变体。
- en: Handling rare words and spelling variations
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理稀有词汇和拼写变体
- en: The rise of LLMs has revolutionized how we interact with technology and process
    information, particularly in the world of handling spelling variations and rare
    words. Before the emergence of LLMs, managing these linguistic challenges required
    extensive manual effort, often involving specialized knowledge and painstakingly
    crafted algorithms. Traditional spell-checkers and language processors struggled
    with rare words and variations, leading to frequent errors and inefficiencies.
    Today, LLMs such as GPT-4, Lllama3, and others have transformed this landscape
    by leveraging vast datasets and sophisticated machine-learning techniques to understand
    and generate text that accommodates a wide range of spelling variations and uncommon
    terminology. These models can recognize and correct misspellings, provide contextually
    appropriate suggestions, and accurately interpret rare words, enhancing the precision
    and reliability of text processing.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的崛起彻底改变了我们与技术互动和处理信息的方式，特别是在处理拼写变化和罕见词汇的领域。在LLMs出现之前，管理这些语言挑战需要大量的人工努力，通常涉及专业知识和精心设计的算法。传统的拼写检查器和语言处理工具在处理罕见词和变化时常常力不从心，导致频繁的错误和低效。今天，像GPT-4、Llama3等LLMs通过利用庞大的数据集和复杂的机器学习技术，已经彻底改变了这一局面，它们能够理解并生成适应各种拼写变化和罕见术语的文本。这些模型能够识别并修正拼写错误，提供上下文适当的建议，并准确解释罕见词汇，从而提高文本处理的精准度和可靠性。
- en: Dealing with rare words
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理罕见词
- en: In the era of LLMs such as GPT-3 and GPT-4, handling rare words has become less
    of a challenge compared to traditional NLP methods. These models have been trained
    on vast and diverse datasets, enabling them to understand and generate text with
    rare or even unseen words. However, there are still some considerations for text
    preprocessing and handling rare words effectively.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在像GPT-3和GPT-4这样的LLMs时代，处理罕见词相比传统的自然语言处理（NLP）方法已经不再是一个大问题。这些模型在庞大而多样的数据集上进行了训练，使它们能够理解并生成带有罕见甚至未见过的词汇的文本。然而，在文本预处理和有效处理罕见词方面仍然需要一些注意事项。
- en: So, how can we handle rare words with LLMs? There are some key concepts we need
    to understand, starting with tokenization. We won’t explore tokenization in detail
    here as we have a dedicated section later on; for now, let’s say that LLMs use
    **subword tokenization** methods that break down rare words into more common subword
    units. This helps in managing **Out-of-Vocabulary** (**OOV**) words by decomposing
    them into familiar components. The other interesting thing about LLMs is that
    even if they don’t know the word per se, they have contextual understanding capabilities,
    meaning that LLMs leverage context to infer the meaning of rare words.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何使用LLMs处理罕见词呢？我们需要理解一些关键概念，从分词开始。我们这里不深入探讨分词，因为稍后会有专门的部分进行讨论；现在，假设LLMs使用**子词分词**方法，将罕见词拆解成更常见的子词单元。这有助于通过将罕见词拆解成熟悉的组件来管理**词汇外**（**OOV**）词汇。关于LLMs的另一个有趣之处是，即使它们本身不认识某个词，它们也具备上下文理解能力，这意味着LLMs能够通过上下文推测罕见词的含义。
- en: 'In the following code example, we will test GPT-2 to see if it can handle rare
    words. You can find the code in the repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/4.rare_words.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/4.rare_words.py):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们将测试GPT-2是否能处理罕见词。您可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/4.rare_words.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/4.rare_words.py)找到代码：
- en: 'Let’s import the required libraries:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入所需的库：
- en: '[PRE32]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Initialize the GPT-2 tokenizer and model:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化GPT-2的分词器和模型：
- en: '[PRE33]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Define a text prompt with a rare word:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用罕见词定义文本提示：
- en: '[PRE34]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Encode the input text to tokens:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入文本编码为标记：
- en: '[PRE35]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Generate text until the output length reaches 50 tokens. The model generates
    text based on the input prompt, leveraging its understanding of the context to
    handle the rare word:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成文本直到输出长度达到50个标记。模型根据输入提示生成文本，利用其对上下文的理解来处理罕见词：
- en: '[PRE36]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `generate` function in the given code snippet is used to produce text output
    from a model based on the input tokens provided. The parameters used in this function
    call control various aspects of the text generation process:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 给定代码片段中的`generate`函数用于根据提供的输入标记生成模型的文本输出。此函数调用中的参数控制了文本生成过程中的各个方面：
- en: '`indexed_tokens`: This represents the input sequence that the model will use
    to start generating text. It consists of tokenized text that serves as the starting
    point for generation.'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indexed_tokens`：这表示模型将用来开始生成文本的输入序列。它由令牌化的文本组成，作为生成的起点。'
- en: '`max_length=50`: This parameter sets the maximum length of the generated text.
    The model will generate up to 50 tokens, including the input tokens, ensuring
    that the output doesn’t exceed this length.'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length=50`：此参数设置生成文本的最大长度。模型将生成多达50个令牌，包括输入令牌，确保输出不超过此长度。'
- en: '`num_beams=5`: This controls the beam search process, where the model keeps
    track of the top five most likely sequences during generation. Beam search helps
    improve the quality of the generated text by exploring multiple possible outcomes
    simultaneously and selecting the most likely one.'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams=5`：这控制梁搜索过程，模型在生成过程中跟踪最有可能的五个序列。梁搜索通过同时探索多个可能的结果并选择最可能的结果来提高生成文本的质量。'
- en: '`no_repeat_ngram_size=2`: This prevents the model from repeating any sequence
    of two tokens (bigrams) within the generated text. It helps produce more coherent
    and less repetitive output by ensuring that the same phrases don’t appear multiple
    times.'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_repeat_ngram_size=2`：这防止模型在生成文本中重复任何两个令牌（二元组）。通过确保相同的短语不会多次出现，它有助于生成更连贯和少重复的输出。'
- en: '`early_stopping=True`: This parameter allows the generation process to stop
    early if all beams have reached the end of the text sequence (e.g., a sentence-ending
    token). This can make the generation process more efficient by avoiding unnecessary
    continuation when a complete and sensible output has already been produced.'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping=True`：此参数允许生成过程在所有梁都到达文本序列末端（例如，句子结束令牌）时提前停止。通过在已经生成了完整且合理的输出时避免不必要的继续，这可以使生成过程更高效。'
- en: 'These parameters can be adjusted depending on the desired output. For instance,
    increasing `max_length` generates longer text, while modifying `num_beams` can
    balance quality and computational cost. Adjusting `no_repeat_ngram_size` changes
    the strictness of repetition prevention, and toggling `early_stopping` can affect
    the efficiency and length of the generated text. *I would advise that you go and
    play with these configurations to see how their output* *is affected*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数可以根据所需的输出进行调整。例如，增加`max_length`会生成更长的文本，而修改`num_beams`可以在质量和计算成本之间进行平衡。调整`no_repeat_ngram_size`可以改变重复预防的严格性，而切换`early_stopping`可能会影响生成文本的效率和长度。*我建议你去尝试这些配置，看看它们的输出*
    *会如何受到影响*：
- en: 'The generated tokens are decoded back into human-readable text:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成的令牌被解码成可读的文本：
- en: '[PRE37]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Print the decoded text:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印解码后的文本：
- en: '[PRE38]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As we can see, the model understood the meaning of *quokka* and created a sequence
    of words, additional text that continues from the prompt, showcasing the language
    generation capabilities of LLMs. This is possible because LLMs turn the tokens
    into a numerical representation called **embeddings**, as we will see later on,
    that capture the meaning of words.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，模型理解了*短尾树袋鼠*的含义，并创建了一个单词序列，这是从提示中继续的额外文本，展示了LLM的语言生成能力。这是可能的，因为LLM将令牌转换为称为**嵌入**的数字表示，我们将在稍后看到，它捕捉了单词的含义。
- en: We discussed the use of rare words in text preprocessing. Let’s now move to
    another challenge—spelling errors and typos.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了在文本预处理中使用罕见词。现在让我们转向另一个挑战——拼写错误和拼写错误。
- en: Addressing spelling variations and typos
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理拼写变体和拼写错误
- en: The challenge with spelling variations and typos is that it can lead to *different
    tokenizations for similar words*. In the era of LLMs, handling spelling and typos
    has become more sophisticated. LLMs can understand contexts and generate text
    that often corrects such errors implicitly. However, explicit preprocessing to
    correct spelling mistakes can still enhance the performance of these models, especially
    in applications where accuracy is critical. There are different ways to address
    spelling variations and mistakes, as we will see in the following section, starting
    with spelling correction.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写变体和拼写错误的挑战在于它可能导致*相似单词的不同标记化方式*。在LLM时代，处理拼写和拼写错误已变得更加复杂。LLM可以理解上下文并生成文本，通常会隐式地纠正这些错误。然而，显式预处理以纠正拼写错误仍然可以提高这些模型的性能，特别是在准确性至关重要的应用中。有多种方法可以解决拼写变体和错误，我们将在接下来的部分中看到，从拼写校正开始。
- en: Spelling correction
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拼写校正
- en: 'Let’s create an example of fixing spelling mistakes using an LLM with Hugging
    Face Transformers. We’ll use the experimental `oliverguhr/spelling-correction-english-base`
    spelling correction model for this demonstration. You can find the full code at
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/5.spelling_checker.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/5.spelling_checker.py):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用 Hugging Face Transformers 的大语言模型（LLM）创建一个修正拼写错误的示例。我们将使用实验性的`oliverguhr/spelling-correction-english-base`拼写校正模型进行演示。你可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/5.spelling_checker.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/5.spelling_checker.py)找到完整的代码：
- en: 'Define the spelling function pipeline. Inside this function, we initialize
    the spelling correction pipeline using the `oliverguhr/spelling-correction-english-base`
    model. This model is specifically trained for spelling correction tasks:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义拼写校正函数管道。在这个函数内部，我们使用`oliverguhr/spelling-correction-english-base`模型初始化拼写校正管道。这个模型是专门为拼写校正任务训练的：
- en: '[PRE39]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We use the pipeline to generate the corrected text. The `max_length` parameter
    is set to `2048` to allow for longer input texts:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用管道生成校正后的文本。`max_length`参数设置为`2048`，以便处理较长的输入文本：
- en: '[PRE40]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Test the function with some sample text containing spelling mistakes:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用包含拼写错误的示例文本测试该函数：
- en: '[PRE41]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: It’s important to note that this is an experimental model, and its performance
    may vary depending on the complexity and context of the input text. For more robust
    spelling and grammar correction, you might consider using more advanced models;
    however, some of them need authentication to download or sign agreements. So,
    for simplicity, we used an experimental model here. You can replace it with any
    model you have access to, from Llama3 to GPT4 and others.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这是一个实验性模型，它的表现可能会因输入文本的复杂性和上下文而有所不同。为了更稳健的拼写和语法校正，你可以考虑使用更高级的模型；然而，其中一些模型需要认证才能下载或签署协议。因此，为了简便起见，我们在这里使用了一个实验性模型。你可以将其替换为你能够访问的任何模型，从
    Llama3 到 GPT4 等等。
- en: The significance of spelling correction in text preprocessing tasks takes us
    nicely to the concept of fuzzy matching, a technique that further enhances the
    accuracy and relevance of generated content by accommodating minor errors and
    variations in input text.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写校正对于文本预处理任务的重要性将我们引入了模糊匹配的概念，这是一种通过容忍输入文本中的小错误和变化，进一步提高生成内容的准确性和相关性的技术。
- en: Fuzzy matching
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模糊匹配
- en: Fuzzy matching is a technique used to compare strings for similarity, even when
    they are not exactly the same. It’s like finding words that are “kind of similar”
    or “close enough.” So, we can use fuzzy matching algorithms to identify and map
    similar words, as well as to solve for variations and minor misspellings. We can
    enhance the spelling correction function by adding fuzzy matching using the `TheFuzz`
    library.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊匹配是一种用于比较字符串相似性的技术，即使它们并不完全相同。它就像是在寻找“有点相似”或“足够接近”的单词。因此，我们可以使用模糊匹配算法来识别和映射相似的单词，以及解决变体和小的拼写错误。我们可以通过添加使用`TheFuzz`库的模糊匹配来增强拼写校正功能。
- en: 'Let’s go through the code that you can find at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/6.fuzzy_matching.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/6.fuzzy_matching.py):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们浏览一下你可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/6.fuzzy_matching.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/6.fuzzy_matching.py)找到的代码：
- en: 'We’ll start by installing the library:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从安装库开始：
- en: '[PRE42]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let’s import the required libraries:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入所需的库：
- en: '[PRE43]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Initialize the spelling correction pipeline:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化拼写校正管道：
- en: '[PRE44]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `oliverguhr/spelling-correction-english-base` model is specifically fine-tuned
    for the task of spelling correction, making it a highly effective and efficient
    tool for spelling correction. This model has been trained to recognize and correct
    common spelling errors in English text, leading to greater accuracy. It is optimized
    for text-to-text generation, which allows it to efficiently generate corrected
    versions of input text with minimal computational overhead. Additionally, its
    training likely involved exposure to datasets containing spelling errors and their
    corrections, enabling it to make informed and contextually appropriate corrections.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`oliverguhr/spelling-correction-english-base` 模型专门为拼写修正任务进行了精细调整，使其成为一个高效且有效的拼写修正工具。该模型已经经过训练，能够识别并修正英语文本中的常见拼写错误，从而提高准确性。它经过优化，适用于文本到文本的生成，使其能够高效地生成输入文本的修正版本，并且计算开销最小。此外，模型的训练可能涉及了包含拼写错误及其修正的语料库，使其能够做出有根据且符合语境的修正。'
- en: 'Generate the corrected text as in the previous section:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成与上一节中相同的修正文本：
- en: '[PRE45]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Split the original and corrected texts into words:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始文本和修正后的文本分解为单词：
- en: '[PRE46]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create a dictionary of common English words (you can expand this list):'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个常见英语单词的词典（你可以扩展这个列表）：
- en: '[PRE47]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Fuzzy match each word:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模糊匹配每个单词：
- en: '[PRE48]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Test the function with some sample text containing spelling mistakes:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用包含拼写错误的一些示例文本测试函数：
- en: '[PRE49]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Print the results:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印结果：
- en: '[PRE50]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Now, as you can see, not all the spelling mistakes are corrected. We could get
    some better performance by fine-tuning the model on the examples it usually misses.
    However, there is good news! The rise of LLMs has made it less critical to correct
    spelling mistakes because these models are designed to understand and process
    text contextually. Even when words are misspelled, LLMs can infer the intended
    meaning by analyzing the surrounding words and overall sentence structure. This
    ability reduces the need for perfect spelling, as the primary focus shifts to
    conveying the message rather than ensuring every word is spelled correctly.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，并非所有拼写错误都已被修正。通过针对模型常常遗漏的例子进行微调，我们可以获得更好的表现。然而，好消息是！大语言模型（LLM）的兴起使得拼写错误的修正变得不那么重要，因为这些模型设计上是为了理解和处理文本的上下文。即使单词拼写错误，LLM也能通过分析周围的单词和整体句子结构推断出意图的含义。这种能力减少了对拼写完美的需求，因为焦点转向了传达信息，而不是确保每个单词的拼写都正确。
- en: After completing the initial text preprocessing steps, the next critical phase
    is **chunking**. This process involves breaking the cleaned text into smaller,
    meaningful units. Let’s discuss that in the following section.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 完成初步的文本预处理步骤后，下一步至关重要的是**分块**。这一过程涉及将清理过的文本分解成更小、更有意义的单元。我们将在接下来的部分讨论这一点。
- en: Chunking
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分块
- en: Chunking is an essential preprocessing step in NLP that involves breaking down
    text into smaller, manageable units, or “chunks.” This process is crucial for
    various applications, including text summarization, sentiment analysis, information
    extraction, and more.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 分块是自然语言处理（NLP）中的一个基本预处理步骤，它涉及将文本拆分成更小、更易管理的单元，或称“块”。这一过程对于多种应用至关重要，包括文本摘要、情感分析、信息提取等。
- en: Why is chunking becoming more and more important? By breaking down large documents,
    chunking enhances manageability and efficiency, particularly for models with *token
    limits*, preventing overload and enabling smoother processing. It also improves
    accuracy by allowing models to *focus on smaller, coherent segments of text*,
    which reduces noise and complexity compared to analyzing entire documents. Additionally,
    chunking helps maintain context within each segment, which is essential for tasks
    such as machine translation and text generation, ensuring that the model comprehends
    and processes the text effectively.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么分块变得越来越重要？通过将大型文档分解，分块提高了可管理性和效率，尤其是对于具有*令牌限制*的模型，防止过载并实现更平稳的处理。它还通过允许模型*专注于更小、更连贯的文本片段*来提高准确性，相较于分析整个文档，这样可以减少噪音和复杂性。此外，分块有助于在每个片段中保持上下文，这对于机器翻译和文本生成等任务至关重要，确保模型能够有效理解和处理文本。
- en: Chunking can be implemented in many different ways; for instance, summarization
    may benefit from paragraph-level chunks, whereas sentiment analysis might use
    sentence-level chunks to capture nuanced emotional tones. In the following sections,
    we will focus on fixed-length, recursive, and semantic chunking as we see them
    more often in the data world.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 分块可以通过多种方式实现；例如，摘要任务可能更适合段落级分块，而情感分析可能使用句子级分块来捕捉细微的情感变化。在接下来的部分中，我们将专注于固定长度分块、递归分块和语义分块，因为它们在数据领域中更为常见。
- en: Implementing fixed-length chunking
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现固定长度分块
- en: '**Fixed-length chunking** involves breaking text into chunks of a *predefined
    length*, either by character count or token count. It is usually preferred because
    it is very simple to implement and ensures uniform chunk sizes. However, as the
    split is random, it may split sentences or semantic units, leading to a loss of
    context. It is suitable for tasks where uniform chunk sizes are needed, such as
    certain types of text classification.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**固定长度分块**涉及将文本分成*预定义长度*的块，可以按字符数或标记数来划分。通常更为优选，因为它实现简单且确保块的大小一致。然而，由于划分是随机的，它可能会把句子或语义单元拆开，导致上下文的丧失。它适用于需要统一块大小的任务，如某些类型的文本分类。'
- en: 'To showcase fixed-length chunking, we are going to work with review data again,
    but we will include a few lengthier reviews. You can see the full example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/7.fixed_chunking.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/7.fixed_chunking.py):'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示固定长度分块，我们将再次使用评论数据，但这次会包括一些较长的评论。你可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/7.fixed_chunking.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/7.fixed_chunking.py)查看完整示例：
- en: 'Let’s start by loading the example data:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先加载示例数据：
- en: '[PRE51]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Import the `TokenTextSplitter` class:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`TokenTextSplitter`类：
- en: '[PRE52]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Initialize the `TokenTextSplitter` class with a chunk size of `50` tokens and
    no overlap:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`TokenTextSplitter`类，设置块大小为`50`个标记，并且没有重叠：
- en: '[PRE53]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Combine the reviews into a single text block for chunking:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将评论合并成一个文本块进行分块：
- en: '[PRE54]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Split the text into token-based chunks:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本拆分为基于标记的块：
- en: '[PRE55]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Print the chunks:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印分块：
- en: '[PRE56]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'To understand how varying chunk sizes affect the output, you can modify the
    `chunk_size` parameter. For instance, you might try chunk sizes of `20`, `70`,
    and `150` tokens. Here, you can see how you can adapt the code to test different
    chunk sizes:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解不同块大小如何影响输出，你可以修改`chunk_size`参数。例如，你可以尝试`20`、`70`和`150`标记大小。这里，你可以看到如何调整代码来测试不同的块大小：
- en: '[PRE57]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We successfully divided our review into the required chunks, but before moving
    forward, it’s crucial to understand the significance of the `chunk_overlap=0`parameter.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地将评论划分为所需的块，但在继续之前，理解`chunk_overlap=0`参数的重要性是至关重要的。
- en: Chunk overlap
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 块重叠
- en: '**Chunk overlap** refers to the number of characters or tokens that are *shared*
    between adjacent chunks when splitting a text. It’s the amount of text that “overlaps”
    between one chunk and the next.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**块重叠**是指在拆分文本时，*相邻块之间共享*的字符或标记数。它是两个块之间“重叠”的文本量。'
- en: Chunk overlap is crucial as it helps preserve context and enhance the coherence
    of the text. By ensuring that adjacent chunks share some common content, overlap
    *maintains continuity* and prevents important information from being lost at the
    boundaries. For instance, if a document is divided into chunks without overlap,
    a critical piece of information could be split between two chunks, potentially
    rendering it inaccessible or causing a loss of meaning. In retrieval tasks, such
    as searching or question-answering, overlap ensures that relevant details are
    captured even if they fall across chunk boundaries, thereby improving the effectiveness
    of the retrieval process. For example, if a chunk ends mid-sentence, the overlap
    ensures that the entire sentence is considered, which is essential for accurate
    comprehension and response generation.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 块重叠非常重要，因为它有助于保持上下文并增强文本的连贯性。通过确保相邻的块共享一些共同的内容，重叠*保持连续性*，防止重要信息在边界处丢失。例如，如果文档被分割成没有重叠的块，可能会有关键信息被分割成两个块，导致无法访问或丧失意义。在检索任务中，如搜索或问答，重叠确保即使相关细节跨越块边界，也能被捕捉到，从而提高检索过程的效果。例如，如果一个块在句子中间结束，重叠确保整个句子都会被考虑到，这是准确理解和生成回答所必需的。
- en: 'Let’s consider a simple example to illustrate chunk overlap:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子来说明块重叠：
- en: '[PRE58]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'With a chunk size of five words and an overlap of one word, we’ll get the following
    results:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用五个单词的块大小和一个单词的重叠，我们将得到以下结果：
- en: '[PRE59]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: As you can see, each chunk overlaps with the next by *two words*, helping to
    maintain context and prevent loss of meaning at chunk boundaries. Fixed-length
    chunking divides text into segments of a uniform size, but this method can sometimes
    fail to capture meaningful units of text, especially when dealing with natural
    language’s inherent variability. Transitioning to paragraph chunking, on the other
    hand, allows for a more contextually coherent approach by segmenting text based
    on its natural structure.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，每个块与下一个块之间有*两个单词*的重叠，这有助于保持上下文并防止在块边界丢失意义。固定长度的分块将文本分割成大小均匀的段落，但这种方法有时会无法捕捉到有意义的文本单元，特别是在处理自然语言固有的变动性时。另一方面，转向段落分块，通过根据文本的自然结构进行分割，提供了一种更具上下文连贯性的方法。
- en: Implementing RecursiveCharacter chunking
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现递归字符分块
- en: '`RecursiveCharacterTextSplitter` is a sophisticated text-splitting tool designed
    to handle more complex text segmentation tasks, especially when dealing with lengthy
    documents that need to be broken down into smaller, meaningful chunks. Unlike
    basic text splitters that simply cut text into fixed or variable-sized chunks,
    `RecursiveCharacterTextSplitter` uses a recursive approach to divide text, ensuring
    that each chunk is both contextually coherent and appropriately sized for processing
    by natural language models. Continuing from the review example, we will now demonstrate
    how to split a document into paragraphs using `RecursiveCharacterTextSplitter`.
    You can find the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/8.paragraph_chunking.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/8.paragraph_chunking.py):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`RecursiveCharacterTextSplitter` 是一个复杂的文本分割工具，专为处理更复杂的文本分割任务而设计，特别是当处理需要分解成更小、更有意义的块的长文档时。与简单的文本分割器不同，后者只是将文本切割成固定或可变大小的块，`RecursiveCharacterTextSplitter`
    使用递归方法来分割文本，确保每个块在上下文上既连贯又适合自然语言模型处理。从回顾示例开始，我们将演示如何使用 `RecursiveCharacterTextSplitter`
    将文档分割成段落。完整的代码可以在 [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/8.paragraph_chunking.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/8.paragraph_chunking.py)
    中找到：'
- en: 'We create a `RecursiveCharacterTextSplitter` instance:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个 `RecursiveCharacterTextSplitter` 实例：
- en: '[PRE60]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The `RecursiveCharacterTextSplitter` instance is instantiated with specific
    parameters:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`RecursiveCharacterTextSplitter` 实例是通过特定参数实例化的：'
- en: '`separators`: This is a list of separators used to split the text. Here, it
    includes double newlines (`\n\n`), single newlines (`\n`), spaces (), and empty
    strings (`""`). This helps the splitter to use natural text boundaries and whitespace
    for chunking.'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`separators`：这是一个分隔符列表，用于分割文本。在这里，它包括双换行符（`\n\n`）、单换行符（`\n`）、空格（）、以及空字符串（`""`）。这有助于分割器使用自然的文本边界和空白来进行分块。'
- en: '`chunk_size`: This is the maximum size of each chunk, set to 200 characters.
    This means each chunk will be *up to* 200 characters long.'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_size`：这是每个块的最大大小，设置为 200 个字符。这意味着每个块将*最多*包含 200 个字符。'
- en: '`chunk_overlap`: This is the number of characters overlapping between adjacent
    chunks, set to 0\. This means there is no overlap between chunks.'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_overlap`：这是相邻块之间重叠的字符数，设置为 0。也就是说，块之间没有重叠。'
- en: '`length_function`: This is a function used to measure the length of the text,
    set to `len`, which calculates the number of characters in a string.'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length_function`：这是一个用于衡量文本长度的函数，设置为 `len`，它计算字符串中的字符数。'
- en: 'Split the text into chunks:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本拆分成块：
- en: '[PRE61]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Print the chunks. In this first one, the user is very satisfied with the smartphone
    camera, praising the sharpness and vibrant colors of the photos. However, the
    user is disappointed with the laptop’s performance, citing frequent lags:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印这些块。在第一个块中，用户对智能手机的相机非常满意，赞扬了照片的清晰度和生动的色彩。然而，用户对笔记本电脑的性能感到失望，提到了频繁的卡顿问题：
- en: '[PRE62]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The user is pleased with the blender, noting its effectiveness in making smoothies,
    its power, and its ease of cleaning. They consider it a good value for the price:'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户对搅拌机很满意，指出其在制作果昔方面的高效性、强大的功率和易于清洁的特点。他们认为其性价比很高：
- en: '[PRE63]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The user had a negative experience with customer support, mentioning long wait
    times and unresolved issues. The user finds the book to be a fascinating read
    with an engaging storyline and well-developed characters, and they highly recommend
    it to readers:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户在与客户支持的互动中有不好的体验，提到了长时间等待和未解决的问题。用户认为这本书非常吸引人，情节引人入胜，人物刻画深入，他们强烈推荐给读者：
- en: '[PRE64]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We are left with one remaining word:'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们剩下一个单词：
- en: '[PRE65]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, these chunks are not perfect, but let’s understand how `RecursiveCharacterTextSplitter`
    works so that you can adjust it to your use case:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些块并不完美，但让我们了解 `RecursiveCharacterTextSplitter` 的工作原理，这样你就可以根据自己的使用场景进行调整：
- en: '**Chunk size target**: The splitter aims for chunks of about 200 characters,
    but this is a maximum rather than a strict requirement. It will try to create
    chunks as close to 200 characters as possible without exceeding this limit.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**块大小目标**：分割器的目标是生成大约 200 个字符的块，但这只是一个最大值，而不是严格要求。它将尽量创建接近 200 个字符的块，但不会超过这个限制。'
- en: '**Recursive approach**: The recursive nature means it will apply these rules
    repeatedly, working its way through the separator list until it finds an appropriate
    split point.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归方法**：递归性质意味着它会重复应用这些规则，通过分隔符列表逐步找到合适的分割点。'
- en: '**Preserving semantic meaning**: By using this approach, the splitter attempts
    to keep semantically related content together. For example, it will try to avoid
    splitting in the middle of a paragraph or sentence if possible.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保持语义意义**：通过使用这种方法，分割器尝试将语义相关的内容保持在一起。例如，它会尽量避免在段落或句子中间进行分割。'
- en: '`chunk_overlap` set to `0`, there’s no repetition of content between chunks.
    Each chunk is distinct.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_overlap` 设置为 `0`，这意味着块之间没有内容重复，每个块都是独立的。'
- en: '`len` function is used to measure chunk size, meaning it’s counting characters
    rather than tokens.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`len` 函数用于衡量块的大小，即它计算的是字符而不是词元。'
- en: The length_function parameter
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`length_function` 参数'
- en: The `length_function` parameter in `RecursiveCharacterTextSplitter` is a flexible
    option that allows you to define *how the length of text chunks is measured*.
    While `len` is the default and most common choice, there are many other options,
    from token-based to word-based to custom implementations.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`RecursiveCharacterTextSplitter` 中的 `length_function` 参数是一个灵活的选项，允许你定义*如何衡量文本块的长度*。虽然
    `len` 是默认且最常见的选择，但也有很多其他选项，从基于词元的到基于单词的，再到自定义实现。'
- en: While recursive chunking focuses on creating chunks based on fixed sizes and
    natural separators, semantic chunking takes this a step further by grouping text
    based on its meaning and context. This method ensures that chunks are not only
    coherent in length but also semantically meaningful, improving the relevance and
    accuracy of downstream NLP tasks.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 递归切块法专注于根据固定大小和自然分隔符创建块，而语义切块法则更进一步，通过根据文本的意义和上下文对其进行分组。这种方法确保了块不仅在长度上连贯，而且在语义上具有意义，从而提高了后续自然语言处理任务的相关性和准确性。
- en: Implementing semantic chunking
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现语义切块
- en: 'Semantic chunking involves breaking text into chunks based on semantic meaning
    rather than just syntactic rules or fixed lengths. Behind the scenes, they use
    *embeddings* to group related sentences together (we will deep dive into embeddings
    in [*Chapter 13*](B19801_13.xhtml#_idTextAnchor302)*, Image and Audio Preprocessing
    with LLMs*). We usually use semantic chunking for tasks requiring a deep understanding
    of context, such as question answering and thematic analysis. Let’s deep dive
    into the process behind semantic chunking:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分块涉及根据语义意义而非仅仅是句法规则或固定长度来拆分文本。在幕后，使用*嵌入*将相关的句子聚集在一起（我们将在[*第13章*](B19801_13.xhtml#_idTextAnchor302)中深入探讨嵌入，章节标题为《图像与音频预处理与LLMs》）。我们通常使用语义分块处理需要深度理解上下文的任务，例如问答系统和主题分析。让我们深入了解语义分块背后的过程：
- en: '**Text input**: The process begins with a text input, which could be a document,
    a collection of sentences, or any textual data that needs to be processed.'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本输入**：过程从文本输入开始，可以是一个文档、一组句子或任何需要处理的文本数据。'
- en: '**Embedding generation**: Each segment of the text (typically sentences or
    small groups of sentences (chunks)) is converted into a high-dimensional vector
    representation using embeddings. These embeddings are generated by pre-trained
    language models and the key to understand here is that these embeddings capture
    the semantic meaning of the text. In other words, we convert text into a numerical
    representation *that encodes* *its meaning*!'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入生成**：文本的每个片段（通常是句子或小组句子（块））都被转换为高维向量表示，使用嵌入生成。这些嵌入是由预训练语言模型生成的，关键是要理解这些嵌入捕捉了文本的语义含义。换句话说，我们将文本转化为一个数值表示，*它编码了*
    *其意义*！'
- en: '**Similarity measurement**: The embeddings are then compared to measure the
    semantic similarity between different parts of the text. Techniques such as cosine
    similarity are often used to quantify how closely related different segments are.'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**相似度测量**：然后将这些嵌入进行比较，以衡量文本不同部分之间的语义相似度。常用的技术如余弦相似度，用于量化不同片段之间的相关性。'
- en: '**Clustering**: Based on the similarity scores, sentences or text segments
    are clustered together. The clustering algorithm groups sentences that are semantically
    similar into the same chunk. This ensures that each chunk maintains semantic coherence
    and context.'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**聚类**：根据相似度评分，将句子或文本片段聚集在一起。聚类算法将语义相似的句子归为同一组，这样可以确保每个组内的内容保持语义一致性和上下文连贯性。'
- en: '**Chunk creation**: The clustered sentences are then combined to form chunks.
    These chunks are designed to be semantically meaningful units of text, which can
    be more effectively processed by NLP models.'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**块创建**：聚类后的句子被组合成块。这些块被设计为语义上有意义的文本单元，可以更有效地被NLP模型处理。'
- en: 'Let’s go back to the product review example and see what chunk we generate
    with semantic chunking. You can find the code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/9.semantic_chunking.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/9.semantic_chunking.py):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到产品评论的示例，看看通过语义分块生成了什么样的块。你可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/9.semantic_chunking.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/9.semantic_chunking.py)找到代码：
- en: 'Initialize `SemanticChunker` with `HuggingFaceEmbeddings`:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`HuggingFaceEmbeddings`初始化`SemanticChunker`：
- en: '[PRE66]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Split the text into chunks:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本拆分成块：
- en: '[PRE67]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Print the chunks:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印块：
- en: '[PRE68]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Each chunk contains related sentences that form a coherent segment. For example,
    chunk 1 discusses various product performances, while chunk 2 includes customer
    support experience and a book review. The chunks also maintain context within
    each segment, ensuring that related information is grouped together. A point for
    improvement is that chunk 1 includes reviews of different products (smartphone,
    laptop, and blender), and chunk 2 mixes a customer support experience with a book
    review, which could be seen as semantically unrelated. In this case, we could
    further split the text into smaller, more focused chunks to make it more coherent
    or/and tweak the parameters of the sematic chunker:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 每个块包含相关的句子，这些句子构成一个连贯的段落。例如，第一个块讨论了各种产品的性能，而第二个块则包括了客户支持体验和书评。这些块在每个段落内保持上下文一致，确保相关信息被组合在一起。需要改进的一点是，第一个块包含了不同产品（智能手机、笔记本电脑和搅拌机）的评价，而第二个块则将客户支持体验与书评混合，这可能被视为语义上不相关。在这种情况下，我们可以进一步将文本拆分成更小、更集中的块，以提高其连贯性，或/和调整语义切分器的参数。
- en: '[PRE69]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You can find more details about these parameters in the documentation:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在文档中找到更多关于这些参数的细节：
- en: '[https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html](https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html](https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html)'
- en: 'However, the steps to improve chunking in our case could look something like
    this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们这个案例中，改进切分的步骤可能是这样的：
- en: Use different embedding models to see which provides the best embeddings for
    your text
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的嵌入模型，看看哪一种为你的文本提供了最佳的嵌入。
- en: Tweak the buffer size to find the right balance between chunk size and coherence
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整缓冲区大小，以找到块大小和连贯性之间的最佳平衡
- en: Adjust the threshold type and amount to optimize where chunks are split based
    on semantic breaks
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整阈值类型和数量，以优化基于语义断点的块切分位置
- en: Customize the regular expression for sentence splitting to better fit the structure
    of your text
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义句子分割的正则表达式，以更好地适应文本的结构
- en: Transitioning from chunking to tokenization involves moving from a process where
    text is divided into larger, often syntactically significant segments (chunks)
    to a process where text is divided into smaller, more granular units (tokens).
    Let’s take a look at how **tokenization** works.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 从切分（chunking）到分词（tokenization）的过渡，意味着从一个将文本划分为更大、更具语法意义的段落（块）的过程，转向将文本划分为更小、更细粒度单元（词元）的过程。让我们来看一下**分词**是如何工作的。
- en: Tokenization
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词
- en: Tokenization is the process of breaking down a sequence of text into smaller
    units, or tokens, which can be words, subwords, or characters. This process is
    essential for converting text into a format suitable for *computational processing*,
    enabling models to learn patterns at a finer granularity.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将一段文本拆分成更小的单元或词元的过程，词元可以是单词、子词或字符。这个过程对于将文本转化为适合*计算处理*的格式至关重要，使得模型能够在更精细的粒度上学习模式。
- en: Some key terms in the Tokenization phase are `[CLS]` for classification, `[SEP]`
    for separation, etc.). Each token in the vocabulary is assigned an ID, which the
    model uses to represent the token internally. These IDs are integers and typically
    range from 0 to the size of the vocabulary minus one.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词阶段，一些关键术语包括 `[CLS]` 用于分类，`[SEP]` 用于分隔等。词汇表中的每个词项都会被分配一个 ID，模型内部使用这个 ID 来表示该词项。这些
    ID 是整数，通常范围从 0 到词汇表大小减一。
- en: Can all the words in the world fit into a vocabulary? The answer is *no*! OOV
    words are words that are not present in the model’s vocabulary.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 世界上所有的词汇都能放进一个词汇表里吗？答案是*不行*! OOV（Out-Of-Vocabulary）词是指模型词汇表中没有的词。
- en: Now that we know the main terms that are used, let’s explore the different types
    of tokenization and the challenges associated with them.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了常用的术语，让我们来探讨不同类型的分词以及与之相关的挑战。
- en: Word tokenization
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词分词
- en: Word tokenization involves splitting text into individual words.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 单词分词是将文本拆分为单个单词的过程。
- en: For example, the sentence “Tokenization is crucial in NLP!” would be tokenized
    into `["Tokenization", "is", "crucial", "in", "``NLP", "!"]`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，句子“Tokenization is crucial in NLP!”会被分词成`["Tokenization", "is", "crucial",
    "in", "NLP", "!"]`。
- en: Word tokenization preserves whole words, which can be beneficial for tasks requiring
    word-level understanding. It works well for languages with clear word boundaries.
    It is a simple solution but can lead to problems with OOV words, especially in
    specialized domains such as medical texts and texts with a lot of misspellings.
    You can find the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/10.word_tokenisation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/10.word_tokenisation.py).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 词语分词保留了完整的词语，这对于需要词语级理解的任务是有益的。它在词语边界明确的语言中效果良好。这是一种简单的解决方案，但可能导致 OOV 词汇的问题，特别是在医学文本和包含许多拼写错误的文本等专业领域中。你可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/10.word_tokenisation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/10.word_tokenisation.py)找到完整的代码。
- en: 'Let’s see a code example:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个代码示例：
- en: 'Download the necessary NLTK data (run this once):'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载必要的 NLTK 数据（只需运行一次）：
- en: '[PRE70]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Take the following as sample text:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以以下文本作为示例：
- en: '[PRE71]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Perform word tokenization:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行词语分词：
- en: '[PRE72]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Print the output:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印输出：
- en: '[PRE73]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This type of word tokenization is useful when dealing with simple, well-formed
    text where each word is clearly separated by spaces and punctuation. It’s an easy
    method that aligns well with how humans perceive words. However, different forms
    of the same word (e.g., “run”, “running”, “ran”) are treated as separate tokens,
    which can dilute the model’s understanding. It can also result in a large vocabulary,
    especially for languages with rich morphology or many unique words. Finally, these
    models struggle with words not seen during training, leading to OOV tokens.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这种词语分词方法适用于处理简单、结构良好的文本，其中每个词语都通过空格和标点符号清晰分隔。它是一种简单的方法，与人类感知词语的方式非常契合。然而，相同词语的不同形式（例如，“run”、“running”、“ran”）被视为不同的标记，这可能会削弱模型的理解能力。它还可能导致词汇量过大，特别是在形态丰富或具有许多独特词汇的语言中。最后，这些模型在训练过程中没有出现过的词语会成为
    OOV 标记。
- en: Given the limitations of word tokenization, subword tokenization methods have
    become popular. Subword tokenization strikes a balance between word-level and
    character-level tokenization, addressing many of the shortcomings of both.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于词语分词的局限性，子词分词方法变得越来越流行。子词分词在词语级别分词和字符级别分词之间取得了平衡，解决了两者的许多不足之处。
- en: Subword tokenization
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子词分词
- en: '**Subword tokenization** splits text into smaller units than words, typically
    subwords. By breaking the words into known subwords, it can handle OOV words.
    It reduces the vocabulary size and parameter count significantly. Let’s see the
    different options in the subword tokenization in the following sections.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**子词分词**将文本拆分成比词语更小的单位，通常是子词。通过将词语拆分成已知的子词，它可以处理 OOV（未登录词）问题。它显著减少了词汇量和参数数量。接下来的部分将介绍子词分词的不同选项。'
- en: Byte Pair Encoding (BPE)
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字节对编码（BPE）
- en: 'BPE starts with individual characters and iteratively merges the most frequent
    pairs of tokens to create subwords. It was originally developed as a data compression
    algorithm but has been adapted for tokenization in NLP tasks. The process looks
    like this:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 从单个字符开始，迭代地合并最频繁的标记对以创建子词。它最初作为一种数据压缩算法开发，但已经被改编用于 NLP 任务中的分词。过程如下：
- en: Start with a vocabulary of individual characters.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从单个字符的词汇表开始。
- en: Calculate the frequency of all character pairs in the text.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算文本中所有字符对的频率。
- en: Merge the most frequent pair of characters to form a new token.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并最频繁的字符对形成新的标记。
- en: Repeat the process until the desired vocabulary size is reached.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复该过程直到达到所需的词汇量。
- en: 'This frequency-based merging strategy can be useful for languages with simpler
    morphological structures (e.g., English) or when a straightforward yet robust
    tokenization is needed. It is simple and computationally efficient due to the
    frequency-based merging. Let’s demonstrate an example of how to implement BPE
    for tokenization. You can find the full example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/11.bpe_tokeniser.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/11.bpe_tokeniser.py):'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于频率的合并策略对于具有简单形态结构的语言（例如英语）或需要直接且稳健的分词时非常有用。由于基于频率的合并，它简单且计算高效。我们来演示一个如何实现BPE分词的例子。你可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/11.bpe_tokeniser.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/11.bpe_tokeniser.py)找到完整示例：
- en: 'Load the pre-trained tokenizer:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练的分词器：
- en: '[PRE74]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Load the sample text:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载示例文本：
- en: '[PRE75]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Tokenize the text:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对文本进行分词：
- en: '[PRE76]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Convert tokens to input IDs:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将符号转换为输入ID：
- en: '[PRE77]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Print the tokens and the IDs, as shown here:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印符号和ID，如下所示：
- en: '[PRE78]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: The special character “Ġ” in the tokenization output has a specific meaning
    in the context of BPE tokenization. It indicates that the token following it originally
    had a preceding space or was at the beginning of the text, so it allows for preserving
    information about word boundaries and spacing in the original text.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 分词输出中的特殊字符“Ġ”在BPE分词的上下文中具有特定含义。它表示紧随其后的符号原本前面有空格或位于文本的开头，因此它允许保留有关原始文本中单词边界和空格的信息。
- en: 'Let’s explain the output we see:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一下我们看到的输出：
- en: '`Token` and `ization`: These are subwords of “Tokenization”, split without
    a “Ġ” because *they’re part of the* *same word*.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Token` 和 `ization`：这些是“Tokenization”的子词，分割时没有“Ġ”，因为*它们是同一个单词*的一部分。'
- en: '`in`, `medical`, `texts`, and so on: These tokens start with **Ġ**, indicating
    they were *separate* words in the original text.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in`、`medical`、`texts`等：这些符号以**Ġ**开头，表示它们在原文中是*独立*的单词。'
- en: '`hyper`, `lip`, `id`, and `emia`: These are subwords of “hyperlipidemia”, a
    medical term. The `hyper` shows it’s a new word, while the subsequent subwords
    don’t have `hyperlipidemia` is broken into `hyper` (prefix meaning *excessive*),
    `lip` (related to fats), `id` (connecting element), and `emia` (suffix meaning
    *blood condition*).'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hyper`、`lip`、`id` 和 `emia`：这些是“hyperlipidemia”（高脂血症）的子词。“hyper”表示这是一个新词，而后续的子词没有“hyperlipidemia”被分解成
    `hyper`（表示*过量*的前缀）、`lip`（与脂肪相关）、`id`（连接元素）和 `emia`（表示*血液状况*的后缀）。'
- en: Having explored BPE tokenization and its impact on text processing, we now turn
    our attention to WordPiece tokenization, another powerful method that further
    refines the handling of subword units in NLP tasks.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了BPE分词及其对文本处理的影响后，我们现在将注意力转向WordPiece分词，这是一种进一步优化NLP任务中子词单元处理的强大方法。
- en: WordPiece tokenization
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WordPiece分词
- en: 'WordPiece, used by BERT, starts with a base vocabulary of characters, and iteratively
    adds the most frequent subword units. The process looks like this:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: WordPiece，BERT使用的分词方法，从一个字符的基本词汇表开始，迭代地添加最频繁的子词单元。过程如下所示：
- en: Start with a base vocabulary of individual characters and a special token for
    unknown words.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从单个字符的基本词汇表和一个用于未知词汇的特殊符号开始。
- en: Iteratively merge the most frequent pairs of tokens (starting with characters)
    to form new tokens until a predefined vocabulary size is reached.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代地合并最频繁的符号对（从字符开始）以形成新符号，直到达到预定义的词汇表大小。
- en: For any given word, the longest matching subword units from the vocabulary are
    used. This process is known as **maximum matching**.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于任何给定的词，使用词汇表中最长的匹配子词单元。这个过程称为**最大匹配**。
- en: 'WordPiece tokenization is effective for languages with complex word structures
    (e.g., Korean and Japanese) and when handling a diverse vocabulary efficiently
    is crucial. Its effectiveness comes from the fact that the merges are chosen based
    on maximizing the likelihood, leading to potentially more meaningful subwords.
    However, everything comes with a cost, and in this case, it is more computationally
    intensive due to the likelihood maximization step. Let’s have a look at a code
    example of how to perform WordPiece tokenization using the BERT tokenizer. You
    can find the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/12.tokenisation_wordpiece.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/12.tokenisation_wordpiece.py):'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: WordPiece 分词对于结构复杂的语言（例如韩语和日语）和高效处理多样化词汇至关重要。其有效性源于根据最大化可能性选择合并，因此可能会生成更有意义的子词。然而，一切都有代价，在这种情况下，由于可能性最大化步骤，它的计算密集性更高。让我们看一个使用BERT分词器执行WordPiece分词的代码示例。您可以在
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/12.tokenisation_wordpiece.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/12.tokenisation_wordpiece.py)
    找到完整的代码。
- en: 'Load the pre-trained tokenizer:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练分词器：
- en: '[PRE79]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Take some sample text:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取一些样本文本：
- en: '[PRE80]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Tokenize the text. This method splits the input text into WordPiece tokens.
    For example, `unaffordable` is broken down into `un`, `##``afford`, `##able`:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对文本进行分词。该方法将输入文本分割成WordPiece标记。例如，`unaffordable` 被分解为 `un`, `##afford`, `##able`：
- en: '[PRE81]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Convert tokens to input IDs:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标记转换为输入ID：
- en: '[PRE82]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The `##` prefix is used to denote that the token is a continuation of the previous
    token. Thus, it helps in reconstructing the original word by indicating that the
    token should be appended to the preceding token without a space.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '`##` 前缀用于表示该标记是前一个标记的延续。因此，它有助于通过指示应将标记附加到前一个标记而不加空格来重构原始单词。'
- en: After examining tokenization methods such as BPE and WordPiece, it is crucial
    to consider how tokenizers can be tailored to handle specialized data, such as
    medical texts, to ensure precise and contextually relevant processing in these
    specific domains.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在审查了诸如BPE和WordPiece等分词方法之后，关键是考虑如何调整分词器以处理专门领域的数据，例如医学文本，以确保在这些特定领域中进行精确和上下文相关的处理。
- en: Domain-specific data
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领域特定数据
- en: 'When working with domain-specific data such as medical texts, it’s crucial
    to ensure that the tokenizer can handle specialized vocabulary effectively. When
    a domain has a high frequency of unique terms or specialized vocabulary, standard
    tokenizers may not perform optimally. In this case, domain-specific tokenizers
    can better capture the nuances and terminology of the field, leading to improved
    model performance. When you are faced with this challenge, there are some options
    available:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理诸如医学文本之类的领域特定数据时，确保分词器能够有效处理专门词汇至关重要。当领域具有高频率的独特术语或专门词汇时，标准分词器可能无法达到最佳性能。在这种情况下，领域特定分词器可以更好地捕捉领域的细微差别和术语，从而提高模型性能。当面临这一挑战时，有一些可选方案：
- en: Train a tokenizer on a corpus of domain-specific texts to create a vocabulary
    that includes specialized terms
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在领域特定文本语料库上训练一个分词器，创建包含专门术语的词汇表
- en: Consider extending existing tokenizers with domain-specific tokens instead of
    training from scratch
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑通过领域特定标记扩展现有的分词器，而不是从头开始训练
- en: However, how can you know that you need to go the extra mile and tune the tokenizer
    on the dataset? Let’s find out.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如何确定您需要更进一步地调整数据集上的分词器呢？让我们找找答案。
- en: Evaluating the need for specialized tokenizers
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估是否需要专门的分词器
- en: 'As we explained, when working with domain-specific data, such as medical texts,
    it’s essential to evaluate whether a specialized tokenizer is needed to ensure
    accurate and contextually relevant processing. Let’s have a look at several key
    factors to consider:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所解释的，当处理诸如医学文本之类的领域特定数据时，评估是否需要专门的分词器是至关重要的。让我们看看几个考虑的关键因素：
- en: '**Analyze OOV rate**: Determine the percentage of words in your domain-specific
    corpus that are not included in the standard tokenizer’s vocabulary. A high OOV
    rate suggests that many important terms in your domain are not being recognized,
    highlighting the need for a specialized tokenizer to better handle the unique
    vocabulary.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分析 OOV 率**：确定您领域特定语料库中不包含在标准分词器词汇表中的单词的百分比。高 OOV 率表明您领域中许多重要术语未被识别，突显出需要专门的分词器来更好地处理独特的词汇。'
- en: '**Examine tokenization quality**: Check how standard tokenizers split domain-specific
    terms by manually reviewing sample tokenizations. If crucial terms, such as medical
    terminology, are frequently broken into meaningless subwords, this indicates that
    the tokenizer is not well-suited for the domain and may require customization.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检查分词质量**：通过手动检查样本分词，查看标准分词器如何分割领域特定术语。如果关键术语（如医学术语）经常被分解为无意义的子词，这表明分词器不适合该领域，可能需要定制化。'
- en: '**Compression ratio**: Measure the average number of tokens per sentence using
    both standard and domain-specific tokenizers. A significantly lower ratio with
    a domain-specific tokenizer suggests it is more efficient at compressing and representing
    domain knowledge, reducing redundancy, and improving performance.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压缩比**：使用标准和领域特定分词器测量每个句子的平均词汇数量。领域特定分词器显示出显著较低的比率，表明它在压缩和表示领域知识方面更为高效，减少冗余，提高性能。'
- en: For instance, in a medical corpus, terms such as *myocardial infarction* might
    be tokenized as `myo`, `cardial`, and `infarction` by a standard tokenizer, leading
    to a loss of meaningful context. A specialized medical tokenizer, however, might
    recognize *myocardial infarction* as a single term, preserving its meaning and
    improving the quality of downstream tasks such as entity recognition and text
    generation. Similarly, if a standard tokenizer results in an OOV rate of 15% compared
    to just 3% with a specialized tokenizer, it clearly indicates the need for a tailored
    approach. Lastly, if the compression ratio using a standard tokenizer is 1.8 tokens
    per sentence versus 1.2 tokens with a specialized tokenizer, it shows that the
    specialized tokenizer is more efficient and effective in capturing the domain-specific
    nuances.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在医学语料库中，术语如 *心肌梗死* 可能被标准分词器分成 `myo`、`cardial` 和 `infarction`，导致意义丧失。然而，专门的医学分词器可能将
    *心肌梗死* 识别为一个单独的术语，保留其含义并提升下游任务如实体识别和文本生成的质量。类似地，如果标准分词器的 OOV 率为 15%，而专门分词器仅为 3%，这清楚地表明需要定制化的需求。最后，如果使用标准分词器的压缩比为每句话
    1.8 个词汇，而专门分词器为 1.2 个词汇，则表明专门分词器在捕捉领域特定细微差别方面更为高效。
- en: 'Let’s implement a small application to evaluate the tokenizers for different
    medical data. The code for the example is available at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/13.specialised_tokenisers.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/13.specialised_tokenisers.py):'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个小应用程序，以评估不同医疗数据的分词器。示例的代码可在 [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/13.specialised_tokenisers.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/13.specialised_tokenisers.py)
    上找到：
- en: 'Initialize Stanza for biomedical text:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化用于生物医学文本的 Stanza：
- en: '[PRE83]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Initialize the standard GPT-2 tokenizer:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化标准 GPT-2 分词器：
- en: '[PRE84]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Set `pad_token` to `eos_token`:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `pad_token` 设置为 `eos_token`：
- en: '[PRE85]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Set `pad_token_id` for the model:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为模型设置 `pad_token_id`：
- en: '[PRE86]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'A sample medical corpus consisting of sentences related to myocardial infarction
    and heart conditions is defined:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个包含与心肌梗死和心脏病相关的句子的样本医学语料库：
- en: '[PRE87]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The following `stanza_tokenize` function uses the Stanza pipeline to tokenize
    the text and return a list of tokens:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的 `stanza_tokenize` 函数使用 Stanza 流水线对文本进行分词并返回一个词汇列表：
- en: '[PRE88]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The `calculate_oov_and_compression` function tokenizes each sentence in the
    corpus and calculates the OOV rate, as well as the average tokens per sentence,
    and returns all tokens. For standard tokenizers, it checks whether tokens are
    in the vocabulary, while for Stanza, it does not check OOV tokens explicitly:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`calculate_oov_and_compression` 函数对语料库中的每个句子进行分词并计算 OOV 率以及平均每句话的词汇数，并返回所有的词汇。对于标准分词器，它检查词汇表中是否存在这些标记，而对于
    Stanza，则不会显式检查 OOV 标记：'
- en: '[PRE89]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The `analyze_token_utilization` function calculates the frequency of each token
    in the corpus and returns a dictionary of token utilization percentages:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`analyze_token_utilization`函数计算语料库中每个标记的频率，并返回一个标记利用率百分比的字典：'
- en: '[PRE90]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The `calculate_perplexity` function calculates the perplexity of the model
    on the given text, which is a measure of how well the model predicts the sample:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`calculate_perplexity`函数计算给定文本的困惑度，这是衡量模型预测样本能力的指标：'
- en: '[PRE91]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The following script evaluates both the standard GPT-2 tokenizer and the Stanza
    medical tokenizer by calculating the OOV rate, average tokens per sentence, token
    utilization, and perplexity. Finally, it prints the results for each tokenizer
    and compares their performance on the `myocardial` `infarction` term:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下脚本通过计算OOV率、每句平均标记数、标记利用率和困惑度，评估了标准GPT-2分词器和Stanza医学分词器的性能。最后，它打印每个分词器的结果并比较它们在`myocardial`
    `infarction`术语上的表现：
- en: '[PRE92]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Let’s see the results of the two tokenizers presented in the following table:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下下表中展示的两个分词器的结果：
- en: '| **Metric** | **Standard** **GPT-2 tokenizer** | **Stanza** **Medical tokenizer**
    |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| **指标** | **标准** **GPT-2分词器** | **Stanza** **医学分词器** |'
- en: '| **OOV rate** | 0.00% | 0.00% |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| **OOV率** | 0.00% | 0.00% |'
- en: '| **Average tokens** **per sentence** | 10.80 | 7.60 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| **每句平均标记数** | 10.80 | 7.60 |'
- en: '| **Top five most** **used tokens** | . : 9.26% | . : 13.16% |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| **最常用的五个** **标记** | . : 9.26% | . : 13.16% |'
- en: '| ocard : 5.56% | infarction : 7.89% |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| ocard : 5.56% | infarction : 7.89% |'
- en: '| ial : 5.56% | myocardial : 5.26% |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| ial : 5.56% | myocardial : 5.26% |'
- en: '| Ġinf : 5.56% | heart : 5.26% |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| Ġinf : 5.56% | heart : 5.26% |'
- en: '| ar : 5.56% | The : 2.63% |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| ar : 5.56% | The : 2.63% |'
- en: Table 12.1 – Comparison of GPT-2 tokenizer and specialized medical tokenizer
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.1 – GPT-2分词器与专用医学分词器的比较
- en: 'As we see in the table, both tokenizers show an OOV rate of 0.00%, indicating
    that all tokens in the corpus are recognized by both tokenizers. The Stanza Medical
    tokenizer has a lower average rate of tokens per sentence (7.60) compared to the
    Standard GPT-2 tokenizer (10.80). This suggests that the Stanza Medical tokenizer
    is more efficient at compressing domain-specific terms into fewer tokens. The
    Standard GPT-2 tokenizer splits meaningful medical terms into smaller subwords,
    leading to less meaningful token utilization (e.g., `ocard`, `ial`, `inf`, and
    `ar`). However, the Stanza Medical tokenizer maintains the integrity of medical
    terms (e.g., `infarction` and `myocardial`), making the tokens more meaningful
    and contextually relevant. Based on the analysis, the Stanza Medical tokenizer
    should be preferred for medical text processing because of the following reasons:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 如表中所示，两个分词器的OOV率均为0.00%，这意味着语料库中的所有标记都被两个分词器识别。Stanza Medical分词器的每句平均标记数（7.60）低于标准GPT-2分词器（10.80）。这表明Stanza
    Medical分词器在将领域特定术语压缩成更少的标记方面更为高效。标准GPT-2分词器将有意义的医学术语拆分成更小的子词，从而导致标记利用效率较低（例如，`ocard`，`ial`，`inf`，和`ar`）。然而，Stanza
    Medical分词器保持了医学术语的完整性（例如，`infarction`和`myocardial`），使标记更加有意义且与上下文相关。基于分析，Stanza
    Medical分词器应该更适用于医学文本处理，原因如下：
- en: It efficiently tokenizes domain-specific terms into fewer tokens
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将领域特定术语高效地分词成更少的标记
- en: It preserves the integrity and meaning of medical terms
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它保持医学术语的完整性和意义
- en: It provides more meaningful and contextually relevant tokens, which is crucial
    for tasks such as entity recognition and text generation in the medical domain
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了更多有意义且与上下文相关的标记，这对于医学领域中的实体识别和文本生成等任务至关重要
- en: The Standard GPT-2 tokenizer, while useful for general text, splits medical
    terms into subwords, leading to potential loss of context and meaning, making
    it less suitable for specialized medical text.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 标准GPT-2分词器在处理一般文本时很有用，但它将医学术语拆分成子词，这可能导致上下文和意义的丧失，因此不太适合专门的医学文本。
- en: Vocabulary size trade-offs
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇大小权衡
- en: Larger vocabularies can capture more domain-specific terms but increase the
    model size and computational requirements. Find a balance that adequately covers
    domain terminology without excessive growth.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的词汇表可以捕捉到更多的领域特定术语，但会增加模型的大小和计算需求。找到一个平衡点，既能充分覆盖领域术语，又不至于过度膨胀。
- en: Having evaluated the performance of different tokenization methods, including
    their handling of OOV terms and their efficiency in compressing domain-specific
    knowledge, the next logical step is to explore how these tokenized outputs are
    transformed into meaningful numerical representations through embedding techniques.
    This transition is crucial, as embeddings form the foundation of how models understand
    and process the tokenized text.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估了不同的分词方法的表现后，包括它们如何处理 OOV 词汇以及在压缩领域特定知识时的效率，下一步的逻辑是探讨这些分词输出是如何通过嵌入技术转化为有意义的数值表示的。这一过渡非常关键，因为嵌入构成了模型理解和处理分词文本的基础。
- en: Turning tokens into embeddings
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 tokens 转换为嵌入
- en: Embeddings are numerical representations of words, phrases, or entire documents
    in a high-dimensional vector space. Essentially, we represent words as arrays
    of numbers to capture their semantic meaning. These numerical arrays aim to encode
    the underlying significance of words and sentences, allowing models to understand
    and process text in a meaningful way. Let’s explore the process from tokenization
    to embedding.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是词语、短语或整个文档在高维向量空间中的数值表示。实质上，我们将词语表示为数字数组，以捕捉它们的语义意义。这些数值数组旨在编码词语和句子的潜在意义，使得模型能够以有意义的方式理解和处理文本。让我们从分词到嵌入的过程进行探索。
- en: The process starts with tokenization, whereby text is split into manageable
    units called tokens. For instance, the sentence “The cat sat on the mat” might
    be tokenized into individual words or subword units such as [“The”, “cat”, “sat”,
    “on”, “the”, “mat”]. Once the text is tokenized, each token is mapped to an embedding
    vector using an embedding layer or lookup table. This table is often initialized
    with random values and then trained to capture meaningful relationships between
    words. For instance, `cat` might be represented as a 300-dimensional vector.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从分词开始，将文本拆分为可管理的单位，称为 tokens。例如，句子“The cat sat on the mat”可能被分词为单个词或子词单元，如
    [“The”, “cat”, “sat”, “on”, “the”, “mat”]。一旦文本被分词，每个 token 会通过嵌入层或查找表映射到一个嵌入向量。这个查找表通常会用随机值初始化，然后进行训练，以捕捉词语之间的有意义关系。例如，`cat`
    可能被表示为一个 300 维的向量。
- en: Advanced models such as transformers (e.g., BERT or GPT) generate contextual
    embeddings, where the vector representation of a word is influenced by its surrounding
    words. This allows the model to understand nuances and context, such as distinguishing
    between “bank” in “river bank” versus “financial bank.”
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 像 BERT 或 GPT 这样的高级模型会生成上下文化的嵌入，其中一个词的向量表示会受到其周围词语的影响。这使得模型能够理解细微差别和上下文，比如区分“bank”在“river
    bank”和“financial bank”中的不同含义。
- en: Let’s go and have a look at these models in more detail.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解这些模型。
- en: BERT – Contextualized Embedding Models
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT – 上下文化嵌入模型
- en: BERT is a powerful NLP model developed by Google. It belongs to the family of
    transformer-based models and is pre-trained on massive amounts of text data to
    learn contextualized representations of words.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是谷歌开发的强大 NLP 模型。它属于基于 transformer 的模型家族，并且在大量文本数据上进行预训练，以学习词语的上下文化表示。
- en: 'The BERT embedding model is a *component* of the BERT architecture that generates
    contextualized word embeddings. Unlike traditional word embeddings that assign
    a fixed vector representation to each word, BERT embeddings are context-dependent,
    capturing the meaning of words in the context of the entire sentence. Here’s an
    explanation of how to use the BERT embedding model [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/14.embedding_bert.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/14.embedding_bert.py):'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 嵌入模型是 BERT 架构的一个*组件*，用于生成上下文化的词嵌入。与传统的词嵌入不同，传统词嵌入为每个词分配一个固定的向量表示，而 BERT
    嵌入是上下文相关的，能够捕捉词语在整个句子中的意义。以下是如何使用 BERT 嵌入模型的解释 [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/14.embedding_bert.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/14.embedding_bert.py)：
- en: 'Load a pre-trained BERT model and tokenizer:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练的 BERT 模型和分词器：
- en: '[PRE93]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Encode the input text:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码输入文本：
- en: '[PRE94]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Obtain BERT embeddings:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取 BERT 嵌入：
- en: '[PRE95]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Print the embeddings:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印嵌入：
- en: '[PRE96]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: The shape of the embeddings tensor will be (`1`, `sequence_length`, `hidden_size`).
    Here, `sequence_length` is the number of tokens in the input sentence, and `hidden_size`
    is the size of the hidden states in the BERT model (`768` for `bert-base-uncased`).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入张量的形状将是（`1`，`sequence_length`，`hidden_size`）。其中，`sequence_length`是输入句子中的标记数，`hidden_size`是BERT模型中的隐藏状态大小（`bert-base-uncased`为`768`）。
- en: 'The `[CLS]` token embedding represents the entire input sentence and is often
    used for classification tasks. It’s the first token in the output tensor:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '`[CLS]` 标记嵌入表示整个输入句子，通常用于分类任务。它是输出张量中的第一个标记：'
- en: '[PRE97]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The embedding for the first actual word in the sentence represents the contextualized
    embedding for that specific word. The embedding associated with the first actual
    word in a sentence is not just a static or isolated representation of that word.
    Instead, it’s a “context-aware” or “contextualized” embedding, meaning it reflects
    how the word’s meaning is influenced by the surrounding words in the sentence.
    In simpler terms, this embedding captures not only the word’s intrinsic meaning
    but also how that meaning changes based on the context provided by the other words
    around it. This is a key feature of models such as BERT, which generate different
    embeddings for the same word depending on its usage in different contexts:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中第一个实际单词的嵌入表示该特定单词的上下文化嵌入。句子中第一个实际单词的嵌入不仅仅是该单词的静态或孤立表示。相反，它是一个“上下文感知”或“上下文化”的嵌入，意味着它反映了该单词的含义如何受到句子中周围单词的影响。简单来说，这个嵌入不仅捕捉了单词的内在含义，还反映了基于周围单词提供的上下文，单词含义如何变化。这是像BERT这样的模型的一个关键特性，它根据单词在不同上下文中的使用，生成不同的嵌入。
- en: '[PRE98]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: The key thing to understand here is that we started with text and we have vectors
    or embeddings as outputs. The tokenization step is happening behind the scenes
    when we use the tokenizer provided by the `transformers` library. The tokenizer
    converts the input sentence into tokens and their corresponding token IDs, which
    are then passed to the BERT model. Remember that each word in the sentence has
    its own embedding that reflects its meaning within the context of the sentence.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要理解的关键点是，我们从文本开始，输出是向量或嵌入。使用`transformers`库提供的分词器时，分词步骤是在幕后进行的。分词器将输入句子转换为标记及其对应的标记ID，然后传递给BERT模型。请记住，句子中的每个单词都有自己的嵌入，反映了该单词在句子上下文中的含义。
- en: BERT’s versatility has enabled it to perform exceptionally well across a wide
    array of NLP tasks. However, the need for more efficient and task-specific embeddings
    has led to the development of models such as **BAAI General Embedding** (**BGE**).
    BGE is designed to be smaller and faster, providing high-quality embeddings optimized
    for tasks such as semantic similarity and information retrieval.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的多功能性使其在各种NLP任务中表现出色。然而，随着对更高效和任务特定的嵌入需求的增加，出现了像**BAAI通用嵌入**（**BGE**）这样的模型。BGE设计为更小、更快，提供高质量的嵌入，优化了语义相似性和信息检索等任务。
- en: BGE
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BGE
- en: The BAAI/bge-small-en model is part of a series of BGE models developed by the
    **Beijing Academy of Artificial Intelligence** (**BAAI**). These models are designed
    for generating high-quality embeddings for text, typically used in various NLP
    tasks such as text classification, semantic search, and more.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: BAAI/bge-small-en模型是**北京人工智能研究院**（**BAAI**）开发的一系列BGE模型的一部分。这些模型旨在生成高质量的文本嵌入，通常用于文本分类、语义搜索等各种NLP任务。
- en: 'These models generate embeddings (vector representations) for text. The embeddings
    capture the semantic meaning of the text, making them useful for tasks such as
    similarity search, clustering, and classification. The `bge-small-en` model is
    a smaller, English-specific model in this series. Let’s see an example. The full
    code for this example is available at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/15.embedding_bge.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/15.embedding_bge.py):'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型为文本生成嵌入（向量表示）。嵌入捕捉文本的语义含义，使其在诸如相似性搜索、聚类和分类等任务中非常有用。`bge-small-en`模型是该系列中的一个较小的、专为英语设计的模型。我们来看一个例子。此示例的完整代码可在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/15.embedding_bge.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/15.embedding_bge.py)查看：
- en: 'Define the model name and parameters:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型名称和参数：
- en: '[PRE99]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Initialize the embeddings model:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化嵌入模型：
- en: '[PRE100]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'We sample a few sentences to embed:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随机选取几句话进行嵌入：
- en: '[PRE101]'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Generate embeddings for each sentence:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个句子生成嵌入：
- en: '[PRE102]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Print the embeddings:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印嵌入：
- en: '[PRE103]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: BGE models, such as bge-small-en, are designed to be smaller and more efficient
    for embedding generation tasks compared to larger, more general models such as
    BERT. This efficiency translates to reduced memory usage and faster inference
    times, making BGE models particularly suitable for applications where computational
    resources are limited or where real-time processing is crucial. While BERT is
    a versatile, general-purpose model capable of handling a wide range of NLP tasks,
    BGE models are specifically optimized for generating high-quality embeddings.
    This optimization allows BGE models to provide comparable or even superior performance
    for specific tasks, such as semantic search and information retrieval, where the
    quality of embeddings is paramount. By focusing on the precision and semantic
    richness of embeddings, BGE models leverage advanced techniques such as learned
    sparse embeddings, which combine the benefits of both dense and sparse representations.
    This targeted optimization enables BGE models to excel in scenarios that demand
    nuanced text representation and efficient processing, making them a better choice
    for embedding-centric applications compared to the more generalized BERT model.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 像bge-small-en这样的BGE模型，设计上比BERT等较大、通用的模型更小、更高效，适合用于嵌入生成任务。这种高效性转化为更低的内存使用和更快的推理时间，使得BGE模型特别适合计算资源有限或实时处理至关重要的应用。尽管BERT是一个多功能的通用模型，能够处理广泛的NLP任务，但BGE模型特别针对生成高质量的嵌入进行了优化。这种优化使得BGE模型能够在特定任务中提供可比或甚至更优的性能，如语义搜索和信息检索，在这些任务中，嵌入的质量至关重要。通过专注于嵌入的精确度和语义丰富性，BGE模型利用了先进的技术，如学习稀疏嵌入，结合了稠密和稀疏表示的优势。这种有针对性的优化使得BGE模型在需要细致文本表示和高效处理的场景中表现出色，相比更通用的BERT模型，它们在以嵌入为核心的应用中是更好的选择。
- en: Building on the success of both BERT and BGE, the introduction of **General
    Text Embeddings** (**GTEs**) marks another significant step forward. GTE models
    are specifically fine-tuned to deliver robust and efficient embeddings tailored
    for various text-related applications.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERT和BGE成功的基础上，**通用文本嵌入**（**GTEs**）的引入标志着又一步重要的进展。GTE模型专门针对各种文本相关应用进行了精细调优，以提供强大且高效的嵌入。
- en: GTE
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GTE
- en: 'GTEs represent the next generation of embedding models, designed to address
    the growing demand for specialized and efficient text representations. GTE models
    excel in providing high-quality embeddings for specific tasks such as semantic
    similarity, clustering, and information retrieval. Let’s see them in action. The
    full code is available at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/16.embedding_gte.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/16.embedding_gte.py):'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: GTE代表了下一代嵌入模型，旨在应对对专业化和高效文本表示日益增长的需求。GTE模型在为特定任务（如语义相似性、聚类和信息检索）提供高质量嵌入方面表现出色。让我们看看它们的实际应用。完整的代码可在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/16.embedding_gte.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/16.embedding_gte.py)找到：
- en: 'Load the GTE-base model:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载GTE-base模型：
- en: '[PRE104]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'We sample a few random texts to embed:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随机选取一些文本进行嵌入：
- en: '[PRE105]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Generate the embeddings:'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成嵌入：
- en: '[PRE106]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Print the shape of the embeddings:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印嵌入的形状：
- en: '[PRE107]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Print the first few values of the first embedding:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印第一个嵌入的前几个值：
- en: '[PRE108]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: One of the standout features of GTE is its efficiency. By maintaining a smaller
    model size and faster inference times, GTE is well-suited for real-time applications
    and environments with constrained computational resources. This efficiency does
    not come at the cost of performance; GTE models continue to deliver exceptional
    results across various text-processing tasks. However, their reduced complexity
    handling can be a limitation, as the smaller model size may impede their ability
    to process highly intricate or nuanced texts effectively. This could result in
    the less accurate capture of subtle contextual details, affecting performance
    in more complex scenarios. Additionally, a GTE’s focus on efficiency might lead
    to diminished generalization capabilities; although it excels in specific tasks,
    it may struggle to adapt to a wide variety of diverse or less common language
    inputs. Furthermore, the model’s smaller size may constrain its fine-tuning flexibility,
    potentially limiting its ability to adapt to specialized tasks or domains due
    to a reduced capacity for learning and storing intricate patterns specific to
    niche applications.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: GTE的一大亮点是其高效性。通过保持较小的模型尺寸和更快的推理时间，GTE非常适合实时应用和计算资源受限的环境。这种高效性并不以牺牲性能为代价；GTE模型在各种文本处理任务中依然能交付出色的结果。然而，它们减少的复杂性处理可能成为一个限制，因为较小的模型尺寸可能妨碍它们有效处理高度复杂或细致的文本。这可能导致对微妙上下文细节的捕捉不够准确，从而影响更复杂场景下的表现。此外，GTE对高效性的专注可能导致其泛化能力下降；尽管在特定任务中表现优秀，但它可能在适应多样化或不常见的语言输入时遇到困难。而且，模型较小的尺寸可能限制其微调的灵活性，由于学习和存储特定领域复杂模式的能力较弱，可能无法很好地适应专业化任务或领域。
- en: Selecting the right embedding model
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择合适的嵌入模型
- en: When selecting a model for your application, start by identifying your specific
    use case and domain. Whether you need a model for classification, clustering,
    retrieval, or summarization, and whether your domain is legal, medical, or general
    text, will significantly influence your choice.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在为你的应用选择模型时，首先确定你的具体使用场景和领域。你是否需要一个用于分类、聚类、检索或摘要的模型，且你的领域是法律、医学还是通用文本，将显著影响你的选择。
- en: Next, evaluate the model’s *size and memory usage*. Larger models generally
    provide better performance but come with increased computational requirements
    and higher latency. Begin with a smaller model for initial prototyping and consider
    transitioning to a larger one if your needs evolve. Pay attention to the embedding
    dimensions, as larger dimensions offer a richer representation of the data but
    are also more computationally intensive. Striking a balance between capturing
    detailed information and maintaining operational efficiency is key.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，评估模型的*大小和内存使用情况*。较大的模型通常能提供更好的性能，但也伴随着更高的计算要求和较高的延迟。在初期原型开发阶段，可以选择较小的模型，随着需求的发展，再考虑过渡到更大的模型。注意嵌入维度，因为更大的维度能够提供更丰富的数据表示，但也更具计算强度。在捕捉详细信息与保持操作效率之间找到平衡非常重要。
- en: Assess *inference time carefully*, particularly if you have real-time application
    requirements; models with higher latency might necessitate GPU acceleration to
    meet performance standards. Finally, evaluate the model’s performance using benchmarks
    such as the **Massive Text Embedding Benchmark** (**MTEB**) to compare across
    various metrics. Consider both intrinsic evaluations, which examine the model’s
    understanding of semantic and syntactic relationships, and extrinsic evaluations,
    which assess performance on specific downstream tasks.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细评估*推理时间*，特别是如果你有实时应用需求；延迟较高的模型可能需要GPU加速才能达到性能标准。最后，使用像**Massive Text Embedding
    Benchmark**（**MTEB**）这样的基准测试来评估模型的性能，以便在不同的度量标准之间进行比较。考虑到内在评估，它考察模型对语义和句法关系的理解，以及外在评估，它则评估模型在特定下游任务上的表现。
- en: Solving real problems with embeddings
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用嵌入解决实际问题
- en: 'With the advancements in embedding models such as BERT, BGE, and GTE, we can
    tackle a wide range of challenges across various domains. These models enable
    us to solve different problems, as presented here:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 随着BERT、BGE和GTE等嵌入模型的进展，我们可以应对各个领域的广泛挑战。这些模型使我们能够解决不同的问题，具体如下：
- en: '**Semantic search**: Embeddings improve search relevance by capturing the contextual
    meaning of queries and documents, enhancing the accuracy of search results.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义搜索**：嵌入通过捕捉查询和文档的上下文含义来提高搜索相关性，从而提升搜索结果的准确性。'
- en: '**Recommendation systems**: They facilitate personalized content suggestions
    based on user preferences and behaviors, tailoring recommendations to individual
    needs.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐系统**：它们根据用户的偏好和行为，提供个性化的内容推荐，量身定制推荐内容以满足个人需求。'
- en: '**Text classification**: Embeddings enable accurate categorization of documents
    into predefined classes, such as for sentiment analysis or topic identification.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类**：嵌入使得文档能够准确地归类到预定义的类别中，例如情感分析或主题识别。'
- en: '**Information retrieval**: They enhance the accuracy of retrieving relevant
    documents from extensive datasets, improving the efficiency of information retrieval
    systems.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息检索**：它们提高了从庞大数据集中检索相关文档的准确性，提升了信息检索系统的效率。'
- en: '**Natural language understanding**: Embeddings support tasks such as NER, helping
    systems identify and classify key entities within text.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言理解**：嵌入支持如命名实体识别（NER）等任务，帮助系统识别和分类文本中的关键实体。'
- en: '**Clustering techniques**: They improve the organization of similar documents
    or topics in large datasets, aiding in better clustering and data management.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类技术**：它们能够改善大型数据集中相似文档或主题的组织结构，帮助实现更好的聚类和数据管理。'
- en: '**Multimodal data processing**: Embeddings are essential for integrating and
    analyzing text, image, and audio data, leading to more comprehensive insights
    and enhanced decision-making capabilities.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态数据处理**：嵌入对于整合和分析文本、图像和音频数据至关重要，有助于提供更全面的洞察和增强的决策能力。'
- en: Let’s summarize the learnings from this chapter.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下本章的学习内容。
- en: Summary
  id: totrans-450
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we had a look at text preprocessing, which is an essential
    step in NLP. We saw different text cleaning techniques, from handling HTML tags
    and capitalization to addressing numerical values and whitespace challenges. We
    deep-dived into tokenization, examining word and subword tokenization, with practical
    Python examples. Finally, we explored various methods for embedding documents
    and introduced some of the most popular embedding models available today.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了文本预处理，这是自然语言处理中的一个关键步骤。我们介绍了不同的文本清理技术，从处理HTML标签和大小写到解决数字值和空格问题。我们深入探讨了分词，分析了词汇分词和子词分词，并提供了实用的Python示例。最后，我们介绍了多种文档嵌入方法，并介绍了当前最受欢迎的嵌入模型。
- en: In the next chapter, we will continue our journey with unstructured data, delving
    into image and audio preprocessing.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续探索非结构化数据，深入研究图像和音频的预处理。
