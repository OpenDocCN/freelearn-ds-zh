- en: Chapter 4. Using Hadoop Streaming with R
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章. 使用Hadoop Streaming与R
- en: 'In the previous chapter, we learned how to integrate R and Hadoop with the
    help of RHIPE and RHadoop and also sample examples. In this chapter, we are going
    to discuss the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何借助RHIPE和RHadoop将R与Hadoop进行集成，并且通过示例进行演示。在本章中，我们将讨论以下主题：
- en: Understanding the basics of Hadoop streaming
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Hadoop Streaming的基础
- en: Understanding how to run Hadoop streaming with R
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何使用R运行Hadoop Streaming
- en: Exploring the HadoopStreaming R package
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索HadoopStreaming R包
- en: Understanding the basics of Hadoop streaming
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Hadoop Streaming的基础
- en: Hadoop streaming is a Hadoop utility for running the Hadoop MapReduce job with
    executable scripts such as Mapper and Reducer. This is similar to the pipe operation
    in Linux. With this, the text input file is printed on stream (`stdin`), which
    is provided as an input to Mapper and the output (`stdout`) of Mapper is provided
    as an input to Reducer; finally, Reducer writes the output to the HDFS directory.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop Streaming是一个Hadoop工具，用于通过可执行脚本（如Mapper和Reducer）运行Hadoop MapReduce作业。这类似于Linux中的管道操作。通过这种方式，文本输入文件会被打印到流中（`stdin`），该流作为输入提供给Mapper，Mapper的输出（`stdout`）则作为输入提供给Reducer；最终，Reducer将输出写入HDFS目录。
- en: The main advantage of the Hadoop streaming utility is that it allows Java as
    well as non-Java programmed MapReduce jobs to be executed over Hadoop clusters.
    Also, it takes care of the progress of running MapReduce jobs. The Hadoop streaming
    supports the Perl, Python, PHP, R, and C++ programming languages. To run an application
    written in other programming languages, the developer just needs to translate
    the application logic into the Mapper and Reducer sections with the key and value
    output elements. We learned in [Chapter 2](ch02.html "Chapter 2. Writing Hadoop
    MapReduce Programs"), *Writing Hadoop MapReduce Programs*, that to create Hadoop
    MapReduce jobs we need Mapper, Reducer, and Driver as the three main components.
    Here, creating the driver file for running the MapReduce job is optional when
    we are implementing MapReduce with R and Hadoop.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop Streaming工具的主要优势在于它允许Java以及非Java编写的MapReduce作业在Hadoop集群上执行。此外，它还负责处理正在运行的MapReduce作业的进度。Hadoop
    Streaming支持Perl、Python、PHP、R和C++编程语言。若要运行用其他编程语言编写的应用程序，开发人员只需将应用程序逻辑转换为Mapper和Reducer部分，并且指定键值输出元素。我们在[第二章](ch02.html
    "第二章. 编写Hadoop MapReduce程序")，*编写Hadoop MapReduce程序*中已经了解到，要创建Hadoop MapReduce作业，我们需要Mapper、Reducer和Driver作为三个主要组件。在这里，当我们用R和Hadoop实现MapReduce时，创建用于运行MapReduce作业的驱动程序文件是可选的。
- en: This chapter is written with the intention of integrating R and Hadoop. So we
    will see the example of R with Hadoop streaming. Now, we will see how we can use
    Hadoop streaming with the R script written with Mapper and Reducer. From the following
    diagrams, we can identify the various components of the Hadoop streaming MapReduce
    job.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的编写目的是为了将R与Hadoop进行集成。因此，我们将展示R与Hadoop Streaming的示例。现在，我们将看到如何使用Hadoop Streaming与R脚本（包括Mapper和Reducer）一起使用。从下图中，我们可以识别出Hadoop
    Streaming MapReduce作业的各个组成部分。
- en: '![Understanding the basics of Hadoop streaming](img/3282OS_04_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![理解Hadoop Streaming的基础](img/3282OS_04_01.jpg)'
- en: Hadoop streaming components
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop Streaming组件
- en: Now, assume we have implemented our Mapper and Reducer as `code_mapper.R` and
    `code_reducer.R`. We will see how we can run them in an integrated environment
    of R and Hadoop. This can be run with the Hadoop streaming command with various
    generic and streaming options.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经将Mapper和Reducer实现为`code_mapper.R`和`code_reducer.R`。我们将看到如何在R和Hadoop的集成环境中运行它们。这可以通过带有各种通用和Streaming选项的Hadoop
    Streaming命令来运行。
- en: 'Let''s see the format of the Hadoop streaming command:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下Hadoop Streaming命令的格式：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The following diagram shows an example of the execution of Hadoop streaming,
    a MapReduce job with several streaming options.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了Hadoop Streaming的执行示例，这是一个带有多个Streaming选项的MapReduce作业。
- en: '![Understanding the basics of Hadoop streaming](img/3282OS_04_02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![理解Hadoop Streaming的基础](img/3282OS_04_02.jpg)'
- en: Hadoop streaming command options
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop Streaming命令选项
- en: In the preceding image, there are about six unique important components that
    are required for the entire Hadoop streaming MapReduce job. All of them are streaming
    options except jar.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图片中，有大约六个独特的重要组件是执行整个Hadoop Streaming MapReduce作业所必需的。它们都属于Streaming选项，除了jar。
- en: 'The following is a line-wise description of the preceding Hadoop streaming
    command:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面Hadoop Streaming命令的逐行描述：
- en: '**Line 1**: This is used to specify the Hadoop jar files (setting up the classpath
    for the Hadoop jar)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Line 1**: 此选项用于指定Hadoop jar文件（为Hadoop jar设置类路径）。'
- en: '**Line 2**: This is used for specifying the input directory of HDFS'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Line 2**: 此选项用于指定HDFS的输入目录。'
- en: '**Line 3**: This is used for specifying the output directory of HDFS'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Line 3**: 此选项用于指定HDFS的输出目录。'
- en: '**Line 4**: This is used for making a file available to a local machine'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Line 4**: 此选项用于将文件提供给本地机器。'
- en: '**Line 5**: This is used to define the available R file as Mapper'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Line 5**: 此选项用于定义可用的R文件作为Mapper。'
- en: '**Line 6**: This is used for making a file available to a local machine'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Line 6**: 此选项用于将文件提供给本地机器。'
- en: '**Line 7**: This is used to define the available R file as Reducer'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Line 7**: 此选项用于定义可用的R文件作为Reducer。'
- en: 'The main six Hadoop streaming components of the preceding command are listed
    and explained as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令中的六个主要Hadoop流式组件列出了并解释如下：
- en: '**jar:** This option is used to run a jar with coded classes that are designed
    for serving the streaming functionality with Java as well as other programmed
    Mappers and Reducers. It''s called the Hadoop streaming jar.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**jar:** 此选项用于运行带有编码类的jar文件，这些类设计用于提供Java流功能，以及其他编程的Mapper和Reducer。它被称为Hadoop流式jar。'
- en: '**input****:** This option is used for specifying the location of input dataset
    (stored on HDFS) to Hadoop streaming MapReduce job.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input****:** 此选项用于指定输入数据集的位置（存储在HDFS上），并传递给Hadoop流式MapReduce作业。'
- en: '**output:** This option is used for telling the HDFS output directory (where
    the output of the MapReduce job will be written) to Hadoop streaming MapReduce
    job.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output:** 此选项用于指定HDFS输出目录（MapReduce作业的输出将写入该目录），并传递给Hadoop流式MapReduce作业。'
- en: '**file:** This option is used for copying the MapReduce resources such as Mapper,
    Reducer, and Combiner to computer nodes (Tasktrackers) to make it local.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**file:** 此选项用于将MapReduce资源（如Mapper、Reducer和Combiner）复制到计算机节点（Tasktrackers），以使其本地化。'
- en: '**mapper:** This option is used for identification of the executable `Mapper`
    file.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mapper:** 此选项用于标识可执行的`Mapper`文件。'
- en: '**reducer:** This option is used for identification of the executable `Reducer`
    file.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**reducer:** 此选项用于标识可执行的`Reducer`文件。'
- en: 'There are other Hadoop streaming command options too, but they are optional.
    Let''s have a look at them:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他Hadoop流式命令选项，但它们是可选的。让我们来看看它们：
- en: '`inputformat`: This is used to define the input data format by specifying the
    Java class name. By default, it''s `TextInputFormat`.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputformat`: 用于通过指定Java类名称来定义输入数据格式。默认情况下，它是`TextInputFormat`。'
- en: '`outputformat`: This is used to define the output data format by specifying
    the Java class name. By default, it''s `TextOutputFormat`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputformat`: 用于通过指定Java类名称来定义输出数据格式。默认情况下，它是`TextOutputFormat`。'
- en: '`partitioner`: This is used to include the class or file written with the code
    for partitioning the output as (key, value) pairs of the Mapper phase.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`partitioner`: 用于包括编写的类或文件，以实现将Mapper阶段的输出按（键，值）对进行分区。'
- en: '`combiner`: This is used to include the class or file written with the code
    for reducing the Mapper output by aggregating the values of keys. Also, we can
    use the default combiner that will simply combine all the key attribute values
    before providing the Mapper''s output to the Reducer.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combiner`: 用于包括编写的类或文件，用于通过聚合键的值来减少Mapper输出的值。另外，我们也可以使用默认的combiner，它会在将Mapper的输出提供给Reducer之前，简单地合并所有键属性的值。'
- en: '`cmdenv`: This option will pass the environment variable to the streaming command.
    For example, we can pass `R_LIBS = /your /path /to /R /libraries`.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cmdenv`: 此选项用于将环境变量传递给流式命令。例如，可以传递`R_LIBS = /your /path /to /R /libraries`。'
- en: '`inputreader`: This can be used instead of the `inputformat` class for specifying
    the record reader class.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputreader`: 可用此选项替代`inputformat`类来指定记录读取器类。'
- en: '`verbose`: This is used to verbose the output.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`: 用于详细显示输出。'
- en: '`numReduceTasks`: This is used to specify the number of Reducers.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numReduceTasks`: 此选项用于指定Reducer的数量。'
- en: '`mapdebug`: This is used to debug the script of the `Mapper` file when the
    Mapper task fails.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mapdebug`: 当Mapper任务失败时，用于调试`Mapper`文件的脚本。'
- en: '`reducedebug`: This is used to debug the script of the `Reducer` file when
    the Reducer task fails.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reducedebug`: 当Reducer任务失败时，用于调试`Reducer`文件的脚本。'
- en: Now, it's time to look at some generic options for the Hadoop streaming MapReduce
    job.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候来看一些Hadoop流式MapReduce作业的通用选项了。
- en: '`conf`: This is used to specify an application configuration file.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conf`: 用于指定应用程序配置文件。'
- en: '[PRE1]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`D`: This is used to define the value for a specific MapReduce or HDFS property.
    For example:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`D`：用于定义特定MapReduce或HDFS属性的值。例如：'
- en: '`-D property = value or to specify the temporary HDFS directory`.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-D property = value 或指定临时HDFS目录`。'
- en: '[PRE2]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'or to specify the total number of zero Reducers:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或指定零Reducer的总数：
- en: '[PRE3]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `-D` option only works when a tool is implemented.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`-D`选项仅在工具实现时有效。'
- en: '`fs`: This is used to define the Hadoop NameNode.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs`：用于定义Hadoop NameNode。'
- en: '[PRE4]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`jt`: This is used to define the Hadoop JobTracker.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jt`：用于定义Hadoop JobTracker。'
- en: '[PRE5]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`files`: This is used to specify the large or multiple text files from HDFS.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`files`：用于指定来自HDFS的大型或多个文本文件。'
- en: '[PRE6]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`libjars`: This is used to specify the multiple jar files to be included in
    the classpath.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`libjars`：用于指定要包含在类路径中的多个jar文件。'
- en: '[PRE7]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`archives`: This is used to specify the jar files to be unarchived on the local
    machine.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`archives`：用于指定在本地机器上解压的jar文件。'
- en: '[PRE8]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Understanding how to run Hadoop streaming with R
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解如何使用R运行Hadoop流式处理
- en: Now, we understood what Hadoop streaming is and how it can be called with Hadoop
    generic as well as streaming options. Next, it's time to know how an R script
    can be developed and run with R. For this, we can consider a better example than
    a simple word count program.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们了解了Hadoop流式处理是什么以及如何使用Hadoop通用选项和流式选项调用它。接下来，了解如何开发和运行R脚本。为此，我们可以考虑一个比简单的词频统计程序更好的示例。
- en: 'The four different stages of MapReduce operations are explained here as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce操作的四个不同阶段如下所述：
- en: Understanding a MapReduce application
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解MapReduce应用程序
- en: Understanding how to code a MapReduce application
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何编写MapReduce应用程序
- en: Understanding how to run a MapReduce application
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何运行MapReduce应用程序
- en: Understanding how to explore the output of a MapReduce application
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何探索MapReduce应用程序的输出
- en: Understanding a MapReduce application
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解MapReduce应用程序
- en: 'Problem definition: The problem is to segment a page visit by the geolocation.
    In this problem, we are going to consider the website [http://www.gtuadmissionhelpline.com/](http://www.gtuadmissionhelpline.com/),
    which has been developed to provide guidance to students who are looking for admission
    in the Gujarat Technological University. This website contains the college details
    of various fields such as Engineering (diploma, degree, and masters), Medical,
    Hotel Management, Architecture, Pharmacy, MBA, and MCA. With this MapReduce application,
    we will identify the fields that visitors are interested in geographically.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 问题定义：该问题是根据地理位置对页面访问进行分段。在这个问题中，我们将考虑网站[http://www.gtuadmissionhelpline.com/](http://www.gtuadmissionhelpline.com/)，该网站旨在为希望进入古吉拉特科技大学的学生提供指导。该网站包含多个领域的学院信息，如工程（大专、学位和硕士）、医学、酒店管理、建筑、药学、MBA和MCA。通过这个MapReduce应用程序，我们将识别访客在地理位置上的兴趣领域。
- en: For example, most of the online visitors from Valsad city visit the pages of
    MBA colleges more often. Based on this, we can identify the mindset of Valsad
    students; they are highly interested in getting admissions in the MBA field. So,
    with this website traffic dataset, we can identify the city-wise interest levels.
    Now, if there are no MBA colleges in Valsad, it will be a big issue for them.
    They will need to relocate to other cities; this may increase the cost of their
    education.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，来自瓦尔萨德市的大多数在线访客更常访问MBA院校的页面。基于此，我们可以识别瓦尔萨德学生的心态；他们对进入MBA领域有着浓厚的兴趣。因此，通过这个网站流量数据集，我们可以识别按城市划分的兴趣水平。现在，如果瓦尔萨德没有MBA院校，对他们来说将是一个大问题。他们将需要搬到其他城市，这可能会增加他们的教育成本。
- en: By using this type of data, the Gujarat Technological University can generate
    informative insights for students from different cities.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这种数据，古吉拉特科技大学可以为来自不同城市的学生生成有价值的洞察。
- en: 'Input dataset source: To perform this type of analysis, we need to have the
    web traffic data for that website. Google Analytics is one of the popular and
    free services for tracking an online visitor''s metadata from the website. Google
    Analytics stores the web traffic data in terms of various dimensions ad metrics.
    We need to design a specific query to extract the dataset from Google Analytics.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集来源：要执行此类分析，我们需要获取该网站的Web流量数据。Google Analytics是一个流行的免费服务，用于跟踪在线访客的元数据。Google
    Analytics按各种维度和指标存储Web流量数据。我们需要设计一个特定的查询，从Google Analytics中提取数据集。
- en: 'Input dataset: The extracted Google Analytics dataset contains the following
    four data columns:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集：提取的 Google Analytics 数据集包含以下四个数据列：
- en: '`date`: This is the date of visit and in the form of YYYY/MM/DD.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`date`：这是访问日期，格式为 YYYY/MM/DD。'
- en: '`country`: This is the country of the visitor.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`country`：这是访问者所在的国家。'
- en: '`city`: This is the city of the visitor.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`city`：这是访问者所在的城市。'
- en: '`pagePath`: This is the URL of a page of the website.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pagePath`：这是网站页面的 URL。'
- en: 'The head section of the input dataset is as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集的头部如下：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The expected output format is shown in the following diagram:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的输出格式如下图所示：
- en: '![Understanding a MapReduce application](img/3282OS_04_03.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![理解 MapReduce 应用程序](img/3282OS_04_03.jpg)'
- en: 'The following is a sample output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输出：
- en: '![Understanding a MapReduce application](img/3282OS_04_04.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![理解 MapReduce 应用程序](img/3282OS_04_04.jpg)'
- en: Understanding how to code a MapReduce application
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解如何编写 MapReduce 应用程序
- en: 'In this section, we will learn about the following two units of a MapReduce
    application:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将学习 MapReduce 应用程序的以下两个单元：
- en: Mapper code
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mapper 代码
- en: Reducer code
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reducer 代码
- en: Let's start with the Mapper code.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 Mapper 代码开始。
- en: 'Mapper code: This R script, named `ga-mapper.R`, will take care of the Map
    phase of a MapReduce job.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Mapper 代码：这个名为 `ga-mapper.R` 的 R 脚本将处理 MapReduce 作业的 Map 阶段。
- en: The Mapper's job is to work on each line and extract a pair (key, value) and
    pass it to the Reducer to be grouped/aggregated. In this example, each line is
    an input to Mapper and the output `City:PagePath`. `City` is a key and `PagePath`
    is a value. Now Reducer can get all the page paths for a given city; hence, it
    can be grouped easily.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Mapper 的工作是处理每一行，提取一对（键，值）并将其传递给 Reducer 以进行分组/聚合。在这个示例中，每一行是输入到 Mapper 的数据，输出为
    `City:PagePath`。`City` 是键，`PagePath` 是值。现在 Reducer 可以获取给定城市的所有页面路径，因此可以轻松地进行分组。
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We extract the third and fourth element for the city and pagePath respectively.
    Then, they will be written to the stream as key-value pairs and fed to Reducer
    for further processing.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分别提取城市和页面路径的第三个和第四个元素。然后，它们将作为键值对写入流并传递给 Reducer 以进一步处理。
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As soon as the output of the Mapper phase as (key, value) pairs is available
    to the standard output, Reducers will read the line-oriented output from `stdout`
    and convert it into final aggregated key-value pairs.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Mapper 阶段的输出作为（键，值）对可用，Reducers 将从 `stdout` 读取按行输出，并将其转换为最终的聚合键值对。
- en: Let's see how the Mapper output format is and how the input data format of Reducer
    looks like.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Mapper 输出格式是什么样的，以及 Reducer 输入数据格式是怎样的。
- en: 'Reducer code: This R script named `ga_reducer.R` will take care of the Reducer
    section of the MapReduce job.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer 代码：这个名为 `ga_reducer.R` 的 R 脚本将处理 MapReduce 作业的 Reducer 部分。
- en: As we discussed, the output of Mapper will be considered as the input for Reducer.
    Reducer will read these city and pagePath pairs, and combine all of the values
    with its respective key elements.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所讨论的，Mapper 的输出将作为 Reducer 的输入。Reducer 会读取这些城市和页面路径对，并将所有值与其相应的键元素进行合并。
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The Mapper output is written in two main fields with `\t` as the separator and
    the data line-by-line; hence, we have split the data by using `\t` to capture
    the two main attributes (key and values) from the stream input.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Mapper 输出以两个主要字段写入，使用 `\t` 作为分隔符，并按行输出数据；因此，我们通过使用 `\t` 分割数据，以便从流输入中捕获两个主要属性（键和值）。
- en: After collecting the key and value, the Reducer will compare it with the previously
    captured value. If not set previously, then set it; otherwise, combine it with
    the previous character value using the `combine` function in R and finally, print
    it to the HDFS output location.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 收集键和值后，Reducer 会将其与之前捕获的值进行比较。如果之前没有设置，则进行设置；否则，使用 R 中的 `combine` 函数将其与之前的字符值结合，最后将其打印到
    HDFS 输出位置。
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Understanding how to run a MapReduce application
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解如何运行 MapReduce 应用程序
- en: After the development of the Mapper and Reducer script with the R language,
    it's time to run them in the Hadoop environment. Before we execute this script,
    it is recommended to test them on the sample dataset with simple pipe operations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 R 语言开发 Mapper 和 Reducer 脚本后，是时候在 Hadoop 环境中运行它们了。在执行这个脚本之前，建议先在简单的管道操作上测试它们，使用样本数据集进行验证。
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding command will run the developed Mapper and Reducer scripts over
    a local machine. But it will run similar to the Hadoop streaming job. We need
    to test this for any issue that might occur at runtime or for the identification
    of programming or logical mistakes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将在本地计算机上运行开发的 Mapper 和 Reducer 脚本。但它将类似于 Hadoop 流式作业的运行方式。我们需要测试这个命令，以便发现可能在运行时出现的问题，或者识别编程或逻辑错误。
- en: 'Now, we have Mapper and Reducer tested and ready to be run with the Hadoop
    streaming command. This Hadoop streaming operation can be executed by calling
    the generic `jar` command followed with the streaming command options as we learned
    in the *Understanding the basics of Hadoop streaming* section of this chapter.
    We can execute the Hadoop streaming job in the following ways:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经测试并准备好使用 Hadoop 流式命令运行 Mapper 和 Reducer。这个 Hadoop 流式操作可以通过调用通用的 `jar`
    命令并附带流式命令选项来执行，就像我们在本章的 *理解 Hadoop 流式处理的基础知识* 部分中学到的那样。我们可以通过以下方式执行 Hadoop 流式作业：
- en: From a command prompt
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从命令提示符
- en: R or the RStudio console
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 或 RStudio 控制台
- en: The execution command with the generic and streaming command options will be
    the same for both the ways.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 执行命令与通用命令选项和流式命令选项在两种方式下是相同的。
- en: Executing a Hadoop streaming job from the command prompt
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从命令提示符执行 Hadoop 流式作业
- en: 'As we already learned in the section *Understanding the basics of Hadoop streaming*,
    the execution of Hadoop streaming MapReduce jobs developed with R can be run using
    the following command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 *理解 Hadoop 流式处理的基础知识* 部分中学到的，使用 R 开发的 Hadoop 流式 MapReduce 作业的执行可以使用以下命令：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Executing the Hadoop streaming job from R or an RStudio console
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 R 或 RStudio 控制台执行 Hadoop 流式作业
- en: 'Being an R user, it will be more appropriate to run the Hadoop streaming job
    from an R console. This can be done with the `system` command:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 R 用户，从 R 控制台运行 Hadoop 流式作业会更合适。这可以通过 `system` 命令完成：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This preceding command is similar to the one that you have already used in the
    command prompt to execute the Hadoop streaming job with the generic options as
    well as the streaming options.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个前面的命令与您已在命令提示符中使用的命令相似，用于执行带有通用选项和流式选项的 Hadoop 流式作业。
- en: Understanding how to explore the output of MapReduce application
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解如何探索 MapReduce 应用的输出
- en: After completing the execution successfully, it's time to explore the output
    to check whether the generated output is important or not. The output will be
    generated along with two directories, `_logs` and `_SUCCESS`. `_logs` will be
    used for tracking all the operations as well as errors; `_SUCCESS` will be generated
    only on the successful completion of the MapReduce job.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 执行成功后，就该探索输出，以检查生成的输出是否重要。输出将与两个目录 `_logs` 和 `_SUCCESS` 一起生成。`_logs` 用于跟踪所有操作和错误；`_SUCCESS`
    仅在 MapReduce 作业成功完成时生成。
- en: 'Again, the commands can be fired in the following two ways:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，可以通过以下两种方式触发命令：
- en: From a command prompt
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从命令提示符
- en: From an R console
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 R 控制台
- en: Exploring an output from the command prompt
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从命令提示符查看输出
- en: 'To list the generated files in the output directory, the following command
    will be called:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出输出目录中生成的文件，将调用以下命令：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The snapshot for checking the output is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 用于检查输出的快照如下：
- en: '![Exploring an output from the command prompt](img/3282OS_04_05.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![从命令提示符查看输出](img/3282OS_04_05.jpg)'
- en: Exploring an output from R or an RStudio console
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 R 或 RStudio 控制台查看输出
- en: The same command can be used with the `system` method in the R (with RStudio)
    console.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的命令可以在 R（与 RStudio）控制台中通过 `system` 方法使用。
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A screenshot of the preceding function is shown as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 前面函数的截图如下：
- en: '![Exploring an output from R or an RStudio console](img/3282OS_04_06.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![从 R 或 RStudio 控制台查看输出](img/3282OS_04_06.jpg)'
- en: Understanding basic R functions used in Hadoop MapReduce scripts
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Hadoop MapReduce 脚本中使用的基本 R 函数
- en: 'Now, we will see some basic utility functions used in Hadoop Mapper and Reducer
    for data processing:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看一些在 Hadoop Mapper 和 Reducer 中用于数据处理的基本实用函数：
- en: '`file`: This function is used to create the connection to a file for the reading
    or writing operation. It is also used for reading and writing from/to `stdin`
    or `stdout`. This function will be used at the initiation of the Mapper and Reducer
    phase.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`file`：此函数用于创建文件连接以进行读写操作。它还用于从 `stdin` 或 `stdout` 读写。此函数将在 Mapper 和 Reducer
    阶段的初始化时使用。'
- en: '[PRE19]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`write`: This function is used to write data to a file or standard input. It
    will be used after the key and value pair is set in the Mapper.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write`：此函数用于将数据写入文件或标准输入。它将在 Mapper 中设置好键值对后使用。'
- en: '[PRE20]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`print`: This function is used to write data to a file or standard input. It
    will be used after the key and value pair is ready in the Mapper.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`print`：此函数用于将数据写入文件或标准输入。它将在 Mapper 中准备好键值对后使用。'
- en: '[PRE21]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`close`: This function can be used for closing the connection to the file after
    the reading or writing operation is completed. It can be used with Mapper and
    Reducer at the close (`conn`) end when all the processes are completed.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`close`：此函数可用于在读取或写入操作完成后关闭与文件的连接。它可以在所有处理完成时与 Mapper 和 Reducer 一起使用，位于关闭（`conn`）端。'
- en: '`stdin`: This is a standard connection corresponding to the input. The `stdin()`
    function is a text mode connection that returns the connection object. This function
    will be used in Mapper as well as Reducer.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stdin`：这是对应输入的标准连接。`stdin()` 函数是一个文本模式连接，返回连接对象。此函数将在 Mapper 和 Reducer 中使用。'
- en: '[PRE22]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`stdout`: This is a standard connection corresponding to the output. The `stdout()`
    function is a text mode connection that also returns the object. This function
    will be used in Mapper as well as Reducer.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stdout`：这是对应输出的标准连接。`stdout()` 函数是一个文本模式连接，也会返回对象。此函数将在 Mapper 和 Reducer 中使用。'
- en: '[PRE23]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`sink`: `sink` drives the R output to the connection. If there is a file or
    stream connection, the output will be returned to the file or stream. This will
    be used in Mapper and Reducer for tracking all the functional outputs as well
    as the errors.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sink`：`sink` 将 R 输出发送到连接。如果是文件或流连接，输出将返回到文件或流。这将在 Mapper 和 Reducer 中用于跟踪所有功能输出以及错误。'
- en: '[PRE24]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Monitoring the Hadoop MapReduce job
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控 Hadoop MapReduce 作业
- en: A small syntax error in the Reducer phase leads to a failure of the MapReduce
    job. After the failure of a Hadoop MapReduce job, we can track the problem from
    the Hadoop MapReduce administration page, where we can get information about running
    jobs as well as completed jobs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer 阶段的一个小语法错误会导致 MapReduce 作业失败。在 Hadoop MapReduce 作业失败后，我们可以通过 Hadoop
    MapReduce 管理页面跟踪问题，在该页面上可以获取关于正在运行的作业以及已完成作业的信息。
- en: In case of a failed job, we can see the total number of completed/failed Map
    and Reduce jobs. Clicking on the failed jobs will provide the reason for the failing
    of those particular number of Mappers or Reducers.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果作业失败，我们可以看到已完成/失败的 Map 和 Reduce 作业的总数。点击失败的作业将提供导致这些特定 Mappers 或 Reducers
    失败的原因。
- en: 'Also, we can check the real-time progress of that running MapReduce job with
    the JobTracker console as shown in the following screenshot:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们可以通过 JobTracker 控制台查看正在运行的 MapReduce 作业的实时进度，如下图所示：
- en: '![Monitoring the Hadoop MapReduce job](img/3282OS_04_07.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![监控 Hadoop MapReduce 作业](img/3282OS_04_07.jpg)'
- en: Monitoring Hadoop MapReduce job
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 监控 Hadoop MapReduce 作业
- en: 'Through the command, we can check the history of that particular MapReduce
    job by specifying its output directory with the following command:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通过命令，我们可以通过指定其输出目录来检查特定 MapReduce 作业的历史，使用以下命令：
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The following command will print the details of the MapReduce job, failed and
    reasons for killed up jobs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将打印 MapReduce 作业的详细信息、失败的作业以及被终止作业的原因。
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The preceding command will print about the successful task and the task attempts
    made for each task.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将打印每个任务的成功任务和任务尝试的详细信息。
- en: Exploring the HadoopStreaming R package
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 HadoopStreaming R 包
- en: HadoopStreaming is an R package developed by *David S. Rosenberg*. We can say
    this is a simple framework for MapReduce scripting. This also runs without Hadoop
    for operating data in a streaming fashion. We can consider this R package as a
    Hadoop MapReduce initiator. For any analyst or developer who is not able to recall
    the Hadoop streaming command to be passed in the command prompt, this package
    will be helpful to quickly run the Hadoop MapReduce job.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: HadoopStreaming 是一个由 *David S. Rosenberg* 开发的 R 包。我们可以说这是一个简单的 MapReduce 脚本框架。它还可以在没有
    Hadoop 的情况下以流式方式操作数据。我们可以将这个 R 包看作是 Hadoop MapReduce 的启动器。对于任何无法回忆起 Hadoop 流式命令的分析师或开发者，这个包将有助于快速运行
    Hadoop MapReduce 作业。
- en: 'The three main features of this package are as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该包的三个主要功能如下：
- en: 'Chunkwise data reading: The package allows chunkwise data reading and writing
    for Hadoop streaming. This feature will overcome memory issues.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分块数据读取：该包允许分块数据读取和写入 Hadoop 流式处理。此功能将解决内存问题。
- en: 'Supports various data formats: The package allows the reading and writing of
    data in three different data formats.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持多种数据格式：该软件包允许以三种不同的数据格式读取和写入数据。
- en: 'Robust utility for the Hadoop streaming command: The package also allows users
    to specify the command-line argument for Hadoop streaming.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对Hadoop流式命令的强大工具：该软件包还允许用户为Hadoop流式命令指定命令行参数。
- en: 'This package is mainly designed with three functions for reading the data efficiently:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本软件包主要设计了三个功能，以高效读取数据：
- en: '`hsTableReader`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hsTableReader`'
- en: '`hsKeyValReader`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hsKeyValReader`'
- en: '`hsLineReader`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hsLineReader`'
- en: Now, let's understand these functions and their use cases. After that we will
    understand these functions with the help of the word count MapReduce job.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解这些功能及其用例。之后，我们将通过单词计数的MapReduce作业来理解这些功能。
- en: Understanding the hsTableReader function
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解`hsTableReader`函数
- en: The `hsTableReader` function is designed for reading data in the table format.
    This function assumes that there is an input connection established with the file,
    so it will retrieve the entire row. It assumes that all the rows with the same
    keys are stored consecutively in the input ﬁle.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`hsTableReader`函数设计用于读取表格格式的数据。该函数假设已经与文件建立了输入连接，因此它将检索整行数据。它假设所有具有相同键的行都连续存储在输入文件中。'
- en: As the Hadoop streaming job guarantees that the output rows of Mappers will
    be sorted before providing to the reducers, there is no need to use the `sort`
    function in a Hadoop streaming MapReduce job. When we are not running this over
    Hadoop, we explicitly need to call the `sort` function after the `Mapper` function
    gets execute.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Hadoop流式作业保证在提供给Reducer之前，Mapper的输出行会被排序，因此在Hadoop流式MapReduce作业中不需要使用`sort`函数。当我们不在Hadoop上运行时，必须在`Mapper`函数执行后显式调用`sort`函数。
- en: 'Defining a function of `hsTableReader`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`hsTableReader`函数：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The terms in the preceding code are as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中的术语如下：
- en: '`file`: This is a connection object, stream, or string.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`file`：这是一个连接对象、流或字符串。'
- en: '`chunkSize`: This indicates the maximum number of lines to be read at a time
    by the function. `-1` means all the lines at a time.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunkSize`：指示函数每次读取的最大行数。`-1`表示一次读取所有行。'
- en: '`cols`: This means a list of column names as "what" argument to scan.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cols`：表示作为“what”参数扫描的列名列表。'
- en: '`skip`: This is used to skip the first n data rows.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip`：用于跳过前n行数据。'
- en: '`FUN`: This function will use the data entered by the user.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FUN`：此函数将使用用户输入的数据。'
- en: '`carryMemLimit`: This indicates the maximum memory limit for the values of
    a single key.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`carryMemLimit`：指示单个键值的最大内存限制。'
- en: '`carryMaxRows`: This indicates the maximum rows to be considered or read from
    the file.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`carryMaxRows`：指示要考虑或读取的最大行数。'
- en: '`stringsAsFactors`: This defines whether the strings are converted to factors
    or not (`TRUE` or `FALSE`).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stringsAsFactors`：定义是否将字符串转换为因子（`TRUE` 或 `FALSE`）。'
- en: 'For example, data in file:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，文件中的数据：
- en: '[PRE28]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output for the preceding code is as shown in the following screenshot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下图所示：
- en: '![Understanding the hsTableReader function](img/3282OS_04_08.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![理解`hsTableReader`函数](img/3282OS_04_08.jpg)'
- en: 'The data read by `hsTableReader` is as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`hsTableReader`读取的数据如下：'
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output for the preceding code is as shown in the following screenshot:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下图所示：
- en: '![Understanding the hsTableReader function](img/3282OS_04_09.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![理解`hsTableReader`函数](img/3282OS_04_09.jpg)'
- en: Understanding the hsKeyValReader function
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解`hsKeyValReader`函数
- en: The `hsKeyValReader` function is designed for reading the data available in
    the key-value pair format. This function also uses `chunkSize` for defining the
    number of lines to be read at a time, and each line consists of a key string and
    a value string.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`hsKeyValReader`函数设计用于读取以键值对格式存储的数据。该函数还使用`chunkSize`定义一次读取的行数，每行由键字符串和值字符串组成。'
- en: '[PRE30]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The terms of this function are similar to `hsTablereader()`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的术语与`hsTablereader()`类似。
- en: 'Example:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE31]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output for the preceding code is as shown in the following screenshot:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下图所示：
- en: '![Understanding the hsKeyValReader function](img/3282OS_04_10.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![理解`hsKeyValReader`函数](img/3282OS_04_10.jpg)'
- en: Understanding the hsLineReader function
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解`hsLineReader`函数
- en: The `hsLineReader` function is designed for reading the entire line as a string
    without performing the data-parsing operation. It repeatedly reads the `chunkSize`
    lines of data from the file and passes a character vector of these strings to
    `FUN`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`hsLineReader` 函数设计用于将整行读取为字符串，而不执行数据解析操作。它反复从文件中读取 `chunkSize` 行数据，并将这些字符串的字符向量传递给
    `FUN`。'
- en: '[PRE32]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The terms of this function are similar to `hsTablereader()`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的术语与 `hsTablereader()` 相似。
- en: 'Example:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE33]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output for the preceding code is as shown in the following screenshot:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下图所示：
- en: '![Understanding the hsLineReader function](img/3282OS_04_11.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![理解 hsLineReader 函数](img/3282OS_04_11.jpg)'
- en: You can get more information on these methods as well as other existing methods
    at [http://cran.r-project.org/web/packages/HadoopStreaming/HadoopStreaming.pdf](http://cran.r-project.org/web/packages/HadoopStreaming/HadoopStreaming.pdf).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[http://cran.r-project.org/web/packages/HadoopStreaming/HadoopStreaming.pdf](http://cran.r-project.org/web/packages/HadoopStreaming/HadoopStreaming.pdf)上获取关于这些方法以及其他现有方法的更多信息。
- en: Now, we will implement the above data-reading methods with the Hadoop MapReduce
    program to be run over Hadoop. In some of the cases, the key-values pairs or data
    rows will not be fed in the machine memory; so reading that data chunk wise will
    be more appropriate than improving the machine configuration.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 Hadoop MapReduce 程序实现上述数据读取方法，并在 Hadoop 上运行。在某些情况下，键值对或数据行不会被加载到机器内存中，因此逐块读取数据比改进机器配置更为合适。
- en: 'Problem definition:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 问题定义：
- en: 'Hadoop word count: As we already know what a word count application is, we
    will implement the above given methods with the concept of word count. This R
    script has been reproduced here from the HadoopStreaming R package, which can
    be downloaded along with the HadoopStreaming R library distribution as the sample
    code.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 词频统计：由于我们已经知道什么是词频统计应用，我们将使用上述方法来实现词频统计的概念。这个 R 脚本已从 HadoopStreaming
    R 包中复制，这个包可以和 HadoopStreaming R 库一起作为示例代码下载。
- en: 'Input dataset: This has been taken from [Chapter 1](ch01.html "Chapter 1. Getting
    Ready to Use R and Hadoop") of *Anna Karenina* (novel) by the Russian writer *Leo
    Tolstoy*.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集：该数据集取自俄罗斯作家 *列夫·托尔斯泰* 的小说 *安娜·卡列尼娜* [第一章](ch01.html "Chapter 1. Getting
    Ready to Use R and Hadoop")。
- en: 'R script: This section contains the code of the Mapper, Reducer, and the rest
    of the configuration parameters.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: R 脚本：本节包含了 Mapper、Reducer 及其他配置参数的代码。
- en: 'File: `hsWordCnt.R`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 文件：`hsWordCnt.R`
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Running a Hadoop streaming job
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 Hadoop 流式作业
- en: Since this is a Hadoop streaming job, it will run same as the executed previous
    example of a Hadoop streaming job. For this example, we will use a shell script
    to execute the `runHadoop.sh` file to run Hadoop streaming.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个 Hadoop 流式作业，它将像之前执行的 Hadoop 流式作业一样运行。对于这个例子，我们将使用一个 shell 脚本来执行 `runHadoop.sh`
    文件以运行 Hadoop 流式作业。
- en: 'Setting up the system environment variable:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 设置系统环境变量：
- en: '[PRE35]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Setting up the MapReduce job parameters:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 MapReduce 作业参数：
- en: '[PRE36]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Removing the existing output directory:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 删除现有输出目录：
- en: '[PRE37]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Designing the Hadoop MapReduce command with generic and streaming options:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 设计具有通用选项和流式选项的 Hadoop MapReduce 命令：
- en: '[PRE38]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Extracting the output from HDFS to the local directory:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 从 HDFS 提取输出到本地目录：
- en: '[PRE39]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Executing the Hadoop streaming job
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行 Hadoop 流式作业
- en: We can now execute the Hadoop streaming job by executing the command, `runHadoop.sh`.
    To execute this, we need to set the user permission.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过执行命令 `runHadoop.sh` 来执行 Hadoop 流式作业。为了执行此操作，我们需要设置用户权限。
- en: '[PRE40]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Executing via the following command:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下命令执行：
- en: '[PRE41]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Finally, it will execute the whole Hadoop streaming job and then copy the output
    to the local directory.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它将执行整个 Hadoop 流式作业，然后将输出复制到本地目录。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have learned most of the ways to integrate R and Hadoop for performing data
    operations. In the next chapter, we will learn about the data analytics cycle
    for solving real world data analytics problems with the help of R and Hadoop.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了大部分将 R 与 Hadoop 集成以执行数据操作的方法。在下一章中，我们将学习如何利用 R 和 Hadoop 解决现实世界的数据分析问题。
