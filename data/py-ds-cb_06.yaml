- en: Chapter 6. Machine Learning 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章 机器学习 1
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Preparing data for model building
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模型构建准备数据
- en: Finding the nearest neighbors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找最近邻
- en: Classifying documents using Naïve Bayes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯对文档进行分类
- en: Building decision trees to solve multiclass problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建决策树解决多类别问题
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we will look at supervised learning techniques. In the previous
    chapter, we saw unsupervised techniques including clustering and learning vector
    quantization. We will start with a classification problem and then proceed to
    regression. The input for a classification problem is a set of records or instances
    in the next chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探讨监督学习技术。在上一章中，我们介绍了包括聚类和学习向量量化在内的无监督技术。我们将从一个分类问题开始，然后转向回归问题。分类问题的输入是在下一章中的一组记录或实例。
- en: Each record or instance can be written as a set (X,y), where X is a set of attributes
    and y is a corresponding class label.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 每条记录或实例可以写作一个集合（X，y），其中X是属性集，y是对应的类别标签。
- en: Learning a target function, F, that maps each record's attribute set to one
    of the predefined class label, y, is the job of a classification algorithm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 学习一个目标函数F，该函数将每条记录的属性集映射到预定义的类别标签y，是分类算法的任务。
- en: 'The general steps for a classification algorithm are as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分类算法的一般步骤如下：
- en: Find an appropriate algorithm
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 寻找合适的算法
- en: Learn a model using a training set, and validate the model using a test set
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练集学习模型，并使用测试集验证模型
- en: Apply the model to predict any unseen instance or record
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用模型预测任何未见过的实例或记录
- en: 'The first step is to identify the right classification algorithm. There is
    no prescribed way of choosing the right algorithm, it comes from repeated trial
    and error. After choosing the algorithm, a training and a test set is created,
    which is provided to the algorithm to learn a model, that is, a target function
    F, as defined previously. After creating the model using a training set, a test
    set is used to validate the model. Usually, we use a confusion matrix to validate
    the model. We will discuss more about confusion matrices in our recipe: Finding
    the nearest neighbors.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是确定正确的分类算法。没有固定的方法来选择合适的算法，这需要通过反复的试错过程来完成。选择算法后，创建训练集和测试集，并将其提供给算法，以学习一个模型，也就是前面定义的目标函数F。通过训练集创建模型后，使用测试集来验证模型。通常，我们使用混淆矩阵来验证模型。我们将在我们的食谱中进一步讨论混淆矩阵：查找最近邻。
- en: We will begin with a recipe that will show us how to divide our input dataset
    into training and test sets. We will follow this with a lazy learner algorithm
    for classification, called K-Nearest Neighbor. We will then look at Naïve Bayes
    classifiers. We will venture into a recipe that deals with multiclass problems
    using decision trees. Our choice of algorithms in this chapter is not random.
    All the three algorithms that we will cover in this chapter are capable of handling
    multiclass problems, in addition to binary problems. In multiclass problems, we
    have more than two class labels to which the instances can belong.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个食谱开始，展示如何将输入数据集划分为训练集和测试集。接下来我们将介绍一种懒惰学习者算法用于分类，称为K-最近邻算法。然后我们将看一下朴素贝叶斯分类器。接下来我们将探讨一个使用决策树处理多类别问题的食谱。本章我们选择的算法并非随意选择。我们将在本章中介绍的三种算法都能够处理多类别问题，除了二分类问题。在多类别问题中，我们有多个类别标签，实例可以属于这些类别中的任意一个。
- en: Preparing data for model building
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为模型构建准备数据
- en: 'In this recipe, we will look at how to create a train and a test dataset from
    the given dataset for the classification problem. A test dataset is never shown
    to the model. In real-world scenarios, we typically build another dataset called
    dev. Dev stands for development dataset: a dataset that we can use to continuously
    tune our model during successive runs. The model is trained using the train set,
    and model performance metrics such as accuracy are measured in dev. Based on this
    result, the model is further tuned in case improvements are required. In later
    chapters, we will cover recipes that can do more sophisticated data splitting
    than just a simple train test split.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将展示如何从给定的数据集中创建训练集和测试集用于分类问题。测试数据集永远不会被展示给模型。在实际应用中，我们通常会建立另一个名为dev的数据集。Dev代表开发数据集：我们可以用它在模型的连续运行过程中持续调优模型。模型使用训练集进行训练，模型的性能度量，如准确率，则在dev数据集上进行评估。根据这些结果，如果需要改进，模型将进一步调优。在后续章节中，我们将介绍一些比简单的训练测试集拆分更复杂的数据拆分方法。
- en: Getting ready
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the Iris dataset for this recipe. It's easy to demonstrate the concept
    with this dataset as we are familiar with it because we have used it for many
    of our previous recipes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用鸢尾花数据集来进行这个示例。由于我们已经在许多之前的示例中使用过它，所以使用这个数据集展示概念非常简单。
- en: How to do it…
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This was pretty simple. Let''s see if the class labels are proportionately
    distributed between the training and the test sets. This is a typical class imbalance
    problem:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单。让我们检查一下类别标签是否在训练集和测试集之间按比例分配。这是一个典型的类别不平衡问题：
- en: 'def get_class_distribution(y):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'def get_class_distribution(y):'
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s see how we distribute the class labels uniformly between the train and
    the test sets:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将类别标签均匀分配到训练集和测试集中：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works…
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作…
- en: After we import the necessary library modules, we must write a convenient function,
    `get_iris_data()`, which will return the Iris dataset. We then column concatenate
    the `x` and `y` arrays into a single array called `input_dataset`. We then shuffle
    the dataset so that the records can be distributed randomly to the test and the
    train datasets. The function returns a single array of both the instances and
    the class labels.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们导入必要的库模块后，必须编写一个方便的函数`get_iris_data()`，该函数将返回鸢尾花数据集。然后，我们将`x`和`y`数组按列连接成一个名为`input_dataset`的单一数组。接着，我们对数据集进行打乱，以便记录能够随机分配到测试集和训练集。该函数返回一个包含实例和类别标签的单一数组。
- en: We want to include 80 percent of the record in our training dataset, and use
    the remaining as our test dataset. The `train_size` and `test_size` variables
    hold a percentage of the values, which should be in the training and testing dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将80%的记录包含在训练数据集中，剩余的用作测试数据集。`train_size`和`test_size`变量分别表示应该分配到训练集和测试集中的百分比值。
- en: We must call the `get_iris_data()` function in order to get the input data.
    We then leverage the `train_test_split` function from scikit-learn's `cross_validation`
    model to split the input dataset into two.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须调用`get_iris_data()`函数以获取输入数据。然后，我们利用scikit-learn的`cross_validation`模块中的`train_test_split`函数将输入数据集拆分成两部分。
- en: 'Finally, we can print the size of the original dataset, followed by the test
    and the train datasets:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以打印原始数据集的大小，接着是测试集和训练集的大小：
- en: '![How it works…](img/B04041_06_04.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作…](img/B04041_06_04.jpg)'
- en: Our original dataset has 150 rows and five columns. Remember that there are
    only four attributes; the fifth column is the class label. We had column concatenated
    x and y.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的原始数据集有150行和五列。记住，只有四个属性；第五列是类别标签。我们已经将`x`和`y`按列连接。
- en: As you can see, 80 percent of the 150 rows, that is, 120 records, have been
    assigned to our training set. We have shown how we can easily split our input
    data into the train and the test sets.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，150行数据中的80%（即120条记录）已被分配到我们的训练集中。我们已经展示了如何轻松地将输入数据拆分为训练集和测试集。
- en: Remember this is a classification problem. The algorithm should be trained to
    predict the correct class label for a given unknown instance or record. For this,
    we need to provide the algorithm and an equal distribution of all the classes
    during training. The Iris dataset is a three-class problem. We should have equal
    representation from all the three classes. Let's see if our method has taken care
    of this.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这是一个分类问题。算法应该被训练来预测给定未知实例或记录的正确类别标签。为此，我们需要在训练期间为算法提供所有类别的均匀分布。鸢尾花数据集是一个三类问题，我们应该确保所有三类都有相等的代表性。让我们看看我们的方法是否已经解决了这个问题。
- en: We must define a function called `get_class_distribution`, which takes a single
    `y` parameter's array of class labels. This function returns a dictionary, where
    the key is the class label and the value is a percentage of the number of records
    for this distribution. Thus, this dictionary gives us the distribution of the
    class labels. We must call this function in the following function to get to know
    what our class distribution is in the train and the test datasets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须定义一个名为`get_class_distribution`的函数，该函数接受一个单一的`y`参数（类别标签数组）。此函数返回一个字典，其中键是类别标签，值是该分布记录的百分比。因此，字典提供了类别标签的分布情况。我们必须在以下函数中调用此函数，以了解训练集和测试集中的类别分布。
- en: The `print_class_label_split` function is self-explanatory. We must pass the
    train and the test datasets as the argument. As we have concatenated our `x` and
    `y`, the last column is our class label. We then extract the train and test class
    labels in `y_train` and `y_test`. We pass them to `get_class_distribution` to
    get a dictionary of the class labels and their distribution, and finally, we print
    it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`print_class_label_split`函数不言自明。我们必须将训练集和测试集作为参数传递。由于我们已经将`x`和`y`连接在一起，最后一列就是我们的类别标签。然后，我们提取`y_train`和`y_test`中的训练集和测试集类别标签。我们将它们传递给`get_class_distribution`函数，以获取类别标签及其分布的字典，最后打印出来。'
- en: 'We can then finally invoke `print_class_label_split`, and our output should
    look as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以调用`print_class_label_split`，我们的输出应该如下所示：
- en: '![How it works…](img/B04041_06_05.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_06_05.jpg)'
- en: Let's now examine the output. As you can see, our training set has a different
    distribution of the class labels compared with the test set. Exactly 40 percent
    of the instances in the test set belong to `class label 1`. This is not the right
    way to do the split. We should have an equal distribution in both the training
    and the test datasets.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们查看输出结果。如您所见，我们的训练集和测试集中的类别标签分布不同。测试集中正好40%的实例属于`class label 1`。这不是正确的分割方式。我们应该在训练集和测试集之间拥有相等的分布。
- en: 'In the final piece of code, we leverage `StratifiedShuffleSplit` from scikit-learn
    in order to achieve equal class distribution in the training and the test sets.
    Let''s examine the parameters of `StratifiedShuffleSplit`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一段代码中，我们使用了来自scikit-learn的`StratifiedShuffleSplit`，以实现训练集和测试集中类别的均匀分布。让我们查看`StratifiedShuffleSplit`的参数：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The first parameter is the input dataset. We pass all the rows and the last
    column. Our test size is defined by the `test_size` variable, which we had initially
    declared. We can assume that we need only one split using the `n_iter` variable.
    We then proceed to invoke `print_class_label_split` to print the class label distribution.
    Let''s examine the output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是输入数据集。我们传递所有行和最后一列。我们的测试集大小由我们最初声明的`test_size`变量定义。我们可以假设仅使用`n_iter`变量进行一次分割。然后，我们继续调用`print_class_label_split`函数打印类别标签的分布情况。让我们查看输出结果：
- en: '![How it works…](img/B04041_06_06.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_06_06.jpg)'
- en: Now, we have the class labels distributed uniformly between the test and train
    sets.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经在测试集和训练集之间均匀分布了类别标签。
- en: There's more...
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We need to prepare the data carefully before its use in a machine learning algorithm.
    Providing a uniform class distribution to both the train and the test sets is
    key to building a successful classification model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用机器学习算法之前，我们需要仔细准备数据。为训练集和测试集提供均匀的类别分布是构建成功分类模型的关键。
- en: In practical machine learning scenarios, we create another dataset called as
    dev set in addition to the train and test sets. We may not get our model right
    in the first iteration. We don't want to show our test dataset to our model as
    this may bias our next iteration of model building. Hence, we create this dev
    set, which we can use as we iterate through our model building exercise.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际的机器学习场景中，除了训练集和测试集外，我们还会创建另一个称为开发集（dev set）的数据集。我们可能在第一次迭代中无法正确构建模型。我们不希望将测试数据集展示给我们的模型，因为这可能会影响我们下一次模型构建的结果。因此，我们创建了这个开发集，可以在迭代模型构建过程中使用。
- en: The 80/20 rule of thumb that we specified in this recipe is an ideal scenario.
    However, in many practical applications, we may not have enough data to leave
    out that many instances for a test set. There are a few practical techniques,
    such as cross-validation, which come into play in such scenarios. In our next
    chapter, we will look at the various cross-validation techniques.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中指定的80/20法则是一个理想化的场景。然而，在许多实际应用中，我们可能没有足够的数据来留下如此多的实例作为测试集。在这种情况下，一些实用技术，如交叉验证，便派上用场。在下一章中，我们将探讨各种交叉验证技术。
- en: Finding the nearest neighbors
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找最近邻
- en: Before we jump into our recipe, let's spend some time understanding how to check
    if our classification model is performing to our satisfaction. In the introduction
    section, we introduced a term called confusion matrix.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入具体步骤之前，让我们先花些时间了解如何检查我们的分类模型是否达到预期的表现。在介绍部分，我们提到过一个术语叫做混淆矩阵。
- en: 'A confusion matrix is a matrix arrangement of the actual versus the predicted
    class labels. Let''s say we have a two-class problem, that is, our `y` can take
    either value, `T` or `F`. Let''s say we trained a classifier to predict our `y`.
    In our test data, we know the actual value of `y`. We have the predicted value
    of our `y` from our model. w0 values we can fill our confusion matrix, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是实际标签与预测标签的矩阵排列。假设我们有一个二分类问题，即`y`可以取值为`T`或`F`。假设我们训练了一个分类器来预测`y`。在我们的测试数据中，我们知道`y`的实际值，同时我们也有模型预测的`y`值。我们可以按照以下方式填充混淆矩阵：
- en: '![Finding the nearest neighbors](img/B04041_06_02.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![查找最近邻](img/B04041_06_02.jpg)'
- en: Here is a table where we conveniently list our results from the test set. Remember
    that we know the class labels in our test set; hence, we can compare our classification
    model output with the actual class label.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个表格，我们方便地列出了来自测试集的结果。记住，我们知道测试集中的类别标签，因此我们可以将分类模型的输出与实际类别标签进行比较。
- en: Under `TP`, which is an abbreviation for True Positive, we have a count of all
    those records in the test set whose label is `T`, and where the model also predicted
    `T`
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`TP`（真正例）下，我们统计的是测试集中所有标签为`T`的记录，且模型也预测为`T`的记录数。
- en: Under `FN`, which is an abbreviation for False Negative, we have a count of
    all the records whose actual label is `T`, but the algorithm predicted N
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`FN`（假阴性）的情况下，我们统计的是所有实际标签为`T`，但算法预测为N的记录数。
- en: '`FP` stands for False Positive, where the actual label is `F`, but the algorithm
    predicted it as `T`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FP`表示假阳性，即实际标签为`F`，但算法预测为`T`'
- en: '`TN` stands for True Negative, where the algorithm predicted both the label
    and the actual class label as `F`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TN`表示真阴性，即算法预测的标签和实际类别标签都为`F`'
- en: With the knowledge of this confusion matrix, we can now derive performance metrics
    with which we can measure the quality of our classification model. In future chapters,
    we will explore more metrics, but for now, we will introduce the accuracy and
    error rate.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解这个混淆矩阵，我们现在可以推导出一些性能指标，用于衡量我们分类模型的质量。在未来的章节中，我们将探索更多的指标，但现在我们将介绍准确率和错误率。
- en: 'Accuracy is defined as the ratio of a correct prediction to the total number
    of predictions. From the confusion matrix, we know that the sum of TP and TN is
    the total number of correct predictions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率定义为正确预测与总预测次数的比率。从混淆矩阵中，我们可以知道TP和TN的总和是正确预测的总数：
- en: '![Finding the nearest neighbors](img/B04041_06_03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![查找最近邻](img/B04041_06_03.jpg)'
- en: Accuracy from the training set is always very optimistic. One should look at
    the test set's accuracy value to determine the true performance of the model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练集得出的准确率通常过于乐观。应该关注测试集上的准确率值，以确定模型的真实表现。
- en: 'Armed with this knowledge, let''s jump into our recipe. The first classification
    algorithm that we will look at is K-Nearest Neighbor, in short, KNN. Before going
    into the details of KNN, let''s look at a very simple classification algorithm,
    called the rote classifier algorithm. The rote classifier memorizes the entire
    training data, that is, it loads all the data in the memory. We need to perform
    classification on an unseen new training instance: it will try to match the new
    training instance with any of the training instances in the memory. It matches
    every attribute of the test instance with every attribute in the training instance.
    If it finds a match, it predicts the class label of the test instance as the class
    label of the matched training instance.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握了这些知识后，让我们进入我们的示例。我们首先要看的分类算法是 K-最近邻，简称 KNN。在详细讲解 KNN 之前，让我们先看看一个非常简单的分类算法，叫做死记硬背分类算法。死记硬背分类器会记住整个训练数据，也就是说，它将所有数据加载到内存中。当我们需要对一个新的训练实例进行分类时，它会尝试将新的训练实例与内存中的任何训练实例进行匹配。它会将测试实例的每个属性与训练实例中的每个属性进行匹配。如果找到匹配项，它会将测试实例的类别标签预测为匹配的训练实例的类别标签。
- en: You should know by now that this classifier will fail if the test instance is
    not similar to any of the training instances loaded into the memory.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该知道，如果测试实例与加载到内存中的任何训练实例不相似，这个分类器会失败。
- en: KNN is similar to rote classifier, except that instead of looking for an exact
    match, it uses a similarity measure. Similar to rote classifier, KNN loads all
    the training sets into the memory. When it needs to classify a test instance,
    it measures the distance between the test instance and all the training instances.
    Using this distance, it chooses K closest instances in the training set. Now,
    the prediction for the test set is based on the majority classes of the K nearest
    neighbors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 类似于死记硬背分类器，不同之处在于它不是寻找完全匹配，而是使用相似度度量。与死记硬背分类器类似，KNN 将所有训练集加载到内存中。当它需要对一个测试实例进行分类时，它会计算测试实例与所有训练实例之间的距离。利用这个距离，它会选择训练集中
    K 个最接近的实例。现在，测试集的预测是基于 K 个最近邻的多数类别。
- en: For example, if we have a two-class classification problem and we choose our
    K value as three, and if the given test record's three nearest neighbors have
    classes, 1, 1, and 0, it will classify the test instance as 1, which is the majority.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个二分类问题，并且选择我们的 K 值为3，那么如果给定的测试记录的三个最近邻类别分别为 1、1 和 0，它将把测试实例分类为1，也就是多数类别。
- en: KNN belongs to a family of algorithms called instance-based learning. Additionally,
    as the decision to classify a test instance is taken last, it's also called a
    lazy learner.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 属于一种叫做基于实例学习的算法家族。此外，由于对测试实例的分类决定是在最后做出的，因此它也被称为懒学习者。
- en: Getting ready
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备中
- en: 'For this recipe, we will generate some data using scikit''s make_classification
    method. We will generate a matrix of four columns / attributes / features and
    100 instances:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将使用 scikit 的 `make_classification` 方法生成一些数据。我们将生成一个包含四列/属性/特征和100个实例的矩阵：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `get_data` function internally calls `make_classification` to generate test
    data for any classification task.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_data` 函数内部调用 `make_classification` 来生成任何分类任务的测试数据。'
- en: 'It''s always good practice to visualize the data before starting to feed it
    into any algorithm. Our `plot_data` function produces a scatter plot between all
    the variables:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据输入任何算法之前，最好先对数据进行可视化。我们的 `plot_data` 函数会生成所有变量之间的散点图：
- en: '![Getting ready](img/B04041_06_07.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![准备中](img/B04041_06_07.jpg)'
- en: We have plotted all the variable combinations. The top two charts there in show
    combinations between the 0th and 1st column, followed by 0th and 2nd. The points
    are also colored by their class labels. This gives an idea of how much information
    is these variable combination to do a classification task.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经绘制了所有变量的组合。顶部的两个图表显示了第0列与第1列之间的组合，接着是第0列与第2列的组合。数据点也按其类别标签上色。这展示了这些变量组合在进行分类任务时提供了多少信息。
- en: How to do it…
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'We will separate our dataset preparation and model training into two different
    methods: `get_train_test` to get the train and test data, and `build_model` to
    build our model. Finally, we will use `test_model` to validate the usefulness
    of our model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集准备和模型训练分成两个不同的方法：`get_train_test` 用来获取训练和测试数据，`build_model` 用来构建模型。最后，我们将使用
    `test_model` 来验证我们模型的有效性：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works…
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何运作…
- en: Let's try to follow the code from our main method. We must start by calling
    `get_data` and plotting it using `plot_data`, as described in the previous section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试遵循主方法中的代码。我们必须从调用`get_data`并使用`plot_data`进行绘图开始，如前一部分所述。
- en: As mentioned previously, we need to separate a part of the training data for
    the testing that is required to evaluate our model. We then invoke the `get_train_test`
    method to do the same.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们需要将一部分训练数据分离出来用于测试，以评估我们的模型。然后，我们调用`get_train_test`方法来完成这一操作。
- en: In `get_train_test`, we decide our train test split size, which is the standard
    80/20\. We then use 80 percent of our data to train our model. Now, we combine
    both x and y to a single matrix before the split using NumPy's `column_stack`
    method.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在`get_train_test`中，我们决定了训练测试集的分割比例，标准为80/20。然后，我们使用80%的数据来训练我们的模型。现在，我们在拆分之前，使用NumPy的`column_stack`方法将x和y合并为一个矩阵。
- en: We then leverage `StratifiedShuffleSplit` discussed in the previous recipe in
    order to get a uniform class label distribution between our training and test
    sets.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们利用在前面配方中讨论的`StratifiedShuffleSplit`，以便在训练集和测试集之间获得均匀的类标签分布。
- en: Armed with our train and test sets, we are now ready to build our classifier.
    We must invoke the build model with our training set, attributes `x`, and class
    labels `y`. This function also takes `K`, the number of neighbors, as a parameter,
    with a default value of two.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借我们的训练集和测试集，我们现在准备好构建分类器了。我们必须使用我们的训练集、属性`x`和类标签`y`来调用构建模型函数。该函数还接收`K`作为参数，表示邻居的数量，默认值为二。
- en: We use scikit-learn's KNN implementation, `KNeighborsClassifier`. We then create
    an object of the classifier and call the `fit` method to build our model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了scikit-learn的KNN实现，`KNeighborsClassifier`。然后，我们创建分类器对象并调用`fit`方法来构建我们的模型。
- en: We are ready to test how good the model is using our training data. We can pass
    our training data (x and y) and our model to the `test_model` function.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好使用训练数据来测试模型的表现了。我们可以将训练数据（x和y）以及模型传递给`test_model`函数。
- en: We know our actual class labels (y). We then invoke the predict function with
    our x to get the predicted labels. We then print some of the model evaluation
    metrics. We can start with printing the accuracy of the model, follow it up with
    a confusion matrix, and then finally show the output of a function called `classification_report`.
    scikit-learn's metrics module provides a function called `classification_report`,
    which can print several model evaluation metrics.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们的实际类标签（y）。然后，我们调用预测函数，使用我们的x来获得预测标签。接着，我们打印出一些模型评估指标。我们可以从打印模型的准确率开始，接着是混淆矩阵，最后展示一个名为`classification_report`的函数的输出。scikit-learn的metrics模块提供了一个名为`classification_report`的函数，可以打印出多个模型评估指标。
- en: 'Let''s look at our model metrics:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们的模型指标：
- en: '![How it works…](img/B04041_06_08.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![How it works…](img/B04041_06_08.jpg)'
- en: As you can see, our accuracy score is 91.25 percent. We will not repeat the
    definition of accuracy; you can refer to the introduction section.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的准确率得分是91.25%。我们不会重复准确率的定义，你可以参考介绍部分。
- en: Let's now look at our confusion matrix. The top left cell is the true positive
    cell. We can see that we have no false negatives but we have seven false positives
    (the first cell in the 2nd row).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下混淆矩阵。左上角的单元格是正确正类单元格。我们可以看到没有假负类，但有七个假正类（第二行的第一个单元格）。
- en: 'Finally, we have precision, recall, an F1 score, and support in our classification
    report. Let''s look at their definitions:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在分类报告中得到了精确度、召回率、F1分数和支持度。让我们来看看它们的定义：
- en: '`Precision` is the ratio of the true positive and the sum of the true positive
    and false positive'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`精确度`是正确正类与正确正类和假正类之和的比率'
- en: '`Accuracy` is the ratio of the true positive and the sum of the true positive
    and false negative'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`准确率`是正确正类与正确正类和假负类之和的比率'
- en: An F1 score is the harmonic mean of precision and sensitivity
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数是精确度和敏感度的调和平均值
- en: We will see more about this metric in a separate recipe in a future chapter.
    For now, let's say we shall have high precision and recall values.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在未来章节的单独配方中更详细地讨论这个指标。现在，暂时可以说我们会有较高的精确度和召回率值。
- en: 'It is good to know that we have around 91 percent accuracy for our model, but
    the real test will be when it is run on the test data. Let''s see the metrics
    for our test data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 知道我们模型的准确率约为91%是好的，但真正的考验将是在测试数据上运行时。让我们看看测试数据的指标：
- en: '![How it works…](img/B04041_06_09.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![How it works…](img/B04041_06_09.jpg)'
- en: It is good to know that our model has 95 percent accuracy for the test data,
    which is an indication of a good job in fitting the model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 很高兴地知道，我们的模型在测试数据上的准确率为95%，这表明我们在拟合模型方面做得很好。
- en: There's more…
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'Let''s look a little bit deeper into the model that we have built:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解我们已经构建的模型：
- en: '![There''s more…](img/B04041_06_10.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![更多内容…](img/B04041_06_10.jpg)'
- en: We invoked a function called `get_params`. This function returns all the parameters
    that are passed to the model. Let's examine each of the parameters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用了一个名为`get_params`的函数。该函数返回所有传递给模型的参数。让我们来检查每个参数。
- en: The first parameter refers to the underlying data structure used by the KNN
    implementation. As every record in the training set has to be compared against
    every other record, brute force implementation may be compute-resource heavy.
    Hence, we can choose either `kd_tree` or `ball_tree` as the data structure. A
    brute will use the brute force method of looping through all the records for every
    record.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数指的是KNN实现所使用的底层数据结构。由于训练集中的每个记录都必须与其他所有记录进行比较，暴力实现可能会占用大量计算资源。因此，我们可以选择
    `kd_tree` 或 `ball_tree` 作为数据结构。暴力方法会对每个记录使用暴力法，通过循环遍历所有记录。
- en: Leaf size is the parameter that is passed to the `kd_tree` or `ball_tree` method.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 叶子大小是传递给 `kd_tree` 或 `ball_tree` 方法的参数。
- en: Metric is the distance measure used to find the neighbors. The p-value of two
    reduces Minkowski to Euclidean distance.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 度量是用于寻找邻居的距离度量方法。两者的p值将 Minkowski 距离简化为欧几里得距离。
- en: Finally, we have the weights parameter. KNN decides the class label of the test
    instance based on the class label of its K nearest neighbors. A majority vote
    decides the class label for the test instance. However, if we set the weights
    to distance, then each neighbor is given a weight that is inversely proportional
    to its distance. Hence, in order to decide the class label of a test set, weighted
    voting is performed, rather than simple voting.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有了权重参数。KNN根据其K个最近邻的类别标签来决定测试实例的类别标签。多数投票决定测试实例的类别标签。但是，如果我们将权重设置为距离，那么每个邻居会被赋予一个与其距离成反比的权重。因此，在决定测试集的类别标签时，进行的是加权投票，而不是简单投票。
- en: See also
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: '*Preparing data for model building* recipe in [Chapter 6](ch06.xhtml "Chapter 6. Machine
    Learning 1"), *Machine Learning I*'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为模型构建准备数据* 这一方法在[第6章](ch06.xhtml "第6章 机器学习I")，*机器学习I*中有提到。'
- en: '*Working with distance measures* recipe in [Chapter 5](ch05.xhtml "Chapter 5. Data
    Mining – Needle in a Haystack"), *Data Mining - Finding a needle in a haystack*'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*距离度量法工作* 这一方法在[第5章](ch05.xhtml "第5章 数据挖掘——大海捞针")，*数据挖掘——大海捞针*中有提到。'
- en: Classifying documents using Naïve Bayes
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类文档
- en: 'We will look at a document classification problem in this recipe. The algorithm
    that we will use is the Naïve Bayes classifier. The Bayes'' rule is the engine
    powering the Naïve Bayes algorithm, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们将查看一个文档分类问题。我们将使用的算法是朴素贝叶斯分类器。贝叶斯定理是驱动朴素贝叶斯算法的引擎，如下所示：
- en: '![Classifying documents using Naïve Bayes](img/B04041_06_11.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类文档](img/B04041_06_11.jpg)'
- en: 'It shows how likely it is for the event X to happen, given that we know event
    Y has already happened. Now, in our recipe, we will categorize or classify the
    text. Ours is a binary classification problem: given a movie review, we want to
    classify if the review is positive or negative.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 它展示了事件X发生的可能性，前提是我们已经知道事件Y已经发生。在我们的方案中，我们将对文本进行分类或分组。我们的分类问题是二分类问题：给定一条电影评论，我们想要分类评论是正面还是负面。
- en: 'In Bayesian terminology, we need to find the conditional probability: the probability
    that the review is positive given the review, and the probability that the review
    is negative given the review. Let''s write it as an equation:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯术语中，我们需要找到条件概率：给定评论，评论为正的概率，和评论为负的概率。我们可以将其表示为一个方程：
- en: '![Classifying documents using Naïve Bayes](img/B04041_06_12.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类文档](img/B04041_06_12.jpg)'
- en: For any review, if we have the preceding two probability values, we can classify
    the review as positive or negative by comparing these values. If the conditional
    probability for negative is greater than the conditional probability for positive,
    we classify the review as negative, and vice versa.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何评论，如果我们有前面的两个概率值，我们可以通过比较这些值将评论分类为正面或负面。如果负面的条件概率大于正面的条件概率，我们将评论分类为负面，反之亦然。
- en: 'Let''s now discuss these probabilities using Bayes'' rule:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过贝叶斯定理来讨论这些概率：
- en: '![Classifying documents using Naïve Bayes](img/B04041_06_13.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类文档](img/B04041_06_13.jpg)'
- en: As we are going to compare these two equations to finalize our prediction, we
    can ignore the denominator, which is a simple scaling factor.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将比较这两个方程来最终确定预测结果，我们可以忽略分母，因为它只是一个简单的缩放因子。
- en: The LHS (left-hand side) of the preceding equation is called the posterior probability.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程的左侧（LHS）称为后验概率。
- en: 'Let''s look at the numerator of the RHS (right-hand side):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下右侧（RHS）的分子部分：
- en: '![Classifying documents using Naïve Bayes](img/B04041_06_14.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类文档](img/B04041_06_14.jpg)'
- en: '`P(positive)` is the probability of a positive class called the prior. It''s
    our belief about the positive class label distribution based on our training set.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`P(positive)` 是正类的概率，称为先验概率。这是我们基于训练集对正类标签分布的信念。'
- en: 'We will estimate it from our training test. It''s calculated as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从我们的训练测试中进行估计。计算公式如下：
- en: '![Classifying documents using Naïve Bayes](img/B04041_06_15.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类文档](img/B04041_06_15.jpg)'
- en: 'P(review|positive) is the likelihood. It answers the question: what is the
    likelihood of getting the review, given that the class is positive. Again, we
    will estimate it from our training set.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: P(review|positive) 是似然度。它回答了这样一个问题：在给定类别为正类的情况下，获得该评论的可能性是多少。同样，我们将从我们的训练集中进行估计。
- en: Before we expand on the likelihood equation further, let's introduce the concept
    of independence assumption. The algorithm is prefixed as naïve because of this
    assumption. Contrary to the reality, we assume that the words appear in a document
    independent of each other. We will use this assumption to calculate the likelihood.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步展开似然度方程之前，让我们引入独立性假设的概念。由于这个假设，该算法被称为朴素的。与实际情况相反，我们假设文档中的词是相互独立的。我们将利用这个假设来计算似然度。
- en: 'A review is a list of words. Let''s put it in mathematical notation:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一条评论是一个单词列表。我们可以将其用数学符号表示如下：
- en: '![Classifying documents using Naïve Bayes](img/B04041_06_16.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类文档](img/B04041_06_16.jpg)'
- en: With the independence assumption, we can say that the probability of each of
    these words occurring together in a review is the product of all the individual
    probabilities of the constituent words in the review.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 基于独立性假设，我们可以说，这些词在评论中共同出现的概率是评论中各个单词的单独概率的乘积。
- en: 'Now we can write the likelihood equation as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将似然度方程写成如下形式：
- en: '![Classifying documents using Naïve Bayes](img/B04041_06_17.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类文档](img/B04041_06_17.jpg)'
- en: So, given a new review, we can use these two equations, the prior and likelihood,
    to calculate whether the review is positive or negative.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，给定一个新的评论，我们可以利用这两个方程——先验概率和似然度，来计算该评论是正面的还是负面的。
- en: 'Hopefully, you have followed till now. There is still a one last piece to the
    puzzle: how do we calculate the probability for the individual words?'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你到现在为止都跟上了。现在还有最后一块拼图：我们如何计算每个单词的概率？
- en: '![Classifying documents using Naïve Bayes](img/B04041_06_18.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类文档](img/B04041_06_18.jpg)'
- en: This step refers to training the model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步是指训练模型。
- en: From our training set, we will take each review. We also know its label. For
    each word in this review, we will calculate the conditional probability and store
    it in a table. We can thus use these values to predict any future test instance.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的训练集出发，我们将取出每个评论，并且我们也知道它的标签。对于评论中的每一个词，我们将计算条件概率并将其存储在表格中。这样，我们就可以利用这些值来预测未来的测试实例。
- en: Enough theory! Let's dive into our recipe.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 理论够了！让我们深入了解我们的步骤。
- en: Getting ready
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we will use the NLTK library for both the data and the algorithm.
    During the installation of NLTK, we can also download the datasets. One such dataset
    is the movie review dataset. The movie review data is segregated into two categories,
    positive and negative. For each category, we have a list of words; the reviews
    are preseparated into words:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个步骤，我们将使用 NLTK 库来处理数据和算法。在安装 NLTK 时，我们也可以下载数据集。一个这样的数据集是电影评论数据集。电影评论数据集被分为两类，正面和负面。对于每个类别，我们都有一个词列表；评论已经预先分割成单词：
- en: '[PRE6]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As shown here, we will include the datasets by importing the corpus module from
    NLTK.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如此处所示，我们将通过从 NLTK 导入语料库模块来包括数据集。
- en: We will leverage the `NaiveBayesClassifier` class, defined in NLTK, to build
    the model. We will pass our training data to a function called `train()` to build
    our model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用 `NaiveBayesClassifier` 类，这个类定义在 NLTK 中，用来构建模型。我们将把我们的训练数据传递给一个名为 `train()`
    的函数，以建立我们的模型。
- en: How to do it…
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Let''s start with importing the necessary function. We will follow it up with
    two utility functions. The first one retrieves the movie review data and the second
    one helps us split our data for the model into training and testing:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入必要的函数开始。接下来我们将导入两个工具函数。第一个函数用来获取电影评论数据，第二个函数帮助我们将数据划分为训练集和测试集：
- en: '[PRE7]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will now introduce three functions, which are primarily feature-generating
    functions. We need to provide features or attributes to our classifier. These
    functions, given a review, generate a set of features from the review:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将介绍三个函数，这些函数主要用于特征生成。我们需要为分类器提供特征或属性。这些函数根据评论生成一组特征：
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s now write a function to build our model and later probe our model to
    find the usefulness of our model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写一个函数来构建我们的模型，并稍后检查我们的模型，以评估其有效性：
- en: '[PRE9]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It is very hard to get the model right at the first pass. We need to play around
    with different features, and parameter tuning. This is mostly a trial and error
    process. In the next section of code, we will show our different passes by improving
    our model:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次尝试时，很难将模型调整到最佳状态。我们需要尝试不同的特征和参数调整。这基本上是一个反复试验的过程。在下一节代码中，我们将通过改进模型来展示我们的不同尝试：
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we will write a code with which we can invoke all our functions that
    were defined previously:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写一段代码，以便调用我们之前定义的所有函数：
- en: '[PRE11]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works…
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Let''s try to follow this recipe from the main function. We started with invoking
    the `get_data` function. As explained before, the movie review data is stored
    as two categories, positive and negative. Our first loop goes through these categories.
    With these categories, we retrieved the file IDs for these categories in the second
    loop. Using these file IDs, we retrieve the words, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试按照主函数中的步骤来执行这个过程。我们从调用`get_data`函数开始。如前所述，电影评论数据被存储为两类：正面和负面。我们的第一个循环遍历这些类别。在第二个循环中，我们使用这些类别来检索文件ID。使用这些文件ID，我们进一步获取单词，如下所示：
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We will append these words to a list called `dataset`. The class label is appended
    to another list called `y_labels`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些单词附加到一个名为`dataset`的列表中。类别标签将附加到另一个名为`y_labels`的列表中。
- en: 'Finally, we return the words and corresponding class labels:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们返回单词及其相应的类别标签：
- en: '[PRE13]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Equipped with the dataset, we need to divide this dataset into the test and
    the train datasets:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借数据集，我们需要将其划分为测试集和训练集：
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We invoked the `get_train_test` function with an input dataset and the class
    labels. This function provides us with a stratified sample. We are using 70 percent
    of our data for the training set and the rest for the test set.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用了`get_train_test`函数，传入一个输入数据集和类别标签。此函数为我们提供了一个分层样本。我们使用70%的数据作为训练集，剩余的作为测试集。
- en: 'Once again, we invoke `get_train_test` with the test dataset returned from
    the previous step:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次调用`get_train_test`，并传入上一步返回的测试数据集：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We created a separate dataset and called it the dev dataset. We need this dataset
    to tune our model. We want our test set to really behave as a test set. We don't
    want to expose our test set during the different passes of our model building
    exercise.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个单独的数据集，并称其为开发集。我们需要这个数据集来调整我们的模型。我们希望我们的测试集真正充当测试集的角色。在构建模型的不同过程中，我们不希望暴露测试集。
- en: 'Let''s print the size of our train, dev, and test datasets:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出我们的训练集、开发集和测试集的大小：
- en: '![How it works…](img/B04041_06_19.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的……](img/B04041_06_19.jpg)'
- en: As you can see, 70 percent of the original data is assigned to our training
    set. We have again split the rest 30 percent into a 70/30 percent split for `Dev`
    and `Testing`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，70%的原始数据被分配到我们的训练集中。其余的30%再次被划分为`Dev`和`Testing`的70/30分配。
- en: 'Let''s start our model building activity. We will call `build_model_cycle_1`
    with our training and dev datasets. In this function, we will first create our
    features by calling `build_word_feature` using a map on all the instances in our
    dataset. The `build_word_feature` is a simple feature-generating function. Every
    word is a feature. The output of this function is a dictionary of features, where
    the key is the word itself and the value is one. These types of features are typically
    called Bag of Words (BOW). The `build_word_features` is invoked using both the
    training and the dev data:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始我们的模型构建活动。我们将调用`build_model_cycle_1`，并使用我们的训练集和开发集数据。在这个函数中，我们将首先通过调用`build_word_feature`并对数据集中的所有实例进行map操作来创建特征。`build_word_feature`是一个简单的特征生成函数。每个单词都是一个特征。这个函数的输出是一个特征字典，其中键是单词本身，值为1。这种类型的特征通常被称为词袋模型（BOW）。`build_word_features`函数同时在训练集和开发集数据上调用：
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will now proceed to train our model with the generated feature:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将继续使用生成的特征来训练我们的模型：
- en: '[PRE17]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We need to test how good our model is. We use the `probe_model` function to
    do this. `Probe_model` takes three parameters. The first parameter is the model
    of interest, the second parameter is the feature against which we want to see
    how good our model is, and the last parameter is a string used for display purposes.
    The `probe_model` function calculates the accuracy metric using the accuracy function
    in the `nltk.classify` module.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要测试我们的模型有多好。我们使用`probe_model`函数来实现这一点。`Probe_model`接受三个参数。第一个参数是我们感兴趣的模型，第二个参数是我们希望评估模型性能的特征，最后一个参数是用于显示目的的字符串。`probe_model`函数通过在`nltk.classify`模块中使用准确度函数来计算准确率指标。
- en: 'We invoke `probe_model` twice: once with the training data to see how good
    the model is on our training dataset, and then once with our dev dataset:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`probe_model`两次：第一次使用训练数据来查看模型在训练集上的表现，第二次使用开发集数据：
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s now look at the accuracy figures:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下准确率数据：
- en: '![How it works…](img/B04041_06_20.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_06_20.jpg)'
- en: 'Our model is behaving very well using the training data. This is not surprising
    as the model has already seen it during the training phase. It''s doing a good
    job at classifying the training record correctly. However, our dev accuracy is
    very poor. Our model is able to classify only 60 percent of the dev instances
    correctly. Surely our features are not informative enough to help our model classify
    the unseen instances with a good accuracy. It will be good to see which features
    are contributing more towards discriminating a review into positive and negative:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在使用训练数据时表现得非常好。这并不令人惊讶，因为模型在训练阶段已经见过这些数据。它能够正确地分类训练记录。然而，我们的开发集准确率很差。我们的模型只能正确分类60%的开发集实例。显然，我们的特征对于帮助模型高效分类未见过的实例并不够有信息量。查看哪些特征对将评论分类为正面或负面有更大贡献是很有帮助的：
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We will invoke the `show_features` function to look at the features'' contribution
    towards the model. The `Show_features` function utilizes the `show_most_informative_feature`
    function from the NLTK classifier object. The most important features in our first
    model are as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调用`show_features`函数来查看各特征对模型的贡献。`Show_features`函数利用NLTK分类器对象中的`show_most_informative_feature`函数。我们第一个模型中最重要的特征如下：
- en: '![How it works…](img/B04041_06_21.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_06_21.jpg)'
- en: 'The way to read it is: the feature `stupidity = 1` is 15 times more effective
    for classifying a review as negative.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 读取方式是：特征`stupidity = 1`在将评论分类为负面时比其他特征更有效15倍。
- en: Let's now do a second round of building this model using a new set of features.
    We will do this by invoking `build_model_cycle_2`. `build_model_cycle_2` is very
    similar to `build_model_cycle_1` except for the feature generation function called
    inside map function.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用一组新的特征来进行第二轮模型构建。我们将通过调用`build_model_cycle_2`来实现。`build_model_cycle_2`与`build_model_cycle_1`非常相似，唯一不同的是在map函数中调用了不同的特征生成函数。
- en: 'The feature generation function is called `build_negate_features`. Typically,
    words such as not and no are called negation words. Let''s assume that our reviewer
    says that the movie is not good. If we use our previous feature generator, the
    word good would be treated equally in both the positive and negative reviews.
    We know that the word good should be used to discriminate the positive reviews.
    To avoid this problem, we will look for the negation words no and not in our word
    list. We want to modify our example sentence as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 特征生成函数称为 `build_negate_features`。通常，像 “not” 和 “no” 这样的词被称为否定词。假设我们的评论者说这部电影不好。如果我们使用之前的特征生成器，单词
    “good” 在正面和负面评论中会被平等对待。但我们知道，单词 “good” 应该用于区分正面评论。为了避免这个问题，我们将查找单词列表中的否定词“no”和“not”。我们希望将我们的示例句子修改如下：
- en: '[PRE20]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This way, `no_good` can be used as a good feature to discriminate the negative
    reviews from the positive reviews. The `build_negate_features` function does this
    job.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，`no_good` 可以作为一个很好的特征，帮助区分负面评论与正面评论。`build_negate_features` 函数完成了这个工作。
- en: 'Let''s now look at our probing output for the model built with this negation
    feature:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看使用此否定特征构建的模型的探测输出：
- en: '![How it works…](img/B04041_06_22.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理…](img/B04041_06_22.jpg)'
- en: 'We improved our model accuracy on our dev data by almost 2 percent. Let''s
    now look at the most informative features for this model:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在开发数据上的模型准确率提高了近 2%。现在让我们看看这个模型中最具信息量的特征：
- en: '![How it works…](img/B04041_06_23.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理…](img/B04041_06_23.jpg)'
- en: Look at the last feature. Adding negation to funny, the '`Not_funny`' feature
    is 11.7 times more informative for discriminating a review as negative.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 看看最后一个特征。添加否定词后，“`Not_funny`” 特征对于区分负面评论比其他特征信息量多 11.7 倍。
- en: Can we do better on our model accuracy ? Currently, we are at 70 percent. Let's
    do a third run with a new set of features. We will do this by invoking `build_model_cycle_3`.
    `build_model_cycle_3` is very similar to `build_model_cycle_2` except for the
    feature generation function called inside map function.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能提高模型的准确性吗？目前，我们的准确率是 70%。让我们进行第三次实验，使用一组新的特征。我们将通过调用 `build_model_cycle_3`
    来实现。`build_model_cycle_3` 与 `build_model_cycle_2` 非常相似，除了在 map 函数内部调用的特征生成函数不同。
- en: 'The `build_keyphrase_features` function is used as a feature generator. Let''s
    look at the function in detail. Instead of using the words as features, we will
    generate key phrases from the review and use them as features. Key phrases are
    phrases that we consider important using some metric. Key phrases can be made
    of either two, three, or n words. In our case, we will use two words (bigrams)
    to build our key phrase. The metric that we will use is the raw frequency count
    of these phrases. We will choose the phrases whose frequency count is higher.
    We will do some simple preprocessing before generating our key phrases. We will
    remove all the stopwords and punctuation from our word list. The `remove_stop_words`
    function is invoked to remove the stopwords and punctuation. NLTK''s corpus module
    has a list of English stopwords. We can retrieve it as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`build_keyphrase_features` 函数用作特征生成器。让我们详细看看这个函数。我们将不使用单词作为特征，而是从评论中生成关键词组，并将它们用作特征。关键词组是我们通过某种度量认为重要的短语。关键词组可以由两个、三个或多个单词组成。在我们的例子中，我们将使用两个单词（二元组）来构建关键词组。我们将使用的度量是这些短语的原始频率计数。我们将选择频率计数较高的短语。在生成关键词组之前，我们将进行一些简单的预处理。我们将从单词列表中移除所有停用词和标点符号。`remove_stop_words`
    函数会被调用来移除停用词和标点符号。NLTK 的语料库模块中有一个英文停用词列表。我们可以按如下方式检索它：'
- en: '[PRE21]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Similarly, the string module in Python maintains a list of punctuation. We
    will remove the stopwords and punctuation as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Python 的字符串模块维护着一个标点符号列表。我们将按如下方式移除停用词和标点符号：
- en: '[PRE22]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'However, we will not remove not and `no`. We will create a new set of stopwords
    by not, including the negation words in the previous step:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不会移除“not”和“no”。我们将通过“not”创建一组新的停用词，其中包含前一步中的否定词：
- en: '[PRE23]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will leverage the `BigramCollocationFinder` class from NLTK to generate
    our key phrases:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用 NLTK 的 `BigramCollocationFinder` 类来生成我们的关键词组：
- en: '[PRE24]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Our metric is the frequency count. You can see that we specified it as `raw_freq`
    in the last line. We will ask the collocation finder to return us a maximum of
    400 phrases.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的度量是频率计数。你可以看到，在最后一行中我们将其指定为 `raw_freq`。我们将要求搭配查找器返回最多 400 个短语。
- en: 'Loaded with our new feature, we will proceed to build our model and test the
    correctness of our model. Let''s look at the output of our model:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 装载了新特征后，我们将继续构建模型并测试模型的正确性。让我们看看模型的输出：
- en: '![How it works…](img/B04041_06_24.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理…](img/B04041_06_24.jpg)'
- en: 'Yes! We have achieved a great deal of improvement on our dev set. From 68 percent
    accuracy in our first pass with word features, we have moved from 12 percent up
    to 80 percent with our key phrase features. Let''s now expose our test set to
    this model and check the accuracy:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！我们在开发集上取得了很大进展。从第一次使用词语特征时的68％准确率，我们通过关键短语特征将准确率从12％提升至80％。现在让我们将测试集暴露给这个模型，检查准确度：
- en: '[PRE25]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![How it works…](img/B04041_06_25.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理…](img/B04041_06_25.jpg)'
- en: 'Our test set''s accuracy is greater than our dev set''s accuracy. We did a
    good job in training a good model that works well on an unseen dataset. Before
    we end this recipe, let''s look at the key phrases that are the most informative:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试集准确度高于开发集准确度。我们在训练一个在未见数据集上表现良好的模型方面做得不错。在结束本教程之前，我们来看一下最有信息量的关键短语：
- en: '![How it works…](img/B04041_06_26.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理…](img/B04041_06_26.jpg)'
- en: The key phrase, Oscar nomination, is 10 times more helpful in discriminating
    a review as positive. You can't deny this. We can see that our key phrases are
    very informative, and hence, our model performed better than the previous two
    runs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 关键短语“奥斯卡提名”在区分评论为正面时比其他任何特征都要有帮助，效果是其他10倍。你无法否认这一点。我们可以看到我们的关键短语信息量非常大，因此我们的模型比前两次运行表现得更好。
- en: There's more…
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: How did we know that 400 key phrases and the metric frequency count is the best
    parameter for bigram generation? Trial and error. Though we didn't list our trial
    and error process, we pretty much ran it with various combinations such as 200
    phrases with pointwise mutual information, and similar other methods.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何知道400个关键短语和度量频次计数是双词生成的最佳参数的呢？通过反复试验。虽然我们没有列出我们的试验过程，但基本上我们使用了不同的组合，比如200个短语与逐点互信息（pointwise
    mutual information），以及其他类似的方法。
- en: This is what needs to be done in the real world. However, instead of a blind
    search through the parameter space every time, we looked at the most informative
    features. This gave us a clue on the discriminating power of the features.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是现实世界中需要做的事情。然而，我们不是每次都盲目地搜索参数空间，而是关注了最有信息量的特征。这给了我们有关特征区分能力的线索。
- en: See also
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Preparing data for model building* recipe in [Chapter 6](ch06.xhtml "Chapter 6. Machine
    Learning 1"), *Machine Learning I*'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml "第6章. 机器学习I")中的*为模型构建准备数据*教程，*机器学习I*
- en: Building decision trees to solve multiclass problems
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建决策树来解决多类问题
- en: 'In this recipe, we will look at building decision trees to solve multiclass
    classification problems. Intuitively, a decision tree can be defined as a process
    of arriving at an answer by asking a series of questions: a series of if-then
    statements arranged hierarchically forms a decision tree. Due to this nature,
    they are easy to understand and interpret.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将讨论构建决策树来解决多类分类问题。从直觉上看，决策树可以定义为通过一系列问题来得出答案的过程：一系列的“如果-那么”语句按层次结构排列构成了决策树。正因为这种特性，它们易于理解和解释。
- en: 'Refer to the following link for a detailed introduction to decision trees:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下链接，详细了解决策树的介绍：
- en: '[https://en.wikipedia.org/wiki/Decision_tree](https://en.wikipedia.org/wiki/Decision_tree)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Decision_tree](https://en.wikipedia.org/wiki/Decision_tree)'
- en: 'Theoretically, many decision trees can be built for a given dataset. Some of
    the trees are more accurate than others. There are efficient algorithms for developing
    a reasonably accurate tree in a limited time. One such algorithm is Hunt''s algorithm.
    Algorithms such as ID3, C4.5, and CART (Classification and Regression Trees) are
    based on Hunt''s algorithm. Hunt''s algorithm can be outlined as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，可以为给定的数据集构建许多决策树。某些树比其他树更准确。有一些高效的算法可以在有限的时间内开发出一个合理准确的树。一个这样的算法是亨特算法。像ID3、C4.5和CART（分类和回归树）这样的算法都基于亨特算法。亨特算法可以概述如下：
- en: 'Given a dataset, D, of n records and with each record having m attributes /
    features / columns and each record labeled either y1, y2, or y3, the algorithm
    proceeds as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据集D，其中有n条记录，每条记录包含m个属性/特征/列，并且每条记录被标记为y1、y2或y3，算法的处理流程如下：
- en: If all the records in D belong to the same class label, say y1, then y1 is the
    leaf node of the tree and is labeled y1.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果D中的所有记录都属于同一类别标签，例如y1，那么y1就是树的叶子节点，并被标记为y1。
- en: If D has records that belong to more than one class label, a feature test condition
    is employed to divide the records into smaller subsets. Let's say in the initial
    run, we run a feature test condition on all the attributes and find a single attribute
    that is able to split the datasets into three smaller subsets. This attribute
    becomes the root node. We apply the test condition on all the three subsets to
    figure out the next level of nodes. This process is performed iteratively.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果D中的记录属于多个类标签，则会采用特征测试条件将记录划分为更小的子集。假设在初始运行时，我们对所有属性进行特征测试条件，并找到一个能够将数据集划分为三个子集的属性。这个属性就成为根节点。我们对这三个子集应用测试条件，以找出下一层的节点。这个过程是迭代进行的。
- en: Notice that when we defined our classification, we defined three class labels,
    y1, y2, and y3\. This is different from the problems that we solved in the previous
    two recipes, where we had only two labels. This is a multiclass problem. Our Iris
    dataset that we used in most of our recipes is a three-class problem. We had our
    records distributed across three class labels. We can generalize it to an n-class
    problem. Digit recognition is another example by which we need to classify a given
    image in one of the digits between zero and nine. Many real-word problems are
    inherently multiclass. Some algorithms are also inherently capable of handling
    multiclass problems. No change is required for these algorithms. The algorithms
    that we will discuss in the chapters are all capable of handling multiclass problems.
    Decision trees, Naïve Bayes, and KNN algorithms are good at handling multiclass
    problems.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们定义分类时，我们定义了三个类别标签y1、y2和y3。与前两个食谱中我们只处理了两个标签的问题不同，这是一个多类问题。我们在大多数食谱中使用的鸢尾花数据集就是一个三类问题。我们的记录分布在三个类别标签之间。我们可以将其推广到n类问题。数字识别是另一个例子，我们需要将给定图像分类为零到九之间的某个数字。许多现实世界的问题本质上就是多类问题。一些算法也天生可以处理多类问题。对于这些算法，无需进行任何修改。我们将在各章节中讨论的算法都能处理多类问题。决策树、朴素贝叶斯和KNN算法擅长处理多类问题。
- en: Let's see how we can leverage decision trees to handle multiclass problems in
    this recipe. It also helps to have a good understanding of decision trees. Random
    forest, which we will venture into in the next chapter, is a more sophisticated
    method used widely in the industry and is based on decision trees.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何利用决策树来处理这个食谱中的多类问题。理解决策树的原理也有助于此。随机森林是下一章我们将探讨的一种更为复杂的方法，它在行业中被广泛应用，且基于决策树。
- en: Let's now dive into our decision tree recipe.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解决策树的使用方法。
- en: Getting ready
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'We will use the Iris dataset to demonstrate how to build decision trees. Decision
    trees are a non-parametric supervised learning method that can be used to solve
    both classification and regression problems. As explained previously, the advantages
    of using decision trees are manifold, as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用鸢尾花数据集来演示如何构建决策树。决策树是一种非参数的监督学习方法，可以用来解决分类和回归问题。如前所述，使用决策树的优点有很多，具体如下：
- en: They are easily interpretable
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们易于解释
- en: 'They require very little data preparation and data-to-feature conversion: remember
    our feature generation methods in the previous recipe'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们几乎不需要数据准备和特征转换：请记住我们在前一个食谱中的特征生成方法
- en: They naturally support multiclass problems
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们天然支持多类问题
- en: 'Decision trees are not without problems. Some of the problems that they pose
    are as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树并非没有问题。它们存在的一些问题如下：
- en: 'They can easily overfit: a high accuracy in a training set and very poor performance
    with a test data.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们很容易发生过拟合：在训练集上表现出高准确率，而在测试数据上表现很差。
- en: There can be millions of trees that can be fit to a given dataset.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以为给定数据集拟合出数百万棵树。
- en: The class imbalance problem may affect decision trees heavily. The class imbalance
    problem arises when our training set does not consist of an equal number of instances
    for both the class labels in a binary classification problem. This applies to
    multiclass problems as well.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别不平衡问题可能会对决策树产生较大影响。类别不平衡问题出现在我们的训练集在二分类问题中未包含相等数量的实例标签时。这个问题在多类问题中也同样适用。
- en: An important part of decision trees is the feature test condition. Let's spend
    some time understanding the feature test condition. Typically, each attribute
    in our instance can be either understood.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个重要部分是特征测试条件。让我们花些时间理解特征测试条件。通常，我们的实例中的每个属性可以被理解为以下之一。
- en: '**Binary attribute**: This is where an attribute can take two possible values,
    for example, true or false. The feature test condition should return two values
    in this case.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**二元属性**：这是指一个属性可以取两个可能的值，比如true或false。在这种情况下，特征测试条件应该返回两个值。'
- en: '**Nominal attribute**: This is where an attribute can take more than two values,
    for example, n values. The feature test condition should either output n output
    or group them into binary splits.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**名义属性**：这是指一个属性可以取多个值，比如n个值。特征测试条件应该返回n个输出，或者将它们分组为二元拆分。'
- en: '**Ordinal attribute**: This is where an implicit order in their values exists.
    For example, let''s take an imaginary attribute called size, which can take on
    the values small, medium, or large. There are three values that the attribute
    can take and there is an order for them: small, medium, large. They are handled
    by the feature attribute test condition that is similar to the nominal attribute.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**顺序属性**：这是指属性的值之间存在隐含的顺序关系。例如，假设我们有一个虚拟属性叫做size，它可以取小、中、大这三个值。这个属性有三个值，并且它们之间有顺序：小、中、大。它们由特征属性测试条件处理，这类似于名义属性的处理方式。'
- en: '**Continuous attributes**: These are attributes that can take continuous values.
    They are discretized into ordinal attributes and then handled.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续属性**：这些是可以取连续值的属性。它们会被离散化为顺序属性，然后进行处理。'
- en: A feature test condition is a way to split the input records into subsets based
    on a criteria or metric called impurity. This impurity is calculated with respect
    to the class label for each attribute in the instance. The attribute contributing
    to the highest impurity is chosen as the data splitting attribute, or in other
    words, the node for that level in the tree.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 特征测试条件是一种根据叫做“杂质”的标准或度量，将输入记录拆分为子集的方法。这个杂质是相对于每个属性在实例中的类别标签计算的。贡献最大杂质的属性被选为数据拆分属性，或者换句话说，就是树中该层的节点。
- en: Let's see an example to explain it. We will use a measure called entropy to
    calculate the impurity.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来解释它。我们将使用一种叫做熵的度量来计算杂质。
- en: 'Entropy is defined as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的定义如下：
- en: '![Getting ready](img/B04041_06_27.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/B04041_06_27.jpg)'
- en: 'Where:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![Getting ready](img/B04041_06_28.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/B04041_06_28.jpg)'
- en: 'Let''s consider an example:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子：
- en: '[PRE26]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can now calculate the entropy for this set, as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以根据这个集合计算熵，如下所示：
- en: '![Getting ready](img/B04041_06_29.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/B04041_06_29.jpg)'
- en: 'The entropy for this set is 0\. An entropy of 0 indicates homogeneity. It is
    very easy to code entropy in Python. Look at the following code list:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这个集合的熵为0。熵为0表示同质性。在Python中编写熵计算非常简单。看一下以下的代码示例：
- en: '[PRE27]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For the purpose of finding the best splitting variable, we will leverage the
    entropy. First, we will calculate the entropy based on the class labels, as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最佳的分割变量，我们将利用熵。首先，我们将根据类别标签计算熵，如下所示：
- en: '![Getting ready](img/B04041_06_30.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/B04041_06_30.jpg)'
- en: Let's define another term called information gain. Information gain is a measure
    to find which attribute in the given instance is most useful for discrimination
    between the class labels.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义另一个术语，称为信息增益。信息增益是一种衡量在给定实例中哪个属性对区分类别标签最有用的度量方法。
- en: 'Information gain is the difference between an entropy of the parent and an
    average entropy of the child nodes. At each level in the tree, we will use information
    gain to build the tree:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益是父节点的熵与子节点的平均熵之间的差值。在树的每一层，我们将使用信息增益来构建树：
- en: '[https://en.wikipedia.org/wiki/Information_gain_in_decision_trees](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Information_gain_in_decision_trees](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)'
- en: 'We will start with all the attributes in a training set and calculate the overall
    entropy. Let''s look at the following example:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从训练集中的所有属性开始，计算整体的熵。让我们来看以下的例子：
- en: '![Getting ready](img/B04041_06_34.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/B04041_06_34.jpg)'
- en: 'The preceding dataset is imaginary data collected for a user to figure out
    what kind of movies he is interested in. There are four attributes: the first
    one is about whether the user watches a movie based on the lead actor, the second
    attribute is about if the user makes his decision to watch the movie based on
    whether or not it won an Oscar, and the third one is about if the user decides
    to watch a movies based on whether or not it is a box office success.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的数据集是为用户收集的虚拟数据，用来帮助他了解自己喜欢什么类型的电影。数据集包含四个属性：第一个属性是用户是否根据主演决定观看电影，第二个属性是用户是否根据电影是否获得奥斯卡奖来决定是否观看，第三个属性是用户是否根据电影是否票房成功来决定是否观看。
- en: 'In order to build a decision tree for the preceding example, we will start
    with the entropy calculation of the whole dataset. This is a two-class problem,
    hence c = 2\. Additionally, there are a total of four records, hence, the entropy
    of the whole dataset is as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建前面示例的决策树，我们将从整个数据集的熵计算开始。这是一个二类问题，因此 c = 2。并且，数据集共有四条记录，因此整个数据集的熵如下：
- en: '![Getting ready](img/B04041_06_36.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![准备中](img/B04041_06_36.jpg)'
- en: The overall entropy of the dataset is 0.811.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的总体熵值为 0.811。
- en: 'Now, let''s look at the first attribute, the lead attribute. For a lead actor,
    Y, there is one instance class label that says Y and another one that says N.
    For a lead actor, N, both the instance class labels are N. We will calculate the
    average entropy as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下第一个属性，即主演属性。对于主演为 Y 的实例，有一个实例类别标签为 Y，另一个实例类别标签为 N。对于主演为 N 的实例，两个实例类别标签均为
    N。我们将按如下方式计算平均熵：
- en: '![Getting ready](img/B04041_06_35.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![准备中](img/B04041_06_35.jpg)'
- en: It's an average entropy. There are two records with a lead actor as Y and two
    records with lead actors as N; hence, we have `2/4.0` multiplied to the entropy
    value.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个平均熵值。主演为 Y 的记录有两条，主演为 N 的记录也有两条；因此，我们有 `2/4.0` 乘以熵值。
- en: As the entropy is calculated for this subset of data, we can see that out of
    the two records, one of them has a class label of Y and another one has a class
    label of N for the lead actor Y. Similarly, for the lead actor N, both the records
    have a class label of N. Thus, we get the average entropy for this attribute.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当对这个数据子集计算熵时，我们可以看到在两个记录中，主演 Y 的一个记录的类别标签为 Y，另一个记录的类别标签为 N。类似地，对于主演为 N，两个记录的类别标签均为
    N。因此，我们得到了该属性的平均熵。
- en: The average entropy value for the lead actor attribute is 0.5.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 主演属性的平均熵值为 0.5。
- en: The information gain is now 0.811 – 0.5 = 0.311.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益现在是 0.811 – 0.5 = 0.311。
- en: Similarly, we will find the information gain for all the attributes. The attribute
    with the highest information gain wins and becomes the root node of the decision
    tree.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们将找到所有属性的信息增益。具有最高信息增益的属性胜出，并成为决策树的根节点。
- en: The same process is repeated in order to find the second level of the nodes,
    and so on.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的过程将重复进行，以便找到节点的第二级，依此类推。
- en: How to do it…
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Let''s load the required libraries. We will follow it with two functions, one
    to load the data and the second one to split the data into a training set and
    a test it:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载所需的库。接下来，我们将编写两个函数，一个用于加载数据，另一个用于将数据拆分为训练集和测试集：
- en: '[PRE28]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s write the functions to help us build and test the decision tree model:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写函数来帮助我们构建和测试决策树模型：
- en: '[PRE29]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, the main function to invoke all the other functions that we defined
    is as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，调用我们定义的所有其他函数的主函数如下：
- en: '[PRE30]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works…
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Let''s start with the main function. We invoke `get_data` in the `x`, `y`,
    and `label_names` variables in order to retrieve the Iris dataset. We took the
    label names so that when we see our model accuracy, we can measure it by individual
    labels. As said previously, the Iris data poses a three-class problem. We will
    need to build a classifier that can classify any new instances in one of the tree
    types: setosa, versicolor, or virginica.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从主函数开始。我们在 `x`、`y` 和 `label_names` 变量中调用 `get_data` 来检索鸢尾花数据集。我们获取标签名称，以便在看到模型准确性时，可以根据单个标签进行衡量。如前所述，鸢尾花数据集是一个三类问题。我们需要构建一个分类器，可以将任何新实例分类为三种类型之一：setosa、versicolor
    或 virginica。
- en: Once again, as in the previous recipes, `get_train_test` returns stratified
    train and test datasets. We then leverage `StratifiedShuffleSplit` from scikit-learn
    to get the training and test datasets with an equal class label distribution.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，`get_train_test`返回分层的训练和测试数据集。然后，我们利用scikit-learn中的`StratifiedShuffleSplit`获取具有相等类标签分布的训练和测试数据集。
- en: 'We must invoke the `build_model` method to induce a decision tree on our training
    set. The `DecisionTreeClassifier` class in the model tree of scikit-learn implements
    a decision tree:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须调用`build_model`方法，在我们的训练集上构建决策树。scikit-learn中的`DecisionTreeClassifier`类实现了一个决策树：
- en: '[PRE31]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see, we specified that our feature test condition is an entropy using
    the `criterion` variable. We then build the model by calling the `fit` function
    and return the model to the calling program.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们指定了我们的特征测试条件是使用`criterion`变量来设置的熵。然后，我们通过调用`fit`函数来构建模型，并将模型返回给调用程序。
- en: Now, let's proceed to evaluate our model by using the `test_model` function.
    The model takes instances `x` , class labels `y`, decision tree model `model`,
    and the name of the class labels `label_names`.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续使用`test_model`函数来评估我们的模型。该模型接受实例`x`、类标签`y`、决策树模型`model`以及类标签名称`label_names`。
- en: 'The module metric in scikit-learn provides three evaluation criteria:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的模块度量提供了三种评估标准：
- en: '[PRE32]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We defined accuracy in the previous recipe and the introduction section.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的步骤和引言部分定义了准确率。
- en: A confusion matrix prints the confusion matrix defined in the introduction section.
    A confusion matrix is a good way of evaluating the model performance. We are interested
    in the cell values having true positive and false positive values.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵打印了引言部分定义的混淆矩阵。混淆矩阵是一种评估模型表现的好方法。我们关注的是具有真正例和假正例值的单元格值。
- en: Finally, we also have `classification_report` to print the precision, recall,
    and F1 score.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还有`classification_report`来打印精确度、召回率和F1得分。
- en: 'We must evaluate the model on the training data first:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须首先在训练数据上评估模型：
- en: '![How it works…](img/B04041_06_31.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_06_31.jpg)'
- en: We have done a great job with the training dataset. We have 100 percent accuracy.
    The true test is with the test dataset where the rubber meets the road.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练数据集上做得非常好，准确率达到了100%。真正的考验是使用测试数据集，那里才是决定成败的地方。
- en: 'Let''s look at the model evaluation using the test dataset:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用测试数据集来查看模型评估：
- en: '![How it works…](img/B04041_06_32.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_06_32.jpg)'
- en: Our classifier has performed extremely well with the test set as well.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器在测试集上的表现也非常出色。
- en: There's more…
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Let's probe the model to see how the various features contributed towards discriminating
    the classes.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探查一下模型，看看各个特征如何有助于区分不同类别。
- en: '[PRE33]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The tree classifier object provides an attribute called `feature_importances_`,
    which can be called to retrieve the importance of the various features towards
    building our model.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器对象提供了一个属性`feature_importances_`，可以调用它来获取各个特征对构建我们模型的重要性。
- en: We wrote a simple function, `get_feature_names`, in order to retrieve the names
    of our attributes. However, this can be added as a part of `get_data`.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们写了一个简单的函数`get_feature_names`，用于获取特征名称。不过，这也可以作为`get_data`的一部分来添加。
- en: 'Let''s look at the print statement output:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下打印语句的输出：
- en: '![There''s more…](img/B04041_06_33.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多…](img/B04041_06_33.jpg)'
- en: This looks as if the petal width and petal length are contributing more towards
    our classification.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来好像花瓣宽度和花瓣长度在我们的分类中贡献更大。
- en: 'Interestingly, we can also export the tree built by the classifier as a dot
    file and it can be visualized using the GraphViz package. In the last line, we
    export our model as a dot file:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们还可以将分类器构建的树导出为dot文件，并使用GraphViz包进行可视化。在最后一行，我们将模型导出为dot文件：
- en: '[PRE34]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can download and install the Graphviz package to visualize this:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以下载并安装Graphviz包来可视化它：
- en: '[http://www.graphviz.org/](http://www.graphviz.org/)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.graphviz.org/](http://www.graphviz.org/)'
- en: See also
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: '*Preparing data for model building* recipe in [Chapter 6](ch06.xhtml "Chapter 6. Machine
    Learning 1"), *Machine Learning I*'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为模型构建准备数据*的步骤见[第6章](ch06.xhtml "第6章 机器学习 I")，*机器学习 I*'
- en: '*Finding nearest neighbors* recipe in [Chapter 6](ch06.xhtml "Chapter 6. Machine
    Learning 1"), *Machine Learning I*'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查找最近邻*的步骤见[第6章](ch06.xhtml "第6章 机器学习 I")，*机器学习 I*'
- en: '*Classifying documents using Naive Bayes* recipe in [Chapter 6](ch06.xhtml
    "Chapter 6. Machine Learning 1"), *Machine Learning I*'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第六章](ch06.xhtml "第六章. 机器学习 1")中，*使用朴素贝叶斯分类文档*的技巧，*机器学习 I*
