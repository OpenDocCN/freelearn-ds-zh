- en: Chapter 3. Apache Spark Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。Apache Spark Streaming
- en: 'The Apache Streaming module is a stream processing-based module within Apache
    Spark. It uses the Spark cluster to offer the ability to scale to a high degree.
    Being based on Spark, it is also highly fault tolerant, having the ability to
    rerun failed tasks by checkpointing the data stream that is being processed. The
    following areas will be covered in this chapter after an initial section, which
    will provide a practical overview of how Apache Spark processes stream-based data:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Streaming模块是Apache Spark中基于流处理的模块。它利用Spark集群提供高度扩展的能力。基于Spark，它也具有高度的容错性，能够通过检查点数据流重新运行失败的任务。在本章的初始部分之后，将涵盖以下领域，这部分将提供Apache
    Spark处理基于流的数据的实际概述：
- en: Error recovery and checkpointing
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误恢复和检查点
- en: TCP-based Stream Processing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于TCP的流处理
- en: File Streams
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件流
- en: Flume Stream source
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flume流源
- en: Kafka Stream source
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka流源
- en: For each topic, I will provide a worked example in Scala, and will show how
    the stream-based architecture can be set up and tested.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个主题，我将在Scala中提供一个示例，并展示如何设置和测试基于流的架构。
- en: Overview
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概览
- en: When giving an overview of the Apache Spark streaming module, I would advise
    you to check the [http://spark.apache.org/](http://spark.apache.org/) website
    for up-to-date information, as well as the Spark-based user groups such as `<[user@spark.apache.org](mailto:user@spark.apache.org)>`.
    My reason for saying this is because these are the primary places where Spark
    information is available. Also the extremely fast (and increasing) pace of change
    means that by the time you read this new Spark functionality and versions, will
    be available. So, in the light of this, when giving an overview, I will try to
    generalize.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍Apache Spark流模块时，我建议您查看[http://spark.apache.org/](http://spark.apache.org/)网站以获取最新信息，以及Spark用户组，如`<[user@spark.apache.org](mailto:user@spark.apache.org)>`。我之所以这样说是因为这些是Spark信息可获得的主要地方。而且，极快（并且不断增加）的变化速度意味着到您阅读此内容时，新的Spark功能和版本将会可用。因此，在这种情况下，当进行概述时，我会尽量概括。
- en: '![Overview](img/B01989_03_01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![概览](img/B01989_03_01.jpg)'
- en: The previous figure shows potential data sources for Apache Streaming, such
    as **Kafka**, **Flume**, and **HDFS**. These feed into the Spark Streaming module,
    and are processed as discrete streams. The diagram also shows that other Spark
    module functionality, such as machine learning, can be used to process the stream-based
    data. The fully processed data can then be an output for **HDFS**, **databases**,
    or **dashboards**. This diagram is based on the one at the Spark streaming website,
    but I wanted to extend it for both—expressing the Spark module functionality,
    and for dashboarding options. The previous diagram shows a MetricSystems feed
    being fed from Spark to Graphite. Also, it is possible to feed Solr-based data
    to Lucidworks banana (a port of kabana). It is also worth mentioning here that
    Databricks (see [Chapter 8](ch08.html "Chapter 8. Spark Databricks"), *Spark Databricks*
    and [Chapter 9](ch09.html "Chapter 9. Databricks Visualization"), *Databricks
    Visualization*) can also present the Spark stream data as a dashboard.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了Apache Streaming的潜在数据来源，例如**Kafka**，**Flume**和**HDFS**。这些数据源输入到Spark
    Streaming模块中，并作为离散流进行处理。该图还显示了其他Spark模块功能，例如机器学习，可以用来处理基于流的数据。经过完全处理的数据可以作为**HDFS**，**数据库**或**仪表板**的输出。这个图是基于Spark
    streaming网站上的图，但我想扩展它，既表达了Spark模块的功能，也表达了仪表板的选项。前面的图显示了从Spark到Graphite的MetricSystems数据源。此外，还可以将基于Solr的数据源提供给Lucidworks
    banana（kabana的一个端口）。值得在这里提到的是Databricks（见[第8章](ch08.html "第8章。Spark Databricks")，*Spark
    Databricks*和[第9章](ch09.html "第9章。Databricks Visualization")，*Databricks Visualization*）也可以将Spark流数据呈现为仪表板。
- en: '![Overview](img/B01989_03_02.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![概览](img/B01989_03_02.jpg)'
- en: When discussing Spark discrete streams, the previous figure, again taken from
    the Spark website at [http://spark.apache.org/](http://spark.apache.org/), is
    the diagram I like to use. The green boxes in the previous figure show the continuous
    data stream sent to Spark, being broken down into a **discrete stream** (**DStream**).
    The size of each element in the stream is then based on a batch time, which might
    be two seconds. It is also possible to create a window, expressed as the previous
    red box, over the DStream. For instance, when carrying out trend analysis in real
    time, it might be necessary to determine the top ten Twitter-based Hashtags over
    a ten minute window.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论Spark离散流时，前面的图，再次取自Spark网站[http://spark.apache.org/](http://spark.apache.org/)，是我喜欢使用的图。前面的图中的绿色框显示了连续的数据流发送到Spark，被分解为**离散流**（**DStream**）。然后，流中每个元素的大小基于批处理时间，可能是两秒。还可以创建一个窗口，表示为前面的红色框，覆盖DStream。例如，在实时进行趋势分析时，可能需要确定在十分钟窗口内的前十个基于Twitter的Hashtags。
- en: 'So, given that Spark can be used for Stream processing, how is a Stream created?
    The following Scala-based code shows how a Twitter stream can be created. This
    example is simplified because Twitter authorization has not been included, but
    you get the idea (the full example code is in the *Checkpointing* section). The
    Spark stream context, called `ssc`, is created using the spark context `sc`. A
    batch time is specified when it is created; in this case, five seconds. A Twitter-based
    DStream, called `stream`, is then created from the `Streamingcontext` using a
    window of 60 seconds:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，鉴于Spark可以用于流处理，如何创建流呢？以下基于Scala的代码显示了如何创建Twitter流。这个例子是简化的，因为没有包括Twitter授权，但您可以理解（完整的示例代码在*检查点*部分）。使用Spark上下文`sc`创建了名为`ssc`的Spark流上下文。在创建时指定了批处理时间；在这种情况下是五秒。然后从`Streamingcontext`创建了基于Twitter的DStream，称为`stream`，并使用了60秒的窗口：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The stream processing can be started with the stream context start method (shown
    next), and the `awaitTermination` method indicates that it should process until
    stopped. So, if this code is embedded in a library-based application, it will
    run until the session is terminated, perhaps with a *Crtl* + *C*:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理可以使用流上下文开始方法（下面显示），`awaitTermination`方法表示应该一直处理直到停止。因此，如果此代码嵌入在基于库的应用程序中，它将一直运行直到会话终止，也许使用*Crtl*
    + *C*：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This explains what Spark streaming is, and what it does, but it does not explain
    error handling, or what to do if your stream-based application fails. The next
    section will examine Spark streaming error management and recovery.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了Spark流是什么以及它的作用，但没有解释错误处理，或者如果基于流的应用程序失败该怎么办。下一节将讨论Spark流错误管理和恢复。
- en: Errors and recovery
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误和恢复
- en: Generally, the question that needs to be asked for your application is; is it
    critical that you receive and process all the data? If not, then on failure you
    might just be able to restart the application and discard the missing or lost
    data. If this is not the case, then you will need to use checkpointing, which
    will be described in the next section.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于您的应用程序需要问的问题是：是否关键接收和处理所有数据？如果不是，那么在失败时，您可能只需重新启动应用程序并丢弃丢失的数据。如果不是这种情况，那么您将需要使用检查点，这将在下一节中描述。
- en: It is also worth noting that your application's error management should be robust
    and self-sufficient. What I mean by this is that; if an exception is non-critical,
    then manage the exception, perhaps log it, and continue processing. For instance,
    when a task reaches the maximum number of failures (specified by `spark.task.maxFailures`),
    it will terminate processing.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，您的应用程序的错误管理应该是健壮和自给自足的。我的意思是，如果异常是非关键的，那么管理异常，也许记录它，并继续处理。例如，当任务达到最大失败次数（由`spark.task.maxFailures`指定）时，它将终止处理。
- en: Checkpointing
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查点
- en: 'It is possible to set up an HDFS-based checkpoint directory to store Apache
    Spark-based streaming information. In this Scala example, data will be stored
    in HDFS, under `/data/spark/checkpoint`. The following HDFS file system `ls` command
    shows that before starting, the directory does not exist:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 可以设置一个基于HDFS的检查点目录来存储Apache Spark基于流的信息。在这个Scala示例中，数据将存储在HDFS的`/data/spark/checkpoint`目录下。下面的HDFS文件系统`ls`命令显示，在开始之前，该目录不存在：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The Twitter-based Scala code sample given next, starts by defining a package
    name for the application, and by importing Spark, streaming, context, and Twitter-based
    functionality. It then defines an application object named `stream1`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来给出的基于Twitter的Scala代码示例，首先定义了应用程序的包名称，并导入了Spark、流、上下文和基于Twitter的功能。然后定义了一个名为`stream1`的应用程序对象：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, a method is defined called `createContext`, which will be used to create
    both the spark, and streaming contexts. It will also checkpoint the stream to
    the HDFS-based directory using the streaming context checkpoint method, which
    takes a directory path as a parameter. The directory path being the value (`cpDir`)
    that was passed into the `createContext` method:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来定义了一个名为`createContext`的方法，该方法将用于创建spark和流上下文，并将流检查点到基于HDFS的目录，使用流上下文检查点方法，该方法以目录路径作为参数。目录路径是传递给`createContext`方法的值（`cpDir`）：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, the main method is defined, as is the `HDFS` directory, as well as Twitter
    access authority and parameters. The Spark streaming context `ssc` is either retrieved
    or created using the HDFS `checkpoint` directory via the `StreamingContext` method—`getOrCreate`.
    If the directory doesn''t exist, then the previous method called `createContext`
    is called, which will create the context and checkpoint. Obviously, I have truncated
    my own Twitter auth. keys in this example for security reasons:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，主要方法已经定义，`HDFS`目录也已经定义，还有Twitter访问权限和参数。Spark流上下文`ssc`要么通过`StreamingContext`方法-`getOrCreate`从HDFS
    `checkpoint`目录中检索或创建。如果目录不存在，则调用之前的`createContext`方法，该方法将创建上下文和检查点。显然，出于安全原因，我在此示例中截断了自己的Twitter授权密钥。
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Having run this code, which has no actual processing, the HDFS `checkpoint`
    directory can be checked again. This time it is apparent that the `checkpoint`
    directory has been created, and the data has been stored:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 运行了这段代码，没有实际处理，可以再次检查HDFS `checkpoint`目录。这次明显可以看到`checkpoint`目录已经创建，并且数据已经存储：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This example, taken from the Apache Spark website, shows how checkpoint storage
    can be set up and used. But how often is checkpointing carried out? The Meta data
    is stored during each stream batch. The actual data is stored with a period, which
    is the maximum of the batch interval, or ten seconds. This might not be ideal
    for you, so you can reset the value using the method:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子来自Apache Spark网站，展示了如何设置和使用检查点存储。但是检查点操作有多频繁？元数据在每个流批次期间存储。实际数据存储的周期是批处理间隔或十秒的最大值。这可能不是您理想的设置，因此您可以使用该方法重置该值：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Where `newRequiredInterval` is the new checkpoint interval value that you require,
    generally you should aim for a value which is five to ten times your batch interval.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`newRequiredInterval`是您需要的新检查点间隔值，通常应该瞄准是批处理间隔的五到十倍。
- en: Checkpointing saves both the stream batch and metadata (data about the data).
    If the application fails, then when it restarts, the checkpointed data is used
    when processing is started. The batch data that was being processed at the time
    of failure is reprocessed, along with the batched data since the failure.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点保存了流批次和元数据（关于数据的数据）。如果应用程序失败，那么在重新启动时，将使用检查点数据进行处理。在失败时正在处理的批处理数据将被重新处理，以及失败后的批处理数据。
- en: Remember to monitor the HDFS disk space being used for check pointing. In the
    next section, I will begin to examine the streaming sources, and will provide
    some examples of each type.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 记得监控用于检查点的HDFS磁盘空间。在下一节中，我将开始检查流源，并提供每种类型的一些示例。
- en: Streaming sources
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流源
- en: I will not be able to cover all the stream types with practical examples in
    this section, but where this chapter is too small to include code, I will at least
    provide a description. In this chapter, I will cover the TCP and file streams,
    and the Flume, Kafka, and Twitter streams. I will start with a practical TCP-based
    example.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将无法涵盖所有流类型的实际示例，但在本章太小以至于无法包含代码的情况下，我至少会提供一个描述。在本章中，我将涵盖TCP和文件流，以及Flume、Kafka和Twitter流。我将从一个实际的基于TCP的示例开始。
- en: This chapter examines stream processing architecture. For instance, what happens
    in cases where the stream data delivery rate exceeds the potential data processing
    rate? Systems like Kafka provide the possibility of solving this issue by providing
    the ability to use multiple data topics and consumers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了流处理架构。例如，在流数据传递速率超过潜在数据处理速率的情况下会发生什么？像Kafka这样的系统提供了通过使用多个数据主题和消费者来解决这个问题的可能性。
- en: TCP stream
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TCP流
- en: 'There is a possibility of using the Spark streaming context method called `socketTextStream`
    to stream data via TCP/IP, by specifying a hostname and a port number. The Scala-based
    code example in this section will receive data on port `10777` that was supplied
    using the `netcat` Linux command. The code sample starts by defining the package
    name, and importing Spark, the context, and the streaming classes. The object
    class named `stream2` is defined, as it is the main method with arguments:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能使用Spark流上下文方法`socketTextStream`通过指定主机名和端口号来通过TCP/IP流式传输数据。本节中的基于Scala的代码示例将在端口`10777`上接收使用`netcat`
    Linux命令提供的数据。代码示例从定义包名开始，并导入Spark、上下文和流类。定义了一个名为`stream2`的对象类，因为它是带有参数的主方法：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The number of arguments passed to the class is checked to ensure that it is
    the hostname and the port number. A Spark configuration object is created with
    an application name defined. The Spark and streaming contexts are then created.
    Then, a streaming batch time of 10 seconds is set:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 检查传递给类的参数数量，以确保它是主机名和端口号。创建了一个具有应用程序名称的Spark配置对象。然后创建了Spark和流上下文。然后，设置了10秒的流批处理时间：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A DStream called `rawDstream` is created by calling the `socketTextStream` method
    of the streaming context using the host and port name parameters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用主机和端口名称参数调用流上下文的`socketTextStream`方法创建了一个名为`rawDstream`的DStream。
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'A top-ten word count is created from the raw stream data by splitting words
    by spacing. Then a (key,value) pair is created as `(word,1)`, which is reduced
    by the key value, this being the word. So now, there is a list of words and their
    associated counts. Now, the key and value are swapped, so the list becomes (`count`
    and `word`). Then, a sort is done on the key, which is now the count. Finally,
    the top 10 items in the `rdd`, within the DStream, are taken and printed out:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过按空格拆分单词，从原始流数据创建了一个前十个单词计数。然后创建了一个（键，值）对，即`(word,1)`，通过键值进行了减少，这就是单词。现在，有一个单词及其相关计数的列表。现在，键和值被交换，所以列表变成了（`count`和`word`）。然后，对现在是计数的键进行排序。最后，从DStream中的`rdd`中取出前10个项目并打印出来：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The code closes with the Spark Streaming start, and `awaitTermination` methods
    being called to start the stream processing and await process termination:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 代码以Spark Streaming的启动和调用`awaitTermination`方法结束，以启动流处理并等待处理终止：
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The data for this application is provided, as I stated previously, by the Linux
    `netcat` (`nc`) command. The Linux `cat` command dumps the contents of a log file,
    which is piped to `nc`. The `lk` options force `netcat` to listen for connections,
    and keep on listening if the connection is lost. This example shows that the port
    being used is `10777`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序的数据是由Linux的`netcat` (`nc`)命令提供的，正如我之前所说的。Linux的`cat`命令会将日志文件的内容转储到`nc`。`lk`选项强制`netcat`监听连接，并在连接丢失时继续监听。该示例显示使用的端口是`10777`：
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output from this TCP-based stream processing is shown here. The actual
    output is not as important as the method demonstrated. However, the data shows,
    as expected, a list of 10 log file words in descending count order. Note that
    the top word is empty because the stream was not filtered for empty words:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于TCP的流处理的输出如下所示。实际输出并不像所示方法那样重要。然而，数据显示了预期的结果，即按降序列出了10个日志文件单词。请注意，顶部的单词为空，因为流没有过滤空单词：
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is interesting if you want to stream data using Apache Spark streaming,
    based upon TCP/IP from a host and port. But what about more exotic methods? What
    if you wish to stream data from a messaging system, or via memory-based channels?
    What if you want to use some of the big data tools available today like Flume
    and Kafka? The next sections will examine these options, but first I will demonstrate
    how streams can be based upon files.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要使用Apache Spark流处理基于TCP/IP的主机和端口的流数据，这是很有趣的。但是更奇特的方法呢？如果您希望从消息系统或通过基于内存的通道流式传输数据呢？如果您想要使用今天可用的一些大数据工具，比如Flume和Kafka呢？接下来的章节将探讨这些选项，但首先我将演示如何基于文件创建流。
- en: File streams
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件流
- en: 'I have modified the Scala-based code example in the last section, to monitor
    an HDFS-based directory, by calling the Spark streaming context method called
    `textFileStream`. I will not display all of the code, given this small change.
    The application class is now called `stream3`, which takes a single parameter—the
    `HDFS` directory. The directory path could be on NFS or AWS S3 (all the code samples
    will be available with this book):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经修改了上一节中基于Scala的代码示例，通过调用Spark流上下文方法`textFileStream`来监视基于HDFS的目录。鉴于这个小改变，我不会显示所有的代码。应用程序类现在称为`stream3`，它接受一个参数——`HDFS`目录。目录路径可以是NFS或AWS
    S3（所有代码示例都将随本书提供）：
- en: '[PRE15]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The stream processing is the same as before. The stream is split into words,
    and the top-ten word list is printed. The only difference this time is that the
    data must be put into the `HDFS` directory while the application is running. This
    is achieved with the HDFS file system `put` command here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理与以前相同。流被分割成单词，并打印出前十个单词列表。这次唯一的区别是，在应用程序运行时，数据必须放入`HDFS`目录中。这是通过HDFS文件系统的`put`命令实现的：
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As you can see, the `HDFS` directory used is `/data/spark/stream/`, and the
    text-based source log file is `anaconda.storage.log` (under `/var/log/`). As expected,
    the same word list and count is printed:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，使用的`HDFS`目录是`/data/spark/stream/`，文本源日志文件是`anaconda.storage.log`（位于`/var/log/`下）。如预期的那样，打印出相同的单词列表和计数：
- en: '[PRE17]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: These are simple streaming methods based on TCP, and file system data. But what
    if I want to use some of the built-in streaming functionality within Spark streaming?
    This will be examined next. The Spark streaming Flume library will be used as
    an example.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是基于TCP和文件系统数据的简单流式处理方法。但是，如果我想要在Spark流处理中使用一些内置的流处理功能呢？接下来将对此进行检查。将使用Spark流处理Flume库作为示例。
- en: Flume
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flume
- en: 'Flume is an Apache open source project and product, which is designed to move
    large amounts of data at a big data scale. It is highly scalable, distributed,
    and reliable, working on the basis of data source, data sink, and data channels,
    as the diagram here, taken from the [http://flume.apache.org/](http://flume.apache.org/)
    website, shows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Flume是一个Apache开源项目和产品，旨在以大数据规模移动大量数据。它具有高度可扩展性、分布式和可靠性，基于数据源、数据汇和数据通道工作，如此图所示，取自[http://flume.apache.org/](http://flume.apache.org/)网站：
- en: '![Flume](img/B01989_03_03.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![Flume](img/B01989_03_03.jpg)'
- en: Flume uses agents to process data streams. As can be seen in the previous figure,
    an agent has a data source, a data processing channel, and a data sink. A clearer
    way to describe this is via the following figure. The channel acts as a queue
    for the sourced data and the sink passes the data to the next link in the chain.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Flume使用代理来处理数据流。如前图所示，代理具有数据源、数据处理通道和数据汇。更清晰地描述这一点的方法是通过以下图。通道充当源数据的队列，而汇将数据传递给链中的下一个链接。
- en: '![Flume](img/B01989_03_04.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Flume](img/B01989_03_04.jpg)'
- en: Flume agents can form Flume architectures; the output of one agent's sink can
    be the input to a second agent. Apache Spark allows two approaches to using Apache
    Flume. The first is an Avro push-based in-memory approach, whereas the second
    one, still based on Avro, is a pull-based system, using a custom Spark sink library.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Flume代理可以形成Flume架构；一个代理的sink的输出可以是第二个代理的输入。Apache Spark允许使用两种方法来使用Apache Flume。第一种是基于Avro的推送式内存方法，而第二种仍然基于Avro，是一个基于拉取的系统，使用自定义的Spark
    sink库。
- en: 'I installed Flume via the Cloudera CDH 5.3 cluster manager, which installs
    a single agent. Checking the Linux command line, I can see that Flume version
    1.5 is now available:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过Cloudera CDH 5.3集群管理器安装了Flume，它安装了一个单一代理。检查Linux命令行，我可以看到Flume版本1.5现在可用：
- en: '[PRE18]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The Flume-based Spark example that I will initially implement here, is the
    Flume-based push approach, where Spark acts as a receiver, and Flume pushes the
    data to Spark. The following figure represents the structure that I will implement
    on a single node:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在这里最初实现的基于Flume的Spark示例是基于Flume的推送方法，其中Spark充当接收器，Flume将数据推送到Spark。以下图表示我将在单个节点上实现的结构：
- en: '![Flume](img/B01989_03_05.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![Flume](img/B01989_03_05.jpg)'
- en: The message data will be sent to port `10777` on a host called `hc2r1m1` using
    the Linux `netcat` (`nc`) command. This will act as a source (`source1`) for the
    Flume agent (`agent1`), which will have an in-memory channel called `channel1`.
    The sink used by `agent1` will be Apache Avro based, again on a host called `hc2r1m1`,
    but this time, the port number will be `11777`. The Apache Spark Flume application
    `stream4` (which I will describe shortly) will listen for Flume stream data on
    this port.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 消息数据将使用Linux `netcat` (`nc`)命令发送到名为`hc2r1m1`的主机的端口`10777`。这将作为Flume代理(`agent1`)的源(`source1`)，它将有一个名为`channel1`的内存通道。`agent1`使用的sink将再次基于Apache
    Avro，但这次是在名为`hc2r1m1`的主机上，端口号将为`11777`。Apache Spark Flume应用程序`stream4`（我将很快描述）将在此端口上监听Flume流数据。
- en: 'I start the streaming process by executing the `netcat` (`nc`) command next,
    against the `10777` port. Now, when I type text into this window, it will be used
    as a Flume source, and the data will be sent to the Spark application:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过执行`netcat` (`nc`)命令来启动流处理过程，针对`10777`端口。现在，当我在此窗口中输入文本时，它将被用作Flume源，并且数据将被发送到Spark应用程序：
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In order to run my Flume agent, `agent1`, I have created a Flume configuration
    file called `agent1.flume.cfg`, which describes the agent's source, channel, and
    sink. The contents of the file are as follows. The first section defines the `agent1`
    source, channel, and sink names.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行我的Flume代理`agent1`，我创建了一个名为`agent1.flume.cfg`的Flume配置文件，描述了代理的源、通道和sink。文件的内容如下。第一部分定义了`agent1`的源、通道和sink名称。
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The next section defines `source1` to be netcat based, running on the host
    called `hc2r1m1`, and `10777` port:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节定义`source1`为基于netcat的，运行在名为`hc2r1m1`的主机上，端口为`10777`：
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `agent1` channel, `channel1`, is defined as a memory-based channel with
    a maximum event capacity of 1000 events:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`agent1`的通道`channel1`被定义为一个基于内存的通道，最大事件容量为1000个事件：'
- en: '[PRE22]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, the `agent1` sink, `sink1`, is defined as an Apache Avro sink on the
    host called `hc2r1m1`, and `11777` port:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`agent1`的sink `sink1` 被定义为在名为`hc2r1m1`的主机上的Apache Avro sink，并且端口为`11777`：
- en: '[PRE23]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'I have created a Bash script called `flume.bash` to run the Flume agent, `agent1`.
    It looks like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了一个名为`flume.bash`的Bash脚本来运行Flume代理`agent1`。它看起来像这样：
- en: '[PRE24]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The script calls the Flume executable `flume-ng`, passing the `agent1` configuration
    file. The call specifies the agent named `agent1`. It also specifies the Flume
    configuration directory to be `/etc/flume-ng/conf/`, the default value. Initially,
    I will use a `netcat` Flume source with a Scala-based example to show how data
    can be sent to an Apache Spark application. Then, I will show how an RSS-based
    data feed can be processed in a similar way. So initially, the Scala code that
    will receive the `netcat` data looks like this. The class package name and the
    application class name are defined. The necessary classes for Spark and Flume
    are imported. Finally, the main method is defined:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本调用Flume可执行文件`flume-ng`，传递`agent1`配置文件。调用指定了名为`agent1`的代理。它还指定了Flume配置目录为`/etc/flume-ng/conf/`，默认值。最初，我将使用基于Scala的`netcat`
    Flume源示例来展示数据如何被发送到Apache Spark应用程序。然后，我将展示如何以类似的方式处理基于RSS的数据源。因此，最初接收`netcat`数据的Scala代码如下。定义了类包名称和应用程序类名称。导入了Spark和Flume所需的类。最后，定义了主方法：
- en: '[PRE25]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The host and port name arguments for the data stream are checked and extracted:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 检查并提取了数据流的主机和端口名称参数：
- en: '[PRE26]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The Spark and streaming contexts are created. Then, the Flume-based data stream
    is created using the stream context host and port number. The Flume-based class
    `FlumeUtils` has been used to do this by calling it''s `createStream` method:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了Spark和流上下文。然后，使用流上下文主机和端口号创建了基于Flume的数据流。通过调用Flume基类`FlumeUtils`的`createStream`方法来实现这一点：
- en: '[PRE27]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, a stream event count is printed, and (for debug purposes while we
    test the stream) the stream content is dumped. After this, the stream context
    is started and configured to run until terminated via the application:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，打印了一个流事件计数，并（在我们测试流时用于调试目的）转储了流内容。之后，流上下文被启动并配置为在应用程序终止之前运行：
- en: '[PRE28]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Having compiled it, I will run this application using `spark-submit`. In the
    other chapters of this book, I will use a Bash-based script called `run_stream.bash`
    to execute the job. The script looks like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 编译完成后，我将使用`spark-submit`运行此应用程序。在本书的其他章节中，我将使用一个名为`run_stream.bash`的基于Bash的脚本来执行该作业。脚本如下所示：
- en: '[PRE29]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'So, this script sets some Spark-based variables, and a JAR library path for
    this job. It takes which Spark class to run, as its first parameter. It passes
    all the other variables, as parameters, to the Spark application class job. So,
    the execution of the application looks like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，此脚本设置了一些基于Spark的变量，并为此作业设置了JAR库路径。它将要运行的Spark类作为其第一个参数。它将所有其他变量作为参数传递给Spark应用程序类作业。因此，应用程序的执行如下所示：
- en: '[PRE30]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This means that the Spark application is ready, and is running as a Flume sink
    on port `11777`. The Flume input is ready, running as a netcat task on port `10777`.
    Now, the Flume agent, `agent1`, can be started using the Flume script called `flume.bash`
    to send the netcat source-based data to the Apache Spark Flume-based sink:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着Spark应用程序已准备就绪，并作为Flume接收器在端口`11777`上运行。Flume输入已准备就绪，作为端口`10777`上的netcat任务运行。现在，可以使用名为`flume.bash`的Flume脚本启动Flume代理`agent1`，以将netcat源数据发送到基于Apache
    Spark Flume的接收器：
- en: '[PRE31]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, when the text is passed to the netcat session, it should flow through
    Flume, and be processed as a stream by Spark. Let''s try it:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当文本传递到netcat会话时，它应该通过Flume流动，并由Spark作为流进行处理。让我们试一试：
- en: '[PRE32]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Three simple pieces of text have been added to the netcat session, and have
    been acknowledged with an `OK`, so that they can be passed to Flume. The debug
    output in the Flume session shows that the events (one per line ) have been received
    and processed:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 已经向netcat会话添加了三个简单的文本片段，并收到了`OK`的确认，以便它们可以传递给Flume。Flume会话中的调试输出显示已接收和处理了事件（每行一个）：
- en: '[PRE33]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, in the Spark `stream4` application session, three events have been
    received and processed. In this case, dumped to the session to prove the point
    that the data arrived. Of course, this is not what you would normally do, but
    I wanted to prove data transit through this configuration:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在Spark`stream4`应用程序会话中，已接收和处理了三个事件。在这种情况下，将其转储到会话以证明数据已到达。当然，这不是您通常会做的事情，但我想证明数据通过此配置传输：
- en: '[PRE34]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This is interesting, but it is not really a production-worthy example of Spark
    Flume data processing. So, in order to demonstrate a potentially real data processing
    approach, I will change the Flume configuration file source details so that it
    uses a Perl script, which is executable as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣，但实际上并不是一个生产值得的Spark Flume数据处理示例。因此，为了演示潜在的真实数据处理方法，我将更改Flume配置文件的源细节，以便使用一个Perl脚本，如下所示：
- en: '[PRE35]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The Perl script, which is referenced previously, `rss.perl`, just acts as a
    source of Reuters science news. It receives the news as XML, and converts it into
    JSON format. It also cleans the data of unwanted noise. First, it imports packages
    like LWP and `XML::XPath` to enable XML processing. Then, it specifies a science-based
    Reuters news data source, and creates a new LWP agent to process the data, similar
    to this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的Perl脚本`rss.perl`只是作为路透社科学新闻的数据源。它将新闻作为XML接收，并将其转换为JSON格式。它还清理了不需要的噪音数据。首先，导入了像LWP和`XML::XPath`这样的包以启用XML处理。然后，它指定了基于科学的路透社新闻数据源，并创建了一个新的LWP代理来处理数据，类似于这样：
- en: '[PRE36]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then an infinite while loop is opened, and an HTTP `GET` request is carried
    out against the URL. The request is configured, and the agent makes the request
    via a call to the request method:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后打开一个无限循环，对URL执行HTTP的`GET`请求。请求被配置，代理通过调用请求方法发出请求：
- en: '[PRE37]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'If the request is successful, then the XML data returned, is defined as the
    decoded content of the request. Title information is extracted from the XML, via
    an XPath call using the path called `/rss/channel/item/title`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果请求成功，那么返回的XML数据被定义为请求的解码内容。通过使用路径`/rss/channel/item/title`调用XPath来从XML中提取标题信息：
- en: '[PRE38]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'For each node in the extracted title data title XML string, data is extracted.
    It is cleaned of unwanted XML tags, and added to a Perl-based array called `titles`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从提取的标题数据标题XML字符串中的每个节点，都会提取数据。清除不需要的XML标签，并添加到名为`titles`的基于Perl的数组中：
- en: '[PRE39]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The same process is carried out for description-based data in the request response
    XML. The XPath value used this time is `/rss/channel/item/description/`. There
    are many more tags to be cleaned from the description data, so there are many
    more Perl searches, and line replacements that act on this data (`s///g`):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于请求响应XML中的基于描述的数据，进行相同的处理过程。这次使用的XPath值是`/rss/channel/item/description/`。需要清理的描述数据标签更多，因此有更多的Perl搜索和行替换操作（`s///g`）：
- en: '[PRE40]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, the XML-based title and description data is output in the RSS JSON
    format using a `print` command. The script then sleeps for 30 seconds, and requests
    more RSS news information to process:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，基于XML的标题和描述数据以RSS JSON格式输出，使用`print`命令。然后脚本休眠30秒，并请求更多的RSS新闻信息进行处理：
- en: '[PRE41]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'I have created a second Scala-based stream processing code example called `stream5`.
    It is similar to the `stream4` example, but it now processes the `rss` item data
    from the stream. A case class is defined next to process the category, title,
    and summary from the XML `rss` information. An html location is defined to store
    the resulting data that comes from the Flume channel:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经创建了第二个基于Scala的流处理代码示例，名为`stream5`。它类似于`stream4`示例，但现在它处理来自流的`rss`项数据。接下来定义了一个案例类来处理XML
    `rss`信息中的类别、标题和摘要。定义了一个HTML位置来存储来自Flume通道的结果数据：
- en: '[PRE42]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `rss` stream data from the Flume-based event is converted into a string.
    It is then formatted using the case class called `RSSItem`. If there is event
    data, it is then written to an HDFS directory using the previous `hdfsdir` path:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从基于Flume的事件的`rss`流数据转换为字符串。然后使用名为`RSSItem`的案例类进行格式化。如果有事件数据，那么将使用先前的`hdfsdir`路径将其写入HDFS目录：
- en: '[PRE43]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Running this code sample, it is possible to see that the Perl `rss` script
    is producing data, because the Flume script output indicates that 80 events have
    been accepted and received:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码示例，可以看到Perl `rss`脚本正在生成数据，因为Flume脚本输出表明已接受和接收了80个事件：
- en: '[PRE44]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The Scala Spark application `stream5` has processed 80 events in two batches:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Scala Spark应用程序`stream5`已经处理了80个事件，分为两批：
- en: '[PRE45]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'And the events have been stored on HDFS, under the expected directory, as the
    Hadoop file system `ls` command shows here:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 事件已存储在HDFS中，位于预期目录下，如Hadoop文件系统`ls`命令所示：
- en: '[PRE46]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Also, using the Hadoop file system `cat` command, it is possible to prove that
    the files on HDFS contain rss feed news-based data as shown here:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用Hadoop文件系统`cat`命令，可以证明HDFS上的文件包含rss feed新闻数据，如下所示：
- en: '[PRE47]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This Spark stream-based example has used Apache Flume to transmit data from
    an rss source, through Flume, to HDFS via a Spark consumer. This is a good example,
    but what if you want to publish data to a group of consumers? In the next section,
    I will examine Apache Kafka—a publish subscribe messaging system, and determine
    how it can be used with Spark.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基于Spark流的示例使用Apache Flume从rss源传输数据，通过Flume，通过Spark消费者传输到HDFS。这是一个很好的例子，但如果您想要向一组消费者发布数据怎么办？在下一节中，我将研究Apache
    Kafka——一个发布订阅消息系统，并确定它如何与Spark一起使用。
- en: Kafka
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka
- en: Apache Kafka ([http://kafka.apache.org/](http://kafka.apache.org/)) is a top
    level open-source project in Apache. It is a big data publish/subscribe messaging
    system that is fast and highly scalable. It uses message brokers for data management,
    and ZooKeeper for configuration, so that data can be organized into consumer groups
    and topics. Data in Kafka is split into partitions. In this example, I will demonstrate
    a receiver-less Spark-based Kafka consumer, so that I don't need to worry about
    configuring Spark data partitions when compared to my Kafka data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka ([http://kafka.apache.org/](http://kafka.apache.org/)) 是Apache中的一个顶级开源项目。它是一个快速且高度可扩展的大数据发布/订阅消息系统。它使用消息代理进行数据管理，并使用ZooKeeper进行配置，以便数据可以组织成消费者组和主题。Kafka中的数据被分成分区。在这个示例中，我将演示一个无接收器的基于Spark的Kafka消费者，因此与我的Kafka数据相比，我不需要担心配置Spark数据分区。
- en: In order to demonstrate Kafka-based message production and consumption, I will
    use the Perl RSS script from the last section as a data source. The data passing
    into Kafka and onto Spark will be Reuters RSS news data in the JSON format.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示基于Kafka的消息生产和消费，我将使用上一节中的Perl RSS脚本作为数据源。传递到Kafka并传递到Spark的数据将是JSON格式的Reuters
    RSS新闻数据。
- en: As topic messages are created by message producers, they are then placed in
    partitions in message order sequence. The messages in the partitions are retained
    for a configurable time period. Kafka then stores the offset value for each consumer,
    which is that consumer's position (in terms of message consumption) in that partition.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当消息生产者创建主题消息时，它们会按消息顺序顺序放置在分区中。分区中的消息将保留一段可配置的时间。Kafka然后为每个消费者存储偏移值，该值是该消费者在该分区中的位置（以消息消费为准）。
- en: 'I am currently using Cloudera''s CDH 5.3 Hadoop cluster. In order to install
    Kafka, I need to download a Kafka JAR library file from: [http://archive.cloudera.com/csds/kafka/](http://archive.cloudera.com/csds/kafka/).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我目前正在使用Cloudera的CDH 5.3 Hadoop集群。为了安装Kafka，我需要从[http://archive.cloudera.com/csds/kafka/](http://archive.cloudera.com/csds/kafka/)下载Kafka
    JAR库文件。
- en: 'Having downloaded the file, and given that I am using CDH cluster manager,
    I then need to copy the file to the `/opt/cloudera/csd/` directory on my NameNode
    CentOS server, so that it will be visible to install:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 下载文件后，鉴于我正在使用CDH集群管理器，我需要将文件复制到我的NameNode CentOS服务器上的`/opt/cloudera/csd/`目录，以便安装时可见：
- en: '[PRE48]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'I then need to restart the Cloudera cluster manager server on my NameNode,
    or master server, so that the change will be recognized. This was done as root
    using the service command, which is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我需要重新启动我的NameNode或主服务器上的Cloudera集群管理器服务器，以便识别更改。这是以root用户使用service命令完成的，命令如下：
- en: '[PRE49]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, the Kafka parcel should be visible within the CDH manager under **Hosts**
    | **Parcels**, as shown in the following figure. You can follow the usual download,
    distribution, and activate cycle for the CDH parcel installation:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Kafka包应该在CDH管理器的**主机** | **包裹**下可见，如下图所示。您可以按照CDH包安装的常规下载、分发和激活周期进行操作：
- en: '![Kafka](img/B01989_03_06.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![Kafka](img/B01989_03_06.jpg)'
- en: I have installed Kafka message brokers on each Data Node, or Spark Slave machine
    in my cluster. I then set the Kafka broker ID values for each Kafka broker server,
    giving them a `broker.id` number of 1 through 4\. As Kafka uses ZooKeeper for
    cluster data configuration, I wanted to keep all the Kafka data in a top level
    node called `kafka` in ZooKeeper. In order to do this, I set the Kafka ZooKeeper
    root value, called `zookeeper.chroot`, to `/kafka`. After making these changes,
    I restarted the CDH Kafka servers for the changes to take effect.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我在集群中的每个数据节点或Spark从节点机器上安装了Kafka消息代理。然后为每个Kafka代理服务器设置了Kafka代理ID值，分别为1到4。由于Kafka使用ZooKeeper进行集群数据配置，我希望将所有Kafka数据保留在ZooKeeper中名为`kafka`的顶级节点中。为了做到这一点，我将Kafka
    ZooKeeper根值设置为`zookeeper.chroot`，称为`/kafka`。在进行这些更改后，我重新启动了CDH Kafka服务器，以使更改生效。
- en: 'With Kafka installed, I can check the scripts available for testing. The following
    listing shows Kafka-based scripts for message producers and consumers, as well
    as scripts for managing topics, and checking consumer offsets. These scripts will
    be used in this section in order to demonstrate Kafka functionality:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了Kafka后，我可以检查可用于测试的脚本。以下清单显示了基于Kafka的消息生产者和消费者脚本，以及用于管理主题和检查消费者偏移的脚本。这些脚本将在本节中使用，以演示Kafka的功能：
- en: '[PRE50]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In order to run the installed Kafka servers, I need to have the broker server
    ID's (`broker.id`) values set, else an error will occur. Once Kafka is installed
    and running, I will need to prepare a message producer script. The simple Bash
    script given next, called `kafka.bash`, defines a comma-separated broker list
    of hosts and ports. It also defines a topic called `rss`. It then calls the Perl
    script `rss.perl` to generate the RSS-based data. This data is then piped into
    the Kafka producer script called `kafka-console-producer` to be sent to Kafka.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行已安装的Kafka服务器，我需要设置经纪人服务器ID（`broker.id`）值，否则将出现错误。安装并运行Kafka后，我需要准备一个消息生产者脚本。下面给出的简单Bash脚本名为`kafka.bash`，它定义了一个以逗号分隔的主机和端口的经纪人列表。它还定义了一个名为`rss`的主题。然后，它调用Perl脚本`rss.perl`生成基于RSS的数据。然后将这些数据传送到名为`kafka-console-producer`的Kafka生产者脚本以发送到Kafka。
- en: '[PRE51]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Notice that I have not mentioned Kafka topics at this point. When a topic is
    created in Kafka, the number of partitions can be specified. In the following
    example, the `kafka-topics` script has been called with the `create` option. The
    number of partitions have been set to `5`, and the data replication factor has
    been set to `3`. The ZooKeeper server string has been defined as `hc2r1m2-4` with
    a port number of `2181`. Also note that the top level ZooKeeper Kafka node has
    been defined as `/kafka` in the ZooKeeper string:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我还没有在这一点上提到Kafka主题。在Kafka中创建主题时，可以指定分区的数量。在下面的示例中，使用`create`选项调用了`kafka-topics`脚本。分区的数量设置为`5`，数据复制因子设置为`3`。ZooKeeper服务器字符串已定义为`hc2r1m2-4`，端口号为`2181`。还要注意，顶级ZooKeeper
    Kafka节点在ZooKeeper字符串中被定义为`/kafka`：
- en: '[PRE52]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'I have also created a Bash script called `kafka_list.bash` for use during testing,
    which checks all the Kafka topics that have been created, and also the Kafka consumer
    offsets. It calls the `kafka-topics` commands with a `list` option, and a `ZooKeeper`
    string to get a list of created topics. It then calls the Kafka script called
    `kafka-consumer-offset-checker` with a `ZooKeeper` string—the topic name and a
    group name to get a list of consumer offset values. Using this script, I can check
    that my topics are created, and the topic data is being consumed correctly:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我还创建了一个名为`kafka_list.bash`的Bash脚本，用于测试时检查已创建的所有Kafka主题以及Kafka消费者偏移。它使用`kafka-topics`命令调用`list`选项和`ZooKeeper`字符串来获取已创建主题的列表。然后，它使用Kafka脚本`kafka-consumer-offset-checker`调用`ZooKeeper`字符串、主题名称和组名称来获取消费者偏移值的列表。使用此脚本，我可以检查我的主题是否已创建，并且主题数据是否被正确消耗：
- en: '[PRE53]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, I need to create the Apache Spark Scala-based Kafka consumer code. As
    I said, I will create a receiver-less example, so that the Kafka data partitions
    match in both, Kafka and Spark. The example is called `stream6`. First, the package
    is defined, and the classes are imported for Kafka, spark, context, and streaming.
    Then, the object class called `stream6`, and the main method are defined. The
    code looks like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我需要创建基于Apache Spark Scala的Kafka消费者代码。正如我所说的，我将创建一个无接收器的示例，以便Kafka数据分区在Kafka和Spark中匹配。示例被称为`stream6`。首先，定义了包，并导入了Kafka、spark、context和streaming的类。然后，定义了名为`stream6`的对象类和主方法。代码如下：
- en: '[PRE55]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, the class parameters (broker''s string, group ID, and topic) are checked
    and processed. If the class parameters are incorrect, then an error is printed,
    and execution stops, else the parameter variables are defined:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，检查和处理了类参数（经纪人字符串、组ID和主题）。如果类参数不正确，则打印错误并停止执行，否则定义参数变量：
- en: '[PRE56]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The Spark context is defined in terms of an application name. Again the Spark
    URL has been left as the default. The streaming context has been created using
    the Spark context. I have left the stream batch interval at 10 seconds, which
    is the same as the last example. However, you can set it using a parameter of
    your choice:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Spark上下文根据应用程序名称进行了定义。同样，Spark URL保持默认值。使用Spark上下文创建了流上下文。我将流批处理间隔保持为10秒，与上一个示例相同。但是，您可以使用自己选择的参数进行设置：
- en: '[PRE57]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Next, the broker list and group ID are set up as parameters. These values are
    then used to create a Kafka-based Spark stream called `rawDStream`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，设置了经纪人列表和组ID作为参数。然后使用这些值创建了一个名为`rawDStream`的基于Kafka的Spark流：
- en: '[PRE58]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'I have again printed the stream event count for debug purposes, so that I know
    when the application is receiving and processing the data:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 出于调试目的，我再次打印了流事件计数，以便我知道应用程序何时接收和处理数据。
- en: '[PRE59]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The HDSF location for the Kafka data has been defined as `/data/spark/kafka/rss/`.
    It has been mapped from the DStream into the variable lines. Using the `foreachRDD`
    method, a check on the data count is carried out on the `lines` variable, before
    saving the data into HDFS using the `saveAsTextFile` method:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka数据的HDSF位置已定义为`/data/spark/kafka/rss/`。它已从DStream映射到变量`lines`。使用`foreachRDD`方法，在`lines`变量上进行数据计数检查，然后使用`saveAsTextFile`方法将数据保存到HDFS中。
- en: '[PRE60]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Finally, the Scala script closes by starting the stream processing, and setting
    the application class to run until terminated with `awaitTermination`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Scala脚本通过启动流处理并将应用程序类设置为使用`awaitTermination`直到终止来关闭：
- en: '[PRE61]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'With all of the scripts explained and the Kafka CDH brokers running, it is
    time to examine the Kafka configuration, which if you remember is maintained by
    Apache ZooKeeper (all of the code samples that have been described so far will
    be released with the book). I will use the `zookeeper-client` tool, and connect
    to the `zookeeper` server on the host called `hc2r1m2` on the `2181` port. As
    you can see here, I have received a connected message from the `client` session:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释了所有脚本并运行了Kafka CDH代理之后，现在是时候检查Kafka配置了，您可能还记得这是由Apache ZooKeeper维护的（迄今为止描述的所有代码示例都将随本书一起发布）。我将使用`zookeeper-client`工具，并连接到名为`hc2r1m2`的主机上的`2181`端口上的`zookeeper`服务器。如您在此处所见，我已从`client`会话收到了连接消息。
- en: '[PRE62]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'If you remember, I specified the top level ZooKeeper directory for Kafka to
    be `/kafka`. If I examine this now via a client session, I can see the Kafka ZooKeeper
    structure. I will be interested in `brokers` (the CDH Kafka broker servers), and
    `consumers` (the previous Spark Scala code). The ZooKeeper `ls` commands show
    that the four Kafka servers have registered with ZooKeeper, and are listed by
    their `broker.id` configuration values one to four:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您记得，我指定了Kafka的顶级ZooKeeper目录为`/kafka`。如果我现在通过客户端会话检查这一点，我可以看到Kafka ZooKeeper结构。我将对`brokers`（CDH
    Kafka代理服务器）和`consumers`（先前的Spark Scala代码）感兴趣。ZooKeeper `ls`命令显示，四个Kafka服务器已在ZooKeeper中注册，并按其`broker.id`配置值从一到四列出。
- en: '[PRE63]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'I will create the topic that I want to use for this test using the Kafka script
    `kafka-topics` with a `create` flag. I do this manually, because I can demonstrate
    the definition of the data partitions while I do it. Note that I have set the
    partitions in the Kafka `topic rss` to five as shown in the following piece of
    code. Note also that the ZooKeeper connection string for the command has a comma-separated
    list of ZooKeeper servers, terminated by the top level ZooKeeper Kafka directory
    called `/kafka`. This means that the command puts the new topic in the proper
    place:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用Kafka脚本`kafka-topics`和`create`标志创建我想要用于此测试的主题。我这样做是因为我可以在手动操作时演示数据分区的定义。请注意，我已经在Kafka
    `topic rss`中设置了五个分区，如下面的代码所示。还要注意，命令的ZooKeeper连接字符串是由逗号分隔的ZooKeeper服务器列表组成的，以`/kafka`结尾，这意味着命令将新主题放在适当的位置。
- en: '[PRE64]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, when I use the ZooKeeper client to check the Kafka topic configuration,
    I can see the correct topic name, and the expected number of the partitions:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我使用ZooKeeper客户端检查Kafka主题配置时，我可以看到正确的主题名称和预期的分区数。
- en: '[PRE65]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This describes the configuration for the Kafka broker servers in ZooKeeper,
    but what about the data consumers? Well, the following listing shows where the
    data will be held. Remember though, at this time, there is no consumer running,
    so it is not represented in ZooKeeper:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这描述了ZooKeeper中Kafka代理服务器的配置，但数据消费者的情况如何呢？好吧，以下清单显示了数据将被保存的位置。但请记住，此时没有运行消费者，因此在ZooKeeper中没有表示。
- en: '[PRE66]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In order to start this test, I will run my Kafka data producer, and consumer
    scripts. I will also check the output of the Spark application class and need
    to check the Kafka partition offsets and HDFS to make sure that the data has arrived.
    This is quite complicated, so I will add a diagram here in the following figure
    to explain the test architecture.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始这个测试，我将运行我的Kafka数据生产者和消费者脚本。我还需要检查Spark应用程序类的输出，并需要检查Kafka分区偏移和HDFS，以确保数据已到达。这非常复杂，所以我将在下图中添加一个图表来解释测试架构。
- en: The Perl script called `rss.perl` will be used to provide a data source for
    a Kafka data producer, which will feed data into the CDH Kafka broker servers.
    The data will be stored in ZooKeeper, in the structure that has just been examined,
    under the top level node called `/kafka`. The Apache Spark Scala-based application
    will then act as a Kafka consumer, and read the data that it will store under
    HDFS.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 名为`rss.perl`的Perl脚本将用于为Kafka数据生产者提供数据源，该数据生产者将数据提供给CDH Kafka代理服务器。数据将存储在ZooKeeper中，结构刚刚在顶级节点`/kafka`下进行了检查。然后，基于Apache
    Spark Scala的应用程序将充当Kafka消费者，并读取将存储在HDFS中的数据。
- en: '![Kafka](img/B01989_03_07.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![Kafka](img/B01989_03_07.jpg)'
- en: 'In order to try and explain the complexity here, I will also examine my method
    of running the Apache Spark class. It will be started via the `spark-submit` command.
    Remember again that all of these scripts will be released with this book, so that
    you can examine them in your own time. I always use scripts for server test management,
    so that I encapsulate complexity, and command execution is quickly repeatable.
    The script, `run_stream.bash`, is like many example scripts that have already
    been used in this chapter, and this book. It accepts a class name and the class
    parameters, and runs the class via spark-submit:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试解释这里的复杂性，我还将检查运行Apache Spark类的方法。它将通过`spark-submit`命令启动。请再次记住，所有这些脚本都将随本书一起发布，这样您就可以在自己的时间内对它们进行检查。我总是使用脚本进行服务器测试管理，以便封装复杂性，并且命令执行可以快速重复。脚本`run_stream.bash`类似于本章和本书中已经使用过的许多示例脚本。它接受一个类名和类参数，并通过spark-submit运行该类。
- en: '[PRE67]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'I then used a second script, which calls the `run_kafka_example.bash` script
    to execute the Kafka consumer code in the previous `stream6` application class.
    Note that this script sets up the full application class name—the broker server
    list. It also sets up the topic name, called `rss`, to use for data consumption.
    Finally, it defines a consumer group called `group1`. Remember that Kafka is a
    publish/subscribe message brokering system. There may be many producers and consumers
    organized by topic, group, and partition:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我使用了第二个脚本，调用`run_kafka_example.bash`脚本来执行先前`stream6`应用程序类中的Kafka消费者代码。请注意，此脚本设置了完整的应用程序类名-代理服务器列表。它还设置了一个名为`rss`的主题名称，用于数据消耗。最后，它定义了一个名为`group1`的消费者组。请记住，Kafka是一个发布/订阅消息代理系统。可以通过主题、组和分区组织许多生产者和消费者：
- en: '[PRE68]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'So, I will start the Kafka consumer by running the `run_kafka_example.bash`
    script, which in turn will run the previous `stream6` Scala code using spark-submit.
    While monitoring Kafka data consumption using the script called `kafka_list.bash`,
    I was able to get the `kafka-consumer-offset-checker` script to list the Kafka-based
    topics, but for some reason, it will not check the correct path (under `/kafka`
    in ZooKeeper) when checking the offsets as shown here:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我将通过运行`run_kafka_example.bash`脚本来启动Kafka消费者，然后将运行先前的`stream6` Scala代码使用spark-submit。在使用名为`kafka_list.bash`的脚本监视Kafka数据消耗时，我能够让`kafka-consumer-offset-checker`脚本列出基于Kafka的主题，但由于某种原因，它在检查偏移时不会检查正确的路径（在ZooKeeper中的`/kafka`下）如下所示：
- en: '[PRE69]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'By starting the Kafka producer rss feed using the script `kafka.bash`, I can
    now start feeding the rss-based data through Kafka into Spark, and then into HDFS.
    Periodically checking the `spark-submit` session output it can be seen that events
    are passing through the Spark-based Kafka DStream. The following output comes
    from the stream count in the Scala code, and shows that at that point, 28 events
    were processed:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`kafka.bash`脚本启动Kafka生产者rss feed，我现在可以开始通过Kafka将基于rss的数据馈送到Spark，然后进入HDFS。定期检查`spark-submit`会话输出，可以看到事件通过基于Spark的Kafka
    DStream传递。下面的输出来自Scala代码中的流计数，并显示在那一点上，处理了28个事件：
- en: '[PRE70]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'By checking HDFS under the `/data/spark/kafka/rss/` directory, via the Hadoop
    file system `ls` command, it can be seen that there is now data stored on HDFS:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`/data/spark/kafka/rss/`目录下检查HDFS，通过Hadoop文件系统`ls`命令，可以看到现在在HDFS上存储了数据：
- en: '[PRE71]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'By checking the contents of this directory, it can be seen that an HDFS part
    data file exists, which should contain the RSS-based data from Reuters:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查这个目录的内容，可以看到存在一个HDFS部分数据文件，应该包含来自路透社的基于RSS的数据：
- en: '[PRE72]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Using the Hadoop file system `cat` command below, I can dump the contents of
    this HDFS-based file to check its contents. I have used the Linux `head` command
    to limit the data to save space. Clearly this is RSS Reuters science based information
    that the Perl script `rss.perl` has converted from XML to RSS JSON format.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用下面的Hadoop文件系统`cat`命令，我可以转储这个基于HDFS的文件的内容以检查其内容。我已经使用了Linux的`head`命令来限制数据以节省空间。显然，这是Perl脚本`rss.perl`从XML转换为RSS
    JSON格式的RSS路透社科学信息。
- en: '[PRE73]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This ends this Kafka example. It can be seen that Kafka brokers have been installed
    and configured. It shows that an RSS data-based Kafka producer has fed data into
    the brokers. It has been proved, using the ZooKeeper client, that the Kafka architecture,
    matching the brokers, topics, and partitions has been set up in ZooKeeper. Finally,
    it has been shown using the Apache Spark-based Scala code, in the `stream6` application,
    that the Kafka data has been consumed and saved to HDFS.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了这个Kafka示例。可以看到Kafka代理已经安装和配置。它显示了一个基于RSS数据的Kafka生产者已经将数据馈送到代理中。使用ZooKeeper客户端已经证明了Kafka架构，匹配代理、主题和分区已经在ZooKeeper中设置。最后，使用基于Apache
    Spark的Scala代码，在`stream6`应用程序中已经显示了Kafka数据已被消耗并保存到HDFS中。
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: I could have provided streaming examples for systems like Kinesis, as well as
    queuing systems, but there was not room in this chapter. Twitter streaming has
    been examined by example in the checkpointing section.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我本可以提供像Kinesis这样的系统的流式示例，以及排队系统，但在本章中没有足够的空间。Twitter流已经在检查点部分的示例中进行了检查。
- en: This chapter has provided practical examples of data recovery via checkpointing
    in Spark streaming. It has also touched on the performance limitations of checkpointing
    and shown that that the checkpointing interval should be set at five to ten times
    the Spark stream batch interval. Checkpointing provides a stream-based recovery
    mechanism in the case of Spark application failure.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了通过Spark流检查点进行数据恢复的实际示例。它还触及了检查点的性能限制，并表明检查点间隔应设置为Spark流批处理间隔的五到十倍。检查点提供了一种基于流的恢复机制，以防Spark应用程序失败。
- en: This chapter has provided some stream-based worked examples for TCP, File, Flume,
    and Kafka-based Spark stream coding. All the examples here are based on Scala,
    and are compiled with `sbt`. All of the code will be released with this book.
    Where the example architecture has become over-complicated, I have provided an
    architecture diagram (I'm thinking of the Kafka example here).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了一些基于流的TCP、文件、Flume和Kafka的Spark流编码示例。这里的所有示例都是基于Scala的，并且使用`sbt`进行编译。所有的代码都将随本书一起发布。当示例架构变得过于复杂时，我提供了一个架构图（我在这里考虑的是Kafka示例）。
- en: It is clear to me that the Apache Spark streaming module contains a rich source
    of functionality that should meet most of your needs, and will grow as future
    releases of Spark are delivered. Remember to check the Apache Spark website ([http://spark.apache.org/](http://spark.apache.org/)),
    and join the Spark user list via `<[user@spark.apache.org](mailto:user@spark.apache.org)>`.
    Don't be afraid to ask questions, or make mistakes, as it seems to me that mistakes
    teach more than success.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，Apache Spark流模块包含了丰富的功能，应该能满足大部分需求，并且随着未来版本的Spark发布而不断增长。记得查看Apache Spark网站（[http://spark.apache.org/](http://spark.apache.org/)），并通过`<[user@spark.apache.org](mailto:user@spark.apache.org)>`加入Spark用户列表。不要害怕提问，或犯错误，因为在我看来，错误教会的比成功多。
- en: The next chapter will examine the Spark SQL module, and will provide worked
    examples of SQL, data frames, and accessing Hive among other topics.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将审查Spark SQL模块，并提供SQL、数据框架和访问Hive等主题的实例。
