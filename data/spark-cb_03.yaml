- en: Chapter 3. External Data Sources
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 外部数据源
- en: One of the strengths of Spark is that it provides a single runtime that can
    connect with various underlying data sources.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的一个优点是它提供了一个可以连接各种底层数据源的单一运行时。
- en: 'In this chapter, we will connect to different data sources. This chapter is
    divided into the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将连接到不同的数据源。本章分为以下几个示例：
- en: Loading data from the local filesystem
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从本地文件系统加载数据
- en: Loading data from HDFS
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从HDFS加载数据
- en: Loading data from HDFS using a custom InputFormat
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义InputFormat从HDFS加载数据
- en: Loading data from Amazon S3
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从亚马逊S3加载数据
- en: Loading data from Apache Cassandra
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Apache Cassandra加载数据
- en: Loading data from relational databases
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从关系数据库加载数据
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Spark provides a unified runtime for big data. HDFS, which is Hadoop's filesystem,
    is the most used storage platform for Spark as it provides cost-effective storage
    for unstructured and semi-structured data on commodity hardware. Spark is not
    limited to HDFS and can work with any Hadoop-supported storage.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Spark为大数据提供了统一的运行时。HDFS，即Hadoop的文件系统，是Spark最常用的存储平台，因为它提供了成本效益的存储方式，可以在通用硬件上存储非结构化和半结构化数据。Spark不仅限于HDFS，还可以与任何Hadoop支持的存储一起使用。
- en: Hadoop supported storage means a storage format that can work with Hadoop's
    `InputFormat` and `OutputFormat` interfaces. `InputFormat` is responsible for
    creating `InputSplits` from input data and dividing it further into records. `OutputFormat`
    is responsible for writing to storage.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop支持的存储意味着可以与Hadoop的`InputFormat`和`OutputFormat`接口一起使用的存储格式。`InputFormat`负责从输入数据创建`InputSplits`，并将其进一步划分为记录。`OutputFormat`负责写入存储。
- en: 'We will start with writing to the local filesystem and then move over to loading
    data from HDFS. In the *Loading data from HDFS* recipe, we will cover the most
    common file format: regular text files. In the next recipe, we will cover how
    to use any `InputFormat` interface to load data in Spark. We will also explore
    loading data stored in Amazon S3, a leading cloud storage platform.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从本地文件系统开始写入，然后转移到从HDFS加载数据。在*从HDFS加载数据*的示例中，我们将介绍最常见的文件格式：常规文本文件。在下一个示例中，我们将介绍如何在Spark中使用任何`InputFormat`接口来加载数据。我们还将探讨如何加载存储在亚马逊S3中的数据，这是一个领先的云存储平台。
- en: We will explore loading data from Apache Cassandra, which is a NoSQL database.
    Finally, we will explore loading data from a relational database.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索从Apache Cassandra加载数据，这是一个NoSQL数据库。最后，我们将探索从关系数据库加载数据。
- en: Loading data from the local filesystem
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从本地文件系统加载数据
- en: Though the local filesystem is not a good fit to store big data due to disk
    size limitations and lack of distributed nature, technically you can load data
    in distributed systems using the local filesystem. But then the file/directory
    you are accessing has to be available on each node.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本地文件系统不适合存储大数据，因为磁盘大小限制和缺乏分布式特性，但从技术上讲，你可以使用本地文件系统在分布式系统中加载数据。但是你要访问的文件/目录必须在每个节点上都可用。
- en: Please note that if you are planning to use this feature to load side data,
    it is not a good idea. To load side data, Spark has a broadcast variable feature,
    which will be discussed in upcoming chapters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您计划使用此功能来加载辅助数据，这不是一个好主意。为了加载辅助数据，Spark有一个广播变量功能，将在接下来的章节中讨论。
- en: In this recipe, we will look at how to load data in Spark from the local filesystem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将看看如何从本地文件系统中加载数据到Spark中。
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s start with the example of Shakespeare''s "to be or not to be":'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从莎士比亚的"to be or not to be"的例子开始：
- en: 'Create the `words` directory by using the following command:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建`words`目录：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Get into the `words` directory:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入`words`目录：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create the `sh.txt` text file and enter `"to be or not to be"` in it:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`sh.txt`文本文件，并在其中输入`"to be or not to be"`：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Start the Spark shell:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Load the `words` directory as RDD:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`words`目录加载为RDD：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Count the number of lines:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算行数：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Divide the line (or lines) into multiple words:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将行（或行）分成多个单词：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Convert `word` to (word,1)—that is, output `1` as the value for each occurrence
    of `word` as a key:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`word`转换为（word,1）—即，将`1`作为每个`word`的出现次数的值输出为键：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Use the `reduceByKey` method to add the number of occurrences for each word
    as a key (this function works on two consecutive values at a time, represented
    by `a` and `b`):'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`reduceByKey`方法将每个单词的出现次数作为键添加（此函数一次处理两个连续的值，表示为`a`和`b`）：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Print the RDD:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印RDD：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Doing all of the preceding operations in one step is as follows:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个步骤中执行所有前面的操作如下：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives the following output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![How to do it...](img/B03056_03_01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/B03056_03_01.jpg)'
- en: Loading data from HDFS
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从HDFS加载数据
- en: HDFS is the most widely used big data storage system. One of the reasons for
    the wide adoption of HDFS is schema-on-read. What this means is that HDFS does
    not put any restriction on data when data is being written. Any and all kinds
    of data are welcome and can be stored in a raw format. This feature makes it ideal
    storage for raw unstructured data and semi-structured data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS是最广泛使用的大数据存储系统。HDFS被广泛采用的原因之一是模式在读取时。这意味着在写入数据时，HDFS不会对数据施加任何限制。任何类型的数据都受欢迎并且可以以原始格式存储。这个特性使其成为原始非结构化数据和半结构化数据的理想存储介质。
- en: When it comes to reading data, even unstructured data needs to be given some
    structure to make sense. Hadoop uses `InputFormat` to determine how to read the
    data. Spark provides complete support for Hadoop's `InputFormat` so anything that
    can be read by Hadoop can be read by Spark as well.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取数据方面，即使是非结构化数据也需要给予一些结构以理解。Hadoop使用`InputFormat`来确定如何读取数据。Spark完全支持Hadoop的`InputFormat`，因此任何Hadoop可以读取的内容也可以被Spark读取。
- en: The default `InputFormat` is `TextInputFormat`. `TextInputFormat` takes the
    byte offset of a line as a key and the content of a line as a value. Spark uses
    the `sc.textFile` method to read using `TextInputFormat`. It ignores the byte
    offset and creates an RDD of strings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的`InputFormat`是`TextInputFormat`。`TextInputFormat`将行的字节偏移量作为键，行的内容作为值。Spark使用`sc.textFile`方法使用`TextInputFormat`进行读取。它忽略字节偏移量并创建一个字符串的RDD。
- en: Sometimes the filename itself contains useful information, for example, time-series
    data. In that case, you may want to read each file separately. The `sc.wholeTextFiles`
    method allows you to do that. It creates an RDD with the filename and path (for
    example, `hdfs://localhost:9000/user/hduser/words`) as a key and the content of
    the whole file as the value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有时文件名本身包含有用的信息，例如时间序列数据。在这种情况下，您可能希望单独读取每个文件。`sc.wholeTextFiles`方法允许您这样做。它创建一个RDD，其中文件名和路径（例如`hdfs://localhost:9000/user/hduser/words`）作为键，整个文件的内容作为值。
- en: Spark also supports reading various serialization and compression-friendly formats
    such as Avro, Parquet, and JSON using DataFrames. These formats will be covered
    in coming chapters.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Spark还支持使用DataFrame读取各种序列化和压缩友好的格式，如Avro、Parquet和JSON。这些格式将在接下来的章节中介绍。
- en: In this recipe, we will look at how to load data in the Spark shell from HDFS.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将学习如何从HDFS中的Spark shell加载数据。
- en: How to do it...
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s do the word count, which counts the number of occurrences of each word.
    In this recipe, we will load data from HDFS:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行单词计数，计算每个单词的出现次数。在本教程中，我们将从HDFS加载数据。
- en: 'Create the `words` directory by using the following command:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建`words`目录：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Change the directory to `words`:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改目录到`words`：
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create the `sh.txt text` file and enter `"to be or not to be"` in it:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`sh.txt text`文件并在其中输入`"to be or not to be"`：
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Start the Spark shell:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Load the `words` directory as the RDD:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`words`目录加载为RDD：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `sc.textFile` method also supports passing an additional argument for the
    number of partitions. By default, Spark creates one partition for each `InputSplit`
    class, which roughly corresponds to one block.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`sc.textFile`方法还支持传递用于分区数的额外参数。默认情况下，Spark为每个`InputSplit`类创建一个分区，这大致对应一个块。'
- en: You can ask for a higher number of partitions. It works really well for compute-intensive
    jobs such as in machine learning. As one partition cannot contain more than one
    block, having fewer partitions than blocks is not allowed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以要求更多的分区。这对于计算密集型作业（如机器学习）非常有效。由于一个分区不能包含多个块，因此分区数不能少于块数。
- en: 'Count the number of lines (the result will be `1`):'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算行数（结果将为`1`）：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Divide the line (or lines) into multiple words:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将行（或行）分成多个单词：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Convert word to (word,1)—that is, output `1` as a value for each occurrence
    of `word` as a key:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单词转换为(word,1)——也就是说，将`word`作为键的每次出现输出`1`作为值：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Use the `reduceByKey` method to add the number of occurrences of each word
    as a key (this function works on two consecutive values at a time, represented
    by `a` and `b`):'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`reduceByKey`方法将每个单词的出现次数作为键相加（此函数一次处理两个连续的值，由`a`和`b`表示）：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Print the RDD:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印RDD：
- en: '[PRE20]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Doing all of the preceding operations in one step is as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一步中执行所有前面的操作如下：
- en: '[PRE21]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This gives the following output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![How to do it...](img/B03056_03_01.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/B03056_03_01.jpg)'
- en: There's more…
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Sometimes we need to access the whole file at once. Sometimes the filename contains
    useful data like in the case of time-series. Sometimes you need to process more
    than one line as a record. `sparkContext.wholeTextFiles` comes to the rescue here.
    We will look at weather dataset from [ftp://ftp.ncdc.noaa.gov/pub/data/noaa/](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要一次访问整个文件。有时文件名包含有用的数据，比如时间序列。有时您需要将多行作为一个记录进行处理。`sparkContext.wholeTextFiles`在这里派上了用场。我们将查看来自[ftp://ftp.ncdc.noaa.gov/pub/data/noaa/](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/)的天气数据集。
- en: 'Here''s what a top-level directory looks like:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层目录的样子如下：
- en: '![There''s more…](img/B03056_03_02.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多...](img/B03056_03_02.jpg)'
- en: 'Looking into a particular year directory—for example, 1901 resembles the following
    screenshot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 查看特定年份目录，例如1901年，如下截图所示：
- en: '![There''s more…](img/B03056_03_03.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多...](img/B03056_03_03.jpg)'
- en: Data here is divided in such a way that each filename contains useful information,
    that is, USAF-WBAN-year, where USAF is the US air force station number and WBAN
    is the weather bureau army navy location number.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的数据被划分为每个文件名包含有用信息的方式，即USAF-WBAN-year，其中USAF是美国空军站点编号，WBAN是天气局陆军海军位置编号。
- en: You will also notice that all files are compressed as gzip with a `.gz` extension.
    Compression is handled automatically so all you need to do is to upload data in
    HDFS. We will come back to this dataset in the coming chapters.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会注意到所有文件都以`.gz`扩展名压缩为gzip。压缩是自动处理的，所以您只需要将数据上传到HDFS。我们将在接下来的章节中回到这个数据集。
- en: 'Since the whole dataset is not large, it can be uploaded in HDFS in the pseudo-distributed
    mode also:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于整个数据集并不大，因此也可以在伪分布式模式下上传到HDFS中：
- en: 'Download data:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据：
- en: '[PRE22]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Load the weather data in HDFS:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在HDFS中加载天气数据：
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Start the Spark shell:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Load weather data for 1901 in the RDD:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在RDD中加载1901年的天气数据：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Cache weather in the RDD so that it is not recomputed every time it''s accessed:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将天气缓存在RDD中，以便每次访问时不需要重新计算：
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In Spark, there are various StorageLevels at which the RDD can be persisted.
    `rdd.cache` is a shorthand for the `rdd.persist(MEMORY_ONLY)` StorageLevel.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，RDD可以持久化在各种StorageLevels上。`rdd.cache`是`rdd.persist(MEMORY_ONLY)` StorageLevel的简写。
- en: 'Count the number of elements:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算元素的数量：
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Since the whole contents of a file are loaded as an element, we need to manually
    interpret the data, so let''s load the first element:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于整个文件的内容被加载为一个元素，我们需要手动解释数据，因此让我们加载第一个元素：
- en: '[PRE28]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Read the value of the first RDD:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取第一个RDD的值：
- en: '[PRE29]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `firstElement` contains tuples in the form (string, string). Tuples can
    be accessed in two ways:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`firstElement`包含以(string, string)形式的元组。元组可以通过两种方式访问：'
- en: Using a positional function starting with `_1`.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从`_1`开始的位置函数。
- en: Using the `productElement` method, for example, `tuple.productElement(0)`. Indexes
    here start with `0` like most other methods.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`productElement`方法，例如`tuple.productElement(0)`。这里的索引从`0`开始，就像大多数其他方法一样。
- en: 'Split `firstValue` by lines:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过行来分割`firstValue`：
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Count the number of elements in `firstVals`:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`firstVals`中的元素数量：
- en: '[PRE31]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The schema of weather data is very rich with the position of the text working
    as a delimiter. You can get more information about schemas at the national weather
    service website. Let''s get wind speed, which is from section 66-69 (in meter/sec):'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 天气数据的模式非常丰富，文本的位置作为分隔符。您可以在国家气象局网站上获取有关模式的更多信息。让我们获取风速，它来自66-69节（以米/秒为单位）：
- en: '[PRE32]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Loading data from HDFS using a custom InputFormat
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义InputFormat从HDFS加载数据
- en: 'Sometimes you need to load data in a specific format and `TextInputFormat`
    is not a good fit for that. Spark provides two methods for this purpose:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有时您需要以特定格式加载数据，而`TextInputFormat`不适合。Spark为此提供了两种方法：
- en: '`sparkContext.hadoopFile`: This supports the old MapReduce API'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sparkContext.hadoopFile`：支持旧的MapReduce API'
- en: '`sparkContext.newAPIHadoopFile`: This supports the new MapReduce API'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sparkContext.newAPIHadoopFile`：支持新的MapReduce API'
- en: These two methods provide support for all of Hadoop's built-in InputFormats
    interfaces as well as any custom `InputFormat`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法支持所有Hadoop内置的InputFormats接口以及任何自定义`InputFormat`。
- en: How to do it...
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We are going to load text data in key-value format and load it in Spark using
    `KeyValueTextInputFormat`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以键值格式加载文本数据，并使用`KeyValueTextInputFormat`将其加载到Spark中：
- en: 'Create the `currency` directory by using the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建`currency`目录：
- en: '[PRE33]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Change the current directory to `currency`:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将当前目录更改为`currency`：
- en: '[PRE34]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create the `na.txt` text file and enter currency values in key-value format
    delimited by tab (key: country, value: currency):'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`na.txt`文本文件，并以制表符分隔的键值格式输入货币值（键：国家，值：货币）：
- en: '[PRE35]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You can create more files for each continent.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为每个大陆创建更多的文件。
- en: 'Upload the `currency` folder to HDFS:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`currency`文件夹上传到HDFS：
- en: '[PRE36]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Start the Spark shell:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE37]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Import statements:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入语句：
- en: '[PRE38]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Load the `currency` directory as the RDD:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`currency`目录加载为RDD：
- en: '[PRE39]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Convert it from tuple of (Text,Text) to tuple of (String,String):'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其从（Text，Text）元组转换为（String，String）元组：
- en: '[PRE40]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Count the number of elements in the RDD:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算RDD中的元素数量：
- en: '[PRE41]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Print the values:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印值：
- en: '[PRE42]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![How to do it...](img/B03056_03_04.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/B03056_03_04.jpg)'
- en: Note
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can use this approach to load data in any Hadoop-supported `InputFormat`
    interface.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用此方法加载任何Hadoop支持的`InputFormat`接口的数据。
- en: Loading data from Amazon S3
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Amazon S3加载数据
- en: Amazon **Simple Storage Service** (**S3**) provides developers and IT teams
    with a secure, durable, and scalable storage platform. The biggest advantage of
    Amazon S3 is that there is no up-front IT investment and companies can build capacity
    (just by clicking a button a button) as they need.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊**简单存储服务**（**S3**）为开发人员和IT团队提供了一个安全、耐用和可扩展的存储平台。Amazon S3的最大优势在于没有预先的IT投资，公司可以根据需要构建容量（只需点击一个按钮）。
- en: Though Amazon S3 can be used with any compute platform, it integrates really
    well with Amazon's cloud services such as Amazon **Elastic Compute Cloud** (**EC2**)
    and Amazon **Elastic Block Storage** (**EBS**). For this reason, companies who
    use **Amazon Web Services** (**AWS**) are likely to have significant data is already
    stored on Amazon S3.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Amazon S3可以与任何计算平台一起使用，但它与亚马逊的云服务（如亚马逊**弹性计算云**（**EC2**）和亚马逊**弹性块存储**（**EBS**））结合得非常好。因此，使用**Amazon
    Web Services**（**AWS**）的公司可能已经在Amazon S3上存储了大量数据。
- en: This makes a good case for loading data in Spark from Amazon S3 and that is
    exactly what this recipe is about.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好地说明了从Amazon S3中加载数据到Spark的情况，这正是这个教程要讲的。
- en: How to do it...
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s start with the AWS portal:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从AWS门户开始：
- en: Go to [http://aws.amazon.com](http://aws.amazon.com) and log in with your username
    and password.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往[http://aws.amazon.com](http://aws.amazon.com)并使用您的用户名和密码登录。
- en: Once logged in, navigate to **Storage & Content Delivery** | **S3** | **Create
    Bucket**:![How to do it...](img/B03056_03_05.jpg)
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，导航至**存储和内容交付** | **S3** | **创建存储桶**：![如何操作...](img/B03056_03_05.jpg)
- en: Enter the bucket name—for example, `com.infoobjects.wordcount`. Please make
    sure you enter a unique bucket name (no two S3 buckets can have the same name
    globally).
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入存储桶名称，例如`com.infoobjects.wordcount`。请确保输入唯一的存储桶名称（全球没有两个S3存储桶可以具有相同的名称）。
- en: Select **Region**, click on **Create**, and then on the bucket name you created
    and you will see the following screen:![How to do it...](img/B03056_03_06.jpg)
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**区域**，单击**创建**，然后单击您创建的存储桶名称，您将看到以下屏幕：![如何操作...](img/B03056_03_06.jpg)
- en: Click on **Create Folder** and enter `words` as the folder name.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击**创建文件夹**，输入`words`作为文件夹名称。
- en: 'Create the `sh.txt` text file on the local filesystem:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地文件系统上创建`sh.txt`文本文件：
- en: '[PRE43]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Navigate to **Words** | **Upload** | **Add Files** and choose `sh.txt` from
    the dialog box, as shown in the following screenshot:![How to do it...](img/B03056_03_07.jpg)
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航至**Words** | **上传** | **添加文件**，并从对话框中选择`sh.txt`，如下图所示：![如何操作...](img/B03056_03_07.jpg)
- en: Click on **Start Upload**.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击**开始上传**。
- en: Select **sh.txt** and click on **Properties** and it will show you details of
    the file:![How to do it...](img/B03056_03_08.jpg)
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**sh.txt**，单击**属性**，它将显示文件的详细信息：![如何操作...](img/B03056_03_08.jpg)
- en: Set `AWS_ACCESS_KEY` and `AWS_SECRET_ACCESS_KEY` as environment variables.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`AWS_ACCESS_KEY`和`AWS_SECRET_ACCESS_KEY`设置为环境变量。
- en: 'Open the Spark shell and load the `words` directory from `s3` in the `words`
    RDD:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Spark shell并从`s3`中的`words`目录加载`words` RDD：
- en: '[PRE44]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now the RDD is loaded and you can continue doing regular transformations and
    actions on the RDD.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在RDD已加载，您可以继续对RDD进行常规转换和操作。
- en: Note
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Sometimes there is confusion between `s3://` and `s3n://`. `s3n://` means a
    regular file sitting in the S3 bucket but readable and writable by the outside
    world. This filesystem puts a 5 GB limit on the file size.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有时会混淆`s3://`和`s3n://`。`s3n://`表示位于S3存储桶中的常规文件，但可以被外部世界读取和写入。该文件系统对文件大小有5GB的限制。
- en: '`s3://` means an HDFS file sitting in the S3 bucket. It is a block-based filesystem.
    The filesystem requires you to dedicate a bucket for this filesystem. There is
    no limit on file size in this system.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`s3://`表示位于S3存储桶中的HDFS文件。这是一个基于块的文件系统。该文件系统要求您为此文件系统专门分配一个存储桶。在此系统中，文件大小没有限制。'
- en: Loading data from Apache Cassandra
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Apache Cassandra加载数据
- en: Apache Cassandra is a NoSQL database with a masterless ring cluster structure.
    While HDFS is a good fit for streaming data access, it does not work well with
    random access. For example, HDFS will work well when your average file size is
    100 MB and you want to read the whole file. If you frequently access the *n*th
    line in a file or some other part as a record, HDFS would be too slow.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Cassandra是一个无主环集群结构的NoSQL数据库。虽然HDFS非常适合流数据访问，但对于随机访问效果不佳。例如，当你的平均文件大小为100MB并且想要读取整个文件时，HDFS会很好地工作。但是，如果你经常访问文件中的第n行或其他部分作为记录，HDFS将会太慢。
- en: Relational databases have traditionally provided a solution to that, providing
    low latency, random access, but they do not work well with big data. NoSQL databases
    such as Cassandra fill the gap by providing relational database type access but
    in a distributed architecture on commodity servers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，关系数据库提供了解决方案，提供低延迟、随机访问，但它们在处理大数据方面效果不佳。Cassandra等NoSQL数据库通过在商品服务器上提供分布式架构中的关系数据库类型访问来填补这一空白。
- en: In this recipe, we will load data from Cassandra as a Spark RDD. To make that
    happen Datastax, the company behind Cassandra, has contributed `spark-cassandra-connector`.
    This connector lets you load Cassandra tables as Spark RDDs, write Spark RDDs
    back to Cassandra, and execute CQL queries.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将从Cassandra加载数据作为Spark RDD。为了实现这一点，Cassandra背后的公司Datastax贡献了`spark-cassandra-connector`。这个连接器让你将Cassandra表加载为Spark
    RDD，将Spark RDD写回Cassandra，并执行CQL查询。
- en: How to do it...
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to load data from Cassandra:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤从Cassandra加载数据：
- en: 'Create a keyspace named `people` in Cassandra using the CQL shell:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用CQL shell在Cassandra中创建一个名为`people`的keyspace：
- en: '[PRE45]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Create a column family (from CQL 3.0 onwards, it can also be called a **table**)
    `person` in newer versions of Cassandra:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在较新版本的Cassandra中创建一个列族（从CQL 3.0开始，也可以称为**表**）`person`：
- en: '[PRE46]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Insert a few records in the column family:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列族中插入几条记录：
- en: '[PRE47]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Add Cassandra connector dependency to SBT:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Cassandra连接器依赖项添加到SBT：
- en: '[PRE48]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You can also add the Cassandra dependency to Maven:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以将Cassandra依赖项添加到Maven中：
- en: '[PRE49]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Alternatively, you can also download the `spark-cassandra-connector` JAR to
    use directly with the Spark shell:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您也可以直接下载`spark-cassandra-connector` JAR并在Spark shell中使用：
- en: '[PRE50]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you would like to build the `uber` JAR with all dependencies, refer to the
    *There's more…* section.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要构建带有所有依赖项的`uber` JAR，请参考*更多内容...*部分。
- en: Now start the Spark shell.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在启动Spark shell。
- en: 'Set the `spark.cassandra.connection.host` property in the Spark shell:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Spark shell中设置`spark.cassandra.connection.host`属性：
- en: '[PRE51]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Import Cassandra-specific libraries:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入特定于Cassandra的库：
- en: '[PRE52]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Load the `person` column family as an RDD:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`person`列族加载为RDD：
- en: '[PRE53]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Count the number of records in the RDD:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算RDD中的记录数：
- en: '[PRE54]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Print data in the RDD:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印RDD中的数据：
- en: '[PRE55]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Retrieve the first row:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索第一行：
- en: '[PRE56]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Get the column names:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取列名：
- en: '[PRE57]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Cassandra can also be accessed through Spark SQL. It has a wrapper around `SQLContext`
    called `CassandraSQLContext`; let''s load it:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cassandra也可以通过Spark SQL访问。它在`SQLContext`周围有一个名为`CassandraSQLContext`的包装器；让我们加载它：
- en: '[PRE58]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Load the `person` data as `SchemaRDD`:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`person`数据加载为`SchemaRDD`：
- en: '[PRE59]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Retrieve the `person` data:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索`person`数据：
- en: '[PRE60]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: There's more...
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Spark Cassandra's connector library has a lot of dependencies. The connector
    itself and several of its dependencies are third-party to Spark and are not available
    as part of the Spark installation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Cassandra的连接器库有很多依赖项。连接器本身和它的一些依赖项是Spark的第三方，不作为Spark安装的一部分提供。
- en: These dependencies need to be made available to the driver as well as executors
    at runtime. One way to do this is to bundle all transitive dependencies, but that
    is a laborious and error-prone process. The recommended approach is to bundle
    all the dependencies along with the connector library. This will result in a fat
    JAR, popularly known as the `uber` JAR.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些依赖项需要在驱动程序和执行器运行时提供。一种方法是捆绑所有传递依赖项，但这是一个费力且容易出错的过程。推荐的方法是将所有依赖项与连接器库一起捆绑。这将产生一个fat
    JAR，通常称为`uber` JAR。
- en: 'SBT provides the `sbt-assembly` plugin, which makes creating `uber` JARs very
    easy. The following are the steps to create an `uber` JAR for `spark-cassandra-connector`.
    These steps are general enough so that you can use them to create any `uber` JAR:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: SBT提供了`sb-assembly`插件，可以很容易地创建`uber` JAR。以下是创建`spark-cassandra-connector`的`uber`
    JAR的步骤。这些步骤足够通用，可以用来创建任何`uber` JAR：
- en: 'Create a folder named `uber`:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`uber`的文件夹：
- en: '[PRE61]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Change the directory to `uber`:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目录更改为`uber`：
- en: '[PRE62]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Open the SBT prompt:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开SBT提示符：
- en: '[PRE63]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Give this project a name `sc-uber`:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给这个项目命名为`sc-uber`：
- en: '[PRE64]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Save the session:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存会话：
- en: '[PRE65]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Exit the session:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 退出会话：
- en: '[PRE66]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This will create `build.sbt`, `project`, and `target` folders in the `uber`
    folder as shown in the following screenshot:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在`uber`文件夹中创建`build.sbt`，`project`和`target`文件夹，如下面的屏幕截图所示：
- en: '![There''s more...](img/B03056_03_09.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![更多内容...](img/B03056_03_09.jpg)'
- en: 'Add the `spark-cassandra-driver` dependency to `build.sbt` at the end after
    leaving a blank line as shown in the following screenshot:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`build.sbt`的末尾添加`spark-cassandra-driver`依赖项，留下一个空行，如下面的屏幕截图所示：
- en: '[PRE67]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![There''s more...](img/B03056_03_10.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![更多内容...](img/B03056_03_10.jpg)'
- en: We will use `MergeStrategy.first` as the default. Besides that, there are some
    files, such as `manifest.mf`, that every JAR bundles for metadata, and we can
    simply discard them. We are going to use `MergeStrategy.discard` for that. The
    following is the screenshot of `build.sbt` with `assemblyMergeStrategy` added:![There's
    more...](img/B03056_03_11.jpg)
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`MergeStrategy.first`作为默认选项。此外，有一些文件，如`manifest.mf`，每个JAR都会捆绑用于元数据，我们可以简单地丢弃它们。我们将使用`MergeStrategy.discard`。以下是带有`assemblyMergeStrategy`的`build.sbt`的屏幕截图：![更多内容...](img/B03056_03_11.jpg)
- en: 'Now create `plugins.sbt` in the `project` folder and type the following for
    the `sbt-assembly` plugin:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在在`project`文件夹中创建`plugins.sbt`，并为`sbt-assembly`插件输入以下内容：
- en: '[PRE68]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We are ready to build (`assembly`) a JAR now:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备构建（装配）一个JAR：
- en: '[PRE69]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The `uber` JAR is now created in `target/scala-2.10/sc-uber-assembly-0.1-SNAPSHOT.jar`.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`uber` JAR现在创建在`target/scala-2.10/sc-uber-assembly-0.1-SNAPSHOT.jar`中。'
- en: 'Copy it to a suitable location where you keep all third-party JARs—for example,
    `/home/hduser/thirdparty`—and rename it to an easier name (unless you like longer
    names):'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其复制到一个适当的位置，您可以在那里保存所有第三方JAR文件，例如`/home/hduser/thirdparty`，并将其重命名为更简单的名称（除非您喜欢更长的名称）：
- en: '[PRE70]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Load the Spark shell with the `uber` JAR using `--jars`:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`--jars`加载`uber` JAR启动Spark shell：
- en: '[PRE71]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'To submit the Scala code to a cluster, you can call `spark-submit` with the
    same JARS option:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将Scala代码提交到集群，可以使用相同的JARS选项调用`spark-submit`：
- en: '[PRE72]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Merge strategies in sbt-assembly
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: sbt-assembly中的合并策略
- en: If multiple JARs have files with the same name and the same relative path, the
    default merge strategy for the `sbt-assembly` plugin is to verify that content
    is same for all the files and error out otherwise. This strategy is called `MergeStrategy.deduplicate`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果多个JAR具有具有相同名称和相同相对路径的文件，则`sbt-assembly`插件的默认合并策略是验证所有文件的内容是否相同，否则会出错。此策略称为`MergeStrategy.deduplicate`。
- en: 'The following are the available merge strategies in the `sbt-assembly plugin`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: sbt-assembly插件中可用的合并策略如下：
- en: '| Strategy name | Description |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 策略名称 | 描述 |'
- en: '| --- | --- |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `MergeStrategy.deduplicate` | The default strategy |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.deduplicate` | 默认策略 |'
- en: '| `MergeStrategy.first` | Picks first file according to classpath |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.first` | 根据类路径选择第一个文件 |'
- en: '| `MergeStrategy.last` | Picks last file according to classpath |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.last` | 根据类路径选择最后一个文件 |'
- en: '| `MergeStrategy.singleOrError` | Errors out (merge conflict not expected)
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.singleOrError` | 出错（不期望合并冲突） |'
- en: '| `MergeStrategy.concat` | Concatenates all matching files together |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.concat` | 将所有匹配的文件连接在一起 |'
- en: '| `MergeStrategy.filterDistinctLines` | Concatenates leaving out duplicates
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.filterDistinctLines` | 连接并排除重复行 |'
- en: '| `MergeStrategy.rename` | Renames files |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.rename` | 重命名文件 |'
- en: Loading data from relational databases
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从关系数据库加载数据
- en: A lot of important data lies in relational databases that Spark needs to query.
    JdbcRDD is a Spark feature that allows relational tables to be loaded as RDDs.
    This recipe will explain how to use JdbcRDD.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Spark需要查询的许多重要数据存储在关系数据库中。JdbcRDD是一个Spark功能，允许将关系表加载为RDD。本教程将解释如何使用JdbcRDD。
- en: Spark SQL to be introduced in the next chapter includes a data source for JDBC.
    This should be preferred over the current recipe as results are returned as DataFrames
    (to be introduced in the next chapter), which can be easily processed by Spark
    SQL and also joined with other data sources.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍的Spark SQL包括一个用于JDBC的数据源。这应该优先于当前的教程，因为结果将作为DataFrame（将在下一章中介绍）返回，可以很容易地由Spark
    SQL处理，并与其他数据源连接。
- en: Getting ready
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Please make sure that the JDBC driver JAR is visible on the client node and
    all slaves nodes on which executor will run.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保JDBC驱动程序JAR在客户端节点和所有执行程序将运行的所有从节点上可见。
- en: How to do it...
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Perform the following steps to load data from relational databases:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤从关系数据库中加载数据：
- en: 'Create a table named `person` in MySQL using the following DDL:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下DDL在MySQL中创建名为`person`的表：
- en: '[PRE73]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Insert some data:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入一些数据：
- en: '[PRE74]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Download `mysql-connector-java-x.x.xx-bin.jar` from [http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/).
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/)下载`mysql-connector-java-x.x.xx-bin.jar`。
- en: 'Make the MySQL driver available to the Spark shell and launch it:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使MySQL驱动程序可用于Spark shell并启动它：
- en: '[PRE75]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Note
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that `path-to-mysql-jar` is not the actual path name. You should
    use the actual path name.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`path-to-mysql-jar`不是实际的路径名。您应该使用实际的路径名。
- en: 'Create variables for the username, password, and JDBC URL:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建用户名、密码和JDBC URL的变量：
- en: '[PRE76]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Import JdbcRDD:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入JdbcRDD：
- en: '[PRE77]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Import JDBC-related classes:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入与JDBC相关的类：
- en: '[PRE78]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Create an instance of the JDBC driver:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建JDBC驱动程序的实例：
- en: '[PRE79]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Load JdbcRDD:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载JdbcRDD：
- en: '[PRE80]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now query the results:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在查询结果：
- en: '[PRE81]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Save the RDD to HDFS:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将RDD保存到HDFS：
- en: '[PRE82]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: How it works…
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '工作原理… '
- en: 'JdbcRDD is an RDD that executes a SQL query on a JDBC connection and retrieves
    the results. The following is a JdbcRDD constructor:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: JdbcRDD是一个在JDBC连接上执行SQL查询并检索结果的RDD。以下是一个JdbcRDD构造函数：
- en: '[PRE83]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The two ?'s are bind variables for a prepared statement inside JdbcRDD. The
    first ? is for the offset (lower bound), that is, which row should we start computing
    with, the second ? is for the limit (upper bound), that is, how many rows should
    we read.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 两个?是JdbcRDD内部准备语句的绑定变量。第一个?是偏移量（下限），也就是说，我们应该从哪一行开始计算，第二个?是限制（上限），也就是说，我们应该读取多少行。
- en: JdbcRDD is a great way to load data in Spark directly from relational databases
    on an ad-hoc basis. If you would like to load data in bulk from RDBMS, there are
    other approaches that would work better, for example, Apache Sqoop is a powerful
    tool that imports and exports data from relational databases to HDFS.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: JdbcRDD是一种直接从关系数据库中以临时基础加载数据到Spark的好方法。如果您想要从RDBMS中批量加载数据，还有其他更好的方法，例如，Apache
    Sqoop是一个强大的工具，可以将数据从关系数据库导入到HDFS，并从HDFS导出数据。
