- en: Chapter 7. Deployment and Monitoring
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 部署和监控
- en: We have explored the development of the Stock Screener Application in previous
    chapters; it is now time to consider how to deploy it in the production environment.
    In this chapter, we will discuss the most important aspects of deploying a Cassandra
    database in production. These aspects include the selection of an appropriate
    combination of replication strategy, snitch, and replication factor to make up
    a fault-tolerant, highly available cluster. Then we will demonstrate the procedure
    to migrate our Cassandra development database of the Stock Screener Application
    to a production database. However, cluster maintenance is beyond the scope of
    this book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中探讨了Stock Screener应用程序的开发；现在是时候考虑如何在生产环境中部署它了。在本章中，我们将讨论在生产环境中部署Cassandra数据库最重要的方面。这些方面包括选择合适的复制策略、snitch和复制因子的组合，以形成一个容错性高、高可用的集群。然后我们将演示将Stock
    Screener应用程序的Cassandra开发数据库迁移到生产数据库的过程。然而，集群维护超出了本书的范围。
- en: Moreover, a live production system that continuously operates certainly requires
    monitoring of its health status. We will cover the basic tools and techniques
    of monitoring a Cassandra cluster, including the nodetool utility, JMX and MBeans,
    and system log.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一个持续运行的实时生产系统当然需要对其健康状况进行监控。我们将介绍监控Cassandra集群的基本工具和技术，包括nodetool实用程序、JMX和MBeans以及系统日志。
- en: Finally, we will explore ways of boosting the performance of Cassandra other
    than using the defaults. Actually, performance tuning can be made at several levels,
    from the lowest hardware and system configuration to the highest application coding
    techniques. We will focus on the **Java Virtual Machine** (**JVM**) level, because
    Cassandra heavily relies on its underlying performance. In addition, we will touch
    on how to tune caches for a table.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨除了使用默认设置之外提高Cassandra性能的方法。实际上，性能调整可以在多个级别进行，从最低的硬件和系统配置到最高的应用程序编码技术。我们将重点关注**Java虚拟机**（**JVM**）级别，因为Cassandra高度依赖于其底层性能。此外，我们还将涉及如何调整表的缓存。
- en: Replication strategies
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制策略
- en: This section is about the data replication configuration of a Cassandra cluster.
    It will cover replication strategies, snitches, and the configuration of the cluster
    for the Stock Screener Application.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍Cassandra集群的数据复制配置。它将涵盖复制策略、snitch以及为Stock Screener应用程序配置集群。
- en: Data replication
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据复制
- en: Cassandra, by design, can work in a huge cluster across multiple data centers
    all over the globe. In such a distributed environment, network bandwidth and latency
    must be critically considered in the architecture, and careful planning in advance
    is required, otherwise it would lead to catastrophic consequences. The most obvious
    issue is the time clock synchronization—the genuine means of resolving transaction
    conflicts that can threaten data integrity in the whole cluster. Cassandra relies
    on the underlying operating system platform to provide the time clock synchronization
    service. Furthermore, a node is highly likely to fail at some time and the cluster
    must be resilient to this typical node failure. These issues have to be thoroughly
    considered at the architecture level.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra，按照设计，可以在全球多个数据中心的大型集群中运行。在这样的分布式环境中，网络带宽和延迟必须在架构中给予关键性的考虑，并且需要提前进行仔细规划，否则可能会导致灾难性的后果。最明显的问题就是时钟同步——这是解决可能威胁整个集群数据完整性的交易冲突的真正手段。Cassandra依赖于底层操作系统平台来提供时钟同步服务。此外，节点在某个时间点高度可能发生故障，集群必须能够抵御这种典型的节点故障。这些问题必须在架构层面进行彻底的考虑。
- en: Cassandra adopts data replication to tackle these issues, based on the idea
    of using space in exchange of time. It simply consumes more storage space to make
    data replicas so as to minimize the complexities in resolving the previously mentioned
    issues in a cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra采用数据复制来应对这些问题，基于使用空间来交换时间的理念。它简单地消耗更多的存储空间来制作数据副本，以最小化在集群中解决之前提到的问题的复杂性。
- en: Data replication is configured by the so-called replication factor in a **keyspace**.
    The replication factor refers to the total number of copies of each row across
    the cluster. So a replication factor of `1` (as seen in the examples in previous
    chapters) means that there is only one copy of each row on one node. For a replication
    factor of `2`, two copies of each row are on two different nodes. Typically, a
    replication factor of `3` is sufficient in most production scenarios.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据复制是通过所谓的复制因子在 **键空间** 中配置的。复制因子指的是集群中每行数据的总副本数。因此，复制因子为 `1`（如前几章中的示例所示）表示每行数据只有一个副本在单个节点上。对于复制因子为
    `2`，每行数据有两个副本在不同的节点上。通常，大多数生产场景中复制因子为 `3` 就足够了。
- en: All data replicas are equally important. There are neither master nor slave
    replicas. So data replication does not have scalability issues. The replication
    factor can be increased as more nodes are added. However, the replication factor
    should not be set to exceed the number of nodes in the cluster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据副本同等重要。没有主副本或从副本。因此，数据复制没有可扩展性问题。随着更多节点的添加，可以增加复制因子。然而，复制因子不应设置超过集群中的节点数。
- en: 'Another unique feature of Cassandra is its awareness of the physical location
    of nodes in a cluster and their proximity to each other. Cassandra can be configured
    to know the layout of the data centers and racks by a correct IP address assignment
    scheme. This setting is known as replication strategy and Cassandra provides two
    choices for us: `SimpleStrategy` and `NetworkTopologyStrategy`.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra 的另一个独特特性是它了解集群中节点的物理位置以及它们之间的邻近性。Cassandra 可以通过正确的 IP 地址分配方案配置来了解数据中心和机架的布局。这个设置被称为复制策略，Cassandra
    为我们提供了两个选择：`SimpleStrategy` 和 `NetworkTopologyStrategy`。
- en: SimpleStrategy
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SimpleStrategy
- en: '`SimpleStrategy` is used on a single machine or on a cluster in a single data
    center. It places the first replica on a node determined by the partitioner, and
    then the additional replicas are placed on the next nodes in a clockwise fashion
    without considering the data center and rack locations. Even though this is the
    default replication strategy when creating a keyspace, if we ever intend to have
    more than one data center, we should use `NetworkTopologyStrategy` instead.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`SimpleStrategy` 在单台机器或单个数据中心内的集群中使用。它将第一个副本放置在分区器确定的节点上，然后以顺时针方向将额外的副本放置在下一个节点上，不考虑数据中心和机架的位置。尽管这是创建键空间时的默认复制策略，但如果我们打算拥有多个数据中心，我们应该使用
    `NetworkTopologyStrategy`。'
- en: NetworkTopologyStrategy
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NetworkTopologyStrategy
- en: '`NetworkTopologyStrategy` becomes aware of the locations of data centers and
    racks by understanding the IP addresses of the nodes in the cluster. It places
    replicas in the same data center by the clockwise mechanism until the first node
    in another rack is reached. It attempts to place replicas on different racks because
    the nodes in the same rack often fail at the same time due to power, network issues,
    air conditioning, and so on.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`NetworkTopologyStrategy` 通过了解集群中节点的 IP 地址来了解数据中心和机架的位置。它通过顺时针机制将副本放置在相同的数据中心，直到达到另一个机架的第一个节点。它试图在不同的机架上放置副本，因为同一机架的节点往往由于电源、网络问题、空调等原因同时失败。'
- en: As mentioned, Cassandra knows the physical location from the IP addresses of
    the nodes. The mapping of the IP addresses to the data centers and racks is referred
    to as a **snitch**. Simply put, a snitch determines which data centers and racks
    the nodes belong to. It optimizes read operations by providing information about
    the network topology to Cassandra such that read requests can be routed efficiently.
    It also affects how replicas can be distributed in consideration of the physical
    location of data centers and racks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Cassandra 通过节点的 IP 地址了解其物理位置。IP 地址到数据中心和机架的映射称为 **snitch**。简单来说，snitch
    确定节点属于哪些数据中心和机架。它通过向 Cassandra 提供有关网络拓扑的信息来优化读取操作，以便读取请求可以有效地路由。它还影响副本在考虑数据中心和机架的物理位置时的分布。
- en: 'There are many types of snitches available for different scenarios and each
    comes with its pros and cons. They are briefly described as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根据不同的场景，有各种类型的 snitch 可用，每种都有其优缺点。以下简要描述如下：
- en: '`SimpleSnitch`: This is used for single data center deployments only'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SimpleSnitch`: 这仅用于单个数据中心的部署'
- en: '`DynamicSnitch`: This monitors the performance of read operations from different
    replicas, and chooses the best replica based on historical performance'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DynamicSnitch`: 这监控来自不同副本的读操作性能，并根据历史性能选择最佳副本'
- en: '`RackInferringSnitch`: This determines the location of the nodes by data center
    and rack corresponding to the IP addresses'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RackInferringSnitch`: 这通过数据中心和与IP地址对应的机架来确定节点的位置'
- en: '`PropertyFileSntich`: This determines the locations of the nodes by data center
    and rack'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PropertyFileSnitch`: 这通过数据中心和机架确定节点的位置'
- en: '`GossipingPropertyFileSnitch`: This automatically updates all nodes using gossip
    when adding new nodes'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GossipingPropertyFileSnitch`: 这在添加新节点时使用gossip自动更新所有节点'
- en: '`EC2Snitch`: This is used with Amazon EC2 in a single region'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EC2Snitch`: 这与单个区域的Amazon EC2一起使用'
- en: '`EC2MultiRegionSnitch`: This is used with Amazon EC2 in multiple regions'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EC2MultiRegionSnitch`: 这用于跨多个区域的Amazon EC2'
- en: '`GoogleCloudSnitch`: This is used with Google Cloud Platform across one or
    more regions'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GoogleCloudSnitch`: 这用于跨一个或多个区域的Google Cloud Platform'
- en: '`CloudstackSnitch`: This is used for Apache Cloudstack environments'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CloudstackSnitch`: 这用于Apache Cloudstack环境'
- en: Note
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Snitch Architecture**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Snitch架构**'
- en: For more detailed information, please refer to the documentation made by DataStax
    at [http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html](http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更详细的信息，请参阅DataStax制作的文档，[http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html](http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html)。
- en: 'The following figure illustrates an example of a cluster of eight nodes in
    four racks across two data centers using `RackInferringSnitch` and a replication
    factor of three per data center:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了使用`RackInferringSnitch`和每个数据中心三个副本因子的四个机架中八个节点的集群示例：
- en: '![NetworkTopologyStrategy](img/8884OS_07_01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![网络拓扑策略](img/8884OS_07_01.jpg)'
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: All nodes in the cluster must use the same snitch setting.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的所有节点必须使用相同的snitch设置。
- en: Let us look at the IP address assignment in **Data Center 1** first. The IP
    addresses are grouped and assigned in a top-down fashion. All the nodes in **Data
    Center 1** are in the same **123.1.0.0** subnet. For those nodes in **Rack 1**,
    they are in the same **123.1.1.0** subnet. Hence, **Node 1** in **Rack 1** is
    assigned an IP address of **123.1.1.1** and **Node 2** in **Rack 1** is **123.1.1.2**.
    The same rule applies to **Rack 2** such that the IP addresses of **Node 1** and
    **Node 2** in **Rack 2** are **123.1.2.1** and **123.1.2.2**, respectively. For
    **Data Center 2**, we just change the subnet of the data center to **123.2.0.0**
    and the racks and nodes in **Data Center 2** are then changed similarly.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看**数据中心1**中的IP地址分配。IP地址是分组并自上而下分配的。**数据中心1**中的所有节点都在同一个**123.1.0.0**子网中。对于**机架1**中的节点，它们都在同一个**123.1.1.0**子网中。因此，**机架1**中的**节点1**被分配了IP地址**123.1.1.1**，而**机架1**中的**节点2**是**123.1.1.2**。同样的规则适用于**机架2**，因此**机架2**中**节点1**和**节点2**的IP地址分别是**123.1.2.1**和**123.1.2.2**。对于**数据中心2**，我们只需将数据中心的子网更改为**123.2.0.0**，然后**数据中心2**中的机架和节点相应地改变。
- en: 'The `RackInferringSnitch` deserves a more detailed explanation. It assumes
    that the network topology is known by properly assigned IP addresses based on
    the following rule:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`RackInferringSnitch`值得更详细的解释。它假设网络拓扑是通过以下规则正确分配的IP地址而知的：'
- en: '*IP address = <arbitrary octet>.<data center octet>.<rack octet>.<node octet>*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*IP地址 = <任意八位字节>.<数据中心八位字节>.<机架八位字节>.<节点八位字节>*'
- en: The formula for IP address assignment is shown in the previous paragraph. With
    this very structured assignment of IP addresses, Cassandra can understand the
    physical location of all nodes in the cluster.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: IP地址分配的公式在上一段中显示。有了这种非常结构化的IP地址分配，Cassandra可以理解集群中所有节点的物理位置。
- en: Another thing that we need to understand is the replication factor of the three
    replicas that are shown in the previous figure. For a cluster with `NetworkToplogyStrategy`,
    the replication factor is set on a per data center basis. So in our example, three
    replicas are placed in **Data Center 1** as illustrated by the dotted arrows in
    the previous diagram. **Data Center 2** is another data center that must have
    three replicas. Hence, there are six replicas in total across the cluster.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要了解的是，如图中所示的前三个副本的复制因子。对于具有`NetworkToplogyStrategy`的集群，复制因子是在每个数据中心的基础上设置的。因此，在我们的例子中，三个副本放置在**数据中心1**，如图中虚线箭头所示。**数据中心2**是另一个必须有三个副本的数据中心。因此，整个集群中共有六个副本。
- en: We will not go through every combination of the replication factor, snitch and
    replication strategy here, but we should now understand the foundation of how
    Cassandra makes use of them to flexibly deal with different cluster scenarios
    in real-life production.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里详细说明复制因子、snitch和复制策略的每一种组合，但我们应该现在理解Cassandra如何利用它们来灵活地处理实际生产中的不同集群场景的基础。
- en: Setting up the cluster for Stock Screener Application
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为股票筛选器应用程序设置集群
- en: Let us return to the Stock Screener Application. The cluster it runs on in [Chapter
    6](ch06.html "Chapter 6. Enhancing a Version"), *Enhancing a Version*, is a single-node
    cluster. In this section, we will set up a cluster of two nodes that can be used
    in small-scale production. We will also migrate the existing data in the development
    database to the new fresh production cluster. It should be noted that for quorum
    reads/writes, it's usually best practice to use an odd number of nodes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到股票筛选器应用程序。它在[第6章](ch06.html "第6章。增强版本")，*增强版本*中运行的集群是一个单节点集群。在本节中，我们将设置一个可以用于小规模生产的两个节点集群。我们还将把开发数据库中的现有数据迁移到新的生产集群。需要注意的是，对于仲裁读取/写入，通常最好使用奇数个节点。
- en: System and network configuration
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统和网络配置
- en: 'The steps of installation and setup of the operating system and network configuration
    are assumed to be done. Moreover, both nodes should have Cassandra freshly installed.
    The system configuration of the two nodes is identical and shown as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设操作系统和网络配置的安装和设置步骤已经完成。此外，两个节点都应该安装了新的Cassandra。两个节点的系统配置相同，如下所示：
- en: 'OS: Ubuntu 12.04 LTS 64-bit'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统：Ubuntu 12.04 LTS 64位
- en: 'Processor: Intel Core i7-4771 CPU @3.50GHz x 2'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理器：Intel Core i7-4771 CPU @3.50GHz x 2
- en: 'Memory: 2 GB'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：2 GB
- en: 'Disk: 20 GB'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬盘：20 GB
- en: Global settings
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全局设置
- en: 'The cluster is named **Test Cluster**, in which both the **ubtc01** and **ubtc02**
    nodes are in the same rack, `RACK1`, and in the same data center, `NY1`. The logical
    architecture of the cluster to be set up is depicted in the following diagram:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 集群被命名为**Test Cluster**，其中**ubtc01**和**ubtc02**节点位于同一个机架`RACK1`，并且位于同一个数据中心`NY1`。将要设置的集群的逻辑架构如下所示：
- en: '![Global settings](img/8884OS_07_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![全局设置](img/8884OS_07_02.jpg)'
- en: 'In order to configure a Cassandra cluster, we need to modify a few properties
    in the main configuration file, `cassandra.yaml`, for Cassandra. Depending on
    the installation method of Cassandra, `cassandra.yaml` is located in different
    directories:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置一个Cassandra集群，我们需要修改Cassandra的主配置文件`cassandra.yaml`中的几个属性。根据Cassandra的安装方式，`cassandra.yaml`位于不同的目录中：
- en: 'Package installation: `/etc/cassandra/`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件包安装：`/etc/cassandra/`
- en: 'Tarball installation: `<install_location>/conf/`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打包安装：`<install_location>/conf/`
- en: 'The first thing to do is to set the properties in `cassandra.yaml` for each
    node. As the system configuration of both nodes is the same, the following modification
    on `cassandra.yaml` settings is identical to them:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是为每个节点设置`cassandra.yaml`中的属性。由于两个节点的系统配置相同，以下对`cassandra.yaml`设置的修改与它们相同：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The reason for using `GossipingPropertyFileSnitch` is that we want the Cassandra
    cluster to automatically update all nodes with the gossip protocol when adding
    a new node.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`GossipingPropertyFileSnitch`的原因是我们希望Cassandra集群在添加新节点时能够自动通过gossip协议更新所有节点。
- en: 'Apart from `cassandra.yaml`, we also need to modify the data center and rack
    properties in `cassandra-rackdc.properties` in the same location as `cassandra.yaml`.
    In our case, the data center is `NY1` and the rack is `RACK1`, as shown in the
    following code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`cassandra.yaml`之外，我们还需要修改与`cassandra.yaml`相同位置的`cassandra-rackdc.properties`中的数据中心和机架属性。在我们的例子中，数据中心是`NY1`，机架是`RACK1`，如下所示：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Configuration procedure
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置过程
- en: 'The configuration procedure of the cluster (refer to the following bash shell
    scripts: `setup_ubtc01.sh` and `setup_ubtc02.sh`) is enumerated as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 集群的配置流程（参考以下 bash 脚本：`setup_ubtc01.sh` 和 `setup_ubtc02.sh`）如下列举：
- en: 'Stop Cassandra service:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止 Cassandra 服务：
- en: '[PRE2]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Remove the system keyspace:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除系统键空间：
- en: '[PRE3]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Modify `cassandra.yaml` and `cassandra-rackdc.properties` in both nodes based
    on the global settings as specified in the previous section
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据前一小节中指定的全局设置，在两个节点上修改 `cassandra.yaml` 和 `cassandra-rackdc.properties`：
- en: 'Start the seed node `ubtc01` first:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先启动种子节点 `ubtc01`：
- en: '[PRE4]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then start `ubtc02`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后启动 `ubtc02`：
- en: '[PRE5]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Wait for a minute and check if `ubtc01` and `ubtc02` are both up and running:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待一分钟，检查 `ubtc01` 和 `ubtc02` 是否都处于运行状态：
- en: '[PRE6]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A successful result of setting up the cluster should resemble something similar
    to the following screenshot, showing that both nodes are up and running:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 集群设置成功的结果应类似于以下截图，显示两个节点都处于运行状态：
- en: '![Configuration procedure](img/8884OS_07_03.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![配置流程](img/8884OS_07_03.jpg)'
- en: Legacy data migration procedure
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 旧数据迁移流程
- en: 'We now have the cluster ready but it is empty. We can simply rerun the Stock
    Screener Application to download and fill in the production database again. Alternatively,
    we can migrate the historical prices collected in the development single-node
    cluster to this production cluster. In the case of the latter approach, the following
    procedure can help us ease the data migration task:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了准备好的集群，但它仍然是空的。我们可以简单地重新运行股票筛选器应用程序来重新下载并填充生产数据库。或者，我们可以将开发单节点集群中收集的历史价格迁移到这个生产集群。在后一种方法中，以下流程可以帮助我们简化数据迁移任务：
- en: 'Take a snapshot of the `packcdma` keyspace in the development database (ubuntu
    is the hostname of the development machine):'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开发数据库中对 `packcdma` 键空间进行快照（ubuntu 是开发机器的主机名）：
- en: '[PRE7]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Record the snapshot directory, in this example, **1412082842986**
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录快照目录，在此示例中，**1412082842986**
- en: 'To play it safe, copy all SSTables under the snapshot directory to a temporary
    location, say `~/temp/`:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了安全起见，将快照目录下的所有 SSTables 复制到临时位置，例如 `~/temp/`：
- en: '[PRE8]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Open cqlsh to connect to `ubtc01` and create a keyspace with the appropriate
    replication strategy in the production cluster:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 cqlsh 连接到 `ubtc01` 并在 production 集群中创建具有适当复制策略的键空间：
- en: '[PRE9]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create the `alert_by_date`, `alertlist`, `quote`, and `watchlist` tables:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `alert_by_date`、`alertlist`、`quote` 和 `watchlist` 表：
- en: '[PRE10]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Load the SSTables back to the production cluster using the `sstableloader`
    utility:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `sstableloader` 工具将 SSTables 重新加载到生产集群中：
- en: '[PRE11]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Check the legacy data in the production database on `ubtc02`:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `ubtc02` 上检查生产数据库中的旧数据：
- en: '[PRE12]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Although the previous steps look complicated, it is not difficult to understand
    what they want to achieve. It should be noted that we have set the replication
    factor per data center as `2` to provide data redundancy on both nodes, as shown
    in the `CREATE KEYSPACE` statement. The replication factor can be changed in future
    if needed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的步骤看起来很复杂，但理解它们想要实现的目标并不困难。需要注意的是，我们已经将每个数据中心的复制因子设置为 `2`，以在两个节点上提供数据冗余，如
    `CREATE KEYSPACE` 语句所示。如果需要，复制因子可以在将来更改。
- en: Deploying the Stock Screener Application
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署股票筛选器应用程序
- en: 'As we have set up the production cluster and moved the legacy data into it,
    it is time to deploy the Stock Screener Application. The only thing needed to
    modify is the code to establish Cassandra connection to the production cluster.
    This is very easy to do with Python. The code in `chapter06_006.py` is modified
    to work with the production cluster as `chapter07_001.py`. A new test case named
    `testcase003()` is created to replace `testcase002()`. To save pages, the complete
    source code of `chapter07_001.py` is not shown here; only the `testcase003()`
    function is depicted as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经设置了生产集群并将旧数据移动到其中，现在是时候部署股票筛选器应用程序了。唯一需要修改的是代码以建立 Cassandra 与生产集群的连接。这使用
    Python 非常容易完成。`chapter06_006.py` 中的代码已修改为与生产集群一起工作，作为 `chapter07_001.py`。创建了一个名为
    `testcase003()` 的新测试用例来替换 `testcase002()`。为了节省页面，这里没有显示 `chapter07_001.py` 的完整源代码；只描述了
    `testcase003()` 函数如下：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The cluster connection code right at the beginning of the `testcase003()` function
    is passed with an array of the nodes to be connected (`ubtc01` and `ubtc02`).
    Here we adopted the default `RoundRobinPolicy` as the connection load balancing
    policy. It is used to decide how to distribute requests among all possible coordinator
    nodes in the cluster. There are many other options which are described in the
    driver API documentation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`testcase003()` 函数开头直接传递给集群连接代码的是一个要连接的节点数组（`ubtc01` 和 `ubtc02`）。在这里，我们采用了默认的
    `RoundRobinPolicy` 作为连接负载均衡策略。它用于决定如何在集群中所有可能的协调节点之间分配请求。还有许多其他选项，这些选项在驱动程序 API
    文档中有描述。'
- en: Note
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Cassandra Driver 2.1 Documentation**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**Cassandra 驱动 2.1 文档**'
- en: For the complete API documentation of the Python Driver 2.1 for Apache Cassandra,
    you can refer to [http://datastax.github.io/python-driver/api/index.html](http://datastax.github.io/python-driver/api/index.html).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Apache Cassandra 的 Python 驱动 2.1 的完整 API 文档，您可以参考[http://datastax.github.io/python-driver/api/index.html](http://datastax.github.io/python-driver/api/index.html)。
- en: Monitoring
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控
- en: As the application system goes live, we need to monitor its health day-by-day.
    Cassandra provides a number of tools for this purpose. We will introduce some
    of them with pragmatic recommendations. It is remarkable that each operating system
    also provides a bunch of tools and utilities for monitoring, for example, `top`,
    `df`, `du` on Linux and Task Manager on Windows. However, they are beyond the
    scope of this book.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用程序系统的上线，我们需要每天监控其健康状况。Cassandra 提供了多种工具来完成这项任务。我们将介绍其中的一些，并附上实用的建议。值得注意的是，每个操作系统也提供了一系列用于监控的工具和实用程序，例如
    Linux 上的 `top`、`df`、`du` 和 Windows 上的任务管理器。然而，这些内容超出了本书的范围。
- en: Nodetool
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Nodetool
- en: The nodetool utility should not be new to us. It is a command-line interface
    used to monitor Cassandra and perform routine database operations. It includes
    the most important metrics for tables, server, and compaction statistics, and
    other useful commands for administration.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: nodetool 实用工具对我们来说应该不陌生。它是一个命令行界面，用于监控 Cassandra 并执行常规数据库操作。它包括表格、服务器和压缩统计信息的最重要指标，以及其他用于管理的有用命令。
- en: 'Here are the most commonly used `nodetool` options:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了最常用的 `nodetool` 选项：
- en: '`status`: This provides a concise summary of the cluster, such as the state,
    load, and IDs'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`status`：这提供了关于集群的简洁摘要，例如状态、负载和 ID'
- en: '`netstats`: This gives the network information for a node, focusing on read
    repair operations'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`netstats`：这提供了关于节点的网络信息，重点关注读取修复操作'
- en: '`info`: This gives valuable node information including token, on disk load,
    uptime, Java heap memory usage, key cache, and row cache'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info`：这提供了包括令牌、磁盘负载、运行时间、Java 堆内存使用、键缓存和行缓存在内的有价值节点信息'
- en: '`tpstats`: This provides statistics about the number of active, pending, and
    completed tasks for each stage of Cassandra operations by thread pool'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tpstats`：这提供了关于 Cassandra 操作每个阶段的活跃、挂起和完成的任务数量的统计信息'
- en: '`cfstats`: This gets the statistics about one or more tables, such as read-and-write
    counts and latencies, metrics about SSTable, memtable, bloom filter, and compaction.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cfstats`：这获取一个或多个表的统计信息，例如读写次数和延迟，以及关于 SSTable、memtable、布隆过滤器和大小的指标。'
- en: Note
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A detailed documentation of nodetool can be referred to at [http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html](http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: nodetool 的详细文档可以参考[http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html](http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html)。
- en: JMX and MBeans
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JMX 和 MBeans
- en: Cassandra is written in the Java language and so it natively supports **Java
    Management Extensions** (**JMX**). We may use JConsole, a JMX-compliant tool,
    to monitor Cassandra.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra 是用 Java 语言编写的，因此它原生支持 **Java 管理扩展**（**JMX**）。我们可以使用符合 JMX 规范的工具 JConsole
    来监控 Cassandra。
- en: Note
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**JConsole**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**JConsole**'
- en: JConsole is included with Sun JDK 5.0 and higher versions. However, it consumes
    a significant amount of system resources. It is recommended that you run it on
    a remote machine rather than on the same host as a Cassandra node.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: JConsole 包含在 Sun JDK 5.0 及更高版本中。然而，它消耗了大量的系统资源。建议您在远程机器上运行它，而不是在 Cassandra 节点所在的同一主机上运行。
- en: 'We can launch JConsole by typing `jconsole` in a terminal. Assuming that we
    want to monitor the local node, when the **New Connection** dialog box pops up,
    we type `localhost:7199` (`7199` is the port number of JMX) in the **Remote Process**
    textbox, as depicted in the following screenshot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在终端中输入`jconsole`来启动JConsole。假设我们想监控本地节点，当**新连接**对话框弹出时，我们在**远程进程**文本框中输入`localhost:7199`（`7199`是JMX的端口号），如下面的截图所示：
- en: '![JMX and MBeans](img/8884OS_07_04.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![JMX 和 MBeans](img/8884OS_07_04.jpg)'
- en: 'After having connected to the local Cassandra instance, we will see a well-organized
    GUI showing six separate tabs placed horizontally on the top, as seen in the following
    screenshot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到本地Cassandra实例后，我们将看到一个组织良好的GUI，顶部水平放置了六个单独的标签页，如下面的截图所示：
- en: '![JMX and MBeans](img/8884OS_07_05.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![JMX 和 MBeans](img/8884OS_07_05.jpg)'
- en: 'The tabs of the GUI are explained as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: GUI标签页的解释如下：
- en: '**Overview**: This displays overview information about the JVM and monitored
    values'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概览**：此部分显示有关JVM和监控值的概述信息'
- en: '**Memory**: This displays information about heap and non-heap memory usage
    and garbage collection metrics'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存**：此部分显示有关堆和非堆内存使用情况以及垃圾回收指标的信息'
- en: '**Threads**: This displays information about thread use'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：此部分显示有关线程使用情况的信息'
- en: '**Classes**: This displays information about class loading'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类**：此部分显示有关类加载的信息'
- en: '**VM Summary**: This displays information about the JVM'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟机摘要**：此部分显示有关JVM的信息'
- en: '**MBeans**: This displays information about specific Cassandra metrics and
    operations'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MBeans**：此部分显示有关特定Cassandra指标和操作的信息'
- en: 'Furthermore, Cassandra provides five MBeans for JConsole. They are briefly
    introduced as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Cassandra为JConsole提供了五个MBeans。以下是它们的简要介绍：
- en: '`org.apache.cassandra.db`: This includes caching, table metrics, and compaction'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`org.apache.cassandra.db`：这包括缓存、表指标和压缩'
- en: '`org.apache.cassandra.internal`: These are internal server operations such
    as gossip and hinted handoff'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`org.apache.cassandra.internal`：这些是内部服务器操作，如gossip和hinted handoff'
- en: '`org.apache.cassandra.metrics`: These are various metrics of the Cassandra
    instance such as cache and compaction'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`org.apache.cassandra.metrics`：这些是Cassandra实例的各种指标，如缓存和压缩'
- en: '`org.apache.cassandra.net`: This has Inter-node communication including FailureDetector,
    MessagingService and StreamingService'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`org.apache.cassandra.net`：这包括节点间通信，包括FailureDetector、MessagingService和StreamingService'
- en: '`org.apache.cassandra.request`: These include tasks related to read, write,
    and replication operations'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`org.apache.cassandra.request`：这些包括与读取、写入和复制操作相关的任务'
- en: Note
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**MBeans**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**MBeans**'
- en: An **Managed Bean** (**MBean**) is a Java object that represents a manageable
    resource such as an application, a service, a component, or a device running in
    the JVM. It can be used to collect statistics on concerns such as performance,
    resource usage, or problems, for getting and setting application configurations
    or properties, and notifying events like faults or state changes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**托管Bean（MBean**）是一个Java对象，它代表一个可管理的资源，例如在JVM中运行的应用程序、服务、组件或设备。它可以用来收集有关性能、资源使用或问题等问题的统计信息，用于获取和设置应用程序配置或属性，以及通知事件，如故障或状态变化。'
- en: The system log
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统日志
- en: 'The most rudimentary, yet the most powerful, monitoring tool is Cassandra''s
    system log. The default location of the system log is named `system.log` under
    `/var/log/cassandra/`. It is simply a text file and can be viewed or edited by
    any text editor. The following screenshot shows an extract of `system.log`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最基础但也是最强大的监控工具是Cassandra的系统日志。系统日志的默认位置位于`/var/log/cassandra/`下的名为`system.log`的目录中。它只是一个文本文件，可以使用任何文本编辑器查看或编辑。以下截图显示了`system.log`的摘录：
- en: '![The system log](img/8884OS_07_06.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![系统日志](img/8884OS_07_06.jpg)'
- en: This piece of log looks long and weird. However, if you are a Java developer
    and you are familiar with the standard log library, Log4j, it is pretty straightforward.
    The beauty of Log4j is the provision of different log levels for us to control
    the granularity of the log statements to be recorded in `system.log`. As shown
    in the previous figure, the first word of each line is `INFO`, meaning that the
    log statement is a piece of information. Other log level choices include `FATAL`,
    `ERROR`, `WARN`, `DEBUG`, and `TRACE`, from the least verbose to the most verbose.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这条日志看起来很长且奇怪。然而，如果你是Java开发者并且熟悉标准日志库Log4j，它就相当简单。Log4j的美丽之处在于它为我们提供了不同的日志级别，以便我们控制记录在`system.log`中的日志语句的粒度。如图所示，每行的第一个单词是`INFO`，表示这是一条信息日志。其他日志级别选项包括`FATAL`、`ERROR`、`WARN`、`DEBUG`和`TRACE`，从最不详细到最详细。
- en: The system log is very valuable in troubleshooting problems as well. We may
    increase the log level to `DEBUG` or `TRACE` for troubleshooting. However, running
    a production Cassandra cluster in the `DEBUG` or `TRACE` mode will degrade its
    performance significantly. We must use them with great care.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 系统日志在故障排除问题中也非常有价值。我们可能需要将日志级别提高到`DEBUG`或`TRACE`以进行故障排除。然而，在生产Cassandra集群中以`DEBUG`或`TRACE`模式运行将显著降低其性能。我们必须非常小心地使用它们。
- en: 'We can change the standard log level in Cassandra by adjusting the `log4j.rootLogger`
    property in `log4j-server.properties` in the Cassandra configuration directory.
    The following screenshot shows the content of `log4j-server.properties` in ubtc02:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调整Cassandra配置目录中的`log4j-server.properties`文件中的`log4j.rootLogger`属性来更改Cassandra的标准日志级别。以下截图展示了ubtc02上的`log4j-server.properties`的内容：
- en: '![The system log](img/8884OS_07_07.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![系统日志](img/8884OS_07_07.jpg)'
- en: It is important to mention that `system.log` and `log4j-server.properties` are
    only responsible for a single node. For a cluster of two nodes, we will have two
    `system.log` and two `log4j-server.properties` on the respective nodes.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，`system.log`和`log4j-server.properties`仅负责单个节点。对于两个节点的集群，我们将在各自的节点上拥有两个`system.log`和两个`log4j-server.properties`。
- en: Performance tuning
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能调优
- en: Performance tuning is a large and complex topic that in itself can be a whole
    course. We can only scratch the surface of it in this short section. Similar to
    monitoring in the last section, operating system-specific performance tuning techniques
    are beyond the scope of this book.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 性能调优是一个庞大且复杂的话题，本身就可以成为一门完整的课程。我们只能在这简短的章节中触及表面。与上一节中的监控类似，特定于操作系统的性能调优技术超出了本书的范围。
- en: Java virtual machine
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Java虚拟机
- en: 'Based on the information given by the monitoring tools and the system log,
    we can discover opportunities for performance tuning. The first things we usually
    watch are the Java heap memory and garbage collection. JVM''s configuration settings
    are controlled in the environment settings file for Cassandra, `cassandra-env.sh`,
    located in `/etc/cassandra/`. An example is shown in the following screenshot:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于监控工具和系统日志提供的信息，我们可以发现性能调优的机会。我们通常首先关注的是Java堆内存和垃圾回收。Cassandra的环境设置文件`cassandra-env.sh`中控制了JVM的配置设置，该文件位于`/etc/cassandra/`目录下。以下截图展示了示例：
- en: '![Java virtual machine](img/8884OS_07_08.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![Java虚拟机](img/8884OS_07_08.jpg)'
- en: Basically, it already has the boilerplate options calculated to be optimized
    for the host system. It is also accompanied with explanation for us to tweak specific
    JVM parameters and the startup options of a Cassandra instance when we experience
    real issues; otherwise, these boilerplate options should not be altered.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，它已经包含了为优化主机系统计算出的样板选项。它还附带了解释，以便我们在遇到实际问题时调整特定的JVM参数和Cassandra实例的启动选项；否则，这些样板选项不应被更改。
- en: Note
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A detailed documentation on how to tune JVM for Cassandra can be found at [http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html](http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在[http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html](http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html)可以找到有关如何为Cassandra调整JVM的详细文档。
- en: Caching
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存
- en: Another area we should pay attention to is caching. Cassandra includes integrated
    caching and distributes cache data around the cluster. For a cache specific to
    a table, we will focus on the partition key cache and the row cache.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该注意的一个领域是缓存。Cassandra包括集成的缓存并在集群周围分布缓存数据。对于特定于表的缓存，我们将关注分区键缓存和行缓存。
- en: Partition key cache
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分区键缓存
- en: The partition key cache, or key cache for short, is a cache of the partition
    index for a table. Using the key cache saves processor time and memory. However,
    enabling just the key cache makes the disk activity actually read the requested
    data rows.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 分区键缓存，或简称键缓存，是表的分区索引缓存。使用键缓存可以节省处理器时间和内存。然而，仅启用键缓存会使磁盘活动实际上读取请求的数据行。
- en: Row cache
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 行缓存
- en: The row cache is similar to a traditional cache. When a row is accessed, the
    entire row is pulled into memory, merging from multiple SSTables when required,
    and cached. This prevents Cassandra from retrieving that row using disk I/O again,
    which can tremendously improve read performance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 行缓存类似于传统的缓存。当访问行时，整个行会被拉入内存，在需要时从多个SSTables合并，并缓存。这可以防止Cassandra再次使用磁盘I/O检索该行，从而极大地提高读取性能。
- en: When both row cache and partition key cache are configured, the row cache returns
    results whenever possible. In the event of a row cache miss, the partition key
    cache might still provide a hit that makes the disk seek much more efficient.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当同时配置了行缓存和分区键缓存时，行缓存尽可能返回结果。在行缓存未命中时，分区键缓存可能仍然提供命中，使磁盘查找更加高效。
- en: However, there is one caveat. Cassandra caches all the rows of a partition when
    reading that partition. So if the partition is large or only a small portion of
    the partition is read every time, the row cache might not be beneficial. It is
    very easy to be misused and consequently the JVM will be exhausted, causing Cassandra
    to fail. That is why the row cache is disabled by default.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个注意事项。当读取分区时，Cassandra会缓存该分区的所有行。因此，如果分区很大，或者每次只读取分区的一小部分，行缓存可能并不有利。它很容易被误用，从而导致JVM耗尽，导致Cassandra失败。这就是为什么行缓存默认是禁用的。
- en: Note
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We usually enable either the key or row cache for a table, not both at the same
    time.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常为表启用键缓存或行缓存中的一个，而不是同时启用两者。
- en: Monitoring cache
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控缓存
- en: 'Either the `nodetool info` command or JMX MBeans can provide assistance in
    monitoring cache. We should make changes to cache options in small, incremental
    adjustments, and then monitor the effects of each change using the nodetool utility.
    The last two lines of output of the `nodetool info` command, as seen in the following
    figure, contain the `Row Cache` and `Key Cache` metrics of `ubtc02`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要么使用`nodetool info`命令，要么使用JMX MBeans来提供监控缓存的帮助。我们应该对缓存选项进行小范围的增量调整，然后使用nodetool实用程序监控每次更改的效果。以下图中的`nodetool
    info`命令的最后两行输出包含了`ubtc02`的`Row Cache`和`Key Cache`指标：
- en: '![Monitoring cache](img/8884OS_07_09.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![监控缓存](img/8884OS_07_09.jpg)'
- en: In the event of high memory consumption, we can consider tuning data caches.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存消耗过高的情况下，我们可以考虑调整数据缓存。
- en: Enabling/disabling cache
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用/禁用缓存
- en: 'We use the CQL to enable or disable caching by altering the cache property
    of a table. For instance, we use the `ALTER TABLE` statement to enable the row
    cache for `watchlist`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用CQL通过更改表的缓存属性来启用或禁用缓存。例如，我们使用`ALTER TABLE`语句来启用`watchlist`的行缓存：
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Other available table caching options include `ALL`, `KEYS_ONLY` and `NONE`.
    They are quite self-explanatory and we do not go through each of them here.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可用的表缓存选项包括`ALL`、`KEYS_ONLY`和`NONE`。它们相当直观，我们在此不逐一介绍。
- en: Note
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Further information about data caching can be found at [http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html](http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据缓存的更多信息，可以在[http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html](http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html)找到。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter highlights the most important aspects of deploying a Cassandra
    cluster into the production environment. Cassandra can be taught to understand
    the physical location of the nodes in the cluster in order to intelligently manage
    its availability, scalability and performance. We deployed the Stock Screener
    Application to the production environment, though the scale is small. It is also
    valuable for us to learn how to migrate legacy data from a non-production environment.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了将Cassandra集群部署到生产环境中的最重要的方面。Cassandra可以学习理解集群中节点的物理位置，以便智能地管理其可用性、可扩展性和性能。尽管规模较小，我们还是将股票筛选器应用程序部署到了生产环境中。学习如何从非生产环境迁移旧数据对我们来说也很有价值。
- en: We then learned the basics of monitoring and performance tuning which are a
    must for a live running system. If you have experience in deploying other database
    and system, you may well appreciate the neatness and simplicity of Cassandra.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们学习了监控和性能调优的基础知识，这对于一个正在运行的系统来说是必不可少的。如果你有部署其他数据库和系统的经验，你可能会非常欣赏Cassandra的整洁和简单。
- en: In the next chapter, we will have a look at the supplementary information pertinent
    to application design and development. We will also summarize of the essence of
    each chapter.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨与应用设计和开发相关的补充信息。我们还将总结每一章的精髓。
