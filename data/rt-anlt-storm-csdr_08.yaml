- en: Chapter 8. Cassandra Management and Maintenance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。Cassandra管理和维护
- en: In this chapter, we will learn about the gossip protocol of Cassandra. Thereafter,
    we will delve into Cassandra administration and management in terms of understanding
    scaling and reliability in action. This will equip you with the ability to handle
    situations that you would not like to come across but do happen in production,
    such as handling recoverable nodes, rolling restarts, and so on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习Cassandra的八卦协议。然后，我们将深入了解Cassandra管理和管理，以了解扩展和可靠性的实际情况。这将使您能够处理您不希望遇到但在生产中确实发生的情况，例如处理可恢复节点、滚动重启等。
- en: 'The topics that will be covered in the chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Cassandra—gossip protocol
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cassandra——八卦协议
- en: Cassandra scaling—adding a new node to a cluster
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cassandra扩展——向集群添加新节点
- en: Replacing a node
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替换节点
- en: Replication factor changes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制因子更改
- en: Node tool commands
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点工具命令
- en: Rolling restarts and fault tolerance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动重启和容错
- en: Cassandra monitoring tools
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cassandra监控工具
- en: So, this chapter will help you understand the basics of Cassandra, as well as
    the various options required for the maintenance and management of Cassandra activities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本章将帮助您了解Cassandra的基础知识，以及维护和管理Cassandra活动所需的各种选项。
- en: Cassandra – gossip protocol
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassandra - 八卦协议
- en: Gossip is a protocol wherein periodically the nodes exchange information with
    other nodes about the nodes they know; this way, all the nodes obtain information
    about each other via this peer-to-peer communication mechanism. It's very similar
    to real-world and social media world gossip.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 八卦是一种协议，其中节点定期与其他节点交换关于它们所知道的节点的信息；这样，所有节点都通过这种点对点通信机制获取关于彼此的信息。这与现实世界和社交媒体世界的八卦非常相似。
- en: Cassandra executes this mechanism every second, and one node is capable of exchanging
    gossip information with up to three nodes in the cluster. All these gossip messages
    have a version associated with them to track the chronology, and the older gossip
    interaction updates are overwritten chronologically by newer ones.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra每秒执行一次这个机制，一个节点能够与集群中最多三个节点交换八卦信息。所有这些八卦消息都有与之关联的版本，以跟踪时间顺序，旧的八卦交互更新会被新的覆盖。
- en: 'Now that we know what Cassandra''s gossip is like at a very high level, let''s
    have a closer look at it and understand the purpose of this chatty protocol. Here
    are the two broad purposes served by having this in place:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道Cassandra的八卦在很高的层面上是什么样子，让我们更仔细地看看它，并了解这个多嘴的协议的目的。以下是通过实施这个协议所达到的两个广泛目的：
- en: Bootstrapping
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引导
- en: Failure scenario handling—detection and recovery
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障场景处理——检测和恢复
- en: Let's understand what they mean in action and what their contribution is towards
    the well-being and stability of a Cassandra cluster.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解它们在实际行动中的意义以及它们对Cassandra集群的健康和稳定性的贡献。
- en: Bootstrapping
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引导
- en: Bootstrapping is a process that is triggered in a cluster when a node joins
    the ring for the first time. It's the seed nodes that we define under the `Cassandra.yaml`
    configuration file that help the new nodes obtain the information about the cluster,
    ring, keyset, and partition ranges. It's recommended that you keep the setting
    similar throughout the cluster; otherwise, you could run into partitions within
    the cluster. A node remembers which nodes it has gossiped with even after it restarts.
    One more point to remember about seed nodes is that their purpose is to serve
    the nodes at the time of bootstrap; beyond this, its neither a single point of
    failure, nor does it serve any other purpose.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 引导是在集群中触发的一个过程，当一个节点第一次加入环时。我们在`Cassandra.yaml`配置文件下定义的种子节点帮助新节点获取有关集群、环、密钥集和分区范围的信息。建议您在整个集群中保持类似的设置；否则，您可能会在集群内遇到分区。一个节点在重新启动后会记住它与哪些节点进行了八卦。关于种子节点还有一点要记住，那就是它们的目的是在引导时为节点提供服务；除此之外，它既不是单点故障，也不提供任何其他目的。
- en: Failure scenario handling – detection and recovery
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障场景处理——检测和恢复
- en: Well, the gossip protocol is Cassandra's own efficient way of knowing when a
    failure has occurred; that is, the entire ring gets to know about a downed host
    through gossip. On a contrary, situation when a node joins the cluster, the same
    mechanism is employed to inform the all nodes in the ring.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，八卦协议是Cassandra自己有效地知道何时发生故障的方式；也就是说，整个环都通过八卦知道了一个宕机的主机。相反的情况是，当一个节点加入集群时，同样的机制被用来通知环中的所有节点。
- en: Once Cassandra detects a failure of a nodes on the ring, it stops routing the
    client requests to it—failure definitely has some impact on the overall performance
    of the cluster. However, it's never a blocker until we have enough replicas for
    consistency to be served to the client.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Cassandra检测到环中的节点故障，它就会停止将客户端请求路由到该节点——故障确实对集群的整体性能产生了一定影响。然而，除非我们有足够的副本以确保一致性提供给客户端，否则它永远不会成为阻碍。
- en: Another interesting fact about gossip is that it happens at various levels—Cassandra
    gossip, like real-world gossip, could be secondhand or thirdhand and so on; this
    is the manifestation of indirect gossip.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 关于八卦的另一个有趣事实是，它发生在各个层面——Cassandra的八卦，就像现实世界的八卦一样，可能是二手或三手等等；这是间接八卦的表现。
- en: Failure of a node could be actual or virtual. This means that either a node
    can actually fail due to system hardware giving away, or the failure could be
    virtual, wherein, for a while, network latency is so high that it would seem that
    the node is not responding. The latter scenarios, most of the time, are self-recoverable;
    that is, after a while, networks return to normalcy, and the nodes are detected
    in the ring once again. The live nodes keep trying to ping and gossip with the
    failed node periodically to see if they are up. If a node is to be declared as
    permanently departed from the cluster, we require some admin intervention to explicitly
    remove the node from the ring.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的故障可能是实际的或虚拟的。这意味着节点可能由于系统硬件故障而实际失败，或者故障可能是虚拟的，即在一段时间内，网络延迟非常高，以至于似乎节点没有响应。后一种情况大多数情况下是自我恢复的；也就是说，一段时间后，网络恢复正常，节点再次在环中被检测到。活动节点会定期尝试对失败的节点进行ping和gossip，以查看它们是否正常。如果要将节点声明为永久离开集群，我们需要一些管理员干预来明确地从环中删除节点。
- en: When a node is joined back to the cluster after quite a while, it's possible
    that it might have missed a couple of writes (inserts/updates/deletes), and thus,
    the data on the node is far from being accurate as per the latest state of data.
    It's advisable to run a repair using the `nodetool repair` command.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点在相当长时间后重新加入集群时，可能会错过一些写入（插入/更新/删除），因此，节点上的数据远非根据最新数据状态准确。建议使用`nodetool repair`命令运行修复。
- en: Cassandra cluster scaling – adding a new node
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassandra集群扩展-添加新节点
- en: 'Cassandra scales very easily, and with zero downtime. This is one of the reasons
    why it is chosen over many other contenders. The steps are pretty straightforward
    and simple:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra非常容易扩展，并且无需停机。这是它被选择而不是许多其他竞争者的原因之一。步骤非常简单明了：
- en: 'You need to set up Cassandra on the nodes to be added. Don''t start the Cassandra
    process yet; first, follow these steps:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要在要添加的节点上设置Cassandra。但是先不要启动Cassandra进程；首先按照以下步骤操作：
- en: Update the seed nodes in `Cassandra.yaml` under `seed_provider`.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`seed_provider`下的`Cassandra.yaml`中更新种子节点。
- en: Make sure the `tmp` folders are clean.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保`tmp`文件夹是干净的。
- en: Add `auto_bootstrap` to `Cassandra.yaml` and set it to `true`.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Cassandra.yaml`中添加`auto_bootstrap`并将其设置为`true`。
- en: Update `cluster_name` in `Cassandra.yaml`.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Cassandra.yaml`中更新`cluster_name`。
- en: Update `listen_address`/`broadcast_address` in `Cassandra.yaml`.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新`Cassandra.yaml`中的`listen_address`/`broadcast_address`。
- en: Start all the new nodes one by one, pausing for at least 5 minutes between two
    consecutive starts.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐个启动所有新节点，每两次启动之间至少暂停5分钟。
- en: 'Once the node is started, it will proclaim its share of data based on the token
    range it owns and start streaming that in. This could be verified using the `nodetoolnetstat`
    command, as shown in the following code:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦节点启动，它将根据自己拥有的标记范围宣布其数据份额并开始流式传输。可以使用`nodetoolnetstat`命令进行验证，如下面的代码所示：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After all the nodes are joined to the cluster, it''s strictly recommended that
    you run a `nodetool cleanup` command on all the nodes. This is recommended so
    that they relinquish the control of the keys that were formerly owned by them
    but now belong to the new nodes that have joined the cluster. Here is the command
    and the execution output:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有节点加入集群后，强烈建议在所有节点上运行`nodetool cleanup`命令。这是为了让它们放弃以前由它们拥有但现在属于已加入集群的新节点的键的控制。以下是命令和执行输出：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that the `NodeCmd` process is actually the cleanup process for the Cassandra
    daemon. The disk space reclaimed after the cleanup on the preceding node is shown
    here:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，`NodeCmd`进程实际上是Cassandra守护程序的清理过程。在前一个节点上清理后回收的磁盘空间显示在这里：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Cassandra cluster – replacing a dead node
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassandra集群-替换死节点
- en: This section captures the various situations and scenarios that can occur and
    cause failures in a Cassandra cluster. We will also equip you with the knowledge
    and talk about the steps to handle these situations. These situations are specific
    to version 1.1.6 but can be applied to others as well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了可能发生并导致Cassandra集群故障的各种情况和场景。我们还将为您提供处理这些情况的知识并讨论相关步骤。这些情况特定于版本1.1.6，但也适用于其他版本。
- en: 'Say, this is the problem: you''re running an n node, for example let''s say
    there are three node clusters and from that one node goes down; this will result
    in unrecoverable hardware failure. The solution is this: replace the dead nodes
    with new nodes.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设问题是这样的：您正在运行一个n节点，例如，假设有三个节点集群，其中一个节点宕机；这将导致不可恢复的硬件故障。解决方案是：用新节点替换死节点。
- en: 'The following are the steps to achieve the solution:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是实现解决方案的步骤：
- en: 'Confirm the node failure using the `nodetool ring` command:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nodetool ring`命令确认节点故障：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The dead node will be shown as `DOWN`; let''s assume `node3` is down:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 死节点将显示为`DOWN`；假设`node3`已宕机：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Install and configure Cassandra on the replacement node. Make sure we remove
    the old installation, if any, from the replaced Cassandra node using the following
    command:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在替换节点上安装和配置Cassandra。确保使用以下命令从替换的Cassandra节点中删除旧安装（如果有）：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, `/var/lib/cassandra` is the path of the Cassandra data directory for Cassandra.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`/var/lib/cassandra`是Cassandra的数据目录的路径。
- en: Configure `Cassandra.yaml` so that it holds the same non-default settings as
    that of the pre-existing Cassandra cluster.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置`Cassandra.yaml`，使其具有与现有Cassandra集群相同的非默认设置。
- en: Set the `initial_token` range in the `cassandra.yaml` file of the replacement
    node to the value of the dead node's token 1, that is, `42535295865117307932921825928971026431`.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在替换节点的`cassandra.yaml`文件中，将`initial_token`范围设置为死节点的标记1的值，即`42535295865117307932921825928971026431`。
- en: 'Starting the new node will join the cluster at one place prior to the dead
    node in the ring:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动新节点将在环中死节点的前一个位置加入集群：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We are almost done. Just run `nodetool repair` on each node on each keyspace:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们快要完成了。只需在每个keyspace的每个节点上运行`nodetool repair`：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Remove the token of the dead node from the ring using the following command:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从环中删除死节点的令牌：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This command needs to be executed on all the remaining nodes to make sure all
    the live nodes know that the dead node is no longer available.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令需要在所有剩余的节点上执行，以确保所有活动节点知道死节点不再可用。
- en: This removes the dead node from the cluster; now we are done.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将从集群中删除死节点；现在我们完成了。
- en: The replication factor
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制因子
- en: 'Occasionally, there are instances when we come across situations where we make
    changes to the replication factor. For example, I started with a smaller cluster
    so I kept my replication factor as 2\. Later, I scaled out from 4 nodes to 8 nodes,
    and thus to make my entire setup more fail-safe, I increased my replication factor
    to 4\. In such situations, the following steps are to be followed:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 偶尔，我们会遇到需要改变复制因子的情况。例如，我开始时使用较小的集群，所以将复制因子保持为2。后来，我从4个节点扩展到8个节点，为了使整个设置更加安全，我将复制因子增加到4。在这种情况下，需要按照以下步骤进行操作：
- en: 'The following is the command to update the replication factor and/or change
    the strategy. Execute these commands on the Cassandra CLI:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是用于更新复制因子和/或更改策略的命令。在Cassandra CLI上执行这些命令：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once the command has been updated, you have to execute the `nodetool` repair
    on each of the nodes one by one (in succession) so that all the keys are correctly
    replicated as per the new replication values:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦命令已更新，您必须依次在每个节点上执行`nodetool`修复，以确保所有键根据新的复制值正确复制：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The following `compactionstats` command is used to track the progress of the
    `nodetool repair` command.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`compactionstats`命令用于跟踪`nodetool repair`命令的进度。
- en: The nodetool commands
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: nodetool命令
- en: 'The `nodetool` command in Cassandra is the most handy tool in the hands of
    a Cassandra administrator. It has all the tools and commands that are required
    for all types of situational handling of various nodes. Let''s look at a few widely
    used ones closely:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra中的`nodetool`命令是Cassandra管理员手中最方便的工具。它具有所有类型的节点各种情况处理所需的工具和命令。让我们仔细看看一些广泛使用的命令：
- en: '`Ring`: This command depicts the state of nodes (normal, down, leaving, joining,
    and so on). The ownership of the token range and percentage ownership of the keys
    along with the data centre and rack details is as follows:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ring`：此命令描述节点的状态（正常、关闭、离开、加入等）。令牌范围的所有权和键的百分比所有权以及数据中心和机架详细信息如下：'
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will be something like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Join`: This is the option you can use with `nodetool`, which needs to be executed
    to add the new node to the cluster. When a new node joins the cluster, it starts
    streaming the data from other nodes until it receives all the keys as per its
    designated ownership based on the token in the ring. The status for this can be
    checked using the `netsat` commands:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Join`：这是您可以与`nodetool`一起使用的选项，需要执行以将新节点添加到集群中。当新节点加入集群时，它开始从其他节点流式传输数据，直到根据环中的令牌确定的所有键都到达其指定的所有权。可以使用`netsat`命令检查此状态：'
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`Info`: This `nodetool` option gets all the required information about the
    node specified in the following command:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Info`：此`nodetool`选项获取有关以下命令指定的节点的所有必需信息：'
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Cleanup`: This is the option that is generally used when we scale the cluster.
    New nodes are added and thus the existing nodes need to relinquish the control
    of the keys that now belong to the new entrants in the cluster:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cleanup`：这通常是在扩展集群时使用的选项。添加新节点，因此现有节点需要放弃现在属于集群中新成员的键的控制权：'
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`Compaction`: This is one of the most useful tools. It''s used to explicitly
    issue the `compact` command to Cassandra. This can be done on the entire node,
    key space, or at the column family level:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Compaction`：这是最有用的工具之一。它用于明确向Cassandra发出`compact`命令。这可以在整个节点、键空间或列族级别执行：'
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Cassandra has two types of compactions: minor compaction and major compaction.
    The minor cycle of compaction gets executed whenever a new `sstable` data is created
    to remove all the tombstones (that is, the deleted entries).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra有两种类型的压缩：小压缩和大压缩。小压缩周期在创建新的`sstable`数据时执行，以删除所有墓碑（即已删除的条目）。
- en: The major compaction is something that's triggered manually, using the preceding
    `nodetool` command. This can be applied to the node, keyspace, and a column family
    level.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 主要压缩是手动触发的，使用前面的`nodetool`命令。这可以应用于节点、键空间和列族级别。
- en: '`Decommission`: This is, in a way, the opposite of bootstrap and is triggered
    when we want a node to leave the cluster. The moment a live node receives the
    command, it stops accepting new rights, flushes the `memtables`, and starts streaming
    the data from itself to the nodes that would be a new owner of the key range it
    currently owns:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Decommission`：这在某种程度上是引导的相反，当我们希望节点离开集群时触发。一旦活动节点接收到命令，它将停止接受新的权限，刷新`memtables`，并开始从自身流式传输数据到将成为当前拥有键范围的新所有者的节点：'
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`Removenode`: This command is executed when a node is dead, that is, physically
    unavailable. This informs the other nodes about the node being unavailable. Cassandra
    replication kicks into action to restore the correct replication by creating copies
    of data as per the new ring ownership:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Removenode`：当节点死亡，即物理不可用时，执行此命令。这通知其他节点节点不可用。Cassandra复制开始工作，通过根据新的环所有权创建数据的副本来恢复正确的复制：'
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`Repair`: This `nodetool repair` command is executed to fix the data on any
    node. This is a very important tool to ensure that there is data consistency and
    the nodes that join the cluster back after a period of time exist. Let''s assume
    a cluster with four nodes that are catering to continuous writes through a storm
    topology. Here, one of the nodes goes down and joins the ring again after an hour
    or two. Now, during this duration, the node might have missed some writes; to
    fix this data, we should execute a `repair` command on the node:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`修复`：执行此`nodetool repair`命令以修复任何节点上的数据。这是确保数据一致性以及在一段时间后重新加入集群的节点存在的非常重要的工具。假设有一个由四个节点组成的集群，这些节点通过风暴拓扑不断进行写入。在这里，其中一个节点下线并在一两个小时后重新加入环。现在，在此期间，该节点可能错过了一些写入；为了修复这些数据，我们应该在节点上执行`repair`命令：'
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Cassandra fault tolerance
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassandra容错
- en: Well, one of the prime reasons for using Cassandra as a data store is its fault-tolerant
    capabilities. It's not driven by a typical master-slave architecture, where failure
    of the master becomes a single point of system breakdown. Instead, it harbors
    a concept of operating in a ring mode so that there is no single point of failure.
    Whenever required, we can restart the nodes without the dread of bringing the
    whole cluster down; there are various situations where this capability comes in
    handy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Cassandra作为数据存储的主要原因之一是其容错能力。它不是由典型的主从架构驱动的，其中主节点的故障成为系统崩溃的单一点。相反，它采用环模式的概念，因此没有单一故障点。在需要时，我们可以重新启动节点，而不必担心将整个集群带下线；在各种情况下，这种能力都非常方便。
- en: 'There are situations where we need to restart Cassandra, but Cassandra''s ring
    architecture equips the administrator to do this seamlessly with zero downtime
    for the cluster. This means that in situations such as the following that requires
    a Cassandra cluster to be restarted, a Cassandra administrator can restart the
    nodes one by one instead of bringing down the entire cluster and then starting
    it:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有时需要重新启动Cassandra，但Cassandra的环架构使管理员能够在不影响整个集群的情况下无缝进行此操作。这意味着在需要重新启动Cassandra集群的情况下，例如需要逐个重新启动节点而不是将整个集群带下线然后重新启动的情况下，Cassandra管理员可以逐个重新启动节点：
- en: Starting the Cassandra daemon with changes in the memory configuration
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内存配置更改启动Cassandra守护程序
- en: Enabling JMX on an already running Cassandra cluster
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在已运行的Cassandra集群上启用JMX
- en: Sometimes machines have routine maintenance and need restarts
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时机器需要例行维护和重新启动
- en: Cassandra monitoring systems
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassandra监控系统
- en: Now that we have discussed the various management aspects of Cassandra, let's
    explore the various dashboarding and monitoring options for the Cassandra cluster.
    There are various free and licensed tools available that we'll discuss now.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了Cassandra的各种管理方面，让我们探索Cassandra集群的各种仪表板和监控选项。现在有各种免费和许可的工具可用，我们将在下面讨论。
- en: JMX monitoring
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JMX监控
- en: 'You can use a type of monitoring for Cassandra that is based on `jconsole`.
    Here are the steps to connect to Cassandra using `jconsole`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用基于`jconsole`的一种监控Cassandra的类型。以下是使用`jconsole`连接到Cassandra的步骤：
- en: In the Command Prompt, execute the `jconsole` command:![JMX monitoring](img/00051.jpeg)
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令提示符中，执行`jconsole`命令：![JMX监控](img/00051.jpeg)
- en: In the next step, you have to specify the Cassandra node IP and port for connectivity:![JMX
    monitoring](img/00052.jpeg)
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，您必须指定Cassandra节点的IP和端口以进行连接：![JMX监控](img/00052.jpeg)
- en: Once you are connected, JMX provides a variety of graphs and monitoring utilities:![JMX
    monitoring](img/00053.jpeg)
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦连接，JMX提供各种图形和监控实用程序：![JMX监控](img/00053.jpeg)
- en: The developers can monitor heap memory usage using the jconsole **Memory** tab.
    This will help you understand the utilization of node resources.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以使用jconsole的**内存**选项卡监视堆内存使用情况。这将帮助您了解节点资源的利用情况。
- en: The limitation with jconsole is that it performs node-specific monitoring and
    not Cassandra-ring-based monitoring and dashboarding. Let's explore the other
    tools in the context.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: jconsole的限制在于它执行特定于节点的监控，而不是基于Cassandra环的监控和仪表板。让我们在这个背景下探索其他工具。
- en: Datastax OpsCenter
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Datastax OpsCenter
- en: This is a datastax-provided utility with a graphical interface that lets the
    user monitor and execute administrative activities from one central dashboard.
    Note that a free version is available only for nonproduction usage.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个由Datastax提供的实用程序，具有图形界面，可以让用户从一个中央仪表板监视和执行管理活动。请注意，免费版本仅适用于非生产用途。
- en: 'Datastax Ops center provides a lot of graphical representations for various
    important system **Key Performance Indicators** (**KPIs**), such as performance
    trends, summary, and so on. Its UI also provides a historic data analysis and
    drill down capability on single data points. OpsCenter stores all its metrics
    in Cassandra itself. The key features of the OpsCenter utility are as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Datastax Ops Center为各种重要的系统**关键性能指标**（**KPI**）提供了许多图形表示，例如性能趋势、摘要等。其用户界面还提供了对单个数据点的历史数据分析和深入分析能力。OpsCenter将其所有指标存储在Cassandra本身中。OpsCenter实用程序的主要特点如下：
- en: KPI-based monitoring for the entire cluster
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于KPI的整个集群监控
- en: Alerts and alarms
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 警报和报警
- en: Configuration management
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置管理
- en: Easy to set up
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于设置
- en: 'You can install and set up OpsCenter using the following simple steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下简单步骤安装和设置OpsCenter：
- en: 'Run the following command to get started:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令开始：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Connect to OpsCenter in a web browser at `http://localhost:8888`.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Web浏览器中连接到OpsCenter，网址为`http://localhost:8888`。
- en: You will get a welcome screen where you will have options to spawn a new cluster
    or connect to an existing one.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将获得一个欢迎屏幕，在那里您将有选项生成一个新集群或连接到现有集群。
- en: Next, configure the agent; once this is done, OpsCenter is ready for use.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，配置代理；一旦完成，OpsCenter即可使用。
- en: 'Here is a screenshot from the application:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是应用程序的屏幕截图：
- en: '![Datastax OpsCenter](img/00054.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![Datastax OpsCenter](img/00054.jpeg)'
- en: 'Here we choose the metric to be executed and whether the operation is to be
    performed on a specific node or all the nodes. The following screenshot captures
    OpsCenter starting up and recognizing the various nodes in the cluster:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择要执行的度量标准以及操作是在特定节点上执行还是在所有节点上执行。以下截图捕捉了OpsCenter启动并识别集群中的各个节点的情况：
- en: '![Datastax OpsCenter](img/00055.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Datastax OpsCenter](img/00055.jpeg)'
- en: 'The following screenshot captures various KPIs in the aspects of read and writes
    to the cluster, the overall cluster latency, disk I/O, and so on:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图捕捉了集群读写、整体集群延迟、磁盘I/O等方面的各种关键绩效指标：
- en: '![Datastax OpsCenter](img/00056.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![Datastax OpsCenter](img/00056.jpeg)'
- en: Quiz time
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测验时间
- en: Q.1\. State whether the following statements are true or false.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Q.1. 判断以下陈述是真还是假。
- en: Cassandra has a single point of failure.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cassandra存在单点故障。
- en: A dead node is immediately detected in a Cassandra ring.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cassandra环中立即检测到死节点。
- en: Gossip is a data exchange protocol.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gossip是一种数据交换协议。
- en: The `decommission` and `removenode` commands are same.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`decommission`和`removenode`命令是相同的。'
- en: Q.2\. Fill in the blanks.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Q.2. 填空。
- en: _______________ is the command used to run compactions.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: _______________ 是运行压缩的命令。
- en: _______________ is the command to get the information about a live node.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: _______________ 是获取有关活动节点信息的命令。
- en: ___________ is the command that displays the entire cluster information.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ___________ 是显示整个集群信息的命令。
- en: 'Q.3\. Execute the following use case to see Cassandra high availability and
    replications:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Q.3. 执行以下用例以查看Cassandra的高可用性和复制：
- en: Creating a 4-node Cassandra cluster.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个4节点的Cassandra集群。
- en: Creating a keyspace with a replication factor of 3.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个副本因子为3的键空间。
- en: Bringing down a Cassandra daemon on one the nodes.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭一个节点上的Cassandra守护程序。
- en: Executing `nestat` on each node to see the data streaming.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个节点上执行`nestat`以查看数据流。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about the concepts of the gossip protocol and adapted
    tools used for various scenarios such as scaling the cluster, replacing a dead
    node, compaction, and repair operations on Cassandra.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了疏散协议的概念和用于各种场景的适应工具，例如扩展集群、替换死节点、压缩和修复Cassandra上的操作。
- en: In the next chapter, we will discuss storm cluster maintenance and operational
    aspects.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论风暴集群的维护和运营方面。
