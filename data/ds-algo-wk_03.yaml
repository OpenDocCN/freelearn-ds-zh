- en: Decision Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: A decision tree is the arrangement of data in a tree structure where, at each
    node, data is separated into different branches according to the value of the
    attribute at the node.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是将数据按树状结构排列的方式，在每个节点处，根据该节点的属性值将数据分成不同的分支。
- en: To construct a decision tree, we will use a standard ID3 learning algorithm
    that chooses an attribute that classifies data samples in the best possible way
    to maximize the information gain—a measure based on information entropy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建决策树，我们将使用标准的ID3学习算法，该算法选择一个属性，能以最好的方式对数据样本进行分类，从而最大化信息增益——这是一个基于信息熵的度量方法。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: What a decision tree is and how to represent data in a decision tree through
    the swim preference example
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是什么，如何通过游泳偏好的示例在决策树中表示数据
- en: The concepts of information entropy and information gain, theoretically in the
    first instance, before applying the swim preference example in practical terms
    in the *Information theory* section
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息熵和信息增益的概念，首先在理论上进行说明，然后在*信息论*部分通过游泳偏好的示例进行实际应用
- en: How to use a ID3 algorithm to construct a decision tree from the training data,
    and its implementation in Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用ID3算法从训练数据构建决策树，并在Python中实现
- en: How to classify new data items using the constructed decision tree through the
    swim preference example
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过游泳偏好的示例，使用构建的决策树对新的数据项进行分类
- en: How to carry out an alternative analysis of the chess playing problem in [Chapter
    2](4f54d05d-6791-47c5-a260-c2fd563a55cf.xhtml), Naive Bayes, using decision trees
    and how the results of the two algorithms may differ
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用决策树对[第2章](4f54d05d-6791-47c5-a260-c2fd563a55cf.xhtml)中的国际象棋问题进行替代分析，采用朴素贝叶斯方法，并比较两种算法结果的差异
- en: You will verify your understanding in the *Problems* section and look at when
    to use decision trees as a method of analysis
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将在*问题*部分验证自己的理解，并了解何时使用决策树作为分析方法
- en: How to deal with data inconsistencies during decision tree construction with
    the *Going shopping* example
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在决策树构建过程中处理数据不一致性，使用*购物问题*示例进行说明
- en: Swim preference – representing data using a decision tree
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游泳偏好——通过决策树表示数据
- en: 'We may have certain preferences that determine whether or not we would swim.
    These can be recorded in a table, as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能有一些偏好，决定是否游泳。这些偏好可以记录在表格中，如下所示：
- en: '| **Swimming suit** | **Water temperature** | **Swim preference** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **泳衣** | **水温** | **游泳偏好** |'
- en: '| None | Cold | No |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 冷 | 否 |'
- en: '| None | Warm | No |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 温暖 | 否 |'
- en: '| Small | Cold | No |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 小 | 冷 | 否 |'
- en: '| Small | Warm | No |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 小 | 温暖 | 否 |'
- en: '| Good | Cold | No |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 好 | 冷 | 否 |'
- en: '| Good | Warm | Yes |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 好 | 温暖 | 是 |'
- en: 'The data in this table can alternatively be presented in the following decision
    tree:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这张表中的数据可以通过以下决策树呈现：
- en: '![](img/b974fd83-2b36-4328-8c77-02a7deccd42d.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b974fd83-2b36-4328-8c77-02a7deccd42d.png)'
- en: 'Figure 3.1: Decision tree for the Swim preference example'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：游泳偏好示例的决策树
- en: At the root node, we ask the question—do you have a swimming suit? The response
    to the question separates the available data into three groups, each with two
    rows. If the attribute is `swimming suit = none`, then two rows have the swim
    preference attribute as `no`. Therefore, there is no need to ask a question about
    the temperature of the water, as all the samples with the `swimming suit = none`
    attribute would be classified as `no`. This is also true for the `swimming suit
    = small` attribute. In the case of `swimming suit = good`, the remaining two rows
    can be divided into two classes – `no` and `yes`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在根节点，我们问一个问题——你有泳衣吗？问题的回答将可用数据分成三个组，每个组有两行。如果属性是`泳衣 = 无`，那么这两行的游泳偏好都是`否`。因此，不需要再问水温的问题，因为所有具有`泳衣
    = 无`属性的样本都会被分类为`否`。`泳衣 = 小`属性也一样。对于`泳衣 = 好`，剩下的两行可以分为两类——`否`和`是`。
- en: Without further information, we would not be able to classify each row correctly.
    Fortunately, there is one more question that can be asked about each row that
    classifies it correctly. For the row with the `water=cold` attribute, the swimming
    preference is `no`. For the row with the `water=warm` attribute, the swimming
    preference is `yes`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有更多的信息，我们将无法正确分类每一行。幸运的是，对于每一行，还有一个问题可以问，这个问题能够正确地将其分类。对于`水=冷`的行，游泳偏好是`否`；对于`水=温暖`的行，游泳偏好是`是`。
- en: To summarize, starting with the root node, we ask a question at every node and,
    based on the answer, we move down the tree until we reach a leaf node where we
    find the class of the data item corresponding to those answers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，从根节点开始，我们在每个节点上提问，并根据答案向下移动树直到到达叶节点，在那里我们可以找到与这些答案对应的数据项的类别。
- en: This is how we can use a ready-made decision tree to classify samples of data.
    But it is also important to know how to construct a decision tree from data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们如何使用现成的决策树来分类数据样本。但同样重要的是，知道如何从数据中构建决策树。
- en: Which attribute has a question at which node? How does this reflect on the construction
    of a decision tree? If we change the order of the attributes, can the resulting
    decision tree classify better than another tree?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个属性在什么节点上有一个问题？这如何影响决策树的构建？如果我们改变属性的顺序，最终得到的决策树能否比另一棵树更好地进行分类？
- en: Information theory
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息论
- en: Information theory studies the quantification of information, its storage, and
    communication. We introduce concepts of information entropy and information gain,
    which are used to construct a decision tree using the ID3 algorithm.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 信息论研究信息的量化、存储和传递。我们引入了信息熵和信息增益的概念，它们用于使用ID3算法构建决策树。
- en: Information entropy
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息熵
- en: The information entropy of any given piece data is a measure of the smallest
    amount of information necessary to represent a data item from that data. The units
    of information entropy are familiar - bits, bytes, kilobytes, and so on. The lower
    the information entropy, the more regular the data is, and the more patterns occur
    in the data, thus, the smaller the quantity of information required to represent
    it. That is how compression tools on computers can take large text files and compress
    them to a much smaller size, as words and word expressions keep reoccurring, forming
    a pattern.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 任何给定数据的**信息熵**是表示该数据项所需的最小信息量的度量。信息熵的单位是熟悉的——比特、字节、千字节等等。信息熵越低，数据就越规律，数据中出现的模式也就越多，从而表示该数据所需的信息量越小。这就是为什么计算机上的压缩工具可以将大型文本文件压缩到更小的大小，因为单词和短语不断重复，形成了模式。
- en: Coin flipping
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抛硬币
- en: Imagine we flip an unbiased coin. We would like to know whether the result is
    heads or tails. How much information do we need to represent the result? Both
    words, head, and tail, consist of four characters, and if we represent one character
    with one byte (8 bits), as is standard in the ASCII table, then we would need
    four bytes, or 32 bits, to represent the result.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们抛一枚不偏的硬币。我们想知道结果是正面还是反面。我们需要多少信息来表示这个结果？无论是正面还是反面，两个单词都由四个字符组成，如果我们用一个字节（8位）来表示一个字符，按照ASCII表的标准，那么我们需要四个字节，或者32位，来表示这个结果。
- en: But information entropy is the smallest amount of data necessary to represent
    the result. We know that there are only two possible results—heads or tails. If
    we agree to represent head with 0 and tail with 1, then 1 bit would be sufficient
    to communicate the result efficiently. Here, the data is the space of the possibilities
    of the result of the coin throw. It is the set `{head,tail}` that can be represented
    as a set `{0,1}`. The actual result is a data item from this set. It turns out
    that the entropy of the set is 1\. This is owing to the probability of head and
    tail both being 50%.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但信息熵是表示结果所需的最小数据量。我们知道结果只有两种可能——正面或反面。如果我们同意用0表示正面，用1表示反面，那么1比特足以有效地传达结果。在这里，数据是硬币抛掷结果可能性的空间。它是集合`{head,
    tail}`，可以表示为集合`{0, 1}`。实际的结果是该集合中的一个数据项。结果表明，这个集合的熵是1。因为正面和反面的概率都是50%。
- en: Now imagine that the coin is biased and throws heads 25% of the time and tails
    75% of the time. What would the entropy of the probability space `{0,1}` be this
    time? We could certainly represent the result with one bit of information. But
    can we do better? One bit is, of course, indivisible, but maybe we could generalize
    the concept of information to indiscrete amounts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设硬币是偏的，每次抛出时正面朝上的概率是25%，反面朝上的概率是75%。这时，概率空间`{0,1}`的熵是多少？我们当然可以用1比特的信息表示结果。但是我们能做得更好吗？1比特当然是不可分割的，但也许我们可以将信息的概念推广到非离散的量。
- en: In the previous example, we knew nothing about the previous result of the coin
    flip unless we looked at the coin. But in the example with the biased coin, we
    know that the result is more likely to be tails. If we recorded *n* results of
    coin flips in a file representing heads with 0 and tails with 1, then about 75 percent
    of the bits there would have the value 1, and 25 percent of them would have the
    value 0\. The size of such a file would be *n* bits. But since it is more regular
    (a pattern of 1s prevails in it), a good compression tool should be able to compress
    it to less than *n* bits.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，除非我们查看硬币，否则我们对前一次抛硬币的结果一无所知。但在偏向硬币的例子中，我们知道结果更可能是反面。如果我们在文件中记录了*n*次抛硬币的结果，将正面表示为0，反面表示为1，那么其中大约75%的比特将为1，25%的比特将为0。这样的文件大小将为*n*比特。但由于它更有规律（1的模式占主导地位），一个好的压缩工具应该能够将其压缩到小于*n*比特。
- en: To learn the theory behind to compression and how much information is necessary
    to represent a data item, let's take a look at a precise definition of information
    entropy.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习压缩背后的理论以及表示数据项所需的信息量，让我们来看一下信息熵的精确定义。
- en: Definition of information entropy
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息熵的定义
- en: 'Suppose that we are given a probability space, *S*, with the elements *1, 2,
    ..., n*. The probability that an element, *i*, will be chosen from the probability
    space is *p[i]*. The information entropy of the probability space is then defined
    as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们给定了一个概率空间*S*，其中包含元素*1, 2, ..., n*。从概率空间中选择元素*i*的概率为*p[i]*。该概率空间的信息熵定义如下：
- en: '![](img/7e5fdefb-a6bf-44f2-8797-bb12161e973b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e5fdefb-a6bf-44f2-8797-bb12161e973b.png)'
- en: In the preceding formula, *log[2]* is a binary logarithm.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*log[2]*是二进制对数。
- en: 'Hence, the information entropy of the probability space of unbiased coin throws
    is as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，公正抛硬币的概率空间的信息熵如下：
- en: '![](img/25c77162-1f29-42fb-9d8a-3aacab40d80e.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25c77162-1f29-42fb-9d8a-3aacab40d80e.png)'
- en: 'When the coin is biased, with a 25% chance of heads and a 75% chance of tails,
    then the information entropy of such a space is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当硬币偏向时，正面出现的概率为25%，反面为75%，此时该空间的**信息熵**如下：
- en: '![](img/cd1627da-fe47-48b2-8b52-dde0f8070f10.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd1627da-fe47-48b2-8b52-dde0f8070f10.png)'
- en: This is less than 1\. Thus, for example, if we had a large file with about 25%
    of 0 bits and 75% of 1 bits, a good compression tool should be able to compress
    it down to about 81.12% of its size.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值小于1。因此，例如，如果我们有一个大文件，其中约25%的比特是0，75%的比特是1，那么一个好的压缩工具应该能够将其压缩到大约81.12%的大小。
- en: Information gain
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息增益
- en: Information gain is the amount of information entropy gained as a result of
    a certain procedure. For example, if we would like to know the results of three
    fair coins, then the information entropy is 3\. But if we could look at the third
    coin, then the information entropy of the result for the remaining two coins would
    be 2\. Thus, by looking at the third coin, we gained one bit of information, so
    the information gain is 1.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益是通过某一过程获得的信息熵量。例如，如果我们想知道三次公正的抛硬币结果，那么信息熵为3。但如果我们能查看第三次抛掷的结果，那么剩下两次抛掷的结果的信息熵为2。因此，通过查看第三次抛掷，我们获得了一位信息，所以信息增益为1。
- en: 'We may also gain the information entropy by dividing the whole set, *S*, into
    sets, grouping them by a similar pattern. If we group elements by their value
    of an attribute, *A*, then we define the information gain as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过将整个集合*S*划分为按相似模式分组的集合，来获得信息熵。如果我们按属性*A*的值对元素进行分组，那么我们将信息增益定义如下：
- en: '![](img/f5d65d98-1431-4e05-9704-64d62f7dc1a5.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5d65d98-1431-4e05-9704-64d62f7dc1a5.png)'
- en: Here, *S[v]*, is a set with the elements of *S* that have the value, *v*, for
    the attribute, *A*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*S[v]* 是集合*S*中所有属性*A*值为*v*的元素集合。
- en: Swim preference – information gain calculation
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游泳偏好 – 信息增益计算
- en: 'Let''s calculate the information gain for the six rows in the swim preference
    example by taking a `swimming suit` as an attribute. Because we are interested
    in whether a given row of data is classified as `no` or `yes` in response to the
    question as to whether you should go for a swim, we will use the swim preference
    to calculate the entropy and information gain. We partition the set *S* using
    the `swimming suit` attribute:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将`游泳衣`作为属性，计算游泳偏好示例中六行数据的**信息增益**。因为我们关心的是给定数据行是否会被分类为`否`或`是`，即是否应该去游泳的问题，因此我们将使用游泳偏好来计算熵和信息增益。我们用`游泳衣`属性来划分集合*S*：
- en: '*S[none]={(none,cold,no),(none,warm,no)}*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*S[none]={(none,cold,no),(none,warm,no)}*'
- en: '*S[small]={(small,cold,no),(small,warm,no)}*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*S[small]={(small,cold,no),(small,warm,no)}*'
- en: '*S[good]={(good,cold,no),(good,warm,yes)}*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*S[good]={(good,cold,no),(good,warm,yes)}*'
- en: 'The information entropy of *S* is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*S* 的信息熵为：'
- en: '*E(S)=![](img/36bd8ff7-5288-4b68-aa7e-e2ae36444bce.png)*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*E(S)=![](img/36bd8ff7-5288-4b68-aa7e-e2ae36444bce.png)*'
- en: 'The information entropy of the partitions is:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 划分后的信息熵为：
- en: '*E(S[none])=-(2/2)*log[2](2/2)=-log[2](1)=0,* since all instances have the
    class `no`.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*E(S[none])=-(2/2)*log[2](2/2)=-log[2](10*，因为所有实例的类别是 `no`。'
- en: '*E(S[small])=0* for a similar reason.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*E(S[small])=0*，原因类似。'
- en: '*E(S[good])=![](img/c6aad04d-c94c-4ac3-8bcb-009ec62772bc.png)*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*E(S[good])=![](img/c6aad04d-c94c-4ac3-8bcb-009ec62772bc.png)*'
- en: 'Therefore, the information gain is:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，信息增益为：
- en: '*IG(S,swimming suit)=E(S)-[(2/6)*E(S[none])+(2/6)*E(S[small])+(2/6)*E(S[good])]*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*IG(S,泳装)=E(S)-[(2/6)*E(S[none])+(2/6)*E(S[small])+(2/6)*E(S[good])]*'
- en: '*=0.65002242164-(1/3)=0.3166890883*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*=0.65002242164-(1/3)=0.3166890883*'
- en: 'If we chose the `water temperature` attribute to partition the set, *S*, what
    would be the information gain, *IG(S,water temperature)*? The water temperature
    partitions the set, *S*, into the following sets:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择 `水温` 属性来划分集合 *S*，那么信息增益 *IG(S,水温)* 会是多少？水温将集合 *S* 划分为以下集合：
- en: '*S[cold]={(none,cold,no),(small,cold,no),(good,cold,no)}*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*S[cold]={(none,cold,no),(small,cold,no),(good,cold,no)}*'
- en: '*S[warm]={(none,warm,no),(small,warm,no),(good,warm,yes)}*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*S[warm]={(none,warm,no),(small,warm,no),(good,warm,yes)}*'
- en: 'Their entropies are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的熵如下：
- en: '*E(S[cold])=0,* since all instances are classified as no.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*E(S[cold])=0*，因为所有实例都被分类为否。'
- en: '*E(S[warm])=-(2/3)*log[2](2/3)-(1/3)*log[2](1/3)~0.91829583405*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*E(S[warm])=-(2/3)*log[2](2/3)-(1/3)*log[2](1/3)~0.91829583405*'
- en: 'Therefore, the information gain from partitioning the set, *S*, using the *water
    temperature* attribute is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用 *水温* 属性划分集合 *S* 得到的信息增益如下：
- en: '*IG(S,water temperature)=E(S)-[(1/2)*E(S[cold])+(1/2)*E(S[warm])]*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*IG(S,水温)=E(S)-[(1/2)*E(S[cold])+(1/2)*E(S[warm])]*'
- en: '*= 0.65002242164-0.5*0.91829583405=0.19087450461*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*= 0.65002242164-0.5*0.91829583405=0.19087450461*'
- en: This is less than *IG(S,swimming suit)*. Therefore, we can gain more information
    about the set, *S*, (the classification of its instances) by partitioning it as
    per the `swimming suit` attribute instead of the `water temperature` attribute.
    This finding will be the basis of the ID3 algorithm when constructing a decision
    tree in the next section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这小于 *IG(S,泳装)*。因此，我们可以通过根据 `泳装` 属性划分集合 *S*（其实例的分类）来获得更多信息，而不是使用 `水温` 属性。这一发现将成为
    ID3 算法在下一节构建决策树的基础。
- en: ID3 algorithm – decision tree construction
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ID3 算法 – 决策树构建
- en: The ID3 algorithm constructs a decision tree from data based on the information
    gain. In the beginning, we start with the set, *S*. The data items in the set, *S*,
    have various properties, according to which we can partition the set, *S*. If
    an attribute, *A*, has the values *{v[1], ..., v[n]}*, then we partition the set,
    *S*, into the sets *S[1]*, ..., *S[n]*, where the set, S[i], is a subset of the
    set, *S*, where the elements have the value, *v[i]*, for the attribute, *A*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ID3 算法基于信息增益从数据中构建决策树。一开始，我们从集合 *S* 开始。集合 *S* 中的数据项有各种属性，根据这些属性，我们可以对集合 *S*
    进行划分。如果某个属性 *A* 有值 *{v[1], ..., v[n]}*，那么我们将集合 *S* 划分为 *S[1]*, ..., *S[n]*，其中集合
    *S[i]* 是集合 *S* 的一个子集，包含属性 *A* 对应值为 *v[i]* 的元素。
- en: If each element in the set, *S*, has the attributes *A[1], ..., A[m]*, then
    we can partition the set, *S*, according to any of the possible attributes. The
    ID3 algorithm partitions the set, *S*, according to the attribute that yields
    the highest information gain. Now suppose that it has the attribute, *A[1]*. Then,
    for the set, *S*, we have the partitions *S[1], ..., S[n]*, where *A[1]* has the
    possible values *{v[1],..., v[n]}*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果集合 *S* 中的每个元素都有属性 *A[1], ..., A[m]*，那么我们可以根据任意可能的属性划分集合 *S*。ID3 算法根据产生最大信息增益的属性来划分集合
    *S*。现在假设它有属性 *A[1]*，那么对于集合 *S*，我们有划分 *S[1], ..., S[n]*，其中 *A[1]* 的可能值为 *{v[1],
    ..., v[n]}*。
- en: Since we have not constructed a tree yet, we first place a root node. For every
    partition of *S*, we place a new branch from the root. Every branch represents
    one value of the selected attributes. A branch has data samples with the same
    value for that attribute. For every new branch, we can define a new node that
    will have data samples from its ancestor branch.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们还没有构建树，因此首先放置一个根节点。对于 *S* 的每一个划分，从根节点放置一条新分支。每一条分支代表选定属性的一个值。一条分支包含该属性具有相同值的数据样本。对于每一条新分支，我们可以定义一个新节点，该节点将包含其祖先分支的数据样本。
- en: Once we have defined a new node, we choose another of the remaining attributes
    with the highest information gain for the data at that node to partition the data
    further at that node, and then define new branches and nodes. This process can
    be repeated until we run out of all the attributes for the nodes, or even earlier,
    until all the data at the node has the same class as our interest. In the case
    of the swim preference example, there are only two possible classes for the swimming
    preference—class `no` and class `yes`. The last node is called a **leaf node**,
    and decides the class of a data item from the data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了一个新的节点，我们就选择剩余属性中信息增益最高的属性，以便进一步划分该节点上的数据，然后定义新的分支和节点。这个过程可以重复进行，直到节点上的所有属性用完，或者更早地，当节点上的所有数据具有相同的分类（我们感兴趣的类）时。在游泳偏好示例的情况下，游泳偏好只有两个可能的类别——`no`类和`yes`类。最后的节点称为**叶节点**，并决定数据项的分类。
- en: Swim preference – decision tree construction by the ID3 algorithm
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游泳偏好 – 通过ID3算法构建的决策树
- en: 'Here, we describe, step by step, how an ID3 algorithm would construct a decision
    tree from the given data samples in the swim preference example. The initial set
    consists of six data samples:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们一步一步地描述ID3算法如何从给定的游泳偏好示例数据中构建决策树。初始集合由六个数据样本组成：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the previous sections, we calculated the information gains for both, and
    the only non- classifying attributes, `swimming suit`, and `water temperature`,
    as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们计算了两个属性的增益，以及唯一的非分类属性`游泳衣`和`水温`的增益，计算如下：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Hence, we would choose the `swimming suit` attribute as it has a higher information
    gain. There is no tree drawn yet, so we start from the root node. As the `swimming
    suit` attribute has three possible values – `{none, small, good}`, we draw three
    possible branches out of it for each. Each branch will have one partition from
    the partitioned set *S: S[none]*, *S[small]*, and *S[good]*. We add nodes to the
    ends of the branches. *S*[*none*] data samples have the same swimming preference
    class = `no`, so we do not need to branch that node with a further attribute and
    partition the set. Thus, the node with the data, *S[none]*, is already a leaf
    node. The same is true for the node with the data, *S[small]*.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们会选择`游泳衣`属性，因为它具有较高的信息增益。此时还没有画出树，所以我们从根节点开始。由于`游泳衣`属性有三个可能的值——`{none, small,
    good}`，我们为每个值绘制三个可能的分支。每个分支将有一个来自划分集*S：S[none]*，*S[small]*，和*S[good]*的划分。我们在分支的末端添加节点。*S*[*none*]数据样本具有相同的游泳偏好类别=`no`，因此我们不需要用进一步的属性来分支该节点并划分集合。因此，包含数据*S[none]*的节点已经是叶节点。对于包含数据*S[small]*的节点也一样。
- en: But the node with the data, *S[good]*, has two possible classes for swimming
    preference. Therefore, we will branch the node further. There is only one non-classifying
    attribute left—`water temperature`. So there is no need to calculate the information
    gain for that attribute with the data, *S[good]*. From the node, *S[good]*, we
    will have two branches, each with a partition from the set, *S[good]*. One branch
    will have the set of the data sample, *S[good,] [cold]={(good,cold,no)};* the
    other branch will have the partition, *S[good,] [warm]={(good,warm,yes)}*. Each
    of these two branches will end with a node. Each node will be a leaf node, because
    each node has data samples of the same value for the classifying swimming preference
    attribute.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，包含数据的节点，*S[good]*，有两种可能的游泳偏好类别。因此，我们会进一步分支该节点。剩下的唯一非分类属性是`水温`。因此，使用数据*S[good]*计算该属性的信息增益就没有必要了。从节点*S[good]*开始，我们会有两个分支，每个分支都来自集合*S[good]*的一个划分。一个分支将包含数据样本*S[good,][cold]={(good,cold,no)}*；另一个分支将包含划分*S[good,][warm]={(good,warm,yes)}*。这两个分支将以一个节点结束。每个节点将是叶节点，因为每个节点的数据样本在分类游泳偏好属性上具有相同的值。
- en: The resulting decision tree has four leaf nodes and is the tree in *Figure 3.1
    – Decision tree for the swim preference example*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 结果决策树有四个叶节点，并且是*图3.1 – 游泳偏好示例的决策树*中的树。
- en: Implementation
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: 'We implement an ID3 algorithm that constructs a decision tree for the data
    given in a CSV file. All sources are in the chapter directory. The most important
    parts of the source code are given here:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了一个ID3算法，它从CSV文件中给定的数据构建决策树。所有的源代码都位于章节目录中。源代码中最重要的部分如下：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Program input**:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**程序输入**：'
- en: 'We input the data from the swim preference example into the program to construct
    a decision tree:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将游泳偏好示例的数据输入程序以构建决策树：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Program output**:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**程序输出**：'
- en: 'We construct a decision tree from the `swim.csv` data file, with the verbosity
    set to `0`. The reader is encouraged to set the verbosity to `2` to see a detailed
    explanation of how exactly the decision tree is constructed:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 `swim.csv` 数据文件构建了一个决策树，冗余度设置为 `0`。建议读者将冗余度设置为 `2`，以便查看决策树构建过程的详细解释：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Classifying with a decision tree
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树进行分类
- en: Once we have constructed a decision tree from the data with the attributes *A[1],
    ..., A[m]* and the classes *{c[1], ..., c[k]}*, we can use this decision tree
    to classify a new data item with the attributes *A[1], ..., A[m]* into one of
    the classes *{c[1], ..., c[k]}*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从具有属性 *A[1], ..., A[m]* 和类别 *{c[1], ..., c[k]}* 的数据中构建了决策树，我们就可以使用该决策树将一个具有属性
    *A[1], ..., A[m]* 的新数据项分类到类别 *{c[1], ..., c[k]}* 中。
- en: 'Given a new data item that we would like to classify, we can think of each
    node, including the root, as a question for the data sample: *What value does
    that data sample have for the selected attribute, A[i]?* Then, based on the answer,
    we select a branch of the decision tree and move on to the next node. Then, another
    question is answered about the data sample, and another, until the data sample
    reaches the leaf node. A leaf node has one of the classes *{c[1], ..., c[k]}* associated
    with it; for example, *c[i]*. Then, the decision tree algorithm would classify
    the data sample into the class, *c[i]*.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个我们希望分类的新数据项，我们可以将每个节点，包括根节点，视为对数据样本的一个问题：*该数据样本在选定属性 A[i] 上的值是什么？* 然后，根据答案，我们选择决策树的一个分支并继续前进到下一个节点。然后，关于数据样本会再问一个问题，继续这样下去，直到数据样本到达叶节点。叶节点与某个类别
    *{c[1], ..., c[k]}* 相关联；例如，*c[i]*。然后，决策树算法将该数据样本分类到该类别 *c[i]* 中。
- en: Classifying a data sample with the swimming preference decision tree
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用游泳偏好决策树对数据样本进行分类
- en: Let's construct a decision tree for the swimming preference example using the
    ID3 algorithm. Now that we have constructed the decision tree, we would like to
    use it to classify a data sample, *(good,cold,?)* into one of the two classes
    in the set *{no,yes}*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 ID3 算法为游泳偏好示例构建决策树。现在我们已经构建了决策树，我们希望使用它将一个数据样本，*(好，冷，？)* 分类到 *{否, 是}*
    这两个类别中的一个。
- en: 'Start with a data sample at the root of the tree. The first attribute that
    branches from the root is `swimming suit`, so we ask for the value of the `swimming
    suit` attribute of the sample *(good, cold,?)*. We learn that the value of the
    attribute is `swimming suit=good`; therefore, move down the rightmost branch with
    that value for its data samples. We arrive at the node with the `water temperature` attribute
    and ask the question: *What is the value of the water temperature attribute for
    the data sample (good, cold,?)?* We learn that for that data sample, we have `water
    temperature=cold`; therefore, we move down the left-hand branch into the leaf
    node. This leaf is associated with the `swimming preference=no` class. Therefore,
    the decision tree would classify the data sample *(good, cold,?)* to be in that
    swimming preference class; in other words, to complete it (by replacing the question
    mark with the exact data) to the data sample *(**good, cold, no)*.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从树的根开始一个数据样本。第一个从根分支出的属性是 `泳衣`，所以我们询问样本的 `泳衣` 属性值 (*好，冷，？*)。我们得知该属性值为 `泳衣=好`；因此，沿着带有该值的数据样本的最右分支向下移动。我们到达了一个带有
    `水温` 属性的节点，并问：*对于该数据样本，水温属性的值是什么（好，冷，？）*？我们得知对于该数据样本，水温为 `冷`；因此，我们向下移动到左侧分支进入叶节点。这个叶节点与
    `游泳偏好=否` 类别相关联。因此，决策树会将数据样本 (*好，冷，？*) 分类为该游泳偏好类别；换句话说，完成它（通过用确切的数据替换问号），变为数据样本
    (***好，冷，否***)。
- en: Therefore, the decision tree says that if you have a good swimming suit, but
    the water temperature is cold, then you would still not want to swim based on
    the data collected in the table.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，决策树表示，如果你有一件好的泳衣，但水温很冷，那么根据表中收集的数据，你仍然不想游泳。
- en: Playing chess – analysis with a decision tree
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下棋——使用决策树分析
- en: 'Let''s take an example from Chapter 2, *Naive Bayes*, again:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再来一个来自第2章的例子，*朴素贝叶斯*：
- en: '| **Temperature** | **Wind** | **Sunshine** | **Play** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **温度** | **风力** | **阳光** | **游玩** |'
- en: '| Cold | Strong | Cloudy | No |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 强风 | 多云 | 否 |'
- en: '| Cold | Strong | Cloudy | No |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 强风 | 多云 | 否 |'
- en: '| Warm | None | Sunny | Yes |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无风 | 晴天 | 是 |'
- en: '| Hot | None | Sunny | No |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 炎热 | 无风 | 晴天 | 否 |'
- en: '| Hot | Breeze | Cloudy | Yes |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 炎热 | 微风 | 多云 | 是 |'
- en: '| Warm | Breeze | Sunny | Yes |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 微风 | 晴天 | 是 |'
- en: '| Cold | Breeze | Cloudy | No |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 微风 | 多云 | 否 |'
- en: '| Cold | None | Sunny | Yes |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 无风 | 晴天 | 是 |'
- en: '| Hot | Strong | Cloudy | Yes |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 热 | 强风 | 阴天 | 是 |'
- en: '| Warm | None | Cloudy | Yes |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无风 | 阴天 | 是 |'
- en: '| Warm | Strong | Sunny | ? |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 强风 | 晴天 | ? |'
- en: We would like to find out whether our friend would like to play chess with us
    in the park. But this time, we would like to use decision trees to find the answer.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想知道我们的朋友是否愿意在公园里和我们下棋。这一次，我们希望使用决策树来找到答案。
- en: Analysis
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析
- en: 'We have the initial set, *S*, of the data samples, as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有初始的数据集*S*，如下所示：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'First, we determine the information gain for each of the three non-classifying
    attributes: `temperature`, `wind`, and `sunshine`. The possible values for `temperature`
    are `Cold`, `Warm`, and `Hot`. Therefore, we will partition the set, *S*, into
    three sets:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们确定三个非分类属性的*信息增益*：`temperature`（温度）、`wind`（风速）、`sunshine`（阳光）。`temperature`的可能值为`Cold`（冷）、`Warm`（温暖）和`Hot`（热）。因此，我们将集合*S*划分为三个子集：
- en: '[PRE7]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We calculate the information entropies for the sets first:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算这些集合的信息熵：
- en: '![](img/b6d23d69-5c62-455a-808f-83f5f3a7ba2b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6d23d69-5c62-455a-808f-83f5f3a7ba2b.png)'
- en: '![](img/9d5fdd96-20e6-42be-a66f-168df9b6fba5.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d5fdd96-20e6-42be-a66f-168df9b6fba5.png)'
- en: '![](img/73ac61fc-b892-4050-9542-6cae5c95cf0d.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73ac61fc-b892-4050-9542-6cae5c95cf0d.png)'
- en: '![](img/55f6d338-236f-4860-bcfc-a6afa7c374e3.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55f6d338-236f-4860-bcfc-a6afa7c374e3.png)'
- en: '![](img/04be8669-816b-4abf-b8a9-84f08fa5d441.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04be8669-816b-4abf-b8a9-84f08fa5d441.png)'
- en: '![](img/1bc93be0-e80e-4478-a7eb-7770febac121.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1bc93be0-e80e-4478-a7eb-7770febac121.png)'
- en: '![](img/5cc2ba99-5f78-406c-9384-04a1f5897775.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cc2ba99-5f78-406c-9384-04a1f5897775.png)'
- en: 'The possible values for the `wind` attribute are `None`, `Breeze`, and `Strong`.
    Thus, we will split the set, *S*, into the three partitions:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`wind`（风速）属性的可能值为`None`（无风）、`Breeze`（微风）和`Strong`（强风）。因此，我们将集合*S*划分为三个子集：'
- en: '![](img/913e09cf-1f4a-4838-a44d-7e386857de33.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/913e09cf-1f4a-4838-a44d-7e386857de33.png)'
- en: '![](img/21c1f8f6-ef95-446f-8f5b-0ef3181db4af.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21c1f8f6-ef95-446f-8f5b-0ef3181db4af.png)'
- en: '![](img/050cedb3-7fc0-4d70-aced-ccd53cc97b6a.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/050cedb3-7fc0-4d70-aced-ccd53cc97b6a.png)'
- en: 'The information entropies of the sets are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这些集合的信息熵如下：
- en: '![](img/29b98f94-6d23-4d88-be49-85cb9528fa23.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29b98f94-6d23-4d88-be49-85cb9528fa23.png)'
- en: '![](img/25ec3d85-19a3-4732-9cbe-b70e8bc0361b.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25ec3d85-19a3-4732-9cbe-b70e8bc0361b.png)'
- en: '![](img/03d4f9db-4723-4d5f-b1c6-603be3bcef37.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03d4f9db-4723-4d5f-b1c6-603be3bcef37.png)'
- en: '![](img/66ec446d-ffd5-4028-b7ed-485fabe7551b.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66ec446d-ffd5-4028-b7ed-485fabe7551b.png)'
- en: '![](img/7f61d2d0-fb0c-4ac0-a6bf-c9dd108c8bf0.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f61d2d0-fb0c-4ac0-a6bf-c9dd108c8bf0.png)'
- en: '![](img/c30c697d-c7f2-4338-86c5-74c992d534b1.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c30c697d-c7f2-4338-86c5-74c992d534b1.png)'
- en: 'Finally, the third attribute, `Sunshine`, has two possible values, `Cloudy`
    and `Sunny`. Hence, it splits the set, *S*, into two sets:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第三个属性`Sunshine`（阳光）有两个可能值：`Cloudy`（阴天）和`Sunny`（晴天）。因此，它将集合*S*划分为两个子集：
- en: '![](img/db8b17d8-8329-4731-9ca0-520393b753d8.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db8b17d8-8329-4731-9ca0-520393b753d8.png)'
- en: '![](img/9f9c929a-dd35-49a6-8e98-3646356ffa2b.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f9c929a-dd35-49a6-8e98-3646356ffa2b.png)'
- en: 'The entropies of the sets are as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些集合的信息熵如下：
- en: '![](img/74517c8e-9378-421b-841a-ba76bc3fbd31.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74517c8e-9378-421b-841a-ba76bc3fbd31.png)'
- en: '![](img/0013e3ef-8416-470d-a331-41774c28103f.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0013e3ef-8416-470d-a331-41774c28103f.png)'
- en: '![](img/b022dcbf-097b-4106-b005-385de8861495.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b022dcbf-097b-4106-b005-385de8861495.png)'
- en: '![](img/b5b3caaa-c2e7-4042-849b-82707ebfb928.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5b3caaa-c2e7-4042-849b-82707ebfb928.png)'
- en: '*IG(S,wind)* and *IG(S,temperature)* are greater than *IG(S,sunshine)*. Both
    of them are equal; therefore, we can choose any of the attributes to form the
    three branches; for example, the first one, `Temperatur``e`. In that case, each
    of the three branches would have the data samples *S[cold]*, *S[warm]*, and *S[hot]*.
    At those branches, we could apply the algorithm further to form the rest of the
    decision tree. Instead, we will use the program to complete it:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*IG(S,wind)*和*IG(S,temperature)*大于*IG(S,sunshine)*。这两个值相等，因此我们可以选择任意一个属性来形成三个分支；例如，选择第一个属性`Temperature`（温度）。在这种情况下，每个分支会包含数据样本*S[cold]*、*S[warm]*和*S[hot]*。在这些分支上，我们可以进一步应用算法来形成剩余的决策树。相反，我们可以使用程序来完成这个过程：'
- en: '**Input**:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Output**:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Classification
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: Now that we have constructed the decision tree, we would like to use it to classify
    a data sample *(warm,strong,sunny,?)* into one of the two classes in the set *{no,yes}*.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了决策树，我们想用它来将一个数据样本*(warm,strong,sunny,?)*分类到集合*{no,yes}*中的一个类别。
- en: We start at the root. What value does the `temperature` attribute have in that
    instance? `Warm`, so we go to the middle branch. What value does the `wind` attribute have
    in that instance? `Strong`, so the instance would fall into the class `No` since
    we have already arrived at the leaf node.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从根节点开始。该实例的`temperature`属性值是什么？是`温暖`，因此我们进入中间分支。该实例的`wind`属性值是什么？是`强`，因此该实例将归类为`否`，因为我们已经到达了叶节点。
- en: So, our friend would not want to play chess with us in the park, according to
    the decision tree classification algorithm. Please note that the Naive Bayes algorithm
    stated otherwise. An understanding of the problem is required to choose the best
    possible method. At other times, a method with greater accuracy is one that takes
    into consideration the results of several algorithms or several classifiers, as
    in the case of the random forest algorithm in [Chapter 4](0a6e5d42-ab32-49c4-934f-7f1954eb1a25.xhtml), *Random
    Forests*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据决策树分类算法，我们的朋友不会在公园与我们下棋。请注意，朴素贝叶斯算法得出的结论恰好相反。要选择最佳的方法，需要对问题有一定的理解。在其他时候，准确性更高的方法是考虑多个算法或多个分类器结果的方法，正如在[第4章](0a6e5d42-ab32-49c4-934f-7f1954eb1a25.xhtml)中所示的**随机森林**算法。
- en: Going shopping – dealing with data inconsistencies
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去购物 – 处理数据不一致性
- en: 'We have the following data about the shopping preferences of our friend, Jane:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下关于我们朋友简的购物偏好的数据：
- en: '| **Temperature** | **Rain** | **Shopping** |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| **温度** | **雨** | **购物** |'
- en: '| Cold | None | Yes |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 无 | 是 |'
- en: '| Warm | None | No |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无 | 否 |'
- en: '| Cold | Strong | Yes |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 强 | 是 |'
- en: '| Cold | None | No |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 无 | 否 |'
- en: '| Warm | Strong | No |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 强 | 否 |'
- en: '| Warm | None | Yes |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无 | 是 |'
- en: '| Cold | None | ? |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 无 | ? |'
- en: We would like to find out, using a decision tree, whether Jane would go shopping
    if the outside temperature was cold with no rain.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过决策树来找出，如果外部温度寒冷且没有雨，简是否会去购物。
- en: Analysis
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析
- en: 'Here, we should be careful, as there are instances of the data that have the
    same values for the same attributes, but different classes; that is, `(cold,none,yes)`
    and `(cold,none,no)`. The program we made would form the following decision tree:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要小心，因为某些数据实例在相同属性下有相同的值，但类别不同；即`(cold,none,yes)`和`(cold,none,no)`。我们编写的程序会形成以下决策树：
- en: '[PRE10]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: But at the leaf node `[Rain=None]` with the parent `[Temperature=Cold]`, there
    are two data samples with both classes, `no` and `yes`. We cannot, therefore,
    classify an instance `(cold,none,?)` accurately. For the decision tree algorithm
    to work better, we would have to provide a class at the leaf node with the greatest
    weight—that is, the majority class. An even better approach would be to collect
    values for more attributes for the data samples so that we can make a decision
    more accurately.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 但在叶节点`[Rain=None]`且父节点为`[Temperature=Cold]`时，有两个数据样本分别属于`否`和`是`两个类别。因此，我们无法准确地分类一个实例`(cold,none,?)`。为了使决策树算法更有效，我们需要在叶节点提供权重最大、即占多数的类别。一个更好的方法是为数据样本收集更多属性值，这样我们就可以更准确地做出决策。
- en: Therefore, given the available data, we are uncertain whether Jane would go
    shopping or not.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，鉴于现有数据，我们无法确定简是否会去购物。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we looked at how a decision tree ID3 algorithm first constructs
    a decision tree from the input data and then classifies a new data instance using
    the constructed tree. The decision tree was constructed by selecting the attribute
    for branching with the highest information gain. We studied how information gain
    measures the amount of information that can be learned in terms of the gain in
    information entropy.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们了解了决策树ID3算法如何从输入数据中首先构建决策树，然后使用构建好的树对新数据实例进行分类。决策树是通过选择信息增益最高的属性来进行分支构建的。我们研究了信息增益如何衡量在信息熵增益方面能够学到的知识量。
- en: We also learned that the decision tree algorithm can achieve a different result
    from other algorithms, such as Naive Bayes.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还了解到，决策树算法可能与其他算法，如朴素贝叶斯算法，产生不同的结果。
- en: In the next chapter, we will learn how to combine various algorithms or classifiers
    into a decision forest (called **random forest**) in order to achieve a more accurate
    result.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何将多种算法或分类器组合成一个决策森林（称为**随机森林**），以获得更准确的结果。
- en: Problems
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: '**Problem 1**: What is the information entropy of the following multisets?'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1**：以下多重集合的信息熵是多少？'
- en: a) {1,2}, b) {1,2,3}, c) {1,2,3,4}, d) {1,1,2,2}, e) {1,1,2,3}
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: a) {1,2}，b) {1,2,3}，c) {1,2,3,4}，d) {1,1,2,2}，e) {1,1,2,3}
- en: '**Problem 2**: What is the information entropy of the probability space induced
    by the biased coin that shows head with a probability of 10%, and tail with a
    probability of 90%?'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2**：偏置硬币的概率空间的信息熵是多少？该硬币显示正面概率为10%，反面为90%？'
- en: '**Problem 3**: Let''s take another example of playing chess from [Chapter 2](4f54d05d-6791-47c5-a260-c2fd563a55cf.xhtml),
    *Naive Bayes*:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3**：让我们来看一下[第2章](4f54d05d-6791-47c5-a260-c2fd563a55cf.xhtml)中的另一个例子，*朴素贝叶斯*，关于下棋的例子：'
- en: a) What is the information gain for each of the non-classifying attributes in
    the table?
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: a) 表中每个非分类属性的信息增益是多少？
- en: b) What is the decision tree constructed from the given table?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: b) 根据给定的表格，构建出的决策树是什么？
- en: c) How would you classify a data sample `(Warm,Strong,Spring,?)` according to
    the constructed decision tree?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: c) 如何根据构建的决策树对数据样本`(温暖, 强风, 春季, ?)`进行分类？
- en: '| **Temperature** | **Wind** | **Season** | **Play** |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **温度** | **风速** | **季节** | **感受** |'
- en: '| Cold | Strong | Winter | No |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 寒冷 | 强风 | 冬季 | 否 |'
- en: '| Warm | Strong | Autumn | No |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 强风 | 秋季 | 否 |'
- en: '| Warm | None | Summer | Yes |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无 | 夏季 | 是 |'
- en: '| Hot | None | Spring | No |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 热 | 无 | 春季 | 否 |'
- en: '| Hot | Breeze | Autumn | Yes |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 热 | 微风 | 秋季 | 是 |'
- en: '| Warm | Breeze | Spring | Yes |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 微风 | 春季 | 是 |'
- en: '| Cold | Breeze | Winter | No |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 寒冷 | 微风 | 冬季 | 否 |'
- en: '| Cold | None | Spring | Yes |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 寒冷 | 无 | 春季 | 是 |'
- en: '| Hot | Strong | Summer | Yes |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 热 | 强风 | 夏季 | 是 |'
- en: '| Warm | None | Autumn | Yes |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无 | 秋季 | 是 |'
- en: '| Warm | Strong | Spring | ? |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 强风 | 春季 | ? |'
- en: '**Problem 4**: **Mary and temperature preferences**: Let''s take the example
    from [Chapter 1](e0824a1e-65dc-4fee-a0e5-56170fdb36b9.xhtml), *Classification
    Using K Nearest Neighbors*, regarding Mary''s temperature preferences:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 4**：**玛丽与温度偏好**：让我们来看一下[第1章](e0824a1e-65dc-4fee-a0e5-56170fdb36b9.xhtml)中的例子，*使用K近邻进行分类*，关于玛丽的温度偏好：'
- en: '| **Temperature in °C** | **Wind speed in kmph** | **Mary''s perception** |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| **温度（°C）** | **风速（km/h）** | **玛丽的感知** |'
- en: '| 10 | 0 | Cold |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0 | 寒冷 |'
- en: '| 25 | 0 | Warm |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 0 | 温暖 |'
- en: '| 15 | 5 | Cold |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 5 | 寒冷 |'
- en: '| 20 | 3 | Warm |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 3 | 温暖 |'
- en: '| 18 | 7 | Cold |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 18 | 7 | 寒冷 |'
- en: '| 20 | 10 | Cold |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 10 | 寒冷 |'
- en: '| 22 | 5 | Warm |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 5 | 温暖 |'
- en: '| 24 | 6 | Warm |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 6 | 温暖 |'
- en: We would like to use decision trees to decide whether our friend, Mary, would
    feel warm or cold in a room with a temperature of 16°C and a wind speed of 3 km/h.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想使用决策树来判断我们的朋友玛丽在温度为16°C，风速为3 km/h的房间中会感觉温暖还是寒冷。
- en: Can you please explain how a decision tree algorithm could be used here and
    how beneficial it would be to use it for this example?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你能解释一下如何在这里使用决策树算法吗？并说明使用它对于这个例子的益处？
- en: Analysis
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析
- en: '**Problem 1**: Here are entropies of the multisets:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1**：以下是多重集的熵值：'
- en: a) ![](img/7f1d570b-442a-4461-bcf2-a3969e95bac8.png)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: a) ![](img/7f1d570b-442a-4461-bcf2-a3969e95bac8.png)
- en: b) ![](img/cdb58fb4-3f36-4ea9-84ae-99a1d32285d3.png)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: b) ![](img/cdb58fb4-3f36-4ea9-84ae-99a1d32285d3.png)
- en: c) ![](img/7912ec03-379f-4fc1-9d4d-d9e98db19757.png)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: c) ![](img/7912ec03-379f-4fc1-9d4d-d9e98db19757.png)
- en: d) ![](img/20279a9c-d349-4c0b-bf66-d277eb2ff08c.png)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: d) ![](img/20279a9c-d349-4c0b-bf66-d277eb2ff08c.png)
- en: e) ![](img/4b287548-1003-4188-aced-deabb5645e5b.png)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: e) ![](img/4b287548-1003-4188-aced-deabb5645e5b.png)
- en: Note here that the information entropy of the multisets that have more than
    two classes is greater than 1, so we need more than one bit of information to
    represent the result. But is this true for every multiset that has more than two
    classes of elements?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，具有多个类别的多重集的信息熵大于1，因此我们需要多个比特来表示结果。但对于每个具有多个类别元素的多重集，这种情况都成立吗？
- en: '**Problem 2**: *![](img/bcd665ff-2e2f-444c-9b5f-bc9e17a3eae3.png)*'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2**：*！[](img/bcd665ff-2e2f-444c-9b5f-bc9e17a3eae3.png)*'
- en: '**Problem 3**: a) The information gains for the three attributes are as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3**：a) 三个属性的信息增益如下：'
- en: '![](img/7a94df6b-1916-46b4-a830-dc6ce3356092.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a94df6b-1916-46b4-a830-dc6ce3356092.png)'
- en: '![](img/43db9692-58e3-423a-9431-040c14eee8ac.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43db9692-58e3-423a-9431-040c14eee8ac.png)'
- en: '![](img/a1267b6c-2d2e-450c-a578-38aec73de774.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1267b6c-2d2e-450c-a578-38aec73de774.png)'
- en: 'b) Therefore, we would choose the `season` attribute to branch from the root
    node as it has the highest information gain. Alternatively, we can put all the
    input data into the program to construct a decision tree, as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: b) 因此，我们会选择`季节`属性作为从根节点分支的依据，因为它具有最高的信息增益。或者，我们可以将所有输入数据放入程序中，构建决策树，如下所示：
- en: '[PRE11]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: c) According to the decision tree constructed, we would classify the data sample
    `(warm,strong,spring,?)` to the class `Play=Yes` by going to the bottom-most branch
    from the root node and then arriving at the leaf node by taking the middle branch.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: c) 根据构建的决策树，我们将数据样本`(warm,strong,spring,?)`分类为`Play=Yes`，方法是从根节点向下走到最底层分支，然后通过走中间分支到达叶子节点。
- en: '**Problem 4**: Here, the decision tree algorithm may not perform that well
    without any processing of the data. If we considered every class of a temperature,
    then 25°C would still not occur in the decision tree as it is not in the input
    data, so we would not be able to classify how Mary would feel at 16°C and a wind
    speed of 3 km/h.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 4**：在这里，决策树算法在没有对数据进行任何处理的情况下可能表现不佳。如果我们考虑每一种温度类别，那么25°C仍然不会出现在决策树中，因为它不在输入数据中，因此我们无法分类玛丽在16°C和3
    km/h的风速下的感受。'
- en: We could alternatively divide the temperature and wind speed into intervals
    in order to reduce the classes so that the resulting decision tree could classify
    the input instance. But it is this division, the intervals into which 25°C and
    3 km/h should be classified, that is the fundamental part of the analysis procedure
    for this type of problem. Thus, decision trees without any serious modification
    could not analyze the problem well.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择将温度和风速划分为区间，以减少类别数，从而使得最终的决策树能够对输入实例进行分类。但正是这种划分，即25°C和3 km/h应该归类到哪些区间，才是分析这种问题的基本过程。因此，未经任何重大修改的决策树无法很好地分析这个问题。
