- en: '*Chapter 8*: Creating an End-to-End Machine Learning Workflow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：创建一个端到端的机器学习工作流'
- en: In previous chapters, we learned about Pachyderm basics and how to install Pachyderm
    locally and on a cloud platform. We've deployed our first pipeline, learned how
    to update a pipeline, and performed some fundamental Pachyderm operations, such
    as splitting. I hope by now you are convinced that Pachyderm is an extremely versatile
    tool that gives you a lot of flexibility and power in managing your machine learning
    pipelines. To make it even more obvious, we will deploy a much more complex example
    than the ones that we have deployed so far. We hope this chapter will be especially
    fun for you to work on and will expand your understanding of data infrastructure
    quirks even more.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们学习了 Pachyderm 基础知识，以及如何在本地和云平台上安装 Pachyderm。我们已经部署了第一个管道，学习了如何更新管道，并执行了一些基本的
    Pachyderm 操作，如拆分。我希望到现在为止，你已经相信 Pachyderm 是一个功能极其强大的工具，能够提供很多灵活性和处理机器学习管道的能力。为了让这一点更加明显，我们将部署一个比之前更复杂的示例，远超之前的任何部署。希望这一章能特别有趣，能让你更加深入理解数据基础设施的特性。
- en: In this chapter, we will deploy a multistep **Natural Language Processing**
    (**NLP**) workflow that will demonstrate how to use Pachyderm at scale.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将部署一个多步骤的 **自然语言处理** (**NLP**) 工作流，演示如何大规模使用 Pachyderm。
- en: 'This chapter includes the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括以下主题：
- en: NLP example overview
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP 示例概述
- en: Creating repositories and pipelines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建仓库和管道
- en: Creating an NER pipeline
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个命名实体识别 (NER) 管道
- en: Retraining an NER model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新训练一个命名实体识别 (NER) 模型
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires you to have the following components installed and configured.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求您安装并配置以下组件。
- en: 'For a local macOS installation, you need the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本地 macOS 安装，您需要以下组件：
- en: macOS Mojave, Catalina, Big Sur, or later
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: macOS Mojave, Catalina, Big Sur 或更高版本
- en: Docker Desktop for Mac 10.14
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Desktop for Mac 10.14
- en: '`minikube` v1.9.0 or later'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minikube` v1.9.0 或更高版本'
- en: '`pachctl` 2.0.0 or later'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pachctl` 2.0.0 或更高版本'
- en: Pachyderm 2.0.0 or later
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pachyderm 2.0.0 或更高版本
- en: 'For a local Windows installation, you need the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本地 Windows 安装，您需要以下组件：
- en: Windows Pro 64-bit v10 or later
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Windows Pro 64位 v10 或更高版本
- en: '**Windows Subsystem for Linux** (**WSL**) 2 or later'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Windows Subsystem for Linux** (**WSL**) 2 或更高版本'
- en: Microsoft PowerShell v6.2.1 or later
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft PowerShell v6.2.1 或更高版本
- en: Hyper-V
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyper-V
- en: '`minikube` v1.9.0 or later'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minikube` v1.9.0 或更高版本'
- en: '`pachctl` 2.0.0 or later'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pachctl` 2.0.0 或更高版本'
- en: Pachyderm 2.0.0 or later
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pachyderm 2.0.0 或更高版本
- en: 'For an **Amazon Elastic Kubernetes Service** (**Amazon EKS**) installation,
    you need the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **Amazon 弹性 Kubernetes 服务** (**Amazon EKS**) 安装，您需要以下组件：
- en: '`kubectl` v.18 or later'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl` v.18 或更高版本'
- en: '`eksctl`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eksctl`'
- en: '`aws-iam-authenticator`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aws-iam-authenticator`'
- en: '`pachctl` 2.0.0 or later'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pachctl` 2.0.0 或更高版本'
- en: Pachyderm 2.0.0 or later
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pachyderm 2.0.0 或更高版本
- en: 'For a Microsoft Azure cloud installation, you need the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Microsoft Azure 云安装，您需要以下组件：
- en: '`kubectl` v.18 or later'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl` v.18 或更高版本'
- en: Azure CLI
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure CLI
- en: '`pachctl` 2.0.0 or later'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pachctl` 2.0.0 或更高版本'
- en: Pachyderm 2.0.0 or later
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pachyderm 2.0.0 或更高版本
- en: '`jq` 1.5 or later'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jq` 1.5 或更高版本'
- en: 'For a **Google Kubernetes Engine** (**GKE**) cloud installation, you need the
    following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **Google Kubernetes Engine** (**GKE**) 云安装，您需要以下组件：
- en: Google Cloud SDK v124.0.0 or later
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud SDK v124.0.0 或更高版本
- en: '`kubectl` v.18 or later'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl` v.18 或更高版本'
- en: '`pachctl` 2.0.0 or later'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pachctl` 2.0.0 或更高版本'
- en: Pachyderm 2.0.0 or later
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pachyderm 2.0.0 或更高版本
- en: 'The minimum virtual hardware requirements for a cloud or local virtual machine
    are as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 云端或本地虚拟机的最低硬件要求如下：
- en: '**Number of CPUs**: 4'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU 数量**：4'
- en: '**Memory**: 8,192 MB'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存**：8,192 MB'
- en: '**Disk**: 20 GB'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**磁盘**：20 GB'
- en: Now that we know the technical requirements needed to accomplish the tasks in
    this chapter, we can deploy a Pachyderm instance with sufficient resources to
    run the example described in this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了完成本章任务所需的技术要求，我们可以部署一个具有足够资源的 Pachyderm 实例来运行本章描述的示例。
- en: Adjusting virtual machine parameters
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整虚拟机参数
- en: To run the example described in this section, you must make sure that the virtual
    machine that runs Pachyderm has enough memory and CPU to accommodate the pipeline
    requirements. This applies to both cloud and local environments.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本节描述的示例，您必须确保运行 Pachyderm 的虚拟机有足够的内存和 CPU，以满足管道的需求。无论是云环境还是本地环境，均适用此要求。
- en: If you are running Pachyderm on a cloud platform, make sure that you have deployed
    Kubernetes on a virtual machine flavor that adheres to the minimum hardware requirements
    listed in the *Technical requirements* section for this chapter. Then, redeploy
    your Pachyderm cluster as described in [*Chapter 5*](B17085_05_Final_SB_Epub.xhtml#_idTextAnchor123),
    *Installing Pachyderm on a Cloud Platform*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在云平台上运行Pachyderm，请确保已在符合本章*技术要求*部分中列出的最低硬件要求的虚拟机规格上部署了Kubernetes。然后，按照[*第5章*](B17085_05_Final_SB_Epub.xhtml#_idTextAnchor123)《在云平台上安装Pachyderm》的描述重新部署您的Pachyderm集群。
- en: If you are running Pachyderm in `minikube` on your local computer, make sure
    that the `minikube` virtual machine is large enough. If you have a `minikube`
    machine deployed as described in [*Chapter 4*](B17085_04_Final_SB_Epub.xhtml#_idTextAnchor096),
    *Installing Pachyderm Locally*, you need to delete it and deploy a new `minikube`
    virtual machine with a larger CPU and memory.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在本地计算机上通过`minikube`运行Pachyderm，请确保`minikube`虚拟机足够大。如果您按照[*第4章*](B17085_04_Final_SB_Epub.xhtml#_idTextAnchor096)《本地安装Pachyderm》的描述部署了`minikube`虚拟机，您需要删除它，并部署一个具有更大CPU和内存的新`minikube`虚拟机。
- en: 'To do so, complete the following steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，请完成以下步骤：
- en: 'Uninstall the old Pachyderm cluster:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卸载旧的Pachyderm集群：
- en: '[PRE0]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The system response is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 系统响应如下：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Delete the existing `minikube` virtual machine:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除现有的`minikube`虚拟机：
- en: '[PRE2]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see the following system response:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下系统响应：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After your old machine is deleted, start a new virtual machine with the following
    parameters:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除旧机器后，启动一个新的虚拟机，并使用以下参数：
- en: '[PRE4]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This command returns the following response:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令返回以下响应：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, redeploy your Pachyderm cluster as described in [*Chapter 4*](B17085_04_Final_SB_Epub.xhtml#_idTextAnchor096),
    *Installing Pachyderm Locally*. For simplicity, here are the commands that you
    need to run:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，按照[*第4章*](B17085_04_Final_SB_Epub.xhtml#_idTextAnchor096)《本地安装Pachyderm》的描述，重新部署您的Pachyderm集群。为了简单起见，以下是您需要运行的命令：
- en: '[PRE6]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should see the following system response:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下系统响应：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the following commands to connect to `pachd`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令连接到`pachd`：
- en: '[PRE8]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have Pachyderm deployed with enough resources to run the example
    in this chapter, let's review the NLP pipeline that we will create.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经部署了足够资源的Pachyderm来运行本章中的示例，让我们回顾一下我们将要创建的NLP管道。
- en: NLP example overview
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP示例概述
- en: In this section, we will review the end-to-end machine learning workflow that
    will help us understand how to schedule it in Pachyderm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾端到端的机器学习工作流，帮助我们理解如何在Pachyderm中调度它。
- en: To demonstrate this functionality, we will create an NLP pipeline that will
    perform various text optimizations against the *The Legend of Sleepy Hollow* short
    story book written by *Washington Irving*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示此功能，我们将创建一个NLP管道，对*《沉睡谷的传说》*这本由*华盛顿·欧文*所写的短篇小说书进行各种文本优化。
- en: But first, let's review what NLP is and the typical stages that are involved
    in NLP.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们回顾一下什么是NLP以及NLP涉及的典型阶段。
- en: Introduction to NLP
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理简介
- en: NLP is a machine learning technique that enables you to analyze natural text,
    namely speech or written text. This branch of artificial intelligence has existed
    for many years, but with the advancement of computer and internet technologies,
    it has found new implementations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是一种机器学习技术，使您能够分析自然文本，即语音或书面文本。这一人工智能分支已经存在多年，但随着计算机和互联网技术的进步，它找到了新的应用方式。
- en: 'So, how can you use NLP in your business or academic research? There are many
    ways, but the most common ones include the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何在您的商业或学术研究中使用自然语言处理（NLP）呢？有很多方法，但最常见的包括以下几种：
- en: '**Speech recognition**: Technology that enables computers to understand human
    voice and speech.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音识别**：使计算机能够理解人类语音的技术。'
- en: '**Chatbots**: Software that can answer questions and learn from provided answers.
    Older chatbots were based on rules defined by standard software engineering techniques,
    meaning that they could not evolve and could only produce mediocre responses.
    Newer bots are much more advanced.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天机器人**：能够回答问题并从提供的答案中学习的软件。旧版聊天机器人基于标准软件工程技术定义的规则，因此它们无法进化，只能产生平庸的回答。新型聊天机器人则更加先进。'
- en: '**Machine translation**: Technology that automates the task of translating
    text and speech from one language to another. The most common example is, of course,
    Google Translate.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：自动化将文本和语音从一种语言翻译成另一种语言的技术。最常见的例子当然是谷歌翻译。'
- en: '**Text extraction, summarization, and classification**: A very needed technique
    in our world of information overload. NLP enables you to create pipelines that
    provide insights about a text, such as a summary of a research paper or information
    about the keywords used on a page.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本提取、总结和分类**：在信息过载的世界中，这是一项非常需要的技术。NLP使你能够创建提供文本洞察的管道，比如研究论文的摘要或页面上使用的关键词信息。'
- en: '**Sentiment analysis**: A famous technique that helps classify information
    by its positive or negative tone. The most famous implementation of this technology
    is the Gmail email classifier that sorts your email into three categories: **Primary**,
    **Promotions**, and **Social** emails.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：一种帮助根据信息的正面或负面情感来分类的著名技术。这项技术最著名的应用是Gmail邮箱分类器，它将你的电子邮件分为三类：**主邮件**、**促销邮件**和**社交邮件**。'
- en: These are the main examples of NLP. However, this list is not complete. NLP
    is also used in bioinformatics to analyze genetic data, in finance for understanding
    market events and trends, in healthcare to understand patient information, and
    in many other areas.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是NLP的主要示例。然而，这个列表并不完整。NLP还被应用于生物信息学中分析基因数据，在金融领域用于理解市场事件和趋势，在医疗保健中用于理解患者信息，以及在许多其他领域。
- en: Now that we know the areas in which NLP is applicable, let's review the main
    phases that are involved in building an NLP pipeline.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了NLP适用的领域，让我们回顾一下构建NLP管道所涉及的主要阶段。
- en: Learning the NLP phases
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习NLP阶段
- en: 'As we discussed in the previous section, NLP is used to solve a variety of
    text- and speech-related tasks. Like with other areas of machine learning, when
    you need to solve an NLP problem, you need to build a pipeline. There are a few
    definitions of an NLP pipeline, but typically, phases of an NLP pipeline include
    the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节讨论的那样，NLP用于解决各种与文本和语音相关的任务。像机器学习的其他领域一样，当你需要解决一个NLP问题时，你需要构建一个管道。NLP管道有几种定义，但通常情况下，NLP管道的阶段包括以下内容：
- en: '**Text preprocessing or** **cleaning**: This stage includes operations such
    as word and sentence segmentation, tokenization, removal of stop words and punctuation,
    converting words to lowercase, and lemmatization or stemming.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本预处理或** **清理**：这一阶段包括诸如单词和句子分割、标记化、去除停用词和标点符号、将单词转换为小写字母、词形还原或词干提取等操作。'
- en: '**Structure analysis**: This stage goes deeper into analyzing what the text
    is about. It includes operations such as **Part-of-Speech** (**POS**) tagging,
    dependency parsing, and chunking.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构分析**：这一阶段深入分析文本的主题。它包括如**词性**（**POS**）标注、依赖解析和词块切分等操作。'
- en: '**Feature extraction**: This stage deals with answering specific questions
    about your data and finding relationships between your text entities. It might
    include tasks such as **Named Entity Recognition** (**NER**) and **Named Entity
    Disambiguation** (**NED**) or **linking** and sentiment analysis.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：这一阶段主要是回答关于数据的特定问题，并找出文本实体之间的关系。它可能包括如**命名实体识别**（**NER**）和**命名实体消歧**（**NED**）或**链接**和情感分析等任务。'
- en: '**Modeling**: This stage is where you train your model on training data and
    test it to further put it into a production environment.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建模**：这一阶段是你在训练数据上训练模型并进行测试，以便将其进一步投入生产环境。'
- en: Depending on what your use case is, your pipeline might include all or some
    of these stages and they might also be in a different order.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的使用案例，管道可能包含所有或部分这些阶段，而且这些阶段的顺序也可能不同。
- en: 'The following diagram demonstrates an example NLP pipeline:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个示例的自然语言处理管道：
- en: '![Figure 8.1 – NLP pipeline'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.1 – 自然语言处理管道'
- en: '](img/B17085_08_001.png)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17085_08_001.png)'
- en: Figure 8.1 – NLP pipeline
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 自然语言处理管道
- en: Now that we know the phases of an NLP pipeline, let's look more closely at the
    example that we will implement.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了NLP管道的阶段，让我们更仔细地看看我们将要实现的示例。
- en: Reviewing the NLP example
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾NLP示例
- en: 'In our example, we will be using the text of *The Legend of Sleepy Hollow*
    by *Washington Irving* and in the end, we will create and train an NER pipeline
    that will help us to answer the question of who the main characters in the book
    are. To create this multistep workflow, we will need to create the following pipelines:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将使用*华盛顿·欧文*的《睡谷传奇》文本，最终我们将创建并训练一个NER管道，帮助我们回答书中主要人物是谁的问题。为了创建这个多步骤的工作流，我们需要创建以下管道：
- en: '**Data cleaning pipeline**: This pipeline will download the text from a provided
    URL and clean it of any HTML tags, headings, and other irrelevant content. Then,
    it tokenizes the text, removes stop words and punctuation, and then does stemming
    and lemmatization.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清洗管道**：该管道将从提供的 URL 下载文本，并清除所有 HTML 标签、标题及其他无关内容。然后，它将对文本进行分词、去除停用词和标点符号，接着进行词干提取和词形还原。'
- en: '**POS tagging pipeline**: This pipeline will add POS tags to the cleaned text
    from the previous pipeline based on the position of the words in the sentences
    and context.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**POS 标注管道**：该管道将基于单词在句子中的位置和上下文，为从前一个管道清洗过的文本添加词性标注。'
- en: '**NER pipeline**: This pipeline will run a pretrained model against our text
    and attempt to label the results with the correct entities.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NER 管道**：该管道将使用一个预训练模型处理我们的文本，并尝试为结果标注正确的实体。'
- en: '**NER training pipeline**: This pipeline will train a new NER model based on
    provided training data to correct the results of the first NER pipeline.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NER 训练管道**：该管道将基于提供的训练数据训练一个新的 NER 模型，以纠正第一个 NER 管道的结果。'
- en: '**Improved NER pipeline**: This pipeline will run the new NER model against
    our text and output the list of characters in the story to a text file.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进版 NER 管道**：该管道将使用新的 NER 模型处理我们的文本，并将故事中的人物列表输出到一个文本文件。'
- en: 'Here is a diagram of our full NLP pipeline workflow:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们完整 NLP 管道工作流的示意图：
- en: '![Figure 8.2 – Pachyderm NLP pipeline'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2 – Pachyderm NLP 管道'
- en: '](img/B17085_08_002.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17085_08_002.jpg)'
- en: Figure 8.2 – Pachyderm NLP pipeline
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – Pachyderm NLP 管道
- en: Now that we have reviewed the pipeline steps, let's create all the needed repositories
    and pipelines step by step.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了管道步骤，让我们一步步创建所有需要的仓库和管道。
- en: Creating repositories and pipelines
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建仓库和管道
- en: In this section, we will create all the pipelines that we reviewed in the previous
    section. The six-step workflow will clean the data, apply POS tagging, perform
    NER, train a new custom mode based on the provided data, run the improved pipeline,
    and output the results to the final repo.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建在前一节中回顾过的所有管道。六步工作流程将清理数据，应用词性标注（POS），执行命名实体识别（NER），根据提供的数据训练一个新的自定义模型，运行改进的管道，并将结果输出到最终的代码库。
- en: The first step is to create the data cleaning pipeline that will strip the text
    from the elements we won't need for further processing.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建数据清洗管道，它将从文本中剥离我们在进一步处理时不需要的元素。
- en: Important note
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You need to download all files for this example from [https://github.com/PacktPublishing/Reproducible-Data-Science-with-Pachyderm/tree/main/Chapter08-End-to-End-Machine-Learning-Workflow](https://github.com/PacktPublishing/Reproducible-Data-Science-with-Pachyderm/tree/main/Chapter08-End-to-End-Machine-Learning-Workflow).
    The Docker image is stored at [https://hub.docker.com/repository/docker/svekars/nlp-example](https://hub.docker.com/repository/docker/svekars/nlp-example).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要从[https://github.com/PacktPublishing/Reproducible-Data-Science-with-Pachyderm/tree/main/Chapter08-End-to-End-Machine-Learning-Workflow](https://github.com/PacktPublishing/Reproducible-Data-Science-with-Pachyderm/tree/main/Chapter08-End-to-End-Machine-Learning-Workflow)下载所有示例文件。Docker
    镜像存储在[https://hub.docker.com/repository/docker/svekars/nlp-example](https://hub.docker.com/repository/docker/svekars/nlp-example)。
- en: Creating the data cleaning pipeline
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据清洗管道
- en: Data cleaning is typically performed before any other types of tasks. For this
    pipeline, we have created a Python script that uses the **Natural Language Toolkit**
    (**NLTK**) platform to perform the data cleaning task. NLTK is an open source
    set of libraries that enables you to complete a variety of NLP-related tasks,
    including tokenization, stemming, removing stop words, and lemmatization.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗通常是在执行其他任务之前进行的。对于这个管道，我们创建了一个 Python 脚本，使用**自然语言工具包**（**NLTK**）平台来执行数据清洗任务。NLTK
    是一个开源的库集合，能够完成各种 NLP 相关任务，包括分词、词干提取、去除停用词和词形还原。
- en: 'Here is the pipeline specification that we will use for this pipeline:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将用于该管道的管道规范：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This pipeline performs the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道执行以下操作：
- en: Takes a URL provided in the `data.txt` file from the `data` repository
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`data` 仓库中的`data.txt`文件获取提供的 URL
- en: Uses the `svekars/nlp-example:1.0` image
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用` svekars/nlp-example:1.0`镜像
- en: Runs the `data-clean.py` script added to the `svekars/nlp-example:1.0` image
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`data-clean.py`脚本，该脚本已添加到`svekars/nlp-example:1.0`镜像中
- en: 'You might have noticed that the glob pattern used in the pipeline specification
    uses only one file—`data.txt`. This is a file where we will have a URL to the
    *The Legend of Sleepy Hollow* text located at the *Project Gutenberg: Free eBooks*
    website. To access the website, go to [https://gutenberg.org](https://gutenberg.org).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '你可能注意到，在管道规格中使用的glob模式只处理一个文件——`data.txt`。这个文件包含了指向位于*Project Gutenberg: Free
    eBooks*网站的*《睡谷传奇》*文本的URL。要访问该网站，请前往[https://gutenberg.org](https://gutenberg.org)。'
- en: 'Now that we''ve reviewed the pipeline, let''s look closer at what our script
    does. Here is a list of the components that we will be importing in the `data-clean.py`
    script:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了管道，让我们更仔细地看看我们的脚本做了什么。以下是我们将在`data-clean.py`脚本中导入的组件列表：
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We need `BeautifulSoup` to parse the HTML file with our text. We use `urlopen`
    to open the URL inside of the `data.txt` file. We need NLTK with `stopwords`,
    `word_tokenize`, `PorterStemmer`, `WordNet`, and `WordNetLemmatizer` to perform
    various NLP operations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要`BeautifulSoup`来解析包含我们文本的HTML文件。我们使用`urlopen`来打开`data.txt`文件中的URL。我们需要NLTK以及`stopwords`、`word_tokenize`、`PorterStemmer`、`WordNet`和`WordNetLemmatizer`来执行各种NLP操作。
- en: 'The first part of code opens the `data.txt` file that we have placed in the
    data repository, reads the file, and uses the `BeautifulSoup` HTML parser to parse
    the text. In the `paragraphs` line, we strip the text from all other HTML elements
    but the `<p>` HTML tag:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一部分打开我们已放置在数据仓库中的`data.txt`文件，读取文件并使用`BeautifulSoup` HTML解析器解析文本。在`paragraphs`行中，我们去除除了`<p>`
    HTML标签以外的所有HTML元素：
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The second part of the script saves the downloaded text to the text file in
    the output repository. We will need our downstream pipelines:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的第二部分将下载的文本保存到输出仓库中的文本文件。我们将需要下游管道：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the next part of the code, we use the `word_tokenize` NLTK method to break
    the text into individual tokens and save them into the `tokens.txt` file in the
    output repository:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的下一部分中，我们使用`word_tokenize` NLTK方法将文本拆分为单独的标记，并将其保存到输出仓库中的`tokens.txt`文件中：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The next part of the code takes the tokenized text from previously, removes
    the stop words from it, and saves the result in the `no_stopwords.txt` file in
    the output repository. The **stop words** are the words that include articles,
    pronouns, and other commonly used words that don''t add a lot of value to the
    text and can be ignored for research purposes to save processing time:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的下一部分将之前标记化的文本去除停用词，并将结果保存到输出仓库中的`no_stopwords.txt`文件中。**停用词**是指包括冠词、代词等常用词，这些词对文本的价值较小，可以忽略，以节省处理时间：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The next part of the code removes punctuation from the text that was already
    tokenized and stripped of stop words. The code saves the results into a separate
    file called `no_punctuation.txt`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的下一部分移除已经进行过分词并去除停用词的文本中的标点符号。代码将结果保存到一个名为`no_punctuation.txt`的单独文件中：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next is stemming. `grouping` and `grouped` would be reduced to just `group`.
    Sometimes, this technique might be considered too aggressive and lemmatization
    can be used instead. The stemmed output is saved to `stemmed.txt`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是词干提取。`grouping`和`grouped`会被简化为`group`。有时，这种技术可能被认为过于激进，可以使用词形还原来代替。词干化后的输出会保存到`stemmed.txt`：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The final part of this script is lemmatization, which uses NLTK''s WordNet
    Lemmatizer database to perform lemmatization on the text that was tokenized and
    stripped of stop words and punctuation. This last piece of code saves the results
    to the `lematized.txt` file. We will use that file in our next pipeline:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的最后一部分是词形还原，它使用NLTK的WordNet Lemmatizer数据库对已经标记化且去除了停用词和标点符号的文本进行词形还原。最后一段代码将结果保存到`lematized.txt`文件中。我们将在下一个管道中使用该文件：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now that we know what our pipeline does, let's create it.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了管道的功能，让我们来创建它。
- en: 'To create the `data-clean.py` pipeline, complete the following steps:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建`data-clean.py`管道，请完成以下步骤：
- en: 'Open your terminal and verify that Pachyderm is up and running:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并验证Pachyderm是否正常运行：
- en: '[PRE18]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The system output is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create the data repo:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据仓库：
- en: '[PRE20]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Verify that that data repository was created:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证数据仓库是否已创建：
- en: '[PRE21]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The system output is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'From the directory where you have your `data.txt` file, put it into the data
    repository:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从你存放`data.txt`文件的目录，将其放入数据仓库：
- en: '[PRE23]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The system output is as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This file has just one line—a link to the text of *The Legend of Sleepy Hollow*
    on the Gutenberg website: [https://www.gutenberg.org/files/41/41-h/41-h.htm](https://www.gutenberg.org/files/41/41-h/41-h.htm).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件只有一行——一个指向古腾堡网站上《沉睡山庄的传说》文本的链接：[https://www.gutenberg.org/files/41/41-h/41-h.htm](https://www.gutenberg.org/files/41/41-h/41-h.htm)。
- en: 'Check that the file was placed in the repository with the `file` type:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查文件是否已放入具有`file`类型的仓库：
- en: '[PRE25]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The system output is as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create the `data-clean` pipeline:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`data-clean`流水线：
- en: '[PRE27]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: No output is returned.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 不会返回任何输出。
- en: 'Check that the pipeline was created and is starting or running:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查流水线是否已创建并且开始运行：
- en: '[PRE28]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The system output is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After a minute or so, the pipeline should finish running and upload the results
    to the output repository. Check the repository:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大约一分钟后，流水线应完成运行并将结果上传到输出仓库。请检查仓库：
- en: '[PRE30]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You should see the following output:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到以下输出：
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see, Pachyderm automatically created an output repository called
    `data-clean` and uploaded 315.8 KiB to the master branch of that repo.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Pachyderm自动创建了一个名为`data-clean`的输出仓库，并将315.8 KiB的数据上传到该仓库的主分支。
- en: 'Let''s list the files in the repo:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们列出仓库中的文件：
- en: '[PRE32]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The system output is as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE33]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You can see that the pipeline has uploaded six files into the output repository.
    We have deliberately saved them in separate files so that you can see the difference
    between them. You can view the contents of each file and compare them. For example,
    to open the `lemmatized.txt` file on macOS, run the following:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以看到，流水线已经将六个文件上传到输出仓库。我们故意将它们保存在不同的文件中，这样你可以看到它们之间的差异。你可以查看每个文件的内容并进行比较。例如，在macOS上打开`lemmatized.txt`文件，可以运行以下命令：
- en: '[PRE34]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You should see the following output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到以下输出：
- en: '![Figure 8.3 – Lemmatized words'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3 – 词形还原的单词'
- en: '](img/B17085_08_003.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17085_08_003.jpg)'
- en: Figure 8.3 – Lemmatized words
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 词形还原的单词
- en: In this section, we have created a pipeline that cleans our text. In the next
    section, we will create our next pipeline, which will apply POS tags to our lemmatized
    text.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经创建了一个清理文本的流水线。接下来的章节，我们将创建下一个流水线，应用POS标签到我们的词形还原文本。
- en: Creating the POS tagging pipeline
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建POS标注流水线
- en: POS tagging is an NLP technique that labels each word with a relevant part of
    speech. This process is used in many NLP problems, such as text disambiguation
    and text-to-speech conversion.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: POS标注是一种NLP技术，它为每个单词标记相关的词性。这个过程在许多NLP问题中都非常有用，比如文本歧义消解和文本转语音。
- en: 'For this task, we have used **spaCy**, a free library that performs POS tagging,
    NER, and other tasks. For example, say you have the following sentence:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这项任务，我们使用了**spaCy**，这是一款免费的库，能够执行POS标注、命名实体识别（NER）等任务。例如，假设你有如下句子：
- en: '*Whoever is happy will make others happy too.*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*任何快乐的人都会让别人也感到快乐。*'
- en: 'Here is an example of what POS tagging with spaCy looks like:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用spaCy进行POS标注的示例：
- en: '![Figure 8.4 – POS tagging example](img/Table_011.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – POS标注示例](img/Table_011.jpg)'
- en: Figure 8.4 – POS tagging example
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – POS标注示例
- en: We will use spaCy to find POS tags in our lemmatized text from the `data-clean`
    pipeline.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用spaCy从`data-clean`流水线中找到我们词形还原文本的POS标签。
- en: 'Here is what our POS tagging pipeline specification will look like:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们POS标注流水线规范的样子：
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This pipeline performs the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 该流水线执行以下操作：
- en: Takes the `lemmatized.txt` file from the `data-clean` repository
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`data-clean`仓库获取`lemmatized.txt`文件
- en: Uses the `svekars/nlp-example:1.0` Docker image
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`svekars/nlp-example:1.0` Docker镜像
- en: Runs the `pos-tag.py` script against our lemmatized text
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`pos-tag.py`脚本处理我们的词形还原文本
- en: Outputs a table with all POS tags found in the text inthe `pos_table.txt` file,
    a file with the total number for each POS tag, to `pos_number.txt` and creates
    a dependency graph saved as a `pos-tag-dependency.svg` file
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出一张表格，其中包含在`pos_table.txt`文件中找到的所有POS标签，另有一份包含每种POS标签总数的文件`pos_number.txt`，并创建一个依赖关系图，保存为`pos-tag-dependency.svg`文件
- en: Now that we have reviewed what the pipeline does, let's take a look at the `pos-tag.py`
    script.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了流水线的工作内容，接下来让我们看看`pos-tag.py`脚本。
- en: 'The script imports the following components and libraries:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本导入了以下组件和库：
- en: '[PRE36]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We need `spacy` and its modules to perform POS tagging, count them, and visualize
    the results. We import the `en_core_web_sm` pretrained spaCy model to do the tagging
    task. We need IPython as a spaCy dependency. Finally, we are using `pathlib` and
    `redirect_stdout` to save the results.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要`spacy`及其模块来执行词性标注、统计它们并可视化结果。我们导入`en_core_web_sm`预训练的spaCy模型来完成标注任务。我们需要IPython作为spaCy的依赖。最后，我们使用`pathlib`和`redirect_stdout`来保存结果。
- en: 'The first part of the code imports a pretrained spaCy model called `en_core_web_sm`.
    POS tagging requires you to use either a pretrained model or your own custom model.
    The `en_core_web_sm` model does a good job of tagging POSes. Therefore, we will
    just use it. Then, the script opens our `lematized.txt` file, tags all the words
    in the file, and prints out the result to the `pos-table.txt` file:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一部分导入了一个名为`en_core_web_sm`的预训练spaCy模型。词性标注要求你使用预训练模型或自定义模型。`en_core_web_sm`模型在词性标注方面表现良好。因此，我们将直接使用它。然后，脚本打开我们的`lematized.txt`文件，对文件中的所有单词进行词性标注，并将结果输出到`pos-table.txt`文件：
- en: '[PRE37]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The next part of the code counts the number of each tag in the processed text
    and outputs the results to the `pos-number.txt` file:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的下一部分统计处理过的文本中每个标签的数量，并将结果输出到`pos-number.txt`文件：
- en: '[PRE38]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, the last part of the script generates a dependency graph and saves
    it as an SVG image, `pos-tag-dependency.svg`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，脚本的最后部分生成一个依赖图，并将其保存为SVG图像`pos-tag-dependency.svg`：
- en: '[PRE39]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now, let's create our pipeline.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的管道。
- en: 'To create a POS tagging pipeline, do the following:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个词性标注管道，请执行以下操作：
- en: 'Open your terminal and verify that Pachyderm is up and running:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的终端并验证Pachyderm是否已启动并运行：
- en: '[PRE40]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You should see the following output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '[PRE41]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create the POS tagging pipeline:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建词性标注管道：
- en: '[PRE42]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: No system output is returned.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 没有返回系统输出。
- en: 'Check that the pipeline was created and is running:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查管道是否已创建并正在运行：
- en: '[PRE43]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This is the output that this command returns:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这是此命令返回的输出：
- en: '[PRE44]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'When the pipeline finishes running, check the output repository:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当管道运行完成后，检查输出仓库：
- en: '[PRE45]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This command returns the following output:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令返回以下输出：
- en: '[PRE46]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Pachyderm has created an output repository called `pos-tag` and uploaded 10.82
    MiB to the master branch of this repository.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm创建了一个名为`pos-tag`的输出仓库，并将10.82 MiB文件上传到该仓库的主分支。
- en: 'Now, let''s take a look at files that were uploaded to the output repo:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们查看上传到输出仓库的文件：
- en: '[PRE47]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This command returns the following system output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令返回以下系统输出：
- en: '[PRE48]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s take a look at the number of each tag we have in our text:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看我们文本中每个标签的数量：
- en: '[PRE49]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You should see the following output:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '[PRE50]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, let''s take a look at the dependency graph. If you are on macOS, run
    the following:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们看看依赖图。如果你使用的是macOS，请运行以下命令：
- en: '[PRE51]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Google Chrome opens the file:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Google Chrome打开文件：
- en: '![](img/B17085_08_005.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17085_08_005.jpg)'
- en: Figure 8.5 – POS dependency graph
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 词性依赖图
- en: Important note
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You might need to scroll down in your browser to see the graph. Because we ran
    POS tagging against the whole book, this graph is very long. You'll need to scroll
    horizontally to see it all.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要在浏览器中向下滚动才能看到图表。由于我们对整本书进行了词性标注，这个图表非常长。你需要横向滚动才能查看完整内容。
- en: In this section, we configured a POS tagging pipeline by using Pachyderm and
    spaCy, as well as visualizing it with a dependency graph. Next, we'll configure
    an NER pipeline that will help us find the main characters of the story.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用Pachyderm和spaCy配置了一个词性标注管道，并通过依赖图进行了可视化。接下来，我们将配置一个命名实体识别（NER）管道，帮助我们找到故事中的主要人物。
- en: Creating an NER pipeline
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建命名实体识别（NER）管道
- en: 'NER is an information extraction technique that recognizes entities in text
    and puts them in certain categories, such as person, location, and organization.
    For example, say we have the following phrase:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）是一种信息提取技术，它能够识别文本中的实体，并将它们归类为特定类别，如人物、地点和组织。例如，假设我们有以下短语：
- en: '*Snap Inc. Announces First Quarter 2021 Financial Results*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*Snap Inc.宣布2021年第一季度财务结果*'
- en: 'If you use spaCy''s `en_core_web_lg` against this phrase, you will get the
    following results:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这个短语使用spaCy的`en_core_web_lg`模型，你将得到以下结果：
- en: '[PRE52]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Name recognition can be useful in a variety of tasks. In this section, we will
    use it to retrieve the main characters of *The Legend of Sleepy Hollow*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 名字识别可以在多种任务中派上用场。在本节中，我们将使用它来提取《沉睡谷传说》中的主要人物。
- en: 'Here is what our NER pipeline specification will look like:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们NER管道规范的样子：
- en: '[PRE53]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This pipeline performs the following:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道执行以下操作：
- en: Takes the original text of *The Legend of Sleepy Hollow* from the `data-clean`
    repository
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`data-clean`仓库获取*The Legend of Sleepy Hollow*的原始文本
- en: Uses the `svekars/nlp-example:1.0` Docker image
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`svekars/nlp-example:1.0` Docker镜像
- en: Runs the `ner.py` script
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`ner.py`脚本
- en: Outputs the results to the `ner` repository
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果输出到`ner`仓库
- en: 'Now, let''s look at what the `ner.py` script does. Here is the list of components
    the script imports:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看`ner.py`脚本的功能。以下是脚本导入的组件列表：
- en: '[PRE54]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We need `spacy` to perform NER and the `displacy` module to visualize the results.
    `redirect_stdout` is a handy way to redirect printed output to a file.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要`spacy`来执行NER任务，`displacy`模块用于可视化结果。`redirect_stdout`是一种将打印输出重定向到文件的便捷方法。
- en: 'The rest of the code imports spaCy''s pretrained model called `en_core_web_lg`.
    This model seems to perform better on NER tasks than its counterpart that we used
    in the POS tagging pipeline. Then, the script grabs the original text stored in
    the `text.txt` file in the `data-clean` repository and performs the NER task:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码导入了spaCy的预训练模型`en_core_web_lg`。这个模型似乎在NER任务上的表现优于我们在POS标注管道中使用的模型。接着，脚本从`data-clean`仓库中的`text.txt`文件中获取原始文本，并执行NER任务：
- en: '[PRE55]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Finally, the script visualizes the results with `displacy` and saves them in
    the HTML format:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，脚本使用`displacy`可视化结果并将其保存为HTML格式：
- en: '[PRE56]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now that we know what our script does, let's create the pipeline.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了脚本的功能，接下来我们来创建管道。
- en: 'To create the NER pipeline, complete the following steps:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建NER管道，完成以下步骤：
- en: 'Open your terminal and verify that Pachyderm is up and running:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端，验证Pachyderm是否已启动并正在运行：
- en: '[PRE57]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This command returns the following output:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令返回以下输出：
- en: '[PRE58]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Create the POS tagging pipeline:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建POS标注管道：
- en: '[PRE59]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: No system output is returned.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 没有返回系统输出。
- en: 'Check that the pipeline was created and is starting or running:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查管道是否创建并已开始运行：
- en: '[PRE60]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'You should see the following output:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到以下输出：
- en: '[PRE61]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'When the pipeline finishes running, check the output repository:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当管道运行完毕后，检查输出仓库：
- en: '[PRE62]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The command returns this system output:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令返回以下系统输出：
- en: '[PRE63]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Pachyderm has created an output repository called `ner` and uploaded 43.49 MiB
    to the master branch.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm创建了一个名为`ner`的输出仓库，并上传了43.49 MiB的数据到主分支。
- en: 'Let''s take a look at the files that were uploaded to the output repo:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看上传到输出仓库的文件：
- en: '[PRE64]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The following output is returned:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下输出：
- en: '[PRE65]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: This repository has two files. One is the list of all instances of entities
    found in the file and the other is the visualization for all the entities.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 该仓库有两个文件。一个是文件中找到的所有实体的列表，另一个是所有实体的可视化结果。
- en: 'Print the first 10 lines of the `ner-list.txt` file to the terminal:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`ner-list.txt`文件的前10行打印到终端：
- en: '[PRE66]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This command returns the following output:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令返回以下输出：
- en: '[PRE67]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: As you can see, the NER model has identified many entities in the text. Let's
    open the HTML file to view all the entities.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，NER模型已经识别了文本中的许多实体。让我们打开HTML文件，查看所有实体。
- en: 'Open the HTML file:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开HTML文件：
- en: '[PRE68]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The file will open in Google Chrome:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 文件将在Google Chrome中打开：
- en: '![Figure 8.6 – NER labels'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.6 – NER 标签'
- en: '](img/B17085_08_006.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17085_08_006.jpg)'
- en: Figure 8.6 – NER labels
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – NER 标签
- en: You can see that the spaCy model has identified many entities in the text correctly.
    However, if you start browsing, you'll notice that it has skipped some of them.
    For example, it did not tag *Headless Horseman* as **PERSON** in some cases. This
    is a known accuracy problem of pretrained models. In the next section, we will
    correct this by retraining our model to use these entities.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，spaCy模型已经正确地识别了文本中的许多实体。然而，如果你开始浏览，你会注意到它遗漏了一些实体。例如，在某些情况下，它没有将*Headless
    Horseman*标记为**PERSON**。这是预训练模型的一个已知准确性问题。在下一节中，我们将通过重新训练模型来修正这个问题，以便使用这些实体。
- en: Retraining an NER model
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新训练NER模型
- en: Inaccuracy in NER pipeline results is a common problem. The only way to fix
    it is to retrain an existing model or train your own model completely from scratch.
    Training a model from scratch is a difficult and lengthy operation. In our case,
    we don't need to necessarily train a completely new model but instead, we can
    retrain the existing model to understand the missing context. To accomplish this
    task, we will put training data into the `data-clean` repository, create a training
    pipeline that will train on that data, save our model to an output repository,
    and then run the retrained model against our original text again.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: NER（命名实体识别）管道结果的不准确性是一个常见问题。解决这个问题的唯一方法是重训练现有模型，或者从头开始完全训练一个新的模型。从头训练一个模型是一个既困难又耗时的操作。在我们的案例中，我们不一定需要训练一个完全新的模型，而是可以重训练现有模型，以理解缺失的上下文。为了完成这个任务，我们将把训练数据放入
    `data-clean` 仓库，创建一个训练管道在该数据上进行训练，将模型保存到输出仓库中，然后再次运行重训练后的模型对原始文本进行处理。
- en: 'In Pachyderm terms, this means that we will create two pipelines:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pachyderm 的术语中，这意味着我们将创建两个管道：
- en: The first pipeline, called `retrain`, will train our model and output the new
    model to the `train` output repository.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个管道，名为 `retrain`，将训练我们的模型并将新模型输出到 `train` 输出仓库。
- en: The second pipeline, called `my-model`, will use the new model to analyze our
    text and upload the results to the `my-model` repository.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个管道，名为 `my-model`，将使用新模型分析我们的文本，并将结果上传到 `my-model` 仓库。
- en: Now, let's create the retrain pipeline.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建重训练管道。
- en: Creating the retrain pipeline
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建重训练管道
- en: 'For this pipeline, we will create the following pipeline specification:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个管道，我们将创建以下管道规范：
- en: '[PRE69]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This pipeline takes the `training-data.json` file that has our training data
    and runs the `ner-improved.py` script to improve the existing model. The results
    are saved to the `retrain` repository. For this example, we do not need a lot
    of training examples, but in a real-life use case, you would have to have hundreds
    of examples to improve the accuracy of your model.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道采用包含我们训练数据的 `training-data.json` 文件，并运行 `ner-improved.py` 脚本以改进现有模型。结果将保存到
    `retrain` 仓库中。对于这个示例，我们不需要太多的训练实例，但在实际使用中，你可能需要成百上千的示例来提高模型的准确性。
- en: 'Here is a list of the components the `ner-improved.py` script imports:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `ner-improved.py` 脚本导入的组件列表：
- en: '[PRE70]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We need `spacy` with `minibatch` and `Example.from_dict` methods to train the
    model. We use `random` to shuffle the files into a different order for better
    training. The `simplejson` Python decoder is needed to read the training data
    file in the JSON format and `redirect_stdout` is needed to save the results in
    an output.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要 `spacy` 库中的 `minibatch` 和 `Example.from_dict` 方法来训练模型。我们使用 `random` 来打乱文件顺序，以便更好地训练。需要
    `simplejson` Python 解码器来读取 JSON 格式的训练数据文件，并且需要 `redirect_stdout` 来保存输出结果。
- en: 'The next part of the script loads the spaCy model, reads the training data
    file, and opens a spaCy NER pipeline:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的下一部分加载 spaCy 模型，读取训练数据文件，并打开 spaCy NER 管道：
- en: '[PRE71]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This part of the code uses an optimizer, which performs the gradient descent
    calculation. Then, the script specifies that only the NER pipeline needs to be
    trained and all others should be ignored. The next `for` loop performs the actual
    training, updates the model, and prints the losses. We will train our model for
    30 iterations:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分代码使用了一个优化器，它执行梯度下降计算。然后，脚本指定只需训练 NER 管道，忽略其他管道。接下来的 `for` 循环执行实际的训练，更新模型，并打印损失值。我们将训练模型
    30 次：
- en: '[PRE72]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The last part of the script tests the retrained pipeline against a test text
    and outputs the results to the `ner-improved.txt` text. The retrained model is
    saved using `pickle.dump` as in the `ner-improved-model.p` file directory in the
    `output` repository:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的最后一部分测试重训练管道在测试文本上的效果，并将结果输出到 `ner-improved.txt` 文件。重训练后的模型将使用 `pickle.dump`
    保存在 `output` 仓库中的 `ner-improved-model.p` 文件目录中：
- en: '[PRE73]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now, let''s create the pipeline:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建管道：
- en: 'Open your terminal and verify that Pachyderm is up and running:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并验证 Pachyderm 是否正在运行：
- en: '[PRE74]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The system output is as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE75]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Put `training-data.json` in the data repository:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `training-data.json` 放入数据仓库：
- en: '[PRE76]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Create the retrain pipeline:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建重训练管道：
- en: '[PRE77]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: No system output is returned.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 没有系统输出返回。
- en: 'Check that the pipeline was created and is starting or running:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查管道是否已创建，并且正在启动或运行：
- en: '[PRE78]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The system output is as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE79]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'After some time, check the output repository:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一段时间后，检查输出仓库：
- en: '[PRE80]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The system output is as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE81]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: As you can see, Pachyderm uploaded 816.7 MiB to the `retrain` repository. This
    is our retrained model that takes this much space.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Pachyderm 将 816.7 MiB 上传到 `retrain` 仓库。这是我们重新训练的模型，占用了这么多空间。
- en: 'List the files in the `retrain` repo:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出 `retrain` 仓库中的文件：
- en: '[PRE82]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The system output is as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE83]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Let''s look at the `ner-improved.txt` file, which should have the results of
    running the retrained model against the test text:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 `ner-improved.txt` 文件，它应该包含重新训练的模型对测试文本运行的结果：
- en: '[PRE84]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The system output is as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE85]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Great! *Headless Horseman* and *Ichabod Crane* are defined as **PERSON**.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！*Headless Horseman* 和 *Ichabod Crane* 被定义为 **PERSON**。
- en: Now that we have retrained our model, let's deploy our final pipeline, which
    will give us improved NER and output all the characters of the story into one
    file.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经重新训练了模型，接下来让我们部署最终的流水线，它将提供改进后的 NER，并将故事中的所有人物输出到一个文件中。
- en: Deploying the retrained pipeline
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署重新训练的流水线
- en: 'Our retrained pipeline needs to be a cross-pipeline to combine our new retrained
    model with our text:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的重新训练流水线需要是一个跨流水线，以便将我们新的重新训练模型与文本结合起来：
- en: '[PRE86]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: This pipeline will take the same `text.txt` file as in the original NER pipeline
    and create a cross-product of our retrained model with that text. It will output
    the results to the `my-model` repository. The resulting files will have an HTML
    file with better NER tagging and a text file with the list of characters in *The
    Legend of Sleepy Hollow*.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这个流水线将使用与原始 NER 流水线相同的 `text.txt` 文件，并将我们重新训练的模型与该文本的交叉产品生成。它将结果输出到 `my-model`
    仓库中。生成的文件将包含一个改进了 NER 标记的 HTML 文件和一个包含 *The Legend of Sleepy Hollow* 中人物列表的文本文件。
- en: '`ner-my-model.py` is very similar to the original `ner.py` script with the
    following differences.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '`ner-my-model.py` 与原始的 `ner.py` 脚本非常相似，主要有以下区别：'
- en: 'It loads our improved model by using `pickle.load` instead of the original
    spaCy model:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过使用 `pickle.load` 而不是原始的 spaCy 模型来加载我们的改进模型：
- en: '[PRE87]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'It counts the total number of instances for each `/pfs/out/person-label-count.txt`
    file:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 它统计每个 `/pfs/out/person-label-count.txt` 文件中实例的总数：
- en: '[PRE88]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: It saves the HTML visualization to `ner-improved-labels.html`. It saves all
    entities to `ner-improved-list.txt` in the `my-model` repository.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 它将 HTML 可视化保存到 `ner-improved-labels.html` 文件中。它将所有实体保存到 `ner-improved-list.txt`
    文件中，该文件位于 `my-model` 仓库。
- en: 'Let''s create our final pipeline:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建最终的流水线：
- en: 'Open your terminal and verify that Pachyderm is up and running:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并确认 Pachyderm 正在运行：
- en: '[PRE89]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The system output is as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE90]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Create the `my-model` pipeline:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `my-model` 流水线：
- en: '[PRE91]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: This command does not return any output.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令不会返回任何输出。
- en: 'Check that the pipeline was created and is starting or running:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查流水线是否已创建并正在启动或运行：
- en: '[PRE92]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The system output is as follows:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE93]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Note that the `my-model` pipeline is starting. It is different from all other
    pipelines we've created for this example. Because we are saving our model in the
    `retrain` repository and we need to combine it with the text in the data repository,
    a standard pipeline won't be able to accomplish this. That's why we create a cross-pipeline
    that combines two inputs.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`my-model` 流水线正在启动。它不同于我们为此示例创建的所有其他流水线。因为我们将模型保存在 `retrain` 仓库中，并且需要将其与数据仓库中的文本结合，所以标准流水线无法完成这个任务。为了实现这一目标，我们创建了一个跨流水线，将两个输入结合起来。
- en: 'Let''s check the output repository:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查输出仓库：
- en: '[PRE94]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The system output is as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE95]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Pachyderm uploaded 26.15 MiB to the `my-model` repository. This is the result
    of our computation.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm 将 26.15 MiB 上传到 `my-model` 仓库。这是我们计算的结果。
- en: 'List the files in the `my-model` repo:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出 `my-model` 仓库中的文件：
- en: '[PRE96]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'The system output is as follows:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE97]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Let''s look at the `person-label-count.txt` file, which should provide the
    total count for each unique **PERSON** instance:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 `person-label-count.txt` 文件，它应该提供每个独特 **PERSON** 实例的总计数：
- en: '[PRE98]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The system output is as follows:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 系统输出如下：
- en: '[PRE99]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: As you can see, the output is still not entirely accurate because we see instances
    of *Ichabod* and *Ichabod Crane* separately. If we provide more training data,
    we can improve these results. However, you already see the most often-listed characters
    in this list and can understand that *Ichabod Crane* is likely the main character
    of the story.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，输出结果仍然不完全准确，因为我们看到 *Ichabod* 和 *Ichabod Crane* 分别出现。如果我们提供更多的训练数据，就能改善这些结果。不过，你已经能看到这个列表中最常出现的人物，并且可以理解
    *Ichabod Crane* 很可能是这个故事的主要人物。
- en: 'Open the HTML file to view the highlighted version of the results:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 HTML 文件以查看高亮版本的结果：
- en: '[PRE100]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: This concludes our experiment with the spaCy NER model. You can add more training
    data to see how the accuracy will improve with more training examples.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们关于spaCy NER模型的实验。你可以添加更多的训练数据，看看在更多训练样本的情况下，准确度会如何提高。
- en: Now let's clean up our cluster.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们清理我们的集群。
- en: Cleaning up
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理中
- en: 'After you are done experimenting, you might want to clean up your cluster so
    that you start your next experiment with a fresh install. To clean up the environment,
    do the following:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 完成实验后，你可能想要清理集群，以便以全新安装开始下一个实验。要清理环境，请执行以下操作：
- en: 'Delete all pipelines and repositories:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除所有的管道和仓库：
- en: '[PRE101]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Verify that no repositories and pipelines exist in your cluster:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证你的集群中没有仓库和管道：
- en: '[PRE102]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'You should see the following output:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '[PRE103]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: You have successfully cleaned up your cluster.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 你已成功清理了你的集群。
- en: Summary
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned how to build a complex machine learning workflow
    with the NER pipeline example. We have learned how to clean the data with the
    NTLK library, how to do POS tagging, and finally, how to retrain a spaCy model
    inside Pachyderm and output results for preview. You can do much more and tweak
    this example further to achieve better accuracy of NER by adding more training
    data and tweaking the model training parameters.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用NER管道示例构建一个复杂的机器学习工作流。我们学习了如何使用NLTK库清理数据，如何进行词性标注（POS tagging），以及如何在Pachyderm中重新训练spaCy模型并输出结果以供预览。你可以做更多的操作，通过添加更多训练数据和调整模型训练参数，进一步提升NER的准确度。
- en: In the next chapter, we will learn how to do hyperparameter tuning in Pachyderm
    on an example of housing price prediction.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何在Pachyderm中进行超参数调优，以预测房价为例。
- en: Further reading
  id: totrans-398
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'NLTK documentation: [https://www.nltk.org/](https://www.nltk.org/)'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLTK文档：[https://www.nltk.org/](https://www.nltk.org/)
- en: 'spaCy documentation: [https://spacy.io/api/doc](https://spacy.io/api/doc)'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy文档：[https://spacy.io/api/doc](https://spacy.io/api/doc)
- en: '*The Legend of Sleepy Hollow* on the Gutenberg project website: [https://www.gutenberg.org/files/41/41-h/41-h.htm](https://www.gutenberg.org/files/41/41-h/41-h.htm)'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*睡谷的传说* 在古腾堡项目网站上的链接：[https://www.gutenberg.org/files/41/41-h/41-h.htm](https://www.gutenberg.org/files/41/41-h/41-h.htm)'
