- en: Attention Mechanisms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制
- en: In the preceding two chapters, we learned about convolutional neural networks
    and recurrent neural networks, both of which have been very effective for sequential
    tasks such as machine translation, image captioning, object recognition, and so
    on. But we have also seen that they have limitations. RNNs have problems with
    long-term dependencies. In this chapter, we will cover attention mechanisms, which
    have been increasing in popularity and have shown incredible results in language-
    and vision-related tasks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们学习了卷积神经网络（CNN）和循环神经网络（RNN），这两者在机器翻译、图像描述、物体识别等顺序任务中非常有效。但我们也看到它们有一些局限性。RNN存在长时间依赖性的问题。在本章中，我们将介绍注意力机制，它在语言和视觉相关任务中越来越受欢迎，并且取得了惊人的成果。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Overview of attention
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制概述
- en: Understanding neural Turing machines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解神经图灵机
- en: Exploring the types of attention
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索注意力的类型
- en: Transformers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer
- en: Let's get started!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Overview of attention
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制概述
- en: When we go about our lives (in the real world), our brains don't observe every
    detail in our environment at all times; instead, we focus on (or pay greater attention
    to) information that is relevant to the task at hand. For example, when we are
    driving, we are able to adjust our focal length to focus on different details,
    some of which are closer and others are further away, and then act on what we
    observe. Similarly, when we are conversing with others, we usually don't listen
    carefully to each and every word; we listen to only part of what is spoken and
    use it to infer the relationships with some of the words to figure out what the
    other person is saying. Often, when we are reading/listening to someone, we can
    use some words to infer what the person is going to say next based on what we
    have already read/heard.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在现实世界中生活时，大脑并不会时刻观察到我们周围环境的每个细节；相反，我们会专注于（或者说更关注）与当前任务相关的信息。例如，当我们开车时，我们能够调整焦距，专注于不同的细节，有些更近一些，有些则远一些，然后根据我们观察到的情况做出反应。类似地，当我们与他人交谈时，我们通常并不会仔细听每一个字；我们只听其中一部分内容，并根据听到的内容推断出与其他词语的关系，从而理解对方的意思。通常，在我们阅读或听别人说话时，我们可以根据已读或已听的内容推测出对方接下来要说什么。
- en: But why do we need these attention mechanisms in deep learning? Let's take a
    stroll down memory lane to [Chapter 10](70ef350e-d1bd-4348-b5ec-aae11454bd69.xhtml),
    *Recurrent Neural Networks*, where we learned about sequence-to-sequence models
    (RNNs), which, as we saw, can be used for tasks such as language-to-language translation.
    We should recall that these types of models have an encoder-decoder architecture
    where the encoder takes in the input and compresses the information into an embedding
    space (or context vector), while the decoder takes in the context vector and transforms
    it back into the desired output. Both the encoder and decoder are RNNs (possibly
    with LSTMs or GRUs for long-term memory).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么深度学习需要这些注意力机制呢？让我们回顾一下[第10章](70ef350e-d1bd-4348-b5ec-aae11454bd69.xhtml)，*循环神经网络*，在那一章中我们学习了序列到序列的模型（RNN），正如我们所看到的，这些模型可以用于语言翻译等任务。我们应该回想起，这些模型采用了编码器-解码器架构，其中编码器将输入压缩到嵌入空间（或上下文向量），而解码器则将上下文向量转化为所需的输出。编码器和解码器都是RNN（可能配合LSTM或GRU来处理长期记忆）。
- en: In the previous chapter, we also came across a few limitations that RNNs have –
    in particular, the problem of vanishing or exploding gradients, which hinders
    long-term dependencies. And thus, the attention mechanism was created. It was
    created for the sole purpose of solving this very problem of memorizing long sentences
    that RNNs face.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们还遇到了一些RNN的局限性——特别是梯度消失或爆炸的问题，这妨碍了长时间依赖的处理。因此，注意力机制应运而生。它的创建就是为了解决RNN在记忆长句子时遇到的这个问题。
- en: In RNNs, the hidden state at each time step takes the previous hidden state
    as input (which contains the context of the sequence it has seen so far) and the
    input for that time step, and then the final hidden state is passed to the decoder
    sequence. Attention mechanisms differ by creating connections between the context
    vector and the whole sequence of inputs. This way, we no longer have to worry
    about how much will end up being forgotten. And similar to all the other connections
    in ANNs, these attention connections are weighted, which means they can be adjusted
    for each output. Essentially, the context vector controls the alignment between
    the input and target (output).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN中，每个时间步的隐藏状态以前一个隐藏状态（包含迄今为止看到的序列的上下文信息）和该时间步的输入作为输入，最终的隐藏状态会传递给解码器序列。注意力机制通过在上下文向量和整个输入序列之间建立连接来进行区分。这样，我们不再需要担心有多少信息最终会被遗忘。与ANN中的所有其他连接类似，这些注意力连接是有权重的，这意味着它们可以根据每个输出进行调整。本质上，上下文向量控制输入与目标（输出）之间的对齐。
- en: 'Let''s suppose we have an input, [![](img/413fee1b-9c96-4ebb-b7a6-891b310bf9de.png),] and
    a target, [![](img/3ad54583-0855-4162-816a-9258c7deb0b8.png)], where the hidden
    states produced by the encoder (a vanilla RNN for simplicity, though this could
    be any RNN architecture) will be [![](img/ef65b64f-2805-4934-af79-4a87e722fc13.png)].
    The hidden states for the decoder here are slightly different from what we have
    previously seen:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个输入，[![](img/413fee1b-9c96-4ebb-b7a6-891b310bf9de.png)]，和一个目标，[![](img/3ad54583-0855-4162-816a-9258c7deb0b8.png)]，其中由编码器（为了简单起见为一个普通的RNN，尽管它可以是任何RNN架构）生成的隐藏状态为[![](img/ef65b64f-2805-4934-af79-4a87e722fc13.png)]。解码器的隐藏状态与我们之前看到的略有不同：
- en: '![](img/75092bed-dc07-4216-93f8-bbad1af80139.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75092bed-dc07-4216-93f8-bbad1af80139.png)'
- en: For all [![](img/582257a5-b5d8-4325-95f4-ebc9a5eecf71.png)]. Here, *c[t]* (the
    context vector) is a sum of all hidden states from the input. This is weighted
    by the alignment scores so that [![](img/a299bc60-e389-40a1-ad66-9b7dc1102e23.png)] and [![](img/3f88f612-9a6e-4b6c-afa1-3065390e28d6.png),] which
    determine the alignment of *y[t]* and *x[i]* by assigning a score based on how
    well the two match. Each *α[t,i]* here is a weight that decides the extent to
    which each source's hidden state should have an impact on each output.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 [![](img/582257a5-b5d8-4325-95f4-ebc9a5eecf71.png)]，这里，*c[t]*（上下文向量）是所有输入的隐藏状态的总和。这个总和根据对齐得分进行加权，以便 [![](img/a299bc60-e389-40a1-ad66-9b7dc1102e23.png)]
    和 [![](img/3f88f612-9a6e-4b6c-afa1-3065390e28d6.png)] 确定 *y[t]* 和 *x[i]* 的对齐，通过根据两者的匹配程度分配得分。每个
    *α[t,i]* 都是一个权重，决定了每个源的隐藏状态对每个输出的影响程度。
- en: 'The preceding score function is parameterized by an MLP with a single hidden
    layer and is calculated using the following equation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的得分函数由一个具有单层隐藏层的MLP进行参数化，并通过以下方程计算：
- en: '![](img/cd8fe6a1-807b-4538-a06c-300672586fc9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd8fe6a1-807b-4538-a06c-300672586fc9.png)'
- en: Here, **V**[a] and **W**[a] are weight matrices to be learned.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**V**[a] 和 **W**[a] 是需要学习的权重矩阵。
- en: Before we dive deep into the inner workings of various attention mechanisms,
    let's take a look at neural Turing machines.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨各种注意力机制的内部工作原理之前，先来了解一下神经图灵机。
- en: Understanding neural Turing machines
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经图灵机
- en: The **Turing machine (TM)** was proposed by Alan Turing in 1936, and it is a
    mathematical model of computation made up of an infinitely long tape and a head
    that interacts with the tape by reading, editing, and moving symbols on it. It
    works by manipulating symbols on the strip according to a predefined set of rules.
    The tape is made up of an endless number of cells, each of which can contain one
    of three symbols – 0, 1, or blank (" "). Therefore, this is referred to as a **three-symbol
    Turing machine**. Regardless of how simple it seems, it is capable of simulating
    any computer algorithm, regardless of complexity. The tape that these computations
    are done on can be considered to be the machine's memory, akin to how our modern-day
    computers have memory. However, the Turing machine differs from modern-day computers
    as it has limited memory and computational limitations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**图灵机（TM）**由艾伦·图灵在1936年提出，它是由一条无限长的带子和一个通过读取、编辑和移动带子上的符号与带子互动的头部组成的计算模型。它通过按照预定的规则操作带子上的符号来工作。带子由无限多个单元组成，每个单元可以包含三种符号之一——0、1或空白（"
    "）。因此，这被称为**三符号图灵机**。尽管它看起来很简单，但它能够模拟任何计算机算法，无论其复杂性如何。执行这些计算的带子可以被视为机器的内存，类似于我们现代计算机的内存。然而，图灵机与现代计算机的不同之处在于它具有有限的内存和计算能力。'
- en: In [Chapter 10](70ef350e-d1bd-4348-b5ec-aae11454bd69.xhtml), *Recurrent Neural
    Networks*, we learned that this type of ANN is Turing complete, which means that
    when they are properly trained, they can simulate any arbitrary procedure. But
    this is only in theory. In practice, we have seen that this is not the case because
    they do have their limitations. To overcome these limitations, in 2014, Alex Graves
    et al. proposed augmenting an RNN with a large, addressable memory (similar to
    the tape in a TM), thereby giving it the name **neural Turing machine** (**NTM**).
    This model, as the authors stated, *is a differentiable computer that can be trained
    by gradient descent, yielding a practical mechanism for learning programs*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第10章](70ef350e-d1bd-4348-b5ec-aae11454bd69.xhtml)，*递归神经网络* 中，我们学习到这种类型的ANN是图灵完备的，这意味着当它们经过适当训练时，可以模拟任何任意的过程。但这仅仅是理论上的。在实践中，我们看到情况并非如此，因为它们确实有其局限性。为了克服这些局限性，2014年，Alex
    Graves 等人提出将RNN与一个大型、可寻址的内存（类似于图灵机中的磁带）结合，从而赋予其“神经图灵机”（**NTM**）这一名称。正如作者所言，*它是一个可微分的计算机，可以通过梯度下降进行训练，提供一个学习程序的实用机制*。
- en: 'The NTM borrows the idea of working memory (a process in human cognition),
    which is the same as short-term memory, and applies it to RNNs, thereby giving
    it the ability to selectively read from and write to memory using an attentional
    controller. We can see what the NTM looks like in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: NTM借鉴了工作记忆（人类认知中的一个过程）的概念，工作记忆与短期记忆相同，并将其应用于RNN，从而赋予其通过注意力控制器选择性地读取和写入内存的能力。我们可以在以下图示中看到NTM的样子：
- en: '![](img/f4a1ea46-836f-40e8-930f-97194df0c5f4.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4a1ea46-836f-40e8-930f-97194df0c5f4.png)'
- en: In the preceding diagram, we can see that there are two main components to the
    NTM – the controller, which is a neural network, and the memory, which contains
    processed information. The controller interacts with the external world by receiving
    an input vector and outputting an output vector, but it differs from the ANNs
    from previous chapters in that it also interacts with the memory matrix using
    the selective read and write operations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们可以看到NTM有两个主要组成部分——控制器，它是一个神经网络，以及内存，它包含处理过的信息。控制器通过接收输入向量并输出输出向量与外部世界进行交互，但它与前几章的ANNs不同，因为它还通过选择性读取和写入操作与内存矩阵进行交互。
- en: Reading
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取
- en: Let's suppose our memory is an *N*×*M* matrix and that it is denoted by [![](img/89ed3e76-7c89-472c-baa9-0719fa281259.png)],
    where *t* is the time step, *N* is the number of rows (memory locations), and
    *M* is the size of the vector at each location. Then, we have another vector of
    weights, **w***[t]*, which determines how much attention to designate to various
    locations in the memory (that is, the rows of the matrix). Each of the *N* weightings
    in the weight vector is normalized, which tells us that [![](img/f0751cb7-2506-4c20-a962-5770f6760dee.png)] and
    that for all *i*, we have [![](img/ff03c942-ed7d-4fd4-98dd-0107e30786ca.png)],
    where [![](img/8c79fda9-6f9b-4578-877e-779aed81de97.png)] is the *i^(th)* element
    of **w***[t]*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的内存是一个 *N*×*M* 矩阵，记作 [![](img/89ed3e76-7c89-472c-baa9-0719fa281259.png)]，其中
    *t* 是时间步长，*N* 是行数（内存位置），*M* 是每个位置的向量大小。然后，我们有另一个权重向量，**w***[t]*，它决定了要分配给内存中不同位置（即矩阵的行）的注意力。权重向量中的每个
    *N* 个权重都经过标准化，这意味着 [![](img/f0751cb7-2506-4c20-a962-5770f6760dee.png)]，对于所有 *i*，我们有
    [![](img/ff03c942-ed7d-4fd4-98dd-0107e30786ca.png)]，其中 [![](img/8c79fda9-6f9b-4578-877e-779aed81de97.png)]
    是 **w***[t]* 的 *i^(th)* 元素。
- en: 'The read vector, **r***[t]*, which is returned by the head, can be calculated
    as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由头部返回的读取向量，**r***[t]*，可以按如下方式计算：
- en: '![](img/c44483be-ed81-472b-8e8d-00fdf2e411b4.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c44483be-ed81-472b-8e8d-00fdf2e411b4.png)'
- en: Here, [![](img/df41d01e-24c7-462e-b773-70d193858dad.png)] is the *i**^(th)*
    memory vector. From the preceding equation, we can also see that **r***[t]* can be
    differentiated with respect to both the weight and the memory.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/df41d01e-24c7-462e-b773-70d193858dad.png)] 是 *i^(th)* 内存向量。从前面的方程式中，我们还可以看到
    **r***[t]* 可以相对于权重和内存进行求导。
- en: Writing
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 写入
- en: The writing operation takes inspiration from the input and forget gates of LSTMs,
    where some information is erased and then replaced (or added).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 写入操作的灵感来源于LSTM的输入和遗忘门，其中一些信息会被擦除，然后被替换（或添加）。
- en: 'The memory is then updated using two formulas, the first of which erases the
    memory and the second of which adds memory:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，内存通过两个公式进行更新，第一个公式擦除内存，第二个公式添加内存：
- en: '[![](img/822fbdb1-fcc6-4d44-8fe5-ad3d0d8c3e07.png)]'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/822fbdb1-fcc6-4d44-8fe5-ad3d0d8c3e07.png)]'
- en: '[![](img/989ecad8-4f02-44fc-9254-b44a6ec316d2.png)]'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/989ecad8-4f02-44fc-9254-b44a6ec316d2.png)]'
- en: Here, [![](img/7c145547-1bdf-4c9f-bfcd-2e6896726b26.png)] is the erase vector,
    **a***[t]* is the add vector, and **1** is a vector containing only ones. From
    these equations, we can see that the memory at a specific location is only erased
    when both the weight and erase vector are equal to one, and it is left unchanged
    otherwise. Since both the erase and add operations are differentiable, so is the
    entire write operation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， [![](img/7c145547-1bdf-4c9f-bfcd-2e6896726b26.png)] 是擦除向量，**a***[t]* 是加向量，**1** 是一个只包含1的向量。从这些公式中，我们可以看出，特定位置的记忆只有在权重和擦除向量都等于1时才会被擦除，否则保持不变。由于擦除和加操作都是可微的，因此整个写操作也是可微的。
- en: Addressing mechanisms
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定址机制
- en: Now that we know how the reading and writing operations work, let's dive into
    how the weights are produced. The weights are outputs that are a combination of
    two mechanisms—a content-based addressing mechanism and a location-based addressing
    mechanism.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了读写操作是如何进行的，让我们深入了解权重是如何生成的。权重是两个机制的结合输出——内容基定址机制和基于位置的定址机制。
- en: Content-based addressing mechanism
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于内容的定址机制
- en: 'This addressing mechanism focuses on the similarity between the key values, **k***[t]*,
    which are output by the controller based on the input it receives, and the memory
    rows. Based on this similarity, it creates an attention vector. It is calculated
    using the following equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定址机制聚焦于控制器根据接收到的输入输出的键值**k***[t]*与记忆行之间的相似性。基于这种相似性，它生成一个注意力向量。其计算公式如下：
- en: '![](img/b222ee36-4589-454c-82ee-19c6e0154f62.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b222ee36-4589-454c-82ee-19c6e0154f62.png)'
- en: Here, [![](img/1f70300a-f477-43ac-9aea-422931307a42.png)] is a normalized weight
    and *β[t]* is a strength multiplier.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， [![](img/1f70300a-f477-43ac-9aea-422931307a42.png)] 是一个归一化的权重，*β[t]* 是一个强度乘数。
- en: Location-based address mechanism
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于位置的定址机制
- en: 'Before we learn how location-based addressing works, we need to define our
    interpolation gate, which blends together the content-based attention at the current
    time step with the weights in the attention vector from the previous time step.
    This can be done using the following formula:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习位置基定址如何工作的之前，我们需要定义我们的插值门，它将当前时间步的内容基注意力与前一时间步的注意力向量中的权重进行混合。这可以通过以下公式完成：
- en: '![](img/36836a94-5a7c-49ce-8879-58328cb3b19f.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36836a94-5a7c-49ce-8879-58328cb3b19f.png)'
- en: Here, [![](img/faa5bda9-522d-4210-90c3-fdb43444d11d.png)] is the scalar interpolation
    gate.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， [![](img/faa5bda9-522d-4210-90c3-fdb43444d11d.png)] 是标量插值门。
- en: 'Location-based addressing works by summing the values in the attention vector,
    each of which are weighted by a shifting weight, **s***[t]*, which is a distribution
    of the allowable integer shifts. For example, if it can shift between -1 and +1,
    then the allowable shifts that can be performed are -1, 0, and +1\. Now, we can
    formulate this rotation so that the shifting weight applies to [![](img/aacecb17-2a38-4a64-b209-f1af249e8a94.png)] as
    a circular convolution. We can observe this in the following equation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于位置的定址机制通过对注意力向量中的值进行求和来工作，每个值都由一个移动权重**s***[t]*加权，该权重是允许的整数移动分布。例如，如果它可以在-1和+1之间移动，那么可以执行的移动为-1、0和+1。现在，我们可以将这种旋转公式化，使得移动权重作用于[![](img/aacecb17-2a38-4a64-b209-f1af249e8a94.png)] 作为一个循环卷积。我们可以在以下公式中观察到这一点：
- en: '![](img/b8bf7525-a591-4646-b6d4-6ea2f03911a5.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8bf7525-a591-4646-b6d4-6ea2f03911a5.png)'
- en: 'To prevent any leakage or blurriness being caused by the shifting weight, we
    sharpen the attention vector, **w***[t]*, using the following equation:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止由于权重变化而引起的泄漏或模糊，我们使用以下公式来锐化注意力向量，**w***[t]*：
- en: '![](img/68981f9d-1a84-4010-9f72-699945912555.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68981f9d-1a84-4010-9f72-699945912555.png)'
- en: Here, [![](img/3d7b5ad2-d552-41b0-b704-7a70110a8fb0.png)] is a positive scalar
    value.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， [![](img/3d7b5ad2-d552-41b0-b704-7a70110a8fb0.png)] 是一个正的标量值。
- en: And lastly, the values that are output by the controller are unique for each
    read and write head.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，控制器输出的值对于每个读写头来说是唯一的。
- en: Exploring the types of attention
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索注意力机制的类型
- en: Attention has proven to be so effective in machine translation that it has been
    expanded into natural language processing, statistical learning, speech understanding,
    object detection and recognition, image captioning, and visual question answering.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力在机器翻译中已被证明是非常有效的，甚至扩展到自然语言处理、统计学习、语音理解、目标检测与识别、图像描述以及视觉问答等领域。
- en: The purpose of attention is to estimate how correlated (connected) two or more
    elements are to one another.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的目的是估算两个或多个元素之间的相关性（联系）。
- en: 'However, there isn''t just one kind of attention. There are many types, such
    as the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，注意力不仅仅只有一种类型。实际上，有很多种类型，例如以下几种：
- en: '**Self-attention**: Captures the relationship between different positions of
    a sequence of inputs'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自注意力**：捕捉输入序列中不同位置之间的关系'
- en: '**Global or soft attention**: Focuses on the entire sequence of inputs'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局或软注意力**：关注输入序列的整个部分'
- en: '**Local or hard attention**: Focuses on only part of the sequence of inputs'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部或硬注意力**：只关注输入序列的某一部分'
- en: Let's take a look at these in more detail.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下这些内容。
- en: Self-attention
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自注意力
- en: Self-attention finds relationships between different positions of the input
    sequence and then computes a representation of the same sequence of inputs. You
    can think of this as summarizing the input. This is somewhat similar to how the
    LSTM we saw in the previous chapter works, where it tries to learn a correlation
    between the previous inputs and the current one and decide what is relevant and
    what isn't.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力在输入序列的不同位置之间寻找关系，并计算该输入序列的表示。你可以将其视为对输入的总结。这有点类似于我们在上一章看到的LSTM，它尝试学习前一个输入与当前输入之间的相关性，并决定哪些是相关的，哪些是不相关的。
- en: Comparing hard and soft attention
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较硬注意力和软注意力
- en: These types of attention were created for generating captions for images. A
    CNN is first used to extract features and then compress them into an encoding.
    To decode it, an LSTM is used to produce words that describe the image. But that
    isn't important right now—distinguishing between soft and hard attention is.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型的注意力最初是为生成图像的描述性文字而创建的。首先使用CNN提取特征，然后将其压缩成编码。为了对编码进行解码，使用LSTM生成描述图像的词语。但现在我们不需要关注这些内容——区分软注意力和硬注意力才是重点。
- en: In soft attention, the alignment weights that are learned during training are
    softly placed over patches in an image so that it focuses on part(s) of an image
    more than others.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在软注意力中，训练过程中学到的对齐权重被柔和地放置在图像的各个区域上，使得它更关注图像的某些部分。
- en: On the other hand, in hard attention, we focus only on part of the image at
    a time. This only makes a binary decision about where to focus on, and it is much
    harder to train in comparison to soft attention. This is because it is non-differentiable
    and needs to be trained using reinforcement learning. Since reinforcement learning
    is beyond the scope of this book, we won't be covering hard attention.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在硬注意力中，我们每次只专注于图像的某一部分。它只做出一个二元决策，决定关注哪里，而且与软注意力相比，训练起来要难得多。这是因为它是不可微分的，需要通过强化学习来训练。由于强化学习超出了本书的范围，我们将不讨论硬注意力。
- en: Comparing global and local attention
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较全局注意力和局部注意力
- en: Global attention shares some similarities with how soft attention works in that
    it takes all of the input into consideration.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 全局注意力与软注意力的工作原理有些相似，都是考虑了所有输入。
- en: Local attention differs from global attention as it can be seen as a mix of
    hard and soft attention and considers only a subset of the input. It starts by
    predicting a single aligned position for the current output. Then, a window centered
    around the current input is used to create a context vector.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 局部注意力与全局注意力不同，它可以看作是硬注意力和软注意力的混合，只考虑输入的一个子集。它首先预测当前输出的一个对齐位置。然后，围绕当前输入的窗口用于创建上下文向量。
- en: Transformers
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer模型
- en: For those of you who got excited at the title (transformers), this section sadly
    has nothing to do with Optimus Prime or Bumblebee. In all seriousness now, we
    have seen that attention mechanisms work well with architectures such as RNNs
    and CNNs, but they are also powerful enough to be used on their own, as evidenced
    by Vaswani in 2017, in his paper *Attention Is All you Need*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些在看到标题时感到兴奋的朋友（Transformer），很遗憾，本节与擎天柱或大黄蜂无关。说正经的，我们已经看到，注意力机制在RNN和CNN等架构中表现良好，但它们足够强大，可以单独使用，正如Vaswani在2017年发表的论文《Attention
    Is All You Need》中所证明的那样。
- en: The transformer model is made entirely out of self-attention mechanisms to perform
    sequence-to-sequence tasks without the need for any form of recurrent unit. Wait,
    but how? Let's break down the architecture and find out how this is possible.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型完全由自注意力机制构成，能够执行序列到序列的任务，而不需要任何形式的循环单元。等等，怎么做到的呢？让我们拆解一下架构，看看这是怎么可能的。
- en: RNNs take in the encoded input and then decode it in order to map it to a target
    output. However, the transformer differs here by instead treating the encoding
    as a set of key-value pairs (**K**, **V**) which has dimensions (=*n*) equal to
    the length of (the sequence of) the inputs. The decoder is treated as a query, *Q*,
    that has dimensions (=*m*) equal to the length of (the sequence of) the outputs.
    Each output is created by mapping the key-value pair to a query.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: RNN接收编码后的输入，然后解码以映射到目标输出。然而，变换器在这里有所不同，它将编码处理为一组键值对（**K**，**V**），其维度（=*n*）等于输入序列的长度。解码器被视为查询（*Q*），其维度（=*m*）等于输出序列的长度。每个输出是通过将键值对映射到查询来生成的。
- en: 'The attention here is calculated using a scaled dot product that calculates
    the weighted sum of the values using the formula:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的注意力通过一个缩放点积计算，它使用以下公式计算值的加权和：
- en: '![](img/15adfd36-aa62-4691-88c6-2f3fb55816b1.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15adfd36-aa62-4691-88c6-2f3fb55816b1.png)'
- en: 'Now, instead of calculating the attention just once, we do it multiple times
    in parallel. This is referred to as multi-head attention. In the following diagram,
    we can see a visualization of the scaled-dot product attention and multi-head
    attention calculations:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不再只计算一次注意力，而是并行计算多次。这被称为多头注意力。在下图中，我们可以看到缩放点积注意力和多头注意力计算的可视化：
- en: '![](img/ef2a08a1-b527-4b82-9c3e-45482680fffa.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef2a08a1-b527-4b82-9c3e-45482680fffa.png)'
- en: 'The output from each of these attention calculations is then concatenated and
    we apply a linear transformation to them to ensure their dimensions match what
    is expected. The reason for this is that multi-head attention enables the model
    to focus on information from different subspaces at different positions at the
    same time, which single attention is unable to do. This works as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每次注意力计算的输出会被拼接在一起，然后我们对它们应用线性变换，以确保它们的维度与预期匹配。这样做的原因是，单一注意力无法同时集中在不同子空间中的信息，而多头注意力能够做到这一点。这是如何工作的：
- en: '![](img/576b2c53-92df-47f7-99ec-72ab718cee0b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/576b2c53-92df-47f7-99ec-72ab718cee0b.png)'
- en: 'Here, each head is calculated as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个头的计算如下：
- en: '![](img/a9c5f484-f6d4-40f5-9db9-04ab09f37ee2.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9c5f484-f6d4-40f5-9db9-04ab09f37ee2.png)'
- en: Here, [![](img/3c17b956-2a69-4754-9395-12648bb86c05.png)], [![](img/c62b23de-a698-40f1-92ef-b3e231814ff3.png)],
    [![](img/bc0a1890-75a3-4f9d-ab82-003990c33e59.png)], and [![](img/bda05be2-efc2-405e-b383-366cff0cb582.png)] are
    trainable parameters.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/3c17b956-2a69-4754-9395-12648bb86c05.png)]、[![](img/c62b23de-a698-40f1-92ef-b3e231814ff3.png)]、[![](img/bc0a1890-75a3-4f9d-ab82-003990c33e59.png)]和[![](img/bda05be2-efc2-405e-b383-366cff0cb582.png)]是可训练的参数。
- en: Let's now turn our attention to the encoder and decoder.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将注意力转向编码器和解码器。
- en: 'The encoder is composed of a stack of six identical layers, each of which is
    made up of two sub-layers. The first of these sub-layers is a multi-head self-attention
    layer, while the second is an FNN that applies identical weights individually
    to each element in the entire sequence. This is similar to a convolution, which
    would apply the same kernel at each position. The FNN can be represented as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器由六个相同的层堆叠而成，每层由两个子层组成。第一个子层是一个多头自注意力层，而第二个是一个前馈神经网络（FNN），它对整个序列中的每个元素应用相同的权重。这类似于卷积操作，在每个位置应用相同的卷积核。FNN可以表示为：
- en: '![](img/3b931e2a-73e3-4e27-8fb5-2720f129dfd3.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b931e2a-73e3-4e27-8fb5-2720f129dfd3.png)'
- en: 'Each has a residual connection to a layer normalization. What this does is
    identify particular pieces of information in the text/image from the whole, that
    is, the most important parts that we need to pay greater attention to. The computation
    for this encoder is as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每个都有一个残差连接到层归一化。这么做的目的是从整体中识别出文本/图像中的特定信息，也就是说，识别出我们需要更加关注的最重要部分。该编码器的计算如下：
- en: '![](img/2ded83e2-93f0-443d-949c-72393172e3d8.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ded83e2-93f0-443d-949c-72393172e3d8.png)'
- en: 'The encoder architecture looks as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器架构如下所示：
- en: '![](img/4b77edc5-bb87-46be-b191-0a5fa9cf6bfb.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b77edc5-bb87-46be-b191-0a5fa9cf6bfb.png)'
- en: 'The preceding layer normalization transforms the input so that it has zero
    mean and a variance of one. It does this using the following formulas:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的层归一化将输入转换，使其均值为零，方差为一。它使用以下公式来实现：
- en: '[![](img/cccb8ef3-b601-4fae-b7d8-e1e61632c20f.png)]'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/cccb8ef3-b601-4fae-b7d8-e1e61632c20f.png)]'
- en: '[![](img/ffe595af-befe-4705-b4aa-607a95331212.png)]'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/ffe595af-befe-4705-b4aa-607a95331212.png)]'
- en: Here, [![](img/ed7445b2-b7b8-4903-8110-2df9990b9276.png)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/ed7445b2-b7b8-4903-8110-2df9990b9276.png)]。
- en: 'The decoder (whose architecture is shown in the following diagram) is also
    composed of a stack of six identical layers; however, each of these layers is
    made up of three sub-layers. The first two of these sub-layers is a multi-head
    attention layer, and each is followed by a layer normalization, which is where
    the residual connection is. The first of the sub-layers is modified with a mask
    so that the positions don''t go to the subsequent positions and try to use future
    predictions to predict the current one:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器（其架构如以下图所示）也由六个相同的层堆叠而成；然而，这些层中的每一层都由三个子层组成。前两个子层是多头注意力层，每个子层后面跟着一个层归一化，这是残差连接所在的地方。第一个子层被修改为带有掩码，以确保位置不传递到后续的位置，避免使用未来的预测来预测当前的输出：
- en: '![](img/0102dd9b-ba8a-43b9-9da7-cf30151f5a7c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0102dd9b-ba8a-43b9-9da7-cf30151f5a7c.png)'
- en: The output of the decoder is then passed to a linear layer, which is what we
    apply a softmax to.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的输出随后传递给一个线性层，我们在这里应用softmax。
- en: In the architecture, we can quite easily notice that there are no convolutions
    or recurrent connections for the model to make use of in the sequence, which is
    where it sees this information. To deal with this, the authors of this method
    used positional encodings to inject information about the absolute and relative
    positions of the elements in the sequence into the input embeddings of the input
    embeddings, which are at the bottom of the encoder and decoder.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在该架构中，我们可以很容易地注意到，模型在序列中并没有使用卷积或递归连接，这是它看到这些信息的地方。为了解决这个问题，这种方法的作者使用了位置编码，将元素在序列中的绝对和相对位置的信息注入到输入嵌入中，这些嵌入位于编码器和解码器的底部。
- en: 'This gives us the full transformer architecture, which can be seen in the following
    diagram:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了完整的变换器架构，可以在以下图示中看到：
- en: '![](img/e0d59783-c120-47ba-8063-8cd4e283eb99.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0d59783-c120-47ba-8063-8cd4e283eb99.png)'
- en: 'The position encodings are calculated using the following two equations:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码是使用以下两个公式计算的：
- en: '[![](img/2c14a7e8-9ba0-42cf-9cb3-4e59c36b3fe9.png)]'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/2c14a7e8-9ba0-42cf-9cb3-4e59c36b3fe9.png)]'
- en: '[![](img/d39565ad-0607-41f8-b379-d86d677c9120.png)]'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/d39565ad-0607-41f8-b379-d86d677c9120.png)]'
- en: Here, *pos* is the position and *i* is the dimension.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*pos* 是位置，*i* 是维度。
- en: Now, let's conclude this chapter.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结本章内容。
- en: Summary
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about a hot new area in deep learning known as attention
    mechanisms. These are used to allow networks to focus on specific parts of input.
    This helps the network overcome the problem of long-term dependencies. We also
    learned about how these attention mechanisms can be used instead of sequential
    models such as RNNs to produce state-of-the-art results on tasks such as machine
    translation and sentence generation. However, they can also be used to focus on
    relevant parts of images. This can be used for tasks such as visual question answering,
    where we may want our network to tell us what is happening in a given scene.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了深度学习中的一个热门新领域——注意力机制。这些机制用于让网络关注输入的特定部分，从而帮助网络克服长期依赖问题。我们还学习了如何使用这些注意力机制来替代诸如RNN之类的顺序模型，以在机器翻译和句子生成等任务中产生最先进的结果。然而，它们也可以用来关注图像中的相关部分。这可以用于诸如视觉问答的任务，在这种任务中，我们可能希望网络告诉我们给定场景中发生了什么。
- en: In the next chapter, we will learn about generative models.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习生成模型。
