- en: Chapter 9.  News Dictionary and Real-Time Tagging System
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。新闻词典和实时标记系统
- en: While a hierarchical data warehouse stores data in files of folders, a typical
    Hadoop based system relies on a flat architecture to store your data. Without
    proper data governance or a clear understanding of what your data is all about,
    there is an undeniable chance of turning data lakes into swamps, where an interesting
    dataset such as GDELT would be nothing more than a folder containing a vast amount
    of unstructured text files. For that reason, data classification is probably one
    of the most widely used machine learning techniques in large scale organizations
    as it allows users to properly categorize and label their data, publish these
    categories as part of their metadata solutions, and therefore access specific
    information in the most efficient way. Without a proper tagging mechanism executed upfront,
    ideally at ingest, finding all news articles about a specific topic would require
    parsing the entire dataset looking for specific keywords. In this chapter, we
    will be describing an innovative way of labeling incoming GDELT data in a non-supervised
    way and in near real time using both Spark Streaming and the 1% Twitter firehose.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分层数据仓库将数据存储在文件夹中的文件中，但典型的基于Hadoop的系统依赖于扁平架构来存储您的数据。如果没有适当的数据治理或对数据的清晰理解，将数据湖变成沼泽的机会是不可否认的，其中一个有趣的数据集，如GDELT，将不再是一个包含大量非结构化文本文件的文件夹。因此，数据分类可能是大规模组织中最广泛使用的机器学习技术之一，因为它允许用户正确分类和标记其数据，将这些类别作为其元数据解决方案的一部分发布，从而以最有效的方式访问特定信息。如果没有一个在摄入时执行的适当标记机制，理想情况下，找到关于特定主题的所有新闻文章将需要解析整个数据集以寻找特定关键字。在本章中，我们将描述一种创新的方法，以一种非监督的方式和近乎实时地使用Spark
    Streaming和1%的Twitter firehose标记传入的GDELT数据。
- en: 'We will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Bootstrapping a Naive Bayes classifier using Stack Exchange data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Stack Exchange数据引导朴素贝叶斯分类器
- en: Lambda versus Kappa architecture for real-time streaming applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lambda与Kappa架构用于实时流应用程序
- en: Kafka and Twitter4J within a Spark Streaming application
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark Streaming应用程序中使用Kafka和Twitter4J
- en: Thread safety when deploying models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署模型时的线程安全性
- en: Using Elasticsearch as a caching layer
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Elasticsearch作为缓存层
- en: The mechanical Turk
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机械土耳其人
- en: Data classification is a supervised learning technique. This means that you
    can only predict the labels and categories you have learned from a training dataset.
    Because the latter has to be properly labeled, this becomes the main challenge
    which we will be addressing in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分类是一种监督学习技术。这意味着您只能预测您从训练数据集中学到的标签和类别。因为后者必须被正确标记，这成为我们将在本章中解决的主要挑战。
- en: Human intelligence tasks
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类智能任务
- en: None of our data, within the context of news articles, has been properly labeled
    upfront; there is strictly nothing we can learn out of it. Common sense for data
    scientists is to start labeling some input records manually, records that will
    serve as a training dataset. However, because the number of classes may be relatively
    large, at least in our case (hundreds of labels), the amount of data to label
    could be significant (thousands of articles) and would require tremendous effort.
    A first solution is to outsource this laborious task to a "Mechanical Turk", the
    term being used as reference to one of the most famous hoaxes in history where
    an *automated* chess player fooled most of the world leaders ([https://en.wikipedia.org/wiki/The_Turk](https://en.wikipedia.org/wiki/The_Turk)).
    This commonly describes a process that can be done by a machine, but in reality
    it is done by a hidden person, hence a Human Intelligence Task.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在新闻文章的背景下，我们的数据没有得到适当的标记；我们无法从中学到任何东西。数据科学家的常识是手动开始标记一些输入记录，这些记录将作为训练数据集。然而，因为类别的数量可能相对较大，至少在我们的情况下（数百个标签），需要标记的数据量可能相当大（数千篇文章），并且需要巨大的努力。一个解决方案是将这项繁琐的任务外包给“机械土耳其人”，这个术语被用来指代历史上最著名的骗局之一，一个*自动*国际象棋选手愚弄了世界上大多数领导人（[https://en.wikipedia.org/wiki/The_Turk](https://en.wikipedia.org/wiki/The_Turk)）。这通常描述了一个可以由机器完成的过程，但实际上是由一个隐藏的人完成的，因此是一个人类智能任务。
- en: For the readers information, a Mechanical Turk initiative has been started at
    Amazon ([https://www.mturk.com/mturk/welcome](https://www.mturk.com/mturk/welcome)),
    where individuals can register to perform human intelligence tasks such as labeling
    input data or detecting sentiment of a text content. Crowdsourcing this task could
    be one viable solution assuming you can share this internal (and potentially confidential)
    dataset to a third party. An alternative solution described here is to bootstrap
    a classification model using a pre-existing labeled dataset.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于读者的信息，亚马逊已经启动了一个机械土耳其人计划（[https://www.mturk.com/mturk/welcome](https://www.mturk.com/mturk/welcome)），个人可以注册执行人类智能任务，如标记输入数据或检测文本内容的情感。众包这项任务可能是一个可行的解决方案，假设您可以将这个内部（可能是机密的）数据集分享给第三方。这里描述的另一种解决方案是使用预先存在的标记数据集引导分类模型。
- en: Bootstrapping a classification model
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引导分类模型
- en: A text classification algorithm usually learns from term frequencies vectors;
    a possible approach is to train a model using external resources with a similar
    context. For instance, one could classify unlabeled IT related content using categories
    learned from a full dump of the Stack Overflow website. Because Stack Exchange
    is not only reserved for IT professionals, one could find various datasets in
    many different contexts that would serve many purposes ([https://archive.org/download/stackexchange](https://archive.org/download/stackexchange)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类算法通常从术语频率向量中学习；一种可能的方法是使用具有类似上下文的外部资源训练模型。例如，可以使用从Stack Overflow网站的完整转储中学到的类别对未标记的IT相关内容进行分类。因为Stack
    Exchange不仅仅是为IT专业人士保留的，人们可以在许多不同的上下文中找到各种数据集，这些数据集可以服务于许多目的（[https://archive.org/download/stackexchange](https://archive.org/download/stackexchange)）。
- en: Learning from Stack Exchange
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从Stack Exchange学习
- en: 'We will demonstrate here how to bootstrap a simple Naive Bayes classification
    model using the home brewing related dataset from the Stack Exchange website:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里演示如何使用来自Stack Exchange网站的与家酿啤酒相关的数据集来引导一个简单的朴素贝叶斯分类模型：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We create a few methods that pull both the body and labels from all XML documents,
    extract the clean text content out of the HTML encoded body (using the Goose scraper
    introduced in [Chapter 6](ch06.xhtml "Chapter 6. Scraping Link-Based External
    Data"), *Scraping Link-Based External Data*) and finally convert our RDD of XML
    documents into a Spark DataFrame. The different methods are not reported here,
    but they can be found in our code repository. One needs to note that Goose scraper
    can be used offline by providing the HTML content (as a string) alongside a dummy
    URL.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一些方法，从所有XML文档中提取正文和标签，从HTML编码的正文中提取干净的文本内容（使用[第6章](ch06.xhtml "第6章.基于链接的外部数据抓取")中介绍的Goose抓取器，*基于链接的外部数据抓取*），最后将我们的XML文档RDD转换为Spark
    DataFrame。这里没有报告不同的方法，但它们可以在我们的代码库中找到。需要注意的是，Goose抓取器可以通过提供HTML内容（作为字符串）和一个虚拟URL来离线使用。
- en: 'We provide the reader with a convenient `parse` method that can be used for
    pre-processing any `Post.xml` data from the Stack Exchange website. This function
    is part of our `StackBootstraping` code available in our code repository:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个方便的`parse`方法，可用于预处理来自Stack Exchange网站的任何`Post.xml`数据。这个函数是我们的`StackBootstraping`代码的一部分，可以在我们的代码库中找到：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Building text features
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建文本特征
- en: 'With our beer content properly labeled, the remaining process is to bootstrap
    the algorithm itself. For that purpose, we use a simple Naive Bayes classification
    algorithm that determines the conditional probability of a label given an item''s
    features. We first collect all distinct labels, assign a unique identifier (as
    `Double`), and broadcast our label dictionary to the Spark executors:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有了正确标记的啤酒内容，剩下的过程就是引导算法本身。为此，我们使用一个简单的朴素贝叶斯分类算法，确定给定项目特征的标签的条件概率。我们首先收集所有不同的标签，分配一个唯一的标识符（作为`Double`），并将我们的标签字典广播到Spark执行器：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: As introduced earlier, make sure that large collections that are used inside
    a Spark transformation have been broadcast to all Spark executors. This reduces
    the cost associated to network transfer.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，请确保在Spark转换中使用的大型集合已广播到所有Spark执行器。这将减少与网络传输相关的成本。
- en: A `LabeledPoint` is composed of both a label (as `Double`) and features (as
    `Vector`). A common practice to build features out of text content is to build
    term frequency vectors, where each word across all documents corresponds to a
    specific dimension. With around hundreds of thousands of dimensions (the estimated
    number of words in English is 1,025,109), this highly dimensional space will be
    particularly inefficient for most machine learning algorithms. In fact, when Naive
    Bayes multiplies probabilities (lower than 1), there is a certain risk of reaching
    0 due to machine precision issue (numerical underflow as described in [Chapter
    14](ch14.xhtml "Chapter 14. Scalable Algorithms"), *Scalable Algorithm*). Data
    scientists overcome that constraint using the principle of dimensionality reduction,
    projecting a sparse vector into a denser space while preserving distance measures
    (the principle of dimensionality reduction will be covered in [Chapter 10](ch10.xhtml
    "Chapter 10. Story De-duplication and Mutation"), *Story De-duplication and Mutation*).
    Although we can find many algorithms and techniques for that purpose, we will use
    the hashing utility provided by Spark.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`LabeledPoint`由标签（作为`Double`）和特征（作为`Vector`）组成。构建文本内容特征的常见做法是构建词项频率向量，其中每个单词在所有文档中对应一个特定的维度。在英语中大约有数十万个维度（英语单词估计数量为1,025,109），这种高维空间对于大多数机器学习算法来说将特别低效。事实上，当朴素贝叶斯算法计算概率（小于1）时，由于机器精度问题（如[第14章](ch14.xhtml
    "第14章.可扩展算法")中描述的数值下溢，*可扩展算法*），存在达到0的风险。数据科学家通过使用降维原理来克服这一限制，将稀疏向量投影到更密集的空间中，同时保持距离度量（降维原理将在[第10章](ch10.xhtml
    "第10章.故事去重和变异")中介绍，*故事去重和变异*）。尽管我们可以找到许多用于此目的的算法和技术，但我们将使用Spark提供的哈希工具。'
- en: With a vector size of *n* (default of 2^(20)), its `transform` method groups
    all words in *n* different buckets in respect to their hash values, and sums up
    the bucket frequencies to build denser vectors.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在* n *（默认为2^(20)）的向量大小下，其`transform`方法将所有单词分组到* n *个不同的桶中，根据它们的哈希值对桶频率进行求和以构建更密集的向量。
- en: 'Prior to a dimensionality reduction, which can be an expensive operation, vector
    size can be greatly reduced by stemming and cleaning the text content. We use
    the Apache Lucene analyzer here:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行昂贵的降维操作之前，可以通过对文本内容进行词干处理和清理来大大减少向量大小。我们在这里使用Apache Lucene分析器：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We remove all punctuation and numbers and feed the plain text object to a Lucene
    analyzer, collecting each clean word as a `CharTermAttribute`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们去除所有标点和数字，并将纯文本对象提供给Lucene分析器，将每个干净的单词收集为`CharTermAttribute`：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With this approach, we transform the text [Mastering Spark for Data Science
    - V1] into [master spark data science], hence reducing the number of words (therefore
    dimensions) from our input vectors. Finally, we normalize our term frequency vector
    using the MLlib `normalizer` class:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法，我们将文本[Mastering Spark for Data Science - V1]转换为[master spark data science]，从而减少了输入向量中的单词数量（因此减少了维度）。最后，我们使用MLlib的`normalizer`类来规范化我们的词项频率向量：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Tip
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Hash functions can lead to dramatic overestimates due to collisions (two different
    words of complete different meanings could share a same hash value). We will be
    discussing the Random Indexing technique in [Chapter 10](ch10.xhtml "Chapter 10. Story
    De-duplication and Mutation"), *Story De-duplication and Mutation*, in order to
    limit the number of collisions while preserving the distance measure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希函数可能会导致由于碰撞而产生严重的高估（两个完全不同含义的单词可能共享相同的哈希值）。我们将在[第10章](ch10.xhtml "第10章.故事去重和变异")中讨论随机索引技术，以限制碰撞的数量同时保持距离度量。
- en: Training a Naive Bayes model
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练朴素贝叶斯模型
- en: 'We train a Naive Bayes algorithm as follows and test our classifier using a
    test dataset that we did not include in the training data points. We finally display
    the first five predictions in the following example. The labels on the left-hand
    side are the original labels from our test content; on the right-hand side are
    the results of the Naive Bayes classification. An `ipa` has been predicted as
    `hangover`, validating with certainty the accuracy of our classification algorithm:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下方式训练朴素贝叶斯算法，并使用我们没有包含在训练数据点中的测试数据集测试我们的分类器。最后，在下面的例子中显示了前五个预测。左侧的标签是我们测试内容的原始标签；右侧是朴素贝叶斯分类的结果。`ipa`被预测为`hangover`，从而确证了我们分类算法的准确性：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For convenience, we abstract all these methods and expose the following ones
    within a `Classifier` object that will be used later:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们将所有这些方法抽象出来，并在稍后将使用的`Classifier`对象中公开以下方法：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We have demonstrated how to export labeled data from external sources, how
    to build a term frequency vector, and how to train a simple Naive Bayes classification
    model. The high level workflow used here is represented in the following figure
    and is common for most classification use cases:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经演示了如何从外部来源导出标记数据，如何构建词项频率向量，以及如何训练一个简单的朴素贝叶斯分类模型。这里使用的高级工作流程如下图所示，对于大多数分类用例来说都是通用的：
- en: '![Training a Naive Bayes model](img/image_09_001.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![训练朴素贝叶斯模型](img/image_09_001.jpg)'
- en: 'Figure 1: Classification workflow'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：分类工作流程
- en: The next step is to start classifying the original unlabeled data (assuming
    our content is still brewery related). This closes the introduction of Naive Bayes
    classification and how a bootstrapped model could steal ground truth from external
    resources. Both these techniques will be used in our classification system in
    the following section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是开始对原始未标记数据进行分类（假设我们的内容仍然与酿酒有关）。这结束了朴素贝叶斯分类的介绍，以及一个自举模型如何从外部资源中获取真实信息。这两种技术将在以下部分中用于我们的分类系统。
- en: Laziness, impatience, and hubris
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 懒惰，急躁和傲慢
- en: Here comes the second of our main challenges that we will be facing within our
    context of news articles. Assuming someone spent days manually labeling data,
    this would solve our classification problem for known categories at a particular
    point in time, and would probably only be valid when back-testing our data. Who
    knows what the news headline would be on tomorrow's newspaper; no one can define
    all the fine-grained labels and topics that will be covered in the near future
    (although broader categories can still be defined). This would require lots of
    effort to constantly re-evaluate, retrain and redeploy our model whenever a new
    trending topic arises. As a concrete example, no one was talking about the topic
    of Brexit a year ago; this topic is now heavily mentioned in news articles.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们在新闻文章环境中将面临的第二个主要挑战。假设有人花了几天时间手动标记数据，这将解决我们已知类别的分类问题，可能只在回测我们的数据时有效。谁知道明天报纸的新闻标题会是什么；没有人能定义将来将涵盖的所有细粒度标签和主题（尽管仍然可以定义更广泛的类别）。这将需要大量的努力来不断重新评估、重新训练和重新部署我们的模型，每当出现新的热门话题时。具体来说，一年前没有人谈论“脱欧”这个话题；现在这个话题在新闻文章中被大量提及。
- en: 'From our experience, data scientists should bear in mind a famous quote from
    Larry Wall, inventor of the Perl programming language:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，数据科学家应该记住Perl编程语言的发明者Larry Wall的一句名言：
- en: '*"We will encourage you to develop the three great virtues of a programmer,
    laziness, impatience and hubris".*'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们将鼓励您培养程序员的三大美德，懒惰、急躁和傲慢”。
- en: '*Laziness* makes you go to great efforts to reduce overall energy expenditure'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*懒惰*会让你付出巨大的努力来减少总体能量消耗'
- en: '*Impatience* makes you write programs that don''t just react to your needs but
    anticipates them'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*急躁*会让你编写不仅仅是满足你需求的程序，而是能够预测你的需求'
- en: '*Hubris* makes you write programs that people won''t want to say bad things
    about'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*傲慢*会让你编写程序，别人不愿意说坏话'
- en: We want to avoid efforts related to both the preparation and the maintenance
    of a classification model (laziness) and to programmatically anticipate the arising
    of new topics (impatience), though this could sound like an ambitious task (but
    what is hubris if not an excessive pride in achieving the impossible?). Social
    networks are a fantastic place to steal ground truth from. In fact, when people
    tweet news articles, they unconsciously help us label our data. We do not need
    to pay for Mechanical Turks when we potentially have millions of users doing the
    job for us. In other terms, we crowdsource the labeling of GDELT data to Twitter
    users.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望避免与分类模型的准备和维护相关的努力（懒惰），并在程序上预测新主题的出现（急躁），尽管这听起来可能是一个雄心勃勃的任务（但如果不是对实现不可能的过度自豪，那又是什么呢？）。社交网络是一个从中获取真实信息的绝佳地方。事实上，当人们在Twitter上发布新闻文章时，他们无意中帮助我们标记我们的数据。我们不需要支付机械土耳其人的费用，当我们潜在地有数百万用户为我们做这项工作时。换句话说，我们将GDELT数据的标记外包给Twitter用户。
- en: 'Any article mentioned on Twitter will help us build a term frequency vector
    while the associated hashtags will be used as proper labels. In the following
    example, adorable news about President Obama meeting Prince George wearing a bathrobe
    has been classified as [#Obama] and [#Prince] [http://www.wfmynews2.com/entertainment/adorable-prince-george-misses-bedtime-meets-president-obama/149828772](http://www.wfmynews2.com/entertainment/adorable-prince-george-misses-bedtime-meets-president-obama/149828772):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter上提到的任何文章都将帮助我们构建一个词项频率向量，而相关的标签将被用作正确的标签。在下面的例子中，关于奥巴马总统穿着睡袍会见乔治王子的可爱新闻已被分类为[#Obama]和[#Prince]
    [http://www.wfmynews2.com/entertainment/adorable-prince-george-misses-bedtime-meets-president-obama/149828772](http://www.wfmynews2.com/entertainment/adorable-prince-george-misses-bedtime-meets-president-obama/149828772)：
- en: '![Laziness, impatience, and hubris](img/image_09_002.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![懒惰，急躁和傲慢](img/image_09_002.jpg)'
- en: 'Figure 2: President Obama meets Prince George, #Obama, #Prince'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：奥巴马总统会见乔治王子，#Obama，#Prince
- en: 'In the following example, we pay tribute to all of music''s great losses of
    2016 by machine learning topics [#DavidBowie], [#Prince], [#GeorgeMichael], and
    [#LeonardCohen] within the same news article from The Guardian [https://www.theguardian.com/music/2016/dec/29/death-stars-musics-greatest-losses-of-2016](https://www.theguardian.com/music/2016/dec/29/death-stars-musics-greatest-losses-of-2016):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们通过机器学习主题[#DavidBowie]，[#Prince]，[#GeorgeMichael]和[#LeonardCohen]来向2016年音乐界的所有巨大损失致敬，这些主题都在同一篇来自《卫报》的新闻文章中（https://www.theguardian.com/music/2016/dec/29/death-stars-musics-greatest-losses-of-2016）：
- en: '![Laziness, impatience, and hubris](img/image_09_003.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![懒惰、急躁和傲慢](img/image_09_003.jpg)'
- en: 'Figure 3: Music''s great losses in 2016 - source'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：2016年音乐界的巨大损失-来源
- en: Using this approach, our algorithm will be constantly and automatically re-evaluated,
    learning from arising topics on its own, hence working in a non-supervised way
    (although being a supervised learning algorithm in the proper sense).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我们的算法将不断自动重新评估，从而自行学习出现的主题，因此以一种非监督的方式工作（尽管在适当意义上是一种监督学习算法）。
- en: Designing a Spark Streaming application
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计Spark Streaming应用程序
- en: Building a real-time application differs from batch processing in terms of architecture
    and components involved. While the latter can easily be built bottom-up, where
    programmers add functionalities and components when needed, the former usually
    needs to be built top-down with a solid architecture in place. In fact, due to
    the constraints of volume and velocity (or veracity in a streaming context), an
    inadequate architecture will prevent programmers from adding new functionalities.
    One always needs a clear understanding of how streams of data are interconnected,
    how and where they are processed, cached, and retrieved.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 构建实时应用程序在架构和涉及的组件方面与批处理处理有所不同。后者可以轻松地自下而上构建，程序员在需要时添加功能和组件，而前者通常需要在一个稳固的架构基础上自上而下构建。事实上，由于数据量和速度（或在流处理上下文中的真实性）的限制，不恰当的架构将阻止程序员添加新功能。人们总是需要清楚地了解数据流如何相互连接，以及它们是如何被处理、缓存和检索的。
- en: A tale of two architectures
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两种架构的故事
- en: 'In terms of stream processing using Apache Spark, there are two emerging architectures
    that should be considered: Lambda architecture and Kappa architecture. Before
    we delve into the details of the two architectures, let''s discuss the problems
    they are trying to solve, what they have in common, and in what context you would
    use each.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Apache Spark进行流处理方面，有两种新兴的架构需要考虑：Lambda架构和Kappa架构。在深入讨论这两种架构的细节之前，让我们讨论它们试图解决的问题，它们有什么共同之处，以及在什么情况下使用每种架构。
- en: The CAP theorem
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CAP定理
- en: 'For years, engineers working on highly-distributed systems have been concerned
    with handling network outages. The following is a scenario of particular interest,
    consider:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，处理网络中断一直是高度分布式系统的工程师们关注的问题。以下是一个特别感兴趣的情景，请考虑：
- en: '![The CAP theorem](img/image_09_004.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![CAP定理](img/image_09_004.jpg)'
- en: 'Figure 4: Distributed system outage'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：分布式系统故障
- en: 'Normal operation of a typical distributed system is where users perform actions
    and the system uses techniques, such as replication, caching, and indexing, to
    ensure correctness and timely response. But what happens when something goes wrong:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 典型分布式系统的正常运行是用户执行操作，系统使用复制、缓存和索引等技术来确保正确性和及时响应。但当出现问题时会发生什么：
- en: '![The CAP theorem](img/image_09_005.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![CAP定理](img/image_09_005.jpg)'
- en: 'Figure 5: Distributed system split brain syndrome'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：分布式系统的分裂大脑综合症
- en: Here, a network outage has effectively prevented users from performing their
    actions safely. Yes, a simple network failure causes a complication that not only
    affects the function and performance as you might expect but also the correctness
    of the system.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，网络中断实际上阻止了用户安全地执行他们的操作。是的，一个简单的网络故障引起了一个并不仅仅影响功能和性能的复杂情况，正如你可能期望的那样，还影响了系统的正确性。
- en: In fact, the system now suffers from what is known as *split brain syndrome*.
    In this situation, the two parts of the system are no longer able to talk to each
    other, so any modifications performed by users on one side are not visible on
    the opposite side. It's almost like there are two separate systems, each maintaining
    their own internal state, which would become quite different over time. Crucially,
    a user may report different answers when running the same queries on either side.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，系统现在遭受了所谓的*分裂大脑综合症*。在这种情况下，系统的两个部分不再能够相互通信，因此用户在一侧进行的任何修改在另一侧是不可见的。这几乎就像有两个独立的系统，每个系统都维护着自己的内部状态，随着时间的推移会变得截然不同。至关重要的是，用户在任一侧运行相同查询时可能会报告不同的答案。
- en: 'This is but one example in the general case of failure within a distributed
    system, and although much time has been devoted to solving these problems, there
    are still only three practical approaches:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是分布式系统中失败的一般情况之一，尽管已经花费了大量时间来解决这些问题，但仍然只有三种实际的方法：
- en: Prevent users from making any updates until the underlying problem is resolved
    and in the meantime preserve the current state of the system (last known state
    before failure) as correct (that is, sacrifice *partition tolerance*).
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基础问题得到解决之前，阻止用户进行任何更新，并同时保留系统的当前状态（故障前的最后已知状态）作为正确的（即牺牲*分区容忍性*）。
- en: Allow users to continue doing updates as before, but accept that answers may
    be different and will have to converge at some point when the underlying problem
    is corrected (that is, sacrifice *consistency*).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 允许用户继续进行更新，但要接受答案可能不同，并且在基础问题得到纠正时必须收敛（即牺牲*一致性*）。
- en: Shift all users onto one part of the system and allow them to continue doing
    updates as before. The other part of the system is treated as failed and a partial
    reduction of processing power is accepted until the problem is resolved - the
    system may become less responsive as a result (that is, sacrifice *Availability*).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有用户转移到系统的一部分，并允许他们继续进行更新。系统的另一部分被视为失败，并接受部分处理能力的降低，直到问题解决为止 - 系统可能因此变得不太响应（即牺牲*可用性*）。
- en: The preceding conjuncture is more formally stated as CAP theorem ([http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html](http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html)).
    It reasons that in an environment where failures are a fact of life and you cannot
    sacrifice functionality (1) you must choose between having consistent answers
    (2) or full capability (3). You cannot have both as it's a trade-off.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的结论更正式地陈述为CAP定理（[http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html](http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html)）。它认为在一个故障是生活中的事实且你不能牺牲功能性的环境中（1），你必须在一致的答案（2）和完整的功能性（3）之间做出选择。你不能两者兼得，因为这是一种权衡。
- en: Tip
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In fact, it's more correct here to describe "failures" as the more general term,
    "partition tolerance", as this type of failure could refer to any division of
    the system - a network outage, server reboot, a full disk, and so on - it is not
    necessarily specifically network problems.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，更正确的描述是将“故障”描述为更一般的术语“分区容错”，因为这种类型的故障可能指的是系统的任何分割 - 网络中断、服务器重启、磁盘已满等 - 它不一定是特定的网络问题。
- en: Needless to say this is a simplification, but nonetheless, most data processing
    systems will fit into one of these broad categories in the event of a failure.
    Furthermore, it turns out that most traditional database systems favor consistency,
    achieving this using well-understood computer science methods such as transactions,
    write-ahead logs, and pessimistic locking.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，这是一种简化，但尽管如此，在故障发生时，大多数数据处理系统都会属于这些广泛的类别之一。此外，事实证明，大多数传统数据库系统都倾向于一致性，通过使用众所周知的计算机科学方法来实现，如事务、预写日志和悲观锁定。
- en: However, in today's online world, where users expect 24/7 access to services,
    many of which are revenue-generating; Internet of Things or real-time decision
    making, a scalable fault-tolerant approach is required. Consequently, there has
    been a surge in efforts to produce alternatives that ensure availability in the
    event of failure (indeed the Internet itself was born from this very need).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在今天的在线世界中，用户期望全天候访问服务，其中许多服务都是收入来源；物联网或实时决策，需要一种可扩展的容错方法。因此，人们努力寻求确保在故障发生时可用性的替代方案的努力激增（事实上，互联网本身就是出于这种需求而诞生的）。
- en: It turns out that striking the right balance between implementing highly-available
    systems that also provide an acceptable level of consistency is a challenge. In
    order to manage the necessary trade-offs, approaches tend to provide weaker definitions
    of consistency, that is, *eventual consistency* where stale data is usually tolerated
    for a short while, and over time the correct data is agreed upon. Yet even with
    this compromise, they still require the use of far more complicated techniques
    hence they are more difficult to build and maintain.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，在实现高可用系统并提供可接受水平一致性之间取得平衡是一种挑战。为了管理必要的权衡，方法往往提供更弱的一致性定义，即*最终一致性*，在这种情况下，通常容忍一段时间的陈旧数据，并且随着时间的推移，正确的数据会得到认可。然而，即使在这种妥协情况下，它们仍然需要使用更复杂的技术，因此更难以构建和维护。
- en: Tip
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: With more onerous implementations, vector-clocks and read-repair are involved
    in order to handle concurrency and prevent data corruption
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在更繁重的实现中，需要使用向量时钟和读修复来处理并发并防止数据损坏。
- en: The Greeks are here to help
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 希腊人在这里可以提供帮助
- en: 'Both Lambda and Kappa architectures provide simpler solutions to the previously
    described problems. They advocate the use of modern big data technologies, such
    as Apache Spark and Apache Kafka as the basis for consistent available processing
    systems, where logic can be developed without the need to reason about failure.
    They are applicable in situations with the following characteristics:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda和Kappa架构都提供了对先前描述的问题更简单的解决方案。它们倡导使用现代大数据技术，如Apache Spark和Apache Kafka作为一致性可用处理系统的基础，逻辑可以在不需要考虑故障的情况下进行开发。它们适用于具有以下特征的情况：
- en: An unbounded, inbound stream of information, potentially from multiple sources
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无限的、入站的信息流，可能来自多个来源
- en: Analytical processing over a very large, cumulative dataset
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对非常大的累积数据集进行分析处理
- en: User queries with time-based guarantees on data consistency
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户查询对数据一致性有时间保证
- en: Zero-tolerance for degradation of performance or downtime
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对性能下降或停机的零容忍
- en: 'Where you have these conditions, you can consider either architecture as a
    general candidate. Each adheres to the following core principles that help simplify
    issues around data consistency, concurrent access, and prevention of data corruption:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有这些条件的情况下，您可以考虑将任一架构作为一般候选。每种架构都遵循以下核心原则，有助于简化数据一致性、并发访问和防止数据损坏的问题：
- en: '**Data immutability**: Data is only ever created or read. It is never updated
    or deleted. Treating data this way greatly simplifies the model required to keep
    your data consistent.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据不可变性：数据只能创建或读取。它永远不会被更新或删除。以这种方式处理数据极大地简化了保持数据一致性所需的模型。
- en: '**Human fault tolerance**: When fixing or upgrading software during the normal
    course of the software development lifecycle, it is often necessary to deploy
    new versions of analytics and replay historical data through the system in order
    to produce revised answers. Indeed, when managing systems dealing directly with
    data of this capability is often critical. The batch layer provides a durable
    store of historical data and hence allows for any mistakes to be recovered.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人为容错**：在软件开发生命周期的正常过程中修复或升级软件时，通常需要部署新版本的分析并通过系统重放历史数据以产生修订答案。事实上，在管理直接处理具有此能力的数据的系统时，这通常是至关重要的。批处理层提供了历史数据的持久存储，因此允许恢复任何错误。'
- en: It's these principles that form the basis of their eventually-consistent solutions
    without the need to worry about complexities such as read-repairs or vector-clocks; they're
    definitely more developer-friendly architectures!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正是这些原则构成了它们最终一致的解决方案的基础，而无需担心诸如读取修复或向量时钟之类的复杂性；它们绝对是更友好的开发人员架构！
- en: So, let's discuss some of the reasons to choose one over the other. Let's first
    consider the Lambda architecture.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们讨论一些选择一个而不是另一个的原因。让我们首先考虑Lambda架构。
- en: Importance of the Lambda architecture
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambda架构的重要性
- en: 'The Lambda architecture, as first proposed by Nathan Marz, typically ls something
    like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda架构，最初由Nathan Marz提出，通常是这样的：
- en: '![Importance of the Lambda architecture](img/image_09_006.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![Lambda架构的重要性](img/image_09_006.jpg)'
- en: 'Figure 6: Lambda architecture'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Lambda架构
- en: 'In essence, data is dual-routed into two layers:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，数据被双路由到两个层：
- en: A **Batch layer** capable of computing a snapshot at a given point in time
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理层**能够在给定时间点计算快照'
- en: A **Real-Time layer** capable of processing incremental changes since the last
    snapshot
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时层**能够处理自上次快照以来的增量更改'
- en: The **Serving layer** is then used to merge these two views of the data together
    producing a single up-to-date version of the truth.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务层**然后用于将这两个数据视图合并在一起，产生一个最新的真相版本。'
- en: 'In addition to the previously described general characteristics, Lambda architecture
    is most suitable when you have either of the following specific conditions:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了先前描述的一般特征之外，Lambda架构在以下特定条件下最适用：
- en: Complex or time-consuming bulk or batch algorithms that have no equivalent or
    alternative incremental iterative algorithm (and approximations are not acceptable)
    so you need a batch layer.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂或耗时的批量或批处理算法，没有等效或替代的增量迭代算法（并且近似值是不可接受的），因此您需要批处理层。
- en: 'Guarantees on data consistency cannot be met by the batch layer alone, regardless
    of parallelism of the system, so you need a real-time layer. For example, you
    have:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论系统的并行性如何，批处理层单独无法满足数据一致性的保证，因此您需要实时层。例如，您有：
- en: Low latency write-reads
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低延迟写入-读取
- en: Arbitrarily wide ranges of data, that is, years
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任意宽范围的数据，即年份
- en: Heavy data skew
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重数据倾斜
- en: 'Where you have either one of these conditions, you should consider using the
    Lambda architecture. However, before going ahead, be aware that it brings with
    it the following qualities that may present challenges:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您具有以下任一条件之一，您应该考虑使用Lambda架构。但是，在继续之前，请注意它带来的可能会带来挑战的以下特性：
- en: 'Two data pipelines: There are separate workflows for batch and stream processing
    and, although where possible you can attempt to reuse core logic and libraries,
    the flows themselves must be managed individually at runtime.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个数据管道：批处理和流处理有单独的工作流程，尽管在可能的情况下可以尝试重用核心逻辑和库，但流程本身必须在运行时单独管理。
- en: 'Complex code maintenance: For all but simple aggregations, the algorithms in
    the batch and real time layers will need to be different. This is particularly
    true for machine learning algorithms, where there is an entire field devoted to
    this study called online machine learning ([https://en.wikipedia.org/wiki/Online_machine_learning](https://en.wikipedia.org/wiki/Online_machine_learning)),
    which can involve implementing incremental iterative algorithms, or approximation
    algorithms, outside of existing frameworks.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的代码维护：除了简单的聚合之外，批处理和实时层中的算法将需要不同。这对于机器学习算法尤其如此，其中有一个专门研究这一领域的领域，称为在线机器学习（[https://en.wikipedia.org/wiki/Online_machine_learning](https://en.wikipedia.org/wiki/Online_machine_learning)），其中可能涉及实现增量迭代算法或近似算法，超出现有框架之外。
- en: 'Increased complexity in the serving layer: Aggregations, unions, and joins
    are necessary in the serving layer in order to merge deltas with aggregations.
    Engineers should be careful that this does not split out into consuming systems.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务层中的复杂性增加：为了将增量与聚合合并，服务层中需要进行聚合、联合和连接。工程师们应该小心，不要将其分解为消费系统。
- en: Despite these challenges, the Lambda architecture is a robust and useful approach
    that has been implemented successfully in many institutions and organizations,
    including Yahoo!, Netflix, and Twitter.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，Lambda架构是一种强大而有用的方法，已经成功地在许多机构和组织中实施，包括Yahoo！、Netflix和Twitter。
- en: Importance of the Kappa architecture
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambda架构的重要性
- en: 'The Kappa architecture takes simplification one step further by putting the
    concept of a *distributed log* at its center. This allows the removal of the batch
    layer altogether and consequently creates a vastly simpler design. There are many
    different implementations of Kappa, but generally it looks like this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Kappa架构通过将*分布式日志*的概念置于中心，进一步简化了概念。这样可以完全删除批处理层，从而创建一个更简单的设计。Kappa有许多不同的实现，但通常看起来是这样的：
- en: '![Importance of the Kappa architecture](img/image_09_007.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Kappa架构的重要性](img/image_09_007.jpg)'
- en: 'Figure 7: Kappa architecture'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：Kappa架构
- en: 'In this architecture, the distributed log essentially provides the characteristics
    of data immutability and re-playability. By introducing the concept of *mutable
    state store* in the processing layer, it unifies the computation model by treating
    all processing as stream processing, even batch, which is considered just a special
    case of stream. Kappa architecture is most suitable when you have either of the
    following specific conditions:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，分布式日志基本上提供了数据不可变性和可重放性的特性。通过在处理层引入*可变状态存储*的概念，它通过将所有处理视为流处理来统一计算模型，即使是批处理，也被视为流的特例。当您具有以下特定条件之一时，Kappa架构最适用：
- en: Guarantees on data consistency can be met using the existing batch algorithm
    by increasing parallelism of the system to reduce latency
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增加系统的并行性来减少延迟，可以满足数据一致性的保证。
- en: Guarantees on data consistency can be met by implementing incremental iterative
    algorithms
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过实现增量迭代算法可以满足数据一致性的保证
- en: 'If either one of these options is viable, then Kappa architecture should provide
    a modern, scalable approach to meet your batch and streaming requirements. However,
    it''s worth considering the constraints and challenges of the technologies chosen
    for any implementation you may decide on. The potential limitations include:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这两种选择中的任何一种是可行的，那么Kappa架构应该提供一种现代、可扩展的方法来满足您的批处理和流处理需求。然而，值得考虑所选择的任何实现的约束和挑战。潜在的限制包括：
- en: 'Exactly-once semantics: Many popular distributed messaging systems, such as
    Apache Kafka, don''t currently support exactly-once message delivery semantics.
    This means that, for now, consuming systems have to deal with receiving data duplicates
    themselves. This is typically done by using checkpoints, unique keys, idempotent
    writes, or other such de-duplication techniques, but it does increase complexity
    and hence makes the solution more difficult to build and maintain.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确一次语义：许多流行的分布式消息系统，如Apache Kafka，目前不支持精确一次消息传递语义。这意味着，目前，消费系统必须处理接收到的数据重复。通常通过使用检查点、唯一键、幂等写入或其他去重技术来完成，但这会增加复杂性，因此使解决方案更难构建和维护。
- en: 'Out of order event handling: Many streaming implementations, such as Apache
    Spark, do not currently support updates ordered by the event time, instead they
    use the processing time, that is, the time the event was first observed by the
    system. Consequently, updates could be received out of order and the system needs
    to be able to handle this. Again, this increases code complexity and makes the
    solution more difficult to build and maintain.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无序事件处理：许多流处理实现，如Apache Spark，目前不支持按事件时间排序的更新，而是使用处理时间，即系统首次观察到事件的时间。因此，更新可能会无序接收，系统需要能够处理这种情况。同样，这会增加代码复杂性，使解决方案更难构建和维护。
- en: 'No strong consistency, that is, linearizability: As all updates are applied
    asynchronously, there are no guarantees that write will take effect immediately
    (although they will be eventually consistent). This means that in some circumstances
    you would not immediately be able to "read your writes".'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有强一致性，即线性一致性：由于所有更新都是异步应用的，不能保证写入会立即生效（尽管它们最终会一致）。这意味着在某些情况下，您可能无法立即“读取您的写入”。
- en: In the next chapter, we will discuss incremental iterative algorithms, how data
    skew or server failures affect consistency, and how the back-pressure features
    in Spark Streaming can help reduce failures. With regards to what has been explained
    in this section, we will build our classification system following a Kappa architecture.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论增量迭代算法，数据倾斜或服务器故障如何影响一致性，以及Spark Streaming中的反压特性如何帮助减少故障。关于本节中所解释的内容，我们将按照Kappa架构构建我们的分类系统。
- en: Consuming data streams
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消费数据流
- en: 'Similar to a batch processing job, we create a new Spark application using
    a `SparkConf` object and a context. In a streaming application, the context is
    created using a batch size parameter that will be used for any incoming stream
    (both GDELT and Twitter layers, part of the same context, will both be tied to
    the same batch size). GDELT data being published every 15 minutes, our batch size
    will be naturally 15 minutes as we want to predict categories in a pseudo real-time
    basis:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与批处理作业类似，我们使用`SparkConf`对象和上下文创建一个新的Spark应用程序。在流处理应用程序中，上下文是使用批处理大小参数创建的，该参数将用于任何传入的流（GDELT和Twitter层，作为同一上下文的一部分，都将绑定到相同的批处理大小）。由于GDELT数据每15分钟发布一次，我们的批处理大小自然将是15分钟，因为我们希望基于伪实时基础预测类别：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Creating a GDELT data stream
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建GDELT数据流
- en: There are many ways of publishing external data into a Spark streaming application.
    One could open a simple socket and start publishing data over the netcat utility, or
    could be streaming data through a Flume agent monitoring an external directory.
    Production systems usually use Kafka as a default broker for both its high throughput
    and its overall reliability (data is replicated over multiple partitions). Surely,
    we could be using the same Apache NiFi stack as described in [Chapter 10](ch10.xhtml
    "Chapter 10. Story De-duplication and Mutation"), *Story De-duplication and Mutation*,
    but we want to describe here a much easier route simply by "piping" articles URLs
    (extracted from GDELT records) into our Spark application through a Kafka topic.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多将外部数据发布到Spark流处理应用程序的方法。可以打开一个简单的套接字并开始通过netcat实用程序发布数据，或者可以通过监视外部目录的Flume代理流式传输数据。生产系统通常使用Kafka作为默认代理，因为它具有高吞吐量和整体可靠性（数据被复制到多个分区）。当然，我们可以使用与[第10章](ch10.xhtml
    "第10章。故事去重和变异")中描述的相同的Apache NiFi堆栈，*故事去重和变异*，但我们想在这里描述一个更简单的路线，即通过Kafka主题将文章URL（从GDELT记录中提取）传送到我们的Spark应用程序中。
- en: Creating a Kafka topic
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Kafka主题
- en: 'Creating a new Kafka topic is quite easy (in a test environment). Extra care
    must be taken on production environments by choosing the right number of partitions
    and replication factors. Also note that a proper zookeeper quorum must be installed
    and configured. We start the Kafka server and create a topic named `gzet`, using
    one partition only and a replication factor of 1:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试环境中创建一个新的Kafka主题非常容易。在生产环境中，必须特别注意选择正确数量的分区和复制因子。还要注意安装和配置适当的zookeeper quorum。我们启动Kafka服务器并创建一个名为`gzet`的主题，只使用一个分区和一个复制因子：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Publishing content to a Kafka topic
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将内容发布到Kafka主题
- en: 'We can feed the Kafka queue by piping content to the `kafka-console-producer`
    utility. We use `awk`, `sort`, and `uniq` commands as we are only interested in
    the distinct URLs from GDELT records (`URL` is the last field of our tab separated
    values, hence the `$NF`):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将内容传送到`kafka-console-producer`实用程序来向Kafka队列提供数据。我们使用`awk`、`sort`和`uniq`命令，因为我们只对GDELT记录中的不同URL感兴趣（`URL`是我们的制表符分隔值的最后一个字段，因此是`$NF`）：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For convenience, we create a simple bash script that listens for new files on
    the GDELT website, downloads and extracts content to a temporary directory, and
    executes the preceding command. The script can be found in our code repository
    (`gdelt-stream.sh`).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们创建了一个简单的bash脚本，用于监听GDELT网站上的新文件，下载和提取内容到临时目录，并执行上述命令。该脚本可以在我们的代码存储库（`gdelt-stream.sh`）中找到。
- en: Consuming Kafka from Spark Streaming
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从Spark Streaming中消费Kafka
- en: 'Kafka is an official source of Spark Streaming, available using the following
    dependency:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是Spark Streaming的官方来源，可使用以下依赖项：
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We define the number of Spark partitions that will be used for processing data
    from the gzet topic (10 here) together with the zookeeper quorum. We return the
    message itself (the URLs piped to our Kafka producer) in order to build our stream
    of article URLs:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义将用于处理来自gzet主题的数据的Spark分区数量（这里是10），以及zookeeper quorum。我们返回消息本身（传送到我们的Kafka生产者的URL），以构建我们的文章URL流：
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Consuming Kafka from Spark Streaming](img/image_09_008.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![从Spark Streaming中消费Kafka](img/image_09_008.jpg)'
- en: 'Figure 8: GDELT online layer'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：GDELT在线层
- en: In the preceding figure we show how GDELT data will be processed in batches
    by listening to a Kafka topic. Each batch will be analyzed and the articles downloaded
    using the HTML parser described in [Chapter 6](ch06.xhtml "Chapter 6. Scraping
    Link-Based External Data"), *Scraping Link-Based External Data*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们展示了GDELT数据将如何通过监听Kafka主题进行批处理。每个批次将被分析，并使用[第6章](ch06.xhtml "第6章。基于链接的外部数据抓取")中描述的HTML解析器*基于链接的外部数据抓取*下载文章。
- en: Creating a Twitter data stream
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Twitter数据流
- en: 'The obvious constraint of using Twitter is the constraint of scale. With over
    500 million tweets a day, our application needs to be written in the most distributed
    and scalable way in order to handle the large amount of input data. Furthermore,
    if only 2% of these tweets contained a reference to an external URL, we would
    still have a million URLs to fetch and analyze per day (in addition to the thousands
    coming from GDELT). Because we do not have a dedicated architecture to handle
    this veracity of data for the purpose of this book, we will be using the 1% firehose
    provided for free by Twitter. One simply needs to register a new application on
    the Twitter website ([https://apps.twitter.com](https://apps.twitter.com)) and
    retrieve both its associated application settings and authorization tokens. Note,
    however, that the Twitter connector is no longer part of core Spark Streaming
    since version `2.0.0`. As part of the Apache Bahir project ([http://bahir.apache.org/](http://bahir.apache.org/)),
    it can be used with the following maven `dependency`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Twitter的明显限制是规模的限制。每天有超过5亿条推文，我们的应用程序需要以最分布式和可扩展的方式编写，以处理大量的输入数据。此外，即使只有2%的推文包含对外部URL的引用，我们每天仍然需要获取和分析100万个URL（除了来自GDELT的数千个URL）。由于我们没有专门的架构来处理这些数据，因此我们将使用Twitter免费提供的1%
    firehose。只需在Twitter网站上注册一个新应用程序（[https://apps.twitter.com](https://apps.twitter.com)），并检索其关联的应用程序设置和授权令牌。但是请注意，自Spark
    Streaming版本`2.0.0`以来，Twitter连接器不再是核心Spark Streaming的一部分。作为Apache Bahir项目的一部分（[http://bahir.apache.org/](http://bahir.apache.org/)），可以使用以下maven`dependency`：
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Because Spark Streaming uses `twitter4j` in the background, the configuration
    is done using the `ConfigurationBuilder` object from the `twitter4j` libraries:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因为Spark Streaming在后台使用`twitter4j`，所以配置是使用`twitter4j`库中的`ConfigurationBuilder`对象完成的：
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We create our data stream by supplying an array of keywords (can be specific
    hashtags). In our case, we want to listen to all 1%, no matter the keywords or
    hashtags used (discovering new hashtags is actually part of our application),
    hence providing an empty array:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过提供一个关键字数组（可以是特定的标签）来创建我们的数据流。在我们的情况下，我们希望收听所有1%，无论使用哪些关键字或标签（发现新标签实际上是我们应用程序的一部分），因此提供一个空数组：
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Processing Twitter data
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理Twitter数据
- en: The second main constraint of using Twitter is the constraint of noise. When
    most classification models are trained against dozens of different classes, we
    will be working against hundreds of thousands of distinct hashtags per day. We
    will be focusing on popular topics only, meaning the trending topics occurring
    within a defined batch window. However, because a 15 minute batch size on Twitter
    will not be sufficient enough to detect trends, we will apply a 24-hour moving
    window where all hashtags will be observed and counted, and where only the most
    popular ones will be kept.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Twitter的第二个主要限制是噪音的限制。当大多数分类模型针对数十个不同的类进行训练时，我们将针对每天数十万个不同的标签进行工作。我们只关注热门话题，即在定义的批处理窗口内发生的热门话题。然而，由于Twitter上的15分钟批处理大小不足以检测趋势，因此我们将应用一个24小时的移动窗口，其中将观察和计数所有标签，并仅保留最受欢迎的标签。
- en: '![Processing Twitter data](img/image_09_009.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![处理Twitter数据](img/image_09_009.jpg)'
- en: 'Figure 9: Twitter online layer, batch and window size'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：Twitter在线层，批处理和窗口大小
- en: Using this approach, we reduce the noise of unpopular hashtags, making our classifier
    much more accurate and scalable, and significantly reducing the number of articles
    to fetch as we only focus on trending URLs mentioned alongside popular topics.
    This allows us to save lots of time and resources spent analyzing irrelevant data
    (with regards to a classification model).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我们减少了不受欢迎标签的噪音，使我们的分类器更加准确和可扩展，并显著减少了要获取的文章数量，因为我们只关注与热门话题一起提及的流行URL。这使我们能够节省大量时间和资源，用于分析与分类模型无关的数据。
- en: Extracting URLs and hashtags
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取URL和标签
- en: 'We extract both the clean hashtags (that are more than x characters long and
    that do not contain numbers; yet another measure of reducing noise) and references
    to valid URLs. Note the Scala `Try` method that catches any exception when testing
    a `URL` object. Only the tweets matching both of these two conditions will be
    kept:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取干净的标签（长度超过x个字符且不包含数字；这是减少噪音的另一种措施）和对有效URL的引用。请注意Scala的`Try`方法，它在测试`URL`对象时捕获任何异常。只有符合这两个条件的推文才会被保留：
- en: '[PRE17]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Keeping popular hashtags
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保留热门标签
- en: 'The basic idea of this step is to execute a simple word count over a 24h time
    window. We extract all hashtags, assign a value of 1, and count the number of
    occurrences using a reduce function. In a streaming context, the `reduceByKey`
    function can be applied over a window (that must be larger than the batch size)
    using `reduceByKeyAndWindow` method. Although this term frequency dictionary will
    always be available at each batch, the current top ten hashtags are printed out
    every 15 minutes, data will be counted over a larger period (24h):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步的基本思想是在24小时的时间窗口内执行一个简单的词频统计。我们提取所有的标签，赋予一个值为1，并使用reduce函数计算出现次数。在流处理上，`reduceByKey`函数可以使用`reduceByKeyAndWindow`方法在一个窗口上应用（必须大于批处理大小）。尽管这个词频字典在每个批处理中都是可用的，但当前的前十个标签每15分钟打印一次，数据将在一个较长的时间段（24小时）内计数：
- en: '[PRE18]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In a batch processing context, one could easily join an RDD of hashtags with
    the Twitter RDD in order to keep only the "hottest" tweets (tweets mentioning
    an article alongside a popular hashtag). In a streaming context, data streams
    cannot be joined as each stream contains several RDDs. Instead, we transform a
    `DStream` with another one using the `transformWith` function that takes an anonymous
    function as an argument and applies it on each of their RDDs. We transform our
    Twitter stream with our hashtag stream by applying a function that filters out
    the unpopular tweets. Note that we use Spark context to broadcast our current
    top *n* hashtags (limited to the top 100 here):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在批处理上下文中，可以轻松地将标签的RDD与Twitter RDD连接，以保留只有“最热门”推文（提及与热门标签一起的文章的推文）。在流处理上下文中，数据流不能连接，因为每个流包含多个RDD。相反，我们使用`transformWith`函数将一个`DStream`与另一个`DStream`进行转换，该函数接受一个匿名函数作为参数，并在它们的每个RDD上应用它。我们通过应用一个过滤不受欢迎推文的函数，将我们的Twitter流与我们的标签流进行转换。请注意，我们使用Spark上下文广播我们当前的前n个标签（在这里限制为前100个）：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Because the returned stream will only contain the "hottest" URLs, the amount
    of data should be drastically reduced. Although we cannot guarantee at this stage
    whether or not the URL points to a proper text content (could be a YouTube video
    or a simple image), at least we know we won't waste effort fetching content about
    useless topics.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因为返回的流将只包含“最热门”的URL，所以数据量应该大大减少。虽然在这个阶段我们无法保证URL是否指向正确的文本内容（可能是YouTube视频或简单的图片），但至少我们知道我们不会浪费精力获取与无用主题相关的内容。
- en: Expanding shortened URLs
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展缩短的URL
- en: 'URLs available on Twitter are shortened. The only way to detect the true source
    programmatically is to "open the box" for all of them, wasting, sadly, lots of
    time and effort on potentially irrelevant content. It is also worth mentioning
    that many web scrapers would not handle shortened URLs efficiently (including
    Goose scraper). We expand URLs by opening an HTTP connection, disabling redirects,
    and looking at the `Location` header. We also provide the method with a list of
    "untrusted" sources, sources that are, for the context of a classification model,
    not providing any useful content (such as videos from [https://www.youtube.com](https://www.youtube.com)):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter上的URL是缩短的。以编程方式检测真实来源的唯一方法是为所有URL“打开盒子”，可悲的是，这浪费了大量的时间和精力，可能是无关紧要的内容。值得一提的是，许多网络爬虫无法有效地处理缩短的URL（包括Goose爬虫）。我们通过打开HTTP连接、禁用重定向并查看`Location`头来扩展URL。我们还为该方法提供了一个“不受信任”的来源列表，这些来源对于分类模型的上下文来说并没有提供任何有用的内容（例如来自[https://www.youtube.com](https://www.youtube.com)的视频）：
- en: '[PRE20]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Tip
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Similar to what has been done in the previous chapter, we thoroughly catch any
    possible exceptions arising from an HTTP connection. Any uncaught exception (could
    be a simple 404 error) would make this task re-evaluate on different Spark executors
    before raising a fatal exception, exiting our Spark application.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一章中所做的类似，我们彻底捕捉由HTTP连接引起的任何可能的异常。任何未捕获的异常（可能是一个简单的404错误）都会使这个任务在引发致命异常之前重新评估不同的Spark执行器，退出我们的Spark应用程序。
- en: Fetching HTML content
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取HTML内容
- en: 'We''ve already introduced web scrapers in a previous chapter, using Goose library
    recompiled for Scala 2.11\. We will create a method that takes a `DStream` as
    input instead of an RDD, and only keep the valid text content with at least 500
    words. We will finally return a stream of text alongside the associated hashtags
    (the popular ones):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在上一章介绍了网络爬虫，使用了为Scala 2.11重新编译的Goose库。我们将创建一个以`DStream`作为输入的方法，而不是RDD，并且只保留至少500个单词的有效文本内容。最后，我们将返回一个文本流以及相关的标签（热门的标签）。
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We apply the same approach for GDELT data where all the content (text, title,
    description, and so on) will also be returned. Note the `reduceByKey` method,
    which acts as a distinct function for our data stream:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对GDELT数据应用相同的方法，其中所有内容（文本、标题、描述等）也将被返回。请注意`reduceByKey`方法，它充当我们数据流的一个不同函数：
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Using Elasticsearch as a caching layer
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Elasticsearch作为缓存层
- en: 'Our ultimate goal is to train a new classifier at each batch (every 15 minutes).
    However, the classifier will be trained using more than just the few records we
    downloaded within that current batch. We somehow have to cache the text content
    over a larger period of time (set to 24h) and retrieve it whenever we need to
    train a new classifier. With Larry Wall''s quote in mind, we will try to be as
    lazy as possible maintaining the data consistency over this online layer. The
    basic idea is to use a **Time to live** (**TTL**) parameter that will seamlessly
    drop any outdated record. The Cassandra database provides this feature out of
    the box (so does HBase or Accumulo), but Elasticsearch is already part of our
    core architecture and can easily be used for that purpose. We will create the
    following mapping for the `gzet`/`twitter` index with the `_ttl` parameter enabled:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终目标是在每个批处理（每15分钟）中训练一个新的分类器。然而，分类器将使用不仅仅是我们在当前批次中下载的少数记录。我们不知何故必须在较长时间内缓存文本内容（设置为24小时），并在需要训练新分类器时检索它。考虑到Larry
    Wall的引用，我们将尽可能懒惰地维护在线层上的数据一致性。基本思想是使用**生存时间**（**TTL**）参数，它将无缝地丢弃任何过时的记录。Cassandra数据库提供了这个功能（HBase或Accumulo也是如此），但Elasticsearch已经是我们核心架构的一部分，可以轻松用于此目的。我们将为`gzet`/`twitter`索引创建以下映射，并启用`_ttl`参数：
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Our records will exist on Elasticsearch for a period of 24h (the TTL value
    is defined on insert) after which any record will simply be discarded. As we delegate
    the maintenance tasks to Elasticsearch, we can safely pull all possible records
    from our online cache without worrying too much about any outdated value. All
    the retrieved data will be used as a training set for our classifier. The high
    level process is reported in the following figure:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的记录将在Elasticsearch上存在24小时（TTL值在插入时定义），之后任何记录将被简单丢弃。由于我们将维护任务委托给Elasticsearch，我们可以安全地从在线缓存中拉取所有可能的记录，而不用太担心任何过时的值。所有检索到的数据将用作我们分类器的训练集。高层过程如下图所示：
- en: '![Using Elasticsearch as a caching layer](img/image_09_010.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![使用Elasticsearch作为缓存层](img/image_09_010.jpg)'
- en: 'Figure 10: Using Elasticsearch as a caching layer'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：使用Elasticsearch作为缓存层
- en: For each RDD in our data stream, we retrieve all existing records from the previous
    24h, cache our current set of Twitter content, and train a new classifier. Converting
    a data stream into RDDs is a simple operation using the `foreachRDD` function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据流中的每个RDD，我们从前24小时中检索所有现有记录，缓存我们当前的Twitter内容，并训练一个新的分类器。将数据流转换为RDD是使用`foreachRDD`函数的简单操作。
- en: 'We persist current records into Elasticsearch using the `saveToEsWithMeta`
    function from the Elasticsearch API. This function accepts the `TTL` parameter
    as part of the metadata map (set to 24h, in seconds, and formatted as String):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Elasticsearch API中的`saveToEsWithMeta`函数将当前记录持久化到Elasticsearch中。此函数接受`TTL`参数作为元数据映射的一部分（设置为24小时，以秒为单位，并格式化为字符串）：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It is worth executing a simple check on Elasticsearch in order to make sure
    that the `TTL` parameter has been properly set, and is effectively decreasing
    every second. Once it has reached 0, the indexed document should be dropped. The
    following simple command prints out the `_ttl` value for document ID [`AVRr9LaCoYjYhZG9lvBl`]
    every second. This uses a simple `jq` utility ([https://stedolan.github.io/jq/download](https://stedolan.github.io/jq/download)/)
    to parse JSON objects from the command line:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 值得在Elasticsearch上执行简单的检查，以确保`TTL`参数已经正确设置，并且每秒都在有效减少。一旦达到0，索引的文档应该被丢弃。以下简单命令每秒打印出文档ID
    [`AVRr9LaCoYjYhZG9lvBl`] 的`_ttl`值。这使用一个简单的`jq`实用程序（[https://stedolan.github.io/jq/download](https://stedolan.github.io/jq/download)/）从命令行解析JSON对象：
- en: '[PRE25]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'All the online records (records with unexpired TTL) can be retrieved into an
    RDD using the following function. Similar to what we''ve done in [Chapter 7](ch07.xhtml
    "Chapter 7. Building Communities"), *Building communities*, extracting lists from
    Elasticsearch is far easier using JSON parsing than Spark DataFrame:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下函数将所有在线记录（具有未过期TTL的记录）检索到RDD中。与我们在[第7章](ch07.xhtml "第7章。构建社区")中所做的类似，*构建社区*，使用JSON解析从Elasticsearch中提取列表比使用Spark
    DataFrame要容易得多：
- en: '[PRE26]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We download all Twitter contents from our caching layer while saving our current
    batch. The remaining process is to train our classification algorithm. This method
    is discussed in the following section:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从缓存层下载所有Twitter内容，同时保存我们当前的批处理。剩下的过程是训练我们的分类算法。这个方法在下一节中讨论：
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Classifying data
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类数据
- en: The remaining part of our application is to start classifying data. As introduced
    earlier, the reason for using Twitter was to steal ground truth from external
    resources. We will train a Naive Bayes classification model using Twitter data
    while predicting categories of the GDELT URLs. The convenient side of using a
    Kappa architecture approach is that we do not have to worry much about exporting
    some common pieces of code across different applications or different environments.
    Even better, we do not have to export/import our model between a batch and a speed
    layer (both GDELT and Twitter, sharing the same Spark context, are part of the
    same physical layer). We could save our model to HDFS for auditing purposes, but
    we simply need to pass a reference to a Scala object between both classes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用的剩余部分是开始对数据进行分类。如前所介绍的，使用Twitter的原因是从外部资源中窃取地面真相。我们将使用Twitter数据训练一个朴素贝叶斯分类模型，同时预测GDELT
    URL的类别。使用Kappa架构方法的便利之处在于，我们不必太担心在不同应用程序或不同环境之间导出一些常见的代码。更好的是，我们不必在批处理层和速度层之间导出/导入我们的模型（GDELT和Twitter，共享相同的Spark上下文，都是同一物理层的一部分）。我们可以将我们的模型保存到HDFS以进行审计，但我们只需要在两个类之间传递一个Scala对象的引用。
- en: Training a Naive Bayes model
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练朴素贝叶斯模型
- en: 'We''ve already introduced both the concept of bootstrapping a Naive Bayes model
    using Stack Exchange datasets and the use of a `Classifier` object that builds
    `LabeledPoints` out of text content. We will create a `ClassifierModel` case class
    that wraps both a Naive Bayes model and its associated labels dictionary and exposes
    both a `predict` and a `save` method:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了使用Stack Exchange数据集引导朴素贝叶斯模型的概念，以及使用`分类器`对象从文本内容构建`LabeledPoints`。我们将创建一个`ClassifierModel`
    case类，它包装了朴素贝叶斯模型及其相关的标签字典，并公开了`predict`和`save`方法：
- en: '[PRE28]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Because more than one hashtag could be necessary to fully describe an article
    content, we will predict instead a probability distribution using the `predictProbabilities`
    function. We convert our label identifier (as `Double`) to the original category
    (as `String`) using the label dictionary we saved alongside the model. Finally
    we can save, for auditing purposes only, both our model and the label dictionary
    into HDFS.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因为可能需要多个标签来完全描述一篇文章的内容，所以我们将使用`predictProbabilities`函数来预测概率分布。我们使用保存在模型旁边的标签字典将我们的标签标识符（作为`Double`）转换为原始类别（作为`String`）。最后，我们可以将我们的模型和标签字典保存到HDFS，仅供审计目的。
- en: Tip
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: All MLlib models support both a save and a load function. Data will be persisted
    as `ObjectFile` in HDFS, and can be easily retrieved and deserialized. Using ML
    library, objects are saved into parquet format. One would need, however, to save
    additional pieces of information; such as in our example, the label dictionary
    used for training that model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所有MLlib模型都支持保存和加载功能。数据将以`ObjectFile`的形式持久化在HDFS中，并且可以轻松地检索和反序列化。使用ML库，对象被保存为parquet格式。然而，需要保存额外的信息；例如在我们的例子中，用于训练该模型的标签字典。
- en: Thread safety
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程安全
- en: 'Our `Classifier` is a singleton object, and, as per the singleton pattern,
    should be thread safe. That means that parallel threads should not modify a same
    state using, for instance, a setter method. In our current architecture, only
    Twitter will be training and updating a new model every 15 minutes, models that
    will be only used by the GDELT service (no concurrent update). However, there
    are two important things to take into consideration:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`分类器`是一个单例对象，根据单例模式，应该是线程安全的。这意味着并行线程不应该使用相同的状态进行修改，例如使用setter方法。在我们当前的架构中，只有Twitter每15分钟训练和更新一个新模型，这些模型将只被GDELT服务使用（没有并发更新）。然而，有两件重要的事情需要考虑：
- en: Firstly, our model has been trained using distinct labels (hashtags found in
    a 24h time window, extracted every 15 minutes). A new model will be trained against
    an updated dictionary. Both the model and the labels are tightly coupled, and
    therefore must be synchronized. In the unlikely event of GDELT pulling labels
    while Twitter is updating a model, our predictions will be inconsistent. We ensure
    thread safety by wrapping both labels and models within our same `ClassifierModel`
    case class.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们的模型是使用不同的标签进行训练的（在24小时时间窗口内找到的标签，每15分钟提取一次）。新模型将根据更新的字典进行训练。模型和标签都是紧密耦合的，因此必须同步。在GDELT在Twitter更新模型时拉取标签的不太可能事件中，我们的预测将是不一致的。我们通过将标签和模型都包装在同一个`ClassifierModel`
    case类中来确保线程安全。
- en: 'The second (although less critical) concern is that our process is parallel.
    That means that similar tasks will be executed simultaneously from different executors,
    on different chunks of data. At a point in time, we would need to ensure that
    all models are the same version on each executor, although predicting a particular
    chunk of data with a slightly less up-to-date model will still technically be
    valid (as long as the model and labels are synchronized). We illustrate this statement
    with the two following examples. The first one cannot guarantee consistency of
    models across executors:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个（虽然不太关键）问题是我们的过程是并行的。这意味着相似的任务将同时从不同的执行器上执行，处理不同的数据块。在某个时间点，我们需要确保每个执行器上的所有模型都是相同版本的，尽管使用略旧的模型预测特定数据块仍然在技术上是有效的（只要模型和标签是同步的）。我们用以下两个例子来说明这个说法。第一个例子无法保证执行器之间模型的一致性：
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The second example (used by default by Spark) broadcasts a model to all executors
    at once, hence guaranteeing the overall consistency of the predicting phase:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个例子（Spark默认使用）将模型广播到所有执行器，从而保证预测阶段的整体一致性：
- en: '[PRE30]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In our `Classifier` singleton object, we define our model as a global variable
    (as optional as it may not exist yet) that will be updated after each call to
    the `train` method:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`分类器`单例对象中，我们将我们的模型定义为一个全局变量（因为它可能还不存在），在每次调用`train`方法后将更新该模型：
- en: '[PRE31]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Coming back to our Twitter stream, for each RDD, we build our training set (abstracted
    within our `Classifier`), train a new model, and then save it to HDFS:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的Twitter流，对于每个RDD，我们构建我们的训练集（在我们的`分类器`中抽象出来），训练一个新模型，然后将其保存到HDFS：
- en: '[PRE32]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Predict the GDELT data
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测GDELT数据
- en: 'Using the `Classifier` singleton object, we can access the latest model published
    from the Twitter processor. For each RDD, for each article, we simply predict
    the hashtags probability distribution that describes each article''s text content:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`分类器`单例对象，我们可以访问Twitter处理器发布的最新模型。对于每个RDD，对于每篇文章，我们只需预测描述每篇文章文本内容的标签概率分布：
- en: '[PRE33]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We only keep probabilities higher than 25% and publish each article together
    with its predicted hashtags into our Elasticsearch cluster. Publishing the results
    officially marks the end of our classification application. We report the full
    architecture here:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只保留高于25%的概率，并将每篇文章与其预测的标签一起发布到我们的Elasticsearch集群。发布结果正式标志着我们分类应用的结束。我们在这里报告完整的架构：
- en: '![Predict the GDELT data](img/image_09_011.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![预测GDELT数据](img/image_09_011.jpg)'
- en: 'Figure 11: An innovative way of tagging news articles'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：标记新闻文章的创新方式
- en: Our Twitter mechanical Turk
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的Twitter机械土耳其
- en: The accuracy of a classification algorithm should be measured against a test
    dataset, meaning a labeled dataset that was not included in the training phase.
    We do not have access to such a dataset (this is the reason we bootstrapped our
    model initially), hence we cannot compare the original versus predicted categories.
    Instead of the true accuracy, we can estimate an overall confidence level by visualizing
    our results. With all our data on Elasticsearch, we build a Kibana dashboard with
    an additional plugin for tag cloud visualizations ([https://github.com/stormpython/tagcloud](https://github.com/stormpython/tagcloud)).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 分类算法的准确性应该根据测试数据集来衡量，即在训练阶段未包含的标记数据集。我们无法访问这样的数据集（这是我们最初引导模型的原因），因此我们无法比较原始与预测的类别。我们可以通过可视化我们的结果来估计整体置信水平，而不是真实的准确性。有了我们在Elasticsearch上的所有数据，我们构建了一个Kibana仪表板，并增加了一个用于标签云可视化的插件（[https://github.com/stormpython/tagcloud](https://github.com/stormpython/tagcloud)）。
- en: The following figure shows the number of GDELT articles that were analyzed and
    predicted on May 1, 2016\. Around 18,000 articles have been downloaded in less
    than 24h (by batch interval of 15 minutes). At each batch, we observe no more
    than 100 distinct predicted hashtags; this is fortunate as we only kept the top
    100 popular hashtags occurring within a 24h time window. Besides, it gives us
    hints about both GDELT and Twitter following a relatively normal distribution
    (batches are not skewed around a particular category).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了在2016年5月1日分析和预测的GDELT文章数量。在不到24小时内下载了大约18000篇文章（每15分钟一个批次）。在每个批次中，我们观察到不超过100个不同的预测标签；这是幸运的，因为我们只保留了在24小时时间窗口内出现的前100个热门标签。此外，它给了我们一些关于GDELT和Twitter遵循相对正常分布的线索（批次不会围绕特定类别偏斜）。
- en: '![Our Twitter mechanical Turk](img/image_09_012.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![我们的Twitter机械土耳其](img/image_09_012.jpg)'
- en: 'Figure 12: Predicted articles on May 1'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：预测文章于5月1日
- en: In addition to these 18,000 articles, we also extracted around 700 Twitter text
    content labeled against our 100 popular hashtags, making each topic covered by
    seven articles on average. Although this training set is already a good start
    for the content of this book, we could probably expand it by being less restrictive
    in terms content or by grouping similar hashtags into broader categories. We could
    also increase the TTL value on Elasticsearch. Increasing the number of observations
    while limiting Twitter noise should definitely improve the overall model accuracy.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这18000篇文章，我们还提取了大约700条Twitter文本内容，标记了我们100个热门标签，平均每个主题被七篇文章覆盖。尽管这个训练集已经是本书内容的一个良好开端，但我们可能可以通过在内容方面放宽限制或将类似的标签分组成更广泛的类别来扩展它。我们还可以增加Elasticsearch上的TTL值。增加观察数量同时限制Twitter噪音肯定会提高整体模型的准确性。
- en: We observe the most popular hashtags in that particular window to be [#mayday]
    and [#trump]. We also observe at least as many [#nevertrump] as [#maga], hence
    satisfying both of the two US political parties. This will be confirmed using
    the US election data in [Chapter 11](ch11.xhtml "Chapter 11. Anomaly Detection
    on Sentiment Analysis"), *Anomaly Detection on Sentiment Analysis*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到在特定时间窗口内最流行的标签是[#mayday]和[#trump]。我们还观察到至少与[#maga]一样多的[#nevertrump]，因此满足了美国两个政党的要求。这将在[第11章](ch11.xhtml
    "第11章 异常检测情感分析")中使用美国选举数据进行确认，*情感分析异常检测*。
- en: 'Finally, we select a particular hashtag and retrieve all its associated keywords.
    This is important as it basically validates the consistency of our classification
    algorithm. Our hope is that for each hashtag coming from Twitter, the significant
    terms from GDELT will be consistent enough and should all be related to the same
    hashtag meaning. We focus on the [**#trump**] tag and access the Trump cloud in
    the following figure:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们选择一个特定的标签并检索其所有相关关键词。这很重要，因为它基本上验证了我们分类算法的一致性。我们希望对于来自Twitter的每个标签，来自GDELT的重要术语足够一致，并且应该都与相同的标签含义相关。我们关注[**#trump**]标签，并在下图中访问特朗普云：
- en: '![Our Twitter mechanical Turk](img/image_09_013.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![我们的Twitter机械土耳其](img/image_09_013.jpg)'
- en: 'Figure 13: The #Trump cloud'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：#特朗普云
- en: We observe most of the significant terms (every article predicted as [**#trump**])
    to be all about the presidential campaign, the United States, primary, and so
    on. It also contains names of candidates running for the presidential (Hillary
    Clinton and Ted Cruz). Although we still find some articles and keywords that
    are not Donald Trump related, this validates a certain consistency to our algorithm.
    For many records (more than 30% of them), the results were even above all our
    initial expectations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到大多数重要术语（每篇文章预测为[**#trump**]）都与总统竞选、美国、初选等有关。它还包含了参加总统竞选的候选人的名字（希拉里·克林顿和特德·克鲁兹）。尽管我们仍然发现一些与唐纳德·特朗普无关的文章和关键词，但这验证了我们算法的一定一致性。对于许多记录（超过30%），结果甚至超出了我们最初的期望。
- en: Summary
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Although we were impressed with many of the overall model consistencies, we
    appreciate that we certainly did not build the most accurate classification system
    ever. Crowd sourcing this task to millions of users was an ambitious task and
    by far not the easiest way of getting clearly defined categories. However, this
    simple proof of concept shows us a few important things:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们对许多整体模型的一致性印象深刻，但我们意识到我们肯定没有构建最准确的分类系统。将这项任务交给数百万用户是一项雄心勃勃的任务，绝对不是获得明确定义的类别的最简单方式。然而，这个简单的概念验证向我们展示了一些重要的东西：
- en: It technically validates our Spark Streaming architecture.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在技术上验证了我们的Spark Streaming架构。
- en: It validates our assumption of bootstrapping GDELT using an external dataset.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它验证了我们使用外部数据集引导GDELT的假设。
- en: It made us lazy, impatient, and proud.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它让我们变得懒惰、不耐烦和骄傲。
- en: It learns without any supervision and eventually gets better at every batch.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在没有任何监督的情况下学习，并且在每个批次中最终变得更好。
- en: 'No data scientist can build a fully functional and highly accurate classification
    system in just a few weeks, especially not on dynamic data; a proper classifier
    needs to be evaluated, trained, re-evaluated, tuned, and retrained for at least
    the first few months, and then re-evaluated every half a year at the very least.
    Our goal here was to describe the components involved in a real-time machine learning
    application and to help data scientists sharpen their creative minds (out-of-the-box
    thinking is the #1 virtue of a modern data scientist).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 没有数据科学家可以在短短几周内构建一个完全功能且高度准确的分类系统，尤其是在动态数据上；一个合适的分类器需要至少在最初的几个月内进行评估、训练、重新评估、调整和重新训练，然后至少每半年进行一次重新评估。我们的目标是描述实时机器学习应用中涉及的组件，并帮助数据科学家锐化他们的创造力（跳出常规思维是现代数据科学家的首要美德）。
- en: In the next chapter, we will be focusing on article mutation and story de-duplication;
    how likely is a topic to evolve over time, how likely is a clique of people (or
    community) likely to mutate over time? By de-duplicating articles into stories,
    stories into epics, can we predict the possible outcomes based on previous observations?
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将专注于文章变异和故事去重；一个话题随着时间的推移有多大可能会发展，一个人群（或社区）有多大可能会随时间变异？通过将文章去重为故事，故事去重为史诗，我们能否根据先前的观察来预测可能的结果？
