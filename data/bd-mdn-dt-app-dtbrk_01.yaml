- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: An Introduction to Delta Live Tables
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Delta Live Tables 简介
- en: In this chapter, we will examine how the data industry has evolved over the
    last several decades. We’ll also look at why real-time data processing has significant
    ties to how a business can react to the latest signals in data. We’ll address
    why trying to build your own streaming solution from scratch may not be sustainable,
    and why the maintenance does not easily scale over time. By the end of the chapter,
    you should completely understand the types of problems the **Delta Live Tables**
    ( **DLT** ) framework solves and the value the framework brings to data engineering
    teams.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨过去几十年来数据行业如何发展。我们还将讨论为何实时数据处理与企业如何应对数据中的最新信号有着密切关系。我们将说明为什么从零开始构建自己的流处理解决方案可能无法持续，并且为什么维护随着时间推移并不容易扩展。到本章结束时，你应该完全理解
    **Delta Live Tables**（**DLT**）框架解决的问题类型，以及该框架为数据工程团队带来的价值。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将覆盖以下主要主题：
- en: The emergence of the l akehouse
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 湖仓的出现
- en: The importance of real-time data in the l akehouse
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时数据在湖仓中的重要性
- en: The maintenance predicament of a streaming application
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流应用程序的维护困境
- en: What is the Delta Live Tables framework?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Delta Live Tables 框架？
- en: How are Delta Live Tables related to Delta Lake?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Live Tables 与 Delta Lake 有何关系？
- en: An introduction to Delta Live Tables concepts
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Live Tables 概念介绍
- en: A quick Delta Lake primer
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Lake 快速入门
- en: A hands-on example – creating your first Delta Live Tables pipeline
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实战示例 – 创建你的第一个 Delta Live Tables 管道
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: It’s recommended to have access to a Databricks premium workspace to follow
    along with the code examples at the end of the chapter. It’s also recommended
    to have Databricks workspace permissions to create an all-purpose cluster and
    a DLT pipeline using a cluster policy. Users will create and attach a notebook
    to a cluster and execute the notebook cells. All code samples can be downloaded
    from this chapter’s GitHub repository, located at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter01](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter01)
    . This chapter will create and run a new DLT pipeline using the Core product edition.
    As a result, the pipeline is estimated to consume around 5–10 **Databricks** **Units**
    ( **DBUs** ).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐访问 Databricks 高级工作区，以便跟随章节末尾的代码示例进行操作。还建议具有 Databricks 工作区权限，能够创建通用集群和使用集群策略创建
    DLT 管道。用户将创建并将笔记本附加到集群，并执行笔记本单元格。所有代码示例可以从本章的 GitHub 仓库下载，地址为 [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter01](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter01)。本章将使用核心产品版本创建并运行一个新的
    DLT 管道。因此，预计该管道将消耗约 5–10 **Databricks** **单位** (**DBU**)。
- en: The emergence of the lakehouse
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 湖仓的出现
- en: During the early 1980s, the data warehouse was a great tool for processing structured
    data. Combined with the right indexing methods, data warehouses allowed us to
    serve **business intelligence** ( **BI** ) reports at blazing speeds. However,
    after the turn of the century, data warehouses could not keep up with newer data
    formats such as JSON, as well as new data modalities such as audio and video.
    Simply put, data warehouses struggled to process semi-structured and unstructured
    data that most businesses used. Additionally, data warehouses struggled to scale
    to millions or billions of rows, common in the new information era of the early
    2000s. Overnight, batch data processing jobs soon ran into BI reports scheduled
    to refresh during the early morning business hours.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1980 年代初期，数据仓库是处理结构化数据的好工具。结合适当的索引方法，数据仓库使我们能够以极快的速度提供**商业智能**（**BI**）报告。然而，进入
    21 世纪后，数据仓库无法跟上像 JSON 这样的新数据格式，以及音频和视频等新型数据模态。简单来说，数据仓库在处理大多数企业使用的半结构化和非结构化数据时遇到困难。此外，数据仓库在处理数百万或数十亿行数据时也力不从心，这在
    2000 年代初期的信息时代变得越来越普遍。突然之间，批处理数据处理任务很快与在早晨商业工作时段安排刷新的 BI 报告发生冲突。
- en: At the same time, cloud computing became a popular choice among organizations
    because it provided enterprises with an elastic computing capacity that could
    quickly grow or shrink, based on the current computing demand, without having
    to deal with the upfront costs of provisioning and installing additional hardware
    on-premises.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，云计算成为组织中的流行选择，因为它提供了一种弹性计算能力，可以根据当前的计算需求快速扩展或收缩，而无需处理预配和安装额外的硬件设备在本地。
- en: Modern **extract, transform, and load** ( **ETL** ) processing engines such
    as Apache Hadoop and Apache Spark™ addressed the performance problem of processing
    big data ETL pipelines, ushering in a new concept, a **data lake** . Conversely,
    data lakes were terrible for serving BI reports and oftentimes offered degrading
    performance experiences for many concurrent user sessions. Furthermore, data lakes
    had poor data governance. They were prone to sloppy data wrangling patterns, leading
    to many expensive copies of the same datasets that frequently diverged from the
    source of truth. As a result, these data lakes quickly earned the nickname of
    *data swamps* . The big data industry needed a change. The lakehouse pattern was
    this change and aimed to combine the best of both worlds – fast BI reports and
    fast ETL processing of structured, semi-structured, and unstructured data in the
    cloud.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现代**抽取、转换和加载**（**ETL**）处理引擎，如 Apache Hadoop 和 Apache Spark™，解决了处理大数据 ETL 流水线的性能问题，引入了一个新概念，即**数据湖**。然而，数据湖在为
    BI 报告提供服务方面表现不佳，并且通常会为许多并发用户会话提供性能下降的体验。此外，数据湖的数据治理也很差。它们容易出现松散的数据整理模式，导致多个昂贵的数据副本，这些副本经常偏离数据真相。因此，这些数据湖很快就被称为*数据沼泽*。大数据行业需要变革。湖屋模式正是这种变革，旨在结合两者的优势
    – 云中结构化、半结构化和非结构化数据的快速 BI 报告和快速 ETL 处理。
- en: The Lambda architectural pattern
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambda 架构模式
- en: In the early 2010s, data streaming took a foothold in the data industry, and
    many enterprises needed a way to support both batch ETL processing and append-only
    streams of data. Furthermore, data architectures with many concurrent ETL processes
    needed to simultaneously read and change the underlying data. It was not uncommon
    for organizations to experience frequent conflicting write failures that led to
    data corruption and even data loss. As a result, in many early data architectures,
    a two-pronged Lambda architecture was built to provide a layer of isolation between
    these processes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在2010年代初期，数据流处理在数据行业占据了一席之地，许多企业需要一种方法来支持批量 ETL 处理和仅附加数据流。此外，具有许多并发 ETL 进程的数据架构需要同时读取和更改底层数据。组织经常遇到频繁的冲突写入失败，导致数据损坏甚至数据丢失。因此，在许多早期数据架构中，构建了两叉
    Lambda 架构，以在这些过程之间提供一层隔离。
- en: '![Figure 1.1 – A Lambda architecture was oftentimes created to support both
    real-time streaming workloads and batch processes such as BI reports](img/B22011_01_001.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – Lambda 架构经常被创建以支持实时流式工作负载和批处理过程，例如 BI 报告](img/B22011_01_001.jpg)'
- en: Figure 1.1 – A Lambda architecture was oftentimes created to support both real-time
    streaming workloads and batch processes such as BI reports
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – Lambda 架构经常被创建以支持实时流式工作负载和批处理过程，例如 BI 报告
- en: Using the Lambda architecture, downstream processes such as BI reports or **Machine
    Learning** ( **ML** ) model training could execute calculations on a snapshot
    of data, while streaming processes could apply near real-time data changes in
    isolation. However, these Lambda architectures duplicated data to support concurrent
    batch and streaming workloads, leading to inconsistent data changes that needed
    to be reconciled at the end of each business day.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Lambda 架构，下游过程（例如 BI 报告或**机器学习**（**ML**）模型训练）可以在数据快照上执行计算，而流处理过程可以单独应用几乎实时的数据更改。然而，这些
    Lambda 架构会复制数据以支持并发的批处理和流式工作负载，导致不一致的数据更改，需要在每个工作日结束时进行调解。
- en: Introducing the medallion architecture
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入勋章架构
- en: In an effort to clean up data lakes and prevent bad data practices, data lake
    architects needed a data processing pattern that would meet the high demands of
    modern-day ETL processing. In addition, organizations needed a simplified architecture
    for batch and streaming workloads, easy data rollbacks, good data auditing, and
    strong data isolation, while scaling to process terabytes or even petabytes of
    data daily.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清理数据湖并防止不良数据操作，数据湖架构师需要一种能够满足现代 ETL 处理高需求的数据处理模式。此外，组织还需要一种简化的架构，来处理批量和流式工作负载，支持数据回滚、良好的数据审计以及强大的数据隔离，并能够扩展到每天处理数
    TB 甚至 PB 级的数据。
- en: As a result, a design pattern within the lakehouse emerged, commonly referred
    to as the medallion architecture. This data processing pattern physically isolates
    data processing and improves data quality by applying business-level transformations
    in successive data hops, also called **data layers** .
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，Lakehouse 中出现了一种设计模式，通常称为徽章架构。该数据处理模式通过在连续的数据跳跃中应用业务级转换，物理隔离数据处理并提高数据质量，这些跳跃也被称为**数据层**。
- en: '![Figure 1.2 – The lakehouse medallion architecture](img/B22011_01_002.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – Lakehouse 徽章架构](img/B22011_01_002.jpg)'
- en: Figure 1.2 – The lakehouse medallion architecture
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – Lakehouse 徽章架构
- en: 'A typical design pattern for organizing data within a lakehouse (as shown in
    *Figure 1* *.2* ) includes three distinct data layers – a bronze layer, a silver
    layer, and finally, a gold layer:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一种典型的组织 Lakehouse 内部数据的设计模式（如 *图 1.2* 所示）包括三个不同的数据层——青铜层、银层和最后的金层：
- en: The bronze layer serves as a landing zone for raw, unprocessed data .
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 青铜层作为原始未处理数据的登陆区。
- en: Filtered, cleaned, and augmented data with a defined structure and enforced
    schema will be stored in the silver layer .
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过过滤、清理和增强的数据，具有定义的结构和强制执行的模式，将存储在银层中。
- en: Lastly, a refined, or gold layer, will deliver pristine, business-level aggregations
    ready to be consumed by downstream BI and ML systems .
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，经过精炼的金层，将提供洁净的业务级聚合，准备供下游 BI 和 ML 系统使用。
- en: Moreover, this simplified data architecture unifies batch and streaming workloads,
    by storing datasets in a big data format that supports concurrent batch and streaming
    data operations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种简化的数据架构通过将数据集存储在支持并发批量和流式数据操作的大数据格式中，统一了批量和流式工作负载。
- en: The Databricks lakehouse
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Databricks Lakehouse
- en: The Databricks lakehouse combines the processing power of a new high-performance
    processing engine, called the Photon Engine, with the augmentation of Apache Spark.
    Combined with open data formats for data storage, and support for a wide range
    of data types, including structured, semi-structured, and unstructured data, the
    Photon engine can process a wide variety of workloads using a single, consistent
    snapshot of the data in cheap and resilient cloud storage. I n addition, the Databricks
    lakehouse simplifies data architecture by unifying batch and streaming processing
    with a single API – the Spark DataFrame API. Lastly, the Databricks lakehouse
    was built with data governance and data security in mind, allowing organizations
    to centrally define data access patterns and consistently apply them across their
    businesses.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks Lakehouse 将一种新的高性能处理引擎——光子引擎的处理能力与 Apache Spark 的增强功能相结合。结合开源数据格式进行数据存储，并支持广泛的数据类型，包括结构化数据、半结构化数据和非结构化数据，光子引擎可以使用数据的单一一致性快照，处理各种工作负载，同时采用廉价且具备弹性的云存储。此外，Databricks
    Lakehouse 通过统一批处理和流处理，使用单一 API——Spark DataFrame API，简化了数据架构。最后，Databricks Lakehouse
    考虑到数据治理和数据安全，允许组织集中定义数据访问模式，并在整个业务中一致地应用这些模式。
- en: 'In this book, we’ll cover three major features that the Databricks lakehouse
    is anchored in:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将讨论 Databricks Lakehouse 的三个主要特性：
- en: The Delta Lake format
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Lake 格式
- en: The Photon Engine
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 光子引擎
- en: Unity Catalog
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unity Catalog
- en: While Delta Lake can be used to process both batch and streaming workloads concurrently,
    most data teams choose to implement their ETL pipelines using a batch execution
    model, mainly for simplicity’s sake. Let’s look at why that might be the case.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Delta Lake 可以同时处理批量和流式工作负载，但大多数数据团队选择使用批量执行模型来实现他们的 ETL 管道，主要是出于简便考虑。让我们看看为什么会是这样。
- en: The maintenance predicament of a streaming application
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式应用程序的维护困境
- en: Spark Structured Streaming provides near-real-time stream processing with fault
    tolerance, and exactly-once processing guarantees through the use of a DataFrame
    API that is near-identical to batch processing in Spark. As a result of a common
    DataFrame API, data engineering teams can convert existing batch Spark workloads
    to streaming with minimal effort. However, as the volume of data increases and
    the number of ingestion sources and data pipelines naturally grows over time,
    data engineering teams face the burden of augmenting existing data pipelines to
    keep up with new data transformations or changing business logic. In addition,
    Spark Streaming comes with additional configuration maintenance such as updating
    checkpoint locations, managing watermarks and triggers, and even backfilling tables
    when a significant data change or data correction occurs. Advanced data engineering
    teams may even be expected to build data validation and system monitoring capabilities,
    adding even more custom pipeline features to maintain. Over time, data pipeline
    complexity will grow, and data engineering teams will spend most of their time
    maintaining the operation of data pipelines in production and less time gleaning
    insights from their enterprise data. It’s evident that a framework is needed that
    allows data engineers to quickly declare data transformations, manage data quality,
    and rapidly deploy changes to production where they can monitor pipeline operations
    from a UI or other notification systems.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Structured Streaming 提供近实时流处理，具备容错性，并通过使用几乎与 Spark 批处理相同的 DataFrame API
    保证精确一次的处理。因此，由于 DataFrame API 的统一，数据工程团队可以以最小的努力将现有的批处理 Spark 工作负载转换为流处理。然而，随着数据量的增加以及摄取源和数据管道数量的自然增长，数据工程团队面临着增加现有数据管道的负担，以跟上新的数据转换或变化的业务逻辑。此外，Spark
    Streaming 还需要额外的配置维护，如更新检查点位置、管理水印和触发器，甚至在发生重大数据变化或数据更正时回填表格。高级数据工程团队甚至可能需要构建数据验证和系统监控能力，为了维护更多的自定义管道功能。随着时间的推移，数据管道的复杂性会增加，数据工程团队会花费大部分时间来维护生产环境中数据管道的运行，而不是从企业数据中提取洞察。显然，需要一个框架，使数据工程师能够快速声明数据转换、管理数据质量，并快速将变更部署到生产环境中，在
    UI 或其他通知系统中监控管道操作。
- en: What is the DLT framework?
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 DLT 框架？
- en: DLT is a declarative framework that aims to simplify the development and maintenance
    operations of a data pipeline by abstracting away a lot of the boilerplate complexities.
    For example, rather than declaring how to transform, enrich, and validate data,
    data engineers can declare what transformations to apply to newly arriving data.
    Furthermore, DLT provides support to enforce data quality, preventing a data lake
    from becoming a data swamp. DLT gives data teams the ability to choose how to
    handle poor-quality data, whether that means printing a warning message to the
    system logs, dropping invalid data, or failing a data pipeline run altogether.
    Lastly, DLT automatically handles the mundane data engineering tasks of maintaining
    optimized data file sizes of the underlying tables, as well as cleaning up obsolete
    data files that are no longer present in the Delta transaction log ( **Optimize**
    and **Vacuum** operations are covered later in the *A quick Delta Lake primer*
    section). DLT aims to ease the maintenance and operational burden on data engineering
    teams so that they can focus their time on uncovering business value from the
    data stored in their lakehouse, rather than spending time managing operational
    complexities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: DLT 是一个声明式框架，旨在通过抽象掉许多模板复杂性，简化数据管道的开发和维护操作。例如，数据工程师可以声明要对新到达的数据应用哪些转换，而不是声明如何转换、丰富和验证数据。此外，DLT
    还支持强制执行数据质量，防止数据湖变成数据沼泽。DLT 使数据团队能够选择如何处理低质量数据，无论是将警告信息打印到系统日志、丢弃无效数据，还是完全失败数据管道的运行。最后，DLT
    自动处理日常的数据工程任务，如维护底层表的优化数据文件大小，以及清理 Delta 事务日志中不再存在的过时数据文件（**优化**和**清理**操作将在后面的*Delta
    Lake 快速入门*部分介绍）。DLT 旨在减轻数据工程团队的维护和运营负担，让他们能够将时间集中在从存储在湖仓中的数据中发现业务价值，而不是花时间管理运营复杂性。
- en: How is DLT related to Delta Lake?
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DLT 与 Delta Lake 有什么关系？
- en: The DLT framework relies heavily on the Delta Lake format to incrementally process
    data at every step of the way. For example, streaming tables and materialized
    views defined in a DLT pipeline are backed by a Delta table. Features that make
    Delta Lake an ideal storage format for a streaming pipeline include support for
    **Atomicity, Consistency, Isolation, and Durability** ( **ACID** ) transactions
    so that concurrent data modifications such as inserts, updates, and deletions
    can be incrementally applied to a streaming table. Plus, Delta Lake features scalable
    metadata handling, allowing Delta Lake to easily scale to petabytes and beyond.
    If there is incorrect data computation, Delta Lake offers time travel – the ability
    to restore a copy of a table to a previous snapshot. Lastly, Delta Lake inherently
    tracks audit information in each table’s transaction log. Provenance information
    such as what type of operation modified the table, by what cluster, by which user,
    and at what precise timestamp are all captured alongside the data files. Let’s
    look at how DLT leverages Delta tables to quickly and efficiently define data
    pipelines that can scale over time.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: DLT 框架在每一步中都严重依赖 Delta Lake 格式来逐步处理数据。例如，在 DLT 管道中定义的流式表和物化视图是由 Delta 表支持的。使
    Delta Lake 成为流式管道理想存储格式的功能包括支持**原子性、一致性、隔离性和持久性**（**ACID**）事务，因此可以将并发的数据修改（如插入、更新和删除）逐步应用到流式表中。此外，Delta
    Lake 具有可扩展的元数据处理能力，允许 Delta Lake 容易扩展到 PB 级别及更大。如果数据计算有误，Delta Lake 提供时间旅行——恢复表的某个历史快照的功能。最后，Delta
    Lake 本身会跟踪每个表的事务日志中的审计信息。诸如修改表的操作类型、操作的集群、操作的用户和具体的时间戳等溯源信息，都与数据文件一起被捕获。让我们来看看
    DLT 如何利用 Delta 表快速高效地定义随时间扩展的数据管道。
- en: Introducing DLT concepts
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 DLT 概念
- en: The DLT framework automatically manages task orchestration, cluster creation,
    and exception handling, allowing data engineers to focus on defining transformations,
    data enrichment, and data validation logic. Data engineers will define a data
    pipeline using one or more dataset types. Under the hood, the DLT system will
    determine how to keep these datasets up to date. A data pipeline using the DLT
    framework is made up of the streaming tables, materialized views, and views dataset
    types, which we’ll discuss in detail in the following sections. We’ll also briefly
    discuss how to visualize the pipeline, view its triggering method, and look at
    the entire pipeline data flow from a bird’s-eye view. We’ll also briefly understand
    the different types of Databricks compute and runtime, and Unity Catalog. Let’s
    go ahead and get started.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DLT 框架自动管理任务调度、集群创建和异常处理，使数据工程师能够专注于定义转换、数据丰富和数据验证逻辑。数据工程师将使用一个或多个数据集类型定义数据管道。在幕后，DLT
    系统会确定如何保持这些数据集的最新状态。使用 DLT 框架的数据管道由流式表、物化视图和视图数据集类型组成，我们将在接下来的部分详细讨论这些内容。我们还将简要讨论如何可视化管道，查看其触发方式，并从鸟瞰图查看整个管道数据流。我们还将简要了解不同类型的
    Databricks 计算和运行时，以及 Unity Catalog。让我们开始吧。
- en: Streaming tables
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式表
- en: Streaming tables leverage the benefits of Delta Lake and Spark Structured Streaming
    to incrementally process new data as it arrives. This dataset type is useful when
    data must be ingested, transformed, or enriched at a high throughput and low latency.
    Streaming tables were designed specifically for data sources that append new data
    only and do not include data modification, such as updates or deletes. As a result,
    this type of dataset can scale to large data volumes, since it can incrementally
    apply data transformations as soon as new data arrives and does not need to recompute
    the entire table history during a pipeline update.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 流式表利用 Delta Lake 和 Spark Structured Streaming 的优势，逐步处理到达的新数据。此数据集类型在需要高吞吐量和低延迟地摄取、转换或丰富数据时非常有用。流式表专为仅追加新数据且不包括数据修改（如更新或删除）的数据源设计。因此，这种数据集类型可以扩展到大数据量，因为它可以在新数据到达时逐步应用数据转换，并且在管道更新时不需要重新计算整个表的历史记录。
- en: Materialized views
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 物化视图
- en: Materialized views leverage Delta Lake to compute the latest changes to a dataset
    and materialize the results in cloud storage. This dataset type is great when
    the data source includes data modifications such as updates and deletions, or
    a data aggregation must be performed. Under the hood, the DLT framework will perform
    the calculations to recompute the latest data changes to the dataset, using the
    full table’s history. The output of this calculation is stored in cloud storage
    so that future queries can reference the pre-computed results, as opposed to re-performing
    the full calculations each time the table is queried. As a result, this type of
    dataset will incur additional storage and compute costs each time the materialized
    view is updated. Furthermore, materialized views can be published to Unity Catalog,
    so the results can be queried outside of the DLT data pipeline. This is great
    when you need to share the output of a query across multiple data pipelines.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 物化视图利用Delta Lake计算数据集的最新变更，并将结果物化存储在云存储中。当数据源包含如更新和删除等数据修改，或必须执行数据聚合时，这种数据集类型非常适合。在后台，DLT框架将执行计算，重新计算数据集的最新数据变更，使用整个表的历史记录。该计算的输出存储在云存储中，以便未来的查询可以引用预计算的结果，而不是每次查询表时重新执行完整的计算。因此，每次物化视图更新时，这种类型的数据集将产生额外的存储和计算成本。此外，物化视图可以发布到Unity
    Catalog中，这样结果就可以在DLT数据管道之外进行查询。这在你需要跨多个数据管道共享查询结果时非常有用。
- en: Views
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视图
- en: Views also recompute the latest results of a particular query but do not materialize
    the results to cloud storage, which helps save on storage costs. This dataset
    type is great when you want to quickly check the intermediate result of data transformations
    in a data pipeline or apply other ad hoc data validations. Furthermore, the results
    of this dataset type cannot be published to Unity Catalog and are only available
    within the context of the data pipeline.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 视图也会重新计算特定查询的最新结果，但不会将结果物化到云存储中，从而帮助节省存储成本。这种数据集类型非常适合当你想快速检查数据管道中数据转换的中间结果或执行其他临时数据验证时。此外，这种数据集类型的结果不能发布到Unity
    Catalog，仅在数据管道上下文中可用。
- en: 'The following table summarizes the differences between the different dataset
    types in the DLT framework and when it’s appropriate to use one dataset type versus
    the other:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了DLT框架中不同数据集类型的差异，以及在何时使用某种数据集类型比其他类型更合适：
- en: '| **Dataset type** | **When to** **use it** |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **数据集类型** | **何时使用** |'
- en: '| Streaming t able | Ingestion workloads, when you need to continuously append
    new data to a target table with high throughput and low latency. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 流表 | 数据摄取工作负载，当你需要以高吞吐量和低延迟不断将新数据追加到目标表时。 |'
- en: '| Materialized v iew | Data operations that include data modifications, such
    as updates and deletions, or you need to perform aggregations on the full table
    history. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 物化视图 | 包含数据修改的操作，如更新和删除，或者你需要对整个表的历史记录执行聚合。 |'
- en: '| View | When you need to query intermediate data without publishing the results
    to Unity Catalog (e.g., perform data quality checks on intermediate transformations)
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 视图 | 当你需要查询中间数据但不将结果发布到Unity Catalog时（例如，执行中间转换的数据质量检查） |'
- en: Table 1.1 – Each dataset type in DLT serves a different purpose
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.1 – DLT中的每种数据集类型都有不同的用途
- en: Pipeline
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道
- en: A DLT pipeline is the logical data processing graph of one or more streaming
    tables, materialized views, or views. The DLT framework will take dataset declarations,
    using either the Python API or SQL API, and infer the dependencies between each
    dataset. Once a pipeline update runs, the DLT framework will update the datasets
    in the correct order using a dependency graph, called a **dataflow graph** .
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: DLT管道是一个或多个流表、物化视图或视图的逻辑数据处理图。DLT框架将使用Python API或SQL API进行数据集声明，并推断每个数据集之间的依赖关系。一旦管道更新运行，DLT框架将使用依赖图按正确的顺序更新数据集，这个依赖图被称为**数据流图**。
- en: Pipeline triggers
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道触发器
- en: A pipeline will be executed based on some triggering event. DLT offers three
    types of triggers – manual, scheduled, and continuous triggers. Once triggered,
    the pipeline will initialize and execute the dataflow graph, updating each of
    the dataset states.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 管道将根据某些触发事件执行。DLT提供三种类型的触发器——手动触发、定时触发和连续触发。一旦触发，管道将初始化并执行数据流图，更新每个数据集的状态。
- en: Workflow
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作流
- en: Databricks workflows is a managed orchestration feature of the **Databricks
    Data Intelligence Platform** that allows data engineers to chain together one
    or more dependent data processing tasks. For more complex data processing use
    cases, it may be necessary to build a data pipeline using multiple, nested DLT
    pipelines. For those use cases, Databricks workflows can simplify the orchestration
    of these data processing tasks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 工作流是**Databricks 数据智能平台**的一个托管编排功能，允许数据工程师将一个或多个相互依赖的数据处理任务串联起来。对于更复杂的数据处理用例，可能需要使用多个嵌套的
    DLT 管道来构建数据管道。在这些用例中，Databricks 工作流可以简化这些数据处理任务的编排。
- en: Types of Databricks compute
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Databricks 计算类型
- en: There are four types of computational resources available to Databricks users
    from the Databricks Data Intelligence Platform.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 数据智能平台为用户提供四种类型的计算资源。
- en: Job computes
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作业计算
- en: A job compute is an ephemeral collection of **virtual machines** ( **VMs** )
    with the **Databricks Runtime** ( **DBR** ) installed that are dynamically provisioned
    for the duration of a scheduled job. Once the job is complete, the VMs are immediately
    released back to the cloud provider. Since job clusters do not utilize the UI
    components of the Databricks Data Intelligence Platform (e.g., notebooks and the
    query editor), job clusters assess a lower **Databricks Unit** ( **DBU** ) for
    the entirety of their execution.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作业计算是一个包含安装有**Databricks Runtime**（**DBR**）的短期虚拟机（**VMs**）的集合，这些虚拟机会根据计划作业的时间动态配置。作业完成后，虚拟机会立即释放回云服务提供商。由于作业集群不使用
    Databricks 数据智能平台的 UI 组件（例如，笔记本和查询编辑器），作业集群在执行期间会评估较低的**Databricks 单位**（**DBU**）。
- en: All-purpose computes
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通用计算
- en: An all-purpose compute is a collection of ephemeral VMs with the DBR installed
    that is dynamically provisioned by a user, directly from the Databricks UI via
    a button click, or via the Databricks REST API (using the **/api/2.0/clusters/create**
    endpoint, for example), and they remain running until a user, or an expiring auto-termination
    timer, terminates the cluster. Upon termination, the VMs are returned to the cloud
    provider, and Databricks stops assessing additional DBUs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通用计算是一个包含安装有 DBR 的短期虚拟机的集合，用户可以通过点击 Databricks 用户界面上的按钮，或通过 Databricks REST
    API（例如使用**/api/2.0/clusters/create** 端点）动态创建它们，且虚拟机会一直运行，直到用户或自动终止计时器终止该集群。终止后，虚拟机会返回给云服务提供商，Databricks
    停止评估额外的 DBU。
- en: Instance pools
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实例池
- en: Instance pools are a feature in Databricks that helps reduce the time it takes
    to provision additional VMs and install the DBR. Instance pools will pre-provision
    VMs from the cloud provider and hold them in a logical container, similar to a
    valet keeping your car running in a valet parking lot.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实例池是 Databricks 的一个功能，帮助减少配置额外虚拟机并安装 DBR 所需的时间。实例池会从云服务提供商预先配置虚拟机，并将它们保存在一个逻辑容器中，类似于代客停车场中的代客停车员保持您的汽车运转。
- en: For some cloud providers, it can take 15 minutes or more to provision an additional
    VM, leading to longer troubleshooting cycles or ad hoc development tasks, such
    as log inspection or rerunning failed notebook cells during the development of
    new features.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些云服务提供商，提供额外的虚拟机可能需要 15 分钟或更长时间，这会导致更长的故障排除周期或临时开发任务，例如在开发新特性时检查日志或重新运行失败的笔记本单元格。
- en: Additionally, instance pools improve efficiency when many jobs are scheduled
    to execute closely together or with overlapping schedules. For example, as one
    job finishes, rather than releasing the VMs back to the cloud provider, the job
    cluster can place the VMs into the instance pool to be reused by the next job.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当许多作业计划紧密或重叠执行时，实例池可以提高效率。例如，当一个作业完成时，虚拟机不会释放回云服务提供商，而是可以将虚拟机放入实例池，以供下一个作业重复使用。
- en: Before returning the VMs to the instance pool, the Databricks container installed
    on the VM is destroyed, and a new container is installed on the VM containing
    the DBR when the next scheduled job requests the VM.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在将虚拟机返回到实例池之前，安装在虚拟机上的 Databricks 容器会被销毁，并且当下一个计划作业请求虚拟机时，安装有 DBR 的新容器会被安装到虚拟机上。
- en: Important note
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Databricks will not assess additional DBUs while VM(s) are up and running. However,
    the cloud provider will continue to charge for as long as the VMs are held in
    the instance pools.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟机（VM）正在运行时，Databricks 不会额外评估 DBU。然而，云服务提供商会继续收取虚拟机在实例池中的费用。
- en: To help control costs, instance pools provide an autoscaling feature that allows
    the size of the pool to grow and shrink, in response to demand. For example, the
    instance pool might grow to 10 VMs during peak hours but shrink back to 1 or 2
    during lulls in the processing demand.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助控制成本，实例池提供了一个自动扩展功能，允许池的大小根据需求自动增长或缩小。例如，在高峰时段，实例池可能会增长到 10 个虚拟机，但在处理需求低谷时，池的大小会缩小到
    1 或 2 个虚拟机。
- en: Databricks SQL warehouses
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Databricks SQL 仓库
- en: The last type of computational resource featured in the Databricks Data Intelligence
    Platform is **Databricks SQL** ( **DBSQL** ) warehouses. DBSQL warehouses are
    designed to run SQL workloads such as queries, reports, and dashboards. Furthermore,
    DBSQL warehouses are pre-configured computational resources designed to limit
    the configuration that a data analyst or SQL analyst would need to optimize for
    ad hoc data exploration and query execution. DBSQL warehouses are preconfigured
    with the latest DBRs, leverage the Databricks Photon engine, and have advanced
    Spark configuration settings preconfigured to optimize performance. A DBSQL warehouse
    also includes additional performance features such as results caching and disk
    caching, which can accelerate workloads by moving data closer to the hardware
    performing the query calculations. Combined with the processing speed of the Photon
    engine, the DBSQL warehouse achieves cloud warehouse speeds that Apache Spark
    once struggled to meet.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 数据智能平台中的最后一种计算资源是 **Databricks SQL** (**DBSQL**) 仓库。DBSQL 仓库旨在运行
    SQL 工作负载，如查询、报告和仪表板。此外，DBSQL 仓库是预配置的计算资源，旨在限制数据分析师或 SQL 分析师在临时数据探索和查询执行中需要优化的配置。DBSQL
    仓库预先配置了最新的 DBR，利用 Databricks Photon 引擎，并具有预配置的高级 Spark 配置设置以优化性能。DBSQL 仓库还包括其他性能特性，如结果缓存和磁盘缓存，这可以通过将数据更靠近执行查询计算的硬件来加速工作负载。结合
    Photon 引擎的处理速度，DBSQL 仓库实现了 Apache Spark 曾经难以达到的云仓库速度。
- en: Databricks Runtime
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Databricks Runtime
- en: The Databricks Runtime is a set of libraries pre-installed on the driver and
    worker nodes of a cluster during cluster initialization. These libraries include
    popular Java, R, and Python libraries to assist end users with ad hoc data wrangling
    or other development tasks. The libraries include core components that interface
    with the Databricks backend services to support rich platform features, such as
    collaborative notebooks, workflows, and cluster metrics. Furthermore, DBR includes
    other performance features such as data file caching (known as disk caching),
    the Databricks Photon engine for accelerated Spark processing, and other computational
    speed-ups. DBR comes in two varieties, Standard and ML, which are tailored to
    assist with the workloads anticipated to be run, based on the end user persona.
    For example, DBR for ML would have popular Python libraries such as TensorFlow
    and scikit-learn pre-installed to assist end users with the training of ML models,
    feature engineering, and other ML development tasks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks Runtime 是一组在集群初始化期间预先安装在集群的驱动节点和工作节点上的库。这些库包括流行的 Java、R 和 Python
    库，帮助最终用户进行临时数据整理或其他开发任务。这些库包含与 Databricks 后端服务接口的核心组件，支持丰富的平台功能，如协作笔记本、工作流和集群指标。此外，DBR
    还包括其他性能特性，如数据文件缓存（称为磁盘缓存）、用于加速 Spark 处理的 Databricks Photon 引擎，以及其他计算加速功能。DBR 有两种版本，Standard
    和 ML，分别针对不同的工作负载进行优化，依据最终用户的角色来调整。例如，适用于 ML 的 DBR 会预先安装流行的 Python 库，如 TensorFlow
    和 scikit-learn，以帮助最终用户进行 ML 模型训练、特征工程和其他 ML 开发任务。
- en: Unity Catalog
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Unity Catalog
- en: As the name suggests, Unity Catalog is a centralized governance store that is
    intended to span multiple Databricks workspaces. Rather than repeatedly defining
    the data governance policies for users and groups within each Databricks workspace,
    Unity Catalog allows data administrators to define access policies once in a centralized
    location. As a result, Unity Catalog acts as a single source of truth for data
    governance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，Unity Catalog 是一个集中式治理存储，旨在跨多个 Databricks 工作区进行管理。与其在每个 Databricks 工作区中重复定义用户和组的数据治理策略，Unity
    Catalog 允许数据管理员在一个集中位置定义访问策略。结果，Unity Catalog 成为数据治理的单一真实来源。
- en: In addition to data access policies, Unity Catalog also features data auditing,
    data lineage, data discovery, and data sharing capabilities, which will be covered
    in *Chapters 5* , *6* , and *7* .
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据访问策略外，Unity Catalog 还具有数据审计、数据血统、数据发现和数据共享功能，这些内容将在*第 5 章*、*第 6 章* 和*第 7
    章*中介绍。
- en: Unity Catalog is tightly integrated into the Databricks lakehouse, making it
    easy to build near-real-time data pipelines with strong data security in mind,
    using an open lakehouse storage format such as Delta Lake.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Unity Catalog 紧密集成到 Databricks Lakehouse 中，使得构建具备强大数据安全性的近实时数据管道变得简单，使用 Delta
    Lake 等开放的 Lakehouse 存储格式。
- en: A quick Delta Lake primer
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速了解 Delta Lake
- en: Delta Lake is a big data file protocol built around a multi-version transaction
    log that provides features such as ACID transactions, schema enforcement, time
    travel, data file management, and other performance features on top of existing
    data files in a l akehouse.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 是一种大数据文件协议，围绕多版本事务日志构建，提供诸如 ACID 事务、模式强制、时间旅行、数据文件管理等性能特性，基于现有的 Lakehouse
    数据文件。
- en: Originally, big data architectures had many concurrent processes that both read
    and modified data, leading to data corruption and even data loss. As previously
    mentioned, a two-pronged Lambda architecture was created, providing a layer of
    isolation between processes that applied streaming updates to data and downstream
    processes that needed a consistent snapshot of the data, such as BI workloads
    that generated daily reports or refreshed dashboards. However, these Lambda architectures
    duplicated data to support these batch and streaming workloads, leading to inconsistent
    data changes that needed to be reconciled at the end of each business day.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，大数据架构有许多并发的进程，这些进程既读取又修改数据，导致数据损坏甚至丢失。如前所述，创建了一个双管齐下的 Lambda 架构，在对数据进行流式更新的进程与需要一致数据快照的下游进程（如生成每日报告或刷新仪表盘的
    BI 工作负载）之间提供了一个隔离层。然而，这些 Lambda 架构重复了数据，以支持这些批处理和流式工作负载，导致了不一致的数据变化，需要在每个工作日结束时进行调和。
- en: Fortunately, the Delta Lake format provides a common storage layer for a lakehouse
    across disparate workloads and unifies both batch and streaming workloads. As
    such, a Delta table serves as the foundation for a Delta Live Table. Under the
    hood, a Delta Live Table is backed by a Delta table that is added to a dataflow
    graph, and whose state is updated by the DLT system whenever a DLT pipeline update
    is executed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Delta Lake 格式提供了一个共同的存储层，能够跨不同工作负载统一批处理和流式工作负载。因此，Delta 表成为 Delta Live
    表的基础。在后台，Delta Live 表由一个 Delta 表支持，该表被添加到数据流图中，并且每当 DLT 管道更新执行时，DLT 系统会更新其状态。
- en: The architecture of a Delta table
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta 表的架构
- en: The Delta transaction log is a key piece of the architecture for this big data
    format. Each Delta table contains a transaction log, which is a directory name,
    **_delta_log** , located at a ta ble’s root directory. The transaction log is
    a multi-version system of records that keeps track of the table’s state over a
    linear period of time.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Delta 事务日志是该大数据格式架构的关键部分。每个 Delta 表都包含一个事务日志，这是一个目录名为**_delta_log**，位于表的根目录下。事务日志是一个多版本的记录系统，用于跟踪表在一段线性时间内的状态。
- en: '![Figure 1.3 – The Delta transaction log sits alongside the partition directories
    and data files in a separate directory titled _delta_log](img/B22011_01_003.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – Delta 事务日志与分区目录和数据文件一起，存储在一个名为 _delta_log 的单独目录中](img/B22011_01_003.jpg)'
- en: Figure 1.3 – The Delta transaction log sits alongside the partition directories
    and data files in a separate directory titled _delta_log
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – Delta 事务日志与分区目录和数据文件一起，存储在一个名为 _delta_log 的单独目录中
- en: The transaction log informs the Delta Lake engine which data files to read to
    answer a particular query.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 事务日志通知 Delta Lake 引擎读取哪些数据文件来回答特定的查询。
- en: Within each transaction log directory, there will be one or more files stored
    in the JSON format, as well as other metadata information to help quickly and
    efficiently calculate the Delta table’s state (covered in the following section).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个事务日志目录中，将存储一个或多个 JSON 格式的文件，以及其他元数据，帮助快速有效地计算 Delta 表的状态（将在下一节中介绍）。
- en: As new data is appended, updated, or even deleted from a Delta table, these
    changes are recorded, or committed, to this directory as metadata information,
    stored as atomic JSON files. The JSON files are named using an ordered integer,
    starting with **00…0.json** and incrementing by one after each successful transaction
    commit.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 随着新数据的附加、更新或甚至删除，这些变化会被记录或提交到该目录作为元数据，以原子 JSON 文件的形式存储。JSON 文件的命名使用有序整数，从**00…0.json**开始，每次成功提交事务后递增一个。
- en: If a Delta table is partitioned, there will be one or more subdirectories containing
    the partitioning column information within the table’s root directory. Hive-style
    table partitioning is a very common performance technique that can speed up a
    query by collocating similar data within the same directory. The data is collocated
    by a particular column’s value (e.g., **"date=2024-01-05"** ). Optionally, there
    can be even more subdirectories nested within these partition directories, depending
    upon how many columns a table is partitioned by.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Delta 表是分区的，则表的根目录中将包含一个或多个子目录，存储分区列信息。Hive 风格的表分区是一种常见的性能优化技术，可以通过将相似数据存储在同一目录中来加速查询。数据按特定列的值（例如，**"date=2024-01-05"**）进行聚集。根据表分区的列数，分区目录中甚至可能包含更多嵌套的子目录。
- en: Within these partition subdirectories are one or more data files, stored using
    the Apache Parquet format. Apache Parquet is a popular columnar storage format,
    with efficient data compression and encoding schemes that yield fast data storage
    and retrieval for big data workloads. As a result, this open format was chosen
    as a foundation to store the data files that make up a Delta table.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些分区子目录中包含一个或多个数据文件，数据文件采用 Apache Parquet 格式存储。Apache Parquet 是一种流行的列式存储格式，具有高效的数据压缩和编码方案，能够快速存储和检索大数据工作负载的数据。因此，选择这种开放格式作为存储
    Delta 表数据文件的基础。
- en: The contents of a transaction commit
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事务提交的内容
- en: As mentioned earlier, the transaction log is the single source of truth for
    a Delta table. Each committed transaction (the JSON file under the **_delta_log**
    directory) will contain metadata information about the operation, or action, being
    applied to a particular Delta table. These JSON files can be viewed as a set of
    actions. Although there can be many concurrent transactions, the history of transaction
    commits is replayed in a linear order by table readers, and the result is the
    latest state of the Delta table.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，事务日志是 Delta 表的唯一真实来源。每个已提交的事务（**_delta_log** 目录下的 JSON 文件）将包含关于操作或应用于特定
    Delta 表的操作的元数据信息。这些 JSON 文件可以视为一组操作。虽然可以有多个并发事务，但事务提交历史会被表读取器按线性顺序回放，结果就是 Delta
    表的最新状态。
- en: 'Each JSON file could contain any of the following actions, as outlined by the
    Delta Lake protocol:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 JSON 文件可能包含以下 Delta Lake 协议中列出的操作：
- en: '**Change metadata** : This type of action is used to update the name, schema,
    or partitioning information of a table.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更改元数据**：此类型的操作用于更新表的名称、模式或分区信息。'
- en: '**Add file** : Perhaps the most frequent action applied, this action adds a
    new data file to a table along with statistical information about the first 32
    columns of a Delta table.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加文件**：可能是最常见的操作，此操作将一个新的数据文件添加到表中，并附带有关 Delta 表前 32 列的统计信息。'
- en: '**Remove file** : This action will logically delete a particular data file.
    Note that the physical data file will remain in cloud storage even after this
    transaction is committed (there’s more about this topic in the *Tombstoned data*
    *files* section).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除文件**：此操作将逻辑删除特定数据文件。请注意，物理数据文件将在此事务提交后仍保留在云存储中（有关此主题的更多内容，请参见 *墓碑数据* *文件*
    部分）。'
- en: '**Add Change Data Capture (CDC) information** : This action is used to add
    a CDC file that will contain all the data that has changed as a result of a particular
    table transaction.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加变更数据捕获（CDC）信息**：此操作用于添加一个 CDC 文件，该文件将包含由于特定表操作而发生变化的所有数据。'
- en: '**Transaction identifiers** : This action is used for Structured Streaming
    workloads and will contain the unique identifier for a particular stream, as well
    as the epoch identifier for the most recently committed Structured Streaming micro-batch.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事务标识符**：此操作用于结构化流式工作负载，将包含特定流的唯一标识符，以及最近提交的结构化流微批次的纪元标识符。'
- en: '**Protocol evolution** : Provides backward compatibility and ensures that old
    Delta Lake table readers can read the metadata information within the transaction
    log.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协议演进**：提供向后兼容性，确保旧版 Delta Lake 表读取器能够读取事务日志中的元数据。'
- en: '**Commit provenance information** : This type of action will conta in information
    about the process of committing a particular data transaction to a table. This
    will include information including the timestamp, the operation type, cluster
    identifier, and user information.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提交溯源信息**：这种操作将包含关于将特定数据事务提交到表的过程的信息。这将包括时间戳、操作类型、集群标识符和用户信息等信息。'
- en: '**Domain metadata** : This type of action sets the configuration for a particular
    domain. There are two types of domain metadata – system domain and user-controlled
    domain.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域元数据**：这种操作为特定领域设置配置。领域元数据有两种类型——系统领域和用户控制领域。'
- en: '**Sidecar file information** : This type of action will commit a separate metadata
    file to the transaction log, which contains summary information about the checkpoint
    file that was created (checkpoints are covered in the following section).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**侧车文件信息**：这种操作会将一个单独的元数据文件提交到事务日志，其中包含有关创建的检查点文件的汇总信息（检查点将在接下来的部分中讲解）。'
- en: Supporting concurrent table reads and writes
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持并发的表格读取与写入
- en: There are two types of concurrency control methods in storage systems – pessimistic
    concurrency control and optimistic concurrency control. A pessimistic concurrency
    control will attempt to thwart possible table conflicts by locking an entire table
    until an ongoing transaction has been completed. Conversely, optimistic concurrency
    control does not lock a table and will permit potential transaction conflicts
    to happen.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 存储系统中有两种并发控制方法——悲观并发控制和乐观并发控制。悲观并发控制通过锁定整个表，直到当前事务完成，来防止可能的表冲突。相反，乐观并发控制不会锁定表格，而是允许潜在的事务冲突发生。
- en: The authors of the Delta Lake protocol chose to implement the Delta Lake format
    using optimistic concurrency control. The reason why this design choice was made
    is that most big data workloads will append new data to an existing table, as
    opposed to modifying existing data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 协议的作者选择使用乐观并发控制来实现 Delta Lake 格式。做出这一设计选择的原因是，大多数大数据工作负载会将新数据附加到现有表中，而不是修改现有数据。
- en: 'As an example, let’s look at how Delta Lake will deal with a concurrent write
    conflict between two table writers – Table Writer A and Table Writer B:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，让我们看看 Delta Lake 如何处理两个表写入者之间的并发写入冲突——表写入者 A 和表写入者 B：
- en: '![Figure 1.4 – Delta Lake implements an optimistic concurrency scheme to handle
    concurrent write conflicts](img/B22011_01_004.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – Delta Lake 实现了乐观并发控制方案来处理并发写入冲突](img/B22011_01_004.jpg)'
- en: Figure 1.4 – Delta Lake implements an optimistic concurrency scheme to handle
    concurrent write conflicts
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – Delta Lake 实现了乐观并发控制方案来处理并发写入冲突
- en: Imagine that the two table writers modify the same data files and attempt to
    commit the data changes that conflict with one another. Let’s look at how Delta
    Lake handles this type of scenario.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设两个表的写入者修改了相同的数据文件，并尝试提交彼此冲突的数据更改。我们来看看 Delta Lake 是如何处理这种情况的。
- en: Writer A will first record the starting version identifier of the transaction
    that it will attempt to commit to the transaction log.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写入者 A 首先会记录它尝试提交到事务日志的事务的起始版本标识符。
- en: Writer A will then write all the data files for the transaction that it would
    like to commit.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写入者 A 随后将写入所有它希望提交的事务数据文件。
- en: Next, Writer A will attempt to commit the transaction to the Delta transaction
    log.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，写入者 A 将尝试将事务提交到 Delta 事务日志。
- en: At the same time, Writer B has already committed their transaction using the
    same table version identifier.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与此同时，写入者 B 已经使用相同的表版本标识符提交了他们的事务。
- en: Writer A detects Writer B’s commit and replays the commit information to determine
    whether any of the underlying data files have changed (e.g., the data has been
    updated).
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写入者 A 检测到写入者 B 的提交，并重放提交信息，以确定底层数据文件是否发生了变化（例如，数据是否已被更新）。
- en: If no data has changed (for example, both Writer A and Writer B commit append-only
    operations to the transaction log), then Writer A will increment the version identifier
    by 1 and attempt to recommit the transaction to the transaction log.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有数据发生变化（例如，写入者 A 和写入者 B 都只是将附加操作提交到事务日志），那么写入者 A 会将版本标识符加 1，并尝试重新提交该事务到事务日志。
- en: If the data has changed, then Writer A will need to recompute the transaction
    from scratch, increment the version identifier, and attempt to recommit the transaction.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果数据发生变化，那么写入者A将需要从头开始重新计算事务，递增版本标识符，并尝试重新提交事务。
- en: Tombstoned data files
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除数据文件
- en: When an update is applied to a Delta table that requires the data within a file
    to be updated, a new file using the **AddFile** operation will be created. Similarly,
    the file containing the out-of-date data will be logically deleted, using a **RemoveFile**
    operation. Then, both actions will be committed to the transaction log.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当对Delta表应用更新，且需要更新文件中的数据时，将使用**AddFile**操作创建一个新文件。同样，包含过时数据的文件将通过**RemoveFile**操作逻辑删除。然后，这两个操作将提交到事务日志中。
- en: By default, Delta Lake will retain the table metadat a (transaction log data)
    for 30 days before being automatically removed from cloud storage. When a particular
    data file is removed from the Delta transaction log, this is often referred to
    as a **tombstoned file** .
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Delta Lake会保留表元数据（事务日志数据）30天，之后会自动从云存储中删除。当某个数据文件从Delta事务日志中删除时，通常称之为**已删除文件**。
- en: To help control cloud storage costs, these tombstoned files, or files that no
    longer make up the latest Delta table state and are no longer referenced in the
    transaction log, can be removed from cloud storage altogether. A separate Delta
    Lake file management utility, called the **Vacuum** command, can be run as a separate
    process to identify all the tombstoned data files and remove them.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助控制云存储成本，这些已删除的文件，或者那些不再是最新Delta表状态的一部分并且不再在事务日志中被引用的文件，可以从云存储中彻底删除。一个独立的Delta
    Lake文件管理工具，称为**Vacuum**命令，可以作为独立进程运行，识别所有已删除的数据文件并将其移除。
- en: 'Furthermore, the **Vacuum** command is configurable, and the length of time
    to remove the table files can be specified as an optional input parameter. For
    example, the following code snippet will execute the **Vacuum** command on the
    Delta table, **yellow_taxi** , remov ing data files from the last 14 days of table
    history:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**Vacuum**命令是可配置的，可以通过可选的输入参数指定删除表文件的时间长度。例如，以下代码片段将对Delta表**yellow_taxi**执行**Vacuum**命令，移除表历史中最后14天的数据文件：
- en: '[PRE0]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we’ll see in the upcoming chapter, this process is automatically run and
    managed for DLT pipelines.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在接下来的章节中看到的那样，这个过程会自动运行并管理DLT管道。
- en: Calculating Delta table state
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算Delta表状态
- en: As alluded to in the previous section, Delta Lake will automatically compact
    metadata in a transaction log. As you can imagine, in big data workloads with
    thousands or even millions of transactions each day, a Delta table can rapidly
    grow in size. Similarly, the commit information in the transaction log will also
    grow comparatively.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所提到的，Delta Lake会自动在事务日志中压缩元数据。正如你可以想象的那样，在拥有成千上万甚至数百万个每日事务的大数据工作负载中，Delta表的大小会迅速增长。同样，事务日志中的提交信息也会相应增长。
- en: For every 10th commit, Delta Lake will create a checkpoint file, using the Apache
    Parquet format, that contains the latest table state information.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每第10次提交，Delta Lake将创建一个检查点文件，使用Apache Parquet格式，包含最新的表状态信息。
- en: '![Figure 1.5 – For every 10th commit, Delta Lake will write a checkpoint file](img/B22011_01_005.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5 – 对于每第10次提交，Delta Lake将写入一个检查点文件](img/B22011_01_005.jpg)'
- en: Figure 1.5 – For every 10th commit, Delta Lake will write a checkpoint file
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – 对于每第10次提交，Delta Lake将写入一个检查点文件
- en: Under the hood, a Delta Lake reader creates a separate Apache Spark Job to efficiently
    read the Delta table’s commit logs. For example, to calculate the latest table
    state, a Delta Lake reader will begin by reading the latest checkpoint file and
    applying the transaction commits that may have occurred after the file was created.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，Delta Lake读取器会创建一个独立的Apache Spark作业来高效读取Delta表的提交日志。例如，要计算最新的表状态，Delta Lake读取器将首先读取最新的检查点文件，并应用在文件创建之后可能发生的事务提交。
- en: Storing the table state information in checkpoint files alongside the data files
    in cloud storage was another pivotal design choice for the Delta Lake format.
    By using this method, calculating a table’s state could scale much better than
    other methods, such as using the Hive Metastore to serve table metadata information.
    Traditional big data metastores, such as the Hive Metastore, struggle to scale
    when many large, heavily active tables are queried concurrently and the table
    metadata information needs to be retrieved to answer queries.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 将表状态信息存储在与数据文件一起的检查点文件中，这是 Delta Lake 格式的另一个关键设计选择。通过这种方法，计算表的状态比其他方法（例如使用 Hive
    Metastore 提供表元数据）能够更好地扩展。当许多大型、活跃度高的表同时查询时，传统的大数据元数据存储（如 Hive Metastore）在检索表元数据以回答查询时难以扩展。
- en: To further speed up queries, Delta Lake readers will also cache the table state
    in local memory; that way, table readers can calculate which data files will answer
    a particular table query much faster.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步加快查询速度，Delta Lake 读者还将表状态缓存到本地内存中；这样，表读取器可以更快地计算哪些数据文件能回答特定的表查询。
- en: Time travel
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间旅行
- en: Another file management utility in Delta Lake is the time travel feature that
    allows end users to query a table’s state from a previous version. Time travel
    offers two methods to specify table state – using the table version number assigned
    in the transaction log or by using a timestamp.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 中的另一个文件管理工具是时间旅行功能，它允许最终用户查询表的先前版本状态。时间旅行提供了两种方法来指定表的状态——使用事务日志中分配的表版本号或使用时间戳。
- en: 'Users can query a previous Delta table’s state directly using the SQL syntax:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以使用 SQL 语法直接查询先前的 Delta 表状态：
- en: '[PRE1]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Similarly, Python users on the Databricks Data Intelligence Platform can use
    the Python API:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Databricks 数据智能平台上的 Python 用户可以使用 Python API：
- en: '[PRE2]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It’s important to note that, by default, the **Vacuum** utility will remove
    all data files from a particular Delta table, from the last seven days of table
    versions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，默认情况下，**Vacuum** 工具将删除特定 Delta 表中过去七天的所有数据文件。
- en: As a result, if the **Vacuum** command is run and a user attempts to query table
    history beyond the last seven days, the end user will receive a runtime exception,
    specifying that the data referenced in the transaction log no longer exists.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果运行 **Vacuum** 命令并且用户尝试查询超过最后七天的表历史记录，最终用户将收到运行时异常，说明事务日志中引用的数据不再存在。
- en: Furthermore, Delta Lake’s time travel feature was designed to correct recent
    data issues, so it should not be used for long-term data storage requirements,
    such as implementing an auditing system with a history spanning years.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Delta Lake 的时间旅行功能是为修复近期数据问题而设计的，因此不应用于长期数据存储要求，例如实现跨越数年的审计系统。
- en: Tracking table changes using change data feed
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用变更数据馈送跟踪表的更改
- en: Delta Lake’s **Change Data Feed** ( **CDF** ) feature tracks row-level changes
    that have been made to a Delta table, as well as metadata about those changes.
    For example, CDF will capture information about the operation type, the timestamp
    confirming that the change was made, and other provenance information such as
    cluster identification and user information.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 的 **变更数据馈送**（**CDF**）功能跟踪已对 Delta 表进行的行级更改，以及有关这些更改的元数据。例如，CDF 会捕获有关操作类型、确认更改发生的时间戳以及其他来源信息（如集群标识和用户信息）。
- en: For update operations, CDF will capture a snapshot of the row before an update,
    as well as a snapshot of the row after the update has been applied.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更新操作，CDF 会捕获更新前行的快照，以及更新应用后行的快照。
- en: '![Figure 1.6 – CDF captures the operation type, commit version, and timestamp](img/B22011_01_006.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.6 – CDF 捕获操作类型、提交版本和时间戳](img/B22011_01_006.jpg)'
- en: Figure 1.6 – CDF captures the operation type, commit version, and timestamp
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – CDF 捕获操作类型、提交版本和时间戳
- en: 'This feature is not enabled by default but can be configured by updating a
    Delta table’s properties. For example, CDF can be enabled on an existing table
    by altering the table, using a SQL **ALTER** statement:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能默认未启用，但可以通过更新 Delta 表的属性进行配置。例如，可以通过修改表格并使用 SQL **ALTER** 语句来启用 CDF：
- en: '[PRE3]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, CDF can also be enabled when a table is created by including the
    table property as a part of the **CREATE** **TABLE** statement:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在创建表时，也可以通过将表属性包含在 **CREATE** **TABLE** 语句中来启用 CDF：
- en: '[PRE4]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we’ll see in the next chapter, this feature is important in how DLT can efficiently
    apply changes from a source table to downstream datasets and implement **slowly
    changing** **dimensions** ( **SCDs** ).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在下一章中看到的那样，这个功能在 DLT 如何高效地将源表的变更应用到下游数据集并实现**慢变化维度**（**SCDs**）方面非常重要。
- en: A hands-on example – creating your first Delta Live Tables pipeline
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动手示例 – 创建您的第一个 Delta Live Tables 数据管道
- en: In this section, we’ll use a NYC taxi sample dataset to declare a data pipeline,
    using the DLT framework, and apply a basic transformation to enrich the data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 NYC 出租车示例数据集来声明一个数据管道，使用 DLT 框架，并应用基本的转换来丰富数据。
- en: Important note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: To get the most value out of this section, it’s recommended to have Databricks
    workspace permissions to create an all-purpose cluster and a DLT pipeline, using
    a cluster policy. In this section, you will attach a notebook to a cluster, execute
    notebook cells, as well as create and run a new DLT pipeline.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从本节中获得最大的价值，建议具有 Databricks 工作区权限，允许创建一个通用集群和 DLT 数据管道，使用集群策略。在本节中，您将把笔记本连接到集群，执行笔记本单元格，并创建和运行一个新的
    DLT 数据管道。
- en: Let’s start by creating a new all-purpose cluster. Navigate to the Databricks
    Compute UI by selecting the **Compute** button from the sidebar navigation on
    the left side.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建一个新的通用集群开始。通过从左侧导航栏选择 **计算** 按钮，导航到 Databricks 计算 UI。
- en: '![Figure 1.7 – Navigate to the Compute UI from the left-hand sidebar](img/B22011_01_007.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – 从左侧边栏导航到计算 UI](img/B22011_01_007.jpg)'
- en: Figure 1.7 – Navigate to the Compute UI from the left-hand sidebar
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 从左侧边栏导航到计算 UI
- en: Click the button titled **Create compute** at the top right. Next, provide a
    name for the cluster. For this exercise, the cluster can be a small, single-node
    cluster. Click the **Single node** radio button for the cluster type. Select the
    latest DBR in the runtime dropdown. Accept the defaults and click the **Create
    compute** button once again.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 点击右上角的 **创建计算** 按钮。接下来，为集群提供一个名称。在此练习中，集群可以是一个小型单节点集群。选择 **单节点** 单选按钮作为集群类型。在运行时下拉菜单中选择最新的
    DBR。接受默认设置后，再次点击 **创建计算** 按钮。
- en: Now that we have a cluster up and running, we can begin the development of our
    very first data pipeline.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经启动了一个集群，可以开始开发我们的第一个数据管道。
- en: Let’s first start by creating a new Databricks notebook under your workspace
    home directory. Create a new notebook by clicking the **Workspace** button on
    the left sidebar, clicking on the **Add** dropdown, and selecting **Notebook**
    . Give the notebook a meaningful name, such as **My First DLT Pipeline** . This
    new notebook is where we will declare the datasets and dependencies that will
    make up our Delta Live Table pipeline.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先在您的工作区主页目录下创建一个新的 Databricks 笔记本。通过点击左侧边栏的 **工作区** 按钮，点击 **添加** 下拉菜单并选择
    **笔记本**，创建一个新的笔记本。给笔记本起一个有意义的名字，例如 **我的第一个 DLT 数据管道**。这个新笔记本将用于声明构成我们 Delta Live
    Table 数据管道的数据集和依赖关系。
- en: 'All Databricks workspaces come with a set of sample datasets, located at **/databricks-datasets**
    in the Databricks FileSystem. You can browse the list of available datasets by
    listing the directory contents, using the Databricks FileSystem utility:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Databricks 工作区都附带一组示例数据集，位于 Databricks 文件系统中的**/databricks-datasets**。您可以通过列出目录内容，使用
    Databricks 文件系统工具浏览可用的数据集列表：
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, we need to import the **dlt** Python module. The **dlt** module contains
    function decorators that will instruct the DLT system on how to build our data
    pipeline, the dependencies, and an internal data processing graph, called a dataflow
    graph.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要导入 **dlt** Python 模块。**dlt** 模块包含函数装饰器，这些装饰器将指示 DLT 系统如何构建我们的数据管道、依赖关系以及一个名为数据流图的内部数据处理图。
- en: 'Add the following line to a new notebook cell:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下内容添加到一个新的笔记本单元格中：
- en: '[PRE6]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'DLT is built on top of PySpark, so we can leverage Spark DataFrames to define
    how to ingest data from cloud storage and how to apply data transformations. Let’s
    start by defining a function that will use Spark to read the NYC taxi sample dataset
    from the **/** **databricks-datasets** directory:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: DLT 基于 PySpark 构建，因此我们可以利用 Spark DataFrames 来定义如何从云存储中摄取数据以及如何应用数据转换。我们从定义一个函数开始，该函数将使用
    Spark 从 **/** **databricks-datasets** 目录读取 NYC 出租车示例数据集：
- en: '[PRE7]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this example, we’ve declared a simple function with a meaningful name, and
    when invoked, the function will use Spark to read the raw data stored in the yellow
    taxi dataset and return it as a streaming DataFrame.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们声明了一个有意义名称的简单函数，当调用时，该函数将使用 Spark 读取存储在黄出租车数据集中的原始数据，并将其作为流式 DataFrame
    返回。
- en: 'Now, we need to tell the DLT framework that we should use this declared function
    as a part of a data pipeline. We can do this by adding the **@dlt.table()** function
    decorator. This function decorator will create a Delta Live Table from the function
    and add it to the pipeline’s dataflow graph. Let’s also add some descriptive text
    to the optional **comment** parameter of this function decorator:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要告诉 DLT 框架，我们应该将此声明的函数作为数据管道的一部分使用。我们可以通过添加 **@dlt.table()** 函数装饰器来实现。这个函数装饰器将从函数创建一个
    Delta Live Table，并将其添加到管道的数据流图中。让我们还向这个函数装饰器的可选 **comment** 参数添加一些描述性文本：
- en: '[PRE8]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After executing the notebook cell, the Databricks Data Intelligence Platform
    will detect a DLT table, print the output schema, and prompt you to create a new
    DLT pipeline.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 执行笔记本单元格后，Databricks 数据智能平台将检测到一个 DLT 表，打印输出架构，并提示您创建一个新的 DLT 管道。
- en: '![Figure 1.8 – Databricks will parse the DLT table declaration and print the
    output schema](img/B22011_01_008.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 – Databricks 将解析 DLT 表声明并打印输出架构](img/B22011_01_008.jpg)'
- en: Figure 1.8 – Databricks will parse the DLT table declaration and print the output
    schema
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – Databricks 将解析 DLT 表声明并打印输出架构
- en: Let’s click the **Create Pipeline** button to generate a new DLT pipeline. Give
    the data pipeline a meaningful name, such as **Yellow Taxi Cab Pipeline** . Select
    **Core** as the product edition and **Triggered** as the pipeline execution mode.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们点击 **创建管道** 按钮以生成一个新的 DLT 管道。为数据管道指定一个有意义的名称，例如 **黄出租车管道**。选择 **Core** 作为产品版本，并将
    **触发式** 作为管道执行模式。
- en: '![Figure 1.9 – Create a new DLT pipeline using the Core product edition](img/B22011_01_009.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 – 使用 Core 产品版本创建一个新的 DLT 管道](img/B22011_01_009.jpg)'
- en: Figure 1.9 – Create a new DLT pipeline using the Core product edition
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 使用 Core 产品版本创建一个新的 DLT 管道
- en: Next, under the **Target Location** settings, select the Unity Catalog radio
    button, and specify the target catalog and schema where you would like to store
    the dataset. Under the **Compute** settings, set **Min workers** to **1** and
    **Max workers** to **1** . Then, accept the defaults by clicking the **Create**
    button. Finally, click the **Start** button to execute the data pipeline. You
    will be taken to a visual representation of the dataflow graph.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在 **目标位置** 设置中，选择 Unity Catalog 单选按钮，并指定您希望存储数据集的目标目录和模式。在 **计算** 设置下，将
    **最小工作节点数** 设置为 **1**，将 **最大工作节点数** 设置为 **1**。然后，点击 **创建** 按钮接受默认设置。最后，点击 **启动**
    按钮执行数据管道。您将看到数据流图的可视化表示。
- en: '![Figure 1.10 – The dataflow graph will contain the streaming table we declared
    in our notebook](img/B22011_01_010.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.10 – 数据流图将包含我们在笔记本中声明的流式表](img/B22011_01_010.jpg)'
- en: Figure 1.10 – The dataflow graph will contain the streaming table we declared
    in our notebook
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 – 数据流图将包含我们在笔记本中声明的流式表
- en: As you can see, our dataflow graph consists of a single streaming table, which
    is a new dataset that will ingest raw NYC taxi trip data from the **/databricks-datasets/**
    location on the Databricks FileSystem. While a trivial example, this example shows
    the declarative nature of DLT framework, as well as how quickly we can declare
    a data pipeline using the familiar PySpark API. Furthermore, you should now have
    a feel for how we can monitor and view the latest state of our data pipeline from
    the DLT UI.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的数据流图由一个单一的流式表组成，这是一个新的数据集，将从 Databricks 文件系统中的 **/databricks-datasets/**
    位置获取原始纽约市出租车旅行数据。虽然这是一个简单的示例，但它展示了 DLT 框架的声明式特性，以及我们如何快速使用熟悉的 PySpark API 声明数据管道。此外，您现在应该对如何从
    DLT UI 监控和查看我们数据管道的最新状态有了一个基本的了解。
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we examined how and why the data industry has settled on a
    lakehouse architecture, which aims to merge the scalability of ETL processing
    and the fast data warehousing speeds for BI workloads under a single, unified
    architecture. We learned how real-time data processing is essential to uncovering
    value from the latest data as soon as it arrives, but real-time data pipelines
    can halt the productivity of data engineering teams as complexity grows over time.
    Finally, we learned the core concepts of the Delta Live Tables framework and how,
    with just a few lines of PySpark code and function decorators, we can quickly
    declare a real-time data pipeline that is capable of incrementally processing
    data with high throughput and low latency.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了数据行业为何以及如何选择湖仓架构，这种架构旨在将ETL处理的可扩展性与BI工作负载下的数据仓库速度融合到一个统一的架构中。我们了解了实时数据处理如何成为从最新数据中发掘价值的关键，但随着时间的推移，实时数据管道的复杂性增加可能会影响数据工程团队的生产力。最后，我们学习了Delta
    Live Tables框架的核心概念，以及如何通过几行PySpark代码和函数装饰器，快速声明一个能够高效且低延迟地增量处理数据的实时数据管道。
- en: In the next chapter, we’ll take a deep dive into the advanced settings of Delta
    Live Tables pipelines and how the framework will optimize the underlying datasets
    for us. Then, we’ll look at more advanced data transformations, using a real-world
    use case to develop a data pipeline.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨Delta Live Tables管道的高级设置，以及该框架如何为我们优化底层数据集。然后，我们将通过一个实际的案例来开发数据管道，学习更复杂的数据转换。
