- en: Using Spark SQL in Machine Learning Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在机器学习应用中使用Spark SQL
- en: n this chapter, we will present typical use cases for using Spark SQL in machine
    learning applications. We will focus on the Spark machine learning API called
    `spark.ml`, which is the recommended solution for implementing ML workflows. The
    `spark.ml` API is built on DataFrames and provides many ready-to-use packages,
    including feature extractors, Transformers, selectors, and machine learning algorithms,
    such as classification, regression, and clustering algorithms. We will also use
    Apache Spark to perform **exploratory data analysis** (**EDA**), data pre-processing,
    feature engineering, and developing machine learning pipelines using `spark.ml`
    APIs and algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍在机器学习应用中使用Spark SQL的典型用例。我们将重点介绍名为`spark.ml`的Spark机器学习API，这是实现ML工作流的推荐解决方案。`spark.ml`
    API建立在DataFrames之上，并提供许多现成的包，包括特征提取器、转换器、选择器以及分类、回归和聚类算法等机器学习算法。我们还将使用Apache Spark来执行**探索性数据分析**（**EDA**）、数据预处理、特征工程以及使用`spark.ml`
    API和算法开发机器学习管道。
- en: 'More specifically, in this chapter, you will learn the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，在本章中，您将学习以下主题：
- en: Machine learning applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习应用
- en: Key components of Spark ML pipelines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark ML管道的关键组件
- en: Understand Feature engineering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解特征工程
- en: Implementing machine learning pipelines/applications
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施机器学习管道/应用程序
- en: Code examples using Spark MLlib
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark MLlib的代码示例
- en: Introducing machine learning applications
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍机器学习应用
- en: Machine learning, predictive analytics, and related data science topics are
    becoming increasingly popular for solving real-world problems across varied business
    domains.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，预测分析以及相关数据科学主题正在越来越受欢迎，用于解决各种业务领域的实际问题。
- en: Today, machine learning applications are driving mission-critical business decision-making
    in many organizations. These applications include recommendation engines, targeted
    advertising, speech recognition, fraud detection, image recognition and categorization,
    and so on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，机器学习应用正在推动许多组织的关键业务决策。这些应用包括推荐引擎、定向广告、语音识别、欺诈检测、图像识别和分类等。
- en: In the next section, we will introduce the key components of the Spark ML pipeline
    API.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍Spark ML管道API的关键组件。
- en: Understanding Spark ML pipelines and their components
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解Spark ML管道及其组件
- en: The machine learning pipeline API was introduced in Apache Spark 1.2\. Spark
    MLlib provides an API for developers to create and execute complex ML workflows.
    The Pipeline API lets developers quickly assemble distributed machine learning
    pipelines as the API has been standardized for applying different machine learning
    algorithms. Additionally, we can also combine multiple machine learning algorithms
    into a single pipeline. These pipelines consist of several key components that
    ease the implementation of data analytics and machine learning applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习管道API在Apache Spark 1.2中引入。Spark MLlib为开发人员提供了一个API，用于创建和执行复杂的ML工作流。管道API使开发人员能够快速组装分布式机器学习管道，因为API已经标准化，可以应用不同的机器学习算法。此外，我们还可以将多个机器学习算法组合成一个管道。这些管道包括几个关键组件，可以简化数据分析和机器学习应用的实现。
- en: 'The main components of an ML pipeline are listed here:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ML管道的主要组件如下：
- en: '**Datasets**: Spark SQL DataFrames/Datasets are used for storing and processing
    data in an ML pipeline. The DataFrames/Datasets API provides a standard API and
    a common way of dealing with both static data (typically, for batch processing)
    as well as streaming data (typically, for online stream processing). As we will
    see in the following sections, these Datasets will be used for storing and processing
    input data, transformed input data, feature vectors, labels, predictions, and
    so on.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：Spark SQL DataFrames/Datasets用于存储和处理ML管道中的数据。DataFrames/Datasets API提供了一个标准API和一种通用的处理静态数据（通常用于批处理）以及流数据（通常用于在线流处理）的方式。正如我们将在接下来的章节中看到的，这些数据集将被用于存储和处理输入数据、转换后的输入数据、特征向量、标签、预测等。'
- en: '**Pipelines**: ML workflows are implemented as pipelines consisting of a sequence
    of stages. For example, you could have a text preprocessing pipeline for the "Complete
    Submission Text File" of a `10-K` filing on the Edgar website. Such a pipeline
    would take the lines from the document as input at one end and produce a list
    of words as output, after passing through a series of Transformers (that apply
    regexes and other filters to the data in a particular sequence). Several examples
    of data and ML pipelines are presented in this chapter, as well as in [Chapter
    9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing Applications
    with Spark SQL*.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：ML工作流程被实现为由一系列阶段组成的管道。例如，您可以在Edgar网站的“10-K”申报的“完整提交文本文件”上有一个文本预处理管道。这样的管道会将文档中的行作为输入，经过一系列转换器（应用正则表达式和其他过滤器以特定顺序处理数据）后，产生一个单词列表作为输出。本章节以及[第9章](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c)中提供了几个数据和ML管道的示例。'
- en: '**Pipeline stages**: Each pipeline stage comprises of a Transformer or an Estimator
    that is executed in a specified sequence.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道阶段**：每个管道阶段包括按指定顺序执行的转换器或估计器。'
- en: '**Transformer**: This is an algorithm that transforms an input DataFrame into
    another DataFrame with one or more features added to it. There are several Transformers
    such as RegexTokenizer, Binarizer, OneHotEncoder, various indexers (for example,
    `StringIndexer` and `VectorIndexer`) and others available as a part of the library.
    You can also define your own custom Transformers like we do in [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c),
    *Developing Applications with Spark SQL.*'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器：这是一个将输入DataFrame转换为另一个DataFrame，并向其添加一个或多个特征的算法。库中有几个变压器，如RegexTokenizer、Binarizer、OneHotEncoder、各种索引器（例如`StringIndexer`和`VectorIndexer`）等。您还可以像我们在[第9章](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c)中所做的那样，定义自己的自定义变压器，*使用Spark
    SQL开发应用程序*。
- en: '**Estimator**: This is a machine learning algorithm that learns from the input
    data provided. The input to an estimator is a DataFrame and the output is a Transformer.
    There are several Estimators available in the MLlib libraries such as `LogisticRegression`,
    `RandomForest`, and so on. The output Transformers from these Estimators are the
    corresponding models, such as the LogisticRegressionModel, RandomForestModel,
    and so on.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计器：这是一个从提供的输入数据中学习的机器学习算法。估计器的输入是一个DataFrame，输出是一个变压器。MLlib库中有几个可用的估计器，如`LogisticRegression`、`RandomForest`等。这些估计器的输出变压器是相应的模型，如LogisticRegressionModel、RandomForestModel等。
- en: Understanding the steps in a pipeline application development process
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解管道应用开发过程中的步骤
- en: 'A machine learning pipeline application development process typically includes
    the following steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习管道应用开发过程通常包括以下步骤：
- en: '**Data ingestion**: The input data ingested by a typical machine learning pipeline
    comes from multiple data sources, often in several different formats (as described
    in [Chapter 2](part0035.html#11C3M0-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL for Processing Structured and SemiStructured Data*). These sources can
    include files, databases (RDBMSs, NoSQL, Graph, and so on), Web Services (for
    example, REST end-points), Kafka and Amazon Kinesis streams, and others.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄入：典型的机器学习管道摄入的输入数据来自多个数据源，通常以几种不同的格式（如[第2章](part0035.html#11C3M0-e9cbc07f866e437b8aa14e841622275c)中描述的*使用Spark
    SQL处理结构化和半结构化数据*）。这些来源可以包括文件、数据库（关系型数据库、NoSQL、图数据库等）、Web服务（例如REST端点）、Kafka和Amazon
    Kinesis流等。
- en: '**Data cleansing and preprocessing**: Data cleansing is a critical step in
    the overall data analytics pipeline. This preprocessing step fixes data quality
    issues and makes it suitable for consumption by the machine learning models. For
    example, we may need to remove HTML tags and replace special characters (such
    as `&nbsp`; and others) from the source HTML documents. We might have to rename
    columns (or specify them) as per the required standardized formats for Spark MLlib
    pipelines. Most importantly, we will also need to combine various columns in the
    DataFrame into a single column containing the feature vector.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据清洗和预处理：数据清洗是整体数据分析管道中的关键步骤。这个预处理步骤修复数据质量问题，并使其适合机器学习模型消费。例如，我们可能需要从源HTML文档中删除HTML标记并替换特殊字符（如`&nbsp`;等）。我们可能需要根据Spark
    MLlib管道的标准化格式重命名列（或指定列）。最重要的是，我们还需要将DataFrame中的各个列组合成包含特征向量的单个列。
- en: '**Feature engineering**: In this step, we extract and generate specific features
    from the input data using various techniques. These are then combined into a feature
    vector and passed to the next step in the process. Typically, a `VectorAssembler`
    is used to create the feature vector from the specified DataFrame columns.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程：在这一步中，我们使用各种技术从输入数据中提取和生成特定特征。然后将这些特征组合成特征向量，并传递到流程的下一步。通常，使用`VectorAssembler`从指定的DataFrame列创建特征向量。
- en: '**Model training**: Machine learning model training involves specifying an
    algorithm and some training data (which the model can learn from). Typically,
    we split our input Dataset into training and test Datasets, by randomly selecting
    a certain proportion of the input records for each of these Datasets. The model
    is trained by calling the `fit()` method on the training Dataset.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练：机器学习模型训练涉及指定算法和一些训练数据（模型可以从中学习）。通常，我们将输入数据集分割为训练数据集和测试数据集，通过随机选择一定比例的输入记录来创建这些数据集。通过在训练数据集上调用`fit()`方法来训练模型。
- en: '**Model validation**: This step involves evaluating and tuning the ML model
    to assess how good the predictions are. In this step, the model is applied to
    the test Dataset, using the `transform()` method, and appropriate performance
    measures for the model are computed, for example, accuracy, error, and so on.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型验证：这一步涉及评估和调整ML模型，以评估预测的准确性。在这一步中，将模型应用于测试数据集，使用`transform()`方法，并计算模型的适当性能指标，例如准确性、误差等。
- en: '**Model selection**: In this step, we choose the parameters for the Transformers
    and Estimators that produce the best ML model. Typically, we create a parameter
    grid and execute a grid-search for the most suitable set of parameters for a given
    model using a process called cross-validation. The best model returned by the
    cross validator can be saved and later loaded into the production environment.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型选择：在这一步中，我们选择变压器和估计器的参数，以产生最佳的ML模型。通常，我们创建一个参数网格，并使用交叉验证的过程执行网格搜索，以找到给定模型的最合适的参数集。交叉验证返回的最佳模型可以保存，并在生产环境中加载。
- en: '**Model Deployment**: Finally, we deploy the best model for production. For
    some models, it may be easier to convert the model parameters (such as coefficients,
    intercepts, or decision trees with branching logic) to other formats (such as
    JSON) for simpler and more efficient deployments in complex production environments.
    More details on such deployments can be got from [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c), *Spark
    SQL in Large-Scale Application Architectures*.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型部署**：最后，我们将最佳模型部署到生产环境中。对于一些模型，将模型参数（如系数、截距或具有分支逻辑的决策树）转换为其他格式（如JSON），以便在复杂的生产环境中进行更简单、更高效的部署可能更容易。有关此类部署的更多详细信息，请参阅[第12章](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c)，*大规模应用架构中的Spark
    SQL*。'
- en: The deployed models will need to be continuously maintained, upgraded, optimized,
    and so on, in the production environment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的模型将需要在生产环境中进行持续维护、升级、优化等。
- en: Introducing feature engineering
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入特征工程
- en: Feature engineering is the process of using domain knowledge of the data to
    create features that are key to applying machine learning algorithms. Any attribute
    can be a feature, and choosing a good set of features that helps solve the problem
    and produce acceptable results is key to the whole process. This step is often
    the most challenging aspect of machine learning applications. Both the quality
    and quantity/number of features greatly influences the overall quality of the
    model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是利用数据的领域知识创建对应用机器学习算法至关重要的特征的过程。任何属性都可以成为特征，选择一组有助于解决问题并产生可接受结果的良好特征是整个过程的关键。这一步通常是机器学习应用中最具挑战性的方面。特征的质量和数量/数量对模型的整体质量有很大影响。
- en: Better features also means more flexibility because they can result in good
    results even when less than optimal models are used. Most ML models can pick up
    on the structure and patterns in the underlying data, reasonably well. The flexibility
    of good features allows us to use less complex models that are faster and easier
    to understand and maintain. Better features also typically result in simpler models.
    Such features make it easier to select the right models and the most optimized
    parameters.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的特征也意味着更灵活，因为它们可以在使用不太理想的模型时产生良好的结果。大多数机器学习模型都能很好地捕捉基础数据的结构和模式。良好特征的灵活性使我们能够使用更简单、更快速、更易于理解和维护的模型。更好的特征通常也会导致更简单的模型。这些特征使得更容易选择正确的模型和最优化的参数。
- en: For a good blog on feature engineering, refer: Discover Feature Engineering,
    How to Engineer Features and How to Get Good at It, Jason Brownlee, at: [https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有关特征工程的优秀博客，请参考：发现特征工程，如何进行特征工程以及如何擅长它，Jason Brownlee，网址：[https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
- en: Producing a feature vector for every piece of information in a real-world Dataset
    is impractical from a processing and computing costs perspective. Typically, feature
    transformations, such as indexing and binning, are used to reduce the dimensionality
    of predictor variables. Additionally, irrelevant and low-frequency values are
    generally removed from the model, and continuous variables are grouped into a
    reasonable number of bins. Some of the original features may be highly correlated,
    or redundant, and can, therefore, be dropped from further consideration as well.
    Additionally, multiple features can be combined to yield new features (thereby
    reducing the overall dimensionality as well). Depending on the model, we may also
    have to normalize the values of some variables to avoid skewed results from using
    absolute values. We apply transformations on the training Dataset to obtain a
    feature vector that will be fed into a machine learning algorithm.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从处理和计算成本的角度来看，为真实世界数据集中的每一条信息生成一个特征向量是不切实际的。通常，特征转换，如索引和分箱，用于减少预测变量的维度。此外，通常会从模型中删除不相关和低频值，并将连续变量分组为合理数量的箱。一些原始特征可能高度相关或冗余，因此可以从进一步考虑中删除。此外，多个特征可以组合以产生新特征（从而降低总体维度）。根据模型，我们可能还需要对某些变量的值进行归一化，以避免使用绝对值产生偏斜的结果。我们对训练数据集应用转换，以获得将输入机器学习算法的特征向量。
- en: Thus, feature engineering is an iterative process comprising of multiple cycles
    of data selection and model evaluation. If the problem is well defined then the
    iterative process can be stopped at an appropriate point, and other configurations,
    or models, attempted.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，特征工程是一个迭代过程，包括多个数据选择和模型评估周期。如果问题定义良好，那么迭代过程可以在适当的时候停止，并尝试其他配置或模型。
- en: Creating new features from raw data
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从原始数据中创建新特征
- en: Selecting features from raw data could lead to many different feature sets however
    we need to keep the ones that are most relevant to the problem to be solved.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始数据中选择特征可能会导致许多不同的特征集，但我们需要保留与要解决的问题最相关的特征。
- en: Feature selection can reveal the importance of various features, but those features
    have to be identified first. The number of features can be limited by our ability
    to collect the data but once collected it is entirely dependent on our selection
    process. Usually, they need to be created manually, and this requires time, patience,
    creativity, and familiarity with the raw input data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择可以揭示各种特征的重要性，但首先必须识别这些特征。特征的数量可能受到我们收集数据的能力的限制，但一旦收集到数据，它完全取决于我们的选择过程。通常，它们需要手动创建，这需要时间、耐心、创造力和对原始输入数据的熟悉程度。
- en: Transformations of the raw input data depends on the nature of the data. For
    example, with textual data, it could mean generating document vectors while with
    image data, it could mean applying various filters to extract the contours from
    an image. Hence, the process is largely manual, slow, iterative, and requires
    lots of domain expertise.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 原始输入数据的转换取决于数据的性质。例如，对于文本数据，这可能意味着生成文档向量，而对于图像数据，这可能意味着应用各种滤波器来提取图像的轮廓。因此，这个过程在很大程度上是手动的、缓慢的、迭代的，并且需要大量的领域专业知识。
- en: Estimating the importance of a feature
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计特征的重要性
- en: We have to choose a subset from among hundreds and thousands of potential features
    to include in the modeling process. Making these choices requires deeper insights
    about the features that may have the greatest impact upon model performance. Typically,
    the features under consideration are scored and then ranked as per their scores.
    Generally, the features with the highest scores are selected for inclusion in
    the training Dataset, while others are ignored. Additionally, we may generate
    new features from the raw data features, as well. How do we know whether these
    generated features are helpful to the task at hand?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须从数百甚至数千个潜在特征中选择一个子集，包括在建模过程中。做出这些选择需要更深入地了解可能对模型性能产生最大影响的特征。通常，正在考虑的特征会被评分，然后根据它们的分数排名。一般来说，得分最高的特征会被选择包括在训练数据集中，而其他特征会被忽略。此外，我们也可以从原始数据特征中生成新特征。我们如何知道这些生成的特征对手头的任务有帮助呢？
- en: Different approaches can be used for estimating the importance of features.
    For example, we can group sets of related features and compare the performance
    of the model without those features to the complete model (with the dropped features
    included). We can also execute a k-fold cross-validation for both, the complete
    and dropped models, and compare them on various statistical measures. However,
    this approach can be too expensive to run continuously in production as it requires
    building every model k-times (for a k-fold cross-validation) for every feature
    group, which can be many (depending on the level of grouping). So, in practice,
    this exercise is performed periodically on a representative sample of models.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用不同的方法来估计特征的重要性。例如，我们可以将相关特征集合分组，并比较没有这些特征的模型与完整模型的性能（包括删除的特征）。我们还可以对完整模型和删除模型进行k折交叉验证，并在各种统计指标上进行比较。然而，这种方法在生产中可能会太昂贵，因为它需要为每个特征组建立每个模型k次（对于k折交叉验证），而这可能会很多（取决于分组的级别）。因此，在实践中，这种练习定期在代表性模型样本上进行。
- en: Other effective techniques for feature engineering include visualization and
    applying specific methods known to work well on certain types of data. Visualization
    can be a powerful tool to quickly analyze relationships between features and evaluate
    the impact of generated features. Using well-known approaches and methods in various
    domains can help accelerate the feature engineering process. For example, for
    textual data, using n-grams, TF-IDF, feature hashing, and others, are well known
    and widely applied feature engineering methods.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的其他有效技术包括可视化和应用已知对某些类型数据有效的特定方法。可视化可以是一个强大的工具，快速分析特征之间的关系，并评估生成特征的影响。在各个领域使用众所周知的方法和技术可以加速特征工程的过程。例如，对于文本数据，使用n-gram、TF-IDF、特征哈希等方法是众所周知且广泛应用的特征工程方法。
- en: Understanding dimensionality reduction
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解降维
- en: Primarily, dimensionality reduction deals with achieving a suitable reduction
    in the number of predictor variables in the model. It helps by selecting features
    that become part of the training Dataset after limiting the number of resulting
    columns in the feature matrix using various transformations. The attributes that
    are evaluated to be largely irrelevant to the problem need to be removed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 主要地，降维处理着眼于在模型中实现预测变量数量的适当减少。它通过选择成为训练数据集一部分的特征来帮助，在使用各种转换限制特征矩阵中结果列的数量后。被评估为与问题高度不相关的属性需要被移除。
- en: There will be some features that will be more important to the model's accuracy
    than others. There will also be some features that become redundant in the presence
    of other features.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特征对模型的准确性比其他特征更重要。在其他特征存在的情况下，有些特征会变得多余。
- en: Feature selection addresses these challenges by selecting a subset of features
    that is most useful in solving the problem. Feature selection algorithms may compute
    correlation coefficients, covariances, and other statistics for choosing a good
    set of features. A feature is generally included, if it is highly correlated with
    the dependent variable (the thing being predicted). We can also use **Principal
    Component Analysis** (**PCA**) and unsupervised clustering methods for feature
    selection. More advanced methods may search through various feature sets creating
    and evaluating models automatically in order to derive the best predictive subgroup
    of features.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择通过选择在解决问题中最有用的特征子集来解决这些挑战。特征选择算法可以计算相关系数、协方差和其他统计数据，以选择一组好的特征。如果一个特征与因变量（被预测的事物）高度相关，通常会包括该特征。我们还可以使用主成分分析（PCA）和无监督聚类方法进行特征选择。更先进的方法可能会通过自动创建和评估各种特征集来搜索，以得出最佳的预测特征子组。
- en: Deriving good features
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 好特征的衍生
- en: In this section, we will provide additional tips for deriving good features
    and measures for assessing them. These features can be handcrafted by a domain
    expert or automated using methods such as PCA or deep learning (see [Chapter 10](part0178.html#59O440-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL in Deep Learning Applications*)*.* Each of these approaches can
    be used independently, or jointly, to arrive at the best set of features.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提供有关提取良好特征和评估这些特征的措施的额外提示。这些特征可以由领域专家手工制作，也可以使用PCA或深度学习等方法自动化（有关这些方法的更多细节，请参见[第10章](part0178.html#59O440-e9cbc07f866e437b8aa14e841622275c)，*在深度学习应用中使用Spark
    SQL*）。这些方法可以独立或联合使用，以得到最佳的特征集。
- en: When working on machine learning projects, tasks such as data preparation and
    cleansing are key, in addition to the actual learning models and algorithms used
    to solve the business problems. In the absence of data pre-processing steps in
    machine learning applications, the resulting patterns will not be accurate or
    useful and/or the prediction results will have a lower accuracy.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目中，数据准备和清理等任务与解决业务问题所使用的实际学习模型和算法一样重要。在机器学习应用中缺乏数据预处理步骤时，得到的模式将不准确或无用，预测结果的准确性也会降低。
- en: 'Here, we present a few general tips to derive a good set of pre-processed features
    and cleansed data:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了一些关于提取良好的预处理特征和清理数据的一般提示：
- en: Explore grouping for categorical values and/or limiting the number of categorical
    values that become predictor variables in the feature matrix to only the most
    common ones.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索对分类值进行分组和/或限制成为特征矩阵中预测变量的分类值的数量，仅选择最常见的值。
- en: Evaluate and add new features by computing polynomial features from the provided
    features. However, be careful to avoid overfitting, which can occur when the model
    closely fits the data containing excessive number of features. This results in
    a model that memorizes the data, rather than learns from it, which in turn reduces
    its ability to predict new data accurately.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过从提供的特征计算多项式特征来评估并添加新特征。但是，要小心避免过拟合，当模型紧密拟合包含过多特征的数据时可能会发生过拟合。这会导致模型记住数据，而不是从中学习，从而降低其准确预测新数据的能力。
- en: Evaluating each feature to test its correlation with the classes independently
    by using a ranking metric, such as Pearson's correlation. We can then select a
    subset of features, such as the top 10%, or the top 20%, of the ranked features.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用排名指标（如Pearson相关系数）独立地评估每个特征与类的相关性。然后我们可以选择特征的子集，例如排名前10％或前20％的特征。
- en: Evaluating how good each feature is using criteria, such as Gini index and entropy.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用诸如Gini指数和熵之类的标准来评估每个特征的好坏。
- en: Exploring the covariance between features; for example, if two features are
    changing the same way, it will probably not serve the overall purpose to select
    both of them as features.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索特征之间的协方差；例如，如果两个特征以相同的方式变化，选择它们作为特征可能不会为整体目的服务。
- en: A model may also underfit the training Dataset, which will result in lower model
    accuracy. When your model is underfitting a Dataset, you should consider introducing
    new features.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型也可能对训练数据集欠拟合，这将导致模型准确性降低。当模型对数据集欠拟合时，应考虑引入新特征。
- en: A date-time field contains a lot of information that can be difficult for a
    model to take advantage of in its original format. Decompose date-time fields
    into separate fields for month, day, year, and so on, to allow models to leverage
    these relationships.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期时间字段包含大量信息，模型很难利用其原始格式。将日期时间字段分解为月份、日期、年份等单独的字段，以便模型利用这些关系。
- en: Apply linear Transformers to numerical fields, such as weights and distances,
    for use in regression and other algorithms.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将线性变换器应用于数值字段，例如权重和距离，以用于回归和其他算法。
- en: Explore storing a quantity measure (such weights or distances) as a rate or
    an aggregate quantity over a time interval to expose structures, such as seasonality.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索将数量度量（如权重或距离）存储为速率或时间间隔内的聚合数量，以暴露结构，如季节性。
- en: In the next section, we will present a detailed code example of a Spark ML pipeline.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将提供Spark ML管道的详细代码示例。
- en: Implementing a Spark ML classification model
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施Spark ML分类模型
- en: The first step in implementing a machine learning model is to perform EDA on
    input data. This analysis would typically involve data visualization using tools
    such as Zeppelin, assessing feature types (numeric/categorical), computing basic
    statistics, computing covariances, and correlation coefficients, creating pivot
    tables, and so on (for more details on EDA, see [Chapter 3](part0045.html#1AT9A0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL for Data Exploration*).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施机器学习模型的第一步是对输入数据进行EDA。这种分析通常涉及使用Zeppelin等工具进行数据可视化，评估特征类型（数值/分类），计算基本统计数据，计算协方差和相关系数，创建数据透视表等（有关EDA的更多细节，请参见[第3章](part0045.html#1AT9A0-e9cbc07f866e437b8aa14e841622275c)，*使用Spark
    SQL进行数据探索*）。
- en: The next step involves executing data pre-processing and/or data munging operations.
    In almost all cases, the real-world input data will not be high quality data ready
    for use in a model.  There will be several transformations required to convert
    the features from the source format to final variables; for example, categorical
    features may need to be transformed to a binary variable for each categorical
    value using one-hot encoding technique (for more details on data munging, see
    [Chapter 4](part0057.html#1MBG20-e9cbc07f866e437b8aa14e841622275c), *Using Spark
    SQL for Data Munging*).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及执行数据预处理和/或数据整理操作。在几乎所有情况下，现实世界的输入数据都不会是高质量的数据，可以直接用于模型。需要进行几次转换，将特征从源格式转换为最终变量；例如，分类特征可能需要使用一种独热编码技术将每个分类值转换为二进制变量（有关数据整理的更多细节，请参见[第4章](part0057.html#1MBG20-e9cbc07f866e437b8aa14e841622275c)，*使用Spark
    SQL进行数据整理*）。
- en: Next is the feature engineering step. In this step, we will derive new features
    to include, along with other existing features, in the training data. Use the
    tips provided earlier in this chapter to derive a good set of features to be ultimately used
    in training the model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是特征工程步骤。在这一步中，我们将导出新特征，以及其他现有特征，包括在训练数据中。使用本章前面提供的提示来导出一组良好的特征，最终用于训练模型。
- en: Finally, we will train the model using the selected features and test it using
    the test Dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用选定的特征训练模型，并使用测试数据集进行测试。
- en: 'For a good blog containing a detailed step-by-step example of a classification
    model applied to the Kaggle knowledge challenge--Titanic: Machine Learning from
    Disaster, refer: *Building Classification model using Apache Spark* by Vishnu
    Viswanath at: [http://vishnuviswanath.com/spark_lr.html](http://vishnuviswanath.com/spark_lr.html).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个很好的博客，其中包含了一个详细的分类模型应用于Kaggle知识挑战-泰坦尼克号：灾难中的机器学习的逐步示例，参见：*使用Apache Spark构建分类模型*，作者Vishnu
    Viswanath，网址为：[http://vishnuviswanath.com/spark_lr.html](http://vishnuviswanath.com/spark_lr.html)。
- en: 'These steps, along with the operations and input/output at each stage, are
    shown in the following figure:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤以及每个阶段的操作和输入/输出如下图所示：
- en: '![](img/00182.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00182.jpeg)'
- en: We will use the publicly available diabetes Dataset that consists of 101,766
    rows, representing ten years of clinical care records from 130 US hospitals and
    integrated delivery networks. It includes over 50 features (attributes) representing
    patient and hospital outcomes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用公开可用的糖尿病数据集，该数据集包含101,766行，代表了130家美国医院和综合配送网络的十年临床护理记录。它包括50多个特征（属性），代表患者和医院的结果。
- en: The Dataset can be downloaded from the UCI website at [https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以从UCI网站下载，网址为[https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008)。
- en: The source ZIP file contains two CSV files. The first file, `diabetic_data.csv`,
    is the main input Dataset, and the second file, `IDs_mapping.csv`, is the master
    data for `admission_type_id`, `discharge_disposition_id`, and `admission_source_id`.
    The second file is small enough to manually split into three parts, one for each
    set of ID mappings.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 源ZIP文件包含两个CSV文件。第一个文件`diabetic_data.csv`是主要的输入数据集，第二个文件`IDs_mapping.csv`是`admission_type_id`、`discharge_disposition_id`和`admission_source_id`的主数据。第二个文件足够小，可以手动分成三部分，每部分对应一个ID映射集。
- en: 'The example in this section closely follows the approach and analysis presented
    in: Impact of HbA1c measurement on hospital readmission rates: analysis of 70,000
    clinical database patient records, by Beata Strack, Jonathan P. DeShazo, Chris
    Gennings,Juan L. Olmo,Sebastian Ventura,Krzysztof J. Ciosand John N. Clore, Biomed
    Res Int. 2014; 2014: 781670, available at: [http://europepmc.org/articles/PMC3996476](http://europepmc.org/articles/PMC3996476).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '本节中的示例紧密遵循了Beata Strack、Jonathan P. DeShazo、Chris Gennings、Juan L. Olmo、Sebastian
    Ventura、Krzysztof J. Cios和John N. Clore在Biomed Res Int. 2014; 2014: 781670中提出的方法和分析：HbA1c测量对医院再入院率的影响，可在[http://europepmc.org/articles/PMC3996476](http://europepmc.org/articles/PMC3996476)上找到。'
- en: 'First, we will import all the packages required for this coding exercise:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入此编码练习所需的所有软件包：
- en: '![](img/00183.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00183.jpeg)'
- en: Exploring the diabetes Dataset
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索糖尿病数据集
- en: The Dataset contains attributes/features originally selected by clinical experts
    based on their potential connection to the diabetic condition or management.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含最初由临床专家选择的属性/特征，这些属性/特征基于它们与糖尿病状况或管理的潜在联系。
- en: A full list of the features and their description is available at [https://www.hindawi.com/journals/bmri/2014/781670/tab1/](https://www.hindawi.com/journals/bmri/2014/781670/tab1/).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 特征及其描述的完整列表可在[https://www.hindawi.com/journals/bmri/2014/781670/tab1/](https://www.hindawi.com/journals/bmri/2014/781670/tab1/)上找到。
- en: 'We load the input data into a Spark DataFrame, as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入数据加载到Spark DataFrame中，如下所示：
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can display the schema of the DataFrame created in the previous step to
    list the columns or fields in the DataFrame, as shown:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显示在上一步中创建的DataFrame的模式，以列出DataFrame中的列或字段，如下所示：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/00184.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00184.jpeg)'
- en: 'Next, we print out a few sample records to get a high-level sense of the values
    contained in the fields of the Dataset:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打印一些样本记录，以对数据集中字段中包含的值有一个高层次的了解：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/00185.gif)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00185.gif)'
- en: 'We can also compute the basic statistics for numerical columns using `dataFrame.describe("column")`.
    For example, we display the count, mean, standard deviation, and min and max values
    of a few numeric data columns:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`dataFrame.describe("column")`计算数值列的基本统计信息。例如，我们显示一些数值数据列的计数、平均值、标准差以及最小和最大值：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The original input Dataset contains incomplete, redundant, and noisy information,
    as expected in any real-world Dataset. There are several fields that have a high
    percentage of missing values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 原始输入数据集包含不完整、冗余和嘈杂的信息，这在任何真实世界的数据集中都是预期的。有几个字段存在高比例的缺失值。
- en: 'We compute the number of records that have specific fields missing, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算具有特定字段缺失的记录数，如下所示：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As computed precedingly, the features with many missing values are identified
    to be weight, payer code, and medical specialty. We drop the weight and payer
    code columns, however, the medical specialty attribute ( potentially, a very relevant
    feature) is retained:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，具有许多缺失值的特征被确定为体重、付款码和医疗专业。我们删除体重和付款码列，但是医疗专业属性（可能是一个非常相关的特征）被保留：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The Dataset also contains records of multiple inpatient visits by some of the
    patients. Here, we extract a set of patients with multiple inpatient visits.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集还包含一些患者的多次住院就诊记录。在这里，我们提取了一组有多次住院就诊的患者。
- en: 'We observe that the overall number of such patients is significant:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到这类患者的总数是显著的：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As mentioned in the reference study/paper, such observations cannot be considered
    as statistically independent, hence, we include only the first encounter for each
    patient. After these operations are completed, we verify that there are no patient
    records remaining in the DataFrame corresponding to multiple visit records:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如参考研究/论文中所述，这样的观察结果不能被视为统计独立，因此我们只包括每位患者的第一次就诊。完成这些操作后，我们验证了DataFrame中没有剩余的患者记录对应于多次就诊记录：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As in the reference study/paper, we also remove records of encounters that
    resulted in a patient''s death to avoid bias:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与参考研究/论文中一样，我们也删除了导致患者死亡的就诊记录，以避免偏见：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After performing the mentioned operations, we were left with `69,934` encounters
    that constitute the final Dataset used for further analysis.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述操作后，我们剩下了`69,934`次就诊，构成了用于进一步分析的最终数据集。
- en: 'Next, we execute a set of `JOIN` operations to understand the data better in
    terms of top categories of `discharge disposition`, `admission types`, and `admission
    sources`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们执行一系列的`JOIN`操作，以更好地了解数据在`discharge disposition`、`admission types`和`admission
    sources`的顶级类别方面的情况：
- en: '![](img/00186.gif)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00186.gif)'
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/00187.gif)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00187.gif)'
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/00188.gif)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00188.gif)'
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the next section, we will execute a series of data munging or data pre-processing
    steps to improve the overall data quality.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将执行一系列数据整理或数据预处理步骤，以提高整体数据质量。
- en: Pre-processing the data
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: There are several data munging steps required to preprocess the data. We will
    start by addressing the missing field, values. We have several options when dealing
    with null or missing values. We can drop them using `df.na.drop()`, or fill them
    with default values using `df.na.fill()`. Such fields can be replaced with the
    most commonly occurring values for that column, and in the case of numeric fields
    they can also be replaced with average values. Additionally, you can also train
    a regression model on the column and use it to predict the field values for rows
    where values are missing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理数据需要进行几个数据整理步骤。我们将从处理缺失字段值开始。在处理空值或缺失值时，我们有几个选项。我们可以使用`df.na.drop()`删除它们，或者使用`df.na.fill()`用默认值填充它们。这样的字段可以用该列的最常出现的值替换，对于数值字段，也可以用平均值替换。此外，您还可以对该列训练回归模型，并用它来预测缺失值的行的字段值。
- en: Here, we have the missing fields represented with a `?` value in the Dataset,
    so we use the `df.na.replace()` function to replace them with the "Missing" string.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，数据集中的缺失字段用`?`表示，因此我们使用`df.na.replace()`函数将其替换为"Missing"字符串。
- en: 'This operation is illustrated for the  `medical_specialty` field, as shown:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作以`medical_specialty`字段为例，如下所示：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `medical_specialty` field can have values, such as cardiology, internal
    medicine, family/general practice, surgeon, or Missing. To a model, the Missing
    value appears like any other choice for `medical_specialty`. We could have created
    a new binary feature called `has_ medical_specialty` and assigned it a value of
    `1` when a row contained the value and `0` when it was unknown or missing. Alternatively,
    we could also have created a binary feature for each value of `medical_specialty`,
    such as `Is_Cardiology`, `Is_Surgeon`, and `Is_Missing`. These additional features
    can then be used instead of, or in addition to, the `medical_specialty` feature
    in different models.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`medical_specialty`字段可以有值，例如心脏病学、内科、家庭/全科医学、外科医生或缺失。对于模型来说，缺失值看起来就像`medical_specialty`的任何其他选择。我们可以创建一个名为`has_medical_specialty`的新二元特征，并在行包含该值时赋值为`1`，在未知或缺失时赋值为`0`。或者，我们也可以为`medical_specialty`的每个值创建一个二元特征，例如`Is_Cardiology`、`Is_Surgeon`和`Is_Missing`。然后，这些额外的特征可以在不同的模型中代替或补充`medical_specialty`特征。'
- en: 'Next, guided by the analysis contained in the original paper, we drop a set
    of columns from further analysis in this chapter, to keep the size of the problem
    reasonable, as shown:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，根据原始论文中的分析，我们在本章的进一步分析中删除了一组列，以保持问题的规模合理，如下所示：
- en: '![](img/00189.gif)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00189.gif)'
- en: 'As in the referenced study/paper, we also consider four groups of encounters:
    no HbA1c test performed, HbA1c performed and in normal range, HbA1c performed
    and the result is greater than 8 percent, and HbA1c performed and the result is
    greater than 8 percent.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与参考研究/论文中一样，我们也考虑了四组就诊情况：未进行HbA1c测试、进行了HbA1c测试并且结果在正常范围内、进行了HbA1c测试并且结果大于8%、进行了HbA1c测试并且结果大于8%。
- en: 'These steps to accomplish this grouping are executed as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这些分组的步骤如下：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Since our primary objective focuses on factors that lead to early readmission,
    the readmission attribute (or the outcome) has two values: `Readmitted`, if the
    patient was readmitted within 30 days of discharge or `Not Readmitted`, which
    covers both readmission after 30 days and no readmission at all.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的主要目标是关注导致早期再入院的因素，再入院属性（或结果）有两个值：`Readmitted`，如果患者在出院后30天内再次入院，或`Not Readmitted`，它涵盖了30天后的再入院和根本没有再入院。
- en: 'We create a new ordinal feature, called `Readmitted`, with two values: `Readmitted`
    and `Not Readmitted`. You can use similar approaches for age categories as well:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个新的有序特征，名为`Readmitted`，有两个值：`Readmitted`和`Not Readmitted`。您也可以对年龄类别使用类似的方法：
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We display the numbers of several features versus the values of the target
    variable, as follows. This will help identify skews in the number of records based
    on various attributes in the input Dataset:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们显示了几个特征的数量与目标变量的值，如下所示。这将有助于识别基于输入数据集中各种属性的记录数量的偏差：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we group the various age ranges into various categories and adds it as
    a column to obtain our final version of the Dataset, as illustrated. Additionally,
    we remove the three rows where the `gender` is `Unknown/Invalid`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将各种年龄范围分组为不同的类别，并将其添加为一列，以获得我们的最终版本的数据集，如图所示。此外，我们删除了`gender`为`Unknown/Invalid`的三行：
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/00190.gif)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00190.gif)'
- en: 'The schema for the final DataFrame after the pre-processing steps is, as shown:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 经过预处理步骤后的最终DataFrame的模式如下所示：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00191.gif)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00191.gif)'
- en: 'We display a few sample records from the final DataFrame, as shown:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们显示了最终数据框中的一些样本记录，如下所示：
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/00192.gif)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00192.gif)'
- en: After completing the data preprocessing phase, we now shift our focus to building
    the machine learning pipeline.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 完成数据预处理阶段后，我们现在将重点转移到构建机器学习管道上。
- en: Building the Spark ML pipeline
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建Spark ML管道
- en: In our example ML pipeline, we will have a sequence of pipeline components,
    which are detailed in the following sections.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例ML管道中，我们将有一系列管道组件，这些组件在以下部分详细说明。
- en: Using StringIndexer for indexing categorical features and labels
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用StringIndexer对分类特征和标签进行索引
- en: In this exercise, we will be training a random forest classifier. First, we
    will index the categorical features and labels as required by `spark.ml`. Next,
    we will assemble the feature columns into a vector column because every `spark.ml`
    machine learning algorithm expects it. Finally, we can train our random forest
    on a training Dataset. Optionally, we can also unindex the labels to make them
    more readable.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将训练一个随机森林分类器。首先，我们将按照`spark.ml`的要求对分类特征和标签进行索引。接下来，我们将把特征列组装成一个向量列，因为每个`spark.ml`机器学习算法都需要它。最后，我们可以在训练数据集上训练我们的随机森林。可选地，我们还可以对标签进行反索引，以使它们更易读。
- en: There are several ready-to-use Transformers available to index categorical features.
    We can assemble all the features into one vector (using `VectorAssembler`) and
    then use a `VectorIndexer` to index it. The drawback of `VectorIndexer` is that
    it will index every feature that has less than `maxCategories` number of different
    values. It does not differentiate whether a given feature is categorical or not.
    Alternatively, we can index every categorical feature one by one using a `StringIndexer`,
    as follows.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个现成的转换器可用于对分类特征进行索引。我们可以使用`VectorAssembler`将所有特征组装成一个向量（使用`VectorAssembler`），然后使用`VectorIndexer`对其进行索引。`VectorIndexer`的缺点是它将对每个具有少于`maxCategories`个不同值的特征进行索引。它不区分给定特征是否是分类的。或者，我们可以使用`StringIndexer`逐个对每个分类特征进行索引，如下所示。
- en: We use a `StringIndexer` to transform String features to `Double` values. For
    example, the `raceIndexer` is an estimator that transforms the race column, that
    is, it generates indices for the different races in the input column, and creates
    a new output column called `raceCat`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`StringIndexer`将字符串特征转换为`Double`值。例如，`raceIndexer`是一个估计器，用于转换种族列，即为输入列中的不同种族生成索引，并创建一个名为`raceCat`的新输出列。
- en: 'The `fit()` method then converts the column into a `StringType` and counts
    the numbers of each race. These steps are shown here:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()`方法然后将列转换为`StringType`并计算每个种族的数量。这些步骤如下所示：'
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/00193.gif)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00193.gif)'
- en: '`raceIndexer.transform()` assigns the generated index to each value of the
    race in the column. For example, `AfricanAmerican` is assigned `1.0`, `Caucasian`
    is assigned `0.0`, and so on, as shown.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`raceIndexer.transform()`将生成的索引分配给列中每个种族的值。例如，`AfricanAmerican`被分配为`1.0`，`Caucasian`被分配为`0.0`，依此类推。'
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Similarly, we create indexers for the gender, age groups, HbA1c test results,
    change of medications, and diabetes medications prescribed, and fit them to the
    resulting DataFrames at each step:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们为性别、年龄组、HbA1c测试结果、药物变化和糖尿病处方药物创建索引器，并在每个步骤中将它们适配到生成的数据框中：
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We print the schema of the resulting DataFrame containing the columns for various
    indexers:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印包含各种索引器列的最终数据框的模式：
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/00194.gif)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00194.gif)'
- en: 'We can also index the labels using `StringIndexer`, as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`StringIndexer`对标签进行索引，如下所示：
- en: '[PRE23]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Alternatively, we can also define our feature indexers, succinctly, as illustrated.
    The sequence of `StringIndexers` can then be concatenated with the numeric features
    to derive the features vector using a `VectorAssembler`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以简洁地定义我们的特征索引器，如下所示。然后可以使用`VectorAssembler`将`StringIndexers`的序列与数值特征连接起来，以生成特征向量：
- en: '[PRE24]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We need not have explicitly called the `fit()` and `transform()` methods for
    each indexer, that can be handled by the pipeline, automatically.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要为每个索引器显式调用`fit()`和`transform()`方法，这可以由管道自动处理。
- en: 'The behavior of the pipeline can be summarized are listed as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的行为可以总结如下：
- en: It will execute each stage and pass the result of the current stage to the next
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将执行每个阶段，并将当前阶段的结果传递给下一个阶段
- en: If the stage is a Transformer, then the pipeline calls `transform()` on it
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果阶段是一个转换器，那么管道会调用它的`transform()`
- en: If the stage is an Estimator, then the pipeline first calls `fit()` followed
    by `transform()`
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果阶段是一个估计器，那么管道首先调用`fit()`，然后调用`transform()`
- en: If it is the last stage in the pipeline, then the Estimator will not call `transform()` (after `fit()` )
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果它是管道中的最后一个阶段，那么估计器在`fit()`之后不会调用`transform()`
- en: Using VectorAssembler for assembling features into one column
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用VectorAssembler将特征组装成一个列
- en: 'Now that our indexing is done, we need to assemble all our feature columns
    into a single column containing a vector that groups all our features. To do that,
    we''ll use the `VectorAssembler` Transformer, as in the following steps. However,
    first, due to the significant skew in the number of records for each label, we
    sample the records in appropriate proportions to have nearly equal numbers of
    records for each label:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的索引工作已经完成，我们需要将所有特征列组装成一个包含所有特征的向量列。为此，我们将使用`VectorAssembler`转换器，如下所示。然而，首先，由于每个标签的记录数量存在显著偏差，我们需要以适当的比例对记录进行抽样，以使每个标签的记录数量几乎相等：
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Alternatively, we can also achieve the same by following the steps listed here:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以按照以下步骤实现相同的效果：
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We apply the `transform()` operation and print a few sample records of the
    resulting DataFrame, as shown:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用`transform()`操作并打印生成的数据框的一些样本记录，如下所示：
- en: '[PRE27]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/00195.gif)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00195.gif)'
- en: '`VectorIndexer` is used for indexing the features. We will pass all the feature
    columns that are used for the prediction to create a new vector column called
    `indexedFeatures`, as shown:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorIndexer`用于对特征进行索引。我们将传递用于预测的所有特征列，以创建一个名为`indexedFeatures`的新向量列，如下所示：'
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the next section, we will train a random forest classifier.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将训练一个随机森林分类器。
- en: Using a Spark ML classifier
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark ML分类器
- en: 'Now that that data is in the proper format expected by `spark.ml` machine learning
    algorithms, we will create a `RandomForestClassifier` component, as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经符合`spark.ml`机器学习算法的预期格式，我们将创建一个`RandomForestClassifier`组件，如下所示：
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The standardized DataFrame format, allows easy replacement of the `RandomForestClassifier`
    with other `spark.ml` classifiers, such as `DecisionTreeClassifier` and `GBTClassifier`,
    as shown:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化的DataFrame格式，允许轻松地用其他`spark.ml`分类器替换`RandomForestClassifier`，例如`DecisionTreeClassifier`和`GBTClassifier`，如下所示：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the following section, we will create our pipeline by assembling the label
    and feature indexers, and the random forest classifier as stages in the pipeline.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将通过将标签和特征索引器以及随机森林分类器组装成管道的阶段来创建我们的管道。
- en: Creating a Spark ML pipeline
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建Spark ML管道
- en: 'Next, we will create a pipeline object using all the components we have defined
    till now. Since all the different steps have been implemented, we can assemble
    our pipeline, as shown:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用到目前为止定义的所有组件来创建一个管道对象。由于所有不同的步骤都已经实现，我们可以组装我们的管道，如下所示：
- en: '[PRE31]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the next section, we will create training and test Datasets from the input
    Dataset.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将从输入数据集中创建训练和测试数据集。
- en: Creating the training and test Datasets
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建训练和测试数据集
- en: 'To train and evaluate the model, we split the input data, randomly, between
    two DataFrames: a training set (containing 80 percent of the records) and a test
    set(containing 20 percent of the records). We will train the model using the training
    set and then evaluate it using the test set. The following can be used to split
    input data:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练和评估模型，我们将在两个DataFrame之间随机拆分输入数据：一个训练集（包含80%的记录）和一个测试集（包含20%的记录）。我们将使用训练集训练模型，然后使用测试集评估模型。以下内容可用于拆分输入数据：
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We will now use the pipeline to fit the training data. A `PipelineModel` object
    is returned as a result of fitting the pipeline to the training data:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用管道来拟合训练数据。拟合管道到训练数据会返回一个`PipelineModel`对象：
- en: '[PRE33]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the next section, we will make predictions on our test Dataset.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将对我们的测试数据集进行预测。
- en: Making predictions using the PipelineModel
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PipelineModel进行预测
- en: 'The `PipelineModel` object from the previous step is used for making predictions
    on the test Dataset, as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 前一步骤中的`PipelineModel`对象用于对测试数据集进行预测，如下所示：
- en: '[PRE34]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/00196.gif)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00196.gif)'
- en: 'Next, we will evaluate our model by measuring the accuracy of the predictions,
    as shown in the following steps:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过测量预测的准确性来评估我们的模型，如下所示：
- en: '[PRE35]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we can also print our random forest model to understand the logic
    being used in the ten trees created in our model, as illustrated:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以打印我们的随机森林模型，以了解我们模型中创建的十棵树中使用的逻辑，如下所示：
- en: '[PRE36]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](img/00197.gif)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00197.gif)'
- en: In the next section, we will show the process of cross-validation to pick the
    best predictive model from a set of parameters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将展示通过交叉验证从一组参数中选择最佳预测模型的过程。
- en: Selecting the best model
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最佳模型
- en: In order to select the best model, we will perform a grid search over a set
    of parameters. For each combination of parameters, we will perform cross-validation
    and retain the best model according to some performance indicator. This process
    can be tedious, but `spark.ml` simplifies it with an easy-to-use API.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择最佳模型，我们将对一组参数进行网格搜索。对于每组参数的组合，我们将进行交叉验证，并根据某些性能指标保留最佳模型。这个过程可能会很繁琐，但是`spark.ml`通过易于使用的API简化了这一过程。
- en: For cross-validation, we choose a value `k` for the number of folds, for example,
    a value of `3` will split the Dataset into three parts. From those three parts,
    three different training and test data pairs will be generated (two-thirds of
    the data for training and one-third for test). The model is evaluated on the average
    of the chosen performance indicator over the three pairs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于交叉验证，我们选择一个值`k`作为折叠数，例如，`3`的值将把数据集分为三部分。从这三部分中，将生成三个不同的训练和测试数据对（用于训练的数据占三分之二，用于测试的数据占三分之一）。模型将根据三个对的选择性能指标的平均值进行评估。
- en: There is a set of values assigned to each of the parameters. The parameters
    used in our example are `maxBins` (the maximum number of bins used for discretizing
    continuous features and for splitting features at each node), `maxDepth` (the
    maximum depth of a tree), and `impurity` (the criterion used for information gain
    calculations).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 每个参数都分配了一组值。我们示例中使用的参数是`maxBins`（用于离散化连续特征和在每个节点分割特征的最大箱数）、`maxDepth`（树的最大深度）和`impurity`（用于信息增益计算的标准）。
- en: 'First, we create a grid of parameters, as illustrated:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个参数网格，如下所示：
- en: '[PRE37]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/00198.gif)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00198.gif)'
- en: Next, we will define an Evaluator which, as its name implies, will evaluate
    our model according to some metric. There are built-in Evaluators available for
    regression, and binary and multi-class classification models.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个评估器，根据其名称，它将根据某些指标评估我们的模型。内置的评估器可用于回归，二元和多类分类模型。
- en: '[PRE38]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, after choosing `k=2` (set to a higher number for real-world models),
    the number of folds the data will be split into during cross-validation, we can
    create a `CrossValidator` object, as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，选择`k=2`（对于真实世界的模型设置更高的数字），数据在交叉验证期间将被分成的折叠数，我们可以创建一个`CrossValidator`对象，如下所示：
- en: '[PRE39]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: You have to be careful when running cross-validation, especially on bigger Datasets,
    as it will train **k x p models**, where *p* is the product of the number of values
    for each param in your grid. So, for a *p* of `18`, the cross-validation will
    train `36` different models.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行交叉验证时，特别是在更大的数据集上，需要小心，因为它将训练**k x p个模型**，其中*p*是网格中每个参数值的数量的乘积。因此，对于*p*为`18`，交叉验证将训练`36`个不同的模型。
- en: 'Since our `CrossValidator` is an Estimator, we can obtain the best model for
    our data by calling the `fit()` method on it:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的`CrossValidator`是一个估计器，我们可以通过调用`fit()`方法来获得我们数据的最佳模型：
- en: '[PRE40]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can now make predictions on `testData`, as illustrated here. Note a slight
    improvement in the accuracy value as compared to the values before cross-validation:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以对`testData`进行预测，如下所示。与交叉验证之前的值相比，准确度值略有改善：
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](img/00199.gif)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00199.gif)'
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In the next section, we will show you the power of common interfaces that Spark
    ML exposes to ease the development and testing of ML pipelines.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将展示Spark ML公开的常用接口的强大功能，以便简化ML管道的开发和测试。
- en: Changing the ML algorithm in the pipeline
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在管道中更改ML算法
- en: In an earlier section, we showed how easy it is replace the RandomForestClassifier
    with other classifiers, such as, the DecisionTreeClassifier or the GBTClassifer.  In
    this section, we will replace the random forest classifier with a logistic regression
    model. Logistic regression explains the relationship between a binary-valued dependent
    variable based on other variables called independent variables. The binary values,
    `0` or `1`, can represent prediction values such as pass/fail, yes/no, dead/alive,
    and so on. Based on the values of the independent variables, it predicts the probability
    that the dependent variable takes one of the categorical values, such as a `0`
    or a `1`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在较早的部分中，我们展示了如何轻松地用其他分类器替换RandomForestClassifier，例如DecisionTreeClassifier或GBTClassifer。在本节中，我们将用逻辑回归模型替换随机森林分类器。逻辑回归解释了一个基于其他变量（称为自变量）的二元值因变量之间的关系。二元值`0`或`1`可以表示预测值，例如通过/不通过，是/否，死/活等。根据自变量的值，它预测因变量取一个分类值（例如`0`或`1`）的概率。
- en: 'First, we will create a `LogtisticRegression` component, as shown:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个`LogtisticRegression`组件，如下所示：
- en: '[PRE43]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can use the label and feature indexers from before and combine them with
    the logistic regression component to create a new pipeline, as follows. Furthermore,
    we use the `fit()` and `transform()` methods to train and then make predictions
    on the test Dataset. Note that the code looks very similar to the approach used
    earlier for the random forest pipeline:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前的标签和特征索引器，并将它们与逻辑回归组件结合起来创建一个新的管道，如下所示。此外，我们使用`fit()`和`transform()`方法来训练，然后对测试数据集进行预测。请注意，代码看起来与之前用于随机森林管道的方法非常相似：
- en: '[PRE44]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Over the next several sections, we will introduce a series of tools and utilities
    available in Spark, that can be used to achieve better ML models.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将介绍Spark中提供的一系列工具和实用程序，可用于实现更好的ML模型。
- en: Introducing Spark ML tools and utilities
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Spark ML工具和实用程序
- en: In the following sections, we will explore various tools and utilities that
    Spark ML offers to select features and create superior ML models easily and efficiently.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将探索Spark ML提供的各种工具和实用程序，以便轻松高效地选择特征并创建优秀的ML模型。
- en: Using Principal Component Analysis to select features
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用主成分分析选择特征
- en: As mentioned earlier, we can derive new features using **Principal Component
    Analysis** (**PCA**) on the data. This approach depends on the problem, so it
    is imperative to have a good understanding about the domain.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以使用**主成分分析**（**PCA**）在数据上派生新特征。这种方法取决于问题，因此有必要对领域有很好的理解。
- en: This exercise typically requires creativity and common sense to choose a set
    of features that may be relevant to the problem. A more extensive exploratory
    data analysis is typically required to help understand the data better and/or
    to identify patterns that lead to a good set of features.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习通常需要创造力和常识来选择一组可能与问题相关的特征。通常需要进行更广泛的探索性数据分析，以帮助更好地理解数据和/或识别导致一组良好特征的模式。
- en: PCA is a statistical procedure that converts a set of potentially correlated
    variables into a, typically, reduced set of linearly uncorrelated variables. The
    resulting set of uncorrelated variables are called principal components. A `PCA`
    class trains a model to project vectors to a lower dimensional space. The following
    example shows how to project our multi-dimensional feature vector into three-dimensional
    principal components.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种统计程序，它将一组可能相关的变量转换为通常较少的一组线性不相关的变量。得到的一组不相关的变量称为主成分。`PCA`类训练一个模型，将向量投影到一个较低维度的空间。以下示例显示了如何将我们的多维特征向量投影到三维主成分。
- en: According to Wikipedia, [https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis),
    "This transformation is defined in such a way that the first principal component
    has the largest possible variance (that is, it accounts for as much of the variability
    in the data as possible), and each succeeding component, in turn, has the highest
    variance possible under the constraint that it is orthogonal to the preceding
    components."
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 根据维基百科[https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)，“这种转换是这样定义的，以便第一个主成分具有最大可能的方差（即，它尽可能多地解释数据的变异性），而每个随后的成分依次具有在约束下可能的最高方差，即它与前面的成分正交。”
- en: 'We will be building our model using the Dataset used to fit in the random forest
    algorithm used for classification earlier in this chapter:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用用于在本章前面用于分类的随机森林算法拟合的数据集来构建我们的模型：
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Using encoders
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用编码器
- en: 'In this section, we use one-hot encoding to map a column of label indices to
    a column of binary vectors with, at most, a single one-value. This encoding allows
    algorithms that expect continuous features, such as `LogisticRegression`, to use
    categorical features:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用独热编码将标签索引的列映射到二进制向量的列，最多只有一个值为1。这种编码允许期望连续特征的算法（如`LogisticRegression`）使用分类特征：
- en: '[PRE46]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](img/00200.gif)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00200.gif)'
- en: Using Bucketizer
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Bucketizer
- en: Bucketizer is used to transform a column of continuous features to a column
    of feature buckets. We specify the `n+1` splits parameter for mapping continuous
    features into n buckets. The splits should be in a strictly increasing order.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Bucketizer用于将连续特征的列转换为特征桶的列。我们指定`n+1`分割参数，将连续特征映射到n个桶中。分割应严格按升序排列。
- en: 'Typically, we add `Double.NegativeInfinity` and `Double.PositiveInfinity` as
    the outer bounds of the splits to prevent potential out of Bucketizer bounds exceptions.
    In the following example, we specify six splits, and then define a `bucketizer`
    for the `num_lab_procedures` feature (with values varying from `1` to `126` in
    our Dataset), as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们将`Double.NegativeInfinity`和`Double.PositiveInfinity`添加为分割的外部边界，以防止潜在的Bucketizer边界异常。在下面的示例中，我们指定了六个分割，然后为数据集中的`num_lab_procedures`特征（值在`1`到`126`之间）定义了一个`bucketizer`，如下所示：
- en: '[PRE47]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](img/00201.gif)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00201.gif)'
- en: Using VectorSlicer
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用VectorSlicer
- en: A `VectorSlicer` is a Transformer that takes a feature vector and returns a
    new feature vector that is a subset of the original features. It is useful for
    extracting features from a vector column. We can use a `VectorSlicer` to test
    our model with different numbers and combinations of features.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorSlicer`是一个转换器，它接受一个特征向量并返回原始特征的子集的新特征向量。它用于从向量列中提取特征。我们可以使用`VectorSlicer`来测试我们的模型使用不同数量和组合的特征。'
- en: 'In the following example, we use four features initially, and then drop one
    of them. These slices of features can be used to test the importance of including/excluding
    features from the set of features:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们最初使用四个特征，然后放弃其中一个。这些特征切片可用于测试包括/排除特征对于特征集的重要性：
- en: '[PRE48]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Using Chi-squared selector
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卡方选择器
- en: '`ChiSqSelector` enables chi-squared feature selection. It operates on labeled
    data with categorical features. `ChiSqSelector` uses the chi-squared test of independence
    to choose the features. In our example, we use the `numTopFeatures` to choose
    a fixed number of top features that yield features with the most predictive power:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChiSqSelector`启用卡方特征选择。它在具有分类特征的标记数据上运行。`ChiSqSelector`使用独立性的卡方检验来选择特征。在我们的示例中，我们使用`numTopFeatures`来选择一定数量的具有最大预测能力的顶级特征：'
- en: '[PRE49]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](img/00202.jpeg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00202.jpeg)'
- en: Using a Normalizer
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用标准化器
- en: 'We can normalize the data using a Normalizer object (a Transformer). The input
    to a Normalizer is a column created by the `VectorAssembler.` It normalizes the
    values in the column to produce a new column containing the normalized values.a
    new feature vector with a subarray of the original features. It is useful for
    extracting features from a vector column This normalization can help standardize
    input data and improve the behavior of learning algorithms:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`Normalizer`对象（一个转换器）对数据进行标准化。`Normalizer`的输入是由`VectorAssembler`创建的列。它将列中的值标准化，产生一个包含标准化值的新列。
- en: '[PRE50]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Retrieving our original labels
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索我们的原始标签
- en: '`IndexToString` is the reverse operation of `StringIndexer` that converts the
    indices back to their original labels. The random forests transform method of
    the model produced by the `RandomForestClassifier` produces a prediction column
    that contains indexed labels that we need unindexed to retrieve the original label
    values, as shown:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`IndexToString`是`StringIndexer`的反向操作，将索引转换回它们的原始标签。由`RandomForestClassifier`生成的模型的随机森林变换方法产生一个包含索引标签的预测列，我们需要对其进行非索引化以检索原始标签值，如下所示：'
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In the next section, we will switch focus to presenting an example of Spark
    ML clustering using the k-means algorithm.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将转向介绍使用k-means算法的Spark ML聚类的示例。
- en: Implementing a Spark ML clustering model
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施Spark ML聚类模型
- en: In this section, we will explain clustering with Spark ML. We will use a publicly
    available Dataset about the student's knowledge status about a subject.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释使用Spark ML进行聚类。我们将使用一个关于学生对某一主题的知识状态的公开可用数据集。
- en: The Dataset is available for download from the UCI website at [https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可从UCI网站下载，网址为[https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling)。
- en: 'The attributes of the records contained in the Dataset have reproduced here
    from the UCI website mentioned previously for reference:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中包含的记录属性已从前面提到的UCI网站复制在这里供参考：
- en: '**STG**: The degree of study time for goal object materials (input value)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**STG**：目标对象材料的学习时间程度（输入值）'
- en: '**SCG**: The degree of repetition number of users for goal object materials
    (input value)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SCG**：用户对目标对象材料的重复次数程度（输入值）'
- en: '**STR**: The degree of study time of users for related objects with the goal
    object (input value)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**STR**：用户对目标对象相关对象的学习时间程度（输入值）'
- en: '**LPR**: The exam performance of a user for related objects with the goal object
    (input value)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LPR**：用户对目标对象相关对象的考试表现（输入值）'
- en: '**PEG**: The exam performance of a user for goal objects (input value)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PEG**：用户对目标对象（输入值）的考试表现'
- en: '**UNS**: The knowledge level of the user (target value)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UNS**：用户的知识水平（目标值）'
- en: 'First, we will write a UDF to create two levels representing the two categories
    of the students--beneath average and beyond average from the five contained in
    the original Dataset. We combine the training and test CSV files to obtain a sufficient
    number of input records:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将编写一个UDF来创建两个级别，表示学生的两个类别——低于平均水平和高于平均水平，从原始数据集中包含的五个类别中。我们将训练和测试CSV文件合并，以获得足够数量的输入记录：
- en: '[PRE52]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We will read the input Dataset and create a column called `label` that is populated
    by the UDF. This allows us to have nearly equal numbers of records for each category:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将读取输入数据集，并创建一个名为`label`的列，由UDF填充。这样可以让我们每个类别的记录数几乎相等：
- en: '[PRE53]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, we cast the numeric fields to `Double` values, verify the number of records
    in the DataFrame, display a few sample records, and print the schema, as shown:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把数字字段转换为`Double`值，验证DataFrame中的记录数，显示一些样本记录，并打印模式，如下所示：
- en: '[PRE54]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, we will use `VectorAssembler` to create the features column, as shown:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`VectorAssembler`来创建特征列，如下所示：
- en: '[PRE55]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'You can use `explainParams` to list the details of the k-means model, as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`explainParams`来列出k-means模型的详细信息，如下所示：
- en: '[PRE56]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Evaluate the quality of clustering by computing the **Within Set Sum of Squared
    Errors** (**WSSSE**). The standard k-means algorithm aims at minimizing the sum
    of squares of the distance between the centroid to the points in each cluster.
    Increasing the value of `k` can reduce this error. The optimal `k` is usually
    one where there is an "elbow" in the WSSSE graph:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算**平方误差总和**（**WSSSE**）来评估聚类的质量。标准的k-means算法旨在最小化每个簇中点到质心的距离的平方和。增加`k`的值可以减少这种误差。通常，最佳的`k`是WSSSE图中出现“拐点”的地方：
- en: '[PRE57]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Here, we will compute the number of differences between the labels and the
    predicted values in our Dataset:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将计算数据集中标签和预测值之间的差异数量：
- en: '[PRE58]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, we will separate the DataFrames containing the prediction values of `0`
    and `1` to display sample records:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将分开包含预测值为`0`和`1`的DataFrames以显示样本记录：
- en: '[PRE59]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '![](img/00203.gif)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00203.gif)'
- en: '[PRE60]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![](img/00204.gif)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00204.gif)'
- en: 'We can also use `describe` to see the summary statistics for each of the predicted
    labels, as illustrated:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`describe`来查看每个预测标签的摘要统计信息，如下所示：
- en: '[PRE61]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](img/00205.gif)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00205.gif)'
- en: '[PRE62]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![](img/00206.gif)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00206.gif)'
- en: '[PRE63]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Next, we will feed in a couple of test input records, and the model will predict
    the cluster for them:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将输入一些测试输入记录，模型将预测它们的簇：
- en: '[PRE64]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Topics, such as streaming ML applications and architectures for large-scale
    processing, will be covered in detail in [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c),
    *Spark SQL in Large-Scale Application Architectures*.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 关于流式机器学习应用程序和大规模处理架构等主题，将在[第12章](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c)中详细介绍，*大规模应用架构中的Spark
    SQL*。
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced machine learning applications. We covered one
    of the most important topics in machine learning, called feature engineering.
    Additionally, we provided code examples using Spark ML APIs to build a classification
    pipeline and a clustering application. Additionally, we also introduced a few
    tools and utilities that can help select features and build models more easily
    and efficiently.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了机器学习应用程序。我们涵盖了机器学习中最重要的一个主题，称为特征工程。此外，我们提供了使用Spark ML API构建分类管道和聚类应用程序的代码示例。此外，我们还介绍了一些工具和实用程序，可以帮助更轻松、更高效地选择特征和构建模型。
- en: In the next chapter, we will introduce GraphFrame applications and provide examples
    of using Spark SQL DataFrame/Dataset APIs to build graph applications. We will
    also apply various graph algorithms to graph applications.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍GraphFrame应用程序，并提供使用Spark SQL DataFrame/Dataset API构建图应用程序的示例。我们还将对图应用程序应用各种图算法。
