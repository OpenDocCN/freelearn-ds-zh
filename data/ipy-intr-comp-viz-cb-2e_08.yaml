- en: Chapter 8. Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 机器学习
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Getting started with scikit-learn
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用scikit-learn
- en: Predicting who will survive on the Titanic with logistic regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑回归预测谁能在泰坦尼克号上生还
- en: Learning to recognize handwritten digits with a K-nearest neighbors classifier
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K近邻分类器学习识别手写数字
- en: Learning from text – Naive Bayes for Natural Language Processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本中学习 – 使用朴素贝叶斯进行自然语言处理
- en: Using support vector machines for classification tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用支持向量机进行分类任务
- en: Using a random forest to select important features for regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林选择回归任务中的重要特征
- en: Reducing the dimensionality of a dataset with a Principal Component Analysis
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主成分分析（PCA）降低数据集的维度
- en: Detecting hidden structures in a dataset with clustering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聚类方法发现数据集中的隐藏结构
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we were interested in getting insight into data, understanding
    complex phenomena through partial observations, and making informed decisions
    in the presence of uncertainty. Here, we are still interested in analyzing and
    processing data using statistical tools. However, the goal is not necessarily
    to *understand* the data, but to *learn* from it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们关注的是通过部分观察获得数据的洞察、理解复杂现象，并在不确定性面前做出明智决策。在这里，我们仍然关注使用统计工具分析和处理数据。然而，目标不一定是*理解*数据，而是*从数据中学习*。
- en: Learning from data is close to what we do as humans. From our experience, we
    intuitively learn general facts and relations about the world, even if we don't
    fully understand their complexity. The increasing computational power of computers
    makes them able to learn from data too. That's the heart of **machine learning**,
    a modern and fascinating branch of artificial intelligence, computer science,
    statistics, and applied mathematics. For more information on machine learning,
    refer to [http://en.wikipedia.org/wiki/Machine_learning](http://en.wikipedia.org/wiki/Machine_learning).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中学习类似于我们人类的学习方式。从经验中，我们直觉地学习世界的普遍事实和关系，尽管我们未必完全理解其复杂性。随着计算机计算能力的增强，它们也能从数据中学习。这就是**机器学习**的核心，机器学习是人工智能、计算机科学、统计学和应用数学中的一个现代而迷人的分支。有关机器学习的更多信息，请参考
    [http://en.wikipedia.org/wiki/Machine_learning](http://en.wikipedia.org/wiki/Machine_learning)。
- en: This chapter is a hands-on introduction to some of the most basic methods in
    machine learning. These methods are routinely used by data scientists. We will
    use these methods with **scikit-learn**, a popular and user-friendly Python package
    for machine learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是对机器学习一些最基础方法的实践介绍。这些方法是数据科学家日常使用的。我们将使用**scikit-learn**，一个流行且用户友好的Python机器学习包，来运用这些方法。
- en: A bit of vocabulary
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些词汇
- en: In this introduction, we will explain the fundamental definitions and concepts
    of machine learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本介绍中，我们将解释机器学习的基本定义和概念。
- en: Learning from data
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从数据中学习
- en: In machine learning, most data can be represented as a table of numerical values.
    Every row is called an **observation**, a **sample**, or a **data point**. Every
    column is called a **feature** or a **variable**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，大部分数据可以表示为一个数值表格。每一行称为**观察值**、**样本**或**数据点**。每一列称为**特征**或**变量**。
- en: Let's call *N* the number of rows (or the number of points) and *D* the number
    of columns (or number of features). The number *D* is also called the **dimensionality**
    of the data. The reason is that we can view this table as a set *E* of **vectors**
    in a space with *D* dimensions (or **vector space**). Here, a vector **x** contains
    *D* numbers *(x[1], ..., x[D])*, also called **components**. This mathematical
    point of view is very useful and we will use it throughout this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 设*N*为行数（或数据点的数量），*D*为列数（或特征的数量）。数字 *D* 也被称为数据的**维度**。原因是我们可以将这个表格视为一个在 *D* 维空间中的**向量集**
    *E*（或**向量空间**）。在这里，一个向量 **x** 包含 *D* 个数字 *(x[1], ..., x[D])*，也叫做**分量**。这种数学视角非常有用，我们将在本章中持续使用它。
- en: 'We generally make the distinction between supervised learning and unsupervised
    learning:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常区分监督学习和无监督学习：
- en: '**Supervised learning** is when we have a **label** *y* associated with a data
    point *x*. The goal is to *learn* the mapping from *x* to *y* from our data. The
    data gives us this mapping for a finite set of points, but what we want is to
    *generalize* this mapping to the full set *E*.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**是指我们有一个与数据点*x*相关联的**标签***y*。目标是从我们的数据中*学习*从*x*到*y*的映射。数据为我们提供了一个有限点集的映射，但我们希望将这个映射*泛化*到整个集合*E*。'
- en: '**Unsupervised learning** is when we don''t have any labels. What we want to
    do is discover some form of hidden structure in the data.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**是指我们没有任何标签。我们希望做的是发现数据中的某种隐藏结构。'
- en: Supervised learning
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'Mathematically, supervised learning consists of finding a function *f* that
    maps the set of points *E* to a set of labels *F*, knowing a finite set of associations
    *(x, y)*, which is given by our data. This is what *generalization* is about:
    after observing the pairs *(x[i], y[i])*, given a new *x*, we are able to find
    the corresponding *y* by applying the function *f* to *x*. For more information
    on supervised learning, refer to [http://en.wikipedia.org/wiki/Supervised_learning](http://en.wikipedia.org/wiki/Supervised_learning).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，监督学习包括找到一个函数*f*，将点集*E*映射到标签集*F*，并且已知一个有限的关联集*(x, y)*，这些数据来自我们的数据集。这就是*泛化*的意义所在：在观察了对*(x[i],
    y[i])*后，给定一个新的*x*，我们能够通过将函数*f*应用到*x*上来找到相应的*y*。关于监督学习的更多信息，请参阅[http://en.wikipedia.org/wiki/Supervised_learning](http://en.wikipedia.org/wiki/Supervised_learning)。
- en: 'It is a common practice to split the set of data points into two subsets: the
    **training set** and the **test set**. We learn the function *f* on the training
    set and test it on the test set. This is essential when assessing the predictive
    power of a model. By training and testing a model on the same set, our model might
    not be able to generalize well. This is the fundamental concept of **overfitting**,
    which we will detail later in this chapter.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通常做法是将数据集分为两个子集：**训练集**和**测试集**。我们在训练集上学习函数*f*，并在测试集上进行测试。这对于评估模型的预测能力至关重要。如果在同一数据集上训练和测试模型，我们的模型可能无法很好地泛化。这就是**过拟合**的基本概念，我们将在本章后面详细讲解。
- en: We generally make the distinction between classification and regression, two
    particular instances of supervised learning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常区分分类和回归，它们是监督学习的两种特定实例。
- en: '**Classification** is when our labels *y* can only take a finite set of values
    (categories). Examples include:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类**是指我们的标签*y*只能取有限的值（类别）。例如：'
- en: '**Handwritten digit recognition**: *x* is an image with a handwritten digit;
    *y* is a digit between 0 and 9'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手写数字识别**：*x*是一个包含手写数字的图像；*y*是一个0到9之间的数字'
- en: '**Spam filtering**: *x* is an e-mail and *y* is 1 or 0, depending on whether
    that e-mail is spam or not'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**垃圾邮件过滤**：*x*是电子邮件，*y*是1或0，取决于该电子邮件是否是垃圾邮件'
- en: '**Regression** is when our labels *y* can take any real (continuous) value.
    Examples include:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**是指我们的标签*y*可以取任何实数（连续值）。例如：'
- en: Predicting stock market data
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股票市场数据预测
- en: Predicting sales
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 销售预测
- en: Detecting the age of a person from a picture
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图片中检测一个人的年龄
- en: 'A classification task yields a division of our space *E* in different regions
    (also called **partition**), each region being associated to one particular value
    of the label *y*. A regression task yields a mathematical model that associates
    a real number to any point *x* in the space *E*. This difference is illustrated
    in the following figure:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务将我们的空间*E*划分为不同的区域（也称为**划分**），每个区域与标签*y*的某个特定值相关联。回归任务则产生一个数学模型，将一个实数与空间*E*中的任何点*x*关联。这一差异可以通过下图来说明：
- en: '![Supervised learning](img/4818OS_08_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/4818OS_08_01.jpg)'
- en: Difference between classification and regression
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 分类与回归的区别
- en: Classification and regression can be combined. For example, in the **probit
    model**, although the dependent variable is binary (classification), the *probability*
    that this variable belongs to one category can also be modeled (regression). We
    will see an example in the recipe about logistic regression. For more information
    on the probit model, refer to [http://en.wikipedia.org/wiki/Probit_model](http://en.wikipedia.org/wiki/Probit_model).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 分类与回归可以结合使用。例如，在**probit模型**中，尽管因变量是二元的（分类），但该变量属于某一类别的*概率*也可以通过回归来建模。我们将在关于逻辑回归的示例中看到这一点。关于probit模型的更多信息，请参阅[http://en.wikipedia.org/wiki/Probit_model](http://en.wikipedia.org/wiki/Probit_model)。
- en: Unsupervised learning
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Broadly speaking, unsupervised learning helps us discover systemic structures
    in our data. This is harder to grasp than supervised learning, in that there is
    generally no precise question and answer. For more information on unsupervised
    learning, refer to [http://en.wikipedia.org/wiki/Unsupervised_learning](http://en.wikipedia.org/wiki/Unsupervised_learning).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 广义而言，非监督学习帮助我们发现数据中的系统性结构。这比监督学习更难理解，因为通常没有明确的问题和答案。欲了解更多非监督学习的信息，请参考[http://en.wikipedia.org/wiki/Unsupervised_learning](http://en.wikipedia.org/wiki/Unsupervised_learning)。
- en: 'Here are a few important terms related to unsupervised learning:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些与非监督学习相关的重要术语：
- en: '**Clustering**: Grouping similar points together within **clusters**'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：将相似的点聚集在**簇**内'
- en: '**Density estimation**: Estimating a probability density function that can
    explain the distribution of the data points'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密度估计**：估计一个概率密度函数，用以解释数据点的分布'
- en: '**Dimension reduction**: Getting a simple representation of high-dimensional
    data points by projecting them onto a lower-dimensional space (notably for **data
    visualization**)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：通过将数据点投影到低维空间，获取高维数据点的简单表示（特别是用于**数据可视化**）'
- en: '**Manifold learning**: Finding a low-dimensional manifold containing the data
    points (also known as **nonlinear dimension reduction**)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流形学习**：找到包含数据点的低维流形（也称为**非线性降维**）'
- en: Feature selection and feature extraction
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征选择和特征提取
- en: In a supervised learning context, when our data contains many features, it is
    sometimes necessary to choose a subset of them. The features we want to keep are
    those that are most relevant to our question. This is the problem of **feature
    selection**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习的背景下，当我们的数据包含许多特征时，有时需要选择其中的一个子集。我们想保留的特征是那些与我们问题最相关的特征。这就是**特征选择**的问题。
- en: Additionally, we might want to extract new features by applying complex transformations
    on our original dataset. This is **feature extraction**. For example, in computer
    vision, training a classifier directly on the pixels is not the most efficient
    method in general. We might want to extract the relevant points of interest or
    make appropriate mathematical transformations. These steps depend on our dataset
    and on the questions we want to answer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可能希望通过对原始数据集应用复杂的变换来提取新特征。这就是**特征提取**。例如，在计算机视觉中，直接在像素上训练分类器通常不是最有效的方法。我们可能希望提取相关的兴趣点或进行适当的数学变换。这些步骤取决于我们的数据集和我们希望回答的问题。
- en: For example, it is often necessary to preprocess the data before learning models.
    **Feature scaling** (or **data normalization**) is a common **preprocessing**
    step where features are linearly rescaled to fit in the range *[-1,1]* or *[0,1]*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在使用学习模型之前，通常需要对数据进行预处理。**特征缩放**（或**数据归一化**）是一个常见的**预处理**步骤，其中特征会线性地重新缩放，以适应范围*[-1,1]*或*[0,1]*。
- en: Feature extraction and feature selection involve a balanced combination of domain
    expertise, intuition, and mathematical methods. These early steps are crucial,
    and they might be even more important than the learning steps themselves. The
    reason is that the few dimensions that are relevant to our problem are generally
    hidden in the high dimensionality of our dataset. We need to uncover the low-dimensional
    structure of interest to improve the efficiency of the learning models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取和特征选择涉及领域专业知识、直觉和数学方法的平衡组合。这些早期步骤至关重要，可能比学习步骤本身还要重要。原因是与我们问题相关的少数维度通常隐藏在数据集的高维度中。我们需要揭示感兴趣的低维结构，以提高学习模型的效率。
- en: We will see a few feature selection and feature extraction methods in this chapter.
    Methods that are specific to signals, images, or sounds will be covered in [Chapter
    10](ch10.html "Chapter 10. Signal Processing"), *Signal Processing*, and [Chapter
    11](ch11.html "Chapter 11. Image and Audio Processing"), *Image and Audio Processing*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到一些特征选择和特征提取方法。特定于信号、图像或声音的方法将在[第10章](ch10.html "第10章. 信号处理")，*信号处理*，和[第11章](ch11.html
    "第11章. 图像和音频处理")，*图像和音频处理*中介绍。
- en: 'Here are a few further references:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些进一步的参考资料：
- en: Feature selection in scikit-learn, documented at [http://scikit-learn.org/stable/modules/feature_selection.html](http://scikit-learn.org/stable/modules/feature_selection.html)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在scikit-learn中的特征选择，文档说明请参见[http://scikit-learn.org/stable/modules/feature_selection.html](http://scikit-learn.org/stable/modules/feature_selection.html)
- en: Feature selection on Wikipedia at [http://en.wikipedia.org/wiki/Feature_selection](http://en.wikipedia.org/wiki/Feature_selection)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的特征选择 [http://en.wikipedia.org/wiki/Feature_selection](http://en.wikipedia.org/wiki/Feature_selection)
- en: Overfitting, underfitting, and the bias-variance tradeoff
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合、欠拟合与偏差-方差权衡
- en: A central notion in machine learning is the trade-off between **overfitting**
    and **underfitting**. A model may be able to represent our data accurately. However,
    if it is *too* accurate, it might not generalize well to unobserved data. For
    example, in facial recognition, a too-accurate model would be unable to identify
    someone who styled their hair differently that day. The reason is that our model
    might learn irrelevant features in the training data. On the contrary, an insufficiently
    trained model would not generalize well either. For example, it would be unable
    to correctly recognize twins. For more information on overfitting, refer to [http://en.wikipedia.org/wiki/Overfitting](http://en.wikipedia.org/wiki/Overfitting).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的一个核心概念是**过拟合**与**欠拟合**之间的权衡。一个模型可能能够准确地表示我们的数据。然而，如果它*过于*准确，它可能无法很好地推广到未见过的数据。例如，在面部识别中，过于精确的模型可能无法识别当天发型不同的人。原因是我们的模型可能会在训练数据中学习到无关的特征。相反，一个训练不足的模型也无法很好地推广。例如，它可能无法正确识别双胞胎。有关过拟合的更多信息，请参考
    [http://en.wikipedia.org/wiki/Overfitting](http://en.wikipedia.org/wiki/Overfitting)。
- en: A popular solution to reduce overfitting consists of adding *structure* to the
    model, for example, with **regularization**. This method favors simpler models
    during training (Occam's razor). You will find more information at [http://en.wikipedia.org/wiki/Regularization_%28mathematics%29](http://en.wikipedia.org/wiki/Regularization_%28mathematics%29).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个减少过拟合的常见方法是向模型中添加*结构*，例如，通过**正则化**。这种方法在训练过程中倾向于选择更简单的模型（奥卡姆剃刀原则）。你可以在 [http://en.wikipedia.org/wiki/Regularization_%28mathematics%29](http://en.wikipedia.org/wiki/Regularization_%28mathematics%29)
    找到更多信息。
- en: The **bias-variance dilemma** is closely related to the issue of overfitting
    and underfitting. The **bias** of a model quantifies how precise it is across
    training sets. The **variance** quantifies how sensitive the model is to small
    changes in the training set. A **robust** model is not overly sensitive to small
    changes. The dilemma involves minimizing both bias and variance; we want a precise
    and robust model. Simpler models tend to be less accurate but more robust. Complex
    models tend to be more accurate but less robust. For more information on the bias-variance
    dilemma, refer to [http://en.wikipedia.org/wiki/Bias-variance_dilemma](http://en.wikipedia.org/wiki/Bias-variance_dilemma).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差-方差困境**与过拟合和欠拟合问题密切相关。一个模型的**偏差**量化了它在多个训练集上的准确性。**方差**量化了模型对训练集小变化的敏感性。一个**鲁棒**的模型不会对小的变化过于敏感。这个困境涉及到同时最小化偏差和方差；我们希望模型既精确又鲁棒。简单的模型通常不太准确，但更鲁棒；复杂的模型往往更准确，但鲁棒性差。有关偏差-方差困境的更多信息，请参考
    [http://en.wikipedia.org/wiki/Bias-variance_dilemma](http://en.wikipedia.org/wiki/Bias-variance_dilemma)。'
- en: The importance of this trade-off cannot be overstated. This question pervades
    the entire discipline of machine learning. We will see concrete examples in this
    chapter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个权衡的重要性无法被过分强调。这个问题贯穿了整个机器学习领域。我们将在本章中看到具体的例子。
- en: Model selection
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型选择
- en: As we will see in this chapter, there are many supervised and unsupervised algorithms.
    For example, well-known classifiers that we will cover in this chapter include
    logistic regression, nearest-neighbors, Naive Bayes, and support vector machines.
    There are many other algorithms that we can't cover here.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中所看到的，存在许多监督学习和无监督学习的算法。例如，本章将讨论一些著名的分类器，包括逻辑回归、最近邻、朴素贝叶斯和支持向量机。还有许多其他算法我们无法在这里讨论。
- en: No model performs uniformly better than the others. One model may perform well
    on one dataset and badly on another. This is the question of **model selection**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何模型能在所有情况下都表现得比其他模型更好。一个模型可能在某个数据集上表现良好，而在另一个数据集上表现差。这就是**模型选择**的问题。
- en: We will see systematic methods to assess the quality of a model on a particular
    dataset (notably cross-validation). In practice, machine learning is not an "exact
    science" in that it frequently involves trial and error. We need to try different
    models and empirically choose the one that performs best.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到一些系统化的方法来评估模型在特定数据集上的质量（特别是交叉验证）。实际上，机器学习并不是一门“精确的科学”，因为它通常涉及试错过程。我们需要尝试不同的模型，并通过经验选择表现最好的那个。
- en: That being said, understanding the details of the learning models allows us
    to gain intuition about which model is best adapted to our current problem.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，理解学习模型的细节使我们能够直观地了解哪个模型最适合当前问题。
- en: 'Here are a few references on this question:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于这个问题的参考资料：
- en: Model selection on Wikipedia, available at [http://en.wikipedia.org/wiki/Model_selection](http://en.wikipedia.org/wiki/Model_selection)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的模型选择，详见 [http://en.wikipedia.org/wiki/Model_selection](http://en.wikipedia.org/wiki/Model_selection)
- en: Model evaluation in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 文档中的模型评估，详见 [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html)
- en: Blog post on how to choose a classifier, available at [http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于如何选择分类器的博客文章，详见 [http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/)
- en: Machine learning references
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习参考资料
- en: 'Here are a few excellent, math-heavy textbooks on machine learning:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些优秀的、数学内容较多的机器学习教科书：
- en: '*Pattern Recognition and Machine Learning*, *Christopher M. Bishop*, *(2006)*,
    *Springer*'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模式识别与机器学习*，*Christopher M. Bishop*，*(2006)*，*Springer*'
- en: '*Machine Learning – A Probabilistic Perspective*, *Kevin P. Murphy*, *(2012)*,
    *MIT Press*'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习——一种概率视角*，*Kevin P. Murphy*，*(2012)*，*MIT Press*'
- en: '*The Elements of Statistical Learning*, *Trevor Hastie*, *Robert Tibshirani*,
    *Jerome Friedman*, *(2009)*, *Springer*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*统计学习的元素*，*Trevor Hastie*，*Robert Tibshirani*，*Jerome Friedman*，*(2009)*，*Springer*'
- en: 'Here are a few books more oriented toward programmers without a strong mathematical
    background:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些更适合没有强数学背景的程序员的书籍：
- en: '*Machine Learning for Hackers*, *Drew Conway*, *John Myles White*, *(2012)*,
    *O''Reilly Media*'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*黑客的机器学习*，*Drew Conway*，*John Myles White*，*(2012)*，*O''Reilly Media*'
- en: '*Machine Learning in Action*, *Peter Harrington*, (2012), *Manning Publications
    Co.*'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习实战*，*Peter Harrington*，(2012)，*Manning Publications Co.*'
- en: You will find many other references online.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在线找到更多的参考资料。
- en: Important classes of machine learning methods that we couldn't cover in this
    chapter include neural networks and deep learning. Deep learning is the subject
    of very active research in machine learning. Many state-of-the-art results are
    currently achieved by using deep learning methods. For more information on deep
    learning, refer to [http://en.wikipedia.org/wiki/Deep_learning](http://en.wikipedia.org/wiki/Deep_learning).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们未能涵盖的机器学习方法的重要类别包括神经网络和深度学习。深度学习是机器学习中一个非常活跃的研究领域。许多最先进的成果目前都是通过使用深度学习方法实现的。有关深度学习的更多信息，请参见
    [http://en.wikipedia.org/wiki/Deep_learning](http://en.wikipedia.org/wiki/Deep_learning)。
- en: Getting started with scikit-learn
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 scikit-learn
- en: In this recipe, we introduce the basics of the machine learning **scikit-learn**
    package ([http://scikit-learn.org](http://scikit-learn.org)). This package is
    the main tool we will use throughout this chapter. Its clean API makes it really
    easy to define, train, and test models. Plus, scikit-learn is specifically designed
    for speed and (relatively) big data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们介绍了机器学习 **scikit-learn** 包的基础知识 ([http://scikit-learn.org](http://scikit-learn.org))。这个包是我们在本章中将要使用的主要工具。它简洁的
    API 使得定义、训练和测试模型变得非常容易。而且，scikit-learn 专为速度和（相对）大数据而设计。
- en: We will show here a very basic example of linear regression in the context of
    curve fitting. This toy example will allow us to illustrate key concepts such
    as linear models, overfitting, underfitting, regularization, and cross-validation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里展示一个非常基础的线性回归示例，应用于曲线拟合的背景下。这个玩具示例将帮助我们说明关键概念，如线性模型、过拟合、欠拟合、正则化和交叉验证。
- en: Getting ready
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You can find all instructions to install scikit-learn in the main documentation.
    For more information, refer to [http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html).
    With anaconda, you can type `conda install scikit-learn` in a terminal.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在主文档中找到安装 scikit-learn 的所有指令。更多信息，请参考 [http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html)。使用
    anaconda 时，你可以在终端中输入 `conda install scikit-learn`。
- en: How to do it...
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We will generate a one-dimensional dataset with a simple model (including some
    noise), and we will try to fit a function to this data. With this function, we
    can predict values on new data points. This is a **curve fitting regression**
    problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成一个一维数据集，使用一个简单的模型（包括一些噪声），并尝试拟合一个函数到这些数据上。通过这个函数，我们可以对新的数据点进行预测。这是一个**曲线拟合回归**问题。
- en: 'First, let''s make all the necessary imports:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们进行所有必要的导入：
- en: '[PRE0]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We now define a deterministic nonlinear function underlying our generative
    model:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在定义一个确定性的非线性函数，作为我们生成模型的基础：
- en: '[PRE1]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We generate the values along the curve on *[0,2]*:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们生成在 *[0,2]* 范围内的曲线值：
- en: '[PRE2]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s generate data points within *[0,1]*. We use the function *f* and
    we add some Gaussian noise:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们生成在 *[0,1]* 范围内的数据点。我们使用函数 *f* 并加入一些高斯噪声：
- en: '[PRE3]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s plot our data points on *[0,1]*:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在 *[0,1]* 范围内绘制我们的数据点：
- en: '[PRE4]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![How to do it...](img/4818OS_08_02.jpg)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_02.jpg)'
- en: In the image, the dotted curve represents the generative model.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在图像中，虚线曲线表示生成模型。
- en: Now, we use scikit-learn to fit a linear model to the data. There are three
    steps. First, we create the model (an instance of the `LinearRegression` class).
    Then, we fit the model to our data. Finally, we predict values from our trained
    model.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用scikit-learn将线性模型拟合到数据上。这个过程有三个步骤。首先，我们创建模型（`LinearRegression` 类的一个实例）。然后，我们将模型拟合到我们的数据上。最后，我们从训练好的模型中预测值。
- en: '[PRE5]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We need to convert `x` and `x_tr` to column vectors, as it is a general convention
    in scikit-learn that observations are rows, while features are columns. Here,
    we have seven observations with one feature.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们需要将`x`和`x_tr`转换为列向量，因为在scikit-learn中，一般约定观测值是行，特征是列。在这里，我们有七个观测值，每个观测值有一个特征。
- en: 'We now plot the result of the trained linear model. We obtain a regression
    line in green here:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在绘制训练后的线性模型结果。这里我们得到了一条绿色的回归线：
- en: '[PRE6]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![How to do it...](img/4818OS_08_03.jpg)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_03.jpg)'
- en: 'The linear fit is not well-adapted here, as the data points are generated according
    to a nonlinear model (an exponential curve). Therefore, we are now going to fit
    a nonlinear model. More precisely, we will fit a polynomial function to our data
    points. We can still use linear regression for this, by precomputing the exponents
    of our data points. This is done by generating a **Vandermonde matrix**, using
    the `np.vander` function. We will explain this trick in *How it works…*. In the
    following code, we perform and plot the fit:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性拟合在这里并不适用，因为数据点是根据非线性模型（指数曲线）生成的。因此，我们现在将拟合一个非线性模型。更准确地说，我们将为数据点拟合一个多项式函数。我们仍然可以使用线性回归来做到这一点，方法是预先计算数据点的指数。这是通过生成**范德蒙德矩阵**来完成的，使用的是`np.vander`函数。我们将在*它是如何工作的...*部分解释这个技巧。在下面的代码中，我们执行并绘制拟合结果：
- en: '[PRE7]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![How to do it...](img/4818OS_08_04.jpg)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_04.jpg)'
- en: We have fitted two polynomial models of degree 2 and 5\. The degree 2 polynomial
    appears to fit the data points less precisely than the degree 5 polynomial. However,
    it seems more robust; the degree 5 polynomial seems really bad at predicting values
    outside the data points (look for example at the *x* ![How to do it...](img/4818OS_08_41.jpg)
    * 1* portion). This is what we call overfitting; by using a too-complex model,
    we obtain a better fit on the trained dataset, but a less robust model outside
    this set.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们拟合了两个多项式模型，分别是2次和5次的多项式。2次多项式似乎拟合数据点的精度不如5次多项式。然而，它似乎更稳健；5次多项式在预测数据点外的值时表现得非常差（例如，可以查看
    *x* ![如何操作...](img/4818OS_08_41.jpg) * 1* 部分）。这就是我们所说的过拟合；通过使用一个过于复杂的模型，我们在训练数据集上获得了更好的拟合，但在数据集外的模型却不够稳健。
- en: Note
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note the large coefficients of the degree 5 polynomial; this is generally a
    sign of overfitting.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意5次多项式的系数非常大；这通常是过拟合的一个标志。
- en: We will now use a different learning model called **ridge regression**. It works
    like linear regression except that it prevents the polynomial's coefficients from
    becoming too big. This is what happened in the previous example. By adding a **regularization**
    **term** in the **loss function**, ridge regression imposes some structure on
    the underlying model. We will see more details in the next section.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将使用一种不同的学习模型，称为**岭回归**。它与线性回归类似，不同之处在于它防止多项式系数变得过大。这正是前一个例子中发生的情况。通过在**损失函数**中添加**正则化****项**，岭回归对基础模型施加了一些结构。我们将在下一节中看到更多细节。
- en: The ridge regression model has a meta-parameter, which represents the weight
    of the regularization term. We could try different values with trial and error
    using the `Ridge` class. However, scikit-learn provides another model called `RidgeCV`,
    which includes a parameter search with **cross-validation**. In practice, this
    means that we don't have to tweak this parameter by hand—scikit-learn does it
    for us. As the models of scikit-learn always follow the fit-predict API, all we
    have to do is replace `lm.LinearRegression()` with `lm.RidgeCV()` in the previous
    code. We will give more details in the next section.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 岭回归模型有一个超参数，表示正则化项的权重。我们可以使用 `Ridge` 类通过反复试验尝试不同的值。然而，scikit-learn 提供了另一个叫做
    `RidgeCV` 的模型，它包括 **交叉验证**的参数搜索。实际上，这意味着我们无需手动调整该参数——scikit-learn 会为我们完成。由于 scikit-learn
    的模型始终遵循 fit-predict API，我们只需在之前的代码中将 `lm.LinearRegression()` 替换为 `lm.RidgeCV()`。我们将在下一节中提供更多细节。
- en: '[PRE8]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![How to do it...](img/4818OS_08_05.jpg)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何实现...](img/4818OS_08_05.jpg)'
- en: This time, the degree 5 polynomial seems more precise than the simpler degree
    2 polynomial (which now causes **underfitting**). Ridge regression mitigates the
    overfitting issue here. Observe how the degree 5 polynomial's coefficients are
    much smaller than in the previous example.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这次，5 次多项式似乎比简单的 2 次多项式更加精确（后者现在导致 **欠拟合**）。岭回归在此处缓解了过拟合问题。观察 5 次多项式的系数比前一个例子中要小得多。
- en: How it works...
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this section, we explain all the aspects covered in this recipe.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将解释本食谱中涉及的所有方面。
- en: The scikit-learn API
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: scikit-learn API
- en: scikit-learn implements a clean and coherent API for supervised and unsupervised
    learning. Our data points should be stored in a *(N,D)* matrix *X*, where *N*
    is the number of observations and *D* is the number of features. In other words,
    each row is an observation. The first step in a machine learning task is to define
    what the matrix *X* is exactly.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 为监督学习和无监督学习实现了一个简洁且一致的 API。我们的数据点应该存储在一个 *(N,D)* 矩阵 *X* 中，其中 *N*
    是观测值的数量，*D* 是特征的数量。换句话说，每一行都是一个观测值。机器学习任务的第一步是明确矩阵 *X* 的确切含义。
- en: In a supervised learning setup, we also have a *target*, an *N*-long vector
    *y* with a scalar value for each observation. This value is either continuous
    or discrete, depending on whether we have a regression or classification problem,
    respectively.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们还需要一个 *目标*，一个长度为 *N* 的向量 *y*，每个观测值对应一个标量值。这个值是连续的或离散的，具体取决于我们是回归问题还是分类问题。
- en: In scikit-learn, models are implemented in classes that have the `fit()` and
    `predict()` methods. The `fit()` method accepts the data matrix *X* as input,
    and *y* as well for supervised learning models. This method *trains* the model
    on the given data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，模型通过包含 `fit()` 和 `predict()` 方法的类来实现。`fit()` 方法接受数据矩阵 *X* 作为输入，对于监督学习模型，还接受
    *y*。此方法用于在给定数据上*训练*模型。
- en: The `predict()` method also takes data points as input (as a *(M,D)* matrix).
    It returns the labels or transformed points as predicted by the trained model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()` 方法也接受数据点作为输入（作为 *(M,D)* 矩阵）。它返回训练模型预测的标签或转换后的点。'
- en: Ordinary least squares regression
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 普通最小二乘回归
- en: '**Ordinary least squares regression** is one of the simplest regression methods.
    It consists of approaching the output values *y[i]* with a linear combination
    of *X[ij]*:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**普通最小二乘回归** 是最简单的回归方法之一。它通过 *X[ij]* 的线性组合来逼近输出值 *y[i]*：'
- en: '![Ordinary least squares regression](img/4818OS_08_06.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![普通最小二乘回归](img/4818OS_08_06.jpg)'
- en: 'Here, *w = (w[1], ..., w[D])* is the (unknown) **parameter vector**. Also,
    ![Ordinary least squares regression](img/4818OS_08_33.jpg) represents the model''s
    output. We want this vector to match the data points *y* as closely as possible.
    Of course, the exact equality ![Ordinary least squares regression](img/4818OS_08_34.jpg)
    cannot hold in general (there is always some noise and uncertainty—models are
    always idealizations of reality). Therefore, we want to *minimize* the difference
    between these two vectors. The ordinary least squares regression method consists
    of minimizing the following **loss function**:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w = (w[1], ..., w[D])* 是（未知的）**参数向量**。另外，![普通最小二乘回归](img/4818OS_08_33.jpg)
    代表模型的输出。我们希望这个向量与数据点 *y* 尽可能匹配。当然，精确的相等式 ![普通最小二乘回归](img/4818OS_08_34.jpg) 通常是不可能成立的（总会有一些噪声和不确定性——模型始终是对现实的理想化）。因此，我们希望*最小化*这两个向量之间的差异。普通最小二乘回归方法的核心是最小化以下
    **损失函数**：
- en: '![Ordinary least squares regression](img/4818OS_08_07.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![普通最小二乘回归](img/4818OS_08_07.jpg)'
- en: This sum of the components squared is called the **L²** **norm**. It is convenient
    because it leads to *differentiable* loss functions so that gradients can be computed
    and common optimization procedures can be performed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件的平方和称为**L²** **范数**。它之所以方便，是因为它导致了*可微分*的损失函数，从而可以计算梯度，并进行常见的优化过程。
- en: Polynomial interpolation with linear regression
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用线性回归的多项式插值
- en: Ordinary least squares regression fits a linear model to the data. The model
    is linear both in the data points *x[i]* and in the parameters *w[j]*. In our
    example, we obtain a poor fit because the data points were generated according
    to a nonlinear generative model (an exponential function).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 普通最小二乘回归将线性模型拟合到数据上。该模型在数据点*x[i]*和参数*w[j]*中都是线性的。在我们的例子中，由于数据点是根据非线性生成模型（一个指数函数）生成的，因此我们获得了较差的拟合。
- en: 'However, we can still use the linear regression method with a model that is
    linear in *w[j]* but nonlinear in *x[i]*. To do this, we need to increase the
    number of dimensions in our dataset by using a basis of polynomial functions.
    In other words, we consider the following data points:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然可以使用线性回归方法，模型在*w[j]*上是线性的，但在*x[i]*上是非线性的。为此，我们需要通过使用多项式函数的基来增加数据集的维度。换句话说，我们考虑以下数据点：
- en: '![Polynomial interpolation with linear regression](img/4818OS_08_08.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![使用线性回归的多项式插值](img/4818OS_08_08.jpg)'
- en: Here, *D* is the maximum degree. The input matrix *X* is therefore the **Vandermonde
    matrix** associated to the original data points *x[i]*. For more information on
    the Vandermonde matrix, refer to [http://en.wikipedia.org/wiki/Vandermonde_matrix](http://en.wikipedia.org/wiki/Vandermonde_matrix).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*D*是最大阶数。因此，输入矩阵*X*是与原始数据点*x[i]*相关的**范德蒙德矩阵**。有关范德蒙德矩阵的更多信息，请参见[http://en.wikipedia.org/wiki/Vandermonde_matrix](http://en.wikipedia.org/wiki/Vandermonde_matrix)。
- en: Here, it is easy to see that training a linear model on these new data points
    is equivalent to training a polynomial model on the original data points.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，很容易看出，在这些新数据点上训练线性模型等同于在原始数据点上训练多项式模型。
- en: Ridge regression
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 岭回归
- en: Polynomial interpolation with linear regression can lead to overfitting if the
    degree of the polynomials is too large. By capturing the random fluctuations (noise)
    instead of the general trend of the data, the model loses some of its predictive
    power. This corresponds to a divergence of the polynomial's coefficients *w[j]*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性回归的多项式插值如果多项式的阶数过大，可能导致过拟合。通过捕捉随机波动（噪声）而不是数据的一般趋势，模型失去了部分预测能力。这对应于多项式系数*w[j]*的发散。
- en: A solution to this problem is to prevent these coefficients from growing unboundedly.
    With **ridge regression** (also known as **Tikhonov regularization**), this is
    done by adding a *regularization* term to the loss function. For more details
    on Tikhonov regularization, refer to [http://en.wikipedia.org/wiki/Tikhonov_regularization](http://en.wikipedia.org/wiki/Tikhonov_regularization).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是防止这些系数无限增大。通过**岭回归**（也称为**Tikhonov正则化**），这是通过向损失函数中添加*正则化*项来实现的。有关Tikhonov正则化的更多信息，请参见[http://en.wikipedia.org/wiki/Tikhonov_regularization](http://en.wikipedia.org/wiki/Tikhonov_regularization)。
- en: '![Ridge regression](img/4818OS_08_09.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![岭回归](img/4818OS_08_09.jpg)'
- en: By minimizing this loss function, we not only minimize the error between the
    model and the data (first term, related to the bias), but also the size of the
    model's coefficients (second term, related to the variance). The bias-variance
    trade-off is quantified by the hyperparameter ![Ridge regression](img/4818OS_08_43.jpg),
    which specifies the relative weight between the two terms in the loss function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化这个损失函数，我们不仅最小化了模型与数据之间的误差（第一项，与偏差相关），还最小化了模型系数的大小（第二项，与方差相关）。偏差-方差权衡通过超参数![岭回归](img/4818OS_08_43.jpg)量化，它指定了损失函数中两项之间的相对权重。
- en: Here, ridge regression led to a polynomial with smaller coefficients, and thus
    a better fit.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，岭回归导致了具有较小系数的多项式，因此得到了更好的拟合。
- en: Cross-validation and grid search
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉验证和网格搜索
- en: A drawback of the ridge regression model compared to the ordinary least squares
    model is the presence of an extra hyperparameter ![Cross-validation and grid search](img/4818OS_08_43.jpg).
    The quality of the prediction depends on the choice of this parameter. One possibility
    would be to fine-tune this parameter manually, but this procedure can be tedious
    and can also lead to overfitting problems.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归模型相较于普通最小二乘法模型的一个缺点是多了一个额外的超参数 ![交叉验证与网格搜索](img/4818OS_08_43.jpg)。预测的质量取决于该参数的选择。一种可能的做法是手动微调这个参数，但这个过程可能非常繁琐，并且可能会导致过拟合问题。
- en: To solve this problem, we can use a **grid search**; we loop over many possible
    values for ![Cross-validation and grid search](img/4818OS_08_43.jpg), and we evaluate
    the performance of the model for each possible value. Then, we choose the parameter
    that yields the best performance.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以使用 **网格搜索**；我们遍历多个可能的 ![交叉验证与网格搜索](img/4818OS_08_43.jpg) 值，并评估每个可能值下模型的性能。然后，我们选择使性能最好的参数。
- en: How can we assess the performance of a model with a given ![Cross-validation
    and grid search](img/4818OS_08_43.jpg) value? A common solution is to use **cross-validation**.
    This procedure consists of splitting the dataset into a training set and a test
    set. We fit the model on the train set, and we test its predictive performance
    on the *test set*. By testing the model on a different dataset than the one used
    for training, we reduce overfitting.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何评估具有给定 ![交叉验证与网格搜索](img/4818OS_08_43.jpg) 值的模型性能？一种常见的解决方案是使用 **交叉验证**。该过程将数据集分为训练集和测试集。我们在训练集上拟合模型，然后在
    *测试集* 上测试其预测性能。通过在与训练集不同的数据集上测试模型，我们可以减少过拟合的风险。
- en: There are many ways to split the initial dataset into two parts like this. One
    possibility is to remove *one* sample to form the train set and to put this one
    sample into the test set. This is called **Leave-One-Out** cross-validation. With
    *N* samples, we obtain *N* sets of train and test sets. The cross-validated performance
    is the average performance on all these set decompositions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以将初始数据集分成两部分。一个方法是移除 *一个* 样本，形成训练集，并将这个样本放入测试集中。这被称为 **留一法** 交叉验证。对于 *N*
    个样本，我们将得到 *N* 组训练集和测试集。交叉验证性能是所有这些数据集划分的平均性能。
- en: As we will see later, scikit-learn implements several easy-to-use functions
    to do cross-validation and grid search. In this recipe, there exists a special
    estimator called `RidgeCV` that implements a cross-validation and grid search
    procedure that is specific to the ridge regression model. Using this class ensures
    that the best hyperparameter ![Cross-validation and grid search](img/4818OS_08_43.jpg)
    is found automatically for us.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们稍后所见，scikit-learn 实现了几个易于使用的函数，用于进行交叉验证和网格搜索。在这个示例中，存在一个名为`RidgeCV`的特殊估计器，它实现了特定于岭回归模型的交叉验证和网格搜索过程。使用这个类可以自动为我们找到最佳的超参数
    ![交叉验证与网格搜索](img/4818OS_08_43.jpg)。
- en: There's more…
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'Here are a few references about least squares:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于最小二乘法的参考资料：
- en: Ordinary least squares on Wikipedia, available at [http://en.wikipedia.org/wiki/Ordinary_least_squares](http://en.wikipedia.org/wiki/Ordinary_least_squares)
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的普通最小二乘法，详情请见 [http://en.wikipedia.org/wiki/Ordinary_least_squares](http://en.wikipedia.org/wiki/Ordinary_least_squares)
- en: Linear least squares on Wikipedia, available at [http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)](http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics))
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的线性最小二乘法，详情请见 [http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)](http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics))
- en: 'Here are a few references about cross-validation and grid search:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于交叉验证和网格搜索的参考资料：
- en: Cross-validation in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 文档中的交叉验证，详情请见 [http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html)
- en: Grid search in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/grid_search.html](http://scikit-learn.org/stable/modules/grid_search.html)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 文档中的网格搜索，详情请见 [http://scikit-learn.org/stable/modules/grid_search.html](http://scikit-learn.org/stable/modules/grid_search.html)
- en: Cross-validation on Wikipedia, available at [http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29)
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的交叉验证，详情请见 [http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29)
- en: 'Here are a few references about scikit-learn:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于 scikit-learn 的参考资料：
- en: scikit-learn basic tutorial available at [http://scikit-learn.org/stable/tutorial/basic/tutorial.html](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 基础教程，见 [http://scikit-learn.org/stable/tutorial/basic/tutorial.html](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)
- en: scikit-learn tutorial given at the SciPy 2013 conference, available at [https://github.com/jakevdp/sklearn_scipy2013](https://github.com/jakevdp/sklearn_scipy2013)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 在 SciPy 2013 年会上的教程，见 [https://github.com/jakevdp/sklearn_scipy2013](https://github.com/jakevdp/sklearn_scipy2013)
- en: See also
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: The *Using support vector machines for classification tasks* recipe
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用支持向量机进行分类任务* 的实例'
- en: Predicting who will survive on the Titanic with logistic regression
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归预测谁会在泰坦尼克号上生还
- en: In this recipe, we will introduce **logistic regression**, a basic classifier.
    We will also show how to perform a **grid search** with **cross-validation**.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们将介绍 **逻辑回归**，一种基本的分类器。我们还将展示如何使用 **网格搜索** 和 **交叉验证**。
- en: We will apply these techniques on a **Kaggle** dataset where the goal is to
    predict survival on the Titanic based on real data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 **Kaggle** 数据集上应用这些技术，该数据集的目标是根据真实数据预测泰坦尼克号上的生还情况。
- en: Tip
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Kaggle ([www.kaggle.com/competitions](http://www.kaggle.com/competitions)) hosts
    machine learning competitions where anyone can download a dataset, train a model,
    and test the predictions on the website. The author of the best model might even
    win a prize! It is a fun way to get started with machine learning.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle ([www.kaggle.com/competitions](http://www.kaggle.com/competitions)) 主办机器学习比赛，任何人都可以下载数据集，训练模型，并在网站上测试预测结果。最佳模型的作者甚至可能获得奖品！这是一个很好的入门机器学习的方式。
- en: Getting ready
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Download the *Titanic* dataset from the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从本书的 GitHub 仓库下载 *Titanic* 数据集，链接：[https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data)。
- en: The dataset has been obtained from [www.kaggle.com/c/titanic-gettingStarted](http://www.kaggle.com/c/titanic-gettingStarted).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集来源于 [www.kaggle.com/c/titanic-gettingStarted](http://www.kaggle.com/c/titanic-gettingStarted)。
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We import the standard packages:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入标准库：
- en: '[PRE9]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We load the training and test datasets with pandas:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 pandas 加载训练集和测试集：
- en: '[PRE10]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s keep only a few fields for this example, and also convert the `sex`
    field to a binary variable so that it can be handled correctly by NumPy and scikit-learn.
    Finally, we remove the rows that contain `NaN` values:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了简化示例，我们只保留少数几个字段，并将 `sex` 字段转换为二进制变量，以便它能被 NumPy 和 scikit-learn 正确处理。最后，我们移除包含
    `NaN` 值的行：
- en: '[PRE11]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we convert this `DataFrame` object to a NumPy array so that we can pass
    it to scikit-learn:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将 `DataFrame` 对象转换为 NumPy 数组，以便将其传递给 scikit-learn：
- en: '[PRE12]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s have a look at the survival of male and female passengers as a function
    of their age:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看男性和女性乘客的生还情况，按年龄分类：
- en: '[PRE13]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![How to do it...](img/4818OS_08_10.jpg)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/4818OS_08_10.jpg)'
- en: 'Let''s try to train a `LogisticRegression` classifier in order to predict the
    survival of people based on their gender, age, and class. We first need to create
    a train and a test dataset:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试训练一个 `LogisticRegression` 分类器，以预测人们基于性别、年龄和舱位的生还情况。我们首先需要创建一个训练集和一个测试集：
- en: '[PRE14]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We train the model and we get the predicted values on the test set:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练模型，并在测试集上获取预测值：
- en: '[PRE15]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following figure shows the actual and predicted results:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图显示了实际结果和预测结果：
- en: '[PRE16]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![How to do it...](img/4818OS_08_11.jpg)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/4818OS_08_11.jpg)'
- en: In this screenshot, the first line shows the survival of several people from
    the test set (white for survival, black otherwise). The second line shows the
    values predicted by the model.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个截图中，第一行显示了测试集中几个人的生还情况（白色表示生还，黑色表示未生还）。第二行显示了模型的预测值。
- en: 'To get an estimation of the model''s performance, we compute the cross-validation
    score with the `cross_val_score()` function. This function uses a three-fold stratified
    cross-validation procedure by default, but this can be changed with the `cv` keyword
    argument:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估模型的性能，我们使用 `cross_val_score()` 函数计算交叉验证分数。默认情况下，该函数使用三折分层交叉验证过程，但可以通过 `cv`
    关键字参数进行修改：
- en: '[PRE17]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This function returns, for each pair of train and test set, a prediction score
    (we give more details in *How it works…*).
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个函数返回每一对训练集和测试集的预测分数（我们在*如何运作…*中提供了更多细节）。
- en: 'The `LogisticRegression` class accepts a *C* hyperparameter as an argument.
    This parameter quantifies the regularization strength. To find a good value, we
    can perform a grid search with the generic `GridSearchCV` class. It takes an estimator
    as input and a dictionary of parameter values. This new estimator uses cross-validation
    to select the best parameter:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`LogisticRegression`类接受*C*超参数作为参数。该参数量化了正则化强度。为了找到合适的值，我们可以使用通用的`GridSearchCV`类进行网格搜索。它接受一个估算器作为输入以及一个包含参数值的字典。这个新的估算器使用交叉验证来选择最佳参数：'
- en: '[PRE18]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is the performance of the best estimator:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是最佳估算器的性能：
- en: '[PRE19]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Performance is slightly better after the *C* hyperparameter has been chosen
    with a grid search.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用*C*超参数通过网格搜索选择后，性能略有提高。
- en: How it works...
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Logistic regression is *not* a regression model, it is a classification model.
    Yet, it is closely related to linear regression. This model predicts the probability
    that a binary variable is 1, by applying a **sigmoid function** (more precisely,
    a logistic function) to a linear combination of the variables. The equation of
    the sigmoid is:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归*不是*一个回归模型，而是一个分类模型。然而，它与线性回归有着密切的关系。该模型通过将**sigmoid函数**（更准确地说是逻辑函数）应用于变量的线性组合，预测一个二元变量为1的概率。sigmoid函数的方程为：
- en: '![How it works...](img/4818OS_08_12.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![操作步骤...](img/4818OS_08_12.jpg)'
- en: 'The following figure shows a logistic function:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个逻辑函数：
- en: '![How it works...](img/4818OS_08_13.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![操作步骤...](img/4818OS_08_13.jpg)'
- en: A logistic function
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数
- en: If a binary variable has to be obtained, we can round the value to the closest
    integer.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要得到一个二元变量，我们可以将值四舍五入为最接近的整数。
- en: The parameter *w* is obtained with an optimization procedure during the learning
    step.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 参数*w*是在学习步骤中通过优化过程得到的。
- en: There's more...
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Here are a few references:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些参考资料：
- en: Logistic regression on Wikipedia, available at [http://en.wikipedia.org/wiki/Logistic_regression](http://en.wikipedia.org/wiki/Logistic_regression)
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的逻辑回归，网址：[http://en.wikipedia.org/wiki/Logistic_regression](http://en.wikipedia.org/wiki/Logistic_regression)
- en: Logistic regression in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn文档中的逻辑回归，网址：[http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
- en: See also
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The *Getting started with scikit-learn* recipe
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开始使用scikit-learn*的教程'
- en: The *Learning to recognize handwritten digits with a K-nearest neighbors classifier*
    recipe
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用K近邻分类器识别手写数字*的教程'
- en: The *Using support vector machines for classification tasks* recipe
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用支持向量机进行分类任务*的教程'
- en: Learning to recognize handwritten digits with a K-nearest neighbors classifier
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用K近邻分类器识别手写数字
- en: In this recipe, we will see how to recognize handwritten digits with a **K-nearest
    neighbors** (**K-NN**) classifier. This classifier is a simple but powerful model,
    well-adapted to complex, highly nonlinear datasets such as images. We will explain
    how it works later in this recipe.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将展示如何使用**K近邻**（**K-NN**）分类器来识别手写数字。这个分类器是一个简单但强大的模型，适用于复杂的、高度非线性的数据集，如图像。稍后我们将在本教程中详细解释它的工作原理。
- en: How to do it...
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'We import the modules:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入模块：
- en: '[PRE20]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s load the *digits* dataset, part of the `datasets` module of scikit-learn.
    This dataset contains handwritten digits that have been manually labeled:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载*digits*数据集，这是scikit-learn的`datasets`模块的一部分。这个数据集包含已手动标记的手写数字：
- en: '[PRE21]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the matrix `X`, each row contains *8 * 8=64* pixels (in grayscale, values
    between 0 and 16). The row-major ordering is used.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在矩阵`X`中，每一行包含*8 * 8=64*个像素（灰度值，范围在0到16之间）。使用行主序排列。
- en: 'Let''s display some of the images along with their labels:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们显示一些图像及其标签：
- en: '[PRE22]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![How to do it...](img/4818OS_08_14.jpg)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![操作步骤...](img/4818OS_08_14.jpg)'
- en: 'Now, let''s fit a K-nearest neighbors classifier on the data:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们对数据拟合一个K近邻分类器：
- en: '[PRE23]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s evaluate the score of the trained classifier on the test dataset:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在测试数据集上评估训练后的分类器的得分：
- en: '[PRE24]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now, let's see if our classifier can recognize a *handwritten* digit!
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的分类器能否识别*手写*数字！
- en: '[PRE25]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![How to do it...](img/4818OS_08_15.jpg)'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![操作步骤...](img/4818OS_08_15.jpg)'
- en: 'Can our model recognize this number? Let''s see:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的模型能识别这个数字吗？让我们来看看：
- en: '[PRE26]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Good job!
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 干得好！
- en: How it works...
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This example illustrates how to deal with images in scikit-learn. An image is
    a 2D *(N, M)* matrix, which has *NM* features. This matrix needs to be flattened
    when composing the data matrix; each row is a full image.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了如何处理scikit-learn中的图像。图像是一个2D的*(N, M)*矩阵，有*NM*个特征。在组合数据矩阵时，这个矩阵需要被展平；每一行都是一个完整的图像。
- en: 'The idea of K-nearest neighbors is as follows: given a new point in the feature
    space, find the *K* closest points from the training set and assign the label
    of the majority of those points.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: K最近邻算法的思想如下：在特征空间中给定一个新点，找到训练集中距离最近的*K*个点，并将它们中大多数点的标签赋给该新点。
- en: The distance is generally the Euclidean distance, but other distances can be
    used too.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 距离通常是欧氏距离，但也可以使用其他距离。
- en: 'The following image shows the space partition obtained with a 15-nearest-neighbors
    classifier on a toy dataset (with three labels):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了在玩具数据集上使用15最近邻分类器获得的空间划分（带有三个标签）：
- en: '![How it works...](img/4818OS_08_16.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![How it works...](img/4818OS_08_16.jpg)'
- en: K-nearest neighbors space partition
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: K最近邻空间划分
- en: The number *K* is a hyperparameter of the model. If it is too small, the model
    will not generalize well (high variance). In particular, it will be highly sensitive
    to outliers. By contrast, the precision of the model will worsen if *K* is too
    large. At the extreme, if *K* is equal to the total number of points, the model
    will always predict the exact same value disregarding the input (high bias). There
    are heuristics to choose this hyperparameter (see the next section).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 数字*K*是模型的一个超参数。如果*K*太小，模型泛化能力不好（高方差）。特别是，它对异常值非常敏感。相反，如果*K*太大，模型的精度会变差。在极端情况下，如果*K*等于总数据点数，模型将始终预测相同的值，而不考虑输入（高偏差）。有一些启发式方法来选择这个超参数（请参阅下一节）。
- en: It should be noted that no model is learned by a K-nearest neighbor algorithm;
    the classifier just stores all data points and compares any new target points
    with them. This is an example of **instance-based learning**. It is in contrast
    to other classifiers such as the logistic regression model, which explicitly learns
    a simple mathematical model on the training data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，K最近邻算法并没有学习任何模型；分类器只是存储所有数据点，并将任何新的目标点与它们进行比较。这是**基于实例的学习**的一个例子。这与其他分类器（如逻辑回归模型）形成对比，后者明确地在训练数据上学习一个简单的数学模型。
- en: The K-nearest neighbors method works well on complex classification problems
    that have irregular decision boundaries. However, it might be computationally
    intensive with large training datasets because a large number of distances have
    to be computed for testing. Dedicated tree-based data structures such as **K-D
    trees** or **ball trees** can be used to accelerate the search of nearest neighbors.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: K最近邻方法在复杂的分类问题上表现良好，这些问题具有不规则的决策边界。但是，对于大型训练数据集，它可能计算密集，因为必须为测试计算大量距离。可以使用专用的基于树的数据结构（如K-D树或球树）来加速最近邻的搜索。
- en: The K-nearest neighbors method can be used for classification, like here, and
    also for regression problems. The model assigns the average of the target value
    of the nearest neighbors. In both cases, different weighting strategies can be
    used.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: K最近邻方法可以用于分类（如本例）以及回归问题。该模型分配最近邻居的目标值的平均值。在这两种情况下，可以使用不同的加权策略。
- en: There's more…
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'Here are a few references:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些参考资料：
- en: The K-NN algorithm in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/neighbors.html](http://scikit-learn.org/stable/modules/neighbors.html)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn文档中的K-NN算法，网址为[http://scikit-learn.org/stable/modules/neighbors.html](http://scikit-learn.org/stable/modules/neighbors.html)
- en: The K-NN algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm](http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的K-NN算法，网址为[http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm](http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
- en: Blog post about how to choose the K hyperparameter, available at [http://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/](http://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 博客文章介绍如何选择K超参数，网址为[http://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/](http://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/)
- en: Instance-based learning on Wikipedia, available at [http://en.wikipedia.org/wiki/Instance-based_learning](http://en.wikipedia.org/wiki/Instance-based_learning)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的基于实例的学习，网址为[http://en.wikipedia.org/wiki/Instance-based_learning](http://en.wikipedia.org/wiki/Instance-based_learning)
- en: See also
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The *Predicting who will survive on the Titanic with logistic regression* recipe
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用逻辑回归预测谁将在泰坦尼克号上幸存*食谱'
- en: The *Using support vector machines for classification tasks* recipe
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用支持向量机进行分类任务*食谱'
- en: Learning from text – Naive Bayes for Natural Language Processing
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文本中学习 – 自然语言处理中的朴素贝叶斯
- en: In this recipe, we show how to handle text data with scikit-learn. Working with
    text requires careful preprocessing and feature extraction. It is also quite common
    to deal with highly sparse matrices.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们展示了如何使用scikit-learn处理文本数据。处理文本需要仔细的预处理和特征提取。同时，处理高稀疏矩阵也很常见。
- en: We will learn to recognize whether a comment posted during a public discussion
    is considered insulting to one of the participants. We will use a labeled dataset
    from Impermium, released during a Kaggle competition.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习识别在公共讨论中发布的评论是否被认为是侮辱某个参与者的。我们将使用来自Impermium的标注数据集，该数据集是在Kaggle竞赛中发布的。
- en: Getting ready
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Download the *Troll* dataset from the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 从书籍的GitHub仓库下载*Troll*数据集，网址是[https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data)。
- en: This dataset was obtained from Kaggle, at [www.kaggle.com/c/detecting-insults-in-social-commentary](http://www.kaggle.com/c/detecting-insults-in-social-commentary).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可以从Kaggle下载，网址是[www.kaggle.com/c/detecting-insults-in-social-commentary](http://www.kaggle.com/c/detecting-insults-in-social-commentary)。
- en: How to do it...
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Let''s import our libraries:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入我们的库：
- en: '[PRE27]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s open the CSV file with pandas:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用pandas打开CSV文件：
- en: '[PRE28]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Each row is a comment. We will consider two columns: whether the comment is
    insulting (1) or not (0) and the unicode-encoded contents of the comment:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每一行是一个评论。我们将考虑两个列：评论是否为侮辱性（1）还是非侮辱性（0），以及评论的Unicode编码内容：
- en: '[PRE29]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we are going to define the feature matrix `X` and the labels `y`:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义特征矩阵`X`和标签`y`：
- en: '[PRE30]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Obtaining the feature matrix from the text is not trivial. scikit-learn can
    only work with numerical matrices. So how do we convert text into a matrix of
    numbers? A classical solution is to first extract a **vocabulary**, a list of
    words used throughout the corpus. Then, we count, for each sample, the frequency
    of each word. We end up with a **sparse matrix**, a huge matrix containing mostly
    zeros. Here, we do this in two lines. We will give more details in *How it works…*.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从文本中获得特征矩阵并不是一件简单的事。scikit-learn只能处理数值矩阵。那么，我们如何将文本转换成数值矩阵呢？一种经典的解决方案是，首先提取一个**词汇表**，即语料库中使用的所有词汇的列表。然后，我们为每个样本计算每个词汇的出现频率。我们最终得到一个**稀疏矩阵**，这是一个大矩阵，主要包含零。这里，我们只用两行代码来完成。我们将在*它是如何工作的…*部分提供更多细节。
- en: Note
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The general rule here is that whenever one of our features is categorical (that
    is, the presence of a word, a color belonging to a fixed set of *n* colors, and
    so on), we should *vectorize* it by considering one binary feature per item in
    the class. For example, instead of a feature `color` being `red`, `green`, or
    `blue`, we should consider three *binary* features `color_red`, `color_green`,
    and `color_blue`. We give further references in the *There's more…* section.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的一般规则是，当我们的特征是分类变量时（即，某个词的出现，属于固定集合的*n*种颜色中的一种，等等），我们应该通过考虑每个类别项的二元特征来*向量化*它。例如，`color`特征如果是`red`、`green`或`blue`，我们应该考虑三个*二元*特征：`color_red`、`color_green`和`color_blue`。我们将在*更多内容…*部分提供更多参考资料。
- en: '[PRE31]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'There are 3947 comments and 16469 different words. Let''s estimate the sparsity
    of this feature matrix:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 共有3947条评论和16469个不同的词汇。让我们估算一下这个特征矩阵的稀疏性：
- en: '[PRE32]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we are going to train a classifier as usual. We first split the data into
    a train and test set:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们像往常一样训练分类器。我们首先将数据划分为训练集和测试集：
- en: '[PRE33]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We use a **Bernoulli Naive Bayes classifier** with a grid search on the ![How
    to do it...](img/4818OS_08_43.jpg) parameter:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用**伯努利朴素贝叶斯分类器**，并对![如何做到...](img/4818OS_08_43.jpg)参数进行网格搜索：
- en: '[PRE34]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s check the performance of this classifier on the test dataset:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下这个分类器在测试数据集上的表现：
- en: '[PRE35]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s take a look at the words corresponding to the largest coefficients (the
    words we find frequently in insulting comments):'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看一下与最大系数对应的词汇（我们在侮辱性评论中经常找到的词汇）：
- en: '[PRE36]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, let''s test our estimator on a few test sentences:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们在几个测试句子上测试我们的估算器：
- en: '[PRE37]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: That's not bad, but we can probably do better.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果不错，但我们可能能够做得更好。
- en: How it works...
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: scikit-learn implements several utility functions to obtain a sparse feature
    matrix from text data. A **vectorizer** such as `CountVectorizer()` extracts a
    vocabulary from a corpus (`fit`) and constructs a sparse representation of the
    corpus based on this vocabulary (`transform`). Each sample is represented by the
    vocabulary's word frequencies. The trained instance also contains attributes and
    methods to map feature indices to the corresponding words (`get_feature_names()`)
    and conversely (`vocabulary_`).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn实现了多个实用函数，可以从文本数据中获取稀疏特征矩阵。像**CountVectorizer()**这样的**向量化器**从语料库中提取词汇（`fit`），并基于该词汇构建语料库的稀疏表示（`transform`）。每个样本由词汇的词频表示。训练后的实例还包含属性和方法，用于将特征索引映射到对应的词汇（`get_feature_names()`）及反向映射（`vocabulary_`）。
- en: N-grams can also be extracted. These are pairs or tuples of words occurring
    successively (the `ngram_range` keyword).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以提取N-gram。这些是连续出现的词对或元组（`ngram_range`关键字）。
- en: The frequency of the words can be weighted in different ways. Here, we have
    used **tf-idf**, or **term frequency-inverse document frequency**. This quantity
    reflects how important a word is to a corpus. Frequent words in comments have
    a high weight except if they appear in most comments (which means that they are
    common terms, for example, "the" and "and" would be filtered out using this technique).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 词频可以以不同的方式加权。这里，我们使用了**tf-idf**，即**词频-逆文档频率**。这个量度反映了一个词对于语料库的重要性。评论中频繁出现的词语有较高的权重，除非它们出现在大多数评论中（这意味着它们是常见词汇，例如“the”和“and”会被这种技术过滤掉）。
- en: Naive Bayes algorithms are Bayesian methods based on the naive assumption of
    independence between the features. This strong assumption drastically simplifies
    the computations and leads to very fast yet decent classifiers.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法是基于特征之间独立性假设的贝叶斯方法。这一强假设极大简化了计算，从而使分类器非常快速且效果不错。
- en: There's more…
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'Here are a few references:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些参考资料：
- en: Text feature extraction in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn文档中的文本特征提取，见[http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)
- en: Term frequency-inverse document-frequency on Wikipedia, available at [http://en.wikipedia.org/wiki/tf-idf](http://en.wikipedia.org/wiki/tf-idf)
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的词频-逆文档频率，见[http://en.wikipedia.org/wiki/tf-idf](http://en.wikipedia.org/wiki/tf-idf)
- en: Vectorizer in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn文档中的向量化器，见[http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)
- en: Naive Bayes classifier on Wikipedia, at [http://en.wikipedia.org/wiki/Naive_Bayes_classifier](http://en.wikipedia.org/wiki/Naive_Bayes_classifier)
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的朴素贝叶斯分类器，见[http://en.wikipedia.org/wiki/Naive_Bayes_classifier](http://en.wikipedia.org/wiki/Naive_Bayes_classifier)
- en: Naive Bayes in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/naive_bayes.html](http://scikit-learn.org/stable/modules/naive_bayes.html)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn文档中的朴素贝叶斯，见[http://scikit-learn.org/stable/modules/naive_bayes.html](http://scikit-learn.org/stable/modules/naive_bayes.html)
- en: Impermium Kaggle challenge, at [http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/](http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/)
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impermium Kaggle挑战，见[http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/](http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/)
- en: Document classification example in scikit-learn's documentation, at [http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html](http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html)
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn文档中的文档分类示例，见[http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html](http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html)
- en: Tip
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Besides scikit-learn, which has good support for text processing, we should
    also mention NLTK (available at [www.nltk.org](http://www.nltk.org)), a Natural
    Language Toolkit in Python.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 除了scikit-learn，它在文本处理方面有很好的支持，我们还应该提到NLTK（可在[www.nltk.org](http://www.nltk.org)获取），这是一个Python中的自然语言工具包。
- en: See also
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The *Predicting who will survive on the Titanic with logistic regression* recipe
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用逻辑回归预测谁会在泰坦尼克号上生还*的教程'
- en: The *Learning to recognize handwritten digits with a K-nearest neighbors classifier*
    recipe
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用K最近邻分类器学习识别手写数字*的教程'
- en: The *Using support vector machines for classification tasks* recipe
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用支持向量机进行分类任务*的教程'
- en: Using support vector machines for classification tasks
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机进行分类任务
- en: In this recipe, we introduce **support vector machines**, or **SVMs**. These
    powerful models can be used for classification and regression. Here, we illustrate
    how to use linear and nonlinear SVMs on a simple classification task.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们介绍了**支持向量机**，或**SVM**。这些强大的模型可以用于分类和回归。在这里，我们展示了如何在一个简单的分类任务中使用线性和非线性SVM。
- en: How to do it...
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s import the packages:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入所需的包：
- en: '[PRE38]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We generate 2D points and assign a binary label according to a linear operation
    on the coordinates:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们生成二维点，并根据坐标的线性操作分配二进制标签：
- en: '[PRE39]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We now fit a linear **Support Vector Classifier** (**SVC**). This classifier
    tries to separate the two groups of points with a linear boundary (a line here,
    but more generally a hyperplane):'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们拟合一个线性**支持向量分类器**（**SVC**）。该分类器尝试用一个线性边界（这里是线，但更一般地是超平面）将两组点分开：
- en: '[PRE40]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We define a function that displays the boundaries and decision function of
    a trained classifier:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个函数，用于显示训练好的分类器的边界和决策函数：
- en: '[PRE41]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let''s take a look at the classification results with the linear SVC:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看线性SVC的分类结果：
- en: '[PRE42]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![How to do it...](img/4818OS_08_17.jpg)'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_17.jpg)'
- en: The linear SVC tried to separate the points with a line and it did a pretty
    good job here.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性SVC尝试用一条线将点分开，它在这里做得相当不错。
- en: 'We now modify the labels with an `XOR` function. A point''s label is 1 if the
    coordinates have different signs. This classification is not linearly separable.
    Therefore, a linear SVC fails completely:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用`XOR`函数修改标签。如果坐标的符号不同，则该点的标签为1。这个分类任务是不可线性分割的。因此，线性SVC完全失败：
- en: '[PRE43]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![How to do it...](img/4818OS_08_18.jpg)'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_18.jpg)'
- en: 'Fortunately, it is possible to use nonlinear SVCs by using nonlinear **kernels**.
    Kernels specify a nonlinear transformation of the points into a higher dimensional
    space. Transformed points in this space are assumed to be more linearly separable.
    By default, the `SVC` classifier in scikit-learn uses the **Radial Basis Function**
    (**RBF**) kernel:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 幸运的是，可以通过使用非线性**核函数**来使用非线性SVC。核函数指定将点进行非线性变换，映射到更高维的空间中。假设该空间中的变换点更容易线性可分。默认情况下，scikit-learn中的`SVC`分类器使用**径向基函数**（**RBF**）核函数：
- en: '[PRE44]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![How to do it...](img/4818OS_08_19.jpg)'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_19.jpg)'
- en: This time, the nonlinear SVC successfully managed to classify these nonlinearly
    separable points.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这次，非线性SVC成功地将这些非线性可分的点进行了分类。
- en: How it works...
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: A two-class linear SVC tries to find a hyperplane (defined as a linear equation)
    that best separates the two sets of points (grouped according to their labels).
    There is also the constraint that this separating hyperplane needs to be as far
    as possible from the points. This method works best when such a hyperplane exists.
    Otherwise, this method can fail completely, as we saw in the `XOR` example. `XOR`
    is known as being a nonlinearly separable operation.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类线性SVC尝试找到一个超平面（定义为线性方程），以最好地分离两组点（根据它们的标签分组）。另外，还有一个约束条件是，这个分离超平面需要尽可能远离点。这种方法在存在这样的超平面时效果最好。否则，这种方法可能完全失败，就像我们在`XOR`示例中看到的那样。`XOR`被认为是一个非线性可分的操作。
- en: The SVM classes in scikit-learn have a *C* hyperparameter. This hyperparameter
    trades off misclassification of training examples against simplicity of the decision
    surface. A low *C* value makes the decision surface smooth, while a high *C* value
    aims at classifying all training examples correctly. This is another example where
    a hyperparameter quantifies the bias-variance trade-off. This hyperparameter can
    be chosen with cross-validation and grid search.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的SVM类有一个*C*超参数。这个超参数在训练样本的误分类与决策面简洁性之间进行权衡。较低的*C*值使得决策面平滑，而较高的*C*值则试图正确分类所有训练样本。这是另一个超参数量化偏差-方差权衡的例子。可以通过交叉验证和网格搜索来选择这个超参数。
- en: The linear SVC can also be extended to multiclass problems. The multiclass SVC
    is directly implemented in scikit-learn.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 线性SVC也可以扩展到多分类问题。多分类SVC在scikit-learn中直接实现。
- en: The nonlinear SVC works by considering a nonlinear transformation ![How it works...](img/4818OS_08_35.jpg)
    from the original space into a higher dimensional space. This nonlinear transformation
    can increase the linear separability of the classes. In practice, all dot products
    are replaced by the ![How it works...](img/4818OS_08_36.jpg) kernel.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性SVC通过考虑从原始空间到更高维空间的非线性转换![它是如何工作的...](img/4818OS_08_35.jpg)来工作。这个非线性转换可以增加类别之间的线性可分性。在实践中，所有点积都被![它是如何工作的...](img/4818OS_08_36.jpg)核所替代。
- en: '![How it works...](img/4818OS_08_20.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4818OS_08_20.jpg)'
- en: Nonlinear SVC
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性SVC
- en: 'There are several widely-used nonlinear kernels. By default, SVC uses Gaussian
    radial basis functions:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种广泛使用的非线性核。默认情况下，SVC使用高斯径向基函数：
- en: '![How it works...](img/4818OS_08_21.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4818OS_08_21.jpg)'
- en: Here, ![How it works...](img/4818OS_08_37.jpg) is a hyperparameter of the model
    that can be chosen with grid search and cross-validation.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![它是如何工作的...](img/4818OS_08_37.jpg)是模型的超参数，可以通过网格搜索和交叉验证来选择。
- en: The ![How it works...](img/4818OS_08_42.jpg) function does not need to be computed
    explicitly. This is the **kernel trick**; it suffices to know the kernel *k(x,
    x')*. The existence of a function ![How it works...](img/4818OS_08_42.jpg) corresponding
    to a given kernel *k(x, x')* is guaranteed by a mathematical theorem in functional
    analysis.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '![它是如何工作的...](img/4818OS_08_42.jpg)函数不需要显式计算。这就是**核技巧**；只需要知道核函数*k(x, x'')*即可。给定核函数*k(x,
    x'')*所对应的函数的存在是由函数分析中的数学定理所保证的。'
- en: There's more…
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'Here are a few references about support vector machines:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于支持向量机的参考资料：
- en: Exclusive OR on Wikipedia, available at [http://en.wikipedia.org/wiki/Exclusive_or](http://en.wikipedia.org/wiki/Exclusive_or)
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的异或，访问地址：[http://en.wikipedia.org/wiki/Exclusive_or](http://en.wikipedia.org/wiki/Exclusive_or)
- en: Support vector machines on Wikipedia, available at [http://en.wikipedia.org/wiki/Support_vector_machine](http://en.wikipedia.org/wiki/Support_vector_machine)
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的支持向量机，访问地址：[http://en.wikipedia.org/wiki/Support_vector_machine](http://en.wikipedia.org/wiki/Support_vector_machine)
- en: SVMs in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/svm.html](http://scikit-learn.org/stable/modules/svm.html)
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn文档中的SVM，访问地址：[http://scikit-learn.org/stable/modules/svm.html](http://scikit-learn.org/stable/modules/svm.html)
- en: Kernel trick on Wikipedia, available at [http://en.wikipedia.org/wiki/Kernel_method](http://en.wikipedia.org/wiki/Kernel_method)
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的核技巧，访问地址：[http://en.wikipedia.org/wiki/Kernel_method](http://en.wikipedia.org/wiki/Kernel_method)
- en: Notes about the kernel trick available at [www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于核技巧的说明，访问地址：[www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)
- en: An example with a nonlinear SVM available at [http://scikit-learn.org/0.11/auto_examples/svm/plot_svm_nonlinear.html](http://scikit-learn.org/0.11/auto_examples/svm/plot_svm_nonlinear.html)
    (this example inspired this recipe)
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关于非线性SVM的示例，访问地址：[http://scikit-learn.org/0.11/auto_examples/svm/plot_svm_nonlinear.html](http://scikit-learn.org/0.11/auto_examples/svm/plot_svm_nonlinear.html)（这个示例启发了这个配方）
- en: See also
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见：
- en: The *Predicting who will survive on the Titanic with logistic regression* recipe
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用逻辑回归预测谁会在泰坦尼克号上生存*的配方'
- en: The *Learning to recognize handwritten digits with a K-nearest neighbors classifier*
    recipe
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用K近邻分类器识别手写数字*的配方'
- en: Using a random forest to select important features for regression
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林选择回归的重要特征
- en: '**Decision trees** are frequently used to represent workflows or algorithms.
    They also form a method for nonparametric supervised learning. A tree mapping
    observations to target values is learned on a training set and gives the outcomes
    of new observations.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**经常被用来表示工作流程或算法。它们还构成了一种非参数监督学习方法。在训练集上学习一个将观察值映射到目标值的树，并且可以对新观察值做出预测。'
- en: '**Random forests** are ensembles of decision trees. Multiple decision trees
    are trained and aggregated to form a model that is more performant than any of
    the individual trees. This general idea is the purpose of **ensemble learning**.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**是决策树的集成。通过训练多个决策树并将其结果聚合，形成的模型比任何单一的决策树更具表现力。这个概念就是**集成学习**的目的。'
- en: There are many types of ensemble methods. Random forests are an instance of
    **bootstrap aggregating**, also called **bagging**, where models are trained on
    randomly drawn subsets of the training set.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类型的集成方法。随机森林是**自助聚合**（也称为**bagging**）的一种实现，其中模型是在从训练集中随机抽取的子集上训练的。
- en: Random forests yield information about the importance of each feature for the
    classification or regression task. In this recipe, we will find the most influential
    features of Boston house prices using a classic dataset that contains a range
    of diverse indicators about the houses' neighborhood.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林能提供每个特征在分类或回归任务中的重要性。在本实例中，我们将使用一个经典数据集，利用该数据集中的各类房屋邻里指标来找出波士顿房价的最具影响力特征。
- en: How to do it...
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We import the packages:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入所需的包：
- en: '[PRE45]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We load the Boston dataset:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载波士顿数据集：
- en: '[PRE46]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The details of this dataset can be found in `data[''DESCR'']`. Here is the
    description of some features:'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集的详细信息可以在 `data['DESCR']` 中找到。以下是一些特征的描述：
- en: '*CRIM*: Per capita crime rate by town'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CRIM*：各镇的人均犯罪率'
- en: '*NOX*: Nitric oxide concentration (parts per 10 million)'
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NOX*：氮氧化物浓度（每千万分之一）'
- en: '*RM*: Average number of rooms per dwelling'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RM*：每套住房的平均房间数'
- en: '*AGE*: Proportion of owner-occupied units built prior to 1940'
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AGE*：1940年前建造的自有住房单位的比例'
- en: '*DIS*: Weighted distances to five Boston employment centers'
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DIS*：到波士顿五个就业中心的加权距离'
- en: '*PTRATIO*: Pupil-teacher ratio by town'
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PTRATIO*：各镇的师生比'
- en: '*LSTAT*: Percentage of lower status of the population'
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LSTAT*：低社会经济地位人口的比例'
- en: '*MEDV*: Median value of owner-occupied homes in $1000s'
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MEDV*：以 $1000 为单位的自有住房中位数价格'
- en: The target value is *MEDV*.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标值是 *MEDV*。
- en: 'We create a `RandomForestRegressor` model:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个 `RandomForestRegressor` 模型：
- en: '[PRE47]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We get the samples and the target values from this dataset:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从这个数据集中获取样本和目标值：
- en: '[PRE48]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s fit the model:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来拟合模型：
- en: '[PRE49]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The importance of our features can be found in `reg.feature_importances_`.
    We sort them by decreasing order of importance:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征的重要性可以通过 `reg.feature_importances_` 查看。我们按重要性递减顺序对其进行排序：
- en: '[PRE50]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, we plot a histogram of the features'' importance:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们绘制特征重要性的直方图：
- en: '[PRE51]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![How to do it...](img/4818OS_08_22.jpg)'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何实现...](img/4818OS_08_22.jpg)'
- en: 'We find that *LSTAT* (proportion of lower status of the population) and *RM*
    (number of rooms per dwelling) are the most important features determining the
    price of a house. As an illustration, here is a scatter plot of the price as a
    function of *LSTAT*:'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们发现 *LSTAT*（低社会经济地位人口比例）和 *RM*（每套住房的房间数）是决定房价的最重要特征。作为说明，这里是房价与 *LSTAT* 的散点图：
- en: '[PRE52]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![How to do it...](img/4818OS_08_23.jpg)'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何实现...](img/4818OS_08_23.jpg)'
- en: How it works...
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Several algorithms can be used to train a decision tree. scikit-learn uses the
    **CART**, or **Classification and Regression Trees**, algorithm. This algorithm
    constructs binary trees using the feature and threshold that yield the largest
    information gain at each node. Terminal nodes give the outcomes of input values.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 训练决策树可以使用多种算法。scikit-learn 使用 **CART**（分类与回归树）算法。该算法利用在每个节点上产生最大信息增益的特征和阈值构建二叉树。终端节点给出输入值的预测结果。
- en: Decision trees are simple to understand. They can also be visualized with **pydot**,
    a Python package for drawing graphs and trees. This is useful when we want to
    understand what a tree has learned exactly (**white box model**); the conditions
    that apply on the observations at each node can be expressed easily with Boolean
    logic.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树易于理解。它们还可以通过 **pydot** 这一 Python 包进行可视化，用于绘制图形和树形结构。当我们希望了解决策树究竟学到了什么时，这非常有用（**白盒模型**）；每个节点上的观测条件可以通过布尔逻辑简洁地表达。
- en: However, decision trees may suffer from overfitting, notably when they are too
    deep, and they might be unstable. Additionally, global convergence toward an optimal
    model is not guaranteed, particularly when greedy algorithms are used for training.
    These problems can be mitigated by using ensembles of decision trees, notably
    random forests.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，决策树可能会受到过拟合的影响，特别是在树过深时，且可能不稳定。此外，尤其是在使用贪婪算法训练时，全局收敛到最优模型并没有保证。这些问题可以通过使用决策树集成方法来缓解，特别是随机森林。
- en: In a random forest, multiple decision trees are trained on bootstrap samples
    of the training dataset (randomly sampled with replacement). Predictions are made
    with the averages of individual trees' predictions (bootstrap aggregating or bagging).
    Additionally, random subsets of the features are chosen at each node (**random
    subspace method**). These methods lead to an overall better model than the individual
    trees.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林中，多个决策树在训练数据集的自助样本上进行训练（通过有放回的随机抽样）。预测结果通过各个树的预测结果平均值来得出（自助聚合或袋装法）。此外，在每个节点上会随机选择特征的子集（**随机子空间方法**）。这些方法能使得整体模型优于单棵树。
- en: There's more...
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Here are a few references:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些参考资料：
- en: Ensemble learning in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/ensemble.html](http://scikit-learn.org/stable/modules/ensemble.html)
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn文档中的集成学习，链接：[http://scikit-learn.org/stable/modules/ensemble.html](http://scikit-learn.org/stable/modules/ensemble.html)
- en: API reference of `RandomForestRegressor` available at [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestRegressor`的API参考，链接：[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)'
- en: Random forests on Wikipedia, available at [http://en.wikipedia.org/wiki/Random_forest](http://en.wikipedia.org/wiki/Random_forest)
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的随机森林，链接：[http://en.wikipedia.org/wiki/Random_forest](http://en.wikipedia.org/wiki/Random_forest)
- en: Decision tree learning on Wikipedia, available at [http://en.wikipedia.org/wiki/Decision_tree_learning](http://en.wikipedia.org/wiki/Decision_tree_learning)
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的决策树学习，链接：[http://en.wikipedia.org/wiki/Decision_tree_learning](http://en.wikipedia.org/wiki/Decision_tree_learning)
- en: Bootstrap aggregating on Wikipedia, available at [http://en.wikipedia.org/wiki/Bootstrap_aggregating](http://en.wikipedia.org/wiki/Bootstrap_aggregating)
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的自助法聚合，链接：[http://en.wikipedia.org/wiki/Bootstrap_aggregating](http://en.wikipedia.org/wiki/Bootstrap_aggregating)
- en: Random subspace method on Wikipedia, available at [http://en.wikipedia.org/wiki/Random_subspace_method](http://en.wikipedia.org/wiki/Random_subspace_method)
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的随机子空间方法，链接：[http://en.wikipedia.org/wiki/Random_subspace_method](http://en.wikipedia.org/wiki/Random_subspace_method)
- en: Ensemble learning on Wikipedia, available at [http://en.wikipedia.org/wiki/Ensemble_learning](http://en.wikipedia.org/wiki/Ensemble_learning)
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的集成学习，链接：[http://en.wikipedia.org/wiki/Ensemble_learning](http://en.wikipedia.org/wiki/Ensemble_learning)
- en: See also
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The *Using support vector machines for classification tasks* recipe
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用支持向量机进行分类任务*的食谱'
- en: Reducing the dimensionality of a dataset with a principal component analysis
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过主成分分析（PCA）来减少数据集的维度
- en: In the previous recipes, we presented *supervised learning* methods; our data
    points came with discrete or continuous labels, and the algorithms were able to
    learn the mapping from the points to the labels.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的食谱中，我们介绍了*有监督学习*方法；我们的数据点带有离散或连续的标签，算法能够学习从点到标签的映射关系。
- en: Starting with this recipe, we will present **unsupervised learning** methods.
    These methods might be helpful prior to running a supervised learning algorithm.
    They can give a first insight into the data.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个食谱开始，我们将介绍**无监督学习**方法。这些方法在运行有监督学习算法之前可能会有所帮助，能够为数据提供初步的洞察。
- en: 'Let''s assume that our data consists of points *x[i]* without any labels. The
    goal is to discover some form of hidden structure in this set of points. Frequently,
    data points have intrinsic low dimensionality: a small number of features suffice
    to accurately describe the data. However, these features might be hidden among
    many other features not relevant to the problem. Dimension reduction can help
    us find these structures. This knowledge can considerably improve the performance
    of subsequent supervised learning algorithms.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的数据由没有标签的点*x[i]*组成。目标是发现这些点集合中的某种隐藏结构。数据点通常具有内在的低维性：少量特征就足以准确描述数据。然而，这些特征可能被许多与问题无关的特征所掩盖。降维可以帮助我们找到这些结构。这些知识可以显著提升后续有监督学习算法的性能。
- en: Another useful application of unsupervised learning is **data visualization**;
    high-dimensional datasets are hard to visualize in 2D or 3D. Projecting the data
    points on a subspace or submanifold yields more interesting visualizations.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的另一个有用应用是**数据可视化**；高维数据集很难在二维或三维中进行可视化。将数据点投影到子空间或子流形上，可以得到更有趣的可视化效果。
- en: In this recipe, we will illustrate a basic unsupervised linear method, **principal
    component analysis** (**PCA**). This algorithm lets us project data points linearly
    on a low-dimensional subspace. Along the **principal components**, which are vectors
    forming a basis of this low-dimensional subspace, the variance of the data points
    is maximum.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将演示一种基本的无监督线性方法——**主成分分析**（**PCA**）。该算法可以让我们将数据点线性地投影到低维子空间上。在**主成分**方向上，这些向量构成了该低维子空间的基，数据点的方差最大。
- en: 'We will use the classic *Iris flower* dataset as an example. This dataset contains
    the width and length of the petal and sepal of 150 iris flowers. These flowers
    belong to one of three categories: *Iris setosa*, *Iris virginica*, and *Iris
    versicolor*. We have access to the category in this dataset (labeled data). However,
    because we are interested in illustrating an unsupervised learning method, we
    will only use the data matrix *without* the labels.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用经典的*鸢尾花*数据集作为示例。这个数据集包含了150朵鸢尾花的花瓣和萼片的宽度和长度。这些花属于三种类别之一：*Iris setosa*、*Iris
    virginica*和*Iris versicolor*。我们可以在这个数据集中获取到类别信息（有标签数据）。然而，由于我们感兴趣的是展示一种无监督学习方法，我们将只使用不带标签的数据矩阵。
- en: How to do it...
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We import NumPy, matplotlib, and scikit-learn:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入NumPy、matplotlib和scikit-learn：
- en: '[PRE53]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The *Iris flower* dataset is available in the `datasets` module of scikit-learn:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*鸢尾花*数据集可以在scikit-learn的`datasets`模块中找到：'
- en: '[PRE54]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Each row contains four parameters related to the morphology of the flower.
    Let''s display the first two dimensions. The color reflects the iris variety of
    the flower (the label, between 0 and 2):'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每一行包含与花朵形态学相关的四个参数。让我们展示前两个维度。颜色反映了花朵的鸢尾花种类（标签，介于0和2之间）：
- en: '[PRE55]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![How to do it...](img/4818OS_08_24.jpg)'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_24.jpg)'
- en: Tip
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you're reading the printed version of this book, you might not be able to
    distinguish the colors. You will find the colored images on the book's website.
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你正在阅读这本书的印刷版，可能无法区分颜色。你可以在本书的网站上找到彩色图像。
- en: 'We now apply PCA on the dataset to get the transformed matrix. This operation
    can be done in a single line with scikit-learn: we instantiate a `PCA` model and
    call the `fit_transform()` method. This function computes the principal components
    and projects the data on them:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在对数据集应用PCA以获得变换后的矩阵。这个操作可以通过scikit-learn中的一行代码完成：我们实例化一个`PCA`模型并调用`fit_transform()`方法。这个函数计算主成分并将数据投影到这些主成分上：
- en: '[PRE56]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We now display the same dataset, but in a new coordinate system (or equivalently,
    a linearly transformed version of the initial dataset):'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在展示相同的数据集，但采用新的坐标系统（或者等效地，是初始数据集的线性变换版本）：
- en: '[PRE57]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![How to do it...](img/4818OS_08_25.jpg)'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_25.jpg)'
- en: Points belonging to the same classes are now grouped together, even though the
    `PCA` estimator did *not* use the labels. The PCA was able to find a projection
    maximizing the variance, which corresponds here to a projection where the classes
    are well separated.
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 属于同一类别的点现在被分到了一起，即使`PCA`估计器*没有*使用标签。PCA能够找到一个最大化方差的投影，这里对应的是一个类别分得很好的投影。
- en: 'The `scikit.decomposition` module contains several variants of the classic
    `PCA` estimator: `ProbabilisticPCA`, `SparsePCA`, `RandomizedPCA`, `KernelPCA`,
    and others. As an example, let''s take a look at `KernelPCA`, a nonlinear version
    of PCA:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`scikit.decomposition`模块包含了经典`PCA`估计器的几个变体：`ProbabilisticPCA`、`SparsePCA`、`RandomizedPCA`、`KernelPCA`等。作为示例，让我们来看一下`KernelPCA`，PCA的非线性版本：'
- en: '[PRE58]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![How to do it...](img/4818OS_08_26.jpg)'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_26.jpg)'
- en: How it works...
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Let''s look at the mathematical ideas behind PCA. This method is based on a
    matrix decomposition called **Singular Value Decomposition** (**SVD**):'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看PCA背后的数学思想。这种方法基于一种叫做**奇异值分解**（**SVD**）的矩阵分解：
- en: '![How it works...](img/4818OS_08_27.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4818OS_08_27.jpg)'
- en: Here, *X* is the *(N,D)* data matrix, *U* and *V* are orthogonal matrices, and
    ![How it works...](img/4818OS_08_38.jpg) is a *(N,D)* diagonal matrix.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*X*是*(N,D)*的数据矩阵，*U*和*V*是正交矩阵，而![它是如何工作的...](img/4818OS_08_38.jpg)是一个*(N,D)*的对角矩阵。
- en: 'PCA transforms *X* into *X''* defined by:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: PCA将*X*转换为由以下定义的*X'*：
- en: '![How it works...](img/4818OS_08_28.jpg)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4818OS_08_28.jpg)'
- en: The diagonal elements of ![How it works...](img/4818OS_08_38.jpg) are the **singular
    values** of *X*. By convention, they are generally sorted in descending order.
    The columns of *U* are orthonormal vectors called the **left singular vectors**
    of *X*. Therefore, the columns of *X'* are the left singular vectors multiplied
    by the singular values.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '![它是如何工作的...](img/4818OS_08_38.jpg)的对角线元素是*X*的**奇异值**。根据惯例，它们通常按降序排列。*U*的列是称为*X*的**左奇异向量**的正交归一化向量。因此，*X''*的列是左奇异向量乘以奇异值。'
- en: In the end, PCA converts the initial set of observations, which are made of
    possibly correlated variables, into vectors of linearly uncorrelated variables
    called **principal components**.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，PCA将初始的一组观测值（这些观测值可能包含相关变量）转换为一组线性无关的变量，称为**主成分**。
- en: The first new feature (or first component) is a transformation of all original
    features such that the dispersion (variance) of the data points is the highest
    in that direction. In the subsequent principal components, the variance is decreasing.
    In other words, PCA gives us an alternative representation of our data where the
    new features are sorted according to how much they account for the variability
    of the points.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个新特性（或第一个主成分）是将所有原始特征进行转换，使得数据点的离散度（方差）在该方向上最大。在随后的主成分中，方差逐渐减小。换句话说，PCA为我们提供了一种数据的新表示，其中新特征按照它们对数据点变异性的解释程度进行排序。
- en: There's more…
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Here are a few further references:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些进一步的参考：
- en: Iris flower dataset on Wikipedia, available at [http://en.wikipedia.org/wiki/Iris_flower_data_set](http://en.wikipedia.org/wiki/Iris_flower_data_set)
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的鸢尾花数据集，访问 [http://en.wikipedia.org/wiki/Iris_flower_data_set](http://en.wikipedia.org/wiki/Iris_flower_data_set)
- en: PCA on Wikipedia, available at [http://en.wikipedia.org/wiki/Principal_component_analysis](http://en.wikipedia.org/wiki/Principal_component_analysis)
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的PCA，访问 [http://en.wikipedia.org/wiki/Principal_component_analysis](http://en.wikipedia.org/wiki/Principal_component_analysis)
- en: SVD decomposition on Wikipedia, available at [http://en.wikipedia.org/wiki/Singular_value_decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition)
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的SVD分解，访问 [http://en.wikipedia.org/wiki/Singular_value_decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition)
- en: Iris dataset example available at [http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鸢尾花数据集示例，访问 [http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
- en: Decompositions in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/decomposition.html](http://scikit-learn.org/stable/modules/decomposition.html)
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn文档中的分解方法，访问 [http://scikit-learn.org/stable/modules/decomposition.html](http://scikit-learn.org/stable/modules/decomposition.html)
- en: Unsupervised learning tutorial with scikit-learn available at [http://scikit-learn.org/dev/tutorial/statistical_inference/unsupervised_learning.html](http://scikit-learn.org/dev/tutorial/statistical_inference/unsupervised_learning.html)
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn的无监督学习教程，访问 [http://scikit-learn.org/dev/tutorial/statistical_inference/unsupervised_learning.html](http://scikit-learn.org/dev/tutorial/statistical_inference/unsupervised_learning.html)
- en: See also
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The *Detecting hidden structures in a dataset with clustering* recipe
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用聚类检测数据集中的隐藏结构*的配方'
- en: Detecting hidden structures in a dataset with clustering
  id: totrans-442
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用聚类检测数据集中的隐藏结构
- en: A large part of unsupervised learning is devoted to the **clustering** problem.
    The goal is to group similar points together in a totally unsupervised way. Clustering
    is a hard problem, as the very definition of **clusters** (or **groups**) is not
    necessarily well posed. In most datasets, stating that two points should belong
    to the same cluster may be context-dependent or even subjective.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的一个重要部分是**聚类**问题。其目标是以完全无监督的方式将相似的点聚集在一起。聚类是一个困难的问题，因为**簇**（或**组**）的定义并不总是明确的。在大多数数据集中，声称两个点应该属于同一簇可能是依赖于上下文，甚至可能是主观的。
- en: There are many clustering algorithms. We will see a few of them in this recipe,
    applied to a toy example.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法有很多种。我们将在这个配方中看到其中的几个，并将它们应用于一个玩具示例。
- en: How to do it...
  id: totrans-445
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s import the libraries:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入库：
- en: '[PRE59]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let''s generate a random dataset with three clusters:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们生成一个包含三个簇的随机数据集：
- en: '[PRE60]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We need a couple of functions to relabel and display the results of the clustering
    algorithms:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要一些函数来重新标记并展示聚类算法的结果：
- en: '[PRE61]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now, we cluster the dataset with the **K-means** algorithm, a classic and simple
    clustering algorithm:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用**K-means**算法对数据集进行聚类，这是一种经典且简单的聚类算法：
- en: '[PRE62]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![How to do it...](img/4818OS_08_29.jpg)'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_29.jpg)'
- en: Tip
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you're reading the printed version of this book, you might not be able to
    distinguish the colors. You will find the colored images on the book's website.
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你正在阅读本书的印刷版，可能无法区分颜色。你可以在本书网站上找到彩色图片。
- en: 'This algorithm needs to know the number of clusters at initialization time.
    In general, however, we do not necessarily know the number of clusters in the
    dataset. Here, let''s try with `n_clusters=3` (that''s cheating, because we happen
    to know that there are 3 clusters!):'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个算法在初始化时需要知道簇的数量。然而，一般来说，我们并不一定知道数据集中簇的数量。在这里，让我们尝试使用`n_clusters=3`（这算是作弊，因为我们恰好知道有3个簇！）：
- en: '[PRE63]'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '![How to do it...](img/4818OS_08_30.jpg)'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_30.jpg)'
- en: 'Let''s try a few other clustering algorithms implemented in scikit-learn. The
    simplicity of the API makes it really easy to try different methods; it is just
    a matter of changing the name of the class:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试一些在 scikit-learn 中实现的其他聚类算法。由于 API 非常简洁，尝试不同的方法变得非常容易；只需要改变类名即可：
- en: '[PRE64]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![How to do it...](img/4818OS_08_31.jpg)'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_08_31.jpg)'
- en: 'The first two algorithms required the number of clusters as input. The next
    two did not, but they were able to find the right number: 3\. The last one failed
    at finding the correct number of clusters (this is *overclustering*—too many clusters
    have been found).'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前两个算法需要输入簇的数量。接下来的两个算法不需要，但它们能够找到正确的簇数：3。最后一个算法没有找到正确的簇数（这就是*过度聚类*——发现了过多的簇）。
- en: How it works...
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The K-means clustering algorithm consists of partitioning the data points *x[j]*
    into *K* clusters *S[i]* so as to minimize the within-cluster sum of squares:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 聚类算法通过将数据点 *x[j]* 分配到 *K* 个簇 *S[i]* 中，从而最小化簇内平方和：
- en: '![How it works...](img/4818OS_08_32.jpg)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4818OS_08_32.jpg)'
- en: Here, ![How it works...](img/4818OS_08_39.jpg) is the center of the cluster
    *i* (average of all points in *S[i]*).
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里， ![它是如何工作的...](img/4818OS_08_39.jpg) 是簇 *i* 的中心（即 *S[i]* 中所有点的平均值）。
- en: 'Although it is very hard to solve this problem exactly, approximation algorithms
    exist. A popular one is **Lloyd''s** **algorithm**. It consists of starting from
    an initial set of *K* means ![How it works...](img/4818OS_08_39.jpg) and alternating
    between two steps:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管精确地解决这个问题非常困难，但存在一些近似算法。一个流行的算法是 **Lloyd's** **算法**。它的基本过程是从一组初始的 *K* 均值出发
    ![它是如何工作的...](img/4818OS_08_39.jpg)，并交替进行两个步骤：
- en: In the *assignment* step, the points are assigned to the cluster associated
    to the closest mean
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*分配*步骤中，数据点会被分配到与最近均值相关的簇
- en: In the *update* step, the means are recomputed from the last assignments
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*更新*步骤中，从最后的分配中重新计算均值
- en: The algorithm converges to a solution that is not guaranteed to be optimal.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法收敛到一个解，但并不能保证是最优解。
- en: The **expectation-maximization algorithm** can be seen as a probabilistic version
    of the K-means algorithm. It is implemented in the `mixture` module of scikit-learn.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望最大化算法**可以看作是 K-means 算法的概率版本。它在 scikit-learn 的 `mixture` 模块中实现。'
- en: The other clustering algorithms used in this recipe are explained in the scikit-learn
    documentation. There is no clustering algorithm that works uniformly better than
    all the others, and every algorithm has its strengths and weaknesses. You will
    find more details in the references in the next section.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 本方案中使用的其他聚类算法在 scikit-learn 文档中有说明。没有哪种聚类算法在所有情况下都能表现得比其他算法更好，每种算法都有其优缺点。你可以在下一节的参考资料中找到更多细节。
- en: There's more...
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'Here are a few references:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些参考文献：
- en: The K-means clustering algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering)
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的 K-means 聚类算法，链接为 [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering)
- en: The expectation-maximization algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/Expectation-maximization_algorithm](http://en.wikipedia.org/wiki/Expectation-maximization_algorithm)
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的期望最大化算法，链接为 [http://en.wikipedia.org/wiki/Expectation-maximization_algorithm](http://en.wikipedia.org/wiki/Expectation-maximization_algorithm)
- en: Clustering in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 文档中的聚类内容，可在 [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)
    查看
- en: See also
  id: totrans-479
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: The *Reducing the dimensionality of a dataset with principal component analysis*
    recipe
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用主成分分析（PCA）降维数据集*的方案'
