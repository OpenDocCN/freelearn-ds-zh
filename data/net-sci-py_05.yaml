- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Even Easier Scraping!
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更简便的抓取方法！
- en: In the previous chapter, we covered the basics of web scraping, which is the
    act of harvesting data from the web for your uses and projects. In this chapter,
    we will explore even easier approaches to web scraping and will also introduce
    you to social media scraping. The previous chapter was very long, as we had a
    lot to cover, from defining scraping to explaining how the `Requests` library,
    and `BeautifulSoup` can be used to collect web data. I will show simpler approaches
    to getting useful text data with less cleaning involved. Keep in mind that these
    easier ways do not necessarily replace what was explained in the previous chapter.
    When working with data or in software projects and things do not immediately work
    out, it is useful to have options. But for now, we’re going to push forward with
    a simpler approach to scraping web content, as well as giving an introduction
    to scraping social media text.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讲解了网页抓取的基础知识，即从网络上获取数据以供自己使用和项目应用。在本章中，我们将探讨更简便的网页抓取方法，并且介绍社交媒体抓取的内容。上一章非常长，因为我们需要覆盖很多内容，从定义抓取到解释如何使用`Requests`库和`BeautifulSoup`来收集网页数据。我会展示一些更简单的方法，获取有用的文本数据，减少清理工作。请记住，这些简单的方法并不一定能取代上一章中解释的内容。在处理数据或进行软件项目时，当事情没有立即按预期工作时，拥有多个选择是非常有用的。但目前，我们将采用更简便的方法来抓取网页内容，并且介绍如何抓取社交媒体的文本数据。
- en: First, we will cover the `Newspaper3k` Python library, as well as the `Twitter
    V2` Python Library.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍`Newspaper3k` Python库，以及`Twitter V2` Python库。
- en: When I say that `Newspaper3k` is an easier approach to collecting web text data,
    that is because the authors of `Newspaper3k` did an amazing job of simplifying
    the process of collecting and enriching web data. They have done much of the work
    that you would normally need to do for yourself. For instance, if you wanted to
    gather metadata about a website, such as what language it uses, what keywords
    are in a story, or even the summary of a news story, `Newspaper3k` gives that
    to you. This would otherwise be a lot of work. It’s easier because, this way,
    you don’t have to do it from scratch.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当我说`Newspaper3k`是一种更简便的收集网页文本数据的方法时，这是因为`Newspaper3k`的作者在简化收集和丰富网页数据的过程上做得非常出色。他们已经完成了你通常需要自己做的许多工作。例如，如果你想收集关于网站的元数据，比如它使用的语言、故事中的关键词，或者甚至是新闻故事的摘要，`Newspaper3k`都能提供给你。这本来是需要很多工作的。它之所以更简便，是因为这样你不需要从头开始做。
- en: Second, you will learn how to use the `Twitter V2` Python Library, because this
    is a very easy way to harvest tweets from Twitter, and this will be useful for
    NLP as well as network analysis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步，你将学习如何使用`Twitter V2` Python库，因为这是一个非常简便的方法来从Twitter抓取推文，这对于自然语言处理（NLP）和网络分析非常有用。
- en: 'We will be covering the following topics in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Why cover Requests and BeautifulSoup?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要介绍Requests和BeautifulSoup？
- en: Getting started with Newspaper3k
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Newspaper3k入门
- en: Introducing the Twitter Python Library
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Twitter Python库
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using the NetworkX and pandas Python libraries.
    Both of these libraries should be installed by now, so they should be ready for
    your use. If they are not installed, you can install Python libraries with the
    following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用NetworkX和pandas Python库。这两个库现在应该已经安装完毕，所以可以直接使用。如果还没有安装，您可以通过以下命令安装Python库：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For instance, to install NetworkX, you would do this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要安装NetworkX，可以执行以下命令：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will also be discussing a few other libraries:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将讨论一些其他的库：
- en: '`Requests`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Requests`'
- en: '`BeautifulSoup`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BeautifulSoup`'
- en: '`Newspaper3k`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Newspaper3k`'
- en: Requests should already be included with Python and should not need to be installed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Requests应该已经包含在Python中，通常不需要单独安装。
- en: '`BeautifulSoup` can be installed with the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`BeautifulSoup`可以通过以下方式安装：'
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Newspaper3k` can be installed with this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`Newspaper3k`可以通过以下方式安装：'
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In [*Chapter 4*](B17105_04.xhtml#_idTextAnchor158), we also introduced a `draw_graph()`
    function that uses both NetworkX and scikit-network. You will need that code whenever
    we do network visualization. Keep it handy!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B17105_04.xhtml#_idTextAnchor158)中，我们还介绍了一个`draw_graph()`函数，它使用了NetworkX和scikit-network库。每次进行网络可视化时，你都需要使用到这段代码，记得保留它！
- en: 'You can find all the code for this chapter in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到本章的所有代码：[https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python)。
- en: Why cover Requests and BeautifulSoup?
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要介绍`Requests`和`BeautifulSoup`？
- en: We all like it when things are easy, but life is challenging, and things don’t
    always work out the way we want. In scraping, that’s laughably common. Initially,
    you can count on more things going wrong than going right, but if you are persistent
    and know your options, you will eventually get the data that you want.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都喜欢事物变得简单，但生活充满挑战，事情并不总是如我们所愿。在抓取过程中，这种情况常常发生，简直让人发笑。最初，你可以预期更多的事情会出错而不是成功，但只要你坚持不懈并了解自己的选择，最终你会获得你想要的数据。
- en: In the previous chapter, we covered the `Requests` Python library, because this
    gives you the ability to access and use any publicly available web data. You get
    a lot of freedom when working with `Requests`. This gives you the data, but making
    the data useful is difficult and time-consuming. We then used `BeautifulSoup`,
    because it is a rich library for dealing with HTML. With `BeautifulSoup`, you
    can be more specific about the kinds of data you extract and use from a web resource.
    For instance, we can easily harvest all of the external links from a website,
    or even the full text of a website, excluding all HTML.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了`Requests` Python库，因为它让你能够访问和使用任何公开可用的网页数据。使用`Requests`时，你有很大的自由度。它能为你提供数据，但使数据变得有用则是一个既困难又耗时的过程。接着，我们使用了`BeautifulSoup`，因为它是一个强大的HTML处理库。通过`BeautifulSoup`，你可以更具体地指定从网页资源中提取和使用的数据类型。例如，我们可以轻松地从一个网站收集所有的外部链接，甚至获取网站的完整文本，排除所有HTML代码。
- en: However, `BeautifulSoup` doesn’t give perfectly clean data by default, especially
    if you are scraping news stories from hundreds of websites, all of which have
    different headers and footers. The methods we explored in the previous chapter
    for using offsets to chop off headers and footers are useful when you are collecting
    text from only a few websites, and you can easily set up a few rules for dealing
    with each unique website, but then you have a scalability problem. The more websites
    you decide to scrape, the more randomness you need to be able to deal with. The
    headers are different, navigation is likely different, and the languages may be
    different. The difficulties faced during scraping are part of what makes it so
    interesting to me.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`BeautifulSoup`默认并不会提供完美清洁的数据，特别是当你从成百上千的网站抓取新闻故事时，这些网站都有不同的页头和页脚。我们在上一章中探讨的使用偏移量来裁剪页头和页脚的方法，在你只从少数几个网站收集文本时非常有用，你可以轻松为每个独特的网站设置一些规则进行处理，但当你想扩展抓取的网站数量时，就会遇到可扩展性问题。你决定抓取的网站越多，你就需要处理更多的随机性。页头不同，导航结构可能不同，甚至语言可能不同。抓取过程中遇到的困难正是让我觉得它非常有趣的一部分。
- en: Introducing Newspaper3k
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍`Newspaper3k`
- en: If you want to do NLP or transform text data into network data for use in Social
    Network Analysis and Network Science, you need clean data. `Newspaper3k` gives
    you the next step after `BeautifulSoup`, abstracting away even more of the cleanup,
    giving you clean text and useful metadata with less work. Much of the performance
    and data cleanup have been abstracted away. Also, since you now understand some
    approaches to cleaning data from the previous chapter, when you see the cleanliness
    of Newspaper3k’s data, you will probably have a better understanding of what is
    happening under the hood, and hopefully, you will also be quite impressed and
    thankful for the work they have done and the time they are saving for you.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想做自然语言处理（NLP）或将文本数据转化为社交网络分析和网络科学中使用的网络数据，你需要干净的数据。`Newspaper3k`提供了继`BeautifulSoup`之后的下一步，它进一步抽象了数据清理的过程，为你提供了更干净的文本和有用的元数据，减少了工作量。许多性能优化和数据清理工作都被抽象了出来。而且，由于你现在已经了解了上一章中关于数据清理的一些方法，当你看到`Newspaper3k`的数据清洁度时，你可能会更好地理解背后发生的事情，并且希望你会对他们的工作感到非常赞赏，感谢他们为你节省的时间。
- en: For now, we can consider `Newspaper3k` the “easy way” of collecting text off
    of the web, but it’s easy because it builds off of previous foundations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以认为`Newspaper3k`是从网上收集文本的“简便方法”，但它之所以简便，是因为它建立在之前的基础之上。
- en: 'You will still likely need to use `Requests` and `BeautifulSoup` in your web
    scraping and content analysis projects. We needed to cover them first. When pursuing
    a web scraping project, rather than start with the most basic and reinventing
    every tool we need along the way, perhaps we should figure out where to start
    by asking ourselves a few questions:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的网页抓取和内容分析项目中，你仍然可能需要使用`Requests`和`BeautifulSoup`。我们需要先了解这些内容。当进行网页抓取项目时，不要一开始就从最基础的部分开始，一边重造每个需要的工具，也许我们应该通过问自己几个问题，来决定从哪里开始：
- en: Can I do this with `Newspaper3k`?
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以用`Newspaper3k`做吗？
- en: No? OK, can I do this with `Requests` and `BeautifulSoup`?
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不行吗？好吧，那我能用`Requests`和`BeautifulSoup`做吗？
- en: No? OK, can I at least get to some data using `Requests`?
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不行吗？好吧，那我至少可以使用`Requests`获取一些数据吗？
- en: Wherever you get useful results is where you should start. If you can get everything
    you need from `Newpaper3k`, start there. If you can’t get what you need with `Newspaper3k`,
    but you can with `BeautifulSoup`, start there. If neither of these approaches
    works, then you’re going to need to use `Requests` to pull data, and then you’ll
    need to write text cleanup code.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你在哪里获得有用的结果，就应该从哪里开始。如果你能从`Newspaper3k`中获得所需的一切，就从那里开始。如果用`Newspaper3k`不能得到你需要的内容，但用`BeautifulSoup`可以，那就从`BeautifulSoup`开始。如果这两种方法都不可行，那你就需要使用`Requests`来获取数据，然后写清理文本的代码。
- en: I usually recommend that people start at the basics and only add complexity
    as needed, but I do not recommend this approach for data collection or when it
    comes to text scraping. There is little use in reinventing HTML parsers. There
    is no glory in unnecessary headaches. Use whatever gets you useful data the quickest,
    so long as it meets your project needs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常建议人们从基础开始，只有在必要时才增加复杂性，但对于数据收集或文本抓取，我不推荐这种方法。重造HTML解析器几乎没有用处，没有必要去制造不必要的麻烦。只要它能满足你的项目需求，使用任何能快速提供有用数据的工具。
- en: What is Newspaper3k?
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Newspaper3k？
- en: '`Newspaper3k` is a Python library that is useful for loading web text from
    news websites. However, unlike with `BeautifulSoup`, the objective is less about
    flexibility as it is about getting useful data *quickly*. I am most impressed
    with Newspaper3k’s ability to clean data, as the results are quite pure. I have
    compared my work using `BeautifulSoup` and `Newspaper3k`, and I am very impressed
    with the latter.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`Newspaper3k`是一个Python库，用于从新闻网站加载网页文本。然而，与`BeautifulSoup`不同，`Newspaper3k`的目标不在于灵活性，而是快速获取有用数据*快速*。我最为钦佩的是Newspaper3k的清洗数据的能力，结果相当纯净。我曾比较过使用`BeautifulSoup`和`Newspaper3k`的工作，后者给我留下了非常深刻的印象。'
- en: '`Newspaper3k` is not a replacement for `BeautifulSoup`. It can do some things
    that `BeautifulSoup` can do, but not everything, and it wasn’t written with flexibility
    for dealing with HTML in mind, which is where `BeautifulSoup` is strong. You give
    it a website, and it gives you the text on that website. With `BeautifulSoup`,
    you have more flexibility, in that you can choose to look for only links, paragraphs,
    or headers. `Newspaper3k` gives you text, summaries, and keywords. `BeautifulSoup`
    is a step below that in abstraction. It is important to understand where different
    libraries sit in terms of tech stacks. `BeautifulSoup` is a high-level abstraction
    library, but it is lower level than `Newspaper3k`. Similarly, `BeautifulSoup`
    is a higher-level library than `Requests`. It’s like the movie *Inception* – there
    are layers and layers and layers.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`Newspaper3k`不是`BeautifulSoup`的替代品。它可以做一些`BeautifulSoup`能做的事情，但并不是所有的，而且它的设计并没有考虑到处理HTML时的灵活性，这正是`BeautifulSoup`的强项。你给它一个网站，它会返回该网站的文本。使用`BeautifulSoup`时，你有更多的灵活性，你可以选择只查看链接、段落或标题。而`Newspaper3k`则提供文本、摘要和关键词。`BeautifulSoup`在这一抽象层次上略低一些。理解不同库在技术栈中的位置非常重要。`BeautifulSoup`是一个高层次的抽象库，但它的层次低于`Newspaper3k`。同样，`BeautifulSoup`的层次也高于`Requests`。这就像电影《盗梦空间》——有层层叠叠的结构。'
- en: What are Newspaper3k’s uses?
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Newspaper3k的用途是什么？
- en: '`Newspaper3k` is useful for getting to the clean text that exists in a web
    news story. That means parsing the HTML, chopping out non-useful text, and returning
    the news story, headline, keywords, and even text summary. The fact that it can
    return keywords and text summaries means that it’s got some pretty interesting
    NLP capabilities in the background. You don’t need to create a machine learning
    model for text summarization. `Newspaper3k` will do the work for you transparently,
    and surprisingly fast.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`Newspaper3k`对于获取网页新闻故事中的干净文本非常有用。这意味着它解析HTML，剪切掉无用的文本，并返回新闻故事、标题、关键词，甚至是文本摘要。它能返回关键词和文本摘要，意味着它背后具有一些相当有趣的自然语言处理能力。你不需要为文本摘要创建机器学习模型，`Newspaper3k`会透明地为你完成这项工作，而且速度惊人。'
- en: '`Newspaper3k` seems to have been inspired by the idea of parsing online news,
    but it is not limited to that. I have also used it for scraping blogs. If you
    have a website that you’d like to try scraping, give `Newspaper3k` a chance and
    see how it does. If that doesn’t work, use `BeautifulSoup`.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`Newspaper3k`似乎受到了在线新闻解析的启发，但它不限于此。我也曾用它来抓取博客。如果你有一个网站想尝试抓取，不妨给`Newspaper3k`一个机会，看看效果如何。如果不行，可以使用`BeautifulSoup`。'
- en: One weakness of `Newspaper3k` is that it is unable to parse websites that use
    JavaScript obfuscation to hide their content inside JavaScript rather than HTML.
    Web developers occasionally do this to discourage scraping, for various reasons.
    If you point `Newspaper3k` or `BeautifulSoup` at a website that is using JavaScript
    obfuscation, both of them will return very little or no useful results, as the
    data is hidden in JavaScript, which neither of these libraries is built to handle.
    The workaround is to use a library such as `Selenium` along with `Requests`, and
    that will often be enough to get to the data that you want. `Selenium` is outside
    the scope of this book, and often feels like more trouble than it is worth, so
    please explore the documentation if you get stuck behind JavaScript obfuscation,
    or just move on and scrape easier websites. Most websites are scrapable, and the
    ones that aren’t can often just be ignored as they may not be worth the effort.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`Newspaper3k`的一个弱点是，它无法解析使用JavaScript混淆技术将内容隐藏在JavaScript中，而不是HTML中的网站。网页开发者有时会这样做，以阻止抓取，原因多种多样。如果你将`Newspaper3k`或`BeautifulSoup`指向一个使用JavaScript混淆的网站，它们通常只会返回很少甚至没有有用的结果，因为数据隐藏在JavaScript中，而这两个库并不适用于处理这种情况。解决方法是使用像`Selenium`配合`Requests`这样的库，这通常足以获取你需要的数据。`Selenium`超出了本书的范围，而且经常让人觉得麻烦，不值得花费太多时间。因此，如果你在JavaScript混淆面前卡住了，可以查阅相关文档，或干脆跳过并抓取更简单的网站。大多数网站是可以抓取的，那些不能抓取的通常可以忽略，因为它们可能不值得投入太多精力。'
- en: Getting started with Newspaper3k
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用`Newspaper3k`
- en: 'Before you can use `Newspaper3k`, you must install it. This is as simple as
    running the following command:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`Newspaper3k`之前，你必须先安装它。这非常简单，只需要运行以下命令：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At one point in a previous installation, I received an error stating that an
    NLTK component was not downloaded. Keep an eye out for weird errors. The fix was
    as simple as running a command for an NLTK download. Other than that, the library
    has worked very well for me. Once the installation is complete, you will be able
    to import it into your Python code and make use of it immediately.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的安装过程中，我曾收到一个错误，提示某个NLTK组件未下载。留意奇怪的错误信息。解决办法非常简单，只需要运行一个NLTK下载命令。除此之外，库的表现一直非常好。安装完成后，你可以立即将它导入到Python代码中并使用。
- en: In the previous chapter, I showed flexible but more manual approaches to scraping
    websites. A lot of junk text snuck through, and cleaning the data was quite involved
    and difficult to standardize. `Newspaper3k` takes scraping to another level, making
    it easier than I have ever seen anywhere else. I recommend that you use `Newspaper3k`
    for your news scraping whenever you can.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我展示了灵活但更为手动的抓取网站方法。许多无用的文本悄悄混入其中，数据清理相当繁琐，且难以标准化。`Newspaper3k`将抓取提升到了另一个层次，让它变得比我见过的任何地方都要简单。我推荐你尽可能使用`Newspaper3k`进行新闻抓取。
- en: Scraping all news URLs from a website
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从一个网站抓取所有新闻URL
- en: 'Harvesting URLs from a domain using `Newspaper3k` is simple. This is all of
    the code required to load all the hyperlinks from a web domain:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Newspaper3k`从域名中提取URL非常简单。这是加载网页域名中所有超链接所需的全部代码：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However, there is one thing I want to point out: when you scrape all URLs this
    way, you will also find what I consider “junk URLs” that point to other areas
    of the website, not to articles. These can be useful, but in most cases, I just
    want the article URLs. This is what the URLs will look like if I don’t do anything
    to remove the junk:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我想指出一个问题：当你以这种方式抓取所有 URL 时，你也会发现我认为是“垃圾 URL”的一些链接，这些 URL 指向网站的其他区域，而不是文章。这些
    URL 可能有用，但在大多数情况下，我只想要文章的 URL。如果我不采取任何措施去删除垃圾，URL 会像这样：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Please take note that if you crawl a website at a different time, you will likely
    get different results. New content may have been added, and old content may have
    been removed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你在不同的时间爬取一个网站，你可能会得到不同的结果。新内容可能已被添加，旧内容可能已被删除。
- en: 'Everything under those first two URLs is what I consider junk, in most of my
    scraping. I want the article URLs, and those are URLs to specific category pages.
    There are several ways that this problem can be addressed:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我大多数抓取中，那些位于前两个 URL 下方的内容就是我认为的垃圾。我只需要文章 URL，那些是指向特定分类页面的 URL。有几种方法可以解决这个问题：
- en: You could drop URLs that include the word “category.” In this case, that looks
    perfect.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以删除包含“category”一词的 URL。在这种情况下，这看起来非常合适。
- en: You could drop URLs where the length of the URL is greater than a certain threshold.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以删除那些 URL 长度超过某个阈值的 URL。
- en: You could combine the two options into a single approach.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将这两种选项合并成一种方法。
- en: 'For this example, I have decided to go with the third option. I will drop all
    URLs that include the word “category,” as well as any URLs that are less than
    60 characters in length. You may want to experiment with various cutoff thresholds
    to see what works for you. The simple cleanup code looks like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我决定选择第三个选项。我将删除所有包含“category”一词的 URL，以及长度小于 60 个字符的 URL。你也许想尝试不同的截止阈值，看看哪个最适合你。简单的清理代码如下：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Our URL list now looks much cleaner, containing only article URLs. This is
    what we need:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的 URL 列表看起来更加干净，只包含文章 URL。这正是我们需要的：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We now have a clean URL list that we can iterate through, scrape each story,
    and load the text for our use.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个干净的 URL 列表，我们可以遍历它，抓取每个故事，并加载文本供我们使用。
- en: Before moving on, one thing that you should notice is that on this single web
    domain, the stories that they publish are multilingual. Most of the stories that
    they publish are in English, but some of them are not. If you were to point `Newspaper3k`
    at the domain (rather than at individual story URLs), it would likely be unable
    to correctly classify the language of the domain. It is best to do language lookups
    at the story level, not the domain level. I will show how to do this at the story
    level.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，你应该注意到，在这个单一的网络域上，他们发布的故事是多语言的。他们发布的大部分故事是英语的，但也有一些不是。如果你将 `Newspaper3k`
    指向该域（而不是指向单独的故事 URL），它可能无法正确地识别该域的语言。最好在故事级别做语言查找，而不是在域级别。我会展示如何在故事级别做这个。
- en: Scraping a news story from a website
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从网站抓取新闻故事
- en: 'We now have a list of story URLs that we want to scrape article text and metadata
    from. The next step is to use a chosen URL and harvest any data that we want.
    For this example, I will download and use the first story URL in our URL list:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个包含故事 URL 的列表，我们想从中抓取文章文本和元数据。下一步是使用选定的 URL，收集我们需要的任何数据。对于这个例子，我将下载并使用我们
    URL 列表中的第一个故事 URL：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'There are a few confusing lines in this code snippet, so I will explain it
    line by line:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码片段中有几行比较令人困惑，我会逐行解释：
- en: First, I load the `Article` function from the newspaper library, as that is
    used for downloading article data.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我从报纸库中加载 `Article` 函数，因为它用于下载文章数据。
- en: Next, I point `Article` at the first URL from our URL list, which is `urls[0]`.
    It has not done anything at this point; it has just been pointed at the source
    URL.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我将 `Article` 指向我们的 URL 列表中的第一个 URL，也就是 `urls[0]`。此时它并没有执行任何操作；它只是被指向了源 URL。
- en: Then, I download and parse the text from the given URL. This is useful for grabbing
    full text and headlines, but it will not capture article keywords.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我从给定的 URL 下载并解析文本。这对于获取完整的文章和标题很有用，但它不会捕获文章的关键词。
- en: Finally, I run the `nlp` component of `Article` to extract keywords.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我运行 `Article` 的 `nlp` 组件来提取关键词。
- en: With these four steps, I should now have all the data that I want for this article.
    Let’s dive in and see what we have!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这四个步骤，我现在应该已经拥有了这篇文章所需的所有数据。让我们深入看看，看看我们有什么！
- en: What is the article title?
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章标题是什么？
- en: '[PRE10]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Nice and clean. What about the text?
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 干净利落。那文本呢？
- en: '[PRE14]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: What is the article summary?
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章摘要是什么？
- en: '[PRE18]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: What language was the article written in?
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章是用什么语言写的？
- en: '[PRE23]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: What keywords were found in the article?
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章中发现了哪些关键词？
- en: '[PRE27]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: What image accompanies this story?
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇文章配的是什么图片？
- en: '[PRE45]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: And there is even more that you can do with `Newspaper3k`. I encourage you to
    read the library’s documentation and see what else can be useful to your work.
    You can read more at [https://newspaper.readthedocs.io/en/latest/](https://newspaper.readthedocs.io/en/latest/).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，你还可以使用 `Newspaper3k` 做更多的事情。我鼓励你阅读该库的文档，看看还有哪些功能能对你的工作有所帮助。你可以在[https://newspaper.readthedocs.io/en/latest/](https://newspaper.readthedocs.io/en/latest/)阅读更多内容。
- en: Scraping nicely and blending in
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优雅地抓取并融入其中
- en: 'There are two things that I try to do when building any scraper:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建任何爬虫时，我通常会做两件事：
- en: Blend in with the crowd
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与人群融为一体
- en: Don’t scrape too aggressively
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要过度抓取
- en: 'There is some overlap between both of these. If I blend in with actual website
    visitors, my scrapers will be less noticeable, and less likely to get blocked.
    Second, if I don’t scrape too aggressively, my scrapers are not likely to be noticed,
    and thus also less likely to be blocked. However, the second one is important,
    as it is not friendly to hit web servers too aggressively. It is better to throw
    in a 0.5 or 1-second wait between URL scrapes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者之间有些重叠。如果我与真实的网页访问者融为一体，我的爬虫就不那么显眼，也不太容易被封锁。其次，如果我不进行过度抓取，我的爬虫也不太可能被注意到，从而减少被封锁的可能性。然而，第二点更为重要，因为对
    web 服务器进行过度抓取是不友好的。最好在 URL 抓取之间设置 0.5 或 1 秒的等待：
- en: 'For the first idea, blending in with the crowd, you can spoof a browser user-agent.
    For instance, if you want your scraper to pretend to be the latest Mozilla browser
    running on macOS, this is how to do so:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第一个想法，即与人群融为一体，你可以伪装成一个浏览器用户代理。例如，如果你希望你的爬虫伪装成在 macOS 上运行的最新 Mozilla 浏览器，可以按如下方式操作：
- en: '[PRE49]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, to add a 1-second sleep between each URL scrape, you could use a `sleep`
    command:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，为了在每次抓取 URL 之间添加 1 秒钟的暂停，你可以使用 `sleep` 命令：
- en: '[PRE53]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The `user_agent` configuration is often enough to get past simple bot detection,
    and the 1-second sleep is a friendly thing to do that also helps with blending
    in.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`user_agent` 配置通常足以绕过简单的机器人检测，且 1 秒的暂停是一个友好的操作，有助于与人群融为一体。'
- en: Converting text into network data
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文本转换为网络数据
- en: 'To convert our freshly scraped text into network data, we can reuse the function
    that was created in the previous chapter. As a reminder, this function was created
    to use an NLP technique called **Named-Entity Recognition** (**NER**) to extract
    the people, places, and organizations mentioned in a document:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们刚刚抓取的文本转换为网络数据，我们可以重用前一章创建的函数。提醒一下，这个函数是使用一种名为**命名实体识别**（**NER**）的 NLP
    技术来提取文档中提到的人物、地点和组织：
- en: 'Here is the function that we will use:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们将使用的函数：
- en: '[PRE55]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We can simply throw our scraped text into this function and it should return
    an entity list:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以简单地将抓取的文本传入此函数，它应该返回一个实体列表：
- en: '[PRE76]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Perfect! Now, we can pass these entities to one additional function to get a
    `pandas` DataFrame of edge list data that we can use to construct a network.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完美！现在，我们可以将这些实体传递给另一个函数，以获取我们可以用来构建网络的 `pandas` DataFrame 边缘列表数据。
- en: 'Next, we will use the `get_network_data` function, which is coded as follows:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `get_network_data` 函数，其代码如下：
- en: '[PRE80]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'We can use it by passing in an entity list:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过传入实体列表来使用它：
- en: '[PRE92]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Upon inspection, this looks great. A network edge list must contain a source
    node and a target node, and we’ve now got both in place:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 经检查，效果很好。网络边缘列表必须包含一个源节点和一个目标节点，而现在我们已经有了这两者：
- en: '![Figure 5.1 – pandas DataFrame edge list of entities](img/B17105_05_001.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – pandas DataFrame 实体边缘列表](img/B17105_05_001.jpg)'
- en: Figure 5.1 – pandas DataFrame edge list of entities
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – pandas DataFrame 实体边缘列表
- en: That’s great. The fourth row is interesting, as NER successfully caught two
    different ways of representing the CDC, both spelled out and as an acronym. There
    seems to be a false positive in the first row, but I will explain how to clean
    network data in the next chapter. This is perfect for now.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。第四行很有趣，因为 NER 成功识别出了两种不同的表示 CDC 的方式，既有全称也有缩写。第一行似乎有一个误报，但我会在下一章讲解如何清理网络数据。现在这一切都很完美。
- en: End-to-end Network3k scraping and network visualization
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端到端 Network3k 抓取与网络可视化
- en: We now have everything we need to demonstrate two things. I want to show how
    to scrape several URLs and combine the data into a single DataFrame for use and
    storage, and you will learn how to convert raw text into network data and visualize
    it. We did the latter in the previous chapter, but it will be useful to do it
    one more time so that the learning sticks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有演示两个事项所需的东西。我想展示如何抓取多个URL，并将数据合并为一个DataFrame以供使用和存储，你将学习如何将原始文本转换为网络数据并可视化它。在上一章中我们做过这个，但再做一次会更有助于加深记忆。
- en: Combining multiple URL scrapes into a DataFrame
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将多个URL抓取结果合并到一个DataFrame中
- en: 'Between these two demonstrations, this is the most foundational and important
    part. We will use the results of this process in our next demonstration. In most
    real-world scraping projects, it is not useful to scrape a single URL repeatedly.
    Typically, you want to repeat these steps for any given domain that you scrape:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个演示之间，这部分是最基础和最重要的。我们将在下一个演示中使用此过程的结果。在大多数实际的抓取项目中，重复抓取单个URL没有什么意义。通常，你想对你抓取的任何域名重复这些步骤：
- en: Scrape all URLs.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抓取所有的URL。
- en: Drop the ones that you have already scraped text for.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除你已经抓取过文本的URL。
- en: Scrape the text of the URLs that remain.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抓取剩余URL的文本。
- en: For this demonstration, we will only be doing *step 1* and *step 3*. For your
    projects, you will usually need to come up with a process to drop the URLs you
    have already scraped, and this is dependent on where you are writing the post-scraped
    data. Essentially, you need to take a look at what you have and disregard any
    URLs that you have already used. This prevents repeated work, unnecessary scraping
    noise, unnecessary scraping burden on web servers, and duplicated data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个演示，我们只会做*步骤1*和*步骤3*。对于你的项目，你通常需要想出一个方法来删除你已经抓取的URL，这取决于你将抓取后的数据写入哪里。本质上，你需要检查你已经抓取的内容，并忽略你已经使用过的URL。这可以防止重复工作、不必要的抓取噪音、不必要的抓取负担以及重复的数据。
- en: 'The following code scrapes all URLs for a given domain, scrapes text for each
    URL discovered, and creates a `pandas` DataFrame for use or writing output to
    a file or a database. I am throwing one additional Python library at this: `tqdm`.
    The `tqdm` library is useful when you want to understand how long a process will
    take. If you are using this in backend automation, you will likely not want the
    `tqdm` functionality, but it is useful now, as you are learning.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码抓取给定域名的所有URL，抓取每个发现的URL的文本，并创建一个`pandas` DataFrame，供使用或输出到文件或数据库。我还加入了一个额外的Python库：`tqdm`。`tqdm`库在你想了解一个过程需要多长时间时非常有用。如果你在后台自动化中使用这个功能，你可能不需要`tqdm`的功能，但现在它很有用，因为你正在学习。
- en: 'You can install `tqdm` by running `pip` `install tqdm`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行`pip` `install tqdm`来安装`tqdm`：
- en: '![Figure 5.2 – TQDM progress bar in action](img/B17105_05_002.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – TQDM进度条的实际应用](img/B17105_05_002.jpg)'
- en: Figure 5.2 – TQDM progress bar in action
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – TQDM进度条的实际应用
- en: 'This is the end-to-end Python code that takes a domain name and returns a `pandas`
    DataFrame of scraped stories:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的Python代码，它接受一个域名并返回一个抓取的故事的`pandas` DataFrame：
- en: '[PRE94]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'To use this function, you can run the following code, and point it at any news
    domain of interest:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个函数，你可以运行以下代码，并指向任何感兴趣的新闻域名：
- en: '[PRE95]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: You should now have a clean DataFrame of news stories to work with. If you are
    running into 404 (Page Not Found) errors, you may need to place some try/except
    exception handling code into the function. I leave this and other edge cases in
    your hands. However, the closer the time is between URL scraping and article text
    scraping, the less likely you will run into 404 errors.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该有一个干净的新闻故事DataFrame可供使用。如果你遇到404（页面未找到）错误，可能需要在函数中添加一些try/except异常处理代码。我将这些和其他边缘情况留给你处理。不过，URL抓取和文章文本抓取的时间间隔越短，遇到404错误的可能性就越小。
- en: Let’s inspect the results!
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检查一下结果！
- en: '![Figure 5.3 – pandas DataFrame of scraped URL data](img/B17105_05_003.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 抓取的URL数据的pandas DataFrame](img/B17105_05_003.jpg)'
- en: Figure 5.3 – pandas DataFrame of scraped URL data
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 抓取的URL数据的pandas DataFrame
- en: Cool! The `tqdm` progress bar worked until completion, and we can also see that
    the final story’s language was set to Spanish. This is exactly what we want. If
    we had tried to detect the language of the overall domain, by scraping the landing
    page (home page), the language detection component might have given a false reading
    or even returned nothing. This website has both English and Spanish language stories,
    and we can see this at the story level.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 酷！`tqdm`进度条一直工作到完成，我们还可以看到最终故事的语言已被设置为西班牙语。这正是我们想要的。如果我们试图通过抓取主页（landing page）来检测整个域的语言，语言检测组件可能会给出错误的结果，甚至什么都不返回。这个网站既有英语故事也有西班牙语故事，我们可以在故事级别看到这一点。
- en: Capturing the language of a piece of text is very useful for NLP work. Often,
    machine learning classifiers that are trained in one language will have difficulty
    when used against another, and when using unsupervised machine learning (clustering)
    against text data, data written in various languages will clump together. My advice
    is to use the captured language data to split your data by language for any downstream
    NLP enrichment work. You will have better results this way, and your results will
    be much simpler to analyze as well.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉一段文本的语言对于NLP工作非常有用。通常，经过一种语言训练的机器学习分类器在应用到另一种语言时会遇到困难，而在使用无监督机器学习（聚类）处理文本数据时，用不同语言写的数据会聚集在一起。我的建议是，利用捕捉到的语言数据将数据按语言拆分，以便进行后续的NLP扩展工作。这样你会获得更好的结果，同时分析结果也会变得更加简单。
- en: Next, let’s use these stories to create network data and visualizations!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用这些故事来创建网络数据和可视化！
- en: Converting text data into a network for visualization
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将文本数据转换成网络进行可视化
- en: Several times in this book, we have taken text, extracted entities, created
    network data, created a network, and then visualized the network. We will be doing
    the same thing here. The difference is that we now have a `pandas` DataFrame that
    consists of several news articles, and each of them can be converted into a network.
    For this example, I’ll only do it twice. From this point on, you should have no
    trouble converting text into networks, and you can reuse the code that has already
    been written.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们已经多次提取文本、提取实体、创建网络数据、构建网络并进行可视化。我们在这里也会做同样的事情。不同之处在于，我们现在有一个包含多篇新闻文章的`pandas`
    DataFrame，每篇文章都可以转换成一个网络。对于这个示例，我只做两次。从现在开始，你应该不会再有困难将文本转换成网络，你也可以重用已经写好的代码。
- en: 'Our entity extraction is built upon an English language NLP model, so let’s
    only use English language stories. To keep things simple, we will do this demonstration
    with the second and fourth articles in the DataFrame, as these gave interesting
    and clean results:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实体提取是基于英语语言的NLP模型构建的，因此我们只使用英语语言的故事。为了简化演示，我们将使用DataFrame中的第二篇和第四篇文章，因为它们给出了有趣且干净的结果：
- en: 'First, we will use the second article. You should see that I am loading `df[''text''][1]`,
    where `[1]` is the second row as indexing starts at `0`:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用第二篇文章。你应该能看到我正在加载`df['text'][1]`，其中`[1]`是第二行，因为索引是从`0`开始的：
- en: '[PRE96]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'This is the network visualization:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这是网络可视化：
- en: '![Figure 5.4 – Network visualization of article entity relationships (second
    article)](img/B17105_05_004.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 文章实体关系的网络可视化（第二篇文章）](img/B17105_05_004.jpg)'
- en: Figure 5.4 – Network visualization of article entity relationships (second article)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 文章实体关系的网络可视化（第二篇文章）
- en: This looks good but is very simple. A news article is typically about a few
    individuals and organizations, so this is not surprising. We can still see a relationship
    between a couple of wildlife groups, a relationship between a person and a university,
    and a relationship between **Australia** and **Koalas**. All of this seems realistic.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来不错，但非常简单。一篇新闻文章通常涉及几个个人和组织，所以这并不令人惊讶。我们仍然可以看到一些野生动物保护组织之间的关系，一个人与大学之间的关系，以及**澳大利亚**和**考拉**之间的关系。所有这些看起来都很现实。
- en: 'Next, let’s try the fourth article:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们尝试第四篇文章：
- en: '[PRE101]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'This is the network visualization. This one is much more interesting and involved:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是网络可视化。这一部分更加有趣且复杂：
- en: '![Figure 5.5 – Network visualization of article entity relationships (fourth
    article)](img/B17105_05_005.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 文章实体关系的网络可视化（第四篇文章）](img/B17105_05_005.jpg)'
- en: Figure 5.5 – Network visualization of article entity relationships (fourth article)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 文章实体关系的网络可视化（第四篇文章）
- en: This is a much richer set of entities than is commonly found in news stories,
    and in fact, this story has more entities and relationships than the book that
    we investigated in the previous chapter, *The Metamorphosis*. This looks great,
    and we can investigate the relationships that have been uncovered.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个比新闻报道中常见的实体集合更加丰富的集合，事实上，这个故事中涉及的实体和关系比我们在上一章研究的《变形记》这本书还要多。这看起来很棒，我们可以进一步研究这些被揭示出来的关系。
- en: From this point on in this book, I will primarily be using Twitter data to create
    networks. I wanted to explain how to do this to any text, as this gives freedom
    to uncover relationships in any text, not just social media text. However, you
    should understand this by now. The remainder of this book will focus more on analyzing
    networks than on creating network data. Once text data has been converted into
    a network, the rest of the network analysis information is equally relevant and
    useful.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 从本书这一章开始，我将主要使用 Twitter 数据来创建网络。我想解释如何对任何文本进行这种操作，因为这让我们能够自由地揭示任何文本中的关系，而不仅仅是社交媒体文本。然而，你应该已经理解这一点了。本书剩余部分将更多关注分析网络，而不是创建网络数据。一旦文本数据转化为网络，其余的网络分析信息同样具有相关性和实用性。
- en: Introducing the Twitter Python Library
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Twitter Python 库
- en: Twitter is a goldmine for NLP projects. It is a very active social network with
    not too strict moderation, which means that users are pretty comfortable posting
    about a wide variety of topics. This means that Twitter can be useful for studying
    lighthearted topics, but it can also be used to study more serious topics. You
    have a lot of flexibility.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 是一个非常适合 NLP 项目的宝藏。它是一个非常活跃的社交网络，且审查制度不太严格，这意味着用户在讨论各种话题时都很自在。这意味着 Twitter
    不仅适合研究轻松的话题，也可以用来研究更为严肃的话题。你有很大的灵活性。
- en: Twitter also has a simple API to work with, compared to other social networks.
    It is relatively simple to get started with, and it can be used to capture data
    that can be used for a lifetime of NLP research. In my personal NLP research,
    learning to scrape Twitter supercharged and accelerated my NLP learning. Learning
    NLP is much more enjoyable when you have data that is interesting to you. I have
    used Twitter data to understand various networks, create original NLP techniques,
    and create machine learning training data. I don’t use Twitter much, but I have
    found it to be a goldmine for all things NLP.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他社交网络相比，Twitter 也有一个相对简单的 API。它很容易入门，并且可以用来捕捉数据，这些数据可以为未来的 NLP 研究提供支持。在我个人的
    NLP 研究中，学习如何抓取 Twitter 数据极大地推动了我的 NLP 学习。当你有了自己感兴趣的数据时，学习 NLP 变得更加有趣。我曾使用 Twitter
    数据来理解各种网络、创造原创的 NLP 技术以及生成机器学习训练数据。我虽然不常用 Twitter，但我发现它对所有与 NLP 相关的内容来说都是一座金矿。
- en: What is the Twitter Python Library?
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Twitter Python 库？
- en: For several years, Twitter has been exposing its API, to allow software developers
    and researchers to make use of their data. The API is a bit of a challenge to
    use as the documentation is a bit scattered and confusing, so a Python library
    was created to make working with the API much easier. I will explain how to use
    the Python library, but you will need to explore the Twitter API itself to learn
    more about the various limits that Twitter has set in place to prevent overuse
    of the API.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，Twitter 一直在开放其 API，以便软件开发者和研究人员能够利用他们的数据。由于文档有些零散且令人困惑，使用该 API 是一项挑战，因此创建了一个
    Python 库来简化与 API 的交互。我将解释如何使用这个 Python 库，但你需要自行探索 Twitter API，了解 Twitter 为防止过度使用
    API 所设置的各种限制。
- en: You can read more about the Twitter API at [https://developer.twitter.com/en/docs](https://developer.twitter.com/en/docs).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://developer.twitter.com/en/docs](https://developer.twitter.com/en/docs)查看更多关于
    Twitter API 的信息。
- en: What are the Twitter Library’s uses?
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Twitter 库有哪些用途？
- en: Once you can use the Twitter library and API, you have total flexibility in
    what you use it for to research. You could use it to learn about the K-Pop music
    scene, or you could use it to keep an eye on the latest happenings in machine
    learning or data science.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你学会了使用 Twitter 库和 API，你就可以完全自由地用它来做任何研究。你可以用它来了解 K-Pop 音乐圈，或者用它来关注机器学习或数据科学领域的最新动态。
- en: Another use of this is analyzing entire audiences. For instance, if an account
    has 50,000 followers, you can use the Twitter Library to load data about all 50,000
    of the followers, including their usernames and descriptions. With these descriptions,
    you could use clustering techniques to identify the various subgroups that exist
    in a larger group. You could also use this kind of data to potentially identify
    bots and other forms of artificial amplification.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用途是分析整个受众。例如，如果一个账户有 50,000 个粉丝，你可以使用 Twitter 库加载关于所有 50,000 个粉丝的数据，包括他们的用户名和描述。利用这些描述，你可以使用聚类技术来识别更大群体中的各种子群体。你还可以使用这类数据潜在地识别机器人和其他形式的人工放大。
- en: I recommend that you find something that you are curious about, and then chase
    it and see where it leads. This curiosity is an excellent driver for building
    skills in NLP and Social Network Analysis.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你找到一些你感兴趣的东西，然后追寻它，看看它会引导你到哪里。这种好奇心是培养 NLP 和社交网络分析技能的极好驱动力。
- en: What data can be harvested from Twitter?
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可以从 Twitter 收集哪些数据？
- en: Even compared to 2 years ago, it seems that Twitter has expanded its API offerings
    to allow for many different kinds of data science and NLP projects. However, in
    their **version one** (**V1**) API, Twitter would return a dictionary containing
    a great deal of the data that they have available. This has changed a bit in their
    V2 API, as they now require developers to specify which data they are requesting.
    This has made it more difficult to know all of the data that Twitter has made
    available. Anyone who will be working with the Twitter API is going to need to
    spend time reading through the documentation to see what is available.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 即使与两年前相比，Twitter 似乎已经扩展了其 API 提供的功能，允许更多不同类型的数据科学和 NLP 项目。然而，在他们的 **版本一**（**V1**）API
    中，Twitter 会返回一个包含大量数据的字典，这些数据都是他们可以提供的。这在他们的 V2 API 中有所变化，现在他们要求开发者明确指定请求哪些数据。这使得知道
    Twitter 提供的所有数据变得更加困难。任何与 Twitter API 一起工作的人都需要花时间阅读文档，以了解有哪些可用的数据。
- en: 'For my research, I am typically only interested in a few things:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我的研究，我通常只对以下几个方面感兴趣：
- en: Who is posting something?
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁在发布内容？
- en: What have they posted?
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们发布了什么？
- en: When was it posted?
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是什么时候发布的？
- en: Who are they mentioning?
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们提到了谁？
- en: What hashtags are they using?
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们使用了什么话题标签？
- en: All of this is easy to pull from the Twitter API, and I will show you how. But
    this is not the limit of Twitter’s offerings. I have recently been impressed by
    some of the more recently discovered data that the V2 API exposes, but I do not
    understand it well enough to write about it, yet. When you run into something
    that feels like it should be exposed by the API, check the documentation. In my
    experience working with the Twitter API, some things that should be exposed by
    default now take a bit of extra work than in V1\. Try to figure out how to get
    what you need.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些内容都可以轻松地通过 Twitter API 获取，我会向你展示如何操作。但这并不是 Twitter 提供的全部功能。我最近对 V2 API 曝露的一些新发现的数据印象深刻，但我还没有足够的了解来写关于它的内容。当你遇到感觉应该由
    API 曝露的内容时，查阅文档。在我与 Twitter API 工作的经验中，一些本应默认暴露的内容，现在相比于 V1，可能需要做更多额外的工作。尝试找出如何获取你需要的内容。
- en: Getting Twitter API access
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取 Twitter API 访问权限
- en: 'Before you can do anything with the Twitter API, the first thing you need to
    do is get access:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在你能使用 Twitter API 做任何事情之前，首先你需要做的就是获得访问权限：
- en: First, create a Twitter account. You don’t need to use it to post anything,
    but you do need to have an account.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个 Twitter 账户。你不需要用它发布任何内容，但你确实需要有一个账户。
- en: 'Next, go to the following URL to request API access: [https://developer.twitter.com/en/apply-for-access](https://developer.twitter.com/en/apply-for-access).'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，访问以下 URL 请求 API 访问权限：[https://developer.twitter.com/en/apply-for-access](https://developer.twitter.com/en/apply-for-access)。
- en: Applying for access can vary in time from a few minutes to a few days. You will
    need to fill out a few forms specifying how you will be using the data and agreeing
    to abide by Twitter’s Terms of Service. In describing your use of Twitter data,
    you can specify that you are using this for learning NLP and Social Network Analysis.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 申请访问权限的时间可能从几分钟到几天不等。你需要填写一些表格，说明你将如何使用这些数据，并同意遵守 Twitter 的服务条款。在描述你使用 Twitter
    数据的方式时，你可以说明你是为了学习 NLP 和社交网络分析而使用这些数据。
- en: 'Once you have been granted access, you will have your own Developer Portal.
    Search around in the authentication section until you see something like this:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你获得了访问权限，你将拥有自己的开发者门户。在身份验证部分搜索，直到看到类似这样的内容：
- en: '![Figure 5.6 – Twitter Authentication Bearer Token](img/B17105_05_006.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – Twitter 认证 Bearer Token](img/B17105_05_006.jpg)'
- en: Figure 5.6 – Twitter Authentication Bearer Token
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – Twitter 认证 Bearer Token
- en: Specifically, you are looking for a **Bearer Token**. Generate one and keep
    it somewhere safe. You will use this to authenticate with the Twitter API.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你需要一个 **Bearer Token**。生成一个并将其保存在安全的地方。你将用它来进行 Twitter API 的身份验证。
- en: Once you have generated a Bearer Token, you should be all set to work with the
    Twitter API through the Twitter Python library.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你生成了 Bearer Token，你就可以通过 Twitter Python 库开始与 Twitter API 进行交互了。
- en: Authenticating with Twitter
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Twitter 身份验证
- en: 'Before you can authenticate, you need to install the Twitter Python library:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行身份验证之前，你需要安装 Twitter Python 库：
- en: 'You can do so by running the following:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过运行以下代码来实现：
- en: '[PRE106]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Next, in your notebook of choice, try importing the library:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在你选择的笔记本中，尝试导入库：
- en: '[PRE107]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Next, you will need to authenticate with Twitter using your Bearer Token. Replace
    the `bearer_token` text in the following code with your own Bearer Token and try
    authenticating:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你需要使用 Bearer Token 来进行 Twitter 身份验证。将以下代码中的 `bearer_token` 文本替换为你自己的 Bearer
    Token，然后尝试进行身份验证：
- en: '[PRE108]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: If this doesn’t fail, then you should be authenticated and ready to start scraping
    tweets, connections, and more.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有失败，那么你应该已经通过身份验证，并准备好开始抓取推文、连接以及更多数据。
- en: Scraping user tweets
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抓取用户推文
- en: 'I’ve created two helper functions for loading user tweets into a `pandas` DataFrame.
    If you want more data than this function returns, you will need to extend `tweet_fields`,
    and possibly add `user_fields`:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了两个辅助函数，用于将用户推文加载到 `pandas` DataFrame 中。如果你需要更多的数据，可以扩展 `tweet_fields`，并可能需要添加
    `user_fields`：
- en: 'Please see the `search_tweets()` function in the following code block to see
    how `user_fields` can be added to a Twitter call. This function does not use `user_fields`,
    as we are already passing in a username:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请查看以下代码块中的 `search_tweets()` 函数，了解如何将 `user_fields` 添加到 Twitter 调用中。此函数不使用 `user_fields`，因为我们已经传入了用户名：
- en: '[PRE110]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'This first function takes a Twitter username and returns its `user_id`. This
    is important because some Twitter calls require a `user_id`, not a `username`.
    The following function uses `user_id` to look up a user’s tweets:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个函数接受一个 Twitter 用户名并返回其 `user_id`。这是很重要的，因为某些 Twitter 调用需要 `user_id` 而不是 `username`。以下函数使用
    `user_id` 来查找该用户的推文：
- en: '[PRE113]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: In this function, the `twitter_api.get_timelines()` function is doing most of
    the work. I have specified `tweet_fields` that I want, I’ve passed in a user’s
    `user_id`, I’ve specified that I want the latest `100` tweets by that person,
    and I’ve specified that I want the data returned in JSON format, which is easy
    to convert into a `pandas` DataFrame. If I call this function, I should get immediate
    results.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，`twitter_api.get_timelines()` 函数完成了大部分工作。我已经指定了我需要的 `tweet_fields`，传入了用户的
    `user_id`，指定了要获取该用户最近的 `100` 条推文，并指定返回的数据格式为 JSON，方便转换为 `pandas` DataFrame。如果调用这个函数，我应该能立即获得结果。
- en: 'Let’s see what Santa Claus talks about:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看圣诞老人都在谈些什么：
- en: '[PRE120]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'We should now see a preview of five of Santa’s tweets:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该能看到圣诞老人五条推文的预览：
- en: '![Figure 5.7 – pandas DataFrame of Santa Claus tweets](img/B17105_05_007.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 圣诞老人推文的 pandas DataFrame](img/B17105_05_007.jpg)'
- en: Figure 5.7 – pandas DataFrame of Santa Claus tweets
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 圣诞老人推文的 pandas DataFrame
- en: Perfect. We now have the most recent `100` tweets made by Santa Claus.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 完美。现在我们已经获取到了圣诞老人最近的 `100` 条推文。
- en: 'I have one additional helper function that I would like to give you. This one
    takes the `text` field and extracts entities and hashtags; we’ll be using these
    to draw social networks:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我还为你提供了一个额外的辅助函数。这个函数会提取 `text` 字段中的实体和标签，我们将利用这些信息绘制社交网络：
- en: '[PRE122]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'We can add this function as an enrichment step:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将这个函数作为一个增强步骤添加进去：
- en: '[PRE131]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'This gives us additional useful data:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了额外的有用数据：
- en: '![Figure 5.8 – pandas DataFrame of enriched Santa Claus tweets](img/B17105_05_008.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 丰富版圣诞老人推文的 pandas DataFrame](img/B17105_05_008.jpg)'
- en: Figure 5.8 – pandas DataFrame of enriched Santa Claus tweets
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 丰富版圣诞老人推文的 pandas DataFrame
- en: This is perfect for the rest of the work we will be doing in this chapter, but
    before we move on, we will also see how to scrape connections.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常适合我们在本章接下来的工作，但在继续之前，我们也将看看如何抓取连接信息。
- en: Scraping user following
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抓取用户关注列表
- en: 'We can easily scrape all accounts that an account follows. This can be done
    with the following function:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松抓取一个账户所关注的所有账户。可以通过以下函数来完成：
- en: '[PRE134]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Here, I have specified `max_results=1000`. That is the maximum that Twitter
    will return at a time, but you can load much more than 1,000\. You will need to
    pass in a `'next_token'` key to continue harvesting sets of `1000` followers.
    You can do something similar to load more than 100 tweets by a person. Ideally,
    you should use recursion in programming to do this, if you have the need. You
    can use the preceding function to load the first batch, and you should be able
    to extend it if you need to build in recursion.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我指定了`max_results=1000`。这是Twitter一次最多返回的结果数，但你可以加载比1000条更多的数据。你需要传入一个`'next_token'`键，继续获取`1000`条粉丝的数据集。你也可以做类似的操作来加载某个用户的超过100条推文。理想情况下，如果你有这个需求，应该在编程中使用递归。你可以使用前面的函数加载第一批数据，如果需要构建递归功能，应该能够扩展它。
- en: 'You can call the following function:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以调用以下函数：
- en: '[PRE135]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'This will give you results in this format:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你提供以下格式的结果：
- en: '![Figure 5.9 – pandas DataFrame of accounts Santa Claus follows on Twitter](img/B17105_05_009.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 圣诞老人追踪的Twitter账户的pandas DataFrame](img/B17105_05_009.jpg)'
- en: Figure 5.9 – pandas DataFrame of accounts Santa Claus follows on Twitter
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 圣诞老人追踪的Twitter账户的pandas DataFrame
- en: For investigating subgroups that exist inside a group, it is useful to include
    the account description, as people are often descriptive about their interests
    and political affiliations. To capture the description, you need to include the
    description in the `user_fields` list.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 对于调查一个群体内部存在的子群体，包含账户描述是非常有用的，因为人们通常会描述自己的兴趣和政治倾向。为了捕获描述，你需要将描述包含在`user_fields`列表中。
- en: Scraping user followers
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抓取用户粉丝
- en: 'Scraping followers is nearly identical. Here is the code:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 抓取粉丝几乎是相同的。以下是代码：
- en: '[PRE136]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'You can call the function:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以调用这个函数：
- en: '[PRE137]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: This will give you results in the same format as was shown previously. Be sure
    to include the account description.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你提供与之前所示相同格式的结果。确保包含账户描述。
- en: Scraping using search terms
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用搜索词抓取
- en: Collecting tweets about a search term is also useful. You can use this to explore
    *who* participates in discussions about a search term, but also to collect the
    tweets themselves, for reading and processing.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 收集关于搜索词的推文也很有用。你可以用这个来探索*谁*参与了关于某个搜索词的讨论，同时也可以收集这些推文，以便阅读和处理。
- en: 'Here is the code that I have written for scraping by search term:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我为根据搜索词抓取推文编写的代码：
- en: '[PRE138]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: This function is a bit more involved than the previous functions, as I have
    specified `tweet_fields` as well as `user_fields` that I am interested in. To
    capture the username, I needed to specify an expansion on `author_id`, and finally,
    I want 100 of the latest tweets. If you want to include additional data, you will
    need to explore the Twitter API to find out how to add the data field of interest.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数比之前的函数稍微复杂一些，因为我指定了`tweet_fields`和`user_fields`，这是我感兴趣的内容。为了捕获用户名，我需要在`author_id`上指定一个扩展，最后，我想要100条最新的推文。如果你想包含额外的数据，你需要探索Twitter
    API，找出如何添加你感兴趣的数据字段。
- en: 'You can call the function like so:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像这样调用该函数：
- en: '[PRE139]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'I am also enriching the `pandas` DataFrame so that it includes user mentions
    and hashtags via the `wrangle_and_enrich()` function call. This results in the
    following `pandas` DataFrame:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我还通过`wrangle_and_enrich()`函数调用，丰富了`pandas` DataFrame，使其包括用户提及和话题标签。这样就得到了以下的`pandas`
    DataFrame：
- en: '![Figure 5.10 – pandas DataFrame of Twitter search tweets](img/B17105_05_010.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – Twitter搜索推文的pandas DataFrame](img/B17105_05_010.jpg)'
- en: Figure 5.10 – pandas DataFrame of Twitter search tweets
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – Twitter搜索推文的pandas DataFrame
- en: These search tweets will be perfect for creating social network visualizations
    as the tweets come from multiple accounts. In the top two rows of data, you may
    visually notice that there is a relationship between **intempestades**, **pascal_bornet**,
    and **cogautocom**. This would show as connected nodes if we were to visualize
    this network.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这些搜索推文非常适合用于创建社交网络可视化，因为这些推文来自多个账户。在数据的前两行，你可能会在视觉上注意到**intempestades**、**pascal_bornet**和**cogautocom**之间有关系。如果我们要可视化这个网络，它们将显示为相连的节点。
- en: Converting Twitter tweets into network data
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将Twitter推文转换为网络数据
- en: Converting social media data into network data is much easier than raw text.
    With Twitter, this is fortunate, as tweets tend to be quite short. This is because
    users frequently tag each other in their tweets for visibility and interaction,
    and they often associate their tweets with hashtags as well, and these associations
    can be used to build networks.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 将社交媒体数据转换为网络数据比处理原始文本要容易得多。幸运的是，推特的情况正好符合这一点，因为推文通常很简短。这是因为用户经常在推文中互相标记，以增加可见性和互动，而且他们经常将推文与话题标签关联，这些关联可以用来构建网络。
- en: 'Using user mentions and hashtags, there are several different kinds of networks
    that we can create:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 使用用户提及和话题标签，我们可以创建几种不同类型的网络：
- en: '*Account to Mention Networks (@ -> @)*: Useful for analyzing social networks.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*账户到提及网络 (@ -> @)*：有助于分析社交网络。'
- en: '*Account to Hashtag Networks (@ -> #)*: Useful for finding communities that
    exist around a theme (hashtag).'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*账户到话题标签网络 (@ -> #)*：有助于找到围绕某个主题（话题标签）存在的社区。'
- en: '*Mention to Hashtag Network (@ -> #)*: Similar to the previous one, but linking
    to mentioned accounts, not the tweet account. This is also useful for finding
    communities.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提及到话题标签网络 (@ -> #)*：类似于之前的网络，但链接到被提及的账户，而不是推文账户。这对于寻找社区也很有用。'
- en: '*Hashtag to Hashtag Networks (# -> #)*: Useful for finding related themes (hashtags)
    and emerging trending topics.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*话题标签到话题标签网络 (# -> #)*：有助于发现相关的主题（话题标签）和新兴的趋势话题。'
- en: Additionally, you could use NER to extract additional entities from the text,
    but tweets are pretty short, so this may not give much useful data.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以使用NER从文本中提取额外的实体，但推文通常比较简短，因此这可能不会提供太多有用的数据。
- en: In the next few sections, you will learn how to do the first and third types
    of networks.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个部分中，你将学习如何构建第一类和第三类网络。
- en: Account to Mention Network (@ -> @)
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 账户到提及网络 (@ -> @)
- en: 'I have created a useful helper function to convert a `pandas` DataFrame into
    this Account to Mention network data:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了一个有用的辅助函数，将`pandas` DataFrame转换为此账户到提及网络数据：
- en: '[PRE140]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'There’s quite a lot going on in this function:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数中涉及的内容非常多：
- en: First, we take a copy of the username, users, and text fields from the `df`
    DataFrame and use them in the `user_network_df` DataFrame. Each row of the `users`
    field contains a list of users, so we then “explode” the `users` field, creating
    a separate row for each user in the DataFrame. We also drop rows that do not contain
    any users.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从`df` DataFrame中复制用户名、用户和文本字段，并将它们用于`user_network_df` DataFrame。`users`字段的每一行包含一个用户列表，因此我们会“展开”`users`字段，为每个用户在DataFrame中创建一行。我们还会删除不包含任何用户的行。
- en: Next, we remove all `@` characters so that the data and visualization will be
    more readable.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们移除所有的`@`字符，这样数据和可视化结果会更加易读。
- en: Then, we rename all the columns in the DataFrame, in preparation for creating
    our graph. NetworkX’s graphs expect a source and target field, and the count field
    can also be passed in as additional data.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们重命名DataFrame中的所有列，为创建我们的图形做准备。NetworkX的图形需要源和目标字段，`count`字段也可以作为额外数据传入。
- en: Next, we do aggregation and count each source-target relationship in the DataFrame.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们进行聚合，并统计DataFrame中每个源-目标关系的数量。
- en: Finally, we sort and return the DataFrame. We did not need to sort the DataFrame,
    but I tend to do this, as it can help with looking through the DataFrame or troubleshooting.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们对DataFrame进行排序并返回。虽然我们不需要对DataFrame进行排序，但我习惯这样做，因为它有助于查看DataFrame或进行故障排除。
- en: 'You can pass the `search_tweets` DataFrame to this function:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将`search_tweets` DataFrame传递给这个函数：
- en: '[PRE141]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'You will get an edge list DataFrame back of user relationships. We will use
    this to construct and visualize our network. Look closely and you should see that
    there is an additional **count** field. We will use this in later chapters as
    a threshold for choosing which edges and nodes to show in a visualization:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到一个边列表DataFrame，表示用户之间的关系。我们将利用这个来构建和可视化我们的网络。仔细观察，你应该能看到有一个额外的**count**字段。我们将在后面的章节中使用这个字段作为选择哪些边和节点在可视化中显示的阈值：
- en: '![Figure 5.11 – Account to Mention pandas DataFrame edge list](img/B17105_05_011.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 账户到提及 pandas DataFrame 边列表](img/B17105_05_011.jpg)'
- en: Figure 5.11 – Account to Mention pandas DataFrame edge list
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 账户到提及 pandas DataFrame 边列表
- en: Each row in this DataFrame shows a relationship between one user (**source**)
    and another (**target**).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这个DataFrame中的每一行表示一个用户（**源**）与另一个用户（**目标**）之间的关系。
- en: 'Mention to Hashtag Network (@ -> #)'
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '提及到话题标签网络 (@ -> #)'
- en: 'I have created a useful helper function to convert a `pandas` DataFrame into
    **Mention** to Hashtag network data. This function is similar to the previous
    one, but we load users and hashtags and do not use the original account’s username
    at all:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了一个有用的辅助函数，将 `pandas` DataFrame 转换为 **提及** 到话题标签的网络数据。这个函数与之前的类似，但我们加载了用户和话题标签，而完全不使用原账户的用户名：
- en: '[PRE142]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'You will get an edge list DataFrame back of user relationships. As shown previously,
    a **count** field is also returned, and we will use it in a later chapter as a
    threshold for choosing which nodes and edges to show:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 你将获得一个用户关系的边列表 DataFrame。如前所示，还会返回一个 **count** 字段，我们将在后面的章节中将其用作选择显示哪些节点和边的阈值：
- en: '![Figure 5.12 – Mention to Hashtag pandas DataFrame edge list](img/B17105_05_012.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – 提及到话题标签的 pandas DataFrame 边列表](img/B17105_05_012.jpg)'
- en: Figure 5.12 – Mention to Hashtag pandas DataFrame edge list
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 提及到话题标签的 pandas DataFrame 边列表
- en: Each row in this DataFrame shows a relationship between one user (source) and
    a hashtag (target).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 DataFrame 中的每一行显示一个用户（源）和一个话题标签（目标）之间的关系。
- en: End-to-end Twitter scraping
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头到尾抓取 Twitter 数据
- en: 'I hope that the preceding code and examples have shown how easy it is to use
    the Twitter API to scrape tweets, and I hope you can also see how easy it is to
    transform tweets into networks. For this chapter’s final demonstration, I want
    you to follow a few steps:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望之前的代码和示例已经展示了如何轻松使用 Twitter API 来抓取推文，也希望你能看到如何轻松地将推文转化为网络。在本章的最终演示中，我希望你能按照以下步骤进行：
- en: Load a `pandas` DataFrame containing tweets related to Network Science.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个包含与网络科学相关的推文的 `pandas` DataFrame。
- en: Enrich the DataFrame so that it includes user mentions and hashtags as separate
    fields.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丰富该 DataFrame，使其包括用户提及和话题标签作为独立字段。
- en: Create Account to Mention network data.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建账户到提及网络数据。
- en: Create Mention to Hashtag network data.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建提及到话题标签的网络数据。
- en: Create an Account to Mention network.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建账户到提及网络。
- en: Create a Mention to Hashtag network.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建提及到话题标签的网络。
- en: Visualize the Account to Mention network.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化账户到提及的网络。
- en: Visualize the Mention to Hashtag network.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化提及到话题标签的网络。
- en: Let’s do this sequentially in code, reusing the Python functions we have been
    using throughout this chapter.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中顺序进行操作，重用本章中使用的 Python 函数。
- en: 'Here is the code for the first six steps:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前六步的代码：
- en: '[PRE143]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: It is really that simple. There are a lot of moving pieces under the hood, but
    the more that you practice with network data, the simpler it becomes to write
    this kind of code.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 真的就这么简单。虽然背后有很多复杂的操作，但你越多地练习网络数据，编写这类代码就会变得越简单。
- en: 'Both of these networks are now ready for visualization:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个网络现在都已准备好进行可视化：
- en: 'I’ll start with the Account to Mention network visualization. This is a social
    network. We can draw it like so:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我将从账户到提及的网络可视化开始。这是一个社交网络。我们可以这样绘制它：
- en: '[PRE144]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'This should render a network visualization:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会渲染出一个网络可视化图：
- en: '![Figure 5.13 – Account to Mention social network visualization](img/B17105_05_013.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13 – 账户到提及社交网络可视化](img/B17105_05_013.jpg)'
- en: Figure 5.13 – Account to Mention social network visualization
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – 账户到提及社交网络可视化
- en: This is a bit difficult to read since the account names overlap.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点难以阅读，因为账户名重叠了。
- en: 'Let’s see how the network looks without labels:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看没有标签的网络长什么样：
- en: '[PRE145]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'This will give us a network visualization without node labels. This will allow
    us to see what the whole network looks like:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个没有节点标签的网络可视化图。这样我们可以看到整个网络的样子：
- en: '![Figure 5.14 – Account to Mention social network visualization (no labels)](img/B17105_05_014.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 – 账户到提及社交网络可视化（无标签）](img/B17105_05_014.jpg)'
- en: Figure 5.14 – Account to Mention social network visualization (no labels)
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – 账户到提及社交网络可视化（无标签）
- en: Wow! To me, that’s beautiful and useful. I can see that there are several islands
    or clusters of users. If we look closer, we will be able to identify communities
    that exist in the data. We will do this in [*Chapter 9*](B17105_09.xhtml#_idTextAnchor364),
    which is all about Community Detection.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！对我来说，这真是既美丽又有用。我看到有几个用户群体或集群。如果我们仔细观察，将能够识别出数据中存在的社区。我们将在[*第 9 章*](B17105_09.xhtml#_idTextAnchor364)中进行此操作，本章专门讲解社区检测。
- en: 'Now, let’s look at the Mention to Hashtag network:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看提及到话题标签的网络：
- en: '[PRE146]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'This should render a network visualization:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会渲染出一个网络可视化图：
- en: '![Figure 5.15 – Mention to Hashtag network visualization](img/B17105_05_015.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – 提及到话题标签的网络可视化](img/B17105_05_015.jpg)'
- en: Figure 5.15 – Mention to Hashtag network visualization
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 从提及到话题标签的网络可视化
- en: Unlike the Account to Mention network, this one is easily readable. We can see
    users that are associated with various hashtags. There’s no value in showing this
    without labels, as it is unreadable and unusable without them. This marks the
    end of the demonstration.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 与“提及账户”网络不同，这个网络更易于阅读。我们可以看到与各种话题标签相关联的用户。没有标签的话显示这些数据没有任何价值，因为没有标签它是无法读取和使用的。这标志着演示的结束。
- en: Summary
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered two easier ways to scrape text data from the internet.
    `Newspaper3k` made short work of scraping news websites, returning clean text,
    headlines, keywords, and more. It allowed us to skip steps we’d done using `BeautifulSoup`
    and get to clean data much quicker. We used this clean text and NER to create
    and visualize networks. Finally, we used the Twitter Python library and V2 API
    to scrape tweets and connections, and we also used tweets to create and visualize
    networks. Between what you learned in this chapter and the previous one, you now
    have a lot of flexibility in scraping the web and converting text into networks
    so that you can explore embedded and hidden relationships.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了两种更简单的方法来抓取互联网上的文本数据。`Newspaper3k`轻松抓取了新闻网站，返回了干净的文本、标题、关键词等。它让我们跳过了使用`BeautifulSoup`时的一些步骤，能够更快地得到清洁数据。我们使用这些清洁的文本和命名实体识别（NER）来创建并可视化网络。最后，我们使用了Twitter的Python库和V2
    API来抓取推文和连接，我们也用推文来创建和可视化网络。通过本章和前一章所学，你现在在抓取网络和将文本转换为网络的过程中有了更大的灵活性，这样你就能探索嵌入的和隐藏的关系。
- en: 'Here is some good news: collecting and cleaning data is the most difficult
    part of what we are going to do, and this marks the end of data collection and
    most of the cleanup. After this chapter, we will mostly be having fun with networks!'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有个好消息：收集和清理数据是我们要做的工作中最困难的部分，这标志着数据收集和大部分清理工作的结束。在本章之后，我们将大部分时间都在享受与网络相关的乐趣！
- en: In the next chapter, we will look at graph construction. We will make use of
    the techniques we used in this chapter to create networks for analysis and visualization.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究图形构建。我们将利用本章中使用的技术，创建用于分析和可视化的网络。
