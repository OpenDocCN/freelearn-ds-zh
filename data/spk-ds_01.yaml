- en: Chapter 1.  Big Data and Data Science – An Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 大数据与数据科学——简介
- en: '*Big data is definitely a big deal!* It promises a wealth of opportunities
    by deriving hidden insights in huge data silos and by opening new avenues to excel
    in business. Leveraging **big data** through advanced analytics techniques has
    become a no-brainer for organizations to create and maintain their competitive
    advantage.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*大数据绝对是一个大问题！* 它承诺通过在巨大的数据孤岛中提取隐藏的洞察力，并开辟新的商业发展途径，带来丰富的机会。通过先进的分析技术利用**大数据**，已经成为组织创造和保持竞争优势的一种理所当然的选择。'
- en: This chapter explains what big data is all about, the various challenges with
    big data analysis and how **Apache Spark** pitches in as the de facto standard
    to address computational challenges and also serves as a data science platform.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了大数据的概念、大数据分析面临的各种挑战，以及**Apache Spark**如何作为事实标准来应对计算挑战，并作为一个数据科学平台。
- en: 'The topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题如下：
- en: Big data overview - what is all the fuss about?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据概述——到底有什么大惊小怪的？
- en: Challenges with big data analytics - why was it so difficult?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据分析的挑战——为什么它如此困难？
- en: Evolution of big data analytics - the data analytics trend
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据分析的演变——数据分析趋势
- en: Spark for data analytics - the solution to big data challenges
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 数据分析——解决大数据挑战的方案
- en: The Spark stack - all that makes it up for a complete big data solution
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 堆栈——构成完整大数据解决方案的所有组件
- en: Big data overview
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据概述
- en: Much has already been spoken and written about what big data is, but there is
    no specific standard as such to clearly define it. It is actually a relative term
    to some extent. Whether small or big, your data can be leveraged only if you can
    analyze it properly. To make some sense out of your data, the right set of analysis
    techniques is needed and selecting the right tools and techniques is of utmost
    importance in data analytics. However, when the data itself becomes a part of
    the problem and the computational challenges need to be addressed prior to performing
    data analysis, it becomes a big data problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关于大数据是什么，已经有很多讨论和书面材料，但实际上并没有一个明确的标准来清晰定义它。它在某种程度上是一个相对的术语。无论数据是大还是小，只有在能够正确分析数据的情况下，才能有效利用它。为了从数据中提取一些有意义的内容，需要正确的分析技术，而选择合适的工具和技术在数据分析中至关重要。然而，当数据本身成为问题的一部分，且需要在进行数据分析之前解决计算挑战时，这就成了一个大数据问题。
- en: A revolution took place in the World Wide Web, also referred to as Web 2.0,
    which changed the way people used the Internet. Static web pages became interactive
    websites and started collecting more and more data. Technological advancements
    in cloud computing, social media, and mobile computing created an explosion of
    data. Every digital device started emitting data and many other sources started
    driving the data deluge. The dataflow from every nook and corner generated varieties
    of voluminous data, at speed! The formation of big data in this fashion was a
    natural phenomenon, because this is how the World Wide Web had evolved and no
    explicit efforts were involved in specifics. This is about the past! If you consider
    the change that is happening now, and is going to happen in future, the volume
    and speed of data generation is beyond what one can anticipate. I am propelled
    to make such a statement because every device is getting smarter these days, thanks
    to the **Internet of Things** (**IoT**).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在万维网（也称为Web 2.0）发生了一场革命，改变了人们使用互联网的方式。静态网页变成了互动网站，开始收集越来越多的数据。云计算、社交媒体和移动计算的技术进步引发了数据的爆炸。每个数字设备都开始生成数据，许多其他来源也推动了数据洪流。来自每个角落的数据流以惊人的速度生成各种海量数据！大数据以这种方式的形成是一种自然现象，因为这正是万维网的演变方式，具体细节并没有刻意推动。这是关于过去的！如果考虑现在正在发生并将在未来发生的变化，数据生成的体量和速度超出了任何人的预期。我之所以做出这样的陈述，是因为每个设备现在都变得更加智能，这要归功于**物联网**（**IoT**）。
- en: The IT trend was such that the technological advancements also facilitated the
    data explosion. Data storage had experienced a paradigm shift with the advent
    of cheaper clusters of online storage pools and the availability of commodity
    hardware with bare minimal price. Storing data from disparate sources in its native
    form in a single data lake was rapidly gaining over carefully designed data marts
    and data warehouses. Usage patterns also shifted from rigid schema-driven, RDBMS-based
    approaches to schema-less, continuously available **NoSQL** data-store-driven
    solutions. As a result, the rate of data creation, whether structured, semi-structured,
    or unstructured, started accelerating like never before.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 信息技术的趋势是，技术进步也促进了数据的爆炸。随着更便宜的在线存储池集群的出现以及基本硬件的低价可用性，数据存储经历了范式转变。将来自不同来源的数据以原始形式存储在单一数据湖中，迅速取代了精心设计的数据集市和数据仓库。使用模式也从严格的模式驱动的基于RDBMS的方法转向了无模式、持续可用的**NoSQL**数据存储驱动的解决方案。因此，无论是结构化数据、半结构化数据，还是非结构化数据，其创建速度都前所未有地加快。
- en: Organizations are very much convinced that not only can specific business questions
    be answered by leveraging big data; it also brings in opportunities to cover the
    uncovered possibilities in businesses and address the uncertainties associated
    with this. So, apart from the natural data influx, organizations started devising
    strategies to generate more and more data to maintain their competitive advantages
    and to be future ready. Here, an example would help to understand this better.
    Imagine sensors are installed on the machines of a manufacturing plant which are
    constantly emitting data, and hence the status of the machine parts, and a company
    is able to predict when the machine is going to fail. It lets the company prevent
    a failure or damage and avoid unplanned downtime, saving a lot of money.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 组织非常确信，利用大数据不仅可以回答特定的业务问题，还能带来覆盖尚未探索的业务可能性并解决相关的不确定性。因此，除了自然的数据涌入，组织开始制定战略，以生成越来越多的数据来维持其竞争优势，并为未来做好准备。在这里，一个例子有助于更好地理解这一点。假设在一个制造工厂的机器上安装了传感器，传感器不断发送数据，从而实时了解机器部件的状态，而公司能够预测机器何时会发生故障。这使得公司能够防止故障或损坏，避免计划外停机，节省大量资金。
- en: Challenges with big data analytics
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据分析的挑战
- en: There are broadly two types of formidable challenges in the analysis of big
    data. The first challenge is the requirement for a massive computation platform,
    and once it is in place, the second challenge is to analyze and make sense out
    of huge data at scale.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据分析中，通常有两类主要挑战。第一个挑战是需要一个庞大的计算平台，一旦平台建立，第二个挑战就是如何在大规模上分析并理解海量数据。
- en: Computational challenges
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算挑战
- en: With the increase in data, the storage requirement for big data also grew more
    and more. Data management became a cumbersome task. The latency involved in accessing
    the disk storage due to the seek time became the major bottleneck even though
    the processing speed of the processor and the frequency of RAM were up to the
    mark.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据的增加，大数据的存储需求也不断增长。数据管理变得愈加繁琐。由于磁盘存储的寻址时间，访问数据时的延迟成为了主要瓶颈，尽管处理器的处理速度和内存的频率已经达到了标准。
- en: Fetching structured and unstructured data from across the gamut of business
    applications and data silos, consolidating them, and processing them to find useful
    business insights was challenging. There were only a few applications that could
    address any one area, or just a few areas of diversified business requirement.
    However, integrating those applications to address most of the business requirements
    in a unified way only increased the complexity.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从各种业务应用程序和数据孤岛中获取结构化和非结构化数据，将其整合并处理以发现有用的业务洞察是一项挑战。只有少数应用程序能够解决任何一个领域或仅几个多样化业务需求的领域。然而，将这些应用程序整合以统一方式解决大多数业务需求，只会增加复杂性。
- en: To address these challenges, people turned to the distributed computing framework
    with distributed file system, for example, Hadoop and **Hadoop Distributed File
    System** (**HDFS**). This could eliminate the latency due to disk I/O, as the
    data could be read in parallel across the cluster of machines.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，人们转向了分布式计算框架和分布式文件系统，例如Hadoop和**Hadoop分布式文件系统**（**HDFS**）。这可以消除磁盘I/O带来的延迟，因为数据可以在机器集群中并行读取。
- en: Distributed computing technologies had existed for decades before, but gained
    more prominence only after the importance of big data was realized in the industry.
    So, technology platforms such as Hadoop and HDFS or Amazon S3 became the industry
    standard. On top of Hadoop, many other solutions such as Pig, Hive, Sqoop, and
    others were developed to address different kinds of industry requirements such
    as storage, **Extract, Transform, and Load** (**ETL**), and data integration to
    make Hadoop a unified platform.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算技术在此之前已经存在了几十年，但直到行业认识到大数据的重要性后，它们才逐渐变得更加突出。因此，像Hadoop、HDFS和Amazon S3这样的技术平台成为了行业标准。在Hadoop之上，开发了许多其他解决方案，如Pig、Hive、Sqoop等，以应对不同的行业需求，如存储、**提取、转换和加载**（**ETL**）和数据集成，旨在使Hadoop成为一个统一的平台。
- en: Analytical challenges
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析挑战
- en: Analyzing data to find some hidden insights has always been challenging because
    of the additional intricacies involved in dealing with huge datasets. The traditional
    BI and OLAP solutions could not address most of the challenges that arose due
    to big data. As an example, if there were multiple dimensions to a dataset, say
    100, it got really difficult to compare these variables with one another to draw
    a conclusion because there would be around 100C2 combinations for it. Such cases
    required statistical techniques such as *correlation* and the like to find the
    hidden patterns.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分析数据以发现隐藏的洞察力一直以来都是一项挑战，尤其是在处理庞大数据集时所涉及的额外复杂性。传统的BI和OLAP解决方案无法解决由于大数据带来的大多数挑战。例如，如果数据集有多个维度，比如100个，就很难将这些变量彼此进行比较以得出结论，因为这样会有大约100C2种组合。此类情况需要使用统计学技术，如*相关性*等，来发现隐藏的模式。
- en: Though there were statistical solutions to many problems, it got really difficult
    for data scientists or analytics professionals to slice and dice the data to find
    intelligent insights unless they loaded the entire dataset into a **DataFrame**
    in memory. The major roadblock was that most of the general-purpose algorithms
    for statistical analysis and machine learning were single-threaded and written
    at a time when datasets were usually not so huge and could fit in the RAM on a
    single computer. Those algorithms written in R or Python were no longer very useful
    in their native form to be deployed on a distributed computing environment because
    of the limitation of in-memory computation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在许多统计学解决方案来解决这些问题，但数据科学家或分析专业人员要切分数据并挖掘有价值的洞察力变得异常困难，除非他们将整个数据集加载到内存中的**DataFrame**中。主要的障碍是，大多数通用的统计分析和机器学习算法都是单线程的，并且是在数据集通常不那么庞大、可以适应单台计算机的RAM时编写的。这些用R或Python编写的算法，在分布式计算环境中部署时已经不再非常有用，因为存在内存计算的限制。
- en: To address this challenge, statisticians and computer scientists had to work
    together to rewrite most of the algorithms that would work well in a distributed
    computing environment. Consequently, a library called **Mahout** for machine learning
    algorithms was developed on Hadoop for parallel processing. It had most of the
    common algorithms that were being used most often in the industry. Similar initiatives
    were taken for other distributed computing frameworks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一挑战，统计学家和计算机科学家不得不共同合作，重写大多数能够在分布式计算环境中良好运行的算法。因此，开发了一个名为**Mahout**的机器学习算法库，用于在Hadoop上进行并行处理。它包含了行业中最常用的多数算法。类似的举措也在其他分布式计算框架中进行。
- en: Evolution of big data analytics
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据分析的发展
- en: The previous section outlined how the computational and data analytics challenges
    were addressed for big data requirements. It was possible because of the convergence
    of several related trends such as low-cost commodity hardware, accessibility to
    big data, and improved data analytics techniques. Hadoop became a cornerstone
    in many large, distributed data processing infrastructures.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节概述了如何应对大数据需求的计算和数据分析挑战。这得以实现，是因为若干相关趋势的融合，如低成本的商品硬件、大数据的可获取性以及改进的数据分析技术。Hadoop成为了许多大型分布式数据处理基础设施的基石。
- en: However, people soon started realizing the limitations of Hadoop. Hadoop solutions
    were best suited for only specific types of big data requirements such as ETL;
    it gained popularity for such requirements only.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人们很快意识到Hadoop的局限性。Hadoop解决方案仅适用于特定类型的大数据需求，如ETL；因此，它只因应这些需求而获得了广泛的应用。
- en: There were scenarios when data engineers or analysts had to perform ad hoc queries
    on the data sets for interactive data analysis. Every time they ran a query on
    Hadoop, the data was read from the disk (HDFS-read) and loaded into the memory
    - which was a costly affair. Effectively, jobs were running at the speed of I/O
    transfers over the network and cluster of disks, instead of the speed of CPU and
    RAM.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下，数据工程师或分析师需要对数据集执行临时查询以进行交互式数据分析。每次他们在Hadoop上运行查询时，数据都会从磁盘（HDFS读取）中读取并加载到内存中——这是一项代价高昂的操作。实际上，作业运行的速度受限于网络和磁盘集群的I/O传输速度，而不是CPU和RAM的速度。
- en: 'The following is a pictorial representation of the scenario:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该场景的图示表示：
- en: '![Evolution of big data analytics](img/image_01_001.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![大数据分析的演变](img/image_01_001.jpg)'
- en: One more case where Hadoop's MapReduce model could not fit in well was with
    machine learning algorithms that were iterative in nature. Hadoop MapReduce was
    underperforming, with huge latency in iterative computation. Since MapReduce had
    a restricted programming model with forbidden communication between Map and Reduce
    workers, the intermediate results needed to be stored in a stable storage. So,
    those were pushed on to the HDFS, which in turn writes into the instead of saving
    in RAM and then loading back in the memory for the subsequent iteration, similarly
    for the rest of the iterations. The number of disk I/O was dependent on the number
    of iterations involved in an algorithm and this was topped with the serialization
    and deserialization overhead while saving and loading the data. Overall, it was
    computationally expensive and could not get the level of popularity compared to
    what was expected of it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的MapReduce模型无法很好适应的另一个场景是迭代性机器学习算法。Hadoop MapReduce的性能不佳，迭代计算存在巨大延迟。由于MapReduce有一个受限的编程模型，并且不允许Map和Reduce工作节点之间进行通信，所需的中间结果必须存储在持久化存储中。因此，这些结果被推送到HDFS中，而不是保存在RAM中，然后在后续的迭代中重新加载到内存。磁盘I/O的数量依赖于算法中的迭代次数，而每次保存和加载数据时都会有序列化和反序列化的开销。总体而言，这在计算上是昂贵的，且未能达到预期的流行程度。
- en: 'The following is a pictorial representation of this scenario:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该场景的图示表示：
- en: '![Evolution of big data analytics](img/image_01_002.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![大数据分析的演变](img/image_01_002.jpg)'
- en: To address this, tailor-made solutions were developed, for example, Google's
    Pregel, which was an iterative graph processing algorithm and was optimized for
    inter-process communication and in-memory storage for the intermediate results
    to make it run faster. Similarly, many other solutions were developed or redesigned
    that would best suit some of the specific needs that the algorithms used were
    designed for.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，开发了定制的解决方案，例如Google的Pregel，这是一种迭代图处理算法，优化了进程间通信和中间结果的内存存储，以加快运行速度。类似地，许多其他解决方案被开发或重新设计，以更好地满足某些算法的特定需求。
- en: Instead of redesigning all the algorithms, a general-purpose engine was needed
    that could be leveraged by most of the algorithms for in-memory computation on
    a distributed computing platform. It was also expected that such a design would
    result in faster execution of iterative computation and ad hoc data analysis.
    This is how the Spark project paved its way out at the AMPLab at UC Berkeley.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与其重新设计所有算法，不如需要一个通用引擎，能够被大多数算法用来在分布式计算平台上进行内存计算。预计这种设计将导致迭代计算和临时数据分析的执行速度更快。这也是Spark项目在UC伯克利AMPLab中崭露头角的原因。
- en: Spark for data analytics
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析的Spark
- en: Soon after the Spark project was successful in the AMP labs, it was made open
    source in 2010 and transferred to the Apache Software Foundation in 2013\. It
    is currently being led by Databricks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark项目在AMP实验室成功之后，它于2010年开源，并于2013年转交给了Apache软件基金会。目前，它由Databricks主导。
- en: 'Spark offers many distinct advantages over other distributed computing platforms,
    such as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark相较于其他分布式计算平台，具有许多显著优势，例如：
- en: A faster execution platform for both iterative machine learning and interactive
    data analysis
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于迭代性机器学习和交互式数据分析的更快执行平台
- en: Single stack for batch processing, SQL queries, real-time stream processing,
    graph processing, and complex data analytics
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一堆栈用于批处理、SQL查询、实时流处理、图形处理和复杂数据分析
- en: Provides high-level API to develop a diverse range of distributed applications
    by hiding the complexities of distributed programming
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供高层次的 API，开发多种分布式应用程序，隐藏分布式编程的复杂性
- en: Seamless support for various data sources such as RDBMS, HBase, Cassandra, Parquet,
    MongoDB, HDFS, Amazon S3, and so on
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无缝支持各种数据源，如 RDBMS、HBase、Cassandra、Parquet、MongoDB、HDFS、Amazon S3 等
- en: '![Spark for data analytics](img/image_01_003.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![用于数据分析的 Spark](img/image_01_003.jpg)'
- en: 'The following is a pictorial representation of in-memory data sharing for iterative
    algorithms:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是迭代算法的内存数据共享示意图：
- en: '![Spark for data analytics](img/image_01_004.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![用于数据分析的 Spark](img/image_01_004.jpg)'
- en: Spark hides the complexities in writing the core MapReduce jobs and provides
    most of the functionalities through simple function calls. Because of its simplicity,
    it is able to cater to wider and bigger audience groups such as data scientists,
    data engineers, statisticians, and R/Python/Scala/Java developers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 隐藏了编写核心 MapReduce 作业的复杂性，并通过简单的函数调用提供大多数功能。由于其简单性，Spark 能够满足更广泛的受众群体，如数据科学家、数据工程师、统计学家以及
    R/Python/Scala/Java 开发者。
- en: The Spark architecture broadly consists of a data storage layer, management
    framework, and API. It is designed to work on top of an HDFS filesystem, and thereby
    leverages the existing ecosystem. Deployment could be as a standalone server or
    on distributed computing frameworks such as Apache Mesos or YARN. An API is provided
    for Scala, the language in which Spark is written, along with Java, R and Python.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 架构大体上包括一个数据存储层、管理框架和 API。它被设计为运行在 HDFS 文件系统之上，因此可以利用现有的生态系统。部署方式可以是独立服务器，也可以是在分布式计算框架上，如
    Apache Mesos 或 YARN。提供了 Scala 语言的 API，Spark 就是用这种语言编写的，同时还支持 Java、R 和 Python。
- en: The Spark stack
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 技术栈
- en: Spark is a general-purpose cluster computing system that empowers other higher-level
    components to leverage its core engine. It is interoperable with Apache Hadoop,
    in the sense that it can read and write data from/to HDFS and can also integrate
    with other storage systems that are supported by the Hadoop API.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个通用的集群计算系统，它赋能其他更高层次的组件利用其核心引擎。它与 Apache Hadoop 兼容，意味着它可以从 HDFS 读取和写入数据，并且还可以与
    Hadoop API 支持的其他存储系统集成。
- en: While it allows building other higher-level applications on top of it, it already
    has a few components built on top that are tightly integrated with its core engine
    to take advantage of the future enhancements at the core. These applications come
    bundled with Spark to cover the broader sets of requirements in the industry.
    Most of the real-world applications need to be integrated across projects to solve
    specific business problems that usually have a set of requirements. This is eased
    out with Apache Spark as it allows its higher level components to be seamlessly
    integrated, such as libraries in a development project.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它允许在其基础上构建其他更高层次的应用程序，但它已经有几个紧密集成的组件，这些组件与其核心引擎高度结合，以利用未来的核心增强功能。这些应用程序与 Spark
    一起捆绑，旨在满足行业中更广泛的需求。大多数现实世界的应用程序需要跨项目进行集成，以解决通常有一组特定需求的业务问题。Apache Spark 使这一过程变得更加轻松，因为它允许其更高层次的组件无缝集成，例如在开发项目中的库。
- en: 'Also, with Spark''s built-in support for Scala, Java, R and Python, a broader
    range of developers and data engineers are able to leverage the entire Spark stack:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，得益于 Spark 内置对 Scala、Java、R 和 Python 的支持，更广泛的开发者和数据工程师能够利用整个 Spark 技术栈：
- en: '![The Spark stack](img/image_01_005.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 技术栈](img/image_01_005.jpg)'
- en: Spark core
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 核心
- en: The Spark core, in a way, is similar to the kernel of an operating system. It
    is the general execution engine, which is fast as well as fault tolerant. The
    entire Spark ecosystem is built on top of this core engine. It is mainly designed
    to do job scheduling, task distribution, and monitoring of jobs across worker
    nodes. It is also responsible for memory management, interacting with various
    heterogeneous storage systems, and various other operations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 核心在某种程度上类似于操作系统的内核。它是通用的执行引擎，既快速又容错。整个 Spark 生态系统都建立在这个核心引擎之上。它主要负责作业调度、任务分配和在工作节点之间监控作业。它还负责内存管理、与各种异构存储系统的交互，以及其他各种操作。
- en: The primary building block of Spark core is the **Resilient Distributed Dataset**
    (**RDD**), which is an immutable, fault-tolerant collection of elements. Spark
    can create RDDs from a variety of data sources such as HDFS, local filesystems,
    Amazon S3, other RDDs, NoSQL data stores such as Cassandra, and so on. They are
    resilient in the sense that they automatically rebuild on failure. RDDs are built
    through lazy parallel transformations. They may be cached and partitioned, and
    may or may not be materialized.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 核心的主要构建模块是**弹性分布式数据集**（**RDD**），它是一个不可变的、容错的元素集合。Spark 可以从多种数据源创建 RDD，例如
    HDFS、本地文件系统、Amazon S3、其他 RDD、Cassandra 等 NoSQL 数据存储。它们是具有弹性的，意味着在失败时会自动重建。RDD
    通过惰性并行转换构建。它们可以被缓存和分区，并且可能是或不是物化的。
- en: The entire Spark core engine may be viewed as a set of simple operations on
    distributed datasets. All the scheduling and execution of jobs in Spark is done
    based on the methods associated with each RDD. Also, the methods associated with
    each RDD define their own ways of distributed in-memory computation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 Spark 核心引擎可以视为对分布式数据集进行的一组简单操作。在 Spark 中，所有作业的调度和执行都基于与每个 RDD 关联的方法。此外，和每个
    RDD 关联的方法定义了它们自己的分布式内存计算方式。
- en: Spark SQL
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark SQL
- en: This module of Spark is designed to query, analyze, and perform operations on
    structured data. This is a very important component in the entire Spark stack
    because of the fact that most of the organizational data is structured, though
    unstructured data is growing rapidly. Acting as a distributed query engine, it
    enables Hadoop Hive queries to run up to 100 times faster on it without any modification.
    Apart from Hive, it also supports Apache Parquet, an efficient columnar storage,
    JSON, and other structured data formats. Spark SQL enables running SQL queries
    along with complex programs written in Python, Scala, and Java.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块设计用于查询、分析和对结构化数据进行操作。这个组件在整个 Spark 堆栈中非常重要，因为大多数组织数据是结构化的，尽管非结构化数据正在快速增长。作为一个分布式查询引擎，它使得
    Hadoop Hive 查询在不做任何修改的情况下能够提升最多 100 倍的速度。除了 Hive，它还支持高效的列式存储 Apache Parquet、JSON
    以及其他结构化数据格式。Spark SQL 使得可以运行 SQL 查询并与用 Python、Scala 和 Java 编写的复杂程序一起使用。
- en: Spark SQL provides a distributed programming abstraction called **DataFrames**,
    referred to as SchemaRDD before, which had fewer functions associated with it.
    DataFrames are distributed collections of named columns, analogous to SQL tables
    or Python's Pandas DataFrames. They can be constructed with a variety of data
    sources that have schemas with them such as Hive, Parquet, JSON, other RDBMS sources,
    and also from Spark RDDs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 提供了一种分布式编程抽象，称为**数据框**，之前被称为 SchemaRDD，它具有较少的相关功能。数据框是命名列的分布式集合，类似于
    SQL 表或 Python 的 Pandas 数据框。它们可以通过多种具有模式的数据源构建，例如 Hive、Parquet、JSON、其他关系型数据库源以及
    Spark RDD。
- en: Spark SQL can be used for ETL processing across different formats and then running
    ad hoc analysis. Spark SQL comes with an optimizer framework called Catalyst that
    can transform SQL queries for better efficiency.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 可用于跨不同格式进行 ETL 处理，并进行临时分析。Spark SQL 配备了一个名为 Catalyst 的优化框架，它可以将 SQL
    查询转换为更高效的形式。
- en: Spark streaming
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 流处理
- en: The processing window for the enterprise data is becoming shorter than ever.
    To address the real-time processing requirement of the industry, this component
    of Spark was designed, which is fault tolerant as well as scalable. Spark enables
    real-time data analytics on live streams of data by supporting data analysis,
    machine learning, and graph processing on them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 企业数据的处理窗口正在比以往任何时候都要短。为了解决行业的实时处理需求，Spark 设计了这一组件，它具有容错性和可扩展性。Spark 支持对实时数据流进行数据分析、机器学习和图处理，从而实现实时数据分析。
- en: It provides an API called **Discretised Stream** (**DStream**) to manipulate
    the live streams of data. The live streams of data are sliced up into small batches
    of, say, *x* seconds. Spark treats each batch as an RDD and processes them as
    basic RDD operations. DStreams can be created out of live streams of data from
    HDFS, Kafka, Flume, or any other source which can stream data on the TCP socket.
    By applying some higher-level operations on DStreams, other DStreams can be produced.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了一个名为**离散化流**（**DStream**）的 API，用于操作实时数据流。实时数据流被切分成小批次，比如每 *x* 秒。Spark 将每个批次当作
    RDD 进行处理，执行基本的 RDD 操作。DStream 可以通过来自 HDFS、Kafka、Flume 或任何其他可以通过 TCP 套接字传输数据的源创建。通过对
    DStream 应用一些更高级的操作，可以生成其他 DStream。
- en: The final result of Spark streaming can either be written back to the various
    data stores supported by Spark or can be pushed to any dashboard for visualization.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 流处理的最终结果可以写回 Spark 支持的各种数据存储，或者可以推送到任何仪表盘进行可视化。
- en: MLlib
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLlib
- en: MLlib is the built-in machine learning library in the Spark stack. This was
    introduced in Spark 0.8\. Its goal is to make machine learning scalable and easy.
    Developers can seamlessly use Spark SQL, Spark Streaming, and GraphX in their
    programming language of choice, be it Java, Python, or Scala. MLlib provides the
    necessary functions to perform various statistical analyses such as correlations,
    sampling, hypothesis testing, and so on. This component also has a broad coverage
    of applications and algorithms in classification, regression, collaborative filtering,
    clustering, and decomposition.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 是 Spark 堆栈中的内置机器学习库。它在 Spark 0.8 中被引入。其目标是使机器学习具有可扩展性且易于使用。开发人员可以无缝地在其选择的编程语言中使用
    Spark SQL、Spark Streaming 和 GraphX，无论是 Java、Python 还是 Scala。MLlib 提供了执行各种统计分析所需的功能，如相关性、抽样、假设检验等。该组件还涵盖了分类、回归、协同过滤、聚类和分解等领域的广泛应用和算法。
- en: The machine learning workflow involves collecting and preprocessing data, building
    and deploying the model, evaluating the results, and refining the model. In the
    real world, the preprocessing steps take up significant effort. These are typically
    multi-stage workflows involving expensive intermediate read/write operations.
    Often, these processing steps may be performed multiple times over a period of
    time. A new concept called **ML Pipelines** was introduced to streamline these
    preprocessing steps. A Pipeline is a sequence of transformations where the output
    of one stage is the input of another, forming a chain. The ML Pipeline leverages
    Spark and MLlib and enables developers to define reusable sequences of transformations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作流程涉及收集和预处理数据、构建和部署模型、评估结果以及优化模型。在现实中，预处理步骤需要大量的工作。通常这些是多阶段工作流程，涉及昂贵的中间读/写操作。通常，这些处理步骤可能会在一段时间内多次执行。为了简化这些预处理步骤，提出了一个新概念——**ML
    管道**。管道是一个变换序列，其中一个阶段的输出是另一个阶段的输入，形成一个链条。ML 管道利用 Spark 和 MLlib，允许开发人员定义可重用的变换序列。
- en: GraphX
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphX
- en: GraphX is a thin-layered unified graph analytics framework on Spark. It was
    designed to be a general-purpose distributed dataflow framework in place of specialized
    graph processing frameworks. It is fault tolerant and also exploits in-memory
    computation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX 是 Spark 上的一个薄层统一图分析框架。它被设计为一个通用的分布式数据流框架，替代了专门的图处理框架。它是容错的，并且利用了内存计算。
- en: '**GraphX** is an embedded graph processing API for manipulating graphs (for
    example, social networks) and to do graph parallel computation (for example, Google''s
    Pregel). It combines the advantages of both graph-parallel and data-parallel systems
    on the Spark stack to unify exploratory data analysis, iterative graph computation,
    and ETL processing. It extends the RDD abstraction to introduce the **Resilient
    Distributed Graph** (**RDG**), which is a directed graph with properties associated
    to each of its vertices and edges.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**GraphX** 是一个嵌入式图处理 API，用于操作图（例如社交网络）并进行图并行计算（例如 Google 的 Pregel）。它结合了图并行和数据并行系统在
    Spark 堆栈中的优势，统一了探索性数据分析、迭代图计算和 ETL 处理。它扩展了 RDD 抽象，介绍了 **弹性分布式图**（**RDG**），这是一个有向图，每个顶点和边都有相关的属性。'
- en: GraphX includes a decently large collection of graph algorithms, such as PageRank,
    K-Core, Triangle Count, LDA, and so on.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX 包含了一组相当大的图算法，例如 PageRank、K-Core、Triangle Count、LDA 等等。
- en: SparkR
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SparkR
- en: The SparkR project was started to integrate the statistical analysis and machine
    learning capability of R with the scalability of Spark. It addressed the limitation
    of R, which was its ability to process as much data as fitted in the memory of
    a single machine. R programs can now scale in a distributed setting through SparkR.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 项目的启动是为了将 R 的统计分析和机器学习功能与 Spark 的可扩展性相结合。它解决了 R 的局限性——即只能处理适合单台机器内存的数据。现在，R
    程序可以通过 SparkR 在分布式环境中扩展。
- en: SparkR is actually an R Package that provides an R shell to leverage Spark's
    distributed computing engine. With R's rich set of built-in packages for data
    analytics, data scientists can analyze large datasets interactively at scale.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 实际上是一个 R 包，它提供了一个 R shell 来利用 Spark 的分布式计算引擎。借助 R 丰富的内置数据分析包，数据科学家可以在大规模上交互式地分析大型数据集。
- en: Summary
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we briefly covered what big data is all about. We then discussed
    the computational and analytical challenges involved in big data analytics. Later,
    we looked at how the analytics space in the context of big data has evolved over
    a period of time and what the trend has been. We also covered how Spark addressed
    most of the big data analytics challenges and became a general-purpose unified
    analytics platform for data science as well as parallel computation. At the end
    of this chapter, we just gave you a heads-up on the Spark stack and its components.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要介绍了大数据的概念。然后，我们讨论了大数据分析中涉及的计算和分析挑战。接着，我们回顾了大数据分析领域在一段时间内如何发展以及趋势如何。我们还介绍了
    Spark 如何解决大多数大数据分析挑战，并成为一个通用的统一分析平台，适用于数据科学以及并行计算。本章的最后，我们只是简要介绍了 Spark 堆栈及其组件。
- en: In the next chapter, we will learn about the Spark programming model. We will
    take a deep dive into the basic building block of Spark, which is the RDD. Also,
    we will learn how to program with the RDD API on Scala and Python.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习 Spark 编程模型。我们将深入了解 Spark 的基本构建块——RDD。此外，我们还将学习如何在 Scala 和 Python
    上使用 RDD API 进行编程。
- en: References
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Apache Spark overview:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 概述：
- en: '[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)'
- en: '[https://databricks.com/spark/about](https://databricks.com/spark/about)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/spark/about](https://databricks.com/spark/about)'
- en: 'Apache Spark architecture:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 架构：
- en: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
