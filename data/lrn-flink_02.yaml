- en: Chapter 2.  Data Processing Using the DataStream API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。使用DataStream API进行数据处理
- en: Real-time analytics is currently an important issue. Many different domains
    need to process data in real time. So far there have been multiple technologies
    trying to provide this capability. Technologies such as Storm and Spark have been
    on the market for a long time now. Applications derived from the **Internet of
    Things** (**IoT**) need data to be stored, processed, and analyzed in real or
    near real time. In order to cater for such needs, Flink provides a streaming data
    processing API called DataStream API.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 实时分析目前是一个重要问题。许多不同的领域需要实时处理数据。到目前为止，已经有多种技术试图提供这种能力。像Storm和Spark这样的技术已经在市场上存在很长时间了。源自**物联网**（**IoT**）的应用程序需要实时或几乎实时地存储、处理和分析数据。为了满足这些需求，Flink提供了一个名为DataStream
    API的流数据处理API。
- en: 'In this chapter, we are going to look at the details relating to DataStream
    API, covering the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细了解DataStream API的相关细节，涵盖以下主题：
- en: Execution environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行环境
- en: Data sources
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源
- en: Transformations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换
- en: Data sinks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据汇
- en: Connectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接器
- en: Use case - sensor data analytics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例 - 传感器数据分析
- en: 'Any Flink program works on a certain defined anatomy as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 任何Flink程序都遵循以下定义的解剖结构：
- en: '![Data Processing Using the DataStream API](img/image_02_001.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![使用DataStream API进行数据处理](img/image_02_001.jpg)'
- en: We will be looking at each step and how we can use DataStream API with this
    anatomy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步了解每个步骤以及如何使用此解剖结构的DataStream API。
- en: Execution environment
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行环境
- en: 'In order to start writing a Flink program, we first need to get an existing
    execution environment or create one. Depending upon what you are trying to do,
    Flink supports:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始编写Flink程序，我们首先需要获取现有的执行环境或创建一个。根据您要做什么，Flink支持：
- en: Getting an already existing Flink environment
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取已存在的Flink环境
- en: Creating a local environment
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建本地环境
- en: Creating a remote environment
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建远程环境
- en: Typically, you only need to use `getExecutionEnvironment()`. This will do the
    right thing based on your context. If you are executing on a local environment
    in an IDE then it will start a local execution environment. Otherwise, if you
    are executing the JAR then the Flink cluster manager will execute the program
    in a distributed manner.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您只需要使用`getExecutionEnvironment()`。这将根据您的上下文执行正确的操作。如果您在IDE中执行本地环境，则会启动本地执行环境。否则，如果您执行JAR文件，则Flink集群管理器将以分布式方式执行程序。
- en: If you want to create a local or remote environment on your own then you can
    also choose do so by using methods such as `createLocalEnvironment()` and `createRemoteEnvironment` (`String
    host`, `int port`, `String`, and `.jar` files).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想自己创建本地或远程环境，那么您也可以选择使用`createLocalEnvironment()`和`createRemoteEnvironment`（`String
    host`，`int port`，`String`和`.jar`文件）等方法来执行。
- en: Data sources
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据源
- en: Sources are places where the Flink program expects to get its data from. This
    is a second step in the Flink program's anatomy. Flink supports a number of pre-implemented
    data source functions. It also supports writing custom data source functions so
    anything that is not supported can be programmed easily. First let's try to understand
    the built-in source functions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源是Flink程序期望从中获取数据的位置。这是Flink程序解剖的第二步。Flink支持多个预先实现的数据源函数。它还支持编写自定义数据源函数，因此可以轻松编程任何不受支持的内容。首先让我们尝试了解内置的源函数。
- en: Socket-based
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于套接字
- en: 'DataStream API supports reading data from a socket. You just need to specify
    the host and port to read the data from and it will do the work:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DataStream API支持从套接字读取数据。您只需要指定要从中读取数据的主机和端口，它就会完成工作：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can also choose to specify the delimiter:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以选择指定分隔符：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can also specify the maximum number of times the API should try to fetch
    the data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以指定API应尝试获取数据的最大次数：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: File-based
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于文件
- en: You can also choose to stream data from a file source using file-based source
    functions in Flink. You can use `readTextFile(String path)` to stream data from
    a file specified in the path. By default it will read `TextInputFormat` and will
    read strings line by line.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以选择使用Flink中基于文件的源函数从文件源中流式传输数据。您可以使用`readTextFile(String path)`从指定路径的文件中流式传输数据。默认情况下，它将读取`TextInputFormat`并逐行读取字符串。
- en: 'If the file format is other than text, you can specify the same using these
    functions:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文件格式不是文本，您可以使用这些函数指定相同的内容：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Flink also supports reading file streams as they are produced using the `readFileStream()`
    function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Flink还支持读取文件流，因为它们使用`readFileStream()`函数生成：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You just need to specify the file path, the polling interval in which the file
    path should be polled, and the watch type. Watch types consist of three types:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您只需要指定文件路径、轮询间隔（应轮询文件路径的时间间隔）和观察类型。观察类型包括三种类型：
- en: '`FileMonitoringFunction.WatchType.ONLY_NEW_FILES` is used when the system should
    process only new files'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当系统应该仅处理新文件时，使用`FileMonitoringFunction.WatchType.ONLY_NEW_FILES`
- en: '`FileMonitoringFunction.WatchType.PROCESS_ONLY_APPENDED` is used when the system
    should process only appended contents of files'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当系统应该仅处理文件的附加内容时，使用`FileMonitoringFunction.WatchType.PROCESS_ONLY_APPENDED`
- en: '`FileMonitoringFunction.WatchType.REPROCESS_WITH_APPENDED` is used when the
    system should re-process not only the appended contents of files but also the
    previous content in the file'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当系统应该重新处理文件的附加内容以及文件中的先前内容时，使用`FileMonitoringFunction.WatchType.REPROCESS_WITH_APPENDED`
- en: 'If the file is not a text file, then we do have an option to use following
    function, which lets us define the file input format:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文件不是文本文件，那么我们可以使用以下函数，它让我们定义文件输入格式：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Internally, it divides the reading file task into two sub-tasks. One sub task
    only monitors the file path based on the `WatchType` given. The second sub-task
    does the actual file reading in parallel. The sub-task which monitors the file
    path is a non-parallel sub-task. Its job is to keep scanning the file path based
    on the polling interval and report files to be processed, split the files, and
    assign the splits to the respective downstream threads:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，它将读取文件任务分为两个子任务。一个子任务仅基于给定的`WatchType`监视文件路径。第二个子任务并行进行实际的文件读取。监视文件路径的子任务是一个非并行子任务。它的工作是根据轮询间隔不断扫描文件路径，并报告要处理的文件，拆分文件，并将拆分分配给相应的下游线程：
- en: '![File-based](img/image_02_002.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![基于文件的](img/image_02_002.jpg)'
- en: Transformations
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: Data transformations transform the data stream from one form into another. The
    input could be one or more data streams and the output could also be zero, or
    one or more data streams. Now let's try to understand each transformation one
    by one.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换将数据流从一种形式转换为另一种形式。输入可以是一个或多个数据流，输出也可以是零个、一个或多个数据流。现在让我们逐个尝试理解每个转换。
- en: Map
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射
- en: This is one of the simplest transformations, where the input is one data stream
    and the output is also one data stream.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的转换之一，其中输入是一个数据流，输出也是一个数据流。
- en: 'In Java:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In Scala:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: FlatMap
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlatMap
- en: FlatMap takes one record and outputs zero, one, or more than one record.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: FlatMap接受一个记录并输出零个、一个或多个记录。
- en: 'In Java:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In Scala:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Filter
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤
- en: Filter functions evaluate the conditions and then, if they result as true, only
    emit the record. Filter functions can output zero records.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤函数评估条件，然后，如果结果为真，则仅发出记录。过滤函数可以输出零条记录。
- en: 'In Java:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In Scala:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: KeyBy
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KeyBy
- en: KeyBy logically partitions the stream-based on the key. Internally it uses hash
    functions to partition the stream. It returns `KeyedDataStream`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: KeyBy根据键逻辑地将流分区。在内部，它使用哈希函数来分区流。它返回`KeyedDataStream`。
- en: 'In Java:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In Scala:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Reduce
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少
- en: Reduce rolls out the `KeyedDataStream` by reducing the last reduced value with
    the current value. The following code does the sum reduce of a `KeyedDataStream`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Reduce通过将上次减少的值与当前值进行减少来展开`KeyedDataStream`。以下代码执行了`KeyedDataStream`的求和减少。
- en: 'In Java:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In Scala:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Fold
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折叠
- en: Fold rolls out the `KeyedDataStream` by combining the last folder stream with
    the current record. It emits a data stream back.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Fold通过将上次的文件夹流与当前记录组合起来来展开`KeyedDataStream`。它发出一个数据流。
- en: 'In Java:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In Scala:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding given function when applied on a stream of (1,2,3,4,5) would
    emit a stream like this: `Start=1=2=3=4=5`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于流(1,2,3,4,5)的前面给定的函数将发出这样的流：`Start=1=2=3=4=5`
- en: Aggregations
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合
- en: DataStream API supports various aggregations such as `min`, `max`, `sum`, and
    so on. These functions can be applied on `KeyedDataStream` in order to get rolling
    aggregations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: DataStream API支持各种聚合，如`min`、`max`、`sum`等。这些函数可以应用于`KeyedDataStream`，以便进行滚动聚合。
- en: 'In Java:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In Scala:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The difference between `max` and `maxBy` is that max returns the maximum value
    in a stream but `maxBy` returns a key that has a maximum value. The same applies
    to `min` and `minBy`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`max`和`maxBy`之间的区别在于max返回流中的最大值，但`maxBy`返回具有最大值的键。对`min`和`minBy`也适用相同的规则。'
- en: Window
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 窗口
- en: The `window` function allows the grouping of existing `KeyedDataStreams` by
    time or other conditions. The following transformation emits groups of records
    by a time window of 10 seconds.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`window`函数允许按时间或其他条件对现有的`KeyedDataStreams`进行分组。以下转换通过10秒的时间窗口发出记录组。'
- en: 'In Java:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In Scala:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Flink defines slices of data in order to process (potentially) infinite data
    streams. These slices are called windows. This slicing helps processing data in
    chunks by applying transformations. To do windowing on a stream, we need to assign
    a key on which the distribution can be made and a function which describes what
    transformations to perform on a windowed stream.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Flink定义了数据的切片，以处理（可能是）无限的数据流。这些切片称为窗口。这种切片有助于通过应用转换来以块的方式处理数据。要对流进行窗口处理，我们需要分配一个键，以便进行分发，并且需要一个描述在窗口流上执行什么转换的函数。
- en: To slice streams into windows, we can use pre-implemented Flink window assigners.
    We have options such as, tumbling windows, sliding windows, global and session
    windows. Flink also allows you to write custom window assigners by extending `WindowAssginer`
    class. Let's try to understand how these various assigners work.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要将流切片成窗口，我们可以使用预先实现的Flink窗口分配器。我们有选项，如滚动窗口、滑动窗口、全局和会话窗口。Flink还允许您通过扩展`WindowAssginer`类来编写自定义窗口分配器。让我们尝试理解这些各种分配器是如何工作的。
- en: Global windows
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全局窗口
- en: Global windows are never-ending windows unless specified by a trigger. Generally
    in this case, each element is assigned to one single per-key global Window. If
    we don't specify any trigger, no computation will ever get triggered.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 全局窗口是永不结束的窗口，除非由触发器指定。通常在这种情况下，每个元素都分配给一个单一的按键全局窗口。如果我们不指定任何触发器，将永远不会触发任何计算。
- en: Tumbling windows
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滚动窗口
- en: Tumbling windows are created based on certain times. They are fixed-length windows
    and non over lapping. Tumbling windows should be useful when you need to do computation
    of elements in specific time. For example, tumbling window of 10 minutes can be
    used to compute a group of events occurring in 10 minutes time.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 根据特定时间创建滚动窗口。它们是固定长度的窗口，不重叠。当您需要在特定时间内对元素进行计算时，滚动窗口应该是有用的。例如，10分钟的滚动窗口可用于计算在10分钟内发生的一组事件。
- en: Sliding windows
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滑动窗口
- en: Sliding windows are like tumbling windows but they are overlapping. They are
    fixed-length windows overlapping the previous ones by a user given window slide
    parameter. This type of windowing is useful when you want to compute something
    out of a group of events occurring in a certain time frame.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口类似于滚动窗口，但它们是重叠的。它们是固定长度的窗口，通过用户给定的窗口滑动参数与前一个窗口重叠。当您想要计算在特定时间范围内发生的一组事件时，这种窗口处理非常有用。
- en: Session windows
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 会话窗口
- en: Session windows are useful when windows boundaries need to be decided upon the
    input data. Session windows allows flexibility in window start time and window
    size. We can also provide session gap configuration parameter which indicates
    how long to wait before considering the session in closed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 会话窗口在需要根据输入数据决定窗口边界时非常有用。会话窗口允许窗口开始时间和窗口大小的灵活性。我们还可以提供会话间隙配置参数，指示在考虑会话关闭之前等待多长时间。
- en: WindowAll
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WindowAll
- en: The `windowAll` function allows the grouping of regular data streams. Generally
    this is a non-parallel data transformation as it runs on non-partitioned streams
    of data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`windowAll`函数允许对常规数据流进行分组。通常这是一个非并行的数据转换，因为它在非分区数据流上运行。'
- en: 'In Java:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In Scala:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Similar to regular data stream functions, we have window data stream functions
    as well. The only difference is they work on windowed data streams. So window
    reduce works like the `Reduce` function, Window fold works like the `Fold` function,
    and there are aggregations as well.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规数据流函数类似，我们也有窗口数据流函数。唯一的区别是它们适用于窗口化的数据流。因此，窗口缩减类似于`Reduce`函数，窗口折叠类似于`Fold`函数，还有聚合函数。
- en: Union
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联合
- en: The `Union` function performs the union of two or more data streams together.
    This does the combining of data streams in parallel. If we combine one stream
    with itself then it outputs each record twice.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`Union`函数执行两个或多个数据流的并集。这会并行地组合数据流。如果我们将一个流与自身组合，则每个记录都会输出两次。'
- en: 'In Java:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In Scala:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Window join
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 窗口连接
- en: We can also join two data streams by some keys in a common window. The following
    example shows the joining of two streams in a Window of `5` seconds where the
    joining condition of the first attribute of the first stream is equal to the second
    attribute of the other stream.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过一些键在一个公共窗口中连接两个数据流。下面的示例显示了在`5`秒的窗口中连接两个流的情况，其中第一个流的第一个属性的连接条件等于另一个流的第二个属性。
- en: 'In Java:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE26]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In Scala:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE27]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Split
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割
- en: This function splits the stream into two or more streams based on the criteria.
    This can be used when you get a mixed stream and you may want to process each
    data separately.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数根据条件将流拆分为两个或多个流。当您获得混合流并且可能希望分别处理每个数据时，可以使用此函数。
- en: 'In Java:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In Scala:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Select
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择
- en: This function allows you to select a specific stream from the split stream.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数允许您从拆分流中选择特定流。
- en: 'In Java:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In Scala:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE31]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Project
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目
- en: The `Project` function allows you to select a sub-set of attributes from the
    event stream and only sends selected elements to the next processing stream.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`Project`函数允许您从事件流中选择一部分属性，并仅将选定的元素发送到下一个处理流。'
- en: 'In Java:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE32]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In Scala:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE33]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding function selects the attribute numbers `2` and `3` from the given
    records. The following is the sample input and output records:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数从给定记录中选择属性编号`2`和`3`。以下是示例输入和输出记录：
- en: '[PRE34]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Physical partitioning
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物理分区
- en: Flink allows us to perform physical partitioning of the stream data. You have
    an option to provide custom partitioning. Let us have a look at the different
    types of partitioning.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Flink允许我们对流数据进行物理分区。您可以选择提供自定义分区。让我们看看不同类型的分区。
- en: Custom partitioning
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义分区
- en: As mentioned earlier, you can provide custom implementation of a partitioner.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，您可以提供分区器的自定义实现。
- en: 'In Java:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In Scala:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE36]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: While writing a custom partitioner you need make sure you implement an efficient
    hash function.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写自定义分随机器时，您需要确保实现有效的哈希函数。
- en: Random partitioning
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机分区
- en: Random partitioning randomly partitions data streams in an evenly manner.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随机分区以均匀的方式随机分区数据流。
- en: 'In Java:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE37]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In Scala:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE38]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Rebalancing partitioning
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新平衡分区
- en: This type of partitioning helps distribute the data evenly. It uses a round
    robin method for distribution. This type of partitioning is good when data is
    skewed.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的分区有助于均匀分布数据。它使用轮询方法进行分发。当数据发生偏斜时，这种类型的分区是很好的。
- en: 'In Java:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE39]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In Scala:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE40]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Rescaling
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新缩放
- en: Rescaling is used to distribute the data across operations, perform transformations
    on sub-sets of data and combine them together. This rebalancing happens over a
    single node only, hence it does not require any data transfer across networks.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 重新缩放用于在操作之间分发数据，对数据子集执行转换并将它们组合在一起。这种重新平衡仅在单个节点上进行，因此不需要在网络上进行任何数据传输。
- en: 'The following diagram shows the distribution:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了分布情况：
- en: '![Rescaling](img/image_02_003.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![重新缩放](img/image_02_003.jpg)'
- en: 'In Java:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE41]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In Scala:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE42]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Broadcasting
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广播
- en: Broadcasting distributes all records to each partition. This fans out each and
    every element to all partitions.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 广播将所有记录分发到每个分区。这会将每个元素扩展到所有分区。
- en: 'In Java:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE43]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In Scala:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE44]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Data sinks
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据接收器
- en: 'After the data transformations are done, we need to save results into some
    place. The following are some options Flink provides us to save results:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换完成后，我们需要将结果保存到某个地方。以下是Flink提供的一些保存结果的选项：
- en: '`writeAsText()`: Writes records one line at a time as strings.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writeAsText()`: 逐行将记录写为字符串。'
- en: '`writeAsCsV()`: Writes tuples as comma separated value files. Row and fields
    delimiter can also be configured.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writeAsCsV()`: 将元组写为逗号分隔值文件。还可以配置行和字段分隔符。'
- en: '`print()/printErr()`: Writes records to the standard output. You can also choose
    to write to the standard error.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`print()/printErr()`: 将记录写入标准输出。您也可以选择写入标准错误。'
- en: '`writeUsingOutputFormat()`: You can also choose to provide a custom output
    format. While defining the custom format you need to extend the `OutputFormat`
    which takes care of serialization and deserialization.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writeUsingOutputFormat()`: 您还可以选择提供自定义输出格式。在定义自定义格式时，您需要扩展负责序列化和反序列化的`OutputFormat`。'
- en: '`writeToSocket()`: Flink supports writing data to a specific socket as well.
    It is required to define `SerializationSchema` for proper serialization and formatting.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writeToSocket()`: Flink还支持将数据写入特定的套接字。需要定义`SerializationSchema`以进行适当的序列化和格式化。'
- en: Event time and watermarks
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事件时间和水印
- en: Flink Streaming API takes inspiration from Google Data Flow model. It supports
    different concepts of time for its streaming API. In general, there three places
    where we can capture time in a streaming environment. They are as follows
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Flink Streaming API受到Google Data Flow模型的启发。它支持其流式API的不同时间概念。一般来说，在流式环境中有三个地方可以捕获时间。它们如下
- en: Event time
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件时间
- en: The time at which event occurred on its producing device. For example in IoT
    project, the time at which sensor captures a reading. Generally these event times
    needs to embed in the record before they enter Flink. At the time processing,
    these timestamps are extracted and considering for windowing. Event time processing
    can be used for out of order events.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 事件发生的时间是指其产生设备上的时间。例如，在物联网项目中，传感器捕获读数的时间。通常这些事件时间需要在记录进入Flink之前嵌入。在处理时，这些时间戳被提取并考虑用于窗口处理。事件时间处理可以用于无序事件。
- en: Processing time
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理时间
- en: Processing time is the time of machine executing the stream of data processing.
    Processing time windowing considers only that timestamps where event is getting
    processed. Processing time is simplest way of stream processing as it does not
    require any synchronization between processing machines and producing machines.
    In distributed asynchronous environment processing time does not provide determinism
    as it is dependent on the speed at which records flow in the system.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间是机器执行数据处理流的时间。处理时间窗口只考虑事件被处理的时间戳。处理时间是流处理的最简单方式，因为它不需要处理机器和生产机器之间的任何同步。在分布式异步环境中，处理时间不提供确定性，因为它取决于记录在系统中流动的速度。
- en: Ingestion time
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摄取时间
- en: This is time at which a particular event enters Flink. All time based operations
    refer to this timestamp. Ingestion time is more expensive operation than processing
    but it gives predictable results. Ingestion time programs cannot handle any out
    of order events as it assigs timestamp only after the event is entered the Flink
    system.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是特定事件进入Flink的时间。所有基于时间的操作都参考这个时间戳。摄取时间比处理时间更昂贵，但它提供可预测的结果。摄取时间程序无法处理任何无序事件，因为它只在事件进入Flink系统后分配时间戳。
- en: Here is an example which shows how to set event time and watermarks. In case
    of ingestion time and processing time, we just need to the time characteristics
    and watermark generation is taken care automatically. Following is a code snippet
    for the same.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，显示了如何设置事件时间和水印。在摄取时间和处理时间的情况下，我们只需要时间特征，水印生成会自动处理。以下是相同的代码片段。
- en: 'In Java:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE45]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In Scala:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE46]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In case of event time stream programs, we need to specify the way to assign
    watermarks and timestamps. There are two ways of assigning watermarks and timestamps:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件时间流程序中，我们需要指定分配水印和时间戳的方式。有两种分配水印和时间戳的方式：
- en: Directly from data source attribute
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接从数据源属性
- en: Using a timestamp assigner
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用时间戳分配器
- en: To work with event time streams, we need to assign the time characteristic as
    follows
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用事件时间流，我们需要按照以下方式分配时间特征
- en: 'In Java:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE47]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'In Scala:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE48]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: It is always best to store event time while storing the record in source. Flink
    also supports some pre-defined timestamp extractors and watermark generators.
    Refer to [https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/event_timestamp_extractors.html](https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/event_timestamp_extractors.html).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储记录时，最好同时存储事件时间。Flink还支持一些预定义的时间戳提取器和水印生成器。参考[https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/event_timestamp_extractors.html](https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/event_timestamp_extractors.html)。
- en: Connectors
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接器
- en: Apache Flink supports various connectors that allow data read/writes across
    various technologies. Let's learn more about this.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink支持允许在各种技术之间读取/写入数据的各种连接器。让我们更多地了解这一点。
- en: Kafka connector
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka连接器
- en: Kafka is a publish-subscribe, distributed, message queuing system that allows
    users to publish messages to a certain topic; this is then distributed to the
    subscribers of the topic. Flink provides options to define a Kafka consumer as
    a data source in Flink Streaming. In order to use the Flink Kafka connector, we
    need to use a specific JAR file.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是一个发布-订阅的分布式消息队列系统，允许用户向特定主题发布消息；然后将其分发给主题的订阅者。Flink提供了在Flink Streaming中将Kafka消费者定义为数据源的选项。为了使用Flink
    Kafka连接器，我们需要使用特定的JAR文件。
- en: 'The following diagram shows how the Flink Kafka connector works:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Flink Kafka连接器的工作原理：
- en: '![Kafka connector](img/image_02_004.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![Kafka连接器](img/image_02_004.jpg)'
- en: 'We need to use the following Maven dependency to use the connector. I have
    been using Kafka version 0.9 so I will be adding the following dependency in `pom.xml`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用以下Maven依赖项来使用连接器。我一直在使用Kafka版本0.9，所以我将在`pom.xml`中添加以下依赖项：
- en: '[PRE49]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now let''s try to understand how to use the Kafka consumer as the Kafka source:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们试着理解如何将Kafka消费者作为Kafka源来使用：
- en: 'In Java:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE50]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In Scala:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE51]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In the preceding code, we first set the properties of the Kafka host and the
    zookeeper host and port. Next we need to specify the topic name, in this case
    `mytopic`. So if any messages get published to the `mytopic` topic, they will
    be processed by the Flink streams.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们首先设置了Kafka主机和zookeeper主机和端口的属性。接下来，我们需要指定主题名称，在本例中为`mytopic`。因此，如果任何消息发布到`mytopic`主题，它们将被Flink流处理。
- en: If you get data in a different format, then you can also specify your custom
    schema for deserialization. By default, Flink supports string and JSON deserializers.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您以不同的格式获取数据，那么您也可以为反序列化指定自定义模式。默认情况下，Flink支持字符串和JSON反序列化器。
- en: In order to enable fault tolerance, we need to enable checkpointing in Flink.
    Flink is keen on taking snapshots of the state in a periodic manner. In the case
    of failure, it will restore to the last checkpoint and then restart the processing.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现容错，我们需要在Flink中启用检查点。Flink会定期对状态进行快照。在发生故障时，它将恢复到最后一个检查点，然后重新启动处理。
- en: 'We can also define the Kafka producer as a sink. This will write the data to
    a Kafka topic. The following is a way to write data to a Kafka topic:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将Kafka生产者定义为接收器。这将把数据写入Kafka主题。以下是将数据写入Kafka主题的方法：
- en: 'In Scala:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE52]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In Java:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE53]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Twitter connector
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Twitter连接器
- en: 'These days, it is very important to have the ability to fetch data from Twitter
    and process it. Many companies use Twitter data for doing sentiment analytics
    for various products, services, movies, reviews, and so on. Flink provides the
    Twitter connector as one data source. To use the connector, you need to have a
    Twitter account. Once you have a Twitter account, you need to create a Twitter
    application and generate authentication keys to be used by the connector. Here
    is a link that will help you to generate tokens: [https://dev.twitter.com/oauth/overview/application-owner-access-tokens](https://dev.twitter.com/oauth/overview/application-owner-access-tokens).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，从Twitter获取数据并处理数据非常重要。许多公司使用Twitter数据来进行各种产品、服务、电影、评论等的情感分析。Flink提供Twitter连接器作为一种数据源。要使用连接器，您需要拥有Twitter账户。一旦您拥有了Twitter账户，您需要创建一个Twitter应用程序并生成用于连接器的身份验证密钥。以下是一个链接，可以帮助您生成令牌：[https://dev.twitter.com/oauth/overview/application-owner-access-tokens](https://dev.twitter.com/oauth/overview/application-owner-access-tokens)。
- en: 'The Twitter connector can be used through the Java or Scala API:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter连接器可以通过Java或Scala API使用：
- en: '![Twitter connector](img/image_02_005.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![Twitter连接器](img/image_02_005.jpg)'
- en: 'Once tokens are generated, we can start writing the program to fetch data from
    Twitter. First we need to add a Maven dependency:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 生成令牌后，我们可以开始编写程序从Twitter获取数据。首先我们需要添加一个Maven依赖项：
- en: '[PRE54]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next we add Twitter as a data source. The following is the sample code:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将Twitter作为数据源。以下是示例代码：
- en: 'In Java:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE55]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In Scala:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE56]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In the preceding code, we first set properties for the token we got. And then
    we add the `TwitterSource`. If the given information is correct then you will
    start fetching the data from Twitter. `TwitterSource` emits the data in a JSON
    string format. A sample Twitter JSON looks like the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们首先为我们得到的令牌设置属性。然后我们添加`TwitterSource`。如果给定的信息是正确的，那么您将开始从Twitter获取数据。`TwitterSource`以JSON字符串格式发出数据。示例Twitter
    JSON如下所示：
- en: '[PRE57]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '`TwitterSource` provides various endpoints. By default it uses `StatusesSampleEndpoint`,
    which returns a set of random tweets. If you need to add some filters and don''t
    want to use the default endpoint, you can implement the `TwitterSource.EndpointInitializer`
    interface.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`TwitterSource`提供各种端点。默认情况下，它使用`StatusesSampleEndpoint`，返回一组随机推文。如果您需要添加一些过滤器，并且不想使用默认端点，可以实现`TwitterSource.EndpointInitializer`接口。'
- en: Now that we know how to fetch data from Twitter, we can then decide what to
    do with this data depending upon our use case. We can process, store, or analyze
    the data.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何从Twitter获取数据，然后可以根据我们的用例决定如何处理这些数据。我们可以处理、存储或分析数据。
- en: RabbitMQ connector
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RabbitMQ连接器
- en: RabbitMQ is a widely used distributed, high-performance, message queuing system.
    It is used as a message delivery system for high throughput operations. It allows
    you to create a distributed message queue and include publishers and subscribers
    in the queue. More reading on RabbitMQ can be done at following link [https://www.rabbitmq.com/](https://www.rabbitmq.com/)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ是一个广泛使用的分布式、高性能的消息队列系统。它用作高吞吐量操作的消息传递系统。它允许您创建分布式消息队列，并在队列中包括发布者和订阅者。可以在以下链接进行更多关于RabbitMQ的阅读[https://www.rabbitmq.com/](https://www.rabbitmq.com/)
- en: Flink supports fetching and publishing data to and from RabbitMQ. It provides
    a connector that can act as a data source of data streams.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持从RabbitMQ获取和发布数据。它提供了一个连接器，可以作为数据流的数据源。
- en: 'For the RabbitMQ connector to work, we need to provide following information:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使RabbitMQ连接器工作，我们需要提供以下信息：
- en: RabbitMQ configurations such as host, port, user credentials, and so on.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RabbitMQ配置，如主机、端口、用户凭据等。
- en: Queue, the RabbitMQ queue name which you wish to subscribe.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 队列，您希望订阅的RabbitMQ队列的名称。
- en: Correlation IDs is a RabbitMQ feature used for correlating the request and response
    by a unique ID in a distributed world. The Flink RabbitMQ connector provides an
    interface to set this to true or false depending on whether you are using it or
    not.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联ID是RabbitMQ的一个特性，用于在分布式世界中通过唯一ID相关请求和响应。Flink RabbitMQ连接器提供了一个接口，可以根据您是否使用它来设置为true或false。
- en: Deserialization schema--RabbitMQ stores and transports the data in a serialized
    manner to avoid network traffic. So when the message is received, the subscriber
    should know how to deserialize the message. The Flink connector provides us with
    some default deserializers such as the string deserializer.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反序列化模式--RabbitMQ以序列化方式存储和传输数据，以避免网络流量。因此，当接收到消息时，订阅者应该知道如何反序列化消息。Flink连接器为我们提供了一些默认的反序列化器，如字符串反序列化器。
- en: 'RabbitMQ source provides us with the following options on stream deliveries:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ源为我们提供了以下关于流传递的选项：
- en: 'Exactly once: Using RabbitMQ correlation IDs and the Flink check-pointing mechanism
    with RabbitMQ transactions'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确切一次：使用RabbitMQ关联ID和Flink检查点机制与RabbitMQ事务
- en: 'At-least once: When Flink checkpointing is enabled but RabbitMQ correlation
    IDs are not set'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一次：当启用Flink检查点但未设置RabbitMQ关联ID时
- en: No strong delivery guarantees with the RabbitMQ auto-commit mode
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RabbitMQ自动提交模式没有强有力的交付保证
- en: 'Here is a diagram to help you understand the RabbitMQ connector in better manner:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个图表，可以帮助您更好地理解RabbitMQ连接器：
- en: '![RabbitMQ connector](img/image_02_006.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![RabbitMQ连接器](img/image_02_006.jpg)'
- en: 'Now let''s look at how to write code to get this connector working. Like other
    connectors, we need to add a Maven dependency to the code:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何编写代码来使这个连接器工作。与其他连接器一样，我们需要向代码添加一个Maven依赖项：
- en: '[PRE58]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The following snippet shows how to use the RabbitMQ connector in Java:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码段显示了如何在Java中使用RabbitMQ连接器：
- en: '[PRE59]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Similarly, in Scala the code can written as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在Scala中，代码可以写成如下形式：
- en: '[PRE60]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We can also use the RabbitMQ connector as a Flink sink. If you want to send
    processes back to some different RabbitMQ queue, the following is a way to do
    so. We need to provide three important configurations:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用RabbitMQ连接器作为Flink sink。如果要将处理过的数据发送回不同的RabbitMQ队列，可以按以下方式操作。我们需要提供三个重要的配置：
- en: RabbitMQ configurations
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RabbitMQ配置
- en: Queue name--Where to send back the processed data
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 队列名称--要将处理过的数据发送回哪里
- en: Serialization schema--Schema for RabbitMQ to convert the data into bytes
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列化模式--RabbitMQ的模式，将数据转换为字节
- en: 'The following is sample code in Java to show how to use this connector as a
    Flink sink:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Java中的示例代码，展示了如何将此连接器用作Flink sink：
- en: '[PRE61]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The same can be done in Scala:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中也可以这样做：
- en: '[PRE62]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: ElasticSearch connector
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ElasticSearch连接器
- en: ElasticSearch is a distributed, low-latency, full text search engine system
    that allows us to index documents of our choice and then allows us to do a full
    text search over the set of documents. More on ElasticSearch can be read here
    at [https://www.elastic.co/](https://www.elastic.co/).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ElasticSearch是一个分布式、低延迟的全文搜索引擎系统，允许我们索引我们选择的文档，然后允许我们对文档集进行全文搜索。有关ElasticSearch的更多信息可以在此处阅读：[https://www.elastic.co/](https://www.elastic.co/)。
- en: In many use cases, you may want to process data using Flink and then store it
    in ElasticSearch. To enable this, Flink supports the ElasticSearch connector.
    So far, ElasticSearch has had two major releases. Flink supports them both.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多用例中，您可能希望使用Flink处理数据，然后将其存储在ElasticSearch中。为此，Flink支持ElasticSearch连接器。到目前为止，ElasticSearch已经发布了两个主要版本。Flink支持它们两个。
- en: 'For ElasticSearch 1.X, the following Maven dependency needs to be added:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ElasticSearch 1.X，需要添加以下Maven依赖项：
- en: '[PRE63]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The Flink connector provides a sink to write data to ElasticSearch. It uses
    two methods to connect to ElasticSearch:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Flink连接器提供了一个sink，用于将数据写入ElasticSearch。它使用两种方法连接到ElasticSearch：
- en: Embedded node
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入式节点
- en: Transport client
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传输客户端
- en: 'The following diagram illustrates this:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了这一点：
- en: '![ElasticSearch connector](img/image_02_007.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![ElasticSearch连接器](img/image_02_007.jpg)'
- en: Embedded node mode
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入式节点模式
- en: In the embedded node mode, the sink uses BulkProcessor to send the documents
    to ElasticSearch. We can configure how many requests to buffer before sending
    documents to ElasticSearch.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入式节点模式中，sink使用BulkProcessor将文档发送到ElasticSearch。我们可以配置在将文档发送到ElasticSearch之前缓冲多少个请求。
- en: 'The following is the code snippet:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码片段：
- en: '[PRE64]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'In the preceding code snippet, we create a hash map with configurations such
    as the cluster name and how many documents to buffer before sending the request.
    Then we add the sink to the stream, specifying the index, type, and the document
    to store. Similar code in Scala follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码片段中，我们创建了一个哈希映射，其中包含集群名称以及在发送请求之前要缓冲多少个文档的配置。然后我们将sink添加到流中，指定要存储的索引、类型和文档。在Scala中也有类似的代码：
- en: '[PRE65]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Transport client mode
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传输客户端模式
- en: ElasticSearch allows connections through the transport client on port 9300\.
    Flink supports connecting using those through its connector. The only thing we
    need to mention here is all the ElasticSearch nodes present in the cluster in
    configurations.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ElasticSearch允许通过端口9300的传输客户端进行连接。Flink支持通过其连接器使用这些连接。这里唯一需要提到的是配置中存在的所有ElasticSearch节点。
- en: 'The following is the snippet in Java:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Java中的片段：
- en: '[PRE66]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Here as well we provide the details about the cluster name, nodes, ports, maximum
    requests to send in bulk, and so on. Similar code in Scala can be written as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还提供了有关集群名称、节点、端口、发送的最大请求数等的详细信息。在Scala中，类似的代码可以编写如下：
- en: '[PRE67]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Cassandra connector
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cassandra连接器
- en: 'Cassandra is a distributed, low latency, NoSQL database. It is a key value-based
    database. Many high throughput applications use Cassandra as their primary database.
    Cassandra works with a distributed cluster mode, where there is no master-slave
    architecture. Reads and writes can be felicitated by any node. More on Cassandra
    can be found at: [http://cassandra.apache.org/](http://cassandra.apache.org/).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra是一个分布式、低延迟的NoSQL数据库。它是一个基于键值的数据库。许多高吞吐量应用程序将Cassandra用作其主要数据库。Cassandra使用分布式集群模式，其中没有主从架构。任何节点都可以进行读取和写入。有关Cassandra的更多信息可以在此处找到：[http://cassandra.apache.org/](http://cassandra.apache.org/)。
- en: 'Apache Flink provides a connector which can write data to Cassandra. In many
    applications, people may want to store streaming data from Flink into Cassandra.
    The following diagram shows a simple design of the Cassandra sink:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink提供了一个连接器，可以将数据写入Cassandra。在许多应用程序中，人们可能希望将来自Flink的流数据存储到Cassandra中。以下图表显示了Cassandra
    sink的简单设计：
- en: '![Cassandra connector](img/image_02_008.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![Cassandra连接器](img/image_02_008.jpg)'
- en: 'Like other connectors, to get this we need to add it as a maven dependency:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他连接器一样，要获得此连接器，我们需要将其添加为Maven依赖项：
- en: '[PRE68]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Once the dependency is added, we just need to add the Cassandra sink with its
    configurations, as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦添加了依赖项，我们只需要添加Cassandra sink及其配置，如下所示：
- en: 'In Java:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE69]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The preceding code writes stream of data into a table called **events**. The
    table expects an event ID and a message. Similarly in Scala:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将数据流写入名为**events**的表中。该表期望事件ID和消息。在Scala中也是如此：
- en: '[PRE70]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Use case - sensor data analytics
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例 - 传感器数据分析
- en: Now that we have looked at various aspects of DataStream API, let's try to use
    these concepts to solve a real world use case. Consider a machine which has sensor
    installed on it and we wish to collect data from these sensors and calculate average
    temperature per sensor every five minutes.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看过了DataStream API的各个方面，让我们尝试使用这些概念来解决一个真实的用例。考虑一个安装了传感器的机器，我们希望从这些传感器收集数据，并计算每五分钟每个传感器的平均温度。
- en: 'Following would be the architecture:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是架构：
- en: '![Use case - sensor data analytics](img/image_02_009.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![用例 - 传感器数据分析](img/image_02_009.jpg)'
- en: In this scenario, we assume that sensors are sending information to Kafka topic
    called **temp** with information as (timestamp, temperature, sensor-ID). Now we
    need to write code to read data from Kafka topics and processing it using Flink
    transformation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们假设传感器正在向名为**temp**的Kafka主题发送信息，信息格式为（时间戳，温度，传感器ID）。现在我们需要编写代码从Kafka主题中读取数据，并使用Flink转换进行处理。
- en: Here important thing to consider is as we already have timestamp values coming
    from sensor, we can use Event Time computations for time factors. This means we
    would be able to take care of events even if they reach out of order.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要考虑的重要事情是，由于我们已经从传感器那里得到了时间戳数值，我们可以使用事件时间计算来处理时间因素。这意味着即使事件到达时是无序的，我们也能够处理这些事件。
- en: We start with simple streaming execution environment which will be reading data
    from Kafka. Since we have timestamps in events, we will be writing a custom timestamp
    and watermark extractor to read the timestamp values and do window processing
    based on that. Here is code snippet for the same.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从简单的流执行环境开始，它将从Kafka中读取数据。由于事件中有时间戳，我们将编写自定义的时间戳和水印提取器来读取时间戳数值，并根据此进行窗口处理。以下是相同的代码片段。
- en: '[PRE71]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Here we assume that we receive events in Kafka topics as strings and in the
    format:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设我们从Kafka主题中以字符串格式接收事件，并且格式为：
- en: '[PRE72]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The following an example code to extract timestamp from record:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从记录中提取时间戳的示例代码：
- en: '[PRE73]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now we simply created keyed data stream and perform average calculation on
    temperature values as shown in the following code snippet:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们简单地创建了分区数据流，并对温度数值进行了平均计算，如下面的代码片段所示：
- en: '[PRE74]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: When execute the preceding given code, and if proper sensor events are published
    on Kafka topics then we will get the average temperature per sensor every five
    minutes.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行上述给定的代码时，如果在Kafka主题上发布了适当的传感器事件，那么我们将每五分钟得到每个传感器的平均温度。
- en: The complete code is available on GitHub at [https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter02/flink-streaming](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter02/flink-streaming).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在GitHub上找到：[https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter02/flink-streaming](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter02/flink-streaming)。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we started with Flink''s most powerful API: DataStream API.
    We looked at how data sources, transformations, and sinks work together. Then
    we looked at various technology connectors such as ElasticSearch, Cassandra, Kafka,
    RabbitMQ, and so on.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从Flink最强大的API开始：DataStream API。我们看了数据源、转换和接收器是如何一起工作的。然后我们看了各种技术连接器，比如ElasticSearch、Cassandra、Kafka、RabbitMQ等等。
- en: At the end, we also tried to apply our learning to solve a real-world sensor
    data analytics use case.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还尝试将我们的学习应用于解决真实世界的传感器数据分析用例。
- en: In the next chapter, we are going to learn about another very important API
    from Flink's ecosystem point of view the DataSet API.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习Flink生态系统中另一个非常重要的API，即DataSet API。
