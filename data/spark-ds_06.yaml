- en: Chapter 6.  Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 机器学习
- en: We are the consumers of machine learning every day, whether we notice or not.
    E-mail providers such as Google automatically push some incoming mails into the `Spam`
    folder and online shopping sites such as Amazon or social networking sites such
    as Facebook jump in with unsolicited recommendations that are surprisingly useful.
    So, what enables these software products to reconnect long lost friends? These
    are just a few examples of machine learning in action.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每天都在使用机器学习，无论我们是否注意到。例如，谷歌等电子邮件提供商会自动将一些收件箱中的邮件推送到“垃圾邮件”文件夹中，亚马逊等在线购物网站或Facebook等社交网络网站会提供出人意料的有用的推荐。那么，是什么使这些软件产品能够重新连接失散已久的朋友呢？这些只是机器学习在实际中的一些例子。
- en: Formally, machine learning is a part of **Artificial Intelligence** (**AI**)
    which deals with a class of algorithms that can learn from data and make predictions.
    The techniques and underlying concepts are drawn from the field of statistics.
    Machine learning exists at the intersection of computer science and statistics
    and is considered one of the most important components of data science. It has
    been around for quite some time now, but its complexity has only increased with
    increase in data and scalability requirements. Machine learning algorithms tend
    to be resource intensive and iterative in nature, which render them a poor fit
    for MapReduce paradigm. MapReduce works very well for single pass algorithms but
    does not cater so well for multi-pass counterparts. The Spark research program
    was started precisely to address this challenge. Apache Spark is equipped with
    efficient algorithms in its MLlib library that are designed to perform well even
    in iterative computational requirements.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，机器学习是**人工智能**（**AI**）的一部分，它处理一类可以从数据中学习并进行预测的算法。这些技术和基本概念来自统计学领域。机器学习存在于计算机科学和统计学的交叉点，被认为是数据科学中最重要的组成部分之一。它已经存在了一段时间，但随着数据量和可扩展性要求的增加，其复杂性也在增加。机器学习算法往往需要大量资源，并且具有迭代性质，这使它们不适合MapReduce范式。MapReduce非常适用于单次遍历算法，但对于多次遍历的算法并不那么适用。Spark研究项目正是为了解决这一挑战而启动的。Apache
    Spark在其MLlib库中配备了高效的算法，即使在迭代计算需求下也能表现良好。
- en: The previous chapter outlined the data analytics' life cycle and its various
    components such as data cleaning, data transformation, sampling techniques, and
    graphical techniques to visualize the data, along with concepts covering descriptive
    statistics and inferential statistics. We also looked at some of the statistical
    testing that could be performed on the Spark platform. Further to the basics we
    built up in the previous chapter, we are going to cover in this chapter most of
    the machine learning algorithms and how to use them to build models on Spark.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章概述了数据分析的生命周期及其各个组成部分，如数据清洗、数据转换、抽样技术和可视化数据的图形技术，以及涵盖描述性统计和推断统计的概念。我们还研究了一些可以在Spark平台上执行的统计测试。在上一章中建立的基础上，我们将在本章中涵盖大部分机器学习算法以及如何使用它们在Spark上构建模型。
- en: 'As a prerequisite for this chapter, basic understanding of machine learning
    algorithms and computer science fundamentals are nice to have. However, we have
    covered some theoretical basics of the algorithms with right set of practical
    examples to make those more comprehendible and easy to implement. The topics covered
    in this chapter are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的先决条件，对机器学习算法和计算机科学基础的基本理解是很有帮助的。然而，我们已经涵盖了一些算法的理论基础，并配以一套合适的实际例子，使这些更易于理解和实施。本章涵盖的主题有：
- en: Introduction to machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习介绍
- en: The evolution
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演变
- en: Supervised learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: MLlib and the Pipeline API
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLlib和Pipeline API
- en: MLlib
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLlib
- en: ML pipeline
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML管道
- en: Introduction to machine learning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习介绍
- en: Parametric methods
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数方法
- en: Non-parametric methods
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数方法
- en: Regression methods
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归方法
- en: Linear regression
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Regularization on regression
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归正则化
- en: Classification methods
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类方法
- en: Logistic regression
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Linear Support Vector Machines (SVMs)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性支持向量机（SVM）
- en: Decision trees
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Impurity measures
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不纯度度量
- en: Stopping rule
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停止规则
- en: Split canditate
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分裂候选
- en: Advantages of decision tress
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的优势
- en: Example
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例子
- en: Ensembles
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成
- en: Random forests
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Gradient boosted trees
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: Multilayer perceptron classifier
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知器分类器
- en: Clustering techniques
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类技术
- en: K-means clustering
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值聚类
- en: Summary
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: Introduction
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Machine learning is all about learning by example data; examples that produce
    a particular output for a given input. There are various business use cases for
    machine learning. Let us look at a few examples to get an idea of what exactly
    it is:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习就是通过示例数据进行学习的过程；这些示例为给定输入产生特定输出。机器学习有各种各样的商业用例。让我们看一些例子，以了解它到底是什么：
- en: A recommendation engine that recommends users what they might be interested
    in buying
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐引擎，推荐用户可能感兴趣的购买商品
- en: Customer segmentation (grouping customers who share similar characteristics)
    for marketing campaigns
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户细分（将具有相似特征的客户分组）用于营销活动
- en: Disease classification for cancer - malignant/benign
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 癌症的疾病分类-恶性/良性
- en: Predictive modeling, for example, sales forecasting, weather forecasting
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测建模，例如，销售预测，天气预测
- en: Drawing business inferences, for example, understanding what effect will change
    the price of a product have on sales
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制业务推论，例如，了解产品价格变化对销售的影响
- en: The evolution
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演变
- en: The concept of statistical learning was existent even before the first computer
    system was introduced. In the nineteenth century, the least squares technique
    (now called linear regression) had already been developed. For classification
    problems, Fisher came up with **Linear Discriminant Analysis** (**LDA**). Around
    the 1940s, an alternative to LDA, known as **logistic regression**, was proposed
    and all these approaches not only improved with time, but also inspired the development
    of other new algorithms.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学习的概念甚至在第一台计算机系统出现之前就已存在。在19世纪，最小二乘法（现在称为线性回归）已经被发展出来。对于分类问题，费舍尔提出了**线性判别分析**（**LDA**）。大约在20世纪40年代，LDA的替代方案，即**逻辑回归**，被提出，所有这些方法不仅随着时间的推移得到改进，而且还激发了其他新算法的发展。
- en: During those times, computation was a big problem as it was done using pen and
    paper. So fitting non-linear equations was not quite feasible as it required a
    lot of computations. After the 1980s, with improvements in technology and the
    introduction of computer systems, classification/regression trees were introduced.
    Slowly, with further advancements in technology and computing systems, statistical
    learning in a way converged with what is now known as machine learning.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些时代，计算是一个大问题，因为它是用纸和笔完成的。因此，拟合非线性方程并不太可行，因为它需要大量的计算。20世纪80年代后，随着技术的改进和计算机系统的引入，分类/回归树被引入。随着技术和计算系统的进一步发展，统计学习在某种程度上与现在所称的机器学习融合在一起。
- en: Supervised learning
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'As discussed in the previous section, machine learning is all about learning
    by example data. Based on how the algorithms understand data and get trained on
    it, they are broadly divided into two categories: **supervised learning** and
    **unsupervised learning**.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所讨论的，机器学习完全是基于示例数据的学习。根据算法如何理解数据并对其进行训练，它们大致分为两类：**监督学习**和**无监督学习**。
- en: 'Supervised statistical learning involves building a model based on one or more
    inputs for a particular output. This means that the output that we get can supervise
    our analysis based on the inputs we supply. In other words, for each observation of
    the predictor variables (for example, age, education, and expense variables),
    there is an associated response measurement of the outcome variable (for example,
    salary). Refer to the following table to get an idea of the example dataset where
    we are trying to predict the **Salary** based on the **Age**, **Education,** and
    **Expense** variables:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 监督统计学习涉及基于一个或多个输入构建模型以获得特定输出。这意味着我们获得的输出可以根据我们提供的输入监督我们的分析。换句话说，对于预测变量的每个观察（例如年龄、教育和费用变量），都有一个相关的结果变量的响应测量（例如工资）。参考以下表格，以了解我们正在尝试根据**年龄**、**教育**和**费用**变量预测**工资**的示例数据集：
- en: '![Supervised learning](img/image_06_001.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/image_06_001.jpg)'
- en: Supervised algorithms can be used for predicting, estimating, classifying, and
    other similar requirements which we will cover in the following sections.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 监督算法可用于预测、估计、分类和其他类似要求，我们将在以下部分进行介绍。
- en: Unsupervised learning
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised statistical learning involves building a model based on one or
    more inputs but with no intention to produce a specific output. This means that
    there is no response/output variable to predict explicitly; but the output is
    usually the groups of data points that share some similar characteristics. Unlike
    supervised learning, you are not aware of the groups/labels to classify the data
    points into, per say, and you leave it to the algorithm to decide by itself.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督统计学习涉及基于一个或多个输入构建模型，但没有产生特定输出的意图。这意味着没有明确的响应/输出变量需要预测；但输出通常是共享某些相似特征的数据点的组。与监督学习不同，您不知道要将数据点分类到哪些组/标签中，而是让算法自行决定。
- en: Here, there is no concept of a `training` dataset that is used to `relate` the
    outcome variable with the `predictor` variables by building a model and then validate
    the model using the `test` dataset. The output of unsupervised algorithm cannot
    supervise your analysis based on the inputs you supply. Such algorithms can learn
    relationships and structure from data. *Clustering* and *Association rule learning*
    are examples of unsupervised learning techniques.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，没有“训练”数据集的概念，该数据集用于通过构建模型将结果变量与“预测”变量相关联，然后使用“测试”数据集验证模型。无监督算法的输出不能监督您基于您提供的输入进行分析。这样的算法可以从数据中学习关系和结构。*聚类*和*关联规则学习*是无监督学习技术的例子。
- en: 'The following image depicts how clustering is used to group the data items
    that share some similar characteristics:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像描述了聚类如何用于将共享某些相似特征的数据项分组：
- en: '![Unsupervised learning](img/image_06_002.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/image_06_002.jpg)'
- en: MLlib and the Pipeline API
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLlib和管道API
- en: Let us first learn some Spark fundamentals to be able to perform the machine
    learning operations on it. We will discuss the MLlib and the pipeline API in this
    section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先学习一些Spark基础知识，以便能够在其上执行机器学习操作。我们将在本节讨论MLlib和管道API。
- en: MLlib
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLlib
- en: MLlib is the machine learning library built on top of Apache Spark which homes
    most of the algorithms that can be implemented at scale. The seamless integration
    of MLlib with other components such as GraphX, SQL, and Streaming provides developers
    with an opportunity to assemble complex, scalable, and efficient workflows relatively
    easily. The MLlib library consists of common learning algorithms and utilities
    including classification, regression, clustering, collaborative filtering, and
    dimensionality reduction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是建立在Apache Spark之上的机器学习库，其中包含大多数可以大规模实施的算法。MLlib与GraphX、SQL和Streaming等其他组件的无缝集成为开发人员提供了一个相对容易地组装复杂、可扩展和高效的工作流的机会。MLlib库包括常见的学习算法和实用程序，包括分类、回归、聚类、协同过滤和降维。
- en: MLlib works in conjunction with the `spark.ml` package which provides a high
    level Pipeline API. The fundamental difference between these two packages is that
    MLlib (`spark.mllib`) works on top of RDDs whereas the ML (`spark.ml`) package
    works on top of DataFrames and supports ML Pipeline. Currently, both packages
    are supported by Spark but it is recommended to use the `spark.ml` package.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib与`spark.ml`包配合使用，后者提供了高级Pipeline API。这两个包之间的基本区别在于MLlib（`spark.mllib`）在RDD之上工作，而ML（`spark.ml`）包在DataFrame之上工作，并支持ML
    Pipeline。目前，Spark支持这两个包，但建议使用`spark.ml`包。
- en: 'Fundamental data types in this library are vectors and matrices. Vectors are
    local, and may be dense or sparse. Dense vectors are stored as an array of values.
    Sparse vectors are stored as two arrays; the first array stores the non-zero value
    indices and the second array stores the actual values. All element values are
    stored as doubles and indices are stored as integers starting from zero. Understanding
    the fundamental structures goes a long way in effective use of the libraries and
    it should help code up any new algorithm from scratch. Let us see some example
    code for a better understanding of these two vector representations:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此库中的基本数据类型是向量和矩阵。向量是本地的，可以是密集的或稀疏的。密集向量存储为值数组。稀疏向量存储为两个数组；第一个数组存储非零值索引，第二个数组存储实际值。所有元素值都存储为双精度浮点数，索引存储为从零开始的整数。了解基本结构对于有效使用库非常重要，它应该有助于从头开始编写任何新算法。让我们看一些示例代码，以更好地理解这两种向量表示：
- en: '**Scala**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Python:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'Python:'
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Matrices may be local or distributed, dense or sparse. A local matrix is stored
    on a single machine as a single dimensional array. A dense local matrix is stored
    in column major order (column members are contiguous) whereas a sparse matrix
    values are stored in **Compressed Sparse Column** (**CSC**) format in column major
    order. In this format, the matrix is stored in the form of three arrays. The first
    array contains row indices of non-zero values, the second array has the beginning
    value index for each column, and the third one is an array of all the non-zero
    values. Indices are of type integer starting from zero. The first array contains
    values from zero to the number of rows minus one. The third array has elements
    of type double. The second array requires some explanation. Every entry in this
    array corresponds to the index of the first non-zero element in each column. For
    example, assume that there is only one non-zero element in each column in a 3
    by 3 matrix. Then the second array would contain 0,1,2 as its elements. The first
    array contains row positions and the third array contains three values. If none
    of the elements in a column are non-zero, you will note the same index repeating
    in the second array. Let us examine some example code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵可以是本地的或分布式的，密集的或稀疏的。本地矩阵存储在单个机器上作为一维数组。密集本地矩阵按列主序存储（列成员是连续的），而稀疏矩阵值以**压缩稀疏列**（**CSC**）格式按列主序存储。在这种格式中，矩阵以三个数组的形式存储。第一个数组包含非零值的行索引，第二个数组包含每列的起始值索引，第三个数组是所有非零值的数组。索引的类型为从零开始的整数。第一个数组包含从零到行数减一的值。第三个数组的元素类型为双精度浮点数。第二个数组需要一些解释。该数组中的每个条目对应于每列中第一个非零元素的索引。例如，假设在一个3乘3的矩阵中每列只有一个非零元素。那么第二个数组的元素将包含0,1,2。第一个数组包含行位置，第三个数组包含三个值。如果某列中的元素都不是非零的，你会注意到第二个数组中重复相同的索引。让我们看一些示例代码：
- en: '**Scala:**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Python:**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Distributed matrices are the most sophisticated ones and choosing the right
    type of distributed matrix is very important. A distributed matrix is backed by
    one or more RDDs. The row and column indices are of the type `long` to support
    very large matrices. The basic type of distributed matrix is a `RowMatrix`, which
    is simply backed by an RDD of its rows.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式矩阵是最复杂的，选择正确的分布式矩阵类型非常重要。分布式矩阵由一个或多个RDD支持。行和列的索引类型为`long`，以支持非常大的矩阵。分布式矩阵的基本类型是`RowMatrix`，它简单地由其行的RDD支持。
- en: 'Each row in turn is a local vector. This is suitable when the number of columns
    is very low. Remember, we need to pass RDDs to create distributed matrices, unlike
    the local ones. Let us look at an example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行依次是一个本地向量。当列数非常低时，这是合适的。记住，我们需要传递RDD来创建分布式矩阵，不像本地矩阵。让我们看一个例子：
- en: '**Scala:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Python:**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: An `IndexedRowMatrix` stores a row index prefixed to the row entry. This is
    useful in executing joins. You need to pass `IndexedRow` objects to create an
    `IndexedRowMatrix`. An `IndexedRow` object is a wrapper with a long `Index` and
    a `Vector` of row elements.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`IndexedRowMatrix`将行索引前缀到行条目中。这在执行连接时非常有用。您需要传递`IndexedRow`对象来创建`IndexedRowMatrix`。`IndexedRow`对象是一个包装器，带有长`Index`和一组行元素的`Vector`。'
- en: A `CoordinatedMatrix` stores data as tuples of row, column indexes, and element
    value. A `BlockMatrix` represents a distributed matrix in blocks of local matrices.
    Methods to convert matrices from one type to another are provided but these are
    expensive operations and should be used with caution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`CoordinatedMatrix`将数据存储为行、列索引和元素值的元组。`BlockMatrix`表示分布式矩阵，以本地矩阵块的形式存储。提供了从一种类型转换为另一种类型的方法，但这些是昂贵的操作，应谨慎使用。'
- en: ML pipeline
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML pipeline
- en: A real life machine learning workflow is an iterative cycle of data extraction,
    data cleansing, pre-processing, exploration, feature extraction, model fitting,
    and evaluation. ML Pipeline on Spark is a simple API for users to set up complex
    ML workflows. It was designed to address some of the pain areas such as parameter
    tuning, or training many models based on different splits of data (cross-validation),
    or different sets of parameters. Writing scripts to automate this whole thing
    is no more a requirement and can be taken care of within the Pipeline API itself.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现实生活中的机器学习工作流程是数据提取、数据清洗、预处理、探索、特征提取、模型拟合和评估的迭代循环。Spark上的ML Pipeline是用户设置复杂ML工作流的简单API。它旨在解决一些痛点，如参数调整，或基于数据不同拆分（交叉验证）或不同参数集训练多个模型。编写脚本来自动化整个过程不再是必需的，可以在Pipeline
    API中处理。
- en: The Pipeline API consists of a series of pipeline stages (implemented as abstractions
    such as *transformers* and *estimators*) to get executed in a desired order.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Pipeline API由一系列流水线阶段（实现为*transformers*和*estimators*等抽象）组成，以按所需顺序执行。
- en: In the ML Pipeline, you can invoke the data cleaning/transformation functions
    as discussed in the previous chapter and call the machine learning algorithms
    that are available in the MLlib. This can be done in an iterative fashion till
    you get the desired performance of your model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML Pipeline中，您可以调用前一章中讨论的数据清洗/转换函数，并调用MLlib中可用的机器学习算法。这可以以迭代的方式进行，直到获得所需的模型性能。
- en: '![ML pipeline](img/image_06_003.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![ML流水线](img/image_06_003.jpg)'
- en: Transformer
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer
- en: A transformer is an abstraction which implements the `transform()` method to
    convert one DataFrame into another. If the method is a feature transformer, the
    resulting DataFrame might contain some additional transformed columns based on
    the operation you performed. However, if the method is a learning model, then
    the resulting DataFrame would contain an extra column with predicted outcomes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是一个抽象，实现`transform()`方法将一个DataFrame转换为另一个。如果该方法是特征转换器，则生成的DataFrame可能包含基于您执行的操作的一些额外转换列。但是，如果该方法是学习模型，则生成的DataFrame将包含一个带有预测结果的额外列。
- en: Estimator
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Estimator
- en: An Estimator is an abstraction that can be any learning algorithm which implements
    the `fit()` method to get trained on a DataFrame to produce a model. Technically,
    this model is a transformer for the given DataFrame.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Estimator是一个抽象，可以是任何实现`fit()`方法以在DataFrame上进行训练以生成模型的学习算法。从技术上讲，该模型是给定DataFrame的transformer。
- en: 'Example: Logistic regression is a learning algorithm, hence an estimator. Calling
    `fit()` trains a logistic regression model, which is a resultant model, and hence
    a transformer which can produce a DataFrame containing a predicted column.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：逻辑回归是一种学习算法，因此是一个estimator。调用`fit()`训练逻辑回归模型，这是一个结果模型，因此是一个transformer，可以生成包含预测列的DataFrame。
- en: The following example demonstrates a simple, single stage pipeline.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了一个简单的单阶段流水线。
- en: '**Scala:**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Python:**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: "The above example showed pipeline creation and execution although with a single\
    \ stage, a Tokenizer in this context. Spark provides several \"\x80\x9Cfeature\
    \ transformers\x80\" out of the box. These feature transformers are quite handy\
    \ during data cleaning and data preparation phases."
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例展示了流水线的创建和执行，尽管只有一个阶段，在这种情况下是一个分词器。Spark提供了几种“特征转换器”作为开箱即用的功能。这些特征转换器在数据清洗和数据准备阶段非常方便。
- en: The following example shows a real world example of converting raw text into
     feature vectors. If you are not familiar with TF-IDF, read this short tutorial
    from [http://www.tfidf.com](http://www.tfidf.com).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了将原始文本转换为特征向量的真实示例。如果您对TF-IDF不熟悉，请阅读来自[http://www.tfidf.com](http://www.tfidf.com)的简短教程。
- en: '**Scala:**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Python:**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This example has created and executed a multi-stage pipeline that has converted
    text to a feature vector that can be processed by machine learning algorithms.
    Let us see a few more features before we move on.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例已创建并执行了一个多阶段流水线，将文本转换为可以由机器学习算法处理的特征向量。在我们继续之前，让我们看看更多功能。
- en: '**Scala:**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Python:**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Introduction to machine learning
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: 'In the previous sections of the book, we learnt how the response/outcome variable
    is related to the predictor variables, typically in a supervised learning context.
    There are various different names for both of those types of variables that people
    use these days. Let us see some of the synonymous terms for them and we will use
    them interchangeably in the book:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几节中，我们学习了响应/结果变量如何与预测变量相关联，通常在监督学习环境中。这些类型的变量人们现在使用各种不同的名称。让我们看看它们的一些同义词，并在书中交替使用它们：
- en: '**Input variables (X)**: Features, predictors, explanatory variables, independent
    variables'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入变量（X）**：特征，预测变量，解释变量，自变量'
- en: '**Output variables (Y)**: Response variable, dependent variable'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出变量（Y）**：响应变量，因变量'
- en: 'If there is a relation between *Y* and *X* where *X=X[1], X[2], X[3],..., X[n]*
    (n different predictors) then it can be written as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*Y*与*X*之间存在关系，其中*X=X[1], X[2], X[3],..., X[n]*（n个不同的预测变量），则可以写成如下形式：
- en: '![Introduction to machine learning](img/image_06_004.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_004.jpg)'
- en: Here ![Introduction to machine learning](img/image_06_005.jpg)is a function
    that represents how *X* describes *Y* and is unknown! This is what we figure out
    using the observed data points at hand. The term
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里![机器学习简介](img/image_06_005.jpg)是一个表示*X*描述*Y*且未知的函数！这是我们使用手头观察到的数据点来找出的。术语
- en: '![Introduction to machine learning](img/image_06_006.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_006.jpg)'
- en: is a random error term with mean zero and is independent of *X*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个均值为零且与*X*无关的随机误差项。
- en: There are basically two types of errors associated with such an equation - reducible
    errors and irreducible errors. As the name suggests, a reducible error is associated
    with the function and can be minimized by improving the accuracy of
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与这样一个方程相关的基本上有两种类型的错误 - 可减少的错误和不可减少的错误。顾名思义，可减少的错误与函数相关，可以通过提高准确性来最小化
- en: '![Introduction to machine learning](img/image_06_007.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_007.jpg)'
- en: by using a better learning algorithm or by tuning the same algorithm. Since
    *Y* is also a function of
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用更好的学习算法或调整相同的算法。由于*Y*也是一个函数
- en: '![Introduction to machine learning](img/image_06_008.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_008.jpg)'
- en: ', which is independent of *X*, there would still be some error associated that
    cannot be addressed. This is called an irreducible error ('
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ，这是独立于*X*的，仍然会有一些与之相关的错误，无法解决。这被称为不可减少的错误（
- en: '![Introduction to machine learning](img/image_06_009.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_009.jpg)'
- en: ). There are always some factors which influence the outcome variable but are
    not considered in building the model (as they are unknown most of the time), and
    contribute to the irreducible error term. So, our approaches discussed throughout
    this book will only be focused on minimizing the reducible error.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ）。总是有一些因素影响结果变量，但在建模时未考虑（因为大多数情况下它们是未知的），并且导致不可减少的错误项。因此，我们在本书中讨论的方法只关注最小化可减少的错误。
- en: Most of the machine learning models that we build can be used for either prediction
    or for inference, or a combination of both. For some of the algorithms, the function
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的大多数机器学习模型可以用于预测或推断，或者两者结合。对于一些算法，函数
- en: '![Introduction to machine learning](img/image_06_010.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_010.jpg)'
- en: can be represented as an equation which tells us how the dependent variable
    *Y* is related to the independent variables (*X1*, *X2*,..., *Xn*). In such cases,
    we can do both inference and prediction. However, some of the algorithms are black
    box, where we can only predict and no inference is possible, because how *Y* is
    related to *X* is unknown.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 可以表示为一个方程，告诉我们因变量*Y*如何与自变量（*X1*，*X2*，...，*Xn*）相关。在这种情况下，我们既可以进行推断，也可以进行预测。然而，一些算法是黑匣子，我们只能进行预测，无法进行推断，因为*Y*与*X*的关系是未知的。
- en: Note that the linear machine learning models can be more apt for an inference
    setting because they are more interpretable to business users. However, on a prediction
    setting, there can be better algorithms providing more accurate predictions but
    they are less interpretable. When inference is the target, we should prefer the
    restrictive models such as linear regression for better interpretability, and
    when only prediction is the goal, we may choose to use highly flexible models
    such as **Support Vector Machines** (**SVM**) that are less interpretable and
    more accurate (this may not hold true in all cases, however). You need to be careful
    in choosing an algorithm based on the business requirement, by accounting for
    the trade-off between interpretability and accuracy. Let us dive deeper into understanding
    the fundamentals behind these concepts.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，线性机器学习模型可能更适合推断设置，因为它们对业务用户更具可解释性。然而，在预测设置中，可能有更好的算法提供更准确的预测，但它们的可解释性较差。当推断是目标时，我们应该更喜欢使用诸如线性回归之类的限制性模型，以获得更好的可解释性，而当只有预测是目标时，我们可以选择使用高度灵活的模型，例如**支持向量机**（**SVM**），这些模型不太可解释，但更准确（然而，这在所有情况下可能并不成立）。在选择算法时，您需要根据业务需求来权衡可解释性和准确性之间的权衡。让我们深入了解这些概念背后的基本原理。
- en: Basically, we need a set of data points (training data) to build a model to
    estimate
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们需要一组数据点（训练数据）来构建一个模型来估计
- en: '![Introduction to machine learning](img/image_06_011.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_011.jpg)'
- en: '*(X)* so that *Y =*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X)*，以便*Y =*'
- en: '![Introduction to machine learning](img/image_06_012.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_012.jpg)'
- en: '*(X)*. Broadly, such learning methods can be either parametric or non-parametric.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X)*。广义上说，这样的学习方法可以是参数化的，也可以是非参数化的。'
- en: Parametric methods
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数方法
- en: Parametric methods follow a two-step process. In the first step, you assume
    the shape of
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 参数方法遵循两步过程。在第一步中，您假设
- en: '![Parametric methods](img/image_06_013.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参数方法](img/image_06_013.jpg)'
- en: '*()*. For example, *X* is linearly related to *Y*, so the function of *X,*
    which is'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*()*。例如，*X*与*Y*呈线性关系，因此*X*的函数，即'
- en: '![Parametric methods](img/image_06_014.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参数方法](img/image_06_014.jpg)'
- en: '*(X),* can be represented with a linear equation as shown next:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X)*，可以用下面显示的线性方程表示：'
- en: '![Parametric methods](img/Beta1.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参数方法](img/Beta1.jpg)'
- en: 'After the model is selected, the second step is to estimate the parameters
    *Î²0*, *Î²1*,..., *Î²n* by using the data points at hand to train the model, so
    that:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择后，第二步是通过使用手头的数据点来训练模型来估计参数*Î²0*，*Î²1*，...，*Î²n*，以便：
- en: '![Parametric methods](img/Beta-2.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参数方法](img/Beta-2.jpg)'
- en: The one disadvantage to this parametric approach is that our assumption of linearity
    for ![Parametric methods](img/image_06_016.jpg) *()* might not hold true in real
    life situations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种参数化方法的一个缺点是我们对于![参数方法](img/image_06_016.jpg) *()* 在现实生活中的情况下可能不成立。
- en: Non-parametric methods
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非参数方法
- en: We do not make any assumptions about the linear relation between *Y* and *X*
    as well as data distributions of variables, and hence the form of
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不对*Y*和*X*之间的线性关系以及变量的数据分布做任何假设，因此
- en: '![Non-parametric methods](img/image_06_017.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![非参数方法](img/image_06_017.jpg)'
- en: '*()* in non-parametric. Since it does not assume any form of'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*()* 在非参数化中。因为它不假设任何形式的'
- en: '![Non-parametric methods](img/image_06_018.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![非参数方法](img/image_06_018.jpg)'
- en: '*()*, it can produce better results by fitting well with data points, which
    could be an advantage.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*()*，通过与数据点很好地拟合，可以产生更好的结果，这可能是一个优势。'
- en: So, the non-parametric methods require more data points compared to parametric
    methods to estimate
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，与参数方法相比，非参数方法需要更多的数据点来估计
- en: '![Non-parametric methods](img/image_06_019.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![非参数方法](img/image_06_019.jpg)'
- en: '*()* accurately. Note however, it can lead to overfitting problems if not handled
    properly. We will discuss more on this as we move further.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*()*准确。但是请注意，如果处理不当，它可能会导致过度拟合问题。随着我们的进展，我们将更多地讨论这个问题。'
- en: Regression methods
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归方法
- en: Regression methods are a type of supervised learning. If the response variable
    is quantitative/continuous (takes on numeric values such as age, salary, height,
    and so on), then the problem can be called a regression problem regardless of
    the explanatory variables' type. There are various kinds of modeling techniques
    to address the regression problems. In this section, our focus will be on linear
    regression techniques and some different variations of it.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 回归方法是一种监督学习的类型。如果响应变量是定量/连续的（取数值，如年龄、工资、身高等），则无论解释变量的类型如何，问题都可以称为回归问题。有各种建模技术来解决回归问题。在本节中，我们将重点放在线性回归技术和一些不同的变体上。
- en: 'Regression methods can be used to predict any real valued outcomes. Following
    are a few examples:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 回归方法可用于预测任何实值结果。以下是一些例子：
- en: Predict the salary of an employee based on his educational level, location,
    type of job, and so on
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据教育水平、地点、工作类型等预测员工的工资
- en: Predict stock prices
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测股票价格
- en: Predict buying potential of a customer
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测客户的购买潜力
- en: Predict the time a machine would take before failing
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测机器故障前需要的时间
- en: Linear regression
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: Further to what we discussed in the previous section *Parametric methods*, after
    the assumption of linearity is made for
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节*参数方法*中讨论的内容之后，假设线性是
- en: '![Linear regression](img/image_06_020.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_020.jpg)'
- en: '*(X)*, we need the training data to fit a model that would describe the relation
    between explanatory variables (denoted as *X*) and the response variable (denoted
    as *Y*). When there is only one explanatory variable present, it is called simple
    linear regression and when there are multiple explanatory variables present, it
    is called multiple linear regression. The simple linear regression is all about
    fitting a straight line in a 2-D setting, and when there are say two predictor
    variables, it would fit a plane in a 3-D setting, and so on for higher dimensional
    settings when there are more than two variables.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X)*，我们需要训练数据来拟合一个描述解释变量（表示为*X*）和响应变量（表示为*Y*）之间关系的模型。当只有一个解释变量时，称为简单线性回归，当有多个解释变量时，称为多元线性回归。简单线性回归就是在二维设置中拟合一条直线，当有两个预测变量时，它将在三维设置中拟合一个平面，以此类推，当有两个以上的变量时，它将在更高维的设置中拟合一个平面。'
- en: 'The usual form of a linear regression equation can be represented as:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归方程的通常形式可以表示为：
- en: Y' =
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Y' =
- en: '![Linear regression](img/image_06_021.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_021.jpg)'
- en: (X) +
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: (X) +
- en: '![Linear regression](img/image_06_022.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_022.jpg)'
- en: Here *Y'* represents the predicted outcome variable.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*Y'*代表了预测的结果变量。
- en: 'A linear regression equation with only one predictor variable can be given
    as:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个预测变量的线性回归方程可以表示为：
- en: '![Linear regression](img/Beta11.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/Beta11.jpg)'
- en: 'A linear regression equation with multiple predictor variables can be given
    as:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 具有多个预测变量的线性回归方程可以表示为：
- en: '![Linear regression](img/Beta22.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/Beta22.jpg)'
- en: Here ![Linear regression](img/image_06_025.jpg) is the irreducible error term
    independent of *X* and has a mean of zero. We do not have any control over it,
    but we can work towards optimizing
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里![线性回归](img/image_06_025.jpg)是与*X*无关的不可减小的误差项，均值为零。我们无法控制它，但我们可以努力优化
- en: '![Linear regression](img/image_06_026.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_026.jpg)'
- en: '*(X)*. Since none of the models can achieve a 100 percent accuracy, there would
    always be some error associated with it because of the irreducible error component
    ('
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X)*。由于没有任何模型可以达到100%的准确性，总会有一些与之相关的误差，因为不可减小的误差组成部分('
- en: '![Linear regression](img/image_06_027.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_027.jpg)'
- en: ).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: )。
- en: 'The most common approach of fitting a linear regression is called **least squares**,
    also known as, the **Ordinary Least Squares** (**OLS**) approach. This method
    finds the regression line that best fits the observed data points by minimizing
    the sum of squares of the vertical deviations from each data point to the regression
    line. To get a better understanding on how the linear regression works, let us
    look at a simple linear regression of the following form for now:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合线性回归最常见的方法称为**最小二乘法**，也称为**普通最小二乘法**（**OLS**）方法。该方法通过最小化每个数据点到回归线的垂直偏差的平方和来找到最适合观察数据点的回归线。为了更好地理解线性回归的工作原理，让我们现在看一个简单线性回归的形式：
- en: '![Linear regression](img/Beta33.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/Beta33.jpg)'
- en: 'Where, *Î²0* is the Y-intercept of the regression line and *Î²1* defines the
    slope of the line. What it means is that *Î²1* is the average change in *Y* for
    every one unit change in *X*. Let us take an example with *X* and *Y*:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*Î²0*是回归线的Y截距，*Î²1*定义了线的斜率。这意味着*Î²1*是*X*每变化一个单位时*Y*的平均变化。让我们举个*X*和*Y*的例子：
- en: '| **X** | **Y** |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **X** | **Y** |'
- en: '| **1** | 12 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 12 |'
- en: '| **2** | 20 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: 2 20
- en: '| **3** | 13 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 13 |'
- en: '| **4** | 38 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 38 |'
- en: '| **5** | 27 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 27 |'
- en: 'If we fit a linear regression line through the data points as shown in the
    preceding table, then it would appear as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过前面表格中显示的数据点拟合一条线性回归线，那么它将如下所示：
- en: '![Linear regression](img/image_06_028.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_028.jpg)'
- en: 'The red vertical lines in the preceding figure indicate the error of prediction
    which can be defined as the difference between the actual *Y* value and the predicted
    *Y''* value. If you square these differences and sum them up, it is called the
    **Sum of Squared Error** (**SSE**), which is the most common measure that is used
    to find the best fitting line. The following table shows how to calculate the
    SSE:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上图中的红色垂直线表示预测误差，可以定义为实际Y值与预测Y'值之间的差异。如果平方这些差异并将它们相加，就称为残差平方和(SSE)，这是用于找到最佳拟合线的最常用的度量。下表显示了如何计算SSE：
- en: '| **X** | **Y** | **Y''** | **Y-Y''** | **(Y-Y'') 2** |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **X** | **Y** | **Y''** | **Y-Y''** | **(Y-Y'') 2** |'
- en: '| **1** | 12 | 12.4 | 0.4 | 0.16 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 12 | 12.4 | 0.4 | 0.16 |'
- en: '| **2** | 20 | 17.2 | 2.8 | 7.84 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 20 | 17.2 | 2.8 | 7.84 |'
- en: '| **3** | 13 | 22 | -9 | 81 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 13 | 22 | -9 | 81 |'
- en: '| **4** | 38 | 26.8 | 11.2 | 125.44 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 38 | 26.8 | 11.2 | 125.44 |'
- en: '| **5** | 27 | 31.6 | -4.6 | 21.16 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 27 | 31.6 | -4.6 | 21.16 |'
- en: '|  |  |  | SUM | 235.6 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| | | | 总和 | 235.6 |'
- en: 'In the above table, the term **(Y-Y'')** is called the residual. The **Residual
    Sum of Squares** (**RSS**) can be represented as:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在上表中，术语(Y-Y')称为残差。残差平方和(RSS)可以表示为：
- en: '*RSS = residual[1]² + residual[2]² + residual[3]² + ......+ residual[n]²*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*RSS = 残差[1]² + 残差[2]² + 残差[3]² + ......+ 残差[n]²*'
- en: Note that regression is highly susceptible to outliers and can introduce huge
    RSS error if not handled prior to applying regression.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，回归对异常值非常敏感，如果在应用回归之前不加以处理，可能会引入巨大的RSS误差。
- en: After a regression line is fit into the observed data points, you should examine
    the residuals by plotting them on the Y-Axis against explanatory the variable
    on the X-Axis. If the plot is nearly a straight line, then your assumption about
    linear relationship is valid, or else it may indicate the presence of some kind
    of non-linear relationship. In case of the presence of nonlinear relationships,
    you may have to account for the non-linearity. One of the techniques is by adding
    higher order polynomials to the equation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察到的数据点中拟合回归线后，应该通过将它们在Y轴上绘制出来，并将解释变量放在X轴上来检查残差。如果图表几乎是一条直线，那么你对线性关系的假设是有效的，否则可能表明存在某种非线性关系。在存在非线性关系的情况下，可能需要考虑非线性。其中一种技术是将高阶多项式添加到方程中。
- en: We saw that RSS was an important characteristic in fitting the regression line
    (while building the model). Now, to assess how good your regression fit is (once
    the model is built), you need two other statistics - **Residual Standard Error**
    (**RSE**) and **R²** statistic.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到RSS是拟合回归线时的一个重要特征（在构建模型时）。现在，为了评估回归拟合的好坏（一旦模型建立好），你需要另外两个统计量 - 残差标准误差(RSE)和R²统计量。
- en: 'We discussed the irreducible error component *Îµ*, because of which there would
    always be some level of error with your regression (even if your equation exactly
    fits your data points and you have estimated the coefficients properly). RSE is
    an estimate of standard deviation of *Îµ* which can be defined as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了不可减小的误差组件Îµ，因此即使你的方程完全拟合数据点并且正确估计了系数，你的回归仍然会有一定水平的误差。RSE是Îµ的标准差的估计，可以定义如下：
- en: '![Linear regression](img/image_06_029.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_029.jpg)'
- en: This means that the actual values would deviate from the true regression line
    by a factor of RSE on an average.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着实际值与真实回归线的偏差平均为RSE的因素。
- en: Since RSE is actually measured in the units of *Y* (refer to how we calculated
    RSS in the previous section), it is difficult to say that it is the only best
    statistic for the model accuracy.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RSE实际上是以Y的单位来衡量的（参考我们在上一节中如何计算RSS），很难说它是模型准确性的唯一最佳统计量。
- en: 'So, an alternative approach was introduced, called the R² statistic (also known
    as the coefficient of determination). The formula to calculate R² is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，引入了一种另类方法，称为R²统计量（也称为决定系数）。计算R²的公式如下：
- en: '![Linear regression](img/image_06_030.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_030.jpg)'
- en: 'The **Total Sum of Squares** (**TSS**) can be calculated as:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 总平方和(TSS)可以计算如下：
- en: '![Linear regression](img/image_06_031.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_031.jpg)'
- en: Note here that TSS measures the total variance inherent in *Y* even before performing
    the regression to predict *Y*. Observe that there is no *Y'* in it. On the contrary,
    RSS represents the variability in *Y* that is unexplained after regression. This
    means that (*TSS - RSS*) is able to explain the variability in response after
    regression is performed.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里要注意，TSS测量了在执行回归预测Y之前Y中固有的总方差。注意它里面没有Y'。相反，RSS代表了回归后未能解释的Y中的变异性。这意味着(TSS -
    RSS)能够解释回归后响应的变异性。
- en: The *R²* statistic usually ranges from 0 to 1, but can be negative if the fit
    is worse than fitting just a horizontal line, but that is rarely the case. A value
    close to 1 indicates that the regression equation could explain a large proportion
    of the variability in the response variable and is a good fit. On the contrary,
    a value close to 0 indicates that the regression did not explain much of the variance
    in the response variable and is not a good fit. As an example, an *R²* of 0.25
    means that 25 percent of the variance in *Y* is explained by *X* and is indicating
    to tune the model for improvement.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: R²统计量通常在0到1之间，但如果拟合比拟合水平线更差，可能会是负数，但这种情况很少见。接近1的值表示回归方程可以解释响应变量中大部分的变异性，是一个很好的拟合。相反，接近0的值表示回归没有解释响应变量中的大部分方差，不是一个很好的拟合。例如，R²为0.25意味着25%的Y的方差由X解释，并且表明需要调整模型以改进。
- en: Let us now discuss how to address the non-linearity in the dataset through regression.
    As discussed earlier, when you find nonlinear relations, it needs to be handled
    properly. To model a non-linear equation using the same linear regression technique,
    you have to create the higher order features, which will be treated as just another
    variable by the regression technique. For example, if *salary* is a feature/variable
    that is predicting the *buying potential*, and we find that there is a non-linear
    relationship between them, then we might create a feature called (*salary3*) depending
    on how much of the non-linearity needs to be addressed. Note that while you create
    such higher order features, you also have to keep the base features. In this example,
    you have to use both (*salary*) and (*salary3*) in the regression equation.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论如何通过回归来处理数据集中的非线性。正如前面讨论的，当发现非线性关系时，需要妥善处理。为了使用相同的线性回归技术建模非线性方程，您必须创建更高阶的特征，这些特征将被回归技术视为另一个变量。例如，如果*薪水*是一个特征/变量，用于预测*购买潜力*，并且我们发现它们之间存在非线性关系，那么我们可能会创建一个名为（*salary3*）的特征，具体取决于需要解决多少非线性。请注意，当您创建这些更高阶特征时，您还必须保留基本特征。在这个例子中，您必须在回归方程中同时使用（*salary*）和（*salary3*）。
- en: So far, we have kind of assumed that all the predictor variables are continuous.
    What if there are categorical predictors? In such cases, we have to dummy-code
    those variables (say 1 for male and 0 for female) so that the regression technique
    generates two equations, one for gender = male (the equation will have the gender
    variable) and the other for gender = female (the equation will not have the gender
    variable as it will be dropped as coded 0). At times, with very few categorical
    variables, it may be a good idea to divide the dataset based on the levels of
    categorical variables and build separate models for them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有点假设所有的预测变量都是连续的。如果有分类预测变量怎么办？在这种情况下，我们必须对这些变量进行虚拟编码（比如男性为1，女性为0），以便回归技术生成两个方程，一个用于性别=男性（方程将包含性别变量），另一个用于性别=女性（方程将不包含性别变量，因为它将被编码为0）。有时，对于非常少的分类变量，根据分类变量的级别划分数据集并为其构建单独的模型可能是一个好主意。
- en: One major advantage of the least squares linear regression is that it explains
    how the outcome variable is related to the predictor variables. This makes it
    very interpretable and can be used to draw inferences as well as to do predictions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘线性回归的一个主要优势是它解释了结果变量与预测变量的关系。这使得它非常可解释，并且可以用于推断以及预测。
- en: Loss function
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: Many machine learning problems can be formulated as a convex optimization problem.
    The objective of this problem is to find the values of the coefficients for which
    the squared loss is minimum. This objective function has basically two components
    - regularizer and the loss function. The regularizer is there to control the complexity
    of the model (so it does not overfit) and the loss function is there to estimate
    the coefficients of the regression function for which squared loss (RSS) is minimum.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习问题可以被制定为凸优化问题。这个问题的目标是找到使平方损失最小的系数值。这个目标函数基本上有两个组成部分 - 正则化器和损失函数。正则化器用于控制模型的复杂性（以防止过拟合），损失函数用于估计回归函数的系数，使得平方损失（RSS）最小。
- en: 'The loss function used for least squares is called **squared loss**, as shown
    next:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘法使用的损失函数称为**平方损失**，如下所示：
- en: '![Loss function](img/image_06_032.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![损失函数](img/image_06_032.jpg)'
- en: Here *Y* is the response variable (real valued), *W* is the weight vector (value
    of the coefficients), and *X* is the feature vector. So
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*Y*是响应变量（实值），*W*是权重向量（系数的值），*X*是特征向量。所以
- en: '![Loss function](img/Capture-1.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![损失函数](img/Capture-1.jpg)'
- en: gives the predicted values which we equate with the actual values *Y* to find
    the squared loss that needs to be minimized.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 给出了预测值，我们将其与实际值*Y*相等，以找到需要最小化的平方损失。
- en: The algorithm used to estimate the coefficients is called **gradient descent**.
    There are different types of loss functions and optimization algorithms for different
    kinds of machine learning algorithms which we will cover as and when needed.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 用于估计系数的算法称为**梯度下降**。不同类型的损失函数和优化算法适用于不同类型的机器学习算法，我们将根据需要进行介绍。
- en: Optimization
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化
- en: Ultimately, the linear methods have to optimize the loss function. Under the
    hood, linear methods use convex optimization methods to optimize the objective
    functions. MLlib has **Stochastic Gradient Descent** (**SGD**) and **Limited Memory
    - Broyden-Fletcher-Goldfarb-Shanno** (**L-BFGS**) supported out of the box. Currently,
    most algorithm APIs support SGD and a few support L-BFGS.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，线性方法必须优化损失函数。在幕后，线性方法使用凸优化方法来优化目标函数。MLlib支持**随机梯度下降**（**SGD**）和**有限内存 - Broyden-Fletcher-Goldfarb-Shanno**（**L-BFGS**）。目前，大多数算法API支持SGD，少数支持L-BFGS。
- en: SGD is a first-order optimization technique that works best for large scale
    data and distributed computing environment. Optimization problems whose objective
    function (loss function) is written as a sum are best suited to be solved using
    SGD.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: SGD是一种适用于大规模数据和分布式计算环境的一阶优化技术。目标函数（损失函数）被写成求和形式的优化问题最适合使用SGD来解决。
- en: L-BFGS is an optimization algorithm in the family of quasi-Newton methods to
    solve the optimization problems. L-BFGS often achieves a rapider convergence compared
    with other first-order optimization techniques such as SGD.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: L-BFGS是一种在拟牛顿方法家族中的优化算法，用于解决优化问题。与SGD等一阶优化技术相比，L-BFGS通常能够实现更快的收敛。
- en: Some of the linear methods available in MLlib support both SGD and L-BFGS. You
    should choose one over the other depending on the objective function under consideration.
    In general, L-BFGS is recommended over SGD as it converges faster but you need
    to evaluate carefully based on the requirement.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib中提供的一些线性方法都支持SGD和L-BFGS。您应该根据所考虑的目标函数选择其中一种。一般来说，L-BFGS比SGD更快地收敛，但您需要根据需求进行仔细评估。
- en: Regularizations on regression
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归的正则化
- en: With large weights (coefficient values), it is easier to overfit the model.
    Regularization is a technique used mainly to eliminate the overfitting problem
    by controlling the complexity of the model. This is usually done when you see
    a difference between the model performance on training data and test data. If
    the training performance is more than that of the test data, it could be a case
    of overfitting (high variance case).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 具有较大权重（系数值）时，容易过拟合模型。正则化是一种主要用于通过控制模型复杂性来消除过拟合问题的技术。通常在看到模型在训练数据和测试数据上的性能差异时进行。如果训练性能高于测试数据，可能是过拟合（高方差）的情况。
- en: To address this, a regularization technique was introduced that would penalize
    the loss function. It is always recommended to use any of the regularizations
    techniques, especially when the training data has a small number of observations.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，引入了一种会对损失函数进行惩罚的正则化技术。在训练数据观测数量较少时，通常建议使用任何一种正则化技术。
- en: Before we discuss further on the regularization techniques, we have to understand
    what *bias* and *variance* mean in a supervised learning setting and why there
    is always a trade-off associated. While both are related to errors, a *biased*
    model means that it is biased towards some erroneous assumption and may miss the
    relation between the predictor variables and the response variable to some extent.
    This is a case of underfitting! On the other hand, a *high variance* model means
    that it tries to touch every data point and ends up modelling the random noise
    present in the dataset. It represents the case of overfitting.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步讨论正则化技术之前，我们必须了解在监督学习环境中，“偏差”和“方差”是什么意思，以及为什么总是存在相关的权衡。虽然两者都与错误有关，“偏差”模型意味着它偏向于某些错误的假设，并且可能在一定程度上忽略预测变量和响应变量之间的关系。这是欠拟合的情况！另一方面，“高方差”模型意味着它试图触及每个数据点，并最终对数据集中存在的随机噪声进行建模。这代表了过拟合的情况。
- en: Linear regression with the L2 penalty (L2 regularization) is called **ridge
    regression** and with the L1 penalty (L1 regularization) is called **lasso regression**.
    When both L1 and L2 penalties are used together, it is called **elastic net regression**.
    We will discuss them one by one in the following section.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 带有L2惩罚（L2正则化）的线性回归称为**岭回归**，带有L1惩罚（L1正则化）的线性回归称为**套索回归**。当同时使用L1和L2惩罚时，称为**弹性网络回归**。我们将在下一节依次讨论它们。
- en: L2 regularized problems are usually easy to solve compared to L1 regularized
    problems due to smoothness, but the L1 regularized problems can cause sparsity
    in weights leading to smaller and more interpretable models. Because of this,
    lasso is at times used for feature selection.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 与L1正则化问题相比，L2正则化问题通常更容易解决，因为它更加平滑，但L1正则化问题可能导致权重的稀疏性，从而导致更小且更可解释的模型。因此，套索有时用于特征选择。
- en: Ridge regression
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 岭回归
- en: 'When we add the L2 penalty (also known as the **shrinkage penalty**) to the
    loss function of least squares, it becomes the ridge regression, as shown next:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在最小二乘损失函数中加入L2惩罚（也称为**收缩惩罚**）时，就变成了岭回归，如下所示：
- en: '![Ridge regression](img/image_06_034.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![岭回归](img/image_06_034.jpg)'
- en: Here *λ* (greater than 0) is a tuning parameter which is determined separately.
    The second term in the preceding equation is called the shrinkage penalty and
    can be small only if the coefficients (*Î²0*, *Î²1*...and so on) are small and
    close to 0\. When *λ = 0*, the ridge regression becomes least squares. As lambda
    approaches infinity, the regression coefficients approach zero (but are never
    zero).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*λ*（大于0）是一个单独确定的调整参数。在前述方程的第二项被称为收缩惩罚，只有当系数（*Î²0*，*Î²1*...等等）很小时并且接近0时，它才会很小。当*λ=0*时，岭回归变为最小二乘法。当lambda趋近于无穷大时，回归系数趋近于零（但永远不会为零）。
- en: The ridge regression generates different sets of coefficient values for each
    value of *λ*. So, the lambda value needs to be carefully selected using cross-validation.
    As we increase the lambda value, the flexibility of the regression line decreases,
    thereby decreasing variance and increasing bias.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归为每个*λ*值生成不同的系数值集。因此，需要使用交叉验证来谨慎选择lambda值。随着lambda值的增加，回归线的灵活性减少，从而减少方差并增加偏差。
- en: Note that the shrinkage penalty is applied to all the explanatory variables
    except the intercept term *Î²0*.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，收缩惩罚适用于除截距项*Î²0*之外的所有解释变量。
- en: The ridge regression works really well when the training data is less or even
    in the case where the number of predictors or features are more than the number
    of observations. Also, the computation needed for ridge is almost the same as
    that of least squares.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练数据较少或者预测变量或特征的数量超过观测数量时，岭回归效果非常好。此外，岭回归所需的计算几乎与最小二乘法相同。
- en: Since ridge does not reduce any coefficient value to zero, all the variables
    will be present in the model which can make it less interpretable if the number
    of variables is high.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于岭回归不会将任何系数值减少到零，所有变量都将出现在模型中，这可能会使模型在变量数量较多时变得不太可解释。
- en: Lasso regression
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 套索回归
- en: 'Lasso was introduced after ridge. When we add the L1 penalty to the loss function
    of least squares, it becomes lasso regression, as shown next:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 套索回归是在岭回归之后引入的。当我们在最小二乘损失函数中加入L1惩罚时，就变成了套索回归，如下所示：
- en: '![Lasso regression](img/image_06_035.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![套索回归](img/image_06_035.jpg)'
- en: The difference here is that instead of taking the squared coefficients, it takes
    the mod of the coefficient. Unlike ridge, it can force some of its coefficients
    to be exactly zero which can result in elimination of some of the variables. So,
    lasso can be used for variable selection as well!
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的区别在于，它不是取平方系数，而是取系数的模。与岭回归不同，它可以强制一些系数为零，这可能导致一些变量的消除。因此，Lasso也可以用于变量选择！
- en: Lasso generates different sets of coefficient values for each value of lambda.
    So lambda value needs to be carefully selected using cross-validation. Like ridge,
    as you increase lambda, variance decreases and bias increases.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso为每个lambda值生成不同的系数值集。因此需要使用交叉验证来谨慎选择lambda值。与岭回归一样，随着lambda的增加，方差减小，偏差增加。
- en: Lasso produces better interpretable models compared to ridge because it usually
    has a subset of the total number of variables. When there are many categorical
    variables, it is advisable to choose lasso over ridge.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso相对于岭回归产生更好的可解释模型，因为它通常只有总变量数的子集。当存在许多分类变量时，建议选择Lasso而不是岭回归。
- en: In reality, neither ridge nor lasso is always better over the other. Lasso usually
    performs well with a small number of predictor variables that have substantial
    coefficients and the rest have very small coefficients. Ridge usually performs
    better when there are many predictors and almost all have substantial yet similar
    coefficient sizes.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，岭回归和Lasso并不总是一个比另一个更好。Lasso通常在具有实质性系数的少量预测变量和其余具有非常小系数的情况下表现良好。当存在许多预测变量且几乎所有预测变量具有实质性但相似的系数大小时，岭回归通常表现更好。
- en: Ridge is good for grouped selection and can also address multicollinearity problems.
    Lasso, on the other hand, cannot do grouped selection and tends to pick only one
    of the predictors. Also, if a group of predictors are highly correlated amongst
    themselves, Lasso tends to pick only one of them and shrink the others to zero.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归适用于分组选择，也可以解决多重共线性问题。另一方面，Lasso不能进行分组选择，倾向于只选择一个预测变量。此外，如果一组预测变量彼此高度相关，Lasso倾向于只选择其中一个，并将其他收缩为零。
- en: Elastic net regression
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 弹性网络回归
- en: 'When we add both L1 and L2 penalties to the loss function of least squares,
    it becomes elastic net regression, as shown next:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在最小二乘的损失函数中同时添加L1和L2惩罚时，它就成为了弹性网络回归，如下所示：
- en: '![Elastic net regression](img/image_06_036.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![弹性网络回归](img/image_06_036.jpg)'
- en: 'Following are the advantages of elastic net regression:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是弹性网络回归的优点：
- en: Enforces sparsity and helps remove least effective variables
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强制稀疏性并帮助去除最不有效的变量
- en: Encourages grouping effect
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鼓励分组效应
- en: Combines the strengths of both ridge and lasso
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合了岭回归和Lasso的优点
- en: 'The Naive version of elastic net regression incurs a double shrinkage problem
    which leads to increased bias and poorer prediction accuracy. To address this,
    one approach could be rescaling the estimated coefficients by multiplying (*1+
    λ2*) with them:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Naive版本的弹性网络回归存在双收缩问题，导致增加偏差和较差的预测准确性。为了解决这个问题，一种方法是通过将估计系数乘以(*1+ λ2*)来重新缩放它们：
- en: '**Scala**'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE12]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Python**:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE13]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Classification methods
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类方法
- en: If the response variable is qualitative/categorical (takes on categorical values
    such as gender, loan default, marital status, and such), then the problem can
    be called a classification problem regardless of the explanatory variables' type.
    There are various types of classification methods, but we will focus on logistic
    regression and Support Vector Machines in this section.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果响应变量是定性/分类的（取诸如性别、贷款违约、婚姻状况等分类值），那么问题可以被称为分类问题，而不管解释变量的类型。有各种类型的分类方法，但在本节中我们将专注于逻辑回归和支持向量机。
- en: 'Following are a few examples of some implications of classification methods:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些分类方法的一些含义的例子：
- en: A customer buys a product or does not buy it
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个顾客购买产品或不购买产品
- en: A person is diabetic or not diabetic
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个人是否患有糖尿病
- en: An individual applying for a loan would default or not
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个申请贷款的个人是否违约
- en: An e-mail receiver would read the e-mail or not
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个电子邮件接收者是否阅读电子邮件
- en: Logistic regression
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression measures the relation between the explanatory variables
    and the categorical response variable. We do not use linear regression for the
    categorical response variable because the response variable is not on a continuous
    scale and hence the error terms are not normally distributed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归衡量了解释变量和分类响应变量之间的关系。我们不使用线性回归来处理分类响应变量，因为响应变量不是在连续尺度上，因此误差项不是正态分布的。
- en: 'So logistic regression is a classification algorithm. Instead of modelling
    the response variable *Y* directly, logistic regression models the probability
    distribution of *P(Y*|*X)* that *Y* belongs to a particular category. The conditional
    distribution of (*Y*|*X*) is a Bernoulli distribution rather than a Gaussian distribution.
    The logistic regression equation can be represented as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，逻辑回归是一种分类算法。逻辑回归不直接对响应变量*Y*建模，而是对*Y*属于特定类别的概率分布*P(Y*|*X)*进行建模。条件分布(*Y*|*X*)是伯努利分布，而不是高斯分布。逻辑回归方程可以表示如下：
- en: '![Logistic regression](img/image_06_037.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image_06_037.jpg)'
- en: 'For a two class classification, the output of the model should be restricted
    to only one of the two classes (say either 0 or 1). Since logistic regression
    predicts probabilities and not classes directly, we use a logistic function (also
    known as the, *sigmoid function*) to restrict the output to a single class:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类，模型的输出应该限制为两个类中的一个（比如0或1）。由于逻辑回归预测的是概率而不是直接的类，我们使用逻辑函数（也称为*sigmoid函数*）来将输出限制为单个类：
- en: '![Logistic regression](img/image_06_038.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image_06_038.jpg)'
- en: 'Solving for the preceding equation gives us the following:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 解决上述方程得到以下结果：
- en: '![Logistic regression](img/Capture-2.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/Capture-2.jpg)'
- en: 'It can be further simplified as:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 可以进一步简化为：
- en: '![Logistic regression](img/image_06_040.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image_06_040.jpg)'
- en: The quantity on the left *P(X)/1-P(X)* is called the *odds*. The value of odds
    ranges from 0 to infinity. The values close to 0 indicate very less probability
    and the ones bigger in numbers indicate high probability. At times odds are used
    directly instead of probabilities, depending on the situation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 左边的数量 *P(X)/1-P(X)* 被称为 *赔率*。赔率的值范围从0到无穷大。接近0的值表示概率很低，而数字较大的值表示高概率。有时根据情况直接使用赔率而不是概率。
- en: 'If we take the log of the odds, it becomes log-odd or logit and can be shown
    as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取赔率的对数，它就变成了对数赔率或logit，可以表示如下：
- en: '![Logistic regression](img/image_06_041.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image_06_041.jpg)'
- en: You can see from the previous equation that logit is linearly related to *X*.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的方程可以看出，logit与 *X* 线性相关。
- en: In the situation where there are two classes, 1 and 0, then we predict *Y =
    1* if *p >= 0.5* and *Y = 0* when *p < 0.5*. So logistic regression is actually
    a linear classifier with decision boundary at *p = 0.5*. There could be business
    cases where *p* is just not set to 0.5 by default and you may have to figure out
    the right value using some mathematical techniques.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在有两个类别1和0的情况下，如果 *p >= 0.5* 则预测 *Y = 1*，如果 *p < 0.5* 则预测 *Y = 0*。因此，逻辑回归实际上是一个决策边界在
    *p = 0.5* 处的线性分类器。在某些业务案例中，*p* 并不是默认设置为0.5，您可能需要使用一些数学技术来找出正确的值。
- en: A method known as maximum likelihood is used to fit the model by computing the
    regression coefficients, and the algorithm can be a gradient descent like in a
    linear regression setting.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为最大似然的方法用于通过计算回归系数来拟合模型，算法可以是梯度下降，就像在线性回归设置中一样。
- en: 'In logistic regression, the loss function should address the misclassification
    rate. So, the loss function used for logistic regression is called *logistic loss*,
    as shown next:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，损失函数应该解决误分类率。因此，逻辑回归使用的损失函数称为 *逻辑损失*，如下所示：
- en: '![Logistic regression](img/image_06_042.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image_06_042.jpg)'
- en: Note
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that logistic regression is also prone to overfitting when you use higher
    order polynomial to better fit a model. To solve this, you can use regularization
    terms like you did in linear regression. As of this writing, Spark does not support
    regularized logistic regression so we will skip this part for now.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，当您使用高阶多项式更好地拟合模型时，逻辑回归也容易过拟合。为了解决这个问题，您可以像在线性回归中那样使用正则化项。截至目前，Spark不支持正则化的逻辑回归，因此我们暂时跳过这部分。 '
- en: Linear Support Vector Machines (SVM)
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性支持向量机（SVM）
- en: '**Support Vector Machines** (**SVM**) is a type of supervised machine learning
    algorithm and can be used for both classification and regression. However, it
    is more popular in addressing the classification problems, and since Spark offers
    it as an SVM classifier, we will limit our discussion to the classification setting
    only. When used as a classifier, unlike logistic regression, it is a non-probabilistic
    classifier.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）是一种监督式机器学习算法，可用于分类和回归。但是，它在解决分类问题方面更受欢迎，由于Spark将其作为SVM分类器提供，因此我们将仅限于讨论分类设置。在用作分类器时，与逻辑回归不同，它是一种非概率分类器。'
- en: The SVM has evolved from a simple classifier called the **maximal margin classifier**.
    Since the maximal margin classifier required that the classes be separable by
    a linear boundary, it could not be applied to many datasets. So it was extended
    to an improved version called the **support vector classifier** that could address
    the cases where the classes overlapped and there were no clear separation between
    the classes. The support vector classifier was further extended to what we call
    an SVM to accommodate the non-linear class boundaries. Let us discuss the evolution
    of the SVM step by step so we get a clear understanding of how it works.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: SVM已经从一个称为**最大间隔分类器**的简单分类器发展而来。由于最大间隔分类器要求类别可由线性边界分开，因此它无法应用于许多数据集。因此，它被扩展为一个称为**支持向量分类器**的改进版本，可以处理类别重叠且类别之间没有明显分离的情况。支持向量分类器进一步扩展为我们所说的SVM，以适应非线性类边界。让我们逐步讨论SVM的演变，以便更清楚地了解它的工作原理。
- en: 'If there are *p* dimensions (features) in a dataset, then we fit a hyperplane
    in that p-dimensional space whose equation can be defined as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集中有 *p* 个维度（特征），那么我们在p维空间中拟合一个超平面，其方程可以定义如下：
- en: '![Linear Support Vector Machines (SVM)](img/image_06_043.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![线性支持向量机（SVM）](img/image_06_043.jpg)'
- en: 'This hyperplane is called the separating hyperplane that forms the decision
    boundary. The result will be classified based on the result; if greater than 0,
    then on one side and if less than 0, then on the other side, as shown in the following
    figure:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这个超平面被称为形成决策边界的分离超平面。结果将根据结果进行分类；如果大于0，则在一侧，如果小于0，则在另一侧，如下图所示：
- en: '![Linear Support Vector Machines (SVM)](img/image_06_044.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![线性支持向量机（SVM）](img/image_06_044.jpg)'
- en: Observe in the preceding figure that there can be multiple hyperplanes (they
    can be infinite). There should be a reasonable way to choose the best hyperplane.
    This is where we select the maximal margin hyperplane. If you compute the perpendicular
    distance of all data points to the separating hyperplane, then the smallest distance
    would be called as the margin. So, for the maximal margin classifier, the hyperplane
    should have the highest margin.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的图表，可以有多个超平面（它们可以是无限的）。应该有一个合理的方法来选择最佳的超平面。这就是我们选择最大间隔超平面的地方。如果计算所有数据点到分离超平面的垂直距离，那么最小距离将被称为间隔。因此，对于最大间隔分类器，超平面应具有最大间隔。
- en: The training observations that are close yet equidistant from the separating
    hyperplane are known as support vectors. For any slight change in the support
    vectors, the hyperplane would also get reoriented. These support vectors actually
    define the margin. Now, what if the two classes under consideration are not separable?
    We would probably want a classifier that does not perfectly separate the two classes
    and has a softer boundary that allows some level of misclassification as well.
    This requirement led to the introduction of the support vector classifier (also
    known as the soft margin classifier).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 距离分隔超平面接近但等距离的训练观测被称为支持向量。对支持向量进行微小改变会导致超平面重新定位。这些支持向量实际上定义了边缘。那么，如果考虑的两个类别是不可分的呢？我们可能希望有一个分类器，它不完全分离两个类别，并且具有一个更柔和的边界，允许一定程度的误分类。这一要求导致了支持向量分类器的引入（也称为软边界分类器）。
- en: "Mathematically, it is the slack variable in the equation that allows for misclassification.\
    \ Also, there is a tuning parameter in the support vector classifier which should\
    \ be selected using cross-\x80\x93validation. This tuning parameter is the one\
    \ that trades off between bias and variance and should be handled with care. When\
    \ it is large, the margin is wider and includes many support vectors, and has\
    \ low variance and high bias. If it is small, then the margin will have fewer\
    \ support vectors and the classifier will have low bias but high variance."
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，正是方程中的松弛变量允许了误分类。此外，在支持向量分类器中有一个调节参数，应该使用交叉验证来选择。这个调节参数是在偏差和方差之间进行权衡的参数，应该小心处理。当它很大时，边缘会更宽，包含许多支持向量，具有低方差和高偏差。如果它很小，那么边缘将有更少的支持向量，分类器将具有低偏差但高方差。
- en: 'The loss function for the SVM can be represented as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的损失函数可以表示如下：
- en: '![Linear Support Vector Machines (SVM)](img/image_06_045.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![线性支持向量机（SVM）](img/image_06_045.jpg)'
- en: As of this writing, Spark supports only linear SVMs. By default, linear SVMs
    are trained with an L2 regularization. Spark also supports alternative L1 regularization.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，Spark仅支持线性SVM。默认情况下，线性SVM使用L2正则化进行训练。Spark还支持替代的L1正则化。
- en: 'So far so good! But how would the support vector classifier work when there
    is a non-linear boundary between the classes, as shown in the following image:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切顺利！但是当类别之间存在非线性边界时，支持向量分类器会如何工作呢，就像下面的图片所示的那样：
- en: '![Linear Support Vector Machines (SVM)](img/image_06_046.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![线性支持向量机（SVM）](img/image_06_046.jpg)'
- en: Any linear classifier, such as a support vector classifier, would perform very
    poorly in the preceding situation. If it draws a straight line through the data
    points, then the classes would not be separated properly. This is a case of non-linear
    class boundaries. A solution to this problem is the SVM. In other words, when
    a support vector classifier is fused with a non-linear kernel, it becomes an SVM.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 任何线性分类器，比如支持向量分类器，在前述情况下表现都非常糟糕。如果它通过数据点画一条直线，那么类别就无法正确分离。这是非线性类边界的情况。解决这个问题的方法是支持向量机（SVM）。换句话说，当支持向量分类器与非线性核融合时，它就成为了SVM。
- en: Similar to the way we introduced higher order polynomial terms in the regression
    equation to account for the non-linearity, something can also be done in the SVM
    context. The SVM uses something called kernels to take care of different kinds
    of non-linearity in the dataset; different kernels for different kinds of non-linearity.
    Kernel methods map the data into higher dimensional space as the data might get
    well separated if it does so. Also, it makes distinguishing different classes
    easier. Let us discuss a few of the important kernels so as to be able to select
    the right one.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在回归方程中引入高阶多项式项以解决非线性问题的方式类似，在SVM的情境下也可以做一些处理。SVM使用称为核的东西来处理数据集中不同类型的非线性；不同类型的非线性需要不同的核。核方法将数据映射到更高维的空间，这样做可能会使数据得到更好的分离。同时，它也使得区分不同类别变得更容易。让我们讨论一下一些重要的核，以便能够选择合适的核。
- en: Linear kernel
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性核
- en: This is one of the most basic type of kernels that allows us to pick out only
    lines or hyperplanes. It is equivalent to a support vector classifier. It cannot
    address the non-linearity if present in the dataset.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最基本类型的核之一，它允许我们只选择线或超平面。它相当于支持向量分类器。如果数据集中存在非线性，它就无法解决。
- en: Polynomial kernel
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多项式核
- en: This allows us to address some level of non-linearity to the extent of the order
    of polynomials. This works well when the training data is normalized. This kernel
    usually has more hyperparameters and therefore increases the complexity of the
    model.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们在多项式阶数的范围内解决一定程度的非线性。当训练数据被归一化时，这种方法效果很好。这个核通常有更多的超参数，因此增加了模型的复杂性。
- en: Radial Basis Function kernel
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 径向基函数核
- en: When you are not really sure of which kernel to use, **Radial Basis Function**
    (**RBF**) can be a good default choice. It allows you to pick out even circles
    or hyperspheres. Though this usually performs better than linear or polynomial
    kernel, it does not perform well when the number of features is huge.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当你不确定使用哪种核时，径向基函数（RBF）可能是一个不错的默认选择。它允许你选择甚至是圆或超球体。尽管这通常比线性或多项式核表现更好，但当特征数量很大时，它的表现就不那么好了。
- en: Sigmoid kernel
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmoid核
- en: The sigmoid kernel has its roots in neural networks. So, an SVM with a sigmoid
    kernel is equivalent to a neural network with a two layered perceptron.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid核源自神经网络。因此，具有Sigmoid核的SVM等效于具有两层感知器的神经网络。
- en: Training an SVM
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练SVM
- en: 'While training an SVM, the modeler has to take a number of decisions:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练SVM时，建模者需要做出一些决策：
- en: How to pre-process the data (transformation and scaling). The categorical variables
    should be converted to numeric ones by dummifying them. Also, scaling the numeric
    values is needed (either 0 to 1 or -1 to +1).
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何预处理数据（转换和缩放）。分类变量应通过虚拟化转换为数值变量。此外，需要对数值进行缩放（0到1或-1到+1）。
- en: "Which kernel to use (check using cross-\x80\x93validation if you are unable\
    \ to visualize the data and/ or conclude on it)."
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用哪种核（如果无法可视化数据和/或对其进行结论，则使用交叉验证进行检查）。
- en: "What parameters to set for the SVM: penalty parameter and the kernel parameter\
    \ (find using cross-\x80\x93validation or grid search)"
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM的参数设置：惩罚参数和核参数（使用交叉验证或网格搜索进行查找）
- en: If needed, you can use an entropy based feature selection to include only the
    important features in your model.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可以使用基于熵的特征选择来在模型中仅包括重要特征。
- en: '**Scala**:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE14]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '`mllib` has already entered maintenance mode and SVM is still not available
    under ml so only Scala code is provided for illustration.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`mllib`已经进入维护模式，SVM在ml下仍不可用，因此仅提供Scala代码以供说明。'
- en: Decision trees
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: A decision tree is a non-parametric supervised learning algorithm which can
    be used for both classification and regression. Decision trees are like inverted
    trees with the root node at the top and leaf nodes forming downwards. There are
    different algorithms to split the dataset into branch-like segments. Each leaf
    node is assigned to a class that represents the most appropriate target values.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种非参数的监督学习算法，可用于分类和回归。决策树就像倒置的树，根节点在顶部，叶节点向下形成。有不同的算法将数据集分割成类似分支的段。每个叶节点分配给代表最合适目标值的类。
- en: Decision trees do not require any scaling or transformations of the dataset
    and work as the data is. They can handle both categorical and continuous features,
    and also address non-linearity in the dataset. At its core, a decision tree is
    a greedy algorithm (it considers the best split at the moment and does not take
    into consideration the future scenarios) that performs a recursive binary partitioning
    of the feature space. Splitting is done based on information gain at each node
    because information gain measures how well a given attribute separates the training
    examples as per the target class or value. The first split happens for the feature
    that generates maximum information gain and becomes the root node.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树不需要对数据集进行任何缩放或转换，并且可以处理分类和连续特征，还可以处理数据集中的非线性。在其核心，决策树是一种贪婪算法（它考虑当前的最佳分割，并不考虑未来的情况），它对特征空间进行递归二元分区。分割是基于每个节点的信息增益进行的，因为信息增益衡量了给定属性如何根据目标类别或值分隔训练示例。第一个分割发生在生成最大信息增益的特征上，并成为根节点。
- en: The information gain at a node is the difference between the parent node impurity
    and the weighted sum of two child node impurities. To estimate information gain,
    Spark currently has two impurity measures for classification problems and one
    impurity measure for regression, as explained next.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的信息增益是父节点不纯度与两个子节点不纯度加权和之间的差异。为了估计信息增益，Spark目前针对分类问题有两种不纯度度量，针对回归问题有一种不纯度度量，如下所述。
- en: Impurity measures
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不纯度度量
- en: 'Impurity is a measure of homogeneity and the best criteria for recursive partitioning.
    By calculating the impurity, the best split candidate is decided. Most of the
    impurity measures are probability based:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 不纯度是同质性的度量，也是递归分区的最佳标准。通过计算不纯度，决定最佳的分割候选。大多数不纯度度量都是基于概率的：
- en: '*Probability of a class = number of observations of that class / total number
    of observations*'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*类的概率=该类的观察次数/总观察次数*'
- en: Let us spend some time on different types of important impurity measures that
    are supported by Spark.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一些时间来了解Spark支持的不同类型的重要不纯度度量。
- en: Gini Index
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基尼指数
- en: 'The Gini Index is mainly intended for the continuous attributes or features
    in a dataset. If not, it would assume that all the attributes and features are
    continuous. The split makes the child nodes more *purer* than the parent node.
    Gini tends to find the largest class - the class of response variable that has
    got the maximum observations. It can be defined as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼指数主要用于数据集中的连续属性或特征。如果不是，它将假定所有属性和特征都是连续的。分割使得子节点比父节点更*纯净*。基尼倾向于找到最大的类 - 响应变量的类别，其观察次数最多。可以定义如下：
- en: '![Gini Index](img/image_06_047.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![基尼指数](img/image_06_047.jpg)'
- en: If all observations of a response belong to a single class, then probability
    *P* of that class *j*, that is (*Pj*), will be 1 as there is only one class, and
    *(Pj)2* would also be 1\. This makes the Gini Index to be zero.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如果响应的所有观察属于单个类，则该类的概率*P*，即(*Pj*)，将为1，因为只有一个类，*(Pj)2*也将为1。这使得基尼指数为零。
- en: Entropy
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熵
- en: 'Entropy is mainly intended for the categorical attributes or features in a
    dataset. It can be defined as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 熵主要用于数据集中的分类属性或特征。可以定义如下：
- en: '![Entropy](img/image_06_048.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![熵](img/image_06_048.jpg)'
- en: If all observations of a response belong to a single class, then the probability
    of that class (*Pj*) will be 1, and *log(P)* would be zero. This makes the entropy
    to be zero.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如果响应的所有观察属于单个类，则该类的概率(*Pj*)将为1，*log(P)*将为零。这使得熵为零。
- en: 'The following graph depicts the probability of a fair coin toss:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了公平硬币抛掷的概率：
- en: '![Entropy](img/Capture-3.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![熵](img/Capture-3.jpg)'
- en: Just to explain the preceding graph, if you toss a fair coin, the probability
    of a head or a tail would be 0.5, so there will be maximum observations at a probability
    of 0.5.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 仅为了解释前面的图表，如果抛掷一个公平硬币，正面或反面的概率将为0.5，因此在概率为0.5时观察次数最多。
- en: If the data sample is completely homogeneous then the entropy will be zero,
    and if the sample can be equally divided into two, then the entropy will be one.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据样本完全同质，则熵将为零，如果样本可以平均分为两部分，则熵将为一。
- en: It is a little slower to compute than Gini because it has to compute the log
    as well.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 与Gini相比，计算速度稍慢，因为它还必须计算对数。
- en: Variance
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方差
- en: 'Unlike the Gini Index and entropy, variance is used for calculating information
    gain for regression problems. Variance can be defined as:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 与基尼指数和熵不同，方差用于计算回归问题的信息增益。方差可以定义为：
- en: '![Variance](img/image_06_050.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![方差](img/image_06_050.jpg)'
- en: Stopping rule
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停止规则
- en: 'The recursive tree construction is stopped at a node when one of the following
    conditions is met:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 当满足以下条件之一时，递归树构造停止在一个节点上：
- en: The node depth is equal to the `maxDepth` training parameter
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点深度等于`maxDepth`训练参数
- en: No split candidate leads to an information gain greater than `minInfoGain`
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有分裂候选者导致信息增益大于`minInfoGain`
- en: No split candidate produces child nodes, each of which have at least a `minInstancesPerNode`
    training instances
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有分裂候选者产生子节点，每个子节点至少有一个`minInstancesPerNode`训练实例
- en: Split candidates
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分裂候选者
- en: A dataset typically has a mixture of categorical and continuous features. How
    the features get split further into split candidates is something we should understand
    because we at times need some level of control over them to build a better model.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集通常包含混合的分类和连续特征。我们应该了解特征如何进一步分裂为分裂候选者，因为有时我们需要一定程度的控制来构建更好的模型。
- en: Categorical features
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类特征
- en: "For a categorical feature with *M* possible values (categories), one could\
    \ come up with *2(M-\x88\x921)-\x88\x921* split candidates. Whether for binary\
    \ classification or regression, the number of split candidates can be reduced\
    \ to *M-\x88\x921* by ordering the categorical feature values by the average label."
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: "对于具有*M*个可能值（类别）的分类特征，可以提出*2(M-\x88\x921)-\x88\x921*个分裂候选者。无论是二元分类还是回归，通过按平均标签对分类特征值进行排序，可以将分裂候选者的数量减少到*M-\x88\
    \x921*。"
- en: For example, consider a binary classification (0/1) problem with one categorical
    feature that has three categories A, B, and C, and their corresponding proportions
    of label-1 response variables are 0.2, 0.6, and 0.4 respectively. In this case,
    the categorical features can be ordered as A, C, B. So, the two split candidates
    (*M-1* = *3-1* = *2*) can be *A | (C, B)* and *A, (C | B)* where '*|'* denotes
    the split.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个具有三个类别A、B和C的分类特征的二元分类（0/1）问题，它们对应的标签-1响应变量的比例分别为0.2、0.6和0.4。在这种情况下，分类特征可以被排序为A、C、B。因此，两个分裂候选者（*M-1*
    = *3-1* = *2*）可以是*A | (C, B)*和*A, (C | B)*，其中“|”表示分裂。
- en: Continuous features
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续特征
- en: For a continuous feature variable, there can be a chance that no two values
    are the same (at least we can assume so). If there are *n* observations, then
    *n* split candidates might not be a good idea, especially in a big data setting.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续特征变量，可能存在没有两个相同值的情况（至少我们可以假设如此）。如果有*n*个观察结果，那么*n*个分裂候选者可能不是一个好主意，特别是在大数据环境中。
- en: In Spark, it is done by performing a quantile calculation on a sample of data,
    and binning the data accordingly. You can still have control over the maximum
    bins that you would like to allow, using the `maxBins` parameter. The maximum
    default value for `maxBins` is `32`.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，通过对数据样本进行分位数计算，并相应地对数据进行分箱来实现。您仍然可以通过使用`maxBins`参数来控制允许的最大箱数。`maxBins`的最大默认值为`32`。
- en: Advantages of decision trees
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树的优势
- en: They are simple to understand and interpret, so easy to explain to business
    users
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们易于理解和解释，因此易于向业务用户解释
- en: They works for both classification and regression
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们适用于分类和回归
- en: Both qualitative and quantitative data can be accommodated in constructing the
    decision trees
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建决策树时，可以容纳定性和定量数据
- en: Information gains in decision trees are biased in favor of the attributes with
    more levels.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树中的信息增益偏向于具有更多级别的属性。
- en: Disadvantages of decision trees
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树的缺点
- en: They do not work that greatly for effectively continuous outcome variables
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们对于连续结果变量的有效性不是很好
- en: Performance is poor when there are many classes and the dataset is small
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当类别很多且数据集很小时，性能较差。
- en: Axis parallel split reduces the accuracy
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轴平行分裂降低了准确性
- en: They suffer from high variance as they try to fit almost all data points
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们因试图拟合几乎所有数据点而遭受高方差
- en: Example
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 例子
- en: "Implementation -\x80\x93 wise there are no major differences between classification\
    \ and regression trees. Let us have a look at the practical implementation of\
    \ it on Spark."
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 实现方面，在分类和回归树之间没有主要区别。让我们在Spark上实际实现它。
- en: '**Scala:**'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE15]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Ensembles
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成
- en: As the name suggests, ensemble methods use multiple learning algorithms to obtain
    a more accurate model in terms of prediction accuracy. Usually these techniques
    require more computing power and make the model more complex, which makes it difficult
    to interpret. Let us discuss the various types of ensemble techniques available
    on Spark.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，集成方法使用多个学习算法来获得更准确的模型，通常这些技术需要更多的计算能力，并使模型更复杂，这使得难以解释。让我们讨论Spark上可用的各种类型的集成技术。
- en: Random forests
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: A random forest is an ensemble technique for the decision trees. Before we get
    to random forests, let us see how it has evolved. We know that decision trees
    usually have high variance issues and tend to overfit the model. To address this,
    a concept called *bagging* (also known as bootstrap aggregating) was introduced.
    For the decision trees, the idea was to take multiple training sets (bootstrapped
    training sets) from the dataset and create separate decision trees out of those,
    and then average them out for regression trees. For the classification trees,
    we can take the majority vote or the most commonly occurring class from all the
    trees. These trees grew deep and were not pruned at all. This definitely reduced
    the variance though the individual trees might have high variance.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是决策树的集成技术。在我们讨论随机森林之前，让我们看看它是如何发展的。我们知道决策树通常存在高方差问题，并且倾向于过度拟合模型。为了解决这个问题，引入了一个称为*bagging*（也称为自举聚合）的概念。对于决策树，想法是从数据集中获取多个训练集（自举训练集），并从中创建单独的决策树，然后对回归树进行平均。对于分类树，我们可以从所有树中获取多数投票或最常出现的类。这些树生长深入，并且根本没有被修剪。这确实减少了方差，尽管单个树可能具有高方差。
- en: One problem with the plain bagging approach was that for most of the bootstrapped
    training sets, the strong predictors took their positions at the top split which
    almost made the bagged trees look similar. This meant that the prediction also
    looked similar and if you averaged them out, then it did not reduce the variance
    to the extent expected. To address this, a technique was needed which would take
    a similar approach as that of bagged trees but eliminate the correlation amongst
    the trees, hence the *random forest*.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 纯粹的bagging方法的一个问题是，对于大多数自举训练集，强预测变量占据了顶部分裂的位置，这几乎使得袋装树看起来相似。这意味着预测也看起来相似，如果你对它们进行平均，那么它并没有像预期的那样减少方差。为了解决这个问题，需要一种技术，它将采用与袋装树类似的方法，但消除树之间的相关性，因此产生了*随机森林*。
- en: In this approach, you build bootstrapped training samples to create decision
    trees, but the only difference is that every time a split happens, a random sample
    of P predictors are chosen from a total of say K predictors. This is how a random
    forest injects randomness to this approach. As a thumb rule, we can take P as
    the square root of Q.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，您构建自举训练样本以创建决策树，但唯一的区别是每次发生分裂时，从总共K个预测变量中选择P个预测变量的随机样本。这就是随机森林向这种方法注入随机性的方式。作为一个经验法则，我们可以将P取为Q的平方根。
- en: 'Like in the case of bagging, in this approach you also average the predictions
    if your goal is regression and take the majority vote if the goal is classification.
    Spark provides some tuning parameters to tune this model, which are as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在bagging的情况下，如果你的目标是回归，你也会平均预测结果，如果目标是分类，你会采取多数投票。Spark提供了一些调整参数来调整这个模型，如下所示：
- en: '`numTrees`: You can specify the number of trees to consider in the random forest.
    If the numbers are high then the variance in prediction would be less, but the
    time required would be more.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numTrees`：您可以指定在随机森林中考虑的树的数量。如果数字很高，那么预测的方差会较小，但所需的时间会更长。'
- en: '`maxDepth`: You can specify the maximum depth of each tree. An increased depth
    makes the trees more powerful in terms of prediction accuracy. Though they tend
    to overfit the individual trees, the overall output is still good because we average
    the results anyway, which reduces the variance.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxDepth`：您可以指定每棵树的最大深度。增加深度会使树在预测准确度方面更加强大。尽管它们倾向于过度拟合单独的树，但总体输出仍然很好，因为我们无论如何都会平均结果，从而减少方差。'
- en: '`subsamplingRate`: This parameter is mainly used to speed up training. It is
    used to set the bootstrapped training sample size. A value less than 1.0 speeds
    up the performance.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsamplingRate`：这个参数主要用于加速训练。它用于设置自举训练样本的大小。小于1.0的值可以加快性能。'
- en: '`featureSubsetStrategy`: This parameter can also help speed up the execution.
    It is used to set the number of features to use as split candidates for every
    node. It should be set carefully as too low or too high a value can impact the
    accuracy of the model.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`featureSubsetStrategy`：这个参数也可以帮助加快执行。它用于设置每个节点用作分裂候选的特征数。它应该谨慎设置，因为太低或太高的值可能会影响模型的准确性。'
- en: Advantages of random forests
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林的优势
- en: They run faster as the execution happens in parallel
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们运行速度更快，因为执行是并行进行的
- en: They are less prone to overfitting
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不太容易过度拟合
- en: They are easy to tune
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们易于调整
- en: Prediction accuracy is more compared to trees or bagged trees
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与树或袋装树相比，预测准确度更高
- en: They work well even when the predictor variables are a mixture of categorical
    and continuous features, and do not require scaling
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们即使在预测变量是分类和连续特征的混合时也能很好地工作，并且不需要缩放
- en: Gradient-Boosted Trees
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: Like random forests, **Gradient-Boosted Trees** (**GBTs**) are also an ensemble
    of trees. They can be applied to both classification and regression problems.
    Unlike bagged trees or random forests, where trees are built in parallel on independent
    datasets and are independent of each other, GBTs are built sequentially. Each
    tree is grown using the result of the previously grown tree. Note that GBTs do
    not work on bootstrapped samples.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林一样，**梯度提升树**（**GBTs**）也是一种树的集成。它们可以应用于分类和回归问题。与袋装树或随机森林不同，树是顺序构建的。每棵树都是使用先前生长树的结果来生长的。请注意，GBT不适用于自举样本。
- en: On each iteration, GBTs use the current ensemble at hand to predict the labels
    for the training instances and compares them with true labels and estimates the
    error. The training instances with poor prediction accuracy get relabeled so that
    the decision trees get corrected in the next iteration based on the error rate
    for the previous mistakes.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，GBT使用当前集成来预测训练实例的标签，并将它们与真实标签进行比较，并估计错误。预测准确度较差的训练实例将被重新标记，以便基于先前错误的错误率在下一次迭代中纠正决策树。
- en: 'The mechanism behind finding the error rate and relabeling the instances is
    based on the loss function. GBTs are designed to reduce this loss function for
    every iteration. The following types of loss functions are supported by Spark:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 找到错误率并重新标记实例的机制是基于损失函数的。GBT旨在减少每次迭代的损失函数。Spark支持以下类型的损失函数：
- en: '**Log loss**: This is used for classification problems.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对数损失**：这用于分类问题。'
- en: '**Squared error (L2 loss)**: This is used for regression problems and is set
    by default. It is the summation of the squared differences between the actual
    and predicted output for all the observations. Outliers should be treated well
    for this loss function to perform well.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平方误差（L2损失）**：这用于回归问题，并且默认设置。它是所有观察值的实际值和预测输出之间的平方差异的总和。对于这种损失函数，异常值应该得到很好的处理才能表现良好。'
- en: '**Absolute error (L1 loss)**: This is also used for regression problems. It
    is the summation of the absolute differences between the actual and predicted
    output for all the observations. It is more robust to outliers compared to squared
    error.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**绝对误差（L1损失）**：这也用于回归问题。它是所有观察值的实际值和预测输出之间的绝对差异的总和。与平方误差相比，它对异常值更具鲁棒性。'
- en: 'Spark provides some tuning parameters to tune this model, which are as follows:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一些调整参数来调整此模型，如下所示：
- en: '`loss`: You can pass a loss function as discussed in the previous section,
    depending on the dataset you are dealing with and whether you intend to do classification
    or regression.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`：您可以根据前面讨论的数据集和您打算进行分类或回归的意图，传递一个损失函数。'
- en: '`numIterations`: Each iteration produces only one tree! If you set this very
    high, then the time needed for execution will also be high as the operation would
    be sequential and can also lead to overfitting. It should be carefully set for
    better performance and accuracy.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numIterations`：每次迭代只生成一棵树！如果将此设置得很高，那么执行所需的时间也会很长，因为操作将是顺序的，并且还可能导致过拟合。应该谨慎设置以获得更好的性能和准确性。'
- en: '`learningRate`: This is not really a tuning parameter. If the algorithm''s
    behavior is unstable then reducing this can help stabilize the model.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learningRate`：这实际上不是一个调整参数。如果算法的行为不稳定，那么减小这个值可以帮助稳定模型。'
- en: '`algo`: *Classification* or *regression* is set based on what you want.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`algo`：*分类*或*回归*是根据您的需求设置的。'
- en: GBTs can overfit the models with a greater number of trees, so Spark provides
    the `runWithValidation` method to prevent overfitting.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: GBT可能会过度拟合具有更多树的模型，因此Spark提供了`runWithValidation`方法来防止过拟合。
- en: Tip
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: As of this writing, GBTs on Spark do not yet support multiclass classification.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，Spark上的GBT尚不支持多类分类。
- en: "Let us look at an example to illustrate GBTs in action. The example dataset\
    \ contains average marks and attendance of twenty students. The data also contains\
    \ result as Pass or Fail, which follow a set of criteria. However, a couple of\
    \ students (ids 1009 and 1020) were \"\x80\x9Cgranted\" Pass status event though\
    \ they did not really qualify. Now our task is to check if the models pick up\
    \ these two students are not."
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个示例来说明GBT的工作原理。示例数据集包含二十名学生的平均分和出勤情况。数据还包含结果为通过或失败，遵循一组标准。然而，一对学生（id为1009和1020）被“授予”通过状态，尽管他们实际上并没有资格。现在我们的任务是检查模型是否选择了这两名学生。
- en: 'The Pass criteria are as follows:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标准如下：
- en: "Marks should be at least 40 and Attendance should be at least \"\x80\x9CEnough\""
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分数应至少为40，出勤应至少为“足够”
- en: "If Marks are between 40 and 60, then attendance should be \"\x80\x9CFull\"\
    \ to pass"
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分数在40到60之间，则出勤应为“全勤”才能通过
- en: The following example also emphasizes on reuse of pipeline stages across multiple
    models. So, we build a DecisionTree classifier first and then a GBT. We build
    two different pipelines that share stages.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例还强调了在多个模型中重复使用管道阶段。因此，我们首先构建一个DecisionTree分类器，然后构建一个GBT。我们构建了两个共享阶段的不同管道。
- en: '**Input**:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: '[PRE16]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Scala:**'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE17]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Python**:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python：**'
- en: '[PRE18]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Multilayer perceptron classifier
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器分类器
- en: A **multilayer perceptron classifier** (**MLPC**) is a feedforward artificial
    neural network with multiple layers of nodes connected to each other in a directed
    fashion. It uses a supervised learning technique called *backpropagation* for
    training the network.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器分类器**（**MLPC**）是一种前馈人工神经网络，具有多层节点以有向方式相互连接。它使用一种称为*反向传播*的监督学习技术来训练网络。'
- en: Nodes in the intermediary layer use the sigmoid function to restrict the output
    between 0 and 1, and the nodes in the output layer use the `softmax` function,
    which is a generalized version of the sigmoid function.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层的节点使用sigmoid函数将输出限制在0和1之间，输出层的节点使用`softmax`函数，这是sigmoid函数的广义版本。
- en: '**Scala**:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE19]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Clustering techniques
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类技术
- en: Clustering is an unsupervised learning technique where there is no response
    variable to supervise the model. The idea is to cluster the data points that have
    some level of similarity. Apart from exploratory data analysis, it is also used
    as a part of a supervised pipeline where classifiers or regressors can be built
    on the distinct clusters. There are a bunch of clustering techniques available.
    Let us look into a few important ones that are supported by Spark.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督学习技术，其中没有响应变量来监督模型。其思想是对具有某种相似性水平的数据点进行聚类。除了探索性数据分析外，它还可作为监督管道的一部分，其中可以在不同的簇上构建分类器或回归器。有许多聚类技术可用。让我们看一下由Spark支持的一些重要技术。
- en: K-means clustering
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means聚类
- en: K-means is one of the most common clustering techniques. The k-means problem
    is to find cluster centers that minimize the intra-class variance, that is, the
    sum of squared distances from each data point being clustered to its cluster center
    (the center that is closest to it). You have to specify in advance the number
    of clusters you want in the dataset.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: K-means是最常见的聚类技术之一。k-means问题是找到最小化簇内方差的簇中心，即，从要进行聚类的每个数据点到其簇中心（最接近它的中心）的平方距离之和。您必须预先指定数据集中要使用的簇的数量。
- en: 'Since it uses the Euclidian distance measure to find the differences between
    the data points, the features need to be scaled to a comparable unit prior to
    using k-means. The Euclidian distance can be better explained in a graphical way
    as follows:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它使用欧几里得距离度量来找到数据点之间的差异，因此在使用k-means之前，需要将特征缩放到可比较的单位。欧几里得距离可以用图形方式更好地解释如下：
- en: '![K-means clustering](img/image_06_051.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![K-means聚类](img/image_06_051.jpg)'
- en: 'Given a set of data points (*x1*, *x2*, ..., *xn*) with as many dimensions
    as the number of variables, k-means clustering aims to partition the n observations
    into k (less than *n*) sets where *S = {S1, S2, ..., Sk}*, so as to minimize the
    **within-cluster sum of squares** (**WCSS**). In other words, its objective is
    to find:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组数据点（*x1*，*x2*，...，*xn*），具有与变量数量相同的维度，k-means聚类旨在将n个观察结果分成k（小于*n*）个集合，其中*S
    = {S1，S2，...，Sk}*，以最小化**簇内平方和**（**WCSS**）。换句话说，它的目标是找到：
- en: '![K-means clustering](img/image_06_052.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![K-means聚类](img/image_06_052.jpg)'
- en: 'Spark requires the following parameters to be passed to this algorithm:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: Spark需要将以下参数传递给此算法：
- en: '`k`: This is the number of desired clusters.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k`：这是所需簇的数量。'
- en: '`maxIterations`: This is the maximum number of iterations to run.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxIterations`：这是运行的最大迭代次数。'
- en: '`initializationMode`: This specifies either random initialization or initialization
    via k-means||.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializationMode`：这指定随机初始化或通过k-means||初始化。'
- en: '`runs`: This is the number of times to run the k-means algorithm (k-means is
    not guaranteed to find a globally optimal solution, and when run multiple times
    on a given dataset, the algorithm returns the best clustering result).'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runs`：这是运行k-means算法的次数（k-means不能保证找到全局最优解，当在给定数据集上运行多次时，算法返回最佳的聚类结果）。'
- en: '`initializationSteps`: This determines the number of steps in the k-means||
    algorithm.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializationSteps`：这确定k-means||算法中的步数。'
- en: '`epsilon`: This determines the distance threshold within which we consider
    k-means to have converged.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon`：这确定我们认为k-means已经收敛的距离阈值。'
- en: '`initialModel`: This is an optional set of cluster centers used for initialization.
    If this parameter is supplied, only one run is performed.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialModel`：这是用于初始化的一组可选的聚类中心。如果提供了此参数，将只执行一次运行。'
- en: Disadvantages of k-means
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means的缺点
- en: It works only on the numeric features
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它只适用于数值特征
- en: It requires scaling before implementing the algorithm
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实施算法之前需要进行缩放
- en: It is susceptible to local optima (the solution to this is k-means++)
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它容易受到局部最优解的影响（解决方法是k-means++）
- en: Example
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: Let us run k-means clustering on the same students data.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在相同的学生数据上运行k-means聚类。
- en: '[PRE20]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Python**:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE21]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Summary
  id: totrans-443
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explained various machine learning algorithms, how they
    are implemented in the MLlib library and how they can be used with the pipeline API
    for a streamlined execution. The concepts were covered with Python and Scala code
    examples for a ready reference.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解释了各种机器学习算法，以及它们在MLlib库中的实现方式，以及如何在管道API中使用它们进行流畅的执行。这些概念通过Python和Scala代码示例进行了解释，以供参考。
- en: In the next chapter, we will discuss how Spark supports R programming language
    focusing on some of the algorithms and their executions similar to what we covered
    in this chapter.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论Spark如何支持R编程语言，重点关注一些算法及其执行，类似于我们在本章中涵盖的内容。
- en: References
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'Supported algorithms in MLlib:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib中支持的算法：
- en: '[http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)'
- en: '[http://spark.apache.org/docs/latest/mllib-decision-tree.html](http://spark.apache.org/docs/latest/mllib-decision-tree.html)'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/mllib-decision-tree.html](http://spark.apache.org/docs/latest/mllib-decision-tree.html)'
- en: 'Spark ML Programming Guide:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML编程指南：
- en: '[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)'
- en: 'Advanced datascience on spark.pdf from June 2015 summit slides:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年6月峰会幻灯片中的高级数据科学在spark.pdf中：
- en: '[https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html](https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html)'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html](https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html)'
- en: '[https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html](https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html)'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html](https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html)'
- en: '[https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)'
