- en: Using Spark SQL for Processing Structured and Semistructured Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL处理结构化和半结构化数据
- en: In this chapter, we will familiarize you with using Spark SQL with different
    types of data sources and data storage formats. Spark provides easy and standard
    structures (that is, RDDs and DataFrames/Datasets) to work with both structured
    and semistructured data. We include some of the data sources that are most commonly
    used in big data applications, such as, relational data, NoSQL databases, and
    files (CSV, JSON, Parquet, and Avro). Spark also allows you to define and use
    custom data sources. A series of hands-on exercises in this chapter will enable
    you to use Spark with different types of data sources and data formats.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何使用Spark SQL与不同类型的数据源和数据存储格式。Spark提供了易于使用的标准结构（即RDD和DataFrame/Datasets），可用于处理结构化和半结构化数据。我们包括一些在大数据应用中最常用的数据源，如关系数据、NoSQL数据库和文件（CSV、JSON、Parquet和Avro）。Spark还允许您定义和使用自定义数据源。本章中的一系列实践练习将使您能够使用Spark处理不同类型的数据源和数据格式。
- en: 'In this chapter, you shall learn the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下主题：
- en: Understanding data sources in Spark applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Spark应用中的数据源
- en: Using JDBC to work with relational databases
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用JDBC与关系数据库交互
- en: Using Spark with MongoDB (NoSQL database)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark与MongoDB（NoSQL数据库）
- en: Working with JSON data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理JSON数据
- en: Using Spark with Avro and Parquet Datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark与Avro和Parquet数据集
- en: Understanding data sources in Spark applications
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解Spark应用中的数据源
- en: Spark can connect to many different data sources, including files, and SQL and
    NoSQL databases. Some of the more popular data sources include files (CSV, JSON,
    Parquet, AVRO), MySQL, MongoDB, HBase, and Cassandra.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以连接到许多不同的数据源，包括文件、SQL和NoSQL数据库。一些更受欢迎的数据源包括文件（CSV、JSON、Parquet、AVRO）、MySQL、MongoDB、HBase和Cassandra。
- en: '![](img/00028.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.jpeg)'
- en: In addition, it can also connect to special purpose engines and data sources,
    such as ElasticSearch, Apache Kafka, and Redis. These engines enable specific
    functionality in Spark applications such as search, streaming, caching, and so
    on. For example, Redis enables deployment of cached machine learning models in
    high performance applications. We discuss more on Redis-based application deployment
    in [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c), *Spark
    SQL in Large-Scale Application Architectures*. Kafka is extremely popular in Spark
    streaming applications, and we will cover more details on Kafka-based streaming
    applications in [Chapter 5](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL in Streaming Applications,* and [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c), *Spark
    SQL in Large-Scale Application Architectures*. The DataSource API enables Spark
    connectivity to a wide variety of data sources including custom data sources.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还可以连接到专用引擎和数据源，如ElasticSearch、Apache Kafka和Redis。这些引擎可以在Spark应用中实现特定功能，如搜索、流处理、缓存等。例如，Redis可以在高性能应用中部署缓存的机器学习模型。我们将在[第12章](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c)中讨论更多关于基于Redis的应用部署的内容，即*大规模应用架构中的Spark
    SQL*。Kafka在Spark流处理应用中非常受欢迎，我们将在[第5章](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c)和[第12章](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c)中详细介绍基于Kafka的流处理应用，即*在流处理应用中使用Spark
    SQL*和*大规模应用架构中的Spark SQL*。DataSource API使Spark能够连接到各种数据源，包括自定义数据源。
- en: Refer to the Spark packages website [https://spark-packages.org/](https://spark-packages.org/) to
    work with various data sources, algorithms, and specialized Datasets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考Spark软件包网站[https://spark-packages.org/](https://spark-packages.org/)，以使用各种数据源、算法和专用数据集。
- en: In [Chapter 1](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c), *Getting
    Started with Spark SQL,*  we used CSV and JSON files on our filesystem as input
    data sources and used SQL to query them. However, using Spark SQL to query data
    residing in files is not a replacement for using databases. Initially, some people
    used HDFS as a data source because of the simplicity and the ease of using Spark
    SQL for querying such data. However, the execution performance can vary significantly
    based on the queries being executed and the nature of the workloads. Architects
    and developers need to understand which data stores to use in order to best meet
    their processing requirements. We discuss some high-level considerations for selecting
    Spark data sources below.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c)中，*开始使用Spark SQL*，我们使用文件系统上的CSV和JSON文件作为输入数据源，并使用SQL进行查询。但是，使用Spark
    SQL查询存储在文件中的数据并不是使用数据库的替代品。最初，一些人使用HDFS作为数据源，因为使用Spark SQL查询此类数据的简单性和便利性。然而，执行性能可能会根据执行的查询和工作负载的性质而有显著差异。架构师和开发人员需要了解使用哪些数据存储来最好地满足其处理需求。我们将在下面讨论选择Spark数据源的一些高级考虑因素。
- en: Selecting Spark data sources
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择Spark数据源
- en: Filesystems are a great place to dump large volumes of data and for supporting
    general purpose processing of large Datasets. Some of the benefits you will get
    by using files are inexpensive storage, flexible processing, and scale. The decision
    to store large-scale data in files is usually driven by the prohibitive costs
    of storing the same on commercial databases. Additionally, file storage is also
    preferred when the nature of the data does not benefit from typical database optimizations,
    for example, unstructured data. Additionally, workloads, such as machine learning
    applications, with iterative in-memory processing requirements and distributed
    algorithms may be better suited to run on distributed file systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统是存储大量数据和支持大型数据集通用处理的理想场所。使用文件的一些好处包括廉价的存储、灵活的处理和可扩展性。将大规模数据存储在文件中的决定通常是由商业数据库存储同样数据的成本限制所驱动的。此外，当数据的性质不适合典型的数据库优化时，例如非结构化数据时，通常也会优先选择文件存储。此外，具有迭代内存处理需求和分布式算法的工作负载，例如机器学习应用，可能更适合在分布式文件系统上运行。
- en: The types of data you would typically store on filesystems are archival data,
    unstructured data, massive social media and other web-scale Datasets, and backup
    copies of primary data stores. The types of workloads best supported on files
    are batch workloads, exploratory data analysis, multistage processing pipelines,
    and iterative workloads. Popular use cases for using files include ETL pipelines,
    splicing data across varied data sources, such as log files, CSV, Parquet, zipped
    file formats, and so on. In addition, you can choose to store the same data in
    multiple formats optimized for your specific processing requirements.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在文件系统上存储的数据类型包括归档数据、非结构化数据、大规模社交媒体和其他网络规模数据集，以及主要数据存储的备份副本。最适合在文件上支持的工作负载类型包括批处理工作负载、探索性数据分析、多阶段处理管道和迭代工作负载。使用文件的热门案例包括ETL管道、跨多种数据源拼接数据，如日志文件、CSV、Parquet、压缩文件格式等。此外，您可以选择以针对特定处理需求进行优化的多种格式存储相同的数据。
- en: What's not so great about Spark connected to a filesystem are use cases involving
    frequent random accesses, frequent inserts, frequent/incremental updates, and
    reporting or search operations under heavy load conditions across many users.
    These use cases are discussed in more detail as we move on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与Spark连接到文件系统不太适合的是频繁的随机访问、频繁的插入、频繁/增量更新以及在多用户情况下承受重负载条件下的报告或搜索操作。随着我们的深入，将更详细地讨论这些使用案例。
- en: Queries selecting a small subset of records from your distributed storage are
    supported in Spark but are not very efficient, because it would typically require
    Spark to go through all your files to find your result row(s). This may be acceptable
    for data exploration tasks but not for sustained processing loads from several
    concurrent users. If you need to frequently and randomly access your data, using
    a database can be a more effective solution. Making the data available to your
    users using a traditional SQL database and creating indexes on the key columns
    can better support this use case. Alternatively, key-value NoSQL stores can also
    retrieve the value of a key a lot more efficiently.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中支持从分布式存储中选择少量记录的查询，但效率不高，因为通常需要Spark浏览所有文件以找到结果行。这对于数据探索任务可能是可以接受的，但对于来自多个并发用户的持续处理负载则不行。如果您需要频繁和随机地访问数据，使用数据库可能是更有效的解决方案。使用传统的SQL数据库使数据可用于用户，并在关键列上创建索引可以更好地支持这种使用案例。另外，键值NoSQL存储也可以更有效地检索键的值。
- en: As each insert creates a new file, the inserts are reasonably fast however querying
    becomes low as the Spark jobs will need to open all these files and read from
    them to support queries. Again, a database used to support frequent inserts may
    be a much better solution. Alternatively, you can also routinely compact your
    Spark SQL table files to reduce the overall number of files. Use the `Select *`
    and `coalesce` DataFrame commands to write the data out from a DataFrame created
    from multiple input files to a single / combined output file.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每次插入都会创建一个新文件，插入速度相当快，但查询速度较慢，因为Spark作业需要打开所有这些文件并从中读取以支持查询。同样，用于支持频繁插入的数据库可能是更好的解决方案。另外，您还可以定期压缩Spark
    SQL表文件，以减少总文件数量。使用`Select *`和`coalesce` DataFrame命令，将从多个输入文件创建的DataFrame中的数据写入单个/组合输出文件。
- en: Other operations and use cases, such as frequent/incremental updates, reporting,
    and searching are better handled using databases or specialized engines. Files
    are not optimized for updating random rows. However, databases are ideal for executing
    efficient update operations. You can connect Spark to HDFS and use BI tools, such
    as Tableau, but it is better to dump the data to a database for serving concurrent
    users under load. Typically, it is better to use Spark to read the data, perform
    aggregations, and so on, and then write the results out to a database that serves
    end users. In the search use case, Spark will need to go through each row to find
    and return the search results, thereby impacting performance. In this case, using
    the specialized engines such as ElasticSearch and Apache Solr may be a better
    solution than using Spark.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其他操作和使用案例，如频繁/增量更新、报告和搜索，最好使用数据库或专门的引擎来处理。文件不适合更新随机行。然而，数据库非常适合执行高效的更新操作。您可以将Spark连接到HDFS并使用BI工具，如Tableau，但最好将数据转储到数据库以为承受负载的并发用户提供服务。通常，最好使用Spark读取数据，执行聚合等操作，然后将结果写入为最终用户提供服务的数据库。在搜索使用案例中，Spark将需要浏览每一行以查找并返回搜索结果，从而影响性能。在这种情况下，使用专门的引擎，如ElasticSearch和Apache
    Solr，可能比使用Spark更好。
- en: In cases where the data is heavily skewed, or for executing faster joins on
    a cluster, we can use cluster by or bucketing techniques to improve performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据严重倾斜的情况下，或者在集群上执行更快的连接时，我们可以使用集群或分桶技术来提高性能。
- en: Using Spark with relational databases
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark与关系数据库
- en: There is a huge debate on whether relational databases fit into big data processing
    scenarios. However, it's undeniable that vast quantities of structured data in
    enterprises live in such databases, and organizations rely heavily on the existing
    RDBMSs for their critical business transactions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 关于关系数据库是否适合大数据处理场景存在着巨大的争论。然而，不可否认的是，企业中大量结构化数据存储在这些数据库中，并且组织在关键业务交易中严重依赖现有的关系数据库管理系统。
- en: A vast majority of developers are most comfortable working with relational databases
    and the rich set of tools available from leading vendors. Increasingly, cloud
    service providers, such as Amazon AWS, have made administration, replication,
    and scaling simple enough for many organizations to transition their large relational
    databases to the cloud.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 绝大多数开发人员最喜欢使用关系数据库和主要供应商提供的丰富工具集。越来越多的云服务提供商，如亚马逊AWS，已经简化了许多组织将其大型关系数据库转移到云端的管理、复制和扩展。
- en: 'Some good big data use cases for relational databases include the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 关系数据库的一些很好的大数据使用案例包括以下内容：
- en: Complex OLTP transactions
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的OLTP事务
- en: Applications or features that need ACID compliance
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要ACID合规性的应用程序或功能
- en: Support for standard SQL
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持标准SQL
- en: Real-time ad hoc query functionality
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时自发查询功能
- en: Systems implementing many complex relationships
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施许多复杂关系的系统
- en: For an excellent coverage of NoSQL and relational use cases, refer to the blog
    titled What the heck are you actually using NoSQL for? at [http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-for.html](http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-for.html).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有关NoSQL和关系使用情况的出色覆盖，请参阅标题为“你到底在使用NoSQL做什么？”的博客[http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-for.html](http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-for.html)。
- en: 'In Spark, it is easy to work with relational data and combine it with other
    data sources in different forms and formats:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，很容易处理关系数据并将其与不同形式和格式的其他数据源结合起来：
- en: '![](img/00029.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00029.jpeg)'
- en: As an example that illustrates using Spark with a MySQL database, we will implement
    a use-case in which we split the data between HDFS and MySQL. The MySQL database
    will be targeted to support interactive queries from concurrent users, while the
    data on HDFS will be targeted for batch processing, running machine learning applications,
    and for making the data available to BI tools. In this example, we assume that
    the interactive queries are against the current month's data only. Hence, we will
    retain only the current month's data in MySQL and write out the rest of data to
    HDFS (in JSON format).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用Spark与MySQL数据库的示例，我们将实现一个用例，其中我们将数据在HDFS和MySQL之间进行分割。MySQL数据库将用于支持来自并发用户的交互式查询，而HDFS上的数据将用于批处理、运行机器学习应用程序以及向BI工具提供数据。在此示例中，我们假设交互式查询仅针对当前月份的数据。因此，我们将只保留当前月份的数据在MySQL中，并将其余数据写入HDFS（以JSON格式）。
- en: 'The implementation steps, we will follow are:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循的实施步骤如下：
- en: Create MySQL database.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建MySQL数据库。
- en: Define a table.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个表。
- en: Create a user ID and grant privileges.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建用户ID并授予权限。
- en: Start Spark shell with MySQL JDBC driver.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MySQL JDBC驱动程序启动Spark shell。
- en: Create a RDD from input data file, separate the header, define a schema, and
    create a DataFrame.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入数据文件创建一个RDD，分离标题，定义模式并创建一个DataFrame。
- en: Create a new column for timestamps.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为时间戳创建一个新列。
- en: Separate the data into two DataFrames based on the timestamp value (data for
    the current month and rest of data previous months).
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据时间戳值（当前月份数据和以前月份的其余数据）将数据分成两个DataFrame。
- en: Drop the original invoiceDate column and then rename the timestamp column to
    invoiceDate.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除原始invoiceDate列，然后将时间戳列重命名为invoiceDate。
- en: Write out the DataFrame containing current month data to the MySQL table.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将包含当前月份数据的DataFrame写入MySQL表中。
- en: Write out the DataFrame containing data (other than current month data) to HDFS
    (in JSON format).
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将包含数据（除当前月份数据之外的数据）的DataFrame写入HDFS（以JSON格式）。
- en: If you do not have MySQL already installed and available, you can download it
    from [https://www.mysql.com/downloads/](https://www.mysql.com/downloads/). Follow
    the installation instructions for your specific OS to install the database. Also,
    download the JDBC connector available on the same website.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未安装和可用MySQL，可以从[https://www.mysql.com/downloads/](https://www.mysql.com/downloads/)下载。按照特定操作系统的安装说明安装数据库。此外，从同一网站下载可用的JDBC连接器。
- en: 'After you have your MySQL database server up and running, fire up the MySQL
    shell. In the following steps, we will create a new database and define a transactions
    table. We use a transnational dataset that contains all the transactions occurring
    between 01/12/2010 and 09/12/2011 for a UK-based and registered nonstore online
    retail. The dataset has been contributed by Dr Daqing Chen, Director: Public Analytics
    group, School of Engineering, London South Bank University and is available at
    [https://archive.ics.uci.edu/ml/datasets/Online+Retail](https://archive.ics.uci.edu/ml/datasets/Online+Retail).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的MySQL数据库服务器运行起来后，启动MySQL shell。在接下来的步骤中，我们将创建一个新数据库并定义一个交易表。我们使用一个包含所有发生在2010年12月1日至2011年12月9日之间的交易的交易数据集，这是一个基于英国注册的非实体在线零售的数据集。该数据集由伦敦南岸大学工程学院公共分析小组主任Dr
    Daqing Chen贡献，并可在[https://archive.ics.uci.edu/ml/datasets/Online+Retail](https://archive.ics.uci.edu/ml/datasets/Online+Retail)上找到。
- en: 'You should see a screen similar to the following when you start the MySQL shell:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启动MySQL shell时，应该看到类似以下的屏幕：
- en: '![](img/00030.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00030.jpeg)'
- en: 'Create a new database called `retailDB` to store our customer transactions
    data:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`retailDB`的新数据库来存储我们的客户交易数据：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we define a transactions table with `transactionID` as the primary key.
    In a production scenario, you would also create indexes on other fields, such
    as `CustomerID`, to support queries more efficiently:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们使用`transactionID`作为主键定义了一个交易表。在生产场景中，您还将在其他字段上创建索引，例如`CustomerID`，以更有效地支持查询：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we verify the transactions table schema using the `describe` command
    to ensure that it is exactly how we want it:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用`describe`命令验证交易表模式，以确保它完全符合我们的要求：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/00031.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00031.jpeg)'
- en: Create a user ID `retaildbuser` and grant all privileges to it. We will use
    this user from our Spark shell for connecting and executing our queries.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`retaildbuser`的用户ID并授予其所有权限。我们将从我们的Spark shell中使用此用户进行连接和执行查询。
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Start the Spark shell with the classpath containing the path to the MySQL JDBC
    driver, as follows:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动包含MySQL JDBC驱动程序路径的Spark shell，如下所示：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create a `RDD` containing all the rows from our downloaded Dataset:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含我们下载的数据集中所有行的`RDD`：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Separate the header from the rest of the data:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标题与其余数据分开：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the fields and define a schema for our data records, as follows:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义字段并为我们的数据记录定义模式，如下所示：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create an `RDD` of Row objects, create a DataFrame using the previously created
    schema:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含Row对象的`RDD`，使用先前创建的模式创建一个DataFrame：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Add a column called `ts` (a timestamp column) to the DataFrame, as follows:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向DataFrame添加名为`ts`（时间戳列）的列，如下所示：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/00032.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00032.jpeg)'
- en: 'Create a table object and execute appropriate SQLs to separate the table data
    into two DataFrames based on the timestamps:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个表对象，并执行适当的SQL将表数据基于时间戳分成两个DataFrame：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Drop the `invoiceDate` column in our new DataFrame.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除我们新DataFrame中的`invoiceDate`列。
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Rename the `ts` column to `invoiceDate`, as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`ts`列重命名为`invoiceDate`，如下所示：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/00033.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00033.jpeg)'
- en: 'Create a variable to point to the database URL. Additionally, create a `Properties` object
    to hold the user ID and password required for connecting to `retailDB.` Next,
    connect to the MySQL database and insert the records from the "current month"
    into the transactions table:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个指向数据库URL的变量。另外，创建一个`Properties`对象来保存连接到`retailDB`所需的用户ID和密码。接下来，连接到MySQL数据库，并将“当前月份”的记录插入到transactions表中：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Select the columns of interest from the DataFrame (containing data other than
    for the current month), and write them out to the HDFS filesystem in JSON format:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从DataFrame中选择感兴趣的列（包含当前月份以外的数据），并以JSON格式将其写入HDFS文件系统：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Using Spark with MongoDB (NoSQL database)
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark处理MongoDB（NoSQL数据库）
- en: In this section, we will use Spark with one of the most popular NoSQL databases
    - MongoDB. MongoDB is a distributed document database that stores data in JSON-like
    format. Unlike the rigid schemas in relational databases, the data structure in
    MongoDB is a lot more flexible and the stored documents can have arbitrary fields.
    This flexibility combined with high availability and scalability features make
    it a good choice for storing data in many applications. It is also free and open-source
    software.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Spark与最流行的NoSQL数据库之一 - MongoDB。 MongoDB是一个分布式文档数据库，以类似JSON的格式存储数据。与关系数据库中的严格模式不同，MongoDB中的数据结构更加灵活，存储的文档可以具有任意字段。这种灵活性与高可用性和可扩展性功能结合在一起，使其成为许多应用程序中存储数据的良好选择。它还是免费和开源软件。
- en: If you do not have MongoDB already installed and available, then you can download
    it from [https://www.mongodb.org/downloads](https://www.mongodb.com/download-center#community).
    Follow the installation instructions for your specific OS to install the database.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未安装和可用MongoDB，则可以从[https://www.mongodb.org/downloads](https://www.mongodb.com/download-center#community)下载。按照特定操作系统的安装说明安装数据库。
- en: The New York City schools directory dataset for this example has been taken
    from the New York City Open Data website and can be downloaded from [https://nycplatform.socrata.com/data?browseSearch=&scope=&agency=&cat=education&type=datasets](https://nycplatform.socrata.com/data?browseSearch=&scope=&agency=&cat=education&type=datasets).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的纽约市学校目录数据集来自纽约市开放数据网站，可从[https://nycplatform.socrata.com/data?browseSearch=&scope=&agency=&cat=education&type=datasets](https://nycplatform.socrata.com/data?browseSearch=&scope=&agency=&cat=education&type=datasets)下载。
- en: After you have your MongoDB database server up and running, fire up the MongoDB
    shell. In the following steps, we will create a new database, define a collection,
    and insert New York City school's data using the MongoDB import utility from the
    command line.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的MongoDB数据库服务器运行后，启动MongoDB shell。在接下来的步骤中，我们将创建一个新数据库，定义一个集合，并使用命令行中的MongoDB导入实用程序插入纽约市学校的数据。
- en: 'You should see a screen similar to the following when you start the MongoDB
    shell:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启动MongoDB shell时，应该看到类似以下的屏幕：
- en: '![](img/00034.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00034.jpeg)'
- en: Next, execute the use `<DATABASE>` command to select an existing database or
    create a new one, if it does not exist.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，执行`use <DATABASE>`命令选择现有数据库或创建一个新数据库（如果不存在）。
- en: If you make a mistake while creating a new collection, you can use the `db.dropDatabase()`
    and/or `db.collection.drop()` commands to delete the dababase and/or the collection,
    respectively, and then recreate it with the required changes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在创建新集合时出现错误，可以使用`db.dropDatabase()`和/或`db.collection.drop()`命令分别删除数据库和/或集合，然后根据需要重新创建它。
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `mongoimport` utility needs to be executed from the command prompt (and
    not in the `mongodb` shell):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`mongoimport`实用程序需要从命令提示符（而不是`mongodb` shell）中执行：'
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can list the imported collection and print a record to validate the import operation,
    as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以列出导入的集合并打印记录以验证导入操作，如下所示：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00035.jpeg)![](img/00036.jpeg)![](img/00037.jpeg)![](img/00038.jpeg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00035.jpeg)![](img/00036.jpeg)![](img/00037.jpeg)![](img/00038.jpeg)'
- en: You can download the `mongo-spark-connector jar` for Spark 2.2 (`mongo-spark-connector_2.11-2.2.0-assembly.jar`)
    from [http://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.11/2.2.0/](http://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.11/2.2.0/).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[http://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.11/2.2.0/](http://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.11/2.2.0/)下载适用于Spark
    2.2的`mongo-spark-connector jar`（`mongo-spark-connector_2.11-2.2.0-assembly.jar`）。
- en: 'Next, start the Spark shell with the `mongo-spark-connector_2.11-2.2.0-assembly.jar` file
    specified on the command line:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用命令行指定`mongo-spark-connector_2.11-2.2.0-assembly.jar`文件启动Spark shell：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we define the URIs for `read` and `write` operations from Spark:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了从Spark进行`read`和`write`操作的URI：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define a `case` class for the school record, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个学校记录的`case`类，如下所示：
- en: '![](img/00039.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00039.jpeg)'
- en: Next, you can create a DataFrame from our collection and display a record from
    our newly created DataFrame.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以从我们的集合创建一个DataFrame，并显示新创建的DataFrame中的记录。
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/00040.jpeg)![](img/00041.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00040.jpeg)![](img/00041.jpeg)'
- en: 'Note: The following sections will be updated with the latest versions of the
    connector packages later.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：以下各节将在稍后使用最新版本的连接器包进行更新。
- en: In the next several sections, we describe using Spark with several popular big
    data file formats.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将描述使用Spark处理几种流行的大数据文件格式。
- en: Using Spark with JSON data
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark处理JSON数据
- en: JSON is a simple, flexible, and compact format used extensively as a data-interchange
    format in web services. Spark's support for JSON is great. There is no need for
    defining the schema for the JSON data, as the schema is automatically inferred.
    In addition, Spark greatly simplifies the query syntax required to access fields
    in complex JSON data structures. We will present detailed examples of JSON data
    in [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c), *Spark
    SQL in Large-Scale Application Architectures*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是一种简单、灵活和紧凑的格式，在Web服务中广泛用作数据交换格式。Spark对JSON的支持非常好。不需要为JSON数据定义模式，因为模式会自动推断。此外，Spark极大地简化了访问复杂JSON数据结构中字段所需的查询语法。我们将在[第12章](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c)《大规模应用架构中的Spark
    SQL》中详细介绍JSON数据的示例。
- en: 'The dataset for this example contains approximately 1.69 million Amazon reviews
    for the electronics category, and can be downloaded from: [http://jmcauley.ucsd.edu/data/amazon/](http://jmcauley.ucsd.edu/data/amazon/).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的数据集包含大约169万条电子产品类别的亚马逊评论，可从以下网址下载：[http://jmcauley.ucsd.edu/data/amazon/](http://jmcauley.ucsd.edu/data/amazon/)。
- en: 'We can directly read a JSON dataset to create Spark SQL DataFrame. We will
    read in a sample set of order records from a JSON file:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接读取JSON数据集以创建Spark SQL DataFrame。我们将从JSON文件中读取一组订单记录的示例集：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can print the schema of the newly created DataFrame to verify the fields
    and their characteristics using the `printSchema` method.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`printSchema`方法打印新创建的DataFrame的模式，以验证字段及其特性。
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/00042.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00042.jpeg)'
- en: 'Once, the JSON Dataset is converted to a Spark SQL DataFrame, you can work
    with it extensively in a standard way. Next, we will execute an SQL statement
    to select certain columns from our orders that are received from customers in
    a specific age bracket:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦JSON数据集转换为Spark SQL DataFrame，您可以以标准方式进行大量操作。接下来，我们将执行SQL语句，从特定年龄段的客户接收的订单中选择特定列：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Display the results of the SQL execution (stored in another DataFrame) using
    the `show` method, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`show`方法显示SQL执行结果（存储在另一个DataFrame中），如下所示：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/00043.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00043.jpeg)'
- en: 'We can access the array elements of the `helpful` column in the `reviewDF`
    DataFrame (using DSL) as shown:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用DSL访问`reviewDF` DataFrame中`helpful`列的数组元素，如下所示：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/00044.jpeg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00044.jpeg)'
- en: An example of writing out a DataFrame as a JSON file was presented in an earlier
    section where we selected the columns of interest from the DataFrame (containing
    data other than for the current month), and wrote them out to the HDFS filesystem
    in JSON format.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们演示了将DataFrame写出为JSON文件的示例，其中我们从DataFrame中选择了感兴趣的列（包含当前月份之外的数据），并将其写出为JSON格式到HDFS文件系统。
- en: Using Spark with Avro files
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Avro文件的Spark
- en: Avro is a very popular data serialization system that provides a compact and
    fast binary data format. Avro files are self-describing because the schema is
    stored along with the data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Avro是一个非常流行的数据序列化系统，提供了紧凑和快速的二进制数据格式。Avro文件是自描述的，因为模式与数据一起存储。
- en: You can download `spark-avro connector` JAR from [https://mvnrepository.com/artifact/com.databricks/spark-avro_2.11/3.2.0](https://mvnrepository.com/artifact/com.databricks/spark-avro_2.11/3.2.0).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[https://mvnrepository.com/artifact/com.databricks/spark-avro_2.11/3.2.0](https://mvnrepository.com/artifact/com.databricks/spark-avro_2.11/3.2.0)下载`spark-avro
    connector` JAR。
- en: We will switch to Spark 2.1 for this section. At the time of writing this book
    due to a documented bug in the `spark-avro connector` library, we are getting
    exceptions while writing Avro files (using `spark-avro connector 3.2`) with Spark
    2.2.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节切换到Spark 2.1。在撰写本书时，由于`spark-avro connector`库中的已记录的错误，我们在使用`spark-avro
    connector 3.2`与Spark 2.2时遇到异常。
- en: 'Start Spark shell with the spark-avro JAR included in the session:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 启动包含spark-avro JAR的Spark shell会话：
- en: '[PRE26]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We will use the JSON file from the previous section containing the Amazon reviews
    data to create the `Avro` file. Create a DataFrame from the input JSON file and
    display the number of records:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用前一节中包含亚马逊评论数据的JSON文件来创建`Avro`文件。从输入JSON文件创建一个DataFrame，并显示记录数：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we filter all the reviews with an overall rating of less than `3`, `coalesce`
    the output to a single file, and write out the resulting DataFrame to an `Avro`
    file:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们过滤所有评分低于`3`的评论，将输出合并为单个文件，并将结果DataFrame写出为`Avro`文件：
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we show how to read an `Avro` file by creating a DataFrame from the `Avro`
    file created in the previous step and display the number of records in it:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示如何通过从上一步创建的`Avro`文件创建一个DataFrame来读取`Avro`文件，并显示其中的记录数：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we select a few columns and display five records from the results DataFrame
    by specifying `show(5)`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择几列，并通过指定`show(5)`显示结果DataFrame的前五条记录：
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](img/00045.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00045.jpeg)'
- en: 'Next, we specify compression options for `Avro` files by setting the Spark
    session configuration values:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过设置Spark会话配置值为`Avro`文件指定压缩选项：
- en: '[PRE31]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, when we write the DataFrame, the `Avro` file is stored in a compressed
    format:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们写入DataFrame时，`Avro`文件以压缩格式存储：
- en: '[PRE32]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You can also write out the DataFrame partitioned by a specific column. Here,
    we partition based on the `overall` column (containing `values < 3` in each row):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以按特定列对DataFrame进行分区。在这里，我们基于`overall`列（每行包含`值<3`）进行分区：
- en: '[PRE33]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The screenshot of the Avro files from this session are shown here. Notice the
    sizes of the compressed version (67 MB) versus the original file (97.4 MB) . Additionally,
    notice the two separate directories created for the partitioned (by `overall`
    values) `Avro` files.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此会话中Avro文件的屏幕截图显示在此处。请注意压缩版本（67 MB）与原始文件（97.4 MB）的大小。此外，请注意为分区（按`overall`值）`Avro`文件创建的两个单独目录。
- en: '![](img/00046.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00046.jpeg)'
- en: 'For more details on `spark-avro`, refer: [https://github.com/databricks/spark-avro](https://github.com/databricks/spark-avro)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`spark-avro`的更多详细信息，请参阅：[https://github.com/databricks/spark-avro](https://github.com/databricks/spark-avro)
- en: Using Spark with Parquet files
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Parquet文件的Spark
- en: Apache Parquet is a popular columnar storage format. It is used in many big
    data applications in the Hadoop ecosystem. Parquet supports very efficient compression
    and encoding schemes that can give a significant boost to the performance of such
    applications. In this section, we show you the simplicity with which you can directly
    read Parquet files into a standard Spark SQL DataFrame.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet是一种流行的列存储格式。它在Hadoop生态系统中的许多大数据应用程序中使用。Parquet支持非常高效的压缩和编码方案，可以显著提高这些应用程序的性能。在本节中，我们向您展示了您可以直接将Parquet文件读入标准Spark
    SQL DataFrame的简单性。
- en: 'Here, we use the reviewsDF created previously from the Amazon reviews contained
    in a JSON formatted file and write it out in the Parquet format to create the
    Parquet file. We use `coalesce(1)` to create a single output file:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用之前从Amazon评论的JSON格式文件中创建的reviewsDF，并将其以Parquet格式写出，以创建Parquet文件。我们使用`coalesce(1)`来创建一个单一的输出文件：
- en: '[PRE34]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the next step, we create a DataFrame from the Parquet file using just one
    statement:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们使用一个语句从Parquet文件创建一个DataFrame：
- en: '[PRE35]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After the DataFrame is created, you can operate on it as you normally would
    with the DataFrames created from any other data source. Here, we register the
    DataFrame as a temp view and query it using SQL:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 创建DataFrame后，您可以像处理来自任何其他数据源创建的DataFrame一样对其进行操作。在这里，我们将DataFrame注册为临时视图，并使用SQL进行查询：
- en: '[PRE36]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, we specify two parameters to display the records in the resulting DataFrame.
    The first parameter specifies the number of records to display and a value of
    false for the second parameter shows the full values in the columns (with no truncation).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定了两个参数来显示结果DataFrame中的记录。第一个参数指定要显示的记录数，第二个参数的值为false时显示列中的完整值（不截断）。
- en: '[PRE37]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/00047.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00047.jpeg)'
- en: Defining and using custom data sources in Spark
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark中定义和使用自定义数据源
- en: You can define your own data sources and combine the data from such sources
    with data from other more standard data sources (for example, relational databases,
    Parquet files, and so on). In [Chapter 5](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL in Streaming Applications*, we define a custom data source for streaming
    data from public APIs available from **Transport for London** (**TfL**) site.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以定义自己的数据源，并将这些数据源的数据与其他更标准的数据源（例如关系数据库、Parquet文件等）的数据结合起来。在[第5章](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c)中，*在流应用中使用Spark
    SQL*，我们为从**伦敦交通**（TfL）网站提供的公共API中流式数据定义了一个自定义数据源。
- en: Refer to the video *Spark DataFrames Simple and Fast Analysis of Structured
    Data - Michael Armbrust (Databricks)* at [https://www.youtube.com/watch?v=xWkJCUcD55w](https://www.youtube.com/watch?v=xWkJCUcD55w)
    for a good example of defining a data source for Jira and creating a Spark SQL
    DataFrame from it.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 参考视频*Spark DataFrames Simple and Fast Analysis of Structured Data - Michael
    Armbrust (Databricks)* [https://www.youtube.com/watch?v=xWkJCUcD55w](https://www.youtube.com/watch?v=xWkJCUcD55w)
    中定义Jira数据源并从中创建Spark SQL DataFrame的良好示例。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we demonstrated using Spark with various data sources and data
    formats. We used Spark to work with a relational database (MySQL), NoSQL database
    (MongoDB), semistructured data (JSON), and data storage formats commonly used
    in the Hadoop ecosystem (Avro and Parquet). This sets you up very nicely for the
    more advanced Spark application-oriented chapters to follow.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们演示了使用Spark与各种数据源和数据格式。我们使用Spark来处理关系数据库（MySQL）、NoSQL数据库（MongoDB）、半结构化数据（JSON）以及在Hadoop生态系统中常用的数据存储格式（Avro和Parquet）。这为您非常好地准备了接下来更高级的Spark应用程序导向章节。
- en: In the next chapter, we will shift our focus from the mechanics of working with
    Spark to how Spark SQL can be used to explore data, perform data quality checks,
    and visualize data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把焦点从处理Spark的机制转移到如何使用Spark SQL来探索数据、执行数据质量检查和可视化数据。
