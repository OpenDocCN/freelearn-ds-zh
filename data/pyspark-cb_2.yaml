- en: Abstracting Data with RDDs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RDDs抽象数据
- en: 'In this chapter, we will cover how to work with Apache Spark Resilient Distributed
    Datasets. You will learn the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何使用Apache Spark的弹性分布式数据集。您将学习以下示例：
- en: Creating RDDs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建RDDs
- en: Reading data from files
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文件中读取数据
- en: Overview of RDD transformations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD转换概述
- en: Overview of RDD actions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD操作概述
- en: Pitfalls of using RDDs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RDDs的陷阱
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '**Resilient Distributed Datasets** (**RDDs**) are collections of immutable
    JVM objects that are distributed across an Apache Spark cluster. Please note that
    if you are new to Apache Spark, you may want to initially skip this chapter as
    Spark DataFrames/Datasets are both significantly easier to develop and typically
    have faster performance. More information on Spark DataFrames can be found in
    the next chapter.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集**（**RDDs**）是分布在Apache Spark集群中的不可变JVM对象的集合。请注意，如果您是Apache Spark的新手，您可能希望最初跳过本章，因为Spark
    DataFrames/Datasets在开发上更容易，并且通常具有更快的性能。有关Spark DataFrames的更多信息，请参阅下一章。'
- en: An RDD is the most fundamental dataset type of Apache Spark; any action on a
    Spark DataFrame eventually gets *translated* into a highly optimized execution
    of transformations and actions on RDDs (see the paragraph on catalyst optimizer
    in [Chapter 3](part0120.html#3IE3G0-dc04965c02e747b9b9a057725c821827), *Abstracting
    Data with DataFrames*, in the *Introduction* section).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: RDD是Apache Spark最基本的数据集类型；对Spark DataFrame的任何操作最终都会被*转换*为对RDD的高度优化的转换和操作的执行（请参阅[第3章](part0120.html#3IE3G0-dc04965c02e747b9b9a057725c821827)中关于数据帧的抽象的段落，*介绍*部分）。
- en: Data in an RDD is split into chunks based on a key and then dispersed across
    all the executor nodes. RDDs are highly resilient, that is, there are able to
    recover quickly from any issues as the same data chunks are replicated across
    multiple executor nodes. Thus, even if one executor fails, another will still
    process the data. This allows you to perform your functional calculations against
    your dataset very quickly by harnessing the power of multiple nodes. RDDs keep
    a log of all the execution steps applied to each chunk. This, on top of the data
    replication, speeds up the computations and, if anything goes wrong, RDDs can
    still recover the portion of the data lost due to an executor error.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: RDD中的数据根据键分成块，然后分散到所有执行节点上。RDDs具有很高的弹性，即相同的数据块被复制到多个执行节点上，因此即使一个执行节点失败，另一个仍然可以处理数据。这使您可以通过利用多个节点的能力快速对数据集执行功能计算。RDDs保留了应用于每个块的所有执行步骤的日志。这加速了计算，并且如果出现问题，RDDs仍然可以恢复由于执行器错误而丢失的数据部分。
- en: While it is common to lose a node in distributed environments (for example,
    due to connectivity issues, hardware problems), distribution and replication of
    the data defends against data loss, while data lineage allows the system to recover
    quickly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式环境中丢失节点是很常见的（例如，由于连接问题、硬件问题），数据的分发和复制可以防止数据丢失，而数据谱系允许系统快速恢复。
- en: Creating RDDs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建RDDs
- en: For this recipe, we will start creating an RDD by generating the data within
    the PySpark. To create RDDs in Apache Spark, you will need to first install Spark
    as shown in the previous chapter. You can use the PySpark shell and/or Jupyter
    notebook to run these code samples.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将通过在PySpark中生成数据来开始创建RDD。要在Apache Spark中创建RDDs，您需要首先按照上一章中所示安装Spark。您可以使用PySpark
    shell和/或Jupyter笔记本来运行这些代码示例。
- en: Getting ready
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We require a working installation of Spark. This means that you would have
    followed the steps outlined in the previous chapter. As a reminder, to start PySpark
    shell for your local Spark cluster, you can run this command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个已安装的Spark。这意味着您已经按照上一章中概述的步骤进行了操作。作为提醒，要为本地Spark集群启动PySpark shell，您可以运行以下命令：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Where `n` is the number of cores.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`n`是核心数。
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To quickly create an RDD, run PySpark on your machine via the bash terminal,
    or you can run the same query in a Jupyter notebook. There are two ways to create
    an RDD in PySpark: you can either use the `parallelize()` method—a collection
    (list or an array of some elements) or reference a file (or files) located either
    locally or through an external source, as noted in subsequent recipes.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速创建RDD，请通过bash终端在您的机器上运行PySpark，或者您可以在Jupyter笔记本中运行相同的查询。在PySpark中创建RDD有两种方法：您可以使用`parallelize()`方法-一个集合（一些元素的列表或数组）或引用一个文件（或文件），可以是本地的，也可以是通过外部来源，如后续的示例中所述。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To view what is inside your RDD, you can run the following code snippet:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看RDD中的内容，您可以运行以下代码片段：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Spark context parallelize method
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark上下文并行化方法
- en: 'Under the covers, there are quite a few actions that happened when you created
    your RDD. Let''s start with the RDD creation and break down this code snippet:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建RDD时，实际上发生了很多操作。让我们从RDD的创建开始，分解这段代码：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Focusing first on the statement in the `sc.parallelize()` method, we first
    created a Python list (that is, `[A, B, ..., E]`) composed of a list of arrays
    (that is, `(''Mike'', 19), (''June'', 19), ..., (''Scott'', 17)`). The `sc.parallelize()`
    method is the SparkContext''s `parallelize` method to create a parallelized collection.
    This allows Spark to distribute the data across multiple nodes, instead of depending
    on a single node to process the data:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先关注`sc.parallelize()`方法中的语句，我们首先创建了一个Python列表（即`[A, B, ..., E]`），由数组列表组成（即`('Mike',
    19), ('June', 19), ..., ('Scott', 17)`）。`sc.parallelize()`方法是SparkContext的`parallelize`方法，用于创建并行化集合。这允许Spark将数据分布在多个节点上，而不是依赖单个节点来处理数据：
- en: '![](img/00020.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00020.jpeg)'
- en: Now that we have created `myRDD` as a parallelized collection, Spark can operate
    against this data in parallel. Once created, the distributed dataset (`distData`)
    can be operated on in parallel. For example, we can call `myRDD.reduceByKey(add)`
    to add up the grouped by keys of the list; we have recipes for RDD operations
    in subsequent sections of this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了`myRDD`作为并行化集合，Spark可以并行操作这些数据。一旦创建，分布式数据集（`distData`）可以并行操作。例如，我们可以调用`myRDD.reduceByKey(add)`来对列表的按键进行求和；我们在本章的后续部分中有RDD操作的示例。
- en: .take(...) method
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .take(...) 方法
- en: Now that you have created your RDD (`myRDD`), we will use the `take()` method
    to return the values to the console (or notebook cell). We will now execute an
    RDD action (more information on this in subsequent recipes), `take()`. Note that
    a common approach in PySpark is to use `collect()`, which returns all values in
    your RDD from the Spark worker nodes to the driver. There are performance implications
    when working with a large amount of data as this translates to large volumes of
    data being transferred from the Spark worker nodes to the driver. For small amounts
    of data (such as this recipe), this is perfectly fine, but, as a matter of habit,
    you should pretty much always use the `take(n)` method instead; it returns the
    first `n` elements of the RDD instead of the whole dataset. It is a more efficient
    method because it first scans one partition and uses those statistics to determine
    the number of partitions required to return the results.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经创建了您的RDD（`myRDD`），我们将使用`take()`方法将值返回到控制台（或笔记本单元格）。我们现在将执行一个RDD操作（有关此操作的更多信息，请参见后续示例），`take()`。请注意，PySpark中的一种常见方法是使用`collect()`，它将从Spark工作节点将所有值返回到驱动程序。在处理大量数据时会有性能影响，因为这意味着大量数据从Spark工作节点传输到驱动程序。对于小量数据（例如本示例），这是完全可以的，但是，习惯上，您应该几乎总是使用`take(n)`方法；它返回RDD的前`n`个元素而不是整个数据集。这是一种更有效的方法，因为它首先扫描一个分区，并使用这些统计信息来确定返回结果所需的分区数。
- en: Reading data from files
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文件中读取数据
- en: 'For this recipe, we will create an RDD by reading a local file in PySpark.
    To create RDDs in Apache Spark, you will need to first install Spark as noted
    in the previous chapter. You can use the PySpark shell and/or Jupyter notebook
    to run these code samples. Note that while this recipe is specific to reading
    local files, a similar syntax can be applied for Hadoop, AWS S3, Azure WASBs,
    and/or Google Cloud Storage:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将通过在PySpark中读取本地文件来创建一个RDD。要在Apache Spark中创建RDDs，您需要首先按照上一章中的说明安装Spark。您可以使用PySpark
    shell和/或Jupyter笔记本来运行这些代码示例。请注意，虽然本示例特定于读取本地文件，但类似的语法也适用于Hadoop、AWS S3、Azure WASBs和/或Google
    Cloud Storage。
- en: '| Storage type | Example |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 存储类型 | 示例 |'
- en: '| Local files | `sc.textFile(''/local folder/filename.csv'')` |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 本地文件 | `sc.textFile(''/local folder/filename.csv'')` |'
- en: '| Hadoop HDFS | `sc.textFile(''hdfs://folder/filename.csv'')` |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Hadoop HDFS | `sc.textFile(''hdfs://folder/filename.csv'')` |'
- en: '| AWS S3 ([https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html))
    | `sc.textFile(''s3://bucket/folder/filename.csv'')` |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| AWS S3 ([https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html))
    | `sc.textFile(''s3://bucket/folder/filename.csv'')` |'
- en: '| Azure WASBs ([https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-blob-storage](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-blob-storage))
    | `sc.textFile(''wasb://bucket/folder/filename.csv'')` |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Azure WASBs ([https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-blob-storage](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-blob-storage))
    | `sc.textFile(''wasb://bucket/folder/filename.csv'')` |'
- en: '| Google Cloud Storage ([https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters))
    | `sc.textFile(''gs://bucket/folder/filename.csv'')` |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Google Cloud Storage ([https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters))
    | `sc.textFile(''gs://bucket/folder/filename.csv'')` |'
- en: '| Databricks DBFS ([https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html))
    | `sc.textFile(''dbfs://folder/filename.csv'')` |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Databricks DBFS ([https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html))
    | `sc.textFile(''dbfs://folder/filename.csv'')` |'
- en: Getting ready
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we will be reading a tab-delimited (or comma-delimited) file,
    so please ensure that you have a text (or CSV) file available. For your convenience, you
    can download the `airport-codes-na.txt` and `departuredelays.csv` files from [https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data](https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data).
    Ensure your local Spark cluster can access this file (for example, `~/data/flights/airport-codes-na.txt`).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将读取一个制表符分隔（或逗号分隔）的文件，所以请确保您有一个文本（或CSV）文件可用。为了您的方便，您可以从[https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data](https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data)下载`airport-codes-na.txt`和`departuredelays.csv`文件。确保您的本地Spark集群可以访问此文件（例如，`~/data/flights/airport-codes-na.txt`）。
- en: How to do it...
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Once you start the PySpark shell via the bash terminal (or you can run the
    same query within Jupyter notebook), execute the following query:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过bash终端启动PySpark shell后（或者您可以在Jupyter笔记本中运行相同的查询），执行以下查询：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you are running Databricks, the same file is already included in the `/databricks-datasets`
    folder; the command is:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在运行Databricks，同样的文件已经包含在`/databricks-datasets`文件夹中；命令是：
- en: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    element: element.split("\t"))`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    element: element.split("\t"))`'
- en: 'When running the query:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 运行查询时：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The resulting output is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出为：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Diving in a little deeper, let''s determine the number of rows in this RDD.
    Note that more information on RDD actions such as `count()` is included in subsequent
    recipes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 深入一点，让我们确定这个RDD中的行数。请注意，有关RDD操作（如`count()`）的更多信息包含在后续的示例中：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Also, let''s find out the number of partitions that support this RDD:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，让我们找出支持此RDD的分区数：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: .textFile(...) method
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .textFile(...)方法
- en: 'To read the file, we are using SparkContext''s `textFile()` method via this
    command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取文件，我们使用SparkContext的`textFile()`方法通过这个命令：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Only the first parameter is required, which indicates the location of the text
    file as per `~/data/flights/airport-codes-na.txt`. There are two optional parameters
    as well:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 只有第一个参数是必需的，它指示文本文件的位置为`~/data/flights/airport-codes-na.txt`。还有两个可选参数：
- en: '`minPartitions`: Indicates the minimum number of partitions that make up the
    RDD. The Spark engine can often determine the best number of partitions based
    on the file size, but you may want to change the number of partitions for performance
    reasons and, hence, the ability to specify the minimum number.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minPartitions`：指示组成RDD的最小分区数。Spark引擎通常可以根据文件大小确定最佳分区数，但出于性能原因，您可能希望更改分区数，因此可以指定最小数量。'
- en: '`use_unicode`: Engage this parameter if you are processing Unicode data.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_unicode`：如果处理Unicode数据，请使用此参数。'
- en: 'Note that if you were to execute this statement without the subsequent `map()` function,
    the resulting RDD would not reference the tab-delimiter—basically a list of strings
    that is:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您执行此语句而没有后续的`map()`函数，生成的RDD将不引用制表符分隔符——基本上是一个字符串列表：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: .map(...) method
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .map(...)方法
- en: 'To make sense of the tab-delimiter with an RDD, we will use the `.map(...)` function
    to transform the data from a list of strings to a list of lists:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解RDD中的制表符，我们将使用`.map(...)`函数将数据从字符串列表转换为列表列表：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The key components of this map transformation are:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此映射转换的关键组件是：
- en: '`lambda`: An anonymous function (that is, a function defined without a name)
    composed of a single expression'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lambda`：一个匿名函数（即，没有名称定义的函数），由一个单一表达式组成'
- en: '`split`: We''re using PySpark''s split function (within `pyspark.sql.functions`)
    to split a string around a regular expression pattern; in this case, our delimiter
    is a tab (that is, `\t`)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split`：我们使用PySpark的split函数（在`pyspark.sql.functions`中）来围绕正则表达式模式分割字符串；在这种情况下，我们的分隔符是制表符（即`\t`）'
- en: 'Putting the `sc.textFile()` and `map()` functions together allows us to read
    the text file and split by the tab-delimiter to produce an RDD composed of a parallelized
    list of lists collection:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将`sc.textFile()`和`map()`函数放在一起，可以让我们读取文本文件，并按制表符分割，生成由并行化列表集合组成的RDD：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Partitions and performance
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区和性能
- en: 'Earlier in this recipe, if we had run `sc.textFile()` without specifying `minPartitions` for
    this dataset, we would only have two partitions:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，如果我们在没有为这个数据集指定`minPartitions`的情况下运行`sc.textFile()`，我们只会有两个分区：
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'But as noted, if the `minPartitions` flag is specified, then you would get
    the specified four partitions (or more):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 但是请注意，如果指定了`minPartitions`标志，那么您将获得指定的四个分区（或更多）：
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A key aspect of partitions for your RDD is that the more partitions you have,
    the higher the parallelism. Potentially, having more partitions will improve your
    query performance. For this portion of the recipe, let''s use a slightly larger
    file, `departuredelays.csv`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RDD的分区的一个关键方面是，分区越多，并行性越高。潜在地，有更多的分区将提高您的查询性能。在这部分示例中，让我们使用一个稍大一点的文件，`departuredelays.csv`：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As noted in the preceding code snippet, by default, Spark will create two partitions
    and take 3.33 seconds (on my small cluster) to count the 1.39 million rows in
    the departure delays CSV file.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码片段所述，默认情况下，Spark将创建两个分区，并且在我的小集群上花费3.33秒（在出发延误CSV文件中计算139万行）。
- en: 'Executing the same command, but also specifying `minPartitions` (in this case,
    eight partitions), you will notice that the `count()` method completed in 2.96
    seconds (instead of 3.33 seconds with eight partitions). Note that these values
    may be different based on your machine''s configuration, but the key takeaway
    is that modifying the number of partitions may result in faster performance due
    to parallelization. Check out the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 执行相同的命令，但同时指定`minPartitions`（在这种情况下，为八个分区），您会注意到`count()`方法在2.96秒内完成（而不是使用八个分区的3.33秒）。请注意，这些值可能根据您的机器配置而有所不同，但关键是修改分区的数量可能会由于并行化而导致更快的性能。查看以下代码：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Overview of RDD transformations
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD转换概述
- en: 'As noted in preceding sections, there are two types of operation that can be
    used to shape data in an RDD: transformations and actions. A transformation, as
    the name suggests, *transforms* one RDD into another. In other words, it takes
    an existing RDD and transforms it into one or more output RDDs. In the preceding
    recipes, we had used a `map()` function, which is an example of a transformation
    to split the data by its tab-delimiter.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的部分所述，RDD中可以使用两种类型的操作来塑造数据：转换和操作。转换，顾名思义，*将*一个RDD转换为另一个。换句话说，它接受一个现有的RDD，并将其转换为一个或多个输出RDD。在前面的示例中，我们使用了`map()`函数，这是一个通过制表符分割数据的转换的示例。
- en: Transformations are lazy (unlike actions). They only get executed when an action
    is called on an RDD. For example, calling the `count()` function is an action;
    more information is available in the following section on actions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 转换是懒惰的（不像操作）。它们只有在RDD上调用操作时才会执行。例如，调用`count()`函数是一个操作；有关操作的更多信息，请参阅下一节。
- en: Getting ready
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe will be reading a tab-delimited (or comma-delimited) file, so please
    ensure that you have a text (or CSV) file available. For your convenience, you
    can download the `airport-codes-na.txt` and `departuredelays.csv` files from [https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data](https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data).
    Ensure your local Spark cluster can access this file (for example, `~/data/flights/airport-codes-na.txt`).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱将阅读一个制表符分隔（或逗号分隔）的文件，请确保您有一个文本（或CSV）文件可用。 为了您的方便，您可以从[https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data](https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data)下载`airport-codes-na.txt`和`departuredelays.csv`文件。确保您的本地Spark集群可以访问此文件（例如，`~/data/flights/airport-codes-na.txt`）。
- en: If you are running Databricks, the same file is already included in the `/databricks-datasets`
    folder; the command is
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在运行Databricks，同样的文件已经包含在`/databricks-datasets`文件夹中；命令是
- en: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    line: line.split("\t"))`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    line: line.split("\t"))`'
- en: 'Many of the transformations in the next section will use the RDDs `airports` or
    `flights`; let''s set them up using this code snippet:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节中的许多转换将使用RDDs `airports`或`flights`；让我们使用以下代码片段设置它们：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How to do it...
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: In this section, we list common Apache Spark RDD transformations and code snippets.
    A more complete list can be found at [https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations),
    [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and
    [https://training.databricks.com/visualapi.pdf](https://training.databricks.com/visualapi.pdf).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们列出了常见的Apache Spark RDD转换和代码片段。更完整的列表可以在[https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)、[https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)和[https://training.databricks.com/visualapi.pdf](https://training.databricks.com/visualapi.pdf)找到。
- en: 'The transformations include the following common tasks:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 转换包括以下常见任务：
- en: 'Removing the header line from your text file: `zipWithIndex()`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本文件中删除标题行：`zipWithIndex()`
- en: 'Selecting columns from your RDD: `map()`'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从RDD中选择列：`map()`
- en: 'Running a `WHERE` (filter) clause: `filter()`'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`WHERE`（过滤器）子句：`filter()`
- en: 'Getting the distinct values: `distinct()`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取不同的值：`distinct()`
- en: 'Getting the number of partitions: `getNumPartitions()`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取分区的数量：`getNumPartitions()`
- en: 'Determining the size of your partitions (that is, the number of elements within
    each partition): `mapPartitionsWithIndex()`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定分区的大小（即每个分区中的元素数量）：`mapPartitionsWithIndex()`
- en: .map(...) transformation
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .map(...)转换
- en: The `map(f)` transformation returns a new RDD formed by passing each element
    through a function, `f`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`map(f)`转换通过将每个元素传递给函数`f`来返回一个新的RDD。'
- en: 'Look at the following code snippet:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下代码片段：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will produce the following output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: .filter(...) transformation
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .filter(...)转换
- en: 'The `filter(f)`  transformation returns a new RDD based on selecting elements
    for which the `f` function returns true. Therefore, look at the following code
    snippet:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter(f)`转换根据`f`函数返回true的选择元素返回一个新的RDD。因此，查看以下代码片段：'
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will produce the following output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: .flatMap(...) transformation
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .flatMap(...)转换
- en: 'The `flatMap(f) `transformation is similar to map, but the new RDD flattens
    out all of the elements (that is, a sequence of events). Let''s look at the following
    snippet:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap(f)`转换类似于map，但新的RDD会展平所有元素（即一系列事件）。让我们看一下以下片段：'
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code will produce the following output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将产生以下输出：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: .distinct() transformation
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .distinct()转换
- en: 'The `distinct()` transformation returns a new RDD containing the distinct elements
    of the source RDD. So, look at the following code snippet:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`distinct()`转换返回包含源RDD的不同元素的新RDD。因此，查看以下代码片段：'
- en: '[PRE29]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will return the following output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下输出：
- en: '[PRE30]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: .sample(...) transformation
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .sample(...)转换
- en: The `sample(withReplacement, fraction, seed)` transformation samples a fraction
    of the data, with or without replacement (the `withReplacement` parameter), based
    on a random seed.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample(withReplacement, fraction, seed)`转换根据随机种子从数据中抽取一部分数据，可以选择是否有放回（`withReplacement`参数）。'
- en: 'Look at the following code snippet:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下代码片段：
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can expect the following result:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以期待以下结果：
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: .join(...) transformation
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .join(...)转换
- en: The `join(RDD')` transformation returns an RDD of *(key, (val_left, val_right))*
    when calling RDD *(key, val_left)* and RDD *(key, val_right)*. Outer joins are
    supported through left outer join, right outer join, and full outer join.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`join(RDD'')`转换在调用RDD *(key, val_left)*和RDD *(key, val_right)*时返回一个*(key, (val_left,
    val_right))*的RDD。左外连接、右外连接和完全外连接都是支持的。'
- en: 'Look at the following code snippet:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下代码片段：
- en: '[PRE33]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will give you the following result:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下结果：
- en: '[PRE34]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: .repartition(...) transformation
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .repartition(...)转换
- en: 'The `repartition(n)` transformation repartitions the RDD into `n` partitions
    by randomly reshuffling and uniformly distributing data across the network. As
    noted in the preceding recipes, this can improve performance by running more parallel
    threads concurrently. Here''s a code snippet that does precisely that:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`repartition(n)`转换通过随机重分区和均匀分布数据来将RDD重新分区为`n`个分区。正如前面的食谱中所述，这可以通过同时运行更多并行线程来提高性能。以下是一个精确执行此操作的代码片段：'
- en: '[PRE35]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: .zipWithIndex() transformation
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .zipWithIndex()转换
- en: The `zipWithIndex()` transformation appends (or ZIPs) the RDD with the element
    indices. This is very handy when wanting to remove the header row (first row)
    of a file.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`zipWithIndex()`转换会将RDD附加（或ZIP）到元素索引上。当想要删除文件的标题行（第一行）时，这非常方便。'
- en: 'Look at the following code snippet:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下代码片段：
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This will generate this result:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成这个结果：
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To remove the header from your data, you can use the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要从数据中删除标题，您可以使用以下代码：
- en: '[PRE38]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code will skip the header, as shown as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将跳过标题，如下所示：
- en: '[PRE39]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: .reduceByKey(...) transformation
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .reduceByKey(...) 转换
- en: The `reduceByKey(f)` transformation reduces the elements of the RDD using `f` by
    the key. The `f` function should be commutative and associative so that it can
    be computed correctly in parallel.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey(f)` 转换使用`f`按键减少RDD的元素。`f`函数应该是可交换和可结合的，这样它才能在并行计算中正确计算。'
- en: 'Look at the following code snippet:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 看下面的代码片段：
- en: '[PRE40]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This will generate the following output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE41]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: .sortByKey(...) transformation
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .sortByKey(...) 转换
- en: 'The `sortByKey(asc)` transformation orders *(key, value)* RDD by *key* and
    returns an RDD in ascending or descending order. Look at the following code snippet:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`sortByKey(asc)` 转换按*key*对*(key, value)* RDD进行排序，并以升序或降序返回一个RDD。看下面的代码片段：'
- en: '[PRE42]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will produce this output:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE43]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: .union(...) transformation
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .union(...) 转换
- en: 'The `union(RDD)` transformation returns a new RDD that is the union of the
    source and argument RDDs. Look at the following code snippet:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`union(RDD)` 转换返回一个新的RDD，该RDD是源RDD和参数RDD的并集。看下面的代码片段：'
- en: '[PRE44]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This will generate the following output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE45]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: .mapPartitionsWithIndex(...) transformation
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .mapPartitionsWithIndex(...) 转换
- en: 'The `mapPartitionsWithIndex(f)` is similar to map but runs the `f` function separately
    on each partition and provides an index of the partition. It is useful to determine
    the data skew within partitions (check the following snippet):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapPartitionsWithIndex(f)` 类似于map，但在每个分区上单独运行`f`函数，并提供分区的索引。它有助于确定分区内的数据倾斜（请查看以下代码片段）：'
- en: '[PRE46]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding code will produce the following result:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将产生以下结果：
- en: '[PRE47]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: How it works...
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Recall that a transformation takes an existing RDD and transforms it into one
    or more output RDDs. It is also a lazy process that is not initiated until an
    action is executed. In the following join example, the action is the `take()`
    function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，转换会将现有的RDD转换为一个或多个输出RDD。它也是一个懒惰的过程，直到执行一个动作才会启动。在下面的连接示例中，动作是`take()`函数：
- en: '[PRE48]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'To better understand what is happening when running this join, let''s review
    the Spark UI. Every Spark session launches a web-based UI, which is, by default,
    on port `4040`, for example, `http://localhost:4040`. It includes the following
    information:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解运行此连接时发生了什么，让我们回顾一下Spark UI。每个Spark会话都会启动一个基于Web的UI，默认情况下在端口`4040`上，例如`http://localhost:4040`。它包括以下信息：
- en: A list of scheduler stages and tasks
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器阶段和任务列表
- en: A summary of RDD sizes and memory usage
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD大小和内存使用情况摘要
- en: Environmental information
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境信息
- en: Information about the running executors
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关正在运行的执行器的信息
- en: For more information, please refer to the Apache Spark Monitoring documentation
    page at [https://spark.apache.org/docs/latest/monitoring.html](https://spark.apache.org/docs/latest/monitoring.html).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅Apache Spark监控文档页面[https://spark.apache.org/docs/latest/monitoring.html](https://spark.apache.org/docs/latest/monitoring.html)。
- en: To dive deeper into Spark internals, a great video is Patrick Wendell's *Tuning
    and Debugging in Apache* *Spark* video, which is available at [https://www.youtube.com/watch?v=kkOG_aJ9KjQ](https://www.youtube.com/watch?v=kkOG_aJ9KjQ).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解Spark内部工作，一个很好的视频是Patrick Wendell的*Apache Spark* *调优和调试*视频，可在[https://www.youtube.com/watch?v=kkOG_aJ9KjQ](https://www.youtube.com/watch?v=kkOG_aJ9KjQ)上找到。
- en: 'As can be seen in the following DAG visualization, the join statement and two
    preceding map transformations have a single job (Job 24) that created two stages
    (Stage 32 and Stage 33):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如下DAG可视化所示，连接语句和两个前面的`map`转换都有一个作业（作业24），创建了两个阶段（第32阶段和第33阶段）：
- en: '![](img/00021.jpeg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00021.jpeg)'
- en: Details for Job 24
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 作业24的详细信息
- en: 'Let''s dive deeper into these two stages:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解这两个阶段：
- en: '![](img/00022.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00022.jpeg)'
- en: Details of Stage 32
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 第32阶段的详细信息
- en: 'To better understand the tasks executed in the first stage (Stage 32), we can
    dive deeper into the stage''s DAG Visualization as well as the Event Timeline:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解第一阶段（第32阶段）中执行的任务，我们可以深入了解阶段的DAG可视化以及事件时间线：
- en: The two `textFile` callouts are to extract the two different files (`departuredelays.csv`
    and `airport-codes-na.txt`)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个`textFile`调用是为了提取两个不同的文件（`departuredelays.csv`和`airport-codes-na.txt`）
- en: Once the `map` functions are complete, to support the `join`, Spark executes
    `UnionRDD` and `PairwiseRDD` to perform the basics behind the join as part of
    the `union` task
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦`map`函数完成，为了支持`join`，Spark执行`UnionRDD`和`PairwiseRDD`来执行连接背后的基本操作作为`union`任务的一部分
- en: 'In the next stage, the `partitionBy` and `mapPartitions` tasks shuffle and
    re-map the partitions prior to providing the output via the `take()` function:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个阶段，`partitionBy`和`mapPartitions`任务在通过`take()`函数提供输出之前重新洗牌和重新映射分区：
- en: '![](img/00023.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00023.jpeg)'
- en: Details of Stage 33
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第33阶段的详细信息
- en: Note that that if you execute the same statements without the `take()` function
    (or some other *action*), only *transformation* operations will be executed with
    nothing showing up in the Spark UI denoting lazy processing.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您执行相同的语句而没有`take()`函数（或其他*动作*），只有*转换*操作将被执行，而在Spark UI中没有显示懒惰处理。
- en: 'For example, if you were to execute the following code snippet, note that the
    output is a pointer to a Python RDD:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您执行以下代码片段，请注意输出是指向Python RDD的指针：
- en: '[PRE49]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Overview of RDD actions
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD动作概述
- en: 'As noted in preceding sections, there are two types of Apache Spark RDD operations:
    transformations and actions. An *action* returns a value to the driver after running
    a computation on the dataset, typically on the workers. In the preceding recipes,
    the `take()` and `count()` RDD operations are examples of *actions*.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的部分所述，Apache Spark RDD操作有两种类型：转换和动作。*动作*在数据集上运行计算后将一个值返回给驱动程序，通常在工作节点上。在前面的示例中，`take()`和`count()`
    RDD操作是*动作*的示例。
- en: Getting ready
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: This recipe will be reading a tab-delimited (or comma-delimited) file, so please
    ensure that you have a text (or CSV) file available. For your convenience, you
    can download the `airport-codes-na.txt` and `departuredelays.csv` files from learning [http://bit.ly/2nroHbh](http://bit.ly/2nroHbh).
    Ensure your local Spark cluster can access this file (`~/data/flights/airport-codes-na.txt`).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例将读取一个制表符分隔（或逗号分隔）的文件，请确保您有一个文本（或CSV）文件可用。为了您的方便，您可以从[http://bit.ly/2nroHbh](http://bit.ly/2nroHbh)下载`airport-codes-na.txt`和`departuredelays.csv`文件。确保您的本地Spark集群可以访问此文件（`~/data/flights/airport-codes-na.txt`）。
- en: If you are running Databricks, the same file is already included in the `/databricks-datasets`
    folder; the command is
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在运行Databricks，则相同的文件已经包含在`/databricks-datasets`文件夹中；命令是
- en: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    line: line.split("\t"))`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    line: line.split("\t"))`'
- en: 'Many of the transformations in the next section will use the RDDs `airports` or
    `flights`; let''s set them up by using the following code snippet:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节中的许多转换将使用RDDs `airports`或`flights`；让我们通过以下代码片段来设置它们：
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: How to do it...
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: The following list outlines common Apache Spark RDD transformations and code
    snippets. A more complete list can be found in the Apache Spark documentation, RDD
    Programing Guide | Transformations, at [https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations),
    the PySpark RDD API at [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD),
    and Essential Core and Intermediate Spark Operations at [https://training.databricks.com/visualapi.pdf](https://training.databricks.com/visualapi.pdf).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表概述了常见的Apache Spark RDD转换和代码片段。更完整的列表可以在Apache Spark文档的RDD编程指南 | 转换中找到，网址为[https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)，PySpark
    RDD API网址为[https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)，以及Essential
    Core and Intermediate Spark Operations网址为[https://training.databricks.com/visualapi.pdf](https://training.databricks.com/visualapi.pdf)。
- en: .take(...) action
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .take(...) 操作
- en: 'We have already discussed this, but, for the sake of completeness, the `take(*n*)`
    action returns an array with the first `n` elements of the RDD. Look at the following
    code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过这个问题，但为了完整起见，`take(*n*)`操作将返回一个包含RDD的前`n`个元素的数组。看一下以下代码：
- en: '[PRE51]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This will generate the following output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE52]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: .collect() action
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .collect() 操作
- en: 'We have also cautioned you about using this action; `collect()`returns all
    of the elements from the workers to the driver. Thus, look at the following code:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还警告您不要使用此操作；`collect()`将所有元素从工作节点返回到驱动程序。因此，看一下以下代码：
- en: '[PRE53]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This will generate the following output:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE54]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: .reduce(...) action
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .reduce(...) 操作
- en: 'The `reduce(f)` action aggregates the elements of an RDD by `f`. The `f` function should
    be commutative and associative so that it can be computed correctly in parallel.
    Look at the following code:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce(f)` 操作通过`f`聚合RDD的元素。`f`函数应该是可交换和可结合的，以便可以正确并行计算。看一下以下代码：'
- en: '[PRE55]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This will produce the following result:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '[PRE56]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We need to make an important note here, however. When using `reduce()`, the
    reducer function needs to be associative and commutative; that is, a change in
    the order of elements and operands does not change the result.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们需要在这里做出重要的说明。使用`reduce()`时，缩减器函数需要是可结合和可交换的；也就是说，元素和操作数的顺序变化不会改变结果。
- en: 'Associativity rule: `(6 + 3) + 4 = 6 + (3 + 4)` Commutative rule: ` 6 + 3 +
    4 = 4 + 3 + 6`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 结合规则：`(6 + 3) + 4 = 6 + (3 + 4)` 交换规则：` 6 + 3 + 4 = 4 + 3 + 6`
- en: Error can occur if you ignore the aforementioned rules.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果忽略上述规则，可能会出现错误。
- en: 'As an example, see the following RDD (with one partition only!):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看一下以下RDD（只有一个分区！）：
- en: '`data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 1)`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 1)`'
- en: 'Reducing data to divide the current result by the subsequent one, we would
    expect a value of 10:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据减少到将当前结果除以后续结果，我们期望得到一个值为10：
- en: '`works = data_reduce.reduce(lambda x, y: x / y)`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`works = data_reduce.reduce(lambda x, y: x / y)`'
- en: 'Partitioning the data into three partitions will produce an incorrect result:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分区为三个分区将产生不正确的结果：
- en: '`data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 3) data_reduce.reduce(lambda
    x, y: x / y)`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 3) data_reduce.reduce(lambda
    x, y: x / y)`'
- en: It will produce `0.004`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 它将产生`0.004`。
- en: .count() action
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .count() 操作
- en: 'The `count()` action returns the number of elements in the RDD. See the following
    code:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`count()`操作返回RDD中元素的数量。请参阅以下代码：'
- en: '[PRE57]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This will produce the following result:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '[PRE58]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: .saveAsTextFile(...) action
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .saveAsTextFile(...) 操作
- en: 'The `saveAsTextFile()` action saves your RDD into a text file; note that each
    partition is a separate file. See the following snippet:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`saveAsTextFile()`操作将RDD保存到文本文件中；请注意，每个分区都是一个单独的文件。请参阅以下代码片段：'
- en: '[PRE59]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This will actually save the following files:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上将保存以下文件：
- en: '[PRE61]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: How it works...
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Recall that actions return a value to the driver after running a computation
    on the dataset, typically on the workers. Examples of some Spark actions include
    `count()` and `take()`; for this section, we will be focusing on `reduceByKey()`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，操作在对数据集进行计算后将值返回给驱动程序，通常在工作节点上。一些Spark操作的示例包括`count()`和`take()`；在本节中，我们将重点关注`reduceByKey()`：
- en: '[PRE62]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'To better understand what is happening when running this join, let''s review
    the Spark UI. Every Spark Session launches a web-based UI, which is, by default,
    on port `4040`, for example, `http://localhost:4040`. It includes the following
    information:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解运行此连接时发生了什么，让我们来看一下Spark UI。每个Spark会话都会启动一个基于Web的UI，默认情况下在端口`4040`上，例如`http://localhost:4040`。它包括以下信息：
- en: A list of scheduler stages and tasks
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器阶段和任务列表
- en: A summary of RDD sizes and memory usage
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD大小和内存使用情况摘要
- en: Environmental information
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境信息
- en: Information about the running executors
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关正在运行的执行器的信息
- en: For more information, please refer to the Apache Spark Monitoring documentation
    page at [https://spark.apache.org/docs/latest/monitoring.html](https://spark.apache.org/docs/latest/monitoring.html).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅Apache Spark监控文档页面[https://spark.apache.org/docs/latest/monitoring.html](https://spark.apache.org/docs/latest/monitoring.html)。
- en: To dive deeper into Spark internals, a great video is Patrick Wendell's *Tuning
    and Debugging in Apache Spark* video, which is available at [https://www.youtube.com/watch?v=kkOG_aJ9KjQ](https://www.youtube.com/watch?v=kkOG_aJ9KjQ).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解Spark内部工作，可以观看Patrick Wendell的*调整和调试Apache Spark*视频，网址为[https://www.youtube.com/watch?v=kkOG_aJ9KjQ](https://www.youtube.com/watch?v=kkOG_aJ9KjQ)。
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '![](img/00024.jpeg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00024.jpeg)'
- en: 'Digging further into the tasks that make up each stage, notice that the bulk
    of the work is done in **Stage 18**. Note the eight parallel tasks that end up
    processing data, from extracting it from the file (`/tmp/data/departuredelays.csv`)
    to executing `reduceByKey()` in parallel:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步深入研究构成每个阶段的任务，注意到大部分工作是在**Stage 18**中完成的。注意到有八个并行任务最终处理数据，从文件(`/tmp/data/departuredelays.csv`)中提取数据到并行执行`reduceByKey()`：
- en: '![](img/00025.jpeg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00025.jpeg)'
- en: Details of Stage 18
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 第18阶段的详细信息
- en: 'A few important callouts are as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些重要的要点：
- en: Spark's `reduceByKey(f)` assumes the `f` function is commutative and associative
    so that it can be computed correctly in parallel. As noted in the Spark UI, all
    eight tasks are processing the data extraction (`sc.textFile`) and `reduceByKey()`
    in parallel, providing faster performance.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的`reduceByKey(f)`假设`f`函数是可交换和可结合的，以便可以正确地并行计算。如Spark UI中所示，所有八个任务都在并行处理数据提取(`sc.textFile`)和`reduceByKey()`，提供更快的性能。
- en: 'As noted in the *Getting ready* section of this recipe, we executed `sc.textFile($fileLocation,
    minPartitions=8)..`. This forced the RDD to have eight partitions (at least eight
    partitions), which translated to eight tasks being executed in parallel:'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如本教程的*Getting ready*部分所述，我们执行了`sc.textFile($fileLocation, minPartitions=8)..`。这迫使RDD有八个分区（至少有八个分区），这意味着会有八个任务并行执行：
- en: '![](img/00026.jpeg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00026.jpeg)'
- en: Now that you have executed `reduceByKey()`, we will run `take(5)`, which executes
    another stage that shuffles the eight partitions from the workers to the single
    driver node; that way, the data can be collected for viewing in the console.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经执行了`reduceByKey()`，我们将运行`take(5)`，这将执行另一个阶段，将来自工作节点的八个分区洗牌到单个驱动节点；这样，数据就可以被收集起来在控制台中查看。
- en: Pitfalls of using RDDs
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RDD的缺陷
- en: The key concern associated with using RDDs is that they can take a lot of time
    to master. The flexibility of running functional operators such as map, reduce,
    and shuffle allows you to perform a wide variety of transformations against your
    data. But with this power comes great responsibility, and it is potentially possible
    to write code that is inefficient, such as the use of `GroupByKey`; more information
    can be found in *Avoid GroupByKey* at [https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RDD的关键问题是可能需要很长时间才能掌握。运行诸如map、reduce和shuffle等功能操作符的灵活性使您能够对数据执行各种各样的转换。但是，这种强大的功能也伴随着巨大的责任，有可能编写效率低下的代码，比如使用`GroupByKey`；更多信息可以在[https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)中找到。
- en: 'Generally, you will typically have slower performance when using RDDs compared
    to Spark DataFrames, as noted in the following diagram:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，与Spark DataFrames相比，使用RDDs通常会有较慢的性能，如下图所示：
- en: '![](img/00027.jpeg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00027.jpeg)'
- en: 'Source: Introducing DataFrames in Apache Spark for Large Scale Data Science
    at https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：在Apache Spark中引入数据框架进行大规模数据科学，网址为https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html
- en: It is also important  to note that with Apache Spark 2.0+, datasets have functional
    operators (giving you flexibility similar to RDDs), yet also utilize the catalyst
    optimizer, providing faster performance. More information on datasets will be
    discussed in the next chapter.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意的是，使用Apache Spark 2.0+，数据集具有功能操作符（给您类似于RDD的灵活性），同时还利用了catalyst优化器，提供更快的性能。有关数据集的更多信息将在下一章中讨论。
- en: The reason RDDs are slow—especially within the context of PySpark—is because
    whenever a PySpark program is executed using RDDs, there is a potentially large
    overhead to execute the job. As noted in the following diagram, in the PySpark
    driver, the `Spark Context` uses `Py4j` to launch a JVM using `JavaSparkContext`.
    Any RDD transformations are initially mapped to `PythonRDD` objects in Java.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: RDD之所以慢——特别是在PySpark的上下文中——是因为每当使用RDDs执行PySpark程序时，执行作业可能会产生很大的开销。如下图所示，在PySpark驱动程序中，`Spark
    Context`使用`Py4j`启动一个使用`JavaSparkContext`的JVM。任何RDD转换最初都会在Java中映射到`PythonRDD`对象。
- en: 'Once these tasks are pushed out to the Spark worker(s), `PythonRDD` objects
    launch Python `subprocesses` using pipes to send both code and data to be processed
    in Python:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些任务被推送到Spark worker(s)，`PythonRDD`对象就会启动Python `subprocesses`，使用管道发送代码和数据以在Python中进行处理：
- en: '![](img/00028.jpeg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.jpeg)'
- en: While this approach allows PySpark to distribute the processing of the data
    to multiple Python `subprocesses` on multiple workers, as you can see, there is
    a lot of context switching and communications overhead between Python and the
    JVM.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法允许PySpark将数据处理分布到多个Python `subprocesses`上的多个工作节点，但正如您所看到的，Python和JVM之间存在大量的上下文切换和通信开销。
- en: 'An excellent resource on PySpark performance is Holden Karau’s *Improving PySpark
    Performance: Spark Performance Beyond the JVM* at [http://bit.ly/2bx89bn](http://bit.ly/2bx89bn).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '关于PySpark性能的一个很好的资源是Holden Karau的*Improving PySpark Performance: Spark Performance
    Beyond the JVM*，网址为[http://bit.ly/2bx89bn](http://bit.ly/2bx89bn)。'
- en: This is even more apparent when using Python UDFs, as the performance is significantly
    slower because all of the data will need to be transferred to the driver prior
    to using a Python UDF. Note that vectorized UDFs were introduced as part of Spark
    2.3 and will improve PySpark UDF performance. For more information, please refer
    to *Introducing Vectorized UDFs for PySpark* at [https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Python UDF时，这一点更加明显，因为性能明显较慢，因为所有数据都需要在使用Python UDF之前传输到驱动程序。请注意，向量化UDF是作为Spark
    2.3的一部分引入的，并将改进PySpark UDF的性能。有关更多信息，请参阅[https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)上的*Introducing
    Vectorized UDFs for PySpark*。
- en: Getting ready
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'As in the previous sections, let''s make use of the `flights` dataset and create
    an RDD and a DataFrame against this dataset:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的部分一样，让我们利用`flights`数据集并针对该数据集创建一个RDD和一个DataFrame：
- en: '[PRE64]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: How to do it...
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this section, we will run the same `group by` statement—one via an RDD using
    `reduceByKey()`, and one via a DataFrame using Spark SQL `GROUP BY`. For this
    query, we will sum the time delays grouped by originating city and sort according
    to the originating city:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将运行相同的`group by`语句——一个是通过使用`reduceByKey()`的RDD，另一个是通过使用Spark SQL `GROUP
    BY`的DataFrame。对于这个查询，我们将按出发城市对延迟时间进行求和，并根据出发城市进行排序：
- en: '[PRE65]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'For this particular configuration, it took 11.08 seconds to extract the columns,
    execute `reduceByKey()` to summarize the data, execute `sortByKey()` to order
    it, and then return the values to the driver:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种特定配置，提取列，执行`reduceByKey()`对数据进行汇总，执行`sortByKey()`对其进行排序，然后将值返回到驱动程序共需11.08秒：
- en: '[PRE66]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'There are many advantages of Spark DataFrames, including, but not limited to
    the following:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrames有许多优点，包括但不限于以下内容：
- en: You can execute Spark SQL statements (not just through the Spark DataFrame API)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以执行Spark SQL语句（不仅仅是通过Spark DataFrame API）
- en: There is a schema associated with your data so you can specify the column name
    instead of position
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与位置相比，您的数据有一个关联的模式，因此您可以指定列名
- en: In this configuration and example, the query completes in 4.76 seconds, while
    RDDs complete in 11.08 seconds
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种配置和示例中，查询完成时间为4.76秒，而RDD完成时间为11.08秒
- en: 'It is impossible to improve your RDD query by specifying `minPartitions` within
    `sc.textFile()` when originally loading the data to increase the number of partitions:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初加载数据以增加分区数时，通过在`sc.textFile()`中指定`minPartitions`来改进RDD查询是不可能的：
- en: '`flights = sc.textFile(''/databricks-datasets/flights/departuredelays.csv'',
    minPartitions=8), ...`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`flights = sc.textFile(''/databricks-datasets/flights/departuredelays.csv'',
    minPartitions=8), ...`'
- en: '[PRE67]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: For this configuration, the same query returned in 6.63 seconds. While this
    approach is faster, its still slower than DataFrames; in general, DataFrames are
    faster out of the box with the default configuration.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种配置，相同的查询返回时间为6.63秒。虽然这种方法更快，但仍然比DataFrames慢；一般来说，DataFrames在默认配置下更快。
- en: How it works...
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'To better understand the performance of the previous RDD and DataFrame, let''s
    return to the Spark UI. For starters, when we run the `flights` RDD query, three
    separate jobs are executed, as can be seen in Databricks Community Edition in
    the following screenshot:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解以前的RDD和DataFrame的性能，让我们返回到Spark UI。首先，当我们运行`flights` RDD查询时，将执行三个单独的作业，如在以下截图中在Databricks
    Community Edition中可以看到的那样：
- en: '![](img/00029.jpeg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00029.jpeg)'
- en: 'Each of these jobs spawn their own set of stages to initially read the text
    (or CSV) file, execute  `reduceByKey()`, and execute the `sortByKey()` functions:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 每个作业都会生成自己的一组阶段，最初读取文本（或CSV）文件，执行`reduceByKey()`，并执行`sortByKey()`函数：
- en: '![](img/00030.jpeg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00030.jpeg)'
- en: 'With two additional jobs to complete the `sortByKey()` execution:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两个额外的作业来完成`sortByKey()`的执行：
- en: '![](img/00031.jpeg)![](img/00032.jpeg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00031.jpeg)![](img/00032.jpeg)'
- en: As can be observed, by using RDDs directly, there can potentially be a lot of
    overhead, generating multiple jobs and stages to complete a single query.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，通过直接使用RDD，可能会产生大量的开销，生成多个作业和阶段来完成单个查询。
- en: 'In the case of Spark DataFrames, for this query it is much simpler for it to
    consist of a single job with two stages. Note that the Spark UI has a number of
    DataFrame-specific set tasks, such as `WholeStageCodegen` and `Exchange`, that
    significantly improve the performance of Spark dataset and DataFrame queries.
    More information about the Spark SQL engine catalyst optimizer can be found in
    the next chapter:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Spark DataFrames，在这个查询中，它更简单，它由一个包含两个阶段的单个作业组成。请注意，Spark UI有许多特定于DataFrame的任务，如`WholeStageCodegen`和`Exchange`，它们显著改进了Spark数据集和DataFrame查询的性能。有关Spark
    SQL引擎催化剂优化器的更多信息可以在下一章中找到：
- en: '![](img/00033.jpeg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00033.jpeg)'
