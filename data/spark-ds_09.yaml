- en: Chapter 9. Visualizing Big Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。可视化大数据
- en: Proper data visualization has solved many business problems in the past without
    much statistics or machine learning being involved. Even today, with so many technological
    advancements, applied statistics, and machine learning, proper visuals are the
    end deliverables for business users to consume information or the output of some
    analyses. Conveying the right information in the right format is something that
    data scientists yearn for, and an effective visual is worth a million words. Also,
    representing the models and the insights generated in a way that is easily consumable
    by the business is extremely important. Nonetheless, exploring big data visually
    is very cumbersome and challenging. Since Spark is designed for big data processing,
    it also supports big data visualization along with it. There are many tools and
    techniques that have been built on Spark for this purpose.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 适当的数据可视化在过去解决了许多业务问题，而没有涉及太多统计学或机器学习。即使在今天，随着技术的不断进步、应用统计学和机器学习，适当的可视化仍然是业务用户消化信息或某些分析的最终交付物。以正确的格式传达正确的信息是数据科学家渴望的，有效的可视化价值连城。此外，以一种易于业务消化的方式表示生成的模型和见解也非常重要。尽管如此，通过可视化探索大数据是非常繁琐和具有挑战性的。由于Spark是为大数据处理而设计的，它也支持大数据可视化。已经在Spark上为此目的构建了许多工具和技术。
- en: The previous chapters outlined how to model structured and unstructured data
    and generate insights from it. In this chapter, we will look at data visualization
    from two broad perspectives-one is from a data scientist's perspective—where visualization
    is the basic need to explore and understand the data effectively, and the other
    is from a business user's perspective, where the visuals are end deliverables
    to the business and must be easily comprehendible. We will explore various data
    visualization tools such as *IPythonNotebook* and *Zeppelin* that can be used
    on Apache Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章概述了如何对结构化和非结构化数据进行建模，并从中生成见解。在本章中，我们将从两个广泛的视角来看数据可视化——一个是从数据科学家的视角，可视化是探索和有效理解数据的基本需求，另一个是从业务用户的视角，其中可视化是业务的最终交付物，必须易于理解。我们将探索各种数据可视化工具，如*IPythonNotebook*和*Zeppelin*，可以在Apache
    Spark上使用。
- en: 'As a prerequisite for this chapter, a basic understanding of SQL and programming
    in Python, Scala, or other such frameworks, is nice to have. The topics covered
    in this chapter are listed as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的先决条件，对SQL和Python、Scala或其他类似框架的编程有基本的了解是很有帮助的。本章涵盖的主题如下：
- en: Why visualize data?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要可视化数据？
- en: A data engineer's perspective
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程师的视角
- en: A data scientist's perspective
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家的视角
- en: A business user's perspective
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务用户的视角
- en: Data visualization tools
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化工具
- en: IPython notebook
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPython笔记本
- en: Apache Zeppelin
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Zeppelin
- en: Third-party tools
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三方工具
- en: Data visualization techniques
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化技术
- en: Summarizing and visualizing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结和可视化
- en: Subsetting and visualizing
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子集和可视化
- en: Sampling and visualizing
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽样和可视化
- en: Modeling and visualizing
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模和可视化
- en: Why visualize data?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要可视化数据？
- en: 'Data visualization deals with representing data in a visual form so as to enable
    people to understand the underlying patterns and trends. Geographical maps, the
    bar and line charts of the seventeenth century, are some examples of early data
    visualizations. Excel is perhaps a familiar data visualization tool that most
    of us have already used. All data analytics tools have been equipped with sophisticated,
    interactive data visualization dashboards. However, the recent surge in big data,
    streaming, and real-time analytics has been pushing the boundaries of these tools
    and they seem to be bursting at the seams. The idea is to make the visualizations
    look simple, accurate, and relevant while hiding away all the complexity. As per
    the business needs, any visualization solution should ideally have the following
    characteristics:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化涉及以视觉形式表示数据，以便使人们能够理解其中的模式和趋势。地理地图，十七世纪的条形图和折线图，是早期数据可视化的一些例子。Excel可能是我们大多数人已经使用过的熟悉的数据可视化工具。所有数据分析工具都配备了复杂的交互式数据可视化仪表板。然而，大数据、流式数据和实时分析的最近激增一直在推动这些工具的边界，它们似乎已经到了极限。其想法是使可视化看起来简单、准确和相关，同时隐藏所有复杂性。根据业务需求，任何可视化解决方案理想上应具有以下特点：
- en: Interactivity
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互动性
- en: Reproducibility
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重现性
- en: Control over the details
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对细节的控制
- en: Apart from these, if the solution allows users to collaborate over the visuals
    or reports and share with each other, then that would make up an end-to-end visualization
    solution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，如果解决方案允许用户在可视化或报告上进行协作并相互分享，那么这将构成一个端到端的可视化解决方案。
- en: Big data visualization in particular poses its own challenges because we may
    end up with more data than pixels on the screen. Manipulating large data usually
    requires memory- and CPU-intensive processing and may have long latency. Add real-time
    or streaming data to the mix and the problem becomes even more challenging. Apache
    Spark is designed from the ground up just to tackle this latency by parallelizing
    CPU and memory usage. Before exploring the tools and techniques to visualize and
    work with big data, let's first understand the visualization needs of data engineers,
    data scientists, and business users.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是大数据可视化本身就存在着自己的挑战，因为我们可能会得到比屏幕上的像素更多的数据。处理大量数据通常需要内存和CPU密集型处理，并可能具有较长的延迟。将实时或流式数据添加到混合中，问题变得更加具有挑战性。Apache
    Spark是从头开始设计的，专门用于通过并行化CPU和内存使用来解决这种延迟。在探索可视化和处理大数据的工具和技术之前，让我们首先了解数据工程师、数据科学家和业务用户的可视化需求。
- en: A data engineer's perspective
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据工程师的视角
- en: 'Data engineers play a crucial role in almost every data-driven requirement:
    sourcing data from different data sources, consolidating them, cleaning and preprocessing
    them, analyzing them, and then the final reporting with visuals and dashboards.
    Their activities can be broadly stated as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师在几乎每一个数据驱动的需求中扮演着至关重要的角色：从不同数据源获取数据，整合它们，清洗和预处理它们，分析它们，然后通过可视化和仪表板进行最终报告。他们的活动可以广泛地陈述如下：
- en: Visualize the data from different sources to be able to integrate and consolidate
    it to form a single data matrix
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化来自不同来源的数据，以便将其集成和 consolida te 成一个单一的数据矩阵
- en: Visualize and find various anomalies in the data, such as missing values, outliers
    and so on (this could be while scraping, sourcing, ETLing, and so on) and get
    those fixed
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化并发现数据中的各种异常，如缺失值、异常值等（这可能是在抓取、获取、ETL 等过程中），并将其修复
- en: Advise the data scientists on the properties and characteristics of the dataset
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就数据集的属性和特征向数据科学家提供建议
- en: Explore various possible ways of visualizing the data and finalize the ones
    that are more informative and intuitive as per the business requirement
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索可视化数据的各种可能方式，并最终确定根据业务需求更具信息量和直观性的方式
- en: Observe here that the data engineers not only play a key role in sourcing and
    preparing the data, but also take a call on the most suitable visualization outputs
    for the business users. They usually work very closely to the business as well
    to have a very clear understanding on the business requirement and the specific
    problem at hand.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据工程师不仅在获取和准备数据方面起着关键作用，还会根据商业用户的需求选择最合适的可视化输出。他们通常也与业务密切合作，以对业务需求和手头的具体问题有非常清晰的理解。
- en: A data scientist's perspective
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学家的视角
- en: A data scientist's need for visualizing data is different from that of data
    engineers. Please note that in some businesses, there are professionals who play
    a dual role of data engineers and data scientists.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家对可视化数据的需求与数据工程师不同。请注意，在一些企业中，有一些专业人员既扮演数据工程师又扮演数据科学家的双重角色。
- en: 'Data scientists need to visualize data to be able to take the right decisions
    in performing statistical analysis and ensure proper execution of the analytics
    projects. They would like to slice and dice data in various possible ways to find
    hidden insights. Let''s take a look at some example requirements that a data scientist
    might have to visualize the data:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家需要可视化数据，以便在进行统计分析时做出正确的决策，并确保分析项目的正确执行。他们希望以各种可能的方式切分数据，以找到隐藏的见解。让我们看一些数据科学家可能需要可视化数据的示例要求：
- en: See the data distribution of the individual variables
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看各个变量的数据分布
- en: Visualize outliers in the data
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化数据中的异常值
- en: Visualize the percentage of missing data in a dataset for all variables
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化数据集中所有变量的缺失数据百分比
- en: Plot the correlation matrix to find the correlated variables
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制相关矩阵以找到相关的变量
- en: Plot the behavior of residuals after a regression
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制回归后残差的行为
- en: After a data cleaning or transformation activity, plot the variable again and
    see how it behaves
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据清洗或转换活动之后，重新绘制变量并观察其行为
- en: Please note that some of the things just mentioned are quite similar to the
    case of data engineers. However, data scientists could have a more scientific/statistical
    intention behind such analyses. For example, data scientists may see an outlier
    from a different perspective and treat it statistically, but a data engineer might
    think of the various options that could have triggered this.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，刚才提到的一些事情与数据工程师的情况非常相似。然而，数据科学家可能在这些分析背后有更科学/统计的意图。例如，数据科学家可能从不同的角度看待异常值并进行统计处理，而数据工程师可能会考虑触发这种异常的各种选项。
- en: A business user's perspective
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业用户的视角
- en: 'A business user''s perspective is completely different from that of data engineers
    or data scientists. Business users are usually the consumers of information! They
    would like to extract more and more information from the data, and for that, the
    correct visuals play a key role. Also, most business questions are more complex
    and causal these days. The old-school reports are no longer enough. Let''s look
    at some example queries that business users would like to extract from reports,
    visuals, and dashboards:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个商业用户的视角与数据工程师或数据科学家完全不同。商业用户通常是信息的消费者！他们希望从数据中提取更多信息，为此，正确的可视化起着关键作用。此外，大多数商业问题如今更加复杂和因果关系。老式报告已经不再足够。让我们看一些商业用户希望从报告、可视化和仪表板中提取的示例查询：
- en: Who are the high-value customers in so-and-so region?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某个地区，高价值客户是谁？
- en: What are the common characteristics of these customers?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些客户的共同特征是什么？
- en: Predict whether a new customer would be high-value
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测新客户是否会是高价值客户
- en: Advertising in which media would give maximum ROI?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在哪种媒体上做广告会带来最大的投资回报？
- en: What if I do not advertise in a newspaper?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我不在报纸上做广告会怎样？
- en: What are the factors influencing a customer's buying behavior?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 影响客户购买行为的因素是什么？
- en: Data visualization tools
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化工具
- en: Out of the many different visualization options, choosing the right visual depends
    on specific requirements. Similarly, selecting a visualization tool depends on
    both the target audience and the business requirement.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多不同的可视化选项中，选择合适的可视化取决于具体的需求。同样，选择可视化工具取决于目标受众和业务需求。
- en: Data scientists or data engineers would prefer a more interactive console for
    quick and dirty analysis. The visuals they use are usually not intended for business
    users. They would like to dissect the data in every possible way to get more meaningful
    insights. So, they usually prefer a notebook-type interface that supports these
    activities. A notebook is an interactive computational environment where they
    can combine code chunks and plot data for explorations. There are notebooks such
    as **IPython**/**Jupyter** or **DataBricks**, to name a few available options.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家或数据工程师更倾向于一个更具交互性的控制台进行快速而肮脏的分析。他们使用的可视化通常不是为业务用户而设计的。他们希望以各种可能的方式剖析数据，以获得更有意义的见解。因此，他们通常更喜欢支持这些活动的笔记本类型界面。笔记本是一个交互式的计算环境，他们可以在其中组合代码块和绘制数据进行探索。有一些可用选项，如**IPython**/**Jupyter**或**DataBricks**等笔记本。
- en: Business users would prefer a more intuitive and informative visual that they
    can share with each other or use to generate reports. They expect to receive the
    end result through visuals. There are hundreds and thousands of tools, including
    some popular ones such as **Tableau**, that businesses use; but quite often, developers
    have to custom-build specific types for some unique requirements and expose them
    through web applications. Microsoft's **PowerBI** and open source solutions such
    as **Zeppelin** are a few examples.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 业务用户更倾向于更直观和信息丰富的可视化，他们可以分享给彼此或用来生成报告。他们期望通过可视化收到最终结果。有数以百计的工具，包括一些流行的工具如**Tableau**，企业使用；但很多时候，开发人员必须根据一些独特的需求定制特定类型，并通过Web应用程序公开它们。微软的**PowerBI**和**Zeppelin**等开源解决方案就是其中的几个例子。
- en: IPython notebook
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IPython笔记本
- en: The IPython/Jupyter notebook on top of Spark's **PySpark** API is an excellent
    combination for data scientists to explore and visualize the data. The notebook
    internally spins up a new instance of the PySpark kernel. There are other kernels
    available; for example, the Apache **Toree** kernel can be used to support Scala
    as well.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark的**PySpark** API之上的IPython/Jupyter笔记本是数据科学家探索和可视化数据的绝佳组合。笔记本内部会启动一个新的PySpark内核实例。还有其他可用的内核；例如，Apache
    **Toree**内核可以用于支持Scala。
- en: For many data scientists, it is the default choice because of its capability
    of integrating text, code, formula, and graphics in one JSON document file. The
    IPython notebook supports `matplotlib`, which is a 2D visualization library that
    can produce production-quality visuals. Generating plots, histograms, scatterplots,
    charts, and so on becomes easy and simple. It also supports the `seaborn` library,
    which is actually built upon matplotlib but is easy to use as it provides higher
    level abstraction and hides the underlying complexities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多数据科学家来说，它是默认选择，因为它能够在一个JSON文档文件中集成文本、代码、公式和图形。IPython笔记本支持`matplotlib`，这是一个可以生成高质量可视化的2D可视化库。生成图表、直方图、散点图、图表等变得简单而容易。它还支持`seaborn`库，实际上是建立在matplotlib之上的，但易于使用，因为它提供了更高级的抽象并隐藏了底层复杂性。
- en: Apache Zeppelin
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Zeppelin
- en: Apache Zeppelin is built upon JVM and integrates well with Apache Spark. It
    is a browser-based or frontend-based open source tool that has its own notebook.
    It supports Scala, Python, R, SQL, and other graphical modules to serve as a visualization
    solution not only to business users but also to data scientists. In the following
    section on visualization techniques, we will take a look at how Zeppelin supports
    Apache Spark code to generate interesting visuals. You need to download Zeppelin
    ([https://zeppelin.apache.org/](https://zeppelin.apache.org/)) in order to try
    out the examples.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Zeppelin是建立在JVM之上的，并与Apache Spark很好地集成在一起。它是一个基于浏览器或前端的开源工具，具有自己的笔记本。它支持Scala、Python、R、SQL和其他图形模块，不仅为业务用户提供可视化解决方案，也为数据科学家提供支持。在下面关于可视化技术的部分，我们将看看Zeppelin如何支持Apache
    Spark代码生成有趣的可视化。您需要下载Zeppelin（[https://zeppelin.apache.org/](https://zeppelin.apache.org/)）来尝试这些示例。
- en: Third-party tools
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三方工具
- en: There are many products that support Apache Spark as the underlying data processing
    engine and are built to fit in the organizational big data ecosystem. While leveraging
    the processing power of Spark, they provide the visualization interface that supports
    a variety of interactive visuals, and they also support collaboration. Tableau
    is one such example of a tool that leverages Spark.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多产品支持Apache Spark作为底层数据处理引擎，并且构建以适应组织的大数据生态系统。在利用Spark的处理能力的同时，它们提供了支持各种交互式可视化的可视化界面，它们还支持协作。Tableau就是一个利用Spark的工具的例子。
- en: Data visualization techniques
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化技术
- en: Data visualization is at the center of every stage in the data analytics life
    cycle. It is especially important for exploratory analysis and for communicating
    results. In either case, the goal is to transform data into a format that's efficient
    for human consumption. The approach of delegating the transformation to client-side
    libraries does not scale to large datasets. The transformation has to happen on
    the server side, sending only the relevant data to the client for rendering. Most
    of the common transformations are available in Apache Spark out of the box. Let's
    have a closer look at these transformations.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化是数据分析生命周期的每个阶段的核心。它对于探索性分析和沟通结果尤为重要。在任何情况下，目标都是将数据转换为人类消费的高效格式。将转换委托给客户端库的方法无法扩展到大型数据集。转换必须在服务器端进行，仅将相关数据发送到客户端进行渲染。大多数常见的转换在Apache
    Spark中都是开箱即用的。让我们更仔细地看看这些转换。
- en: Summarizing and visualizing
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结和可视化
- en: '**Summarizing and visualizing** is a technique used by many **Business Intelligence**
    (**BI**) tools. Since summarization will be a concise dataset regardless of the
    size of the underlying dataset, the graphs look simple enough and easy to render.
    There are various ways to summarize the data such as aggregating, pivoting, and
    so on. If the rendering tool supports interactivity and has drill-down capabilities,
    the user gets to explore subsets of interest from the complete data. We will show
    how to do the summarization rapidly and interactively with Spark through the Zeppelin
    notebook.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结和可视化**是许多**商业智能**（**BI**）工具使用的技术。由于总结将是一个简洁的数据集，无论底层数据集的大小如何，图表看起来都足够简单且易于呈现。有各种各样的数据总结方法，例如聚合、透视等。如果呈现工具支持交互性并具有钻取功能，用户可以从完整数据中探索感兴趣的子集。我们将展示如何通过Zeppelin笔记本快速和交互式地使用Spark进行总结。'
- en: The following image shows the Zeppelin notebook with source code and a grouped
    bar chart. The dataset contains 24 observations with sales information of two
    products, **P1** and **P2**, for 12 months. The first cell contains code to read
    a text file and register data as a temporary table. This cell uses the default
    Spark interpreter using Scala. The second cell uses the SQL interpreter which
    is supported by out-of-the-box visualization options. You can switch the chart
    types by clicking on the right icon. Note that the visualization is similar for
    either Scala or Python or R interpreters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图片显示了带有源代码和分组条形图的Zeppelin笔记本。数据集包含24个观测值，其中包含两种产品**P1**和**P2**的12个月销售信息。第一个单元格包含读取文本文件并将数据注册为临时表的代码。这个单元格使用默认的Spark解释器使用Scala。第二个单元格使用了SQL解释器，该解释器支持开箱即用的可视化选项。您可以通过点击右侧图标切换图表类型。请注意，无论是Scala还是Python还是R解释器，可视化都是相似的。
- en: 'Summarization examples are as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 总结示例如下：
- en: 'The source code to read data and register as a SQL View:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据并注册为SQL视图的源代码：
- en: '**Scala (default)**:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala（默认）**：'
- en: '![Summarizing and visualizing](img/image_09_001.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![总结和可视化](img/image_09_001.jpg)'
- en: '**PySpark**:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**PySpark**：'
- en: '![Summarizing and visualizing](img/image_09_002.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![总结和可视化](img/image_09_002.jpg)'
- en: '**R**:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**R**：'
- en: '![Summarizing and visualizing](img/image_09_003.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![总结和可视化](img/image_09_003.jpg)'
- en: All three are reading the data file and registering as a temporary SQL view.
    Note that minor differences exist in the preceding three scripts. For example,
    we need to remove the header row for R and set the column names. The next step
    is to produce the visualization, which works from the `%sql` interpreter. The
    following first picture shows the script to produce the quarterly sales for each
    product. It also shows the chart types available out of the box, followed by the
    settings and their selection. You can collapse the settings after making selections.
    You can even make use of Zeppelin's in-built dynamic forms, say to accept a product
    during runtime. The second picture shows the actual output.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个都是读取数据文件并将其注册为临时SQL视图。请注意，在前面的三个脚本中存在一些细微差异。例如，我们需要删除R的标题行并设置列名。下一步是生成可视化，它可以从`%sql`解释器中工作。下面的第一张图片显示了生成每种产品季度销售额的脚本。它还显示了开箱即用的图表类型，然后是设置及其选择。在进行选择后，您可以折叠设置。您甚至可以利用Zeppelin内置的动态表单，比如在运行时接受一个产品。第二张图片显示了实际输出。
- en: The script to produce quarterly sales for two products:![Summarizing and visualizing](img/image_09_004.jpg)
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于生成两种产品季度销售额的脚本：![总结和可视化](img/image_09_004.jpg)
- en: The output produced:![Summarizing and visualizing](img/image_09_005.jpg)
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成的输出：![总结和可视化](img/image_09_005.jpg)
- en: We have seen Zeppelin's inbuilt visualization in the preceding example. But
    we can use other plotting libraries as well. Our next example utilizes the PySpark
    interpreter with matplotlib in Zeppelin to draw a histogram. This example code
    computes bin intervals and bin counts using RDD's histogram function and brings
    in just this summarized data to the driver node. Frequency is provided as weights
    while plotting the bins to give the same visual understanding as a normal histogram
    but with very low data transfer.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的例子中已经看到了Zeppelin内置的可视化。但是我们也可以使用其他绘图库。我们的下一个例子利用了PySpark解释器和Zeppelin中的matplotlib来绘制直方图。这个例子的代码使用RDD的直方图函数计算箱子间隔和箱子计数，并将这些总结数据带到驱动节点。在绘制箱子时，频率被作为权重提供，以便给出与普通直方图相同的视觉理解，但数据传输非常低。
- en: 'The histogram examples are as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图示例如下：
- en: '![Summarizing and visualizing](img/image_09_006.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![总结和可视化](img/image_09_006.jpg)'
- en: 'This is the generated output (it may come as a separate window):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成的输出（可能会显示为单独的窗口）：
- en: '![Summarizing and visualizing](img/image_09_007.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![总结和可视化](img/image_09_007.jpg)'
- en: In the preceding example of preparing histograms, note that the bucket counts
    could be parameterized using the inbuilt dynamic forms support.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的直方图准备示例中，请注意可以使用内置的动态表单支持来参数化桶计数。
- en: Subsetting and visualizing
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子集和可视化
- en: Sometimes, we may have a large dataset but we may be interested only in a subset
    of it. Divide and conquer is one approach where we explore a small portion of
    data at a time. Spark allows data subsetting using SQL-like filters and aggregates
    on row-column datasets as well as graph data. Let us perform SQL subsetting first,
    followed by a GraphX example.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能有一个大型数据集，但我们可能只对其中的一个子集感兴趣。分而治之是一种方法，我们可以一次探索一小部分数据。Spark允许使用类似SQL的过滤器和聚合在行列数据集以及图形数据上对数据进行子集化。让我们先进行SQL子集化，然后进行一个GraphX示例。
- en: The following example takes bank data available with Zeppelin and extracts a
    few relevant columns of data related to managers only. It uses the `google visualization
    library` to plot a bubble chart. The data was read using PySpark. Data subsetting
    and visualization are carried out using R. Note that we can choose any of the
    interpreters to these tasks and the choice here was just arbitrary.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例获取了Zeppelin提供的银行数据，并提取了与仅经理相关的几列数据。它使用了`google可视化库`来绘制气泡图。数据是使用PySpark读取的。数据子集和可视化是使用R进行的。请注意，我们可以选择任何解释器来执行这些任务，这里的选择只是任意的。
- en: 'The data subsetting example using SQL is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SQL进行数据子集示例如下：
- en: Read data and register the SQL view:![Subsetting and visualizing](img/image_09_008.jpg)
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据并注册SQL视图：![子集和可视化](img/image_09_008.jpg)
- en: Subset managers' data and show a bubble plot:![Subsetting and visualizing](img/image_09_009.jpg)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 子集经理的数据并显示气泡图：![子集和可视化](img/image_09_009.jpg)
- en: 'The next example demonstrates some GraphX processing that uses data provided
    by the **Stanford Network Analysis Project** (**SNAP**). The script extracts a
    subgraph covering a given set of nodes. Here, each node represents a Facebook
    ID and an edge represents a connection between the two nodes (or people). Further,
    the script identifies direct connections for a given node (id: 144). These are
    the level 1 nodes. Then it identifies the direct connections to these *level 1
    nodes*, which form *level 2 nodes* to the given node. Even though a second-level
    contact may be connected to more than one first-level contact, it is shown only
    once thereby forming a connection tree without crisscrossing edges. Since the
    connection tree may have too many nodes, the script limits up to three connections
    at level 1 as well as level 2, thereby showing only 12 nodes under the given root
    node (one root + three level 1 nodes + three of each level 2 nodes).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '下一个示例演示了使用**Stanford Network Analysis Project** (**SNAP**)提供的数据进行的一些GraphX处理。脚本提取了覆盖给定节点集的子图。在这里，每个节点代表一个Facebook
    ID，边代表两个节点（或人）之间的连接。此外，脚本识别了给定节点（id: 144）的直接连接。这些是级别1节点。然后它识别了这些*级别1节点*的直接连接，这些连接形成了给定节点的*级别2节点*。即使第二级联系可能连接到多个第一级联系，但它只显示一次，从而形成一个没有交叉边的连接树。由于连接树可能有太多的节点，脚本限制了级别1和级别2的连接，因此在给定根节点下只显示12个节点（一个根+三个级别1节点+每个级别2节点三个）。'
- en: '**Scala**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note the use of `z.put` and `z.get`. This is a mechanism to exchange data between
    cells/interpreters in Zeppelin.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`z.put`和`z.get`的使用。这是在Zeppelin中在单元格/解释器之间交换数据的机制。
- en: Now that we have created a data frame with level 1 contacts and their direct
    contacts, we are all set to draw the tree. The following script uses the graph
    visualization library igraph and Spark R.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个包含级别1联系人及其直接联系人的数据框，我们已经准备好绘制树了。以下脚本使用了图形可视化库igraph和Spark R。
- en: 'Extract nodes and edges. Plot the tree:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 提取节点和边。绘制树：
- en: '![Subsetting and visualizing](img/image_09_010.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![子集和可视化](img/image_09_010.jpg)'
- en: 'The preceding script gets parent nodes from the nodes table, which are the
    parents of level 2 nodes as well as direct connections to the given head node.
    Ordered pairs of head nodes and level 1 nodes are created and assigned to `edges1`.
    The next step explodes the array of level 2 nodes to form one row per each array
    element. The data frame thus obtained is transposed and pasted to form edge pairs.
    Since paste converts data into strings, they are reconverted to numeric. These
    are the level 2 edges. The level 1 and level 2 edges are concatenated to form
    a single list of edges. These are fed to form the graph as shown next. Note that
    the smudge in `headNode` is 144, though not visible in the following figure:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的脚本从节点表中获取父节点，这些父节点是级别2节点的父节点，也是给定头节点的直接连接。创建头节点和级别1节点的有序对，并分配给`edges1`。下一步将级别2节点的数组展开，形成每个数组元素一行。因此获得的数据框被转置并粘贴在一起形成边对。由于粘贴将数据转换为字符串，因此它们被重新转换为数字。这些是级别2的边。级别1和级别2的边被连接在一起形成一个边的单个列表。这些被用来形成下面显示的图形。请注意，`headNode`中的模糊是144，尽管在下图中看不到：
- en: '![Subsetting and visualizing](img/image_09_011.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![子集和可视化](img/image_09_011.jpg)'
- en: Connection tree for the given node
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 给定节点的连接树
- en: Sampling and visualizing
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽样和可视化
- en: Sampling and visualizing has been used by statisticians for a long time. Through
    sampling techniques, we take a portion of the dataset and work on it. We will
    show how Spark supports different sampling techniques such as **random sampling**,
    **stratified sampling**, and **sampleByKey**, and so on. The following example
    is created using the Jupyter notebook, PySpark kernel, and `seaborn` library.
    The data file is the bank dataset provided by Zeppelin. The first plot shows the
    balance for each education category. The colors indicate marital status.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样和可视化长期以来一直被统计学家使用。通过抽样技术，我们可以取得数据集的一部分并对其进行处理。我们将展示Spark如何支持不同的抽样技术，如**随机抽样**、**分层抽样**和**sampleByKey**等。以下示例是使用Jupyter笔记本、PySpark内核和`seaborn`库创建的。数据文件是Zeppelin提供的银行数据集。第一个图显示了每个教育类别的余额。颜色表示婚姻状况。
- en: 'Read data and take a random sample of 5%:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据并随机抽样5%：
- en: '![Sampling and visualizing](img/image_09_012.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![抽样和可视化](img/image_09_012.jpg)'
- en: 'Render data using `stripplot`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`stripplot`渲染数据：
- en: '![Sampling and visualizing](img/image_09_013.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![抽样和可视化](img/image_09_013.jpg)'
- en: 'The preceding example showed a random sample of available data, which is much
    better than completely plotting the population. But if the levels in the categorical
    variable of interest (in this case, `education`) are too many, then this plot
    becomes hard to read. For example, if we want to plot the balance for job instead
    of `education`, there will be too many strips, making the picture look cluttered.
    Instead, we can take desired sample of desired categorical levels only and then
    examine the data. Note that this is different from subsetting because we will
    not be able to specify the sample ratio in normal subsetting using SQL `WHERE`
    clauses. We need to use `sampleByKey` for that, as shown next. The following example
    takes only two jobs and with specific sampling ratios:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例展示了可用数据的随机样本，这比完全绘制总体要好得多。但是，如果感兴趣的分类变量（在本例中是`education`）的级别太多，那么这个图就会变得难以阅读。例如，如果我们想要绘制`job`的余额而不是`education`，那么会有太多的条带，使图片看起来凌乱。相反，我们可以只取所需分类级别的所需样本，然后检查数据。请注意，这与子集不同，因为我们无法使用SQL的`WHERE`子句来指定正常子集中的样本比例。我们需要使用`sampleByKey`来做到这一点，如下所示。以下示例仅采用两种工作和特定的抽样比例：
- en: '![Sampling and visualizing](img/image_09_014.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![抽样和可视化](img/image_09_014.jpg)'
- en: Stratified sampling
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 分层抽样
- en: Modeling and visualizing
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模和可视化
- en: 'Modeling and visualizing are possible with Spark''s **MLLib** and **ML** modules.
    Spark''s unified programming model and diverse programming interfaces enable combining
    these techniques into a single environment to get insights from the data. We have
    already covered most of the modeling techniques in the previous chapters. However,
    here are a few examples for your reference:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 建模和可视化是可能的，使用Spark的**MLLib**和**ML**模块。Spark的统一编程模型和多样的编程接口使得将这些技术结合到一个单一的环境中以从数据中获得洞察成为可能。我们已经在前几章中涵盖了大部分建模技术。然而，以下是一些示例供您参考：
- en: '**Clustering**: K-means, Gaussian Mixture Modeling'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：K均值，高斯混合建模'
- en: '**Classification and regression**: Linear model, Decision tree, Naïve Bayes,
    SVM'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类和回归**：线性模型，决策树，朴素贝叶斯，支持向量机'
- en: '**Dimensionality reduction**: Singular value decomposition, Principal component
    analysis'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：奇异值分解，主成分分析'
- en: '**Collaborative Filtering**'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤**'
- en: '**Statistical testing**: Correlations, Hypothesis testing'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计测试**：相关性，假设检验'
- en: 'The following example takes a model from the [Chapter 7](ch07.xhtml "Chapter 7. 
    Extending Spark with SparkR"), *Extending Spark with SparkR*, which tries to predict
    the students'' pass or fail results using a Naïve Bayes model. The idea is to
    make use of the out-of-the-box functionality provided by Zeppelin and inspect
    the model behavior. So, we load the data, perform data preparation, build the
    model, and run the predictions. Then we register the predictions as an SQL view
    so as to harness inbuilt visualization:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例来自[第7章](ch07.xhtml "第7章。 用SparkR扩展Spark")，*用SparkR扩展Spark*，它尝试使用朴素贝叶斯模型预测学生的及格或不及格结果。这个想法是利用Zeppelin提供的开箱即用的功能，并检查模型的行为。因此，我们加载数据，进行数据准备，构建模型，并运行预测。然后我们将预测注册为SQL视图，以便利用内置的可视化：
- en: '[PRE1]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Modeling and visualizing](img/image_09_015.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![建模和可视化](img/image_09_015.jpg)'
- en: The next step is to write the desired SQL query and define the appropriate settings.
    Note the use of the UNION operator in SQL and the way the match column is defined.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是编写所需的SQL查询并定义适当的设置。请注意SQL中UNION运算符的使用以及匹配列的定义方式。
- en: 'Define SQL to view model performance:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 定义SQL以查看模型性能：
- en: '![Modeling and visualizing](img/image_09_016.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![建模和可视化](img/image_09_016.jpg)'
- en: 'The following picture helps us understand where the model prediction deviates
    from the actual data. Such visualizations are helpful in taking business users''
    inputs since they do not require any prior knowledge of data science to comprehend:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片帮助我们理解模型预测与实际数据的偏差。这样的可视化有助于接受业务用户的输入，因为他们不需要任何数据科学的先验知识来理解：
- en: '![Modeling and visualizing](img/image_09_017.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![建模和可视化](img/image_09_017.jpg)'
- en: Visualize model performance
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化模型性能
- en: We usually evaluate statistical models with error metrics, but visualizing them
    graphically instead of seeing the numbers makes them more intuitive because it
    is usually easier to understand a diagram than numbers in a table. For example,
    the preceding visualization can be easily understood by people outside the data
    science community as well.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用误差度量来评估统计模型，但是将它们以图形方式可视化而不是看到数字，使它们更直观，因为通常更容易理解图表而不是表中的数字。例如，前面的可视化也可以被数据科学社区之外的人轻松理解。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored most of the widely used visualization tools and
    techniques supported on Spark in a big data setup. We explained some of the techniques
    with code snippets for better understanding of visualization needs at different
    stages of the data analytics life cycle. We also saw how business requirements
    are satisfied with proper visualization techniques by addressing the challenges
    of big data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了在大数据设置中支持的大多数常用可视化工具和技术。我们通过代码片段解释了一些技术，以更好地理解数据分析生命周期不同阶段的可视化需求。我们还看到了如何通过适当的可视化技术满足业务需求，以解决大数据的挑战。
- en: The next chapter is the culmination of all the concepts explained till now .
    We will walk through the Complete Data Analysis Life Cycle through an example
    dataset.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章是到目前为止解释的所有概念的高潮。我们将通过一个示例数据集走完完整的数据分析生命周期。
- en: References
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考
- en: '21 Essential Data Visualization Tools: [http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html](http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 21个必备的数据可视化工具：[http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html](http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html)
- en: 'Apache Zeppelin notebook home page: [https://zeppelin.apache.org/](https://zeppelin.apache.org/)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Zeppelin笔记本主页：[https://zeppelin.apache.org/](https://zeppelin.apache.org/)
- en: 'Jupyter notebook home page: [https://jupyter.org/](https://jupyter.org/)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter笔记本主页：[https://jupyter.org/](https://jupyter.org/)
- en: 'Using IPython Notebook with Apache Spark: [http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/](http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Spark的IPython Notebook：[http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/](http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/)
- en: 'Apache Toree, which enables interactive workloads between applications and
    Spark cluster. Can be used with jupyter to run Scala code: [https://toree.incubator.apache.org/](https://toree.incubator.apache.org/)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Toree，可在应用程序和Spark集群之间进行交互式工作负载。可与jupyter一起使用以运行Scala代码：[https://toree.incubator.apache.org/](https://toree.incubator.apache.org/)
- en: 'GoogleVis package using R: [https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html](https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R的GoogleVis软件包：[https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html](https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html)
- en: 'GraphX Programming Guide: [http://spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GraphX编程指南：[http://spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html)
- en: 'Going viral with R''s igraph package: [https://www.r-bloggers.com/going-viral-with-rs-igraph-package/](https://www.r-bloggers.com/going-viral-with-rs-igraph-package/)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R的igraph软件包进行病毒式传播：[https://www.r-bloggers.com/going-viral-with-rs-igraph-package/](https://www.r-bloggers.com/going-viral-with-rs-igraph-package/)
- en: 'Plotting with categorical data: [https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial](https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分类数据绘图：[https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial](https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial)
- en: Data source citations
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据来源引用
- en: '**Bank data source (citation)**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**银行数据来源（引用）**'
- en: '[Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for
    Bank Direct Marketing: An Application of the CRISP-DM Methodology'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Moro等人，2011] S. Moro，R. Laureano和P. Cortez。使用数据挖掘进行银行直接营销：CRISP-DM方法论的应用'
- en: In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling
    Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011\. EUROSIS
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在P. Novais等人（编），欧洲模拟与建模会议 - ESM'2011论文集，第117-121页，葡萄牙吉马良斯，2011年10月。EUROSIS
- en: Available at [pdf] [http://hdl.handle.net/1822/14838](http://hdl.handle.net/1822/14838)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可在[pdf] [http://hdl.handle.net/1822/14838](http://hdl.handle.net/1822/14838)找到
- en: '[bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt'
- en: '**Facebook data Source (citation)**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**Facebook数据来源（引用）**'
- en: J. McAuley and J. Leskovec. Learning to Discover Social Circles in Ego Networks.
    NIPS, 2012.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. McAuley和J. Leskovec。学习在自我网络中发现社交圈。NIPS，2012年。
