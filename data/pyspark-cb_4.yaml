- en: Preparing Data for Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为建模准备数据
- en: 'In this chapter, we will cover how to clean up your data and prepare it for
    modeling. You will learn the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何清理数据并为建模做准备。您将学习以下内容：
- en: Handling duplicates
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理重复项
- en: Handling missing observations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失观察
- en: Handling outliers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理异常值
- en: Exploring descriptive statistics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索描述性统计
- en: Computing correlations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算相关性
- en: Drawing histograms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制直方图
- en: Visualizing interactions between features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化特征之间的相互作用
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Now that we have a thorough understanding of how RDDs and DataFrames work and
    what they can do, we can start preparing ourselves and our data for modeling.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对RDD和DataFrame的工作原理以及它们的功能有了深入的了解，我们可以开始为建模做准备了。
- en: 'Someone famous (Albert Einstein) once said (paraphrasing):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有名的人（阿尔伯特·爱因斯坦）曾经说过（引用）：
- en: '"The universe and the problems with any dataset are infinite, and I am not
    sure about the former."'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '"宇宙和任何数据集的问题都是无限的，我对前者不太确定。"'
- en: The preceding is of course a joke. However, any dataset you work with, be it
    acquired at work, found online, collected yourself, or obtained through any other
    means, is dirty until proven otherwise; you should not trust it, you should not
    play with it, you should not even look at it until such time that you have proven
    to yourself that it is sufficiently clean (there is no such thing as totally clean).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的话当然是一个笑话。然而，您处理的任何数据集，无论是在工作中获取的、在线找到的、自己收集的，还是通过其他方式获取的，都是脏的，直到证明为止；您不应该信任它，不应该玩弄它，甚至不应该看它，直到您自己证明它足够干净（没有完全干净的说法）。
- en: 'What problems can your dataset have? Well, to name a few:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据集可能会出现哪些问题？嗯，举几个例子：
- en: '**Duplicated observations**: These arise through systemic and operator''s faults'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复的观察**：这些是由系统和操作员的错误导致的'
- en: '**Missing observations**: These can emerge due to sensor problems, respondents''
    unwillingness to provide an answer to a question, or simply some data corruption'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失观察**：这可能是由于传感器问题、受访者不愿回答问题，或者仅仅是一些数据损坏导致的'
- en: '**Aanomalous observations**: Observations that, when you look at them, stand
    out when compared with the rest of the dataset or a population'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常观察**：与数据集或人口其他部分相比，观察结果在观察时显得突出'
- en: '**Encoding**: Text fields that are not normalized (for example, words are not
    stemmed or use synonyms), in different languages, or you can encounter gibberish
    text input, and date and date time fields may not encoded the same way'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码**：文本字段未经规范化（例如，单词未经词干处理或使用同义词），使用不同语言，或者您可能遇到无意义的文本输入，日期和日期时间字段可能没有以相同的方式编码'
- en: '**Untrustworthy answers (true especially for surveys)**: When respondents lie
    for any reason; this type of dirty data is much harder to work with and clean
    up'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可信的答案（尤其是调查）**：受访者因任何原因而撒谎；这种脏数据更难处理和清理'
- en: As you can see, your data might be plagued by thousands upon thousands of traps
    that are just waiting for you to fall for them. Cleaning up the data and getting
    familiar with it is what we (as data scientists) do 80% of the time (the remaining
    20% we spend building models and complaining about cleaning data). So fasten your
    seatbelt and prepare for *a bumpy ride* that is necessary for us to trust the
    data that we have and get familiar with it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，您的数据可能会受到成千上万个陷阱的困扰，它们正等待着您去陷入其中。清理数据并熟悉数据是我们（作为数据科学家）80%的时间所做的事情（剩下的20%我们花在建模和抱怨清理数据上）。所以系好安全带，准备迎接*颠簸的旅程*，这是我们信任我们拥有的数据并熟悉它所必需的。
- en: 'In this chapter, we will work with a small dataset of `22` records:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个包含`22`条记录的小数据集：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Throughout the subsequent recipes, we will clean up the preceding dataset and
    learn a little bit more about it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的教程中，我们将清理前面的数据集，并对其进行更深入的了解。
- en: Handling duplicates
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理重复项
- en: Duplicates show up in data for many reasons, but sometimes it's really hard
    to spot them. In this recipe, we will show you how to spot the most common ones
    and handle them using Spark.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中出现重复项的原因很多，但有时很难发现它们。在这个教程中，我们将向您展示如何发现最常见的重复项，并使用Spark进行处理。
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. If you
    do not have one, you might want to go back to [Chapter 1](part0026.html#OPEK0-dc04965c02e747b9b9a057725c821827),
    *Installing and Configuring Spark,* and follow the recipes you will find there.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此教程，您需要一个可用的Spark环境。如果没有，请返回[第1章](part0026.html#OPEK0-dc04965c02e747b9b9a057725c821827)，*安装和配置Spark*，并按照那里找到的教程进行操作。
- en: We will work on the dataset from the introduction. All the code that you will
    need in this chapter can be found in the GitHub repository we set up for the book: [http://bit.ly/2ArlBck](http://bit.ly/2ArlBck).
    Go to `Chapter04` and open the [4.Preparing data for modeling.ipynb](https://github.com/drabastomek/PySparkCookbook/blob/devel/Chapter04/4.Preparing%20data%20for%20modeling.ipynb) notebook.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用介绍中的数据集。本章中所需的所有代码都可以在我们为本书设置的GitHub存储库中找到：[http://bit.ly/2ArlBck](http://bit.ly/2ArlBck)。转到`Chapter04`并打开[4.Preparing
    data for modeling.ipynb](https://github.com/drabastomek/PySparkCookbook/blob/devel/Chapter04/4.Preparing%20data%20for%20modeling.ipynb)笔记本。
- en: No other prerequisites are required.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'A duplicate is a record in your dataset that appears more than once. It is
    an exact copy. Spark DataFrames have a convenience method to remove the duplicated
    rows, the `.dropDuplicates()` transformation:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 重复项是数据集中出现多次的记录。它是一个完全相同的副本。Spark DataFrames有一个方便的方法来删除重复的行，即`.dropDuplicates()`转换：
- en: 'Check whether any rows are duplicated, as follows:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查是否有任何重复行，如下所示：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If any are duplicates, remove them:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有重复项，请删除它们：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works...
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'You should know this one by now, but the `.count()` method counts how many
    rows there are in our DataFrame. The second command checks how many distinct rows
    we have. Execute these two commands on our `dirty_data`. DataFrame produces `(22,
    21)` as the result. So, we now know that we have two records in our dataset that
    are exact copies of each other. Let''s see which ones:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该知道这个了，但是`.count()`方法计算我们的DataFrame中有多少行。第二个命令检查我们有多少个不同的行。在我们的`dirty_data`
    DataFrame上执行这两个命令会产生`(22, 21)`的结果。因此，我们现在知道我们的数据集中有两条完全相同的记录。让我们看看哪些：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's unpack what's happening here. First, we use the `.groupby(...)` method
    to define what columns to use for the aggregation; in this example, we essentially
    use all of them as we want to find all the distinct combinations of all the columns
    in our dataset. Next, we count how many times such a combination of values occurs
    using the `.count()` method; the method adds the `count` column to our dataset.
    Using the `.filter(...)` method, we select all the rows that occur in our dataset
    more than once and print them to the screen using the `.show()` action.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解开这里发生的事情。首先，我们使用`.groupby(...)`方法来定义用于聚合的列；在这个例子中，我们基本上使用了所有列，因为我们想找到数据集中所有列的所有不同组合。接下来，我们使用`.count()`方法计算这样的值组合发生的次数；该方法将`count`列添加到我们的数据集中。使用`.filter(...)`方法，我们选择数据集中出现多次的所有行，并使用`.show()`操作将它们打印到屏幕上。
- en: 'This produces the following result:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下结果：
- en: '![](img/00078.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00078.jpeg)'
- en: So, the row with `Id` equal to `16` is the duplicated one. So, let's drop it
    using the `.dropDuplicates(...)` method. Finally, running the `full_removed.count()`
    command confirms that we now have 21 records.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`Id`等于`16`的行是重复的。因此，让我们使用`.dropDuplicates(...)`方法将其删除。最后，运行`full_removed.count()`命令确认我们现在有21条记录。
- en: There's more...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Well, there's more to it, as you might imagine. There are still some records
    that are duplicated in our `full_removed` DataFrame. Let's have a closer look.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，还有更多的内容，你可能会想象。在我们的`full_removed` DataFrame中仍然有一些重复的记录。让我们仔细看看。
- en: Only IDs differ
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 只有ID不同
- en: 'If you collect data over time, you might record the same data but with different
    IDs. Let''s check whether our DataFrame has any such records. The following snippet
    will help you do this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您随时间收集数据，可能会记录具有不同ID但相同数据的数据。让我们检查一下我们的DataFrame是否有这样的记录。以下代码片段将帮助您完成此操作：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Just like before, we first group by all the columns but we exclude the `''Id''`
    column, then count how many records we get given from this grouping, and finally
    we extract those with `''count > 1''` and show them on the screen. After running
    the preceding code, here''s what we get:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 就像以前一样，我们首先按所有列分组，但是我们排除了`'Id'`列，然后计算给定此分组的记录数，最后提取那些具有`'count > 1'`的记录并在屏幕上显示它们。运行上述代码后，我们得到以下结果：
- en: '![](img/00079.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00079.jpeg)'
- en: 'As you can see, we have four records with different IDs but that are the same
    cars: the `BMW` `440i Coupe` and the `Hyundai` `G80 AWD`.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们有四条不同ID但是相同车辆的记录：`BMW` `440i Coupe`和`Hyundai` `G80 AWD`。
- en: 'We could also check the counts, like before:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以像以前一样检查计数：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `.dropDuplicates(...)` method can handle such situations easily. All we
    need to do is to pass to the `subset` parameter a list of all the columns we want
    it to consider while searching for the duplicates. Here''s how:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`.dropDuplicates(...)`方法可以轻松处理这种情况。我们需要做的就是将要考虑的所有列的列表传递给`subset`参数，以便在搜索重复项时使用。方法如下：'
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once again, we select all the columns but the `''Id''` columns to define which
    columns to use to determine the duplicates. If we now count the total number of
    rows in the `id_removed` DataFrame, we should get `19`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们选择除了`'Id'`列之外的所有列来定义重复的列。如果我们现在计算`id_removed` DataFrame中的总行数，应该得到`19`：
- en: '![](img/00080.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00080.jpeg)'
- en: And that's precisely what we got!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们得到的！
- en: ID collisions
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ID碰撞
- en: You might also assume that if there are two records with the same ID, they are
    duplicated. Well, while this might be true, we would have already removed them
    by now when dropping the records based on all the columns. Thus, at this point,
    any duplicated IDs are more likely collisions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还会假设，如果有两条具有相同ID的记录，它们是重复的。嗯，虽然这可能是真的，但是当我们基于所有列删除记录时，我们可能已经删除了它们。因此，在这一点上，任何重复的ID更可能是碰撞。
- en: 'Duplicated IDs might arise for a multitude of reasons: an instrumentation error
    or insufficient data structure to store the IDs, or if the IDs represent some
    hash function of the record elements, there might be collisions arising from the
    choice of the hash function. These are just a few of the reasons why you might
    have duplicated IDs but the records are not really duplicated.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 重复的ID可能出现多种原因：仪器误差或存储ID的数据结构不足，或者如果ID表示记录元素的某个哈希函数，可能会出现哈希函数的选择引起的碰撞。这只是您可能具有重复ID但记录实际上并不重复的原因中的一部分。
- en: 'Let''s check whether this is true for our dataset:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下我们的数据集是否符合这一点：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this example, instead of subsetting records and then counting the records,
    then counting the distinct records, we will use the `.agg(...)` method. To this
    end, we first import all the functions from the `pyspark.sql.functions` module.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用`.agg(...)`方法，而不是对记录进行子集化，然后计算记录数，然后计算不同的记录数。为此，我们首先从`pyspark.sql.functions`模块中导入所有函数。
- en: For a list of all the functions available in `pyspark.sql.functions`, please
    refer to [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`pyspark.sql.functions`中所有可用函数的列表，请参阅[https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)。
- en: 'The two functions we''ll use will allow us to do the counting in one go: the
    `.count(...)` method counts all the records with non-null values in the specified
    column, while the `.countDistinct(...)` returns a count of distinct values in
    such a column. The `.alias(...)` method allows us to specify a friendly name for
    the columns resulting from the counting. Here''s what we get after counting:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的两个函数将允许我们一次完成计数：`.count(...)`方法计算指定列中非空值的所有记录的数量，而`.countDistinct(...)`返回这样一列中不同值的计数。`.alias(...)`方法允许我们为计数结果的列指定友好的名称。在计数之后，我们得到了以下结果：
- en: '![](img/00081.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00081.jpeg)'
- en: 'OK, so we have two records with the same IDs. Again, let''s check which IDs
    are duplicated:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们有两条具有相同ID的记录。再次，让我们检查哪些ID是重复的：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As before, we first group by the values in the `''Id''` column, and then show
    all the records with a `count` greater than `1`. Here''s what we get:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们首先按`'Id'`列中的值进行分组，然后显示所有具有大于`1`的`count`的记录。这是我们得到的结果：
- en: '![](img/00082.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00082.jpeg)'
- en: 'Well, it looks like we have two records with `''Id == 3''`. Let''s check whether
    they''re the same:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，看起来我们有两条`'Id == 3'`的记录。让我们检查它们是否相同：
- en: '![](img/00083.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00083.jpeg)'
- en: These are definitely not the same records but they share the same ID. In this
    situation, we can create a new ID that will be unique (we have already made sure
    we do not have other duplicates in our dataset). PySpark's SQL functions module
    offers a `.monotonically_increasing_id()` method that creates a unique stream
    of IDs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些绝对不是相同的记录，但它们共享相同的ID。在这种情况下，我们可以创建一个新的ID，这将是唯一的（我们已经确保数据集中没有其他重复）。PySpark的SQL函数模块提供了一个`.monotonically_increasing_id()`方法，它创建一个唯一的ID流。
- en: The `.monotonically_increasing_id()`—generated ID is guaranteed to be unique
    as long as your data lives in less than one billion partitions and with less than
    eight billion records in each. That's a pretty big number.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`.monotonically_increasing_id()`生成的ID保证是唯一的，只要你的数据存在少于十亿个分区，并且每个分区中的记录少于八十亿条。这是一个非常大的数字。'
- en: 'Here''s a snippet that will create and replace our ID column with a unique
    one:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个代码段，将创建并替换我们的ID列为一个唯一的ID：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We are creating the ID column first and then selecting all the other columns
    except the original `''Id''` column. Here''s what the new IDs look like:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建ID列，然后选择除原始`'Id'`列之外的所有其他列。新的ID看起来是这样的：
- en: '![](img/00084.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00084.jpeg)'
- en: The numbers are definitely unique. We are now ready to handle the other problems
    in our dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字绝对是唯一的。我们现在准备处理数据集中的其他问题。
- en: Handling missing observations
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失观察
- en: Missing observations are pretty much the second-most-common issue in datasets.
    These arise for many reasons, as we have already alluded to in the introduction.
    In this recipe, we will learn how to deal with them.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失观察在数据集中几乎是第二常见的问题。这是由于许多原因引起的，正如我们在介绍中已经提到的那样。在这个示例中，我们将学习如何处理它们。
- en: Getting ready
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备好了
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `new_id` DataFrame we created in the previous recipe,
    so we assume you have followed the steps to remove the duplicated records.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个示例，你需要一个可用的Spark环境。此外，我们将在前一个示例中创建的`new_id` DataFrame上进行操作，因此我们假设你已经按照步骤删除了重复的记录。
- en: No other prerequisites are required.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Since our data has two dimensions (rows and columns), we need to check the
    percentage of data missing in each row and each column to make a determination
    of what to keep, what to drop, and what to (potentially) impute:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据有两个维度（行和列），我们需要检查每行和每列中缺失数据的百分比，以确定保留什么，放弃什么，以及（可能）插补什么：
- en: 'To calculate how many missing observations there are in a row, use the following
    snippet:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要计算一行中有多少缺失观察，使用以下代码段：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To calculate how much data is missing in each column, use the following code:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要计算每列中缺少多少数据，使用以下代码：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let's walk through these step by step.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地走过这些。
- en: How it works...
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Let's now take a look at how to handle missing observations in rows and columns
    in detail in the following sections.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细看看如何处理行和列中的缺失观察。
- en: Missing observations per row
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每行的缺失观察
- en: 'To calculate how much data is missing from a row, it is easier to work with
    RDDs as we can loop through each element of an RDD''s record and count how many
    values are missing. Thus, the first thing we do is we access `.rdd` within our
    `new_id` DataFrame. Using the `.map(...)` transformation, we loop through each
    row, extract `''Id''`, and count how many times an element is missing using the `sum([c
    == None for c in row])` expression. The outcome of these operations is an RDD
    of elements that each has two values: the ID of the row and the count of missing
    values.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算一行中缺少多少数据，更容易使用RDD，因为我们可以循环遍历RDD记录的每个元素，并计算缺少多少值。因此，我们首先访问`new_id` DataFrame中的`.rdd`。使用`.map(...)`转换，我们循环遍历每一行，提取`'Id'`，并使用`sum([c
    == None for c in row])`表达式计算缺少元素的次数。这些操作的结果是每个都有两个值的元素的RDD：行的ID和缺失值的计数。
- en: Next, we only select those that have more than one missing value and `.collect()`
    those records on the driver. We then create a simple DataFrame, `.orderBy(...)`,
    by the count of missing values in a descending order and show the records.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只选择那些有多于一个缺失值的记录，并在驱动程序上`.collect()`这些记录。然后，我们创建一个简单的DataFrame，通过缺失值的计数按降序`.orderBy(...)`，并显示记录。
- en: 'The result looks as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![](img/00085.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00085.jpeg)'
- en: 'As you can see, one of the records has five out of eight values missing. Let''s
    see that record:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，其中一条记录有八个值中的五个缺失。让我们看看那条记录：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code shows that one of the `Mercedes-Benz` records has most of
    its values missing:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码显示了`Mercedes-Benz`记录中大部分值都缺失：
- en: '![](img/00086.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00086.jpeg)'
- en: 'So, we can drop the whole observation as there isn''t really much value contained
    in this record. To achieve this goal, we can use the `.dropna(...)` method of
    DataFrames: `merc_out = new_id.dropna(thresh=4)`.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以删除整个观测值，因为这条记录中实际上并没有太多价值。为了实现这个目标，我们可以使用DataFrame的`.dropna(...)`方法：`merc_out
    = new_id.dropna(thresh=4)`。
- en: If you use `.dropna()` without passing any parameters, any record that has a
    missing value will be removed.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用`.dropna()`而不传递任何参数，任何具有缺失值的记录都将被删除。
- en: We specify `thresh=4`, so we only remove the records that have a minimum of
    four non-missing values; our record has only three useful pieces of information.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定`thresh=4`，所以我们只删除具有至少四个非缺失值的记录；我们的记录只有三个有用的信息。
- en: 'Let''s confirm: running `new_id.count(), merc_out.count()` produces `(19, 18)`,
    so yes, indeed, we removed one of the records. Did we really remove the `Mercedes-Benz`
    one? Let''s check:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认一下：运行`new_id.count(), merc_out.count()`会产生`(19, 18)`，所以是的，确实，我们移除了一条记录。我们真的移除了`Mercedes-Benz`吗？让我们检查一下：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/00087.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00087.jpeg)'
- en: Missing observations per column
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每列的缺失观测值
- en: We also need to check whether there are columns with a particularly low incidence
    of useful information. There's a lot of things happening in the code we presented,
    so let's unpack it step by step.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要检查是否有某些列中有特别低的有用信息发生率。在我们提供的代码中发生了很多事情，所以让我们一步一步地拆开它。
- en: 'Let''s start with the inner list:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从内部列表开始：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We loop through all the columns in the `merc_out` DataFrame and count how many
    non-missing values we find in each column. We then divide it by the total count
    of all the rows and subtract this from 1 so we get the percentage of missing values.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历`merc_out` DataFrame中的所有列，并计算我们在每列中找到的非缺失值的数量。然后我们将它除以所有行的总数，并从中减去1，这样我们就得到了缺失值的百分比。
- en: We imported `pyspark.sql.functions` as `fn` earlier in the chapter.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章前面导入了`pyspark.sql.functions`作为`fn`。
- en: However, what we're actually doing here is not really calculating anything.
    The way Python stores this information, at this time, is just as a list of objects,
    or pointers, to certain operations. Only after we pass the list to the `.agg(...)`
    method does it get translated into PySpark's internal execution graph (which only
    gets executed when we call the `.collect()` action).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们在这里实际上做的并不是真正的计算。Python存储这些信息的方式，此时只是作为一系列对象或指针，指向某些操作。只有在我们将列表传递给`.agg(...)`方法后，它才会被转换为PySpark的内部执行图（只有在我们调用`.collect()`动作时才会执行）。
- en: The `.agg(...)` method accepts a set of parameters, not as a list object, but
    as a comma-separated list of parameters. Therefore, instead of passing the list
    itself to the `.agg(...)` method, we included `'*'` in front of the list, which
    unfolds each element of our list and passes it like a parameter to our method.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`.agg(...)`方法接受一组参数，不是作为列表对象，而是作为逗号分隔的参数列表。因此，我们没有将列表本身传递给`.agg(...)`方法，而是在列表前面包含了`''*''`，这样我们的列表的每个元素都会被展开，并像参数一样传递给我们的方法。'
- en: The `.collect()` method will return a list of one element—a `Row` object with
    aggregated information. We can transform `Row` into a dictionary using the `.asDict()`
    method and then extract all the `items` from it. This will result in a list of
    tuples, where the first element is the column name (we used the `.alias(...)`
    method to append `'_miss'` to each column) and the second element is the percentage
    of missing observations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`.collect()`方法将返回一个元素的列表——一个包含聚合信息的`Row`对象。我们可以使用`.asDict()`方法将`Row`转换为字典，然后提取其中的所有`items`。这将导致一个元组的列表，其中第一个元素是列名（我们使用`.alias(...)`方法将`''_miss''`附加到每一列），第二个元素是缺失观测值的百分比。'
- en: 'While looping through the elements of the sorted list, we just print them to
    the screen:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环遍历排序列表的元素时，我们只是将它们打印到屏幕上：
- en: '![](img/00088.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00088.jpeg)'
- en: 'Well, it looks like most of the information in the `MSRP` column is missing.
    Thus, we can drop it, as it will not bring us any useful information:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，看起来`MSRP`列中的大部分信息都是缺失的。因此，我们可以删除它，因为它不会给我们带来任何有用的信息：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We still have two columns with some missing information. Let's do something
    about them.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然有两列有一些缺失信息。让我们对它们做点什么。
- en: There's more...
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: PySpark allows you to impute the missing observations. You can either pass a
    value that every `null` or `None` in your data will be replaced with, or you can
    pass a dictionary with different values for each column with missing observations.
    In this example, we will use the latter approach and will specify a ratio between
    the fuel economy and displacement, and between the number of cylinders and displacement.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark允许你填补缺失的观测值。你可以传递一个值，所有数据中的`null`或`None`都将被替换，或者你可以传递一个包含不同值的字典，用于每个具有缺失观测值的列。在这个例子中，我们将使用后一种方法，并指定燃油经济性和排量之间的比例，以及气缸数和排量之间的比例。
- en: 'First, let''s create our dictionary:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建我们的字典：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we are effectively calculating our multipliers. In order to replace the
    missing values in the fuel economy, we will use the following formula:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有效地计算了我们的乘数。为了替换燃油经济性中的缺失值，我们将使用以下公式：
- en: '![](img/00089.jpeg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00089.jpeg)'
- en: 'For the number of cylinders, we will use the following equation:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于气缸数，我们将使用以下方程：
- en: '![](img/00090.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00090.jpeg)'
- en: Our preceding code uses these two formulas to calculate the multiplier for each
    row and then takes the average of these.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先前的代码使用这两个公式来计算每一行的乘数，然后取这些乘数的平均值。
- en: This is not going to be totally accurate but given the data we have, it should
    be accurate enough.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这不会是完全准确的，但鉴于我们拥有的数据，它应该足够准确。
- en: 'Here, we also present yet another way of creating a dictionary out of your
    (small!) Spark DataFrame: use the `.toPandas()` method to convert the Spark DataFrame
    to a pandas DataFrame. The DataFrame of pandas has a `.to_dict(...)` method that
    will allow you to convert our data to a dictionary. The `''records''` parameter
    instructs the method to convert each row to a dictionary where the key is the
    column name with the corresponding record value.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还提供了另一种将您的（小型！）Spark DataFrame创建为字典的方法：使用`.toPandas()`方法将Spark DataFrame转换为pandas
    DataFrame。 pandas DataFrame具有`.to_dict(...)`方法，该方法将允许您将我们的数据转换为字典。 `'records'`参数指示方法将每一行转换为一个字典，其中键是具有相应记录值的列名。
- en: Check out this link to read more about the `.to_dict(...)` method: [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 查看此链接以了解更多关于`.to_dict(...)`方法的信息：[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html)。
- en: 'Our resulting dictionary looks like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果字典如下：
- en: '![](img/00091.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00091.jpeg)'
- en: 'Let''s use it now to impute our missing data:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用它来填补我们的缺失数据：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: First, we convert our original data so it also reflects the ratios we specified
    earlier. Next, we use the multipliers dictionary to fill in the missing values,
    and finally we revert the columns to their original state.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将原始数据转换为反映我们之前指定的比率的数据。接下来，我们使用乘数字典填充缺失值，最后将列恢复到其原始状态。
- en: Note that each time we use the `.withColumn(...)` method, we overwrite the original
    column names.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每次使用`.withColumn(...)`方法时，都会覆盖原始列名。
- en: 'The resulting DataFrame looks as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的DataFrame如下所示：
- en: '![](img/00092.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00092.jpeg)'
- en: As you can see, the resulting values for the cylinders and the fuel economy
    are not totally accurate but still are arguably better than replacing them with
    some predefined value.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，汽缸和燃油经济性的结果值并不完全准确，但仍然可以说比用预定义值替换它们要好。
- en: See also
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Check out PySpark's documentation on the missing observation methods: [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看PySpark关于缺失观察方法的文档：[https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions)
- en: Handling outliers
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理异常值
- en: Observations that differ greatly from the rest of the observations, that is,
    they are located in the long tail(s) of the data distribution, are outliers. In
    this recipe, we will learn how to locate and handle the outliers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是与其余观察结果差异很大的观察结果，即它们位于数据分布的长尾部分，本配方中，我们将学习如何定位和处理异常值。
- en: Getting ready
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `imputed` DataFrame we created in the previous recipe,
    so we assume you have followed the steps to handle missing observations.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此配方，您需要有一个可用的Spark环境。此外，我们将在前一个配方中创建的`imputed`DataFrame上进行操作，因此我们假设您已经按照处理缺失观察的步骤进行了操作。
- en: No other prerequisites are required.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Let's start with a popular definition of an outlier.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从异常值的一个常见定义开始。
- en: 'A point, ![](img/00093.jpeg), that meets the following criteria:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一个点，![](img/00093.jpeg)，符合以下标准：
- en: '![](img/00094.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00094.jpeg)'
- en: 'Is not considered an outlier; any point outside this range is. In the preceding
    equation, *Q¹* is the first quartile (25^(th) percentile), *Q³* is the third quartile,
    and *IQR* is the **interquartile range** and is defined as the difference between *Q³* and *Q¹* :
    IQR= *Q³-Q¹*.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 不被视为异常值；在此范围之外的任何点都是异常值。在上述方程中，*Q¹*是第一四分位数（25^(th)百分位数），*Q³*是第三四分位数，*IQR*是**四分位距**，定义为*Q³*和*Q¹*的差值：IQR=
    *Q³-Q¹*。
- en: 'To flag the outliers, follow these steps:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要标记异常值，请按照以下步骤进行：
- en: 'Let''s calculate our ranges first:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先计算我们的范围：
- en: '[PRE18]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we flag the outliers:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们标记异常值：
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How it works...
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We will only be looking at the numerical variables: the displacement, cylinders,
    and the fuel economy.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只会查看数值变量：排量、汽缸和燃油经济性。
- en: We loop through all these features and calculate the first and third quartiles
    using the `.approxQuantile(...)` method. The method takes the feature (column)
    name as its first parameter, the float (or list of floats) of quartiles to calculate
    as the second parameter, and the third parameter specifies the relative target
    precision (setting this value to 0 will find exact quantiles but it can be very
    expensive).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们循环遍历所有这些特征，并使用`.approxQuantile(...)`方法计算第一和第三四分位数。该方法将特征（列）名称作为第一个参数，要计算的四分位数的浮点数（或浮点数列表）作为第二个参数，第三个参数指定相对目标精度（将此值设置为0将找到精确的四分位数，但可能非常昂贵）。
- en: 'The method returns a list of two (in our case) values: *Q¹* and *Q³*. We then
    calculate the interquartile range and append the `(feature_name, [lower_bound,
    upper_bound])` tuple to the `cut_off_point` list. After converting to a dictionary,
    our cut-off points are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法返回两个（在我们的情况下）值的列表：*Q¹*和*Q³*。然后我们计算四分位距，并将`(feature_name, [lower_bound, upper_bound])`元组附加到`cut_off_point`列表中。转换为字典后，我们的截断点如下：
- en: '![](img/00095.jpeg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00095.jpeg)'
- en: 'So, now we can use these to flag our outlying observations. We will only select
    the ID columns and then loop through our features to check whether they fall outside
    of our calculated bounds. Here''s what we get:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们可以使用这些来标记我们的异常观察结果。我们只会选择ID列，然后循环遍历我们的特征，以检查它们是否落在我们计算的边界之外。这是我们得到的结果：
- en: '![](img/00096.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00096.jpeg)'
- en: 'So, we have two outliers in the fuel economy column. Let''s check the records:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在燃油经济性列中有两个异常值。让我们检查记录：
- en: '[PRE20]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'First, we join our `imputed` DataFrame with the `outliers` one and then we
    filter on the `FuelEconomy_o` flag to select our outlying records only. Finally,
    we just extract the most relevant columns to show:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将我们的`imputed` DataFrame与`outliers`进行连接，然后我们根据`FuelEconomy_o`标志进行筛选，仅选择我们的异常记录。最后，我们只提取最相关的列以显示：
- en: '![](img/00097.jpeg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.jpeg)'
- en: So we have `SPARK ACTIV` and `CAMRY HYBRID LE` as the outliers. `SPARK ACTIV`
    became an outlier due to our imputation logic, as we had to impute its fuel economy
    values; given that its engine's displacement is 1.4 liters, our logic didn't work
    out well. Well, there are other ways you can impute the values. The Camry, being
    a hybrid, is definitely an outlier in a dataset dominated by large and turbo-charged
    engines; it should not surprise us to see it here.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有`SPARK ACTIV`和`CAMRY HYBRID LE`作为异常值。`SPARK ACTIV`由于我们的填充逻辑而成为异常值，因为我们不得不填充其燃油经济值；考虑到其引擎排量为1.4升，我们的逻辑并不奏效。好吧，您可以用其他方法填充值。作为混合动力车，凯美瑞在由大型涡轮增压引擎主导的数据集中显然是一个异常值；看到它出现在这里并不奇怪。
- en: 'Trying to build a machine learning model based on data with outliers can lead
    to some untrustworthy results or a model that does not generalize well, so we
    normally remove these from our dataset:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试基于带有异常值的数据构建机器学习模型可能会导致一些不可信的结果或无法很好泛化的模型，因此我们通常会从数据集中删除这些异常值：
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: See also
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Check out this website for more information about outliers: [http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm](http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm)
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看此网站以获取有关异常值的更多信息：[http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm](http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm)
- en: Exploring descriptive statistics
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索描述性统计
- en: Descriptive statistics are the most fundamental measures you can calculate on
    your data. In this recipe, we will learn how easy it is to get familiar with our
    dataset in PySpark.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性统计是您可以在数据上计算的最基本的度量。在本示例中，我们将学习在PySpark中熟悉我们的数据集是多么容易。
- en: Getting ready
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `no_outliers` DataFrame we created in the *Handling
    outliers* recipe so we assume you have followed the steps to handle duplicates,
    missing observations, and outliers.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的Spark环境。此外，我们将使用在*处理异常值*示例中创建的`no_outliers` DataFrame，因此我们假设您已经按照处理重复项、缺失观测值和异常值的步骤进行了操作。
- en: No other prerequisites are required.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Calculating the descriptive statistics for your data is extremely easy in PySpark.
    Here''s how:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在PySpark中计算数据的描述性统计非常容易。以下是方法：
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That's it!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！
- en: How it works...
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The preceding code barely needs an explanation. The `.describe(...)` method
    takes a list of columns you want to calculate the descriptive statistics on and
    returns a DataFrame with basic descriptive statistics: count, mean, standard deviation,
    minimum value, and maximum value.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码几乎不需要解释。`.describe(...)`方法接受要计算描述性统计的列的列表，并返回一个包含基本描述性统计的DataFrame：计数、平均值、标准偏差、最小值和最大值。
- en: You can specify both numeric and string columns as input parameters to `.describe(...)`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将数字和字符串列都指定为`.describe(...)`的输入参数。
- en: 'Here''s what we get from running the `.describe(...)` method on our `features`
    list of columns:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在`features`列上运行`.describe(...)`方法得到的结果：
- en: '![](img/00098.jpeg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00098.jpeg)'
- en: As expected, we have `16` records in total. Our dataset seems to be skewed (used
    here as a loose term, not in statistical terms) toward larger engines as the mean
    displacement is `3.44` liters with six cylinders. Fuel economy, for such sizable
    engines, seems to be decent, though, at 19 MPG.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们总共有`16`条记录。我们的数据集似乎偏向于较大的引擎，因为平均排量为`3.44`升，有六个汽缸。对于如此庞大的引擎来说，燃油经济性似乎还不错，为19英里/加仑。
- en: There's more...
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'If you do not pass a list of columns to calculate the descriptive statistics
    over, PySpark will return the statistics for each and every column in your DataFrame.
    Check out the following snippet:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不传递要计算描述性统计的列的列表，PySpark将返回DataFrame中每一列的统计信息。请查看以下代码片段：
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It will result in the following table:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下表：
- en: '![](img/00099.jpeg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpeg)'
- en: As you can see, even the string columns got their descriptive statistics which
    are, however, fairly questionable to interpret.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，即使字符串列也有它们的描述性统计，但解释起来相当可疑。
- en: Descriptive statistics for aggregated columns
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合列的描述性统计
- en: 'Sometimes you want to calculate some descriptive statistics within a group
    of values. In this example, we will calculate some basic stats for cars with different
    numbers of cylinders:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您希望在一组值中计算一些描述性统计。在此示例中，我们将为具有不同汽缸数量的汽车计算一些基本统计信息：
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: First, we select our `features` list of columns so we reduce the number of data
    we need to analyze. Next, we aggregate our data over the cylinders column and
    use the (already familiar) `.agg(...)` method to calculate the count, mean, and
    standard deviation over fuel economy and displacement.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们选择我们的`features`列列表，以减少我们需要分析的数据量。接下来，我们在汽缸列上聚合我们的数据，并使用（已经熟悉的）`.agg(...)`方法来计算燃油经济性和排量的计数、平均值和标准偏差。
- en: 'There are more aggregation functions available in the `pyspark.sql.functions`
    module: `avg(...)`, `count(...)`, `countDistinct(...)`, `first(...)`, `kurtosis(...)`,
    `max(...)`, `mean(...)`, `min(...)`, `skewness(...)`, `stddev_pop(...)`, `stddev_samp(...)`,
    `sum(...)`, `sumDistinct(...)`, `var_pop(...)`, `var_samp(...)`, and `variance(...)`.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`pyspark.sql.functions`模块中还有更多的聚合函数：`avg(...)`, `count(...)`, `countDistinct(...)`,
    `first(...)`, `kurtosis(...)`, `max(...)`, `mean(...)`, `min(...)`, `skewness(...)`,
    `stddev_pop(...)`, `stddev_samp(...)`, `sum(...)`, `sumDistinct(...)`, `var_pop(...)`,
    `var_samp(...)`, 和 `variance(...)`.'
- en: 'Here''s the resulting table:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果表：
- en: '![](img/00100.jpeg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.jpeg)'
- en: 'We can read two things from this table:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这个表中得出两点结论：
- en: Our imputation method is truly inaccurate, so next time we should come up with
    a better method.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的填充方法真的不准确，所以下次我们应该想出一个更好的方法。
- en: '`MPG_avg` for six cylinder cars is higher than for four cylinder cars and it
    would be suspicious. This is why you should be getting intimate with your data,
    as you can then spot such hidden traps in your data.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 六缸汽车的`MPG_avg`高于四缸汽车，这可能有些可疑。这就是为什么你应该熟悉你的数据，因为这样你就可以发现数据中的隐藏陷阱。
- en: What to do with such finding goes beyond the scope of this book. But, the point
    is that this is why a data scientist would spend 80% of their time cleaning the
    data and getting familiar with it, so the model that is built with such data can
    be relied on.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如何处理这样的发现超出了本书的范围。但是，重点是这就是为什么数据科学家会花80%的时间来清理数据并熟悉它，这样建立在这样的数据上的模型才能得到可靠的依赖。
- en: See also
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: There are many other statistics you can calculate on your data that we did not
    cover here (but that PySpark will allow you to calculate). For a more comprehensive
    overview, we suggest you check out this website: [https://www.socialresearchmethods.net/kb/statdesc.php](https://www.socialresearchmethods.net/kb/statdesc.php).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在你的数据上计算许多其他统计量，我们在这里没有涵盖（但PySpark允许你计算）。为了更全面地了解，我们建议你查看这个网站：[https://www.socialresearchmethods.net/kb/statdesc.php](https://www.socialresearchmethods.net/kb/statdesc.php)。
- en: Computing correlations
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算相关性
- en: Features correlated with the outcome are desirable, but those that are also
    correlated among themselves can make the model unstable. In this recipe, we will
    show you how to calculate correlations between features.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 与结果相关的特征是可取的，但那些在彼此之间也相关的特征可能会使模型不稳定。在这个配方中，我们将向你展示如何计算特征之间的相关性。
- en: Getting ready
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `no_outliers` DataFrame we created in the *Handling
    outliers* recipe, so we assume you have followed the steps to handle duplicates,
    missing observations, and outliers.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个步骤，你需要一个可用的Spark环境。此外，我们将使用我们在*处理离群值*配方中创建的`no_outliers` DataFrame，所以我们假设你已经按照处理重复项、缺失观察和离群值的步骤进行了操作。
- en: No other prerequisites are required.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To calculate the correlations between two features, all you have to do is to
    provide their names:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算两个特征之间的相关性，你只需要提供它们的名称：
- en: '[PRE26]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: That's it!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！
- en: How it works...
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `.corr(...)` method takes two parameters, the names of the two features
    you want to calculate the correlation coefficient between.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`.corr(...)`方法接受两个参数，即你想要计算相关系数的两个特征的名称。'
- en: Currently, only the Pearson correlation coefficient is available.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 目前只有皮尔逊相关系数是可用的。
- en: The preceding command will produce a correlation coefficient equal to `0.938`
    for our dataset.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将为我们的数据集产生一个相关系数等于`0.938`。
- en: There's more...
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'If you want to calculate a correlation matrix, you need to do this somewhat
    manually. Here''s our solution:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想计算一个相关矩阵，你需要手动完成这个过程。以下是我们的解决方案：
- en: '[PRE27]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The preceding code is effectively looping through the list of our `features`
    and computing the pair-wise correlations between them to fill the upper-triangular
    portion of the matrix.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码实际上是在我们的`features`列表中循环，并计算它们之间的成对相关性，以填充矩阵的上三角部分。
- en: We introduced the `features` list in the *Handling outliers *recipe earlier.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*处理离群值*配方中介绍了`features`列表。
- en: 'The calculated coefficient is then appended to the `temp` list which, in return,
    gets added to the `corr` list. Finally, we create the correlations DataFrame.
    Here''s what it looks like:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将计算出的系数附加到`temp`列表中，然后将其添加到`corr`列表中。最后，我们创建了相关性DataFrame。它看起来是这样的：
- en: '![](img/00101.jpeg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.jpeg)'
- en: As you can see, the only strong correlation is between `Displacement` and `Cylinders`
    and this, of course, comes as no surprise. `FuelEconomy` is not really correlated
    with the displacement as there are other factors that affect `FuelEconomy`, such
    as drag and weight of the car. However, if you were trying to predict, for example,
    maximum speed and assuming (and it is a fair assumption to make) that both `Displacement`
    and `Cylinders` would be highly positively correlated with the maximum speed,
    then you should only use one of them.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，唯一的强相关性是`Displacement`和`Cylinders`之间的，这当然不足为奇。`FuelEconomy`与排量并没有真正相关，因为还有其他影响`FuelEconomy`的因素，比如汽车的阻力和重量。然而，如果你试图预测，例如最大速度，并假设（这是一个合理的假设），`Displacement`和`Cylinders`都与最大速度高度正相关，那么你应该只使用其中一个。
- en: Drawing histograms
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制直方图
- en: Histograms are the easiest way to visually *inspect* the distribution of your
    data. In this recipe, we will show you how to do this in PySpark.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图是直观检查数据分布的最简单方法。在这个配方中，我们将向你展示如何在PySpark中做到这一点。
- en: Getting ready
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `no_outliers` DataFrame we created in the *Handling
    outliers* recipe, so we assume you have followed the steps to handle duplicates,
    missing observations, and outliers.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个步骤，你需要一个可用的Spark环境。此外，我们将使用我们在*处理离群值*配方中创建的`no_outliers` DataFrame，所以我们假设你已经按照处理重复项、缺失观察和离群值的步骤进行了操作。
- en: No other prerequisites are required.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'There are two ways to produce histograms in PySpark:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在PySpark中有两种生成直方图的方法：
- en: Select feature you want to visualize, `.collect()` it on the driver, and then
    use the matplotlib's native `.hist(...)` method to draw the histogram
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择你想要可视化的特征，在驱动程序上`.collect()`它，然后使用matplotlib的本地`.hist(...)`方法来绘制直方图
- en: Calculate the counts in each histogram bin in PySpark and only return the counts
    to the driver for visualization
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PySpark中计算每个直方图箱中的计数，并将计数返回给驱动程序进行可视化
- en: 'The former solution will work for small datasets (such as ours in this chapter)
    but it will break your driver if the data is too big. Moreover, there''s a good
    reason why we distribute the data so we can do the computations in parallel instead
    of in a single thread. Thus, in this recipe, we will only show you the second
    solution. Here''s the snippet that does all the calculations for us:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个解决方案适用于小数据集（例如本章中的数据），但如果数据太大，它将破坏您的驱动程序。此外，我们分发数据的一个很好的原因是，我们可以并行计算而不是在单个线程中进行计算。因此，在这个示例中，我们只会向您展示第二个解决方案。这是为我们做所有计算的片段：
- en: '[PRE28]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How it works...
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The preceding code is pretty self-explanatory. First, we select the feature
    of interest (in our case, the fuel economy).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码非常容易理解。首先，我们选择感兴趣的特征（在我们的例子中是燃油经济）。
- en: The Spark DataFrames do not have a native histogram method, so that's why we
    switch to the underlying RDD.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrames没有本地的直方图方法，这就是为什么我们要切换到底层的RDD。
- en: Next, we flatten our results into a long list (instead of a `Row` object) and
    use the `.histogram(...)` method to calculate our histogram.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将结果展平为一个长列表（而不是一个`Row`对象），并使用`.histogram(...)`方法来计算我们的直方图。
- en: The `.histogram(...)` method accepts either an integer that would specify the
    number of buckets to allocate our data to or a list with a specified bucket limit.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`.histogram(...)`方法接受一个整数，该整数指定要将我们的数据分配到的桶的数量，或者是一个具有指定桶限制的列表。'
- en: Check out PySpark's documentation on the `.histogram(...)` at [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.histogram](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.histogram).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 查看PySpark关于`.histogram(...)`的文档：[https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.histogram](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.histogram)。
- en: 'The method returns a tuple of two elements: the first element is a list of
    bin bounds, and the other element is the counts of elements in the corresponding
    bins. Here''s what this looks like for our fuel economy feature:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法返回两个元素的元组：第一个元素是一个bin边界的列表，另一个元素是相应bin中元素的计数。这是我们的燃油经济特征的样子：
- en: '![](img/00102.jpeg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.jpeg)'
- en: 'Note that we specified that we want the `.histogram(...)` method to bucketize
    our data into five bins, but there are six elements in the first list. However,
    we still have five buckets in our dataset: *[8.97, 12.38), [ 12.38, 15.78), [15.78,
    19.19), [19.19, 22.59)*, and *[22.59, 26.0)*.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们指定`.histogram(...)`方法将我们的数据分桶为五个bin，但第一个列表中有六个元素。但是，我们的数据集中仍然有五个桶：*[8.97,
    12.38), [ 12.38, 15.78), [15.78, 19.19), [19.19, 22.59)*和*[22.59, 26.0)*。
- en: 'We cannot create any plots in PySpark natively without going through a lot
    of setting up (see, for example, this: [https://plot.ly/python/apache-spark/](https://plot.ly/python/apache-spark/)).
    The easier way is to prepare a DataFrame with our data and use some *magic* (well,
    sparkmagics, but it still counts!) locally on the driver.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能在PySpark中本地创建任何图表，而不经过大量设置（例如，参见这个：[https://plot.ly/python/apache-spark/](https://plot.ly/python/apache-spark/)）。更简单的方法是准备一个包含我们的数据的DataFrame，并在驱动程序上使用一些*魔法*（好吧，是sparkmagics，但它仍然有效！）。
- en: 'First, we need to extract our data and create a temporary `histogram_MPG` table:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要提取我们的数据并创建一个临时的`histogram_MPG`表：
- en: '[PRE29]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We create a two-column DataFrame where the first column contains the bin lower
    bound and the second column contains the corresponding count. The `.registerTempTable(...)`
    method (as the name suggests) registers a temporary table so we can actually use
    it with the `%%sql` magic:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个两列的DataFrame，其中第一列包含bin的下限，第二列包含相应的计数。`.registerTempTable(...)`方法（顾名思义）注册一个临时表，这样我们就可以在`%%sql`魔法中使用它：
- en: '[PRE30]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The preceding command selects all the records from our temporary `histogram_MPG`
    table and outputs it to the locally-accessible `hist_MPG` variable; the `-q` switch
    is there so nothing gets printed out to the notebook.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的命令从我们的临时`histogram_MPG`表中选择所有记录，并将其输出到本地可访问的`hist_MPG`变量；`-q`开关是为了确保笔记本中没有打印出任何内容。
- en: 'With `hist_MPG` locally accessible, we can now use it to produce our plot:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 有了本地可访问的`hist_MPG`，我们现在可以使用它来生成我们的图表：
- en: '[PRE31]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`%%local` executes whatever is located in that notebook cell in local mode.
    First, we import the `matplotlib` library and specify that it produces the plots
    inline within the notebook instead of popping up a new window each time a plot
    is produced. `plt.style.use(...)` changes the styles of our charts.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`%%local`在本地模式下执行笔记本单元格中的任何内容。首先，我们导入`matplotlib`库，并指定它在笔记本中内联生成图表，而不是每次生成图表时弹出一个新窗口。`plt.style.use(...)`更改我们图表的样式。'
- en: For a full list of available styles, check out [https://matplotlib.org/devdocs/gallery/style_sheets/style_sheets_reference.html](https://matplotlib.org/devdocs/gallery/style_sheets/style_sheets_reference.html).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看可用样式的完整列表，请查看[https://matplotlib.org/devdocs/gallery/style_sheets/style_sheets_reference.html](https://matplotlib.org/devdocs/gallery/style_sheets/style_sheets_reference.html)。
- en: 'Next, we create a figure and add a subplot to it that we will be drawing in.
    Finally, we use the `.bar(...)` method to plot our histogram and set the title.
    Here''s what the chart looks like:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个图表，并向其中添加一个子图，最后，我们使用`.bar(...)`方法来绘制我们的直方图并设置标题。图表的样子如下：
- en: '![](img/00103.jpeg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.jpeg)'
- en: That's it!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！
- en: There's more...
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Matplotlib is not the only library we can use to plot histograms. Bokeh (available
    at [https://bokeh.pydata.org/en/latest/](https://bokeh.pydata.org/en/latest/))
    is another powerful plotting library, built on top of `D3.js`, which allows you
    to interactively *play* with your charts.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib不是我们绘制直方图的唯一库。Bokeh（可在[https://bokeh.pydata.org/en/latest/](https://bokeh.pydata.org/en/latest/)找到）是另一个功能强大的绘图库，建立在`D3.js`之上，允许您与图表进行交互。
- en: Check out the gallery of examples at [https://bokeh.pydata.org/en/latest/docs/gallery.html](https://bokeh.pydata.org/en/latest/docs/gallery.html).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在[https://bokeh.pydata.org/en/latest/docs/gallery.html](https://bokeh.pydata.org/en/latest/docs/gallery.html)上查看示例的图库。
- en: 'Here''s how you plot with Bokeh:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用Bokeh绘图的方法：
- en: '[PRE32]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'First, we load all the necessary components of Bokeh; the `output_notebook()`
    method makes sure that we produce the chart inline in the notebook instead of
    opening a new window each time. Next, we produce the labels to put on our chart.
    Then, we define our figure: the `x_range` parameter specifies the number of points
    on the *x* axis and the `plot_height` sets the height of our plot. Finally, we
    use the `.vbar(...)` method to draw the bars of our histogram; the `x` parameter
    is the labels to put on our plot, and the `top` parameter specifies the counts.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载Bokeh的所有必要组件；`output_notebook()`方法确保我们在笔记本中内联生成图表，而不是每次都打开一个新窗口。接下来，我们生成要放在图表上的标签。然后，我们定义我们的图形：`x_range`参数指定*x*轴上的点数，`plot_height`设置我们图表的高度。最后，我们使用`.vbar(...)`方法绘制我们直方图的条形；`x`参数是要放在我们图表上的标签，`top`参数指定计数。
- en: 'The result looks as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![](img/00104.jpeg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00104.jpeg)'
- en: It's the same information, but you can interact with this chart in your browser.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相同的信息，但您可以在浏览器中与此图表进行交互。
- en: See also
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: If you want to further customize your histograms, here is a page that might
    be useful: [https://plot.ly/matplotlib/histograms/](https://plot.ly/matplotlib/histograms/)
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想要进一步自定义您的直方图，这是一个可能有用的页面：[https://plot.ly/matplotlib/histograms/](https://plot.ly/matplotlib/histograms/)
- en: Visualizing interactions between features
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化特征之间的相互作用
- en: Plotting the interactions between features can further your understanding of
    not only the distribution of your data, but also how the features relate to each
    other. In this recipe, we will show you how to create scatter plots from your
    data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制特征之间的相互作用可以进一步加深您对数据分布的理解，也可以了解特征之间的关系。在这个配方中，我们将向您展示如何从您的数据中创建散点图。
- en: Getting ready
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `no_outliers` DataFrame we created in the *Handling
    outliers* recipe, so we assume you have followed the steps to handle duplicates,
    missing observations, and outliers.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此操作，您需要拥有一个可用的Spark环境。此外，我们将在*处理异常值*配方中创建的`no_outliers` DataFrame上进行操作，因此我们假设您已经按照处理重复项、缺失观察和异常值的步骤进行了操作。
- en: No other prerequisites are required.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Once again, we will select our data from the DataFrame and expose it locally:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将从DataFrame中选择我们的数据并在本地公开它：
- en: '[PRE33]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works...
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we select the two features we want to learn more about to see how they
    interact with each other; in our case they are the displacement and cylinders
    features.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们选择我们想要了解其相互作用的两个特征；在我们的案例中，它们是排量和汽缸特征。
- en: Our example here is small so we can work with all our data. However, in the
    real world, you should sample your data first before attempting to plot billions
    of data points.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例很小，所以我们可以使用所有的数据。然而，在现实世界中，您应该在尝试绘制数十亿数据点之前首先对数据进行抽样。
- en: 'After registering the temp table, we use the `%%sql` magic to select all the
    data from the `scatter` table and expose it locally as a `scatter_source`. Now,
    we can start plotting:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在注册临时表之后，我们使用`%%sql`魔术方法从`scatter`表中选择所有数据并在本地公开为`scatter_source`。现在，我们可以开始绘图了：
- en: '[PRE34]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: First, we load the Matplotlib library and set it up.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载Matplotlib库并对其进行设置。
- en: See the *Drawing histograms* recipe for a more detailed explanation of what
    these Matplotlib commands do.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 有关这些Matplotlib命令的更详细解释，请参阅*绘制直方图*配方。
- en: Next, we create a figure and add a subplot to it. Then, we draw a scatter plot
    using our data; the *x* axis will represent the number of cylinders and the *y*
    axis will represent the displacement. Finally, we set the axes labels and the
    chart title.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个图形并向其添加一个子图。然后，我们使用我们的数据绘制散点图；*x*轴将代表汽缸数，*y*轴将代表排量。最后，我们设置轴标签和图表标题。
- en: 'Here''s what the final result looks like:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果如下所示：
- en: '![](img/00105.jpeg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.jpeg)'
- en: There's more...
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You can create an interactive version of the preceding chart using `bokeh`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`bokeh`创建前面图表的交互版本：
- en: '[PRE35]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: First, we create the canvas, the figure we will be plotting on. Next, we set
    our labels. Finally, we use the `.circle(...)` method to plot the dots on the
    canvas.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建画布，即我们将绘图的图形。接下来，我们设置我们的标签。最后，我们使用`.circle(...)`方法在画布上绘制点。
- en: 'The final result looks as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果如下所示：
- en: '![](img/00106.jpeg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00106.jpeg)'
