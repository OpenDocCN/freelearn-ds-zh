- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Sentiment Analysis – NLP and Crypto News
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析 – 自然语言处理（NLP）与加密新闻
- en: '**Natural language processing** (**NLP**) falls under the umbrella of artificial
    intelligence and is concerned with the comprehension of text by computers. Recent
    advancements in this field, exemplified by the emergence of tools such as ChatGPT,
    have become an integral part of our daily lives. Yet, the financial sector has
    been leveraging NLP for quite some time, particularly for fundamental analysis.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）属于人工智能领域，致力于计算机对文本的理解。近年来，随着如ChatGPT等工具的出现，NLP已经成为我们日常生活中不可或缺的一部分。然而，金融行业已经在相当长一段时间里利用NLP，尤其是在基本面分析方面。'
- en: Fundamental analysis seeks to ascertain the intrinsic value of assets such as
    stocks, tokens, or NFT art based on publicly accessible information. In traditional
    finance, textual data is sourced from periodic submissions to the SEC (such as
    Form 10K or 10Q), including financial statements, specialized media news, social
    media platforms such as X (formerly Twitter), and other avenues. Web3 has fostered
    a similar environment where market activities are continuous, and X and news platforms
    serve as predominant textual resources. It’s worth noting that while most Web3
    companies may not yet be obligated to file regulatory reports, it’s probable that
    such data sources will eventually become available for most companies.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 基本面分析旨在根据公开可得的信息确定资产的内在价值，如股票、代币或NFT艺术作品。在传统金融中，文本数据来源于定期向美国证券交易委员会（SEC）提交的报告（如10K表格或10Q表格），包括财务报表、专业媒体新闻、社交媒体平台如X（前身为Twitter）以及其他渠道。Web3创造了一个类似的环境，市场活动持续不断，X和新闻平台成为主要的文本资源。值得注意的是，虽然大多数Web3公司目前可能还没有义务提交监管报告，但这些数据源最终有可能会成为大多数公司可以获取的资源。
- en: 'NLP in the financial sector encompasses various applications, including the
    following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理在金融领域的应用涵盖了多个方面，包括以下内容：
- en: '**Sentiment analysis**: Determining the positivity, negativity, or neutrality
    of text, which could be news articles, social media posts (tweets, Reddit, and
    so on), and more. These algorithms can also provide insights into polarity and
    subjectivity, aiding in assessing sentiments toward companies, industries, markets,
    government decisions, crypto developments, and more.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：确定文本的积极性、消极性或中立性，这些文本可能是新闻文章、社交媒体帖子（推文、Reddit等）等。这些算法还可以提供关于情感极性和主观性的见解，帮助评估对公司、行业、市场、政府决策、加密货币发展等的情感。'
- en: '**Topic modeling**: This helps classify and organize large volumes of financial
    documents based on the underlying topics they cover. This aids in efficiently
    managing and accessing relevant information.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题建模**：帮助根据文档所涵盖的主题对大量财务文档进行分类和组织。这有助于高效管理和访问相关信息。'
- en: '**Summarization**: In a world where content is being created at a non-stop
    rate, there simply is not enough time and/or resources to analyze and give hierarchy
    to all of it. NLP techniques are being applied to collect and create short summaries
    of documents that are easy to process by analysts.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摘要**：在一个内容不断产生的世界里，显然没有足够的时间和/或资源去分析和给每个内容排出层次。NLP技术正被应用于收集并创建简短的文档摘要，方便分析人员处理。'
- en: '**Fraud detection**: Scrutinizing emails, chats, financial documents, transcribed
    conversations, and more using NLP techniques to uncover patterns that are potentially
    indicative of fraudulent activities.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈检测**：利用NLP技术审查电子邮件、聊天记录、财务文件、转录对话等，发掘潜在的欺诈活动模式。'
- en: '**Trading**: Incorporating NLP tools into trading strategies to signal or predict
    market trends, or enhance the decision-making process of fundamental analysis
    traders.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交易**：将NLP工具纳入交易策略中，发出市场趋势的信号或预测，或增强基本面分析交易员的决策过程。'
- en: The data source for NLP techniques is text that is categorized as unstructured.
    We are surrounded by it, with more being produced every second. We explored some
    text sources in [*Chapter 3*](B19446_03.xhtml#_idTextAnchor114); we will explore
    some others in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: NLP技术的数据源是被归类为非结构化的文本。我们周围充斥着这样的文本，且每秒钟都会产生更多。在[*第3章*](B19446_03.xhtml#_idTextAnchor114)中我们探讨了其中一些文本来源；本章将继续探讨其他文本来源。
- en: In this chapter, we will analyze the sentiment of the news sourced from Crypto
    Panic. To do so, we will build a **neural network** (**NN**) and explain the concept
    and use of pre-trained embeddings. A crypto news dataset and a traditional finance
    news dataset will be employed to train our model. Additionally, we’ll learn the
    essentials of how to pre-process text for NN utilization and how to evaluate the
    results of such a model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将分析来自 Crypto Panic 的新闻情感。为此，我们将构建一个**神经网络**（**NN**），并解释预训练嵌入的概念和使用方法。我们将使用加密新闻数据集和传统金融新闻数据集来训练我们的模型。此外，我们还将学习如何为神经网络使用预处理文本的基本知识，以及如何评估此类模型的结果。
- en: At the time of writing, ChatGPT is a tangible reality. Publicly accessible information
    reveals its training on an extensive multilingual corpus, utilizing reinforcement
    learning to progressively enhance its performance. We’ll learn how to incorporate
    ChatGPT for sentiment analysis on the Crypto Panic dataset. This can be useful
    to implement as a ready-to-use tool while we build a specialized corpus to train
    our models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，ChatGPT 已经成为现实。公开的信息揭示了它在一个广泛的多语言语料库上进行了训练，并利用强化学习不断提高其性能。我们将学习如何将 ChatGPT
    集成到 Crypto Panic 数据集的情感分析中。这可以作为一个现成的工具来使用，同时我们构建一个专门的语料库来训练我们的模型。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要主题：
- en: A deep learning pipeline for sentiment analysis, encompassing preparation, model
    construction, training, and evaluation phases.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种用于情感分析的深度学习管道，包括准备、模型构建、训练和评估阶段。
- en: Integrating ChatGPT for sentiment analysis
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成 ChatGPT 进行情感分析
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we’ll utilize tools from the libraries that were introduced
    in [*Chapter 7*](B19446_07.xhtml#_idTextAnchor228) – that is, scikit-learn and
    Keras. Additionally, we will employ **NLTK**, a Python library that proves valuable
    for working with human language data. NLTK includes a range of modules and functions
    that empower us to execute tasks such as tokenization, stemming, and part-of-speech
    tagging on our selected databases. This library streamlines the process of processing
    extensive text datasets so that they’re ready to be integrated with machine learning
    or deep learning models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 [*第七章*](B19446_07.xhtml#_idTextAnchor228) 中介绍的库工具——即 scikit-learn
    和 Keras。此外，我们还将使用 **NLTK**，这是一个 Python 库，对处理人类语言数据非常有帮助。NLTK 包含了一系列模块和函数，使我们能够执行诸如分词、词干提取和词性标注等任务。这个库简化了处理大量文本数据集的过程，使它们能够与机器学习或深度学习模型进行集成。
- en: 'If you have not worked with NLTK before, it can be installed with the following
    code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前没有使用过 NLTK，可以使用以下代码进行安装：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The documentation for `nltk` can be found at https://www.nltk.org. Another
    essential library when handling text manipulation and cleaning is **re**, short
    for **Regular Expression**. A regular expression is a sequence of characters that
    defines a search pattern. Here’s an example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk` 的文档可以在 https://www.nltk.org 找到。另一个在处理文本操作和清理时非常重要的库是 **re**，即**正则表达式**的缩写。正则表达式是一串字符，用于定义搜索模式。以下是一个示例：'
- en: '| **Pattern** | **Search Criteria** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **模式** | **搜索标准** |'
- en: '| [a-z] | Any single character in the range a-z |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| [a-z] | 匹配范围 a-z 中的任何单个字符 |'
- en: '| [0-9A-Fa-f] | Match any hexadecimal digit |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| [0-9A-Fa-f] | 匹配任何十六进制数字 |'
- en: Table 8.1 – Example of a “re” pattern
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 – “re” 模式示例
- en: The `re` library provides functions and methods to employ the aforementioned
    patterns. For instance, `re.sub` replaces all characters that match the pattern
    with a specified string. A comprehensive list of functions is available at https://docs.python.org/3/library/re.xhtml#module-re.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`re` 库提供了使用上述模式的函数和方法。例如，`re.sub` 将所有与模式匹配的字符替换为指定的字符串。有关函数的完整列表，请访问 https://docs.python.org/3/library/re.xhtml#module-re。'
- en: Throughout our work, we will utilize Google’s Colaboratory platform, which already
    includes the core library imports. However, for specific tasks, additional imports
    will be required.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们将使用 Google 的 Colaboratory 平台，该平台已包含核心库的导入。然而，对于特定任务，还需要其他的导入。
- en: You can find all the data and code files for this chapter in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter08](https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter08).
    We recommend that you read through the code files in the `Chapter08` folder to
    follow along.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到本章的所有数据和代码文件，地址为[https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter08](https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter08)。我们建议你浏览`Chapter08`文件夹中的代码文件，以便跟随学习。
- en: Example datasets
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例数据集
- en: 'We will merge two headline datasets in this chapter; you can find links to
    the relevant sources in the *Further reading* section and the code. The datasets
    are as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将合并两个标题数据集；你可以在*进一步阅读*部分和代码中找到相关来源的链接。数据集如下：
- en: '**Financial Phrase Bank**: This dataset is also available on Kaggle and contains
    a headline accompanied by sentiment analysis labels from the viewpoint of a retail
    investor. It comprises multiple datasets, each of which categorizes sentences
    based on the level of consensus regarding the sentiment of the phrase. For this
    exercise, we will utilize the *Sentence_AllAgree* dataset.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融短语库**：该数据集也可在Kaggle上获取，包含一个标题，并附带零售投资者角度的情感分析标签。它由多个数据集组成，每个数据集根据短语情感共识的程度对句子进行分类。在本练习中，我们将使用*Sentence_AllAgree*数据集。'
- en: '**CryptoGDELT2022**: Presented in the paper *Cryptocurrency Curated News Event
    Database From GDELT*, this dataset comprises news events extracted from the **Global
    Database of Events, Language, and Tone** (**GDELT**). It covers news events between
    March 31, 2021, and April 30, 2022\. The dataset includes various sentiment scores
    and manual labeling methods. In this exercise, we will exclusively employ manual
    labeling.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CryptoGDELT2022**：该数据集来自论文*Cryptocurrency Curated News Event Database From
    GDELT*，包含从**全球事件、语言与情感数据库**（**GDELT**）提取的新闻事件。它涵盖了2021年3月31日至2022年4月30日之间的新闻事件。该数据集包含多种情感得分和手动标注方法。在本练习中，我们将仅使用手动标注。'
- en: Let’s get started with building our pipeline.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建我们的管道。
- en: Building our pipeline
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建我们的管道
- en: In an NLP pipeline, preparation generally encompasses a pre-processing step
    where we clean and normalize the data. Following that, a feature representation
    step translates the language into input that can be consumed by our chosen models.
    Once this is completed, we are ready to build, train, and evaluate the model.
    This strategic plan will be implemented throughout the subsequent sections.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP管道中，准备工作通常包括一个预处理步骤，在该步骤中我们清理和规范化数据。接下来，特征表示步骤将语言转换为可以供我们选择的模型使用的输入。完成这些后，我们便可以开始构建、训练和评估模型。这个战略计划将在接下来的章节中实现。
- en: Preparation
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备
- en: Language manifests in numerous variations. There are formatting nuances, such
    as capitalization or punctuation; words that serve as linguistic aids without
    true semantic meaning, such as prepositions; and special characters, including
    emojis, further enrich the landscape. To work with this data, we must transform
    raw text into a dataset while following a similar criterion as numeric datasets.
    This cleaning process enables us to eliminate outliers, reduce noise, manage vocabulary
    size, and optimize data for ingestion by NLP models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 语言在多种形式中表现出来。存在格式化的细微差异，例如大写或标点符号；一些作为语言辅助的词汇，其本身没有真正的语义意义，比如介词；以及包括表情符号在内的特殊字符，进一步丰富了语言的表现形式。为了处理这些数据，我们必须将原始文本转换为数据集，并遵循与数字数据集类似的标准。这个清理过程使我们能够去除异常值、减少噪音、管理词汇大小，并优化数据以便NLP模型处理。
- en: 'A basic flow diagram of the data cleaning pipeline can be seen in the following
    figure:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理管道的基本流程图如下所示：
- en: '![Figure 8.1 – Cleaning diagram](img/B19446_08_01.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 清理图示](img/B19446_08_01.jpg)'
- en: Figure 8.1 – Cleaning diagram
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 清理图示
- en: Let’s delve into some of these steps.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解这些步骤。
- en: Normalization
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规范化
- en: Normalization encompasses a series of tasks, including lowercasing and removing
    HTML traces, links, and emojis. Our objective is to clean our database so that
    only words remain.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化包括一系列任务，如小写化、移除HTML痕迹、链接和表情符号。我们的目标是清理数据库，只留下有效的词汇。
- en: Furthermore, this process involves eliminating words that lack informativeness
    for our model or that could potentially introduce bias, leading to suboptimal
    predictions. Depending on the task, we will select words that we may have to delete.
    This step also addresses the process of removing words that may have escaped the
    *stop words* cleaning process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这一过程还涉及去除那些对模型缺乏信息量或可能引入偏差的词汇，从而导致不理想的预测。根据任务的不同，我们将选择需要删除的词汇。这个步骤还处理了去除可能遗漏在
    *停用词* 清理过程中的词汇。
- en: 'We can also convert Unidecode text into ASCII text. Unidecode accommodates
    characters from diverse languages, allowing them to be translated into their nearest
    ASCII counterpart. For instance, the Spanish character “ñ” becomes “n” in ASCII.
    We implement this transformation in `Database_and_Preprocessing.ipynb` using the
    following code snippet:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将 Unidecode 文本转换为 ASCII 文本。Unidecode 支持来自不同语言的字符，将它们转化为最接近的 ASCII 对应字符。例如，西班牙字符“ñ”在
    ASCII 中变为“n”。我们在 `Database_and_Preprocessing.ipynb` 中通过以下代码片段实现了这一转换：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Normalization fosters uniformity by rendering all text in a consistent format,
    directing the model’s focus toward content rather than superficial differences.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化通过将所有文本呈现为一致的格式来促进统一性，指引模型的注意力集中在内容上，而非表面差异。
- en: Stop words
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停用词
- en: Our objective here is to exclude words that contribute minimal semantic value
    or meaning to our model. Frequently employed words such as articles (“a,” “the,”
    “an”), prepositions (“on,” “at,” “from,” “to”), conjunctions (“and,” “so,” “although”),
    and pronouns (“she,” “he,” “it”) play functional roles in language but lack substantial
    semantic content that the model can leverage.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的目标是排除对模型贡献较小的语义价值或意义的词汇。经常使用的词汇，如冠词（“a”，“the”，“an”）、介词（“on”，“at”，“from”，“to”）、连词（“and”，“so”，“although”）和代词（“she”，“he”，“it”），在语言中起到功能性作用，但缺乏模型可以利用的实质性语义内容。
- en: 'Consequently, this collection of words is generally filtered out during preprocessing.
    The stop words collection can be downloaded by language and directly applied to
    our cleaning process using NLTK. In `Database_and_Preprocessing.ipynb`, we downloaded
    the English stop words with the following code snippet:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些词汇通常在预处理过程中被过滤掉。可以按语言下载停用词集，并通过 NLTK 直接应用于我们的清理过程。在 `Database_and_Preprocessing.ipynb`
    中，我们通过以下代码片段下载了英语停用词：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This process reduces the noise in the data and helps improve the efficiency
    of the model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程减少了数据中的噪声，并有助于提高模型的效率。
- en: Tokenization
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词
- en: 'Tokenization involves splitting the text within our database into smaller units
    of meaning referred to as tokens. These units can take the form of sentences or
    words. For instance, let’s take a look at the following headline:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将数据库中的文本拆分为较小的有意义单位，称为词元。这些单位可以是句子或单词。例如，我们来看一下以下标题：
- en: '*“SEC investigating Coinbase for its Earn product, wallet service, and* *exchange
    activity”*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*“SEC 正在调查 Coinbase 的 Earn 产品、钱包服务和* **交易活动**”*'
- en: 'When tokenized into words, this yields the following output:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当分词为单词时，输出结果如下：
- en: '`[''SEC'', ''investigating'', ''Coinbase'', ''Earn'', ''product'', '','', ''wallet'',
    ''service'', ''``exchange'', ''activity'']`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`[''SEC'', ''investigating'', ''Coinbase'', ''Earn'', ''product'', '','', ''wallet'',
    ''service'', ''``exchange'', ''activity'']`'
- en: 'Tokenization results in a structured input that the NLP model can process effectively,
    facilitating data analysis. Such analysis guides decisions on vocabulary size
    for dimensionality reduction, a demonstrated example of which can be found in
    `Database_and_Preprocessing.ipynb`. This showcases the top recurring words in
    the analyzed dataset. Such analysis gives the following result:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 分词产生了模型可以有效处理的结构化输入，促进了数据分析。此类分析指导了词汇大小的选择，用于降维，在 `Database_and_Preprocessing.ipynb`
    中可以找到此类降维的示例。此分析展示了在分析过的数据集中最常出现的词汇。该分析得出以下结果：
- en: '![Figure 8.2 – Top recurring words in analyzed headlines](img/B19446_08_02.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 分析过的标题中最常出现的词](img/B19446_08_02.jpg)'
- en: Figure 8.2 – Top recurring words in analyzed headlines
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 分析过的标题中最常出现的词
- en: Lemmatization and part-of-speech (POS) tagging
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词形还原和词性标注（POS）
- en: These techniques reduce words to their base or root form. It allows us to reduce
    the diversity of words we have to process by selecting those words that are conjugated
    and replacing them with their root word. In our example sentence (“SEC investigating
    Coinbase for its Earn product, wallet service, and exchange activity”), the word
    *investigating* will change to *investigate*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术将单词还原为其基本形式或词根。这使我们能够通过选择那些已变化的单词并用其词根替代它们，减少我们需要处理的单词多样性。在我们的示例句子（“SEC正在调查Coinbase的Earn产品、钱包服务和交易活动”）中，单词*investigating*将变为*investigate*。
- en: 'The accuracy of lemmatization hinges on the library’s comprehension of the
    word’s context or function within the sentence. POS tagging contributes this contextual
    information to our analysis. Here’s an example:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原的准确性取决于库对单词在句子中上下文或功能的理解。词性标注为我们的分析提供了这种上下文信息。以下是一个示例：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'POS tagging helps us programmatically assign a context to each word, depending
    on its position in the sentence or document. Here’s an example:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注帮助我们根据单词在句子或文档中的位置，程序化地为每个单词分配上下文。以下是一个示例：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The result of cleaning the sentence looks like this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 清理后的句子结果如下所示：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s look at some additional preprocessing techniques:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些额外的预处理技术：
- en: '**Stemming**: This involves removing prefixes and suffixes from words to derive
    a common base form, known as the “stem.” The resulting stem may not always form
    a valid word, but it aims to capture the core meaning.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取**：这涉及从单词中去除前缀和后缀，以推导出一个共同的基本形式，称为“词干”。得到的词干可能并不总是形成有效的单词，但它旨在捕捉核心含义。'
- en: '**Named entity recognition** (**NER**): This technique automatically identifies
    and classifies named entities (for example, names of people, places, organizations,
    and dates) in text. NER extracts structured information from unstructured text,
    categorizing entities into predefined classes. An example of this approach is
    offered by the X (formerly Twitter) dataset, which we covered in [*Chapter 3*](B19446_03.xhtml#_idTextAnchor114).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）：该技术自动识别并分类文本中的命名实体（例如，人名、地名、组织名和日期）。NER从非结构化文本中提取结构化信息，将实体分类到预定义的类别中。我们在[《第三章》](B19446_03.xhtml#_idTextAnchor114)中讨论的X（前身为Twitter）数据集就是这一方法的一个示例。'
- en: '**Dependency parsing**: This technique analyzes a sentence’s grammatical structure
    to establish relationships between words. It creates a hierarchical structure
    where each word is linked to its governing word (the “head”) and assigned a grammatical
    role (the “dependency label”).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依存句法分析**：该技术分析句子的语法结构，以建立单词之间的关系。它创建了一个层次结构，其中每个单词与其支配单词（“头”）相连，并分配一个语法角色（“依存标签”）。'
- en: Checkpoint
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点
- en: A step-by-step version of this pipeline is detailed in `Database_and_Preprocessing.ipynb`.
    If you want to skip this section, the resulting `.csv` file has been uploaded
    to this book's GitHub and is accessible at `preprocessed.csv`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道的逐步版本已详细列出在`Database_and_Preprocessing.ipynb`中。如果你想跳过这一部分，生成的`.csv`文件已经上传到本书的GitHub，并可以通过`preprocessed.csv`访问。
- en: Feature representation
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征表示
- en: Following the preprocessing phase, the next step involves transforming the resultant
    raw text data into features that the model can utilize for statistical inference.
    The goal is to extract relevant information from the text and encode it in a way
    that algorithms can understand. There are multiple ways to achieve this, but they
    generally involve representing words as vectors and measuring the frequency of
    words in a document. Some common techniques are bag of words, **term frequency-inverse
    document frequency** (**TF-IDF**), and word embeddings. Let’s briefly describe
    them.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理阶段之后，下一步是将得到的原始文本数据转换为模型可以利用的特征，用于统计推断。目标是从文本中提取相关信息，并以算法能够理解的方式进行编码。实现这一目标的方法有多种，但通常涉及将单词表示为向量，并衡量单词在文档中的频率。一些常见的技术包括词袋模型、**词频-逆文档频率**（**TF-IDF**）和词嵌入。我们简要描述它们。
- en: Bag of words
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词袋模型
- en: 'This technique builds a vector that represents all the words in the document
    by the number of times that word appears. It ignores the order of the sentence
    or the context of the word. Its implementation can be accomplished using `sklearn.feature_extraction.text.CountVectorizer`
    from the scikit-learn library. This approach is basic and loses important contextual
    information, and it may also create a very sparse matrix because the vocabulary
    is vast:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术通过统计单词出现的次数构建一个表示文档中所有单词的向量。它忽略句子的顺序和单词的上下文。可以使用来自scikit-learn库的`sklearn.feature_extraction.text.CountVectorizer`来实现该方法。这个方法比较基础，丢失了重要的上下文信息，并且可能会创建一个非常稀疏的矩阵，因为词汇表非常庞大：
- en: '![Figure 8.3 – Bag of words. Text extracted from https://rif.technology/content-hub/crypto-credit-card/](img/B19446_08_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 词袋模型。文本摘自 https://rif.technology/content-hub/crypto-credit-card/](img/B19446_08_03.jpg)'
- en: Figure 8.3 – Bag of words. Text extracted from https://rif.technology/content-hub/crypto-credit-card/
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 词袋模型。文本摘自 https://rif.technology/content-hub/crypto-credit-card/
- en: Moreover, this approach can be enhanced by incorporating n-grams, which involve
    concatenating two or more words that hold contextual meaning together. For example,
    “natural language”, “machine learning,” and “press release” encapsulate specific
    concepts when combined, but in isolation, they do not retain the concept. Incorporating
    n-grams into bag of words can expand the vocabulary further.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过结合n-grams（即将具有上下文意义的两个或多个单词连接在一起）可以增强这种方法。例如，“自然语言”，“机器学习”和“新闻稿”在组合时能 encapsulate
    特定的概念，但单独来看，它们并不保留这些概念。将n-grams融入到词袋模型中，可以进一步扩展词汇表。
- en: TF-IDF
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF-IDF
- en: This is an alternate technique where `sklearn.feature_extraction.text.TfidfVectorizer`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种替代方法，使用`sklearn.feature_extraction.text.TfidfVectorizer`。
- en: Word embeddings
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Word embeddings represent words as dense vectors within a continuous space.
    This approach retains information about context and semantic meaning by capturing
    relationships between words. **Word2Vec** and **GloVe** ([https://www.tensorflow.org/tutorials/text/word2vec](https://www.tensorflow.org/tutorials/text/word2vec))
    are popular algorithms that generate word embeddings. These embeddings can either
    be pre-trained on large text corpora or fine-tuned for specific tasks. In our
    model, we employ Glove ([https://nlp.stanford.edu/projects/glove](https://nlp.stanford.edu/projects/glove))
    vectors.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入将单词表示为连续空间中的密集向量。通过捕捉单词之间的关系，这种方法保留了上下文和语义信息。**Word2Vec** 和 **GloVe** ([https://www.tensorflow.org/tutorials/text/word2vec](https://www.tensorflow.org/tutorials/text/word2vec))
    是生成词嵌入的流行算法。这些嵌入可以在大型文本语料库上进行预训练，或者针对特定任务进行微调。在我们的模型中，我们使用了GloVe ([https://nlp.stanford.edu/projects/glove](https://nlp.stanford.edu/projects/glove))
    向量。
- en: 'Glove, in particular, is a pre-trained vector that’s developed via an unsupervised
    learning algorithm. This method leverages linear substructures prevalent in texts
    and gauges semantic similarity by measuring vector distances. The GloVe website
    provides a classic example illustrating the relationships discerned by the model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe，特别是，通过一种无监督学习算法开发的预训练向量。这种方法利用文本中普遍存在的线性子结构，通过测量向量之间的距离来评估语义相似性。GloVe网站提供了一个经典示例，说明了模型所识别的关系：
- en: '![](img/B19446_08_04.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19446_08_04.jpg)'
- en: 'Figure 8.4 – Example of structures found in the words by GloVe (source: [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/))'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – GloVe发现的单词结构示例（来源：[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)）
- en: Model building
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型构建
- en: We are working on a supervised task to classify Crypto Panic headlines into
    positive, negative, and neutral. For this purpose, we will employ the `Modeling.ipynb`
    file.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在处理一个监督学习任务，目标是将Crypto Panic的头条新闻分类为正面、负面和中立。为此，我们将使用`Modeling.ipynb`文件。
- en: LSTM is a type of **recurrent neural network** (**RNN**) that’s capable of learning
    long-term dependencies that significantly outperform regular RNNs with text tasks.
    This structure is commonly used in NLP tasks as it can model sequences of input
    data well and retain dependencies between words in a sentence or document. Consequently,
    it can predict not only based on the current input but also consider long-distance
    information – that is, the context – and not just specific words. It is important
    to note that while LSTMs can be more effective in capturing long-term dependencies,
    their performance can also depend on factors such as the specific task, the size
    of the dataset, and the model’s architecture. In some cases, other advanced architectures
    such as transformer-based models (such as BERT and GPT) have also demonstrated
    superior performance in certain NLP tasks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是一种**递归神经网络**（**RNN**），能够学习长期依赖关系，在文本任务中显著优于常规的 RNN。该结构常用于自然语言处理任务，因为它能够很好地建模输入数据的序列并保留句子或文档中单词之间的依赖关系。因此，它不仅可以基于当前输入进行预测，还可以考虑远程信息——即上下文——而不仅仅是特定的单词。需要注意的是，虽然
    LSTM 在捕捉长期依赖关系方面可能更有效，但其性能也可能取决于一些因素，如具体任务、数据集的大小以及模型的架构。在某些情况下，基于 Transformer
    的高级架构（如 BERT 和 GPT）在某些自然语言处理任务中也表现出了更优的性能。
- en: 'Christopher Olah provides a great introduction to the model in his blog, describing
    it as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Christopher Olah 在他的博客中对该模型进行了很好的介绍，他是这样描述的：
- en: “*Humans don’t start their thinking from scratch every second. As you read this
    essay, you understand each word based on your understanding of previous words.
    You don’t throw everything away and start thinking from scratch again. Your thoughts*
    *have persistence.*”
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: “*人类在思考时不会每一秒钟都从头开始。当你阅读这篇文章时，你是基于对前一个单词的理解来理解每一个新单词的。你不会把所有东西都抛弃然后重新开始思考。你的思维*
    *具有持续性*。”
- en: LSTM serves as a specialized form of RNN that’s designed to detect patterns
    in data sequences, whether they arise from sensor data, asset prices, or natural
    language. Its distinct feature lies in its capacity to preserve information over
    extended periods compared to conventional RNNs. RNNs have a short-term memory
    that retains information in the current neuron, resulting in a limited ability
    to predict with longer sequences. When the memory is exceeded, the model simply
    discards the oldest data and replaces it with new data, without considering whether
    the discarded data was important or not. LSTM overcomes this short-term memory
    problem by selectively retaining relevant information in the **cell state** in
    addition to the traditional short-term memory stored in the **hidden state**.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是一种专门的 RNN 变种，旨在检测数据序列中的模式，无论这些模式来源于传感器数据、资产价格还是自然语言。其独特之处在于与传统的 RNN 相比，LSTM
    能够在更长的时间内保存信息。RNN 具有短期记忆，只能在当前神经元中保留信息，这导致它在处理更长序列时的预测能力有限。当内存超出时，模型会简单地丢弃最旧的数据，并用新数据替换它，而不考虑被丢弃的数据是否重要。LSTM
    通过选择性地将相关信息保存在**单元状态**中，除了传统的短期记忆存储在**隐藏状态**外，克服了这种短期记忆问题。
- en: At the beginning of each computational step, we have the current input, *x(t)*,
    the previous state of the long-term memory, *c(t-1)*, and the previous state of
    the short-term memory stored in the hidden state, *h(t-1)*. At the end of the
    process, we obtain an updated cell state and a new hidden state. The cell state
    carries information along with the timestamps of our dataset, allowing it to extract
    meaning from the order of the input data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个计算步骤的开始，我们有当前的输入 *x(t)*、长时记忆的前一个状态 *c(t-1)*，以及存储在隐藏状态中的短期记忆的前一个状态 *h(t-1)*。在过程的结束时，我们获得更新后的单元状态和新的隐藏状态。单元状态携带着信息和数据集的时间戳，使其能够从输入数据的顺序中提取意义。
- en: 'These three inputs navigate through three gates, each serving a specific function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个输入通过三个门进行处理，每个门都有其特定功能：
- en: '**Forget gate**: This gate determines which current and previous information
    is retained and which is discarded. It integrates the previous status of the hidden
    state and the current input, passing them through a sigmoid function. The sigmoid
    function outputs values between 0 and 1, with 0 indicating that the previous information
    is considered irrelevant and can be forgotten, and 1 indicating that it should
    be preserved. The result is multiplied by the current cell state.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：该门决定哪些当前信息和先前信息被保留，哪些被丢弃。它将隐藏状态的先前状态和当前输入进行整合，并通过一个sigmoid函数进行传递。sigmoid函数输出的值在0到1之间，0表示先前的信息被认为无关并可被遗忘，1则表示该信息应被保留。最终的结果将乘以当前的单元状态。'
- en: '**Input gate**: This gate determines the importance of the current input in
    solving the task. It quantifies the relevance of the new information carried by
    the input, *x(t)*. The current input is multiplied by the hidden state from the
    previous run. All information deemed important by the input gate is added to the
    cell state, forming the new cell state, *c(t)*. This new cell state becomes the
    current state of the long-term memory and will be used in the next run.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：该门决定当前输入在解决任务中的重要性。它量化了由输入 *x(t)* 传递的新信息的相关性。当前输入与上次运行的隐藏状态相乘。输入门认为重要的所有信息都被加到单元状态中，形成新的单元状态
    *c(t)*。这个新的单元状态成为长期记忆的当前状态，并将在下一次运行中使用。'
- en: '**Output gate**: The output of the LSTM model is computed in the hidden state:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：LSTM模型的输出在隐藏状态中计算：'
- en: '![Figure 8.5 – Cell states](img/B19446_08_05.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 单元状态](img/B19446_08_05.jpg)'
- en: Figure 8.5 – Cell states
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 单元状态
- en: 'To interact with the LSTM, we need to input vectors with uniform lengths. To
    fulfill this requirement, we must encode the preprocessed input text sequentially,
    as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与LSTM进行交互，我们需要输入具有统一长度的向量。为了满足这一要求，我们必须按照以下方式顺序地对预处理后的输入文本进行编码：
- en: '`tokenizer.texts_to_sequences(X_train)`: This step transforms each text into
    a sequence of integers using the tokenizer’s most frequently encountered words.
    If the tokenizer lacks certain words in its vocabulary, a predefined `<OOV>` (out-of-vocabulary)
    token is employed.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer.texts_to_sequences(X_train)`：此步骤将每个文本转换为一个整数序列，使用tokenizer中最常见的词汇。如果tokenizer的词汇表中缺少某些词汇，则会使用预定义的`<OOV>`（超出词汇表）标记。'
- en: '`pad_sequences`: This function transforms the previously converted sequences
    into a 2D array of shape: (number of sequences, length of the sequence desired).
    The length or `maxlen` argument can be user-defined or defaulted to the longest
    sequence in the list. Additionally, the user can select whether padding occurs
    at the sequence’s beginning or end.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_sequences`：此函数将先前转换的序列转换为形状为（序列数，所需序列长度）的二维数组。`maxlen`参数可以由用户定义，或者默认设置为列表中最长期序列的长度。此外，用户可以选择填充发生在序列的开始还是结束。'
- en: 'The model we have constructed features the embedding layer as its foremost
    element, with `trainable = False` to retain insights from Glove500\. Should we
    opt for training from scratch, we have to set that parameter to `True`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的模型以嵌入层作为其核心元素，并设置`trainable = False`以保持来自Glove500的见解。如果我们选择从头开始训练，则需要将该参数设置为`True`：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Furthermore, our design incorporates an LSTM layer, a dense layer, and a dropout
    layer. The dropout layer, a regularization technique that’s frequently employed
    to prevent overfitting, operates by randomly deactivating (that is, setting to
    zero) a fraction of neurons during each forward pass in training. This helps prevent
    the network from relying too heavily on specific neurons and encourages the network
    to learn more robust and generalized features:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的设计包括了一个LSTM层、一个全连接层和一个dropout层。dropout层是一种常用的正则化技术，旨在防止过拟合，其通过在每次训练的前向传播中随机停用（即将其置为零）一部分神经元来操作。这有助于防止网络过度依赖特定的神经元，并鼓励网络学习更为鲁棒且具备泛化能力的特征：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For the last dense layer, we use `''softmax''` activation, which assigns decimal
    probabilities to each trained class:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最后的全连接层，我们使用`'softmax'`激活函数，它为每个训练类别分配一个小数概率：
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We compile by utilizing the loss function of `''categorical_crossentropy''`,
    a standard choice for multiclass classification tasks encompassing more than two
    classes, as is the scenario here:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过利用`'categorical_crossentropy'`作为损失函数来编译模型，这是多类别分类任务（包括两个以上类别）的标准选择，正如在这里的情况：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Checkpoint
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点
- en: A step-by-step version of this part of the pipeline is shown in `Modeling.ipynb`.
    If you want to skip this section, the resulting model and tokenizer have been
    uploaded to this book’s GitHub repository and are accessible via `chapter8_model.h5`
    and `text_tokenizer.json`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该部分管道的逐步版本显示在`Modeling.ipynb`中。如果你想跳过这一部分，生成的模型和分词器已经上传到本书的 GitHub 仓库，并可以通过`chapter8_model.h5`和`text_tokenizer.json`进行访问。
- en: Training and evaluation
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练与评估
- en: 'We can train the model with the following code snippet:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码片段训练模型：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Upon completing the training, we assess its performance and outcomes through
    three distinct methods:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成训练后，我们通过三种不同的方法评估其性能和结果：
- en: '**Mini test**: We look for test samples, apply the model, and conduct evaluations.
    We must always remember to preprocess our samples and pass them to our model in
    the same shape as it is ready to consume.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**小测试**：我们查找测试样本，应用模型，并进行评估。我们必须始终记得对样本进行预处理，并将它们以模型准备好的形状传递给模型。'
- en: '`log` folder for storing the resulting training and validation outcomes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`log` 文件夹用于存储训练和验证结果：'
- en: '[PRE11]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This folder is referred to in the training instructions provided with `callbacks=[tensorboard_callback])`.
    TensorBoard then accesses this folder to display the results.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件夹在随附的训练说明中提到（`callbacks=[tensorboard_callback]`）。TensorBoard 然后访问这个文件夹以显示结果。
- en: '**ROC AUC curve**: According to Jason Brownlee’s blog, “*A ROC curve is a diagnostic
    plot for summarizing the behavior of a model by calculating the false positive
    rate and true positive rate for a set of predictions by the model under different
    thresholds*.” The ROC curve is an evaluation metric for binary tasks. To apply
    it to our multiclass case, we must transform the multiclass problem into a binary
    one using either the **one-versus-one** (**OvO**) or **one-versus-all** (**OvA**)/**one-versus-rest**
    (**OvR**) approach. In OvR, we assess each class against the others. In OvO, we
    evaluate each class against every other class in pairs. Choosing between these
    techniques depends on specific problem nuances, class count, computational resources,
    and dataset characteristics. Certain machine learning libraries, such as scikit-learn,
    offer the choice between OvA and OvO strategies for multi-class classification
    algorithms.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROC AUC 曲线**：根据 Jason Brownlee 的博客，“*ROC 曲线是通过计算不同阈值下模型预测集的假阳性率和真阳性率来总结模型行为的诊断图*。”
    ROC 曲线是二分类任务的评估指标。为了将其应用于我们的多类问题，我们必须通过 **一对一** (**OvO**) 或 **一对多** (**OvA**)/**一对其余**
    (**OvR**) 方法将多类问题转化为二类问题。在 OvR 中，我们评估每个类别与其他类别的关系；在 OvO 中，我们评估每对类别之间的关系。选择这些技术取决于特定问题的细节、类别数量、计算资源和数据集特点。某些机器学习库，如
    scikit-learn，为多类分类算法提供了 OvA 和 OvO 策略的选择。'
- en: 'In this case, we use the OvA approach, where we measure how well our model
    predicts each label, considering one as true and all the others as false. That
    way, we can plot the ROC AUC. The closer to 1, the better the model; the further
    it approaches 0.5, the less skillful it is:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用 OvA 方法，其中我们衡量模型预测每个标签的效果，将一个标签视为真实，其他所有标签视为假。通过这种方式，我们可以绘制 ROC AUC
    曲线。曲线越接近 1，模型越好；越接近 0.5，模型的表现越差：
- en: '![Figure 8.6 – ROC AUC curve applied](img/B19446_08_06.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 应用的 ROC AUC 曲线](img/B19446_08_06.jpg)'
- en: Figure 8.6 – ROC AUC curve applied
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 应用的 ROC AUC 曲线
- en: Both accuracy and the ROC AUC curve can be overly optimistic when dealing with
    imbalanced datasets.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理不平衡数据集时，准确率和 ROC AUC 曲线可能会过于乐观。
- en: '**F1 score**: When addressing a multiclass classification problem through the
    OvA perspective, we acquire a binary set of values that allow us to calculate
    precision, recall, and the F1 score. The F1 score proves more suitable when the
    aim is to minimize both false positives and false negatives. This metric amalgamates
    information from both precision and recall and is their harmonic mean. The F1
    score formula is as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**F1 分数**：通过 OvA 角度解决多类分类问题时，我们获得一组二值值，从中可以计算精确度、召回率和 F1 分数。F1 分数在目标是最小化假阳性和假阴性时更加适用。这个指标结合了精确度和召回率的信息，是它们的调和平均数。F1
    分数的公式如下：'
- en: F*1* Score = 2 *  Precision * Recall  _____________  Precision + Recall
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: F*1* 分数 = 2 *  精确度 * 召回率  _____________  精确度 + 召回率
- en: 'This is succinctly summarized by Joos Korstanje in his blog:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Joos Korstanje 在他的博客中简洁地总结了这一点：
- en: '*A model will obtain a high F1 score if both precision and recall* *are high*'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果模型的精确度和召回率都很高，模型将获得高 F1 分数*'
- en: '*A model will obtain a low F1 score if both precision and recall* *are low*'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果精确度和召回率* *都较低，模型将获得较低的 F1 分数*'
- en: '*A model will obtain a medium F1 score if precision or recall is low and the
    other* *is high*'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果精确度或召回率较低，而另一个较高*，模型将获得中等的 F1 分数。'
- en: 'The aforementioned metric can be generated with the subsequent code snippet:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上述度量可以通过以下代码片段生成：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The weighted average F1 score for the model is `0.72`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的加权平均 F1 分数为 `0.72`。
- en: 'We can save the trained model for future use using the following code snippet:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码片段保存已训练的模型，以备将来使用：
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the `Chapter08/app.py` file in this book’s GitHub repository, we’ve developed
    an app that retrieves titles from the Cryptopanic API, applies the trained sentiment
    model, and displays the outcome in the console.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书 GitHub 仓库中的 `Chapter08/app.py` 文件里，我们开发了一个应用程序，它从 Cryptopanic API 获取标题，应用已训练的情感模型，并在控制台中显示结果。
- en: A note on NLP challenges
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 NLP 挑战的说明
- en: 'Due to the inherent complexities of human language, NLP faces several challenges
    that may significantly impact the performance and accuracy of its models. However,
    potential mitigation strategies exist to address these challenges:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人类语言固有的复杂性，NLP 面临着多个挑战，这些挑战可能会显著影响其模型的表现和准确性。然而，存在潜在的缓解策略来应对这些挑战：
- en: '**Ambiguity**: Words and phrases often carry multiple meanings, with the correct
    interpretation depending on the context. This complexity poses challenges, even
    for native and non-native speakers of a language, as seen in metaphors. Similarly,
    models encounter difficulties in interpreting user intent. To tackle this, models
    can be designed to incorporate broader contextual information, leveraging surrounding
    words and phrases for more accurate meaning inference.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**歧义性**：单词和短语通常具有多重含义，正确的解释依赖于上下文。这种复杂性即使对母语和非母语使用者来说也是一种挑战，尤其是在隐喻的使用中。同样，模型在解释用户意图时也会遇到困难。为了解决这个问题，可以设计模型以纳入更广泛的上下文信息，通过利用周围的单词和短语来进行更准确的意义推断。'
- en: '**Language diversity**: Languages exhibit wide variations in grammar, syntax,
    and semantics. Additionally, slang, regional dialects, and cultural nuances further
    contribute to linguistic diversity. NLP models, trained on a specific type of
    data, may struggle to generalize to diverse linguistic contexts. To address this
    limitation, models can be trained on more extensive and diverse datasets encompassing
    various linguistic patterns.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言多样性**：语言在语法、句法和语义上展现出广泛的差异。此外，俚语、地区方言和文化差异进一步加剧了语言的多样性。NLP 模型如果只在特定类型的数据上训练，可能难以在多样的语言环境中推广应用。为了解决这一限制，模型可以在更广泛且多样化的数据集上进行训练，以涵盖各种语言模式。'
- en: '**Data sparsity**: NLP models heavily rely on vast amounts of labeled data
    for training. However, acquiring labeled data for all conceivable language variations
    and applications proves challenging. Transfer learning techniques, such as pre-training
    on a large corpus and fine-tuning for specific tasks, offer a viable solution
    by mitigating the need for extensive labeled data.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据稀缺性**：NLP 模型在训练过程中严重依赖大量的标注数据。然而，为所有可能的语言变体和应用获取标注数据是一个挑战。迁移学习技术，如在大语料库上进行预训练并对特定任务进行微调，通过减轻对大量标注数据的需求，提供了一个可行的解决方案。'
- en: '**Ethical considerations and bias**: NLP models may inadvertently learn bias
    present in training data, resulting in biased outputs. Addressing this issue requires
    the curation of diverse and representative training datasets, regular audits of
    models for bias, and the implementation of fairness-aware training techniques.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**伦理考量与偏差**：NLP 模型可能会无意中学习到训练数据中的偏差，从而导致偏向的输出。解决这个问题需要精心策划多样化和具有代表性的训练数据集、定期审查模型的偏差，并实施公平意识的训练技术。'
- en: While NLP faces numerous challenges, ongoing research and advancements in machine
    learning techniques contribute to overcoming these hurdles.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 NLP 面临许多挑战，但机器学习技术的持续研究和进步有助于克服这些障碍。
- en: ChatGPT integration
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT 集成
- en: When pressed for time and unable to gather the necessary data for training a
    new model to build a classifier, we can explore the option of utilizing pre-trained
    models, such as those offered by TextBlob, spaCy, or the Hugging Face libraries.
    Additionally, we can seamlessly integrate readily available models such as ChatGPT.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当时间紧迫且无法收集必要的数据来训练新模型以构建分类器时，我们可以考虑使用预训练模型，如 TextBlob、spaCy 或 Hugging Face 库提供的模型。此外，我们还可以无缝集成现成的模型，如
    ChatGPT。
- en: 'I asked ChatGPT to present itself and this is what it said:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我让 ChatGPT 自我介绍，以下是它的回答：
- en: “*I’m ChatGPT, a large language model developed by OpenAI, based on the GPT-3
    architecture. I’m trained on a vast amount of text data and can generate human-like
    responses to a wide variety of questions and prompts. My main purpose is to assist
    users in generating text, whether it’s writing an article, answering questions,
    or completing creative tasks. I have a broad range of knowledge on various topics,
    including science, technology, history, and literature, among others. I’m designed
    to understand natural language and can respond to text-based input in a* *conversational
    manner*.”
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: “*我是ChatGPT，一个由OpenAI开发的大型语言模型，基于GPT-3架构。我在大量文本数据上进行训练，能够生成类人回答，回应各种问题和提示。我的主要目的是帮助用户生成文本，无论是写文章、回答问题，还是完成创意任务。我在各种话题上拥有广泛的知识，包括科学、技术、历史和文学等。我旨在理解自然语言，能够以*
    *对话式的方式* 回复基于文本的输入。*”
- en: Of particular relevance to this chapter is the final statement. This tool is
    well-versed in diverse vocabulary and is primed to process textual input.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本章特别相关的是最后一句话。该工具精通多样的词汇，能够处理文本输入。
- en: ChatGPT has a different architecture compared to LSTM. It uses the **transformer**
    architecture, which allows it to understand and generate natural language text.
    Transformers use self-attention mechanisms to capture relationships between words
    in a sentence, allowing for parallel processing of words rather than sequential
    processing like in LSTMs. Transformers are used to translate languages, summarize
    long articles, answer questions, complete sentences, and even create stories.
    BERT and GPT are popular transformer models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的架构与LSTM不同。它使用**transformer**架构，使其能够理解和生成自然语言文本。Transformer使用自注意力机制来捕捉句子中单词之间的关系，从而允许对单词进行并行处理，而不像LSTM那样进行顺序处理。Transformer被用于翻译语言、总结长文章、回答问题、完成句子，甚至创作故事。BERT和GPT是常见的Transformer模型。
- en: 'In the `Chapter07/chat_gpt integration` file, we’ve replicated the same use
    case as the previous segment, where we interacted with the Cryptopanic API to
    extract titles, apply the ChatGPT model, and display the output in the console,
    yielding excellent results. To facilitate this, an API key is required, which
    can be generated by following these steps on the OpenAI website:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Chapter07/chat_gpt integration`文件中，我们复制了与前一部分相同的用例，在该用例中，我们与Cryptopanic API进行了交互，提取标题，应用ChatGPT模型，并在控制台中显示输出，取得了优异的结果。为了方便操作，需要一个API密钥，可以通过以下步骤在OpenAI官网生成：
- en: Visit [https://platform.openai.com/docs/api-reference](https://platform.openai.com/docs/api-reference),
    go to the sign-up section, and proceed to sign up on their website.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问[https://platform.openai.com/docs/api-reference](https://platform.openai.com/docs/api-reference)，前往注册部分，然后在其网站上进行注册。
- en: 'On the left-hand side, you will see a dropdown menu that says **View API keys**.
    Click on this to access the page for generating a new API key:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧，你会看到一个下拉菜单，写着**查看API密钥**。点击这个菜单，进入生成新API密钥的页面：
- en: '![Figure 8.7 – ChatGPT – API keys landing page](img/B19446_08_07.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – ChatGPT – API密钥登录页面](img/B19446_08_07.jpg)'
- en: Figure 8.7 – ChatGPT – API keys landing page
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – ChatGPT – API密钥登录页面
- en: 'It is essential to generate and securely store the generated API keys as they
    can’t be retrieved once they’ve been generated:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成并安全存储生成的API密钥非常重要，因为一旦生成，它们就无法再被检索：
- en: '![Figure 8.8 – Chat GPT – API key generated](img/B19446_08_08.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – Chat GPT – API密钥生成](img/B19446_08_08.jpg)'
- en: Figure 8.8 – Chat GPT – API key generated
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – Chat GPT – API密钥生成
- en: The idea of this section is to recognize that ChatGPT exists and can do great
    work as well as solve the sentiment analysis problem by connecting the API, which
    may be a temporary solution if there is not enough data to train a specialized
    model.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的是要认识到ChatGPT的存在，它能够完成出色的工作，并通过连接API来解决情感分析问题，这可能是一个临时解决方案，特别是在没有足够数据来训练专用模型的情况下。
- en: It is possible to fine-tune ChatGPT for a specific task or domain using task-specific
    data. This process enables the model to adapt to the nuances and requirements
    of the target application. For instance, we can customize the model to generate
    shorter answers, reduce the amount of context required in a prompt for improved
    responses, and define how it handles edge cases. Let’s imagine we would like to
    integrate a specialized bot into our company’s internal communications system
    that delivers concise summaries of cryptocurrency news with a specific tone or
    format. This could be done with this training process. Detailed documentation
    for this process is available at [https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning),
    and a step-by-step tutorial can be found in the *Further* *reading* section.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用任务特定的数据对 ChatGPT 进行微调，以适应特定任务或领域。此过程使模型能够适应目标应用的细微差别和需求。例如，我们可以将模型定制为生成更简短的回答，减少在提示中所需的上下文量，以提高响应效果，并定义它如何处理边缘情况。假设我们希望将一个专门的机器人集成到公司内部通讯系统中，提供简洁的加密货币新闻摘要，具有特定的语气或格式。这可以通过该训练过程来实现。有关该过程的详细文档，请参阅
    [https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)，并且可以在
    *进一步* *阅读* 部分找到逐步教程。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The field of NLP is rapidly evolving, providing an effective means to extract
    insights from unstructured data such as text. Throughout this chapter, we introduced
    the field, illustrated a typical task within it, delineated the workflow, discussed
    pertinent data, and executed model training using embeddings. Additionally, we
    demonstrated the model evaluation process and showcased its integration into a
    program that sources headlines from the CryptoPanic API.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）领域正在迅速发展，为从非结构化数据（如文本）中提取洞察提供了有效的手段。在本章中，我们介绍了该领域，展示了其中的一个典型任务，阐明了工作流程，讨论了相关数据，并使用嵌入进行模型训练。此外，我们还展示了模型评估过程，并展示了如何将其集成到一个程序中，该程序从
    CryptoPanic API 获取头条新闻。
- en: It’s worth emphasizing that amassing a substantial volume of data is pivotal
    for high model accuracy. Nevertheless, in cases where constructing such a model
    isn’t feasible, alternative solutions are available. We explored one such solution
    involving the ChatGPT API, which provides access to a text bot trained on a comprehensive
    corpus of data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 值得强调的是，积累大量数据对高模型准确性至关重要。然而，在无法构建此类模型的情况下，也可以采取替代方案。我们探讨了其中一种解决方案，涉及使用 ChatGPT
    API，它提供了访问经过全面数据集训练的文本机器人。
- en: In the subsequent chapter, we will delve into the support that data teams can
    extend to artistic groups who are seeking to transform their artworks into unique
    products through the utilization of NFTs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨数据团队如何为艺术团体提供支持，帮助他们通过使用 NFT 将艺术作品转化为独特的产品。
- en: Further reading
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于本章内容的主题，请查阅以下资源：
- en: 'Introduction:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引言：
- en: Bird, Steven, Edward Loper and Ewan Klein (2009), *Natural Language Processing
    with Python*. O’Reilly Media Inc. Available at [https://www.nltk.org/book/](https://www.nltk.org/book/).
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bird, Steven, Edward Loper 和 Ewan Klein（2009），*Python 自然语言处理*。O’Reilly Media
    Inc. 可在 [https://www.nltk.org/book/](https://www.nltk.org/book/) 查阅。
- en: Yordanov, V. (2019, August 13). *Introduction to natural language processing
    for text*. Medium. Available at https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yordanov, V.（2019年8月13日）。*文本自然语言处理简介*。Medium。可在 https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63
    查阅。
- en: Gabriel Doyle, and Charles Elkan. (n.d.). *Financial Topic Models*. Available
    at [https://pages.ucsd.edu/~gdoyle/papers/doyle-elkan-2009-nips-paper.pdf](https://pages.ucsd.edu/~gdoyle/papers/doyle-elkan-2009-nips-paper.pdf).
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gabriel Doyle 和 Charles Elkan。（无日期）。*金融主题模型*。可在 [https://pages.ucsd.edu/~gdoyle/papers/doyle-elkan-2009-nips-paper.pdf](https://pages.ucsd.edu/~gdoyle/papers/doyle-elkan-2009-nips-paper.pdf)
    查阅。
- en: Sigmoider. (2018, May 3). *Get started with NLP (Part I)*. Medium. Available
    at https://medium.com/@gon.esbuyo/get-started-with-nlp-part-i-d67ca26cc828.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoider.（2018年5月3日）。*开始使用 NLP（第一部分）*。Medium。可在 https://medium.com/@gon.esbuyo/get-started-with-nlp-part-i-d67ca26cc828
    查阅。
- en: 'Suhyeon Kim, Haecheong Park, and Junghye Lee. (n.d.). *Word2vec-based latent
    semantic analysis (W2V-LSA) for topic modeling: A study on blockchain technology
    trend analysis*. Available at [https://www.sciencedirect.com/science/article/pii/S0957417420302256](https://www.sciencedirect.com/science/article/pii/S0957417420302256).'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suhyeon Kim, Haecheong Park, 和 Junghye Lee.（无日期）。*基于Word2vec的潜在语义分析（W2V-LSA）用于主题建模：区块链技术趋势分析的研究*。[https://www.sciencedirect.com/science/article/pii/S0957417420302256](https://www.sciencedirect.com/science/article/pii/S0957417420302256)。
- en: 'State of data science and machine learning (2022). *Kaggle: Your Machine Learning
    and Data Science Community*. Available at [https://www.kaggle.com/kaggle-survey-2022](https://www.kaggle.com/kaggle-survey-2022).'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学与机器学习的现状（2022）。*Kaggle：您的机器学习和数据科学社区*。[https://www.kaggle.com/kaggle-survey-2022](https://www.kaggle.com/kaggle-survey-2022)。
- en: '*Very good ChatGPT fine tuning tutorial: Tech-At-Work*. (2023, September 11).
    Easily Fine Tune ChatGPT 3.5 to Outperform GPT-4! [Video]. YouTube. Available
    at [https://www.youtube.com/watch?v=8Ieu2v0v4oc](https://www.youtube.com/watch?v=8Ieu2v0v4oc).'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*非常好的ChatGPT微调教程：Tech-At-Work*。 (2023年9月11日）。轻松微调ChatGPT 3.5，超越GPT-4！[视频]。YouTube。[https://www.youtube.com/watch?v=8Ieu2v0v4oc](https://www.youtube.com/watch?v=8Ieu2v0v4oc)。'
- en: 'Example database:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例数据库：
- en: 'Malo, P., Sinha, A., Korhonen, P., Wallenius, J., and Takala, P. (2014). *Good
    debt or bad debt: Detecting semantic orientations in economic texts*. Journal
    of the Association for Information Science and Technology, 65(4), 782-796\. Available
    at [https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news).'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malo, P., Sinha, A., Korhonen, P., Wallenius, J., 和 Takala, P. (2014年）。*好债务还是坏债务：检测经济文本中的语义取向*。信息科学与技术协会期刊，65(4)，782-796。[https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news)。
- en: Manoel Fernando Alonso Gadi, and Miguel Ángel Sicilia. (2022, October 10). *Cryptocurrency
    Curated News Event Database From GDELT* [pdf]. Research Square. Available at https://assets.researchsquare.com/files/rs-2145757/v1_covered.pdf?c=1665769708.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manoel Fernando Alonso Gadi 和 Miguel Ángel Sicilia.（2022年10月10日）。*来自GDELT的加密货币精选新闻事件数据库*
    [pdf]。Research Square。[https://assets.researchsquare.com/files/rs-2145757/v1_covered.pdf?c=1665769708](https://assets.researchsquare.com/files/rs-2145757/v1_covered.pdf?c=1665769708)。
- en: 'Preprocessing:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理：
- en: Bird, S., Klein, E., and Loper, E. (2009). *Natural language processing with
    Python*. O’Reilly Media.
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bird, S., Klein, E., 和 Loper, E. (2009年）。*使用Python进行自然语言处理*。O’Reilly Media。
- en: '*Sklearn.feature_extraction.text.CountVectorizer*. (n.d.). scikit-learn. Retrieved
    March 24, 2023\. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.xhtml).'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sklearn.feature_extraction.text.CountVectorizer*。（无日期）。scikit-learn。检索于2023年3月24日。[https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.xhtml)。'
- en: '*Sklearn.feature_extraction.text.TfidfVectorizer*. (n.d.). scikit-learn. Retrieved
    March 24, 2023\. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.xhtml).'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sklearn.feature_extraction.text.TfidfVectorizer*。（无日期）。scikit-learn。检索于2023年3月24日。[https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.xhtml)。'
- en: 'Model:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型：
- en: Dudeperf3ct. (2019, January 28). *Force of LSTM and GRU*. Blog. [https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/#bag-of-words-model](https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/#bag-of-words-model).
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dudeperf3ct。 (2019年1月28日）。*LSTM和GRU的力量*。博客。[https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/#bag-of-words-model](https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/#bag-of-words-model)。
- en: Brandon Rohrer. (n.d.). *Recurrent Neural Networks (RNN) and Long Short-Term
    Memory (LSTM)* [Video]. YouTube. Available at https://www.youtube.com/watch?v=WCUNPb-5EYI&list=PLVZqlMpoM6kaJX_2lLKjEhWI0NlqHfqzp.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brandon Rohrer。 (无日期）。*循环神经网络（RNN）和长短期记忆（LSTM）* [视频]。YouTube。[https://www.youtube.com/watch?v=WCUNPb-5EYI&list=PLVZqlMpoM6kaJX_2lLKjEhWI0NlqHfqzp](https://www.youtube.com/watch?v=WCUNPb-5EYI&list=PLVZqlMpoM6kaJX_2lLKjEhWI0NlqHfqzp)。
- en: 'Pennington, J. (n.d.). *GloVe: Global vectors for word representation*. The
    Stanford Natural Language Processing Group. Available at [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/).'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pennington, J.（无日期）。*GloVe：全球词向量表示*。斯坦福自然语言处理组。[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)。
- en: Jason Brownlee. (2020). *Deep Convolutional Neural Network for Sentiment Analysis
    (Text Classification)*. Machine Learning Mastery. Available at [https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/).
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jason Brownlee. (2020). *深度卷积神经网络在情感分析（文本分类）中的应用*。机器学习大师。可在[https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/)获取。
- en: 'Evaluation:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估：
- en: T., B. (2022, December 9). *Comprehensive guide on Multiclass classification
    metrics*. Medium. Available at [https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd](https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd).
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: T., B. (2022年12月9日). *多分类分类指标的综合指南*。Medium。可在[https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd](https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd)获取。
- en: Jason Brownlee (2021). *Tour of Evaluation Metrics for Imbalanced Classification*.
    Machine Learning Mastery. Available at https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jason Brownlee (2021). *不平衡分类的评估指标概览*。机器学习大师。可在https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/获取。
- en: Korstanje, J. (2021, August 31). *The F1 score*. Medium. Available at [https://towardsdatascience.com/the-f1-score-bec2bbc38aa6](https://towardsdatascience.com/the-f1-score-bec2bbc38aa6).
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korstanje, J. (2021年8月31日). *F1得分*。Medium。可在[https://towardsdatascience.com/the-f1-score-bec2bbc38aa6](https://towardsdatascience.com/the-f1-score-bec2bbc38aa6)获取。
