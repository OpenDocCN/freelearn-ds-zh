- en: Testing Apache Spark Jobs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试Apache Spark作业
- en: In this chapter, we will test Apache Spark jobs and learn how to separate logic
    from the Spark engine.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将测试Apache Spark作业，并学习如何将逻辑与Spark引擎分离。
- en: We will first cover unit testing of our code, which will then be used by the
    integration test in SparkSession. Later, we will be mocking data sources using
    partial functions, and then learn how to leverage ScalaCheck for property-based
    testing for a test as well as types in Scala. By the end of this chapter, we will
    have performed tests in different versions of Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先对我们的代码进行单元测试，然后在SparkSession中进行集成测试。之后，我们将使用部分函数模拟数据源，然后学习如何利用ScalaCheck进行基于属性的测试以及Scala中的类型。在本章结束时，我们将在不同版本的Spark中执行测试。
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Separating logic from Spark engine-unit testing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将逻辑与Spark引擎分离-单元测试
- en: Integration testing using SparkSession
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkSession进行集成测试
- en: Mocking data sources using partial functions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用部分函数模拟数据源
- en: Using ScalaCheck for property-based testing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ScalaCheck进行基于属性的测试
- en: Testing in different versions of Spark
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同版本的Spark中进行测试
- en: Separating logic from Spark engine-unit testing
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将逻辑与Spark引擎分离-单元测试
- en: Let's start by separating logic from the Spark engine.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从将逻辑与Spark引擎分离开始。
- en: 'In this section, we will cover the following topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下主题：
- en: Creating a component with logic
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建具有逻辑的组件
- en: Unit testing of that component
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该组件的单元测试
- en: Using the case class from the model class for our domain logic
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型类的案例类进行领域逻辑
- en: Let's look at the logic first and then the simple test.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看逻辑，然后是简单的测试。
- en: 'So, we have a `BonusVerifier` object that has only one method, `quaifyForBonus`,
    that takes our `userTransaction` model class. According to our login in the following
    code, we load user transactions and filter all users that are qualified for a
    bonus. First, we need to test it to create an RDD and filter it. We need to create
    a SparkSession and also create data for mocking an RDD or DataFrame, and then
    test the whole Spark API. Since this involves logic, we will test it in isolation.
    The logic is as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有一个`BonusVerifier`对象，只有一个方法`quaifyForBonus`，它接受我们的`userTransaction`模型类。根据以下代码中的登录，我们加载用户交易并过滤所有符合奖金资格的用户。首先，我们需要测试它以创建一个RDD并对其进行过滤。我们需要创建一个SparkSession，并为模拟RDD或DataFrame创建数据，然后测试整个Spark
    API。由于这涉及逻辑，我们将对其进行隔离测试。逻辑如下：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We have a list of super users with the `A`, `X`, and `100-million` user IDs.
    If our `userTransaction.userId` is within the `superUsers` list, and if the `userTransaction.amount`
    is higher than `100`, then the user qualifies for a bonus; otherwise, they don't.
    In the real world, the qualifier for bonus logic will be even more complex, and
    thus it is very important to test the logic in isolation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个超级用户列表，其中包括`A`、`X`和`100-million`用户ID。如果我们的`userTransaction.userId`在`superUsers`列表中，并且`userTransaction.amount`高于`100`，那么用户就有资格获得奖金；否则，他们就没有资格。在现实世界中，奖金资格逻辑将更加复杂，因此非常重要的是对逻辑进行隔离测试。
- en: 'The following code shows our test using the `userTransaction` model. We know
    that our user transaction includes `userId` and `amount`. The following example
    shows our domain model object, which is shared between a Spark execution integration
    test and our unit testing, separated from Spark:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了我们使用`userTransaction`模型的测试。我们知道我们的用户交易包括`userId`和`amount`。以下示例显示了我们的领域模型对象，它在Spark执行集成测试和我们的单元测试之间共享，与Spark分开：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We need to create our `UserTransaction` for user ID `X` and the amount `101`,
    as shown in the following example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为用户ID `X` 和金额`101`创建我们的`UserTransaction`，如下例所示：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will then pass `userTransaction` to `qualifyForBonus` and the result should
    be `true`. This user should qualify for a bonus, as shown in the following output:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将`userTransaction`传递给`qualifyForBonus`，结果应该是`true`。这个用户应该有资格获得奖金，如下输出所示：
- en: '![](img/49908d22-b2a3-4b0b-9150-e533a6cda546.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49908d22-b2a3-4b0b-9150-e533a6cda546.png)'
- en: 'Now, let''s write a test for the negative use case, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为负面用例编写一个测试，如下所示：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we have a user, `X`, that spends `99` for which our results should be
    false. When we validate our code, we can see, from the following output, that
    our test has passed:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一个用户`X`，花费`99`，所以我们的结果应该是false。当我们验证我们的代码时，我们可以看到从以下输出中，我们的测试已经通过了：
- en: '![](img/fee898ff-e179-4e5b-941a-a434a8c812c9.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fee898ff-e179-4e5b-941a-a434a8c812c9.png)'
- en: 'We have covered two cases, but in real-world scenarios, there are many more.
    For example, if we want to test the case where we are specifying `userId`, which
    is not from this superuser list, and we have `some_new_user` that spends a lot
    of money, in our case, `100000`, we get the following result:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了两种情况，但在现实世界的场景中，还有更多。例如，如果我们想测试指定`userId`不在这个超级用户列表中的情况，我们有一个花了很多钱的`some_new_user`，在我们的案例中是`100000`，我们得到以下结果：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s assume that it should not qualify, and so such logic is a bit complex.
    Therefore, we are testing it in a unit test way:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设它不应该符合条件，因此这样的逻辑有点复杂。因此，我们以单元测试的方式进行测试：
- en: '![](img/028fcf7e-9829-424e-a479-5497fc29c89d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/028fcf7e-9829-424e-a479-5497fc29c89d.png)'
- en: Our tests are very fast and so we are able to check that everything works as
    expected without introducing Spark at all. In the next section, we'll be changing
    the logic with integration testing using SparkSession.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试非常快，因此我们能够检查一切是否按预期工作，而无需引入Spark。在下一节中，我们将使用SparkSession进行集成测试来更改逻辑。
- en: Integration testing using SparkSession
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SparkSession进行集成测试
- en: Let's now learn about integration testing using SparkSession.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们学习如何使用SparkSession进行集成测试。
- en: 'In this section, we will cover the following topics:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下主题：
- en: Leveraging SparkSession for integration testing
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用SparkSession进行集成测试
- en: Using a unit tested component
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用经过单元测试的组件
- en: 'Here, we are creating the Spark engine. The following line is crucial for the
    integration test:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在创建Spark引擎。以下行对于集成测试至关重要：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It is not a simple line just to create a lightweight object. SparkSession is
    a really heavy object and constructing it from scratch is an expensive operation
    from the perspective of resources and time. Tests such as creating SparkSession
    will take more time compared to the unit testing from the previous section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个轻量级对象并不是一件简单的事情。SparkSession是一个非常重的对象，从头开始构建它是一项昂贵的操作，从资源和时间的角度来看。与上一节的单元测试相比，诸如创建SparkSession的测试将花费更多的时间。
- en: For the same reason, we should use unit tests often to convert all edge cases
    and use integration testing only for the smaller part of the logic, such as the
    capital edge case.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 出于同样的原因，我们应该经常使用单元测试来转换所有边缘情况，并且仅在逻辑的较小部分，如资本边缘情况时才使用集成测试。
- en: 'The following example shows the array we are creating:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了我们正在创建的数组：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following example shows the RDD we are creating:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了我们正在创建的RDD：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is the first time that Spark has been involved in our integration testing.
    Creating an RDD is also a time-consuming operation. Compared to just creating
    an array, it is really slow to create an RDD because that is also a heavy object.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Spark第一次参与我们的集成测试。创建RDD也是一个耗时的操作。与仅创建数组相比，创建RDD真的很慢，因为它也是一个重量级对象。
- en: 'We will now use our `data.filter` to pass a `qualifyForBonus` function, as
    shown in the following example:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用我们的`data.filter`来传递一个`qualifyForBonus`函数，如下例所示：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This function was already unit tested, so we don't need to consider all edge
    cases, different IDs, different amounts, and so on. We are just creating a couple
    of IDs with some amounts to test whether or not our whole chain of logic is working
    as expected.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数已经经过单元测试，所以我们不需要考虑所有边缘情况，不同的ID，不同的金额等等。我们只是创建了一些ID和一些金额来测试我们整个逻辑链是否按预期工作。
- en: 'After we have applied this logic, our output should be similar to the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 应用了这个逻辑之后，我们的输出应该类似于以下内容：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s start this test and check how long it takes to execute a single integration
    test, as shown in the following output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个测试，检查执行单个集成测试需要多长时间，如下输出所示：
- en: '![](img/e492184d-4a2b-4069-bb7f-86937d6c0393.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e492184d-4a2b-4069-bb7f-86937d6c0393.png)'
- en: It took around `646 ms` to execute this simple test.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这个简单测试大约需要`646毫秒`。
- en: 'If we want to cover every edge case, the value will be multiplied by a factor
    of hundreds compared to the unit test from the previous section. Let''s start
    this unit test with three edge cases, as shown in the following output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要覆盖每一个边缘情况，与上一节的单元测试相比，值将乘以数百倍。让我们从三个边缘情况开始这个单元测试，如下输出所示：
- en: '![](img/4cb08050-118a-499c-8694-469a568f3a1e.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cb08050-118a-499c-8694-469a568f3a1e.png)'
- en: We can see that our test took only `18 ms`, which means that it was 20 times
    faster, even though we covered three edge cases, compared to integration tests
    with only one case.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的测试只花了`18毫秒`，这意味着即使我们覆盖了三个边缘情况，与只有一个情况的集成测试相比，速度快了20倍。
- en: Here, we have covered a lot of logic with hundreds of edge cases, and we can
    conclude that it is really wise to have unit tests at the lowest possible level.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们覆盖了许多逻辑，包括数百个边缘情况，我们可以得出结论，尽可能低的级别进行单元测试是非常明智的。
- en: In the next section, we will be mocking data sources using partial functions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用部分函数来模拟数据源。
- en: Mocking data sources using partial functions
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用部分函数模拟数据源
- en: 'In this section, we will cover the following topics:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下主题：
- en: Creating a Spark component that reads data from Hive
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个从Hive读取数据的Spark组件
- en: Mocking the component
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟组件
- en: Testing the mock component
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模拟组件
- en: 'Let''s assume that the following code is our production line:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 假设以下代码是我们的生产线：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, we are using the `UserDataLogic.loadAndGetAmount` function, which needs
    to load our user data transaction and get the amount of the transaction. This
    method takes two arguments. The first argument is a `sparkSession` and the second
    argument is the `provider` of `sparkSession`, which takes `SparkSession` and returns
    `DataFrame`, as shown in the following example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`UserDataLogic.loadAndGetAmount`函数，它需要加载我们的用户数据交易并获取交易的金额。这个方法需要两个参数。第一个参数是`sparkSession`，第二个参数是`sparkSession`的`provider`，它接受`SparkSession`并返回`DataFrame`，如下例所示：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For production, we will load user transactions and see that the `HiveDataLoader`
    component has only one method, `sparkSession.sql`, and `("select * from transactions")`,
    as shown in the following code block:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产，我们将加载用户交易，并查看`HiveDataLoader`组件只有一个方法，`sparkSession.sql`和`("select * from
    transactions")`，如下代码块所示：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This means that the function goes to Hive to retrieve our data and returns a
    DataFrame. According to our logic, it executes the `provider` that is returning
    a DataFrame and from a DataFrame, it is only selecting `amount`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着该函数去Hive检索我们的数据并返回一个DataFrame。根据我们的逻辑，它执行了返回DataFrame的`provider`，然后从DataFrame中选择`amount`。
- en: 'This logic is not simple we can test because our SparkSession `provider` is
    interacting with the external system in production. So, we can create a function
    such as the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个逻辑并不简单，因为我们的SparkSession `provider`在生产中与外部系统进行交互。因此，我们可以创建一个如下的函数：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s see how to test such a component. First, we will create a DataFrame
    of user transactions, which is our mock data, as shown in the following example:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何测试这样一个组件。首先，我们将创建一个用户交易的DataFrame，这是我们的模拟数据，如下例所示：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: However, we need to save the data to Hive, embed it, and then start Hive.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们需要将数据保存到Hive中，嵌入它，然后启动Hive。
- en: 'Since we are using the partial functions, we can pass a partial function as
    a second argument, as shown in the following example:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了部分函数，我们可以将部分函数作为第二个参数传递，如下例所示：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first argument is `spark`, but it is not used in our method this time. The
    second argument is a method that is taking SparkSession and returning DataFrame.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是`spark`，但这次我们的方法中没有使用它。第二个参数是一个接受SparkSession并返回DataFrame的方法。
- en: However, our execution engine, architecture, and code do not consider whether
    this SparkSession is used or if the external call is made; it only wants to return
    DataFrame. We can `_` our first argument because it's ignored and just return
    DataFrame as the return type.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的执行引擎、架构和代码并不考虑这个SparkSession是否被使用，或者是否进行了外部调用；它只想返回DataFrame。我们可以使用`_`作为我们的第一个参数，因为它被忽略，只返回DataFrame作为返回类型。
- en: And so our `loadAndGetAmount` will get a mock DataFrame, which is the DataFrame
    that we created.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们的`loadAndGetAmount`将获得一个模拟DataFrame，这是我们创建的DataFrame。
- en: 'But, for the logic shown, it is transparent and doesn''t consider whether the
    DataFrame comes from Hive, SQL, Cassandra, or any other source, as shown in the
    following example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对于所示的逻辑，它是透明的，不考虑DataFrame是来自Hive、SQL、Cassandra还是其他任何来源，如下例所示：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In our example, `df` comes from the memory that we created for the purposes
    of the test. Our logic continues and it selects the amount.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，`df`来自我们为测试目的创建的内存。我们的逻辑继续并选择了数量。
- en: 'Then, we show our columns, `res.show()` , and that logic should end up with
    one column amount. Let''s start this test, as shown in the following example:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们展示我们的列，`res.show()`，并且该逻辑应该以一个列的数量结束。让我们开始这个测试，如下例所示：
- en: '![](img/8dca11c2-8423-432f-9233-897f3fba401b.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dca11c2-8423-432f-9233-897f3fba401b.png)'
- en: We can see from the preceding example that our resulting DataFrame has one column
    amount in `100` and `200` values. This means it worked as expected, without the
    need to start an embedding Hive. The key here is to use a provider and not embed
    our select start within our logic.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从上面的例子中看到，我们的结果DataFrame在`100`和`200`值中有一个列的数量。这意味着它按预期工作，而无需启动嵌入式Hive。关键在于使用提供程序而不是在逻辑中嵌入我们的选择开始。
- en: In the next section, we'll be using ScalaCheck for property-based tests.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用ScalaCheck进行基于属性的测试。
- en: Using ScalaCheck for property-based testing
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ScalaCheck进行基于属性的测试
- en: 'In this section, we will cover the following topics:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下主题：
- en: Property-based testing
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于属性的测试
- en: Creating a property-based test
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建基于属性的测试
- en: Let's look at a simple property-based test. We need to import a dependency before
    we define properties. We also need a dependency for the ScalaCheck library, which
    is a library for property-based tests.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的基于属性的测试。在定义属性之前，我们需要导入一个依赖项。我们还需要一个ScalaCheck库的依赖项，这是一个用于基于属性的测试的库。
- en: 'In the previous section, every test extended `FunSuite`. We used functional
    tests, but we had to provide arguments explicitly. In this example, we''re extending
    `Properties` from the ScalaCheck library and testing a `StringType`, as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，每个测试都扩展了`FunSuite`。我们使用了功能测试，但是必须显式提供参数。在这个例子中，我们扩展了来自ScalaCheck库的`Properties`，并测试了`StringType`，如下所示：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Our ScalaCheck will generate a random string for us. If we create a property-based
    test for a custom type, then that is not known to the ScalaCheck. We need to provide
    a generator that will generate instances of that specific type.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的ScalaCheck将为我们生成一个随机字符串。如果我们为自定义类型创建基于属性的测试，那么ScalaCheck是不知道的。我们需要提供一个生成器，它将生成该特定类型的实例。
- en: 'First, let''s define the first property of our string type in the following
    way:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们以以下方式定义我们字符串类型的第一个属性：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`forAll` is a method from the ScalaCheck property. We will pass an arbitrary
    number of arguments here, but they need to be of the type that we are testing.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`forAll`是ScalaCheck属性的一个方法。我们将在这里传递任意数量的参数，但它们需要是我们正在测试的类型。'
- en: Let's assume that we want to get two random strings, and in those strings, the
    invariants should be perceived.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要获得两个随机字符串，并且在这些字符串中，不变性应该被感知。
- en: 'If we are adding the length of string `a` to the length of string `b`, the
    sum of that should be greater or equal to `a.length,` because if `b` is `0` then
    it will be equal, as shown in the following example:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将字符串`a`的长度加上字符串`b`的长度，那么它们的总和应该大于或等于`a.length`，因为如果`b`是`0`，那么它们将相等，如下例所示：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: However, this is an invariant of `string` and for every input string, it should
    be `true`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这是`string`的不变性，对于每个输入字符串，它应该是`true`。
- en: 'The second property that we are defining is a bit more complex, as shown in
    the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在定义的第二个属性更复杂，如下代码所示：
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code, we asked the ScalaCheck runtime engine to share three
    strings this time, that is, `a`, `b`, and `c`. We will test this when we create
    a list of strings.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们要求ScalaCheck运行时引擎这次共享三个字符串，即`a`、`b`和`c`。当我们创建一个字符串列表时，我们将测试这个。
- en: 'Here, we are creating a list of strings, that is, `a`, `b`, `c`, as shown in
    the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在创建一个字符串列表，即`a`、`b`、`c`，如下代码所示：
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When we map every element to `length`, the sum of those elements should be equal
    to adding everything by length. Here, we have `a.length + b.length + c.length` and
    we will test the collections API to check if the map and other functions work
    as expected.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将每个元素映射到`length`时，这些元素的总和应该等于通过长度添加所有元素。在这里，我们有`a.length + b.length + c.length`，我们将测试集合API，以检查映射和其他函数是否按预期工作。
- en: 'Let''s start this property-based test to check if our properties are correct,
    as shown in the following example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个基于属性的测试，以检查我们的属性是否正确，如下例所示：
- en: '![](img/9319cfda-c10b-40bf-8d3b-9e41fa26fe7f.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9319cfda-c10b-40bf-8d3b-9e41fa26fe7f.png)'
- en: 'We can see that the `StringType.length` property of `string` passed and executed
    `100` tests. It could be surprising that `100` tests were executed, but let''s
    try to see what was passed as the arguments by using the following code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`string`的`StringType.length`属性通过并执行了`100`次测试。`100`次测试被执行可能会让人惊讶，但让我们尝试看看通过以下代码传递了什么参数：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will print the `a` argument and `b` argument, and retry our property by
    testing the following output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将打印`a`参数和`b`参数，并通过测试以下输出来重试我们的属性：
- en: '![](img/a7df281d-ce0d-408d-a174-96b691f2a45b.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7df281d-ce0d-408d-a174-96b691f2a45b.png)'
- en: We can see that a lot of weird strings were generated, so this is an edge case
    that we were not able to create up-front. Property-based testing will create a
    very weird unique code that isn't a proper string. So, this is a great tool for
    testing whether our logic is working as expected for a specific type.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到生成了许多奇怪的字符串，因此这是一个我们无法事先创建的边缘情况。基于属性的测试将创建一个非常奇怪的唯一代码，这不是一个合适的字符串。因此，这是一个用于测试我们的逻辑是否按预期针对特定类型工作的好工具。
- en: In the next section, we'll be testing in different versions of Spark.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将在不同版本的Spark中进行测试。
- en: Testing in different versions of Spark
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在不同版本的Spark中进行测试
- en: 'In this section, we will cover the following topics:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下主题：
- en: Changing the component to work with Spark pre-2.x
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将组件更改为与Spark pre-2.x一起使用
- en: Mock testing pre-2.x
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mock测试 pre-2.x
- en: RDD mock testing
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD模拟测试
- en: Let's start with the mocking data sources from the third section of this chapter—*Mocking
    data sources using partial functions*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从本章第三节开始，模拟数据源——*使用部分函数模拟数据源*。
- en: Since we were testing `UserDataLogic.loadAndGetAmount`, notice that everything
    operates on the DataFrame and thus we had a SparkSession and DataFrame.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在测试`UserDataLogic.loadAndGetAmount`，请注意一切都在DataFrame上操作，因此我们有一个SparkSession和DataFrame。
- en: 'Now, let''s compare it to the Spark pre-2.x. We can see that this time, we
    are unable to use DataFrames. Let''s assume that the following example shows our
    logic from the previous Sparks:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将其与Spark pre-2.x进行比较。我们可以看到这一次我们无法使用DataFrame。假设以下示例显示了我们在以前的Spark中的逻辑：
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can see that we are not able to use DataFrames this time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这一次我们无法使用DataFrame。
- en: 'In the previous section, `loadAndGetAmount` was taking `spark` and DataFrame,
    but the DataFrame in the following example is an RDD, not a DataFrame anymore,
    and so we are passing an `rdd`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，`loadAndGetAmount`正在接受`spark`和DataFrame，但在下面的示例中，DataFrame是一个RDD，不再是DataFrame，因此我们传递了一个`rdd`：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'However, we need to create a different `UserDataLogicPre2` for Spark that takes
    SparkSession and returns an RDD after mapping an RDD of an integer, as shown in
    the following example:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们需要为Spark创建一个不同的`UserDataLogicPre2`，它接受SparkSession并在映射整数的RDD之后返回一个RDD，如下例所示：
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding code, we can see that the `provider` is executing our provider
    logic, mapping every element, getting it as an `int`. Then, we get the amount.
    `Row` is a generic type that can have a variable number of arguments.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以看到`provider`正在执行我们的提供程序逻辑，映射每个元素，将其作为`int`获取。然后，我们得到了金额。`Row`是一个可以有可变数量参数的泛型类型。
- en: In Spark pre-2.x, we do not have `SparkSession` and therefore we need to use `SparkContext` and
    change our login accordingly.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark pre-2.x中，我们没有`SparkSession`，因此需要使用`SparkContext`并相应地更改我们的登录。
- en: Summary
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we first learned how to separate logic from the Spark engine.
    We then looked at a component that was well-tested in separation without the Spark
    engine, and we carried out integration testing using SparkSession. For this, we created
    a SparkSession test by reusing the component that was already well-tested. By
    doing that, we did not have to cover all edge cases in the integration test and
    our test was much faster. We then learned how to leverage partial functions to
    supply mocked data that's provided at the testing phase. We also covered ScalaCheck
    for property-based testing. By the end of this chapter, we had tested our code
    in different versions of Spark and learned how to change our DataFrame mock test
    to RDD.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先学习了如何将逻辑与Spark引擎分离。然后，我们查看了一个在没有Spark引擎的情况下经过良好测试的组件，并使用SparkSession进行了集成测试。为此，我们通过重用已经经过良好测试的组件创建了一个SparkSession测试。通过这样做，我们不必在集成测试中涵盖所有边缘情况，而且我们的测试速度更快。然后，我们学习了如何利用部分函数在测试阶段提供模拟数据。我们还介绍了ScalaCheck用于基于属性的测试。在本章结束时，我们已经在不同版本的Spark中测试了我们的代码，并学会了将DataFrame模拟测试更改为RDD。
- en: In the next chapter, we will learn how to leverage the Spark GraphX API.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何利用Spark GraphX API。
