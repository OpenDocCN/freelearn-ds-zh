- en: Chapter 4.  Unified Data Access
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 统一数据访问
- en: Data integration from disparate data sources had always been a daunting feat.
    The three V's of big data and ever-shrinking processing time frames have made
    the task even more challenging. Delivering a clear view of well-curated data in
    near real time is extremely important for business. However, real-time curated
    data along with the ability to perform different operations such as ETL, ad hoc
    querying, and machine learning in a unified fashion is what is emerging as a key
    business differentiator.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 来自不同数据源的数据集成一直是一项艰巨的任务。大数据的三个V和不断缩短的处理时间框架使这项任务变得更加具有挑战性。在几乎实时地提供清晰的精心策划的数据对于业务来说非常重要。然而，实时策划的数据以及在统一方式中执行ETL、临时查询和机器学习等不同操作的能力正在成为关键的业务差异化因素。
- en: Apache Spark was created to offer a single general-purpose engine that can process
    data from a variety of data sources and support large-scale data processing for
    various different operations. Spark enables developers to combine SQL, Streaming,
    graphs, and machine learning algorithms in a single workflow!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark的创建是为了提供一个可以处理各种数据源数据并支持各种不同操作的单一通用引擎。Spark使开发人员能够在单个工作流中结合SQL、流式处理、图形和机器学习算法！
- en: 'In the previous chapters, we discussed **Resilient Distributed Datasets** (**RDDs**)
    as well as DataFrames. In [Chapter 3](ch03.xhtml "Chapter 3.  Introduction to
    DataFrames"), *Introduction to DataFrames*, we introduced Spark SQL and the Catalyst
    optimizer. This chapter builds on this foundation and delves deeper into these
    topics to help you realize the real essence of unified data access. We''ll introduce
    new constructs such as Datasets and Structured Streaming. Specifically, we''ll
    discuss the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了**弹性分布式数据集**（**RDDs**）以及数据框架。在[第3章](ch03.xhtml "第3章 数据框架简介")中，*数据框架简介*，我们介绍了Spark
    SQL和Catalyst优化器。本章将在此基础上深入探讨这些主题，帮助您认识到统一数据访问的真正本质。我们将介绍新的构造，如数据集和结构化流。具体来说，我们将讨论以下内容：
- en: Data abstractions in Apache Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark中的数据抽象
- en: Datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集
- en: Working with Datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据集
- en: Dataset API limitations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集API的限制
- en: Spark SQL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: SQL operations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL操作
- en: Under the hood
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底层
- en: Structured Streaming
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流
- en: Spark streaming programming model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark流式编程模型
- en: Under the hood
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底层
- en: Comparison with other streaming engines
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他流处理引擎的比较
- en: Continuous applications
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续应用
- en: Summary
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: Data abstractions in Apache Spark
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark中的数据抽象
- en: The MapReduce framework and its popular open source implementation Hadoop enjoyed
    widespread adoption in the past decade. However, iterative algorithms and interactive
    ad-hoc querying are not well supported. Any data sharing between jobs or stages
    within an algorithm is always through disk writes and reads as against in-memory
    data sharing. So, the logical next step would be to have a mechanism that facilitates
    reuse of intermediate results across multiple jobs. RDD is a general-purpose data
    abstraction that was developed to address this requirement.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，MapReduce框架及其流行的开源实现Hadoop得到了广泛的应用。然而，迭代算法和交互式临时查询得到的支持并不好。作业或算法内部阶段之间的任何数据共享都是通过磁盘读写进行的，而不是通过内存数据共享。因此，逻辑上的下一步将是有一种机制，可以在多个作业之间重复使用中间结果。RDD是一个通用的数据抽象，旨在解决这一需求。
- en: RDD is the core abstraction in Apache Spark. It is an immutable, fault-tolerant
    distributed collection of statically typed objects that are usually stored in-memory.
    RDD API offer simple operations such as map, reduce, and filter that can be composed
    in arbitrary ways.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RDD是Apache Spark中的核心抽象。它是一个不可变的、容错的分布式集合，通常存储在内存中，其中包含静态类型的对象。RDD API提供了简单的操作，如map、reduce和filter，可以以任意方式组合。
- en: DataFrame abstraction is built on top of RDD and it adds "named" columns. So,
    a Spark DataFrame has rows of named columns similar to relational database tables
    and DataFrames in R and Python (pandas). This familiar higher level abstraction
    makes the development effort much easier because it lets you perceive data like
    an SQL table or an Excel file. Moreover, the Catalyst optimizer, under the hood,
    compiles the operations and generates JVM bytecode for efficient execution. However,
    the named columns approach gives rise to a new problem. Static type information
    is no longer available to the compiler, and hence we lose the advantage of compile-time
    type safety.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框架抽象建立在RDD之上，并添加了“命名”列。因此，Spark数据框架具有类似关系数据库表和R和Python（pandas）中的数据框架的命名列行。这种熟悉的高级抽象使开发工作变得更加容易，因为它让您可以像处理SQL表或Excel文件一样处理数据。此外，底层的Catalyst优化器编译操作并生成JVM字节码以进行高效执行。然而，命名列方法也带来了一个新问题。编译器不再具有静态类型信息，因此我们失去了编译时类型安全的优势。
- en: Dataset API was introduced to combine the best traits from both RDDs and DataFrames
    plus some more features of its own. Datasets provide row and column data abstraction
    similar to the DataFrames, but with a structure defined on top of them. This structure
    may be defined by a case class in Scala or a class in Java. They provide type
    safety and lambda functions like RDDs. So, they support both typed methods such
    as `map` and `groupByKey` as well as untyped methods such as `select` and `groupBy`.
    In addition to the Catalyst optimizer, Datasets leverage in-memory encoding provided
    by the Tungsten execution engine, which improves performance even further.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集API被引入，以结合RDD和数据框架的最佳特性，以及一些自己的特性。数据集提供了类似数据框架的行和列数据抽象，但在其之上定义了一种结构。这种结构可以由Scala中的case类或Java中的类定义。它们提供了类型安全和类似RDD的lambda函数。因此，它们支持诸如`map`和`groupByKey`之类的类型化方法，也支持诸如`select`和`groupBy`之类的无类型方法。除了Catalyst优化器外，数据集还利用了Tungsten执行引擎提供的内存编码，进一步提高了性能。
- en: The data abstractions introduced so far form the core abstractions. There are
    some more specialized data abstractions that work on top of these abstractions.
    Streaming APIs are introduced to process real-time streaming data from various
    sources such as Flume and Kafka. These APIs work together to provide data engineers
    a unified, continuous DataFrame abstraction that can be used for interactive and
    batch queries. Another example of specialized data abstraction is a GraphFrame.
    This enables developers to analyze social networks and any other graphs alongside
    Excel-like two-dimensional data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止引入的数据抽象形成了核心抽象。还有一些更专门的数据抽象是在这些抽象之上工作的。引入了流式API来处理来自各种来源（如Flume和Kafka）的实时流数据。这些API共同工作，为数据工程师提供了一个统一的、连续的DataFrame抽象，可用于交互式和批量查询。另一个专门的数据抽象的例子是GraphFrame。这使开发人员能够分析社交网络和任何其他图形，以及类似Excel的二维数据。
- en: 'Now with the basics of the available data abstractions in mind, let''s understand
    what we exactly mean by a unified data access platform:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑到现有数据抽象的基础知识，让我们了解一下我们所说的统一数据访问平台到底是什么：
- en: '![Data abstractions in Apache Spark](img/image_04_001.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark中的数据抽象](img/image_04_001.jpg)'
- en: The intention behind this unified platform is that it not only lets you combine
    the static and streaming data together, but also allows various different kinds
    of operations on the data in a unified way! From the developer's perspective,
    a Dataset is the core abstraction to work with, and Spark SQL is the main interface
    to the Spark functionality. A two-dimensional data structure coupled with a SQL
    declarative programming interface had been a familiar way of dealing with data,
    thereby shortening the learning curve for the data engineers. So, understanding
    the unified platform translates to understanding Datasets and Spark SQL.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 统一平台背后的意图是它不仅可以让您将静态和流式数据结合在一起，还可以以统一的方式对数据进行各种不同类型的操作！从开发人员的角度来看，数据集是与之一起工作的核心抽象，而Spark
    SQL是与Spark功能交互的主要接口。与SQL声明式编程接口相结合的二维数据结构一直是处理数据的一种熟悉方式，从而缩短了数据工程师的学习曲线。因此，理解统一平台意味着理解数据集和Spark
    SQL。
- en: Datasets
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: Apache Spark **Datasets** are an extension of the DataFrame API that provide
    a type-safe object-oriented programming interface. This API was first introduced
    in the 1.6 release. Spark 2.0 version brought out unification of DataFrame and
    Dataset APIs. DataFrame becomes a generic, untyped Dataset; or a Dataset is a
    DataFrame with an added structure. The term "structure" in this context refers
    to a pattern or an organization of underlying data, more like a table schema in
    RDBMS parlance. The structure imposes a limit on what can be expressed or contained
    in the underlying data. This in turn enables better optimizations in memory organization
    as well as physical execution. Compile-time type checking leads to catching errors
    earlier than during runtime. For example, a type mismatch in a SQL comparison
    does not get caught until runtime, whereas it would be caught during compile time
    itself if it were expressed as a sequence of operations on Datasets. However,
    the inherent dynamic nature of Python and R implies that there is no compile-time
    type safety, and hence the concept Datasets does not apply to those languages.
    The unification of Datasets and DataFrames applies to Scala and Java API only.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark的**数据集**是DataFrame API的扩展，提供了一种类型安全的面向对象的编程接口。这个API首次在1.6版本中引入。Spark
    2.0版本带来了DataFrame和Dataset API的统一。DataFrame变成了一个通用的、无类型的数据集；或者说数据集是一个带有附加结构的DataFrame。在这个上下文中，“结构”指的是底层数据的模式或组织，更像是RDBMS术语中的表模式。结构对可以在底层数据中表达或包含的内容施加了限制。这反过来使得内存组织和物理执行的优化更好。编译时类型检查导致在运行时之前捕获错误。例如，在SQL比较中的类型不匹配直到运行时才被捕获，而如果它被表达为对数据集的一系列操作，它将在编译时被捕获。然而，Python和R的固有动态特性意味着没有编译时类型安全，因此数据集的概念不适用于这些语言。数据集和DataFrame的统一仅适用于Scala和Java
    API。
- en: At the core of Dataset abstraction are the **encoders**. These encoders translate
    between JVM objects and Spark's internal Tungsten binary format. This internal
    representation bypasses JVM's memory management and garbage collection. Spark
    has its own C-style memory access that is specifically written to address the
    kind of workflows it supports. The resultant internal representations take less
    memory and have efficient memory management. Compact memory representation leads
    to reduced network load during shuffle operations. The encoders generate compact
    byte code that directly operates on serialized objects without de-serializing,
    thereby enhancing performance. Knowing the schema early on results in a more optimal
    layout in memory when caching Datasets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集抽象的核心是**编码器**。这些编码器在JVM对象和Spark内部Tungsten二进制格式之间进行转换。这种内部表示绕过了JVM的内存管理和垃圾回收。Spark有自己的C风格内存访问，专门用于解决它支持的工作流类型。由此产生的内部表示占用更少的内存，并具有高效的内存管理。紧凑的内存表示导致在洗牌操作期间减少网络负载。编码器生成紧凑的字节码，直接在序列化对象上操作，而无需反序列化，从而提高性能。早期了解模式会导致在缓存数据集时内存布局更加优化。
- en: Working with Datasets
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用数据集
- en: In this section, we will create Datasets and perform transformations and actions,
    much like DataFrames and RDDs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将创建数据集并执行转换和操作，就像DataFrame和RDD一样。
- en: 'Example 1-creating a Dataset from a simple collection:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 示例1-从简单集合创建数据集：
- en: '**Scala:**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As shown in the last example in the preceding code, `case class` adds structure
    information. Spark uses this structure to create the best data layout and encoding.
    The following code shows us the structure and the plan for execution:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在前面的代码中的最后一个示例中所示，`case class`添加了结构信息。Spark使用这个结构来创建最佳的数据布局和编码。以下代码向我们展示了结构和执行计划：
- en: '**Scala:**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding example shows the structure and the implementation physical plan
    as anticipated. If you want to get a more detailed execution plan, you have to
    pass explain (true), which prints extended information, including the logical
    plan as well.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子显示了预期的结构和实现物理计划。如果您想获得更详细的执行计划，您必须传递explain（true），这将打印扩展信息，包括逻辑计划。
- en: We have examined Dataset creation from simple collections and RDDs. We have
    already discussed that DataFrames are just untyped Datasets. The following examples
    show conversion between Datasets and DataFrames.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从简单集合和RDD中创建了数据集。我们已经讨论过DataFrame只是无类型数据集。以下示例显示了数据集和DataFrame之间的转换。
- en: Example 2-converting the Dataset to a DataFrame
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 示例2-将数据集转换为DataFrame
- en: '**Scala:**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This example looks very much like the examples we have seen in [Chapter 3](ch03.xhtml
    "Chapter 3.  Introduction to DataFrames"), *Introduction to DataFrames*. These
    conversions become very handy in the real world. Consider adding a structure (aka
    case class) to imperfect data. You may first read that data into a DataFrame,
    perform cleansing, and then convert it to a Dataset. Another use case could be
    that you want to expose only a subset (rows and columns) of the data based on
    some runtime information, such as `user_id`. You could read the data into a DataFrame,
    register it as a temporary table, apply conditions, and expose the subset as a
    Dataset. The following example creates a `DataFrame` first and then converts it
    into `Dataset`. Note that the DataFrame column names must match the case class.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子看起来非常像我们在[第3章](ch03.xhtml"第3章。DataFrame简介")中看到的例子，*DataFrame简介*。这些转换在现实世界中非常方便。考虑向不完整的数据添加结构（也称为案例类）。您可以首先将数据读入DataFrame，进行清洗，然后将其转换为数据集。另一个用例可能是，您希望基于某些运行时信息（例如`user_id`）仅公开数据的子集（行和列）。您可以将数据读入DataFrame，将其注册为临时表，应用条件，并将子集公开为数据集。以下示例首先创建一个`DataFrame`，然后将其转换为`Dataset`。请注意，DataFrame列名必须与案例类匹配。
- en: Example 3-convert a DataFrame to a Dataset
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 示例3-将DataFrame转换为数据集
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The explain command response shows `WholeStageCodegen`, which fuses multiple
    operations into a single Java function call. This enhances performance due to
    reduction in multiple virtual function calls. Code generation had been around
    in Spark engine since 1.1, but at that time it was limited to expression evaluation
    and a small number of operations such as filter. In contrast, whole stage code
    generation from Tungsten generates code for the entire query plan.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 解释命令的响应显示`WholeStageCodegen`，它将多个操作融合为单个Java函数调用。这通过减少多个虚拟函数调用来增强性能。自1.1以来，代码生成一直存在于Spark引擎中，但当时它仅限于表达式评估和一小部分操作，如过滤。相比之下，Tungsten的整个阶段代码生成为整个查询计划生成代码。
- en: Creating Datasets from JSON
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从JSON创建数据集
- en: Datasets can be created from JSON files, similar to DataFrames. Note that a
    JSON file may contain several records, but each record has to be on one line.
    If your source JSON has newlines, you have to programmatically remove them. The
    JSON records may have arrays and may be nested. They need not have uniform schema.
    The following example file has JSON records with one record having an additional
    tag and an array of data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以像DataFrame一样从JSON文件中创建。请注意，JSON文件可能包含多个记录，但每个记录必须在一行上。如果您的源JSON有换行符，您必须以编程方式将其删除。JSON记录可能包含数组并且可能是嵌套的。它们不需要具有统一的模式。以下示例文件包含具有附加标记和数据数组的JSON记录。
- en: Example 4-creating a Dataset from JSON
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 示例4-从JSON创建数据集
- en: '**Scala:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Datasets API's limitations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集API的限制
- en: 'Even though the Datasets API is created using the best of both RDDs and DataFrames,
    it still has some limitations as of its current stage of development:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据集API是使用RDD和DataFrame的最佳部分创建的，但在当前开发阶段仍存在一些限制：
- en: While querying the dataset, the selected fields should be given specific data
    types as in the case class, or else the output will become a DataFrame. An example
    is `auth.select(col("first_name").as[String])`.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在查询数据集时，所选字段应该具有与案例类相同的特定数据类型，否则输出将变为DataFrame。例如`auth.select(col("first_name").as[String])`。
- en: Python and R are inherently dynamic in nature, and hence typed Datasets do not
    fit in.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python和R在本质上是动态的，因此类型化的数据集不适合。
- en: Spark SQL
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: '**Spark SQL** is a Spark module for structured data processing that was introduced
    in Spark 1.0\. This module is a tightly integrated relational engine that inert-operates
    with the core Spark API. It enables data engineers to write applications that
    load structured data from disparate sources and join them as a unified, and possibly
    continuous, Excel-like data frames; and then they can implement complex ETL workflows
    and advanced analytics.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark SQL**是Spark 1.0引入的用于结构化数据处理的Spark模块。该模块是一个与核心Spark API紧密集成的关系引擎。它使数据工程师能够编写应用程序，从不同来源加载结构化数据，并将它们作为统一的、可能连续的类似Excel的数据框进行连接；然后他们可以实现复杂的ETL工作流和高级分析。'
- en: The Spark 2.0 release brought in significant unification of APIs and expanded
    the SQL capabilities, including support for subqueries. The Dataset API and DataFrames
    API are now unified, with DataFrames being a "kind" of Datasets. The unified APIs
    build the foundation for Spark's future, spanning across all libraries. Developers
    can impose "structure" onto their data and can work with high-level declarative
    APIs, thereby improving performance as well as their productivity. The performance
    gains come as a result of the underlying optimization layer. DataFrames, Datasets,
    and SQL share the same optimization and execution pipeline.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0版本带来了API的显著统一和扩展的SQL功能，包括对子查询的支持。数据集API和DataFrame API现在是统一的，DataFrame是数据集的一种“类型”。统一的API为Spark的未来奠定了基础，跨越所有库。开发人员可以将“结构”强加到其数据上，并可以使用高级声明性API，从而提高性能和生产率。性能增益是由底层优化层带来的。数据框，数据集和SQL共享相同的优化和执行管道。
- en: SQL operations
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL操作
- en: SQL operations are most widely used constructs for data manipulation. Some of
    most used operations are, selecting all or some of the columns, filtering based
    on one or more conditions, sorting and grouping operations, and computing summary
    functions such as `average` on GroupedData. The  `JOIN` operations on multiple
    data sources and `set` operations such as `union`, `intersect` and `minus` are
    some other operations that are widely performed. Furthermore, data frames are
    registered as temporary tables and passed traditional SQL statements to perform
    the aforementioned operations. **User-Defined Functions** (**UDF**) are defined
    and used with and without registration. We'll be focusing on window operations,
    which have been just introduced in Spark 2.0\. They address sliding window operations.
    For example, if you want to report the average peak temperature every day in the
    past seven days, then you are operating on a sliding window of seven days until
    today. Here is an example that computes average sales per month for the past three
    months. The data file contains 24 observations showing monthly sales for two products,
    P1 and P2.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: SQL操作是用于数据操作的最广泛使用的构造。一些最常用的操作是，选择所有或一些列，基于一个或多个条件进行过滤，排序和分组操作，以及计算`average`等汇总函数。多个数据源上的`JOIN`操作和`set`操作，如`union`，`intersect`和`minus`，是广泛执行的其他操作。此外，数据框被注册为临时表，并传递传统的SQL语句来执行上述操作。**用户定义的函数**（**UDF**）被定义并用于注册和不注册。我们将专注于窗口操作，这是在Spark
    2.0中刚刚引入的。它们处理滑动窗口操作。例如，如果您想要报告过去七天内每天的平均最高温度，那么您正在操作一个直到今天的七天滑动窗口。这是一个示例，计算过去三个月每月的平均销售额。数据文件包含24个观测值，显示了两种产品P1和P2的月销售额。
- en: Example 5-window example with moving average computation
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 示例5-使用移动平均计算的窗口示例
- en: '**Scala:**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Python:**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Under the hood
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在幕后
- en: When a developer is writing programs using RDD API, efficient execution for
    the workload on hand is his/her responsibility. The data types and computations
    are not available for Spark. In contrast, when a developer is using DataFrames
    and Spark SQL, the underlying engine has information about the schema and operations.
    In this case, the developer can write less code while the optimizer does all the
    hard work.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当开发人员使用RDD API编写程序时，高效执行手头的工作负载是他/她的责任。数据类型和计算对于Spark来说是不可用的。相比之下，当开发人员使用DataFrames和Spark
    SQL时，底层引擎具有关于模式和操作的信息。在这种情况下，开发人员可以编写更少的代码，而优化器会做所有的艰苦工作。
- en: The Catalyst optimizer contains libraries for representing trees and applying
    rules to transform the trees. These tree transformations are applied to create
    the most optimized logical and physical execution plans. In the final phase, it
    generates Java bytecode using a special feature of the Scala language called **quasiquotes**.
    The optimizer also enables external developers to extend the optimizer by adding
    data-source-specific rules that result in pushing operations to external systems,
    or support for new data types.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器包含用于表示树和应用规则以转换树的库。这些树转换被应用于创建最优化的逻辑和物理执行计划。在最后阶段，它使用Scala语言的一个特殊功能**quasiquotes**生成Java字节码。优化器还使外部开发人员能够通过添加数据源特定规则来扩展优化器，这些规则导致将操作推送到外部系统，或支持新的数据类型。
- en: 'The Catalyst optimizer arrives at the most optimized plan to execute the operations
    on hand. The actual execution and related improvements are provided by the Tungsten
    engine. The goal of Tungsten is to improve the memory and CPU efficiency of Spark
    backend execution. The following are some salient features of this engine:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器得出了最优化的计划来执行手头的操作。实际的执行和相关的改进由Tungsten引擎提供。Tungsten的目标是提高Spark后端执行的内存和CPU效率。以下是该引擎的一些显着特点：
- en: Reducing the memory footprint and eliminating garbage collection overheads by
    bypassing (off-heap) Java memory management.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过绕过（堆外）Java内存管理来减少内存占用和消除垃圾收集开销。
- en: Code generation fuses across multiple operators and too many virtual function
    calls are avoided. The generated code looks like hand-optimized code.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成在多个操作符之间融合，避免了过多的虚拟函数调用。生成的代码看起来像手动优化的代码。
- en: Memory layout is in columnar, in-memory parquet format because that enables
    vectorized processing and is also closer to usual data access operations.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存布局是以列为基础的，内存中的parquet格式，因为这样可以实现矢量化处理，并且更接近通常的数据访问操作。
- en: In-memory encoding using encoders. Encoders use runtime code generation to build
    custom byte code for faster and compact serialization and deserialization. Many
    operations can be performed in-place without deserialization because they are
    already in Tungsten binary format.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编码器进行内存编码。编码器使用运行时代码生成来构建自定义字节码，以实现更快和更紧凑的序列化和反序列化。许多操作可以在原地执行，而无需反序列化，因为它们已经处于Tungsten二进制格式中。
- en: Structured Streaming
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: Streaming is a seemingly broad topic! If you take a closer look at the real-world
    problems, businesses do not just want a streaming engine to make decisions in
    real time. There has always been a need to integrate both batch stack and streaming
    stack, and integrate with external storage systems and applications. Also, the
    solution should be such that it should adapt to dynamic changes in business logic
    to address new and changing business requirements.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理似乎是一个广泛的话题！如果您仔细观察现实世界的问题，企业不仅希望流处理引擎实时做出决策。一直以来，都需要集成批处理栈和流处理栈，并与外部存储系统和应用程序集成。此外，解决方案应该能够适应业务逻辑的动态变化，以满足新的和不断变化的业务需求。
- en: Apache Spark 2.0 has the first version of the higher level stream processing
    API called the **Structured Streaming** engine. This scalable and fault-tolerant
    engine leans on the Spark SQL API to simplify the development of real-time, continuous
    big data applications. It is probably the first successful attempt in unifying
    the batch and streaming computation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 2.0具有称为**结构化流**引擎的高级流处理API的第一个版本。这个可扩展和容错的引擎依赖于Spark SQL API来简化实时连续的大数据应用程序的开发。这可能是统一批处理和流处理计算的第一次成功尝试。
- en: 'At a technical level, Structured Streaming leans on the Spark SQL API, which
    extends DataFrames/Datasets, which we already discussed in the previous sections.
    Spark 2.0 lets you perform radically different activities in a unified way, such
    as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术层面上，结构化流依赖于Spark SQL API，它扩展了数据框/数据集，我们在前面的部分已经讨论过。Spark 2.0让您以统一的方式执行根本不同的活动，例如：
- en: Building ML models and applying them on streaming data
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建ML模型并将其应用于流数据
- en: Combining streaming data with other static data
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将流数据与其他静态数据结合
- en: Performing ad hoc, interactive, and batch queries
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行临时、交互和批量查询
- en: Changing queries at runtime
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行时更改查询
- en: Aggregating data streams and serving using Spark SQL JDBC
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合数据流并使用Spark SQL JDBC提供服务
- en: Unlike other streaming engines, Spark lets you combine real-time **Streaming
    Data** with **Static data** and lets you perform the preceding operations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他流式引擎不同，Spark允许您将实时**流数据**与**静态数据**结合，并执行前述操作。
- en: '![Structured Streaming](img/image_04_002.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![结构化流](img/image_04_002.jpg)'
- en: Fundamentally, Structured Streaming is empowered by Spark SQL's Catalyst optimizer.
    So, it frees up the developers from worrying about the underlying plumbing of
    making queries more efficient while dealing with static or real-time streams of
    data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，结构化流由Spark SQL的Catalyst优化器赋予了能力。因此，它使开发人员不必担心处理静态或实时数据流时使查询更有效的底层管道。
- en: As of this writing, Structured Streaming of Spark 2.0 is focused on ETL, and
    later versions will have more operators and libraries.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，Spark 2.0的结构化流主要集中在ETL上，以后的版本将具有更多的操作符和库。
- en: Let us look at a simple example. The following example listens to **System Activity
    Report** (**sar**) on Linux on a local machine and computes the average free memory.
    System Activity Report gives system activity statistics and the current example
    collects memory usage, reported 20 times at a 2-second interval. The Spark stream
    reads this streaming output and computes average memory. We use a handy networking
    utility **netcat** (**nc**) to redirect the `sar` output onto a given port. The
    options `l` and `k` specify that `nc` should listen for an incoming connection
    and it has to keep listening for another connection even after its current connection
    is completed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子。以下示例监听Linux上本地机器上的**系统活动报告**（**sar**）并计算平均空闲内存。系统活动报告提供系统活动统计信息，当前示例收集内存使用情况，以2秒的间隔报告20次。Spark流读取这个流式输出并计算平均内存。我们使用一个方便的网络实用工具**netcat**（**nc**）将`sar`输出重定向到给定端口。选项`l`和`k`指定`nc`应该监听传入连接，并且在当前连接完成后，它必须继续监听另一个连接。
- en: '**Scala:**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: Example 6-Streaming example
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 示例6-流式示例
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Python:**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding example defined a continuous data frame (also known as stream)
    to listen to a particular port, perform some transformations, and aggregations
    and show continuous output.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例定义了一个连续的数据框（也称为流）来监听特定端口，执行一些转换和聚合，并显示连续的输出。
- en: The Spark streaming programming model
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark流式编程模型
- en: 'As demonstrated earlier in this chapter, there is just a single API to take
    care of both static and streaming data. The idea is to treat the real-time data
    stream as a table that is continuously being appended, as shown in the following
    figure:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章前面所示，只需使用单个API来处理静态和流数据。其想法是将实时数据流视为不断追加的表，如下图所示：
- en: '![The Spark streaming programming model](img/image_04_003.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![Spark流式编程模型](img/image_04_003.jpg)'
- en: 'So whether for static or streaming data, you just fire up the batch-like queries
    as you would do on static data tables, and Spark runs it as an incremental query
    on the unbounded input table, as shown in the following figure:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论是静态数据还是流数据，您只需像在静态数据表上那样启动类似批处理的查询，Spark会将其作为无界输入表上的增量查询运行，如下图所示：
- en: '![The Spark streaming programming model](img/image_04_004.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![Spark流式编程模型](img/image_04_004.jpg)'
- en: 'So, the developers define a query on the input table, in the same way for both
    static-bounded as well as dynamic-unbounded table. Let us understand the various
    technical jargons for this whole process to understand how it works:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，开发人员以相同的方式在输入表上定义查询，无论是静态有界表还是动态无界表。让我们了解整个过程的各种技术术语，以了解它是如何工作的：
- en: '**Input:** Data from sources as an append-only table'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入：**来自源的追加表的数据'
- en: '**Trigger:** When to check the input for new data'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**触发器：**何时检查输入以获取新数据'
- en: '**Query:** What operation to perform on the data, such as filter, group, and
    so on'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询：**对数据执行的操作，例如过滤、分组等'
- en: '**Result:** The resultant table at every trigger interval'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果：**每个触发间隔的结果表'
- en: '**Output:** Choose what part of the result to write to the data sink after
    every trigger'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出：**选择在每个触发后写入数据接收器的结果的哪一部分'
- en: 'Let''s now look at how the Spark SQL planner treats the whole process:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看Spark SQL规划器如何处理整个过程：
- en: '![The Spark streaming programming model](img/image_04_005.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![Spark流式编程模型](img/image_04_005.jpg)'
- en: 'Courtesy: Databricks'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 提供：Databricks
- en: The preceding screenshot is very simply explained in the structured programming
    guide at the official Apache Spark site, as indicated in the *References* section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的屏幕截图在官方Apache Spark网站的结构化编程指南中有非常简单的解释，如*参考*部分所示。
- en: '![The Spark streaming programming model](img/image_04_006.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![Spark流式编程模型](img/image_04_006.jpg)'
- en: 'At this point, we need to know about the supported output models. Every time
    the result table is updated, the changes need to be written to an external system,
    such as HDFS, S3, or any other database. We usually prefer to write output incrementally.
    For this purpose, Structured Streaming provides three output modes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要了解支持的输出模式。每次更新结果表时，更改都需要写入外部系统，如 HDFS、S3 或任何其他数据库。我们通常倾向于增量写入输出。为此，结构化流提供了三种输出模式：
- en: '**Append:** In the external storage, only the new rows appended to the result
    table since the last trigger will be written. This is applicable only on queries
    where existing rows in the result table cannot change (for example, a map on an
    input stream).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Append:**在外部存储中，自上次触发以来追加到结果表的新行将被写入。这仅适用于查询，其中结果表中的现有行不会更改（例如，对输入流的映射）。'
- en: '**Complete:** In the external storage, the entire updated result table will
    be written as is.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Complete:**在外部存储中，整个更新的结果表将按原样写入。'
- en: '**Update:** In the external storage, only the rows that were updated in the
    result table since the last trigger will be changed. This mode works for output
    sinks that can be updated in place, such as a MySQL table.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新：**在外部存储中，自上次触发以来在结果表中更新的行将被更改。此模式适用于可以就地更新的输出接收器，例如 MySQL 表。'
- en: In our example, we used complete mode, which was straightaway writing to the
    console. You may want to write into some external file such as Parquet to get
    a better understanding.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用了完整模式，直接写入控制台。您可能希望将数据写入一些外部文件，如 Parquet，以便更好地理解。
- en: Under the hood
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 底层原理
- en: 'If you look at the "behind the screen" execution mechanism of the operations
    performed on **DataFrames/Datasets**, it would appear as the following figure
    suggests:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看在**DataFrames/Datasets**上执行的操作的“幕后”执行机制，它将如下图所示：
- en: '![Under the hood](img/image_04_007.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![底层原理](img/image_04_007.jpg)'
- en: 'Please note here that the **Planner** knows apriori how to convert a streaming
    **Logical Plan** to a continuous series of **Incremental Execution Plans**. This
    can be represented by the following figure:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**Planner**事先知道如何将流式**Logical Plan**转换为一系列**Incremental Execution Plans**。这可以用以下图表示：
- en: '![Under the hood](img/image_04_008.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![底层原理](img/image_04_008.jpg)'
- en: The **Planner** can poll the data sources for new data to be able to plan the
    execution in an optimized way.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**Planner**可以轮询数据源以便以优化的方式规划执行。'
- en: Comparison with other streaming engines
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与其他流式引擎的比较
- en: 'We have discussed many unique features of Structured Streaming. Let us now
    have a comparative view with other available streaming engines:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了结构化流的许多独特特性。现在让我们与其他可用的流式引擎进行比较：
- en: '![Comparison with other streaming engines](img/image_04_009.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![与其他流式引擎的比较](img/image_04_009.jpg)'
- en: 'Courtesy: Databricks'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 提供：Databricks
- en: Continuous applications
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续应用程序
- en: We discussed how unified data access is empowered by Spark. It lets you process
    data in a myriad of ways to build end-to-end continuous applications by enabling
    various analytic workloads, such as ETL processing, ad hoc queries, online machine
    learning modeling, or to generate necessary reports... all of this in a unified
    way by letting you work on both static as well as streaming data using a high-level,
    SQL-like API. In this way, Structured Streaming has substantially simplified the
    development and maintenance of real-time, continuous applications.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了Spark如何赋予统一数据访问的能力。它让您以多种方式处理数据，通过启用各种分析工作负载来构建端到端的连续应用程序，例如ETL处理、adhoc查询、在线机器学习建模，或生成必要的报告...所有这些都可以通过让您使用高级的、类似SQL的API来处理静态和流式数据的方式来统一进行，从而大大简化了实时连续应用程序的开发和维护。
- en: '![Continuous applications](img/image_04_010.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![连续应用程序](img/image_04_010.jpg)'
- en: 'Courtesy: Databricks'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 提供：Databricks
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed what is really meant by unified data access and
    how Spark serves this purpose. We took a closer look at the Datasets API and how
    real-time streaming is empowered through it. We learned the advantages of Datasets
    and also their limitations. We also looked at the fundamentals behind continuous
    applications.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了统一数据访问的真正含义以及Spark如何实现这一目的。我们仔细研究了Datasets API以及实时流如何通过它得到增强。我们了解了Datasets的优势以及它们的局限性。我们还研究了连续应用程序背后的基本原理。
- en: In the following chapter, we will look at the various ways in which we can leverage
    the Spark platform for data analysis operations at scale.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨利用Spark平台进行规模化数据分析操作的各种方法。
- en: References
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf](http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf)
    : Spark SQL: Relational Data Processing in Spark'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf](http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf)：Spark
    SQL：Spark中的关系数据处理'
- en: '[https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)
    : A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets - When to
    use them and why'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)：三种Apache
    Spark API的故事：RDDs、DataFrames和Datasets-何时使用它们以及为什么'
- en: '[https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html](https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html)
    : Introducing Apache Spark Datasets'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html](https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html)：介绍Apache
    Spark Datasets'
- en: '[https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)
    : Deep Dive into Spark SQL''s Catalyst Optimizer'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)：深入了解Spark
    SQL的Catalyst优化器'
- en: '[https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)
    : Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)
    : Apache Spark作为编译器：在笔记本电脑上每秒连接十亿行'
- en: '[https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)
    : Bringing Spark closer to baremetal'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)
    : 将Spark靠近裸金属'
- en: '[https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)
    : Structured Streaming API details'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)
    : Apache Spark中的结构化流API详细信息'
- en: '[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
    : Spark Structured Streaming Programming Guide'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
    : Spark结构化流编程指南'
- en: '[https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/](https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/):
    Structuring Apache Spark SQL, DataFrames, Datasets, and Streaming by Michael Armbrust'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/](https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/):
    Michael Armbrust介绍Apache Spark SQL，DataFrames，Datasets和Streaming'
- en: '[https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html):
    Apache Spark Key terms explained'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html):
    Apache Spark关键术语解释'
