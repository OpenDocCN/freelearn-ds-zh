- en: Generative Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型
- en: So far in this book, we have covered the three main types of neural networks—**feedforward
    neural networks** (**FNNs**), **convolutional neural networks** (**CNNs**), and
    **recurrent neural networks** (**RNNs**). Each of them are discriminative models;
    that is, they learned to discriminate (differentiate) between the classes we wanted
    them to be able to predict, such as *is this language French or English?*, *is
    this song classic rock or 90s pop?*, and *what are the objects present in this
    scene?*. However, deep neural networks don't just stop there. They can also be
    used to improve image or video resolution or generate entirely new images and
    data. These types of models are known as **generative models**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们已经介绍了三种主要的神经网络类型——**前馈神经网络**（**FNNs**）、**卷积神经网络**（**CNNs**）和**循环神经网络**（**RNNs**）。它们都是判别模型；也就是说，它们学会了区分（区分）我们希望它们能够预测的类别，例如*这个语言是法语还是英语？*，*这首歌是经典摇滚还是90年代流行歌？*，以及*这个场景中有哪些物体？*。然而，深度神经网络不仅仅止步于此。它们还可以用于提升图像或视频的分辨率，或者生成全新的图像和数据。这些类型的模型被称为**生成模型**。
- en: 'In this chapter, we will cover the following topics related to generative models:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖与生成模型相关的以下主题：
- en: Why we need generative models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们需要生成模型
- en: Autoencoders
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器
- en: Generative adversarial networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: Flow-based networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于流的网络
- en: Why we need generative models
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们需要生成模型
- en: All the various neural network architectures we have learned about in this book
    have served a specific purpose—to make a prediction about some given data. Each
    of these neural networks has its own respective strengths for various tasks. A
    CNN is very effective for object recognition tasks or music genre classification,
    an RNN is very effective for language translation or time series prediction, and
    FNNs are great for regression or classification. Generative models, on the other
    hand, are those that model the data, *p(x)*, that we can sample data from, which
    is different from discriminative models, which learn to estimate conditional distributions,
    such as *p(•|x)*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们学习到的各种神经网络架构都有一个特定的目的——对给定的数据进行预测。每个神经网络在不同的任务中都有其各自的优势。CNN在物体识别任务或音乐类型分类中非常有效，RNN在语言翻译或时间序列预测中非常有效，而FNNs在回归或分类任务中表现优异。另一方面，生成模型是那些建模数据*p(x)*的模型，我们可以从中抽样数据，这与判别模型不同，后者学习估计条件分布，例如*p(•|x)*。
- en: But how does this benefit us? What can we use generative models for? Well, there
    are a couple of reasons why it is important for us to understand how generative
    models work. To start, in image recognition, we have to learn to estimate a high-dimensional
    space of the *p(y[i ]| x)* form*,* which we can use to predict which class our
    data belongs to. You should remember that these models require a lot of training
    data. Now, what we could do instead is make it so that our data is generated from
    a low-dimensional latent variable, ![](img/027d2051-202b-49c6-a219-f8a475795732.png),
    which makes our probability function come to [![](img/4f4a5787-45fc-414c-996b-013313462790.png)].
    What we need to do now is change our prediction problem top, *(y[i ]| z)*. Another
    way that we can make use of generative models is to understand what our neural
    networks have learned. As we know, deep neural networks are quite complex and
    knowing what exactly they have or haven't learned is quite challenging to figure
    out. So, what we can do is sample from them and compare these drawn samples to
    the real data. Lastly, we can use generative models to create synthetic data to
    train our models on if we have a shortage of data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但这对我们有什么好处呢？我们可以用生成模型做什么？嗯，有几个原因解释了为什么理解生成模型的工作原理对我们很重要。首先，在图像识别中，我们必须学习估计一个高维空间，形式为*p(y[i]|
    x)*，我们可以用它来预测我们的数据属于哪个类别。你应该记得，这些模型需要大量的训练数据。那么，我们可以做的是让我们的数据从低维潜在变量生成，![](img/027d2051-202b-49c6-a219-f8a475795732.png)，这会使我们的概率函数变成[![](img/4f4a5787-45fc-414c-996b-013313462790.png)]。现在我们需要做的是将我们的预测问题改成*(y[i]|
    z)*。我们还可以通过另一种方式利用生成模型，那就是理解我们的神经网络学到了什么。正如我们所知，深度神经网络相当复杂，要弄清楚它们究竟学到了什么或者没有学到什么是非常具有挑战性的。因此，我们可以从中采样，并将这些采样与真实数据进行比较。最后，如果数据不足，我们可以使用生成模型来创建合成数据，进而训练我们的模型。
- en: Now that we know what generative models can be used for, let's explore some
    of the more popular ones and learn how they work.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道生成模型可以用来做什么，让我们来探索一些更常见的生成模型，并了解它们是如何工作的。
- en: Autoencoders
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: An **autoencoder** is an unsupervised type of FNN that learns to reconstruct
    high-dimensional data using latent-encoded data. You can think of it as trying
    to learn an identity function (that is, take *x*as input and then predict *x*).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器**是一种无监督的前馈神经网络（FNN），它通过潜在编码的数据来学习重构高维数据。你可以把它看作是在尝试学习一个恒等函数（也就是说，输入*x*并预测输出*x*）。'
- en: 'Let''s start by taking a look at the following diagram, which shows you what
    an autoencoder looks like:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看下面的图开始，这将展示自编码器的样子：
- en: '![](img/d72007f7-570f-492d-a99a-81b0f5419821.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d72007f7-570f-492d-a99a-81b0f5419821.png)'
- en: As you can see, the network is split into two components—an encoder and a decoder—which
    are mirror images of each other. The two components are connected to each other
    through a bottleneck layer (sometimes referred to as either a latent-space representation
    or compression) that has dimensions that are a lot smaller than the input. You
    should note that the network architecture is symmetric, but that doesn't necessarily
    mean its weights need be. But why? What does this network learn and how does it
    do it? Let's take a look at both networks and explore what they're doing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，网络分为两个部分——编码器和解码器——它们是彼此的镜像。两个部分通过一个瓶颈层（有时称为潜在空间表示或压缩）连接，该层的维度远小于输入层。你应该注意到，网络结构是对称的，但这并不意味着它的权重必须对称。那么为什么呢？这个网络学习了什么，它是如何做到的？我们来看看这两个网络，探索它们的作用。
- en: 'The encoder network takes in high-dimensional input and reduces it to lower-dimensional
    latent code (that is, it learns the patterns in the input data). This works similarly
    to principal component analysis and matrix factorization. It works as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器网络接收高维输入，并将其减少到低维潜在编码（即它学习输入数据中的模式）。这类似于主成分分析和矩阵分解。其工作原理如下：
- en: '![](img/c5caa1c6-61d8-436a-8faa-def8e087c0fd.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5caa1c6-61d8-436a-8faa-def8e087c0fd.png)'
- en: 'The decoder network takes as input the lower-dimensional latent code (the patterns),
    which contains all the main information about the input, and reconstructs the
    original input (or as close to the original input as possible) from it. It works
    as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器网络接收低维潜在编码（即模式）作为输入，潜在编码包含了关于输入的所有主要信息，并从中重构原始输入（或尽可能接近原始输入）。其工作原理如下：
- en: '![](img/ad1243a9-b986-4eb0-b79a-a37179e012cb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad1243a9-b986-4eb0-b79a-a37179e012cb.png)'
- en: 'We can combine the preceding two equations and express the autoencoder as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面的两个方程结合起来，表达为如下形式：
- en: '![](img/0251ffe4-131f-4928-a10f-d6bb16787405.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0251ffe4-131f-4928-a10f-d6bb16787405.png)'
- en: Our goal is for the original input to be as close (ideally, identical) to the
    reconstructed output—that is, ![](img/a3f50e2e-0a9a-46ef-b7ad-5e5693082f85.png).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是使原始输入与重构输出尽可能接近（理想情况下，相同）——也就是说，![](img/a3f50e2e-0a9a-46ef-b7ad-5e5693082f85.png)。
- en: 'Both the encoder and decoder have separate weights, but we learn the parameters
    together to output the reconstructed data, which is nearly identical to the original
    input. During training, we can use the **MSE** loss:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器有各自的权重，但我们一起学习这些参数，以输出重构数据，该数据几乎与原始输入相同。在训练过程中，我们可以使用**均方误差（MSE）**损失：
- en: '![](img/55975824-f4be-4347-812a-195b276dca84.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55975824-f4be-4347-812a-195b276dca84.png)'
- en: This type of autoencoder is commonly referred to as an **undercomplete autoencoder**
    because the bottleneck layer is much smaller than the dimensions of the input
    and the output layer.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的自编码器通常被称为**欠完备自编码器**，因为瓶颈层的维度远小于输入和输出层的维度。
- en: But what goes on in this bottleneck layer that allows the decoder to reconstruct
    the input from it? This latent coding, which is a high-dimensional space being
    mapped to a lower-dimensional one, learns a manifold, which is a topological space
    that resembles Euclidean space at each point (we will shine more light on topological
    spaces and manifolds in [Chapter 12](9e02b8b3-2351-4537-9ec1-88f2946ed358.xhtml),
    *Geometric Deep Learning*). We can represent this manifold as a vector field and
    visualize the data clusters. It is this vector field that the autoencoder is learning
    to reconstruct inputs from. Each data point can be found on this manifold and
    we can project this back into higher-dimensional space to reconstruct it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在这个瓶颈层中发生了什么，使得解码器能够从中重建输入数据呢？这种潜在编码是将高维空间映射到低维空间的过程，它学习了一个流形，流形是一个拓扑空间，在每个点上都类似欧几里得空间（我们将在[第12章](9e02b8b3-2351-4537-9ec1-88f2946ed358.xhtml)《几何深度学习》中详细讲解拓扑空间和流形）。我们可以将这个流形表示为一个向量场，并可视化数据的聚类。正是这个向量场，自动编码器正在学习如何从中重建输入数据。每个数据点都可以在这个流形上找到，我们可以将其投射回高维空间以重建它。
- en: 'Let''s suppose we have the MNIST dataset, which contains images of handwritten
    digits from 0-9\. In the following screenshot, we can see some of the images from
    the dataset:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 MNIST 数据集，它包含了从 0 到 9 的手写数字图像。在以下截图中，我们可以看到一些来自数据集的图像：
- en: '![](img/6b98beeb-e3ce-4df3-a008-0c6be83c8a93.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b98beeb-e3ce-4df3-a008-0c6be83c8a93.png)'
- en: 'The encoder network takes this data as input and encodes it into a lower-dimensional
    latent bottleneck layer, which contains a compressed representation of this higher-dimensional
    input and shows it to us in two dimensions. This embedding space looks as follows,
    where each of the colors represents a specific digit:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器网络将这些数据作为输入，并将其编码成一个低维的潜在瓶颈层，这个瓶颈层包含了高维输入的压缩表示，并将其显示在二维空间中。这个嵌入空间如下所示，其中每种颜色代表一个特定的数字：
- en: '![](img/17071535-d5ac-49b4-bc03-e9e34d47f9b6.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17071535-d5ac-49b4-bc03-e9e34d47f9b6.png)'
- en: About now, you are probably wondering what purpose an architecture such as this
    serves. What could we gain from training a model to recreate and output its own
    input? A number of things, as it turns out—we could use it to compress data and
    store it to save space and reconstruct it when we need to access it, we could
    remove noise from images or audio files, or we could use it for dimensionality
    reduction for data visualization.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能会想知道这样的架构有什么用途。我们通过训练模型来重建并输出自己的输入能得到什么好处呢？结果表明，我们可以从中获得许多好处——我们可以利用它来压缩数据并存储，从而节省空间，等到需要访问时再重建数据；我们还可以从图像或音频文件中去除噪声，或者用它进行数据可视化的降维处理。
- en: However, just because this architecture can be used to compress images, doesn't
    mean this is similar to a data compression algorithm such as MP3 or JPEG. An autoencoder
    is only able to compress data that it has seen during training, so if it was trained
    on images of cars, it would be quite ineffective in compressing images of horses
    since the features it has learned are specific to cars, which don't generalize
    well to horses. Compression algorithms such as MP3 and JPEG, on the other hand,
    don't learn the features of the inputs they receive; they make general assumptions
    about their inputs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅因为这种架构可以用来压缩图像，并不意味着它与 MP3 或 JPEG 这样的数据压缩算法相似。自动编码器只能压缩它在训练过程中见过的数据，因此，如果它是基于汽车图像进行训练的，那么它在压缩马的图像时效果会非常差，因为它学习到的特征是特定于汽车的，这些特征不能很好地推广到马身上。另一方面，像
    MP3 和 JPEG 这样的压缩算法并不会学习输入的特征；它们会对输入做出一般性假设。
- en: 'In the following diagram, you can see an autoencoder compressing an image into
    latent space and reconstructing it in the output:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到自动编码器将一张图像压缩到潜在空间，并在输出中重建它：
- en: '![](img/95f51791-3bdc-407e-8563-5ec844780914.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95f51791-3bdc-407e-8563-5ec844780914.png)'
- en: You can see, in the diagram, that the autoencoder has managed to reconstruct
    the input image and it still looks like the number four, but it isn't an exact
    replica; some of the information has been lost. This isn't an error in training;
    this is by design. Autoencoders are designed to be *lossy* and only approximately
    copy the input data so that it can extract only what is necessary by prioritizing
    what it deems more useful.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图中看到，自动编码器成功地重建了输入图像，并且它仍然看起来像数字四，但它不是完全复制的；一些信息丢失了。这不是训练中的错误，这是有意为之。自动编码器设计为*有损的*，仅近似复制输入数据，从而能够提取出必要的信息，优先考虑它认为更有用的部分。
- en: As we have seen so far in this book, adding layers and going deeper into autoencoders
    does have its advantages; it allows our neural network to capture greater complexities
    and reduces the required computational cost (in comparison to going wider and
    shallower). Similarly, we can add additional layers to our encoder and decoder.
    This is particularly true in the case of dealing with images because we know that
    convolutional layers bring better results than flattening the image and using
    it as input.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中看到的，增加层数并深入自编码器确实有其优势；它允许我们的神经网络捕捉更大的复杂性，并减少所需的计算成本（与加宽变浅相比）。类似地，我们可以为编码器和解码器添加额外的层。这在处理图像时尤为重要，因为我们知道卷积层比将图像展平并作为输入使用能带来更好的结果。
- en: Let's now explore some of the variations of autoencoders that allow us to achieve
    the aforementioned tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索一些自编码器的变体，这些变体可以帮助我们实现上述任务。
- en: The denoising autoencoder
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: The **denoising autoencoder** (**DAE**) is a variation of the preceding autoencoder
    as it learns to reconstruct corrupted or noisy inputs with near certainty. Suppose
    we have an image and, for some reason, it is blurry or some of the pixels have
    been corrupted and we'd like to improve the resolution of the image (kind of how
    they do in the movies when they can find clues in images with relatively low resolution).
    We can pass it through our DAE and get back a fully reconstructed image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**去噪自编码器**（**DAE**）是前述自编码器的变体，它学习如何以接近确定性的方式重构被损坏或带噪声的输入。假设我们有一张图像，由于某些原因，它变得模糊，或者某些像素已经损坏，我们希望提高图像的分辨率（就像电影中，他们能够从相对低分辨率的图像中找到线索一样）。我们可以通过我们的
    DAE 来处理它，得到一张完全重构的图像。'
- en: We start by corrupting the initial input using a conditional distribution, [![](img/48d9fd15-b6d5-4734-bc0c-0ea2d324eef4.png)]—which
    is basically a stochastic mapping—and it returns back to us the corrupted samples.
    Now that we have our new input, our autoencoder will learn how to reconstruct
    the uncorrupted data—that is, [![](img/cdcfb74f-cb73-4ede-8f26-47bbeb429aea.png)]—and
    to train this, our data will be the [![](img/ac839076-6290-470a-90a9-605594b4e505.png)] pairs.
    What we want the decoder to learn is [![](img/cbeb2203-75c2-49f2-b8e3-20329c634b53.png),] where
    as before, *z* was the output of the encoder.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用条件分布[![](img/48d9fd15-b6d5-4734-bc0c-0ea2d324eef4.png)]开始损坏初始输入——这基本上是一个随机映射——它将返回给我们损坏的样本。现在我们有了新的输入，我们的自编码器将学习如何重构未损坏的数据——即[![](img/cdcfb74f-cb73-4ede-8f26-47bbeb429aea.png)]——为了训练这个，我们的数据将是[![](img/ac839076-6290-470a-90a9-605594b4e505.png)]配对。我们希望解码器学习的是[![](img/cbeb2203-75c2-49f2-b8e3-20329c634b53.png)，]在之前的情形中，*z*
    是编码器的输出。
- en: 'The preceding corruption works as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的损坏过程如下：
- en: '![](img/994ab74e-bc89-4e82-beec-a58712976674.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/994ab74e-bc89-4e82-beec-a58712976674.png)'
- en: Here, *σ²* is the variance of the noise.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*σ²* 是噪声的方差。
- en: 'We can train our DAE just as any other FNN and perform gradient descent on
    the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像训练任何其他前馈神经网络（FNN）一样训练我们的 DAE，并在以下内容上执行梯度下降：
- en: '![](img/b77edc12-94b3-42b3-aa3d-962f89e3fe00.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b77edc12-94b3-42b3-aa3d-962f89e3fe00.png)'
- en: Here, [![](img/ef738a85-58c6-4ba5-a214-3a9d04ef9af2.png)] is the distribution
    of the training data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，[![](img/ef738a85-58c6-4ba5-a214-3a9d04ef9af2.png)] 是训练数据的分布。
- en: As mentioned, the encoder projects high-dimensional data into a lower-dimensional
    space, called **latent space**, and learns the shape of the manifold. It then
    tries to map the corrupted data onto or near to this manifold to figure out what
    it could be and then pieces it together in the reconstruction process to obtain
    *x* by estimating [![](img/49c3ecbb-84c6-4e50-8271-cb027389c7d8.png)] and minimizing
    the square error, [![](img/d7b19ff6-a81c-4be3-8a6f-532cfcce6398.png)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，编码器将高维数据投影到低维空间，这个空间被称为**潜在空间**，并学习流形的形状。然后，它尝试将损坏的数据映射到这个流形上或接近流形，以确定它可能是什么，然后在重构过程中将其拼接起来，通过估计[![](img/49c3ecbb-84c6-4e50-8271-cb027389c7d8.png)]并最小化平方误差[![](img/d7b19ff6-a81c-4be3-8a6f-532cfcce6398.png)]来得到
    *x*。
- en: 'We can view this process in the following diagram:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下图中查看这个过程：
- en: '![](img/512b9835-4815-4deb-a858-d919d12181cd.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/512b9835-4815-4deb-a858-d919d12181cd.png)'
- en: Here, the black curve is the learned manifold in the latent space and you can
    see the noisy points, ![](img/868b5a11-6951-4911-b0cb-5db8b860c160.png), are projected
    onto the closest point on the manifold to estimate what it could be.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，黑色曲线是潜在空间中学习到的流形，你可以看到噪声点， ![](img/868b5a11-6951-4911-b0cb-5db8b860c160.png)，它们被投影到流形上最接近的点，以估计它们可能是什么。
- en: The variational autoencoder
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: The **variational autoencoder** (**VAE**) is another type of autoencoder, but
    with some particular differences. In fact, instead of learning functions, *f()* and
    *g()*, it learns the probability density function of the input data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**变分自编码器**（**VAE**）是另一种类型的自编码器，但它有一些特别的区别。事实上，它不是学习函数 *f()* 和 *g()*，而是学习输入数据的概率密度函数。'
- en: 'Let''s suppose we have a distribution, *p[θ]*, and it is parameterized by θ.
    Here, we can express the relationship between *x* and *z* as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个分布 *p[θ]*，它由 θ 参数化。在这里，我们可以表达 *x* 和 *z* 之间的关系，如下所示：
- en: '*p[θ]*(*z)*: The prior'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p[θ]*(*z*): 先验分布'
- en: '*p[θ]*(*x *| *z)*: The likelihood (the distribution of the input given the
    latent space)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p[θ]*(*x*|*z*): 似然函数（给定潜在空间的输入分布）'
- en: '*p[θ](z *| *x)*: The posterior (the distribution of the latent space given
    the input)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p[θ](z*|*x*): 后验分布（给定输入的潜在空间分布）'
- en: The aforementioned distributions are parameterized by neural networks, which
    enables them to capture complex nonlinearities and, as we know, we train them
    using gradient descent.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的分布是由神经网络参数化的，这使得它们能够捕捉复杂的非线性特征，并且正如我们所知道的，我们通过梯度下降来训练它们。
- en: But why did the authors of this method decide to deviate from the previous approach
    to learning a distribution? There are a few reasons why this is more effective.
    To start, the data we would often be dealing with is noisy and so instead, modelling
    the distribution is better for us. The goal here, as you may have guessed, is
    to generate data that has a statistic that is similar to that of the input.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么该方法的作者决定偏离之前的分布学习方法呢？有几个原因表明这种方法更有效。首先，我们常常处理的数据是有噪声的，因此建模分布对我们来说更为合适。这里的目标，正如你可能已经猜到的，是生成具有与输入数据相似统计特征的数据。
- en: 'Before we move further, let''s take a look at what a VAE looks like:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步讨论之前，让我们来看一下 VAE 的结构：
- en: '![](img/05560f37-c3a9-4577-b778-ccd6d466ea74.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05560f37-c3a9-4577-b778-ccd6d466ea74.png)'
- en: As you can see, it shares some similarities with the autoencoder but, as we
    mentioned, instead of *z *= *f*(*x*) and *x' *= *g*(*z*), we learn *p *= (*z *| *x*) and *p *= (*x*
    | *z*), respectively. However, because there is now a random variable in between
    the input and the output, this architecture cannot be trained through regular
    backpropagation; we instead do backpropagation through the latent distribution's
    parameters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它与自编码器有一些相似之处，但正如我们所提到的，*z* = *f*(*x*) 和 *x'* = *g*(*z*) 的关系不再成立，我们学习的是
    *p* = (*z* | *x*) 和 *p* = (*x* | *z*)，分别表示输入和输出之间的分布。然而，由于现在输入和输出之间有一个随机变量，这种架构不能通过常规的反向传播来训练；我们需要通过潜在分布的参数来进行反向传播。
- en: 'Once we know the prior and likelihood distributions and the real parameters, *θ^**, we
    can generate samples by repeatedly doing the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道了先验分布、似然分布和真实参数 *θ^**，我们可以通过反复执行以下步骤来生成样本：
- en: Randomly generate samples from [![](img/8c20a7d0-fdd4-4ee1-b103-d7fac9970784.png).]
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机生成来自[![](img/8c20a7d0-fdd4-4ee1-b103-d7fac9970784.png)]的样本。
- en: Generate a [![](img/f6f5782a-900c-4f3f-a71d-f3a23909dd88.png) ]sample.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个[![](img/f6f5782a-900c-4f3f-a71d-f3a23909dd88.png)]样本。
- en: Using our knowledge of probability from [Chapter 3](719fc119-9e7a-4fce-be04-eb1e49bed753.xhtml),
    *Probability and Statistics*, we know that *θ^** should maximize the probability
    of a real data sample being generated; that is, [![](img/421c4862-83be-4bee-afe8-726d2ec9a759.png)].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们在[第3章](719fc119-9e7a-4fce-be04-eb1e49bed753.xhtml)《概率与统计》中学到的概率知识，我们知道 *θ^**
    应该最大化生成真实数据样本的概率；即，[![](img/421c4862-83be-4bee-afe8-726d2ec9a759.png)]。
- en: 'The equation used to generate the data now is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成数据的方程现在如下所示：
- en: '![](img/9171c9c5-59e2-4b3a-b492-957e990b943e.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9171c9c5-59e2-4b3a-b492-957e990b943e.png)'
- en: 'Now, suppose we can approximate the distribution of *x* by repeatedly sampling *z[i]*,
    as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们可以通过反复采样 *z[i]* 来逼近 *x* 的分布，如下所示：
- en: '![](img/20d3ca5b-6f53-4b30-a811-f19e81b43fba.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20d3ca5b-6f53-4b30-a811-f19e81b43fba.png)'
- en: 'But, in order to do this, we would need a lot of samples, the majority of which
    would likely be zero or close to zero. This is intractable (that is, not computationally
    practical). So, what we do instead is learn another distribution (that is tractable)—[![](img/8e402986-ad06-40b0-bc9b-10840cd86f07.png)]—to
    approximate the posterior,  [![](img/f18cf9db-05ce-4863-acaa-9234f32dab91.png)].
    Naturally, we want these two distributions to be close to each other so that they
    are able to better approximate the posterior distribution; so, we use **Kullback-Leibler**
    (**KL**) **divergence** to measure the distance between them and try to minimize
    it with respect to φ. We can see how we do this in the following equations:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，要做到这一点，我们需要大量的样本，其中大多数样本可能是零或接近零的。这是不可处理的（即在计算上不可行）。因此，我们所做的是学习另一个分布（即可处理的分布）——[![](img/8e402986-ad06-40b0-bc9b-10840cd86f07.png)]——以近似后验分布，[![](img/f18cf9db-05ce-4863-acaa-9234f32dab91.png)]。自然地，我们希望这两个分布尽可能接近，以便它们能够更好地近似后验分布；因此，我们使用**Kullback-Leibler**（**KL**）**散度**来衡量它们之间的距离，并尽量通过φ来最小化这个距离。我们可以通过以下公式来看我们是如何做到的：
- en: '![](img/ed9330f0-f13e-4b44-92d8-ab992675540a.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed9330f0-f13e-4b44-92d8-ab992675540a.png)'
- en: 'From Bayes'' rule, we know the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据贝叶斯规则，我们知道以下公式：
- en: '![](img/626d0eba-dbd7-4a40-b4fd-5a400a618659.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/626d0eba-dbd7-4a40-b4fd-5a400a618659.png)'
- en: 'If we take its logarithm, we get the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对其取对数，会得到以下结果：
- en: '![](img/2ac4ede8-3db1-4349-bbf5-f2bfedf63e61.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ac4ede8-3db1-4349-bbf5-f2bfedf63e61.png)'
- en: 'We can plug this back into the equation for KL divergence and get the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其代入KL散度的公式中，得到以下结果：
- en: '![](img/290ba423-80cd-429e-8a32-889b6f1a7d9d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/290ba423-80cd-429e-8a32-889b6f1a7d9d.png)'
- en: Since *p*(*x*) doesn't depend on *z*, we can keep it on the outside.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*p*(*x*)不依赖于*z*，我们可以将其保留在外面。
- en: 'We can now rearrange the equation into the following form:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将公式重新排列成以下形式：
- en: '![](img/0f22aa0f-a974-49b8-a609-1af841c05c64.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f22aa0f-a974-49b8-a609-1af841c05c64.png)'
- en: Since [![](img/cf20ab81-e971-4392-904f-2a9eccabb4a6.png)], the goal here is
    to maximize the lower bound of [![](img/128ecb66-7f7b-4649-bcc9-48e95720cc8f.png)] because [![](img/9457681b-2850-4c3d-8c49-bb25a5ca47e2.png)],
    and we do so because the output of KL divergence is non-zero and non-negative.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 [![](img/cf20ab81-e971-4392-904f-2a9eccabb4a6.png)]，这里的目标是最大化 [![](img/128ecb66-7f7b-4649-bcc9-48e95720cc8f.png)] 的下界，因为 [![](img/9457681b-2850-4c3d-8c49-bb25a5ca47e2.png)]，我们这么做是因为KL散度的输出是非零且非负的。
- en: 'But wait—what is the encoder and what is the decoder? This is an autoencoder,
    after all. Interestingly, it has been right in front of us all along. The encoder
    in a VAE is [![](img/607228c7-74f5-4d5a-884b-4fcde1f4979c.png)] and is usually
    assumed to be Gaussian:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 等等——编码器是什么，解码器又是什么？毕竟这是一个自编码器。有趣的是，它一直就在我们面前。VAE中的编码器是 [![](img/607228c7-74f5-4d5a-884b-4fcde1f4979c.png)] ，通常假设它是高斯分布的：
- en: '![](img/56baf6da-fe75-4de4-92e0-159d8b3c7c19.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56baf6da-fe75-4de4-92e0-159d8b3c7c19.png)'
- en: The decoder is [![](img/4bdb1aef-6c4d-4b6b-9220-63d9544af357.png).] Both of
    these are modeled using neural networks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器是 [![](img/4bdb1aef-6c4d-4b6b-9220-63d9544af357.png).] 这两者都是通过神经网络建模的。
- en: Generative adversarial networks
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: The **generative adversarial network** (**GAN**) is a game theory-inspired neural
    network architecture that was created by Ian Goodfellow in 2014\. It comprises
    two networks—a generator network and a critic network—both of which compete against
    each other in a minimax game, which allows both of them to improve simultaneously
    by trying to better the other.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GAN**）是一种受博弈论启发的神经网络架构，由Ian Goodfellow于2014年创建。它由两个网络组成——生成器网络和判别器网络——这两个网络在一个极小化博弈中相互竞争，使得它们能够通过努力超越对方来同时改进。'
- en: In the last couple of years, GANs have produced some phenomenal results in tasks
    such as creating images that are indistinguishable from real images, generating
    music when given some recordings, and even generating text. But these models are
    known for being notoriously difficult to train. Let's now find out what exactly
    GANs are, how they bring about such tremendous results, and what makes them so
    challenging to train.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，GAN在任务上取得了令人瞩目的成果，比如创造出与真实图像无法区分的图像、在给定一些录音的情况下生成音乐，甚至生成文本。但这些模型以训练困难而著称。现在，让我们来看看GAN到底是什么，它们是如何带来如此惊人的结果的，以及它们为何如此难以训练。
- en: As we know, discriminative models learn a conditional distribution and try to
    predict a label given input data—that is, *P(Y | X)*. Generative models, on the
    other hand, model a joint distribution—that is, *P(X, Y)*—and, using Bayes' rule,
    they can, when given the label, generate the data. So, like VAEs, they learn the
    distribution, *P(X)*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，判别模型学习条件分布，并试图给定输入数据预测一个标签——即 *P(Y | X)*。而生成模型则建模联合分布——即 *P(X, Y)*——并且，利用贝叶斯定理，在给定标签时，它们可以生成数据。所以，像
    VAE 一样，它们学习分布 *P(X)*。
- en: The critic network is a discriminator (*D*) with parameters, *θ^((D))*, and
    its job is to determine whether the data being fed into it is real or fake. The
    generator network is a generator (*G*) with parameters, *θ^((G))*, whose job is
    to learn to create synthetic data samples from noise that can fool the discriminator
    into thinking the synthetic data is real with a high probability.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 评论网络是一个判别器 (*D*)，它有参数 *θ^((D))*, 它的任务是判断输入数据是真实的还是伪造的。生成器网络是一个生成器 (*G*)，它有参数
    *θ^((G))*, 任务是学习从噪声中创建合成数据样本，并使判别器高概率地认为这些合成数据是真实的。
- en: As we have seen in this book, discriminator models are brilliant at learning
    to map input data to a desired label (output) and can determine whether an object
    is present in an image, as well as tracking an object in a video and translating
    languages. However, they are unable to use what they have learned to generate
    entirely new data the way we are able to use our imagination.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中所见，判别器模型非常擅长学习将输入数据映射到期望的标签（输出），并能够判断一个对象是否出现在图像中，追踪视频中的物体以及进行语言翻译。然而，它们无法像我们一样利用所学知识生成全新的数据。
- en: 'Before we proceed, let''s take a look at what this architecture looks like.
    In the following diagram, you can see how GANs are structured:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们看看这个架构是怎样的。在下图中，你可以看到 GAN 的结构：
- en: '![](img/40b465e7-16bb-4cfd-8f66-ba8fb25e9282.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40b465e7-16bb-4cfd-8f66-ba8fb25e9282.png)'
- en: 'Now that we know what GANs look like, let''s see how they work. We can summarize
    a GAN with the following equation:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道 GAN 的结构是怎样的，接下来让我们看看它是如何工作的。我们可以用以下方程来总结 GAN：
- en: '![](img/31fed236-4edc-4fb9-9dec-8f122cc1ab1d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31fed236-4edc-4fb9-9dec-8f122cc1ab1d.png)'
- en: The discriminator's goal is for [![](img/f81c3f4e-92fd-4fa0-be07-5e0501343897.png)] and [![](img/1da30232-4881-49a6-a8ed-6494bf0a7bf5.png)],
    while the generator's goal is for [![](img/ab926380-284a-4422-8c5c-da1fac2adc7a.png)].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的目标是让 [![](img/f81c3f4e-92fd-4fa0-be07-5e0501343897.png)] 和 [![](img/1da30232-4881-49a6-a8ed-6494bf0a7bf5.png)]，而生成器的目标是让
    [![](img/ab926380-284a-4422-8c5c-da1fac2adc7a.png)]。
- en: 'Since the generator and discriminator have different goals, naturally, they
    would have different cost functions. The respective losses for the discriminator
    and the generator are as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成器和判别器有不同的目标，自然它们会有不同的成本函数。判别器和生成器的各自损失如下：
- en: '[![](img/109a40d3-30ba-4b01-b5e0-908ecb13ac35.png)]'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/109a40d3-30ba-4b01-b5e0-908ecb13ac35.png)]'
- en: '[![](img/7f419fab-1427-43d2-b731-924ca5ed369e.png)]'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/7f419fab-1427-43d2-b731-924ca5ed369e.png)]'
- en: 'Naturally, neither of the two networks has a direct effect on the parameters
    of the other. As mentioned, since this is a game-theoretic-inspired architecture,
    we treat this as a two-player game and our objective is to find the Nash equilibria
    for cases where *x* is as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，两个网络之间没有直接影响对方的参数。如前所述，由于这是一个受博弈论启发的架构，我们将其视为一个双人博弈，我们的目标是找到当 *x* 如下时的纳什均衡：
- en: '![](img/291c82b4-7896-462f-b7b5-242643fb58dc.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/291c82b4-7896-462f-b7b5-242643fb58dc.png)'
- en: This is a saddle point. When we achieve this, the discriminator is unable to
    differentiate between the real data and the generated data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个鞍点。当我们达到这个点时，判别器无法区分真实数据和生成的数据。
- en: 'How do we now find the optimal value for the discriminator? Well, to start,
    we know the loss function and from it, we can find the optimal *D*(*x*) value:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们现在如何找到判别器的最优值呢？首先，我们知道损失函数，通过它我们可以找到最优的 *D*(*x*) 值：
- en: '![](img/260a4d2f-7168-4f94-90d0-e809ffe3d2af.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/260a4d2f-7168-4f94-90d0-e809ffe3d2af.png)'
- en: 'However, when trained, the generator ideally outputs *x*, so we can rewrite
    the loss function as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在训练时，生成器理想情况下会输出 *x*，因此我们可以将损失函数重写为：
- en: '![](img/c84c5302-08ae-4e03-9fd9-89205e92bbfe.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c84c5302-08ae-4e03-9fd9-89205e92bbfe.png)'
- en: 'Here, *p[r]* is the real data distribution and *p[g]* is the generated data
    distribution. Now, we have the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*p[r]* 是实际的数据分布，*p[g]* 是生成的数据分布。现在，我们有以下内容：
- en: '![](img/b56b112a-cf62-443a-ad49-af8588a9415c.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b56b112a-cf62-443a-ad49-af8588a9415c.png)'
- en: 'To make life a bit easier, let''s substitute parts of our equation with the
    following variables:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让生活稍微简单一些，让我们用以下变量替代方程中的部分内容：
- en: '[![](img/6213aff8-ec9c-4e54-9e9d-f4414f7e5e88.png)]'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/6213aff8-ec9c-4e54-9e9d-f4414f7e5e88.png)]'
- en: '[![](img/927d0693-2d9f-45d9-8636-c8775eaa0ccb.png)]'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/927d0693-2d9f-45d9-8636-c8775eaa0ccb.png)]'
- en: '[![](img/baa45245-695d-4137-bd58-bd1ab899d5a1.png)]'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/baa45245-695d-4137-bd58-bd1ab899d5a1.png)]'
- en: 'Since we are sampling over all the possible values of *x*, we can write the
    preceding three variables as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们是对所有可能的 *x* 值进行采样，我们可以将前面的三个变量写成如下形式：
- en: '![](img/b5919b48-c8b7-4114-ba23-781a606c0ea5.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5919b48-c8b7-4114-ba23-781a606c0ea5.png)'
- en: 'Now, to find the optimal value of the discriminator, we equate the preceding
    derivative to 0 and get the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了找到判别器的最优值，我们将前面的导数设为0，并得到以下结果：
- en: '![](img/923deeaa-23a0-4031-91a2-bb816fba906e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/923deeaa-23a0-4031-91a2-bb816fba906e.png)'
- en: 'So, when [![](img/b9b8b765-d95a-456b-a813-32c5c64bd69d.png)], [![](img/f8fc37cb-8d6d-411c-bcf4-dfae37d22703.png)],
    which satisfies our condition. The loss function now becomes the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当 [![](img/b9b8b765-d95a-456b-a813-32c5c64bd69d.png)]， [![](img/f8fc37cb-8d6d-411c-bcf4-dfae37d22703.png)]，这满足了我们的条件。损失函数现在变成了如下：
- en: '![](img/ea87562f-0b41-42ad-b2c8-78250f7de0e6.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea87562f-0b41-42ad-b2c8-78250f7de0e6.png)'
- en: 'Now that we know how to find the optimal discriminator, naturally, you may
    be wondering how we can find the optimal generator. Our goal here is to minimize
    the **Jensen–Shannon** (**JS**) divergence between the true and generated distributions,
    which is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何找到最优的判别器，自然，你可能会想知道如何找到最优的生成器。我们的目标是最小化**Jensen–Shannon**（**JS**）散度，计算真实分布和生成分布之间的差异，公式如下：
- en: '![](img/ff1eb53f-681e-4029-bd0c-9d5fcda33012.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff1eb53f-681e-4029-bd0c-9d5fcda33012.png)'
- en: So, [![](img/69392744-4b77-4862-bc40-0bf406354007.png)], which tells us that
    if our generator is in fact optimal, then ![](img/ed4cef85-4c81-4a68-97b4-cf5a07de2a33.png).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以， [![](img/69392744-4b77-4862-bc40-0bf406354007.png)]，这告诉我们，如果我们的生成器确实是最优的，那么！[](img/ed4cef85-4c81-4a68-97b4-cf5a07de2a33.png)。
- en: There you have it—that's how GANs work. However, there are certain problems
    associated with GANs. In particular, the convergence of the two networks is not
    guaranteed since the gradient descent of either model does not directly impact
    the other, and model parameters tend to oscillate and destabilize. Another problem
    is mode collapse, which is a result of improper convergence, which means the generator
    only outputs a select few generated samples, which it knows will trick the discriminator
    into thinking are real. Since the generator starts to output the same few samples
    over and over again, the discriminator learns to classify them as fake. Mode collapse
    is a rather challenging problem to solve. Lastly, our discriminator could become
    so good that the gradient of the generator vanishes and it ends up not learning
    anything at all.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是GAN的工作原理。然而，GAN存在一些问题，特别是两个网络的收敛性并不保证，因为其中任何一个模型的梯度下降不会直接影响另一个模型，而且模型参数往往会发生震荡并导致不稳定。另一个问题是模式崩溃，这是由于不当的收敛导致的，意思是生成器只输出少数几种生成样本，这些样本它知道能欺骗判别器让其认为是真的。由于生成器开始一次又一次地输出相同的几个样本，判别器便学会将这些样本分类为假的。模式崩溃是一个相当具有挑战性的问题。最后，我们的判别器可能变得太强，以至于生成器的梯度消失，最终什么也学不到。
- en: If we were to compare VAEs and GANs, both of which are generative models, we
    would see that with GANs, our goal is to minimize the divergence between the two
    distributions, while with VAEs, our objective is to minimize a bound on the divergence
    of the two distributions. This is a much easier task, but it doesn't produce results
    in quite the same way as the GAN.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较变分自编码器（VAE）和生成对抗网络（GAN），这两者都是生成模型，我们会发现，在GAN中，我们的目标是最小化两个分布之间的散度，而在VAE中，我们的目标是最小化两个分布散度的界限。这是一个更容易的任务，但其生成的结果并不像GAN那样。
- en: Wasserstein GANs
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Wasserstein GANs
- en: In the preceding section, we learned about GANs, how they work, and how they
    face some problems during training. Now, we will learn about the **Wasserstein
    GAN** (**WGAN**), which makes use of the Wasserstein distance. It is a function
    that measures the distance between two probability distributions on a given metric
    space. Imagine we're on a beach and we decide to model a three-dimensional probability
    distribution in the sand. The Wasserstein distance measures the least amount of
    energy that would be required to move and reshape the distribution into another
    one. So, we can say that the cost is the product of the total mass of sand we
    moved and the distance it was moved.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们学习了 GAN，了解了它们的工作原理以及它们在训练中面临的一些问题。现在，我们将学习**Wasserstein GAN**（**WGAN**），它利用
    Wasserstein 距离。Wasserstein 距离是一个函数，用于测量在给定度量空间中两个概率分布之间的距离。假设我们在沙滩上，决定在沙子上建模一个三维概率分布。Wasserstein
    距离测量的是将分布移动并重塑成另一个分布所需的最小能量。因此，我们可以说，这个代价是我们移动的沙子总质量与它移动的距离的乘积。
- en: 'What this does for GANs is it smoothens the gradient and prevents the discriminator
    from being overtrained. The losses of our discriminator and generator are now,
    respectively, as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GAN 来说，这可以平滑梯度，并防止判别器过度训练。我们判别器和生成器的损失分别如下：
- en: '[![](img/2dee8404-9204-4be6-a4c5-7f52fc37385d.png)]'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/2dee8404-9204-4be6-a4c5-7f52fc37385d.png)]'
- en: '[![](img/b1f96433-8bc8-4995-8e99-42baf1b83a52.png)]'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/b1f96433-8bc8-4995-8e99-42baf1b83a52.png)]'
- en: Why does this perform better than JS and KL divergence? Let's find out using
    the following example.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么它比 JS 和 KL 散度表现得更好？让我们通过以下例子来找出答案。
- en: 'We have two distributions, *P* and *Q*, with the following parameters:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个分布，*P* 和 *Q*，它们的参数如下：
- en: '![](img/c1a04e0b-b4ae-4395-9ef6-00320abdfd3d.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1a04e0b-b4ae-4395-9ef6-00320abdfd3d.png)'
- en: 'Now, let''s compare KL divergence with JS divergence with the Wasserstein distance.
    If *θ *≠ 0, then we can observe the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将 KL 散度与 JS 散度以及 Wasserstein 距离进行比较。如果 *θ* ≠ 0，那么我们可以观察到以下内容：
- en: '![](img/52a69257-5188-4755-a639-ddba70da71b5.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52a69257-5188-4755-a639-ddba70da71b5.png)'
- en: 'When [![](img/634e1de6-527d-411e-8c14-56503413042a.png)], we can observe the
    following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当 [![](img/634e1de6-527d-411e-8c14-56503413042a.png)] 时，我们可以观察到以下内容：
- en: '![](img/b6ab4a3f-4cc6-4ad8-b9fb-0da9bb363ccb.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6ab4a3f-4cc6-4ad8-b9fb-0da9bb363ccb.png)'
- en: 'As you can see, the Wasserstein distance has some clear advantages over KL
    and JS divergence in that it is differentiable with respect to *θ*, which improves
    the stability of the learning. So, the loss function now becomes the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Wasserstein 距离相对于 KL 和 JS 散度有一些明显的优势，因为它对 *θ* 可微，这提高了学习的稳定性。因此，损失函数现在变为以下形式：
- en: '![](img/7302f3fe-b94e-4baf-a1e4-d4c3ad5b8889.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7302f3fe-b94e-4baf-a1e4-d4c3ad5b8889.png)'
- en: This is K-Lipschitz continuous—that is, [![](img/15eb568b-9e53-47ec-ba92-b3f7950ee3fd.png)] for [![](img/1ee9e4c5-561d-48b6-9f80-bec07dfe502b.png)] and [![](img/77a62cc9-5f04-4c0d-83e5-3630202c36e7.png)].
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 K-Lipschitz 连续的——即，[![](img/15eb568b-9e53-47ec-ba92-b3f7950ee3fd.png)] 对
    [![](img/1ee9e4c5-561d-48b6-9f80-bec07dfe502b.png)] 和 [![](img/77a62cc9-5f04-4c0d-83e5-3630202c36e7.png)]。
- en: Sadly, despite the benefits of WGAN over GAN, it is still difficult to train.
    There are a number of variants of GAN that attempt to address this problem.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 可惜的是，尽管 WGAN 相比于 GAN 有很多优势，它仍然难以训练。有许多 GAN 的变种试图解决这个问题。
- en: Flow-based networks
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于流的网络
- en: So far in this chapter, we have studied two kinds of generative models—GANs
    and VAEs—but there is also another kind, known as **flow-based generative models**,
    which directly learn the probability density function of the data distribution,
    which is something that the previous models do not do. Flow-based models make
    use of normalizing flows, which overcomes the difficulty that GANs and VAEs face
    in trying to learn the distribution. This approach can transform a simple distribution
    into a more complex one through a series of invertible mappings. We repeatedly
    apply the change of variables rule, which allows the initial probability density
    to flow through the series of invertible mappings, and at the end, we get the
    target probability distribution.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们学习了两种生成模型——GAN 和 VAE——但还有另一种模型，称为**基于流的生成模型**，它直接学习数据分布的概率密度函数，而这是前面提到的模型所没有做到的。基于流的模型利用了归一化流，克服了
    GAN 和 VAE 在学习分布时所面临的困难。这种方法可以通过一系列可逆映射将简单的分布转化为更复杂的分布。我们反复应用变量变化规则，这使得初始概率密度通过这一系列可逆映射流动，最终得到目标概率分布。
- en: Normalizing flows
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化流
- en: Before we can proceed with understanding how flow-based models work, let's recap
    some concepts such as the Jacobian matrix, calculating the determinant of a matrix
    and the change of the variable theorem in probability, and then go on to understand
    what a normalizing flow is.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入理解基于流的模型之前，先回顾一些概念，如雅可比矩阵、计算矩阵的行列式以及概率论中的变量变换定理，然后再继续理解什么是归一化流。
- en: As a refresher, the Jacobian matrix is an *m*×*n*-dimensional matrix that contains
    the first derivatives of a function, which maps an *n*-dimensional vector to an
    *m*-dimensional vector. Each element of this matrix is represented by [![](img/29e402b5-bbab-476c-b8ac-783cbd303693.png)].
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 作为复习，雅可比矩阵是一个*m*×*n*维的矩阵，包含一个函数的第一导数，该函数将一个*n*维向量映射到*m*维向量。该矩阵的每个元素表示为[![](img/29e402b5-bbab-476c-b8ac-783cbd303693.png)]。
- en: 'The determinant can only be found for a square matrix. So, let''s suppose we
    have an *n*×*n* matrix, *M*. Its determinant can be found using the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 行列式只能对方阵进行求解。所以，假设我们有一个*n*×*n*的矩阵*M*，它的行列式可以通过以下方式求得：
- en: '![](img/e0fa08dd-9519-47da-8127-03198f2c1cab.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0fa08dd-9519-47da-8127-03198f2c1cab.png)'
- en: Here, the sum is calculated over all *n*! permutations, [![](img/6dc58252-2789-433d-b0bc-a62a70b168ec.png)] of [![](img/c4976b6a-35ea-4253-bf9c-b4e3d4e06840.png)],
    and σ(•) tells us the signature of the permutation. However, if |*M*|= 0, then
    *M* is not invertible.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，求和是对所有*n*!排列进行计算，[![](img/6dc58252-2789-433d-b0bc-a62a70b168ec.png)] 对[![](img/c4976b6a-35ea-4253-bf9c-b4e3d4e06840.png)]，而σ(•)告诉我们排列的符号。然而，如果|*M*|=
    0，则*M*不可逆。
- en: 'Now, let''s say we have a random variable, ![](img/8bf87813-7568-4cab-a2e2-5000eb6b5361.png),
    whose probability density function is *z *∼ π(*z*). Using this, we can make a
    new random variable as the result of a one-to-one mapping, *x *= *f*(*z*). Since
    this function is invertible, we know that *z *= *f^(-1)*(*x*). But what, then,
    is the probability density function of our new random variable? From our knowledge
    of probability distributions, we know that the following is true:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有一个随机变量，![](img/8bf87813-7568-4cab-a2e2-5000eb6b5361.png)，其概率密度函数为*z*∼π(*z*)。基于此，我们可以通过一一映射得到一个新的随机变量，*x*
    = *f*(*z*)。由于该函数是可逆的，我们知道*z* = *f^(-1)*(*x*)。那么，我们新随机变量的概率密度函数是什么呢？根据我们对概率分布的知识，我们知道以下是成立的：
- en: '![](img/c0ad8a7a-96be-498c-8b98-7791c01540f9.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ad8a7a-96be-498c-8b98-7791c01540f9.png)'
- en: From [Chapter 1](3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml), *Vector Calculus*,
    we should remember that an integral is the area under a curve and in probability,
    this is always equal to 1\. This area under the curve can be sliced into infinitesimal
    rectangles of Δ*z* width and the height of this rectangle at *z* is π(*z*).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第一章](3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml)，《向量微积分》中，我们应该记住，积分是曲线下的面积，在概率论中，这个面积总是等于1。曲线下的这一面积可以被切分为宽度为Δ*z*的无穷小矩形，这个矩形在*z*处的高度是π(*z*)。
- en: 'Knowing *z*=*f^(-1)*(*x*) tells us that the ratio of a small change in *z* with
    respect to a small change in *x* gives us the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 知道*z*=*f^(-1)*(*x*)告诉我们，*z*相对于*x*的微小变化的比率给出如下结果：
- en: '![](img/b9c16ee8-b64a-4205-ac5a-e5e0ee1bcdfe.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9c16ee8-b64a-4205-ac5a-e5e0ee1bcdfe.png)'
- en: 'We can rewrite this as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其重写如下：
- en: '![](img/47dc8756-310c-438d-8db6-9d7fa96abb2a.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47dc8756-310c-438d-8db6-9d7fa96abb2a.png)'
- en: 'Now, we can rewrite our preceding distribution as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将前述分布重写如下：
- en: '![](img/fb731b44-94a7-4564-90ca-4ce53c7874bf.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb731b44-94a7-4564-90ca-4ce53c7874bf.png)'
- en: 'Since we''ll we working with vectors, we can express the preceding equation
    in terms of multiple variables, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将处理向量，我们可以将前述方程用多个变量来表示，如下所示：
- en: '![](img/94dcb68d-ab02-47e0-a80d-cd5ba4d6a80c.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94dcb68d-ab02-47e0-a80d-cd5ba4d6a80c.png)'
- en: Great! Now that we have those concepts fresh in our memory, let's move on to
    understanding what exactly a normalizing flow is.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在这些概念已经清晰地在我们脑海中，我们继续理解什么是归一化流。
- en: Getting a good probability density estimation is quite important in deep learning,
    but it is often very challenging to do. So instead, we use a normalizing flow
    to approximate the distribution more efficiently by transforming a simple distribution
    into a more complex one by applying a series of invertible functions on it. The
    name comes from the fact that the change of variable normalizes the probability
    density after applying a mapping and the flow means that these simpler transformations
    can be applied continuously to create a much more complex transformation. It is
    also required for these transformation functions to be easily invertible and the
    determinant needs to be simple to compute.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 获取一个好的概率密度估计在深度学习中非常重要，但通常非常具有挑战性。因此，我们使用归一化流来通过一系列可逆函数将简单分布转化为更复杂的分布，从而更高效地逼近目标分布。归一化流这一名称来源于变量变化后使概率密度规范化，并且流意味着这些简单的变换可以连续应用，创造出更加复杂的变换。对于这些变换函数来说，它们需要容易逆转，并且行列式需要容易计算。
- en: 'Let''s take an initial distribution, apply *K* transformations (or mappings)
    to it, and see how we obtain *x* from it. It works as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个初始分布，应用*K*次变换（或映射），看我们如何从中获得*x*。其过程如下：
- en: '![](img/0bb924a7-3778-4797-8e5a-8d722754aa7e.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bb924a7-3778-4797-8e5a-8d722754aa7e.png)'
- en: 'We can also use the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用以下方法：
- en: '![](img/f79ea1e1-bc5e-417a-a862-484c16af00a7.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f79ea1e1-bc5e-417a-a862-484c16af00a7.png)'
- en: 'Here, we have the following parameters:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有以下参数：
- en: '[![](img/aa1b5a6d-50d9-4f9a-ae3f-bcccc5bceead.png)]'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/aa1b5a6d-50d9-4f9a-ae3f-bcccc5bceead.png)]'
- en: '[![](img/34268cef-0b70-47fa-8db1-633f030d1c7c.png)]'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/34268cef-0b70-47fa-8db1-633f030d1c7c.png)]'
- en: '[![](img/2d50e53a-6e3e-474a-af31-46507e7170d8.png)]'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/2d50e53a-6e3e-474a-af31-46507e7170d8.png)]'
- en: '[![](img/9d406955-4ba0-42b3-9215-683476e995d6.png) ](from the change of variables
    theorem)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/9d406955-4ba0-42b3-9215-683476e995d6.png)](来自变量变换定理)'
- en: The determinant is a Jacobian matrix.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 行列式是一个雅可比矩阵。
- en: 'Let''s expand on the fourth equation that we used to find *p[i]*(*z[i]*) to
    get a clearer picture of it:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展开我们用来找到*p[i]*(*z[i]*)的第四个方程，以便更清晰地理解它：
- en: '![](img/06cb55a0-63b9-43f8-af85-3cecba2acd67.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06cb55a0-63b9-43f8-af85-3cecba2acd67.png)'
- en: 'If we take the logarithm of both sides, we get the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对两边取对数，就可以得到如下结果：
- en: '![](img/aec07547-86dd-4479-a7c9-57712b13e70c.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aec07547-86dd-4479-a7c9-57712b13e70c.png)'
- en: 'This tells us the relationship that exists between the sequence of variables
    and from this, we can obtain the relationship between *x* and the initial distribution, *z[0]*,
    through expansion, which looks as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们变量序列之间的关系，并且通过扩展，我们可以获得*x*与初始分布*z[0]*之间的关系，其形式如下：
- en: '![](img/e40121bf-896f-4d9e-afd2-4fc6074ac1e5.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e40121bf-896f-4d9e-afd2-4fc6074ac1e5.png)'
- en: It is this process that is referred to as **normalizing the flow**.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为**归一化流**。
- en: Real-valued non-volume preserving
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实值非体积保持
- en: So far in this chapter, we have covered two very popular generative neural network
    architectures—VAEs and GANs—both of which are quite powerful and have brought
    about tremendous results in generating new data. However, both of these architectures
    also have their challenges. Flow-based generative models, on the other hand, while
    not as popular, do have their merits.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本章我们已经介绍了两种非常流行的生成神经网络架构——变分自编码器（VAEs）和生成对抗网络（GANs）——这两者都非常强大，并在生成新数据方面取得了巨大成果。然而，这两种架构也各有其挑战。另一方面，基于流的生成模型虽然不如前两者流行，但也有其优点。
- en: 'Some of the advantages of flow-based generative models are as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 基于流的生成模型的一些优势如下：
- en: They have exact latent-variable inference and log-likelihood evaluation, whereas
    in VAEs, we can only approximately infer from latent variables, and GANs cannot
    infer the latent as they do not have an encoder.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们具有精确的潜变量推理和对数似然评估，而在变分自编码器（VAEs）中，我们只能大致推断潜变量，而生成对抗网络（GANs）由于没有编码器，无法进行潜变量的推断。
- en: They are efficient to parallelize for both synthesis and inference.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在合成和推理过程中都很高效，便于并行化。
- en: They have a useful latent space for downstream tasks and so are able to interpolate
    between data points and modify existing data points.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们为下游任务提供了有用的潜在空间，因此能够在数据点之间进行插值，并修改现有数据点。
- en: They are much more memory-efficient in comparison to GANs and VAEs.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与生成对抗网络（GANs）和变分自编码器（VAEs）相比，它们更加节省内存。
- en: In this section, we will take an in-depth look at a generative probabilistic
    model known as **real-valued non-volume preserving** (**real NVP**) transformations,
    which can tractably model high-dimensional data. This model works by stacking
    together a sequence of invertible bijective transformations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨一种被称为**实值非体积保持**（**real NVP**）的生成概率模型，它能够有效地建模高维数据。该模型通过将一系列可逆的双射变换堆叠在一起实现。
- en: 'Let''s suppose we have a *D-*dimensional input, *x*, it is split into two parts
    by *d < D*, and the output, *y*, is computed using the following two equations:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个*D-*维输入，*x*，它被分成两部分，*d < D*，输出*y*由以下两个方程计算得到：
- en: '[![](img/0d2b5289-c930-4d5b-b128-7704840f8530.png)]'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/0d2b5289-c930-4d5b-b128-7704840f8530.png)]'
- en: '[![](img/8893c52b-151e-4a91-b563-25a72539d774.png)]'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/8893c52b-151e-4a91-b563-25a72539d774.png)]'
- en: Here, ![](img/a5180735-3ffe-4ef0-97ea-f218b54ae4aa.png) is an element-wise product; *s(*•*)* and
    *t(*•*)* are scale and translation functions that map [![](img/262094d6-d674-45ee-a88d-d32ef4c8d2d3.png)].
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/a5180735-3ffe-4ef0-97ea-f218b54ae4aa.png)是逐元素乘积；*s(*•*)*和*t(*•*)*是尺度和平移函数，映射[![](img/262094d6-d674-45ee-a88d-d32ef4c8d2d3.png)]。
- en: Using our knowledge of normalizing flows, we know that this method must satisfy
    two properties—it must be easily invertible and its Jacobian matrix must be simple
    to compute. Let's now check whether this method fits both of these criteria.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们对归一化流的了解，我们知道这种方法必须满足两个特性——它必须是容易可逆的，并且其雅可比矩阵必须易于计算。现在，让我们检查一下这种方法是否符合这两个标准。
- en: 'In the following equation, we can see that it is, in fact, quite simple to
    find the inverse:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下方程中，我们可以看到，实际上找到逆是相当简单的：
- en: '![](img/7ff50bf8-de63-4c93-a715-4f04b61cffab.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ff50bf8-de63-4c93-a715-4f04b61cffab.png)'
- en: Computing the inverse of the coupling layer doesn't require us to compute the
    inverse of *s(*•*)* and *t(*•*)*, which is great because in this case, both of
    those functions are CNNs and would be very difficult to invert.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 计算耦合层的逆并不需要我们计算*s(*•*)*和*t(*•*)*的逆，这一点非常好，因为在这种情况下，这两个函数都是卷积神经网络（CNN），而且很难反转。
- en: 'Now, we can determine how easy the Jacobian matrix is to compute:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以确定雅可比矩阵的计算难度：
- en: '![](img/354c6a6e-3fa3-4921-8f52-9ac9e565aec5.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/354c6a6e-3fa3-4921-8f52-9ac9e565aec5.png)'
- en: 'This is a lower-triangular matrix. Should we want to find the determinant of
    the Jacobian matrix, we can do so using the following formula:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个下三角矩阵。如果我们想要找到雅可比矩阵的行列式，可以使用以下公式：
- en: '![](img/a1861661-c791-4e2b-8785-0c0c1d431c48.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1861661-c791-4e2b-8785-0c0c1d431c48.png)'
- en: These two equations for the mapping tell us that when we combine the coupling
    layers during a forward compute, some of the parts remain unaffected. To overcome
    this, the authors of this method coupled the layers using an alternating pattern
    so that all of the parts are updated eventually.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这两条映射方程告诉我们，当我们在正向计算中组合耦合层时，某些部分保持不变。为了解决这个问题，这种方法的作者采用了交替模式来耦合层，以确保所有部分最终都会被更新。
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this section, we covered a variety of generative models that learn the distribution
    of true data and try to generate data that is indistinguishable from it. We started
    with a simple autoencoder and built on it to understand a variant of it that uses
    variational inference to generate data similar to the input. We then went on to
    learn about GANs, which pit two models—a discriminator and a generator—against
    each other in a game so that the generator tries to learn to create data that
    looks real enough to fool the discriminator into thinking it is real.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了多种生成模型，这些模型学习真实数据的分布，并尝试生成与真实数据无法区分的数据。我们从一个简单的自编码器开始，并基于它理解了一个变体，该变体使用变分推理生成类似输入的数据。接着我们学习了生成对抗网络（GAN），它将两个模型——判别器和生成器——对抗起来，在博弈中让生成器学习创建足够真实的数据，以便骗过判别器让其认为数据是真的。
- en: Finally, we learned about flow-based networks, which approximate a complex probability
    density using a simpler one by applying several invertible transformations on
    it. These models are used in a variety of tasks, including—but not limited to—synthetic
    data generation to overcome data limitations and extracting insights from data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们了解了基于流的网络，这些网络通过对数据应用几个可逆变换，近似一个复杂的概率密度，使用较简单的密度。这些模型用于各种任务，包括——但不限于——合成数据生成，用以克服数据限制和从数据中提取洞见。
- en: In the next chapter, we will learn about transfer and meta-learning, which cover
    various methods involving transferring the knowledge a network has already learned
    for one task to bootstrap learning for another task. We will make a distinction
    between these two methods.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习转移学习和元学习，它们涵盖了将网络已经为一个任务学习到的知识转移到另一个任务上，以促进学习的各种方法。我们将区分这两种方法。
