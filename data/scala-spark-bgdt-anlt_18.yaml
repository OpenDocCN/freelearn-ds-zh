- en: Testing and Debugging Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和调试Spark
- en: '"Everyone knows that debugging is twice as hard as writing a program in the
    first place. So if you''re as clever as you can be when you write it, how will
    you ever debug it?"'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “每个人都知道调试比一开始编写程序要难两倍。所以，如果你在编写程序时尽可能聪明，那么你将如何调试它？”
- en: '- Brian W. Kernighan'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- Brian W. Kernighan'
- en: In an ideal world, we write perfect Spark codes and everything runs perfectly
    all the time, right? Just kidding; in practice, we know that working with large-scale
    datasets is hardly ever that easy, and there are inevitably some data points that
    will expose any corner cases with your code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的世界中，我们编写完美的Spark代码，一切都完美运行，对吧？开个玩笑；实际上，我们知道处理大规模数据集几乎从来都不那么容易，必然会有一些数据点会暴露出代码的任何边缘情况。
- en: 'Considering the aforementioned challenges, therefore, in this chapter, we will
    see how difficult it can be to test an application if it is distributed; then,
    we will see some ways to tackle this. In a nutshell, the following topics will
    be cover throughout this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到上述挑战，在本章中，我们将看到如果应用程序是分布式的，测试可能有多么困难；然后，我们将看到一些解决方法。简而言之，本章将涵盖以下主题：
- en: Testing in a distributed environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式环境中进行测试
- en: Testing Spark application
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试Spark应用程序
- en: Debugging Spark application
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调试Spark应用程序
- en: Testing in a distributed environment
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在分布式环境中进行测试
- en: 'Leslie Lamport defined the term distributed system as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 莱斯利·兰波特（Leslie Lamport）对分布式系统的定义如下：
- en: '"A distributed system is one in which I cannot get any work done because some
    machine I have never heard of has crashed."'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: “分布式系统是指我无法完成任何工作，因为我从未听说过的某台机器已经崩溃了。”
- en: Resource sharing through **World Wide Web** (aka **WWW**), a network of connected
    computers (aka a cluster), is a good example of distributed systems. These distributed
    environments are often complex and lots of heterogeneity occurs frequently. Testing
    in these kinds of the heterogeneous environments is also challenging. In this
    section, at first, we will observe some commons issues that are often raised while
    working with such system.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**万维网**（又称**WWW**）进行资源共享，连接的计算机网络（又称集群），是分布式系统的一个很好的例子。这些分布式环境通常非常复杂，经常发生许多异构性。在这些异构环境中进行测试也是具有挑战性的。在本节中，首先我们将观察一些在使用这种系统时经常出现的常见问题。
- en: Distributed environment
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式环境
- en: 'There are numerous definitions of distributed systems. Let''s see some definition
    and then we will try to correlate the aforementioned categories afterward. Coulouris
    defines a distributed system as *a system in which hardware or software components
    located at networked computers communicate and coordinate their actions only by
    message passing*. On the other hand, Tanenbaum defines the term in several ways:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多关于分布式系统的定义。让我们看一些定义，然后我们将尝试在之后将上述类别相关联。Coulouris将分布式系统定义为*一个系统，其中位于网络计算机上的硬件或软件组件仅通过消息传递进行通信和协调*。另一方面，Tanenbaum以多种方式定义这个术语：
- en: '*A collection of independent computers that appear to the users of the system
    as a single computer.*'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一组独立的计算机，对系统的用户来说，它们看起来像是一个单一的计算机。*'
- en: '*A system that consists of a collection of two or more independent Computers
    which coordinate their processing through the exchange of synchronous or asynchronous
    message passing.*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*由两个或两个以上独立计算机组成的系统，它们通过同步或异步消息传递来协调它们的处理。*'
- en: '*A distributed system is a collection of autonomous computers linked by a network
    with software designed to produce an integrated computing facility.*'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分布式系统是由网络连接的自主计算机组成的集合，其软件旨在产生一个集成的计算设施。*'
- en: 'Now, based on the preceding definition, distributed systems can be categorized
    as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据前面的定义，分布式系统可以分为以下几类：
- en: Only hardware and software are distributed:The local distributed system is connected
    through LAN.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有硬件和软件是分布式的：本地分布式系统通过局域网连接。
- en: Users are distributed, but there are computing and hardware resources that are
    running backend, for example, WWW.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是分布式的，但是运行后端的计算和硬件资源，例如WWW。
- en: 'Both users and hardware/software are distributed: Distributed computing cluster
    that is connected through WAN. For example, you can get these types of computing
    facilities while using Amazon AWS, Microsoft Azure, Google Cloud, or Digital Ocean''s
    droplets.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户和硬件/软件都是分布式的：通过WAN连接的分布式计算集群。例如，您可以在使用Amazon AWS、Microsoft Azure、Google Cloud或Digital
    Ocean的droplets时获得这些类型的计算设施。
- en: Issues in a distributed system
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式系统中的问题
- en: Here we will discuss some major issues that need to be taken care of during
    the software and hardware testing so that Spark jobs run smoothly in cluster computing,
    which is essentially a distributed computing environment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论一些在软件和硬件测试过程中需要注意的主要问题，以便Spark作业在集群计算中顺利运行，这本质上是一个分布式计算环境。
- en: 'Note that all the issues are unavoidable, but we can at least tune them for
    betterment. You should follow the instructions and recommendations given in the
    previous chapter. According to *Kamal Sheel Mishra* and *Anil Kumar Tripathi*,
    *Some Issues, Challenges and Problems of Distributed Software System*, in *International
    Journal of Computer Science and Information Technologies*, Vol. 5 (4), 2014, 4922-4925\.
    URL: [https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf](https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf),
    there are several issues that need to be addressed while working with software
    or hardware in a distributed environment:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有这些问题都是不可避免的，但我们至少可以调整它们以获得更好的效果。您应该遵循上一章中给出的指示和建议。根据*Kamal Sheel Mishra*和*Anil
    Kumar Tripathi*在*国际计算机科学和信息技术杂志*第5卷（4），2014年，4922-4925页中的*分布式软件系统的一些问题、挑战和问题*，URL：[https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf](https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf)，在分布式环境中工作时需要解决几个问题：
- en: Scalability
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Heterogeneous languages, platform, and architecture
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异构语言、平台和架构
- en: Resource management
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源管理
- en: Security and privacy
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全和隐私
- en: Transparency
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 透明度
- en: Openness
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放性
- en: Interoperability
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互操作性
- en: Quality of service
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务质量
- en: Failure management
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 失败管理
- en: Synchronization
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步
- en: Communications
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通信
- en: Software architectures
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件架构
- en: Performance analysis
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能分析
- en: Generating test data
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成测试数据
- en: Component selection for testing
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试组件选择
- en: Test sequence
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试顺序
- en: Testing for system scalability and performance
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试系统的可伸缩性和性能
- en: Availability of source code
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源代码的可用性
- en: Reproducibility of events
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件的可重现性
- en: Deadlocks and race conditions
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 死锁和竞争条件
- en: Testing for fault tolerance
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试容错性
- en: Scheduling issue for distributed system
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式系统的调度问题
- en: Distributed task allocation
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式任务分配
- en: Testing distributed software
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试分布式软件
- en: Monitoring and control mechanism from the hardware abstraction level
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从硬件抽象级别的监控和控制机制
- en: 'It''s true that we cannot fully solve all of these issues, but However, using
    Spark, we can at least control a few of them that are related to distributed system.
    For example, scalability, resource management, quality of service, failure management,
    synchronization, communications, scheduling issue for distributed system, distributed
    task allocation, and monitoring and control mechanism in testing distributed software.
    Most of them were discussed in the previous two chapters. On the other hand, we
    can address some issues in the testing and software side: such as software architectures,
    performance analysis, generating test data, component selection for testing, test
    sequence, testing for system scalability and performance, and availability of
    source code. These will be covered explicitly or implicitly in this chapter at
    least.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，我们无法完全解决所有这些问题，但是，使用Spark，我们至少可以控制一些与分布式系统相关的问题。例如，可伸缩性、资源管理、服务质量、故障管理、同步、通信、分布式系统的调度问题、分布式任务分配以及测试分布式软件中的监控和控制机制。其中大部分在前两章中已经讨论过。另一方面，我们可以解决一些与测试和软件相关的问题：如软件架构、性能分析、生成测试数据、测试组件选择、测试顺序、测试系统的可伸缩性和性能，以及源代码的可用性。这些问题至少在本章中将被明确或隐含地涵盖。
- en: Challenges of software testing in a distributed environment
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在分布式环境中软件测试的挑战
- en: There are some common challenges associated with the tasks in an agile software
    development, and those challenges become more complex while testing the software
    in a distributed environment before deploying them eventually. Often team members
    need to merge the software components in parallel after the bugs proliferating.
    However, based on urgency, often the merging occurs before testing phase. Sometimes,
    many stakeholders are distributed across teams. Therefore, there's a huge potential
    for misunderstanding and teams often lose in between.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在敏捷软件开发中有一些常见的挑战，而在最终部署之前在分布式环境中测试软件时，这些挑战变得更加复杂。通常团队成员需要在错误不断增加后并行合并软件组件。然而，基于紧急性，合并通常发生在测试阶段之前。有时，许多利益相关者分布在不同的团队中。因此，存在误解的巨大潜力，团队经常在其中失去。
- en: For example, Cloud Foundry ([https://www.cloudfoundry.org/](https://www.cloudfoundry.org/))
    is an open source heavily distributed PaaS software system for managing deployment
    and scalability of applications in the Cloud. It promises different features such
    as scalability, reliability, and elasticity that come inherently to deployments
    on Cloud Foundry require the underlying distributed system to implement measures
    to ensure robustness, resiliency, and failover.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Cloud Foundry（[https://www.cloudfoundry.org/](https://www.cloudfoundry.org/)）是一个开源的、高度分布式的PaaS软件系统，用于管理云中应用程序的部署和可伸缩性。它承诺不同的功能，如可伸缩性、可靠性和弹性，这些功能在Cloud
    Foundry上的部署中是内在的，需要底层分布式系统实施措施来确保健壮性、弹性和故障转移。
- en: 'The process of software testing is long known to comprise *unit testing*, *integration
    testing*, *smoke testing*, *acceptance testing*, *scalability testing*, *performance
    testing*, and *quality of service testing*. In Cloud Foundry, the process of testing
    a distributed system is shown in the following figure:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，软件测试的过程包括*单元测试*、*集成测试*、*烟雾测试*、*验收测试*、*可伸缩性测试*、*性能测试*和*服务质量测试*。在Cloud Foundry中，测试分布式系统的过程如下图所示：
- en: '![](img/00106.jpeg)**Figure 1:** An example of software testing in a distributed
    environment like Cloud'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00106.jpeg)**图1：**像Cloud这样的分布式环境中软件测试的一个例子'
- en: As shown in the preceding figure (first column), the process of testing in a
    distributed environment like Cloud starts with running unit tests against the
    smallest points of contract in the system. Following successful execution of all
    the unit tests, integration tests are run to validate the behavior of interacting
    components as part of a single coherent software system (second column) running
    on a single box (for example, a **Virtual Machine** (**VM**) or bare metal). However,
    while these tests validate the overall behavior of the system as a monolith, they
    do not guarantee system validity in a distributed deployment. Once integration
    tests pass, the next step (third column) is to validate distributed deployment
    of the system and run the smoke tests.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图（第一列）所示，在像Cloud这样的分布式环境中进行测试的过程始于针对系统中最小的接口点运行单元测试。在所有单元测试成功执行后，运行集成测试来验证作为单一连贯软件系统的相互作用组件的行为（第二列），这些组件运行在单个盒子上（例如，一个虚拟机（VM）或裸机）。然而，虽然这些测试验证了系统作为单体的整体行为，但并不保证在分布式部署中系统的有效性。一旦集成测试通过，下一步（第三列）是验证系统的分布式部署并运行烟雾测试。
- en: As you know, that the successful configuration of the software and execution
    of unit tests prepares us to validate acceptability of system behavior. This verification
    is done by running acceptance tests (fourth column). Now, to overcome the aforementioned
    issues and challenges in distributed environments, there are also other hidden
    challenges that need to be solved by researchers and big data engineers, but those
    are actually out of the scope of this book.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所知，软件的成功配置和单元测试的执行使我们能够验证系统行为的可接受性。通过运行验收测试（第四列）来进行验证。现在，为了克服分布式环境中前面提到的问题和挑战，还有其他隐藏的挑战需要研究人员和大数据工程师来解决，但这些实际上超出了本书的范围。
- en: Now that we know what real challenges are for the software testing in a distributed
    environment, now let's start testing our Spark code a bit. The next section is
    dedicated to testing Spark applications.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了分布式环境中软件测试的真正挑战是什么，现在让我们开始对我们的Spark代码进行一些测试。下一节将专门讨论测试Spark应用程序。
- en: Testing Spark applications
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试Spark应用程序
- en: 'There are many ways to try to test your Spark code, depending on whether it''s
    Java (you can do basic JUnit tests to test non-Spark pieces) or ScalaTest for
    your Scala code. You can also do full integration tests by running Spark locally
    or on a small test cluster. Another awesome choice from Holden Karau is using
    Spark-testing base. You probably know that there is no native library for unit
    testing in Spark as of yet. Nevertheless, we can have the following two alternatives
    to use two libraries:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以尝试测试您的Spark代码，具体取决于它是Java（您可以进行基本的JUnit测试来测试非Spark部分）还是ScalaTest用于您的Scala代码。您还可以通过在本地或小型测试集群上运行Spark来进行完整的集成测试。Holden
    Karau提供的另一个很棒的选择是使用Spark-testing base。您可能知道目前还没有用于Spark的本机单元测试库。尽管如此，我们可以有以下两种替代方法来使用两个库：
- en: ScalaTest
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ScalaTest
- en: Spark-testing base
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark测试基础
- en: However, before starting to test your Spark applications written in Scala, some
    background knowledge about unit testing and testing Scala methods is a mandate.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在开始测试用Scala编写的Spark应用程序之前，对单元测试和测试Scala方法的背景知识是必需的。
- en: Testing Scala methods
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试Scala方法
- en: 'Here, we will see some simple techniques for testing Scala methods. For Scala
    users, this is the most familiar unit testing framework (you can also use it for
    testing Java code and soon for JavaScript). ScalaTest supports a number of different
    testing styles, each designed to support a specific type of testing need. For
    details, see ScalaTest User Guide at [http://www.scalatest.org/user_guide/selecting_a_style](http://www.scalatest.org/user_guide/selecting_a_style).
    Although ScalaTest supports many styles, one of the quickest ways to get started
    is to use the following ScalaTest traits and write the tests in the **TDD** (**test-driven
    development**) style:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看到一些测试Scala方法的简单技术。对于Scala用户来说，这是最熟悉的单元测试框架（您也可以用它来测试Java代码，很快也可以用于JavaScript）。ScalaTest支持多种不同的测试样式，每种样式都设计用于支持特定类型的测试需求。有关详细信息，请参阅ScalaTest用户指南[http://www.scalatest.org/user_guide/selecting_a_style](http://www.scalatest.org/user_guide/selecting_a_style)。尽管ScalaTest支持许多样式，但快速入门的一种方法是使用以下ScalaTest特质，并以**TDD**（测试驱动开发）风格编写测试：
- en: '`FunSuite`'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`FunSuite`'
- en: '`Assertions`'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Assertions`'
- en: '`BeforeAndAfter`'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`BeforeAndAfter`'
- en: Feel free to browse the preceding URLs to learn more about these traits; that
    will make rest of this tutorial go smoothly.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 随时浏览前述URL以了解有关这些特质的更多信息；这将使本教程的其余部分顺利进行。
- en: It is to be noted that the TDD is a programming technique to develop software,
    and it states that you should start development from tests. Hence, it doesn't
    affect how tests are written, but when tests are written. There is no trait or
    testing style to enforce or encourage TDD in `ScalaTest.FunSuite`, `Assertions`,
    and `BeforeAndAfter` are only more similar to the xUnit testing frameworks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是TDD是一种开发软件的编程技术，它规定您应该从测试开始开发。因此，它不影响测试的编写方式，而是测试的编写时间。在`ScalaTest.FunSuite`中没有特质或测试样式来强制或鼓励TDD，`Assertions`和`BeforeAndAfter`只是更类似于xUnit测试框架。
- en: 'There are three assertions available in the ScalaTest in any style trait:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何样式特质中，ScalaTest中有三种断言可用：
- en: '`assert`: This is used for general assertions in your Scala program.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assert`：这用于在您的Scala程序中进行一般断言。'
- en: '`assertResult`: This helps differentiate expected value from the actual values.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assertResult`：这有助于区分预期值和实际值。'
- en: '`assertThrows`: This is used to ensure a bit of code throws an expected exception.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assertThrows`：这用于确保一小段代码抛出预期的异常。'
- en: 'The ScalaTest''s assertions are defined in the trait `Assertions`, which is
    further extended by `Suite`. In brief, the `Suite` trait is the super trait for
    all the style traits. According to the ScalaTest documentation at [http://www.scalatest.org/user_guide/using_assertions](http://www.scalatest.org/user_guide/using_assertions),
    the `Assertions` trait also provides the following features:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ScalaTest的断言是在特质`Assertions`中定义的，该特质进一步由`Suite`扩展。简而言之，`Suite`特质是所有样式特质的超级特质。根据ScalaTest文档[http://www.scalatest.org/user_guide/using_assertions](http://www.scalatest.org/user_guide/using_assertions)，`Assertions`特质还提供以下功能：
- en: '`assume` to conditionally cancel a test'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assume`：有条件地取消测试'
- en: '`fail` to fail a test unconditionally'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fail`：无条件地使测试失败'
- en: '`cancel` to cancel a test unconditionally'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cancel`：无条件地取消测试'
- en: '`succeed` to make a test succeed unconditionally'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`succeed`：无条件使测试成功'
- en: '`intercept` to ensure a bit of code throws an expected exception and then make
    assertions about the exception'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intercept`：确保一小段代码抛出预期的异常，然后对异常进行断言'
- en: '`assertDoesNotCompile` to ensure a bit of code does not compile'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assertDoesNotCompile`：确保一小段代码不会编译'
- en: '`assertCompiles` to ensure a bit of code does compile'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assertCompiles`：确保一小段代码确实编译'
- en: '`assertTypeError` to ensure a bit of code does not compile because of a type
    (not parse) error'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assertTypeError`：确保一小段代码由于类型（而不是解析）错误而无法编译'
- en: '`withClue` to add more information about a failure'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`withClue`：添加有关失败的更多信息'
- en: 'From the preceding list, we will show a few of them. In your Scala program,
    you can write assertions by calling `assert` and passing a `Boolean` expression
    in. You can simply start writing your simple unit test case using `Assertions`.
    The `Predef` is an object, where this behavior of assert is defined. Note that
    all the members of the `Predef` get imported into your every Scala source file.
    The following source code will print `Assertion success` for the following case:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的列表中，我们将展示其中的一些。在您的Scala程序中，您可以通过调用`assert`并传递`Boolean`表达式来编写断言。您可以简单地使用`Assertions`开始编写简单的单元测试用例。`Predef`是一个对象，其中定义了assert的行为。请注意，`Predef`的所有成员都会被导入到您的每个Scala源文件中。以下源代码将为以下情况打印`Assertion
    success`：
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'However, if you make `a = 2` and `b = 1`, for example, the assertion will fail
    and you will experience the following output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您使`a = 2`和`b = 1`，例如，断言将失败，您将看到以下输出：
- en: '![](img/00271.jpeg)**Figure 2:** An example of assertion fail'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：断言失败的一个示例
- en: If you pass a true expression, assert will return normally. However, assert
    will terminate abruptly with an Assertion Error if the supplied expression is
    false. Unlike the `AssertionError` and `TestFailedException` forms, the ScalaTest's
    assert provides more information that will tell you exactly in which line the
    test case failed or for which expression. Therefore, ScalaTest's assert provides
    better error messages than Scala's assert.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果传递一个真表达式，assert将正常返回。但是，如果提供的表达式为假，assert将突然终止并出现断言错误。与`AssertionError`和`TestFailedException`形式不同，ScalaTest的assert提供了更多信息，可以告诉您测试用例失败的确切行或表达式。因此，ScalaTest的assert提供了比Scala的assert更好的错误消息。
- en: 'For example, for the following source code, you should experience `TestFailedException`
    that will tell that 5 did not equal 4:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于以下源代码，您应该会遇到`TestFailedException`，告诉您5不等于4：
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following figure shows the output of the preceding Scala test:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了前面的Scala测试的输出：
- en: '![](img/00180.jpeg)**Figure 3:** An example of TestFailedException'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：TestFailedException的一个示例
- en: 'The following source code explains the use of the `assertResult` unit test
    to test the result of your method:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下源代码解释了使用`assertResult`单元测试来测试方法的结果：
- en: '[PRE2]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding assertion will be failed and Scala will throw an exception `TestFailedException`
    and prints `Expected 3 but got 4` (*Figure 4*):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的断言将失败，Scala将抛出异常`TestFailedException`并打印`Expected 3 but got 4`（*图4*）：
- en: '![](img/00060.jpeg)**Figure 4:** Another example of TestFailedException'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：TestFailedException的另一个示例
- en: 'Now, let''s see a unit testing to show expected exception:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一个单元测试来显示预期的异常：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you try to access an array element outside the index, the preceding code
    will tell you if you''re allowed to access the first character of the preceding
    string `Hello world!`. If your Scala program can access the value in an index,
    the assertion will fail. This also means that the test case has failed. Thus,
    the preceding test case will fail naturally since the first index contains the
    character `H`, and you should experience the following error message (*Figure
    5*):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尝试访问超出索引的数组元素，前面的代码将告诉您是否允许访问前面字符串`Hello world!`的第一个字符。如果您的Scala程序可以访问索引中的值，断言将失败。这也意味着测试用例失败了。因此，前面的测试用例自然会失败，因为第一个索引包含字符`H`，您应该看到以下错误消息（*图5*）：
- en: '![](img/00337.jpeg)**Figure 5:** Third example of TestFailedException'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：TestFailedException的第三个示例
- en: 'However, now let''s try to access the index at position `-1` as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在让我们尝试访问位置为`-1`的索引，如下所示：
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now the assertion should be true, and consequently, the test case will be passed.
    Finally, the code will terminate normally. Now, let''s check our code snippets
    if it will compile or not. Very often, you may wish to ensure that a certain ordering
    of the code that represents emerging "user error" does not compile at all. The
    objective is to check the strength of the library against the error to disallow
    unwanted result and behavior. ScalaTest''s `Assertions` trait includes the following
    syntax for that purpose:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在断言应该为真，因此测试用例将通过。最后，代码将正常终止。现在，让我们检查我们的代码片段是否会编译。很多时候，您可能希望确保代表出现的“用户错误”的代码的某种排序根本不会编译。目标是检查库对错误的强度，以阻止不需要的结果和行为。ScalaTest的`Assertions`
    trait包括以下语法：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you want to ensure that a snippet of code does not compile because of a
    type error (as opposed to a syntax error), use the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想确保一段代码由于类型错误（而不是语法错误）而无法编译，请使用以下方法：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A syntax error will still result on a thrown `TestFailedException`. Finally,
    if you want to state that a snippet of code does compile, you can make that more
    obvious with the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 语法错误仍会导致抛出`TestFailedException`。最后，如果您想要声明一段代码确实编译，可以使用以下方法更明显地表达：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'A complete example is shown as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的示例如下所示：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output of the preceding code is shown in the following figure:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下图所示：
- en: '![](img/00369.jpeg)**Figure 6:** Multiple tests together'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：多个测试一起
- en: Now we would like to finish the Scala-based unit testing due to page limitation.
    However, for other unit test cases, you can refer the Scala test guideline at
    [http://www.scalatest.org/user_guide](http://www.scalatest.org/user_guide).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于页面限制，我们现在想要结束基于Scala的单元测试。但是，对于其他单元测试用例，您可以参考Scala测试指南[http://www.scalatest.org/user_guide](http://www.scalatest.org/user_guide)。
- en: Unit testing
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单元测试
- en: In software engineering, often, individual units of source code are tested to
    determine whether they are fit for use or not. This way of software testing method
    is also called the unit testing. This testing ensures that the source code developed
    by a software engineer or developer meets the design specifications and works
    as intended.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，通常会对源代码的单个单元进行测试，以确定它们是否适合使用。这种软件测试方法也称为单元测试。这种测试确保软件工程师或开发人员开发的源代码符合设计规范并按预期工作。
- en: 'On the other hand, the goal of unit testing is to separate each part of the
    program (that is, in a modular way). Then try to observe if all the individual
    parts are working normally. There are several benefits of unit testing in any
    software system:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，单元测试的目标是以模块化的方式分离程序的每个部分。然后尝试观察所有单独部分是否正常工作。在任何软件系统中，单元测试有几个好处：
- en: '**Find problems early:** It finds bugs or missing parts of the specification
    early in the development cycle.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**早期发现问题：**它可以在开发周期的早期发现错误或规范的缺失部分。'
- en: '**Facilitates change:** It helps in refactoring and up gradation without worrying
    about breaking functionality.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**促进变更：**它有助于重构和升级，而不必担心破坏功能。'
- en: '**Simplifies integration:** It makes integration tests easier to write.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化集成：**它使集成测试更容易编写。'
- en: '**Documentation:** It provides a living documentation of the system.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档：**它提供了系统的实时文档。'
- en: '**Design:** It can act as the formal design of the project.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设计：**它可以作为项目的正式设计。'
- en: Testing Spark applications
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试Spark应用程序
- en: 'We have already seen how to test your Scala code using built-in `ScalaTest`
    package of Scala. However, in this subsection, we will see how we could test our
    Spark application written in Scala. The following three methods will be discussed:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用Scala的内置`ScalaTest`包测试您的Scala代码。但是，在本小节中，我们将看到如何测试我们用Scala编写的Spark应用程序。将讨论以下三种方法：
- en: '**Method 1:** Testing Spark applications using JUnit'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法1：**使用JUnit测试Spark应用程序'
- en: '**Method 2:** Testing Spark applications using `ScalaTest` package'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法2：**使用`ScalaTest`包测试Spark应用程序'
- en: '**Method 3:** Testing Spark applications using Spark testing base'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法3：**使用Spark测试基础测试Spark应用程序'
- en: Methods 1 and 2 will be discussed here with some practical codes. However, a
    detailed discussion on method 3 will be provided in the next subsection. To keep
    the understanding easy and simple, we will use the famous word counting applications
    to demonstrate methods 1 and 2.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里将讨论方法1和方法2，并提供一些实际代码。但是，对方法3的详细讨论将在下一小节中提供。为了使理解简单易懂，我们将使用著名的单词计数应用程序来演示方法1和方法2。
- en: 'Method 1: Using Scala JUnit test'
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法1：使用Scala JUnit测试
- en: 'Suppose you have written an application in Scala that can tell you how many
    words are there in a document or text file as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经在Scala中编写了一个应用程序，可以告诉您文档或文本文件中有多少个单词，如下所示：
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preceding code simply parses a text file and performs a `flatMap` operation
    by simply splitting the words. Then, it performs another operation to take only
    the distinct words into consideration. Finally, the `myWordCounter` method counts
    how many words are there and returns the value of the counter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码简单地解析文本文件，并通过简单地拆分单词执行`flatMap`操作。然后，它执行另一个操作，只考虑不同的单词。最后，`myWordCounter`方法计算有多少个单词，并返回计数器的值。
- en: 'Now, before proceeding into formal testing, let''s check if the preceding method
    works well. Just add the main method and create an object as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在进行正式测试之前，让我们检查上述方法是否有效。只需添加主方法并创建一个对象，如下所示：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you execute the preceding code, you should observe the following output:
    `Number of words: 214`. Fantastic! It really works as a local application. Now,
    test the preceding test case using Scala JUnit test case.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您执行上述代码，您应该观察到以下输出：`单词数量：214`。太棒了！它真的作为一个本地应用程序运行。现在，使用Scala JUnit测试用例测试上述测试用例。
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you see the earlier code carefully, I have used the `Test` annotation before
    the `test()` method. Inside the `test()` method, I invoked the `assert()` method,
    where the actual testing occurs. Here we tried to check if the return value of
    the `myWordCounter()` method is equal to 214\. Now run the earlier code as a Scala
    Unit test as follows (*Figure 7*):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细查看先前的代码，您会发现在`test()`方法之前我使用了`Test`注解。在`test()`方法内部，我调用了`assert()`方法，其中实际的测试发生。在这里，我们尝试检查`myWordCounter()`方法的返回值是否等于214。现在将先前的代码作为Scala单元测试运行，如下所示（*图7*）：
- en: '![](img/00151.jpeg)**Figure 7:** Running Scala code as Scala JUnit Test'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00151.jpeg)**图7：**将Scala代码作为Scala JUnit测试运行'
- en: 'Now if the test case passes, you should observe the following output on your
    Eclipse IDE (*Figure 8*):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果测试用例通过，您应该在Eclipse IDE上观察以下输出（*图8*）：
- en: '![](img/00173.jpeg)**Figure 8:** Word count test case passed'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00173.jpeg)**图8：**单词计数测试用例通过'
- en: 'Now, for example, try to assert in the following way:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，尝试以以下方式断言：
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If the preceding test case fails, you should observe the following output (*Figure
    9*):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果上述测试用例失败，您应该观察到以下输出（*图9*）：
- en: '![](img/00299.jpeg)**Figure 9:** Test case failed'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00299.jpeg)**图9：**测试用例失败'
- en: Now let's have a look at method 2 and how it helps us for the betterment.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下方法2以及它如何帮助我们改进。
- en: 'Method 2: Testing Scala code using FunSuite'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法2：使用FunSuite测试Scala代码
- en: 'Now, let''s redesign the preceding test case by returning only the RDD of the
    texts in the document, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过仅返回文档中文本的RDD来重新设计上述测试用例，如下所示：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'So, the `prepareWordCountRDD()` method in the preceding class returns an RDD
    of string and integer values. Now, if we want to test the `prepareWordCountRDD()`
    method''s functionality, we can do it more explicit by extending the test class
    with `FunSuite` and `BeforeAndAfterAll` from the `ScalaTest` package of Scala.
    The testing works in the following ways:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，上述类中的`prepareWordCountRDD()`方法返回一个字符串和整数值的RDD。现在，如果我们想要测试`prepareWordCountRDD()`方法的功能，我们可以通过将测试类扩展为`ScalaTest`包的`FunSuite`和`BeforeAndAfterAll`来更明确地进行测试。测试以以下方式进行：
- en: Extend the test class with `FunSuite` and `BeforeAndAfterAll` from the `ScalaTest`
    package of Scala
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将测试类扩展为`ScalaTest`包的`FunSuite`和`BeforeAndAfterAll`
- en: Override the `beforeAll()` that creates Spark context
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖`beforeAll()`创建Spark上下文
- en: Perform the test using the `test()` method and use the `assert()` method inside
    the `test()` method
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`test()`方法执行测试，并在`test()`方法内部使用`assert()`方法
- en: Override the `afterAll()` method that stops the Spark context
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖`afterAll()`方法停止Spark上下文
- en: 'Based on the preceding steps, let''s see a class for testing the preceding
    `prepareWordCountRDD()` method:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的步骤，让我们看一个用于测试前面的`prepareWordCountRDD()`方法的类：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The first test says that if two RDDs materialize in two different ways, the
    contents should be the same. Thus, the first test should get passed. We will see
    this in following example. Now, for the second test, as we have seen previously,
    the word count of RDD is 214, but let's assume it unknown for a while. If it's
    214 coincidentally, the test case should pass, which is its expected behavior.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个测试说，如果两个RDD以两种不同的方式实现，内容应该是相同的。因此，第一个测试应该通过。我们将在下面的示例中看到这一点。现在，对于第二个测试，正如我们之前看到的，RDD的单词计数为214，但让我们假设它暂时未知。如果它恰好是214，测试用例应该通过，这是预期的行为。
- en: 'Thus, we are expecting both tests to be passed. Now, on Eclipse, run the test
    suite as `ScalaTest-File`, as shown in the following figure:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们期望两个测试都通过。现在，在Eclipse上，运行测试套件作为`ScalaTest-File`，如下图所示：
- en: '![](img/00342.jpeg) **Figure 10:** running the test suite as ScalaTest-File'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00342.jpeg) **图10：**作为ScalaTest-File运行测试套件'
- en: Now you should observe the following output (*Figure 11*). The output shows
    how many test cases we performed and how many of them passed, failed, canceled,
    ignored, or were (was) in pending. It also shows the time to execute the overall
    test.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该观察以下输出（*图11*）。输出显示我们执行了多少个测试用例，其中有多少通过、失败、取消、忽略或挂起。它还显示了执行整体测试所需的时间。
- en: '![](img/00268.jpeg)**Figure 11:** Test result when running the two test suites
    as ScalaTest-file'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00268.jpeg)**图11：**运行两个测试套件作为ScalaTest文件的测试结果'
- en: 'Fantastic! The test case passed. Now, let''s try changing the compare value
    in the assertion in the two separate tests using the `test()` method as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！测试用例通过了。现在，让我们尝试使用`test()`方法在两个单独的测试中更改断言中的比较值，如下所示：
- en: '[PRE15]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, you should expect that the test case will be failed. Now run the earlier
    class as `ScalaTest-File` (*Figure 12*):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该期望测试用例将失败。现在运行之前的类作为`ScalaTest-File`（*图12*）：
- en: '![](img/00029.jpeg)**Figure 12:** Test result when running the preceding two
    test suites as ScalaTest-File'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00029.jpeg)**图12：**运行前面的两个测试套件作为ScalaTest-File的测试结果'
- en: Well done! We have learned how to perform the unit testing using Scala's FunSuite.
    However, if you evaluate the preceding method carefully, you should agree that
    there are several disadvantages. For example, you need to ensure an explicit management
    of `SparkContext` creation and destruction. As a developer or programmer, you
    have to write more lines of code for testing a sample method. Sometimes, code
    duplication occurs as the *Before* and the *After* step has to be repeated in
    all test suites. However, this is debatable since the common code could be put
    in a common trait.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！我们已经学会了如何使用Scala的FunSuite进行单元测试。然而，如果你仔细评估前面的方法，你会同意存在一些缺点。例如，您需要确保显式管理`SparkContext`的创建和销毁。作为开发人员或程序员，您必须编写更多的代码行来测试一个样本方法。有时，代码重复出现，因为*Before*和*After*步骤必须在所有测试套件中重复。然而，这是值得讨论的，因为通用代码可以放在一个共同的特性中。
- en: Now the question is how could we improve our experience? My recommendation is
    using the Spark testing base to make life easier and more straightforward. We
    will discuss how we could perform the unit testing the Spark testing base.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是我们如何改善我们的体验？我的建议是使用Spark测试基础使生活更轻松和更直接。我们将讨论如何使用Spark测试基础进行单元测试。
- en: 'Method 3: Making life easier with Spark testing base'
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法3：使用Spark测试基础使生活更轻松
- en: Spark testing base helps you to test your most of the Spark codes with ease.
    So, what are the pros of this method then? There are many in fact. For example,
    using this the code is not verbose but we can get very succinct code. The API
    is itself richer than that of ScalaTest or JUnit. Multiple languages support,
    for example, Scala, Java, and Python. It has the support of built-in RDD comparators.
    You can also use it for testing streaming applications. And finally and most importantly,
    it supports both local and cluster mode testings. This is most important for the
    testing in a distributed environment.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Spark测试基础帮助您轻松测试大部分Spark代码。那么，这种方法的优点是什么呢？实际上有很多。例如，使用这种方法，代码不啰嗦，但我们可以得到非常简洁的代码。API本身比ScalaTest或JUnit更丰富。多语言支持，例如Scala、Java和Python。它支持内置的RDD比较器。您还可以用它来测试流应用程序。最后但最重要的是，它支持本地和集群模式的测试。这对于在分布式环境中进行测试非常重要。
- en: The GitHub repo is located at [https://github.com/holdenk/spark-testing-base](https://github.com/holdenk/spark-testing-base).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub仓库位于[https://github.com/holdenk/spark-testing-base](https://github.com/holdenk/spark-testing-base)。
- en: 'Before starting the unit testing with Spark testing base, you should include
    the following dependency in the Maven friendly `pom.xml` file in your project
    tree for Spark 2.x as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Spark测试基础进行单元测试之前，您应该在Maven友好的`pom.xml`文件中包含以下依赖项，以便在Spark 2.x项目树中使用：
- en: '[PRE16]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For SBT, you can add the following dependency:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SBT，您可以添加以下依赖项：
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that it is recommended to add the preceding dependency in the `test` scope
    by specifying `<scope>test</scope>` for both the Maven and SBT cases. In addition
    to these, there are other considerations such as memory requirements and OOMs
    and disabling the parallel execution. The default Java options in the SBT testing
    are too small to support for running multiple tests. Sometimes it's harder to
    test Spark codes if the job is submitted in local mode! Now you can naturally
    understand how difficult it would be in a real cluster mode -i.e. YARN or Mesos.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，建议在Maven和SBT的情况下通过指定`<scope>test</scope>`将前面的依赖项添加到`test`范围中。除此之外，还有其他考虑因素，如内存需求和OOM以及禁用并行执行。SBT测试中的默认Java选项太小，无法支持运行多个测试。有时，如果作业以本地模式提交，测试Spark代码会更加困难！现在您可以自然地理解在真正的集群模式下（即YARN或Mesos）会有多么困难。
- en: 'To get rid of this problem, you can increase the amount of memory in your `build.sbt`
    file in your project tree. Just add the following parameters as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了摆脱这个问题，您可以在项目树中的`build.sbt`文件中增加内存量。只需添加以下参数：
- en: '[PRE18]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'However, if you are using Surefire, you can add the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您使用Surefire，可以添加以下内容：
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In your Maven-based build, you can make it by setting the value in the environmental
    variable. For more on this issue, refer to [https://maven.apache.org/configure.html](https://maven.apache.org/configure.html).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于Maven的构建中，您可以通过设置环境变量的值来实现。有关此问题的更多信息，请参阅[https://maven.apache.org/configure.html](https://maven.apache.org/configure.html)。
- en: 'This is just an example to run spark testing base''s own tests. Therefore,
    you might need to set bigger value. Finally, make sure that you have disabled
    the parallel execution in your SBT by adding the following line of code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个运行spark测试基础自己测试的例子。因此，您可能需要设置更大的值。最后，请确保您已经通过添加以下代码行来禁用SBT中的并行执行：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'On the other hand, if you''re using surefire, make sure that `forkCount` and
    `reuseForks` are set as 1 and true, respectively. Let''s see an example of using
    Spark testing base. The following source code has three test cases. The first
    test case is the dummy that compares if 1 is equal to 1 or not, which obviously
    will be passed. The second test case counts the number of words from the sentence,
    say `Hello world! My name is Reza`, and compares if this has six words or not.
    The final and the last test case tries to compare two RDDs:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您使用surefire，请确保`forkCount`和`reuseForks`分别设置为1和true。让我们看一个使用Spark测试基础的例子。以下源代码有三个测试用例。第一个测试用例是一个比较，看看1是否等于1，显然会通过。第二个测试用例计算句子中单词的数量，比如`Hello
    world! My name is Reza`，并比较是否有六个单词。最后一个测试用例尝试比较两个RDD：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'From the preceding source code, we can see that we can perform multiple test
    cases using Spark testing base. Upon successful execution, you should observe
    the following output (*Figure 13*):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的源代码中，我们可以看到我们可以使用Spark测试基础执行多个测试用例。成功执行后，您应该观察到以下输出（*图13*）：
- en: '![](img/00280.jpeg)![](img/00093.jpeg)**Figure 13:** A successful execution
    and passed test using Spark testing base'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00280.jpeg)![](img/00093.jpeg)**图13：**使用Spark测试基础进行成功执行和通过测试的示例'
- en: Configuring Hadoop runtime on Windows
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Windows上配置Hadoop运行时
- en: We have already seen how to test your Spark applications written in Scala on
    Eclipse or IntelliJ, but there is another potential issue that should not be overlooked.
    Although Spark works on Windows, Spark is designed to be run on the UNIX-like
    operating system. Therefore, if you are working on Windows environment, then extra
    care needs to be taken.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何在Eclipse或IntelliJ上测试用Scala编写的Spark应用程序，但还有一个潜在的问题不容忽视。尽管Spark可以在Windows上运行，但Spark是设计为在类UNIX操作系统上运行的。因此，如果您在Windows环境中工作，则需要额外小心。
- en: 'While using Eclipse or IntelliJ to develop your Spark applications for solving
    data analytics, machine learning, data science, or deep learning applications
    on Windows, you might face an I/O exception error and your application might not
    compile successfully or may be interrupted. Actually, the thing is that Spark
    expects that there is a runtime environment for Hadoop on Windows too. For example,
    if you run a Spark application, say `KMeansDemo.scala`, on Eclipse for the first
    time, you will experience an I/O exception saying the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Eclipse或IntelliJ在Windows上开发用于解决数据分析、机器学习、数据科学或深度学习应用程序的Spark应用程序时，您可能会遇到I/O异常错误，您的应用程序可能无法成功编译或可能被中断。实际上，问题在于Spark期望在Windows上也有一个Hadoop的运行时环境。例如，如果您在Eclipse上首次运行Spark应用程序，比如`KMeansDemo.scala`，您将遇到一个I/O异常，内容如下：
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The reason is that by default, Hadoop is developed for the Linux environment,
    and if you are developing your Spark applications on Windows platform, a bridge
    is required that will provide an environment for the Hadoop runtime for Spark
    to be properly executed. The details of the I/O exception can be seen in the following
    figure:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是默认情况下，Hadoop是为Linux环境开发的，如果您在Windows平台上开发Spark应用程序，则需要一个桥梁，为Spark的Hadoop运行时提供一个正确执行的环境。I/O异常的详细信息可以在下图中看到：
- en: '![](img/00088.gif)**Figure 14:** I/O exception occurred due to the failure
    of not to locate the winutils binary in the Hadoop binary path'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00088.gif)**图14：**由于未能在Hadoop二进制路径中找到winutils二进制而发生的I/O异常'
- en: Now, how to get rid of this problem then? The solution is straightforward. As
    the error message says, we need to have an executable, namely `winutils.exe`.
    Now download the `winutils.exe` file from [https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin),
    paste it in the Spark distribution directory, and configure Eclipse. More specifically,
    suppose your Spark distribution containing Hadoop is located at `C:/Users/spark-2.1.0-bin-hadoop2.7`.
    Inside the Spark distribution, there is a directory named bin. Now, paste the
    executable there (that is, `path = C:/Users/spark-2.1.0-binhadoop2.7/bin/`).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何解决这个问题呢？解决方案很简单。正如错误消息所说，我们需要一个可执行文件，即`winutils.exe`。现在从[https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin)下载`winutils.exe`文件，将其粘贴到Spark分发目录中，并配置Eclipse。更具体地说，假设您的包含Hadoop的Spark分发位于`C:/Users/spark-2.1.0-bin-hadoop2.7`。在Spark分发中，有一个名为bin的目录。现在，将可执行文件粘贴到那里（即`路径=C:/Users/spark-2.1.0-binhadoop2.7/bin/`）。
- en: 'The second phase of the solution is going to Eclipse and then selecting the
    main class (that is, `KMeansDemo.scala` in this case), and then going to the Run
    menu. From the Run menu, go to the Run Configurations option and from there select
    the Environment tab, as shown in the following figure:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案的第二阶段是转到Eclipse，然后选择主类（即本例中的`KMeansDemo.scala`），然后转到运行菜单。从运行菜单中，转到运行配置选项，然后从中选择环境选项卡，如下图所示：
- en: '![](img/00049.jpeg)**Figure 15:** Solving the I/O exception occurred due to
    the absence of winutils binary in the Hadoop binary path'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00049.jpeg)**图15：**由于Hadoop二进制路径中缺少winutils二进制而发生的I/O异常的解决方案'
- en: If you select the tab, you a will have the option to create a new environmental
    variable for Eclipse suing the JVM. Now create a new environmental variable named
    `HADOOP_HOME` and put the value as `C:/Users/spark-2.1.0-bin-hadoop2.7/`. Now
    press on Apply button and rerun your application, and your problem should be resolved.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择该选项卡，您将有选项为Eclipse使用JVM创建新的环境变量。现在创建一个名为`HADOOP_HOME`的新环境变量，并将值设置为`C:/Users/spark-2.1.0-bin-hadoop2.7/`。现在点击“应用”按钮并重新运行您的应用程序，您的问题应该得到解决。
- en: It is to be noted that while working with Spark on Windows in a PySpark, the
    `winutils.exe` file is required too. For PySpark reference, refer to the [Chapter
    19](part0571.html#H0HH61-21aec46d8593429cacea59dbdcd64e1c), *PySpark and SparkR*.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在Windows上使用PySpark时，也需要`winutils.exe`文件。有关PySpark的参考，请参阅[第19章](part0571.html#H0HH61-21aec46d8593429cacea59dbdcd64e1c)，*PySpark和SparkR*。
- en: Please make a note that the preceding solution is also applicable in debugging
    your applications. Sometimes, even if the preceding error occurs, your Spark application
    will run properly. However, if the size of the dataset is large, it is most likely
    that the preceding error will occur.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的解决方案也适用于调试您的应用程序。有时，即使出现前面的错误，您的Spark应用程序也会正常运行。但是，如果数据集的大小很大，前面的错误很可能会发生。
- en: Debugging Spark applications
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试Spark应用程序
- en: In this section, we will see how to debug Spark applications that are running
    locally (on Eclipse or IntelliJ), standalone or cluster mode in YARN or Mesos.
    However, before diving deeper, it is necessary to know about logging in the Spark
    application.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何调试在Eclipse或IntelliJ上本地运行（独立或集群模式在YARN或Mesos中）的Spark应用程序。然而，在深入讨论之前，有必要了解Spark应用程序中的日志记录。
- en: Logging with log4j with Spark recap
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用log4j记录Spark回顾
- en: 'We have already discussed this topic in [Chapter 14](part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c),
    *Time to Put Some Order - Cluster Your Data with Spark MLlib*. However, let''s
    replay the same contents to make your brain align with the current discussion
    *Debugging Spark applications*. As stated earlier, Spark uses log4j for its own
    logging. If you configured Spark properly, Spark gets logged all the operation
    to the shell console. A sample snapshot of the file can be seen from the following
    figure:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第14章](part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c)，*使用Spark
    MLlib对数据进行集群化*中讨论过这个话题。然而，让我们重复相同的内容，以使您的思维与当前讨论*调试Spark应用程序*保持一致。如前所述，Spark使用log4j进行自身的日志记录。如果您正确配置了Spark，Spark会将所有操作记录到shell控制台。以下是文件的样本快照：
- en: '![](img/00259.jpeg)**Figure 16:** A snap of the log4j.properties file'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00259.jpeg)**图16：** log4j.properties文件的快照'
- en: 'Set the default spark-shell log level to WARN. When running the spark-shell,
    the log level for this class is used to overwrite the root logger''s log level
    so that the user can have different defaults for the shell and regular Spark apps.
    We also need to append JVM arguments when launching a job executed by an executor
    and managed by the driver. For this, you should edit the `conf/spark-defaults.conf`.
    In short, the following options can be added:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 将默认的spark-shell日志级别设置为WARN。运行spark-shell时，此类的日志级别用于覆盖根记录器的日志级别，以便用户可以为shell和常规Spark应用程序设置不同的默认值。当启动由执行器执行并由驱动程序管理的作业时，我们还需要附加JVM参数。为此，您应该编辑`conf/spark-defaults.conf`。简而言之，可以添加以下选项：
- en: '[PRE23]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To make the discussion clearer, we need to hide all the logs generated by Spark.
    We then can redirect them to be logged in the file system. On the other hand,
    we want our own logs to be logged in the shell and a separate file so that they
    don''t get mixed up with the ones from Spark. From here, we will point Spark to
    the files where our own logs are, which in this particular case is `/var/log/sparkU.log`.
    This `log4j.properties` file is then picked up by Spark when the application starts,
    so we don''t have to do anything aside of placing it in the mentioned location:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使讨论更清晰，我们需要隐藏Spark生成的所有日志。然后我们可以将它们重定向到文件系统中进行记录。另一方面，我们希望我们自己的日志被记录在shell和单独的文件中，这样它们就不会与Spark的日志混在一起。从这里开始，我们将指向Spark的文件，其中我们自己的日志所在，特别是`/var/log/sparkU.log`。这个`log4j.properties`文件在应用程序启动时被Spark接管，因此我们除了将其放在指定的位置之外，不需要做任何事情：
- en: '[PRE24]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the preceding code, everything is printed as INFO once the log level is
    set to `INFO` until you set the level to new level for example `WARN`. However,
    after that no info or trace and so on, that will note be printed. In addition
    to that, there are several valid logging levels supported by log4j with Spark.
    The successful execution of the preceding code should generate the following output:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，一旦将日志级别设置为`INFO`，则所有内容都将以INFO打印，直到将级别设置为新级别，例如`WARN`。然而，在那之后，不会打印任何信息或跟踪等，不会被打印。除此之外，log4j与Spark支持几个有效的日志记录级别。前面的代码成功执行应该生成以下输出：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can also set up the default logging for Spark shell in `conf/log4j.properties`.
    Spark provides a template of the log4j as a property file, and we can extend and
    modify that file for logging in Spark. Move to the `SPARK_HOME/conf` directory
    and you should see the `log4j.properties.template` file. You should use the following
    `conf/log4j.properties.template` after renaming it to `log4j.properties`. While
    developing your Spark application, you can put the `log4j.properties` file under
    your project directory while working on an IDE-based environment such as Eclipse.
    However, to disable logging completely, just set the `log4j.logger.org` flags
    as `OFF` as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在`conf/log4j.properties`中设置Spark shell的默认日志记录。Spark提供了log4j的模板作为属性文件，我们可以扩展和修改该文件以记录Spark的日志。转到`SPARK_HOME/conf`目录，您应该看到`log4j.properties.template`文件。将其重命名为`log4j.properties`后，您应该使用以下`conf/log4j.properties.template`。在开发Spark应用程序时，您可以将`log4j.properties`文件放在项目目录下，例如在Eclipse等基于IDE的环境中工作。但是，要完全禁用日志记录，只需将`log4j.logger.org`标志设置为`OFF`，如下所示：
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'So far, everything is very easy. However, there is a problem we haven''t noticed
    yet in the preceding code segment. One drawback of the `org.apache.log4j.Logger`
    class is that it is not serializable, which implies that we cannot use it inside
    a closure while doing operations on some parts of the Spark API. For example,
    suppose we do the following in our Spark code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切都很容易。然而，在前面的代码段中，我们还没有注意到一个问题。`org.apache.log4j.Logger`类的一个缺点是它不可序列化，这意味着我们不能在对Spark
    API的某些部分进行操作时在闭包内使用它。例如，假设我们在我们的Spark代码中执行以下操作：
- en: '[PRE27]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should experience an exception that says `Task` not serializable as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会遇到一个异常，显示“任务”不可序列化，如下所示：
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'At first, we can try to solve this problem in a naive way. What you can do
    is just make the Scala class (that does the actual operation) `Serializable` using
    `extends Serializable` . For example, the code looks as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以尝试以一种天真的方式解决这个问题。您可以做的是使执行实际操作的Scala类（使用`extends Serializable`）可序列化。例如，代码如下所示：
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This section is intended for carrying out a discussion on logging. However,
    we take the opportunity to make it more versatile for general purpose Spark programming
    and issues. In order to overcome the `task not serializable` error in a more efficient
    way, compiler will try to send the whole object (not only the lambda) by making
    it serializable and forces SPark to accept that. However, it increases shuffling
    significantly, especially for big objects! The other ways are making the whole
    class `Serializable` or by declaring the instance only within the lambda function
    passed in the map operation. Sometimes, keeping the not `Serializable` objects
    across the nodes can work. Lastly, use the `forEachPartition()` or `mapPartitions()`
    instead of just `map()` and create the not `Serializable` objects. In summary,
    these are the ways to solve the problem around:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在讨论日志记录。然而，我们借此机会使其更具通用性，适用于Spark编程和问题。为了更有效地克服“任务不可序列化”错误，编译器将尝试通过使其可序列化并强制SPark接受整个对象（而不仅仅是lambda）来发送整个对象。然而，这会显著增加洗牌，特别是对于大对象！其他方法包括使整个类“Serializable”或仅在map操作中传递的lambda函数内声明实例。有时，跨节点保留不可序列化的对象也可以起作用。最后，使用`forEachPartition()`或`mapPartitions()`而不仅仅是`map()`并创建不可序列化的对象。总之，这些是解决该问题的方法：
- en: Serializable the class
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使类可序列化
- en: Declare the instance only within the lambda function passed in the map
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅在map中传递的lambda函数内声明实例
- en: Make the NotSerializable object as a static and create it once per machine
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将NotSerializable对象设置为静态，并在每台机器上创建一次
- en: Call the `forEachPartition ()` or `mapPartitions()` instead of `map()` and create
    the NotSerializable object
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`forEachPartition()`或`mapPartitions()`而不是`map()`并创建NotSerializable对象
- en: 'In the preceding code, we have used the annotation `@transient lazy`, which
    marks the `Logger` class to be nonpersistent. On the other hand, object containing
    the method apply (i.e. `MyMapperObject`) that instantiate the object of the `MyMapper`
    class is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了`@transient lazy`注解，将`Logger`类标记为非持久化。另一方面，包含`apply`方法（即`MyMapperObject`）的对象，它实例化了`MyMapper`类的对象如下：
- en: '[PRE30]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, the object containing the `main()` method is as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，包含`main()`方法的对象如下：
- en: '[PRE31]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, let''s see another example that provides better insight to keep fighting
    the issue we are talking about. Suppose we have the following class that computes
    the multiplication of two integers:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看另一个例子，它提供了更好的洞察力，以继续解决我们正在讨论的问题。假设我们有一个计算两个整数乘法的类如下：
- en: '[PRE32]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, essentially, if you try to use this class for computing the multiplication
    in the lambda closure using `map()`, you will get the `Task Not Serializable`
    error that we described earlier. Now we simply can use `foreachPartition()` and
    the lambda inside as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您尝试在lambda闭包中使用此类来计算乘法，您将得到我们之前描述的“任务不可序列化”错误。现在我们可以简单地使用`foreachPartition()`和lambda，如下所示：
- en: '[PRE33]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, if you compile it, it should return the desired result. For your ease,
    the complete code with the `main()` method is as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您编译它，应该返回所需的结果。为了方便起见，包含`main()`方法的完整代码如下：
- en: '[PRE34]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE35]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Debugging the Spark application
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试Spark应用程序
- en: "In this section, we will discuss how to debug Spark applications running on\
    \ locally on Eclipse or IntelliJ, as standalone or cluster mode in YARN or Mesos.\
    \ Before getting started, you can also read the debugging documentation at [\uFEFF\
    https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/](https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/)."
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: "在本节中，我们将讨论如何在Eclipse或IntelliJ上本地运行或以YARN或Mesos的独立或集群模式运行的Spark应用程序进行调试。在开始之前，您还可以阅读[\uFEFF\
    https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/](https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/)上的调试文档。"
- en: Debugging Spark application on Eclipse as Scala debug
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Eclipse上调试Spark应用程序作为Scala调试
- en: 'To make this happen, just configure your Eclipse to debug your Spark applications
    as a regular Scala code debug. To configure select Run | Debug Configuration |
    Scala Application as shown in the following figure:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，只需将您的Eclipse配置为调试您的Spark应用程序，就像调试常规的Scala代码一样。要配置，请选择Run | Debug Configuration
    | Scala Application，如下图所示：
- en: '![](img/00022.jpeg)**Figure 17:** Configuring Eclipse to debug Spark applications
    as a regular Scala code debug'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00022.jpeg)**图17：**配置Eclipse以调试Spark应用程序，作为常规的Scala代码调试'
- en: 'Suppose we want to debug our `KMeansDemo.scala` and ask Eclipse (you can have
    similar options on InteliJ IDE) to start the execution at line 56 and set the
    breakpoint in line 95\. To do so, run your Scala code as debugging and you should
    observe the following scenario on Eclipse:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要调试我们的`KMeansDemo.scala`并要求Eclipse（您也可以在InteliJ IDE上有类似的选项）从第56行开始执行，并在第95行设置断点。要这样做，运行您的Scala代码进行调试，您应该在Eclipse上观察到以下情景：
- en: '![](img/00327.jpeg)**Figure 18:** Debugging Spark applications on Eclipse'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00327.jpeg)**图18：**在Eclipse上调试Spark应用程序'
- en: 'Then, Eclipse will pause on the line you ask it to stop the execution in line
    95, as shown in the following screenshot:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Eclipse将在你要求它在第95行停止执行时暂停，如下面的截图所示：
- en: '![](img/00221.jpeg)**Figure 19:** Debugging Spark applications on Eclipse (breakpoint)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00221.jpeg)**图19：**在Eclipse上调试Spark应用程序（断点）'
- en: In summary, to simplify the preceding example, if there is any error between
    line 56 and line 95, Eclipse will show where the error actually occurs. Otherwise,
    it will follow the normal workflow if not interrupted.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，为了简化上面的例子，如果在第56行和第95行之间有任何错误，Eclipse将显示错误实际发生的位置。否则，如果没有中断，它将按照正常的工作流程进行。
- en: Debugging Spark jobs running as local and standalone mode
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地和独立模式下运行Spark作业的调试
- en: 'While debugging your Spark application locally or as standalone mode, you should
    know that debugging the driver program and debugging one of the executors is different
    since using these two types of nodes requires different submission parameters
    passed to `spark-submit`. Throughout this section, I''ll use port 4000 as the
    address. For example, if you want to debug the driver program, you can add the
    following to your `spark-submit` command:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地或独立模式下调试你的Spark应用程序时，你应该知道调试驱动程序程序和调试执行程序之间是不同的，因为使用这两种类型的节点需要传递不同的提交参数给`spark-submit`。在本节中，我将使用端口4000作为地址。例如，如果你想调试驱动程序程序，你可以将以下内容添加到你的`spark-submit`命令中：
- en: '[PRE36]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: After that, you should set your remote debugger to connect to the node where
    you have submitted the driver program. For the preceding case, port number 4000
    was specified. However, if something (that is, other Spark jobs, other applications
    or services, and so on) is already running on that port, you might also need to
    customize that port, that is, change the port number.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你应该设置你的远程调试器连接到你提交驱动程序的节点。对于前面的情况，指定了端口号4000。然而，如果某些东西（即其他Spark作业、其他应用程序或服务等）已经在该端口上运行，你可能还需要自定义该端口，即更改端口号。
- en: 'On the other hand, connecting to an executor is similar to the preceding option,
    except for the address option. More specifically, you will have to replace the
    address with your local machine''s address (IP address or host name with the port
    number). However, it is always a good practice and recommended to test that you
    can access your local machine from the Spark cluster where the actual computing
    occurs. For example, you can use the following options to make the debugging environment
    enable to your `spark-submit` command:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，连接到执行程序与前面的选项类似，除了地址选项。更具体地说，你需要用你本地机器的地址（IP地址或带有端口号的主机名）替换地址。然而，测试你是否可以从实际计算发生的Spark集群访问你的本地机器是一种良好的实践和建议。例如，你可以使用以下选项使调试环境对你的`spark-submit`命令启用：
- en: '[PRE37]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In summary, use the following command to submit your Spark jobs (the `KMeansDemo`
    application in this case):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，使用以下命令提交你的Spark作业（在这种情况下是`KMeansDemo`应用程序）：
- en: '[PRE38]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, start your local debugger in a listening mode and start your Spark program.
    Finally, wait for the executor to attach to your debugger. You will observe the
    following message on your terminal:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，启动你的本地调试器处于监听模式，并启动你的Spark程序。最后，等待执行程序连接到你的调试器。你将在你的终端上看到以下消息：
- en: '[PRE39]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'It is important to know that you need to set the number of executors to 1 only.
    Setting multiple executors will all try to connect to your debugger and will eventually
    create some weird problems. It is to be noted that sometimes setting the `SPARK_JAVA_OPTS`
    helps in debugging your Spark applications that are running locally or as standalone
    mode. The command is as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要知道，你只需要将执行程序的数量设置为1。设置多个执行程序将尝试连接到你的调试器，并最终创建一些奇怪的问题。需要注意的是，有时设置`SPARK_JAVA_OPTS`有助于调试在本地或独立模式下运行的Spark应用程序。命令如下：
- en: '[PRE40]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: However, since Spark release 1.0.0, `SPARK_JAVA_OPTS` has been deprecated and
    replaced by `spark-defaults.conf` and command line arguments to Spark-submit or
    Spark-shell. It is also to be noted that setting `spark.driver.extraJavaOptions`
    and `spark.executor.extraJavaOptions`, which we saw in the previous section, in
    `spark-defaults.conf` is not a replacement for `SPARK_JAVA_OPTS`. But to be frank,
    `SPARK_JAVA_OPTS`, it still works pretty well and you can try as well.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自Spark 1.0.0发布以来，`SPARK_JAVA_OPTS`已被弃用，并由`spark-defaults.conf`和传递给Spark-submit或Spark-shell的命令行参数取代。需要注意的是，在`spark-defaults.conf`中设置`spark.driver.extraJavaOptions`和`spark.executor.extraJavaOptions`并不是`SPARK_JAVA_OPTS`的替代。但坦率地说，`SPARK_JAVA_OPTS`仍然运行得很好，你也可以尝试一下。
- en: Debugging Spark applications on YARN or Mesos cluster
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在YARN或Mesos集群上调试Spark应用程序
- en: 'When you run a Spark application on YARN, there is an option that you can enable
    by modifying `yarn-env.sh`:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN上运行Spark应用程序时，有一个选项可以通过修改`yarn-env.sh`来启用：
- en: '[PRE41]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, the remote debugging will be available through port 4000 on your Eclipse
    or IntelliJ IDE. The second option is by setting the `SPARK_SUBMIT_OPTS`. You
    can use either Eclipse or IntelliJ to develop your Spark applications that can
    be submitted to be executed on remote multinode YARN clusters. What I do is that
    I create a Maven project on Eclipse or IntelliJ and package my Java or Scala application
    as a jar file and then submit it as a Spark job. However, in order to attach your
    IDE such as Eclipse or IntelliJ debugger to your Spark application, you can define
    all the submission parameters using the `SPARK_SUBMIT_OPTS` environment variable
    as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，远程调试将通过Eclipse或IntelliJ IDE上的端口4000可用。第二个选项是通过设置`SPARK_SUBMIT_OPTS`。你可以使用Eclipse或IntelliJ开发你的Spark应用程序，然后将其提交以在远程多节点YARN集群上执行。我在Eclipse或IntelliJ上创建一个Maven项目，并将我的Java或Scala应用程序打包为一个jar文件，然后将其提交为一个Spark作业。然而，为了将你的IDE（如Eclipse或IntelliJ）调试器连接到你的Spark应用程序，你可以使用`SPARK_SUBMIT_OPTS`环境变量定义所有的提交参数，如下所示：
- en: '[PRE42]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then submit your Spark job as follows (please change the values accordingly
    based on your requirements and setup):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按照以下方式提交你的Spark作业（请根据你的需求和设置相应地更改值）：
- en: '[PRE43]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'After running the preceding command, it will wait until you connect your debugger,
    as shown in the following: `Listening for transport dt_socket at address: 4000`.
    Now you can configure your Java remote application (Scala application will work
    too) on the IntelliJ debugger, as shown in the following screenshot:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '运行上述命令后，它将等待您连接调试器，如下所示：`Listening for transport dt_socket at address: 4000`。现在，您可以在IntelliJ调试器上配置您的Java远程应用程序（Scala应用程序也可以），如下截图所示：'
- en: '![](img/00331.jpeg)**Figure 20:** Configuring remote debugger on IntelliJ'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：在IntelliJ上配置远程调试器
- en: 'For the preceding case, 10.200.1.101 is the IP address of the remote computing
    node where your Spark job is basically running. Finally, you will have to start
    the debugger by clicking on Debug under IntelliJ''s Run menu. Then, if the debugger
    connects to your remote Spark app, you will see the logging info in the application
    console on IntelliJ. Now if you can set the breakpoints and the rests of them
    are normal debugging. The following figure shows an example how will you see on
    the IntelliJ when pausing a Spark job with a breakpoint:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述情况，10.200.1.101是远程计算节点的IP地址，您的Spark作业基本上是在该节点上运行的。最后，您将需要通过在IntelliJ的运行菜单下单击“调试”来启动调试器。然后，如果调试器连接到您的远程Spark应用程序，您将在IntelliJ的应用程序控制台中看到日志信息。现在，如果您可以设置断点，其他操作都是正常的调试。下图显示了在IntelliJ上暂停具有断点的Spark作业时的示例：
- en: '![](img/00007.jpeg)**Figure 21:** An example how will you see on the IntelliJ
    when pausing a Spark job with a breakpoint'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：在IntelliJ上暂停Spark作业并设置断点时的示例
- en: 'Although it works well, but sometimes I experienced that using `SPARK_JAVA_OPTS`
    won''t help you much in the debug process on Eclipse or even IntelliJ. Instead,
    use and export `SPARK_WORKER_OPTS` and `SPARK_MASTER_OPTS` while running your
    Spark jobs on a real cluster (YARN, Mesos, or AWS) as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它运行良好，但有时我发现在Eclipse甚至IntelliJ上使用`SPARK_JAVA_OPTS`并不会对调试过程有太大帮助。相反，当在真实集群（YARN、Mesos或AWS）上运行Spark作业时，请使用和导出`SPARK_WORKER_OPTS`和`SPARK_MASTER_OPTS`，如下所示：
- en: '[PRE44]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then start your Master node as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按以下方式启动您的Master节点：
- en: '[PRE45]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now open an SSH connection to your remote machine where the Spark job is actually
    running and map your localhost at 4000 (aka `localhost:4000`) to `host_name_to_your_computer.org:5000`,
    assuming the cluster is at `host_name_to_your_computer.org:5000` and listening
    on port 5000\. Now that your Eclipse will consider that you''re just debugging
    your Spark application as a local Spark application or process. However, to make
    this happen, you will have to configure the remote debugger on Eclipse, as shown
    in the following figure:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打开一个SSH连接到实际运行Spark作业的远程机器，并将您的本地主机映射到`host_name_to_your_computer.org:5000`的4000端口（即`localhost:4000`），假设集群位于`host_name_to_your_computer.org:5000`并在端口5000上监听。现在，您的Eclipse将认为您只是在调试本地Spark应用程序或进程。但是，要实现这一点，您将需要在Eclipse上配置远程调试器，如下图所示：
- en: '![](img/00141.jpeg)**Figure 22:** Connecting remote host on Eclipse for debugging
    Spark application'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：在Eclipse上连接远程主机以调试Spark应用程序
- en: That's it! Now you can debug on your live cluster as if it were your desktop.
    The preceding examples are for running with the Spark Master set as YARN-client.
    However, it should also work when running on a Mesos cluster. If you're running
    using YARN-cluster mode, you may have to set the driver to attach to your debugger
    rather than attaching your debugger to the driver since you won't necessarily
    know in advance what mode the driver will be executing on.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在您可以像在桌面上一样在您的实时集群上进行调试。上述示例是在将Spark Master设置为YARN-client模式下运行时的。但是，当在Mesos集群上运行时，它也应该起作用。如果您使用YARN-cluster模式运行，您可能需要将驱动程序设置为连接到调试器，而不是将调试器附加到驱动程序，因为您不一定会预先知道驱动程序将在哪种模式下执行。
- en: Debugging Spark application using SBT
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SBT调试Spark应用程序
- en: 'The preceding setting works mostly on Eclipse or IntelliJ using the Maven project.
    Suppose that you already have your application done and are working on your preferred
    IDEs such as IntelliJ or Eclipse as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 上述设置在大多数情况下适用于使用Maven项目的Eclipse或IntelliJ。假设您已经完成了应用程序，并且正在使用您喜欢的IDE（如IntelliJ或Eclipse）进行工作，如下所示：
- en: '[PRE46]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, if you want to get this job to the local cluster (standalone), the very
    first step is packaging the application with all its dependencies into a fat JAR.
    For doing this, use the following command:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您想将此作业提交到本地集群（独立运行），第一步是将应用程序及其所有依赖项打包成一个fat JAR。为此，请使用以下命令：
- en: '[PRE47]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This will generate the fat JAR. Now the task is to submit the Spark job to
    a local cluster. You need to have spark-submit script somewhere on your system:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成fat JAR。现在的任务是将Spark作业提交到本地集群。您需要在系统的某个地方有spark-submit脚本：
- en: '[PRE48]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding command exports a Java argument that will be used to start Spark
    with the debugger:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令导出一个Java参数，该参数将用于启动带有调试器的Spark：
- en: '[PRE49]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In the preceding command, `--class` needs to point to a fully qualified class
    path to your job. Upon successful execution of this command, your Spark job will
    be executed without breaking at the breakpoints. Now to get the debugging facility
    on your IDE, say IntelliJ, you need to configure to connect to the cluster. For
    more details on the official IDEA documentation, refer to [http://stackoverflow.com/questions/21114066/attach-intellij-idea-debugger-to-a-running-java-process](http://stackoverflow.com/questions/21114066/attach-intellij-idea-debugger-to-a-running-java-process).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，`--class`需要指向作业的完全限定类路径。成功执行此命令后，您的Spark作业将在不中断断点的情况下执行。现在，要在您的IDE（比如IntelliJ）上获得调试功能，您需要配置连接到集群。有关官方IDEA文档的更多详细信息，请参考[http://stackoverflow.com/questions/21114066/attach-intellij-idea-debugger-to-a-running-java-process](http://stackoverflow.com/questions/21114066/attach-intellij-idea-debugger-to-a-running-java-process)。
- en: It is to be noted that if you just create a default remote run/debug configuration
    and leave the default port of 5005, it should work fine. Now, when you submit
    the job for the next time and see the message to attach the debugger, you have
    eight seconds to switch to IntelliJ IDEA and trigger this run configuration. The
    program will then continue to execute and pause at any breakpoint you defined.
    You can then step through it like any normal Scala/Java program. You can even
    step into Spark functions to see what it's doing under the hood.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，如果您只创建一个默认的远程运行/调试配置并保留默认端口5005，它应该可以正常工作。现在，当您提交下一次作业并看到附加调试器的消息时，您有八秒钟切换到IntelliJ
    IDEA并触发此运行配置。程序将继续执行并在您定义的任何断点处暂停。然后，您可以像任何普通的Scala/Java程序一样逐步执行它。您甚至可以进入Spark函数以查看它在幕后做了什么。
- en: Summary
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you saw how difficult the testing and debugging your Spark
    applications are. These can even be more critical in a distributed environment.
    We also discussed some advanced ways to tackle them altogether. In summary, you
    learned the way of testing in a distributed environment. Then you learned a better
    way of testing your Spark application. Finally, we discussed some advanced ways
    of debugging Spark applications.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您看到了测试和调试Spark应用程序有多么困难。在分布式环境中，这甚至可能更加关键。我们还讨论了一些解决这些问题的高级方法。总之，您学会了在分布式环境中进行测试的方法。然后，您学会了更好地测试您的Spark应用程序。最后，我们讨论了一些调试Spark应用程序的高级方法。
- en: We believe that this book will help you to gain some good understanding of Spark.
    Nevertheless, due to page limitation, we could not cover many APIs and their underlying
    functionalities. If you face any issues, please don't forget to report this to
    Spark user mailing list at `user@spark.apache.org`. Before doing so, make sure
    that you have subscribed to it.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信这本书将帮助您对Spark有一些很好的理解。然而，由于页面限制，我们无法涵盖许多API及其基本功能。如果您遇到任何问题，请不要忘记向Spark用户邮件列表`user@spark.apache.org`报告。在这样做之前，请确保您已经订阅了它。
- en: This is more or less the end of our little journey with advanced topics on Spark.
    Now, a general suggestion from our side to you as readers or if you are relatively
    newer to the data science, data analytics, machine learning, Scala, or Spark is
    that you should at first try to understand what types of analytics you want to
    perform. To be more specific, for example, if your problem is a machine learning
    problem, try to guess what type of learning algorithms should be the best fit,
    that is, classification, clustering, regression, recommendation, or frequent pattern
    mining. Then define and formulate the problem, and after that, you should generate
    or download the appropriate data based on the feature engineering concept of Spark
    that we have discussed earlier. On the other hand, if you think that you can solve
    your problem using deep learning algorithms or APIs, you should use other third-party
    algorithms and integrate with Spark and work straight away.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这更多或多少是我们在Spark高级主题上的小旅程的结束。现在，我们对您作为读者的一般建议是，如果您对数据科学、数据分析、机器学习、Scala或Spark相对较新，您应该首先尝试了解您想要执行的分析类型。更具体地说，例如，如果您的问题是一个机器学习问题，尝试猜测哪种类型的学习算法应该是最合适的，即分类、聚类、回归、推荐或频繁模式挖掘。然后定义和规划问题，之后，您应该基于我们之前讨论过的Spark的特征工程概念生成或下载适当的数据。另一方面，如果您认为您可以使用深度学习算法或API解决问题，您应该使用其他第三方算法并与Spark集成并立即工作。
- en: Our final recommendation to the readers is to browse the Spark website (at [http://spark.apache.org/](http://spark.apache.org/))
    regularly to get the updates and also try to incorporate the regular Spark-provided
    APIs with other third-party applications or tools to get the best result of the
    collaboration.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后的建议是，读者定期浏览Spark网站（[http://spark.apache.org/](http://spark.apache.org/)）以获取更新，并尝试将常规提供的Spark
    API与其他第三方应用程序或工具结合起来，以获得合作的最佳结果。
