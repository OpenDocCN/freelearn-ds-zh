- en: Installing and Configuring Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和配置Spark
- en: 'In this chapter, we will cover how to install and configure Spark, either as
    a local instance, a multi-node cluster, or in a virtual environment. You will
    learn the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何安装和配置Spark，无论是作为本地实例、多节点集群还是虚拟环境。您将学习以下示例：
- en: Installing Spark requirements
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Spark要求
- en: Installing Spark from sources
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从源代码安装Spark
- en: Installing Spark from binaries
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从二进制文件安装Spark
- en: Configuring a local instance of Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Spark的本地实例
- en: Configuring a multi-node instance of Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Spark的多节点实例
- en: Installing Jupyter
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Jupyter
- en: Configuring a session in Jupyter
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Jupyter中配置会话
- en: Working with Cloudera Spark images
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Cloudera Spark镜像
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: We cannot begin a book on Spark (well, on PySpark) without first specifying
    what Spark is. Spark is a powerful, flexible, open source, data processing and
    querying engine. It is extremely easy to use and provides the means to solve a
    huge variety of problems, ranging from processing unstructured, semi-structured,
    or structured data, through streaming, up to machine learning. With over 1,000
    contributors from over 250 organizations (not to mention over 3,000 Spark Meetup
    community members worldwide), Spark is now one of the largest open source projects
    in the portfolio of the Apache Software Foundation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能在Spark（或PySpark）的书籍中开始之前，不先说明Spark是什么。Spark是一个强大、灵活、开源的数据处理和查询引擎。它非常易于使用，并提供了解决各种问题的手段，从处理非结构化、半结构化或结构化数据，到流处理，再到机器学习。有来自250多个组织的1000多名贡献者（更不用说全球3000多名Spark
    Meetup社区成员），Spark现在是Apache软件基金会组合中最大的开源项目之一。
- en: 'The origins of Spark can be found in 2012 when it was first released; Matei
    Zacharia developed the first versions of the Spark processing engine at UC Berkeley
    as part of his PhD thesis. Since then, Spark has become extremely popular, and
    its popularity stems from a number of reasons:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的起源可以追溯到2012年，当时它首次发布；Matei Zacharia在加州大学伯克利分校开发了Spark处理引擎的最初版本，作为他的博士论文的一部分。从那时起，Spark变得非常流行，其流行程度源于许多原因：
- en: '**It is fast**: It is estimated that Spark is 100 times faster than Hadoop
    when working purely in memory, and around 10 times faster when reading or writing
    data to a disk.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它是快速的**：据估计，Spark在纯内存工作时比Hadoop快100倍，在读取或写入数据到磁盘时大约快10倍。'
- en: '**It is flexible**: You can leverage the power of Spark from a number of programming
    languages; Spark natively supports interfaces in Scala, Java, Python, and R.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它是灵活的**：您可以从多种编程语言中利用Spark的强大功能；Spark原生支持Scala、Java、Python和R的接口。'
- en: '**It is extendible**: As Spark is an open source package, you can easily extend
    it by introducing your own classes or extending the existing ones.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它是可扩展的**：由于Spark是一个开源软件包，您可以通过引入自己的类或扩展现有类来轻松扩展它。'
- en: '**It is powerful**: Many machine learning algorithms are already implemented
    in Spark so you do not need to add more tools to your stack—most of the data engineering
    and data science tasks can be accomplished while working in a single environment.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它是强大的**：许多机器学习算法已经在Spark中实现，因此您无需向堆栈添加更多工具——大多数数据工程和数据科学任务可以在单个环境中完成。'
- en: '**It is familiar**: Data scientists and data engineers, who are accustomed
    to using Python''s `pandas`, or R''s `data.frames` or `data.tables`, should have
    a much gentler learning curve (although the differences between these data types
    exist). Moreover, if you know SQL, you can also use it to wrangle data in Spark!'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它是熟悉的**：习惯于使用Python的`pandas`、R的`data.frames`或`data.tables`的数据科学家和数据工程师应该有一个更加温和的学习曲线（尽管这些数据类型之间存在差异）。此外，如果您了解SQL，也可以在Spark中使用它来整理数据！'
- en: '**It is scalable**: Spark can run locally on your machine (with all the limitations
    such a solution entails). However, the same code that runs locally can be deployed
    to a cluster of thousands of machines with little-to-no changes.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它是可扩展的**：Spark可以在您的机器上本地运行（带有此类解决方案的所有限制）。但是，相同的代码可以在成千上万台机器的集群上部署，几乎不需要进行任何更改。'
- en: For the remainder of this book, we will assume that you are working in a Unix-like
    environment such as Linux (throughout this book, we will use Ubuntu Server 16.04
    LTS) or macOS (running macOS High Sierra); all the code provided has been tested
    in these two environments. For this chapter (and some other ones, too), an internet
    connection is also required as we will be downloading a bunch of binaries and
    sources from the internet.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们将假设您正在类Unix环境中工作，如Linux（在本书中，我们将使用Ubuntu Server 16.04 LTS）或macOS（运行macOS
    High Sierra）；所有提供的代码都在这两个环境中进行了测试。对于本章（以及其他一些章节），还需要互联网连接，因为我们将从互联网上下载一堆二进制文件和源文件。
- en: We will not be focusing on installing Spark in a Windows environment as it is
    not truly supported by the Spark developers. However, if you are inclined to try,
    you can follow some of the instructions you will find online, such as from the
    following link: [http://bit.ly/2Ar75ld](http://bit.ly/2Ar75ld).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会专注于在Windows环境中安装Spark，因为这并不是Spark开发人员真正支持的。但是，如果您有兴趣尝试，可以按照您在网上找到的一些说明，例如从以下链接：[http://bit.ly/2Ar75ld](http://bit.ly/2Ar75ld)。
- en: Knowing how to use the command line and how to set some environment variables
    on your system is useful, but not really required—we will guide you through the
    steps.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何使用命令行以及如何在系统上设置一些环境变量是有用的，但并非真正必需——我们将指导您完成这些步骤。
- en: Installing Spark requirements
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Spark要求
- en: Spark requires a handful of environments to be present on your machine before
    you can install and use it. In this recipe, we will focus on getting your machine
    ready for Spark installation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装和使用Spark之前，您的机器需要具备一些环境。在这个示例中，我们将专注于准备您的机器以安装Spark。
- en: Getting ready
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a bash Terminal and an internet connection.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个示例，您需要一个bash终端和一个互联网连接。
- en: 'Also, before we start any work, you should clone the GitHub repository for
    this book. The repository contains all the codes (in the form of notebooks) and
    all the data you will need to follow the examples in this book. To clone the repository,
    go to [http://bit.ly/2ArlBck](http://bit.ly/2ArlBck), click on the Clone or download
    button, and copy the URL that shows up by clicking on the icon next to it:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，在开始任何工作之前，您应该克隆本书的GitHub存储库。存储库包含了本书中所有示例的代码（以笔记本的形式）和所有所需的数据。要克隆存储库，请转到[http://bit.ly/2ArlBck](http://bit.ly/2ArlBck)，单击“克隆或下载”按钮，并复制显示的URL，方法是单击旁边的图标：
- en: '![](img/00005.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00005.jpeg)'
- en: 'Next, go to your Terminal and issue the following command:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，转到您的终端并发出以下命令：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If your `git` environment is set up properly, the whole GitHub repository should
    clone to your disk. No other prerequisites are required.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的`git`环境设置正确，整个GitHub存储库应该克隆到您的磁盘上。不需要其他先决条件。
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'There are just truly two main requirements for installing PySpark: Java and
    Python. Additionally, you can also install Scala and R if you want to use those
    languages, and we will also check for Maven, which we will use to compile the
    Spark sources.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 安装PySpark只需满足两个主要要求：Java和Python。此外，如果您想要使用这些语言，还可以安装Scala和R，我们还将检查Maven，我们将用它来编译Spark源代码。
- en: 'To do this, we will use the `checkRequirements.sh` script to check for all
    the requirements: the script is located in the `Chapter01` folder from the GitHub
    repository.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用`checkRequirements.sh`脚本来检查所有要求：该脚本位于GitHub存储库的`Chapter01`文件夹中。
- en: 'The following code block shows the high-level portions of the script found
    in the `Chapter01/checkRequirements.sh` file. Note that some portions of the code
    were omitted here for brevity:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块显示了在`Chapter01/checkRequirements.sh`文件中找到的脚本的高级部分。请注意，出于简洁起见，此处省略了部分代码：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How it works...
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: First, we will specify all the required packages and their required minimum
    versions; looking at the preceding code, you can see that Spark 2.3.1 requires
    Java 1.8+ and Python 3.4 or higher (and we will always be checking for these two
    environments). Additionally, if you want to use R or Scala, the minimal requirements
    for these two packages are 3.1 and 2.11, respectively. Maven, as mentioned earlier,
    will be used to compile the Spark sources, and for doing that, Spark requires
    at least the 3.3.9 version of Maven.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将指定所有所需的软件包及其所需的最低版本；从前面的代码中可以看出，Spark 2.3.1需要Java 1.8+和Python 3.4或更高版本（我们将始终检查这两个环境）。此外，如果您想要使用R或Scala，这两个软件包的最低要求分别为3.1和2.11。如前所述，Maven将用于编译Spark源代码，为此，Spark至少需要Maven的3.3.9版本。
- en: You can check the Spark requirements here: [https://spark.apache.org/docs/latest/index.html](https://spark.apache.org/docs/latest/index.html)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处检查Spark的要求：[https://spark.apache.org/docs/latest/index.html](https://spark.apache.org/docs/latest/index.html)
- en: You can check the requirements for building Spark here: [https://spark.apache.org/docs/latest/building-spark.html](https://spark.apache.org/docs/latest/building-spark.html).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处检查构建Spark的要求：[https://spark.apache.org/docs/latest/building-spark.html](https://spark.apache.org/docs/latest/building-spark.html)。
- en: 'Next, we parse the command-line arguments:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们解析命令行参数：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You, as a user, can specify whether you want to check additionally for R, Scala,
    and Maven dependencies. To do so, run the following code from your command line
    (the following code will check for all of them):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作为用户，您可以指定是否要额外检查R、Scala和Maven的依赖关系。要这样做，请从命令行运行以下代码（以下代码将检查所有这些）：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following is also a perfectly valid usage:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下也是一个完全有效的用法：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we call three functions: `printHeader`, `checkJava`, and `checkPython`.
    The `printHeader` function is nothing more than just a simple way for the script
    to state what it does and it really is not that interesting, so we will skip it
    here; it is, however, fairly self-explanatory, so you are welcome to peruse the
    relevant portions of the `checkRequirements.sh` script yourself.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们调用三个函数：`printHeader`，`checkJava`和`checkPython`。`printHeader`函数只是脚本陈述其功能的一种简单方式，这并不是很有趣，所以我们将在这里跳过它；但它相当容易理解，所以您可以自行查看`checkRequirements.sh`脚本的相关部分。
- en: 'Next, we will check whether Java is installed. First, we just print to the
    Terminal that we are performing checks on Java (this is common across all of our
    functions, so we will only mention it here):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查Java是否已安装。首先，我们只是在终端打印，说明我们正在对Java进行检查（这在我们所有的功能中都很常见，所以我们只在这里提一下）：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Following this, we will check if the Java environment is installed on your
    machine:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查Java环境是否已安装在您的计算机上：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: First, we use the `type` command to check if the `java` command is available;
    the `type -p` command returns the location of the `java` binary if it exists.
    This also implies that the `bin` folder containing Java binaries has been added
    to the `PATH`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`type`命令检查`java`命令是否可用；`type -p`命令返回`java`二进制文件的位置（如果存在）。这也意味着包含Java二进制文件的`bin`文件夹已添加到`PATH`中。
- en: If you are certain you have the binaries installed (be it Java, Python, R, Scala,
    or Maven), you can jump to the *Updating PATH* section in this recipe to see how
    to let your computer know where these binaries live.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您确定已安装了二进制文件（无论是Java、Python、R、Scala还是Maven），您可以跳转到本教程中的*更新路径*部分，了解如何让计算机知道这些二进制文件的位置。
- en: If this fails, we will revert to checking if the `JAVA_HOME` environment variable
    is set, and if it is, we will try to see if it contains the required `java` binary: `[[
    -x "$JAVA_HOME/bin/java" ]]`. Should this fail, the program will print the message
    that no Java environment could be found and will exit (without checking for other
    required packages, like Python).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这失败了，我们将回退到检查`JAVA_HOME`环境变量是否已设置，如果设置了，我们将尝试查看它是否包含所需的`java`二进制文件：`[[ -x
    "$JAVA_HOME/bin/java" ]]`。如果这失败，程序将打印找不到Java环境的消息，并退出（而不检查其他所需的软件包，如Python）。
- en: 'If, however, the Java binary is found, then we can check its version:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果找到了Java二进制文件，我们可以检查其版本：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We first execute the `java -version` command in the Terminal, which would normally
    produce an output similar to the following screenshot:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在终端中执行`java -version`命令，这通常会产生类似以下截图的输出：
- en: '![](img/00006.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00006.jpeg)'
- en: We then pipe the previous output to `awk` to split (the `-F` switch) the rows
    at the quote `'"'` character (and will only use the first line of the output as
    we filter the rows down to those that contain `/version/`) and take the second
    (the `$2`) element as the version of the Java binaries installed on our machine.
    We will store it in the `_java_version` variable, which we also print to the screen
    using the `echo` command.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将先前的输出管道传输到`awk`，以在引号`'"'`字符处拆分（使用`-F`开关）行（并且只使用输出的第一行，因为我们将行过滤为包含`/version/`的行），并将第二个（`$2`）元素作为我们机器上安装的Java二进制文件的版本。我们将把它存储在`_java_version`变量中，并使用`echo`命令将其打印到屏幕上。
- en: If you do not know what `awk` is or how to use it, we recommend this book from
    Packt: [http://bit.ly/2BtTcBV](http://bit.ly/2BtTcBV).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不知道`awk`是什么或如何使用它，我们建议阅读Packt的这本书：[http://bit.ly/2BtTcBV](http://bit.ly/2BtTcBV)。
- en: Finally, we check if the `_java_version` we just obtained is lower than `_java_required`.
    If this evaluates to true, we will stop the execution, instead telling you to
    install the required version of Java.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们检查我们刚刚获得的`_java_version`是否低于`_java_required`。如果这是真的，我们将停止执行，而是告诉您安装所需版本的Java。
- en: 'The logic implemented in the `checkPython`, `checkR`, `checkScala`, and `checkMaven`
    functions follows in a very similar way. The only differences are in what binary
    we call and in the way we check the versions:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`checkPython`，`checkR`，`checkScala`和`checkMaven`函数中实现的逻辑方式非常相似。唯一的区别在于我们调用的二进制文件以及我们检查版本的方式：'
- en: 'For Python, we run `"$_python" --version 2>&1 | awk -F '' '' ''{print $2}''`,
    as checking the Python version (for Anaconda distribution) would print out the
    following to the screen: Python 3.5.2 :: Anaconda 2.4.1 (x86_64)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于Python，我们运行`"$_python" --version 2>&1 | awk -F '' '' ''{print $2}''`，因为检查Python版本（对于Anaconda发行版）会将以下内容打印到屏幕上：Python
    3.5.2 :: Anaconda 2.4.1 (x86_64)'
- en: For R, we use `"$_r" --version 2>&1 | awk -F ' ' '/R version/ {print $3}'`,
    as checking the R's version would write (a lot) to the screen; we only use the
    line that starts with `R version`: R version 3.4.2 (2017-09-28) -- "Short Summer"
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于R，我们使用`"$_r" --version 2>&1 | awk -F ' ' '/R version/ {print $3}'`，因为检查R的版本会在屏幕上写入（很多）；我们只使用以`R
    version`开头的行：R version 3.4.2 (2017-09-28) -- "Short Summer"
- en: For Scala, we utilize `"$_scala" -version 2>&1 | awk -F ' ' '{print $5}'`, given
    that checking Scala's version prints the following: Scala code runner version
    2.11.8 -- Copyright 2002-2016, LAMP/EPFL
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Scala，我们使用`"$_scala" -version 2>&1 | awk -F ' ' '{print $5}'`，因为检查Scala的版本会打印以下内容：Scala代码运行器版本2.11.8
    -- 版权所有2002-2016，LAMP/EPFL
- en: For Maven, we check `"$_mvn" --version 2>&1 | awk -F ' ' '/Apache Maven/ {print
    $3}'`, as Maven prints out the following (and more!) when asked for its version: Apache
    Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T00:58:13-07:00)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Maven，我们检查`"$_mvn" --version 2>&1 | awk -F ' ' '/Apache Maven/ {print $3}'`，因为当要求其版本时，Maven会打印以下内容（等等！）：Apache
    Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T00:58:13-07:00)
- en: If you want to learn more, you should now be able to read the other functions
    with ease.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多，现在应该能够轻松阅读其他函数。
- en: There's more...
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: If any of your dependencies are not installed, you need to install them before
    continuing with the next recipe. It goes beyond the scope of this book to guide
    you step-by-step through the installation process of all of these, but here are
    some helpful links to show you how to do it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的任何依赖项尚未安装，您需要在继续下一个配方之前安装它们。本书的范围超出了逐步指导您完成所有这些安装过程的范围，但是以下是一些有用的链接，以指导您如何执行此操作。
- en: Installing Java
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Java
- en: Installing Java is pretty straightforward.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Java非常简单。
- en: On macOS, go to [https://www.java.com/en/download/mac_download.jsp](https://www.java.com/en/download/mac_download.jsp)
    and download the version appropriate for your system. Once downloaded, follow
    the instructions to install it on your machine. If you require more detailed instructions,
    check this link: [http://bit.ly/2idEozX](http://bit.ly/2idEozX).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在macOS上，转到[https://www.java.com/en/download/mac_download.jsp](https://www.java.com/en/download/mac_download.jsp)并下载适合您系统的版本。下载后，请按照说明在您的机器上安装它。如果您需要更详细的说明，请查看此链接：[http://bit.ly/2idEozX](http://bit.ly/2idEozX)。
- en: On Linux, check the following link [http://bit.ly/2jGwuz1](http://bit.ly/2jGwuz1) for
    Linux Java installation instructions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux上，检查以下链接[http://bit.ly/2jGwuz1](http://bit.ly/2jGwuz1)获取Linux Java安装说明。
- en: Installing Python
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Python
- en: We have been using (and highly recommend) the Anaconda version of Python as
    it comes with the most commonly used packages included with the installer. It
    also comes built-in with the `conda` package management tool that makes installing
    other packages a breeze.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在使用（并强烈推荐）Anaconda版本的Python，因为它包含了安装程序中包含的最常用的软件包。它还内置了`conda`软件包管理工具，使安装其他软件包变得轻而易举。
- en: You can download Anaconda from [http://www.continuum.io/downloads](http://www.continuum.io/downloads);
    select the appropriate version that will fulfill Spark's requirements. For macOS
    installation instructions, you can go to [http://bit.ly/2zZPuUf](http://bit.ly/2zZPuUf)
    and for a Linux installation manual check, you can go to [http://bit.ly/2ASLUvg](http://bit.ly/2ASLUvg).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[http://www.continuum.io/downloads](http://www.continuum.io/downloads)下载Anaconda；选择适合Spark要求的版本。有关macOS安装说明，您可以访问[http://bit.ly/2zZPuUf](http://bit.ly/2zZPuUf)，有关Linux安装手册，请访问[http://bit.ly/2ASLUvg](http://bit.ly/2ASLUvg)。
- en: Installing R
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装R
- en: 'R is distributed via **Comprehensive R Archive Network** (**CRAN**). The macOS
    version can be downloaded from here, [https://cran.r-project.org/bin/macosx/](https://cran.r-project.org/bin/macosx/),
    whereas the Linux one is available here: [https://cran.r-project.org/bin/linux/](https://cran.r-project.org/bin/linux/).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: R通过**Comprehensive R Archive Network** (**CRAN**)分发。macOS版本可以从这里下载，[https://cran.r-project.org/bin/macosx/](https://cran.r-project.org/bin/macosx/)，而Linux版本可以从这里下载：[https://cran.r-project.org/bin/linux/](https://cran.r-project.org/bin/linux/)。
- en: Download the version appropriate for your machine and follow the installation
    instructions on the screen. For the macOS version, you can choose to install just
    the R core packages without the GUI and everything else as Spark does not require
    those.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下载适合你的机器版本，并按照屏幕上的安装说明进行安装。对于macOS版本，你可以选择仅安装R核心包而不安装GUI和其他内容，因为Spark不需要这些。
- en: Installing Scala
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Scala
- en: Installing Scala is even simpler.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Scala甚至更简单。
- en: 'Go to [http://bit.ly/2Am757R](http://bit.ly/2Am757R) and download the `sbt-*.*.*.tgz` archive
    (at the time of writing this book, the latest version is `sbt-1.0.4.tgz`). Next,
    in your Terminal, navigate to the folder you have just downloaded Scala to and
    issue the following commands:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 前往[http://bit.ly/2Am757R](http://bit.ly/2Am757R)并下载`sbt-*.*.*.tgz`存档（在撰写本书时，最新版本是`sbt-1.0.4.tgz`）。接下来，在你的终端中，导航到你刚下载Scala的文件夹，并输入以下命令：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That's it. Now, you can skip to the *Updating PATH* section in this recipe to
    update your `PATH`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。现在，你可以跳到本教程中的*更新PATH*部分来更新你的`PATH`。
- en: Installing Maven
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Maven
- en: 'Maven''s installation is quite similar to that of Scala. Go to [https://maven.apache.org/download.cgi](https://maven.apache.org/download.cgi)
    and download the `apache-maven-*.*.*-bin.tar.gz` archive. At the time of writing
    this book, the newest version was 3.5.2\. Similarly to Scala, open the Terminal,
    navigate to the folder you have just downloaded the archive to, and type:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Maven的安装与Scala的安装非常相似。前往[https://maven.apache.org/download.cgi](https://maven.apache.org/download.cgi)并下载`apache-maven-*.*.*-bin.tar.gz`存档。在撰写本书时，最新版本是3.5.2。与Scala类似，打开终端，导航到刚下载存档的文件夹，并键入：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Once again, that is it for what you need to do with regards to installing Maven.
    Check the next subsection for instructions on how to update your `PATH`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你需要做的关于安装Maven的事情。查看下一小节，了解如何更新你的`PATH`。
- en: Updating PATH
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新PATH
- en: Unix-like operating systems (Windows, too) use the concept of a `PATH` to search
    for binaries (or executables, in the case of Windows). The `PATH` is nothing more
    than a list of folders separated by the colon character `':'` that tells the operating
    system where to look for binaries.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 类Unix操作系统（包括Windows）使用`PATH`的概念来搜索二进制文件（或在Windows的情况下是可执行文件）。`PATH`只是一个由冒号字符`':'`分隔的文件夹列表，告诉操作系统在哪里查找二进制文件。
- en: 'To add something to your `PATH` (and make it a permanent change), you need
    to edit either the `.bash_profile` (macOS) or `.bashrc` (Linux) files; these are
    located in the root folder for your user. Thus, to add both Scala and Maven binaries
    to the PATH, you can do the following (on macOS):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要将某些内容添加到你的`PATH`（并使其成为永久更改），你需要编辑`.bash_profile`（macOS）或`.bashrc`（Linux）文件；这些文件位于你的用户根文件夹中。因此，要将Scala和Maven二进制文件添加到PATH，你可以执行以下操作（在macOS上）：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'On Linux, the equivalent looks as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux上，等价的代码如下：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The preceding commands simply append to the end of either of the `.bash_profile`
    or `.bashrc` files using the redirection operator `>>`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令只是使用重定向运算符`>>`将内容追加到`.bash_profile`或`.bashrc`文件的末尾。
- en: 'Once you execute the preceding commands, restart your Terminal, and:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述命令后，重新启动你的终端，并：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It should now include paths to both the Scala and Maven binaries.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该包括Scala和Maven二进制文件的路径。
- en: Installing Spark from sources
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从源代码安装Spark
- en: 'Spark is distributed in two ways: either as precompiled binaries or as a source
    code that gives you the flexibility to choose, for example, whether you need support
    for Hive or not. In this recipe, we will focus on the latter.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Spark以两种方式分发：作为预编译的二进制文件或作为源代码，让你可以选择是否需要支持Hive等。在这个教程中，我们将专注于后者。
- en: Getting ready
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a bash Terminal and an internet connection.
    Also, to follow through with this recipe, you will have to have already checked
    and/or installed all the required environments we went through in the previous
    recipe. In addition, you need to have administrative privileges (via the `sudo`
    command) which will be necessary to move the compiled binaries to the destination
    folder.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个教程，你需要一个bash终端和一个互联网连接。此外，为了完成这个教程，你必须已经检查和/或安装了我们在上一个教程中提到的所有必需的环境。此外，你需要有管理员权限（通过`sudo`命令），这将是将编译后的二进制文件移动到目标文件夹所必需的。
- en: If you are not an administrator on your machine, you can call the script with
    the `-ns` (or `--nosudo`) parameter. The destination folder will then switch to
    your home directory and will create a `spark` folder within it. By default, the
    binaries will be moved to the `/opt/spark` folder and that's why you need administrative
    rights.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不是机器上的管理员，可以使用`-ns`（或`--nosudo`）参数调用脚本。目标文件夹将切换到你的主目录，并在其中创建一个`spark`文件夹。默认情况下，二进制文件将移动到`/opt/spark`文件夹，这就是为什么你需要管理员权限的原因。
- en: No other prerequisites are required.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'There are five major steps we will undertake to install Spark from sources
    (check the highlighted portions of the code):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行五个主要步骤来从源代码安装Spark（检查代码的突出部分）：
- en: Download the sources from Spark's website
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Spark的网站下载源代码
- en: Unpack the archive
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压缩存档
- en: Build
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建
- en: Move to the final destination
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移动到最终目的地
- en: Create the necessary environmental variables
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建必要的环境变量
- en: 'The skeleton for our code looks as follows (see the `Chapter01/installFromSource.sh`
    file):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码框架如下（参见`Chapter01/installFromSource.sh`文件）：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works...
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'First, we specify the location of Spark''s source code. The `_spark_archive`
    contains the name of the archive; we use `awk` to extract the last element (here,
    it is specified by the `$NF` flag) from the `_spark_source`. The `_spark_dir`
    contains the name of the directory our archive will unpack into; in our current
    case, this will be `spark-2.3.1`. Finally, we specify our destination folder where
    we will be going to move the binaries to: it will either be `/opt/spark` (default)
    or your home directory if you use the `-ns` (or `--nosudo`) switch when calling
    the `./installFromSource.sh` script.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们指定Spark源代码的位置。`_spark_archive`包含存档的名称；我们使用`awk`从`_spark_source`中提取最后一个元素（在这里，由`$NF`标志指定）。`_spark_dir`包含我们的存档将解压缩到的目录的名称；在我们当前的情况下，这将是`spark-2.3.1`。最后，我们指定我们将要移动二进制文件的目标文件夹：它要么是`/opt/spark`（默认值），要么是您的主目录，如果在调用`./installFromSource.sh`脚本时使用了`-ns`（或`--nosudo`）开关。
- en: 'Next, we check the OS name we are using:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查我们正在使用的操作系统名称：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'First, we get the short name of the operating system using the `uname` command;
    the `-s` switch returns a shortened version of the OS name. As mentioned earlier,
    we only focus on two operating systems: macOS and Linux, so if you try to run
    this script on Windows or any other system, it will stop. This portion of the
    code is necessary to set the `_machine` flag properly: macOS and Linux use different
    methods to download the Spark source codes and different bash profile files to
    set the environment variables.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`uname`命令获取操作系统的简短名称；`-s`开关返回操作系统名称的缩写版本。如前所述，我们只关注两个操作系统：macOS和Linux，因此，如果您尝试在Windows或任何其他系统上运行此脚本，它将停止。代码的这一部分是必要的，以正确设置`_machine`标志：macOS和Linux使用不同的方法来下载Spark源代码和不同的bash配置文件来设置环境变量。
- en: 'Next, we print out the header (we will skip the code for this part here, but
    you are welcome to check the `Chapter01/installFromSource.sh` script). Following
    this, we download the necessary source codes:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打印出标题（我们将跳过此部分的代码，但欢迎您检查`Chapter01/installFromSource.sh`脚本）。在此之后，我们下载必要的源代码：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: First, we check whether a `_temp` folder exists and, if it does, we delete it.
    Next, we recreate an empty `_temp` folder and download the sources into it; on
    macOS, we use the `curl` method while on Linux, we use `wget` to download the
    sources.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查`_temp`文件夹是否存在，如果存在，则删除它。接下来，我们重新创建一个空的`_temp`文件夹，并将源代码下载到其中；在macOS上，我们使用`curl`方法，而在Linux上，我们使用`wget`来下载源代码。
- en: Did you notice the ellipsis `'...'` character in our code? Whenever we use such
    a character, we omit some less relevant or purely informational portions of the
    code. They are still present, though, in the sources checked into the GitHub repository.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到我们的代码中的省略号`'...'`字符了吗？每当我们使用这样的字符时，我们省略了一些不太相关或纯粹信息性的代码部分。但是，它们仍然存在于GitHub存储库中检查的源代码中。
- en: Once the sources land on our machine, we unpack them using the `tar` tool, `tar
    -xf $_spark_archive`. This happens inside the `unpack` function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦源代码落在我们的机器上，我们就使用`tar`工具解压它们，`tar -xf $_spark_archive`。这发生在`unpack`函数内部。
- en: 'Finally, we can start building the sources into binaries:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以开始将源代码构建成二进制文件：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We use the `make-distribution.sh` script (distributed with Spark) to create
    our own Spark distribution, named `pyspark-cookbook`. The previous command will
    build the Spark distribution for Hadoop 2.7 and with Hive support. We will also
    be able to deploy it over YARN. Underneath the hood, the `make-distribution.sh`
    script is using Maven to compile the sources.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`make-distribution.sh`脚本（与Spark一起分发）来创建我们自己的Spark分发，名为`pyspark-cookbook`。上一个命令将为Hadoop
    2.7构建Spark分发，并支持Hive。我们还可以在YARN上部署它。在幕后，`make-distribution.sh`脚本正在使用Maven来编译源代码。
- en: 'Once the compilation finishes, we need to move the binaries to the `_spark_destination`
    folder:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 编译完成后，我们需要将二进制文件移动到`_spark_destination`文件夹：
- en: '[PRE34]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: First, we check if the folder in the destination exists and, if it does, we
    remove it. Next, we simply move (`mv`) the `$_spark_dir` folder to its new home.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查目标文件夹中是否存在该文件夹，如果存在，我们将其删除。接下来，我们简单地将`$_spark_dir`文件夹移动到其新位置。
- en: This is when you will need to type in the password if you did not use the `-ns`
    (or `--nosudo`) flag when invoking the `installFromSource.sh` script.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是当您在调用`installFromSource.sh`脚本时没有使用`-ns`（或`--nosudo`）标志时，您将需要输入密码的时候。
- en: 'One of the last steps is to add new environment variables to your bash profile
    file:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步之一是向您的bash配置文件添加新的环境变量：
- en: '[PRE38]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'First, we check what OS system we''re on and select the appropriate bash profile
    file. We also grab the current date (the `_today` variable) so that we can include
    that information in our bash profile file, and create its safe copy (just in case,
    and if one does not already exist). Next, we start to append new lines to the
    bash profile file:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查我们所在的操作系统，并选择适当的bash配置文件。我们还获取当前日期（`_today`变量），以便在我们的bash配置文件中包含该信息，并创建其安全副本（以防万一，如果尚不存在）。接下来，我们开始向bash配置文件追加新行：
- en: We first set the `SPARK_HOME` variable to the `_spark_destination`; this is
    either going to be the `/opt/spark` or `~/spark` location.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先将`SPARK_HOME`变量设置为`_spark_destination`；这要么是`/opt/spark`，要么是`~/spark`的位置。
- en: The `PYSPARK_SUBMIT_ARGS` variable is used when you invoke `pyspark`. It instructs
    Spark to use four cores of your CPU; changing it to `--master local[*]` will use
    all the available cores.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在调用`pyspark`时，`PYSPARK_SUBMIT_ARGS`变量用于指示Spark使用您CPU的四个核心；将其更改为`--master local[*]`将使用所有可用的核心。
- en: We specify the `PYSPARK_PYTHON` variable so, in case of multiple Python installations
    present on the machine, `pyspark` will use the one that we checked for in the
    first recipe.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们指定`PYSPARK_PYTHON`变量，以便在机器上存在多个Python安装时，`pyspark`将使用我们在第一个配方中检查的那个。
- en: Setting the `PYSPARK_DRIVER_PYTHON` to `jupyter` will start a Jupyter session
    (instead of the PySpark interactive shell).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`PYSPARK_DRIVER_PYTHON`设置为`jupyter`将启动Jupyter会话（而不是PySpark交互式shell）。
- en: 'The `PYSPARK_DRIVER_PYTHON_OPS` instructs Jupyter to:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PYSPARK_DRIVER_PYTHON_OPS`指示Jupyter：'
- en: Start a `notebook`
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始一个`笔记本`
- en: 'Do not open the browser by default: use the `--NotebookApp.open_browser=False`
    flag'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要默认打开浏览器：使用`--NotebookApp.open_browser=False`标志
- en: Change the default port (`8888`) to `6661` (because we are big fans of not having
    things at default for safety reasons)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将默认端口（`8888`）更改为`6661`（因为我们非常喜欢出于安全原因而不使用默认设置）
- en: Finally, we add the `bin` folder from `SPARK_HOME` to the `PATH`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将`SPARK_HOME`中的`bin`文件夹添加到`PATH`中。
- en: The last step is to `cleanUp` after ourselves; we simply remove the `_temp`
    folder with everything in it.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是在完成后进行`cleanUp`；我们只需删除`_temp`文件夹及其中的所有内容。
- en: 'Now that we have installed Spark, let''s test if everything works. First, in
    order to make all the environment variables accessible in the Terminal''s session,
    we need to refresh the `bash` session: you can either close and reopen the Terminal,
    or execute the following command (on macOS):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了Spark，让我们测试一下是否一切正常。首先，为了使所有环境变量在终端会话中可访问，我们需要刷新`bash`会话：您可以关闭并重新打开终端，或者在macOS上执行以下命令：
- en: '[PRE44]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'On Linux, execute the following command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux上，执行以下命令：
- en: '[PRE45]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, you should be able to execute the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您应该能够执行以下操作：
- en: '[PRE46]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'If all goes well, you should see a response similar to the one shown in the
    following screenshot:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，您应该看到类似于以下截图的响应：
- en: '![](img/00007.jpeg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00007.jpeg)'
- en: There's more...
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Instead of using the `make-distribution.sh` script from Spark, you can use
    Maven directly to compile the sources. For instance, if you wanted to build the
    default version of Spark, you could simply type (from the `_spark_dir` folder):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 不使用Spark的`make-distribution.sh`脚本，您可以直接使用Maven编译源代码。例如，如果您想构建Spark的默认版本，只需在`_spark_dir`文件夹中键入：
- en: '[PRE47]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This would default to Hadoop 2.6\. If your version of Hadoop was 2.7.2 and
    was deployed over YARN, you can do the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这将默认为Hadoop 2.6。如果您的Hadoop版本为2.7.2并且已部署在YARN上，则可以执行以下操作：
- en: '[PRE48]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You can also use Scala to build Spark:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用Scala构建Spark：
- en: '[PRE49]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: See also
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: If you want to study more on how to build and/or enable certain features of
    Spark, check Spark's website: [http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html)
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想进一步学习如何构建和/或启用Spark的某些功能，请查看Spark的网站：[http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html)
- en: Installing Spark from binaries
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从二进制文件安装Spark
- en: Installing Spark from already precompiled binaries is even easier than doing
    the same from the sources. In this recipe, we will show you how to do this by
    downloading the binaries from the web or by using `pip`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 从预编译的二进制文件安装Spark甚至比从源代码进行相同操作更容易。在本教程中，我们将向您展示如何通过从网上下载二进制文件或使用`pip`来实现这一点。
- en: Getting ready
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a bash Terminal and an internet connection.
    Also, to follow through with this recipe, you will need to have already checked
    and/or installed all the required environments we went through in the *Installing
    Spark requirements* recipe. In addition, you need to have administrative privileges
    (via the `sudo` command), as these will be necessary to move the compiled binaries
    to the destination folder.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此教程，您需要一个bash终端和互联网连接。此外，为了完成此教程，您需要已经检查和/或安装了我们在*安装Spark要求*教程中介绍的所有必需环境。此外，您需要具有管理权限（通过`sudo`命令），因为这将是将编译后的二进制文件移动到目标文件夹所必需的。
- en: If you are not an administrator on your machine, you can call the script with
    the `-ns` (or `--nosudo`) parameter. The destination folder will then switch to
    your home directory and will create a `spark` folder within it; by default, the
    binaries will be moved to the `/opt/spark` folder and that's why you need administrative
    rights.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不是计算机上的管理员，可以使用`-ns`（或`--nosudo`）参数调用脚本。目标文件夹将切换到您的主目录，并在其中创建一个`spark`文件夹；默认情况下，二进制文件将移动到`/opt/spark`文件夹，因此您需要管理权限。
- en: No other prerequisites are required.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To install from the binaries, we only need four steps (see the following source
    code) as we do not need to compile the sources:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要从二进制文件安装，我们只需要四个步骤（请参阅以下源代码），因为我们不需要编译源代码：
- en: Download the precompiled binaries from Spark's website.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Spark的网站下载预编译的二进制文件。
- en: Unpack the archive.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压缩存档。
- en: Move to the final destination.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移动到最终目的地。
- en: Create the necessary environmental variables.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建必要的环境变量。
- en: 'The skeleton for our code looks as follows (see the `Chapter01/installFromBinary.sh`
    file):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码框架如下（请参阅`Chapter01/installFromBinary.sh`文件）：
- en: '[PRE50]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: How it works...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The code is exactly the same as with the previous recipe so we will not be repeating
    it here; the only major difference is that we do not have the `build` stage in
    this script, and the `_spark_source` variable is different.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 代码与上一个教程完全相同，因此我们不会在此重复；唯一的主要区别是在此脚本中我们没有`build`阶段，并且`_spark_source`变量不同。
- en: 'As in the previous recipe, we start by specifying the location of Spark''s
    source code, which is in `_spark_source`. The `_spark_archive` contains the name
    of the archive; we use `awk` to extract the last element. The `_spark_dir` contains
    the name of the directory our archive will unpack into; in our current case, this
    will be `spark-2.3.1`. Finally, we specify our destination folder where we will
    be moving the binaries to: it will either be `/opt/spark` (default) or your home
    directory if you use the `-ns` (or `--nosudo`) switch when calling the `./installFromBinary.sh`
    script.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一个教程一样，我们首先指定Spark源代码的位置，即`_spark_source`。`_spark_archive`包含存档的名称；我们使用`awk`来提取最后一个元素。`_spark_dir`包含我们的存档将解压缩到的目录的名称；在我们当前的情况下，这将是`spark-2.3.1`。最后，我们指定我们将移动二进制文件的目标文件夹：它将是`/opt/spark`（默认）或者如果您在调用`./installFromBinary.sh`脚本时使用了`-ns`（或`--nosudo`）开关，则是您的主目录。
- en: 'Next, we check the OS name. Depending on whether you work in a Linux or macOS
    environment, we will use different tools to download the archive from the internet
    (check the `downloadThePackage` function). Also, when setting up the environment
    variables, we will output to different bash profile files: the `.bash_profile`
    on macOS and the `.bashrc` on Linux (check the `setEnvironmentVariables` function).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查操作系统名称。根据您是在Linux还是macOS环境中工作，我们将使用不同的工具从互联网下载存档（检查`downloadThePackage`函数）。此外，在设置环境变量时，我们将输出到不同的bash配置文件：macOS上的`.bash_profile`和Linux上的`.bashrc`（检查`setEnvironmentVariables`函数）。
- en: 'Following the OS check, we download the package: on macOS, we use `curl` and
    on Linux, we use `wget` tools to attain this goal. Once the package is downloaded,
    we unpack it using the `tar` tool, and then move it to its destination folder.
    If you are running with `sudo` privileges (without the `-ns` or `--nosudo` parameters),
    the binaries will be moved to the `/opt/spark` folder; if not—they will end up
    in the `~/spark` folder.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行操作系统检查后，我们下载软件包：在macOS上，我们使用`curl`，在Linux上，我们使用`wget`工具来实现这个目标。软件包下载完成后，我们使用`tar`工具解压缩，然后将其移动到目标文件夹。如果您具有`sudo`权限（没有`-ns`或`--nosudo`参数），则二进制文件将移动到`/opt/spark`文件夹；否则，它们将放在`~/spark`文件夹中。
- en: 'Finally, we add environment variables to the appropriate bash profile files:
    check the previous recipe for an explanation of what is being added and for what
    reason. Also, follow the steps at the end of the previous recipe to test if your
    environment is working properly.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将环境变量添加到适当的bash配置文件中：查看前一个教程以了解添加的内容及原因。同时，按照前一个教程的步骤测试您的环境是否正常工作。
- en: There's more...
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Nowadays, there is an even simpler way to install PySpark on your machine, that
    is, by using pip.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，在您的计算机上安装PySpark的方法更加简单，即使用pip。
- en: '`pip` is Python''s package manager. If you installed Python 2.7.9 or Python
    3.4 from [http://python.org](http://python.org), then `pip` is already present
    on your machine (the same goes for our recommended Python distribution—Anaconda).
    If you do not have `pip`, you can easily install it from here:  [https://pip.pypa.io/en/stable/installing/](https://pip.pypa.io/en/stable/installing/).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip`是Python的软件包管理器。如果您从[http://python.org](http://python.org)安装了Python 2.7.9或Python
    3.4，则`pip`已经存在于您的计算机上（我们推荐的Python发行版Anaconda也是如此）。如果您没有`pip`，可以从这里轻松安装它：[https://pip.pypa.io/en/stable/installing/](https://pip.pypa.io/en/stable/installing/)。'
- en: 'To install PySpark via `pip`, just issue the following command in the Terminal:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过`pip`安装PySpark，只需在终端中输入以下命令：
- en: '[PRE56]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Or, if you use Python 3.4+, you may also try:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您使用Python 3.4+，也可以尝试：
- en: '[PRE57]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'You should see the following screen in your Terminal:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在终端中看到以下屏幕：
- en: '![](img/00008.jpeg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00008.jpeg)'
- en: Configuring a local instance of Spark
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置本地Spark实例
- en: There is actually not much you need to do to configure a local instance of Spark.
    The beauty of Spark is that all you need to do to get started is to follow either
    of the previous two recipes (installing from sources or from binaries) and you
    can begin using it. In this recipe, however, we will walk you through the most
    useful `SparkSession` configuration options.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，配置本地Spark实例并不需要做太多事情。Spark的美妙之处在于，您只需要按照之前的两种方法之一（从源代码或二进制文件安装），就可以开始使用它。但是，在本教程中，我们将为您介绍最有用的`SparkSession`配置选项。
- en: Getting ready
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order to follow this recipe, a working Spark environment is required. This
    means that you will have to have gone through the previous three recipes and have
    successfully installed and tested your environment, or had a working Spark environment
    already set up.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了按照本教程，需要一个可用的Spark环境。这意味着您必须已经完成了前三个教程，并成功安装和测试了您的环境，或者已经设置了一个可用的Spark环境。
- en: No other prerequisites are necessary.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To configure your session, in a Spark version which is lower that version 2.0,
    you would normally have to create a `SparkConf` object, set all your options to
    the right values, and then build the `SparkContext` ( `SqlContext` if you wanted
    to use `DataFrames`, and `HiveContext` if you wanted access to Hive tables). Starting
    from Spark 2.0, you just need to create a `SparkSession`, just like in the following
    snippet:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置您的会话，在低于2.0版本的Spark版本中，通常需要创建一个`SparkConf`对象，将所有选项设置为正确的值，然后构建`SparkContext`（如果要使用`DataFrames`，则为`SqlContext`，如果要访问Hive表，则为`HiveContext`）。从Spark
    2.0开始，您只需要创建一个`SparkSession`，就像以下代码片段中一样：
- en: '[PRE58]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: How it works...
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'To create a `SparkSession`, we will use the `Builder` class (accessed via the
    `.builder` property of the `SparkSession` class). You can specify some basic properties
    of the `SparkSession` here:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个`SparkSession`，我们将使用`Builder`类（通过`SparkSession`类的`.builder`属性访问）。您可以在这里指定`SparkSession`的一些基本属性：
- en: The `.master(...)` allows you to specify the driver node (in our preceding example,
    we would be running a local session with two cores)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.master(...)`允许您指定驱动节点（在我们之前的示例中，我们将使用两个核心运行本地会话）'
- en: The `.appName(...)` gives you means to specify a friendly name for your app
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.appName(...)`允许您为您的应用程序指定友好的名称'
- en: The `.config(...)` method allows you to refine your session's behavior further;
    the list of the most important `SparkSession` parameters is outlined in the following
    table
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.config(...)`方法允许您进一步完善会话的行为；最重要的`SparkSession`参数列表在下表中概述'
- en: The `.getOrCreate()` method returns either a new `SparkSession` if one has not
    been created yet, or returns a pointer to an already existing `SparkSession`
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.getOrCreate()`方法返回一个新的`SparkSession`（如果尚未创建），或者返回指向已经存在的`SparkSession`的指针'
- en: 'The following table gives an example list of the most useful configuration
    parameters for a local instance of Spark:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了本地Spark实例的最有用的配置参数示例列表：
- en: Some of these parameters are also applicable if you are working in a cluster
    environment with multiple worker nodes. In the next recipe, we will explain how
    to set up and administer a multi-node Spark cluster deployed over YARN.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在具有多个工作节点的集群环境中工作，这些参数也适用。在下一个教程中，我们将解释如何设置和管理部署在YARN上的多节点Spark集群。
- en: '| **Parameter** | **Function** | **Default** |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **功能** | **默认** |'
- en: '| `spark.app.name` | Specifies a friendly name for your application | (none)
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `spark.app.name` | 指定应用程序的友好名称 | (无) |'
- en: '| `spark.driver.cores` | Number of cores for the driver node to use. This is
    only applicable for app deployments in a cluster mode (see the following `spark.submit.deployMode` parameter).
    | 1 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `spark.driver.cores` | 驱动节点要使用的核心数。这仅适用于集群模式下的应用程序部署（参见下面的`spark.submit.deployMode`参数）。
    | 1 |'
- en: '| `spark.driver.memory` | Specifies the amount of memory for the driver process.
    If using `spark-submit` in client mode, you should specify this in a command line
    using `--driver-memory` switch rather than configuring your session using this
    parameter as JVM would have already started at this point. | 1g |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `spark.driver.memory` | 指定驱动程序进程的内存量。如果在客户端模式下使用`spark-submit`，您应该在命令行中使用`--driver-memory`开关来指定这个参数，而不是在配置会话时使用这个参数，因为JVM在这一点上已经启动了。
    | 1g |'
- en: '| `spark.executor.cores` | Number of cores for an executor to use. Setting
    this parameter while running locally allows you to use all the available cores
    on your machine. | 1 in YARN deployment, all available cores on the worker in
    standalone and Mesos deployments |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `spark.executor.cores` | 每个执行器要使用的核心数。在本地运行时设置此参数允许您使用机器上所有可用的核心。 | YARN部署中为1，在独立和Mesos部署中为工作节点上的所有可用核心
    |'
- en: '| `spark.executor.memory` | Specifies the amount of memory per each executor
    process. | 1g |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `spark.executor.memory` | 指定每个执行器进程的内存量。 | 1g |'
- en: '| `spark.submit.pyFiles` | List of `.zip`, `.egg`, or `.py` files, separated
    by commas. These will be added to the `PYTHONPATH` so that they are accessible
    for Python apps. | (none) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `spark.submit.pyFiles` | 以逗号分隔的`.zip`、`.egg`或`.py`文件列表。这些文件将被添加到`PYTHONPATH`中，以便Python应用程序可以访问它们。
    | (无) |'
- en: '| `spark.submit.deployMode` | Deploy mode of the Spark driver program. Specifying
    `''client''` will launch the driver program locally on the machine (it can be
    the driver node), while specifying `''cluster''` will utilize one of the nodes
    on a remote cluster. | (none) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `spark.submit.deployMode` | Spark驱动程序程序的部署模式。指定`''client''`将在本地（可以是驱动节点）启动驱动程序程序，而指定`''cluster''`将利用远程集群上的一个节点。
    | (无) |'
- en: '| `spark.pyspark.python` | Python binary that should be used by the driver
    and all the executors. | (none) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `spark.pyspark.python` | 驱动程序和所有执行器应该使用的Python二进制文件。 | (无) |'
- en: There are some environment variables that also allow you to further fine-tune
    your Spark environment. Specifically, we are talking about the `PYSPARK_DRIVER_PYTHON`
    and `PYSPARK_DRIVER_PYTHON_OPTS` variables. We have already covered these in the
    *Installing Spark from sources* recipe.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些环境变量可以让您进一步微调您的Spark环境。具体来说，我们正在谈论`PYSPARK_DRIVER_PYTHON`和`PYSPARK_DRIVER_PYTHON_OPTS`变量。我们已经在*从源代码安装Spark*教程中介绍过这些内容。
- en: See also
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'Check the full list of all available configuration options here: [https://spark.apache.org/docs/latest/configuration.html](https://spark.apache.org/docs/latest/configuration.html)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里检查所有可用配置选项的完整列表：[https://spark.apache.org/docs/latest/configuration.html](https://spark.apache.org/docs/latest/configuration.html)
- en: Configuring a multi-node instance of Spark
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Spark的多节点实例
- en: Setting up a multi-node Spark cluster requires quite a few more steps to get
    it ready. In this recipe, we will go step-by-step through the script that will
    help you with this process; the script needs to run on the driver node and all
    the executors to set up the environment.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个多节点Spark集群需要做更多的准备工作。在这个教程中，我们将逐步介绍一个脚本，该脚本将帮助您完成此过程；该脚本需要在驱动节点和所有执行器上运行以设置环境。
- en: Getting ready
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we are solely focusing on a Linux environment (we are using
    Ubuntu Server 16.04 LTS). The following prerequisites are required before you
    can follow with the rest of the recipe:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们只关注Linux环境（我们使用的是Ubuntu Server 16.04 LTS）。在您继续进行下一步之前，需要满足以下先决条件：
- en: A clean installation of a Linux distribution; in our case, we have installed
    Ubuntu Server 16.04 LTS on each machine in our cluster of three Dell R710s.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 干净安装Linux发行版；在我们的情况下，我们在我们的三台Dell R710机器上都安装了Ubuntu Server 16.04 LTS。
- en: Each machine needs to be connected to the internet and accessible from your
    local machine. You will need the machines' IPs and their hostnames; on Linux,
    you can check the IP by issuing the `ifconfig` command and reading the `inet addr`.
    To check your hostname, type at `cat/etc/hostname`.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每台机器都需要连接到互联网，并且可以从本地机器访问。您需要机器的IP和主机名；在Linux上，您可以通过发出`ifconfig`命令并阅读`inet addr`来检查IP。要检查您的主机名，请在`cat/etc/hostname`处输入。
- en: On each server, we added a user group called `hadoop`. Following this, we have
    created a user called `hduser` and added it to the `hadoop` group. Also, make
    sure that the `hduser` has `sudo` rights. If you do not know how to do this, check
    the *See also* section of this recipe.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每台服务器上，我们添加了一个名为`hadoop`的用户组。在此之后，我们创建了一个名为`hduser`的用户，并将其添加到`hadoop`组中。还要确保`hduser`具有`sudo`权限。如果您不知道如何做到这一点，请查看本教程的*参见*部分。
- en: Make sure you have added the ability to reach your servers via SSH. If you cannot
    do this, run `sudo apt-get install openssh-server openssh-client` on each server
    to install the necessary environments.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您已经添加了通过SSH访问服务器的能力。如果无法做到这一点，请在每台服务器上运行`sudo apt-get install openssh-server
    openssh-client`来安装必要的环境。
- en: If you want to read and write to Hadoop and Hive, you need to have these two
    environments installed and configured on your cluster. Check [https://data-flair.training/blogs/install-hadoop-2-x-on-ubuntu/](https://data-flair.training/blogs/install-hadoop-2-x-on-ubuntu/) for
    Hadoop installation and configuration and [http://www.bogotobogo.com/Hadoop/BigData_hadoop_Hive_Install_On_Ubuntu_16_04.php](http://www.bogotobogo.com/Hadoop/BigData_hadoop_Hive_Install_On_Ubuntu_16_04.php)
    for Hive.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想要读写Hadoop和Hive，你需要在你的集群上安装和配置这两个环境。查看[Hadoop安装和配置](https://data-flair.training/blogs/install-hadoop-2-x-on-ubuntu/)和[Hive](http://www.bogotobogo.com/Hadoop/BigData_hadoop_Hive_Install_On_Ubuntu_16_04.php)。
- en: If you have these two environments set up, some of the steps from our script
    would be obsolete. However, we will present all of the steps as follows, assuming
    you only want the Spark environment.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经设置好了这两个环境，我们脚本中的一些步骤将变得多余。然而，我们将按照以下方式呈现所有步骤，假设你只需要Spark环境。
- en: No other prerequisites are required.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: 'For the purpose of automating the deployment of the Spark environment in a
    cluster setup, you will also have to:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自动部署集群设置中的Spark环境，你还需要：
- en: 'Create a `hosts.txt` file. Each entry on the list is the IP address of one
    of the servers followed by two spaces and a hostname. **Do not delete the `driver:`
    nor `executors:` lines**. Also, note that we only allow one driver in our cluster
    (some clusters support redundant drivers). An example of the content of this file
    is as follows:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`hosts.txt`文件。列表中的每个条目都是一个服务器的IP地址，后面跟着两个空格和一个主机名。**不要删除`driver:`和`executors:`行**。还要注意，我们的集群只允许一个driver（一些集群支持冗余driver）。这个文件的内容示例如下：
- en: '[PRE59]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'On your local machine, add the IPs and hostnames to your `/etc/hosts` file
    so you can access the servers via hostnames instead of IPs (once again, we are
    assuming you are running a Unix-like system such as macOS or Linux). For example,
    the following command will add `pathfinder` to our `/etc/hosts` file: `sudo echo
    192.168.1.160  pathfinder >> /etc/hosts`. Repeat this for all machines from your
    server.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的本地机器上，将IP地址和主机名添加到你的`/etc/hosts`文件中，这样你就可以通过主机名而不是IP地址访问服务器（我们再次假设你正在运行类Unix系统，如macOS或Linux）。例如，以下命令将在我们的`/etc/hosts`文件中添加`pathfinder`：`sudo
    echo 192.168.1.160  pathfinder >> /etc/hosts`。对你的服务器上的所有机器都重复这个步骤。
- en: Copy the `hosts.txt` file to each machine in your cluster; we assume the file
    will be placed in the root folder for the `hduser`. You can attain this easily
    with the `scp hosts.txt hduser@<your-server-name>:~` command, where `<your-server-name>`
    is the hostname of the machine.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`hosts.txt`文件复制到你集群中的每台机器上；我们假设文件将放在`hduser`的根文件夹中。你可以使用`scp hosts.txt hduser@<your-server-name>:~`命令轻松实现这一点，其中`<your-server-name>`是机器的主机名。
- en: To run the `installOnRemote.sh` script (see the `Chapter01/installOnRemote.sh`
    file) from your local machine, do the following: `ssh -tq hduser@<your-server-name>
    "echo $(base64 -i installOnRemote.sh) | base64 -d | sudo bash"`. We will go through
    these steps in detail in the `installOnRemote.sh` script in the next section.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从你的本地机器运行`installOnRemote.sh`脚本（参见`Chapter01/installOnRemote.sh`文件），执行以下操作：`ssh
    -tq hduser@<your-server-name> "echo $(base64 -i installOnRemote.sh) | base64 -d
    | sudo bash"`。我们将在下一节详细介绍`installOnRemote.sh`脚本中的这些步骤。
- en: Follow the prompts on the screen to finalize the installation and configuration
    steps. Repeat step 4 for each machine in your cluster.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照屏幕上的提示完成安装和配置步骤。对于你集群中的每台机器都要重复第4步。
- en: How to do it...
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: The `installOnRemote.sh` script for this recipe can be found in the `Chapter01`
    folder in the GitHub repository: [http://bit.ly/2ArlBck](http://bit.ly/2ArlBck).
    Some portions of the script are very similar to the ones we have outlined in the
    previous recipes, so we will skip those; you can refer to previous recipes for
    more information (especially the *Installing Spark requirements* and the *Installing
    Spark from binaries* recipes).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的`installOnRemote.sh`脚本可以在GitHub存储库的`Chapter01`文件夹中找到：[http://bit.ly/2ArlBck](http://bit.ly/2ArlBck)。脚本的一些部分与我们在之前的步骤中概述的部分非常相似，因此我们将跳过这些部分；你可以参考之前的步骤获取更多信息（特别是*安装Spark要求*和*从二进制文件安装Spark*的步骤）。
- en: 'The top-level structure of the script is as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的顶层结构如下：
- en: '[PRE61]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: We have highlighted the portions of the script that are more relevant to this
    recipe in bold font.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经用粗体字突出显示了与本节相关的脚本部分。
- en: How it works...
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: As with the previous recipes, we will first specify where we are going to download
    the Spark binaries from and create all the relevant global variables we are going
    to use later.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的步骤一样，我们首先要指定从哪里下载Spark二进制文件，并创建我们稍后要使用的所有相关全局变量。
- en: 'Next, we read in the `hosts.txt` file:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们读取`hosts.txt`文件：
- en: '[PRE69]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We store the path to the file in the `input` variable. The `driver` and the
    `executors` variables are flags we use to skip the `"driver:"` and the `"executors:"`
    lines from the input file. The `_executors` empty string will store the list of
    executors, which are delimited by a newline `"\n"`.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将文件路径存储在`input`变量中。`driver`和`executors`变量是我们用来跳过输入文件中的`"driver:"`和`"executors:"`行的标志。`_executors`空字符串将存储执行者的列表，这些列表由换行符`"\n"`分隔。
- en: '**IFS** stands for **internal field separator**. Whenever `bash` reads a line
    from a file, it will split it on that character. Here, we will set it to an empty
    character `''''` so that we preserve the double spaces between the IP address
    and the hostname.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**IFS**代表**内部字段分隔符**。每当`bash`从文件中读取一行时，它将根据该字符进行分割。在这里，我们将其设置为空字符`''''`，以便保留IP地址和主机名之间的双空格。'
- en: 'Next, we start reading the file, line-by-line. Let''s see how the logic works
    inside the loop; we''ll start a bit out of order so that the logic is easier to
    understand:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们逐行读取文件。让我们看看循环内部的逻辑是如何工作的；我们将有点乱序开始，以便逻辑更容易理解：
- en: If the `line` we just read equals to `"driver:"` (the `if [[ "$line" = "driver:"
    ]];` conditional), we set the `driver` flag to `1` so that when the next `line`
    is read, we store it as a `_driverNode` (this is done inside the `if [[ "$driver"
    = "1" ]];` conditional). Inside that conditional, we also reset the `executors`
    flag to `0`. The latter is done in case you start with executors first, followed
    by a single driver in the `hosts.txt`. Once the `line` with the driver node information
    is read, we reset the `driver` flag to `0`.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们刚刚读取的`line`等于`"driver:"`（`if [[ "$line" = "driver:" ]];`条件），我们将`driver`标志设置为`1`，这样当下一行被读取时，我们将其存储为`_driverNode`（这是在`if
    [[ "$driver" = "1" ]];`条件内完成的）。在该条件内，我们还将`executors`标志重置为`0`。后者是为了防止您首先启动执行程序，然后在`hosts.txt`中启动单个驱动程序。一旦读取了包含驱动程序节点信息的`line`，我们将`driver`标志重置为`0`。
- en: On the other hand, if the `line` we just read equals to `"executors:"` (the `if
    [[ "$line" = "executors:" ]];` conditional), we set the `executors` flag to `1`
    (and reset the `driver` flag to `0`). This guarantees that the next line read
    will be appended to the `_executors` string, separated by the `"\n"` newline character
    (this happens inside the `if [[ "$executors" = "1" ]];` conditional). Note that
    we do not set the `executor` flag to `0` as we allow for more than one executor.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，如果我们刚刚读取的`line`等于`"executors:"`（`if [[ "$line" = "executors:" ]];`条件），我们将`executors`标志设置为`1`（并将`driver`标志重置为`0`）。这确保下一行将被附加到`_executors`字符串中，并用`"\n"`换行字符分隔（这是在`if
    [[ "$executors" = "1" ]];`条件内完成的）。请注意，我们不将`executor`标志设置为`0`，因为我们允许有多个执行程序。
- en: If we encounter an empty line—which we can check for in bash with the `if [[
    -z "${line}" ]];` conditional—we skip it.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们遇到空行（在bash中可以通过`if [[ -z "${line}" ]];`条件来检查），我们将跳过它。
- en: You might notice that we use the `"<"` redirection pipe to read in the data
    (indicated here by the input variable).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到我们使用`"<"`重定向管道来读取数据（这里由输入变量表示）。
- en: You can read more about the redirection pipes here: [http://www.tldp.org/LDP/abs/html/io-redirection.html](http://www.tldp.org/LDP/abs/html/io-redirection.html).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里阅读更多关于重定向管道的信息：[http://www.tldp.org/LDP/abs/html/io-redirection.html](http://www.tldp.org/LDP/abs/html/io-redirection.html)。
- en: 'Since Spark requires Java and Scala to work, next we have to check if Java
    is installed, and we will install Scala (as it normally isn''t present while Java
    might be). This is achieved with the following functions:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark需要Java和Scala才能工作，接下来我们必须检查Java是否已安装，并安装Scala（因为通常情况下Java可能已安装而Scala可能没有）。这是通过以下函数实现的：
- en: '[PRE76]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: The logic here doesn't differ much from what we presented in the *Installing
    Spark requirements* recipe. The only notable difference in the `checkJava` function
    is that if we do not find Java on the `PATH` variable or inside the `JAVA_HOME`
    folder, we do not exit but run `installJava`, instead.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的逻辑与我们在*安装Spark要求*配方中呈现的内容并没有太大的不同。`checkJava`函数中唯一显著的区别是，如果我们在`PATH`变量或`JAVA_HOME`文件夹中找不到Java，我们不会退出，而是运行`installJava`。
- en: There are many ways to install Java; we have already presented you with one
    of them earlier in this book—check the *Installing Java* section in the *Installing
    Spark requirements* recipe. Here, we used the built-in `apt-get` tool.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Java有很多种方法；我们在本书的早些时候已经向您介绍了其中一种方法——请查看*安装Spark要求*配方中的*安装Java*部分。在这里，我们使用了内置的`apt-get`工具。
- en: The `apt-get` tool is a convenient, fast, and efficient utility for installing
    packages on your Linux machine. **APT** stands for **Advanced Packaging Tool**.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`apt-get`工具是在Linux机器上安装软件包的方便、快速和高效的实用程序。**APT**代表**高级包装工具**。'
- en: 'First, we install the `python-software-properties`. This set of tools provides
    an abstraction of the used `apt` repositories. It enables easy management of distribution
    as well as independent software vendor software sources. We need this as in the
    next line we add the `add-apt-repository`; we add a new repository as we want
    the Oracle Java distribution. The `sudo apt-get update` command refreshes the
    contents of the repositories and, in our current case, fetches all the packages
    available in  `ppa:webupd8team/java`. Finally, we install the Java package: just
    follow the prompts on the screen. We will install Scala the same way.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们安装`python-software-properties`。这套工具提供了对使用的`apt`存储库的抽象。它使得易于管理发行版以及独立软件供应商的软件源。我们需要这个工具，因为在下一行我们要添加`add-apt-repository`；我们添加一个新的存储库，因为我们需要Oracle
    Java发行版。`sudo apt-get update`命令刷新存储库的内容，并在我们当前的情况下获取`ppa:webupd8team/java`中所有可用的软件包。最后，我们安装Java软件包：只需按照屏幕上的提示操作。我们将以同样的方式安装Scala。
- en: The default location where the package should install is `/usr/lib/jvm/java-8-oracle`.
    If this is not the case or you want to install it in a different folder, you will
    have to alter the `_java_destination` variable inside the script to reflect the
    new destination.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 软件包应该安装的默认位置是`/usr/lib/jvm/java-8-oracle`。如果不是这种情况，或者您想要将其安装在不同的文件夹中，您将不得不修改脚本中的`_java_destination`变量以反映新的目的地。
- en: 'The advantage of using this tool is this: if there are already Java and Scala
    environments installed on a machine, using `apt-get` will either skip the installation
    (if the environment is up-to-date with the one available on the server) or ask
    you to update to the newest version.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个工具的优势在于：如果机器上已经安装了Java和Scala环境，使用`apt-get`将要么跳过安装（如果环境与服务器上可用的环境保持最新），要么要求您更新到最新版本。
- en: We will also install the Anaconda distribution of Python (as mentioned many
    times previously, since we highly recommend this distribution). To achieve this
    goal, we must download the `Anaconda3-5.0.1-Linux-x86_64.sh` script first and
    then follow the prompts on the screen. The `-b` parameter to the script will not
    update the `.bashrc` file (we will do that later), the `-u` switch will update
    the Python environment in case `/usr/local/python` already exists, and `-p` will
    force the installation to that folder.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将安装Python的Anaconda发行版（如之前多次提到的，因为我们强烈推荐这个发行版）。为了实现这个目标，我们必须首先下载`Anaconda3-5.0.1-Linux-x86_64.sh`脚本，然后按照屏幕上的提示进行操作。脚本的`-b`参数不会更新`.bashrc`文件（我们稍后会做），`-u`开关将更新Python环境（如果`/usr/local/python`已经存在），`-p`将强制安装到该文件夹。
- en: 'Having passed the required installation steps, we will now update the `/etc/hosts`
    files on the remote machines:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成所需的安装步骤后，我们现在将更新远程机器上的`/etc/hosts`文件：
- en: '[PRE79]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'This is a simple function that, first, creates a copy of the `/etc/hosts` file,
    and then appends the IPs and hostnames of the machines in our cluster. Note that
    the format required by the `/etc/hosts` file is the same as in the `hosts.txt`
    file we use: per row, an IP address of the machine followed by two spaces followed
    by the hostname.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的函数，首先创建`/etc/hosts`文件的副本，然后将我们集群中机器的IP和主机名附加到其中。请注意，`/etc/hosts`文件所需的格式与我们使用的`hosts.txt`文件相同：每行一个机器的IP地址，后面跟着两个空格，然后是主机名。
- en: We use two spaces for readability purposes—one space separating an IP and the
    hostname would also work.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为了可读性的目的使用两个空格——一个空格分隔IP和主机名也可以。
- en: Also, note that we do not use the `echo` command here, but `printf`; the reason
    behind this is that the `printf` command prints out a formatted version of the
    string, properly handling the newline `"\n"` characters.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意我们这里不使用`echo`命令，而是使用`printf`；这样做的原因是`printf`命令打印出字符串的格式化版本，正确处理换行符`"\n"`。
- en: 'Next, we configure the passwordless SSH sessions (check the following *See
    also* subsection) to aid communication between the driver node and the executors:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们配置无密码SSH会话（请查看下面的*另请参阅*子节）以帮助驱动节点和执行器之间的通信。
- en: '[PRE85]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Inside this function, we first check if we are on the driver node, as defined
    in the `hosts.txt` file, as we only need to perform these tasks on the driver.
    The `read -ra temp <<< "$_driverNode"` command reads the `_driverNode` (in our
    case, it is `192.168.1.160  pathfinder`), and splits it at the space character
    (remember what `IFS` stands for?). The `-a` switch instructs the `read` method
    to store the split `_driverNode` string in the `temp` array and the `-r` parameter
    makes sure that the backslash does not act as an escape character. We store the
    name of the driver in the `_driver_machine` variable and append it to the `_all_machines`
    string (we will use this later).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们首先检查我们是否在驱动节点上，如`hosts.txt`文件中定义的那样，因为我们只需要在驱动节点上执行这些任务。`read -ra temp
    <<< "$_driverNode"`命令读取`_driverNode`（在我们的例子中，它是`192.168.1.160  pathfinder`），并在空格字符处拆分它（记住`IFS`代表什么？）。`-a`开关指示`read`方法将拆分的`_driverNode`字符串存储在`temp`数组中，`-r`参数确保反斜杠不起转义字符的作用。我们将驱动程序的名称存储在`_driver_machine`变量中，并将其附加到`_all_machines`字符串中（我们稍后会用到）。
- en: If we are executing this script on the driver machine, the first thing we must
    do is remove the old SSH key (using the `rm` function with the `-f`, force switch)
    and create a new one. The `sudo -u hduser` switch allows us to perform these actions
    as the `hduser` (instead of the `root` user).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在驱动机器上执行此脚本，我们首先必须删除旧的SSH密钥（使用`rm`函数和`-f`强制开关），然后创建一个新的。`sudo -u hduser`开关允许我们以`hduser`的身份执行这些操作（而不是`root`用户）。
- en: When we submit the script to run from our local machine, we start an SSH session
    as a root on the remote machine. You will see how this is done shortly, so take
    our word on that for now.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从本地机器提交脚本运行时，我们在远程机器上以root身份开始一个SSH会话。您很快就会看到这是如何做的，所以现在就相信我们的话吧。
- en: We will use the `ssh-keygen` method to create the SSH key pair. The `-t` switch
    allows us to select the encryption algorithm (we are using RSA encryption), the
    `-P` switch determines the password to use (we want this passwordless, so we choose
    `""`), and the `-f` parameter specifies the filename for storing the keys.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`ssh-keygen`方法创建SSH密钥对。`-t`开关允许我们选择加密算法（我们使用RSA加密），`-P`开关确定要使用的密码（我们希望无密码，所以选择`""`），`-f`参数指定存储密钥的文件名。
- en: 'Next, we loop through all the executors: we need to append the contents of
    `~/.ssh/id_rsa.pub` to their `~/.ssh/authorized_keys` files. We split the `_executors`
    at the `"\n"` character and loop through all of them. To deliver the contents
    of the `id_rsa.pub` file to the executors, we use the `cat` tool to print out
    the contents of the `id_rsa.pub` file and then pipe it to the `ssh` tool. The
    first parameter we pass to the `ssh` is the username and the hostname we want
    to connect to. Next, we pass the commands we want to execute on the remote machine.
    First, we attempt to create the `.ssh` folder if one does not exist. This is followed
    by outputting the `id_rsa.pub` file to `.ssh/authorized_keys`.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们循环遍历所有的执行器：我们需要将`~/.ssh/id_rsa.pub`的内容附加到它们的`~/.ssh/authorized_keys`文件中。我们在`"\n"`字符处拆分`_executors`，并循环遍历所有的执行器。为了将`id_rsa.pub`文件的内容传递给执行器，我们使用`cat`工具打印`id_rsa.pub`文件的内容，然后将其传递给`ssh`工具。我们传递给`ssh`的第一个参数是我们要连接的用户名和主机名。接下来，我们传递我们要在远程机器上执行的命令。首先，我们尝试创建`.ssh`文件夹（如果不存在）。然后将`id_rsa.pub`文件输出到`.ssh/authorized_keys`。
- en: Following the SSH session's configurations on the cluster, we download the Spark
    binaries, unpack them, and move them to `_spark_destination`.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群上配置SSH会话后，我们下载Spark二进制文件，解压它们，并将它们移动到`_spark_destination`。
- en: We have outlined these steps in the *Installing Spark from sources* and *Installing
    Spark from binaries* sections, so we recommend that you check them out.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*从源代码安装Spark*和*从二进制文件安装Spark*部分中概述了这些步骤，因此建议您查看它们。
- en: 'Finally, we need to set two Spark configuration files: the `spark-env.sh` and
    the `slaves` files:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要设置两个Spark配置文件：`spark-env.sh`和`slaves`文件。
- en: '[PRE86]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: We need to append the `JAVA_HOME` variable to `spark-env.sh` so that Spark can
    find the necessary libraries. We must also specify the number of cores per worker
    to be `12`; this goal is attained by setting the `SPARK_WORKER_CORES` variable.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将`JAVA_HOME`变量附加到`spark-env.sh`中，以便Spark可以找到必要的库。我们还必须指定每个worker的核心数为`12`；这个目标是通过设置`SPARK_WORKER_CORES`变量来实现的。
- en: 'You might want to tune the `SPARK_WORKER_CORES` value to your needs. Check
    this spreadsheet for help: [http://c2fo.io/img/apache-spark-config-cheatsheet/C2FO-Spark-Config-Cheatsheet.xlsx](http://c2fo.io/img/apache-spark-config-cheatsheet/C2FO-Spark-Config-Cheatsheet.xlsx)
    (which is available from here: [http://c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/](http://c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/)).'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要根据自己的需求调整`SPARK_WORKER_CORES`的值。查看此电子表格以获取帮助：[http://c2fo.io/img/apache-spark-config-cheatsheet/C2FO-Spark-Config-Cheatsheet.xlsx](http://c2fo.io/img/apache-spark-config-cheatsheet/C2FO-Spark-Config-Cheatsheet.xlsx)（也可以从这里获取：[http://c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/](http://c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/)）。
- en: Next, we have to output the hostnames of all the machines in our cluster to
    the `slaves` file.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将集群中所有机器的主机名输出到`slaves`文件中。
- en: 'In order to execute the script on the remote machine, and since we need to
    run it in an elevated mode (as `root` using `sudo`), we need to encrypt the script
    before we send it over the wire. An example of how this is done is as follows
    (from macOS to remote Linux):'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在远程机器上执行脚本，并且由于我们需要以提升的模式运行它（使用`sudo`作为`root`），我们需要在发送脚本之前对脚本进行加密。如下是如何完成这个过程的示例（从macOS到远程Linux）：
- en: '[PRE87]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Or from Linux to remote Linux:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 或者从Linux到远程Linux：
- en: '[PRE88]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: The preceding script uses the `base64` encryption tool to encrypt the `installOnRemote.sh`
    script before pushing it over to the remote. Once on the remote, we once again
    use `base64` to decrypt the script (the `-d` switch) and run it as `root` (via
    `sudo`). Note that in order to run this type of script, we also pass the `-tq`
    switch to the `ssh` tool; the `-t` option forces a pseudo Terminal allocation
    so that we can execute arbitrary screen-based scripts on the remote machine, and
    the `-q` option quiets all the messages but those from our script.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在将其推送到远程之前，上述脚本使用`base64`加密工具对`installOnRemote.sh`脚本进行加密。一旦在远程上，我们再次使用`base64`来解密脚本（使用`-d`开关）并以`root`（通过`sudo`）运行它。请注意，为了运行这种类型的脚本，我们还向`ssh`工具传递了`-tq`开关；`-t`选项强制分配伪终端，以便我们可以在远程机器上执行任意基于屏幕的脚本，`-q`选项使所有消息都安静，除了我们的脚本消息。
- en: 'Assuming all goes well, once the script finishes executing on all your machines,
    Spark has been successfully installed and configured on your cluster. However,
    before you can use Spark, you need either to close the connection to your driver
    and SSH to it again, or type:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一切顺利，一旦脚本在所有机器上执行完毕，Spark就已经成功安装并配置在您的集群上。但是，在您可以使用Spark之前，您需要关闭与驱动程序的连接并再次SSH到它，或者输入：
- en: '[PRE89]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: This is so that the newly created environment variables are available, and your
    `PATH` is updated.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这样新创建的环境变量才能可用，并且您的`PATH`被更新。
- en: 'To start your cluster, you can type:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动您的集群，可以输入：
- en: '[PRE90]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: And all the machines in the cluster should be coming to life and be recognized
    by Spark.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的所有机器应该开始运行并被Spark识别。
- en: 'In order to check if everything started properly, type:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查一切是否正常启动，输入：
- en: '[PRE91]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'And it should return something similar to the following (in our case, we had
    three machines in our cluster):'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该返回类似于以下内容（在我们的情况下，我们的集群中有三台机器）：
- en: '[PRE92]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: See also
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Here''s a list of useful links that might help you to go through with this
    recipe:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些有用的链接列表，可能会帮助您完成这个配方：
- en: If you do not know how to add a user group, check this link: [https://www.techonthenet.com/linux/sysadmin/ubuntu/create_group_14_04.php](https://www.techonthenet.com/linux/sysadmin/ubuntu/create_group_14_04.php)
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您不知道如何添加用户组，请查看此链接：[https://www.techonthenet.com/linux/sysadmin/ubuntu/create_group_14_04.php](https://www.techonthenet.com/linux/sysadmin/ubuntu/create_group_14_04.php)
- en: To add a `sudo` user, check this link: [https://www.digitalocean.com/community/tutorials/how-to-add-and-delete-users-on-ubuntu-16-04](https://www.digitalocean.com/community/tutorials/how-to-add-and-delete-users-on-ubuntu-16-04)
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要添加一个`sudo`用户，请查看此链接：[https://www.digitalocean.com/community/tutorials/how-to-add-and-delete-users-on-ubuntu-16-04](https://www.digitalocean.com/community/tutorials/how-to-add-and-delete-users-on-ubuntu-16-04)
- en: Here are step-by-step manual instructions on how to install Spark: [https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/](https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/).
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是安装Spark的逐步手动说明：[https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/](https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/)。
- en: Here is how to set a passwordless SSH communication between machines: [https://www.tecmint.com/ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps/](https://www.tecmint.com/ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps/)
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是如何在机器之间设置无密码SSH通信的方法：[https://www.tecmint.com/ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps/](https://www.tecmint.com/ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps/)
- en: Installing Jupyter
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Jupyter
- en: Jupyter provides a means to conveniently cooperate with your Spark environment.
    In this recipe, we will guide you in how to install Jupyter on your local machine.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter提供了一种方便地与您的Spark环境合作的方式。在这个配方中，我们将指导您如何在本地机器上安装Jupyter。
- en: Getting ready
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: We require a working installation of Spark. This means that you will have followed
    the steps outlined in the first, and either the second or third recipes. In addition,
    a working Python environment is also required.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个可用的Spark安装。这意味着您将按照第一个、第二或第三个配方中概述的步骤进行操作。此外，还需要一个可用的Python环境。
- en: No other prerequisites are required.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: If you do not have `pip` installed on your machine, you will need to install
    it before proceeding.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的机器上没有安装`pip`，您需要在继续之前安装它。
- en: 'To do this, open your Terminal and type (on macOS):'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要做到这一点，请打开终端并键入（在macOS上）：
- en: '[PRE93]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Or the following on Linux:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在Linux上：
- en: '[PRE94]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Next, type (applies to both operating systems):'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，键入（适用于两种操作系统）：
- en: '[PRE95]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: This will install `pip` on your machine.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在您的计算机上安装`pip`。
- en: 'All you have to do now is install Jupyter with the following command:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您现在只需使用以下命令安装Jupyter：
- en: '[PRE96]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: How it works...
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`pip` is a management tool for installing Python packages for **PyPI**, the
    **Python Package Index**. This service hosts a wide range of Python packages and
    is the easiest and quickest way to distribute your Python packages.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip`是一个用于在**PyPI**，**Python软件包索引**上安装Python软件包的管理工具。 该服务托管了各种Python软件包，并且是分发Python软件包的最简单和最快速的方式。'
- en: 'However, calling `pip install` does not only search for the packages on PyPI:
    in addition, VCS project URLs, local project directories, and local or remote
    source archives are also scanned.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，调用`pip install`不仅会在PyPI上搜索软件包：此外，还会扫描VCS项目URL、本地项目目录和本地或远程源存档。
- en: 'Jupyter is one of the most popular interactive shells that supports developing
    code in a wide variety of environments: Python is not the only one that''s supported.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter是最受欢迎的交互式shell之一，支持在各种环境中开发代码：Python不是唯一受支持的环境。
- en: 'Directly from [http://jupyter.org](http://jupyter.org):'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 直接来自[http://jupyter.org](http://jupyter.org)：
- en: '"The Jupyter Notebook is an open-source web application that allows you to
    create and share documents that contain live code, equations, visualizations,
    and narrative text. Uses include: data cleaning and transformation, numerical
    simulation, statistical modeling, data visualization, machine learning, and much
    more."'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: “Jupyter Notebook是一个开源的Web应用程序，允许您创建和共享包含实时代码、方程式、可视化和叙述文本的文档。用途包括：数据清理和转换、数值模拟、统计建模、数据可视化、机器学习等等。”
- en: 'Another way to install Jupyter, if you are using Anaconda distribution for
    Python, is to use its package management tool, the `conda`. Here''s how:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Jupyter的另一种方法，如果您正在使用Anaconda Python发行版，则是使用其软件包管理工具`conda`。 这是方法：
- en: '[PRE97]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Note that `pip install` will also work in Anaconda.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`pip install`在Anaconda中也可以工作。
- en: There's more...
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Now that you have Jupyter on your machine, and assuming you followed the steps
    of either the *Installing Spark from sources* or the *Installing Spark from binaries*
    recipes, you should be able to start using Jupyter to interact with PySpark.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的计算机上有了Jupyter，并且假设您已经按照*从源代码安装Spark*或*从二进制文件安装Spark*教程的步骤进行操作，您应该能够开始使用Jupyter与PySpark进行交互。
- en: 'To refresh your memory, as part of installing Spark scripts, we have appended
    two environment variables to the bash profile file: `PYSPARK_DRIVER_PYTHON` and
    `PYSPARK_DRIVER_PYTHON_OPTS`. Using these two environment variables, we set the
    former to use `jupyter` and the latter to start a `notebook` service.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提醒您，作为安装Spark脚本的一部分，我们已将两个环境变量附加到bash配置文件中：`PYSPARK_DRIVER_PYTHON`和`PYSPARK_DRIVER_PYTHON_OPTS`。
    使用这两个环境变量，我们将前者设置为使用`jupyter`，将后者设置为启动`notebook`服务。
- en: 'If you now open your Terminal and type:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在打开终端并键入：
- en: '[PRE98]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'When you open your browser and navigate to `http://localhost:6661`, you should
    see a window not that different from the one in the following screenshot:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 当您打开浏览器并导航到`http://localhost:6661`时，您应该看到一个与以下屏幕截图中的窗口并没有太大不同：
- en: '![](img/00009.jpeg)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00009.jpeg)'
- en: See also
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Check out [https://pypi.python.org/pypi](https://pypi.python.org/pypi), as the
    number of really cool projects available for Python is mind-boggling
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看[https://pypi.python.org/pypi](https://pypi.python.org/pypi)，因为Python可用的真正酷项目数量令人难以置信
- en: Configuring a session in Jupyter
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Jupyter中配置会话
- en: Working in Jupyter is great as it allows you to develop your code interactively,
    and document and share your notebooks with colleagues. The problem, however, with
    running Jupyter against a local Spark instance is that the `SparkSession` gets
    created automatically and by the time the notebook is running, you cannot change
    much in that session's configuration.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter中工作非常棒，因为它允许您以交互方式开发代码，并与同事记录和共享笔记本。 但是，使用本地Spark实例运行Jupyter的问题在于`SparkSession`会自动创建，并且在笔记本运行时，您无法在该会话的配置中进行太多更改。
- en: 'In this recipe, we will learn how to install Livy, a REST service to interact
    with Spark, and `sparkmagic`, a package that will allow us to configure sessions
    interactively as well:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将学习如何安装Livy，这是一个与Spark交互的REST服务，以及`sparkmagic`，这是一个允许我们以交互方式配置会话的软件包：
- en: '![](img/00010.jpeg)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00010.jpeg)'
- en: Source: http://bit.ly/2iO3EwC
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：http://bit.ly/2iO3EwC
- en: Getting ready
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'We assume that you either have installed Spark via binaries or compiled the
    sources as we have shown you in the previous recipes. In other words, by now,
    you should have a working Spark environment. You will also need Jupyter: if you
    do not have it, follow the steps from the previous recipe to install it.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您已经通过二进制文件安装了Spark，或者按照我们在之前的教程中向您展示的那样编译了源代码。 换句话说，到目前为止，您应该已经拥有一个可用的Spark环境。
    您还需要Jupyter：如果没有，请按照上一个教程中的步骤安装它。
- en: No other prerequisites are required.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-394
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To install Livy and `sparkmagic`, we have created a script that will do this
    automatically with minimal interaction from you. You can find it in the `Chapter01/installLivy.sh` folder.
    You should be familiar with most of the functions that we''re going to use here
    by now, so we will focus only on those that are different (highlighted in bold
    in the following code). Here is the high-level view of the script''s structure:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Livy和`sparkmagic`，我们已经创建了一个脚本，将自动执行此操作，您只需进行最少的交互。 您可以在`Chapter01/installLivy.sh`文件夹中找到它。
    到目前为止，您应该已经熟悉我们将在此处使用的大多数功能，因此我们将仅关注不同的功能（在以下代码中以粗体显示）。 这是脚本结构的高级视图：
- en: '[PRE99]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: How it works...
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As with all other scripts we have presented so far, we will begin by setting
    some global variables.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今为止介绍的所有其他脚本一样，我们将首先设置一些全局变量。
- en: If you do not know what these mean, check the *Installing Spark from sources*
    recipe.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不知道这些是什么意思，请查看*从源代码安装Spark*教程。
- en: Livy requires some configuration files from Hadoop. Thus, as part of this script,
    we allow you to install Hadoop should it not be present on your machine. That
    is why we now allow you to pass arguments to the `downloadThePackage`, `unpack`,
    and `moveTheBinaries` functions.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: Livy需要一些来自Hadoop的配置文件。因此，在此脚本的一部分中，我们允许您安装Hadoop，如果您的计算机上没有安装Hadoop。这就是为什么我们现在允许您向`downloadThePackage`，`unpack`和`moveTheBinaries`函数传递参数。
- en: The changes to the functions are fairly self-explanatory, so for the sake of
    space, we will not be pasting the code here. You are more than welcome, though,
    to peruse the relevant portions of the `installLivy.sh` script.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 对函数的更改相当容易理解，因此出于空间考虑，我们将不在此处粘贴代码。不过，您可以随时查看`installLivy.sh`脚本的相关部分。
- en: Installing Livy drills down literally to downloading the package, unpacking
    it, and moving it to its final destination (in our case, this is `/opt/livy`).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Livy实际上是下载软件包，解压缩并将其移动到最终目的地（在我们的情况下，这是`/opt/livy`）。
- en: 'Checking if Hadoop is installed is the next thing on our to-do list. To run
    Livy with local sessions, we require two environment variables: `SPARK_HOME` and `HADOOP_CONF_DIR`;
    the `SPARK_HOME` is definitely set but if you do not have Hadoop installed, you
    most likely will not have the latter environment variable set:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 检查Hadoop是否已安装是我们要做的下一件事。要在本地会话中运行Livy，我们需要两个环境变量：`SPARK_HOME`和`HADOOP_CONF_DIR`；`SPARK_HOME`肯定已设置，但如果您没有安装Hadoop，您很可能不会设置后一个环境变量：
- en: '[PRE100]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: The `checkHadoop` function first checks if the `hadoop` binary is present on
    the `PATH`; if not, it will check if the `HADOOP_HOME` variable is set and, if
    it is, it will check if the `hadoop` binary can be found inside the `$HADOOP_HOME/bin`
    folder. If both attempts fail, the script will ask you if you want to install
    the latest version of Hadoop; the default answer is `n` but if you answer `y`,
    the installation will begin.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '`checkHadoop`函数首先检查`PATH`上是否存在`hadoop`二进制文件；如果没有，它将检查`HADOOP_HOME`变量是否已设置，如果设置了，它将检查`$HADOOP_HOME/bin`文件夹中是否可以找到`hadoop`二进制文件。如果两次尝试都失败，脚本将询问您是否要安装Hadoop的最新版本；默认答案是`n`，但如果您回答`y`，安装将开始。'
- en: Once the installation finishes, we will begin installing the additional kernels
    for the Jupyter Notebooks.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，我们将开始安装Jupyter笔记本的其他内核。
- en: A kernel is a piece of software that translates the commands from the frontend
    notebook to the backend environment (like Python). For a list of available Jupyter
    kernels check out the following link: [https://github.com/jupyter/jupyter/wiki/Jupyter-kernels](https://github.com/jupyter/jupyter/wiki/Jupyter-kernels).
    Here are some instructions on how to develop a kernel yourself: [http://jupyter-client.readthedocs.io/en/latest/kernels.html](http://jupyter-client.readthedocs.io/en/latest/kernels.html).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 内核是一种软件，它将来自前端笔记本的命令转换为后端环境（如Python）的命令。有关可用Jupyter内核的列表，请查看以下链接：[https://github.com/jupyter/jupyter/wiki/Jupyter-kernels](https://github.com/jupyter/jupyter/wiki/Jupyter-kernels)。以下是如何自己开发内核的一些说明：[http://jupyter-client.readthedocs.io/en/latest/kernels.html](http://jupyter-client.readthedocs.io/en/latest/kernels.html)。
- en: 'Here''s the function that handles the kernel''s installation:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是处理内核安装的函数：
- en: '[PRE102]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'First, we install the `sparkmagic` package for Python. Quoting directly from [https://github.com/jupyter-incubator/sparkmagic](https://github.com/jupyter-incubator/sparkmagic):'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为Python安装`sparkmagic`软件包。直接引用自[https://github.com/jupyter-incubator/sparkmagic](https://github.com/jupyter-incubator/sparkmagic)：
- en: '"Sparkmagic is a set of tools for interactively working with remote Spark clusters
    through Livy, a Spark REST server, in Jupyter Notebooks. The Sparkmagic project
    includes a set of magics for interactively running Spark code in multiple languages,
    as well as some kernels that you can use to turn Jupyter into an integrated Spark
    environment."'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: “Sparkmagic是一组工具，用于通过Livy（Spark REST服务器）在Jupyter笔记本中与远程Spark集群进行交互。Sparkmagic项目包括一组魔术方法，用于以多种语言交互地运行Spark代码，以及一些内核，您可以使用这些内核将Jupyter转换为集成的Spark环境。”
- en: The following command enables the Javascript extensions in Jupyter Notebooks
    so that `ipywidgets` can work properly; if you have an Anaconda distribution of
    Python, this package will be installed automatically.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令启用Jupyter笔记本中的Javascript扩展，以便`ipywidgets`可以正常工作；如果您使用的是Python的Anaconda发行版，此软件包将自动安装。
- en: Following this, we install the kernels. We need to switch to the folder where
    `sparkmagic` was installed into. The `pip show <package>` command displays all
    relevant information about the installed packages; from the output, we only extract
    the `Location` using `awk`.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们安装内核。我们需要切换到`sparkmagic`安装的文件夹。`pip show <package>`命令显示有关安装软件包的所有相关信息；从输出中，我们只使用`awk`提取`Location`。
- en: 'To install the kernels, we use the `jupyter-kernelspec install <kernel>` command.
    For example, the command will install the `sparkmagic` kernel for the Scala API
    of Spark:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 安装内核时，我们使用`jupyter-kernelspec install <kernel>`命令。例如，该命令将为Spark的Scala API安装`sparkmagic`内核：
- en: '[PRE103]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Once all the kernels are installed, we enable Jupyter to use `sparkmagic` so
    that we can change clusters programmatically. Finally, we will install the `autovizwidget`,
    an auto-visualization library for `pandas dataframes`.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 安装所有内核后，我们启用Jupyter使用`sparkmagic`，以便我们可以以编程方式更改集群。最后，我们将安装`autovizwidget`，这是一个用于`pandas数据框`的自动可视化库。
- en: This concludes the Livy and `sparkmagic` installation part.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了Livy和`sparkmagic`的安装部分。
- en: There's more...
  id: totrans-419
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Now that we have everything in place, let's see what this can do.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 既然一切就绪，让我们看看这能做什么。
- en: 'First, start Jupyter (note that we do not use the `pyspark` command):'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 首先启动Jupyter（请注意，我们不使用`pyspark`命令）：
- en: '[PRE104]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'You should now be able to see the following options if you want to add a new
    notebook:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要添加新的笔记本，现在应该能够看到以下选项：
- en: '![](img/00011.jpeg)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.jpeg)'
- en: If you click on PySpark, it will open a notebook and connect to a kernel.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单击PySpark，它将打开一个笔记本并连接到一个内核。
- en: 'There are a number of available *magics* to interact with the notebooks; type
    `%%help` to list them all. Here''s the list of the most important ones:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可用的*魔术*与笔记本互动；键入`%%help`以列出所有魔术。以下是最重要的魔术列表：
- en: '| **Magic** | **Example** | **Explanation** |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| **魔术** | **示例** | **说明** |'
- en: '| `info` | `%%info` | Outputs session information from Livy. |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| `info` | `%%info` | 从Livy输出会话信息。 |'
- en: '| `cleanup` | `%%cleanup -f` | Delete all sessions running on the current Livy
    endpoint. The `-f` switch forces the cleanup. |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| `cleanup` | `%%cleanup -f` | 删除当前Livy端点上运行的所有会话。`-f`开关强制清理。 |'
- en: '| `delete` | `%%delete -f -s 0` | Deletes the session specified by the `-s`
    switch; the `-f` switch forces the deletion. |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| `delete` | `%%delete -f -s 0` | 删除由`-s`开关指定的会话；`-f`开关强制删除。 |'
- en: '| `configure` | `%%configure -f``{"executorMemory": "1000M", "executorCores":
    4}` | Arguably the most useful magic. Allows you to configure your session. Check
    [http://bit.ly/2kSKlXr](http://bit.ly/2kSKlXr) for the full list of available
    configuration parameters. |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| `configure` | `%%configure -f``{"executorMemory": "1000M", "executorCores":
    4}` | 可能是最有用的魔术。允许您配置会话。查看[http://bit.ly/2kSKlXr](http://bit.ly/2kSKlXr)获取可用配置参数的完整列表。
    |'
- en: '| `sql` | `%%sql -o tables -q``SHOW TABLES` | Executes an SQL query against
    the current `SparkSession`. |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| `sql` | `%%sql -o tables -q``SHOW TABLES` | 对当前的`SparkSession`执行SQL查询。 |'
- en: '| `local` | `%%local``a=1` | All the code in the notebook cell with this magic
    will be executed locally against the Python environment. |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| `local` | `%%local``a=1` | 笔记本单元格中带有此魔术的所有代码将在Python环境中本地执行。 |'
- en: 'Once you have configured your session, you will get information back from Livy
    about the active sessions that are currently running:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您配置了会话，您将从Livy那里得到有关当前正在运行的活动会话的信息：
- en: '![](img/00012.jpeg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00012.jpeg)'
- en: 'Let''s try to create a simple data frame using the following code:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用以下代码创建一个简单的数据框架：
- en: '[PRE105]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Once you execute the preceding code in a cell inside the notebook, only then
    will the `SparkSession` be created:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本内的单元格中执行上述代码后，才会创建`SparkSession`：
- en: '![](img/00013.jpeg)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00013.jpeg)'
- en: 'If you execute `%%sql` magic, you will get the following:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您执行`%%sql`魔术，您将得到以下内容：
- en: '![](img/00014.jpeg)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00014.jpeg)'
- en: See also
  id: totrans-442
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Check the Livy REST API in case you want to submit jobs programmatically: [https://livy.incubator.apache.org/docs/latest/rest-api.html](https://livy.incubator.apache.org/docs/latest/rest-api.html).
    Also, for a list of configurable parameters available in `sparkmagic`, go to: [https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Pyspark%20Kernel.ipynb](https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Pyspark%20Kernel.ipynb).
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想以编程方式提交作业，请检查 Livy REST API：[https://livy.incubator.apache.org/docs/latest/rest-api.html](https://livy.incubator.apache.org/docs/latest/rest-api.html)。另外，要查看`sparkmagic`中可用的可配置参数列表，请访问：[https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Pyspark%20Kernel.ipynb](https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Pyspark%20Kernel.ipynb)。
- en: Working with Cloudera Spark images
  id: totrans-444
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Cloudera Spark镜像
- en: Cloudera is a company that was founded in 2008 by ex-employees of Google, Yahoo!,
    Oracle, and Facebook. It was an early adopter of open source technologies like
    Apache Hadoop when it was still fresh from the oven; as a matter of a fact, the
    author of Hadoop itself joined the company shortly thereafter. Today, Cloudera
    sells licenses for a broad array of open source products, mostly from the Apache
    Software Foundation, and also provides consulting services.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudera是一家成立于2008年的公司，由Google，Yahoo！，Oracle和Facebook的前员工创立。当Apache Hadoop刚刚推出时，它就是开源技术的早期采用者；事实上，Hadoop的作者本人随后不久就加入了该公司。如今，Cloudera销售来自Apache
    Software Foundation的广泛的开源产品许可证，并提供咨询服务。
- en: In this recipe, we will look at a free virtual image from Cloudera that we can
    use to learn how to use the newest technologies supported by the company.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将查看Cloudera的免费虚拟镜像，以便学习如何使用该公司支持的最新技术。
- en: Getting ready
  id: totrans-447
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To go through this recipe, you will need a working installation of a VirtualBox,
    a free virtualization tool from Oracle.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本教程，您需要安装Oracle的免费虚拟化工具VirtualBox。
- en: 'Here are the instructions for installing VirtualBox:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是安装VirtualBox的说明：
- en: 'On Windows: [https://www.htpcbeginner.com/install-virtualbox-on-windows/](https://www.htpcbeginner.com/install-virtualbox-on-windows/)'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上：[https://www.htpcbeginner.com/install-virtualbox-on-windows/](https://www.htpcbeginner.com/install-virtualbox-on-windows/)
- en: 'On Linux: [https://www.packtpub.com/books/content/installing-virtualbox-linux](https://www.packtpub.com/books/content/installing-virtualbox-linux)
    On Mac: [https://www.youtube.com/watch?v=lEvM-No4eQo](https://www.youtube.com/watch?v=lEvM-No4eQo).'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux上：[https://www.packtpub.com/books/content/installing-virtualbox-linux](https://www.packtpub.com/books/content/installing-virtualbox-linux)
    在Mac上：[https://www.youtube.com/watch?v=lEvM-No4eQo](https://www.youtube.com/watch?v=lEvM-No4eQo)。
- en: 'To run the VMs, you will need:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行VM，您需要：
- en: A 64-bit host; Windows 10, macOS, and most of the Linux distributions are 64-bit
    systems
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 64位主机；Windows 10，macOS和大多数Linux发行版都是64位系统
- en: A minimum 4 GB of RAM dedicated for the VM, thus a system with a minimum of
    8 GB of RAM is required
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少需要4GB的RAM专用于VM，因此需要至少8GB的RAM系统
- en: No other prerequisites are required.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 无需其他先决条件。
- en: How to do it...
  id: totrans-456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'To begin with, in order to download the Cloudera QuickStart VM:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，要下载Cloudera QuickStart VM：
- en: Go to [https://www.cloudera.com/downloads/quickstart_vms/5-12.html](https://www.cloudera.com/downloads/quickstart_vms/5-12.html).
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问[https://www.cloudera.com/downloads/quickstart_vms/5-12.html](https://www.cloudera.com/downloads/quickstart_vms/5-12.html)。
- en: Select VirtualBox as your platform from the dropdown on the right, and click
    on the Get it now button.
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从右侧的下拉菜单中选择VirtualBox作为您的平台，然后单击立即获取按钮。
- en: 'A window to register will show up; fill it in as appropriate and follow the
    instructions on the screen:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将显示一个注册窗口；根据需要填写并按照屏幕上的说明操作：
- en: '![](img/00015.jpeg)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00015.jpeg)'
- en: Note, that it is a 6 GB+ download, so it may take a while.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是一个超过6GB的下载，所以可能需要一些时间。
- en: Once downloaded, open the VirtualBox.
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载后，打开VirtualBox。
- en: Go to File | Import appliance, click on the button next to the path selection,
    and find the `.ovf` file (it should be accompanied by the `.vmdk` file, which
    is appropriate for the version you just downloaded).
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到文件|导入虚拟机，单击路径选择旁边的按钮，并找到`.ovf`文件（它应该有一个`.vmdk`文件，适用于您刚下载的版本）。
- en: On macOS, the image is automatically decompressed upon downloading. On Windows
    and Linux, you might need to unzip the archive first.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在macOS上，图像在下载后会自动解压缩。在Windows和Linux上，您可能需要先解压缩存档文件。
- en: 'You should see a progress bar that is similar to this one:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一个类似于这样的进度条：
- en: '![](img/00016.jpeg)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00016.jpeg)'
- en: 'Once imported, you should see a window like this:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 导入后，您应该看到一个像这样的窗口：
- en: '![](img/00017.jpeg)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00017.jpeg)'
- en: 'If you now click on Start, you should see a new window pop up, and Cloudera
    VM (that is built on CentOS) should start booting up. Once done, a window similar
    to the following one should show up on your screen:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果现在点击“启动”，您应该会看到一个新窗口弹出，Cloudera VM（构建在CentOS上）应该开始启动。完成后，您的屏幕上应该会出现一个类似下面的窗口：
- en: '![](img/00018.jpeg)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00018.jpeg)'
- en: How it works...
  id: totrans-472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'There is really not much to configure: Cloudera QuickStart VM has everything
    you need to get going. As a matter of fact, this is a much simpler solution for
    Windows users than installing all the necessary environments. However, at the
    time of writing this book, it only comes with Spark 1.6.0:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，没有太多需要配置的：Cloudera QuickStart VM已经包含了您启动所需的一切。事实上，对于Windows用户来说，这比安装所有必要的环境要简单得多。然而，在撰写本书时，它只配备了Spark
    1.6.0：
- en: '![](img/00019.jpeg)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00019.jpeg)'
- en: Nothing, however, can stop you from upgrading to Spark 2.3.1 by following either
    the *Installing Spark from sources* or *Installing Spark from binaries* recipes
    we presented earlier in this book.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过按照我们在本书中提出的*从源代码安装Spark*或*从二进制文件安装Spark*的方法，您可以升级到Spark 2.3.1。
