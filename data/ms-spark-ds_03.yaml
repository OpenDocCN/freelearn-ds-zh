- en: Chapter 3. Input Formats and Schema
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 输入格式和模式
- en: The aim of this chapter is to demonstrate how to load data from its raw format
    onto different schemas, therefore enabling a variety of different kinds of downstream
    analytics to be run over the same data. When writing analytics, or even better,
    building libraries of reusable software, you generally have to work with interfaces
    of fixed input types. Therefore, having flexibility in how you transition data
    between schemas, depending on the purpose, can deliver considerable downstream
    value, both in terms of widening the type of analysis possible and the re-use
    of existing code.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是演示如何将数据从原始格式加载到不同的模式中，从而使得可以对相同的数据运行各种不同类型的下游分析。在编写分析报告，甚至更好的是构建可重用软件库时，通常需要使用固定输入类型的接口。因此，根据目的在不同模式之间灵活转换数据，可以在下游提供相当大的价值，无论是在扩大可能的分析类型还是重复使用现有代码方面。
- en: Our primary objective is to learn about the data format features that accompany
    Spark, although we will also delve into the finer points of data management by
    introducing proven methods that will enhance your data handling and increase your
    productivity. After all, it is most likely that you will be required to formalize
    your work at some point, and an introduction to how to avoid the potential long-term
    pitfalls is invaluable when writing analytics, and long after.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要目标是了解伴随Spark的数据格式特性，尽管我们也将深入探讨数据管理的细节，介绍能够增强数据处理并提高生产力的成熟方法。毕竟，很可能在某个时候需要正式化您的工作，了解如何避免潜在的长期问题在撰写分析报告时和很久之后都是非常宝贵的。
- en: With this is mind, we will use this chapter to look at the traditionally well
    understood area of *data schemas*. We will cover key areas of traditional database
    modeling and explain how some of these cornerstone principles are still applicable
    to Spark.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们将利用本章来研究传统上理解良好的*数据模式*领域。我们将涵盖传统数据库建模的关键领域，并解释一些这些基石原则如何仍然适用于Spark。
- en: In addition, while honing our Spark skills, we will analyze the GDELT data model
    and show how to store this large dataset in an efficient and scalable manner.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当我们磨练我们的Spark技能时，我们将分析GDELT数据模型，并展示如何以高效和可扩展的方式存储这个大型数据集。
- en: 'We will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: 'Dimensional modeling: benefits and weaknesses in relation to Spark'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度建模：与Spark相关的优点和缺点
- en: Focus on the GDELT model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注GDELT模型
- en: Lifting the lid on schema-on-read
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 揭开按需模式的盖子
- en: Avro object model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Avro对象模型
- en: Parquet storage model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet存储模型
- en: Let's start with some best practice.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些最佳实践开始。
- en: A structured life is a good life
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化生活是美好的生活
- en: When learning about the benefits of Spark and big data, you may have heard discussions
    about *structured* data versus *semi-structured* data versus *unstructured* data.
    While Spark promotes the use of structured, semi-structured, and unstructured
    data, it also provides the basis for its consistent treatment. The only constraint
    being that it should be *record-based*. Providing they are record-based, datasets
    can be transformed, enriched and manipulated in the same way, regardless of their
    organization.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解Spark和大数据的好处时，您可能听过关于*结构化*数据与*半结构化*数据与*非结构化*数据的讨论。虽然Spark推广使用结构化、半结构化和非结构化数据，但它也为这些数据的一致处理提供了基础。唯一的约束是它应该是*基于记录*的。只要是基于记录的，数据集就可以以相同的方式进行转换、丰富和操作，而不管它们的组织方式如何。
- en: However, it is worth noting that having unstructured data does not necessitate
    taking an unstructured *approach*. Having identified techniques for exploring
    datasets in the previous chapter, it would be tempting to dive straight into stashing
    data somewhere accessible and immediately commencing simple profiling analytics.
    In real life situations, this activity often takes precedence over due diligence.
    Once again, we would encourage you to consider several key areas of interest,
    for example, file integrity, data quality, schedule management, version management,
    security, and so on, before embarking on this exploration. These should not be
    ignored and many are large topics in their own right.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，拥有非结构化数据并不意味着采取非结构化的*方法*。在上一章中已经确定了探索数据集的技术，很容易就会有冲动直接将数据存储在可访问的地方，并立即开始简单的分析。在现实生活中，这种活动经常优先于尽职调查。再次，我们鼓励您考虑几个关键领域，例如文件完整性、数据质量、时间表管理、版本管理、安全性等等，在开始这项探索之前。这些都不应被忽视，许多都是非常重要的话题。
- en: Therefore, while we have already covered many of these concerns in [Chapter
    2](ch02.xhtml "Chapter 2. Data Acquisition"), *Data Acquisition*, and will study
    more later, for example in [Chapter 13](ch13.xhtml "Chapter 13. Secure Data"),
    *Secure Data*, in this chapter we are going to focus on data input and output
    formats specifically, exploring some of the methods that we can employ to ensure
    better data handling and management.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然我们已经在[第2章](ch02.xhtml "第2章. 数据获取")中涵盖了许多这些问题，*数据获取*，并且以后还会学习更多，例如在[第13章](ch13.xhtml
    "第13章. 安全数据")中，*安全数据*，但在本章中，我们将专注于数据输入和输出格式，探索一些我们可以采用的方法，以确保更好的数据处理和管理。
- en: GDELT dimensional modeling
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GDELT维度建模
- en: As we have chosen to use GDELT for analysis purposes in this book, we will introduce
    our first example using this dataset. First, let's select some data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们选择在本书中使用GDELT进行分析，我们将首先介绍使用这个数据集的第一个示例。首先，让我们选择一些数据。
- en: 'There are two streams of data available: **Global Knowledge Graph** (**GKG**)
    and **Events**.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个可用的数据流：**全球知识图谱**（**GKG**）和**事件**。
- en: For this chapter, we are going to use GKG data to create a time-series dataset
    queryable from Spark SQL. This will give us a great starting point to create some
    simple introductory analytics.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将使用GKG数据来创建一个可以从Spark SQL查询的时间序列数据集。这将为我们提供一个很好的起点，以创建一些简单的入门分析。
- en: In the next chapters, [Chapter 4](ch04.xhtml "Chapter 4. Exploratory Data Analysis"),
    *Exploratory Data Analysis* and [Chapter 5](ch05.xhtml "Chapter 5. Spark for Geographic
    Analysis"), *Spark for Geographic Analysis*, we'll go into more detail but stay
    with GKG. Then, in [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"), *Building
    Communities*, we will explore events by producing our own network graph of persons
    and using it in some cool analytics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，[第4章](ch04.xhtml "第4章. 探索性数据分析"), *探索性数据分析* 和 [第5章](ch05.xhtml "第5章. 用于地理分析的Spark"),
    *用于地理分析的Spark*，我们将更详细地讨论，但仍然与GKG保持联系。然后，在[第7章](ch07.xhtml "第7章. 构建社区"), *构建社区*，我们将通过生成自己的人员网络图来探索事件，并在一些酷炫的分析中使用它。
- en: GDELT model
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GDELT模型
- en: GDELT has been around for more than 20 years and, during that time, has undergone
    some significant revisions. For our introductory examples, to keep things simple,
    let's limit our range of data from 1st April 2013, when GDELT had a major file
    structure overhaul, introducing the GKG files. It's worth noting that the principles
    discussed in this chapter are applicable to all versions of GDELT data, however,
    the specific schemas and **Uniform Resource Identifiers** (**URIs**) prior to
    this date may be different to the ones described. The version we will use is GDELT
    v2.1, which is the latest version at the time of writing. But again, it's worth
    noting that this varies only slightly from GDELT 2.0.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GDELT已经存在了20多年，在这段时间里经历了一些重大的修订。为了保持简单，让我们限制我们的数据范围从2013年4月1日开始，当时GDELT进行了一次重大的文件结构改革，引入了GKG文件。值得注意的是，本章讨论的原则适用于所有版本的GDELT数据，但是在此日期之前的特定模式和**统一资源标识符**（**URI**）可能与描述的不同。我们将使用的版本是GDELT
    v2.1，这是撰写时的最新版本。但值得注意的是，这与GDELT 2.0只有轻微的不同。
- en: 'There are two data tracks within GKG data:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: GKG数据中有两条数据轨道：
- en: The entire knowledge graph, along with all of its fields.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整个知识图，以及它的所有字段。
- en: The subset of the graph, which contains a set of predefined categories.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包含一组预定义类别的图的子集。
- en: We'll look at the first track.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先查看第一条轨道。
- en: First look at the data
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 首次查看数据
- en: We discussed how to download GDELT data in [Chapter 2](ch02.xhtml "Chapter 2. Data
    Acquisition"), *Data Acquisition*, so if you already have a NiFi pipeline configured
    to download the GKG data, just ensure that it's available in HDFS. However, if
    you have not completed that chapter, then we would encourage you to do this first,
    as it explains why you should take a structured approach to obtaining data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第2章](ch02.xhtml "第2章. 数据获取")中讨论了如何下载GDELT数据，因此，如果您已经配置了NiFi管道来下载GKG数据，只需确保它在HDFS中可用。但是，如果您还没有完成该章节，我们鼓励您首先这样做，因为它解释了为什么应该采取结构化方法来获取数据。
- en: While we have gone to great lengths to discourage the use of ad hoc data downloading,
    the scope of this chapter is of course known and therefore, if you are interested
    in following the examples seen here, you can skip the use of NiFi and obtain the
    data directly (in order to get started as quickly as possible).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经竭尽全力阻止临时数据下载的使用，但本章的范围当然是已知的，因此，如果您有兴趣跟随这里看到的示例，可以跳过使用NiFi直接获取数据（以便尽快开始）。
- en: 'If you do wish to download a sample, here''s a reminder of where to find the
    GDELT 2.1 GKG master file list:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望下载一个样本，这里是在哪里找到GDELT 2.1 GKG主文件列表的提醒：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Make a note of a couple of the latest entries that match `.gkg.csv.zip`, copy
    them using your favorite HTTP tool, and upload them into HDFS. For example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 记下与`.gkg.csv.zip`匹配的最新条目，使用您喜欢的HTTP工具进行复制，并将其上传到HDFS。例如：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that you have unzipped your CSV file and loaded it into HDFS, let's get
    on and look at the data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经解压了CSV文件并将其加载到HDFS中，让我们继续并查看数据。
- en: Note
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: It is not actually necessary to unzip data before loading to HDFS. Spark's `TextInputFormat` class
    supports compressed types and will decompress transparently. However, as we unzipped
    the content in our NiFi pipeline in the previous chapter, decompression is performed
    here for consistency.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载到HDFS之前，实际上不需要解压数据。Spark的`TextInputFormat`类支持压缩类型，并且会自动解压缩。但是，由于我们在上一章中在NiFi管道中解压了内容，为了保持一致性，这里进行了解压缩。
- en: Core global knowledge graph model
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心全球知识图模型
- en: There are some important principles to understand which will certainly save
    time in the long run, whether in terms of computing or human effort. Like many
    CSVs, this file is hiding some complexity that, if not understood well at this
    stage, could become a real problem during our large scale analytics later. The
    GDELT documentation describes the data. It can be found here: [http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf](http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些重要的原则需要理解，这将在长远来看节省时间，无论是在计算还是人力方面。就像许多CSV文件一样，这个文件隐藏了一些复杂性，如果在这个阶段不理解清楚，可能会在我们进行大规模分析时成为一个真正的问题。GDELT文档描述了数据。可以在这里找到：[http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf](http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf)。
- en: 'It indicates that each CSV line is newline delimite, and structured as in *Figure
    1*:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它表明每个CSV行都是以换行符分隔的，并且结构如*图1*所示：
- en: '![Core global knowledge graph model](img/image_03_001.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![核心全球知识图模型](img/image_03_001.jpg)'
- en: Figure 1 GDELT GKG v2.1
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 GDELT GKG v2.1
- en: On the face of it, this appears to be a nice, simple model whereby we can simply
    query a field and use the enclosed data-exactly like the CSV files we import and
    export to Microsoft Excel every day. However, if we examine the fields in more
    detail, it becomes clear that some of the fields are actually references to external
    sources and others are flattened data, actually represented by other tables.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这似乎是一个不错的简单模型，我们可以简单地查询一个字段并使用其中的数据，就像我们每天导入和导出到Microsoft Excel的CSV文件一样。然而，如果我们更详细地检查字段，就会清楚地看到一些字段实际上是对外部来源的引用，而另一些字段是扁平化的数据，实际上是由其他表表示的。
- en: Hidden complexity
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏的复杂性
- en: 'The flattened data structures in a core GKG model represent hidden complexity.
    For example, looking at field V2GCAM in the documentation, it outlines the idea
    that this is a series of comma-delimited blocks containing colon-delimited key-value
    pairs, the pairs representing GCAM variables, and their respective counts. Like
    so:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 核心GKG模型中的扁平化数据结构代表了隐藏的复杂性。例如，查看文档中的V2GCAM字段，它概述了这样一个想法，即这是一个包含冒号分隔的键值对的逗号分隔块的系列，这些对表示GCAM变量及其相应计数。就像这样：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we reference the GCAM specification, [http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT](http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT)
    we can translate this to:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们参考GCAM规范，[http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT](http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT)，我们可以将其翻译为：
- en: '![Hidden complexity](img/table.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![隐藏的复杂性](img/table.jpg)'
- en: There are also other fields that work in the same way, such as `V2Locations`,
    `V2Persons`, `V2Organizations`, and so on. So, what's really going on here? What
    are all these nested structures and why would you choose to represent data in
    this way? Actually, it turns out that this is a convenient way to collapse a **dimensional
    model** so that it can be represented in single line records without any loss
    of data or cross-referencing. In fact, it's a frequently used technique, known
    as *denormalization*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他以相同方式工作的字段，比如`V2Locations`，`V2Persons`，`V2Organizations`等等。那么，这里到底发生了什么？所有这些嵌套结构是什么意思？为什么选择以这种方式表示数据？实际上，事实证明，这是一种方便的方法，可以将*维度模型*折叠成单行记录，而不会丢失数据或交叉引用。事实上，这是一种经常使用的技术，称为*非规范化*。
- en: Denormalized models
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非规范化模型
- en: Traditionally, a dimensional model is a database table structure that comprises
    many fact and dimension tables. They are often referred to as having star or snowflake
    schemas due to their appearance in entity-relation diagrams. In such a model,
    a *fact* is a value that can be counted or summed and typically provides a measurement
    at a given point in time. As they are often based on transactions, or repeating
    events, the number of facts are prone to growing very large. A *dimension* on
    the other hand is a logical grouping of information whose purpose is to qualify
    or contextualize facts. They usually provide an entry point for interpreting facts
    by means of grouping or aggregation. Also, dimensions can be hierarchical and
    one dimension can reference another. We can see a diagram of the expanded GKG
    dimensional structure in *Figure 2*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，维度模型是一个包含许多事实和维度表的数据库表结构。它们通常被称为星型或雪花模式，因为它们在实体关系图中的外观。在这样的模型中，*事实*是一个可以计数或求和的值，通常在给定时间点提供测量。由于它们通常基于交易或重复事件，事实的数量很可能会变得非常庞大。另一方面，*维度*是信息的逻辑分组，其目的是为了限定或给事实提供背景。它们通常通过分组或聚合来解释事实的入口点。此外，维度可以是分层的，一个维度可以引用另一个维度。我们可以在*图2*中看到扩展的GKG维度结构的图表。
- en: 'In our GCAM example, the facts are the entries found in the above table, and
    the dimension is the GCAM reference itself. While this may seem like a simple,
    logical abstraction, it does mean that we have an important area of concern that
    we should consider carefully: dimensional modeling is great for traditional databases
    where data can be split into tables–in this case, GKG and GCAM tables–as these
    types of databases, by their very nature, are optimized for that structure. For
    example, the operations for looking up values or aggregating facts are available
    natively. When using Spark, however, some of the operations that we take for granted
    can be very expensive. For example, if we wanted to average all of the GCAM fields
    for millions of entries, then we would have a very large computation to perform.
    We will discuss this in more detail in the following diagram:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的GCAM示例中，事实是上表中的条目，维度是GCAM参考本身。虽然这可能看起来是一个简单的逻辑抽象，但这意味着我们有一个重要的关注点需要仔细考虑：维度建模对于传统数据库非常适用，其中数据可以分割成表
    - 在这种情况下，GKG和GCAM表 - 因为这些类型的数据库，本质上是针对这种结构进行了优化。例如，查找值或聚合事实的操作是本地可用的。然而，在使用Spark时，我们认为理所当然的一些操作可能非常昂贵。例如，如果我们想要对数百万条条目的所有GCAM字段进行平均，那么我们将有一个非常庞大的计算任务。我们将在下图中更详细地讨论这个问题：
- en: '![Denormalized models](img/image_03_002.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![非规范化模型](img/image_03_002.jpg)'
- en: Figure 2 GDELT GKG 2.1 expanded
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 GDELT GKG 2.1扩展
- en: Challenges with flattened data
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扁平化数据的挑战
- en: Having explored the GKG data schema, we now know that the taxonomy is a typical
    star schema with a single fact table referencing multiple dimension tables. With
    this hierarchical structure, we will certainly struggle should we need to slice-and-dice
    data in the same way a traditional database would allow.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了GKG数据模式之后，我们现在知道，这个分类法是一个典型的星型模式，有一个单一的事实表引用多个维度表。有了这种层次结构，如果我们需要以与传统数据库相同的方式切片和切块数据，我们肯定会遇到困难。
- en: But what makes it so difficult to process on Spark? Let's look at three different
    issues inherent with this type of organization.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，是什么让在Spark上处理变得如此困难呢？让我们来看看这种类型组织固有的三个不同问题。
- en: Issue 1 - Loss of contextual information
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题1 - 上下文信息的丢失
- en: First, there is the matter of the various arrays used within each record of
    the dataset. For example, `V1Locations`, `V1Organizations`, and `V1Persons` fields
    all contain a list of 0 or more objects. As we do not have the original body of
    the text used to derive this information (although we can sometimes obtain it
    if the source is WEB, JSTOR, and so on, since those will contain links to the
    source document), we lose the context of the relationships between the entities.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，数据集中的每条记录中使用的各种数组是一个问题。例如，`V1Locations`，`V1Organizations`和`V1Persons`字段都包含一个或多个对象的列表。由于我们没有用于获取此信息的原始文本（尽管有时我们可以获取到，如果来源是WEB、JSTOR等，因为这些将包含指向源文件的链接），我们失去了实体之间关系的上下文。
- en: For example, if we have [Barack Obama, David Cameron, Francois Hollande, USA,
    France, GB, Texaco, Esso, Shell] in our data, then we could make the assumption
    that the source article is related to a meeting between heads of state over an
    oil crisis. However, this is only an assumption and may not be the case, if we
    were truly objective, we could equally assume that the article was related to
    companies who had employees with famous names.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的数据中有[Barack Obama, David Cameron, Francois Hollande, USA, France, GB,
    Texaco, Esso, Shell]，那么我们可以假设这篇文章与一场关于石油危机的国家元首会议有关。然而，这只是一个假设，也许并非如此，如果我们真的客观，我们同样可以假设这篇文章与拥有著名名字的公司有关。
- en: To help us to infer these relationships between entities, we can develop a time
    series model that takes all of the individual contents of a GDELT field, over
    a certain time period, and performs an expansion join. Thus, on a simple level,
    those pairs that are seen more often are more likely to actually relate to each
    other and we can start to make some more concrete assumptions. For example, if
    we see [Barack Obama, USA] 100,000 times in our timeseries and [Barack Obama,
    France] only 5000 times, then it is very likely that there is a strong relationship
    between the first pair, and a secondary relationship between the second. In other
    words, we can identify the tenuous relationships and remove them when needed.
    This method can be used at scale to identify relationships between apparently
    unrelated entities. In [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"), *Building
    Communities*, we use this principle to identify relationships between some very
    unlikely people!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们推断实体之间的关系，我们可以开发一个时间序列模型，它接受一定时间段内GDELT字段的所有个体内容，并执行扩展连接。因此，在简单的层面上，那些更常见的对可能更有关联，我们可以开始做一些更具体的假设。例如，如果我们在时间序列中看到[Barack
    Obama, USA]出现了10万次，而[Barack Obama, France]只出现了5000次，那么第一对之间很可能存在强关系，而第二对之间存在次要关系。换句话说，我们可以识别脆弱的关系，并在需要时移除它们。这种方法可以被用来在规模上识别明显无关的实体之间的关系。在[第7章](ch07.xhtml
    "第7章 建立社区"), *建立社区*中，我们使用这个原则来识别一些非常不太可能的人之间的关系！
- en: 'Issue 2: Re-establishing dimensions'
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题2：重新建立维度
- en: 'With any denormalized data it should be possible to reconstruct, or inflate,
    the original dimensional model. With this in mind, let''s look at a useful Spark
    function that will help us to expand our arrays and produce a flattened result;
    it''s called `DataFrame.explode`, and here''s an illustrative example:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何非规范化的数据，应该可以重建或膨胀原始的维度模型。考虑到这一点，让我们来看一个有用的Spark函数，它将帮助我们扩展数组并产生一个扁平化的结果；它被称为`DataFrame.explode`，下面是一个说明性的例子：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using this method, we can easily expand arrays and then perform the grouping
    of our choice. Once expanded, the data is readily aggregated using the `DataFrame`
    methods and can even be done using SparkSQL. An example of this can be found in
    the Zeppelin notebooks in our repository.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我们可以轻松扩展数组，然后执行我们选择的分组。一旦扩展，数据就可以使用`DataFrame`方法轻松聚合，甚至可以使用SparkSQL进行。我们的存储库中的Zeppelin笔记本中可以找到一个例子。
- en: It is important to understand that, while this function is simple to implement,
    it is not necessarily performant and may hide the underlying processing complexity
    required. In fact, there is an example of the explode function using GKG data
    within the Zeppelin notebook that accompanies this chapter, whereby, if the explode
    functions are not reasonably scoped, then the function returns a heap space issue
    as it runs out of memory.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，虽然这个函数很容易实现，但不一定高效，并且可能隐藏所需的底层处理复杂性。事实上，在本章附带的Zeppelin笔记本中有一个使用GKG数据的explode函数的例子，如果explode函数的范围不合理，那么函数会因为内存耗尽而返回堆空间问题。
- en: This function does not solve the inherent problem of consuming large amounts
    of system resources, and so you should still take care when using it. And while
    this general problem cannot be solved, it can be managed by performing only the
    groupings and joins necessary, or by calculating them ahead of time and ensuring
    they complete within the resources available. You may even wish to write an algorithm
    that splits a dataset and performs the grouping sequentially, persisting each
    time. We explore methods to help us with this problem, and other common processing
    issues, in [Chapter 14](ch14.xhtml "Chapter 14. Scalable Algorithms"), *Scalable
    Algorithms*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数并不能解决消耗大量系统资源的固有问题，因此在使用时仍需小心。虽然这个一般性问题无法解决，但可以通过仅执行必要的分组和连接，或者提前计算它们并确保它们在可用资源内完成来进行管理。你甚至可能希望编写一个算法，将数据集拆分并按顺序执行分组，每次持久化。我们在[第14章](ch14.xhtml
    "第14章 可扩展算法")中探讨了帮助我们解决这个问题以及其他常见处理问题的方法，*可扩展算法*。
- en: 'Issue 3: Including reference data'
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题3：包含参考数据
- en: 'For this issue, let''s look at the GDELT event data, which we have expanded
    in *Figure 3*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，让我们来看一下我们在*图3*中扩展的GDELT事件数据：
- en: '![Issue 3: Including reference data](img/image_03_003.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![问题3：包含参考数据](img/image_03_003.jpg)'
- en: Figure 3 GDELT Events Taxonomy
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图3 GDELT事件分类
- en: 'This type of diagrammatic representation draws attention to the relationships
    in the data and gives an indication of how we might want to inflate it. Here,
    we see many fields that are just codes and would require translation back into
    their original descriptions in order to present anything meaningful. For example,
    in order to interpret the `Actor1CountryCode` (GDELT events), we will need to
    join the event data with one or more separate reference datasets that provide
    the translation text. In this case, the documentation tells us to reference the
    CAMEO dataset located here: [http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf](http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种图解表示方式引起了对数据关系的关注，并表明了我们可能希望如何扩展它。在这里，我们看到许多字段只是代码，需要将其翻译回原始描述，以呈现有意义的内容。例如，为了解释`Actor1CountryCode`（GDELT事件），我们需要将事件数据与一个或多个提供翻译文本的单独参考数据集进行连接。在这种情况下，文档告诉我们参考位于这里的CAMEO数据集：[http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf](http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf)。
- en: This type of join has always presented a serious problem at data scale and there
    are various ways to handle it depending upon the given scenario - it is important
    at this stage to understand exactly how your data will be used, which joins may
    be required immediately, and which may be deferred until sometime in the future.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的连接在数据规模上一直存在严重问题，并且根据给定的情况有各种处理方法-在这个阶段重要的是要准确了解您的数据将如何使用，哪些连接可能需要立即执行，哪些可能推迟到将来的某个时候。
- en: 'In the case where we choose to completely denormalize, or flatten, the data
    before processing, then it makes sense to do the join upfront. In this case, follow-on
    analytics will certainly be more efficient, as the relevant joins have already
    been completed:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们选择在处理之前完全去规范化或展开数据的情况下，提前进行连接是有意义的。在这种情况下，后续的分析肯定会更有效，因为相关的连接已经完成：
- en: 'So, in our example:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的例子中：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For each code in the record, there is a join to the respective reference table,
    and the entire record becomes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于记录中的每个代码，都要连接到相应的参考表，整个记录变为：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is a simple change, but is one that uses a lot of disk space if performed
    across large numbers of rows. The trade-off is that the joins have to be performed
    at some point, perhaps at ingest or as a regular batch job after ingest; it is
    perfectly reasonable to ingest the data as is, and perform flattening of the dataset
    at a time that is convenient to the user. In any case, the flattened data can
    be consumed by any analytic and data analysts need not concern themselves with
    this potentially hidden issue.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的改变，但如果在大量行上执行，会占用大量磁盘空间。权衡的是，连接必须在某个时刻执行，可能是在摄取时或在摄取后作为定期批处理作业；将数据摄取为原样，并在方便用户的时候对数据集进行展开是完全合理的。无论如何，展开的数据可以被任何分析工具使用，数据分析师不需要关注这个潜在的隐藏问题。
- en: On the other hand, often, deferring the join until later in the processing can
    mean that there are fewer records to join with – as there may have been aggregation
    steps in the pipeline. In this case, joining to tables at the last possible opportunity
    pays off because, often, the reference or dimension tables are small enough to
    be broadcast joins, or map-side joins. As this is such an important topic, we
    will continue to look at different ways of approaching join scenarios throughout
    the book.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，通常，推迟连接直到处理的后期可能意味着要连接的记录较少-因为可能在管道中有聚合步骤。在这种情况下，尽可能晚地连接到表是值得的，因为通常参考或维度表足够小，可以进行广播连接或映射端连接。由于这是一个如此重要的主题，我们将继续在整本书中探讨不同的处理连接场景的方法。
- en: Loading your data
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载您的数据
- en: 'As we have outlined in previous chapters, traditional system engineering commonly
    adopts a pattern to move the data from its source to its destination, that is,
    ETL, whereas Spark tends to rely on schema-on-read. As it''s important to understand
    how these concepts relate to schemas and input formats, let''s describe this aspect
    in more detail:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中所概述的，传统的系统工程通常采用一种模式，将数据从其源移动到其目的地，即ETL，而Spark倾向于依赖于读时模式。由于重要的是理解这些概念与模式和输入格式的关系，让我们更详细地描述这个方面：
- en: '![Loading your data](img/B05261_03_04.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![加载您的数据](img/B05261_03_04.jpg)'
- en: On the face of it, the ETL approach seems to be sensible, and indeed has been
    implemented by just about every organization that stores and handles data. There
    are some very popular, feature-rich products out there that perform the ETL task
    very well - not to mention Apache's open source offering, Apache Camel [http://camel.apache.org/etl-example.html](http://camel.apache.org/etl-example.html).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上看，ETL方法似乎是合理的，事实上，几乎每个存储和处理数据的组织都已经实施了这种方法。有一些非常受欢迎、功能丰富的产品非常擅长执行ETL任务-更不用说Apache的开源产品Apache
    Camel了[http://camel.apache.org/etl-example.html](http://camel.apache.org/etl-example.html)。
- en: 'However, this apparently straightforward approach belies the true effort required
    to implement even a simple data pipeline. This is because we must ensure that
    all data complies with a fixed schema before we can use it. For example, if we
    wanted to ingest some data from a starting directory, the minimal work is as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种表面上简单的方法掩盖了实施甚至简单数据管道所需的真正努力。这是因为我们必须确保所有数据在使用之前都符合固定的模式。例如，如果我们想要从起始目录摄取一些数据，最小的工作如下：
- en: Ensure we are always looking at the pickup directory.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保我们始终关注接送目录。
- en: When data arrives, collect it.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据到达时，收集它。
- en: Ensure the data is not missing anything and validate according to a predefined
    ruleset.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保数据没有遗漏任何内容，并根据预定义的规则集进行验证。
- en: Extract the parts of the data that we are interested in, according to a predefined
    ruleset.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据预定义的规则集提取我们感兴趣的数据部分。
- en: Transform these selected parts according to a predefined schema.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据预定义的模式转换这些选定的部分。
- en: Load the data to a repository (for example, a database) using the correct versioned
    schema.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正确的版本化模式将数据加载到存储库（例如数据库）。
- en: Deal with any failed records.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理任何失败的记录。
- en: 'We can immediately see a number of formatting issues here that must be addressed:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即可以看到这里有一些必须解决的格式问题：
- en: We have a predefined ruleset and, therefore, this must be version controlled.
    Any mistakes will mean bad data in the end database and a re-ingest of that data
    through the ETL process to correct it (very time and resource expensive). Any
    change to the format of the inbound dataset, and this ruleset must be changed.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有一个预定义的规则集，因此必须进行版本控制。任何错误都将意味着最终数据库中存在错误数据，并且需要通过ETL过程重新摄入数据以进行更正（非常耗时和资源密集）。对入站数据集格式的任何更改都将导致此规则集的更改。
- en: Any change to the target schema will require very careful management. At the
    very least, a version control change in the ETL, and possibly even a reprocessing
    of some or all of the previous data (which could be a very time consuming and
    expensive backhaul).
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对目标模式的任何更改都需要非常谨慎的管理。至少，ETL中需要进行版本控制的更改，甚至可能需要重新处理之前的一些或全部数据（这可能是一个非常耗时和昂贵的回程）。
- en: Any change to the end repository will result in at least a version control schema
    change, and perhaps even a new ETL module (again, very time and resource intensive).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对终端存储库的任何更改都将导致至少一个版本控制模式的更改，甚至可能是一个新的ETL模块（再次非常耗时和资源密集）。
- en: Inevitably, there will be some bad data that makes it through to the database.
    Therefore, an administrator will need set rules to monitor the referential integrity
    of tables to ensure damage is kept to a minimum and arrange for the re-ingestion
    of any corrupted data.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不可避免地，会有一些错误数据进入数据库。因此，管理员需要制定规则来监控表的引用完整性，以确保损坏最小化，并安排重新摄入任何损坏的数据。
- en: If we now consider these issues and massively increase the volume, velocity,
    variety, and veracity of the data, it is easy to see that our straightforward
    ETL system has quickly grown into a near unmanageable system. Any formatting,
    schema, and business rule changes will have a negative impact. In some cases,
    there may not be enough processor and memory resources to even keep pace, due
    to all the processing steps required. Data cannot be ingested until all of the
    ETL steps have been agreed and are in place. In large corporations it can take
    months to agree schema transforms before any implementation even commences, thus
    resulting in a large backlog, or even loss of data. All this results in a brittle
    system that is difficult to change.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在考虑这些问题，并大幅增加数据的数量、速度、多样性和真实性，很容易看出我们简单的ETL系统已经迅速发展成一个几乎无法管理的系统。任何格式、模式和业务规则的更改都将产生负面影响。在某些情况下，甚至可能没有足够的处理器和内存资源来跟上，因为需要进行所有的处理步骤。在所有ETL步骤达成一致并就位之前，数据无法被摄入。在大型公司中，可能需要数月时间来达成模式转换的一致意见，然后才能开始任何实施，从而导致大量积压，甚至丢失数据。所有这些都导致了一个难以改变的脆弱系统。
- en: Schema agility
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式敏捷性
- en: 'To overcome this, schema-on-read encourages us to shift to a very simple principle:
    *apply schema to the data at runtime, as opposed to applying it on load (that
    is, at ingest)*. In other words, a schema is applied to the data when it is read
    in for processing. This simplifies the ETL process somewhat:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一点，基于读取的模式鼓励我们转向一个非常简单的原则：*在运行时对数据应用模式，而不是在加载时应用模式（即，在摄入时）*。换句话说，当数据被读取进行处理时，会对数据应用模式。这在某种程度上简化了ETL过程：
- en: '![Schema agility](img/B05261_03_05.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![模式敏捷性](img/B05261_03_05.jpg)'
- en: Of course, it does not mean you eliminate the transform step entirely. You're
    simply *deferring* the act of validation, applying business rules, error handling,
    ensuring referential integrity, enriching, aggregating, and otherwise inflating
    the model until the point you are ready to use it. The idea is that, by this point,
    you should know more about the data and certainly about the way you wish to use
    it. Therefore, you can use this increased knowledge of the data to effect efficiencies
    in the loading method. Again, this is a trade-off. What you save in upfront processing
    costs, you may lose in duplicate processing and potential inconsistency. However,
    techniques such as persistence, indexing, memorization, and caching can all help
    here. As mentioned in the previous chapter, this process is commonly known as
    ELT due to the reversal in the order of processing steps.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这并不意味着你完全消除了转换步骤。你只是*推迟*了验证、应用业务规则、错误处理、确保引用完整性、丰富、聚合和其他膨胀模型的行为，直到你准备使用它的时候。这个想法是，到了这个时候，你应该对数据有更多了解，当然也对你希望使用数据的方式有更多了解。因此，你可以利用对数据的增加了解来提高加载方法的效率。同样，这是一个权衡。你在前期处理成本上节省的部分，可能会在重复处理和潜在的不一致性上损失。然而，持久化、索引、记忆和缓存等技术都可以在这方面提供帮助。正如前一章所述，这个过程通常被称为ELT，因为处理步骤的顺序发生了逆转。
- en: One benefit of this approach is that it allows greater freedom to make appropriate
    decisions about the way you represent and model data for any given use case. For
    example, there are a variety of ways that data can be structured, formatted, stored,
    compressed, or serialized, and it makes sense to choose the most appropriate method
    given the set of specific requirements related to the particular problem you are
    trying to solve.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个好处是，它允许更大的自由度，以便对数据的表示和建模方式做出适当的决策，以满足特定用例的相关特定要求。例如，数据可以以各种方式进行结构化、格式化、存储、压缩或序列化，因此选择最合适的方法是有意义的，考虑到你试图解决的特定问题集。
- en: One of the most important opportunities that this approach provides is that
    you can choose how to physically lay out the data, that is, decide on the directory
    structure where data is kept. It is generally not advised to store all your data
    in a single directory because, as the number of files grows, it takes punitively
    longer amounts of time for the underlying filesystem to address them. But, ideally,
    we want to be able to specify the smallest possible data split to fulfill the
    functionality and efficiently store and retrieve at the volumes required. Therefore,
    data should be logically grouped depending upon the analysis that is required
    and the amount of data that you expect to receive. In other words, data may be
    divided across directories based upon type, subtype, date, time, or some other
    relevant property, but it should be ensured that no single directory bears undue
    burden. Another important point to realize here is that, once the data is landed,
    it can always be reformatted or reorganized at a later date, whereas, in an ETL
    paradigm, this is usually far more difficult.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法提供的最重要的机会之一是你可以选择如何物理布置数据，也就是决定数据存放的目录结构。通常不建议将所有数据存储在单个目录中，因为随着文件数量的增长，底层文件系统需要更长的时间来处理它们。但是，理想情况下，我们希望能够指定最小可能的数据拆分，以满足功能需求并有效地存储和检索所需的数据量。因此，数据应根据所需的分析和预期接收的数据量进行逻辑分组。换句话说，数据可以根据类型、子类型、日期、时间或其他相关属性分成不同的目录，但必须确保没有单个目录承担过重的负担。另一个重要的观点是，一旦数据落地，就可以在以后重新格式化或重新组织，而在ETL范式中，这通常更加困难。
- en: In addition to this, ELT can also have a surprising benefit on **change management**
    and **version control**. For example, if external factors cause the data schema
    to change, you can simply load different data to a new directory in your data
    store and use a flexible schema tolerant serialization library, such as Avro or
    Parquet, which both support **schema evolution** (we will look at these later
    in this chapter); or, if the results of a particular job are unsatisfactory, we
    need only change the internals of that one job before rerunning it. This means
    that schema changes become something that can be managed on a per analytic basis,
    rather than on a per feed basis, and the impact of change is better isolated and
    managed.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ELT还可以对**变更管理**和**版本控制**产生意想不到的好处。例如，如果外部因素导致数据架构发生变化，您可以简单地将不同的数据加载到数据存储的新目录中，并使用灵活的模式容忍序列化库，如Avro或Parquet，它们都支持**模式演化**（我们将在本章后面讨论这些）；或者，如果特定作业的结果不尽人意，我们只需要更改该作业的内部，然后重新运行它。这意味着模式更改变成了可以根据每个分析进行管理，而不是根据每个数据源进行管理，变更的影响得到了更好的隔离和管理。
- en: By the way, it's worth considering a hybrid approach, particularly useful in
    streaming use cases, whereby some processing can be done during collection and
    ingest, and others during runtime. The decision around whether to use ETL or ELT
    is not necessarily a binary one. Spark provides features that give you control
    over your data pipelines. In turn, this affords you the flexibility to transform
    or persist data when it makes sense to do so, rather than adopting a one-size-fits-all
    approach.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，值得考虑一种混合方法，特别适用于流式使用情况，即在收集和摄取过程中可以进行一些处理，而在运行时可以进行其他处理。关于使用ETL或ELT的决定并不一定是二元的。Spark提供了功能，让您控制数据管道。这为您提供了在合适的时候转换或持久化数据的灵活性，而不是采用一刀切的方法。
- en: The best way to determine which approach to take is to learn from the actual
    day-to-day use of a particular dataset and adjust its processing accordingly,
    identifying bottlenecks and fragility as more experience is gained. There may
    also be corporate rules levied, such as virus scanning or data security, which
    will determine a particular route. We'll look more into this at the end of the
    chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 确定采取哪种方法的最佳方式是从特定数据集的实际日常使用中学习，并相应地调整其处理，随着经验的积累，识别瓶颈和脆弱性。还可能会有公司规定，如病毒扫描或数据安全，这将决定特定的路线。我们将在本章末尾更深入地讨论这一点。
- en: Reality check
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现实检验
- en: As with most things in computing, there's no silver bullet. ELT and schema-on-read
    will not fix all your data formatting problems, but they are useful tools in the
    toolbox and, generally speaking, the pros usually outweigh the cons. It is worth
    noting, however, that there are situations where you can actually introduce difficulties
    if you're not careful.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算机中的大多数事物一样，没有银弹。ELT和基于读取的模式不会解决所有数据格式化问题，但它们是工具箱中有用的工具，一般来说，优点通常大于缺点。然而，值得注意的是，如果不小心，有时会引入困难的情况。
- en: In particular, it can be more involved to perform ad hoc analysis on complex
    data models (as opposed to in databases). For example, in the simple case of extracting
    a list of all of the names of the cities mentioned in news articles, in a SQL
    database you could essentially run `select CITY from GKG`, whereas, in Spark,
    you first need to understand the data model, parse and validate the data, and
    then create the relevant table and handle any errors on-the-fly, sometimes each
    time you run the query.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在复杂数据模型上执行临时分析可能更加复杂（与数据库相比）。例如，在简单情况下，从新闻文章中提取提到的所有城市的名称列表，在SQL数据库中，您可以基本上运行`select
    CITY from GKG`，而在Spark中，您首先需要了解数据模型，解析和验证数据，然后创建相关表并在运行时处理任何错误，有时每次运行查询都要这样做。
- en: Again, this is a trade-off. With schema-on-read you lose the built-in data representation
    and inherent knowledge of a fixed schema, but you gain the flexibility to apply
    different models or views as required. As usual, Spark provides features designed
    to assist in exploiting this approach, such as, transformations, `DataFrames`,
    `SparkSQL`, and REPL, and when used properly, they allow you to maximize the benefits
    of schema-on-read. We'll learn more about this as we go furthur.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这是一个权衡。使用schema-on-read，您失去了内置的数据表示和固定模式的固有知识，但您获得了根据需要应用不同模型或视图的灵活性。像往常一样，Spark提供了旨在帮助利用这种方法的功能，例如转换、`DataFrames`、`SparkSQL`和REPL，当正确使用时，它们允许您最大限度地利用schema-on-read的好处。随着我们的学习，我们将进一步了解这一点。
- en: GKG ELT
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GKG ELT
- en: As our NiFi pipeline writes data as is to HDFS, we can take full advantage of
    schema-on-read and immediately start to use it without having to wait for it to
    be processed. If you would like to be a bit more advanced, then you could load
    the data in a splittable and/or zipped format such as `bzip2` (native to Spark).
    Let's take a look at a simple example.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的NiFi管道将数据原样写入HDFS，我们可以充分利用schema-on-read，并立即开始使用它，而无需等待它被处理。如果您想要更加先进，那么您可以以可分割和/或压缩的格式（例如`bzip2`，Spark原生支持）加载数据。让我们看一个简单的例子。
- en: Note
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: HDFS uses a block system to store data. In order to store and leverage data
    in the most efficient way, HDFS files should be splittable where possible. If
    the CSV GDELT files are loaded using `TextOutputFormat` class, for example, then
    files larger than the block size will be split across filesize/blocksize blocks.
    Partial blocks do not occupy a full block size on disk.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS使用块系统来存储数据。为了以最有效的方式存储和利用数据，HDFS文件应尽可能可分割。例如，如果使用`TextOutputFormat`类加载CSV
    GDELT文件，那么大于块大小的文件将被分割成文件大小/块大小的块。部分块不会占据磁盘上的完整块大小。
- en: By using `DataFrames`, we can write SQL statements to explore the data or with
    datasets we can chain fluent methods, but in either case there is some initial
    preparation required.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`DataFrames`，我们可以编写SQL语句来探索数据，或者使用数据集我们可以链接流畅的方法，但在任何情况下都需要一些初始准备。
- en: 'The good news is that usually this can be done entirely by Spark, as it supports
    the transparent loading of data into Datasets via case classes, using **Encoders**
    and so most of the time you won''t need to worry too much about the inner workings.
    Indeed, when you have a relatively simple data model, it''s usually enough to
    define a case class, map your data onto it, and convert to a dataset using `toDS`
    method. However, in most real-world scenarios, where data models are more complex,
    you will be required to write your own custom parser. Custom parsers are nothing
    new in data engineering, but in a schema-on-read setting, they are often required
    to be used by data scientists, as the interpretation of data is done at runtime
    and not load time. Here''s an example of the use of the custom GKG parser to be
    found in our repository:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，通常这可以完全由Spark完成，因为它支持通过case类将数据透明地加载到数据集中，使用**Encoders**，所以大部分时间您不需要过多地担心内部工作。事实上，当您有一个相对简单的数据模型时，通常定义一个case类，将数据映射到它，并使用`toDS`方法转换为数据集就足够了。然而，在大多数现实世界的场景中，数据模型更复杂，您将需要编写自己的自定义解析器。自定义解析器在数据工程中并不新鲜，但在schema-on-read设置中，它们通常需要被数据科学家使用，因为数据的解释是在运行时而不是加载时完成的。以下是我们存储库中可找到的自定义GKG解析器的使用示例：
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can seen preceding that, once the data is parsed, it can be used in the
    full variety of Spark APIs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，一旦数据被解析，它可以在各种Spark API中使用。
- en: 'If you''re more comfortable using SQL, you can define your own schema, register
    a table, and use SparkSQL. In either approach, you can choose how to load the
    data based on how it will be used, allowing for more flexibility over which aspects
    you spend time parsing. For example, the most basic schema for loading GKG is
    to treat every field as a String, like so:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您更喜欢使用SQL，您可以定义自己的模式，注册一个表，并使用SparkSQL。在任何一种方法中，您都可以根据数据的使用方式选择如何加载数据，从而更灵活地决定您花费时间解析的方面。例如，加载GKG的最基本模式是将每个字段都视为字符串，就像这样：
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And now you can execute SQL queries, like so:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以执行SQL查询，就像这样：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With this approach, you can start profiling the data straight away and it's
    useful for many data engineering tasks. When you're ready, you can choose other
    elements of the GKG record to expand. We'll see more about this in the next chapter.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法，您可以立即开始对数据进行概要分析，这对许多数据工程任务都是有用的。当您准备好时，您可以选择GKG记录的其他元素进行扩展。我们将在下一章中更多地了解这一点。
- en: 'Once you have a DataFrame, you can convert it into a Dataset by defining a
    case class and casting, like so:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了一个DataFrame，你可以通过定义一个case类和转换来将其转换为一个Dataset，就像这样：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Position matters
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置很重要
- en: It's worth noting here that, when loading data from CSV, Spark's schema matching
    is entirely *positional*. This means that, when Spark tokenizes a record based
    on the given separator, it assigns each token to a field in the schema using its
    position, even if a header is present. Therefore, if a column is omitted in the
    schema definition, or your dataset changes over time due to data drift or data
    versioning, you may get a misalignment that Spark will not necessarily warn you
    about!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，当从CSV加载数据时，Spark的模式匹配完全是*位置*的。这意味着，当Spark根据给定的分隔符对记录进行标记时，它将根据其位置将每个标记分配给模式中的一个字段，即使存在标题。因此，如果在模式定义中省略了一个列，或者由于数据漂移或数据版本化而导致数据集随时间变化，您可能会遇到Spark不一定会警告您的错位！
- en: 'Therefore, we recommend doing basic data profiling and data quality checks
    on a routine basis to mitigate these situations. You can use the built-in functions
    in `DataFrameStatFunctions` to assist with this. Some examples are shown as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们建议定期进行基本数据概要和数据质量检查，以减轻这些情况。您可以使用`DataFrameStatFunctions`中的内置函数来协助处理这些情况。一些示例如下所示：
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next, let's explain a great way to put some structure around our code, and also
    reduce the amount of code written, by using Avro or Parquet.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们解释一种很好的方法来给我们的代码加上一些结构，并通过使用Avro或Parquet来减少编写的代码量。
- en: Avro
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Avro
- en: We have seen how easy it can be to ingest some data and use Spark to analyze
    it without the need for any traditional ETL tools. While it is very useful to
    work in an environment where schemas are all but ignored, this is not realistic
    in the commercial world. There is, however, a good middle ground, which gives
    us some great advantages over both ETL and unbounded data processing-Avro.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何轻松地摄取一些数据并使用Spark进行分析，而无需任何传统的ETL工具。在一个几乎忽略所有模式的环境中工作非常有用，但这在商业世界中并不现实。然而，有一个很好的折中方案，它比ETL和无限数据处理都有很大的优势-Avro。
- en: 'Apache Avro is serialization technology, similar in purpose to Google''s protocol
    buffers. Like many other serialization technologies, Avro uses a schema to describe
    data, but the key to its usefulness is that it provides the following features:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Avro是一种序列化技术，类似于Google的协议缓冲。与许多其他序列化技术一样，Avro使用模式来描述数据，但其有用性的关键在于它提供了以下功能：
- en: '**It stores the schema alongside the data**. This allows for efficient storage
    because the schema is only stored once, at the top of the file. It also means
    that data can be read even if the original class files are no longer available.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它将模式与数据一起存储**。这样可以有效地存储，因为模式只存储一次，位于文件顶部。这也意味着即使原始类文件不再可用，也可以读取数据。'
- en: '**It supports schema-on-read and schema evolution**. This means it can implement
    different schemas for the reading and writing of data, providing the advantages
    of schema versioning without the disadvantages of large administrative overhead
    every time we wish to make data amendments.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它支持读取时模式和模式演变**。这意味着它可以实现不同的模式来读取和写入数据，提供了模式版本控制的优势，而不会带来大量的行政开销，每次我们希望进行数据修改时。'
- en: '**It is language agnostic**. Therefore, it can be used with any tool or technology
    that allows custom serialization framework. It is particularly useful for writing
    directly to Hive, for example.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它是与语言无关的**。因此，它可以与允许自定义序列化框架的任何工具或技术一起使用。例如，直接写入Hive时特别有用。'
- en: As Avro stores the schema with the enclosed data, it is *self-describing*. So
    instead of struggling to read the data because you have no classes, or trying
    to guess which version of a schema applies, or in the worst case having to throw
    away the data altogether, we can simply interrogate the Avro file for the schema
    that the data was written with.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Avro将模式与封闭数据一起存储，它是*自描述的*。因此，我们可以简单地查询Avro文件，以获取写入数据的模式，而不是因为没有类而难以读取数据，或者尝试猜测哪个版本的模式适用，或者在最坏的情况下不得不放弃数据。
- en: Avro also allows amendments to a schema in the form of additive changes, or
    appends, that can be accommodated thus making a specific implementation backwards
    compatible with older data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Avro还允许以添加更改或附加的形式对模式进行修订，从而可以容纳这些更改，使特定实现向后兼容旧数据。
- en: As Avro represents data in a binary form, it can be transferred and manipulated
    more efficiently. Also, it takes up less space on disk due to its inherent compression.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Avro以二进制形式表示数据，因此可以更有效地传输和操作。此外，由于其固有的压缩，它在磁盘上占用的空间更少。
- en: For the reasons stated above, Avro is an incredibly popular serialization format,
    used by a wide variety of technologies and end-systems, and you will no doubt
    have cause to use it at some point. Therefore, in the next sections we will demonstrate
    two different ways to read and write Avro-formatted data. The first is an elegant
    and simple method that uses a third party, purpose-built library, called `spark-avro`,
    and the second is an under-the-covers method, useful for understanding how the
    mechanics of Avro work.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述原因，Avro是一种非常流行的序列化格式，被广泛用于各种技术和终端系统，您无疑会在某个时候使用它。因此，在接下来的几节中，我们将演示读取和写入Avro格式数据的两种不同方法。第一种是一种优雅而简单的方法，使用一个名为`spark-avro`的第三方专门构建的库，第二种是一种底层方法，有助于理解Avro的工作原理。
- en: Spark-Avro method
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark-Avro方法
- en: 'To address the complexities of implementing Avro, the `spark-avro` library
    has been developed. This can be imported in the usual ways, using maven:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决实现Avro的复杂性，开发了`spark-avro`库。这可以像往常一样使用maven导入：
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For this implementation, we will create the Avro schema using a `StructType`
    object, transform the input data using an `RDD`, and create a `DataFrame` from
    the two. Finally, the result can be written to file, in Avro format, using the
    `spark-avro` library.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实现，我们将使用`StructType`对象创建Avro模式，使用`RDD`转换输入数据，并从中创建一个`DataFrame`。最后，可以使用`spark-avro`库将结果以Avro格式写入文件。
- en: 'The `StructType` object is a variation on the `GkgCoreSchema` used above and
    in [Chapter 4](ch04.xhtml "Chapter 4. Exploratory Data Analysis"), *Exploratory
    Data Analysis*, and is constructed as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`StructType`对象是上面使用的`GkgCoreSchema`的变体，在[第4章](ch04.xhtml "第4章。探索性数据分析")中也是如此，*探索性数据分析*，构造如下：'
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We have used a number of custom `StructTypes`, which could be specified inline
    for `GkgSchema`, but which we have broken out for ease of reading.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了许多自定义`StructTypes`，可以为`GkgSchema`内联指定，但为了便于阅读，我们已经将它们拆分出来。
- en: 'For example, `GkgRecordIdStruct` is:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`GkgRecordIdStruct`是：
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Before we use this schema, we must first produce an `RDD` by parsing the input
    GDELT data into a `Row`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用此模式之前，我们必须首先通过解析输入的GDELT数据生成一个`RDD`：
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here you see a number of custom parsing functions, for instance, `createGkgRecordID`,
    that take raw data and contain the logic for reading and interpreting each field.
    As GKG fields are complex and often contain *nested data structures*, we need
    a way to embed them into the `Row`. To help us out, Spark allows us to treat them
    as `Rows` inside `Rows`. Therefore, we simply write parsing functions that return
    `Row` objects, like so:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到许多自定义解析函数，例如`createGkgRecordID`，它接受原始数据并包含读取和解释每个字段的逻辑。由于GKG字段复杂且通常包含*嵌套数据结构*，我们需要一种将它们嵌入`Row`中的方法。为了帮助我们，Spark允许我们将它们视为`Rows`内部的`Rows`。因此，我们只需编写返回`Row`对象的解析函数，如下所示：
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Putting the code together, we see the entire solution in just a few lines:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码放在一起，我们可以在几行代码中看到整个解决方案：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Reading the Avro files into a `DataFrame` is similarly simple:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 将Avro文件读入`DataFrame`同样简单：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This gives a neat solution for dealing with Avro files, but what's going on
    under the covers?
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这为处理Avro文件提供了一个简洁的解决方案，但在幕后发生了什么呢？
- en: Pedagogical method
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教学方法
- en: In order to explain how Avro works, let's take a look at a roll your own solution.
    In this case, the first thing we need to do is to create an Avro schema for the
    version or versions of data that we intend to ingest.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释Avro的工作原理，让我们来看一个自定义解决方案。在这种情况下，我们需要做的第一件事是为我们打算摄取的数据版本或版本创建Avro模式。
- en: 'There are Avro implementations for several languages, including Java. These
    implementations allow you to generate bindings for Avro so that you can serialize
    and deserialize your data objects efficiently. We are going to use a maven plugin
    to help us automatically compile these bindings using an Avro IDL representation
    of the GKG schema. The bindings will be in the form of a Java class that we can
    use later on to help us build Avro objects. Use the following imports in your
    project:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种语言的Avro实现，包括Java。这些实现允许您为Avro生成绑定，以便您可以高效地序列化和反序列化数据对象。我们将使用一个maven插件来帮助我们使用GKG模式的Avro
    IDL表示自动编译这些绑定。这些绑定将以Java类的形式存在，我们以后可以使用它们来帮助我们构建Avro对象。在您的项目中使用以下导入：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can now take a look at our Avro IDL schema created from a subset of the
    available Avro types:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看一下我们从可用Avro类型的子集创建的Avro IDL模式：
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: val inputFile = new File("gkg.csv");
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: val inputFile = new File（“gkg.csv”）;
- en: val outputFile = new File("gkg.avro");
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: val outputFile = new File（“gkg.avro”）;
- en: val userDatumWriter = new
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: val userDatumWriter = new
- en: SpecificDatumWriter[Specification](classOf[Specification])
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: SpecificDatumWriter[Specification]（classOf[Specification]）
- en: val dataFileWriter = new
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: val dataFileWriter = new
- en: DataFileWriter[Specification](userDatumWriter)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: DataFileWriter[Specification]（userDatumWriter）
- en: dataFileWriter.create(Specification.getClassSchema, outputFile)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: dataFileWriter.create（Specification.getClassSchema，outputFile）
- en: for (line <- Source.fromFile(inputFile).getLines())
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于（line <- Source.fromFile（inputFile）.getLines（））
- en: dataFileWriter.append(generateAvro(line))
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: dataFileWriter.append（generateAvro（line））
- en: dataFileWriter.close()
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: dataFileWriter.close（）
- en: 'def generateAvro(line: String): Specification = {'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: def generateAvro（line：String）：Specification = {
- en: val values = line.split("\t",-1)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: val values = line.split（“\t”，-1）
- en: if(values.length == 27){
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果values.length == 27）{
- en: val specification = Specification.newBuilder()
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: val specification = Specification.newBuilder（）
- en: .setGkgRecordId(createGkgRecordId(values{0}))
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: .setGkgRecordId（createGkgRecordId（values{0}））
- en: .setV21Date(values{1}.toLong)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: .setV21Date（values{1}.toLong）
- en: .setV2SourceCollectionIdentifier(
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: .setV2SourceCollectionIdentifier(
- en: createSourceCollectionIdentifier(values{2}))
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: createSourceCollectionIdentifier（values{2}））
- en: .setV21SourceCommonName(values{3})
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: .setV21SourceCommonName（values{3}）
- en: .setV2DocumentIdentifier(values{4})
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: .setV2DocumentIdentifier（values{4}）
- en: .setV1Counts(createV1CountArray(values{5}))
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: .setV1Counts（createV1CountArray（values{5}））
- en: .setV21Counts(createV21CountArray(values{6}))
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: .setV21Counts（createV21CountArray（values{6}））
- en: .setV1Themes(createV1Themes(values{7}))
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: .setV1Themes（createV1Themes（values{7}））
- en: .setV2EnhancedThemes(createV2EnhancedThemes(values{8}))
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 创建V2EnhancedThemes（values{8}）
- en: .setV1Locations(createV1LocationsArray(values{9}))
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: .setV1Locations（createV1LocationsArray（values{9}））
- en: .
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: .
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 。
- en: .
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: '}'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE20]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'def createSourceCollectionIdentifier(str: String) :    SourceCollectionIdentifier
    = {'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: def createSourceCollectionIdentifier（str：String）：SourceCollectionIdentifier
    = {
- en: str.toInt match {
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: str.toInt match {
- en: case 1 => SourceCollectionIdentifier.WEB
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 情况1 => SourceCollectionIdentifier.WEB
- en: case 2 => SourceCollectionIdentifier.CITATIONONLY
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 情况2 => SourceCollectionIdentifier.CITATIONONLY
- en: case 3 => SourceCollectionIdentifier.CORE
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 情况3 => SourceCollectionIdentifier.CORE
- en: case 4 => SourceCollectionIdentifier.DTIC
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 情况4 => SourceCollectionIdentifier.DTIC
- en: case 5 => SourceCollectionIdentifier.JSTOR
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 情况5 => SourceCollectionIdentifier.JSTOR
- en: case 6 => SourceCollectionIdentifier.NONTEXTUALSOURCE
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 情况6 => SourceCollectionIdentifier.NONTEXTUALSOURCE
- en: case _ => SourceCollectionIdentifier.WEB
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 情况_ => SourceCollectionIdentifier.WEB
- en: '}'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'def createV1LocationsArray(str: String): Array[Location] = {'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: def createV1LocationsArray（str：String）：Array[Location] = {
- en: val counts = str.split(";")
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: val counts = str.split（“;”）
- en: counts map(createV1Location(_))
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 计数映射（createV1Location（_））
- en: '}'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE21]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When to perform Avro transformation
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时执行Avro转换
- en: 'In order to make best use of Avro, next, we need to decide when it is best
    to transform the data. Converting to Avro is a relatively expensive operation,
    so it should be done at the point when it makes most sense. Once again, it''s
    a tradeoff. This time, it''s between a flexible data model supporting unstructured
    processing, exploratory data analysis, ad hoc querying, and a structured type
    system. There are two main options to consider:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最好地利用Avro，接下来，我们需要决定何时最好转换数据。转换为Avro是一个相对昂贵的操作，因此应在最有意义的时候进行。再次，这是一个权衡。这一次，它是在灵活的数据模型支持非结构化处理、探索性数据分析、临时查询和结构化类型系统之间。有两个主要选项要考虑：
- en: '**Convert as late as possible**: it is possible to perform Avro conversion
    in each and every run of a job. There are some obvious drawbacks here, so it''s
    best to consider persisting Avro files at some point, to avoid the recalculation.
    You could do this lazily upon the first time, but chances are this would get confusing
    quite quickly. The easier option is to periodically run a batch job over the data
    at rest. This job''s only task would be to create Avro data and write it back
    to disk. This approach gives us full control over when the conversion jobs are
    executed. In busy environments, jobs can be scheduled for quiet periods and priority
    can be allocated on an ad hoc basis. The downside is that we need to know how
    long the processing is going to take in order to ensure there is enough time for
    completion. If processing is not completed before the next batched data arrives,
    then a backlog builds and it can be difficult to catch up.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**尽可能晚地转换**：可以在每次作业运行时执行Avro转换。这里有一些明显的缺点，所以最好考虑在某个时候持久化Avro文件，以避免重新计算。您可以在第一次懒惰地执行此操作，但很快就会变得混乱。更容易的选择是定期对静态数据运行批处理作业。该作业的唯一任务是创建Avro数据并将其写回磁盘。这种方法使我们完全控制转换作业的执行时间。在繁忙的环境中，可以安排作业在安静的时期运行，并且可以根据需要分配优先级。缺点是我们需要知道处理需要多长时间，以确保有足够的时间完成。如果处理在下一批数据到达之前没有完成，那么就会积压，并且很难赶上。'
- en: '**Convert as early as possible**: the alternative approach is to create an
    ingest pipeline, whereby the incoming data is converted to Avro on the fly (particularly
    useful in streaming scenarios). By doing this, we are in danger of approaching
    an ETL-style scenario, so it is really a judgment call as to which approach best
    suits the specific environment in use at the time.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**尽早转换**：另一种方法是创建一个摄取管道，其中传入的数据在飞行中转换为Avro（在流式场景中特别有用）。通过这样做，我们有可能接近ETL式的场景，因此真的是一个判断，哪种方法最适合当前使用的特定环境。'
- en: Now, let's look at a related technology that is used extensively throughout
    Spark, that is Apache Parquet.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下在Spark中广泛使用的相关技术，即Apache Parquet。
- en: Parquet
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Parquet
- en: Apache Parquet is a columnar storage format specifically designed for the Hadoop
    ecosystem. Traditional row-based storage formats are optimized to work with one
    record at a time, meaning they can be slow for certain types of workload. Instead,
    Parquet serializes and stores data by column, thus allowing for optimization of
    storage, compression, predicate processing, and bulk sequential access across
    large datasets - exactly the type of workload suited to Spark!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet是专为Hadoop生态系统设计的列式存储格式。传统的基于行的存储格式被优化为一次处理一条记录，这意味着它们对于某些类型的工作负载可能会很慢。相反，Parquet通过列序列化和存储数据，从而允许对存储、压缩、谓词处理和大型数据集的批量顺序访问进行优化-这正是适合Spark的工作负载类型！
- en: As Parquet implements per column data compaction, it's particularly suited to
    CSV data, especially with fields of low cardinality, and file sizes can see huge
    reductions when compared to Avro.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Parquet实现了按列数据压缩，因此特别适用于CSV数据，特别是低基数字段，与Avro相比，文件大小可以大大减小。
- en: '[PRE22]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parquet also integrates with Avro natively. Parquet takes an Avro in-memory
    representation of data and maps to its internal data types. It then serializes
    the data to disk using the Parquet columnar file format.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet还与Avro原生集成。Parquet采用Avro内存表示的数据，并映射到其内部数据类型。然后，它使用Parquet列式文件格式将数据序列化到磁盘上。
- en: 'We have seen how to apply Avro to the model, now we can take the next step
    and use this Avro model to persist data to disk via the Parquet format. Again,
    we will show the current method and then some lower-level code for demonstrative
    purposes. First, the recommended method:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何将Avro应用于模型，现在我们可以迈出下一步，使用这个Avro模型通过Parquet格式将数据持久化到磁盘上。再次，我们将展示当前的方法，然后为了演示目的，展示一些更低级别的代码。首先是推荐的方法：
- en: '[PRE23]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now for the detail behind how Avro and Parquet relate to each other:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来详细了解Avro和Parquet之间的关系：
- en: '[PRE24]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As before, the lower-level code is quite verbose, although it does give some
    insight into the various steps required. You can find the full code in our repository.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，低级别的代码相当冗长，尽管它确实提供了对所需各种步骤的一些见解。您可以在我们的存储库中找到完整的代码。
- en: We now have a great model to store and retrieve our GKG data that uses Avro
    and Parquet and can easily be implemented using `DataFrames`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个很好的模型来存储和检索我们的GKG数据，它使用Avro和Parquet，并且可以很容易地使用`DataFrames`来实现。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen why datasets should always be thoroughly understood
    before too much exploration work is undertaken. We have discussed the details
    of structured data and dimensional modeling, particularly with respect to how
    this applies to the GDELT dataset, and have expanded the GKG model to show its
    underlying complexity.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到为什么在进行太多的探索工作之前，数据集应该被彻底理解。我们已经讨论了结构化数据和维度建模的细节，特别是关于这如何适用于GDELT数据集，并扩展了GKG模型以展示其潜在复杂性。
- en: We have explained the difference between the traditional ETL and newer schema-on-read
    ELT techniques, and have touched upon some of the issues that data engineers face
    regarding data storage, compression, and data formats - specifically the advantages
    and implementations of Avro and Parquet. We have also demonstrated that there
    are several ways to explore data using the various Spark API, including examples
    of how to use SQL on the Spark shell.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经解释了传统ETL和新的基于模式读取ELT技术之间的区别，并且已经触及了数据工程师在数据存储、压缩和数据格式方面面临的一些问题，特别是Avro和Parquet的优势和实现。我们还演示了使用各种Spark
    API来探索数据的几种方法，包括如何在Spark shell上使用SQL的示例。
- en: We can conclude this chapter by mentioning that the code in our repository pulls
    everything together and is a full model for reading in raw GKG files (use the
    Apache NiFi GDELT data ingest pipeline from [Chapter 1](ch02.xhtml "Chapter 2. Data
    Acquisition"),* Data Acquisition* if you require some data).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过提到我们的存储库中的代码将所有内容汇总，并且是一个用于读取原始GKG文件的完整模型（如果需要一些数据，请使用Apache NiFi GDELT数据摄取管道来自[第1章](ch02.xhtml
    "第2章.数据获取")，*数据获取*）。
- en: In the next chapter, we will dive deeper into the GKG model by exploring the
    techniques used to explore and analyze data at scale. We will see how to develop
    and enrich our GKG data model using SQL, and investigate how Apache Zeppelin notebooks
    can provide a richer data science experience.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨GKG模型，探索用于大规模探索和分析数据的技术。我们将看到如何使用SQL开发和丰富我们的GKG数据模型，并调查Apache Zeppelin笔记本如何提供更丰富的数据科学体验。
