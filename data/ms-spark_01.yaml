- en: Chapter 1. Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。Apache Spark
- en: Apache Spark is a distributed and highly scalable in-memory data analytics system,
    providing the ability to develop applications in Java, Scala, Python, as well
    as languages like R. It has one of the highest contribution/involvement rates
    among the Apache top level projects at this time. Apache systems, such as Mahout,
    now use it as a processing engine instead of MapReduce. Also, as will be shown
    in [Chapter 4](ch04.html "Chapter 4. Apache Spark SQL"), *Apache Spark SQL*, it
    is possible to use a Hive context to have the Spark applications process data
    directly to and from Apache Hive.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个分布式和高度可扩展的内存数据分析系统，提供了在Java、Scala、Python以及R等语言中开发应用程序的能力。它在目前的Apache顶级项目中具有最高的贡献/参与率。现在，像Mahout这样的Apache系统使用它作为处理引擎，而不是MapReduce。此外，正如在[第4章](ch04.html
    "第4章。Apache Spark SQL")中所示，*Apache Spark SQL*，可以使用Hive上下文，使Spark应用程序直接处理Apache
    Hive中的数据。
- en: Apache Spark provides four main submodules, which are SQL, MLlib, GraphX, and
    Streaming. They will all be explained in their own chapters, but a simple overview
    would be useful here. The modules are interoperable, so data can be passed between
    them. For instance, streamed data can be passed to SQL, and a temporary table
    can be created.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark提供了四个主要的子模块，分别是SQL、MLlib、GraphX和Streaming。它们将在各自的章节中进行解释，但在这里简单的概述会很有用。这些模块是可互操作的，因此数据可以在它们之间传递。例如，流式数据可以传递到SQL，然后创建一个临时表。
- en: 'The following figure explains how this book will address Apache Spark and its
    modules. The top two rows show Apache Spark, and its four submodules described
    earlier. However, wherever possible, I always try to show by giving an example
    how the functionality may be extended using the extra tools:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图解释了本书将如何处理Apache Spark及其模块。前两行显示了Apache Spark及其前面描述的四个子模块。然而，尽可能地，我总是试图通过示例来展示如何使用额外的工具来扩展功能：
- en: '![Apache Spark](img/B01989_01_01.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark](img/B01989_01_01.jpg)'
- en: For instance, the data streaming module explained in [Chapter 3](ch03.html "Chapter 3. Apache
    Spark Streaming"), *Apache Spark Streaming*, will have worked examples, showing
    how data movement is performed using Apache **Kafka** and **Flume**. The **MLlib**
    or the machine learning module will have its functionality examined in terms of
    the data processing functions that are available, but it will also be extended
    using the H2O system and deep learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[第3章](ch03.html "第3章。Apache Spark Streaming")中解释的数据流模块，*Apache Spark Streaming*，将有工作示例，展示如何使用Apache
    **Kafka**和**Flume**执行数据移动。机器学习模块**MLlib**将通过可用的数据处理功能进行功能检查，但也将使用H2O系统和深度学习进行扩展。
- en: The previous figure is, of course, simplified. It represents the system relationships
    presented in this book. For instance, there are many more routes between Apache
    Spark modules and HDFS than the ones shown in the preceding diagram.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图当然是简化的。它代表了本书中呈现的系统关系。例如，Apache Spark模块与HDFS之间的路线比前面的图中显示的要多得多。
- en: The Spark SQL chapter will also show how Spark can use a Hive Context. So, a
    Spark application can be developed to create Hive-based objects, and run Hive
    QL against Hive tables, stored in HDFS.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL章节还将展示Spark如何使用Hive上下文。因此，可以开发一个Spark应用程序来创建基于Hive的对象，并对存储在HDFS中的Hive表运行Hive
    QL。
- en: '[Chapter 5](ch05.html "Chapter 5. Apache Spark GraphX"), *Apache Spark GraphX*,
    and [Chapter 6](ch06.html "Chapter 6. Graph-based Storage"), *Graph-based Storage*,
    will show how the Spark GraphX module can be used to process big data scale graphs,
    and how they can be stored using the Titan graph database. It will be shown that
    Titan will allow big data scale graphs to be stored, and queried as graphs. It
    will show, by an example, that Titan can use both, **HBase** and **Cassandra**
    as a storage mechanism. When using HBase, it will be shown that implicitly, Titan
    uses HDFS as a cheap and reliable distributed storage mechanism.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](ch05.html "第5章。Apache Spark GraphX") *Apache Spark GraphX* 和 [第6章](ch06.html
    "第6章。基于图的存储") *基于图的存储* 将展示Spark GraphX模块如何用于处理大数据规模的图，以及如何使用Titan图数据库进行存储。将展示Titan允许存储和查询大数据规模的图。通过一个例子，将展示Titan可以同时使用**HBase**和**Cassandra**作为存储机制。当使用HBase时，将会显示Titan隐式地使用HDFS作为一种廉价可靠的分布式存储机制。'
- en: So, I think that this section has explained that Spark is an in-memory processing
    system. When used at scale, it cannot exist alone—the data must reside somewhere.
    It will probably be used along with the Hadoop tool set, and the associated eco-system.
    Luckily, Hadoop stack providers, such as Cloudera, provide the CDH Hadoop stack
    and cluster manager, which integrates with Apache Spark, Hadoop, and most of the
    current stable tool set. During this book, I will use a small CDH 5.3 cluster
    installed on CentOS 6.5 64 bit servers. You can use an alternative configuration,
    but I find that CDH provides most of the tools that I need, and automates the
    configuration, leaving me more time for development.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我认为本节已经解释了Spark是一个内存处理系统。在大规模使用时，它不能独立存在——数据必须存放在某个地方。它可能会与Hadoop工具集以及相关的生态系统一起使用。幸运的是，Hadoop堆栈提供商，如Cloudera，提供了与Apache
    Spark、Hadoop和大多数当前稳定工具集集成的CDH Hadoop堆栈和集群管理器。在本书中，我将使用安装在CentOS 6.5 64位服务器上的小型CDH
    5.3集群。您可以使用其他配置，但我发现CDH提供了我需要的大多数工具，并自动化了配置，为我留下更多的时间进行开发。
- en: Having mentioned the Spark modules and the software that will be introduced
    in this book, the next section will describe the possible design of a big data
    cluster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提到了Spark模块和本书中将介绍的软件后，下一节将描述大数据集群的可能设计。
- en: Overview
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this section, I wish to provide an overview of the functionality that will
    be introduced in this book in terms of Apache Spark, and the systems that will
    be used to extend it. I will also try to examine the future of Apache Spark, as
    it integrates with cloud storage.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我希望提供一个关于本书中将介绍的Apache Spark功能以及将用于扩展它的系统的概述。我还将尝试审视Apache Spark与云存储集成的未来。
- en: When you examine the documentation on the Apache Spark website ([http://spark.apache.org/](http://spark.apache.org/)),
    you will see that there are topics that cover SparkR and Bagel. Although I will
    cover the four main Spark modules in this book, I will not cover these two topics.
    I have limited time and scope in this book so I will leave these topics for reader
    investigation or for a future date.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当您查看Apache Spark网站（[http://spark.apache.org/](http://spark.apache.org/)）上的文档时，您会发现有涵盖SparkR和Bagel的主题。虽然我会在本书中涵盖四个主要的Spark模块，但我不会涵盖这两个主题。我在本书中时间和范围有限，所以我会把这些主题留给读者自行探究或将来研究。
- en: Spark Machine Learning
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark机器学习
- en: 'The Spark MLlib module offers machine learning functionality over a number
    of domains. The documentation available at the Spark website introduces the data
    types used (for example, vectors and the LabeledPoint structure). This module
    offers functionality that includes:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib模块提供了在多个领域进行机器学习功能。Spark网站上提供的文档介绍了使用的数据类型（例如，向量和LabeledPoint结构）。该模块提供的功能包括：
- en: Statistics
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计
- en: Classification
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Regression
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: Collaborative Filtering
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协同过滤
- en: Clustering
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Dimensionality Reduction
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度约简
- en: Feature Extraction
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取
- en: Frequent Pattern Mining
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频繁模式挖掘
- en: Optimization
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化
- en: The Scala-based practical examples of KMeans, Naïve Bayes, and Artificial Neural
    Networks have been introduced and discussed in [Chapter 2](ch02.html "Chapter 2. Apache
    Spark MLlib"), *Apache Spark MLlib* of this book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Scala的KMeans、朴素贝叶斯和人工神经网络的实际示例已在本书的[第2章](ch02.html "第2章。Apache Spark MLlib")
    *Apache Spark MLlib*中介绍和讨论。
- en: Spark Streaming
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: Stream processing is another big and popular topic for Apache Spark. It involves
    the processing of data in Spark as streams, and covers topics such as input and
    output operations, transformations, persistence, and check pointing among others.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理是Apache Spark的另一个重要和受欢迎的主题。它涉及在Spark中作为流处理数据，并涵盖输入和输出操作、转换、持久性和检查点等主题。
- en: '[Chapter 3](ch03.html "Chapter 3. Apache Spark Streaming"), *Apache Spark Streaming*,
    covers this area of processing, and provides practical examples of different types
    of stream processing. It discusses batch and window stream configuration, and
    provides a practical example of checkpointing. It also covers different examples
    of stream processing, including Kafka and Flume.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](ch03.html "第3章。Apache Spark Streaming") *Apache Spark Streaming*，涵盖了这一领域的处理，并提供了不同类型的流处理的实际示例。它讨论了批处理和窗口流配置，并提供了一个实际的检查点示例。它还涵盖了不同类型的流处理示例，包括Kafka和Flume。'
- en: There are many more ways in which stream data can be used. Other Spark module
    functionality (for example, SQL, MLlib, and GraphX) can be used to process the
    stream. You can use Spark streaming with systems such as Kinesis or ZeroMQ. You
    can even create custom receivers for your own user-defined data sources.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据还有许多其他用途。其他Spark模块功能（例如SQL、MLlib和GraphX）可以用于处理流。您可以将Spark流处理与Kinesis或ZeroMQ等系统一起使用。您甚至可以为自己定义的数据源创建自定义接收器。
- en: Spark SQL
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark SQL
- en: From Spark version 1.3 data frames have been introduced into Apache Spark so
    that Spark data can be processed in a tabular form and tabular functions (like
    select, filter, groupBy) can be used to process data. The Spark SQL module integrates
    with Parquet and JSON formats to allow data to be stored in formats that better
    represent data. This also offers more options to integrate with external systems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark版本1.3开始，数据框架已经引入到Apache Spark中，以便以表格形式处理Spark数据，并且可以使用表格函数（如select、filter、groupBy）来处理数据。Spark
    SQL模块与Parquet和JSON格式集成，允许数据以更好地表示数据的格式存储。这也提供了更多与外部系统集成的选项。
- en: The idea of integrating Apache Spark into the Hadoop Hive big data database
    can also be introduced. Hive context-based Spark applications can be used to manipulate
    Hive-based table data. This brings Spark's fast in-memory distributed processing
    to Hive's big data storage capabilities. It effectively lets Hive use Spark as
    a processing engine.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将Apache Spark集成到Hadoop Hive大数据数据库中的想法也可以介绍。基于Hive上下文的Spark应用程序可用于操作基于Hive的表数据。这使得Spark的快速内存分布式处理能力可以应用到Hive的大数据存储能力上。它有效地让Hive使用Spark作为处理引擎。
- en: Spark graph processing
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark图处理
- en: The Apache Spark GraphX module allows Spark to offer fast, big data in memory
    graph processing. A graph is represented by a list of vertices and edges (the
    lines that connect the vertices). GraphX is able to create and manipulate graphs
    using the property, structural, join, aggregation, cache, and uncache operators.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark GraphX模块使Spark能够提供快速的大数据内存图处理。图由顶点和边的列表（连接顶点的线）表示。GraphX能够使用属性、结构、连接、聚合、缓存和取消缓存操作来创建和操作图。
- en: 'It introduces two new data types to support graph processing in Spark: VertexRDD
    and EdgeRDD to represent graph vertexes and edges. It also introduces graph processing
    example functions, such as PageRank and triangle processing. Many of these functions
    will be examined in [Chapter 5](ch05.html "Chapter 5. Apache Spark GraphX"), *Apache
    Spark GraphX*.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 它引入了两种新的数据类型来支持Spark中的图处理：VertexRDD和EdgeRDD来表示图的顶点和边。它还介绍了图处理的示例函数，例如PageRank和三角形处理。这些函数中的许多将在[第5章](ch05.html
    "第5章。Apache Spark GraphX") *Apache Spark GraphX*中进行研究。
- en: Extended ecosystem
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展生态系统
- en: When examining big data processing systems, I think it is important to look
    at not just the system itself, but also how it can be extended, and how it integrates
    with external systems, so that greater levels of functionality can be offered.
    In a book of this size, I cannot cover every option, but hopefully by introducing
    a topic, I can stimulate the reader's interest, so that they can investigate further.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在审查大数据处理系统时，我认为重要的是不仅要看系统本身，还要看它如何扩展，以及它如何与外部系统集成，以便提供更高级别的功能。在这样大小的书中，我无法涵盖每个选项，但希望通过介绍一个主题，我可以激发读者的兴趣，以便他们可以进一步调查。
- en: I have used the H2O machine learning library system to extend Apache Spark's
    machine learning module. By using an H2O deep learning Scala-based example, I
    have shown how neural processing can be introduced to Apache Spark. I am, however,
    aware that I have just scratched the surface of H2O's functionality. I have only
    used a small neural cluster and a single type of classification functionality.
    Also, there is a lot more to H2O than deep learning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经使用了H2O机器学习库系统来扩展Apache Spark的机器学习模块。通过使用基于Scala的H2O深度学习示例，我展示了如何将神经处理引入Apache
    Spark。然而，我知道我只是触及了H2O功能的表面。我只使用了一个小型神经集群和一种分类功能。此外，H2O还有很多其他功能。
- en: As graph processing becomes more accepted and used in the coming years, so will
    graph based storage. I have investigated the use of Spark with the NoSQL database
    Neo4J, using the Mazerunner prototype application. I have also investigated the
    use of the Aurelius (Datastax) Titan database for graph-based storage. Again,
    Titan is a database in its infancy, which needs both community support and further
    development. But I wanted to examine the future options for Apache Spark integration.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图形处理在未来几年变得更加被接受和使用，基于图形的存储也将如此。我已经调查了使用NoSQL数据库Neo4J的Spark，使用了Mazerunner原型应用程序。我还调查了Aurelius（Datastax）Titan数据库用于基于图形的存储。同样，Titan是一个新生的数据库，需要社区支持和进一步发展。但我想研究Apache
    Spark集成的未来选项。
- en: The future of Spark
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark的未来
- en: The next section will show that the Apache Spark release contains scripts to
    allow a Spark cluster to be created on AWS EC2 storage. There are a range of options
    available that allow the cluster creator to define attributes such as cluster
    size and storage type. But this type of cluster is difficult to resize, which
    makes it difficult to manage changing requirements. If the data volume changes
    or grows over time a larger cluster maybe required with more memory.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将展示Apache Spark发布包含的脚本，允许在AWS EC2存储上创建一个Spark集群。有一系列选项可供选择，允许集群创建者定义属性，如集群大小和存储类型。但这种类型的集群很难调整大小，这使得管理变化的需求变得困难。如果数据量随时间变化或增长，可能需要更大的集群和更多的内存。
- en: Luckily, the people that developed Apache Spark have created a new start-up
    called Databricks [https://databricks.com/](https://databricks.com/), which offers
    web console-based Spark cluster management, plus a lot of other functionality.
    It offers the idea of work organized by notebooks, user access control, security,
    and a mass of other functionality. It is described at the end of this book.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，开发Apache Spark的人创建了一个名为Databricks的新创企业[https://databricks.com/](https://databricks.com/)，它提供基于Web控制台的Spark集群管理，以及许多其他功能。它提供了笔记本组织的工作思路，用户访问控制、安全性和大量其他功能。这些内容在本书的最后进行了描述。
- en: It is a service in its infancy, currently only offering cloud-based storage
    on Amazon AWS, but it will probably extend to Google and Microsoft Azure in the
    future. The other cloud-based providers, that is, Google and Microsoft Azure,
    are also extending their services, so that they can offer Apache Spark processing
    in the cloud.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它目前只在亚马逊AWS上提供基于云的存储服务，但将来可能会扩展到谷歌和微软Azure。其他基于云的提供商，即谷歌和微软Azure，也在扩展他们的服务，以便他们可以在云中提供Apache
    Spark处理。
- en: Cluster design
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群设计
- en: As I already mentioned, Apache Spark is a distributed, in-memory, parallel processing
    system, which needs an associated storage mechanism. So, when you build a big
    data cluster, you will probably use a distributed storage system such as Hadoop,
    as well as tools to move data like Sqoop, Flume, and Kafka.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，Apache Spark是一个分布式、内存中、并行处理系统，需要一个关联的存储机制。因此，当你构建一个大数据集群时，你可能会使用分布式存储系统，比如Hadoop，以及用于数据移动的工具，如Sqoop、Flume和Kafka。
- en: 'I wanted to introduce the idea of edge nodes in a big data cluster. Those nodes
    in the cluster will be client facing, on which reside the client facing components
    like the Hadoop NameNode or perhaps the Spark master. The majority of the big
    data cluster might be behind a firewall. The edge nodes would then reduce the
    complexity caused by the firewall, as they would be the only nodes that would
    be accessible. The following figure shows a simplified big data cluster:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我想介绍大数据集群中边缘节点的概念。集群中的这些节点将面向客户端，上面有像Hadoop NameNode或者Spark主节点这样的客户端组件。大多数大数据集群可能在防火墙后面。边缘节点将减少防火墙带来的复杂性，因为它们是唯一可访问的节点。下图显示了一个简化的大数据集群：
- en: '![Cluster design](img/B01989_01_02.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![集群设计](img/B01989_01_02.jpg)'
- en: It shows four simplified cluster racks with switches and edge node computers,
    facing the client across the firewall. This is, of course, stylized and simplified,
    but you get the idea. The general processing nodes are hidden behind a firewall
    (the dotted line), and are available for general processing, in terms of Hadoop,
    Apache Spark, Zookeeper, Flume, and/or Kafka. The following figure represents
    a couple of big data cluster edge nodes, and attempts to show what applications
    might reside on them.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 它显示了四个简化的集群机架，带有交换机和边缘节点计算机，面向防火墙的客户端。当然，这是风格化和简化的，但你明白了。一般处理节点隐藏在防火墙后面（虚线），可用于一般处理，比如Hadoop、Apache
    Spark、Zookeeper、Flume和/或Kafka。下图代表了一些大数据集群边缘节点，并试图展示可能驻留在它们上面的应用程序。
- en: 'The edge node applications will be the master applications similar to the Hadoop
    NameNode, or the Apache Spark master server. It will be the components that are
    bringing the data into and out of the cluster such as Flume, Sqoop, and Kafka.
    It can be any component that makes a user interface available to the client user
    similar to Hive:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘节点应用程序将是类似于Hadoop NameNode或Apache Spark主服务器的主应用程序。它将是将数据带入和带出集群的组件，比如Flume、Sqoop和Kafka。它可以是任何使用户界面对客户用户可用的组件，类似于Hive：
- en: '![Cluster design](img/B01989_01_03.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![集群设计](img/B01989_01_03.jpg)'
- en: Generally, firewalls, while adding security to the cluster, also increase the
    complexity. Ports between system components need to be opened up, so that they
    can talk to each other. For instance, Zookeeper is used by many components for
    configuration. Apache Kafka, the publish subscribe messaging system, uses Zookeeper
    for configuring its topics, groups, consumers, and producers. So client ports
    to Zookeeper, potentially across the firewall, need to be open.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，防火墙在增加集群安全性的同时也增加了复杂性。系统组件之间的端口需要打开，以便它们可以相互通信。例如，Zookeeper被许多组件用于配置。Apache
    Kafka，发布订阅消息系统，使用Zookeeper来配置其主题、组、消费者和生产者。因此，潜在地需要打开防火墙的客户端端口到Zookeeper。
- en: Finally, the allocation of systems to cluster nodes needs to be considered.
    For instance, if Apache Spark uses Flume or Kafka, then in-memory channels will
    be used. The size of these channels, and the memory used, caused by the data flow,
    need to be considered. Apache Spark should not be competing with other Apache
    components for memory usage. Depending upon your data flows and memory usage,
    it might be necessary to have the Spark, Hadoop, Zookeeper, Flume, and other tools
    on distinct cluster nodes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要考虑将系统分配给集群节点。例如，如果Apache Spark使用Flume或Kafka，则将使用内存通道。需要考虑这些通道的大小和由数据流引起的内存使用。Apache
    Spark不应该与其他Apache组件竞争内存使用。根据您的数据流和内存使用情况，可能需要在不同的集群节点上拥有Spark、Hadoop、Zookeeper、Flume和其他工具。
- en: Generally, the edge nodes that act as cluster NameNode servers, or Spark master
    servers, will need greater resources than the cluster processing nodes within
    the firewall. For instance, a CDH cluster node manager server will need extra
    memory, as will the Spark master server. You should monitor edge nodes for resource
    usage, and adjust in terms of resources and/or application location as necessary.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，作为集群NameNode服务器或Spark主服务器的边缘节点将需要比防火墙内的集群处理节点更多的资源。例如，CDH集群节点管理器服务器将需要额外的内存，同样Spark主服务器也是如此。您应该监视边缘节点的资源使用情况，并根据需要调整资源和/或应用程序位置。
- en: This section has briefly set the scene for the big data cluster in terms of
    Apache Spark, Hadoop, and other tools. However, how might the Apache Spark cluster
    itself, within the big data cluster, be configured? For instance, it is possible
    to have many types of Spark cluster manager. The next section will examine this,
    and describe each type of Apache Spark cluster manager.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要介绍了Apache Spark、Hadoop和其他工具在大数据集群中的情景。然而，在大数据集群中，Apache Spark集群本身如何配置呢？例如，可以有多种类型的Spark集群管理器。下一节将对此进行探讨，并描述每种类型的Apache
    Spark集群管理器。
- en: Cluster management
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群管理
- en: 'The following diagram, borrowed from the spark.apache.org website, demonstrates
    the role of the Apache Spark cluster manager in terms of the master, slave (worker),
    executor, and Spark client applications:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下图从spark.apache.org网站借来，展示了Apache Spark集群管理器在主节点、从节点（工作节点）、执行器和Spark客户端应用程序方面的作用：
- en: '![Cluster management](img/B01989_01_04.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![集群管理](img/B01989_01_04.jpg)'
- en: The Spark context, as you will see from many of the examples in this book, can
    be defined via a Spark configuration object, and a Spark URL. The Spark context
    connects to the Spark cluster manager, which then allocates resources across the
    worker nodes for the application. The cluster manager allocates executors across
    the cluster worker nodes. It copies the application jar file to the workers, and
    finally it allocates tasks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将从本书的许多示例中看到的那样，Spark上下文可以通过Spark配置对象和Spark URL来定义。Spark上下文连接到Spark集群管理器，然后为应用程序在工作节点之间分配资源。集群管理器在集群工作节点之间分配执行器。它将应用程序jar文件复制到工作节点，最后分配任务。
- en: The following subsections describe the possible Apache Spark cluster manager
    options available at this time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节描述了目前可用的可能的Apache Spark集群管理器选项。
- en: Local
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地
- en: By specifying a Spark configuration local URL, it is possible to have the application
    run locally. By specifying local[n], it is possible to have Spark use `<n>` threads
    to run the application locally. This is a useful development and test option.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定一个Spark配置本地URL，可以让应用程序在本地运行。通过指定local[n]，可以让Spark使用`<n>`个线程在本地运行应用程序。这是一个有用的开发和测试选项。
- en: Standalone
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独立模式
- en: 'Standalone mode uses a basic cluster manager that is supplied with Apache Spark.
    The spark master URL will be as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 独立模式使用了Apache Spark提供的基本集群管理器。Spark主URL将如下所示：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `<hostname>` is the name of the host on which the Spark master is running.
    I have specified `7077` as the port, which is the default value, but it is configurable.
    This simple cluster manager, currently, only supports FIFO (first in first out)
    scheduling. You can contrive to allow concurrent application scheduling by setting
    the resource configuration options for each application. For instance, using `spark.core.max`
    to share cores between applications.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`<hostname>`是运行Spark主节点的主机名。我已经指定了端口`7077`，这是默认值，但它是可配置的。目前，这个简单的集群管理器只支持FIFO（先进先出）调度。您可以通过为每个应用程序设置资源配置选项来构建允许并发应用程序调度。例如，使用`spark.core.max`来在应用程序之间共享核心。
- en: Apache YARN
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache YARN
- en: At a larger scale when integrating with Hadoop YARN, the Apache Spark cluster
    manager can be YARN, and the application can run in one of two modes. If the Spark
    master value is set as yarn-cluster, then the application can be submitted to
    the cluster, and then terminated. The cluster will take care of allocating resources
    and running tasks. However, if the application master is submitted as yarn-client,
    then the application stays alive during the life cycle of processing, and requests
    resources from YARN.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在与Hadoop YARN集成的较大规模下，Apache Spark集群管理器可以是YARN，并且应用程序可以在两种模式下运行。如果将Spark主值设置为yarn-cluster，那么应用程序可以提交到集群，然后终止。集群将负责分配资源和运行任务。然而，如果应用程序主作为yarn-client提交，那么应用程序在处理的生命周期内保持活动，并从YARN请求资源。
- en: Apache Mesos
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Mesos
- en: Apache Mesos is an open source system for resource sharing across a cluster.
    It allows multiple frameworks to share a cluster by managing and scheduling resources.
    It is a cluster manager, which provides isolation using Linux containers, allowing
    multiple systems, like Hadoop, Spark, Kafka, Storm, and more to share a cluster
    safely. It is highly scalable to thousands of nodes. It is a master slave-based
    system, and is fault tolerant, using Zookeeper for configuration management.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Mesos是一个用于跨集群共享资源的开源系统。它允许多个框架通过管理和调度资源来共享集群。它是一个集群管理器，使用Linux容器提供隔离，允许多个系统（如Hadoop、Spark、Kafka、Storm等）安全地共享集群。它可以高度扩展到数千个节点。它是一个基于主从的系统，并且具有容错性，使用Zookeeper进行配置管理。
- en: 'For a single master node Mesos cluster, the Spark master URL will be in this
    form:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个主节点Mesos集群，Spark主URL将采用以下形式：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Where `<hostname>` is the host name of the Mesos master server, the port is
    defined as `5050`, which is the default Mesos master port (this is configurable).
    If there are multiple Mesos master servers in a large scale high availability
    Mesos cluster, then the Spark master URL would look like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`<hostname>`是Mesos主服务器的主机名，端口被定义为`5050`，这是Mesos主端口的默认值（可配置）。如果在大规模高可用性Mesos集群中有多个Mesos主服务器，则Spark主URL将如下所示：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So, the election of the Mesos master server will be controlled by Zookeeper.
    The `<hostname>` will be the name of a host in the Zookeeper quorum. Also, the
    port number `2181` is the default master port for Zookeeper.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Mesos主服务器的选举将由Zookeeper控制。`<hostname>`将是Zookeeper quorum中的主机名。此外，端口号`2181`是Zookeeper的默认主端口。
- en: Amazon EC2
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon EC2
- en: 'The Apache Spark release contains scripts for running Spark in the cloud against
    Amazon AWS EC2-based servers. The following listing, as an example, shows Spark
    1.3.1 installed on a Linux CentOS server, under the directory called `/usr/local/spark/`.
    The EC2 resources are available in the Spark release EC2 subdirectory:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark发行版包含用于在亚马逊AWS EC2基础服务器上运行Spark的脚本。以下示例显示了在Linux CentOS服务器上安装的Spark
    1.3.1，位于名为`/usr/local/spark/`的目录下。Spark发行版EC2子目录中提供了EC2资源：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In order to use Apache Spark on EC2, you will need to set up an Amazon AWS
    account. You can set up an initial free account to try it out here: [http://aws.amazon.com/free/](http://aws.amazon.com/free/).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要在EC2上使用Apache Spark，您需要设置一个Amazon AWS帐户。您可以在此处设置一个初始免费帐户来尝试：[http://aws.amazon.com/free/](http://aws.amazon.com/free/)。
- en: If you take a look at [Chapter 8](ch08.html "Chapter 8. Spark Databricks"),
    *Spark Databricks* you will see that such an account has been set up, and is used
    to access [https://databricks.com/](https://databricks.com/). The next thing that
    you will need to do is access your AWS IAM Console, and select the **Users** option.
    You either create or select a user. Select the **User Actions** option, and then
    select **Manage Access Keys**. Then, select **Create Access Key**, and then **Download
    Credentials**. Make sure that your downloaded key file is secure, assuming that
    you are on Linux chmod the file with permissions `= 600` for user-only access.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看[第8章](ch08.html "第8章。Spark Databricks")*Spark Databricks*，您会看到已经设置了这样一个帐户，并且用于访问[https://databricks.com/](https://databricks.com/)。接下来，您需要访问AWS
    IAM控制台，并选择**用户**选项。您可以创建或选择一个用户。选择**用户操作**选项，然后选择**管理访问密钥**。然后，选择**创建访问密钥**，然后**下载凭据**。确保您下载的密钥文件是安全的，假设您在Linux上，使用`chmod`命令将文件权限设置为`600`，以便仅用户访问。
- en: 'You will now have your **Access Key ID**, **Secret Access Key**, key file,
    and key pair name. You can now create a Spark EC2 cluster using the `spark-ec2`
    script as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经拥有了**访问密钥ID**、**秘密访问密钥**、密钥文件和密钥对名称。您现在可以使用`spark-ec2`脚本创建一个Spark EC2集群，如下所示：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `<pairname>` is the key pair name that you gave when your access details
    were created; `<awskey.pem>` is the file that you downloaded. The name of the
    cluster that you are going to create is called `<cluster1>`. The region chosen
    here is in the western USA, `us-west-1`. If you live in the Pacific, as I do,
    it might be wiser to choose a nearer region like `ap-southeast-2`. However, if
    you encounter allowance access issues, then you will need to try another zone.
    Remember also that using cloud-based Spark clustering like this will have higher
    latency and poorer I/O in general. You share your cluster hosts with multiple
    users, and your cluster maybe in a remote region.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`<pairname>`是在创建访问详细信息时给出的密钥对名称；`<awskey.pem>`是您下载的文件。您要创建的集群的名称称为`<cluster1>`。此处选择的区域位于美国西部，`us-west-1`。如果您像我一样住在太平洋地区，可能更明智的选择一个更近的区域，如`ap-southeast-2`。但是，如果遇到访问问题，则需要尝试另一个区域。还要记住，像这样使用基于云的Spark集群将具有更高的延迟和较差的I/O性能。您与多个用户共享集群主机，您的集群可能位于远程地区。
- en: 'You can use a series of options to this basic command to configure the cloud-based
    Spark cluster that you create. The `–s` option can be used:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用一系列选项来配置您创建的基于云的Spark集群。`-s`选项可以使用：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This allows you to define how many worker nodes to create in your Spark EC2
    cluster, that is, `–s 5` for a six node cluster, one master, and five slave workers.
    You can define the version of Spark that your cluster runs, rather than the default
    latest version. The following option starts a cluster with Spark version 1.3.1:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许您定义在您的Spark EC2集群中创建多少个工作节点，即`-s 5`表示六个节点集群，一个主节点和五个从节点。您可以定义您的集群运行的Spark版本，而不是默认的最新版本。以下选项启动了一个带有Spark版本1.3.1的集群：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The instance type used to create the cluster will define how much memory is
    used, and how many cores are available. For instance, the following option will
    set the instance type to be `m3.large`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建集群的实例类型将定义使用多少内存和可用多少核心。例如，以下选项将将实例类型设置为`m3.large`：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The current instance types for Amazon AWS can be found at: [http://aws.amazon.com/ec2/instance-types/](http://aws.amazon.com/ec2/instance-types/).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon AWS的当前实例类型可以在[http://aws.amazon.com/ec2/instance-types/](http://aws.amazon.com/ec2/instance-types/)找到。
- en: 'The following figure shows the current (as of July 2015) AWS M3 instance types,
    model details, cores, memory, and storage. There are many instance types available
    at this time; for instance, T2, M4, M3, C4, C3, R3, and more. Examine the current
    availability and choose appropriately:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了当前（截至2015年7月）AWS M3实例类型、型号细节、核心、内存和存储。目前有许多实例类型可用；例如T2、M4、M3、C4、C3、R3等。检查当前可用性并选择适当的：
- en: '![Amazon EC2](img/B01989_01_05.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![Amazon EC2](img/B01989_01_05.jpg)'
- en: 'Pricing is also very important. The current AWS storage type prices can be
    found at: [http://aws.amazon.com/ec2/pricing/](http://aws.amazon.com/ec2/pricing/).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 定价也非常重要。当前AWS存储类型的价格可以在此找到：[http://aws.amazon.com/ec2/pricing/](http://aws.amazon.com/ec2/pricing/)。
- en: The prices are shown by region with a drop-down menu, and a price by hour. Remember
    that each storage type is defined by cores, memory, and physical storage. The
    prices are also defined by operating system type, that is, Linux, RHEL, and Windows.
    Just select the OS via a top-level menu.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 价格按地区显示，并有一个下拉菜单和按小时计价。请记住，每种存储类型都由核心、内存和物理存储定义。价格也由操作系统类型定义，即Linux、RHEL和Windows。只需通过顶级菜单选择操作系统。
- en: The following figure shows an example of pricing at the time of writing (July
    2015); it is just provided to give an idea. Prices will differ over time, and
    by service provider. They will differ by the size of storage that you need, and
    the length of time that you are willing to commit to.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了写作时（2015年7月）的定价示例；它只是提供一个想法。价格会随时间而变化，而且会因服务提供商而异。它们会根据你需要的存储大小和你愿意承诺的时间长度而有所不同。
- en: Be aware also of the costs of moving your data off of any storage platform.
    Try to think long term. Check whether you will need to move all, or some of your
    cloud-based data to the next system in, say, five years. Check the process to
    move data, and include that cost in your planning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意将数据从任何存储平台移出的成本。尽量考虑长期。检查你是否需要在未来五年将所有或部分基于云的数据移动到下一个系统。检查移动数据的过程，并将该成本纳入你的规划中。
- en: '![Amazon EC2](img/B01989_01_06.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![Amazon EC2](img/B01989_01_06.jpg)'
- en: As described, the preceding figure shows the costs of AWS storage types by operating
    system, region, storage type, and hour. The costs are measured per unit hour,
    so systems such as [https://databricks.com/](https://databricks.com/) do not terminate
    EC2 instances, until a full hour has elapsed. These costs will change with time
    and need to be monitored via (for AWS) the AWS billing console.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，上图显示了AWS存储类型的成本，按操作系统、地区、存储类型和小时计价。成本是按单位小时计算的，因此像[https://databricks.com/](https://databricks.com/)这样的系统在完整的小时过去之前不会终止EC2实例。这些成本会随时间变化，需要通过（对于AWS）AWS计费控制台进行监控。
- en: You may also have problems when wanting to resize your Spark EC2 cluster, so
    you will need to be sure of the master slave configuration before you start. Be
    sure how many workers you are going to require, and how much memory you need.
    If you feel that your requirements are going to change over time, then you might
    consider using [https://databricks.com/](https://databricks.com/), if you definitely
    wish to work with Spark in the cloud. Go to [Chapter 8](ch08.html "Chapter 8. Spark
    Databricks"), *Spark Databricks* and see how you can set up, and use [https://databricks.com/](https://databricks.com/).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要调整你的Spark EC2集群大小时，你需要确保在开始之前确定主从配置。确定你需要多少工作节点和需要多少内存。如果你觉得你的需求会随着时间改变，那么你可能会考虑使用[https://databricks.com/](https://databricks.com/)，如果你确实希望在云中使用Spark。前往[第8章](ch08.html
    "第8章 Spark Databricks") *Spark Databricks*，看看你如何设置和使用[https://databricks.com/](https://databricks.com/)。
- en: In the next section, I will examine Apache Spark cluster performance, and the
    issues that might impact it.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我将研究Apache Spark集群性能以及可能影响它的问题。
- en: Performance
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能
- en: 'Before moving on to the rest of the chapters covering functional areas of Apache
    Spark and extensions to it, I wanted to examine the area of performance. What
    issues and areas need to be considered? What might impact Spark application performance
    starting at the cluster level, and finishing with actual Scala code? I don''t
    want to just repeat what the Spark website says, so have a look at the following
    URL: `http://spark.apache.org/docs/<version>/tuning.html`.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续涵盖Apache Spark的其他章节之前，我想要研究性能领域。需要考虑哪些问题和领域？什么可能会影响从集群级别开始到实际Scala代码结束的Spark应用程序性能？我不想只是重复Spark网站上的内容，所以请查看以下网址：`http://spark.apache.org/docs/<version>/tuning.html`。
- en: Here, `<version>` relates to the version of Spark that you are using, that is,
    latest, or 1.3.1 for a specific version. So, having looked at that page, I will
    briefly mention some of the topic areas. I am going to list some general points
    in this section without implying an order of importance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`<version>`指的是你正在使用的Spark版本，即最新版本或特定版本的1.3.1。因此，在查看了该页面之后，我将简要提及一些主题领域。在本节中，我将列出一些一般要点，而不意味着重要性的顺序。
- en: The cluster structure
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群结构
- en: The size and structure of your big data cluster is going to affect performance.
    If you have a cloud-based cluster, your IO and latency will suffer in comparison
    to an unshared hardware cluster. You will be sharing the underlying hardware with
    multiple customers, and that the cluster hardware maybe remote.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你的大数据集群的大小和结构将影响性能。如果你有一个基于云的集群，你的IO和延迟会与未共享硬件的集群相比受到影响。你将与多个客户共享基础硬件，并且集群硬件可能是远程的。
- en: Also, the positioning of cluster components on servers may cause resource contention.
    For instance, if possible, think carefully about locating Hadoop NameNodes, Spark
    servers, Zookeeper, Flume, and Kafka servers in large clusters. With high workloads,
    you might consider segregating servers to individual systems. You might also consider
    using an Apache system such as Mesos in order to share resources.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，集群组件在服务器上的定位可能会导致资源争用。例如，如果可能的话，仔细考虑在大集群中定位Hadoop NameNodes、Spark服务器、Zookeeper、Flume和Kafka服务器。在高工作负载下，你可能需要考虑将服务器分隔到单独的系统中。你可能还需要考虑使用Apache系统，如Mesos，以共享资源。
- en: Also, consider potential parallelism. The greater the number of workers in your
    Spark cluster for large data sets, the greater the opportunity for parallelism.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，考虑潜在的并行性。对于大数据集，Spark集群中的工作节点数量越多，就越有并行处理的机会。
- en: The Hadoop file system
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop文件系统
- en: 'You might consider using an alternative to HDFS, depending upon your cluster
    requirements. For instance, MapR has the MapR-FS NFS-based read write file system
    for improved performance. This file system has a full read write capability, whereas
    HDFS is designed as a write once, read many file system. It offers an improvement
    in performance over HDFS. It also integrates with Hadoop and the Spark cluster
    tools. Bruce Penn, an architect at MapR, has written an interesting article describing
    its features at: [https://www.mapr.com/blog/author/bruce-penn](https://www.mapr.com/blog/author/bruce-penn).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的集群需求，您可能考虑使用HDFS的替代方案。例如，MapR具有基于MapR-FS NFS的读写文件系统，可提高性能。该文件系统具有完整的读写功能，而HDFS设计为一次写入，多次读取的文件系统。它比HDFS性能更好。它还与Hadoop和Spark集群工具集成。MapR的架构师Bruce
    Penn撰写了一篇有趣的文章，描述了其特性：[https://www.mapr.com/blog/author/bruce-penn](https://www.mapr.com/blog/author/bruce-penn)。
- en: Just look for the blog post entitled `Comparing MapR-FS and HDFS NFS and Snapshots`.
    The links in the article describe the MapR architecture, and possible performance
    gains.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 只需查找名为“比较MapR-FS和HDFS NFS和快照”的博客文章。文章中的链接描述了MapR架构和可能的性能提升。
- en: Data locality
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据本地性
- en: Data locality or the location of the data being processed is going to affect
    latency and Spark processing. Is the data sourced from AWS S3, HDFS, the local
    file system/network, or a remote source?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数据本地性或正在处理的数据的位置将影响延迟和Spark处理。数据是来自AWS S3、HDFS、本地文件系统/网络还是远程来源？
- en: As the previous tuning link mentions, if the data is remote, then functionality
    and data must be brought together for processing. Spark will try to use the best
    data locality level possible for task processing.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的调整链接所述，如果数据是远程的，那么功能和数据必须被整合在一起进行处理。Spark将尝试使用最佳的数据本地性级别来进行任务处理。
- en: Memory
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存
- en: 'In order to avoid **OOM** (**Out of Memory**) messages for the tasks, on your
    Apache Spark cluster, you can consider a number of areas:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在Apache Spark集群上出现**OOM**（**内存不足**）消息，您可以考虑以下几个方面：
- en: Consider the level of physical memory available on your Spark worker nodes.
    Can it be increased?
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑Spark工作节点上可用的物理内存级别。能增加吗？
- en: Consider data partitioning. Can you increase the number of partitions in the
    data used by your Spark application code?
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑数据分区。您能增加Spark应用程序代码中使用的数据分区数量吗？
- en: Can you increase the storage fraction, the memory used by the JVM for storage
    and caching of RDD's?
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您能增加存储分数，即JVM用于存储和缓存RDD的内存使用吗？
- en: Consider tuning data structures used to reduce memory.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑调整用于减少内存的数据结构。
- en: Consider serializing your RDD storage to reduce the memory usage.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑将RDD存储序列化以减少内存使用。
- en: Coding
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码
- en: Try to tune your code to improve Spark application performance. For instance,
    filter your application-based data early in your ETL cycle. Tune your degree of
    parallelism, try to find the resource-expensive parts of your code, and find alternatives.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试调整代码以提高Spark应用程序的性能。例如，在ETL周期的早期筛选应用程序数据。调整并行度，尝试找到代码中资源密集型的部分，并寻找替代方案。
- en: Cloud
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云
- en: 'Although, most of this book will concentrate on examples of Apache Spark installed
    on physically server-based clusters (with the exception of [https://databricks.com/](https://databricks.com/)),
    I wanted to make the point that there are multiple cloud-based options out there.
    There are cloud-based systems that use Apache Spark as an integrated component,
    and cloud-based systems that offer Spark as a service. Even though this book cannot
    cover all of them in depth, I thought that it would be useful to mention some
    of them:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书大部分内容将集中在安装在基于物理服务器的集群上的Apache Spark的示例上（除了[https://databricks.com/](https://databricks.com/)），我想指出有多种基于云的选项。有一些基于云的系统将Apache
    Spark作为集成组件，还有一些基于云的系统提供Spark作为服务。尽管本书无法对所有这些进行深入介绍，但我认为提到其中一些可能会有用：
- en: Databricks is covered in two chapters in this book. It offers a Spark cloud-based
    service, currently using AWS EC2\. There are plans to extend the service to other
    cloud suppliers ([https://databricks.com/](https://databricks.com/)).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书的两章涵盖了Databricks。它提供了一个基于Spark的云服务，目前使用AWS EC2。计划将该服务扩展到其他云供应商（[https://databricks.com/](https://databricks.com/)）。
- en: At the time of writing (July 2015) this book, Microsoft Azure has been extended
    to offer Spark support.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在撰写本书时（2015年7月），微软Azure已扩展到提供Spark支持。
- en: Apache Spark and Hadoop can be installed on Google Cloud.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark和Hadoop可以安装在Google Cloud上。
- en: The Oryx system has been built at the top of Spark and Kafka for real-time,
    large-scale machine learning ([http://oryx.io/](http://oryx.io/)).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oryx系统是基于Spark和Kafka构建的实时大规模机器学习系统（[http://oryx.io/](http://oryx.io/)）。
- en: The velox system for serving machine learning prediction is based upon Spark
    and KeystoneML ([https://github.com/amplab/velox-modelserver](https://github.com/amplab/velox-modelserver)).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于提供机器学习预测的velox系统基于Spark和KeystoneML（[https://github.com/amplab/velox-modelserver](https://github.com/amplab/velox-modelserver)）。
- en: PredictionIO is an open source machine learning service built on Spark, HBase,
    and Spray ([https://prediction.io/](https://prediction.io/)).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PredictionIO是建立在Spark、HBase和Spray上的开源机器学习服务（[https://prediction.io/](https://prediction.io/)）。
- en: SeldonIO is an open source predictive analytics platform, based upon Spark,
    Kafka, and Hadoop ([http://www.seldon.io/](http://www.seldon.io/)).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SeldonIO是一个基于Spark、Kafka和Hadoop的开源预测分析平台（[http://www.seldon.io/](http://www.seldon.io/)）。
- en: Summary
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In closing this chapter, I would invite you to work your way through each of
    the Scala code-based examples in the following chapters. I have been impressed
    by the rate at which Apache Spark has evolved, and I am also impressed at the
    frequency of the releases. So, even though at the time of writing, Spark has reached
    1.4, I am sure that you will be using a later version. If you encounter problems,
    tackle them logically. Try approaching the Spark user group for assistance (`<[user@spark.apache.org](mailto:user@spark.apache.org)>`),
    or check the Spark website at [http://spark.apache.org/](http://spark.apache.org/).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章时，我想邀请你逐个阅读以下章节中基于Scala代码的示例。我对Apache Spark的发展速度印象深刻，也对其发布频率印象深刻。因此，即使在撰写本文时，Spark已经达到1.4版本，我相信你将使用更新的版本。如果遇到问题，请以逻辑方式解决。尝试向Spark用户组寻求帮助（`<[user@spark.apache.org](mailto:user@spark.apache.org)>`），或者查看Spark网站：[http://spark.apache.org/](http://spark.apache.org/)。
- en: 'I am always interested to hear from people, and connect with people on sites
    such as LinkedIn. I am keen to hear about the projects that people are involved
    with and new opportunities. I am interested to hear about Apache Spark, the ways
    that you use it and the systems that you build being used at scale. I can be contacted
    on LinkedIn at: [linkedin.com/profile/view?id=73219349](http://linkedin.com/profile/view?id=73219349).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直对与人交流感兴趣，也愿意在LinkedIn等网站上与人联系。我渴望了解人们参与的项目和新机遇。我对Apache Spark、你使用它的方式以及你构建的系统在规模上的应用很感兴趣。你可以通过LinkedIn联系我：[linkedin.com/profile/view?id=73219349](http://linkedin.com/profile/view?id=73219349)。
- en: 'Or, I can be contacted via my website at [http://semtech-solutions.co.nz/](http://semtech-solutions.co.nz/),
    or finally, by email at: `<[info@semtech-solutions.co.nz](mailto:info@semtech-solutions.co.nz)>`.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以通过我的网站联系我：[http://semtech-solutions.co.nz/](http://semtech-solutions.co.nz)，最后，也可以通过电子邮件联系我：`<[info@semtech-solutions.co.nz](mailto:info@semtech-solutions.co.nz)>`。
