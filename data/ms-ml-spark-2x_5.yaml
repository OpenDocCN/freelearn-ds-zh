- en: Word2vec for Prediction and Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于预测和聚类的Word2vec
- en: In the previous chapters, we covered some basic NLP steps, such as tokenization,
    stoplist removal, and feature creation, by creating a **Term Frequency - Inverse
    Document Frequency** (**TF-IDF**) matrix with which we performed a supervised
    learning task of predicting the sentiment of movie reviews. In this chapter, we
    are going to extend our previous example to now include the amazing power of word
    vectors, popularized by Google researchers, Tomas Mikolov and Ilya Sutskever,
    in their paper, *Distributed Representations of Words and Phrases and their Compositionality.*
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们涵盖了一些基本的NLP步骤，比如分词、停用词移除和特征创建，通过创建一个**词频-逆文档频率**（**TF-IDF**）矩阵，我们执行了一个监督学习任务，预测电影评论的情感。在本章中，我们将扩展我们之前的例子，现在包括由Google研究人员Tomas
    Mikolov和Ilya Sutskever推广的词向量的惊人力量，他们在论文*Distributed Representations of Words and
    Phrases and their Compositionality*中提出。
- en: We will start with a brief overview of the motivation behind word vectors, drawing
    on our understanding of the previous NLP feature extraction techniques, and we'll
    then explain the concept behind the family of algorithms that represent the word2vec
    framework (indeed, word2vec is not just one single algorithm). Then, we will discuss
    a very popular extension of word2vec called doc2vec, whereby we are interested
    in *vectorizing* entire documents into a single fixed array of N numbers. We'll
    further research on this hugely popular field of NLP, or cognitive computing research.
    Next, we will apply a word2vec algorithm to our movie review dataset, examine
    the resulting word vectors, and create document vectors by taking the average
    of the individual word vectors in order to perform a supervised learning task.
    Finally, we will use these document vectors to run a clustering algorithm to see
    how well our movie review vectors group together.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从词向量背后的动机进行简要概述，借鉴我们对之前NLP特征提取技术的理解，然后解释代表word2vec框架的一系列算法的概念（确实，word2vec不仅仅是一个单一的算法）。然后，我们将讨论word2vec的一个非常流行的扩展，称为doc2vec，我们在其中对整个文档进行*向量化*，转换为一个固定长度的N个数字的数组。我们将进一步研究这个极其流行的NLP领域，或认知计算研究。接下来，我们将把word2vec算法应用到我们的电影评论数据集中，检查生成的词向量，并通过取个别词向量的平均值来创建文档向量，以执行一个监督学习任务。最后，我们将使用这些文档向量来运行一个聚类算法，看看我们的电影评论向量有多好地聚集在一起。
- en: 'The power of word vectors is an exploding area of research that companies such
    as Google and Facebook have invested in heavily, given its power of encoding the
    semantic and syntactic meaning of individual words, which we will discuss shortly.
    It''s no coincidence that Spark implemented its own version of word2vec, which
    can also be found in Google''s Tensorflow library and Facebook''s Torch. More
    recently, Facebook announced a new real-time text processing called deep text, using
    their pretrained word vectors, in which they showcased their belief in this amazing
    technology and the implications it has or is having on their business applications.
    However, in this chapter, we''ll just cover a small portion of this exciting area,
    including the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量的力量是一个爆炸性的研究领域，谷歌和Facebook等公司都在这方面进行了大量投资，因为它具有对个别单词的语义和句法含义进行编码的能力，我们将很快讨论。不是巧合的是，Spark实现了自己的word2vec版本，这也可以在谷歌的Tensorflow库和Facebook的Torch中找到。最近，Facebook宣布了一个名为deep
    text的新的实时文本处理，使用他们预训练的词向量，他们展示了他们对这一惊人技术的信念以及它对他们的业务应用产生的或正在产生的影响。然而，在本章中，我们将只涵盖这个激动人心领域的一小部分，包括以下内容：
- en: Explanation of the word2vec algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释word2vec算法
- en: Generalization of the word2vec idea, resulting in doc2vec
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec思想的泛化，导致doc2vec
- en: Application of both the algorithms on the movie reviews dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种算法在电影评论数据集上的应用
- en: Motivation of word vectors
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词向量的动机
- en: 'Similar to the work we did in the previous chapter, traditional NLP approaches
    rely on converting individual words--which we created via tokenization--into a
    format that a computer algorithm can learn (that is, predicting the movie sentiment).
    Doing this required us to convert a single review of *N* tokens into a fixed representation
    by creating a TF-IDF matrix. In doing so, we did two important things *behind
    the scenes*:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一章中所做的工作类似，传统的NLP方法依赖于将通过分词创建的个别单词转换为计算机算法可以学习的格式（即，预测电影情感）。这需要我们将*N*个标记的单个评论转换为一个固定的表示，通过创建一个TF-IDF矩阵。这样做在*幕后*做了两件重要的事情：
- en: Individual words were assigned an integer ID (for example, a hash). For example,
    the word *friend* might be assigned to 39,584, while the word *bestie* might be
    assigned to 99,928,472\. Cognitively, we know that *friend* is very similar to
    *bestie*; however, any notion of similarity is lost by converting these tokens
    into integer IDs.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 个别的单词被分配了一个整数ID（例如，一个哈希）。例如，单词*friend*可能被分配为39,584，而单词*bestie*可能被分配为99,928,472。认知上，我们知道*friend*和*bestie*非常相似；然而，通过将这些标记转换为整数ID，任何相似性的概念都会丢失。
- en: By converting each token into an integer ID, we consequently lose the context
    with which the token was used. This is important because in order to understand
    the cognitive meaning of words, and thereby train a computer to learn that *friend*
    and *bestie* are similar, we need to understand how the two tokens are used (for
    example, their respective contexts).
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将每个标记转换为整数ID，我们因此失去了标记使用的上下文。这很重要，因为为了理解单词的认知含义，从而训练计算机学习*friend*和*bestie*是相似的，我们需要理解这两个标记是如何使用的（例如，它们各自的上下文）。
- en: Given this limited functionality of traditional NLP techniques with respect
    to encoding the semantic and syntactic meaning of words, Tomas Mikolov and other
    researchers explored methods that employ neural networks to better *encode* the
    meaning of words as a vector of *N* numbers (for example, vector *bestie* = [0.574,
    0.821, 0.756, ... , 0.156]). When calculated properly, we will discover that the
    vectors for *bestie* and *friend* are close in space, whereby closeness is defined
    as a cosine similarity. It turns out that these vector representations (often
    referred to as *word embeddings*) give us the ability to capture a richer understanding
    of text.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到传统NLP技术在编码单词的语义和句法含义方面的有限功能，托马斯·米科洛夫和其他研究人员探索了利用神经网络来更好地将单词的含义编码为*N*个数字的向量的方法（例如，向量*好朋友*
    = [0.574, 0.821, 0.756, ... , 0.156]）。当正确计算时，我们会发现*好朋友*和*朋友*的向量在空间中是接近的，其中接近是指余弦相似度。事实证明，这些向量表示（通常称为*单词嵌入*）使我们能够更丰富地理解文本。
- en: Interestingly, using word embeddings also gives us the ability to learn the
    same semantics across multiple languages despite differences in the written form
    (for example, Japanese and English). For example, the Japanese word for movie
    is *eiga* (![](img/00112.jpeg)); therefore, it follows that using word vectors,
    these two words, *movie* and ![](img/00113.jpeg)*,* should be close in the vector
    space despite their differences in appearance. Thus, the word embeddings allow
    for applications to be language-agnostic--yet another reason why this technology
    is hugely popular!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，使用单词嵌入还使我们能够学习跨多种语言的相同语义，尽管书面形式有所不同（例如，日语和英语）。例如，电影的日语单词是*eiga*（![](img/00112.jpeg)）；因此，使用单词向量，这两个单词，*movie*和![](img/00113.jpeg)*，在向量空间中应该是接近的，尽管它们在外观上有所不同。因此，单词嵌入允许应用程序是语言无关的——这也是为什么这项技术非常受欢迎的另一个原因！
- en: Word2vec explained
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec解释
- en: 'First things first: word2vec does not represent a *single* algorithm but rather
    a family of algorithms that attempt to encode the semantic and syntactic *meaning*
    of words as a vector of *N* numbers (hence, word-to-vector = word2vec). We will
    explore each of these algorithms in depth in this chapter, while also giving you
    the opportunity to read/research other areas of *vectorization* of text, which
    you may find helpful.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要明确的是，word2vec并不代表*单一*算法，而是一系列试图将单词的语义和句法*含义*编码为*N*个数字的向量的算法（因此，word-to-vector
    = word2vec）。我们将在本章中深入探讨这些算法的每一个，同时也给您机会阅读/研究文本*向量化*的其他领域，这可能会对您有所帮助。
- en: What is a word vector?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是单词向量？
- en: 'In its simplest form, a word vector is merely a one-hot-encoding, whereby every
    element in the vector represents a word in our vocabulary, and the given word
    is *encoded* with `1` while all the other words elements are encoded with `0`.
    Suppose our vocabulary only has the following movie terms: **Popcorn**, **Candy**,
    **Soda**, **Tickets**, and **Blockbuster**.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，单词向量仅仅是一种独热编码，其中向量中的每个元素代表词汇中的一个单词，给定的单词被编码为`1`，而所有其他单词元素被编码为`0`。假设我们的词汇表只包含以下电影术语：**爆米花**，**糖果**，**苏打水**，**电影票**和**票房大片**。
- en: 'Following the logic we just explained, we could encode the term **Tickets** as
    follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们刚刚解释的逻辑，我们可以将术语**电影票**编码如下：
- en: '![](img/00114.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00114.jpeg)'
- en: Using this simplistic form of encoding, which is what we do when we create a
    bag-of-words matrix, there is no meaningful comparison we can make between words
    (for example, *is Popcorn related to Soda; is Candy similar to Tickets?)*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种简单的编码形式，也就是我们创建词袋矩阵时所做的，我们无法对单词进行有意义的比较（例如，*爆米花是否与苏打水相关；糖果是否类似于电影票？*）。
- en: 'Given these obvious limitations, word2vec attempts to remedy this via distributed
    representations for words. Suppose that for each word, we have a distributed vector
    of, say, 300 numbers that represent a single word, whereby each word in our vocabulary
    is also represented by a distribution of weights across those 300 elements. Now,
    our picture would drastically change to look something like this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些明显的限制，word2vec试图通过为单词提供分布式表示来解决这个问题。假设对于每个单词，我们有一个分布式向量，比如说，由300个数字表示一个单词，其中我们词汇表中的每个单词也由这300个元素中的权重分布来表示。现在，我们的情况将会发生显著变化，看起来会像这样：
- en: '![](img/00115.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00115.jpeg)'
- en: 'Now, given this distributed representation of individual words as 300 numeric
    values, we can make meaningful comparisons among words using a cosine similarity,
    for example. That is, using the vectors for **Tickets** and **Soda**, we can determine
    that the two terms are not related, given their vector representations and their
    cosine similarity to one another. And that''s not all we can do! In their ground-breaking
    paper, Mikolov et. al also performed mathematical functions of word vectors to
    make some incredible findings; in particular, the authors give the following *math
    problem* to their word2vec dictionary:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，鉴于将单词的分布式表示为300个数字值，我们可以使用余弦相似度等方法在单词之间进行有意义的比较。也就是说，使用**电影票**和**苏打水**的向量，我们可以确定这两个术语不相关，根据它们的向量表示和它们之间的余弦相似度。这还不是全部！在他们具有突破性的论文中，米科洛夫等人还对单词向量进行了数学函数的运算，得出了一些令人难以置信的发现；特别是，作者向他们的word2vec字典提出了以下*数学问题*：
- en: '*V(King) - V(Man) + V(Woman) ~ V(Queen)*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*V(国王) - V(男人) + V(女人) ~ V(皇后)*'
- en: It turns out that these distributed vector representations of words are extremely
    powerful in comparison questions (for example, is A related to B?), which is all
    the more remarkable when you consider that this semantic and syntactic learned
    knowledge comes from observing lots of words and their context with no other information
    necessary. That is, we did not have to tell our machine that *Popcorn* is a food,
    noun, singular, and so on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，与传统NLP技术相比，这些单词的分布式向量表示在比较问题（例如，A是否与B相关？）方面非常强大，这在考虑到这些语义和句法学习知识是来自观察大量单词及其上下文而无需其他信息时显得更加令人惊讶。也就是说，我们不需要告诉我们的机器*爆米花*是一种食物，名词，单数等等。
- en: How is this made possible? Word2vec employs the power of neural networks in
    a supervised fashion to learn the vector representation of words (which is an
    unsupervised task). If that sounds a bit like an oxymoron at first, fear not!
    Everything will be made clearer with a few examples, starting first with the **Continuous
    Bag-of-Words** model, commonly referred to as just the **CBOW** model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何实现的呢？Word2vec以一种受监督的方式利用神经网络的力量来学习单词的向量表示（这是一项无监督的任务）。如果一开始听起来有点像矛盾，不用担心！通过一些示例，一切都会变得更清晰，首先从**连续词袋**模型开始，通常简称为**CBOW**模型。
- en: The CBOW model
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CBOW模型
- en: 'First, let''s consider a simple movie review, which will act as our base example
    in the next few sections:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑一个简单的电影评论，这将成为接下来几节中的基本示例：
- en: '![](img/00116.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00116.jpeg)'
- en: 'Now, imagine that we have a window that acts as a slider, which includes the
    main word currently in focus (highlighted in red in the following image), in addition
    to the five words before and after the focus word (highlighted in yellow):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象我们有一个窗口，它就像一个滑块，包括当前焦点单词（在下图中用红色突出显示），以及焦点单词前后的五个单词（在下图中用黄色突出显示）：
- en: '![](img/00117.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.jpeg)'
- en: 'The words in yellow form the context that surrounds the current focus word,
    *ideas*. These context words act as inputs to our feed-forward neural network,
    whereby each word is encoded via one-hot-encoding (all other elements are zeroed
    out) with one hidden layer and one output layer:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 黄色的单词形成了围绕当前焦点单词*ideas*的上下文。这些上下文单词作为输入传递到我们的前馈神经网络，每个单词通过单热编码（其他元素被清零）编码，具有一个隐藏层和一个输出层：
- en: '![](img/00118.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.jpeg)'
- en: In the preceding diagram, the total size of our vocabulary (for example, post-tokenization)
    is denoted by a capital C, whereby we perform one-hot-encoding of each word within
    the context window--in this case, the five words before and after our focus word,
    *ideas*. At this point, we propagate our encoded vectors to our hidden layer via
    weighted sum--just like a *normal* feed-forward neural network--whereby, we specify
    beforehand the number of weights in our hidden layer. Finally, a sigmoid function
    is applied from the single hidden layer to the output layer, which attempts to
    predict the current focus word. This is achieved by maximizing the conditional
    probability of observing the focus word (*idea*), given the context of its surrounding
    words (**film**, **with**, **plenty**, **of**, **smart**, **regarding**, **the**,
    **impact**, **of**, and **alien**). Notice that the output layer is also of the
    same size as our initial vocabulary, C.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们的词汇表的总大小（例如，分词后）由大写C表示，我们对上下文窗口中的每个单词进行单热编码--在这种情况下，是焦点单词*ideas*前后的五个单词。在这一点上，我们通过加权和将编码向量传播到我们的隐藏层，就像*正常*的前馈神经网络一样--在这里，我们预先指定了隐藏层中的权重数量。最后，我们将一个sigmoid函数应用于单隐藏层到输出层，试图预测当前焦点单词。这是通过最大化观察到焦点单词（*idea*）在其周围单词的上下文（**film**，**with**，**plenty**，**of**，**smart**，**regarding**，**the**，**impact**，**of**和**alien**）的条件概率来实现的。请注意，输出层的大小也与我们最初的词汇表C相同。
- en: 'Herein lies the interesting property of both the families of the word2vec algorithm:
    it''s an unsupervised learning algorithm at heart and relies on supervised learning
    to learn individual word vectors. This is true for the CBOW model and also the
    skip-gram model, which we will cover next. Note that at the time of writing this
    book, Spark''s MLlib only incorporates the skip-gram model of word2vec.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是word2vec算法族的有趣特性所在：它本质上是一种无监督学习算法，并依赖于监督学习来学习单词向量。这对于CBOW模型和跳字模型都是如此，接下来我们将介绍跳字模型。需要注意的是，在撰写本书时，Spark的MLlib仅包含了word2vec的跳字模型。
- en: The skip-gram model
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跳字模型
- en: 'In the previous model, we used a window of words before and after the focus
    word to predict the focus word. The skip-gram model takes a similar approach but
    reverses the architecture of the neural network. That is, we are going to start
    with the focus word as our input into our network and then try to predict the
    surrounding contextual words using a single hidden layer:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的模型中，我们使用了焦点词前后的单词窗口来预测焦点词。跳字模型采用了类似的方法，但是颠倒了神经网络的架构。也就是说，我们将以焦点词作为输入到我们的网络中，然后尝试使用单隐藏层来预测周围的上下文单词：
- en: '![](img/00119.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.jpeg)'
- en: As you can see, the skip-gram model is the exact opposite of the CBOW model.
    The training goal of the network is to minimize the summed prediction error across
    all the context words in the output layer, which, in our example, is an input
    of *ideas* and an output layer that predicts *film*, *with*, *plenty*, *of*, *smart*,
    *regarding*, *the*, *impact*, *of*, and *alien*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，跳字模型与CBOW模型完全相反。网络的训练目标是最小化输出层中所有上下文单词的预测误差之和，在我们的示例中，输入是*ideas*，输出层预测*film*，*with*，*plenty*，*of*，*smart*，*regarding*，*the*，*impact*，*of*和*alien*。
- en: In the previous chapter, you saw that we used a tokenization function that removed
    stopwords, such as *the*, *with*, *to*, and so on, which we have not shown here
    intentionally to clearly convey our examples without losing the reader. In the
    example that follows, we will perform the same tokenization function as [Chapter
    4](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893), *Predicting Movie Reviews
    Using NLP and Spark Streaming*, which will remove the stopwords.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，您看到我们使用了一个分词函数，该函数删除了停用词，例如*the*，*with*，*to*等，我们故意没有在这里展示，以便清楚地传达我们的例子，而不让读者迷失。在接下来的示例中，我们将执行与[第4章](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893)相同的分词函数，*使用NLP和Spark
    Streaming预测电影评论*，它将删除停用词。
- en: Fun with word vectors
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词向量的有趣玩法
- en: 'Now that we have condensed words (tokens) into vectors of numbers, we can have
    some fun with them. A few classic examples from the original Google paper that
    you can try for yourself are as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将单词（标记）压缩成数字向量，我们可以对它们进行一些有趣的操作。您可以尝试一些来自原始Google论文的经典示例，例如：
- en: '**Mathematical operations**: As mentioned earlier, the canonical example of
    this is *v(King) - v(Man) + v(Woman) ~ v(Queen)*. Using simple addition, such
    as *v(software) + v(engineer)*, we can come up with some fascinating relationships;
    here are a few more examples:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数学运算**：正如前面提到的，其中一个经典的例子是*v(国王) - v(男人) + v(女人) ~ v(皇后)*。使用简单的加法，比如*v(软件)
    + v(工程师)*，我们可以得出一些迷人的关系；以下是一些更多的例子：'
- en: '![](img/00120.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.jpeg)'
- en: '**Similarity**: Given that we are working with a vector space, we can use the
    cosine similarity to compare one token against many in order to see similar tokens.
    For example, similar words to *v(Spark)* might be *v(MLlib)*, *v(scala)*, *v(graphex)*,
    and so on.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似性**：鉴于我们正在处理一个向量空间，我们可以使用余弦相似度来比较一个标记与许多其他标记，以查看相似的标记。例如，与*v(Spark)*相似的单词可能是*v(MLlib)*、*v(scala)*、*v(graphex)*等等。'
- en: '**Matches/Not Matches**: Which words from a given list do not go together?
    For example, *doesn''t_match[v(lunch, dinner, breakfast, Tokyo)] == v(Tokyo)*.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**匹配/不匹配**：给定一个单词列表，哪些单词是不匹配的？例如，*doesn''t_match[v(午餐, 晚餐, 早餐, 东京)] == v(东京)*。'
- en: '**A is to B as C is to ?**: As per the Google paper, here is a list of word
    comparisons that are made possible by using the skip-gram implementation of word2vec:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A对B就像C对？**：根据Google的论文，以下是通过使用word2vec的skip-gram实现可能实现的单词比较列表：'
- en: '![](img/00121.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00121.jpeg)'
- en: Cosine similarity
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: Word similarity/dissimilarity is measured via cosine similarity, which has a
    very nice property of being bound between `-1` and `1`. Perfect similarity between
    two words will yield a score of `1`, no relation will yield `0`, and `-1` means
    that they are opposites.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过余弦相似度来衡量单词的相似性/不相似性，这个方法的一个很好的特性是它的取值范围在`-1`和`1`之间。两个单词之间的完全相似将产生一个得分为`1`，没有关系将产生`0`，而`-1`表示它们是相反的。
- en: Note that the cosine similarity function for the word2vec algorithm (again,
    just the CBOW implementation in Spark, for now) is already baked into MLlib, which
    we will see shortly.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，word2vec算法的余弦相似度函数（目前仅在Spark中的CBOW实现中）已经内置到MLlib中，我们很快就会看到。
- en: 'Take a look at the following diagram:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下下面的图表：
- en: '![](img/00122.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00122.jpeg)'
- en: For those who are interested in other measures of similarity, a recent research
    has been published that makes a strong case for using **Earth-Mover's Distance**
    (**EMD**), which is a different method from cosine similarity, requiring some
    additional calculation, but shows promising early results.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对其他相似性度量感兴趣的人，最近发表了一项研究，强烈建议使用**Earth-Mover's Distance**（**EMD**），这是一种与余弦相似度不同的方法，需要一些额外的计算，但显示出了有希望的早期结果。
- en: Doc2vec explained
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释doc2vec
- en: As we mentioned in the chapter's introduction, there is an extension of word2vec
    that encodes entire *documents* as opposed to individual words. In this case,
    a document is what you make of it, be it a sentence, a paragraph, an article,
    an essay, and so on. Not surprisingly, this paper came out after the original
    word2vec paper but was also, not surprisingly, coauthored by Tomas Mikolov and
    Quoc Le. Even though MLlib has yet to introduce doc2vec into their stable of algorithms,
    we feel it is necessary for a data science practitioner to know about this extension
    of word2vec, given its promise of and results with supervised learning and information
    retrieval tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章介绍中提到的，有一个word2vec的扩展，它编码整个*文档*而不是单个单词。在这种情况下，文档可以是句子、段落、文章、散文等等。毫不奇怪，这篇论文是在原始word2vec论文之后发表的，但同样也是由Tomas
    Mikolov和Quoc Le合著的。尽管MLlib尚未将doc2vec引入其算法库，但我们认为数据科学从业者有必要了解这个word2vec的扩展，因为它在监督学习和信息检索任务中具有很大的潜力和结果。
- en: Like word2vec, doc2vec (sometimes referred to as *paragraph vectors*) relies
    on a supervised learning task to learn distributed representations of documents
    based on contextual words. Doc2vec is also a family of algorithms, whereby the
    architecture will look extremely similar to the CBOW and skip-gram models of word2vec
    that you learned in the previous sections. As you will see next, implementing
    doc2vec will require a parallel training of both individual word vectors and document
    vectors that represent what we deem as a *document*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与word2vec一样，doc2vec（有时称为*段落向量*）依赖于监督学习任务，以学习基于上下文单词的文档的分布式表示。Doc2vec也是一类算法，其架构将与你在前几节学到的word2vec的CBOW和skip-gram模型非常相似。接下来你会看到，实现doc2vec将需要并行训练单词向量和代表我们所谓的*文档*的文档向量。
- en: The distributed-memory model
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式记忆模型
- en: 'This particular flavor of doc2vec closely resembles the CBOW model of word2vec,
    whereby the algorithm tries to predict a *focus word* given its surrounding *context
    words* but with the addition of a paragraph ID. Think of this as another individual
    contextual word vector that helps with the prediction task but is constant throughout
    what we consider to be a document. Continuing our previous example, if we have
    this movie review (we define one document as one movie review) and our focus word
    is *ideas*, we will now have the following architecture:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特定的doc2vec模型与word2vec的CBOW模型非常相似，算法试图预测一个*焦点单词*，给定其周围的*上下文单词*，但增加了一个段落ID。可以将其视为另一个帮助预测任务的上下文单词向量，但在我们认为的文档中是恒定的。继续我们之前的例子，如果我们有这个电影评论（我们定义一个文档为一个电影评论），我们的焦点单词是*ideas*，那么我们现在将有以下架构：
- en: '![](img/00123.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.jpeg)'
- en: 'Note that as we move down the document and change the *focus word* from *ideas* to
    *regarding*, our context words will obviously change; however, **Document ID:
    456** remains the same. This is a crucial point in doc2vec, as the document ID
    is used in the prediction task:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们在文档中向下移动并将*焦点单词*从*ideas*更改为*regarding*时，我们的上下文单词显然会改变；然而，**文档ID：456**保持不变。这是doc2vec中的一个关键点，因为文档ID在预测任务中被使用：
- en: '![](img/00124.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00124.jpeg)'
- en: The distributed bag-of-words model
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式词袋模型
- en: 'The last algorithm in doc2vec is modeled after the word2vec skip-gram model,
    with one exception--instead of using the *focus* word as the input, we will now
    take the document ID as the input and try to predict *randomly sampled* words
    from the document. That is, we will completely ignore the context words in our
    output altogether:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: doc2vec中的最后一个算法是模仿word2vec跳字模型，唯一的区别是--我们现在将文档ID作为输入，尝试预测文档中*随机抽样*的单词，而不是使用*焦点*单词作为输入。也就是说，我们将完全忽略输出中的上下文单词：
- en: '![](img/00125.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00125.jpeg)'
- en: Like word2vec, we can take similarities of documents of N words using these
    *paragraph vectors*, which have proven hugely successful in both supervised and
    unsupervised tasks. Here are some of the experiments that Mikolov et. al ran using,
    notably, the supervised task that leverages the same dataset we used in the last
    two chapters!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与word2vec一样，我们可以使用这些*段落向量*对N个单词的文档进行相似性比较，在监督和无监督任务中都取得了巨大成功。以下是Mikolov等人在最后两章中使用的相同数据集进行的一些实验！
- en: '![](img/00126.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00126.jpeg)'
- en: 'Information-retrieval tasks (three paragraphs, the first should *sound* closer
    to the second than the third paragraph):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索任务（三段，第一段应该*听起来*比第三段更接近第二段）：
- en: '![](img/00127.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00127.jpeg)'
- en: In the subsequent sections, we are going to create a *poor man's document vector* by
    taking the average of individual word vectors to form our document vector, which
    will encode entire movie reviews of n-length into vectors of dimension, 300.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将通过取个别词向量的平均值来创建一个*穷人的文档向量*，以将n长度的整个电影评论编码为300维的向量。
- en: At the time of writing this book, Spark's MLlib does not have an implementation
    of doc2vec; however, there are many projects that are leveraging this technology,
    which are in the incubation phase and which you can test out.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，Spark的MLlib没有doc2vec的实现；然而，有许多项目正在利用这项技术，这些项目处于孵化阶段，您可以测试。
- en: Applying word2vec and exploring our data with vectors
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用word2vec并使用向量探索我们的数据
- en: 'Now that you have a good understanding of word2vec, doc2vec, and the incredible
    power of word vectors, it''s time we turned our focus to our original IMDB dataset,
    whereby we will perform the following preprocessing:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经对word2vec、doc2vec以及词向量的强大功能有了很好的理解，是时候将我们的注意力转向原始的IMDB数据集，我们将进行以下预处理：
- en: Split words in each movie review by a space
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个电影评论中按空格拆分单词
- en: Remove punctuation
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除标点符号
- en: Remove stopwords and all alphanumeric words
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除停用词和所有字母数字单词
- en: Using our tokenization function from the previous chapter, we will end with
    an array of comma-separated words
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们从上一章的标记化函数，最终得到一个逗号分隔的单词数组
- en: Because we have already covered the preceding steps in [Chapter 4](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893),
    *Predicting Movie Reviews Using NLP and Spark Streaming*, we'll quickly reproduce
    them in this section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经在[第4章](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893)中涵盖了前面的步骤，*使用NLP和Spark
    Streaming预测电影评论*，我们将在本节中快速重现它们。
- en: 'As usual, we begin with starting the Spark shell, which is our working environment:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们从启动Spark shell开始，这是我们的工作环境：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the prepared environment, we can directly load the data:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好的环境中，我们可以直接加载数据：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can also define the tokenization function to split the reviews into tokens,
    removing all the common words:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以定义标记化函数，将评论分割成标记，删除所有常见单词：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With all the building blocks ready, we just apply them to the loaded input
    data, augmenting them by a new column, `reviewTokens`, which holds a list of words
    extracted from the review:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所有构建块准备就绪后，我们只需将它们应用于加载的输入数据，通过一个新列`reviewTokens`对它们进行增强，该列保存从评论中提取的单词列表：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `reviewTokens` column is a perfect input for the word2vec model. We can
    build it using the Spark ML library:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`reviewTokens`列是word2vec模型的完美输入。我们可以使用Spark ML库构建它：'
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Spark implementation has several additional hyperparameters:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Spark实现具有几个额外的超参数：
- en: '`setMinCount`: This is the minimum frequency with which we can create a word.
    It is another processing step so that the model is not running on super rare terms
    with low counts.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setMinCount`：这是我们可以创建单词的最小频率。这是另一个处理步骤，以便模型不会在低计数的超级稀有术语上运行。'
- en: '`setNumIterations`: Typically, we see that a higher number of iterations leads
    to more *accurate* word vectors (think of these as the number of epochs in a traditional
    feed-forward neural network). The default value is set to `1`.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setNumIterations`：通常，我们看到更多的迭代次数会导致更*准确*的词向量（将这些视为传统前馈神经网络中的时代数）。默认值设置为`1`。'
- en: '`setVectorSize`: This is where we declare the size of our vectors. It can be
    any integer with a default size of `100`. Many of the *public* word vectors that
    come pretrained tend to favor larger vector sizes; however, this is purely application-dependent.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setVectorSize`：这是我们声明向量大小的地方。它可以是任何整数，默认大小为`100`。许多*公共*预训练的单词向量倾向于更大的向量大小；然而，这纯粹取决于应用。'
- en: '`setLearningRate`: Just like a *regular* neural network, which we learned about
    in [Chapter 2](part0038.html#147LC0-d18ba71168a441bd917775fac13ca893), *Detecting
    Dark Matter- The Higgs-Boson Particle,* discretion is needed in part by the data
    scientist--too little a learning rate and the model will take forever-and-a-day
    to converge. However, if the learning rate is too large, one risks a non-optimal
    set of learned weights in the network. The default value is `0`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setLearningRate`：就像我们在[第2章](part0038.html#147LC0-d18ba71168a441bd917775fac13ca893)中学到的*常规*神经网络一样，数据科学家需要谨慎--学习率太低，模型将永远无法收敛。然而，如果学习率太大，就会有风险在网络中得到一组非最优的学习权重。默认值为`0`。'
- en: 'Now that our model has finished, it''s time to inspect some of our word vectors!
    Recall that whenever you are unsure of what values your model can produce, always
    hit the *tab* button, as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经完成，是时候检查一些我们的词向量了！请记住，每当您不确定您的模型可以产生什么值时，总是按*tab*按钮，如下所示：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00128.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00128.jpeg)'
- en: Let's take a step back and consider what we just did here. First, we condensed
    the word, *funny*, to a vector composed of an array of 100 floating point numbers
    (recall that this is the default value for the Spark implementation of the word2vec
    algorithm). Because we have reduced all the words in our corpus of reviews to
    the same distributed representation of 100 numbers, we can make comparisons using
    the cosine similarity, which is what the second number in our result set reflects
    (the highest cosine similarity in this case is for the word *nutty*)*.*
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退一步考虑我们刚刚做的事情。首先，我们将单词*funny*压缩为由100个浮点数组成的向量（回想一下，这是Spark实现的word2vec算法的默认值）。因为我们已经将评论语料库中的所有单词都减少到了相同的分布表示形式，即100个数字，我们可以使用余弦相似度进行比较，这就是结果集中的第二个数字所反映的（在这种情况下，最高的余弦相似度是*nutty*一词）*.*
- en: 'Note that we can also access the vector for *funny *or any other word in our
    dictionary using the `getVectors` function, as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还可以使用`getVectors`函数访问*funny*或字典中的任何其他单词的向量，如下所示：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00129.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00129.jpeg)'
- en: A lot of interesting research has been done on clustering similar words together
    based on these representations as well. We will revisit clustering later in this
    chapter when we'll try to cluster similar movie reviews after we perform a hacked
    version of doc2vec in the next section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些表示，已经进行了许多有趣的研究，将相似的单词聚类在一起。在本章后面，当我们在下一节执行doc2vec的破解版本后，我们将重新讨论聚类。
- en: Creating document vectors
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建文档向量
- en: 'So, now that we can create vectors that encode the *meaning* of words, and
    we know that any given movie review post tokenization is an array of *N* words,
    we can begin creating a poor man''s doc2vec by taking the average of all the words
    that make up the review. That is, for each review, by averaging the individual
    word vectors, we lose the specific sequencing of words, which, depending on the
    sensitivity of your application, can make a difference:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们可以创建编码单词*含义*的向量，并且我们知道任何给定的电影评论在标记化后是一个由*N*个单词组成的数组，我们可以开始创建一个简易的doc2vec，方法是取出构成评论的所有单词的平均值。也就是说，对于每个评论，通过对个别单词向量求平均值，我们失去了单词的具体顺序，这取决于您的应用程序的敏感性，可能会产生差异：
- en: '*v(word_1) + v(word_2) + v(word_3) ... v(word_Z) / count(words in review)*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*v(word_1) + v(word_2) + v(word_3) ... v(word_Z) / count(words in review)*'
- en: 'Ideally, one would use a flavor of doc2vec to create document vectors; however,
    doc2vec has yet to be implemented in MLlib at the time of writing this book, so
    for now, we are going to use this simple version, which, as you will see, has
    surprising results. Fortunately, the Spark ML implementation of the word2vec model
    already averages word vectors if the model contains a list of tokens. For example,
    we can show that the phrase, *funny movie*, has a vector that is equal to the
    average of the vectors of the `funny` and `movie` tokens:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，人们会使用doc2vec的一种变体来创建文档向量；然而，截至撰写本书时，MLlib尚未实现doc2vec，因此，我们暂时使用这个简单版本，正如您将看到的那样，它产生了令人惊讶的结果。幸运的是，如果模型包含一个标记列表，Spark
    ML实现的word2vec模型已经对单词向量进行了平均。例如，我们可以展示短语*funny movie*的向量等于`funny`和`movie`标记的向量的平均值：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00130.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00130.jpeg)'
- en: 'Hence, we can prepare our simple version of doc2vec by a simple model transformation:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过简单的模型转换准备我们的简易版本doc2vec：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As practitioners in this field, we have had the unique opportunity to work with
    various flavors of document vectors, including word averaging, doc2vec, LSTM auto-encoders,
    and skip-thought vectors. What we have found is that for small word snippets,
    where the sequencing of words isn't crucial, the simple word averaging does a
    surprisingly good job as supervised learning tasks. That is, not to say that it
    could be improved with doc2vec and other variants but is rather an observation
    based on the many use cases we have seen across various customer applications.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这个领域的从业者，我们有机会与各种文档向量的不同变体一起工作，包括单词平均、doc2vec、LSTM自动编码器和跳跃思想向量。我们发现，对于单词片段较小的情况，单词的顺序并不重要，简单的单词平均作为监督学习任务效果出奇的好。也就是说，并不是说它不能通过doc2vec和其他变体来改进，而是基于我们在各种客户应用程序中看到的许多用例的观察结果。
- en: Supervised learning task
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习任务
- en: 'Like in the previous chapter, we need to prepare the training and validation
    data. In this case, we''ll reuse the Spark API to split the data:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在前一章中一样，我们需要准备训练和验证数据。在这种情况下，我们将重用Spark API来拆分数据：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, let''s perform a grid search using a simple decision tree and a few hyperparameters:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用一个简单的决策树和一些超参数进行网格搜索：
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can now inspect the result and show the best model AUC:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以检查结果并显示最佳模型AUC：
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00131.jpeg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00131.jpeg)'
- en: 'Using this simple grid search on a decision tree, we can see that our *poor
    man''s doc2vec* produces an AUC of 0.7054\. Let''s also expose our exact training
    and test data to H2O and try a deep learning algorithm using the Flow UI:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个简单的决策树网格搜索，我们可以看到我们的*简易doc2vec*产生了0.7054的AUC。让我们还将我们的确切训练和测试数据暴露给H2O，并尝试使用Flow
    UI运行深度学习算法：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that we have successfully published our dataset as H2O frames, let''s open
    the Flow UI and run a deep learning algorithm:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功将我们的数据集发布为H2O框架，让我们打开Flow UI并运行深度学习算法：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'First, note that if we run the `getFrames` command, we will see the two RDDs
    that we seamlessly passed from Spark to H2O:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请注意，如果我们运行`getFrames`命令，我们将看到我们无缝从Spark传递到H2O的两个RDD：
- en: '![](img/00132.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00132.jpeg)'
- en: 'We need to change the type of column label from a numeric column to a categorical
    one by clicking on Convert to enum for both the frames:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通过单击Convert to enum将标签列的类型从数值列更改为分类列，对两个框架都进行操作：
- en: '![](img/00133.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00133.jpeg)'
- en: 'Next, we will run a deep learning model with all of the hyperparameters set
    to their default value and the first column set to be our label:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将运行一个深度学习模型，所有超参数都设置为默认值，并将第一列设置为我们的标签：
- en: '![](img/00134.jpeg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00134.jpeg)'
- en: 'If you did not explicitly create a train/test dataset, you can also perform
    an *n folds cross-validation* using the *nfolds* hyperparameter previously:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有明确创建训练/测试数据集，您还可以使用先前的*nfolds*超参数执行*n折交叉验证*：
- en: '![](img/00135.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00135.jpeg)'
- en: 'After running the model training, we can view the model output by clicking
    View to see the AUC on both the training and validation datasets:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 运行模型训练后，我们可以点击“查看”查看训练和验证数据集上的AUC：
- en: '![](img/00136.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00136.jpeg)'
- en: We see a higher AUC for our simple deep learning model of ~ 0.8289\. This is
    a result without any tuning or hyperparameter searching.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们简单的深度学习模型的AUC更高，约为0.8289。这是没有任何调整或超参数搜索的结果。
- en: 'What are some other steps that we can perform to improve the AUC even more?
    We could certainly try a new algorithm with grid searching for hyperparameters,
    but more interestingly, can we tune the document vectors? The answer is yes and
    no! It''s a partial *no* because, as you will recall, word2vec is an unsupervised
    learning task at heart; however, we can get an idea of the strength of our vectors
    by observing some of the similar words returned. For example, let''s take the
    word `drama`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以执行哪些其他步骤来进一步改进AUC？我们当然可以尝试使用网格搜索超参数来尝试新算法，但更有趣的是，我们可以调整文档向量吗？答案是肯定和否定！这部分是否定的，因为正如您所记得的，word2vec本质上是一个无监督学习任务；但是，通过观察返回的一些相似单词，我们可以了解我们的向量的强度。例如，让我们看看单词`drama`：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00137.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00137.jpeg)'
- en: 'Intuitively, we can look at the results and ask whether these five words are
    *really the best* synonyms (that is, the best cosine similarities) of the the
    word *drama*. Let''s now try rerunning our word2vec model by modifying its input
    parameters:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们可以查看结果，并询问这五个单词是否*真的是*单词*drama*的最佳同义词（即最佳余弦相似性）。现在让我们尝试通过修改其输入参数重新运行我们的word2vec模型：
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00138.jpeg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00138.jpeg)'
- en: You should immediately notice that the synonyms are *better* in terms of similarity
    to the word in question, but also note that the cosine similarities are significantly
    higher for the terms as well. Recall that the default number of iterations for
    word2vec is 1 and now we have set it to `250`, allowing our network to really
    triangulate on some quality word vectors, which can further be improved with more
    preprocessing steps and further tweaking of our hyperparameters for word2vec,
    which should produce document vectors of better quality.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该立即注意到同义词在相似性方面*更好*，但也要注意余弦相似性对这些术语来说显着更高。请记住，word2vec的默认迭代次数为1，现在我们已将其设置为`250`，允许我们的网络真正定位一些高质量的词向量，这可以通过更多的预处理步骤和进一步调整word2vec的超参数来进一步改进，这应该产生更好质量的文档向量。
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Many companies such as Google freely give pretrained word vectors (trained
    on a subset of Google News, incorporating the top three million words/phrases)
    for various vector dimensions: for example, 25d, 50d, 100d, 300d, and so on. You
    can find the code (and the resulting word vectors) here. In addition to Google
    News, there are other sources of trained word vectors, which use Wikipedia and
    various languages. One question you might have is that if companies such as Google
    freely provide pretrained word vectors, why bother building your own? The answer
    to the question is, of course, application-dependent; Google''s pretrained dictionary
    has three different vectors for the word *java* based on capitalization (JAVA,
    Java, and java mean different things), but perhaps, your application is just about
    coffee, so only one *version* of java is all that is really needed.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司（如谷歌）免费提供预训练的词向量（在Google News的子集上训练，包括前三百万个单词/短语）以供各种向量维度使用：例如，25d、50d、100d、300d等。您可以在此处找到代码（以及生成的词向量）。除了Google
    News，还有其他来源的训练词向量，使用维基百科和各种语言。您可能会有一个问题，即如果谷歌等公司免费提供预训练的词向量，为什么还要自己构建？这个问题的答案当然是应用相关的；谷歌的预训练词典对于单词*java*有三个不同的向量，基于大小写（JAVA、Java和java表示不同的含义），但也许，您的应用只涉及咖啡，因此只需要一个*版本*的java。
- en: Our goal for this chapter was to give you a clear and concise explanation of
    the word2vec algorithms and very popular extensions of this algorithm, such as
    doc2vec and sequence-to-sequence learning models, which employ various flavors
    of recurrent neural networks. As always, one chapter is hardly enough time to
    cover this extremely exciting field in natural language processing, but hopefully,
    this is enough to whet your appetite for now!
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是为您清晰简洁地解释word2vec算法以及该算法的非常流行的扩展，如doc2vec和序列到序列学习模型，这些模型采用各种风格的循环神经网络。正如总是一章的时间远远不足以涵盖自然语言处理这个极其激动人心的领域，但希望这足以激发您的兴趣！
- en: As practitioners and researchers in this field, we (the authors) are constantly
    thinking of new ways of representing documents as fixed vectors, and there are
    a plenty of papers dedicated to this problem. You can consider *LDA2vec* and *Skip-thought
    Vectors* for further reading on the subject.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这一领域的从业者和研究人员，我们（作者）不断思考将文档表示为固定向量的新方法，有很多论文致力于解决这个问题。您可以考虑*LDA2vec*和*Skip-thought
    Vectors*以进一步阅读该主题。
- en: 'Some other blogs to add to your reading list regarding **Natural Language Processing**
    (**NLP**) and *Vectorizing* are as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些博客可添加到您的阅读列表，涉及**自然语言处理**（**NLP**）和*向量化*，如下所示：
- en: Google's research blog ([https://research.googleblog.com/](https://research.googleblog.com/))
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌的研究博客（[https://research.googleblog.com/](https://research.googleblog.com/)）
- en: The NLP blog (always well thought out posts with a lot of links for further
    reading,) ([http://nlpers.blogspot.com/](http://nlpers.blogspot.com/)))
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP博客（始终考虑周到的帖子，带有大量链接供进一步阅读，）（[http://nlpers.blogspot.com/](http://nlpers.blogspot.com/)）
- en: The Stanford NLP blog ([http://nlp.stanford.edu/blog/](http://nlp.stanford.edu/blog/))
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福NLP博客（[http://nlp.stanford.edu/blog/](http://nlp.stanford.edu/blog/)）
- en: In the next chapter, we will see word vectors again, where we'll combine all
    of what you have learned so far to tackle a problem that requires *the kitchen
    sink* with respect to the various processing tasks and model inputs. Stick around!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将再次看到词向量，我们将结合到目前为止学到的所有知识来解决一个需要在各种处理任务和模型输入方面“应有尽有”的问题。 敬请关注！
