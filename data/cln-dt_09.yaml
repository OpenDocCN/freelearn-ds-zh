- en: Chapter 9. Stack Overflow Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 Stack Overflow项目
- en: This is the first of two full, chapter-length projects where we will put everything
    we have learned about data cleaning into practice. We can think of each project
    as a dinner party where we show off our best skills from our data science kitchen.
    To host a successful dinner party, we should of course have our menu and guest
    list planned in advance. However, the mark of a true expert is how we react when
    things do not go exactly according to plan. We have all had that moment when we
    forget to buy an important ingredient, despite our carefully prepared recipes
    and shopping lists. Will we be able to adjust our plan to meet the new challenges
    we meet along the way?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两个完整的、章节级的项目中的第一个，我们将在其中实践所有关于数据清理的知识。我们可以把每个项目看作是一场晚宴，在这场晚宴上，我们展示自己在数据科学厨房中的最佳技能。要举办一场成功的晚宴，当然需要提前规划好菜单和嘉宾名单。然而，真正的专家标志是我们如何应对那些事情不完全按照计划进行的时刻。我们每个人都曾经历过这样的时刻：尽管我们精心准备了食谱和购物清单，但还是忘记购买一个重要的食材。我们能否调整计划，面对途中遇到的新挑战？
- en: In this chapter, we will tackle some data cleaning using the publicly-released
    Stack Overflow database dump. Stack Overflow is part of the Stack Exchange family
    of question-and-answer websites. On these sites, writing good questions and answers
    can earn a user points and badges that accumulate over time. To practice our data
    cleaning skills, we will use the same six-step method we introduced back in [Chapter
    1](part0014.xhtml#aid-DB7S1 "Chapter 1. Why Do You Need Clean Data?"), *Why Do
    You Need Clean Data?*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用公开发布的Stack Overflow数据库转储进行一些数据清理。Stack Overflow是Stack Exchange问答网站家族的一部分。在这些网站上，编写优秀的问题和答案可以为用户赢得积分和徽章，这些积分和徽章会随着时间积累。为了练习我们的数据清理技能，我们将使用我们在[第1章](part0014.xhtml#aid-DB7S1
    "第1章. 为什么你需要清洁数据?")中介绍的相同的六步方法，*为什么你需要清洁数据？*。
- en: Decide what kind of problem we are trying to solve — why are we looking at this
    data?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定我们要解决的是什么问题——我们为什么要看这些数据？
- en: Collect and store our data, which consists of downloading and extracting the
    data dump provided by Stack Overflow, creating a MySQL database to hold the data,
    and writing scripts to import the data into the MySQL database. Because the Stack
    Overflow dataset is so massive, we will also create some smaller test tables,
    filled with randomly selected rows.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集和存储我们的数据，包括下载并提取由Stack Overflow提供的数据转储，创建一个MySQL数据库来存储数据，并编写脚本将数据导入MySQL数据库。由于Stack
    Overflow数据集庞大，我们还将创建一些较小的测试表，填充随机选择的行。
- en: Perform some trial cleaning tasks on the test tables before attempting to clean
    the entire dataset.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在尝试清理整个数据集之前，先对测试表执行一些试验性的清理任务。
- en: Analyze the data. Do we need to perform any calculations? Should we write some
    aggregate functions to count or sum the data? Do we need to transform the data
    in some way?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析数据。我们是否需要进行计算？我们是否应该编写一些聚合函数来计数或求和数据？我们是否需要以某种方式转换数据？
- en: Provide visualizations of the data, if possible.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可能的话，提供数据的可视化。
- en: Resolve the problem we set out to investigate. Did our process work? Were we
    successful?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决我们最初要调查的问题。我们的过程是否有效？我们成功了吗？
- en: That is a lot of work, but the more we prepare in advance and the earlier we
    start, the more likely we will be able to call our data science dinner party a
    success.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要大量的工作，但我们提前准备得越充分，开始得越早，我们就越有可能将我们的数据科学晚宴称为成功。
- en: Step one – posing a question about Stack Overflow
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步——提出关于Stack Overflow的问题
- en: To start our project, we need to pose a reasonably interesting question that
    requires some simple data analysis to answer. Where should we begin? First, let's
    review what we know about Stack Overflow. We know that it is a question-and-answer
    website for programmers, and we can assume that programmers probably use a lot
    of source code, error logs, and configuration files in the questions they are
    asking and answering. Furthermore, we know that sometimes posting these kinds
    of long text dumps on a web-based platform like Stack Overflow can be awkward
    because of line lengths, formatting, and other readability issues.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始我们的项目，我们需要提出一个合理有趣的问题，需要一些简单的数据分析来回答。我们该从哪里开始？首先，让我们回顾一下我们对Stack Overflow的了解。我们知道它是一个程序员的问答网站，我们可以假设程序员在提问和回答时可能会使用大量的源代码、错误日志和配置文件。此外，我们知道，有时在像Stack
    Overflow这样的基于Web的平台上发布这些长文本转储会因为行长、格式和其他可读性问题而显得很尴尬。
- en: Seeing so many questions and answers with frustratingly large amounts of text
    made me wonder whether programmers on Stack Overflow ever link to their code or
    log files through an external **paste site** such as Pastebin or JSFiddle, for
    example [http://www.Pastebin.com](http://www.Pastebin.com) is a website where
    you can paste in large amounts of text, such as source code or a log file, and
    the site gives you back a short URL that you can share with others. Most paste
    sites also allow for source code syntax highlighting, which Stack Overflow does
    not do by default.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 看到那么多包含大量文本的提问和回答让我不禁想知道，Stack Overflow 上的程序员是否会通过外部**粘贴站点**（例如 [http://www.Pastebin.com](http://www.Pastebin.com)）链接到他们的代码或日志文件。Pastebin
    是一个可以粘贴大量文本的网站，如源代码或日志文件，网站会返回一个短链接，供你与他人分享。大多数粘贴站点也支持源代码语法高亮，而 Stack Overflow
    默认不支持这一功能。
- en: Using paste sites is very common on IRC and e-mail, but what about on Stack
    Overflow? On one hand, just like in IRC or e-mail, providing a link could make
    a question or an answer shorter, and therefore the rest of the question is much
    easier to read. But on the other hand, depending on the paste site being used,
    the URL is not guaranteed to be functional forever. This means that a question
    or an answer could lose its value over time due to **link rot**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 粘贴站点在 IRC 和电子邮件中非常常见，但在 Stack Overflow 上呢？一方面，就像在 IRC 或电子邮件中一样，提供一个链接可以使问题或回答更简洁，从而使其余的内容更容易阅读。但另一方面，根据使用的粘贴站点，URL
    不一定能永远有效。这意味着，随着时间的推移，问题或回答可能会因为**链接腐烂**而失去价值。
- en: Tools like JSFiddle complicate this issue in one additional way. On an **interactive
    paste** site like JSFiddle, not only can you paste in your source code and get
    a URL for it, but you can also allow others to edit and run the code right in
    the browser. This could be extremely helpful in a question-and-answer scenario
    on Stack Overflow, especially in a browser-based language like JavaScript. However,
    the issue of link rot still exists. Additionally, JSFiddle is a little trickier
    to use for beginners than a simple code dump site like Pastebin.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 像 JSFiddle 这样的工具在某些方面使这个问题变得更加复杂。在一个**互动粘贴**站点（如 JSFiddle）上，你不仅可以粘贴源代码并获得一个
    URL，还可以允许他人在浏览器中编辑并运行代码。这在 Stack Overflow 的问答场景中非常有帮助，尤其是在像 JavaScript 这样的基于浏览器的语言中。然而，链接腐烂的问题依然存在。此外，对于初学者来说，JSFiddle
    比像 Pastebin 这样简单的代码粘贴站点要稍微复杂一些。
- en: '![Step one – posing a question about Stack Overflow](img/image00300.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![步骤一 – 提出关于 Stack Overflow 的问题](img/image00300.jpeg)'
- en: JSFiddle has four windows, one each for HTML, CSS, JavaScript, and the result.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: JSFiddle 有四个窗口，分别用于 HTML、CSS、JavaScript 和结果展示。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'In the community discussion area for Stack Overflow, there has been quite a
    bit of debate about whether paste sites should be used at all, and what the policy
    should be for questions or answers that include paste site links and no actual
    code. In general, even though people tend to agree that a paste site can be useful,
    they also recognize that it is important to protect the longevity and utility
    of Stack Overflow itself. The community has decided that posting questions or
    answers with only links and no code should be avoided. A good place to start if
    you want to recap that discussion is this link: [http://meta.stackexchange.com/questions/149890/](http://meta.stackexchange.com/questions/149890/).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Stack Overflow 的社区讨论区里，关于是否应该使用粘贴站点，尤其是对于只包含粘贴站点链接而没有实际代码的提问或回答，展开了相当多的争论。总的来说，尽管人们普遍认为粘贴站点很有用，但他们也认识到保护
    Stack Overflow 自身的长期性和实用性至关重要。社区决定避免发布仅包含链接而没有代码的提问或回答。如果你想回顾这场讨论，好的起点是这个链接：[http://meta.stackexchange.com/questions/149890/](http://meta.stackexchange.com/questions/149890/)。
- en: 'For our purposes here, we do not need to choose sides in this debate. Instead,
    we can just ask some simple data-driven questions like:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在这里的讨论，我们不需要站队。在这个辩论中，我们可以提出一些简单的基于数据的问题，例如：
- en: How frequently do people use tools like Pastebin and JSFiddle (and other similar
    paste sites) on Stack Overflow?
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人们在 Stack Overflow 上使用像 Pastebin 和 JSFiddle（以及其他类似的粘贴站点）的频率有多高？
- en: Do they use paste sites more in questions or in answers?
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们在提问中还是回答中更常使用粘贴站点？
- en: Do posts that reference a paste site URL tend to also include source code; if
    so, how much?
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引用粘贴站点 URL 的帖子通常会包含源代码吗？如果包含，通常是多少？
- en: We can use these questions for our motivation as we collect, store, and clean
    our Stack Overflow data. Even if it turns out that some of these questions are
    too hard or impossible to answer, remembering what our overall purpose is will
    help direct the type of cleaning we need to do. Keeping our questions in the forefront
    of our minds will stop us from getting too far off track or performing tasks that
    will turn out to be pointless or a waste of time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些问题作为动机，收集、存储和清理我们的 Stack Overflow 数据。即使结果是其中的一些问题太难或无法回答，记住我们的总体目标将有助于指导我们需要进行的清理类型。将问题牢记在心可以防止我们偏离轨道，避免执行最终会变得毫无意义或浪费时间的任务。
- en: Step two – collecting and storing the Stack Overflow data
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二步——收集和存储 Stack Overflow 数据
- en: At the time of writing, Stack Exchange provides the data for their entire family
    of websites—including Stack Overflow—as XML files free for anyone to download.
    In this section, we will download the Stack Overflow files, and import the data
    into a database on our MySQL server. Finally, we will create a few smaller versions
    of these tables for testing purposes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 写作时，Stack Exchange 提供了他们所有网站（包括 Stack Overflow）的数据——以 XML 文件的形式，任何人都可以免费下载。在本节中，我们将下载
    Stack Overflow 文件，并将数据导入到我们 MySQL 服务器的数据库中。最后，我们将创建这些表的几个小版本进行测试。
- en: Downloading the Stack Overflow data dump
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载 Stack Overflow 数据转储
- en: 'All the data available from Stack Exchange can be downloaded at the Internet
    Archive. The September 2014 dump is the latest one available at the time of writing.
    Each Stack Exchange site has one or more files for it, each of which is linked
    to this details page: [https://archive.org/details/stackexchange](https://archive.org/details/stackexchange).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Stack Exchange 上的所有数据可以从互联网档案馆下载。2014 年 9 月的转储是写作时最新的版本。每个 Stack Exchange 网站都有一个或多个与之相关的文件，每个文件都链接到该详细信息页面：[https://archive.org/details/stackexchange](https://archive.org/details/stackexchange)。
- en: 'We are only interested in the eight Stack Overflow files that appear alphabetically
    as shown in the following list:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只关心按字母顺序排列的八个 Stack Overflow 文件，具体如下所示：
- en: '![Downloading the Stack Overflow data dump](img/image00301.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![下载 Stack Overflow 数据转储](img/image00301.jpeg)'
- en: Archive.org listing showing the eight Stack Overflow files we are interested
    in.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Archive.org 列出显示我们感兴趣的八个 Stack Overflow 文件。
- en: For each file in the list, right-click the link and direct your browser to save
    the file to disk.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于列表中的每个文件，右键点击链接，并指示你的浏览器将文件保存到磁盘。
- en: Unarchiving the files
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解压文件
- en: Notice that each file has a `.7z` extension. This is a compressed archive format.
    It can be uncompressed and unarchived using the matching 7-Zip software, or another
    compatible software package. 7-Zip was not one of the more common file archivers
    that we discussed in [Chapter 2](part0020.xhtml#aid-J2B82 "Chapter 2. Fundamentals
    – Formats, Types, and Encodings"), *Fundamentals – Formats, Types, and Encodings*
    and you may not already have a compatible unarchiver installed on your computer,
    so we can consider this our first small wrinkle that we need to work around. Try
    double-clicking on the file to open it, but if you have no installed software
    associated with the `.7z` extension, you will need to install an appropriate 7-Zip
    unarchiver.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个文件都具有 `.7z` 扩展名。这是一种压缩归档格式。可以使用匹配的 7-Zip 软件或其他兼容的软件包进行解压缩和解档。7-Zip 不是我们在[第二章](part0020.xhtml#aid-J2B82
    "第二章：基础 - 格式、类型与编码")中讨论的最常见的文件归档工具，*基础 - 格式、类型与编码*，而且你可能电脑上还没有安装兼容的解压软件，因此我们可以把它当作第一个小问题，需要绕过。尝试双击文件以打开它，但如果你没有安装与
    `.7z` 扩展名相关联的软件，你将需要安装一个适当的 7-Zip 解压工具。
- en: 'For Windows, you can download the 7-Zip software from their website: [http://www.7-zip.org](http://www.7-zip.org)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Windows，你可以从他们的网站下载 7-Zip 软件：[http://www.7-zip.org](http://www.7-zip.org)
- en: For Mac OS X, you can download and install The Unarchiver, a no-cost utility
    available at [http://unarchiver.c3.cx](http://unarchiver.c3.cx)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Mac OS X，你可以下载并安装 The Unarchiver，这是一款免费的实用工具，下载地址：[http://unarchiver.c3.cx](http://unarchiver.c3.cx)
- en: Once you have the software installed, unpack each file in turn. The uncompressed
    files are quite large, so make sure you have disk space that is large enough to
    hold them.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了软件，逐个解压每个文件。解压后的文件相当大，所以请确保你有足够的磁盘空间来存放它们。
- en: Tip
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: On my system right now, comparing the compressed to uncompressed file sizes
    shows that the uncompressed versions are about ten times larger than the compressed
    versions. These files also take several minutes each to unarchive, depending on
    the specifications of the system you are working on, so set aside time for this
    step.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我目前的系统上，比较压缩和未压缩文件的大小显示，未压缩版本大约是压缩版本的十倍。这些文件解压时也需要几分钟时间，具体取决于你正在使用的系统规格，因此请为此步骤预留时间。
- en: Creating MySQL tables and loading data
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 MySQL 表并加载数据
- en: We now have eight `.xml` files, each of which will map to one table in the database
    we are about to build. To create the database and tables, we could either point
    and click our way through it using phpMyAdmin or some other graphical tool, or
    we can run the following simple SQL written by Georgios Gousios and available
    at [https://gist.github.com/gousiosg/7600626](https://gist.github.com/gousiosg/7600626).
    This code includes `CREATE` and `LOAD INFILE` statements for the first six tables,
    but since this script was written, the database dump has had two additional tables
    added to it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有八个 `.xml` 文件，每个文件将映射到我们即将构建的数据库中的一个表。为了创建数据库和表，我们可以使用 phpMyAdmin 或其他图形工具通过点击的方式完成，或者我们可以运行
    Georgios Gousios 编写并可在 [https://gist.github.com/gousiosg/7600626](https://gist.github.com/gousiosg/7600626)
    获得的以下简单 SQL 代码。此代码包含前六个表的 `CREATE` 和 `LOAD INFILE` 语句，但自从编写此脚本以来，数据库转储已经添加了两个额外的表。
- en: 'To build the new table structure, we can run the `head` command in our Terminal
    window or shell in order to inspect the first few lines of this file. From Terminal,
    run it on the smallest of the XML files, `PostLinks.xml`, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建新的表结构，我们可以在终端窗口或 shell 中运行 `head` 命令，以检查文件的前几行。在终端中，运行该命令在最小的 XML 文件 `PostLinks.xml`
    上，方法如下：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first four lines from the results are shown here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结果中的前四行如下所示：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Each row in our new database table should correspond to one of the XML `<row>`
    lines, and each attribute shown in the row line represents one column in the database
    table. We can perform the same `head` command on the `Tags.xml` file to see what
    its columns should be. The following SQL code will handle the `CREATE` statements
    and the `LOAD` statements for the two additional tables:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们新数据库表中的每一行应对应 XML `<row>` 行中的一行，行中显示的每个属性表示数据库表中的一个列。我们可以对 `Tags.xml` 文件执行相同的
    `head` 命令，查看它的列应该是什么。以下 SQL 代码将处理两个额外表的 `CREATE` 语句和 `LOAD` 语句：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the `LOAD XML` syntax is slightly changed, so that we can keep our
    files locally. If your `.xml` files are on your local machine rather than on the
    database server itself, simply add the word `LOCAL` to the `LOAD XML` statements,
    as shown in the preceding code, and you can reference the full path to your file.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`LOAD XML` 语法略有变化，因此我们可以将文件保存在本地。如果你的`.xml`文件存储在本地计算机上而不是数据库服务器上，只需在`LOAD
    XML`语句中添加`LOCAL`字样，如前面的代码所示，然后可以引用文件的完整路径。
- en: 'More information about the MySQL `LOAD XML` syntax is described in the MySQL
    documentation here: [http://dev.mysql.com/doc/refman/5.5/en/load-xml.html](http://dev.mysql.com/doc/refman/5.5/en/load-xml.html).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 MySQL `LOAD XML` 语法的更多信息，请参阅 MySQL 文档：[http://dev.mysql.com/doc/refman/5.5/en/load-xml.html](http://dev.mysql.com/doc/refman/5.5/en/load-xml.html)。
- en: At this point, we have a fully functional MySQL database comprised of eight
    tables, each of which is filled with data. However, these tables are very large.
    There are over 190 million rows in only eight tables. One thing we will notice
    as we start to clean the data and prepare it for analysis, is that if we make
    a mistake on a very large table like `posts`, `comments`, `votes`, or `post_history`,
    rebuilding the table will take a long time. In the next step, we learn how to
    create test tables, so that we contain the damage if one of our programs or queries
    goes awry.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经拥有一个功能完整的 MySQL 数据库，包含八个表，每个表都填充了数据。然而，这些表非常大，只有八个表就有超过一亿九千万行。当我们开始清理数据并为分析做准备时，我们会注意到，如果在像`posts`、`comments`、`votes`或`post_history`这样的超大表上犯错，重建该表将需要很长时间。在下一步中，我们将学习如何创建测试表，以便如果程序或查询出现问题，我们能将损失控制在最小范围。
- en: Building test tables
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建测试表
- en: In this section, we will build eight smaller versions of our original tables,
    each randomly populated with data from the original tables.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建八个较小版本的原始表，每个表都随机填充来自原始表的数据。
- en: 'Our first step is to re-run the `CREATE` statements, but this time prefix each
    table name with `test_`, as shown with one table here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是重新运行`CREATE`语句，但这次在每个表名之前加上`test_`前缀，如下所示：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Other than the addition of `test_` to the front of the table name, these eight
    test tables will be identical to the others we made earlier.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在表名之前添加`test_`外，这八个测试表将与我们之前创建的其他表完全相同。
- en: Next, we need to populate our new test tables with data. We could simply select
    the first 1,000 rows from each table and load those into our test tables. However,
    the downside of doing that is that the rows are in order based on when they were
    inserted into the Stack Overflow database, so we will not have a good sample of
    rows from different dates and time in our subset if we just ask for the first
    1,000 rows. We would like the rows we select to be of a fairly random distribution.
    How can we select a set of rows randomly? We have not had to tackle this question
    before in this book, so here is another case where we have to be ready to try
    new things in order to have our data science dinner party go off without a hitch.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要向新的测试表中填充数据。我们可以简单地从每个表中选择前 1,000 行并加载到测试表中。然而，做这样做的缺点是这些行是根据它们插入 Stack
    Overflow 数据库的顺序排列的，因此如果我们仅请求前 1,000 行，我们的子集就不会有来自不同日期和时间的良好样本。我们希望选择的行有较为随机的分布。我们如何随机选择一组行？在本书中，我们之前并没有处理过这个问题，因此这是另一个需要我们准备尝试新方法的情况，以确保我们的数据科学晚宴顺利进行。
- en: There are a few possibilities for selecting random rows, some of which are more
    efficient than others. Efficiency will be important to us in this project, since
    the tables we are working with are quite large. One thing that makes our random
    row selection a little trickier than expected, is that while our tables do have
    a numeric primary key as the `Id` column, these `Id` numbers are not sequential.
    There are many holes, for example, in the `post_links` table, the first few values
    in the `Id` column are 19, 37, 42, and 48.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种选择随机行的方式，其中一些比其他方式更高效。效率对于我们这个项目来说非常重要，因为我们正在处理的表非常大。让我们随机选择行的一个小难点是，虽然我们的表有一个数字类型的主键作为`Id`列，但这些`Id`号码并不是连续的。例如，在`post_links`表中，`Id`列的前几个值分别是
    19、37、42 和 48。
- en: 'Holes in the data are problematic because a simple randomizer operates like
    this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中的空缺是一个问题，因为简单的随机生成器是这样操作的：
- en: 'Construct a PHP script that asks for the lowest and highest `Id` values in
    the table, like this:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个 PHP 脚本，要求提供表中最低和最高的`Id`值，如下所示：
- en: '[PRE4]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, still in the script, generate some random number between the `min` and
    the `max` value, and request the row with that random value:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，仍然在脚本中，生成一个介于`min`和`max`值之间的随机数，并请求该随机值对应的行：
- en: '[PRE5]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Repeat step 2 for as many rows as you need.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要重复步骤 2，直到获取所需的行数。
- en: Unfortunately, doing this in the Stack Overflow database tables, for example,
    on our `post_links` table, will result in many failed queries, since our data
    has so many holes in the `Id` column, for example, what if step 2 in the preceding
    example generated the number 38? There is no `Id` of 38 in our `post_links` table.
    This means we will need to detect this error and try again with a new random value.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，举个例子，在 Stack Overflow 数据库表中执行此操作，例如在我们的`post_links`表上，将导致许多查询失败，因为我们的数据在`Id`列中有很多空缺。例如，如果前面的示例中的步骤
    2 生成了数字 38，怎么办？我们的`post_links`表中没有`Id`为 38 的记录。这意味着我们需要检测到这个错误，并尝试用一个新的随机值重新执行。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: At this point, someone who knows a little SQL — but not a lot — will usually
    suggest that we just ask MySQL to `ORDER BY rand()` on the column with the `Id`
    in it, and then perform a `LIMIT` command to skim off the number of records we
    want. The problem with this idea is that even if the column we are ordering is
    an indexed column, `ORDER BY rand()` has to read every row in order to assign
    a new random number to it. So, on a very large table, like the tables we have
    in the Stack Overflow database, this does not scale at all. We will be waiting
    way too long for an `ORDER BY rand()` query to finish. `ORDER BY rand()` is a
    tolerable solution for small tables, but not for the sizes we are working with
    here.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一步，一个懂一些SQL的人 — 但不多 — 通常会建议我们只需让MySQL在包含`Id`的列上执行`ORDER BY rand()`，然后执行`LIMIT`命令来挑选我们想要的记录数。这个想法的问题是，即使我们排序的列是索引列，`ORDER
    BY rand()`仍然必须读取每一行来分配一个新的随机数。因此，在像Stack Overflow数据库中那样的大表上，这种方法完全不适用。我们将不得不等待`ORDER
    BY rand()`查询完成，等待时间会太长。`ORDER BY rand()`对于小表是可以接受的解决方案，但对于我们在这里处理的大表大小则不适用。
- en: 'The following PHP script shows how our final random row selection process will
    work to build eight test tables, each with exactly 1,000 rows. Each table will
    be populated by row values that are selected as randomly as possible with as little
    effort as possible, and without us over-engineering this simple problem:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下PHP脚本展示了我们的最终随机行选择过程是如何工作的，它将构建八个测试表，每个表恰好包含1,000行。每个表将通过尽可能随机地选择行值，并尽量减少努力，且不对这个简单问题进行过度设计来填充：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After running that code, we can see we have a nice selection of eight test tables
    to work with if we need them. Testing with these smaller tables ensures that our
    cleaning exercises will go more smoothly and mistakes can be contained. If we
    find that we need more rows in our random tables, we can simply raise the `$table_target_size`
    command and run this again.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 运行该代码后，我们可以看到如果需要的话，我们有一组八个测试表可以使用。使用这些较小的表进行测试能确保我们的清理工作顺利进行，且错误能够被控制。如果我们发现需要更多的行数在我们的随机表中，我们可以简单地提高`$table_target_size`命令并重新运行。
- en: Building test tables is a great habit to get into, once you know how easy it
    is to create them in a simple and useful way.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 构建测试表是一个很好的习惯，一旦你知道如何以简单和有用的方式创建它们。
- en: Step three – cleaning the data
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三步 – 清理数据
- en: Remembering that our goal is to begin analyzing how frequently certain URLs
    are referenced in questions, answers, and comments, it makes sense to begin in
    the text of the Stack Overflow `posts` and `comments` tables. However, since those
    tables are so large, we will use the `test_posts` and `test_comments` tables that
    we just created instead. Then, once we are confident that the queries work perfectly,
    we can re-run them on the larger tables.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们的目标是开始分析在问题、回答和评论中某些URL被引用的频率，因此从Stack Overflow的`posts`和`comments`表中的文本开始是合乎逻辑的。然而，由于这些表非常大，我们将使用我们刚刚创建的`test_posts`和`test_comments`表来代替。然后，一旦我们确信查询完美无缺，我们可以在更大的表上重新运行它们。
- en: 'This cleaning task is very similar to the way we stored the URLs extracted
    from tweets in [Chapter 7](part0045.xhtml#aid-1AT9A1 "Chapter 7. RDBMS Cleaning
    Techniques"), *RDBMS Cleaning Techniques*. However, this project has its own set
    of specific rules:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个清理任务与我们在[第7章](part0045.xhtml#aid-1AT9A1 "第7章 RDBMS清理技术")中提取推文中的URL存储方式非常相似，*RDBMS清理技术*。然而，这个项目有自己的一套具体规则：
- en: Since posts and comments are different entities to begin with, we should make
    separate tables for the URLs that come from posts (including questions and answers)
    and the URLs that come from comments.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于帖子和评论本身就是不同的实体，我们应该为来自帖子（包括问题和回答）和来自评论的URL分别创建不同的表。
- en: Each question, answer, or comment can have multiple URLs inside it. We should
    store all of the URLs, and we should also track the unique identifier for which
    post or comment that URL came from.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个问题、回答或评论可以包含多个URL。我们应该存储所有的URL，同时也应该追踪这些URL来自于哪个帖子或评论的唯一标识符。
- en: Each question and answer can also have formatted source code in it. The `<code>`
    tag is used to delimit source code in the Stack Overflow posts. Separating code
    from posts will help us answer questions about the co-existence of paste site
    URLs and source code. How much code will typically accompany such a link, if any?
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个问题和回答也可以包含格式化的源代码。`<code>`标签用于在Stack Overflow的帖子中界定源代码。将代码与帖子分开将帮助我们回答有关粘贴站点URL和源代码共存的问题。通常有多少代码会与这样的链接一起出现，如果有的话？
- en: Note
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Technically, posts *can* be created without the `<code>` tags, but usually someone
    will quickly edit the errant post to include these useful tags, and will get the
    Stack Overflow points for doing so. For brevity's sake, in this project, we will
    assume that code is included in the `<code>` tags.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从技术上讲，帖子*可以*在没有`<code>`标签的情况下创建，但通常会有人很快编辑这些不规范的帖子，加入这些有用的标签，并因此获得Stack Overflow的积分。为了简洁起见，在本项目中，我们假设代码会被包含在`<code>`标签中。
- en: According to the Stack Overflow database dump documentation (available at [http://meta.stackexchange.com/questions/2677/](http://meta.stackexchange.com/questions/2677/)),
    there are actually eight types of posts, of which questions and answers are just
    two types. So, we will need to limit our queries to posts that have `postTypeId=1`
    for questions and `postTypeId=2` for answers.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据Stack Overflow数据库转储文档（可以在[http://meta.stackexchange.com/questions/2677/](http://meta.stackexchange.com/questions/2677/)查看），实际上有八种帖子类型，其中问题和答案只是两种类型。因此，我们需要将查询限制为`postTypeId=1`表示问题，`postTypeId=2`表示答案。
- en: To ensure that we are only extracting URLs from comments made to questions or
    answers, and not other types of posts, we will need to do a join back to the posts
    table and limit our results to `postTypeId=1` or `postTypeId=2`.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了确保我们只从评论中提取指向问题或答案的URL，而不是其他类型的帖子，我们需要将查询连接回帖子表，并将结果限制为`postTypeId=1`或`postTypeId=2`。
- en: Creating the new tables
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建新表
- en: 'The SQL query to create the database tables we need to store these URLs is
    as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 创建我们需要的数据库表来存储这些URL的SQL查询如下：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We also need to create a table that can hold the code we stripped out of the
    posts:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要创建一个表来存储我们从帖子中剥离出的代码：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: At this point, we have three new tables that will store our cleaned URLs and
    cleaned source code. In the next section, we will extract URLs and code and fill
    up these new tables.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有了三个新表，这些表将存储我们的清理后的URL和清理后的源代码。在接下来的部分中，我们将提取URL和代码，并填充这些新表。
- en: Extracting URLs and populating the new tables
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取URL并填充新表
- en: 'We can modify the scripts we wrote earlier in [Chapter 7](part0045.xhtml#aid-1AT9A1
    "Chapter 7. RDBMS Cleaning Techniques"), *RDBMS Cleaning Techniques*, to extract
    URLs in this new Stack Overflow environment as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以修改我们之前在[第7章](part0045.xhtml#aid-1AT9A1 "第7章. RDBMS清理技术")中编写的脚本，*RDBMS清理技术*，以在这个新的Stack
    Overflow环境中提取URL，具体如下：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We now have fully populated `clean_post_urls` and `clean_comment_urls` tables.
    For my randomly filled test tables, running this script only yields around 100
    comment URLs and 700 post URLs. Still, that is enough to test out our ideas before
    running them on the full dataset.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完全填充了`clean_post_urls`和`clean_comment_urls`表。对于我随机填充的测试表，运行这个脚本只会得到大约100个评论URL和700个帖子URL。不过，这些数据足以在我们对整个数据集运行脚本之前，先测试一下我们的思路。
- en: Extracting code and populating new tables
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取代码并填充新表
- en: To extract the text embedded in `<code>` tags and populate our new `clean_posts_code`
    table, we can run the following script. It is similar to the URL extractor, except
    that it does not need to search comments, because those do not have code delimited
    with a `<code>` tag.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取`<code>`标签中嵌入的文本并填充我们的新`clean_posts_code`表，我们可以运行以下脚本。这个过程类似于URL提取器，唯一不同的是它不需要搜索评论，因为评论中没有用`<code>`标签界定的代码。
- en: 'In my version of the randomly selected test table, the initial `SELECT` yields
    about 800 rows out of 1,000 total rows in the `test_post` table. However, each
    post can have multiple code snippets in it, so the final table ends up having
    over 2,000 rows in it. The following PHP code extracts the text embedded in the
    `<code>` tag:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我随机选择的测试表中，初始的`SELECT`查询从`test_post`表中提取了约800行，表中总共有1000行。然而，每个帖子可能包含多个代码片段，因此最终的表格有超过2000行。以下PHP代码提取了`<code>`标签中嵌入的文本：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We now have a list of all the code that has been printed in each post, and we
    have stored that in the `clean_post_code` table.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一个包含每个帖子中打印出的所有代码的列表，并已将其存储在`clean_post_code`表中。
- en: Step four – analyzing the data
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四步 – 分析数据
- en: 'In this section, we write some code to answer our three questions from the
    beginning of this chapter. We were interested in finding:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们编写一些代码来回答本章开始时的三个问题。我们感兴趣的是寻找：
- en: The counts of different paste sites mentioned by URLs in posts and comments
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帖子和评论中提到的不同粘贴网站的URL数量
- en: The counts of paste site URLs in questions compared to answers
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较问题和答案中粘贴网站URL的数量
- en: Statistics about `<code>` prevalence in posts with a paste site URL
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计带有粘贴网站URL的帖子中`<code>`标签的普及率
- en: Which paste sites are most popular?
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哪些粘贴网站最受欢迎？
- en: 'To answer this first question, we will generate a JSON representation of the
    paste site URLs and counts using the `clean_posts_urls` and `clean_comments_urls`
    tables. This simple analysis will help us find out which pastebin websites are
    popular in this Stack Overflow data dump. The following PHP queries the database
    for the paste sites we have prelisted in the `$pastebins` array and then performs
    a count of the matching URLs from the posts and comments. It uses the test tables,
    so the numbers are much smaller than they would be if we used the real tables:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们将生成一个JSON表示，包含paste站点URL和计数，使用`clean_posts_urls`和`clean_comments_urls`表格。这项简单的分析将帮助我们找出哪些pastebin网站在这个Stack
    Overflow数据集中特别受欢迎。以下PHP查询从数据库中查询我们预先列出的`$pastebins`数组中的paste站点，并执行从帖子和评论中匹配URL的计数。它使用的是测试表格，因此这些数字要比实际表格中的数字小得多：
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can view the JSON output from this script when run against the test tables
    by looking at the output of the script. My random rows produced the following
    counts:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看运行该脚本时从测试表格中得到的JSON输出，通过查看脚本的输出。我的随机行产生了以下计数：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that your values may be different since you have a different set of
    randomly selected URLs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，由于你选择的随机URL集合不同，你的值可能会有所不同。
- en: When we move to the *Step 5 – visualizing the data* section of this chapter,
    we will use this JSON code to build a bar graph. But first, let's answer the other
    two questions we posed earlier.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进入本章的*步骤5 – 数据可视化*部分时，我们将使用这个JSON代码来构建一个条形图。但首先，让我们先回答之前提出的另外两个问题。
- en: Which paste sites are popular in questions and which are popular in answers?
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哪些paste站点在问题中流行，哪些在回答中流行？
- en: 'Our second question was whether the pastebin URLs are more prevalent in question
    posts or answer posts. To start to develop this answer, we will run a series of
    SQL queries. The first query simply asks how many posts are in the `clean_posts_urls`
    table of each type, both questions and answers:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个问题是，pastebin URL在问题帖子中更常见，还是在回答帖子中更常见。为了开始解决这个问题，我们将运行一系列SQL查询。第一个查询仅仅是询问`clean_posts_urls`表中每种类型的帖子数，问题和回答：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The results show that in my randomly selected test set, I have 237 questions
    and 440 answers:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，在我随机选择的测试集中，我有237个问题和440个回答：
- en: '![Which paste sites are popular in questions and which are popular in answers?](img/image00302.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![哪些paste站点在问题中流行，哪些在回答中流行？](img/image00302.jpeg)'
- en: phpMyAdmin shows the count of question URLs and answer URLs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: phpMyAdmin显示问题URL和回答URL的计数。
- en: 'Now, we would want to know the answer to this question: of those 677 URLs,
    divided by questions and answers, how many reference specifically one of the six
    pastebin URLs? We can run the following SQL code to find out:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要知道这个问题的答案：在这677个URL中，按问题和回答分类，有多少个专门引用了六个pastebin网站中的某一个？我们可以运行以下SQL代码来找出答案：
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The results look like the following table. A total of 18 questions reference
    one of the paste sites, whereas 24 answers reference one of the paste sites.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下表所示。共有18个问题引用了某个paste站点，而24个回答引用了某个paste站点。
- en: '![Which paste sites are popular in questions and which are popular in answers?](img/image00303.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![哪些paste站点在问题中流行，哪些在回答中流行？](img/image00303.jpeg)'
- en: phpMyAdmin shows the count of question and answer URLs referencing a pastebin.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: phpMyAdmin显示了引用pastebin的问答URL的计数。
- en: 'One thing to keep in mind about these queries is that they count individual
    URLs. So, if a particular `postId` referenced five URLs, those get counted five
    times. If I am interested in how many posts used a paste site URL once or more,
    I need to modify the first line of both queries as follows. This query counts
    distinct postings in the URLs table:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，这些查询统计的是每个URL的出现次数。所以，如果某个`postId`引用了五个URL，那么它们会被计数五次。如果我关心的是有多少帖子使用了某个paste站点URL一次或更多次，我需要修改两个查询的第一行，如下所示。这个查询统计了URLs表格中不同的帖子：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following screenshot shows how many questions and answers include a URL:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了有多少问题和回答包含了一个URL：
- en: '![Which paste sites are popular in questions and which are popular in answers?](img/image00304.jpeg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![哪些paste站点在问题中流行，哪些在回答中流行？](img/image00304.jpeg)'
- en: phpMyAdmin shows how many questions and answers include any URL.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: phpMyAdmin显示了有多少问题和回答包含了任何URL。
- en: 'This query counts the particular posts in the URLs table that mention a paste
    site:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询统计了在URLs表格中提到paste站点的特定帖子：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The results for this paste site query are as follows, and, as expected, the
    numbers are smaller. In our test set, **11** questions used at least one pastebin
    URL, and so did **16** answers. Combined, 37 posts reference a pastebin URL at
    least once.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个粘贴网站查询的结果如下，正如预期的那样，数字较小。在我们的测试集里，**11** 个问题使用了至少一个粘贴站 URL，**16** 个答案也如此。合计，37
    个帖子至少引用了一个粘贴站 URL。
- en: '![Which paste sites are popular in questions and which are popular in answers?](img/image00305.jpeg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![哪些粘贴网站在问题中流行，哪些在答案中流行？](img/image00305.jpeg)'
- en: PhpMyAdmin shows how many questions and answers include any paste site URL.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: PhpMyAdmin 显示了包含任何粘贴网站 URL 的问题和答案的数量。
- en: 'Even though these results seem to show that people reference paste site URLs
    more in answers than questions, we need to compare them in terms of the overall
    number of questions and answers. We should report our result values as a percentage
    of the total for that post type, question or answer. Taking the totals into account,
    we can now say something like this: "Considering only the questions and answers
    that used any kind of URL at least once, 11 questions out of 81 used at least
    one paste site URL (13.6 percent), and 16 answers out of 222 used at least one
    paste site URL (7.2 percent)." With that in mind, it appears that the questions
    actually outstripped answers in referencing a paste site, almost two to one.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些结果似乎显示人们在答案中引用粘贴网站 URL 的频率高于问题中，但我们需要从问题和答案的总体数量来进行比较。我们应该将结果值报告为该帖子类型（问题或答案）的总数的百分比。考虑到总数，我们现在可以说类似这样的话：“只考虑那些至少一次使用了某种
    URL 的问题和答案，81 个问题中有 11 个使用了至少一个粘贴网站 URL（13.6%），222 个答案中有 16 个使用了至少一个粘贴网站 URL（7.2%）。”
    综上所述，实际上问题在引用粘贴网站方面超过了答案，几乎是两倍。
- en: 'At this point in any data analysis project, you must have a flood of questions,
    like:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何数据分析项目的这个阶段，你一定会有一大堆问题，比如：
- en: How has the usage of paste site URLs in questions and answers changed over time?
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 粘贴网站 URL 在问题和答案中的使用随时间发生了什么变化？
- en: How do questions with paste site URLs fare on voting and favorites?
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有粘贴网站 URL 的问题在投票和收藏中表现如何？
- en: What are the characteristics of users who post questions with paste site URLs?
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布带有粘贴网站 URL 的问题的用户有什么特点？
- en: But since this is a book about data cleaning, and since we still have not even
    visualized this data, I will restrain myself and not answer these for the time
    being. We still have one of our original three questions to answer, and then we
    will move on to visualizing some of our results.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于这是一本关于数据清洗的书，而且我们仍然没有可视化这些数据，我会克制自己，暂时不回答这些问题。我们还有一个原始的三个问题没有回答，然后我们将继续可视化我们的一些结果。
- en: Do posts contain both URLs to paste sites and source code?
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 帖子中是否同时包含粘贴站的 URL 和源代码？
- en: 'Answering our third question requires us to compare the amount of code in the
    Stack Overflow questions to the amount in the Stack Overflow answers, paying particular
    attention to the posts that include some sort of source code, delimited by the
    `<code>` tags. In the *Step three – cleaning the data* section, we extracted all
    code from the posts in our test tables, and created a new table to hold these
    code snippets. Now, a simple query to figure out how many code-containing posts
    there are is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 回答我们第三个问题需要将 Stack Overflow 问题中的代码量与答案中的代码量进行比较，特别关注那些包含某种源代码（由 `<code>` 标签分隔）的帖子。在
    *第三步 - 清洗数据* 部分中，我们从测试表中的帖子中提取了所有代码，并创建了一个新表来存放这些代码片段。现在，一个简单的查询来找出包含代码的帖子数量如下：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In my sample set, this yields 664 code-containing posts, out of the total 1,000
    test posts. Another way to put this is: 664 out of 1,000 posts contain at least
    one <code> tag.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的样本集中，这产生了 664 个包含代码的帖子，来自 1,000 个测试帖子。换句话说：1,000 个帖子中有 664 个包含至少一个 `<code>`
    标签。
- en: 'To figure out how many of these code-containing posts also contained any URL,
    we can run the following SQL query:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出这些包含代码的帖子中有多少也包含了任何 URL，我们可以运行以下 SQL 查询：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: My sample set yields 175 rows for this. We can interpret that by saying that
    17.5 percent of the original test set of 1,000 posts contains code and a URL.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我的样本集返回了 175 行数据。我们可以这样解释：原始测试集 1,000 个帖子中，17.5% 包含了代码和 URL。
- en: 'Now, to figure out how many of the code-containing posts also contained a paste
    site URL, we will narrow down the SQL query even further:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了找出有多少包含代码的帖子也包含了粘贴站 URL，我们将进一步缩小 SQL 查询的范围：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: From these results, we can see that only a tiny set of 25 posts contained both
    source code and a paste site URL. From the second question, we know that 37 distinct
    posts (both questions and answers) used some sort of paste site URL at least once.
    So, 25 out of 37 is about 68 percent. It will be interesting to run these queries
    on the larger dataset to see how those values come out.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些结果中，我们可以看到，只有25篇帖子同时包含源代码和粘贴站点URL。从第二个问题中，我们知道37篇不同的帖子（包括问题和答案）至少使用过一次某种粘贴站点URL。因此，25比37大约是68%。在更大的数据集上运行这些查询，看看这些值如何变化，将会很有趣。
- en: In the meantime, we will carry out some simple visualizations of at least one
    of our questions so that we can close the loop on one complete round of the data
    science six-step process.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，我们将对至少一个问题进行简单的可视化，以便完成数据科学六步法的一个完整回合。
- en: Step five – visualizing the data
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五步 – 可视化数据
- en: The visualization step is sort of like the dessert course in our dinner party.
    Everyone loves a rich graphic and they look so nice. However, since our focus
    in this book is on cleaning rather than analysis and visualization, our graphics
    here will be very simple. In the code that follows, we will use the JavaScript
    D3 visualization libraries to display the results of the first question graphically.
    This visualization will be much simpler than the D3 visualization we did in [Chapter
    4](part0028.xhtml#aid-QMFO2 "Chapter 4. Speaking the Lingua Franca – Data Conversions"),
    *Speaking the Lingua Franca – Data Conversions*. In that chapter, you will recall
    that we built a fairly complicated network diagram, but here, a simple bar graph
    will suffice since all we have to display is just a few labels and counts.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化步骤有点像我们晚宴中的甜点环节。每个人都喜欢丰富的图形，它们看起来非常漂亮。然而，由于本书的重点在于数据清理而非分析与可视化，我们这里的图形将非常简单。在接下来的代码中，我们将使用JavaScript
    D3可视化库，以图形方式展示第一个问题的结果。这次可视化比我们在[第四章](part0028.xhtml#aid-QMFO2 "第四章 – 说共同语言 –
    数据转换")中做的D3可视化要简单得多。你会记得，在那一章中，我们构建了一个相当复杂的网络图，但在这里，简单的条形图就足够了，因为我们只需要展示一些标签和计数。
- en: 'The HTML and JavaScript/D3 code is as follows. This code extends the *Let''s
    Build a Bar Graph* tutorial by Mike Bostock, available at [http://bl.ocks.org/mbostock/3885304](http://bl.ocks.org/mbostock/3885304).
    One of the ways that I extended this code was to make it read the JSON file we
    generated earlier in our `q1.php` script. Our JSON file printed really nicely,
    and was already sorted high to low, so building a little bar graph from that will
    be easy:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是HTML和JavaScript/D3代码。该代码扩展了Mike Bostock的*让我们制作一个条形图*教程，教程地址为[http://bl.ocks.org/mbostock/3885304](http://bl.ocks.org/mbostock/3885304)。我扩展这段代码的方式之一是让它读取我们之前在`q1.php`脚本中生成的JSON文件。我们的JSON文件格式很漂亮，并且已经按从高到低排序，因此从中构建一个小条形图非常容易：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can save this as `q1chart.html`, and view it in the browser. The code calls
    our `q1.php` script, which generates the JSON file that D3 then uses to build
    this chart, the left-hand-side of which is shown here:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其保存为`q1chart.html`，并在浏览器中查看。该代码调用了我们的`q1.php`脚本，后者生成JSON文件，D3则用它来构建这个图表，左侧部分如下所示：
- en: '![Step five – visualizing the data](img/image00306.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![第五步 – 可视化数据](img/image00306.jpeg)'
- en: D3 visualization of the JSON produced from a count of three URLs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: D3可视化展示从三个URL计数生成的JSON数据。
- en: The bar graph shows that the URLs pointing to JSFiddle seem to be the most common,
    at least in my version of the randomly selected test dataset. We knew that just
    from looking at the JSON output from `q1.php`, but it is nice to see it graphically
    as well. In the next section, we will summarize the results and our procedure,
    and talk about where to go next with this project.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 条形图显示指向JSFiddle的URL似乎是最常见的，至少在我版本的随机选择测试数据集中是这样。我们仅通过查看`q1.php`的JSON输出就知道了这一点，但看到图形化的展示仍然让人感觉很直观。接下来的部分，我们将总结结果和过程，并讨论下一步该如何推进这个项目。
- en: Step six – problem resolution
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六步 – 问题解决
- en: From the queries and visualizations we developed in the *Step four – analyzing
    the data* and *Step five – visualizing the data* sections, we can now attempt
    to answer each of the three questions that prompted this project in the first
    place.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们在*第四步 – 分析数据*和*第五步 – 可视化数据*部分开发的查询和可视化中，我们现在可以尝试回答当初促使这个项目的三个问题。
- en: With our first question, we wanted to find counts of the different paste sites
    mentioned by URL in posts and comments. The `q1.php` script and bar graph we made
    to visualize the data show that, at least in the test data, JSFiddle was the most
    commonly referenced of the six paste site URLs we looked at.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个问题中，我们希望查找在帖子和评论中按URL提到的不同粘贴站点的数量。我们创建的`q1.php`脚本和条形图可视化数据显示，至少在测试数据中，JSFiddle是我们查看的六个粘贴站点URL中最常被提及的。
- en: The second question was about whether paste site URLs were more prevalent in
    questions or answers. Our queries show that paste site URLs were about twice as
    likely to occur in questions as opposed to answers, but the numbers for both were
    very small, at least in our test set.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是关于粘贴站点URL在问题和答案中是否更为普遍。我们的查询显示，粘贴站点URL出现在问题中的几率是出现在答案中的大约两倍，但无论是问题还是答案中的数量都很少，至少在我们的测试集中是这样。
- en: For the third question, we wanted to look for whether people were actually heeding
    the advice of Stack Overflow and posting code in addition to a paste site URL.
    In our test set, the queries show that 25 postings (out of 37) include both a
    paste site URL and the recommended accompanying source code. This is a rate of
    about 68 percent compliance.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第三个问题，我们希望查看人们是否真的听从了Stack Overflow的建议，在发布粘贴站点URL的同时也附上了源代码。在我们的测试集中，查询结果显示，37条记录中有25条同时包含了粘贴站点URL和推荐的源代码。这意味着大约有68%的合规率。
- en: There are many additional questions we could ask and answer at this point, and
    many exciting ways we could extend this simple study into something that could
    be even more interesting. For now though, we will focus on the storage and cleaning
    procedures needed to extend this project to use the full dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以提出并回答许多其他问题，也有很多激动人心的方法，可以将这个简单的研究扩展成更有趣的内容。但现在，我们将专注于存储和清理程序，以便将这个项目扩展到使用完整的数据集。
- en: Moving from test tables to full tables
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从测试表迁移到完整表
- en: At the beginning of this project, we made a set of test tables so that we could
    develop our project in a stress-free environment using tables with only 1,000
    rows each. Using small tables with manageable numbers of rows is important in
    cases where we are not sure that our queries will work as we want them to, or
    where we want to experiment with tricky joins, subqueries, weird regular expressions,
    and so on. At this point, though, if we feel good about the queries and scripts
    we have written, it is time to rewrite our procedures to use the full-size tables.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目的开始阶段，我们创建了一组测试表，以便在一个无压力的环境中开发项目，每个表只有1,000行数据。使用行数可控的小型表格非常重要，尤其是在我们不确定查询是否按预期工作的情况下，或者当我们需要尝试一些复杂的连接、子查询、奇怪的正则表达式等。此时，如果我们对已经编写的查询和脚本感到满意，就可以开始重写过程，使用完整大小的表格。
- en: 'Here are the steps we will follow to move the project over to full-size tables:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将采取的步骤，将项目迁移到完整表：
- en: '`DROP` the test tables:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DROP`测试表：'
- en: '[PRE21]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Empty the `cleaned_posts_code`, `cleaned_posts_urls`, and `cleaned_comments_urls`
    tables as follows:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，清空`cleaned_posts_code`、`cleaned_posts_urls`和`cleaned_comments_urls`表：
- en: '[PRE22]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Edit the `urlExtractor.php` and `codeExtractor.php` scripts to `SELECT` from
    the `posts` table rather than the `test_posts` table. These queries can be edited
    as follows:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`urlExtractor.php`和`codeExtractor.php`脚本，使其从`posts`表中`SELECT`而不是从`test_posts`表中选择。可以按如下方式编辑这些查询：
- en: '[PRE23]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Re-run the `urlExtractor.php` and `codeExtractor.php` scripts so that they will
    repopulate the clean code and URL tables we emptied (truncated) earlier.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新运行`urlExtractor.php`和`codeExtractor.php`脚本，以便它们重新填充之前清空（截断）的干净代码和URL表。
- en: At this point, we have the cleaned code and URL tables ready to be analyzed
    and visualized. Take your time when performing these steps, and know that many
    of these queries and scripts will likely take a long time to finish. The posts
    table is quite large and many of the queries we are writing are selected against
    a text column using wildcards.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经准备好清理后的代码和URL表进行分析和可视化。在执行这些步骤时请耐心，了解许多查询和脚本可能需要很长时间才能完成。`posts`表非常大，且我们编写的许多查询都是针对使用通配符的文本列进行选择的。
- en: Summary
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this project, we posed a few questions about the prevalence of URLs on Stack
    Overflow, specifically those related to paste sites like [http://www.Pastebin.com](http://www.Pastebin.com)
    and [http://www.JSFiddle.net](http://www.JSFiddle.net). To get started answering
    these questions, we downloaded data from the Stack Overflow postings (and other
    Stack Overflow data as well) from the Stack Exchange public file release. We built
    a MySQL database and eight tables to hold this data. We then created smaller 1,000-row
    versions of each of those tables for testing purposes, populated with a randomly
    selected sample of the data. From these test tables, we extracted the URLs mentioned
    in each question, answer, and comment, and saved them to a new clean table. We
    also extracted the source code found in the questions and answers, and saved those
    snippets to a new table as well. Finally, we were able to build some simple queries
    and visualizations to help us answer the questions we posed at the beginning.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们提出了几个关于Stack Overflow上URL普及的问题，特别是那些与粘贴网站相关的链接，如[http://www.Pastebin.com](http://www.Pastebin.com)和[http://www.JSFiddle.net](http://www.JSFiddle.net)。为了开始回答这些问题，我们从Stack
    Exchange的公开文件发布中下载了Stack Overflow帖子（以及其他Stack Overflow数据）。我们建立了一个MySQL数据库，并创建了八个表来存储这些数据。然后，我们为测试目的创建了每个表的1,000行小版本，这些版本填充了随机选择的数据样本。通过这些测试表，我们提取了每个问题、答案和评论中提到的URL，并将它们保存到一个新的干净表格中。我们还提取了问题和答案中的源代码，并将这些代码片段保存到一个新的表格中。最后，我们能够构建一些简单的查询和可视化工具，帮助我们回答最初提出的问题。
- en: Despite its modest results, from a data cleaning perspective, our dinner party
    was a success. We were able to make a coherent plan, and take methodical steps
    to put the plan into action and alter it when needed. We are now ready for our
    final project, and a completely different dinner party menu.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果相对简单，从数据清理的角度来看，我们的“晚宴”还是成功的。我们能够制定一个连贯的计划，并采取系统的步骤来执行计划，并在必要时调整。现在我们已经准备好迎接我们的最终项目，以及一个完全不同的晚宴菜单。
- en: In the next chapter, we will collect and clean our own version of a famous Twitter
    dataset.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将收集并清理我们自己版本的著名Twitter数据集。
