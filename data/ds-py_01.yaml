- en: Chapter 1. Programming and Data Science – A New Toolset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 编程与数据科学——一种新工具集
- en: '"Data is a precious thing and will last longer than the systems themselves."'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “数据是宝贵的，它将比系统本身存在得更久。”
- en: – *Tim Berners-Lee*, *inventor of the World Wide Web*
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: – *蒂姆·伯纳斯-李*，*万维网的发明人*
- en: ([https://en.wikipedia.org/wiki/Tim_Berners-Lee](https://en.wikipedia.org/wiki/Tim_Berners-Lee))
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://en.wikipedia.org/wiki/Tim_Berners-Lee](https://en.wikipedia.org/wiki/Tim_Berners-Lee))
- en: 'In this introductory chapter, I''ll start the conversation by attempting to
    answer a few fundamental questions that will hopefully provide context and clarity
    for the rest of this book:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节的开始，我将尝试回答一些基础问题，希望这些问题能为本书的后续内容提供背景和清晰度：
- en: What is data science and why it's on the rise
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是数据科学以及为什么它正在兴起
- en: Why is data science here to stay
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么数据科学将长久存在
- en: Why do developers need to get involved in data science
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么开发人员需要参与数据科学
- en: 'Using my experience as a developer and recent data science practitioner, I''ll
    then discuss a concrete data pipeline project that I worked on and a data science
    strategy that derived from this work, which is comprised of three pillars: data,
    services, and tools. I''ll end the chapter by introducing Jupyter Notebooks which
    are at the center of the solution I''m proposing in this book.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 结合我作为开发人员和最近的数据科学实践者的经验，我将讨论一个具体的数据管道项目，以及从这个项目中衍生出的一种数据科学策略，这个策略由三大支柱组成：数据、服务和工具。最后，我将介绍Jupyter
    Notebooks，它是我在本书中提出的解决方案的核心。
- en: What is data science
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是数据科学
- en: If you search the web for a definition of data science, you will certainly find
    many. This reflects the reality that data science means different things to different
    people. There is no real consensus on what data scientists exactly do and what
    training they must have; it all depends on the task they're trying to accomplish,
    for example, data collection and cleaning, data visualization, and so on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在网上搜索“数据科学”的定义，你一定会找到很多不同的解释。这反映了数据科学对不同的人意味着不同的东西。对于数据科学家究竟做什么以及他们必须接受什么样的训练，实际上并没有达成一致意见；这完全取决于他们要完成的任务，例如数据收集与清洗、数据可视化等。
- en: 'For now, I''ll try to use a universal and, hopefully, consensual definition:
    *data science refers to the activity of analyzing a large amount of data in order
    to extract knowledge and insight leading to actionable decisions*. It''s still
    pretty vague though; one can ask what kind of knowledge, insight, and actionable
    decision are we talking about?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我会尝试使用一个普遍且希望得到共识的定义：*数据科学指的是分析大量数据，以提取知识和见解，从而做出可操作的决策*。不过这仍然有些模糊；人们可以问，我们到底在谈论什么样的知识、见解和可操作的决策？
- en: 'To orient the conversation, let''s reduce the scope to three fields of data
    science:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了引导话题，让我们将数据科学的领域范围缩小到三个领域：
- en: '**Descriptive analytics**: Data science is associated with information retrieval
    and data collection techniques with the goal of reconstituting past events to
    identify patterns and find insights that help understand what happened and what
    caused it to happen. An example of this is looking at sales figures and demographics
    by region to categorize customer preferences. This part requires being familiar
    with statistics and data visualization techniques.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述性分析**：数据科学与信息检索和数据收集技术相关，目的是重建过去的事件，以识别模式并发现有助于理解发生了什么以及是什么导致其发生的见解。一个例子是查看销售数据和按地区划分的人口统计信息，以便对客户偏好进行分类。这部分需要熟悉统计学和数据可视化技术。'
- en: '**Predictive analytics**: Data science is a way to predict the likelihood that
    some events are currently happening or will happen in the future. In this scenario,
    the data scientist looks at past data to find explanatory variables and build
    statistical models that can be applied to other data points for which we''re trying
    to predict the outcome, for example, predicting the likelihood that a credit card
    transaction is fraudulent in real-time. This part is usually associated with the
    field of machine learning.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测性分析**：数据科学是一种预测当前正在发生或将来会发生某些事件的可能性的方法。在这种情况下，数据科学家会查看过去的数据，找出解释变量，并构建可以应用于其他数据点的统计模型，以预测其结果。例如，实时预测信用卡交易是否存在欺诈行为。这部分通常与机器学习领域相关。'
- en: '**Prescriptive analytics**: In this scenario, data science is seen as a way
    to make better decisions, or perhaps I should say data-driven decisions. The idea
    is to look at multiple options and using simulation techniques, quantify, and
    maximize the outcome, for example, optimizing the supply chain by looking at minimizing
    operating costs.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规范性分析**：在这种情况下，数据科学被看作是一种做出更好决策的方法，或者我应该说是基于数据的决策。这个想法是，查看多个选项，并使用模拟技术量化并最大化结果，例如，通过优化供应链来最小化运营成本。'
- en: In essence, descriptive data science answers the question of *what* (does the
    data tells me), predictive data science answers the question of *why* (is the
    data behaving a certain way), and prescriptive data science answers the questions
    of *how* (do we optimize the data toward a specific goal).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，描述性数据科学回答了*什么*（数据告诉我什么），预测性数据科学回答了*为什么*（数据为什么以某种方式表现），而规范性数据科学回答了*如何*（我们如何将数据优化以实现特定目标）。
- en: Is data science here to stay?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学会一直存在吗？
- en: 'Let''s get straight to the point from the start: I strongly think that the
    answer is yes.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一开始就直截了当地说：我坚信答案是肯定的。
- en: 'However, that was not always the case. A few years back, when I first started
    hearing about data science as a concept, I initially thought that it was yet another
    marketing buzzword to describe an activity that already existed in the industry:
    **Business Intelligence** (**BI**). As a developer and architect working mostly
    on solving complex system integration problems, it was easy to convince myself
    that I didn''t need to get directly involved in data science projects, even though
    it was obvious that their numbers were on the rise, the reason being that developers
    traditionally deal with data pipelines as black boxes that are accessible with
    well-defined APIs. However, in the last decade, we''ve seen exponential growth
    in data science interest both in academia and in the industry, to the point it
    became clear that this model would not be sustainable.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，情况并非总是如此。几年前，当我第一次听说数据科学作为一个概念时，我最初以为这只是另一个营销术语，用来描述行业中已经存在的活动：**商业智能**（**BI**）。作为一名开发者和架构师，主要解决复杂的系统集成问题，我很容易说服自己不需要直接参与数据科学项目，尽管显然它们的数量在不断增加，原因是开发者传统上将数据管道视为可以通过明确定义的API访问的黑箱。然而，在过去的十年里，我们看到数据科学在学术界和工业界的兴趣呈指数级增长，直到它变得非常清楚，这种模型将无法持续下去。
- en: As data analytics are playing a bigger and bigger role in a company's operational
    processes, the developer's role was expanded to get closer to the algorithms and
    build the infrastructure that would run them in production. Another piece of evidence
    that data science has become the new *gold rush* is the extraordinary growth of
    data scientist jobs, which have been ranked number one for 2 years in a row on
    Glassdoor ([https://www.prnewswire.com/news-releases/glassdoor-reveals-the-50-best-jobs-in-america-for-2017-300395188.html](https://www.prnewswire.com/news-releases/glassdoor-reveals-the-50-best-jobs-in-america-for-2017-300395188.html))
    and are consistently posted the most by employers on Indeed. Headhunters are also
    on the prowl on LinkedIn and other social media platforms, sending tons of recruiting
    messages to whoever has a profile showing any data science skills.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据分析在公司运营过程中扮演着越来越重要的角色，开发者的角色也扩展到了更接近算法的部分，并构建能够在生产环境中运行它们的基础设施。数据科学已成为新的*淘金热*的另一个证据就是数据科学家职位的非凡增长，这些职位连续两年在Glassdoor上排名第一（[https://www.prnewswire.com/news-releases/glassdoor-reveals-the-50-best-jobs-in-america-for-2017-300395188.html](https://www.prnewswire.com/news-releases/glassdoor-reveals-the-50-best-jobs-in-america-for-2017-300395188.html)），并且在Indeed上经常是雇主发布最多的职位。猎头也在LinkedIn和其他社交媒体平台上活跃，向任何拥有数据科学技能资料的人发送大量招聘信息。
- en: One of the main reasons behind all the investment being made into these new technologies
    is the hope that it will yield major improvements and greater efficiencies in
    the business. However, even though it is a growing field, data science in the
    enterprise today is still confined to experimentation instead of being a core
    activity as one would expect given all the hype. This has lead a lot of people
    to wonder if data science is a passing fad that will eventually subside and yet
    another technology bubble that will eventually pop, leaving a lot of people behind.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些新技术背后投资的主要原因之一，是希望它们能为企业带来显著的改进和更高的效率。然而，尽管这是一个正在发展的领域，今天企业中的数据科学仍然局限于实验，而没有像大家预期的那样成为核心活动。这使得许多人开始怀疑，数据科学是否只是一个短暂的潮流，最终会消退，成为又一个技术泡沫，最终破灭，留下许多人被抛在后头。
- en: These are all good points, but I quickly realized that it was more than just
    a passing fad; more and more of the projects I was leading included the integration
    of data analytics into the core product features. Finally, it is when the IBM
    Watson Question Answering system won at a game of *Jeopardy!* against two experienced
    champions, that I became convinced that data science, along with the cloud, big
    data, and **Artificial Intelligence** (**AI**), was here to stay and would eventually
    change the way we think about computer science.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是很好的观点，但我很快意识到这不仅仅是一个昙花一现的潮流；我所领导的越来越多的项目开始将数据分析融入到核心产品功能中。最终，当IBM Watson问答系统在*危险边缘*游戏中战胜了两位经验丰富的冠军时，我深信数据科学、云计算、大数据和**人工智能**（**AI**）将会长久存在，并最终改变我们对计算机科学的理解。
- en: Why is data science on the rise?
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么数据科学在崛起？
- en: There are multiple factors involved in the meteoric rise of data science.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的迅速崛起涉及多个因素。
- en: First, the amount of data being collected keeps growing at an exponential rate.
    According to recent market research from the IBM Marketing Cloud ([https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=WRL12345GBEN](https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=WRL12345GBEN))
    something like 2.5 quintillion bytes are created every day (to give you an idea
    of how big that is, that's 2.5 billion of billion bytes), but yet only a tiny
    fraction of this data is ever analyzed, leaving tons of missed opportunities on
    the table.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，收集的数据量正在以指数级的速度增长。根据IBM营销云的最新市场研究（[https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=WRL12345GBEN](https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=WRL12345GBEN)），每天大约会产生2.5万亿字节的数据（为了让你更清楚它有多大，这相当于25亿亿字节），但这些数据中只有极小的一部分会被分析，导致错失了大量的机会。
- en: Second, we're in the midst of a cognitive revolution that started a few years
    ago; almost every industry is jumping on the AI bandwagon, which includes **natural
    language processing** (**NLP**) and machine learning. Even though these fields
    existed for a long time, they have recently enjoyed the renewed attention to the
    point that they are now among the most popular courses in colleges as well as
    getting the lion's share of open source activities. It is clear that, if they
    are to survive, companies need to become more agile, move faster, and transform
    into digital businesses, and as the time available for decision-making is shrinking
    to near real-time, they must become fully data-driven. If you also include the
    fact that AI algorithms need high-quality data (and a lot of it) to work properly,
    we can start to understand the critical role played by data scientists.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们正处于一场几年前开始的认知革命中；几乎所有行业都在纷纷加入AI的浪潮，这其中包括**自然语言处理**（**NLP**）和机器学习。尽管这些领域已经存在了很长时间，但最近它们获得了重新的关注，以至于现在它们已经成为大学中最受欢迎的课程之一，并且占据了开源活动的大部分份额。显然，如果企业想要生存下去，它们需要变得更加敏捷，更快地行动，并转型为数字化企业。而随着决策时间的缩短，几乎接近实时，它们必须完全依赖数据。再加上AI算法需要高质量（并且大量的）数据才能正常工作，我们可以开始理解数据科学家所扮演的关键角色。
- en: Third, with advances in cloud technologies and the development of **Platform
    as a Service** (**PaaS**), access to massive compute engines and storage has never
    been easier or cheaper. Running big data workloads, once the purview of large
    corporations, is now available to smaller organizations or any individuals with
    a credit card; this, in turn, is fueling the growth of innovation across the board.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，随着云技术的进步和**平台即服务**（**PaaS**）的发展，访问海量计算引擎和存储变得前所未有的简单且廉价。曾经只有大型企业才能承担的大数据工作负载，现在对小型组织或任何拥有信用卡的个人都可用；这反过来又推动了各领域创新的增长。
- en: For these reasons, I have no doubt that, similar to the AI revolution, data
    science is here to stay and that its growth will continue for a long time. But
    we also can't ignore the fact that data science hasn't yet realized its full potential
    and produced the expected results, in particular helping companies in their transformation
    into data-driven organizations. Most often, the challenge is achieving that next
    step, which is to transform data science and analytics into a core business activity
    that ultimately enables clear-sighted, intelligent, bet-the-business decisions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，我毫不怀疑，类似于人工智能革命，数据科学将长期存在，而且它的增长将持续很长时间。但我们也不能忽视数据科学尚未充分发挥其潜力，并且未能产生预期的结果，特别是在帮助公司转型为数据驱动型组织方面。最常见的挑战是实现下一步，即将数据科学和分析转化为核心业务活动，最终推动清晰、智能的“赌命”决策。
- en: What does that have to do with developers?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这与开发者有什么关系？
- en: This is a very important question that we'll spend a lot of time developing
    in the coming chapters. Let me start by looking back at my professional journey;
    I spent most of my career as a developer, dating back over 20 years ago, working
    on many aspects of computer science.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常重要的问题，我们将在接下来的章节中花费大量时间探讨。让我先回顾一下我的职业生涯；我大部分时间作为开发者，回溯到20多年前，参与了计算机科学的多个方面。
- en: I started by building various tools that helped with software internationalization
    by automating the process of translating the user interface into multiple languages.
    I then worked on a LotusScript (scripting language for Lotus Notes) editor for
    Eclipse that would interface directly with the underlying compiler. This editor
    provided first-class development features, such as content assist, which provides
    suggestions, real-time syntax error reporting, and so on. I then spent a few years
    building middleware components based on Java EE and OSGI ([https://www.osgi.org](https://www.osgi.org))
    for the Lotus Domino server. During that time, I led a team that modernized the
    Lotus Domino programming model by bringing it to the latest technologies available
    at the time. I was comfortable with all aspects of software development, frontend,
    middleware, backend data layer, tooling, and so on; I was what some would call
    a full-stack developer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我开始通过构建各种工具来帮助软件国际化，自动化将用户界面翻译成多种语言的过程。随后，我在Eclipse中开发了一个LotusScript（Lotus Notes的脚本语言）编辑器，该编辑器可以直接与底层编译器进行交互。这个编辑器提供了第一流的开发功能，例如内容提示（提供建议）、实时语法错误报告等等。接着，我花了几年时间为Lotus
    Domino服务器构建基于Java EE和OSGI的中间件组件（[https://www.osgi.org](https://www.osgi.org)）。在此期间，我带领团队通过将其引入当时可用的最新技术来现代化Lotus
    Domino编程模型。我对软件开发的各个方面都很熟悉，包括前端、中间件、后端数据层、工具等；我可以说是一个全栈开发者。
- en: That was until I saw a demo of the IBM Watson Question Answering system that
    beat longtime champions Brad Rutter and Ken Jennings at a game of *Jeopardy!*
    in 2011\. Wow! This was groundbreaking, a computer program capable of answering
    natural language questions. I was very intrigued and, after doing some research,
    meeting with a few researchers involved in the project, and learning about the
    techniques used to build this system, such as NLP, machine learning, and general
    data science, I realized how much potential this technology would have if applied
    to other parts of the business.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 直到我看到IBM Watson问答系统的演示，2011年它在《危险边缘！》节目中击败了长期冠军Brad Rutter和Ken Jennings。哇！这真是一个突破性进展，一个能够回答自然语言问题的计算机程序。我非常感兴趣，在做了一些研究，拜访了几个参与该项目的研究人员，并了解了构建此系统所用的技术，如NLP、机器学习和通用数据科学后，我意识到，如果将这项技术应用到商业的其他领域，它将有巨大的潜力。
- en: A few months later, I got an opportunity to join the newly formed Watson Division
    at IBM, leading a tooling team with the mission to build data ingestion and accuracy
    analysis capabilities for the Watson system. One of our most important requirements
    was to make sure the tools were easy to use by our customers, which is why, in
    retrospect, giving this responsibility to a team of developers was the right move.
    From my perspective, stepping into that job was both challenging and enriching.
    I was leaving a familiar world where I excelled at designing architectures based
    on well-known patterns and implementing frontend, middleware, or backend software
    components to a world focused mostly on working with a large amount of data; acquiring
    it, cleansing it, analyzing it, visualizing it, and building models. I spent the
    first six months drinking from the firehose, reading, and learning about NLP,
    machine learning, information retrieval, and statistical data science, at least
    enough to be able to work on the capabilities I was building.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月后，我有机会加入IBM新成立的Watson部门，领导一个工具团队，任务是为Watson系统构建数据摄取和准确性分析功能。我们的一个重要需求是确保工具对客户易于使用，这也是为什么回头看，将这个责任交给一个开发团队是正确的决定。从我的角度来看，接手这个职位既具挑战性又充满收获。我离开了一个熟悉的世界，在那里我擅长设计基于常见模式的架构，实施前端、中间件或后端软件组件，进入了一个主要聚焦于大数据工作的世界：获取数据、清洗数据、分析数据、可视化数据并构建模型。我花了前六个月像从火管中喝水一样，阅读和学习自然语言处理（NLP）、机器学习、信息检索和统计数据科学，至少足以能够参与我所构建的功能。
- en: It was at that time, interacting with the research team to bring these algorithms
    to market, that I realized how important developers and data scientists needed
    to collaborate better. The traditional approach of having data scientists solve
    complex data problems in isolation and then throw the results "over the wall"
    to developers for them to operationalize them is not sustainable and doesn't scale,
    considering that the amount of data to process keeps growing exponentially and
    the required time to market keeps shrinking.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在那个时候，我与研究团队合作将这些算法推向市场，我意识到开发人员和数据科学家需要更好地协作。传统的方法是让数据科学家单独解决复杂的数据问题，然后将结果“抛给”开发人员，由他们来实现这些结果。但考虑到数据处理量不断呈指数级增长，以及市场所需时间日益缩短，这种做法是不可持续的，且无法扩展。
- en: 'Instead, their role needs to be shifting toward working as one team, which
    means that data scientists must work and think like software developers and vice
    versa. Indeed, this looks very good on paper: on the one hand, data scientists
    will benefit from tried-and-true software development methodologies such as Agile—with
    its rapid iterations and frequent feedback approach—but also from a rigorous software
    development life cycle that brings compliance with enterprise needs, such as security,
    code reviews, source control, and so on. On the other hand, developers will start
    thinking about data in a new way: as analytics meant to discover insights instead
    of just a persistence layer with queries and **CRUD** (short for, **create**,
    **read**, **update**, **delete**) APIs.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，他们的角色需要转变为作为一个团队共同工作，这意味着数据科学家必须像软件开发人员一样思考和工作，反之亦然。事实上，这在理论上看起来非常好：一方面，数据科学家将从成熟的软件开发方法论中受益，例如敏捷开发——其快速迭代和频繁反馈的方式——同时也能从严谨的软件开发生命周期中获益，带来符合企业需求的合规性，例如安全性、代码审查、源代码控制等。另一方面，开发人员将开始以全新的方式思考数据：将其视为用于发现洞察的分析，而不仅仅是具有查询和**CRUD**（即**创建**、**读取**、**更新**、**删除**）API的持久层。
- en: Putting these concepts into practice
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将这些概念付诸实践
- en: 'After 4 years as the Watson Core Tooling lead architect building self-service
    tooling for the Watson Question Answering system, I joined the Developer Advocacy
    team of the Watson Data Platform organization which has the expanded mission of creating
    a platform that brings the portfolio of data and cognitive services to the IBM
    public cloud. Our mission was rather simple: win the hearts and minds of developers
    and help them be successful with their data and AI projects.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在担任Watson核心工具首席架构师并为Watson问答系统构建自助工具四年后，我加入了IBM Watson数据平台组织的开发者倡导团队，后者的使命是创建一个平台，将数据和认知服务的产品组合带到IBM公共云。我们的任务相当简单：赢得开发者的心，并帮助他们在数据和AI项目中取得成功。
- en: 'The work had multiple dimensions: education, evangelism, and activism. The
    first two are pretty straightforward, but the concept of activism is relevant
    to this discussion and worth explaining in more details. As the name implies,
    activism is about bringing change where change is needed. For our team of 15 developer
    advocates, this meant walking in the shoes of developers as they try to work with
    data—whether they''re only getting started or already operationalizing advanced
    algorithms—feel their pain and identify the gaps that should be addressed. To that end,
    we built and made open source numerous sample data pipelines with real-life use
    cases.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作有多个维度：教育、宣传和激进主义。前两者比较直接，但激进主义的概念与这次讨论相关，并且值得详细解释。顾名思义，激进主义是关于在需要改变的地方带来变革。对于我们15位开发者倡导者的团队来说，这意味着要站在开发者的角度思考他们在处理数据时的体验——无论他们是刚刚开始，还是已经在应用高级算法——感受他们的痛点并识别应该解决的空白。为此，我们构建并开源了多个带有真实场景案例的数据管道示例。
- en: 'At a minimum, each of these projects needed to satisfy three requirements:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，这些项目需要满足三个基本要求：
- en: The raw data used as input must be publicly available
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用作输入的原始数据必须是公开可用的
- en: Provide clear instructions for deploying the data pipeline on the cloud in a reasonable
    amount of time
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供清晰的指令，确保数据管道可以在合理时间内部署到云端
- en: Developers should be able to use the project as a starting point for similar
    scenarios, that is, the code must be highly customizable and reusable
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发者应该能够将该项目作为类似场景的起点，即代码必须具备高度的可定制性和可重用性
- en: 'The experience and insights we gained from these exercises were invaluable:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些练习中获得的经验和洞察是无价的：
- en: Understanding which data science tools are best suited for each task
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解哪些数据科学工具最适合每项任务
- en: Best practice frameworks and languages
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳实践框架和语言
- en: Best practice architectures for deploying and operationalizing analytics
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署和操作分析的最佳实践架构
- en: 'The metrics that guided our choices were multiple: accuracy, scalability, code
    reusability, but most importantly, improved collaboration between data scientists
    and developers.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 指导我们选择的指标有很多：准确性、可扩展性、代码重用性，但最重要的是，改善数据科学家与开发者之间的协作。
- en: Deep diving into a concrete example
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨一个具体的例子
- en: 'Early on, we wanted to build a data pipeline that extracted insights from Twitter
    by doing sentiment analysis of tweets containing specific hashtags and to deploy
    the results to a real-time dashboard. This application was a perfect starting
    point for us, because the data science analytics were not too complex, and the
    application covered many aspects of a real-life scenario:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们希望构建一个数据管道，通过对包含特定标签的推文进行情感分析，提取Twitter中的洞察，并将结果部署到实时仪表板上。这个应用程序对我们来说是一个完美的起点，因为数据科学分析并不复杂，且该应用程序涵盖了许多现实场景的方面：
- en: High volume, high throughput streaming data
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高流量、高吞吐量的流式数据
- en: Data enrichment with sentiment analysis NLP
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过情感分析NLP进行数据增强
- en: Basic data aggregation
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本数据聚合
- en: Data visualization
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Deployment into a real-time dashboard
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署到实时仪表板
- en: 'To try things out, the first implementation was a simple Python application
    that used the tweepy library (the official Twitter library for Python: [https://pypi.python.org/pypi/tweepy](https://pypi.python.org/pypi/tweepy))
    to connect to Twitter and get a stream of tweets and textblob (the simple Python
    library for basic NLP: [https://pypi.python.org/pypi/textblob](https://pypi.python.org/pypi/textblob))
    for sentiment analysis enrichment.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试这个方案，第一次实现是一个简单的Python应用程序，使用了tweepy库（Python的官方Twitter库：[https://pypi.python.org/pypi/tweepy](https://pypi.python.org/pypi/tweepy)）来连接Twitter，获取一系列的推文流，以及textblob库（用于基本NLP的简单Python库：[https://pypi.python.org/pypi/textblob](https://pypi.python.org/pypi/textblob)）进行情感分析的丰富。
- en: The results were then saved into a JSON file for analysis. This prototype was
    a great way to getting things started and experiment quickly, but after a few
    iterations we quickly realized that we needed to get serious and build an architecture
    that satisfied our enterprise requirements.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，结果被保存到JSON文件中进行分析。这个原型是启动项目并快速实验的一个绝佳方式，但经过几轮迭代后，我们迅速意识到，需要认真构建一个满足企业需求的架构。
- en: Data pipeline blueprint
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管道蓝图
- en: 'At a high level, data pipelines can be described using the following generic
    blueprint:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，数据管道可以通过以下通用蓝图来描述：
- en: '![Data pipeline blueprint](img/B09699_01_01.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![Data pipeline blueprint](img/B09699_01_01.jpg)'
- en: Data pipeline workflow
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道工作流
- en: The main objective of a data pipeline is to operationalize (that is, *provide
    direct business value*) the data science analytics outcome in a scalable, repeatable
    process, and with a high degree of automation. Examples of analytics could be
    a recommendation engine to entice consumers to buy more products, for example,
    the Amazon recommended list, or a dashboard showing **Key Performance Indicators**
    (**KPIs**) that can help a CEO make future decisions for the company.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道的主要目标是将数据科学分析结果转化为可操作的（即*提供直接的业务价值*）输出，并且在可扩展、可重复的过程中实现高度自动化。分析的例子可以是一个推荐引擎，用于激励消费者购买更多产品，例如，亚马逊的推荐列表，或者一个展示**关键绩效指标**（**KPI**）的仪表板，帮助CEO为公司的未来决策提供依据。
- en: 'There are multiple persons involved in the building of a data pipeline:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 构建数据管道的过程中涉及多个角色：
- en: '**Data engineers**: They are responsible for designing and operating information
    systems. In other words, data engineers are responsible for interfacing with data
    sources to acquire the data in its raw form and then massage it (some call this
    data wrangling) until it is ready to be analyzed. In the Amazon recommender system
    example, they would implement a streaming processing pipeline that captures and
    aggregates specific consumer transaction events from the e-commerce system of
    records and stores them into a data warehouse.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据工程师**：他们负责设计和运营信息系统。换句话说，数据工程师负责与数据源对接，获取原始数据，然后进行处理（有些人称之为数据清洗），直到数据准备好进行分析。在亚马逊推荐系统的例子中，他们会实现一个流处理管道，捕捉并聚合来自电商系统的特定消费者交易事件，并将这些数据存储到数据仓库中。'
- en: '**Data scientists**: They analyze the data and build the analytics that extract
    insight. In our Amazon recommender system example, they could use a Jupyter Notebook
    that connects to the data warehouse to load the dataset and build a recommendation
    engine using, for example, collaborative filtering algorithm ([https://en.wikipedia.org/wiki/Collaborative_filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据科学家**：他们分析数据并构建提取洞察的分析模型。在亚马逊推荐系统的例子中，他们可能会使用Jupyter Notebook连接到数据仓库，加载数据集，并使用例如协同过滤算法来构建推荐引擎（[https://en.wikipedia.org/wiki/Collaborative_filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)）。'
- en: '**Developers**: They are responsible for operationalizing the analytics into
    an application targeted at line of business users (business analysts, C-Suite,
    end users, and so on). Again, in the Amazon recommender system, the developer
    will present the list of recommended products after the user has completed a purchase
    or via a periodic email.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发人员**：他们负责将分析功能转化为面向业务用户（如业务分析师、高层管理人员、最终用户等）的应用程序。在亚马逊推荐系统中，开发人员将在用户完成购买后或通过定期电子邮件呈现推荐的产品列表。'
- en: '**Line of business users**: This encompasses all users that consume the output
    of data science analytics, for example, business analysts analyzing dashboards
    to monitor the health of a business or the end user using an application that
    provides a recommendation as to what to buy next.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务用户**：指的是所有使用数据科学分析输出的用户，例如，业务分析师分析仪表板以监控业务健康状况，或最终用户使用一个提供推荐的应用程序来决定下一步购买什么。'
- en: Note
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In real-life, it is not uncommon that the same person plays more than one of
    the roles described here; this may mean that one person has multiple, different
    needs when interacting with a data pipeline.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，通常同一个人可能会扮演多个角色；这意味着一个人在与数据管道互动时可能有多个不同的需求。
- en: 'As the preceding diagram suggests, building a data science pipeline is iterative
    in nature and adheres to a well-defined process:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的图示所示，构建数据科学管道是一个迭代过程，并遵循一个明确定义的流程：
- en: '**Acquire Data**: This step includes acquiring the data in its raw form from
    a variety of sources: structured (RDBMS, system of records, and so on) or unstructured
    (web pages, reports, and so on):'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**获取数据**：此步骤包括从各种来源获取原始数据：结构化（关系数据库、记录系统等）或非结构化（网页、报告等）：'
- en: '**Data cleansing**: Check for integrity, fill missing data, fix incorrect data,
    and data munging'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清洗**：检查数据完整性，填补缺失数据，修复不正确的数据，以及数据预处理'
- en: '**Data prep**: Enrich, detect/remove outliers, and apply business rules'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据准备**：丰富数据，检测/移除异常值，并应用业务规则'
- en: '**Analyze**: This step combines descriptive (understand the data) and prescriptive
    (build models) activities:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分析**：此步骤结合了描述性（理解数据）和规范性（构建模型）活动：'
- en: '**Explore**: Find statistical properties, for example, central tendency, standard
    deviation, distribution, and variable identification, such as univariate and bivariate
    analysis, the correlation between variables, and so on.'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：发现统计特性，例如集中趋势、标准差、分布，以及变量识别，如单变量和双变量分析、变量之间的相关性等。'
- en: '**Visualization**: This step is extremely important to properly analyze the
    data and form hypotheses. Visualization tools should provide a reasonable level
    of interactivity to facilitate understanding of the data.'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化**：这个步骤对于正确分析数据和形成假设至关重要。可视化工具应该提供合理的互动性，以便帮助理解数据。'
- en: '**Build model**: Apply inferential statistics to form hypotheses, such as selecting
    features for the models. This step usually requires expert domain knowledge and
    is subject to a lot of interpretation.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建模型**：应用推断统计学来形成假设，例如为模型选择特征。这个步骤通常需要专家领域知识，并且有很大的解释空间。'
- en: '**Deploy**: Operationalize the output of the analysis phase:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署**：将分析阶段的结果转化为可操作的实际应用：'
- en: '**Communicate**: Generate reports and dashboards that communicate the analytic
    output clearly for consumption by the line of business user (C-Suite, business
    analyst, and so on)'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**沟通**：生成报告和仪表板，清晰地传达分析结果，以供业务用户（如高层管理人员、业务分析师等）使用'
- en: '**Discover**: Set a business outcome objective that focuses on discovering
    new insights and business opportunities that can lead to a new source of revenue'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发现**：设定一个聚焦于发现新见解和商业机会的业务目标，这些见解和机会可能会带来新的收入来源'
- en: '**Implement**: Create applications for end-users'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实施**：为最终用户创建应用程序'
- en: '**Test**: This activity should really be included in every step, but here we''re talking
    about creating a feedback loop from field usage:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试**：这个活动应该贯穿于每一步，但在这里我们讨论的是创建一个来自实际应用的反馈循环：'
- en: Create metrics that measure the accuracy of the models
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建衡量模型准确性的指标
- en: Optimize the models, for example, get more data, find new features, and so on
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化模型，例如获取更多数据、寻找新特征等
- en: What kind of skills are required to become a data scientist?
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成为数据科学家需要哪些技能？
- en: In the industry, the reality is that data science is so new that companies do
    not yet have a well-defined career path for it. How do you get hired for a data
    scientist position? How many years of experience is required? What skills do you
    need to bring to the table? Math, statistics, machine learning, information technology,
    computer science, and what else?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在行业中，现实情况是，数据科学如此新兴，以至于公司还没有为其制定出明确的职业发展路径。那么，如何才能获得数据科学家的职位呢？需要多少年的经验？需要具备哪些技能？数学、统计学、机器学习、信息技术、计算机科学，还有什么？
- en: 'Well, the answer is probably a little bit of everything plus one more critical
    skill: domain-specific expertise.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其实，答案可能是“稍微懂一点儿各个领域”再加上一项关键技能：领域特定的专业知识。
- en: There is a debate going on around whether applying generic data science techniques
    to any dataset without an intimate understanding of its meaning, leads to the
    desired business outcome. Many companies are leaning toward making sure data scientists
    have substantial amount of domain expertise, the rationale being that without
    it you may unknowingly introduce bias at any steps, such as when filling the gaps
    in the data cleansing phase or during the feature selection process, and ultimately
    build models that may well fit a given dataset but still end up being worthless.
    Imagine a data scientist working with no chemistry background, studying unwanted
    molecule interactions for a pharmaceutical company developing new drugs. This
    is also probably why we're seeing a multiplication of statistics courses specialized
    in a particular domain, such as biostatistics for biology, or supply chain analytics
    for analyzing operation management related to supply chains, and so on.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有一种争论，关于是否在没有深入了解数据意义的情况下，将通用的数据科学技术应用于任何数据集，能否实现期望的商业结果。许多公司倾向于确保数据科学家具有足够的领域专业知识，其理由是，没有这一点，你可能会在任何步骤中无意间引入偏差，比如在数据清洗阶段填补空缺时，或在特征选择过程中，最终构建的模型可能非常适合某个数据集，但依然没有实际价值。想象一下，一个没有化学背景的数据科学家，正在为一家制药公司研究不需要的分子相互作用，帮助其开发新药。或许这也是为什么我们看到越来越多专门针对某一领域的统计学课程，例如生物学的生物统计学，或是供应链分析，专注于分析与供应链相关的运营管理等。
- en: 'To summarize, a data scientist should be in theory somewhat proficient in the
    following areas:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，一名数据科学家在理论上应该对以下领域有所熟练掌握：
- en: Data engineering / information retrieval
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程 / 信息检索
- en: Computer science
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机科学
- en: Math and statistics
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学与统计学
- en: Machine learning
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习
- en: Data visualization
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Business intelligence
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商业智能
- en: Domain-specific expertise
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领域专业知识
- en: Note
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意事项
- en: If you are thinking about acquiring these skills but don't have the time to attend
    traditional classes, I strongly recommend using online courses.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在考虑获取这些技能，但没有时间参加传统的课堂学习，我强烈推荐使用在线课程。
- en: 'I particularly recommend this course: [https://www.coursera.org/](https://www.coursera.org/):
    [https://www.coursera.org/learn/data-science-course](https://www.coursera.org/learn/data-science-course).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '我特别推荐这门课程：[https://www.coursera.org/](https://www.coursera.org/): [https://www.coursera.org/learn/data-science-course](https://www.coursera.org/learn/data-science-course)。'
- en: 'The classic Drew''s Conway Venn Diagram provides an excellent visualization
    of what is data science and why data scientists are a bit of a unicorn:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Drew的Conway维恩图提供了一个关于什么是数据科学以及为什么数据科学家有点像独角兽的优秀可视化图示：
- en: '![What kind of skills are required to become a data scientist?](img/B09699_01_02.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![成为数据科学家需要什么样的技能？](img/B09699_01_02.jpg)'
- en: Drew's Conway Data Science Venn Diagram
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Drew的Conway数据科学维恩图
- en: By now, I hope it becomes pretty clear that the perfect data scientist that
    fits the preceding description is more an exception than the norm and that, most
    often, the role involves multiple personas. Yes, that's right, the point I'm trying
    to make is that *data science is a team sport* and this idea will be a recurring
    theme throughout this book.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我希望你已经明白，符合上述描述的完美数据科学家更像是例外而非常态，而且通常这个角色涉及多个身份。没错，我想表达的观点是，*数据科学是一项团队运动*，这个观点将在本书中反复出现。
- en: IBM Watson DeepQA
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IBM Watson DeepQA
- en: One project that exemplifies the idea that data science is a team sport is the
    IBM DeepQA research project which originated as an IBM grand challenge to build an
    artificial intelligence system capable of answering natural language questions
    against predetermined domain knowledge. The **Question Answering** (**QA**) system
    should be good enough to be able to compete with human contestants at the *Jeopardy!*
    popular television game show.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个项目很好地说明了数据科学是一项团队运动的观点，那就是IBM DeepQA研究项目。这个项目最初是IBM发起的一个重大挑战，目的是构建一个能够回答自然语言问题的人工智能系统，依据预定的领域知识。**问答（QA）**系统应该足够好，能够与《危险边缘！》这档受欢迎的电视游戏节目中的人类选手竞争。
- en: 'As is widely known, this system dubbed IBM Watson went on to win the competition
    in 2011 against two of the most seasoned *Jeopardy!* champions: Ken Jennings and
    Brad Rutter. The following photo was taken from the actual game that aired on
    February 2011:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，这个名为IBM Watson的系统在2011年赢得了比赛，战胜了两位最有经验的*危险边缘！*冠军：Ken Jennings和Brad Rutter。以下照片来自2011年2月播出的实际比赛：
- en: '![IBM Watson DeepQA](img/B09699_01_03.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![IBM Watson DeepQA](img/B09699_01_03.jpg)'
- en: IBM Watson battling Ken Jennings and Brad Rutter at Jeopardy!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: IBM Watson与Ken Jennings和Brad Rutter在《危险边缘！》上的对决！
- en: 'Source: https://upload.wikimedia.org/wikipedia/e'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：https://upload.wikimedia.org/wikipedia/e
- en: It was during the time that I was interacting with the research team that built
    the IBM Watson QA computer system that I got to take a closer look at the DeepQA
    project architecture and realized first-hand how many data science fields were actually
    put to use.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在与构建IBM Watson QA计算机系统的研究团队互动时，我才得以更深入地了解DeepQA项目架构，并亲身体验到许多数据科学领域实际上是如何被应用的。
- en: 'The following diagram depicts a high-level architecture of the DeepQA data
    pipeline:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示描述了DeepQA数据管道的高层次架构：
- en: '![IBM Watson DeepQA](img/B09699_01_04.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![IBM Watson DeepQA](img/B09699_01_04.jpg)'
- en: Watson DeepQA architecture diagram
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Watson DeepQA架构图
- en: 'Source: https://researcher.watson.ibm.com/researcher/files/us-mi'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：https://researcher.watson.ibm.com/researcher/files/us-mi
- en: 'As the preceding diagram shows, the data pipeline for answering a question
    is composed of the following high-level steps:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，回答问题的数据管道由以下高层次步骤组成：
- en: '**Question & Topic Analysis (natural language processing)**: This step uses
    a deep parsing component which detects dependency and hierarchy between the words
    that compose the question. The goal is to have a deeper understanding of the question
    and extracts fundamental properties, such as the following:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**问题与主题分析（自然语言处理）**：此步骤使用一个深度解析组件，检测构成问题的单词之间的依赖关系和层次结构。目标是更深入地理解问题，并提取出基本属性，例如：'
- en: '**Focus**: What is the question about?'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重点**：这个问题到底是在问什么？'
- en: '**Lexical Answer Type** (**LAT**): What is the type of the expected answer,
    for example, a person, a place, and so on. This information is very important
    during the scoring of candidate answers as it provides an early filter for answers
    that don''t match the LAT.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇答案类型（**LAT**）**：期望答案的类型，例如一个人，一个地方等等。在候选答案的评分过程中，这些信息非常重要，因为它为与LAT不匹配的答案提供了早期过滤。'
- en: '**Named-entity resolution**: This resolves an entity into a standardized name,
    for example, "Big Apple" to "New York".'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体解析**：将一个实体解析为标准化的名称，例如，将“Big Apple”解析为“New York”。'
- en: '**Anaphora resolution**: This links pronouns to previous terms in the question,
    for example, in the sentence "On Sept. 1, 1715 Louis XIV died in this city, site
    of a fabulous palace **he** built," the pronoun "he" refers to Louis XIV.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指代解析**：将代词与问题中的先前术语连接起来，例如，在句子“1715年9月1日，路易十四在这座城市去世，**他**建造了这座宏伟的宫殿”中，代词“他”指代的是路易十四。'
- en: '**Relations detection**: This detects relations within the question, for example,
    "She divorced Joe DiMaggio in 1954" where the relation is "Joe DiMaggio Married
    X." These type of relations (Subject->Predicate->Object) can be used to query
    triple stores and yield high-quality candidate answers.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关系检测**：检测问题中的关系，例如，“她在1954年与乔·迪马吉奥离婚”，其中的关系是“乔·迪马吉奥娶了X”。这类关系（主语->谓语->宾语）可以用于查询三元组存储，并得到高质量的候选答案。'
- en: '**Question class**: This maps the question to one of the predefined types used
    in *Jeopardy!*, for example, factoid, multiple-choice, puzzle, and so on.'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题分类**：将问题映射到*Jeopardy!*中预定义的类型之一，例如，事实类问题、多项选择题、谜题等。'
- en: '**Primary search and Hypothesis Generation (information retrieval)**: This
    step relies heavily on the results of the question analysis step to assemble a
    set of queries adapted to the different answer sources available. Some example
    of answer sources include a variety of full-text search engines, such as Indri
    ([https://www.lemurproject.org/indri.php](https://www.lemurproject.org/indri.php))
    and Apache Lucene/Solr ([http://lucene.apache.org/solr](http://lucene.apache.org/solr)),
    document-oriented and title-oriented search (Wikipedia), triple stores, and so
    on. The search results are then used to generate candidate answers. For example,
    title-oriented results will be directly used as candidates while document searches
    will require more detailed analysis of the passages (again using NLP techniques)
    to extract possible candidate answers.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初步搜索与假设生成（信息检索）**：这一步骤主要依赖于问题分析步骤的结果，构建一组适应不同答案来源的查询。答案来源的一些例子包括各种全文搜索引擎，如Indri（[https://www.lemurproject.org/indri.php](https://www.lemurproject.org/indri.php)）和Apache
    Lucene/Solr（[http://lucene.apache.org/solr](http://lucene.apache.org/solr)），面向文档和标题的搜索（Wikipedia），三元组存储等等。搜索结果随后用于生成候选答案。例如，面向标题的结果将直接作为候选答案，而文档搜索则需要对段落进行更详细的分析（再次使用NLP技术）以提取可能的候选答案。'
- en: '**Hypothesis and Evidence scoring (NLP and information retrieval)**: For each
    candidate answer, another round of search is performed to find additional supporting
    evidence using different scoring techniques. This step also acts as a prescreening
    test where some of the candidate answers are eliminated, such as the answers that
    do not match the LAT computed from step 1. The output of this step is a set of
    machine learning features corresponding to the supporting evidence found. These
    features will be used as input to a set of machine learning models for scoring
    the candidate answers.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**假设与证据评分（NLP和信息检索）**：对于每个候选答案，进行另一次搜索以寻找更多支持证据，并使用不同的评分技术。此步骤还充当筛选测试，某些不符合步骤1中计算的LAT的候选答案将被淘汰。此步骤的输出是一组与找到的支持证据对应的机器学习特征。这些特征将作为输入，传递给一组机器学习模型，用于对候选答案进行评分。'
- en: '**Final merging and scoring (machine learning)**: During this final step, the system
    identifies variants of the same answer and merges them together. It also uses
    machine learning models to select the best answers ranked by their respective
    scores, using the features generated in step 3\. These machine learning models
    have been trained on a set of representative questions with the correct answers
    against a corpus of documents that has been pre-ingested.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最终合并与评分（机器学习）**：在最后一步中，系统识别相同答案的变体并将它们合并。它还使用机器学习模型，根据步骤3中生成的特征，选择由各自得分排名的最佳答案。这些机器学习模型已在一组代表性问题上进行训练，使用正确答案和预先摄取的文档语料库进行对比。'
- en: As we continue the discussion on how data science and AI are changing the field of
    computer science, I thought it was important to look at the state of the art.
    IBM Watson is one of these flagship projects that has paved the way to more advances
    we've seen since it beats Ken Jennings and Brad Rutter at the game of *Jeopardy!*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讨论数据科学和人工智能如何改变计算机科学领域时，我认为有必要看看最先进的技术。IBM Watson是这些旗舰项目之一，它为我们自它击败Ken Jennings和Brad
    Rutter参加*Jeopardy!*（危险游戏）以来的进步铺平了道路。
- en: Back to our sentiment analysis of Twitter hashtags project
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回到我们的Twitter话题情感分析项目
- en: 'The quick data pipeline prototype we built gave us a good understanding of
    the data, but then we needed to design a more robust architecture and make our application
    enterprise ready. Our primary goal was still to gain experience in building data
    analytics, and not spend too much time on the data engineering part. This is why
    we tried to leverage open source tools and frameworks as much as possible:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的快速数据管道原型使我们对数据有了更好的理解，但接下来我们需要设计一个更强大的架构，并使我们的应用程序准备好进入企业级。我们的主要目标仍然是积累构建数据分析的经验，而不是在数据工程部分花费太多时间。这就是为什么我们尽可能利用开源工具和框架的原因：
- en: '**Apache Kafka** ([https://kafka.apache.org](https://kafka.apache.org)): This
    is a scalable streaming platform for processing the high volume of tweets in a
    reliable and fault-tolerant way.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka** ([https://kafka.apache.org](https://kafka.apache.org)): 这是一个可扩展的流处理平台，用于以可靠和容错的方式处理大量的推文。'
- en: '**Apache Spark** ([https://spark.apache.org](https://spark.apache.org)): This
    is an in-memory cluster-computing framework. Spark provides a programming interface
    that abstracts a complexity of parallel computing.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Spark** ([https://spark.apache.org](https://spark.apache.org)): 这是一个内存计算集群框架。Spark提供了一个编程接口，抽象了并行计算的复杂性。'
- en: '**Jupyter Notebooks** ([http://jupyter.org](http://jupyter.org)): These interactive
    web-based documents (Notebooks) let users remotely connect to a computing environment
    (Kernel) to create advanced data analytics. Jupyter Kernels support a variety
    of programming languages (Python, R, Java/Scala, and so on) as well as multiple
    computing frameworks (Apache Spark, Hadoop, and so on).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter Notebooks** ([http://jupyter.org](http://jupyter.org)): 这些基于Web的交互式文档（Notebooks）允许用户远程连接到计算环境（内核），以创建先进的数据分析。Jupyter内核支持多种编程语言（Python、R、Java/Scala等）以及多个计算框架（Apache
    Spark、Hadoop等）。'
- en: For the sentiment analysis part, we decided to replace the code we wrote using
    the textblob Python library with the Watson Tone Analyzer service ([https://www.ibm.com/watson/services/tone-analyzer](https://www.ibm.com/watson/services/tone-analyzer)),
    which is a cloud-based rest service that provides advanced sentiment analysis
    including detection of emotional, language, and social tone. Even though the Tone
    Analyzer is not open source, a free version that can be used for development and
    trial is available on IBM Cloud ([https://www.ibm.com/cloud](https://www.ibm.com/cloud)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于情感分析部分，我们决定用Watson Tone Analyzer服务 ([https://www.ibm.com/watson/services/tone-analyzer](https://www.ibm.com/watson/services/tone-analyzer))
    替换我们使用textblob Python库编写的代码。Watson Tone Analyzer是一个基于云的REST服务，提供包括情感、语言和社交语气检测在内的高级情感分析。尽管Tone
    Analyzer不是开源的，但IBM云上提供了一个可供开发和试用的免费版本 ([https://www.ibm.com/cloud](https://www.ibm.com/cloud))。
- en: 'Our architecture now looks like this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的架构现在如下所示：
- en: '![Back to our sentiment analysis of Twitter hashtags project](img/B09699_01_05.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![回到我们的Twitter话题情感分析项目](img/B09699_01_05.jpg)'
- en: Twitter sentiment analysis data pipeline architecture
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter情感分析数据管道架构
- en: 'In the preceding diagram, we can break down the workflow in to the following
    steps:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图中，我们可以将工作流程分解为以下几个步骤：
- en: Produce a stream of tweets and publish them into a Kafka topic, which can be
    thought of as a channel that groups events together. In turn, a receiver component
    can subscribe to this topic/channel to consume these events.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 产生一系列推文并将其发布到Kafka主题中，Kafka主题可以看作是将事件聚集在一起的通道。接收组件可以订阅该主题/通道以消费这些事件。
- en: 'Enrich the tweets with emotional, language, and social tone scores: use Spark
    Streaming to subscribe to Kafka topics from component **1** and send the text
    to the Watson Tone Analyzer service. The resulting tone scores are added to the
    data for further downstream processing. This component was implemented using Scala
    and, for convenience, was run using a Jupyter Scala Notebook.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过情感、语言和社交语气得分来丰富推文：使用 Spark Streaming 订阅来自组件**1**的 Kafka 主题，并将文本发送到 Watson
    Tone Analyzer 服务。生成的语气得分将被添加到数据中，供下游进一步处理。此组件是使用 Scala 实现的，为了方便起见，它是通过 Jupyter
    Scala Notebook 运行的。
- en: 'Data analysis and exploration: For this part, we decided to go with a Python
    Notebook simply because Python offer a more attractive ecosystem of libraries,
    especially around data visualizations.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据分析和探索：对于这一部分，我们决定使用 Python Notebook，因为 Python 提供了更具吸引力的库生态系统，尤其是在数据可视化方面。
- en: Publish results back to Kafka.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果发布回 Kafka。
- en: Implement a real-time dashboard as a Node.js application.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个实时仪表板作为 Node.js 应用。
- en: 'With a team of three people, it took us about 8 weeks to get the dashboard
    working with real-time Twitter sentiment data. There are multiple reasons for
    this seemingly long time:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在三人团队的协作下，我们花了大约 8 周的时间，成功将仪表板与实时 Twitter 情感数据对接。这个看似较长的时间有多个原因：
- en: Some of the frameworks and services, such as Kafka and Spark Streaming, were new
    to us and we had to learn how to use their APIs.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些框架和服务，比如 Kafka 和 Spark Streaming，对我们来说是全新的，我们必须学习如何使用它们的 API。
- en: The dashboard frontend was built as a standalone Node.js application using the
    Mozaïk framework ([https://github.com/plouc/mozaik](https://github.com/plouc/mozaik)),
    which made it easy to build powerful live dashboards. However, we found a few
    limitations with the code, which forced us to dive into the implementation and
    write patches, hence adding delays to the overall schedule.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仪表板前端是作为一个独立的 Node.js 应用构建的，使用了 Mozaïk 框架（[https://github.com/plouc/mozaik](https://github.com/plouc/mozaik)），它使得构建强大的实时仪表板变得简单。然而，我们发现代码中有一些局限性，这迫使我们深入实现，编写补丁，因此对整体进度造成了一定延迟。
- en: 'The results are shown in the following screenshot:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 结果展示在以下截图中：
- en: '![Back to our sentiment analysis of Twitter hashtags project](img/B09699_01_06.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![返回我们的 Twitter 标签情感分析项目](img/B09699_01_06.jpg)'
- en: Twitter sentiment analysis real-ime dashboard
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 情感分析实时仪表板
- en: Lessons learned from building our first enterprise-ready data pipeline
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从构建我们第一个企业级数据管道中学到的经验教训
- en: Leveraging open source frameworks, libraries, and tools definitely helped us
    be more efficient in implementing our data pipeline. For example, Kafka and Spark
    were pretty straightforward to deploy and easy to use, and when we were stuck,
    we could always rely on the developer community for help by using, for example,
    question and answer sites, such as [https://stackoverflow.com](https://stackoverflow.com).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 利用开源框架、库和工具，的确帮助我们在实现数据管道时提高了效率。例如，Kafka 和 Spark 部署相对简单，使用也非常方便，而且当我们遇到瓶颈时，我们可以随时依赖开发者社区的帮助，例如通过使用像[https://stackoverflow.com](https://stackoverflow.com)这样的问题解答网站。
- en: Using a cloud-based managed service for the sentiment analysis step, such as
    the IBM Watson Tone Analyzer ([https://www.ibm.com/watson/services/tone-analyzer](https://www.ibm.com/watson/services/tone-analyzer))
    was another positive. It allowed us to abstract out the complexity of training
    and deploying a model, making the whole step more reliable and certainly more
    accurate than if we had implemented it ourselves.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于云的托管服务进行情感分析步骤，比如 IBM Watson Tone Analyzer（[https://www.ibm.com/watson/services/tone-analyzer](https://www.ibm.com/watson/services/tone-analyzer)），也是一个积极的做法。它让我们可以抽象出训练和部署模型的复杂性，使得整个步骤更可靠，显然比我们自己实现要准确得多。
- en: It was also super easy to integrate as we only needed to make a REST request
    (also known as an HTTP request, see [https://en.wikipedia.org/wiki/Representational_state_transfer](https://en.wikipedia.org/wiki/Representational_state_transfer)
    for more information on REST architecture) to get our answers. Most of the modern
    web services now conform to the REST architecture, however, we still need to know
    the specification for each of the APIs, which can take a long time to get right.
    This step is usually made simpler by using an SDK library, which is often provided
    for free and in most popular languages, such as Python, R, Java, and Node.js.
    SDK libraries provide higher level programmatic access to the service by abstracting
    out the code that generates the REST requests. The SDK would typically provide
    a class to represent the service, where each method would encapsulate a REST API
    while taking care of user authentication and other headers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 它也非常容易集成，因为我们只需要发送一个REST请求（也称为HTTP请求，关于REST架构的更多信息，请参考[https://en.wikipedia.org/wiki/Representational_state_transfer](https://en.wikipedia.org/wiki/Representational_state_transfer)）就能获得我们的答案。现在，大多数现代Web服务都遵循REST架构，然而，我们仍然需要了解每个API的规范，这通常需要很长时间才能正确实现。通过使用SDK库，这一步通常会变得更简单，SDK库通常是免费的，并且支持大多数流行的编程语言，如Python、R、Java和Node.js。SDK库通过抽象出生成REST请求的代码，提供了更高级的程序化访问服务。SDK通常会提供一个类来表示服务，每个方法封装一个REST
    API，并处理用户认证和其他请求头。
- en: On the tooling side, we were very impressed with Jupyter Notebooks, which provided
    excellent features, such as collaboration and full interactivity (we'll cover
    Notebooks in more detail later on).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在工具方面，我们对Jupyter Notebooks印象深刻，它提供了许多出色的功能，例如协作和完全交互性（我们稍后会更详细地介绍Notebooks）。
- en: 'Not everything was smooth though, as we struggled in a few key areas:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，并非一切都很顺利，我们在一些关键领域遇到了困难：
- en: Which programming language to choose for some of the key tasks, such as data
    enrichment and data analysis. We ended up using Scala and Python, even though
    there was little experience on the team, mostly because they are very popular
    among data scientists and also because we wanted to learn them.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择编程语言方面，对于一些关键任务，如数据丰富和数据分析，我们最终选择了Scala和Python，尽管团队经验不多，主要是因为它们在数据科学家中非常受欢迎，而且我们也希望学习它们。
- en: Creating visualizations for data exploration was taking too much time. Writing
    a simple chart with a visualization library, such as Matplotlib or Bokeh required
    writing too much code. This, in turn, slowed down our need for fast experimentation.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据探索的可视化图表耗费了太多时间。使用可视化库（如Matplotlib或Bokeh）编写一个简单的图表需要写大量的代码。这反过来又拖慢了我们对快速实验的需求。
- en: Operationalizing the analytics into a real-time dashboard was way too hard to
    be scalable. As mentioned before, we needed to write a full-fledged standalone
    Node.js application that consumes data from Kafka and needed to be deployed as
    a cloud-foundry application ([https://www.cloudfoundry.org](https://www.cloudfoundry.org))
    on the IBM Cloud. Understandably, this task required quite a long time to complete
    the first time, but we also found that it was difficult to update as well. Changes
    in the analytics that write data to Kafka needed to be synchronized with the changes
    on the dashboard application as well.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分析功能操作化为实时仪表板的过程非常困难，难以扩展。如前所述，我们需要编写一个完整的独立Node.js应用程序，该应用程序从Kafka消费数据，并且需要作为云平台应用程序（[https://www.cloudfoundry.org](https://www.cloudfoundry.org)）部署在IBM
    Cloud上。可以理解的是，这个任务第一次完成时需要相当长的时间，但我们还发现，更新它也很困难。写入Kafka的分析功能需要与仪表板应用程序上的更改同步。
- en: Data science strategy
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学战略
- en: 'If data science is to continue to grow and graduate into a core business activity,
    companies must find a way to scale it across all layers of the organization and
    overcome all the difficult challenges we discussed earlier. To get there, we identified
    three important pillars that architects planning a data science strategy should
    focus on, namely, data, services, and tools:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据科学要继续增长并成为核心业务活动，公司必须找到一种方法，在整个组织的所有层面上扩展它，并克服我们之前讨论的所有困难挑战。为了实现这一目标，我们确定了三大重要支柱，架构师在规划数据科学战略时应重点关注这三点：数据、服务和工具：
- en: '![Data science strategy](img/B09699_01_07.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学战略](img/B09699_01_07.jpg)'
- en: Three pillars of dat science at scale
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模数据科学的三大支柱
- en: '**Data is your most valuable resource**: You need a proper data strategy to
    make sure data scientists have easy access to the curated contents they need.
    Properly classifying the data, set appropriate governance policies, and make the
    metadata searchable will reduce the time data scientists spend acquiring the data
    and then asking for permission to use it. This will not only increase their productivity,
    it will also improve their job satisfaction as they will spend more time working
    on doing actual data science.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据是你最宝贵的资源**：你需要一个合适的数据策略，确保数据科学家可以轻松访问所需的精心策划的内容。通过对数据进行适当的分类、设定合适的治理政策，并使元数据可搜索，将减少数据科学家获取数据并请求使用权限的时间。这不仅能提高他们的生产力，还能提升他们的工作满意度，因为他们会有更多的时间从事实际的数据科学工作。'
- en: '*Setting a data strategy that enables data scientists to easily access high-quality
    data that''s relevant to them increases productivity and morale and ultimately
    leads to a higher rate of successful outcomes.*'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*制定一个数据策略，使数据科学家能够轻松访问与其相关的高质量数据，能够提高生产力和士气，并最终带来更高的成功率。*'
- en: '**Services**: Every architect planning for data science should be thinking
    about a **service-oriented architecture** (**SOA**). Contrary to traditional monolithic
    applications where all the features are bundled together into a single deployment,
    a service-oriented system breaks down functionalities into services which are
    designed to do a few things but to do it very well, with high performance and
    scalability. These systems are then deployed and maintained independently from
    each other giving scalability and reliability to the whole application infrastructure.
    For example, you could have a service that runs algorithms to create a deep learning
    model, another one would persist the models and let applications run it to make
    predictions on customer data, and so on.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务**：每个规划数据科学的架构师都应该考虑采用**面向服务架构**（**SOA**）。与传统的单体应用程序不同，后者将所有功能捆绑在一起进行单一部署，面向服务的系统将功能拆解为若干服务，每个服务只负责做少数几件事，但做得非常出色，且具备高性能和可扩展性。这些系统可以独立部署和维护，相互之间没有依赖，从而为整个应用基础设施提供可扩展性和可靠性。例如，你可以有一个运行算法创建深度学习模型的服务，另一个则负责持久化模型并允许应用程序运行它来对客户数据进行预测，等等。'
- en: 'The advantages are obvious: high reusability, easier maintenance, reduced time
    to market, scalability, and much more. In addition, this approach would fit nicely
    into a cloud strategy giving you a growth path as the size of your workload increases
    beyond existing capacities. You also want to prioritize open source technologies
    and standardize on open protocols as much as possible.'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优势显而易见：高度的可重用性、更容易的维护、更短的上市时间、可扩展性等等。此外，这种方法还非常适合融入云策略，当你的工作负载超出现有容量时，它能够提供增长路径。你还需要优先考虑开源技术，并尽可能标准化开放协议。
- en: '*Breaking processes into smaller functions infuses scalability, reliability,
    and repeatability into the system.*'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*将流程拆分为更小的功能模块，能够为系统注入可扩展性、可靠性和重复性。*'
- en: '**Tools do matter!** Without the proper tools, some tasks become extremely
    difficult to complete (at least that''s the rationale I use to explain why I fail
    at fixing stuff around the house). However, you also want to keep the tools simple,
    standardized, and reasonably integrated so they can be used by less skilled users
    (even if I was given the right tool, I''m not sure I would have been able to complete
    the house fixing task unless it''s simple enough to use). Once you decrease the
    learning curve to use these tools, non-data scientist users will feel more comfortable
    using them.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具确实很重要！** 没有合适的工具，一些任务将变得极其难以完成（至少这是我用来解释自己为什么修不好家里东西的理由）。然而，你也需要保持工具的简单性、标准化，并合理地集成它们，以便即便是技术水平较低的用户也能使用（即使我得到了正确的工具，我也不确定自己能否完成家务修理任务，除非它足够简单）。一旦你降低了这些工具的学习曲线，非数据科学家的用户将会感到更加舒适使用它们。'
- en: '*Making the tools simpler to use contributes to breaking the silos and increases
    collaboration between data science, engineering, and business teams.*'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*简化工具的使用有助于打破信息孤岛，并增强数据科学、工程和业务团队之间的协作。*'
- en: Jupyter Notebooks at the center of our strategy
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jupyter Notebooks 是我们策略的核心
- en: 'In essence, Notebooks are web documents composed of editable cells that let
    you run commands interactively against a backend engine. As their name indicates,
    we can think of them as the digital version of a paper scratch pad used to write
    notes and results about experiments. The concept is very powerful and simple at
    the same time: a user enters code in the language of his/her choice (most implementations
    of Notebooks support multiple languages, such as Python, Scala, R, and many more),
    runs the cell and gets the results interactively in an output area below the cell
    that becomes part of the document. Results could be of any type: text, HTML, and
    images, which is great for graphing data. It''s like working with a traditional
    **REPL** (short for, **Read-Eval-Print-Loop**) program on steroids since the Notebook
    can be connected to powerful compute engines (such as Apache Spark ([https://spark.apache.org](https://spark.apache.org))
    or Python Dask ([https://dask.pydata.org](https://dask.pydata.org)) clusters)
    allowing you to experiment with big data if needed.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，Notebooks 是由可编辑单元格组成的网页文档，允许你与后端引擎进行交互式命令操作。正如其名称所示，我们可以将其视为纸质便签的数字版，用于记录实验的笔记和结果。这个概念既强大又简单：用户可以在自己选择的编程语言中输入代码（大多数
    Notebooks 实现支持多种语言，如 Python、Scala、R 等），运行单元并在单元下方的输出区域互动式地查看结果，这些结果会成为文档的一部分。结果可以是任何类型：文本、HTML
    和图像，非常适合数据图表化。这就像是在传统的 **REPL**（**读取-评估-打印-循环**）程序上使用增强版，因为 Notebook 可以连接到强大的计算引擎（例如
    Apache Spark（[https://spark.apache.org](https://spark.apache.org)）或 Python Dask（[https://dask.pydata.org](https://dask.pydata.org)）集群），如果需要，可以进行大数据实验。
- en: Within Notebooks, any classes, functions, or variables created in a cell are
    visible in the cells below, enabling you to write complex analytics piece by piece,
    iteratively testing your hypotheses and fixing problems before moving on to the
    next phase. In addition, users can also write rich text using the popular Markdown
    language or mathematical expressions using LaTeX ([https://www.latex-project.org/](https://www.latex-project.org/)),
    to describe their experiments for others to read.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Notebooks 中，任何在单元格中创建的类、函数或变量都可以在下方的单元格中看到，允许你逐步编写复杂的分析，迭代地测试假设并在进入下一个阶段之前解决问题。此外，用户还可以使用流行的
    Markdown 语言编写富文本，或者使用 LaTeX（[https://www.latex-project.org/](https://www.latex-project.org/)）编写数学表达式，来描述他们的实验供其他人阅读。
- en: 'The following figure shows parts of a sample Jupyter Notebook with a Markdown
    cell explaining what the experiment is about, a code cell written in Python to
    create 3D plots, and the actual 3D charts results:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个示例 Jupyter Notebook 的部分内容，其中包含一个 Markdown 单元，解释实验的内容，一个用 Python 编写的代码单元用于创建
    3D 图表，以及实际的 3D 图表结果：
- en: '![Jupyter Notebooks at the center of our strategy](img/B09699_01_08.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![Jupyter Notebooks 在我们战略中的核心地位](img/B09699_01_08.jpg)'
- en: ample Jupyter Notebook
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 样本 Jupyter Notebook
- en: Why are Notebooks so popular?
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么 Notebooks 如此受欢迎？
- en: In the last few years, Notebooks have seen a meteoric growth in popularity as
    the tool of choice for data science-related activities. There are multiple reasons
    that can explain it, but I believe the main one is its versatility, making it
    an indispensable tool not just for data scientists but also for most of the personas
    involved in building data pipelines, including business analysts and developers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，Notebooks 作为数据科学相关活动的首选工具，迎来了爆炸式的增长。解释这一现象的原因有很多，但我认为主要原因是其多功能性，使得它不仅是数据科学家不可或缺的工具，也是参与构建数据管道的大多数角色，包括业务分析师和开发人员的必备工具。
- en: For data scientists, Notebooks are ideal for iterative experimentation because
    it enables them to quickly load, explore, and visualize data. Notebooks are also
    an excellent collaboration tool; they can be exported as JSON files and easily
    shared across the team, allowing experiments to be identically repeated and debugged
    when needed. In addition, because Notebooks are also web applications, they can
    be easily integrated into a multi-users cloud-based environment providing an even
    better collaborative experience.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据科学家来说，Notebooks 非常适合迭代实验，因为它使得他们能够快速加载、探索和可视化数据。Notebooks 也是一个出色的协作工具；它们可以导出为
    JSON 文件，并轻松地在团队之间共享，允许实验在需要时能够精确重复和调试。此外，由于 Notebooks 还是网页应用，它们可以轻松集成到基于云的多用户环境中，从而提供更好的协作体验。
- en: These environments can also provide on-demand access to large compute resources
    by connecting the Notebooks with clusters of machines using frameworks such as
    Apache Spark. Demand for these cloud-based Notebook servers is rapidly growing
    and as a result, we're seeing an increasing number of **SaaS** (short for, **Software
    as a Service**) solutions, both commercial with, for example, IBM Data Science
    Experience ([https://datascience.ibm.com](https://datascience.ibm.com)) or DataBricks
    ([https://databricks.com/try-databricks](https://databricks.com/try-databricks))
    and open source with JupyterHub ([https://jupyterhub.readthedocs.io/en/latest](https://jupyterhub.readthedocs.io/en/latest)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这些环境还可以通过使用像 Apache Spark 这样的框架，将笔记本与机器集群连接，从而按需提供大规模计算资源。这些基于云的笔记本服务器的需求正在迅速增长，因此，我们看到越来越多的**SaaS**（即**软件即服务**）解决方案，不仅包括商业产品，如
    IBM 数据科学体验（[https://datascience.ibm.com](https://datascience.ibm.com)）或 DataBricks（[https://databricks.com/try-databricks](https://databricks.com/try-databricks)），还包括开源解决方案，如
    JupyterHub（[https://jupyterhub.readthedocs.io/en/latest](https://jupyterhub.readthedocs.io/en/latest)）。
- en: For business analysts, Notebooks can be used as presentation tools that in most
    cases provide enough capabilities with its Markdown support to replace traditional
    PowerPoints. Charts and tables generated can be directly used to effectively communicate
    results of complex analytics; there's no need to copy and paste anymore, plus
    changes in the algorithms are automatically reflected in the final presentation.
    For example, some Notebook implementations, such as Jupyter, provide an automated
    conversion of the cell layout to the slideshow, making the whole experience even
    more seamless.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于商业分析师，笔记本可以作为演示工具，在大多数情况下，凭借其对 Markdown 的支持，提供足够的功能来取代传统的 PowerPoint。生成的图表和表格可以直接用于有效地传达复杂分析的结果；不再需要复制粘贴，并且算法的更改会自动反映在最终演示中。例如，某些笔记本实现（如
    Jupyter）提供了将单元格布局自动转换为幻灯片的功能，使得整个体验更加无缝。
- en: Note
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For reference, here are the steps to produce these slides in Jupyter Notebooks:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，以下是在 Jupyter 笔记本中制作这些幻灯片的步骤：
- en: Using the **View** | **Cell Toolbar** | **Slideshow**, first annotate each cell
    by choosing between **Slide**, **Sub-Slide**, **Fragment**, **Skip**, or **Notes**.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**视图** | **单元工具栏** | **幻灯片放映**，首先通过选择**幻灯片**、**子幻灯片**、**片段**、**跳过**或**笔记**来标注每个单元格。
- en: 'Use the `nbconvert jupyter` command to convert the Notebook into a Reveal.js-powered
    HTML slideshow:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `nbconvert jupyter` 命令将笔记本转换为 Reveal.js 驱动的 HTML 幻灯片：
- en: 'Optionally, you can fire up a web application server to access these slides
    online:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选地，你可以启动一个 web 应用服务器，以便在线访问这些幻灯片：
- en: '[PRE0]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For developers, the situation is much less clear-cut. On the one hand, developers
    love REPL programming, and Notebooks offer all the advantages of an interactive
    REPL with the added bonuses that it can be connected to a remote backend. By virtue
    of running in a browser, results can contain graphics and, since they can be saved,
    all or part of the Notebook can be reused in different scenarios. So, for a developer,
    provided that your language of choice is available, Notebooks offer a great way
    to try and test things out, such as fine-tuning an algorithm or integrating a
    new API. On the other hand, there is little Notebook adoption by developers for
    data science activities that can complement the work being done by data scientists,
    even though they are ultimately responsible for operationalizing the analytics
    into applications that address customer needs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开发人员来说，情况要复杂得多。一方面，开发人员喜欢 REPL 编程，而笔记本提供了交互式 REPL 的所有优势，此外，它还可以连接到远程后端。由于它是在浏览器中运行的，结果可以包含图形，而且由于它们可以被保存，整个笔记本或其部分内容可以在不同的场景中重复使用。因此，对于开发人员来说，只要你选择的语言可用，笔记本提供了一种很好的方式来尝试和测试不同的内容，比如微调算法或集成新的
    API。另一方面，尽管开发人员最终负责将分析结果转化为满足客户需求的应用程序，但他们对笔记本的采纳率很低，尤其是在数据科学活动中，这些活动本可以补充数据科学家的工作。
- en: To improve the software development life cycle and reduce time to value, they
    need to start using the same tools, programming languages, and frameworks as data
    scientists, including Python with its rich ecosystem of libraries and Notebooks,
    which have become such an important data science tool. Granted that developers
    have to meet the data scientist in the middle and get up to speed on the theory
    and concept behind data science. Based on my experience, I highly recommend using
    **MOOCs** (short for, **Massive Open Online Courses**) such as Coursera ([https://www.coursera.org](https://www.coursera.org))
    or EdX ([http://www.edx.org](http://www.edx.org)), which provide a wide variety
    of courses for every level.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善软件开发生命周期并缩短价值实现的时间，开发者需要开始使用与数据科学家相同的工具、编程语言和框架，包括Python及其丰富的库生态系统，以及已经成为数据科学重要工具的Notebooks。尽管开发者必须在数据科学的理论和概念上与数据科学家站在一起并迅速掌握，但根据我的经验，我强烈推荐使用**MOOCs**（大规模在线开放课程），如Coursera（[https://www.coursera.org](https://www.coursera.org)）或EdX（[http://www.edx.org](http://www.edx.org)），这些平台提供各种各样的课程，适合各个层次的学习者。
- en: However, having used Notebooks quite extensively, it is clear that, while being
    very powerful, they are primarily designed for data scientists, leaving developers
    with a steep learning curve. They also lack application development capabilities
    that are so critical for developers. As we've seen in the *Sentiment analysis
    of Twitter Hashtags* project, building an application or a dashboard based on
    the analytics created in a Notebook can be very difficult and require an architecture
    that can be difficult to implement and that has a heavy footprint on the infrastructure.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管Notebook功能强大，但它们主要是为数据科学家设计的，开发者需要面临陡峭的学习曲线。它们还缺乏开发者所需的应用开发功能，这对开发者至关重要。正如我们在*Twitter话题标签情感分析*项目中所看到的，基于Notebook中创建的分析构建应用程序或仪表板可能非常困难，需要一种架构，而这种架构不仅难以实现，而且对基础设施的负担较重。
- en: It is to address these gaps that I decided to create the PixieDust ([https://github.com/ibm-watson-data-lab/pixiedust](https://github.com/ibm-watson-data-lab/pixiedust))
    library and open source it. As we'll see in the next chapters, the main goal of
    PixieDust is to lower the *cost of entry* for new users (whether it be data scientists
    or developers) by providing simple APIs for loading and visualizing data. PixieDust
    also provides a developer framework with APIs for easily building applications,
    tools, and dashboards that can run directly in the Notebook and also be deployed
    as web applications.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 正是为了解决这些问题，我决定创建PixieDust（[https://github.com/ibm-watson-data-lab/pixiedust](https://github.com/ibm-watson-data-lab/pixiedust)）库并将其开源。如我们在接下来的章节中将看到的，PixieDust的主要目标是通过提供简单的API来加载和可视化数据，从而降低新用户（无论是数据科学家还是开发者）的*入门成本*。PixieDust还提供了一个开发者框架，带有API，方便开发者构建可以直接在Notebook中运行并部署为Web应用程序的应用、工具和仪表板。
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, I gave my perspective on data science as a developer, discussing
    the reasons why I think that data science along with AI and Cloud has the potential
    to define the next era of computing. I also discussed the many problems that must
    be addressed before it can fully realize its potential. While this book doesn't
    pretend to provide a magic recipe that solves all these problems, it does try
    to answer the difficult but critical question of democratizing data science and
    more specifically *bridging the gap between data scientists and developers*.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我从开发者的角度分享了我对数据科学的看法，讨论了为什么我认为数据科学与人工智能（AI）和云计算（Cloud）一起，具有定义下一代计算时代的潜力。我还讨论了在数据科学充分实现其潜力之前，必须解决的许多问题。尽管本书并不打算提供一个解决所有这些问题的“魔法食谱”，但它确实尝试回答一个既困难又关键的问题，那就是如何实现数据科学的民主化，特别是如何*弥合数据科学家和开发者之间的鸿沟*。
- en: In the next few chapters, we'll dive into the PixieDust open source library
    and learn how it can help Jupyter Notebooks users be more efficient when working
    with data. We'll also deep dive on the PixieApp application development framework
    that enables developers to leverage the analytics implemented in the Notebook
    to build application and dashboards.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将深入探讨PixieDust开源库，了解它如何帮助Jupyter Notebook用户在处理数据时提高效率。我们还将深入研究PixieApp应用开发框架，该框架使开发者能够利用Notebook中实现的分析来构建应用程序和仪表板。
- en: In the remaining chapters, we will deep dive into many examples that show how
    data scientists and developers can collaborate effectively to build end-to-end
    data pipelines, iterate on the analytics, and deploy them to end users at a fraction
    of the time. The sample applications will cover many industry use-cases, such
    as image recognition, social media, and financial data analysis which include
    data science use cases like descriptive analytics, machine learning, natural language
    processing, and streaming data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨许多示例，展示数据科学家和开发者如何有效地协作，构建端到端的数据管道，迭代分析，并将其在更短的时间内部署到最终用户。这些示例应用将涵盖多个行业用例，如图像识别、社交媒体和金融数据分析，其中包括数据科学用例，如描述性分析、机器学习、自然语言处理和*流数据*。
- en: We will not discuss deeply the theory behind all the algorithms covered in the
    sample applications (which is beyond the scope of this book and would take more
    than one book to cover), but we will instead emphasize how to leverage the open
    source ecosystem to rapidly complete the task at hand (model building, visualization,
    and so on) and operationalize the results into applications and dashboards.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入讨论示例应用中涵盖的所有算法背后的理论（这超出了本书的范围，且需要一本书来讲解），而是将重点强调如何利用开源生态系统快速完成手头的任务（如模型构建、可视化等），并将结果转化为应用程序和仪表板。
- en: Note
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The provided sample applications are written mostly in Python and come with
    complete source code. The code has been extensively tested and is ready to be
    re-used and customized in your own projects.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的示例应用主要使用Python编写，并附带完整的源代码。代码经过广泛测试，已经准备好可以在您的项目中重复使用和定制。
