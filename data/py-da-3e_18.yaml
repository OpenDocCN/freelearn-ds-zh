- en: Parallel Computing Using Dask
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Dask 进行并行计算
- en: Dask is one of the simplest ways to process your data in a parallel manner.
    The platform is for pandas lovers who struggle with large datasets. Dask offers
    scalability in a similar manner to Hadoop and Spark and the same flexibility that
    Airflow and Luigi provide. Dask can be used to work on pandas DataFrames and Numpy
    arrays that cannot fit into RAM. It splits these data structures and processes
    them in parallel while making minimal code changes. It utilizes your laptop power
    and has the ability to run locally. We can also deploy it on large distributed
    systems as we deploy Python applications. Dask can execute data in parallel and
    processes it in less time. It also scales the computation power of your workstation
    without migrating to a larger or distributed environment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 是处理数据并行方式的最简单方法之一。这个平台适合那些在大数据集上挣扎的 pandas 爱好者。Dask 提供类似 Hadoop 和 Spark
    的可扩展性，同时具备 Airflow 和 Luigi 提供的灵活性。Dask 可以用来处理无法放入 RAM 中的 pandas 数据框和 NumPy 数组。它将这些数据结构拆分并进行并行处理，同时对代码的修改最小化。它利用笔记本电脑的计算能力，并可以本地运行。我们也可以像部署
    Python 应用一样在大规模分布式系统上部署 Dask。Dask 可以并行执行数据，并减少处理时间。它还可以在不迁移到更大或分布式环境的情况下，扩展工作站的计算能力。
- en: 'The main objective of this chapter is to learn how to perform flexible parallel
    computation on large datasets using Dask. The platform provides three data types
    for parallel execution: Dask Arrays, Dask DataFrames, and Dask Bags. The Dask
    array is like a NumPy array, while Dask DataFrames are like pandas DataFrames.
    Both can execute data in parallel. A Dask Bag is a wrapper for Python objects
    so that they can perform operations simultaneously. Another concept we''ll cover
    in this chapter is Dask Delayed, which parallelizes code. Dask also offers data
    preprocessing and machine learning model development in parallel mode.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要目标是学习如何使用 Dask 在大规模数据集上进行灵活的并行计算。该平台提供三种并行执行的数据类型：Dask 数组、Dask 数据框和 Dask
    包。Dask 数组类似于 NumPy 数组，而 Dask 数据框则类似于 pandas 数据框。两者都可以并行执行数据。Dask 包是 Python 对象的封装器，使它们能够同时执行操作。本章还将介绍
    Dask Delayed，这是一个将代码并行化的概念。Dask 还提供了数据预处理和机器学习模型开发的并行模式。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Parallel computing using Dask
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Dask 进行并行计算
- en: Dask data types
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 数据类型
- en: Dask Delayed
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask Delayed
- en: Preprocessing data at scale
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模数据预处理
- en: Machine learning at scale
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模机器学习
- en: Let's get started!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Parallel computing using Dask
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Dask 进行并行计算
- en: 'Python is one of the most popular programming languages among data professionals.
    Python data science libraries such as Numpy, Pandas, Scipy, and Scikit-learn can
    sequentially perform data science tasks. However, with large datasets, these libraries
    will become very slow due to not being scalable beyond a single machine. This
    is where Dask comes into the picture. Dask helps data professionals handle datasets
    that are larger than the RAM size on a single machine. Dask utilizes the multiple
    cores of a processor or uses it as a distributed computed environment. Dask has
    the following qualities:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Python 是数据专业人员中最流行的编程语言之一。Python 的数据科学库，如 Numpy、Pandas、Scipy 和 Scikit-learn，可以顺序地执行数据科学任务。然而，对于大数据集，这些库在处理时会变得非常缓慢，因为它们无法在单台机器上进行扩展。这时，Dask
    就派上了用场。Dask 帮助数据专业人员处理超过单台机器 RAM 大小的数据集。Dask 利用处理器的多个核心，或者将其作为分布式计算环境使用。Dask 具有以下特点：
- en: It is familiar with existing Python libraries
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它熟悉现有的 Python 库
- en: It offers flexible task scheduling
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供灵活的任务调度
- en: It offers a single and distributed environment for parallel computation
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为并行计算提供了单机和分布式环境
- en: It performs fast operations with lower latency and overhead
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它执行快速操作，具有较低的延迟和开销
- en: It can scale up and scale down
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以实现横向扩展和缩减
- en: 'Dask offers similar concepts to pandas, NumPy, and Scikit-learn, which makes
    it easier to learn. It is an open source parallel computing Python library that
    runs on top of pandas, Numpy, and Scikit-learn across multiple cores of a CPU
    or multiple systems. For example, if a laptop has a quad-core processor, then
    Dask will use 4 cores for processing the data. If the data won''t fit in the RAM,
    it will be partitioned into chunks before processing. Dask scales up the pandas
    and NumPy capacity to deal with moderately large datasets. Let''s understand how
    Dask perform operations in parallel by looking at the following diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供了与 pandas、NumPy 和 Scikit-learn 相似的概念，这使得它更容易学习。它是一个开源的并行计算 Python 库，运行在
    pandas、NumPy 和 Scikit-learn 之上，可以跨多个 CPU 核心或多个系统运行。例如，如果一台笔记本电脑有四核处理器，Dask 就会使用
    4 个核心来处理数据。如果数据无法全部装入内存，它会在处理之前将数据分割成多个块。Dask 扩展了 pandas 和 NumPy 的能力，用来处理适中的大数据集。让我们通过以下图表了解
    Dask 如何并行执行操作：
- en: '![](img/69c5e3db-7f88-419a-9439-f46457b33a4c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69c5e3db-7f88-419a-9439-f46457b33a4c.png)'
- en: Dask creates a task graph to execute a program in parallel mode. In the task
    graph, nodes represent the task, and the edges between the nodes represent the
    dependency of one task over another.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 创建一个任务图来并行执行程序。在任务图中，节点表示任务，节点之间的边表示一个任务对另一个任务的依赖关系。
- en: 'Let''s install the Dask library on our local system. By default, Anaconda has
    Dask installed already, but if you want to reinstall or update Dask, you can use
    the following command:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在本地系统上安装 Dask 库。默认情况下，Anaconda 已经安装了 Dask，但如果你想重新安装或更新 Dask，可以使用以下命令：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can also install it using the `pip` command, as shown here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用`pip`命令来安装它，如下所示：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With that, we have learned how to install the `dask` library for parallel and
    fast execution. Now, let's look at the core data types of the Dask library.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经学会了如何安装 `dask` 库以实现并行和快速执行。现在，让我们来看看 Dask 库的核心数据类型。
- en: Dask data types
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask 数据类型
- en: In computer programming, data types are basic building blocks for writing any
    kind of functionality. They help us work with different types of variables. Data
    types are the kind of values that are stored in variables. They can be primary
    and secondary.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机编程中，数据类型是编写任何功能的基本构建块。它们帮助我们处理不同类型的变量。数据类型是存储在变量中的值的种类。数据类型可以是基本数据类型和次级数据类型。
- en: 'Primary data types are the basic data types such as int, float, and char, while
    secondary data types are developed using primary data types such as lists, arrays,
    strings, and DataFrames. Dask offers three data structures for parallel operations:
    DataFrames, Bags, and Arrays. These data structures split data into multiple partitions
    and distribute them to multiple nodes in the cluster. A Dask DataFrame is a combination
    of multiple small pandas DataFrames and it operates in a similar manner. Dask
    Arrays are like NumPy arrays and support all the operations of Numpy. Finally,
    Dask Bags are used to process large Python objects.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基本数据类型是像 int、float 和 char 这样的基本数据类型，而次级数据类型是通过基本数据类型开发的，如列表、数组、字符串和数据框。Dask
    提供了三种用于并行操作的数据结构：数据框、袋和数组。这些数据结构将数据分割成多个分区，并将它们分配到集群中的多个节点。Dask 数据框是多个小的 pandas
    数据框的组合，它的操作方式类似。Dask 数组类似于 NumPy 数组，并支持 NumPy 的所有操作。最后，Dask 袋用于处理大型 Python 对象。
- en: Now, it's time to explore these data types. We'll start with Dask Arrays.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到了探索这些数据类型的时候了。我们从 Dask 数组开始。
- en: Dask Arrays
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask 数组
- en: 'A Dask Array is an abstraction of the NumPy n-dimensional array, processed
    in parallel and partitioned into multiple sub-arrays. These small arrays can be
    on local or distributed remote machines. Dask Arrays can compute large-sized arraysby
    utilizing all the available cores in the system. They can be applied to statistics,
    optimization, bioinformatics, business domains, environmental science, and many
    more fields. They also support lots of NumPy operations, such as arithmetic and
    scalar operations, aggregation operations, matrices, and linear algebra operations.
    However, they do not support unknown shapes. Also, the `tolist` and `sort` operations
    are difficult to perform in parallel. Let''s understand how Dask Arrays decompose
    data into a NumPy array and execute them in parallel by taking a look at the following
    diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 数组是 NumPy n 维数组的抽象，通过并行处理并分割成多个子数组。这些小数组可以存储在本地或分布式远程机器上。Dask 数组通过利用系统中所有可用的核心来计算大型数组。它们可以应用于统计、优化、生物信息学、商业领域、环境科学等多个领域。它们还支持大量的
    NumPy 操作，如算术和标量运算、聚合操作、矩阵和线性代数运算。然而，它们不支持未知形状。此外，`tolist`和`sort`操作在并行执行时较为困难。让我们通过以下图示来了解
    Dask 数组如何将数据分解成 NumPy 数组并并行执行：
- en: '![](img/440d99ec-54de-42e2-9a13-9c42a62657c7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/440d99ec-54de-42e2-9a13-9c42a62657c7.png)'
- en: 'As we can see, there are multiple blocks of different shapes, all of which
    represent NumPy arrays. These arrays form a Dask Array and can be executed on
    multiple machines. Let''s create an array using Dask:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，有多个形状不同的块，这些块都表示 NumPy 数组。这些数组形成了一个 Dask 数组，并可以在多个机器上执行。让我们用 Dask 创建一个数组：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This results in the following output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the preceding example, we used the `compute()` function to get the final
    output. The `da.arange()` function will only create the computational graph, while
    the `compute()` function is used to execute that graph. We have generated 18 values
    with a chunk size of 4 using the `da.arange()` function. Let''s also check the
    chunks in each partition:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们使用了`compute()`函数来获取最终输出。`da.arange()`函数只会创建计算图，而`compute()`函数用于执行该图。我们使用`da.arange()`函数生成了18个值，块大小为4。我们也来检查一下每个分区中的块：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This results in the following output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding example, an array with 18 values was partitioned into five
    parts with a chunk size of 4, where these initial chunks have 4 values each and
    the last one has 2 values.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，一个包含18个值的数组被分成了五个部分，每个部分的大小为4，其中前四个部分各包含4个值，最后一个部分包含2个值。
- en: Dask DataFrames
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask DataFrame
- en: 'Dask DataFrames are abstractions of pandas DataFrames. They are processed in
    parallel and partitioned into multiple smaller pandas DataFrames, as shown in
    the following diagram:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame 是 pandas DataFrame 的抽象。它们是并行处理的，并分割成多个较小的 pandas DataFrame，如下图所示：
- en: '![](img/3d495ac7-de60-4729-a550-1411cd567081.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d495ac7-de60-4729-a550-1411cd567081.png)'
- en: 'These small DataFrames can be stored on local or distributed remote machines.
    Dask DataFrames can compute large-sized DataFrames by utilizing all the available
    cores in the system. They coordinate the DataFrames using indexing and support
    standard pandas operations such as `groupby`, `join`, and `time series`. Dask
    DataFrames perform operations such as element-wise, row-wise, `isin()`, and date
    faster compared to `set_index()` and `join()` on index operations. Now, let''s
    experiment with the performance or execution speed of Dask:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些小的 DataFrame 可以存储在本地或分布式远程机器上。Dask DataFrame 通过利用系统中所有可用的核心来计算大型 DataFrame。它们通过索引来协调
    DataFrame，并支持标准的 pandas 操作，如`groupby`、`join`和`时间序列`。与在索引操作中使用`set_index()`和`join()`相比，Dask
    DataFrame 在元素级、行级、`isin()`和日期操作上执行速度更快。现在，让我们来测试一下 Dask 的性能或执行速度：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This results in the following output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the preceding code, we tested the read time of a file using the pandas `read_csv()`
    function. Now, let''s test the read time for the Dask `read_csv()` function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 pandas 的`read_csv()`函数测试了文件的读取时间。现在，让我们测试 Dask 的`read_csv()`函数的读取时间：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In both examples, we can observe that the execution time for data reading is
    reduced when using the Dask `read_csv()` function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个例子中，我们可以看到，使用 Dask 的`read_csv()`函数时，数据读取的执行时间有所减少。
- en: DataFrame Indexing
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DataFrame 索引
- en: 'Dask DataFrames support two types of index: label-based and positional indexing.
    The main problem with Dask Indexing is that it does not maintain the partition''s
    information. This means it is difficult to perform row indexing; only column indexing
    is possible. `DataFrame.iloc` only supports integer-based indexing, while `DataFrame.loc`
    supports label-based indexing. `DataFrame.iloc` only selects columns.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame 支持两种类型的索引：基于标签的索引和基于位置的索引。Dask 索引的主要问题是它不维护分区信息。这意味着执行行索引很困难；只可能进行列索引。`DataFrame.iloc`
    仅支持基于整数的索引，而 `DataFrame.loc` 支持基于标签的索引。`DataFrame.iloc` 只能选择列。
- en: 'Let''s perform these indexing operations on a Dask DataFrame:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对 Dask DataFrame 执行这些索引操作：
- en: 'First, we must create a DataFrame and perform column indexing:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须创建一个 DataFrame 并执行列索引：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This results in the following output:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding example, we created a pandas DataFrame (with `p`, `q`, and
    `r` indexes and `P` and `Q` columns) and converted it into a Dask DataFrame.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们创建了一个 pandas DataFrame（具有 `p`、`q` 和 `r` 索引，以及 `P` 和 `Q` 列），并将其转换为
    Dask DataFrame。
- en: 'The column selection process in Dask is similar to what we do in pandas. Let''s
    select a single column in our Dask DataFrame:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Dask 中，列选择过程与我们在 pandas 中的操作类似。让我们在 Dask DataFrame 中选择单列：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding code, we selected a single column by passing the name of the
    column. For multiple column selection, we need to pass a list of columns.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们通过传递列名选择了单个列。要选择多个列，我们需要传递列名的列表。
- en: 'Let''s select multiple columns in our Dask DataFrame:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在 Dask DataFrame 中选择多列：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following output:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we have selected two columns from the list of columns available.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从可用的列列表中选择了两列。
- en: 'Now, let''s create a DataFrame with an integer index:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个具有整数索引的 DataFrame：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding code, we created a pandas DataFrame and converted it into a
    Dask DataFrame using the `from_pandas()` function.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们创建了一个 pandas DataFrame，并使用 `from_pandas()` 函数将其转换为 Dask DataFrame。
- en: 'Let''s select the required column using a positional integer index:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用基于位置的整数索引选择所需的列：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This results in the following output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding code, we swapped the column's location using `iloc` while using
    a positional integer index.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们使用 `iloc` 交换了列的位置，同时使用了基于位置的整数索引。
- en: 'If we try to select all the rows, we will get a `NotImplementedError`, as shown
    here:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们尝试选择所有行，我们会遇到 `NotImplementedError` 错误，如下所示：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This results in the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE21]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code block, we can see that the `DataFrame.iloc` only supports
    selecting columns.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码块中，我们可以看到 `DataFrame.iloc` 仅支持选择列。
- en: Filter data
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤数据
- en: 'We can filter the data from a Dask DataFrame similar to how we would do this
    for a pandas DataFrame. Let''s take a look at the following example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像对待 pandas DataFrame 一样过滤 Dask DataFrame 中的数据。让我们看一个例子：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This results in the following output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/5e709ff8-2afe-406e-8c8f-78b7900d9288.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e709ff8-2afe-406e-8c8f-78b7900d9288.png)'
- en: 'In the preceding code, we read the human resource CSV file using the `read_csv()`
    function into the Dask DataFrame. This output is only showing some of the columns.
    However, when you run the notebook for yourself, you will be able to see all the
    available columns. Let''s filter the low-salary employees in the dataset:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们使用 `read_csv()` 函数将人力资源 CSV 文件读入 Dask DataFrame。此输出只显示了一些列。然而，当你自己运行笔记本时，你将能够看到所有可用的列。让我们过滤数据集中低薪的员工：
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This results in the following output:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/07172a2b-fe5d-45df-b710-a62a3d81b469.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07172a2b-fe5d-45df-b710-a62a3d81b469.png)'
- en: In the preceding code, we filtered the low-salary employees through the condition
    into the brackets.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们通过条件将低薪员工筛选到括号内。
- en: Groupby
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Groupby
- en: 'The `groupby` operation is used to aggregate similar items. First, it splits
    the data based on the values, finds an aggregate of similar values, and combines
    the aggregated results. This can be seen in the following code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupby` 操作用于聚合相似的项目。首先，它根据值拆分数据，找到相似值的聚合，然后将聚合结果合并。这可以在以下代码中看到：'
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This results in the following output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/b612b1d2-b4e8-4d83-8598-8da093bb00ae.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b612b1d2-b4e8-4d83-8598-8da093bb00ae.png)'
- en: In the preceding example, we grouped the data based on the left column (it shows
    an employee who stayed or left the company) and aggregated it by the mean value.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们根据左侧列（显示了员工是留下还是离开公司）对数据进行了分组，并按均值进行了聚合。
- en: Converting a pandas DataFrame into a Dask DataFrame
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将pandas DataFrame转换为Dask DataFrame
- en: 'Dask DataFrames are implemented based on pandas DataFrames. For data analysts,
    it is necessary to learn how to convert a Dask DataFrame into a pandas DataFrame.
    Take a look at the following code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames是基于pandas DataFrames实现的。对于数据分析师来说，学习如何将Dask DataFrame转换为pandas
    DataFrame是必要的。请查看以下代码：
- en: '[PRE25]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This results in the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE26]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, we have used the `from_pandas()` method to convert a pandas DataFrame
    into a Dask DataFrame.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`from_pandas()`方法将一个pandas DataFrame转换为Dask DataFrame。
- en: Converting a Dask DataFrame into a pandas DataFrame
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将Dask DataFrame转换为pandas DataFrame
- en: 'In the previous subsection, we converted a pandas DataFrame into a Dask DataFrame.
    Similarly, we can convert a Dask DataFrame into a pandas DataFrame using the `compute()`
    method, as shown here:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一小节中，我们将一个pandas DataFrame转换为Dask DataFrame。同样，我们也可以使用`compute()`方法将Dask DataFrame转换为pandas
    DataFrame，如下所示：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This results in the following output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE28]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s learn about another important topic: Dask Bags.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习另一个重要的主题：Dask Bags。
- en: Dask Bags
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask Bags
- en: A Dask Bag is an abstraction over generic Python objects. It performs `map`,
    `filter`, `fold`, and `groupby` operations in the parallel interface of smaller
    Python objects using a Python iterator. This execution is similar to PyToolz or
    the PySpark RDD. Dask Bags are more suitable for unstructured and semi-structured
    datasets such as text, JSON, and log files. They perform multiprocessing for computation
    for faster processing but will not perform well with inter-worker communication.
    Bags are immutable types of structures that cannot be changed and are slower compared
    to Dask Arrays and DataFrames. Bags also perform slowly on the `groupby` operation,
    so it is recommended that you use `foldby` instead of `groupby`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Dask Bag是对通用Python对象的抽象。它通过Python迭代器在较小的Python对象的并行接口中执行`map`、`filter`、`fold`和`groupby`操作。其执行方式类似于PyToolz或PySpark
    RDD。Dask Bags更适合用于非结构化和半结构化数据集，如文本、JSON和日志文件。它们通过多进程计算来加速处理，但在工作节点间通信方面表现不佳。Bags是不可变的数据结构，不能更改，并且与Dask
    Arrays和DataFrames相比速度较慢。Bags在执行`groupby`操作时也较慢，因此推荐使用`foldby`代替`groupby`。
- en: Now, let's create various Dask Bag objects and perform operations on them.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建各种Dask Bag对象并对它们执行操作。
- en: Creating a Dask Bag using Python iterable items
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python可迭代项创建Dask Bag
- en: 'Let''s create some Dask Bag objects using Python iterable items:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Python可迭代项创建一些Dask Bag对象：
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This results in the following output:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE30]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In the preceding code, we created a bag of list items using the `from_sequence()`
    method. The `from_Sequence()` method takes a list and places it into `npartitions`
    (a number of partitions). Let''s filter odd numbers from the list:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`from_sequence()`方法创建了一个列表项的bag。`from_Sequence()`方法接受一个列表并将其放入`npartitions`（分区数）。让我们从列表中过滤出奇数：
- en: '[PRE31]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This results in the following output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE32]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the preceding code, we filtered the odd numbers from the bag of lists using
    the `filter()` method. Now, let''s square each item of the bag using the `map`
    function:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`filter()`方法从列表的bag中过滤出了奇数。现在，让我们使用`map`函数将bag中的每个项平方：
- en: '[PRE33]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This results in the following output:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE34]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the preceding code, we used the `map()` function to map the bag items. We
    mapped these items to their square value.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`map()`函数对bag中的项进行了映射。我们将这些项映射到它们的平方值。
- en: Creating a Dask Bag using a text file
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用文本文件创建Dask Bag
- en: 'We can create a Dask Bag using a text file by using the `read_text()` method,
    as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`read_text()`方法，通过文本文件创建一个Dask Bag，代码如下：
- en: '[PRE35]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This results in the following output:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE36]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding code, we read a text file into a `dask bag` object by using
    the `read_text()` method. This allowed us to show the two initial items in the
    Dask Bag.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`read_text()`方法将文本文件读取到一个`dask bag`对象中。这让我们能够展示Dask Bag中的前两个项目。
- en: Storing a Dask Bag in a text file
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将Dask Bag存储到文本文件中
- en: 'Let''s store a Dask Bag in a text file:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将Dask Bag存储到文本文件中：
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This results in the following output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE38]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the preceding code, `to_textfiles()` converted the `bag` object into a text
    file.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`to_textfiles()`将`bag`对象转换为一个文本文件。
- en: Storing a Dask Bag in a DataFrame
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 Dask Bag 存储到 DataFrame 中
- en: 'Let''s store a Dask Bag in a DataFrame:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 Dask Bag 存储到 DataFrame 中：
- en: '[PRE39]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This results in the following output:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/a9775f85-da6d-40b1-8966-54dda0e973a0.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9775f85-da6d-40b1-8966-54dda0e973a0.png)'
- en: In the preceding example, we created a Dask Bag of dictionary items and converted
    it into a Dask DataFrame using the `to_dataframe()` method. In the next section,
    we'll look at Dask Delayed.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们创建了一个包含字典项的 Dask Bag，并使用 `to_dataframe()` 方法将其转换为 Dask DataFrame。在下一节中，我们将讨论
    Dask 延迟。
- en: Dask Delayed
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask 延迟
- en: Dask Delayed is an approach we can use to parallelize code. It can delay the
    dependent function calls in task graphs and provides complete user control over
    parallel processes while improving performance. Its lazy computation helps us
    control the execution of functions. However, this differs from the execution timings
    of functions for parallel execution.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 延迟是一种用于并行化代码的方法。它可以延迟任务图中的依赖函数调用，并在提高性能的同时为用户提供对并行进程的完全控制。它的惰性计算帮助我们控制函数的执行。然而，这与并行执行中函数的执行时机有所不同。
- en: 'Let''s understand the concept of Dask Delayed by looking at an example:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来理解 Dask 延迟的概念：
- en: '[PRE40]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This results in the following output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE41]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In the preceding example, two methods, `cube` and `average`, were annotated
    with `@dask.delayed`. A list of three numbers was created and a cube containing
    every value was computed. After computing the cube of list values, we calculated
    the average of all the values. All these operations are lazy in nature and are
    computed later when the output is expected from the programmer and the flow of
    execution is stored in a computational graph. We executed this using the `compute()`
    method. Here, all the cube operations will execute in a parallel fashion.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，`cube` 和 `average` 两个方法都被 `@dask.delayed` 注解。创建了一个包含三个数字的列表，并计算了每个值的立方。在计算完列表值的立方后，我们计算了所有值的平均值。所有这些操作都是惰性计算，只有当程序员需要输出时，计算才会执行，执行流会被存储在计算图中。我们通过
    `compute()` 方法来执行这一操作。在这里，所有的立方操作都将并行执行。
- en: Now, we will visualize the computational graph. However, before we can do this,
    we need to install the Graphviz editor.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将可视化计算图。然而，在此之前，我们需要安装 Graphviz 编辑器。
- en: 'On Windows, we can install Graphviz using `pip`. We must also set the path
    in an environment variable:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 上，我们可以使用 `pip` 安装 Graphviz。我们还必须在环境变量中设置路径：
- en: '[PRE42]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'On Mac, we can install it using `brew`, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Mac 上，我们可以使用 `brew` 安装，如下所示：
- en: '[PRE43]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'On Ubuntu, we need to install it on a Terminal using the `sudo apt-get` command:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ubuntu 上，我们需要通过 `sudo apt-get` 命令在终端中安装：
- en: '[PRE44]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let''s visualize the computational graph:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们可视化计算图：
- en: '[PRE45]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This results in the following output:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/8ca62af1-10ff-4937-b676-38a5a916aa17.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ca62af1-10ff-4937-b676-38a5a916aa17.png)'
- en: In the preceding example, we printed a computational graph using the `visualize()`
    method. In this graph, all the cube operations were executed in a parallel fashion
    and their result was consumed by the `average()` function.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们使用 `visualize()` 方法打印了一个计算图。在该图中，所有的立方体操作都是并行执行的，并且它们的结果被 `average()`
    函数消费。
- en: Preprocessing data at scale
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模数据预处理
- en: Dask preprocessing offers scikit-learn functionalities such as scalers, encoders,
    and train/test splits. These preprocessing functionalities work well with Dask
    DataFrames and Arrays since they can fit and transform data in parallel. In this
    section, we will discuss feature scaling and feature encoding.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 预处理提供了 scikit-learn 功能，如缩放器、编码器和训练/测试拆分。这些预处理功能与 Dask DataFrame 和数组配合良好，因为它们可以并行地拟合和转换数据。在本节中，我们将讨论特征缩放和特征编码。
- en: Feature scaling in Dask
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask 中的特征缩放
- en: 'As we discussed in Chapter 7, *Cleaning Messy Data*, feature scaling, also
    known as feature normalization, is used to scale the features at the same level.
    It can handle issues regarding different column ranges and units. Dask also offers
    scaling methods that have parallel execution capacity. It uses most of the methods
    that scikit-learn offers:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第 7 章 *清理混乱数据* 中讨论的，特征缩放，也叫做特征归一化，用于将特征缩放到相同的水平。它可以处理不同列范围和单位的问题。Dask 还提供了具有并行执行能力的缩放方法。它使用了
    scikit-learn 提供的大多数方法：
- en: '| **Scaler** | **Description** |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| **Scaler** | **描述** |'
- en: '| MinMaxScaler | Transforms features by scaling each feature to a given range
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| MinMaxScaler | 通过将每个特征缩放到给定范围来转换特征 |'
- en: '| RobustScaler | Scales features using statistics that are robust to outliers
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| RobustScaler | 使用对异常值稳健的统计方法进行特征缩放 |'
- en: '| StandardScaler | Standardizes features by removing the mean and scaling them
    to unit variance |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| StandardScaler | 通过去除均值并将特征缩放到单位方差来标准化特征 |'
- en: 'Let''s scale the `last_evaluation` (employee performance score) column of the
    human resource dataset:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对人力资源数据集中的`last_evaluation`（员工表现得分）列进行缩放：
- en: '[PRE46]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This results in the following output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/b1909ce6-1ceb-45ef-a434-838e69870c19.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1909ce6-1ceb-45ef-a434-838e69870c19.png)'
- en: 'In the preceding code, we read the human resource CSV file using the `read_csv()`
    function into a Dask DataFrame. The preceding output only shows some of the columns
    that are available. However, when you run the notebook for yourself, you''ll be
    able to see all the columns in the dataset. Now, let''s scale the `last_evalaution`
    column (last evaluated performance score):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`read_csv()`函数将人力资源CSV文件读取到Dask DataFrame中。前面的输出仅显示了部分可用列。然而，当你自己运行笔记本时，你将能够看到数据集中的所有列。现在，让我们对`last_evalaution`列（最后评估的表现得分）进行缩放：
- en: '[PRE47]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This results in the following output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE48]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the preceding example, we scaled the `last_evaluation` (last evaluated performance
    score) column. We scaled it from a range of 0-1 range to a range of 0-100\. Next,
    we will look at feature encoding in Dask.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们对`last_evaluation`（最后评估的表现得分）列进行了缩放。我们将其从0-1的范围缩放到0-100的范围。接下来，我们将看一下Dask中的特征编码。
- en: Feature encoding in Dask
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask中的特征编码
- en: 'As we discussed in Chapter 7, *Cleaning Messy Data*, feature encoding is a
    very useful technique for handling categorical features. Dask also offers encoding
    methods that have parallel execution capacity. It uses most of the methods that
    scikit-learn offers:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第7章*清理杂乱数据*中讨论的那样，特征编码是处理类别特征的一个非常有用的技术。Dask还提供了具有并行执行能力的编码方法。它使用了scikit-learn提供的大多数方法：
- en: '| **Encoder** | **Description** |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **编码器** | **描述** |'
- en: '| LabelEncoder | Encodes labels with a value between 0 and 1 that''s less than
    the number of classes available. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| LabelEncoder | 将标签编码为介于0和1之间的值，且该值小于可用类别的数量。 |'
- en: '| OneHotEncoder | Encodes categorical integer features as a one-hot encoding.
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| OneHotEncoder | 将类别整数特征编码为独热编码。 |'
- en: '| OrdinalEncoder | Encodes a categorical column as an ordinal variable. |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| OrdinalEncoder | 将类别列编码为序数变量。 |'
- en: 'Let''s try using these methods:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用这些方法：
- en: '[PRE49]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This results in the following output:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/db24794a-b6d2-4580-b831-673e354e2a22.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db24794a-b6d2-4580-b831-673e354e2a22.png)'
- en: 'In the preceding code, we read the human resource CSV file using the `read_csv()`
    function into a Dask DataFrame. The preceding output only shows some of the columns
    that are available. However, when you run the notebook for yourself, you''ll be
    able to see all the columns in the dataset. Now, let''s scale the `last_evalaution`
    column (last evaluated performance score):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`read_csv()`函数将人力资源CSV文件读取到Dask DataFrame中。前面的输出仅显示了部分可用列。然而，当你自己运行笔记本时，你将能够看到数据集中的所有列。现在，让我们对`last_evalaution`列（最后评估的表现得分）进行缩放：
- en: '[PRE50]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This results in the following output:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '|  | **salary_low** | **salary_medium** | **salary_high** |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | **salary_low** | **salary_medium** | **salary_high** |'
- en: '| **0** | 1.0 | 0.0 | 0.0 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1.0 | 0.0 | 0.0 |'
- en: '| **1** | 0.0 | 1.0 | 0.0 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0.0 | 1.0 | 0.0 |'
- en: '| **2** | 0.0 | 1.0 | 0.0 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 0.0 | 1.0 | 0.0 |'
- en: '| **3** | 1.0 | 0.0 | 0.0 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 1.0 | 0.0 | 0.0 |'
- en: '| **4** | 1.0 | 0.0 | 0.0 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 1.0 | 0.0 | 0.0 |'
- en: In the preceding example, the `scikit-learn` pipeline was created using `Categorizer()`
    and `OneHotEncoder()`. The Salary column of the Human Resource data was then encoded
    using the `fit()` and `transform()` methods. Note that the categorizer will convert
    the columns of a DataFrame into categorical data types.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，`scikit-learn`管道是通过`Categorizer()`和`OneHotEncoder()`创建的。然后，使用`fit()`和`transform()`方法对人力资源数据的薪资列进行了编码。请注意，分类器将把DataFrame的列转换为类别数据类型。
- en: 'Similarly, we can also encode the Salary column using the ordinal encoder.
    Let''s take a look at an example:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们也可以使用序数编码器对薪资列进行编码。让我们来看一个例子：
- en: '[PRE51]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This results in the following output:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE52]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In the preceding example, the scikit-learn pipeline was created using `Categorizer()`
    and `OrdinalEncoder()`. The Salary column of the Human Resource data was then
    encoded using the `fit()` and `transform()` methods. Note that the categorizer
    will convert the columns of a DataFrame in categorical data types.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，scikit-learn管道是通过`Categorizer()`和`OrdinalEncoder()`创建的。然后，使用`fit()`和`transform()`方法对人力资源数据的薪资列进行了编码。请注意，分类器将把DataFrame的列转换为类别数据类型。
- en: Machine learning at scale
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模机器学习
- en: Dask offers Dask-ML services for large-scale machine learning operations using
    Python. Dask-ML decreases the model training time for medium-sized datasets and
    experiments with hyperparameter tuning. It offers scikit-learn-like machine learning
    algorithms for ML operations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供了 Dask-ML 服务，用于使用 Python 进行大规模机器学习操作。Dask-ML 能够缩短中型数据集的模型训练时间，并进行超参数调优实验。它为机器学习操作提供了类似于
    scikit-learn 的算法。
- en: 'We can scale scikit-learn in three different ways: parallelize scikit-learn
    using `joblib` by using random forest and SVC; reimplement algorithms using Dask
    Arrays using generalized linear models, preprocessing, and clustering; and partner
    it with distributed libraries such as XGBoost and Tensorflow.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过三种不同方式扩展 scikit-learn：使用 `joblib` 并通过随机森林和支持向量机（SVC）并行化 scikit-learn；使用
    Dask Arrays 重新实现算法，采用广义线性模型、预处理和聚类；并将其与分布式库如 XGBoost 和 Tensorflow 配对使用。
- en: Let's start by looking at parallel computing using scikit-learn.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先来看一下使用 scikit-learn 进行并行计算。
- en: Parallel computing using scikit-learn
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行并行计算
- en: 'To perform parallel computing using scikit-learn on a single CPU, we need to
    use `joblib`. This makes scikit-learn operations parallel computable. The `joblib`
    library performs parallelization on Python jobs. Dask can help us perform parallel
    operations on multiple scikit-learn estimators. Let''s take a look:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要在单个 CPU 上使用 scikit-learn 进行并行计算，我们需要使用 `joblib`。这使得 scikit-learn 操作可以并行计算。`joblib`
    库在 Python 作业上执行并行化。Dask 可以帮助我们在多个 scikit-learn 估计器上执行并行操作。让我们来看看：
- en: 'First, we need to read the dataset. We can load the dataset using a pandas
    DataFrame, like so:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要读取数据集。我们可以像这样使用 pandas DataFrame 加载数据集：
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This results in the following output:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '![](img/f8d1eb2a-6eea-4c18-9a03-a2ab565ab35f.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8d1eb2a-6eea-4c18-9a03-a2ab565ab35f.png)'
- en: In the preceding code, we read the human resource CSV file using the `read_csv()`
    function into a Dask DataFrame. The preceding output only shows some of the columns
    that are available. However, when you run the notebook for yourself, you will
    be able to see all the columns in the dataset. Now, let's scale the `last_evalaution`
    column (last evaluated performance score).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们通过 `read_csv()` 函数将人力资源 CSV 文件读入 Dask DataFrame。前面的输出仅显示了部分可用的列。然而，当你自己运行该笔记本时，你将能看到数据集中的所有列。现在，让我们对
    `last_evalaution` 列（最后评估的表现分数）进行缩放。
- en: 'Next, we must select the dependent and independent columns. To do this, select
    the columns and divide the data into dependent and independent variables, as follows:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要选择因变量和自变量。为此，选择列并将数据分为因变量和自变量，如下所示：
- en: '[PRE54]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Create a scheduler and generate the model in parallel. Import the `dask.distributed`
    client to create a scheduler and worker on a local machine:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个调度程序，并并行生成模型。导入 `dask.distributed` 客户端以在本地计算机上创建调度程序和工作者：
- en: '[PRE55]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The next step is to create a parallel backend using `sklearn.externals.joblib`
    and write the normal `scikit-learn` code:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是使用 `sklearn.externals.joblib` 创建并行后端，并编写正常的 `scikit-learn` 代码：
- en: '[PRE56]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'This results in the following output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '[PRE57]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The preceding parallel generated random forest model has given us 92% accuracy,
    which is very good.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 前面并行生成的随机森林模型给出了 92% 的准确率，非常好。
- en: Reimplementing ML algorithms for Dask
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为 Dask 重新实现机器学习算法
- en: 'Some machine learning algorithms have been reimplemented by the Dask development
    team using Dask Arrays and DataFrames. The following algorithms have been reimplemented:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习算法已由 Dask 开发团队使用 Dask Arrays 和 DataFrames 重新实现。以下是已重新实现的算法：
- en: Linear machine learning models such as linear regression and logistic regression
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性机器学习模型，如线性回归和逻辑回归
- en: Preprocessing with scalers and encoders
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用缩放器和编码器进行预处理
- en: Unsupervised algorithms such as k-means clustering and spectral clustering
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督算法，如 k-means 聚类和谱聚类
- en: In the following subsection, we will build a logistic regression model and perform
    clustering on the dataset.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们将构建一个逻辑回归模型，并对数据集进行聚类。
- en: Logistic regression
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Let''s build a classifier using logistic regression:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用逻辑回归构建一个分类器：
- en: 'Load the dataset into a Dask DataFrame, as follows:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集加载到 Dask DataFrame 中，如下所示：
- en: '[PRE58]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This results in the following output:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '![](img/f287e4e5-469b-41fc-8117-06e8045a37fb.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f287e4e5-469b-41fc-8117-06e8045a37fb.png)'
- en: In the preceding code, we read the human resource CSV file using the `read_csv()`
    function into a Dask DataFrame. The preceding output only shows some of the columns
    that are available. However, you run the notebook for yourself, you will be able
    to see all the columns in the dataset. Now, let's scale the `last_evalaution`
    column (last evaluated performance score).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们使用`read_csv()`函数将人力资源CSV文件读取到Dask DataFrame中。之前的输出只显示了可用的部分列。然而，当你自己运行笔记本时，你将能够看到数据集中所有的列。现在，让我们对`last_evalaution`列（最后评估的性能得分）进行缩放。
- en: 'Next, select the required column for classification and divide it into dependent
    and independent variables:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，选择所需的列进行分类，并将其划分为依赖变量和独立变量：
- en: '[PRE59]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now, let''s create a `LogisticRegression` model. First, import `LogisticRegression`
    and `train_test_split`. Once you''ve imported the required libraries, divide the
    dataset into two parts; that is, training and testing datasets:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个`LogisticRegression`模型。首先，导入`LogisticRegression`和`train_test_split`。导入所需的库后，将数据集分成两部分，即训练数据集和测试数据集：
- en: '[PRE60]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Instantiate the model and fit it to a training dataset. Now, you can predict
    the test data and compute the model''s accuracy, as follows:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化模型并将其拟合到训练数据集上。现在，你可以预测测试数据并计算模型的准确性，如下所示：
- en: '[PRE61]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This results in the following output:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE62]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: As we can see, the model is offering an accuracy of 77.5%, which is considered
    good.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，模型提供的准确率为77.5%，这被认为是一个不错的结果。
- en: Clustering
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类
- en: 'The developers of Dask have also reimplemented various k-means clustering algorithms.
    Let''s perform clustering using Dask:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的开发人员还重新实现了各种k-means聚类算法。让我们使用Dask进行聚类：
- en: 'Read the human resource data into a Dask DataFrame, as follows:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将人力资源数据读取到Dask DataFrame中，如下所示：
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'This results in the following output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/297bf3c5-6313-4b48-9d89-c7ca02af7461.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/297bf3c5-6313-4b48-9d89-c7ca02af7461.png)'
- en: In the preceding code, we read the human resource CSV file using the `read_csv()`
    function into a Dask DataFrame. The preceding output only shows some of the columns
    that are available. However, when you run the notebook for yourself, you will
    be able to see all the columns in the dataset. Now, let's scale the `last_evalaution`
    column (last evaluated performance score).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们使用`read_csv()`函数将人力资源CSV文件读取到Dask DataFrame中。之前的输出只显示了可用的部分列。然而，当你自己运行笔记本时，你将能够看到数据集中所有的列。现在，让我们对`last_evalaution`列（最后评估的性能得分）进行缩放。
- en: 'Next, select the required column for k-means clustering. We have selected the
    `satisfaction_level` and `last_evaluation` columns here:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，选择所需的列进行k-means聚类。我们在这里选择了`satisfaction_level`和`last_evaluation`列：
- en: '[PRE64]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, let''s create a k-means clustering model. First, import k-means. Once
    you''ve imported the required libraries, fit them onto the dataset and get the
    necessary labels. We can find these labels by using the `compute()` method:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个k-means聚类模型。首先，导入k-means。导入所需的库后，将其拟合到数据集上并获取必要的标签。我们可以使用`compute()`方法找到这些标签：
- en: '[PRE65]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This results in the following output:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE66]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In the preceding code, we created the k-means model with three clusters, fitted
    the model, and predicted the labels for the cluster.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们创建了一个包含三个聚类的k-means模型，拟合了模型，并预测了聚类标签。
- en: 'Now, we will visualize the k-means results using the `matplotlib` library:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用`matplotlib`库来可视化k-means的结果：
- en: '[PRE67]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This results in the following output:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/90f1c4ae-6f0b-423a-8161-42c3f618b457.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90f1c4ae-6f0b-423a-8161-42c3f618b457.png)'
- en: In the preceding code, we visualized the clusters using `matplotlib.pyplot`.
    Here, we have plotted the satisfaction score on the X-axis, the performance score
    on the Y-axis, and distinguished between the clusters by using different colors.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们使用`matplotlib.pyplot`对聚类进行了可视化。在这里，我们将满意度得分绘制在X轴上，性能得分绘制在Y轴上，并通过使用不同的颜色来区分聚类。
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we focused on how to perform parallel computation on basic
    data science Python libraries such as pandas, Numpy, and scikit-learn. Dask provides
    a complete abstraction for DataFrames and Arrays for processing moderately large
    datasets over single/multiple core machines or multiple nodes in a cluster.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们重点讲解了如何在基础数据科学Python库（如pandas、Numpy和scikit-learn）上执行并行计算。Dask为DataFrame和数组提供了完整的抽象，可以在单核/多核机器或集群中的多个节点上处理中等规模的数据集。
- en: We started this chapter by looking at Dask data types such as DataFrames, Arrays,
    and Bags. After that, we focused on Dask Delayed, preprocessing, and machine learning
    algorithms in a parallel environment.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从本章开始时，先了解了Dask的数据类型，如DataFrame、数组和Bag。接着，我们重点介绍了Dask Delayed、预处理以及在并行环境下的机器学习算法。
- en: This was the last chapter of this book, which means our learning journey ends
    here. We have focused on core Python libraries for data analysis and machine learning
    such as pandas, Numpy, Scipy, and scikit-learn. We have also focused on Python
    libraries that can be used for text analytics, image analytics, and parallel computation
    such as NLTK, spaCy, OpenCV, and Dask. Of course, your learning process doesn't
    need to stop here; keep learning new things and about the latest changes. Try
    to explore and change code based on your business or client needs. You can also
    start private or personal projects for learning purposes. If you are unable to
    decide on what kind of project you want to start, you can participate in Kaggle
    competitions at [http://www.kaggle.com/](http://www.kaggle.com/) and more!
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的最后一章，意味着我们的学习旅程到此为止。我们重点介绍了用于数据分析和机器学习的核心Python库，如pandas、Numpy、Scipy和scikit-learn。我们还关注了可以用于文本分析、图像分析和并行计算的Python库，如NLTK、spaCy、OpenCV和Dask。当然，你的学习过程不需要止步于此；继续学习新知识并关注最新的变化。尝试根据你的业务或客户需求探索和修改代码。你也可以开始私人或个人项目来进行学习。如果你无法决定要开始什么样的项目，可以参加Kaggle竞赛，访问[http://www.kaggle.com/](http://www.kaggle.com/)等平台！
