- en: 12\. Feature Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12. 特征工程
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: By the end of this chapter, you will be able to merge multiple datasets together;
    bin categorical and numerical variables; perform aggregation on data; and manipulate
    dates using `pandas`.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将能够将多个数据集合并在一起；对分类变量和数值变量进行分箱处理；对数据进行聚合操作；以及使用`pandas`操作日期。
- en: This chapter will introduce you to some of the key techniques for creating new
    variables on an existing dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向你介绍一些在现有数据集上创建新变量的关键技术。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the previous chapters, we learned how to analyze and prepare a dataset in
    order to increase its level of quality. In this chapter, we will introduce you
    to another interesting topic: creating new features, also known as feature engineering.
    You already saw some of these concepts in *Chapter 3*, *Binary Classification*,
    but we will dive a bit deeper into it in this chapter.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了如何分析和准备数据集，以提高其质量水平。在本章中，我们将向你介绍另一个有趣的话题：创建新特征，也叫做特征工程。你已经在*第3章*《二分类》中看到了一些这些概念，但我们将在本章中对其进行更深入的探讨。
- en: The objective of feature engineering is to provide more information for the
    analysis you are performing on or the machine learning algorithms you will train
    on. Adding more information will help you to achieve better and more accurate
    results.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的目标是为你所进行的分析或将要训练的机器学习算法提供更多的信息。增加更多的信息将帮助你获得更好、更准确的结果。
- en: New features can come from internal data sources such as another table from
    databases or from different systems. For instance, you may want to link data from
    the CRM tool used in your company to the data from a marketing tool. The added
    features can also come from external sources such as open-source data or shared
    data from partners or providers. For example, you may want to link the volume
    of sales with a weather API or with governmental census data. But it can also
    come from the original dataset by creating new variables from existing ones.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 新特征可以来自内部数据源，如数据库中的另一个表，或者来自不同的系统。例如，你可能想要将公司使用的CRM工具中的数据与营销工具中的数据连接起来。添加的特征也可以来自外部数据源，如开源数据或来自合作伙伴或供应商的共享数据。例如，你可能想要将销售量与天气API或政府普查数据联系起来。但它也可以通过从现有特征中创建新变量而来自原始数据集。
- en: Let's pause for a second and understand why feature engineering is so important
    for training machine learning algorithms. We are all aware that these algorithms
    have achieved incredible results in recent years in finding extremely complex
    patterns from data. But their main limitations lie in the fact that they can only
    analyze and find meaningful patterns within the data provided as input. If the
    data is incorrect, incomplete, or missing important features, the algorithms will
    not be able to perform correctly.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停片刻，理解为什么特征工程对训练机器学习算法如此重要。我们都知道，这些算法近年来在从数据中发现极其复杂的模式方面取得了令人难以置信的成果。但它们的主要局限性在于，它们只能分析和发现输入数据中存在的有意义的模式。如果数据不正确、不完整，或缺少重要特征，算法将无法正确执行。
- en: On the other hand, we humans tend to understand the broader context and see
    the bigger picture quite easily. For instance, if you were tasked with analyzing
    customer churn, even before looking at the existing data, you would already expect
    it to have some features describing customer attributes such as demographics,
    services or products subscribed to, and subscription date. And once we receive
    the data, we can highlight the features that we think are important and missing
    from the dataset. This is the reason why data scientists, with their expertise
    and experience, need to think about the additional information that will help
    algorithms to understand and detect more meaningful patterns from this enriched
    data. Without further ado, let's jump in.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们人类往往能轻松地理解更广泛的背景并看到更宏观的全貌。例如，如果你被要求分析客户流失，在查看现有数据之前，你就已经可以预期数据中会包含一些描述客户属性的特征，比如人口统计信息、订阅的服务或产品以及订阅日期等。一旦我们收集到数据，我们可以突出那些我们认为重要但数据集中缺失的特征。这就是为什么数据科学家需要凭借他们的专业知识和经验，思考哪些附加信息能够帮助算法理解并从丰富的数据中发现更有意义的模式。事不宜迟，让我们深入探讨。
- en: Merging Datasets
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并数据集
- en: Most organizations store their data in data stores such as databases, data warehouses,
    or data lakes. The flow of information can come from different systems or tools.
    Most of the time, the data is stored in a relational database composed of multiple
    tables rather than a single one with well-defined relationships between them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数组织将其数据存储在数据存储中，例如数据库、数据仓库或数据湖。信息的流动可以来自不同的系统或工具。大多数时候，数据存储在由多个表组成的关系型数据库中，而不是单一的表，且这些表之间有明确的关系。
- en: For instance, an online store could have multiple tables for recording all the
    purchases made on its platform. One table might contain information relating to
    existing customers, another one might list all existing and past products in the
    catalog, and a third one might contain all of the transactions that occurred,
    and so on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个在线商店可能有多个表来记录平台上所有的购买。一个表可能包含有关现有客户的信息，另一个表可能列出所有现有和过去的目录产品，而第三个表可能包含所有发生的交易，等等。
- en: If you were working on a project recommending products to customers for an e-commerce
    platform such as Amazon, you may have been given only the data from the transactions
    table. In that case, you would like to get some attributes for each product and
    customer and would have to ask to extract these additional tables you need and
    then merge the three tables together before building your recommendation system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在为像亚马逊这样的电子商务平台开发产品推荐系统，你可能只得到了交易表的数据。在这种情况下，你可能需要获取每个产品和客户的一些属性，并要求提取这些额外的表格，然后将三个表格合并在一起，最后再构建推荐系统。
- en: 'Let''s see how we can merge multiple data sources with a real example: the
    Online Retail dataset we used in the previous chapter. We will add new information
    regarding whether the transactions happened on public holidays in the UK or not.
    This additional data may help the model to understand whether there are some correlations
    between sales and some public holidays such as Christmas or the Queen''s birthday,
    which is a holiday in countries such as Australia.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何通过一个实际的例子来合并多个数据源：我们在上一章使用的在线零售数据集。我们将添加关于交易是否发生在英国的公共假期的信息。这些附加数据可能帮助模型理解销售和一些公共假期之间是否有相关性，例如圣诞节或女王生日，这在像澳大利亚这样的国家是一个假期。
- en: Note
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The list of public holidays in the UK will be extracted from this site: [https://packt.live/2twsFVR](https://packt.live/2twsFVR).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 英国的公共假期列表将从此网站中提取：[https://packt.live/2twsFVR](https://packt.live/2twsFVR)。
- en: 'First, we need to import the Online Retail dataset into a `pandas` DataFrame:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将在线零售数据集导入到`pandas` DataFrame中：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You should get the following output.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出。
- en: '![Figure 12.01: First five rows of the Online Retail dataset'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.01：在线零售数据集的前五行'
- en: '](img/B15019_12_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_01.jpg)'
- en: 'Figure 12.1: First five rows of the Online Retail dataset'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：在线零售数据集的前五行
- en: Next, we are going to load all the public holidays in the UK into another `pandas`
    DataFrame. From *Chapter 10*, *Analyzing a Dataset* we know the records of this
    dataset are only for the years 2010 and 2011\. So we are going to extract public
    holidays for those two years, but we need to do so in two different steps as the
    API provided by `date.nager` is split into single years only.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把所有英国的公共假期加载到另一个`pandas` DataFrame中。从*第10章*，*数据集分析*中我们知道，这个数据集的记录仅限于2010年和2011年。因此，我们将提取这两年的公共假期，但需要分两步进行，因为`date.nager`提供的API仅按单一年份进行分割。
- en: 'Let''s focus on 2010 first:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们关注2010年：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can print its shape to see how many rows and columns it has:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印它的形状，看看它有多少行和列：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You should get the following output.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出。
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can see there were `13` public holidays in that year and there are `8` different columns.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，那一年有`13`个公共假期，并且有`8`个不同的列。
- en: 'Let''s print the first five rows of this DataFrame:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印这个DataFrame的前五行：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should get the following output:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.02: First five rows of the UK 2010 public holidays DataFrame'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.02：英国2010年公共假期DataFrame的前五行'
- en: '](img/B15019_12_02.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_02.jpg)'
- en: 'Figure 12.2: First five rows of the UK 2010 public holidays DataFrame'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：英国2010年公共假期DataFrame的前五行
- en: 'Now that we have the list of public holidays for 2010, let''s extract the ones
    for 2011:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了2010年的公共假期列表，让我们提取2011年的公共假期：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You should get the following output.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出。
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'There were `15` public holidays in 2011\. Now we need to combine the records
    of these two DataFrames. We will use the `.append()` method from `pandas` and
    assign the results into a new DataFrame:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2011年共有`15`个公共假期。现在我们需要将这两个数据框的记录合并。我们将使用`pandas`的`.append()`方法，并将结果分配到一个新的数据框中：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s check we have the right number of rows after appending the two DataFrames:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查在合并这两个数据框后，是否得到了正确的行数：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should get the following output:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We got `28` records, which corresponds with the total number of public holidays
    in 2010 and 2011\.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了`28`条记录，正好对应2010年和2011年的公共假期总数。
- en: 'In order to merge two DataFrames together, we need to have at least one common
    column between them, meaning the two DataFrames should have at least one column
    that contains the same type of information. In our example, we are going to merge
    this DataFrame using the `Date` column with the Online Retail DataFrame on the
    `InvoiceDate` column. We can see that the data format of these two columns is
    different: one is a date (`yyyy-mm-dd`) and the other is a datetime (`yyyy-mm-dd
    hh:mm:ss`).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将两个数据框合并，我们需要确保它们之间至少有一列公共列，也就是说，两个数据框应至少有一列包含相同类型的信息。在我们的例子中，我们将通过`Date`列和在线零售数据框中的`InvoiceDate`列进行合并。我们可以看到这两列的数据格式不同：一个是日期格式（`yyyy-mm-dd`），另一个是日期时间格式（`yyyy-mm-dd
    hh:mm:ss`）。
- en: So, we need to transform the `InvoiceDate` column into date format (`yyyy-mm-dd`).
    One way to do it (we will see another one later in this chapter) is to transform
    this column into text and then extract the first 10 characters for each cell using
    the `.str.slice()` method.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们需要将`InvoiceDate`列转换为日期格式（`yyyy-mm-dd`）。一种方法是将此列转换为文本，然后使用`.str.slice()`方法提取每个单元格的前10个字符。
- en: 'For example, the date 2010-12-01 08:26:00 will first be converted into a string
    and then we will keep only the first 10 characters, which will be 2010-12-01\.
    We are going to save these results into a new column called `InvoiceDay`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，日期`2010-12-01 08:26:00`将首先被转换为字符串，然后我们将只保留前10个字符，即`2010-12-01`。我们将这些结果保存到一个新列`InvoiceDay`中：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 12.03: First five rows after creating InvoiceDay'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.03：创建`InvoiceDay`后的前五行'
- en: '](img/B15019_12_03.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_03.jpg)'
- en: 'Figure 12.3: First five rows after creating InvoiceDay'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3：创建`InvoiceDay`后的前五行
- en: Now `InvoiceDay` from the online retail DataFrame and `Date` from the UK public
    holidays DataFrame have similar information, so we can merge these two DataFrames
    together using `.merge()` from `pandas`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，来自在线零售数据框的`InvoiceDay`列和来自英国公共假期数据框的`Date`列包含相似的信息，因此我们可以使用`pandas`的`.merge()`方法将这两个数据框合并在一起。
- en: 'There are multiple ways to join two tables together:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以将两张表连接在一起：
- en: The left join
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左连接
- en: The right join
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右连接
- en: The inner join
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内连接
- en: The outer join
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外连接
- en: The Left Join
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 左连接
- en: 'The left join will keep all the rows from the first DataFrame, which is the
    *Online Retail* dataset (the left-hand side) and join it to the matching rows
    from the second DataFrame, which is the *UK Public Holidays* dataset (the right-hand
    side), as shown in *Figure 12.04*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 左连接将保留第一个数据框（即*在线零售*数据集，位于左侧）中的所有行，并将其与第二个数据框（即*英国公共假期*数据集，位于右侧）中的匹配行连接，如*图
    12.04*所示：
- en: '![Figure 12.04: Venn diagram for left join'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.04：左连接的文氏图'
- en: '](img/B15019_12_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_04.jpg)'
- en: 'Figure 12.4: Venn diagram for left join'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4：左连接的文氏图
- en: 'To perform a left join, we need to specify to the .merge() method the following parameters:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行左连接，我们需要在`.merge()`方法中指定以下参数：
- en: '`how = ''left''` for a left join'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`how = ''left''`表示进行左连接'
- en: '`left_on = InvoiceDay` to specify the column used for merging from the left-hand
    side (here, the `Invoiceday` column from the Online Retail DataFrame)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`left_on = InvoiceDay` 用于指定从左侧（这里是在线零售数据框中的`InvoiceDay`列）合并的列'
- en: '`right_on = Date` to specify the column used for merging from the right-hand
    side (here, the `Date` column from the UK Public Holidays DataFrame)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`right_on = Date` 用于指定从右侧（这里是英国公共假期数据框中的`Date`列）合并的列'
- en: 'These parameters are clubbed together as shown in the following code snippet:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数如下面的代码片段所示，已被组合在一起：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You should get the following output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We got the exact same number of rows as the original Online Retail DataFrame,
    which is expected for a left join. Let''s have a look at the first five rows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了与原始在线零售数据框完全相同的行数，这是左连接所期望的结果。让我们看看前五行：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You should get the following output:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.05: First five rows of the left-merged DataFrame'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.05：左连接合并后的前五行数据'
- en: '](img/B15019_12_05.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_05.jpg)'
- en: 'Figure 12.5: First five rows of the left-merged DataFrame'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：左连接合并后的前五行数据
- en: We can see that the eight columns from the public holidays DataFrame have been
    merged to the original one. If no row has been matched from the second DataFrame
    (in this case, the public holidays one), `pandas` will fill all the cells with
    missing values (`NaT` or `NaN`), as shown in *Figure 12.05*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，来自公共假期数据框的八列已经与原始数据框合并。如果第二个数据框（在这种情况下是公共假期数据框）没有匹配的行，`pandas` 将用缺失值（`NaT`
    或 `NaN`）填充所有单元格，如*图12.05*所示。
- en: The Right Join
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 右连接
- en: 'The right join is similar to the left join except it will keep all the rows
    from the second DataFrame (the right-hand side) and tries to match it with the
    first one (the left-hand side), as shown in *Figure 12.06*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 右连接与左连接类似，不同之处在于它会保留第二个数据框（右侧）的所有行，并尝试将其与第一个数据框（左侧）进行匹配，如*图12.06*所示：
- en: '![Figure 12.06: Venn diagram for right join'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.06：右连接的韦恩图'
- en: '](img/B15019_12_06.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_06.jpg)'
- en: 'Figure 12.6: Venn diagram for right join'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6：右连接的韦恩图
- en: 'We just need to specify the parameters:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要指定以下参数：
- en: '`how` `= ''right`'' to the `.merge()` method to perform this type of join.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`how` `= ''right''`到`.merge()`方法中来执行这种类型的连接。'
- en: We will use the exact same columns used for merging as the previous example,
    which is `InvoiceDay` for the Online Retail DataFrame and `Date` for the UK Public
    Holidays one.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用与前一个示例相同的列进行合并，即`InvoiceDay`（在线零售数据框）和`Date`（英国公共假期数据框）。
- en: 'These parameters are clubbed together as shown in the following code snippet:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数被组合在一起，如下所示的代码片段：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You should get the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can see there are fewer rows as a result of the right join, but it doesn't
    get the same number as for the Public Holidays DataFrame. This is because there
    are multiple rows from the Online Retail DataFrame that match one single date
    in the public holidays one.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，右连接的结果行数较少，但它并没有得到与公共假期数据框相同的行数。这是因为在线零售数据框中有多行与公共假期数据框中的单一日期匹配。
- en: 'For instance, looking at the first rows of the merged DataFrame, we can see
    there were multiple purchases on January 4, 2011, so all of them have been matched
    with the corresponding public holiday. Have a look at the following code snippet:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，查看合并后的数据框的前几行，我们可以看到2011年1月4日有多次购买，因此所有这些购买都已与相应的公共假期匹配。看看以下代码片段：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should get the following output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.07: First five rows of the right-merged DataFrame'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.07：右连接合并后的前五行数据'
- en: '](img/B15019_12_07.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_07.jpg)'
- en: 'Figure 12.7: First five rows of the right-merged DataFrame'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7：右连接合并后的前五行数据
- en: 'There are two other types of merging: inner and outer.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两种其他类型的合并：内连接和外连接。
- en: 'An inner join will only keep the rows that match between the two tables:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 内连接将只保留两个表之间匹配的行：
- en: '![Figure 12.08: Venn diagram for inner join'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.08：内连接的韦恩图'
- en: '](img/B15019_12_08.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_08.jpg)'
- en: 'Figure 12.8: Venn diagram for inner join'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8：内连接的韦恩图
- en: You just need to specify the `how = 'inner'` parameter in the `.merge()` method.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要在`.merge()`方法中指定`how = 'inner'`参数。
- en: 'These parameters are clubbed together as shown in the following code snippet:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数被组合在一起，如下所示的代码片段：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You should get the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can see there are only 9,579 observations that happened during a public holiday
    in the UK.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在英国有9,579条观察记录发生在公共假期期间。
- en: 'The outer join will keep all rows from both tables (matched and unmatched),
    as shown in *Figure 12.09*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 外连接将保留两个表中的所有行（匹配的和不匹配的），如*图12.09*所示：
- en: '![Figure 12.09: Venn diagram for outer join'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.09：外连接的韦恩图'
- en: '](img/B15019_12_09.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_09.jpg)'
- en: 'Figure 12.9: Venn diagram for outer join'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9：外连接的韦恩图
- en: 'As you may have guessed, you just need to specify the `how == ''outer''` parameter
    in the `.merge()` method:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜测的那样，你只需要在`.merge()`方法中指定`how == 'outer'`参数：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You should get the following output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Before merging two tables, it is extremely important for you to know what your
    focus is. If your objective is to expand the number of features from an original
    dataset by adding the columns from another one, then you will probably use a left
    or right join. But be aware you may end up with more observations due to potentially
    multiple matches between the two tables. On the other hand, if you are interested
    in knowing which observations matched or didn't match between the two tables,
    you will either use an inner or outer join.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在合并两张表之前，了解你的重点非常重要。如果你的目标是通过添加另一个数据集的列来扩展原始数据集的特征数量，那么你可能会使用左连接或右连接。但请注意，由于两个表之间可能存在多重匹配，你可能会得到更多的观测值。另一方面，如果你对两张表之间的匹配或不匹配的观测值感兴趣，那么你将使用内连接或外连接。
- en: 'Exercise 12.01: Merging the ATO Dataset with the Postcode Data'
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习12.01：将ATO数据集与Postcode数据集合并
- en: In this exercise, we will merge the ATO dataset (28 columns) with the Postcode
    dataset (150 columns) to get a richer dataset with an increased number of columns.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将把ATO数据集（28列）与Postcode数据集（150列）合并，以得到一个列数更多的丰富数据集。
- en: Note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Australian Taxation Office (ATO) dataset can be found in the Packt GitHub
    repository: [https://packt.live/39B146q](https://packt.live/39B146q).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚税务局（ATO）数据集可以在Packt GitHub库中找到：[https://packt.live/39B146q](https://packt.live/39B146q)。
- en: 'The Postcode dataset can be found here: [https://packt.live/2sHAPLc](https://packt.live/2sHAPLc).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Postcode数据集可以在这里找到：[https://packt.live/2sHAPLc](https://packt.live/2sHAPLc)。
- en: 'The sources of the dataset are as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的来源如下：
- en: 'The **Australian Taxation Office** (**ATO**): [https://packt.live/361i1p3](https://packt.live/361i1p3).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**澳大利亚税务局**（**ATO**）：[https://packt.live/361i1p3](https://packt.live/361i1p3)。'
- en: 'The Postcode dataset: [https://packt.live/2umIn6u](https://packt.live/2umIn6u).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Postcode数据集：[https://packt.live/2umIn6u](https://packt.live/2umIn6u)。
- en: 'The following steps will help you complete the exercise:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习：
- en: Open up a new Colab notebook.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Colab笔记本。
- en: 'Now, begin with the `import` of the `pandas` package:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，开始导入`pandas`包：
- en: '[PRE21]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Assign the link to the ATO dataset to a variable called `file_url`:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将ATO数据集的链接赋值给一个名为`file_url`的变量：
- en: '[PRE22]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Using the `.read_csv()` method from the `pandas` package, load the dataset
    into a new DataFrame called `df`:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`包中的`.read_csv()`方法，将数据集加载到一个名为`df`的新DataFrame中：
- en: '[PRE23]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Display the dimensions of this DataFrame using the `.shape` attribute:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.shape`属性显示此DataFrame的维度：
- en: '[PRE24]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You should get the following output:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE25]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The ATO dataset contains `2471` rows and `28` columns.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ATO数据集包含`2471`行和`28`列。
- en: 'Display the first five rows of the ATO DataFrame using the `.head()` method:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.head()`方法显示ATO DataFrame的前五行：
- en: '[PRE26]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You should get the following output:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.10: First five rows of the ATO dataset'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图12.10：ATO数据集的前五行'
- en: '](img/B15019_12_10.jpg)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_10.jpg)'
- en: 'Figure 12.10: First five rows of the ATO dataset'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.10：ATO数据集的前五行
- en: Both DataFrames have a column called `Postcode` containing postcodes, so we
    will use it to merge them together.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个DataFrame都有一个名为`Postcode`的列，包含邮政编码，因此我们将使用它来合并它们。
- en: Note
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Postcode is the name used in Australia for zip code. It is an identifier for
    postal areas.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Postcode是澳大利亚用于表示邮政编码的名称，是邮政区域的标识符。
- en: We are interested in learning more about each of these postcodes. Let's make
    sure they are all unique in this dataset.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们希望了解每个邮政编码的更多信息。让我们确保它们在该数据集中都是唯一的。
- en: 'Display the number of unique values for the `Postcode` variable using the `.nunique()`
    method:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.nunique()`方法显示`Postcode`变量的唯一值数量：
- en: '[PRE27]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should get the following output:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE28]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: There are `2473` unique values in this column and the DataFrame has `2473` rows,
    so we are sure the `Postcode` variable contains only unique values.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该列中有`2473`个唯一值，且DataFrame有`2473`行，因此我们可以确认`Postcode`变量只包含唯一值。
- en: 'Now, assign the link to the second Postcode dataset to a variable called `postcode_df`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将第二个Postcode数据集的链接赋值给一个名为`postcode_df`的变量：
- en: '[PRE29]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Load the second Postcode dataset into a new DataFrame called `postcode_df` using
    the `.read_excel()` method.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.read_excel()`方法将第二个Postcode数据集加载到一个名为`postcode_df`的新DataFrame中。
- en: We will only load the *Individuals Table 6B* sheet as this is where the data
    is located so we need to provide this name to the `sheet_name` parameter. Also,
    the header row (containing the name of the variables) in this spreadsheet is located
    on the third row so we need to specify it to the header parameter.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们只加载 *Individuals Table 6B* 表单，因为数据就在这里，所以我们需要将此名称提供给 `sheet_name` 参数。此外，此电子表格中包含变量名称的标题行位于第三行，因此我们需要将其指定给
    header 参数。
- en: '[PRE30]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Print the dimensions of `postcode_df` using the `.shape` attribute:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.shape` 属性打印 `postcode_df` 的维度：
- en: '[PRE31]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You should get the following output:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE32]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This DataFrame contains `2567` rows for `150` columns. By merging it with the
    ATO dataset, we will get additional information for each postcode.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该 DataFrame 包含 `2567` 行和 `150` 列。通过与ATO数据集的合并，我们将获得每个邮政编码的附加信息。
- en: 'Print the first five rows of `postcode_df` using the `.head()` method:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.head()` 方法打印 `postcode_df` 的前五行：
- en: '[PRE33]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You should get the following output:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.11: First five rows of the Postcode dataset'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.11：邮政编码数据集的前五行'
- en: '](img/B15019_12_11.jpg)'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_11.jpg)'
- en: 'Figure 12.11: First five rows of the Postcode dataset'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.11：邮政编码数据集的前五行
- en: We can see that the second column contains the postcode value, and this is the
    one we will use to merge on with the ATO dataset. Let's check if they are unique.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到第二列包含邮政编码值，这就是我们用来与ATO数据集合并的列。让我们检查一下它们是否唯一。
- en: 'Print the number of unique values in this column using the `.nunique()` method
    as shown in the following code snippet:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.nunique()` 方法打印该列中唯一值的数量，如下代码片段所示：
- en: '[PRE34]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You should get the following output:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE35]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: There are `2567` unique values, and this corresponds exactly to the number of
    rows of this DataFrame, so we're absolutely sure this column contains unique values.
    This also means that after merging the two tables, there will be only one-to-one
    matches. We won't have a case where we get multiple rows from one of the datasets
    matching with only one row of the other one. For instance, postcode `2029` from
    the ATO DataFrame will have exactly one match in the second Postcode DataFrame.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有 `2567` 个唯一值，这恰好对应于此 DataFrame 的行数，因此我们可以完全确定这一列包含唯一值。这也意味着，在合并两个表后，将会是一对一的匹配。我们不会遇到某个数据集中的多行与另一个数据集的单行匹配的情况。例如，ATO数据集中的邮政编码
    `2029` 将会在第二个邮政编码 DataFrame 中恰好匹配一行。
- en: 'Perform a left join on the two DataFrames using the `.merge()` method and save
    the results into a new DataFrame called `merged_df`. Specify the `how=''left''`
    and `on=''Postcode''` parameters:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.merge()` 方法对两个 DataFrame 执行左连接，并将结果保存到一个名为 `merged_df` 的新 DataFrame 中。指定
    `how='left'` 和 `on='Postcode'` 参数：
- en: '[PRE36]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Print the dimensions of the new merged DataFrame using the `.shape` attribute:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.shape` 属性打印新合并的 DataFrame 的维度：
- en: '[PRE37]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You should get the following output:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE38]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We got exactly `2473` rows after merging, which is what we expect as we used
    a left join and there was a one-to-one match on the `Postcode` column from both
    original DataFrames. Also, we now have `177` columns, which is the objective of
    this exercise. But before concluding it, we want to see whether there are any
    postcodes that didn't match between the two datasets. To do so, we will be looking
    at one column from the right-hand side DataFrame (the Postcode dataset) and see
    if there are any missing values.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 合并后我们得到了正好 `2473` 行，这是我们期望的结果，因为我们使用了左连接，并且两个原始 DataFrame 中的 `Postcode` 列存在一对一的匹配。此外，我们现在有了
    `177` 列，这也是本次练习的目标。但在得出结论之前，我们想看看两个数据集之间是否有不匹配的邮政编码。为此，我们将查看来自右侧 DataFrame（邮政编码数据集）的一列，看看是否有任何缺失值。
- en: 'Print the total number of missing values from the `''State/Territory1''` column
    by combining the `.isna()` and `.sum()` methods:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过结合 `.isna()` 和 `.sum()` 方法打印 `'State/Territory1'` 列中缺失值的总数：
- en: '[PRE39]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You should get the following output:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE40]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: There are four postcodes from the ATO dataset that didn't match the Postcode code.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ATO数据集中有四个邮政编码未与邮政编码代码匹配。
- en: Let's see which ones they are.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看它们是什么。
- en: 'Print the missing postcodes using the `.iloc()` method, as shown in the following
    code snippet:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.iloc()` 方法打印缺失的邮政编码，如下代码片段所示：
- en: '[PRE41]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You should get the following output:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.12: List of unmatched postcodes'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.12：不匹配的邮政编码列表'
- en: '](img/B15019_12_12.jpg)'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_12.jpg)'
- en: 'Figure 12.12: List of unmatched postcodes'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12：不匹配的邮政编码列表
- en: The missing postcodes from the Postcode dataset are `3010`, `4462`, `6068`,
    and `6758`. In a real project, you would have to get in touch with your stakeholders
    or the data team to see if you are able to get this data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Postcode数据集中的缺失邮政编码有`3010`、`4462`、`6068`和`6758`。在实际项目中，你需要联系你的利益相关者或数据团队，看看是否能获取这些数据。
- en: We have successfully merged the two datasets of interest and have expanded the
    number of features from `28` to `177`. We now have a much richer dataset and will
    be able to perform a more detailed analysis of it.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地合并了两个感兴趣的数据集，并将特征数量从`28`扩展到了`177`。现在，我们有了一个更丰富的数据集，可以对其进行更详细的分析。
- en: Note
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/324bV67](https://packt.live/324bV67).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这一特定部分的源代码，请参考[https://packt.live/324bV67](https://packt.live/324bV67)。
- en: You can also run this example online at [https://packt.live/2CDYv80](https://packt.live/2CDYv80).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个例子，链接：[https://packt.live/2CDYv80](https://packt.live/2CDYv80)。
- en: In the next topic, you will be introduced to the binning variables.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个主题中，你将学习分箱变量。
- en: Binning Variables
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分箱变量
- en: As mentioned earlier, feature engineering is not only about getting information
    not present in a dataset. Quite often, you will have to create new features from
    existing ones. One example of this is consolidating values from an existing column
    to a new list of values.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，特征工程不仅仅是获取数据集中没有的信息。很多时候，你需要从现有特征中创建新的特征。一个例子就是将现有列的值合并成一个新的值列表。
- en: For instance, you may have a very high number of unique values for some of the
    categorical columns in your dataset, let's say over 1,000 values for each variable.
    This is actually quite a lot of information that will require extra computation
    power for an algorithm to process and learn the patterns from. This can have a
    significant impact on the project cost if you are using cloud computing services
    or on the delivery time of the project.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你的数据集中某些分类列可能包含非常多的唯一值，比如每个变量超过1,000个值。这实际上是非常庞大的信息量，需要额外的计算能力才能让算法处理并从中学习模式。如果你使用的是云计算服务，这可能会对项目成本产生重大影响，或者会延迟项目的交付时间。
- en: One possible solution is to not use these columns and drop them, but in that
    case, you may lose some very important and critical information for the business.
    Another solution is to create a more consolidated version of these columns by
    reducing the number of unique values to a smaller number, let's say 100\. This
    would drastically speed up the training process for the algorithm without losing
    too much information. This kind of transformation is called binning and, traditionally,
    it refers to numerical variables, but the same logic can be applied to categorical
    variables as well.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的解决方案是不使用这些列并将其删除，但在这种情况下，你可能会失去一些对业务非常重要和关键的信息。另一种解决方案是通过减少唯一值的数量，创建这些列的一个更简化版本，比如将数量减少到100个。这样可以大大加快算法的训练过程，同时不会丢失太多信息。这种转化方法称为分箱（binning），传统上它是指数值变量，但同样的逻辑也可以应用于分类变量。
- en: 'Let''s see how we can achieve this on the Online Retail dataset. First, we
    need to load the data:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在“在线零售”数据集上实现这一点。首先，我们需要加载数据：
- en: '[PRE42]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In *Chapter 10*, *Analyzing a Dataset* we learned that the `Country` column
    contains `38` different unique values:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第10章*，*数据集分析*中，我们了解到`Country`列包含`38`个不同的唯一值：
- en: '[PRE43]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You should get the following output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.13: List of unique values for the Country column'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.13：Country列的唯一值列表]'
- en: '](img/B15019_12_13.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_13.jpg)'
- en: 'Figure 12.13: List of unique values for the Country column'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13：Country列的唯一值列表
- en: We are going to group some of the countries together into regions such as Asia,
    the Middle East, and America. We will leave the European countries as is.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把一些国家分组到不同的地区，比如亚洲、中东和美洲。我们会保持欧洲国家不变。
- en: 'First, let''s create a new column called `Country_bin` by copying the `Country` column:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过复制`Country`列来创建一个新的列，命名为`Country_bin`：
- en: '[PRE44]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we are going to create a list called `asian_countries` containing the
    name of Asian countries from the list of unique values for the `Country` column:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个名为`asian_countries`的列表，包含`Country`列中唯一值列表中的亚洲国家名称：
- en: '[PRE45]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'And finally, using the `.loc()` and `.isin()` methods from `pandas`, we are
    going to change the value of `Country_bin` to `Asia` for all of the countries
    that are present in the `asian_countries` list:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`pandas`的`.loc()`和`.isin()`方法，我们将把`Country_bin`的值更改为`Asia`，适用于`asian_countries`列表中所有的国家：
- en: '[PRE46]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, if we print the list of unique values for this new column, we will see
    the three Asian countries (`Japan`, `Hong Kong`, and `Singapore`) have been replaced
    by the value `Asia`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们打印这个新列的唯一值列表，我们会看到三个亚洲国家（`Japan`、`Hong Kong`和`Singapore`）已经被替换为`Asia`：
- en: '[PRE47]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'You should get the following output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '![Figure 12.14: List of unique values for the Country_bin column'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.14：`Country_bin`列的唯一值列表'
- en: after binning Asian countries
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对亚洲国家进行分箱后
- en: '](img/B15019_12_14.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_14.jpg)'
- en: 'Figure 12.14: List of unique values for the Country_bin column after binning
    Asian countries'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14：对亚洲国家进行分箱后的`Country_bin`列唯一值列表
- en: 'Let''s perform the same process for Middle Eastern countries:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对中东国家执行相同的操作：
- en: '[PRE48]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You should get the following output:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '![Figure 12.15: List of unique values for the Country_bin column after'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.15：对`Country_bin`列进行分箱后的唯一值列表'
- en: binning Middle Eastern countries
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对中东国家进行分箱
- en: '](img/B15019_12_15.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_15.jpg)'
- en: 'Figure 12.15: List of unique values for the Country_bin column after binning
    Middle Eastern countries'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.15：对中东国家进行分箱后的`Country_bin`列唯一值列表
- en: 'Finally, let''s group all countries from North and South America together:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将所有来自北美和南美的国家归为一组：
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You should get the following output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '![Figure 12.16: List of unique values for the Country_bin column after binning'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.16：对`Country_bin`列进行分箱后的唯一值列表'
- en: countries from North and South America
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 来自北美和南美的国家
- en: '](img/B15019_12_16.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_16.jpg)'
- en: 'Figure 12.16: List of unique values for the Country_bin column after binning
    countries from North and South America'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.16：对北美和南美国家进行分箱后的`Country_bin`列唯一值列表
- en: '[PRE50]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You should get the following output:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '[PRE51]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`30` is the number of unique values for the `Country_bin` column. So we reduced
    the number of unique values in this column from `38` to `30`:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`30`是`Country_bin`列的唯一值数量。所以我们将该列的唯一值从`38`减少到`30`：'
- en: We just saw how to group categorical values together, but the same process can
    be applied to numerical values as well. For instance, it is quite common to group
    people's ages into bins such as 20s (20 to 29 years old), 30s (30 to 39), and
    so on.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到如何将分类值分组在一起，但同样的过程也可以应用于数值值。例如，将人的年龄分为不同的区间，如20岁（20到29岁）、30岁（30到39岁）等，这是很常见的做法。
- en: Have a look at *Exercise 12.02*, *Binning the YearBuilt variable from the AMES
    Housing dataset*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下*练习12.02*，*从AMES Housing数据集中对YearBuilt变量进行分箱*。
- en: 'Exercise 12.02: Binning the YearBuilt Variable from the AMES Housing Dataset'
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习12.02：从AMES Housing数据集中对YearBuilt变量进行分箱
- en: In this exercise, we will create a new feature by binning an existing numerical
    column in order to reduce the number of unique values from `112` to `15`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过对现有的数值列进行分箱，来创建一个新特征，从而将唯一值的数量从`112`减少到`15`。
- en: Note
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset we will be using in this exercise is the Ames Housing dataset and
    it can be found in our GitHub repository: [https://packt.live/35r2ahN](https://packt.live/35r2ahN).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此练习中使用的数据集是Ames Housing数据集，可以在我们的GitHub仓库中找到：[https://packt.live/35r2ahN](https://packt.live/35r2ahN)。
- en: 'This dataset was compiled by Dean De Cock: [https://packt.live/2uojqHR](https://packt.live/2uojqHR).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是由Dean De Cock编制的：[https://packt.live/2uojqHR](https://packt.live/2uojqHR)。
- en: This dataset contains the list of residential home sales in the city of Ames,
    Iowa between 2010 and 2016.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含2010年至2016年期间，爱荷华州阿姆斯市的住宅房屋销售清单。
- en: 'More information about each variable can be found here: [https://packt.live/2sT88L4](https://packt.live/2sT88L4).'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 有关每个变量的更多信息可以在这里找到：[https://packt.live/2sT88L4](https://packt.live/2sT88L4)。
- en: Open up a new Colab notebook.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Colab笔记本。
- en: 'Import the `pandas` and `altair` packages:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`和`altair`包：
- en: '[PRE52]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Assign the link to the dataset to a variable called `file_url`:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集链接赋值给一个名为`file_url`的变量：
- en: '[PRE53]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Using the `.read_csv()` method from the `pandas` package, load the dataset
    into a new DataFrame called `df`:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`包的`.read_csv()`方法，将数据集加载到一个新的DataFrame中，命名为`df`：
- en: '[PRE54]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Display the first five rows using the `.head()` method:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.head()`方法显示前五行：
- en: '[PRE55]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'You should get the following output:'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '![Figure 12.17: First five rows of the AMES housing DataFrame'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图12.17：AMES房屋数据框的前五行'
- en: '](img/B15019_12_17.jpg)'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_17.jpg)'
- en: 'Figure 12.17: First five rows of the AMES housing DataFrame'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.17：AMES housing DataFrame 的前五行
- en: 'Display the number of unique values on the column using `.nunique()`:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.nunique()` 显示列中的独特值数量：
- en: '[PRE56]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You should get the following output:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE57]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'There are `112` different or unique values in the `YearBuilt` column:'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`YearBuilt` 列中有 `112` 个不同或独特的值：'
- en: 'Print a scatter plot using `altair` to visualize the number of records built
    per year. Specify `YearBuilt:O` as the x-axis and `count()` as the y-axis in the
    `.encode()` method:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `altair` 打印一个散点图，来可视化每年建筑的记录数量。在 `.encode()` 方法中指定 `YearBuilt:O` 作为 x 轴，`count()`
    作为 y 轴：
- en: '[PRE58]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'You should get the following output:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.18: First five rows of the AMES housing DataFrame'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.18：AMES housing DataFrame 的前五行'
- en: '](img/B15019_12_18.jpg)'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_18.jpg)'
- en: 'Figure 12.18: First five rows of the AMES housing DataFrame'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.18：AMES housing DataFrame 的前五行
- en: Note
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The output is not shown on GitHub due to its limitations. If you run this on
    your Colab file, the graph will be displayed.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于 GitHub 的限制，输出未显示。如果你在 Colab 文件中运行，图形将会显示。
- en: There weren't many properties sold in some of the years. So, you can group them
    by decades (groups of 10 years).
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在某些年份中，出售的房产不多。因此，你可以按十年（10 年一组）进行分组。
- en: 'Create a list called `year_built` containing all the unique values in the `YearBuilt`
    column:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `year_built` 的列表，包含 `YearBuilt` 列中的所有独特值：
- en: '[PRE59]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Create another list that will compute the decade for each year in `year_built`.
    Use list comprehension to loop through each year and apply the following formula:
    `year - (year % 10)`.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建另一个列表，用于计算 `year_built` 中每年对应的十年。使用列表推导遍历每一年并应用以下公式：`year - (year % 10)`。
- en: For example, this formula applied to the year 2015 will give 2015 - (2015 %
    10), which is 2015 – 5 equals 2010.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，应用此公式于2015年，将得到2015 - (2015 % 10)，即2015 - 5等于2010。
- en: '[PRE60]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Create a sorted list of unique values from `decade_list` and save the result
    into a new variable called `decade_built`. To do so, transform `decade_list` into
    a set (this will exclude all duplicates) and then use the `sorted()` function
    as shown in the following code snippet:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含 `decade_list` 中所有独特值的排序列表，并将结果保存在一个名为 `decade_built` 的新变量中。为此，将 `decade_list`
    转换为集合（这将排除所有重复项），然后使用 `sorted()` 函数，如以下代码片段所示：
- en: '[PRE61]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Print the values of `decade_built`:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 `decade_built` 的值：
- en: '[PRE62]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You should get the following output:'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.19: List of decades'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.19：十年列表'
- en: '](img/B15019_12_19.jpg)'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_19.jpg)'
- en: 'Figure 12.19: List of decades'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.19：十年列表
- en: Now we have the list of decades we are going to bin the `YearBuilt` column with.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们有了将 `YearBuilt` 列进行分组的十年列表。
- en: 'Create a new column on the `df` DataFrame called `DecadeBuilt` that will bin
    each value from `YearBuilt` into a decade. You will use the `.cut()` method from
    `pandas` and specify the `bins=decade_built` parameter:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `df` DataFrame 中创建一个名为 `DecadeBuilt` 的新列，将 `YearBuilt` 中的每个值按十年分组。你将使用 `pandas`
    的 `.cut()` 方法，并指定 `bins=decade_built` 参数：
- en: '[PRE63]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Print the first five rows of the DataFrame but only for the `''YearBuilt''`
    and `''DecadeBuilt''` columns:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 DataFrame 的前五行，仅显示 `'YearBuilt'` 和 `'DecadeBuilt'` 列：
- en: '[PRE64]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'You should get the following output:'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.20: First five rows after binning'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.20：分组后的前五行'
- en: '](img/B15019_12_20.jpg)'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_20.jpg)'
- en: 'Figure 12.20: First five rows after binning'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.20：分组后的前五行
- en: We can see each year has been properly assigned to the relevant decade.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每年已正确地分配到了相关的十年。
- en: Note
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3kXnC7c](https://packt.live/3kXnC7c).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参考 [https://packt.live/3kXnC7c](https://packt.live/3kXnC7c)。
- en: This section does not currently have an online interactive example, but can
    be run as usual on Google Colab.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线互动示例，但可以在 Google Colab 上照常运行。
- en: We have successfully created a new feature from the `YearBuilt` column by binning
    its values into groups of decades. We have reduced the number of unique values
    from `112` to `15`.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经通过将 `YearBuilt` 列的值按十年分组，成功创建了一个新特征。我们将独特值的数量从 `112` 减少到 `15`。
- en: Manipulating Dates
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作日期
- en: In most datasets you will be working on, there will be one or more columns containing
    date information. Usually, you will not feed that type of information directly
    as input to a machine learning algorithm. The reason is you don't want it to learn
    extremely specific patterns, such as customer A bought product X on August 3,
    2012, at 08:11 a.m. The model would be overfitting in that case and wouldn't be
    able to generalize to future data.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在你将要处理的大多数数据集中，会有一列或多列包含日期信息。通常，你不会将此类信息直接作为输入提供给机器学习算法。原因是你不希望它学习到过于特定的模式，例如顾客
    A 在 2012 年 8 月 3 日上午 08:11 购买了产品 X。如果这样，模型将会过拟合，无法对未来数据进行泛化。
- en: What you really want is the model to learn patterns, such as customers with
    young kids tending to buy unicorn toys in December, for instance. Rather than
    providing the raw dates, you want to extract some cyclical characteristics such
    as the month of the year, the day of the week, and so on. We will see in this
    section how easy it is to get this kind of information using the `pandas` package.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 你真正希望的是模型学习到一些模式，例如，带小孩的顾客倾向于在 12 月购买独角兽玩具。与其提供原始日期，你希望提取一些周期性的特征，如年份的月份、星期几等。我们将在本节中看到，使用
    `pandas` 包提取这些信息是多么简单。
- en: Note
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There is an exception to this rule of thumb. If you are performing a time-series
    analysis, this kind of algorithm requires a date column as an input feature, but
    this is out of the scope of this book.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这个经验法则有一个例外。如果你正在进行时间序列分析，这种算法需要一个日期列作为输入特征，但这超出了本书的范围。
- en: 'In *Chapter 10*, *Analyzing a Dataset* you were introduced to the concept of
    data types in `pandas`. At that time, we mainly focused on numerical variables
    and categorical ones but there is another important one: `datetime`. Let''s have
    a look again at the type of each column from the Online Retail dataset:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 10 章*，*分析数据集* 中，你已了解了 `pandas` 中数据类型的概念。那时，我们主要关注的是数值变量和类别变量，但还有一个重要的数据类型：`datetime`。让我们再次查看在线零售数据集中每列的类型：
- en: '[PRE65]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'You should get the following output:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.21: Data types for the variables in the Online Retail dataset'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.21：在线零售数据集中变量的数据类型'
- en: '](img/B15019_12_21.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_21.jpg)'
- en: 'Figure 12.21: Data types for the variables in the Online Retail dataset'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.21：在线零售数据集中变量的数据类型
- en: 'We can see that `pandas` automatically detected that `InvoiceDate` is of type
    `datetime`. But for some other datasets, it may not recognize dates properly.
    In this case, you will have to manually convert them using the `.to_datetime()`
    method:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`pandas` 自动检测到 `InvoiceDate` 是 `datetime` 类型。但对于一些其他数据集，它可能无法正确识别日期。在这种情况下，你需要使用
    `.to_datetime()` 方法手动转换它们：
- en: '[PRE66]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Once the column is converted to `datetime`, pandas provides a lot of attributes
    and methods for extracting time-related information. For instance, if you want
    to get the year of a date, you use the `.dt.year` attribute:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦列被转换为 `datetime` 类型，pandas 提供了许多属性和方法来提取与时间相关的信息。例如，如果你想获取某个日期的年份，可以使用 `.dt.year`
    属性：
- en: '[PRE67]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'You should get the following output:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.22: Extracted year for each row for the InvoiceDate column'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.22：为每行提取的 `InvoiceDate` 列的年份'
- en: '](img/B15019_12_22.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_22.jpg)'
- en: 'Figure 12.22: Extracted year for each row for the InvoiceDate column'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.22：为每行提取的 `InvoiceDate` 列的年份
- en: 'As you may have guessed, there are attributes for extracting the month and
    day of a date: `.dt.month` and `.dt.day` respectively. You can get the day of
    the week from a date using the `.dt.dayofweek` attribute:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，提取日期的月和日的属性分别是 `.dt.month` 和 `.dt.day`。你可以使用 `.dt.dayofweek` 属性从日期中获取星期几：
- en: '[PRE68]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: You should get the following output.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出。
- en: '![Figure 12.23: Extracted day of the week for each row for the InvoiceDate
    column'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.23：为每行提取的 `InvoiceDate` 列的星期几'
- en: '](img/B15019_12_23.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_23.jpg)'
- en: 'Figure 12.23: Extracted day of the week for each row for the InvoiceDate column'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.23：为每行提取的 `InvoiceDate` 列的星期几
- en: Note
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the whole list of available attributes here: [https://packt.live/2ZUe02R](https://packt.live/2ZUe02R).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到所有可用属性的完整列表：[https://packt.live/2ZUe02R](https://packt.live/2ZUe02R)。
- en: 'With datetime columns, you can also perform some mathematical operations. We
    can, for instance, add `3` days to each date by using pandas time-series offset
    object, `pd.tseries.offsets.Day(3)`:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 datetime 列，你还可以进行一些数学运算。例如，我们可以通过使用 pandas 的时间序列偏移对象 `pd.tseries.offsets.Day(3)`，为每个日期添加
    `3` 天：
- en: '[PRE69]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You should get the following output:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '![Figure 12.24: InvoiceDate column offset by three days'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.24：InvoiceDate 列偏移了三天'
- en: '](img/B15019_12_24.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_24.jpg)'
- en: 'Figure 12.24: InvoiceDate column offset by three days'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.24：InvoiceDate 列偏移了三天
- en: 'You can also offset days by business days using `pd.tseries.offsets.BusinessDay()`.
    For instance, if we want to get the previous business days, we do:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 `pd.tseries.offsets.BusinessDay()` 按工作日偏移天数。例如，如果我们想获得前一个工作日，可以这样做：
- en: '[PRE70]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'You should get the following output:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '![Figure 12.25: InvoiceDate column offset by -1 business day'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.25：InvoiceDate 列偏移了 -1 个工作日'
- en: '](img/B15019_12_25.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_25.jpg)'
- en: 'Figure 12.25: InvoiceDate column offset by -1 business day'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.25：InvoiceDate 列偏移了 -1 个工作日
- en: 'Another interesting date manipulation operation is to apply a specific time-frequency
    using `pd.Timedelta()`. For instance, if you want to get the first day of the
    month from a date, you do:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的日期操作是使用 `pd.Timedelta()` 应用特定的时间频率。例如，如果您想从某个日期获取该月的第一天，可以这样做：
- en: '[PRE71]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'You should get the following output:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '![Figure 12.26: InvoiceDate column transformed to the start of the month'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.26：InvoiceDate 列转换为月初'
- en: '](img/B15019_12_26.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_26.jpg)'
- en: 'Figure 12.26: InvoiceDate column transformed to the start of the month'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.26：InvoiceDate 列转换为月初
- en: As you have seen in this section, the `pandas` package provides a lot of different
    APIs for manipulating dates. You have learned how to use a few of the most popular
    ones. You can now explore the other ones on your own.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在本节中所见，`pandas` 包提供了许多不同的 API 来操作日期。您已经学会了如何使用其中一些最常用的 API。现在，您可以自行探索其他的 API。
- en: 'Exercise 12.03: Date Manipulation on Financial Services Consumer Complaints'
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 12.03：金融服务消费者投诉的日期操作
- en: 'In this exercise, we will learn how to extract time-related information from
    two existing date columns using `pandas` in order to create six new columns:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将学习如何使用 `pandas` 从两个现有的日期列中提取与时间相关的信息，并创建六个新列：
- en: Note
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset we will be using in this exercise is the Financial Services Customer
    Complaints dataset and it can be found on our GitHub repository: [https://packt.live/2ZYm9Dp](https://packt.live/2ZYm9Dp).'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本次练习中使用的数据集是金融服务客户投诉数据集，您可以在我们的 GitHub 仓库中找到它：[https://packt.live/2ZYm9Dp](https://packt.live/2ZYm9Dp)。
- en: 'The original dataset can be found here: [https://packt.live/35mFhMw](https://packt.live/35mFhMw).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集可以在这里找到：[https://packt.live/35mFhMw](https://packt.live/35mFhMw)。
- en: Open up a new Colab notebook.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Colab 笔记本。
- en: 'Import the `pandas` package:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 包：
- en: '[PRE72]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Assign the link to the dataset to a variable called `file_url`:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集的链接赋值给一个名为 `file_url` 的变量：
- en: '[PRE73]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Use the `.read_csv()` method from the `pandas` package and load the dataset
    into a new DataFrame called `df`:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pandas` 包中的 `.read_csv()` 方法将数据集加载到一个名为 `df` 的新数据框中：
- en: '[PRE74]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Display the first five rows using the `.head()` method:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.head()` 方法显示前五行：
- en: '[PRE75]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'You should get the following output:'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '![Figure 12.27: First five rows of the Customer Complaint DataFrame'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.27：客户投诉数据框的前五行'
- en: '](img/B15019_12_27.jpg)'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_27.jpg)'
- en: 'Figure 12.27: First five rows of the Customer Complaint DataFrame'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.27：客户投诉数据框的前五行
- en: 'Print out the data types for each column using the `.dtypes` attribute:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.dtypes` 属性打印每一列的数据类型：
- en: '[PRE76]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'You should get the following output:'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '![Figure 12.28: Data types for the Customer Complaint DataFrame'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.28：客户投诉数据框的各列数据类型'
- en: '](img/B15019_12_28.jpg)'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_28.jpg)'
- en: 'Figure 12.28: Data types for the Customer Complaint DataFrame'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.28：客户投诉数据框的各列数据类型
- en: The `Date received` and `Date sent to company` columns haven't been recognized
    as datetime, so we need to manually convert them.
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Date received` 和 `Date sent to company` 列尚未被识别为日期时间格式，因此我们需要手动将它们转换为日期时间。'
- en: 'Convert the `Date received` and `Date sent to company` columns to datetime
    using the `pd.to_datetime()` method:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pd.to_datetime()` 方法将 `Date received` 和 `Date sent to company` 列转换为日期时间格式：
- en: '[PRE77]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Print out the data types for each column using the `.dtypes` attribute:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.dtypes` 属性打印每一列的数据类型：
- en: '[PRE78]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'You should get the following output:'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '![Figure 12.29: Data types for the Customer Complaint DataFrame after conversion'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.29：客户投诉数据框在转换后的数据类型'
- en: '](img/B15019_12_29.jpg)'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_29.jpg)'
- en: 'Figure 12.29: Data types for the Customer Complaint DataFrame after conversion'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.29：客户投诉数据框在转换后的数据类型
- en: Now these two columns have the right data types. Now let's create some new features
    from these two dates.
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在这两列的数据类型已经正确。接下来，让我们从这两列日期中创建一些新特征。
- en: 'Create a new column called `YearReceived`, which will contain the year of each
    date from the `Date Received` column using the `.dt.year` attribute:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`YearReceived`的新列，其中将使用`.dt.year`属性提取`Date Received`列中的年份：
- en: '[PRE79]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Create a new column called `MonthReceived`, which will contain the month of
    each date using the `.dt.month` attribute:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`MonthReceived`的新列，其中将使用`.dt.month`属性提取每个日期的月份：
- en: '[PRE80]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Create a new column called `DayReceived`, which will contain the day of the
    month for each date using the `.dt.day` attribute:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`DayReceived`的新列，其中将使用`.dt.day`属性提取每个日期的天：
- en: '[PRE81]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Create a new column called `DowReceived`, which will contain the day of the
    week for each date using the `.dt.dayofweek` attribute:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`DowReceived`的新列，其中将使用`.dt.dayofweek`属性提取每个日期的星期几：
- en: '[PRE82]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Display the first five rows using the `.head()` method:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.head()`方法显示前五行：
- en: '[PRE83]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'You should get the following output:'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.30: First five rows of the Customer Complaint DataFrame'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图12.30：客户投诉数据框的前五行'
- en: after creating four new features
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建了四个新特征后
- en: '](img/B15019_12_30.jpg)'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_30.jpg)'
- en: 'Figure 12.30: First five rows of the Customer Complaint DataFrame after creating
    four new features'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.30：在创建四个新特征后，客户投诉数据框的前五行
- en: 'We can see we have successfully created four new features: `YearReceived`,
    `MonthReceived`, `DayReceived`, and `DowReceived`. Now let''s create another that
    will indicate whether the date was during a weekend or not.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，我们成功地创建了四个新特征：`YearReceived`、`MonthReceived`、`DayReceived`和`DowReceived`。现在让我们创建另一个特征，指示日期是否在周末。
- en: 'Create a new column called `IsWeekendReceived`, which will contain binary values
    indicating whether the `DowReceived` column is over or equal to `5` (`0` corresponds
    to Monday, `5` and `6` correspond to Saturday and Sunday respectively):'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`IsWeekendReceived`的新列，其中将包含二进制值，指示`DowReceived`列的值是否大于或等于`5`（`0`代表星期一，`5`和`6`分别代表星期六和星期日）：
- en: '[PRE84]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Display the first `5` rows using the `.head()` method:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.head()`方法显示前`5`行：
- en: '[PRE85]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'You should get the following output:'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.31: First five rows of the Customer Complaint DataFrame'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图12.31：客户投诉数据框的前五行'
- en: after creating the weekend feature
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建周末特征后
- en: '](img/B15019_12_31.jpg)'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_31.jpg)'
- en: 'Figure 12.31: First five rows of the Customer Complaint DataFrame after creating
    the weekend feature'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.31：在创建周末特征后，客户投诉数据框的前五行
- en: We have created a new feature stating whether each complaint was received during
    a weekend or not. Now we will feature engineer a new column with the numbers of
    days between `Date sent to company` and `Date received`.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经创建了一个新特征，表明每个投诉是否是在周末收到的。接下来，我们将创建一个新列，表示`Date sent to company`和`Date received`之间的天数差。
- en: 'Create a new column called `RoutingDays`, which will contain the difference
    between `Date sent to company` and `Date received`:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`RoutingDays`的新列，其中将包含`Date sent to company`和`Date received`之间的差值：
- en: '[PRE86]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Print out the data type of the new `''RoutingDays''` column using the `.dtype` attribute:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.dtype`属性打印新列`'RoutingDays'`的数据类型：
- en: '[PRE87]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'You should get the following output:'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.32: Data type of the RoutingDays column'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图12.32：`RoutingDays`列的数据类型'
- en: '](img/B15019_12_32.jpg)'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_32.jpg)'
- en: 'Figure 12.32: Data type of the RoutingDays column'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.32：`RoutingDays`列的数据类型
- en: The result of subtracting two datetime columns is a new datetime column (`dtype('<M8[ns]'`),
    which is a specific datetime type for the `numpy` package). We need to convert
    this data type into an `int` to get the number of days between these two days.
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从两个日期时间列相减的结果是一个新的日期时间列（`dtype('<M8[ns]'`），这是一种特定的日期时间类型，属于`numpy`包）。我们需要将该数据类型转换为`int`，以便计算这两天之间的天数。
- en: 'Transform the `RoutingDays` column using the `.dt.days` attribute:'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.dt.days`属性转换`RoutingDays`列：
- en: '[PRE88]'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Display the first five rows using the `.head()` method:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.head()`方法显示前五行：
- en: '[PRE89]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'You should get the following output:'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.33: First five rows of the Customer Complaint DataFrame after creating
    RoutingDays'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图12.33：在创建`RoutingDays`后，客户投诉数据框的前五行'
- en: '](img/B15019_12_33.jpg)'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_33.jpg)'
- en: 'Figure 12.33: First five rows of the Customer Complaint DataFrame after creating
    RoutingDays'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.33：在创建`RoutingDays`后，客户投诉数据框的前五行
- en: In this exercise, you put into practice different techniques to feature engineer
    new variables from datetime columns on a real-world dataset. From the two `Date
    sent to company` and `Date received` columns, you successfully created six new
    features that will provide additional valuable information.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你将实践不同的技术，从真实世界的数据集中基于日期时间列进行特征工程，生成新的变量。通过 `Date sent to company` 和
    `Date received` 两列，你成功创建了六个新特征，这些特征将提供额外的有价值信息。
- en: Note
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3kWvhmf](https://packt.live/3kWvhmf).
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考 [https://packt.live/3kWvhmf](https://packt.live/3kWvhmf)。
- en: You can also run this example online at [https://packt.live/316YT8z](https://packt.live/316YT8z).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例：[https://packt.live/316YT8z](https://packt.live/316YT8z)。
- en: For instance, we were able to find patterns such as the number of complaints
    tends to be higher in November or on a Friday. We also found that routing the
    complaints takes more time when they are received during the weekend, which may
    be due to the limited number of staff at that time of the week.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们能够发现一些模式，比如投诉数量通常在十一月或星期五较高。我们还发现，当投诉在周末收到时，处理投诉需要更多时间，这可能是由于那时工作人员有限。
- en: Performing Data Aggregation
  id: totrans-439
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行数据聚合
- en: 'Alright. We are getting close to the end of this chapter. But before we wrap
    it up, there is one more technique to explore for creating new features: data
    aggregation. The idea behind it is to summarize a numerical column for specific
    groups from another column. We already saw an example of how to aggregate two
    numerical variables from the ATO dataset (Average net tax and Average total deductions)
    for each cluster found by k-means using the `.pivot_table()` method in *Chapter
    5*, *Performing Your First Cluster Analysis*. But at that time, we aggregated
    the data not to create new features but to understand the difference between these
    clusters.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经接近本章的结尾了。但在结束之前，还有一个要探索的技术：数据聚合。其背后的想法是，通过另一列中的特定分组来总结某个数值列。我们已经看到如何使用
    `.pivot_table()` 方法在 *第5章*《进行第一次聚类分析》中，对 ATO 数据集中的两个数值变量（平均净税和平均总扣除）进行聚合。但那时，我们进行数据聚合并非为了创建新特征，而是为了理解这些聚类之间的差异。
- en: You may wonder to yourself in which cases you would want to perform feature
    engineering using data aggregation. If you already have a numerical column that
    contains a value for each record, why would you need to summarize it and add this
    information back to the DataFrame? It feels like we are just adding the same information
    but with fewer details. But there are actually multiple good reasons for using
    this technique.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会好奇，在什么情况下你会想使用数据聚合进行特征工程。如果你已经有一个数值列，其中包含每个记录的值，为什么还需要对其进行汇总，并将这些信息添加回 DataFrame
    中呢？这似乎是在添加相同的信息，只是没有那么详细。但其实，使用这种技术有多个合理的原因。
- en: One potential reason might be that you want to normalize another numerical column
    using this aggregation. For instance, if you are working on a dataset for a retailer
    that contains all the sales for each store around the world, the volume of sales
    may differ drastically for a country compared to another one as they don't have
    the same population. In this case, rather than using the raw sales figures for
    each store, you would calculate a ratio (or a percentage) of the sales of a store
    divided by the total volume of sales in its country. With this new ratio feature,
    some of the stores that looked as though they were underperforming because their
    raw volume of sales was not as high as for other countries may actually be performing
    much better than the average in its country.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 一个潜在的原因可能是你希望通过这种聚合来规范化另一个数值列。例如，如果你正在处理一个零售商的数据集，包含全球每个门店的所有销售数据，不同国家的销售量可能会有很大差异，因为它们的人口不同。在这种情况下，与其使用每个门店的原始销售数字，你可以计算一个比例（或百分比），即某个门店的销售额除以该国家的总销售额。通过这个新的比例特征，那些看起来因为销售额较低而表现不佳的门店，实际上可能在该国的平均水平上表现更好。
- en: 'In `pandas`, it is quite easy to perform data aggregation. We just need to
    combine the following methods successively: `.groupby()` and `.agg()`.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `pandas` 中，进行数据聚合非常简单。我们只需要依次结合以下方法：`.groupby()` 和 `.agg()`。
- en: We will need to specify the list of columns that will be grouped together to
    the `.groupby()` method. If you are familiar with pivot tables in Excel, this
    corresponds to the `Rows` field.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将需要分组的列名列表提供给 `.groupby()` 方法。如果你熟悉 Excel 透视表，这相当于 `Rows` 字段。
- en: 'The `.agg()` method expects a dictionary with the name of a column as a key
    and the aggregation function as a value such as `{''column_name'': ''aggregation_function''}`.
    In an Excel pivot table, the aggregated column is referred to as `values`.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '`.agg()` 方法期望一个字典，其中列名作为键，聚合函数作为值，例如 `{''column_name'': ''aggregation_function''}`。在
    Excel 透视表中，聚合后的列称为 `values`。'
- en: 'Let''s see how to do it on the Online Retail dataset. First, we need to import
    the data:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在在线零售数据集上实现这个操作。首先，我们需要导入数据：
- en: '[PRE90]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Let''s calculate the total quantity of items sold for each country. We will
    specify the `Country` column as the grouping column:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算每个国家的总销售数量。我们将 `Country` 列作为分组列：
- en: '[PRE91]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'You should get the following output:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.34: Sum of Quantity per Country (truncated)'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.34：每个国家的数量总和（截断）'
- en: '](img/B15019_12_34.jpg)'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_34.jpg)'
- en: 'Figure 12.34: Sum of Quantity per Country (truncated)'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.34：每个国家的数量总和（截断）
- en: 'This result gives the total volume of items sold for each country. We can see
    that Australia has almost sold four times more items than Belgium. This level
    of information may be too high-level and we may want a bit more granular detail.
    Let''s perform the same aggregation but this time we will group on two columns:
    `Country` and `StockCode`. We just need to provide the names of these columns
    as a list to the `.groupby()` method:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果给出了每个国家销售的物品总量。我们可以看到，澳大利亚的销售量几乎是比利时的四倍。这种层级的信息可能太过粗略，我们可能需要更多的细节。让我们进行相同的聚合，但这次我们将根据两列进行分组：`Country`
    和 `StockCode`。我们只需要将这些列的名称作为列表传递给 `.groupby()` 方法：
- en: '[PRE92]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'You should get the following output:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.35: Sum of Quantity per Country and StockCode'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.35：每个国家和 StockCode 的数量总和'
- en: '](img/B15019_12_35.jpg)'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_35.jpg)'
- en: 'Figure 12.35: Sum of Quantity per Country and StockCode'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.35：每个国家和 StockCode 的数量总和
- en: We can see how many items have been sold for each country. We can note that
    Australia has sold the same quantity of products `20675`, `20676`, and `20677`
    (`216` each). This may indicate that these products are always sold together.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个国家的销售数量。可以注意到，澳大利亚对产品 `20675`、`20676` 和 `20677` 销售数量相同（每个 `216` 件）。这可能表示这些产品总是一起销售。
- en: 'We can add one more layer of information and get the number of items sold for
    each country, the product, and the date. To do so, we first need to create a new
    feature that will extract the date component of `InvoiceDate` (we just learned
    how to do this in the previous section):'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以添加更多的信息层级，获取每个国家、每个产品和每个日期的销售数量。为此，我们首先需要创建一个新特征，从 `InvoiceDate` 中提取日期组件（我们在上一节中刚刚学到如何做这件事）：
- en: '[PRE93]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Then, we can add this new column in the `.groupby()` method:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以在 `.groupby()` 方法中添加这个新列：
- en: '[PRE94]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'You should get the following output:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.36: Sum of Quantity per Country, StockCode, and Invoice_Date'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.36：每个国家、StockCode 和 Invoice_Date 的数量总和'
- en: '](img/B15019_12_36.jpg)'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_36.jpg)'
- en: 'Figure 12.36: Sum of Quantity per Country, StockCode, and Invoice_Date'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.36：每个国家、StockCode 和 Invoice_Date 的数量总和
- en: We have generated a new DataFrame with the total quantity of items sold per
    country, item ID, and date. We can see the item with `StockCode 15036` was quite
    popular on `2011-05-17` in `Australia` – there were `600` sold items. On the other
    hand, only `6` items of `Stockcode` `20665` were sold on `2011-03-24` in `Australia`.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经生成了一个新的数据框，其中包含每个国家、物品 ID 和日期的总销售数量。我们可以看到 `StockCode 15036` 在 `2011-05-17`
    在 `Australia` 非常畅销，共售出了 `600` 件。而另一方面，在 `2011-03-24`，`StockCode 20665` 在 `Australia`
    只售出了 `6` 件。
- en: 'We can now merge this additional information back into the original DataFrame.
    But before that, there is an additional data transformation step required: reset
    the column index. The `pandas` package creates a multi-level index after data
    aggregation by default. You can think of it as though the column names were stored
    in multiple rows instead of one only. To change it back to a single level, you
    need to call the `.reset_index()` method:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这些额外的信息合并回原始数据框中。但是在此之前，需要执行一个额外的数据转换步骤：重置列索引。`pandas` 包默认在数据聚合后创建一个多级索引。你可以把它想象成列名被存储在多行中，而不是仅存储在一行中。要将其更改回单级索引，你需要调用
    `.reset_index()` 方法：
- en: '[PRE95]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'You should get the following output:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.37: DataFrame containing data aggregation information'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.37：包含数据汇总信息的DataFrame'
- en: '](img/B15019_12_37.jpg)'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_37.jpg)'
- en: 'Figure 12.37: DataFrame containing data aggregation information'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.37：包含数据汇总信息的DataFrame
- en: 'Now we can merge this new DataFrame into the original one using the `.merge()`
    method we saw earlier in this chapter:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用本章前面提到的`.merge()`方法，将这个新的DataFrame合并到原始的DataFrame中：
- en: '[PRE96]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'You should get the following output:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.38: Merged DataFrame (truncated)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.38：合并后的DataFrame（已截断）'
- en: '](img/B15019_12_38.jpg)'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_38.jpg)'
- en: 'Figure 12.38: Merged DataFrame (truncated)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.38：合并后的DataFrame（已截断）
- en: We can see there are two columns called `Quantity_x` and `Quantity_y` instead
    of `Quantity`.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，原来的`Quantity`列被替换成了`Quantity_x`和`Quantity_y`。
- en: The reason is that, after merging, there were two different columns with the
    exact same name (`Quantity`), so by default, pandas added a suffix to differentiate
    them.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是，在合并后，出现了两个具有完全相同名称的不同列（`Quantity`），因此默认情况下，pandas添加了后缀以区分它们。
- en: 'We can fix this situation either by replacing the name of one of those two
    columns before merging or we can replace both of them after merging. To replace
    column names, we can use the `.rename()` method from `pandas` by providing a dictionary
    with the old name as the key and the new name as the value, such as `{''old_name'':
    ''new_name''}`.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以通过合并前替换其中一列的名称，或者合并后替换两列的名称来解决这个问题。要替换列名，我们可以使用`pandas`的`.rename()`方法，提供一个字典，其中旧名称为键，新名称为值，例如`{''old_name'':
    ''new_name''}`。'
- en: 'Let''s replace the column names after merging with `Quantity` and `DailyQuantity`:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 合并后，我们将列名替换为`Quantity`和`DailyQuantity`：
- en: '[PRE97]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'You should get the following output:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.39: Renamed DataFrame (truncated)'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.39：重命名后的DataFrame（已截断）'
- en: '](img/B15019_12_39.jpg)'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_39.jpg)'
- en: 'Figure 12.39: Renamed DataFrame (truncated)'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.39：重命名后的DataFrame（已截断）
- en: 'Now we can create a new feature that will calculate the ratio between the items
    sold with the daily total quantity of sold items in the corresponding country:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个新特征，用于计算每个国家中售出物品的数量与该国家每日总售出数量之间的比例：
- en: '[PRE98]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'You should get the following output:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.40: Final DataFrame with new QuantityRatio feature'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.40：包含新`QuantityRatio`特征的最终DataFrame'
- en: '](img/B15019_12_40.jpg)'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_40.jpg)'
- en: 'Figure 12.40: Final DataFrame with new QuantityRatio feature'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.40：包含新`QuantityRatio`特征的最终DataFrame
- en: 'In this section, we learned how performing data aggregation can help us to
    create new features by calculating the ratio or percentage for each grouping of
    interest. Looking at the first and second rows, we can see there were `6` items
    sold for `StockCode` transactions `84123A` and `71053`. But if we look at the
    newly created `DailyQuantity` column, we can see that `StockCode` `84123A` is
    more popular: on that day (`2010-12-01`), the store sold `454` units of it but
    only `33` of `StockCode` `71053`. `QuantityRatio` is showing us the third transaction
    sold `8` items of `StockCode` `84406B` and this single transaction accounted for
    20% of the sales of that item on that day. By performing data aggregation, we
    have gained additional information for each record and have put the original information
    from the dataset into perspective.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们学习了如何通过数据汇总，帮助我们通过计算每个感兴趣的分组的比率或百分比来创建新特征。看第一行和第二行，我们可以看到在`StockCode`交易`84123A`和`71053`中，卖出了`6`个商品。但是如果我们看一下新创建的`DailyQuantity`列，我们会发现`StockCode`
    `84123A`更受欢迎：在那一天（`2010-12-01`），该店售出了`454`个`84123A`，但仅售出了`33`个`71053`。`QuantityRatio`显示，第三笔交易卖出了`StockCode`
    `84406B`的`8`个商品，这笔单笔交易占当天该商品销售量的20%。通过进行数据汇总，我们为每一条记录获得了额外的信息，并且将数据集中的原始信息放到了一个新的视角中。
- en: 'Exercise 12.04: Feature Engineering Using Data Aggregation on the AMES Housing
    Dataset'
  id: totrans-498
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习12.04：使用数据汇总进行特征工程，基于AMES房价数据集
- en: 'In this exercise, we will create new features using data aggregation. First,
    we''ll calculate the maximum `SalePrice` and `LotArea` for each neighborhood and
    by `YrSold`. Then, we will add this information back to the dataset, and finally,
    we will calculate the ratio of each property sold with these two maximum values:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将通过数据汇总创建新的特征。首先，我们将计算每个街区的最大`SalePrice`和`LotArea`，并按`YrSold`分组。然后，我们将这些信息添加回数据集，最后，我们将计算每个房产销售量与这两个最大值的比率：
- en: Note
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset we will be using in this exercise is the Ames Housing dataset and
    it can be found in our GitHub repository: [https://packt.live/35r2ahN](https://packt.live/35r2ahN).'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习中我们将使用的数据库是Ames Housing数据库，可以在我们的GitHub仓库中找到：[https://packt.live/35r2ahN](https://packt.live/35r2ahN)。
- en: Open up a new Colab notebook.
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Colab笔记本。
- en: 'Import the `pandas` and `altair` packages:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`和`altair`包：
- en: '[PRE99]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Assign the link to the dataset to a variable called `file_url`:'
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集的链接赋值给一个名为`file_url`的变量：
- en: '[PRE100]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Using the `.read_csv()` method from the `pandas` package, load the dataset
    into a new DataFrame called `df`:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`包的`.read_csv()`方法，将数据集加载到一个名为`df`的新数据框中：
- en: '[PRE101]'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Perform data aggregation to find the maximum `SalePrice` for each `Neighborhood`
    and the `YrSold` using the `.groupby.agg()` method and save the results in a new
    DataFrame called `df_agg`:'
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.groupby.agg()`方法进行数据聚合，找出每个`Neighborhood`和`YrSold`的最大`SalePrice`，并将结果保存到一个名为`df_agg`的新数据框中：
- en: '[PRE102]'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Rename the `df_agg` columns to `Neighborhood`, `YrSold`, and `SalePriceMax`:'
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`df_agg`的列名重命名为`Neighborhood`、`YrSold`和`SalePriceMax`：
- en: '[PRE103]'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Print out the first five rows of `df_agg`:'
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出`df_agg`的前五行：
- en: '[PRE104]'
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'You should get the following output:'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.41: First five rows of the aggregated DataFrame'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.41：聚合后的数据框前五行'
- en: '](img/B15019_12_41.jpg)'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_41.jpg)'
- en: 'Figure 12.41: First five rows of the aggregated DataFrame'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.41：聚合后的数据框前五行
- en: 'Merge the original DataFrame, `df`, to `df_agg` using a left join (`how=''left''`)
    on the `Neighborhood` and `YrSold` columns using the `merge()` method and save
    the results into a new DataFrame called `df_new`:'
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`merge()`方法，通过对`Neighborhood`和`YrSold`列进行左连接（`how='left'`），将原始数据框`df`与`df_agg`合并，并将结果保存到一个名为`df_new`的新数据框中：
- en: '[PRE105]'
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Print out the first five rows of `df_new`:'
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出`df_new`的前五行：
- en: '[PRE106]'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'You should get the following output:'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.42: First five rows of df_new'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.42：`df_new`的前五行'
- en: '](img/B15019_12_42.jpg)'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_42.jpg)'
- en: 'Figure 12.42: First five rows of df_new'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.42：`df_new`的前五行
- en: Note that we are displaying the last eight columns of the output.
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们显示的是输出的最后八列。
- en: 'Create a new column called `SalePriceRatio` by dividing `SalePrice` by `SalePriceMax`:'
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`SalePriceRatio`的新列，将`SalePrice`除以`SalePriceMax`：
- en: '[PRE107]'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Print out the first five rows of `df_new`:'
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出`df_new`的前五行：
- en: '[PRE108]'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'You should get the following output:'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.43: First five rows of df_new after feature engineering'
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.43：特征工程后`df_new`的前五行'
- en: '](img/B15019_12_43.jpg)'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_43.jpg)'
- en: 'Figure 12.43: First five rows of df_new after feature engineering'
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.43：特征工程后`df_new`的前五行
- en: Note that we are displaying the last eight columns of the output.
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们显示的是输出的最后八列。
- en: 'Perform data aggregation to find the maximum `LotArea` for each `Neighborhood`
    and `YrSold` using the `.groupby.agg()` method and save the results in a new DataFrame
    called `df_agg2`:'
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.groupby.agg()`方法进行数据聚合，找出每个`Neighborhood`和`YrSold`的最大`LotArea`，并将结果保存到一个名为`df_agg2`的新数据框中：
- en: '[PRE109]'
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Rename the column of `df_agg2` to `Neighborhood`, `YrSold`, and `LotAreaMax`
    and print out the first five columns:'
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`df_agg2`的列名重命名为`Neighborhood`、`YrSold`和`LotAreaMax`，并打印出前五列：
- en: '[PRE110]'
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'You should get the following output:'
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.44: First five rows of the aggregated DataFrame'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.44：聚合后的数据框前五行'
- en: '](img/B15019_12_44.jpg)'
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_44.jpg)'
- en: 'Figure 12.44: First five rows of the aggregated DataFrame'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.44：聚合后的数据框前五行
- en: 'Merge the original DataFrame, `df`, to `df_agg2` using a left join (`how=''left''`)
    on the `Neighborhood` and `YrSold` columns using the `merge()` method and save
    the results into a new DataFrame called `df_final`:'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`merge()`方法，通过对`Neighborhood`和`YrSold`列进行左连接（`how='left'`），将原始数据框`df`与`df_agg2`合并，并将结果保存到一个名为`df_final`的新数据框中：
- en: '[PRE111]'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Create a new column called `LotAreaRatio` by dividing `LotArea` by `LotAreaMax`:'
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`LotAreaRatio`的新列，将`LotArea`除以`LotAreaMax`：
- en: '[PRE112]'
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Print out the first five rows of `df_final` for the following columns: `Id`,
    `Neighborhood`, `YrSold`, `SalePrice`, `SalePriceMax`, `SalePriceRatio`, `LotArea`,
    `LotAreaMax`, `LotAreaRatio`:'
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出`df_final`的前五行，包含以下列：`Id`、`Neighborhood`、`YrSold`、`SalePrice`、`SalePriceMax`、`SalePriceRatio`、`LotArea`、`LotAreaMax`、`LotAreaRatio`：
- en: '[PRE113]'
  id: totrans-550
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'You should get the following output:'
  id: totrans-551
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 12.45: First five rows of the final DataFrame'
  id: totrans-552
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.45：最终数据框的前五行'
- en: '](img/B15019_12_45.jpg)'
  id: totrans-553
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_12_45.jpg)'
- en: 'Figure 12.45: First five rows of the final DataFrame'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.45：最终数据框的前五行
- en: Note
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Q8a5eU](https://packt.live/2Q8a5eU).
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考 [https://packt.live/2Q8a5eU](https://packt.live/2Q8a5eU)。
- en: You can also run this example online at [https://packt.live/2Q8dhXI](https://packt.live/2Q8dhXI).
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在在线环境中运行此示例，网址为 [https://packt.live/2Q8dhXI](https://packt.live/2Q8dhXI)。
- en: This is it. We just created two new features that give the ratio of `SalePrice`
    and `LotArea` for a property compared to the highest one that was sold in the
    same year and the same neighborhood. We can now easily and fairly compare the
    properties. For instance, from the output of the last step, we can note that the
    fifth property size (`Id` `5` and `LotArea` `14260`) was almost as close (`LotAreaRatio`
    `0.996994`) as the biggest property sold (`LotArea` `14303`) in the same area
    and the same year. But its sale price (`SalePrice` `250000`) was significantly
    lower (`SalePriceRatio` is `0.714286`) than the highest one (`SalePrice` `350000`).
    This indicates that other features of the property had an impact on the sale price.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们刚刚创建了两个新特征，分别表示一个房产的 `SalePrice` 与 `LotArea` 与同一年同一社区出售的最大房产的比率。现在我们可以轻松且公正地比较这些房产。例如，从上一步的输出中我们可以看到，第五个房产的大小（`Id`
    `5` 和 `LotArea` `14260`）几乎与同一地区、同一年售出的最大房产（`LotArea` `14303`）一样接近（`LotAreaRatio`
    `0.996994`）。但它的售价（`SalePrice` `250000`）显著低于（`SalePriceRatio` 为 `0.714286`）最高售价（`SalePrice`
    `350000`）。这表明房产的其他特征对售价产生了影响。
- en: 'Activity 12.01: Feature Engineering on a Financial Dataset'
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 12.01：财务数据集的特征工程
- en: You are working for a major bank in the Czech Republic and you have been tasked
    to analyze the transactions of existing customers. The data team has extracted
    all the tables from their database they think will be useful for you to analyze
    the dataset. You will need to consolidate the data from those tables into a single
    DataFrame and create new features in order to get an enriched dataset from which
    you will be able to perform an in-depth analysis of customers' banking transactions.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 你在捷克共和国的一家大银行工作，负责分析现有客户的交易数据。数据团队已经从他们的数据库中提取了所有认为对你分析数据集有用的表格。你需要将这些表格中的数据合并成一个单一的
    DataFrame，并创建新特征，以获得一个丰富的数据集，从中你可以进行深入的客户银行交易分析。
- en: 'You will be using only the following four tables:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 你将只使用以下四个表格：
- en: '`account`: The characteristics of a customer''s bank account for a given branch'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`account`: 给定分行的客户银行账户特征'
- en: '`client`: Personal information related to the bank''s customers'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`client`: 与银行客户相关的个人信息'
- en: '`disp`: A table that links an account to a customer'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disp`: 将账户与客户关联的表格'
- en: '`trans`: A list of all historical transactions by account'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trans`: 记录每个账户的所有历史交易'
- en: Note
  id: totrans-566
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you want to know more about these tables, you can look at the data dictionary
    for this dataset: [https://packt.live/2QSev9F](https://packt.live/2QSev9F).'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想了解这些表格的更多信息，可以查看该数据集的数据字典：[https://packt.live/2QSev9F](https://packt.live/2QSev9F)。
- en: 'The following steps will help you complete this activity:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成此任务：
- en: Download and load the different tables from this dataset into Python.
  id: totrans-569
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并将此数据集中的不同表格加载到 Python 中。
- en: Analyze each table with the `.shape` and `.head()` methods.
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.shape` 和 `.head()` 方法分析每个表格。
- en: Find the common/similar column(s) between tables that will be used for merging
    based on the analysis from *Step 2*.
  id: totrans-571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据*步骤 2*的分析，找到用于合并的表格之间的共同/相似列。
- en: There should be four common tables. Merge the four tables together using `pd.merge()`.
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该有四个公共表格。使用 `pd.merge()` 将四个表格合并在一起。
- en: Rename the column names after merging with `.rename()`.
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.rename()` 在合并后重命名列名。
- en: Check there is no duplication after merging with `.duplicated()` and `.sum()`.
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并后，使用 `.duplicated()` 和 `.sum()` 检查是否存在重复项。
- en: Transform the data type for date columns using `.to_datetime()`.
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.to_datetime()` 转换日期列的数据类型。
- en: Create two separate features from `birth_number` to get the date of birth and
    sex for each customer.
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `birth_number` 中创建两个单独的特征，以获取每个客户的出生日期和性别。
- en: Note
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'This is the rule used for coding the data related to birthday and sex in this
    column: the number is in the YYMMDD format for men, the number is in the YYMM+50DD
    format for women, where YYMMDD is the date of birth.'
  id: totrans-578
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本列中与生日和性别相关的数据编码规则如下：男性的号码为 YYMMDD 格式，女性的号码为 YYMM+50DD 格式，其中 YYMMDD 表示出生日期。
- en: Fix data quality issues with `.isna()`.
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.isna()` 修复数据质量问题。
- en: 'Create a new feature that will calculate customers'' ages when they opened
    an account using date operations:'
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新特征，使用日期操作计算客户开户时的年龄：
- en: Note
  id: totrans-581
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset was originally shared by Berka, Petr for the Discovery Challenge
    PKDD''99: [https://packt.live/2ZVaG7J](https://packt.live/2ZVaG7J).'
  id: totrans-582
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集最初由Berka, Petr在Discovery Challenge PKDD'99中共享：[https://packt.live/2ZVaG7J](https://packt.live/2ZVaG7J).
- en: 'The datasets you will be using in this activity can be found on our GitHub
    repository:'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将在这个活动中使用的数据集可以在我们的GitHub仓库找到：
- en: '[https://packt.live/2QpUOXC](https://packt.live/2QpUOXC).'
  id: totrans-584
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://packt.live/2QpUOXC](https://packt.live/2QpUOXC).'
- en: '[https://packt.live/36sN2BR](https://packt.live/36sN2BR).'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://packt.live/36sN2BR](https://packt.live/36sN2BR).'
- en: '[https://packt.live/2MZLzLB](https://packt.live/2MZLzLB).'
  id: totrans-586
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://packt.live/2MZLzLB](https://packt.live/2MZLzLB).'
- en: '[https://packt.live/2rW9hkE](https://packt.live/2rW9hkE).'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://packt.live/2rW9hkE](https://packt.live/2rW9hkE).'
- en: 'The CSV version can be found here: [https://packt.live/2N150nn](https://packt.live/2N150nn).'
  id: totrans-588
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CSV版本可以在这里找到：[https://packt.live/2N150nn](https://packt.live/2N150nn).
- en: '**Expected output:**'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 12.46: Expected output with the merged rows'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 12.46: 期望输出与合并行'
- en: '](img/B15019_12_46.jpg)'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_12_46.jpg)'
- en: 'Figure 12.46: Expected output with the merged rows'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.46：期望输出与合并行
- en: Note
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The solution to this activity can be found at the following address: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在以下地址找到：[https://packt.live/2GbJloz](https://packt.live/2GbJloz).
- en: Summary
  id: totrans-595
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概要
- en: We first learned how to analyze a dataset and get a very good understanding
    of its data using data summarization and data visualization. This is very useful
    for finding out what the limitations of a dataset are and identifying data quality
    issues. We saw how to handle and fix some of the most frequent issues (duplicate
    rows, type conversion, value replacement, and missing values) using `pandas`'
    APIs.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先学习了如何分析数据集并通过数据汇总和数据可视化对其数据进行深入理解。这对于发现数据集的限制和识别数据质量问题非常有用。我们看到如何使用`pandas`的API来处理和修复一些最常见的问题（重复行、类型转换、值替换和缺失值）。
- en: Finally, we went through several feature engineering techniques. It was not
    possible to cover all the existing techniques for creating features. The objective
    of this chapter was to introduce you to critical steps that can significantly
    improve the quality of your analysis and the performance of your model. But remember
    to regularly get in touch with either the business or the data engineering team
    to get confirmation before transforming data too drastically. Preparing a dataset
    does not always mean having the cleanest dataset possible but rather getting the
    one that is closest to the true information the business is interested in. Otherwise,
    you may find incorrect or meaningless patterns. As we say, *with great power comes
    great responsibility*.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了几种特征工程技术。不可能涵盖所有用于创建特征的现有技术。本章的目标是介绍可以显著提高分析质量和模型性能的关键步骤。但是请记住，在数据转换过于激进之前，务必定期与业务或数据工程团队联系以确认。准备数据集并不总是意味着拥有最干净的数据集，而是获得最接近业务感兴趣的真实信息的数据集。否则，您可能会发现不正确或无意义的模式。正如我们所说，*伴随着巨大的权力而来的是巨大的责任*。
- en: The next chapter opens a new part of this book that presents data science use
    cases end to end. *Chapter 13*, *Imbalanced Datasets*, will walk you through an
    example of an imbalanced dataset and how to deal with such a situation.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将开启本书呈现数据科学应用案例的新篇章。*第13章*，*不平衡数据集*，将通过一个不平衡数据集的示例来引导你如何处理这种情况。
