- en: Storm Scheduler
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm调度程序
- en: In the previous chapters, we covered the basics of Storm, the installation of
    Storm, the development and deployment of Storm, and the Trident topology in Storm
    clusters. In this chapter, we are focusing on Storm schedulers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们介绍了Storm的基础知识，Storm的安装，Storm的开发和部署，以及Storm集群中的Trident拓扑。在本章中，我们将专注于Storm调度程序。
- en: 'In this chapter, we are going to cover the following points:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下要点：
- en: Introduction to Storm schedulers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm调度程序介绍
- en: Default scheduler
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认调度程序
- en: Isolation scheduler
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隔离调度程序
- en: Resource-aware scheduler
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源感知调度程序
- en: Customer-aware scheduler
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户感知调度程序
- en: Introduction to Storm scheduler
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm调度程序介绍
- en: As mentioned in the first two chapters, the Nimbus is responsible for deploying
    the topology and the supervisor is responsible for performing the computation
    tasks as defined in a Storm topology's spouts and bolts components. As we have
    shown, we can configure the number of worker slots for each supervisor node that
    are assigned to a topology as per the scheduler policy, as well as the number
    of workers allocated to a topology. In short, Storm schedulers help the Nimbus
    to decide the worker distribution of any given topology.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如前两章所述，Nimbus负责部署拓扑，监督者负责执行Storm拓扑的spouts和bolts组件中定义的计算任务。正如我们所展示的，我们可以根据调度程序策略为每个监督者节点配置分配给拓扑的工作插槽数量，以及为拓扑分配的工作节点数量。简而言之，Storm调度程序帮助Nimbus决定任何给定拓扑的工作分配。
- en: Default scheduler
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 默认调度程序
- en: The Storm default scheduler assigns component executors as evenly as possible
    between all the workers (supervisor slots) assigned to a given topology.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Storm默认调度程序在给定拓扑分配的所有工作节点（监督者插槽）之间尽可能均匀地分配组件执行器。
- en: 'Let''s consider a sample topology with one spout and one bolt, with both components
    having two executors. The following diagram shows the assignment of executors
    if we have submitted the topology by allocating two workers (supervisor slots):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个包含一个spout和一个bolt的示例拓扑，两个组件都有两个执行器。如果我们通过分配两个工作节点（监督者插槽）提交了拓扑，下图显示了执行器的分配：
- en: '![](img/00041.gif)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00041.gif)'
- en: As shown in the preceding diagram, each worker node contains one executor for
    a spout and one executor for a bolt. The even distribution of executors between
    workers is only possible if the number of executors in each component is divisible
    by the number of workers assigned to a topology.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，每个工作节点包含一个spout的执行器和一个bolt的执行器。只有当每个组件中的执行器数量可以被分配给拓扑的工作节点数量整除时，才能在工作节点之间均匀分配执行器。
- en: Isolation scheduler
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隔离调度程序
- en: The isolation scheduler provides a mechanism for the easy and safe sharing of
    Storm cluster resources among many topologies. The isolation scheduler helps to
    allocate/reserve the dedicated sets of Storm nodes for topologies within the Storm
    cluster.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 隔离调度程序提供了一种在许多拓扑之间轻松安全地共享Storm集群资源的机制。隔离调度程序有助于在Storm集群中为拓扑分配/保留专用的Storm节点集。
- en: 'We need to define the following property in the Nimbus configuration file to
    switch to the isolation scheduler:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在Nimbus配置文件中定义以下属性以切换到隔离调度程序：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can allocate/reserve the resources for any topology by specifying the topology
    name and the number of nodes inside the `isolation.scheduler.machines` property,
    as mentioned in the following section. We need to define the `isolation.scheduler.machines`
    property in the Nimbus configuration, as Nimbus is responsible for the distribution
    of topology workers between Storm nodes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在`isolation.scheduler.machines`属性中指定拓扑名称和节点数量来为任何拓扑分配/保留资源，如下一节所述。我们需要在Nimbus配置中定义`isolation.scheduler.machines`属性，因为Nimbus负责在Storm节点之间分配拓扑工作节点：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding configuration, two nodes are assigned to `Topology-Test1`,
    one node is assigned to `Topology-Test2`, and four nodes are assigned to `Topology-Test3`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述配置中，`Topology-Test1`分配了两个节点，`Topology-Test2`分配了一个节点，`Topology-Test3`分配了四个节点。
- en: 'Here are the key points of the isolation scheduler:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是隔离调度程序的关键要点：
- en: The topologies mentioned in the isolation list are given priority over non-isolation
    topologies, which means that resources will be allocated to isolation topologies
    first if there's competition with non-isolation topologies
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隔离列表中提到的拓扑优先于非隔离拓扑，这意味着如果与非隔离拓扑竞争，资源将首先分配给隔离拓扑
- en: There is no way to change the isolation setting of topologies during runtime
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行时没有办法更改拓扑的隔离设置。
- en: The isolation topology solves the multitenancy problem by assigning dedicated
    machines to topologies
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隔离调度程序通过为拓扑分配专用机器来解决多租户问题
- en: Resource-aware scheduler
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源感知调度程序
- en: 'A resource-aware scheduler helps users specify the amount of resources required
    for a single instance of any component (spout or bolt). We can enable the resource-aware
    scheduler by specifying the following property in the `storm.yaml` file:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 资源感知调度程序帮助用户指定单个组件实例（spout或bolt）所需的资源量。我们可以通过在`storm.yaml`文件中指定以下属性来启用资源感知调度程序：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Component-level configuration
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组件级配置
- en: 'You can allocate the memory requirement to any component. Here are the methods
    available to allocate the memory to a single instance of any component:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为任何组件分配内存需求。以下是可用于为任何组件的单个实例分配内存的方法：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Alternatively, you can use the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用以下方法：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following is the definition of each argument:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每个参数的定义：
- en: '`onHeap`: The amount of on heap space an instance of this component will consume
    in megabytes'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onHeap`：此组件实例将消耗的堆内存空间量（以兆字节为单位）'
- en: '`offHeap`: The amount of off heap space an instance of this component will
    consume in megabytes'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offHeap`：此组件实例将消耗的堆外内存空间量（以兆字节为单位）'
- en: The data type of both `onHeap` and `offHeap` is `Number`, and the default value
    is `0.0`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`onHeap`和`offHeap`的数据类型均为`Number`，默认值为`0.0`。'
- en: Memory usage example
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存使用示例
- en: 'Let''s consider a topology that has two components--one spout and one bolt:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个具有两个组件（一个spout和一个bolt）的拓扑：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The memory request for a single instance of the `spout1` component is 1.5 GB
    (1 GB on heap and 0.5 GB off heap), which means that the total memory request
    for the `spout1` component is 4 x 1.5 GB = 6 GB.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`spout1`组件的单个实例的内存请求为1.5 GB（堆上1 GB，堆外0.5 GB），这意味着`spout1`组件的总内存请求为4 x 1.5 GB
    = 6 GB。'
- en: 'The memory request for a single instance of the `bolt1` component is 0.5 GB
    (0.5 GB on heap and 0.0 GB off heap), which means that the total memory request
    for the `bolt1` component is 5 x 0.5 GB = 2.5 GB. The method of calculating the
    total memory required for both components can be summarized as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`bolt1`组件的单个实例的内存请求为0.5 GB（堆上0.5 GB，堆外0.0 GB），这意味着`bolt1`组件的总内存请求为5 x 0.5 GB
    = 2.5 GB。计算两个组件所需的总内存的方法可以总结如下：'
- en: '*Total memory allocated to topology = spout1 + bolt1 = 6 + 2.5 = 8.5 GB*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*拓扑分配的总内存= spout1 + bolt1 = 6 + 2.5 = 8.5 GB*'
- en: You can also allocate the CPU requirement to any component.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将CPU需求分配给任何组件。
- en: 'Here is the method required to allocate the amount of CPU resources to a single
    instance of any component:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为任何给定组件的单个实例分配CPU资源量所需的方法：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `amount` is the amount of CPU resources an instance of any given component
    will consume. CPU usage is a difficult concept to define. Different CPU architectures
    perform differently depending on the task at hand. By convention, a CPU core will
    typically have 100 points. If you feel that your processors are more or less powerful,
    you can adjust this accordingly. Heavy tasks that are CPU-bound will get 100 points,
    as they can consume an entire core. Medium tasks should get 50, light tasks 25,
    and tiny tasks 10.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`amount`是任何给定组件实例将消耗的CPU资源量。 CPU使用是一个难以定义的概念。不同的CPU架构根据手头的任务而表现不同。按照惯例，CPU核心通常有100个点。如果您觉得您的处理器更强大或更弱，可以相应地进行调整。CPU密集型的重型任务将获得100分，因为它们可以占用整个核心。中等任务应该获得50分，轻型任务25分，微小任务10分。'
- en: CPU  usage example
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU使用示例
- en: 'Let''s consider a topology that has two components--one spout and one bolt:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个具有两个组件（一个spout和一个bolt）的拓扑：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Worker-level configuration
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作节点级配置
- en: 'You can allocate the heap size per worker/slot. Here is the method required
    to define the heap size of each worker node:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为每个工作节点/插槽分配堆大小。以下是定义每个工作节点的堆大小所需的方法：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, `size` is the amount of heap space available to a single worker in megabytes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`size`是以兆字节为单位的单个工作节点可用的堆空间量。
- en: 'Here is an example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Node-level configuration
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点级配置
- en: 'We can configure the amount of memory and CPU a Storm node can use by setting
    the following properties in the `storm.yaml` file. We need to set the following
    properties on each Storm node:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在`storm.yaml`文件中设置以下属性来配置Storm节点可以使用的内存和CPU量。我们需要在每个Storm节点上设置以下属性：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is an example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, `100` means an entire core, as discussed previously.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`100`表示整个核心，如前面讨论的。
- en: Global component configuration
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全局组件配置
- en: 'As mentioned in the previous section, we can define the memory and CPU requirements
    for each component by defining the topology. The user can also set the default
    resource usage of components in the `storm.yaml` file. If we are defining the
    component configuration in the code, then the code value will overwrite the default
    one:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，我们可以通过定义拓扑来为每个组件定义内存和CPU需求。用户还可以在`storm.yaml`文件中设置组件的默认资源使用情况。如果我们在代码中定义组件配置，那么代码值将覆盖默认值：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Custom scheduler
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义调度程序
- en: In Storm, Nimbus uses a scheduler to assign tasks to the supervisors. The default
    scheduler aims to allocate computing resources evenly to topologies. It works
    well in terms of fairness among topologies, but it is impossible for users to
    predict the placement of topology components in the Storm cluster, regarding which
    component of a topology needs to be assigned to which supervisor node.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在Storm中，Nimbus使用调度程序将任务分配给监督者。默认调度程序旨在将计算资源均匀分配给拓扑。在拓扑之间公平性方面表现良好，但用户无法预测Storm集群中拓扑组件的放置，即拓扑的哪个组件需要分配给哪个监督者节点。
- en: 'Let''s consider an example. Say that we have a topology that has one spout
    and two bolts, and each of the components has one executor and one task. The following
    diagram shows the distribution of the topology if we submit the topology to a
    Storm cluster. Assume that the number of workers assigned to the topology is three
    and the number of supervisors in the Storm cluster is three:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子。假设我们有一个具有一个spout和两个bolts的拓扑，每个组件都有一个执行器和一个任务。如果我们将拓扑提交到Storm集群，则以下图表显示了拓扑的分布。假设分配给拓扑的工作节点数量为三，Storm集群中的监督者数量为三：
- en: '![](img/00042.gif)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00042.gif)'
- en: 'Let''s assume that the last bolt in our topology, **Bolt2**, needs to process
    some data using a GPU rather than the CPU, and there''s only one of the supervisors
    with a GPU. We need to write our own custom scheduler to achieve the assignment
    of any component to a specific supervisor node. Here are the steps we need to
    perform to achieve this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的拓扑中的最后一个bolt **Bolt2** 需要使用GPU而不是CPU来处理一些数据，并且只有一个监督者具有GPU。我们需要编写自己的自定义调度程序来实现将任何组件分配给特定监督者节点的任务。以下是我们需要执行的步骤：
- en: Configure changes in the supervisor node.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置监督者节点中的更改。
- en: Configure settings at the component level.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在组件级别配置设置。
- en: Write a custom scheduler class.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写自定义调度程序类。
- en: Register the customer scheduler class.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册自定义调度程序类。
- en: Configuration changes in the supervisor node
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置监督者节点中的更改
- en: 'Storm offers a field in the supervisor''s configuration for users to specify
    custom scheduling metadata. In this case, we type `/tag` in the supervisors, along
    with the type they are running on, which we do with a single line of config in
    their `$STORM_HOME/conf/storm.yaml` file. For example, each supervisor node should
    have the following in its config:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Storm在监督节点的配置中为用户提供了一个字段，用于指定自定义调度元数据。在这种情况下，我们在监督节点中输入`/tag`和它们运行的类型，这是通过在它们的`$STORM_HOME/conf/storm.yaml`文件中的一行配置完成的。例如，每个监督节点的配置应该包含以下内容：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We need to restart the supervisor node after adding the configuration changes
    to each supervisor node. You need to use the CPU type for all non-GPU machines.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在对每个监督节点添加配置更改后，我们需要重新启动监督节点。对于所有非GPU机器，您需要使用CPU类型。
- en: Configuration setting at component level
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组件级别的配置设置
- en: 'This step is done when building the topology with `TopologyBuilder` in the
    main method of a topology. `ComponentConfigurationDeclarer` has a method called
    `addConfiguration(String config, String value)` that allows adding of custom configuration--that
    is, metadata. In our case, we add the type information using this method:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步是在拓扑结构中使用`TopologyBuilder`的主方法中完成的。`ComponentConfigurationDeclarer`有一个叫做`addConfiguration(String
    config, String value)`的方法，允许添加自定义配置，也就是元数据。在我们的情况下，我们使用这个方法添加类型信息：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding code shows that we have typed our `bolt2` component with `type`
    as `GPU`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码显示我们已经用`type`为`GPU`对`bolt2`组件进行了类型化。
- en: Writing a custom supervisor class
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写自定义监督类
- en: 'We can write our `CustomScheduler` class by implementing the `org.apache.storm.scheduler.IScheduler`
    interface. The interface contains two important methods:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过实现`org.apache.storm.scheduler.IScheduler`接口来编写我们的`CustomScheduler`类。这个接口包含两个重要的方法：
- en: '`prepare(Map conf)`: This method only initializes the scheduler'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prepare(Map conf)`：这个方法只是初始化调度程序。'
- en: '`schedule(Topologies topologies, Cluster cluster)`: This method contains logic
    that is responsible for topology workers in the cluster supervisor slots'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schedule(Topologies topologies, Cluster cluster)`：这个方法包含负责在集群监督节点插槽中进行拓扑工作的逻辑。'
- en: '`CustomScheduler` contains the following private method, which is responsible
    for assigning workers to the cluster supervisor slots.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`CustomScheduler`包含以下私有方法，负责将工作程序分配给集群监督节点的插槽。'
- en: 'The `getSupervisorsByType()` method returns the map. The key of the map represents
    the node type (for example, CPU or GPU) and the value contains the list of supervisor
    nodes of that type:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`getSupervisorsByType()`方法返回映射。映射的键表示节点类型（例如，CPU或GPU），值包含该类型监督节点的列表：'
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `populateComponentsByType()` method also returns the map. The key of the
    map represents the type (CPU or GPU) and the value contains the list of components
    of the topology that needs to be assigned to that type of supervisor node. We
    use an untyped type here to group components with no types. The purpose of this
    is to effectively handle these untyped components in the same way that the default
    scheduler performs its assigning. That means that a topology with no typed components
    will be successfully scheduled in the same way, with no issues, across untyped
    supervisors:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`populateComponentsByType()`方法也返回映射。映射的键表示类型（CPU或GPU），值包含需要分配给该类型监督节点的拓扑组件的列表。我们在这里使用一个无类型的类型来将没有类型的组件分组。这样做的目的是有效地处理这些无类型的组件，就像默认调度程序执行分配一样。这意味着没有类型组件的拓扑将以相同的方式成功调度，跨无类型的监督节点没有问题：'
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `populateComponentsByTypeWithStormInternals()` method returns the details
    of the internal components that Storm launches to a topology''s data flow:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`populateComponentsByTypeWithStormInternals()`方法返回Storm启动的内部组件的详细信息。'
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The first three methods manage the maps of the supervisors and components.
    Now, we will write the `typeAwareScheduler()` method, which will use both the
    maps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 前三种方法管理监督和组件的映射。现在，我们将编写`typeAwareScheduler()`方法，它将使用这两个映射：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Apart from the preceding four methods, we are also using more methods that do
    the following things.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的四种方法，我们还使用了更多的方法来执行以下操作。
- en: Converting component IDs to executors
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将组件ID转换为执行程序
- en: Now let's make the jump from component IDs to actual executors, as that's the
    level at which the Storm cluster deals with assignments.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从组件ID跳转到实际的执行程序，因为这是Storm集群处理分配的级别。
- en: 'The process is quite straightforward:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程非常简单：
- en: Get a map of executors by component from the cluster
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从集群获取按组件的执行程序的映射
- en: Check which components' executors need scheduling, according to the cluster
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据集群检查哪些组件的执行程序需要调度
- en: 'Create a map of the types to the executors, populating only those executors
    that are awaiting scheduling:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建类型到执行程序的映射，只填充等待调度的执行程序：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Converting supervisors to slots
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将监督转换为插槽
- en: 'And now for the final conversion we have to perform: jumping from supervisors
    down to slots. As before with components and their executors, we need this because
    the cluster assigns executors at the slot level, not the supervisor level.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是我们必须执行的最终转换：从监督到插槽的跳转。与组件及其执行程序一样，我们需要这个，因为集群在插槽级别分配执行程序，而不是监督级别。
- en: 'There are a few things to do at this point; we have broken the process down
    into smaller methods to preserve readability. The main steps we need to perform
    are as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上有一些事情要做；我们已经将这个过程分解成更小的方法来保持可读性。我们需要执行的主要步骤如下：
- en: Find out which slots we can assign to, given a list of supervisors for a type.
    This is simply the case of using a for loop that collects all supervisors' slots,
    and then returning as many of the slots as are requested by the topology.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 找出我们可以分配的插槽，给定一个类型的监督节点列表。这只是使用一个for循环收集所有监督节点的插槽，然后返回拓扑所请求的插槽数量。
- en: Divide the executors awaiting scheduling for the type into even groups across
    the slots.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 将等待调度的类型的执行程序分成均匀的组。
- en: Populate a map with entries in the slot to the executors.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 用条目填充插槽到执行程序的映射。
- en: The idea here is to call the `populateComponentExecutorsToSlotsMap` method once
    per type, which results in a single map holding all the assignments we need to
    perform.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是每种类型调用`populateComponentExecutorsToSlotsMap`方法一次，这将导致一个包含我们需要执行的所有分配的单个映射。
- en: 'As explained in the code''s comments, we previously found that sometimes we
    would eagerly assign a type''s executors to a slot, only to have a successive
    type fail to assign its executors, leading to partial scheduling. We have since
    made sure that the flow of scheduling ensures that no partial scheduling is ever
    performed (either all are scheduled or none are), at the cost of an extra for
    loop, as we believe that''s a cleaner state for a topology to be in:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码注释中所解释的，我们先前发现有时我们会急切地将类型的执行者分配给一个插槽，只是为了让后续的类型无法分配其执行者，导致部分调度。我们已经确保调度流程确保不会执行部分调度（要么全部被调度，要么全部不被调度），尽管这会增加一个额外的循环，但我们认为这是拓扑结构的更清洁状态：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Registering a CustomScheduler class
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注册一个CustomScheduler类
- en: 'We need to create a JAR for the `CustomScheduler` class, and place it in `$STORM_HOME/lib/`,
    and tell Nimbus to use the new scheduler by appending the following lines to the
    configuration file at `$STORM_HOME/conf/storm.yaml`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为`CustomScheduler`类创建一个JAR，并将其放在`$STORM_HOME/lib/`中，并通过将以下行附加到`$STORM_HOME/conf/storm.yaml`配置文件中告诉Nimbus使用新的调度程序：
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Restart the Nimbus daemon to reflect the changes to the configuration.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 重新启动Nimbus守护程序以反映对配置的更改。
- en: 'Now, if we deploy the same topology shown in the previous diagram, then the
    distribution of executors will look like this (**Bolt2** is assigned to a GPU-typed
    supervisor):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们部署与上一个图中显示的相同的拓扑结构，那么执行者的分布将如下所示（**Bolt2**分配给了一个GPU类型的监督者）：
- en: '![](img/00043.gif)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00043.gif)'
- en: Summary
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the built-in Storm scheduler and also covered
    how we can write and configure a custom scheduler.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了内置的Storm调度程序，还介绍了如何编写和配置自定义调度程序。
- en: In the next chapter, we will be covering the monitoring of a Storm cluster using
    Graphite and Ganglia.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍使用Graphite和Ganglia监视Storm集群。
