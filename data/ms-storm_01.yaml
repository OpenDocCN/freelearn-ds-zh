- en: Real-Time Processing and Storm Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时处理和Storm介绍
- en: With the exponential growth in the amount of data being generated and advanced
    data-capturing capabilities, enterprises are facing the challenge of making sense
    out of this mountain of raw data. On the batch processing front, Hadoop has emerged
    as the go-to framework to deal with big data. Until recently, there has been a
    void when one looks for frameworks to build real-time stream processing applications.
    Such applications have become an integral part of a lot of businesses as they
    enable them to respond swiftly to events and adapt to changing situations. Examples
    of this are monitoring social media to analyze public response to any new product
    that you launch and predicting the outcome of an election based on the sentiments
    of election-related posts.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成的数据量呈指数级增长和先进的数据捕获能力，企业面临着从这些海量原始数据中获取信息的挑战。在批处理方面，Hadoop已成为处理大数据的首选框架。直到最近，当人们寻找构建实时流处理应用程序的框架时，一直存在空白。这些应用程序已成为许多企业的重要组成部分，因为它们使企业能够迅速响应事件并适应不断变化的情况。其例子包括监视社交媒体以分析公众对您推出的任何新产品的反应，并根据与选举相关的帖子的情绪来预测选举结果。
- en: 'Organizations are collecting a large volume of data from external sources and
    want to evaluate/process the data in real time to get market trends, detect fraud,
    identify user behavior, and so on. The need for real-time processing is increasing
    day by day and we require a real-time system/platform that should support the
    following features:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 组织正在从外部来源收集大量数据，并希望实时评估/处理数据以获取市场趋势、检测欺诈、识别用户行为等。实时处理的需求日益增加，我们需要一个支持以下功能的实时系统/平台：
- en: '**Scalable**: The platform should be horizontally scalable without any down
    time.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展**：平台应具有水平可扩展性，无需任何停机时间。'
- en: '**Fault tolerance**: The platform should be able to process the data even after
    some of the nodes in a cluster go down.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：即使集群中的一些节点出现故障，平台也应能够处理数据。'
- en: '**No data lost**: The platform should provide the guaranteed processing of
    messages.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无数据丢失**：平台应提供消息的可靠处理。'
- en: '**High throughput**: The system should be able to support millions of records
    per second and also support any size of messages.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高吞吐量**：系统应能够支持每秒数百万条记录，并支持任何大小的消息。'
- en: '**Easy to operate**: The system should have easy installation and operation.
    Also, the expansion of clusters should be an easy process.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于操作**：系统应具有易于安装和操作的特点。此外，集群的扩展应是一个简单的过程。'
- en: '**Multiple languages**: The platform should support multiple languages. The
    end user should be able to write code in different languages. For example, a user
    can write code in Python, Scala, Java, and so on. Also, we can execute different
    language code inside the one cluster.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多语言**：平台应支持多种语言。最终用户应能够用不同的语言编写代码。例如，用户可以用Python、Scala、Java等编写代码。此外，我们可以在一个集群中执行不同语言的代码。'
- en: '**Cluster isolation**: The system should support isolation so that dedicated
    processes can be assigned to dedicated machines for processing.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群隔离**：系统应支持隔离，以便为处理分配专用进程到专用机器。'
- en: Apache Storm
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Storm
- en: Apache Storm has emerged as the platform of choice for industry leaders to develop
    distributed, real-time, data processing platforms. It provides a set of primitives
    that can be used to develop applications that can process a very large amount
    of data in real time in a highly scalable manner.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Storm已成为行业领袖开发分布式实时数据处理平台的首选平台。它提供了一组原语，可用于开发可以高度可扩展地实时处理大量数据的应用程序。
- en: 'Storm is to real-time processing what Hadoop is to batch processing. It is
    open source software, and managed by Apache Software Foundation. It has been deployed
    to meet real-time processing needs by companies such as Twitter, Yahoo!, and Flipboard.
    Storm was first developed by Nathan Marz at BackType, a company that provided
    social search applications. Later, BackType was acquired by Twitter, and it is
    a critical part of their infrastructure. Storm can be used for the following use
    cases:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 风暴对实时处理就像Hadoop对批处理一样重要。它是开源软件，由Apache软件基金会管理。它已经被Twitter、Yahoo!和Flipboard等公司部署，以满足实时处理的需求。Storm最初是由BackType的Nathan
    Marz开发的，BackType是一家提供社交搜索应用的公司。后来，BackType被Twitter收购，成为其基础设施的关键部分。Storm可以用于以下用例：
- en: '**Stream processing**: Storm is used to process a stream of data and update
    a variety of databases in real time. This processing occurs in real time and the
    processing speed needs to match the input data speed.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流处理**：Storm用于处理数据流并实时更新各种数据库。这种处理是实时的，处理速度需要与输入数据速度匹配。'
- en: '**Continuous computation**: Storm can do continuous computation on data streams
    and stream the results to clients in real time. This might require processing
    each message as it comes in or creating small batches over a short time. An example
    of continuous computation is streaming trending topics on Twitter into browsers.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续计算**：Storm可以对数据流进行持续计算，并实时将结果传输给客户端。这可能需要在每条消息到达时进行处理，或者在短时间内创建小批处理。持续计算的一个例子是将Twitter上的热门话题流式传输到浏览器中。'
- en: '**Distributed RPC**: Storm can parallelize an intense query so that you can
    compute it in real time.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式RPC**：Storm可以并行处理复杂查询，以便您可以实时计算它。'
- en: '**Real-time analytics**: Storm can analyze and respond to data that comes from
    different data sources as they happen in real time.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时分析**：Storm可以分析并响应来自不同数据源的实时数据。'
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What is a Storm?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Storm？
- en: Features of Storm
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm的特点
- en: Architecture and components of a Storm cluster
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm集群的架构和组件
- en: Terminologies of Storm
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm的术语
- en: Programming language
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程语言
- en: Operation modes
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作模式
- en: Features of Storm
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm的特点
- en: 'The following are some of the features of Storm that make it a perfect solution
    to process streams of data in real time:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些使Storm成为实时处理数据流的完美解决方案的特点：
- en: '**Fast**: Storm has been reported to process up to 1 million tuples/records
    per second per node.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速**：据报道，Storm每个节点每秒可以处理高达100万个元组/记录。'
- en: '**Horizontally scalable**: Being fast is a necessary feature to build a high
    volume/velocity data processing platform, but a single node will have an upper
    limit on the number of events that it can process per second. A node represents
    a single machine in your setup that executes Storm applications. Storm, being
    a distributed platform, allows you to add more nodes to your Storm cluster and
    increase the processing capacity of your application. Also, it is linearly scalable,
    which means that you can double the processing capacity by doubling the nodes.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**横向可扩展**：快速是构建高容量/高速数据处理平台的必要特性，但单个节点对其每秒处理事件数量有上限。节点代表设置中的单台机器，执行Storm应用程序。作为分布式平台，Storm允许您向Storm集群添加更多节点，并增加应用程序的处理能力。此外，它是线性可扩展的，这意味着通过增加节点可以使处理能力加倍。'
- en: '**Fault tolerant**: Units of work are executed by worker processes in a Storm
    cluster. When a worker dies, Storm will restart that worker, and if the node on
    which the worker is running dies, Storm will restart that worker on some other
    node in the cluster. This feature will be covered in more detail in [Chapter 3](part0056.html#1LCVG0-6149dd15e07b443593cc93f2eb31ee7b),
    *Storm Parallelism and Data Partitioning*.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错**：Storm集群中的工作单元由工作进程执行。当工作进程死掉时，Storm将重新启动该工作进程，如果运行该工作进程的节点死掉，Storm将在集群中的其他节点上重新启动该工作进程。这个特性将在[第3章](part0056.html#1LCVG0-6149dd15e07b443593cc93f2eb31ee7b)中详细介绍，*Storm并行性和数据分区*。'
- en: '**Guaranteed data processing**: Storm provides strong guarantees that each
    message entering a Storm process will be processed at least once. In the event
    of failures, Storm will replay the lost tuples/records. Also, it can be configured
    so that each message will be processed only once.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理保证**：Storm提供强有力的保证，即进入Storm进程的每条消息至少会被处理一次。在发生故障时，Storm将重放丢失的元组/记录。此外，它可以配置为每条消息只被处理一次。'
- en: '**Easy to operate**: Storm is simple to deploy and manage. Once the cluster
    is deployed, it requires little maintenance.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于操作**：Storm部署和管理都很简单。一旦部署了集群，就需要很少的维护。'
- en: '**Programming language agnostic**: Even though the Storm platform runs on **Java
    virtual machine** (**JVM**), the applications that run over it can be written
    in any programming language that can read and write to standard input and output
    streams.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程语言无关**：尽管Storm平台在**Java虚拟机**（**JVM**）上运行，但在其上运行的应用程序可以用任何能够读写标准输入和输出流的编程语言编写。'
- en: Storm components
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm组件
- en: A Storm cluster follows a master-slave model where the master and slave processes
    are coordinated through ZooKeeper. The following are the components of a Storm
    cluster.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Storm集群遵循主从模型，其中主和从进程通过ZooKeeper协调。以下是Storm集群的组件。
- en: Nimbus
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Nimbus
- en: The Nimbus node is the master in a Storm cluster. It is responsible for distributing
    the application code across various worker nodes, assigning tasks to different
    machines, monitoring tasks for any failures, and restarting them as and when required.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Nimbus节点是Storm集群中的主节点。它负责在各个工作节点之间分发应用程序代码，将任务分配给不同的机器，监视任务是否出现故障，并在需要时重新启动它们。
- en: Nimbus is stateless and stores all of its data in ZooKeeper. There is a single
    Nimbus node in a Storm cluster. If the active node goes down, then the passive
    node will become an Active node. It is designed to be fail-fast, so when the active
    Nimbus dies, the passive node will become an active node, or the down node can
    be restarted without having any effect on the tasks already running on the worker
    nodes. This is unlike Hadoop, where if the JobTracker dies, all the running jobs
    are left in an inconsistent state and need to be executed again. The Storm workers
    can work smoothly even if all the Nimbus nodes go down but the user can't submit
    any new jobs into the cluster or the cluster will not be able to reassign the
    failed workers to another node.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Nimbus是无状态的，它将所有数据存储在ZooKeeper中。在Storm集群中只有一个Nimbus节点。如果活动节点宕机，那么备用节点将成为活动节点。它被设计为快速失败，因此当活动Nimbus宕机时，备用节点将成为活动节点，或者宕机的节点可以重新启动而不会对工作节点上已经运行的任务产生任何影响。这与Hadoop不同，如果JobTracker宕机，所有正在运行的作业都会处于不一致状态，需要重新执行。即使所有Nimbus节点都宕机，Storm工作节点也可以正常工作，但用户无法向集群提交任何新作业，或者集群将无法重新分配失败的工作节点到另一个节点。
- en: Supervisor nodes
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主管节点
- en: Supervisor nodes are the worker nodes in a Storm cluster. Each supervisor node
    runs a supervisor daemon that is responsible for creating, starting, and stopping
    worker processes to execute the tasks assigned to that node. Like Nimbus, a supervisor
    daemon is also fail-fast and stores all of its states in ZooKeeper so that it
    can be restarted without any state loss. A single supervisor daemon normally handles
    multiple worker processes running on that machine.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 主管节点是Storm集群中的工作节点。每个主管节点运行一个主管守护进程，负责创建、启动和停止工作进程以执行分配给该节点的任务。与Nimbus一样，主管守护进程也是快速失败的，并将其所有状态存储在ZooKeeper中，以便可以在不丢失状态的情况下重新启动。通常，单个主管守护进程会处理在该机器上运行的多个工作进程。
- en: The ZooKeeper cluster
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ZooKeeper集群
- en: In any distributed application, various processes need to coordinate with each
    other and share some configuration information. ZooKeeper is an application that
    provides all these services in a reliable manner. As a distributed application,
    Storm also uses a ZooKeeper cluster to coordinate various processes. All of the
    states associated with the cluster and the various tasks submitted to Storm are
    stored in ZooKeeper. Nimbus and supervisor nodes do not communicate directly with
    each other, but through ZooKeeper. As all data is stored in ZooKeeper, both Nimbus
    and the supervisor daemons can be killed abruptly without adversely affecting
    the cluster.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何分布式应用程序中，各种进程需要相互协调并共享一些配置信息。ZooKeeper是一个应用程序，以可靠的方式提供所有这些服务。作为一个分布式应用程序，Storm也使用ZooKeeper集群来协调各种进程。与ZooKeeper中的所有状态和提交给Storm的各种任务相关的所有数据都存储在ZooKeeper中。Nimbus和监督节点不直接相互通信，而是通过ZooKeeper。由于所有数据都存储在ZooKeeper中，因此Nimbus和监督守护程序都可以突然被杀死而不会对集群产生不利影响。
- en: 'The following is an architecture diagram of a Storm cluster:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个Storm集群的架构图：
- en: '![](img/00005.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00005.jpeg)'
- en: The Storm data model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm数据模型
- en: The basic unit of data that can be processed by a Storm application is called
    a tuple. Each tuple consists of a predefined list of fields. The value of each
    field can be a byte, char, integer, long, float, double, Boolean, or byte array.
    Storm also provides an API to define your own datatypes, which can be serialized
    as fields in a tuple.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Storm应用程序可以处理的基本数据单元称为元组。每个元组由预定义的字段列表组成。每个字段的值可以是字节、字符、整数、长整数、浮点数、双精度浮点数、布尔值或字节数组。Storm还提供了一个API来定义自己的数据类型，这些数据类型可以作为元组中的字段进行序列化。
- en: A tuple is dynamically typed, that is, you just need to define the names of
    the fields in a tuple and not their datatype. The choice of dynamic typing helps
    to simplify the API and makes it easy to use. Also, since a processing unit in
    Storm can process multiple types of tuples, it's not practical to declare field
    types.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 元组是动态类型的，也就是说，您只需要定义元组中字段的名称而不需要它们的数据类型。动态类型的选择有助于简化API并使其易于使用。此外，由于Storm中的处理单元可以处理多种类型的元组，因此声明字段类型并不实际。
- en: Each of the fields in a tuple can be accessed by its name, `getValueByField(String)`,
    or its positional index, `getValue(int)`, in the tuple. Tuples also provide convenient
    methods such as `getIntegerByField(String)` that save you from typecasting the
    objects. For example, if you have a *Fraction (numerator, denominator)* tuple,
    representing fractional numbers, then you can get the value of the numerator by
    either using `getIntegerByField("numerator")` or `getInteger(0)`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 元组中的每个字段都可以通过其名称`getValueByField(String)`或其位置索引`getValue(int)`来访问。元组还提供了方便的方法，例如`getIntegerByField(String)`，可以使您免于对对象进行类型转换。例如，如果您有一个表示分数的*Fraction
    (numerator, denominator)*元组，那么您可以通过使用`getIntegerByField("numerator")`或`getInteger(0)`来获取分子的值。
- en: You can see the full set of operations supported by `org.apache.storm.tuple.Tuple`
    in the Java doc that is located at [https://storm.apache.org/releases/1.0.2/javadocs/org/apache/storm/tuple/Tuple.html](https://storm.apache.org/releases/1.0.2/javadocs/org/apache/storm/tuple/Tuple.html).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在位于[https://storm.apache.org/releases/1.0.2/javadocs/org/apache/storm/tuple/Tuple.html](https://storm.apache.org/releases/1.0.2/javadocs/org/apache/storm/tuple/Tuple.html)的Java文档中查看`org.apache.storm.tuple.Tuple`支持的完整操作集。
- en: Definition of a Storm topology
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm拓扑的定义
- en: 'In Storm terminology, a topology is an abstraction that defines the graph of
    the computation. You create a Storm topology and deploy it on a Storm cluster
    to process data. A topology can be represented by a direct acyclic graph, where
    each node does some kind of processing and forwards it to the next node(s) in
    the flow. The following diagram is a sample Storm topology:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在Storm术语中，拓扑是定义计算图的抽象。您可以创建一个Storm拓扑并将其部署到Storm集群中以处理数据。拓扑可以用有向无环图表示，其中每个节点都进行某种处理并将其转发到流程中的下一个节点。以下图是一个示例Storm拓扑：
- en: '![](img/00006.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00006.jpeg)'
- en: 'The following are the components of a Storm topology:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Storm拓扑的组件：
- en: '**Tuple**: A single message/record that flows between the different instances
    of a topology is called a tuple.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tuple**：在拓扑的不同实例之间流动的单个消息/记录称为元组。'
- en: '**Stream**: The key abstraction in Storm is that of a stream. A stream is an
    unbounded sequence of tuples that can be processed in parallel by Storm. Each
    stream can be processed by a single or multiple types of bolts (the processing
    units in Storm, which are defined later in this section). Thus, Storm can also
    be viewed as a platform to transform streams. In the preceding diagram, streams
    are represented by arrows. Each stream in a Storm application is given an ID and
    the bolts can produce and consume tuples from these streams on the basis of their
    ID. Each stream also has an associated schema for the tuples that will flow through
    it.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Stream**：Storm中的关键抽象是流。流是一系列可以由Storm并行处理的元组。每个流可以由单个或多个类型的bolt（Storm中的处理单元，在本节后面定义）并行处理。因此，Storm也可以被视为转换流的平台。在前面的图中，流用箭头表示。Storm应用程序中的每个流都被赋予一个ID，bolt可以根据其ID从这些流中产生和消费元组。每个流还有一个与其流经的元组相关的模式。'
- en: '**Spout**: A spout is the source of tuples in a Storm topology. It is responsible
    for reading or listening to data from an external source, for example, by reading
    from a log file or listening for new messages in a queue and publishing them--emitting
    in Storm terminology into streams. A spout can emit multiple streams, each of
    a different schema. For example, it can read records of 10 fields from a log file
    and emit them as different streams of seven-fields tuples and four-fields tuples
    each.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spout**：Spout是Storm拓扑中元组的来源。它负责从外部来源读取或监听数据，例如从日志文件中读取或监听队列中的新消息并发布它们--在Storm术语中发射到流中。Spout可以发射多个流，每个流具有不同的模式。例如，它可以从日志文件中读取包含10个字段的记录，并将它们作为包含七个字段元组和四个字段元组的不同流发射出去。'
- en: The `org.apache.storm.spout.ISpout` interface is the interface used to define
    spouts. If you are writing your topology in Java, then you should use `org.apache.storm.topology.IRichSpout`
    as it declares methods to use with the `TopologyBuilder` API. Whenever a spout
    emits a tuple, Storm tracks all the tuples generated while processing this tuple,
    and when the execution of all the tuples in the graph of this source tuple is
    complete, it will send an acknowledgement back to the spout. This tracking happens
    only if a message ID was provided when emitting the tuple. If null was used as
    the message ID, this tracking will not happen.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`org.apache.storm.spout.ISpout`接口是用于定义喷口的接口。如果您在Java中编写拓扑，则应使用`org.apache.storm.topology.IRichSpout`，因为它声明了与`TopologyBuilder`API一起使用的方法。每当喷口发射一个元组时，Storm会跟踪处理此元组时生成的所有元组，当源元组的图中所有元组的执行完成时，它将向喷口发送确认。只有在发射元组时提供了消息ID时才会发生此跟踪。如果使用null作为消息ID，则不会发生此跟踪。'
- en: A tuple processing timeout can also be defined for a topology, and if a tuple
    is not processed within the specified timeout, a fail message will be sent back
    to the spout. Again, this will happen only if you define a message ID. A small
    performance gain can be extracted out of Storm at the risk of some data loss by
    disabling the message acknowledgements, which can be done by skipping the message
    ID while emitting tuples.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以为拓扑定义元组处理超时，如果元组在指定的超时时间内未被处理，将向喷口发送失败消息。再次强调，只有在定义消息ID时才会发生这种情况。通过跳过发射元组时的消息ID来禁用消息确认，可以从Storm中获得一些小的性能提升，但也会有一些数据丢失的风险。
- en: 'The important methods of spout are:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 喷口的重要方法有：
- en: '`nextTuple()`: This method is called by Storm to get the next tuple from the
    input source. Inside this method, you will have the logic of reading data from
    external sources and emitting them to an instance of `org.apache.storm.spout.ISpoutOutputCollector`.
    The schema for streams can be declared by using the `declareStream` method of
    `org.apache.storm.topology.OutputFieldsDeclarer`.'
  id: totrans-58
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nextTuple()`: Storm调用此方法从输入源获取下一个元组。在此方法内部，您将具有从外部源读取数据并将其发射到`org.apache.storm.spout.ISpoutOutputCollector`实例的逻辑。可以使用`org.apache.storm.topology.OutputFieldsDeclarer`的`declareStream`方法声明流的模式。'
- en: If a spout wants to emit data to more than one stream, it can declare multiple
    streams using the `declareStream` method and specify a stream ID while emitting
    the tuple. If there are no more tuples to emit at the moment, this method will
    not be blocked. Also, if this method does not emit a tuple, then Storm will wait
    for 1 millisecond before calling it again. This waiting time can be configured
    using the `topology.sleep.spout.wait.strategy.time.ms` setting.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果喷口希望向多个流发射数据，可以使用`declareStream`方法声明多个流，并在发射元组时指定流ID。如果此时没有更多的元组要发射，此方法将不会被阻塞。此外，如果此方法不发射元组，则Storm将在再次调用它之前等待1毫秒。可以使用`topology.sleep.spout.wait.strategy.time.ms`设置来配置此等待时间。
- en: '`ack(Object msgId)`: This method is invoked by Storm when the tuple with the
    given message ID is completely processed by the topology. At this point, the user
    should mark the message as processed and do the required cleaning up, such as
    removing the message from the message queue so that it does not get processed
    again.'
  id: totrans-60
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ack(Object msgId)`: 当具有给定消息ID的元组被拓扑完全处理时，Storm将调用此方法。在这一点上，用户应标记消息已处理，并进行必要的清理，例如从消息队列中删除消息，以便不再处理它。'
- en: '`fail(Object msgId)`: This method is invoked by Storm when it identifies that
    the tuple with the given message ID has not been processed successfully or has
    timed out of the configured interval. In such scenarios, the user should do the
    required processing so that the messages can be emitted again by the `nextTuple`
    method. A common way to do this is to put the message back in the incoming message
    queue.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fail(Object msgId)`: 当Storm识别出具有给定消息ID的元组未能成功处理或超时配置的时间间隔时，将调用此方法。在这种情况下，用户应进行必要的处理，以便通过`nextTuple`方法再次发射消息。一个常见的做法是将消息放回传入消息队列。'
- en: '`open()`: This method is called only once--when the spout is initialized. If
    it is required to connect to an external source for the input data, define the
    logic to connect to the external source in the open method, and then keep fetching
    the data from this external source in the `nextTuple` method to emit it further.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`open()`: 当喷口初始化时，只调用一次此方法。如果需要连接到外部源以获取输入数据，应在open方法中定义连接到外部源的逻辑，然后在`nextTuple`方法中不断从外部源获取数据以进一步发射它。'
- en: Another point to note while writing your spout is that none of the methods should
    be blocking, as Storm calls all the methods in the same thread. Every spout has
    an internal buffer to keep track of the status of the tuples emitted so far. The
    spout will keep the tuples in this buffer until they are either acknowledged or
    failed, calling the `ack` or `fail` method, respectively. Storm will call the
    `nextTuple` method only when this buffer is not full.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写喷口时需要注意的另一点是，不能阻塞任何方法，因为Storm在同一线程中调用所有方法。每个喷口都有一个内部缓冲区，用于跟踪到目前为止发射的元组的状态。喷口将保留这些元组在缓冲区中，直到它们被确认或失败，分别调用`ack`或`fail`方法。只有当此缓冲区不满时，Storm才会调用`nextTuple`方法。
- en: '**Bolt**: A bolt is the processing powerhouse of a Storm topology and is responsible
    for transforming a stream. Ideally, each bolt in the topology should be doing
    a simple transformation of the tuples, and many such bolts can coordinate with
    each other to exhibit a complex transformation.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bolt**: 一个bolt是Storm拓扑的处理引擎，负责转换流。理想情况下，拓扑中的每个bolt都应该对元组进行简单的转换，许多这样的bolt可以相互协调，展示复杂的转换。'
- en: The `org.apache.storm.task.IBolt` interface is preferably used to define bolts,
    and if a topology is written in Java, you should use the `org.apache.storm.topology.IRichBolt`
    interface. A bolt can subscribe to multiple streams of other components--either
    spouts or other bolts--in the topology and similarly can emit output to multiple
    streams. Output streams can be declared using the `declareStream` method of `org.apache.storm.topology.OutputFieldsDeclarer`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`org.apache.storm.task.IBolt`接口通常用于定义bolt，如果拓扑是用Java编写的，则应该使用`org.apache.storm.topology.IRichBolt`接口。Bolt可以订阅拓扑中其他组件（spouts或其他bolts）的多个流，同样也可以向多个流发出输出。可以使用`org.apache.storm.topology.OutputFieldsDeclarer`的`declareStream`方法声明输出流。'
- en: 'The important methods of a bolt are:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个bolt的重要方法有：
- en: '`execute(Tuple input)`: This method is executed for each tuple that comes through
    the subscribed input streams. In this method, you can do whatever processing is
    required for the tuple and then produce the output either in the form of emitting
    more tuples to the declared output streams, or other things such as persisting
    the results in a database.'
  id: totrans-67
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`execute(Tuple input)`: 对于通过订阅的输入流传入的每个元组，将执行此方法。在此方法中，您可以对元组进行所需的任何处理，然后以发出更多元组到声明的输出流的形式，或者其他操作，比如将结果持久化到数据库。'
- en: You are not required to process the tuple as soon as this method is called,
    and the tuples can be held until required. For example, while joining two streams,
    when a tuple arrives you can hold it until its counterpart also comes, and then
    you can emit the joined tuple.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用此方法时，您不需要立即处理元组，可以将元组保留直到需要。例如，在连接两个流时，当一个元组到达时，您可以将其保留，直到其对应的元组也到达，然后您可以发出连接的元组。
- en: The metadata associated with the tuple can be retrieved by the various methods
    defined in the `Tuple` interface. If a message ID is associated with a tuple,
    the execute method must publish an `ack` or `fail` event using `OutputCollector`
    for the bolt, or else Storm will not know whether the tuple was processed successfully.
    The `org.apache.storm.topology.IBasicBolt` interface is a convenient interface
    that sends an acknowledgement automatically after the completion of the execute
    method. If a fail event is to be sent, this method should throw `org.apache.storm.topology.FailedException`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与元组相关的元数据可以通过`Tuple`接口中定义的各种方法来检索。如果元组关联了消息ID，则execute方法必须使用`OutputCollector`为bolt发布`ack`或`fail`事件，否则Storm将不知道元组是否被成功处理。`org.apache.storm.topology.IBasicBolt`接口是一个方便的接口，在execute方法完成后会自动发送确认。如果要发送失败事件，此方法应该抛出`org.apache.storm.topology.FailedException`。
- en: '`prepare(Map stormConf, TopologyContext context, OutputCollector collector)`:
    A bolt can be executed by multiple workers in a Storm topology. The instance of
    a bolt is created on the client machine and then serialized and submitted to Nimbus.
    When Nimbus creates the worker instances for the topology, it sends this serialized
    bolt to the workers. The work will desterilize the bolt and call the `prepare`
    method. In this method, you should make sure the bolt is properly configured to
    execute tuples. Any state that you want to maintain can be stored as instance
    variables for the bolt that can be serialized/deserialized later.'
  id: totrans-70
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prepare(Map stormConf, TopologyContext context, OutputCollector collector)`:
    在Storm拓扑中，一个bolt可以由多个worker执行。Bolt的实例在客户端机器上创建，然后序列化并提交给Nimbus。当Nimbus为拓扑创建worker实例时，它会将这个序列化的bolt发送给worker。worker将解序列化bolt并调用`prepare`方法。在这个方法中，您应该确保bolt被正确配置以执行元组。您希望保持的任何状态可以存储为bolt的实例变量，稍后可以进行序列化/反序列化。'
- en: Operation modes in Storm
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm中的操作模式
- en: 'Operation modes indicate how the topology is deployed in Storm. Storm supports
    two types of operation modes to execute the Storm topology:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 操作模式指示了拓扑在Storm中的部署方式。Storm支持两种类型的操作模式来执行Storm拓扑：
- en: '**Local mode**: In local mode, Storm topologies run on the local machine in
    a single JVM. This mode simulates a Storm cluster in a single JVM and is used
    for the testing and debugging of a topology.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地模式**：在本地模式下，Storm拓扑在单个JVM中在本地机器上运行。这种模式模拟了单个JVM中的Storm集群，并用于拓扑的测试和调试。'
- en: '**Remote mode**: In remote mode, we will use the Storm client to submit the
    topology to the master along with all the necessary code required to execute the
    topology. Nimbus will then take care of distributing your code.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**远程模式**：在远程模式下，我们将使用Storm客户端将拓扑提交给主节点，以及执行拓扑所需的所有必要代码。Nimbus将负责分发您的代码。'
- en: In the next chapter, we are going to cover both local and remote mode in more
    detail, along with a sample example.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更详细地介绍本地模式和远程模式，以及一个示例。
- en: Programming languages
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编程语言
- en: Storm was designed from the ground up to be usable with any programming language.
    At the core of Storm is a thrift definition for defining and submitting topologies.
    Since thrift can be used in any language, topologies can be defined and submitted
    in any language.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Storm从一开始就被设计为可用于任何编程语言。Storm的核心是用于定义和提交拓扑的thrift定义。由于thrift可以在任何语言中使用，因此可以用任何语言定义和提交拓扑。
- en: Similarly, spouts and bolts can be defined in any language. Non-JVM spouts and
    bolts communicate with Storm over a JSON-based protocol over `stdin`/`stdout`.
    Adapters that implement this protocol exist for Ruby, Python, JavaScript, and
    Perl. You can refer to [https://github.com/apache/storm/tree/master/storm-multilang](https://github.com/apache/storm/tree/master/storm-multilang)
    to find out about the implementation of these adapters.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，spouts和bolts可以用任何语言定义。非JVM spouts和bolts通过`stdin`/`stdout`上的基于JSON的协议与Storm通信。实现这种协议的适配器存在于Ruby、Python、JavaScript和Perl中。您可以参考[https://github.com/apache/storm/tree/master/storm-multilang](https://github.com/apache/storm/tree/master/storm-multilang)了解这些适配器的实现。
- en: Storm-starter has an example topology, [https://github.com/apache/storm/tree/master/examples/storm-starter/multilang/resources](https://github.com/apache/storm/tree/master/examples/storm-starter/multilang/resources),
    which implements one of the bolts in Python.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Storm-starter有一个示例拓扑，[https://github.com/apache/storm/tree/master/examples/storm-starter/multilang/resources](https://github.com/apache/storm/tree/master/examples/storm-starter/multilang/resources)，其中使用Python实现了其中一个bolt。
- en: Summary
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced you to the basics of Storm and the various components
    that make up a Storm cluster. We saw a definition of different deployment/operation
    modes in which a Storm cluster can operate.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了Storm的基础知识以及构成Storm集群的各种组件。我们看到了Storm集群可以操作的不同部署/运行模式的定义。
- en: In the next chapter, we will set up a single and three-node Storm cluster and
    see how we can deploy the topology on a Storm cluster. We will also see different
    types of stream groupings supported by Storm and the guaranteed message semantic
    provided by Storm.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将建立一个单节点和三节点的Storm集群，并看看如何在Storm集群上部署拓扑。我们还将看到Storm支持的不同类型的流分组以及Storm提供的消息语义保证。
