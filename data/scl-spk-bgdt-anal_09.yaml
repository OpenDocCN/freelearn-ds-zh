- en: Stream Me Up, Scotty - Spark Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Stream Me Up, Scotty - Spark Streaming
- en: '*"I really like streaming services. It''s a great way for people to find your
    music"*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*“我真的很喜欢流媒体服务。这是人们发现你音乐的一个好方式”*'
- en: '- Kygo'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- Kygo'
- en: 'In this chapter, we will learn about Spark Streaming and find out how we can
    take advantage of it to process streams of data using the Spark API. Moreover,
    in this chapter, we will learn various ways of processing real-time streams of
    data using a practical example to consume and process tweets from Twitter. In
    a nutshell, the following topics will be covered throughout this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍 Spark Streaming，并了解我们如何利用 Spark API 处理数据流。此外，在本章中，我们将通过一个实际的例子，学习如何处理实时数据流，使用
    Twitter 的推文来消费和处理数据。简而言之，本章将涵盖以下主题：
- en: A brief introduction to streaming
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流媒体简介
- en: Spark Streaming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: Discretized streams
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散化流
- en: Stateful/stateless transformations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有状态/无状态转换
- en: Checkpointing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点
- en: Interoperability with streaming platforms (Apache Kafka)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与流媒体平台的互操作性（Apache Kafka）
- en: Structured streaming
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流
- en: A Brief introduction to streaming
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流媒体简介
- en: In today's world of interconnected devices and services, it is hard to even
    spend a few hours a day without our smartphone to check Facebook, or order an
    Uber ride, or tweet something about the burger you just bought, or check the latest
    news or sports updates on your favorite team. We depend on our phones and Internet,
    for a lot of things, whether it is to get work done, or just browse, or e-mail
    your friend. There is simply no way around this phenomenon, and the number and
    variety of applications and services will only grow over time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今互联设备和服务的世界里，我们几乎不可能每天花几小时而不依赖智能手机去查看 Facebook，或者订车，或者发推特分享你刚买的汉堡，或者查看你最喜欢的球队的最新新闻或体育动态。我们依赖手机和互联网做很多事情，无论是工作，还是浏览，还是给朋友发邮件。这个现象已经无可避免，而且应用和服务的数量与种类只会随着时间增长。
- en: As a result, the smart devices are everywhere, and they generate a lot of data
    all the time. This phenomenon, also broadly referred to as the Internet of Things,
    has changed the dynamics of data processing forever. Whenever you use any of the
    services or apps on your iPhone, or Droid or Windows phone, in some shape or form,
    real-time data processing is at work. Since so much is depending on the quality
    and value of the apps, there is a lot of emphasis on how the various startups
    and established companies are tackling the complex challenges of **SLAs** (**Service
    Level Agreements**), and usefulness and also the timeliness of the data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，智能设备无处不在，并且它们时刻产生大量数据。这种现象，也被广泛称为物联网，永远改变了数据处理的动态。无论何时你使用 iPhone、Android
    或 Windows 手机上的任何服务或应用程序，以某种形式，实时数据处理都在发挥作用。由于大量依赖于应用程序的质量和价值，人们非常关注各种初创公司和成熟公司如何应对**SLA**（**服务级别协议**）、有用性以及数据的及时性等复杂挑战。
- en: One of the paradigms being researched and adopted by organisations and service
    providers is the building of very scalable, near real-time or real-time processing
    frameworks on a very cutting-edge platform or infrastructure. Everything must
    be fast and also reactive to changes and failures. You would not like it if your
    Facebook updated once every hour or if you received email only once a day; so,
    it is imperative that data flow, processing, and the usage are all as close to
    real time as possible. Many of the systems we are interested in monitoring or
    implementing generate a lot of data as an indefinite continuous stream of events.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织和服务提供商正在研究并采用的一个范式是构建非常可扩展的、接近实时或实时的数据处理框架，运行在非常前沿的平台或基础设施上。所有东西都必须快速并且对变化和故障具有反应能力。如果你的
    Facebook 每小时更新一次，或者你每天只收到一次电子邮件，你可能不会喜欢；因此，数据流、处理和使用必须尽可能接近实时。我们感兴趣的许多系统都会生成大量数据，作为一个无限期持续的事件流。
- en: As in any other data processing system, we have the same fundamental challenges
    of a collection of data, storage, and processing of data. However, the additional
    complexity is due to the real-time needs of the platform. In order to collect
    such indefinite streams of events and then subsequently process all such events
    in order to generate actionable insights, we need to use highly scalable specialized
    architectures to deal with tremendous rates of events. As such, many systems have
    been built over the decades starting from AMQ, RabbitMQ, Storm, Kafka, Spark,
    Flink, Gearpump, Apex, and so on.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他数据处理系统一样，我们面临着数据收集、存储和处理的基本挑战。然而，额外的复杂性来自于平台的实时需求。为了收集这些不确定的事件流，并随后处理所有这些事件以生成可操作的见解，我们需要使用高度可扩展的专业架构来应对海量的事件速率。因此，许多系统已经在几十年中发展起来，从AMQ、RabbitMQ、Storm、Kafka、Spark、Flink、Gearpump、Apex等开始。
- en: Modern systems built to deal with such large amounts of streaming data come
    with very flexible and scalable technologies that are not only very efficient
    but also help realize the business goals much better than before. Using such technologies,
    it is possible to consume data from a variety of data sources and then use it
    in a variety of use cases almost immediately or at a later time as needed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对如此大量的流数据，现代系统采用了非常灵活且可扩展的技术，这些技术不仅效率极高，而且比以往更能帮助实现业务目标。通过使用这些技术，几乎可以立即或根据需要在稍后的时间里，消费来自各种数据源的数据，并将其用于各种使用场景。
- en: Let us talk about what happens when you take out your smartphone and book an
    Uber ride to go to the airport. With a few touches on the smartphone screen, you're
    able to select a point, choose the credit card, make the payment, and book the
    ride. Once you're done with your transaction, you then get to monitor the progress
    of your car real-time on a map on your phone. As the car is making its way toward
    you, you're able to monitor exactly where the car is and you can also make a decision
    to pick up coffee at the local Starbucks while you're waiting for the car to pick
    you up.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈当你拿出智能手机并预定 Uber 车去机场时发生了什么。通过在手机屏幕上轻点几下，你可以选择一个地点、选择信用卡、完成支付并预定乘车。交易完成后，你可以在手机上的地图上实时监控车辆的进展。当汽车向你驶来时，你可以精确看到车辆的位置，同时，你也可以决定在等车时去附近的
    Starbucks 买杯咖啡。
- en: You could also make informed decisions regarding the car and the subsequent
    trip to the airport by looking at the expected time of arrival of the car. If
    it looks like the car is going to take quite a bit of time picking you up, and
    if this poses a risk to the flight you are about to catch, you could cancel the
    ride and hop in a taxi that just happens to be nearby. Alternatively, if it so
    happens that the traffic situation is not going to let you reach the airport on
    time, thus posing a risk to the flight you are due to catch, then you also get
    to make a decision regarding rescheduling or canceling your flight.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过查看预计的汽车到达时间来做出关于汽车和随后的机场之行的明智决策。如果看起来汽车接你需要花费相当长的时间，而且这可能会影响到你即将乘坐的航班，那么你可以取消这次乘车并选择附近恰好有空的出租车。或者，如果恰好由于交通情况无法按时到达机场，从而可能影响到你即将乘坐的航班，那么你也可以做出重新安排或取消航班的决定。
- en: Now in order to understand how such real-time streaming architectures work to
    provide such invaluable information, we need to understand the basic tenets of
    streaming architectures. On the one hand, it is very important for a real-time
    streaming architecture to be able to consume extreme amounts of data at very high
    rates while , on the other hand, also ensuring reasonable guarantees that the
    data that is getting ingested is also processed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了理解这种实时流架构是如何工作并提供如此宝贵的信息的，我们需要理解流架构的基本原则。一方面，实时流架构必须能够以极高的速率消费大量数据，另一方面，还需要确保获取的数据也能够被处理。
- en: 'The following images diagram shows a generic stream processing system with
    a producer putting events into a messaging system while a consumer is reading
    from the messaging system:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个通用的流处理系统，生产者将事件放入消息系统，消费者则从消息系统读取事件：
- en: '![](img/00125.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00125.jpeg)'
- en: 'Processing of real-time streaming data can be categorized into the following
    three essential paradigms:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实时流数据处理可以分为以下三种基本范式：
- en: At least once processing
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一次处理
- en: At most once processing
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至多一次处理
- en: Exactly once processing
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确一次处理
- en: Let's look at what these three stream processing paradigms mean to our business
    use cases.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这三种流处理范式对我们的业务用例意味着什么。
- en: While exactly once processing of real-time events is the ultimate nirvana for
    us, it is very difficult to always achieve this goal in different scenarios. We
    have to compromise on the property of exactly once processing in cases where the
    benefit of such a guarantee is outweighed by the complexity of the implementation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管实时事件的**精确一次处理**是我们的最终目标，但在不同场景下，始终实现这一目标非常困难。我们必须在某些情况下对精确一次处理的特性进行妥协，因为这样的保证的好处往往被实现的复杂性所抵消。
- en: At least once processing
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 至少一次处理
- en: The at least once processing paradigm involves a mechanism to save the position
    of the last event received **only after** the event is actually processed and
    results persisted somewhere so that, if there is a failure and the consumer restarts,
    the consumer will read the old events again and process them. However, since there
    is no guarantee that the received events were not processed at all or partially
    processed, this causes a potential duplication of events as they are fetched again.
    This results in the behavior that events ate processed at least once.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 至少一次处理范式涉及一种机制，在事件实际处理并且结果已持久化之后，**仅仅在**事件被处理后才保存最后接收到事件的位置，这样，如果发生故障并且消费者重启，消费者将重新读取旧的事件并进行处理。然而，由于无法保证接收到的事件没有被完全处理或部分处理，这会导致事件被重复获取，从而可能导致事件重复处理。这就导致了“事件至少被处理一次”的行为。
- en: At least once is ideally suitable for any application that involves updating
    some instantaneous ticker or gauge to show current values. Any cumulative sum,
    counter, or dependency on the accuracy of aggregations (`sum`, `groupBy`, and
    so on) does not fit the use case for such processing simply because duplicate
    events will cause incorrect results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 至少一次处理理想适用于任何涉及更新某些瞬时计量器或仪表来显示当前值的应用程序。任何累计总和、计数器或依赖于聚合结果准确性的应用场景（如`sum`、`groupBy`等）不适合采用这种处理方式，因为重复的事件会导致错误的结果。
- en: 'The sequence of operations for the consumer are as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者的操作顺序如下：
- en: Save results
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存结果
- en: Save offsets
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存偏移量
- en: 'Shown in the following is an illustration of what happens if there are a failure
    and **consumer** restarts. Since the events have already been processed but the
    offsets have not saved, the consumer will read from the previous offsets saved,
    thus causing duplicates. Event 0 is processed twice in the following figure:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，展示了如果发生故障并且**消费者**重启时会发生什么情况。由于事件已经处理完毕，但偏移量未保存，消费者将从之前保存的偏移量处开始读取，从而导致重复。下图中事件0被处理了两次：
- en: '![](img/00277.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00277.jpeg)'
- en: At most once processing
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 至多一次处理
- en: The At most once processing paradigm involves a mechanism to save the position
    of the last event received before the event is actually processed and results
    persisted somewhere so that, if there is a failure and the consumer restarts,
    the consumer will not try to read the old events again. However, since there is
    no guarantee that the received events were all processed, this causes potential
    loss of events as they are never fetched again. This results in the behavior that
    the events are processed at most once or not processed at all.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 至多一次处理范式涉及一种机制，在事件实际处理并且结果已持久化之前保存最后接收到事件的位置，这样，如果发生故障并且消费者重启，消费者将不会再尝试读取旧的事件。然而，由于无法保证接收到的事件都已处理完，这会导致潜在的事件丢失，因为这些事件再也不会被获取。这样就导致了“事件至多被处理一次或根本未被处理”的行为。
- en: At most once is ideally suitable for any application that involves updating
    some instantaneous ticker or gauge to show current values, as well as any cumulative
    sum, counter, or other aggregation, provided accuracy is not mandatory or the
    application needs absolutely all events. Any events lost will cause incorrect
    results or missing results.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 至多一次处理理想适用于任何涉及更新某些瞬时计量器或仪表来显示当前值的应用程序，以及任何累计总和、计数器或其他聚合操作，只要不要求精确度或应用程序不需要所有事件。任何丢失的事件都会导致错误的结果或缺失的结果。
- en: 'The sequence of operations for the consumer are as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者的操作顺序如下：
- en: Save offsets
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存偏移量
- en: Save results
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存结果
- en: 'Shown in the following is an illustration of what happens if there are a failure
    and the **consumer** restarts. Since the events have not been processed but offsets
    are saved, the consumer will read from the saved offsets, causing a gap in events
    consumed. Event 0 is never processed in the following figure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了如果发生故障且**消费者**重新启动时的情况。由于事件尚未处理，但偏移量已经保存，消费者将从保存的偏移量读取，导致事件消费出现间隙。在下图中，事件0从未被处理：
- en: '![](img/00340.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00340.jpeg)'
- en: Exactly once processing
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确一次处理
- en: The Exactly once processing paradigm is similar to the at least once paradigm,
    and involves a mechanism to save the position of the last event received only
    after the event has actually been processed and the results persisted somewhere
    so that, if there is a failure and the consumer restarts, the consumer will read
    the old events again and process them. However, since there is no guarantee that
    the received events were not processed at all or were partially processed, this
    causes a potential duplication of events as they are fetched again. However, unlike
    the at least once paradigm, the duplicate events are not processed and are dropped,
    thus resulting in the exactly once paradigm.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 精确一次处理范式类似于至少一次范式，涉及一种机制，只有在事件实际被处理并且结果已经持久化到某处后，才保存接收到的最后一个事件的位置。因此，如果发生故障且消费者重新启动，消费者将再次读取旧的事件并处理它们。然而，由于无法保证接收到的事件完全没有处理或仅部分处理，这可能会导致事件的潜在重复，因为它们会被再次获取。然而，与至少一次范式不同，重复的事件不会被处理，而是被丢弃，从而实现了精确一次范式。
- en: Exactly once processing paradigm is suitable for any application that involves
    accurate counters, aggregations, or which in general needs every event processed
    only once and also definitely once (without loss).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 精确一次处理范式适用于任何需要准确计数、聚合，或一般需要每个事件仅处理一次且一定要处理一次（不丢失）的应用。
- en: 'The sequence of operations for the consumer are as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者的操作顺序如下：
- en: Save results
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存结果
- en: Save offsets
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存偏移量
- en: 'The following is illustration shows what happens if there are a failure and
    the **consumer** restarts. Since the events have already been processed but offsets
    have not saved, the consumer will read from the previous offsets saved, thus causing
    duplicates. Event 0 is processed only once in the following figure because the
    **consumer** drops the duplicate event 0:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了如果发生故障且**消费者**重新启动时的情况。由于事件已经处理，但偏移量没有保存，消费者将从先前保存的偏移量读取，从而导致重复。下图中事件0只被处理一次，因为**消费者**丢弃了重复的事件0：
- en: '![](img/00105.jpeg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.jpeg)'
- en: 'How does the Exactly once paradigm drop duplicates? There are two techniques
    which can help here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 精确一次范式如何丢弃重复项？有两种技术可以帮助解决这个问题：
- en: Idempotent updates
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 幂等更新
- en: Transactional updates
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 事务性更新
- en: Spark Streaming also implements structured streaming in Spark 2.0+, which support
    Exactly once processing out of the box. We will look at structured streaming later
    in this chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming在Spark 2.0+中也实现了结构化流处理，支持开箱即用的精确一次处理。我们将在本章稍后讨论结构化流处理。
- en: Idempotent updates involve saving results based on some unique ID/key generated
    so that, if there is a duplicate, the generated unique ID/key will already be
    in the results (for instance, a database) so that the consumer can drop the duplicate
    without updating the results. This is complicated as it's not always possible
    or easy to generate unique keys. It also requires additional processing on the
    consumer end. Another point is that the database can be separate for results and
    offsets.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 幂等更新涉及根据某个唯一的ID/键保存结果，以便如果有重复，生成的唯一ID/键已经存在于结果中（例如，数据库），这样消费者就可以丢弃重复项而无需更新结果。这是复杂的，因为并非总能生成唯一的键，且生成唯一键并不总是容易的。它还需要消费者端额外的处理。另一个问题是，数据库可能会将结果和偏移量分开。
- en: Transactional updates save results in batches that have a transaction beginning
    and a transaction commit phase within so that, when the commit occurs, we know
    that the events were processed successfully. Hence, when duplicate events are
    received, they can be dropped without updating results. This technique is much
    more complicated than the idempotent updates as now we need some transactional
    data store. Another point is that the database must be the same for results and
    offsets.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 事务性更新将结果保存在批次中，批次具有事务开始和事务提交阶段，因此当提交发生时，我们知道事件已成功处理。因此，当接收到重复事件时，可以在不更新结果的情况下将其丢弃。这种技术比幂等更新复杂得多，因为现在我们需要一些事务性数据存储。另一个要点是，结果和偏移量的数据库必须相同。
- en: You should look into the use case you're trying to build and see if at least
    once processing, or At most once processing, can be reasonably wide and still
    achieve an acceptable level of performance and accuracy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该研究您尝试构建的用例，并查看至少一次处理或最多一次处理是否可以合理地广泛应用，并仍然能够达到可接受的性能和准确性水平。
- en: We will be looking at the paradigms closely when we learn about Spark Streaming
    and how to use Spark Streaming and consume events from Apache Kafka in the following
    sections.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，当我们学习 Spark Streaming，以及如何使用 Spark Streaming 和消费来自 Apache Kafka 的事件时，我们将密切关注这些范式。
- en: Spark Streaming
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: Spark Streaming is not the first streaming architecture to come into existence.
    Several technologies have existenced over time to deal with the real-time processing
    needs of various business use cases. Twitter Storm was one of the first popular
    stream processing technologies out there and was in used by many organizations
    fulfilling the needs of many businesses.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 并不是第一个出现的流处理架构。多种技术随着时间的推移应运而生，以应对各种业务用例的实时处理需求。Twitter Storm
    是最早的流处理技术之一，并被许多组织广泛使用，满足了许多企业的需求。
- en: Apache Spark comes with a streaming library, which has rapidly evolved to be
    the most widely used technology. Spark Streaming has some distinct advantages
    over the other technologies, the first and foremost being the tight integration
    between Spark Streaming APIs and the Spark core APIs making building a dual purpose
    real-time and batch analytical platform feasible and efficient than otherwise.
    Spark Streaming also integrates with Spark ML and Spark SQL, as well as GraphX,
    making it the most powerful stream processing technology that can serve many unique
    and complex use cases. In this section, we will look deeper into what Spark Streaming
    is all about.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 配备了一个流处理库，该库已迅速发展为最广泛使用的技术。Spark Streaming 在其他技术之上具有一些明显优势，首先是
    Spark Streaming API 与 Spark 核心 API 之间的紧密集成，使得构建一个同时支持实时和批量分析的平台变得可行和高效。Spark Streaming
    还集成了 Spark ML 和 Spark SQL，以及 GraphX，使其成为能够服务许多独特和复杂用例的最强大的流处理技术。在本节中，我们将深入了解 Spark
    Streaming 的所有内容。
- en: For more information on Spark Streaming, you can refer to [https://spark.apache.org/docs/2.1.0/streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/streaming-programming-guide.html).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于 Spark Streaming 的信息，请参阅 [https://spark.apache.org/docs/2.1.0/streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/streaming-programming-guide.html)。
- en: Spark Streaming supports several input sources and can write results to several
    sinks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 支持多种输入源，并可以将结果写入多个输出目标。
- en: '![](img/00004.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00004.jpeg)'
- en: While Flink, Heron (successor to Twitter Storm), Samza, and so on all handle
    events as they are collected with minimal latency, Spark Streaming consumes continuous
    streams of data and then processes the collected data in the form of micro-batches.
    The size of the micro-batch can be as low as 500 milliseconds but usually cannot
    go lower than that.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Flink、Heron（Twitter Storm 的继任者）、Samza 等都可以在收集事件时以最低的延迟处理事件，但 Spark Streaming
    则会连续消耗数据流，并以微批处理的形式处理收集到的数据。微批的大小可以低至 500 毫秒，但通常不会低于此值。
- en: Apache Apex, Gear pump, Flink, Samza, Heron, or other upcoming technologies
    compete with Spark Streaming in some use cases. If you need true event-by-event
    processing, then Spark Streaming is not the right fit for your use case.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Apex、Gear pump、Flink、Samza、Heron 或其他即将推出的技术在某些用例中与 Spark Streaming 竞争。如果您需要真正的事件处理，则
    Spark Streaming 不适合您的用例。
- en: The way streaming works are by creating batches of events at regular time intervals
    as per configuration and delivering the micro-batches of data at every specified
    interval for further processing.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理的工作方式是根据配置定期创建事件批次，并在每个指定的时间间隔交付数据的微批处理以供进一步处理。
- en: '![](img/00011.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.jpeg)'
- en: Just like `SparkContext,` Spark Streaming has a `StreamingContext`, which is
    the main entry point for the streaming job/application. `StreamingContext` is
    dependent on `SparkContext`. In fact, the `SparkContext` can be directly used
    in the streaming job. The `StreamingContext` is similar to the `SparkContext`,
    except that `StreamingContext` also requires the program to specify the time interval
    or duration of the batching interval, which can be in milliseconds or minutes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`SparkContext`一样，Spark Streaming也有一个`StreamingContext`，它是流处理作业/应用的主要入口点。`StreamingContext`依赖于`SparkContext`。实际上，`SparkContext`可以直接用于流处理作业中。`StreamingContext`与`SparkContext`相似，不同之处在于，`StreamingContext`还要求程序指定批处理时间间隔或持续时间，单位可以是毫秒或分钟。
- en: Remember that `SparkContext` is the main point of entry, and the task scheduling
    and resource management is part of `SparkContext`, so `StreamingContext` reuses
    the logic.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`SparkContext`是主要的入口点，任务调度和资源管理是`SparkContext`的一部分，因此`StreamingContext`复用了这部分逻辑。
- en: StreamingContext
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StreamingContext
- en: '`StreamingContext` is the main entry point for streaming and essentially takes
    care of the streaming application, including checkpointing, transformations, and
    actions on DStreams of RDDs.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingContext`是流处理的主要入口点，负责流处理应用的各个方面，包括检查点、转换和对DStreams的RDD操作。'
- en: Creating StreamingContext
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 StreamingContext
- en: 'A new StreamingContext can be created in two ways:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 新的StreamingContext可以通过两种方式创建：
- en: 'Create a `StreamingContext` using an existing `SparkContext` as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用现有的`SparkContext`创建一个`StreamingContext`，如下所示：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a `StreamingContext` by providing the configuration necessary for a
    new `SparkContext` as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过提供新`SparkContext`所需的配置来创建一个`StreamingContext`，如下所示：
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A third method is to use `getOrCreate()`, which is used to either recreate
    a `StreamingContext` from checkpoint data or to create a new `StreamingContext`.
    If checkpoint data exists in the provided `checkpointPath`, then `StreamingContext`
    will be recreated from the checkpoint data. If the data does not exist, then the
    `StreamingContext` will be created by calling the provided `creatingFunc`:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三种方法是使用`getOrCreate()`，它用于从检查点数据重新创建一个`StreamingContext`，或者创建一个新的`StreamingContext`。如果在提供的`checkpointPath`中存在检查点数据，`StreamingContext`将从该检查点数据重新创建。如果数据不存在，则通过调用提供的`creatingFunc`来创建`StreamingContext`：
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Starting StreamingContext
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动 StreamingContext
- en: 'The `start()` method starts the execution of the streams defined using the
    `StreamingContext`. This essentially starts the entire streaming application:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`start()`方法启动使用`StreamingContext`定义的流的执行。它实际上启动了整个流处理应用：'
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Stopping StreamingContext
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停止 StreamingContext
- en: Stopping the `StreamingContext` stops all processing and you will have to recreate
    a new `StreamingContext` and invoke `start()` on it to restart the application.
    There are two APIs useful to stop a stream processing application.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 停止`StreamingContext`会停止所有处理，你需要重新创建一个新的`StreamingContext`并调用`start()`来重新启动应用。以下是两个用于停止流处理应用的API。
- en: 'Stop the execution of the streams immediately (do not wait for all received
    data to be processed):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 立即停止流的执行（不等待所有接收的数据被处理）：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Stop the execution of the streams, with the option of ensuring that all received
    data has been processed:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 停止流的执行，并确保所有接收的数据都已经被处理：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Input streams
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入流
- en: 'There are several types of input streams such as `receiverStream` and `fileStream`
    that can be created using the `StreamingContext` as shown in the following subsections:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种类型的输入流，例如`receiverStream`和`fileStream`，可以使用`StreamingContext`创建，具体内容见以下小节：
- en: receiverStream
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: receiverStream
- en: Create an input stream with any arbitrary user implemented receiver. It can
    be customized to meet the use cases.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个输入流，并使用任何自定义的用户实现接收器。它可以根据使用案例进行定制。
- en: Find more details at [http://spark.apache.org/docs/latest/streaming-custom-receivers.html](http://spark.apache.org/docs/latest/streaming-custom-receivers.html).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请访问[http://spark.apache.org/docs/latest/streaming-custom-receivers.html](http://spark.apache.org/docs/latest/streaming-custom-receivers.html)。
- en: 'Following is the API declaration for the `receiverStream`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`receiverStream`的API声明：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: socketTextStream
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: socketTextStream
- en: 'This creates an input stream from TCP source `hostname:port`. Data is received
    using a TCP socket and the received bytes are interpreted as UTF8 encoded `\n`
    delimited lines:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从TCP源`hostname:port`创建一个输入流。数据通过TCP套接字接收，接收到的字节将被解释为UTF8编码的`\n`分隔行：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: rawSocketStream
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: rawSocketStream
- en: Create an input stream from network source `hostname:port`, where data is received
    as serialized blocks (serialized using the Spark's serializer) that can be directly
    pushed into the block manager without deserializing them. This is the most efficient
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个输入流，从网络源`hostname:port`接收数据，数据以序列化块的形式接收（使用Spark的序列化器进行序列化），这些数据可以在不反序列化的情况下直接推送到块管理器中。这是最高效的方式。
- en: way to receive data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接收数据的方式。
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: fileStream
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: fileStream
- en: 'Create an input stream that monitors a Hadoop-compatible filesystem for new
    files and reads them using the given key-value types and input format. Files must
    be written to the monitored directory by moving them from another location within
    the same filesystem. File names starting with a dot (`.`) are ignored, so this
    is an obvious choice for the moved file names in the monitored directory. Using
    an atomic file rename function call, the filename which starts with `.` can be
    now renamed to an actual usable filename so that `fileStream` can pick it up and
    let us process the file content:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个输入流，监控一个与Hadoop兼容的文件系统，监听新文件并使用给定的键值类型和输入格式读取它们。文件必须通过从同一文件系统中的其他位置移动到监控的目录来写入。以`.`开头的文件名将被忽略，因此这对于被移动到监控目录中的文件名来说是一个明显的选择。通过使用原子文件重命名函数调用，可以将以`.`开头的文件名重命名为一个实际可用的文件名，从而使`fileStream`能够接管并处理文件内容：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: textFileStream
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: textFileStream
- en: 'Create an input stream that monitors a Hadoop-compatible filesystem for new
    files and reads them as text files (using a key as `LongWritable`, value as Text,
    and input format as `TextInputFormat`). Files must be written to the monitored
    directory by moving them from another location within the same filesystem. File
    names starting with . are ignored:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个输入流，监控一个与Hadoop兼容的文件系统，监听新文件并将它们作为文本文件读取（使用`LongWritable`作为键，Text作为值，输入格式为`TextInputFormat`）。文件必须通过从同一文件系统中的其他位置移动到监控的目录来写入。以`.`开头的文件名将被忽略：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: binaryRecordsStream
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: binaryRecordsStream
- en: 'Create an input stream that monitors a Hadoop-compatible filesystem for new
    files and reads them as flat binary files, assuming a fixed length per record,
    generating one byte array per record. Files must be written to the monitored directory
    by moving them from another location within the same filesystem. File names starting
    with `.` are ignored:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个输入流，监控一个与Hadoop兼容的文件系统，监听新文件并将它们作为平面二进制文件读取，假设每个记录的长度是固定的，并为每个记录生成一个字节数组。文件必须通过从同一文件系统中的其他位置移动到监控的目录来写入。以`.`开头的文件名将被忽略：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: queueStream
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: queueStream
- en: 'Create an input stream from a queue of RDDs. In each batch, it will process
    either one or all of the RDDs returned by the queue:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个从RDD队列读取的输入流。在每个批次中，它将处理队列返回的一个或所有RDD：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: textFileStream example
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: textFileStream 示例
- en: 'Shown in the following is a simple example of Spark Streaming using `textFileStream`.
    In this example, we create a `StreamingContext` from the spark-shell `SparkContext`
    (`sc`) and an interval of 10 seconds. This starts the `textFileStream`, which
    monitors the directory named **streamfiles** and processes any new file found
    in the directory. In this example, we are simply printing the number of elements
    in the RDD:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了一个简单的Spark Streaming示例，使用`textFileStream`。在这个例子中，我们从spark-shell的`SparkContext`（`sc`）和一个10秒的时间间隔创建一个`StreamingContext`。这将启动`textFileStream`，该流监控名为**streamfiles**的目录，并处理目录中找到的任何新文件。在这个例子中，我们仅仅打印出RDD中元素的数量：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: twitterStream example
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: twitterStream 示例
- en: 'Let us look at another example of how we can process tweets from Twitter using
    Spark Streaming:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一个如何使用Spark Streaming处理Twitter推文的例子：
- en: First, open a terminal and change the directory to `spark-2.1.1-bin-hadoop2.7`.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，打开一个终端并将目录更改为` spark-2.1.1-bin-hadoop2.7`。
- en: Create a folder `streamouts` under the `spark-2.1.1-bin-hadoop2.7` folder where
    you have spark installed. When the application runs, `streamouts` folder will
    have collected tweets to text files.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在` spark-2.1.1-bin-hadoop2.7`文件夹下创建一个`streamouts`文件夹，其中安装了Spark。当应用程序运行时，`streamouts`文件夹将收集推文并保存为文本文件。
- en: 'Download the following jars into the directory:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载以下的JAR文件到目录中：
- en: '[http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar](http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar)'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar](http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar)'
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar)'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar)'
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar)'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar)'
- en: 'Launch spark-shell with the jars needed for Twitter integration specified:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指定的 Twitter 集成所需的 jars 启动 spark-shell：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we can write a sample code. Shown in the following is the code to test
    Twitter event processing:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以编写一个示例代码。以下是用于测试 Twitter 事件处理的代码：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You will see the `streamouts` folder contains several `tweets` output in text
    files. You can now open the directory `streamouts` and check that the files contain
    `tweets`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到 `streamouts` 文件夹中包含几个以文本文件形式输出的 `tweets`。现在可以打开 `streamouts` 目录，并检查文件是否包含
    `tweets`。
- en: Discretized streams
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离散化流
- en: Spark Streaming is built on an abstraction called **Discretized Streams** referred,
    to as **DStreams**. A DStream is represented as a sequence of RDDs, with each
    RDD created at each time interval. The DStream can be processed in a similar fashion
    to regular RDDs using similar concepts such as a directed cyclic graph-based execution
    plan (Directed Acyclic Graph). Just like a regular RDD processing, the transformations
    and actions that are part of the execution plan are handled for the DStreams.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 基于一个叫做 **离散化流（Discretized Streams）** 的抽象，简称 **DStreams**。DStream
    表示为一系列 RDD，每个 RDD 在每个时间间隔内创建。DStream 可以像常规 RDD 一样使用类似的概念（如基于有向无环图的执行计划）进行处理。与常规
    RDD 处理类似，执行计划中的转换操作和行动操作也会用于 DStream。
- en: DStream essentially divides a never ending stream of data into smaller chunks
    known as micro-batches based on a time interval, materializing each individual
    micro-batch as a RDD which can then processed as a regular RDD. Each such micro-batch
    is processed independently and no state is maintained between micro-batches thus
    making the processing stateless by nature. Let's say the batch interval is 5 seconds,
    then while events are being consumed, real-time and a micro-batch are created
    at every 5-second interval and the micro-batch is handed over for further processing
    as an RDD. One of the main advantages of Spark Streaming is that the API calls
    used to process the micro-batch of events are very tightly integrated into the
    spark for APIs to provide seamless integration with the rest of the architecture.
    When a micro-batch is created, it gets turned into an RDD, which makes it a seamless
    process using spark APIs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: DStream 本质上是将源源不断的数据流基于时间间隔划分为较小的块，称为微批次（micro-batches），并将每个微批次物化为一个 RDD，然后可以像常规
    RDD 一样进行处理。每个微批次独立处理，并且不同微批次之间不会维护状态，因此其处理方式本质上是无状态的。假设批次间隔是 5 秒，那么在事件消费的同时，每隔
    5 秒就会创建一个微批次，并将该微批次作为 RDD 交给后续处理。Spark Streaming 的主要优势之一是，用于处理微批次事件的 API 调用与 Spark
    API 紧密集成，从而实现与架构其他部分的无缝集成。当创建微批次时，它会被转化为一个 RDD，使得使用 Spark API 进行处理成为一个无缝的过程。
- en: 'The `DStream` class looks like the following in the source code showing the
    most important variable, a `HashMap[Time, RDD]` pairs:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`DStream` 类在源代码中的样子如下，展示了最重要的变量，一个 `HashMap[Time, RDD]` 对：'
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Shown in the following is an illustration of a DStream comprising an RDD created
    every **T** seconds:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 DStream 的示例，展示了每 **T** 秒创建的 RDD：
- en: '![](img/00076.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00076.jpeg)'
- en: In the following example, a streaming context is created to create micro-batches
    every 5 seconds and to create an RDD, which is just like a Spark core API RDD.
    The RDDs in the DStream can be processed just like any other RDD.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，创建一个流上下文，每 5 秒创建一个微批次，并创建一个 RDD，这与 Spark 核心 API 中的 RDD 类似。DStream 中的
    RDD 可以像任何其他 RDD 一样进行处理。
- en: 'The steps involved in building a streaming application are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 构建流处理应用程序的步骤如下：
- en: Create a `StreamingContext` from the `SparkContext`.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `SparkContext` 创建一个 `StreamingContext`。
- en: Create a `DStream` from `StreamingContext`.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `StreamingContext` 创建一个 `DStream`。
- en: Provide transformations and actions that can be applied to each RDD.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供可以应用于每个 RDD 的转换和行动操作。
- en: Finally, the streaming application is started by calling `start()` on the `StreamingContext`.
    This starts the entire process of consuming and processing real-time events.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过调用`StreamingContext`上的`start()`启动流式应用程序。这将启动整个消费和处理实时事件的过程。
- en: Once the Spark Streaming application has started, no further operations can
    be added. A stopped context cannot be restarted and you have to create a new streaming
    context if such a need arises.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Spark Streaming应用程序启动，就无法再添加更多操作。已停止的上下文无法重新启动，如果有此需求，必须创建一个新的流式上下文。
- en: 'Shown in the following is an example of how to create a simple streaming job
    accessing Twitter:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何创建一个简单的流式作业来访问Twitter的示例：
- en: 'Create a `StreamingContext` from the `SparkContext`:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`SparkContext`创建一个`StreamingContext`：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Create a `DStream` from `StreamingContext`:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`StreamingContext`创建一个`DStream`：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Provide transformations and actions that can be applied to each RDD:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供可以应用于每个RDD的转换和操作：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, the streaming application is started by calling `start()` on the `StreamingContext`.
    This starts the entire process of consuming and processing real-time events:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过调用`StreamingContext`上的`start()`启动流式应用程序。这将启动整个消费和处理实时事件的过程：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Created a `DStream` of type `ReceiverInputDStream`, which is defined as an
    abstract class for defining any `InputDStream` that has to start a receiver on
    worker nodes to receive external data. Here, we are receiving from Twitter stream:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了一个类型为`ReceiverInputDStream`的`DStream`，它被定义为一个抽象类，用于定义任何需要在工作节点上启动接收器以接收外部数据的`InputDStream`。在这里，我们从Twitter流中接收数据：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you run a transformation `flatMap()` on the `twitterStream`, you get a `FlatMappedDStream`,
    as shown in the following:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在`twitterStream`上运行`flatMap()`转换，您将获得一个`FlatMappedDStream`，如下所示：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Transformations
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: Transformations on a DStream are similar to the transformations applicable to
    a Spark core RDD. Since DStream consists of RDDs, a transformation also applies
    to each RDD to generate a transformed RDD for the RDD, and then a transformed
    DStream is created. Every transformation creates a specific `DStream` derived
    class.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对`DStream`的转换类似于适用于Spark核心RDD的转换。由于DStream由RDD组成，转换也适用于每个RDD，以生成转换后的RDD，然后创建一个转换后的DStream。每个转换都会创建一个特定的`DStream`派生类。
- en: 'The following diagram shows the hierarchy of `DStream` classes starting from
    the parent `DStream` class. We can also see the different classes inheriting from
    the parent class:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了`DStream`类的层次结构，从父类`DStream`开始。我们还可以看到从父类继承的不同类：
- en: '![](img/00019.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00019.jpeg)'
- en: There are a lot of `DStream` classes purposely built for the functionality.
    Map transformations, window functions, reduce actions, and different types of
    input streams are all implemented using different class derived from `DStream`
    class.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多`DStream`类是专门为此功能构建的。Map转换、窗口函数、reduce操作和不同类型的输入流都是通过不同的`DStream`派生类实现的。
- en: 'Shown in the following is an illustration of a transformation on a base DStream
    to generate a filtered DStream. Similarly, any transformation is applicable to
    a DStream:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个关于基础DStream的转换示例，用于生成一个过滤后的DStream。类似地，任何转换都可以应用于DStream：
- en: '![](img/00382.jpeg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00382.jpeg)'
- en: Refer to the following table for the types of transformations possible.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考下表了解可能的转换类型。
- en: '| Transformation | Meaning |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 含义 |'
- en: '| `map(func)` | This applies the transformation function to each element of
    the DStream and returns a new DStream. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| `map(func)` | 这将转换函数应用于DStream的每个元素，并返回一个新的DStream。 |'
- en: '| `flatMap(func)` | This is similar to map; however, just like RDD''s `flatMap`
    versus map, using `flatMap` operates on each element and applies `flatMap`, producing
    multiple output items per each input. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| `flatMap(func)` | 这类似于map；然而，就像RDD的`flatMap`与map的区别，使用`flatMap`操作每个元素，并应用`flatMap`，每个输入生成多个输出项。
    |'
- en: '| `filter(func)` | This filters out the records of the DStream to return a
    new DStream. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| `filter(func)` | 这会过滤掉DStream中的记录，返回一个新的DStream。 |'
- en: '| `repartition(numPartitions)` | This creates more or fewer partitions to redistribute
    the data to change the parallelism. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| `repartition(numPartitions)` | 这会创建更多或更少的分区，以重新分配数据，从而改变并行度。 |'
- en: '| `union(otherStream)` | This combines the elements in two source DStreams
    and returns a new DStream. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| `union(otherStream)` | 这会将两个源DStream中的元素合并，并返回一个新的DStream。 |'
- en: '| `count()` | This returns a new DStream by counting the number of elements
    in each RDD of the source DStream. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| `count()` | 这通过计算源DStream中每个RDD的元素数量来返回一个新的DStream。 |'
- en: '| `reduce(func)` | This returns a new DStream by applying the `reduce` function
    on each element of the source DStream. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| `reduce(func)` | 这通过对源DStream的每个元素应用`reduce`函数，返回一个新的DStream。 |'
- en: '| `countByValue()` | This computes the frequency of each key and returns a
    new DStream of (key, long) pairs. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| `countByValue()` | 这计算每个键的频率，并返回一个新的DStream，其中的元素是(key, long)对。 |'
- en: '| `reduceByKey(func, [numTasks])` | This aggregates the data by key in the
    source DStream''s RDDs and returns a new DStream of (key, value) pairs. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKey(func, [numTasks])` | 这通过在源DStream的RDD上按键聚合数据，并返回一个新的DStream，其中的元素是（键，值）对。
    |'
- en: '| `join(otherStream, [numTasks])` | This joins two DStreams of *(K, V)* and
    *(K, W)* pairs and returns a new DStream of *(K, (V, W))* pairs combining the
    values from both DStreams. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `join(otherStream, [numTasks])` | 这将两个DStream的*(K, V)*和*(K, W)*对连接在一起，并返回一个新的DStream，它的元素是*(K,
    (V, W))*对，合并了两个DStream中的值。 |'
- en: '| `cogroup(otherStream, [numTasks])` | `cogroup()`, when called on a DStream
    of *(K, V)* and *(K, W)* pairs, will return a new DStream of *(K, Seq[V], Seq[W])*
    tuples. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `cogroup(otherStream, [numTasks])` | 当在*(K, V)*和*(K, W)*对的DStream上调用`cogroup()`时，它将返回一个新的DStream，其中的元素是*(K,
    Seq[V], Seq[W])*元组。 |'
- en: '| `transform(func)` | This applies a transformation function on each RDD of
    the source DStream and returns a new DStream. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| `transform(func)` | 这在源DStream的每个RDD上应用一个转换函数，并返回一个新的DStream。 |'
- en: '| `updateStateByKey(func)` | This updates the state for each key by applying
    the given function on the previous state of the key and the new values for the
    key. Typically, it used to maintain a state machine. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| `updateStateByKey(func)` | 这通过在每个键的先前状态和该键的新值上应用给定的函数，更新每个键的状态。通常用于维持一个状态机。
    |'
- en: Window operations
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口操作
- en: 'Spark Streaming provides windowed processing, which allows you to apply transformations
    over a sliding window of events. The sliding window is created over an interval
    specified. Every time the window slides over a source DStream, the source RDDs,
    which fall within the window specification, are combined and operated upon to
    generate the windowed DStream. There are two parameters that need to be specified
    for the window:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 提供了窗口处理功能，允许你在滑动窗口中的事件上应用转换。滑动窗口是在指定的间隔上创建的。每当窗口滑过一个源DStream时，符合窗口规范的源RDD会被合并并进行操作，生成窗口化的DStream。窗口有两个参数需要指定：
- en: '**Window length: This specifies the length in interval considered as the window**'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**窗口长度：这是指定的窗口考虑的时间间隔长度**'
- en: 'Sliding interval: This is the interval at which the window is created'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滑动间隔：这是窗口创建的间隔。
- en: The window length and the sliding interval must both be a multiple of the block
    interval.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口长度和滑动间隔必须都是块间隔的倍数。
- en: 'Shown in the following is an illustration shows a DStream with a sliding window
    operation showing how the old window (dotted line rectangle) slides by one interval
    to the right into the new window (solid line rectangle):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示了一个示意图，显示了一个DStream的滑动窗口操作，演示了旧窗口（虚线矩形）如何滑动一个间隔到右边，进入新的窗口（实线矩形）：
- en: '![](img/00028.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.jpeg)'
- en: Some of the common window operation are as follows.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的窗口操作如下。
- en: '| Transformation | Meaning |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 含义 |'
- en: '| `window(windowLength, slideInterval)` | This creates a window on the source
    DStream and returns the same as a new DStream. |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| `window(windowLength, slideInterval)` | 这在源DStream上创建一个窗口，并返回相同的DStream作为新的DStream。
    |'
- en: '| `countByWindow(windowLength, slideInterval)` | This returns count of elements
    in the DStream by applying a sliding window. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| `countByWindow(windowLength, slideInterval)` | 这通过应用滑动窗口返回DStream中元素的计数。
    |'
- en: '| `reduceByWindow(func, windowLength, slideInterval)` | This returns a new
    DStream by applying the reduce function on each element of the source DStream
    after creating a sliding window of length `windowLength`. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByWindow(func, windowLength, slideInterval)` | 这通过在源DStream的每个元素上应用reduce函数，并在创建一个长度为`windowLength`的滑动窗口后，返回一个新的DStream。
    |'
- en: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | This
    aggregates the data by key in the window applied to the source DStream''s RDDs
    and returns a new DStream of (key, value) pairs. The computation is provided by
    function `func`. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | 这通过在源DStream的RDD上应用窗口进行按键聚合，并返回一个新的DStream，其中的元素是（键，值）对。计算由`func`函数提供。
    |'
- en: '| `reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`
    | This aggregates the data by key in the window applied to the source DStream''s
    RDDs and returns a new DStream of (key, value) pairs. The key difference between
    the preceding function and this one is the `invFunc`, which provides the computation
    to be done at the beginning of the sliding window. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`
    | 该函数通过键对源DStream的RDD进行窗口聚合，并返回一个新的包含（键，值）对的DStream。与前面的函数的主要区别在于`invFunc`，它提供了在滑动窗口开始时需要执行的计算。
    |'
- en: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | This computes
    the frequency of each key and returns a new DStream of (key, long) pairs within
    the sliding window as specified. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | 该函数计算每个键的频率，并返回一个新的包含（键，长整型）对的DStream，该DStream符合指定的滑动窗口。
    |'
- en: Let us look at the Twitter stream example in more detail. Our goal is to print
    the top five words used in tweets streamed every five seconds, using a window
    of length 15 seconds, sliding every 10 seconds. Hence, we can get the top five
    words in 15 seconds.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下Twitter流的示例。我们的目标是每5秒打印推文中使用的前五个单词，使用一个长度为15秒、每10秒滑动一次的窗口。因此，我们可以在15秒内获取前五个单词。
- en: 'To run this code, follow these steps:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此代码，请按照以下步骤操作：
- en: First, open a terminal and change directory to `spark-2.1.1-bin-hadoop2.7`.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，打开终端并切换到`spark-2.1.1-bin-hadoop2.7`目录。
- en: Create a folder `streamouts` under the `spark-2.1.1-bin-hadoop2.7` folder where
    you have spark installed. When the application runs, the `streamouts` folder will
    have collected tweets to text files.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`spark-2.1.1-bin-hadoop2.7`文件夹下创建一个`streamouts`文件夹，该文件夹是你安装spark的地方。当应用程序运行时，`streamouts`文件夹将收集推文并保存为文本文件。
- en: 'Download the following jars into the directory:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下jar下载到目录中：
- en: '[http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar](http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar)'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar](http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar)'
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar)'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar)'
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar)'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar)'
- en: 'Launch spark-shell with the jars needed for Twitter integration specified:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指定所需Twitter集成的jar启动spark-shell：
- en: '[PRE23]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we can write the code. Shown in the following is the code used to test
    Twitter event processing:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以编写代码。下面是用于测试Twitter事件处理的代码：
- en: '[PRE24]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is displayed on the console every 15 seconds and looks something
    like the following:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出每15秒显示在控制台上，输出结果类似如下：
- en: '[PRE25]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Stateful/stateless transformations
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态/无状态转换
- en: 'As seen previously, Spark Streaming uses a concept of DStreams, which are essentially
    micro-batches of data created as RDDs. We also saw types of transformations that
    are possible on DStreams. The transformations on DStreams can be grouped into
    two types: **Stateless transformations** and **Stateful transformations.**'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Spark Streaming使用DStream的概念，DStream本质上是以RDD形式创建的数据微批次。我们还看到了DStream上可能进行的转换类型。DStream的转换可以分为两种类型：**无状态转换**和**有状态转换**。
- en: In Stateless transformations, the processing of each micro-batch of data does
    not depend on the previous batches of data. Thus, this is a stateless transformation,
    with each batch doing its own processing independently of anything that occurred
    prior to this batch.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在无状态转换中，每个数据微批的处理不依赖于先前批次的数据。因此，这是一种无状态转换，每个批次独立处理，而不依赖于之前发生的任何事情。
- en: In Stateful transformations, the processing of each micro-batch of data depends
    on the previous batches of data either fully or partially. Thus, this is a stateful
    transformation, with each batch considering what happened prior to this batch
    and then using the information while computing the data in this batch.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在有状态转换中，每个微批数据的处理都完全或部分依赖于之前的数据批次。因此，这是一个有状态转换，每个批次在计算当前批次数据时都会考虑之前发生的事情，并利用这些信息。
- en: Stateless transformations
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无状态转换
- en: Stateless transformations transform one DStream to another by applying transformations
    to each of the RDDs within the DStream. Transformations such as `map()`, `flatMap()`,
    `union()`, `join()`, and `reduceByKey` are all examples of stateless transformations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态转换通过对DStream中的每个RDD应用转换，将一个DStream转换为另一个DStream。像`map()`、`flatMap()`、`union()`、`join()`和`reduceByKey`等转换都属于无状态转换的例子。
- en: 'Shown in the following is an illustration showing a `map()` transformation
    on `inputDStream` to generate a new `mapDstream`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个插图，展示了对`inputDStream`进行`map()`转换以生成新的`mapDstream`：
- en: '![](img/00210.jpeg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00210.jpeg)'
- en: Stateful transformations
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态转换
- en: Stateful transformations operate on a DStream, but the computations depend on
    the previous state of processing. Operations such as `countByValueAndWindow`,
    `reduceByKeyAndWindow` , `mapWithState`, and `updateStateByKey` are all examples
    of stateful transformations. In fact, all window-based transformations are all
    stateful because, by the definition of window operations, we need to keep track
    of the window length and sliding interval of DStream.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有状态转换操作一个DStream，但计算依赖于处理的前一状态。像`countByValueAndWindow`、`reduceByKeyAndWindow`、`mapWithState`和`updateStateByKey`等操作都是有状态转换的例子。事实上，所有基于窗口的转换都是有状态的，因为根据窗口操作的定义，我们需要跟踪DStream的窗口长度和滑动间隔。
- en: Checkpointing
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查点
- en: Real-time streaming applications are meant to be long running and resilient
    to failures of all sorts. Spark Streaming implements a checkpointing mechanism
    that maintains enough information to recover from failures.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 实时流式应用程序旨在长时间运行，并能够容忍各种故障。Spark Streaming实现了一种检查点机制，能够保留足够的信息，以便在发生故障时进行恢复。
- en: 'There are two types of data that needs to be checkpointed:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进行检查点的数据有两种类型：
- en: Metadata checkpointing
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据检查点
- en: Data checkpointing
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据检查点
- en: 'Checkpointing can be enabled by calling `checkpoint()` function on the `StreamingContext`
    as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在`StreamingContext`上调用`checkpoint()`函数来启用检查点功能，如下所示：
- en: '[PRE26]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Specifies the directory where the checkpoint data will be reliably stored.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 指定检查点数据将可靠存储的目录。
- en: Note that this must be a fault-tolerant file system like HDFS.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这必须是容错的文件系统，如HDFS。
- en: 'Once checkpoint directory is set, any DStream can be checkpointed into the
    directory based on a specified interval. Looking at the Twitter example, we can
    checkpoint each DStream every 10 seconds into the directory `checkpoints`:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了检查点目录，任何DStream都可以根据指定的间隔将数据检查到该目录中。以Twitter示例为例，我们可以每10秒将每个DStream检查到`checkpoints`目录中：
- en: '[PRE27]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `checkpoints` directory looks something like the following after few seconds,
    showing the metadata as well as the RDDs and the `logfiles` are maintained as
    part of the checkpointing:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`checkpoints`目录在几秒钟后看起来像下面这样，显示了元数据以及RDD和`logfiles`作为检查点的一部分：'
- en: '![](img/00246.jpeg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00246.jpeg)'
- en: Metadata checkpointing
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元数据检查点
- en: '**Metadata checkpointing** saves information defining the streaming operations,
    which are represented by a **Directed Acyclic Graph** (**DAG**) to the HDFS. This
    can be used to recover the DAG, if there is a failure and the application is restarted.
    The driver restarts and reads the metadata from HDFS, and rebuilds the DAG and
    recovers all the operational state before the crash.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**元数据检查点**保存定义流式操作的信息，这些操作由**有向无环图**（**DAG**）表示，并将其保存到HDFS中。这些信息可以在发生故障并重新启动应用程序时用于恢复DAG。驱动程序会重新启动并从HDFS读取元数据，重建DAG并恢复崩溃前的所有操作状态。'
- en: 'Metadata includes the following:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据包括以下内容：
- en: '**Configuration**: the configuration that was used to create the streaming
    application'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置**：用于创建流式应用程序的配置'
- en: '**DStream operations**: the set of DStream operations that define the streaming
    application'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DStream操作**：定义流式应用程序的DStream操作集'
- en: '**Incomplete batches**: batches whose jobs are queued but have not completed
    yet'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不完整批次**：排队中的作业但尚未完成的批次'
- en: Data checkpointing
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据检查点
- en: Data checkpointing saves the actual RDDs to HDFS so that, if there is a failure
    of the Streaming application, the application can recover the checkpointed RDDs
    and continue from where it left off. While streaming application recovery is a
    good use case for the data checkpointing, checkpointing also helps in achieving
    better performance whenever some RDDs are lost because of cache cleanup or loss
    of an executor by instantiating the generated RDDs without a need to wait for
    all the parent RDDs in the lineage (DAG) to be recomputed.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 数据检查点将实际的RDD保存到HDFS中，这样，如果流应用程序发生故障，应用程序可以恢复检查点的RDD并从上次中断的地方继续。虽然流应用程序恢复是数据检查点的一个典型应用场景，但检查点也有助于在某些RDD因缓存清理或执行器丢失而丢失时，通过实例化生成的RDD而无需等待所有父RDD在DAG（有向无环图）中重新计算，从而实现更好的性能。
- en: 'Checkpointing must be enabled for applications with any of the following requirements:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有以下任何要求的应用程序，必须启用检查点：
- en: '**Usage of stateful transformations**: If either `updateStateByKey` or `reduceByKeyAndWindow`
    (with inverse function) is used in the application, then the checkpoint directory
    must be provided to allow for periodic RDD checkpointing.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有状态转换的使用**：如果应用程序中使用了`updateStateByKey`或`reduceByKeyAndWindow`（带有逆向函数），则必须提供检查点目录，以允许周期性地进行RDD检查点。'
- en: '**Recovering from failures of the driver running the application**: Metadata
    checkpoints are used to recover with progress information.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从驱动程序故障中恢复**：元数据检查点用于通过进度信息进行恢复。'
- en: If your streaming application does not have the stateful transformations, then
    the application can be run without enabling checkpointing.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的流应用程序没有使用有状态转换，那么可以在不启用检查点的情况下运行该应用程序。
- en: There might be loss of data received but not processed yet in your streaming
    application.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 您的流应用程序可能会丢失接收到但尚未处理的数据。
- en: Note that checkpointing of RDDs incurs the cost of saving each RDD to storage.
    This may cause an increase in the processing time of those batches where RDDs
    get checkpointed. Hence, the interval of checkpointing needs to be set carefully
    so as not to cause performance issues. At tiny batch sizes (say 1 second), checkpointing
    too frequently every tiny batch may significantly reduce operation throughput.
    Conversely, checkpointing too infrequently causes the lineage and task sizes to
    grow, which may cause processing delays as the amount of data to be persisted
    is large.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，RDD的检查点会产生将每个RDD保存到存储中的成本。这可能导致检查点化的RDD所在的批次处理时间增加。因此，必须小心设置检查点的间隔，以避免引发性能问题。在非常小的批次（例如1秒）下，检查点每个微小批次的频率过高，可能会显著降低操作吞吐量。相反，检查点的频率过低会导致血统和任务大小增长，这可能会引起处理延迟，因为需要持久化的数据量较大。
- en: For stateful transformations that require RDD checkpointing, the default interval
    is a multiple of the batch interval that is at least 10 seconds.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要RDD检查点的有状态转换，默认间隔是批处理间隔的倍数，至少为10秒。
- en: A checkpoint interval of 5 to 10 sliding intervals of a DStream is a good setting
    to start with.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: DStream的5到10个滑动间隔的检查点间隔是一个良好的初始设置。
- en: Driver failure recovery
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 驱动程序故障恢复
- en: Driver failure recovery can be accomplished by using `StreamingContext.getOrCreate()`
    to either initialize `StreamingContext` from an existing checkpoint or to create
    a new StreamingContext.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序故障恢复可以通过使用`StreamingContext.getOrCreate()`来实现，该方法可以从现有的检查点初始化`StreamingContext`或创建一个新的`StreamingContext`。
- en: 'The two conditions for a streaming application when started are as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 启动流应用程序时需要满足以下两个条件：
- en: When the program is being started for the first time, it needs to create a new
    `StreamingContext`, set up all the streams, and then call `start()`
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当程序第一次启动时，需要创建一个新的`StreamingContext`，设置所有的流，然后调用`start()`。
- en: When the program is being restarted after failure, it needs to initialize a
    `StreamingContext` from the checkpoint data in the checkpoint directory and then
    call `start()`
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当程序在失败后重新启动时，需要从检查点目录中的检查点数据初始化一个`StreamingContext`，然后调用`start()`。
- en: 'We will implement a function `createStreamContext()`, which creates the `StreamingContext`
    and sets up the various DStreams to parse the tweets and generate the top five
    tweet hashtags every 15 seconds using a window. But instead of calling `createStreamContext(`)
    and then calling `ssc.start()` , we will call `getOrCreate()` so that if the `checkpointDirectory`
    exists, then the context will be recreated from the checkpoint data. If the directory
    does not exist (the application is running for the first time), then the function
    `createStreamContext()` will be called to create a new context and set up the
    DStreams:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个函数 `createStreamContext()`，该函数创建 `StreamingContext` 并设置各种 DStreams 来解析推文，并使用窗口每
    15 秒生成前五个推文标签。但是，我们不会调用 `createStreamContext()` 然后调用 `ssc.start()`，而是会调用 `getOrCreate()`，这样如果
    `checkpointDirectory` 存在，则将从检查点数据重新创建上下文。如果目录不存在（应用程序首次运行），则将调用 `createStreamContext()`
    来创建新的上下文并设置 DStreams：
- en: '[PRE28]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Shown in the following is the code showing the definition of the function and
    how `getOrCreate()` can be called:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示的代码显示了函数定义以及如何调用 `getOrCreate()`：
- en: '[PRE29]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Interoperability with streaming platforms (Apache Kafka)
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与流处理平台（Apache Kafka）的互操作性
- en: Spark Streaming has very good integration with Apache Kafka, which is the most
    popular messaging platform currently. Kafka integration has several approaches,
    and the mechanism has evolved over time to improve the performance and reliability.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 与 Apache Kafka 集成非常好，后者是目前最流行的消息平台。Kafka 集成有多种方法，并且随着时间的推移机制已经演变，以提高性能和可靠性。
- en: 'There are three main approaches for integrating Spark Streaming with Kafka:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种主要方法可以将 Spark Streaming 与 Kafka 集成：
- en: Receiver-based approach
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于接收器的方法
- en: Direct stream approach
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接流处理方法
- en: Structured streaming
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: Receiver-based approach
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于接收器的方法
- en: The receiver-based approach was the first integration between Spark and Kafka.
    In this approach, the driver starts receivers on the executors that pull data
    using high-level APIs, from Kafka brokers. Since receivers are pulling events
    from Kafka brokers, receivers update the offsets into Zookeeper, which is also
    used by Kafka cluster. The key aspect is the usage of a **WAL** (**Write Ahead
    Log**), which the receiver keeps writing to as it consumes data from Kafka. So,
    when there is a problem and executors or receivers are lost or restarted, the
    WAL can be used to recover the events and process them. Hence, this log-based
    design provides both durability and consistency.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 基于接收器的方法是 Spark 和 Kafka 之间的第一个集成方法。在此方法中，驱动程序在执行器上启动接收器，使用高级 API 从 Kafka brokers
    拉取数据。由于接收器从 Kafka brokers 拉取事件，接收器会将偏移量更新到 Zookeeper 中，这也被 Kafka 集群使用。关键之处在于使用
    **WAL**（**Write Ahead Log**），接收器在消费 Kafka 数据时持续写入 WAL。因此，当存在问题并且执行器或接收器丢失或重启时，可以使用
    WAL 恢复事件并处理它们。因此，这种基于日志的设计提供了持久性和一致性。
- en: Each receiver creates an input DStream of events from a Kafka topic while querying
    Zookeeper for the Kafka topics, brokers, offsets, and so on. After this, the discussion
    we had about DStreams in previous sections comes into play.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 每个接收器都会创建一个来自 Kafka 主题的输入 DStream，并查询 Zookeeper 获取 Kafka 主题、brokers、偏移量等信息。在此之后，我们之前讨论的
    DStreams 将发挥作用。
- en: Long-running receivers make parallelism complicated as the workload is not going
    to be properly distributed as we scale the application. Dependence on HDFS is
    also a problem along with the duplication of write operations. As for the reliability
    needed for exactly once paradigm of processing, only the idempotent approach will
    work. The reason why a transactional approach, will not work in the receiver-based
    approach is that there is no way to access the offset ranges from the HDFS location
    or Zookeeper.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间运行的接收器使得并行性复杂化，因为随着应用程序的扩展，工作负载不会被正确分配。依赖于 HDFS 也是一个问题，还有写操作的重复。至于处理的幂等性所需的可靠性，只有幂等方法才能起作用。基于接收器的方法之所以无法使用事务方法，是因为无法从
    HDFS 位置或 Zookeeper 访问偏移范围。
- en: The receiver-based approach works with any messaging system, so it's more general
    purpose.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 基于接收器的方法适用于任何消息系统，因此更通用。
- en: 'You can create a receiver-based stream by invoking the `createStream()` API
    as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调用 `createStream()` API 来创建基于接收器的流，如下所示：
- en: '[PRE30]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Shown in the following is an example of creating a receiver-based stream that
    pulls messages from Kafka brokers:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下显示了一个示例，展示了如何创建一个从 Kafka brokers 拉取消息的基于接收器的流：
- en: '[PRE31]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Shown in the following is an illustration of how the driver launches receivers
    on executors to pull data from Kafka using the high-level API. The receivers pull
    the topic offset ranges from the Kafka Zookeeper cluster and then also update
    Zookeeper as they pull events from the brokers:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了驱动程序如何在执行器上启动接收器，通过高级API从Kafka拉取数据。接收器从Kafka Zookeeper集群中拉取主题偏移范围，然后在从代理拉取事件时更新Zookeeper：
- en: '![](img/00078.jpeg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00078.jpeg)'
- en: Direct stream
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接流
- en: The direct stream based approach is the newer approach with respect to Kafka
    integration and works by using the driver to connect to the brokers directly and
    pull events. The key aspect is that using direct stream API, Spark tasks work
    on a 1:1 ratio when looking at spark partition to Kafka topic/partition. No dependency
    on HDFS or WAL makes it flexible. Also, since now we can have direct access to
    offsets, we can use idempotent or transactional approach for exactly once processing.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 基于直接流的方法是相较于Kafka集成的新方法，它通过驱动程序直接连接到代理并拉取事件。关键点在于，使用直接流API时，Spark任务与Kafka主题/分区之间是1:1的关系。这种方法不依赖于HDFS或WAL，使其更加灵活。而且，由于我们现在可以直接访问偏移量，可以使用幂等或事务性的方法进行精确一次处理。
- en: Create an input stream that directly pulls messages from Kafka brokers without
    using any receiver. This stream can guarantee that each message from Kafka is
    included in transformations exactly once.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个输入流，直接从Kafka代理拉取消息，无需使用任何接收器。此流可以保证从Kafka来的每条消息都在转换中仅出现一次。
- en: 'Properties of a direct stream are as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 直接流的属性如下：
- en: '**No receivers**: This stream does not use any receiver, but rather directly
    queries Kafka.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有接收器**：此流不使用任何接收器，而是直接查询Kafka。'
- en: '**Offsets**: This does not use Zookeeper to store offsets, and the consumed
    offsets are tracked by the stream itself. You can access the offsets used in each
    batch from the generated RDDs.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏移量**：此方法不使用Zookeeper来存储偏移量，消费的偏移量由流本身跟踪。你可以从生成的RDD中访问每个批次使用的偏移量。'
- en: '**Failure recovery**: To recover from driver failures, you have to enable checkpointing
    in the `StreamingContext`.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障恢复**：为了从驱动程序故障中恢复，你必须在`StreamingContext`中启用检查点。'
- en: '**End-to-end semantics**: This stream ensures that every record is effectively
    received and transformed exactly once, but gives no guarantees on whether the
    transformed data are outputted exactly once.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端语义**：此流确保每条记录都被有效接收并且转换仅发生一次，但无法保证转换后的数据是否准确地输出一次。'
- en: 'You can create a direct stream by using KafkaUtils, `createDirectStream()`
    API as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用KafkaUtils，`createDirectStream()` API来创建直接流，如下所示：
- en: '[PRE32]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Shown in the following is an example of a direct stream created to pull data
    from Kafka topics and create a DStream:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个示例，说明如何创建一个直接流，从Kafka主题拉取数据并创建DStream：
- en: '[PRE33]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The direct stream API can only be used with Kafka, so this is not a general
    purpose approach.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 直接流API只能与Kafka一起使用，因此这不是一种通用方法。
- en: 'Shown in the following is an illustration of how the driver pulls offset information
    from Zookeeper and directs the executors to launch tasks to pull events from brokers
    based on the offset ranges prescribed by the driver:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了驱动程序如何从Zookeeper拉取偏移量信息，并指导执行器根据驱动程序指定的偏移范围启动任务，从Kafka代理拉取事件：
- en: '![](img/00118.jpeg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.jpeg)'
- en: Structured streaming
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: Structured streaming is new in Apache Spark 2.0+ and is now in GA from Spark
    2.2 release. You will see details in the next section along with examples of how
    to use structured streaming.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理是Apache Spark 2.0+中的新特性，从Spark 2.2版本开始已进入GA阶段。接下来你将看到详细信息，并附有如何使用结构化流处理的示例。
- en: For more details on the Kafka integration in structured streaming, refer to
    [https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 关于结构化流处理中的Kafka集成的更多详细信息，请参阅[https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)。
- en: 'An example of how to use Kafka source stream in structured streaming is as
    follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在结构化流处理中使用Kafka源流的示例如下：
- en: '[PRE34]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'An example of how to use Kafka source instead of source stream (in case you
    want more batch analytics approach) is as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用Kafka源流而不是源流（如果你需要更多批处理分析方法）的示例如下：
- en: '[PRE35]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Structured streaming
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: Structured streaming is a scalable and fault-tolerant stream processing engine
    built on top of Spark SQL engine. This brings stream processing and computations
    closer to batch processing, rather than the DStream paradigm and challenges involved
    with Spark streaming APIs at this time. The structured streaming engine takes
    care of several challenges like exactly-once stream processing, incremental updates
    to results of processing, aggregations, and so on.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理是构建在Spark SQL引擎之上的一个可扩展且具有容错性的流处理引擎。这使得流处理和计算更加接近批处理，而不是当前Spark流处理API所面临的DStream范式及其挑战。结构化流引擎解决了多个挑战，例如精准一次流处理、增量更新处理结果、聚合等。
- en: The structured streaming API also provides the means to tackle a big challenge
    of Spark streaming, that is, Spark streaming processes incoming data in micro-batches
    and uses the received time as a means of splitting the data, thus not considering
    the actual event time of the data. The structured streaming allows you to specify
    such an event time in the data being received so that any late coming data is
    automatically handled.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理API还提供了解决Spark流处理的一个重大挑战的方法，即，Spark流处理是以微批次方式处理传入数据，并使用接收时间作为拆分数据的依据，因此并不考虑数据的实际事件时间。结构化流处理允许你在接收到的数据中指定事件时间，从而自动处理任何迟到的数据。
- en: The structured streaming is GA in Spark 2.2, and the APIs are marked GA. Refer
    to [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理在Spark 2.2中已经是GA（一般可用版），并且API已标记为GA。参考[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)。
- en: 'The key idea behind structured streaming is to treat a live data stream as
    an unbounded table being appended to continuously as events are processed from
    the stream. You can then run computations and SQL queries on this unbounded table
    as you normally do on batch data. A Spark SQL query for instance will process
    the unbounded table:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理的核心思想是将实时数据流视为一个无限制的表，随着事件的处理，该表会不断地被附加新的数据。你可以像对待批量数据一样，对这个无限制的表进行计算和SQL查询。例如，Spark
    SQL查询会处理这个无限制的表：
- en: '![](img/00348.jpeg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00348.jpeg)'
- en: As the DStream keeps changing with time, more and more data will be processed
    to generate the results. Hence, the unbounded input table is used to generate
    a result table. The output or results table can be written to an external sink
    known as **Output**.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 随着DStream随时间变化，越来越多的数据将被处理以生成结果。因此，无限制输入表被用来生成结果表。输出或结果表可以写入被称为**输出**的外部存储。
- en: 'The **Output** is what gets written out and can be defined in a different mode:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**是指写入的内容，可以通过不同模式进行定义：'
- en: '**Complete mode**: The entire updated result table will be written to the external
    storage. It is up to the storage connector to decide how to handle the writing
    of the entire table.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整模式**：整个更新后的结果表将写入外部存储。由存储连接器决定如何处理整个表的写入。'
- en: '**Append mode**: Only any new rows appended to the result table since the last
    trigger will be written to the external storage. This is applicable only on the
    queries where existing rows in the result table are not expected to change.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**追加模式**：仅将自上次触发以来附加到结果表中的新行写入外部存储。这仅适用于那些预期结果表中的现有行不会发生变化的查询。'
- en: '**Update mode**: Only the rows that were updated in the result table since
    the last trigger will be written to the external storage. Note that this is different
    from the complete mode in that this mode only outputs the rows that have changed
    since the last trigger. If the query doesn''t contain aggregations, it will be
    equivalent to Append mode.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新模式**：自上次触发以来仅更新的结果表中的行将写入外部存储。请注意，这与完整模式不同，因为该模式只输出自上次触发以来发生变化的行。如果查询不包含聚合操作，那么它将等同于追加模式。'
- en: 'Shown in the following is an illustration of the output from the unbounded
    table:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示的是来自无限制表的输出示意图：
- en: '![](img/00001.jpeg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00001.jpeg)'
- en: We will show an example of creating a Structured streaming query by listening
    to input on localhost port 9999.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示一个通过监听本地主机端口9999创建结构化流查询的示例。
- en: 'If using a Linux or Mac, it''s easy to start a simple server on port 9999:
    nc -lk 9999.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用Linux或Mac，启动一个简单的服务器并监听端口9999非常简单：nc -lk 9999。
- en: 'Shown in the following is an example where we start by creating an `inputStream`
    calling SparkSession''s `readStream` API and then extracting the words from the
    lines. Then we group the words and count the occurrences before finally writing
    the results to the output stream:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，我们首先创建一个`inputStream`，调用SparkSession的`readStream` API，然后从行中提取单词。接着，我们对单词进行分组并计算出现次数，最后将结果写入输出流：
- en: '[PRE36]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As you keep typing words in the terminal, the query keeps updating and generating
    results which are printed on the console:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 当你继续在终端输入时，查询会不断更新并生成结果，这些结果会打印到控制台：
- en: '[PRE37]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Handling Event-time and late data
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理事件时间和迟到数据
- en: '**Event time** is the time inside the data itself. Traditional Spark Streaming
    only handled time as the received time for the DStream purposes, but this is not
    enough for many applications where we need the event time. For example, if you
    want to get the number of times hashtag appears in a tweet every minute, then
    you should want to use the time when the data was generated, not when Spark receives
    the event. To get event time into the mix, it is very easy to do so in structured
    streaming by considering the event time as a column in the row/event. This allows
    window-based aggregations to be run using the event time rather than the received
    time. Furthermore, this model naturally handles data that has arrived later than
    expected based on its event time. Since Spark is updating the result table, it
    has full control over updating old aggregates when there is late data as well
    as cleaning up old aggregates to limit the size of intermediate state data. There
    is also support for watermarking event streams, which allows the user to specify
    the threshold of late data and allows the engine to accordingly clean up the old
    state.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**事件时间**是数据本身内部的时间。传统的Spark Streaming只将时间视为接收时间，用于DStream的目的，但对于许多应用程序来说，这不足以满足需求，我们需要的是事件时间。例如，如果你想要每分钟计算推文中某个标签出现的次数，那么你应该使用数据生成的时间，而不是Spark接收事件的时间。为了在结构化流处理中引入事件时间，可以将事件时间视为行/事件中的一列。这使得基于窗口的聚合可以使用事件时间而非接收时间来运行。此外，这种模型自然处理比预期晚到达的数据，因为它基于事件时间进行处理。由于Spark正在更新结果表，它完全控制在出现迟到数据时如何更新旧的聚合，并清理旧的聚合以限制中间状态数据的大小。同时，还支持对事件流进行水印处理，允许用户指定迟到数据的阈值，并使引擎根据该阈值清理旧状态。'
- en: Watermarks enable the engine to track the current event times and determine
    whether the event needs to be processed or has been already processed by checking
    the threshold of how late data can be received. For instance, if the event time
    is denoted by `eventTime` and the threshold interval of late arriving data is
    `lateThreshold`, then by checking the difference between the `max(eventTime) -
    lateThreshold` and comparing with the specific window starting at time T, the
    engine can determine if the event can be considered for processing in this window
    or not.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 水印使引擎能够追踪当前事件时间，并通过检查接收数据的迟到阈值，判断事件是否需要处理或已经处理。例如，假设事件时间用`eventTime`表示，迟到数据的阈值间隔为`lateThreshold`，则通过检查`max(eventTime)
    - lateThreshold`与从时间T开始的特定窗口的比较，引擎可以确定该事件是否可以在该窗口中进行处理。
- en: 'Shown in the following is an extension of the preceding example on structured
    streaming listening on port 9999\. Here we are enabling `Timestamp` as part of
    the input data so that we can do Window operations on the unbounded table to generate
    results:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示的是前面示例的扩展，演示了结构化流处理监听端口9999的情况。在这里，我们启用了`Timestamp`作为输入数据的一部分，这样我们就可以在无限制的表上执行窗口操作以生成结果：
- en: '[PRE38]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Fault tolerance semantics
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容错语义
- en: Delivering **end-to-end exactly once semantics** was one of the key goals behind
    the design of Structured streaming, which implements the Structured streaming
    sources, the output sinks, and the execution engine to reliably track the exact
    progress of the processing so that it can handle any kind of failure by restarting
    and/or reprocessing. Every streaming source is assumed to have offsets (similar
    to Kafka offsets) to track the read position in the stream. The engine uses checkpointing
    and write ahead logs to record the offset range of the data being processed in
    each trigger. The streaming sinks are designed to be idempotent for handling reprocessing.
    Together, using replayable sources and idempotent sinks, Structured streaming
    can ensure end-to-end exactly once semantics under any failure.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 **端到端精确一次语义** 是设计结构化流处理的关键目标之一，它通过实现结构化流处理源、输出端和执行引擎，可靠地跟踪处理的精确进度，从而在发生任何类型的失败时通过重启和/或重新处理来处理。每个流源都假定具有偏移量（类似于
    Kafka 偏移量），用来跟踪流中的读取位置。引擎使用检查点和预写日志来记录每个触发器中正在处理数据的偏移范围。流输出端被设计为幂等性，以便处理重新处理操作。通过使用可重放的流源和幂等性输出端，结构化流处理可以确保在任何失败情况下实现端到端的精确一次语义。
- en: Remember that exactly once the paradigm is more complicated in traditional streaming
    using some external database or store to maintain the offsets.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，传统流处理中使用外部数据库或存储来维护偏移量时，"精确一次"的范式更加复杂。
- en: 'The structured streaming is still evolving and has several challenges to overcome
    before it can be widely used. Some of them are as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理仍在发展中，面临一些挑战需要克服，才能广泛应用。以下是其中的一些挑战：
- en: Multiple streaming aggregations are not yet supported on streaming datasets
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流数据集上，不支持多重流聚合操作。
- en: Limiting and taking first *N* rows is not supported on streaming datasets
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流数据集上，不支持限制或获取前 *N* 行操作。
- en: Distinct operations on streaming datasets are not supported
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流数据集上的去重操作不被支持。
- en: Sorting operations are supported on streaming datasets only after an aggregation
    step is performed and that too exclusively when in complete output mode
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有在执行聚合步骤后，并且仅在完全输出模式下，才支持对流数据集进行排序操作。
- en: Any kind of join operations between two streaming datasets are not yet supported
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前尚不支持两个流数据集之间的任何类型连接操作。
- en: Only a few types of sinks - file sink and for each sink are supported
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前只支持少数几种类型的输出端 - 文件输出端和每个输出端。
- en: Summary
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: In this chapter, we discussed the concepts of the stream processing systems,
    Spark streaming, DStreams of Apache Spark, what DStreams are, DAGs and lineages
    of DStreams, Transformations, and Actions. We also looked at window concept of
    stream processing. We also looked at a practical examples of consuming tweets
    from Twitter using Spark Streaming.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了流处理系统的概念，Spark 流处理、Apache Spark 的 DStreams、DStreams 的定义、DAGs 和 DStreams
    的血统、转换和动作。我们还探讨了流处理中的窗口概念。最后，我们还看了一个实际示例，使用 Spark Streaming 消费 Twitter 中的推文。
- en: In addition, we looked at receiver-based and direct stream approaches of consuming
    data from Kafka. In the end, we also looked at the new structured streaming, which
    promises to solve many of the challenges such as fault tolerance and exactly once
    semantics on the stream. We also discussed how structured streaming also simplifies
    the integration with messaging systems such as Kafka or other messaging systems.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还研究了基于接收者和直接流式处理的两种从 Kafka 消费数据的方法。最后，我们还看了新型的结构化流处理，它承诺解决许多挑战，比如流处理中的容错性和"精确一次"语义问题。我们还讨论了结构化流处理如何简化与消息系统（如
    Kafka 或其他消息系统）的集成。
- en: In the next chapter, we will look at graph processing and how it all works.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨图形处理及其工作原理。
