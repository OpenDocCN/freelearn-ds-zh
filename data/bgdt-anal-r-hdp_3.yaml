- en: Chapter 3. Integrating R and Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. R与Hadoop的集成
- en: From the first two chapters we got basic information on how to install the R
    and Hadoop tools. Also, we learned what the key features of Hadoop are and why
    they are integrated with R for Big Data solutions to business data problems. So
    with the integration of R and Hadoop we can forward data analytics to Big Data
    analytics. Both of these middleware are still getting improved for being used
    along with each other.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从前两章我们获取了关于如何安装R与Hadoop工具的基本信息。同时，我们也了解了Hadoop的关键特性以及为什么它们与R集成，用于解决大数据业务问题。因此，借助R与Hadoop的集成，我们可以将数据分析提升到大数据分析的层面。这两个中间件仍在不断改进，以便彼此兼容使用。
- en: In [Chapter 2](ch02.html "Chapter 2. Writing Hadoop MapReduce Programs"), *Writing
    Hadoop MapReduce Programs*, we learned how to write a MapReduce program in Hadoop.
    In this chapter, we will learn to develop the MapReduce programs in R that run
    over the Hadoop cluster. This chapter will provide development tutorials on R
    and Hadoop with RHIPE and RHadoop. After installing R and Hadoop, we will see
    how R and Hadoop can be integrated using easy steps.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html "第2章. 编写Hadoop MapReduce程序")，*编写Hadoop MapReduce程序*中，我们学习了如何在Hadoop中编写MapReduce程序。在本章中，我们将学习如何在R中开发在Hadoop集群上运行的MapReduce程序。本章将提供关于R和Hadoop以及RHIPE和RHadoop的开发教程。安装R和Hadoop后，我们将看到如何通过简单步骤将R与Hadoop进行集成。
- en: Before we start moving on to the installation, let's see what are the advantages
    of R and Hadoop integration within an organization. Since statisticians and data
    analysts frequently use the R tool for data exploration as well as data analytics,
    Hadoop integration is a big boon for processing large-size data. Similarly, data
    engineers who use Hadoop tools, such as system, to organize the data warehouse
    can perform such logical analytical operations to get informative insights that
    are actionable by integrating with R tool.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始安装之前，先来看看R与Hadoop集成在组织中的优势。由于统计学家和数据分析师常常使用R工具进行数据探索和数据分析，Hadoop集成为处理大规模数据提供了极大的便利。同样，使用Hadoop工具（如系统）来组织数据仓库的数据工程师，可以通过与R工具的集成，执行逻辑分析操作，从而获得可操作的有价值的洞见。
- en: Therefore, the integration of such data-driven tools and technologies can build
    a powerful scalable system that has features of both of them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些基于数据的工具和技术的集成可以构建一个强大且可扩展的系统，具备两者的特性。
- en: '![Integrating R and Hadoop](img/3282OS_03_01.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![R与Hadoop的集成](img/3282OS_03_01.jpg)'
- en: 'Three ways to link R and Hadoop are as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将R与Hadoop连接的三种方式如下：
- en: RHIPE
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RHIPE
- en: RHadoop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RHadoop
- en: Hadoop streaming
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop流式处理
- en: In this chapter, we will be learning integration and analytics with RHIPE and
    RHadoop. Hadoop streaming will be covered in [Chapter 4](ch04.html "Chapter 4. Using
    Hadoop Streaming with R"), *Using Hadoop Streaming with R*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习使用RHIPE和RHadoop进行集成与分析。Hadoop流式处理将在[第4章](ch04.html "第4章. 使用Hadoop流式处理与R")，*使用Hadoop流式处理与R*中讲解。
- en: Introducing RHIPE
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍RHIPE
- en: '**RHIPE** stands for **R and Hadoop Integrated Programming Environment**. As
    mentioned on [http://www.datadr.org/](http://www.datadr.org/), it means "in a
    moment" in Greek and is a merger of R and Hadoop. It was first developed by *Saptarshi
    Guha* for his PhD thesis in the Department of Statistics at Purdue University
    in 2012\. Currently this is carried out by the Department of Statistics team at
    Purdue University and other active Google discussion groups.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**RHIPE**代表**R和Hadoop集成编程环境**。正如在[http://www.datadr.org/](http://www.datadr.org/)上所提到的，它在希腊语中意为“瞬间”，是R与Hadoop的结合体。它最初由*Saptarshi
    Guha*为其在普渡大学统计系的博士论文于2012年开发。现在由普渡大学统计系团队及其他活跃的Google讨论组进行维护。'
- en: 'The RHIPE package uses the **Divide and Recombine** technique to perform data
    analytics over Big Data. In this technique, data is divided into subsets, computation
    is performed over those subsets by specific R analytics operations, and the output
    is combined. RHIPE has mainly been designed to accomplish two goals that are as
    follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: RHIPE包使用**分割与重组合**技术对大数据进行数据分析。在此技术中，数据被划分为子集，针对这些子集执行特定的R分析操作，最后将输出结果合并。RHIPE主要旨在实现以下两个目标：
- en: Allowing you to perform in-depth analysis of large as well as small data.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让你能够对大数据和小数据进行深入分析。
- en: Allowing users to perform the analytics operations within R using a lower-level
    language. RHIPE is designed with several functions that help perform **Hadoop
    Distribute File System** (**HDFS**) as well as MapReduce operations using a simple
    R console.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许用户使用较低级语言在 R 中执行分析操作。RHIPE 设计有多个函数，可帮助在简单的 R 控制台中执行 **Hadoop 分布式文件系统**（**HDFS**）以及
    MapReduce 操作。
- en: RHIPE is a lower-level interface as compared to HDFS and MapReduce operation.
    Use the latest supported version of RHIPE which is 0.73.1 as `Rhipe_0.73.1-2.tar.gz`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RHIPE 是与 HDFS 和 MapReduce 操作相比较的较低级接口。使用最新支持的 RHIPE 版本 0.73.1，文件名为 `Rhipe_0.73.1-2.tar.gz`。
- en: Installing RHIPE
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 RHIPE
- en: 'As RHIPE is a connector of R and Hadoop, we need Hadoop and R installed on
    our machine or in our clusters in the following sequence:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 RHIPE 是 R 和 Hadoop 的连接器，我们需要按照以下顺序在我们的机器或集群中安装 Hadoop 和 R：
- en: Installing Hadoop.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Hadoop。
- en: Installing R.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 R。
- en: Installing protocol buffers.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装协议缓冲区。
- en: Setting up environment variables.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置环境变量。
- en: Installing rJava.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 rJava。
- en: Installing RHIPE.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 RHIPE。
- en: Let us begin with the installation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始安装。
- en: Installing Hadoop
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Hadoop
- en: As we are here to integrate R and Hadoop with the RHIPE package library, we
    need to install Hadoop on our machine. It will be arbitrary that it either be
    a single node or multinode installation depending on the size of the data to be
    analyzed.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在这里要将 R 和 Hadoop 与 RHIPE 包库集成，因此需要在我们的机器上安装 Hadoop。这将是一个单节点或多节点安装，具体取决于要分析的数据大小。
- en: As we have already learned how to install Hadoop in Ubuntu, we are not going
    to repeat the process here. If you haven't installed it yet, please refer to [Chapter
    1](ch01.html "Chapter 1. Getting Ready to Use R and Hadoop"), *Getting Ready to
    Use R and Hadoop*, for guidance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经学会了如何在 Ubuntu 上安装 Hadoop，我们在此不再重复该过程。如果您尚未安装，请参阅 [第一章](ch01.html "第一章。准备使用
    R 和 Hadoop")，*准备使用 R 和 Hadoop*，以获取指导。
- en: Installing R
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 R
- en: If we use a multinode Hadoop architecture, there are a number of TaskTracker
    nodes for executing the MapReduce job. So, we need to install R over all of these
    TaskTracker nodes. These TaskTracker nodes will start process over the data subsets
    with developed map and reduce logic with the consideration of key values.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用多节点 Hadoop 架构，则有多个 TaskTracker 节点用于执行 MapReduce 作业。因此，我们需要在所有这些 TaskTracker
    节点上安装 R。这些 TaskTracker 节点将根据键值对的考虑，使用开发的映射和减少逻辑启动数据子集的过程。
- en: Installing protocol buffers
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装协议缓冲区
- en: Protocol buffers just serialize the data to make it platform independent, neutral,
    and robust (primarily used for structured data). Google uses the same protocol
    for data interchange. RHIPE depends on protocol buffers 2.4.1 for data serialization
    over the network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 协议缓冲区只需将数据序列化，使其平台无关、中立和健壮（主要用于结构化数据）。Google 使用相同的协议进行数据交换。RHIPE 依赖于协议缓冲区 2.4.1
    以进行网络上的数据序列化。
- en: 'This can be installed using the following command:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令安装：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Environment variables
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境变量
- en: 'In order for RHIPE to compile and work correctly, it is better to ensure that
    the following environment variables are set appropriately:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 RHIPE 能够正确编译和工作，最好确保以下环境变量设置正确：
- en: For configuring the Hadoop libraries, we need to set two variables, `PKG_CONFIG_PATH`
    and `LD_LIBRARY_PATH`, to the `~./bashrc` file of `hduser` (Hadoop user) so that
    it can automatically be set when the user logs in to the system.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置 Hadoop 库，我们需要将两个变量 `PKG_CONFIG_PATH` 和 `LD_LIBRARY_PATH` 设置到 `hduser`（Hadoop
    用户）的 `~./bashrc` 文件中，以便在用户登录系统时自动设置。
- en: Here, `PKG_CONFIG_PATH` is an environment variable that holds the path of the
    `pkg-config` script for retrieving information about installed libraries in the
    system, and `LD_LIBRARY_PATH` is an environment variable that holds the path of
    native shared libraries.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`PKG_CONFIG_PATH` 是一个环境变量，保存了系统中安装库的 `pkg-config` 脚本路径，而 `LD_LIBRARY_PATH`
    是一个环境变量，保存了本地共享库的路径。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can also set all these variables from your R console, as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以从 R 控制台设置所有这些变量，如下所示：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Where `HADOOP_HOME` is used for specifying the location of the Hadoop directory,
    `HADOOP_BIN` is used for specifying the location of binary files of Hadoop, and
    `HADOOP_CONF_DIR` is used for specifying the configuration files of Hadoop.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `HADOOP_HOME` 用于指定 Hadoop 目录的位置，`HADOOP_BIN` 用于指定 Hadoop 二进制文件的位置，`HADOOP_CONF_DIR`
    用于指定 Hadoop 配置文件的位置。
- en: Setting the variables is temporary and valid up to a particular R session. If
    we want to make this variable permanent, as initialized automatically when the
    R session initializes, we need to set these variables to the `/etc/R/Renviron`
    file as we set the environment variable in `.bashrc` of a specific user profile.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 设置变量是临时的，并且在特定的 R 会话期间有效。如果我们希望在每次 R 会话初始化时自动初始化这些变量，使其永久有效，我们需要将这些变量设置到 `/etc/R/Renviron`
    文件中，就像在特定用户配置文件的 `.bashrc` 中设置环境变量一样。
- en: The rJava package installation
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: rJava 包安装
- en: Since RHIPE is a Java package, it acts like a Java bridge between R and Hadoop.
    RHIPE serializes the input data to a Java type, which has to be serialized over
    the cluster. It needs a low-level interface to Java, which is provided by rJava.
    So, we will install rJava to enable the functioning of RHIPE.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 RHIPE 是一个 Java 包，它充当 R 和 Hadoop 之间的 Java 桥梁。RHIPE 将输入数据序列化为 Java 类型，这些数据需要在集群上序列化。它需要一个低级的
    Java 接口，而这一接口由 rJava 提供。因此，我们将安装 rJava 以启用 RHIPE 的功能。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Installing RHIPE
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 RHIPE
- en: Now, it's time to install the RHIPE package from its repository.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候从 RHIPE 的软件库安装 RHIPE 包了。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we are ready with a RHIPE system for performing data analytics with R and
    Hadoop.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好一个 RHIPE 系统，可以使用 R 和 Hadoop 执行数据分析。
- en: Understanding the architecture of RHIPE
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 RHIPE 的架构
- en: Let's understand the working of the RHIPE library package developed to integrate
    R and Hadoop for effective Big Data analytics.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解 RHIPE 库包的工作原理，RHIPE 被开发出来用于将 R 和 Hadoop 集成，以进行高效的大数据分析。
- en: '![Understanding the architecture of RHIPE](img/3282OS_03_02.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![理解 RHIPE 的架构](img/3282OS_03_02.jpg)'
- en: Components of RHIPE
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: RHIPE 的组件
- en: There are a number of Hadoop components that will be used for data analytics
    operations with R and Hadoop.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多 Hadoop 组件将用于与 R 和 Hadoop 的数据分析操作。
- en: 'The components of RHIPE are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: RHIPE 的组件如下：
- en: '**RClient**: RClient is an R application that calls the **JobTracker** to execute
    the job with an indication of several MapReduce job resources such as Mapper,
    Reducer, input format, output format, input file, output file, and other several
    parameters that can handle the MapReduce jobs with RClient.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RClient**：RClient 是一个 R 应用程序，它调用 **JobTracker** 来执行作业，并指示多个 MapReduce 作业资源，如
    Mapper、Reducer、输入格式、输出格式、输入文件、输出文件以及其他能处理 MapReduce 作业的参数。'
- en: '**JobTracker**: A JobTracker is the master node of the Hadoop MapReduce operations
    for initializing and monitoring the MapReduce jobs over the Hadoop cluster.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JobTracker**：JobTracker 是 Hadoop MapReduce 操作的主节点，负责初始化并监控 Hadoop 集群上的 MapReduce
    作业。'
- en: '**TaskTracker**: TaskTracker is a slave node in the Hadoop cluster. It executes
    the MapReduce jobs as per the orders given by JobTracker, retrieve the input data
    chunks, and run R-specific `Mapper` and `Reducer` over it. Finally, the output
    will be written on the HDFS directory.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TaskTracker**：TaskTracker 是 Hadoop 集群中的一个从节点，负责根据 JobTracker 的指令执行 MapReduce
    作业，获取输入数据块，并在其上运行 R 特定的 `Mapper` 和 `Reducer`。最终，输出将被写入 HDFS 目录。'
- en: '**HDFS**: HDFS is a filesystem distributed over Hadoop clusters with several
    data nodes. It provides data services for various data operations.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HDFS**：HDFS 是一个分布式文件系统，部署在 Hadoop 集群上，包含多个数据节点。它为各种数据操作提供数据服务。'
- en: Understanding RHIPE samples
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 RHIPE 示例
- en: In this section, we will create two RHIPE MapReduce examples. These two examples
    are defined with the basic utility of the Hadoop MapReduce job from a RHIPE package.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建两个 RHIPE MapReduce 示例。这两个示例通过 RHIPE 包中的基本 Hadoop MapReduce 作业功能来定义。
- en: RHIPE sample program (Map only)
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RHIPE 示例程序（仅 Map 阶段）
- en: 'MapReduce problem definition: The goal of this MapReduce sample program is
    to test the RHIPE installation by using the `min` and `max` functions over numeric
    data with the Hadoop environment. Since this is a sample program, we have included
    only the Map phase, which will store its output in the HDFS directory.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 问题定义：这个 MapReduce 示例程序的目标是通过在 Hadoop 环境中使用 `min` 和 `max` 函数对数值数据进行测试，以验证
    RHIPE 的安装。由于这是一个示例程序，我们只包含了 Map 阶段，并将其输出存储在 HDFS 目录中。
- en: To start the development with RHIPE, we need to initialize the RHIPE subsystem
    by loading the library and calling the `rhinit()` method.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 RHIPE 开发，我们需要通过加载库并调用 `rhinit()` 方法来初始化 RHIPE 子系统。
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Input: We insert a numerical value rather than using a file as an input.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：我们插入一个数值，而不是使用文件作为输入。
- en: 'Map phase: The Map phase of this MapReduce program will call 10 different iterations
    and in all of those iterations, random numbers from 1 to 10 will be generated
    as per their iteration number. After that, the max and min values for that generated
    numbers will be calculated.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Map阶段：该MapReduce程序的Map阶段将调用10次不同的迭代，在每次迭代中，根据迭代次数生成1到10之间的随机数。之后，将计算这些生成的数值的最大值和最小值。
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Output: Finally the output of the Map phase will be considered here as an output
    of this MapReduce job and it will be stored to HDFS at `/app/hadoop/RHIPE/`.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：最终，Map阶段的输出将作为此MapReduce作业的输出，并将存储到HDFS中的`/app/hadoop/RHIPE/`路径下。
- en: 'Defining the MapReduce job by the `rhwatch()` method of the RHIPE package:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过RHIPE包的`rhwatch()`方法定义MapReduce作业：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Reading the MapReduce output from HDFS:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从HDFS读取MapReduce输出：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For displaying the result in a more readable form in the table format, use
    the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要以更可读的表格格式显示结果，请使用以下代码：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Output:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '![RHIPE sample program (Map only)](img/3282OS_03_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![RHIPE示例程序（仅Map）](img/3282OS_03_03.jpg)'
- en: Word count
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单词计数
- en: 'MapReduce problem definition: This RHIPE MapReduce program is defined for identifying
    the frequency of all of the words that are present in the provided input text
    files.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce问题定义：此RHIPE MapReduce程序的目的是识别提供的输入文本文件中所有单词的频率。
- en: Also note that this is the same MapReduce problem as we saw in [Chapter 2](ch02.html
    "Chapter 2. Writing Hadoop MapReduce Programs"), *Writing Hadoop MapReduce Programs*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，这与我们在[第2章](ch02.html "第2章. 编写Hadoop MapReduce程序")中看到的MapReduce问题相同，*编写Hadoop
    MapReduce程序*。
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Input: We will use the `CHANGES.txt` file, which comes with Hadoop distribution,
    and use it with this MapReduce algorithm. By using the following command, we will
    copy it to HDFS:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：我们将使用随Hadoop分发包提供的`CHANGES.txt`文件，并将其与此MapReduce算法一起使用。通过以下命令，我们将其复制到HDFS：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Map phase: The Map phase contains the code for reading all the words from a
    file and assigning all of them to value `1`.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Map阶段：Map阶段包含读取文件中所有单词并将其赋值为`1`的代码。
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Reduce phase: With this reducer task, we can calculate the total frequency
    of the words in the input text files.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Reduce阶段：通过这个reduce任务，我们可以计算输入文本文件中单词的总频率。
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Defining the MapReduce job object: After defining the word count mapper and
    reducer, we need to design the `driver` method that can execute this MapReduce
    job by calling `Mapper` and `Reducer` sequentially.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 定义MapReduce作业对象：在定义了单词计数的mapper和reducer后，我们需要设计`driver`方法，依次调用`Mapper`和`Reducer`来执行此MapReduce作业。
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Reading the MapReduce output:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 读取MapReduce输出：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of MapReduce job will be stored to `output_data`, we will convert
    this output into R supported dataframe format. The dataframe output will be stored
    to the `results` variable. For displaying the MapReduce output in the data frame
    the format will be as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce作业的输出将存储到`output_data`，我们将把这个输出转换为R支持的数据框格式。数据框输出将存储到`results`变量中。为了以数据框格式显示MapReduce输出，格式如下：
- en: 'Output for `head (results)`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`head (results)`的输出：'
- en: '![Word count](img/3282OS_03_04.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![单词计数](img/3282OS_03_04.jpg)'
- en: 'Output for `tail (results)`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`tail (results)`的输出：'
- en: '![Word count](img/3282OS_03_05.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![单词计数](img/3282OS_03_05.jpg)'
- en: Understanding the RHIPE function reference
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解RHIPE函数参考
- en: RHIPE is specially designed for providing a lower-level interface over Hadoop.
    So R users with a RHIPE package can easily fire the Hadoop data operations over
    large datasets that are stored on HDFS, just like the `print()` function called
    in R.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: RHIPE特别设计为提供Hadoop的低级接口。因此，拥有RHIPE包的R用户可以轻松地对存储在HDFS上的大数据集执行Hadoop数据操作，就像在R中调用`print()`函数一样。
- en: 'Now we will see all the possible functional uses of all methods that are available
    in RHIPE library. All these methods are with three categories: Initialization,
    HDFS, and MapReduce operations.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到RHIPE库中所有方法的所有可能功能用法。所有这些方法都分为三类：初始化、HDFS和MapReduce操作。
- en: Initialization
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化
- en: 'We use the following command for initialization:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下命令进行初始化：
- en: '`rhinit`: This is used to initialize the Rhipe subsystem.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhinit`：用于初始化Rhipe子系统。'
- en: '`rhinit(TRUE,TRUE)`'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`rhinit(TRUE,TRUE)`'
- en: HDFS
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HDFS
- en: 'We use the following command for HDFS operations:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下命令进行HDFS操作：
- en: '`rhls`: This is used to retrieve all directories from HDFS.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhls`：用于从HDFS检索所有目录。'
- en: Its syntax is `rhls(path)`
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其语法为`rhls(path)`
- en: '`rhls("/")`'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`rhls("/")`'
- en: 'Output:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出：
- en: '![HDFS](img/3282OS_03_06.jpg)'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![HDFS](img/3282OS_03_06.jpg)'
- en: '`hdfs.getwd`: This is used for acquiring the current working HDFS directory.
    Its syntax is `hdfs.getwd()`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.getwd`：用于获取当前工作 HDFS 目录。其语法为 `hdfs.getwd()`。'
- en: '`hdfs.setwd`: This is used for setting up the current working HDFS directory.
    Its syntax is `hdfs.setwd("/RHIPE")`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.setwd`：用于设置当前工作 HDFS 目录。其语法为 `hdfs.setwd("/RHIPE")`'
- en: '`rhput`: This is used to copy a file from a local directory to HDFS. Its syntax
    is `rhput(src,dest)` and `rhput("/usr/local/hadoop/NOTICE.txt","/RHIPE/")`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhput`：用于将文件从本地目录复制到 HDFS。其语法为 `rhput(src,dest)` 和 `rhput("/usr/local/hadoop/NOTICE.txt","/RHIPE/")`。'
- en: '`rhcp`: This is used to copy a file from one HDFS location to another HDFS
    location. Its syntax is `rhcp(''/RHIPE/1/change.txt'',''/RHIPE/2/change.txt'')`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhcp`：用于将文件从一个 HDFS 位置复制到另一个 HDFS 位置。其语法为 `rhcp(''/RHIPE/1/change.txt'',''/RHIPE/2/change.txt'')`。'
- en: '`rhdel`: This is used to delete a directory/file from HDFS. Its syntax is `rhdel("/RHIPE/1")`.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhdel`：用于从 HDFS 删除目录/文件。其语法为 `rhdel("/RHIPE/1")`。'
- en: '`rhget`: This is used to copy the HDFS file to a local directory. Its syntax
    is `rhget("/RHIPE/1/part-r-00000", "/usr/local/")`.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhget`：用于将 HDFS 文件复制到本地目录。其语法为 `rhget("/RHIPE/1/part-r-00000", "/usr/local/")`。'
- en: '`rwrite`: This is used to write the R data to HDFS. its syntax is `rhwrite(list(1,2,3),"/tmp/x")`.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rwrite`：用于将 R 数据写入 HDFS。其语法为 `rhwrite(list(1,2,3),"/tmp/x")`。'
- en: MapReduce
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MapReduce
- en: 'We use the following commands for MapReduce operations:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下命令进行 MapReduce 操作：
- en: '`rhwatch`: This is used to prepare, submit, and monitor MapReduce jobs.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhwatch`：用于准备、提交并监控 MapReduce 作业。'
- en: '[PRE16]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`rhex`: This is used to execute the MapReduce job from over Hadoop cluster.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhex`：用于从 Hadoop 集群上执行 MapReduce 作业。'
- en: '[PRE17]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`rhjoin`: This is used to check whether the MapReduce job is completed or not.
    Its syntax is `rhjoin(job)`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhjoin`：用于检查 MapReduce 作业是否已完成。其语法为 `rhjoin(job)`。'
- en: '`rhkill`: This is used to kill the running MapReduce job. Its syntax is `rhkill(job)`.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhkill`：用于终止正在运行的 MapReduce 作业。其语法为 `rhkill(job)`。'
- en: '`rhoptions`: This is used for getting or setting the RHIPE configuration options.
    Its syntax is `rhoptions()`.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhoptions`：用于获取或设置 RHIPE 配置选项。其语法为 `rhoptions()`。'
- en: '`rhstatus`: This is used to get the status of the RHIPE MapReduce job. Its
    syntax is `rhstatus(job)`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhstatus`：用于获取 RHIPE MapReduce 作业的状态。其语法为 `rhstatus(job)`。'
- en: '[PRE18]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Introducing RHadoop
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 RHadoop
- en: 'RHadoop is a collection of three R packages for providing large data operations
    with an R environment. It was developed by Revolution Analytics, which is the
    leading commercial provider of software based on R. RHadoop is available with
    three main R packages: `rhdfs`, `rmr`, and `rhbase`. Each of them offers different
    Hadoop features.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: RHadoop 是一个包含三个 R 包的集合，用于在 R 环境中提供大数据操作。它由 Revolution Analytics 开发，Revolution
    Analytics 是基于 R 的商业软件的领先提供商。RHadoop 提供三个主要的 R 包：`rhdfs`、`rmr` 和 `rhbase`。每个包提供不同的
    Hadoop 功能。
- en: '`rhdfs` is an R interface for providing the HDFS usability from the R console.
    As Hadoop MapReduce programs write their output on HDFS, it is very easy to access
    them by calling the `rhdfs` methods. The R programmer can easily perform read
    and write operations on distributed data files. Basically, `rhdfs` package calls
    the HDFS API in backend to operate data sources stored on HDFS.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhdfs` 是一个 R 接口，提供从 R 控制台访问 HDFS 的功能。由于 Hadoop MapReduce 程序将输出写入 HDFS，因此通过调用
    `rhdfs` 方法，可以轻松访问它们。R 程序员可以轻松地对分布式数据文件执行读写操作。基本上，`rhdfs` 包通过后台调用 HDFS API 来操作存储在
    HDFS 上的数据源。'
- en: '`rmr` is an R interface for providing Hadoop MapReduce facility inside the
    R environment. So, the R programmer needs to just divide their application logic
    into the map and reduce phases and submit it with the `rmr` methods. After that,
    `rmr` calls the Hadoop streaming MapReduce API with several job parameters as
    input directory, output directory, mapper, reducer, and so on, to perform the
    R MapReduce job over Hadoop cluster.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rmr` 是一个 R 接口，用于在 R 环境内提供 Hadoop MapReduce 功能。因此，R 程序员只需要将应用逻辑分为 map 和 reduce
    阶段，并使用 `rmr` 方法提交。之后，`rmr` 会调用 Hadoop streaming MapReduce API，输入参数包括输入目录、输出目录、mapper、reducer
    等，来在 Hadoop 集群上执行 R MapReduce 作业。'
- en: '`rhbase` is an R interface for operating the Hadoop HBase data source stored
    at the distributed network via a Thrift server. The `rhbase` package is designed
    with several methods for initialization and read/write and table manipulation
    operations.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhbase` 是一个 R 接口，用于通过 Thrift 服务器操作存储在分布式网络上的 Hadoop HBase 数据源。`rhbase` 包设计了多个方法，用于初始化、读写和表操作。'
- en: Here it's not necessary to install all of the three RHadoop packages to run
    the Hadoop MapReduce operations with R and Hadoop. If we have stored our input
    data source at the HBase data source, we need to install `rhbase`; else we require
    `rhdfs` and `rmr` packages. As Hadoop is most popular for its two main features,
    Hadoop MapReduce and HDFS, both of these features will be used within the R console
    with the help of RHadoop `rhdfs` and `rmr` packages. These packages are enough
    to run Hadoop MapReduce from R. Basically, `rhdfs` provides HDFS data operations
    while `rmr` provides MapReduce execution operations.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，运行 Hadoop MapReduce 操作并不一定需要安装所有三个 RHadoop 包。如果我们将输入数据源存储在 HBase 数据源中，我们需要安装
    `rhbase`；否则，我们需要 `rhdfs` 和 `rmr` 包。由于 Hadoop 主要因其两个主要特性——Hadoop MapReduce 和 HDFS——而广受欢迎，这两个特性将在
    R 控制台中通过 RHadoop 的 `rhdfs` 和 `rmr` 包得到应用。这些包足以在 R 中运行 Hadoop MapReduce。基本上，`rhdfs`
    提供 HDFS 数据操作，而 `rmr` 提供 MapReduce 执行操作。
- en: RHadoop also includes another package called `quick check`, which is designed
    for debugging the developed MapReduce job defined by the `rmr` package.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: RHadoop 还包括另一个名为 `quick check` 的包，用于调试由 `rmr` 包定义的 MapReduce 作业。
- en: In the next section, we will see their architectural relationships as well as
    their installation steps.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将看到它们的架构关系以及安装步骤。
- en: Understanding the architecture of RHadoop
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 RHadoop 架构
- en: 'Since Hadoop is highly popular because of HDFS and MapReduce, Revolution Analytics
    has developed separate R packages, namely, `rhdfs`, `rmr`, and `rhbase`. The architecture
    of RHadoop is shown in the following diagram:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Hadoop 因 HDFS 和 MapReduce 而非常受欢迎，Revolution Analytics 开发了单独的 R 包，即 `rhdfs`、`rmr`
    和 `rhbase`。RHadoop 的架构如以下图所示：
- en: '![Understanding the architecture of RHadoop](img/3282OS_03_07.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![理解 RHadoop 架构](img/3282OS_03_07.jpg)'
- en: RHadoop Ecosystem
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: RHadoop 生态系统
- en: Installing RHadoop
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 RHadoop
- en: In this section, we will learn some installation tricks for the three RHadoop
    packages including their prerequisites.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍三个 RHadoop 包的安装技巧以及它们的前提条件。
- en: '**R and Hadoop installation**: As we are going to use an R and Hadoop integrated
    environment, we need Hadoop as well as R installed on our machine. If you haven''t
    installed yet, see [Chapter 1](ch01.html "Chapter 1. Getting Ready to Use R and
    Hadoop"), *Getting Ready to Use R and Hadoop*. As we know, if we have too much
    data, we need to scale our cluster by increasing the number of nodes. Based on
    this, to get RHadoop installed on our system we need Hadoop with either a single
    node or multimode installation as per the size of our data.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R 和 Hadoop 安装**：由于我们将使用 R 和 Hadoop 集成的环境，因此需要在计算机上安装 Hadoop 和 R。如果尚未安装，请参阅
    [第 1 章](ch01.html "第 1 章. 准备使用 R 和 Hadoop")，*准备使用 R 和 Hadoop*。如我们所知，如果数据量过大，需要通过增加节点数来扩展集群。基于此，为了在系统上安装
    RHadoop，我们需要根据数据的大小安装单节点或多节点 Hadoop。'
- en: RHadoop is already tested with several Hadoop distributions provided by Cloudera,
    Hortonworks, and MapR.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RHadoop 已经在 Cloudera、Hortonworks 和 MapR 提供的多个 Hadoop 发行版上进行了测试。
- en: '**Installing the R packages**: We need several R packages to be installed that
    help it to connect R with Hadoop. The list of the packages is as follows:'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安装 R 包**：我们需要安装多个 R 包，它们帮助 R 与 Hadoop 连接。包的列表如下：'
- en: rJava
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: rJava
- en: RJSONIO
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RJSONIO
- en: itertools
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: itertools
- en: digest
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: digest
- en: Rcpp
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rcpp
- en: httr
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: httr
- en: functional
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: functional
- en: devtools
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: devtools
- en: plyr
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: plyr
- en: reshape2
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: reshape2
- en: 'We can install them by calling the execution of the following R command in
    the R console:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以通过在 R 控制台中执行以下 R 命令来安装它们：
- en: '[PRE19]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Setting environment variables**: We can set this via the R console using
    the following code:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置环境变量**：我们可以通过 R 控制台使用以下代码来设置环境变量：'
- en: '[PRE20]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'or, we can also set the R console via the command line as follows:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，我们也可以通过命令行设置 R 控制台，如下所示：
- en: '[PRE21]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Installing RHadoop [`rhdfs`, `rmr`, `rhbase`]
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 RHadoop [`rhdfs`，`rmr`，`rhbase`]
- en: 'Download RHadoop packages from GitHub repository of Revolution Analytics: [https://github.com/RevolutionAnalytics/RHadoop](https://github.com/RevolutionAnalytics/RHadoop).'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Revolution Analytics 的 GitHub 仓库下载 RHadoop 软件包：[https://github.com/RevolutionAnalytics/RHadoop](https://github.com/RevolutionAnalytics/RHadoop)。
- en: '`rmr`: [`rmr-2.2.2.tar.gz`]'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rmr`：[`rmr-2.2.2.tar.gz`]'
- en: '`rhdfs`: [`rhdfs-1.6.0.tar.gz`]'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhdfs`：[`rhdfs-1.6.0.tar.gz`]'
- en: '`rhbase`: [`rhbase-1.2.0.tar.gz`]'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhbase`：[`rhbase-1.2.0.tar.gz`]'
- en: Installing packages.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装软件包。
- en: 'For rmr we use:'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 rmr，我们使用：
- en: '[PRE22]'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For `rhdfs` we use:'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `rhdfs`，我们使用：
- en: '[PRE23]'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For `rhbase` we use:'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `rhbase`，我们使用：
- en: '[PRE24]'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Tip
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To install rhbase, we need to have HBase and Zookeeper installed on our Hadoop
    cluster.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要安装 rhbase，我们需要在 Hadoop 集群上安装 HBase 和 Zookeeper。
- en: Understanding RHadoop examples
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 RHadoop 示例
- en: 'Once we complete the installation of RHadoop, we can test the setup by running
    the MapReduce job with the `rmr2` and `rhdfs` libraries in the RHadoop sample
    program as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成RHadoop的安装，我们可以通过运行RHadoop示例程序中的`rmr2`和`rhdfs`库来测试设置，如下所示：
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After running these lines, simply pressing *Ctrl* + *Enter* will execute this
    MapReduce program. If it succeeds, the last line will appear as shown in the following
    screenshot:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这些代码行后，按下*Ctrl* + *Enter*将执行此MapReduce程序。如果成功，最后一行将如下面的截图所示：
- en: '![Understanding RHadoop examples](img/3282OS_03_08.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![理解RHadoop示例](img/3282OS_03_08.jpg)'
- en: Where characters of that last line indicate the output location of the MapReduce
    job.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 那一行字符表示MapReduce作业的输出位置。
- en: To read the result of the executed MapReduce job, copy the output location,
    as provided in the last line, and pass it to the `from.dfs()` function of `rhdfs`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取已执行MapReduce作业的结果，复制最后一行提供的输出位置，并将其传递给`rhdfs`的`from.dfs()`函数。
- en: '![Understanding RHadoop examples](img/3282OS_03_09.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![理解RHadoop示例](img/3282OS_03_09.jpg)'
- en: Where the first column of the previous output indicates the max value, and the
    second one the min value.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，前一输出的第一列表示最大值，第二列表示最小值。
- en: Word count
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单词计数
- en: 'MapReduce problem definition: This RHadoop MapReduce program is defined for
    identifying the frequency of all the words that are present in the provided input
    text files.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce问题定义：这个RHadoop MapReduce程序定义了识别提供的输入文本文件中所有单词频率的任务。
- en: Also, note that this is the same MapReduce problem as we learned in the previous
    section about RHIPE in [Chapter 2](ch02.html "Chapter 2. Writing Hadoop MapReduce
    Programs"), *Writing Hadoop MapReduce Programs*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，请注意，这是我们在[第2章](ch02.html "Chapter 2. Writing Hadoop MapReduce Programs")
    *编写Hadoop MapReduce程序*中学习的相同MapReduce问题。
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Map phase: This `map` function will read the text file line by line and split
    them by spaces. This map phase will assign `1` as a value to all the words that
    are caught by the mapper.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Map阶段：这个`map`函数将逐行读取文本文件并按空格拆分。这个Map阶段将给所有被映射器捕获的单词分配`1`作为值。
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Reduce phase: Reduce phase will calculate the total frequency of all the words
    by performing sum operations over words with the same keys.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Reduce阶段：Reduce阶段将通过对具有相同键的单词进行求和操作来计算所有单词的总频率。
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Defining the MapReduce job: After defining the word count mapper and reducer,
    we need to create the `driver` method that starts the execution of MapReduce.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 定义MapReduce作业：在定义完单词计数的映射器和归约器之后，我们需要创建`driver`方法来启动MapReduce的执行。
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Executing the MapReduce job: We will execute the RHadoop MapReduce job by passing
    the input data location as a parameter for the `wordcount` function.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 执行MapReduce作业：我们将通过传递输入数据位置作为`wordcount`函数的参数来执行RHadoop MapReduce作业。
- en: '[PRE30]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Exploring the `wordcount` output:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 探索`wordcount`输出：
- en: '[PRE31]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Understanding the RHadoop function reference
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解RHadoop函数参考
- en: RHadoop has three different packages, which are in terms of HDFS, MapReduce,
    and HBase operations, to perform operations over the data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: RHadoop有三个不同的包，分别用于HDFS、MapReduce和HBase操作，以便对数据执行操作。
- en: 'Here we will see how to use the `rmr` and `rhdfs` package functions:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看到如何使用`rmr`和`rhdfs`包中的函数：
- en: The hdfs package
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: hdfs包
- en: 'The categorized functions are:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 分类函数如下：
- en: Initialization
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化
- en: '`hdfs.init`: This is used to initialize the `rhdfs` package. Its syntax is
    `hdfs.init()`.'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.init`：用于初始化`rhdfs`包。其语法为`hdfs.init()`。'
- en: '`hdfs.defaults`: This is used to retrieve and set the `rhdfs` defaults. Its
    syntax is `hdfs.defaults()`.'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.defaults`：用于检索和设置`rhdfs`的默认值。其语法为`hdfs.defaults()`。'
- en: 'To retrieve the `hdfs` configuration defaults, refer to the following screenshot:'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要检索`hdfs`配置的默认值，请参考以下截图：
- en: '![The hdfs package](img/3282OS_03_10.jpg)'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![HDFS包](img/3282OS_03_10.jpg)'
- en: File manipulation
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件操作
- en: '`hdfs.put`: This is used to copy files from the local filesystem to the HDFS
    filesystem.'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.put`：用于将文件从本地文件系统复制到HDFS文件系统。'
- en: '[PRE32]'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`hdfs.copy`: This is used to copy files from the HDFS directory to the local
    filesystem.'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.copy`：用于将文件从HDFS目录复制到本地文件系统。'
- en: '[PRE33]'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`hdfs.move`: This is used to move a file from one HDFS directory to another
    HDFS directory.'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.move`：用于将文件从一个HDFS目录移动到另一个HDFS目录。'
- en: '[PRE34]'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`hdfs.rename`: This is used to rename the file stored at HDFS from R.'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.rename`：用于重命名存储在HDFS中的文件（从R中进行操作）。'
- en: '[PRE35]'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`hdfs.delete`: This is used to delete the HDFS file or directory from R.'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.delete`：用于从R中删除HDFS文件或目录。'
- en: '[PRE36]'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`hdfs.rm`: This is used to delete the HDFS file or directory from R.'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.rm`：用于从R中删除HDFS文件或目录。'
- en: '[PRE37]'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`hdfs.chmod`: This is used to change permissions of some files.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.chmod`：用于更改某些文件的权限。'
- en: '[PRE38]'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'File read/write:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件读/写：
- en: '`hdfs.file`: This is used to initialize the file to be used for read/write
    operation.'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.file`：用于初始化文件，以便进行读/写操作。'
- en: '[PRE39]'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`hdfs.write`: This is used to write in to the file stored at HDFS via streaming.'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.write`：用于通过流写入存储在HDFS中的文件。'
- en: '[PRE40]'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`hdfs.close`: This is used to close the stream when a file operation is complete.
    It will close the stream and will not allow further file operations.'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.close`：用于在文件操作完成后关闭流。它会关闭流并且不允许进一步的文件操作。'
- en: '[PRE41]'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`hdfs.read`: This is used to read from binary files on the HDFS directory.
    This will use the stream for the deserialization of the data.'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.read`：用于从HDFS目录中的二进制文件中读取。这将使用流来反序列化数据。'
- en: f = hdfs.file("/RHadoop/2/README.txt","r",buffersize=104857600)
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: f = hdfs.file("/RHadoop/2/README.txt","r",buffersize=104857600)
- en: m = hdfs.read(f)
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: m = hdfs.read(f)
- en: c = rawToChar(m)
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: c = rawToChar(m)
- en: print(c)
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: print(c)
- en: 'Directory operation:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目录操作：
- en: '`hdfs.dircreate` or `hdfs.mkdir`: Both these functions will be used for creating
    a directory over the HDFS filesystem.'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.dircreate` 或 `hdfs.mkdir`：这两个函数用于在HDFS文件系统上创建目录。'
- en: '[PRE42]'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`hdfs.rm` or `hdfs.rmr` or `hdfs.delete` - to delete the directory or file
    from HDFS.'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.rm` 或 `hdfs.rmr` 或 `hdfs.delete` - 用于从HDFS删除目录或文件。'
- en: '[PRE43]'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Utility:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具：
- en: '`hdfs.ls`: This is used to list the directory from HDFS.'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.ls`：用于列出HDFS中的目录。'
- en: '[PRE44]'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![The hdfs package](img/3282OS_03_11.jpg)'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![The hdfs package](img/3282OS_03_11.jpg)'
- en: '`hdfs.file.info`: This is used to get meta information about the file stored
    at HDFS.'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.file.info`：用于获取存储在HDFS中的文件的元信息。'
- en: '[PRE45]'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![The hdfs package](img/3282OS_03_12.jpg)'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![The hdfs package](img/3282OS_03_12.jpg)'
- en: The rmr package
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: rmr包
- en: 'The categories of the functions are as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数的类别如下：
- en: 'For storing and retrieving data:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于数据存储和检索：
- en: '`to.dfs`: This is used to write R objects from or to the filesystem.'
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`to.dfs`：用于从文件系统中写入或写出R对象。'
- en: small.ints = to.dfs(1:10)
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: small.ints = to.dfs(1:10)
- en: '`from.dfs`: This is used to read the R objects from the HDFS filesystem that
    are in the binary encrypted format.'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from.dfs`：用于读取以二进制加密格式存储在HDFS文件系统中的R对象。'
- en: from.dfs('/tmp/RtmpRMIXzb/file2bda3fa07850')
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: from.dfs('/tmp/RtmpRMIXzb/file2bda3fa07850')
- en: 'For MapReduce:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于MapReduce：
- en: '`mapreduce`: This is used for defining and executing the MapReduce job.'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mapreduce`：用于定义和执行MapReduce任务。'
- en: mapreduce(input, output, map, reduce, combine, input.fromat, output.format,
    verbose)
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: mapreduce(input, output, map, reduce, combine, input.fromat, output.format,
    verbose)
- en: '`keyval`: This is used to create and extract key-value pairs.'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keyval`：用于创建和提取键值对。'
- en: keyval(key, val)
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: keyval(key, val)
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Since RHadoop is considered as matured, we will consider it while performing
    data analytics in further chapters. In [Chapter 5](ch05.html "Chapter 5. Learning
    Data Analytics with R and Hadoop"), *Learning Data Analytics with R and Hadoop*
    and [Chapter 6](ch06.html "Chapter 6. Understanding Big Data Analysis with Machine
    Learning"), *Understanding Big Data Analysis with Machine Learning*, we will dive
    into some Big Data analytics techniques as well as see how real world problems
    can be solved with RHadoop. So far we have learned how to write the MapReduce
    program with R and Hadoop using RHIPE and RHadoop. In the next chapter, we will
    see how to write the Hadoop MapReduce program with Hadoop streaming utility and
    also with Hadoop streaming R packages.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RHadoop被认为是成熟的，我们将在后续章节中进行数据分析时考虑它。在[第五章](ch05.html "第五章 学习R和Hadoop的数据分析")，《*学习R和Hadoop的数据分析*》和[第六章](ch06.html
    "第六章 理解机器学习的大数据分析")，《*理解机器学习的大数据分析*》中，我们将深入探讨一些大数据分析技术，并看到如何使用RHadoop解决现实世界中的问题。到目前为止，我们已经学习了如何使用RHIPE和RHadoop编写MapReduce程序。在下一章中，我们将看到如何使用Hadoop流式工具和Hadoop流式R包编写Hadoop
    MapReduce程序。
