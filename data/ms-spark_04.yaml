- en: Chapter 4. Apache Spark SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。Apache Spark SQL
- en: 'In this chapter, I would like to examine Apache Spark SQL, the use of Apache
    Hive with Spark, and DataFrames. DataFrames have been introduced in Spark 1.3,
    and are columnar data storage structures, roughly equivalent to relational database
    tables. The chapters in this book have not been developed in sequence, so the
    earlier chapters might use older versions of Spark than the later ones. I also
    want to examine user-defined functions for Spark SQL. A good place to find information
    about the Spark class API is: `spark.apache.org/docs/<version>/api/scala/index.html`.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我想检查Apache Spark SQL，使用Apache Hive与Spark以及数据框。数据框在Spark 1.3中引入，是列式数据存储结构，大致相当于关系数据库表。本书的章节并非按顺序开发，因此早期章节可能使用比后期章节更旧的Spark版本。我还想检查Spark
    SQL的用户定义函数。关于Spark类API的信息，可以在以下位置找到：`spark.apache.org/docs/<version>/api/scala/index.html`。
- en: 'I prefer to use Scala, but the API information is also available in Java and
    Python formats. The `<version>` value refers to the release of Spark that you
    will be using—1.3.1\. This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我更喜欢使用Scala，但API信息也可用于Java和Python格式。`<version>`值是指您将使用的Spark版本的发布版本-1.3.1。本章将涵盖以下主题：
- en: SQL context
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL上下文
- en: Importing and saving data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入和保存数据
- en: DataFrames
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框
- en: Using SQL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SQL
- en: User-defined functions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户定义的函数
- en: Using Hive
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hive
- en: Before moving straight into SQL and DataFrames, I will give an overview of the
    SQL context.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接进入SQL和数据框之前，我将概述SQL上下文。
- en: The SQL context
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL上下文
- en: 'The SQL context is the starting point for working with columnar data in Apache
    Spark. It is created from the Spark context, and provides the means for loading
    and saving data files of different types, using DataFrames, and manipulating columnar
    data with SQL, among other things. It can be used for the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: SQL上下文是在Apache Spark中处理列数据的起点。它是从Spark上下文创建的，并提供了加载和保存不同类型数据文件的方法，使用数据框，以及使用SQL操作列数据等功能。它可用于以下操作：
- en: Executing SQL via the SQL method
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过SQL方法执行SQL
- en: Registering user-defined functions via the UDF method
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过UDF方法注册用户定义的函数
- en: Caching
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存
- en: Configuration
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置
- en: DataFrames
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框
- en: Data source access
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源访问
- en: DDL operations
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDL操作
- en: 'I am sure that there are other areas, but you get the idea. The examples in
    this chapter are written in Scala, just because I prefer the language, but you
    can develop in Python and Java as well. As shown previously, the SQL context is
    created from the Spark context. Importing the SQL context implicitly allows you
    to implicitly convert RDDs into DataFrames:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信还有其他领域，但你明白我的意思。本章的示例是用Scala编写的，只是因为我更喜欢这种语言，但你也可以用Python和Java进行开发。如前所示，SQL上下文是从Spark上下文创建的。隐式导入SQL上下文允许您将RDD隐式转换为数据框：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For instance, using the previous `implicits` call, allows you to import a CSV
    file and split it by separator characters. It can then convert the RDD that contains
    the data into a data frame using the `toDF` method.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用之前的`implicits`调用，允许您导入CSV文件并按分隔符字符拆分它。然后可以使用`toDF`方法将包含数据的RDD转换为数据框。
- en: It is also possible to define a Hive context for the access and manipulation
    of Apache Hive database table data (Hive is the Apache data warehouse that is
    part of the Hadoop eco-system, and it uses HDFS for storage). The Hive context
    allows a superset of SQL functionality when compared to the Spark context. The
    use of Hive with Spark will be covered in a later section in this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以为访问和操作Apache Hive数据库表数据定义Hive上下文（Hive是Hadoop生态系统的一部分的Apache数据仓库，它使用HDFS进行存储）。与Spark上下文相比，Hive上下文允许使用SQL功能的超集。在本章的后面部分将介绍如何在Spark中使用Hive。
- en: Next, I will examine some of the supported file formats available for importing
    and saving data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将检查一些支持的文件格式，用于导入和保存数据。
- en: Importing and saving data
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入和保存数据
- en: I wanted to add this section about importing and saving data here, even though
    it is not purely about Spark SQL, so I could introduce concepts such as **Parquet**
    and **JSON** file formats. This section also allows me to cover how to access
    and save data in loose text; as well as the CSV, Parquet and JSON formats, conveniently,
    in one place.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在这里添加有关导入和保存数据的部分，即使它并不纯粹关于Spark SQL，这样我就可以介绍诸如**Parquet**和**JSON**文件格式等概念。这一部分还让我能够涵盖如何在一个地方方便地访问和保存松散文本数据，以及CSV、Parquet和JSON格式。
- en: Processing the Text files
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理文本文件
- en: 'Using the Spark context, it is possible to load a text file into an RDD using
    the `textFile` method. Also, the `wholeTextFile` method can read the contents
    of a directory into an RDD. The following examples show how a file, based on the
    local file system (`file://`), or HDFS (`hdfs://`) can be read into a Spark RDD.
    These examples show that the data will be partitioned into six parts for increased
    performance. The first two examples are the same, as they both manipulate a file
    on the Linux file system:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark上下文，可以使用`textFile`方法将文本文件加载到RDD中。此外，`wholeTextFile`方法可以将目录的内容读取到RDD中。以下示例显示了如何将基于本地文件系统（`file://`）或HDFS（`hdfs://`）的文件读取到Spark
    RDD中。这些示例显示数据将被分成六个部分以提高性能。前两个示例相同，因为它们都操作Linux文件系统上的文件：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Processing the JSON files
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理JSON文件
- en: 'JSON is a data interchange format, developed from Javascript. **JSON** actually
    stands for **JavaScript** **Object** **Notation**. It is a text-based format,
    and can be expressed, for instance, as XML. The following example uses the SQL
    context method called `jsonFile` to load the HDFS-based JSON data file named `device.json`.
    The resulting data is created as a data frame:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是一种数据交换格式，由Javascript开发。**JSON**实际上代表**JavaScript** **Object** **Notation**。它是一种基于文本的格式，可以表示为XML。以下示例使用名为`jsonFile`的SQL上下文方法加载基于HDFS的JSON数据文件，名称为`device.json`。生成的数据被创建为数据框：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Data can be saved in JSON format using the data frame `toJSON` method, as shown
    by the following example. First, the Apache Spark and Spark SQL classes are imported:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以使用数据框`toJSON`方法以JSON格式保存，如下例所示。首先导入Apache Spark和Spark SQL类：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, the object class called `sql1` is defined as is a main method with parameters.
    A configuration object is defined that is used to create a spark context. The
    master Spark URL is left as the default value, so Spark expects local mode, the
    local host, and the `7077` port:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义了一个名为`sql1`的对象类，以及一个带参数的主方法。定义了一个配置对象，用于创建一个Spark上下文。主Spark URL保留为默认值，因此Spark期望本地模式，本地主机和`7077`端口：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'An SQL context is created from the Spark context, and a raw text file is loaded
    in CSV format called `adult.test.data_1x`, using the `textFile` method. A schema
    string is then created, which contains the data column names and the schema created
    from it by splitting the string by its spacing, and using the `StructType` and
    `StructField` methods to define each schema column as a string value:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark上下文创建一个SQL上下文，并使用`textFile`方法加载CSV格式的原始文本文件`adult.test.data_1x`。然后创建一个包含数据列名称的模式字符串，并通过将字符串按其间距拆分，并使用`StructType`和`StructField`方法将每个模式列定义为字符串值：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Each data row is then created from the raw CSV data by splitting it with the
    help of a comma as a line divider, and then the elements are added to a `Row()`
    structure. A data frame is created from the schema, and the row data which is
    then converted into JSON format using the `toJSON` method. Finally, the data is
    saved to HDFS using the `saveAsTextFile` method:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过使用逗号作为行分隔符从原始CSV数据中创建每个数据行，然后将元素添加到`Row()`结构中。从模式创建数据框，然后将行数据转换为JSON格式，使用`toJSON`方法。最后，使用`saveAsTextFile`方法将数据保存到HDFS：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: So the resulting data can be seen on HDFS, the Hadoop file system `ls` command
    below shows that the data resides in the `target` directory as a success file
    and two part files.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以在HDFS上看到生成的数据，Hadoop文件系统`ls`命令如下所示，数据驻留在`target`目录中作为成功文件和两个部分文件。
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Using the Hadoop file system''s `cat` command, it is possible to display the
    contents of the JSON data. I will just show a sample to save space:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hadoop文件系统的`cat`命令，可以显示JSON数据的内容。我将展示一个示例以节省空间：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Processing the Parquet data is very similar, as I will show next.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 处理Parquet数据非常类似，接下来我将展示。
- en: Processing the Parquet files
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理Parquet文件
- en: Apache Parquet is another columnar-based data format used by many tools in the
    Hadoop tool set for file I/O, such as Hive, Pig, and Impala. It increases performance
    by using efficient compression and encoding routines.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet是Hadoop工具集中许多工具使用的另一种基于列的数据格式，例如Hive、Pig和Impala。它通过使用高效的压缩和编码例程来提高性能。
- en: 'The Parquet processing example is very similar to the JSON Scala code. The
    DataFrame is created, and then saved in a Parquet format using the save method
    with a type of Parquet:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet处理示例与JSON Scala代码非常相似。创建数据框，然后使用Parquet类型的save方法以Parquet格式保存：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This results in an HDFS-based directory, which contains three Parquet-based
    files: a common Metadata file, a Metadata file, and a temporary file:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成一个基于HDFS的目录，其中包含三个基于Parquet的文件：一个常见的元数据文件，一个元数据文件和一个临时文件：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Listing the contents of the metadata file, using the Hadoop file system''s
    `cat` command, gives an idea of the data format. However the Parquet header is
    binary, and so, it does not display with `more` and `cat`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hadoop文件系统的`cat`命令列出元数据文件的内容，可以了解数据格式。但是Parquet头是二进制的，因此不能使用`more`和`cat`显示：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For more information about possible Spark and SQL context methods, check the
    contents of the classes called `org.apache.spark.SparkContext`, and `org.apache.spark.sql.SQLContext`,
    using the Apache Spark API path here for the specific `<version>` of Spark that
    you are interested in:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有关可能的Spark和SQL上下文方法的更多信息，请检查名为`org.apache.spark.SparkContext`和`org.apache.spark.sql.SQLContext`的类的内容，使用Apache
    Spark API路径，以获取您感兴趣的Spark的特定`<version>`：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the next section, I will examine Apache Spark DataFrames, introduced in Spark
    1.3.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我将研究在Spark 1.3中引入的Apache Spark DataFrames。
- en: DataFrames
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据框
- en: 'I have already mentioned that a DataFrame is based on a columnar format. Temporary
    tables can be created from it, but I will expand on this in the next section.
    There are many methods available to the data frame that allow data manipulation,
    and processing. I have based the Scala code used here, on the code in the last
    section, so I will just show you the working lines and the output. It is possible
    to display a data frame schema as shown here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经提到DataFrame是基于列的格式。可以从中创建临时表，但我将在下一节中展开。数据框可用许多方法允许数据操作和处理。我基于上一节中使用的Scala代码，所以我只会展示工作行和输出。可以像这样显示数据框模式：
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It is possible to use the `select` method to filter columns from the data.
    I have limited the output here, in terms of rows, but you get the idea:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`select`方法从数据中过滤列。在这里，我在行数方面进行了限制，但你可以理解：
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It is possible to filter the data returned from the DataFrame using the `filter`
    method. Here, I have added the occupation column to the output, and filtered on
    the worker age:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`filter`方法过滤从DataFrame返回的数据。在这里，我已经将职业列添加到输出中，并根据工人年龄进行了过滤：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'There is also a `group by` method for determining volume counts within a data
    set. As this is an income-based dataset, I think that volumes within the wage
    brackets would be interesting. I have also used a bigger dataset to give more
    meaningful results:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个`group by`方法用于确定数据集中的数量。由于这是一个基于收入的数据集，我认为工资范围内的数量会很有趣。我还使用了一个更大的数据集以获得更有意义的结果：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This is interesting, but what if I want to compare `income` brackets with `occupation`,
    and sort the results for a better understanding? The following example shows how
    this can be done, and gives the example data volumes. It shows that there is a
    high volume of managerial roles compared to other occupations. This example also
    sorts the output by the occupation column:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣，但如果我想比较`income`档次和`occupation`，并对结果进行排序以更好地理解呢？以下示例显示了如何做到这一点，并给出了示例数据量。它显示与其他职业相比，管理角色的数量很大。此示例还通过职业列对输出进行了排序：
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So, SQL-like actions can be carried out against DataFrames, including `select`,
    `filter`, sort `group by`, and `print`. The next section shows how tables can
    be created from the DataFrames, and how the SQL-based actions are carried out
    against them.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以对数据框执行类似SQL的操作，包括`select`、`filter`、排序`group by`和`print`。下一节将展示如何从数据框创建表，以及如何对其执行基于SQL的操作。
- en: Using SQL
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SQL
- en: 'After using the previous Scala example to create a data frame, from a CSV based-data
    input file on HDFS, I can now define a temporary table, based on the data frame,
    and run SQL against it. The following example shows the temporary table called
    `adult` being defined, and a row count being created using `COUNT(*)`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用先前的Scala示例从HDFS上的基于CSV的数据输入文件创建数据框后，我现在可以定义一个临时表，基于数据框，并对其运行SQL。以下示例显示了临时表`adult`的定义，并使用`COUNT(*)`创建了行数：
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This gives a row count of over 32,000 rows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了超过32,000行的行数：
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It is also possible to limit the volume of the data selected from the table
    using the `LIMIT` SQL option, which is shown in the following example. The first
    10 rows have been selected from the data, this is useful if I just want to check
    data types and quality:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用`LIMIT` SQL选项限制从表中选择的数据量，如下例所示。已从数据中选择了前10行，如果我只想检查数据类型和质量，这是有用的：
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'A sample of the data looks like the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的一个样本如下：
- en: '[PRE21]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'When the schema for this data was created in the Scala-based data frame example
    in the last section, all the columns were created as strings. However, if I want
    to filter the data in SQL using `WHERE` clauses, it would be useful to have proper
    data types. For instance, if an age column stores integer values, it should be
    stored as an integer so that I can execute numeric comparisons against it. I have
    changed my Scala code to include all the possible types:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当在上一节的基于Scala的数据框示例中创建此数据的模式时，所有列都被创建为字符串。但是，如果我想在SQL中使用`WHERE`子句过滤数据，那么拥有正确的数据类型将是有用的。例如，如果年龄列存储整数值，那么它应该存储为整数，以便我可以对其执行数值比较。我已经更改了我的Scala代码，以包括所有可能的类型：
- en: '[PRE22]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'I have also now defined my schema using different types, to better match the
    data, and I have defined the row data in terms of the actual data types, converting
    raw data string values into integer values, where necessary:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在也已经使用不同的类型定义了我的模式，以更好地匹配数据，并且已经根据实际数据类型定义了行数据，将原始数据字符串值转换为整数值：
- en: '[PRE23]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The SQL can now use numeric filters in the `WHERE` clause correctly. If the
    `age` column were a string, this would not work. You can now see that the data
    has been filtered to give age values below 60 years:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: SQL现在可以正确地在`WHERE`子句中使用数值过滤器。如果`age`列是字符串，这将无法工作。现在您可以看到数据已被过滤以给出60岁以下的年龄值：
- en: '[PRE24]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This gives a row count of around 30,000 rows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了大约30,000行的行数：
- en: '[PRE25]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'It is possible to use Boolean logic in the `WHERE`-based filter clauses. The
    following example specifies an age range for the data. Note that I have used variables
    to describe the `select` and `filter` components of the SQL statement. This allows
    me to break down the statement into different parts as they become larger:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在基于`WHERE`的过滤子句中使用布尔逻辑。以下示例指定了数据的年龄范围。请注意，我已经使用变量来描述SQL语句的`select`和`filter`组件。这使我能够将语句分解为不同的部分，因为它们变得更大：
- en: '[PRE26]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Giving a data count of around 23,000 rows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 给出了约23,000行的数据计数：
- en: '[PRE27]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'I can create compound filter clauses using the Boolean terms, such as `AND`,
    `OR`, as well as parentheses:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以使用布尔术语（如`AND`、`OR`）以及括号创建复合过滤子句：
- en: '[PRE28]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This gives me a row count of 17,000 rows, and represents a count of two age
    ranges in the data:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我一个约17,000行的行数，并表示数据中两个年龄范围的计数：
- en: '[PRE29]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It is also possible to use subqueries in Apache Spark SQL. You can see in the
    following example that I have created a subquery called `t1` by selecting three
    columns; `age`, `education`, and `occupation` from the table `adult`. I have then
    used the table called `t1` to create a row count. I have also added a filter clause
    acting on the age column from the table `t1`. Notice also that I have added `group
    by` and `order by` clauses, even though they are empty currently, to my SQL:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apache Spark SQL中也可以使用子查询。您可以在以下示例中看到，我通过从表`adult`中选择三列`age`、`education`和`occupation`来创建了一个名为`t1`的子查询。然后我使用名为`t1`的表创建了一个行数。我还在表`t1`的年龄列上添加了一个过滤子句。还要注意，我已经添加了`group
    by`和`order by`子句，尽管它们目前是空的，到我的SQL中：
- en: '[PRE30]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In order to examine the table joins, I have created a version of the adult
    CSV data file called `adult.train.data2`, which only differs from the original
    by the fact that it has an added first column called `idx`, which is a unique
    index. The Hadoop file system''s `cat` command here shows a sample of the data.
    The output from the file has been limited using the Linux `head` command:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查表连接，我创建了一个名为`adult.train.data2`的成人CSV数据文件的版本，它与原始文件的唯一区别是添加了一个名为`idx`的第一列，这是一个唯一索引。Hadoop文件系统的`cat`命令在这里显示了数据的一个样本。使用Linux的`head`命令限制了文件的输出：
- en: '[PRE31]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The schema has now been redefined to have an integer-based first column called
    `idx` for an index, as shown here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 模式现在已重新定义，具有整数类型的第一列`idx`作为索引，如下所示：
- en: '[PRE32]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'And the raw row RDD in the Scala example now processes the new initial column,
    and converts the string value into an integer:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala示例中的原始行RDD现在处理了新的初始列，并将字符串值转换为整数：
- en: '[PRE33]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We have looked at subqueries. Now, I would like to consider table joins. The
    next example will use the index that was just created. It uses it to join two
    derived tables. The example is somewhat contrived, given that it joins two data
    sets from the same underlying table, but you get the idea. Two derived tables
    are created as subqueries, and are joined at a common index column.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过子查询。现在，我想考虑表连接。下一个示例将使用刚刚创建的索引。它使用它来连接两个派生表。这个示例有点牵强，因为它连接了来自相同基础表的两个数据集，但你明白我的意思。两个派生表被创建为子查询，并在一个公共索引列上连接。
- en: 'The SQL for a table join now looks like this. Two derived tables have been
    created from the temporary table `adult` called `t1` and `t2` as subqueries. The
    new row index column called `idx` has been used to join the data in tables `t1`
    and `t2`. The major `SELECT` statement outputs all seven columns from the compound
    data set. I have added a `LIMIT` clause to minimize the data output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，表连接的SQL如下。从临时表`adult`创建了两个派生表，分别称为`t1`和`t2`作为子查询。新的行索引列称为`idx`已被用来连接表`t1`和`t2`中的数据。主要的`SELECT`语句从复合数据集中输出所有七列。我添加了一个`LIMIT`子句来最小化数据输出：
- en: '[PRE34]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Note that in the major `SELECT` statement, I have to define where the index
    column comes from, so I use `t1.idx`. All the other columns are unique to the
    `t1` and `t2` datasets, so I don''t need to use an alias to refer to them (that
    is, `t1.age`). So, the data that is output now looks like the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在主要的`SELECT`语句中，我必须定义索引列来自哪里，所以我使用了`t1.idx`。所有其他列都是唯一的`t1`和`t2`数据集，所以我不需要使用别名来引用它们（即`t1.age`）。因此，现在输出的数据如下：
- en: '[PRE35]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This gives some idea of the SQL-based functionality within Apache Spark, but
    what if I find that the method that I need is not available? Perhaps, I need a
    new function. This is where the **user-defined functions** (**UDFs**) are useful.
    I will cover them in the next section.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了Apache Spark中基于SQL的功能的一些想法，但如果我发现需要的方法不可用怎么办？也许我需要一个新函数。这就是**用户定义的函数**（**UDFs**）有用的地方。我将在下一节中介绍它们。
- en: User-defined functions
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户定义的函数
- en: In order to create some user-defined functions in Scala, I need to examine my
    data in the previous adult dataset. I plan to create a UDF that will enumerate
    the education column, so that I can convert the column into an integer value.
    This will be useful if I need to use the data for machine learning, and so create
    a LabelPoint structure. The vector used, which represents each record, will need
    to be numeric. I will first determine what kind of unique education values exist,
    then I will create a function to enumerate them, and finally use it in SQL.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Scala中创建一些用户定义的函数，我需要检查之前的成年人数据集中的数据。我计划创建一个UDF，用于枚举教育列，以便我可以将列转换为整数值。如果我需要将数据用于机器学习，并创建一个LabelPoint结构，这将非常有用。所使用的向量，代表每条记录，需要是数值型的。我将首先确定存在哪种唯一的教育值，然后创建一个函数来枚举它们，最后在SQL中使用它。
- en: 'I have created some Scala code to display a sorted list of the education values.
    The `DISTINCT` keyword ensures that there is only one instance of each value.
    I have selected the data as a subtable, using an alias called `edu_dist` for the
    data column to ensure that the `ORDER BY` clause works:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经创建了一些Scala代码来显示教育值的排序列表。`DISTINCT`关键字确保每个值只有一个实例。我已经选择数据作为子表，使用一个名为`edu_dist`的别名来确保`ORDER
    BY`子句起作用：
- en: '[PRE36]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The data looks like the following. I have removed some values to save space,
    but you get the idea:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数据如下。我已经删除了一些值以节省空间，但你明白我的意思：
- en: '[PRE37]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'I have defined a method in Scala to accept the string-based education value,
    and return an enumerated integer value that represents it. If no value is recognized,
    then a special value called `9999` is returned:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我在Scala中定义了一个方法，接受基于字符串的教育值，并返回代表它的枚举整数值。如果没有识别到值，则返回一个名为`9999`的特殊值：
- en: '[PRE38]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'I can now register this function using the SQL context in Scala, so that it
    can be used in an SQL statement:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我可以使用Scala中的SQL上下文注册此函数，以便在SQL语句中使用：
- en: '[PRE39]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The SQL, and the Scala code to enumerate the data then look like this. The
    newly registered function called `enumEdu` is used in the `SELECT` statement.
    It takes the education type as a parameter, and returns the integer enumeration.
    The column that this value forms is aliased to the name `idx`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，SQL和Scala代码用于枚举数据如下。新注册的名为`enumEdu`的函数在`SELECT`语句中使用。它以教育类型作为参数，并返回整数枚举。此值形成的列被别名为`idx`：
- en: '[PRE40]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The resulting data output, as a list of education values and their enumerations,
    looks like the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据输出，作为教育值及其枚举的列表，如下所示：
- en: '[PRE41]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Another example function called `ageBracket` takes the adult integer age value,
    and returns an enumerated age bracket:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个示例函数名为`ageBracket`，它接受成年人的整数年龄值，并返回一个枚举的年龄段：
- en: '[PRE42]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Again, the function is registered using the SQL context so that it can be used
    in an SQL statement:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，使用SQL上下文注册函数，以便在SQL语句中使用：
- en: '[PRE43]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Then, the Scala-based SQL uses it to select the age, age bracket, and education
    value from the adult dataset:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，基于Scala的SQL使用它从成年人数据集中选择年龄、年龄段和教育值：
- en: '[PRE44]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The resulting data then looks like this, given that I have used the `LIMIT`
    clause to limit the output to 10 rows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，由于我使用了`LIMIT`子句将输出限制为10行，因此生成的数据如下：
- en: '[PRE45]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'It is also possible to define functions for use in SQL, inline, during the
    UDF registration using the SQL context. The following example defines a function
    called `dblAge`, which just multiplies the adult''s age by two. The registration
    looks like this. It takes integer parameters (`age`), and returns twice its value:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以在SQL中定义函数，通过SQL上下文在UDF注册期间内联使用。以下示例定义了一个名为`dblAge`的函数，它只是将成年人的年龄乘以二。注册如下。它接受整数参数（`age`），并返回两倍的值：
- en: '[PRE46]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'And the SQL that uses it, now selects the `age`, and the double of the `age`
    value called `dblAge(age)`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 并且使用它的SQL现在选择`age`和`age`值的两倍，称为`dblAge(age)`：
- en: '[PRE47]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The two columns of the output data, which now contain the age and its doubled
    value, now look like this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，输出数据的两列包含年龄及其加倍值，看起来是这样的：
- en: '[PRE48]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: So far, DataFrames, SQL, and user-defined functions have been examined, but
    what if, as in my case, you are using a Hadoop stack cluster, and have Apache
    Hive available? The adult table that I have defined so far is a temporary table,
    but if I access Hive using Apache Spark SQL, I can access the static database
    tables. The next section will examine the steps needed to do this.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，已经检查了DataFrame、SQL和用户定义函数，但是如果像我一样使用Hadoop堆栈集群，并且有Apache Hive可用，会怎么样呢？到目前为止我定义的`adult`表是一个临时表，但是如果我使用Apache
    Spark SQL访问Hive，我可以访问静态数据库表。下一节将检查执行此操作所需的步骤。
- en: Using Hive
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hive
- en: If you have a business intelligence-type workload with low latency requirements
    and multiple users, then you might consider using Impala for your database access.
    Apache Spark on Hive is for batch processing and ETL chains. This section will
    be used to show how to connect Spark to Hive, and how to use this configuration.
    First, I will develop an application that uses a local Hive Metastore, and show
    that it does not store and persist table data in Hive itself. I will then set
    up Apache Spark to connect to the Hive Metastore server, and store tables and
    data within Hive. I will start with the local Metastore server.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有低延迟要求和多用户的商业智能类型工作负载，那么您可能考虑使用Impala来访问数据库。Apache Spark在Hive上用于批处理和ETL链。本节将用于展示如何连接Spark到Hive，以及如何使用此配置。首先，我将开发一个使用本地Hive元数据存储的应用程序，并展示它不会在Hive本身存储和持久化表数据。然后，我将设置Apache
    Spark连接到Hive元数据服务器，并在Hive中存储表和数据。我将从本地元数据服务器开始。
- en: Local Hive Metastore server
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地Hive元数据服务器
- en: 'The following example Scala code shows how to create a Hive context, and create
    a Hive-based table using Apache Spark. First, the Spark configuration, context,
    SQL, and Hive classes are imported. Then, an object class called `hive_ex1`, and
    the main method are defined. The application name is defined, and a Spark configuration
    object is created. The Spark context is then created from the configuration object:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例Scala代码显示了如何使用Apache Spark创建Hive上下文，并创建基于Hive的表。首先导入了Spark配置、上下文、SQL和Hive类。然后，定义了一个名为`hive_ex1`的对象类和主方法。定义了应用程序名称，并创建了一个Spark配置对象。然后从配置对象创建了Spark上下文：
- en: '[PRE49]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, I create a new Hive context from the Spark context, and import the Hive
    implicits, and the Hive context SQL. The `implicits` allow for implicit conversions,
    and the SQL include allows me to run Hive context-based SQL:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我从Spark上下文中创建一个新的Hive上下文，并导入Hive implicits和Hive上下文SQL。`implicits`允许进行隐式转换，而SQL包含允许我运行基于Hive上下文的SQL：
- en: '[PRE50]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The next statement creates an empty table called `adult2` in Hive. You will
    recognize the schema from the adult data that has already been used in this chapter:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个语句在Hive中创建了一个名为`adult2`的空表。您将会在本章中已经使用过的adult数据中识别出模式：
- en: '[PRE51]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, a row count is taken from the table called `adult2` via a `COUNT(*)`,
    and the output value is printed:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过`COUNT(*)`从名为`adult2`的表中获取行计数，并打印输出值：
- en: '[PRE52]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: As expected, there are no rows in the table.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，表中没有行。
- en: '[PRE53]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'It is also possible to create Hive-based external tables in Apache Spark Hive.
    The following HDFS file listing shows that the CSV file called `adult.train.data2`
    exists in the HDFS directory called `/data/spark/hive`, and it contains data:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apache Spark Hive中也可以创建基于Hive的外部表。以下的HDFS文件列表显示了名为`adult.train.data2`的CSV文件存在于名为`/data/spark/hive`的HDFS目录中，并且包含数据：
- en: '[PRE54]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, I adjust my Scala-based Hive SQL to create an external table called `adult3`
    (if it does not exist), which has the same structure as the previous table. The
    row format in this table-create statement specifies a comma as a row column delimiter,
    as would be expected for CSV data. The location option in this statement specifies
    the `/data/spark/hive` directory on HDFS for data. So, there can be multiple files
    on HDFS, in this location, to populate this table. Each file would need to have
    the same data structure matching this table structure:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我调整我的基于Scala的Hive SQL以创建一个名为`adult3`的外部表（如果不存在），该表与先前表具有相同的结构。在此表创建语句中的行格式指定逗号作为行列分隔符，这是CSV数据所期望的。此语句中的位置选项指定了HDFS上的`/data/spark/hive`目录作为数据的位置。因此，在此位置上可以有多个文件在HDFS上，用于填充此表。每个文件都需要具有与此表结构匹配的相同数据结构：
- en: '[PRE55]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'A row count is then taken against the `adult3` table, and the count is printed:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对`adult3`表进行行计数，并打印计数结果：
- en: '[PRE56]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As you can see, the table now contains around 32,000 rows. Since this is an
    external table, the HDFS-based data has not been moved, and the row calculation
    has been derived from the underlying CSV-based data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，表现在包含大约32,000行。由于这是一个外部表，基于HDFS的数据并没有被移动，行计算是从底层基于CSV的数据中推导出来的。
- en: '[PRE57]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'It occurs to me that I want to start stripping dimension data out of the raw
    CSV-based data in the external `adult3` table. After all, Hive is a data warehouse,
    so a part of a general ETL chain using the raw CSV-based data would strip dimensions
    and objects from the data, and create new tables. If I consider the education
    dimension, and try to determine what unique values exist, then for instance, the
    SQL would be as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我意识到我想要从外部的`adult3`表中剥离维度数据。毕竟，Hive是一个数据仓库，因此在使用基于原始CSV数据的一般ETL链的一部分时，会从数据中剥离维度和对象，并创建新的表。如果考虑教育维度，并尝试确定存在哪些唯一值，那么例如，SQL将如下所示：
- en: '[PRE58]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'And the ordered data matches the values that were derived earlier in this chapter
    using Spark SQL:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有序数据与本章早期使用Spark SQL推导出的值匹配：
- en: '[PRE59]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This is useful, but what if I want to create dimension values, and then assign
    integer index values to each of the previous education dimension values. For instance,
    `10th` would be `0`, and `11th` would be `1`. I have set up a dimension CSV file
    for the education dimension on HDFS, as shown here. The contents just contain
    the list of unique values, and an index:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有用，但如果我想创建维度值，然后为以前的教育维度值分配整数索引值怎么办。例如，`10th`将是`0`，`11th`将是`1`。我已经在HDFS上为教育维度设置了一个维度CSV文件，如下所示。内容只包含唯一值的列表和一个索引：
- en: '[PRE60]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, I can run some Hive QL in my Apache application to create an education
    dimension table. First, I drop the education table if it already exists, then
    I create the table by parsing the HDFS CSV file:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我可以在我的Apache应用程序中运行一些Hive QL来创建一个教育维度表。首先，如果教育表已经存在，我会删除它，然后通过解析HDFS CSV文件来创建表：
- en: '[PRE61]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: I can then select the contents of the new education table to ensure that it
    looks correct.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我可以选择新的教育表的内容，以确保它看起来是正确的。
- en: '[PRE62]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'This gives the expected list of indexes and the education dimension values:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了预期的索引列表和教育维度值：
- en: '[PRE63]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: So, I have the beginnings of an ETL pipeline. The raw CSV data is being used
    as external tables, and the dimension tables are being created, which could then
    be used to convert the dimensions in the raw data to numeric indexes. I have now
    successfully created a Spark application, which uses a Hive context to connect
    to a Hive Metastore server, which allows me to create and populate tables.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我已经开始了ETL管道的开端。原始CSV数据被用作外部表，然后创建了维度表，然后可以用来将原始数据中的维度转换为数字索引。我现在已经成功创建了一个Spark应用程序，它使用Hive上下文连接到Hive
    Metastore服务器，这使我能够创建和填充表。
- en: 'I have the Hadoop stack Cloudera CDH 5.3 installed on my Linux servers. I am
    using it for HDFS access while writing this book, and I also have Hive and Hue
    installed and running (CDH install information can be found at the Cloudera website
    at [http://cloudera.com/content/cloudera/en/documentation.html](http://cloudera.com/content/cloudera/en/documentation.html)).
    When I check HDFS for the `adult3` table, which should have been created under
    `/user/hive/warehouse`, I see the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我的Linux服务器上安装了Hadoop堆栈Cloudera CDH 5.3。我正在写这本书时使用它来访问HDFS，并且我还安装并运行了Hive和Hue（CDH安装信息可以在Cloudera网站[http://cloudera.com/content/cloudera/en/documentation.html](http://cloudera.com/content/cloudera/en/documentation.html)找到）。当我检查HDFS中的`adult3`表时，它应该已经创建在`/user/hive/warehouse`下，我看到了以下内容：
- en: '[PRE64]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The Hive-based table does not exist in the expected place for Hive. I can confirm
    this by checking the Hue Metastore manager to see what tables exist in the default
    database. The following figure shows that my default database is currently empty.
    I have added red lines to show that I am currently looking at the default database,
    and that there is no data. Clearly, when I run an Apache Spark-based application,
    with a Hive context, I am connecting to a Hive Metastore server. I know this because
    the log indicates that this is the case and also, my tables created in this way
    persist when Apache Spark is restarted.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Hive的表并不存在于Hive的预期位置。我可以通过检查Hue Metastore管理器来确认这一点，以查看默认数据库中存在哪些表。以下图表显示了我的默认数据库目前是空的。我已经添加了红线，以表明我目前正在查看默认数据库，并且没有数据。显然，当我运行基于Apache
    Spark的应用程序时，使用Hive上下文，我是连接到Hive Metastore服务器的。我知道这是因为日志表明了这一点，而且以这种方式创建的表在重新启动Apache
    Spark时会持久存在。
- en: '![Local Hive Metastore server](img/B01989_04_01.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![本地Hive Metastore服务器](img/B01989_04_01.jpg)'
- en: The Hive context within the application that was just run has used a local Hive
    Metastore server, and has stored data to a local location; actually in this case
    under `/tmp` on HDFS. I now want to use the Hive-based Metastore server, so that
    I can create tables and data in Hive directly. The next section will show how
    this can be done.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 刚刚运行的应用程序中的Hive上下文已经使用了本地Hive Metastore服务器，并将数据存储在本地位置；实际上，在这种情况下是在HDFS上的`/tmp`下。我现在想要使用基于Hive的Metastore服务器，这样我就可以直接在Hive中创建表和数据。接下来的部分将展示如何实现这一点。
- en: A Hive-based Metastore server
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于Hive的Metastore服务器
- en: I already mentioned that I am using Cloudera's CDH 5.3 Hadoop stack. I have
    Hive, HDFS, Hue, and Zookeeper running. I am using Apache Spark 1.3.1 installed
    under `/usr/local/spark`, in order to create and run applications (I know that
    CDH 5.3 is released with Spark 1.2, but I wanted to use DataFrames in this instance,
    which were available in Spark 1.3.x.).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经提到我正在使用Cloudera的CDH 5.3 Hadoop堆栈。我正在运行Hive、HDFS、Hue和Zookeeper。我正在使用安装在`/usr/local/spark`下的Apache
    Spark 1.3.1，以便创建和运行应用程序（我知道CDH 5.3发布了Spark 1.2，但我想在这种情况下使用Spark 1.3.x中可用的DataFrames）。
- en: 'The first thing that I need to do to configure Apache Spark to connect to Hive,
    is to drop the Hive configuration file called `hive-site.xml` into the Spark configuration
    directory on all servers where Spark is installed:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 配置Apache Spark连接到Hive的第一件事是将名为`hive-site.xml`的Hive配置文件放入所有安装了Spark的服务器上的Spark配置目录中：
- en: '[PRE65]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Then, given that I have installed Apache Hive via the CDH Manager to be able
    to use PostgreSQL, I need to install a PostgreSQL connector JAR for Spark, else
    it won''t know how to connect to Hive, and errors like this will occur:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，鉴于我已经通过CDH Manager安装了Apache Hive以便使用PostgreSQL，我需要为Spark安装一个PostgreSQL连接器JAR，否则它将不知道如何连接到Hive，并且会出现类似这样的错误：
- en: '[PRE66]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'I have stripped that error message down to just the pertinent parts, otherwise
    it would have been many pages long. I have determined the version of PostgreSQL
    that I have installed, as follows. It appears to be of version 9.0, determined
    from the Cloudera parcel-based jar file:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经将错误消息简化为只包含相关部分，否则它将非常长。我已经确定了我安装的PostgreSQL的版本，如下所示。从Cloudera基于包的jar文件中确定为9.0版本：
- en: '[PRE67]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, I have used the [https://jdbc.postgresql.org/](https://jdbc.postgresql.org/)
    website to download the necessary PostgreSQL connector library. I have determined
    my Java version to be 1.7, as shown here, which affects which version of library
    to use:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我使用[https://jdbc.postgresql.org/](https://jdbc.postgresql.org/)网站下载必要的PostgreSQL连接器库。我已确定我的Java版本为1.7，如下所示，这会影响要使用的库的版本：
- en: '[PRE68]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The site says that if you are using Java 1.7 or 1.8, then you should use the
    JDBC41 version of the library. So, I have sourced the `postgresql-9.4-1201.jdbc41.jar`
    file. The next step is to copy this file to the Apache Spark install `lib` directory,
    as shown here:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 该网站表示，如果您使用的是Java 1.7或1.8，则应该使用该库的JDBC41版本。因此，我已经获取了`postgresql-9.4-1201.jdbc41.jar`文件。下一步是将此文件复制到Apache
    Spark安装的`lib`目录中，如下所示：
- en: '[PRE69]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now, the PostgreSQL library must be added to the Spark `CLASSPATH`, by adding
    an entry to the file called `compute-classpath.sh`, in the Spark `bin` directory,
    as shown here:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，必须将PostgreSQL库添加到Spark的`CLASSPATH`中，方法是在Spark的`bin`目录中的名为`compute-classpath.sh`的文件中添加一个条目，如下所示：
- en: '[PRE70]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'In my case, I encountered an error regarding Hive versions between CDH 5.3
    Hive and Apache Spark as shown here. I thought that the versions were so close
    that I should be able to ignore this error:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我遇到了有关CDH 5.3 Hive和Apache Spark之间的Hive版本错误，如下所示。我认为版本如此接近，以至于我应该能够忽略这个错误：
- en: '[PRE71]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'I decided, in this case, to switch off schema verification in my Spark version
    of the `hive-site.xml` file. This had to be done in all the Spark-based instances
    of this file, and then Spark restarted. The change is shown here; the value is
    set to `false`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我决定在我的Spark版本的`hive-site.xml`文件中关闭模式验证。这必须在该文件的所有基于Spark的实例中完成，然后重新启动Spark。更改如下所示；值设置为`false`：
- en: '[PRE72]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Now, when I run the same set of application-based SQL as the last section,
    I can create objects in the Apache Hive default database. First, I will create
    the empty table called `adult2` using the Spark-based Hive context:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我运行与上一节相同的基于应用程序的SQL集时，我可以在Apache Hive默认数据库中创建对象。首先，我将使用基于Spark的Hive上下文创建名为`adult2`的空表：
- en: '[PRE73]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'As you can see, when I run the application and check the Hue Metastore browser,
    the table `adult2` now exists:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，当我运行应用程序并检查Hue元数据浏览器时，表`adult2`现在已经存在：
- en: '![A Hive-based Metastore server](img/B01989_04_02.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![基于Hive的元数据服务器](img/B01989_04_02.jpg)'
- en: 'I have shown the table entry previously, and it''s structure is obtained by
    selecting the table entry called `adult2`, in the Hue default database browser:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前展示了表条目，并通过选择称为`adult2`的表条目在Hue默认数据库浏览器中获得了其结构：
- en: '![A Hive-based Metastore server](img/B01989_04_03.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![基于Hive的元数据服务器](img/B01989_04_03.jpg)'
- en: 'Now the external table `adult3` Spark based Hive QL can be executed and data
    access confirmed from Hue. In the last section, the necessary Hive QL was as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以执行基于Spark的Hive QL的外部表`adult3`，并从Hue确认数据访问。在最后一节中，必要的Hive QL如下：
- en: '[PRE74]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'As you can now see, the Hive-based table called `adult3` has been created in
    the default database by Spark. The following figure is again generated from the
    Hue Metastore browser:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以看到，基于Hive的名为`adult3`的表已经由Spark创建在默认数据库中。下图再次生成自Hue元数据浏览器：
- en: '![A Hive-based Metastore server](img/B01989_04_04.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![基于Hive的元数据服务器](img/B01989_04_04.jpg)'
- en: 'The following Hive QL has been executed from the Hue Hive query editor. It
    shows that the `adult3` table is accessible from Hive. I have limited the rows
    to make the image presentable. I am not worried about the data, only the fact
    that I can access it:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Hive QL已从Hue Hive查询编辑器执行。它显示`adult3`表可以从Hive访问。我限制了行数以使图像可呈现。我不担心数据，只关心我能否访问它：
- en: '![A Hive-based Metastore server](img/B01989_04_05.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![基于Hive的元数据服务器](img/B01989_04_05.jpg)'
- en: 'The last thing that I will mention in this section which will be useful when
    using Hive QL from Spark against Hive, will be user-defined functions or UDF''s.
    As an example, I will consider the `row_sequence` function, which is used in the
    following Scala-based code:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在本节中提到的最后一件事对于在Spark中使用Hive QL对Hive进行操作将非常有用，那就是用户定义的函数或UDF。例如，我将考虑`row_sequence`函数，该函数在以下基于Scala的代码中使用：
- en: '[PRE75]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Either existing, or your own, JAR-based libraries can be made available to your
    Spark Hive session via the `ADD JAR` command. Then, the functionality within that
    library can be registered as a temporary function with `CREATE TEMPORARY FUNCTION`
    using the package-based class name. Then, the new function name can be incorporated
    in Hive QL statements.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`ADD JAR`命令可以将现有的或您自己的基于JAR的库添加到Spark Hive会话中。然后，可以使用基于包的类名将该库中的功能注册为临时函数，并在Hive
    QL语句中将新函数名称合并。
- en: This chapter has managed to connect an Apache Spark-based application to Hive,
    and run Hive QL against Hive, so that table and data changes persist in Hive.
    But why is this important? Well, Spark is an in-memory parallel processing system.
    It is an order faster than Hadoop-based Map Reduce in processing speed. Apache
    Spark can now be used as a processing engine, whereas the Hive data warehouse
    can be used for storage. Fast in-memory Spark-based processing speed coupled with
    big data scale structured data warehouse storage available in Hive.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 本章已成功将基于Apache Spark的应用程序连接到Hive，并对Hive运行Hive QL，以便表和数据更改在Hive中持久存在。但为什么这很重要呢？嗯，Spark是一种内存并行处理系统。它的处理速度比基于Hadoop的Map
    Reduce快一个数量级。Apache Spark现在可以作为处理引擎使用，而Hive数据仓库可以用于存储。快速的基于内存的Spark处理速度与Hive中可用的大数据规模结构化数据仓库存储相结合。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter started by explaining the Spark SQL context, and file I/O methods.
    It then showed that Spark and HDFS-based data could be manipulated, as both DataFrames
    with SQL-like methods and with Spark SQL by registering temporary tables. Next,
    user-defined functions were introduced to show that the functionality of Spark
    SQL could be extended by creating new functions to suit your needs, registering
    them as UDF's, and then calling them in SQL to process data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时解释了Spark SQL上下文和文件I/O方法。然后展示了可以操作基于Spark和HDFS的数据，既可以使用类似SQL的方法和DataFrames，也可以通过注册临时表和Spark
    SQL。接下来，介绍了用户定义的函数，以展示Spark SQL的功能可以通过创建新函数来扩展以满足您的需求，将它们注册为UDF，然后在SQL中调用它们来处理数据。
- en: Finally, the Hive context was introduced for use in Apache Spark. Remember that
    the Hive context in Spark offers a super set of the functionality of the SQL context.
    I understand that over time, the SQL context is going to be extended to match
    the Hive Context functionality. Hive QL data processing in Spark using a Hive
    context was shown using both, a local Hive, and a Hive-based Metastore server.
    I believe that the latter configuration is better, as the tables are created,
    and data changes persist in your Hive instance.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Hive上下文被引入用于在Apache Spark中使用。请记住，Spark中的Hive上下文提供了SQL上下文功能的超集。我知道随着时间的推移，SQL上下文将被扩展以匹配Hive上下文的功能。在Spark中使用Hive上下文进行Hive
    QL数据处理时，使用了本地Hive和基于Hive的Metastore服务器。我认为后者的配置更好，因为表被创建，数据更改会持久保存在您的Hive实例中。
- en: In my case, I used Cloudera CDH 5.3, which used Hive 0.13, PostgreSQL, ZooKeeper,
    and Hue. I also used Apache Spark version 1.3.1\. The configuration setup that
    I have shown you is purely for this configuration. If you wanted to use MySQL,
    for instance, you would need to research the necessary changes. A good place to
    start would be the `<[user@spark.apache.org](mailto:user@spark.apache.org)>` mailing
    list.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的案例中，我使用的是Cloudera CDH 5.3，其中使用了Hive 0.13、PostgreSQL、ZooKeeper和Hue。我还使用了Apache
    Spark版本1.3.1。我向您展示的配置设置纯粹是针对这个配置的。如果您想使用MySQL，例如，您需要研究必要的更改。一个好的起点可能是`<[user@spark.apache.org](mailto:user@spark.apache.org)>`邮件列表。
- en: Finally, I would say that Apache Spark Hive context configuration, with Hive-based
    storage, is very useful. It allows you to use Hive as a big data scale data warehouse,
    with Apache Spark for fast in-memory processing. It offers you the ability to
    manipulate your data with not only the Spark-based modules (MLlib, SQL, GraphX,
    and Stream), but also other Hadoop-based tools, making it easier to create ETL
    chains.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想说Apache Spark Hive上下文配置，使用基于Hive的存储，非常有用。它允许您将Hive用作大数据规模的数据仓库，使用Apache
    Spark进行快速的内存处理。它不仅提供了使用基于Spark的模块（MLlib、SQL、GraphX和Stream）操纵数据的能力，还提供了使用其他基于Hadoop的工具，使得创建ETL链更加容易。
- en: The next chapter will examine the Spark graph processing module, GraphX, it
    will also investigate the Neo4J graph database, and the MazeRunner application.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将研究Spark图处理模块GraphX，还将调查Neo4J图数据库和MazeRunner应用程序。
