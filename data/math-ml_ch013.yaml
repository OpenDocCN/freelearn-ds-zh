- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Matrix Factorizations
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: One of the recurring thoughts in this book is that problem-solving is about
    finding the best representations of your objects of study. Say linear transformations
    of a vector space are represented by matrices. Studying one is the same as studying
    the other, but each perspective comes with its own set of tools. Linear transformations
    are geometric, while matrices are algebraic sides of the same coin.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的一个反复出现的观点是，问题解决的关键在于找到你研究对象的最佳表示。 比如，向量空间的线性变换由矩阵表示。 研究一个就等同于研究另一个，但每种视角都有其独特的工具。
    线性变换是几何的，而矩阵则是代数的，二者是同一枚硬币的两面。
- en: This thought can be applied on a smaller scale as well. Recall the LU decomposition
    from Chapter [6](ch011.xhtml#matrices-and-equations). You can think of this as
    another view of matrices.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思路也可以应用于更小的范围。 回想一下第[6章](ch011.xhtml#matrices-and-equations)中的LU分解。 你可以将其视为矩阵的另一种视角。
- en: 'Guess what: It’s not the only one. This chapter is dedicated to the three most
    important ones:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 猜猜看：这不是唯一的。 本章将重点介绍三个最重要的分解方法：
- en: the spectral decomposition,
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 光谱分解，
- en: the singular value decomposition,
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值分解，
- en: and the QR decomposition.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及 QR 分解。
- en: Buckle up. It’s our most challenging adventure yet.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 系好安全带。 这是我们迄今为止最具挑战性的冒险。
- en: 7.1 Special transformations
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 特殊变换
- en: So far, we have aspired to develop a geometric view of linear algebra. Vectors
    are mathematical objects defined by their direction and magnitude. In the spaces
    of vectors, the concept of distance and orthogonality gives rise to a geometric
    structure.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在努力发展线性代数的几何视角。 向量是通过其方向和大小来定义的数学对象。在向量空间中，距离和正交性的概念衍生出了几何结构。
- en: 'Linear transformations, the building blocks of machine learning, are just mappings
    that distort this structure: rotating, stretching, and skewing the geometry. However,
    there are types of transformations that preserve some of the structure. In practice,
    these provide valuable insights, and additionally, they are much easier to work
    with. In this section, we will take a look at the most important ones, those that
    we’ll encounter in machine learning.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 线性变换是机器学习的构建模块，它们只是扭曲结构的映射：旋转、拉伸和扭曲几何形状。 然而，也有一些变换类型能够保持部分结构。 实际上，这些变换提供了宝贵的见解，此外，它们也更容易处理。
    在本节中，我们将重点介绍最重要的变换类型，那些我们在机器学习中会遇到的变换。
- en: 7.1.1 The adjoint transformation
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 伴随变换
- en: In machine learning, the most important stage is the Euclidean space ℝ^n. This
    is where data is represented and manipulated. There, the entire geometric structure
    is defined by the inner product
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，最重要的空间是欧几里得空间 ℝ^n。 这是数据表示和操作的地方。 在那里，整个几何结构由内积定义。
- en: '![ ∑n ⟨x,y ⟩ = xiyi, i=1 ](img/file626.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ⟨x,y ⟩ = xiyi, i=1 ](img/file626.png)'
- en: 'giving rise to the notion of magnitude, direction (in the form of angles),
    and orthogonality. Because of this, transformations that can be related to the
    inner product are special. For instance, if ⟨f(x),f(x)⟩ = ⟨x,x⟩ holds for all
    x ∈ℝ^n and the linear transformation f : ℝ^n →ℝ^n, we know that f leaves the norm
    invariant. That is, distance in the original and the transformed feature space
    have the same meaning.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '产生了大小、方向（以角度的形式）和正交性的概念。 因此，可以与内积相关的变换是特殊的。 例如，如果对所有 x ∈ℝ^n 和线性变换 f : ℝ^n →ℝ^n，满足
    ⟨f(x), f(x)⟩ = ⟨x, x⟩，我们知道 f 保持范数不变。 也就是说，原始和变换后的特征空间中的距离具有相同的意义。'
- en: First, we will establish a general relation between images of vectors under
    a transform and their inner product. This is going to be the foundation for our
    discussions in this chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将建立一个关于变换下向量映像与它们内积之间的普遍关系。 这将成为本章讨论的基础。
- en: Theorem 39\. (The adjoint transformation)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 39.（伴随变换）
- en: 'Let f : ℝ^n →ℝ^n be a linear transformation. Then, there exists a linear transformation
    f^∗ : ℝ^n →ℝ^n for which'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ^n 是一个线性变换。 那么，存在一个线性变换 f^∗ : ℝ^n →ℝ^n，使得'
- en: ⟨f(**x**), **y**⟩ = ⟨**x**, f^*(**y**)⟩
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ⟨f(**x**), **y**⟩ = ⟨**x**, f^*(**y**)⟩
- en: (7.1)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: （7.1）
- en: holds for all x,y ∈ℝ^n. f^∗ is called the adjoint transformation* of f.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 x,y ∈ℝ^n 都成立。 f^∗ 被称为 f 的伴随变换*。
- en: Moreover, if A ∈ℝ^(n×n) is the matrix of f in the standard orthonormal basis,
    then the matrix of f^∗ is A^T . That is,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果 A ∈ℝ^(n×n) 是 f 在标准正交基下的矩阵，那么 f^∗ 的矩阵就是 A^T。 即，
- en: ⟨A**x**, **y**⟩ = ⟨**x**, A^T**y**⟩.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ⟨A**x**, **y**⟩ = ⟨**x**, A^T**y**⟩。
- en: (7.2)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: （7.2）
- en: Proof. Suppose that A ∈ ℝ^(n×n) is the matrix of f in the standard orthonormal
    basis. For any x = (x[1],…,x[n]) and y = (y[1],…,y[n]), the inner product is defined
    by
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。假设 A ∈ ℝ^(n×n) 是 f 在标准正交基中的矩阵。对于任何 x = (x[1],…,x[n]) 和 y = (y[1],…,y[n])，内积定义为
- en: '![ ∑n ⟨x,y ⟩ = xiyi, i=1 ](img/file627.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ⟨x,y ⟩ = xiyi, i=1 ](img/file627.png)'
- en: and Ax can be written as
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Ax 可以写成
- en: '![⌊ ⌋ |a11 a12 ... a1n |⌊ ⌋ ⌊ ∑n ⌋ |a a ... a || x1| | ∑ j=1a1jxj| || 21\.
    2.2 . 2n. |||| x2|| || nj=1a2jxj|| || .. .. .. .. |||| ..|| = || .. || . || ||⌈
    .⌉ ⌈ . ⌉ ⌈an1 an2 ... ann ⌉ xn ∑n anjxj j=1 ](img/file628.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![⌊ ⌋ |a11 a12 ... a1n |⌊ ⌋ ⌊ ∑n ⌋ |a a ... a || x1| | ∑ j=1a1jxj| || 21\.
    2.2 . 2n. |||| x2|| || nj=1a2jxj|| || .. .. .. .. |||| ..|| = || .. || . || ||⌈
    .⌉ ⌈ . ⌉ ⌈an1 an2 ... ann ⌉ xn ∑n anjxj j=1 ](img/file628.png)'
- en: Using this form, we can express ⟨Ax,y⟩ in terms of a[ij]-s, x[i]-s, and y[i]-s.
    For this, we have
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个形式，我们可以将 ⟨Ax,y⟩ 表示为 a[ij]、x[i] 和 y[i] 的形式。为此，我们有
- en: '![ ∑n ∑n ⟨Ax, y⟩ = ( aijxj)yi i=1 j=1 ∑n ∑n = ( aijyi) xj j=1 i=1 ◟--◝◜---◞
    T j- th component of A y = ⟨x,AT y⟩. ](img/file629.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ∑n ⟨Ax, y⟩ = ( aijxj)yi i=1 j=1 ∑n ∑n = ( aijyi) xj j=1 i=1 ◟--◝◜---◞
    T j- th component of A y = ⟨x,AT y⟩. ](img/file629.png)'
- en: 'This shows that the transformation given by f^∗ : x→A^T x satisfies ([7.1](ch013.xhtml#x1-116003r39))
    and ([7.2](ch013.xhtml#x1-116003r39)), which is what we had to show.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '这表明由 f^∗ : x→A^T x 给出的变换满足 ([7.1](ch013.xhtml#x1-116003r39)) 和 ([7.2](ch013.xhtml#x1-116003r39))，这正是我们需要证明的。'
- en: Why is the quantity ⟨Ax,y⟩ that important to us? Because inner products define
    the geometric structure of a vector space. Recall the equation ([2.12](ch008.xhtml#x1-46005r12)),
    allowing us to fully describe any vector using only the inner products with respect
    to an orthonormal basis. In addition, ⟨x,x⟩ = ∥x∥² defines the notion of distance
    and magnitude. Because of this, ([7.1](ch013.xhtml#x1-116003r39)) and ([7.2](ch013.xhtml#x1-116003r39))
    will be quite useful for us.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么内积 ⟨Ax,y⟩ 对我们来说如此重要？因为内积定义了一个向量空间的几何结构。回想方程 ([2.12](ch008.xhtml#x1-46005r12))，它允许我们仅通过与正交基的内积来完全描述任何向量。此外，⟨x,x⟩
    = ∥x∥² 定义了距离和大小的概念。正因为如此，([7.1](ch013.xhtml#x1-116003r39)) 和 ([7.2](ch013.xhtml#x1-116003r39))
    对我们来说将是非常有用的。
- en: As we are about to see, transformations that preserve the inner product are
    rather special, and these relationships provide us with a way to characterize
    them both algebraically and geometrically.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们即将看到的，保持内积的变换是非常特殊的，这些关系为我们提供了代数和几何上描述它们的方式。
- en: 7.1.2 Orthogonal transformations
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 正交变换
- en: Let’s jump straight into the definition.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入定义。
- en: Definition 26\. (Orthogonal transformations)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 26\. （正交变换）
- en: 'Let f : ℝ^n →ℝ^n be an arbitrary linear transformation. f is called orthogonal
    if'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n → ℝ^n 是一个任意的线性变换。如果 f 是正交的，那么'
- en: '![⟨f(x),f(y)⟩ = ⟨x,y ⟩ ](img/file631.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![⟨f(x),f(y)⟩ = ⟨x,y ⟩ ](img/file631.png)'
- en: holds for all x,y ∈ℝ^n.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有的 x, y ∈ ℝ^n 都成立。
- en: 'As a consequence, an orthogonal f preserves the norm: ∥f(x)∥² = ⟨f(x),f(x)⟩
    = ⟨x,x⟩ = ∥x∥². Because the angle enclosed by two vectors is defined by their
    inner product, see equation (2.9), the property ⟨f(x),f(y)⟩ = ⟨x,y⟩ means that
    an orthogonal transform also preserves angles.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正交变换 f 保持范数：∥f(x)∥² = ⟨f(x),f(x)⟩ = ⟨x,x⟩ = ∥x∥²。由于两个向量之间的夹角由它们的内积定义，见方程
    (2.9)，性质 ⟨f(x),f(y)⟩ = ⟨x,y⟩ 意味着正交变换也保持角度。
- en: We can translate the definition to the language of matrices as well. In practice,
    we are always going to work with matrices, so this characterization is essential.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将定义转换为矩阵的语言。在实际应用中，我们总是会使用矩阵，因此这个表述是至关重要的。
- en: Theorem 40\. (Matrices of orthogonal transformations)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 40\. （正交变换的矩阵）
- en: 'Let f : ℝ^n → ℝ^n be a linear transformation and A ∈ ℝ^(n×n) be its matrix
    in the standard orthonormal basis. Then, f is orthogonal if, and only if, A^T
    = A^(−1).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n → ℝ^n 是线性变换，A ∈ ℝ^(n×n) 是 f 在标准正交基中的矩阵。那么，f 是正交的，当且仅当，A^T = A^(−1)。'
- en: Proof. As usual, we have to show the implication in both ways.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。像往常一样，我们需要在两个方向上都证明这个蕴含。
- en: (a) Suppose that f is orthogonal. Then, ([7.2](ch013.xhtml#x1-116003r39)) gives
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 假设 f 是正交的。那么，([7.2](ch013.xhtml#x1-116003r39)) 给出了
- en: '![ T ⟨x, y⟩ = ⟨Ax, Ay ⟩ = ⟨x, A Ay ⟩. ](img/file632.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![ T ⟨x, y⟩ = ⟨Ax, Ay ⟩ = ⟨x, A Ay ⟩. ](img/file632.png)'
- en: Thus, for any given y,
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于任何给定的 y，
- en: '![ T ⟨x,(A A − I)y ⟩ = 0 ](img/file633.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![ T ⟨x,(A A − I)y ⟩ = 0 ](img/file633.png)'
- en: holds for all x. By letting x = (A^T A −I)y, the positive definiteness of the
    inner product implies that (A^T A −I)y = 0 for all y. Thus, A^T A = I, which means
    that A^T is the inverse of A.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有的 x 都成立。通过让 x = (A^T A −I)y，内积的正定性意味着 (A^T A −I)y = 0 对所有 y 成立。因此，A^T A =
    I，这意味着 A^T 是 A 的逆矩阵。
- en: (b) If A^T = A^(−1), we have
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 如果 A^T = A^(−1)，我们有
- en: '![⟨Ax, Ay ⟩ = ⟨x,AT Ay ⟩ −1 = ⟨x,A Ay ⟩ = ⟨x,y ⟩, ](img/file634.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![⟨Ax, Ay ⟩ = ⟨x,AT Ay ⟩ −1 = ⟨x,A Ay ⟩ = ⟨x,y ⟩, ](img/file634.png)'
- en: showing that f is orthogonal.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 显示 f 是正交的。
- en: The fact that A^T = A^(−1) has a profound implication regarding the columns
    of A. If you think back to the definition of matrix multiplication in Section [4.1.2](ch010.xhtml#matrix-operations-revisited),
    the element in the i-th row and j-th column of AB is the inner product of the
    i-th row of A and the j-th column of B.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: A^T = A^(−1) 这一事实对 A 的列有着深远的影响。如果你回想一下第 [4.1.2](ch010.xhtml#matrix-operations-revisited)
    节中矩阵乘法的定义，AB 中第 i 行第 j 列的元素就是 A 的第 i 行与 B 的第 j 列的内积。
- en: To be more precise, if the i-th column is denoted by a[i] = (a[1,i],a[2,i],…,a[n,i]),
    then we have
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更精确地说，若第 i 列记作 a[i] = (a[1,i],a[2,i],…,a[n,i])，则我们有
- en: '![AT A = (⟨ai,aj⟩)n = I, i,j=1 ](img/file635.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![AT A = (⟨ai,aj⟩)n = I, i,j=1 ](img/file635.png)'
- en: that is,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，
- en: '![ ( |{ ⟨a ,a ⟩ = 1 if i = j, i j |( 0 otherwise. ](img/file636.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{ ⟨a ,a ⟩ = 1 如果 i = j, i j |( 其他情况为 0. ](img/file636.png)'
- en: In other words, the columns of A form an orthonormal system. This fact should
    not come as a surprise since orthogonal transformations preserve magnitude and
    orthogonality, and the columns of A are the images of the standard orthonormal
    basis e[1],…,e[n].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，A 的列构成了一个正交归一系统。这个事实应该不让人感到意外，因为正交变换保持了大小和正交性，而 A 的列是标准正交基底 e[1],…,e[n]
    的像。
- en: In machine learning, performing an orthogonal transformation on our features
    is equivalent to looking at them from another perspective, without distortion.
    You might know it already, but this is what Principal Component Analysis (PCA)
    is doing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，对特征进行正交变换相当于从另一个角度看待它们，而不会产生失真。你可能已经知道，但这正是主成分分析（PCA）所做的事情。
- en: 7.2 Self-adjoint transformations and the spectral decomposition theorem
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 自伴随变换与谱分解定理
- en: 'Besides orthogonal transformations, there is another important class: transformations
    whose adjoints are themselves. Bear with me a bit, and we’ll see an example soon.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了正交变换，还有另一类重要的变换：其伴随矩阵就是它本身。稍微忍耐一下，我们很快就会看到一个例子。
- en: Definition 27\. (Self-adjoint transformations)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 27\.（自伴随变换）
- en: 'Let f : ℝ^n →ℝ^n be a linear transformation. f is self-adjoint if f^∗ = f,
    that is,'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ^n 是一个线性变换。如果 f 是自伴随的，则有 f^∗ = f，也就是说，'
- en: ⟨f(**x**), **y**⟩ = ⟨**x**, f(**y**)⟩
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ⟨f(**x**), **y**⟩ = ⟨**x**, f(**y**)⟩
- en: (7.3)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (7.3)
- en: holds for all x,y ∈ℝ^n.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 x,y ∈ℝ^n 都成立。
- en: As always, we are going to translate this into the language of matrices. If
    A is the matrix of f in the standard orthonormal basis, we know that A^T is the
    matrix of the adjoint. For self-adjoint transformations, it implies that A^T =
    A. Matrices such as these are called symmetric, and they have a lot of pleasant
    properties.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我们将这转化为矩阵的语言。如果 A 是 f 在标准正交基底下的矩阵，我们知道 A^T 是其伴随矩阵。对于自伴随变换，意味着 A^T = A。像这样的矩阵称为对称矩阵，它们具有许多良好的性质。
- en: For us, the most important one is that symmetric matrices can be diagonalized!
    (That is,they can be transformed into a diagonal matrix with a check reference)
    The following theorem makes this precise.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们而言，最重要的一点是对称矩阵可以对角化！（也就是说，它们可以被转换成一个对角矩阵，并有检查参考）以下定理精确地说明了这一点。
- en: Theorem 41\. (Spectral decomposition of real symmetric matrices)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 41\.（实对称矩阵的谱分解）
- en: Let A ∈ ℝ^(n×n) be a real symmetric matrix. Then, A has exactly n real eigenvalues
    λ[1] ≥⋅⋅⋅≥λ[n], and the corresponding eigenvectors u[1],…,u[n] can be selected
    such that they form an orthonormal basis.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 设 A ∈ ℝ^(n×n) 是一个实对称矩阵。那么，A 恰好有 n 个实特征值 λ[1] ≥⋅⋅⋅≥λ[n]，并且可以选择对应的特征向量 u[1],…,u[n]，使它们构成一个正交归一基底。
- en: Moreover, if we let Λ = diag(λ[1],…,λ[n]) and U be the orthogonal matrix whose
    columns are u[1],…,u[n], then
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们令 Λ = diag(λ[1],…,λ[n]) 且 U 是一个正交矩阵，其列是 u[1],…,u[n]，则
- en: A = UΛU^T
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: A = UΛU^T
- en: (7.4)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (7.4)
- en: holds.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 成立。
- en: Note that the eigenvalues λ[1] ≥⋅⋅⋅≥λ[n] are not necessarily distinct from each
    other.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，特征值 λ[1] ≥⋅⋅⋅≥λ[n] 不一定彼此不同。
- en: Proof. (Sketch) Since the proof is pretty involved, we are better off getting
    to know the main ideas behind it, without all the mathematical details.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。（大致思路）由于证明过程相当复杂，我们最好先了解其中的主要思路，而不必深入所有的数学细节。
- en: The main steps are the following.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 主要步骤如下。
- en: If the matrix A is symmetric, all of its eigenvalues are real.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果矩阵 A 是对称的，则它的所有特征值都是实数。
- en: Using this, it can be shown that an orthonormal basis can be formed from the
    eigenvectors of A.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用这一点，可以证明可以从 A 的特征向量中形成一个正交归一基底。
- en: Writing the matrix of the transformation x → Ax in this orthonormal basis yields
    a diagonal matrix. Hence, a change of basis yields ([7.4](ch013.xhtml#x1-118004r41)).
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个正交基下将变换 x → Ax 的矩阵写成对角矩阵。因此，基变换得到了 ([7.4](ch013.xhtml#x1-118004r41))。
- en: Showing that the eigenvalues are real requires some complex number magic (which
    is beyond the scope of this chapter). The tough part is the second step. Once
    that has been done, moving to the third one is straightforward, as we have seen
    when talking about eigenspaces and their bases (Section [6.3](ch012.xhtml#eigenvectors-eigenspaces-and-their-bases)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 显示特征值为实数需要一些复数魔法（这超出了本章的范围）。困难的部分是第二步。一旦完成这一步，移动到第三步就很直接了，就像我们在讨论特征空间及其基向量时看到的那样（第
    [6.3](ch012.xhtml#eigenvectors-eigenspaces-and-their-bases) 节）。
- en: 'We still don’t have a hands-on way to diagonalize matrices, but this theorem
    gets us one step closer: at least we know it is possible for symmetric matrices.
    This is an important stepping stone, as we’ll be able to reduce the general case
    to the symmetric one.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然没有一种实际的方法来对角化矩阵，但这个定理让我们更接近一步：至少我们知道对称矩阵的对角化是可能的。这是一个重要的步骤，因为我们可以将一般情况简化到对称情况。
- en: The requirement for a matrix to be symmetric seems like a very special one.
    However, in practice, we can symmetrize matrices in several different ways. For
    any matrix A ∈ℝ^(n×m), the products AA^T and A^T A will be symmetric. For square
    matrices, the average ![A+AT- 2](img/file639.png) also works. So, symmetric matrices
    are more common than you think.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵对称的要求看起来是一个非常特殊的条件。然而，在实践中，我们可以通过几种不同的方式使矩阵对称化。对于任意矩阵 A ∈ℝ^(n×m)，乘积 AA^T 和
    A^T A 将是对称的。对于方阵，平均值 ![A+AT- 2](img/file639.png) 也适用。因此，对称矩阵比你想象的更常见。
- en: The orthogonal matrix U and the corresponding orthonormal basis {u[1],…,u[n]}
    that diagonalizes a symmetric matrix A has a special property that is going to
    be very important in machine learning for the principal component analysis of
    data samples.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正交矩阵 U 和相应的正交基 {u[1],…,u[n]}，对称矩阵 A 的对角化具有一个非常重要的特性，在机器学习中进行数据样本的主成分分析时非常重要。
- en: 'Before looking at the next theorem, we introduce the argmax notation. Recall
    that the expression max[x∈A]f(x) denotes the maximum value of the function over
    the set A. Often, we would like to know where that maximum is attained, which
    is defined by the argmax:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在看下一个定理之前，我们引入 argmax 符号。回想一下，表达式 max[x∈A]f(x) 表示函数在集合 A 上的最大值。通常，我们想知道这个最大值取到的位置，这由
    argmax 定义：
- en: '![ ∗ x = argmaxx ∈Af (x), f(x∗) = max f(x). x∈A ](img/file640.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![ ∗ x = argmaxx ∈Af (x), f(x∗) = max f(x). x∈A ](img/file640.png)'
- en: Now, let’s see the fundamentals of PCA!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 PCA 的基础！
- en: Theorem 42\.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 42。
- en: Let A ∈ ℝ^(n×n) be a real symmetric matrix and let λ[1] ≥ ⋅⋅⋅ ≥ λ[n] be its
    real eigenvalues in decreasing order. Moreover, let U ∈ ℝ^(n×n) be the orthogonal
    matrix that diagonalizes A, with the corresponding orthonormal basis {u[1],…,u[n]}.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 设 A ∈ ℝ^(n×n) 是一个实对称矩阵，λ[1] ≥ ⋅⋅⋅ ≥ λ[n] 是它的实特征值按降序排列。此外，设 U ∈ ℝ^(n×n) 是对角化
    A 的正交矩阵，对应的正交基为 {u[1],…,u[n]}。
- en: Then,
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，
- en: '![ T arg m∥axx∥=1 x Ax = u1, ](img/file642.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![ T arg m∥axx∥=1 x Ax = u1, ](img/file642.png)'
- en: and
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![ max xT Ax = λ1\. ∥x∥=1 ](img/file643.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![ max xT Ax = λ1\. ∥x∥=1 ](img/file643.png)'
- en: 'Proof. Since {u[1],…,u[n]} is an orthonormal basis, any x can be expressed
    as a linear combination of them:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。由于 {u[1],…,u[n]} 是正交基，任何 x 都可以表示为它们的线性组合：
- en: '![ n ∑ x = xiui, xi ∈ ℝ. i=1 ](img/file644.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![ n ∑ x = xiui, xi ∈ ℝ. i=1 ](img/file644.png)'
- en: Thus, since the u[i] are eigenvectors of A,
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于 u[i] 是 A 的特征向量，
- en: '![ ∑n ∑n ∑n Ax = A( xiui) = xiAui = xiλiui. i=1 i=1 i=1 ](img/file645.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ∑n ∑n Ax = A( xiui) = xiAui = xiλiui. i=1 i=1 i=1 ](img/file645.png)'
- en: Plugging it back into x^T Ax, we have
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将其代入 x^T Ax，我们有
- en: '![ T ∑n T ∑n x Ax = ( xjuj) A ( xiui) j=1 i=1 ∑n ∑n = ( xjuTj )A ( xiui) j=1
    i=1 ∑n ∑n = ( xjuTj )( xiλiui) j=1 i=1 ](img/file646.png)![ ∑n T = xixjλiuj ui.
    i,j=1 ](img/file647.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![ T ∑n T ∑n x Ax = ( xjuj) A ( xiui) j=1 i=1 ∑n ∑n = ( xjuTj )A ( xiui) j=1
    i=1 ∑n ∑n = ( xjuTj )( xiλiui) j=1 i=1 ](img/file646.png)![ ∑n T = xixjλiuj ui.
    i,j=1 ](img/file647.png)'
- en: Since the u[i]-s form an orthonormal basis,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 u[i] 形成正交基，
- en: '![ (| T {1 if i = j, uj ui = ⟨ui,uj⟩ = | (0 otherwise. ](img/file648.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![ (| T {1 if i = j, uj ui = ⟨ui,uj⟩ = | (0 otherwise. ](img/file648.png)'
- en: In other words, u[j]^T u[i] vanishes when i≠j. Continuing the above calculation
    with this observation,
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当 i≠j 时，u[j]^T u[i] 为零。继续以上计算时，考虑到这一观察结果，
- en: '![ ](img/file649.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![ ](img/file649.png)'
- en: When ![ ](img/file650.png), the sum ![ ](img/file651.png) is a weighted average
    of the eigenvalues ![ ](img/file652.png). So,
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ![ ](img/file650.png) 时，和 ![ ](img/file651.png) 是特征值 ![ ](img/file652.png)
    的加权平均。因此，
- en: '![ ](img/file653.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![ ](img/file653.png)'
- en: from which x^T Ax ≤ λ[1] follows. (Recall that we can assume without loss in
    generality that the eigenvalues are decreasing.) On the other hand, by plugging
    in x = u[1], we can see that u[1]^T Au[1] = λ[1], so the maximum is indeed attained.
    From these two, the theorem follows.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以得出x^T Ax ≤ λ[1]。（回忆一下，我们可以在不失一般性的情况下假设特征值是递减的。）另一方面，通过代入x = u[1]，我们可以看到u[1]^T
    Au[1] = λ[1]，因此最大值确实达到了。从这两点，定理得以证明。
- en: Remark 6\.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注释 6\。
- en: In other words, Theorem [42](ch013.xhtml#x1-118012r42) gives that the function
    x→x^T Ax assumes its maximum value at u[1], and that maximum value is u[1]^T Au[1]
    = λ[1]. The quantity x^T Ax seems quite mysterious as well, so let’s clarify this
    a bit. If we think in terms of features, the vectors u[1],…,u[n] can be thought
    of as mixtures of the “old” features e[1],…,e[n]. When we have actual observations
    (that is, data), we can use the above process to diagonalize the covariance matrix.
    So, if A denotes this covariance matrix, u[1]^T Au[1] is the variance of the new
    feature u[1].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，定理[42](ch013.xhtml#x1-118012r42)表明，函数x→x^T Ax在u[1]处取得最大值，并且该最大值为u[1]^T
    Au[1] = λ[1]。x^T Ax这个量看起来也相当神秘，因此让我们稍作澄清。如果我们从特征角度来看，向量u[1],…,u[n]可以看作是“旧”特征e[1],…,e[n]的组合。当我们有实际观察数据（即数据）时，可以使用上述过程对协方差矩阵进行对角化。所以，如果A表示这个协方差矩阵，u[1]^T
    Au[1]就是新特征u[1]的方差。
- en: Thus, this theorem says that u[1] is the unique feature that maximizes the variance.
    So, among all the possible choices for new features, u[1] conveys the most information
    about the data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，定理表明u[1]是唯一能够最大化方差的特征。因此，在所有可能的新特征选择中，u[1]传达了关于数据的最多信息。
- en: At this point, we don’t have all the tools to see, but in connection to the
    principal component analysis, this says that the first principal vector is the
    one that maximizes variance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们没有所有工具来看到，但在主成分分析中，这表明第一个主向量是最大化方差的那个。
- en: Theorem [42](ch013.xhtml#x1-118012r42) is just a special case of the following
    general theorem.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 定理[42](ch013.xhtml#x1-118012r42)只是以下一般定理的一个特例。
- en: Theorem 43\.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 43\。
- en: Let A ∈ ℝ^(n×n) be a real symmetric matrix, let λ[1] ≥ ⋅⋅⋅ ≥ λ[n] be its real
    eigenvalues in decreasing order. Moreover, let U ∈ ℝ^(n×n) be the orthogonal matrix
    that diagonalizes A, with the corresponding orthonormal basis {u[1],…,u[n]}.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 设A ∈ ℝ^(n×n)是一个实对称矩阵，λ[1] ≥ ⋅⋅⋅ ≥ λ[n]是其按降序排列的实特征值。此外，设U ∈ ℝ^(n×n)是对角化A的正交矩阵，对应的正交规范基为{u[1],…,u[n]}。
- en: Then, for all k = 1,…,n, we have
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于所有k = 1,…,n，我们有
- en: '![u = argmax{xT Ax : ∥x∥ = 1,x ⊥ {u ,...,u }}, k 1 k−1 ](img/file656.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![u = argmax{xT Ax : ∥x∥ = 1,x ⊥ {u ,...,u }}, k 1 k−1 ](img/file656.png)'
- en: and
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![λ max {xT Ax : ∥x∥ = 1,x ⊥ {u ,...,u }}. k 1 k− 1 ](img/file657.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![λ max {xT Ax : ∥x∥ = 1,x ⊥ {u ,...,u }}. k 1 k− 1 ](img/file657.png)'
- en: '(Sometimes, when the conditions are too complicated, we write max{f(x) : x
    ∈A} instead of max[x∈A]f(x).)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '（有时，当条件过于复杂时，我们写max{f(x) : x ∈A}来代替max[x∈A]f(x)。）'
- en: Proof. The proof is almost identical to the previous one. Since x is required
    to be orthogonal to u[1],…,u[k−1], it can be expressed as
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。证明与之前的几乎相同。由于x需要与u[1],…,u[k−1]正交，它可以表示为
- en: '![ n ∑ x = xiui. i=k ](img/file658.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![ n ∑ x = xiui. i=k ](img/file658.png)'
- en: Following the calculations in the proof of the previous theorem, we have
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前一条定理证明中的计算，我们得出
- en: '![ ∑n xTAx = x2iλi ≤ λk. i=k ](img/file659.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n xTAx = x2iλi ≤ λk. i=k ](img/file659.png)'
- en: On the other hand, similar to before, u[k]^T Au[k] = λ[k], so the theorem follows.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，与之前类似，u[k]^T Au[k] = λ[k]，因此定理得以证明。
- en: 7.3 The singular value decomposition
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 奇异值分解
- en: So, we can diagonalize any real symmetric matrix with an orthogonal transformation.
    That’s great, but what if our matrix is not symmetric? After all, this is a rather
    special case.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以通过正交变换对任何实对称矩阵进行对角化。这很好，但如果我们的矩阵不是对称的呢？毕竟，这是一个相当特殊的情况。
- en: 'How can we do the same for a general matrix? We’ll use a very strong tool,
    straight from the mathematician’s toolkit: wishful thinking. We pretend to have
    the solution, then reverse engineer it. To be specific, let A ∈ℝ^(n×m) be any
    real matrix. (It might not be square.) Since A is not symmetric, we have to relax
    our wishes for factoring it into the form UΛU^T . The most straightforward way
    is to assume that the orthogonal matrices to the left and to the right are not
    each other’s transposes.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何对一般矩阵做同样的操作呢？我们将使用一个非常强大的工具，直接来自数学家的工具箱：异想天开。我们假装有解决方案，然后反向推导。具体来说，设A ∈
    ℝ^(n×m)是任何实矩阵。（它可能不是方阵。）由于A不是对称的，我们必须放宽对将其因式分解为UΛU^T形式的要求。最直接的方法是假设左侧和右侧的正交矩阵不是彼此的转置。
- en: Thus, we are looking for orthogonal matrices U ∈ℝ^(n×n) and V ∈ℝ^(m×m) such
    that
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们正在寻找正交矩阵 U ∈ℝ^(n×n) 和 V ∈ℝ^(m×m)，使得
- en: '![A = U ΣV T ](img/file660.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![A = U ΣV T ](img/file660.png)'
- en: holds for some diagonal Σ ∈ℝ^(n×m). (A non-square matrix Σ = (σ[i,j])[i,j=1]^(n,m)
    is diagonal if σ[i,j] is 0 when i≠j.)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些对角矩阵 Σ ∈ℝ^(n×m)，成立。（非方阵矩阵 Σ = (σ[i,j])[i,j=1]^(n,m) 是对角矩阵，当 i≠j 时，σ[i,j]
    为 0。）
- en: You might be wondering about the notational switch from Λ to Σ. This is because
    Σ will not necessarily contain eigenvalues, but singular values. We’ll explain
    soon.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会好奇为何符号从 Λ 变为 Σ。这是因为 Σ 不一定包含特征值，而是包含奇异值。我们很快会解释。
- en: Here comes the reverse-engineering part. First, as we discussed earlier, AA^T
    and A^T A are symmetric matrices. Second, we can simplify them by using the orthogonality
    of U and V , obtaining
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是逆向工程部分。首先，正如我们之前讨论的，AA^T 和 A^T A 是对称矩阵。其次，我们可以利用 U 和 V 的正交性来简化它们，得到
- en: '![ T T T AA = (UΣV )(VΣU ) 2 T = U Σ U . ](img/file661.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![ T T T AA = (UΣV )(VΣU ) 2 T = U Σ U . ](img/file661.png)'
- en: 'Similarly, we have A^T A = V Σ²V ^T . Good news: We can actually find U and
    V by applying the spectral decomposition theorem (Theorem [41](ch013.xhtml#x1-118004r41))
    to AA^T and A^T A, respectively. Thus, the factorization A = UΣV ^T is valid!
    This form is called the singular value decomposition (SVD), one of the pinnacle
    achievements of linear algebra.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们有 A^T A = V Σ²V ^T。好消息是：我们实际上可以通过将谱分解定理（定理 [41](ch013.xhtml#x1-118004r41)）应用于
    AA^T 和 A^T A 来找到 U 和 V。因此，分解 A = UΣV ^T 是有效的！这种形式被称为奇异值分解（SVD），是线性代数的巅峰成就之一。
- en: Of course, we are not done yet; we just know where to look. Let’s make this
    mathematically precise!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们还没有完成；我们只是知道该从哪里开始。让我们把这个数学上精确化！
- en: Theorem 44\. (Singular value decomposition)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 44\. （奇异值分解）
- en: Let A ∈ℝ^(n×m) be an arbitrary matrix. Then, there exists a diagonal matrix
    Σ ∈ℝ^(n×m) and orthogonal matrices U ∈ℝ^(n×n) and V ∈ℝ^(m×m), such that
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 设 A ∈ℝ^(n×m) 为任意矩阵。那么，存在对角矩阵 Σ ∈ℝ^(n×m) 和正交矩阵 U ∈ℝ^(n×n) 和 V ∈ℝ^(m×m)，使得
- en: '![A = U ΣV T. ](img/file662.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![A = U ΣV T. ](img/file662.png)'
- en: 'What is a non-square diagonal matrix? Let me give you two examples, and you’ll
    immediately get the gist:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是非方阵对角矩阵？让我给你两个例子，你立刻就能理解要点：
- en: '![⌊ ⌋ ⌊ ⌋ 1 0 1 0 0 ||0 2|| , ||0 2 0|| . ⌈ ⌉ ⌈ ⌉ 0 0 ](img/file663.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![⌊ ⌋ ⌊ ⌋ 1 0 1 0 0 ||0 2|| , ||0 2 0|| . ⌈ ⌉ ⌈ ⌉ 0 0 ](img/file663.png)'
- en: We can always write rectangular matrices M ∈ℝ^(n×m) in the forms
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是可以将矩形矩阵 M ∈ℝ^(n×m) 写成以下形式
- en: '![ ⌊ ⌋ M1 m×m (n−m)×m M = ⌈ ⌉ , M1 ∈ ℝ , M2 ∈ ℝ M2 ](img/file664.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ M1 m×m (n−m)×m M = ⌈ ⌉ , M1 ∈ ℝ , M2 ∈ ℝ M2 ](img/file664.png)'
- en: if m/span>n, and
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 m/span>n，并且
- en: '![ [ ] n×n n×(m−n) M = M1 M2 , M1 ∈ ℝ , M2 ∈ ℝ ](img/file665.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![ [ ] n×n n×(m−n) M = M1 M2 , M1 ∈ ℝ , M2 ∈ ℝ ](img/file665.png)'
- en: otherwise. Now, let’s see the proof of the singular value decomposition!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，现在，让我们来看一下奇异值分解的证明！
- en: Proof. (Sketch.) To illustrate the main ideas of the proof, we assume that 1)
    A is square, and 2) A is invertible; that is, 0 is not an eigenvalue of A,
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。（概要。）为了说明证明的主要思想，我们假设 1）A 是方阵，2）A 是可逆的；即，0 不是 A 的特征值，
- en: Since A^T A ∈ ℝ^(m×m) is a real symmetric matrix, we can apply the spectral
    decomposition theorem to obtain a diagonal Σ ∈ ℝ^(m×m) and orthogonal V ∈ℝ^(m×m)
    such that
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 A^T A ∈ ℝ^(m×m) 是实对称矩阵，我们可以应用谱分解定理，得到对角矩阵 Σ ∈ ℝ^(m×m) 和正交矩阵 V ∈ℝ^(m×m)，使得
- en: '![ T 2 T A A = V Σ V ](img/file666.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![ T 2 T A A = V Σ V ](img/file666.png)'
- en: holds. (Recall that the eigenvalues of a symmetric matrix are nonnegative, thus
    we can write the eigenvalues of A^T A in the form Σ².)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 成立。（回忆一下，对称矩阵的特征值是非负的，因此我们可以将 A^T A 的特征值写成 Σ² 的形式。）
- en: As A is invertible, A^T A is invertible as well; thus, 0 is not an eigenvalue
    of A^T A. As a consequence, ΣA^(−1) is well defined. Now, by defining U := AV
    ΣA^(−1), the orthogonality of V gives that
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 A 可逆，A^T A 也可逆；因此，0 不是 A^T A 的特征值。结果，ΣA^(−1) 是良好定义的。现在，通过定义 U := AV ΣA^(−1)，V
    的正交性表明
- en: '![ T −1 T U ΣV = (AV Σ )ΣV T = AV V = A, ](img/file667.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![ T −1 T U ΣV = (AV Σ )ΣV T = AV V = A, ](img/file667.png)'
- en: 'Thus, we are almost finished. The only thing left to show is that U is indeed
    orthogonal, that is, U^T U = I. Here we go:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们几乎完成了。剩下要证明的是 U 确实是正交的，即 U^T U = I。我们开始：
- en: '![U TU = (AV Σ− 1)TAV Σ−1 −1 T T −1 = Σ V A◟◝◜A◞ V Σ =VΣ2VT = Σ−1(V TV )Σ2(V
    TV)Σ −1 = Σ−1Σ2 Σ− 1 = I, ](img/file668.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![U TU = (AV Σ− 1)TAV Σ−1 −1 T T −1 = Σ V A◟◝◜A◞ V Σ =VΣ2VT = Σ−1(V TV )Σ2(V
    TV)Σ −1 = Σ−1Σ2 Σ− 1 = I, ](img/file668.png)'
- en: With that, have the singular value decomposition for the special case of square
    and invertible A.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们得到了平方且可逆 A 的奇异值分解特例。
- en: To keep the complexity bearable, we won’t work the rest of the details out;
    I’ll leave that to you as an exercise. You have all the tools by now.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持复杂度可控，我们不会详细推导剩下的部分；我将把它留给你作为练习。到现在为止，你已经掌握了所有工具。
- en: Let’s take a moment to appreciate the power of the singular value decomposition.
    The columns of U and V are orthogonal matrices, which are rather special transformations.
    As they leave the inner products and the norm invariant, the structure of the
    underlying vector spaces is preserved. The diagonal Σ is also special, as it is
    just a stretching in the direction of the bases. It is very surprising that any
    linear transformation is the composition of these three special ones.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍作停留，欣赏一下奇异值分解的威力。U 和 V 的列是正交矩阵，这是相当特殊的变换。因为它们保持了内积和范数不变，所以保持了底层向量空间的结构。对角矩阵
    Σ 也是特殊的，它只是沿着基方向的拉伸。令人惊讶的是，任何线性变换都是这三种特殊变换的组合。
- en: Besides mapping out the fine structure of linear transformations, SVD offers
    a lot more. For instance, it generalizes the notion of eigenvectors, a concept
    that was defined only for square matrices. With this, we have
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 除了揭示线性变换的精细结构外，SVD 还提供了更多功能。例如，它推广了特征向量的概念，而特征向量仅定义于方阵。通过这个，我们有了
- en: '![AV = U Σ, ](img/file669.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![AV = U Σ, ](img/file669.png)'
- en: which we can take a look at column-wise. Here, Σ is diagonal, but its number
    of elements depends on the smaller one of n or m.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按列查看这个矩阵。这里，Σ 是对角矩阵，但其元素的数量取决于 n 或 m 中较小的那个。
- en: So, if u[i] is the i-th column of U, and v[i] is the i-th column of V , the
    identity AV = UΣ is translated to
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果 u[i] 是 U 的第 i 列，v[i] 是 V 的第 i 列，那么身份式 AV = UΣ 就可以转化为
- en: '![Av = σ u , 0 ≤ i ≤ min (n,m ). i ii ](img/file670.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![Av = σ u , 0 ≤ i ≤ min (n,m ). i ii ](img/file670.png)'
- en: This closely resembles the definition of eigenvalue-eigenvector pairs, except
    that instead of one vector, we have two. The u[i] and v[i] are the so-called left
    and right singular vectors, while the scalars σ[i] = ![√λ- i](img/file671.png)
    are called singular values.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这与特征值-特征向量对的定义非常相似，不同的是这里我们有两个向量，而不是一个。u[i] 和 v[i] 被称为所谓的左奇异向量和右奇异向量，而标量 σ[i]
    = ![√λ- i](img/file671.png) 被称为奇异值。
- en: To sum up, orthogonal transformations give us the singular value decomposition,
    but is that all? Are there any other special transformations and matrix decompositions?
    You bet there is.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，正交变换给出了奇异值分解，但这就是全部吗？还有其他特殊的变换和矩阵分解吗？当然有。
- en: Enter orthogonal projections.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 引入正交投影。
- en: 7.4 Orthogonal projections
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 正交投影
- en: Linear transformations are essentially manipulations of data, revealing other
    (hopefully more useful) representations. Intuitively, we think about them as one-to-one
    mappings, faithfully preserving all the “information” from the input.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 线性变换本质上是数据的操作，揭示了其他（希望是更有用的）表示。直观地，我们将它们看作一一映射，忠实地保留输入的所有“信息”。
- en: This is often not the case, to such an extent that sometimes a lossy compression
    of the data is highly beneficial. To give you a concrete example, consider a dataset
    with a million features, out of which only a couple hundred are useful. What we
    can do is identify the important features and throw away the rest, obtaining a
    representation that is more compact, thus easier to work with.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常不是这样，以至于有时候数据的有损压缩是非常有益的。举个具体的例子，考虑一个包含一百万个特征的数据集，其中只有几百个是有用的。我们可以做的是识别出重要的特征并丢弃其余的，得到一个更紧凑的表示，便于操作。
- en: This notion is formalized by the concept of orthogonal projections. We already
    met them upon our first encounter with the inner products (see ([2.7](ch008.xhtml#x1-45003r3.2.3))).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念通过正交投影的定义被形式化。我们在首次接触内积时就遇到了它们（见([2.7](ch008.xhtml#x1-45003r3.2.3))）。
- en: Projections also play a fundamental role in the Gram-Schmidt process (Theorem [13](ch008.xhtml#x1-47004r13)),
    used to orthogonalize an arbitrary basis. Because we are already somewhat familiar
    with orthogonal projections, a formal definition is due.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 投影在格拉姆-施密特过程（定理 [13](ch008.xhtml#x1-47004r13)）中也起着基础作用，用于将任意基向量正交化。由于我们已经对正交投影有一定的了解，因此需要给出正式的定义。
- en: Definition 28\. (Projections and orthogonal projections)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 28\. （投影与正交投影）
- en: 'Let V be an arbitrary inner product space and P : V → V be a linear transformation.
    P is a projection if P² = P.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '设 V 为一个任意的内积空间，P : V → V 是一个线性变换。如果 P² = P，那么 P 就是一个投影。'
- en: A projection P is orthogonal if the subspaces kerP and imP are orthogonal to
    each other. (That is, for every pair of x ∈ kerP and y ∈ imP, we have ⟨x,y⟩ =
    0.)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果投影 P 是正交的，那么子空间 kerP 和 imP 是彼此正交的。（也就是说，对于 kerP 中的每个 x 和 imP 中的每个 y，我们有 ⟨x,y⟩
    = 0。）
- en: Let’s revisit the examples we have seen so far to get a grip on the definition!
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视迄今为止见过的例子，以便理解其定义！
- en: Example 1\. The simplest one is the orthogonal projection to a single vector.
    That is, if u ∈ℝ^n is an arbitrary vector, the transformation
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 例 1\. 最简单的是到单个向量的正交投影。也就是说，如果 u ∈ℝ^n 是任意向量，则变换
- en: '![proju(x) = ⟨x,u⟩-u ⟨u,u ⟩ ](img/file672.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![proju(x) = ⟨x,u⟩-u ⟨u,u ⟩ ](img/file672.png)'
- en: is the orthogonal projection to (the subspace spanned by) u. (We talked about
    this when discussing the geometric interpretation of inner products in Section [2.2.3](ch008.xhtml#the-geometric-interpretation-of-inner-products),
    where this definition was deduced from a geometric intuition.) Applying this transformation
    repeatedly, we get
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 是 (由) u 的正交投影。（在讨论内积的几何解释时，我们谈到过这一点，见第 [2.2.3](ch008.xhtml#the-geometric-interpretation-of-inner-products)
    节。）反复应用这一变换，我们得到
- en: '![ ⟨x,u⟩ ⟨⟨u,u⟩u,-u⟩ proju(proju (x )) = ⟨u,u ⟩ u ⟨x,u⟩ -⟨u,u⟩⟨u,-u⟩ = ⟨u,u
    ⟩ u = ⟨x,u-⟩u ⟨u,u ⟩ = proj (x). u ](img/file673.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![ ⟨x,u⟩ ⟨⟨u,u⟩u,-u⟩ proju(proju (x )) = ⟨u,u ⟩ u ⟨x,u⟩ -⟨u,u⟩⟨u,-u⟩ = ⟨u,u
    ⟩ u = ⟨x,u-⟩u ⟨u,u ⟩ = proj (x). u ](img/file673.png)'
- en: Thus, faithfully to its name, ![proju ](img/file674.png) is indeed a projection.
    To see that it is orthogonal, let’s examine its kernel and image! Since the value
    of ![proju(x ) ](img/file675.png) is a scalar multiple of ![u ](img/file676.png),
    its image is
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，忠实于其名字，![proju ](img/file674.png) 确实是一个投影。为了看到它是正交的，让我们检查其核和图像！由于 ![proju(x
    ) ](img/file675.png) 的值是 ![u ](img/file676.png) 的标量倍，其图像是
- en: '![im (proj) = span(u). u ](img/file677.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![im (proj) = span(u). u ](img/file677.png)'
- en: Its kernel, the set of vectors mapped to 0 by proj[u], is also easy to find,
    as ![⟨⟨xu,,uu⟩⟩](img/file678.png)u = 0 can only happen if ⟨x,u⟩ = 0, that is,
    if x ⊥u.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其核心，被 proj[u] 映射到 0 的向量集合，也很容易找到，因为 ![⟨⟨xu,,uu⟩⟩](img/file678.png)u = 0 只有当
    ⟨x,u⟩ = 0 时才会发生，即 x ⊥u。
- en: In other words,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，
- en: '![ker(proju) = span (u)⊥, ](img/file679.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![ker(proju) = span (u)⊥, ](img/file679.png)'
- en: where span(u)^⊥ denotes the orthogonal complement (Definition [13](ch008.xhtml#x1-48004r13))
    of span(u). This means that proj[u] is indeed an orthogonal projection.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 span(u)^⊥ 表示 span(u) 的正交补（定义 [13](ch008.xhtml#x1-48004r13)）。这意味着 proj[u]
    确实是一个正交投影。
- en: We can also describe ![proju(x) ](img/file680.png) in terms of matrices. By
    writing out ![proju (x ) ](img/file681.png) component-wise, we have
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以用矩阵描述 ![proju(x) ](img/file680.png)。通过按分量写出 ![proju (x ) ](img/file681.png)，我们有
- en: '![ ⌊ ⌋ | ⟨x,u⟩u1| ⟨x,u⟩ 1 | ⟨x,u⟩u2| proju(x) = -----u = ----2|| . || , ⟨u,
    u⟩ ∥u ∥ |⌈ .. |⌉ ⟨x,u⟩u n ](img/file682.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ | ⟨x,u⟩u1| ⟨x,u⟩ 1 | ⟨x,u⟩u2| proju(x) = -----u = ----2|| . || , ⟨u,
    u⟩ ∥u ∥ |⌈ .. |⌉ ⟨x,u⟩u n ](img/file682.png)'
- en: where u = (u[1],…,u[n]). This looks like some kind of matrix multiplication!
    As we saw earlier, multiplying a matrix and a vector can be described in terms
    of rowwise dot products. (See ([3.3](#)).)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 u = (u[1],…,u[n])。这看起来像某种矩阵乘法！正如我们之前看到的，将矩阵和向量相乘可以用行向量点积来描述。（见 ([3.3](#))。）
- en: So, according to this interpretation of matrix multiplication, we have
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据这种矩阵乘法的解释，我们有
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(13).png)(7.5)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(13).png)(7.5)'
- en: Note that the scaling with ∥u∥² can be incorporated into the “matrix” product
    by writing
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与 ∥u∥² 的比例可以通过将其写成 “矩阵” 乘积中的一部分来整合。
- en: '![ T T uu---= -u--⋅ u--, ∥u∥2 ∥u ∥ ∥u∥ ](img/file686.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![ T T uu---= -u--⋅ u--, ∥u∥2 ∥u ∥ ∥u∥ ](img/file686.png)'
- en: The matrix uu^T ∈ℝ^(n×n), obtained from the product of the vector u ∈ℝ^(n(×1))
    and its transpose u^T ∈ℝ^(1×n), is a rather special one. They are called rank-1
    projection matrices, and they frequently appear in mathematics.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 uu^T ∈ℝ^(n×n)，由向量 u ∈ℝ^(n(×1)) 和其转置 u^T ∈ℝ^(1×n) 的乘积得到，是一个非常特殊的矩阵。它们被称为秩-1
    投影矩阵，并且在数学中经常出现。
- en: (In general, the matrix uv^T is called the outer product of the vectors u and
    v. We won’t use this extensively, but it appears frequently throughout linear
    algebra.)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: （一般来说，矩阵 uv^T 被称为向量 u 和 v 的外积。我们不会广泛使用它，但它在线性代数中经常出现。）
- en: Example 2\. As we saw when introducing the Gram-Schmidt orthogonalization process
    (Theorem [13](ch008.xhtml#x1-47004r13)), the previous example can be generalized
    by projecting to multiple vectors. If u[1],…,u[k] ∈ℝ^n is a set of linearly independent
    and pairwise orthogonal vectors, then the linear transformation
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2\。如我们在引入格拉姆-施密特正交化过程时所看到的（定理 [13](ch008.xhtml#x1-47004r13)），前面的例子可以通过对多个向量进行投影来推广。如果
    \( u[1], \dots, u[k] \in \mathbb{R}^n \) 是一组线性无关且两两正交的向量，则线性变换
- en: '![ ∑k ⟨x, ui⟩ proju1,...,uk(x) = -------ui i=1 ⟨ui,ui⟩ ](img/file687.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑k ⟨x, ui⟩ proju1,...,uk(x) = -------ui i=1 ⟨ui,ui⟩ ](img/file687.png)'
- en: is an orthogonal projection onto the subspace span(u[1],…,u[k]). This is easy
    to see, and I recommend the reader to do this as an exercise. (This can be found
    in the problems section as well.)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 是对子空间 \( \text{span}(u[1], \dots, u[k]) \) 的正交投影。这很容易看出来，我建议读者将此作为练习来完成。（这也可以在问题部分找到。）
- en: 'From ([7.5](#)), we can determine the matrix form of proj[u[1],…,u[k]] as well:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从 ([7.5](#))，我们还可以确定 \( \text{proj}[u[1], \dots, u[k]] \) 的矩阵形式：
- en: '![ ∑k T proj (x) = ( uiu-i) x. u1,...,uk i=1 ∥ui∥2 ◟----◝◜---◞ ∈ℝn×n ](img/file688.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑k T proj (x) = ( uiu-i) x. u1,...,uk i=1 ∥ui∥2 ◟----◝◜---◞ ∈ℝn×n ](img/file688.png)'
- en: This is good to know, as projection matrices are often needed in the implementation
    of certain algorithms.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的知识点，因为投影矩阵通常在某些算法的实现中需要用到。
- en: 7.4.1 Properties of orthogonal projections
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 正交投影的性质
- en: 'Now that we have seen a few examples, it is time to discuss orthogonal projections
    in more general terms. There are lots of reasons why these special transformations
    are useful, and we’ll explore them in this section. First, let’s start with the
    most important one: orthogonal projections also enable the decomposition of vectors
    in terms of a given subspace plus an orthogonal vector.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了一些例子，是时候以更一般的方式讨论正交投影了。正交投影有很多有用的理由，我们将在本节中探讨它们。首先，让我们从最重要的一点开始：正交投影也使得向量能够分解为给定子空间加上一个正交向量。
- en: Theorem 45\.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 45\。
- en: 'Let V be an inner product space and P : V → V be a projection. Then, V = kerP
    + imP; that is, every vector x ∈V can be written as'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '设 \( V \) 为一个内积空间，\( P : V \rightarrow V \) 为一个投影算子。那么，\( V = \ker P + \text{im}
    P \); 也就是说，任意向量 \( x \in V \) 都可以表示为'
- en: '![x = x + x , x ∈ ker P, x ∈ im P. ker im ker im ](img/file689.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![x = x + x , x ∈ ker P, x ∈ im P. ker im ker im ](img/file689.png)'
- en: If P is an orthogonal projection, then x[im] ⊥x[ker].
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 \( P \) 是一个正交投影，则 \( x[\text{im}] \perp x[\ker] \)。
- en: Proof. Every x can be written as
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：每个 \( x \) 可以表示为
- en: '![x = (x − Px )+ Px. ](img/file690.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![x = (x − P x) + P x. ](img/file690.png)'
- en: Since P is idempotent, that is, P² = P, we have
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \( P \) 是幂等的，即 \( P^2 = P \)，我们有
- en: '![P (x − P x) = Px − P (Px) = Px − P x = 0, ](img/file691.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![P (x − P x) = P x − P (P x) = P x − P x = 0, ](img/file691.png)'
- en: that is, x −Px ∈ kerP. By definition, Px ∈ imP, so V = kerV + imV , which proves
    our main proposition.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，\( x - P x \in \ker P \)。根据定义，\( P x \in \text{im} P \)，因此 \( V = \ker
    V + \text{im} V \)，这证明了我们的主要命题。
- en: If P is an orthogonal projection, then again, by definition, x[im] ⊥x[ker],
    which is what we had to show.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 \( P \) 是一个正交投影，那么根据定义，\( x[\text{im}] \perp x[\ker] \)，这正是我们需要证明的。
- en: In addition, orthogonal projections are self-adjoint. This might not sound like
    a big deal, but self-adjointness leads to several very pleasant properties.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正交投影是自伴的。听起来似乎不算什么大事，但自伴性导致了一些非常愉快的性质。
- en: Theorem 46\.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 46\。
- en: 'Let V be an inner product space and P : V →V be an orthogonal projection. Then,
    P is self-adjoint.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '设 \( V \) 为一个内积空间，\( P : V \rightarrow V \) 为一个正交投影。则，\( P \) 是自伴的。'
- en: Proof. According to the definition ([27](ch013.xhtml#x1-118003r27)), all we
    need to show is that
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：根据定义（[27](ch013.xhtml#x1-118003r27)），我们只需要证明
- en: '![⟨Px, y⟩ = ⟨x, Py⟩ ](img/file692.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![⟨P x, y⟩ = ⟨x, P y⟩ ](img/file692.png)'
- en: holds for any x,y ∈V . In the previous result, we have seen that x and y can
    be written as
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意 \( x, y \in V \)，成立。在之前的结果中，我们已经看到 \( x \) 和 \( y \) 可以表示为
- en: '**x** = **x**[ker P] + **x**[im P], **x**[ker P] ∈ ker P, **x**[im P] ∈ im P**y**
    = **y**[ker P] + **y**[im P], **y**[ker P] ∈ ker P, **y**[im P] ∈ im P'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**x** = **x**[\(\ker P\)] + **x**[\(\text{im} P\)], **x**[\(\ker P\)] ∈ \(\ker
    P\), **x**[\(\text{im} P\)] ∈ \(\text{im} P\) **y** = **y**[\(\ker P\)] + **y**[\(\text{im}
    P\)], **y**[\(\ker P\)] ∈ \(\ker P\), **y**[\(\text{im} P\)] ∈ \(\text{im} P\)'
- en: Since P² = P, we have
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \( P^2 = P \)，我们有
- en: '![⟨P x,y⟩ = ⟨PxkerP + Pxim P,ykerP + yim P⟩ = ⟨xim P,ykerP + yim P⟩ = ⟨x ,y
    ⟩+ ⟨x ,y ⟩ ◟-im-P◝◜kerP◞ im P im P =0 = ⟨xim P,yimP ⟩. ](img/file693.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![⟨P x, y⟩ = ⟨P x_{\ker P} + P x_{\text{im} P}, y_{\ker P} + y_{\text{im} P}⟩
    = ⟨x_{\text{im} P}, y_{\ker P} + y_{\text{im} P}⟩ = ⟨x, y⟩ + ⟨x, y⟩ ◟-im-P◝◜kerP◞
    im P im P =0 = ⟨x_{\text{im} P}, y_{\text{im} P}⟩. ](img/file693.png)'
- en: Similarly, it can be shown that ⟨x,Py⟩ = ⟨x[im P] ,y[im P] ⟩. These two identities
    imply ⟨Px,y⟩ = ⟨x,Py⟩, which is what we had to show.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，可以证明 \( \langle x, P y \rangle = \langle x[\text{im} P], y[\text{im} P]
    \rangle \)。这两个恒等式意味着 \( \langle P x, y \rangle = \langle x, P y \rangle \)，这正是我们需要证明的。
- en: One straightforward consequence of self-adjointness is that the kernel of orthogonal
    projections is the orthogonal complement of its image.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 自伴算子的一个直接推论是，正交投影的核是其像的正交补。
- en: 'Theorem 47\. Let V be an inner product space and P : V → V be an orthogonal
    projection. Then,'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '定理 47\. 设 \( V \) 是一个内积空间，\( P : V \to V \) 是一个正交投影。那么，'
- en: '![ ⊥ ker P = (im P ) . ](img/file694.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![ \perp \text{ker} P = (\text{im} P).](img/file694.png)'
- en: Proof. To prove the equality of these two sets, we need to show that (a) kerP
    ⊆ (imP)^⊥, and (b) (imP)^⊥⊆ kerP.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。为了证明这两个集合的相等，我们需要证明 (a) \( \text{ker} P \subseteq (\text{im} P)^\perp \)，以及
    (b) \( (\text{im} P)^\perp \subseteq \text{ker} P \)。
- en: (a) Let x ∈ kerP; that is, suppose that Px = 0\. We need to show that for any
    y ∈ imP, we have ⟨x,y⟩ = 0\. For this, let y[0] ∈V such that Py[0] = y. (This
    is guaranteed to exist, since we took y from the image of P.) Then,
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 设 \( x \in \text{ker} P \)，即假设 \( Px = 0 \)。我们需要证明，对于任何 \( y \in \text{im}
    P \)，有 \( \langle x, y \rangle = 0 \)。为此，设 \( y[0] \in V \)，使得 \( P y[0] = y \)。（这是可以保证存在的，因为我们从
    \( P \) 的像中取了 \( y \)。）那么，
- en: '![⟨x,y ⟩ = ⟨x,P y0⟩ = ⟨P x,y0⟩ = ⟨0,y0⟩ = 0, ](img/file695.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![\langle x, y \rangle = \langle x, P y_0 \rangle = \langle P x, y_0 \rangle
    = \langle 0, y_0 \rangle = 0,](img/file695.png)'
- en: where P is self-adjoint. Thus, x ∈ (imP)^⊥ also holds, implying kerP ⊆ (imP)^⊥.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \( P \) 是自伴的。因此，\( x \in (\text{im} P)^\perp \) 也成立，意味着 \( \text{ker} P \subseteq
    (\text{im} P)^\perp \)。
- en: (b) Now, let x ∈ (imP)^⊥. Then, for any y ∈V , we have ⟨x,Py⟩ = 0\. However,
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 现在，设 \( x \in (\text{im} P)^\perp \)。那么，对于任何 \( y \in V \)，我们有 \( \langle
    x, Py \rangle = 0 \)。然而，
- en: '![⟨Px,y ⟩ = ⟨x,P y⟩ = 0\. ](img/file696.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![\langle Px, y \rangle = \langle x, P y \rangle = 0.](img/file696.png)'
- en: Especially, with the choice y = Px, we have ⟨Px,Px⟩ = 0\. Due to the positive
    definiteness of the inner product, this implies that Px = 0, that is, x ∈ kerP.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，当选择 \( y = Px \) 时，我们有 \( \langle Px, Px \rangle = 0 \)。由于内积的正定性，这意味着 \(
    Px = 0 \)，即 \( x \in \text{ker} P \)。
- en: Summing up all of the above, if P is an orthogonal projection of the inner product
    space V , then
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 总结以上内容，如果 \( P \) 是内积空间 \( V \) 的正交投影，那么
- en: '![V = im P + (im P )⊥. ](img/file697.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![V = \text{im} P + (\text{im} P)^\perp.](img/file697.png)'
- en: Do you recall that when we first encountered the concept of orthogonal complements
    (Definition [13](ch008.xhtml#x1-48004r13)), we proved that V = S + S^⊥ for any
    finite-dimensional inner product space V and its subspace S? We did this with
    the use of a special orthogonal projection. We are getting close to seeing the
    general pattern here.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得当我们首次遇到正交补的概念时（定义 [13](ch008.xhtml#x1-48004r13)），我们证明了对于任何有限维内积空间 \( V \)
    及其子空间 \( S \)，有 \( V = S + S^\perp \) 吗？我们是通过使用一个特殊的正交投影来实现的。我们离看到一般模式已经不远了。
- en: Because the kernel of an orthogonal projection P is an orthogonal complement
    of the image, the transformation I −P is an orthogonal projection as well, with
    the roles of image and kernel reversed.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 因为正交投影 \( P \) 的核是其像的正交补，所以变换 \( I - P \) 也是一个正交投影，且像与核的角色发生了反转。
- en: Theorem 48\.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 48\。
- en: 'Let V be an inner product space and P : V →V be an orthogonal projection. Then,
    I −P is an orthogonal projection as well, and'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '设 \( V \) 是一个内积空间，\( P : V \to V \) 是一个正交投影。那么，\( I - P \) 也是一个正交投影，并且'
- en: '![ker(I − P ) = im P, im (I − P ) = ker P. ](img/file698.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![\text{ker}(I - P) = \text{im} P, \text{im}(I - P) = \text{ker} P.](img/file698.png)'
- en: The proof is so simple that this is left as an exercise for the reader.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 证明非常简单，因此留给读者作为练习。
- en: One more thing to mention. If the image spaces of two orthogonal projections
    match, then the projections themselves are equal. This is a very strong uniqueness
    property, as if you think about it, this is not true for other classes of linear
    transformations.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一点需要提到。如果两个正交投影的像空间相同，那么这些投影本身是相等的。这是一个非常强的唯一性性质，因为如果你仔细想一想，对于其他类线性变换，这种情况并不成立。
- en: Theorem 49\. (Uniqueness of orthogonal projections)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 49\. （正交投影的唯一性）
- en: 'Let V be an inner product space and P,Q : V → V be two orthogonal projections.
    If imP = imQ, then P = Q.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '设 \( V \) 是一个内积空间，\( P, Q : V \to V \) 是两个正交投影。如果 \( \text{im} P = \text{im}
    Q \)，那么 \( P = Q \)。'
- en: Proof. Because of kerP = (imP)^⊥, the equality of the image spaces also imply
    that kerP = kerQ.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。由于 \( \text{ker} P = (\text{im} P)^\perp \)，像空间的相等也意味着 \( \text{ker} P =
    \text{ker} Q \)。
- en: Since V = kerP + imP, every x ∈V can be decomposed as
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \( V = \text{ker} P + \text{im} P \)，每个 \( x \in V \) 都可以分解为
- en: '![x = xkerP + xim P, xkerP ∈ kerP, xim P ∈ im P. ](img/file699.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![x = x_{\text{ker} P} + x_{\text{im} P}, x_{\text{ker} P} \in \text{ker} P,
    x_{\text{im} P} \in \text{im} P.](img/file699.png)'
- en: This decomposition and the equality of the kernel and image spaces give that
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分解和核与像空间的相等给出了
- en: '![Px = P xkerP + P ximP = xim P. ](img/file700.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![Px = P x_{\text{ker} P} + P x_{\text{im} P} = x_{\text{im} P}.](img/file700.png)'
- en: With an identical argument, we have Qx = x[im P] , thus Px = Qx on all vectors
    x ∈V . This proves P = Q.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过类似的推理，我们有 \( Qx = x[\text{im} P] \)，因此对于所有 \( x \in V \)，有 \( Px = Qx \)。这证明了
    \( P = Q \)。
- en: In other words, given a subspace, there can be only one orthogonal projection
    to it. But is there any at all? Yes, and in the next section, we will see that
    it can be described in geometric terms.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，给定一个子空间，只有一个正交投影可以映射到它。那么，这样的投影存在吗？当然存在，在下一节中，我们将看到它可以用几何术语来描述。
- en: 7.4.2 Orthogonal projections are the optimal projections
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 正交投影是最佳投影
- en: 'Orthogonal projections have an extremely pleasant and mathematically useful
    property. In some sense, if P : V →V is an orthogonal projection, Px provides
    the optimal approximation of x among all vectors in imP. To make this precise,
    we can state the following.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '正交投影具有极其愉悦且在数学上非常有用的性质。从某种意义上讲，如果P : V →V 是一个正交投影，则Px在所有imP中的向量中提供了x的最佳近似。为了精确表达这一点，我们可以陈述如下。'
- en: Theorem 50\. (Construction of orthogonal transformations)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 定理50．（正交变换的构造）
- en: 'Let V be a finite-dimensional inner product space and S ⊆V its subspace. Then,
    the transformation P : V →V , defined by'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '设V是一个有限维内积空间，S ⊆V是它的子空间。那么，定义变换P : V →V，如下所示'
- en: '![P : x → argmyi∈nS ∥x − y∥ ](img/file701.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![P : x → argmyi∈nS ∥x − y∥ ](img/file701.png)'
- en: is an orthogonal projection to S.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 是S的正交投影。
- en: In other words, since orthogonal projections to a given subspace are unique
    (as implied by Theorem [49](ch013.xhtml#x1-121008r49)), Px is the closest vector
    to x in the subspace S. Thus, we can denote this as P[S], emphasizing the uniqueness.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，由于正交投影到给定子空间是唯一的（如定理[49](ch013.xhtml#x1-121008r49)所示），因此Px是子空间S中最接近x的向量。因此，我们可以将其表示为P[S]，以强调其唯一性。
- en: Besides having an explicit way to describe orthogonal projections, there is
    one extra benefit. Recall that previously, we showed that
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 除了有明确的方法来描述正交投影之外，还有一个额外的好处。回想之前，我们展示了
- en: '![V = im P + (im P)⊥ ](img/file702.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![V = im P + (im P)⊥ ](img/file702.png)'
- en: holds. Since for any subspace S an orthogonal projection P[S] exists whose image
    set is S, it also follows that V = S + S^⊥. Although we saw this earlier when
    talking about orthogonal complements (Definition [13](ch008.xhtml#x1-48004r13)),
    it is interesting to see a proof that doesn’t require the construction of an orthonormal
    basis in S.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 恒成立。由于对于任何子空间S，都存在一个正交投影P[S]，其像集为S，因此也可以得出V = S + S^⊥。虽然我们在讨论正交补时已经看到过这一点（定义[13](ch008.xhtml#x1-48004r13)），但通过不需要构造S中的正交标准基的证明来看到这一点仍然很有趣。
- en: Interestingly, this is the point where mathematical analysis and linear algebra
    intersect. We don’t have the tools for it yet, but using the concept of convergence,
    the above theorems can be generalized to infinite-dimensional spaces. Infinite-dimensional
    spaces are not particularly relevant to machine learning in practice, yet they
    provide a beautiful mathematical framework for the study of functions. Who knows,
    one day these advanced tools may provide a significant breakthrough in machine
    learning.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这正是数学分析和线性代数交汇的地方。虽然我们目前还没有相关工具，但通过使用收敛的概念，以上定理可以推广到无限维空间。尽管在实际的机器学习中无限维空间并不特别相关，但它为函数研究提供了一个美丽的数学框架。谁知道呢，也许有一天这些先进工具会为机器学习带来重大突破。
- en: 7.5 Computing eigenvalues
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 计算特征值
- en: In the last chapter, we reached the singular value decomposition, one of the
    pinnacle results of linear algebra. We laid out the theoretical groundwork to
    get us to this point.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们达到了奇异值分解，这是线性代数的一个巅峰结果。我们为此打下了理论基础。
- en: 'However, one thing is missing: computing the singular value decomposition in
    practice. Without this, we can’t reap all the rewards this powerful tool offers.
    In this section, we’ll develop two methods for this purpose. One offers a deep
    insight into the behavior of eigenvectors, but it doesn’t work in practice. The
    other offers excellent performance, but it is hard to understand what is happening
    behind the formulas. Let’s start with the first one, illuminating how the eigenvectors
    determine the effects of a linear transformation!'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一件事还没解决：在实践中计算奇异值分解。没有这个工具，我们无法充分发挥这一强大工具的所有优势。在这一节中，我们将开发两种方法来实现这一目的。一种方法为特征向量的行为提供了深刻的洞察，但在实践中并不适用。另一种方法则提供了出色的性能，但难以理解公式背后的具体含义。我们先从第一种方法开始，阐明特征向量是如何决定线性变换的效果的！
- en: 7.5.1 Power iteration for calculating the eigenvectors of real symmetric matrices
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 用幂迭代法计算实对称矩阵的特征向量
- en: 'If you recall, we discovered the singular value decomposition by tracing the
    problem back to the spectral decomposition of symmetric matrices. In turn, we
    can obtain the spectral decomposition by finding an orthonormal basis from the
    eigenvectors of our matrix. The plan is the following: first, we define a procedure
    that finds an orthonormal set of eigenvectors for symmetric matrices. Then, use
    this to compute the singular value decomposition for arbitrary matrices.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得，我们通过追溯问题到对称矩阵的谱分解，发现了奇异值分解。反过来，我们可以通过从矩阵的特征向量中找到正交归一基来获得谱分解。计划如下：首先，我们定义一个过程，用来找到对称矩阵的正交归一特征向量集。然后，利用这个过程计算任意矩阵的奇异值分解。
- en: A naive way would be to find the eigenvalues by solving the polynomial equation
    det(A−λI) = 0 for λ, then compute the corresponding eigenvectors by solving the
    linear equations (A −λI)x = 0.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是通过解多项式方程 det(A−λI) = 0 来找到特征值，然后通过解线性方程 (A −λI)x = 0 来计算相应的特征向量。
- en: However, there are problems with this approach. For an n ×n matrix, the characteristic
    polynomial p(λ) = det(A−λI) is a polynomial of degree n. Even if we could effectively
    evaluate det(A −λI) for any lambda, there are serious issues. Unfortunately, unlike
    for the quadratic equation ax² + bx + c = 0, there are no formulas for finding
    the solutions when n/span>4\. (It is not that mathematicians were just not clever
    enough to find them. No such formula exists.)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法存在问题。对于一个 n × n 矩阵，特征多项式 p(λ) = det(A−λI) 是一个 n 次多项式。即使我们能够有效地评估 det(A
    −λI) 对任意 λ 的值，也存在严重的问题。不幸的是，与二次方程 ax² + bx + c = 0 不同，当 n > 4 时，并没有找到解的公式。(并不是数学家们不够聪明，而是根本没有这样的公式。)
- en: How can we find an alternative approach? Once again, we use the wishful thinking
    approach that worked so well before. Let’s pretend that we know the eigenvalues,
    play around with them, and see if this gives us some useful insight.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何找到一种替代方法呢？我们再次使用之前非常有效的空想法。让我们假设我们知道特征值，玩弄它们，看看这是否能给我们一些有用的启示。
- en: For the sake of simplicity, assume that A is a small symmetric 2 × 2 matrix,
    say with eigenvalues λ[1] = 4 and λ[2] = 2\. Since A is symmetric, we can even
    find a set of corresponding eigenvectors u[1],u[2] such that u[1] and u[2] form
    an orthonormal basis. (That is, both have a unit norm and they are orthogonal
    to each other.) This is guaranteed by the spectral decomposition theorem (Theorem [41](ch013.xhtml#x1-118004r41)).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，假设 A 是一个小的对称 2 × 2 矩阵，特征值为 λ[1] = 4 和 λ[2] = 2。由于 A 是对称的，我们甚至可以找到一组对应的特征向量
    u[1]、u[2]，使得 u[1] 和 u[2] 形成一个正交归一基。 (也就是说，它们的范数为 1，且相互正交。) 这一点由谱分解定理（定理 [41](ch013.xhtml#x1-118004r41)）保证。
- en: Thus, any x ∈ℝ² can be written as x = x[1]u[1] + x[2]u[2] for some nonzero scalars
    x[1],x[2]. What happens if we apply the transformation A to our vector x? Because
    u[i] is eigenvectors, we have
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，任何 x ∈ℝ² 都可以写成 x = x[1]u[1] + x[2]u[2]，其中 x[1]、x[2] 是某些非零标量。如果我们将变换 A 应用到向量
    x 上，会发生什么呢？因为 u[i] 是特征向量，所以我们有
- en: '![Ax = A (x1u1 + x2u2 ) = x1Au1 + x2Au2 = x1λ1u1 + x2λ2u2 = 4x1u1 + 2x2u2\.
    ](img/file703.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![Ax = A (x1u1 + x2u2 ) = x1Au1 + x2Au2 = x1λ1u1 + x2λ2u2 = 4x1u1 + 2x2u2\.
    ](img/file703.png)'
- en: By applying A one more time, we obtain
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 再次应用 A，我们得到
- en: '![ 2 2 2 A x = x1λ1u1 + x2λ2u2 2 2 = 4x1u1 + 2 x2u2\. ](img/file704.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 2 2 A x = x1λ1u1 + x2λ2u2 2 2 = 4x1u1 + 2 x2u2\. ](img/file704.png)'
- en: A pattern starts to emerge. In general, the k-th iteration of A yields
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一种模式开始显现。一般来说，A 的第 k 次迭代结果为
- en: '![Akx = x1 λk1u1 + x2λk2u2 = 4kx1u1 + 2kx2u2\. ](img/file705.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![Akx = x1 λk1u1 + x2λk2u2 = 4kx1u1 + 2kx2u2\. ](img/file705.png)'
- en: By taking an inquisitive look at A^kx, we can note that the contribution of
    u[1] is much more significant than u[2]. Why? Because the coefficient x[1]λ[1]^k
    = 4^kx[1] grows faster than x[2]λ[2]^k = 2^kx[2], regardless of the value of x[1]
    and x[2]. In technical terms, we say that λ[1] dominates λ[2].
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对 A^kx 进行深入观察，我们可以注意到 u[1] 的贡献比 u[2] 更加显著。为什么？因为系数 x[1]λ[1]^k = 4^kx[1] 比
    x[2]λ[2]^k = 2^kx[2] 增长得更快，无论 x[1] 和 x[2] 的值如何。从技术上讲，我们可以说 λ[1] 支配了 λ[2]。
- en: Now, by scaling things down with λ[1]^k, we can extract the eigenvector u[1]!
    That is,
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过将数值缩放为 λ[1]^k，我们可以提取特征向量 u[1]! 也就是说，
- en: '![Akx- λ2- k λk = x1u1 + x2(λ1 ) u2 1 = x1u1 + (something very small)k. ](img/file706.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![Akx- λ2- k λk = x1u1 + x2(λ1 ) u2 1 = x1u1 + (something very small)k. ](img/file706.png)'
- en: If we let k grow infinitely, the contribution of u[2] to ![Akkx- λ1](img/file707.png)
    vanishes. If you are familiar with the concept of limits, you could write
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们让 k 无限增大，u[2] 对 ![Akkx- λ1](img/file707.png) 的贡献将消失。如果你熟悉极限的概念，你可以写出
- en: lim[k → ∞] ![(A_k x - λ_k 1)](img/file708.png) = *x*[1]**u**[1].
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: lim[k → ∞] ![(A_k x - λ_k 1)](img/file708.png) = *x*[1]**u**[1]。
- en: (7.6)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: (7.6)
- en: Remark 7\. (A primer on limits)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 注释7.（极限基础）
- en: If you are not familiar with limits, here is a quick explanation. The identity
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉极限，这里有一个简短的解释。恒等式
- en: '![ Akx lim --k- = x1u1 k→ ∞ λ1 ](img/file709.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![ Akx lim --k- = x1u1 k→ ∞ λ1 ](img/file709.png)'
- en: means that as k grows, the quantity ![Akx- λk1](img/file710.png) gets closer
    and closer to x[1]u[1], until the difference between them is infinitesimal. In
    practice, this means that we can approximate x[1]u[1] by ![Akx- λk1](img/file711.png)
    by selecting a very large k.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着随着k的增加，量 ![Akx- λk1](img/file710.png) 会越来越接近x[1]u[1]，直到它们之间的差异变得微不足道。实际上，这意味着我们可以通过选择一个非常大的k来近似x[1]u[1]，即
    ![Akx- λk1](img/file711.png)。
- en: 'Equation ([7.6](ch013.xhtml#power-iteration-for-calculating-the-eigenvectors-of-real-symmetric-matrices))
    is great news for us! All we have to do is repeatedly apply the transformation
    A to identify the eigenvector for the dominant eigenvalue λ[1]. There is one small
    caveat, though: we have to know the value of λ[1]. We’ll deal with this later,
    but first, let’s record this milestone in the form of a theorem.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式([7.6](ch013.xhtml#power-iteration-for-calculating-the-eigenvectors-of-real-symmetric-matrices))对我们来说是个好消息！我们所要做的就是反复应用变换A，以识别主特征值λ[1]的特征向量。然而，有一个小警告：我们必须知道λ[1]的值。我们稍后会处理这个问题，但首先，让我们以定理的形式记录这一里程碑。
- en: Theorem 51\.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 定理51。
- en: Finding the eigenvector for the dominant eigenvalue with power iteration.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 使用幂迭代法求主特征值的特征向量。
- en: 'Let A ∈ℝ^(n×n) be a real symmetric matrix. Suppose that:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 设A ∈ℝ^(n×n)是一个实对称矩阵。假设：
- en: (a) The eigenvalues of A are λ[1]/span>…/span>λ[n] (that is, λ[1] is the dominant
    eigenvalue).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: (a) A的特征值为λ[1]…λ[n]（即λ[1]是主特征值）。
- en: (b) The corresponding eigenvectors u[1],…,u[n] form an orthonormal basis.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 对应的特征向量u[1],…,u[n]构成一个正交归一基。
- en: Let x ∈ℝ^n be a vector such that when written as the linear combination x =
    ∑ [i=1]^nx[i]u[i], the coefficient x[1] ∈ℝ is nonzero. Then,
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 设x ∈ℝ^n是一个向量，使得当写作线性组合x = ∑ [i=1]^nx[i]u[i]时，系数x[1] ∈ℝ是非零的。那么，
- en: lim[k → ∞] ![(A_k x - λ_k 1)](img/file712.png) = *x*[1]**u**[1].
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: lim[k → ∞] ![(A_k x - λ_k 1)](img/file712.png) = *x*[1]**u**[1]。
- en: (7.7)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: (7.7)
- en: 'Before we jump into the proof, some explanations are in order. Recall that
    if A is symmetric, the spectral decomposition theorem (Theorem [41](ch013.xhtml#x1-118004r41))
    guarantees that it can be diagonalized with a similarity transformation. In its
    proof (sketch), we mentioned that a symmetric matrix has:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入证明之前，需要做一些解释。回想一下，如果A是对称的，谱分解定理（定理[41](ch013.xhtml#x1-118004r41)）保证它可以通过相似变换对角化。在它的证明（概要）中，我们提到过对称矩阵具有：
- en: Real eigenvalues
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实特征值
- en: An orthonormal basis from its eigenvectors
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由特征向量构成的正交归一基
- en: 'Thus, the assumptions (a) and (b) are guaranteed, except for one caveat: the
    eigenvalues are not necessarily distinct. However, this rarely causes problems
    in practice. There are multiple reasons for this, but most importantly, matrices
    with repeated eigenvalues are so rare that they form a zero-probability set. (We’ll
    learn about probability later in the book. For now, we can assume that randomly
    picking a matrix with repeated eigenvalues is impossible.) Thus, stumbling upon
    one is highly unlikely.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，假设(a)和(b)是有保证的，除了一个警告：特征值不一定是不同的。然而，这在实践中很少引发问题。原因有很多，但最重要的是，具有重复特征值的矩阵非常罕见，以至于它们形成了零概率集。（稍后我们将学习概率论。现在，我们可以假设随机选择一个具有重复特征值的矩阵是不可能的。）因此，碰到这种情况的可能性极小。
- en: Proof. Because u[k] is the eigenvector for the eigenvalue λ[k], we have
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。由于u[k]是特征值λ[k]的特征向量，我们有
- en: '*A*^k**x** = ∑[i=1]^n *x*[i]*λ*[i]^k**u**[i]. (7.8)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*^k**x** = ∑[i=1]^n *x*[i]*λ*[i]^k**u**[i]。（7.8）'
- en: Thus,
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '![ n Akx- ∑ λi-k λk = x1u1 + xi(λ1) ui. 1 i=2 ](img/file713.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![ n Akx- ∑ λi-k λk = x1u1 + xi(λ1) ui. 1 i=2 ](img/file713.png)'
- en: Since λ[1] is the dominant eigenvalue, ∥λ[i]∕λ[1]∥/span>1 for i = 2,…,n, so
    (λ[i]∕λ[1])^k → 0 as k →∞. Hence,
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 由于λ[1]是主特征值，对于i = 2,…,n，∥λ[i]∕λ[1]∥/span>1，因此(λ[i]∕λ[1])^k → 0当k →∞时。因此，
- en: '![ Akx- kli→m∞ λk = x1u1\. 1 ](img/file714.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![ Akx- kli→m∞ λk = x1u1\. 1 ](img/file714.png)'
- en: This is what we had to show.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要证明的。
- en: Now, let’s fix the small issue that requires us to know ![λ1 ](img/file715.png).
    Since ![λ1 ](img/file716.png) is the largest eigenvalue, the previous theorem
    shows that ![Akx ](img/file717.png) equals ![x1 λk1u1 ](img/file718.png) plus
    some term that is much smaller, at least compared to this dominant term. We can
    extract this quantity by taking the supremum norm ![∥Akx ∥∞ ](img/file719.png).
    (Recall that for any ![y = (y1,...,yn) ](img/file720.png), the supremum norm is
    defined by ![∥y ∥∞ = max {|y1|,...,|yn|} ](img/file721.png). Keep in mind that
    the ![yi ](img/file722.png)-s are the coefficients of ![y ](img/file723.png) in
    the original basis of our vector space, which is not necessarily our eigenvector
    basis ![u1,...,un ](img/file724.png).)
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们修复一下需要知道![λ1 ](img/file715.png)的小问题。由于![λ1 ](img/file716.png)是最大特征值，前述定理表明![Akx
    ](img/file717.png)等于![x1 λk1u1 ](img/file718.png)加上一些更小的项，至少相较于这个主项更小。我们可以通过取上确界范数![∥Akx
    ∥∞ ](img/file719.png)来提取这个量。（回想一下，对于任何![y = (y1,...,yn) ](img/file720.png)，上确界范数定义为![∥y
    ∥∞ = max {|y1|,...,|yn|} ](img/file721.png)。请记住，![yi ](img/file722.png)-s是![y
    ](img/file723.png)在我们向量空间原始基下的系数，这不一定是我们特征向量基![u1,...,un ](img/file724.png)。）
- en: By factoring out jλ[1]j^k from A^kx, we have
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将jλ[1]j^k从A^kx中提取出来，我们有
- en: '![ ∑n ∥Akx ∥∞ = |λ1|k∥x1u1 + xi(λi)kui∥∞. i=2 λ1 ](img/file725.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ∥Akx ∥∞ = |λ1|k∥x1u1 + xi(λi)kui∥∞. i=2 λ1 ](img/file725.png)'
- en: Intuitively speaking, the remainder term ∑ [i=2]^nx[i](![λi λ1](img/file727.png))^ku[i]
    is small, thus we can approximate the norm as
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，余项∑ [i=2]^nx[i](![λi λ1](img/file727.png))^ku[i]很小，因此我们可以将范数近似为
- en: '![∥Akx ∥∞ ≈ |λ1 |k∥x1u1∥ ∞. ](img/file729.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![∥Akx ∥∞ ≈ |λ1 |k∥x1u1∥ ∞. ](img/file729.png)'
- en: In other words, instead of scaling with ![λk 1 ](img/file730.png), we can scale
    with ![∥Akx ∥∞ ](img/file731.png).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以用![∥Akx ∥∞ ](img/file731.png)来代替![λk 1 ](img/file730.png)进行缩放。
- en: So, we are ready to describe our general eigenvector-finding procedure fully.
    First, we initialize a vector x[0] randomly, then we define the recursive sequence
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经准备好完整描述我们的一般特征向量求解过程。首先，我们随机初始化一个向量x[0]，然后定义递归序列
- en: '![ Axk −1 xk = ----------, k = 1,2,... ∥Axk −1∥∞ ](img/file732.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![ Axk −1 xk = ----------, k = 1,2,... ∥Axk −1∥∞ ](img/file732.png)'
- en: Using the linearity of A, we can see that, in fact,
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 利用A的线性性质，我们可以看到，实际上，
- en: '![ k xk = --A--x0--, ∥Akx0 ∥∞ ](img/file733.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![ k xk = --A--x0--, ∥Akx0 ∥∞ ](img/file733.png)'
- en: but scaling has an additional side benefit, as we don’t have to use large numbers
    at any computational step. With this, ([51](ch013.xhtml#x1-124005r51)) implies
    that
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 但是缩放有一个额外的好处，因为我们在任何计算步骤中都不需要使用大数字。这样，([51](ch013.xhtml#x1-124005r51))表明
- en: '![ --Akx0--- lki→m∞ xk = kl→im∞ ∥Akx0 ∥∞ = u1\. ](img/file734.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![ --Akx0--- lki→m∞ xk = kl→im∞ ∥Akx0 ∥∞ = u1\. ](img/file734.png)'
- en: That is, we can extract the eigenvector for the dominant eigenvalue without
    actually knowing the eigenvalue itself.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以在不实际知道特征值的情况下，提取出对应于主特征值的特征向量。
- en: 7.5.2 Power iteration in practice
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 幂迭代法的实践
- en: Let’s put the power iteration method into practice! The input of our power_iteration
    function is a square matrix A, and we expect the output to be an eigenvector corresponding
    to the dominant eigenvalue.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将幂迭代法付诸实践！我们power_iteration函数的输入是一个方阵A，我们期望输出的是对应于主特征值的特征向量。
- en: Since this is an iterative process, we should define a condition that defines
    when the process should terminate. If the consecutive members of the sequence
    {x[k]}[k=1]^∞ are sufficiently close together, we arrived at a solution. That
    is, if, say,
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个迭代过程，我们应该定义一个条件来确定何时终止该过程。如果序列{x[k]}[k=1]^∞的连续项足够接近，我们就可以得到一个解。也就是说，如果，例如，
- en: '![∥xk+1 − xk∥2 <1× 10−10, ](img/file735.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![∥xk+1 − xk∥2 <1× 10−10, ](img/file735.png)'
- en: we can stop and return the current value. However, this might never happen.
    For those cases, we define a cutoff point, say, k = 100,000, when we terminate
    the computation, even if there is no convergence.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以停止并返回当前值。然而，这可能永远不会发生。对于这种情况，我们定义一个截止点，比如k = 100,000，当计算达到此点时即使没有收敛，也终止计算。
- en: To give us a bit more control, we can also manually define the initialization
    vector x_init.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给我们更多控制权，我们还可以手动定义初始化向量x_init。
- en: '[PRE0]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To test the method, we should use an input for which the correct output is easy
    to calculate by hand. Our usual recurring example
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这个方法，我们应该使用一个容易手工计算出正确输出的输入。我们常用的示例
- en: '![ ⌊ ⌋ 2 1 A = ⌈ ⌉ . 1 2 ](img/file736.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ 2 1 A = ⌈ ⌉ . 1 2 ](img/file736.png)'
- en: should be perfect, as we already know a lot about it.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 应该是完美的，因为我们已经知道很多关于它的信息。
- en: Previously, we have seen in Section [6.2.2](ch012.xhtml#finding-eigenvectors)
    that its eigenvalues are λ[1] = 3 and λ[2] = 1, with corresponding eigenvectors
    u[1] = (1,1) and u[2] = (−1,1).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在[6.2.2节](ch012.xhtml#finding-eigenvectors)中看到，它的特征值为λ[1] = 3和λ[2] = 1，且对应的特征向量为u[1]
    = (1,1)和u[2] = (−1,1)。
- en: Let’s see if our function correctly recovers (a scalar multiple of) u[1] = (1,1)!
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的函数是否正确地恢复了(u[1] = (1,1))的标量倍数！
- en: '[PRE1]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Success! To recover the eigenvalue, we can simply apply the linear transformation
    and compute the proportions.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！为了恢复特征值，我们只需应用线性变换并计算比例。
- en: '[PRE3]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result is 3, as expected.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是3，正如预期的那样。
- en: 7.5.3 Power iteration for the rest of the eigenvectors
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.3 剩余特征向量的幂迭代
- en: Can we modify the power iteration algorithm to recover the other eigenvalues
    as well? In theory, yes. In practice, no. Let me elaborate!
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否修改幂迭代算法来恢复其他特征值？理论上可以。实际操作中不行。让我详细解释！
- en: To get a grip on how to generalize the idea, let’s take another look at the
    equation ([7.8](#)), saying that
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为了掌握如何推广这个想法，让我们再次查看方程([7.8](#))，表示
- en: '![ ∑n Akx = xiλkiui. i=1 ](img/file737.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n Akx = xiλkiui. i=1 ](img/file737.png)'
- en: One of the conditions for ![ k Aλxk- 1](img/file738.png) to converge was that
    x should have a nonzero component of the eigenvector u[1], that is, x[1]≠0.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '![ k Aλxk- 1](img/file738.png) 收敛的条件之一是x应该包含特征向量u[1]的非零分量，也就是说，x[1]≠0。'
- en: What if x[1] = 0? In that case, we have
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如果x[1] = 0怎么办？在这种情况下，我们得到
- en: '![Akx = x2λk2u2 + ⋅⋅⋅+ xnλk2un, ](img/file739.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![Akx = x2λk2u2 + ⋅⋅⋅+ xnλk2un, ](img/file739.png)'
- en: with x[2]λ[2]^ku[2] becoming the dominant term.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 其中x[2]λ[2]^ku[2]成为主导项。
- en: Thus, we have
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '![Akx ∑n λi k --k-= x2u2 + xi( --) uk λ 2 i=3 λ2 = x u + (something very small),
    2 2 k ](img/file740.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![Akx ∑n λi k --k-= x2u2 + xi( --) uk λ 2 i=3 λ2 = x u + (something very small),
    2 2 k ](img/file740.png)'
- en: implying that
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: '![ Akx lim --k-= x2u2\. k→∞ λ 2 ](img/file741.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![ Akx lim --k-= x2u2\. k→∞ λ 2 ](img/file741.png)'
- en: Let’s make this mathematically precise in the following theorem.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在以下定理中将其数学化。
- en: Theorem 52\. (Generalized power iteration)
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 定理52。（广义幂迭代）
- en: 'Let A ∈ℝ^(n×n) be a real symmetric matrix. Suppose that:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 令A ∈ℝ^(n×n)为一个实对称矩阵。假设：
- en: (a) The eigenvalues of A are λ[1]/span>…/span>λ[n] (that is, λ[1] is the dominant
    eigenvalue).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: (a) A的特征值为λ[1]/span>…/span>λ[n]（即，λ[1]是主特征值）。
- en: (b) The corresponding eigenvectors u[1],…,u[n] form an orthonormal basis.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 相应的特征向量u[1]、…,u[n]构成一个正交规范基。
- en: Let ![x ∈ ℝn ](img/file742.png) be a vector such that, when written as a linear
    combination of the basis ![u1,...,un ](img/file743.png), its first nonzero component
    is along ![ul ](img/file744.png) for some ![l = 1,2,...,n ](img/file745.png) (that
    is, ![ ∑ x = ni=l xiui ](img/file746.png)).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 令 ![x ∈ ℝn ](img/file742.png) 为一个向量，使得当它被表示为基向量 ![u1,...,un ](img/file743.png)
    的线性组合时，它的第一个非零分量沿着 ![ul ](img/file744.png) 方向，其中 ![l = 1,2,...,n ](img/file745.png)（即，
    ![ ∑ x = ni=l xiui ](img/file746.png)）。
- en: Then,
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，
- en: '![ k lim A--x = xlul k→ ∞ λkl ](img/file747.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![ k lim A--x = xlul k→ ∞ λkl ](img/file747.png)'
- en: holds.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 成立。
- en: 'The proof looks just like what we have seen a few times already. The question
    is, how can we eliminate the u[1],…,u[l−1] components from any vector? The answer
    is simple: orthogonal projections (Section [7.4](ch013.xhtml#orthogonal-projections)).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 证明过程与我们之前看到的几次非常相似。问题是，如何从任何向量中消除u[1]、…,u[l−1]分量？答案很简单：正交投影（[7.4节](ch013.xhtml#orthogonal-projections)）。
- en: For the sake of simplicity, let’s take a look at extracting the second dominant
    eigenvector with power iteration. Recall that the transformation
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，让我们看看如何用幂迭代提取第二主特征向量。回想一下变换
- en: '![proj (x) = ⟨x, u ⟩u u1 1 1 ](img/file748.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![proj (x) = ⟨x, u ⟩u u1 1 1 1 ](img/file748.png)'
- en: describes the orthogonal projection of any x to u1.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 描述了任何x到u1的正交投影。
- en: In concrete terms, if x = ∑ [i=1]^nx[i]u[i], then
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，如果x = ∑ [i=1]^nx[i]u[i]，那么
- en: '![ ∑n proju (x) = proju ( xiui) 1 1 i=1 ∑n = xiproj (ui) i=1 u1 = x1u1\. ](img/file749.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n proju (x) = proju ( xiui) 1 1 i=1 ∑n = xiproj (ui) i=1 u1 = x1u1\. ](img/file749.png)'
- en: This is the exact opposite of what we are looking for! However, at this point,
    we can see that I − proj[u[1]] is going to be suitable for our purposes. This
    is still an orthogonal projection. Moreover, we have
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们所寻找的相反方向！然而，在此时，我们可以看到I − proj[u[1]]将适合我们的需求。这仍然是正交投影。此外，我们有
- en: '![ ∑n ∑n (I − proju1)( xiui) = xiui, i=1 i=2 ](img/file750.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ∑n (I − proju1)( xiui) = xiui, i=1 i=2 ](img/file750.png)'
- en: That is, I − proj[u[1]] eliminates the u[1] component of x. Thus, if we initialize
    the power iteration with x^∗ = (I − proj[u[1]])(x), the sequence ![--Akx∗-- ∥Akx∗∥∞](img/file751.png)
    will converge to u[2], the second dominant eigenvector.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 即，I − proj[u[1]] 消除了 x 中的 u[1] 分量。因此，如果我们用 x^∗ = (I − proj[u[1]])(x) 初始化幂迭代，序列
    ![--Akx∗-- ∥Akx∗∥∞](img/file751.png) 将收敛到 u[2]，第二个主特征向量。
- en: 'How do we compute ![(I − proj )(x) u1 ](img/file752.png) in practice? Recall
    that in the standard orthonormal basis, the matrix of ![proju1 ](img/file753.png)
    can be written as:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何实际计算 ![(I − proj )(x) u1 ](img/file752.png)？回忆一下，在标准正交基下，![[proju1](img/file753.png)]
    的矩阵可以写成：
- en: '![proju1 = u1uT1\. ](img/file754.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![proju1 = u1uT1\. ](img/file754.png)'
- en: '(Keep in mind that the ![ui ](img/file755.png) vectors form an orthonormal
    basis, so ![∥u1∥ = 1 ](img/file756.png).) Thus, the matrix of ![I − proju1 ](img/file757.png)
    is:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: （请记住，![[ui](img/file755.png)] 向量构成一个正交归一基，因此 ![∥u1∥ = 1 ](img/file756.png)。）因此，![[I
    − proju1](img/file757.png)] 的矩阵是：
- en: '![I − u1uT1, ](img/file758.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![I − u1uT1, ](img/file758.png)'
- en: which we can easily compute.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地计算出这个结果。
- en: For a general vector u, this is how we can do this in NumPy.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个一般的向量 u，我们可以这样在 NumPy 中实现。
- en: '[PRE5]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So, the procedure to find all the eigenvectors is the following.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，找出所有特征向量的过程如下。
- en: Initialize a random ![ (1) x ](img/file759.png) and use the power iteration
    to find ![u1 ](img/file760.png).
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个随机的 ![ (1) x ](img/file759.png)，并使用幂迭代找到 ![u1 ](img/file760.png)。
- en: Project ![x (1) ](img/file761.png) to the orthogonal complement of the subspace
    spanned by ![u 1 ](img/file762.png), thus obtaining
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 ![x (1) ](img/file761.png) 投影到由 ![u 1 ](img/file762.png) 所张成的子空间的正交补空间，从而得到
- en: '![x (2) := (I − proju1)(x(1)), ](img/file763.png)'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![x (2) := (I − proju1)(x(1)), ](img/file763.png)'
- en: which we use as the initial vector of the second round of power iteration, yielding
    the second dominant eigenvector ![u2 ](img/file764.png).
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将其作为第二轮幂迭代的初始向量，从而得到第二个主特征向量 ![u2 ](img/file764.png)。
- en: Project ![x (2) ](img/file765.png) to the orthogonal complement of the subspace
    spanned by ![u 1 ](img/file766.png) and ![u 2 ](img/file767.png), thus obtaining
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 ![x (2) ](img/file765.png) 投影到由 ![u 1 ](img/file766.png) 和 ![u 2 ](img/file767.png)
    所张成的子空间的正交补空间，从而得到
- en: '![x(3) = (I − proju2)(x(2)) (1) = (I − proju1,u2)(x ), ](img/file768.png)'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![x(3) = (I − proju2)(x(2)) (1) = (I − proju1,u2)(x ), ](img/file768.png)'
- en: which we use as the initial vector of the third round of power iteration, yielding
    the third dominant eigenvector ![u3 ](img/file769.png).
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将其作为第三轮幂迭代的初始向量，从而得到第三个主特征向量 ![u3 ](img/file769.png)。
- en: Project ![ (3) x ](img/file770.png) to…
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 ![ (3) x ](img/file770.png) 投影到……
- en: You get the pattern. To implement this in practice, we add the find_eigenvectors
    function.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经明白了这个模式。为了在实践中实现这一点，我们添加了 find_eigenvectors 函数。
- en: '[PRE6]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s test find_eigenvectors out on our old friend
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的老朋友上测试一下 find_eigenvectors
- en: '![ ⌊ ⌋ 2 1 A = ⌈ ⌉! 1 2 ](img/file771.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ 2 1 A = ⌈ ⌉! 1 2 ](img/file771.png)'
- en: '[PRE7]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The result is as we expected. (Don’t be surprised that the eigenvectors are
    not normalized, as we haven’t explicitly done so in the find_eigenvectors function.)
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如我们所料。（不要惊讶于特征向量未被归一化，因为我们在 find_eigenvectors 函数中并没有显式执行这一操作。）
- en: We are ready to actually diagonalize symmetric matrices. Recall that the diagonalizing
    orthogonal matrix U can be obtained by vertically stacking the eigenvectors one
    by one.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好真正对称矩阵进行对角化。回忆一下，通过将特征向量一个个垂直堆叠，可以获得对角化的正交矩阵 U。
- en: '[PRE9]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Awesome!
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！
- en: What’s the problem? Unfortunately, power iteration is numerically unstable.
    For n ×n matrices, where n can be in the millions, this is a serious issue.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 什么问题？不幸的是，幂迭代在数值上不稳定。对于 n × n 的矩阵，其中 n 可能达到数百万，这个问题非常严重。
- en: Why did we talk so much about power iteration, then? Besides being the simplest,
    it offers a deep insight into how linear transformations work.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们要讲这么多关于幂迭代的内容呢？除了它是最简单的，它还提供了对线性变换如何工作的深刻洞察。
- en: The identity
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这个恒等式
- en: '![ ∑n Ax = xiλiui, i=1 ](img/file772.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n Ax = xiλiui, i=1 ](img/file772.png)'
- en: where λ[i] and u[i] are eigenvalue-eigenvector pairs of the symmetric matrix
    A, reflecting how eigenvectors and eigenvalues determine the behavior of the transformation.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 λ[i] 和 u[i] 是对称矩阵 A 的特征值-特征向量对，反映了特征向量和特征值如何决定变换的行为。
- en: If the power iteration is not usable in practice, how can we compute the eigenvalues?
    We will see this in the next section.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 如果幂迭代在实践中不可用，我们该如何计算特征值呢？我们将在下一节中看到这一点。
- en: 7.6 The QR algorithm
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 QR 算法
- en: The algorithm used in practice to compute the eigenvalues is the so-called QR
    algorithm, proposed independently by John G. R. Francis and the Soviet mathematician
    Vera Kublanovskaya. This is where all of the lessons we have learned in linear
    algebra converge. Describing the QR algorithm is very simple, as it is the iteration
    of a matrix decomposition and a multiplication step.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中用于计算特征值的算法是所谓的 QR 算法，由 John G. R. Francis 和苏联数学家 Vera Kublanovskaya 独立提出。这就是我们在学习线性代数时所有知识的汇聚点。描述
    QR 算法非常简单，因为它是矩阵分解和乘法步骤的迭代。
- en: However, understanding why it works is a different question. Behind the scenes,
    the QR algorithm combines many tools we have learned earlier. To start, let’s
    revisit the good old Gram-Schmidt orthogonalization process.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，理解它为什么有效是另一个问题。在幕后，QR 算法结合了我们之前学习的许多工具。首先，让我们回顾一下经典的格拉姆-施密特正交化过程。
- en: 7.6.1 The QR decomposition
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6.1 QR 分解
- en: If you recall, we encountered the Gram-Schmidt orthogonalization process (Theorem [13](ch008.xhtml#x1-47004r13))
    when introducing the concept of orthogonal bases.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得，我们在引入正交基的概念时遇到了格拉姆-施密特正交化过程（定理[13](ch008.xhtml#x1-47004r13)）。
- en: In essence, this algorithm takes an arbitrary basis v[1],…,v[n] and turns it
    into an orthonormal one e[1],…,e[n], such that e[1],…,e[k] spans the same subspace
    as v[1],…,v[k] for all 1 ≤k ≤n. Since we last met this, we have gained a lot of
    perspective on linear algebra, so we are ready to see the bigger picture.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这个算法将一个任意基 v[1],…,v[n] 转换为一个正交标准基 e[1],…,e[n]，使得对于所有 1 ≤k ≤n，e[1],…,e[k]
    与 v[1],…,v[k] 张成相同的子空间。自从我们上次遇到这个问题以来，我们已经对线性代数有了更多的了解，所以我们已经准备好看到更大的图景。
- en: With the orthogonal projections defined by
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 通过定义的正交投影
- en: '![ ∑k ⟨x,ei⟩ proje1,...,ek(x) = ------ei, i=1 ⟨ei,ei⟩ ](img/file773.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑k ⟨x,ei⟩ proje1,...,ek(x) = ------ei, i=1 ⟨ei,ei⟩ ](img/file773.png)'
- en: we can describe the Gram-Schmidt process recursively as
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以递归地描述格拉姆-施密特过程为
- en: '![e1 = v1, ek = vk − proje1,...,ek−1(vk), ](img/file774.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![e1 = v1, ek = vk − proje1,...,ek−1(vk), ](img/file774.png)'
- en: where the e[k] vectors are normalized after.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 e[k] 向量在之后是标准化的。
- en: By expanding this and writing out e[k] explicitly, we have
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展并明确写出 e[k] 后，我们得到
- en: '![e1 = v1 e2 = v2 − ⟨v2,e1⟩e1 ⟨e1,e1⟩ .. . ⟨vn, e1⟩ ⟨vn, en−1⟩ en = vn − ⟨e-,e-⟩e1
    − ⋅⋅⋅− ⟨e----,e---⟩en−1\. 1 1 n− 1 n−1 ](img/file775.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![e1 = v1 e2 = v2 − ⟨v2,e1⟩e1 ⟨e1,e1⟩ .. . ⟨vn, e1⟩ ⟨vn, en−1⟩ en = vn − ⟨e-,e-⟩e1
    − ⋅⋅⋅− ⟨e----,e---⟩en−1\. 1 1 n− 1 n−1 ](img/file775.png)'
- en: A pattern is starting to emerge. By arranging the e[1],…,e[n] terms on one side,
    we obtain
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 一种模式开始显现出来。通过将 e[1],…,e[n] 项排列在一边，我们得到
- en: '![v1 = e1 ⟨v2,e1⟩ v2 = ⟨e-,e-⟩e1 + e2 1 1 ... vn = ⟨vn,e1⟩e1 + ⋅⋅⋅ + -⟨vn,en−1⟩-en−1
    + en. ⟨e1,e1⟩ ⟨en−1,en−1⟩ ](img/file776.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![v1 = e1 ⟨v2,e1⟩ v2 = ⟨e-,e-⟩e1 + e2 1 1 ... vn = ⟨vn,e1⟩e1 + ⋅⋅⋅ + -⟨vn,en−1⟩-en−1
    + en. ⟨e1,e1⟩ ⟨en−1,en−1⟩ ](img/file776.png)'
- en: This is starting to resemble some kind of matrix multiplication! Recall that
    matrix multiplication can be viewed as taking the linear combination of columns.
    (Check ([4.2](#)) if you are uncertain about this.)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 这开始像某种矩阵乘法了！回想一下，矩阵乘法可以看作是对列向量的线性组合。（如果不确定这一点，可以查阅 ([4.2](#))。）
- en: By horizontally concatenating the column vectors v[k] to form the matrix A and
    similarly defining the vector Q from the e[k]-s, we obtain that
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将列向量 v[k] 水平拼接成矩阵 A，并类似地通过 e[k] 向量定义向量 Q，我们得到
- en: '![A = Q∗R ∗ ](img/file777.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![A = Q∗R ∗ ](img/file777.png)'
- en: for some upper triangular R, defined by the coefficients of e[k] in v[k] according
    to the Gram-Schmidt orthogonalization. To be more precise, define
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些上三角矩阵 R，它由根据格拉姆-施密特正交化方法确定的 e[k] 在 v[k] 中的系数定义。更精确地说，定义
- en: '![ ⌊ ⌋ ⌊ ⌋ A = ||v ... v || , Q ∗ = || e ... e || , ⌈ 1 n⌉ ⌈ 1 n⌉ ](img/file778.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ⌊ ⌋ A = ||v ... v || , Q ∗ = || e ... e || , ⌈ 1 n⌉ ⌈ 1 n⌉ ](img/file778.png)'
- en: and
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![ ⌊ ⟨v2,e1⟩ ⟨vn,e1⟩⌋ |1 ⟨e1,e1⟩ ... ⟨e1,e1⟩| ||0 1 ... ⟨vn,e2⟩|| R ∗ = |.
    . ⟨e2,.e2⟩|. ||.. .. ... .. || ⌈ .. ⌉ 0 0 . 1 ](img/file779.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⟨v2,e1⟩ ⟨vn,e1⟩⌋ |1 ⟨e1,e1⟩ ... ⟨e1,e1⟩| ||0 1 ... ⟨vn,e2⟩|| R ∗ = |.
    . ⟨e2,.e2⟩|. ||.. .. ... .. || ⌈ .. ⌉ 0 0 . 1 ](img/file779.png)'
- en: The result A = Q^∗R^∗ is almost what we call the QR factorization. The columns
    of Q^∗ are orthogonal (but not orthonormal), while R^∗ is upper triangular. We
    can easily orthonormalize Q^∗ by factoring out the norms columnwise, thus obtaining
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 A = Q^∗R^∗ 几乎就是我们所说的 QR 分解。Q^∗ 的列是正交的（但不是标准正交的），而 R^∗ 是上三角矩阵。我们可以通过按列提取范数轻松将
    Q^∗ 正交标准化，从而得到
- en: '![ ⌊ ⌋ ∥e ∥ √⟨v2,e1⟩-- ... √⟨vn,e1⟩- ⌊ ⌋ | 1 ⟨e1,e1⟩ ⟨e1,e1⟩| | | || 0 ∥e2∥
    ... √⟨vn,e2⟩|| Q = |⌈ e1-- ... en-|⌉ , R = || ⟨e2,e2⟩|| . ∥e1∥ ∥en∥ || ... ...
    ... ... || ⌈ . ⌉ 0 0 .. ∥en∥ ](img/file780.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ∥e ∥ √⟨v2,e1⟩-- ... √⟨vn,e1⟩- ⌊ ⌋ | 1 ⟨e1,e1⟩ ⟨e1,e1⟩| | | || 0 ∥e2∥
    ... √⟨vn,e2⟩|| Q = |⌈ e1-- ... en-|⌉ , R = || ⟨e2,e2⟩|| . ∥e1∥ ∥en∥ || ... ...
    ... ... || ⌈ . ⌉ 0 0 .. ∥en∥ ](img/file780.png)'
- en: It is easy to see that A = QR still holds. This result is called the QR decomposition,
    and we have just proved the following theorem.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，A = QR仍然成立。这个结果被称为QR分解，我们刚刚证明了以下定理。
- en: Theorem 53\. (QR decomposition)
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 定理53. (QR分解)
- en: Let A ∈ ℝ^(n×n) be an invertible matrix. Then, there exists an orthogonal matrix
    Q ∈ℝ^(n×n) and an upper triangular matrix R ∈ℝ^(n×n) such that
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 设A ∈ ℝ^(n×n)为可逆矩阵。那么，存在一个正交矩阵Q ∈ ℝ^(n×n)和一个上三角矩阵R ∈ ℝ^(n×n)，使得
- en: '![A = QR ](img/file781.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![A = QR ](img/file781.png)'
- en: holds.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 公式成立。
- en: As we are about to see, the QR decomposition is an extremely useful and versatile
    tool (like all other matrix decompositions are). Before we move forward to discuss
    how it can be used to compute the eigenvalues in practice, let’s put what we have
    seen so far into code!
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将要看到的，QR分解是一个极其有用且多功能的工具（就像所有其他矩阵分解一样）。在我们继续讨论它如何用于实际计算特征值之前，让我们先将迄今为止的内容转化为代码！
- en: The QR decomposition algorithm is essentially Gram-Schmidt orthogonalization,
    where we explicitly memorize some coefficients and form a matrix from them. (Recall
    our earlier implementation in Section [3.1.2](ch009.xhtml#the-gramschmidt-orthogonalization-process1)
    if you feel overwhelmed.)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: QR分解算法本质上是Gram-Schmidt正交化，我们显式地记住一些系数并由此形成一个矩阵。（如果你觉得有些困惑，回顾一下我们在第[3.1.2节](ch009.xhtml#the-gramschmidt-orthogonalization-process1)的实现。）
- en: '[PRE11]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now we can put these together to obtain the QR factorization of an arbitrary
    square matrix. (Surprisingly, this works for non-square matrices as well, but
    we won’t be concerned with this.)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这些工具结合起来，得到一个任意方阵的QR分解。（令人惊讶的是，这对于非方阵也适用，但我们不在此讨论。）
- en: '[PRE13]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s try it out on a random 3 × 3 matrix.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对一个随机的3×3矩阵进行尝试。
- en: '[PRE14]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'There are three things to check: (a) A = QR, (b) Q is an orthogonal matrix,
    and (c) R is upper triangular.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 需要检查三件事：（a）A = QR，（b）Q是正交矩阵，以及（c）R是上三角矩阵。
- en: '[PRE15]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Success! There is only one more question left. How does this help us in calculating
    the eigenvalues? Let’s see that now.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！只剩下一个问题了。这个如何帮助我们计算特征值呢？我们现在来看看。
- en: 7.6.2 Iterating the QR decomposition
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6.2 迭代QR分解
- en: Surprisingly, we can discover the eigenvalues of a matrix A by a simple iterative
    process. First, we find the QR decomposition
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，我们可以通过一个简单的迭代过程发现矩阵A的特征值。首先，我们找到QR分解。
- en: '![A = Q R , 1 1 ](img/file783.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![A = Q R , 1 1 ](img/file783.png)'
- en: and define the matrix A[1] by
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 并通过以下方式定义矩阵A[1]
- en: '![A1 = R1Q1, ](img/file784.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![A1 = R1Q1, ](img/file784.png)'
- en: That is, we simply reverse the order of Q and R. Then, we start it all over
    and find the QR decomposition of A[1], and so on, defining the sequence
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们只是反转Q和R的顺序。然后，我们从头开始，找到A[1]的QR分解，依此类推，定义这个序列
- en: '![L(U,V ) = {f : U → V | f is linear}](img/file785.png)(7.9)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![L(U,V ) = {f : U → V | f is linear}](img/file785.png)(7.9)'
- en: In the long run, the diagonal elements of A[k] will get closer and closer to
    the eigenvalues of A. This is called the QR algorithm, which is so simple that
    I didn’t believe it when I first saw it.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 从长远来看，A[k]的对角元素将越来越接近A的特征值。这就是所谓的QR算法，简单到我第一次看到时都不敢相信。
- en: With all of our tools, we can implement the QR algorithm in a few lines.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 有了所有这些工具，我们可以在几行代码中实现QR算法。
- en: '[PRE21]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s test it right away.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们立即测试一下。
- en: '[PRE22]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are almost at the state of the art. Unfortunately, the vanilla QR algorithm
    has some issues, as it can fail to converge. A simple example is given by the
    matrix
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎达到了最前沿的技术。不幸的是，传统的QR算法有一些问题，因为它可能无法收敛。一个简单的例子是由矩阵给出的
- en: '![ ⌊ ⌋ A = ⌈0 1⌉ . 1 0 ](img/file786.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ A = ⌈0 1⌉ . 1 0 ](img/file786.png)'
- en: '[PRE24]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In practice, we can solve this with the introduction of shifts:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，我们可以通过引入位移来解决这个问题：
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(14).png)(7.10)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(14).png)(7.10)'
- en: where α[k] is some scalar. There are multiple approaches to defining the shifts
    themselves (Rayleigh quotient shift, Wilkinson shift, etc.), but the details lie
    much deeper than our study.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 其中α[k]是某个标量。定义这些位移的方法有很多种（例如Rayleigh商位移、Wilkinson位移等），但这些细节超出了我们的研究范围。
- en: 7.7 Summary
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.7 总结
- en: 'I told you that climbing the peak is not easy: so far, this was our hardest
    chapter yet. However, the tools we’ve learned are at the pinnacle of linear algebra.
    We started by studying two special transformations: the self-adjoint and orthogonal
    ones. The former ones gave the spectral decomposition theorem, while the latter
    ones gave the singular value decomposition.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我告诉过你，攀登高峰并不容易：到目前为止，这是我们遇到的最难的一章。然而，我们所学的工具已经达到了线性代数的顶峰。我们首先研究了两种特殊的变换：自伴随变换和正交变换。前者带来了谱分解定理，而后者则给出了奇异值分解。
- en: Undoubtedly, the SVD is one of the most important results in linear algebra,
    stating that every rectangular matrix A can be written in the form
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，奇异值分解（SVD）是线性代数中最重要的结果之一，它表明每个矩形矩阵A都可以写成如下形式：
- en: '![A = U ΣV T, ](img/file787.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![A = U ΣV T, ](img/file787.png)'
- en: 'where U ∈ℝ^(n×n), Σ ∈ℝ^(n×m), and V ∈ℝ^(m×m) are rather special: Σ is diagonal,
    while U and V are orthogonal.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 U ∈ℝ^(n×n)，Σ ∈ℝ^(n×m)，V ∈ℝ^(m×m)是相当特殊的：Σ是对角矩阵，而U和V是正交矩阵。
- en: When viewing A as a linear transformation, the singular value decomposition
    tells us that it can be written as the composition of two distance-preserving
    transformations (the orthogonal ones) and a simple scaling. That’s quite a characterization!
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 当将A视为一个线性变换时，奇异值分解告诉我们，它可以写成两个保持距离的变换（正交变换）和一个简单的缩放的组合。这个表述相当准确！
- en: Speaking of singular values and eigenvalues, how do we find them in practice?
    Definitely not by solving the polynomial equation
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 说到奇异值和特征值，我们如何在实际中找到它们呢？绝对不是通过解多项式方程。
- en: '![det(A − λI ) = 0, ](img/file788.png)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![det(A − λI ) = 0, ](img/file788.png)'
- en: which is a computationally painful problem.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个计算上非常痛苦的问题。
- en: We’ve seen two actual methods for the task. One is the complicated, slow, unstable,
    but illuminating algorithm of power iteration, yielding eigenvectors for the dominant
    eigenvalue of a real and symmetric matrix A via the limit
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了两种实际方法来解决这个任务。一个是复杂、缓慢、不稳定，但富有启发性的幂迭代算法，它通过极限为实对称矩阵A的主特征值求得特征向量
- en: '![ Akx0 lk→im∞ ∥Akx--∥-- = u1\. 0 ∞ ](img/file789.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![ Akx0 lk→im∞ ∥Akx--∥-- = u1\. 0 ∞ ](img/file789.png)'
- en: Although power iteration gives us valuable insight into the structure of such
    matrices, the real deal is the QR algorithm (unrelated to QR codes), originating
    from the vectorized version of the Gram-Schmidt algorithm. The QR algorithm is
    hard to intuitively understand, but despite its mystery, it provides a blazing-fast
    method for computing the eigenvalues in practice.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管幂迭代法为我们提供了有关这类矩阵结构的宝贵见解，但真正的关键是QR算法（与二维码无关），它源自格拉姆-施密特算法的矢量化版本。QR算法难以直观理解，但尽管其神秘性，它提供了一种在实践中快速计算特征值的方法。
- en: What’s next? Now that we are at the peak, it’s time to relax a bit and enjoy
    the beautiful view. The next chapter does just that.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是什么？现在我们已经登上了顶峰，是时候稍微放松一下，享受美丽的景色了。下一章正是如此。
- en: You see, one of the most beautiful and useful topics in linear algebra is the
    connection between matrices and graphs. Because from the right perspective, a
    matrix is a graph, and we can utilize this relationship to study the structure
    of both. Let’s see!
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 你看，线性代数中最美丽和最有用的主题之一是矩阵与图形之间的关系。因为从正确的角度来看，矩阵就是图形，我们可以利用这种关系来研究两者的结构。让我们来看看！
- en: 7.8 Problems
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.8 问题
- en: Problem 1\. Let u[1],…,u[k] ∈ℝ^n be a set of linearly independent and pairwise
    orthogonal vectors. Show that the linear transformation
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 问题1. 设u[1],…,u[k] ∈ℝ^n为一组线性无关且两两正交的向量。证明线性变换
- en: '![ ∑k ⟨x, ui⟩ proju1,...,uk(x) = ⟨u-,u-⟩ui i=1 i i ](img/file790.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑k ⟨x, ui⟩ proju1,...,uk(x) = ⟨u-,u-⟩ui i=1 i i ](img/file790.png)'
- en: is an orthogonal projection.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个正交投影。
- en: Problem 2\. Let u[1],…,u[k] ∈ℝ^n be a set of linearly independent vectors, and
    define the linear transformation
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 问题2. 设u[1],…,u[k] ∈ℝ^n为一组线性无关的向量，并定义线性变换
- en: '![ ∑k ⟨x,ui⟩- fakeproju1,...,uk(x ) = ⟨ui,ui⟩ui. i=1 ](img/file791.png)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑k ⟨x,ui⟩- fakeproju1,...,uk(x ) = ⟨ui,ui⟩ui. i=1 ](img/file791.png)'
- en: 'Is this a projection? (Hint: Study the special case k = 2 and ℝ³. You can visualize
    this if needed.)'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个投影吗？（提示：研究特殊情况k = 2和ℝ³。如果需要，你可以将其可视化。）
- en: 'Problem 3\. Let V be an inner product space and P : V →V be an orthogonal projection.
    Show that I −P is an orthogonal projection as well, and'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '问题3. 设V是一个内积空间，P: V →V是一个正交投影。证明I −P也是一个正交投影，并且'
- en: '![ker(I − P ) = im P, im (I − P ) = ker P ](img/file792.png)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![ker(I − P ) = im P, im (I − P ) = ker P ](img/file792.png)'
- en: holds.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 成立。
- en: Problem 4\. Let A,B ∈ℝ^(n×n) be two square matrix that are written in the block
    matrix form
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 问题4. 设A,B ∈ℝ^(n×n)为两个方阵，它们以块矩阵形式写出
- en: '![ ⌊ ⌋ ⌊ ⌋ A1,1 A1,2 B1,1 B1,2 A = ||A A ||, B = ||B B ||, ⌈ 2,1 2,2⌉ ⌈ 2,1
    2,2⌉ ](img/file793.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ⌊ ⌋ A1,1 A1,2 B1,1 B1,2 A = ||A A ||, B = ||B B ||, ⌈ 2,1 2,2⌉ ⌈ 2,1
    2,2⌉ ](img/file793.png)'
- en: where A[1,1],B[1,1] ∈ℝ^(k×k), A[1,2],B[1,2] ∈ℝ^(k×l), A[2,1],B[2,1] ∈ℝ^(l×k),
    and A[2,2],B[2,2] ∈ℝ^(l×l).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 其中A[1,1],B[1,1] ∈ℝ^(k×k)，A[1,2],B[1,2] ∈ℝ^(k×l)，A[2,1],B[2,1] ∈ℝ^(l×k)，A[2,2],B[2,2]
    ∈ℝ^(l×l)。
- en: Show that
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 证明以下内容
- en: '![ ⌊ ⌋ A1,1B1,1 + A1,2B2,1 A1,1B1,2 + A1,2B2,2 || || AB = ⌈A2,1B1,1 + A2,2B2,1
    A2,1B1,2 + A2,2B2,2⌉ . ](img/file794.png)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ A1,1B1,1 + A1,2B2,1 A1,1B1,2 + A1,2B2,2 || || AB = ⌈A2,1B1,1 + A2,2B2,1
    A2,1B1,2 + A2,2B2,2⌉ . ](img/file794.png)'
- en: Problem 5\. Let A ∈ℝ^(2×2) be the square matrix defined by
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 5\. 设A ∈ℝ^(2×2)是由以下定义的方阵：
- en: '![ ⌊ ⌋ A = ⌈1 1⌉ . 1 0 ](img/file795.png)'
  id: totrans-493
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ A = ⌈1 1⌉ . 1 0 ](img/file795.png)'
- en: (a) Show that the two eigenvalues of A are
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 证明A的两个特征值为
- en: '![ − 1 λ1 = φ, λ2 = − φ , ](img/file796.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![ − 1 λ1 = φ, λ2 = − φ , ](img/file796.png)'
- en: where φ = ![1+√5 --2--](img/file797.png) is the golden ratio.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 其中φ = ![1+√5 --2--](img/file797.png) 是黄金比例。
- en: (b) Show that the eigenvectors of λ[1] and λ[2] are
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 证明λ[1]和λ[2]的特征向量为
- en: '![ ⌊ ⌋ ⌊ −1⌋ f = ⌈φ ⌉, f = ⌈− φ ⌉ , 1 1 2 1 ](img/file798.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ⌊ −1⌋ f = ⌈φ ⌉, f = ⌈− φ ⌉ , 1 1 2 1 ](img/file798.png)'
- en: and show that they are orthogonal; that is, ⟨f[1],f[2]⟩ = 0.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 并证明它们是正交的；即，⟨f[1],f[2]⟩ = 0。
- en: (c) Let U be the matrix formed by the eigenvectors of A, defined by
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 设U为由A的特征向量组成的矩阵，定义为
- en: '![ ⌊ ⌋ φ − φ−1 U = ⌈ ⌉ . 1 1 ](img/file799.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ φ − φ−1 U = ⌈ ⌉ . 1 1 ](img/file799.png)'
- en: Show that
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 证明以下内容
- en: '![ ⌊ ⌋ 1 φ −1 U −1 = √1-⌈ ⌉ . 5 − 1 φ ](img/file800.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ 1 φ −1 U −1 = √1-⌈ ⌉ . 5 − 1 φ ](img/file800.png)'
- en: (d) Show that
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 证明以下内容
- en: '![ − 1 A = UΛU , ](img/file801.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![ − 1 A = UΛU , ](img/file801.png)'
- en: where
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![ ⌊ ⌋ ⌊ ⌋ λ1 0 φ 0 Λ = ⌈ ⌉ = ⌈ −1⌉ . 0 λ2 0 − φ ](img/file802.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ⌊ ⌋ λ1 0 φ 0 Λ = ⌈ ⌉ = ⌈ −1⌉ . 0 λ2 0 − φ ](img/file802.png)'
- en: (e) Were you wondering the purpose of all these mundane computations? Here comes
    the punchline. First, show that
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 你是否曾经想过这些日常计算的目的？接下来是关键。首先，证明
- en: '![ ⌊ ⌋ ⌊ ⌋ n An = ⌈1 1⌉ = ⌈Fn+1 Fn ⌉ , 1 0 Fn Fn−1 ](img/file803.png)'
  id: totrans-509
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ⌊ ⌋ n An = ⌈1 1⌉ = ⌈Fn+1 Fn ⌉ , 1 0 Fn Fn−1 ](img/file803.png)'
- en: where F[n] is the n-th Fibonacci number, defined by the recursive sequence
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 其中F[n]是第n个斐波那契数，定义为递归序列
- en: '![F0 = 0, F1 = 1, Fn = Fn−1 + Fn−2\. ](img/file804.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![F0 = 0, F1 = 1, Fn = Fn−1 + Fn−2\. ](img/file804.png)'
- en: Consequently, show that A = UΛU^(−1) implies that
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，证明A = UΛU^(−1)意味着
- en: '![ n −n Fn = φ--−-(√−-φ)-- . 5 ](img/file805.png)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![ n −n Fn = φ--−−(√−-φ)-- . 5 ](img/file805.png)'
- en: That’s pretty cool!
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 真酷！
- en: Join our community on Discord
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他用户、机器学习专家以及作者本人一起阅读本书。提出问题，为其他读者提供解决方案，通过“问我任何问题”环节与作者互动，等等。扫描二维码或访问链接加入社区。[https://packt.link/math](https://packt.link/math)
- en: '![PIC](img/file1.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
