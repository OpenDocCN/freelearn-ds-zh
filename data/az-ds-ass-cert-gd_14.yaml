- en: '*Chapter 11*: Working with Pipelines'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：使用管道'
- en: In this chapter, you will learn how you can author repeatable processes, defining
    pipelines that consist of multiple steps. You can use these pipelines to author
    training pipelines that transform your data and then train models, or you can
    use them to perform batch inferences using pre-trained models. Once you register
    one of those pipelines, you can invoke it using either an HTTP endpoint or through
    the SDK, or even configure them to execute on a schedule. With this knowledge,
    you will be able to implement and consume pipelines by using the **Azure Machine
    Learning** (**AzureML**) SDK.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何创建可重复使用的过程，定义由多个步骤组成的管道。您可以使用这些管道来创建训练管道，转换数据并训练模型，或者使用它们执行批量推断，使用预训练的模型。一旦注册了这些管道，您可以通过
    HTTP 端点或 SDK 调用它们，甚至可以将其配置为按计划执行。掌握这些知识后，您将能够通过使用 **Azure 机器学习**（**AzureML**）SDK
    来实现和使用管道。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding AzureML pipelines
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 AzureML 管道
- en: Authoring a pipeline
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建管道
- en: Publishing a pipeline to expose it as an endpoint
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布管道以将其暴露为端点
- en: Scheduling a recurring pipeline
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安排定期执行管道
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need to have access to an Azure subscription. Within that subscription,
    you will need a `packt-azureml-rg`. You will need to have either a `Contributor`
    or `Owner` `packt-learning-mlw`, as described in [*Chapter 2*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026),
    *Deploying Azure Machine Learning Workspace Resources*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要拥有一个 Azure 订阅。在该订阅中，您需要一个 `packt-azureml-rg`。您需要拥有 `Contributor` 或 `Owner`
    角色的 `packt-learning-mlw`，如在 [*第2章*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026)
    中的 *部署 Azure 机器学习工作区资源* 所描述的。
- en: You will also need to have a basic understanding of the **Python** language.
    The code snippets target Python version 3.6 or newer. You should also be familiar
    with working in the notebook experience within AzureML studio, something that
    was covered in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要具备 **Python** 语言的基本知识。代码片段针对的是 Python 3.6 或更高版本。您还应该熟悉在 AzureML studio 中使用笔记本体验，这一点在
    [*第8章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117) 中的 *Python 代码实验* 部分有涉及。
- en: This chapter assumes you have registered the **loans** dataset you generated
    in [*Chapter 10*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147), *Understanding
    Model Results*. It is also assumed that you have created a compute cluster named
    **cpu-sm-cluster**, as described in the *Working with compute targets* section
    in [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102), *The AzureML
    Python SDK*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设您已经注册了在 [*第10章*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147) 中生成的 **loans**
    数据集，*理解模型结果*。同时假设您已经创建了名为 **cpu-sm-cluster** 的计算集群，如在 [*第7章*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102)
    中的 *使用计算目标* 部分所描述的那样，*AzureML Python SDK*。
- en: 'You can find all the notebooks and code snippets for this chapter in GitHub
    at the following URL: [http://bit.ly/dp100-ch11](http://bit.ly/dp100-ch11).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 GitHub 上找到本章的所有笔记本和代码片段，网址如下：[http://bit.ly/dp100-ch11](http://bit.ly/dp100-ch11)。
- en: Understanding AzureML pipelines
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 AzureML 管道
- en: In [*Chapter 6*](B16777_06_Final_VK_ePub.xhtml#_idTextAnchor084), *Visual Model
    Training and Publishing*, you saw how you can design a training process using
    building boxes. Similar to those workflows, the AzureML SDK allows you to author
    `Pipelines` that orchestrate multiple steps. For example, in this chapter, you
    will author a `Pipeline` that consists of two steps. The first step pre-processes
    the `Environment`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第6章*](B16777_06_Final_VK_ePub.xhtml#_idTextAnchor084) 中，*可视化模型训练与发布*，您已经看到如何使用构建块设计训练过程。类似于这些工作流，AzureML
    SDK 允许您创建 `Pipelines` 来协调多个步骤。例如，在本章中，您将创建一个由两步组成的 `Pipeline`。第一步对 `Environment`
    进行预处理。
- en: Important note
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Do not confuse the `Pipelines` with the `Pipelines` you read in [*Chapter 10*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147),
    *Understanding Model Results*. The `Pipelines` as a wrapper around the actual
    model class that you want to train and use for inferences.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将 `Pipelines` 与您在 [*第10章*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147)
    中阅读的 `Pipelines` 混淆，*理解模型结果*。`Pipelines` 是围绕您要训练并用于推断的实际模型类的包装器。
- en: 'The AzureML SDK offers quite a few building blocks that you can use to construct
    a `Pipeline`. *Figure 11.1* contains the most popular classes that you may encounter
    in the exam and real-life code:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: AzureML SDK提供了许多构建模块，你可以使用它们来构建`Pipeline`。*图11.1*包含了你可能在考试和实际代码中遇到的最常用的类：
- en: '![Figure 11.1 – Classes available in the AzureML SDK to author your pipelines'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.1 – 用于编写管道的AzureML SDK中的可用类'
- en: '](img/B16777_11_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_11_001.jpg)'
- en: Figure 11.1 – Classes available in the AzureML SDK to author your pipelines
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 用于编写管道的AzureML SDK中的可用类
- en: The **Pipeline** is the core class that defines a workflow that stitches together
    multiple steps. You can pass in parameters to a pipeline by defining them using
    the **PipelineParameter** class. These parameters can be references in one or
    more steps within the **Pipeline**. Once you have finished defining a pipeline,
    you can publish it to register it in the AzureML workspace as a versioned object
    that can be referenced using the **PublishedPipeline** class. This published pipeline
    has an endpoint that you can use to trigger its execution. If you want, you can
    define a **Schedule** and have this **PublishedPipeline** class triggered at a
    specific time interval. **PipelineData** defines temporary storage where one step
    can drop some files for the next one to pick them up. The data dependency between
    those two steps creates an implicit execution order in the **Pipeline**, meaning
    that the dependent step will wait for the first step to complete. You will be
    using all these classes in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pipeline**是定义工作流的核心类，它将多个步骤串联在一起。你可以通过使用**PipelineParameter**类来定义管道参数，并将它们传递给管道。这些参数可以在**Pipeline**中的一个或多个步骤中引用。一旦定义了管道，你可以将其发布以将其注册到AzureML工作区，作为一个版本化对象，并可以通过**PublishedPipeline**类进行引用。此已发布的管道具有一个端点，你可以使用它来触发其执行。如果需要，你可以定义一个**Schedule**，并让这个**PublishedPipeline**类在特定时间间隔触发执行。**PipelineData**定义了临时存储，允许一个步骤将一些文件放入其中，供下一个步骤使用。这两个步骤之间的数据依赖关系在**Pipeline**中创建了一个隐式执行顺序，意味着依赖步骤将在第一个步骤完成后等待执行。你将在本章中使用所有这些类。'
- en: 'In the **azureml.pipeline.steps** module, you will find all the available steps
    you can use. The most commonly used steps are the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在**azureml.pipeline.steps**模块中，你将找到所有可用的步骤。最常用的步骤如下：
- en: '**PythonScriptStep**: This step allows you to execute a Python script. You
    will be using this step in this chapter.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PythonScriptStep**：此步骤允许你执行Python脚本。你将在本章中使用此步骤。'
- en: '`AutoMLConfig` object you saw in [*Chapter 9*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136),
    *Optimizing the ML Model*.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在[*第9章*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136)中看到的`AutoMLConfig`对象，*优化机器学习模型*。
- en: '`HyperDriveConfig` parameter you saw in [*Chapter 9*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136),
    *Optimizing the ML Model*.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在[*第9章*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136)中看到的`HyperDriveConfig`参数，*优化机器学习模型*。
- en: '**DataTransferStep**: A **Pipeline** step that allows you to transfer data
    between AzureML-supported storage options.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataTransferStep**：这是一个**Pipeline**步骤，允许你在AzureML支持的存储选项之间传输数据。'
- en: '**DatabricksStep**: This allows you to execute a DataBricks notebook, Python
    script, or JAR file in an attached DataBricks cluster.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DatabricksStep**：此步骤允许你在附加的DataBricks集群中执行DataBricks笔记本、Python脚本或JAR文件。'
- en: '`Estimator` class that represented a generic training script. Some framework-specific
    estimators inherited from that generic `Estimator` class, such as `TensorFlow`
    and `PyTorch`. To incorporate one of those estimators in your pipelines, you would
    have used an `EstimatorStep`. The whole `Estimator` class and its derivatives
    have been deprecated in favor of `ScriptRunConfig`, which you have used in the
    previous chapters. If, during the exam, you see a deprecated reference to an `EstimatorStep`,
    you can treat it as a `PythonScriptStep`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Estimator`类代表一个通用的训练脚本。一些框架特定的估算器从这个通用的`Estimator`类继承，例如`TensorFlow`和`PyTorch`。要将这些估算器之一纳入你的管道中，你会使用`EstimatorStep`。整个`Estimator`类及其派生类已被弃用，取而代之的是`ScriptRunConfig`，你在前几章中已使用过。如果在考试中看到一个已弃用的`EstimatorStep`引用，你可以将其视为`PythonScriptStep`。'
- en: The last major piece of a **Pipeline** is the data that flows through it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pipeline**的最后一个重要组成部分是流经其中的数据。'
- en: '`(dstore,"/samples/diabetes")` tuple to indicate where you wanted to store
    the data when you called the `register_pandas_dataframe` method of a `TabularDataset`.
    Instead of that tuple, you could have passed the equivalent `DataPath(datastore=dstore,
    path_on_datastore="/samples/diabetes")`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(dstore,"/samples/diabetes")` 元组用于指示在调用 `register_pandas_dataframe` 方法时想要存储数据的位置，方法适用于
    `TabularDataset`。你也可以使用等效的 `DataPath(datastore=dstore, path_on_datastore="/samples/diabetes")`
    来代替这个元组。'
- en: '`outputs` folder was automatically uploaded to a `Run` execution. Similar to
    that folder, you can define additional local folders that will be automatically
    uploaded to a target path in a target datastore. In this chapter, you will be
    using this class to store the produced model in a specific location within the
    default blob storage account.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` 文件夹会自动上传到 `Run` 执行中。与该文件夹类似，你可以定义额外的本地文件夹，这些文件夹会自动上传到目标数据存储区中的目标路径。在本章中，你将使用这个类将生成的模型存储在默认
    Blob 存储帐户中的特定位置。'
- en: '**DataReference** represents a path in a datastore and can be used to describe
    how and where data should be made available in a run. It is no longer the recommended
    approach for data referencing in AzureML. If you encounter it in an obsolete exam
    question, you can treat it as a **DataPath** object.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataReference** 表示数据存储区中的路径，并可用于描述如何以及在哪里在运行时提供数据。它不再是 AzureML 中推荐的数据引用方法。如果你在过时的考试问题中遇到它，你可以将其视为
    **DataPath** 对象。'
- en: In this section, you learned about the building blocks you can use to construct
    an AzureML **Pipeline**. In the next section, you will get some hands-on experience
    of using those classes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，你了解了可以用来构建 AzureML **Pipeline** 的构建模块。在下一节中，你将亲自体验使用这些类。
- en: Authoring a pipeline
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建管道
- en: 'Let''s assume that you need to create a repeatable workflow that has two steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要创建一个可重复的工作流，其中包含两个步骤：
- en: It loads the data from a registered dataset and splits it into training and
    test datasets. These datasets are converted into a special construct needed by
    the `step01`.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它从已注册的数据集中加载数据，并将其拆分为训练数据集和测试数据集。这些数据集被转换成 `step01` 所需的特殊结构。
- en: It loads the pre-processed data and trains a `/models/loans/` folder of the
    default datastore attached to the AzureML workspace. You will be writing the code
    for this step within a folder named `step02`.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它加载预处理过的数据，并训练存储在 AzureML 工作区的默认数据存储区 `/models/loans/` 文件夹中的模型。你将在名为 `step02`
    的文件夹中编写此步骤的代码。
- en: 'Each step will be a separate Python file, taking some arguments to specify
    where to read the data from and where to write the data to. These scripts will
    utilize the same mechanics as the scripts you authored in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*. What is different in this chapter is that instead
    of invoking each Python script separately, you will create a `Pipeline` that will
    invoke those steps one after the other. In *Figure 11.2*, you can see the overall
    inputs and outputs each script is going to have, along with the parameters you
    will need to configure for each step to execute:'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个步骤将是一个单独的 Python 文件，接受一些参数来指定从哪里读取数据以及将数据写入何处。这些脚本将利用你在 [*第 8 章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117)
    中编写的相同机制，*使用 Python 代码进行实验*。本章不同之处在于，你不会单独调用每个 Python 脚本，而是会创建一个 `Pipeline`，依次调用这些步骤。在
    *图 11.2* 中，你可以看到每个脚本将具有的整体输入和输出，以及你需要为每个步骤配置的参数，以便执行：
- en: '![Figure 11.2 – Inputs and outputs of each pipeline step'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.2 – 每个管道步骤的输入和输出'
- en: '](img/B16777_11_002.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_11_002.jpg)'
- en: Figure 11.2 – Inputs and outputs of each pipeline step
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 每个管道步骤的输入和输出
- en: Based on *Figure 11.2*, for each step, you will need to define the compute target
    and the `Environment` that will be used to execute the specific Python script.
    Although each step can have a separate compute target and a separate `Environment`
    specified, you will be running both steps using the same `Environment`, and the
    same compute target to simplify the code. You will be using the out-of-the-box
    `Environment`, which contains standard data science packages, including the **LightGBM**
    library that your scripts will require. You will be executing the steps in the
    **cpu-sm-cluster** cluster you created in [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102),
    *The AzureML Python SDK*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据*图 11.2*，对于每个步骤，你需要定义用于执行特定 Python 脚本的计算目标和`Environment`。虽然每个步骤可以指定一个单独的计算目标和单独的`Environment`，但为了简化代码，你将使用相同的`Environment`和相同的计算目标来运行这两个步骤。你将使用现成的`Environment`，它包含了标准的数据科学包，包括你的脚本所需的**LightGBM**库。你将在你在[*第7章*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102)中创建的**cpu-sm-cluster**集群上执行这些步骤，*AzureML
    Python SDK*。
- en: 'You will start by authoring the `Pipeline`, and then you will author the actual
    Python scripts required for each step. Navigate to the `chapter11` and then create
    a notebook named `chapter11.ipynb`, as seen in *Figure 11.3*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先编写`Pipeline`，然后编写每个步骤所需的实际 Python 脚本。导航到`chapter11`，然后创建一个名为`chapter11.ipynb`的笔记本，如*图
    11.3*所示：
- en: '![Figure 11.3 – Adding the chapter11 notebook to your working files'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.3 – 将 chapter11 笔记本添加到你的工作文件中'
- en: '](img/B16777_11_003.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_11_003.jpg)'
- en: Figure 11.3 – Adding the chapter11 notebook to your working files
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 将 chapter11 笔记本添加到你的工作文件中
- en: 'Open the newly created notebook and follow the steps to author an AzureML pipeline
    using the AzureML SDK:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 打开新创建的笔记本，按照步骤使用 AzureML SDK 编写 AzureML 流水线：
- en: 'You will start by getting a reference to your workspace. Then you will get
    references to the `loans` dataset and the `cpu-sm-cluster`. Add the following
    code to a cell in your notebook:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将从获取对工作区的引用开始。然后，你将获取对`loans`数据集和`cpu-sm-cluster`的引用。在你的笔记本中的一个单元格里添加以下代码：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you are having difficulties understanding this code snippet, please review
    [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102), *The AzureML Python
    SDK*.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你在理解这段代码时遇到困难，请回顾[*第7章*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102)，*AzureML
    Python SDK*。
- en: 'You will need to create a configuration object that will dictate the use of
    the `Environment` when each step gets executed. To do that, you will need to create
    a `RunConfiguration` using the following code:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要创建一个配置对象，它将在每个步骤执行时决定使用哪个`Environment`。为此，你需要使用以下代码创建一个`RunConfiguration`：
- en: '[PRE1]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You will then need to define a temporary storage folder where the first step
    will drop the output files. You will use the `PipelineData` class using the following
    code:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你需要定义一个临时存储文件夹，用于存放第一个步骤的输出文件。你将使用`PipelineData`类，采用以下代码：
- en: '[PRE2]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this code, you are creating an intermediate data location named `training_data`,
    which is stored as a folder in the default datastore that is registered in your
    AzureML workspace. You should not care about the actual path of this temporary
    data, but if you are curious, the actual path of that folder in the default storage
    container is something like `azureml/{step01_run_id}/training_data`.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这段代码中，你正在创建一个名为`training_data`的中间数据位置，它被存储为一个文件夹，位于你 AzureML 工作区中注册的默认数据存储中。你不需要关心这个临时数据的实际路径，但如果你感兴趣的话，那个文件夹在默认存储容器中的实际路径类似于`azureml/{step01_run_id}/training_data`。
- en: 'Now that you have all the prerequisites for your pipeline''s first step, it
    is time to define it. In a new cell, add the following code:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经为流水线的第一步准备好所有的前提条件，是时候定义它了。在一个新的单元格中，添加以下代码：
- en: '[PRE3]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code defines a `PythonScriptStep` that will be using the source code in
    the `step01` folder. It will execute the script named `prepare_data.py`, passing
    the following arguments:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码定义了一个`PythonScriptStep`，它将使用`step01`文件夹中的源代码。它将执行名为`prepare_data.py`的脚本，并传递以下参数：
- en: '`--dataset`: This passes the `loans_ds` dataset ID to that variable. This dataset
    ID is a unique `as_named_input` method. This method is available in both `FileDataset`
    and `TabularDataset` and is only applicable when a `Run` executes within the AzureML
    workspace. To invoke the method, you must provide a name, in this case, `loans`,
    that can be used within the script to retrieve the dataset. The AzureML SDK will
    make the `TabularDataset` object available within the `prepare_data.py` script
    in the `input_datasets` dictionary of the `run` object. Within the `prepare_data.py`
    script, you can get a reference to that dataset using the following code:'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--dataset`: 此参数将`loans_ds`数据集ID传递给该变量。此数据集ID是一个唯一的`as_named_input`方法。此方法在`FileDataset`和`TabularDataset`中均可用，并且仅适用于在AzureML工作区内执行`Run`时。要调用该方法，必须提供一个名称，本例中为`loans`，这个名称可以在脚本内部用于检索数据集。AzureML
    SDK将在`prepare_data.py`脚本中的`run`对象的`input_datasets`字典中提供`TabularDataset`对象的引用。在`prepare_data.py`脚本中，您可以使用以下代码获取对该数据集的引用：'
- en: '[PRE4]'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`--output-path`: This passes the `PipelineData` object you created in *Step
    3*. This parameter will be a string representing a path where the script can store
    its output files. The datastore location is mounted to the local storage of the
    compute node that is about to execute the specific step. This mounting path is
    passed to the script, allowing your script to transparently write the outputs
    directly to the datastore.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--output-path`: 此参数传递了您在*步骤 3*中创建的`PipelineData`对象。该参数将是一个表示脚本可以存储其输出文件的路径字符串。数据存储位置挂载到即将执行特定步骤的计算节点的本地存储上。这个挂载路径被传递给脚本，使您的脚本能够直接将输出透明地写入数据存储。'
- en: Coming back to the arguments you pass to the `PythonScriptStep` initialization,
    you define a name that will be visible in the visual representation of the pipeline
    seen in *Figure 11.6*. In the `runconfig` parameter, you pass the `RunConfiguration`
    object that you defined in *Step 2*. In the `compute_target` parameter, you pass
    the reference to the `cpu-sm-cluster` cluster that you got in *Step 1*.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回到您传递给`PythonScriptStep`初始化的参数，您定义了一个名称，该名称将在*图 11.6*中可见的管道的可视表示中显示。在`runconfig`参数中，您传递了在*步骤
    2*中定义的`RunConfiguration`对象。在`compute_target`参数中，您传递了指向您在*步骤 1*中获取的`cpu-sm-cluster`群集的引用。
- en: In the `outputs` parameter, you pass an array of outputs to which this step
    will be posting data. This is a very important parameter to define the right execution
    order of the steps within the pipeline. Although you are passing the `PipelineData`
    object as an argument to the script, the AzureML SDK is not aware of whether your
    script will be writing or reading data from that location. By explicitly adding
    the `PipelineData` object to the `outputs` parameter, you mark this step as a
    producer of the data stored in the `PipelineData` object. Thus, anyone referencing
    the same object in the corresponding `inputs` parameter will need to execute after
    this `PythonScriptStep`.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在`outputs`参数中，您传递了一个输出数组，该数组将用于此步骤将要发布的数据。这是一个非常重要的参数，用于定义管道中步骤的正确执行顺序。尽管您将`PipelineData`对象作为参数传递给脚本，但AzureML
    SDK不知道您的脚本是否将从该位置写入或读取数据。通过显式将`PipelineData`对象添加到`outputs`参数中，您将此步骤标记为数据存储在`PipelineData`对象中的生产者。因此，任何在相应`inputs`参数中引用相同对象的人都需要在此`PythonScriptStep`之后执行。
- en: The `allow_reuse` Boolean parameter allows you to reuse the outputs of this
    `PythonScriptStep` if the inputs of the script and the source code within the
    `step01` folder haven't changed since the last execution of the pipeline. Since
    the only input of this step is a specific version of the `TabularDataset`, it
    cannot change. Although you did not specify a particular version when you referenced
    the `TabularDataset`, the latest version was automatically selected. This version
    was pinned to the pipeline's definition at creation time. The pipeline will keep
    executing on the pinned version, even if you create a new version of the `TabularDataset`.
    Moreover, since the `allow_reuse` parameter is set to `True`, this step will run
    only once, and from there on, the results will be automatically reused. At the
    end of this section, you will see how this affects the pipeline execution time
    when you rerun the same pipeline.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`allow_reuse`布尔参数允许你在脚本的输入和`step01`文件夹中的源代码自上次执行管道以来没有更改的情况下重用此`PythonScriptStep`的输出。由于此步骤的唯一输入是特定版本的`TabularDataset`，它是不能更改的。尽管你在引用`TabularDataset`时未指定特定版本，但系统会自动选择最新版本。此版本在管道创建时被固定到管道定义中。即使你创建了`TabularDataset`的新版本，管道也会继续在固定的版本上执行。此外，由于`allow_reuse`参数设置为`True`，此步骤将只执行一次，从那时起，结果将自动重用。在本节的末尾，你将看到这如何影响重新运行相同管道时的管道执行时间。'
- en: Important note
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you wanted to force the pipeline to read the latest version of the `loans_ds`
    variable would reference the latest version of the `TabularDataset`. At the end
    of this section, you will also learn how you can pass the training dataset as
    a `PipelineParameter`.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想强制管道读取`loans_ds`变量的最新版本，它将引用`TabularDataset`的最新版本。在本节的末尾，你还将学习如何将训练数据集作为`PipelineParameter`传递。
- en: Now that you have defined the `PythonScriptStep`, it is time to add the missing
    Python script to your files. Next to your notebook, under the `chapter11` folder
    you are currently working on, add a new folder named `step01`. Within that folder,
    add a new Python script file named `prepare_data.py`. The final folder structure
    should be similar to the one shown in *Figure 11.4*:![Figure 11.4 – Folder structure
    for the prepare_data.py script that will be executed in your pipeline
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经定义了`PythonScriptStep`，是时候将缺失的Python脚本添加到你的文件中了。在你当前工作的`chapter11`文件夹下，与你的笔记本并列，添加一个名为`step01`的新文件夹。在该文件夹内，添加一个名为`prepare_data.py`的新Python脚本文件。最终的文件夹结构应类似于*图11.4*所示：![图
    11.4 – 准备数据脚本 prepare_data.py 的文件夹结构，该脚本将在你的管道中执行
- en: '](img/B16777_11_004.jpg)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16777_11_004.jpg)'
- en: Figure 11.4 – Folder structure for the prepare_data.py script that will be executed
    in your pipeline
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.4 – 准备数据脚本 prepare_data.py 的文件夹结构，该脚本将在你的管道中执行
- en: 'Add the following code blocks within the `prepare_data.py` file. Instead of
    typing all this code, you can download it directly from the GitHub repository
    mentioned in the *Technical requirements* section of this chapter:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`prepare_data.py`文件中添加以下代码块。你可以直接从本章的*技术要求*部分提到的GitHub仓库下载这些代码，而无需手动输入所有代码：
- en: '[PRE5]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These are all the imports you will need within the script file. You are going
    to create the training and test datasets using the `train_test_split` method for
    the `lightgbm` library with the `lgb` short alias:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些是你在脚本文件中需要的所有导入。你将使用`train_test_split`方法为`lightgbm`库（使用`lgb`作为短别名）创建训练和测试数据集：
- en: '[PRE6]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This script creates an `ArgumentParser` that parses the arguments you passed
    when you defined the `PythonScriptStep` in *Step 4*. As a reminder, the `--dataset`
    parameter is going to contain the dataset ID that the script will need to process,
    and the `--output-path` parameter will be the local path location where the script
    is supposed to write the transformed datasets:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该脚本创建了一个`ArgumentParser`，用于解析你在*步骤4*中定义`PythonScriptStep`时传递的参数。提醒一下，`--dataset`参数将包含脚本需要处理的数据集ID，而`--output-path`参数将是脚本应该写入转换后的数据集的本地路径位置：
- en: '[PRE7]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Right after parsing the arguments, you are getting a reference to the `Run`
    context. From there, you get a reference to the `loans` dataset, something that
    becomes available to you because you called the `as_named_input` method as discussed
    in *Step 4*. Later in this section, you will read about how you could have rewritten
    this code block to be able to run the same script in your local computer without
    a `Run` context:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在解析完参数后，你会获得对`Run`上下文的引用。之后，你会得到对`loans`数据集的引用，这一点是因为你调用了`as_named_input`方法，正如*第4步*中所讨论的那样。在本节后续部分，你将了解如何重写这段代码，以便能够在没有`Run`上下文的本地计算机上运行相同的脚本：
- en: '[PRE8]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This code block prints the ID of the dataset that your code picked as a reference.
    If you print the ID passed to the `--dataset` parameter, and which is stored in
    the `args.dataset` variable, you will notice that these two values are identical:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码打印了你的代码选择为参考的数据集的ID。如果你打印传递给`--dataset`参数的ID，并且它存储在`args.dataset`变量中，你会注意到这两个值是相同的：
- en: '[PRE9]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this code block, you load the dataset into memory and use the `train_test_split`
    method to split the dataset into training and test features (`x_train` and `x_test`)
    and training and test labels (`y_train` and `y_test`):'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这段代码中，你将数据集加载到内存中，并使用`train_test_split`方法将数据集拆分为训练和测试特征（`x_train`和`x_test`）以及训练和测试标签（`y_train`和`y_test`）：
- en: '[PRE10]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The features and the labels are then converted into `train_data` and `test_data`,
    which are `Dataset` objects. `Dataset` format for training and validation. Note
    that the validation dataset stored in the `test_data` variable needs to reference
    the training data (`train_data`). This is a failsafe mechanism embedded by `output_path`
    folder, if it doesn't already exist, and then use the native `save_binary` method
    of the `Dataset` to serialize the dataset into a binary file that is optimized
    for storing and loading.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征和标签随后被转换为`train_data`和`test_data`，它们是`Dataset`对象。用于训练和验证的`Dataset`格式。请注意，存储在`test_data`变量中的验证数据集需要引用训练数据集（`train_data`）。这是由`output_path`文件夹嵌入的一个安全机制，如果文件夹不存在，它会先创建，然后使用`Dataset`的原生`save_binary`方法将数据集序列化为二进制文件，优化用于存储和加载。
- en: 'In contrast to the scripts you created in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*, the `prepare_data.py` file cannot execute on
    your local computer as an `_OfflineRun`. This is because you have a dependency
    on the `input_datasets` dictionary that is only available if the `Run` is executing
    within the AzureML workspace. If you wanted to test this file locally before using
    it within the `Pipeline`, you could use the following code instead:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与在[*第8章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117)中创建的脚本不同，*使用Python代码实验*，`prepare_data.py`文件无法作为`_OfflineRun`在本地计算机上执行。这是因为你依赖于`input_datasets`字典，而这个字典只有在`Run`在AzureML工作区内执行时才可用。如果你想在将该文件用于`Pipeline`之前在本地测试它，你可以使用以下代码代替：
- en: '[PRE11]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This code checks whether this is an offline run. In that case, it first gets
    a reference to the workspace as you saw in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*, and then it checks whether the `--dataset` parameter
    stored in the `args.dataset` variable is a dataset name. If it is, the latest
    version of the dataset is assigned to the `loans_dataset` variable. If it is not
    a name, the script assumes it is a GUID, which should represent the ID of a specific
    dataset version. In that case, the script tries the `get_by_id` method to retrieve
    the specific dataset or throw an error if the value passed is not a known dataset
    ID. If the run is online, you can still use the `input_datasets` dictionary to
    retrieve the dataset reference.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码检查是否是离线运行。如果是，它首先获取对工作区的引用，正如你在[*第8章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117)中所看到的那样，*使用Python代码实验*，然后检查存储在`args.dataset`变量中的`--dataset`参数是否是数据集名称。如果是，最新版本的数据集会被分配给`loans_dataset`变量。如果不是名称，脚本会认为它是GUID，这应该表示特定数据集版本的ID。在这种情况下，脚本会尝试使用`get_by_id`方法来检索特定数据集，或者如果传递的值不是已知的数据集ID，则抛出错误。如果是在线运行，你仍然可以使用`input_datasets`字典来检索数据集引用。
- en: 'Back to your notebook, you will start defining the prerequisites for the second
    step, the model training phase of your `Pipeline`. In *Figure 11.2*, you saw that
    this step requires a parameter named `learning_rate`. Instead of hardcoding the
    learning rate hyperparameter of the `PipelineParameter` to pass in this value.
    This parameter will be defined at `Pipeline` level, and it will be passed to the
    training script as an argument, as you will see in *Step 9*. To create such a
    parameter, use the following code:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到你的笔记本，你将开始定义第二步的前提条件，即`Pipeline`的模型训练阶段。在*图11.2*中，你看到此步骤需要一个名为`learning_rate`的参数。你将不再将`PipelineParameter`的学习率超参数硬编码到代码中，而是将在`Pipeline`级别定义此参数，并将其作为参数传递给训练脚本，正如你在*步骤9*中将看到的那样。要创建这样的参数，请使用以下代码：
- en: '[PRE12]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This code defines a new `PipelineParameter` named `learning_rate`. The default
    value will be `0.05`, meaning that you can omit to pass this parameter when you
    execute the pipeline, and this default value will be used. You will see later
    in *Step 13* how you can execute the `Pipeline` and specify a value other than
    the default.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码定义了一个名为`learning_rate`的新`PipelineParameter`。默认值为`0.05`，这意味着在执行管道时可以省略传递此参数，系统将使用默认值。稍后在*步骤13*中，你将看到如何执行`Pipeline`并指定一个不同于默认值的值。
- en: 'You will store the trained model in the `/models/loans/` folder of the default
    datastore attached to the AzureML workspace. To specify the exact location where
    you want to store the files, you will use the `OutputFileDatasetConfig` class.
    In a new notebook cell, add the following code:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将把训练好的模型存储在附加到AzureML工作区的默认数据存储的`/models/loans/`文件夹中。要指定你希望存储文件的确切位置，你将使用`OutputFileDatasetConfig`类。在一个新的笔记本单元格中，添加以下代码：
- en: '[PRE13]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this script, you are getting a reference to the default datastore. Then,
    you create an `OutputFileDatasetConfig` object, passing a tuple to the `destination`
    parameter. This tuple consists of the datastore you selected and the path within
    that datastore. You could have selected any datastore you have attached in the
    AzureML workspace. This `OutputFileDatasetConfig` object defines the destination
    to copy the outputs to. If you don't specify the `destination` argument, the default
    `/dataset/{run-id}/{output-name}` value is used. Note that `destination` allows
    you to use placeholders while defining the path. The default value uses both the
    `{run-id}` and `{output-name}` placeholders that are currently supported. These
    placeholders will be replaced with the corresponding values at the appropriate
    time.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在此脚本中，你首先获取对默认数据存储的引用。然后，创建一个`OutputFileDatasetConfig`对象，并将一个元组传递给`destination`参数。这个元组包含了你选择的数据存储和该数据存储中的路径。你本可以选择任何已附加到AzureML工作区的数据存储。这个`OutputFileDatasetConfig`对象定义了输出的目标位置。如果你没有指定`destination`参数，系统将使用默认的`/dataset/{run-id}/{output-name}`值。请注意，`destination`允许在定义路径时使用占位符。默认值使用了`{run-id}`和`{output-name}`两个当前支持的占位符。到适当的时刻，这些占位符将被相应的值替换。
- en: 'Now that you have all the prerequisites defined, you can define the second
    step of your `Pipeline`. In a new cell in your notebook, add the following code:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经定义了所有前提条件，可以定义`Pipeline`的第二步了。在笔记本的新单元格中，添加以下代码：
- en: '[PRE14]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Similar to the `step_01` folder you created in *Step 4*; this code defines a
    `PythonScriptStep` that will invoke the `train_model.py` script located in the
    `step02` folder. It will populate the `--learning-rate` argument using the value
    passed to the `PipelineParameter` you defined in *Step 7*. It will also pass the
    output of `step_01` to the `--input-path` argument. Note that `step01_output`
    is also added to the list of inputs of this `PythonScriptStep`. This forces `step_02`
    to wait for `step_01` to complete in order to consume the data stored in `step01_output`.
    The last script argument is `--output-path`, where you pass the `OutputFileDatasetConfig`
    object you created in the previous step. This object is also added to the list
    of outputs of this `PythonScriptStep`.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类似于你在*步骤4*中创建的`step_01`文件夹；这段代码定义了一个`PythonScriptStep`，它将调用位于`step02`文件夹中的`train_model.py`脚本。它将使用你在*步骤7*中定义的`PipelineParameter`传递的值来填充`--learning-rate`参数。它还会将`step_01`的输出传递给`--input-path`参数。请注意，`step01_output`也被添加到此`PythonScriptStep`的输入列表中。这意味着`step_02`必须等待`step_01`完成，以便消费存储在`step01_output`中的数据。最后一个脚本参数是`--output-path`，你将在这里传递你在上一步创建的`OutputFileDatasetConfig`对象。此对象也被添加到此`PythonScriptStep`的输出列表中。
- en: Let's create the Python script that will be executed by `step_02`. Next to your
    notebook, under the `chapter11` folder you are currently working on, add a new
    folder named `step02`. Within that folder, add a new Python script file named
    `train_model.py`. The final folder structure should be similar to the one shown
    in *Figure 11.5*:![Figure 11.5 – The training script that will be executed in
    the step02 folder of your pipeline
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个Python脚本，该脚本将在`step_02`中执行。在你的笔记本旁边，在你当前工作的`chapter11`文件夹下，创建一个名为`step02`的新文件夹。在该文件夹内，创建一个名为`train_model.py`的Python脚本文件。最终的文件夹结构应该类似于*图11.5*所示：![图11.5
    – 将在管道的step02文件夹中执行的训练脚本
- en: '](img/B16777_11_005.jpg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16777_11_005.jpg)'
- en: Figure 11.5 – The training script that will be executed in the step02 folder
    of your pipeline
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.5 – 将在管道的step02文件夹中执行的训练脚本
- en: 'Open the `train_model.py` file and add the following code blocks to it. Instead
    of typing all this code, you can download it directly from the GitHub repository
    mentioned in the *Technical requirements* section of this chapter:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`train_model.py`文件并添加以下代码块。你可以直接从本章*技术要求*部分提到的GitHub仓库下载这些代码，而不是手动输入。
- en: '[PRE15]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This block imports all modules you will need in the file and creates an `ArgumentParser`
    to read the arguments you will be passing to this script. If you so wished, you
    could have used another famous library for script parameter parsing called `learning_rate`
    argument. Note that this is a float with a default value different from the value
    you defined in *Step 7*, an example that shows that those two default values do
    not need to be the same. When executing the pipeline, `PipelineParameter` will
    be the one defining the actual value:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码导入了文件中需要的所有模块，并创建了一个`ArgumentParser`来读取你将传递给该脚本的参数。如果你愿意，你也可以使用另一个著名的库来解析脚本参数，称为`learning_rate`参数。请注意，这是一个浮动值，默认值与你在*步骤7*中定义的值不同，这表明这两个默认值不需要相同。在执行管道时，`PipelineParameter`将定义实际值：
- en: '[PRE16]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You then parse `input_path` and `output_path`, which are string values that
    point to local folders within the compute where this script is executing. The
    last line parses the incoming arguments and assigns the results to the `args`
    variable:'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后你解析`input_path`和`output_path`，它们是指向执行此脚本的计算机上本地文件夹的字符串值。最后一行解析传入的参数，并将结果分配给`args`变量：
- en: '[PRE17]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After parsing the script arguments, the training and validation datasets are
    loaded:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解析脚本参数后，训练和验证数据集被加载：
- en: '[PRE18]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this code block, a binary classification training process is configured
    that will use the `auc`) metric to evaluate the training progression. `early_stopping_rounds`
    parameter:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这段代码块中，配置了一个二分类训练过程，它将使用`auc`指标来评估训练进度。`early_stopping_rounds`参数：
- en: '[PRE19]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Once the training is complete, the model is serialized using the `joblib` library
    and stored in the `output_path` folder.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练完成后，模型会使用`joblib`库进行序列化，并存储在`output_path`文件夹中。
- en: 'Back to the notebook, it''s time you defined the actual `Pipeline` you have
    been building so far. In a new cell, add the following code:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到笔记本，是时候定义你目前为止构建的实际`Pipeline`了。在一个新的单元格中，添加以下代码：
- en: '[PRE20]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You define a new `Pipeline` object, passing in a list with all the steps you
    want to include. Note that the order of the steps is not important since the real
    execution order is defined by the `step01_output` `PipelineData` dependency you
    specified between those two steps.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你定义一个新的`Pipeline`对象，传入包含你要包括的所有步骤的列表。请注意，步骤的顺序并不重要，因为实际的执行顺序是由你在这两个步骤之间指定的`step01_output`
    `PipelineData`依赖关系定义的。
- en: 'To execute the pipeline, you will need to submit it in an `Experiment`. In
    a new notebook cell, add the following code:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要执行管道，你需要将其提交到`Experiment`中。在一个新的笔记本单元格中，添加以下代码：
- en: '[PRE21]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This code defines a new `Experiment` named `chapter-11-runs` and submits the
    pipeline to run, passing the value of `0.5` to the `learning_rate` parameter you
    defined in *Step 7*.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码定义了一个新的`Experiment`，名为`chapter-11-runs`，并提交管道运行，将`0.5`的值传递给你在*步骤7*中定义的`learning_rate`参数。
- en: 'One of the first outputs of the pipeline execution is the link to the AzureML
    portal. Clicking on that link will get you to the pipeline execution run, as seen
    in *Figure 11.6*:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管道执行的第一个输出之一是指向AzureML门户的链接。点击该链接将进入管道执行运行页面，如*图11.6*所示：
- en: '![Figure 11.6 – A graphical representation of the pipeline you authored in
    this section'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.6 – 你在本节中创建的管道的图形表示'
- en: '](img/B16777_11_006.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_11_006.jpg)'
- en: Figure 11.6 – A graphical representation of the pipeline you authored in this
    section
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – 本节中你所编写的管道的图形表示
- en: Suppose you try to rerun the pipeline by executing the code you wrote in *Step
    13* for a second time. In that case, you will notice that the execution will be
    almost instant (just a few seconds compared to the minute-long execution you should
    have seen the first time). The pipeline detected that no input had changed and
    reused the outputs of the previously executed steps. This demonstrates what the
    `allow_reuse=True` in *Step 4* does, and it also proves that even though we didn't
    specify that parameter in *Step 9*, the default value is `True`. This means that,
    by default, all steps will reuse previous executions if the inputs and the code
    files are the same as the ones of an earlier execution. If you want to force a
    retrain even if the same `learning_rate` variable is passed to the pipeline, you
    can specify `allow_reuse=False` in *Step 9*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你尝试通过第二次执行你在*第13步*中编写的代码来重新运行管道。在这种情况下，你会注意到执行几乎是即时的（相比第一次执行需要的几分钟，这次只需要几秒钟）。管道检测到没有输入发生变化，并重新使用了先前执行步骤的输出。这展示了*第4步*中`allow_reuse=True`的作用，并且证明了即使我们在*第9步*中没有指定该参数，默认值为`True`。这意味着，默认情况下，如果输入和代码文件与之前的执行相同，所有步骤都会重用先前的执行结果。如果你想强制重新训练，即使传递给管道的`learning_rate`变量相同，你可以在*第9步*中指定`allow_reuse=False`。
- en: Important note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'If you wanted to pass the training dataset as a `PipelineParameter`, you would
    have to use the following code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将训练数据集作为`PipelineParameter`传递，你需要使用以下代码：
- en: '`from azureml.data.dataset_consumption_config import DatasetConsumptionConfig`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`from azureml.data.dataset_consumption_config import DatasetConsumptionConfig`'
- en: '`ds_pipeline_param = PipelineParameter(name="dataset ", default_value=loans_ds)`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`ds_pipeline_param = PipelineParameter(name="dataset ", default_value=loans_ds)`'
- en: '`dataset_consumption = DatasetConsumptionConfig("loans", ds_pipeline_param)`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset_consumption = DatasetConsumptionConfig("loans", ds_pipeline_param)`'
- en: Using this code and passing the `dataset_consumption` object in *Step 4* instead
    of `loans_ds.as_named_input('loans')` would allow you to select the input dataset
    and its version while submitting a pipeline to execute.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此代码，并在*第4步*中传递`dataset_consumption`对象，而不是`loans_ds.as_named_input('loans')`，将允许你在提交管道执行时选择输入数据集及其版本。
- en: So far, you have defined a pipeline that executes two Python scripts. `step_01`
    pre-processes the training data and stores it in an intermediate data store for
    `step_02` to pick up. From there, the second step trains a `/models/loans/` folder
    of the default datastore attached to the AzureML workspace. If you have followed
    the steps accurately, the pipeline will have been completed successfully. In real
    life, though, coding issues creep in, and your pipeline may fail to complete.
    In the next section, you will learn how to troubleshoot potential pipeline runtime
    issues.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经定义了一个执行两个Python脚本的管道。`step_01`预处理训练数据并将其存储在一个中间数据存储中，供`step_02`使用。之后，第二步将在附加到AzureML工作区的默认数据存储的`/models/loans/`文件夹中训练模型。如果你准确地按照步骤操作，管道应该会成功完成。然而，在实际操作中，编码问题常常会出现，管道可能会未能完成。在下一节中，你将学习如何排查潜在的管道运行时问题。
- en: Troubleshooting code issues
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排查代码问题
- en: 'So far, your code has worked like a charm. What happens if a script has a coding
    issue or if a dependency is missing? In that case, your pipeline will fail. In
    the graphical representation you saw in *Figure 11.6*, you will be able to identify
    the failing step. If you want to get the details of a specific child step, you
    will have to first locate it using `find_step_run` of the `pipeline_run` object
    you got when you executed the pipeline. In a new cell within your notebook, add
    the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你的代码运行得非常顺利。如果某个脚本出现了编码问题或缺少依赖项怎么办？在这种情况下，管道将会失败。在你在*图11.6*中看到的图形表示中，你将能够识别出失败的步骤。如果你想获取特定子步骤的详细信息，你必须首先使用`pipeline_run`对象中的`find_step_run`来定位它。你可以在笔记本中的新单元格中添加以下代码：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This code finds all steps with the name `0` index. This retrieves a `StepRun`
    object, which is for the `step_02` folder you defined in the previous section.
    `StepRun` inherits from the base `Run` class, exposing the `get_details_with_logs`
    method that is also available in the `ScriptRun` class you were using in [*Chapter
    8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117), *Experimenting with Python
    Code*. This method is handy in troubleshooting potential issues with your dependencies
    or your script code. It produces a lot of helpful information regarding the execution
    of the script, including the log files.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码查找所有名称为 `0` 索引的步骤。这将检索一个 `StepRun` 对象，该对象用于你在上一节中定义的 `step_02` 文件夹。`StepRun`
    继承自基类 `Run`，暴露了 `get_details_with_logs` 方法，该方法也可在你在 [*第 8 章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117)
    中使用的 `ScriptRun` 类中找到，*使用 Python 代码进行实验*。这个方法在排查依赖关系或脚本代码的潜在问题时非常有用。它提供了关于脚本执行的很多有用信息，包括日志文件。
- en: 'If you prefer the AzureML studio web experience, you can navigate to the `Pipeline`
    run. In the graphical representation of the pipeline, select the step you want
    to see the logs for. View the logs in the **Outputs + logs** tab, as shown in
    *Figure 11.7*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更喜欢 AzureML Studio Web 体验，可以导航到 `Pipeline` 运行。在管道的图形表示中，选择你想查看日志的步骤。在 **输出
    + 日志** 标签页中查看日志，如 *图 11.7* 所示：
- en: '![Figure 11.7 – Viewing the logs of the Train model step in the web portal'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.7 – 在 Web 门户中查看训练模型步骤的日志'
- en: '](img/B16777_11_007.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_11_007.jpg)'
- en: Figure 11.7 – Viewing the logs of the Train model step in the web portal
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 在 Web 门户中查看训练模型步骤的日志
- en: So far, you have learned how to author a `Pipeline` and how to troubleshoot
    potential runtime errors. The `Pipeline` you created is not yet registered within
    your workspace, something you will do in the next section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了如何编写 `Pipeline` 和如何排查潜在的运行时错误。你创建的 `Pipeline` 尚未在工作区中注册，接下来你将在下一节进行注册。
- en: Publishing a pipeline to expose it as an endpoint
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将管道发布为端点
- en: So far, you have defined a pipeline using the AzureML SDK. If you had to restart
    the kernel of your Jupyter notebook, you would lose the reference to the pipeline
    you defined, and you would have to rerun all the cells to recreate the pipeline
    object. The AzureML SDK allows you to publish a pipeline that effectively registers
    it as a versioned object within the workspace. Once a pipeline is published, it
    can be submitted without the Python code that constructed it.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经使用 AzureML SDK 定义了一个管道。如果你需要重新启动 Jupyter notebook 的内核，你将失去对已定义管道的引用，并且你需要重新运行所有单元格以重新创建管道对象。AzureML
    SDK 允许你发布管道，从而有效地将其注册为工作区内的版本化对象。一旦管道被发布，就可以在不需要构建它的 Python 代码的情况下提交该管道。
- en: 'In a new cell in your notebook, add the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在 notebook 中的新单元格中，添加以下代码：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code publishes the pipeline and returns a `PublishedPipeline` object, the
    versioned object registered within the workspace. The most interesting attribute
    of that object is the `endpoint`, which returns the REST endpoint URL to trigger
    the execution of the specific pipeline.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码发布管道并返回一个 `PublishedPipeline` 对象，即在工作区中注册的版本化对象。该对象最有趣的属性是 `endpoint`，它返回用于触发特定管道执行的
    REST 端点 URL。
- en: 'To invoke the published pipeline, you will need an authentication header. To
    acquire this security header, you can use the `InteractiveLoginAuthentication`
    class, as seen in the following code snippet:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用已发布的管道，你将需要一个认证头。要获取此安全头，你可以使用 `InteractiveLoginAuthentication` 类，如以下代码片段所示：
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then you can use the Python `requests` package to make a `POST` request to
    the specific endpoint using the following code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用 Python `requests` 包向特定端点发出 `POST` 请求，使用以下代码：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This code only needs the URL and not the actual pipeline code. If you ever lose
    the endpoint URL, you can retrieve it by code through the `list` method of the
    `PublishedPipeline` class, which enumerates all the published pipelines registered
    in the workspace. The preceding script invokes the `REST` endpoint using the HTTP
    POST verb and passing the value `0.02` as the `learning_rate` parameter.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码只需要 URL，而不需要实际的管道代码。如果你丢失了端点 URL，可以通过 `PublishedPipeline` 类的 `list` 方法恢复它，该方法列举了工作区中所有已发布的管道。前面的脚本使用
    HTTP POST 方法调用 `REST` 端点，并传递值 `0.02` 作为 `learning_rate` 参数。
- en: Important note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: If you are unfamiliar with the `POST` method, also referred to as a verb, you
    can learn more in the *Further reading* section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉 `POST` 方法（也称为动词），可以在 *进一步阅读* 部分了解更多信息。
- en: The resulting object from this HTTP request contains information about the execution
    of the pipeline, including `RunUrl`, which allows you to visit the AzureML studio
    portal to monitor the pipeline execution.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 HTTP 请求的结果对象包含有关管道执行的信息，包括`RunUrl`，它允许你访问 AzureML Studio 门户以监控管道执行。
- en: 'When you publish the pipeline, the registered object becomes available in the
    AzureML studio portal. If you navigate to **Endpoints** | **Pipeline endpoints**,
    you will find a list of all your published pipeline endpoints, as seen in *Figure
    11.8*:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当你发布管道时，注册的对象会在 AzureML Studio 门户中可用。如果你导航到**终端** | **管道终端**，你将看到所有已发布管道终端的列表，如*图
    11.8*所示：
- en: '![Figure 11.8 – The published pipeline endpoint'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.8 – 已发布管道的终端'
- en: '](img/B16777_11_008.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_11_008.jpg)'
- en: Figure 11.8 – The published pipeline endpoint
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – 已发布管道的终端
- en: Once you select a pipeline, you can trigger it using a graphical wizard that
    allows you to specify the pipeline parameters and the experiment under which the
    pipeline will execute.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你选择了一个管道，你可以通过图形向导来触发它，向导允许你指定管道参数和管道将要执行的实验。
- en: In this section, you saw how you can publish a pipeline to be able to reuse
    it without having the pipeline definition code. You saw how you can trigger the
    registered pipeline using the `REST` endpoint. In the next section, you will learn
    how to schedule the pipeline to schedule monthly retraining.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你了解了如何发布管道，以便在不需要管道定义代码的情况下重用它。你看到了如何通过`REST`终端触发已注册的管道。在下一节中，你将学习如何调度管道以进行每月的再训练。
- en: Scheduling a recurring pipeline
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度一个重复的管道
- en: 'Being able to invoke a pipeline through the published `REST` endpoint is great
    when you have third-party systems that need to invoke a training process after
    a specific event has occurred. For example, suppose you are using **Azure Data
    Factory** to copy data from your on-premises databases. You could use the **Machine
    Learning Execute Pipeline** activity and trigger a published pipeline, as shown
    in *Figure 11.9*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过已发布的`REST`终端调用管道是非常棒的，尤其是在你有第三方系统需要在特定事件发生后触发训练过程时。例如，假设你正在使用**Azure Data
    Factory**从本地数据库复制数据。你可以使用**Machine Learning Execute Pipeline**活动，触发一个已发布的管道，如*图
    11.9*所示：
- en: '![Figure 11.9 – Sample Azure Data Factory pipeline triggering an AzureML'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.9 – 示例 Azure Data Factory 管道触发一个 AzureML'
- en: published pipeline following a copy activity
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随复制活动的已发布管道
- en: '](img/B16777_11_009.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_11_009.jpg)'
- en: Figure 11.9 – Sample Azure Data Factory pipeline triggering an AzureML published
    pipeline following a copy activity
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – 示例 Azure Data Factory 管道触发一个跟随复制活动的 AzureML 已发布管道
- en: 'If you wanted to schedule the pipeline to be triggered monthly, you would need
    to publish the pipeline as you did in the previous section, get the published
    pipeline ID, create a `ScheduleRecurrence`, and then create the `Schedule`. Return
    to your notebook where you already have a reference to `published_pipeline`. Add
    a new cell with the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想调度管道按月触发，你需要像前一节那样发布管道，获取已发布管道的 ID，创建一个`ScheduleRecurrence`，然后创建`Schedule`。返回到你的笔记本，在那里你已经有了`published_pipeline`的引用。添加一个新单元格，并输入以下代码：
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this code, you define a `ScheduleRecurrence` with monthly frequency. By specifying
    the `start_time = datetime.now()`, you are preventing the immediate execution
    of the pipeline, which is the default behavior when creating a new `Schedule`.
    Once you have the recurrence you want to use, you can schedule the pipeline execution
    by calling the `create` method of the `Schedule` class. You are passing in the
    ID of the `published_pipeline` you want to trigger, and you specify the experiment
    name under which each execution will occur.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，你定义了一个具有每月频率的`ScheduleRecurrence`。通过指定`start_time = datetime.now()`，你防止了管道的立即执行，这是创建新`Schedule`时的默认行为。一旦你设置了想要的重复模式，可以通过调用`Schedule`类的`create`方法来调度管道执行。你将传入你想触发的`published_pipeline`的ID，并指定每次执行时所使用的实验名称。
- en: Important note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Scheduling the execution of the specific pipeline doesn't make any sense as
    no additional training will ever happen since both steps have `allow_reuse=True`.
    If you wanted to retrain every month, you would probably want this setting to
    be `False` and force the execution of both steps when the pipeline schedule was
    invoked. Moreover, in a scheduled pipeline, it is common that the very first step
    fetches new data from various sources attached to the AzureML workspace and then
    transforms the data and trains the model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 安排特定流水线的执行没有任何意义，因为由于两个步骤都设置了 `allow_reuse=True`，将不会发生任何额外的训练。如果你希望每个月都重新训练，你可能希望将这个设置改为
    `False`，并强制在触发流水线调度时执行两个步骤。此外，在一个定期执行的流水线中，通常第一个步骤会从附加到 AzureML 工作区的多个来源获取新数据，然后转换数据并训练模型。
- en: 'If you want to disable a scheduled execution, you can use the `disable` method
    of the `Schedule` class. The following code disables all scheduled pipelines in
    your workspace:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想禁用定期执行，你可以使用 `Schedule` 类的 `disable` 方法。以下代码会禁用工作区中的所有定期流水线：
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This code lists all active schedules within the workspace and then disables
    them one by one. Make sure you don't accidentally disable a pipeline that should
    have been scheduled in your workspace.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码列出了工作区中所有活动的调度，并逐个禁用它们。确保你不会意外禁用本应在工作区中定期执行的流水线。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how you can define AzureML pipelines using the
    AzureML SDK. These pipelines allow you to orchestrate various steps in a repeatable
    manner. You started by defining a training pipeline consisting of two steps. You
    then learned how to trigger the pipeline and how to troubleshoot potential code
    issues. Then you published the pipeline to register it within the AzureML workspace
    and acquire an HTTP endpoint that third-party software systems could use to trigger
    pipeline executions. In the last section, you learned how to schedule the recurrence
    of a published pipeline.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用 AzureML SDK 定义 AzureML 流水线。这些流水线允许你以可重复的方式协调各种步骤。你首先定义了一个包含两个步骤的训练流水线。接着你学习了如何触发流水线以及如何排查潜在的代码问题。然后你将流水线发布到
    AzureML 工作区注册，并获取了一个 HTTP 端点，第三方软件系统可以用来触发流水线执行。在最后一节中，你学习了如何安排已发布流水线的定期执行。
- en: In the next chapter, you will learn how to operationalize the models you have
    been training so far in the book. Within that context, you will use the knowledge
    you acquired in this chapter to author batch inference pipelines, something that
    you can publish and trigger with HTTP or have it scheduled, as you learned in
    this chapter.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何将你迄今为止在书中训练的模型投入生产。你将利用本章所学的知识编写批量推理流水线，这些流水线可以发布并通过 HTTP 触发，或者像本章所学的那样安排执行。
- en: Questions
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: In each chapter, you will find a couple of questions to validate your understanding
    of the topics discussed in this chapter.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一章中，你将找到一些问题来验证你对本章讨论主题的理解。
- en: What affects the execution order of the pipeline steps?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么因素会影响流水线步骤的执行顺序？
- en: a. The order in which the steps were defined when constructing the `Pipeline`
    object.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 在构建 `Pipeline` 对象时定义步骤的顺序。
- en: b. The data dependencies between the steps.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 步骤之间的数据依赖关系。
- en: c. All steps execute in parallel, and you cannot affect the execution order.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 所有步骤并行执行，且你无法影响执行顺序。
- en: 'True or false: All steps within a pipeline need to execute within the same
    compute target and `Environment`.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断对错：流水线中的所有步骤都需要在同一个计算目标和 `Environment` 中执行。
- en: 'True or false: `PythonScriptStep`, by default, reuses the previous execution
    results if nothing has changed in the parameters or the code files.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断对错：`PythonScriptStep` 默认情况下会重新使用之前执行的结果，如果参数或代码文件没有发生变化。
- en: You are trying to debug a child run execution issue. Which of the following
    methods should you call in the `StepRun` object?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在尝试调试一个子任务运行执行问题。你应该在 `StepRun` 对象中调用以下哪个方法？
- en: a. `get_file_names`
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. `get_file_names`
- en: b. `get_details_with_logs`
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. `get_details_with_logs`
- en: c. `get_metrics`
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. `get_metrics`
- en: d. `get_details`
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. `get_details`
- en: You have just defined a pipeline in Python code. What steps do you need to make
    to schedule a daily execution of that pipeline?
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你刚刚在 Python 代码中定义了一个流水线。你需要做哪些步骤来安排该流水线的每日执行？
- en: Further reading
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: This section offers a list of helpful web resources to help you augment your
    knowledge of the AzureML SDK and the various code snippets used in this chapter.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一些有用的网络资源，以帮助你扩展对 AzureML SDK 和本章中使用的各种代码片段的知识。
- en: 'Documentation regarding the **LightGBM** framework used in this chapter: [https://lightgbm.readthedocs.io](https://lightgbm.readthedocs.io)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章节中使用的**LightGBM**框架的文档：[https://lightgbm.readthedocs.io](https://lightgbm.readthedocs.io)
- en: 'HTTP request methods: [https://www.w3schools.com/tags/ref_httpmethods.asp](https://www.w3schools.com/tags/ref_httpmethods.asp%0D)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP请求方法：[https://www.w3schools.com/tags/ref_httpmethods.asp](https://www.w3schools.com/tags/ref_httpmethods.asp%0D)
- en: 'Requests Python library for making HTTP requests: [https://docs.Python-requests.org](https://docs.Python-requests.org)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于发起HTTP请求的Requests Python库：[https://docs.Python-requests.org](https://docs.Python-requests.org)
- en: 'Executing an AzureML pipeline through **Azure Data Factory**: [https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service](https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**Azure Data Factory**执行AzureML管道：[https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service](https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service)
- en: 'The **click** Python library for script parameter parsing and the creation
    of **Command-Line Interface** (**CLI**) applications: [https://click.palletsprojects.com/](https://click.palletsprojects.com/)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于脚本参数解析和创建**命令行界面**（**CLI**）应用程序的**click** Python库：[https://click.palletsprojects.com/](https://click.palletsprojects.com/)
