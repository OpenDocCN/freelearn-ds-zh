- en: Machine Learning with the ML Module
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ML 模块进行机器学习
- en: In this chapter, we will move on to the currently supported machine learning
    module of PySpark—the ML module. The ML module, like MLLib, exposes a vast array
    of machine learning models, almost completely covering the spectrum of the most-used
    (and usable) models. The ML module, however, operates on Spark DataFrames, making
    it much more performant as it can leverage the tungsten execution optimizations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续使用 PySpark 当前支持的机器学习模块——ML 模块。ML 模块像 MLLib 一样，暴露了大量的机器学习模型，几乎完全覆盖了最常用（和可用）的模型。然而，ML
    模块是在 Spark DataFrames 上运行的，因此它的性能更高，因为它可以利用钨执行优化。
- en: 'In this chapter, you will learn about the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下教程：
- en: Introducing Transformers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入变压器
- en: Introducing Estimators
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入估计器
- en: Introducing Pipelines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入管道
- en: Selecting the most predictable features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最可预测的特征
- en: Predicting forest coverage types
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测森林覆盖类型
- en: Estimating forest elevation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估算森林海拔
- en: Clustering forest cover types
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类森林覆盖类型
- en: Tuning hyperparameters
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数
- en: Extracting features from text
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本中提取特征
- en: Discretizing continuous variables
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散化连续变量
- en: Standardizing continuous variables
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化连续变量
- en: Topic mining
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题挖掘
- en: 'In this chapter, we will use data we downloaded from [https://archive.ics.uci.edu/ml/datasets/covertype](https://archive.ics.uci.edu/ml/datasets/covertype).
    The dataset is located in the GitHub repository for this book: `/data/forest_coverage_type.csv`.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用从 [https://archive.ics.uci.edu/ml/datasets/covertype](https://archive.ics.uci.edu/ml/datasets/covertype)
    下载的数据。数据集位于本书的 GitHub 仓库中：`/data/forest_coverage_type.csv`。
- en: 'We load the data in the same manner as before:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与之前相同的方式加载数据：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Introducing Transformers
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入变压器
- en: The `Transformer` class, introduced in Spark 1.3, transforms one dataset into
    another by normally appending one or more columns to the existing DataFrame. Transformers
    are an abstraction around methods that actually transform features; the abstraction
    also includes trained machine learning models (as we will see in the following
    recipes).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transformer` 类是在 Spark 1.3 中引入的，它通过通常将一个或多个列附加到现有的 DataFrame 来将一个数据集转换为另一个数据集。变压器是围绕实际转换特征的方法的抽象；这个抽象还包括训练好的机器学习模型（正如我们将在接下来的教程中看到的）。'
- en: In this recipe, we will introduce two Transformers: `Bucketizer` and `VectorAssembler`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将介绍两个变压器：`Bucketizer` 和 `VectorAssembler`。
- en: We will not be introducing all the Transformers; throughout the rest of this
    chapter, the most useful ones will show up. For the rest, the Spark documentation
    is a good place to learn what they do and how to use them.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会介绍所有的变压器；在本章的其余部分，最有用的变压器将会出现。至于其余的，Spark 文档是学习它们的功能和如何使用它们的好地方。
- en: 'Here is a list of all of the Transformers that convert one feature into another:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将一个特征转换为另一个特征的所有变压器的列表：
- en: '`Binarizer` is a method that, given a threshold, transforms a continuous numerical
    feature into a binary one.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Binarizer` 是一种方法，给定一个阈值，将连续的数值特征转换为二进制特征。'
- en: '`Bucketizer`, similarly to `Binarizer`, uses a list of thresholds to transform
    a continuous numerical variable into a discrete one (with as many levels as the
    length of the list of thresholds plus one).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Bucketizer` 与 `Binarizer` 类似，它使用一组阈值将连续数值变量转换为离散变量（级别数与阈值列表长度加一相同）。'
- en: '`ChiSqSelector` helps to select a predefined number of features that explain
    the most of the variance of a categorical target (a classification model).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChiSqSelector` 帮助选择解释分类目标（分类模型）方差大部分的预定义数量的特征。'
- en: '`CountVectorizer` converts many lists of strings into a `SparseVector` of counts,
    where each column is a flag for each distinct string found in the lists, and the
    value indicates how many times the string was found in the current list.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CountVectorizer` 将许多字符串列表转换为计数的 `SparseVector`，其中每一列都是列表中每个不同字符串的标志，值表示当前列表中找到该字符串的次数。'
- en: '`DCT` stands for the **Discrete Cosine Transform**. It takes a vector of real
    values and returns a vector of cosine functions oscillating at different frequencies.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DCT` 代表**离散余弦变换**。它接受一组实值向量，并返回以不同频率振荡的余弦函数向量。'
- en: '`ElementwiseProduct` can be used to scale your numerical features as it takes
    a vector of values and multiplies it (element by element, as the name suggests)
    by another vector with weights for each value.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ElementwiseProduct` 可以用于缩放您的数值特征，因为它接受一个值向量，并将其（如其名称所示，逐元素）乘以另一个具有每个值权重的向量。'
- en: '`HashingTF` is a hashing trick transformer that returns a vector (of specified
    length) representation for a tokenized text.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HashingTF` 是一个哈希技巧变压器，返回一个指定长度的标记文本表示的向量。'
- en: '`IDF` computes an **Inverse Document Frequency** for a list of records, where
    each record is a numerical representation of a body of text (see either `CountVectorizer`
    or `HashingTF`).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IDF` 计算记录列表的**逆文档频率**，其中每个记录都是文本主体的数值表示（请参阅 `CountVectorizer` 或 `HashingTF`）。'
- en: '`IndexToString` uses the encoding from the `StringIndexerModel` object to reverse
    the string index to original values.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IndexToString` 使用 `StringIndexerModel` 对象的编码将字符串索引反转为原始值。'
- en: '`MaxAbsScaler` rescales the data to be within the `-1` to `1` range.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxAbsScaler` 将数据重新缩放为 `-1` 到 `1` 的范围内。'
- en: '`MinMaxScaler` rescales the data to be within the `0` to `1` range.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MinMaxScaler` 将数据重新缩放为 `0` 到 `1` 的范围内。'
- en: '`NGram` returns pairs, triplets, or *n*-mores of subsequent words of a tokenized
    text.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NGram` 返回一对、三元组或 *n* 个连续单词的标记文本。'
- en: '`Normalizer` scales the data to be of unit norm (by default, `L2`).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Normalizer` 将数据缩放为单位范数（默认为 `L2`）。'
- en: '`OneHotEncoder` encodes a categorical variable into a vector representation
    where only one element is hot, that is, equal to `1` (all others are `0`).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OneHotEncoder` 将分类变量编码为向量表示，其中只有一个元素是热的，即等于 `1`（其他都是 `0`）。'
- en: '`PCA` is a dimensionality reduction method to extract principal components
    from data.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PCA` 是一种从数据中提取主成分的降维方法。'
- en: '`PolynomialExpansion` returns a polynomial expansion of an input vector.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PolynomialExpansion` 返回输入向量的多项式展开。'
- en: '`QuantileDiscretizer` is a similar method to `Bucketizer`, but instead of defining
    the thresholds, only the number of returned bins needs to be specified; the method
    will use quantiles to decide the thresholds.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QuantileDiscretizer`是类似于`Bucketizer`的方法，但不是定义阈值，而是需要指定返回的箱数；该方法将使用分位数来决定阈值。'
- en: '`RegexTokenizer` is a string tokenizer the uses regular expressions to process
    text.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RegexTokenizer` 是一个使用正则表达式处理文本的字符串标记器。'
- en: '`RFormula` is a method to pass R-syntax formula to transform data.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RFormula`是一种传递R语法公式以转换数据的方法。'
- en: '`SQLTransformer` is a method to pass SQL syntax formula to transform data.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SQLTransformer`是一种传递SQL语法公式以转换数据的方法。'
- en: '`StandardScaler` converts a numerical feature to have a 0 mean and a standard
    deviation of 1.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StandardScaler` 将数值特征转换为均值为0，标准差为1。'
- en: '`StopWordsRemover` is used to remove words such as `a` or `the` from tokenized
    text.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StopWordsRemover` 用于从标记化文本中删除诸如 `a` 或 `the` 等单词。'
- en: '`StringIndexer` produces a vector of indices given a list of all words in a
    column.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StringIndexer`根据列中所有单词的列表生成一个索引向量。'
- en: '`Tokenizer` is a default tokenizer that takes a sentence (a string), splits
    it on a space, and normalizes the words.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tokenizer`是一个默认的标记器，它接受一个句子（一个字符串），在空格上分割它，并对单词进行规范化。'
- en: '`VectorAssembler` combines the specified (separate) features into a single
    feature.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VectorAssembler`将指定的（单独的）特征组合成一个特征。'
- en: '`VectorIndexer` takes a categorical variable (already encoded to be numbers)
    and returns a vector of indices.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VectorIndexer`接受一个分类变量（已经编码为数字）并返回一个索引向量。'
- en: '`VectorSlicer` can be thought of as a converse of `VectorAssembler`, as it
    extracts the data from the vector of features given indices.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VectorSlicer` 可以被认为是`VectorAssembler`的相反，因为它根据索引从特征向量中提取数据。'
- en: '`Word2Vec` converts a sentence (or string) into a map of `{string, vector}`
    representation.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Word2Vec`将一个句子（或字符串）转换为`{string，vector}`表示的映射。'
- en: Getting ready
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the forest DataFrame.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此操作，您需要一个可用的Spark环境，并且您已经将数据加载到forest DataFrame中。
- en: No other prerequisites are required.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 无需其他先决条件。
- en: How to do it...
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Any ideas why we could not use `.QuantileDiscretizer(...)` to achieve this?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有想法为什么我们不能使用`.QuantileDiscretizer(...)`来实现这一点？
- en: How it works...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As always, we first load the necessary module we will use throughout, `pyspark.sql.functions`,
    which will allow us to calculate minimum and maximum values of the `Horizontal_Distance_To_Hydrology` feature. `pyspark.ml.feature`
    exposes the `.Bucketizer(...)` transformer for us to use, while NumPy will help
    us to create an equispaced list of thresholds.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，我们首先加载我们将在整个过程中使用的必要模块，`pyspark.sql.functions`，它将允许我们计算`Horizontal_Distance_To_Hydrology`特征的最小值和最大值。`pyspark.ml.feature`为我们提供了`.Bucketizer(...)`转换器供我们使用，而NumPy将帮助我们创建一个等间距的阈值列表。
- en: We want to bucketize our numerical variable into 10 buckets, hence our `buckets_no`
    is equal to `10`. Next, we calculate the minimum and maximum values for the `Horizontal_Distance_To_Hydrology` feature
    and return these two values to the driver. On the driver, we create the list of
    thresholds (the `splits` list); the first parameter to the `np.arange(...)` method
    is the minimum, the second one is the maximum, and the third one defines the size
    of each step.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要将我们的数值变量分成10个桶，因此我们的`buckets_no`等于`10`。接下来，我们计算`Horizontal_Distance_To_Hydrology`特征的最小值和最大值，并将这两个值返回给驱动程序。在驱动程序上，我们创建阈值列表（`splits`列表）；`np.arange(...)`方法的第一个参数是最小值，第二个参数是最大值，第三个参数定义了每个步长的大小。
- en: Now that we have the splits list defined, we pass it to the `.Bucketizer(...)`
    method.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了拆分列表，我们将其传递给`.Bucketizer(...)`方法。
- en: Each transformer (Estimators work similarly) has a very similar API, but two
    parameters are always required: `inputCol` and `outputCol`, which define the input
    and output columns to be consumed and their output, respectively. The two classes—`Transformer` and
    `Estimator`—also universally implement the `.getOutputCol()` method, which returns
    the name of the output column.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每个转换器（估计器的工作方式类似）都有一个非常相似的API，但始终需要两个参数：`inputCol`和`outputCol`，它们分别定义要消耗的输入列和它们的输出列。这两个类——`Transformer`和`Estimator`——也普遍实现了`.getOutputCol()`方法，该方法返回输出列的名称。
- en: 'Finally, we use the `bucketizer` object to transform our DataFrame. Here''s
    what we expect to see:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`bucketizer`对象来转换我们的DataFrame。这是我们期望看到的：
- en: '![](img/00134.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00134.jpeg)'
- en: There's more...
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Almost exclusively, every estimator (or, in other words, an ML model) found
    in the ML module expects to see a *single* column as an input; the column should
    contain all the features a data scientist wants such a model to use. The `.VectorAssembler(...)`
    method, as the name suggests, collates multiple features into a single column.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有在ML模块中找到的估计器（或者换句话说，ML模型）都期望看到一个*单一*列作为输入；该列应包含数据科学家希望这样一个模型使用的所有特征。正如其名称所示，`.VectorAssembler(...)`方法将多个特征汇总到一个单独的列中。
- en: 'Consider the following example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: First, we use the `.VectorAssembler(...)` method to collate all columns from
    our `forest` DataFrame.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`.VectorAssembler(...)`方法从我们的`forest` DataFrame中汇总所有列。
- en: Note that the  `.VectorAssembler(...)` method, unlike other Transformers, has
    the `inputCols` parameter, not `inputCol`, as it accepts a list of columns, not
    just a single column.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与其他转换器不同，`.VectorAssembler(...)`方法具有`inputCols`参数，而不是`inputCol`，因为它接受一个列的列表，而不仅仅是一个单独的列。
- en: We then use the `feat` column (which is now a `SparseVector` of all the features)
    in the `PCA(...)` method to extract the top five most significant principal components.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在`PCA(...)`方法中使用`feat`列（现在是所有特征的`SparseVector`）来提取前五个最重要的主成分。
- en: Notice how we can now use the `.getOutputCol()` method to get the name of the
    output column? It should become more apparent why we do this when we introduce
    pipelines.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们现在如何可以使用`.getOutputCol()`方法来获取输出列的名称？当我们介绍管道时，为什么这样做会变得更明显？
- en: 'The output of the preceding code should look somewhat as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出应该看起来像这样：
- en: '![](img/00135.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00135.jpeg)'
- en: See also
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: For an example of a transformer (and more) check this blog post: [https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42](https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关变换器（以及更多内容）的示例，请查看此博文：[https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42](https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42)
- en: Introducing Estimators
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Estimators
- en: The `Estimator` class, just like the `Transformer` class, was introduced in
    Spark 1.3\. The Estimators, as the name suggests, estimate the parameters of a
    model or, in other words, fit the models to data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`Estimator`类，就像`Transformer`类一样，是在Spark 1.3中引入的。Estimators，顾名思义，用于估计模型的参数，或者换句话说，将模型拟合到数据。'
- en: 'In this recipe, we will introduce two models: the linear SVM acting as a classification
    model, and a linear regression model predicting the forest elevation.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将介绍两个模型：作为分类模型的线性SVM，以及预测森林海拔的线性回归模型。
- en: 'Here is a list of all of the Estimators, or machine learning models, available
    in the ML module:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是ML模块中所有Estimators或机器学习模型的列表：
- en: 'Classification:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类：
- en: '`LinearSVC` is an SVM model for linearly separable problems. The SVM''s kernel
    has the ![](img/00136.jpeg) form (a hyperplane), where ![](img/00137.jpeg) is the
    coefficients (or a normal vector to the hyperplane), ![](img/00138.jpeg) is the
    records, and *b* is the offset.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinearSVC` 是用于线性可分问题的SVM模型。SVM的核心具有![](img/00136.jpeg)形式（超平面），其中![](img/00137.jpeg)是系数（或超平面的法向量），![](img/00138.jpeg)是记录，*b*是偏移量。'
- en: '`LogisticRegression` is a default, *go-to* classification model for linearly
    separable problems. It uses a logit function to calculate the probability of a
    record being a member of a particular class.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LogisticRegression` 是线性可分问题的默认*go-to*分类模型。它使用logit函数来计算记录属于特定类的概率。'
- en: '`DecisionTreeClassifier` is a decision tree-based model used for classification
    purposes. It builds a binary tree with the proportions of classes in the terminal
    nodes determining the class membership.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier` 是用于分类目的的基于决策树的模型。它构建一个二叉树，其中终端节点中类别的比例确定了类的成员资格。'
- en: '`GBTClassifier` is a member of the group of ensemble models. The **Gradient-Boosted
    Trees** (**GBT**) build several weak models that, when combined, form a strong
    classifier. The model can also be applied to solve regression problems.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GBTClassifier` 是集成模型组中的一员。**梯度提升树**（**GBT**）构建了几个弱模型，当组合在一起时形成一个强分类器。该模型也可以应用于解决回归问题。'
- en: '`RandomForestClassifier` is also a member of an ensemble group of models. Unlike
    GBT, however, random forests grows fully-grow decision trees and the total error
    reduction is achieved by reducing variance (while GBTs reduce bias). Just like
    GBT, these models can also be used to solve regression problems.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier` 也是集成模型组中的一员。与GBT不同，随机森林完全生长决策树，并通过减少方差来实现总误差减少（而GBT减少偏差）。就像GBT一样，这些模型也可以用来解决回归问题。'
- en: '`NaiveBayes` uses the Bayes conditional probability theory, ![](img/00139.jpeg),
    to classify observations based on evidence and prior assumptions about the probability
    and likelihood.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NaiveBayes` 使用贝叶斯条件概率理论，![](img/00139.jpeg)，根据关于概率和可能性的证据和先验假设对观察结果进行分类。'
- en: '`MultilayerPerceptronClassifier` is derived from the field of artificial intelligence,
    and, more narrowly, artificial neural networks. The model consists of a directed
    graph of artificial neurons that mimic (to some extent) the fundamental building
    blocks of the brain.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MultilayerPerceptronClassifier` 源自人工智能领域，更狭义地说是人工神经网络。该模型由模拟（在某种程度上）大脑的基本构建模块的人工神经元组成的有向图。'
- en: '`OneVsRest` is a reduction technique that selects only one class in a multinomial
    scenario.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OneVsRest` 是一种在多项式场景中只选择一个类的缩减技术。'
- en: 'Regression:'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归：
- en: '`AFTSurvivalRegression` is a parametric model that predicts life expectancy
    and assumes that a marginal effect of one of the features accelerates or decelerates
    a process failure.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AFTSurvivalRegression` 是一种参数模型，用于预测寿命，并假设特征之一的边际效应加速或减缓过程失败。'
- en: '`DecisionTreeRegressor`, a counterpart of `DecisionTreeClassifier`, is applicable
    for regression problems.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DecisionTreeRegressor`，`DecisionTreeClassifier`的对应物，适用于回归问题。'
- en: '`GBTRegressor`, a counterpart of `GBTClassifier`, is applicable for regression
    problems.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GBTRegressor`，`GBTClassifier`的对应物，适用于回归问题。'
- en: '`GeneralizedLinearRegression` is a family of linear models that allow us to
    specify different kernel functions (or link functions). Unlike linear regression,
    which assumes the normality of error terms, the **Generalized Linear Model** (**GLM**)
    allow models to have other distributions of error terms.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GeneralizedLinearRegression` 是一类允许我们指定不同核函数（或链接函数）的线性模型。与假设误差项正态分布的线性回归不同，**广义线性模型**（**GLM**）允许模型具有其他误差项分布。'
- en: '`IsotonicRegression` fits a free-form and non-decreasing line to data.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IsotonicRegression` 将自由形式和非递减线拟合到数据。'
- en: '`LinearRegression` is the benchmark of regression models. It fits a straight
    line (or a plane defined in linear terms) through the data.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinearRegression` 是回归模型的基准。它通过数据拟合一条直线（或用线性术语定义的平面）。'
- en: '`RandomForestRegressor`, a counterpart of `RandomForestClassifier`, is applicable
    for regression problems.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestRegressor`，`RandomForestClassifier`的对应物，适用于回归问题。'
- en: 'Clustering:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类：
- en: '`BisectingKMeans` is a model that begins with all observations in a single
    cluster and iteratively splits the data into *k* clusters.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BisectingKMeans` 是一个模型，它从一个单一聚类开始，然后迭代地将数据分成*k*个聚类。'
- en: '`Kmeans` separates data into *k* (defined) clusters by iteratively finding
    centroids of clusters by shifting the cluster boundaries so the sum of all distances
    between data points and cluster centroids is minimized.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Kmeans` 通过迭代找到聚类的质心，通过移动聚类边界来最小化数据点与聚类质心之间的距离总和，将数据分成*k*（定义）个聚类。'
- en: '`GaussianMixture` uses *k* Gaussian distributions to break the dataset down
    into clusters.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GaussianMixture` 使用*k*个高斯分布将数据集分解成聚类。'
- en: '`LDA`: The **Latent Dirichlet Allocation** is a model frequently used in topic
    mining. It is a statistical model that makes use of some unobserved (or unnamed)
    groups to cluster observations. For example, a `PLANE_linked` cluster can have
    words included, such as engine, flaps, or wings.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LDA`：**潜在狄利克雷分配**是主题挖掘中经常使用的模型。它是一个统计模型，利用一些未观察到的（或未命名的）组来对观察结果进行聚类。例如，一个`PLANE_linked`集群可以包括诸如engine、flaps或wings等词语。'
- en: Getting ready
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此配方，您需要一个可用的Spark环境，并且您已经将数据加载到`forest` DataFrame中。
- en: No other prerequisites are required.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'First, let''s learn how to build an SVM model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们学习如何构建一个SVM模型：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `.LinearSVC(...)` method is available from `pyspark.ml.classification`,
    so we load it first.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`.LinearSVC(...)`方法来自`pyspark.ml.classification`，因此我们首先加载它。'
- en: Next, we use `.VectorAssembler(...)` to grab all the columns from the `forest`
    DataFrame, but the last one (the `CoverType`) will be used as a label. We will
    predict the forest cover type equal to `1`, that is, whether the forest is a spruce-fir
    type; we achieve this by checking whether `CoverType` is equal to `1` and casting
    the resulting Boolean to an integer. Finally, we select only `label` and `features`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`.VectorAssembler(...)`从`forest` DataFrame中获取所有列，但最后一列（`CoverType`）将用作标签。我们将预测等于`1`的森林覆盖类型，也就是说，森林是否是云杉冷杉类型；我们通过检查`CoverType`是否等于`1`并将结果布尔值转换为整数来实现这一点。最后，我们只选择`label`和`features`。
- en: Next, we create the `LinearSVC` object. We specify the maximum number of iterations
    to 10 and set the regularization parameter (type L2, or ridge) to 1%.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建`LinearSVC`对象。我们将最大迭代次数设置为10，并将正则化参数（L2类型或岭）设置为1%。
- en: 'If you are not familiar with regularization in terms of machine learning, check
    out this website: [http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/](http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对机器学习中的正则化不熟悉，请查看此网站：[http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/](http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/)。
- en: 'Other parameters include:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其他参数包括：
- en: '`featuresCol`: This is set to the name of the features columns, by default
    it is `features` (like in our dataset)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`featuresCol`：默认情况下设置为特征列的名称为`features`（就像在我们的数据集中一样）'
- en: '`labelCol`: This is set to the name of the label column if something other
    than `label`'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labelCol`：如果有其他名称而不是`label`，则设置为标签列的名称'
- en: '`predictionCol`: This is set to the name of the prediction column if you want
    to rename it to something other than `prediction`'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predictionCol`：如果要将其重命名为除`prediction`之外的其他内容，则设置为预测列的名称'
- en: '`tol`: This is a stopping parameter that defines the minimum change between
    iterations in terms of the cost function: if the change (by default) is smaller
    than 10^(-6), the algorithm will assume that it has converged'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tol`：这是一个停止参数，它定义了成本函数在迭代之间的最小变化：如果变化（默认情况下）小于10^(-6)，算法将假定它已经收敛'
- en: '`rawPredictionCol`: This returns the raw value from the generating function
    (before the threshold is applied); you can specify a different name than `rawPrediction`'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rawPredictionCol`：这返回生成函数的原始值（在应用阈值之前）；您可以指定一个不同的名称而不是`rawPrediction`'
- en: '`fitIntercept`: This instructs the model to fit the intercept (constant) as
    well, not only the model coefficients; this is set to `True` by default'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fitIntercept`：这指示模型拟合截距（常数），而不仅仅是模型系数；默认设置为`True`'
- en: '`standardization`: This is set to `True` by default, and it standardizes the
    features before fitting the model'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`standardization`：默认设置为`True`，它在拟合模型之前对特征进行标准化'
- en: '`threshold`: This is set by default to `0.0`; it is a parameter that decides
    what is classified as `1` or `0`'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold`：默认设置为`0.0`；这是一个决定什么被分类为`1`或`0`的参数'
- en: '`weightCol`: This is a column name if each observation was to be weighted differently'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weightCol`：如果每个观察结果的权重不同，则这是一个列名'
- en: '`aggregationDepth`: This is a tree-depth parameter used for aggregation'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregationDepth`：这是用于聚合的树深度参数'
- en: 'Finally, we `.fit(...)` the dataset using the object; the object returns a
    `.LinearSVCModel(...)`. Once the model is estimated, we can extract the estimated
    model''s coefficients like so: `svc_model.coefficients`. Here''s what we get:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用对象`.fit(...)`数据集；对象返回一个`.LinearSVCModel(...)`。一旦模型被估计，我们可以这样提取估计模型的系数：`svc_model.coefficients`。这是我们得到的：
- en: '![](img/00140.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00140.jpeg)'
- en: There's more...
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Now, let''s see whether a linear regression model can be reasonably accurate
    in estimating forest elevation:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看线性回归模型是否可以合理准确地估计森林海拔：
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding code is quite similar to the one presented earlier. As a side
    note, this is true for almost all the ML module models, so testing various models
    is extremely simple.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码与之前介绍的代码非常相似。顺便说一句，这对于几乎所有的ML模块模型都是正确的，因此测试各种模型非常简单。
- en: The difference is in the `label` column—right now, we are using `Elevation` and
    casting it as a `float` (since this is a regression problem).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 区别在于`label`列-现在，我们使用`Elevation`并将其转换为`float`（因为这是一个回归问题）。
- en: Similarly, the linear regression object, `lr_obj`, instantiates the `.LinearRegression(...)`
    object.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，线性回归对象`lr_obj`实例化了`.LinearRegression(...)`对象。
- en: For the full list of parameters to `.LinearRegression(...)`, please refer to
    the documentation: [http://bit.ly/2J9OvEJ](http://bit.ly/2J9OvEJ).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`.LinearRegression(...)`的完整参数列表，请参阅文档：[http://bit.ly/2J9OvEJ](http://bit.ly/2J9OvEJ)。
- en: 'Once the model is estimated, we can check its coefficients by calling `lr_model.coefficients`.
    Here''s what we get:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被估计，我们可以通过调用`lr_model.coefficients`来检查其系数。这是我们得到的：
- en: '![](img/00141.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00141.jpeg)'
- en: 'In addition, `.LinearRegressionModel(...)` calculates a summary that returns
    basic performance statistics:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`.LinearRegressionModel(...)`计算一个返回基本性能统计信息的摘要：
- en: '[PRE6]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code will produce the following result:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将产生以下结果：
- en: '![](img/00142.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00142.jpeg)'
- en: 'Surprisingly, the linear regression does well in this application: 78% R-squared
    is not a bad result.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，线性回归在这个应用中表现不错：78%的R平方并不是一个坏结果。
- en: Introducing Pipelines
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍管道
- en: The `Pipeline` class helps to sequence, or streamline, the execution of separate
    blocks that lead to an estimated model; it chains multiple Transformers and Estimators
    to form a sequential execution workflow.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline`类有助于对导致估计模型的单独块的执行进行排序或简化；它将多个Transformer和Estimator链接在一起，形成一个顺序执行的工作流程。'
- en: Pipelines are useful as they avoid explicitly creating multiple transformed
    datasets as the data gets pushed through different parts of the overall data transformation
    and model estimation process. Instead, Pipelines abstract distinct intermediate
    stages by automating the data flow through the workflow. This makes the code more
    readable and maintainable as it creates a higher abstraction of the system, and
    it helps with code debugging.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 管道很有用，因为它们避免了在整体数据转换和模型估计过程中通过不同部分推送数据时显式创建多个转换数据集。相反，管道通过自动化数据流程来抽象不同的中间阶段。这使得代码更易读和可维护，因为它创建了系统的更高抽象，并有助于代码调试。
- en: In this recipe, we will streamline the execution of a generalized linear regression
    model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个操作步骤中，我们将简化广义线性回归模型的执行。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此操作步骤，您需要一个可用的Spark环境，并且您已经将数据加载到`forest` DataFrame中。
- en: No other prerequisites are required.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'The following code provides a streamlined version of the execution of the linear
    regression model estimation via GLM:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码提供了通过GLM估计线性回归模型的执行的简化版本：
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works...
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The whole code is much shorter than the one we used in the previous example,
    as we do not need to do the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 整个代码比我们在上一个示例中使用的代码要短得多，因为我们不需要做以下工作：
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'However, as before, we specify `vectorAssembler` and `lr_obj` (the `.GeneralizedLinearRegression(...)`
    object). `.GeneralizedLinearRegression(...)` allows us to specify not only the
    model''s family, but also the link function. In order to decide what link function
    and family to choose, we can look at the distribution of our `Elevation` column:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与之前一样，我们指定了`vectorAssembler`和`lr_obj`（`.GeneralizedLinearRegression（...）`对象）。`.GeneralizedLinearRegression（...）`允许我们不仅指定模型的family，还可以指定link函数。为了决定选择什么样的link函数和family，我们可以查看我们的`Elevation`列的分布：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here''s the plot that results from running the preceding code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这是运行上述代码后得到的图表：
- en: '![](img/00143.jpeg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00143.jpeg)'
- en: The distribution is a bit skewed, but to a certain degree, we can assume that
    it follows a normal distribution. Thus, we can use `family = 'gaussian'` (default)
    and `link = 'identity'`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 分布有点偏斜，但在一定程度上，我们可以假设它遵循正态分布。因此，我们可以使用`family = 'gaussian'`（默认）和`link = 'identity'`。
- en: Having created the Transformer (`vectorAssembler`) and the Estimator (`lr_obj`),
    we then put them into a Pipeline. The `stages` parameter is an ordered list of
    the objects to push our data through; in our case, `vectorAssembler` goes first
    as we need to collate all the features, and then we estimate our model using `lr_obj`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了Transformer（`vectorAssembler`）和Estimator（`lr_obj`）之后，我们将它们放入管道中。`stages`参数是一个有序列表，用于将数据推送到我们的数据中；在我们的情况下，`vectorAssembler`首先进行，因为我们需要整理所有的特征，然后我们使用`lr_obj`估计我们的模型。
- en: Finally, we use the pipeline to estimate the model at the same time. The Pipeline's
    `.fit(...)` method calls either the `.transform(...)` method if the object is
    a Transformer, or the `.fit(...)` method if the object is an Estimator. Consequently,
    calling the `.transform(...)` method on `PipelineModel` calls the `.transform(...)`
    methods of both the Transformer and Estimator objects.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用管道同时估计模型。管道的`.fit（...）`方法调用`.transform（...）`方法（如果对象是Transformer），或者`.fit（...）`方法（如果对象是Estimator）。因此，在`PipelineModel`上调用`.transform（...）`方法会调用Transformer和Estimator对象的`.transform（...）`方法。
- en: 'The final result looks as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果如下：
- en: '![](img/00144.jpeg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00144.jpeg)'
- en: As you can see, the results are not that much different from the actual ones.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，结果与实际结果并没有太大不同。
- en: See also
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Check out this blog post (even though it's Scala-specific) for an overview of
    Pipelines: [https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看此博文（尽管它是特定于Scala的）以获取有关管道的概述：[https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)
- en: Selecting the most predictable features
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最可预测的特征
- en: 'A mantra of (almost) every data scientist is: build a simple model while explaining
    as much variance in the target as possible. In other words, you can build a model
    with all your features, but the model may be highly complex and prone to overfitting.
    What''s more, if one of the variables is missing, the whole model might produce
    an erroneous output and some of the variables might simply be unnecessary, as
    other variables would already explain the same portion of the variance (a term
    called *collinearity*).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: （几乎）每个数据科学家的口头禅是：构建一个简单的模型，同时尽可能解释目标中的方差。换句话说，您可以使用所有特征构建模型，但模型可能非常复杂且容易过拟合。而且，如果其中一个变量缺失，整个模型可能会产生错误的输出，有些变量可能根本不必要，因为其他变量已经解释了相同部分的方差（称为*共线性*）。
- en: In this recipe, we will learn how to select the best predicting model when building
    either classification or regression models. We will be reusing what we learn in
    this recipe in the recipes that follow.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个操作步骤中，我们将学习如何在构建分类或回归模型时选择最佳的预测模型。我们将在接下来的操作步骤中重复使用本操作步骤中学到的内容。
- en: Getting ready
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此操作，您需要一个可用的Spark环境，并且您已经将数据加载到`forest` DataFrame中。
- en: No other prerequisites are required.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s begin with a code that will help to select the top 10 features with
    the most predictive power to find the best class for an observation in our `forest`
    DataFrame:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一段代码开始，这段代码将帮助选择具有最强预测能力的前10个特征，以找到`forest` DataFrame中观察结果的最佳类别：
- en: '[PRE10]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How it works...
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we assemble all the features into a single vector using the `.VectorAssembler(...)`
    method. Note that we do not use the last column as it is the `CoverType` feature
    and this is our target.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`.VectorAssembler(...)`方法将所有特征组装成一个单一向量。请注意，我们不使用最后一列，因为它是`CoverType`特征，这是我们的目标。
- en: Next, we use the `.ChiSqSelector(...)` method to select the best features based
    on the pairwise chi-square test between each variable and the target. Based on
    the values from the test, `numTopFeatures`, the most predictable features, are
    selected. The `selected` vector will contain the top 10 (in this case) most predictable
    features. The `labelCol` specifies the target column.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`.ChiSqSelector(...)`方法基于每个变量与目标之间的成对卡方检验来选择最佳特征。根据测试的值，选择`numTopFeatures`个最可预测的特征。`selected`向量将包含前10个（在这种情况下）最可预测的特征。`labelCol`指定目标列。
- en: You can learn more about the chi-square test here: [http://learntech.uwe.ac.uk/da/Default.aspx?pageid=1440](http://learntech.uwe.ac.uk/da/Default.aspx?pageid=1440).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于卡方检验的信息：[http://learntech.uwe.ac.uk/da/Default.aspx?pageid=1440](http://learntech.uwe.ac.uk/da/Default.aspx?pageid=1440)。
- en: 'Let''s check it out:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看：
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here''s what you should see from running the preceding snippet:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从运行前面的代码段中，你应该看到以下内容：
- en: '![](img/00145.jpeg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00145.jpeg)'
- en: As you can see, the resulting `SparseVector` has a length of 10 and includes
    only the most predictable features.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，生成的`SparseVector`长度为10，只包括最可预测的特征。
- en: There's more...
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We cannot use the `.ChiSqSelector(...)` method to select features against targets
    that are continuous, that is, the regression problems. One approach to select
    the best features would be to check correlations between each and every feature
    and the target and select those that are the most highly correlated with the target
    but exhibit little to no correlation with other features:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能使用`.ChiSqSelector(...)`方法来选择连续的目标特征，也就是回归问题。选择最佳特征的一种方法是检查每个特征与目标之间的相关性，并选择那些与目标高度相关但与其他特征几乎没有相关性的特征：
- en: '[PRE12]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There is no automatic way to do this in Spark, but starting with Spark 2.2,
    we can now calculate correlations between features in DataFrames.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中没有自动执行此操作的方法，但是从Spark 2.2开始，我们现在可以计算数据框中特征之间的相关性。
- en: The `.Correlation(...)` method is part of the `pyspark.ml.stat` module, so we
    import it first.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`.Correlation(...)`方法是`pyspark.ml.stat`模块的一部分，所以我们首先导入它。'
- en: Next, we create `.VectorAssembler(...)`, which collates all the columns of the
    `forest` DataFrame. We can now use the Transformer and pass the resulting DataFrame
    to the `Correlation` class. The `.corr(...)` method of the `Correlation` class
    accepts a DataFrame as its first parameter, the name of the column with all the
    features as the second, and the type of correlation to calculate as the third;
    the available values are `pearson` (the default value) and `spearman`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建`.VectorAssembler(...)`，它汇总`forest` DataFrame的所有列。现在我们可以使用Transformer，并将结果DataFrame传递给`Correlation`类。`Correlation`类的`.corr(...)`方法接受DataFrame作为其第一个参数，具有所有特征的列的名称作为第二个参数，要计算的相关性类型作为第三个参数；可用的值是`pearson`（默认值）和`spearman`。
- en: Check out this website for more information about the two correlation methods: [http://bit.ly/2xm49s7](http://bit.ly/2xm49s7).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这个网站，了解更多关于这两种相关性方法的信息：[http://bit.ly/2xm49s7](http://bit.ly/2xm49s7)。
- en: 'Here''s what we would expect to see from running the method:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 从运行该方法中，我们期望看到的内容如下：
- en: '![](img/00146.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00146.jpeg)'
- en: 'Now that we have the correlation matrix, we can extract the top 10 most correlated
    features with our label:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了相关矩阵，我们可以提取与我们的标签最相关的前10个特征：
- en: '[PRE13]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: First, we specify the number of features we want to extract and create a dictionary
    with all the columns from our `forest` DataFrame; note that we ZIP it with the
    index as the correlation matrix does not propagate the feature names, only the
    indices.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们指定要提取的特征数量，并创建一个包含`forest` DataFrame的所有列的字典；请注意，我们将其与索引一起压缩，因为相关矩阵不会传播特征名称，只传播索引。
- en: Next, we extract the first column from the `corr_matrix` (as this is our target,
    the Elevation feature); the `.toArray()` method converts a DenseMatrix to a NumPy
    array representation. Note that we also append the index to the elements of this
    array so we know which element is most correlated with our target.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从`corr_matrix`中提取第一列（因为这是我们的目标，即Elevation特征）；`.toArray()`方法将DenseMatrix转换为NumPy数组表示。请注意，我们还将索引附加到此数组的元素，以便我们知道哪个元素与我们的目标最相关。
- en: Next, we sort the list in descending order by looking at the absolute values
    of the correlation coefficient.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们按相关系数的绝对值降序排序列表。
- en: Finally, we loop through the top 10 (in this case) elements of the resulting
    list and select the column from the `cols` dictionary that corresponds with the
    selected index.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们循环遍历结果列表的前10个元素（在这种情况下），并从`cols`字典中选择与所选索引对应的列。
- en: 'For our problem that aims at estimating the forest elevation, here''s the list
    of features we get:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们旨在估计森林海拔的问题，这是我们得到的特征列表：
- en: '![](img/00147.jpeg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00147.jpeg)'
- en: See also
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: If you are curious to learn more about feature selection, check out this paper: [http://www.stat.wisc.edu/~loh/treeprogs/guide/lchen.pdf](http://www.stat.wisc.edu/~loh/treeprogs/guide/lchen.pdf)
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于特征选择的信息，可以查看这篇论文：[http://www.stat.wisc.edu/~loh/treeprogs/guide/lchen.pdf](http://www.stat.wisc.edu/~loh/treeprogs/guide/lchen.pdf)
- en: Predicting forest coverage types
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测森林覆盖类型
- en: 'In this recipe, we will learn how to process data and build two classification
    models that aim to forecast the forest coverage type: the benchmark logistic regression
    model and the random forest classifier. The problem we have at hand is *multinomial*,
    that is, we have more than two classes that we want to classify our observations
    into.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将学习如何处理数据并构建两个旨在预测森林覆盖类型的分类模型：基准逻辑回归模型和随机森林分类器。我们手头的问题是*多项式*，也就是说，我们有超过两个类别，我们希望将我们的观察结果分类到其中。
- en: Getting ready
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的Spark环境，并且您已经将数据加载到`forest` DataFrame中。
- en: No other prerequisites are required.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Here''s the code that will help us build the logistic regression model:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是帮助我们构建逻辑回归模型的代码：
- en: '[PRE14]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'First, we split the data we have into two subsets: the first one, `forest_train`,
    we will use for training the model, while `forest_test` will be used for testing
    the performance of the model.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据分成两个子集：第一个`forest_train`，我们将用于训练模型，而`forest_test`将用于测试模型的性能。
- en: 'Next, we build the usual stages we have already seen earlier in this chapter:
    we collate all the features we want to use to build our model using `.VectorAssembler(...)`
    and then pass them through the `.ChiSqSelector(...)` method to select the top
    10 most predictive features.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建了本章前面已经看到的通常阶段：我们使用`.VectorAssembler（...）`整理我们要用来构建模型的所有特征，然后通过`.ChiSqSelector（...）`方法选择前10个最具预测性的特征。
- en: 'As the last step before building the Pipeline, we create `logReg_obj`: the
    `.LogisticRegression(...)` object we will use to fit our data with. We use the
    elastic-net type of regularization in this model: the L2 portion is defined in
    the `regParam` parameter, and the L1 portion in `elasticNetParam`. Note that we
    specify the family of the model to be `multinomial` as we are dealing with a multinomial
    classification problem.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建Pipeline之前的最后一步，我们创建了`logReg_obj`：我们将用它来拟合我们的数据的`.LogisticRegression（...）`对象。在这个模型中，我们使用弹性网络类型的正则化：`regParam`参数中定义了L2部分，`elasticNetParam`中定义了L1部分。请注意，我们指定模型的family为`multinomial`，因为我们正在处理多项式分类问题。
- en: You can also specify the `family` parameter to be `auto` or `binomial`, if you
    want the model to self-select, or if you have a binary variable.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要模型自动选择，或者如果您有一个二进制变量，还可以指定`family`参数为`auto`或`binomial`。
- en: Finally, we build the Pipeline and pass the three objects as the list of stages.
    Next, we push our data through the pipeline using the `.fit(...)` method.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们构建了Pipeline，并将这三个对象作为阶段列表传递。接下来，我们使用`.fit（...）`方法将我们的数据通过管道传递。
- en: 'Now that we have the model estimated, we can check how well it performs:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经估计了模型，我们可以检查它的性能如何：
- en: '[PRE15]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: First, we load the `pyspark.ml.evaluation` module as it contains all the evaluation
    methods we will use throughout the rest of this chapter.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载`pyspark.ml.evaluation`模块，因为它包含了我们将在本章其余部分中使用的所有评估方法。
- en: Next, we push `forest_test` through our `pModel` so that we can get the predictions
    for the dataset that the model has never seen before.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`forest_test`通过我们的`pModel`，以便我们可以获得模型以前从未见过的数据集的预测。
- en: Finally, we create the `MulticlassClassificationEvaluator(...)` object, which
    will calculate the performance metrics of our model. `predictionCol` specifies
    the name of the column that contains the predicted class for an observation, and `labelCol`
    specifies the true label.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建了`MulticlassClassificationEvaluator（...）`对象，它将计算我们模型的性能指标。`predictionCol`指定包含观察的预测类的列的名称，`labelCol`指定真实标签。
- en: The `.evaluate(...)` method of the evaluator, if no other parameters are passed
    but the results of the model, will return the F1-score. If you want to retrieve
    either precision, recall, or accuracy, you need to call either `weightedPrecision`,
    `weightedRecall`, or `accuracy`, respectively.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果评估器的`.evaluate（...）`方法没有传递其他参数，而只返回模型的结果，则将返回F1分数。如果要检索精确度、召回率或准确度，则需要分别调用`weightedPrecision`、`weightedRecall`或`accuracy`。
- en: If you are not familiar with classification metrics, they are nicely explained
    here: [https://turi.com/learn/userguide/evaluation/classification.html](https://turi.com/learn/userguide/evaluation/classification.html).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对分类指标不熟悉，可以在此处找到很好的解释：[https://turi.com/learn/userguide/evaluation/classification.html](https://turi.com/learn/userguide/evaluation/classification.html)。
- en: 'Here''s how our logistic regression model performs:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的逻辑回归模型的表现：
- en: '![](img/00148.jpeg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00148.jpeg)'
- en: The accuracy of almost 70% indicates it's not a totally terrible model.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎70%的准确率表明这不是一个非常糟糕的模型。
- en: There's more...
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Let''s see whether the random forest model can do any better:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看随机森林模型是否能做得更好：
- en: '[PRE16]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see from the preceding code, we will be reusing most of the objects
    we have already created for the logistic regression model; all we introduced here
    was `.RandomForestClassifier(...)` and we can reuse the `vectorAssembler` and `selector`
    objects. This is one examples of how simple it is to work with Pipelines.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中可以看出，我们将重用我们已经为逻辑回归模型创建的大多数对象；我们在这里引入的是`.RandomForestClassifier（...）`，我们可以重用`vectorAssembler`和`selector`对象。这是与管道一起工作的简单示例之一。
- en: 'The `.RandomForestClassifier(...)` object will build the random forest model
    for us. In this example, we specified only four parameters, most of which you
    are most likely familiar with, such as `labelCol` and `featuresCol`.  `minInstancesPerNode`
    specifies the minimum number of records still allowed to split the node into two
    sub-nodes, while `numTrees` specifies how many trees in the forest to estimate.
    Other notable parameters include:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`.RandomForestClassifier（...）`对象将为我们构建随机森林模型。在此示例中，我们仅指定了四个参数，其中大多数您可能已经熟悉，例如`labelCol`和`featuresCol`。`minInstancesPerNode`指定允许将节点拆分为两个子节点的最小记录数，而`numTrees`指定要估计的森林中的树木数量。其他值得注意的参数包括：'
- en: '`impurity`: This specifies the criterion used for information gain. By default,
    it is set to `gini` but can also be `entropy`.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`impurity`: 指定用于信息增益的标准。默认情况下，它设置为 `gini`，但也可以是 `entropy`。'
- en: '`maxDepth`: This specifies the maximum depth of any of the trees.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxDepth`: 指定任何树的最大深度。'
- en: '`maxBins`: This specifies the maximum number of bins in any of the trees.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxBins`: 指定任何树中的最大箱数。'
- en: '`minInfoGain`: This specifies the minimum level of information gain between
    iterations.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minInfoGain`: 指定迭代之间的最小信息增益水平。'
- en: For a full specification of the class, see [http://bit.ly/2sgQAFa](http://bit.ly/2sgQAFa).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 有关该类的完整规范，请参阅 [http://bit.ly/2sgQAFa](http://bit.ly/2sgQAFa)。
- en: 'Having estimated the model, let''s see how it performs so we can compare it
    to the logistic regression one:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 估计了模型后，让我们看看它的表现，以便与逻辑回归进行比较：
- en: '[PRE17]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code should produce results similar to the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应该产生类似以下的结果：
- en: '![](img/00149.jpeg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00149.jpeg)'
- en: The results are exactly the same, indicating that the two models perform equally
    well and we might want to increase the number of selected features in the selector
    stage to potentially achieve better results.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 结果完全相同，表明两个模型表现一样好，我们可能希望在选择阶段增加所选特征的数量，以潜在地获得更好的结果。
- en: Estimating forest elevation
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计森林海拔
- en: 'In this recipe, we will build two regression models that will predict forest
    elevation: the random forest regression model and the gradient-boosted trees regressor.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将构建两个回归模型，用于预测森林海拔：随机森林回归模型和梯度提升树回归器。
- en: Getting ready
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的 Spark 环境，并且您已经将数据加载到 `forest` DataFrame 中。
- en: No other prerequisites are required.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: In this recipe, we will only build a two stage Pipeline with the `.VectorAssembler(...)`
    and the `.RandomForestRegressor(...)` stages. We will skip the feature selection
    stage as it is not currently an automated process.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将只构建一个两阶段的管道，使用 `.VectorAssembler(...)` 和 `.RandomForestRegressor(...)`
    阶段。我们将跳过特征选择阶段，因为目前这不是一个自动化的过程。
- en: You can do this manually. Just check the *Selecting the most predictable features*
    recipe earlier from in this chapter.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以手动执行此操作。只需在本章中稍早的 *选择最可预测的特征* 示例中检查。
- en: 'Here''s the full code:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整的代码：
- en: '[PRE18]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: First, as always, we collate all the features we want to use in our model using
    the `.VectorAssembler(...)` method. Note that we only use the columns starting
    from the second one as the first one is our target—the elevation feature.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，像往常一样，我们使用 `.VectorAssembler(...)` 方法收集我们想要在模型中使用的所有特征。请注意，我们只使用从第二列开始的列，因为第一列是我们的目标——海拔特征。
- en: Next, we specify the `.RandomForestRegressor(...)` object. The object uses an
    almost-identical list of parameters as `.RandomForestClassifier(...)`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指定 `.RandomForestRegressor(...)` 对象。该对象使用的参数列表几乎与 `.RandomForestClassifier(...)`
    相同。
- en: See the previous recipe for a list of other notable parameters.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 查看上一个示例，了解其他显著参数的列表。
- en: The last step is to build the Pipeline object; `pip` has only two stages: `vectorAssembler`
    and `rf_obj`.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是构建管道对象；`pip` 只有两个阶段：`vectorAssembler` 和 `rf_obj`。
- en: 'Next, let''s see how our model is performing compared to the linear regression
    model we estimated in the *Introducing Estimators*recipe:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看我们的模型与我们在 *介绍估计器* 示例中估计的线性回归模型相比表现如何：
- en: '[PRE19]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`.RegressionEvaluator(...)` calculates the performance metrics of regression
    models. By default, it returns `rmse`, the root mean-squared error, but it can
    also return:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`.RegressionEvaluator(...)` 计算回归模型的性能指标。默认情况下，它返回 `rmse`，即均方根误差，但也可以返回：'
- en: '`mse`: This is the mean-squared error'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mse`: 这是均方误差'
- en: '`r2`: This is the *R²* metric'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r2`: 这是 *R²* 指标'
- en: '`mae`: This is the mean-absolute error'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mae`: 这是平均绝对误差'
- en: 'From the preceding code, we got:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述代码中，我们得到：
- en: '![](img/00150.jpeg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00150.jpeg)'
- en: This is better than the linear regression model we built earlier, meaning that
    our model might not be as linearly separable as we initially thought.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这比我们之前构建的线性回归模型要好，这意味着我们的模型可能不像我们最初认为的那样线性可分。
- en: Check out this website for more information about the different types of regression
    metrics: [http://bit.ly/2sgpONr](http://bit.ly/2sgpONr).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 查看此网站，了解有关不同类型回归指标的更多信息：[http://bit.ly/2sgpONr](http://bit.ly/2sgpONr)。
- en: There's more...
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Let''s see whether the gradient-boosted trees model can beat the preceding
    result:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看梯度提升树模型是否能击败先前的结果：
- en: '[PRE20]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The only change compared to the random forest regressor is the fact that we
    now use the `.GBTRegressor(...)` class to fit the gradient-boosted trees model
    to our data. The most notable parameters for this class include:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林回归器相比唯一的变化是，我们现在使用 `.GBTRegressor(...)` 类来将梯度提升树模型拟合到我们的数据中。这个类的最显著参数包括：
- en: '`maxDepth`: This specifies the maximum depth of the built trees, which by default
    is set to `5`'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxDepth`: 指定构建树的最大深度，默认设置为 `5`'
- en: '`maxBins`: This specifies the maximum number of bins'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxBins`: 指定最大箱数'
- en: '`minInfoGain`: This specifies the minimum level of information gain between
    iterations'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minInfoGain`: 指定迭代之间的最小信息增益水平'
- en: '`minInstancesPerNode`: This specifies the minimum number of instances when
    the tree will still perform a split'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minInstancesPerNode`: 当树仍然执行分裂时，指定实例的最小数量'
- en: '`lossType`: This specifies the loss type, and accepts the `squared` or `absolute`
    values'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lossType`: 指定损失类型，并接受 `squared` 或 `absolute` 值'
- en: '`impurity`: This is, by default, set to `variance`, and for now (in Spark 2.3)
    is the only option allowed'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`impurity`: 默认设置为 `variance`，目前（在 Spark 2.3 中）是唯一允许的选项'
- en: '`maxIter`: This specifies the maximum number of iterations—a stopping criterion
    for the algorithm'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxIter`: 指定最大迭代次数——算法的停止准则'
- en: 'Let''s check the performance now:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查性能：
- en: '[PRE21]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here''s what we got:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们得到的结果：
- en: '![](img/00151.jpeg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00151.jpeg)'
- en: As you can see, we have still (even though ever-so-slightly) improved over the
    random forest regressor.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，即使我们略微改进了随机森林回归器。
- en: Clustering forest cover types
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类森林覆盖类型
- en: Clustering is an unsupervised family of methods that attempts to find patterns
    in data without any indication of what a class might be. In other words, the clustering
    methods find commonalities between records and groups them into clusters, depending
    on how similar they are to each other, and how dissimilar they are from those
    found in other clusters.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督的方法，试图在没有任何类别指示的情况下找到数据中的模式。换句话说，聚类方法找到记录之间的共同点，并根据它们彼此的相似程度以及与其他聚类中发现的记录的不相似程度将它们分组成聚类。
- en: In this recipe, we will build the most fundamental model of them all—the k-means.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将构建最基本的模型之一——k-means模型。
- en: Getting ready
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此教程，您需要一个可用的Spark环境，并且您已经将数据加载到`forest` DataFrame中。
- en: No other prerequisites are required.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The process of building a clustering model in Spark does not deviate significantly
    from what we have already seen in either the classification or regression examples:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中构建聚类模型的过程与我们在分类或回归示例中已经看到的过程没有明显的偏差：
- en: '[PRE22]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How it works...
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We, as always, start with importing the relevant modules; in this case, it is
    the `pyspark.ml.clustering` module.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们首先导入相关模块；在这种情况下，是`pyspark.ml.clustering`模块。
- en: Next, we collate all the features together that we will use in building the
    model using the well-known `.VectorAssembler(...)` Transformer.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将汇总所有要在构建模型中使用的特征，使用众所周知的`.VectorAssembler（...）`转换器。
- en: 'This is followed by instantiating the `.KMeans(...)` object. We only specified
    two parameters, but the list of the most notable ones is as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 然后实例化`.KMeans（...）`对象。我们只指定了两个参数，但最显著的参数列表如下：
- en: '`k`: This specifies the expected number of clusters and is the only required
    parameter to build the k-means model'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k`：指定预期的聚类数，是构建k-means模型的唯一必需参数'
- en: '`initMode`: This specifies the initialization type of the cluster centroids; `k-means||`
    to use a parallel variant of k-means, or `random` to choose random centroid points'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initMode`：指定聚类中心的初始化类型；`k-means||`使用k-means的并行变体，或`random`选择随机的聚类中心点'
- en: '`initSteps`: This specifies the initialization steps'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initSteps`：指定初始化步骤'
- en: '`maxIter`: This specifies the maximum number of iterations after which the
    algorithm stops, even if it had not achieved a convergence'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxIter`：指定算法停止的最大迭代次数，即使它尚未收敛'
- en: Finally, we build the Pipeline with two stages only.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只构建了包含两个阶段的管道。
- en: 'Once the results are calculated, we can look at what we got. Our aim was to
    see whether there are any underlying patterns found in the type of forest coverage:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出结果，我们可以看看我们得到了什么。我们的目标是看看是否在森林覆盖类型中找到了任何潜在模式：
- en: '[PRE23]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here''s what we got from running the preceding code:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们从运行上述代码中得到的结果：
- en: '![](img/00152.jpeg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/00152.jpeg)
- en: 'As you can see, there do not seem to be many patterns that would differentiate
    the forest cover types. However, let''s see whether our segmentation simply performs
    poorly and that this is why we are not finding any patterns, or whether we are
    finding patterns that are simply not really aligning with `CoverType`:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，似乎没有许多模式可以区分森林覆盖类型。但是，让我们看看我们的分割是否表现不佳，这就是为什么我们找不到任何模式的原因，还是我们找到的模式根本不与`CoverType`对齐：
- en: '[PRE24]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`.ClusteringEvaluator(...)` is a new evaluator available since Spark 2.3 and
    is still experimental. It calculates the Silhouette metrics for the clustering
    results.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`.ClusteringEvaluator（...）`是自Spark 2.3以来可用的新评估器，仍处于实验阶段。它计算聚类结果的轮廓度量。'
- en: To learn more about the silhouette metrics, check out [http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多有关轮廓度量的信息，请查看[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)。
- en: 'Here''s what we got for our k-means model:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的k-means模型：
- en: '![](img/00153.jpeg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/00153.jpeg)
- en: As you can see, we got a decent model, as anything around 0.5 or more indicates
    well-separated clusters.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们得到了一个不错的模型，因为0.5左右的任何值都表示聚类分离良好。
- en: See also
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Check out [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html) for
    a comprehensive overview of the clustering models. Note that many of them are
    not available in Spark.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看[http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)以全面了解聚类模型。请注意，其中许多模型在Spark中不可用。
- en: Tuning hyperparameters
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整超参数
- en: Many models already mentioned in this chapter have multiple parameters that
    determine how the model will perform. Selecting some is relatively straightforward,
    but there are many that we simply cannot set intuitively. That's where hyperparameters-tuning
    comes to play. The hyperparameters-tuning methods help us select the best (or
    close to) set of parameters that maximizes some metric we defined.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中已经提到的许多模型都有多个参数，这些参数决定了模型的性能。选择一些相对简单，但有许多参数是我们无法直观设置的。这就是超参数调整方法的作用。超参数调整方法帮助我们选择最佳（或接近最佳）的参数集，以最大化我们定义的某个度量标准。
- en: In this recipe, we will show you two approaches for hyperparameter-tuning.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将向您展示超参数调整的两种方法。
- en: Getting ready
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame. You would also have
    gone through all the previous recipes as we assume you have a working knowledge
    of Transformers, Estimators, Pipelines, and some of the regression models.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此操作，您需要一个可用的Spark环境，并且已经将数据加载到`forest` DataFrame中。我们还假设您已经熟悉了转换器、估计器、管道和一些回归模型。
- en: No other prerequisites are required.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We start with grid search. It is a brute-force method that simply loops through
    specific values of parameters, building new models and comparing their performance
    given some objective evaluator:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从网格搜索开始。这是一种蛮力方法，简单地循环遍历参数的特定值，构建新模型并比较它们的性能，给定一些客观的评估器：
- en: '[PRE25]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How it works...
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: There's a lot happening here, so let's unpack it step-by-step.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了很多事情，让我们一步一步地解开它。
- en: We already know the `.VectorAssembler(...)`, `.ChiSqSelector(...)`, and `.LogisticRegression(...)`
    classes, so we will not be repeating ourselves here.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了`.VectorAssembler(...)`、`.ChiSqSelector(...)`和`.LogisticRegression(...)`类，因此我们在这里不会重复。
- en: Check out previous recipes if you are not familiar with the preceding concepts.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对前面的概念不熟悉，请查看以前的配方。
- en: The core of this recipe starts with the `logReg_grid` object. This is the `.ParamGridBuilder()`
    class, which allows us to add elements to the grid that the algorithm will loop
    through and estimate the models with all the combinations of all the parameters
    and the specified values.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的核心从`logReg_grid`对象开始。这是`.ParamGridBuilder()`类，它允许我们向网格中添加元素，算法将循环遍历并估计所有参数和指定值的组合的模型。
- en: 'A word of caution: the more parameters you include and the more levels you
    specify, the more models you will have to estimate. The number of models grows
    exponentially in both the number of parameters and in the number of levels you
    specify for these parameters. Beware!'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：您包含的参数越多，指定的级别越多，您将需要估计的模型就越多。模型的数量在参数数量和为这些参数指定的级别数量上呈指数增长。当心！
- en: In this example, we loop through two parameters: `regParam` and `elasticNetParam`.
    For each of the parameters, we specify two levels, thus we will need to build
    four models.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们循环遍历两个参数：`regParam`和`elasticNetParam`。对于每个参数，我们指定两个级别，因此我们需要构建四个模型。
- en: As an evaluator, we once again use `.MulticlassClassificationEvaluator(...)`.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 作为评估器，我们再次使用`.MulticlassClassificationEvaluator(...)`。
- en: 'Next, we specify the `.CrossValidator(...)` object, which binds all these things
    together: our `estimator` will be `logReg_obj`, `estimatorParamMaps` will be equal
    to the built `logReg_grid`, and `evaluator` is going to be `logReg_ev`.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指定`.CrossValidator(...)`对象，它将所有这些东西绑定在一起：我们的`estimator`将是`logReg_obj`，`estimatorParamMaps`将等于构建的`logReg_grid`，而`evaluator`将是`logReg_ev`。
- en: The  `.CrossValidator(...)` object splits the training data into a set of folds
    (by default, `3`) and these are used as separate training and test datasets to
    fit the models. Therefore, we not only need to fit four models based on the parameters
    grid we want to traverse, but also for each of those four models we build three
    models with different training and validation datasets.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '`.CrossValidator(...)`对象将训练数据拆分为一组折叠（默认为`3`），并将它们用作单独的训练和测试数据集来拟合模型。因此，我们不仅需要根据要遍历的参数网格拟合四个模型，而且对于这四个模型中的每一个，我们都要构建三个具有不同训练和验证数据集的模型。'
- en: Note that we first build the Pipeline that is purely data-transformative, that
    is, it only collates the features into the full features vector and then selects
    the top five features with the most predictive power; we do not fit `logReg_obj`
    at this stage.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们首先构建的管道是纯数据转换的，即，它只将特征汇总到完整的特征向量中，然后选择具有最大预测能力的前五个特征；我们在这个阶段不拟合`logReg_obj`。
- en: The model-fitting starts when we use the `cross_v` object to fit the transformed
    data. Only then will Spark estimate four distinct models and select the one that
    performs best.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用`cross_v`对象拟合转换后的数据时，模型拟合开始。只有在这时，Spark才会估计四个不同的模型并选择表现最佳的模型。
- en: 'Having now estimated the models and selected the best performing one, let''s
    see whether the selected model performs better than the one we estimated in the *Predicting
    forest coverage types* recipe:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经估计了模型并选择了表现最佳的模型，让我们看看所选的模型是否比我们在*预测森林覆盖类型*配方中估计的模型表现更好：
- en: '[PRE26]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With the help of the preceding code, we get the following results:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 借助前面的代码，我们得到了以下结果：
- en: '![](img/00154.jpeg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00154.jpeg)'
- en: As you can see, we do slightly worse than the previous model, but this is most
    likely due to the fact that we only selected the top 5 (versus 10 before) features
    with our selector.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们的表现略逊于之前的模型，但这很可能是因为我们只选择了前5个（而不是之前的10个）特征与我们的选择器。
- en: There's more...
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Another approach that aims at finding the best performing model is called **train-validation
    split**. This method performs a split of the training data into two smaller subsets:
    one that is use to train the model, and another one that is used to validate whether
    the model is not overfitting. The split is only performed once, thus in contrast
    to cross-validation, it is less expensive:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种旨在找到表现最佳模型的方法称为**训练验证拆分**。该方法将训练数据拆分为两个较小的子集：一个用于训练模型，另一个用于验证模型是否过拟合。拆分只进行一次，因此与交叉验证相比，成本较低：
- en: '[PRE27]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The preceding code is not that dissimilar from what we saw with `.CrossValidator(...)`.
    The only additional parameter we specify for the `.TrainValidationSplit(...)`
    method is the level of parallelism that controls how many threads are spun up
    when you select the best model.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码与`.CrossValidator(...)`所看到的并没有太大不同。我们为`.TrainValidationSplit(...)`方法指定的唯一附加参数是控制在选择最佳模型时会启动多少线程的并行级别。
- en: 'Using the `.TrainValidationSplit(...)` method produces the same results as
    the  `.CrossValidator(...)` approach:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`.TrainValidationSplit(...)`方法产生与`.CrossValidator(...)`方法相同的结果：
- en: '![](img/00155.jpeg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00155.jpeg)'
- en: Extracting features from text
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文本中提取特征
- en: 'Often, data scientists need to deal with unstructured data such as free-flow
    text: companies receive feedback or recommendations (among other things) from
    customers that can be a gold mine for predicting a customer''s next move or their
    sentiment toward a brand.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据科学家需要处理非结构化数据，比如自由流动的文本：公司收到客户的反馈或建议（以及其他内容），这可能是预测客户下一步行动或他们对品牌情感的宝藏。
- en: In this recipe, we will learn how to extract features from text.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们将学习如何从文本中提取特征。
- en: Getting ready
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个步骤，你需要一个可用的Spark环境。
- en: No other prerequisites are required.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: A general process that aims to extract data from text and convert it into something
    a machine learning model can use starts with the free-flow text. The first step
    is to take each sentence of the text and split it on the space character (most
    often). Next, all the stop words are removed. Finally, simply counting distinct
    words in the text or using a hashing trick takes us into the realm of numerical
    representations of free-flow text.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的过程旨在从文本中提取数据并将其转换为机器学习模型可以使用的内容，首先从自由流动的文本开始。第一步是取出文本的每个句子，并在空格字符上进行分割（通常是）。接下来，移除所有的停用词。最后，简单地计算文本中不同单词的数量或使用哈希技巧将我们带入自由流动文本的数值表示领域。
- en: 'Here''s how to achieve this with Spark''s ML module:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用Spark的ML模块来实现这一点：
- en: '[PRE28]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How it works...
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As mentioned earlier, we start with some text. In our example, we use some extracts
    from Spark's documentation.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，我们从一些文本开始。在我们的例子中，我们使用了一些从Spark文档中提取的内容。
- en: '`.RegexTokenizer(...)` is the text tokenizer that uses regular expressions
    to split the sentence. In our example, we split the sentences on a minimum of
    one (or more) space—that''s the `\s+` expression. However, our pattern also splits
    on either a comma, period, or the quotation marks—that''s the `[,.\"]` part. The
    pipe, `|`, means split on either the spaces or the punctuation marks. The text,
    after passing through `.RegexTokenizer(...)`, will look as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '`.RegexTokenizer(...)`是使用正则表达式来分割句子的文本分词器。在我们的例子中，我们在至少一个（或多个）空格上分割句子——这是`\s+`表达式。然而，我们的模式还会在逗号、句号或引号上进行分割——这是`[,.\"]`部分。管道符`|`表示在空格或标点符号上进行分割。通过`.RegexTokenizer(...)`处理后的文本将如下所示：'
- en: '![](img/00156.jpeg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00156.jpeg)'
- en: Next, we use the `.StopWordsRemover(...)` method to remove the stop words, as
    the name suggests.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`.StopWordsRemover(...)`方法来移除停用词，正如其名称所示。
- en: Check out NLTK's list of the most common stop words: [https://gist.github.com/sebleier/554280](https://gist.github.com/sebleier/554280).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 查看NLTK的最常见停用词列表：[https://gist.github.com/sebleier/554280](https://gist.github.com/sebleier/554280)。
- en: '`.StopWordsRemover(...)` simply scans the tokenized text and discards any stop
    word it encounters. After removing the stop words, our text will look as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '`.StopWordsRemover(...)`简单地扫描标记化文本，并丢弃它遇到的任何停用词。移除停用词后，我们的文本将如下所示：'
- en: '![](img/00157.jpeg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00157.jpeg)'
- en: As you can see, what is left is an essential meaning of the sentence; a human
    can read these words and somewhat make sense of it.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，剩下的是句子的基本含义；人类可以阅读这些词，并且在一定程度上理解它。
- en: 'A hashing trick (or feature hashing) is a method that transforms an arbitrary
    list of features into indices in a vector form. It is a space-efficient way of
    tokenizing text and, at the same time, turning text into a numerical representation.
    The hashing trick uses a hashing function to convert from one representation into
    another. A hashing function is essentially any mapping function that transforms
    one representation into another. Normally, it is a lossy and one-way mapping (or
    conversion); different input can be hashed into the same hash (a term called a **collision**)
    and, once hashed, it is almost always prohibitively difficult to reconstruct the
    input. The `.HashingTF(...)` method takes the input column from the `sq_remover`
    object and transforms (or encodes) the tokenized text into a vector of 20 features.
    Here''s what our text will look like after it has been hashed:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希技巧（或特征哈希）是一种将任意特征列表转换为向量形式的方法。这是一种高效利用空间的方法，用于标记文本，并同时将文本转换为数值表示。哈希技巧使用哈希函数将一种表示转换为另一种表示。哈希函数本质上是任何将一种表示转换为另一种表示的映射函数。通常，它是一种有损和单向的映射（或转换）；不同的输入可以被哈希成相同的哈希值（称为**冲突**），一旦被哈希，几乎总是极其困难来重构输入。`.HashingTF(...)`方法接受`sq_remover`对象的输入列，并将标记化文本转换（或编码）为一个包含20个特征的向量。在经过哈希处理后，我们的文本将如下所示：
- en: '![](img/00158.jpeg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00158.jpeg)'
- en: Now that we have the features hashed, we could potentially use these features
    to train a machine learning model. However, simply counting the occurrences of
    words might lead to misleading conclusions. A better measure is the **term frequency-inverse
    document frequency** (**TF-IDF**). It is a metric that counts how many times a
    word occurs in the whole corpus and then calculates a proportion of the word's
    count in a sentence to its count in the whole corpus. This measure helps to evaluate
    how important a word is to a document in the whole collection of documents. In
    Spark, we use the `.IDF(...)` method, which does this for us.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对特征进行了哈希处理，我们可能可以使用这些特征来训练一个机器学习模型。然而，简单地计算单词出现的次数可能会导致误导性的结论。一个更好的度量是**词频-逆文档频率**（**TF-IDF**）。这是一个度量，它计算一个词在整个语料库中出现的次数，然后计算一个句子中该词出现次数与整个语料库中出现次数的比例。这个度量有助于评估一个词对整个文档集合中的一个文档有多重要。在Spark中，我们使用`.IDF(...)`方法来实现这一点。
- en: 'Here''s what our text would look like after passing the whole Pipeline:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过整个管道后，我们的文本将如下所示：
- en: '![](img/00159.jpeg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00159.jpeg)'
- en: So, effectively, we have encoded the passage from Spark's documentation into
    a vector of 20 elements that we could now use to train a machine learning model.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实际上，我们已经将Spark文档中的内容编码成了一个包含20个元素的向量，现在我们可以用它来训练一个机器学习模型。
- en: There's more...
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Another way of encoding text into a numerical form is by using the Word2Vec
    algorithm. The algorithm computes a distributed representation of words with the
    advantage that similar words are placed close together in the vector space.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本编码成数字形式的另一种方法是使用Word2Vec算法。该算法计算单词的分布式表示，优势在于相似的单词在向量空间中被放在一起。
- en: Check out this tutorial to learn more about Word2Vec and the skip-gram model: [http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这个教程，了解更多关于Word2Vec和skip-gram模型的信息：[http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)。
- en: 'Here''s how we do it in Spark:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中我们是这样做的：
- en: '[PRE29]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will get a vector of five elements from the `.Word2Vec(...)` method. Also,
    only words that occur at least twice in the corpus will be used to create the
    word-embedding. Here''s what the resulting vector will look like:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从`.Word2Vec(...)`方法中得到一个包含五个元素的向量。此外，只有在语料库中至少出现两次的单词才会被用来创建单词嵌入。以下是结果向量的样子：
- en: '![](img/00160.jpeg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00160.jpeg)'
- en: See also
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'To learn more about text-feature engineering, check out this position from
    Packt: [http://bit.ly/2IZ7ZZA](http://bit.ly/2IZ7ZZA)'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于文本特征工程的信息，请查看Packt的这个位置：[http://bit.ly/2IZ7ZZA](http://bit.ly/2IZ7ZZA)
- en: Discretizing continuous variables
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离散化连续变量
- en: Sometimes, it is actually useful to have a discrete representation of a continuous
    variable.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，将连续变量离散化表示实际上是有用的。
- en: In this recipe, we will learn how to discretize a numerical feature with an
    example drawn from the Fourier series.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将学习如何使用傅立叶级数中的一个例子离散化数值特征。
- en: Getting ready
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个配方，你需要一个可用的Spark环境。
- en: No other prerequisites are required.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this recipe, we will use a small dataset that is located in the `data` folder,
    namely, `fourier_signal.csv`:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用位于`data`文件夹中的一个小数据集，即`fourier_signal.csv`：
- en: '[PRE30]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works...
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: First, we read the data into `signal_df`. The `fourier_signal.csv` contains
    a single column called `signal`.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据读入`signal_df`。`fourier_signal.csv`包含一个名为`signal`的单独列。
- en: Next, we use the `.QuantileDiscretizer(...)` method to discretize the signal
    into 10 buckets. The bin ranges are chosen based on quantiles, that is, each bin
    will have the same number of observations.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`.QuantileDiscretizer(...)`方法将信号离散为10个桶。桶的范围是基于分位数选择的，也就是说，每个桶将有相同数量的观察值。
- en: 'Here''s what the original signal looks like (the black line), and what its
    discretized representation looks like:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这是原始信号的样子（黑线），以及它的离散表示的样子：
- en: '![](img/00161.jpeg)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00161.jpeg)'
- en: Standardizing continuous variables
  id: totrans-394
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化连续变量
- en: Building a machine learning model using features that have significantly different
    ranges and resolutions (such as age and salary) might pose not only computational
    problems, but also model-convergence and coefficient-interpretability problems.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 使用具有显著不同范围和分辨率的特征（如年龄和工资）构建机器学习模型可能不仅会带来计算问题，还会带来模型收敛和系数可解释性问题。
- en: In this recipe, we will learn how to standardize continuous variables so they
    have a mean of 0 and a standard deviation of 1.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将学习如何标准化连续变量，使它们的平均值为0，标准差为1。
- en: Getting ready
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment. You will
    also have to have executed the previous recipe.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个配方，你需要一个可用的Spark环境。你还必须执行前面的配方。
- en: No other prerequisites are required.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To standardize the `signal` column we introduced in the previous recipe, we
    will use the `.StandardScaler(...)` method:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标准化我们在前面的配方中引入的`signal`列，我们将使用`.StandardScaler(...)`方法：
- en: '[PRE31]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How it works...
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: First, we need to transform the single feature into a vector representation,
    as the `.StandardScaler(...)` method accepts only vectorized features.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将单个特征转换为向量表示，因为`.StandardScaler(...)`方法只接受向量化的特征。
- en: Next, we instantiate the `.StandardScaler(...)` object. The `withMean` parameter
    instructs the method to center the data with the mean, while the `withStd` parameter
    scales to a standard deviation equal to 1.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实例化`.StandardScaler(...)`对象。`withMean`参数指示方法将数据居中到平均值，而`withStd`参数将数据缩放到标准差等于1。
- en: 'Here''s what the standardized representation of our signal look like. Note
    the different scales for the two lines:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们信号的标准化表示的样子。请注意两条线的不同刻度：
- en: '![](img/00162.jpeg)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00162.jpeg)'
- en: Topic mining
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题挖掘
- en: Sometimes, it is necessary to cluster text documents into buckets based on their
    content.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，有必要根据其内容将文本文档聚类到桶中。
- en: In this recipe, we will walk through an example of assigning a topic to a set
    of short paragraphs extracted from Wikipedia.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将通过一个例子来为从维基百科提取的一组短段落分配一个主题。
- en: Getting ready
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you will need a working Spark environment.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个配方，你需要一个可用的Spark环境。
- en: No other prerequisites are required.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In order to cluster the documents, we first need to extract the features from
    our articles. Note that the following text is abbreviated for space considerations—refer
    to the GitHub repository for the full code:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对文档进行聚类，我们首先需要从我们的文章中提取特征。请注意，以下文本由于空间限制而被缩写，有关完整代码，请参考GitHub存储库：
- en: '[PRE32]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: How it works...
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: First, we create a DataFrame with our articles.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个包含我们文章的DataFrame。
- en: 'Next, we go through pretty much the same steps as we went through in the *Extracting
    features from text*recipe:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将几乎按照*从文本中提取特征*配方中的步骤进行操作：
- en: We split the sentences using `.RegexTokenizer(...)`
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`.RegexTokenizer(...)`拆分句子
- en: We remove the stop words using `.StopWordsRemover(...)`
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`.StopWordsRemover(...)`去除停用词
- en: We count each word's occurrence using `.CountVectorizer(...)`
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`.CountVectorizer(...)`计算每个单词的出现次数
- en: To find the clusters in our data, we will use the **Latent Dirichlet Allocation**
    (**LDA**) model. In our case, we know that we expect to have three clusters, but
    if you do not know how many clusters you might have, you can use one of the techniques
    we introduced in the *Tuning hyperparameters* recipe earlier in this chapter.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的数据中找到聚类，我们将使用**潜在狄利克雷分配**（**LDA**）模型。在我们的情况下，我们知道我们希望有三个聚类，但如果你不知道你可能有多少聚类，你可以使用我们在本章前面介绍的*调整超参数*配方之一。
- en: Finally, we put everything in the Pipeline for our convenience.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们把所有东西都放在管道中以方便我们使用。
- en: 'Once the model is estimated, let''s see how it performs. Here''s a piece of
    code that will help us do that; note the NumPy''s `.argmax(...)` method that helps
    us find the index of the highest value:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被估计，让我们看看它的表现。这里有一段代码可以帮助我们做到这一点；注意NumPy的`.argmax(...)`方法，它可以帮助我们找到最高值的索引：
- en: '[PRE33]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here''s what we get back:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们得到的结果：
- en: '![](img/00163.jpeg)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00163.jpeg)'
- en: As you can see, with proper processing, we can properly extract topics from
    the articles; the articles about galaxies are grouped in cluster 2, geographies
    are in cluster 1, and animals are in 0 cluster.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，通过适当的处理，我们可以从文章中正确提取主题；关于星系的文章被分组在第2个聚类中，地理信息在第1个聚类中，动物在第0个聚类中。
