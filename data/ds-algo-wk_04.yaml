- en: Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: A random forest is a set of random decision trees (similar to the ones described
    in [Chapter 3](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml), *Decision Trees*),
    each generated on a random subset of data. A random forest classifies the features
    that belong to the class that is voted for by the majority of the random decision
    trees. Random forests tend to provide a more accurate classification of a feature
    than decision trees because of their decreased bias and variance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是由一组随机决策树组成的（类似于[第3章](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml)中描述的*决策树*），每棵树都是在数据的随机子集上生成的。随机森林通过大多数随机决策树投票所选的类别来分类特征。与决策树相比，随机森林通常能够提供更准确的分类，因为它们的偏差和方差较低。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The tree bagging (or bootstrap aggregation) technique as part of random forest
    construction, but which can also be extended to other algorithms and methods in
    data science in order to reduce bias and variance and, hence, improve accuracy
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为随机森林构建的一部分，树袋法（或自助聚合）技术，但也可以扩展到数据科学中的其他算法和方法，以减少偏差和方差，从而提高准确性
- en: How to construct a random forest and classify a data item using a random forest
    constructed through the swim preference example
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建一个随机森林并使用通过游泳偏好示例构建的随机森林对数据项进行分类
- en: How to implement an algorithm in Python that will construct a random forest
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在 Python 中实现一个构建随机森林的算法
- en: The differences between the analysis of a problem using the Naive Bayes algorithm,
    decision trees, and random forest using the example of playing chess
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用下棋的例子，分析 Naive Bayes 算法、决策树和随机森林的区别
- en: How the random forest algorithm can overcome the shortcomings of the decision
    tree algorithm and thus outperform it using the example of going shopping
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林算法如何克服决策树算法的缺点，并通过购物的例子展示其优势
- en: How a random forest can express level of confidence in its classification of
    a feature using the example of going shopping
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过购物的例子展示随机森林如何表达其对特征分类的置信度
- en: How decreasing the variance of a classifier can yield more accurate results,
    in the *Problems* section
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过减少分类器的方差来获得更准确的结果，在*问题*部分进行讨论
- en: Introduction to the random forest algorithm
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林算法简介
- en: In general, in order to construct a random forest, first we have to choose the
    number of trees that it will contain. A random forest does not tend to overfit
    (unless the data is very noisy), so choosing many decision trees will not decrease
    the accuracy of the prediction. A random forest does not tend to overfit (unless
    the data is very noisy), so having a higher number of decision trees will not
    decrease the accuracy of the prediction. It is important to have a sufficient
    number of decision trees so that more data is used for classification purposes
    when chosen randomly for the construction of a decision tree. On the other hand,
    the more decision trees there are, the more computational power is required. Also,
    increasing the number of decision trees fails to increase the accuracy of the
    classification by any significant degree.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了构建一个随机森林，首先我们需要选择它包含的树的数量。随机森林通常不会过拟合（除非数据非常嘈杂），因此选择更多的决策树不会降低预测的准确性。随机森林通常不会过拟合（除非数据非常嘈杂），因此增加决策树的数量不会显著降低预测准确性。重要的是要选择足够数量的决策树，以便在随机选择用于构建决策树的数据时，更多的数据用于分类。另一方面，树木越多，所需的计算能力越大。此外，增加决策树的数量并不会显著提高分类准确度。
- en: In practice, you can run the algorithm on a specific number of decision trees,
    increase their number, and compare the results of the classification of smaller
    and bigger forests. If the results are very similar, then there is no reason to
    increase the number of trees.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你可以在特定数量的决策树上运行算法，增加它们的数量，并比较较小和较大森林的分类结果。如果结果非常相似，那么就没有必要增加树木的数量。
- en: To simplify the demonstration, throughout this book, we will use a small number
    of decision trees in a random forest.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化演示，本书中我们将使用少量的决策树来构建随机森林。
- en: Overview of random forest construction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林构建概述
- en: We will describe how each tree is constructed in a random fashion. We construct
    a decision tree by selecting *N* training features randomly. This process of selecting
    the data randomly with a replacement for each tree is called **bootstrap aggregating**,
    or **tree bagging**. The purpose of bootstrap aggregating is to reduce the variance
    and bias in the results of the classification.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将描述如何以随机的方式构建每棵树。我们通过随机选择*N*个训练特征来构建决策树。这个从数据中随机选择并进行替换的过程，称为**自助聚合（bootstrap
    aggregating）**，或**树袋法（tree bagging）**。自助聚合的目的是减少分类结果中的方差和偏差。
- en: Say a feature has *M* variables that are used to classify the feature using
    the decision tree. When we have to make a branching decision at a node, in the
    ID3 algorithm, we choose the variable that resulted in the highest information
    gain. Here, in a random decision tree, at each node, we consider only at most
    *m* variables. We do not consider the ones that were already chosen sampled in
    a random fashion without any replacement from the given *M* variables. Then, of
    these *m* variables, we choose the one that results in the highest information
    gain.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个特征有*M*个变量，这些变量用于使用决策树对特征进行分类。当我们在节点上做出分支决策时，在ID3算法中，我们选择产生最大信息增益的变量。在这里，在随机决策树中，每个节点我们最多只考虑*m*个变量。我们不考虑那些已经被随机选择且没有替换的*M*个变量中的变量。然后，在这*m*个变量中，我们选择产生最大信息增益的变量。
- en: The remainder of the construction of a random decision tree is carried out just
    as it was for a decision tree in [Chapter 3](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml), *Decision
    Trees*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随机决策树的其余构建过程与[第3章](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml)中的决策树构建过程相同，参见*决策树*部分。
- en: Swim preference – analysis involving a random forest
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游泳偏好——涉及随机森林的分析
- en: 'We will use the example from [Chapter 3](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml), *Decision
    Trees* concerning swim preferences. We have the same data table, as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自[第3章](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml)的关于游泳偏好的示例。我们有相同的数据表，具体如下：
- en: '| **Swimming suit** | **Water temperature** | **Swim preference** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **泳衣** | **水温** | **游泳偏好** |'
- en: '| None | Cold | No |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 冷 | 否 |'
- en: '| None | Warm | No |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 温暖 | 否 |'
- en: '| Small | Cold | No |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 小 | 冷 | 否 |'
- en: '| Small | Warm | No |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 小 | 温暖 | 否 |'
- en: '| Good | Cold | No |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 好 | 冷 | 否 |'
- en: '| Good | Warm | Yes |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 好 | 温暖 | 是 |'
- en: We would like to construct a random forest from this data and use it to classify
    an item `(Good,Cold,?)`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望从这些数据构建一个随机森林，并使用它来对一个项目`(好,冷,?)`进行分类。
- en: Analysis
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析
- en: We are given *M=3* variables, according to which a feature can be classified.
    In a random forest algorithm, we usually do not use all three variables to form
    tree branches at each node. We only use a subset (*m*) of variables from *M*.
    So we choose *m* such that *m* is less than, or equal to, *M*. The greater *m*
    is, the stronger the classifier is in each constructed tree. However, as mentioned
    earlier, more data leads to more bias. But, because we use multiple trees (with
    a lower *m*), even if each constructed tree is a weak classifier, their combined
    classification accuracy is strong. As we want to reduce bias in a random forest,
    we may want to consider choosing an *m* parameter that is slightly less than *M*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给定了*M=3*个变量，根据这些变量可以对特征进行分类。在随机森林算法中，我们通常不会在每个节点使用所有三个变量来形成树枝。我们只会使用从*M*中随机选择的一个子集(*m*)的变量。所以我们选择*m*使得*m*小于或等于*M*。*m*的值越大，每个构建的树中的分类器就越强。然而，正如前面提到的，更多的数据会导致更多的偏差。但是，因为我们使用多棵树（每棵树的*m*较小），即使每棵构建的树是一个弱分类器，它们的联合分类准确度仍然较强。为了减少随机森林中的偏差，我们可能希望选择一个略小于*M*的*m*参数。
- en: Hence, we choose the maximum number of variables considered at the node to be: *m=min(M,math.ceil(2*math.sqrt(M)))=min(M,math.ceil(2*math.sqrt(3)))=3*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们选择节点中考虑的最大变量数量为：*m=min(M,math.ceil(2*math.sqrt(M)))=min(M,math.ceil(2*math.sqrt(3)))=3*。
- en: 'We are given the following features:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供以下特征：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When constructing a random decision tree as part of a random forest, we will
    choose only a subset of these features randomly, together with their replacements.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建随机森林中的随机决策树时，我们只会随机选择这些特征的一个子集，并且进行替换。
- en: Random forest construction
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林构建
- en: We will construct a random forest that will consist of two random decision trees.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个由两棵随机决策树组成的随机森林。
- en: Construction of random decision tree number 0
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机决策树构建编号0
- en: 'We are given six features as the input data. Of these, we choose six features
    at random with a replacement for the construction of this random decision tree:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到六个特征作为输入数据。在这些特征中，我们随机选择六个特征并带有替换地用于构建这个随机决策树：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We start the construction with the root node to create the first node of the
    tree. We would like to add children to the [root] node.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从根节点开始构建，创建树的第一个节点。我们想要向`[root]`节点添加子节点。
- en: 'We have the following variables available: `[''swimming_suit'', ''water_temperature'']`.
    As there are fewer of these than the `m=3` parameter, we consider both of them.
    Of these variables, the one with the highest information gain is a swimming suit.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下可用变量：`['swimming_suit', 'water_temperature']`。由于这些变量少于`m=3`参数的数量，我们将考虑这两个变量。在这些变量中，信息增益最高的是游泳衣。
- en: 'Therefore, we will branch the node further on this variable. We will also remove
    this variable from the list of available variables for the children of the current
    node. Using the `swimming_suit` variable, we partition the data in the current
    node as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将根据该变量继续分支节点。我们还将从当前节点的子节点可用变量列表中移除此变量。使用`swimming_suit`变量，我们在当前节点上划分数据，如下所示：
- en: 'Partition for `swimming_suit=Small: [[''Small'', ''Cold'', ''No'']]`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`swimming_suit=Small`的划分：`[[''Small'', ''Cold'', ''No'']]`'
- en: 'Partition for `swimming_suit=None: [[''None'', ''Warm'', ''No''], [''None'',
    ''Warm'', ''No'']]`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`swimming_suit=None`的划分：`[[''None'', ''Warm'', ''No''], [''None'', ''Warm'',
    ''No'']]`'
- en: 'Partition for `swimming_suit=Good: [[''Good'', ''Cold'', ''No''], [''Good'',
    ''Cold'', ''No''], [''Good'', ''Cold'', ''No'']]`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`swimming_suit=Good`的划分：`[[''Good'', ''Cold'', ''No''], [''Good'', ''Cold'',
    ''No''], [''Good'', ''Cold'', ''No'']]`'
- en: Using the preceding partitions, we create the branches and the child nodes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述的划分，我们创建了分支和子节点。
- en: 'We now add a child node, `[swimming_suit=Small]`, to the `[root]` node. This
    branch classifies a single feature: `[[''Small'', ''Cold'', ''No'']]`.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们向`[root]`节点添加一个子节点`[swimming_suit=Small]`。该分支对一个特征进行分类：`[['Small', 'Cold',
    'No']]`。
- en: We would like to add children to the `[swimming_suit=Small]` node.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望向`[swimming_suit=Small]`节点添加子节点。
- en: 'We have the following variable available: `[''water_temperature'']`. As there
    is only one variable here, which is less than the `m=3` parameter, we will consider
    this one. The one with the highest information gain is the `water_temperature` variable. Therefore,
    we will branch the node further on this variable. We will also remove this variable
    from the list of available variables for the children of the current node. For
    the chosen variable, `water_temperature`, all the remaining features have the
    same value: `Cold`. So, we end the branch with a leaf node, adding `[swim=No]`.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下可用变量：`['water_temperature']`。由于这里只有一个变量，且少于`m=3`参数，因此我们将考虑这个变量。信息增益最高的是`water_temperature`变量。因此，我们将根据该变量继续分支节点。我们还将从当前节点的子节点可用变量列表中移除此变量。对于选定的变量`water_temperature`，所有剩余特征的值都是`Cold`。因此，我们在此分支的末尾加上一个叶节点，添加`[swim=No]`。
- en: 'We now add a child node, `[swimming_suit=None]`, to the `[root]` node. This
    branch classifies two features: `[[''None'', ''Warm'', ''No''], [''None'', ''Warm'',
    ''No'']]`.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们向`[root]`节点添加一个子节点`[swimming_suit=None]`。该分支对两个特征进行分类：`[['None', 'Warm',
    'No'], ['None', 'Warm', 'No']]`。
- en: We would like to add children to the `[swimming_suit=None]` node.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望向`[swimming_suit=None]`节点添加子节点。
- en: 'We have the following variable available: `[''water_temperature'']`. As there
    is only one variable here, which is less than the `m=3` parameter, we will consider
    this one.The one with the highest information gain is the `water_temperature` variable. Therefore,
    we will branch the node further on this variable. We will also remove this variable
    from the list of available variables for the children of the current node. For
    the chosen variable, `water_temperature`, all the remaining features have the
    same value: `Warm`. So, we end the branch with a leaf node, adding `[swim=No]`.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下可用变量：`['water_temperature']`。由于这里只有一个变量，且少于`m=3`参数，因此我们将考虑这个变量。信息增益最高的是`water_temperature`变量。因此，我们将根据该变量继续分支节点。我们还将从当前节点的子节点可用变量列表中移除此变量。对于选定的变量`water_temperature`，所有剩余特征的值都是`Warm`。因此，我们在此分支的末尾加上一个叶节点，添加`[swim=No]`。
- en: 'We now add a child node, `[swimming_suit=Good]`, to the `[root]` node. This
    branch classifies three features: `[[''Good'', ''Cold'', ''No''], [''Good'', ''Cold'',
    ''No''], [''Good'', ''Cold'', ''No'']]`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们向`[root]`节点添加一个子节点`[swimming_suit=Good]`。该分支对三个特征进行分类：`[['Good', 'Cold',
    'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No']]`
- en: We would like to add children to the `[swimming_suit=Good]` node.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望向`[swimming_suit=Good]`节点添加子节点。
- en: 'We have the following variable available: `[''water_temperature'']`. As there
    is only one variable here, which is less than the `m=3` parameter, we will consider
    this one. The one with the highest information gain is the `water_temperature`
    variable. Therefore, we will branch the node further on this variable. We will
    also remove this variable from the list of available variables for the children
    of the current node. For the chosen variable, `water_temperature`, all the remaining
    features have the same value: `Cold`. So, we end the branch with a leaf node, adding
    `[swim=No]`.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下变量：`['water_temperature']`。由于这里只有一个变量，它小于`m=3`参数，因此我们会考虑这个变量。信息增益最高的变量是`water_temperature`变量。因此，我们将继续在这个变量上分支。我们还会将该变量从当前节点子节点的可用变量列表中移除。对于选择的变量`water_temperature`，所有剩余特征的值相同：`Cold`。所以，我们以叶节点结束这个分支，添加`[swim=No]`。
- en: Now, we have added all the children nodes to the `[root]` node.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经将所有子节点添加到`[root]`节点。
- en: Construction of random decision tree number 1
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机决策树1的构建
- en: 'We are given six features as input data. Of these, we choose six features at
    random with a replacement for the construction of this random decision tree:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了六个特征作为输入数据。我们从中随机选择六个特征，并允许重复，以构建这个随机决策树：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The remainder of the construction of random decision tree number 1 is similar
    to the construction of the previous random decision tree, number 0\. The only
    difference is that the tree is built using a different randomly generated subset
    (as seen previously) of the initial data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随机决策树1的其余构建过程与之前的随机决策树0类似。唯一的区别是，树是使用不同的随机生成子集（如之前所见）构建的。
- en: We begin construction with the root node to create the first node of the tree.
    We would like to add children to the `[root]` node.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从根节点开始构建，以创建树的第一个节点。我们希望向`[root]`节点添加子节点。
- en: We have the following variables available: `['swimming_suit', 'water_temperature']`.
    As there is only one variable here, which is less than the `m=3` parameter, we
    will consider this one. Of these variables, the one with the highest information
    gain is the `swimming_suit` variable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下变量：`['swimming_suit', 'water_temperature']`。由于这里只有一个变量，它小于`m=3`参数，因此我们会考虑这个变量。在这些变量中，信息增益最高的是`swimming_suit`变量。
- en: 'Therefore, we will branch the node further on this variable. We will also remove
    this variable from the list of available variables for the children of the current
    node. Using the `swimming_suit` variable, we partition the data in the current
    node as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将在这个变量上继续分支。我们还会将该变量从当前节点子节点的可用变量列表中移除。使用`swimming_suit`变量，我们将当前节点的数据划分如下：
- en: 'Partition for `swimming_suit=Small: [[''Small'', ''Warm'', ''No'']]`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`swimming_suit=Small`的划分：`[[''Small'', ''Warm'', ''No'']]`'
- en: 'Partition for `swimming_suit=None: [[''None'', ''Warm'', ''No''], [''None'',
    ''Cold'', ''No''], [''None'', ''Warm'', ''No'']]`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`swimming_suit=None`的划分：`[[''None'', ''Warm'', ''No''], [''None'', ''Cold'',
    ''No''], [''None'', ''Warm'', ''No'']]`'
- en: 'Partition for `swimming_suit=Good: [[''Good'', ''Warm'', ''Yes''], [''Good'',
    ''Cold'', ''No'']]`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`swimming_suit=Good`的划分：`[[''Good'', ''Warm'', ''Yes''], [''Good'', ''Cold'',
    ''No'']]`'
- en: Now, given the partitions, let's create the branches and the child nodes. We
    add a child node, `[swimming_suit=Small]`, to the `[root]` node. This branch classifies
    a single feature: `[['Small', 'Warm', 'No']]`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，给定划分后，让我们创建分支和子节点。我们向`[root]`节点添加一个子节点`[swimming_suit=Small]`。这个分支分类了一个特征：`[['Small',
    'Warm', 'No']]`。
- en: We would like to add children to the `[swimming_suit=Small]` node.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望向`[swimming_suit=Small]`节点添加子节点。
- en: 'We have the following variable available:  `[''water_temperature'']`. As there
    is only one variable here, which is less than the `m=3` parameter, we will consider
    this one. The one with the highest information gain is the `water_temperature` variable. Therefore,
    we will branch the node further on this variable. We will also remove this variable
    from the list of available variables for the children of the current node. For
    the chosen variable, `water_temperature`, all the remaining features have the
    same value: `Warm`. So, we end the branch with a leaf node, adding `[swim=No]`.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下变量：`['water_temperature']`。由于这里只有一个变量，它小于`m=3`参数，因此我们会考虑这个变量。信息增益最高的变量是`water_temperature`变量。因此，我们将继续在这个变量上分支。我们还会将该变量从当前节点子节点的可用变量列表中移除。对于选择的变量`water_temperature`，所有剩余特征的值相同：`Warm`。所以，我们以叶节点结束这个分支，添加`[swim=No]`。
- en: 'We add a child node, `[swimming_suit=None]`, to the `[root]` node. This branch
    classifies three features: `[[''None'', ''Warm'', ''No'']`, `[''None'', ''Cold'',
    ''No'']`, and `[''None'', ''Warm'', ''No'']]`.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个子节点`[swimming_suit=None]`添加到`[root]`节点。此分支分类了三个特征：`[['None', 'Warm', 'No']]`、`[['None',
    'Cold', 'No']]`和`[['None', 'Warm', 'No']]`。
- en: We would like to add children to the `[swimming_suit=None]` node.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要为`[swimming_suit=None]`节点添加子节点。
- en: 'We have the following variable available: `[''water_temperature'']`. As there
    is only one variable here, which is less than the `m=3` parameter, we will consider
    this one. The one with the highest information gain is the `water_temperature` variable. Therefore,
    we will branch the node further on this variable. We will also remove this variable
    from the list of available variables for the children of the current node. Using
    the `water temperature` variable, we partition the data in the current node as
    follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下可用的变量：`['water_temperature']`。由于这里只有一个变量，这个数量小于`m=3`参数，我们将考虑这个变量。信息增益最高的是`water_temperature`变量。因此，我们将基于这个变量进一步拆分节点。我们还将从当前节点的子节点的可用变量列表中移除此变量。使用`water_temperature`变量，我们将数据在当前节点中进行如下分区：
- en: 'Partition for `water_temperature=Cold: [[''None'', ''Cold'', ''No'']]`'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`water_temperature=Cold`的分区：`[[''None'', ''Cold'', ''No'']]`。'
- en: 'Partition for `water_temperature=Warm: [[''None'', ''Warm'', ''No''], [''None'',
    ''Warm'', ''No'']]`; now, given the partitions, let''s create the branches and
    the child nodes'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`water_temperature=Warm`的分区：`[[''None'', ''Warm'', ''No'']]`；现在，根据这些分区，让我们创建分支和子节点。'
- en: 'We add a child node, `[water_temperature=Cold]`, to the `[swimming_suit=None]` node. This
    branch classifies a single feature: `[[''None'', ''Cold'', ''No'']]`.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个子节点`[water_temperature=Cold]`添加到`[swimming_suit=None]`节点。此分支分类了一个特征：`[['None',
    'Cold', 'No']]`。
- en: 'We do not have any available variables on which we could split the node further;
    therefore, we add a leaf node to the current branch of the tree. We add the `[swim=No]` leaf
    node. We add a child node, `[water_temperature=Warm]`, to the `[swimming_suit=None]` node. This
    branch classifies two features: `[[''None'', ''Warm'', ''No'']`, and  `[''None'',
    ''Warm'', ''No'']]`.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有任何可用的变量可以进一步拆分该节点；因此，我们在当前树分支上添加一个叶节点。我们添加了`[swim=No]`叶节点。我们将一个子节点`[water_temperature=Warm]`添加到`[swimming_suit=None]`节点。此分支分类了两个特征：`[['None',
    'Warm', 'No']]`和`[['None', 'Warm', 'No']]`。
- en: We do not have any available variables on which we could split the node further;
    therefore, we add a leaf node to the current branch of the tree. We add the `[swim=No]`
    leaf node.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有任何可用的变量可以进一步拆分该节点；因此，我们在当前树分支上添加一个叶节点。我们添加了`[swim=No]`叶节点。
- en: Now, we have added all the children nodes to the `[swimming_suit=None]` node.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经将所有子节点添加到`[swimming_suit=None]`节点。
- en: 'We add a child node, `[swimming_suit=Good]`, to the `[root]` node. This branch
    classifies two features: `[[''Good'', ''Warm'', ''Yes'']`, and `[''Good'', ''Cold'',
    ''No'']]`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个子节点`[swimming_suit=Good]`添加到`[root]`节点。此分支分类了两个特征：`[['Good', 'Warm', 'Yes']]`和`[['Good',
    'Cold', 'No']]`。
- en: We would like to add children to the `[swimming_suit=Good]` node.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要为`[swimming_suit=Good]`节点添加子节点。
- en: 'We have the following variable available: `[''water_temperature'']`. As there
    is only one variable here, which is less than the `m=3` parameter, we will consider
    this one. The one with the highest information gain is the  `water_temperature` variable. Therefore,
    we will branch the node further on this variable. We will also remove this variable
    from the list of available variables for the children of the current node. Using
    the `water temperature` variable, we partition the data in the current node as
    follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下可用的变量：`['water_temperature']`。由于这里只有一个变量，这个数量小于`m=3`参数，我们将考虑这个变量。信息增益最高的是`water_temperature`变量。因此，我们将基于这个变量进一步拆分节点。我们还将从当前节点的子节点的可用变量列表中移除此变量。使用`water_temperature`变量，我们将数据在当前节点中进行如下分区：
- en: 'Partition for `water_temperature=Cold: [[''Good'', ''Cold'', ''No'']]`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`water_temperature=Cold`的分区：`[[''Good'', ''Cold'', ''No'']]`。'
- en: 'Partition for `water_temperature=Warm: [[''Good'', ''Warm'', ''Yes'']]`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`water_temperature=Warm`的分区：`[[''Good'', ''Warm'', ''Yes'']]`。'
- en: Now, given the partitions, let's create the branches and the child nodes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据这些分区，让我们创建分支和子节点。
- en: 'We add a child node, `[water_temperature=Cold]`, to the `[swimming_suit=Good]` node. This
    branch classifies a single feature: `[[''Good'', ''Cold'', ''No'']]`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个子节点`[water_temperature=Cold]`添加到`[swimming_suit=Good]`节点。此分支分类了一个特征：`[['Good',
    'Cold', 'No']]`。
- en: We do not have any available variables on which we could split the node further;
    therefore, we add a leaf node to the current branch of the tree. We add the `[swim=No]`
    leaf node.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有任何可用的变量来进一步划分节点；因此，我们向当前树的分支添加了一个叶节点。我们添加了`[swim=No]`叶节点。
- en: 'We add a child node, `[water_temperature=Warm]`, to the `[swimming_suit=Good]` node. This
    branch classifies a single feature: `[[''Good'', ''Warm'', ''Yes'']]`.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`[swimming_suit=Good]`节点下添加了一个子节点`[water_temperature=Warm]`。该分支分类一个特征：`[['Good',
    'Warm', 'Yes']]`。
- en: We do not have any available variables on which we could split the node further;
    therefore, we add a leaf node to the current branch of the tree. We add the `[swim=Yes]`
    leaf node.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有任何可用的变量来进一步划分节点；因此，我们向当前树的分支添加了一个叶节点。我们添加了`[swim=Yes]`叶节点。
- en: Now, we have added all the children nodes of the `[swimming_suit=Good]` node.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经添加了`[swimming_suit=Good]`节点的所有子节点。
- en: We have also added all the children nodes to the `[root]` node.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也已经添加了`[root]`节点的所有子节点。
- en: Constructed random forest
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建的随机森林
- en: 'We have completed the construction of the random forest, consisting of two
    random decision trees, as shown in the following code block:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了随机森林的构建，随机森林由两个随机决策树组成，如下方代码块所示：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Classification using random forest
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林进行分类
- en: Because we use only a subset of the original data for the construction of the
    random decision tree, we may not have enough features to form a full tree that
    is able to classify every feature. In such cases, a tree will not return any class
    for a feature that should be classified. Therefore, we will only consider trees
    that classify a feature of a specific class.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们只使用了原始数据的一个子集来构建随机决策树，所以可能没有足够的特征来构建能够分类每个特征的完整树。在这种情况下，树不会为应该分类的特征返回任何类别。因此，我们只会考虑能够分类特定类别特征的树。
- en: The feature we would like to classify is `['Good', 'Cold', '?']`. A random decision
    tree votes for the class to which it classifies a given feature using the same
    method to classify a feature as in [Chapter 3](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml),
    *Decision Trees*. Tree 0 votes for the `No` class. Tree 1 votes for the `No` class. The
    class with the maximum number of votes is `No`. Therefore, the constructed random
    forest classifies the feature `['Good', 'Cold', '?']` according to the class `No`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望分类的特征是`['Good', 'Cold', '?']`。随机决策树通过相同的方法对特征进行分类，就像在[第3章](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml)《决策树》中那样。树0为`No`类别投票。树1为`No`类别投票。获得最多投票的类别是`No`。因此，构建的随机森林将特征`['Good',
    'Cold', '?']`分类为`No`类别。
- en: Implementation of the random forest algorithm
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林算法的实现
- en: We implement a random forest algorithm using a modified decision tree algorithm
    from the previous chapter. We also add an option to set a verbose mode within
    the program that can describe the whole process of how the algorithm works on
    a specific input—how a random forest is constructed with its random decision trees,
    and how this constructed random forest is used to classify other features.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上一章节中的修改版决策树算法实现了一个随机森林算法。我们还在程序中添加了一个选项，可以设置详细模式，描述算法在特定输入上的整个工作过程——如何用随机决策树构建一个随机森林，以及如何使用这个构建好的随机森林来分类其他特征。
- en: 'You are encouraged to consult the `decision_tree.construct_general_tree`  function from
    the previous chapter:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励你参考上一章节中的`decision_tree.construct_general_tree`函数：
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Input**:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: 'As an input file to the implemented algorithm, we provide the data from the
    swim preference example:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作为已实现算法的输入文件，我们提供了来自游泳偏好示例的数据：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Output**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: 'We type the following command in the command line to get the output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在命令行中输入以下命令来获取输出：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`2` means that we would like to construct two decision trees, and `3` is the
    level of the verbosity of the program, which includes detailed explanations of
    the construction of the random forest, the classification of the feature, and
    the graph of the random forest. The last part, `> swim.out`, means that the output
    is written to the `swim.out` file. This file can be found in the chapter directory
    `source_code/4`. This output of the program was used previously to write the analysis
    of the swim preference problem.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`2`表示我们希望构建两棵决策树，`3`表示程序的详细级别，包含了构建随机森林、分类特征以及随机森林图的详细解释。最后一部分，`> swim.out`，表示输出被写入到`swim.out`文件中。此文件可以在章节目录`source_code/4`中找到。程序的输出之前曾用于撰写游泳偏好问题的分析。'
- en: Playing chess example
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下棋示例
- en: 'We will again use the examples from Chapter 2, *Naive Bayes,* and [Chapter
    3](4f3fafce-e9a1-4593-bd9d-847a94cde2bf.xhtml), *Decision Tree*, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用第二章的示例，*朴素贝叶斯*，以及[第三章](4f3fafce-e9a1-4593-bd9d-847a94cde2bf.xhtml)，*决策树*，如下所示：
- en: '| **Temperature** | **Wind** | **Sunshine** | **Play** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **温度** | **风** | **阳光** | **是否玩耍** |'
- en: '| Cold | Strong | Cloudy | No |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 寒冷 | 强风 | 多云 | 否 |'
- en: '| Warm | Strong | Cloudy | No |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 强风 | 多云 | 否 |'
- en: '| Warm | None | Sunny | Yes |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无风 | 阳光 | 是 |'
- en: '| Hot | None | Sunny | No |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 热 | 无风 | 阳光 | 否 |'
- en: '| Hot | Breeze | Cloudy | Yes |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 热 | 微风 | 多云 | 是 |'
- en: '| Warm | Breeze | Sunny | Yes |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 微风 | 阳光 | 是 |'
- en: '| Cold | Breeze | Cloudy | No |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 寒冷 | 微风 | 多云 | 否 |'
- en: '| Cold | None | Sunny | Yes |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 寒冷 | 无风 | 阳光 | 是 |'
- en: '| Hot | Strong | Cloudy | Yes |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 热 | 强风 | 多云 | 是 |'
- en: '| Warm | None | Cloudy | Yes |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无风 | 多云 | 是 |'
- en: '| Warm | Strong | Sunny | ? |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 强风 | 阳光 | ? |'
- en: However, we would like to use a random forest consisting of four random decision
    trees to find the result of the classification.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们希望使用由四棵随机决策树组成的随机森林来进行分类结果的预测。
- en: Analysis
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析
- en: 'We are given *M=4* variables from which a feature can be classified. Thus,
    we choose the maximum number of the variables considered at the node to:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了*M=4*个变量用于特征分类。因此，我们选择在节点中考虑的变量的最大数量为：
- en: '![](img/67f7de1d-b134-48f2-9de2-fc61f30107c0.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67f7de1d-b134-48f2-9de2-fc61f30107c0.png)'
- en: 'We are given the following features:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了以下特征：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When constructing a random decision tree as part of a random forest, we will
    choose only a subset of these features randomly, together with their replacements.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建随机森林中的随机决策树时，我们将仅随机选择这些特征中的一部分，并对其进行替换。
- en: Random forest construction
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林构建
- en: We will construct a random forest that will consist of four random decision
    trees.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个随机森林，由四棵随机决策树组成。
- en: '**Construction of random decision tree number 0**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机决策树构建 0**'
- en: 'We are given 10 features as input data. Of these, we choose all features randomly
    with their replacements for the construction of this random decision tree:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了10个特征作为输入数据。在这些特征中，我们随机选择所有特征及其替换项来构建这棵随机决策树：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We start the construction with the root node to create the first node of the
    tree. We would like to add children to the `[root]` node.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从根节点开始构建，以创建树的第一个节点。我们希望为`[root]`节点添加子节点。
- en: 'We have the following variables available: `[''Temperature'', ''Wind'', ''Sunshine'']`.
    As there are fewer of them than the `m=4` parameter, we consider all of them.
    Of these variables, the one with the highest information gain is the `Temperature` variable. Therefore,
    we will branch the node further on this variable. We will also remove this variable
    from the list of available variables for the children of the current node. Using
    the `Temperature` variable, we partition the data in the current node as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下可用的变量：`['Temperature', 'Wind', 'Sunshine']`。由于这些变量的数量少于`m=4`的参数，因此我们考虑所有的变量。在这些变量中，信息增益最高的是`Temperature`变量。因此，我们将基于此变量进一步分支当前节点。同时，我们会将该变量从当前节点的子节点可用变量列表中移除。利用`Temperature`变量，我们将当前节点的数据划分如下：
- en: 'Partition for `Temperature=Cold: [[''Cold'', ''Breeze'', ''Cloudy'', ''No''],
    [''Cold'', ''None'', ''Sunny'', ''Yes''], [''Cold'', ''Breeze'', ''Cloudy'', ''No''],
    [''Cold'', ''Breeze'', ''Cloudy'', ''No'']]`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Temperature=Cold`的划分：`[[''Cold'', ''Breeze'', ''Cloudy'', ''No''], [''Cold'',
    ''None'', ''Sunny'', ''Yes''], [''Cold'', ''Breeze'', ''Cloudy'', ''No''], [''Cold'',
    ''Breeze'', ''Cloudy'', ''No'']]`'
- en: 'Partition for `Temperature=Warm: [[''Warm'', ''Strong'', ''Cloudy'', ''No''],
    [''Warm'', ''Strong'', ''Cloudy'', ''No''], [''Warm'', ''Breeze'', ''Sunny'',
    ''Yes'']]`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Temperature=Warm`的划分：`[[''Warm'', ''Strong'', ''Cloudy'', ''No''], [''Warm'',
    ''Strong'', ''Cloudy'', ''No''], [''Warm'', ''Breeze'', ''Sunny'', ''Yes'']]`'
- en: 'Partition for `Temperature=Hot: [[''Hot'', ''Breeze'', ''Cloudy'', ''Yes''],
    [''Hot'', ''Breeze'', ''Cloudy'', ''Yes''], [''Hot'', ''Breeze'', ''Cloudy'',
    ''Yes'']]`'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Temperature=Hot`的划分：`[[''Hot'', ''Breeze'', ''Cloudy'', ''Yes''], [''Hot'',
    ''Breeze'', ''Cloudy'', ''Yes''], [''Hot'', ''Breeze'', ''Cloudy'', ''Yes'']]`'
- en: Now, given the partitions, let's create the branches and the child nodes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，给定这些划分，让我们创建分支和子节点。
- en: We add a child node, `[Temperature=Cold]`, to the `[root]` node. This branch
    classifies four features: `[['Cold', 'Breeze', 'Cloudy', 'No']`, `['Cold', 'None',
    'Sunny', 'Yes']`, `['Cold', 'Breeze', 'Cloudy', 'No'],`  and ` ['Cold', 'Breeze',
    'Cloudy', 'No']]`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为`[root]`节点添加一个子节点`[Temperature=Cold]`。此分支对四个特征进行分类：`[['Cold', 'Breeze', 'Cloudy',
    'No']`，`['Cold', 'None', 'Sunny', 'Yes']`，`['Cold', 'Breeze', 'Cloudy', 'No']`，`['Cold',
    'Breeze', 'Cloudy', 'No']]`。
- en: We would like to add children to the `[Temperature=Cold]` node.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望为`[Temperature=Cold]`节点添加子节点。
- en: 'We have the following variables available: `[''Wind'', ''Sunshine'']`. As there
    are fewer of these than the `m=3` parameter, we consider both of them. The one
    with the highest information gain is the `Wind` variable. Therefore, we will branch
    the node further on this variable. We will also remove this variable from the
    list of available variables for the children of the current node. Using the water
    `Wind` variable, we partition the data in the current node as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下可用变量：`['Wind', 'Sunshine']`。由于这些变量的数量少于`m=3`参数，我们考虑了这两个变量。信息增益最高的是`Wind`变量。因此，我们将在此变量上进一步分支。我们还会将该变量从当前节点子节点的可用变量列表中移除。使用`Wind`变量，我们将当前节点的数据划分如下：
- en: 'Partition for `Wind=None: [[''Cold'', ''None'', ''Sunny'', ''Yes'']]`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Wind=None`的划分为：`[[''Cold'', ''None'', ''Sunny'', ''Yes'']]`'
- en: 'Partition for `Wind=Breeze: [[''Cold'', ''Breeze'', ''Cloudy'', ''No''], [''Cold'',
    ''Breeze'', ''Cloudy'', ''No''], [''Cold'', ''Breeze'', ''Cloudy'', ''No'']]`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Wind=Breeze`的划分为：`[[''Cold'', ''Breeze'', ''Cloudy'', ''No''], [''Cold'',
    ''Breeze'', ''Cloudy'', ''No''], [''Cold'', ''Breeze'', ''Cloudy'', ''No'']]`'
- en: Now, given the partitions, let's create the branches and the child nodes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据这些划分，让我们创建分支和子节点。
- en: 'We add a child node, `[Wind=None]`, to the `[Temperature=Cold]` node. This
    branch classifies a single feature: `[[''Cold'', ''None'', ''Sunny'', ''Yes'']]`.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`[Temperature=Cold]`节点下添加了一个子节点`[Wind=None]`。这个分支对单一特征进行分类：`[['Cold', 'None',
    'Sunny', 'Yes']]`。
- en: We would like to add children to the `[Wind=None]` node.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要在`[Wind=None]`节点下添加子节点。
- en: 'We have the following variable: `available[''Sunshine'']`. As there are fewer
    of these than the `m=3` parameter, we consider both of them. The one with the
    highest information gain is the `Sunshine` variable. Therefore, we will branch
    the node further on this variable. We will also remove this variable from the
    list of available variables for the children of the current node. For the chosen
    variable, `Sunshine`, all the remaining features have the same value: `Sunny`.
    So, we end the branch with a leaf node, adding `[Play=Yes]`.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下变量：`available['Sunshine']`。由于这些变量的数量少于`m=3`参数，我们考虑了这两个变量。信息增益最高的是`Sunshine`变量。因此，我们将在此变量上进一步分支。我们还会将该变量从当前节点子节点的可用变量列表中移除。对于所选变量`Sunshine`，所有剩余特征的值相同：`Sunny`。因此，我们将分支终止，并添加叶子节点`[Play=Yes]`。
- en: 'We add a child node, `[Wind=Breeze]`, to the `[Temperature=Cold]` node. This
    branch classifies three features: `[[''Cold'', ''Breeze'', ''Cloudy'', ''No''],
    [''Cold'', ''Breeze'', ''Cloudy'', ''No''],`  and ` [''Cold'', ''Breeze'', ''Cloudy'',
    ''No'']]`.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`[Temperature=Cold]`节点下添加了一个子节点`[Wind=Breeze]`。这个分支对三个特征进行分类：`[['Cold', 'Breeze',
    'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy',
    'No']]`。
- en: We would like to add children to the `[Wind=Breeze]` node.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要在`[Wind=Breeze]`节点下添加子节点。
- en: 'We have the following variable available: `[''Sunshine'']`. As there are fewer
    of these than the `m=3` parameter, we consider both of them. The one with the
    highest information gain is the `Sunshine` variable. Therefore, we will branch
    the node further on this variable. We will also remove this variable from the
    list of available variables for the children of the current node. For the chosen
    variable, Sunshine, all the remaining features have the same value: Cloudy. So,
    we end the branch with a leaf node, adding [Play=No].'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下可用变量：`['Sunshine']`。由于这些变量的数量少于`m=3`参数，我们考虑了这两个变量。信息增益最高的是`Sunshine`变量。因此，我们将在此变量上进一步分支。我们还会将该变量从当前节点子节点的可用变量列表中移除。对于所选变量`Sunshine`，所有剩余特征的值相同：`Cloudy`。因此，我们将分支终止，并添加叶子节点`[Play=No]`。
- en: Now, we have added all the children nodes for the `[Temperature=Cold]` node.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经为`[Temperature=Cold]`节点添加了所有子节点。
- en: 'We add a child node, `[Temperature=Warm]`, to the `[root]` node. This branch
    classifies three features: `[[''Warm'', ''Strong'', ''Cloudy'', ''No''], [''Warm'',
    ''Strong'', ''Cloudy'', ''No''], ` and  ` [''Warm'', ''Breeze'', ''Sunny'', ''Yes'']]`.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`[root]`节点下添加了一个子节点`[Temperature=Warm]`。这个分支对三个特征进行分类：`[['Warm', 'Strong',
    'Cloudy', 'No'], ['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'Breeze', 'Sunny',
    'Yes']]`。
- en: We would like to add children to the `[Temperature=Warm]` node.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要在`[Temperature=Warm]`节点下添加子节点。
- en: 'The available variables that we still have left are `[''Wind'', ''Sunshine'']`.
    As there are fewer of these than the `m=3` parameter, we consider both of them.
    The one with the highest information gain is the `Wind` variable. Thus, we will
    branch the node further on this variable. We will also remove this variable from
    the list of available variables for the children of the current node. Using the
    `Wind` variable, we partition the data in the current node, where each partition
    of the data will be for one of the new branches from the current node, `[Temperature=Warm]`.
    We have the following partitions:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然剩下的可用变量是`['Wind', 'Sunshine']`。由于这些变量的数量少于`m=3`的参数，我们考虑这两个变量。信息增益最高的是`Wind`变量。因此，我们将在这个变量上进一步分支节点。我们还会从当前节点的可用变量列表中移除这个变量。使用`Wind`变量，我们将当前节点中的数据进行划分，每个数据分区将对应当前节点的一个新分支，`[Temperature=Warm]`。我们得到以下分区：
- en: 'Partition for `Wind=Breeze: [[''Warm'', ''Breeze'', ''Sunny'', ''Yes'']]`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Wind=Breeze`的分区为：`[[''Warm'', ''Breeze'', ''Sunny'', ''Yes'']]`'
- en: 'Partition for `Wind=Strong: [[''Warm'', ''Strong'', ''Cloudy'', ''No''], [''Warm'',
    ''Strong'', ''Cloudy'', ''No'']]`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Wind=Strong`的分区为：`[[''Warm'', ''Strong'', ''Cloudy'', ''No''], [''Warm'',
    ''Strong'', ''Cloudy'', ''No'']]`'
- en: Now, given the partitions, let's form the branches and the child nodes.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据这些分区，让我们形成分支和子节点。
- en: 'We add a child node, `[Wind=Breeze]`, to the `[Temperature=Warm]` node. This
    branch classifies a single feature: `[[''Warm'', ''Breeze'', ''Sunny'', ''Yes'']]`.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`[Temperature=Warm]`节点上添加一个子节点，`[Wind=Breeze]`。这个分支对一个特征进行分类：`[['Warm', 'Breeze',
    'Sunny', 'Yes']]`。
- en: We would like to add children to the `[Wind=Breeze]` node.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望为`[Wind=Breeze]`节点添加子节点。
- en: 'We have the following variable available: `[''Sunshine'']`. As there are fewer
    of these than the `m=3` parameter, we consider both of them. The one with the
    highest information gain is the `Sunshine` variable. Therefore, we will branch
    the node further on this variable. We will also remove this variable from the
    list of available variables for the children of the current node. For the chosen
    variable, `Sunshine`, all the remaining features have the same value: `Sunny`.
    So, we end the branch with a leaf node, adding `[Play=Yes]`.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们剩下的可用变量是`['Sunshine']`。由于这些变量的数量少于`m=3`的参数，我们考虑这两个变量。信息增益最高的是`Sunshine`变量。因此，我们将在这个变量上进一步分支节点。我们还会从当前节点的可用变量列表中移除这个变量。对于选择的变量`Sunshine`，所有剩余特征的值相同：`Sunny`。所以，我们以一个叶子节点结束这个分支，添加`[Play=Yes]`。
- en: 'We add a child node, `[Wind=Strong]`, to the `[Temperature=Warm]` node. This
    branch classifies two features: `[[''Warm'', ''Strong'', ''Cloudy'', ''No''],`
     and ` [''Warm'', ''Strong'', ''Cloudy'', ''No'']]`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`[Temperature=Warm]`节点上添加一个子节点，`[Wind=Strong]`。这个分支对两个特征进行分类：`[['Warm', 'Strong',
    'Cloudy', 'No'],` 和 `['Warm', 'Strong', 'Cloudy', 'No']]`
- en: We would like to add children to the `[Wind=Strong]` node.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望为`[Wind=Strong]`节点添加子节点。
- en: 'We have the following variable available: `[''Sunshine'']`. As there are fewer
    of these than the `m=3` parameter, we consider both of them. The one with the
    highest information gain is the `Sunshine` variable. Therefore, we will branch
    the node further on this variable. We will also remove this variable from the
    list of available variables for the children of the current node. For the chosen
    variable, `Sunshine`, all the remaining features have the same value: `Cloudy`.
    So, we end the branch with a leaf node, adding `[Play=No]`.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们剩下的可用变量是`['Sunshine']`。由于这些变量的数量少于`m=3`的参数，我们考虑这两个变量。信息增益最高的是`Sunshine`变量。因此，我们将在这个变量上进一步分支节点。我们还会从当前节点的可用变量列表中移除这个变量。对于选择的变量`Sunshine`，所有剩余特征的值相同：`Cloudy`。所以，我们以一个叶子节点结束这个分支，添加`[Play=No]`。
- en: Now, we have added all the children nodes to the `[Temperature=Warm]` node.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经为`[Temperature=Warm]`节点添加了所有子节点。
- en: 'We add a child node, `[Temperature=Hot]`, to the `[root]` node. This branch
    classifies three features: `[[''Hot'', ''Breeze'', ''Cloudy'', ''Yes''], [''Hot'',
    ''Breeze'', ''Cloudy'', ''Yes''],`  and ` [''Hot'', ''Breeze'', ''Cloudy'', ''Yes'']]`.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`[root]`节点上添加一个子节点，`[Temperature=Hot]`。这个分支对三个特征进行分类：`[['Hot', 'Breeze',
    'Cloudy', 'Yes'], ['Hot', 'Breeze', 'Cloudy', 'Yes'],` 和 `['Hot', 'Breeze', 'Cloudy',
    'Yes']]`。
- en: We would like to add children to the  `[Temperature=Hot]` node.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望为`[Temperature=Hot]`节点添加子节点。
- en: 'We have the following variables available: `[''Wind'', ''Sunshine'']`. As there
    are fewer of these than the `m=3` parameter, we consider both of them. The one with
    the highest information gain is the `Wind` variable. Therefore, we will branch
    the node further on this variable. We will also remove this variable from the
    list of available variables for the children of the current node. For the chosen
    variable, `Wind`, all the remaining features have the same value: `Breeze`. So,
    we end the branch with a leaf node, adding `[Play=Yes]`.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下变量：`['风', '阳光']`。由于这些变量少于`m=3`参数，所以我们考虑这两个变量。信息增益最高的是`风`变量。因此，我们将在该变量上进一步分支节点。我们还将从当前节点的可用变量列表中移除此变量。对于选择的变量`风`，所有剩余的特征值相同：`微风`。因此，我们以一个叶子节点结束该分支，添加`[Play=Yes]`。
- en: Now, we have added all the children nodes to the `[root]` node.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经将所有子节点添加到`[root]`节点。
- en: '**Construction of random decision trees numbers 1, 2, and 3**'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建随机决策树1、2、3**'
- en: We construct the next three trees in a similar fashion. We should note that,
    since the construction is random, a reader who performs another correct construction
    may arrive at a different construction. However, if there are a sufficient number
    of random decision trees in a random forest, then the result of the classification
    should be very similar across all the random constructions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以类似的方式构建接下来的三棵树。需要注意的是，由于构建是随机的，执行另一个正确构建的读者可能会得到不同的结果。然而，如果在随机森林中有足够数量的随机决策树，那么分类结果应该在所有随机构建中非常相似。
- en: The full construction can be found in the program output in the `source_code/4/chess.out` file.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的构建可以在`source_code/4/chess.out`文件中的程序输出中找到。
- en: '**Constructed random forest**:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建的随机森林**：'
- en: '[PRE9]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Classification
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: 'Given the random forest constructed, we classify the  `[''Warm'', ''Strong'',
    ''Sunny'', ''?'']` feature:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 给定构建的随机森林，我们将特征`['Warm', 'Strong', 'Sunny', '?']`分类：
- en: '**Tree 0 votes for the class**: `No`'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树 0 投票给类别**：`No`'
- en: '**Tree 1 votes for the class**: `No`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树 1 投票给类别**：`No`'
- en: '**Tree 2 votes for the class**: `No`'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树 2 投票给类别**：`No`'
- en: '**Tree 3 votes for the class**: `No`'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树 3 投票给类别**：`No`'
- en: The class with the maximum number of votes is `No`. Thus, the constructed random
    forest classifies the feature `['Warm', 'Strong', 'Sunny', '?']` into the class
    `No`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有最多投票数的类别是`No`。因此，构建的随机森林将特征`['Warm', 'Strong', 'Sunny', '?']`分类为`No`类别。
- en: '**Input**:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: 'To perform the preceding analysis, we use a program implemented earlier in
    this chapter. First, we insert the data from the table into the following CSV
    file:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行前面的分析，我们使用了本章早些时候实现的程序。首先，我们将表格中的数据插入到以下CSV文件中：
- en: '[PRE10]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Output**:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: 'We produce the output by executing the following command line:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过执行以下命令行来生成输出：
- en: '[PRE11]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The number `4` here means that we want to construct four decision trees, and
    `2` is the level of verbosity of the program that includes the explanations of
    how a tree is constructed. The last part, `> chess.out`, means that the output
    is written to the `chess.out` file. This file can be found in the chapter directory
    `source_code/4`. We will not put all the output here, as it is very large and
    repetitive. Instead, some of it was included in the preceding analysis and in
    the construction of a random forest.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的数字`4`表示我们希望构建四棵决策树，而`2`是程序的冗长程度，包含了树是如何构建的解释。最后一部分，`> chess.out`，意味着输出会写入到`chess.out`文件中。该文件可以在章节目录`source_code/4`中找到。我们不会将所有的输出都放在这里，因为它非常大且重复。相反，其中一些内容已经包含在前面的分析中，以及在随机森林的构建中。
- en: Going shopping – overcoming data inconsistencies with randomness and measuring
    the level of confidence
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去购物——通过随机性克服数据不一致性并测量置信度
- en: 'We take the problem from the previous chapter. We have the following data relating
    to the shopping preferences of our friend, Jane:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从上一章的问题开始。我们有关于我们朋友简（Jane）购物偏好的以下数据：
- en: '| **Temperature** | **Rain** | **Shopping** |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **温度** | **雨** | **购物** |'
- en: '| Cold | None | Yes |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 无 | 是 |'
- en: '| Warm | None | No |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无 | 否 |'
- en: '| Cold | Strong | Yes |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 强 | 是 |'
- en: '| Cold | None | No |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 无 | 否 |'
- en: '| Warm | Strong | No |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 强 | 否 |'
- en: '| Warm | None | Yes |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无 | 是 |'
- en: '| Cold | None | ? |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 冷 | 无 | ? |'
- en: In the previous chapter, decision trees were not able to classify the feature
    `(Cold, None)`. So, this time, we would like to establish whether Jane would go
    shopping if the temperature was cold and there was no rain using the random forest
    algorithm.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，决策树无法对特征 `(Cold, None)` 进行分类。因此，这次我们希望使用随机森林算法来判断，如果温度寒冷且没有降雨，简会不会去购物。
- en: Analysis
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析
- en: To perform an analysis using the random forest algorithm, we use the program
    implemented.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用随机森林算法进行分析，我们使用已实现的程序。
- en: '**Input**:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: 'We insert the data from the table into the following CSV file:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将表格中的数据插入以下CSV文件：
- en: '[PRE12]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Output**:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: 'We want to use a slightly higher number of trees than we used in the previous
    examples and explanations to obtain more accurate results. We want to construct
    a random forest with 20 trees with low-verbosity output – level 0\. Thus, we undertake
    execution in a terminal:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用比前面的示例和解释中更多的树来获得更精确的结果。我们希望构建一个包含20棵树的随机森林，并且输出的详细程度为低级别——级别0。因此，我们在终端中执行以下操作：
- en: '[PRE13]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: However, we should note that only 12 out of the 20 trees voted for the answer
    `Yes`. Therefore, although we have a definite answer, it might not be that certain,
    similar to the results we got with an ordinary decision tree. However, unlike
    in decision trees, where an answer was not produced because of data inconsistency,
    here, we have an answer.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们应该注意到，只有20棵树中的12棵投了“是”的票。因此，虽然我们有了明确的答案，但它可能不那么确定，类似于我们使用普通决策树时得到的结果。然而，与决策树中的数据不一致导致无法得出答案不同的是，在这里我们得到了一个答案。
- en: Furthermore, by measuring the strength of the voting power for each individual
    class, we can measure the level of confidence that the answer is correct. In this
    case, the feature `['Cold', 'None', '?']` belongs to the `*Yes*` class with a
    confidence level of 12/20, or 60%. To determine the level of certainty of the
    classification more precisely, an even larger ensemble of random decision trees
    would be required.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过衡量每个类别的投票权重，我们可以衡量答案正确的置信度。在此案例中，特征`['Cold', 'None', '?']`属于`*Yes*`类别，置信度为12/20，或者60%。为了更精确地确定分类的确定性，需要使用一个更大的随机决策树集成。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned that a random forest is a set of decision trees,
    where each tree is constructed from a sample chosen randomly from the initial
    data. This process is called **bootstrap aggregating**. Its purpose is to reduce
    variance and bias in classifications made by a random forest. Bias is further
    reduced during the construction of a decision tree by considering only a random
    subset of the variables for each branch of the tree.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解到随机森林是一组决策树，其中每棵树都是从初始数据中随机选择的样本构建的。这个过程叫做**自助聚合**。它的目的是减少随机森林在分类时的方差和偏差。在构建决策树的过程中，通过仅考虑每个分支的随机变量子集，进一步减少偏差。
- en: We also learned that once a random forest is constructed, the result of the
    classification of a random forest is a majority vote from among all the trees
    in a random forest. The level of the majority also determines the level of confidence
    that the answer is correct.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还了解到，一旦构建了随机森林，随机森林的分类结果就是所有树中的多数投票。多数票的水平也决定了答案正确的置信度。
- en: Since random forests consist of decision trees, it is good to use them for every
    problem where a decision tree is a good choice. As random forests reduce the bias
    and variance that exists in decision tree classifiers, they outperform decision
    tree algorithms.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机森林由决策树组成，因此它适用于所有决策树表现良好的问题。因为随机森林可以减少决策树分类器中的偏差和方差，所以它们的性能优于决策树算法。
- en: In the next chapter, we will learn the technique of clustering data into similar
    clusters. We will also exploit this technique to classify data into one of the
    created clusters.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习将数据聚类到相似簇中的技术。我们还将利用该技术将数据分类到已创建的簇中。
- en: Problems
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: '**Problem** **1**: Let''s take the example of playing chess from Chapter 2,
    *Naive Bayes*. How would you classify a `(Warm,Strong,Spring,?)` data sample according
    to the random forest algorithm?'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题** **1**：让我们以第二章《朴素贝叶斯》中的下棋例子为例，如何根据随机森林算法对 `(Warm, Strong, Spring, ?)`
    数据样本进行分类？'
- en: '| **Temperature** | **Wind** | **Season** | **Play** |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **温度** | **风力** | **季节** | **是否玩** |'
- en: '| Cold | Strong | Winter | No |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 寒冷 | 强风 | 冬季 | 否 |'
- en: '| Warm | Strong | Autumn | No |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 强风 | 秋季 | 否 |'
- en: '| Warm | None | Summer | Yes |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 温暖 | 无 | 夏季 | 是 |'
- en: '| Hot | None | Spring | No |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Hot | None | Spring | No |'
- en: '| Hot | Breeze | Autumn | Yes |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Hot | Breeze | Autumn | Yes |'
- en: '| Warm | Breeze | Spring | Yes |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Warm | Breeze | Spring | Yes |'
- en: '| Cold | Breeze | Winter | No |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Cold | Breeze | Winter | No |'
- en: '| Cold | None | Spring | Yes |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Cold | None | Spring | Yes |'
- en: '| Hot | Strong | Summer | Yes |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Hot | Strong | Summer | Yes |'
- en: '| Warm | None | Autumn | Yes |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Warm | None | Autumn | Yes |'
- en: '| Warm | Strong | Spring | ? |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Warm | Strong | Spring | ? |'
- en: '**Problem 2**: Would it be a good idea to use only one tree and a random forest?
    Justify your answer.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2**：只使用一棵树和一个随机森林是一个好主意吗？请说明你的理由。'
- en: '**Problem 3**: Can cross-validation improve the results of the classification
    by the random forest? Justify your answer.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3**：交叉验证能否改善随机森林分类的结果？请说明你的理由。'
- en: Analysis
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析
- en: '**Problem 1:** We run the program to construct the random forest and classify
    the feature (`Warm, Strong, Spring`).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1**：我们运行程序来构建随机森林并对特征（`Warm, Strong, Spring`）进行分类。'
- en: '**Input**:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: '[PRE14]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Output**:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: 'We construct four trees in a random forest, as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在随机森林中构建了四棵树，具体如下：
- en: '[PRE15]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The whole construction and the analysis are stored in the `source_code/4/chess_with_seasons.out` file. Your
    construction may differ because of the randomness involved. From the output, we
    extract the random forest graph, consisting of random decision trees, given the
    random numbers generated during our run.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 整个构建过程和分析结果存储在`source_code/4/chess_with_seasons.out`文件中。由于涉及随机性，你的构建结果可能有所不同。从输出中，我们提取出随机森林图，该图由随机决策树组成，依据的是我们运行过程中生成的随机数。
- en: Executing the preceding command again will most likely result in a different
    output and a different random forest graph. Yet there is a high probability that
    the results of the classification will be similar because of the multiplicity
    of the random decision trees and their voting power combined. The classification
    by one random decision tree may be subject to significant variance. However, the
    majority vote combines the classification from all the trees, thus reducing the
    variance. To verify your understanding, you can compare your classification results
    with the classification by the following random forest graph.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 再次执行前面的命令很可能会导致不同的输出和不同的随机森林图。然而，由于随机决策树的多样性及其投票权的结合，分类结果很可能相似。单个随机决策树的分类可能会受到显著的方差影响。然而，多数投票将所有树的分类结果结合起来，从而减少了方差。为了验证你的理解，你可以将你的分类结果与以下随机森林图中的分类进行比较。
- en: '**Random forest graph and classification:**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林图和分类：**'
- en: 'Let''s have a look at the output of the random forest graph and the classification
    of the feature:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看随机森林图的输出和特征的分类：
- en: '[PRE16]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Problem 2**: When we construct a tree in a random forest, we only use a random
    subset of the data, with replacements. This is to eliminate classifier bias toward
    certain features. However, if we use only one tree, that tree may happen to contain
    features with bias and might be missing an important feature to enable it to provide
    an accurate classification. So, a random forest classifier with one decision tree
    would likely lead to a very poor classification. Therefore, we should construct
    more decision trees in a random forest to benefit from reduced bias and variance
    in the classification.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2**：当我们在随机森林中构建一棵树时，我们只使用数据的随机子集，并进行有放回抽样。这样做是为了消除分类器对某些特征的偏向。然而，如果只使用一棵树，这棵树可能恰好包含有偏向的特征，并且可能缺少一些重要特征，无法提供准确的分类。因此，使用仅有一棵决策树的随机森林分类器可能会导致非常差的分类结果。因此，我们应该在随机森林中构建更多的决策树，以便从减少偏差和方差的分类中获益。'
- en: '**Problem 3**: During cross-validation, we divide the data into training and
    testing data. Training data is used to train the classifier, and testing data
    is used to evaluate which parameters or methods would be the best fit for improving
    classification. Another advantage of cross-validation is the reduction in bias,
    because we only use partial data, thereby decreasing the chance of overfitting
    the specific dataset.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3**：在交叉验证过程中，我们将数据分为训练数据和测试数据。训练数据用于训练分类器，测试数据用于评估哪些参数或方法最适合改进分类。交叉验证的另一个优点是减少了偏差，因为我们只使用部分数据，从而减少了过拟合特定数据集的可能性。'
- en: However, in a decision forest, we address problems that cross-validation addresses
    in an alternative way. Each random decision tree is constructed only on the subset
    of the data—reducing the chance of overfitting. In the end, classification is
    the combination of results from each of these trees. The best decision, in the
    end, is not made by tuning the parameters on a test dataset, but by taking the
    majority vote of all the trees with reduced bias.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在决策森林中，我们以一种替代的方式解决交叉验证所解决的问题。每个随机决策树仅在数据的子集上构建——从而减少了过拟合的可能性。最终，分类是这些树的结果的组合。最终的最佳决策，并不是通过在测试数据集上调优参数来做出的，而是通过对所有树进行多数投票，从而减少偏差。
- en: Hence, cross-validation for a decision forest algorithm would not be of much
    use as it is already intrinsic within the algorithm.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，决策森林算法的交叉验证并不会太有用，因为它已经内置于算法中了。
