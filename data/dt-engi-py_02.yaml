- en: '*Chapter 1*: What is Data Engineering?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第一章*: 什么是数据工程？'
- en: Welcome to *Data Engineering with Python*. While data engineering is not a new
    field, it seems to have stepped out from the background recently and started to
    take center stage. This book will introduce you to the field of data engineering.
    You will learn about the tools and techniques employed by data engineers and you
    will learn how to combine them to build data pipelines. After completing this
    book, you will be able to connect to multiple data sources, extract the data,
    transform it, and load it into new locations. You will be able to build your own
    data engineering infrastructure, including clustering applications to increase
    their capacity to process data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到《Python数据工程》。虽然数据工程不是一个新兴领域，但它似乎最近从幕后走到了台前，开始成为焦点。本书将向您介绍数据工程的领域。您将了解数据工程师使用的工具和技术，以及如何将它们结合起来构建数据管道。完成本书后，您将能够连接到多个数据源，提取数据，转换数据，并将其加载到新的位置。您将能够构建自己的数据工程基础设施，包括集群应用程序以增加其数据处理能力。
- en: In this chapter, you will learn about the roles and responsibilities of data
    engineers and how data engineering works to support data science. You will be
    introduced to the tools used by data engineers, as well as the different areas
    of technology that you will need to be proficient in to become a data engineer.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解数据工程师的角色和职责以及数据工程如何支持数据科学。您将介绍数据工程师使用的工具，以及您需要精通的不同技术领域，以成为一名数据工程师。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: What data engineers do
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程师的工作内容
- en: Data engineering versus data science
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程与数据科学的比较
- en: Data engineering tools
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程工具
- en: What data engineers do
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据工程师的工作内容
- en: '**Data engineering** is part of the big data ecosystem and is closely linked
    to data science. Data engineers work in the background and do not get the same
    level of attention as data scientists, but they are critical to the process of
    data science. The roles and responsibilities of a data engineer vary depending
    on an organization''s level of data maturity and staffing levels; however, there
    are some tasks, such as the extracting, loading, and transforming of data, that
    are foundational to the role of a data engineer.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据工程**是大数据生态系统的一部分，与数据科学紧密相连。数据工程师在幕后工作，并不像数据科学家那样受到同样的关注，但他们对于数据科学的过程至关重要。数据工程师的角色和职责根据组织的数据处理成熟度和人员配置水平而有所不同；然而，有一些任务，如数据的提取、加载和转换，是数据工程师角色的基础。'
- en: At the lowest level, data engineering involves the movement of data from one
    system or format to another system or format. Using more common terms, data engineers
    query data from a source (extract), they perform some modifications to the data
    (transform), and then they put that data in a location where users can access
    it and know that it is production quality (load). The terms **extract**, **transform**,
    and **load** will be used a lot throughout this book and will often be abbreviated
    to **ETL**. This definition of data engineering is broad and simplistic. With
    the help of an example, let's dig deeper into what data engineers do.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在最底层，数据工程涉及将数据从一个系统或格式移动到另一个系统或格式。使用更常见的术语，数据工程师从源（提取）查询数据，他们对数据进行一些修改（转换），然后将这些数据放置在用户可以访问并且知道它是生产质量的地方（加载）。本书中会多次使用**提取**、**转换**和**加载**这些术语，通常缩写为**ETL**。这种数据工程的定义是宽泛且简化的。通过一个例子，让我们更深入地了解数据工程师的工作内容。
- en: An online retailer has a website where you can purchase widgets in a variety
    of colors. The website is backed by a relational database. Every transaction is
    stored in the database. How many blue widgets did the retailer sell in the last
    quarter?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一家在线零售商有一个网站，您可以在网站上购买各种颜色的widgets。该网站由关系型数据库支持。每次交易都存储在数据库中。零售商在上个季度卖出了多少蓝色的widgets？
- en: To answer this question, you could run a SQL query on the database. This doesn't
    rise to the level of needing a data engineer. But as the site grows, running queries
    on the production database is no longer practical. Furthermore, there may be more
    than one database that records transactions. There may be a database at different
    geographical locations – for example, the retailers in North America may have
    a different database than the retailers in Asia, Africa, and Europe.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，您可以在数据库上运行一个SQL查询。这并不需要数据工程师。但随着网站的增长，在生产数据库上运行查询不再实用。此外，可能存在多个记录交易的数据库。可能存在位于不同地理位置的数据库——例如，北美零售商可能有一个与亚洲、非洲和欧洲零售商不同的数据库。
- en: Now you have entered the realm of data engineering. To answer the preceding
    question, a data engineer would create connections to all of the transactional
    databases for each region, extract the data, and load it into a data warehouse.
    From there, you could now count the number of all the blue widgets sold.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经进入了数据工程的领域。为了回答前面的问题，数据工程师会为每个地区的所有交易数据库创建连接，提取数据，并将其加载到数据仓库中。从那里，您现在可以计算所有蓝色widgets的销售数量。
- en: 'Rather than finding the number of blue widgets sold, companies would prefer
    to find the answer to the following questions:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与寻找销售了多少蓝色widgets相比，公司更希望找到以下问题的答案：
- en: How do we find out which locations sell the most widgets?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何找出哪些位置销售最多的widgets？
- en: How do we find out the peak times for selling widgets?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何找出销售widgets的峰值时间？
- en: How many users put widgets in their carts and remove them later?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多少用户将widgets放入购物车并在之后移除？
- en: How do we find out the combinations of widgets that are sold together?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何找出一起销售的widgets组合？
- en: Answering these questions requires more than just extracting the data and loading
    it into a single system. There is a transformation required in between the extract
    and load. There is also the difference in times zones in different regions. For
    instance, the United States alone has four time zones. Because of this, you would
    need to transform time fields to a standard. You will also need a way to distinguish
    sales in each region. This could be accomplished by adding a location field to
    the data. Should this field be spatial – in coordinates or as well-known text
    – or will it just be text that could be transformed in a data engineering pipeline?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题不仅需要提取数据并将其加载到单个系统中，在提取和加载之间还需要进行转换。不同地区存在时区差异。例如，仅美国就有四个时区。因此，您需要将时间字段转换为标准格式。您还需要一种方法来区分每个地区的销售情况。这可以通过在数据中添加一个位置字段来实现。这个字段应该是空间性的——以坐标或已知文本的形式——还是仅仅是以文本形式，可以在数据工程管道中进行转换？
- en: Here, the data engineer would need to extract the data from each database, then
    transform the data by adding an additional field for the location. To compare
    the time zones, the data engineer would need to be familiar with data standards.
    For the time, the **International Organization for Standardization** (**ISO**)
    has a standard – **ISO 8601**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，数据工程师需要从每个数据库中提取数据，然后通过添加一个额外的位置字段来转换数据。为了比较时区，数据工程师需要熟悉数据标准。对于时间，国际标准化组织（**ISO**）有一个标准——**ISO
    8601**。
- en: 'Let''s now answer the questions in the preceding list one by one:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们逐一回答前面列表中的问题：
- en: Extract the data from each database.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从每个数据库中提取数据。
- en: Add a field to tag the location for each transaction in the data
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据中为每个交易的地点添加一个标记字段
- en: Transform the date from local time to ISO 8601.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日期从本地时间转换为ISO 8601。
- en: Load the data into the data warehouse.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据加载到数据仓库中。
- en: 'The combination of extracting, loading, and transforming data is accomplished
    by the creation of a data pipeline. The data comes into the pipeline raw, or dirty
    in the sense that there may be missing data or typos in the data, which is then
    cleaned as it flows through the pipe. After that, it comes out the other side
    into a data warehouse, where it can be queried. The following diagram shows the
    pipeline required to accomplish the task:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建数据管道来完成提取、加载和转换数据的过程。数据以原始或脏数据的形式进入管道，即可能存在缺失数据或数据中的错误，然后在管道中流动时被清理。之后，它从管道的另一侧流出进入数据仓库，在那里可以进行查询。以下图表显示了完成此任务所需的数据管道：
- en: '![Figure 1.1 – A pipeline that adds a location and modifies the date'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1 – 添加位置并修改日期的管道'
- en: '](img/B15739_01_01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15739_01_01.jpg)'
- en: Figure 1.1 – A pipeline that adds a location and modifies the date
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 一个添加位置并修改日期的管道
- en: Knowing a little more about what data engineering is, and what data engineers
    do, you should start to get a sense of the responsibilities and skills that data
    engineers need to acquire. The following section will elaborate on these skills.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据工程是什么以及数据工程师做什么有更多的了解，你应该开始对数据工程师需要获取的责任和技能有感觉。下一节将详细阐述这些技能。
- en: Required skills and knowledge to be a data engineer
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成为数据工程师所需的专业技能和知识
- en: In the preceding example, it should be clear that data engineers need to be
    familiar with many different technologies, and we haven't even mentioned the business
    processes or needs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，应该很清楚数据工程师需要熟悉许多不同的技术，而我们甚至还没有提到业务流程或需求。
- en: At the start of a data pipeline, data engineers need to know how to extract
    data from files in different formats or different types of databases. This means
    data engineers need to know several languages used to perform many different tasks,
    such as SQL and Python.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据管道的开始阶段，数据工程师需要知道如何从不同格式或不同类型的数据库中的文件中提取数据。这意味着数据工程师需要了解用于执行许多不同任务的几种语言，例如
    SQL 和 Python。
- en: During the transformation phase of the data pipeline, data engineers need to
    be familiar with data modeling and structures. They will also need to understand
    the business and what knowledge and insight they are hoping to extract from the
    data because this will impact the design of the data models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据管道的转换阶段，数据工程师需要熟悉数据建模和结构。他们还需要了解业务以及他们希望从数据中提取的知识和见解，因为这会影响数据模型的设计。
- en: The loading of data into the data warehouse means there needs to be a data warehouse
    with a schema to hold the data. This is also usually the responsibility of the
    data engineer. Data engineers will need to know the basics of data warehouse design,
    as well as the types of databases used in their construction.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据加载到数据仓库中意味着需要一个具有模式以存储数据的数据库。这通常也是数据工程师的责任。数据工程师需要了解数据仓库设计的基础知识，以及在其构建中使用的数据库类型。
- en: Lastly, the entire infrastructure that the data pipeline runs on could be the
    responsibility of the data engineer. They need to know how to manage Linux servers,
    as well as how to install and configure software such as Apache Airflow or NiFi.
    As organizations move to the cloud, the data engineer now needs to be familiar
    with spinning up the infrastructure on the cloud platform used by the organization
    – Amazon, Google Cloud Platform, or Azure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据管道运行在其上的整个基础设施可能也是数据工程师的责任。他们需要知道如何管理 Linux 服务器，以及如何安装和配置 Apache Airflow
    或 NiFi 等软件。随着组织向云迁移，数据工程师现在需要熟悉在组织使用的云平台上启动基础设施——亚马逊、谷歌云平台或 Azure。
- en: Having walked through an example of what data engineers do, we can now develop
    a broader definition of data engineering.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解数据工程师的工作示例，我们现在可以更广泛地定义数据工程。
- en: Information
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Data engineering is the development, operation, and maintenance of data infrastructure,
    either on-premises or in the cloud (or hybrid or multi-cloud), comprising databases
    and pipelines to extract, transform, and load data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程是在本地或云（或混合或多云）上开发、运营和维护数据基础设施，包括数据库和用于提取、转换和加载数据的管道。
- en: Data engineering versus data science
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据工程与数据科学
- en: Data engineering is what makes data science possible. Again, depending on the
    maturity of an organization, data scientists may be expected to clean and move
    the data required for analysis. This is not the best use of a data scientist's
    time. Data scientists and data engineers use similar tools (Python, for instance),
    but they specialize in different areas. Data engineers need to understand data
    formats, models, and structures to efficiently transport data, whereas data scientists
    utilize them for building statistical models and mathematical computation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程是使数据科学成为可能的基础。再次强调，根据组织的成熟度，数据科学家可能需要清理和移动分析所需的数据。这不是数据科学家最佳的时间利用方式。数据科学家和数据工程师使用类似的工具（例如
    Python），但他们专注于不同的领域。数据工程师需要了解数据格式、模型和结构，以便有效地传输数据，而数据科学家则利用这些工具来构建统计模型和数学计算。
- en: Data scientists will connect to the data warehouses built by data engineers.
    From there, they can extract the data required for machine learning models and
    analysis. Data scientists may have their models incorporated into a data engineering
    pipeline. A close relationship should exist between data engineers and data scientists.
    Understanding what data scientists need in the data will only serve to help the
    data engineers deliver a better product.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家将连接到数据工程师构建的数据仓库。从那里，他们可以提取用于机器学习模型和分析所需的数据。数据科学家的模型可能被纳入数据工程管道中。数据工程师和数据科学家之间应该存在紧密的关系。了解数据科学家在数据中需要什么，只会帮助数据工程师提供更好的产品。
- en: In the next section, you will learn more about the most common tools used by
    data engineers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解数据工程师最常用的工具。
- en: Data engineering tools
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据工程工具
- en: 'To build data pipelines, data engineers need to choose the right tools for
    the job. Data engineering is part of the overall big data ecosystem and has to
    account for the three Vs of big data:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建数据管道，数据工程师需要选择适合工作的正确工具。数据工程是大数据生态系统的一部分，必须考虑到大数据的三个V：
- en: '**Volume**: The volume of data has grown substantially. Moving a thousand records
    from a database requires different tools and techniques than moving millions of
    rows or handling millions of transactions a minute.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**体积**：数据量已经大幅增长。从一个数据库中移动一千条记录需要与移动数百万行或每分钟处理数百万笔交易不同的工具和技术。'
- en: '**Variety**: Data engineers need tools that handle a variety of data formats
    in different locations (databases, APIs, files).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：数据工程师需要能够处理不同位置（数据库、API、文件）中各种数据格式的工具。'
- en: '**Velocity**: The velocity of data is always increasing. Tracking the activity
    of millions of users on a social network or the purchases of users all over the
    world requires data engineers to operate often in near real time.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：数据速度总是不断增长。跟踪社交网络上数百万用户的活动或全球各地用户的购买行为，要求数据工程师经常在近乎实时的情况下进行操作。'
- en: Programming languages
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编程语言
- en: The lingua franca of data engineering is **SQL**. Whether you use low-code tools
    or a specific programming language, there is almost no way to get around knowing
    SQL. A strong foundation in SQL allows the data engineer to optimize queries for
    speed and can assist in data transformations. SQL is so prevalent in data engineering
    that data lakes and non-SQL databases have tools to allow the data engineer to
    query them in SQL.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程的通用语言是**SQL**。无论你使用低代码工具还是特定的编程语言，几乎无法绕过对SQL的了解。在SQL方面有坚实的基础，可以让数据工程师优化查询以提高速度，并有助于数据转换。SQL在数据工程中如此普遍，以至于数据湖和非SQL数据库都有工具允许数据工程师使用SQL查询它们。
- en: A large number of open source data engineering tools use **Java** and **Scala**
    (Apache projects). Java is a popular, mainstream, object-oriented programming
    language. While debatable, Java is slowly being replaced by other languages that
    run on the **Java Virtual Machine** (**JVM**). Scala is one of these languages.
    Other languages that run on the JVM include **Clojure** and **Groovy**. In the
    next chapter, you will be introduced to **Apache NiFi**. NiFi allows you to develop
    custom processers in Java, Clojure, Groovy, and **Jython**. While Java is an object-oriented
    language, there has been a movement toward functional programming languages, of
    which Clojure and Scala are members.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大量开源数据工程工具使用**Java**和**Scala**（Apache项目）。Java是一种流行的、主流的面向对象编程语言。虽然存在争议，但Java正逐渐被运行在**Java虚拟机**（**JVM**）上的其他语言所取代。Scala是这些语言之一。在JVM上运行的其他语言包括**Clojure**和**Groovy**。在下一章中，你将介绍**Apache
    NiFi**。NiFi允许你使用Java、Clojure、Groovy和**Jython**开发自定义处理器。虽然Java是一种面向对象的语言，但已经出现了一种向函数式编程语言转变的趋势，其中Clojure和Scala是成员。
- en: The focus of this book is on data engineering with Python. It is well-documented
    with a larger user base and cross-platform. Python has become the default language
    for data science and data engineering. Python has an extensive collection of standard
    libraries and third-party libraries. The data science environment in Python is
    unmatched in other languages. Libraries such as `pandas`, `matplotlib`, `numpy`,
    `scipy`, `scikit-learn`, `tensorflow`, `pytorch`, and `NLTK` make up an extremely
    powerful data engineering and data science environment.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本书重点关注使用 Python 进行数据工程。它有很好的文档记录，拥有更大的用户基础和跨平台支持。Python 已成为数据科学和数据工程的默认语言。Python
    拥有一系列丰富的标准库和第三方库。Python 的数据科学环境在其它语言中是无与伦比的。`pandas`、`matplotlib`、`numpy`、`scipy`、`scikit-learn`、`tensorflow`、`pytorch`
    和 `NLTK` 等库构成了一个极其强大的数据工程和数据科学环境。
- en: Databases
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库
- en: 'In most production systems, data will be stored in **relational databases**.
    Most proprietary solutions will use either **Oracle** or **Microsoft SQL Server**,
    while open source solutions tend to use **MySQL** or **PostgreSQL**. These databases
    store data in rows and are well-suited to recording transactions. There are also
    relationships between tables, utilizing primary keys to join data from one table
    to another – thus making them relational. The following table diagram shows a
    simple data model and the relationships between the tables:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数生产系统中，数据将存储在 **关系数据库** 中。大多数专有解决方案将使用 **Oracle** 或 **Microsoft SQL Server**，而开源解决方案倾向于使用
    **MySQL** 或 **PostgreSQL**。这些数据库以行存储数据，非常适合记录交易。表之间也存在关系，利用主键将一个表中的数据与另一个表中的数据连接起来——因此它们是关系型的。以下表格图显示了简单的数据模型和表之间的关系：
- en: '![Figure 1.2 – Relational tables joined on Region = RegionID.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2 – 在 Region = RegionID 上连接的关系表。'
- en: '](img/B15739_01_02.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2 – 在 Region = RegionID 上连接的关系表。'
- en: Figure 1.2 – Relational tables joined on Region = RegionID.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 在 Region = RegionID 上连接的关系表。
- en: 'The most common databases used in data warehousing are **Amazon Redshift**,
    **Google BigQuery**, **Apache Cassandra**, and other NoSQL databases, such as
    **Elasticsearch**. Amazon Redshift, Google BigQuery, and Cassandra deviate from
    the traditional rows of relational databases and store data in a columnar format,
    as shown:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据仓库中使用的最常见数据库是 **Amazon Redshift**、**Google BigQuery**、**Apache Cassandra**
    以及其他 NoSQL 数据库，例如 **Elasticsearch**。Amazon Redshift、Google BigQuery 和 Cassandra
    与传统的行式关系数据库不同，它们以列式格式存储数据，如下所示：
- en: '![Figure 1.3 – Rows stored in a columnar format'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.3 – 以列式格式存储的行'
- en: '](img/B15739_01_03.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B15739_01_03.jpg]'
- en: Figure 1.3 – Rows stored in a columnar format
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 以列式格式存储的行
- en: Columnar databases are better suited for fast queries – therefore making them
    well-suited for data warehouses. All three of the columnar databases can be queried
    using SQL – although Cassandra uses the Cassandra Query Language, it is similar.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列式数据库更适合快速查询——因此非常适合数据仓库。这三个列式数据库都可以使用 SQL 进行查询——尽管 Cassandra 使用 Cassandra 查询语言，但它们是相似的。
- en: In contrast to columnar databases, there are document, or NoSQL, databases,
    such as Elasticsearch. Elasticsearch is actually a search engine based on **Apache
    Lucene**. It is similar to **Apache Solr** but is more user-friendly. Elasticsearch
    is open source, but it does have proprietary components – most notably, the X-Pack
    plugins for machine learning, graphs, security, and alerting/monitoring. Elasticsearch
    uses the Elastic Query **DSL** (**Domain-Specific Language**). It is not SQL,
    but rather a JSON query. Elasticsearch stores data as documents, and while it
    has parent-child documents, it is non-relational (like the columnar databases).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与列式数据库相反，还有文档型或 NoSQL 数据库，例如 Elasticsearch。Elasticsearch 实际上是一个基于 **Apache Lucene**
    的搜索引擎。它与 **Apache Solr** 类似，但更易于使用。Elasticsearch 是开源的，但它确实有一些专有组件——最显著的是用于机器学习、图、安全和警报/监控的
    X-Pack 插件。Elasticsearch 使用 Elastic 查询 **DSL**（**领域特定语言**）。它不是 SQL，而是一种 JSON 查询。Elasticsearch
    以文档的形式存储数据，尽管它有父子文档，但它是非关系型的（类似于列式数据库）。
- en: Once a data engineer extracts data from a database, they will need to transform
    or process it. With big data, it helps to use a data processing engine.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据工程师从数据库中提取数据，他们就需要对其进行转换或处理。在大数据中，使用数据处理引擎很有帮助。
- en: Data processing engines
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据处理引擎
- en: Data processing engines allow data engineers to transform data whether it is
    in batches or streams. These engines allow the parallel execution of transformation
    tasks. The most popular engine is **Apache Spark**. Apache Spark allows data engineers
    to write transformations in Python, Java, and Scala.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理引擎允许数据工程师将数据转换为批量或流式数据。这些引擎允许并行执行转换任务。最受欢迎的引擎是**Apache Spark**。Apache Spark允许数据工程师使用Python、Java和Scala编写转换。
- en: Apache Spark works with Python DataFrames, making it an ideal tool for Python
    programmers. Spark also has **Resilient Distributed Datasets** (**RDDs**). RDDs
    are an immutable and distributed collection of objects. You create them mainly
    by loading in an external data source. RDDs allow fast and distributed processing.
    The tasks in an RDD are run on different nodes within the cluster. Unlike DataFrames,
    they do not try to guess the schema in your data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark与Python DataFrames一起工作，使其成为Python程序员的理想工具。Spark还有**弹性分布式数据集**(**RDDs**)。RDDs是一个不可变和分布式的对象集合。你主要通过加载外部数据源来创建它们。RDDs允许快速和分布式处理。RDD中的任务在集群中的不同节点上运行。与DataFrames不同，它们不会尝试猜测你的数据模式。
- en: Other popular process engines include **Apache Storm**, which utilizes spouts
    to read data and bolts to perform transformations. By connecting them, you build
    a processing pipeline. **Apache Flink** and **Samza** are more modern stream and
    batch processing frameworks that allow you to process unbounded streams. An unbounded
    stream is data that comes in with no known end – a temperature sensor, for example,
    is an unbounded stream. It is constantly reporting temperatures. Flink and Samza
    are excellent choices if you are using Apache Kafka to stream data from a system.
    You will learn more about Apache Kafka later in this book.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其他流行的处理引擎包括**Apache Storm**，它使用喷泉(spouts)读取数据，使用螺栓(bolts)执行转换。通过连接它们，你可以构建一个处理管道。**Apache
    Flink**和**Samza**是更现代的流式和批量处理框架，允许你处理无界流。无界流是没有已知终点的数据，例如，温度传感器就是一个无界流。它不断地报告温度。如果你使用Apache
    Kafka从系统中流式传输数据，Flink和Samza是绝佳的选择。你将在本书的后面部分了解更多关于Apache Kafka的内容。
- en: Data pipelines
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管道
- en: Combining a transactional database, a programming language, a processing engine,
    and a data warehouse results in a pipeline. For example, if you select all the
    records of widget sales from the database, run it through Spark to reduce the
    data to widgets and counts, then dump the result to the data warehouse, you have
    a pipeline. But this pipeline is not very useful if you have to execute manually
    every time you want it to run. Data pipelines need a scheduler to allow them to
    run at specified intervals. The simplest way to accomplish this is by using **crontab**.
    Schedule a cron job for your Python file and sit back and watch it run every *X*
    number of hours.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将事务型数据库、编程语言、处理引擎和数据仓库结合起来，就形成了一个管道。例如，如果你从数据库中选择所有小部件销售的记录，通过Spark将其数据减少到小部件和计数，然后将结果存入数据仓库，你就拥有了一个管道。但是，如果你每次想要运行它时都必须手动执行，这个管道就不是很实用。数据管道需要一个调度器来允许它们在指定的时间间隔运行。最简单的方法是使用**crontab**。为你的Python文件安排一个cron作业，然后坐下来观看它每隔*X*小时运行一次。
- en: Managing all the pipelines in crontab becomes difficult fast. How do you keep
    track of pipelines' successes and failures? How do you know what ran and what
    didn't? How do you handle backpressure – if one task runs faster than the next,
    how do you hold data back, so it doesn't overwhelm the task? As your pipelines
    become more advanced, you will quickly outgrow crontab and will need a better
    framework.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在crontab中管理所有管道很快就会变得困难。你如何跟踪管道的成功和失败？你如何知道什么运行了，什么没有运行？你如何处理背压——如果一个任务比下一个任务运行得快，你如何阻止数据，以免它压倒任务？随着你的管道变得更加复杂，你将很快超出crontab的范围，需要一个更好的框架。
- en: Apache Airflow
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache Airflow
- en: The most popular framework for building data engineering pipelines in Python
    is **Apache Airflow**. Airflow is a workflow management platform built by Airbnb.
    Airflow is made up of a web server, a scheduler, a metastore, a queueing system,
    and executors. You can run Airflow as a single instance, or you can break it up
    into a cluster with many executor nodes – this is most likely how you would run
    it in production. Airflow uses **Directed Acyclic Graphs** (**DAGs**).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中构建数据工程管道最流行的框架是**Apache Airflow**。Airflow是由Airbnb构建的工作流管理平台。Airflow由一个网络服务器、一个调度器、一个元存储、一个排队系统和执行器组成。你可以运行Airflow作为一个单一实例，或者将其拆分为具有许多执行节点集群——这可能是你在生产中运行它的方式。Airflow使用**有向无环图**(**DAGs**)。
- en: 'A DAG is Python code that specifies tasks. A graph is a series of nodes connected
    by a relationship or dependency. In Airflow, they are directed because they flow
    in a direction with each task coming after its dependency. Using the preceding
    example pipeline, the first node would be to execute a SQL statement grabbing
    all the widget sales. This node would connect downstream to another node, which
    would aggregate the widgets and counts. Lastly, this node would connect to the
    final node, which loads the data into the warehouse. The pipeline DAG would look
    as in the following diagram:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 是指定任务的 Python 代码。图是由关系或依赖连接的一系列节点。在 Airflow 中，它们是定向的，因为它们以方向流动，每个任务在其依赖项之后进行。使用前面的示例管道，第一个节点将执行一个
    SQL 语句，获取所有小部件的销售情况。此节点将连接到下游的另一个节点，该节点将汇总小部件和计数。最后，此节点将连接到最终节点，将数据加载到仓库中。管道 DAG
    将看起来如下所示：
- en: '![Figure 1.4 – A DAG showing the flow of data between nodes. The task follows
    the arrows (is directed) from left to right'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.4 – 展示节点间数据流的一个 DAG。任务沿着箭头（有方向性）从左到右进行'
- en: '](img/B15739_01_04.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B15739_01_04](img/B15739_01_04.jpg)'
- en: Figure 1.4 – A DAG showing the flow of data between nodes. The task follows
    the arrows (is directed) from left to right
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 展示节点间数据流的一个 DAG。任务沿着箭头（有方向性）从左到右进行
- en: 'This book will cover the basics of Apache Airflow but will primarily use Apache
    NiFi to demonstrate the principles of data engineering. The following is a screenshot
    of a DAG in Airflow:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将介绍 Apache Airflow 的基础知识，但主要将使用 Apache NiFi 来演示数据工程的原则。以下是一个 Airflow 中 DAG
    的截图：
- en: '![Figure 1.5 – The Airflow GUI showing the details of a DAG'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.5 – 显示 DAG 详细信息的 Airflow 图形用户界面'
- en: '](img/B15739_01_05.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B15739_01_05](img/B15739_01_05.jpg)'
- en: Figure 1.5 – The Airflow GUI showing the details of a DAG
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – 显示 DAG 详细信息的 Airflow 图形用户界面
- en: The GUI is not as polished as NiFi, which we will discuss next.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图形用户界面不如 NiFi 精炼，我们将在下一节讨论。
- en: Apache NiFi
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache NiFi
- en: 'Apache NiFi is another framework for building data engineering pipelines, and
    it too utilizes DAGs. Apache NiFi was built by the National Security Agency and
    is used at several federal agencies. Apache NiFi is easier to set up and is useful
    for new data engineers. The GUI is excellent and while you can use Jython, Clojure,
    Scala, or Groovy to write processors, you can accomplish a lot with a simple configuration
    of existing processors. The following screenshot shows the NiFi GUI and a sample
    DAG:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Apache NiFi 是另一个用于构建数据工程管道的框架，它也利用了 DAG。Apache NiFi 由国家安全局构建，并在几个联邦机构中使用。Apache
    NiFi 更容易设置，对新数据工程师很有用。图形用户界面非常出色，虽然你可以使用 Jython、Clojure、Scala 或 Groovy 来编写处理器，但通过简单配置现有处理器，你可以完成很多事情。以下截图显示了
    NiFi 图形用户界面和示例 DAG：
- en: '![Figure 1.6 – A sample NiFi flow extracting data from a database and sending
    it to Elasticsearch'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.6 – 从数据库提取数据并发送到 Elasticsearch 的示例 NiFi 流'
- en: '](img/Image86923.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 86923](img/Image86923.jpg)'
- en: Figure 1.6 – A sample NiFi flow extracting data from a database and sending
    it to Elasticsearch
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – 从数据库提取数据并发送到 Elasticsearch 的示例 NiFi 流
- en: Apache NiFi also allows clustering and the remote execution of pipelines. It
    has a built-in scheduler and provides the backpressure and monitoring of pipelines.
    Furthermore, Apache NiFi has version control using the NiFi Registry and can be
    used to collect data on the edge using MiNiFi.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Apache NiFi 还允许集群和远程执行管道。它具有内置的调度程序，并提供管道的背压和监控。此外，Apache NiFi 使用 NiFi Registry
    进行版本控制，并可用于使用 MiNiFi 收集边缘数据。
- en: Another Python-based tool for data engineering pipelines is Luigi – developed
    by Spotify. Luigi also uses a graph structure and allows you to connect tasks.
    It has a GUI much like Airflow. Luigi will not be covered in this book but is
    an excellent option for Python-based data engineering.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用于数据工程管道的基于 Python 的工具是 Luigi – 由 Spotify 开发。Luigi 也使用图结构，并允许你连接任务。它有一个类似于
    Airflow 的图形用户界面。Luigi 不会在本书中介绍，但它是基于 Python 的数据工程的优秀选择。
- en: Summary
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned what data engineering is. Data engineering roles
    and responsibilities vary depending on the maturity of an organization's data
    infrastructure. But data engineering, at its simplest, is the creation of pipelines
    to move data from one source or format to another. This may or may not involve
    data transformations, processing engines, and the maintenance of infrastructure.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了数据工程是什么。数据工程的角色和责任取决于组织数据基础设施的成熟度。但数据工程在最简单的情况下，是创建管道以将数据从一个源或格式移动到另一个源或格式。这可能涉及或不涉及数据转换、处理引擎和基础设施的维护。
- en: Data engineers use a variety of programming languages, but most commonly Python,
    Java, or Scala, as well as proprietary and open source transactional databases
    and data warehouses, both on-premises and in the cloud, or a mixture. Data engineers
    need to be knowledgeable in many areas – programming, operations, data modeling,
    databases, and operating systems. The breadth of the field is part of what makes
    it fun, exciting, and challenging. To those willing to accept the challenge, data
    engineering is a rewarding career.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师使用各种编程语言，但最常见的是Python、Java或Scala，以及专有和开源的事务性数据库和数据仓库，无论是在本地还是在云端，或者两者混合。数据工程师需要在许多领域具备知识——编程、运维、数据建模、数据库和操作系统。该领域的广度是使其有趣、激动人心和具有挑战性的部分。对于那些愿意接受挑战的人来说，数据工程是一个有回报的职业。
- en: In the next chapter, we will begin by setting up an environment to start building
    data pipelines.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将首先设置一个环境以开始构建数据管道。
