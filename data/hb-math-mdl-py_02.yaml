- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Machine Learning vis-à-vis Mathematical Modeling
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与数学建模
- en: Having learned about the main components of mathematical optimization, which
    are decision variables, objective functions, and constraints, in the previous
    chapter, it is time to throw light on **machine learning** (**ML**) models, most
    of which can be cast as mathematical models. Humans make machines learn from huge
    amounts of historical data. ML models enhance the decision-making abilities of
    man and machine, exploiting the power of data and algorithms. There is almost
    always some optimization algorithm working in the background of most of these
    models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章学习了数学优化的主要组成部分：决策变量、目标函数和约束条件后，现在是时候探讨**机器学习**（**ML**）模型了，其中大多数可以被看作是数学模型。人类通过大量的历史数据让机器学习。机器学习模型增强了人类和机器的决策能力，充分利用数据和算法的力量。几乎所有这些模型背后都在运行某种优化算法。
- en: The term ML was first popularized by Arthur L. Samuel in the 1950s, who was
    a pioneer in computer science and gaming. Data volume has increased by leaps and
    bounds since then, particularly in the last couple of decades, and making sense
    of huge amounts of data is beyond the scope of the human mind. Hence, ML stepped
    in and found its application in almost all domains to assist humans with the decision-making
    process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）这一术语首次由计算机科学和游戏领域的先驱Arthur L. Samuel在20世纪50年代普及。自那时以来，数据量飞速增长，尤其是在过去几十年中，处理大量数据已经超出了人类大脑的能力范围。因此，机器学习介入并找到了在几乎所有领域的应用，帮助人类做出决策。
- en: Learning problems in data science can be broadly classified into regression,
    classification, and clustering depending on the business problem or use case.
    Regression and classification use supervised algorithms to predict a target, usually
    called the dependent variable, the independent variables being called predictors.
    Clustering makes use of unsupervised learning algorithms where the target is unknown.
    It is worth mentioning that learning in all ML algorithms is not all about optimization,
    an example of which is supervised learning in **k-nearest neighbors** (**kNN**).
    ML is a predominantly predictive tool helping a business plan for the future,
    thereby being beneficial for its bottom line. Businesses also leverage ML in anomaly
    (or outlier) detection and recommendation systems. Strictly mathematical modeling,
    on the other hand, helps businesses make decisions in areas such as electricity
    distribution, employee scheduling, and inventory management.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中的学习问题可以根据业务问题或用例大致分为回归、分类和聚类。回归和分类使用监督学习算法预测目标变量，通常称为因变量，而独立变量则称为预测变量。聚类则使用无监督学习算法，其中目标是未知的。值得一提的是，并非所有机器学习算法中的学习都涉及优化，例如**k-最近邻**（**kNN**）就是一个监督学习的例子。机器学习主要是一个预测工具，帮助企业为未来做好规划，从而有利于其利润。企业还利用机器学习进行异常（或离群点）检测和推荐系统。严格的数学建模则帮助企业在电力分配、员工调度和库存管理等领域做出决策。
- en: 'Some well-known algorithms used in ML models that employ constrained optimization
    are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一些在机器学习模型中使用的约束优化算法如下：
- en: '**Principal component** **analysis** (**PCA**)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）'
- en: Clustering with an expectation maximization algorithm (a Gaussian mixture model,
    for example)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用期望最大化算法进行聚类（例如高斯混合模型）
- en: Support vector machines using the method of Lagrange multipliers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用拉格朗日乘子法的支持向量机
- en: Other ML algorithms that employ unconstrained optimization are **stochastic
    gradient descent** (**SGD**) in neural networks and batch gradient descent in
    deep learning (neural networks with numerous hidden layers between the input and
    output). Apart from these, there are genetic algorithms in evolutionary learning,
    which encompass both constrained and unconstrained optimization problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 其他使用无约束优化的机器学习算法包括神经网络中的**随机梯度下降**（**SGD**）和深度学习中的批量梯度下降（神经网络具有大量的隐藏层，位于输入和输出之间）。除了这些，还有进化学习中的遗传算法，这些算法涵盖了约束和无约束优化问题。
- en: The main components of ML are representation, evaluation, and optimization.
    By representation, we essentially mean putting forth the knowledge and historical
    data statistically to find patterns, in other words, the formulation of a business
    problem to arrive at or estimate the solution. Next is the evaluation of the formulation,
    which we call the model, and fitting our data into and comparing it with known
    examples or data samples. Finally, the algorithm behind the model optimizes its
    weights and biases for a better fit with the data, and the optimization process
    iterates until a desired accuracy for the problem is attained. We will learn about
    PCA and gradient descent in the following chapters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的主要组成部分包括表示、评估和优化。表示是指通过统计地呈现知识和历史数据，以寻找模式，换句话说，就是将一个业务问题表述出来，从而得出或估算解决方案。接下来是对这一表述的评估，我们称之为模型，并将数据与已知的示例或数据样本进行拟合和比较。最后，模型背后的算法优化其权重和偏差，以更好地拟合数据，优化过程会迭代进行，直到达到所需的精度为止。在接下来的章节中，我们将学习主成分分析（PCA）和梯度下降法。
- en: 'This chapter covers the following topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: ML as a mathematical optimization problem
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习作为数学优化问题
- en: ML as a predictive tool
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习作为一种预测工具
- en: Mathematical modeling as a prescriptive tool
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学建模作为一种规范性工具
- en: ML as mathematical optimization
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习作为数学优化
- en: ML can be described as finding the unknown underlying (approximate) function
    that maps input examples to output examples. This is where the ML algorithm defines
    a parametrized mapping function and optimizes or minimizes the error in the function
    to find the values of its parameters. ML is function approximation along with
    function optimization. The function parameters are also called model coefficients.
    Each time we fit a model to a training dataset, we solve an optimization problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以描述为寻找将输入示例映射到输出示例的未知潜在（近似）函数。在这一过程中，机器学习算法定义了一个参数化的映射函数，并通过优化或最小化函数中的误差来求解其参数的值。机器学习就是函数近似与函数优化的结合。函数参数也称为模型系数。每次我们将模型拟合到训练数据集时，都在解决一个优化问题。
- en: Each ML algorithm makes different assumptions about the form of the mapping
    function, which in turn influences the type of optimization to be performed. ML
    is a function approximation method to optimally fit input data. It is particularly
    challenging when the data (the size or the number of examples) is limited. An
    ML algorithm must be chosen in a way that it most efficiently solves an optimization
    problem; for example, SGD is used for neural nets, while ordinary least squares
    and gradient descent are used for linear regression. When we deviate from the
    default algorithms, we need a good reason to do so. In mathematical optimization,
    a heuristic might sometimes be used to determine near-optimal solutions. This
    happens when the classical algorithms are too slow to even find an approximate
    solution or they fail to find an exact solution to the optimization problem. Examples
    of heuristics are a genetic algorithm and a simulated annealing algorithm.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每种机器学习算法对映射函数的形式做出不同的假设，这进而影响要执行的优化类型。机器学习是一种函数近似方法，旨在最优地拟合输入数据。当数据（样本大小或样本数量）有限时，特别具有挑战性。必须选择一个机器学习算法，以最有效的方式解决优化问题；例如，神经网络使用随机梯度下降（SGD），而普通最小二乘法和梯度下降用于线性回归。当我们偏离默认算法时，必须有充分的理由这么做。在数学优化中，有时可能会使用启发式方法来确定近似最优解。这发生在经典算法过于缓慢，无法找到近似解，或者无法找到优化问题的精确解时。启发式算法的例子包括遗传算法和模拟退火算法。
- en: Example 1 – regression
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 1 – 回归
- en: An ML problem is framed as the learning of a mapping function (f) given input
    data (X) and output data (Y) such that Y = f(X). Given new input data, we should
    be able to map each datum onto (or predict) the output with our learned function,
    f. A prediction error is expected in general with noise in observed data and with
    a choice of learning algorithm that approximates the mapping function. Finding
    the set of inputs that results in the minimum error, cost, or loss is essentially
    solving the optimization problem. The choice of mapping function dictates the
    level of difficulty of optimization. The more biased or constrained the choice,
    the easier it is.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器学习问题可以表述为：给定输入数据（X）和输出数据（Y），学习一个映射函数（f），使得 Y = f(X)。给定新的输入数据，我们应该能够通过已学习的函数
    f 将每个数据映射到（或预测）输出。通常，由于观察数据中的噪声和选择的学习算法在逼近映射函数时的误差，预测误差是不可避免的。寻找能够最小化误差、代价或损失的输入集合，本质上就是在解决优化问题。映射函数的选择决定了优化的难度。选择越偏向或约束越强，优化越容易。
- en: For example, linear regression is a constrained model. Using linear algebra,
    it can be solved analytically. The inputs to the mapping function in this case
    are the model coefficients. An optimization algorithm such as iterative local
    search can be used numerically but it is almost always less efficient than an
    analytical solution. A logistic regression (for a classification task) is a less
    constrained model, and an optimization algorithm is required in this case. The
    loss or error here is also called the logistic loss or cross-entropy. While a
    global search optimization algorithm can be used in both types of regression models,
    it is mostly less efficient than using either an analytical method or a local
    search method. An iterative global search (gradient descent, for example) is suitable
    when the search space or landscape is multimodal and nonlinear, as shown in *Figure
    2**.1*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，线性回归是一个受约束的模型。通过线性代数，可以解析求解它。在这种情况下，映射函数的输入是模型系数。优化算法，如迭代局部搜索，可以用数值方法求解，但几乎总是比解析解法效率低。逻辑回归（用于分类任务）是一个约束较少的模型，在这种情况下需要使用优化算法。这里的损失或误差也称为逻辑损失或交叉熵。虽然在这两种回归模型中都可以使用全局搜索优化算法，但它通常不如使用解析方法或局部搜索方法高效。当搜索空间或地形是多模态和非线性时，适合使用迭代全局搜索（如梯度下降），正如*图
    2.1*所示。
- en: '![Figure 2.1: 3D landscape of unconstrained optimization space, where A is
    the current state](img/Figure_02_01_B18943.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1：无约束优化空间的 3D 地形图，其中 A 是当前状态](img/Figure_02_01_B18943.jpg)'
- en: 'Figure 2.1: 3D landscape of unconstrained optimization space, where A is the
    current state'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：无约束优化空间的 3D 地形图，其中 A 是当前状态
- en: Example 2 – neural network
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 2 – 神经网络
- en: A neural network is a flexible model and imposes very few constraints. A network
    typically has an input layer, a hidden layer (can be more than one), and an output
    layer of nodes, and the inputs to the mapping function are weighted to the input
    layer, as shown in *Figure 2**.2*. It is this mapping function that the supervised
    learning algorithm tries to best approximate.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一个灵活的模型，约束非常少。网络通常包含一个输入层、一个隐藏层（可以有多个）和一个输出层节点，映射函数的输入通过加权连接到输入层，如*图 2.2*所示。正是这个映射函数，监督学习算法试图最优地逼近。
- en: '![Figure 2.2: The three essential, minimal layers in a network](img/Figure_02_02_B18943.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2：网络中的三个基本最小层](img/Figure_02_02_B18943.jpg)'
- en: 'Figure 2.2: The three essential, minimal layers in a network'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：网络中的三个基本最小层
- en: The deviation of predicted output from expected output is the error value, and
    this error or cost, shown in *Figure 2**.3*, is minimized while approximating
    the function during model training. A neural network requires an iterative global
    search algorithm. Gradient descent is the preferred method to optimize a neural
    network that has variants, namely, batch and mini-batch gradient descent and SGD.
    One of the most popular SGD algorithms is **Adaptive Moment Estimation** (**Adam**),
    which computes adaptive learning rates for each parameter of the function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 预测输出与期望输出的偏差就是误差值，而这个误差或代价，如*图 2.3*所示，在模型训练过程中被最小化，同时逼近该函数。神经网络需要一个迭代的全局搜索算法。梯度下降是优化神经网络的首选方法，神经网络有多个变种，即批量梯度下降、迷你批量梯度下降和随机梯度下降（SGD）。其中最受欢迎的
    SGD 算法之一是**自适应矩估计**（**Adam**），它为每个参数计算自适应学习率。
- en: '![Figure 2.3: Minimization of cost function J(w) by gradient descent where
    w is the input (courtesy of Python Machine Learning by Sebastian Raschka)](img/Figure_02_03_B18943.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3：通过梯度下降最小化成本函数 J(w)，其中 w 是输入（来自《Python机器学习》Sebastian Raschka的贡献）](img/Figure_02_03_B18943.jpg)'
- en: 'Figure 2.3: Minimization of cost function J(w) by gradient descent where w
    is the input (courtesy of Python Machine Learning by Sebastian Raschka)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：通过梯度下降最小化成本函数 J(w)，其中 w 是输入（来自《Python机器学习》Sebastian Raschka的贡献）
- en: A gradient is a vector of partial derivatives (slope/curvature) of the function
    with respect to input variable values. The gradient descent algorithm, as the
    name suggests, requires the calculation of this gradient. The negative of the
    gradient of each input is followed downhill as the gradient points uphill, to
    lead to new values of the input. A step size is used to scale the gradient and
    control the change of input with respect to the gradient. This step size or increment
    is the learning rate, a hyper-parameter of the algorithm, and is the proportion
    in which network weights are updated. The process is repeated until the minimum
    of the function is located. Gradient descent is adapted to minimize the loss function
    of a predictive model, such as regression or classification. This adaptation results
    in SGD, as shown in *Figure 2**.4*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是函数对输入变量值的偏导数（斜率/曲率）所组成的向量。顾名思义，梯度下降算法需要计算该梯度。每个输入的梯度的负值沿着梯度指向上坡的方向向下走，从而得出新的输入值。步长用于缩放梯度，并控制输入相对于梯度的变化。这个步长或增量就是学习率，是算法的超参数，也是更新网络权重的比例。这个过程会一直重复，直到找到函数的最小值。梯度下降算法被用于最小化预测模型的损失函数，如回归或分类。这个适应过程产生了SGD，如*图
    2.4*所示。
- en: '![Figure 2.4: Gradient descent extension](img/Figure_02_04_B18943.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4：梯度下降扩展](img/Figure_02_04_B18943.jpg)'
- en: 'Figure 2.4: Gradient descent extension'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：梯度下降扩展
- en: SGD is the extension of the gradient descent optimization algorithm, wherein
    the target function is considered to be the loss or error, such as mean squared
    error for regression and cross-entropy for classification. Since the gradients
    of the target function with respect to the inputs are noisy, and deterministic
    to the extent of probabilistic approximation only, the algorithm is referred to
    as “stochastic.” Due to the sparseness and noise in training data, the evaluated
    gradients have statistical noise. Generally speaking, SGD and its variants are
    still the most used optimization algorithms for ML as well as training deep learning
    (artificial neural network) models. The inputs to a neural network are the weights
    (model parameters) and the target function is the prediction error averaged over
    one batch, which is a subset of the training dataset.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SGD是梯度下降优化算法的扩展，其中目标函数被视为损失或误差，例如回归中的均方误差和分类中的交叉熵。由于目标函数相对于输入的梯度是噪声性的，且仅能通过概率近似确定，算法因此被称为“随机”。由于训练数据中的稀疏性和噪声，评估出的梯度带有统计噪声。一般而言，SGD及其变种仍然是机器学习和深度学习（人工神经网络）模型训练中最常用的优化算法。神经网络的输入是权重（模型参数），目标函数是针对一个批次的预测误差的平均值，而这个批次是训练数据集的一个子集。
- en: A popular extension to SGD for the improvement of process efficiency, such as
    finding out the same (or better) loss in fewer iterations, is Adam. The Adam optimization
    method is computationally efficient, requires little memory, and is well suited
    for problems that are large in terms of size and features. The configuration parameters
    of Adam are the learning rate (step size), exponential decay rate (denoted by
    beta 1) for the mean (first moment) estimates, exponential decay rate (denoted
    by beta 2) for variance (second moment) estimates, and epsilon (very small number)
    to prevent any division by zero in the implementation. Larger values of learning
    rate (denoted by alpha) result in faster initial learning before an update and
    lower values of learning rate mean slower learning during the entire training.
    These parameters typically require very little tuning as they have intuitive interpretation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高过程效率（例如在更少的迭代中找到相同或更好的损失），SGD的一个流行扩展是Adam。Adam优化方法在计算上高效，内存占用少，非常适合处理规模大、特征多的问题。Adam的配置参数包括学习率（步长）、用于均值（第一矩）估计的指数衰减率（表示为beta
    1）、用于方差（第二矩）估计的指数衰减率（表示为beta 2），以及防止除零错误的非常小的数值epsilon。较大的学习率（表示为alpha）会导致在更新之前更快的初始学习，而较小的学习率则意味着在整个训练过程中较慢的学习。这些参数通常需要很少的调节，因为它们有直观的解释。
- en: A major challenge in using SGD to train a multi-layer neural network is the
    gradient calculation for nodes in the hidden layer(s) of the network. It can be
    tackled by utilizing a specific technique from calculus called the chain rule,
    and an efficient algorithm that implements this rule is called backpropagation,
    which calculates the gradient of a loss function concerning the model variables.
    The first-order derivative of a function for a specific input variable value is
    the rate of change of the function with that variable, and when there are multiple
    input variables, the (partial) derivatives form a vector. For each weight in the
    network, backpropagation calculates the gradient, which is then used by the SGD
    optimization algorithm to update the weights. Backpropagation works backward from
    the output toward the input of the network, as shown in *Figure 2**.5*. It propagates
    the error in the predicted output to compute the gradient for each input variable,
    basically a backward flow of information from the cost function through the network.
    Backpropagation involves the recursive application of the chain rule, which is
    the calculation of the derivative of a sub-function given the known derivative
    of the parent function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SGD训练多层神经网络的一个主要挑战是计算网络隐藏层节点的梯度。这个问题可以通过利用微积分中的链式法则来解决，一个实现该法则的高效算法叫做反向传播，它计算损失函数对模型变量的梯度。一个特定输入变量值的函数的一级导数是该函数随着该变量的变化率，当有多个输入变量时，（偏）导数形成一个向量。对于网络中的每个权重，反向传播计算出梯度，然后SGD优化算法使用这些梯度来更新权重。反向传播从网络的输出向输入方向传播，如*图2.5*所示。它将预测输出中的误差传播回去，以计算每个输入变量的梯度，基本上是信息从代价函数通过网络向后流动。反向传播涉及链式法则的递归应用，即在已知父函数导数的情况下，计算子函数的导数。
- en: '![Figure 2.5: Backpropagation in a neural network](img/Figure_02_05_B18943.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5：神经网络中的反向传播](img/Figure_02_05_B18943.jpg)'
- en: 'Figure 2.5: Backpropagation in a neural network'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：神经网络中的反向传播
- en: A genetic algorithm does not utilize the structure of the model, meaning it
    does not require gradients. For problems in which we use neural network models,
    we need to optimize the model using gradients that are calculated with backpropagation.
    It is only fair to say that backpropagation is a part of the optimization process,
    the optimization algorithm being SGD.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法不利用模型的结构，这意味着它不需要梯度。对于使用神经网络模型的问题，我们需要使用反向传播计算的梯度来优化模型。可以公平地说，反向传播是优化过程的一部分，而优化算法是SGD。
- en: Now that we have explored ML tasks such as regression, classification, and neural
    nets in the form of mathematical optimization problems, we shall learn about ML
    as a predictive modeling tool and how it is utilized in a few important domains.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了将回归、分类和神经网络等机器学习任务视为数学优化问题的方式，接下来我们将学习机器学习作为一种预测建模工具，并了解它在一些重要领域中的应用。
- en: ML – a predictive tool
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习——一种预测工具
- en: Working through a predictive model involves optimization at multiple steps on
    top of optimally fitting the learning algorithm to the data. It involves transforming
    raw data into a form most appropriate for consumption in learning algorithms.
    An ML model has hyperparameters that can be configured to tailor it to a specific
    dataset. It is a standard practice to test a suite of hyper-parameters for a chosen
    ML algorithm, which is called hyper-parameter tuning or optimization. A grid search
    or random search algorithm is used for such tuning. *Figure 2**.6* shows the two
    search algorithm types. Grid search is more suitable for a quick search of hyperparameters
    and is known to perform well in general. You can also use Bayesian optimization
    for hyper-parameter tuning in some problems. We will learn about these optimization
    techniques in detail in the last part of the book.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过预测模型的工作过程涉及到多个步骤的优化，除去将学习算法最佳拟合数据之外，还包括将原始数据转换为最适合在学习算法中使用的形式。机器学习模型具有可以配置的超参数，以便根据特定数据集进行调整。对于所选择的机器学习算法，测试一组超参数是标准做法，这被称为超参数调优或优化。网格搜索或随机搜索算法通常用于这种调优过程。*图
    2.6*展示了这两种搜索算法类型。网格搜索更适合快速搜索超参数，并且通常表现良好。你也可以在某些问题中使用贝叶斯优化进行超参数调优。我们将在本书的最后部分详细学习这些优化技术。
- en: '![Figure 2.6: Grid search (L) versus random search (R)](img/Figure_02_06_B18943.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6：网格搜索（左）与随机搜索（右）](img/Figure_02_06_B18943.jpg)'
- en: 'Figure 2.6: Grid search (L) versus random search (R)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：网格搜索（左）与随机搜索（右）
- en: An ML practitioner often performs a manual process for predictive model selection
    involving tasks such as data preparation, evaluating models, tuning them, and
    finally, choosing the best model for a given dataset. This can be framed as an
    optimization problem that can be solved with **automated machine learning** (**AutoML**)
    with little user intervention. The automated optimization approach to ML is also
    offered as a cloud product service by companies such as Google and Microsoft.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习实践者通常会执行一个手动过程来进行预测模型选择，这涉及到数据准备、模型评估、调整模型参数，最后为给定数据集选择最佳模型。这可以被看作是一个优化问题，可以通过**自动化机器学习**（**AutoML**）来解决，几乎不需要用户干预。自动化优化方法在机器学习中的应用，也被像谷歌和微软这样的公司提供为云产品服务。
- en: With or without a target variable in the input dataset, an ML algorithm becomes
    supervised or unsupervised learning, respectively. In reinforcement learning,
    certain behaviors are encouraged and others discouraged. The desired behavior
    is reinforced by rewards, which are gained through experiences from the environment.
    These three types of ML are shown in *Figure 2**.7*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 无论输入数据集中是否包含目标变量，机器学习算法分别会成为监督学习或无监督学习。在强化学习中，某些行为会受到鼓励，而其他行为则会被抑制。通过从环境中获得的经验，奖励强化了期望的行为。这三种机器学习类型如*图
    2.7*所示。
- en: '![Figure 2.7: The three kinds of ML – supervised learning, unsupervised learning,
    and reinforcement learning](img/Figure_02_07_B18943.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7：三种机器学习类型——监督学习、无监督学习与强化学习](img/Figure_02_07_B18943.jpg)'
- en: 'Figure 2.7: The three kinds of ML – supervised learning, unsupervised learning,
    and reinforcement learning'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：三种机器学习类型——监督学习、无监督学习与强化学习
- en: We will now talk about a few major domains where the ML model has safely secured
    its place as a predictive tool.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论机器学习模型在一些主要领域中如何安全地作为预测工具占据一席之地。
- en: E-commerce
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电子商务
- en: 'ML models help retailers understand the buying behavior of customers and their
    preferences. From historical purchase patterns of customers and click-through
    rates of products, e-commerce companies effectively recommend products and offer
    to maximize their sales. Personalized recommendations help retailers retain their
    customer base, thus creating loyalty. The following link outlines the particular
    ways ML can be utilized in the e-commerce industry:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型帮助零售商了解顾客的购买行为及其偏好。通过分析顾客的历史购买模式和产品的点击率，电子商务公司能够有效地推荐产品并提供优惠，以最大化销售额。个性化推荐帮助零售商保持顾客基础，从而创造忠诚度。以下链接概述了机器学习在电子商务行业中的具体应用：
- en: '[https://blog.shift4shop.com/machine-learning-ecommerce-industry](https://blog.shift4shop.com/machine-learning-ecommerce-industry)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://blog.shift4shop.com/machine-learning-ecommerce-industry](https://blog.shift4shop.com/machine-learning-ecommerce-industry)'
- en: Sales and marketing
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 销售与市场营销
- en: 'ML models are used in B2B marketing as well. Identifying and acquiring prospects
    with features similar to existing businesses is one use case of customer segmentation.
    Prioritizing known prospects and generating new leads based on the likelihood
    of customers taking action is achieved using lead-scoring algorithms. Companies
    can streamline their sales and marketing activities by being data-driven as well
    as algorithm-driven. Here are some ways sales and marketing have improved when
    driven by ML:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ML模型也被用于B2B营销。通过客户细分，识别并获取与现有企业特征相似的潜在客户是一个应用案例。通过潜在客户评分算法，优先考虑已知的潜在客户，并根据客户采取行动的可能性生成新的线索。公司可以通过数据驱动和算法驱动来简化销售和营销活动。以下是一些机器学习驱动的销售和营销改进方式：
- en: '[https://scinapse.ai/blog/11-ways-machine-learning-can-improve-marketing-and](https://scinapse.ai/blog/11-ways-machine-learning-can-improve-marketing-and)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://scinapse.ai/blog/11-ways-machine-learning-can-improve-marketing-and](https://scinapse.ai/blog/11-ways-machine-learning-can-improve-marketing-and)'
- en: Cybersecurity
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络安全
- en: 'Cyber-attacks may strike an organization at any time and cause serious harm;
    however, they can be predicted and prevented by ML algorithms. From processing
    both structured and unstructured data in a short time, real-time traffic can be
    analyzed to track unusual or anomalous patterns. Companies keep attacks at bay
    by analyzing these outlying points in the data. This also reduces the scope of
    human error stemming from the manual processing of massive volumes of data and
    enables humans to focus on strategizing the protection of the system from cyber-attacks.
    The following data-driven methods pointed out by Kaspersky are worth studying:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 网络攻击可能随时袭击组织并造成严重损害；然而，借助机器学习（ML）算法，它们可以被预测和防止。通过在短时间内处理结构化和非结构化数据，可以实时分析流量，追踪异常或不寻常的模式。公司通过分析数据中的这些离群点来抵御攻击。这也减少了来自手动处理大量数据的人为错误的范围，使人类能够集中精力制定保护系统免受网络攻击的策略。以下由卡巴斯基指出的数据驱动方法值得研究：
- en: '[https://www.kaspersky.com/enterprise-security/wiki-section/products/machine-learning-in-cybersecurity](https://www.kaspersky.com/enterprise-security/wiki-section/products/machine-learning-in-cybersecurity)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaspersky.com/enterprise-security/wiki-section/products/machine-learning-in-cybersecurity](https://www.kaspersky.com/enterprise-security/wiki-section/products/machine-learning-in-cybersecurity)'
- en: Having explored how ML works as a predictive modeling tool in the industry,
    we will learn in the next section how mathematical modeling can be used as a prescriptive
    tool in different sectors.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了机器学习如何作为预测建模工具在行业中应用后，我们将在下一节中学习数学建模如何作为规定性工具在不同领域的应用。
- en: Mathematical modeling – a prescriptive tool
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学建模——一种规定性工具
- en: Businesses often make complex decisions about their course of action to achieve
    objectives with the help of mathematical modeling or heuristics. A mathematical
    model in this sense is a prescriptive analytical tool. Answering the “where” and
    “when” is as important as answering what happened in the past (descriptive analytics)
    and what could happen in the future (predictive analytics). If a business wants
    to drive decisions from data in addition to insights and future predictions, it
    has to use both predictive and prescriptive tools in an integrated fashion.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 企业通常通过数学建模或启发式方法做出关于行动方针的复杂决策，以实现目标。从这个意义上说，数学模型是一种规定性分析工具。回答“在哪里”和“何时”与回答过去发生了什么（描述性分析）和未来可能发生什么（预测性分析）同样重要。如果企业希望从数据中推动决策，并且除了洞察和未来预测之外，还需要使用预测性和规定性工具进行综合应用。
- en: '![Figure 2.8: Mathematical optimization or mathematical modeling](img/Figure_02_08_B18943.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图2.8：数学优化或数学建模](img/Figure_02_08_B18943.jpg)'
- en: 'Figure 2.8: Mathematical optimization or mathematical modeling'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：数学优化或数学建模
- en: We will have a look at examples from industry verticals wherein these work in
    tandem, resulting in higher productivity and profitability.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看一些行业垂直领域的例子，在这些领域，机器学习和其他方法协同工作，带来了更高的生产力和盈利能力。
- en: Finance
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 金融
- en: Financial services, including banks, rely on ML models as well as mathematical
    models to determine the right allocation of their investment portfolios. An ML
    model in the form of time-series forecasting helps with the prediction of asset
    performance, which in turn is channelled into applications leveraging a mathematical
    model. Based on the market movements and forecasts, the mathematical optimization
    application determines the optimal allocation. The best portfolio allocation also
    takes individual investment objectives and preferences into account. These mitigate
    risks and maximize risk-adjusted returns on investments.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 包括银行在内的金融服务行业依赖机器学习模型以及数学模型来确定投资组合的正确分配。以时间序列预测形式呈现的机器学习模型帮助预测资产表现，进而将预测结果输入到应用程序中，应用数学模型。根据市场变动和预测，数学优化应用决定最优的分配方式。最佳的投资组合分配还会考虑个人的投资目标和偏好。这些有助于降低风险并最大化风险调整后的投资回报。
- en: Retail
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零售
- en: Leading retailers utilize ML models to forecast demand for products, especially
    high-selling ones in particular locations at given times. They feed these predictions
    into mathematical models to maximize profits and customer satisfaction. The mathematical
    optimization application, in this case, uses the forecast as input to generate
    optimal production, pricing, inventory and distribution planning, logistics, and
    warehousing, thereby making the best business decisions while minimizing operating
    costs. Supply chain management is a classic example of mathematical optimization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 领先的零售商利用机器学习模型预测产品的需求，特别是在特定地点和时间的畅销产品。他们将这些预测输入数学模型，以最大化利润和客户满意度。在这种情况下，数学优化应用将预测结果作为输入，生成最优的生产、定价、库存和分销计划、物流和仓储，从而在最小化运营成本的同时做出最佳商业决策。供应链管理是数学优化的经典示例。
- en: Energy
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 能源
- en: Governments and industry players are making high-stakes decisions on strategic
    investments in network infrastructure and resources as electric power is making
    a transition from being dependent on fossil fuels to renewables such as solar
    and wind. Organizations are utilizing ML models to predict future power demand
    and capacity needs. These forecasts are fed into mathematical models or mathematical
    optimization applications that generate optimal long-term investment planning
    and help in making decisions about strategic investments.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 政府和行业参与者在网络基础设施和资源的战略投资决策上扮演着关键角色，尤其是随着电力从依赖化石燃料转向太阳能和风能等可再生能源。各组织利用机器学习模型预测未来的电力需求和容量需求。这些预测被输入到数学模型或数学优化应用中，生成最优的长期投资规划，并帮助做出战略投资决策。
- en: Digital advertising
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数字广告
- en: Search engine giants such as Google leverage ML (and deep learning) models to
    predict the products and services individuals will be interested in looking up,
    based on their prior search history and a few other factors. In addition, they
    utilize mathematical models to figure out the online advertisements that can be
    shown to individual users at certain times. Search engine giants use this optimization
    model to charge advertisers and maximize their revenue.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Google 这样的搜索引擎巨头利用机器学习（ML）和深度学习模型，根据用户的历史搜索记录以及其他一些因素，预测个人可能感兴趣的产品和服务。此外，他们还利用数学模型来确定在特定时间可以展示给单个用户的在线广告。搜索引擎巨头使用这种优化模型来向广告商收费并最大化收入。
- en: These domains have added mathematical modeling to their data science toolbox
    that handles complex, significant, and scalable business problems for greater
    value delivery. Other industries, such as telecommunications and cloud computing,
    also use both models to precisely assess long-term demand and capacity needs to
    make the best business decisions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些领域将数学建模加入了他们的数据科学工具箱，以处理复杂、重要且可扩展的商业问题，从而提供更大的价值。其他行业，如电信和云计算，也使用这些模型来精确评估长期需求和容量需求，以做出最佳商业决策。
- en: Summary
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced ML models as problems of mathematical optimization
    or mathematical programming. We found out that an end-to-end ML project is the
    sum of multiple small optimization problems. We also gained knowledge about how
    businesses can unlock the true value of data upon leveraging mathematical models
    (primarily driven by mathematical equations) in addition to ML (driven by data)
    models. We learned that an ML model is predominantly a predictive tool and a mathematical
    model is a prescriptive one.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将机器学习模型介绍为数学优化或数学规划的问题。我们发现，端到端的机器学习项目实际上是多个小的优化问题的总和。我们还了解到，企业如何通过利用数学模型（主要由数学方程驱动）与机器学习（由数据驱动）模型的结合，解锁数据的真正价值。我们学习到，机器学习模型主要是预测工具，而数学模型则是指导性工具。
- en: In the next chapter (which begins the next part of the book), we will take a
    meticulous look at a well-known algorithm called PCA, utilized in an unsupervised
    ML model fit to data with high dimensionality. It is a dimensionality reduction
    technique and one of the most tried and tested mathematical tools employing constrained
    optimization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章（即本书下一个部分的开始），我们将详细探讨一个著名算法，叫做PCA，它用于无监督机器学习模型中，以适应高维数据。PCA是一种降维技术，是一种经过验证的数学工具，采用约束优化方法。
- en: Part 2:Mathematical Tools
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：数学工具
- en: In this part, you will learn some of the most tried and tested mathematical
    tools and algorithms. On the one hand, there are algorithms for data dimensionality
    reduction, optimization of machine learning models, and data classification, which
    are explored through Python code. On the other hand, there are algorithms that
    model the relationships between objects (data points) and estimate the current
    and future states of variables (unknown and immeasurable ones) of a dynamic system.
    There are also other algorithms that predict the next future state probabilistically
    from knowledge of the present state of a process, explained with simple examples
    and Python code.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将学习一些经过验证的数学工具和算法。一方面，涉及数据降维、机器学习模型优化和数据分类的算法，这些内容通过Python代码进行探索。另一方面，涉及建模对象（数据点）之间关系的算法，以及估计动态系统中变量（未知和无法测量的）当前和未来状态的算法。还有一些算法从当前状态的知识出发，概率性地预测下一状态，且通过简单的例子和Python代码进行解释。
- en: 'This part has the following chapters:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 3*](B18943_03.xhtml#_idTextAnchor042), *Principal Component Analysis*'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第3章*](B18943_03.xhtml#_idTextAnchor042)，*主成分分析*'
- en: '[*Chapter 4*](B18943_04.xhtml#_idTextAnchor053), *Gradient Descent*'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B18943_04.xhtml#_idTextAnchor053)，*梯度下降法*'
- en: '[*Chapter 5*](B18943_05.xhtml#_idTextAnchor064), *Support Vector Machine*'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B18943_05.xhtml#_idTextAnchor064)，*支持向量机*'
- en: '[*Chapter 6*](B18943_06.xhtml#_idTextAnchor070), *Graph Theory*'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B18943_06.xhtml#_idTextAnchor070)，*图论*'
- en: '[*Chapter 7*](B18943_07.xhtml#_idTextAnchor081), *Kalman Filter*'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B18943_07.xhtml#_idTextAnchor081)，*卡尔曼滤波器*'
- en: '[*Chapter 8*](B18943_08.xhtml#_idTextAnchor087), *Markov Chain*'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B18943_08.xhtml#_idTextAnchor087)，*马尔可夫链*'
