- en: Databases and Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库与神经网络
- en: In this chapter, we will look at and define **Artificial Neural Network** (**ANN**)
    and draw data from a data developer's knowledge of data, databases, and data models
    to help him or she understand the purpose and use of neural networks, and why
    neural networks are so significant to data science and statistics.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将定义**人工神经网络**（**ANN**），并结合数据开发人员对数据、数据库和数据模型的知识，帮助他们理解神经网络的目的和用途，以及为什么神经网络对数据科学和统计学如此重要。
- en: 'We have organized the information in this chapter into the following key areas:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将本章内容整理为以下几个关键领域：
- en: Definition of a neural network
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的定义
- en: Relating a neural network model to a database model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将神经网络模型与数据库模型联系起来
- en: Looking at R-based neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看基于R的神经网络
- en: Use cases
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例
- en: Ask any data scientist
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问问任何数据科学家
- en: Today, if you ask any data scientist about the statistical methods, (or even
    a few) you will most likely discover that there are two most well-known statistical
    methods used within the practice of data science and the statistics industry today
    for predictive modeling. We introduced these two methods in [Chapter 6](8a0b3272-dfa0-46ca-9e90-f6050f2007cd.xhtml)*,
    Database Progression to Database Regression*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，如果你问任何数据科学家关于统计方法的问题（即便是几个问题），你很可能会发现如今在数据科学和统计行业中，最著名的两种统计方法就是用于预测建模的这两种方法。我们在[第6章](8a0b3272-dfa0-46ca-9e90-f6050f2007cd.xhtml)*数据库发展到数据库回归*中介绍了这两种方法。
- en: 'These two methods are as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法如下：
- en: Linear regression
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic regression
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: The **linear regression** method is probably considered to be the *classic*
    or most common starting point for problems, where the goal is to predict a numerical
    quantity. The **Linear Regression** (or **LR**) model is based on a linear combination
    of input features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归**方法可能被认为是预测数值量时问题的*经典*或最常见的起点。**线性回归**（或**LR**）模型基于输入特征的线性组合。'
- en: The **logistic regression** method uses a nonlinear transformation of this linear
    feature combination in order to restrict the range of the output in the interval
    [0, 1]. In doing so, it predicts the probability that the output belongs to one
    of two classes. Thus, it is a very well-known technique for classification.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归**方法使用对这种线性特征组合的非线性变换，以将输出的范围限制在区间[0, 1]内。这样，它预测输出属于两个类别之一的概率。因此，它是一个非常著名的分类技术。'
- en: Recall that classification is the process of recognizing to which set of categories
    (or sub-populations) a new or different observation belongs, on the basis of a
    training set of data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，分类是根据训练数据集，识别新的或不同的观察结果属于哪一类别（或子人群）的过程。
- en: Both of these methods have their individual strengths, but both also share the
    same disadvantage in that they are not very good at predicting if they have to
    deal with the situation of a high volume of input features.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法各有其独特的优势，但它们也有相同的缺点，即在面对大量输入特征的情况时，它们的预测效果并不好。
- en: In this chapter (as an alternative to linear and logistic regression), we want
    to introduce the concept of ANNs, which is technically a nonlinear approach to
    solving both regression and classification problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中（作为线性回归和逻辑回归的替代方法），我们希望介绍人工神经网络（ANN）的概念，这实际上是一种非线性的方法，用于解决回归和分类问题。
- en: They (ANNs) are significantly more robust when dealing with a higher dimensional
    input feature space and, for classification, they possess a **natural** way to
    handle more than two output classes. We'll talk more about the advantages of  ANNs
    later in this chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理更高维的输入特征空间时（ANNs）具有显著更强的鲁棒性，并且在分类问题中，它们具有一种**自然**的方式来处理多于两个输出类别的问题。我们将在本章后面详细讨论ANN的优势。
- en: Artificial neural networks are a **biologically inspired** statistical method
    or model (based on the structure and functions of biological neural networks),
    with their roots dating all the way back to the 1940s.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络是一种**生物启发式**统计方法或模型（基于生物神经网络的结构和功能），其根源可以追溯到20世纪40年代。
- en: 'Another interesting viewpoint:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的观点：
- en: Neural Networks are a machine learning framework that attempts to mimic the
    learning pattern of natural biological neural networks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一种机器学习框架，试图模仿自然生物神经网络的学习模式。
- en: '- Jose Portilla, Udemy Data Science Instructor.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '- Jose Portilla，Udemy 数据科学讲师。'
- en: Interest in artificial neural networks has varied greatly since then--mostly
    because the first artificial neural network models were very rudimentary and therefore
    found to be limited in practice compared to the expectations at the time.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自那时以来，人工神经网络的兴趣变化很大——主要是因为最早的人工神经网络模型非常初步，因此与当时的预期相比，实践中发现其局限性。
- en: Neural networks have not always been popular, partly because they were, and
    still are in some cases, computationally expensive and partly because they did
    not seem to yield better results when compared with simpler methods such as **support
    vector machines** (**SVMs**). Nevertheless, neural networks have, once again,
    raised attention and become popular.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络并不总是受欢迎，部分原因是它们计算上昂贵，且在某些情况下仍然如此；另外，部分原因是与更简单的方法如**支持向量机**（**SVMs**）相比，它们似乎并没有带来更好的结果。然而，神经网络再次引起了关注并变得流行。
- en: -Michy Alice, September 2015.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: -Michy Alice, 2015年9月。
- en: Furthermore, training a large artificial neural network does necessitate substantial
    computational resources. Recently, there has been a huge resurgence in the interest
    in artificial neural networks as distributed on-demand computing resources are
    now becoming widespread, and an important area of machine learning, known as **deep
    learning**, is already extremely popular and showing great promise.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，训练一个大型人工神经网络确实需要大量的计算资源。最近，由于分布式按需计算资源的普及，人工神经网络的兴趣重新激增，并且被称为**深度学习**的机器学习领域已经非常流行，且展现出巨大潜力。
- en: The timing is right!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 时机恰到好处！
- en: For this reason, it is a great time for a data developer to begin learning about ANNs
    models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在是数据开发者开始学习ANNs模型的好时机。
- en: Deep learning (also known as **deep structured learning**, **hierarchical learning**,
    or **deep machine learning**) is a class of machine learning algorithms that use
    a cascade of many layers of nonlinear processing units for feature extraction
    and transformation ([en.wikipedia.org/wiki/Deep_learning](https://en.wikipedia.org/wiki/Deep_learning)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（也称为**深度结构学习**、**层次学习**或**深度机器学习**）是一类使用多个非线性处理单元层级来进行特征提取和转换的机器学习算法 ([en.wikipedia.org/wiki/Deep_learning](https://en.wikipedia.org/wiki/Deep_learning))。
- en: Let's get started!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧！
- en: Defining neural network
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义神经网络
- en: 'Our approach is to always start with a solid definition. So--what exactly is
    an artificial neural network? Perhaps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法是始终从一个明确的定义开始。那么——什么是人工神经网络呢？或许是：
- en: A computer system modeled on the human brain and nervous system.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模拟人脑和神经系统的计算机系统。
- en: 'A popular understanding or, if we check online for a definition:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的理解，或者如果我们在线查找定义的话：
- en: '*In machine learning and cognitive science, artificial neural networks (ANNs)
    are a family of models inspired by biological neural networks (the central nervous
    systems of animals, in particular, the brain) and are used to estimate or approximate
    functions that can depend on a large number of inputs and are generally unknown.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*在机器学习和认知科学中，人工神经网络（ANNs）是一类受生物神经网络启发的模型（尤其是动物的中央神经系统，特别是大脑），用于估计或近似那些可能依赖大量输入并且通常是未知的函数。*'
- en: So there are many definitions, but in summarization, the common theme through
    all of the definitions you'll find for an  ANNs is that it is defined as a computer
    data model based upon the concepts of how a human brain works.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有许多不同的定义，但总体来说，你会发现所有关于ANNs的定义都围绕着一个共同的主题：它被定义为基于人脑工作原理的计算机数据模型。
- en: '![](img/12fda3d1-48fb-47aa-87ad-2d26010bf6b6.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12fda3d1-48fb-47aa-87ad-2d26010bf6b6.jpg)'
- en: ANN model representation
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ANN模型表示
- en: 'As the preceding screenshot suggests, the construction of a typical artificial
    neural network is based on some number of the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的截图所示，典型的人工神经网络的构建基于以下几个要素：
- en: Nodes
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点
- en: Layers
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层
- en: Connections
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接
- en: Nodes
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点
- en: Each of the layers is made up of a number of interconnected nodes--each node
    containing what is referred to as an **activation function**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层由若干互相连接的节点组成——每个节点包含所谓的**激活函数**。
- en: There will be at least one node in the input layer for each predictor variable
    that exists within the data. The input nodes feed the input values (the **problem
    to be solved**) to each of the nodes in the (next) hidden layer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层中将至少有一个节点用于每个数据中的预测变量。输入节点将输入值（**待解决的问题**）传递给（下一个）隐藏层中的每个节点。
- en: The input(s) to each node are summed to give a single value, *x*. This is then
    inputted to the node's activation function, and the output of the node is the
    output of its activation function, *f*(*x*).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点的输入会被求和，得到一个单一的值，*x*。然后，这个值会被输入到节点的激活函数中，节点的输出就是其激活函数的输出，*f*(*x*)。
- en: There are a number of functions that can be used in artificial neural network
    nodes, including the radial basis function or a simple linear function, but the
    most common is the sigmoid or logistic function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络节点中可以使用多种函数，包括径向基函数或简单的线性函数，但最常见的是Sigmoid函数或逻辑函数。
- en: A sigmoid function is typically the most common and perhaps easiest to understand.
    The sigmoid function is a mathematical function having a characteristic S-shaped
    curve or sigmoid curve. Sigmoid functions have a domain of all real numbers, with
    the return value monotonically increasing most often from 0 to 1 or alternatively
    from −1 to 1, depending on convention. ([https://en.wikipedia.org/wiki/Sigmoid_function](https://en.wikipedia.org/wiki/Sigmoid_function))
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数通常是最常见且也许最容易理解的。Sigmoid函数是一个数学函数，具有特征的S形曲线或Sigmoid曲线。Sigmoid函数的定义域为所有实数，返回值通常单调递增，从0到1，或者根据约定，从-1到1。（[https://en.wikipedia.org/wiki/Sigmoid_function](https://en.wikipedia.org/wiki/Sigmoid_function)）
- en: The nodes within an artificial neural network will always produce or calculate
    a value between 0 and 1, so the output of a node can only ever be in the collection
    0 to 1.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络中的节点总是会产生或计算一个介于0和1之间的值，因此一个节点的输出只能在0到1之间。
- en: Layers
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层
- en: The **layer** is a universal term used to define a collection of **nodes** operating
    together at a specific depth within an artificial neural network.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**层**是一个通用术语，用来定义在人工神经网络中按特定深度协同工作的**节点**集合。'
- en: Hidden layers will have a variable number of nodes (determined by the training
    algorithm used). Each of those nodes will contain numerous functions with logic
    to process the inputs and calculate a result. The resulting value is then passed
    to the next layer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 隐含层将有一个可变数量的节点（由使用的训练算法决定）。每个节点将包含多个逻辑函数，用来处理输入并计算结果。最终结果将被传递到下一层。
- en: 'The layer summary is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 层的总结如下：
- en: '**Input layer**: This contains your data, with one node per variable'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：该层包含你的数据，每个变量对应一个节点。'
- en: '**Hidden layer(s)**: Each layer attempts to learn a different aspect about
    your data by minimizing an error/cost function'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐含层**：每一层通过最小化误差/成本函数，尝试学习关于数据的不同方面。'
- en: '**Output layer**: This is the simplest layer, usually consisting of a single
    output for classification problems'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：这是最简单的一层，通常由一个用于分类问题的输出组成。'
- en: Training
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: Recalling from the previous section describing artificial neural network nodes,
    if all the nodes work the same way or, at least produce the same result (0 or
    1), how does the network differentiate between classes?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆上一节描述的人工神经网络节点，如果所有节点的工作方式相同，或者至少产生相同的结果（0或1），那么网络如何区分不同的类别呢？
- en: It uses assigned weights to each node's inputs. This is a feature (or variable
    in the data) that can have a large or small weighting, which then results in varying
    the contribution that the variable or feature makes to the *sum* in any node.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 它将权重分配给每个节点的输入。这是一个特征（或数据中的变量），其权重可以大也可以小，进而导致变量或特征对任何节点的*和*的贡献有所不同。
- en: In practice, a variable can be assigned a large weight feeding into one node
    and an almost zero weight feeding into another node, which would mean that the
    variable would have a strong influence on the first and practically none on the
    second.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，一个变量可以被赋予一个较大的权重，传递到一个节点，而传递到另一个节点的权重几乎为零，这意味着该变量对第一个节点的影响很大，对第二个节点几乎没有影响。
- en: The sigmoid function (which we also mentioned in the previous section) indicates
    that the node's output switches from a zero to a one when its *x* value crosses
    a threshold. This can transpire in numerous ways, such as if one highly weighted
    input has a high value or if a collection of medium-weighted inputs have high
    values.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数（我们在上一节也提到了）表明，当节点的*x*值跨越一个阈值时，节点的输出会从0变为1。这可以通过多种方式发生，例如当一个高权重的输入具有较高的值，或者一组中等权重的输入具有较高的值时。
- en: Training the artificial neural network is the process of systematically discovering
    the best values of weights to maximize the accurateness of classification.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 训练人工神经网络的过程是系统地发现最优权重值，以最大化分类的准确性。
- en: Solution
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案
- en: The value coming out of a node in the hidden layer is multiplied by a weight
    associated with the node that did the calculation and adds to the weighted values
    of other nodes. This summation becomes the artificial neural network model's output
    or solution. (Observe that the term used for this varies between data scientists,
    some say output, others solution, result, or even outcome).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中某个节点的输出值会与进行计算的节点的权重相乘，并加到其他节点的加权值中。这一求和结果成为人工神经网络模型的输出或解决方案。（需要注意的是，这个术语在数据科学家之间有所不同，有些人称之为输出，有些人称之为解决方案、结果，甚至是结果值）。
- en: Again, the problems are presented to the artificial neural network (through
    the model's input layer), which will communicate to one or (most likely) more
    hidden layers where the actual processing is done via a system of weighted connections.
    The hidden layers then link to an output layer where the solution is given.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，问题被呈现给人工神经网络（通过模型的输入层），该网络会传递到一个或（更可能）多个隐藏层，实际的处理通过加权连接的系统完成。然后，隐藏层与输出层相连接，给出解决方案。
- en: Most artificial neural network models contain some form of an algorithm that
    governs how the model **learns**. This is the algorithm that we discussed earlier
    as the process that **adjusts the weights** of the node connections according
    to the (input) problems that have been submitted to it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人工神经网络模型包含某种形式的算法，决定模型如何**学习**。这个算法就是我们之前讨论的，通过它根据提交给模型的（输入）问题来**调整节点连接的权重**。
- en: This **adjusting of the node's weights** (some call **right-regulating**) is
    similar to the idea of learning to recognize an image (or **determining a solution**)
    based on experience (in this case, the experience might be reviewing example after
    example).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种**调整节点权重**（有些人称之为**右调节**）类似于基于经验（在这种情况下，经验可能是反复查看示例）来识别图像（或**确定解决方案**）的想法。
- en: Understanding the concepts
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解这些概念
- en: To gain an understanding of a new concept, it always helps to draw similarities
    between the familiar and the new.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解一个新概念，通常有助于将熟悉的内容与新知识进行对比。
- en: To that point, to better comprehend the notion of artificial neural networks,
    it would be helpful for us to compare the concepts of ANNs to how a conventional
    database algorithm processes data and information (or how it works).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解人工神经网络的概念，比较ANNs与传统数据库算法如何处理数据和信息（或它是如何工作的）将会是有帮助的。
- en: In the next section, we will compare concepts concerning artificial neural network
    models to those of common data or database models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将比较人工神经网络模型的概念与常见的数据或数据库模型。
- en: Neural network models and database models
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络模型和数据库模型
- en: As we noted in the previous section of this chapter, neural network models are
    a **system of processing nodes**, interconnected across layers that do not process
    data in a sequential manner. How does this scheme compare to a conventional database model or
    program?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章前面部分所指出的那样，神经网络模型是一个**处理节点系统**，跨越不同层级互联，这些层级不会以顺序的方式处理数据。那么，这种结构与传统的数据库模型或程序有什么不同呢？
- en: '![](img/2370e20b-d11c-4195-a1c3-82bfa4fe1780.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2370e20b-d11c-4195-a1c3-82bfa4fe1780.png)'
- en: A conventional database model representation
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 传统数据库模型表示
- en: As we noted in the previous section of this chapter, neural network models are
    a **system of processing nodes**, interconnected across layers that do not process
    data in a sequential manner. How does this scheme compare to a conventional database
    model or program?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章前面部分所指出的那样，神经网络模型是一个**处理节点系统**，跨越不同层级互联，这些层级不会以顺序的方式处理数据。那么，这种结构与传统的数据库模型或程序有什么不同呢？
- en: In the technology industry, a conventional (database) model is sometimes called
    a **serial processing component**, which means that this type of model has a central
    processing unit or CPU (perhaps you can think of this as a large, central *processing
    node*) that accesses raw data and instructions (already stored in memory).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术行业中，传统的（数据库）模型有时被称为**串行处理组件**，这意味着这种类型的模型拥有一个中央处理单元或CPU（或许你可以将其理解为一个大型的中央*处理节点*），用于访问存储在内存中的原始数据和指令。
- en: In a conventional database model, the central processor carries out calculations
    or logic (an algorithm) on input or selected data, stores the calculated results
    (in a specified memory location), then goes on to the next instruction (and data),
    and so on—until a solution or result is reached.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的数据库模型中，中央处理器对输入或选定的数据进行计算或逻辑处理（算法），存储计算结果（在指定的内存位置），然后继续执行下一个指令（和数据），依此类推——直到达到解决方案或结果。
- en: 'This idea is somewhat analogous to the concept of a solitary processing *stream* within
    a neural network:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念有点类似于神经网络中单独处理*流*的概念：
- en: A node accepts or consumes data
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个节点接受或消耗数据
- en: A node applies logic
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个节点应用逻辑
- en: A node outputs its results
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个节点输出它的结果
- en: Similar, but keep in mind that conventional database models are serial in that
    they perform each task (or instruction) in a linear, or one after the other, fashion,
    while multiple node streams within an artificial neural network model perform
    tasks in parallel.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然相似，但请记住，传统数据库模型是串行的，因为它们按线性或一个接一个的方式执行每个任务（或指令），而人工神经网络模型中的多个节点流则并行执行任务。
- en: So, in a serial system, the computational steps are deterministic, sequential,
    and logical, and the state of a given variable can be tracked from one operation
    to another. In comparison, ANNs are not sequential or necessarily deterministic.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在一个串行系统中，计算步骤是确定性的、顺序的和逻辑的，并且可以从一个操作到另一个操作追踪给定变量的状态。相比之下，人工神经网络（ANNs）既不是顺序的，也不一定是确定性的。
- en: There are other key differences, which include the following notions which are
    explained in the following sub-sections.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些关键的不同之处，包括以下在接下来的子部分中解释的概念。
- en: No single or main node
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 没有单一或主要节点
- en: Rather than a single, complex central processor (or node), there are many simple
    nodes--which generally do nothing more than taking the weighted sum of their inputs
    from other nodes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不是一个复杂的中央处理器（或节点），而是许多简单的节点——它们通常只做一件事，即将它们从其他节点接收到的加权和进行计算。
- en: Not serial
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不是串行的
- en: ANNs do not execute programmed instructions linearly or serially; they respond
    in parallel (either simulated or actual) to the inputs presented to them.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络并不按顺序或串行执行编程指令；它们并行响应（无论是模拟的还是实际的）提供给它们的输入。
- en: No memory address to store results
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 没有内存地址来存储结果
- en: There are also no separate memory addresses to store data within an artificial
    neural network model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络模型中，也没有单独的内存地址来存储数据。
- en: Instead, information is contained in the overall activation *state* of the model.
    **Knowledge** is therefore represented by the artificial neural network model
    itself, which is quite literally more than the sum of its individual components.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，信息包含在模型的整体激活*状态*中。因此，**知识**由人工神经网络模型本身表示，它字面上是其各个组成部分的总和之外。
- en: As a final point, for the data developer, the notion of an artificial neural
    network might be imagined as **kicking-off** and running multiple SQL queries
    asynchronously. Imagine creating a SQL **DTS** (**Data Transformation Services**)
    or **SQL Server Integration Services** (**SSIS**) package with a simple branching
    task flow so that DTS will launch tasks in individual **spids** (**Server Process
    ID**). Each of the spids then would align to the idea of a neural network node
    stream, all working in parallel to create a result or solution.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于数据开发者来说，人工神经网络的概念可以想象为**启动**并异步运行多个SQL查询。试想创建一个简单分支任务流的SQL **DTS**（**数据转换服务**）或**SQL
    Server 集成服务**（**SSIS**）包，这样DTS将会在单独的**spids**（**服务器进程ID**）中启动任务。每个spid随后将与神经网络节点流的概念相对齐，所有节点并行工作以创建结果或解决方案。
- en: A very good explanation of how a neural network works--using a data developer-type
    practical example—can be found online authored by Sunil Ray.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常好的神经网络工作原理的解释——通过一个数据开发者类型的实际例子——可以在线找到，作者是Sunil Ray。
- en: 'Understanding and coding Neural Networks from Scratch in Python and R, which,
    as of writing this, can be found at [www.analyticsvidhya.com](http://www.analyticsvidhya.com)
    under: [/blog/2017/05/neural-network-from-scratch-in-python-and-r](https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 理解和从头编写神经网络的Python和R代码，在撰写本文时，可以在[www.analyticsvidhya.com](http://www.analyticsvidhya.com)上找到，链接为：[/blog/2017/05/neural-network-from-scratch-in-python-and-r](https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/)。
- en: 'He terms the following quote:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 他提到以下引述：
- en: '*"If you have been a developer or seen one work you know how it is to search
    for bugs in a code. You would fire various test cases by varying the inputs or
    circumstances and look for the output. The change in output provides you a hint
    on where to look for the bug--which module to check, which lines to read. Once
    you find it, you make the changes and the exercise continues until you have the
    right code/application.*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*“如果你曾经是开发人员，或者看过开发人员的工作，你会知道在代码中搜索bug是什么感觉。你会通过改变输入或环境来运行不同的测试用例，查看输出。输出的变化会给你一些提示，告诉你应该查找哪个模块，阅读哪些行。找到问题后，你会进行修改，然后继续这个过程，直到你有了正确的代码/应用。”*'
- en: '*Neural networks work in very similar manner. It takes several input, processes
    it through multiple neurons from multiple hidden layers and returns the result
    using an output layer. This result estimation process is technically known as
    **Forward Propagation**.*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经网络的工作方式非常相似。它接受多个输入，通过多个隐藏层的多个神经元处理数据，然后通过输出层返回结果。这个结果估算过程在技术上被称为**前向传播**。*'
- en: '*Next, we compare the result with actual output. The task is to make the output
    to neural network as close to actual (desired) output. Each of these neurons are
    contributing some error to final output. How do you reduce the error?*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*接下来，我们将结果与实际输出进行比较。任务是将神经网络的输出尽可能接近实际（期望的）输出。每个神经元都会对最终输出贡献一定的误差。如何减少这些误差？*'
- en: '*We try to minimize the value/ weight of neurons those are contributing more
    to the error and this happens while traveling back to the neurons of the neural
    network and finding where the error lies. This process is known as **Backward
    Propagation**."*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们试图最小化那些对误差贡献较大的神经元的值/权重，而这发生在神经网络反向传播过程中，找出误差所在的地方。这个过程被称为**反向传播**。*'
- en: Okay, that was a lot to take in--but now that we, with any luck, have a respectable
    understanding of what an artificial neural network is and how it works, let's
    look at ANNs and the R programming language.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这确实有点多——但现在，幸运的话，我们已经对人工神经网络是什么以及它如何工作有了一个相对清晰的理解。接下来，让我们看看ANN和R编程语言的结合。
- en: R-based neural networks
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于R的神经网络
- en: Now that we know a bit about how artificial neural networks work, let's review
    some fundamental information on how to implement one with the R programming language.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些人工神经网络的工作原理，让我们回顾一些实现ANN的基本信息，特别是使用R编程语言时。
- en: '![](img/0798f4c1-d3a5-4031-966e-cb4336004936.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0798f4c1-d3a5-4031-966e-cb4336004936.png)'
- en: R-based ANN packages
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 基于R的ANN包
- en: The idea here is not to give a detailed step-by-step instruction manual on how
    to create an intricate and powerful ANN, but more to show how one can easily create
    an ANN model using R—with literally only basic R skills or experience.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目的是不是提供一个详细的步骤指南，告诉你如何创建一个复杂且强大的ANN，而是展示如何通过R语言轻松创建一个ANN模型——实际上只需要基本的R技能或经验。
- en: References
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Yes, you will find that there is plenty of respectable, easy-to-understand,
    and valuable information and examples on artificial neural networks generously
    obtainable on the internet.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你会发现，互联网上有大量尊重且易于理解的、关于人工神经网络的有价值信息和示例，值得一读。
- en: One such resource is offered by Gekko Quant ([http://gekkoquant.com/author/gekkoquant/](http://gekkoquant.com/author/gekkoquant/)) and
    is worth taking some time to locate and read.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个这样的资源由Gekko Quant提供，([http://gekkoquant.com/author/gekkoquant/](http://gekkoquant.com/author/gekkoquant/))，值得花时间查找并阅读。
- en: This information offers a very nice tutorial that produces an artificial neural
    network that takes a single input (a number that you want to calculate a square
    root for) and produces a single output (the square root of the number input).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这段信息提供了一个非常好的教程，展示了如何创建一个人工神经网络，该网络接受一个输入（你想计算平方根的数字）并输出一个结果（该数字的平方根）。
- en: 'The output of this artificial neural network example model is nicely displayed
    in an easy-to-understand format and I''ll show it here as a great example of how
    the results of an ANN model should look:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个人工神经网络示例模型的输出以易于理解的格式展示，我将在这里展示它作为一个很好的示例，说明ANN模型的输出应如何呈现：
- en: '![](img/1cb8d94b-329a-4072-b8b4-4d4c41afd8f4.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1cb8d94b-329a-4072-b8b4-4d4c41afd8f4.jpg)'
- en: Results of an ANN
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的结果
- en: From the output information shown here, you can see (as the author declares)
    that the neural network does a reasonable job at finding the square root (of each
    input number).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里显示的输出信息可以看出（正如作者所声明的），神经网络在求平方根（每个输入数值）的任务中表现得相当不错。
- en: Before a data scientist can begin the process of fitting an artificial neural
    network, some important time-saving groundwork needs to be accomplished, which
    we'll discuss in the next few sections.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学家开始拟合人工神经网络之前，必须完成一些节省时间的重要准备工作，我们将在接下来的几节中讨论这些内容。
- en: To be sure, artificial neural networks are not easy to understand, train, and
    tune; some preparatory or preprocessing first is strongly recommended.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 需要明确的是，人工神经网络并不容易理解、训练和调优；因此，强烈建议先进行一些预处理。
- en: Data prep and preprocessing
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备和预处理
- en: Let's start with the obvious—our data!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从显而易见的开始——我们的数据！
- en: The more experience a data scientist gains with working with ANN models, the
    more he or she will come to understand the importance of formally reviewing and
    preparing the data ahead of he or she can begin attempting to train or fit the
    model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家在使用ANN模型时经验越丰富，就越能理解正式审查和准备数据的重要性，在开始尝试训练或拟合模型之前，提前做好数据准备是至关重要的。
- en: We discussed data cleaning in [Chapter 3](01c3daac-ead5-44c1-b17b-d49862f3067d.xhtml)*, 
    A Developer Approach to Data Cleaning,* so you should have a good understanding
    of the importance of and the process of data cleaning and cleansing at this point,
    so we'll focus here on some data preparation efforts more specific to our topic
    of artificial neural network models.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](01c3daac-ead5-44c1-b17b-d49862f3067d.xhtml)*，开发者的数据清理方法*中讨论了数据清理，因此到此为止，你应该对数据清理和净化的过程有一个较好的理解，在这里我们将重点关注一些更具体的与人工神经网络模型相关的数据准备工作。
- en: It is a popular opinion among data scientists that it may be good practice to
    normalize your data before training an artificial neural network on it. Depending
    on your data, not performing any data normalization may lead to unusable results
    or at least to a very difficult training process; most of the time, the algorithm
    will not converge before the number of maximum iterations allowed--in other words,
    it runs out of attempts!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家普遍认为，在训练人工神经网络之前，对数据进行归一化可能是一个好的做法。根据你的数据，如果不进行任何数据归一化，可能会导致不可用的结果，或者至少是非常困难的训练过程；大多数情况下，算法在达到最大迭代次数之前不会收敛——换句话说，它会用尽所有尝试！
- en: Based on industry experience, the artificial neural network may have difficulty
    converging before the maximum number of iterations allowed if the data is not
    normalized.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 根据行业经验，如果数据没有经过归一化，人工神经网络可能会在达到最大迭代次数之前难以收敛。
- en: Although there are numerous and diverse methods to accomplish normalization
    or normalizing of your data, one of the most universally used is the R built-in `scale()`
    function, which can easily accomplish this task.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多方法可以实现数据归一化，但最常用的一种是R语言内置的`scale()`函数，它可以轻松完成这一任务。
- en: The scale function is a generic R function whose default method centers and/or
    scales (normalized) the columns of a numeric matrix for you.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`scale()`函数是一个通用的R语言函数，其默认方法会为你居中和/或缩放（归一化）数值矩阵的列。'
- en: I like the following scale example using simple R code statements because it
    makes it obvious that the scale function works correctly (as expected?) and is
    a lot more effective than attempting to perform *manual* scaling, that is, writing
    the R code statements to scale the values.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢使用简单的R代码语句来演示以下缩放示例，因为这可以明显看出`scale()`函数正确工作（是否如预期？），而且比手动缩放（即编写R代码语句来缩放值）要有效得多。
- en: 'Take a quick look:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 快速看一下：
- en: '[PRE0]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Finally, here is the output, showing the same results produced out of both
    the manual scale approach as well as out of the scale function:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是输出，显示了手动缩放方法和缩放函数所产生的相同结果：
- en: '![](img/6b7c3f65-d1bc-41a9-a137-a6da235050b3.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b7c3f65-d1bc-41a9-a137-a6da235050b3.png)'
- en: Awesome!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！
- en: Data splitting
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据拆分
- en: The next step in our preprocessing is *splitting the data*. We covered data
    *splitting* in detail in [Chapter 8](29276841-ef5a-4a37-bd4b-ff17f364a93a.xhtml),
    *Database Development and Assessment*, so we won't revisit the topic here.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预处理的下一步是*拆分数据*。我们在[第8章](29276841-ef5a-4a37-bd4b-ff17f364a93a.xhtml)，*数据库开发与评估*中详细讨论了数据*拆分*，所以在这里我们不会再重提这个话题。
- en: However, recall that the process of data splitting will have the objective of
    creating both a training subset of data as well as a testing subset of data, from
    the original dataset or source (based on appropriate logic).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请记住，数据拆分的过程旨在从原始数据集或来源中创建训练子集和测试子集（基于适当的逻辑）。
- en: For example, we could accomplish our data split by *randomly* splitting the
    data into a train and a test set, then fit a linear regression model and test
    it on the test dataset (or use the split method we used in the last chapter, to
    create a 70/30 split of our data).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过*随机*将数据拆分为训练集和测试集，接着拟合线性回归模型，并在测试数据集上进行测试（或者使用我们在上一章中使用的拆分方法，创建一个70/30的拆分）。
- en: Model parameters
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型参数
- en: There is no best practice or recommended rule or policy that will tell the data
    scientist how many layers and/or nodes to use (although there are several more
    or less industry--accepted rules) in an artificial neural network model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 没有最佳实践或推荐规则能够告诉数据科学家在人工神经网络模型中应该使用多少层和/或节点（尽管有几条或多或少被业界接受的规则）。
- en: Customarily, if at all necessary, one hidden layer is enough for most or, a
    vast number of, artificial neural network statistical applications (although you
    may have observed that we showed three hidden layers in our graphical image at
    the beginning of this chapter).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果需要的话，一个隐藏层对于大多数或大量的人工神经网络统计应用已经足够（尽管你可能注意到，我们在本章开始时的图形图像中展示了三个隐藏层）。
- en: As far as the number of nodes, the number should frequently be between the input
    layer size and the output layer size, usually 2/3 of the input size.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 就节点数而言，节点数通常应介于输入层大小和输出层大小之间，通常是输入大小的2/3。
- en: Bottom line—when the data scientist is determining the number of layers and
    nodes to have in his or her artificial neural network model, they will test, test,
    and test again (and again and again) to find the best or most optimal solution
    to meet the current requirements, as there is no guarantee that any past experience
    or *rules* will fit your statistical model best.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 归根结底——当数据科学家确定在其人工神经网络模型中应该有多少层和节点时，他们将不断测试、测试、再测试（一次又一次），以找到最佳或最优化的解决方案，以满足当前的需求，因为无法保证任何过去的经验或*规则*能最适合你的统计模型。
- en: Cross-validation
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: '**Cross-validation** is an important topic that we introduced in [Chapter 2](5f479943-b0ab-4276-b173-c571647b1c5c.xhtml)*,
    Declaring the Objectives*, and again we recall that it IS a very important step
    in building any predictive model.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**是我们在[第二章](5f479943-b0ab-4276-b173-c571647b1c5c.xhtml)*中介绍的一个重要话题，目标声明*，我们再次提醒它是构建任何预测模型时的一个非常重要的步骤。'
- en: 'While there are many different kinds of cross-validation methods, the basic
    idea is a data scientist repeating the following process a number of times:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有很多不同种类的交叉验证方法，但基本的思路是数据科学家重复以下过程若干次：
- en: '**Train me, test me, split me**:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练我，测试我，拆分我**：'
- en: Do the train-test split.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行训练-测试拆分。
- en: Fit the model to the train set.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到训练集。
- en: Test the model on the test set.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上测试模型。
- en: Calculate and review the prediction error.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并回顾预测误差。
- en: Repeat (*n* number of times).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复（*n*次）。
- en: By conducting the preceding process a number of times, the data scientist will
    then be able to calculate the average error that is then used to assess how the
    statistical model is performing (performance is another important topic, one which
    we discussed in [Chapter 8](29276841-ef5a-4a37-bd4b-ff17f364a93a.xhtml)*, Database
    Development and Assessment*).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多次执行上述过程，数据科学家将能够计算出平均误差，并用其来评估统计模型的表现（性能是另一个重要话题，我们在[第八章](29276841-ef5a-4a37-bd4b-ff17f364a93a.xhtml)*，数据库开发与评估*中进行了讨论）。
- en: R packages for ANN development
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于ANN开发的R包
- en: So, how can a data scientist create his or her own **artificial neural network**
    (**ANN**)?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，数据科学家如何创建自己的**人工神经网络**（**ANN**）呢？
- en: The R programming language provides (as of writing this) a nice variety of packages
    to create various types of artificial neural networks. These packages currently
    include the following.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: R编程语言（截至本文编写时）提供了多种创建不同类型人工神经网络的包。这些包目前包括以下内容。
- en: ANN
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ANN
- en: This package provides a feed-forward artificial neural network optimized by
    **genetic algorithm** (**GA**).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本包提供了一个由**遗传算法**（**GA**）优化的前馈人工神经网络。
- en: ANN2
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ANN2
- en: This provides the training of general classification and regression neural networks
    using gradient descent.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了使用梯度下降训练一般的分类和回归神经网络。
- en: NNET
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NNET
- en: The package NNET created by Ripley provides methods to use feedforward neural
    networks with a single hidden layer and for multinomial log-linear models. Specifically,
    this chapter of the book will portray the NNET method. Here, we have briefly described
    the method and parameters used.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Ripley创建的NNET包提供了用于单层隐藏层前馈神经网络和多项式对数线性模型的方法。具体来说，本书的这一章节将介绍NNET方法。在这里，我们简要描述了所使用的方法和参数。
- en: 'Note that I am splitting the data in this way: 90% train set and 10% test set
    in a random way for 10 times.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我是这样拆分数据的：以随机方式将90%的数据作为训练集，10%作为测试集，重复10次。
- en: I am also initializing a progress bar using the `plyr` library because I want
    to keep an eye on the status of the process as the fitting of the neural network
    may take a while.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我还使用`plyr`库初始化了一个进度条，因为我希望在神经网络的拟合过程中保持对状态的关注，毕竟这一过程可能会花费一些时间。
- en: Black boxes
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 黑箱
- en: ANNs can be very challenging to comprehend (even for the most advanced or seasoned
    data scientist). Explaining their outcome (or results) is much more demanding
    than explaining the outcome of a simpler model such as a linear model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）非常难以理解（即使是对于最先进或经验丰富的数据科学家）。解释它们的结果（或输出）比解释简单模型（如线性模型）的结果要复杂得多。
- en: Sometimes, data scientists will understand an artificial neural network model
    at only a very high level. Although one can have only this level of understanding
    and still be able to be productive, not fully understanding the interworkings
    of artificial neural network models can result in a situation referred to as a
    **BBU** model or **black box understood model** or method.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数据科学家对人工神经网络模型的理解仅停留在非常高层次。虽然仅有这样的理解水平也能保持生产力，但如果无法完全理解人工神经网络模型的内部工作原理，可能会导致一种情况，即所谓的**BBU**模型或**黑箱理解模型**或方法。
- en: A black box model, method, or system is one that can be viewed in terms of its
    inputs and outputs, without any knowledge of its internal workings.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 黑箱模型、方法或系统是指仅能从其输入和输出角度来看待，而无法了解其内部工作原理的模型、方法或系统。
- en: Consequently, depending on the kind of application you need, you might want
    to keep in mind that you will have to properly explain the results of the model
    (not just run the model and producing a result) before the results can be useful,
    and therefore invest the time in gaining a proper, detailed understanding of the
    model and methods being used.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据你需要的应用类型，你可能需要记住，在模型结果能够有用之前，你必须能够恰当地解释模型的结果（而不仅仅是运行模型并产生结果），因此需要投入时间去深入理解所使用的模型和方法。
- en: A use case
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个使用案例
- en: 'It has been said (most likely not by a data scientist!) that:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 有人说过（很可能不是数据科学家！）：
- en: '*A neural network model cannot learn anything that a reasonably intelligent
    human could not learn given enough time from the same data.*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经网络模型无法学习任何一个合理聪明的人，在相同数据上经过足够时间学习后无法学到的东西。*'
- en: The key term in this statement is enough time. Data scientists (and human beings
    in general) almost never, ever have the luxury of enough time--and time may be
    the difference between your organization's success and the competition.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这一声明中的关键术语是“足够的时间”。数据科学家（以及人类通常）几乎从未拥有足够的时间——时间可能就是决定你所在组织能否超越竞争对手的关键。
- en: 'In defense of using artificial neural networks (rather than a *reasonably intelligent*
    human) are some of the following additional advantages of artificial neural network
    models:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 支持使用人工神经网络（而非*合理聪明*的人类）的理由之一是，人工神经网络模型的一些额外优势如下：
- en: Make discoveries no one has even imagined yet
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现别人甚至从未想象过的东西
- en: Find solutions in much less time than even a team of people
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在比一团队还短的时间内找到解决方案
- en: Produce results at a much lower cost
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以更低的成本产生结果
- en: Produce consistent results with the inputs they've been trained on and should
    generalize well if tweaked properly
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在经过训练的输入下，产生一致的结果，并且如果适当调整，应该能够很好地泛化
- en: NNs never get bored or distracted
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络永远不会感到厌倦或分心。
- en: With all of these advantages, let's consider some practical use case scenarios
    for artificial neural network models.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些优势后，我们来看看人工神经网络模型的一些实际应用场景。
- en: Popular use cases
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行的应用案例
- en: There are many uses for artificial neural networks. Surveying the industry,
    the most established artificial neural network use cases are the following applications.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络有许多应用。通过调查行业，最成熟的人工神经网络应用案例包括以下几种。
- en: Character recognition
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符识别
- en: Neural networks can be used to recognize handwritten characters, converting
    handwriting in real time to control a computer or for **automatic number-plate
    recognition** (**ANPR)** to automatically read vehicle registration plates.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以用于识别手写字符，将手写内容实时转换为控制计算机，或用于**自动车牌识别**（**ANPR**），自动读取车辆注册号牌。
- en: Image compression
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像压缩
- en: Neural networks can receive and process enormous amounts of information at once,
    making them useful in image compression. With the explosion of big data, applying
    ANN to digital images in order to reduce their cost for storage or transmission
    is a growing opportunity.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络能够一次性接收并处理巨量信息，这使它们在图像压缩中具有重要作用。随着大数据的爆炸式增长，应用人工神经网络（ANN）于数字图像，以降低存储或传输的成本，已成为一个日益增长的机会。
- en: Stock market prediction
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 股票市场预测
- en: The real-time movements of the stock market are extremely complicated due to
    the influence of a large number of factors. Many factors weigh in like high, low,
    open and close price, volume, the price of other securities as well as economic
    indicators. As neural networks can scrutinize large amounts of information rapidly,
    they can be used to predict stock prices.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 股票市场的实时波动极为复杂，受到众多因素的影响。许多因素如最高价、最低价、开盘价、收盘价、交易量、其他证券的价格以及经济指标都会对其产生影响。由于神经网络能够快速分析大量信息，它们可以用于预测股价。
- en: Fraud detection
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欺诈检测
- en: In recent years, the development of new technologies has also provided further
    ways in which criminals may commit fraud. Neural networks can learn suspicious
    patterns from samples to detect approximate classes, clusters, or patterns of
    suspicious behaviour and use them later to detect frauds.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，新技术的发展也为犯罪分子实施欺诈提供了更多方式。神经网络可以通过从样本中学习可疑模式，识别近似的类别、聚类或可疑行为模式，并利用这些模式来进行欺诈检测。
- en: Neuroscience
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经科学
- en: Theoretical and computational neuroscience is the study of theoretical analysis
    and the computational modeling of biological neural systems. As neural systems
    attempt to replicate cognitive processes and behaviour, the field is closely related
    to cognitive and behavioural modeling.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 理论与计算神经科学是生物神经系统的理论分析与计算建模的研究领域。由于神经系统试图复制认知过程和行为，因此该领域与认知与行为建模密切相关。
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we defined neural networks and, from a data developer's knowledge
    of databases and data models, grew to understand the purpose and use of neural
    networks and why neural networks are so important to data science. We also looked
    at an R-based ANN and listed some popular use case examples.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们定义了神经网络，并从数据开发者对数据库和数据模型的知识出发，逐步理解了神经网络的目的和用途，以及神经网络为何对数据科学如此重要。我们还介绍了基于R的ANN，并列出了几个流行的应用案例。
- en: In the next chapter, we will introduce the idea of using statistical boosting
    to better understand data in a database.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍使用统计提升方法，更好地理解数据库中的数据。
