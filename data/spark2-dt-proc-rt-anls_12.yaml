- en: Common Recipes for Implementing a Robust Machine Learning System
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施稳健机器学习系统的常见配方
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖：
- en: Spark's basic statistical API to help you build your own algorithms
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的基本统计API，助你构建自己的算法
- en: ML pipelines for real-life machine learning applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实生活中的机器学习应用的ML管道
- en: Normalizing data with Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行数据标准化
- en: Splitting data for training and testing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分割以进行训练和测试
- en: Common operations with the new Dataset API
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用新的Dataset API进行常见操作
- en: Creating and using RDD versus DataFrame versus Dataset from a text file in Spark
    2.0
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中从文本文件创建和使用RDD、DataFrame与Dataset
- en: LabeledPoint data structure for Spark ML
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark ML中的LabeledPoint数据结构
- en: Getting access to Spark cluster in Spark 2.0+
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0+中访问Spark集群
- en: Getting access to Spark cluster pre-Spark 2.0
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0之前访问Spark集群
- en: Getting access to SparkContext vis-a-vis SparkSession object in Spark 2.0
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中通过SparkSession对象访问SparkContext
- en: New model export and PMML markup in Spark 2.0
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0中的新模型导出和PMML标记
- en: Regression model evaluation using Spark 2.0
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行回归模型评估
- en: Binary classification model evaluation ...
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元分类模型评估...
- en: Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In every line of business, ranging from running a small business to creating
    and managing a mission-critical application, there are a number of tasks that
    are common and need to be included as a part of almost every workflow that is
    required during the course of executing the functions. This is true even for building
    robust machine learning systems. In Spark machine learning, some of these tasks
    range from splitting the data for model development (train, test, validate) to
    normalizing input feature vector data to creating ML pipelines via the Spark API.
    We provide a set of recipes in this chapter to enable the reader to think about
    what is actually required to implement an end-to-end machine learning system.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在各行各业中，无论是经营一家小企业还是开发和维护关键任务应用程序，都有一系列常见的任务需要在执行功能过程中几乎每个工作流程中包含。即使在构建强大的机器学习系统时也是如此。在Spark机器学习中，这些任务包括从数据分割以进行模型开发（训练、测试、验证）到标准化输入特征向量数据，再到通过Spark
    API创建ML管道。本章提供了一系列配方，旨在帮助读者思考实施端到端机器学习系统实际需要的内容。
- en: This chapter attempts to demonstrate a number of common tasks which are present
    in any robust Spark machine learning system implementation. To avoid redundant
    references these common tasks in every recipe covered in this book, we have factored
    out such common tasks as short recipes in this chapter, which can be leveraged
    as needed while reading the other chapters. These recipes can either stand alone
    or be included as pipeline subtasks in a larger system. Please note that these
    common recipes are emphasized in the larger context of machine learning algorithms
    in later chapters, while also including them as independent recipes in this chapter
    for completeness.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章试图展示在任何稳健的Spark机器学习系统实现中存在的多种常见任务。为了避免在本书中每个配方中重复提及这些常见任务，我们将这些常见任务作为本章中的简短配方提取出来，读者在阅读其他章节时可根据需要加以利用。这些配方既可以独立使用，也可以作为大型系统中的管道子任务。请注意，这些常见配方在后续章节中关于机器学习算法的更大背景下得到强调，同时为了完整性，本章也包含了它们作为独立配方。
- en: Spark's basic statistical API to help you build your own algorithms
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的基本统计API，助你构建自己的算法
- en: In this recipe, we cover Spark's multivariate statistical summary (that is,
    *Statistics.colStats*) such as correlation, stratified sampling, hypothesis testing,
    random data generation, kernel density estimators, and much more, which can be
    applied to extremely large datasets while taking advantage of both parallelism
    and resiliency via RDDs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们涵盖了Spark的多变量统计摘要（即*Statistics.colStats*），如相关性、分层抽样、假设检验、随机数据生成、核密度估计器等，这些可以在处理极大数据集时利用RDD的并行性和弹性。
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Spark会话导入必要的包以访问集群，并使用`log4j.Logger`减少Spark产生的输出量：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过构建器模式初始化 Spark 会话并指定配置，从而为 Spark 集群提供入口点：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s retrieve the Spark session underlying the SparkContext to use when generating
    RDDs:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检索底层 SparkContext 的 Spark 会话，以便在生成 RDD 时使用：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we create a RDD with the handcrafted data to illustrate usage of summary
    statistics:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用手工数据创建一个 RDD，以说明摘要统计的使用：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We use Spark''s statistics objects by invoking the method `colStats()` and
    passing the RDD as an argument:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过调用 `colStats()` 方法并传递 RDD 作为参数来使用 Spark 的统计对象：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `colStats()` method will return a `MultivariateStatisticalSummary`, which
    contains the computed summary statistics:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`colStats()` 方法将返回一个 `MultivariateStatisticalSummary`，其中包含计算出的摘要统计信息：'
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止 Spark 会话来关闭程序：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How it works...
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We created an RDD from dense vector data followed by the generation of summary
    statistics on it using the statistics object. Once the `colStats()` method returned,
    we retrieved summary statistics such as the mean, variance, minimum, maximum,
    and so on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个从密集向量数据生成的 RDD，然后使用统计对象在其上生成摘要统计信息。一旦调用 `colStats()` 方法返回，我们便获取了诸如均值、方差、最小值、最大值等摘要统计信息。
- en: There's more...
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: It cannot be emphasized enough how efficient the statistical API is on large
    datasets. These APIs will provide you with basic elements to implement any statistical
    learning algorithm from scratch. Based on our research and experience with half
    versus full matrix factorization, we encourage you to first read the source code
    and make sure that there isn't an equivalent functionality already implemented
    in Spark before implementing your own.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据集上，统计 API 的高效性怎么强调都不为过。这些 API 将为您提供实现任何统计学习算法的基本元素。基于我们的研究和经验，我们鼓励您在实现自己的算法之前，先阅读源代码，确保
    Spark 中没有实现相应功能。
- en: 'While we only demonstrate a basic statistics summary here, Spark comes equipped
    out of the box with:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在此仅演示了基本的统计摘要，但 Spark 自带了以下功能：
- en: 'Correlation: `Statistics.corr(seriesX, seriesY, "type of correlation")`:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性：`Statistics.corr(seriesX, seriesY, "相关性类型")`：
- en: Pearson (default)
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 皮尔逊（默认）
- en: Spearman
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯皮尔曼
- en: 'Stratified sampling - RDD API:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层抽样 - RDD API：
- en: With a replacement RDD
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有替换的 RDD
- en: Without a replacement - requires an additional pass
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无替换 - 需要额外遍历
- en: 'Hypothesis testing:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设检验：
- en: Vector - `Statistics.chiSqTest( vector )`
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量 - `Statistics.chiSqTest( 向量 )`
- en: Matrix - `Statistics.chiSqTest( dense matrix )`
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵 - `Statistics.chiSqTest( 密集矩阵 )`
- en: '**Kolmogorov-Smirnov** (**KS**) test for equality - one or two-sided:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**柯尔莫哥洛夫-斯米尔诺夫**（**KS**）等同性检验 - 单侧或双侧：'
- en: '`Statistics.kolmogorovSmirnovTest(RDD, "norm", 0, 1)`'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Statistics.kolmogorovSmirnovTest(RDD, "norm", 0, 1)`'
- en: 'Random data generator - `normalRDD()`:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机数据生成器 - `normalRDD()`：
- en: Normal - can specify a parameter
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正态 - 可以指定参数
- en: Lots of option plus `map()`s to generate any distribution
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 众多选项加上 `map()` 以生成任何分布
- en: Kernel density estimator - `KernelDensity().estimate( data )`
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核密度估计器 - `KernelDensity().estimate( data )`
- en: A quick reference to the *Goodness of fit* concept in statistics can be found
    at [https://en.wikipedia.org/wiki/Goodness_of_fit](https://en.wikipedia.org/wiki/Goodness_of_fit)
    link.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学中*拟合优度*概念的快速参考可在[https://en.wikipedia.org/wiki/Goodness_of_fit](https://en.wikipedia.org/wiki/Goodness_of_fit)链接找到。
- en: See also
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for more multivariate statistical summary:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 更多多元统计摘要的文档：
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary)'
- en: ML pipelines for real-life machine learning applications
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML 管道，适用于实际机器学习应用
- en: This is the first of two recipes which cover the ML pipeline in Spark 2.0\.
    For a more advanced treatment of ML pipelines with additional details such as
    API calls and parameter extraction, see later chapters in this book.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是涵盖 Spark 2.0 中 ML 管道的两个配方中的第一个。有关 ML 管道的更高级处理，包括 API 调用和参数提取等详细信息，请参阅本书后面的章节。
- en: In this recipe, we attempt to have a single pipeline that can tokenize text,
    use HashingTF (an old trick) to map term frequencies, run a regression to fit
    a model, and then predict which group a new term belongs to (for example, news
    filtering, gesture classification, and so on).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们尝试构建一个单一管道，该管道能够对文本进行分词，使用HashingTF（一种老技巧）映射词频，运行回归以拟合模型，并预测新词属于哪个组（例如，新闻过滤、手势分类等）。
- en: How to do it...
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含了必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Spark会话导入必要的包以访问集群，并使用`log4j.Logger`来减少Spark产生的输出量：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this section, we investigated constructing a simple machine learning pipeline
    with Spark. We began with creating a DataFrame comprised of two groups of text
    documents and then proceeded to set up a pipeline.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们探讨了使用Spark构建简单机器学习管道的过程。我们首先创建了一个包含两组文本文档的DataFrame，随后设置了管道。
- en: First, we created a tokenizer to parse text documents into terms followed by
    the creation of the HashingTF to convert the terms into features. Then, we created
    a logistic regression object to predict which group a new text document belongs
    to.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建了一个分词器来将文本文档解析为词项，随后创建了HashingTF来将这些词项转换为特征。接着，我们创建了一个逻辑回归对象来预测新文本文档属于哪个组。
- en: Second, we constructed the pipeline by passing an array of arguments to it,
    specifying three stages of execution. You will notice each subsequent stage provides
    the result as a specified column while using the previous stage's output column
    as the input.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们通过传递一个参数数组来构建管道，指定了三个执行阶段。您会注意到，每个后续阶段都使用前一阶段的输出列作为输入，并提供一个指定的结果列。
- en: Finally, we trained the model by invoking `fit()` on the pipeline object and
    defining a set of test data for verification. Next, we transformed the test set
    with the model, producing which of the defined two groups the text documents in
    the test set belong to.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过在管道对象上调用`fit()`并定义一组测试数据进行验证来训练模型。接下来，我们使用模型转换测试集，确定测试集中的文本文档属于定义的两个组中的哪一个。
- en: There's more...
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The pipeline in Spark ML was inspired by scikit-learn in Python, which is referenced
    here for completeness:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML中的管道灵感来源于Python中的scikit-learn，此处为完整性引用：
- en: '[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)'
- en: ML pipelines make it easy to combine multiple algorithms used to implement a
    production task in Spark. It would be unusual to see a use case in a real-life
    situation that is made of a single algorithm. Often a number of cooperating ML
    algorithms work together to achieve a complex use case. For example, in LDA-based
    systems (for example, news briefings) or human emotion detection, there are a
    number of steps before and after the core system to be implemented as a single
    pipe to produce any meaningful and production-worthy system. See the following
    link for a real-life use case requiring ...
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习管道使得在Spark中结合多个用于实现生产任务的算法变得容易。在实际应用中，通常不会只使用单一算法。往往需要多个协作的机器学习算法共同工作以实现复杂的用例。例如，在基于LDA的系统（如新闻简报）或人类情感检测中，在核心系统前后都有多个步骤需要实现为一个单一管道，以产生有意义且适用于生产的系统。请参阅以下链接了解一个实际应用案例，该案例需要...
- en: See also
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Documentation for more multivariate statistical summary:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 更多多元统计摘要的文档：
- en: Pipeline docs are available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline)找到
- en: 'Pipeline model that is useful when we load and save the `.load()`, `.save()
    methods`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们加载和保存`.load()`, `.save()`方法时，有用的流水线模型：[此处](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel)
- en: Pipeline stage information is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流水线阶段信息可在[此处](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage)找到。
- en: HashingTF, a nice old trick to map a sequence to their term frequency in text
    analytics is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HashingTF，文本分析中将序列映射到其词频的一个不错老技巧，可在[此处](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF)找到。
- en: Normalizing data with Spark
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行数据归一化
- en: In this recipe, we demonstrate normalizing (scaling) the data prior to importing
    the data into an ML algorithm. There are a good number of ML algorithms such as
    **Support Vector Machine** (**SVM**) that work better with scaled input vectors
    rather than with the raw values.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们展示了在将数据导入ML算法之前进行归一化（缩放）。有许多ML算法（如**支持向量机**（**SVM**））在缩放输入向量上比在原始值上工作得更好。
- en: How to do it...
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Go to the UCI Machine Learning Repository and download the [http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data) file.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往UCI机器学习库并下载[此文件](http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)。
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包，以便Spark会话能够访问集群并使用`log4j.Logger`减少Spark产生的输出量：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define a method to parse wine data into a tuple:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法，将葡萄酒数据解析为元组：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化Spark会话，指定配置，从而为Spark集群提供入口点：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Import `spark.implicits`, therefore adding in behavior with only an `import`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`spark.implicits`，从而仅通过`import`添加行为：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s load the wine data into memory, taking only the first four columns and
    converting the latter three into a new feature vector:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将葡萄酒数据加载到内存中，只取前四列，并将后三列转换为一个新的特征向量：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we generate a DataFrame with two columns:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们生成一个包含两列的DataFrame：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we will print out the DataFrame schema and display data contained within
    the DataFrame:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将打印出DataFrame模式并展示DataFrame中包含的数据：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/da1f5304-1a7d-40c1-9695-cd40675ce2df.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da1f5304-1a7d-40c1-9695-cd40675ce2df.png)'
- en: 'Finally, we generate the scaling model and transform the feature into a common
    range between a negative and positive one displaying the results:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们生成缩放模型并将特征转换为介于负一和正一之间的公共范围，展示结果：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/b0cc65be-4e1c-4114-bab7-19ce12c42628.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0cc65be-4e1c-4114-bab7-19ce12c42628.png)'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark会话来关闭程序：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How it works...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this example, we explored feature scaling which is a critical step in most
    machine learning algorithms such as **classifiers**. We started out by loading
    the wine data files, extracted an identifier, and used the next three columns
    to create a feature vector.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们探讨了特征缩放，这是大多数机器学习算法（如**分类器**）中的关键步骤。我们首先加载葡萄酒数据文件，提取标识符，并使用接下来的三列创建特征向量。
- en: Then, we created a `MinMaxScaler` object, configuring a minimum and maximum
    range to scale our values into. We invoked the scaling model by executing the
    `fit()` method on the scaler class, and then we used the model to scale the values
    in our DataFrame.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们创建了一个`MinMaxScaler`对象，配置了一个最小和最大范围，以便将我们的值进行缩放。通过在缩放器类上调用`fit()`方法来执行缩放模型，然后使用该模型对DataFrame中的值进行缩放。
- en: Finally, we displayed the resulting DataFrame and we noticed feature vector
    values ranges are between negative 1 and positive 1.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们展示了结果的DataFrame，并注意到特征向量值的范围在负1到正1之间。
- en: There's more...
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The roots of normalizing and scaling can be better understood by examining
    the concept of **unit vectors** in introductory linear algebra. Please see the
    following links for some common references for unit vectors:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查线性代数入门中的**单位向量**概念，可以更好地理解归一化和缩放的根源。以下是一些关于单位向量的常见参考链接：
- en: You can refer to unit vectors at [https://en.wikipedia.org/wiki/Unit_vector](https://en.wikipedia.org/wiki/Unit_vector)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在[https://en.wikipedia.org/wiki/Unit_vector](https://en.wikipedia.org/wiki/Unit_vector)参考单位向量
- en: For scalar, you can refer to [https://en.wikipedia.org/wiki/Scalar_(mathematics)](https://en.wikipedia.org/wiki/Scalar_(mathematics))
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于标量，你可以参考[https://en.wikipedia.org/wiki/Scalar_(mathematics)](https://en.wikipedia.org/wiki/Scalar_(mathematics))
- en: In the case of input sensitive algorithms, such as SVM, it is recommended that
    the algorithm is trained on scaled values (for example, range from 0 to 1) of
    the features rather than the absolute values as represented by the original vector.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入敏感的算法，如SVM，建议在特征的缩放值（例如，范围从0到1）上训练算法，而不是原始向量表示的绝对值。
- en: See also
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `MinMaxScaler` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`MinMaxScaler`的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler)获取。'
- en: We want to emphasize that `MinMaxScaler` is an extensive API that extends the
    `Estimator` (a concept from the ML pipeline) and when used correctly can lead
    to achieving coding efficiency and high accuracy results.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想强调`MinMaxScaler`是一个广泛的API，它扩展了`Estimator`（来自ML管道的概念），正确使用时可以实现编码效率和高精度结果。
- en: Splitting data for training and testing
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割数据用于训练和测试
- en: In this recipe, you will learn to use Spark's API to split your available input
    data into different datasets that can be used for training and validation phases.
    It is common to use an 80/20 split, but other variations of splitting the data
    can be considered as well based on your preference.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，你将学习使用Spark的API将可用的输入数据分割成不同的数据集，这些数据集可用于训练和验证阶段。通常使用80/20的分割比例，但也可以根据你的偏好考虑其他数据分割方式。
- en: How to do it...
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Go to the UCI Machine Learning Repository and download the [http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip](http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip) file.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往UCI机器学习库并下载[http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip](http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip)文件。
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或你选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Spark会话导入必要的包以访问集群，并导入`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How it works...
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We began by loading the data file `newsCorpora.csv` and then by way of the `randomSplit()`
    method attached to the dataset object, we split the dataset.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载`newsCorpora.csv`数据文件，然后通过数据集对象上的`randomSplit()`方法，我们分割了数据集。
- en: There's more...
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: To validate the result, we must set up a Delphi technique in which the test
    data is absolutely unknown to the model. See Kaggle competitions for details at [https://www.kaggle.com/competitions](https://www.kaggle.com/competitions).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证结果，我们必须设置一个德尔菲技术，其中测试数据对模型来说是完全未知的。详情请参见Kaggle竞赛，网址为[https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)。
- en: 'Three types of datasets are needed for a robust ML system:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一个健壮的机器学习系统需要三种类型的数据集。
- en: '**Training dataset**: This is used to fit a model to sample'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据集**：用于拟合模型以进行采样。'
- en: '**Validation dataset**: This is used to estimate the delta or prediction error
    for the fitted model (trained by training set)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证数据集**：用于估计拟合模型（由训练集训练）的预测误差或增量。'
- en: '**Test dataset**: This is used to assess the model generalization error once
    a final model is selected'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试数据集**：一旦选定最终模型，用于评估模型的泛化误差。'
- en: See also
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: Documentation for `randomSplit()` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`randomSplit()`的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D)找到。
- en: The `randomSplit()` is a method call within an RDD. While the number of RDD
    method calls can be overwhelming, mastering this Spark concept and API is a must.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`randomSplit()`是在RDD内部调用的方法。尽管RDD方法调用的数量可能令人不知所措，但掌握这个Spark概念和API是必须的。'
- en: 'API signature is as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: API签名如下：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Randomly splits this RDD with the provided weights.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提供的权重随机分割此RDD。
- en: Common operations with the new Dataset API
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用新Dataset API的常见操作
- en: In this recipe, we cover the Dataset API, which is the way forward for data
    wrangling in Spark 2.0 and beyond. In this chapter ,we cover some of the common,
    repetitive operations that are required to work with these new API sets. Additionally,
    we demonstrate the query plan generated by the Spark SQL Catalyst optimizer.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们介绍了Dataset API，这是Spark 2.0及以后版本进行数据整理的前进方式。本章中，我们涵盖了一些处理这些新API集合所需的常见、重复操作。此外，我们还展示了由Spark
    SQL Catalyst优化器生成的查询计划。
- en: How to do it...
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'We will use a JSON data file named `cars.json`, which has been created for
    this example:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用一个名为`cars.json`的JSON数据文件，该文件是为这个示例创建的：
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Set up the package location where the program will reside:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置。
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Import the necessary packages for the Spark session to get access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Spark会话导入必要的包以访问集群，并导入`log4j.Logger`以减少Spark产生的输出量。
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define a Scala `case class` to model the data:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala `case class`来建模数据：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出。
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化Spark会话并指定配置，从而为Spark集群提供入口点。
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Import `spark.implicits`, therefore adding in behavior with only an `import`:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`spark.implicits`，从而通过仅一个`导入`操作添加行为。
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s create a dataset from a Scala list and print out the results:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从一个Scala列表创建数据集并打印结果：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/fcc65713-e335-4952-b943-5c7b600b1f5f.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fcc65713-e335-4952-b943-5c7b600b1f5f.png)'
- en: 'Next, we will load a CSV into memory and transform it into a dataset of type
    `Team`:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载一个CSV文件到内存中，并将其转换为类型为`Team`的数据集。
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/90651fad-883b-4167-84fa-d17beec4469e.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90651fad-883b-4167-84fa-d17beec4469e.png)'
- en: 'Now we demonstrate a transversal of the teams dataset by use of the `map` function,
    yielding a new dataset of city names:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们通过使用`map`函数遍历teams数据集，生成一个新的城市名称数据集。
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](img/3e7b1c1e-6b07-4c24-a5b9-6546ec45201d.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e7b1c1e-6b07-4c24-a5b9-6546ec45201d.png)'
- en: 'Display the execution plan for retrieving city names:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示检索城市名称的执行计划：
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, we save the `teams` dataset to a JSON file:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将`teams`数据集保存到JSON文件中。
- en: '[PRE38]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark会话来关闭程序。
- en: '[PRE39]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: How it works...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: First, we created a dataset from a Scala list and displayed the output to validate
    the creation of the dataset as expected. Second, we loaded a **comma-separated
    value** (**CSV**) file into memory, transforming it into a dataset of type `Team`.
    Third, we executed the `map()` function over our dataset to build a list of team
    city names and printed out the execution plan used to generate the dataset. Finally,
    we persisted the `teams` dataset we previously loaded into a JSON formatted file
    for future use.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从 Scala 列表创建了一个数据集，并显示输出以验证数据集的创建是否符合预期。其次，我们将一个 **逗号分隔值**（**CSV**）文件加载到内存中，将其转换为类型为
    `Team` 的数据集。第三，我们对数据集执行了 `map()` 函数，以构建一个团队城市名称列表，并打印出用于生成数据集的执行计划。最后，我们将之前加载的
    `teams` 数据集持久化为 JSON 格式的文件，以备将来使用。
- en: There's more...
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Please take a note of some interesting points on datasets:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意关于数据集的一些有趣点：
- en: Datasets use *lazy* evaluation
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集采用 *惰性* 评估
- en: Datasets take advantage of the Spark SQL Catalyst optimizer
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集利用了 Spark SQL Catalyst 优化器
- en: Datasets take advantage of the tungsten off-heap memory management
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集利用了 tungsten 的堆外内存管理
- en: There are plenty of systems that will remain pre-Spark 2.0 for the next 2 year
    so you must still learn and master RDDs and DataFrame for practical reasons.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来两年内，许多系统仍将停留在 Spark 2.0 之前，因此出于实际考虑，您必须学习并掌握 RDD 和 DataFrame。
- en: See also
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Documentation for Dataset is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Dataset 的文档可在 [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
    获取。
- en: Creating and using RDD versus DataFrame versus Dataset from a text file in Spark
    2.0
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0 中从文本文件创建和使用 RDD 与 DataFrame 与 Dataset
- en: 'In this recipe, we explore the subtle differences in creating RDD, DataFrame,
    and Dataset from a text file and their relationship to each other via a short
    sample code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们通过一段简短的代码探索了从文本文件创建 RDD、DataFrame 和 Dataset 的细微差别以及它们之间的关系：
- en: '[PRE40]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Assume `spark` is the session name
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 `spark` 是会话名称
- en: How to do it...
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含了必要的 JAR 文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE41]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 Spark 会话导入必要的包以访问集群，并导入 `log4j.Logger` 以减少 Spark 产生的输出量：
- en: '[PRE42]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We also define a `case class` to host the data used:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还定义了一个 `case class` 来承载所使用的数据：
- en: '[PRE43]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为 `ERROR` 以减少 Spark 的日志输出：
- en: '[PRE44]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Initialize a Spark session specifying configurations with the builder pattern,
    thus making ...
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过构建器模式初始化 Spark 会话并指定配置，从而...
- en: How it works...
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'We create an RDD, DataFrame, and Dataset object using a similar method from
    the same text file and confirm the type using the `getClass` method:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相同文本文件的类似方法创建了 RDD、DataFrame 和 Dataset 对象，并使用 `getClass` 方法确认了类型：
- en: '[PRE45]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Please note that they are very similar and sometimes confusing. Spark 2.0 has
    transformed DataFrame into an alias for `Dataset[Row]`, making it truly a dataset.
    We showed the preceding methods to let the user pick an example to create their
    own datatype flavor.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它们非常相似，有时会令人困惑。Spark 2.0 已将 DataFrame 转变为 `Dataset[Row]` 的别名，使其真正成为一个数据集。我们展示了前面的方法，以便用户选择示例来创建自己的数据类型风格。
- en: There's more...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Documentation for datatypes is available at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型的文档可在 [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)
    获取。
- en: If you are unsure as to what kind of data structure you have at hand (sometimes
    the difference is not obvious), use the `getClass` method to verify.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定手头有什么样的数据结构（有时差异并不明显），请使用 `getClass` 方法进行验证。
- en: Spark 2.0 has transformed DataFrame into an alias for `Dataset[Row]`. While
    RDD and Dataram remain fully viable for near future, it is best to learn and code
    new projects using the dataset.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0 已将 DataFrame 转变为 `Dataset[Row]` 的别名。虽然 RDD 和 Dataram 在不久的将来仍然完全可行，但最好学习和使用数据集编写新项目。
- en: See also
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'Documentation for RDD and Dataset is available at the following websites:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 和 Dataset 的文档可在以下网站获取：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)'
- en: LabeledPoint data structure for Spark ML
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark ML中的LabeledPoint数据结构
- en: '**LabeledPoint** is a data structure that has been around since the early days
    for packaging a feature vector along with a label so it can be used in unsupervised
    learning algorithms. We demonstrate a short recipe that uses LabeledPoint, the
    **Seq** data structure, and DataFrame to run a logistic regression for binary
    classification of the data.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**LabeledPoint**是一种自早期以来就存在的数据结构，用于打包特征向量及其标签，以便用于无监督学习算法。我们展示了一个简短的示例，使用LabeledPoint、**Seq**数据结构和DataFrame来运行二元分类数据的逻辑回归。'
- en: How to do it...
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE46]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以使SparkContext能够访问集群：
- en: '[PRE47]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Create Spark''s configuration and SparkContext so we can have access to the
    cluster:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark配置和SparkContext，以便我们能够访问集群：
- en: '[PRE48]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We create the LabeledPoint, using the `SparseVector` and `DenseVector`. In
    the following code blocks, the first four LabeledPoints are created by the `DenseVector`,
    the last two LabeledPoints are created by the `SparseVector`:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建LabeledPoint，使用`SparseVector`和`DenseVector`。在以下代码块中，前四个LabeledPoint通过`DenseVector`创建，最后两个LabeledPoint通过`SparseVector`创建：
- en: '[PRE49]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The DataFrame objects are created from the preceding LabeledPoint.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame对象从前述LabeledPoint创建。
- en: We verify the raw data count and process data count.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们验证原始数据计数和处理数据计数。
- en: 'You can operate a `show()` function call to the DataFrame created:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以对创建的DataFrame调用`show()`函数：
- en: '[PRE50]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: You will see the following in the console:![](img/9cb90b58-293a-46c8-b6db-8461984c9b9f.png)
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将在控制台看到以下内容：![](img/9cb90b58-293a-46c8-b6db-8461984c9b9f.png)
- en: 'We create a simple LogisticRegression model from the data structure we just
    created:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们基于刚创建的数据结构创建了一个简单的LogisticRegression模型：
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In the console, it will show the following `model` parameters:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中，将显示以下`model`参数：
- en: '[PRE52]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后通过停止Spark会话来关闭程序：
- en: '[PRE53]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: How it works...
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We used a LabeledPoint data structure to model features and drive training of
    a logistics regression model. We began by defining a group of LabeledPoints, which
    are used to create a DataFrame for further processing. Then, we created a logistic
    regression object and passed LabeledPoint DataFrame as an argument to it so we
    could train our model. Spark ML APIs are designed to work well with the LabeledPoint
    format and require minimal intervention.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用LabeledPoint数据结构来建模特征并驱动逻辑回归模型的训练。首先定义了一组LabeledPoints，用于创建DataFrame以进行进一步处理。接着，我们创建了一个逻辑回归对象，并将LabeledPoint
    DataFrame作为参数传递给它，以便训练我们的模型。Spark ML API设计得与LabeledPoint格式兼容良好，且几乎无需干预。
- en: There's more...
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'A LabeledPoint is a popular structure used to package data as a `Vector` +
    a `Label` which can be purposed for supervised machine learning algorithms. A
    typical layout of the LabeledPoint is given here:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: LabeledPoint是一种常用结构，用于将数据打包为`Vector` + `Label`，适用于监督机器学习算法。LabeledPoint的典型布局如下所示：
- en: '[PRE54]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Please note that not only dense but also sparse vectors can be used with LabeledPoint,
    which will make a huge difference in efficiency especially if you have a large
    and sparse dataset housed in the driver during testing and development.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，不仅密集向量，稀疏向量也可以与LabeledPoint配合使用，这在测试和开发期间，如果驱动程序中驻留了大型且稀疏的数据集，将会在效率上产生巨大差异。
- en: See also
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: LabeledPoint API documentation is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint)
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LabeledPoint API文档可在此处查阅：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint)
- en: DenseVector API documentation is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseVector API 文档可在 [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector)
    获取
- en: SparseVector API documentation is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparseVector API 文档可在 [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector)
    获取
- en: Getting access to Spark cluster in Spark 2.0
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取 Spark 2.0 中 Spark 集群的访问权限
- en: In this recipe, we demonstrate how to get access to a Spark cluster using a
    single point access named `SparkSession`. Spark 2.0 abstracts multiple contexts
    (such as SQLContext, HiveContext) into a single entry point, `SparkSession`, which
    allows you to get access to all Spark subsystems in a unified way.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们演示了如何通过名为 `SparkSession` 的单点访问来获取 Spark 集群的访问权限。Spark 2.0 将多个上下文（如 SQLContext、HiveContext）抽象为一个入口点
    `SparkSession`，允许您以统一的方式访问所有 Spark 子系统。
- en: How to do it...
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作方法...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在包的位置：
- en: '[PRE55]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Import the necessary packages for SparkContext to get access to the cluster.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以通过 SparkContext 访问集群。
- en: In Spark 2.x, `SparkSession` is more commonly used instead.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 2.x 中，`SparkSession` 更常用。
- en: '[PRE56]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Create Spark''s configuration and `SparkSession` so we can have access to the
    cluster:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 Spark 配置和 `SparkSession`，以便我们能够访问集群：
- en: '[PRE57]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The preceding code utilizes the `master()` function to set the cluster type
    ...
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码利用 `master()` 函数来设置集群类型...
- en: How it works...
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this example, we show how to connect to a Spark cluster using local and remote
    options for an application. First, we create a `SparkSession` object which will
    grant us access to a Spark cluster by specifying whether the cluster is local
    or remote using the `master()` function. You can also specify the master location
    by passing a JVM argument when starting your client program. In addition, you
    can configure an application name and a working data directory. Next, you invoked
    the `getOrCreate()` method to create a new `SparkSession` or hand you a reference
    to an already existing session. Finally, we execute a small sample program to
    prove our `SparkSession` object creation is valid.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们展示了如何使用本地和远程选项连接到 Spark 集群。首先，我们创建一个 `SparkSession` 对象，通过 `master()`
    函数指定集群是本地还是远程，从而授予我们访问 Spark 集群的权限。您还可以通过在启动客户端程序时传递 JVM 参数来指定主节点位置。此外，您可以配置应用程序名称和工作数据目录。接下来，调用
    `getOrCreate()` 方法创建新的 `SparkSession` 或为您提供现有会话的引用。最后，我们执行一个小型示例程序以验证 `SparkSession`
    对象创建的有效性。
- en: There's more...
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: A Spark session has numerous parameters and APIs that can be set and exercised,
    but it is worth consulting the Spark documentation since some of the methods/parameters
    are marked with the status Experimental or left blank - for non-experimental statuses
    (15 minimum as of our last examination).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 会话拥有众多可设置和使用的参数及 API，但建议查阅 Spark 文档，因为其中一些方法/参数被标记为实验性或留空 - 对于非实验性状态（我们上次检查时至少有
    15 个）。
- en: Another change to be aware of is to use `spark.sql.warehouse.dir` for the location
    of the tables. Spark 2.0 uses `spark.sql.warehouse.dir` to set warehouse locations
    to store tables rather than `hive.metastore.warehouse.dir`. The default value
    for `spark.sql.warehouse.dir` is `System.getProperty("user.dir")`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的变化是使用 `spark.sql.warehouse.dir` 来指定表的位置。Spark 2.0 使用 `spark.sql.warehouse.dir`
    设置仓库位置以存储表，而不是 `hive.metastore.warehouse.dir`。`spark.sql.warehouse.dir` 的默认值为
    `System.getProperty("user.dir")`。
- en: Also see `spark-defaults.conf` for more details.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详情请参阅 `spark-defaults.conf`。
- en: 'Also noteworthy are the following:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 同样值得注意的是以下几点：
- en: Some of our favorite and interesting APIs from the Spark ...
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们最喜欢的 Spark 中一些有趣且有用的 API...
- en: See also
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `SparkSession` API documents is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession` API文档可在[此处](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)获取。'
- en: Getting access to Spark cluster pre-Spark 2.0
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0之前访问Spark集群
- en: This is a *pre-Spark 2.0 recipe*, but it will be helpful for developers who
    want to quickly compare and contrast the cluster access for porting pre-Spark
    2.0 programs to Spark 2.0's new paradigm.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一份*Spark 2.0之前的指南*，但对于希望快速比较和对比集群访问方式，以便将Spark 2.0之前程序迁移至Spark 2.0新范式的开发者来说，将大有裨益。
- en: How to do it...
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作方法...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中新建项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在包的位置：
- en: '[PRE58]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为SparkContext导入必要包以访问集群：
- en: '[PRE59]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Create Spark''s configuration and SparkContext so we can have access to the
    cluster:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark配置及SparkContext以便访问集群：
- en: '[PRE60]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The preceding code utilizes the `setMaster()` function to set the cluster master
    location. As you can see, we are running the code in `local` mode.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码利用`setMaster()`函数设置集群主节点位置。如您所见，我们正运行于`本地`模式。
- en: The `-D` option value will be overridden by the cluster master parameter set
    in the code if both exist).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 若两者同时存在，代码中集群主参数设置将覆盖`-D`选项值。
- en: 'The following are the three sample ways to connect to the cluster in different
    modes:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是三种不同模式下连接集群的示例方法：
- en: 'Running in local mode:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行于本地模式：
- en: '[PRE61]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Running in cluster mode:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行于集群模式：
- en: '[PRE62]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Passing the master value in:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传递主节点值：
- en: '[PRE63]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '![](img/51cd6bc9-0345-4cd3-b26b-5ed79aa77840.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51cd6bc9-0345-4cd3-b26b-5ed79aa77840.png)'
- en: 'We use the preceding SparkContext to read a CSV file in and parse the CSV file
    into Spark using the following code:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用上述SparkContext读取CSV文件，并将其解析为Spark中的数据，代码如下：
- en: '[PRE64]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We take the sample result and print them in the console:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将示例结果打印至控制台：
- en: '[PRE65]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: And you will see the following in the console:![](img/803b10ae-f8c7-4b57-9e86-2b96c2dd6712.png)
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台将显示如下内容：![](img/803b10ae-f8c7-4b57-9e86-2b96c2dd6712.png)
- en: 'We then close the program by stopping the SparkContext:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，我们通过停止SparkContext来关闭程序：
- en: '[PRE66]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: How it works...
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this example, we show how to connect to a Spark cluster using the local and
    remote modes prior to Spark 2.0\. First, we create a `SparkConf` object and configure
    all the required parameters. We will specify the master location, application
    name, and working data directory. Next, we create a SparkContext passing the `SparkConf`
    as an argument to access a Spark cluster. Also, you can specify the master location
    my passing a JVM argument when starting your client program. Finally, we execute
    a small sample program to prove our SparkContext is functioning correctly.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 本例展示如何在Spark 2.0之前通过本地和远程模式连接至Spark集群。首先，我们创建一个`SparkConf`对象并配置所有必需参数。我们将指定主节点位置、应用名称及工作数据目录。接着，我们创建SparkContext，将`SparkConf`作为参数传递以访问Spark集群。此外，启动客户端程序时，您可通过传递JVM参数指定主节点位置。最后，我们执行一个小型示例程序以验证SparkContext运行正常。
- en: There's more...
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Prior to Spark 2.0, getting access to a Spark cluster was done via **SparkContext**.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0之前，访问Spark集群通过**SparkContext**实现。
- en: The access to the subsystems such as SQL was per-specific names context (for
    example, SQLContext**)**.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 对子系统（如SQL）的访问需通过特定名称上下文（例如，SQLContext**）。
- en: Spark 2.0 changed how we gain access to a cluster by creating a single unified
    access point (namely, `SparkSession`).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0通过创建单一统一访问点（即`SparkSession`）改变了我们访问集群的方式。
- en: See also
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关内容
- en: Documentation for SparkContext is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: SparkContext文档可在此[查阅](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)。
- en: Getting access to SparkContext vis-a-vis SparkSession object in Spark 2.0
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中通过SparkSession对象访问SparkContext
- en: In this recipe, we demonstrate how to get hold of SparkContext using a SparkSession
    object in Spark 2.0\. This recipe will demonstrate the creation, usage, and back
    and forth conversion of RDD to Dataset. The reason this is important is that even
    though we prefer Dataset going forward, we must still be able to use and augment
    the legacy (pre-Spark 2.0) code mostly utilizing RDD.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们展示了如何在Spark 2.0中通过SparkSession对象获取SparkContext。本教程将演示RDD到Dataset的创建、使用以及来回转换。这样做的重要性在于，尽管我们倾向于使用Dataset，但我们仍需能够使用和增强主要利用RDD的遗留（预Spark
    2.0）代码。
- en: How to do it...
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在包的位置：
- en: '[PRE67]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Spark会话导入必要的包以访问集群，并使用`log4j.Logger`减少Spark产生的输出量：
- en: '[PRE68]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE69]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过构建器模式初始化Spark会话并指定配置，从而为Spark集群提供入口点：
- en: '[PRE70]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: How it works...
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We created RDD using the SparkContext; this was widely used in Spark 1.x. We
    also demonstrated a way to create Dataset in Spark 2.0 using the Session object.
    The conversion back and forth is necessary to deal with pre-Spark 2.0 code in
    production today.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SparkContext创建了RDD；这在Spark 1.x中广泛使用。我们还展示了在Spark 2.0中使用Session对象创建Dataset的方法。为了处理生产环境中的预Spark
    2.0代码，这种来回转换是必要的。
- en: The technical message from this recipe is that while DataSet is the preferred
    method of data wrangling going forward, we can always use the API to go back and
    forth to RDD and vice versa.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的技术要点是，尽管Dataset是未来数据处理的首选方法，我们始终可以使用API在RDD和Dataset之间来回转换。
- en: There's more...
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: More about the datatypes can be found at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据类型的更多信息，请访问[http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)。
- en: See also
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for SparkContext and SparkSession is available at the following
    websites:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: SparkContext和SparkSession的文档可在以下网站找到：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)'
- en: New model export and PMML markup in Spark 2.0
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中的新模型导出和PMML标记
- en: In this recipe, we explore the model export facility available in Spark 2.0
    to use **Predictive Model Markup Language** (**PMML**). This standard XML-based
    language allows you to export and run your models on other systems (some limitations
    apply). You can explore the *There's more...* section for more information.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们探讨了Spark 2.0中的模型导出功能，以使用**预测模型标记语言**（**PMML**）。这种基于XML的标准语言允许您在其他系统上导出和运行模型（存在一些限制）。更多信息，请参阅*还有更多...*部分。
- en: How to do it...
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在包的位置：
- en: '[PRE71]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为SparkContext导入必要的包以访问集群：
- en: '[PRE72]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Create Spark''s configuration and SparkContext:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext：
- en: '[PRE73]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We read the data from a text file; the data file contains a sample dataset
    for a KMeans model:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从文本文件中读取数据；数据文件包含一个用于KMeans模型的示例数据集：
- en: '[PRE74]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We set up the parameters for the KMeans model, and train the model using the
    preceding datasets and parameters:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了KMeans模型的参数，并使用前面提到的数据集和参数来训练模型：
- en: '[PRE75]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We have effectively created a simple KMeans model (by setting the number of
    clusters to 2) from the data structure we just created.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有效地从刚刚创建的数据结构中创建了一个简单的KMeans模型（通过将集群数量设置为2）。
- en: '[PRE76]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'In the console, it will show the following model:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中，将显示以下模型：
- en: '[PRE77]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'We then export the PMML to an XML file in the data directory:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将PMML导出到数据目录中的XML文件：
- en: '[PRE78]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '![](img/f04f499e-1d28-43c3-84dc-4829281503f9.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f04f499e-1d28-43c3-84dc-4829281503f9.png)'
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过停止Spark会话来关闭程序：
- en: '[PRE79]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: How it works...
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: After you spend the time to train a model, the next step will be to persist
    the model for future use. In this recipe, we began by training a KMeans model
    to generate model info for persistence in later steps. Once we have the trained
    model, we invoke the `toPMML()` method on the model converting it into PMML for
    storage. The invocation of the method generates an XML document, then the XML
    document text can easily be persisted to a file.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在花费时间训练模型后，下一步将是持久化模型以供将来使用。在本教程中，我们首先训练了一个KMeans模型，以生成后续步骤中用于持久化的模型信息。一旦我们有了训练好的模型，我们就调用模型的`toPMML()`方法将其转换为PMML格式以便存储。该方法的调用会生成一个XML文档，然后该XML文档文本可以轻松地持久化到文件中。
- en: There's more...
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'PMML is a standard developed by the **Data Mining Group** (**DMG**). The standard
    enables inter-platform interoperability by letting you build on one system and
    then deploy to another system in production. The PMML standard has gained momentum
    and has been adopted by most vendors. At its core, the standard is based on an
    XML document with the following:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: PMML是由**数据挖掘组**（**DMG**）开发的标准。该标准通过允许您在一个系统上构建，然后部署到生产中的另一个系统，实现了跨平台的互操作性。PMML标准已经获得了动力，并已被大多数供应商采用。其核心是基于一个XML文档，包含以下内容：
- en: Header with general information
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含一般信息的头部
- en: Dictionary describing field level definitions used by the third component (the
    model)
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字典描述了第三组件（模型）使用的字段级定义。
- en: Model structure and parameters
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型结构和参数
- en: 'As of this writing, the Spark 2.0 Machine Library support for PMML exporting
    is currently limited to:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文撰写时，Spark 2.0机器学习库对PMML导出的支持目前仅限于：
- en: Linear Regression
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic Regression
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Ridge Regression
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 岭回归
- en: Lasso
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lasso
- en: SVM
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM
- en: KMeans
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KMeans
- en: 'You can export the model to the following file types in Spark:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将模型导出到Spark支持的以下文件类型：
- en: 'Local filesystem:'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地文件系统：
- en: '[PRE80]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Distributed filesystem:'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式文件系统：
- en: '[PRE81]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Output stream--acting as a pipe:'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出流——充当管道：
- en: '[PRE82]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: See also
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: Documentation for `PMMLExportable` API documents at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`PMMLExportable` API的文档可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable)找到。
- en: Regression model evaluation using Spark 2.0
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行回归模型评估
- en: In this recipe, we explore how to evaluate a regression model (a regression
    decision tree in this example). Spark provides the **RegressionMetrics** facility
    which has basic statistical facilities such as **Mean Squared Error** (**MSE**),
    R-Squared, and so on, right out of the box.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们探讨了如何评估回归模型（本例中为回归决策树）。Spark提供了**回归度量**工具，该工具具有基本的统计功能，如**均方误差**（**MSE**）、R平方等，开箱即用。
- en: The objective in this recipe is to understand the evaluation metrics provided
    by Spark out of the box.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的目标是理解Spark原生提供的评估指标。
- en: How to do it...
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含了必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE83]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的SparkContext包以访问集群：
- en: '[PRE84]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Create Spark''s configuration and SparkContext:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext：
- en: '[PRE85]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: We utilize the ...
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们利用了...
- en: How it works...
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we explored the generation of regression metrics to help us
    evaluate our regression model. We began to load a breast cancer data file and
    then split it in a 70/30 ratio to create training and test datasets. Next, we
    trained a `DecisionTree` regression model and utilized it to make predictions
    on our test set. Finally, we took the predictions and generated regression metrics
    which gave us the squared error, R-squared, mean absolute error, and explained
    variance.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们探讨了生成回归度量以帮助评估我们的回归模型。我们首先加载了一个乳腺癌数据文件，然后以70/30的比例将其分割，创建了训练和测试数据集。接下来，我们训练了一个`决策树`回归模型，并利用它对测试集进行预测。最后，我们获取了这些预测结果，并生成了回归度量，这些度量为我们提供了平方误差、R平方、平均绝对误差和解释方差。
- en: There's more...
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can use `RegressionMetrics()` to produce the following statistical measures:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`RegressionMetrics()`来生成以下统计量：
- en: MSE
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方误差(MSE)
- en: RMSE
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方根误差(RMSE)
- en: R-squared
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R平方
- en: MAE
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均绝对误差(MAE)
- en: Explained variance
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释方差
- en: Documentation on regression validation is available at [https://en.wikipedia.org/wiki/Regression_validation](https://en.wikipedia.org/wiki/Regression_validation).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 关于回归验证的文档可在[https://en.wikipedia.org/wiki/Regression_validation](https://en.wikipedia.org/wiki/Regression_validation)找到。
- en: R-Squared/coefficient of determination is available at [https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: R平方/决定系数可在[https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)找到。
- en: See also
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The Wisconsin breast cancer dataset could be downloaded at [ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum](ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum)
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威斯康星乳腺癌数据集可在[ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum](ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum)下载
- en: Regression metrics documents are available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics)
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归度量文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics)获取
- en: Binary classification model evaluation using Spark 2.0
  id: totrans-409
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行二分类模型评估
- en: In this recipe, we demonstrate the use of the `BinaryClassificationMetrics`
    facility in Spark 2.0 and its application to evaluating a model that has a binary
    outcome (for example, a logistic regression).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们展示了在Spark 2.0中使用`BinaryClassificationMetrics`工具及其应用于评估具有二元结果（例如，逻辑回归）的模型。
- en: The purpose here is not to showcase the regression itself, but to demonstrate
    how to go about evaluating it using common metrics such as **receiver operating
    characteristic** (**ROC**), Area Under ROC Curve, thresholds, and so on.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重点不是展示回归本身，而是演示如何使用常见的度量标准（如**接收者操作特征**(**ROC**)、ROC曲线下的面积、阈值等）来评估它。
- en: How to do it...
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含了必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE86]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为SparkContext导入必要的包以访问集群：
- en: '[PRE87]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Create Spark''s configuration and SparkContext:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark配置和SparkContext：
- en: '[PRE88]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'We download the dataset, originally from the UCI, and modify it to fit the
    need for the code:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从UCI下载原始数据集，并对其进行修改以适应代码需求：
- en: '[PRE89]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: The dataset is a modified dataset. The original adult dataset has 14 features,
    among which six are continuous and eight are categorical. In this dataset, continuous
    features are discretized into quantiles, and each quantile is represented by a
    binary feature. We modified the data to fit the purpose of the code. Details of
    the dataset feature can be found at the [http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)
    UCI site.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是一个修改过的数据集。原始的成人数据集有14个特征，其中6个是连续的，8个是分类的。在这个数据集中，连续特征被离散化为分位数，每个分位数由一个二进制特征表示。我们修改了数据以适应代码的目的。数据集特征的详细信息可在[http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)
    UCI网站上找到。
- en: 'We split the dataset into training and test parts in a ratio of 60:40 random
    split, then get the model:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据集以60:40的随机比例分割为训练和测试部分，然后获取模型：
- en: '[PRE90]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'We create the prediction using the model created by the training dataset:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用训练数据集创建的模型来进行预测：
- en: '[PRE91]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'We create the `BinaryClassificationMetrics` object from the predication, and
    start the evaluation on the metrics:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从预测结果创建了`BinaryClassificationMetrics`对象，并开始对度量进行评估：
- en: '[PRE92]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We print out the precision by `Threashold` in the console:'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台输出了按`阈值`的精确度：
- en: '[PRE93]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'From the console output:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出中：
- en: '[PRE94]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'We print out the `recallByThreshold` in the console:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台输出了`recallByThreshold`：
- en: '[PRE95]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'From the console output:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出中：
- en: '[PRE96]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'We print out the `fmeasureByThreshold` in the console:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台输出了`fmeasureByThreshold`：
- en: '[PRE97]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'From the console output:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出中：
- en: '[PRE98]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'From the console output:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出中：
- en: '[PRE99]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'We print out the `Area Under Precision Recall Curve` in the console:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台输出了`Precision Recall曲线下的面积`：
- en: '[PRE100]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'From the console output:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出中：
- en: '[PRE101]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'We print out the Area Under ROC curve in the console:'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台输出了ROC曲线下的面积：
- en: '[PRE102]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'From the console output:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出中：
- en: '[PRE103]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过停止Spark会话来关闭程序：
- en: '[PRE104]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: How it works...
  id: totrans-453
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we investigated the evaluation of metrics for binary classification.
    First, we loaded the data, which is in the `libsvm` format, and split it in the
    ratio of 60:40, resulting in the creation of a training and a test set of data.
    Next, we trained a logistic regression model followed by generating predictions
    from our test set.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们研究了二分类度量的评估。首先，我们加载了`libsvm`格式的数据，并按60:40的比例分割，生成了训练集和测试集。接着，我们训练了一个逻辑回归模型，并从测试集中生成预测。
- en: Once we had our predictions, we created a binary classification metrics object.
    Finally, we retrieved the true positive rate, positive predictive value, receiver
    operating curve, the area under receiver operating curve, the area under precision
    recall curve, and F-measure to evaluate our model for fitness.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们得到预测结果后，我们创建了一个二分类度量对象。最后，我们获取了真阳性率、阳性预测值、接收者操作特征曲线、接收者操作特征曲线下的面积、精确召回曲线下的面积和F度量，以评估模型的适应性。
- en: There's more...
  id: totrans-456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Spark provides the following metrics to facilitate evaluation:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了以下度量以方便评估：
- en: TPR - True Positive Rate
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TPR - 真阳性率
- en: PPV - Positive Predictive Value
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPV - 阳性预测值
- en: F - F-Measure
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F - F度量
- en: ROC - Receiver Operating Curve
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC - 接收者操作特征曲线
- en: AUROC - Area Under Receiver Operating Curve
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUROC - 接收者操作特征曲线下的面积
- en: AUORC - Area Under Precision-Recall Curve
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUORC - 精确召回曲线下的面积
- en: 'The following links should provide a good introductory material for the metrics:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了度量的良好入门材料：
- en: '[https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[接收者操作特征曲线](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)'
- en: '[https://en.wikipedia.org/wiki/Sensitivity_and_specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[灵敏度和特异性](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)'
- en: '[https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score)'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[F1分数](https://en.wikipedia.org/wiki/F1_score)'
- en: See also
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Documentation for the original dataset information is available at the following
    links:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集信息的文档可在以下链接获得：
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html)'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[libsvm工具数据集](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html)'
- en: '[http://archive.ics.uci.edu/ml/datasets.html](http://archive.ics.uci.edu/ml/datasets.html)'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UCI机器学习数据集](http://archive.ics.uci.edu/ml/datasets.html)'
- en: Documentation for binary classification metrics is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类度量的文档可在[此处](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics)获得。
- en: Multiclass classification model evaluation using Spark 2.0
  id: totrans-473
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行多类别分类模型评估
- en: In this recipe, we explore `MulticlassMetrics`, which allows you to evaluate
    a model that classifies the output to more than two labels (for example, red,
    blue, green, purple, do-not-know). It highlights the use of a confusion matrix
    (`confusionMatrix`) and model accuracy.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们探讨了`MulticlassMetrics`，它允许你评估一个将输出分类为两个以上标签（例如，红色、蓝色、绿色、紫色、未知）的模型。它突出了混淆矩阵（`confusionMatrix`）和模型准确性的使用。
- en: How to do it...
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE105]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为SparkContext导入必要的包以访问集群：
- en: '[PRE106]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Create Spark''s configuration and SparkContext:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext：
- en: '[PRE107]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: How it works...
  id: totrans-483
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we explored generating evaluation metrics for a multi-classification
    model. First, we loaded the Iris data into memory and split it in a ratio of 60:40\.
    Second, we trained a logistic regression model with the number of classifications
    set to three. Third, we made predictions with the test dataset and utilized `MultiClassMetric`
    to generate evaluation measurements. Finally, we evaluated metrics such as the
    model accuracy, weighted precision, weighted recall, weighted F1 score, weighted
    false positive rate, and so on.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了为多分类模型生成评估指标。首先，我们将鸢尾花数据加载到内存中，并按60:40的比例分割。其次，我们使用三个分类训练了一个逻辑回归模型。第三，我们使用测试数据集进行预测，并利用`MultiClassMetric`生成评估测量。最后，我们评估了诸如模型准确性、加权精度、加权召回率、加权F1分数、加权假阳性率等指标。
- en: There's more...
  id: totrans-485
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While the scope of the book does not allow for a complete treatment of the confusion
    matrix, a short explanation and a link are provided as a quick reference.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的范围不允许对混淆矩阵进行全面处理，但提供了一个简短的解释和一个链接作为快速参考。
- en: 'The confusion matrix is just a fancy name for an error matrix. It is mostly
    used in unsupervised learning to visualize the performance. It is a layout that
    captures actual versus predicted outcomes with an identical set of labels in two
    dimensions:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵只是一个错误矩阵的华丽名称。它主要用于无监督学习中以可视化性能。它是一种布局，捕捉实际与预测结果，使用两维中相同的标签集：
- en: '**Confusion Matrix**'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '**混淆矩阵**'
- en: '![](img/2517d73f-07b5-4fcf-9750-bd8c0d6a4469.png)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2517d73f-07b5-4fcf-9750-bd8c0d6a4469.png)'
- en: To get a quick introduction to the confusion matrix in unsupervised and supervised
    statistical learning systems, see [https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix).
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速了解无监督和监督统计学习系统中的混淆矩阵，请参见[https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix)。
- en: See also
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Documentation for original dataset information is available at the following
    websites:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集信息的文档可在以下网站获得：
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html)'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html)'
- en: '[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)'
- en: 'Documentation for multiclass classification metrics is available at:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类度量文档可在此处获得：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)'
- en: Multilabel classification model evaluation using Spark 2.0
  id: totrans-497
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行多标签分类模型评估
- en: In this recipe, we explore multilabel classification `MultilabelMetrics` in
    Spark 2.0 which should not be mixed up with the previous recipe dealing with multiclass
    classification `MulticlassMetrics`. The key to exploring this recipe is to concentrate
    on evaluation metrics such as Hamming loss, accuracy, f1-measure, and so on, and
    what they measure.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了Spark 2.0中的多标签分类`MultilabelMetrics`，这不应与前一节中涉及多类分类`MulticlassMetrics`的内容混淆。探索此节的关键是专注于评估指标，如汉明损失、准确性、F1度量等，以及它们所衡量的内容。
- en: How to do it...
  id: totrans-499
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE108]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为SparkContext导入必要的包以访问集群：
- en: '[PRE109]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Create Spark''s configuration and SparkContext:'
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext：
- en: '[PRE110]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'We create the dataset for the evaluation model:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为评估模型创建数据集：
- en: '[PRE111]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'We create the `MultilabelMetrics` object from the predication, and start the
    evaluation on the metrics:'
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从预测结果创建`MultilabelMetrics`对象，并开始对指标进行评估：
- en: '[PRE112]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'We print out the overall statistics summary in the console:'
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台打印出总体统计摘要：
- en: '[PRE113]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'From the console output:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出中：
- en: '[PRE114]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'We print out the individual label value in the console:'
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台打印出各个标签的值：
- en: '[PRE115]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'From the console output:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出中：
- en: '[PRE116]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'We print out the micro-statistics value in the console:'
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台打印出微观统计值：
- en: '[PRE117]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'We print out the Hamming loss and subset accuracy from the metrics in the console:'
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台从度量中打印出汉明损失和子集准确性：
- en: '[PRE118]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: We then close the program by stopping the Spark session.
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过停止Spark会话来关闭程序。
- en: '[PRE119]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: How it works...
  id: totrans-525
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we investigated generating evaluation metrics for the multilabel
    classification model. We began with manually creating a dataset for the model
    evaluation. Next, we passed our dataset as an argument to the `MultilabelMetrics`
    and generated evaluation metrics. Finally, we printed out various metrics such
    as micro recall, micro precision, micro f1-measure, Hamming loss, subset accuracy,
    and so on.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了为多标签分类模型生成评估指标的过程。我们首先手动创建了一个用于模型评估的数据集。接着，我们将数据集作为参数传递给`MultilabelMetrics`，并生成了评估指标。最后，我们打印出了各种指标，如微观召回率、微观精确度、微观F1度量、汉明损失、子集准确性等。
- en: There's more...
  id: totrans-527
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Note that the multilabel and multiclass classifications sound similar, but they
    are two different things.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，多标签分类和多类别分类听起来相似，但实际上是两种不同的概念。
- en: All multilabel `MultilabelMetrics()` method is trying to accomplish is to map
    a number of inputs (x) to a binary vector (y) rather than numerical values in
    a typical classification system.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 所有多标签的`MultilabelMetrics()`方法试图实现的是将多个输入（x）映射到一个二进制向量（y），而不是典型分类系统中的数值。
- en: 'The important metrics associated with the multilabel classification are (see
    the preceding code):'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 与多标签分类相关的重要度量包括（参见前面的代码）：
- en: Accuracy
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性
- en: Hamming loss
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汉明损失
- en: Precision
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度
- en: Recall
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回率
- en: F1
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1分数
- en: 'A full explanation of each parameter is out of scope, but the following link
    provides a short treatment for the multilabel metrics:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 每个参数的完整解释超出了范围，但以下链接提供了对多标签度量的简要说明：
- en: '[https://en.wikipedia.org/wiki/Multi-label_classification](https://en.wikipedia.org/wiki/Multi-label_classification)'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '[多标签分类](https://en.wikipedia.org/wiki/Multi-label_classification)'
- en: See also
  id: totrans-538
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Documentation for multilabel classification metrics:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类度量文档：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics)'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark最新文档中的MultilabelMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics)'
- en: Using the Scala Breeze library to do graphics in Spark 2.0
  id: totrans-541
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scala Breeze库在Spark 2.0中进行图形绘制
- en: In this recipe, we will use the functions `scatter()` and `plot()` from the
    Scala Breeze linear algebra library (part of) to draw a scatter plot from a two-dimensional
    data. Once the results are computed on the Spark cluster, either the actionable
    data can be used in the driver for drawing or a JPEG or GIF can be generated in
    the backend and pushed forward for efficiency and speed (popular with GPU-based
    analytical databases such as MapD)
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Scala Breeze线性代数库（的一部分）中的`scatter()`和`plot()`函数来绘制二维数据的散点图。一旦在Spark集群上计算出结果，要么可以在驱动程序中使用可操作数据进行绘图，要么可以在后端生成JPEG或GIF图像，并推送以提高效率和速度（这在基于GPU的分析数据库如MapD中很流行）
- en: How to do it...
  id: totrans-543
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: First, we need to download the necessary ScalaNLP library. Download the JAR
    from the Maven repository available at [https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar).
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要下载必要的ScalaNLP库。从Maven仓库下载JAR文件，地址为[https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar)。
- en: 'Place the JAR in the `C:\spark-2.0.0-bin-hadoop2.7\examples\jars` directory
    on a Windows machine:'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将JAR文件放置在Windows机器上的`C:\spark-2.0.0-bin-hadoop2.7\examples\jars`目录中：
- en: In macOS, please put the JAR in its correct path. For our setting examples,
    the path is `/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/examples/jars/`.
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在macOS上，请将JAR文件放置在其正确的路径中。对于我们的设置示例，路径是`/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/examples/jars/`。
- en: 'The following is the sample screenshot showing the JARs:'
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是显示JAR文件的示例截图：
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或你选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE120]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Import the ...
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入...
- en: How it works...
  id: totrans-552
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we created a dataset in Spark from random numbers. We then created
    a Breeze figure and set up the basic parameters. We derived *x*, *y* data from
    the created dataset.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们通过随机数在Spark中创建了一个数据集。随后，我们创建了一个Breeze图形并设置了基本参数。我们从创建的数据集中导出了*x*, *y*数据。
- en: We used Breeze's `scatter()` and `plot()` functions to do graphics using the
    Breeze library.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Breeze的`scatter()`和`plot()`函数，通过Breeze库进行图形绘制。
- en: There's more...
  id: totrans-555
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: One can use Breeze as an alternative to more complicated and powerful charting
    libraries such as JFreeChart, demonstrated in the previous chapter. The ScalaNLP
    project tends to be optimized with Scala goodies such as implicit conversions
    that make the coding relatively easier.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们展示了可以使用Breeze作为JFreeChart等更复杂、功能更强大的图表库的替代方案。ScalaNLP项目倾向于利用Scala的特性，如隐式转换，使得编码相对容易。
- en: The Breeze graphics JAR file can be downloaded at [http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar).
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze图形JAR文件可在[http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar)下载。
- en: More about Breeze graphics can be found at [https://github.com/scalanlp/breeze/wiki/Quickstart](https://github.com/scalanlp/breeze/wiki/Quickstart).
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于Breeze图形的信息可在[https://github.com/scalanlp/breeze/wiki/Quickstart](https://github.com/scalanlp/breeze/wiki/Quickstart)找到。
- en: The API document (please note, the API documentation is not necessarily up-to-date)
    can be found at [http://www.scalanlp.org/api/breeze/#package](http://www.scalanlp.org/api/breeze/#package).
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: API文档（请注意，API文档可能不是最新的）可在[http://www.scalanlp.org/api/breeze/#package](http://www.scalanlp.org/api/breeze/#package)找到。
- en: Note that once you are in the root package, you need click on Breeze to ...
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一旦进入根包，你需要点击Breeze以...
- en: See also
  id: totrans-561
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: For more information on Breeze, see the original material on GitHub at [https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze).
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Breeze的更多信息，请参阅GitHub上的原始资料[https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)。
- en: Note that once you are in the root package, you need to click on Breeze to see
    the details.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一旦进入根包，你需要点击Breeze以查看详细信息。
- en: For more information regarding the Breeze API documentation, please download
    the [https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar)
    JAR.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Breeze API文档的更多信息，请下载[https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar)
    JAR。
