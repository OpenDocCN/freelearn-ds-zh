- en: Building a Clustering Model with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark构建聚类模型
- en: In the last few chapters, we covered supervised learning methods, where the
    training data is labeled with the true outcome that we would like to predict (for
    example, a rating for recommendations and class assignment for classification
    or a real target variable in the case of regression).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们涵盖了监督学习方法，其中训练数据带有我们想要预测的真实结果的标签（例如，推荐的评级和分类的类分配，或者在回归的情况下是真实目标变量）。
- en: Next, we will consider the case where we do not have labeled data available.
    This is called **unsupervised learning**, as the model is not supervised with
    the true target label. The unsupervised case is very common in practice, since
    obtaining labeled training data can be very difficult or expensive in many real-world
    scenarios (for example, having humans label training data with class labels for
    classification). However, we would still like to learn some underlying structure
    in the data and use these to make predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将考虑没有可用标记数据的情况。这被称为**无监督学习**，因为模型没有受到真实目标标签的监督。无监督情况在实践中非常常见，因为在许多真实场景中获取标记的训练数据可能非常困难或昂贵（例如，让人类为分类标签标记训练数据）。然而，我们仍然希望学习数据中的一些潜在结构，并使用这些结构进行预测。
- en: This is where unsupervised learning approaches can be useful. Unsupervised learning
    models are also often combined with supervised models; for example, applying unsupervised
    techniques to create new input features for supervised models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是无监督学习方法可以发挥作用的地方。无监督学习模型也经常与监督模型结合使用；例如，应用无监督技术为监督模型创建新的输入特征。
- en: Clustering models are, in many ways, the unsupervised equivalent of classification
    models. With classification, we tried to learn a model that would predict which
    class a given training example belonged to. The model was essentially a mapping
    from a set of features to the class.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类模型在许多方面类似于分类模型的无监督等价物。在分类中，我们试图学习一个模型，可以预测给定训练示例属于哪个类。该模型本质上是从一组特征到类的映射。
- en: In clustering, we would like to segment the data such that each training example
    is assigned to a segment called a **cluster**. The clusters act much like classes,
    except that the true class assignments are unknown.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，我们希望对数据进行分段，以便将每个训练示例分配给一个称为**簇**的段。这些簇的作用很像类，只是真实的类分配是未知的。
- en: 'Clustering models have many use cases that are the same as classification;
    these include the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类模型有许多与分类相同的用例；其中包括以下内容：
- en: Segmenting users or customers into different groups based on behavior characteristics
    and metadata
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据行为特征和元数据将用户或客户分成不同的群体
- en: Grouping content on a website or products in a retail business
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网站上对内容进行分组或在零售业务中对产品进行分组
- en: Finding clusters of similar genes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找相似基因的簇
- en: Segmenting communities in ecology
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生态学中对社区进行分割
- en: Creating image segments for use in image analysis applications such as object
    detection
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建图像段，用于图像分析应用，如目标检测
- en: 'In this chapter, we will:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: Briefly explore a few types of clustering models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简要探讨几种聚类模型
- en: Extract features from the data specifically using the output of one model as
    input features for our clustering model
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据中提取特征，特别是使用一个模型的输出作为我们聚类模型的输入特征
- en: Train a clustering model and use it to make predictions
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个聚类模型并使用它进行预测
- en: Apply performance-evaluation and parameter-selection techniques to select the
    optimal number of clusters to use
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用性能评估和参数选择技术来选择要使用的最佳簇数
- en: Types of clustering models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类模型的类型
- en: There are many different forms of clustering models available, ranging from
    simple to extremely complex ones. The Spark MLlib currently provides k-means clustering,
    which is among the simplest approaches available. However, it is often very effective,
    and its simplicity means it is relatively easy to understand and is scalable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同形式的聚类模型可用，从简单到极其复杂。Spark MLlib目前提供k-means聚类，这是最简单的方法之一。然而，它通常非常有效，而且其简单性意味着相对容易理解并且可扩展。
- en: k-means clustering
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means聚类
- en: k-means attempts to partition a set of data points into *K* distinct clusters
    (where *K* is an input parameter for the model).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: k-means试图将一组数据点分成*K*个不同的簇（其中*K*是模型的输入参数）。
- en: More formally, k-means tries to find clusters so as to minimize the sum of squared
    errors (or distances) within each cluster. This objective function is known as
    the **within cluster sum of squared errors** (**WCSS**).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，k-means试图找到簇，以便最小化每个簇内的平方误差（或距离）。这个目标函数被称为**簇内平方误差和**（**WCSS**）。
- en: '![](img/image_08_001.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_001.png)'
- en: It is the sum, over each cluster, of the squared errors between each point and
    the cluster center.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它是每个簇中每个点与簇中心之间的平方误差的总和。
- en: 'Starting with a set of *K* initial cluster centers (which are computed as the
    mean vector for all data points in the cluster), the standard method for K-means
    iterates between two steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从一组*K*个初始簇中心开始（这些中心是计算为簇中所有数据点的平均向量），K-means的标准方法在两个步骤之间进行迭代：
- en: Assign each data point to the cluster that minimizes the WCSS. The sum of squares
    is equivalent to the squared Euclidean distance; therefore, this equates to assigning
    each point to the **closest** cluster center as measured by the Euclidean distance
    metric.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个数据点分配给最小化WCSS的簇。平方和等于平方欧氏距离；因此，这相当于根据欧氏距离度量将每个点分配给**最接近**的簇中心。
- en: Compute the new cluster centers based on the cluster assignments from the first
    step.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据第一步的簇分配计算新的簇中心。
- en: The algorithm proceeds until either a maximum number of iterations has been
    reached or convergence has been achieved. **Convergence** means that the cluster
    assignments no longer change during the first step; therefore, the value of the
    WCSS objective function does not change either.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法进行到达到最大迭代次数或收敛为止。**收敛**意味着在第一步期间簇分配不再改变；因此，WCSS目标函数的值也不再改变。
- en: For more details, refer to Spark's documentation on clustering at [http://spark.apache.org/docs/latest/mllib-clustering.html](http://spark.apache.org/docs/latest/mllib-clustering.html)
    or refer to [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参考Spark关于聚类的文档[http://spark.apache.org/docs/latest/mllib-clustering.html](http://spark.apache.org/docs/latest/mllib-clustering.html)或参考[http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering)。
- en: 'To illustrate the basics of K-means, we will use the simple dataset we showed
    in our multiclass classification example in [Chapter 6](7bd5bfd3-6301-49dc-ba28-5d6553b57e01.xhtml),
    *Building a Classification Model with Spark*. Recall that we have five classes,
    which are shown in the following figure:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明K-means的基础知识，我们将使用我们在[第6章](7bd5bfd3-6301-49dc-ba28-5d6553b57e01.xhtml)中展示的多类分类示例中所示的简单数据集，*使用Spark构建分类模型*。回想一下，我们有五个类别，如下图所示：
- en: '![](img/image_08_002.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_002.png)'
- en: Multiclass dataset
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 多类数据集
- en: 'However, assume that we don''t actually know the true classes. If we use k-means
    with five clusters, then after the first step, the model''s cluster assignments
    might look like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设我们实际上不知道真实的类别。如果我们使用五个簇的k-means，那么在第一步之后，模型的簇分配可能是这样的：
- en: '![](img/image_08_003.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_003.png)'
- en: Cluster assignments after the first K-means iteration
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次K-means迭代后的簇分配
- en: 'We can see that k-means has already picked out the centers of each cluster
    fairly well. After the next iteration, the assignments might look like those shown
    in the following figure:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，k-means已经相当好地挑选出了每个簇的中心。在下一次迭代之后，分配可能看起来像下图所示的那样：
- en: '![](img/image_08_004.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_004.png)'
- en: Cluster assignments after the second K-means iteration
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次K-means迭代后的簇分配
- en: 'Things are starting to stabilize, but the overall cluster assignments are broadly
    the same as they were after the first iteration. Once the model has converged,
    the final assignments could look like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 事情开始稳定下来，但总体簇分配与第一次迭代后基本相同。一旦模型收敛，最终的分配可能看起来像这样：
- en: '![](img/image_08_005.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_005.png)'
- en: Final cluster assignments for K-means
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: K-means的最终簇分配
- en: As we can see, the model has done a decent job of separating the five clusters.
    The leftmost three are fairly accurate (with a few incorrect points). However,
    the two clusters in the bottom-right corner are less accurate.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，模型已经相当好地分离了这五个簇。最左边的三个相当准确（有一些错误的点）。然而，右下角的两个簇不太准确。
- en: 'This illustrates:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明：
- en: The iterative nature of K-means
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means的迭代性质
- en: The model's dependency on the method of initially selecting clusters' centers
    (here, we will use a random approach)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型对于最初选择簇中心的方法的依赖性（这里，我们将使用随机方法）
- en: That the final cluster assignments can be very good for well-separated data
    but can be poor for data that is more difficult
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终的簇分配对于分离良好的数据可能非常好，但对于更困难的数据可能较差
- en: Initialization methods
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化方法
- en: The standard initialization method for k-means, usually simply referred to as
    the random method, starts by randomly assigning each data point to a cluster before
    proceeding with the first update step.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: k-means的标准初始化方法通常简称为随机方法，它首先随机将每个数据点分配给一个簇，然后进行第一个更新步骤。
- en: Spark ML provides a parallel variant for this initialization method, called
    **K-means ++**, which is the default initialization method used.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML提供了这种初始化方法的并行变体，称为**K-means++**，这是默认的初始化方法。
- en: See [http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods](http://en.wikipedia.org/wiki/K-means_clustering)
    and [http://en.wikipedia.org/wiki/K-means%2B%2B](http://en.wikipedia.org/wiki/K-means%2B%2B)
    for more information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅[http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods](http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods)和[http://en.wikipedia.org/wiki/K-means%2B%2B](http://en.wikipedia.org/wiki/K-means%2B%2B)。
- en: 'The results of using K-means++ are shown here. Note that this time, the difficult
    lower-right points have been mostly correctly clustered:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用K-means++的结果如下所示。请注意，这一次，困难的右下角点大部分被正确地聚类了：
- en: '![](img/image_08_006.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_006.png)'
- en: Final cluster assignments for K-means++
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: K-means++的最终簇分配
- en: There are many other variants of K-means; they focus on initialization methods
    or the core model. One of the more common variants is **fuzzy K-means**. This
    model does not assign each point to one cluster as K-means does (a so-called hard
    assignment). Instead, it is a soft version of K-means, where each point can belong
    to many clusters, and is represented by the relative membership to each cluster.
    So, for *K* clusters, each point is represented as a K-dimensional membership
    vector, with each entry in this vector indicating the membership proportion in
    each cluster.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的K-means变体；它们侧重于初始化方法或核心模型。其中一个更常见的变体是**模糊K-means**。这个模型不像K-means那样将每个点分配给一个簇（所谓的硬分配）。相反，它是K-means的软版本，其中每个点可以属于许多簇，并且由相对于每个簇的成员资格表示。因此，对于*K*个簇，每个点被表示为一个K维成员资格向量，向量中的每个条目表示在每个簇中的成员资格比例。
- en: Mixture models
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合模型
- en: A **mixture model** is essentially an extension of the idea behind fuzzy K-means;
    however, it makes an assumption that there is an underlying probability distribution
    that generates the data. For example, we might assume that the data points are
    drawn from a set of K-independent Gaussian (normal) probability distributions.
    The cluster assignments are also soft, so each point is represented by *K* membership
    weights in each of the *K* underlying probability distributions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合模型**本质上是模糊K均值背后思想的延伸；然而，它假设存在一个生成数据的潜在概率分布。例如，我们可能假设数据点是从一组K个独立的高斯（正态）概率分布中抽取的。簇分配也是软的，因此每个点在K个潜在概率分布中都由*K*成员权重表示。'
- en: See [http://en.wikipedia.org/wiki/Mixture_model](http://en.wikipedia.org/wiki/Mixture_model)
    for further details and for a mathematical treatment of mixture models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有关混合模型的更多详细信息和混合模型的数学处理，请参见[http://en.wikipedia.org/wiki/Mixture_model](http://en.wikipedia.org/wiki/Mixture_model)。
- en: Hierarchical clustering
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层聚类
- en: '**Hierarchical clustering** is a structured clustering approach that results
    in a multilevel hierarchy of clusters, where each cluster might contain many subclusters
    (or child clusters). Each child cluster is, thus, linked to the parent cluster.
    This form of clustering is often also called **tree clustering**.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**分层聚类**是一种结构化的聚类方法，它会产生一个多级别的簇层次结构，其中每个簇可能包含许多子簇。因此，每个子簇都与父簇相关联。这种形式的聚类通常也被称为**树状聚类**。'
- en: 'Agglomerative clustering is a bottom-up approach where:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 凝聚聚类是一种自下而上的方法：
- en: Each data point begins in its own cluster
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据点都开始在自己的簇中
- en: The similarity (or distance) between each pair of clusters is evaluated
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估每对簇之间的相似性（或距离）
- en: The pair of clusters that are most similar are found; this pair is then merged
    to form a new cluster
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到最相似的一对簇；然后将这对簇合并成一个新的簇
- en: The process is repeated until only one top-level cluster remains
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该过程重复进行，直到只剩下一个顶层簇
- en: '**Divisive clustering** is a top-down approach that works in reverse, starting
    with one cluster and at each stage, splitting a cluster into two, until all data
    points are allocated to their own bottom-level cluster.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**分裂聚类**是一种自上而下的方法，从一个簇开始，然后在每个阶段将一个簇分裂成两个，直到所有数据点都被分配到自己的底层簇中。'
- en: '**Top-down clustering** is more complex than bottom-up clustering since a second,
    flat clustering algorithm is needed as a "subroutine''''. Top-down clustering
    has the advantage of being more efficient if we do not generate a complete hierarchy
    to individual document leaves.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**自上而下聚类**比自下而上聚类更复杂，因为需要第二个平面聚类算法作为“子程序”。如果我们不生成完整的层次结构到单个文档叶子，自上而下聚类具有更高的效率。'
- en: You can find more information at [http://en.wikipedia.org/wiki/Hierarchical_clustering](http://en.wikipedia.org/wiki/Hierarchical_clustering).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[http://en.wikipedia.org/wiki/Hierarchical_clustering](http://en.wikipedia.org/wiki/Hierarchical_clustering)找到更多信息。
- en: Extracting the right features from your data
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据中提取正确的特征
- en: Like most of the machine learning models we have encountered so far, k-means
    clustering requires numerical vectors as input. The same feature extraction and
    transformation approaches that we have seen for classification and regression
    are applicable for clustering.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与迄今为止遇到的大多数机器学习模型一样，k均值聚类需要数值向量作为输入。我们已经看到的用于分类和回归的相同特征提取和转换方法也适用于聚类。
- en: As k-means, like least squares regression, uses a squared error function as
    the optimization objective, it tends to be impacted by outliers and features with
    large variance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与最小二乘回归一样，由于k均值使用平方误差函数作为优化目标，它往往会受到异常值和具有大方差的特征的影响。
- en: Clustering could be leveraged to detect outliers as they can cause a lot of
    problems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以用来检测异常值，因为它们可能会引起很多问题。
- en: As for regression and classification cases, input data can be normalized and
    standardized to overcome this, which might improve accuracy. In some cases, however,
    it might be desirable not to standardize data, if, for example, the objective
    is to find segmentations according to certain specific features.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归和分类情况，输入数据可以被标准化和规范化以克服这一问题，这可能会提高准确性。然而，在某些情况下，如果例如目标是根据某些特定特征找到分割，可能不希望标准化数据。
- en: Extracting features from the MovieLens dataset
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从MovieLens数据集中提取特征
- en: 'We will use the ALS algorithm to get numerical features for users and items
    (movies) in this case before we can use the clustering algorithm on the data:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用聚类算法之前，我们将使用ALS算法获取用户和项目（电影）的数值特征：
- en: 'First we load the data `u.data` into a DataFrame:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先将数据`u.data`加载到DataFrame中：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we split it into a 80:20 ratio to get the training and test data:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将其按80:20的比例分割，得到训练和测试数据：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We instantiate the `ALS` class, set the maximum iterations at `5`, and the
    regularization parameter at `0.01`:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实例化`ALS`类，将最大迭代次数设置为`5`，正则化参数设置为`0.01`：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we create a model, followed by calculating predictions:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建一个模型，然后计算预测：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is followed by calculating `userFactors` and `itemFactors`:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着计算`userFactors`和`itemFactors`：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We convert them into libsvmformat and persist them in a file. Notice that we
    are persisting all the features as well two features:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将它们转换为libsvm格式并将它们持久化在一个文件中。请注意，我们持久化所有特征以及两个特征：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of `movie_lens_items_libsvm` will look like the following listing:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_lens_items_libsvm`的输出将如下所示：'
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we persist the top two features (with maximum variation) and persist
    them in a file:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们持久化前两个特征（具有最大变化）并将它们持久化在一个文件中：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of `movie_lens_items_xy` will look like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_lens_items_xy`的输出将如下所示：'
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next we calculate the libsvm format for `userFactors`:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来我们计算`userFactors`的libsvm格式：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output of `movie_lens_users_libsvm` will look like the following listing:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_lens_users_libsvm`的输出将如下所示：'
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next we extract the top two features and persist them in a file:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来我们提取前两个特征并将它们持久化在一个文件中：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of `movie_lens_user_xy` will look like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_lens_user_xy`的输出将如下所示：'
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We will need *xy* features to do clustering for two features so that we can
    create a two-dimensional plot.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要*xy*特征来对两个特征进行聚类，以便我们可以创建一个二维图。
- en: K-means - training a clustering model
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means - 训练聚类模型
- en: Training for K-means in Spark ML takes an approach similar to the other models
    -- we pass a DataFrame that contains our training data to the fit method of the
    `KMeans` object.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark ML中，对K-means进行训练采用了与其他模型类似的方法——我们将包含训练数据的DataFrame传递给`KMeans`对象的fit方法。
- en: Here we use the libsvm data format.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用libsvm数据格式。
- en: Training a clustering model on the MovieLens dataset
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MovieLens数据集上训练聚类模型
- en: We will train a model for both the movie and user factors that we generated
    by running our recommendation model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为我们通过运行推荐模型生成的电影和用户因子训练模型。
- en: We need to pass in the number of clusters *K* and the maximum number of iterations
    for the algorithm to run. Model training might run for less than the maximum number
    of iterations if the change in the objective function from one iteration to the
    next is less than the tolerance level (the default for this tolerance is 0.0001).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要传入簇的数量*K*和算法运行的最大迭代次数。如果从一次迭代到下一次迭代的目标函数的变化小于容差水平（默认容差为0.0001），则模型训练可能会运行少于最大迭代次数。
- en: Spark ML's k-means provides random and K-means || initialization, with the default
    being K-means ||. As both of these initialization methods are based on random
    selection to some extent, each model training run will return a different result.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML的k-means提供了随机和K-means ||初始化，其中默认为K-means ||。由于这两种初始化方法在某种程度上都是基于随机选择的，因此每次模型训练运行都会返回不同的结果。
- en: K-means does not generally converge to a global optimum model, so performing
    multiple training runs and selecting the most optimal model from these runs is
    a common practice. MLlib's training methods expose an option to complete multiple
    model training runs. The best training run, as measured by the evaluation of the
    loss function, is selected as the final model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: K-means通常不会收敛到全局最优模型，因此进行多次训练运行并从这些运行中选择最优模型是一种常见做法。MLlib的训练方法公开了一个选项，可以完成多个模型训练运行。通过评估损失函数的评估，选择最佳训练运行作为最终模型。
- en: 'First we create a `SparkSession` instance and use it to load the `movie_lens_users_libsvm`
    data:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个`SparkSession`实例，并使用它来加载`movie_lens_users_libsvm`数据：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then we create a model:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建一个模型：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we train a K-means model using a dataset of user vectors:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用用户向量数据集训练一个K-means模型：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**K-means**: Making predictions using a clustering model.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-means**：使用聚类模型进行预测。'
- en: Using the trained K-means model is straightforward and similar to the other
    models we have encountered so far, such as classification and regression.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练好的K-means模型是简单的，并且类似于我们迄今为止遇到的其他模型，如分类和回归。
- en: 'We can make predictions for multiple inputs by passing a DataFrame to the transform
    method of the model:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将DataFrame传递给模型的transform方法，我们可以对多个输入进行预测：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The resulting output is a cluster assignment for each data point in the prediction
    column:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出是预测列中每个数据点的聚类分配：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Due to random initialization, the cluster assignments might change from one
    run of the model to another, so your results might differ from those shown earlier.
    The cluster IDs themselves have no inherent meaning; they are simply arbitrarily
    labeled, starting from 0.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机初始化，聚类分配可能会从模型的一次运行到另一次运行发生变化，因此您的结果可能与之前显示的结果不同。聚类ID本身没有固有含义；它们只是从0开始的任意标记。
- en: K-means - interpreting cluster predictions on the MovieLens dataset
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means - 解释MovieLens数据集上的簇预测
- en: We have covered how to make predictions for a set of input vectors, but how
    do we evaluate how good the predictions are? We will cover performance metrics
    a little later; however, here, we will see how to manually inspect and interpret
    the cluster assignments made by our k-means model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了如何对一组输入向量进行预测，但是我们如何评估预测的好坏呢？我们稍后将介绍性能指标；但是在这里，我们将看到如何手动检查和解释我们的k-means模型所做的聚类分配。
- en: While unsupervised techniques have the advantage that they do not require us
    to provide labeled data for training, the disadvantage is that, often, the results
    need to be manually interpreted. Often, we would like to further examine the clusters
    that are found and possibly try to interpret them and assign some sort of labeling
    or categorization to them.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然无监督技术的优点是不需要我们提供标记的训练数据，但缺点是通常需要手动解释结果。通常，我们希望进一步检查找到的簇，并可能尝试解释它们并为它们分配某种标签或分类。
- en: For example, we can examine the clustering of movies we have found to try to
    see whether there is some meaningful interpretation of each cluster, such as a
    common genre or theme among the movies in the cluster. There are many approaches
    we can use, but we will start by taking a few movies in each cluster that are
    closest to the center of the cluster. These movies, we assume, would be the ones
    that are least likely to be marginal in terms of their cluster assignment, and
    so, they should be among the most representative of the movies in the cluster.
    By examining these sets of movies, we can see what attributes are shared by the
    movies in each cluster.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以检查我们找到的电影的聚类，尝试看看是否有一些有意义的解释，比如在聚类中的电影中是否有共同的流派或主题。我们可以使用许多方法，但我们将从每个聚类中取几部最接近聚类中心的电影开始。我们假设这些电影最不可能在其聚类分配方面边缘化，因此它们应该是最具代表性的电影之一。通过检查这些电影集，我们可以看到每个聚类中的电影共享哪些属性。
- en: Interpreting the movie clusters
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释电影簇
- en: 'We will try to list the movie associated with each cluster by joining the dataset
    with the movie name with the prediction output dataset:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试通过将数据集与预测输出数据集中的电影名称进行连接，列出与每个聚类相关联的电影：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Interpreting the movie clusters
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释电影簇
- en: In this section, we review the code where we get the predictions for each label
    and save them in a text file and draw a two-dimensional scatter plot.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾代码，其中我们获取每个标签的预测并将它们保存在文本文件中，并绘制二维散点图。
- en: 'We will create two scatter plots, one for users and the other for items (which
    is movies in this case):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建两个散点图，一个用于用户，另一个用于项目（在这种情况下是电影）：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/image_08_007.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_007.png)'
- en: K-means clustering with user data
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 具有用户数据的K均值聚类
- en: The preceding figure shows K-means clusters for user data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了用户数据的K均值聚类。
- en: '![](img/image_08_008.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_008.png)'
- en: K-means clustering plot with item data
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 具有项目数据的K均值聚类图
- en: The preceding figure shows K-means clusters for item data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了项目数据的K均值聚类。
- en: '![](img/image_08_009.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_009.png)'
- en: K-means plotting effect of number of clusters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: K均值绘制聚类数的效果
- en: The preceding figure shows K-means clusters for user data with two features
    and one iteration.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了具有两个特征和一个迭代的用户数据的K均值聚类。
- en: '![](img/image_08_010.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_010.png)'
- en: The preceding figure shows K-means clusters for user data with two features
    and 10 iterations. Notice how the cluster boundaries are moving.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了具有两个特征和10次迭代的用户数据的K均值聚类。请注意聚类边界的移动。
- en: '![](img/image_08_011.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_011.png)'
- en: The preceding figure shows K-means clusters for user data with two features
    and 10 iterations. Notice how cluster boundaries are moving.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了具有两个特征和10次迭代的用户数据的K均值聚类。请注意聚类边界的移动。
- en: K-means - evaluating the performance of clustering models
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K均值-评估聚类模型的性能
- en: With models such as regression, classification, and recommendation engines,
    there are many evaluation metrics that can be applied to clustering models to
    analyze their performance and the goodness of the clustering of the data points.
    Clustering evaluation is generally divided into either internal or external evaluation.
    Internal evaluation refers to the case where the same data used to train the model
    is used for evaluation. External evaluation refers to using data external to the
    training data for evaluation purposes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用回归、分类和推荐引擎等模型，可以应用许多评估指标来分析聚类模型的性能和数据点的聚类好坏。聚类评估通常分为内部评估和外部评估。内部评估是指使用相同的数据来训练模型和进行评估的情况。外部评估是指使用训练数据之外的数据进行评估。
- en: Internal evaluation metrics
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内部评估指标
- en: Common internal evaluation metrics include the WCSS we covered earlier (which
    is exactly the k-means objective function), the Davies-Bouldin Index, the Dunn
    Index, and the silhouette coefficient. All these measures tend to reward clusters
    where elements within a cluster are relatively close together, while elements
    in different clusters are relatively far away from each other.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的内部评估指标包括我们之前介绍的WCSS（这恰好是K均值的目标函数）、Davies-Bouldin指数、Dunn指数和轮廓系数。所有这些指标都倾向于奖励集群，其中集群内的元素相对较近，而不同集群中的元素相对较远。
- en: The Wikipedia page on clustering evaluation at [http://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation](http://en.wikipedia.org/wiki/Cluster_analysis)
    has more details.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类评估的维基百科页面[http://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation](http://en.wikipedia.org/wiki/Cluster_analysis)有更多细节。
- en: External evaluation metrics
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部评估指标
- en: Since clustering can be thought of as unsupervised classification, if we have
    some form of labeled (or partially labeled) data available, we could use these
    labels to evaluate a clustering model. We can make predictions of clusters (that
    is, the class labels) using the model and evaluate the predictions against the
    true labels using metrics similar to some that we saw for classification evaluation
    (that is, based on true positive and negative and false positive and negative
    rates).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由于聚类可以被视为无监督分类，如果有某种标记（或部分标记）的数据可用，我们可以使用这些标签来评估聚类模型。我们可以使用模型对集群（即类标签）进行预测，并使用类似于分类评估的指标来评估预测（即基于真正和假负率）。
- en: These include the Rand measure, F-measure, Jaccard index, and others.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包括Rand指标、F指标、Jaccard指数等。
- en: See [http://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation](http://en.wikipedia.org/wiki/Cluster_analysis)
    for more information on external evaluation for clustering.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 有关聚类外部评估的更多信息，请参见[http://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation](http://en.wikipedia.org/wiki/Cluster_analysis)。
- en: Computing performance metrics on the MovieLens dataset
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MovieLens数据集上计算性能指标
- en: 'Spark ML provides a convenient `computeCost` function to compute the WSSS objective
    function given a DataFrame. We will compute this metric for the following item
    and user training data:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML提供了一个方便的`computeCost`函数，用于计算给定DataFrame的WSSS目标函数。我们将为以下项目和用户训练数据计算此指标：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This should output the result similar to the following one:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出类似于以下结果：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The best way to measure effectiveness of WSSSE is to plot against iterations
    as shown in the next section.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量WSSSE有效性的最佳方法是如下部分所示的迭代次数。
- en: Effect of iterations on WSSSE
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代对WSSSE的影响
- en: Let us find out the effect of iterations on WSSSE for the MovieLens dataset.
    We will calculate WSSSE for various values of iterations and plot the output.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找出迭代对MovieLens数据集的WSSSE的影响。我们将计算各种迭代值的WSSSE并绘制输出。
- en: 'The code listing is:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单如下：
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE24]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let us plot these numbers to get a better idea:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这些数字以更好地了解：
- en: '![](img/image_08_012.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_012.png)'
- en: Users WSSSE versus iterations
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 用户WSSSE与迭代次数
- en: '![](img/image_08_013.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_013.png)'
- en: Items WSSSE versus iterations
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 项目WSSSE与迭代次数
- en: Bisecting KMeans
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二分KMeans
- en: It is a variation of generic KMeans.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通用KMeans的变体。
- en: 'Reference: [http://www.siam.org/meetings/sdm01/pdf/sdm01_05.pdf](http://www.siam.org/meetings/sdm01/pdf/sdm01_05.pdf)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 参考：[http://www.siam.org/meetings/sdm01/pdf/sdm01_05.pdf](http://www.siam.org/meetings/sdm01/pdf/sdm01_05.pdf)
- en: 'The steps of the algorithm are:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的步骤是：
- en: 'Initialize by randomly selecting a point, say ![](img/B05184_08_x11.png) then
    compute the centroid *w* of *M* and compute:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过随机选择一个点，比如 ![](img/B05184_08_x11.png) 然后计算*M*的质心*w*并计算：
- en: '*![](img/B05184_08_x2.png)*'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/B05184_08_x2.png)*'
- en: The **centroid** is the center of the cluster. A centroid is a vector containing
    one number for each variable, where each number is the mean of a variable for
    the observations in that cluster.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**质心**是聚类的中心。质心是一个包含每个变量的一个数字的向量，其中每个数字是该聚类中观察值的平均值。'
- en: 'Divide *M =[x1, x2, ... xn]* into two, sub-clusters *M[L]* and *M[R]*, according
    to the following rule:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*M =[x1, x2, ... xn]*分成两个子聚类*M[L]*和*M[R]*，根据以下规则：
- en: '![](img/B05184_08_x3.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05184_08_x3.png)'
- en: Compute the centroids of *M[L]* and *M[R]*, *w[L]* and *w[R]*, as in step 2.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算*M[L]*和*M[R]*的质心*w[L]*和*w[R]*，如第2步所示。
- en: If *w[L] = c[L]* and *w[R] = c[R]*, stop.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *w[L] = c[L]* 和 *w[R] = c[R]*，则停止。
- en: Otherwise, let c[L]= w[L] c[R] = w[R] , go to step 2.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，让c[L]= w[L] c[R] = w[R]，转到第2步。
- en: Bisecting K-means - training a clustering model
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二分K均值-训练聚类模型
- en: 'Training for bisecting K-means in Spark ML involves taking an approach similar
    to the other models -- we pass a DataFrame that contains our training data to
    the fit method of the `KMeans` object. Note that here we use the libsvm data format:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark ML中进行二分K均值的训练涉及采用类似其他模型的方法--我们将包含训练数据的DataFrame传递给`KMeans`对象的fit方法。请注意，这里我们使用libsvm数据格式：
- en: 'Instantiate the cluster object:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化聚类对象：
- en: '[PRE25]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the command `show(3)` is shown here:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 命令`show(3)`的输出如下所示：
- en: '[PRE26]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create the `BisectingKMeans` object and set the parameters:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`BisectingKMeans`对象并设置参数：
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Train the data:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数据：
- en: '[PRE28]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE29]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Show movies by cluster:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按聚类显示电影：
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let us print the movies segregated by cluster number:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照聚类编号打印电影：
- en: '[PRE32]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE33]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let us calculate the WSSSE:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算WSSSE：
- en: '[PRE34]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE35]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next we run the predictions for the items:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们对物品进行预测：
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Source code:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码：
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_08/scala/2.0.0/src/main/scala/org/sparksamples/kmeans/BisectingKMeans.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_08/scala/2.0.0/src/main/scala/org/sparksamples/kmeans/BisectingKMeans.scala)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_08/scala/2.0.0/src/main/scala/org/sparksamples/kmeans/BisectingKMeans.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_08/scala/2.0.0/src/main/scala/org/sparksamples/kmeans/BisectingKMeans.scala)'
- en: Plot the user and item clusters.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制用户和物品聚类。
- en: 'As a next step, let us take two features and plot the user and item clusters
    with their respective clusters:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们选择两个特征，并绘制用户和物品聚类及其各自的聚类：
- en: '[PRE37]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/image_08_014.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_014.png)'
- en: Plotting of MovieLens user data with clusters
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 用聚类绘制MovieLens用户数据
- en: The preceding plot shows what user clusters look like for two features.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了两个特征的用户聚类的样子。
- en: '![](img/image_08_015.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_015.png)'
- en: Plotting of MovieLens item (movies) data with clusters
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 用聚类绘制MovieLens物品（电影）数据
- en: The preceding plot shows what item clusters look like for two features.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了两个特征的物品聚类的样子。
- en: WSSSE and iterations
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WSSSE和迭代
- en: In this section, we evaluate the effect or iterations on WSSSE for bisecting
    the K-means algorithm.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了对K均值算法进行二分时迭代次数对WSSSE的影响。
- en: 'The source code is:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码是：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/image_08_016.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_016.png)'
- en: 'Plot: WSSSE versus iterations for users'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图：用户迭代的WSSSE
- en: '![](img/image_08_017.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_017.png)'
- en: 'Plot: WSSSE versus iterations for items in the case of bisecting K-means'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图：在二分K均值情况下物品的WSSSE与迭代次数
- en: It is clear that the algorithm reaches optimal WSSSE by 20 iterations for both
    users and items.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，该算法在20次迭代后对用户和物品都达到了最佳的WSSSE。
- en: Gaussian Mixture Model
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: A mixture model is a probabilistic model of a sub-population within a population.
    These models are used to make statistical inferences about a sub-population, given
    the observations of pooled populations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型是一个人口中子群体的概率模型。这些模型用于对子群体的统计推断，给定汇总人口的观察结果。
- en: A **Gaussian Mixture Model** (**GMM**) is a mixture model represented as a weighted
    sum of Gaussian component densities. Its model coefficients are estimated from
    training data using the iterative **Expectation-Maximization** (**EM**) algorithm
    or **Maximum A Posteriori** (**MAP**) estimation from a trained model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯混合模型**（**GMM**）是一个以高斯分量密度的加权和表示的混合模型。它的模型系数是使用迭代的**期望最大化**（**EM**）算法或从训练模型的**最大后验**（**MAP**）估计的。'
- en: The `spark.ml` implementation uses the EM algorithm.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.ml`的实现使用EM算法。'
- en: 'It has the following parameters:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有以下参数：
- en: '**k**: Number of desired clusters'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k**：期望的聚类数量'
- en: '**convergenceTol**: Maximum change in log-likelihood at which one considers
    convergence achieved'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**convergenceTol**：在认为收敛达到的对数似然的最大变化'
- en: '**maxIterations**: Maximum number of iterations to perform without reaching
    convergence'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**maxIterations**：执行而不收敛的最大迭代次数'
- en: '**initialModel**: Optional starting point from which to start the EM algorithm'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**initialModel**：可选的起始点，从这里开始EM算法'
- en: (if this parameter is omitted, a random starting point will be constructed from
    the data)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: （如果省略此参数，将从数据中构造一个随机起始点）
- en: Clustering using GMM
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GMM进行聚类
- en: We will create clusters for both users and items (movies in this case) to get
    a better understanding of how the algorithm groups users and items.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为用户和物品（在这种情况下是电影）创建聚类，以更好地了解算法如何对用户和物品进行分组。
- en: 'Perform the following steps:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: Load the `libsvm` file for users.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载用户的`libsvm`文件。
- en: 'Create a Gaussian Mixture instance. The instance has the following parameters
    which can be configured:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个高斯混合实例。该实例具有以下可配置的参数：
- en: '[PRE39]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In our case, we will be setting only the number of Gaussian distributions and
    seed number:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们将只设置高斯分布的数量和种子数：
- en: '[PRE40]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create a user model:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个用户模型：
- en: '[PRE41]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The full code listing is:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码清单是：
- en: '[PRE42]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Plotting the user and item data with GMM clustering
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用GMM聚类绘制用户和物品数据
- en: 'In this section, we look at how GMM-based cluster boundaries move as the number
    of iterations increase:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看一下基于GMM的聚类边界随着迭代次数的增加而移动：
- en: '![](img/image_08_018.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_018.png)'
- en: Plot of MovieLens user data clusters assigned by GMM
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: MovieLens用户数据通过GMM分配的聚类图
- en: '![](img/image_08_019.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_019.png)'
- en: Plot of MovieLens item data clusters assigned by GMM
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: MovieLens项目数据通过GMM分配的聚类图
- en: GMM - effect of iterations on cluster boundaries
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GMM - 迭代次数对聚类边界的影响
- en: 'Let us look at how cluster boundaries change as the number of iterations increase
    for GMM:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下随着GMM迭代次数的增加，聚类边界如何变化：
- en: '![](img/image_08_020.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_020.png)'
- en: Cluster plot for GMM for user data with one iteration
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一次迭代的用户数据的GMM聚类图
- en: The preceding figure plots GMM clusters for user data with one iteration.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了使用一次迭代的用户数据的GMM聚类。
- en: '![](img/image_08_021.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_021.png)'
- en: Cluster plot for GMM for user data with 10 iterations
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用10次迭代的用户数据的GMM聚类图
- en: The preceding figure plots GMM clusters for user data with 10 iterations.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了使用10次迭代的用户数据的GMM聚类。
- en: '![](img/image_08_022.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_022.png)'
- en: Cluster plot for GMM for user data with 20 iterations
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 使用20次迭代的用户数据的GMM聚类图
- en: The preceding figure plots GMM clusters for user data with 20 iterations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了使用20次迭代的用户数据的GMM聚类。
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored a new class of model that learns structures from
    unlabeled data -- unsupervised learning. We worked through the required input
    data and feature extraction, and saw how to use the output of one model (a recommendation
    model in our example) as the input to another model (our k-means clustering model).
    Finally, we evaluated the performance of the clustering model, using both manual
    interpretation of the cluster assignments and using mathematical performance metrics.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了一种从未标记数据中学习结构的新模型类别 -- 无监督学习。我们通过所需的输入数据和特征提取进行了工作，并看到如何使用一个模型的输出（在我们的例子中是推荐模型）作为另一个模型的输入（我们的k-means聚类模型）。最后，我们评估了聚类模型的性能，既使用了对聚类分配的手动解释，也使用了数学性能指标。
- en: In the next chapter, we will cover another type of unsupervised learning used
    to reduce our data down to its most important features or components -- dimensionality
    reduction models.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍另一种无监督学习方法，用于将数据减少到其最重要的特征或组件 -- 降维模型。
