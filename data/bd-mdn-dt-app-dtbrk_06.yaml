- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Managing Data Locations in Unity Catalog
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Unity Catalog 中管理数据位置
- en: 'In this chapter, we’ll explore how to effectively manage data storage locations
    using securable objects in **Unity Catalog** – objects that allow administrators
    to grant fine-grained permissions to users, groups, and service principals . We’ll
    cover six types of securable objects for storing data in Unity Catalog: catalogs,
    schemas, tables, volumes, external locations, and connections. We’ll also look
    at how you can effectively govern storage access across various roles and departments
    within your organization, ensuring data security and auditability within the Databricks
    Data Intelligence Platform. Lastly, we’ll outline how to organize and structure
    data across different storage locations within Unity Catalog.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何使用**Unity Catalog**中的可安全管理对象有效地管理数据存储位置——这些对象允许管理员授予用户、群组和服务主体细粒度的权限。我们将介绍六种在
    Unity Catalog 中存储数据的可安全管理对象：目录、模式、表、卷、外部位置和连接。我们还将探讨如何有效地管理组织内不同角色和部门的数据存储访问，确保在
    Databricks 数据智能平台内的数据安全性和可审计性。最后，我们将概述如何在 Unity Catalog 中组织和结构化不同存储位置的数据。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Creating and managing data catalogs in Unity Catalog
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Unity Catalog 中创建和管理数据目录
- en: Setting default storage locations for data within Unity Catalog
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 Unity Catalog 中数据的默认存储位置
- en: Creating and managing external storage locations in Unity Catalog
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Unity Catalog 中创建和管理外部存储位置
- en: Hands-on lab – extracting document text for a generative AI pipeline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践实验 – 为生成性 AI 流水线提取文档文本
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To follow along with the examples provided in this chapter, you’ll need Databricks
    workspace permissions to create and start an all-purpose cluster so that you can
    import and execute the chapter’s accompanying notebooks. All code samples can
    be downloaded from this chapter’s GitHub repository located at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter06](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter06)
    . It’s also recommended that your Databricks user be elevated to a metastore admin
    (covered in [*Chapter 5*](B22011_05.xhtml#_idTextAnchor126) ) so that you can
    add and remove external locations, security credentials, foreign connections,
    and bind catalogs to a Databricks workspace. This chapter will create and run
    several new notebooks, estimated to consume around 5-10 **Databricks** **Units**
    ( **DBUs** ).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本章提供的示例，您需要具备 Databricks 工作区的权限，以便创建并启动一个通用集群，这样您就可以导入并执行本章附带的笔记本。所有代码示例可以从本章的
    GitHub 仓库下载，地址为 [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter06](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter06)。我们还建议将您的
    Databricks 用户提升为元存储管理员（参见[*第 5 章*](B22011_05.xhtml#_idTextAnchor126)），这样您就可以添加和删除外部位置、安全凭证、外部连接，并将目录绑定到
    Databricks 工作区。本章将创建并运行多个新的笔记本，预计消耗约 5-10 **Databricks** **单位** (**DBUs**)。
- en: Creating and managing data catalogs in Unity Catalog
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Unity Catalog 中创建和管理数据目录
- en: A **catalog** is the topmost container in the Unity Catalog object model hierarchy
    for storing data assets. A catalog will contain one or more schemas (or databases),
    which can contain one or many tables, views, models, functions, or volumes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**目录**是 Unity Catalog 对象模型层次结构中用于存储数据资产的最顶层容器。一个目录将包含一个或多个模式（或数据库），这些模式可以包含一个或多个表、视图、模型、函数或卷。'
- en: '![Figure 6.1 – Data is isolated in Unity Catalog using catalogs](img/B22011_06_1.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 数据通过目录在 Unity Catalog 中进行隔离](img/B22011_06_1.jpg)'
- en: Figure 6.1 – Data is isolated in Unity Catalog using catalogs
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 数据通过目录在 Unity Catalog 中进行隔离
- en: A common question is “How many catalogs should my workspace have?” While there
    is no right or wrong answer to the exact number of catalogs one should create
    for their workspace, a good rule of thumb would be to break your workspace catalogs
    up by natural dividing factors such as lines of business, logical work environments,
    teams, or use cases, for example. Furthermore, you should consider the subset
    of groups and users who will have permission to use the data assets as a factor
    in deciding how to create your catalog isolation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的问题是：“我的工作区应该拥有多少个目录？”虽然没有准确的标准来回答应该为工作区创建多少个目录，但一个好的经验法则是根据自然的划分因素来划分工作区目录，例如业务线、逻辑工作环境、团队或使用案例等。此外，您还应考虑哪些组和用户将拥有使用数据资产的权限，这也是决定如何创建目录隔离时的一个因素。
- en: Important note
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is a best practice to not have too few catalogs where you cannot logically
    divide datasets from one another. Similarly, it’s also a best practice to not
    have too many catalogs within a workspace as it makes it difficult for users to
    navigate and discover datasets. You should aim to find somewhere in between.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是避免创建过少的目录，以至于无法合理地将数据集相互区分。同样，也应该避免在一个工作区内创建过多目录，因为这会让用户在浏览和发现数据集时感到困难。目标应该是找到两者之间的平衡。
- en: 'Metastore administrators, or privileged users within Unity Catalog, can grant
    other users the entitlement to create additional catalogs within a metastore.
    For example, the following grant statement executed by a metastore administrator
    will grant the Databricks user, **jane.smith@example.com** , permission to create
    new catalogs within the metastore attached to their Databricks workspace:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据存储管理员或Unity Catalog中的特权用户，可以授予其他用户在元数据存储中创建附加目录的权限。例如，以下由元数据存储管理员执行的授权语句将授予Databricks用户**jane.smith@example.com**在其Databricks工作区附加的元数据存储中创建新目录的权限：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Furthermore, for Databricks workspaces created after November 8th, 2023, a default
    workspace catalog is created within the Unity Catalog metastore, **<workspace_name>_catalog**
    . By default, all users in the workspace will have access to this catalog and
    can create data assets.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于2023年11月8日之后创建的Databricks工作区，会在Unity Catalog元数据存储中创建一个默认工作区目录**<workspace_name>_catalog**。默认情况下，工作区中的所有用户都可以访问此目录并创建数据资产。
- en: Managed data versus external data
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理数据与外部数据
- en: When you deploy a Unity Catalog metastore, part of the deployment process includes
    setting up a new, default cloud storage container at the metastore level. This
    cloud storage container serves as the *default* location for all data assets created
    on the Databricks Data Intelligence Platform. For example, when a user creates
    a new table and they do not specify a **LOCATION** attribute in the **data definition
    language** ( **DDL** ) statement, then the Databricks Data Intelligence Platform
    will store the table data in the default storage container. As a result, the platform
    will take care of managing the life cycle of this table, including the data files,
    metadata, and even characteristics of the table, such as tuning the table layout
    and file sizes. This type of data asset is referred to as a *managed* table because
    the Databricks Data Intelligence Platform will manage the life cycle. Furthermore,
    if the table is dropped, the platform will take care of removing all the table
    metadata and data files.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当您部署Unity Catalog元数据存储时，部署过程的一部分是设置一个新的默认云存储容器，位于元数据存储级别。这个云存储容器作为Databricks数据智能平台上创建的所有数据资产的*默认*位置。例如，当用户创建一个新表时，如果他们没有在**数据定义语言**（**DDL**）语句中指定**LOCATION**属性，Databricks数据智能平台将把表数据存储在默认存储容器中。因此，平台会负责管理此表的生命周期，包括数据文件、元数据，甚至表的特性，如调整表布局和文件大小。这种数据资产被称为*托管*表，因为Databricks数据智能平台会管理其生命周期。此外，如果该表被删除，平台将负责移除所有表的元数据和数据文件。
- en: However, if the user provides a **LOCATION** attribute in the DDL statement,
    they will override the default behavior. Instead, the user is explicitly directing
    the Databricks Data Intelligence Platform to store the data in a location external
    to the default storage container for Unity Catalog. As a result, this type of
    data asset is referred to as an *external* table. Databricks will not manage the
    performance characteristics of the table, such as the size of the files or the
    layout of the files. Unlike managed tables, if an external table is dropped, only
    the entry of the table is removed from Unity Catalog and none of the table metadata
    and data files will be removed from their external location. Instead, the table
    owner will need to take care of deleting the table files from the cloud location,
    since they’ve taken over managing the table life cycle.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果用户在 DDL 语句中提供了**LOCATION**属性，则会覆盖默认行为。相反，用户明确指示 Databricks 数据智能平台将数据存储在
    Unity Catalog 默认存储容器之外的位置。因此，这种类型的数据资产被称为*外部*表。Databricks 不会管理表的性能特性，例如文件的大小或文件的布局。与托管表不同，如果删除外部表，仅会从
    Unity Catalog 中移除表的条目，而表的元数据和数据文件不会从其外部位置中删除。相反，表的所有者需要负责从云位置删除表文件，因为他们已经接管了表生命周期的管理。
- en: Generally speaking, *managed* refers to the Databricks platform managing the
    life cycle and data will be stored in the default storage container, while on
    the other hand, *external* means that the object owner is taking control of the
    object life cycle and the data should be stored in an external storage location.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，*托管*表示 Databricks 平台管理生命周期，数据将存储在默认存储容器中；而*外部*意味着对象所有者控制对象生命周期，数据应该存储在外部存储位置。
- en: In fact, there may be good reasons when you wish to create data assets in a
    different storage location than metastore default location. For example, for privileged
    datasets containing sensitive data, such as **personally identifiable information**
    ( **PII** ) / **protected health information** ( **PHI** ) data, you may wish
    to store these datasets in a separate storage account. Or perhaps you have a contractual
    obligation that requires data to be stored separately in an isolated storage account.
    In any event, it’s quite common to have requirements for data isolation. In the
    next section, let’s look at another securable object in Unity Catalog that allows
    data admins to securely store arbitrary types of data while maintaining strong
    isolation from traditional tables and views.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，您可能有充分的理由希望将数据资产存储在不同于元数据存储（metastore）默认位置的位置。例如，对于包含敏感数据的特权数据集，如**个人身份信息**（**PII**）/**受保护的健康信息**（**PHI**）数据，您可能希望将这些数据集存储在单独的存储账户中。或者，您可能有合同义务要求数据必须单独存储在隔离的存储账户中。无论如何，数据隔离的需求是很常见的。在下一节中，我们将介绍
    Unity Catalog 中的另一个可安全管理的对象，允许数据管理员在保持与传统表和视图的强隔离的同时，安全地存储任意类型的数据。
- en: Saving data to storage volumes in Unity Catalog
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据保存到 Unity Catalog 中的存储卷
- en: A **volume** , short for a **storage volume** , can be used to store files of
    various format types. Furthermore, volumes can be stored alongside tables and
    views in a schema in Unity Catalog. While tables and views are used to store structured
    data, volumes can be used to store structured, semi-structured, or unstructured
    data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷**，即**存储卷**，可用于存储各种格式类型的文件。此外，卷可以与表和视图一起存储在 Unity Catalog 中的模式下。表和视图用于存储结构化数据，而卷可以用来存储结构化、半结构化或非结构化数据。'
- en: '![Figure 6.2 – Storage volumes are stored alongside tables and views within
    a schema in Unity Catalog](img/B22011_06_2.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 存储卷与表和视图一起存储在 Unity Catalog 中的模式中](img/B22011_06_2.jpg)'
- en: Figure 6.2 – Storage volumes are stored alongside tables and views within a
    schema in Unity Catalog
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 存储卷与表和视图一起存储在 Unity Catalog 中的模式中
- en: Volumes can be managed by the Databricks Data Intelligence Platform, where,
    once dropped, the storage container, including the entire contents of the storage
    container, is removed entirely. On the other hand, volumes can be external volumes,
    meaning the volume owner manages the storage location of the storage volume, and
    once dropped, the contents of the storage container are not removed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 卷可以由 Databricks 数据智能平台管理，一旦删除，存储容器及其所有内容将被完全移除。另一方面，卷可以是外部卷，意味着卷的所有者管理存储卷的存储位置，一旦删除，存储容器的内容不会被移除。
- en: Storage volumes simplify the storage of files in Unity Catalog by removing the
    overhead of creating and managing external storage locations and storage credential
    objects within Unity Catalog. Whereas, external locations would need to be created
    with an accompanying storage credential, making provisioning and deprovisioning
    slightly more complex.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 存储卷通过消除在 Unity Catalog 中创建和管理外部存储位置及存储凭证对象的开销，简化了文件的存储。而外部位置则需要创建一个伴随的存储凭证，这使得配置和撤销配置稍微复杂一些。
- en: Storage volumes provide users of a particular schema the flexibility of storing
    arbitrary files in a safe and secure storage location that is managed by the Databricks
    Data Intelligence Platform. By default, storage volumes will persist data in the
    default storage location of the parent schema. For example, if there was no storage
    location provided at the time of the schema creation, then the data in a storage
    volume will be stored in the default storage account for the Unity Catalog metastore.
    Whereas if the schemas were created with an explicit storage location, then by
    default the storage volume will store its contents in this cloud location.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 存储卷为特定模式的用户提供了将任意文件存储在由 Databricks 数据智能平台管理的安全存储位置中的灵活性。默认情况下，存储卷将在父模式的默认存储位置中持久化数据。例如，如果在创建模式时没有提供存储位置，则存储卷中的数据将存储在
    Unity Catalog 元数据存储的默认存储帐户中。而如果模式在创建时指定了明确的存储位置，则默认情况下，存储卷会将其内容存储在该云位置中。
- en: 'A metastore administrator or a privileged user with explicit permission to
    create a volume within a catalog can create or drop a volume. The following example
    grants explicit permission for a Databricks user to create volumes on the development
    catalog:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据存储管理员或具有明确权限的特权用户可以在目录中创建或删除存储卷。以下示例授予 Databricks 用户在开发目录中创建存储卷的明确权限：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A fully qualified volume path is constructed using **/Volumes/** followed by
    the catalog, schema, and volume names. For example, an arbitrary text file can
    be referenced using the following path:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 完全限定的存储卷路径是通过 **/Volumes/** 后跟目录、模式和卷名称来构建的。例如，可以使用以下路径引用一个任意文本文件：
- en: '**/** **Volumes/catalog_name/schema_name/volume_name/subdirectory_name/arbitrary_file.txt**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**/** **Volumes/catalog_name/schema_name/volume_name/subdirectory_name/arbitrary_file.txt**'
- en: In the previous examples, we’ve let the Databricks Data Intelligence Platform
    decide how data is stored using schemas, tables, views, and volumes. However,
    we can set a prescribed cloud location for certain securable objects as well.
    Let’s look at how we can control the storage location using several techniques
    in Unity Catalog.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们让 Databricks 数据智能平台决定如何使用模式、表格、视图和存储卷存储数据。然而，我们也可以为某些可安全控制的对象设置规定的云位置。让我们看看如何使用
    Unity Catalog 中的几种技术来控制存储位置。
- en: Setting default locations for data within Unity Catalog
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Unity Catalog 中数据的默认位置
- en: 'You can control the storage location of data using several techniques in Unity
    Catalog:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用多种技术来控制 Unity Catalog 中数据的存储位置：
- en: '**Default location at the catalog level:** When creating a new catalog, data
    administrators can prescribe a storage location. When creating a data asset, such
    as a table, and no location is specified, then the data will be stored in the
    catalog location.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目录级别的默认位置**：在创建新目录时，数据管理员可以指定存储位置。在创建数据资产时，例如创建表格，且未指定位置，则数据将存储在目录位置中。'
- en: '**Default location at the schema level** : Similarly, you can specify a default
    location at the schema level. The schema location will override any default location
    specified at the catalog level. When creating a data asset, such as a table, and
    no location is specified, then the data will be stored in the schema location.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式级别的默认位置**：类似地，您可以在模式级别指定默认位置。模式位置将覆盖在目录级别指定的任何默认位置。在创建数据资产时，例如创建表格，且未指定位置，则数据将存储在模式位置中。'
- en: '**An external location at the table level** : This is the finest-grained control
    data stewards have over their datasets. The table location will override any default
    location specified at either the catalog or the schema level.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格级别的外部位置**：这是数据管理者对其数据集的最精细粒度的控制。表格位置将覆盖在目录或模式级别指定的任何默认位置。'
- en: '**Volume location** : Closely related to external locations (covered in the
    *Creating and managing external storage locations in Unity Catalog* section),
    volumes allow control over where the table data gets stored in your cloud storage
    location.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷位置**：与外部位置密切相关（见《在 Unity Catalog 中创建和管理外部存储位置》部分），卷允许控制表数据存储在云存储位置的方式。'
- en: Isolating catalogs to specific workspaces
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将目录隔离到特定工作空间
- en: By default, when you create a catalog in Unity Catalog, the catalog will be
    available for metastore admins to grant permissions for users to access across
    *all* Databricks workspaces using that metastore. However, in certain scenarios,
    you may want to override this behavior and enforce stronger isolation of datasets
    residing within a particular catalog. For example, sensitive datasets may only
    be available for data pipeline processing in a production workspace but should
    not be available in lower environments such as a development workspace. A feature
    of Unity Catalog called **catalog binding** helps address this type of scenario.
    With catalog binding, catalog administrators, such as a metastore administrator
    or a catalog owner, can control which workspaces have access to a particular catalog.
    For Databricks workspaces that are not bound to a particular catalog, the catalog
    will not appear in the search results of the Catalog Explorer UI.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当您在 Unity Catalog 中创建目录时，该目录将对元存储管理员可见，允许他们授予用户在使用该元存储的*所有* Databricks
    工作空间中访问权限。然而，在某些情况下，您可能希望覆盖此行为，并强制实施更强的数据集隔离。例如，敏感数据集可能只允许在生产工作空间中用于数据管道处理，但不应在较低环境（如开发工作空间）中使用。Unity
    Catalog 的一个功能，称为**目录绑定**，有助于解决此类场景。通过目录绑定，目录管理员（如元存储管理员或目录所有者）可以控制哪些工作空间可以访问特定目录。对于那些未绑定到特定目录的
    Databricks 工作空间，该目录将不会出现在 Catalog Explorer UI 的搜索结果中。
- en: '![Figure 6.3 – Catalog binding allows data administrators to control data isolation
    and isolation levels per workspace](img/B22011_06_3.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 目录绑定允许数据管理员控制每个工作空间的数据隔离和隔离级别](img/B22011_06_3.jpg)'
- en: Figure 6.3 – Catalog binding allows data administrators to control data isolation
    and isolation levels per workspace
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 目录绑定允许数据管理员控制每个工作空间的数据隔离和隔离级别
- en: Furthermore, data administrators can prescribe the type of actions that are
    available to datasets bound to a particular workspace. For example, say that you
    want to limit the access to read-only for datasets residing within a catalog for
    a testing environment. Data administrators can change the binding settings of
    a catalog either from the UI, using the Catalog Explorer in the Databricks Data
    Intelligence Platform, or using automated tools such as Terraform or the REST
    API. Let’s look at an example of how we could leverage the Databricks REST API
    to bind our testing catalog, which contains PII data to our production workspace.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据管理员可以指定可对绑定到特定工作空间的数据集执行的操作类型。例如，假设您希望限制对测试环境中目录内数据集的只读访问。数据管理员可以通过 UI、在
    Databricks 数据智能平台中使用 Catalog Explorer，或者通过自动化工具（如 Terraform 或 REST API）更改目录的绑定设置。让我们来看一个示例，如何利用
    Databricks REST API 将包含 PII 数据的测试目录绑定到生产工作空间。
- en: 'First, let’s start off by updating the default settings of our catalog so that
    the catalog is not accessible from all workspaces that use our Unity Catalog metastore.
    By default, this attribute is set to **OPEN** , and we would like to isolate our
    catalog to a prescribed workspace only:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过更新目录的默认设置来开始，使得目录不再可以从所有使用我们 Unity Catalog 元存储的工作空间访问。默认情况下，该属性设置为**开放**，我们希望将目录隔离到指定的工作空间：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can also use the Catalog Explorer to verify that the isolation mode of our
    catalog has been updated with our previous request. From the Databricks workspace,
    navigate to the Catalog Explorer from the left sidebar menu. Next, type in the
    name of your catalog in the search box to filter the catalogs and click on the
    name of your catalog. From the Catalog Explorer UI, verify in the details that
    the checkbox titled **All workspaces have access** is no longer checked.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 Catalog Explorer 来验证我们的目录隔离模式是否已根据之前的请求进行了更新。从 Databricks 工作空间，导航到左侧菜单中的
    Catalog Explorer。接下来，在搜索框中输入目录名称以筛选目录，然后点击目录的名称。在 Catalog Explorer UI 中，验证详细信息，确保名为**所有工作空间均可访问**的复选框不再被选中。
- en: '![Figure 6.4 – Catalog binding information can be configured from the Databricks
    UI](img/B22011_06_4.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 目录绑定信息可以从 Databricks UI 配置](img/B22011_06_4.jpg)'
- en: Figure 6.4 – Catalog binding information can be configured from the Databricks
    UI
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 目录绑定信息可以通过 Databricks UI 配置
- en: Now that our catalog is no longer open for metastore administrators to grant
    access from all workspaces that use our Unity Catalog metastore, we want to bind
    the catalog to only the workspaces that we’d like users to have access to.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的目录不再对元存储管理员开放，以便他们从所有使用 Unity Catalog 元存储的工作区授予访问权限，我们希望将目录仅绑定到我们希望用户访问的工作区。
- en: 'In the next example, we’ll again use the Databricks REST API to allow data
    administrators in the production workspace to assign read-only access to the datasets
    in our catalog:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将再次使用 Databricks REST API，允许生产工作区的数据管理员为目录中的数据集分配只读访问权限：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Important note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In the preceding example, we provide the workspace identifier in the payload
    request for binding a catalog to a workspace in Unity Catalog. If you aren’t sure
    what your workspace identifier is, you can quickly find it by inspecting the URL
    of your Databricks workspace. The workspace identifier can be found in the first
    URI segment of the URL to your Databricks workspace and follows the **https://<workspace_name>.cloud.databricks.com/o=<workspace_id>**
    pattern. The workspace identifier will be the numerical value immediately following
    the **o=** URL parameter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们提供了工作区标识符，作为绑定 Unity Catalog 中目录到工作区的请求载荷。如果你不确定你的工作区标识符是什么，可以通过检查
    Databricks 工作区的 URL 快速找到它。工作区标识符可以在 URL 的第一个 URI 部分中找到，并遵循 **https://<workspace_name>.cloud.databricks.com/o=<workspace_id>**
    模式。工作区标识符是紧随 **o=** URL 参数后的数字值。
- en: By now, you should understand the impact that catalog binding has in allowing
    data administrators the ability to control how our data is accessible and further
    isolate datasets in the Databricks Data Intelligence Platform. However, there
    may be certain scenarios in which data administrators need to control the cloud
    storage location, such as meeting contractual obligations of no co-location of
    datasets during pipeline processing. In the next section, let’s look at how data
    administrators can assign specific cloud locations for datasets.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经了解目录绑定对数据管理员控制数据访问方式的重要影响，并进一步在 Databricks 数据智能平台中隔离数据集。然而，可能会有一些特定场景，数据管理员需要控制云存储位置，比如在管道处理过程中履行不将数据集共同存储的合同义务。在接下来的部分中，我们将讨论数据管理员如何为数据集分配特定的云存储位置。
- en: Creating and managing external storage locations in Unity Catalog
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Unity Catalog 中创建和管理外部存储位置
- en: 'One of the strong suits of Databricks is the openness of data, meaning users
    can connect to data stored in a variety of cloud-native storage systems. For example,
    users can connect to data stored in Amazon’s S3 service and join that data with
    another dataset stored in an **Azure Data Lake Storage** ( **ADLS** ) **Gen2**
    storage container. However, one of the downsides is that integration with these
    cloud-native storage services needs complex configuration settings to be set typically
    at the beginning of a notebook execution, or perhaps in an initialization script
    when a cluster starts up. These configuration settings are complex, and at the
    very minimum need to be stored in a Databricks secret, and authentication tokens
    would need to be rotated by cloud admins – a very complex maintenance life cycle
    for an otherwise simple task – loading remote data using Spark’s **DataFrameReader**
    . One of the key benefits that Unity Catalog brings is a securable object called
    a **storage credential** , which aims to simplify this maintenance task, while
    also allowing end users the ability to store and connect to datasets that are
    external to the Databricks Data Intelligence Platform. Cloud admins or metastore
    admins can store cloud service authentication details in a single place and save
    end users, who may not be technical, from having to configure complex details
    of cloud authentication, such as an IAM role identifier. As an example of how
    complex these configuration details can be, the following code snippet can be
    used to configure authentication to an ADLS Gen2 container using the configuration
    that gets set during the code execution:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 的一个强项是数据的开放性，这意味着用户可以连接到存储在各种云原生存储系统中的数据。例如，用户可以连接到存储在 Amazon 的 S3
    服务中的数据，并将这些数据与存储在 **Azure Data Lake Storage** (**ADLS**) **Gen2** 存储容器中的另一个数据集进行连接。然而，缺点之一是与这些云原生存储服务的集成需要复杂的配置设置，这些设置通常需要在笔记本执行开始时进行配置，或者可能在集群启动时通过初始化脚本进行配置。这些配置设置非常复杂，至少需要将其存储在
    Databricks 密钥中，并且身份验证令牌需要由云管理员轮换——对于一个本应简单的任务（使用 Spark 的 **DataFrameReader** 加载远程数据），这是一种非常复杂的维护生命周期。Unity
    Catalog 提供的一个关键好处是一个可安全管理的对象，称为 **存储凭证**，它旨在简化这一维护任务，同时允许最终用户存储和连接外部数据集，而这些数据集并不在
    Databricks 数据智能平台内。云管理员或元数据存储管理员可以将云服务认证详细信息存储在一个地方，从而避免最终用户（可能并不具备技术背景）需要配置云认证的复杂细节，比如
    IAM 角色标识符。为了举例说明这些配置细节可能有多复杂，以下代码片段可以用来配置使用在代码执行过程中设置的配置来进行对 ADLS Gen2 容器的认证：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Storing cloud service authentication using storage credentials
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用存储凭证存储云服务认证信息
- en: A **storage credential** is a securable object within Unity Catalog that abstracts
    away a cloud-native credential for access to a cloud storage account. For example,
    a storage credential may represent an **Identity and Access Management** ( **IAM**
    ) role on the **Amazon Web Services** ( **AWS** ) cloud. A storage credential
    may also represent a **managed identity** or service principal in the Azure cloud.
    Once a storage credential has been created, access to the storage credential can
    be granted to users and groups in Unity Catalog using an explicit grant statement.
    Like other securable objects in Unity Catalog, there are various methods for creating
    a new security credential on the Databricks Data Intelligence Platform. For example,
    a metastore admin may choose to use **American National Standards Institute Structured
    Query Language** ( **ANSI SQL** ) to create the storage credential, or they might
    use the Databricks UI.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**存储凭证** 是 Unity Catalog 中的一个可安全管理的对象，它将云原生凭证抽象化，用于访问云存储账户。例如，存储凭证可能代表 **亚马逊网络服务**（**AWS**）云中的
    **身份与访问管理**（**IAM**）角色。存储凭证也可能代表 Azure 云中的 **托管身份** 或服务主体。一旦创建了存储凭证，访问该存储凭证的权限可以通过显式授权语句授予
    Unity Catalog 中的用户和用户组。与 Unity Catalog 中的其他可安全管理对象类似，Databricks 数据智能平台提供了多种创建新安全凭证的方法。例如，元数据存储管理员可以选择使用
    **美国国家标准协会结构化查询语言**（**ANSI SQL**）来创建存储凭证，或者他们可能使用 Databricks UI。'
- en: '![Figure 6.5 – Storage credentials can be created using Databricks UI](img/B22011_06_5.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – 可以使用 Databricks UI 创建存储凭证](img/B22011_06_5.jpg)'
- en: Figure 6.5 – Storage credentials can be created using Databricks UI
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 可以使用 Databricks UI 创建存储凭证
- en: Storage credentials are paired with another securable object in Unity Catalog
    called an **external location** , and the combination is used to store and access
    data in a specific cloud storage account.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 存储凭证与 Unity Catalog 中的另一个可保护对象**外部位置**配对使用，这种组合用于在特定的云存储帐户中存储和访问数据。
- en: '![Figure 6.6 – A storage credential encapsulates a cloud identity and is used
    by Unity Catalog to access an external storage location](img/B22011_06_6.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 存储凭证封装了一个云身份，并由 Unity Catalog 用于访问外部存储位置](img/B22011_06_6.jpg)'
- en: Figure 6.6 – A storage credential encapsulates a cloud identity and is used
    by Unity Catalog to access an external storage location
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 存储凭证封装了一个云身份，并由 Unity Catalog 用于访问外部存储位置
- en: 'You must be either a Databricks account administrator or a metastore administrator
    for a Unity Catalog metastore, which will include the **CREATE STORAGE CREDENTIAL**
    entitlement. The following example uses the Databricks **command-line interface**
    ( **CLI** ) tool to create a new storage credential in Unity Catalog using an
    IAM role in AWS:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须是 Databricks 帐户管理员或 Unity Catalog 元数据存储的元数据存储管理员，且该角色包括**CREATE STORAGE CREDENTIAL**
    权限。以下示例使用 Databricks **命令行接口**（**CLI**）工具，通过 AWS 中的 IAM 角色在 Unity Catalog 中创建一个新的存储凭证：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s use the SQL API this time to grant permission to the **data-science**
    group to use the credential for accessing cloud storage:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们使用 SQL API 来授予**数据科学**小组使用凭证访问云存储的权限：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: However, storage containers external to the Databricks Data Intelligence Platform
    may not be the only source of data that you wish to connect to from your lakehouse.
    For instance, there may be scenarios in which you may need to connect to an external
    system, such as an existing data warehouse or a relational database, to cross-reference
    data. Let’s turn our attention to **Lakehouse Federation** , which allows lakehouse
    users to query datasets outside of the Databricks Data Intelligence Platform.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Databricks 数据智能平台外部的存储容器可能并不是您希望从湖仓连接的唯一数据源。例如，可能会有某些场景，您可能需要连接到外部系统，例如现有的数据仓库或关系型数据库，以交叉引用数据。让我们将注意力转向**Lakehouse
    Federation**，它允许湖仓用户查询 Databricks 数据智能平台之外的数据集。
- en: Querying external systems using Lakehouse Federation
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Lakehouse Federation 查询外部系统
- en: Lakehouse Federation is a feature in the Databricks Data Intelligence Platform
    that permits users to execute queries on storage systems external to Databricks
    without needing to migrate the data to the lakehouse. Another securable object
    in Unity Catalog, called a **connection** , can be used to federate queries to
    external systems. A connection represents a *read-only* connection to an external
    system, such as a **relational database management system** ( **RDBMS** ), such
    as Postgres or MySQL, or a cloud data warehouse such as Amazon Redshift. This
    is a great way to query external data to quickly prototype new pipelines in your
    lakehouse. Perhaps, even, you need to cross-reference an external dataset and
    don’t want to go through the lengthy process of creating another **extract, transform,
    and load** ( **ETL** ) pipeline to ingest a new data source just yet.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Lakehouse Federation 是 Databricks 数据智能平台中的一项功能，允许用户在不需要将数据迁移到湖仓的情况下，对外部存储系统执行查询。Unity
    Catalog 中的另一个可保护对象，称为**连接**，可以用来将查询联合到外部系统。连接代表与外部系统的*只读*连接，如**关系型数据库管理系统**（**RDBMS**），例如
    Postgres 或 MySQL，或是像 Amazon Redshift 这样的云数据仓库。这是一种快速查询外部数据的方法，可以在湖仓中快速原型化新的管道。也许，您甚至需要交叉引用一个外部数据集，但又不想立即经过创建另一个**提取、转换和加载**（**ETL**）管道的繁琐过程来引入新的数据源。
- en: A list of all connections can be easily viewed in the Databricks Data Intelligence
    Platform by navigating to the Catalog Explorer, expanding the **Connections**
    pane, and clicking a **Foreign Connection** to view the details about a previously
    created connection.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过导航到 Databricks 数据智能平台中的 Catalog Explorer，展开**连接**窗格，并点击**外部连接**来查看所有连接的列表，查看先前创建的连接的详细信息。
- en: 'Let’s look at an example of how we can use the SQL connection API in Databricks
    to create a new foreign connection to the MySQL database. Databricks recommends
    that all credential information be stored in a Databricks secret, which can be
    easily retrieved from SQL using the **secret()** SQL function and providing the
    secret scope and secret key:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，展示如何使用 Databricks 中的 SQL 连接 API 创建一个新的外部连接到 MySQL 数据库。Databricks 建议将所有凭据存储在
    Databricks 密钥库中，并通过 **secret()** SQL 函数轻松地从 SQL 中检索，提供密钥库范围和密钥：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, navigate to the **Connections** UI by clicking on the Catalog Explorer
    in the left-hand side navigation bar, expanding the **External Data** pane, and
    clicking on the **Connections** menu item. You should now see the newly created
    connection to the MySQL database and clicking on it will reveal details about
    the connection.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过点击左侧导航栏中的目录资源管理器，展开 **外部数据** 面板，点击 **连接** 菜单项，导航到 **连接** 界面。此时，你应该能看到新创建的
    MySQL 数据库连接，点击它将显示关于连接的详细信息。
- en: '![Figure 6.7 – Connections to foreign storage systems can be viewed from the
    Catalog Explorer](img/B22011_06_7.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 可以从目录资源管理器查看连接到外部存储系统的情况](img/B22011_06_7.jpg)'
- en: Figure 6.7 – Connections to foreign storage systems can be viewed from the Catalog
    Explorer
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 可以从目录资源管理器查看连接到外部存储系统的情况
- en: Let’s connect everything we’ve learned in the previous sections to build a modern
    data pipeline capable of powering generative AI use cases.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将之前各节所学内容结合起来，构建一个现代数据管道，以支持生成式 AI 的应用场景。
- en: Hands-on lab – extracting document text for a generative AI pipeline
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动手实验 – 提取文档文本以用于生成式 AI 管道
- en: In this example, we’ll look at a typical pipeline used to extract text from
    documents for the purposes of generative AI. This is a very common architectural
    pattern, especially for real-world use cases such as training a chatbot over a
    text corpus. Along the way, we’ll see how storage volumes on the Databricks Data
    Intelligence Platform are a great fit for processing arbitrary files from an external
    cloud storage location. All code samples can be downloaded from this chapter’s
    GitHub repository located at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter06](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter06)
    .
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将展示一个典型的管道，旨在提取文档中的文本，用于生成式 AI。这是一个非常常见的架构模式，特别适用于诸如训练聊天机器人等实际应用场景。在过程中，我们将看到
    Databricks 数据智能平台上的存储卷如何非常适合处理来自外部云存储位置的任意文件。所有代码示例可以从本章的 GitHub 仓库下载，地址为 [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter06](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter06)。
- en: Generating mock documents
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成虚拟文档
- en: The first step in a data pipeline will be a process for generating arbitrary
    text files to extract text from. Let’s begin by creating a new notebook in our
    Databricks workspace which will be used to train our organization’s chatbot. The
    following code example uses the popular **faker** Python library, to randomly
    generate the content within our documents, and the **reportlab** Python library,
    for generating PDF files.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道的第一步将是生成任意文本文件以提取文本的过程。让我们开始在 Databricks 工作区中创建一个新的笔记本，用于训练我们组织的聊天机器人。以下代码示例使用流行的
    **faker** Python 库来随机生成文档内容，并使用 **reportlab** Python 库生成 PDF 文件。
- en: 'Begin by installing the library dependencies in the first notebook cell using
    the **%pip** magic command:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用 **%pip** 魔法命令在第一个笔记本单元中安装库依赖项：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Defining helper functions
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义辅助函数
- en: 'Let’s define a few helper functions that will take a randomly generated paragraph
    of text and save the text as a document. We’ll define three helper functions –
    one helper function for each document format type – plain text, **Portable Document
    Format** ( **PDF** ), and **comma-separated** **values** ( **CSV** ):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义几个辅助函数，这些函数将接收随机生成的段落文本并将其保存为文档。我们将定义三个辅助函数——每个文档格式类型一个辅助函数——纯文本、**可移植文档格式**（**PDF**）和**逗号分隔**
    **值**（**CSV**）：
- en: 'Let’s define the helper function for a plain text file:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个纯文本文件的辅助函数：
- en: '[PRE9]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we define the helper function for a PDF file:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个用于 PDF 文件的辅助函数：
- en: '[PRE10]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Lastly, we define the helper function for a CSV file:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们定义一个用于 CSV 文件的辅助函数：
- en: '[PRE11]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is a great way to simulate a variety of documents you might expect your
    organization to accumulate over time.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种很好的方式来模拟你所在组织随着时间的推移可能会积累的各种文档。
- en: Choosing a file format randomly
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机选择文件格式
- en: 'Next, we will need to randomly choose the file format to save the generated
    documents. Let’s begin by importing the **faker** library and a few Python utility
    libraries that we’ll use to create unpredictable behavior. We’ll also define a
    few global variables that will be used to determine the characteristics of our
    randomly generated documents, such as the number of documents to generate, the
    number of sentences to generate per document, and the types of file formats to
    store the documents in. Add the following code snippet to the notebook:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要随机选择文件格式来保存生成的文档。首先，我们导入 **faker** 库和一些 Python 实用库，这些库将帮助我们创建不可预测的行为。我们还将定义一些全局变量，用于确定我们随机生成文档的特征，比如要生成的文档数量、每个文档生成的句子数量，以及存储文档的文件格式类型。将以下代码片段添加到笔记本中：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, let’s create a simple **for** loop that will serve as the backbone for
    our random document generator. Within the **for** loop, we’ll use the **faker**
    library to create a paragraph of random text having the number of sentences equal
    to the number set by our **num_sentences_per_doc** global variable:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个简单的 **for** 循环，作为我们随机文档生成器的主干。在 **for** 循环中，我们将使用 **faker** 库生成一段随机文本，文本中的句子数量等于由我们全局变量
    **num_sentences_per_doc** 设置的数量：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After the paragraph of random text has been generated, it’s time to choose
    what file format to store the text in. We’ll leverage the **random** Python library
    to randomly select a file format type from the list of file formats defined in
    the **doc_types** global variable. Add the following code snippet to the body
    of the for-loop:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成了一段随机文本后，就该选择以哪种文件格式存储文本了。我们将利用 **random** Python 库，从 **doc_types** 全局变量中定义的文件格式列表中随机选择一种文件格式类型。将以下代码片段添加到
    for 循环体内：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Lastly, we’ll add a sleep timer to simulate unpredictable peaks and lulls in
    the generation of text documents – something that you could expect in a typical
    production environment. Add the following code snippet to the bottom of the for-loop
    body:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将添加一个休眠定时器来模拟文本文档生成中的不可预测的高峰和低谷——这是在典型的生产环境中可能出现的现象。将以下代码片段添加到 for 循环体的底部：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You’ll also notice in the global variables section of the notebook, we’ve defined
    a volume path for our process to save the randomly generated documents:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本的全局变量部分，你还会注意到，我们已经为我们的过程定义了一个卷路径，用于保存随机生成的文档：
- en: '**volume_path =** **f"/Volumes/{catalog_name}/{schema_name}/{volume_name}"**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**volume_path =** **f"/Volumes/{catalog_name}/{schema_name}/{volume_name}"**'
- en: This is a convenient way to reference a cloud storage location as though it
    were a local storage path. Plus, we have all the benefits of strong data governance
    that come with a storage volume in Unity Catalog. For example, all the data is
    secured by default, and other users or processes cannot read these documents until
    we have permission to access the data in the storage volume. Finally, let’s attach
    the new notebook to a running all-purpose cluster in the Databricks Data Intelligence
    Platform and click the **Run all** button at the top of the notebook to begin
    generating and saving new documents to our storage volume location.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种方便的方式，可以将云存储位置引用为本地存储路径。而且，我们还享受 Unity Catalog 存储卷所带来的强大数据治理优势。例如，所有数据默认都是安全的，其他用户或进程在我们获得存储卷访问权限之前无法读取这些文档。最后，让我们将新笔记本附加到
    Databricks 数据智能平台上一个正在运行的通用集群，并点击笔记本顶部的 **Run all** 按钮，开始生成并保存新文档到我们的存储卷位置。
- en: Creating/assembling the DLT pipeline
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建/组装 DLT 管道
- en: 'Now that we’ve generated some text documents, let’s go ahead a create a new
    DLT pipeline that will stream the randomly generated documents and perform a simple
    text extraction. Import the notebook titled **Preprocess Text Documents.py** from
    the chapter’s GitHub repository into your Databricks workspace. You’ll notice
    that we define three new streaming tables, all of which are responsible for ingesting
    the randomly generated text, PDF, and CSV documents. After doing minimal preprocessing,
    the text field from each of these data sources is extracted and joined in a fourth
    table, **text_docs_silver** . This fourth table will serve as the input into our
    chatbot training:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经生成了一些文本文档，接下来让我们创建一个新的 DLT 管道，该管道将流式传输随机生成的文档并执行简单的文本提取。从本章的 GitHub 仓库中导入名为**Preprocess
    Text Documents.py**的笔记本到你的 Databricks 工作区。你会注意到我们定义了三个新的流式表，这些表负责摄取随机生成的文本、PDF
    和 CSV 文档。在进行最小的预处理后，这些数据源的文本字段被提取并在第四个表格**text_docs_silver**中进行合并。这个第四个表将作为我们聊天机器人训练的输入：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After attaching the notebook to a running cluster, you will be prompted to create
    a new DLT pipeline. Go ahead and create a brand new DLT pipeline (covered in [*Chapter
    2*](B22011_02.xhtml#_idTextAnchor052) ), titling the pipeline with a meaningful
    name, such as **doc_ingestion_pipeline** . Select **Triggered** for the processing
    mode and **Core** for the product edition, and accept the remaining defaults.
    Finally, click **Start** to begin an update execution of the newly created DLT
    pipeline.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在将笔记本连接到正在运行的集群后，系统将提示你创建一个新的 DLT 管道。继续创建一个全新的 DLT 管道（在[*第2章*](B22011_02.xhtml#_idTextAnchor052)中介绍），并为管道命名一个有意义的名称，例如**doc_ingestion_pipeline**。选择处理模式为**Triggered**，产品版本选择**Core**，其余选项保持默认设置。最后，点击**Start**以开始更新执行新创建的
    DLT 管道。
- en: '![Figure 6.8 – An overview of a DLT pipeline extracting text from arbitrary
    documents saved to a volume location in Unity Catalog](img/B22011_06_8_new.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – 一个 DLT 管道的概述，提取保存在 Unity Catalog 中的卷位置的任意文档的文本](img/B22011_06_8_new.jpg)'
- en: Figure 6.8 – An overview of a DLT pipeline extracting text from arbitrary documents
    saved to a volume location in Unity Catalog
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 一个 DLT 管道的概述，提取保存在 Unity Catalog 中的卷位置的任意文档的文本。
- en: You should see the DLT pipeline incrementally processing the randomly generated
    text documents, extracting the text from each of the different file types, and
    merging them into a consolidated dataset downstream. This is a simple, yet powerful
    example of how DLT can be combined with a storage volume in Unity Catalog to process
    arbitrary file formats in a real-world use case.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到 DLT 管道正在增量处理随机生成的文本文档，从每种不同的文件类型中提取文本，并将它们合并成一个下游的汇总数据集。这是一个简单而强大的例子，展示了如何将
    DLT 与 Unity Catalog 中的存储卷结合起来，在真实世界的使用案例中处理任意文件格式。
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered a variety of methods for storing data while also
    maintaining fine-grained access control using different securable objects in Unity
    Catalog. We covered how data could be stored using catalogs, schemas, tables,
    views, volumes, and external locations in Unity Catalog. We also saw how organizations
    could bind catalogs to individual Databricks workspaces to isolate datasets and
    even set the level of access to read-only. We covered the differences between
    managed datasets in the Databricks Data Intelligence Platform, as well as how
    we could set prescribed storage locations for storing data using catalogs, schemas,
    tables, volumes, and external locations. We covered how external data sources,
    such as data warehouses, could be queried in place without having to migrate the
    data using Lakehouse Federation. Lastly, we concluded with a hands-on exercise
    implementing the start of a generative AI pipeline for extracting text from documents
    using volumes in Unity Catalog.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了多种存储数据的方法，同时通过 Unity Catalog 中的不同可安全对象保持细粒度的访问控制。我们讲解了如何使用 Unity Catalog
    中的目录、模式、表、视图、卷和外部位置来存储数据。我们还看到组织如何将目录绑定到单个 Databricks 工作区，以隔离数据集，甚至设置只读的访问权限。我们讨论了
    Databricks 数据智能平台中受管数据集的区别，以及如何使用目录、模式、表、卷和外部位置设置存储位置来存储数据。我们介绍了如何使用 Lakehouse
    Federation 在不迁移数据的情况下直接查询外部数据源，如数据仓库。最后，我们通过一个实践操作，展示了如何实现生成式 AI 管道的开始，利用 Unity
    Catalog 中的卷提取文档中的文本。
- en: Now that we have a solid foundation for storing our data and other assets, in
    the next chapter, we’ll be covering tracking lineage across various objects in
    the Databricks Data Intelligence Platform.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为存储数据和其他资产打下了坚实的基础，在下一章中，我们将介绍如何在 Databricks 数据智能平台中跟踪各个对象的血统。
