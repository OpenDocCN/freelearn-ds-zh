- en: Avoiding Shuffle and Reducing Operational Expenses
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免洗牌和减少操作费用
- en: In this chapter, we will learn how to avoid shuffle and reduce the operational
    expense of our jobs, along with detecting a shuffle in a process. We will then
    test operations that cause a shuffle in Apache Spark to find out when we should
    be very careful and which operations we should avoid. Next, we will learn how
    to change the design of jobs with wide dependencies. After that, we will be using
    the `keyBy()` operations to reduce shuffle and, in the last section of this chapter,
    we'll see how we can use custom partitioning to reduce the shuffle of our data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何避免洗牌并减少我们作业的操作费用，以及检测过程中的洗牌。然后，我们将测试在Apache Spark中导致洗牌的操作，以找出我们何时应该非常小心以及我们应该避免哪些操作。接下来，我们将学习如何改变具有广泛依赖关系的作业设计。之后，我们将使用`keyBy()`操作来减少洗牌，在本章的最后一节中，我们将看到如何使用自定义分区来减少数据的洗牌。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Detecting a shuffle in a process
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测过程中的洗牌
- en: Testing operations that cause a shuffle in Apache Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Apache Spark中进行导致洗牌的测试操作
- en: Changing the design of jobs with wide dependencies
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变具有广泛依赖关系的作业设计
- en: Using `keyBy()` operations to reduce shuffle
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`keyBy()`操作来减少洗牌
- en: Using the custom partitioner to reduce shuffle
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义分区器来减少洗牌
- en: Detecting a shuffle in a process
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测过程中的洗牌
- en: In this section, we will learn how to detect a shuffle in a process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何检测过程中的洗牌。
- en: 'In this section, we will cover the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下主题：
- en: Loading randomly partitioned data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载随机分区的数据
- en: Issuing repartition using a meaningful partition key
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用有意义的分区键发出重新分区
- en: Understanding how shuffle occurs by explaining a query
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过解释查询来理解洗牌是如何发生的
- en: We will load randomly partitioned data to see how and where the data is loaded.
    Next, we will issue a partition using a meaningful partition key. We will then
    repartition data to the proper executors using the deterministic and meaningful
    key. In the end, we will explain our queries by using the `explain()` method and
    understand the shuffle. Here, we have a very simple test.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载随机分区的数据，以查看数据是如何加载的以及数据加载到了哪里。接下来，我们将使用有意义的分区键发出一个分区。然后，我们将使用确定性和有意义的键将数据重新分区到适当的执行程序。最后，我们将使用`explain()`方法解释我们的查询并理解洗牌。在这里，我们有一个非常简单的测试。
- en: 'We will create a DataFrame with some data. For example, we created an `InputRecord`
    with some random UID and `user_1`, and another input with random ID in `user_1`,
    and the last record for `user_2`. Let''s imagine that this data is loaded through
    the external data system. The data can be loaded from HDFS or from a database,
    such as Cassandra or NoSQL:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个带有一些数据的DataFrame。例如，我们创建了一个带有一些随机UID和`user_1`的`InputRecord`，以及另一个带有`user_1`中随机ID的输入，以及`user_2`的最后一条记录。假设这些数据是通过外部数据系统加载的。数据可以从HDFS加载，也可以从数据库加载，例如Cassandra或NoSQL：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the loaded data, there is no predefined or meaningful partitioning of our
    data, which means that the input record number 1 can end up in the executor first,
    and record number 2 can end up in the executor second. So, even though the data
    is from the same user, it is likely that we'll be executing operations for the
    specific user.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载的数据中，我们的数据没有预定义或有意义的分区，这意味着输入记录编号1可能会最先出现在执行程序中，而记录编号2可能会最先出现在执行程序中。因此，即使数据来自同一用户，我们也很可能会为特定用户执行操作。
- en: As discussed in the previous chapter, [Chapter 8](fd3ec6c0-4141-4e4a-8244-3f480201b2f6.xhtml),
    *Immutable Design*, we used the `reducebyKey()` method that was taking the user
    ID or specific ID to reduce all values for the specific key. This is a very common
    operation but with some random partitioning. It is good practice to `repartition`
    the data using a meaningful key.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章[第8章](fd3ec6c0-4141-4e4a-8244-3f480201b2f6.xhtml)中所讨论的，*不可变设计*，我们使用了`reducebyKey()`方法，该方法获取用户ID或特定ID以减少特定键的所有值。这是一个非常常见的操作，但具有一些随机分区。最好使用有意义的键`repartition`数据。
- en: 'While using `userID`, we will use `repartition` in a way that the result will
    record the data that has the same user ID. So `user_1`, for example, will end
    up on the first executor:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`userID`时，我们将使用`repartition`的方式，使结果记录具有相同用户ID的数据。因此，例如`user_1`最终将出现在第一个执行程序上：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first executor will have all the data for `userID`. If `InputRecord("1234-3456-1235-1234",
    "user_1")` is on executor 1 and `InputRecord("1123-3456-1235-1234", "user_1")` is
    on executor 2, after partitioning the data from executor 2, we will need to send
    it to executor 1, because it is a parent for this partition key. This causes a
    shuffle. A shuffle is caused by loading data that is not meaningfully partitioned,
    or not partitioned at all. We need to process our data so that we can perform
    operations on a specific key.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个执行程序将拥有所有`userID`的数据。如果`InputRecord("1234-3456-1235-1234", "user_1")`在执行程序1上，而`InputRecord("1123-3456-1235-1234",
    "user_1")`在执行程序2上，在对来自执行程序2的数据进行分区后，我们需要将其发送到执行程序1，因为它是此分区键的父级。这会导致洗牌。洗牌是由于加载数据而导致的，这些数据没有有意义地分区，或者根本没有分区。我们需要处理我们的数据，以便我们可以对特定键执行操作。
- en: 'We can `repartition` the data further, but it should be done at the beginning
    of our chain. Let''s start the test to explain our query:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步`repartition`数据，但应该在链的开头进行。让我们开始测试来解释我们的查询：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We are repartitioning the `userID` expression in a logical plan, but when we
    check the physical plan, it shows that a hash partition is used and that we will
    be hashing on the `userID` value. So, we scan all the RDDs and all the keys that
    have the same hash and are sent to the same executor to achieve our goal:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在逻辑计划中对`userID`表达式进行了重新分区，但当我们检查物理计划时，显示使用了哈希分区，并且我们将对`userID`值进行哈希处理。因此，我们扫描所有RDD和所有具有相同哈希的键，并将其发送到相同的执行程序以实现我们的目标：
- en: '![](img/d4014b30-3c14-4244-8bdd-4ed9875fa4d4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/d4014b30-3c14-4244-8bdd-4ed9875fa4d4.png)
- en: In the next section, we'll test operations that cause a shuffle in Apache Spark.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将测试在Apache Spark中导致洗牌的操作。
- en: Testing operations that cause a shuffle in Apache Spark
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Apache Spark中进行导致洗牌的测试操作
- en: 'In this section, we will test the operations that cause a shuffle in Apache
    Spark. We will cover the following topics:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将测试在Apache Spark中导致洗牌的操作。我们将涵盖以下主题：
- en: Using join for two DataFrames
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用join连接两个DataFrame
- en: Using two DataFrames that are partitioned differently
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分区不同的两个DataFrame
- en: Testing a join that causes a shuffle
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试导致洗牌的连接
- en: A join is a specific operation that causes shuffle, and we will use it to join
    our two DataFrames. We will first check whether it causes shuffle and then we
    will check how to avoid it. To understand this, we will use two DataFrames that
    are partitioned differently and check the operation of joining two datasets or
    DataFrames that are not partitioned or partitioned randomly. It will cause shuffle
    because there is no way to join two datasets with the same partition key if they
    are on different physical machines.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 连接是一种特定的操作，会导致洗牌，我们将使用它来连接我们的两个DataFrame。我们将首先检查它是否会导致洗牌，然后我们将检查如何避免它。为了理解这一点，我们将使用两个分区不同的DataFrame，并检查连接两个未分区或随机分区的数据集或DataFrame的操作。如果它们位于不同的物理机器上，将会导致洗牌，因为没有办法连接具有相同分区键的两个数据集。
- en: Before we join the dataset, we need to send them to the same physical machine.
    We will be using the following test.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们连接数据集之前，我们需要将它们发送到同一台物理机器上。我们将使用以下测试。
- en: 'We need to create `UserData`, which is a case class that we have seen already.
    It has the user ID and data. We have user IDs, that is, `user_1`, `user_2`, and
    `user_4`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建`UserData`，这是一个我们已经见过的案例类。它有用户ID和数据。我们有用户ID，即`user_1`，`user_2`和`user_4`：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then create some transactional data similar to a user ID (`user_1`, `user_2`,
    and `user_3`):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一些类似于用户ID（`user_1`，`user_2`和`user_3`）的交易数据：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We use the `joinWith` transaction on `UserData` by using the `userID` column
    from `UserData` and `transactionData`. Since we have issued an `inner` join, the
    result has two elements because there is a join between the record and the transaction,
    that is, `UserData`, and `UserTransaction`. However, `UserData` has no transaction
    and `Usertransaction` has no user data:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`joinWith`在`UserData`上的交易，使用`UserData`和`transactionData`的`userID`列。由于我们发出了`inner`连接，结果有两个元素，因为记录和交易之间有连接，即`UserData`和`UserTransaction`。但是，`UserData`没有交易，`Usertransaction`没有用户数据：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When we were joining the data, the data was not partitioned because this was
    some random data for Spark. It was unable to know that the user ID column is the
    partition key, as it cannot guess this. Since it is not pre-partitioned, to join
    the data from two datasets, will need to send data from the user ID to the executor.
    Hence, there will be a lot of data shuffling from the executor, which is because
    the data is not partitioned.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们连接数据时，数据没有分区，因为这是Spark的一些随机数据。它无法知道用户ID列是分区键，因为它无法猜测。由于它没有预分区，要连接来自两个数据集的数据，需要将数据从用户ID发送到执行器。因此，由于数据没有分区，将会有大量数据从执行器洗牌。
- en: 'Let''s explain the query, perform an assert, and show the results by starting
    the test:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释查询，执行断言，并通过启动测试显示结果：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can see our result as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的结果如下：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We have `[user_1,1]` and `[user_1,100]`, which is `userID` and `userTransaction`.
    It appears that the join worked properly, but let's look at that physical parameter.
    We had `SortMergeJoin` use `userID` for the first dataset and the second one,
    and then we used `Sort` and `hashPartitioning`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有`[user_1,1]`和`[user_1,100]`，即`userID`和`userTransaction`。看起来连接工作正常，但让我们看看物理参数。我们使用`SortMergeJoin`对第一个数据集和第二个数据集使用`userID`，然后我们使用`Sort`和`hashPartitioning`。
- en: In the previous section, *Detecting a shuffle in a process*, we used the `partition`
    method, which uses `hashPartitioning` underneath. Although we are using `join`,
    we still need to use hash partitioning because our data is not properly partitioned.
    So, we need to partition the first dataset as there will be a lot of shuffling,
    and then we need to do exactly the same thing for the second DataFrame. Again,
    the shuffling will be done twice, and once that data is partitioned on the joined
    field, the join could be local to the executor.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，*检测过程中的洗牌*，我们使用了`partition`方法，该方法在底层使用了`hashPartitioning`。虽然我们使用了`join`，但我们仍然需要使用哈希分区，因为我们的数据没有正确分区。因此，我们需要对第一个数据集进行分区，因为会有大量的洗牌，然后我们需要对第二个DataFrame做完全相同的事情。再次，洗牌将会进行两次，一旦数据根据连接字段进行分区，连接就可以在执行器本地进行。
- en: There will be an assertion of records after executing the physical plan, stating
    that the `userID` user data one is on the same executor as that of user transaction
    `userID` one. Without `hashPartitioning`, there is no guarantee and hence we need
    to do the partitioning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行物理计划后，将对记录进行断言，指出`userID`用户数据一与用户交易`userID`一位于同一执行器上。没有`hashPartitioning`，就没有保证，因此我们需要进行分区。
- en: In the next section, we'll learn how to change the design of jobs with wide
    dependencies, so we will see how to avoid unnecessary shuffling when performing
    a join on two datasets.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何更改具有广泛依赖的作业的设计，因此我们将看到如何在连接两个数据集时避免不必要的洗牌。
- en: Changing the design of jobs with wide dependencies
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改具有广泛依赖的作业的设计
- en: In this section, we will change the job that was performing the `join` on non-partitioned
    data. We'll be changing the design of jobs with wide dependencies.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更改在未分区数据上执行`join`的作业。我们将更改具有广泛依赖的作业的设计。
- en: 'In this section, we will cover the following topics:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下主题：
- en: Repartitioning DataFrames using a common partition key
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用公共分区键对DataFrame进行重新分区
- en: Understanding a join with pre-partitioned data
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解使用预分区数据进行连接
- en: Understanding that we avoided shuffle
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解我们如何避免洗牌
- en: We will be using the `repartition` method on the DataFrame using a common partition
    key. We saw that when issuing a join, repartitioning happens underneath. But often,
    when using Spark, we want to execute multiple operations on the DataFrame. So,
    when we perform the join with other datasets, `hashPartitioning` will need to
    be executed once again. If we do the partition at the beginning when the data
    is loaded, we will avoid partitioning again.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在DataFrame上使用`repartition`方法，使用一个公共分区键。我们发现，当进行连接时，重新分区会在底层发生。但通常，在使用Spark时，我们希望在DataFrame上执行多个操作。因此，当我们与其他数据集执行连接时，`hashPartitioning`将需要再次执行。如果我们在加载数据时进行分区，我们将避免再次分区。
- en: 'Here, we have our example test case, with the data we used previously in the *Testing
    operations that cause a shuffle in Apache Spark* section. We have `UserData` with
    three records for user ID – `user_1`, `user_2`, and `user_4` – and the `UserTransaction` data
    with the user ID – that is, `user_1`, `user_2`, `user_3`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有我们的示例测试用例，其中包含我们之前在Apache Spark的“导致洗牌的测试操作”部分中使用的数据。我们有`UserData`，其中包含三条用户ID的记录
    - `user_1`，`user_2`和`user_4` - 以及`UserTransaction`数据，其中包含用户ID - 即`user_1`，`user_2`，`user_3`：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we need to `repartition` the data, which is the first very important
    thing to do. We are repartitioning our `userData` using the `userId` column:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要对数据进行`repartition`，这是要做的第一件非常重要的事情。我们使用`userId`列来重新分区我们的`userData`：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we will repartition our data using the `userId` column, this time for
    `transactionData`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用`userId`列重新分区我们的数据，这次是针对`transactionData`：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once we have our data repartitioned, we have the assurance that any data that
    has the same partition key – in this example, it''s `userId` – will land on the
    same executor. Because of that, our repartitioned data will not have the shuffle,
    and the joins will be faster. In the end, we are able to join, but this time we
    are joining the pre-partitioned data:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们重新分区了我们的数据，我们就可以确保具有相同分区键的任何数据 - 在本例中是`userId` - 将落在同一个执行器上。因此，我们的重新分区数据将不会有洗牌，连接将更快。最终，我们能够进行连接，但这次我们连接的是预分区的数据：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can show our results using the following code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码显示我们的结果：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示在以下截图中：
- en: '![](img/74702856-ff35-41d9-9e4b-25370715cf4c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74702856-ff35-41d9-9e4b-25370715cf4c.png)'
- en: In the preceding screenshot, we have our physical plans for user ID and transactions.
    We perform a hash partitioning on the user ID column of the user ID data and also
    on the transaction data. After joining the data, we can see that the data is proper
    and that there is a physical plan for the join.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述截图中，我们有用户ID和交易的物理计划。我们对用户ID数据和交易数据的用户ID列执行了哈希分区。在连接数据之后，我们可以看到数据是正确的，并且连接有一个物理计划。
- en: This time, the physical plan is a bit different.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，物理计划有点不同。
- en: We have a `SortMergeJoin` operation, and we are sorting our data that is already
    pre-partitioned in the previous step of our execution engine. In this way, our
    Spark engine will perform the sort-merge join, where we don't need to hash join.
    It will sort data properly and the join will be faster.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个`SortMergeJoin`操作，并且我们正在对我们的数据进行排序，这些数据在我们执行引擎的上一步已经预分区。这样，我们的Spark引擎将执行排序合并连接，无需进行哈希连接。它将正确排序数据，连接将更快。
- en: In the next section, we'll be using `keyBy()` operations to reduce shuffle even
    further.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用`keyBy()`操作来进一步减少洗牌。
- en: Using keyBy() operations to reduce shuffle
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用keyBy()操作来减少洗牌
- en: 'In this section, we will use `keyBy()` operations to reduce shuffle. We will
    cover the following topics:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用`keyBy()`操作来减少洗牌。我们将涵盖以下主题：
- en: Loading randomly partitioned data
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载随机分区的数据
- en: Trying to pre-partition data in a meaningful way
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试以有意义的方式预分区数据
- en: Leveraging the `keyBy()` function
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用`keyBy()`函数
- en: We will load randomly partitioned data, but this time using the RDD API. We
    will repartition the data in a meaningful way and extract the information that
    is going on underneath, similar to DataFrame and the Dataset API. We will learn how
    to leverage the `keyBy()` function to give our data some structure and to cause
    the pre-partitioning in the RDD API.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载随机分区的数据，但这次使用RDD API。我们将以有意义的方式重新分区数据，并提取底层正在进行的信息，类似于DataFrame和Dataset
    API。我们将学习如何利用`keyBy()`函数为我们的数据提供一些结构，并在RDD API中引起预分区。
- en: 'Here is the test we will be using in this section. We are creating two random
    input records. The first record has a random user ID, `user_1`, the second one
    has a random user ID, `user_1`, and the third one has a random user ID, `user_2`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中我们将使用以下测试。我们创建两个随机输入记录。第一条记录有一个随机用户ID，`user_1`，第二条记录有一个随机用户ID，`user_1`，第三条记录有一个随机用户ID，`user_2`：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will extract what is happening underneath Spark using `rdd.toDebugString`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`rdd.toDebugString`提取Spark底层发生的情况：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: At this point, our data is spread randomly and the records for the user ID field
    could be on different executors because the Spark execution engine cannot guess
    whether `user_1` is a meaningful key for us or whether `1234-3456-1235-1234` is.
    We know that `1234-3456-1235-1234` is not a meaningful key, and that it is a unique
    identifier. Using that field as a partition key will give us a random distribution
    and a lot of shuffling because there is no data locality when you use the unique
    field as a partition key.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们的数据是随机分布的，用户ID字段的记录可能在不同的执行器上，因为Spark执行引擎无法猜测`user_1`是否对我们有意义，或者`1234-3456-1235-1234`是否有意义。我们知道`1234-3456-1235-1234`不是一个有意义的键，而是一个唯一标识符。将该字段用作分区键将给我们一个随机分布和大量的洗牌，因为在使用唯一字段作为分区键时没有数据局部性。
- en: 'There is no possibility for Spark to know that data for the same user ID will
    land on the same executor, and that''s why we need to use the user ID field, either
    `user_1`, `user_1`, or `user_2`, when partitioning the data. To achieve that in
    the RDD API, we can use `keyBy(_.userId)` in our data, but this time it will change
    the RDD type:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Spark无法知道相同用户ID的数据将落在同一个执行器上，这就是为什么在分区数据时我们需要使用用户ID字段，即`user_1`、`user_1`或`user_2`。为了在RDD
    API中实现这一点，我们可以在我们的数据中使用`keyBy(_.userId)`，但这次它将改变RDD类型：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we check the RDD type, we''ll see that this time, an RDD is not an input
    record, but it is an RDD of the string and input record. The string is a type
    of the field that we expected here, and it is `userId`. We will also extract information
    about the `keyBy()` function by  using `toDebugString` on the result:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查RDD类型，我们会发现这次，RDD不是输入记录，而是字符串和输入记录的RDD。字符串是我们在这里期望的字段类型，即`userId`。我们还将通过在结果上使用`toDebugString`来提取有关`keyBy()`函数的信息：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once we use `keyBy()`, all the records for the same user ID will land on the
    same executor. As we have discussed, this can be dangerous because if we have
    a skew key, it means that we have a key that has very high cardinality, and we
    can run out of memory. Also, all operations on a result will be key-wise, so we''ll
    be on the pre-partitioned data:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用`keyBy()`，相同用户ID的所有记录都将落在同一个执行器上。正如我们所讨论的，这可能是危险的，因为如果我们有一个倾斜的键，这意味着我们有一个具有非常高基数的键，我们可能会耗尽内存。此外，结果上的所有操作都将按键进行，因此我们将在预分区数据上进行操作：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s start this test. The output will be as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个测试。输出将如下所示：
- en: '![](img/b3deb20a-fd50-456f-bd7e-30bd4a636036.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3deb20a-fd50-456f-bd7e-30bd4a636036.png)'
- en: We can see that our first debug string is a very simple one, and we have only
    the collection on the RDD, but the second one is a bit different. We have a `keyBy()`
    method and we make an RDD underneath it. We have our child RDD and parent RDD
    from the first section, *Testing operations that cause a shuffle in Apache Spark*, from
    when we extended the RDD. This a parent-child chain that's issued by the `keyBy()`
    method.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的第一个调试字符串非常简单，我们只有RDD上的集合，但第二个有点不同。我们有一个`keyBy()`方法，并在其下面创建了一个RDD。我们有来自第一部分的子RDD和父RDD，即*测试在Apache
    Spark中引起洗牌的操作*，当我们扩展了RDD时。这是由`keyBy()`方法发出的父子链。
- en: In the next section, we'll use a custom partitioner to reduce shuffle even further.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用自定义分区器进一步减少洗牌。
- en: Using a custom partitioner to reduce shuffle
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义分区器来减少洗牌
- en: 'In this section, we will use a custom partitioner to reduce shuffle. We will
    cover the following topics:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用自定义分区器来减少洗牌。我们将涵盖以下主题：
- en: Implementing a custom partitioner
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现自定义分区器
- en: Using the partitioner with the `partitionBy` method on Spark
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`partitionBy`方法在Spark上使用分区器
- en: Validating that our data was partitioned properly
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证我们的数据是否被正确分区
- en: 'We will implement a custom partitioner with our custom logic, which will partition
    the data. It will inform Spark where each record should land and on which executor.
    We will be using the `partitionBy` method on Spark. In the end, we will validate
    that our data was partitioned properly. For the purposes of this test, we are
    assuming that we have two executors:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用自定义逻辑实现自定义分区器，该分区器将对数据进行分区。它将告诉Spark每条记录应该落在哪个执行器上。我们将使用Spark上的`partitionBy`方法。最后，我们将验证我们的数据是否被正确分区。为了测试的目的，我们假设有两个执行器：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s assume that we want to split our data evenly into `2` executors and
    that the instances of data with the same key will land on the same executor. So,
    our input data is a list of `UserTransactions`: `"a"`, `"b"`, `"a"`, `"b"`, and `"c"`.
    The values are not so important, but we need to keep them in mind to test the
    behavior later. The `amount` is `100`, `101`, `202`, `1`, and `55` for the given
    `UserTransactions`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想将我们的数据均匀地分成`2`个执行器，并且具有相同键的数据实例将落在同一个执行器上。因此，我们的输入数据是一个`UserTransactions`列表：`"a"`,`"b"`,`"a"`,`"b"`和`"c"`。值并不那么重要，但我们需要记住它们以便稍后测试行为。给定`UserTransactions`的`amount`分别为`100`,`101`,`202`,`1`和`55`：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When we do a `keyBy`, `(_.userId)` is passed to our partitioner and so when
    we issue `partitionBy`, we need to extend `override` methods:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用`keyBy`时，`(_.userId)`被传递给我们的分区器，因此当我们发出`partitionBy`时，我们需要扩展`override`方法：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `getPartition` method takes a `key`, which will be the `userId`. The key
    will be passed here and the type will be a string:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`getPartition`方法接受一个`key`，它将是`userId`。键将在这里传递，类型将是字符串：'
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The signature of these methods is `Any`, so we need to `override` it, and also
    override the number of partitions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的签名是`Any`，所以我们需要`override`它，并且还需要覆盖分区的数量。
- en: 'We then print our two partitions, and `numPartitions` returns the value of `2`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们打印我们的两个分区，`numPartitions`返回值为`2`：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`getPartition` is very simple as it takes the `hashCode` and `numberOfExecutors` modules. It
    ensures that the same key will land on the same executor.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`getPartition`非常简单，因为它获取`hashCode`和`numberOfExecutors`的模块。它确保相同的键将落在同一个执行器上。'
- en: 'We will then map every partition for the respective partition as we get an
    iterator. Here, we are taking `amount` for a test purpose:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将为各自的分区映射每个分区，因为我们得到一个迭代器。在这里，我们正在为测试目的获取`amount`：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the end, we assert `55`, `100`, `202`, `101`, and `1`; the order is random,
    so there is no need to take care of the order:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们断言`55`,`100`,`202`,`101`和`1`；顺序是随机的，所以不需要关心顺序：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If we still want to, we should use a `sortBy` method. Let''s start this test
    and see whether our custom partitioner works as expected. Now, we can start. We
    have `2` partitions, so it works as expected, as shown in the following screenshot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仍然希望，我们应该使用`sortBy`方法。让我们开始这个测试，看看我们的自定义分区器是否按预期工作。现在，我们可以开始了。我们有`2`个分区，所以它按预期工作，如下面的截图所示：
- en: '![](img/b29036a1-2b48-4d1b-a9c1-5c99d1d65299.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b29036a1-2b48-4d1b-a9c1-5c99d1d65299.png)'
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to detect shuffle in a process. We covered testing
    operations that cause a shuffle in Apache Spark. We also learned how to employ
    partitioning in the RDD. It is important to know how to use the API if partitioned
    data is needed, because RDD is still widely used, so we use the `keyBy` operations
    to reduce shuffle. We also learned how to use the custom partitioner to reduce
    shuffle.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何检测过程中的洗牌。我们涵盖了在Apache Spark中导致洗牌的测试操作。我们还学习了如何在RDD中使用分区。如果需要分区数据，了解如何使用API是很重要的，因为RDD仍然被广泛使用，所以我们使用`keyBy`操作来减少洗牌。我们还学习了如何使用自定义分区器来减少洗牌。
- en: In the next chapter, we'll learn how to save data in the correct format using
    the Spark API.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用Spark API以正确的格式保存数据。
