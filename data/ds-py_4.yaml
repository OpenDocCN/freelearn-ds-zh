- en: '*Chapter 5*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*'
- en: Mastering Structured Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握结构化数据
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够：
- en: Work with structured data to create highly accurate models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用结构化数据创建高精度模型
- en: Use the XGBoost library to train boosting models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用XGBoost库训练提升模型
- en: Use the Keras library to train neural network models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras库训练神经网络模型
- en: Fine-tune model parameters to get the best accuracy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调模型参数以获得最佳准确性
- en: Use cross-validation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证
- en: Save and load your trained models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存并加载你的训练模型
- en: This chapter will cover the basics on how to create highly accurate structured
    data models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍如何创建高精度的结构化数据模型的基础知识。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: There are two main types of data, structured and unstructured. Structured data
    refers to data that has a defined format and is usually shaped as a table, such
    as data stored in an Excel sheet or a relational database. Unstructured data does
    not have a predefined schema. Anything that cannot be stored in a table falls
    under this category. Examples include voice files, images, and PDFs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据主要分为两种类型：结构化数据和非结构化数据。结构化数据指的是具有定义格式的数据，通常以表格形式存储，例如存储在Excel表格或关系型数据库中的数据。非结构化数据没有预定义的模式，任何无法存储在表格中的数据都属于这一类。例如语音文件、图片和PDF文件等。
- en: In this chapter, we will focus on structured data and creating machine learning
    models using XGBoost and Keras. The XGBoost algorithm is widely used by industry
    experts and researchers due to the speed at which it delivers high-precision models,
    and also due to its distributed nature. The distributed nature refers to the ability
    to process data and train models in parallel; this enables faster training and
    much shorter turnaround time for data scientists. Keras on the other hand lets
    us create neural network models. Neural networks work much better than boosting
    algorithms in some cases, but finding the right network and the right configuration
    of the network is tough. The following topics will help you get familiar with
    both libraries, making sure that you can tackle any structured data in your data
    science journey.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点讨论结构化数据，并使用XGBoost和Keras创建机器学习模型。XGBoost算法因其能够快速交付高精度模型以及其分布式特性而被业界专家和研究人员广泛使用。分布式特性指的是能够并行处理数据和训练模型；这使得训练速度更快，数据科学家的周转时间更短。另一方面，Keras让我们能够创建神经网络模型。在某些情况下，神经网络的效果远胜于提升算法，但找到合适的网络和网络配置是非常困难的。以下主题将帮助你熟悉这两个库，确保你能应对数据科学旅程中的任何结构化数据。
- en: Boosting Algorithms
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升算法
- en: Boosting is a way to improve the accuracy of any learning algorithm. Boosting
    works by combining rough, high-level rules into a single prediction that is more
    accurate than any single rule. Iteratively, a subset of the training dataset is
    ingested into a "weak" algorithm to generate a weak model. These weak models are
    then combined to form the final prediction. Two of the most effective boosting
    algorithms are gradient boosting machine and XGBoost.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 提升方法是一种提高任何学习算法准确性的方式。提升方法通过将粗略的、高层次的规则结合成一个比任何单一规则更准确的预测。它通过迭代地将训练数据集的一个子集输入到一个“弱”算法中来生成一个弱模型。这些弱模型随后被结合在一起，形成最终的预测。两种最有效的提升算法是梯度提升机和XGBoost。
- en: Gradient Boosting Machine (GBM)
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升机（GBM）
- en: GBM makes use of classification trees as the weak algorithm. The results are
    generated by improving estimations from these weak models using a differentiable
    loss function. The model fits consecutive trees by considering the net loss of
    the previous trees; therefore, each tree is partially present in the final solution.
    Hence, boosting trees decreases the speed of the algorithm, and the transparency
    that they provide gives much better results. The GBM algorithm has a lot of parameters
    and it is sensitive to noise and extreme values. At the same time, GBM overfits
    the data, and thus a proper stopping point is required, but it is often the best
    possible model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GBM利用分类树作为弱算法。结果通过使用可微分的损失函数来改进这些弱模型的估算。该模型通过考虑前一棵树的净损失来拟合连续的树；因此，每棵树都部分地参与了最终的解决方案。因此，提升树会降低算法的速度，但它们提供的透明度能带来更好的结果。GBM算法有很多参数，而且对噪声和极端值非常敏感。同时，GBM会发生过拟合，因此需要一个合适的停止点，但它通常是最佳的模型。
- en: XGBoost (Extreme Gradient Boosting)
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost（极端梯度提升）
- en: XGBoost is the algorithm of choice for researchers across the world when modelling
    structured data. XGBoost also uses trees as the weak algorithm. So, why is it
    the first algorithm that comes to mind when data scientists see structured data?
    XGBoost is portable and distributed, which means that it can be easily used in
    different architectures and can use multiple cores (single machine) or multiple
    machines (clusters). As a bonus, the XGBoost library is written in C++, which
    makes it fast. It is also useful when working with a huge dataset, as it allows
    you to store data on an external disk and not load all the data on to the memory.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost是全球研究人员在构建结构化数据模型时的首选算法。XGBoost同样使用树作为弱学习算法。那么，为什么当数据科学家看到结构化数据时，XGBoost是他们首先想到的算法呢？XGBoost是可移植和分布式的，这意味着它可以在不同的架构中轻松使用，并且可以利用多个核心（单台机器）或多台机器（集群）。作为额外的好处，XGBoost库是用C++编写的，这使它非常快速。当处理大数据集时，它特别有用，因为它允许将数据存储在外部磁盘上，而不必将所有数据加载到内存中。
- en: Note
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read more about XGBoost here: https://arxiv.org/abs/1603.02754'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于XGBoost的内容：[https://arxiv.org/abs/1603.02754](https://arxiv.org/abs/1603.02754)
- en: 'Exercise 44: Using the XGBoost library to Perform Classification'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习44：使用XGBoost库进行分类
- en: In this exercise, we will perform classification on the wholesale customer dataset
    (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data)
    using XGBoost library for Python. The dataset contains purchase data for clients
    of a wholesale distributor. It includes the annual spending on diverse range of
    product categories. We will predict the channel based on the annual spend on various
    products. The channel here describes whether the client is either a horeca (hotel/restaurant/café)
    or a retail customer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用Python的XGBoost库对批发客户数据集进行分类（[https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data](https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data)）。该数据集包含批发分销商客户的购买数据，包括在各种产品类别上的年度花费。我们将根据不同产品的年度支出预测渠道。这里的“渠道”描述了客户是酒店/餐厅/咖啡馆（horeca）客户还是零售客户。
- en: Open the Jupyter Notebook from your virtual environment.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你虚拟环境中的Jupyter Notebook。
- en: Import XGBoost, Pandas, and sklearn for the function that we will use to calculate
    the accuracy. The accuracy is required to understand how our model is performing.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入XGBoost、Pandas和sklearn，这些是我们将用于计算准确度的函数。准确度对于理解我们的模型性能至关重要。
- en: '[PRE0]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Read the wholesale customer dataset using pandas and check to see if it was
    loaded successfully using the following command:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas读取批发客户数据集，并通过以下命令检查数据是否成功加载：
- en: '[PRE1]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Check the first five entries of the dataset using the `head()` command. The
    output is shown in the following screenshot:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`head()`命令检查数据集的前五个条目。输出如下面的截图所示：
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Figure 5.1: Screenshot showing first five elements of dataset](img/C13322_05_01.jpg)'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.1：显示数据集前五个元素的截图](img/C13322_05_01.jpg)'
- en: 'Figure 5.1: Screenshot showing first five elements of dataset'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.1：显示数据集前五个元素的截图
- en: Now the "`data`" dataframe has all the data. It has the target variable, which
    is "`Channel`" in our case, and it has the predictor variables. So, we split the
    data into features (predictor) and labels (target).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，“`data`”数据框包含所有数据。它有目标变量，在我们的例子中是“`Channel`”，以及预测变量。因此，我们将数据分为特征（预测变量）和标签（目标变量）。
- en: '[PRE3]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Create training and test sets as discussed in previous chapters. Here, we use
    an 80:20 split as the number of data points in the dataset is less. You can experiment
    with different splits.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照前面章节的讨论，创建训练集和测试集。在这里，我们使用80:20的比例进行分割，因为数据集中的数据点较少。你可以尝试不同的分割比例。
- en: '[PRE4]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Convert the pandas dataframe into a DMatrix, an internal data structure that
    is used by XGBoost to store training and testing datasets.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将pandas数据框转换为DMatrix，这是XGBoost用于存储训练和测试数据集的内部数据结构。
- en: '[PRE5]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Specify the training parameters and train the model.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定训练参数并训练模型。
- en: Note
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: By default, XGBoost uses all threads available to it for multiprocessing. To
    limit this, you can use the nthread parameter. Refer the next section for more
    information.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 默认情况下，XGBoost会使用所有可用线程进行多线程处理。要限制线程数，你可以使用`nthread`参数。有关更多信息，请参阅下一节。
- en: Predict the "`Channel`" values of the test set using the model that we just
    created.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们刚刚创建的模型预测测试集的“`Channel`”值。
- en: '[PRE7]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Get the accuracy of the model that we have trained for the test dataset.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取我们为测试数据集训练的模型的准确度。
- en: '[PRE8]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output screenshot is as follows:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出截图如下：
- en: '![Figure 6.2: Final accuracy](img/C13322_05_02.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2：最终准确度](img/C13322_05_02.jpg)'
- en: 'Figure 5.2: Final accuracy'
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.2：最终准确度
- en: Congratulations! You just made your first XGBoost model with approximately 90%
    accuracy without much fine-tuning!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚使用大约90%的准确率创建了你的第一个XGBoost模型，而且几乎没有进行调优！
- en: XGBoost Library
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost库
- en: The library we used to perform the above classification is named XGBoost. The
    library enables a lot of customization using the many parameters it has. In the
    following sections, we will dive in and understand the different parameters and
    functions of the XGBoost library.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于执行上述分类的库名为XGBoost。该库通过许多参数提供了大量的自定义选项。在接下来的章节中，我们将深入了解XGBoost库的不同参数和功能。
- en: Note
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information about XGBoost, refer the website: https://xgboost.readthedocs.io'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于XGBoost的信息，请访问网站：https://xgboost.readthedocs.io
- en: '**Training**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练**'
- en: Parameters that affect the training of any XGBoost model are listed below.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 影响任何XGBoost模型训练的参数如下所示。
- en: '`booster`: Even though we mentioned in the introduction that the base learner
    of XGBoost is a regression tree, using this library, we can use linear regression
    as the weak learner as well. Another weak learner, DART booster, is a new method
    to tree boosting, which drops trees at random to prevent overfitting. To use tree
    boosting, pass "`gbtree`" (default); for linear regression, pass "`gblinear`";
    and for tree boosting with dropout, pass "`dart`".'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`booster`: 尽管我们在介绍中提到XGBoost的基础学习器是回归树，但使用此库时，我们也可以将线性回归作为弱学习器使用。另一种弱学习器，DART增强器，是一种新的树增强方法，它通过随机丢弃树来防止过拟合。使用树增强时，传递"`gbtree`"（默认）；使用线性回归时，传递"`gblinear`"；若要进行带有丢弃的树增强，传递"`dart`"。'
- en: Note
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You may learn more about DART from this paper: http://www.jmlr.org/proceedings/papers/v38/korlakaivinayak15.pdf'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以通过这篇论文了解更多关于DART的信息：http://www.jmlr.org/proceedings/papers/v38/korlakaivinayak15.pdf
- en: '`silent`: 0 prints the training logs, whereas 1 is the silent mode.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`silent`: 0打印训练日志，而1则为静默模式。'
- en: '`nthread`: This signifies the number of parallel threads to be used. It defaults
    to the maximum number of threads available in the system.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nthread`: 这表示要使用的并行线程数。默认情况下，它是系统中可用的最大线程数。'
- en: Note
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The parameter silent has been deprecated and has been replaced with verbosity,
    which takes any of the following values: 0 (silent), 1 (warning), 2 (info), 3
    (debug).'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数`silent`已被弃用，现已被`verbosity`替代，`verbosity`可以取以下值：0（静默）、1（警告）、2（信息）、3（调试）。
- en: '`seed`: This is the seed value for the random number generator. Set a constant
    value here to get reproducible results. The default value is 0.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed`: 这是随机数生成器的种子值。在此处设置一个常量值，可以得到可重复的结果。默认值是0。'
- en: '`objective`: This is a function that the model tries to minimize. The next
    few points cover the objective functions.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`objective`: 这是模型尝试最小化的函数。接下来的几个点将介绍目标函数。'
- en: '`reg:linear`: Linear regression should be used with continuous target variables
    (regression problem). (Default)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`reg:linear`: 线性回归应该用于连续型目标变量（回归问题）。 （默认）'
- en: '`binary:logistic`: logistic regression to be used in case of binary classification.
    It outputs probability and not classes.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`binary:logistic`: 用于二分类的逻辑回归。它输出概率而非类别。'
- en: '`binary:hinge`: This is binary classification that outputs predictions of 0
    or 1, rather than the probabilities. Use this when you are not concerned about
    the probabilities.'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`binary:hinge`: 这是二分类，它输出的是0或1的预测，而不是概率。当你不关心概率时，使用此方法。'
- en: '`multi:softmax`: If you want to do a multiclass classification, use this to
    perform the classification using the softmax objective. It is mandatory to set
    the `num_class` parameter to the number of classes for this.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`multi:softmax`: 如果你想进行多类别分类，使用此方法来执行基于softmax目标的分类。必须将`num_class`参数设置为类别数。'
- en: '`multi:softprob`: This works the same as softmax, but the outputs predict the
    probability of each data point instead of predicting a class.'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`multi:softprob`: 这与softmax的作用相同，但输出的是每个数据点的概率，而不是预测类别。'
- en: '`eval_metric`: The performance of a model needs to be observed on the validation
    set (as discussed in *Chapter 1*, *Introduction to Data Science and Data Preprocessing*).
    This parameter takes the evaluation metric for the validation data. The default
    metric is chosen according to the objective function (`rmse` for regression and
    `logloss` for classification). You can use multiple evaluation metrics.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_metric`: 需要在验证集上观察模型的表现（如在*第1章*，*数据科学与数据预处理介绍*中讨论）。此参数用于指定验证数据的评估指标。默认的评估指标根据目标函数选择（回归使用`rmse`，分类使用`logloss`）。你可以使用多个评估指标。'
- en: '`rmse`: Root mean square error (RMSE) penalizes large errors more. So, it is
    appropriate when being off by 1 is more than three times as bad as being off by
    3.'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`rmse`：均方根误差（RMSE）对较大误差的惩罚更重。因此，当误差为 1 比误差为 3 更为严重时，它是合适的。'
- en: '`mae`: Mean absolute error (MAE) can be used in cases where being off by 1
    is similar to being off by 3.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`mae`：平均绝对误差（MAE）适用于当误差为 1 和误差为 3 的情况类似时。'
- en: 'The following graph shows the increase in the error with the increase in difference
    between the actual and predicted values. Here, it would be the following:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图表显示了随着实际值与预测值之间差异的增加，误差也随之增加。图示如下：
- en: '![Figure 5.3: Difference between actual and predicted value](img/C13322_05_03.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3：实际值与预测值的差异](img/C13322_05_03.jpg)'
- en: 'Figure 5.3: Difference between actual and predicted value'
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.3：实际值与预测值的差异
- en: '![Figure 5.4: Variation of penalty with variation in error; |X| is mae and
    X2  is rmse](img/C13322_05_04.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4：随着误差变化，惩罚的变化；|X| 为 mae，X² 为 rmse](img/C13322_05_04.jpg)'
- en: 'Figure 5.4: Variation of penalty with variation in error; |X| is mae and X2
    is rmse'
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.4：随着误差变化，惩罚的变化；|X| 为 mae，X² 为 rmse
- en: '`logloss`: The negative log-likelihood, `logloss` of a model is equivalent
    to maximising the model''s accuracy. It is defined mathematically as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`logloss`：模型的负对数似然（`logloss`）等同于最大化模型的准确度。它在数学上被定义为：'
- en: '![Figure 5.5: Logloss equation diagram](img/C13322_05_05.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5：Logloss 方程示意图](img/C13322_05_05.jpg)'
- en: 'Figure 5.5: Logloss equation diagram'
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.5：Logloss 方程示意图
- en: Here, N is the number of data points, M is the number of classes and is either
    1 or 0 depending on whether the prediction was correct or not, is the probability
    of predicting label *j* for data point *i*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，N 是数据点的数量，M 是类别的数量，1 或 0 取决于预测是否正确，表示的是预测数据点 *i* 为标签 *j* 的概率。
- en: '`AUC`: **Area under the curve** is used widely for binary classification. You
    should always use this if your dataset has a **class imbalance problem**. A class
    imbalance problem occurs when your data is not split up into classes of similar
    sizes; for example, if class A makes up 90% of the data and class B makes up 10%
    of the data. We will talk more about the class imbalance problem in the Handling
    Imbalanced Datasets section.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`AUC`：**曲线下面积**在二分类中广泛使用。如果你的数据集存在**类别不平衡问题**，你应该始终使用它。类别不平衡问题发生在数据未能均匀分配到不同类别中；例如，如果类别
    A 占数据的 90%，而类别 B 占 10%。我们将在“处理不平衡数据集”一节中进一步讨论类别不平衡问题。'
- en: '`aucpr`: Area under the **precision-recall** (**PR**) curve is the same as
    the AUC curve, but should be preferred in case of a highly imbalanced dataset.
    We shall discuss this too in the Handling Imbalanced Datasets section.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`aucpr`：**精准率-召回率**（**PR**）曲线下的面积与 AUC 曲线相同，但在处理高度不平衡的数据集时应优先使用。我们将在“处理不平衡数据集”一节中讨论这一点。'
- en: Note
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: AUC or AUCPR should be used as a rule of thumb whenever you are working with
    a binary dataset.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理二分类数据集时，应作为经验法则使用 AUC 或 AUCPR。
- en: '**Tree Booster**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**树提升器**'
- en: 'Parameters that are specific to tree-based models are listed below:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 特定于基于树的模型的参数如下所示：
- en: '`eta`: This is the learning rate. Modify this value to prevent overfitting
    as discussed in *Chapter 1*, *Introduction to Data Science and Data Preprocessing*.
    The learning rate decides by how much the weights will get updated in each step.
    The gradient of weights gets multiplied by this and then added to the weight.
    This defaults to 0.3 and has a maximum value of 1 and a minimum value of 0.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta`：这是学习率。修改此值以防止过拟合，正如我们在*第 1 章*，*数据科学与数据预处理简介*中讨论的那样。学习率决定了每一步更新权重的幅度。权重的梯度会与学习率相乘，并加到权重上。默认值为
    0.3，最大值为 1，最小值为 0。'
- en: '`gamma`: This is the minimum loss reduction to make a partition. The larger
    gamma is, the more conservative the algorithm will be. Being more conservative
    prevents overfitting. The value depends on the dataset and other parameters used.
    It ranges from 0 to infinity and the default value is 0\. A lower value leads
    to shallow trees and larger values give rise to deeper trees.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`：这是进行划分所需的最小损失减少量。gamma 值越大，算法越保守。更加保守有助于防止过拟合。该值依赖于数据集和使用的其他参数。其范围从
    0 到无限大，默认值为 0。较小的值会导致树较浅，较大的值则会导致树更深。'
- en: Note
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Gamma values above 1 usually do not give good results.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: gamma 值大于 1 通常不会得到好的结果。
- en: '`max_depth`: This is the maximum depth of any tree as discussed in *Chapter
    3*, *Introduction to ML via Sklearn*. Increasing the max depth will make the model
    more likely to overfit. 0 means no limit. It defaults to 6.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：这是任何树的最大深度，如*第3章*《通过Sklearn介绍机器学习》中所讨论的。增加最大深度将使模型更容易发生过拟合。0表示没有限制。默认值为6。'
- en: '`subsample`: Setting this to 0.5 will cause the algorithm to randomly sample
    half of the training data before growing the trees. This prevents overfitting.
    Subsampling occurs once every boosting iteration and defaults to 1, which makes
    the model take the complete dataset and not a sample.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample`：将其设置为0.5将导致算法在生成树之前随机抽取一半的训练数据。这可以防止过拟合。每次提升迭代时会进行一次子采样，默认值为1，这意味着模型会使用完整的数据集，而不是样本。'
- en: '`lambda`: This is the L2 regularization term. L2 regularization adds a squared
    magnitude of the coefficient as the penalty term to the loss function. Increasing
    this value prevents overfitting. Its default value is 1.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lambda`：这是L2正则化项。L2正则化将系数的平方值作为惩罚项添加到损失函数中。增大该值可以防止过拟合。其默认值为1。'
- en: '`alpha`: This is the L1 regularization term. L1 regularization adds an absolute
    magnitude of the coefficient as the penalty term to the loss function. Increasing
    this value prevents overfitting. Its default value is 0.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`：这是L1正则化项。L1正则化将系数的绝对值作为惩罚项添加到损失函数中。增大该值可以防止过拟合。其默认值为0。'
- en: '`scale_pos_weight`: This is useful when the classes are highly imbalanced.
    We will learn more about imbalanced data in the following sections. A typical
    value to consider introducing: the sum of negative instances / the sum of positive
    instances. Its default value is 1.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_pos_weight`：当类别极度不平衡时非常有用。我们将在接下来的章节中学习更多关于不平衡数据的内容。一个典型的考虑值是：负实例的总和
    / 正实例的总和。其默认值为1。'
- en: '`predictor`: There are two predictors. `cpu_predictor` uses CPU for prediction.
    It is the default. `gpu_predictor` uses GPU for prediction.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predictor`：有两个预测器。`cpu_predictor`使用CPU进行预测，默认使用。`gpu_predictor`使用GPU进行预测。'
- en: Note
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Get a list of all the parameters here: https://xgboost.readthedocs.io/en/latest/parameter.html'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 获取所有参数的列表，请访问：https://xgboost.readthedocs.io/en/latest/parameter.html
- en: Controlling Model Overfitting
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制模型过拟合
- en: 'If you observe high accuracy on the training dataset but a low accuracy on
    the test dataset, your model has overfit to the training data, as seen in *Chapter
    1*, *Introduction to Data Science and Data Preprocessing*. There are two main
    ways to limit overfitting in XGBoost:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察到训练数据集上的准确度很高，但在测试数据集上的准确度较低，那么你的模型已经对训练数据过拟合，如*第1章*《数据科学和数据预处理简介》中所示。XGBoost中有两种主要方法可以限制过拟合：
- en: '`max_depth`, `min_child_weight`, and `gamma` while monitoring the training
    and test metrics to get the best model without overfitting to the training dataset.
    You will learn more about this in the following sections.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在监控训练和测试指标的同时调整`max_depth`、`min_child_weight`和`gamma`，以便获得最佳模型而不对训练数据集过拟合。你将在接下来的章节中学到更多相关内容。
- en: '`colsample_bytree` does the same thing as subsampling but with columns instead
    of rows.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colsample_bytree`的作用与子采样相同，但它是对列进行采样，而不是对行进行采样。'
- en: 'To understand better, see the training and accuracy graphs in the following
    figure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，请参见下图中的训练和准确度图表：
- en: '![Figure 5.6: Training and accuracy graphs](img/C13322_05_06.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6：训练和准确度图表](img/C13322_05_06.jpg)'
- en: 'Figure 5.6: Training and accuracy graphs'
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.6：训练和准确度图表
- en: 'To understand the conceptualization of a dataset with overfit and proper-fit
    models, refer to the following figure:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解具有过拟合和适当拟合模型的数据集的概念化，请参阅下图：
- en: '![Figure 5.7: Illustration of a dataset with overfit and proper-fit models](img/C13322_05_07.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7：具有过拟合和适当拟合模型的数据集示意图](img/C13322_05_07.jpg)'
- en: 'Figure 5.7: Illustration of a dataset with overfit and proper-fit models'
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.7：具有过拟合和适当拟合模型的数据集示意图
- en: Note
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The black line represents the model that has a proper fit, whereas the model
    represented by the red line has overfit the dataset.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 黑线表示模型适配良好，而红线表示的模型已经对数据集过拟合。
- en: Handling Imbalanced Datasets
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理不平衡数据集
- en: 'Imbalanced datasets cause a lot of problems to data scientists. One example
    of an imbalanced dataset is credit card fraud data. Here, about 95% of transactions
    will be legitimate and only 5% will be fraudulent. In this case, a model that
    predicts every transaction to be a correct transaction will get 95% accuracy,
    but, it is a very bad model. To see the distribution of your classes, you can
    use the following function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡数据集给数据科学家带来了许多问题。一个不平衡数据集的例子是信用卡欺诈数据。在这种情况下，大约95%的交易是合法的，只有5%是欺诈性的。在这种情况下，一个将所有交易预测为正确的模型会得到95%的准确率，但实际上这是一个非常糟糕的模型。为了查看类别的分布情况，你可以使用以下函数：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output would be as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.8: Class distribution](img/C13322_05_08.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8：类别分布](img/C13322_05_08.jpg)'
- en: 'Figure 5.8: Class distribution'
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.8：类别分布
- en: 'To handle imbalanced datasets, you can use the following methods:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理不平衡数据集，你可以使用以下方法：
- en: '**Undersample the class that has a higher number of records**: In the case
    of credit card fraud, you can randomly sample legitimate transactions to get records
    equal to the fraudulent records. This will result in equal distribution of the
    two classes, fraudulent and legitimate.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对记录数较多的类别进行欠采样**：以信用卡欺诈为例，你可以通过随机抽取合法交易样本，使其记录数与欺诈记录相等。这样就能实现欺诈类别和合法类别的均衡分布。'
- en: '**Oversample the class that has lesser records**: In the case of credit card
    fraud, you can introduce more samples of the fraudulent transactions by adding
    either new data points or by copying the existing data points. This will result
    in equal distribution of the two classes, **fraudulent** and **legitimate**.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对记录数较少的类别进行过采样**：以信用卡欺诈为例，你可以通过添加新数据点或复制现有数据点来增加欺诈交易样本的数量。这样就能实现欺诈类别和合法类别的均衡分布，**欺诈**和**合法**。'
- en: '**Balance the positive and negative weights with scale_pos_weight**: You can
    use this parameter to allot a higher weight to the class with a smaller number
    of data points and thus artificially balance the classes. The value of the parameter
    can be:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 scale_pos_weight 来平衡正负权重**：你可以通过该参数为数据点较少的类别分配更高的权重，从而人为地平衡各类别。该参数的值可以是：'
- en: '![Figure 5.9: Value parameter equation](img/C13322_05_09.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9：值参数方程](img/C13322_05_09.jpg)'
- en: 'Figure 5.9: Value parameter equation'
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.9：值参数方程
- en: 'You can check the distribution of the classes using the following code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码查看各类别的分布情况：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Use AUC or AUCPR for evaluation**: As mentioned earlier, the AUC and AUCPR
    metrics are sensitive to imbalanced dataset, unlike accuracy, which gives you
    a high value for a bad model that predicts the majority class most of the time.
    AUC can be plotted only for binary classification problems. It is a representation
    of the **True Positive Rate vs. the False Positive Rate** at different thresholds
    (0, 0.01, 0.02… 1) of the predicted value. It is shown in the following figure:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 AUC 或 AUCPR 进行评估**：如前所述，AUC 和 AUCPR 指标对于不平衡数据集非常敏感，不像准确率那样，它会给出一个高值，尽管模型通常预测的是多数类别。AUC
    仅适用于二分类问题。它表示的是在不同阈值（0、0.01、0.02...1）下的**真正例率与假正例率**的关系。如下图所示：'
- en: '![Figure 5.10: TPR and FPR equations](img/C13322_05_10.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10：TPR 和 FPR 方程](img/C13322_05_10.jpg)'
- en: 'Figure 5.10: TPR and FPR equations'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.10：TPR 和 FPR 方程
- en: The metric is the area under the curve that we get after plotting **TPR** and
    **FPR**. When dealing with highly skewed datasets, AUCPR gives a better picture
    and is thus preferred. AUCPR is the representation of precision and recall at
    different thresholds
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标是绘制**TPR** 和 **FPR** 后所得到的曲线下面积。在处理高度偏斜的数据集时，AUCPR 能提供更好的结果，因此它是首选。AUCPR
    表示在不同阈值下的精确度和召回率。
- en: '![Figure 6.11: Precision and recall equations](img/C13322_05_11.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11：精确度和召回率方程](img/C13322_05_11.jpg)'
- en: 'Figure 5.11: Precision and recall equations'
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.11：精确度和召回率方程
- en: As a rule of thumb, you should use AUC or AUCPR as the evaluation metric when
    dealing with imbalanced classes as it gives a clearer picture of the model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 作为经验法则，当处理不平衡类别时，应使用 AUC 或 AUCPR 作为评估指标，因为它能提供更清晰的模型表现。
- en: Note
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Machine learning algorithms cannot easily process strings or categorical variables
    represented as strings, so we have to convert them into numbers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法无法轻松处理作为字符串表示的字符串或类别变量，因此我们必须将它们转换为数字。
- en: 'Activity 14: Training and Predicting the Income of a Person'
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 14：训练并预测一个人的收入
- en: 'In this activity, we will attempt to predict whether or not the income of an
    individual exceeds $50,000\. The adult income dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data)
    has its data sourced from the 1994 census dataset (https://archive.ics.uci.edu/ml/datasets/adult)
    and contains information such as income, education qualification of a person,
    and their occupation. Let''s look at the following scenario: You work at a car
    company and you need to create a system by which the sales representatives of
    your firm can figure out what kind of car to sell to which person.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将尝试预测一个人的收入是否超过$50,000。成人收入数据集（[链接](https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data)）的数据来源于1994年人口普查数据集（[链接](https://archive.ics.uci.edu/ml/datasets/adult)），包含个人收入、教育资格和职业等信息。让我们来看一个场景：你在一家汽车公司工作，你需要创建一个系统，让公司的销售代表能够判断应该向哪个人推荐哪种汽车。
- en: To do this, you create a machine learning model that predicts the income of
    a prospective buyer and thus provides the salesperson with the right information
    to sell the right car.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，你需要创建一个机器学习模型来预测潜在买家的收入，从而为销售人员提供正确的信息，以便销售正确的汽车。
- en: Load the income dataset (`adult-data.csv`) using pandas.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas加载收入数据集（`adult-data.csv`）。
- en: 'The data should look like this:![Figure 5.12: Screenshot showing five elements
    of census dataset](img/C13322_05_12.jpg)'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据应该如下所示：![图 5.12：显示人口普查数据集五个元素的截图](img/C13322_05_12.jpg)
- en: 'Figure 5.12: Screenshot showing five elements of census dataset'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.12：显示人口普查数据集五个元素的截图
- en: 'Use the following code to specify column names:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用以下代码来指定列名：
- en: '[PRE11]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Convert all the categorical variables from strings to integers using sklearn.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用sklearn将所有分类变量从字符串转换为整数。
- en: Perform prediction using the XGBoost library and perform parameter tuning to
    improve the accuracy to be more than 80%.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用XGBoost库进行预测，并进行参数调优，使准确率超过80%。
- en: We have successfully predicted the income using the dataset with around 83%
    accuracy.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地使用数据集预测了收入，准确率约为83%。
- en: Note
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 360.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第360页找到。
- en: External Memory Usage
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部内存使用
- en: When you have an exceptionally large dataset that you can't load on to your
    RAM, the external memory feature of the XGBoost library will come to your rescue.
    This feature will train XGBoost models for you without loading the entire dataset
    on the RAM.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有一个异常大的数据集，无法将其加载到内存中时，XGBoost库的外部内存功能将帮助你。这个功能将训练XGBoost模型，而无需将整个数据集加载到内存中。
- en: Using this feature requires minimal effort; you just need to add a cache prefix
    at the end of the filename.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个功能几乎不需要额外努力；你只需要在文件名的末尾添加缓存前缀。
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This feature supports only `libsvm` file. So, we will now convert a dataset
    loaded in pandas into a `libsvm` file to be used with the external memory feature.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 该功能仅支持`libsvm`文件。因此，我们现在将把一个已加载到pandas的数据集转换为`libsvm`文件，以便使用外部内存功能。
- en: Note
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You might have to do this in batches depending on how big your dataset is.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要根据数据集的大小分批进行处理。
- en: '[PRE13]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, `X_train` and `Y_train` are the predictor and target variables respectively.
    The `libsvm` file will get saved into the data folder.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`X_train`和`Y_train`分别是预测变量和目标变量。`libsvm`文件将保存在数据文件夹中。
- en: Cross-validation
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: 'Cross-validation is a technique that helps data scientists evaluate their models
    on unseen data. It is helpful when your dataset isn''t large enough to create
    three splits (training, testing, and validation). Cross-validation helps the model
    avoid overfitting by presenting it with different partitions of the same data.
    It works by feeding different training and validation sets of the dataset for
    every pass of cross-validation. 10-fold cross-validation is the most used, where
    the dataset is divided into 10 completely different subsets and is trained on
    each one of them, and finally, the metrics are averaged out to obtain the accurate
    prediction performance of the model. In every round of cross-validation, we do
    the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一种帮助数据科学家评估模型在未见数据上的表现的技术。当数据集不够大，无法创建三个划分（训练集、测试集和验证集）时，交叉验证特别有用。交叉验证通过给模型呈现同一数据集的不同划分，帮助模型避免过拟合。它通过在每次交叉验证的过程中，向模型提供不同的训练集和验证集来实现。10折交叉验证是最常用的方式，数据集被分成10个完全不同的子集，并对每个子集进行训练，最后平均各项指标，以获得模型的准确预测性能。在每一轮交叉验证中，我们进行以下操作：
- en: Shuffle the dataset and split it into k different groups (k=10 for 10-fold cross-validation).
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集打乱并将其分成k个不同的组（k=10用于10折交叉验证）。
- en: Train the model on k-1 groups and test it on 1 group.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在k-1个组上训练模型，并在1个组上进行测试。
- en: Evaluate the model and store the results.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型并存储结果。
- en: Repeat steps 2 and 3 with different groups until all k combinations are trained.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3，使用不同的组，直到所有k种组合都训练完毕。
- en: The final metric is the mean of the metrics generated in the different rounds.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终指标是不同轮次生成的指标的平均值。
- en: '![Figure 5.13: Illustration of a cross-validation dataset](img/C13322_05_13.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13：交叉验证数据集示意图](img/C13322_05_13.jpg)'
- en: 'Figure 5.13: Illustration of a cross-validation dataset'
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.13：交叉验证数据集示意图
- en: The XGBoost library has an inbuilt function to perform cross-validation. This
    section will help you get you familiar using it.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost库内置了一个进行交叉验证的函数。本节将帮助你熟悉如何使用它。
- en: 'Exercise 45: Using Cross-validation to Find the Best Hyperparameters'
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习45：使用交叉验证找到最佳超参数
- en: In this exercise, we will find the best hyperparameters for the adult dataset
    from the previous activity using the XGBoost library for Python. To do this, we
    will make use of the cross-validation feature of the library.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用XGBoost库在Python中为上一个活动中的成人数据集找到最佳超参数。为此，我们将利用该库的交叉验证功能。
- en: Load the census dataset from *Activity 14* and perform all the preprocessing
    steps.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*活动14*加载人口普查数据集，并执行所有预处理步骤。
- en: '[PRE14]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Use Label Encoder from sklearn to encode strings. First, import Label Encoder,
    then encode all the string categorical columns one by one.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用sklearn中的Label Encoder对字符串进行编码。首先，导入Label Encoder，然后逐个编码所有字符串类型的列。
- en: '[PRE15]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Make train and test sets from the data and convert the data into Dmatrix.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据中创建训练集和测试集，并将数据转换为D矩阵。
- en: '[PRE16]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Instead of using the train function, use the following code to perform 10-fold
    cross-validation and store the result in the `model_metrics` dataframe. The for
    loop iterates over different tree depth values to find the best one for our dataset.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不使用train函数，而是使用以下代码进行10折交叉验证，并将结果存储在`model_metrics`数据框中。for循环遍历不同的树深度值，以找到最适合我们数据集的深度。
- en: '[PRE17]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Visualize the results using Matplotlib.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Matplotlib可视化结果。
- en: '[PRE18]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure 6.14: Graph of max depth with test error](img/C13322_05_14.jpg)'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.14：最大深度与测试误差的图表](img/C13322_05_14.jpg)'
- en: 'Figure 5.14: Graph of max depth with test error'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.14：最大深度与测试误差的图表
- en: From the graph, we understand that the max depth of 9 works best for our dataset
    as it has the lowest test error.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，9的最大深度对于我们的数据集效果最好，因为它的测试误差最低。
- en: Find the best learning rate. Running this piece of code will take a while as
    it iterates over a lot of learning rates for 500 rounds each.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到最佳学习率。运行这段代码会花费一些时间，因为它会遍历多个学习率，每个学习率运行500轮。
- en: '[PRE19]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Visualize the results.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化结果。
- en: '[PRE20]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Figure 5.15: Graph of learning rate with test error](img/C13322_05_15.jpg)'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.15：学习率与测试误差的图表](img/C13322_05_15.jpg)'
- en: 'Figure 5.15: Graph of learning rate with test error'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.15：学习率与测试误差的图表
- en: From the graph, we can see that a learning rate of about 0.01 works best for
    our model as it has the lowest error.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从图中可以看到，约0.01的学习率对我们的模型效果最好，因为它的误差最小。
- en: Let us visualize the training and testing errors for each round for the learning
    rate 0.01.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们可视化学习率为0.01时每一轮的训练和测试误差。
- en: '[PRE21]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Figure 5.16: Graph of training and testing errors with respect to number
    of rounds'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.16：训练和测试误差随轮次变化的图表](img/C13322_05_16.jpg)'
- en: '](img/C13322_05_16.jpg)'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_05_16.jpg)'
- en: 'Figure 5.16: Graph of training and testing errors with respect to number of
    rounds'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.16：训练和测试误差随轮次变化的图表
- en: Note
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE22]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: To understand, check out the output.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要理解这一点，可以查看输出结果。
- en: '![Figure 6.17: Least error ](img/C13322_05_17.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17：最小误差](img/C13322_05_17.jpg)'
- en: 'Figure 5.17: Least error'
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.17：最小误差
- en: Note
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The final model parameters that work best for this dataset:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于该数据集，效果最好的最终模型参数为：
- en: Max depth = 9
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最大深度 = 9
- en: Learning rate = 0.01
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 = 0.01
- en: Number of rounds = 496
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 轮次数 = 496
- en: Saving and Loading a Model
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和加载模型
- en: 'The last piece in mastering structured data is the ability to save and load
    the models that you have trained and fine-tuned. Training a new model every time
    we need a prediction will waste a lot of time, so being able to save a trained
    model is imperative for data scientists. The saved model allows us to replicate
    the results and to create apps and services that make use of the machine learning
    model. The steps are as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握结构化数据的最后一步是能够保存和加载你已经训练和微调的模型。每次需要预测时都重新训练一个新模型会浪费大量时间，因此能够保存已训练的模型对于数据科学家来说至关重要。保存的模型使我们能够复制结果，并创建使用机器学习模型的应用程序和服务。步骤如下：
- en: To save an XGBoost model, you need to call the `save_model` function.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要保存 XGBoost 模型，你需要调用 `save_model` 函数。
- en: '[PRE23]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: To load a previously saved model, you have to call load_model on an initialized
    XGBoost variable.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要加载之前保存的模型，您需要在初始化的 XGBoost 变量上调用 `load_model`。
- en: '[PRE24]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: If you give XGBoost access to all the threads it can get, your computer might
    become slow while training or predicting.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您让 XGBoost 获取所有可用的线程，您的计算机在训练或预测时可能会变慢。
- en: You are now ready to get started on modeling your structured dataset using the
    XGBoost library!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好使用 XGBoost 库来开始建模你的结构化数据集了！
- en: 'Exercise 46: Creating a Python Pcript that Predicts Based on Real-time Input'
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 46：创建一个基于实时输入进行预测的 Python 脚本
- en: In this exercise, we will first create a model and save it. We will then create
    a Python script that will make use of this saved model to perform predictions
    on the data input by the user.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将首先创建一个模型并保存它。然后，我们将创建一个 Python 脚本，利用这个保存的模型对用户输入的数据进行预测。
- en: Load the income dataset from Activity 14 as a pandas dataframe.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将活动 14 中的收入数据集加载为 pandas 数据框。
- en: '[PRE25]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Strip away all trailing spaces.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 去除所有尾随空格。
- en: '[PRE26]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Convert all the categorical variables from strings to integers using scikit.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 scikit 将所有分类变量从字符串转换为整数。
- en: '[PRE27]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Save the label encoder in a pickle file for future use. A pickle file stores
    Python objects so that we can access them later when we need them.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标签编码器保存到一个 pickle 文件中，以供将来使用。pickle 文件存储 Python 对象，以便我们在需要时可以访问它们。
- en: '[PRE28]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Split the dataset into training and testing and create the model.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集拆分为训练集和测试集并创建模型。
- en: Save the model to a file.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型保存到文件。
- en: '[PRE29]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In a Python script, load the model and the label encoder.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Python 脚本中，加载模型和标签编码器。
- en: '[PRE30]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Read the input from the user.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从用户那里读取输入。
- en: '[PRE31]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Create a dataframe to store this data.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据框来存储这些数据。
- en: '[PRE32]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Preprocess the data.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据。
- en: '[PRE33]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Convert into Dmatrix and perform prediction using the model.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换为 Dmatrix 并使用模型进行预测。
- en: '[PRE34]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Perform inverse transformation to get the results.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行逆变换以获取结果。
- en: '[PRE35]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.18: Inverse transformation output](img/C13322_05_18.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18：逆变换输出](img/C13322_05_18.jpg)'
- en: 'Figure 5.18: Inverse transformation output'
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.18：逆变换输出
- en: Note
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure that the values of `workclass` and `occupation` that you enter as
    input are present in the training data, otherwise the script will throw an error.
    This error occurs when the `LabelEncoder` encounters a new value it has not seen
    before.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你输入的 `workclass` 和 `occupation` 值在训练数据中存在，否则脚本会抛出错误。当 `LabelEncoder` 遇到它之前未见过的新值时，就会发生此错误。
- en: Congratulations! You built a script that predicts the outcome using user input
    data. You will now be able to deploy your models anywhere you want to.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你构建了一个使用用户输入数据进行预测的脚本。现在你可以将你的模型部署到任何你想要的地方。
- en: 'Activity 15: Predicting the Loss of Customers'
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 15：预测客户流失
- en: 'In this activity, we will attempt to predict whether a customer will move to
    another telecom provider. The data is sourced from IBM sample datasets. Let''s
    look at the following scenario: You work at a telecom company, and recently, a
    lot of your users have started moving to other providers. Now, to be able to give
    defecting customers a price cut, you need to predict which customer is the most
    likely to defect before they do so. To do this, you need to create a machine learning
    model that predicts which customer will defect.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将尝试预测一个客户是否会转移到另一个电信提供商。数据来自 IBM 示例数据集。让我们来看以下场景：你在一家电信公司工作，最近，许多用户开始转向其他提供商。现在，为了能够给流失的客户提供折扣，你需要预测哪些客户最有可能流失。为此，你需要创建一个机器学习模型，预测哪些客户会流失。
- en: 'Load the telecom churn (`telco-churn.csv`) dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data)
    using pandas. This dataset contains information about the customers of a telecom
    provider. The original source of the dataset is at: https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/.
    It contains multiple fields such as charges, tenure, and streaming information,
    along with a variable that tells us if the customer churned or not. The first
    few rows should look like this:![](img/C13322_05_19.jpg)'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 加载电信流失（`telco-churn.csv`）数据集（https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data）。该数据集包含有关电信服务提供商客户的信息。数据集的原始来源链接为：
    https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/。数据集中包含多个字段，如费用、服务期限、流媒体信息，以及一个变量表示客户是否流失。前几行数据应该如下所示：![](img/C13322_05_19.jpg)
- en: 'Figure 5.19: Screenshot showing first five elements of telecom churn dataset'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.19：显示电信流失数据集前五个元素的截图
- en: Remove unnecessary variables.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除不必要的变量。
- en: 'Convert all the categorical variables from strings to integers using scikit.
    You can use the following code: `data.TotalCharges = pd.to_numeric(data.TotalCharges,
    errors=''coerce'')`'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 scikit 将所有类别变量从字符串转换为整数。你可以使用以下代码： `data.TotalCharges = pd.to_numeric(data.TotalCharges,
    errors='coerce')`
- en: Fix the data type mismatch when loading with pandas.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修复使用 pandas 加载时的数据类型不匹配问题。
- en: Perform prediction using the XGBoost library and perform parameter tuning using
    cross-validation to improve accuracy to be more than 80%.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 XGBoost 库进行预测，并通过交叉验证进行参数调优，提升准确率至超过 80%。
- en: Save your model for future use.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存你的模型以供将来使用。
- en: Note
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 361.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第361页找到。
- en: Neural Networks
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'A neural network is one of the most popular machine learning algorithms available
    to data scientists. It has consistently outperformed traditional machine learning
    algorithms in problems where images or digital media are required to find the
    solution. Given enough data, it outperforms traditional machine learning algorithms
    in structured data problems. Neural networks that have more than 2 layers are
    referred to as deep neural networks and the process of using these "deep" networks
    to solve problems is referred to as deep learning. Two handle unstructured data
    there are two main types of neural networks: a **convolutional neural network**
    (**CNN**) can be used to process images and a **recurrent neural network** (**RNN**)
    can be used to process time series and natural language data. We will talk more
    about CNNs and RNNs in *Chapter 6*, *Decoding Images* and *Chapter 7*, *Processing
    Human Language*. Let us now see how a vanilla neural network really works. In
    this section, we will go over the different parts of a neural network in brief.
    We will explain each topic in detail in the following chapters.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是数据科学家使用的最流行的机器学习算法之一。它在需要图像或数字媒体来解决问题的场景中，始终优于传统的机器学习算法。在拥有足够数据的情况下，它在结构化数据问题中也超越了传统的机器学习算法。拥有超过两层的神经网络被称为深度神经网络，而利用这些“深度”网络解决问题的过程被称为深度学习。为了处理非结构化数据，神经网络有两种主要类型：**卷积神经网络**（**CNN**）可以用于处理图像，**递归神经网络**（**RNN**）可以用于处理时间序列和自然语言数据。我们将在*第6章*《解码图像》和*第7章*《处理人类语言》中深入讨论
    CNN 和 RNN。接下来，让我们看看一个普通的神经网络是如何工作的。在这一部分，我们将简要讲解神经网络的不同部分，接下来的章节会详细解释每个话题。
- en: What Is a Neural Network?
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是神经网络？
- en: The basic unit of a neural network is a neuron. The inspiration for neural networks
    was taken from the biological brain, which is where the name neuron was inspired
    from. All connections in the neural network, like the synapses in the brain, can
    transmit information from one neuron to another. In a neural network, a weighted
    combination of the input signal is aggregated, and the output signal is then transmitted
    forward after passing it through a function. This function is a nonlinear activation
    function and is the neuron's activation threshold. Multiple layers of these interconnected
    neurons form a neural network. Only the non-output layers of a neural network
    include bias units. The weights associated with every neuron along with these
    biases determine the output of the entire network; hence, these are the parameters
    we modify to fit the data during training.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的基本单元是神经元。神经网络的灵感来源于生物大脑，因此神经元这一名称也由此得来。神经网络中的所有连接，就像大脑中的突触一样，可以将信息从一个神经元传递到另一个神经元。在神经网络中，输入信号的加权组合会被汇聚，然后经过一个函数处理后将输出信号传递出去。这个函数是一个非线性激活函数，代表着神经元的激活阈值。多个相互连接的神经元层组成了一个神经网络。神经网络中只有非输出层包含偏置单元。与每个神经元相关的权重和这些偏置共同决定了整个网络的输出；因此，这些就是我们在训练过程中为了拟合数据而修改的参数。
- en: '![Figure 5.20: Representation of single layer neural network](img/C13322_05_20.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.20：单层神经网络的表示](img/C13322_05_20.jpg)'
- en: 'Figure 5.20: Representation of single layer neural network'
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.20：单层神经网络的表示
- en: The first layer of the neural network has nodes equal to the number of independent
    variables in the dataset. This layer is thus called the input layer, which is
    followed by multiple hidden layers, at the end of which is the output layer. Each
    neuron of the input layer takes in one independent variable of the dataset. The
    output layer outputs the final prediction. These outputs can be continuous (such
    as 0.2, 0.6, 0.8) if it is a regression problem or categorical (such as 2, 4,
    5) if it is a classification problem. The training of a neural network modifies
    the weights and biases of the network to minimize the error, which is the difference
    between the expected and the output values. Weights are multiplied with the input
    to the neuron and then the bias value is added to the combination of these weights
    to get the output.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的第一层包含的节点数与数据集中的独立变量数量相等。这个层被称为输入层，接下来是多个隐藏层，最后是输出层。输入层的每个神经元接收数据集中的一个独立变量。输出层则输出最终的预测结果。如果是回归问题，这些输出可以是连续的（如
    0.2、0.6、0.8）；如果是分类问题，则输出为分类标签（如 2、4、5）。神经网络的训练通过调整网络的权重和偏置来最小化误差，这个误差是期望值与输出值之间的差异。权重与输入相乘，偏置值则与这些权重的组合相加，最终得到输出。
- en: '![Figure 5.21: Neuron output](img/C13322_05_21.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.21：神经元输出](img/C13322_05_21.jpg)'
- en: 'Figure 5.21: Neuron output'
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.21：神经元输出
- en: Here, *y* is the output of the neuron and *x* the input, *w* and *b* are the
    weights and bias respectively, and *f* is the activation function, which we will
    learn more about later.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y* 是神经元的输出，*x* 是输入，*w* 和 *b* 分别是权重和偏置，*f* 是激活函数，我们将在后面进一步学习它。
- en: Optimization Algorithms
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化算法
- en: To minimize the error of the model, we train the neural network to minimize
    a predefined loss function using an optimization algorithm. There are many choices
    for this optimization algorithm, and you can choose one depending on your data
    and model. For most of this book, we will work with **stochastic gradient descent**
    (**SGD**), which works well in most cases, but we will explain other optimizers
    as and when they are required. SGD works by iteratively finding out the gradient,
    which is the change in the weights with respect to the error. In mathematical
    terms, it is the partial derivative with respect to the inputs. It finds the gradient
    that would help it minimize a given function, which in our case is called the
    loss function. As we get closer to the solution, this gradient reduces in magnitude,
    thus preventing us from overshooting the optimal solution.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化模型的误差，我们训练神经网络以最小化预定义的损失函数，使用优化算法。对于这个优化算法，有许多选择，具体选择哪个取决于你的数据和模型。在本书的大部分内容中，我们将使用**随机梯度下降（SGD）**，它在大多数情况下表现良好，但我们会在需要时解释其他优化器。SGD通过迭代地计算梯度来工作，梯度是权重相对于误差的变化。在数学上，它是相对于输入的偏导数。它找到的梯度有助于最小化给定的函数，在我们这个案例中就是损失函数。当我们接近解决方案时，梯度的大小会减小，从而防止我们超越最优解。
- en: The most intuitive way to understand SGD is the act of descending the bottom
    of a valley. Initially, we take steep descents, and then when we are close to
    the bottom, the slope reduces.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 理解SGD（随机梯度下降）最直观的方式是将其视为下坡的过程。最初，我们会采取陡峭的下降，然后当我们接近谷底时，坡度会变小。
- en: '![Figure 5.22: Intuition of gradient descent (k represents magnitude of gradient)](img/C13322_05_22.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.22：梯度下降的直观理解（k表示梯度的大小）](img/C13322_05_22.jpg)'
- en: 'Figure 5.22: Intuition of gradient descent (k represents magnitude of gradient)'
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.22：梯度下降的直观理解（k表示梯度的大小）
- en: Hyperparameters
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数
- en: A big parameter that determines the time required to train a model is called
    the **learning rate**, which essentially is the size of the step that we take
    to perform the descent. Too small a step, and it will take the model a long time
    to get to the optimal solution; too big, and it will overshoot the optimal solution.
    To circumvent this, we start with a large learning rate and reduce the learning
    rate after a few steps. This helps us reach the minimum point faster, and due
    to the reduction in step size, prevents the model from overshooting the solution.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 一个决定训练模型所需时间的重要参数叫做**学习率**，它本质上是我们进行下降时采取的步长。步长过小，模型需要很长时间才能找到最优解；步长过大，则会超越最优解。为了解决这个问题，我们从较大的学习率开始，并在进行几步后降低学习率。这有助于我们更快地达到最小值，并且由于步长的减小，防止模型超越解决方案。
- en: Next is the initialization of the weights. We need to perform initialization
    of the weights of a neural network to have a starting point from where we can
    then modify the weights to minimize the error. Initialization plays a major role
    in preventing the **vanishing** and **exploding gradient** problems.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是权重的初始化。我们需要对神经网络的权重进行初始化，以便从一个起始点开始，然后调整权重以最小化误差。初始化在防止**消失梯度**和**爆炸梯度**问题中起着重要作用。
- en: '**Vanishing gradient problem** refers to the reducing gradients with every
    layer as the product of any number smaller than 1 is even smaller, so over multiple
    layers, this value becomes 0.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**消失梯度问题**指的是每经过一层，梯度逐渐变小，因为任何小于1的数相乘会变得更小，因此经过多层后，这个值会变为0。'
- en: '**Exploding gradient problem** occurs when large error gradients add up and
    result in a very large update to the model. If the model loss goes to NaN, this
    could be a problem.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**爆炸梯度问题**发生在较大的误差梯度累加时，导致模型更新过大。如果模型的损失变为NaN，这可能是一个问题。'
- en: Using the **Xavier initialization**, we can prevent these problems as it factors
    the size of the network while initializing the weights. The Xavier initialization
    initializes the weights, drawing them from a truncated normal distribution centered
    on 0 with standard deviation
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**Xavier初始化**，我们可以防止这些问题，因为它在初始化权重时考虑了网络的大小。Xavier初始化通过从以0为中心的截断正态分布中抽取权重进行初始化，标准差为
- en: '![Figure 5.23: Xavier initialization](img/C13322_05_23.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.23：Xavier初始化](img/C13322_05_23.jpg)'
- en: 'Figure 5.23: Standard deviation that the Xavier initialization uses.'
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.23：Xavier初始化使用的标准差。
- en: Where xi is the number of input neurons and yi is the number of output neurons
    for that layer
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 其中xi是该层输入神经元的数量，yi是输出神经元的数量。
- en: This ensures that the variance of both the inputs and the outputs remains the
    same even if the number of layers in the network is very large.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了即使网络中的层数非常大，输入和输出的方差仍然保持不变。
- en: '**Loss Function**'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数**'
- en: Another hyperparameter to consider is the loss function. Different loss functions
    are used depending on the type of the problem, classification or regression. For
    classification, we choose loss functions such as cross entropy and hinge. For
    regression, we use loss functions such as mean squared error, mean absolute error
    (MAE), and Huber. Different functions work well with different datasets. We will
    go over them as we use them.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的超参数是损失函数。根据问题的类型，分类或回归，使用不同的损失函数。对于分类，我们选择交叉熵、铰链等损失函数；对于回归，我们使用均方误差、平均绝对误差（MAE）和Huber损失函数。不同的损失函数适用于不同的数据集。我们将在使用过程中逐步介绍它们。
- en: '**Activation Function**'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**'
- en: While creating the neural network layers, you will have to define an activation
    function, which depends on whether the layer is a hidden layer or an output layer.
    In the case of a hidden layer, we will use the ReLU or the tanh activation functions.
    Activation functions help the neural network model non-linear functions. Almost
    no real-life situation can be solved using a linear model. Now, apart from this,
    different activation functions have different features. Sigmoid output has values
    between 0 and 1, whereas tanh centers the output around 0, which enables better
    learning. ReLU on the other hand prevents the vanishing gradient problem and is
    computationally efficient. This is the representation of a ReLU graph.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建神经网络层时，您需要定义一个激活函数，这取决于该层是隐藏层还是输出层。对于隐藏层，我们将使用 ReLU 或 tanh 激活函数。激活函数帮助神经网络模型实现非线性函数。几乎没有实际问题可以通过线性模型来解决。除了这些，不同的激活函数具有不同的特点。Sigmoid
    输出的值在 0 和 1 之间，而 tanh 则将输出围绕 0 进行中心化，有助于更好的学习。ReLU 则可以防止梯度消失问题，并且在计算上更为高效。下面是
    ReLU 图形的表示。
- en: '![](img/C13322_05_24.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C13322_05_24.jpg)'
- en: 'Figure 5.24: Representation of ReLU activation function'
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.24：ReLU 激活函数的表示
- en: 'Softmax outputs probabilities and is used when multiclass classification is
    being performed, whereas sigmoid outputs a value between 0 and 1 and is used only
    for binary classification. Linear activation is mostly used for models that solve
    the regression problem. A representation of the sigmoid activation function is
    shown in the following figure:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 输出的是概率，用于多分类问题，而 sigmoid 输出的是介于 0 和 1 之间的值，仅用于二分类问题。线性激活通常用于解决回归问题的模型。下图展示了
    sigmoid 激活函数的表示：
- en: '![Figure 5.25: Representation of sigmoid activation function](img/C13322_05_25.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.25：Sigmoid 激活函数的表示](img/C13322_05_25.jpg)'
- en: 'Figure 5.25: Representation of sigmoid activation function'
  id: totrans-290
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.25：Sigmoid 激活函数的表示
- en: The previous section had a lot of new information; if you are confused, do not
    worry. We will apply all these concepts practically in the rest of the chapters,
    which will reinforce all these topics.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 上一部分介绍了很多新信息；如果您感到困惑，不必担心。我们将在接下来的章节中实践所有这些概念，这将加强对这些主题的理解。
- en: Keras
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras
- en: Keras is an open-source, high-level neural network API written in Python. It
    is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit (CNTK),
    or Theano. Keras was developed to enable fast experimentation and thus help in
    rapid application development. Using Keras, one can get from idea to result with
    the least possible delay. Keras supports almost all the latest data science models
    relating to neural networks due to the huge community support. It contains multiple
    implementations of commonly used building blocks such as layers, batch normalization,
    dropout, objective functions, activation functions, and optimizers. Also, Keras
    allows users to create models for smartphones (Android and iOS), the web, or for
    the **Java Virtual Machine** (**JVM**). With Keras, you can train your models
    on your GPU without any change in code.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个用 Python 编写的开源高级神经网络 API，能够在 TensorFlow、微软认知工具包（CNTK）或 Theano 上运行。Keras
    的开发旨在快速实验，从而帮助加速应用程序开发。使用 Keras，用户可以以最短的延迟从创意到结果。由于强大的社区支持，Keras 支持几乎所有最新的数据科学模型，尤其是与神经网络相关的模型。它包含了多种常用构建块的实现，如层、批量归一化、Dropout、目标函数、激活函数和优化器。同时，Keras
    允许用户为智能手机（Android 和 iOS）、Web 或 **Java 虚拟机** (**JVM**) 创建模型。使用 Keras，您可以在 GPU 上训练模型，而无需更改代码。
- en: Given all these features of Keras, it is imperative for data scientists to learn
    how to use all the different aspects of the library. Mastering the use of Keras
    will help you tremendously in your journey as a data scientist. To demonstrate
    the power of Keras, we will now install it and create a single layer neural network
    model.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 Keras 的所有这些特性，数据科学家必须学习如何使用该库的各个方面。掌握 Keras 的使用将极大地帮助你成为一名优秀的数据科学家。为了展示 Keras
    的强大功能，我们将安装它并创建一个单层神经网络模型。
- en: Note
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read more about Keras here: https://keras.io/'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于 Keras 的信息：[https://keras.io/](https://keras.io/)
- en: 'Exercise 47: Installing the Keras library for Python and Using it to Perform
    Classification'
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 47：为 Python 安装 Keras 库并使用它进行分类
- en: In this exercise, we will perform classification on the wholesale customer dataset
    (which we used in *Exercise 44*), using the Keras library for Python.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 Python 的 Keras 库对批发客户数据集（我们在*练习 44*中使用的那个）进行分类。
- en: Run the following command in your virtual environment to install Keras.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在虚拟环境中运行以下命令以安装 Keras。
- en: '[PRE36]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Open Jupyter Notebook from your virtual environment.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从你的虚拟环境中打开 Jupyter Notebook。
- en: Import Keras and other required libraries.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 Keras 和其他所需的库。
- en: '[PRE37]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Read the wholesale customer dataset using pandas and check to see if it was
    loaded successfully using the following command:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 读取批发客户数据集，并使用以下命令检查数据是否成功加载：
- en: '[PRE38]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output should look like this:'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该是这样的：
- en: '![Figure 5.26: Screenshot showing first five elements of dataset](img/C13322_05_26.jpg)'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.26：显示数据集前五个元素的截图](img/C13322_05_26.jpg)'
- en: 'Figure 5.26: Screenshot showing first five elements of dataset'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.26：显示数据集前五个元素的截图
- en: Split the data into features and labels.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为特征和标签。
- en: '[PRE39]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Create training and test sets.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练集和测试集。
- en: '[PRE40]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Create the neural network model.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建神经网络模型。
- en: '[PRE41]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Here, we create a four-layer network, with one input layer, two hidden layers,
    and one output layer. The hidden layers have ReLU activation and the output layer
    has softmax activation.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个四层网络，其中包含一个输入层、两个隐藏层和一个输出层。隐藏层使用 ReLU 激活函数，输出层使用 softmax 激活函数。
- en: Compile and train the model. We use the binary cross-entropy loss function,
    which is the same as the logloss we discussed before; we have chosen the optimizer
    to be stochastic gradient descent. We run the training for five epochs with a
    batch size of eight.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并训练模型。我们使用的是二元交叉熵损失函数，这与我们之前讨论的 logloss 相同；我们选择了随机梯度下降作为优化器。我们将训练运行五个 epoch，批量大小为八。
- en: '[PRE42]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You will see the model training log. Epoch refers to the training iteration,
    and 352 refers to the size of the dataset divided by the batch size. After the
    progress bar, you can see the time taken for one iteration. Next to that, you
    see the average time taken to train each batch. Next comes the loss of the model,
    which over here is the binary cross-entropy loss, followed by the accuracy after
    the iteration. A few of these terms are new, but we will understand each of them
    in the following sections.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将看到模型训练日志。Epoch 表示训练的迭代次数，352 表示数据集的大小除以批量大小。在进度条之后，你可以看到一次迭代所用的时间。接下来，你会看到训练每个批次所需的平均时间。接下来是模型的损失，这里是二元交叉熵损失，然后是迭代后的准确度。这些术语中的一些是新的，但我们将在接下来的章节中逐一了解它们。
- en: '![Figure 5.27: Screenshot of model training logs](img/C13322_05_27.jpg)'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.27：模型训练日志截图](img/C13322_05_27.jpg)'
- en: 'Figure 5.27: Screenshot of model training logs'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.27：模型训练日志截图
- en: Predict the values of the test set.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测测试集的值。
- en: '[PRE43]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Obtain the accuracy of the model.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取模型的准确度。
- en: '[PRE44]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.28: Output accuracy](img/C13322_05_28.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.28：输出准确度](img/C13322_05_28.jpg)'
- en: 'Figure 5.28: Output accuracy'
  id: totrans-328
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.28：输出准确度
- en: Congratulations! You just made your first neural network model with around 81%
    accuracy, without any fine-tuning! You will notice that this accuracy is quite
    low when compared with XGBoost. In the following sections, you will figure out
    how to improve this accuracy. A major reason for the low accuracy is the size
    of the data. For a neural network model to really shine, it must have a large
    dataset to train on; otherwise, it overfits the data.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚成功创建了第一个神经网络模型，准确率约为 81%，而且没有进行任何微调！你会注意到，与 XGBoost 相比，这个准确率相当低。在接下来的章节中，你将学习如何提高这个准确度。准确率较低的一个主要原因是数据的大小。为了让神经网络模型真正发挥作用，它必须有一个大型的数据集来进行训练，否则会出现过拟合。
- en: Keras Library
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keras 库
- en: Keras enables modularity. All initializers, cost functions, optimizers, layers,
    regularizers, and activation functions are standalone modules that can be used
    for any type of data and network architecture. You will find almost all the latest
    functions already implemented in Keras. This allows reusability of code and enables
    fast experimentation. You as a data scientist are not limited by the inbuilt modules;
    it is extremely easy to create your own custom modules and use them with other
    inbuilt modules. This enables research and helps with different use cases. For
    example, you might have to write a custom loss function to maximize the volume
    of cars sold, giving more weight to cars that have bigger margins, leading to
    higher profits.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 使得模块化成为可能。所有的初始化器、损失函数、优化器、层、正则化器和激活函数都是独立的模块，可以用于任何类型的数据和网络架构。你会发现几乎所有最新的函数都已经在
    Keras 中实现。这使得代码的可重用性和快速实验成为可能。作为数据科学家，你不会受到内建模块的限制；创建自己的自定义模块并与其他内建模块一起使用是非常简单的。这促进了研究并有助于不同的应用场景。例如，你可能需要编写一个自定义的损失函数来最大化汽车销量，并对利润更高的汽车赋予更大的权重，从而提高整体利润。
- en: All the different kinds of layers that you would need to create a neural network
    are defined in Keras. We will investigate them as we use them. There are two main
    ways to create neural models in Keras, the sequential model and the functional
    API.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 所有你需要创建神经网络的不同种类的层在 Keras 中都有定义。我们将在使用它们时进行探讨。在 Keras 中创建神经模型的主要方式有两种，顺序模型和功能性
    API。
- en: '**Sequential**: The **sequential model** is a linear stack of layers. This
    is the easiest way to create neural network models with Keras. A snippet of this
    model is given below:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '**顺序模型**：**顺序模型**是一个层的线性堆叠。这是使用 Keras 创建神经网络模型的最简单方式。下面给出了该模型的代码片段：'
- en: '[PRE45]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Functional API**: **Functional API** is the way to go for complex models.
    Due to the linear nature of the sequential model, creating a complex model is
    not possible. The functional API lets you create multiple parts of the model and
    then merge them together. The same model in the functional API is given below:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**功能性 API**：**功能性 API** 是构建复杂模型的方式。由于顺序模型是线性的，无法创建复杂模型。功能性 API 让你能够创建模型的多个部分，然后将它们合并在一起。通过功能性
    API 创建的相同模型如下所示：'
- en: '[PRE46]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: A powerful feature of Keras is callback. Callbacks allow you to use a function
    at any stage of the training process. This proves to be useful to get the statistics
    and save the model at different stages. It can be used to apply a custom decay
    to the learning rate and also to perform early stopping.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的一个强大功能是回调函数。回调函数允许你在训练过程的任何阶段使用函数。这对于获取统计信息和在不同阶段保存模型非常有用。它还可以用来对学习率应用自定义衰减，并执行提前停止。
- en: '[PRE47]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To save models you trained on Keras, you need to just use the following line
    of code:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存你在 Keras 上训练的模型，你只需要使用以下代码：
- en: '[PRE48]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'To load the model from a file, use the following code:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 要从文件中加载模型，使用以下代码：
- en: '[PRE49]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Early stopping is a useful feature that can be implemented using callbacks.
    Early stopping helps you save time when training models. It stops the training
    process if the change in the metric specified is less than the set threshold.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止是一个可以通过回调函数实现的有用功能。提前停止有助于节省训练模型的时间。如果指定的指标变化小于设定的阈值，训练过程将停止。
- en: '[PRE50]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The callback mentioned above stops training if the change in the validation
    loss is less than 0.01 for five epochs.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 上述回调函数将在验证损失的变化小于 0.01 且持续五个周期时停止训练。
- en: Note
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Always use `ModelCheckpoint` to store the model state. This is especially important
    for larger datasets and larger networks.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 始终使用 `ModelCheckpoint` 来存储模型状态。对于大型数据集和更大的网络，这一点尤为重要。
- en: 'Exercise 48: Predicting Avocado Price Using Neural Networks'
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 48：使用神经网络预测鳄梨价格
- en: Let us apply the knowledge that we received in this section to create an excellent
    neural network model that will predict the price of different kinds of avocados.
    The dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data)
    contains information such as average price of the produce, volume of the produce,
    region where the avocado was produced, and size of the bags that were used. It
    also has a few unknown variables that might help us with the model.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用本节所学的知识，创建一个优秀的神经网络模型，用于预测不同种类鳄梨的价格。数据集（https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data）包含了如产物的平均价格、产物的体积、鳄梨生产区域、以及所使用袋子大小等信息。它还包含一些未知变量，可能对我们的模型有所帮助。
- en: Note
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Original source site: [www.hassavocadoboard.com/retail/volume-and-price-data](http://www.hassavocadoboard.com/retail/volume-and-price-data)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 原始来源网站：[www.hassavocadoboard.com/retail/volume-and-price-data](http://www.hassavocadoboard.com/retail/volume-and-price-data)
- en: 'Import the avocado dataset and observe the columns. You will see something
    like this:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入鳄梨数据集并观察列。你将看到如下内容：
- en: '[PRE51]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![Figure 5.29: Screenshot showing avocado dataset](img/C13322_05_29.jpg)'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.29：展示鳄梨数据集的截图](img/C13322_05_29.jpg)'
- en: 'Figure 5.29: Screenshot showing avocado dataset'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.29：展示鳄梨数据集的截图
- en: Look through the data and split the date column into days and months. This will
    help us catch the seasonality while ignoring the year. Now, drop the date and
    unnamed columns.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浏览数据并将日期列拆分为天和月。这将帮助我们捕捉季节性变化，同时忽略年份。现在，删除日期和未命名的列。
- en: '[PRE52]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Encode the categorical variables using the `LabelEncoder` so that Keras can
    use it to train the model.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`LabelEncoder`对分类变量进行编码，以便Keras可以使用它来训练模型。
- en: '[PRE53]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Split the data into training and testing sets.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集。
- en: '[PRE54]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Use callbacks to save the model whenever the loss improves and for early stopping
    of the model if it starts performing poorly.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用回调函数在损失改善时保存模型，并在模型表现不佳时进行早停。
- en: '[PRE55]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Create a neural network model. Here, we make use of the same model as before.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建神经网络模型。这里，我们使用与之前相同的模型。
- en: '[PRE56]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Train and evaluate the model to get the MSE of the model.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练并评估模型，以获取模型的MSE。
- en: '[PRE57]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Check the final output in the following screenshot:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看下方截图中的最终输出：
- en: '![Figure 5.30: MSE of model](img/C13322_05_30.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.30：模型的MSE](img/C13322_05_30.jpg)'
- en: 'Figure 5.30: MSE of model'
  id: totrans-370
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.30：模型的MSE
- en: Congratulations! You have just trained your neural network to get a reasonable
    error on the avocado dataset. The value shown above is the mean square error of
    the model. Modify some hyperparameters and use the rest of the data to see if
    you can get a better error score. Make use of the information provided in the
    previous sections.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚训练了你的神经网络，得到了合理的误差值，应用于鳄梨数据集。上面显示的值是模型的均方误差。修改一些超参数并使用剩余数据，看看能否获得更好的误差分数。利用前面章节提供的信息。
- en: Note
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: A decrease in MSE is favorable. The most optimal value will depend on the situation.
    For example, while predicting the speed of a car, values less than 100 are ideal,
    whereas when predicting the GDP of a country, an MSE of 1000 is good enough.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: MSE（均方误差）下降是理想的。最优值将取决于具体情况。例如，在预测汽车速度时，低于100的值是理想的，而在预测一个国家的GDP时，1000的MSE已经足够好。
- en: Categorical Variables
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类变量
- en: A categorical variable is one whose values can be represented in different categories.
    Examples are colours of a ball, breed of dogs, and zip codes. Mapping these categorical
    variables in a single dimension creates a sort of dependence on each other, which
    is incorrect. Even though these categorical variables do not have an order or
    dependence, inputting them to a neural network as a single feature makes the neural
    network create dependence between these variables depending on the order, whereas
    in reality, the order does not mean anything. In this section, we will learn about
    the ways in which can fix this issue and train effective models.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 分类变量是其值可以表示为不同类别的变量。例如，球的颜色、狗的品种和邮政编码等。将这些分类变量映射到单一维度会导致它们之间相互依赖，这是不正确的。尽管这些分类变量没有顺序或依赖关系，但将它们作为单一特征输入神经网络时，神经网络会根据顺序在这些变量之间创建依赖关系，而实际上，顺序并不代表任何意义。在本节中，我们将学习如何解决这个问题并训练有效的模型。
- en: One-hot Encoding
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独热编码
- en: The easiest and the most widely used method of mapping categorical variables
    is to use one-hot encoding. Using this method, we convert a categorical feature
    into features equal to the number of categories in the feature.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 映射分类变量的最简单且最广泛使用的方法是使用独热编码。使用此方法，我们将一个分类特征转换为等于特征中类别数量的多个特征。
- en: '![Figure 5.31: Categorical feature conversion](img/C13322_05_31.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.31：分类特征转换](img/C13322_05_31.jpg)'
- en: 'Figure 5.31: Categorical feature conversion'
  id: totrans-379
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.31：分类特征转换
- en: 'Use the following steps to convert categorical variables into one-hot encoded
    variables:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下步骤将分类变量转换为独热编码变量：
- en: Convert the data into a number if represented as a data type other than int.
    There are two ways to do this
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果数据类型不是整数，则将数据转换为数字。有两种方法可以做到这一点。
- en: You can directly use the `LabelEncoder` method of sklearn.
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以直接使用sklearn的`LabelEncoder`方法。
- en: 'Create bins to reduce the number of categories. The higher the number of categories,
    the more difficult it is for the model. You can choose an integer to represent
    each bin. Do keep in mind that doing this will lead to a loss in information and
    might result in a bad model. You can perform histogram binning using the following
    rule:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建箱子以减少类别的数量。类别的数量越高，模型的难度越大。你可以选择一个整数来表示每个箱子。请记住，进行这种操作会导致信息的丧失，并可能导致模型效果不佳。你可以使用以下规则进行直方图分箱：
- en: If the number of categorical columns is less than 25, use 5 bins.
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果分类列的数量少于25，使用5个箱子。
- en: If it is between 25 and 100, use n/5 bins, where n is the number of categorical
    columns, and if it is more than 100, use 10 * log (n) bins.
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果分类列的数量在25到100之间，使用n/5个箱子，其中n是分类列的数量；如果超过100，使用10 * log (n)个箱子。
- en: Note
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can combine the categories with frequencies smaller than 5% into one category.
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以将频率小于5%的类别合并为一个类别。
- en: Convert the numerical array from Step 1 into a one-hot vector using the `get_dummies`
    function of pandas.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas的`get_dummies`函数将步骤1中的数值数组转换为一热向量。
- en: '![Figure 5.32: Output of get_dummies function](img/C13322_05_32.jpg)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![图5.32：get_dummies函数的输出](img/C13322_05_32.jpg)'
- en: 'Figure 5.32: Output of get_dummies function'
  id: totrans-390
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.32：get_dummies函数的输出
- en: 'There are two main reasons why one-hot encoding isn''t the best way to use
    categorical data:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码不是使用分类数据的最佳方法，主要有两个原因：
- en: Different values of the categorical variables are assumed to be completely independent
    of each other. This leads to a loss of information about the relationship between
    them.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设分类变量的不同值是完全独立的。这会导致它们之间关系信息的丧失。
- en: Categorical variables with many categories result in a very computationally
    expensive model. Wider datasets require more data points to make meaningful models.
    This is known as the curse of dimensionality.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有许多类别的分类变量会导致一个计算成本非常高的模型。数据集越宽广，生成有意义的模型所需的数据点就越多。这就是所谓的维度灾难。
- en: To work through these issues, we will use entity embedding.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们将使用实体嵌入。
- en: Entity Embedding
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实体嵌入
- en: 'Entity embedding represents categorical features in a multidimensional space.
    This ensures that the network learns the correct relationship between the different
    categories of a feature. The dimensions of this multidimensional space do not
    represent anything specific; it could be anything that the model deems fit to
    learn. For example, in the case of the days of a week, one dimension could be
    whether the day is a weekday or not and another could be the distance from a weekday.
    This method has been inspired from word embedding that is used in natural language
    processing to learn the semantic similarities between words and phrases. Creating
    an embedding can help teach the neural networks how Friday is different from Wednesday
    or how a puppy and a dog are different. For example, a four-dimensional embedding
    matrix for the days of the week will look something like this:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 实体嵌入将分类特征表示在一个多维空间中。这确保了网络学习不同类别之间的正确关系。这个多维空间的维度没有特定的意义；它可以是模型认为合适学习的任何内容。例如，在一周的日子中，一维可以表示是否是工作日，另一维可以表示与工作日的距离。这种方法的灵感来源于自然语言处理中的词嵌入，用于学习词汇和短语之间的语义相似性。创建嵌入可以帮助教会神经网络如何区分星期五和星期三，或者小狗和狗的区别。例如，一周七天的四维嵌入矩阵可能如下所示：
- en: '![Figure 5.33: Four-dimensional embedding matrix'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.33：四维嵌入矩阵](img/C13322_05_33.jpg)'
- en: '](img/C13322_05_33.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_05_33.jpg)'
- en: 'Figure 5.33: Four-dimensional embedding matrix'
  id: totrans-399
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.33：四维嵌入矩阵
- en: 'From the above matrix, you can see that the embedding learns the dependence
    between the categories: it knows that Saturday and Sunday are more similar than
    Thursday and Friday, as the vectors for Saturday and Sunday are similar. Entity
    embedding gives a big edge when you have a lot of categorical variables in your
    dataset. To create entity embedding in Keras, you can use the embedding layer.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的矩阵中，你可以看到嵌入学习了类别之间的依赖关系：它知道周六和周日比周四和周五更相似，因为周六和周日的向量是相似的。当数据集中有大量分类变量时，实体嵌入会提供巨大的优势。要在Keras中创建实体嵌入，你可以使用嵌入层。
- en: Note
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Always try to use word embedding as it gives the best result.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 总是尽量使用词嵌入，因为它能提供最佳的结果。
- en: 'Exercise 49: Predicting Avocado Price Using Entity Embedding'
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习49：使用实体嵌入预测鳄梨价格
- en: Let us use the knowledge of entity embedding to predict the avocado price by
    creating a better neural network model. We will use the same avocado dataset from
    before.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用实体嵌入的知识，通过创建一个更好的神经网络模型来预测鳄梨价格。我们将使用之前的鳄梨数据集。
- en: Import the avocado price dataset and check for null values. Split the date column
    into month and day columns.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入鳄梨价格数据集并检查是否有空值。将日期列拆分为月份和日期列。
- en: '[PRE58]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Encode the categorical variables.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对分类变量进行编码。
- en: '[PRE59]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Split the data into training and testing sets.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集。
- en: '[PRE60]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Create a dictionary that maps categorical column names to the unique values
    in them.
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个字典，将分类列名映射到其中的唯一值。
- en: '[PRE61]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Next, get the input data in the format that the embedding neural network will
    accept.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，获取嵌入神经网络可以接受的输入数据格式。
- en: '[PRE62]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Here, what we are doing is creating a list of arrays of all the variables.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们要做的是创建一个包含所有变量的数组列表。
- en: Next, create a dictionary that will store the output dimensions of the embedding
    layers. This is the number of values the variable will be denoted by. You must
    get to the right number with trial and error.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个字典，用于存储嵌入层的输出维度。这是变量所表示的值的数量。你必须通过反复试验来确定正确的数字。
- en: '[PRE63]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Now, create the embedding layers for the categorical variables. In each iteration
    of the loop, we create one embedding layer for the categorical variable.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为分类变量创建嵌入层。在循环的每次迭代中，我们为分类变量创建一个嵌入层。
- en: '[PRE64]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Now, add the continuous variable to the network and complete the model.
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将连续变量添加到网络中并完成模型。
- en: '[PRE65]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Train the model with the train_input_list, which we created in Step 5 for 50
    epochs.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用我们在第5步创建的train_input_list训练模型，训练50个epoch。
- en: '[PRE66]'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Now, get the weights from the embedding layers to visualize the embedding.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，从嵌入层获取权重以可视化嵌入。
- en: '[PRE67]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Perform PCA and plot the output using the region labels (which you can get by
    performing inverse transform on the dictionary that we created earlier). PCA displays
    similar data points closer, by reducing the dimensionality to two dimensions.
    Here, we plot only the first 25 regions.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行PCA，并使用区域标签绘制输出（你可以通过对我们之前创建的字典执行逆变换获得区域标签）。PCA通过将维度降至二维，将相似的数据点聚集在一起。在这里，我们仅绘制前25个区域。
- en: You can plot all of them if you want.
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想的话，可以绘制所有它们。
- en: '[PRE68]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![Figure 5.34: Graphical representation of avocado growing region using entity
    embedding](img/C13322_05_34.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![图5.34：使用实体嵌入表示鳄梨生长区域的图形](img/C13322_05_34.jpg)'
- en: 'Figure 5.34: Graphical representation of avocado growing region using entity
    embedding'
  id: totrans-430
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.34：使用实体嵌入表示鳄梨生长区域的图形
- en: Congratulations! You improved the accuracy of your model by using entity embedding.
    As you can see from the embedding plot, the model was able to figure out the regions
    with high and low average prices. You can plot the embedding of other variables
    to see what relationships the network makes from the data. Also, try to improve
    the accuracy of this model through hyperparameter tuning.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你通过使用实体嵌入提高了模型的准确性。从嵌入图中可以看出，模型能够识别出平均价格高和低的区域。你可以绘制其他变量的嵌入，看看网络从数据中得出了什么关系。另外，尝试通过超参数调优来提高模型的准确性。
- en: 'Activity 16: Predicting a Customer''s Purchase Amount'
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动16：预测顾客的购买金额
- en: 'In this activity, we will attempt to predict the amount a customer will spend
    on a product category. The dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data)
    contains transactions made in a retail store. Let''s look at the following scenario:
    You work at a big retail chain and want to predict which kind of customer will
    spend how much money on a particular product category. Doing this will help your
    frontline staff recommend the right kind of products to customers, thus increasing
    sales and customer satisfaction. To do this, you need to create a machine learning
    model that predicts the purchase value of a transaction.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将尝试预测顾客在某一产品类别上的消费金额。数据集（https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data）包含零售店中的交易记录。让我们来看一个场景：你在一家大型零售连锁店工作，想预测哪种类型的顾客会在某个特定产品类别上花费多少钱。这样做将帮助你的前线员工向顾客推荐合适的产品，从而提高销售额和顾客满意度。为此，你需要创建一个机器学习模型，预测每笔交易的购买金额。
- en: 'Load the Black Friday dataset using pandas. This dataset is a collection of
    transactions made in a retail store. The information it contains is the age, city,
    marital status of the customer, product category of the item being bought, and
    bill amount. The first few rows should look like this:![Figure 5.35: Screenshot
    showing first five elements of Black Friday dataset ](img/C13322_05_35.jpg)'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas加载Black Friday数据集。该数据集是一个零售店的交易记录集合。它包含的信息包括客户的年龄、城市、婚姻状况、购买商品的类别以及账单金额。前几行应该像这样：![图5.35：显示Black
    Friday数据集前五个元素的截图](img/C13322_05_35.jpg)
- en: 'Figure 5.35: Screenshot showing first five elements of Black Friday dataset'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.35：显示Black Friday数据集前五个元素的截图
- en: Remove unnecessary variables and null values. Remove `Product_Category_2` and
    `Product_Category_3` columns.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 移除不必要的变量和空值。移除`Product_Category_2`和`Product_Category_3`列。
- en: Encode all the categorical variables.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有分类变量进行编码。
- en: Perform prediction by creating a neural network with the Keras library. Make
    use of entity embedding and perform hyperparameter tuning.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用Keras库创建神经网络进行预测。利用实体嵌入并进行超参数调优。
- en: Save your model for future use.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存你的模型以便日后使用。
- en: Note
  id: totrans-440
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 364.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第364页找到。
- en: Summary
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learnt how to create highly accurate structured data models
    and understood what XGBoost is and how to use the library to train models, Before
    we started, we were wondering what neural networks are and how to use the Keras
    library to train models. After learning about neural networks, we moved to handling
    categorical data. Finally, we learned what cross-validation is and how to use
    it.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了如何创建高精度的结构化数据模型，了解了XGBoost是什么，以及如何使用该库来训练模型。在开始之前，我们曾经想知道什么是神经网络，以及如何使用Keras库来训练模型。了解了神经网络后，我们开始处理分类数据。最后，我们学习了什么是交叉验证以及如何使用它。
- en: Now that you have completed this chapter, you can handle any kind of structured
    data and creating machine learning models with it. In the next chapter, you will
    learn how to create neural network models for image data.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完成了这一章的内容，你可以处理任何类型的结构化数据，并用它创建机器学习模型。在下一章，你将学习如何为图像数据创建神经网络模型。
