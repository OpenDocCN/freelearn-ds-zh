- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Supervised Machine Learning on Network Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络数据上的监督学习
- en: In previous chapters, we spent a lot of time exploring how to collect text data
    from the internet, transform it into network data, visualize networks, and analyze
    networks. We were able to use centralities and various network metrics for additional
    contextual awareness about individual nodes’ placement and influence in networks,
    and we used community detection algorithms to identify the various communities
    that exist in a network.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们花费了大量时间探讨如何从互联网上收集文本数据，将其转换为网络数据，进行网络可视化，并分析网络。我们能够使用中心性和各种网络指标来获得有关单个节点在网络中位置和影响力的更多上下文信息，并使用社区检测算法来识别网络中存在的各种社区。
- en: In this chapter, we are going to begin an exploration of how network data can
    be useful in **machine learning** (**ML**). As this is a data science and network
    science book, I expect that many readers will be familiar with ML, but I’ll give
    a very quick explanation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始探索网络数据如何在**机器学习**（**ML**）中发挥作用。由于这是一本数据科学和网络科学的书，我预计许多读者已经对机器学习有所了解，但我会给出一个非常简短的解释。
- en: 'This chapter is composed of the following sections:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括以下几个部分：
- en: Introducing ML
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入机器学习
- en: Beginning with ML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从机器学习开始
- en: Data preparation and feature engineering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备和特征工程
- en: Selecting a model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择模型
- en: Preparing the data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据
- en: Training and validating the model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: Model insights
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型洞察
- en: Other use cases
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他应用案例
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using the Python libraries NetworkX, pandas, and
    scikit-learn. These libraries should be installed by now, so they should be ready
    for your use. If they are not installed, you can install Python libraries with
    the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Python库NetworkX、pandas和scikit-learn。现在这些库应该已经安装好了，因此可以随时使用。如果没有安装，你可以通过以下方式安装Python库：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For instance, to install NetworkX, you would do this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要安装NetworkX，你可以执行以下命令：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In [*Chapter 4*](B17105_04.xhtml#_idTextAnchor158), we also introduced a `draw_graph()`
    function that uses both NetworkX and `scikit-network`. You will need that code
    any time that we do network visualization. Keep it handy!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B17105_04.xhtml#_idTextAnchor158)中，我们还介绍了一个`draw_graph()`函数，利用了NetworkX和`scikit-network`库。每次进行网络可视化时，你都需要使用这段代码。记得保留它！
- en: 'The code for this chapter is on GitHub: [https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在GitHub上找到：[https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python)。
- en: Introducing ML
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入机器学习
- en: 'ML is a set of techniques that enable computers to learn from patterns and
    behavior in data. It is often said that there are three different kinds of ML:
    **Supervised**, **Unsupervised**, and **Reinforcement** learning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是一组技术，能够使计算机从数据中的模式和行为中学习。通常说，机器学习有三种不同的类型：**监督学习**、**无监督学习**和**强化学习**。
- en: In supervised ML, an answer – called a **label** – is provided with the data
    to allow for an ML model to learn the patterns that will allow it to predict the
    correct answer. To put it simply, you give the model data *and* an answer, and
    it figures out how to predict correctly.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，数据会附带一个答案——称为**标签**——以便让机器学习模型学习那些可以帮助其预测正确答案的模式。简单来说，你给模型提供数据*和*答案，然后它会弄清楚如何做出正确预测。
- en: In unsupervised ML, no answer is provided to the model. The goal is usually
    to find clusters of similar pieces of data. For instance, you could use clustering
    to identify the different types of news articles present in a dataset of news
    articles, or to find topics that exist in a corpus of text. This is similar to
    what we have done with community detection.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，模型不会被提供答案。目标通常是找出相似数据的聚类。例如，你可以使用聚类算法来识别数据集中不同类型的新闻文章，或在文本语料库中找出不同的主题。这与我们做的社区检测工作类似。
- en: In reinforcement learning, a model is given a goal and it gradually learns how
    to get to this goal. In many reinforcement learning demos, you’ll see a model
    play pong or another video game, or learn to walk.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，模型会被赋予一个目标，并逐渐学习如何达到这个目标。在许多强化学习演示中，你会看到模型玩乒乓球或其他视频游戏，或者学习走路。
- en: These are ultra-simplistic descriptions of the types of ML, and there are more
    variations (**semi-supervised** and so on). ML is a rich topic, so I encourage
    you to find books if this chapter interests you. For me, it pushed NLP into an
    obsession.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是机器学习类型的极简描述，还有更多的变体（**半监督学习**等）。机器学习是一个广泛的主题，因此如果本章内容让你感兴趣，我鼓励你查阅相关书籍。对我来说，它使自然语言处理成为了一种痴迷。
- en: Beginning with ML
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始机器学习
- en: There are many guides and books on how to do sentiment analysis using NLP. There
    are very few guides and books that give a step-by-step demonstration of how to
    convert graph data into a format that can be used for classification with ML.
    In this chapter, you will see how to use graph data for ML classification.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多关于如何使用自然语言处理（NLP）进行情感分析的指南和书籍，但关于如何将图数据转换成可以用于机器学习分类格式的指南和书籍却少之又少。在本章中，你将看到如何使用图数据进行机器学习分类。
- en: 'For this exercise, I created a little game I’m calling “Spot the Revolutionary.”
    As with the last two chapters, we will be using the networkx *Les Miserables*
    network as it contains enough nodes and communities to be interesting. In previous
    chapters, I pointed out that the revolutionary community was densely connected.
    As a reminder, this is what it looks like:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我创建了一个名为“找出革命者”的小游戏。与前两章一样，我们将使用 *《悲惨世界》* 网络，因为它包含足够多的节点和社区，足够有趣。在前面的章节中，我指出革命者社区是高度连接的。作为提醒，这就是它的样子：
- en: '![Figure 10.1 – Les Miserables Les Amis de l’ABC network](img/B17105_10_001.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 《悲惨世界》ABC革命社群网络](img/B17105_10_001.jpg)'
- en: Figure 10.1 – Les Miserables Les Amis de l’ABC network
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 《悲惨世界》ABC革命社群网络
- en: Each member of the community is connected with each other member, for the most
    part. There are no connections with outside people.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 社区中的每个成员几乎都与其他成员相互连接。没有与外部人员的连接。
- en: Other parts of the network look different.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的其他部分看起来不同。
- en: '![Figure 10.2 – Les Miserables whole network](img/B17105_10_002.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – 《悲惨世界》全网](img/B17105_10_002.jpg)'
- en: Figure 10.2 – Les Miserables whole network
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 《悲惨世界》全网
- en: Even a visual inspection shows that in different parts of the network, nodes
    are are connecteded differently. The structure is different. In some places, connectivity
    resembles a star. In others, it resembles a mesh. Network metrics will give us
    these values, and ML models will be able to use them for prediction.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是视觉检查也能看出，在网络的不同部分，节点的连接方式不同，结构也有所差异。在某些地方，连接类似于星形；在其他地方，连接则像网格。网络指标将为我们提供这些值，机器学习模型可以使用它们进行预测。
- en: We are going to use these metrics to play a game of “Spot the Revolutionary.”
    This will be fun.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些指标来玩“找出革命者”的游戏。这会很有趣。
- en: Note
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: I will not be explaining ML in much depth, only giving a preview of its capabilities.
    If you are interested in data science or software engineering, I strongly recommend
    that you spend some time learning about ML. It is not only for academics, mathematicians,
    and scientists. It gets complicated, so a foundation in mathematics and statistics
    is strongly recommended, but you should feel free to explore the topic.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会深入解释机器学习，只会简要介绍它的能力。如果你对数据科学或软件工程感兴趣，我强烈建议你花时间学习机器学习。它不仅仅适用于学者、数学家和科学家。机器学习变得越来越复杂，因此强烈建议具备数学和统计学基础，但你完全可以自由探索这一主题。
- en: This chapter will not be a mathematics lesson. All work will be done via code.
    I’m going to show one example of using network data for classification. This is
    not the only use case. This is not at all the only use case for using network
    data with ML. I will also be showing only one model (Random Forest), not all available
    models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不会是一堂数学课。所有的工作都将通过代码完成。我将展示一个使用网络数据进行分类的例子。这不是唯一的应用场景，使用网络数据进行机器学习的应用远不止这些。我也只会展示一个模型（随机森林），而不是所有可用的模型。
- en: I’m also going to show that sometimes incorrect predictions can be as insightful
    as correct ones and how there are sometimes useful insights in model predictions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我还将展示有时不正确的预测和正确的预测一样具有启发性，并且有时模型预测中也包含有用的见解。
- en: I’m going to show the workflow to go from graph data to prediction and insights
    so that you can use this in your own experiments. You don’t need a graph **neural
    network** (**NN**) for everything. This is totally possible with simpler models,
    and they can give good insights, too.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我将展示从图数据到预测和见解的工作流程，以便你可以在自己的实验中使用这一方法。你并不需要每次都使用图神经网络（**NN**）。使用更简单的模型也是完全可能的，它们同样能够提供有价值的见解。
- en: Enough disclaimers. Let’s go.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 足够的免责声明。开始吧。
- en: Data preparation and feature engineering
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备和特征工程
- en: Before we can use ML, we first need to collect our data and convert it into
    a format that the model can use. We can’t just feed the graph G to Random Forest
    and call it a day. We could feed a graph’s adjacency matrix and a set of labels
    to Random Forest and it’d work, but I want to showcase some feature engineering
    that we can do.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以使用机器学习之前，首先需要收集数据，并将其转换为模型可以使用的格式。我们不能直接将图G传递给随机森林并就此结束。我们可以将图的邻接矩阵和一组标签传递给随机森林，它也能工作，但我想展示一些我们可以做的特征工程。
- en: 'Feature engineering is using domain knowledge to create additional features
    (most call them columns) that will be useful for our models. For instance, looking
    at the networks from the previous section, if we want to be able to spot the revolutionaries,
    then we may want to give our model additional data such as each node’s number
    of degrees (connections), betweenness centrality, closeness centrality, page rank,
    clustering, and triangles:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是利用领域知识创建额外的特征（大多数人称之为列），这些特征将对我们的模型有用。例如，回顾前一节中的网络，如果我们想能够识别革命者，我们可能希望为模型提供额外的数据，如每个节点的度数（连接数）、介数中心性、紧密中心性、页面排名、聚类系数和三角形：
- en: 'Let’s start by first building our network. This should be easy by now, as we
    have done this several times:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从先构建网络开始。现在应该很容易，因为我们已经做过好几次了：
- en: '[PRE2]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We took these steps in previous chapters, but as a reminder, the *Les Miserables*
    graph comes with edge weights, and I don’t want those. The first line loads the
    graph. The second line creates an edge list from the graph, dropping the edge
    weights. And the third line rebuilds the graph from the edge list, without weights.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的章节中已经采取了这些步骤，但提醒一下，*《悲惨世界》*图谱带有边权重，而我不需要这些。第一行加载图谱，第二行从图谱中创建边列表，去掉边权重，第三行则从边列表重建图谱，去掉权重。
- en: 'We should now have a useful graph. Let’s take a look:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在应该有一个有用的图谱。让我们来看看：
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'That produces the following graph:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图谱：
- en: '![Figure 10.3 – Les Miserables graph without node names](img/B17105_10_003.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – 《悲惨世界》图谱（不带节点名称）](img/B17105_10_003.jpg)'
- en: Figure 10.3 – Les Miserables graph without node names
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 《悲惨世界》图谱（不带节点名称）
- en: Looks good! I can clearly see that there are several different clusters of nodes
    in this network and that other parts of the network are more sparse.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！我可以清楚地看到这个网络中有几个不同的节点簇，而其他部分的网络则更为稀疏。
- en: But how do we convert this crazy tangled knot into something that an ML model
    can use? Well, we looked at centralities and other measures in previous chapters.
    So, we already have the foundation that we will use here. I’m going to create
    several DataFrames with the data that I want and then merge them together for
    use as training data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何将这个混乱的纠结结转化为机器学习模型可以使用的东西呢？好吧，我们在前几章中已经研究过中心性和其他度量。所以我们已经有了在这里使用的基础。我将创建几个包含我所需数据的数据框，然后将它们合并成训练数据。
- en: Degrees
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 度数
- en: 'The **degrees** are simply the number of connections that a node has with other
    nodes. We’ll grab that first:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**度数**只是一个节点与其他节点连接的数量。我们首先获取这个数据：'
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We get the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下输出：
- en: '![Figure 10.4 – Les Miserables feature engineering: degrees](img/B17105_10_004.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – 《悲惨世界》特征工程：度数](img/B17105_10_004.jpg)'
- en: 'Figure 10.4 – Les Miserables feature engineering: degrees'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 《悲惨世界》特征工程：度数
- en: Let’s move on to the next step.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们进入下一步。
- en: Clustering
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: Next, we’ll grab the clustering coefficient, which tells us how densely connected
    the nodes are around a given node. A value of 1.0 means that every node is connected
    to every other node. 0.0 means that no neighbor nodes are connected to other neighbor
    nodes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算聚类系数，它告诉我们节点周围的连接密度。值为1.0表示每个节点都与其他节点相连，值为0.0表示没有邻近节点与其他邻近节点连接。
- en: 'Let’s capture clustering:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来捕捉聚类：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This gives us the clustering output:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们聚类的输出：
- en: '![Figure 10.5 – Les Miserables feature engineering: clustering](img/B17105_10_005.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图10.5 – 《悲惨世界》特征工程：聚类](img/B17105_10_005.jpg)'
- en: 'Figure 10.5 – Les Miserables feature engineering: clustering'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – 《悲惨世界》特征工程：聚类
- en: This tells us that **MlleBaptistine** and **MmeMagloire** both are part of densely
    connected communities, meaning that these two also know the same people. **Napoleon**
    doesn’t have any overlap with other people, and neither does **CountessDeLo**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们 **MlleBaptistine** 和 **MmeMagloire** 都是高度连接的社区的一部分，这意味着这两个人也认识同样的人。**Napoleon**
    与其他人没有任何交集，**CountessDeLo** 也是如此。
- en: Triangles
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 三角形
- en: '`triangle_df` is a count of how many triangles a given node is a part of. If
    a node is part of many different triangles, it is connected with many nodes in
    a network:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`triangle_df` 是统计给定节点属于多少个三角形。如果一个节点属于许多不同的三角形，那么它与网络中的许多节点相连接：'
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives us the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们带来如下结果：
- en: '![Figure 10.6 – Les Miserables feature engineering: triangles](img/B17105_10_006.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 《悲惨世界》特征工程：三角形](img/B17105_10_006.jpg)'
- en: 'Figure 10.6 – Les Miserables feature engineering: triangles'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 《悲惨世界》特征工程：三角形
- en: This is another way of understanding the connectedness of the nodes around a
    node. These nodes are people, so it is a way of understanding the connectedness
    of peopleto other people. Notice that the results are similar but not identical
    to clustering.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是理解节点之间连接性的一种方式。这些节点代表人物，所以它也是理解人与人之间连接性的一种方式。请注意，结果类似于但并不完全相同于聚类。
- en: Betweenness centrality
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介数中心性
- en: '**Betweenness centrality** has to do with a node’s placement between other
    nodes. As a reminder, in a hypothetical situation where there are three people
    (*A*, *B*, and *C*), and if *B* sits between *A* and *C*, then all information
    passing from *A* to *C* flows through person *B*, putting them in an important
    and influential position. That’s just one example of the usefulness of betweenness
    centrality. We can get this information by using the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**介数中心性**与节点在其他节点之间的位置有关。举个例子，假设有三个人（*A*、*B* 和 *C*），如果 *B* 坐在 *A* 和 *C* 之间，那么从
    *A* 到 *C* 传递的所有信息都会通过 *B*，使得 *B* 处于一个重要且有影响力的位置。这只是介数中心性有用性的一个例子。我们可以通过以下代码获取这个信息：'
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This gives us the following output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们带来以下输出：
- en: '![Figure 10.7 – Les Miserables feature engineering: betweenness centrality](img/B17105_10_007.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 《悲惨世界》特征工程：介数中心性](img/B17105_10_007.jpg)'
- en: 'Figure 10.7 – Les Miserables feature engineering: betweenness centrality'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 《悲惨世界》特征工程：介数中心性
- en: Closeness centrality
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接近中心性
- en: '**Closeness centrality** has to do with how close a given node is to all other
    nodes in a network. It has to do with the shortest path. As such, closeness centrality
    is computationally very slow for large networks. However, it will work just fine
    for the *Les Miserables* network, as this is a very small network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**接近中心性**与一个给定节点与网络中所有其他节点的距离有关，具体来说是最短路径。因此，接近中心性在大规模网络中计算起来非常慢。然而，对于*《悲惨世界》*网络来说，它会表现得很好，因为这是一个非常小的网络：'
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Figure 10.8 – Les Miserables feature engineering: closeness centrality](img/B17105_10_008.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8 – 《悲惨世界》特征工程：接近中心性](img/B17105_10_008.jpg)'
- en: 'Figure 10.8 – Les Miserables feature engineering: closeness centrality'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 《悲惨世界》特征工程：接近中心性
- en: PageRank
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PageRank
- en: 'Finally, the `pagerank` remains useful even on large networks. As such, it
    is very commonly used to gauge importance:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，即使在大规模网络中，`pagerank` 仍然有效。因此，它被广泛用于衡量重要性：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This gives us *Figure 10**.9*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们 *图 10.9*。
- en: '![Figure 10.9 – Les Miserables feature engineering: pagerank](img/B17105_10_009.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.9 – 《悲惨世界》特征工程：pagerank](img/B17105_10_009.jpg)'
- en: 'Figure 10.9 – Les Miserables feature engineering: pagerank'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – 《悲惨世界》特征工程：pagerank
- en: Adjacency matrix
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 邻接矩阵
- en: Finally, we can include the **adjacency matrix** in our training data so that
    our models can use neighbor nodes as features for making predictions. For instance,
    let’s say that you have 10 friends but one of them is a criminal, and every person
    that friend introduces you to is also a criminal. You will probably learn over
    time that you shouldn’t associate with that friend or their friends. Your other
    friends do not have that problem. In your head, you’ve already begun to make judgments
    about that person and who they associate with.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将 **邻接矩阵** 纳入训练数据，使得我们的模型可以将邻居节点作为特征来进行预测。例如，假设你有 10 个朋友，但其中一个是罪犯，而每个这个朋友介绍给你的人也都是罪犯。你可能会随着时间的推移，学到不应该和这个朋友或他们的朋友交往。你的其他朋友没有这个问题。在你的脑海中，你已经开始对那个人及其交往的人做出判断。
- en: If we left the adjacency matrix out, the model would attempt to learn from the
    other features, but it’d have no contextual awareness of neighboring nodes. In
    the game of “Spot the Revolutionary,” it would use the centralities, clustering,
    degrees, and other features only, as it’d have no way of learning from anything
    else.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们省略邻接矩阵，模型将试图仅从其他特征中学习，但它无法意识到相邻节点的上下文。在“识别革命者”游戏中，它将仅使用中心性、聚类、度数和其他特征，因为它无法从其他任何地方获取学习信息。
- en: 'We’re going to use the adjacency matrix. It feels almost like leakage (where
    the answer is hidden in another feature) because like often attracts like in a
    social network, but that also shows the usefulness of using network data with
    ML. You can drop the adjacency matrix if you feel that it is cheating. I do not:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用邻接矩阵。这感觉几乎像是泄漏（答案隐藏在另一个特征中），因为在社交网络中，相似的事物往往会相互吸引，但这也展示了将网络数据与机器学习结合的实用性。如果你觉得这是一种作弊方式，可以不使用邻接矩阵，我个人不这么认为：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This code outputs the following DataFrame:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码输出以下 DataFrame：
- en: '![Figure 10.10 – Les Miserables feature engineering: adjacency matrix](img/B17105_10_010.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.10 – 《悲惨世界》特征工程：邻接矩阵](img/B17105_10_010.jpg)'
- en: 'Figure 10.10 – Les Miserables feature engineering: adjacency matrix'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 – 《悲惨世界》特征工程：邻接矩阵
- en: Merging DataFrames
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并数据框
- en: 'Now that we have all of these useful features, it’s time to merge the DataFrames
    together. This is simple, but requires a few steps, as demonstrated in the following
    code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有这些有用的特征，是时候将 DataFrame 合并在一起了。这很简单，但需要几个步骤，以下代码展示了如何操作：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the first step, I created an empty DataFrame just so that I could rerun the
    Jupyter cell over and over without creating duplicate columns with weird names.
    It just saves work and aggravation. Then, I sequentially merge the DataFrames
    into `clf_df`, based on the DataFrame indexes. The DataFrame indexes are character
    names from Les Miserables. This just makes sure that each row from each DataFrame
    is joined together correctly.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步，我创建了一个空的 DataFrame，这样我就可以反复运行 Jupyter 单元，而不需要创建带有奇怪名称的重复列。这只是节省了工作量和减少了烦恼。接着，我按照
    DataFrame 的索引顺序，将各个 DataFrame 合并到 `clf_df` 中。DataFrame 的索引是《悲惨世界》中的角色名称。这确保了每个
    DataFrame 中的每一行都能正确地合并在一起。
- en: '![Figure 10.11 – Les Miserables feature engineering: combined training data](img/B17105_10_011.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.11 – 《悲惨世界》特征工程：合并的训练数据](img/B17105_10_011.jpg)'
- en: 'Figure 10.11 – Les Miserables feature engineering: combined training data'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 – 《悲惨世界》特征工程：合并的训练数据
- en: Adding labels
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加标签
- en: 'Finally, we need to add labels for the revolutionaries. I have done the work
    of quickly looking up the names of the members of Les Amis de l’ABC (Friends of
    the ABC), which is the name of the group of revolutionaries. First, I will add
    the members to a Python list, and then I’ll do a spot check to make sure that
    I’ve spelled their names correctly:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要为革命者添加标签。我已经快速查找了《ABC 朋友们》（Les Amis de l’ABC）成员的名字，这是革命者小组的名称。首先，我会将这些成员添加到
    Python 列表中，然后进行抽查，确保名字拼写正确：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This produces the following DataFrame:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下 DataFrame：
- en: '![Figure 10.12 – Les Misérables feature engineering: Friends of the ABC members](img/B17105_10_012.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.12 – 《悲惨世界》特征工程：ABC 朋友们](img/B17105_10_012.jpg)'
- en: 'Figure 10.12 – Les Misérables feature engineering: Friends of the ABC members'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – 《悲惨世界》特征工程：ABC 朋友们
- en: 'This looks perfect. The list had 11 names, and the DataFrame has 11 rows. To
    create training data for supervised ML, we need to add a `1`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很完美。列表中有 11 个名字，DataFrame 也有 11 行。为了创建监督学习的训练数据，我们需要添加一个 `1`：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It’s that easy. Let’s take a quick look at the DataFrame just to make sure
    we have labels. I’ll sort on the index so that we can see a few `1` labels in
    the data:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单。让我们快速查看一下 DataFrame，确保我们已经有了标签。我会按索引排序，以便在数据中看到一些 `1` 标签：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This outputs the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '![Figure 10.13 – Les Miserables feature engineering: label spot check](img/B17105_10_013.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.13 – 《悲惨世界》特征工程：标签抽查](img/B17105_10_013.jpg)'
- en: 'Figure 10.13 – Les Miserables feature engineering: label spot check'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 – 《悲惨世界》特征工程：标签抽查
- en: Perfect. We have nodes, and they each have a label. A label of **1** means that
    they are a member of Friends of the ABC, and a label of **0** means that they
    are not. With that, our training data is ready for use.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 完美。我们有了节点，每个节点都有标签。标签为 **1** 表示他们是 ABC 朋友会的成员，标签为 **0** 表示他们不是。这样，我们的训练数据就准备好了。
- en: Selecting a model
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择模型
- en: For this exercise, my goal is to simply show you how network data may be useful
    in ML, not to go into great detail about ML. There are many, many, many thick
    books on the subject. This is a book about how NLP and networks can be used together
    to understand the hidden strings that exist around us and the influence that they
    have on us. So, I am going to speed past the discussion on how different models
    work. For this exercise, we are going to use one very useful and powerful model
    that often works well enough. This model is called **Random Forest**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这次练习，我的目标只是向你展示网络数据如何在机器学习（ML）中可能有所帮助，而不是深入探讨机器学习的细节。关于这个主题，有很多非常厚的书籍。这本书是关于如何将自然语言处理（NLP）和网络结合起来，理解我们周围存在的隐形联系以及它们对我们的影响。因此，我将迅速跳过关于不同模型如何工作的讨论。对于这次练习，我们将使用一个非常有用且强大的模型，它通常足够有效。这个模型叫做**随机森林（Random
    Forest）**。
- en: Random Forest can take both numeric and categorical data as input. Our chosen
    features should work very well for this exercise. Random Forest is also easy to
    set up and experiment with, and it’s also very easy to learn what the model found
    most useful for predictions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林（Random Forest）可以接受数值型和类别型数据作为输入。我们选择的特征对于本次练习来说应该非常合适。随机森林的设置和实验也非常简单，而且它也很容易了解模型在预测中最为有用的特征。
- en: Other models would work. I attempted to use **k-nearest neighbors** and had
    nearly the same level of success, and I’m sure that **Logistic regression** would
    have also worked well after some additional preprocessing. **XGBoost** and **SVM**s
    would have also worked. Some of you might also be tempted to use an NN. Please
    feel free. I chose to not use an NN, as the setup is more difficult and the training
    time is typically longer, for an occasional tiny boost to accuracy that may also
    just be a fluke. Experiment with models! It’s a good way to learn, even when you
    are learning what *not* to do.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其他模型也可以使用。我尝试使用了**k-近邻（k-nearest neighbors）**，并且达到了几乎相同的成功水平，我确信**逻辑回归（Logistic
    regression）**在一些额外的预处理之后也会很好用。**XGBoost**和**支持向量机（SVM）**也会有效。你们中的一些人可能也会想使用神经网络（NN）。请随意。我选择不使用神经网络，因为它的设置更复杂，训练时间通常更长，而且可能只能带来微小的准确度提升，这也可能只是偶然。实验不同的模型！这是学习的好方法，即使你是在学习*不该做什么*。
- en: Preparing the data
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'We should do a few more data checks. Most importantly, let’s check the balance
    between classes in the training data:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该再进行几次数据检查。最重要的是，让我们检查一下训练数据中类别的平衡：
- en: 'Start with the following code:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下代码开始：
- en: '[PRE19]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The data is imbalanced, but not too badly.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不平衡，但问题不大。
- en: 'Let’s get this in percentage form, just to make this a little easier to understand:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们以百分比形式展示，这样可以让它更容易理解：
- en: '[PRE24]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It looks like we have about an 86/14 balance between the classes. Not awful.
    Keep this in mind, because the model should be able to predict with about 86%
    accuracy just based on the imbalance alone. It won’t be an impressive model at
    all if it only hits 86%.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们在类别之间大约有86/14的比例。还不错。记住这一点，因为仅仅基于类别不平衡，模型就应该能够以大约86%的准确率进行预测。如果它只有86%的准确率，那它就不会是一个令人印象深刻的模型。
- en: 'Next, we need to cut up our data for our model. We will use the features as
    our `X` data, and the answers as our `y` data. As the label was added last, this
    will be simple:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将数据切割成适合我们模型的数据。我们将使用特征作为`X`数据，答案作为`y`数据。由于标签是最后添加的，这个过程很简单：
- en: '[PRE29]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`X_cols` is every column except for the last one, which is the label. `X` is
    a DataFrame containing only `X_cols` fields, and `y` is an array of our answers.
    Don’t take my word for it. Let’s do a spot check.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_cols`是除了最后一列（即标签）之外的所有列，`X`是一个只包含`X_cols`字段的数据框，`y`是我们答案的数组。不要仅仅听我说，做个抽查吧。'
- en: 'Run this code:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码：
- en: '[PRE32]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This will show a DataFrame.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示一个数据框（DataFrame）。
- en: 'Scroll all the way to the right on the DataFrame. If you don’t see the **label**
    column, we are good to go:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向右滚动数据框。如果你没有看到**标签**列，那我们就可以开始了：
- en: '[PRE33]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This will show the first five labels in `y`. This is an array. We are all set.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示`y`中的前五个标签。这是一个数组。我们准备好了。
- en: Finally, we need to split the data into training data and test data. The training
    data will be used to train the model, and the test data is completely unknown
    to the model. We do not care about the model accuracy on the training data. Remember
    that. We do not care about the model accuracy or any performance metrics about
    the training data. We only care about how the model does on unseen data. That
    will tell us how well it generalizes and how well it will work in the wild. Yes,
    I understand that this model will not be useful in the wild, but that is the idea.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将数据分成训练数据和测试数据。训练数据将用于训练模型，测试数据是模型完全不知道的数据。我们不关心模型在训练数据上的准确性。记住这一点。我们不关心模型的训练数据准确率或任何性能指标。我们只关心模型在未见数据上的表现。这将告诉我们它的泛化能力以及在实际环境中的表现。是的，我知道这个模型在实际环境中不会有太大用处，但这就是我们的思路。
- en: 'We will split the data using the `scikit-learn` `train_test_split` function:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`scikit-learn`的`train_test_split`函数来拆分数据：
- en: '[PRE34]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Since we have so little training data, and since there are so few members of
    Friends of the ABC, I set `test_size` to `0.4`, which is twice as high as the
    default. If there were less imbalance, I would have reduced this to `0.3` or `0.2`.
    If I really wanted the model to have as much training data as possible and I was
    comfortable that it would do well enough, I might even experiment with `0.1`.
    But for this exercise, I went with `0.4`. That’s my reasoning.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的训练数据非常少，而且ABC协会的成员也很少，我将`test_size`设置为`0.4`，是默认值的两倍。如果数据不那么不平衡，我会将其减少到`0.3`或`0.2`。如果我真的希望模型能够使用尽可能多的训练数据，并且我认为它足够好，我甚至可能尝试`0.1`。但是在这个练习中，我选择了`0.4`。这是我的理由。
- en: This function does a 60/40 split of the data, putting 60% of the data into `X_train`
    and `y_train`, and the other 40% into `X_test` and `y_test`. This sets aside 40%
    of the data as unseen data that the model will not be aware of. If the model does
    well against this 40% unseen data, then it’s a decent model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将数据以60/40的比例分割，将60%的数据放入`X_train`和`y_train`，其余40%放入`X_test`和`y_test`。这样就将40%的数据作为模型无法知道的“未见数据”。如果模型能在这40%的未见数据上表现良好，那么它就是一个不错的模型。
- en: We are now ready to train our model and see how well it does!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好训练我们的模型，看看它的表现如何！
- en: Training and validating the model
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: Model training gets the most attention when people talk about ML but it is usually
    the easiest step, once the data has been collected and prepared. A lot of time
    and energy can and should be spent on optimizing your models, via **hyperparameter
    tuning**. Whichever model you are interested in learning about and using, do some
    research on how to tune the model, and any additional steps required for data
    preparation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在人们谈论机器学习时，模型训练是最受关注的部分，但通常它是最简单的一步，只要数据已被收集和准备好。可以并且应该花费大量的时间和精力来优化你的模型，通过**超参数调优**。无论你对哪个模型感兴趣并想使用，做一些关于如何调优该模型的研究，以及数据准备所需的任何额外步骤。
- en: 'With this simple network, the default Random Forest model was already optimal.
    I ran through several checks, and the default model did well enough. Here’s the
    code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个简单的网络，默认的随机森林模型已经是最优的了。我进行了几个检查，发现默认模型已经足够好。以下是代码：
- en: '[PRE36]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We are using a Random Forest classifier, so we first need to import the model
    from the `sklearn.ensemble` module. Random Forest uses an ensemble of decision
    trees to make its predictions. Each ensemble is trained on different features
    from the training data, and then a final prediction is made.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用随机森林分类器，所以我们首先需要从`sklearn.ensemble`模块导入模型。随机森林使用决策树的集成来做出预测。每个集成都基于训练数据中的不同特征进行训练，然后做出最终预测。
- en: Set `random_state` to whatever number you like. I like `1337`, as an old hacker
    joke. It’s *1337*, *leet*, *elite*. Setting `n_jobs` to `-1` ensures that all
    CPUs will be used in training the model. Setting `n_estimators` to `100` will
    allow for 100 ensembles of decision trees. The number of estimators can be experimented
    with. Increasing it can be helpful, but in this case, it was not.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`random_state`为你喜欢的任何数字。我喜欢`1337`，这是一个老黑客笑话。它是*1337*，*leet*，*elite*。将`n_jobs`设置为`-1`，确保在训练模型时使用所有的CPU。将`n_estimators`设置为`100`，将允许使用100个决策树的集成。可以尝试不同的估计器数量。增加它可能有帮助，但在这种情况下没有。
- en: Finally, I collect and print the training accuracy and test accuracy. How are
    our scores?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我收集并打印了训练准确率和测试准确率。我们的得分如何？
- en: '[PRE37]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Not bad results on the test set. This set is unseen data, so we want it to be
    high. As mentioned before, due to class imbalance, the model should at least get
    86% accuracy simply due to 86% of the labels being in the majority class. 93.5%
    is not bad. However, you should be aware of **underfitting** and **overfitting**.
    If both your train and test accuracy are very low, the model is likely underfitting
    the data, and you likely need more data. However, if the training accuracy is
    much higher than the test set accuracy, this can be a sign of overfitting, and
    this model appears to be overfitting. However, with as little data as we have,
    and for the sake of this experiment, good enough is good enough today.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上，结果还不错。这个数据集是未见过的数据，所以我们希望它的准确度较高。如前所述，由于类别不平衡，模型应该至少能达到86%的准确率，因为86%的标签属于大类。93.5%的准确率还算不错。不过，你应当注意**欠拟合**和**过拟合**。如果训练集和测试集的准确率都很低，模型很可能是欠拟合，需要更多的数据。如果训练集的准确率远高于测试集的准确率，这可能是过拟合的表现，而这个模型似乎存在过拟合问题。不过，考虑到我们目前的数据量，以及本次实验的目的，今天的结果也算是“够好了”。
- en: 'It is important that you know that model accuracy is never enough. It doesn’t
    tell you nearly enough about how the model is doing, especially how it is doing
    with the minority class. We should take a look at the confusion matrix and classification
    report to learn more. To use both of these, we should first place the predictions
    for `X_test` into a variable:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须知道，模型准确率永远不足以评估模型的表现。它并不能告诉你模型的表现，特别是在少数类的表现。我们应该查看混淆矩阵和分类报告，以了解更多信息。为了使用这两个工具，我们首先需要将`X_test`的预测结果存入一个变量：
- en: '[PRE38]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Great. We have an array of predictions. Next, let’s import the `confusion_matrix`
    and `classification_report` functions:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们得到了一个预测数组。接下来，让我们导入`confusion_matrix`和`classification_report`函数：
- en: '[PRE39]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can use both by feeding both of them the `X_test` data as well as the predictions
    made against `X_test`. First, let’s look at the confusion matrix:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将`X_test`数据以及对`X_test`做出的预测作为输入，来使用这两个工具。首先，我们来看看混淆矩阵：
- en: '[PRE40]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If this isn’t clear enough, we can also visualize it:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这还不够清楚，我们也可以将其可视化：
- en: '[PRE41]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This produces the following matrix.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下矩阵。
- en: '![Figure 10.14 – Model confusion matrix](img/B17105_10_014.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.14 – 模型混淆矩阵](img/B17105_10_014.jpg)'
- en: Figure 10.14 – Model confusion matrix
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14 – 模型混淆矩阵
- en: The confusion matrix shows how well a model predicts by classes. The figure
    depicts this well. The *y-a*xis shows the true label of either **0** or **1**,
    and the *x-a*xis shows the predicted label of either **0** or **1**. I can see
    that 26 of the characters were correctly predicted to not be members of Friends
    of the ABC (revolutionaries). Our model correctly predicted three of the members
    of Friends of the ABC, but it also predicted that two of the non-members were
    members. We should look into that! Sometimes the misses can help us find problems
    in the data, or they’ll shine a light on an interesting insight.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵展示了模型如何根据类别进行预测。图示很好地展示了这一点。*y轴*显示的是真实标签，值为**0**或**1**，而*x轴*显示的是预测标签，值为**0**或**1**。我可以看到，有26个字符被正确预测为不是ABC革命者（Friends
    of the ABC）成员。我们的模型正确预测了三个ABC革命者的成员，但也错误地预测了两个非成员为成员。我们需要深入研究这个问题！有时候，错误的预测能帮助我们发现数据中的问题，或者给我们带来一些有趣的见解。
- en: 'I also find it extremely helpful to take a look at the classification report:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我还发现查看分类报告极其有帮助：
- en: '[PRE42]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We get the following output:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Figure 10.15 – Model classification report](img/B17105_10_015.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.15 – 模型分类报告](img/B17105_10_015.jpg)'
- en: Figure 10.15 – Model classification report
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.15 – 模型分类报告
- en: 'This report clearly shows us that the model does very well at predicting non-members
    of Friends of the ABC, but it does less well at predicting the revolutionaries.
    Why is that? What is it getting tripped up by? Looking at the networks, the model
    should have been able to learn that there is a clear difference in the structure
    of different groups of people, especially comparing the community of Friends of
    the ABC against everyone else. What gives?! Let’s build a simple DataFrame to
    check:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个报告清楚地显示了模型在预测ABC革命者的非成员时表现很好，但在预测革命者时表现较差。为什么会这样？它是被什么困扰了？从网络上看，模型本应能够学习到，不同群体之间有明显的结构差异，特别是在将ABC革命者与其他人群进行比较时。到底发生了什么？让我们构建一个简单的数据框来检查一下：
- en: '[PRE43]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We get the following DataFrame:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下的数据框：
- en: '![Figure 10.16 – DataFrame for prediction checks](img/B17105_10_016.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.16 – 预测检查的数据框](img/B17105_10_016.jpg)'
- en: Figure 10.16 – DataFrame for prediction checks
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16 – 预测检查的 DataFrame
- en: 'Now let’s create a mask to look up all rows where the label does not match
    the prediction:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个掩码来查找所有标签与预测不匹配的行：
- en: '[PRE44]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: That gives us *Figure 10**.17*.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们*图 10.17*。
- en: '![Figure 10.17 – DataFrame of missed predictions](img/B17105_10_017.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.17 – 漏掉预测的 DataFrame](img/B17105_10_017.jpg)'
- en: Figure 10.17 – DataFrame of missed predictions
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.17 – 漏掉预测的 DataFrame
- en: OK, now we can see what the model got wrong, but why? To save you some time,
    I looked into both of these characters. Joly actually *is* a member of Friends
    of the ABC, and Madame Hucheloup runs a cafe where members of Friends of the ABC
    regularly meet. She was the proprietress of the Corinthe Tavern, the meeting place
    and last defense of the members! Because she was connected with members of the
    group, the model predicted that she also was one of them.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们可以看到模型哪里出了错，但为什么会这样呢？为了节省你的时间，我调查了这两个角色。Joly 实际上是**ABC 朋友会**的成员，而 Madame
    Hucheloup 经营一家咖啡馆，ABC 朋友会的成员经常在这里聚会。她曾是科林特酒馆的老板，那是成员们的聚会场所，也是他们的最后防线！由于她与该小组成员有联系，模型预测她也是其中的一员。
- en: To be fair, I bet a human might have made the same judgment about Madame Hucheloup!
    To me, this is a beautiful misclassification!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 公平来说，我敢打赌一个人类可能也会做出相同的判断，认为 Madame Hucheloup 是其中之一！对我来说，这就是一个美丽的错误分类！
- en: The next steps would be to definitely give Joly a correct label and retrain
    the model. I would leave Madame Hucheloup as is, as she is not a member, but if
    I were counter-insurgency, I would keep an eye on her.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步肯定是给 Joly 一个正确的标签，并重新训练模型。我会保持 Madame Hucheloup 不变，因为她不是成员，但如果我是反叛者，我会密切关注她。
- en: In short, the model did very well, in my opinion, and it did so entirely using
    graph data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我认为模型表现非常好，并且完全使用了图形数据。
- en: Model insights
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型洞察
- en: To me, the model insights are more exciting than building and using the models
    for prediction. I enjoy learning about the world around me, and ML models (and
    networks) allow me to understand the world in a way that my eyes do not. We cannot
    see all of the lines that connect us as people, and we cannot easily understand
    influence based on how the people around us are strategically placed in the social
    networks that exist in real life. These models can help with that! Networks can
    provide the structure to extract contextual awareness of information flow and
    influence. ML can tell us which of these pieces of information is most useful
    in understanding something. Sometimes, ML can cut right through the noise and
    get right to the signals that affect our lives.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，模型洞察比构建和使用模型进行预测更让人兴奋。我喜欢了解周围的世界，而机器学习模型（和网络）让我能够以我的眼睛无法感知的方式理解这个世界。我们看不到所有将我们联系在一起的线条，也很难理解基于周围人的社会网络中他们如何被战略性地安排从而产生的影响。这些模型可以帮助我们做到这一点！网络能够提供提取信息流动和影响的上下文意识的结构。机器学习可以告诉我们哪些信息在理解某些事情时最有用。有时候，机器学习能够穿越噪音，直接找到那些真正影响我们生活的信号。
- en: With the model that we just built, one insight is that the book *Les Miserables*
    has different characters by type in different network structures. Revolutionaries
    are close to each other and tightly connected. Students are also densely connected,
    and I’m surprised and pleased that the model did not false out on a lot of students.
    Other characters in the book have very few connections, and their neighbors are
    sparsely connected. I think it’s beautiful that the author put so much work into
    defining the social network that exists in this story. It can give a new appreciation
    for the creation of the story.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚建立的模型中，一个见解是《悲惨世界》中的不同角色在不同的网络结构中有不同的类型。革命者们彼此靠得很近，并且紧密连接。学生们也有很强的连接性，我感到惊讶并且高兴的是，模型在许多学生身上没有出现错误分类。书中的其他角色几乎没有什么连接，他们的邻居连接稀疏。我认为，作者在定义这个故事中存在的社交网络方面付出了很多努力，这很美妙。它能让我们重新审视这个故事的创作过程。
- en: But what features were most important to the model that helped it make its predictions
    so well? Let’s take a look. Random Forest makes this very easy for us!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，哪些特征对模型最重要，帮助它做出如此准确的预测呢？让我们来看一下。随机森林使我们这一点变得非常容易！
- en: 'You can get to the **feature importances** very easily by doing this:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式非常容易地获取**特征重要性**：
- en: '[PRE45]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'But this data is not very useful in this format. It is much more useful if
    you put the feature importances into a DataFrame, as they can then be sorted and
    visualized:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些数据在这个格式下并不太有用。如果你将特征重要性放入一个 DataFrame，它们就能被排序和可视化，这样会更有用：
- en: '[PRE46]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We get this DataFrame:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了这个 DataFrame：
- en: '![Figure 10.18 – DataFrame for feature importances](img/B17105_10_018.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图10.18 – 特征重要性的DataFrame](img/B17105_10_018.jpg)'
- en: Figure 10.18 – DataFrame for feature importances
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18 – 特征重要性的DataFrame
- en: 'These are the importances in numerical format. This shows the 10 features that
    the model found most useful in making predictions. Notably, a character’s connection
    to Bossuet and Enjolras was a good indicator of whether a character was a revolutionary
    or not. Out of the network features, triangles was the only one to make the top
    10\. The rest of the important features came from the adjacency matrix. Let’s
    visualize this as a bar chart so that we can see more, as well as the level of
    importance:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是数字格式的特征重要性。这展示了模型在进行预测时，认为最有用的10个特征。值得注意的是，角色与博苏埃和昂若拉的关系是判断一个角色是否为革命者的好指标。在网络特征中，三角形是唯一进入前10名的特征。其余的重要特征来自邻接矩阵。让我们通过条形图来可视化这些，以便能看到更多内容，以及每个特征的重要性程度：
- en: '[PRE47]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We get the following plot:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下图表：
- en: '![Figure 10.19 – Horizontal bar chart of feature importances](img/B17105_10_019.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图10.19 – 特征重要性的水平条形图](img/B17105_10_019.jpg)'
- en: Figure 10.19 – Horizontal bar chart of feature importances
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19 – 特征重要性的水平条形图
- en: Much better. This is much easier to look at, and it shows exactly how useful
    the model found each individual feature.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要好得多。这看起来更容易理解，而且它准确地显示了模型认为每个特征的有用程度。
- en: As a side note, you can use feature importances to aggressively identify features
    that you can cut out of your training data, making for leaner models. I often
    create a baseline Random Forest model to assist in aggressive feature selection.
    Aggressive feature selection is what I call it when I ruthlessly cut data that
    I do not need before training models. For this model, I did not do aggressive
    feature selection. I used all of the data that I pulled together.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，你可以利用特征重要性，积极识别出你可以从训练数据中剔除的特征，从而使模型更加精简。我常常会创建一个基准的随机森林模型，以帮助进行积极的特征选择。积极的特征选择是我用来形容在训练模型之前，毫不留情地剔除不必要数据的过程。对于这个模型，我并没有做积极的特征选择。我使用了我收集到的所有数据。
- en: Other use cases
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他使用场景
- en: While this may be interesting, most of us don’t hunt revolutionaries as part
    of our day jobs. So, what good is this? Well, there are lots of uses for doing
    predictions against networks. Lately, graph ML has gotten a lot of interest, but
    most articles and books tend to showcase models built by other people (not how
    to build them from scratch), or use NNs. This is fine, but it’s complicated, and
    not always practical.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能很有趣，但我们大多数人并不会把捕捉革命者当作日常工作的一部分。那么，这有什么用呢？其实，对网络进行预测有很多用途。最近，图神经网络（Graph
    ML）引起了很多关注，但大多数文章和书籍展示的都是别人建立的模型（而不是如何从零开始构建模型），或者使用神经网络。这没问题，但它复杂且不总是实用。
- en: This approach that I showed is lightweight and practical. If you have network
    data, you could do something similar.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我展示的这种方法是轻量且实用的。如果你有网络数据，你也可以做类似的事情。
- en: But what other use cases are there? For me, the ones I’m most interested in
    are bot detection, and the detection of artificial amplification. How would we
    do that? For bot detection, you may want to look at features such as the age of
    the account in days, the number of posts made over time (real people tend to slowly
    learn how to use a social network before becoming active), and so on. For artificial
    amplification, you might look at how many followers an account picks up for each
    tweet that they make. For instance, if an account came online a week ago, made
    2 posts, and picked up 20 million followers, what caused that kind of growth?
    Organic growth is much slower. Perhaps they brought followers from another social
    network. Perhaps their account was pushed by hundreds of blogs.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有哪些其他的使用场景呢？对我来说，我最感兴趣的是机器人检测和人工放大检测。我们该如何进行呢？对于机器人检测，您可能需要关注诸如账户年龄（按天计算）、一段时间内发布的帖子数量等特征（真实用户通常会在变得活跃之前，慢慢学习如何使用社交网络）等内容。对于人工放大，您可能会关注一个账户发布每条推文时，获得的粉丝数。例如，如果一个账户一周前上线，发布了2条帖子，并获得了2000万粉丝，这种增长是如何发生的呢？自然增长要慢得多。也许他们从另一个社交网络带来了粉丝，或者他们的账户被数百个博客推介了。
- en: What other use cases can you think of? Be creative. You know what networks are
    now, and you know how to build them and work with them. What would you like to
    predict or better understand?
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你还能想到其他哪些使用场景吗？发挥创造力吧。你现在知道什么是网络，也知道如何构建它们并与之互动。你希望预测什么，或者更好地理解什么呢？
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We did it! We made it to the end of another chapter. I truly hope you found
    this chapter especially interesting because there are so few sources that explain
    how to do this from scratch. One of the reasons I decided to write this book is
    because I was hoping that ideas like this would take off. So, I hope this chapter
    grabbed your attention and sparked your creativity.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做到了！我们又完成了一个章节。我真心希望你觉得这一章特别有趣，因为很少有资料能解释如何从零开始做这些事情。我决定写这本书的原因之一，就是希望像这样的想法能够得到推广。所以，我希望这一章能引起你的注意，并激发你的创造力。
- en: In this chapter, we transformed an actual network into training data that we
    could use for machine learning. This was a simplified example, but the steps will
    work for any network. In the end, we created a model that was able to identify
    members of Friends of the ABC revolutionary group, though it was a very simple
    model and not suitable for anything real-world.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将一个实际的网络转化为可以用于机器学习的训练数据。这是一个简化的例子，但这些步骤适用于任何网络。最终，我们创建了一个能够识别ABC革命小组成员的模型，尽管它是一个非常简单的模型，并不适用于现实世界中的任何应用。
- en: The next chapter is going to be very similar to this one, but we will be using
    unsupervised ML to identify nodes that are similar to other nodes. Very likely,
    unsupervised ML will also identify members of Friends of the ABC, but it will
    likely also bring out other interesting insights.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将与这一章非常相似，但我们将使用无监督学习来识别与其他节点相似的节点。很可能，无监督学习也会识别出ABC革命小组的成员，但它也可能揭示出其他有趣的见解。
