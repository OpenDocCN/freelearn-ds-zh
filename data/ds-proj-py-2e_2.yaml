- en: 2\. Introduction to Scikit-Learn and Model Evaluation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. Scikit-Learn简介与模型评估
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: After exploring the response variable of the case study data, this chapter introduces
    the core functionality of scikit-learn for training models and making predictions,
    through simple use cases of logistic and linear regression. Evaluation metrics
    for binary classification models, including **true and false positive rates**,
    the **confusion matrix**, the **receiver operating characteristic** (**ROC**)
    **curve**, and the **precision-recall curve**, are demonstrated both from scratch
    and using convenient scikit-learn functionality. By the end of this chapter, you'll
    be able to build and evaluate binary classification models using scikit-learn.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了案例研究数据的响应变量后，本章通过简单的逻辑回归和线性回归使用案例，介绍了scikit-learn在训练模型和进行预测方面的核心功能。我们将展示二分类模型的评估指标，包括**真阳性率和假阳性率**、**混淆矩阵**、**受试者工作特征**（**ROC**）**曲线**以及**精准率-召回率曲线**，既通过从头开始实现，也通过便捷的scikit-learn功能来演示。到本章结束时，你将能够使用scikit-learn构建和评估二分类模型。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, you became familiar with basic Python and then learned
    about the pandas tool for data exploration. Using Python and pandas, you performed
    operations such as loading a dataset, verifying data integrity, and performing
    exploratory analysis of the features, or independent variables, in the data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你已经熟悉了基本的Python，并学习了用于数据探索的pandas工具。通过使用Python和pandas，你执行了如加载数据集、验证数据完整性以及对数据中的特征（即自变量）进行探索性分析等操作。
- en: In this chapter, we will finish our exploration of the data by examining the
    response variable. After we've concluded that the data is of high quality and
    makes sense, we will be ready to move forward with developing machine learning
    models. We will take our first steps with scikit-learn, one of the most popular
    machine learning packages available in the Python language. Before learning the
    details of how mathematical models work in the next chapter, here we'll start
    to get comfortable with the syntax for using them in scikit-learn.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过检查响应变量来完成数据的探索。在我们得出数据质量高且合理的结论后，就可以开始开发机器学习模型了。我们将以scikit-learn作为起步，scikit-learn是Python语言中最流行的机器学习库之一。在下一章学习数学模型的具体细节之前，本章将让我们熟悉在scikit-learn中使用这些模型的语法。
- en: We will also learn some common techniques for answering the question, "Is this
    model good or not?" There are many possible ways to approach model evaluation.
    For business applications, a financial analysis to determine the value that could
    be created by a model is an important way to understand the potential impact of
    your work. Usually, it's best to scope the business opportunity of a project at
    the very beginning. However, as the emphasis of this book is on machine learning
    and predictive modeling, we will demonstrate a financial analysis in the final
    chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将学习一些常用技术，用来回答“这个模型好不好？”这个问题。模型评估有许多不同的方式。对于商业应用来说，进行财务分析以确定模型可能带来的价值，是了解工作潜在影响的重要方式。通常，最好在项目一开始就界定商业机会。然而，由于本书的重点是机器学习和预测建模，我们将在最后一章展示财务分析。
- en: There are several important model evaluation criteria that are considered standard
    knowledge in data science and machine learning. We will cover a few of the most
    widely used classification model performance metrics here.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个重要的模型评估标准被视为数据科学和机器学习中的基本知识。我们将在这里介绍一些最广泛使用的分类模型性能指标。
- en: Exploring the Response Variable and Concluding the Initial Exploration
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索响应变量并完成初步探索
- en: We have now looked through all the **features** to see whether any data is missing,
    as well as to generally examine them. The features are important because they
    constitute the **inputs** to our machine learning algorithm. On the other side
    of the model lies the **output**, which is a prediction of the **response variable**.
    For our problem, this is a binary flag indicating whether or not a credit account
    will default next month.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经检查过所有的**特征**，看看是否有缺失数据，并且对它们进行了一般性检查。特征很重要，因为它们构成了我们机器学习算法的**输入**。在模型的另一端是**输出**，即对**响应变量**的预测。对于我们的问题来说，这是一个二元标志，指示信用账户下个月是否会违约。
- en: The key task for the case study project is to come up with a predictive model
    for this target. Since the response variable is a yes/no flag, this problem is
    called a `'default payment next month'` `= 1`) are said to belong to the **positive
    class**, while those that didn't belong to the **negative class**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 案例研究项目的关键任务是为该目标提出预测模型。由于响应变量是一个是/否标志，因此此问题被称为“下个月是否违约”的问题（`'default payment
    next month'` `= 1`），属于**正类**，而未违约的属于**负类**。
- en: 'The main piece of information to examine regarding the response of a binary
    classification problem is this: what is the proportion of the positive class?
    This is an easy check.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关于二元分类问题响应的主要信息是：正类的比例是多少？这是一个简单的检查。
- en: 'Before we perform this check, we load the packages we need with the following
    code:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行此检查之前，我们使用以下代码加载所需的软件包：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we load the cleaned version of the case study data like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们像这样加载案例研究数据的清理版本：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The cleaned dataset should have been saved as a result of your work in *Chapter
    1*, *Data Exploration and Cleaning*. The path to the cleaned data in the preceding
    code snippet may be different if you saved it in a different location.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 清理后的数据集应该已经保存在*第一章*，*数据探索和清理*中的工作结果中。如果您将其保存在不同的位置，则前面代码片段中的清理数据的路径可能会有所不同。
- en: 'Now, to find the proportion of the positive class, all we need to do is get
    the average of the response variable over the whole dataset. This has the interpretation
    of the default rate. It''s also worthwhile to check the number of samples in each
    class, using `groupby` and `count` in pandas. This is presented in the following
    screenshot:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要找出正类的比例，我们只需得到整个数据集上响应变量的平均值。这被解释为违约率。此外，使用`pandas`中的`groupby`和`count`来检查每个类别的样本数量是值得的。如下屏幕截图所示：
- en: '![Figure 2.1: Class balance of the response variable'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1：响应变量的类别平衡'
- en: '](img/B16925_02_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_01.jpg)'
- en: 'Figure 2.1: Class balance of the response variable'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：响应变量的类别平衡
- en: 'Since the target variable is `1` or `0`, taking the mean of this column indicates
    the fraction of accounts that defaulted: 22%. The proportion of samples in the
    positive class (default = 1), also called the **class fraction** for this class,
    is an important statistic. In binary classification, datasets are described in
    terms of being **balanced** or **imbalanced**: are the proportions of the positive
    and negative classes equal or not? Most machine learning classification models
    are designed to work with balanced data: a 50/50 split between the classes.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标变量为`1`或`0`，取这一列的均值表示违约账户的比例为22%。正类（违约 = 1）的样本比例，也称为此类别的**类别分数**，是一个重要的统计量。在二元分类中，数据集通常描述为**平衡**或**不平衡**：正类和负类的比例是否相等？大多数机器学习分类模型都设计用于处理平衡数据：类别之间的50/50分布。
- en: 'However, in practice, real data is rarely balanced. Consequently, there are
    several methods geared toward dealing with imbalanced data. These include the
    following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际应用中，真实数据很少是平衡的。因此，有几种方法专门用于处理不平衡数据。这些包括以下方法：
- en: '**Undersampling** the majority class: Randomly throwing out samples from the
    majority class until the class fractions are equal, or at least less imbalanced.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过采样**多数类别：随机丢弃多数类别的样本，直到类别分布相等，或至少更少不平衡。'
- en: '**Oversampling** the minority class: Randomly adding duplicate samples of the
    minority class to achieve the same goal.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过采样**少数类别：随机添加少数类别的重复样本，以达到相同的目标。'
- en: '**Weighting samples**: This method is performed as part of the training step,
    so the minority class collectively has as much "emphasis" as the majority class
    in the trained model. The effect of this is similar to oversampling.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权样本**：此方法作为训练步骤的一部分执行，因此少数类在训练模型中具有与多数类相同的“重视”。其效果类似于过采样。'
- en: More sophisticated methods, such as **Synthetic Minority Over-sampling Technique**
    (**SMOTE**).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更复杂的方法，如**合成少数类过采样技术**（**SMOTE**）。
- en: 'While our data is not, strictly speaking, balanced, we also note that a positive
    class fraction of 22% is not particularly imbalanced, either. Some domains, such
    as fraud detection, typically deal with much smaller positive class fractions:
    on the order of 1% or less. This is because the proportion of "bad actors" is
    quite small compared to the total population of transactions; at the same time,
    it is important to be able to identify them if possible. For problems like this,
    it is more likely that using a method to address class imbalance will lead to
    substantially better results.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的数据在严格意义上不是平衡的，但我们也注意到，22%的正类比例并不是特别不平衡。在一些领域，比如欺诈检测，通常处理的正类比例更小，通常在1%或更少。这是因为“坏演员”的比例相对于交易总量非常小；与此同时，如果可能的话，能够识别它们是很重要的。对于这类问题，使用方法来处理类别不平衡更可能带来显著更好的结果。
- en: Now that we've explored the response variable, we have concluded our initial
    data exploration. However, data exploration should be considered an ongoing task
    that you should continually have in mind during any project. As you create models
    and generate new results, it's always good to think about what those results imply
    about the data, which usually requires a quick iteration back to the exploration
    phase. A particularly helpful kind of exploration, which is also typically done
    before model building, is examining the relationship between features and the
    response. We gave a preview of that in *Chapter 1*, *Data Exploration and Cleaning*,
    when we were grouping by the `EDUCATION` feature and examining the mean of the
    response variable. We will also do more of this later. However, this has more
    to do with building a model than checking the inherent quality of the data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了响应变量，初步的数据探索也已经完成。然而，数据探索应该被视为一个持续的任务，应该在任何项目中时刻考虑。当你创建模型并生成新结果时，始终思考这些结果对数据意味着什么，这通常需要快速回到探索阶段进行迭代。一个特别有用的探索方法，通常也是在构建模型之前进行的，是检查特征与响应变量之间的关系。我们在*第一章*《数据探索与清洗》中已经展示了这一点，当时我们按照`EDUCATION`特征进行分组，并检查了响应变量的均值。我们以后还会做更多这样的工作。不过，这更涉及到构建模型，而不是检查数据的内在质量。
- en: 'The initial perusal through all the data that we have just completed is an
    important foundation to lay at the beginning of a project. As you do this, you
    should ask yourself the following questions:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚完成的对所有数据的初步浏览是项目开始时需要打下的重要基础。在此过程中，你应该问自己以下问题：
- en: Is the data **complete**?
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否**完整**？
- en: Are there missing values or other anomalies?
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否存在缺失值或其他异常情况？
- en: Is the data **consistent**?
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否**一致**？
- en: Does the distribution change over time, and if so, is this expected?
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据分布是否随时间变化，如果是，是否可以预期？
- en: Does the data **make sense**?
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否**合理**？
- en: Do the values of the features fit with their definition in the data dictionary?
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征的值是否符合数据字典中的定义？
- en: The latter two questions help you determine whether you think the data is **correct**.
    If the answer to any of these questions is "no," this should be addressed before
    continuing the project.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 后两个问题有助于你判断数据是否**正确**。如果这些问题的答案是“否”，那么在继续项目之前应该解决这些问题。
- en: Also, if you think of any alternative or additional data that might be helpful
    to have and is possible to get, now would be a good point in the project life
    cycle to augment your dataset with it. Examples of this may include postal code-level
    demographic data, which you could **join** to your dataset if you had the addresses
    associated with accounts. We don't have these for the case study data and have
    decided to proceed on this project with the data we have now.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想到任何可能有帮助的额外数据，并且可以获取，现在是项目生命周期中一个很好的时机将其加入到数据集中。例如，如果你有与账户相关的地址数据，可以将邮政编码级别的人口统计数据**加入**到数据集中。我们在案例研究数据中没有这些数据，因此决定在现有数据的基础上继续进行该项目。
- en: Introduction to Scikit-Learn
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn简介
- en: While pandas will save you a lot of time loading, examining, and cleaning data,
    the machine learning algorithms that will enable you to do predictive modeling
    are located in other packages. Scikit-learn is a foundational machine learning
    package for Python that contains many useful algorithms and has also influenced
    the design and syntax of other machine learning libraries in Python. For this
    reason, we focus on scikit-learn to develop skills in the practice of predictive
    modeling. While it's impossible for any one package to offer everything, scikit-learn
    comes pretty close in terms of accommodating a wide range of classic approaches
    for classification, regression, and unsupervised learning. However, it does not
    offer much functionality for some more recent advancements, such as deep learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 pandas 可以节省你大量加载、检查和清理数据的时间，但使你能够进行预测建模的机器学习算法位于其他包中。Scikit-learn 是一个基础的
    Python 机器学习包，包含许多有用的算法，并且也影响了其他 Python 机器学习库的设计和语法。因此，我们将重点学习 scikit-learn，以培养预测建模的实践技能。虽然没有任何一个包能够提供所有功能，但就适配分类、回归和无监督学习的经典方法而言，scikit-learn
    已经做得相当接近了。然而，它对一些较新的进展（如深度学习）并没有太多功能。
- en: 'Here are a few other related packages you should be aware of:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个其他相关的包，你应该了解：
- en: '**SciPy**:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**SciPy**:'
- en: Most of the packages we've used so far, such as NumPy and pandas, are actually
    part of the SciPy ecosystem.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用的大多数包，如 NumPy 和 pandas，实际上都是 SciPy 生态系统的一部分。
- en: SciPy offers lightweight functions for classic methods such as linear regression
    and linear programming.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SciPy 提供轻量级函数，支持经典方法，如线性回归和线性规划。
- en: '**StatsModels**:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**StatsModels**:'
- en: More oriented toward statistics and maybe more comfortable for users familiar
    with R
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更偏向于统计学，可能对于熟悉 R 的用户更为舒适
- en: Can get p-values and confidence intervals on regression coefficients
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以获取回归系数的 p 值和置信区间
- en: Capability for time series models such as ARIMA
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列模型的能力，如 ARIMA
- en: '**XGBoost and LightGBM**:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**XGBoost 和 LightGBM**:'
- en: Offer a suite of state-of-the-art ensemble models that often outperform random
    forests. We will learn about XGBoost in *Chapter 6*, *Gradient Boosting, SHAP
    Values, and Dealing with Missing Data*.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供一套先进的集成模型，这些模型通常比随机森林表现更好。我们将在*第六章*，*梯度提升、SHAP 值和处理缺失数据*中学习 XGBoost。
- en: '**TensorFlow, Keras, and PyTorch**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow, Keras, 和 PyTorch**:'
- en: Deep learning capabilities
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习能力
- en: There are many other Python packages that may come in handy, but this gives
    you an idea of what's out there.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的 Python 包可能会派上用场，但这些给你提供了一个大致的了解。
- en: Scikit-learn offers a wealth of different models for various tasks, but, conveniently,
    the syntax for using them is consistent. In this section, we will illustrate model
    syntax using a **logistic regression** model. Logistic regression, despite its
    name, is actually a classification model. This is one of the simplest, and therefore
    most important, classification models. In the next chapter, we will go through
    the mathematical details of how logistic regression works. Until then, you can
    simply think of it as a black box that can learn from labeled data, then make
    predictions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了丰富的不同模型用于各种任务，但方便的是，它们的使用语法是一致的。在这一节中，我们将使用 **逻辑回归** 模型来说明模型语法。尽管名字中带有“回归”，逻辑回归实际上是一个分类模型。这是最简单的分类模型之一，因此也是最重要的模型之一。在下一章中，我们将详细讲解逻辑回归的数学原理。在此之前，你可以简单地将其视为一个可以从标记数据中学习的黑箱，然后做出预测。
- en: From the first chapter, you should be familiar with the concept of training
    an algorithm on labeled data so that you can use this trained model to then make
    predictions on new data. Scikit-learn encapsulates these core functionalities
    in the `.fit` method for training models, and the `.predict` method for making
    predictions. Because of the consistent syntax, you can call `.fit` and `.predict`
    on any scikit-learn model from linear regression to classification trees.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一章开始，你应该熟悉在标记数据上训练算法的概念，这样你就可以使用训练好的模型对新数据进行预测。Scikit-learn 将这些核心功能封装在 `.fit`
    方法中用于训练模型，`.predict` 方法中用于进行预测。由于语法的一致性，你可以在任何 scikit-learn 模型上调用 `.fit` 和 `.predict`，从线性回归到分类树。
- en: 'The first step is to choose some model, in this example a logistic regression
    model, and instantiate it from the `.fit`, and data, such as information learned
    from the model fitting process. When you instantiate a model class from scikit-learn,
    you are taking the blueprint of the model that scikit-learn makes available to
    you and creating a useful **object** out of it. You can train this object on your
    data and then save it to disk for later use. The following snippets can be used
    to perform this task. The first step is to import the class:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是选择一个模型，在这个例子中是逻辑回归模型，并从`.fit`和数据中实例化它，例如从模型拟合过程学到的信息。当你从scikit-learn实例化一个模型类时，你是拿到scikit-learn为你提供的模型蓝图，并将其创建为一个有用的**对象**。你可以在你的数据上训练这个对象，然后将其保存到磁盘以供以后使用。以下代码片段可以用来执行这个任务。第一步是导入类：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The code to instantiate the class into an object is as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将类实例化为对象的代码如下：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The object is now a variable in our workspace. We can examine it using the
    following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该对象现在是我们工作区中的一个变量。我们可以使用以下代码进行检查：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This should give the following output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会输出以下内容：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice that the act of creating the model object involves essentially no knowledge
    of what logistic regression is or how it works. Although we didn't select any
    particular options when creating the logistic regression model object, we are
    now in fact using many **default options** for how the model is formulated and
    would be trained. In effect, these are choices we have made regarding the details
    of model implementation without having been aware of it. The danger of an easy-to-use
    package such as scikit-learn is that it has the potential to obscure these choices
    from you. However, any time you use a machine learning model that has been prepared
    for you as scikit-learn models have been, your first job is to understand all
    the options that are available. A best practice in such cases is to explicitly
    provide every keyword parameter to the model when you create the object. Even
    if you are just selecting all the default options, this will help increase your
    awareness of the choices that are being made.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，创建模型对象的过程本质上并不需要了解逻辑回归是什么或它如何工作。尽管我们在创建逻辑回归模型对象时没有选择任何特定选项，但我们现在实际上使用了很多**默认选项**来构建和训练模型。实际上，这些是我们在不知道的情况下做出的关于模型实现细节的选择。像scikit-learn这样易于使用的包的危险在于，它可能会让你忽视这些选择。然而，每当你使用一个为你准备好的机器学习模型时，就像scikit-learn模型一样，你的首要任务是理解所有可用的选项。在这种情况下，最佳实践是，在创建对象时明确提供每个关键字参数给模型。即使你只是选择所有默认选项，这也有助于提高你对所做选择的意识。
- en: 'We will review the interpretation of these choices later on, but for now here
    is the code for instantiating a logistic regression model with all the default
    options:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后将回顾这些选择的解释，但现在这里是使用所有默认选项实例化逻辑回归模型的代码：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Even though the object we''ve created here in `my_new_lr` is identical to `my_lr`,
    being explicit like this is especially helpful when you are starting out and learning
    about different kinds of models. Once you''re more comfortable, you may wish to
    just instantiate with the default options and make changes later as necessary.
    Here, we show how this may be done. The following code sets two options and displays
    the current state of the model object:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在`my_new_lr`中创建的对象与`my_lr`完全相同，但像这样显式地指定，尤其在你刚开始学习并了解不同模型时是非常有帮助的。一旦你更加熟悉，你可能希望只使用默认选项进行实例化，并在必要时稍后进行更改。在这里，我们展示了如何做到这一点。以下代码设置了两个选项并显示了模型对象的当前状态：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This should produce the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生以下内容：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that only the options we have updated from the default values are displayed.
    Here, we've taken what is called a `C`, and updated it from its default value
    of `1` to `0.1`. We've also specified a solver. For now, it is enough to understand
    that hyperparameters are options that you supply to the model, before fitting
    it to the data. These options specify the way in which the model will be trained.
    Later, we will explain in detail what all the options are and how you can effectively
    choose values for them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，仅显示了我们已从默认值更新的选项。在这里，我们将一个叫做`C`的参数从默认值`1`更新为`0.1`，并且我们还指定了一个求解器。现在，了解超参数是你在将模型拟合到数据之前提供的选项就足够了。这些选项指定了模型将如何训练。稍后，我们将详细解释所有选项是什么以及如何有效选择它们的值。
- en: 'To illustrate the core functionality, we will fit this nearly default logistic
    regression to some data. Supervised learning algorithms rely on labeled data.
    That means we need both the features, customarily contained in a variable called
    `X`, and the corresponding responses, in a variable called `y`. We will borrow
    the first 10 samples of one feature, and the response, from our dataset to illustrate:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明核心功能，我们将用这个几乎默认的逻辑回归算法来拟合一些数据。监督学习算法依赖于带标签的数据。这意味着我们需要特征，通常包含在一个名为`X`的变量中，以及对应的响应，包含在一个名为`y`的变量中。我们将从数据集中借用前10个样本的一个特征和响应来说明：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'That should show the values of the `EDUCATION` feature for the first 10 samples:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示前10个样本的`EDUCATION`特征值：
- en: '![Figure 2.2: First 10 values of a feature'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2：特征的前10个值'
- en: '](img/B16925_02_02.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_02.jpg)'
- en: 'Figure 2.2: First 10 values of a feature'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：特征的前10个值
- en: 'The corresponding first 10 values of the response variable can be obtained
    as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方式获得响应变量的前10个对应值：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we have selected a couple of Series (that is, columns) from our DataFrame:
    the `EDUCATION` feature we''ve been discussing, and the response variable. Then
    we selected the first 10 elements of each and finally used the `.values` method
    to return NumPy arrays. Also notice that we used the `.reshape` method to reshape
    the features. Scikit-learn expects that the first dimension (that is, the number
    of rows) of the array of features will be equal to the number of samples, so we
    need to make that reshaping for `X`, but not for `y`. The `–1` in the first positional
    argument of `.reshape` means to make the output array shape flexible in that dimension,
    according to how much data goes in. Since we just have a single feature in this
    example, we specified the number of columns as the second argument, `1`, and let
    the `–1` argument indicate that the array should "fill up" along the first dimension
    with as many elements as necessary to accommodate the data, in this case, 10 elements.
    Note that while we''ve extracted the data into NumPy arrays to show how this can
    be done, it''s also possible to use pandas Series as direct input to scikit-learn.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择了DataFrame中的几个Series（即列）：我们讨论过的`EDUCATION`特征和响应变量。然后我们选择了每个特征的前10个元素，并最终使用`.values`方法返回了NumPy数组。还注意到，我们使用了`.reshape`方法来调整特征的形状。Scikit-learn期望特征数组的第一个维度（即行数）等于样本数，因此我们需要对`X`进行这种形状调整，但`y`不需要。`.reshape`的第一个位置参数中的`–1`表示根据输入数据的数量，在该维度上灵活调整输出数组的形状。由于这个例子中我们只有一个特征，所以我们指定了第二个参数，即列数为`1`，并让`–1`参数指示数组应根据需要填充第一维，容纳数据，在这个例子中是10个元素。请注意，虽然我们提取了数据并转换为NumPy数组来展示这种方法，但也可以直接将pandas
    Series作为输入传递给scikit-learn。
- en: 'Let''s now use this data to fit our logistic regression. This is accomplished
    with just one line:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用这些数据来拟合我们的逻辑回归。这只需要一行代码：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is the output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: That's all there is to it. Once the data is prepared and the model is specified,
    fitting the model almost seems like an afterthought. Of course, we are ignoring
    all the important options and what they mean right now. But, technically speaking,
    fitting a model is very easy in terms of the code. You can see that the output
    of this cell just prints the same options we've already seen. While the fitting
    procedure did not return anything aside from this output, a very important change
    has taken place. The `my_new_lr` model object is now a trained model. We say that
    this change happened `my_new_lr`, has been modified. This is similar to modifying
    a DataFrame in place. We can now use our trained model to make predictions using
    the features of new samples, that the model has never "seen" before. Let's try
    the next 10 rows from the `EDUCATION` feature.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 仅此而已。一旦数据准备好并且模型被指定，拟合模型几乎就像是顺便做的事。当然，我们现在忽略了所有重要的选项以及它们的含义。但从技术角度来看，拟合一个模型在代码层面是非常简单的。你可以看到，这个单元的输出只是打印出了我们已经看到的相同选项。虽然拟合过程没有返回任何内容，除了这个输出，但一个非常重要的变化已经发生。`my_new_lr`模型对象现在是一个已训练的模型。我们可以说，这个变化发生在`my_new_lr`，它已经被修改。这类似于修改DataFrame的原地操作。现在我们可以使用训练好的模型，利用新的样本特征来进行预测，而这些样本是模型之前从未“见过”的。让我们试试`EDUCATION`特征的接下来的10行。
- en: 'We can select and view these features using a new variable, `new_X`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个新变量`new_X`来选择和查看这些特征：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Figure 2.3: New features to make predictions for'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3：用于进行预测的新特征'
- en: '](img/B16925_02_03.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_03.jpg)'
- en: 'Figure 2.3: New features to make predictions for'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.3: 新特征用于预测'
- en: 'Making predictions is done like this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 预测是这样进行的：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can also view the true values corresponding to these predictions, since
    this data is labeled:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看与这些预测对应的真实值，因为这些数据是有标签的：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we've illustrated several things. After getting our new feature values,
    we've called the `.predict` method on the trained model. Notice that the only
    argument to this method is a set of features, that is, an "X" that we've called
    `new_X`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了几件事。在获取到新的特征值后，我们调用了经过训练的模型的 `.predict` 方法。请注意，这个方法的唯一参数是一组特征，也就是我们称之为
    `new_X` 的“X”。
- en: How well did our little model do? We may naively observe that since the model
    predicted all 0s, and 80% of the true labels are 0s, we were right 80% of the
    time, which seems pretty good. On the other hand, we entirely failed to successfully
    predict any 1s. So, if those were important, we did not actually do very well.
    While this is just an example to get you familiar with how scikit-learn works,
    it's worth considering what a "good" prediction might look like for this problem.
    We will get into the details of assessing model predictive capabilities shortly.
    For now, congratulate yourself on having gotten your hands dirty with some real
    data and fitting your first machine learning model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的小模型表现得如何？我们可能天真地认为，因为模型预测了所有的 0，而且真实标签中 80% 是 0，所以我们有 80% 的预测是正确的，看起来似乎不错。另一方面，我们完全未能成功预测任何
    1。所以，如果 1 的预测很重要，我们实际上表现得并不好。虽然这只是一个让你熟悉 scikit-learn 工作方式的例子，但值得考虑一下，对于这个问题，什么是“好的”预测。我们很快会详细讨论如何评估模型的预测能力。现在，先为自己鼓掌，因为你已经在实际数据中动手，完成了第一个机器学习模型的拟合。
- en: Generating Synthetic Data
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成合成数据
- en: In the following exercise, you will walk through the model fitting process on
    your own. We’ll motivate this process using a linear regression, one of the best-known
    mathematical models, which should be familiar from basic statistics. It’s also
    called a line of best fit. If you don’t know what it is, you could consult a basic
    statistics resource, although the intent here is to illustrate the mechanics of
    model fitting in sci-kit learn, as opposed to understanding the model in detail.
    We’ll work on that later in the book for other mathematical models that we’ll
    apply to the case study, such as logistic regression. In order to have data to
    work with, you will generate your own `random` library to generate random numbers,
    as well as matplotlib's `scatter` and `plot` functions to create scatter and line
    plots. In the exercise, we'll use scikit-learn for the linear regression part.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，你将独立完成模型拟合过程。我们将通过使用线性回归来引导这个过程，线性回归是最著名的数学模型之一，应该是基本统计学中比较熟悉的内容，也叫作最佳拟合线。如果你不知道它是什么，可以查阅基础统计学资料，尽管这里的目的是展示
    scikit-learn 中模型拟合的机制，而不是深入理解模型。我们将在本书后面讨论其他数学模型的应用，如逻辑回归。在此之前，你将通过使用 `random`
    库来生成随机数，以及 matplotlib 的 `scatter` 和 `plot` 函数来创建散点图和线图，来准备好数据。在线性回归部分的练习中，我们将使用
    scikit-learn。
- en: 'To get started, we use NumPy to make a one-dimensional array of feature values,
    `X`, consisting of 1,000 random real numbers (in other words, not just integers
    but decimals as well) between 0 and 10\. We again use a `.uniform` method of `default_rng`
    (random number generator), which draws from the uniform distribution: it''s equally
    likely to choose any number between `low` (inclusive) and `high` (exclusive),
    and will return an array of whatever `size` you specify. We create a one-dimensional
    array (that is, a vector) with 1,000 elements, then examine the first 10\. All
    of this can be done using the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，我们使用 NumPy 创建一个由 1,000 个随机实数（也就是说，不仅仅是整数，还有小数）组成的单维特征数组 `X`，这些数字的范围在 0
    到 10 之间。我们再次使用 `default_rng`（随机数生成器）的方法 `.uniform`，从均匀分布中抽取：在 `low`（包含）和 `high`（不包含）之间，选择任意数字的概率是相等的，并且返回一个由你指定的
    `size` 大小组成的数组。我们创建一个包含 1,000 个元素的一维数组（即向量），然后检查前 10 个数字。所有这些都可以通过以下代码完成：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output should appear as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '![Figure 2.4: Creating random, uniformly distributed numbers with NumPy'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4: 使用 NumPy 创建随机、均匀分布的数字'
- en: '](img/B16925_02_04.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_04.jpg)'
- en: 'Figure 2.4: Creating random, uniformly distributed numbers with NumPy'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.4: 使用 NumPy 创建随机、均匀分布的数字'
- en: Data for Linear Regression
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归数据
- en: 'Now we need a response variable. For this example, we''ll generate data that
    follows the assumptions of linear regression: the data will exhibit a linear trend
    against the feature, but have normally distributed errors:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一个响应变量。对于这个例子，我们将生成符合线性回归假设的数据：数据将展示出与特征之间的线性趋势，但同时具有正态分布的误差：
- en: '![Figure 2.5: Linear equation with Gaussian noise'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5：带有高斯噪声的线性方程'
- en: '](img/B16925_02_05.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_05.jpg)'
- en: 'Figure 2.5: Linear equation with Gaussian noise'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：带有高斯噪声的线性方程
- en: 'Here, *a* is the slope, *b* is the intercept, and the Gaussian noise has a
    mean of *µ* with a standard deviation of *σ*. In order to write code to implement
    this, we need to make a corresponding vector of responses, `y`, which are calculated
    as the slope times the feature array, `X`, plus some Gaussian noise (again using
    NumPy), and an intercept. The noise will be an array of 1,000 data points with
    the same shape (`size`) as the feature array, `X`, where the mean of the noise
    (`loc`) is 0 and the standard deviation (`scale`) is 1\. This will add a little
    "spread" to our linear data:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*a* 是斜率，*b* 是截距，而高斯噪声的均值是 *µ*，标准差是 *σ*。为了实现这一点，我们需要创建一个对应的响应向量 `y`，它通过斜率乘以特征数组
    `X`，再加上一些高斯噪声（同样使用 NumPy）和一个截距来计算。噪声将是一个包含 1,000 个数据点的数组，它与特征数组 `X` 的形状相同（`size`），噪声的均值（`loc`）为
    0，标准差（`scale`）为 1\. 这样就会为我们的线性数据增加一点“散布”：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now we'd like to visualize this data. We will use matplotlib to plot `y` against
    the feature `X` as a scatter plot. First, we use `.rcParams` to set the resolution
    (`dpi` = dots per inch) for a nice crisp image. Then we create the scatter plot
    with `plt.scatter`, where `X` and `y` are the first two arguments, respectively,
    and the `s` argument specifies a size for the dots.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要可视化这些数据。我们将使用 matplotlib 将 `y` 与特征 `X` 绘制为散点图。首先，我们使用 `.rcParams` 设置图像的分辨率（`dpi`
    = 每英寸点数），以获得清晰的图像。然后，我们使用 `plt.scatter` 创建散点图，其中 `X` 和 `y` 是前两个参数，`s` 参数指定点的大小。
- en: 'This code can be used for plotting:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可以用于绘图：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After executing these cells, you should see something like this in your notebook:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这些单元格后，你应该在你的笔记本中看到类似这样的内容：
- en: '![Figure 2.6: Plot the noisy linear relationship'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.6：绘制带噪声的线性关系'
- en: '](img/B16925_02_06.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_06.jpg)'
- en: 'Figure 2.6: Plot the noisy linear relationship'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：绘制带噪声的线性关系
- en: Looks like some noisy linear data, just like we hoped. Now let's model it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来像一些带噪声的线性数据，正如我们所希望的那样。现在让我们开始建模。
- en: Note
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you''re reading the print version of this book, you can download and browse
    the color versions of some of the images in this chapter by visiting the following
    link: [https://packt.link/0dbUp](https://packt.link/0dbUp).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读本书的印刷版，可以通过访问以下链接下载并浏览本章某些图像的彩色版本：[https://packt.link/0dbUp](https://packt.link/0dbUp)。
- en: 'Exercise 2.01: Linear Regression in Scikit-Learn'
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.01：Scikit-Learn 中的线性回归
- en: 'In this exercise, we will take the synthetic data we just generated and determine
    a line of best fit, or linear regression, using scikit-learn. The first step is
    to import a linear regression model class from scikit-learn and create an object
    from it. The import is similar to the `LogisticRegression` class we worked with
    previously. As with any model class, you should observe what all the default options
    are. Notice that for linear regression, there are not that many options to specify:
    you will use the defaults for this exercise. The default settings include `fit_intercept=True`,
    meaning the regression model will include an intercept term. This is certainly
    appropriate since we added an intercept to the synthetic data. Perform the following
    steps to complete the exercise, noting that the code creating the data for linear
    regression from the preceding section must be run first in the same notebook (as
    seen on GitHub):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用刚刚生成的合成数据，并使用 scikit-learn 确定最佳拟合线，或线性回归。第一步是从 scikit-learn 导入线性回归模型类并创建一个对象。导入的过程类似于我们之前使用的
    `LogisticRegression` 类。和任何模型类一样，你应该观察所有默认选项。请注意，对于线性回归，指定的选项并不多：你将在本练习中使用默认值。默认设置包括
    `fit_intercept=True`，这意味着回归模型将包括截距项。这是完全合适的，因为我们已经在合成数据中添加了截距。请按照以下步骤完成练习，注意前面部分生成线性回归数据的代码必须先在同一个笔记本中运行（如
    GitHub 上所见）：
- en: Note
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Jupyter notebook for this exercise can be found here: [https://packt.link/IaoyM](https://packt.link/IaoyM).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的 Jupyter 笔记本可以在这里找到：[https://packt.link/IaoyM](https://packt.link/IaoyM)。
- en: 'Execute this code to import the linear regression model class and instantiate
    it with all the default options:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行这段代码以导入线性回归模型类并用所有默认选项实例化它：
- en: '[PRE22]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should see the following output:'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会看到以下输出：
- en: '[PRE23]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: No options are displayed since we used all the defaults. Now we can fit the
    model using our synthetic data, remembering to reshape the feature array (as we
    did earlier) so that that samples are along the first dimension. After fitting
    the linear regression model, we examine `lin_reg.intercept_`, which contains the
    intercept of the fitted model, as well as `lin_reg.coef_`, which contains the
    slope.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们使用了所有默认选项，因此没有显示任何选项。现在我们可以使用我们的合成数据来拟合模型，记得像之前一样重新调整特征数组的形状（将样本放置在第一维）。在拟合线性回归模型后，我们查看`lin_reg.intercept_`，它包含拟合模型的截距，以及`lin_reg.coef_`，它包含斜率。
- en: 'Run this code to fit the model and examine the coefficients:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行这段代码以拟合模型并检查系数：
- en: '[PRE24]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You should see this output for the intercept and slope:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会看到截距和斜率的输出：
- en: '[PRE25]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We again see that actually fitting a model in scikit-learn, once the data is
    prepared and the options for the model are decided, is a trivial process. This
    is because all the algorithmic work of determining the model parameters is abstracted
    away from the user. We will discuss this process later, for the logistic regression
    model we'll use on the case study data.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们再次看到，一旦数据准备好并且模型选项确定，实际上在scikit-learn中拟合模型是一个非常简单的过程。这是因为所有关于确定模型参数的算法工作都被抽象化，用户无需关心。稍后我们将讨论这个过程，特别是在我们用来处理案例研究数据的逻辑回归模型。
- en: '`X`. We capture the output of this as a variable, `y_pred`. This is very similar
    to the example shown in *Figure 2.7*, only here we are making predictions on the
    same data used to fit the model (previously, we made predictions on different
    data) and we put the output of the `.predict` method into a variable.'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`X`。我们将其输出捕获为一个变量`y_pred`。这与*图 2.7*中的示例非常相似，只是这里我们是在用于拟合模型的相同数据上进行预测（之前我们是在不同的数据上进行预测），并且我们将`.predict`方法的输出放入一个变量中。'
- en: 'Run this code to make predictions:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行这段代码以进行预测：
- en: '[PRE26]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can plot the predictions, `y_pred`, against feature `X` as a line plot over
    the scatter plot of the feature and response data, like we made in *Figure 2.6*.
    Here, we make the addition of `plt.plot`, which produces a line plot by default,
    to plot the feature and the model-predicted response values for the model training
    data. Notice that we follow the `X` and `y` data with `'r'` in our call to `plt.plot`.
    This keyword argument causes the line to be red and is part of a shorthand syntax
    for plot formatting.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以将预测结果`y_pred`与特征`X`绘制成线图，叠加在特征和响应数据的散点图上，就像我们在*图 2.6*中所做的那样。在这里，我们添加了`plt.plot`，它默认会生成线图，用来绘制特征和模型预测的响应值。请注意，在调用`plt.plot`时，我们在`X`和`y`数据后跟上了`'r'`。这个关键字参数让线条变成红色，是图表格式化的一种简写语法。
- en: 'This code can be used to plot the raw data, as well as the fitted model predictions
    on this data:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码可以用来绘制原始数据，以及在这些数据上拟合的模型预测结果：
- en: '[PRE27]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After executing this cell, you should see something like this:'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行这个单元格后，你应该会看到类似的输出：
- en: '![Figure 2.7: Plotting the data and the regression line'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.7：绘制数据和回归线'
- en: '](img/B16925_02_07.jpg)'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_02_07.jpg)'
- en: 'Figure 2.7: Plotting the data and the regression line'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：绘制数据和回归线
- en: The plot looks like a line of best fit, as expected.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图看起来像是最优拟合线，正如预期的那样。
- en: 'In this exercise, as opposed to when we called `.predict` with logistic regression,
    we made predictions on the same data `X` that we used to train the model. This
    is an important distinction. While here, we are seeing how the model "fits" the
    same data that it was trained on, we previously examined model predictions on
    new, unseen data. In machine learning, we are usually concerned with predictive
    capabilities: we want models that can help us know the likely outcomes of future
    scenarios. However, it turns out that model predictions on both the **training
    data** used to fit the model and the **test data**, which was not used to fit
    the model, are important for understanding the workings of the model. We will
    formalize these notions later in *Chapter 4,* *The Bias-Variance Trade-Off*, when
    we discuss the **bias-variance trade-off**.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，与我们之前在使用逻辑回归时调用 `.predict` 不同，我们在同一数据 `X` 上进行了预测，而这些数据也用于训练模型。这是一个重要的区别。虽然在这里，我们看到模型如何“拟合”它所训练的相同数据，但之前我们检查了模型在新数据上的预测。在机器学习中，我们通常关心的是预测能力：我们希望模型能帮助我们了解未来情景的可能结果。然而，事实证明，无论是模型在用于拟合的**训练数据**上的预测，还是在未用于拟合的**测试数据**上的预测，对于理解模型的工作原理都非常重要。我们将在稍后的*第4章*中正式定义这些概念，即讨论**偏差-方差权衡**时。
- en: Model Performance Metrics for Binary Classification
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二分类模型性能指标
- en: Before we start building predictive models in earnest, we would like to know
    how we can determine, once we've created a model, whether it is "good" in some
    sense of the word. As you may imagine, this question has received a lot of attention
    from researchers and practitioners. Consequently, there is a wide variety of model
    performance metrics to choose from.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始认真构建预测模型之前，我们希望了解如何在创建模型后判断它在某种意义上是否“好”。正如你可能想象的那样，这个问题已引起了研究人员和实践者的广泛关注。因此，有许多不同的模型性能指标可供选择。
- en: Note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For an idea of the range of options, have a look at the scikit-learn model
    evaluation page: [https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解选项的范围，可以查看 scikit-learn 模型评估页面：[https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)。
- en: When selecting a model performance metric to assess the predictive quality of
    a model, it's important to keep two things in mind.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择模型性能指标来评估模型的预测质量时，重要的是要牢记两点。
- en: '**Appropriateness of the metric for the problem**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**该指标是否适用于问题**'
- en: Metrics are typically only defined for a specific class of problems, such as
    classification or regression. For a binary classification problem, several metrics
    characterize the correctness of the yes or no question that the model answers.
    An additional level of detail here is how often the model is correct for each
    class, the positive and negative classes. We will go into detail on these metrics
    here. On the other hand, regression metrics are aimed at measuring how close a
    prediction is to the target quantity. If we are trying to predict the price of
    a house, how close did we come? Are we systematically over- or under-estimating?
    Are we getting the more expensive houses wrong but the cheaper ones right? There
    are many possible ways to look at regression metrics.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 指标通常只为特定类型的问题定义，比如分类或回归。对于二分类问题，有几个指标用来衡量模型回答“是”或“否”问题的正确性。这里的一个附加细节是，模型对于每个类别（正类和负类）的正确率如何。我们将在这里详细讨论这些指标。另一方面，回归指标旨在衡量预测值与目标数量的接近程度。如果我们试图预测房价，我们的预测与实际价格有多接近？我们是系统性地高估还是低估？我们是否在预测更贵的房子时出错，但预测便宜的房子正确？有许多不同的方式来观察回归指标。
- en: '**Does the metric answer the business question?**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个指标能回答业务问题吗？**'
- en: Whatever class of problem you are working on, there will be many choices for
    the metric. Which one is the right one? And even then, how do you know if a model
    is "good enough" in terms of the metric? At some level, this is a subjective question.
    However, we can be objective when we consider what the goal of the model is. In
    a business context, typical goals are to increase profit or reduce loss. Ultimately,
    you need to unify your business question, which is often related to money in some
    way, and the metric you will use to judge your model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你正在处理哪类问题，都有许多选择可以用来衡量指标。哪一个是正确的呢？即便如此，你如何判断模型在该指标下是否“足够好”呢？在某种程度上，这是一个主观性问题。然而，当我们考虑模型的目标时，我们可以变得更加客观。在商业环境中，典型的目标是增加利润或减少损失。最终，你需要统一你的商业问题（通常以某种方式与金钱相关）和你用来评估模型的指标。
- en: For example, in our credit default problem, is there a particularly high cost
    associated with not correctly identifying accounts that will default? Is this
    more important than potentially misclassifying some of the accounts that won't
    default?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的信用违约问题中，未能正确识别将会违约的账户是否会产生特别高的成本？这是否比误分类一些不会违约的账户更为重要？
- en: Later in the book, we'll incorporate the concept of relative costs and benefits
    of correct and incorrect classifications in our problem and conduct a financial
    analysis. First, we'll introduce you to the most common metrics used to assess
    the predictive quality of binary classification models, the kinds of model we
    need to build for our case study.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后续章节中，我们将结合正确和错误分类的相对成本与收益概念，并进行财务分析。首先，我们将介绍一些常用的指标，用于评估二元分类模型的预测质量，这也是我们案例研究中需要构建的模型类型。
- en: 'Splitting the Data: Training and Test Sets'
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据拆分：训练集和测试集
- en: In the scikit-learn introduction of this chapter, we introduced the concept
    of using a trained model to make predictions on new data that the model had never
    "seen" before. It turns out this is a foundational concept in predictive modeling.
    In our quest to create a model that has predictive capabilities, we need some
    kind of measure of how well the model can make predictions on data that were not
    used to fit the model. This is because in fitting a model, the model becomes "specialized"
    at learning the relationship between features and response on the specific set
    of labeled data that were used for fitting. While this is nice, in the end we
    want to be able to use the model to make accurate predictions on new, unseen data,
    for which we don't know the true value of the labels.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的 scikit-learn 介绍中，我们引入了使用训练好的模型对模型从未“见过”的新数据进行预测的概念。事实证明，这是预测建模中的一个基础概念。在我们创建具有预测能力的模型的过程中，我们需要某种衡量标准，来评估模型对未用于拟合模型的数据的预测能力。这是因为在拟合模型时，模型会“专门化”于学习特征和响应之间的关系，且仅限于用于拟合的特定标注数据集。虽然这很不错，但最终我们希望能够使用该模型对新的、未见过的数据做出准确的预测，而我们对于这些数据的标签值并不了解。
- en: For example, in our case study, once we deliver the trained model to our client,
    they will then generate a new dataset of features like those we have now, except
    instead of spanning the period from April to September, they will span from May
    to October. And our client will be using the model with these features, to predict
    whether accounts will default in November.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的案例研究中，一旦我们将训练好的模型交付给客户，他们就会生成一个新的特征数据集，特征与我们现在使用的相似，只不过数据范围不再是从四月到九月，而是从五月到十月。然后，我们的客户将使用这个模型来预测账户是否会在十一月违约。
- en: In order to know how well we can expect our model to predict which accounts
    will actually default in November (which won't be known until December), we can
    take our current dataset and reserve some of the data we have, with known labels,
    from the model training process. This data is referred to as **test data** and
    may also be called **out-of-sample data** since it consists of samples that were
    not used in training the model. Those samples used to train the model are called
    **training data**. The practice of holding out a set of test data gives us an
    idea of how the model will perform when it is used for its intended purpose, to
    make predictions on samples that were not included during model training. In this
    chapter, we'll create an example train/test split to illustrate different binary
    classification metrics.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们预计模型在预测哪些账户将在11月违约时的表现（这在12月之前无法得知），我们可以将当前数据集中的一部分数据保留作为**测试数据**，并从模型训练过程中分离出来。这些数据也可以称为**外样本数据**，因为它们由未参与模型训练的样本组成。用于训练模型的样本称为**训练数据**。将一部分数据保留下来作为测试数据的做法，让我们能够了解模型在实际应用时的表现，即在对未用于训练的数据进行预测时的表现。在本章中，我们将创建一个示例的训练/测试拆分，以说明不同的二元分类指标。
- en: We will use the convenient `train_test_split` functionality of scikit-learn
    to split the data so that 80% will be used for training, holding 20% back for
    testing. These percentages are a common way to make such a split; in general,
    you want enough training data to allow the algorithm to adequately "learn" from
    a representative sample of data. However, these percentages are not set in stone.
    If you have a very large number of samples, you may not need as large a percentage
    of training data, since you will be able to achieve a pretty large, representative
    training set with a lower percentage. We encourage you to experiment with different
    sizes and see the effect. Also, be aware that every problem is different with
    respect to how much data is needed to effectively train a model. There is no hard
    and fast rule for sizing your training and test sets.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用scikit-learn中方便的`train_test_split`功能来拆分数据，使得80%的数据用于训练，剩余的20%用于测试。这些百分比是常见的拆分方式；通常来说，你需要足够的训练数据，以便算法能够从代表性数据样本中充分“学习”。然而，这些百分比并不是固定不变的。如果你拥有大量样本，可能不需要占用那么大比例的训练数据，因为即便使用较小的比例，也能获得一个较大且具有代表性的训练集。我们鼓励你尝试不同的大小并观察其效果。此外，要注意，每个问题对于有效训练模型所需的数据量都是不同的。没有固定的规则来决定训练集和测试集的大小。
- en: 'For our 80/20 split, we can use the code shown in the following snippet:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的80/20数据拆分，我们可以使用以下代码片段：
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Notice that we''ve set `test_size` to `0.2`, or 20%. The size of the training
    data will be automatically set to the remainder, 80%. Let''s examine the shapes
    of our training and test data, to see whether they are as expected, as shown in
    the following output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们将`test_size`设置为`0.2`，即20%。训练数据的大小将自动设置为剩余的80%。让我们检查一下训练数据和测试数据的形状，看看它们是否符合预期，如下所示的输出所示：
- en: '![Figure 2.8: Shape of training and test sets'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8：训练集和测试集的形状'
- en: '](img/B16925_02_08.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_08.jpg)'
- en: 'Figure 2.8: Shape of training and test sets'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：训练集和测试集的形状
- en: You should confirm for yourself that the number of samples (rows) in the training
    and test sets is consistent with an 80/20 split.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该自行确认训练集和测试集中的样本（行数）是否与80/20拆分一致。
- en: In making the train/test split, we've also set the `random_state` parameter,
    which is a random number seed. Using this parameter allows a consistent train/test
    split across runs of this notebook. Otherwise, the random splitting procedure
    would select a different 20% of the data for testing each time the code was run.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行训练/测试拆分时，我们还设置了`random_state`参数，这是一个随机数种子。使用这个参数可以确保在每次运行这个笔记本时，训练/测试拆分是一致的。否则，随机拆分过程每次运行时都会选择不同的20%数据进行测试。
- en: 'The first argument to `train_test_split` is the features, in this case just
    `EDUCATION`, and the second argument is the response. There are four outputs:
    the features of the samples in the training and test sets, respectively, and the
    corresponding response variables that go with these sets of features. All this
    function has done is randomly select 20% of the row indices from the dataset and
    subset out these features and responses as test data, leaving the rest for training.
    Now that we have our training and test data, it''s good to make sure the nature
    of the data is the same between these sets. In particular, is the fraction of
    the positive class similar? You can observe this in the following output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_test_split`的第一个参数是特征，在本例中是`EDUCATION`，第二个参数是响应。该函数有四个输出：分别是训练集和测试集中的样本特征，以及与这些特征集对应的响应变量。此函数所做的只是从数据集中随机选择20%的行索引，并将这些特征和响应子集作为测试数据，剩下的用于训练。现在我们已经有了训练数据和测试数据，确保数据的性质在这些集合中是一致的很重要。特别是，正类的比例是否相似？您可以通过以下输出进行观察：'
- en: '![Figure 2.9: Class fractions in training and test data'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9: 训练数据和测试数据中的类别分布'
- en: '](img/B16925_02_09.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_09.jpg)'
- en: 'Figure 2.9: Class fractions in training and test data'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.9: 训练数据和测试数据中的类别分布'
- en: The positive class fractions in the training and test data are both about 22%.
    This is good, as we can say that the training set is representative of the test
    set. In this case, since we have a pretty large dataset with tens of thousands
    of samples, and the classes are not too imbalanced, we didn't have to take precautions
    to ensure this happens.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集中的正类比例均约为22%。这是好的，因为我们可以说训练集是测试集的代表。在这种情况下，由于我们拥有一个包含数万个样本的大型数据集，并且类别不太失衡，因此我们不必采取额外的预防措施来确保这种情况发生。
- en: However, you can imagine that if the dataset were smaller, and the positive
    class very rare, it may be that the class fractions would be noticeably different
    between the training and test sets, or worse yet, there might be no positive samples
    at all in the test set. In order to guard against such scenarios, you could use
    `stratify` keyword argument of `train_test_split`. This procedure also makes a
    random split of the data into training and test sets but guarantees that the class
    fractions will be equal or very similar.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您可以想象，如果数据集较小，且正类非常稀有，训练集和测试集之间的类别比例可能会明显不同，甚至更糟，测试集中可能根本没有正样本。为了防止这种情况，您可以使用`train_test_split`的`stratify`关键字参数。此过程也会随机地将数据划分为训练集和测试集，但可以确保类别比例相等或非常相似。
- en: Note
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**Out-of-time testing**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**超时测试**'
- en: 'If your data contains both features and responses that span a substantial period
    of time, it''s a good practice to try making your train/test split over time.
    For example, if you have two years of data with features and responses from every
    month, you may wish to try sequentially training the model on 12 months of data
    and testing on the next month, or the month after that, depending on what is operationally
    feasible when the model will be used. You could repeat this until you''ve exhausted
    your data, to get a few different test scores. This will give you useful insights
    into model performance because it simulates the actual conditions the model will
    face when it is deployed: a model trained on old features and responses will be
    used to make predictions on new data. In the case study, the responses only come
    from one point in time (credit defaults within one month), so this is not an option
    here.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据包含跨越较长时间段的特征和响应，最好尝试基于时间进行训练/测试集划分。例如，如果您有两年的数据，每个月都有特征和响应，您可能希望尝试依次用12个月的数据训练模型，然后用下一个月或下下个月的数据进行测试，具体取决于在模型使用时可操作的情况。您可以一直重复这个过程，直到数据用完，以获得几个不同的测试分数。这将为您提供有价值的模型性能洞察，因为它模拟了模型部署时实际面临的条件：一个在旧特征和响应上训练的模型将用于对新数据进行预测。在案例研究中，响应仅来自某一时刻（一个月内的信用违约），所以这里不适用此方法。
- en: Classification Accuracy
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类准确率
- en: 'Now we proceed to fit an example model to illustrate binary classification
    metrics. We will continue to use logistic regression with near-default options,
    choosing the same options we demonstrated in *Chapter 1,* *Data Exploration and
    Cleaning*:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续拟合一个示例模型，以说明二分类度量。我们将继续使用接近默认选项的逻辑回归，选择我们在*第1章*，*数据探索与清洗*中演示的相同选项：
- en: '![Figure 2.10: Loading the model class and creating a model object'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.10: 加载模型类并创建模型对象'
- en: '](img/B16925_02_10.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_10.jpg)'
- en: 'Figure 2.10: Loading the model class and creating a model object'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10：加载模型类并创建模型对象
- en: 'Now we proceed to train the model, as you might imagine, using the labeled
    data from our training set. We proceed immediately to use the trained model to
    make predictions on the features of the samples from the held-out test set:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续训练模型，正如你想象的那样，使用我们训练集中的标签数据。我们接着使用训练好的模型对从保留的测试集中的样本特征进行预测：
- en: '![Figure 2.11: Training a model and making predictions on the test set'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.11：训练模型并对测试集进行预测](img/B16925_02_11.jpg)'
- en: '](img/B16925_02_11.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_11.jpg)'
- en: 'Figure 2.11: Training a model and making predictions on the test set'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11：训练模型并对测试集进行预测
- en: 'We''ve stored the model-predicted labels of the test set in a variable called
    `y_pred`. How should we now assess the quality of these predictions? We have the
    true labels, in the `y_test` variable. First, we will compute what is probably
    the simplest of all binary classification metrics: **accuracy**. Accuracy is defined
    as the proportion of samples that were correctly classified.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将测试集的模型预测标签存储在一个名为 `y_pred` 的变量中。现在我们该如何评估这些预测的质量呢？我们有真实标签，存储在 `y_test`
    变量中。首先，我们将计算可能是所有二分类指标中最简单的一个：**准确度**。准确度被定义为正确分类样本所占的比例。
- en: 'One way to calculate accuracy is to create a logical mask that is `True` whenever
    the predicted label is equal to the actual label, and `False` otherwise. We can
    then take the average of this mask, which will interpret `True` as 1 and `False`
    as 0, giving us the proportion of correct classifications:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 计算准确度的一种方法是创建一个逻辑掩码，当预测标签等于实际标签时，掩码为`True`，否则为`False`。我们可以计算这个掩码的平均值，将`True`视为
    1，`False`视为 0，从而得到正确分类的比例：
- en: '![Figure 2.12: Calculating classification accuracy with a logical mask'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.12：使用逻辑掩码计算分类准确度](img/B16925_02_12.jpg)'
- en: '](img/B16925_02_12.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_12.jpg)'
- en: 'Figure 2.12: Calculating classification accuracy with a logical mask'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12：使用逻辑掩码计算分类准确度
- en: 'This indicates that the model is correct 78% of the time. While this is a pretty
    straightforward calculation, there are actually easier ways to calculate accuracy
    using the convenience of scikit-learn. One way is to use the trained model''s
    `.score` method, passing the features of the test data to make predictions on,
    as well as the test labels. This method makes the predictions and then does the
    same calculation we performed previously, all in one step. Or, we could import
    scikit-learn''s `metrics` library, which includes many model performance metrics,
    such as `accuracy_score`. For this, we pass the true labels and the predicted
    labels:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示模型在 78%的时间里是正确的。虽然这是一个非常直接的计算方法，但实际上使用 scikit-learn 更简便的方法来计算准确度。我们可以使用训练好的模型的
    `.score` 方法，将测试数据的特征传递给它进行预测，同时传递测试标签。该方法会执行预测，然后进行我们之前所做的相同计算，所有这些都可以一步完成。或者，我们可以导入
    scikit-learn 的 `metrics` 库，该库包含许多模型性能指标，比如 `accuracy_score`。为此，我们需要传递真实标签和预测标签：
- en: '![Figure 2.13: Calculating classification accuracy with scikit-learn'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.13：使用 scikit-learn 计算分类准确度](img/B16925_02_13.jpg)'
- en: '](img/B16925_02_13.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_13.jpg)'
- en: 'Figure 2.13: Calculating classification accuracy with scikit-learn'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13：使用 scikit-learn 计算分类准确度
- en: 'These all give the same result, as they should. Now that we know how accurate
    the model is, how do we interpret this metric? On the surface, an accuracy of
    78% may sound good. We are getting most of the predictions right. However, an
    important test for the accuracy of binary classification is to compare things
    to a very simple hypothetical model that only makes one prediction: this hypothetical
    model predicts the majority class for every sample, no matter what the features
    are. While in practice this model is useless, it provides an important extreme
    case with which to compare the accuracy of our trained model. Such extreme cases
    are sometimes referred to as null models.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法都会得到相同的结果，正如预期的那样。现在我们知道了模型的准确度，那么如何解释这个指标呢？表面上看，78%的准确度可能听起来不错。我们大部分的预测都正确。然而，二分类准确度的一个重要测试是将其与一个非常简单的假设模型进行比较：这个假设模型对每个样本的预测都是相同的——无论特征是什么，它总是预测多数类别。虽然在实际中这个模型没有什么用处，但它提供了一个重要的极端情况，供我们用来与已训练模型的准确度进行比较。这样的极端情况有时被称为“零模型”。
- en: Think about what the accuracy of such a null model would be. In our dataset,
    we know that about 22% of the samples are positive. So, the negative class is
    the majority class, with the remaining 78% of the samples. Therefore, a null model
    for this dataset, which always predicts the majority negative class, will be right
    78% of the time. Now when we compare our trained model here to such a null model,
    it becomes clear that an accuracy of 78% is actually not very useful. We can get
    the same accuracy with a model that doesn't pay any attention to the features.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 想想看，这样一个空模型的准确率会是多少。在我们的数据集中，我们知道大约22%的样本是正类。因此，负类是多数类，占剩余的78%的样本。因此，一个总是预测负类的空模型在这个数据集中将有78%的正确率。当我们将训练好的模型与这个空模型进行比较时，就会发现，78%的准确率其实并没有太大意义。我们可以通过一个不关注任何特征的模型来获得相同的准确率。
- en: While we can interpret accuracy in terms of a majority-class null model, there
    are other binary classification metrics that delve a little deeper into how the
    model is performing for negative, as well as positive samples separately.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以通过多数类空模型来解释准确率，但还有其他一些二分类指标可以更深入地了解模型对正类和负类样本的表现。
- en: True Positive Rate, False Positive Rate, and Confusion Matrix
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真阳性率、假阳性率与混淆矩阵
- en: 'In binary classification, there are just two labels to consider: positive and
    negative. As a more descriptive way to look at model performance than the accuracy
    of prediction across all samples, we can also look at the accuracy of only those
    samples that have a positive label. The proportion of these that we successfully
    predict as positive is called the **true positive rate** (**TPR**). If we say
    that **P** is the number of samples in the **positive class** in the test data,
    and **TP** is the number of **true positives**, defined as the number of positive
    samples that were predicted to be positive by the model, then the TPR is as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类中，只有两个标签需要考虑：正类和负类。作为比全体样本准确率更具描述性的模型性能评估方式，我们还可以仅查看那些正类标签样本的预测准确度。我们成功预测为正类的比例称为**真阳性率**（**TPR**）。如果我们设**P**为测试数据中**正类**样本的数量，**TP**为**真阳性**的数量，定义为被模型正确预测为正类的正类样本数，那么TPR公式如下：
- en: '![Figure 2.14: TPR equation'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14：TPR 公式'
- en: '](img/B16925_02_14.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_14.jpg)'
- en: 'Figure 2.14: TPR equation'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14：TPR 公式
- en: 'The flip side of the true positive rate is the **false negative rate** (**FNR**).
    This is the proportion of positive test samples that we incorrectly predicted
    as negative. Such errors are called **false negatives** (**FN**) and the **false
    negative rate** (**FNR**) is calculated as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性率的反面是**假阴性率**（**FNR**）。这表示我们错误地预测为负类的正类测试样本的比例。这样的错误被称为**假阴性**（**FN**），**假阴性率**（**FNR**）的计算公式如下：
- en: '![Figure 2.15: FNR equation'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.15：FNR 公式'
- en: '](img/B16925_02_15.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_15.jpg)'
- en: 'Figure 2.15: FNR equation'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15：FNR 公式
- en: 'Since all the positive samples are either correctly or incorrectly predicted,
    the sum of the number of true positives and the number of false negatives equals
    the total number of positive samples. Mathematically, *P = TP + FN*, and therefore,
    using the definitions of TPR and FNR, we have the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有的正类样本要么被正确预测，要么被错误预测，因此真阳性数与假阴性数的总和等于正类样本的总数。从数学上讲，*P = TP + FN*，因此，结合TPR和FNR的定义，我们可以得到以下公式：
- en: '![Figure 2.16: The relation between the TPR and FNR'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.16：TPR与FNR的关系'
- en: '](img/B16925_02_16.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_16.jpg)'
- en: 'Figure 2.16: The relation between the TPR and FNR'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16：TPR与FNR的关系
- en: Since the TPR and FNR sum to 1, it's sufficient to just calculate one of them.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TPR和FNR的和为1，因此只需要计算其中一个就足够了。
- en: 'Similar to the TPR and FNR, there is the **true negative rate** (**TNR**) and
    the **false positive rate** (**FPR**). If **N** is the number of **negative**
    samples, the sum of **true negative** samples (**TN**) is the number of these
    that are correctly predicted, and the sum of **false positive** (**FP**) samples
    is the number incorrectly predicted as positive:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与TPR和FNR类似，还有**真负率**（**TNR**）和**假阳性率**（**FPR**）。如果**N**是**负类**样本的数量，那么**真负**样本（**TN**）的总和是那些被正确预测为负类的数量，**假阳性**（**FP**）样本的总和则是被错误预测为正类的样本数量：
- en: '![Figure 2.17: TNR equation'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.17：TNR 公式'
- en: '](img/B16925_02_17.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_17.jpg)'
- en: 'Figure 2.17: TNR equation'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.17：TNR 公式
- en: '![Figure 2.18: FPR equation'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.18：FPR 公式'
- en: '](img/B16925_02_18.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_18.jpg)'
- en: 'Figure 2.18: FPR equation'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18：FPR 公式
- en: '![Figure 2.19: Relation between the TNR and FPR'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.19：TNR 与 FPR 之间的关系'
- en: '](img/B16925_02_19.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_19.jpg)'
- en: 'Figure 2.19: Relation between the TNR and FPR'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.19：TNR 与 FPR 之间的关系
- en: 'True and false positives and negatives can be conveniently summarized in a
    table called a **confusion matrix**. A confusion matrix for a binary classification
    problem is a 2 x 2 matrix where the true class is along one axis and the predicted
    class is along the other. The confusion matrix gives a quick summary of how many
    true and false positives and negatives there are:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 真实正例、假正例和假负例可以方便地在一个表格中总结，这个表格叫做**混淆矩阵**。二分类问题的混淆矩阵是一个 2 x 2 的矩阵，其中真实类别位于一个轴上，预测类别位于另一个轴上。混淆矩阵快速总结了真实和假正例及假负例的数量：
- en: '![Figure 2.20: The confusion matrix for binary classification'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.20：二分类的混淆矩阵'
- en: '](img/B16925_02_20.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_20.jpg)'
- en: 'Figure 2.20: The confusion matrix for binary classification'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.20：二分类的混淆矩阵
- en: 'Since we hope to make correct classifications, we hope that the **diagonal**
    entries (that is, the entries along a diagonal line from the top left to the bottom
    right: TN and TP) of the confusion matrix are relatively large, while the off-diagonals
    are relatively small, as these represent incorrect classifications. The accuracy
    metric can be calculated from the confusion matrix by adding up the entries on
    the diagonal, which are predictions that are correct, and dividing by the total
    number of all predictions.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望做出正确的分类，我们希望混淆矩阵的**对角线**条目（即从左上角到右下角的对角线上的条目：TN 和 TP）相对较大，而非对角线条目较小，因为它们代表错误的分类。可以通过将对角线上的条目（即正确的预测）相加，然后除以所有预测的总数来计算准确度。
- en: 'Exercise 2.02: Calculating the True and False Positive and Negative Rates and
    Confusion Matrix in Python'
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.02：在 Python 中计算真实和假正例与假负例率以及混淆矩阵
- en: 'In this exercise, we''ll use the test data and model predictions from the logistic
    regression model we created previously, using only the `EDUCATION` feature. We
    will illustrate how to manually calculate the true and false positive and negative
    rates, as well as the numbers of true and false positives and negatives needed
    for the confusion matrix. Then we will show a quick way to calculate a confusion
    matrix with scikit-learn. Perform the following steps to complete the exercise,
    noting that some code from the previous section must be run before doing this
    exercise (as seen on GitHub):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用之前创建的逻辑回归模型的测试数据和模型预测，只使用`EDUCATION`特征。我们将展示如何手动计算真实正例和假负例率，以及混淆矩阵所需的真实和假正例及假负例的数量。然后我们将展示使用
    scikit-learn 快速计算混淆矩阵的方法。执行以下步骤来完成练习，注意在做此练习之前必须先运行上一部分的代码（如 GitHub 上所示）：
- en: Note
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Jupyter notebook for this exercise can be found here: [https://packt.link/S02kz](https://packt.link/S02kz).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的 Jupyter 笔记本可以在此找到：[https://packt.link/S02kz](https://packt.link/S02kz)。
- en: 'Run this code to calculate the number of positive samples:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此代码计算正例样本的数量：
- en: '[PRE29]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output should appear like this:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '[PRE30]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now we need the number of true positives. These are samples where the true label
    is 1 and the prediction is also 1\. We can identify these with a logical mask
    for the samples that are positive (`y_test==1`) `&` is the logical `y_pred==1`).
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们需要真实正例的数量。这些是实际标签为 1 且预测也为 1 的样本。我们可以通过逻辑掩码来识别这些样本，其中正例为(`y_test==1`)，并且`&`是逻辑运算符，`y_pred==1`）。
- en: 'Use this code to calculate the number of true positives:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码计算真实正例的数量：
- en: '[PRE31]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here is the output:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE32]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The true positive rate is the proportion of true positives to positives, which
    of course would be 0 here.
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真实正例率是指真实正例与所有正例的比例，这在这里当然是 0。
- en: 'Run the following code to obtain the TPR:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码获取真实正例率（TPR）：
- en: '[PRE33]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You will obtain the following output:'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '[PRE34]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Similarly, we can identify the false negatives.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类似地，我们可以识别假负例。
- en: 'Calculate the number of false negatives with this code:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码计算假负例的数量：
- en: '[PRE35]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This should output the following: 1155'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：1155
- en: We'd also like the FNR.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还希望得到假负率（FNR）。
- en: 'Calculate the FNR with this code:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码计算假负率（FNR）：
- en: '[PRE36]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This should output the following:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE37]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '**What have we learned from the true positive and false negative rates?**'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**从真实正例和假负例率中我们学到了什么？**'
- en: First, we can confirm that they sum to 1\. This fact is easy to see because
    the TPR = 0 and the FPR = 1\. What does this tell us about our model? On the test
    set, at least for the positive samples, the model has in fact acted as a majority-class
    null model. Every positive sample was predicted to be negative, so none of them
    was correctly predicted.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们可以确认它们的总和为 1。这个事实很容易看出来，因为 TPR = 0 且 FPR = 1。这告诉我们关于模型什么信息？在测试集上，至少对于正样本，模型实际上表现得像一个多数类零模型。每个正样本都被预测为负样本，因此没有一个被正确预测。
- en: 'Let''s find the TNR and FPR of our test data. Since these calculations are
    very similar to those we looked at previously, we show them all at once and illustrate
    a new Python function:![Figure 2.21: Calculating true negative and false positive
    rates and printing them'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们找出测试数据的 TNR 和 FPR。由于这些计算与我们之前查看的非常相似，因此我们一次性展示它们，并介绍一个新的 Python 函数：![图 2.21：计算真正负类率和假正类率并打印它们
- en: '](img/B16925_02_21.jpg)'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_02_21.jpg)'
- en: 'Figure 2.21: Calculating true negative and false positive rates and printing
    them'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.21：计算真正负类率和假正类率并打印它们
- en: In addition to calculating the TNR and FPR in a similar way that we had previously
    with the TPR and FNR, we demonstrate the `print` function in Python along with
    the `.format` method for strings, which allows substitution of variables in locations
    marked by curly braces `{}`. There is a range of options for formatting numbers,
    such as including a certain number of decimal places.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了像我们之前那样计算 TNR 和 FPR，我们还展示了 Python 中的 `print` 函数，并结合 `.format` 方法来处理字符串，这样可以在大括号
    `{}` 标记的位置替换变量。还有多种选项可以格式化数字，例如保留特定的小数位数。
- en: Note
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: For additional details, refer to [https://docs.python.org/3/tutorial/inputoutput.html](https://docs.python.org/3/tutorial/inputoutput.html).
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如需更多细节，请参考 [https://docs.python.org/3/tutorial/inputoutput.html](https://docs.python.org/3/tutorial/inputoutput.html)。
- en: Now, what have we learned here? In fact, our model behaves exactly like the
    majority-class null model for all samples, both positive and negative. It's clear
    we're going to need a better model.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那么，我们在这里学到了什么？事实上，我们的模型对所有样本（正样本和负样本）表现得像一个多数类零模型。显然，我们需要一个更好的模型。
- en: While we have manually calculated all the entries of the confusion matrix in
    this exercise, in scikit-learn there is a quick way to do this. Note that in scikit-learn,
    the true class is along the vertical axis and the predicted class is along the
    horizontal axis of the confusion matrix, as we presented earlier.
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然我们在本次练习中手动计算了混淆矩阵的所有条目，但在 scikit-learn 中有一种快速的方法来做这件事。请注意，在 scikit-learn 中，真正类位于混淆矩阵的纵轴上，预测类位于横轴上，正如我们之前所展示的那样。
- en: 'Create a confusion matrix in scikit-learn with this code:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在 scikit-learn 中创建混淆矩阵：
- en: '[PRE38]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You will obtain the following output:'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 2.22: The confusion matrix for our example model'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.22：我们示例模型的混淆矩阵'
- en: '](img/B16925_02_22.jpg)'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_02_22.jpg)'
- en: 'Figure 2.22: The confusion matrix for our example model'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.22：我们示例模型的混淆矩阵
- en: 'All the information we need to calculate the TPR, FNR, TNR, and FPR is contained
    in the confusion matrix. We also note that there are many more classification
    metrics that can be derived from the confusion matrix. In fact, some of these
    are actually synonyms for ones we''ve already examined here. For example, the
    TPR is also called **recall** and **sensitivity**. Along with recall, another
    metric that is often used for binary classification is **precision**: this is
    the proportion of positive predictions that are correct (as opposed to the proportion
    of positive samples that are correctly predicted). We''ll get more experience
    with precision in the activity for this chapter.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 TPR、FNR、TNR 和 FPR 所需的所有信息都包含在混淆矩阵中。我们还注意到，可以从混淆矩阵中派生出许多其他分类指标。实际上，其中一些实际上是我们已经讨论过的指标的同义词。例如，TPR
    也叫做 **召回率** 和 **敏感性**。与召回率一起，二分类中常用的另一个指标是 **精确度**：它是正确的正类预测的比例（与正确预测的正样本比例相对）。在本章的活动中，我们将进一步了解精确度。
- en: Note
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**Multiclass classification**'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**多类分类**'
- en: 'Our case study involves a binary classification problem, with only two possible
    outcomes: the account does or does not default. Another important type of machine
    learning classification problem is multiclass classification. In multiclass classification,
    there are several possible mutually exclusive outcomes. A classic example is image
    recognition of handwritten digits; a handwritten digit should be only one of 0,
    1, 2, … 9\. Although multiclass classification is outside the scope of this book,
    the metrics we are learning now for binary classification can be extended to the
    multiclass setting.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的案例研究涉及一个二分类问题，只有两种可能的结果：账户是否违约。另一种重要的机器学习分类问题是多分类问题。在多分类问题中，存在若干个相互排斥的结果。一个经典的例子是手写数字的图像识别；一个手写数字应该只能是0、1、2、…
    9之一。尽管多分类问题超出了本书的范围，但我们现在学习的二分类指标可以扩展到多分类设置中。
- en: 'Discovering Predicted Probabilities: How Does Logistic Regression Make Predictions?'
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现预测概率：逻辑回归如何做出预测？
- en: Now that we're familiar with accuracy, true and false positives and negatives,
    and the confusion matrix, we can explore new ways of using logistic regression
    to learn about more advanced binary classification metrics. So far, we've only
    considered logistic regression as a "black box" that can learn from labeled training
    data and then make binary predictions on new features. While we will learn about
    the workings of logistic regression in detail later in the book, we can begin
    to peek inside the black box now.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了准确率、真阳性和假阳性、真阴性和假阴性，以及混淆矩阵，我们可以探索使用逻辑回归学习更多高级的二分类指标的方法。到目前为止，我们仅将逻辑回归视为一个可以从标注的训练数据中学习，然后对新特征做出二分类预测的“黑箱”。虽然我们稍后会详细学习逻辑回归的工作原理，但我们现在可以开始窥探这个黑箱的内部。
- en: One thing to understand about how logistic regression works is that the raw
    predictions – in other words, the direct outputs from the mathematical equation
    that defines logistic regression – are not binary labels. They are actually **probabilities**
    on a scale from 0 to 1 (although, technically, the equation never allows the probabilities
    to be exactly equal to 0 or 1, as we'll see later). These probabilities are only
    transformed into binary predictions through the use of a **threshold**. The threshold
    is the probability above which a prediction is declared to be positive, and below
    which it is negative. The threshold in scikit-learn is 0.5\. This means any sample
    with a predicted probability of at least 0.5 is identified as positive, and any
    with a predicted probability < 0.5 is decided to be negative. However, we are
    free to use any threshold we want. In fact, choosing the threshold is one of the
    key flexibilities of logistic regression, as well as other machine learning classification
    algorithms that estimate probabilities of class membership.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 理解逻辑回归的工作方式的一件事是，原始预测——换句话说，从定义逻辑回归的数学方程得出的直接输出——并不是二进制标签。它们实际上是一个从0到1的**概率**（尽管从技术上讲，这个方程永远不会允许概率等于0或1，稍后我们将看到）。这些概率只有通过使用**阈值**才能转化为二分类预测。阈值是用来决定预测为正类的概率值，低于该值则预测为负类。scikit-learn中的默认阈值是0.5。这意味着，任何预测概率至少为0.5的样本都会被识别为正类，而预测概率小于0.5的样本则被判定为负类。然而，我们可以自由选择任何我们想要的阈值。事实上，选择阈值是逻辑回归以及其他机器学习分类算法中估计类别成员概率的关键灵活性之一。
- en: 'Exercise 2.03: Obtaining Predicted Probabilities from a Trained Logistic Regression
    Model'
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.03：从训练好的逻辑回归模型中获取预测概率
- en: In the following exercise, we will get familiar with the predicted probabilities
    of logistic regression and how to obtain them from a scikit-learn model.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将熟悉逻辑回归的预测概率，以及如何从scikit-learn模型中获取它们。
- en: We can begin to discover predicted probabilities by further examining the methods
    available to us on the logistic regression model object that we trained earlier
    in this chapter. Recall that before, once we trained the model, we could then
    make binary predictions using the values of features from new samples by passing
    these values to the `.predict` method of the trained model. These are predictions
    made on the assumption of a threshold of 0.5.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过进一步检查在本章中早些时候训练的逻辑回归模型对象上可用的方法，来开始发现预测概率。回想一下，在我们训练模型后，可以通过将新样本的特征值传递给训练好的模型的`.predict`方法，来进行二分类预测。这些是基于0.5的阈值假设做出的预测。
- en: 'However, we can directly access the predicted probabilities of these samples,
    using the `.predict_proba` method. Perform the following steps to complete the
    exercise, keeping in mind that you will need to recreate the same model trained
    previously in the chapter if you are starting a new notebook:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以直接访问这些样本的预测概率，使用`.predict_proba`方法。执行以下步骤来完成练习，请记住，如果您开始一个新的笔记本，您需要重新创建在本章中之前训练的相同模型：
- en: Note
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Jupyter notebook for this exercise can be found here: [https://packt.link/yDyQn](https://packt.link/yDyQn).
    The notebook contains the prerequisite steps of training the model and should
    be executed prior to the first step shown here.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的 Jupyter 笔记本可以在这里找到：[https://packt.link/yDyQn](https://packt.link/yDyQn)。该笔记本包含训练模型的先决步骤，应该在这里显示的第一步之前执行。
- en: 'Obtain the predicted probabilities for the test samples using this code:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码获取测试样本的预测概率：
- en: '[PRE39]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output should be as follows:'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '![Figure 2.23: Predicted probabilities of the test data](img/B16925_02_23.jpg)'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.23：测试数据的预测概率](img/B16925_02_23.jpg)'
- en: 'Figure 2.23: Predicted probabilities of the test data'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.23：测试数据的预测概率
- en: 'We see in the output of this, which we''ve stored in `y_pred_proba`, that there
    are two columns. This is because there are two classes in our classification problem:
    negative and positive. Assuming the negative labels are coded as 0 and the positives
    as 1, as they are in our data, scikit-learn will report the probability of negative
    class membership as the first column, and positive class membership as the second.'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以在存储的`y_pred_proba`的输出中看到，那里有两列。这是因为我们的分类问题中有两个类别：负类和正类。假设负类标签编码为 0，正类标签编码为
    1（如数据中所示），scikit-learn 会将负类成员资格的概率报告为第一列，正类成员资格的概率报告为第二列。
- en: Since the two classes are mutually exclusive and are the only options, the sum
    of predicted probabilities for the two classes should equal 1 for every sample.
    Let's confirm this.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于这两个类别是互斥的，并且是唯一的选项，因此每个样本的两类预测概率和应为 1。让我们确认这一点。
- en: First, we can use `np.sum` over the first dimension (columns) to calculate the
    sum of probabilities for each sample.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们可以在第一维度（列）上使用`np.sum`来计算每个样本的概率和。
- en: 'Calculate the sum of predicted probabilities for each sample with this code:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此代码计算每个样本的预测概率和：
- en: '[PRE40]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE41]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: It certainly looks like all 1s. We should check to see that the result is the
    same shape as the array of test data labels.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 看起来确实全是 1。我们应该检查结果是否与测试数据标签的数组形状相同。
- en: 'Check the array shape with this code:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此代码检查数组形状：
- en: '[PRE42]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This should output the following:'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '[PRE43]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Good; this is the expected shape. Now, to check that each value is 1\. We use
    `np.unique` to show all the unique elements of this array. This is similar to
    `DISTINCT` in SQL. If all the probability sums are indeed 1, there should only
    be one unique element of the probability array: 1.'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 很好；这正是预期的形状。现在，检查每个值是否为 1。我们使用`np.unique`来显示这个数组中所有唯一的元素。这类似于 SQL 中的`DISTINCT`。如果所有概率和确实为
    1，那么概率数组中应该只有一个唯一元素：1。
- en: 'Show all unique array elements with this code:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此代码显示所有唯一的数组元素：
- en: '[PRE44]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This should output the following:'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '[PRE45]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: After confirming our belief in the predicted probabilities, we note that since
    class probabilities sum to 1, it's sufficient to just consider the second column,
    the predicted probability of positive class membership. Let's capture these in
    an array.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在确认我们对预测概率的信心后，我们注意到，由于类概率和为 1，因此只考虑第二列，即正类成员资格的预测概率就足够了。让我们将这些捕获到一个数组中。
- en: 'Run this code to put the second column of the predicted probabilities array
    (predicted probability of membership in the positive class) in an array:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此代码，将预测概率数组的第二列（正类成员资格的预测概率）放入一个数组中：
- en: '[PRE46]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output should be as follows:'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '![Figure 2.24: Predicted probabilities of positive class membership'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.24：正类成员资格的预测概率](img/B16925_02_24.jpg)'
- en: '](img/B16925_02_24.jpg)'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_02_24.jpg)'
- en: 'Figure 2.24: Predicted probabilities of positive class membership'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.24：正类成员资格的预测概率
- en: What do these probabilities look like? One way to find out, and a good diagnostic
    for model output, is to plot the predicted probabilities. A histogram is a natural
    way to do this, for which we can use the matplotlib function, `hist()`. Note that
    if you execute a cell with only the histogram function, you will get the output
    of the NumPy histogram function returned before the plot. This includes the number
    of samples in each bin and the locations of the bin edges.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些预测概率看起来如何？一种了解它们的方法，也是评估模型输出的一个好诊断手段，就是绘制预测概率的直方图。直方图是一种自然的方式，我们可以使用matplotlib的`hist()`函数来实现。请注意，如果你只执行包含直方图函数的代码单元，你会在绘图之前得到NumPy直方图函数的输出。这个输出包括每个桶中的样本数和桶边界的位置。
- en: 'Execute this code to see histogram output and an unformatted plot (not shown here):'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行此代码可以查看直方图输出和一个未格式化的图（此处未显示）：
- en: '[PRE47]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.25: Details of histogram calculation'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.25: 直方图计算的细节'
- en: '](img/B16925_02_25.jpg)'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_02_25.jpg)'
- en: 'Figure 2.25: Details of histogram calculation'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 2.25: 直方图计算的细节'
- en: This may be useful information for you and could also be obtained directly from
    the `np.histogram()` function. However, here we're mainly interested in the plot,
    so we adjust the font size and add some axis labels.
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些信息可能对你有用，并且也可以直接通过`np.histogram()`函数获得。然而，在这里我们主要关注图形，因此我们调整了字体大小并添加了一些坐标轴标签。
- en: 'Run this code for a formatted histogram plot of predicted probabilities:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此代码以获得格式化的预测概率直方图：
- en: '[PRE48]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The plot should look like this:'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图应如下所示：
- en: '![Figure 2.26: Histogram plot of predicted probabilities'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.26: 预测概率的直方图'
- en: '](img/B16925_02_26.jpg)'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_02_26.jpg)'
- en: 'Figure 2.26: Histogram plot of predicted probabilities'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 2.26: 预测概率的直方图'
- en: Notice that in the histogram of probabilities, there are only four bins that
    actually have samples in them, and they are spaced fairly far apart. This is because
    there are only four unique values for the `EDUCATION` feature, which is the only
    feature in our example model.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在预测概率的直方图中，实际上只有四个桶中有样本，而且它们之间的间隔相当大。这是因为在我们的示例模型中，`EDUCATION`特征只有四个独特的值。
- en: Also, notice that all the predicted probabilities are below 0.5\. This is the
    reason every sample was predicted to be negative, using the 0.5 threshold. We
    can imagine that if we set our threshold below 0.5, we would get different results.
    For example, if we set the threshold at 0.25, all of the samples in the smallest
    bin to the far right of *Figure 2.26* would be classified as positive, since the
    predicted probability for all of these is above 0.25\. It would be informative
    for us if we could see how many of these samples actually had positive labels.
    Then we could see whether moving our threshold down to 0.25 would improve the
    performance of our classifier by classifying the samples in the rightmost bin
    as positive.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，注意到所有预测概率都低于0.5。正是因为使用了0.5的阈值，每个样本都被预测为负类。如果我们将阈值设定为低于0.5，我们可能会得到不同的结果。例如，如果我们将阈值设置为0.25，那么*图
    2.26*最右边最小的那一栏中的所有样本都会被分类为正类，因为这些样本的预测概率都高于0.25。如果我们能够看到这些样本中实际上有多少是正类标签，那么这对我们来说是有价值的信息。这样，我们就能知道将阈值调低到0.25是否能通过将最右边一栏的样本分类为正类来改善分类器的性能。
- en: In fact, we can visualize this easily, using a `y_test == 1`, and then to get
    negative samples, where `y_test == 0`.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实际上，我们可以通过以下方式轻松可视化这一点，使用`y_test == 1`来获取正样本，然后使用`y_test == 0`来获取负样本。
- en: 'Isolate the predicted probabilities for positive and negative samples with
    this code:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码隔离正负样本的预测概率：
- en: '[PRE49]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Now we want to plot these as a stacked histogram. The code is similar to the
    histogram we already created, except that we will pass a list of arrays to be
    plotted, which are the arrays of probabilities for positive and negative samples
    we just created, and a keyword indicating we'd like the bars to be stacked, as
    opposed to plotted side by side. We'll also create a legend so that the colors
    are clearly identifiable on the plot.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们希望将这些数据绘制为堆叠直方图。代码与我们之前创建的直方图类似，不同之处在于，我们将传递一个包含数组的列表，这些数组分别是我们刚刚创建的正负样本的预测概率数组，并且添加一个关键字，指示我们希望柱形图堆叠而非并排显示。同时，我们还将创建一个图例，以便颜色能够在图中清晰区分。
- en: 'Plot a stacked histogram using this code:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码绘制堆叠直方图：
- en: '[PRE50]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The plot should look like this:'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图应如下所示：
- en: '![Figure 2.27: Stacked histogram of predicted probabilities by class'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.27: 按类别堆叠的预测概率直方图'
- en: '](img/B16925_02_27.jpg)'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_02_27.jpg)'
- en: 'Figure 2.27: Stacked histogram of predicted probabilities by class'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.27：按类别堆叠的预测概率直方图
- en: The plot shows us the true labels of the samples for each predicted probability.
    Now we can consider what the effect would be of lowering the threshold to 0.25\.
    Take a moment and think about what this would mean, keeping in mind that any sample
    with a predicted probability at or above the threshold would be classified as
    positive.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了每个预测概率的样本的真实标签。现在我们可以考虑将阈值降低到0.25时的效果。花一点时间思考一下这意味着什么，记住任何预测概率达到或超过阈值的样本都将被分类为正样本。
- en: Since nearly all the samples in the small bin to the right of *Figure 2.28*
    are negative samples, if we were to decrease the threshold to 0.25, we would erroneously
    classify these as positive samples and increase our FPR. At the same time, we
    still wouldn't have managed to classify many, if any, positive samples correctly,
    so our TPR wouldn't increase very much at all. Making this change would appear
    to decrease the accuracy of the model.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*图2.28*右侧的小区间几乎全是负样本，如果我们将阈值降低到0.25，我们将错误地将这些样本分类为正样本，并增加我们的FPR。与此同时，我们仍然未能正确分类很多（如果有的话）正样本，因此我们的TPR几乎不会增加。进行这样的改变似乎会降低模型的准确性。
- en: The Receiver Operating Characteristic (ROC) Curve
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接收者操作特征（ROC）曲线
- en: Deciding on a threshold for a classifier is a question of finding the "sweet
    spot" where we are successfully recovering enough true positives, without incurring
    too many false positives. As the threshold is lowered more and more, there will
    be more of both. A good classifier will be able to capture more true positives
    without the expense of a large number of false positives. What would be the effect
    of lowering the threshold even more, with the predicted probabilities from the
    previous exercise? It turns out there is a classic method of visualization in
    machine learning, with a corresponding metric that can help answer this kind of
    question.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为分类器决定一个阈值是一个找到“最佳点”的问题，在这个点上我们成功地回收了足够的真正正样本，同时没有引入太多的假正样本。随着阈值越来越低，正负样本都会增加。一个好的分类器能够捕捉到更多的真正正样本，而不会付出大量假正样本的代价。降低阈值进一步的效果是什么呢？基于前面练习中的预测概率，事实证明，在机器学习中有一种经典的可视化方法和一个相应的度量，可以帮助回答这种问题。
- en: The **receiver operating characteristic** (**ROC**) curve is a plot of the pairs
    of TPRs (*y-axis*) and FPRs (*x-axis*) that result from lowering the threshold
    down from 1 all the way to 0\. You can imagine that if the threshold is 1, there
    are no positive predictions since a logistic regression only predicts probabilities
    strictly between 0 and 1 (endpoints not included). Since there are no positive
    predictions, the TPR and the FPR are both 0, so the ROC curve starts out at (0,
    0). As the threshold is lowered, the TPR will start to increase, hopefully faster
    than the FPR if it's a good classifier. Eventually, when the threshold is lowered
    all the way to 0, every sample is predicted to be positive, including all the
    samples that are, in fact, positive, but also all the samples that are actually
    negative. This means the TPR is 1 but the FPR is also 1\. In between these two
    extremes are the reasonable options for where you may want to set the threshold,
    depending on the relative costs and benefits of true and false positives and negatives
    for the specific problem being considered. In this way, it is possible to get
    a complete picture of the performance of the classifier at all different thresholds
    to decide which one to use.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）曲线是将从1逐渐降低到0的阈值所产生的TPR（*y轴*）和FPR（*x轴*）的配对图。你可以想象，当阈值为1时，没有正预测，因为逻辑回归仅预测0到1之间的概率（不包括端点）。由于没有正预测，TPR和FPR都为0，因此ROC曲线从(0,
    0)开始。随着阈值降低，TPR将开始增加，如果是一个好的分类器，TPR增加的速度应该比FPR更快。最终，当阈值降到0时，每个样本都被预测为正样本，包括所有实际上是正样本的样本，但也包括所有实际上是负样本的样本。这意味着TPR为1，但FPR也是1。在这两个极端之间，是你可能希望设置阈值的合理选项，具体取决于针对特定问题正负样本的相对成本和收益。通过这种方式，你可以全面了解分类器在不同阈值下的性能，从而决定使用哪个阈值。'
- en: 'We could write the code to determine the TPRs and FPRs of the ROC curve by
    using the predicted probabilities and varying the threshold from 1 to 0\. Instead,
    we will use scikit-learn''s convenient functionality, which will take the true
    labels and predicted probabilities as inputs and return arrays of TPRs, FPRs,
    and the thresholds that lead to them. We will then plot the TPRs against the FPRs
    to show the ROC curve. Run this code to use scikit-learn to generate the arrays
    of TPRs and FPRs for the ROC curve, importing the `metrics` module if needed:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编写代码，通过使用预测概率并将阈值从 1 到 0 变化来确定 ROC 曲线的 TPR 和 FPR。相反，我们将使用 scikit-learn 的便捷功能，它将使用真实标签和预测概率作为输入，返回
    TPR、FPR 数组以及导致它们的阈值。然后我们将绘制 TPR 与 FPR 的关系图来展示 ROC 曲线。运行此代码，使用 scikit-learn 生成
    TPR 和 FPR 数组，用于生成 ROC 曲线，必要时导入 `metrics` 模块：
- en: '[PRE51]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now we need to produce a plot. We''ll use `plt.plot`, which will make a line
    plot using the first argument as the *x* values (FPRs), the second argument as
    the *y* values (TPRs), and the shorthand `''*-''` to indicate a line plot with
    star symbols where the data points are located. We add a straight-line plot from
    (0, 0) to (1, 1), which will appear in red (`''r''`) and as a dashed line (`''--''`).
    We''ve also given the plot a legend (which we''ll explain shortly), as well as
    axis labels and a title. This code produces the ROC plot:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要生成一个图表。我们将使用 `plt.plot`，它会使用第一个参数作为 *x* 值（FPR），第二个参数作为 *y* 值（TPR），并使用缩写
    `'*-'` 来表示带有星号符号的线性图，其中数据点所在的位置。我们还添加了一条从（0,0）到（1,1）的直线图，它将显示为红色（`'r'`）并为虚线（`'--'`）。我们还给图表添加了图例（我们稍后会解释），以及坐标轴标签和标题。此代码将生成
    ROC 图：
- en: '[PRE52]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'And the plot should look like this:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 并且图表应如下所示：
- en: '![Figure 2.28: ROC curve for our logistic regression, with a line'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.28：我们的逻辑回归的 ROC 曲线，带有随机机会线供比较](img/B16925_02_28.jpg)'
- en: of random chance shown for comparison
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 用于比较的随机机会显示
- en: '](img/B16925_02_28.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_28.jpg)'
- en: 'Figure 2.28: ROC curve for our logistic regression, with a line of random chance
    shown for comparison'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.28：我们的逻辑回归的 ROC 曲线，带有随机机会线供比较
- en: 'What have we learned from our ROC curve? We can see that it starts at (0,0)
    with a threshold high enough so that there are no positive classifications. Then
    the first thing that happens, as we imagined previously when lowering the threshold
    to about 0.25, is that we get an increase in the FPR, but very little increase
    in the TPR. The effects of continuing to lower the threshold so that the other
    bars from our stacked histogram plot in *Figure 2.28* would be included as positive
    classifications are shown by the subsequent points on the line. We can see the
    thresholds that lead to these rates by examining the threshold array, which is
    not part of the plot. View the thresholds used to calculate the ROC curve using
    this code:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 ROC 曲线中学到了什么？我们可以看到它从（0,0）开始，阈值足够高，以至于没有任何正分类。然后，正如我们之前所想，当将阈值降低到约 0.25
    时，首先发生的事情是我们观察到假阳性率（FPR）增加，但真正的阳性率（TPR）几乎没有增加。继续降低阈值以使堆叠直方图图中*图 2.28*的其他条形图被视为正分类的效果，可以通过线条上的后续点来看。我们可以通过检查阈值数组（它不是图的一部分）来查看导致这些比率的阈值。使用以下代码查看用于计算
    ROC 曲线的阈值：
- en: '[PRE53]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output should be as follows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '[PRE54]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Notice that the first threshold is actually above 1; practically speaking, it
    just needs to be a threshold that's high enough that there are no positive classifications.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一个阈值实际上是大于 1 的；从实际角度来看，它只需要足够高，以至于没有正分类。
- en: 'Now consider what a "good" ROC curve would look like. As we lower the threshold,
    we want to see the TPR increase, which means our classifier is doing a good job
    of correctly identifying positive samples. At the same time, ideally the FPR should
    not increase that much. The ROC curve of an effective classifier would hug the
    upper left corner of the plot: high TPR, low FPR. You can imagine that a perfect
    classifier would get a TPR of 1 (recovers all the positive samples) and an FPR
    of 0 and appear as a sort of square starting at (0,0), going up to (0,1), and
    finishing at (1,1). While in practice this kind of performance is highly unlikely,
    it gives us a limiting case.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑什么样的 ROC 曲线是“好的”。随着我们降低阈值，我们希望看到 TPR 增加，这意味着我们的分类器能够很好地正确识别正样本。同时，理想情况下，FPR
    应该不会增加太多。有效分类器的 ROC 曲线会紧贴图的左上角：高 TPR，低 FPR。你可以想象，完美的分类器将得到 1 的 TPR（恢复所有正样本）和 0
    的 FPR，并且呈现为一种从（0,0）开始，直达（0,1），再到（1,1）的方形曲线。虽然在实践中这种表现极不可能出现，但它给我们提供了一个极限情况。
- en: Further consider what the **area under the curve (AUC)** of such a classifier
    would be, remembering integrals from calculus if you have studied it. The AUC
    of a perfect classifier would be 1, because the shape of the curve would be a
    square on the unit interval [0, 1].
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步考虑这样一个分类器的 **曲线下的面积（AUC）**，如果你曾学习过微积分，可以回想一下积分的概念。完美分类器的 AUC 为 1，因为曲线的形状将在单位区间
    [0, 1] 上形成一个正方形。
- en: 'On the other hand, the line labeled as "Random chance" in our plot is the ROC
    curve that theoretically results from flipping an unbiased coin as a classifier:
    it''s just as likely to get a true positive as a false positive, so lowering the
    threshold introduces more of each in equal proportion and the TPR and FPR increase
    at the same rate. The AUC under this ROC would be half of the perfect classifier''s,
    as you can see graphically, and would be 0.5.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们图中标记为“随机机会”的线是我们绘制的 ROC 曲线，它理论上是通过使用一个公正的硬币来作为分类器时产生的：它产生真阳性和假阳性的概率相同，因此，降低阈值会等比例地引入更多的每种情况，TPR
    和 FPR 以相同的速度增加。这条 ROC 曲线下的 AUC 将是完美分类器 AUC 的一半，正如你从图形中看到的那样，值为 0.5。
- en: So, in general, the ROC AUC is going to be between 0.5 and 1 (although values
    below 0.5 are technically possible). Values close to 0.5 indicate the model can
    do little better than random chance (coin flip) as a classifier, while values
    closer to 1 indicate better performance. The **ROC AUC** is a key metric for the
    quality of a classifier and is widely used in machine learning. The ROC AUC may
    also be referred to as the **C-statistic** (concordance statistic).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一般来说，ROC AUC 的值会在 0.5 和 1 之间（虽然技术上也有可能小于 0.5）。接近 0.5 的值表明模型的分类效果几乎与随机猜测（硬币投掷）相当，而接近
    1 的值则表示更好的性能。**ROC AUC** 是衡量分类器质量的关键指标，并广泛应用于机器学习中。ROC AUC 也可以称为 **C 统计量**（一致性统计量）。
- en: 'Being such an important metric, scikit-learn has a convenient way to calculate
    the ROC AUC. Let''s see what the ROC AUC of the logistic regression classifier
    is, where we can pass the same information that we did to the `roc_curve` function.
    Calculate the area under the ROC curve with this code:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个重要的指标，scikit-learn 提供了一种方便的方式来计算 ROC AUC。让我们看看逻辑回归分类器的 ROC AUC，方法是传递与 `roc_curve`
    函数相同的信息。使用以下代码计算 ROC 曲线下的面积：
- en: '[PRE55]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'And observe the output:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 观察输出：
- en: '[PRE56]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The ROC AUC for the logistic regression is pretty close to 0.5, meaning it's
    not a very effective classifier. This may not be surprising, considering we have
    expended no effort to determine which features out of the candidate pool are actually
    useful at this point. We're just getting used to model fitting syntax and learning
    the way to calculate model quality metrics using a simple model containing only
    the `EDUCATION` feature. Later on, by considering other features, hopefully we'll
    get a higher ROC AUC.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的 ROC AUC 值接近 0.5，这意味着它不是一个非常有效的分类器。考虑到我们目前没有花费任何精力去确定候选特征中哪些实际上是有用的，这一点并不令人惊讶。我们只是习惯于模型拟合语法，并学习如何使用仅包含
    `EDUCATION` 特征的简单模型来计算模型质量指标。稍后，通过考虑其他特征，希望能够获得更高的 ROC AUC。
- en: Note
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**ROC curve: How did it get that name?**'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROC 曲线：它是如何得到这个名字的？**'
- en: During World War II, radar receiver operators were evaluated on their ability
    to judge whether something that appeared on their radar screen was in fact an
    enemy aircraft or not. These decisions involved the same concepts of true and
    false positives and negatives that we are interested in for binary classification.
    The ROC curve was devised as a way to measure the effectiveness of operators of
    radar receiver equipment.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次世界大战期间，雷达接收员根据他们判断雷达屏幕上出现的目标是否为敌机来评估他们的能力。这些决策涉及与我们在二元分类中关注的真阳性、假阳性和真阴性相同的概念。ROC
    曲线就是为了衡量雷达接收设备操作员的有效性而设计的。
- en: Precision
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精度
- en: 'Before embarking on the activity, we will consider the classification metric
    briefly introduced previously: **precision**. Like the ROC curve, this diagnostic
    is useful over a range of thresholds. Precision is defined as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始活动之前，我们将简要考虑之前介绍过的分类指标：**精度**。像 ROC 曲线一样，这个诊断在不同的阈值范围内都很有用。精度定义如下：
- en: '![Figure 2.29: Precision equation'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.29：精度公式'
- en: '](img/B16925_02_29.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_02_29.jpg)'
- en: 'Figure 2.29: Precision equation'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.29：精度公式
- en: 'Consider the interpretation of this, in the sense of varying the threshold
    across the range of predicted probabilities, as we did for the ROC curve. At a
    high threshold, there will be relatively few samples predicted as positive. As
    we lower the threshold, more and more will be predicted as positive. Our hope
    is that as we do this, the number of true positives increases more quickly than
    the number of false positives, as we saw on the ROC curve. Precision looks at
    the ratio of the number of true positives to the sum of true and false positives.
    Think about the denominator here: what is the sum of true and false positives?'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这一点的解释，考虑到在预测概率的范围内变化阈值，就像我们为ROC曲线所做的那样。在高阈值下，预测为正样本的样本相对较少。随着阈值的降低，越来越多的样本将被预测为正样本。我们的期望是，在执行这一操作时，真正的正样本数量会比假正样本的数量增加得更快，正如我们在ROC曲线中所看到的那样。精确度看的是真正的正样本数量与真正和假正样本总和的比例。考虑这里的分母：真正和假正样本的总和是多少？
- en: This sum is in fact the total number of positive predictions, since all positive
    predictions will be either correct or incorrect. So, precision measures the ratio
    of positive predictions that are correct to all positive predictions. For this
    reason, it is also called the `metrics.precision_recall_curve`. Precision and
    recall are often plotted together to assess the quality of positive predictions
    as far as what fraction are correct, while at the same time considering what fraction
    of the positive class a model is able to identify. We’ll plot a precision-recall
    curve in the following activity.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这个总和实际上是所有正预测的总数，因为所有正预测要么是正确的，要么是错误的。因此，精确度衡量的是正确的正预测与所有正预测的比例。因此，它也被称为`metrics.precision_recall_curve`。精确度和召回率通常一起绘制，以评估正预测的质量，考虑哪些部分是正确的，同时考虑模型能够识别正类的比例。我们将在接下来的活动中绘制精确度-召回曲线。
- en: Why might precision be a useful measure of classifier performance? Imagine that
    for every positive model prediction, you are going to take some expensive course
    of action, such as a time-consuming review of content that was flagged as inappropriate
    by an automated procedure. False positives would waste the valuable time of human
    reviewers. You would want to be sure that you were making the right decisions
    on what content received a detailed review. Precision could be a good metric to
    use in this situation.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么精确度是一个有用的分类器性能度量？想象一下，对于每个正的模型预测，你将采取一些昂贵的措施，比如对通过自动化程序标记为不当的内容进行耗时的复审。假正样本会浪费人工审阅者宝贵的时间。在这种情况下，你会希望确保你在做出哪些内容需要详细复审的决定时是正确的。精确度可能是这个情况中一个很好的度量指标。
- en: 'Activity 2.01: Performing Logistic Regression with a New Feature and Creating
    a Precision-Recall Curve'
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 2.01：使用新特征执行逻辑回归并创建精确度-召回曲线
- en: In this activity, you'll train a logistic regression model using a feature besides
    `EDUCATION`. Then you will graphically assess the trade-off between precision
    and recall, as well as calculate the area underneath a precision-recall curve.
    You will also calculate the ROC AUC on both the training and test sets and compare
    them.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你将使用除`EDUCATION`之外的特征来训练一个逻辑回归模型。然后，你将通过图形化评估精确度和召回率之间的权衡，并计算精确度-召回曲线下的面积。你还将计算训练集和测试集上的ROC
    AUC并进行比较。
- en: 'Perform the following steps to complete the activity:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成该活动：
- en: Note
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code and the resulting output for this activity have been loaded in a Jupyter
    notebook that can be found here: [https://packt.link/SvAOD](https://packt.link/SvAOD).'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的代码和结果输出已加载到一个Jupyter notebook中，可以在此处找到：[https://packt.link/SvAOD](https://packt.link/SvAOD)。
- en: Use scikit-learn's `train_test_split` to make a new set of training and test
    data. This time, instead of `EDUCATION`, use `LIMIT_BAL`, the account's credit
    limit, as the feature.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`train_test_split`来生成一组新的训练数据和测试数据。这次，不使用`EDUCATION`，而使用`LIMIT_BAL`，即账户的信用额度，作为特征。
- en: Train a logistic regression model using the training data from your split.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从划分中得到的训练数据来训练一个逻辑回归模型。
- en: Create the array of predicted probabilities for the test data.
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建测试数据的预测概率数组。
- en: Calculate the ROC AUC using the predicted probabilities and the true labels
    of the test data. Compare this to the ROC AUC from using the `EDUCATION` feature.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用测试数据的预测概率和真实标签来计算ROC AUC。将其与使用`EDUCATION`特征的ROC AUC进行比较。
- en: Plot the ROC curve.
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制ROC曲线。
- en: Calculate the data for the **precision-recall curve** on the test data using
    scikit-learn's functionality.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的功能，计算测试数据的**精确率-召回率曲线**的数据。
- en: Plot the precision-recall curve using matplotlib.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 matplotlib 绘制精确率-召回率曲线。
- en: Use scikit-learn to calculate the area under the precision-recall curve. You
    should get a value of approximately 0.315.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 计算精确率-召回率曲线下的面积。你应该得到大约 0.315 的值。
- en: Now recalculate the ROC AUC, except this time do it for the training data. How
    is this different, conceptually and quantitatively, from your earlier calculation?
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在重新计算 ROC AUC，不过这次使用训练数据。与之前的计算在概念上和数量上有何不同？
- en: Note
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Jupyter notebook containing the Python code solution for this activity
    can be found here: [https://packt.link/SvAOD](https://packt.link/SvAOD). Detailed
    step-wise solution to this activity can be found via [this link](B16925_Solution_ePub.xhtml#_idTextAnchor151).'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含此活动 Python 代码解决方案的 Jupyter notebook 可以在这里找到：[https://packt.link/SvAOD](https://packt.link/SvAOD)。此活动的详细逐步解决方案可以通过[这个链接](B16925_Solution_ePub.xhtml#_idTextAnchor151)查看。
- en: Summary
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we finished the initial exploration of the case study data
    by examining the response variable. Once we became confident in the completeness
    and correctness of the dataset, we were prepared to explore the relation between
    features and response and build models.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过检查响应变量完成了案例研究数据的初步探索。一旦我们对数据集的完整性和正确性充满信心，就准备好探索特征与响应之间的关系，并构建模型。
- en: We spent much of this chapter getting used to model fitting in scikit-learn
    at the technical, coding level, and learning about metrics we could use with the
    binary classification problem of the case study. When trying different feature
    sets and different kinds of models, you will need some way to tell if one approach
    is working better than another. Consequently, you'll need to use model performance
    metrics like those we learned in this chapter.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的大部分内容都在技术和编码层面上熟悉了 scikit-learn 中的模型拟合，并学习了我们可以在案例研究的二分类问题中使用的指标。在尝试不同的特征集和模型时，你将需要某种方法来判断哪种方法比另一种更有效。因此，你需要使用我们在本章中学到的模型性能指标。
- en: While accuracy is a familiar and intuitive metric as the percentage of correct
    classifications, we learned why it may not give a useful assessment of the performance
    of a classifier. We learned how to use a majority-class null model to tell whether
    an accuracy rate is truly good, or no better than what would result from simply
    predicting the most common class for all samples. When the data is imbalanced,
    accuracy is usually not the best way to judge a classifier.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管准确率作为正确分类百分比是一个熟悉且直观的指标，但我们了解到它可能无法有效评估分类器的性能。我们学会了如何使用多数类零假设模型来判断一个准确率是否真正优秀，还是仅仅与对所有样本预测最常见类别的结果没有差别。当数据不平衡时，准确率通常不是评判分类器的最佳方式。
- en: 'In order to have a more nuanced view of how a model is performing, it''s necessary
    to separate the positive and negative classes and assess the accuracy of them
    independently. From the resulting counts of true and false positive and negative
    classifications, which can be summarized in a confusion matrix, we can derive
    several other metrics: true and false positive and negative rates. Combining true
    and false positives and negatives with the concept of predicted probabilities
    and a variable threshold of prediction, we can further characterize the usefulness
    of a classifier using the ROC curve, the precision-recall curve, and the areas
    under these curves.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更细致地了解模型的表现，必须将正类和负类分开，并独立评估它们的准确性。从由真正和虚假正负分类汇总而成的混淆矩阵中，我们可以得出其他几个指标：真正率、虚假正率和虚假负率。结合真正和虚假正负以及预测概率和可变预测阈值的概念，我们可以通过
    ROC 曲线、精确率-召回率曲线及其下的面积进一步描述分类器的有效性。
- en: 'With these tools, you are well equipped to answer general questions about the
    performance of a binary classifier in any domain you may be working in. Later
    in the book, we will learn about application-specific ways to assess model performance
    by attaching costs and benefits to true and false positives and negatives. Before
    that, starting in the next chapter, we will begin learning the details behind
    what is possibly the most popular and simplest classification model: **logistic
    regression**.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些工具，你已经能够充分回答任何领域中二分类器性能的一般问题。在本书后面的内容中，我们将学习如何通过为真阳性、假阳性、真阴性和假阴性分配成本和收益，来评估模型性能的应用特定方法。在此之前，从下一章开始，我们将开始学习可能是最流行且最简单的分类模型的细节：**逻辑回归**。
