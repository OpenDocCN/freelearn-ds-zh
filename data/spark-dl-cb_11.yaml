- en: Creating and Visualizing Word Vectors Using Word2Vec
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Word2Vec创建和可视化单词向量
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Acquiring data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取数据
- en: Importing the necessary libraries
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入必要的库
- en: Preparing the data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据
- en: Building and training the model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练模型
- en: Visualizing further
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步可视化
- en: Analyzing further
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步分析
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Before training a neural network on text data and generating text using LSTM
    cells, it is important to understand how text data (such as words, sentences,
    customer reviews, or stories) is converted to word vectors first before it is
    fed into a neural network. This chapter will describe how to convert a text into
    a corpus and generate word vectors from the corpus, which makes it easy to group
    similar words using techniques such as Euclidean distance calculation or cosine
    distance calculation between different word vectors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在对文本数据进行神经网络训练并使用LSTM单元生成文本之前，重要的是要了解文本数据（如单词、句子、客户评论或故事）在输入神经网络之前是如何转换为单词向量的。本章将描述如何将文本转换为语料库，并从语料库生成单词向量，这样就可以使用欧几里得距离计算或余弦距离计算等技术轻松地对相似单词进行分组。
- en: Acquiring data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: The first step is to acquire some data to work with. For this chapter, we will
    require a lot of text data to convert it into tokens and visualize it to understand
    how neural networks rank word vectors based on Euclidean and Cosine distances.
    It is an important step in understanding how different words get associated with
    each other. This, in turn, can be used to design better, more efficient language
    and text-processing models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是获取一些要处理的数据。在本章中，我们需要大量的文本数据，将其转换为标记并进行可视化，以了解神经网络如何根据欧几里得距离和余弦距离对单词向量进行排名。这是了解不同单词如何相互关联的重要步骤。反过来，这可以用于设计更好、更高效的语言和文本处理模型。
- en: Getting ready
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Consider the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下内容：
- en: The text data for the model needs to be in files of `.txt` format, and you must
    ensure that the files are placed in the current working directory. The text data
    can be anything from Twitter feeds, news feeds, customer reviews, computer code,
    or whole books saved in the `.txt` format in the working directory. In our case,
    we have used the *Game of Thrones* books as the input text to our model. However,
    any text can be substituted in place of the books, and the same model will work.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的文本数据需要以`.txt`格式的文件存在，并且您必须确保文件放置在当前工作目录中。文本数据可以是来自Twitter动态、新闻动态、客户评论、计算机代码或以`.txt`格式保存在工作目录中的整本书。在我们的案例中，我们已经将《权力的游戏》书籍作为模型的输入文本。然而，任何文本都可以替换书籍，并且相同的模型也会起作用。
- en: Many classical texts are no longer protected under copyright. This means that
    you can download all of the text for these books for free and use them in experiments,
    such as creating generative models. The best place to get access to free books
    that are no longer protected by copyright is Project Gutenberg ([https://www.gutenberg.org/](https://www.gutenberg.org/)).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多经典文本已不再受版权保护。这意味着您可以免费下载这些书籍的所有文本，并将它们用于实验，比如创建生成模型。获取不再受版权保护的免费书籍的最佳途径是Project
    Gutenberg（[https://www.gutenberg.org/](https://www.gutenberg.org/)）。
- en: How to do it...
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作方法...
- en: 'The steps are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Begin by visiting the Project Gutenberg website and browsing for a book that
    interests you. Click on the book, and then click on UTF-8, which allows you to
    download the book in plain-text format. The link is shown in the following screenshot:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先访问Project Gutenberg网站并浏览您感兴趣的书籍。单击书籍，然后单击UTF-8，这样您就可以以纯文本格式下载书籍。链接如下截图所示：
- en: '![](img/00318.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00318.jpeg)'
- en: Project Gutenberg Dataset download page
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Project Gutenberg数据集下载页面
- en: 'After clicking on Plain Text UTF-8, you should see a page that looks like the
    following screenshot. Right click on the page and click on Save As... Next, rename
    the file to whatever you choose and save it in your working directory:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击纯文本UTF-8后，您应该会看到一个类似以下截图的页面。右键单击页面，然后单击“另存为...”接下来，将文件重命名为您选择的任何名称，并保存在您的工作目录中：
- en: '![](img/00319.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00319.jpeg)'
- en: You should now see a `.txt` file with the specified filename in your current
    working directory.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您应该在当前工作目录中看到一个带有指定文件名的`.txt`文件。
- en: Project Gutenberg adds a standard header and footer to each book; this is not
    part of the original text. Open the file in a text editor, and delete the header
    and the footer.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Project Gutenberg为每本书添加了标准的页眉和页脚；这不是原始文本的一部分。在文本编辑器中打开文件，然后删除页眉和页脚。
- en: How it works...
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The functionality is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: Check for the current working directory using the following command: `pwd`.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令检查当前工作目录：`pwd`。
- en: 'The working directory can be changed using the `cd` command as shown in the
    following screenshot:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`cd`命令更改工作目录，如下截图所示：
- en: '![](img/00320.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00320.jpeg)'
- en: Notice that, in our case, the text files are contained in a folder named `USF`,
    and, therefore, this is set as the working directory. You may similarly store
    one or more `.txt` files in the working directory for use as input to the model.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在我们的案例中，文本文件包含在名为`USF`的文件夹中，因此这被设置为工作目录。您可以类似地将一个或多个`.txt`文件存储在工作目录中，以便作为模型的输入。
- en: UTF-8 specifies the type of encoding of the characters in the text file. **UTF-8**
    stands for **Unicode Transformation Format**. The **8** means it uses **8-bit**
    blocks to represent a character.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: UTF-8指定了文本文件中字符的编码类型。**UTF-8**代表**Unicode转换格式**。**8**表示它使用**8位**块来表示一个字符。
- en: UTF-8 is a compromise character encoding that can be as compact as ASCII (if
    the file is just plain-English text) but can also contain any Unicode characters
    (with some increase in file size).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: UTF-8是一种折衷的字符编码，可以像ASCII一样紧凑（如果文件只是纯英文文本），但也可以包含任何Unicode字符（文件大小会略有增加）。
- en: It is not necessary for the text file to be in a UTF-8 format, as we will use
    the codecs library at a later stage to encode all the text into the Latin1 encoding
    format.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不需要文本文件以UTF-8格式，因为我们将在稍后阶段使用codecs库将所有文本编码为Latin1编码格式。
- en: There's more...
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'For more information about UTF-8 and Latin1 encoding formats, visit the following
    links:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有关UTF-8和Latin1编码格式的更多信息，请访问以下链接：
- en: '[https://en.wikipedia.org/wiki/UTF-8](https://en.wikipedia.org/wiki/UTF-8)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/UTF-8](https://en.wikipedia.org/wiki/UTF-8)'
- en: '[http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html](http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html](http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html)'
- en: See also
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Visit the following link to understand the need for word vectors in neural
    networks better:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 访问以下链接以更好地了解神经网络中单词向量的需求：
- en: '[https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1](https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1](https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1)'
- en: 'Listed below are some other useful articles related to the topic of converting
    words to vectors:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是与将单词转换为向量相关的一些其他有用文章：
- en: '[https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)'
- en: '[https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817](https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817](https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817)'
- en: Importing the necessary libraries
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入必要的库
- en: Before we begin, we require the following libraries and dependencies, which
    need to be imported into our Python environment. These libraries will make our
    tasks a lot easier, as they have readily available functions and models that can
    be used instead of doing that ourselves. This also makes the code more compact
    and readable.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要导入以下库和依赖项，这些库需要导入到我们的Python环境中。这些库将使我们的任务变得更加容易，因为它们具有现成的可用函数和模型，可以代替我们自己进行操作。这也使得代码更加简洁和可读。
- en: Getting ready
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做好准备
- en: 'The following libraries and dependencies will be required to create word vectors
    and plots and visualize the n-dimensional word vectors in a 2D space:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下库和依赖项将需要创建单词向量和绘图，并在2D空间中可视化n维单词向量：
- en: '`future`'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`未来`'
- en: '`codecs`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`codecs`'
- en: '`glob`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glob`'
- en: '``multiprocessing``'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '``multiprocessing``'
- en: '`os`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`os`'
- en: '``pprint``'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '``pprint``'
- en: '`re`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re`'
- en: '`nltk`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk`'
- en: '`Word2Vec`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Word2Vec`'
- en: '`sklearn`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn`'
- en: '`numpy`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`matplotlib`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`'
- en: '`pandas`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`seaborn`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seaborn`'
- en: How to do it...
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Type the following commands into your Jupyter notebook to import all the required
    libraries:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在Jupyter笔记本中键入以下命令以导入所有所需的库： '
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should see an output that looks like the following screenshot:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该看到一个类似以下截图的输出：
- en: '![](img/00321.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00321.jpeg)'
- en: 'Next, import the `stopwords` and `punkt` libraries using the following commands:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令导入`stopwords`和`punkt`库：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output you see must look like the following screenshot:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您看到的输出必须看起来像以下截图：
- en: '![](img/00322.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00322.jpeg)'
- en: How it works...
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section will describe the purpose of each library being used for this recipe.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述用于此配方的每个库的目的。
- en: The `future` library is the missing link between Python 2 and Python 3\. It
    acts as a bridge between the two versions and allows us to use syntax from both
    versions.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`future`库是Python 2和Python 3之间的缺失链接。它充当两个版本之间的桥梁，并允许我们使用两个版本的语法。'
- en: The `codecs` library will be used to perform the encoding of all words present
    in the text file. This constitutes our dataset.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`codecs`库将用于对文本文件中所有单词进行编码。这构成了我们的数据集。'
- en: Regex is the library used to look up or search for a file really quickly. The
    `glob` function allows quick and efficient searching through a large database
    for a required file.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Regex是用于快速查找文件的库。`glob`函数允许快速高效地在大型数据库中搜索所需的文件。
- en: The `multiprocessing` library allows us to perform concurrency, which is a way
    of running multiple threads and having each thread run a different process. It
    is a way of making programs run faster by parallelization.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`multiprocessing`库允许我们执行并发，这是一种运行多个线程并使每个线程运行不同进程的方式。这是一种通过并行化使程序运行更快的方式。'
- en: The `os` library allows easy interaction with the operating system, such as
    a Mac, Windows, and so on, and performs functions such as reading a file.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`os`库允许与操作系统进行简单交互，如Mac、Windows等，并执行诸如读取文件之类的功能。'
- en: The `pprint` library provides a capability for pretty-printing arbitrary Python
    data structures in a form that can be used as input to the interpreter.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pprint`库提供了一种能够以可用作解释器输入的形式对任意Python数据结构进行漂亮打印的功能。'
- en: The `re` module provides regular expression matching operations similar to those
    found in Perl.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`re`模块提供了类似于Perl中的正则表达式匹配操作。'
- en: NLTK is a natural language toolkit capable of tokenizing words in very short
    code. When fed in a whole sentence, the `nltk` function breaks up sentences and
    outputs tokens for each word. Based on these tokens, the words may be organized
    into different categories. NLTK does this by comparing each word with a huge database
    of pre-trained words called a **lexicon**.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NLTK是一个自然语言工具包，能够在非常简短的代码中对单词进行标记。当输入整个句子时，`nltk`函数会分解句子并输出每个单词的标记。基于这些标记，单词可以被组织成不同的类别。NLTK通过将每个单词与一个名为**词汇表**的巨大预训练单词数据库进行比较来实现这一点。
- en: '`Word2Vec` is Google''s model, trained on a huge dataset of word vectors. It
    groups semantically similar words close to one another. This will be the most
    important library for this section.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Word2Vec`是Google的模型，它在一个巨大的单词向量数据集上进行了训练。它将语义上相似的单词归为一类。这将是本节中最重要的库。'
- en: '`sklearn.manifold` allows the dimensionality reduction of the dataset by employing
    **t-distributed Stochastic Neighbor Embedding** (**t-SNE**) techniques. Since
    each word vector is multi-dimensional, we require some form of dimensionality
    reduction techniques to bring the dimensionality of these words down to a lower
    dimensional space so it can be visualized in a 2D space.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sklearn.manifold`允许使用**t-分布随机邻居嵌入**（**t-SNE**）技术对数据集进行降维。由于每个单词向量是多维的，我们需要某种形式的降维技术，将这些单词的维度降低到一个较低的维度空间，以便在2D空间中进行可视化。'
- en: There's more...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '`Numpy` is a commonly used `math` library. `Matplotlib` is the `plotting` library
    we will utilize, and `pandas` provide a lot of flexibility in data handling by
    allowing easy reshaping, slicing, indexing, subsetting, and manipulation of data.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`Numpy`是常用的`math`库。`Matplotlib`是我们将利用的`plotting`库，而`pandas`通过允许轻松重塑、切片、索引、子集和操纵数据，提供了很大的灵活性。'
- en: The `Seaborn` library is another statistical data visualization library that
    we require along with `matplotlib`. `Punkt` and `Stopwords` are two data-processing
    libraries that simplify tasks such as splitting a piece of text from a corpus
    into tokens (that is, via tokenization) and removing `stopwords`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`Seaborn`库是另一个统计数据可视化库，我们需要与`matplotlib`一起使用。`Punkt`和`Stopwords`是两个数据处理库，简化了诸如将语料库中的文本拆分为标记（即通过标记化）和删除`stopwords`等任务。'
- en: See also
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'For more information regarding some of the libraries utilized, visit the following
    links:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用的一些库的更多信息，请访问以下链接：
- en: '[https://docs.python.org/3/library/codecs.html](https://docs.python.org/3/library/codecs.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/3/library/codecs.html](https://docs.python.org/3/library/codecs.html)'
- en: '[https://docs.python.org/2/library/pprint.html](https://docs.python.org/2/library/pprint.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/2/library/pprint.html](https://docs.python.org/2/library/pprint.html)'
- en: '[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)'
- en: '[https://www.nltk.org/](https://www.nltk.org/)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.nltk.org/](https://www.nltk.org/)'
- en: '[https://www.tensorflow.org/tutorials/word2vec](https://www.tensorflow.org/tutorials/word2vec)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/tutorials/word2vec](https://www.tensorflow.org/tutorials/word2vec)'
- en: '[http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html)'
- en: Preparing the data
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: A number of data-preprocessing steps are to be performed before the data is
    fed into the model. This section will describe how to clean the data and prepare
    it so it can be fed into the model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据馈送到模型之前，需要执行一些数据预处理步骤。本节将描述如何清理数据并准备数据，以便将其馈送到模型中。
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: All the text from the `.txt` files is first converted into one big corpus. This
    is done by reading each sentence from each file and adding it to an empty corpus.
    A number of preprocessing steps are then executed to remove irregularities such
    as white spaces, spelling errors, `stopwords`, and so on. The cleaned text data
    has to then be tokenized, and the tokenized sentences are added to an empty array
    by running them through a loop.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先将所有`.txt`文件中的文本转换为一个大的语料库。这是通过从每个文件中读取每个句子并将其添加到一个空语料库中来完成的。然后执行一些预处理步骤，以删除诸如空格、拼写错误、`stopwords`等不规则性。然后必须对清理后的文本数据进行标记化，并通过循环将标记化的句子添加到一个空数组中。
- en: How to do it...
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps are as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Type in the following commands to search for the `.txt` files within the working
    directory and print the names of the files found:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 键入以下命令以在工作目录中搜索`.txt`文件并打印找到的文件的名称：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In our case, there are five books named `got1`, `got2`, `got3`, `got4`, and
    `got5` saved in the working directory.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，工作目录中保存了五本名为`got1`、`got2`、`got3`、`got4`和`got5`的书籍。
- en: 'Create a `corpus`, read each sentence starting with the first file, encode
    it, and add the encoded characters to a `corpus` using the following commands:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`corpus`，读取每个句子，从第一个文件开始，对其进行编码，并使用以下命令将编码字符添加到`corpus`中：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Execute the code in the preceding steps, which should result in an output that
    looks like the following screenshot:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行前面步骤中的代码，应该会产生以下截图中的输出：
- en: '![](img/00323.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00323.jpeg)'
- en: 'Load the English pickle `tokenizer` from `punkt` using the following command:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从`punkt`加载英语pickle`tokenizer`：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Tokenize` the entire `corpus` into sentences using the following command:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将整个`corpus`标记化为句子：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define the function to split sentences into their constituent words as well
    as remove unnecessary characters in the following manner:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以以下方式定义将句子拆分为其组成单词并删除不必要字符的函数：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Add all the raw sentences where each word of the sentence is tokenized to a
    new array of sentences. This is done by using the following code:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个句子的每个单词标记化的原始句子全部添加到一个新的句子数组中。使用以下代码完成：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Print a random sentence from the corpus to visually see how the `tokenizer`
    splits sentences and creates a word list from the result. This is done using the
    following commands:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从语料库中打印一个随机句子，以直观地查看`tokenizer`如何拆分句子并从结果创建一个单词列表。使用以下命令完成：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Count the total tokens from the dataset using the following commands:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令计算数据集中的总标记数：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Executing the tokenizer and tokenizing all the sentences in the corpus should
    result in an output that looks like the one in the following screenshot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 执行分词器并对语料库中的所有句子进行分词应该会产生以下截图中的输出：
- en: '![](img/00324.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00324.jpeg)'
- en: 'Next, removing unnecessary characters, such as hyphens and special characters,
    are done in the following manner. Splitting up all the sentences using the user-defined
    `sentence_to_wordlist()` function produces an output as shown in the following
    screenshot:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，删除不必要的字符，如连字符和特殊字符，是以以下方式完成的。使用用户定义的`sentence_to_wordlist()`函数拆分所有句子会产生以下截图中显示的输出：
- en: '![](img/00325.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00325.jpeg)'
- en: 'Adding the raw sentences to a new array named `sentences[]` produces an output
    as shown in the following screenshot:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始句子添加到名为`sentences[]`的新数组中，将产生如下截图所示的输出：
- en: '![](img/00326.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00326.jpeg)'
- en: 'On printing the total number of tokens in the corpus, we notice that there
    are 1,110,288 tokens in the entire corpus. This is illustrated in the following
    screenshot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在打印语料库中的总标记数时，我们注意到整个语料库中有1,110,288个标记。这在以下截图中有所说明：
- en: '![](img/00327.jpeg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00327.jpeg)'
- en: 'The functionality is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: The pre-trained `tokenizer` from NLTK is used to tokenize the entire corpus
    by counting each sentence as a token. Every tokenized sentence is added to the
    variable `raw_sentences`, which stores the tokenized sentences.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NLTK中的预训练`tokenizer`通过将每个句子计为一个标记来标记整个语料库。每个标记化的句子都被添加到变量`raw_sentences`中，该变量存储了标记化的句子。
- en: In the next step, common stopwords are removed, and the text is cleaned by splitting
    each sentence into its words.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，常见的停用词被移除，并且通过将每个句子分割成单词来清理文本。
- en: A random sentence along with its wordlist is printed to understand how this
    works. In our case, we have chosen to print the 50th sentence in the `raw_sentences` array.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印一个随机句子以及其单词列表，以了解其工作原理。在我们的案例中，我们选择打印`raw_sentences`数组中的第50个句子。
- en: The total number of tokens (in our case, sentences) in the sentences array are
    counted and printed. In our case, we see that 1,110,288 tokens are created by
    the `tokenizer`.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印句子数组中的总标记数（在我们的案例中是句子）。在我们的案例中，我们看到`tokenizer`创建了1,110,288个标记。
- en: There's more...
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有...
- en: 'More information about tokenizing paragraphs and sentences can be found by
    visiting the following links:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有关将段落和句子标记化的更多信息，请访问以下链接：
- en: '[https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize](https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize](https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize)'
- en: '[https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk](https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk](https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk)'
- en: '[https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/](https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/](https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/)'
- en: See also
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'For more information about how regular expressions work, visit the following
    link:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有关正则表达式工作原理的更多信息，请访问以下链接：
- en: '[https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python](https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python](https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python)'
- en: Building and training the model
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和训练模型
- en: 'Once we have the text data in the form of tokens in an array, we are able to
    input it in the array format to the model. First, we have to define a number of
    hyperparameters for the model. This section will describe how to do the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将文本数据以数组形式的标记输入到模型中，我们就能够为模型定义一些超参数。本节将描述如何执行以下操作：
- en: Declare model hyperparameters
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明模型超参数
- en: Build a model using `Word2Vec`
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`Word2Vec`构建模型
- en: Train the model on the prepared dataset
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在准备好的数据集上训练模型
- en: Save and checkpoint the trained model
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和检查点训练好的模型
- en: Getting ready
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Some of the model hyperparameters that are to be declared include the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 需要声明的一些模型超参数包括以下内容：
- en: Dimensionality of resulting word vectors
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的单词向量的维度
- en: Minimum word count threshold
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小词数阈值
- en: Number of parallel threads to run while training the model
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练模型时运行的并行线程数
- en: Context window length
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文窗口长度
- en: Downsampling (for frequently occurring words)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降采样（对于频繁出现的单词）
- en: Setting a seed
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置种子
- en: Once the previously mentioned hyperparameters are declared, the model can be
    built using the `Word2Vec` function from the `Gensim` library.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦前面提到的超参数被声明，就可以使用`Gensim`库中的`Word2Vec`函数构建模型。
- en: How to do it...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps are as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Declare the hyperparameters for the model using the following commands:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令声明模型的超参数：
- en: '[PRE10]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Build the model, using the declared hyperparameters, with the following lines
    of code:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用声明的超参数，使用以下代码行构建模型：
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Build the model''s vocabulary using the tokenized sentences and iterating through
    all the tokens. This is done using the `build_vocab` function in the following
    manner:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标记化的句子构建模型的词汇表，并通过所有标记进行迭代。这是使用以下方式的`build_vocab`函数完成的：
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Train the model using the following command:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令训练模型：
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create a directory named trained, if it doesn''t already exist. Save and checkpoint
    the `trained` model using the following commands:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果尚不存在，请创建一个名为trained的目录。使用以下命令保存和检查点“trained”模型：
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To load the saved model at any point, use the following command:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在任何时候加载保存的模型，请使用以下命令：
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How it works...
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The functionality is as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: The declaration of model parameters does not produce any output. It just makes
    space in the memory to store variables as model parameters. The following screenshot
    describes this process:![](img/00328.jpeg)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型参数的声明不会产生任何输出。它只是在内存中留出空间来存储变量作为模型参数。以下截图描述了这个过程：![](img/00328.jpeg)
- en: The model is built using the preceding hyperparameters. In our case, we have
    named the model `got2vec` ,but the model may be named as per your liking. The
    model definition is illustrated in the following screenshot:![](img/00329.jpeg)
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型是使用前述超参数构建的。在我们的案例中，我们将模型命名为`got2vec`，但模型可以根据您的喜好进行命名。模型定义如下截图所示：![](img/00329.jpeg)
- en: Running the `build_vocab` command on the model should produce an output as seen
    in the following screenshot:![](img/00330.jpeg)
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型上运行`build_vocab`命令应该会产生如下截图所示的输出：![](img/00330.jpeg)
- en: Training the model is done by defining the parameters as seen in the following
    screenshot:![](img/00331.jpeg)
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过定义以下截图中所见的参数来训练模型：![](img/00331.jpeg)
- en: 'The above command produces an output as shown in the following screenshot:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令产生如下截图所示的输出：
- en: '![](img/00332.jpeg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00332.jpeg)'
- en: 'The commands to save, checkpoint, and load the model produce the following
    output, as shown in the screenshot:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存、检查点和加载模型的命令产生如下输出，如截图所示：
- en: '![](img/00333.jpeg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00333.jpeg)'
- en: There's more...
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Consider the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下内容：
- en: In our case, we notice the `build_vocab` function identifies 23,960 different
    word types from a list of 1,110,288 words. However, this number will vary for
    different text corpora.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们注意到`build_vocab`函数从1,110,288个单词的列表中识别出23,960个不同的单词类型。然而，对于不同的文本语料库，这个数字会有所不同。
- en: Each word is represented by a 300-dimensional vector since we have declared
    the dimensionality to be 300\. Increasing this number increases the training time
    of the model but also makes sure the model generalizes easily to new data.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单词都由一个300维向量表示，因为我们已经声明维度为300。增加这个数字会增加模型的训练时间，但也会确保模型很容易地泛化到新数据。
- en: The downsampling rate of 1e![](img/00334.jpeg)3 is found to be a good rate.
    This is specified to let the model know when to downsample frequently occurring
    words, as they are not of much importance when it comes to analysis. Examples
    of such words are this, that, those, them, and so on.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现1e![](img/00334.jpeg)3的下采样率是一个很好的比率。这是为了让模型知道何时对频繁出现的单词进行下采样，因为它们在分析时并不重要。这些单词的例子包括this,
    that, those, them等。
- en: A seed is set to make results reproducible. Setting a seed also makes debugging
    a lot easier.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个种子以使结果可重现。设置种子也使得调试变得更容易。
- en: Training the model takes about 30 seconds using regular CPU computing since
    the model is not very complex.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于模型不是很复杂，使用常规CPU计算训练模型大约需要30秒。
- en: The model, when check-pointed, is saved under the `trained` folder inside the
    working directory.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当检查点被检查时，模型被保存在工作目录内的`trained`文件夹下。
- en: See also
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'For more information on `Word2Vec` models and the Gensim library, visit the
    following link:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`Word2Vec`模型和Gensim库的更多信息，请访问以下链接：[https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)
- en: '[https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)'
- en: Visualizing further
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步可视化
- en: This section will describe how to squash the dimensionality of all the trained
    words and put it all into one giant matrix for visualization purposes. Since each
    word is a 300-dimensional vector, it needs to be brought down to a lower dimension
    for us to visualize it in a 2D space.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述如何压缩所有训练过的单词的维度，并将其全部放入一个巨大的矩阵以进行可视化。由于每个单词都是一个300维的向量，所以需要将其降低到更低的维度，以便我们在2D空间中进行可视化。
- en: Getting ready
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Once the model is saved and checkpointed after training, begin by loading it
    into memory, as you did in the previous section. The libraries and modules that
    will be utilized in this section are:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型在训练后保存和检查点后，开始将其加载到内存中，就像在上一节中所做的那样。在本节中将使用的库和模块有：
- en: '`tSNE`'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tSNE`'
- en: '`pandas`'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`Seaborn`'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Seaborn`'
- en: '`numpy`'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: How to do it...
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps are as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Squash the dimensionality of the 300-dimensional word vectors by using the
    following command:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令压缩300维单词向量的维度：
- en: '[PRE16]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Put all the word vectors into one giant matrix (named `all_word_vectors_matrix`),
    and view it using the following commands:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有单词向量放入一个巨大的矩阵（命名为`all_word_vectors_matrix`），并使用以下命令查看它：
- en: '[PRE17]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Use the `tsne` technique to fit all the learned representations into a two-
    dimensional space using the following command:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将所有学习到的表示拟合到二维空间中：
- en: '[PRE18]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Gather all the word vectors, as well as their associated words, using the following
    code:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码收集所有单词向量及其相关单词：
- en: '[PRE19]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `X` and `Y` coordinates and associated words of the first ten points can
    be obtained using the following command:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令可以获取前十个点的`X`和`Y`坐标以及相关单词：
- en: '[PRE20]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Plot all the points using the following commands:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令绘制所有点：
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'A selected region of the plotted graph can be zoomed into for a closer inspection.
    Do this by slicing the original data using the following function:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以放大绘图图表的选定区域以进行更仔细的检查。通过使用以下函数对原始数据进行切片来实现这一点：
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Plot the sliced data using the following command. The sliced data can be visualized
    as a zoomed-in region of the original plot of all data points:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令绘制切片数据。切片数据可以被视为原始所有数据点的放大区域：
- en: '[PRE23]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: How it works...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The functionality is as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: The t-SNE algorithm is a non-linear dimensionality reduction technique. Computers
    are easily able to interpret and process many dimensions during their computations.
    However, humans are only capable of visualizing two or three dimensions at a time.
    Therefore, these dimensionality reduction techniques come in very handy when trying
    to draw insights from data.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: t-SNE算法是一种非线性降维技术。计算机在计算过程中很容易解释和处理许多维度。然而，人类一次只能可视化两到三个维度。因此，当试图从数据中得出见解时，这些降维技术非常有用。
- en: On applying t-SNE to the 300-dimensional vectors, we are able to squash it into
    just two dimensions to plot it and view it.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将300维向量应用t-SNE后，我们能够将其压缩为只有两个维度来绘制和查看。
- en: By specifying `n_components` as 2, we let the algorithm know that it has to
    squash the data into a two-dimensional space. Once this is done, we add all the
    squashed vectors into one giant matrix named `all_word_vectors_matrix`, which
    is illustrated in the following screenshot:![](img/00335.jpeg)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将 `n_components` 指定为 2，我们让算法知道它必须将数据压缩到二维空间。完成此操作后，我们将所有压缩后的向量添加到一个名为 `all_word_vectors_matrix`
    的巨大矩阵中，如下图所示：![](img/00335.jpeg)
- en: The t-SNE algorithm needs to be trained on all these word vectors. The training
    takes about five minutes on a regular CPU.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: t-SNE 算法需要对所有这些单词向量进行训练。在常规 CPU 上，训练大约需要五分钟。
- en: Once the t-SNE is finished training on all the word vectors, it outputs 2D vectors
    for each word. These vectors may be plotted as points by converting all of them
    into a data frame. This is done as shown in the following screenshot:![](img/00336.jpeg)
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 t-SNE 完成对所有单词向量的训练，它会为每个单词输出 2D 向量。可以通过将它们全部转换为数据框架来将这些向量绘制为点。如下图所示完成此操作：![](img/00336.jpeg)
- en: 'We see that the preceding code produces a number of points where each point
    represents a word along with its X and Y coordinates. On inspection of the first
    twenty points of the data frame, we see an output as illustrated in the following
    screenshot:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们看到上述代码生成了许多点，其中每个点代表一个单词及其 X 和 Y 坐标。检查数据框架的前二十个点时，我们看到如下图所示的输出：
- en: '![](img/00337.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00337.jpeg)'
- en: On plotting all the points using the `all_word_vectors_2D` variable, you should
    see an output that looks similar to the one in the following screenshot:![](img/00338.jpeg)
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用 `all_word_vectors_2D` 变量绘制所有点，您应该会看到类似以下截图的输出：![](img/00338.jpeg)
- en: The above command will produce a plot of all tokens or words generated from
    the entire text as shown in the following screenshot:![](img/00339.jpeg)
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令将生成从整个文本生成的所有标记或单词的绘图，如下图所示：![](img/00339.jpeg)
- en: We can use the `plot_region` function to zoom into a certain area of the plot
    so that we are able to actually see the words, along with their coordinates. This
    step is illustrated in the following screenshot:![](img/00340.jpeg)
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 `plot_region` 函数来放大绘图中的某个区域，以便我们能够实际看到单词及其坐标。这一步骤如下图所示：![](img/00340.jpeg)
- en: An enlarged or zoomed in area of the plot can be visualized by setting the `x_bounds` and
    `y_bounds`, values as shown in the following screenshot:![](img/00341.jpeg)
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过设置 `x_bounds` 和 `y_bounds` 的值，可以可视化绘图的放大区域，如下图所示：![](img/00341.jpeg)
- en: A different region of the same plot can be visualized by varying the `x_bounds`
    and `y_bounds` values as shown in the following two screenshots:![](img/00342.jpeg)
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过改变 `x_bounds` 和 `y_bounds` 的值来可视化相同绘图的不同区域，如下两个截图所示：![](img/00342.jpeg)
- en: '![](img/00343.jpeg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00343.jpeg)'
- en: See also
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'The following additional points are of note:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 还有以下额外的要点：
- en: 'For more information on how the t-SNE algorithm works, visit the following
    link:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关 t-SNE 算法工作原理的更多信息，请访问以下链接：
- en: '[https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)'
- en: 'More information about cosine distance similarity and ranking can be found
    by visiting the following link:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关余弦距离相似性和排名的更多信息，请访问以下链接：
- en: '[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)'
- en: 'Use the following link to explore the different functions of the `Seaborn`
    library:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下链接来探索 `Seaborn` 库的不同功能：
- en: '[https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
- en: Analyzing further
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步分析
- en: This section will describe further analysis that can be performed on the data
    after visualization. For example, exploring cosine distance similarity between
    different word vectors.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述可在可视化后对数据执行的进一步分析。例如，探索不同单词向量之间的余弦距离相似性。
- en: Getting ready
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The following link is a great blog on how cosine distance similarity works
    and also discusses some of the math involved:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接是关于余弦距离相似性工作原理的出色博客，并讨论了一些涉及的数学内容：
- en: '[http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)'
- en: How to do it...
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Consider the following:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下内容：
- en: 'Various natural-language processing tasks can be performed using the different
    functions of `Word2Vec`. One of them is finding the most semantically similar
    words given a certain word (that is, word vectors that have a high cosine similarity
    or a short Euclidean distance between them). This can be done by using the `most_similar` function
    form `Word2Vec`, as shown in the following screenshot:![](img/00344.jpeg)This
    screenshots  all the closest words related to the word `Lannister`:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `Word2Vec` 的不同功能执行各种自然语言处理任务。其中之一是在给定某个单词时找到最语义相似的单词（即具有高余弦相似性或它们之间的欧几里德距离较短的单词向量）。可以使用
    `Word2Vec` 的 `most_similar` 函数来执行此操作，如下图所示：![](img/00344.jpeg)此截图显示了与单词 `Lannister`
    相关的所有最接近的单词：
- en: '![](img/00345.jpeg)This screenshot shows a list of all the words related to
    word `Jon`:![](img/00346.jpeg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00345.jpeg)此截图显示了与单词 `Jon` 相关的所有单词的列表：![](img/00346.jpeg)'
- en: How it works...
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Consider the following:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下内容：
- en: 'There are various methods to measure the semantic similarity between words.
    The one we are using in this section is based on cosine similarity. We can also
    explore linear relationships between words by using the following lines of code:'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有各种方法来衡量单词之间的语义相似性。我们在本节中使用的方法是基于余弦相似性的。我们还可以通过以下代码来探索单词之间的线性关系：
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To find the cosine similarity of nearest words to a given set of words, use
    the following commands:'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要找到给定一组词的最近词的余弦相似度，请使用以下命令：
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The preceding process is illustrated in the following screenshot:![](img/00347.jpeg)
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述过程如下截图所示：![](img/00347.jpeg)
- en: The results are as follows:![](img/00348.jpeg)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果如下：![](img/00348.jpeg)
- en: As seen in this section, word vectors form the basis of all NLP tasks. It is
    important to understand them and the math that goes into building these models
    before diving into more complicated NLP models such as **recurrent neural networks** and **Long
    Short-Term Memory** (**LSTM**) cells.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如本节所示，词向量是所有自然语言处理任务的基础。在深入研究更复杂的自然语言处理模型（如循环神经网络和长短期记忆（LSTM）单元）之前，了解它们以及构建这些模型所涉及的数学是很重要的。
- en: See also
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Further reading can be undertaken for a better understanding of the use of
    cosine distance similarity, clustering and other machine learning techniques used
    in ranking word vectors. Provided below are a few links to useful published papers
    on this topic:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 可以进一步阅读有关使用余弦距离相似性、聚类和其他机器学习技术在排名词向量中的应用的内容，以更好地理解。以下是一些有用的关于这个主题的已发表论文的链接：
- en: '[https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf](https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf](https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf)'
- en: '[http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf](http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf](http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf)'
