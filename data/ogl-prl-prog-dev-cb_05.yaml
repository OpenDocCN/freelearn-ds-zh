- en: Chapter 5. Developing a Histogram OpenCL program
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 开发直方图OpenCL程序
- en: 'In this chapter, we''ll cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下食谱：
- en: Implementing a histogram in C/C++
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在C/C++中实现直方图
- en: OpenCL implementation of the histogram
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直方图的OpenCL实现
- en: Work-item synchronization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作项同步
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Anyone who has taken elementary math in school would know what a histogram is.
    It's one of the myriad of ways by which one can visualize the relationship between
    two sets of data. These two sets of data are arranged on two axes such that one
    axis would represent the distinct values in the dataset and the other axis would
    represent the frequency at which each value occurred.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 任何在学校学过基础数学的人都知道什么是直方图。它是众多可以可视化两组数据之间关系的方法之一。这两组数据被安排在两个轴上，其中一个轴将代表数据集中的不同值，另一个轴将代表每个值出现的频率。
- en: The histogram is an interesting topic to study because its practical applications
    are found in computational image processing, quantitative/qualitative finance,
    computational fluid dynamics, and so on. It is one of the earliest examples of
    OpenCL usage when running on CPUs or GPUs, where several implementations have
    been made and each implementation has its pros and cons.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图是一个有趣的研究主题，因为它的实际应用可以在计算图像处理、定量/定性金融、计算流体动力学等领域找到。它是OpenCL在CPU或GPU上运行时的早期示例之一，其中已经进行了多种实现，每种实现都有其优缺点。
- en: Implementing a Histogram in C/C++
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在C/C++中实现直方图
- en: Before we look at how we can implement this in OpenCL and run the application
    on the desktop GPU, let's take a look at how we can implement it using a single
    thread of execution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看如何在OpenCL中实现它并在桌面GPU上运行应用程序之前，让我们看看如何使用单个执行线程来实现它。
- en: Getting ready
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This study of the sequential code is important because we need a way to make
    sure our sequential code and parallel code produce the same result, which is quite
    often referred to as the **golden reference** implementation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对顺序代码的研究很重要，因为我们需要一种方法来确保我们的顺序代码和并行代码产生相同的结果，这通常被称为**黄金参考**实现。
- en: Note
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In your role as an OpenCL engineer, one of the items on your to-do list would
    probably be to translate sequential algorithms to parallel algorithms, and it's
    important for you to be able to understand how to do so. We attempt to impart
    some of these skills which may not be exhaustive in all sense. One of the foremost
    important skills to have is the ability to identify **parallelizable routines**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在您作为OpenCL工程师的角色中，您的待办事项列表上的一项可能是将顺序算法转换为并行算法，并且您能够理解如何做到这一点非常重要。我们试图传授一些可能并不完全详尽的技能。最重要的技能之一是能够识别**可并行化例程**。
- en: Examining the code that follows, we can begin to understand how the histogram
    program works.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 检查下面的代码，我们可以开始理解直方图程序是如何工作的。
- en: How to do it…
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Here, we present the sequential code in its entirety, where it uses exactly
    one executing thread to create the memory structures of a histogram. At this point,
    you can copy the following code and paste it in a directory of your choice and
    call this program `Ch5/histogram_cpu/histogram.c`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了整个顺序代码，其中它使用一个执行线程来创建直方图的内存结构。在此阶段，您可以复制以下代码并将其粘贴到您选择的目录中，并将此程序命名为
    `Ch5/histogram_cpu/histogram.c`：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To build the program, we are assuming that you have a GNU GCC compiler. Type
    the following command to into a terminal:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建程序，我们假设您有一个GNU GCC编译器。在终端中输入以下命令：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Alternatively, run `make` at the directory `Ch5/histogram_c`, and an executable
    named `histogram` will be deposited in your directory where you issued that command.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，在 `Ch5/histogram_c` 目录下运行 `make`，一个名为 `histogram` 的可执行文件将被放置在您执行该命令的目录中。
- en: To run the program, simply execute the program `histogram` deposited on the
    folder `Ch5/histogram_c`, and it should output nothing. However, feel free to
    inject C's output function `printf`, `sprintf` into the previous code and convince
    yourself that the histogram is working as it should.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行程序，只需在 `Ch5/histogram_c` 文件夹中执行名为 `histogram` 的程序，它应该不会输出任何内容。然而，您可以自由地将C的输出函数
    `printf`、`sprintf` 注入到之前的代码中，并说服自己直方图正在按预期工作。
- en: How it works…
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作…
- en: 'To make a histogram, we need to have an initial dataset where it contains values.
    The values in a histogram are computed by scanning through the dataset and recording
    how many times a scanned value has appeared in the dataset. Hence, the concept
    of **data binning**. The following diagram illustrates this concept:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要制作直方图，我们需要一个包含值的初始数据集。直方图中的值是通过扫描数据集并记录扫描值在数据集中出现的次数来计算的。因此，有了**数据分箱**的概念。以下图表说明了这个概念：
- en: '![How it works…](img/4520OT_05_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作…](img/4520OT_05_01.jpg)'
- en: 'In the following code, we see that the first `for` loop fills up the array
    `data` with values ranging from `0` to `255`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们看到第一个`for`循环将`data`数组填充了从`0`到`255`的值：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The second `for` loop walks the `data` array and records the occurrence of each
    value, and the final `for` loop serves to print out the occurrence of each value.
    That is the essence of data binning.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个`for`循环遍历`data`数组并记录每个值的出现次数，最后的`for`循环用于打印每个值的出现次数。这就是数据分箱的本质。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, you would iterate the binned data and print out what you''ve found:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你会迭代分箱数据并打印出你发现的内容：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we are going to look at how OpenCL can apply data binning into its implementation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨OpenCL如何将其实现应用于数据分箱。
- en: OpenCL implementation of the Histogram
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenCL的直方图实现
- en: In this section, we will attempt to develop your intuition to be able to identify
    possible areas of parallelization and how you can use those techniques to parallelize
    sequential algorithms.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试培养你的直觉，以便能够识别可能的并行化区域以及如何使用这些技术来并行化顺序算法。
- en: Not wanting to delve into too much theory about parallelization, one of the
    key insights about whether a routine/algorithm can be parallelized is to examine
    whether the algorithm allows work to be split among different processing elements.
    Processing elements from the OpenCL's perspective would be the processors, that
    is, CPU/GPU.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不想深入太多关于并行化的理论，关于一个例程/算法是否可以并行化的一个关键洞察是检查算法是否允许工作在不同处理元素之间分割。从OpenCL的角度来看，处理元素将是处理器，即CPU/GPU。
- en: Note
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Recall that OpenCL's work items are execution elements that act on a set of
    data and execute on the processing element. They are often found in a work group
    where all work items can coordinate data reads/writes to a certain degree and
    they share the same kernel and work-group barriers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，OpenCL的工作项是执行元素，它们作用于一组数据并在处理元素上执行。它们通常在工作组中找到，其中所有工作项可以在一定程度上协调数据读取/写入，并且它们共享相同的内核和工作组屏障。
- en: 'Examining the code, you will notice that the first thing that is probably able
    to fulfill the description:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 检查代码，你会注意到第一件事可能是能够满足描述的：
- en: '*"...allows work to be split among different processing elements"*'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“...允许工作在不同处理元素之间分割”*'
- en: This would be to look for `for` loops. This is because loops mean that the code
    is executing the same block of instructions to achieve some outcome, and if we
    play our cards right, we should be able to split the work in the loop and assign
    several threads to execute a portion of the code along with the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为了寻找`for`循环。这是因为循环意味着代码正在执行相同的指令块以实现某种结果，如果我们做得好，我们应该能够将循环中的工作拆分，并为执行代码的一部分分配多个线程。
- en: Getting ready
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In many algorithms, you will see that splitting the work sometimes does not
    necessarily imply that the data needs to be cleanly partitioned, and that's because
    the data is read-only; however, when the algorithm needs to conduct both reads
    and writes to the data, then you need to figure out a way to partition them cleanly.
    That last sentence deserves some explanation. Recall in [Chapter 2](ch02.html
    "Chapter 2. Understanding OpenCL Data Transfer and Partitioning"), *Understanding
    OpenCL Data Transfer and Partitioning*, where we discussed work items and data
    partitioning, and by now you should have understood that OpenCL does not prevent
    you, the developer, from creating race conditions for your data if you miscalculated
    the data indexing or even introduced data dependencies.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多算法中，你会发现有时将工作拆分并不一定意味着数据需要被干净地分割，这是因为数据是只读的；然而，当算法需要对数据进行读写操作时，你需要想出一个方法来干净地分割它们。最后一句话需要一些解释。回想一下[第2章](ch02.html
    "第2章。理解OpenCL数据传输和分割")，*理解OpenCL数据传输和分割*，我们讨论了工作项和数据分割，到现在你应该已经理解了，如果你在数据索引计算错误或者引入了数据依赖，OpenCL不会阻止你，作为开发者，为你的数据创建竞态条件。
- en: With great power, comes great responsibility.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 权力越大，责任越大。
- en: 'In building a data parallel algorithm, it''s important to be able to understand
    a couple of things, and from the perspective of implementing an OpenCL histogram
    program, here are some suggestions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建数据并行算法时，能够理解一些事情非常重要，并且从实现OpenCL直方图程序的角度来看，这里有一些建议：
- en: '**Understand your data structure**: In the previous chapters, we have seen
    how we can allow user-defined structures and regular 1D or 2D arrays to be fed
    into the kernel for execution. You should always search for an appropriate structure
    to use and make sure you watch for the off-by-one errors (in my experience, they
    are more common than anything else).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解你的数据结构**：在前几章中，我们看到了如何允许用户定义的结构和常规的1D或2D数组被输入到内核中进行执行。你应该始终寻找合适的结构来使用，并确保你注意到了偏移量错误（在我的经验中，它们比其他任何东西都更常见）。'
- en: '**Decide how many work items should execute in a work-group**: If the kernel
    only has one work item executing a large dataset, it''s often not efficient to
    do so because of the way the hardware works. It makes sense to configure a sizeable
    number of work items to execute in the kernel so that they take advantage of the
    hardware''s resources and this often increases the temporal and spatial locality
    of data, which means your algorithm runs faster.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决定在一个工作组中应该执行多少个工作项**：如果内核只有一个工作项执行大量数据集，由于硬件的工作方式，这样做通常效率不高。合理地配置大量工作项在内核中执行是有意义的，这样它们就可以利用硬件资源，这通常会增加数据的时空局部性，这意味着你的算法运行得更快。'
- en: '**Decide how to write the eventual result**: In the histogram implementation
    we''ve chosen, this is important because each kernel will process a portion of
    the data and we need to merge them back. We have not seen examples of that before,
    so here''s our chance!'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决定如何编写最终结果**：在我们选择的直方图实现中，这很重要，因为每个内核将处理数据的一部分，我们需要将它们合并。我们之前没有看到过这样的例子，所以这是我们的机会！'
- en: Let's see how those suggestions could apply. The basic idea is to split a large
    array among several work groups. Each work group will process its own data (with
    proper indexing) and store/bin that data in the scratchpad memory provided by
    the hardware, and when the work group has finished its processing, its local memory
    will be stored back to the global memory.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些建议如何应用。基本思路是将一个大数组分配给几个工作组。每个工作组将处理自己的数据（带有适当的索引）并将这些数据存储/存储在硬件提供的暂存器内存中，当工作组完成其处理时，其本地内存将存储回全局内存。
- en: We have chosen the 1D array to contain the initial set of data and this data
    can potentially be infinite, but the author's machine configuration doesn't have
    limitless memory, so there's a real limit. Next, we will split this 1D array into
    several chunks, and this is where it gets interesting.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了一维数组来包含初始数据集，这些数据可能无限，但作者的机器配置没有无限的内存，所以有一个实际的限制。接下来，我们将这个一维数组分成几个块，这很有趣。
- en: Each chunk of data will be cleanly partitioned and executed by a work group.
    This work group has chosen to house 128 work items and each work item will produce
    a bin of size 256 elements or a 256 bin.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据块将被干净地分区并由一个工作组执行。这个工作组选择容纳128个工作项，每个工作项将产生一个大小为256个元素的桶或256个桶。
- en: Each work group will store these into the local memory also known as s**cratchpad
    memory** because we don't want to keep going back and forth global and device
    memory. This is a real performance hit.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每个工作组将把这些数据存储到本地内存中，也称为**暂存器内存**，因为我们不希望不断在全局和设备内存之间来回移动。这会真正影响性能。
- en: In the code presented in the following section, one of the techniques you will
    learn is to use the scratchpad memory or local memory in aiding your algorithm
    to execute faster.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中展示的代码中，你将学习到的一种技术是使用暂存器内存或本地内存来帮助你的算法更快地执行。
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Local memory is a software controlled scratchpad memory, and hence its name.
    The scratchpad allows the kernel to explicitly load items into that memory space,
    and they exist in local memory until the kernel replaces them, or until the work
    group ends its execution. To declare a block of local memory, the `__local` keyword
    is used and you can declare them in the parameters to the kernel call or in the
    body of the kernel. This memory allocation is shared by all work items in the
    work group.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 局部内存是一种软件控制的临时内存，因此得名。临时内存允许内核明确地将项目加载到该内存空间，并且它们存在于局部内存中，直到内核替换它们，或者直到工作组结束其执行。要声明一块局部内存，使用
    `__local` 关键字，并且可以在内核调用的参数中或在其主体中声明它们。这种内存分配由工作组中的所有工作项共享。
- en: The host code cannot read from or write to local memory. Only the kernel can
    access local memory.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 宿主代码不能从或向局部内存读写。只有内核可以访问局部内存。
- en: So far you have seen how to obtain memory allocation from the OpenCL device
    and fire the kernel to consume the input data and reading from that processed
    data subsequently for verification. What you are going to experience in the following
    paragraphs might hurt your head a little, but have faith in yourself, and I'm
    sure we can get this through.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了如何从 OpenCL 设备获取内存分配，并触发内核以消耗输入数据，随后从处理后的数据中读取以进行验证。在接下来的段落中，你可能会感到有些头疼，但请相信自己，我相信我们可以顺利通过。
- en: How to do it…
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'The complete working kernel is presented as follows from `Ch5/histogram/histogram.cl`,
    and we have littered comments in the code so as to aid you in understanding the
    motivation behind the constructs:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的工作内核如下所示，来自 `Ch5/histogram/histogram.cl`，我们在代码中添加了一些注释，以便帮助您理解结构背后的动机：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To compile it on the OSX platform, you would run a compile command similar
    to the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 OS X 平台上编译它，你需要运行类似于以下命令的编译命令：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Alternatively, you can run `make` at the directory `Ch5/histogram`, and you
    would have a binary executable named `Histogram`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以在 `Ch5/histogram` 目录下运行 `make`，你将得到一个名为 `Histogram` 的二进制可执行文件。
- en: 'To run the program, simply execute the program, `Histogram`. A sample output
    on my machine, which is an OS X, is:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行程序，只需执行名为 `Histogram` 的程序。在我的机器上，一个 OS X 的示例输出如下：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works…
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'In the host code, we first assign the necessary data structures that we need
    to implement the histogram. An excerpt from the source `Ch5/histogram/main.c`
    demonstrates the code that creates a single device queue, with the kernel and
    your usual suspects. The variables `inputBuffer` and `intermediateBinBuffer` refer
    to the unbinned array and intermediate bins:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在宿主代码中，我们首先分配实现直方图所需的数据结构。`Ch5/histogram/main.c` 源代码的摘录展示了创建单个设备队列的代码，其中包含内核和你的常规嫌疑人。变量
    `inputBuffer` 和 `intermediateBinBuffer` 指的是未归一化的数组和中间的桶：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So conceptually, the code splits the input data into chunks of 256 elements
    and each such chunk would be loaded into device''s local memory, which would be
    processed by the work items in the work group. The following is an illustration
    of how it looks like:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从概念上讲，代码将输入数据分成256个元素的块，每个这样的块将被加载到设备的局部内存中，然后由工作组中的工作项进行处理。以下是如何看起来：
- en: '![How it works…](img/4520OT_05_02.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/4520OT_05_02.jpg)'
- en: Now, imagine the kernel is going to execute the code and it needs to know how
    to fetch the data from the global memory, process it, and store it back to some
    data store. Since we have chosen to use the local memory as a temporary data store,
    let's take a look at how local memory can be used to help our algorithm, and finally
    examine how it's processed.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象内核将要执行代码，并且它需要知道如何从全局内存中获取数据，处理它，并将其存储回某些数据存储中。由于我们选择使用局部内存作为临时数据存储，让我们看看局部内存如何帮助我们算法，并最终检查其处理过程。
- en: Local memory resembles a lot to any other memory in C, hence you need to initialize
    it to a proper state before you can use it. After this, you need to make sure
    that proper array indexing rules are obeyed since those one-off errors can crash
    your program and might hang your OpenCL device.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 局部内存与 C 中的任何其他内存非常相似，因此在使用之前需要将其初始化到适当的状态。之后，你需要确保遵守适当的数组索引规则，因为那些一次性错误可能会使你的程序崩溃，并可能导致你的
    OpenCL 设备挂起。
- en: 'The initialization of the local memory is carried out by the following program
    statements:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 局部内存的初始化是通过以下程序语句完成的：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: At this point, I should caution you to put on your many-core hat now and imagine
    that 128 threads are executing this kernel. With this understanding, you will
    realize that the entire local memory is set to zero by simple arithmetic. The
    important thing to realize by now, if you haven't, is that each work item should
    not perform any repeated action.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我应该提醒你戴上你的多核帽子，想象一下128个线程正在执行这个内核。有了这个理解，你会意识到整个局部内存通过简单的算术被设置为零。现在，如果你还没有意识到，重要的是每个工作项不应执行任何重复的操作。
- en: Note
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The initialization could have been written in a sequential fashion and it would
    still work, but it means each work item's initialization would overlap with some
    other work item's execution. This is, in general, bad since in our case, it would
    be harmless, but in other cases it means that you could be spending a large amount
    of time debugging your algorithm. This synchronization applies to all work items
    in a work group, but doesn't help in synchronizing between work groups.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化可以以顺序方式编写，并且仍然可以工作，但这意味着每个工作项的初始化将与某些其他工作项的执行重叠。这通常是不好的，因为在我们这个例子中，它可能是无害的，但在其他情况下，它意味着你可能会花费大量时间调试你的算法。这种同步适用于工作组中的所有工作项，但不会帮助在工作组之间进行同步。
- en: Next, we see a statement that we probably have not seen before. This is a form
    of synchronization or memory barrier. The interesting observation about barriers
    is that all the work items must reach this statement before being allowed to proceed
    any further. It's like a starting line for runners in a 100 meter race.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看到一个我们可能之前没有见过的语句。这是一种同步或内存屏障的形式。关于屏障的有趣观察是，所有工作项必须到达这个语句才能继续进行。这就像100米赛跑中跑者的起跑线。
- en: Reason for this is that our algorithm's correctness depends on the fact that
    each element in the local memory must be `0` prior to any work-item wishing to
    read and write to it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是，我们算法的正确性取决于这样一个事实：在任何一个工作项希望读取和写入之前，局部内存中的每个元素都必须是 `0`。
- en: Note
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You should be aware that you cannot set a value for the local memory greater
    than what is available on the OpenCL device. In order to determine what is the
    maximum configured scratchpad memory on your device, you need to employ the API
    `clGetDeviceInfo` passing in the parameter `CL_DEVICE_LOCAL_MEM_SIZE`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该知道，你不能为局部内存设置一个大于OpenCL设备上可用的值的值。为了确定设备上配置的最大临时存储器内存，你需要使用API `clGetDeviceInfo`
    并传入参数 `CL_DEVICE_LOCAL_MEM_SIZE`。
- en: 'Conceptually, here''s what the previous piece of code is doing—each work item
    sets all elements to zero in a column-wise fashion and sets the elements collectively
    as a work group with **128** work items executing it, sweeping from left to right.
    As each item is a `uchar4` data type, you see that the number of rows is **64**
    instead of **256**:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，这是前面那段代码所做的事情——每个工作项以列向量的方式将所有元素设置为零，并以**128**个工作项作为一个工作组集体执行它，从左到右进行扫描。由于每个项都是
    `uchar4` 数据类型，你会看到行数是**64**而不是**256**：
- en: '![How it works…](img/4520OT_05_03.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作…](img/4520OT_05_03.jpg)'
- en: Finally, let's attempt to understand how the values are fetched from global
    memory and stored in the scratchpad.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们尝试理解值是如何从全局内存中检索出来并存储在临时存储器中的。
- en: When a work group begins executing, it will reach into global memory and fetch
    the contents of four values and stores them into a local variable and once that's
    done, the next four statements are executed by each work item to process each
    retrieved value using the component selection syntax, that is, `value.s0, value.s1,
    value.s2, value.s3`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个工作组开始执行时，它会访问全局内存并获取四个值的 内容，并将它们存储到局部变量中，一旦完成，接下来的四个语句将由每个工作项执行，以使用组件选择语法处理检索到的每个值，即
    `value.s0, value.s1, value.s2, value.s3`。
- en: The following illustration, provides how a work item can potentially access
    four rows of data on the scratchpad and update four elements in those rows by
    incrementing them. The important point to remember is that all elements in the
    scratchpad must be written before they can be processed, and hence this is the
    barrier.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的插图展示了工作项如何潜在地访问临时存储器上的四行数据，并通过递增它们来更新这些行中的四个元素。需要记住的重要一点是，在处理之前，临时存储器中的所有元素都必须被写入，因此这就是屏障。
- en: This type of programming technique where we build intermediate data structures
    so that we can obtain the eventual data structure is often called **thread-based
    histograms** in some circles. The technique is often employed when we know what
    the final data structure looks like and we use the same ADT to solve for smaller
    portions of data so that we can merge them in the end.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编程技术，我们构建中间数据结构以便最终获得所需的数据结构，在一些圈子中常被称为**基于线程的直方图**。当我们知道最终数据结构的样子，并使用相同的ADT来解决数据的小部分以便最终合并时，通常会采用这种技术。
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![How it works…](img/4520OT_05_04.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作…](img/4520OT_05_04.jpg)'
- en: If you analyze the memory access pattern, you will realize that what we have
    created an **Abstract Data Type** (**ADT**) known as the **hash table** where
    each row of data in the local memory represents the list of frequencies of the
    occurrence of a value between 0 and 255.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你分析内存访问模式，你会意识到我们创建了一个名为**抽象数据类型**（**ADT**）的**哈希表**，其中本地内存中的每一行数据代表0到255之间值出现的频率列表。
- en: With that understanding, we can come to the final part of solving this problem.
    Again, imagine that the work group has executed to this point, you have basically
    a hash table, and you want to merge all those other hash tables held in the local
    memories of the other work groups.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有这样的理解，我们就可以进入解决这个问题的最后部分。再次想象工作组已经执行到这一点，你基本上有一个哈希表，你想要合并其他工作组在本地内存中持有的所有其他哈希表。
- en: 'To achieve this, we need to basically walk through the hash table, aggregate
    all the values for each row, and we would have our answer. However, now we only
    need one thread to perform all this, otherwise all 128 threads executing the *walk*
    would mean you''re overcounting your values by 128 times! Therefore, to achieve
    this, we make use of the fact that each work item has a local ID in the work group,
    and we execute this code by selecting one particular work item only. The following
    code illustrates this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们基本上需要遍历哈希表，对每一行的所有值进行聚合，然后我们就能得到答案。然而，现在我们只需要一个线程来完成所有这些，否则所有128个执行*遍历*的线程意味着你的值会被重复计算128次！因此，为了实现这一点，我们利用每个工作项在工作组中都有一个局部ID的事实，并且通过只选择一个特定的工作项来执行此代码。以下代码说明了这一点：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There is no particular reason why the first work item is chosen, I guess this
    is done just by convention, and there's no harm choosing other work items, but
    the important thing to remember is that there must only be one executing code.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 选择第一个工作项没有特别的原因，我想这仅仅是一种惯例，选择其他工作项也没有关系，但重要的是要记住，必须只有一个正在执行的代码。
- en: Now we turn our attention back to the host code again, since each intermediate
    bin has been filled conceptually with its respective value from its respective
    portions of the large input array.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将注意力再次转向主代码，因为每个中间桶已经从其相应的大输入数组部分概念上填充了相应的值。
- en: 'The (slightly) interesting part of the host code is simply walking through
    the returned data held in `intermediateBins` and aggregating them to `deviceBin`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 主代码中（稍微）有趣的部分仅仅是遍历`intermediateBins`中返回的数据，并将它们聚合到`deviceBin`中：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: And we are done!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了！
- en: Work item synchronization
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作项同步
- en: 'This section is to introduce you to the concepts of synchronization in OpenCL.
    Synchronization in OpenCL can be classified into two groups:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在向您介绍OpenCL中的同步概念。OpenCL中的同步可以分为两组：
- en: Command queue barriers
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令队列屏障
- en: Memory barriers
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存屏障
- en: Getting ready
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: The command queue barrier ensures that all previously queued commands to a command
    queue have finished execution before any following commands queued in the command
    queue can begin execution.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 命令队列屏障确保在命令队列中排队的所有先前命令完成执行之后，才能开始执行命令队列中排队的任何后续命令。
- en: The work group barrier performs synchronizations between work items in a work
    group executing the kernel. All work items in a work group must execute the barrier
    construct before any are allowed to continue execution beyond the barrier.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 工作组屏障在执行内核的工作组中的工作项之间执行同步。工作组中的所有工作项都必须在允许任何工作项在屏障之后继续执行之前执行屏障构造。
- en: How to do it…
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'There are two APIs for the command queue barriers and they are:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 命令队列屏障有两个API，它们是：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'But as of OpenCL 1.2, the following command queue barriers are deprecated:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但截至OpenCL 1.2，以下命令队列屏障已被弃用：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: These four/two APIs in OpenCL 1.2/1.1 respectively, allow us to perform synchronization
    across the various OpenCL commands, but they do not synchronize the work items.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCL 1.2/1.1 中的这四个/两个 API，允许我们在各种 OpenCL 命令之间进行同步，但它们并不同步工作项。
- en: Note
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: There is no synchronization facility available to synchronize between work groups.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 没有同步设施可用于在工作组之间进行同步。
- en: We have not seen any example codes on how to use this, but it is still good
    to know they exist, if we ever need them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有看到任何关于如何使用此功能的示例代码，但如果我们需要它们，了解它们的存在仍然是好的。
- en: Next, you can place barriers to work items in a work group that performs reads
    and writes to/from local memory or global memory. Previously, you read that all
    work items executing the kernel must execute this function before any are allowed
    to continue execution beyond the barrier. This type of barrier must be encountered
    by all work items in a work group.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以将障碍放置在执行对本地内存或全局内存进行读写操作的工作组的工作项中。之前，你了解到所有执行内核的工作项必须在任何工作项继续执行超过障碍之前，执行此函数。此类障碍必须被工作组中的所有工作项遇到。
- en: How it works…
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The OpenCL API is as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCL API 如下：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: where flags can be `CLK_LOCAL_MEM_FENCE` or `CLK_GLOBAL_MEM_FENCE`. Be careful
    where you place the barrier in the kernel code. If the barrier is needed in a
    conditional statement that is like an `if-then-else` statement, then you must
    make sure all execution paths by the work items can reach that point in the program.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中标志可以是 `CLK_LOCAL_MEM_FENCE` 或 `CLK_GLOBAL_MEM_FENCE`。在内核代码中放置障碍时请小心。如果障碍需要在类似于
    `if-then-else` 的条件语句中，那么你必须确保所有工作项的执行路径都能到达程序中的那个点。
- en: Note
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `CLK_LOCAL_MEM_FENCE` barrier will either flush any variables stored in
    local memory or queue a memory fence to ensure correct ordering of memory operations
    to local memory.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`CLK_LOCAL_MEM_FENCE` 障碍将清除存储在本地内存中的任何变量，或者排队一个内存栅栏以确保本地内存操作的正确顺序。'
- en: The `CLK_GLOBAL_MEM_FENCE` barrier function will queue a memory fence to ensure
    correct ordering of memory operations to global memory.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`CLK_GLOBAL_MEM_FENCE` 障碍函数会将内存栅栏排队，以确保全局内存操作的正确顺序。'
- en: Another side effect of placing such barriers is that when they're to be placed
    in loop construct, all work items must execute the barrier for each iteration
    of the loop before any are allowed to continue execution beyond the barrier. This
    type of barrier also ensures correct ordering of memory operations to local or
    global memory.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 放置此类障碍的另一个副作用是，当它们要放置在循环结构中时，所有工作项必须在任何工作项继续执行超过障碍之前，执行每个循环迭代的障碍。此类障碍也确保了内存操作到本地或全局内存的正确顺序。
