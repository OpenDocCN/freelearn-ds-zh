- en: Using LSTMs in Generative Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生成网络中使用LSTMs
- en: 'After reading this chapter, you will be able to accomplish the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，您将能够完成以下任务：
- en: Downloading novels/books that will be used as input text
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载将用作输入文本的小说/书籍
- en: Preparing and cleansing data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备和清理数据
- en: Tokenizing sentences
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对句子进行标记化
- en: Training and saving the LSTM model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练并保存LSTM模型
- en: Generating similar text using the model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型生成类似的文本
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Due to the drawbacks of **recurrent neural networks** (**RNNs**) when it comes
    to backpropagation, **Long Short-Term Memory Units** (**LSTMs**) and **Gated Recurrent
    Units** (**GRUs**) have been gaining popularity in recent times when it comes
    to learning sequential input data as they are better suited to tackle problems
    of vanishing and exploding gradients.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于**循环神经网络**（**RNNs**）在反向传播时存在一些缺点，**长短期记忆单元**（**LSTMs**）和**门控循环单元**（**GRUs**）在学习顺序输入数据时近来变得越来越受欢迎，因为它们更适合解决梯度消失和梯度爆炸的问题。
- en: Downloading novels/books that will be used as input text
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载将用作输入文本的小说/书籍
- en: In this recipe, we will go the steps that we need to download the novels/books
    which we will use as input text for the execution of this recipe.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将介绍下载小说/书籍所需的步骤，这些将作为本示例的输入文本进行执行。
- en: Getting ready
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Place the input data in the form of a `.txt` file in the working directory.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入数据以`.txt`文件的形式放在工作目录中。
- en: The input may be any kind of text, such as song lyrics, novels, magazine articles,
    and source code.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入可以是任何类型的文本，如歌词、小说、杂志文章和源代码。
- en: Most of the classical texts are no longer protected by copyright and may be
    downloaded for free and used in experiments. The best place to get access to free
    books is Project [Gutenberg](http://www.gutenberg.org/).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数经典文本不再受版权保护，可以免费下载并用于实验。获取免费书籍的最佳途径是Project [Gutenberg](http://www.gutenberg.org/)。
- en: 'In this chapter, we will be using *The Jungle book* by Rudyard Kipling as the
    input to train our model and generate statistically similar text as output. The
    following screenshot shows you how to download the necessary file in `.txt` format:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Rudyard Kipling的《丛林之书》作为输入来训练我们的模型，并生成统计上类似的文本作为输出。下面的截图显示了如何以`.txt`格式下载必要的文件：
- en: '![](img/00172.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00172.jpeg)'
- en: After visiting the website and searching for the required book, click on Plain
    Text UTF-8 and download it. UTF-8 basically specifies the type of encoding. The
    text may be copied and pasted or saved directly to the working directory by clicking
    on the link.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问网站并搜索所需的书籍后，点击“Plain Text UTF-8”并下载。UTF-8基本上指定了编码的类型。可以通过点击链接将文本复制粘贴或直接保存到工作目录中。
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'Before beginning, it always helps to take a look at the data and analyze it.
    After looking at the data, we can see that there are a lot of punctuation marks,
    blank spaces, quotes, and uppercase as well as lowercase letters. We need to prepare
    the data first before performing any kind of analysis on it or feeding it into
    the LSTM network. We require a number of libraries that will make handling data
    easier :'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，先看一下数据并进行分析总是有帮助的。查看数据后，我们可以看到有很多标点符号、空格、引号以及大写和小写字母。在对其进行任何分析或将其馈送到LSTM网络之前，我们需要先准备好数据。我们需要一些能够更轻松处理数据的库：
- en: 'Import the necessary libraries by issuing the following commands:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下命令导入必要的库：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output to the preceding commands looks like the following screenshot:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面命令的输出如下截屏所示：
- en: '![](img/00173.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00173.jpeg)'
- en: 'It is always a good idea to double check the current working directory and
    choose the required folder as the working directory. In our case, the `.txt` file
    is named `junglebook.txt` and is held in the folder named `Chapter 8`. So, we
    will select that folder as the working directory for the whole chapter. This may
    be done as shown in the following screenshot:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 始终要仔细检查当前工作目录，并选择所需的文件夹作为工作目录。在我们的案例中，`.txt`文件名为`junglebook.txt`，保存在名为`Chapter
    8`的文件夹中。因此，我们将选择该文件夹作为整个章节的工作目录。可以按照下面的截图所示进行操作：
- en: '![](img/00174.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00174.jpeg)'
- en: 'Next, load the file into the program''s memory by defining a function named
    `load_document`, which can be done by issuing the following commands:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过定义一个名为`load_document`的函数将文件加载到程序的内存中，可以通过以下命令完成：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Use the previously defined function to load the document into memory and print
    the first `2000` characters of the text file using the following script:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用先前定义的函数将文档加载到内存中，并使用以下脚本打印文本文件的前2000个字符：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Running the preceding function as well as the commands produces the output 
    shown in the following screenshots:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行前述函数以及命令会产生如下截屏所示的输出：
- en: '![](img/00175.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00175.jpeg)'
- en: 'The output to the above code is shown in the screenshot here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下截屏所示：
- en: '![](img/00176.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00176.jpeg)'
- en: 'The following screenshot is a continuation of the previous output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图是前面输出的延续：
- en: '![](img/00177.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00177.jpeg)'
- en: As seen in the preceding screenshots, the first `2000` characters from the `.txt`
    file are printed. It is always a good idea to analyze the data by looking at it
    before performing any preprocessing on it. It will give a better idea of how to
    approach the preprocessing steps.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前面的截图所示，打印了`.txt`文件中的前2000个字符。在执行任何预处理之前，始终先分析数据是个好主意。这将更好地指导我们如何进行预处理步骤。
- en: How it works...
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The `array` function will be used to handle data in the form of arrays. The
    `numpy` library provides this function readily.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`array`函数将用于处理数组形式的数据。`numpy`库提供了这个函数。'
- en: Since our data is only text data, we will require the string library to handle
    all input data as strings before encoding the words as integers, which can be
    fed.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的数据只是文本数据，我们将需要字符串库来处理所有输入数据作为字符串，然后将单词编码为整数，以便进行馈送。
- en: The `tokenizer` function will be used to split all the sentences into tokens,
    where each token represents a word.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokenizer`函数将用于将所有句子拆分为标记，其中每个标记代表一个单词。'
- en: The pickle library will be required in order to save the dictionary into a pickle
    file by using the `dump` function.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pickle库将被需要，以便使用`dump`函数将字典保存到pickle文件中。
- en: The `to_categorical` function from the `keras` library converts a class vector
    (integers) to a binary class matrix, for example, for use with `categorical_crossentropy`,
    which we will require at a later stage in order to map tokens to unique integers
    and vice versa.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`keras`库中的`to_categorical`函数将类向量（整数）转换为二进制类矩阵，例如，用于`categorical_crossentropy`，我们以后将需要将标记映射到唯一整数，反之亦然。'
- en: Some of the other Keras layers required in this chapter are the LSTM layer,
    dense layer, dropout layer, and the embedding layer. The model will be defined
    sequentially, for which we require the sequential model from the `keras` library.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章中所需的其他Keras层包括LSTM层、密集层、dropout层和嵌入层。模型将被顺序定义，因此我们需要`keras`库中的顺序模型。
- en: There's more...
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: You may also use the same model with different types of texts, such as customer
    reviews on websites, tweets, structured text such as source code, mathematics
    theories, and so on.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以使用相同的模型处理不同类型的文本，例如网站上的客户评论、推文、结构化文本（如源代码、数学理论等）等。
- en: The idea of this chapter to understand how LSTMs learn long-term dependencies
    and how they perform better at processing sequential data when compared to recurrent
    neural networks.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的目的是了解LSTM如何学习长期依赖关系，以及与循环神经网络相比，它们在处理序列数据时表现更好的方式。
- en: Another good idea would be to input *Pokémon* names into the model and try to
    generate your own Pokémon names.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个好主意是将* Pokémon *名称输入模型，并尝试生成自己的* Pokémon *名称。
- en: See also
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'More information about the different libraries used can be found at the following
    links:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用的不同库的更多信息可以在以下链接找到：
- en: '[https://www.scipy-lectures.org/intro/numpy/array_object.html](https://www.scipy-lectures.org/intro/numpy/array_object.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.scipy-lectures.org/intro/numpy/array_object.html](https://www.scipy-lectures.org/intro/numpy/array_object.html)'
- en: '[https://docs.python.org/2/library/string.html](https://docs.python.org/2/library/string.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/2/library/string.html](https://docs.python.org/2/library/string.html)'
- en: '[https://wiki.python.org/moin/UsingPickle](https://wiki.python.org/moin/UsingPickle)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://wiki.python.org/moin/UsingPickle](https://wiki.python.org/moin/UsingPickle)'
- en: '[https://keras.io/preprocessing/text/](https://keras.io/preprocessing/text/)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/preprocessing/text/](https://keras.io/preprocessing/text/)'
- en: '[https://keras.io/layers/core/](https://keras.io/layers/core/)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/layers/core/](https://keras.io/layers/core/)'
- en: '[https://keras.io/layers/recurrent/](https://keras.io/layers/recurrent/)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/layers/recurrent/](https://keras.io/layers/recurrent/)'
- en: Preparing and cleansing data
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备和清理数据
- en: This section of this chapter will discuss the various data preparation and text
    preprocessing steps involved before feeding it into the model as input. The specific
    way we prepare the data really depends on how we intend to model it, which in
    turn depends on how we intend to use it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的这一部分将讨论在将其作为输入馈送到模型之前涉及的各种数据准备和文本预处理步骤。我们准备数据的具体方式取决于我们打算对其进行建模的方式，这又取决于我们打算如何使用它。
- en: Getting ready
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The language model will be based on statistics and predict the probability of
    each word given an input sequence of text. The predicted word will be fed in as
    input to the model, to, in turn, generate the next word.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型将基于统计数据，并预测给定文本输入序列的每个单词的概率。预测的单词将被馈送到模型中，以便生成下一个单词。
- en: A key decision is how long the input sequences should be. They need to be long
    enough to allow the model to learn the context for the words to predict. This
    input length will also define the length of the seed text used to generate new
    sequences when we use the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键决定是输入序列应该有多长。它们需要足够长，以使模型能够学习单词的上下文以进行预测。此输入长度还将定义用于生成新序列的种子文本的长度，当我们使用模型时。
- en: For the purpose of simplicity, we will arbitrarily pick a length of 50 words
    for the length of the input sequences.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将任意选择长度为50个单词的输入序列长度。
- en: How to do it...
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Based on reviewing the text (which we did previously), the following are some
    operations that could be performed to clean and preprocess the text in the input
    file. We have presented a few options regarding text preprocessing. However, you
    may want to explore more cleaning operations as an exercise:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对文本的审查（我们之前做过），以下是可以执行的一些操作，以清理和预处理输入文件中的文本。我们提出了一些关于文本预处理的选项。但是，您可能希望探索更多的清理操作作为练习：
- en: Replace dashes `–` with whitespaces so you can split words better
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用空格替换破折号`–`，以便更好地拆分单词
- en: Split words based on whitespaces
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于空格拆分单词
- en: Remove all punctuation from the input text in order to reduce the number of
    unique characters in the text that is fed into the model (for example, Why? becomes
    Why)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除输入文本中的所有标点符号，以减少输入模型的文本中唯一字符的数量（例如，Why? 变为 Why）
- en: Remove all words that are not alphabetic to remove standalone punctuation tokens
    and emoticons
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除所有非字母的单词，以删除独立的标点符号标记和表情符号
- en: Convert all words from uppercase to lowercase in order to reduce the size of
    the total number of tokens further and remove any discrepancies and data redundancy
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有单词从大写转换为小写，以进一步减少标记的总数并消除任何差异和数据冗余
- en: 'Vocabulary size is a decisive factor in language modeling and deciding the
    training time for the model. A smaller vocabulary results in a more efficient
    model that trains faster. While it is good to have a small vocabulary in some
    cases, it helps to have a larger vocabulary in other cases in order to prevent
    overfitting. In order to preprocess the data, we are going to need a function
    that takes in the entire input text, splits it up based on white spaces, removes
    all punctuation, normalizes all cases, and returns a sequence of tokens. For this
    purpose, define the `clean_document` function by issuing the following commands:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇量是语言建模和决定模型训练时间的决定性因素。较小的词汇量会导致训练速度更快的更高效的模型。在某些情况下，拥有较小的词汇量是有益的，但在其他情况下，拥有较大的词汇量可以防止过拟合。为了预处理数据，我们需要一个函数，它接受整个输入文本，根据空格分割文本，删除所有标点，规范化所有情况，并返回一个标记序列。为此，通过以下命令定义`clean_document`函数：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The previously defined function will basically take the loaded document/file
    as its argument and return an array of clean tokens, as shown in the following
    screenshot:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 先前定义的函数基本上会将加载的文档/文件作为其参数，并返回一个干净的标记数组，如下面的屏幕截图所示：
- en: '![](img/00178.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00178.jpeg)'
- en: 'Next, print out some of the tokens and statistics just to develop a better
    understanding of what the `clean_document` function is doing. This step is done
    by issuing the following commands:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，打印出一些标记和统计数据，以更好地了解`clean_document`函数的作用。通过以下命令完成此步骤：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of the preceding set of commands prints the first two hundred tokens
    and is as shown in the following screenshots:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述一系列命令的输出打印了前两百个标记，如下面的屏幕截图所示：
- en: '![](img/00179.jpeg)![](img/00180.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00179.jpeg)![](img/00180.jpeg)'
- en: 'Next, organize all these tokens into sequences, with each sequence containing
    50 words (chosen arbitrarily) using the following commands:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令将所有这些标记组织成序列，每个序列包含50个单词（任意选择）：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The total number of sequences formed from the document may be viewed by printing
    them out, as shown in the following screenshot:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过打印输出文档形成的序列的总数来查看，如下面的屏幕截图所示：
- en: '![](img/00181.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00181.jpeg)'
- en: 'Save all the generated tokens as well as sequences into a file in the working
    directory by defining the `save_doc` function using the following commands:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下命令定义`save_doc`函数，将所有生成的标记以及序列保存到工作目录中的文件中：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To save the sequences, use the following two commands:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存这些序列，请使用以下两个命令：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This process is illustrated in the following screenshot:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程如下屏幕截图所示：
- en: '![](img/00182.jpeg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00182.jpeg)'
- en: 'Next, load the saved document, which contains all the saved tokens and sequences,
    into the memory using the `load_document` function, which is defined as follows:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，加载保存的文档，其中包含所有保存的标记和序列，到内存中使用定义如下的`load_document`函数：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/00183.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00183.jpeg)'
- en: How it works...
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The `clean_document` function removes all whitespaces, punctuation, uppercase
    text, and quotation marks, and splits the entire document into tokens, where each
    token is a word.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`clean_document`函数删除所有空格、标点、大写文本和引号，并将整个文档分割成标记，其中每个标记都是一个单词。'
- en: By printing the total number of tokens and total unique tokens in the document,
    we will note that the `clean_document` function generated 51,473 tokens, out of
    which 5,027 tokens (or words) are unique.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过打印文档中的标记总数和唯一标记总数，我们会注意到`clean_document`函数生成了51,473个标记，其中5,027个标记（或单词）是唯一的。
- en: The `save_document` function then saves all of these tokens as well as unique
    tokens which are required to generate our sequences of 50 words each. Note how,
    by looping through all the generated tokens, we are able to generate a long list
    of 51,422 sequences. These are the same sequences that will be used as input to
    train the language model.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，`save_document`函数保存所有这些标记，以及生成我们每个50个单词的序列所需的唯一标记。请注意，通过循环遍历所有生成的标记，我们能够生成一个包含51,422个序列的长列表。这些序列将用作训练语言模型的输入。
- en: Before training the model on all 51,422 sequences, it is always a good practice
    to save the tokens as well as sequences to file. Once saved, the file can be loaded
    back into the memory using the defined `load_document` function.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对所有51,422个序列进行模型训练之前，将标记以及序列保存到文件中始终是一个良好的做法。一旦保存，可以使用定义的`load_document`函数将文件加载回内存。
- en: The sequences are organized as 50 input tokens and one output token (which means
    that there are 51 tokens per sequence). For predicting each output token, the
    previous 50 tokens will be used as the input to the model. We can do this by iterating
    over the list of tokens from token 51 onwards and taking the previous 50 tokens
    as a sequence, then repeating this process until the end of the list of all tokens.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些序列组织为50个输入标记和一个输出标记（这意味着每个序列有51个标记）。为了预测每个输出标记，将使用前50个标记作为模型的输入。我们可以通过迭代从第51个标记开始的标记列表，并将前50个标记作为一个序列，然后重复此过程直到所有标记列表的末尾来实现这一点。
- en: See also
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Visit the following links for a better understanding of data preparation using
    various functions:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 访问以下链接，以更好地了解使用各种函数进行数据准备：
- en: '[https://docs.python.org/3/library/tokenize.html](https://docs.python.org/3/library/tokenize.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/3/library/tokenize.html](https://docs.python.org/3/library/tokenize.html)'
- en: '[https://keras.io/utils/](https://keras.io/utils/)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/utils/](https://keras.io/utils/)'
- en: '[http://www.pythonforbeginners.com/dictionary/python-split](http://www.pythonforbeginners.com/dictionary/python-split)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.pythonforbeginners.com/dictionary/python-split](http://www.pythonforbeginners.com/dictionary/python-split)'
- en: '[https://www.tutorialspoint.com/python/string_join.htm](https://www.tutorialspoint.com/python/string_join.htm)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tutorialspoint.com/python/string_join.htm](https://www.tutorialspoint.com/python/string_join.htm)'
- en: '[https://www.tutorialspoint.com/python/string_lower.htm](https://www.tutorialspoint.com/python/string_lower.htm)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tutorialspoint.com/python/string_lower.htm](https://www.tutorialspoint.com/python/string_lower.htm)'
- en: Tokenizing sentences
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对句子进行标记
- en: Before defining and feeding data into an LSTM network it is important that the
    data is converted into a form which can be understood by the neural network. Computers
    understand everything in binary code (0s and 1s) and therefore, the textual or
    data in string format needs to be converted into one hot encoded variables.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义和输入数据到LSTM网络之前，重要的是将数据转换为神经网络可以理解的形式。计算机理解的一切都是二进制代码（0和1），因此，文本或字符串格式的数据需要转换为独热编码变量。
- en: Getting ready
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For understanding how one hot encoding works, visit the following links:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解独热编码的工作原理，请访问以下链接：
- en: '[https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)'
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)'
- en: '[https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python](https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python](https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python)'
- en: '[https://www.ritchieng.com/machinelearning-one-hot-encoding/](https://www.ritchieng.com/machinelearning-one-hot-encoding/)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.ritchieng.com/machinelearning-one-hot-encoding/](https://www.ritchieng.com/machinelearning-one-hot-encoding/)'
- en: '[https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)'
- en: How to do it...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'After the going through the previous section you should be able to clean the
    entire corpus and split up sentences. The next steps which involve one hot encoding
    and tokenizing sentences can be done in the following manner:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 经过上一节的学习，您应该能够清理整个语料库并拆分句子。接下来涉及独热编码和标记化句子的步骤可以按以下方式完成：
- en: Once the tokens and sequences are saved to a file and loaded into memory, they
    have to be encoded as integers since the word embedding layer in the model expects
    input sequences to be comprised of integers and not strings.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦标记和序列被保存到文件并加载到内存中，它们必须被编码为整数，因为模型中的词嵌入层期望输入序列由整数而不是字符串组成。
- en: This is done by mapping each word in the vocabulary to a unique integer and
    encoding the input sequences. Later, while making predictions, the predictions
    can be converted (or mapped) back to numbers to look up their associated words
    in the same mapping and reverse map back from integers to words.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是通过将词汇表中的每个单词映射到唯一的整数并对输入序列进行编码来完成的。稍后，在进行预测时，可以将预测转换（或映射）回数字，以查找它们在相同映射中关联的单词，并从整数到单词的反向映射。
- en: 'To perform this encoding, utilize the `Tokenizer` class in the Keras API. Before
    encoding, the tokenizer must be trained on the entire dataset so it finds all
    the unique tokens and assigns each token a unique integer. The commands to do
    so as are  follows:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了执行这种编码，利用Keras API中的Tokenizer类。在编码之前，必须对整个数据集进行训练，以便找到所有唯一的标记，并为每个标记分配一个唯一的整数。要这样做的命令如下：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You also need to calculate the size of the vocabulary before defining the embedding
    layer later. This is determined by calculating the size of the mapping dictionary.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在后面定义嵌入层之前，还需要计算词汇表的大小。这是通过计算映射字典的大小来确定的。
- en: 'Therefore, when specifying the vocabulary size to the Embedding layer, specify
    it as 1 larger than the actual vocabulary. The vocabulary size is therefore defined
    as follows:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，在向嵌入层指定词汇表大小时，将其指定为实际词汇表大小加1。因此，词汇表大小定义如下：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that once the input sequences have been encoded, they need to be separated
    into input and output elements, which can be done by array slicing.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，一旦输入序列已经被编码，它们需要被分成输入和输出元素，这可以通过数组切片来完成。
- en: After separating, one hot encode the output word. This means converting it from
    an integer to an n-dimensional vector of 0 values, one for each word in the vocabulary,
    with a 1 to indicate the specific word at the index of the word's integer value. Keras
    provides the `to_categorical()` function, which can be used to one hot encode
    the output words for each input-output sequence pair.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分离后，对输出单词进行独热编码。这意味着将其从整数转换为n维向量，其中每个词汇表中的单词都有一个0值，用1表示单词的整数值的索引处的特定单词。Keras提供了`to_categorical()`函数，可用于为每个输入-输出序列对独热编码输出单词。
- en: Finally, specify to the Embedding layer how long input sequences are. We know
    that there are 50 words because the model was designed by specifying the sequence
    length as 50, but a good generic way to specify the sequence length is to use
    the second dimension (number of columns) of the input data’s shape.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，指定嵌入层输入序列的长度。我们知道有50个单词，因为模型是通过将序列长度指定为50来设计的，但指定序列长度的一个好的通用方法是使用输入数据形状的第二维（列数）。
- en: 'This can be done by issuing the following commands:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过发出以下命令来完成：
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'This section will describe the outputs you must see on executing the commands
    in the previous section:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述在执行上一节中的命令时必须看到的输出：
- en: 'After running the commands for tokenizing the sentences and calculating vocabulary
    length you must see an output as shown in the following screenshot:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对句子进行标记化和计算词汇表长度的命令运行后，您应该看到如下屏幕截图所示的输出：
- en: '![](img/00184.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00184.jpeg)'
- en: Words are assigned values starting from 1 up to the total number of words (for
    example, 5,027 in this case). The Embedding layer needs to allocate a vector representation
    for each word in this vocabulary from index 1 to the largest index. The index
    of the word at the end of the vocabulary will be 5,027; that means the array must
    be 5,027 + 1 in length.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单词被分配值，从1开始，直到单词的总数（例如，在这种情况下为5,027）。嵌入层需要为词汇表中从索引1到最大索引的每个单词分配一个向量表示。词汇表末尾的单词的索引将是5,027；这意味着数组的长度必须是5,027
    + 1。
- en: 'The output after array slicing and separating sentences into sequences of 50
    words per sequence must look like the following screenshot:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数组切片和将句子分隔成每个序列50个单词的序列后，输出应该如下截图所示：
- en: '![](img/00185.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00185.jpeg)'
- en: The `to_categorical()` function is used so that the model learns to predict
    the probability distribution for the next word.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`to_categorical()`函数，使模型学习预测下一个单词的概率分布。
- en: There's more...
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'More information on reshaping arrays in Python can be found at the following
    links:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在Python中重新整形数组的更多信息，请访问以下链接：
- en: '[https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)'
- en: '[https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/](https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/](https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/)'
- en: Training and saving the LSTM model
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和保存LSTM模型
- en: You can now train a statistical language model from the prepared data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以从准备好的数据中训练统计语言模型。
- en: 'The model that will be trained is a neural language model. It has a few unique
    characteristics:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 将要训练的模型是神经语言模型。它具有一些独特的特点：
- en: It uses a distributed representation for words so that different words with
    similar meanings will have a similar representation
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用分布式表示来表示单词，使得具有相似含义的不同单词具有相似的表示
- en: It learns the representation at the same time as learning the model
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在学习模型的同时学习表示
- en: It learns to predict the probability for the next word using the context of
    the previous 50 words
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它学会使用前50个单词的上下文来预测下一个单词的概率
- en: Specifically, you will use an Embedding Layer to learn the representation of
    words, and a **Long Short-Term Memory** (**LSTM**) recurrent neural network to
    learn to predict words based on their context.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，您将使用嵌入层来学习单词的表示，以及**长短期记忆**（**LSTM**）递归神经网络来学习根据上下文预测单词。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The learned embedding needs to know the size of the vocabulary and the length
    of input sequences as previously discussed. It also has a parameter to specify
    how many dimensions will be used to represent each word. That is the size of the
    embedding vector space.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，学习的嵌入需要知道词汇表的大小和输入序列的长度。它还有一个参数，用于指定将用于表示每个单词的维度的数量。这就是嵌入向量空间的大小。
- en: Common values are 50, 100, and 300\. We will use 100 here, but consider testing
    smaller or larger values and evaluating metrics for those values.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 常见值为50、100和300。我们将在这里使用100，但考虑测试更小或更大的值，并评估这些值的指标。
- en: 'The network will be comprised of the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 网络将由以下组成：
- en: Two LSTM hidden layers with 200 memory cells each. More memory cells and a deeper
    network may achieve better results.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个具有200个记忆单元的LSTM隐藏层。更多的记忆单元和更深的网络可能会取得更好的结果。
- en: A dropout layer with a dropout of 0.3 or 30%, which will aid the network to
    depend less on each neuron/unit and reduce overfitting the data.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个dropout层，dropout率为0.3或30%，这将帮助网络减少对每个神经元/单元的依赖，并减少过拟合数据。
- en: A dense fully connected layer with 200 neurons connects to the LSTM hidden layers
    to interpret the features extracted from the sequence.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有200个神经元的全连接层连接到LSTM隐藏层，以解释从序列中提取的特征。
- en: The output layer, which predicts the next word as a single vector of the size
    of the vocabulary with a probability for each word in the vocabulary.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层预测下一个单词，作为词汇表大小的单个向量，其中每个单词在词汇表中都有一个概率。
- en: A softmax classifier is used in the second dense or fully connected layer to
    ensure the outputs have the characteristics of normalized probabilities (such
    as between 0 and 1).
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二个密集或全连接层中使用softmax分类器，以确保输出具有归一化概率的特性（例如在0和1之间）。
- en: How to do it...
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The model is defined using the following commands and is also illustrated in
    the following screenshot:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令定义模型，并在以下截图中进行说明：
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/00186.jpeg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00186.jpeg)'
- en: Print the model summary just to ensure that the model is constructed as intended.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模型摘要，以确保模型按预期构建。
- en: 'Compile the model, specifying the categorical cross entropy loss needed to
    fit the model. The number of epochs is set to 75 and the model is trained in mini
    batches with a batch size of 250\. This is done using the following commands:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型，指定需要拟合模型的分类交叉熵损失。将epochs数设置为75，并使用批量大小为250的小批量训练模型。使用以下命令完成：
- en: '[PRE13]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the preceding commands is illustrated in the following screenshot:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令的输出在以下截图中进行说明：
- en: '![](img/00187.jpeg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00187.jpeg)'
- en: 'Once the model is done compiling, it is saved using the following commands:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型编译完成后，使用以下命令保存：
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/00188.jpeg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00188.jpeg)'
- en: How it works...
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The model is built using the `Sequential()` function in the Keras framework.
    The first layer in the model is an embedding layer that takes in the vocabulary
    size, vector dimension, and the input sequence length as its arguments.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型是使用Keras框架中的`Sequential()`函数构建的。模型中的第一层是一个嵌入层，它以词汇量、向量维度和输入序列长度作为参数。
- en: The next two layers are LSTM layers with 200 memory cells each. More memory
    cells and a deeper network can be experimented with to check if it improves accuracy.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的两层是每个具有200个内存单元的LSTM层。可以尝试使用更多内存单元和更深的网络来检查是否可以提高准确性。
- en: The next layer is a dropout layer with a dropout probability of 30%, which means
    that there is a 30% chance a certain memory unit is not used during training.
    This prevents overfitting of data. Again, the dropout probabilities can be played
    with and tuned accordingly.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的一层是一个丢弃层，丢弃概率为30%，这意味着在训练过程中某个记忆单元不被使用的概率为30%。这可以防止数据过拟合。同样，可以调整和调优丢弃概率。
- en: The final two layers are two fully connected layers. The first one has a `relu`
    activation function and the second has a softmax classifier. The model summary
    is printed to check whether the model is built according to requirements.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后两层是两个全连接层。第一个具有`relu`激活函数，第二个具有softmax分类器。打印模型摘要以检查模型是否按要求构建。
- en: Notice that in this case, the total number of trainable parameters are 2,115,228\.
    The model summary also shows the number of parameters that will be trained by
    each layer in the model.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，可训练参数的总数为2,115,228。模型摘要还显示了模型中每个层将被训练的参数数量。
- en: The model is trained in mini batches of 250 over 75 epochs, in our case, to
    minimize training time. Increasing the number of epochs to over 100 and utilizing
    smaller batches while training greatly improves the model's accuracy while simultaneously
    reducing loss.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的案例中，模型是在75个时期的小批量中训练的，以最小化训练时间。将时期数增加到100以上，并在训练时使用更小的批量，可以大大提高模型的准确性，同时减少损失。
- en: During training, you will see a summary of performance, including the loss and
    accuracy evaluated from the training data at the end of each batch update. In
    our case, after running the model for 75 epochs, we obtained an accuracy of close
    to 40%.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中，您将看到性能摘要，包括每个批次更新结束时从训练数据评估的损失和准确性。在我们的案例中，运行了75个时期后，我们获得了接近40%的准确性。
- en: The aim of the model is not to remember the text with 100% accuracy, but rather
    to capture the properties of the input text, such as long-term dependencies and
    structures that exist in natural language and sentences.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的目标不是以100%的准确性记住文本，而是捕捉输入文本的属性，如自然语言和句子中存在的长期依赖关系和结构。
- en: The model, after it is done training, is saved in the working directory named
    `junglebook_trained.h5`.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练完成后，模型将保存在名为`junglebook_trained.h5`的工作目录中。
- en: We also require the mapping of words to integers when the model is later loaded
    into memory to make predictions. This is present in the `Tokenizer` object, which
    is also saved using the `dump ()` function in the `Pickle` library.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当模型稍后加载到内存中进行预测时，我们还需要单词到整数的映射。这在`Tokenizer`对象中存在，并且也使用`Pickle`库中的`dump()`函数保存。
- en: There's more...
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Jason Brownlee''s blogs on Machine Learning Mastery have a lot of useful information
    on developing, training, and tuning machine learning models for natural language
    processing. They can be found at the following links:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Jason Brownlee在Machine Learning Mastery的博客上有很多关于开发、训练和调整自然语言处理机器学习模型的有用信息。可以在以下链接找到：
- en: '[https://machinelearningmastery.com/deep-learning-for-nlp/](https://machinelearningmastery.com/deep-learning-for-nlp/)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/deep-learning-for-nlp/](https://machinelearningmastery.com/deep-learning-for-nlp/)'
- en: '[https://machinelearningmastery.com/lstms-with-python/](https://machinelearningmastery.com/lstms-with-python/)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/lstms-with-python/](https://machinelearningmastery.com/lstms-with-python/)'
- en: '[https://machinelearningmastery.com/blog/](https://machinelearningmastery.com/deep-learning-for-nlp/)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/blog/](https://machinelearningmastery.com/deep-learning-for-nlp/)'
- en: See also
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Further information about different keras layers and other functions used in
    this section can be found at the following links:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本节中使用的不同keras层和其他函数的更多信息可以在以下链接找到：
- en: '[https://keras.io/models/sequential/](https://keras.io/models/sequential/)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/models/sequential/](https://keras.io/models/sequential/)'
- en: '[https://docs.python.org/2/library/pickle.html](https://docs.python.org/2/library/pickle.html)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/2/library/pickle.html](https://docs.python.org/2/library/pickle.html)'
- en: '[https://keras.io/optimizers/](https://keras.io/optimizers/)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/optimizers/](https://keras.io/optimizers/)'
- en: '[https://keras.io/models/model/](https://keras.io/models/model/)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/models/model/](https://keras.io/models/model/)'
- en: Generating similar text using the model
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用模型生成类似的文本
- en: Now that you have a trained language model, it can be used. In this case, you
    can use it to generate new sequences of text that have the same statistical properties
    as the source text. This is not practical, at least not for this example, but
    it gives a concrete example of what the language model has learned.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了一个经过训练的语言模型，可以使用它。在这种情况下，您可以使用它来生成具有与源文本相同统计特性的新文本序列。至少对于这个例子来说，这并不实际，但它给出了语言模型学到了什么的一个具体例子。
- en: Getting ready
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Begin by loading the training sequences again. You may do so by using the `load_document()`
    function, which we developed initially. This is done by using the following code:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先重新加载训练序列。您可以使用我们最初开发的`load_document()`函数来实现。通过以下代码实现：
- en: '[PRE15]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of the preceding code is illustrated in the following screenshot:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下截图所示：
- en: '![](img/00189.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00189.jpeg)'
- en: Note that the input filename is now `'junglebook_sequences.txt'`, which will
    load the saved training sequences into the memory. We need the text so that we
    can choose a source sequence as input to the model for generating a new sequence
    of text.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，输入文件名现在是`'junglebook_sequences.txt'`，这将把保存的训练序列加载到内存中。我们需要文本，以便我们可以选择一个源序列作为模型的输入，以生成新的文本序列。
- en: The model will require 50 words as input.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型将需要50个单词作为输入。
- en: 'Later, the expected length of input needs to be specified. This can be determined
    from the input sequences by calculating the length of one line of the loaded data
    and subtracting 1 for the expected output word that is also on the same line,
    as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，需要指定输入的预期长度。这可以通过计算加载的数据的一行的长度并减去1来从输入序列中确定，因为预期的输出单词也在同一行上，如下所示：
- en: '`sequence_length = len(lines[0].split()) - 1`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`sequence_length = len(lines[0].split()) - 1`'
- en: 'Next, load the trained and saved model into memory by executing the following
    commands:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过执行以下命令将训练和保存的模型加载到内存中：
- en: '[PRE16]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first step in generating text is preparing a seed input. Select a random
    line of text from the input text for this purpose. Once selected, print it so
    that you have some idea of what was used. This is done as follows:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成文本的第一步是准备种子输入。为此目的，从输入文本中随机选择一行文本。一旦选择，打印它以便您对使用的内容有一些了解。操作如下：
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00190.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00190.jpeg)'
- en: How to do it...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'You are now ready to generate new words, one at a time. First, encode the seed
    text to integers using the same tokenizer that was used when training the model,
    which is done using the following code:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以逐个生成新单词。首先，使用训练模型时使用的相同标记器将种子文本编码为整数，操作如下：
- en: '`encoded = tokenizer.texts_to_sequences([seed_text])[0]`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`encoded = tokenizer.texts_to_sequences([seed_text])[0]`'
- en: '![](img/00191.jpeg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00191.jpeg)'
- en: 'The model can predict the next word directly by calling `model.predict_classes()`,
    which will return the index of the word with the highest probability:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型可以通过调用`model.predict_classes()`直接预测下一个单词，这将返回具有最高概率的单词的索引：
- en: '[PRE18]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Look up the index in the Tokenizers mapping to get the associated word, as
    shown in the following code:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找标记器映射中的索引以获取相关联的单词，如下所示：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Append this word to the seed text and repeat the process. Importantly, the
    input sequence is going to get too long. We can truncate it to the desired length
    after the input sequence has been encoded to integers. Keras provides the `pad_sequences()` function
    which we can use to perform this truncation, as follows:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个单词附加到种子文本中并重复这个过程。重要的是，输入序列将变得太长。在将输入序列编码为整数后，我们可以将其截断为所需的长度。Keras提供了`pad_sequences()`函数，我们可以使用它来执行这种截断，如下所示：
- en: '[PRE20]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Wrap all of this into a function called `generate_sequence()` that takes as
    input the model, the tokenizer, the input sequence length, the seed text, and
    the number of words to generate. It then returns a sequence of words generated
    by the model. You may use the following code to do so:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有这些封装到一个名为`generate_sequence()`的函数中，该函数以模型、标记器、输入序列长度、种子文本和要生成的单词数量作为输入。然后，它返回模型生成的一系列单词。您可以使用以下代码来实现：
- en: '[PRE21]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/00192.jpeg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00192.jpeg)'
- en: How it works...
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'We are now ready to generate a sequence of new words, given that we have some
    seed text :'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备生成一系列新单词，假设我们有一些种子文本：
- en: 'Start by loading the model into memory again using the following command:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先使用以下命令将模型重新加载到内存中：
- en: '[PRE22]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, load the tokenizer by typing the following command:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过输入以下命令加载标记器：
- en: '[PRE23]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Select a seed text randomly by using the following command:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用以下命令随机选择一个种子文本：
- en: '[PRE24]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, a new sequence is generated by using the following command:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过使用以下命令生成一个新序列：
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'On printing the generated sequence, you will see an output similar to the one
    shown in the following screenshot:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在打印生成的序列时，您将看到类似于以下屏幕截图的输出：
- en: '![](img/00193.jpeg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00193.jpeg)'
- en: 'The model first prints 50 words of the random seed text followed by 50 words
    of the generated text. In this case, the random seed text is as follows:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型首先打印随机种子文本的50个单词，然后打印生成文本的50个单词。在这种情况下，随机种子文本如下：
- en: '*Baskets of dried grass and put grasshoppers in them or catch two praying mantises
    and make them fight or string a necklace of red and black jungle nuts or watch
    a lizard basking on a rock or a snake hunting a frog near the wallows then they
    sing long long songs*'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*篮子里装满了干草，放入蚱蜢，或者捉两只螳螂让它们打架，或者串一串红色和黑色的丛林果仁做成项链，或者看蜥蜴在岩石上晒太阳，或者蛇在泥坑附近捕捉青蛙，然后它们唱着长长的歌*'
- en: 'The 50 words of text generated by the model, in this case, are as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型生成的50个单词如下：
- en: '*with odd native quavers at the end of the review and the hyaena whom he had
    seen the truth they feel twitched to the noises round him for a picture of the
    end of the ravine and snuffing bitten and best of the bulls at the dawn is a native*'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*在评论结束时有奇怪的本地颤音，他看到的鬣狗，他们感到被拉到他周围的噪音，为了峡谷末端的画面，嗅着被咬的和最好的公牛在黎明时是本地人*'
- en: Note how the model outputs a sequence of random words it generated based on
    what it learned from the input text. You will also notice that the model does
    a reasonably good job of mimicking the input text and generating its own stories.
    Though the text does not make much sense, it gives valuable insight into how the
    model learns to place statistically similar words next to each other.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意模型输出了一系列随机单词，这些单词是根据它从输入文本中学到的内容生成的。您还会注意到，模型在模仿输入文本并生成自己的故事方面做得相当不错。尽管文本没有太多意义，但它为我们提供了宝贵的见解，即模型如何学习将统计上相似的单词放在一起。
- en: There's more...
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Upon changing the random seed that was set, the output generated by the network
    also changes. You may not get the exact same output text as the preceding example,
    but it will be very similar to the input used to train the model.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改设置的随机种子后，网络生成的输出也会发生变化。您可能无法获得与前面示例完全相同的输出文本，但它将与用于训练模型的输入非常相似。
- en: 'The following are some screenshots of different results that were obtained
    by running the generated text piece multiple times:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是通过多次运行生成文本片段获得的不同结果的一些屏幕截图：
- en: '![](img/00194.jpeg)![](img/00195.jpeg)![](img/00196.jpeg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00194.jpeg)![](img/00195.jpeg)![](img/00196.jpeg)'
- en: 'The model even generates its own version of the project Gutenberg license,
    as can be seen in the following screenshot:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型甚至生成了自己版本的项目古腾堡许可证，如下屏幕截图所示：
- en: '![](img/00197.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00197.jpeg)'
- en: The model's accuracy can be improved to about 60% by increasing the number of
    epochs from about 100 to 200\. Another method to increase the learning is by training
    the model in mini batches of about 50 and 100\. Try to play around with the different
    hyperparameters and activation functions to see what affects the results in the
    best possible way.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的准确性可以通过将时代数量从大约100增加到200来提高到约60％。另一种增加学习的方法是通过以大约50和100的小批量训练模型。尝试玩弄不同的超参数和激活函数，以查看以最佳方式影响结果的方法。
- en: The model may also be made denser by including more LSTM and dropout layers
    while defining the model. However, know that it will only increase the training
    time if the model is more complex and runs over more epochs.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还可以通过在定义模型时包含更多的LSTM和丢失层来使模型更加密集。但是，请注意，如果模型更复杂并且运行的时代更长，它只会增加训练时间。
- en: After much experimentation, the ideal batch size was found to be between 50
    to 100, and the ideal number of epochs to train the model was determined to be
    between 100 and 200.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过大量实验，发现理想的批处理大小在50到100之间，并且确定了训练模型的理想时代数量在100到200之间。
- en: There is no definitive way of performing the preceding task. You can also experiment
    with different text inputs to the model such as tweets, customer reviews, or HTML
    code.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行前述任务的确切方法并不存在。您还可以尝试使用不同的文本输入到模型，例如推文，客户评论或HTML代码。
- en: Some of the other tasks that can be performed include using a simplified vocabulary
    (such as with all the stopwords removed) to further enhance the unique words in
    the dictionary; tuning the size of the embedding layer and the number of memory
    cells in the hidden layers; and extending the model to use a pre-trained model
    such as Google's Word2Vec (pre-trained word model) to see whether it results in
    a better model.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还可以执行一些其他任务，包括使用简化的词汇表（例如删除所有停用词）以进一步增强字典中的唯一单词；调整嵌入层的大小和隐藏层中的记忆单元数量；并扩展模型以使用预训练模型，例如Google的Word2Vec（预训练词模型），以查看是否会产生更好的模型。
- en: See also
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'More information about the various functions and libraries used in the final
    section of the chapter can be found by visiting the following links:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本章最后一节中使用的各种函数和库的更多信息，请访问以下链接：
- en: '[https://keras.io/preprocessing/sequence/](https://keras.io/preprocessing/sequence/)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/preprocessing/sequence/](https://keras.io/preprocessing/sequence/)'
- en: '[https://wiki.python.org/moin/UsingPickle](https://wiki.python.org/moin/UsingPickle)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://wiki.python.org/moin/UsingPickle](https://wiki.python.org/moin/UsingPickle)'
- en: '[https://docs.python.org/2/library/random.html](https://docs.python.org/2/library/random.html)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/2/library/random.html](https://docs.python.org/2/library/random.html)'
- en: '[https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model)'
