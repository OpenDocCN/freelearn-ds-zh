- en: Chapter 8. Spark Databricks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。Spark Databricks
- en: Creating a big data analytics cluster, importing data, and creating ETL streams
    to cleanse and process the data are hard to do, and also expensive. The aim of
    Databricks is to decrease the complexity and make the process of cluster creation,
    and data processing easier. They have created a cloud-based platform, based on
    Apache Spark that automates cluster creation, and simplifies data import, processing,
    and visualization. Currently, the storage is based upon AWS but, in the future,
    they plan to expand to other cloud providers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 创建大数据分析集群，导入数据，创建ETL流以清洗和处理数据是困难且昂贵的。Databricks的目标是降低复杂性，使集群创建和数据处理过程更加简单。他们创建了一个基于Apache
    Spark的云平台，自动化了集群创建，并简化了数据导入、处理和可视化。目前，存储基于AWS，但未来他们计划扩展到其他云提供商。
- en: 'The same people who designed Apache Spark are involved in the Databricks system.
    At the time of writing this book, the service was only accessible via registration.
    I have been offered a 30-day trial period. Over the next two chapters, I will
    examine the service, and its components, and offer some sample code to show how
    it works. This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 设计Apache Spark的同一批人参与了Databricks系统。在撰写本书时，该服务只能通过注册访问。我获得了30天的试用期。在接下来的两章中，我将检查该服务及其组件，并提供一些示例代码来展示其工作原理。本章将涵盖以下主题：
- en: Installing Databricks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Databricks
- en: AWS configuration
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS配置
- en: Account management
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帐户管理
- en: The menu system
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 菜单系统
- en: Notebooks and folders
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本和文件夹
- en: Importing jobs via libraries
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过库导入作业
- en: Development environments
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发环境
- en: Databricks tables
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks表
- en: The Databricks DbUtils package
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks DbUtils包
- en: Given that this book is provided in a static format, it will be difficult to
    fully examine functionality such as streaming.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于本书以静态格式提供，完全检查流式等功能将会很困难。
- en: Overview
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: The Databricks service, available at the [https://databricks.com/](https://databricks.com/)
    website, is based upon the idea of a cluster. This is similar to a Spark cluster,
    which has already been examined and used in previous chapters. It contains a master,
    workers, and executors. However, the configuration and the size of the cluster
    are automated, depending upon the amount of memory that you specify. Features
    such as security, isolation, process monitoring, and resource management are all
    automatically managed for you. If you have an immediate requirement for a Spark-based
    cluster using 200 GB of memory, for a short period of time, this service can be
    used to dynamically create it, and process your data. You can terminate the cluster
    to reduce your costs when the processing is finished.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks服务，可在[https://databricks.com/](https://databricks.com/)网站上获得，基于集群的概念。这类似于Spark集群，在之前的章节中已经进行了检查和使用。它包含一个主节点、工作节点和执行器。但是，集群的配置和大小是自动化的，取决于您指定的内存量。诸如安全性、隔离、进程监控和资源管理等功能都会自动为您管理。如果您有一个短时间内需要使用200GB内存的基于Spark的集群，这项服务可以动态创建它，并处理您的数据。处理完成后，您可以终止集群以减少成本。
- en: Within a cluster, the idea of a Notebook is introduced, along with a location
    for you to create scripts and run programs. Folders can be created within Notebooks,
    which can be based upon Scala, Python, or SQL. Jobs can be created to execute
    the functionality, and can be called from the Notebook code or the imported libraries.
    Notebooks can call Notebook functionality. Also, the functionality is provided
    to schedule jobs, based on time or event.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中，引入了笔记本的概念，以及一个位置供您创建脚本和运行程序。可以在笔记本中创建基于Scala、Python或SQL的文件夹。可以创建作业来执行功能，并可以从笔记本代码或导入的库中调用。笔记本可以调用笔记本功能。此外，还提供了根据时间或事件安排作业的功能。
- en: This provides you with a feel of what the Databricks service provides. The following
    sections will explain each major item that has been introduced. Please keep in
    mind that what is presented here is new and evolving. Also, I used the AWS US
    East (North Virginia) region for this demonstration, as the Asia Sydney region
    currently has limitations that caused the Databricks install to fail.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这为您提供了Databricks服务提供的感觉。接下来的章节将解释每个引入的主要项目。请记住，这里呈现的内容是新的并且正在发展。此外，我在这个演示中使用了AWS
    US East (North Virginia)地区，因为亚洲悉尼地区目前存在限制，导致Databricks安装失败。
- en: Installing Databricks
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Databricks
- en: 'In order to create this demonstration, I used the AWS offer of a year''s free
    access, which was available at [http://aws.amazon.com/free/](http://aws.amazon.com/free/).
    This has limitations such as 5 GB of S3 storage, and 750 hours of **Amazon Elastic
    Compute Cloud** (**EC2**), but it allowed me low-cost access and reduced my overall
    EC2 costs. The AWS account provides the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建这个演示，我使用了AWS提供的一年免费访问，该访问可在[http://aws.amazon.com/free/](http://aws.amazon.com/free/)上获得。这有一些限制，比如5GB的S3存储和750小时的Amazon
    Elastic Compute Cloud (EC2)，但它让我以较低成本访问并减少了我的整体EC2成本。AWS账户提供以下内容：
- en: An account ID
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帐户ID
- en: An access Key ID
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个访问密钥ID
- en: A secret access Key
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个秘密访问密钥
- en: These items of information are used by Databricks to access your AWS storage,
    install the Databricks systems, and create the cluster components that you specify.
    From the moment of the install, you begin to incur AWS EC2 costs, as the Databricks
    system uses at least two running instances without any clusters. Once you have
    successfully entered your AWS and billing information, you will be prompted to
    launch the Databricks cloud.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息项目被Databricks用来访问您的AWS存储，安装Databricks系统，并创建您指定的集群组件。从安装开始，您就开始产生AWS EC2成本，因为Databricks系统使用至少两个运行实例而没有任何集群。一旦您成功输入了AWS和计费信息，您将被提示启动Databricks云。
- en: '![Installing Databricks](img/B01989_08_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![安装Databricks](img/B01989_08_01.jpg)'
- en: 'Having done this, you will be provided with a URL to access your cloud, an
    admin account, and password. This will allow you to access the Databricks web-based
    user interface, as shown in the following screenshot:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些操作后，您将获得一个URL来访问您的云、一个管理员账户和密码。这将允许您访问Databricks基于Web的用户界面，如下面的截图所示：
- en: '![Installing Databricks](img/B01989_08_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![安装Databricks](img/B01989_08_02.jpg)'
- en: This is the welcome screen. It shows the menu bar at the top of the image, which,
    from left to right, contains the menu, search, help, and account icons. While
    using the system, there may also be a clock-faced icon that shows the recent activity.
    From this single interface, you may search through help screens, and usage examples
    before creating your own clusters and code.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是欢迎界面。它显示了图像顶部的菜单栏，从左到右依次包括菜单、搜索、帮助和账户图标。在使用系统时，还可能有一个显示最近活动的时钟图标。通过这个单一界面，您可以在创建自己的集群和代码之前搜索帮助屏幕和使用示例。
- en: AWS billing
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS计费
- en: 'Please note that, once you have the Databricks system installed, you will start
    incurring the AWS EC2 storage costs. Databricks attempts to minimize your costs
    by keeping EC2 resources active for a full charging period. For instance if you
    terminate a Databricks cluster the cluster-based EC2 instances will still exist
    for the hour in which AWS bills for them. In this way, Databricks can reuse them
    if you create a new cluster. The following screenshot shows that, although I am
    using a free AWS account, and though I have carefully reduced my resource usage,
    I have incurred AWS EC2 costs in a short period of time:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一旦安装了Databricks系统，您将开始产生AWS EC2存储成本。Databricks试图通过保持EC2资源活动来最小化您的成本，以便进行完整的计费周期。例如，如果终止Databricks集群，基于集群的EC2实例仍将存在于AWS为其计费的一个小时内。通过这种方式，如果您创建一个新的集群，Databricks可以重用它们。下面的截图显示，尽管我正在使用一个免费的AWS账户，并且我已经仔细减少了我的资源使用，但我在短时间内产生了AWS
    EC2成本：
- en: '![AWS billing](img/B01989_08_03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![AWS计费](img/B01989_08_03.jpg)'
- en: You need to be aware of the Databricks clusters that you create, and understand
    that, while they exist and are used, AWS costs are being incurred. Only keep the
    clusters that you really require, and terminate any others.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要了解您创建的Databricks集群，并了解，当它们存在并被使用时，将产生AWS成本。只保留您真正需要的集群，并终止其他任何集群。
- en: In order to examine the Databricks data import functionality, I also created
    an AWS S3 bucket, and uploaded data files to it. This will be explained later
    in this chapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查Databricks数据导入功能，我还创建了一个AWS S3存储桶，并将数据文件上传到其中。这将在本章后面进行解释。
- en: Databricks menus
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks菜单
- en: By selecting the top-left menu icon on the Databricks web interface, it is possible
    to expand the menu system. The following screenshot shows the top-level menu options,
    as well as the **Workspace** option, expanded to a folder hierarchy of `/folder1/folder2/`.
    Finally, it shows the actions that can be carried out on `folder2`, that is, creating
    a notebook, creating a dashboard, and more.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择Databricks Web界面上的左上角菜单图标，可以展开菜单系统。下面的截图显示了顶级菜单选项，以及**工作区**选项，展开到`/folder1/folder2/`的文件夹层次结构。最后，它显示了可以在`folder2`上执行的操作，即创建一个笔记本、创建一个仪表板等。
- en: '![Databricks menus](img/B01989_08_04.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Databricks菜单](img/B01989_08_04.jpg)'
- en: All of these actions will be expanded in future sections. The next section will
    examine account management, before moving on to clusters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些操作将在以后的章节中扩展。下一节将介绍账户管理，然后转到集群。
- en: Account management
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 账户管理
- en: 'Account management is quite simplified within Databricks. There is a default
    Administrator account and subsequent accounts can be created, but you need to
    know the Administrator password to do so. Passwords need to be more than eight
    characters long; they should contain at least one digit, one upper case character,
    and one non-alphanumeric character. **Account** options can be accessed from the
    top-right menu option, shown in the following screenshot:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在Databricks中，账户管理非常简化。有一个默认的管理员账户，可以创建后续账户，但您需要知道管理员密码才能这样做。密码需要超过八个字符；它们应该包含至少一个数字、一个大写字母和一个非字母数字字符。**账户**选项可以从右上角的菜单选项中访问，如下面的截图所示：
- en: '![Account management](img/B01989_08_05.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: 账户管理
- en: 'This also allows the user to logout. By selecting the account setting, you
    can change your password. By selecting the **Accounts** menu option, an **Accounts**
    list is generated. There, you will find an option to **Add Account**, and each
    account can be deleted via an **X** option on each account line, as shown in the
    following screenshot:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这也允许用户注销。通过选择账户设置，您可以更改密码。通过选择**账户**菜单选项，将生成一个**账户**列表。在那里，您将找到一个**添加账户**的选项，并且每个账户行都可以通过每个账户行上的**X**选项进行删除，如下面的截图所示：
- en: '![Account management](img/B01989_08_06.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![账户管理](img/B01989_08_06.jpg)'
- en: 'It is also possible to reset the account passwords from the accounts list.
    Selecting the **Add Account** option creates a new account window that requires
    an email address, a full name, the administrator password, and the user''s password.
    So, if you want to create a new user, you need to know your Databricks instance
    Administrator password. You must also follow the rules for new passwords, which
    are as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以从账户列表重置账户密码。选择**添加账户**选项会创建一个新的账户窗口，需要一个电子邮件地址、全名、管理员密码和用户密码。因此，如果您想创建一个新用户，您需要知道您的Databricks实例管理员密码。您还必须遵循新密码的规则，如下所示：
- en: Minimum of eight characters
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少八个字符
- en: 'Must contain at least one digit in the range: 0-9'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须包含0-9范围内的至少一个数字
- en: 'Must contain at least one upper case character in the range: A-Z'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须包含A-Z范围内的至少一个大写字母
- en: 'Must contain at least one non-alphanumeric character: !@#$%![Account management](img/B01989_08_07.jpg)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须包含至少一个非字母数字字符：!@#$%![账户管理](img/B01989_08_07.jpg)
- en: The next section will examine the **Clusters** menu option, and will enable
    you to manage your own Databricks Spark clusters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将介绍**集群**菜单选项，并使您能够管理自己的Databricks Spark集群。
- en: Cluster management
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群管理
- en: Selecting the **Clusters** menu option provides a list of your current Databricks
    clusters and their status. Of course, currently you have none. Selecting the **Add
    Cluster** option allows you to create one. Note that the amount of memory you
    specify determines the size of your cluster. There is a minimum of 54 GB required
    to create a cluster with a single master and worker. For each additional 54 GB
    specified, another worker is added.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 选择**集群**菜单选项会提供您当前的Databricks集群及其状态的列表。当然，当前您还没有。选择**添加集群**选项允许您创建一个。请注意，您指定的内存量决定了您的集群的大小。创建具有单个主节点和工作节点的集群需要至少54GB。对于每增加的54GB，将添加一个工作节点。
- en: '![Cluster management](img/B01989_08_08.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![集群管理](img/B01989_08_08.jpg)'
- en: The following screenshot is a concatenated image, showing a new cluster called
    `semclust1` being created and in a **Pending** state. While **Pending**, the cluster
    has no dashboard, and the cluster nodes are not accessible.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图是一个连接的图像，显示了一个名为`semclust1`的新集群正在创建中，处于**Pending**状态。在**Pending**状态下，集群没有仪表板，集群节点也无法访问。
- en: '![Cluster management](img/B01989_08_09.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![集群管理](img/B01989_08_09.jpg)'
- en: 'Once created the cluster memory is listed and it''s status changes from **Pending**
    to **Running**. A default dashboard has automatically been attached, and the Spark
    master and worker user interfaces can be accessed. It is important to note here
    that Databricks automatically starts and manages the cluster processes. There
    is also an **Option** column to the right of this display that offers the ability
    to **Configure**, **Restart**, or **Terminate** a cluster as shown in the following
    screenshot:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 创建后，集群内存会被列出，并且其状态会从**Pending**变为**Running**。默认情况下会自动附加一个仪表板，并且可以访问Spark主节点和工作节点用户界面。这里需要注意的是，Databricks会自动启动和管理集群进程。在显示的右侧还有一个**Option**列，提供了**配置**、**重启**或**终止**集群的能力，如下面的截图所示：
- en: '![Cluster management](img/B01989_08_10.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![集群管理](img/B01989_08_10.jpg)'
- en: By reconfiguring a cluster, you can change its size. By adding more memory,
    you can add more workers. The following screenshot shows a cluster, created at
    the default size of 54 GB, having its memory extended to `108` GB.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新配置集群，可以改变其大小。通过增加内存，可以增加工作节点。下面的截图显示了一个集群，创建时默认大小为54GB，其内存扩展到了`108`GB。
- en: '![Cluster management](img/B01989_08_11.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![集群管理](img/B01989_08_11.jpg)'
- en: Terminating a cluster removes it, and it cannot be recovered. So, you need to
    be sure that deletion is the correct course of action. Databricks prompts you
    to confirm your action before the termination actually takes place.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 终止集群会将其删除，无法恢复。因此，您需要确保删除是正确的操作。在终止实际发生之前，Databricks会提示您确认您的操作。
- en: '![Cluster management](img/B01989_08_12.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![集群管理](img/B01989_08_12.jpg)'
- en: 'It takes time for a cluster to be both, created and terminated. During termination,
    the cluster is marked with an orange banner, and a state of **Terminating**, as
    shown in the following screenshot:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和终止集群都需要时间。在终止期间，集群会被标记为橙色横幅，并显示**终止**状态，如下面的截图所示：
- en: '![Cluster management](img/B01989_08_13.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![集群管理](img/B01989_08_13.jpg)'
- en: Note that the cluster type in the previous screenshot is shown to be **On-demand**.
    When creating a cluster, it is possible to select a check box called **Use spot
    instances to create a spot cluster**. These clusters are cheaper than the on-demand
    clusters, as they bid for a cheaper AWS spot price. However, they can be slower
    to start than the on-demand clusters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面截图中的集群类型显示为**按需**。创建集群时，可以选择一个名为**使用竞价实例创建竞价集群**的复选框。这些集群比按需集群更便宜，因为它们出价更低的AWS竞价。但是，它们启动可能比按需集群慢。
- en: The Spark user interfaces are the same as those you would expect on a non-Databricks
    Spark cluster. You can examine workers, executors, configuration, and log files.
    As you create clusters, they will be added to your cluster list. One of the clusters
    will be used as the cluster where the dashboards are run. This can be changed
    by using the **Make Dashboard Cluster** option. As you add libraries and Notebooks
    to your cluster, the cluster details entry will be updated with a count of the
    numbers added.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Spark用户界面与您在非Databricks Spark集群上期望的一样。您可以检查工作节点、执行器、配置和日志文件。创建集群时，它们将被添加到您的集群列表中。其中一个集群将被用作运行仪表板的集群。可以通过使用**创建仪表板集群**选项来更改这一点。当您向集群添加库和笔记本时，集群详细信息条目将更新为添加的数量。
- en: The only thing that I would say about the Databricks Spark user interface option
    at this time, because it is familiar, is that it displays the Spark version that
    is used. The following screenshot, extracted from the master user interface, shows
    that the Spark version being used (1.3.0) is very up-to-date. At the time of writing,
    the latest Apache Spark release was 1.3.1, dated 17 April, 2015.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在唯一想说的关于Databricks Spark用户界面选项，因为它很熟悉，就是它显示了使用的Spark版本。下面的截图从主用户界面中提取，显示了正在使用的Spark版本（1.3.0）非常新。在撰写本文时，最新的Apache
    Spark版本是1.3.1，日期为2015年4月17日。
- en: '![Cluster management](img/B01989_08_14.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![集群管理](img/B01989_08_14.jpg)'
- en: The next section will examine Databricks Notebooks and folders—how to create
    them, and how they can be used.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将介绍Databricks笔记本和文件夹——如何创建它们以及它们的用途。
- en: Notebooks and folders
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 笔记本和文件夹
- en: A Notebook is a special type of Databricks folder that can be used to create
    Spark scripts. Notebooks can call the Notebook scripts to create a hierarchy of
    functionality. When created, the type of Notebook must be specified (Python, Scala,
    or SQL), and a cluster can then specify that the Notebook functionality can be
    run against it. The following screenshot shows the Notebook creation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本是一种特殊类型的Databricks文件夹，可用于创建Spark脚本。笔记本可以调用笔记本脚本来创建功能层次结构。创建时，必须指定笔记本的类型（Python、Scala或SQL），然后可以指定集群可以运行笔记本功能。下面的截图显示了笔记本的创建。
- en: '![Notebooks and folders](img/B01989_08_15.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本和文件夹](img/B01989_08_15.jpg)'
- en: 'Note that a menu option, to the right of a Notebook session, allows the type
    of Notebook that is to be changed. The following example shows that a Python notebook
    can be changed to **Scala**, **SQL**, or **Markdown**:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，笔记本会话右侧的菜单选项允许更改笔记本的类型。下面的示例显示了Python笔记本可以更改为**Scala**、**SQL**或**Markdown**：
- en: '![Notebooks and folders](img/B01989_08_16.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本和文件夹](img/B01989_08_16.jpg)'
- en: Note that a Scala Notebook cannot be changed to Python, and a Python Notebook
    cannot be changed to Scala. The terms Python, Scala, and SQL are well understood
    as the development languages, however, **Markdown** is new. Markdown allows formatted
    documentation to be created from formatted commands in text. A simple reference
    can be found at [https://forums.databricks.com/static/markdown/help.html](https://forums.databricks.com/static/markdown/help.html).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Scala笔记本无法更改为Python，Python笔记本也无法更改为Scala。Python、Scala和SQL这些术语作为开发语言是众所周知的，然而，**Markdown**是新的。Markdown允许从文本中的格式化命令创建格式化文档。可以在[https://forums.databricks.com/static/markdown/help.html](https://forums.databricks.com/static/markdown/help.html)找到一个简单的参考。
- en: This means that formatted comments can be added to the Notebook session as scripts
    are created. Notebooks are further subdivided into cells, which contain the commands
    to be executed. Cells can be moved within a Notebook by hovering over the top-left
    corner, and dragging them into position. New cells can be inserted into a cell
    list within a Notebook.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在创建脚本时，格式化的注释可以添加到笔记本会话中。笔记本进一步细分为单元格，其中包含要执行的命令。可以通过悬停在左上角并将其拖放到位置来在笔记本中移动单元格。可以在笔记本中的单元格列表中插入新单元格。
- en: 'Also, using the `%sql` command, within a Scala or Python Notebook cell, allows
    SQL syntax to be used. Typically, the key combination of *Shift* + *Enter* causes
    text blocks in a Notebook or folder to be executed. Using the `%md` command allows
    Markdown comments to be added within a cell. Also, comments can be added to a
    Notebook cell. The menu options available at the top-right section of a Notebook
    cell, shown in the following screenshot, shows comment, as well as the minimize
    and maximize options:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在Scala或Python笔记本单元格中使用`%sql`命令允许使用SQL语法。通常，*Shift* + *Enter*的组合会导致笔记本或文件夹中的文本块被执行。使用`%md`命令允许在单元格内添加Markdown注释。还可以向笔记本单元格添加注释。在笔记本单元格的右上部分显示的菜单选项显示了注释以及最小化和最大化选项：
- en: '![Notebooks and folders](img/B01989_08_17.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本和文件夹](img/B01989_08_17.jpg)'
- en: Multiple web-based sessions may share a Notebook. The actions that occur within
    the Notebook will be populated to each web interface viewing it. Also, the Markdown
    and comment options can be used to enable communication between users to aid the
    interactive data investigation between a distributed group.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 多个基于Web的会话可以共享一个笔记本。在笔记本中发生的操作将被填充到查看它的每个Web界面中。此外，Markdown和注释选项可用于启用用户之间的通信，以帮助分布式组之间的交互式数据调查。
- en: '![Notebooks and folders](img/B01989_08_18.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本和文件夹](img/B01989_08_18.jpg)'
- en: 'The previous screenshot shows the header of a Notebook session for **notebook1**.
    It shows the Notebook name and type (**Scala**). It also shows the option to lock
    the Notebook to make it read only, as well as the option to detach it from its
    cluster. The following screenshot shows the creation of a folder within a Notebook
    workspace:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的屏幕截图显示了**notebook1**的笔记本会话的标题。它显示了笔记本名称和类型（**Scala**）。它还显示了将笔记本锁定以使其只读的选项，以及将其从其集群中分离的选项。下面的屏幕截图显示了在笔记本工作区内创建文件夹的过程：
- en: '![Notebooks and folders](img/B01989_08_19.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本和文件夹](img/B01989_08_19.jpg)'
- en: 'A drop-down menu, from the **Workspace** main menu option, allows for the creation
    of a folder—in this case, named `folder1`. The later sections will describe other
    options in this menu. Once created and selected, a drop-down menu from the new
    folder called `folder1` shows the actions associated with it in the following
    screenshot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从**工作区**主菜单选项的下拉菜单中，可以创建一个文件夹，例如`folder1`。稍后的部分将描述此菜单中的其他选项。创建并选择后，从名为`folder1`的新文件夹的下拉菜单中，显示了与其关联的操作，如下面的屏幕截图所示：
- en: '![Notebooks and folders](img/B01989_08_20.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本和文件夹](img/B01989_08_20.jpg)'
- en: So, a folder can be exported to a DBC archive. It can be locked, or cloned to
    create a copy. It can also be renamed, or deleted. Items can be imported into
    it; for instance, files, which will be explained by example later. Also, new notebooks,
    dashboards, libraries, and folders can be created within it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，文件夹可以导出为DBC存档。它可以被锁定，或者克隆以创建副本。也可以重命名或删除。可以将项目导入其中；例如，稍后将通过示例解释文件。还可以在其中创建新的笔记本、仪表板、库和文件夹。
- en: In the same way as actions can be carried out against a folder, a Notebook has
    a set of possible actions. The following screenshot shows the actions available
    via a drop-down menu for the Notebook called `notebook1`, which is currently attached
    to the running cluster called `semclust1`. It is possible to rename, delete, lock,
    or clone a Notebook. It is also possible to detach it from its current cluster,
    or attach it if it is detached. It is also possible to export the Notebook to
    a file, or a DBC archive.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与文件夹一样，笔记本也有一组可能的操作。下面的屏幕截图显示了通过下拉菜单可用的操作，用于名为`notebook1`的笔记本，它当前附加到名为`semclust1`的运行集群。可以重命名、删除、锁定或克隆笔记本。还可以将其从当前集群中分离，或者如果它被分离，则可以附加它。还可以将笔记本导出到文件或DBC存档。
- en: '![Notebooks and folders](img/B01989_08_21.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本和文件夹](img/B01989_08_21.jpg)'
- en: From the folder **Import** option, files can be imported to a folder. The following
    screenshot shows the file drop-option window that is invoked if this option is
    selected. It is possible to either drop a file onto the upload pane from the local
    server, or click on this pane to open a navigation browser to search the local
    server for files to upload.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从文件夹**导入**选项，文件可以导入到文件夹中。下面的屏幕截图显示了如果选择此选项将调用的文件拖放选项窗口。可以将文件拖放到本地服务器上的上传窗格上，也可以单击该窗格以打开导航浏览器，以搜索要上传的文件。
- en: '![Notebooks and folders](img/B01989_08_22.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本和文件夹](img/B01989_08_22.jpg)'
- en: Note that the files that are uploaded need to be of a specific type. The following
    screenshot shows the supported file types. This is a screenshot taken from the
    file browser when browsing for a file to upload. It also makes sense. The supported
    file types are Scala, SQL, and Python; as well as DBC archives and JAR file libraries.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 需要上传的文件需要是特定类型。以下截图显示了支持的文件类型。这是从文件浏览器中浏览要上传的文件时拍摄的截图。这也是有道理的。支持的文件类型包括Scala、SQL和Python；以及DBC存档和JAR文件库。
- en: '![Notebooks and folders](img/B01989_08_23.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本和文件夹](img/B01989_08_23.jpg)'
- en: Before leaving this section, it should also be noted that Notebooks and folders
    can be dragged and dropped to change their position. The next section will examine
    Databricks jobs and libraries via simple worked examples.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在离开这一部分之前，还应该注意到，可以拖放笔记本和文件夹来改变它们的位置。下一节将通过简单的示例来检查Databricks作业和库。
- en: Jobs and libraries
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作和图书馆
- en: 'Within Databricks, it is possible to import JAR libraries and run the classes
    in them on your clusters. I will create a very simple piece of Scala code to print
    out the first 100 elements of the Fibonacci series as `BigInt` values, locally
    on my Centos Linux server. I will compile my class into a JAR file using SBT,
    run it locally to check the result, and then run it on my Databricks cluster to
    compare the results. The code looks as following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在Databricks中，可以导入JAR库并在集群上运行其中的类。我将创建一个非常简单的Scala代码片段，以在我的Centos Linux服务器上本地打印出斐波那契数列的前100个元素作为`BigInt`值。我将使用SBT将我的类编译成一个JAR文件，在本地运行以检查结果，然后在我的Databricks集群上运行以比较结果。代码如下所示：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Not that the most elegant piece of code, or the best way to create Fibonacci,
    but I just want a sample JAR and class to use with Databricks. When run locally,
    I get the first 100 terms, which look as follows (I''ve clipped this data to save
    space):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是最优雅的代码片段，也不是创建斐波那契数列的最佳方式，但我只是想要一个用于Databricks的示例JAR和类。在本地运行时，我得到了前100个项，如下所示（我已剪辑了这些数据以节省空间）：
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The library that has been created is called `data-bricks_2.10-1.0.jar`. From
    my folder menu, I can create a new Library using the menu drop-down option. This
    allows me to specify the library source as a JAR file, name the new library, and
    load the library JAR file from my local server. The following screenshot shows
    an example of this process:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 已创建的库名为`data-bricks_2.10-1.0.jar`。从我的文件夹菜单中，我可以使用下拉菜单选项创建一个新的库。这允许我指定库源为一个JAR文件，命名新库，并从我的本地服务器加载库JAR文件。以下截图显示了这个过程的一个例子：
- en: '![Jobs and libraries](img/B01989_08_24.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![工作和图书馆](img/B01989_08_24.jpg)'
- en: 'When the library has been created, it can be attached to the cluster called
    `semclust1`, my Databricks cluster, using the **Attach** option. The following
    screenshot shows the new library in the process of attaching:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 创建库后，可以使用**附加**选项将其附加到名为`semclust1`的集群，即我的Databricks集群。以下截图显示了正在附加新库的过程：
- en: '![Jobs and libraries](img/B01989_08_25.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![工作和图书馆](img/B01989_08_25.jpg)'
- en: 'In the following example, a job called **job2** has been created by selecting
    the **jar** option on the **Task** item. For the job, the same JAR file has been
    loaded and the class `db_ex1` has been assigned to run in the library. The cluster
    has been specified as on-demand, meaning that a cluster will be created automatically
    to run the job. The **Active runs** section shows the job running in the following
    screenshot:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，通过在**任务**项目上选择**jar**选项创建了一个名为**job2**的作业。对于该作业，已加载了相同的JAR文件，并将类`db_ex1`分配到库中运行。集群已被指定为按需，这意味着将自动创建一个集群来运行作业。**活动运行**部分显示了作业在以下截图中的运行情况：
- en: '![Jobs and libraries](img/B01989_08_26.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![工作和图书馆](img/B01989_08_26.jpg)'
- en: Once run, the job is moved to the **Completed runs** section of the display.
    The following screenshot, for the same job, shows that it took `47` seconds to
    run, that it was launched manually, and that it succeeded.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，作业将移至显示的**已完成运行**部分。对于相同的作业，以下截图显示了它运行了`47`秒，是手动启动的，并且成功了。
- en: '![Jobs and libraries](img/B01989_08_27.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![工作和图书馆](img/B01989_08_27.jpg)'
- en: By selecting the run named **Run 1** in the previous screenshot, it is possible
    to see the run output. The following screenshot shows the same result as the local
    run, displayed from my local server execution. I have clipped the output text
    to make it presentable and readable on this page, but you can see that the output
    is the same.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在前面的截图中选择名为**Run 1**的运行，可以查看运行输出。以下截图显示了与本地运行相同的结果，显示了来自我的本地服务器执行的结果。我已剪辑输出文本以使其在此页面上呈现和阅读，但您可以看到输出是相同的。
- en: '![Jobs and libraries](img/B01989_08_28.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![工作和图书馆](img/B01989_08_28.jpg)'
- en: So, even from this very simple example, it is obvious that it is possible to
    develop applications remotely, and load them onto a Databricks cluster as JAR
    files in order to execute. However, each time a Databricks cluster is created
    on AWS EC2 storage, the Spark URL changes, so the application must not hard-code
    details such as the Spark master URL. Databricks will automatically set the Spark
    URL.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使从这个非常简单的例子中，很明显可以远程开发应用程序，并将它们作为JAR文件加载到Databricks集群中以执行。然而，每次在AWS EC2存储上创建Databricks集群时，Spark
    URL都会发生变化，因此应用程序不应该硬编码诸如Spark主URL之类的细节。Databricks将自动设置Spark URL。
- en: When running the JAR file classes in this way, it is also possible to define
    class parameters. The jobs may be scheduled to run at a given time, or periodically.
    The job timeouts, and alert email addresses may also be specified.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式运行JAR文件类时，也可以定义类参数。作业可以被安排在特定时间运行，或定期运行。还可以指定作业超时和警报电子邮件地址。
- en: Development environments
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发环境
- en: It has been shown that scripts can be created in Notebooks in Scala, Python,
    or SQL, but it is also possible to use an IDE such as IntelliJ or Eclipse to develop
    code. By installing an SBT plugin into this development environment, it is possible
    to develop code for your Databricks environment. The current release of Databricks,
    as I write this book, is 1.3.2d. The **Release Notes** link, under **New Features**
    on the start page, contains a link to the IDE integration, which is `https://dbc-xxxxxxx-xxxx.cloud.databricks.com/#shell/1547`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明可以在Scala、Python或SQL的笔记本中创建脚本，但也可以使用诸如IntelliJ或Eclipse之类的IDE来开发代码。通过在开发环境中安装SBT插件，可以为Databricks环境开发代码。在我写这本书的时候，Databricks的当前版本是1.3.2d。在起始页面的**新功能**下的**发布说明**链接中包含了IDE集成的链接，即`https://dbc-xxxxxxx-xxxx.cloud.databricks.com/#shell/1547`。
- en: The URL will be of this form, with the section starting with `dbc` changed to
    match the URL for the Databricks cloud that you will create. I won't expand on
    this here, but leave it to you to investigate. In the next section, I will investigate
    the Databricks table data processing functionality.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: URL将采用这种形式，以`dbc`开头的部分将更改以匹配您将创建的Databricks云的URL。我不会在这里展开，而是留给您去调查。在下一节中，我将调查Databricks表数据处理功能。
- en: Databricks tables
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks表
- en: 'The Databricks **Tables** menu option allows you store your data in a tabular
    form with an associated schema. The **Tables** menu option allows you to both
    create a table, and refresh your tables list, as the following screenshot shows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks的**表**菜单选项允许您以表格形式存储数据，并附带模式。**表**菜单选项允许您创建表格，并刷新表格列表，如下面的截图所示：
- en: '![Databricks tables](img/B01989_08_29.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![Databricks表](img/B01989_08_29.jpg)'
- en: Data import
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据导入
- en: You can create tables via data import, and specify the table structure, at the
    same time, in terms of column names and types. If the data that is being imported
    has a header, then the column names can be taken from that, although all the column
    types are assumed to be strings. The following screenshot shows a concatenated
    view of the data import options and form, available when creating a table. The
    import file location options are **S3**, **DBFS**, **JDBC**, and **File**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过数据导入创建表，并同时指定列名和类型的表结构。如果要导入的数据具有标题，则可以从中获取列名，尽管所有列类型都被假定为字符串。下面的截图显示了在创建表时可用的数据导入选项和表单的连接视图。导入文件位置选项包括**S3**、**DBFS**、**JDBC**和**文件**。
- en: '![Data import](img/B01989_08_30.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![数据导入](img/B01989_08_30.jpg)'
- en: 'The previous screenshot shows **S3** selected. In order to browse my **S3**
    bucket for a file to import to a table, I will need to enter the **AWS Key ID**,
    the **Secret Access Key**, and the **AWS S3 Bucket Name**. Then, I could browse,
    select the file, and create a table via preview. In the following screenshot,
    I have selected the **File** option:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图显示了选择了**S3**。为了浏览我的**S3**存储桶以将文件导入表中，我需要输入**AWS Key ID**、**Secret Access
    Key**和**AWS S3 Bucket Name**。然后，我可以浏览、选择文件，并通过预览创建表。在下面的截图中，我选择了**文件**选项：
- en: '![Data import](img/B01989_08_31.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![数据导入](img/B01989_08_31.jpg)'
- en: 'I can either drop my file to import into the upload frame in the following
    screenshot, or click on the frame to browse the local server to select a file
    to upload. Once a file is selected, it is then possible to define the data column
    delimiter, and whether the data contains a header row. It is possible to preview
    the data, and change the column names and data types. It is also possible to specify
    the new table name, and the file type. The following screenshot shows a sample
    file data load to create the table called `shuttle`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以将要导入的文件拖放到下面截图中的上传框中，或者单击框以浏览本地服务器以选择要上传的文件。选择文件后，可以定义数据列分隔符，以及数据是否包含标题行。可以预览数据，并更改列名和数据类型。还可以指定新表名和文件类型。下面的截图显示了加载示例文件数据以创建名为`shuttle`的表：
- en: '![Data import](img/B01989_08_32.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![数据导入](img/B01989_08_32.jpg)'
- en: 'Once created, the menu table list can be refreshed and the table schema viewed
    to confirm the column names and types. In this way, a sample of the table data
    can also be previewed. The table can now be viewed and accessed from an SQL session.
    The following screenshot shows that the **shuttle** table is visible using the
    `show tables` command:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 创建后，菜单表列表可以刷新，并且可以查看表模式以确认列名和类型。通过这种方式，还可以预览表数据的样本。现在可以从SQL会话中查看和访问表。下面的截图显示了使用`show
    tables`命令可见**shuttle**表：
- en: '![Data import](img/B01989_08_33.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![数据导入](img/B01989_08_33.jpg)'
- en: 'Once imported, the data in this table can also be accessed via an SQL session.
    The following screenshot shows a simple SQL session statement to show the data
    extracted from the new **shuttle** table:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦导入，此表中的数据也可以通过SQL会话访问。下面的截图显示了一个简单的SQL会话语句，显示了从新的**shuttle**表中提取的数据：
- en: '![Data import](img/B01989_08_34.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![数据导入](img/B01989_08_34.jpg)'
- en: So, this provides the means to import multiple tables from a variety of data
    sources, and create a complex schema in order to filter and join the data by columns
    and rows, just as you would in a traditional, relational database. It provides
    a familiar approach to big data processing.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这提供了从各种数据源导入多个表格，并创建复杂模式以通过列和行过滤和连接数据的手段，就像在传统的关系数据库中一样。它提供了一种熟悉的大数据处理方法。
- en: This section has described the process by which tables can be created via data
    import, but what about creating tables programmatically, or creating tables as
    external objects? The following sections will provide examples of this approach
    to table management.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了可以通过数据导入创建表的过程，但是如何通过编程方式创建表，或者创建外部对象作为表呢？接下来的部分将提供这种表管理方法的示例。
- en: External tables
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部表
- en: Databricks allows you to create tables against external resources, such as AWS
    S3 files, or local file system files. In this section, I will create an external
    table against an S3-based bucket, path, and a set of files. I will also examine
    both the permissions required in AWS and the access policy used. The following
    screenshot shows an AWS S3 bucket called **dbawss3test2** being created. Permissions
    have been granted to everyone to access the list. I am not suggesting that you
    do this, but ensure that your group can access your bucket.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks允许您针对外部资源（如AWS S3文件或本地文件系统文件）创建表。在本节中，我将针对基于S3的存储桶、路径和一组文件创建外部表。我还将检查AWS中所需的权限和使用的访问策略。以下截图显示了一个名为**dbawss3test2**的AWS
    S3存储桶的创建。已授予所有人访问列表的权限。我并不建议您这样做，但请确保您的组可以访问您的存储桶。
- en: '![External tables](img/B01989_08_35.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![外部表](img/B01989_08_35.jpg)'
- en: 'Also, a policy has been added to aid access. In this case, anonymous users
    have been granted read-only access to the bucket and sub contents. You can create
    a more complex policy to limit the access to your group and assorted files. The
    following screenshot shows the new policy:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还添加了一个策略以帮助访问。在这种情况下，匿名用户已被授予对存储桶和子内容的只读访问权限。您可以创建一个更复杂的策略，以限制对您的组和各种文件的访问。以下截图显示了新策略：
- en: '![External tables](img/B01989_08_36.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![外部表](img/B01989_08_36.jpg)'
- en: 'With an access policy, and a bucket created with the correct access policy,
    I can now create folders and upload files for use with a Databricks external table.
    As the following screenshot shows, I have done just that. The uploaded file has
    ten columns in CSV file format:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有了访问策略和使用正确访问策略创建的存储桶，我现在可以创建文件夹并上传文件以供Databricks外部表使用。如下截图所示，我已经做到了。上传的文件以CSV文件格式有十列：
- en: '![External tables](img/B01989_08_37.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![外部表](img/B01989_08_37.jpg)'
- en: 'Now that the AWS S3 resources have been set up, they need to be mounted to
    Databricks, as the Scala-based example shows next. I have removed my AWS and secret
    keys from the script for security purposes. Your mounted directory will need to
    start with `/mnt` and any of the `/` characters, and your secret key value will
    need to be replaced with `%2F`. The `dbutils.fs` class is being used to create
    the mount and the code executes within a second, as the following result shows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，AWS S3资源已设置好，需要将其挂载到Databricks，如下面基于Scala的示例所示。出于安全目的，我已从脚本中删除了我的AWS和秘密密钥。您的挂载目录将需要以`/mnt`和任何`/`字符开头，并且您的秘密密钥值将需要替换为`%2F`。使用`dbutils.fs`类来创建挂载，代码在一秒内执行，如下结果所示：
- en: '![External tables](img/B01989_08_38.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![外部表](img/B01989_08_38.jpg)'
- en: Now, an external table can be created against this mounted path and the files
    that it contains using a Notebook-based SQL session, as the following screenshot
    shows. The table called `s3test1` has been created against the files that the
    mounted directory contains, and a delimiter is specified as a comma, in order
    to parse the CSV-based content.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以使用基于笔记本的SQL会话针对此挂载路径和其中包含的文件创建外部表，如下截图所示。名为`s3test1`的表已针对挂载目录包含的文件创建，并指定逗号作为分隔符，以解析基于CSV的内容。
- en: '![External tables](img/B01989_08_39.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![外部表](img/B01989_08_39.jpg)'
- en: 'The **Tables** menu option now shows that the **s3test1** table exists, as
    shown in the following screenshot. So, it should be possible to run some SQL against
    this table:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**表**菜单选项现在显示**s3test1**表存在，如下截图所示。因此，应该可以针对此表运行一些SQL：'
- en: '![External tables](img/B01989_08_40.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![外部表](img/B01989_08_40.jpg)'
- en: I have run a `SELECT` statement in an SQL-based Notebook session to get a row
    count from the external table, using the `COUNT(*)` function, as shown in the
    following screenshot. It can be see that the table contains **14500** rows.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我在基于SQL的笔记本会话中运行了一个`SELECT`语句，使用`COUNT(*)`函数从外部表中获取行数，如下截图所示。可以看到表包含**14500**行。
- en: '![External tables](img/B01989_08_41.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![外部表](img/B01989_08_41.jpg)'
- en: 'I will now add another file to the S3-based folder. In this case, it is just
    a copy of the first file in CSV format, so the row count in the external table
    should double. The following screenshot shows the file that is added:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在将向基于S3的文件夹添加另一个文件。在这种情况下，它只是第一个文件的CSV格式副本，因此外部表中的行数应该加倍。以下截图显示了添加的文件：
- en: '![External tables](img/B01989_08_42.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![外部表](img/B01989_08_42.jpg)'
- en: 'Running the same `SELECT` statement against the external table does indeed
    provide a doubled row count of **29000** rows. The following screenshot shows
    the SQL statement, and the output:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对外部表运行相同的`SELECT`语句确实提供了**29000**行的加倍行数。以下截图显示了SQL语句和输出：
- en: '![External tables](img/B01989_08_43.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![外部表](img/B01989_08_43.jpg)'
- en: So, it is easily possible to create external tables within Databricks, and run
    SQL against content that is dynamically changed. The file structure will need
    to be uniform, and the S3 bucket access must be defined if using AWS. The next
    section will examine the DbUtils package provided with Databricks.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在Databricks内部很容易创建外部表，并对动态更改的内容运行SQL。文件结构需要是统一的，如果使用AWS，则必须定义S3存储桶访问权限。下一节将检查Databricks提供的DbUtils包。
- en: The DbUtils package
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DbUtils包
- en: 'The previous Scala-based script, which uses the DbUtils package, and creates
    the mount in the last section, only uses a small portion of the functionality
    of this package. In this section, I would like to introduce some more features
    of the DbUtils package, and the **Databricks File System** (**DBFS**). The help
    option within the DbUtils package can be called within a Notebook connected to
    a Databricks cluster, to learn more about its structure and functionality. As
    the following screenshot shows, executing `dbutils.fs.help()` in a Scala Notebook
    provides help on fsutils, cache, and the mount-based functionality:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 之前基于Scala的脚本使用了DbUtils包，并在最后一节中创建了挂载点，只使用了该包的一小部分功能。在本节中，我想介绍一些DbUtils包和**Databricks文件系统**（**DBFS**）的更多功能。在连接到Databricks集群的笔记本中，可以调用DbUtils包中的帮助选项，以了解其结构和功能。正如下面的截图所示，在Scala笔记本中执行`dbutils.fs.help()`可以提供有关fsutils、cache和基于挂载的功能的帮助：
- en: '![The DbUtils package](img/B01989_08_44.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![DbUtils包](img/B01989_08_44.jpg)'
- en: 'It is also possible to obtain help on individual functions, as the text in
    the previous screenshot shows. The example in the following screenshot explains
    the **cacheTable** function, providing descriptive text and a sample function
    call with the parameter and return types:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以获取关于单个函数的帮助，就像之前截图中的文本所示。下面的截图中的示例解释了**cacheTable**函数，提供了描述性文本和带有参数和返回类型的示例函数调用：
- en: '![The DbUtils package](img/B01989_08_45.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![DbUtils包](img/B01989_08_45.jpg)'
- en: The next section will briefly examine the DBFS before moving on to examining
    more of the `dbutils` functionality.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将简要介绍DBFS，然后继续检查更多的`dbutils`功能。
- en: Databricks file system
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Databricks文件系统
- en: The DBFS can be accessed using URL's of the `dbfs:/*` form, and using the functions
    available within `dbutils.fs`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`dbfs:/*`形式的URL访问DBFS，并使用`dbutils.fs`中可用的函数。
- en: '![Databricks file system](img/B01989_08_46.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![Databricks file system](img/B01989_08_46.jpg)'
- en: The previous screenshot shows the `/mnt` file system being examined using the
    `ls` function, and then showing mount directories—`s3data` and `s3data1`. These
    were the directories created during the previous Scala S3 mount example.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的截图显示了使用`ls`函数检查`/mnt`文件系统，然后显示挂载目录——`s3data`和`s3data1`。这些是在之前的Scala S3挂载示例中创建的目录。
- en: Dbutils fsutils
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dbutils fsutils
- en: 'The `fsutils` group of functions, within the `dbutils` package, covers functions
    such as `cp`, `head`, `mkdirs`, `mv`, `put`, and `rm`. The help calls, shown previously,
    can provide more information about them. You can create a directory on DBFS using
    the `mkdirs` call, as shown next. Note that I have created a number of directories
    under `dbfs:/`, named as `data*` in this session. The following example has created
    the directory called `data2`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`dbutils`包中的`fsutils`函数组包括`cp`、`head`、`mkdirs`、`mv`、`put`和`rm`等函数。之前显示的帮助调用可以提供更多关于它们的信息。您可以使用`mkdirs`调用在DBFS上创建一个目录，如下所示。请注意，我在这个会话中在`dbfs:/`下创建了许多名为`data*`的目录。下面的例子创建了一个名为`data2`的目录：'
- en: '![Dbutils fsutils](img/B01989_08_47.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![Dbutils fsutils](img/B01989_08_47.jpg)'
- en: 'The previous screenshot shows by executing an `ls` that there are many default
    directories that already exist on DBFS. For instance, see the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的截图通过执行`ls`显示了DBFS上已经存在许多默认目录。例如，参见以下内容：
- en: '`/tmp` is a temporary area'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/tmp`是一个临时区域'
- en: '`/mnt` is a mount point for remote directories—that is, S3'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/mnt`是远程目录的挂载点，即S3'
- en: '`/user` is a user storage area that currently contains Hive'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/user`是一个用户存储区域，目前包含Hive'
- en: '`/mount` is an empty directory'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/mount`是一个空目录'
- en: '`/FileStore` is a storage area for tables, JARs, and job JARs'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/FileStore`是用于存储表、JAR和作业JAR的存储区域'
- en: '`/databricks-datasets` is datasets provided by Databricks'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/databricks-datasets`是Databricks提供的数据集'
- en: 'The `dbutils` copy command, shown next, allows a file to be copied to a DBFS
    location. In this instance, the `external1.txt` file had been copied to the `/data2`
    directory, as shown in the following screenshot:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来显示的`dbutils`复制命令允许将文件复制到DBFS位置。在这个例子中，`external1.txt`文件已经被复制到`/data2`目录，如下面的截图所示：
- en: '![Dbutils fsutils](img/B01989_08_48.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![Dbutils fsutils](img/B01989_08_48.jpg)'
- en: The `head` function can be used to return the first maxBytes characters from
    the head of a file on DBFS. The following example shows the format of the `external1.txt`
    file. This is useful, as it tells me that this is a CSV file, and so shows me
    how to process it.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`head`函数可用于从DBFS文件的开头返回前maxBytes个字符。下面的例子显示了`external1.txt`文件的格式。这很有用，因为它告诉我这是一个CSV文件，因此告诉我如何处理它。'
- en: '![Dbutils fsutils](img/B01989_08_49.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Dbutils fsutils](img/B01989_08_49.jpg)'
- en: It is also possible to move files within DBFS. The following screenshot shows
    the `mv` command being used to move the `external1.txt` file from the directory
    `data2` to the directory called `data1`. The `ls` command is then used to confirm
    the move.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以在DBFS内部移动文件。下面的截图显示了使用`mv`命令将`external1.txt`文件从`data2`目录移动到名为`data1`的目录。然后使用`ls`命令确认移动。
- en: '![Dbutils fsutils](img/B01989_08_50.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![Dbutils fsutils](img/B01989_08_50.jpg)'
- en: 'Finally, the remove function (`rm`) is used to remove the file called `external1.txt`,
    which was just moved. The following `ls` function call shows that the file no
    longer exists within the `data1` directory, because there is no `FileInfo` record
    in the function output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用remove函数（`rm`）来删除刚刚移动的名为`external1.txt`的文件。以下的`ls`函数调用显示，该文件不再存在于`data1`目录中，因为在函数输出中没有`FileInfo`记录：
- en: '![Dbutils fsutils](img/B01989_08_51.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![Dbutils fsutils](img/B01989_08_51.jpg)'
- en: The DbUtils cache
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DbUtils缓存
- en: 'The cache functionality, within DbUtils, provides the means to cache (and uncache)
    both tables and files to DBFS. Actually, the tables are saved as files also to
    the DBFS directory called `/FileStore`. The following screenshot shows that the
    cache functions are available:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在DbUtils中的缓存功能提供了缓存（和取消缓存）表和文件到DBFS的方法。实际上，表也被保存为文件到名为`/FileStore`的DBFS目录。下面的截图显示了缓存功能是可用的：
- en: '![The DbUtils cache](img/B01989_08_52.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![DbUtils缓存](img/B01989_08_52.jpg)'
- en: The DbUtils mount
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DbUtils挂载
- en: The mount functionality allows you to mount remote file systems, refresh mounts,
    display mount details, and unmount specific mounted directories. An example of
    an S3 mount was already given in the previous sections, so I won't repeat it here.
    The following screenshot shows the output from the `mounts` function. The `s3data`
    and `s3data1` mounts have been created by me. The other two mounts for root and
    datasets already existed. The mounts are listed in a sequence of the `MountInfo`
    objects. I have rearranged the text to be more meaningful, and to be better presented
    on the page.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 挂载功能允许您挂载远程文件系统，刷新挂载，显示挂载详细信息，并卸载特定的已挂载目录。在前几节中已经给出了S3挂载的示例，所以我在这里不会重复了。以下截图显示了`mounts`函数的输出。`s3data`和`s3data1`挂载是我创建的。根目录和数据集的另外两个挂载已经存在。挂载按`MountInfo`对象的顺序列出。我重新排列了文本，使其更有意义，并更好地呈现在页面上。
- en: '![The DbUtils mount](img/B01989_08_53.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![DbUtils挂载](img/B01989_08_53.jpg)'
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter has introduced Databricks. It shows how the service can be accessed,
    and also shows how it uses AWS resources. Remember that, in the future, the people
    who invented Databricks plan to support other cloud-based platforms, such as Microsoft
    Azure. I thought that it was important to introduce Databricks, because the same
    people who were involved in the development of Apache Spark are involved in this
    system. The natural progression seems to be Hadoop, Spark, then Databricks.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了Databricks。它展示了如何访问该服务，以及它如何使用AWS资源。请记住，未来，发明Databricks的人计划支持其他基于云的平台，如Microsoft
    Azure。我认为介绍Databricks很重要，因为参与Apache Spark开发的人也参与了这个系统。自然的发展似乎是Hadoop，Spark，然后是Databricks。
- en: I will continue the Databricks investigation in the next chapter, because important
    features, such as visualization, have not yet been examined. Also, the major Spark
    functionality modules called GraphX, streaming, MLlib, and SQL have not been introduced
    in Databricks terms. How easy is it to use these modules within Databricks to
    process real data? Read on to find out.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在下一章继续对Databricks进行调查，因为重要的功能，如可视化，尚未被审查。此外，Databricks术语中尚未介绍的主要Spark功能模块称为GraphX，流式处理，MLlib和SQL。在Databricks中使用这些模块处理真实数据有多容易？继续阅读以了解更多。
