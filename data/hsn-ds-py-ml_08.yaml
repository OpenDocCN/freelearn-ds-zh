- en: Dealing with Real-World Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理真实世界的数据
- en: In this chapter, we're going to talk about the challenges of dealing with real-world
    data, and some of the quirks you might run into. The chapter starts by talking
    about the bias-variance trade-off, which is kind of a more principled way of talking
    about the different ways you might overfit and underfit data, and how it all interrelates
    with each other. We then talk about the k-fold cross-validation technique, which
    is an important tool in your chest to combat overfitting, and look at how to implement
    it using Python.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论处理真实世界数据的挑战，以及你可能遇到的一些怪癖。本章首先讨论了偏差-方差的权衡，这是一种更有原则的谈论你可能过拟合和欠拟合数据的不同方式的方式，以及它们如何相互关联。然后我们讨论了k折交叉验证技术，这是你用来对抗过拟合的重要工具，并看看如何使用Python实现它。
- en: Next, we analyze the importance of cleaning your data and normalizing it before
    actually applying any algorithms on it. We see an example to determine the most
    popular pages on a website which will demonstrate the importance of cleaning data.
    The chapter also covers the importance of remembering to normalize numerical data.
    Finally, we look at how to detect outliers and deal with them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们分析了在实际应用任何算法之前清理和归一化数据的重要性。我们看了一个示例来确定网站上最受欢迎的页面，这将展示清理数据的重要性。本章还涵盖了记住归一化数值数据的重要性。最后，我们看看如何检测异常值并处理它们。
- en: 'Specifically, this chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章涵盖以下主题：
- en: Analyzing the bias/variance trade-off
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析偏差/方差的权衡
- en: The concept of k-fold cross-validation and its implementation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k折交叉验证的概念及其实现
- en: The importance of cleaning and normalizing data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理和归一化数据的重要性
- en: An example to determine the popular pages of a website
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定网站的热门页面的示例
- en: Normalizing numerical data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化数值数据
- en: Detecting outliers and dealing with them
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测异常值并处理它们
- en: Bias/variance trade-off
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差/方差的权衡
- en: One of the basic challenges that we face when dealing with real-world data is
    overfitting versus underfitting your regressions to that data, or your models,
    or your predictions. When we talk about underfitting and overfitting, we can often
    talk about that in the context of bias and variance, and the bias-variance trade-off.
    So, let's talk about what that means.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理真实世界数据时面临的一个基本挑战是过拟合与欠拟合你的回归数据，或者你的模型，或者你的预测。当我们谈论欠拟合和过拟合时，我们经常可以在偏差和方差的背景下谈论这一点，以及偏差-方差的权衡。所以，让我们谈谈这意味着什么。
- en: So conceptually, bias and variance are pretty simple. Bias is just how far off
    you are from the correct values, that is, how good are your predictions overall
    in predicting the right overall value. If you take the mean of all your predictions,
    are they more or less on the right spot? Or are your errors all consistently skewed
    in one direction or another? If so, then your predictions are biased in a certain
    direction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，偏差和方差非常简单。偏差就是你离正确值有多远，也就是说，你的预测在整体上预测正确的值有多好。如果你取所有预测的平均值，它们是否更多或更少在正确的位置上？或者你的错误是一直偏向某个方向？如果是这样，那么你的预测就有某个方向的偏差。
- en: Variance is just a measure of how spread out, how scattered your predictions
    are. So, if your predictions are all over the place, then that's high variance.
    But, if they're very tightly focused on what the correct values are, or even an
    incorrect value in the case of high bias, then your variance is small.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 方差只是衡量你的预测有多分散、多散乱的一个指标。所以，如果你的预测到处都是，那就是高方差。但是，如果它们非常集中在正确的值上，甚至在高偏差的情况下也是如此，那么你的方差就很小。
- en: 'Let''s look at some examples. Let''s imagine that the following dartboard represents
    a bunch of predictions we''re making where the real value we''re trying to predict
    is in the center of the bullseye:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些例子。假设以下飞镖板代表我们正在做的一堆预测，我们试图预测的真实值在靶心的中心：
- en: '![](img/fae2cf23-43e6-4526-b005-39df58463d04.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fae2cf23-43e6-4526-b005-39df58463d04.png)'
- en: Starting with the dartboard in the upper left-hand corner, you can see that
    our points are all scattered about the center. So overall, you know the mean error
    comes out to be pretty close to reality. Our bias is actually very low, because
    our predictions are all around the same correct point. However, we have very high
    variance, because these points are scattered about all over the place. So, this
    is an example of low bias and high variance.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从左上角的飞镖板开始，你可以看到我们的点都散落在中心周围。所以总体上，你知道平均误差非常接近实际情况。我们的偏差实际上非常低，因为我们的预测都在同一个正确的点周围。然而，我们的方差非常高，因为这些点散布在各个地方。所以，这是一个低偏差和高方差的例子。
- en: If we move on to the dartboard in the upper right corner, we see that our points
    are all consistently skewed from where they should be, to the Northwest. So this
    is an example of high bias in our predictions, where they're consistently off
    by a certain amount. We have low variance because they're all clustered tightly
    around the wrong spot, but at least they're close together, so we're being consistent
    in our predictions. That's low variance. But, the bias is high. So again, this
    is high bias, low variance.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们转移到右上角的飞镖板，我们会看到我们的点都一直偏离了正确的位置，向西北方向。所以这是我们预测中高偏差的一个例子，它们一直偏离了一定的距离。我们的方差很低，因为它们都紧密地聚集在错误的位置周围，但至少它们是紧密在一起的，所以我们在预测中是一致的。这是低方差。但是，偏差很高。所以再次，这是高偏差，低方差。
- en: In the dartboard in the lower left corner, you can see that our predictions
    are scattered around the wrong mean point. So, we have high bias; everything is
    skewed to some place where it shouldn't be. But our variance is also high. So,
    this is kind of the worst of both worlds here; we have high bias and high variance
    in this example.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在左下角的飞镖板上，你可以看到我们的预测散布在错误的平均点周围。所以，我们有很高的偏差；一切都偏向了不应该去的地方。但我们的方差也很高。所以，这在这个例子中是最糟糕的情况；我们既有高偏差又有高方差。
- en: Finally, in a wonderful perfect world, you would have an example like the lower
    right dartboard, where we have low bias, where everything is centered around where
    it should be, and low variance, where things are all clustered pretty tightly
    around where they should be. So, in a perfect world that's what you end up with.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，在一个完美的世界中，你会有一个像右下方飞镖板那样的例子，那里我们有低偏差，一切都集中在应该的位置，以及低方差，事物都紧密地聚集在应该的位置。所以，在一个完美的世界中，这就是你最终得到的结果。
- en: 'In reality, you often need to choose between bias and variance. It comes down
    to over fitting Vs underfitting your data. Let''s take a look at the following
    example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你经常需要在偏差和方差之间做出选择。这归结为过拟合与欠拟合数据。让我们看看以下例子：
- en: '![](img/7a856ab7-9d88-404c-8cfb-97365495b525.png) ![](img/0386221c-18f7-4cc9-88aa-9139c77d4ba5.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a856ab7-9d88-404c-8cfb-97365495b525.png) ![](img/0386221c-18f7-4cc9-88aa-9139c77d4ba5.png)'
- en: It's a little bit of a different way of thinking of bias and variance. So, in
    the left graph, we have a straight line, and you can think of that as having very
    low variance, relative to these observations. So, there's not a lot of variance
    in this line, that is, there is low variance. But the bias, the error from each
    individual point, is actually high.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种对偏差和方差的不同思考方式。所以，在左边的图表中，我们有一条直线，你可以认为相对于这些观察结果，它具有非常低的方差。所以，这条线的方差不大，也就是说，它具有低方差。但是偏差，每个单独点的误差，实际上是很高的。
- en: Now, contrast that to the overfitted data in the graph at the right, where we've
    kind of gone out of our way to fit the observations. The line has high variance,
    but low bias, because each individual point is pretty close to where it should
    be. So, this is an example of where we traded off variance for bias.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对比一下右边图表中过拟合的数据，我们已经努力去拟合这些观察结果。这条线具有高方差，但低偏差，因为每个单独的点都非常接近它应该在的位置。所以，这就是我们用方差换取偏差的一个例子。
- en: 'At the end of the day, you''re not out to just reduce bias or just reduce variance,
    you want to reduce error. That''s what really matters, and it turns out you can
    express error as a function of bias and variance:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你不是为了只减少偏差或只减少方差，你想要减少错误。这才是真正重要的，结果表明你可以将错误表达为偏差和方差的函数：
- en: '![](img/459a0f30-4b0c-4dfd-a4fb-91b39c005369.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/459a0f30-4b0c-4dfd-a4fb-91b39c005369.png)'
- en: Looking at this, error is equal to bias squared plus variance. So, these things
    both contribute to the overall error, with bias actually contributing more. But
    keep in mind, it's error you really want to minimize, not the bias or the variance
    specifically, and that an overly complex model will probably end up having a high
    variance and low bias, whereas a too simple model will have low variance and high
    bias. However, they could both end up having similar error terms at the end of
    the day. You just have to find the right happy medium of these two things when
    you're trying to fit your data. We'll talk about some more principled ways of
    actually avoiding overfitting in our forthcoming sections. But, it's just the
    concept of bias and variance that I want to get across, because people do talk
    about it and you're going to be expected to know what means.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 看这个，错误等于偏差的平方加上方差。所以，这两个因素都会对总体错误产生影响，实际上偏差的影响更大。但要记住，你真正想要最小化的是错误，而不是偏差或方差特别，一个过于复杂的模型最终可能会产生高方差和低偏差，而一个过于简单的模型会产生低方差和高偏差。然而，它们最终可能都会产生类似的错误项。当你试图拟合你的数据时，你只需要找到这两个因素的正确平衡点。我们将在接下来的部分讨论一些更有原则的方法来避免过拟合。但是，我只是想传达偏差和方差的概念，因为人们确实会谈论它，你会被期望知道它的含义。
- en: Now let's tie that back to some earlier concepts in this book. For example,
    in k-nearest neighbors if we increase the value of K, we start to spread out our
    neighborhood that were averaging across to a larger area. That has the effect
    of decreasing variance because we're kind of smoothing things out over a larger
    space, but it might increase our bias because we'll be picking up a larger population
    that may be less and less relevant to the point we started from. By smoothing
    out KNN over a larger number of neighbors, we can decrease the variance because
    we're smoothing things out over more values. But, we might be introducing bias
    because we're introducing more and more points that are less than less related
    to the point we started with.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们把它与本书中一些早期的概念联系起来。例如，在K最近邻中，如果我们增加K的值，我们开始扩大我们要平均的邻域到一个更大的区域。这会减少方差，因为我们在更大的空间上平滑了事物，但它可能会增加我们的偏差，因为我们可能会选择一个更大的人口，这个人口可能与我们起始的点越来越不相关。通过在更多的邻居上平滑KNN，我们可以减少方差，因为我们在更多的值上平滑了事物。但是，我们可能会引入偏差，因为我们引入了越来越不相关于我们起始点的点。
- en: 'Decision trees is another example. We know that a single decision tree is prone
    to overfitting, so that might imply that it has a high variance. But, random forests
    seek to trade off some of that variance for bias reduction, and it does that by
    having multiple trees that are randomly variant and averages all their solutions
    together. It''s like when we average things out by increasing K in KNN: we can
    average out the results of a decision tree by using more than one decision tree
    using random forests similar idea.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树就是另一个例子。我们知道单个决策树容易过拟合，这可能意味着它具有高方差。但是，随机森林试图通过拥有多个随机变体的树并将它们的解决方案平均在一起来换取一些偏差减少的方差，就像我们通过增加K值来平均KNN的结果一样：我们可以通过使用多个决策树来平均决策树的结果，使用随机森林类似的想法。
- en: This is bias-variance trade-off. You know the decision you have to make between
    how overall accurate your values are, and how spread out they are or how tightly
    clustered they are. That's the bias-variance trade-off and they both contribute
    to the overall error, which is the thing you really care about minimizing. So,
    keep those terms in mind!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是偏差-方差折衷。你知道你必须在整体准确度和散布程度或紧密聚集程度之间做出决定。这就是偏差-方差折衷，它们都对总体错误产生影响，而你真正关心的是最小化这个错误。所以，记住这些术语！
- en: K-fold cross-validation to avoid overfitting
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K折交叉验证以避免过拟合
- en: Earlier in the book, we talked about train and test as a good way of preventing
    overfitting and actually measuring how well your model can perform on data it's
    never seen before. We can take that to the next level with a technique called
    k-fold cross-validation. So, let's talk about this powerful tool in your arsenal
    for fighting overfitting; k-fold cross-validation and learn how that works.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前面，我们谈到了训练和测试作为防止过拟合并实际测量模型在从未见过的数据上的表现的好方法。我们可以通过一种称为k折交叉验证的技术将其提升到一个新的水平。因此，让我们谈谈这个强大的工具，用于对抗过拟合；k折交叉验证，并了解它的工作原理。
- en: 'To recall from train/test, the idea was that we split all of our data that
    we''re building a machine learning model based off of into two segments: a training
    dataset, and a test dataset. The idea is that we train our model only using the
    data in our training dataset, and then we evaluate its performance using the data
    that we reserved for our test dataset. That prevents us from overfitting to the
    data that we have because we''re testing the model against data that it''s never
    seen before.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下训练/测试，其思想是我们将构建机器学习模型的所有数据分成两部分：一个训练数据集和一个测试数据集。我们只使用训练数据集来训练模型，然后使用我们保留的测试数据集来评估其性能。这样可以防止我们对已有数据过拟合，因为我们正在测试模型对其从未见过的数据的表现。
- en: 'However, train/test still has its limitations: you could still end up overfitting
    to your specific train/test split. Maybe your training dataset isn''t really representative
    of the entire dataset, and too much stuff ended up in your training dataset that
    skews things. So, that''s where k-fold cross-validation comes in, it takes train/test
    and kicks it up a notch.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，训练/测试仍然有其局限性：你仍然可能会对特定的训练/测试分割过拟合。也许你的训练数据集并不真正代表整个数据集，太多的东西最终进入了你的训练数据集，导致了偏差。这就是k折交叉验证发挥作用的地方，它将训练/测试提升到一个新的水平。
- en: 'The idea, although it sounds complicated, is fairly simple:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管听起来很复杂，但其实思想相当简单：
- en: Instead of dividing our data into two buckets, one for training and one for
    testing, we divide it into K buckets.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据分成K个桶，而不是两个桶，一个用于训练，一个用于测试。
- en: We reserve one of those buckets for testing purposes, for evaluating the results
    of our model.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们保留其中一个桶用于测试目的，用于评估我们模型的结果。
- en: We train our model against the remaining buckets that we have, K-1, and then
    we take our test dataset and use that to evaluate how well our model did amongst
    all of those different training datasets.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对剩下的桶（K-1）进行模型训练，然后我们拿出我们的测试数据集，用它来评估我们的模型在所有这些不同的训练数据集中的表现如何。
- en: We average those resulting error metrics, that is, those r-squared values, together
    to get a final error metric from k-fold cross-validation.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将这些结果的误差指标（即R平方值）进行平均，得到k折交叉验证的最终误差指标。
- en: That's all it is. It is a more robust way of doing train/test, and that's one
    way of doing it.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。这是一种更健壮的训练/测试方法，这是一种方法。
- en: Now, you might think to yourself well, what if I'm overfitting to that one test
    dataset that I reserved? I'm still using the same test dataset for every one of
    those training datasets. What if that test dataset isn't really representative
    of things either?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能会想，如果我对我保留的那个测试数据集过拟合了怎么办？我仍然对每一个训练数据集使用相同的测试数据集。如果那个测试数据集也不真正代表实际情况呢？
- en: There are variations of k-fold cross-validation that will randomize that as
    well. So, you could randomly pick what the training dataset is as well each time
    around, and just keep randomly assigning things to different buckets and measuring
    the results. But usually, when people talk about k-fold cross-validation, they're
    talking about this specific technique where you reserve one bucket for testing,
    and the remaining buckets for training, and you evaluate all of your training
    datasets against the test dataset when you build a model for each one.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些k折交叉验证的变体，也会对此进行随机化。因此，你也可以每次随机选择训练数据集，并将不同的数据随机分配到不同的桶中进行测量。但通常，当人们谈论k折交叉验证时，他们指的是这种特定的技术，其中你保留一个桶用于测试，其余桶用于训练，并在构建每个模型时使用测试数据集评估所有训练数据集。
- en: Example of k-fold cross-validation using scikit-learn
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行k折交叉验证的示例
- en: Fortunately, scikit-learn makes this really easy to do, and it's even easier
    than doing normal train/test! It's extremely simple to do k-fold cross-validation,
    so you may as well just do it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，scikit-learn使这变得非常容易，甚至比普通的训练/测试更容易！进行k折交叉验证非常简单，所以你可能会选择这样做。
- en: Now, the way this all works in practice is you will have a model that you're
    trying to tune, and you will have different variations of that model, different
    parameters you might want to tweak on it, right?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在实践中，这一切是如何运作的是，你会有一个你想要调整的模型，以及该模型的不同变体，你可能想要对其进行微调的不同参数，对吧？
- en: Like, for example, the degree of polynomial for a polynomial fit. So, the idea
    is to try different values of your model, different variations, measure them all
    using k-fold cross-validation, and find the one that minimizes error against your
    test dataset. That's kind of your sweet spot there. In practice, you want to use
    k-fold cross-validation to measure the accuracy of your model against a test dataset,
    and just keep refining that model, keep trying different values within it, keep
    trying different variations of that model or maybe even different models entirely,
    until you find the technique that reduces error the most, using k-fold cross validation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，多项式拟合的多项式程度。因此，想法是尝试模型的不同值，不同的变体，使用k折交叉验证对它们进行测量，并找到最小化与测试数据集的误差的值。这就是你的最佳选择。在实践中，你想使用k折交叉验证来衡量模型对测试数据集的准确性，并不断完善模型，尝试其中的不同值，尝试模型的不同变体，甚至可能是完全不同的模型，直到找到最大程度减少误差的技术，使用k折交叉验证。
- en: Let's go dive into an example and see how it works. We're going to apply this
    to our Iris dataset again, revisiting SVC, and we'll play with k-fold cross-validation
    and see how simple it is. Let's actually put k-fold cross-validation and train/test
    into practice here using some real Python code. You'll see it's actually very
    easy to use, which is a good thing because this is a technique you should be using
    to measure the accuracy, the effectiveness of your models in supervised learning.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子，看看它是如何工作的。我们将再次将其应用于我们的鸢尾花数据集，重新审视SVC，并且我们将使用k-fold交叉验证来尝试一下，看看它是多么简单。实际上，让我们将k-fold交叉验证和训练/测试应用到实践中，使用一些真正的Python代码。你会发现它实际上非常容易使用，这是一件好事，因为这是一种你应该使用来衡量监督学习模型准确性和有效性的技术。
- en: Please go ahead and open up the `KFoldCrossValidation.ipynb` and follow along
    if you will. We're going to look at the Iris dataset again; remember we introduced
    this when we talk about dimensionality reduction?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请继续打开`KFoldCrossValidation.ipynb`，如果愿意的话可以跟着做。我们将再次看看鸢尾花数据集；还记得我们在谈论降维时介绍过它吗？
- en: Just to refresh your memory, the Iris dataset contains a set of 150 Iris flower
    measurements, where each flower has a length and width of its petal, and a length
    and width of its sepal. We also know which one of 3 different species of Iris
    each flower belongs to. The challenge here is to create a model that can successfully
    predict the species of an Iris flower, just given the length and width of its
    petal and sepal. So, let's go ahead and do that.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你记起来，鸢尾花数据集包含了150个鸢尾花的测量数据，每朵花都有其花瓣和萼片的长度和宽度。我们还知道每朵花属于3种不同的鸢尾花中的哪一种。这里的挑战是创建一个能够成功预测鸢尾花种类的模型，仅仅基于其花瓣和萼片的长度和宽度。所以，让我们继续做这件事。
- en: 'We''re going to use the SVC model. If you remember back again, that''s just
    a way of classifying data that''s pretty robust. There''s a section on that if
    you need to go and refresh your memory:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用SVC模型。如果你还记得，这只是一种对数据进行分类的相当强大的方法。如果需要，可以查看相关部分来复习一下：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: What we do is use the `cross_validation` library from scikit-learn, and we start
    by just doing a conventional train test split, just a single train/test split,
    and see how that will work.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用scikit-learn中的`cross_validation`库，首先进行传统的训练测试分割，只是一个单一的训练/测试分割，看看它的效果如何。
- en: To do that we have a `train_test_split()` function that makes it pretty easy.
    So, the way this works is we feed into `train_test_split()` a set of feature data.
    `iris.data` just contains all the actual measurements of each flower. `iris.target`
    is basically the thing we're trying to predict.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们有一个`train_test_split()`函数，使得这变得相当容易。这样的工作方式是，我们将一组特征数据输入到`train_test_split()`中。`iris.data`只包含每朵花的实际测量数据。`iris.target`基本上是我们要预测的东西。
- en: In this case, it contains all the species for each flower. `test_size` says
    what percentage do we want to train versus test. So, 0.4 means we're going to
    extract 40% of that data randomly for testing purposes, and use 60% for training
    purposes. What this gives us back is 4 datasets, basically, a training dataset
    and a test dataset for both the feature data and the target data. So, `X_train`
    ends up containing 60% of our Iris measurements, and `X_test` contains 40% of
    the measurements used for testing the results of our model. `y_train` and `y_test`
    contain the actual species for each one of those segments.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，它包含了每朵花的所有种类。`test_size`表示我们想要训练与测试的百分比。因此，0.4表示我们将随机提取40%的数据进行测试，并使用60%进行训练。这给我们带来的是4个数据集，基本上是一个用于训练的数据集和一个用于测试的数据集，分别用于特征数据和目标数据。因此，`X_train`最终包含了我们鸢尾花测量的60%，而`X_test`包含了用于测试我们模型结果的测量的40%。`y_train`和`y_test`包含了每个部分的实际种类。
- en: 'Then after that we go ahead and build an SVC model for predicting Iris species
    given their measurements, and we build that only using the training data. We fit
    this SVC model, using a linear kernel, using only the training feature data, and
    the training species data, that is, target data. We call that model `clf`. Then,
    we call the `score()` function on `clf` to just measure its performance against
    our test dataset. So, we score this model against the test data we reserved for
    the Iris measurements, and the test Iris species, and see how well it does:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续构建一个SVC模型，用于预测鸢尾花的种类，只使用训练数据。我们使用线性核来拟合这个SVC模型，只使用训练的特征数据和训练的种类数据，也就是目标数据。我们将该模型称为`clf`。然后，我们在`clf`上调用`score()`函数，只是为了衡量它在我们的测试数据集上的表现。因此，我们将这个模型与我们为鸢尾花测量保留的测试数据集以及测试鸢尾花种类进行比分，看看它的表现如何：
- en: '![](img/7d9b6519-0e08-4385-af2a-4dc643935313.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d9b6519-0e08-4385-af2a-4dc643935313.jpg)'
- en: It turns out it does really well! Over 96% of the time, our model is able to
    correctly predict the species of an Iris that it had never seen before, just based
    on the measurements of that Iris. So that's pretty cool!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明它表现得非常好！超过96%的时间，我们的模型能够基于那些鸢尾花的测量结果，准确预测出它们的种类，即使是它之前从未见过的鸢尾花。所以这很酷！
- en: 'But, this is a fairly small dataset, about 150 flowers if I remember right.
    So, we''re only using 60% of 150 flowers for training and only 40% of 150 flowers
    for testing. These are still fairly small numbers, so we could still be overfitting
    to our specific train/test split that we made. So, let''s use k-fold cross-validation
    to protect against that. It turns out that using k-fold cross-validation, even
    though it''s a more robust technique, is actually even easier to use than train/test.
    So, that''s pretty cool! So, let''s see how that works:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这是一个相当小的数据集，大约有150朵花，如果我没记错的话。因此，我们只使用150朵花的60%进行训练，只使用150朵花的40%进行测试。这些数字仍然相当小，所以我们仍然可能会过度拟合我们所做的特定训练/测试分割。因此，让我们使用k-fold交叉验证来防止这种情况发生。事实证明，使用k-fold交叉验证，即使它是一种更强大的技术，实际上比训练/测试更容易使用。所以，这很酷！那么，让我们看看它是如何工作的：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have a model already, the SVC model that we defined for this prediction,
    and all you need to do is call `cross_val_score()` on the `cross_validation` package.
    So, you pass in this function a model of a given type (`clf`), the entire dataset
    that you have of all of the measurements, that is, all of my feature data (`iris.data`)
    and all of my target data (all of the species), `iris.target`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了一个模型，即我们为这个预测定义的SVC模型，你所需要做的就是在`cross_validation`包上调用`cross_val_score()`。因此，您需要向这个函数传递一个给定类型的模型（`clf`），您拥有的所有测量数据集，也就是所有的特征数据（`iris.data`）和所有的目标数据（所有的物种），`iris.target`。
- en: I want `cv=5` which means it's actually going to use 5 different training datasets
    while reserving `1` for testing. Basically, it's going to run it 5 times, and
    that's all we need to do. That will automatically evaluate our model against the
    entire dataset, split up five different ways, and give us back the individual
    results.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要 `cv=5`，这意味着它实际上会使用5个不同的训练数据集，同时保留 `1` 用于测试。基本上，它会运行5次，这就是我们需要做的全部。这将自动评估我们的模型针对整个数据集，分成五种不同的方式，并将结果返回给我们。
- en: 'If we print back the output of that, it gives us back a list of the actual
    error metric from each one of those iterations, that is, each one of those folds.
    We can average those together to get an overall error metric based on k-fold cross-validation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印出来，它会给我们返回一个实际错误指标的列表，即每个迭代的错误指标，也就是每个折叠的错误指标。我们可以将这些平均起来，得到基于k折交叉验证的总体错误指标：
- en: '![](img/705c48f3-a3b4-4d63-ae15-c4159c0e5c76.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/705c48f3-a3b4-4d63-ae15-c4159c0e5c76.png)'
- en: When we do this over 5 folds, we can see that our results are even better than
    we thought! 98% accuracy. So that's pretty cool! In fact, in a couple of the runs
    we had perfect accuracy. So that's pretty amazing stuff.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在5个折叠上进行时，我们会发现我们的结果甚至比我们想象的要好！98%的准确率。这非常棒！事实上，在几次运行中我们都获得了完美的准确率。这真是令人惊讶的事情。
- en: 'Now let''s see if we can do even better. We used a linear kernel before, what
    if we used a polynomial kernel and got even fancier? Will that be overfitting
    or will it actually better fit the data that we have? That kind of depends on
    whether there''s actually a linear relationship or polynomial relationship between
    these petal measurements and the actual species or not. So, let''s try that out:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看是否可以做得更好。我们之前使用了线性核，如果我们使用多项式核并变得更加花哨会怎样呢？那会是过拟合还是实际上更好地拟合了我们的数据？这取决于这些花瓣测量和实际物种之间是否实际上存在线性关系或多项式关系。所以，让我们试一试：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We''ll just run this all again, using the same technique. But this time, we''re
    using a polynomial kernel. We''ll fit that to our training dataset, and it doesn''t
    really matter where you fit to in this case, because `cross_val_score()` will
    just keep re-running it for you:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次运行所有这些，使用相同的技术。但这次，我们使用多项式核。我们将将其拟合到我们的训练数据集上，而在这种情况下，拟合到哪里并不重要，因为`cross_val_score()`会为您不断重新运行它：
- en: '![](img/705c48f3-a3b4-4d63-ae15-c4159c0e5c76.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/705c48f3-a3b4-4d63-ae15-c4159c0e5c76.png)'
- en: It turns out that when we use a polynomial fit, we end up with an overall score
    that's even lower than our original run. So, this tells us that the polynomial
    kernel is probably overfitting. When we use k-fold cross-validation it reveals
    an actual lower score than with our linear kernel.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，当我们使用多项式拟合时，最终得分甚至比我们原始运行的得分还要低。这告诉我们多项式核可能是过拟合的。当我们使用k折交叉验证时，它显示出的得分比线性核还要低。
- en: The important point here is that if we had just used a single train/test split,
    we wouldn't have realized that we were overfitting. We would have actually gotten
    the same result if we just did a single train/test split here as we did on the
    linear kernel. So, we might inadvertently be overfitting our data there, and not
    have even known it had we not use k-fold cross-validation. So, this is a good
    example of where k-fold comes to the rescue, and warns you of overfitting, where
    a single train/test split might not have caught that. So, keep that in your tool
    chest.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要一点是，如果我们只使用了单一的训练/测试拆分，我们就不会意识到我们过拟合了。如果我们只是在这里进行了单一的训练/测试拆分，我们实际上会得到与线性核相同的结果。因此，我们可能会无意中过拟合我们的数据，并且甚至不知道我们没有使用k折交叉验证时。因此，这是k折交叉验证拯救的一个很好的例子，并警告您过拟合，而单一的训练/测试拆分可能无法发现。因此，请将其放入您的工具箱。
- en: If you want to play around with this some more, go ahead and try different degrees.
    So, you can actually specify a different number of degrees. The default is 3 degrees
    for the polynomial kernel, but you can try a different one, you can try two.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想进一步尝试，可以尝试不同的次数。因此，您实际上可以指定不同的次数。多项式核的默认次数是3次，但您可以尝试不同的次数，可以尝试两次。
- en: Does that do better? If you go down to one, that degrades basically to a linear
    kernel, right? So, maybe there is still a polynomial relationship and maybe it's
    only a second degree polynomial. Try it out and see what you get back. That's
    k-fold cross-validation. As you can see, it's very easy to use thanks to scikit-learn.
    It's an important way to measure how good your model is in a very robust manner.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做会更好吗？如果你降到一次，基本上就会退化为线性核，对吧？所以，也许仍然存在多项式关系，也许只是二次多项式。试一试，看看你得到什么。这就是k折交叉验证。正如你所看到的，由于scikit-learn的便利性，它非常容易使用。这是衡量模型质量的重要方式。
- en: Data cleaning and normalisation
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据清洗和归一化
- en: Now, this is one of the simplest, but yet it might be the most important section
    in this whole book. We're going to talk about cleaning your input data, which
    you're going to spend a lot of your time doing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是最简单的部分之一，但它可能是整本书中最重要的部分。我们将讨论清理输入数据，这将占用您大部分的时间。
- en: How well you clean your input data and understand your raw input data is going
    to have a huge impact on the quality of your results - maybe even more so than
    what model you choose or how well you tune your models. So, pay attention; this
    is important stuff!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您清理输入数据的程度以及了解原始输入数据将对您的结果质量产生巨大影响 - 甚至可能比您选择的模型或调整模型的效果更大。所以，请注意；这很重要！
- en: Cleaning your raw input data is often the most important, and time-consuming,
    part of your job as a data scientist!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 清理原始输入数据通常是数据科学家工作中最重要且耗时的部分！
- en: Let's talk about an inconvenient truth of data science, and that's that you
    spend most of your time actually just cleaning and preparing your data, and actually
    relatively little of it analyzing it and trying out new algorithms. It's not quite
    as glamorous as people might make it out to be all the time. But, this is an extremely
    important thing to pay attention to.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈数据科学的一个不便之真相，那就是你实际上大部分时间都在清理和准备数据，而相对较少的时间用于分析和尝试新的算法。这并不像人们经常说的那样光彩夺目。但是，这是一个非常重要的事情需要注意。
- en: There are a lot of different things that you might find in raw data. Data that
    comes in to you, just raw data, is going to be very dirty, it's going to be polluted
    in many different ways. If you don't deal with it it's going to skew your results,
    and it will ultimately end up in your business making the wrong decisions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据中可能会有很多不同的问题。送到你手上的原始数据会非常肮脏，以许多不同的方式被污染。如果你不处理它，它将会扭曲你的结果，并最终导致你的业务做出错误的决定。
- en: If it comes back that you made a mistake where you ingested a bunch of bad data
    and didn't account for it, didn't clean that data up, and what you told your business
    was to do something based on those results that later turn out to be completely
    wrong, you're going to be in a lot of trouble! So, pay attention!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最终发现你犯了一个错误，即摄入了大量错误数据却没有考虑清理它，然后基于这些结果告诉你的业务做一些后来被证明完全错误的事情，你将会陷入麻烦！所以，请注意！
- en: 'There are a lot of different kinds of problems and data that you need to watch
    out for:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多不同类型的问题和数据需要注意：
- en: '**Outliers**: So maybe you have people that are behaving kind of strangely
    in your data, and when you dig into them, they turn out to be data you shouldn''t
    be looking at the in first place. A good example would be if you''re looking at
    web log data, and you see one session ID that keeps coming back over, and over,
    and over again, and it keeps doing something at a ridiculous rate that a human
    could never do. What you''re probably seeing there is a robot, a script that''s
    being run somewhere to actually scrape your website. It might even be some sort
    of malicious attack. But at any rate, you don''t want that behavior data informing
    your models that are meant to predict the behavior of real human beings using
    your website. So, watching for outliers is one way to identify types of data that
    you might want to strip out of your model when you''re building it.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值**：也许你的数据中有一些行为看起来有点奇怪，当你深入挖掘时，发现这些数据根本不应该被看到。一个很好的例子是，如果你在查看网络日志数据时，发现一个会话ID一次又一次地重复出现，并且以一个人类无法做到的速度进行某些操作。你可能看到的是一个机器人，一个在某处运行的脚本实际上在抓取你的网站。甚至可能是某种恶意攻击。但无论如何，你不希望这些行为数据影响你的模型，这些模型旨在预测真正使用你的网站的人类的行为。因此，观察异常值是一种识别在构建模型时可能需要剔除的数据类型的方法。'
- en: '**Missing data**: What do you do when data''s just not there? Going back to
    the example of a web log, you might have a referrer in that line, or you might
    not. What do you do if it''s not there? Do you create a new classification for
    missing, or not specified? Or do you throw that line out entirely? You have to
    think about what the right thing to do is there.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失数据**：当数据不在那里时，你该怎么办？回到网络日志的例子，那一行可能有一个引荐者，也可能没有。如果没有怎么办？你是创建一个新的分类来表示缺失，还是完全丢弃那一行？你必须考虑在那里做什么才是正确的。'
- en: '**Malicious data**: There might be people trying to game your system, there
    might be people trying to cheat the system, and you don''t want those people getting
    away with it. Let''s say you''re making a recommender system. There could be people
    out there trying to fabricate behavior data in order to promote their new item,
    right? So, you need to be on the lookout for that sort of thing, and make sure
    that you''re identifying the shilling attacks, or other types of attacks on your
    input data, and filtering them out from results and don''t let them win.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**恶意数据**：可能有人试图操纵你的系统，可能有人试图欺骗系统，你不希望这些人得逞。比如说你正在制作一个推荐系统。可能有人试图捏造行为数据以推广他们的新项目，对吧？因此，你需要警惕这种情况，并确保你能识别出操纵攻击或其他类型的攻击，过滤掉它们的结果，不让它们得逞。'
- en: '**Erroneous data**: What if there''s a software bug somewhere in some system
    that''s just writing out the wrong values in some set of situations? It can happen.
    Unfortunately, there''s no good way for you to know about that. But, if you see
    data that just looks fishy or the results don''t make sense to you, digging in
    deeply enough can sometimes uncover an underlying bug that''s causing the wrong
    data to be written in the first place. Maybe things aren''t being combined properly
    at some point. Maybe sessions aren''t being held throughout the entire session.
    People might be dropping their session ID and getting new session IDs as they
    go through a website, for example.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误数据**：如果在某个系统中有软件错误，导致在某些情况下写入了错误的值，该怎么办？这种情况可能发生。不幸的是，你无法知道这一点。但是，如果你看到的数据看起来可疑，或者结果对你来说毫无意义，深入挖掘有时可以发现潜在的错误，导致错误数据首先被写入。也许在某个地方没有正确地组合事物。也许会话没有在整个会话期间保持。例如，人们可能在浏览网站时丢失他们的会话ID，并获得新的会话ID。'
- en: '**Irrelevant data**: A very simple one here. Maybe you''re only interested
    in data from New York City people, or something for some reason. In that case
    all the data from people from the rest of the world is irrelevant to what you''re
    trying to find out. The first thing you want to do is just throw all that data
    that away and restrict your data, whittle it down to the data that you actually
    care about.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无关数据：这里有一个非常简单的例子。也许你只对来自纽约市的人的数据感兴趣，或者出于某种原因。在这种情况下，来自世界其他地方的人的所有数据对于你想要找出的内容都是无关的。你首先要做的就是抛弃所有这些数据，并限制你的数据，将其减少到你真正关心的数据。
- en: '**Inconsistent data**: This is a huge problem. For example, in addresses, people
    can write the same address in many different ways: they might abbreviate street
    or they might not abbreviate street, they might not put street at the end of the
    street name at all. They might combine lines together in different ways, they
    might spell things differently, they might use a zip code in the US or zip plus
    4 code in the US, they might have a country on it, they might not have a country
    on it. You need to somehow figure out what are the variations that you see and
    how can you normalize them all together.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不一致的数据：这是一个巨大的问题。例如，在地址中，人们可以用许多不同的方式写相同的地址：他们可能缩写街道，也可能不缩写街道，他们可能根本不在街道名称后面加上“街”。他们可能以不同的方式组合行，可能拼写不同的东西，可能在美国使用邮政编码或美国的邮政编码加4位，可能在上面有一个国家，也可能没有国家。你需要想办法弄清楚你看到的变化是什么，以及如何将它们全部规范化在一起。
- en: Maybe I'm looking at data about movies. A movie might have different names in
    different countries, or a book might have different names in different countries,
    but they mean the same thing. So, you need to look out for these things where
    you need to normalize your data, where the same data can be represented in many
    different ways, and you need to combine them together in order to get the correct
    results.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也许我在研究有关电影的数据。一部电影在不同国家可能有不同的名称，或者一本书在不同国家可能有不同的名称，但它们意思相同。因此，你需要注意这些地方，需要对数据进行规范化处理，同样的数据可能以许多不同的方式表示，你需要将它们组合在一起以获得正确的结果。
- en: '**Formatting**: This can also be an issue; things can be inconsistently formatted.
    Take the example of dates: in the US we always do month, day, year (MM/DD/YY),
    but in other countries they might do day, month, year (DD/MM/YY), who knows. You
    need to be aware of these formatting differences. Maybe phone numbers have parentheses
    around the area code, maybe they don''t; maybe they have dashes between each section
    of the numbers, maybe they don''t; maybe social security numbers have dashes,
    maybe they don''t. These are all things that you need to watch out for, and you
    need to make sure that variations in formatting don''t get treated as different
    entities, or different classifications during your processing.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式化：这也可能是一个问题；事物可能格式不一致。以日期为例：在美国，我们总是按月、日、年（MM/DD/YY）的顺序，但在其他国家，他们可能按日、月、年（DD/MM/YY）的顺序，谁知道呢。你需要注意这些格式上的差异。也许电话号码的区号周围有括号，也许没有；也许数字的每个部分之间有破折号，也许没有；也许社会保障号码有破折号，也许没有。这些都是你需要注意的事情，你需要确保格式上的变化不会在处理过程中被视为不同的实体或不同的分类。
- en: 'So, there are lots of things to watch out for, and the previous list names
    just the main ones to be aware of. Remember: garbage in, garbage out. Your model
    is only as good as the data that you give to it, and this is extremely, extremely
    true! You can have a very simple model that performs very well if you give it
    a large amount of clean data, and it could actually outperform a complex model
    on a more dirty dataset.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有很多需要注意的事情，前面的列表只是需要注意的主要事项。记住：垃圾进，垃圾出。你的模型只有你给它的数据那么好，这是极其真实的！如果你给它大量干净的数据，甚至一个非常简单的模型也可以表现得非常好，而且实际上可能会胜过一个更复杂的模型在一个更脏的数据集上。
- en: Therefore, making sure that you have enough data, and high-quality data is often
    most of the battle. You'd be surprised how simple some of the most successful
    algorithms used in the real world are. They're only successful by virtue of the
    quality of the data going into it, and the amount of data going into it. You don't
    always need fancy techniques to get good results. Often, the quality and quantity
    of your data counts just as much as anything else.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，确保你有足够的数据和高质量的数据通常是大部分工作。你会惊讶于现实世界中一些最成功的算法有多简单。它们之所以成功，仅仅是因为输入的数据质量和数量。你并不总是需要花哨的技术来获得好的结果。通常情况下，你的数据的质量和数量同其他任何因素一样重要。
- en: Always question your results! You don't want to go back and look for anomalies
    in your input data only when you get a result that you don't like. That will introduce
    an unintentional bias into your results where you're letting results that you
    like, or expect, go through unquestioned, right? You want to question things all
    the time to make sure that you're always looking out for these things because
    even if you find a result you like, if it turns out to be wrong, it's still wrong,
    it's still going to be informing your company in the wrong direction. That could
    come back to bite you later on.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 始终质疑你的结果！你不希望在得到不喜欢的结果时才回头查看你的输入数据中的异常。这将在你的结果中引入一种无意识的偏见，你让你喜欢或期望的结果不经质疑地通过了，对吧？你需要一直质疑事物，以确保你一直留意这些事情，因为即使你找到了一个你喜欢的结果，如果结果是错误的，它仍然是错误的，它仍然会让你的公司朝错误的方向发展。这可能会在以后给你带来麻烦。
- en: As an example, I have a website called No-Hate News. It's non-profit, so I'm
    not trying to make any money by telling you about it. Let's say I just want to
    find the most popular pages on this website that I own. That sounds like a pretty
    simple problem, doesn't it? I should just be able to go through my web logs, and
    count up how many hits each page has, and sort them, right? How hard can it be?!
    Well, turns out it's really hard! So, let's dive into this example and see why
    it's difficult, and see some examples of real-world data cleanup that has to happen.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我有一个名为No-Hate News的网站。这是一个非营利性网站，所以我并不是在告诉你它来赚钱。假设我只想找到我拥有的这个网站上最受欢迎的页面。这听起来是一个相当简单的问题，不是吗？我应该只需要浏览我的网络日志，计算每个页面的点击次数，并对它们进行排序，对吧？有多难呢？嗯，事实证明这真的很难！所以，让我们深入探讨这个例子，看看为什么它很困难，并看看一些必须进行的真实世界数据清理的例子。
- en: Cleaning web log data
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理网络日志数据
- en: We're going to show the importance of cleaning your data. I have some web log
    data from a little website that I own. We are just going to try to find the top
    viewed pages on that website. Sounds pretty simple, but as you'll see, it's actually
    quite challenging! So, if you want to follow along, the `TopPages.ipynb` is the
    notebook that we're working from here. Let's start!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示清理数据的重要性。我有一些来自我拥有的小网站的网络日志数据。我们只是尝试找到该网站上最受欢迎的页面。听起来很简单，但正如您将看到的，实际上相当具有挑战性！所以，如果您想跟着做，`TopPages.ipynb`是我们在这里工作的笔记本。让我们开始吧！
- en: 'I actually have an access log that I took from my actual website. It''s a real
    HTTP access log from Apache and is included in your book materials. So, if you
    do want to play along here, make sure you update the path to move the access log
    to wherever you saved the book materials:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我实际上有一个从我的实际网站中获取的访问日志。这是Apache的真实HTTP访问日志，包含在您的书籍材料中。所以，如果您想参与其中，请确保更新路径，将访问日志移动到您保存书籍材料的位置：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Applying a regular expression on the web log
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网络日志上应用正则表达式
- en: 'So, I went and got the following little snippet of code off of the Internet
    that will parse an Apache access log line into a bunch of fields:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我去网上找了下面的一小段代码，它可以将Apache访问日志行解析成一堆字段：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code contains things like the host, the user, the time, the actual page
    request, the status, the referrer, `user_agent` (meaning which browser actually
    was used to view this page). It builds up what's called a regular expression,
    and we're using the `re` library to use it. That's basically a very powerful language
    for doing pattern matching on a large string. So, we can actually apply this regular
    expression to each line of our access log, and automatically group the bits of
    information in that access log line into these different fields. Let's go ahead
    and run this.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码包含主机、用户、时间、实际页面请求、状态、引用者、`user_agent`（表示用于查看此页面的浏览器）。它构建了一个称为正则表达式的东西，我们使用`re`库来使用它。这基本上是一种在大字符串上进行模式匹配的非常强大的语言。因此，我们可以将这个正则表达式应用到我们访问日志的每一行上，并自动将访问日志行中的信息部分分组到这些不同的字段中。让我们继续运行这个。
- en: The obvious thing to do here, let's just whip up a little script that counts
    up each URL that we encounter that was requested, and keeps count of how many
    times it was requested. Then we can sort that list and get our top pages, right?
    Sounds simple enough!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里要做的明显的事情是，让我们编写一个小脚本，计算我们遇到的每个URL被请求的次数，并记录它被请求的次数。然后我们可以对列表进行排序，得到我们的热门页面，对吧？听起来足够简单！
- en: So, we're going to construct a little Python dictionary called `URLCounts`.
    We're going to open up our log file, and for each line, we're going to apply our
    regular expression. If it actually comes back with a successful match for the
    pattern that we're trying to match, we'll say, Okay this looks like a decent line
    in our access log.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将构建一个名为`URLCounts`的小Python字典。我们将打开我们的日志文件，对于每一行，我们将应用我们的正则表达式。如果它实际上返回了成功匹配我们试图匹配的模式，我们会说，好的，这看起来像是我们访问日志中的一个不错的行。
- en: 'Let''s extract the request field out of it, which is the actual HTTP request,
    the page which is actually being requested by the browser. We''re going to split
    that up into its three components: it consists of an action, like get or post;
    the actual URL being requested; and the protocol being used. Given that information
    split out, we can then just see if that URL already exists in my dictionary. If
    so, I will increment the count of how many times that URL has been encountered
    by `1`; otherwise, I''ll introduce a new dictionary entry for that URL and initialize
    it to the value of `1`. I do that for every line in the log, sort the results
    in reverse order, numerically, and print them out:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从中提取请求字段，也就是浏览器实际请求的实际HTTP请求的页面。我们将把它分成三个部分：它包括一个动作，比如get或post；实际请求的URL；以及使用的协议。在得到这些信息后，我们可以看看该URL是否已经存在于我的字典中。如果是，我将增加该URL已经被遇到的次数`1`；否则，我将为该URL引入一个新的字典条目，并将其初始化为值`1`。我对日志中的每一行都这样做，以数字逆序排序结果，并将其打印出来：
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'So, let''s go ahead and run that:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们继续运行：
- en: '![](img/5299efb0-51ac-451c-b6ac-43d72797555d.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5299efb0-51ac-451c-b6ac-43d72797555d.png)'
- en: Oops! We end up with this big old error here. It's telling us that, we need
    more than `1` value to unpack. So apparently, we're getting some requests fields
    that don't contain an action, a URL, and a protocol that they contain something
    else.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！我们遇到了一个大错误。它告诉我们，我们需要多于`1`个值来解包。所以显然，我们得到了一些不包含动作、URL和协议的请求字段，而包含其他内容。
- en: Let's see what's going on there! So, if we print out all the requests that don't
    contain three items, we'll see what's actually showing up. So, what we're going
    to do here is a similar little snippet of code, but we're going to actually do
    that split on the request field, and print out cases where we don't get the expected
    three fields.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看那里发生了什么！所以，如果我们打印出所有不包含三个项目的请求，我们就会看到实际显示的内容。所以，我们要做的是一个类似的小代码片段，但我们要在请求字段上实际执行拆分，并打印出我们没有得到预期的三个字段的情况。
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s see what''s actually in there:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实际上有什么：
- en: '![](img/2e3d1bcb-e929-4fa6-a80f-8616c134b54d.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e3d1bcb-e929-4fa6-a80f-8616c134b54d.png)'
- en: So, we have a bunch of empty fields. That's our first problem. But, then we
    have the first field that's full just garbage. Who knows where that came from,
    but it's clearly erroneous data. Okay, fine, let's modify our script.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们有一堆空字段。这是我们的第一个问题。但是，然后我们有第一个字段是完全垃圾。谁知道那是从哪里来的，但显然是错误的数据。好吧，让我们修改我们的脚本。
- en: Modification one - filtering the request field
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改一 - 过滤请求字段
- en: 'We''ll actually just throw out any lines that don''t have the expected 3 fields
    in the request. That seems like a legitimate thing to do, because this does in
    fact have completely useless data inside of it, it''s not like we''re missing
    out on anything here by doing that. So, we''ll modify our script to do that. We''ve
    introduced an `if (len(fields) == 3)` line before it actually tries to process
    it. We''ll run that:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上会丢弃任何没有预期的3个字段的行。这似乎是一个合理的做法，因为事实上这确实包含了完全无用的数据，这样做并不会让我们错过任何东西。所以，我们将修改我们的脚本来做到这一点。我们在实际尝试处理之前引入了一个`if
    (len(fields) == 3)`行。我们将运行它：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Hey, we got a result!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，我们得到了一个结果！
- en: '![](img/0f8ec1d7-97ce-4c7c-b1ed-ff868e626183.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f8ec1d7-97ce-4c7c-b1ed-ff868e626183.png)'
- en: But this doesn't really look like the top pages on my website. Remember, this
    is a news site. So, we're getting a bunch of PHP file hits, that's Perl scripts.
    What's going on there? Our top result is this `xmlrpc.php` script, and then `WP_login.php`,
    followed by the homepage. So, not very useful. Then there is `robots.txt`, then
    a bunch of XML files.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 但这看起来并不像是我网站上的热门页面。记住，这是一个新闻网站。所以，我们得到了一堆PHP文件点击，那是Perl脚本。那是怎么回事？我们的最佳结果是这个`xmlrpc.php`脚本，然后是`WP_login.php`，然后是主页。所以，没有什么用。然后是`robots.txt`，然后是一堆XML文件。
- en: You know when I looked into this later on, it turned out that my site was actually
    under a malicious attack; someone was trying to break into it. This `xmlrpc.php`
    script was the way they were trying to guess at my passwords, and they were trying
    to log in using the login script. Fortunately, I shut them down before they could
    actually get through to this website.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，当我后来调查这个问题时，结果发现我的网站实际上受到了恶意攻击；有人试图侵入。这个`xmlrpc.php`脚本是他们试图猜测我的密码的方式，他们试图使用登录脚本登录。幸运的是，在他们真正进入这个网站之前，我就把他们关掉了。
- en: This was an example of malicious data being introduced into my data stream that
    I have to filter out. So, by looking at that, we can see that not only was that
    malicious attack looking at PHP files, but it was also trying to execute stuff.
    It wasn't just doing a get request, it was doing a post request on the script
    to actually try to execute code on my website.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个恶意数据被引入到我的数据流中，我必须过滤掉的例子。所以，通过观察，我们可以看到这次恶意攻击不仅查看了PHP文件，而且还试图执行一些东西。它不仅仅是一个get请求，它是对脚本的post请求，实际上试图在我的网站上执行代码。
- en: Modification two - filtering post requests
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改二 - 过滤post请求
- en: 'Now, I know that the data that I care about, you know in the spirit of the
    thing I''m trying to figure out is, people getting web pages from my website.
    So, a legitimate thing for me to do is to filter out anything that''s not a get
    request, out of these logs. So, let''s do that next. We''re going to check again
    if we have three fields in our request field, and then we''re also going to check
    if the action is get. If it''s not, we''re just going to ignore that line entirely:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我知道我关心的数据，你知道我试图弄清楚的事情的精神是，人们从我的网站获取网页。所以，我可以合理地做的一件事是，过滤掉这些日志中不是get请求的任何内容。所以，让我们接着做这个。我们将再次检查我们的请求字段中是否有三个字段，然后我们还将检查操作是否是get。如果不是，我们将完全忽略该行：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We should be getting closer to what we want now, the following is the output
    of the preceding code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该更接近我们想要的东西了，以下是前面代码的输出：
- en: '![](img/49ea2d9a-f169-429a-8873-46b8e18fbbf0.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49ea2d9a-f169-429a-8873-46b8e18fbbf0.png)'
- en: 'Yeah, this is starting to look more reasonable. But, it still doesn''t really
    pass a sanity check. This is a news website; people go to it to read news. Are
    they really reading my little blog on it that just has a couple of articles? I
    don''t think so! That seems a little bit fishy. So, let''s dive in a little bit,
    and see who''s actually looking at those blog pages. If you were to actually go
    into that file and examine it by hand, you would see that a lot of these blog
    requests don''t actually have any user agent on them. They just have a user agent
    of `-`, which is highly unusual:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这开始看起来更合理了。但是，它仍然没有真正通过合理性检查。这是一个新闻网站；人们去那里是为了阅读新闻。他们真的在看我那个只有几篇文章的小博客吗？我不这么认为！这似乎有点可疑。所以，让我们深入一点，看看到底是谁在看那些博客页面。如果你真的去查看那个文件并手动检查，你会发现很多这些博客请求实际上根本没有任何用户代理。它们只有一个用户代理是`-`，这是非常不寻常的：
- en: '![](img/b4326ae3-6106-4c4d-8aff-af6acc33730a.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4326ae3-6106-4c4d-8aff-af6acc33730a.png)'
- en: If a real human being with a real browser was trying to get this page, it would
    say something like Mozilla, or Internet Explorer, or Chrome or something like
    that. So, it seems that these requests are coming from some sort of a scraper.
    Again, potentially malicious traffic that's not identifying who it is.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个真正的人类和一个真正的浏览器试图获取这个页面，它会显示类似Mozilla、Internet Explorer、Chrome或其他类似的东西。所以，看起来这些请求来自某种刮取器。同样，可能是一种恶意流量，没有标识出是谁。
- en: Modification three - checking the user agents
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改三 - 检查用户代理
- en: 'Maybe, we should be looking at the UserAgents too, to see if these are actual
    humans making requests, or not. Let''s go ahead and print out all the different
    UserAgents that we''re encountering. So, in the same spirit of the code that actually
    summed up the different URLs we were seeing, we can look at all the different
    UserAgents that we were seeing, and sort them by the most popular `user_agent`
    strings in this log:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 也许，我们应该也看看用户代理，看看这些是不是真正的人在发出请求。让我们继续打印出我们遇到的所有不同的用户代理。所以，按照实际总结我们看到的不同URL的代码精神，我们可以查看我们看到的所有不同用户代理，并按照日志中最流行的`user_agent`字符串对它们进行排序：
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We get the following result:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '![](img/bd809047-195b-4c82-b51a-b55840e01965.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd809047-195b-4c82-b51a-b55840e01965.png)'
- en: You can see most of it looks legitimate. So, if it's a scraper, and in this
    case it actually was a malicious attack but they were actually pretending to be
    a legitimate browser. But this dash `user_agent` shows up a lot too. So, I don't
    know what that is, but I know that it isn't an actual browser.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到大部分看起来都是合法的。所以，如果是一个刮取器，而在这种情况下实际上是一次恶意攻击，但他们实际上是在假装成一个合法的浏览器。但这个破折号`user_agent`也经常出现。所以，我不知道那是什么，但我知道那不是一个真正的浏览器。
- en: The other thing I'm seeing is a bunch of traffic from spiders, from web crawlers.
    So, there is Baidu which is a search engine in China, there is Googlebot just
    crawling the page. I think I saw Yandex in there too, a Russian search engine.
    So, our data is being polluted by a lot of crawlers that are just trying to mine
    our website for search engine purposes. Again, that traffic shouldn't count toward
    the intended purpose of my analysis, of seeing what pages these actual human beings
    are looking at on my website. These are all automated scripts.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到的另一件事是有很多来自蜘蛛、网络爬虫的流量。所以，有百度，这是中国的搜索引擎，有Googlebot在爬网页。我想我也在那里看到了Yandex，一个俄罗斯的搜索引擎。所以，我们的数据被很多只是为了挖掘我们网站的搜索引擎目的而爬行的爬虫所污染。再次强调，这些流量不应计入我分析的预期目的，即查看实际人类在我的网站上查看的页面。这些都是自动脚本。
- en: Filtering the activity of spiders/robots
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过滤蜘蛛/机器人的活动
- en: 'Alright, so this gets a little bit tricky. There''s no real good way of identifying
    spiders or robots just based on the user string alone. But we can at least take
    a legitimate crack at it, and filter out anything that has the word "bot" in it,
    or anything from my caching plugin that might be requesting pages in advance as
    well. We''ll also strip out our friend single dash. So, we will once again refine
    our script to, in addition to everything else, strip out any UserAgents that look
    fishy:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这变得有点棘手。仅仅根据用户字符串来识别蜘蛛或机器人没有真正好的方法。但我们至少可以试一试，过滤掉任何包含“bot”这个词的东西，或者来自我的缓存插件的可能提前请求页面的东西。我们还将去除我们的朋友单破折号。所以，我们将再次完善我们的脚本，除了其他一切，还要去除任何看起来可疑的UserAgents：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: What do we get?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了什么？
- en: '![](img/c8f00b0c-8e18-4276-8046-548376597f18.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8f00b0c-8e18-4276-8046-548376597f18.png)'
- en: 'Alright, so here we go! This is starting to look more reasonable for the first
    two entries, the homepage is most popular, which would be expected. Orlando headlines
    is also popular, because I use this website more than anybody else, and I live
    in Orlando. But after that, we get a bunch of stuff that aren''t webpages at all:
    a bunch of scripts, a bunch of CSS files. Those aren''t web pages.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，我们开始了！前两个条目看起来更合理了，主页最受欢迎，这是预料之中的。奥兰多头条也很受欢迎，因为我比其他人更多地使用这个网站，而且我住在奥兰多。但之后，我们得到了一堆根本不是网页的东西：一堆脚本，一堆CSS文件。这些都不是网页。
- en: Modification four - applying website-specific filters
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改四 - 应用特定于网站的过滤器
- en: 'I can just apply some knowledge about my site, where I happen to know that
    all the legitimate pages on my site just end with a slash in their URL. So, let''s
    go ahead and modify this again, to strip out anything that doesn''t end with a
    slash:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我只需应用一些关于我的网站的知识，我碰巧知道我的网站上所有合法的页面都以它们的URL结尾斜杠。所以，让我们继续修改一下，去掉任何不以斜杠结尾的东西：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let's run that!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一下！
- en: '![](img/a5aa652b-c518-403f-821e-576828f6d14a.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5aa652b-c518-403f-821e-576828f6d14a.png)'
- en: Finally, we're getting some results that seem to make sense! So, it looks like,
    that the top page requested from actual human beings on my little No-Hate News
    site is the homepage, followed by `orlando-headlines`, followed by world news,
    followed by the comics, then the weather, and the about screen. So, this is starting
    to look more legitimate.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到了一些看起来合理的结果！看起来，从我小小的No-Hate News网站上实际人类请求的顶级页面是主页，然后是`orlando-headlines`，然后是世界新闻，然后是漫画，然后是天气，然后是关于页面。所以，这开始看起来更合理了。
- en: If you were to dig even deeper though, you'd see that there are still problems
    with this analysis. For example, those feed pages are still coming from robots
    just trying to get RSS data from my website. So, this is a great parable in how
    a seemingly simple analysis requires a huge amount of pre-processing and cleaning
    of the source data before you get results that make any sense.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你再深入一点，你会发现这个分析还存在问题。例如，那些feed页面仍然来自只是想从我的网站获取RSS数据的机器人。所以，这是一个很好的寓言，说明一个看似简单的分析需要大量的预处理和清理源数据，才能得到任何有意义的结果。
- en: Again, make sure the things you're doing to clean your data along the way are
    principled, and you're not just cherry-picking problems that don't match with
    your preconceived notions. So, always question your results, always look at your
    source data, and look for weird things that are in it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 再次确保你在清理数据时所做的事情是有原则的，而不是只是挑选与你先入为主观念不符的问题。所以，始终质疑你的结果，始终查看你的源数据，并寻找其中的奇怪之处。
- en: Activity for web log data
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络日志数据的活动
- en: Alright, if you want to mess with this some more you can solve that feed problem.
    Go ahead and strip out things that include feed because we know that's not a real
    web page, just to get some familiarity with the code. Or, go look at the log a
    little bit more closely, gain some understanding as to where those feed pages
    are actually coming from.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，如果你想再深入研究一下，你可以解决那个feed问题。继续去除包括feed的东西，因为我们知道那不是一个真正的网页，只是为了熟悉代码。或者，更仔细地查看日志，了解那些feed页面实际来自哪里。
- en: 'Maybe there''s an even better and more robust way of identifying that traffic
    as a larger class. So, feel free to mess around with that. But I hope you learned
    your lesson: data cleaning - hugely important and it''s going to take a lot of
    your time!'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 也许有一种更好、更健壮的方法来识别那些流量作为一个更大的类别。所以，随意尝试一下。但我希望你已经学到了教训：数据清理 - 非常重要，而且会花费你大量的时间！
- en: So, it's pretty surprising how hard it was to get some reasonable results on
    a simple question like "What are the top viewed pages on my website?" You can
    imagine if that much work had to go into cleaning the data for such a simple problem,
    think about all the nuanced ways that dirty data might actually impact the results
    of more complex problems, and complex algorithms.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，令人惊讶的是，要在一个简单的问题上获得一些合理的结果，比如“我的网站上哪些页面被浏览次数最多？”竟然是多么困难。你可以想象，如果为了解决这样一个简单的问题需要做这么多工作，那么想想脏数据可能会如何影响更复杂问题和复杂算法的结果。
- en: It's very important to understand your source data, look at it, look at a representative
    sample of it, make sure you understand what's coming into your system. Always
    question your results and tie it back to the original source data to see where
    questionable results are coming from.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 非常重要的是要了解你的数据源，查看它，查看它的代表样本，确保你了解数据输入系统。始终质疑你的结果，并将其与原始数据源联系起来，看看可疑的结果是从哪里来的。
- en: Normalizing numerical data
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值数据的标准化
- en: 'This is a very quick section: I just want to remind you about the importance
    of normalizing your data, making sure that your various input feature data is
    on the same scale, and is comparable. And, sometimes it matters, and sometimes
    it doesn''t. But, you just have to be cognizant of when it does. Just keep that
    in the back of your head because sometimes it will affect the quality of your
    results if you don''t.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常快速的部分：我只是想提醒你关于标准化数据的重要性，确保你的各种输入特征数据在同一尺度上，并且是可比较的。有时很重要，有时不重要。但是，你必须意识到什么时候重要。只要记住这一点，因为有时如果你不这样做，它会影响你的结果的质量。
- en: So, sometimes models will be based on several different numerical attributes.
    If you remember multivariant models, we might have different attributes of a car
    that we're looking at, and they might not be directly comparable measurements.
    Or, for example, if we're looking at relationships between ages and incomes, ages
    might range from 0 to 100, but incomes in dollars might range from 0 to billions,
    and depending on the currency it could be an even larger range! Some models are
    okay with that.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候模型将基于几个不同的数值属性。如果你记得多变量模型，我们可能有不同的汽车属性，它们可能不是直接可比较的测量。或者，例如，如果我们正在研究年龄和收入之间的关系，年龄可能从0到100不等，但以美元计的收入可能从0到数十亿不等，根据货币的不同，范围可能更大！有些模型可以接受这种情况。
- en: If you're doing a regression, usually that's not a big deal. But, other models
    don't perform so well unless those values are scaled down first to a common scale.
    If you're not careful, you can end up with some attributes counting more than
    others. Maybe the income would end up counting much more than the age, if you
    were trying to treat those two values as comparable values in your model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在做回归，通常这不是什么大问题。但是，其他模型在这些值被缩放到一个公共尺度之前表现得不那么好。如果你不小心，你可能会发现一些属性比其他属性更重要。也许收入最终会比年龄更重要，如果你试图将这两个值作为模型中可比较的值来处理的话。
- en: So this can introduce also a bias in the attributes, which can also be a problem.
    Maybe one set of your data is skewed, you know, sometimes you need to normalize
    things versus the actual range seen for that set of values and not just to a 0
    to whatever the maximum is scale. There's no set rule as to when you should and
    shouldn't do this sort of normalization. All I can say is always read the documentation
    for whatever technique you're using.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可能导致属性的偏差，这也可能是一个问题。也许你的数据集中的一组数据是倾斜的，你知道，有时你需要对事物进行标准化，而不仅仅是将其标准化到0到最大值的范围。没有固定的规则来决定何时应该做这种标准化。我只能说的是，无论你使用什么技术，都要始终阅读文档。
- en: So, for example, in scikit-learn their PCA implementation has a `whiten` option
    that will automatically normalize your data for you. You should probably use that.
    It also has some preprocessing modules available that will normalize and scale
    things for you automatically as well.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在scikit-learn中，他们的PCA实现有一个`whiten`选项，它会自动为你标准化你的数据。你应该使用它。它还有一些预处理模块可用，可以自动为你标准化和缩放事物。
- en: Be aware too of textual data that should actually be represented numerically,
    or ordinally. If you have `yes` or `no` data you might need to convert that to
    `1` or `0` and do that in a consistent matter. So again, just read the documentation.
    Most techniques do work fine with raw, un-normalized data, but before you start
    using a new technique for the first time, just read the documentation and understand
    whether or not the inputs should be scaled or normalized or whitened first. If
    so, scikit-learn will probably make it very easy for you to do so, you just have
    to remember to do it! Don't forget to rescale your results when you're done if
    you are scaling the input data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意文本数据实际上应该以数字或顺序方式表示。如果你有`yes`或`no`的数据，你可能需要将其转换为`1`或`0`，并以一致的方式进行转换。所以再次，只需阅读文档。大多数技术在使用原始、未标准化的数据时都能很好地工作，但在第一次使用新技术之前，只需阅读文档，了解输入是否应该首先进行缩放、标准化或白化。如果是这样，scikit-learn可能会让你很容易地做到，你只需要记得这样做！在完成后不要忘记重新缩放你的结果，如果你正在缩放输入数据的话。
- en: If you want to be able to interpret the results you get, sometimes you need
    to scale them back up to their original range after you're done. If you are scaling
    things and maybe even biasing them towards a certain amount before you input them
    into a model, make sure that you unscale them and unbias them before you actually
    present those results to somebody. Or else they won't make any sense! And just
    a little reminder, a little bit of a parable if you will, always check to see
    if you should normalize or whiten your data before you pass it into a given model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要解释你得到的结果，有时你需要在完成后将它们重新缩放到原始范围。如果你在输入模型之前缩放事物，甚至可能使它们倾向于某个特定数量，确保在向某人呈现这些结果之前，你将它们重新缩放和去偏。否则它们就毫无意义了！还有一个小提醒，一个寓言，你应该始终检查是否应该在将数据传递到给定模型之前对其进行标准化或白化。
- en: No exercise associated with this section; it's just something I want you to
    remember. I'm just trying to drive the point home. Some algorithms require whitening,
    or normalization, some don't. So, always read the documentation! If you do need
    to normalize the data going into an algorithm it will usually tell you so, and
    it will make it very easy to do so. Please just be aware of that!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本节与运动无关；这只是我想让你记住的事情。我只是想强调一下。有些算法需要白化或标准化，有些则不需要。所以，请务必阅读文档！如果您确实需要对输入算法的数据进行标准化，它通常会告诉您如何做，而且会使这一过程变得非常容易。请注意这一点！
- en: Detecting outliers
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测异常值
- en: A common problem with real-world data is outliers. You'll always have some strange
    users, or some strange agents that are polluting your data, that act abnormally
    and atypically from the typical user. They might be legitimate outliers; they
    might be caused by real people and not by some sort of malicious traffic, or fake
    data. So sometimes, it's appropriate to remove them, sometimes it isn't. Make
    sure you make that decision responsibly. So, let's dive into some examples of
    dealing with outliers.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 真实世界数据的一个常见问题是异常值。您总会有一些奇怪的用户，或者一些奇怪的代理，它们会污染您的数据，表现出与典型用户不同的异常和非典型行为。它们可能是合法的异常值；它们可能是由真实人员而不是某种恶意流量或虚假数据引起的。因此，有时候适当地将它们移除，有时候则不适当。确保您负责任地做出这个决定。因此，让我们深入一些处理异常值的示例。
- en: For example, if I'm doing collaborative filtering, and I'm trying to make movie
    recommendations or something like that, you might have a few power users that
    have watched every movie ever made, and rated every movie ever made. They could
    end up having an inordinate influence on the recommendations for everybody else.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我正在进行协同过滤，并且试图进行电影推荐之类的事情，您可能会有一些超级用户，他们观看了每部电影，并对每部电影进行了评分。他们可能对每个人的推荐产生了不成比例的影响。
- en: You don't really want a handful of people to have that much power in your system.
    So, that might be an example where it would be a legitimate thing to filter out
    an outlier, and identify them by how many ratings they've actually put into the
    system. Or, maybe an outlier would be someone who doesn't have enough ratings.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您真的不希望少数人在您的系统中拥有如此大的权力。因此，这可能是一个例子，您可以合理地过滤掉异常值，并通过他们实际放入系统的评分数量来识别它们。或者，异常值可能是那些没有足够评分的人。
- en: We might be looking at web log data, like we saw in our example earlier when
    we were doing data cleaning, outliers could be telling you that there's something
    very wrong with your data to begin with. It could be malicious traffic, it could
    be bots, or other agents that should be discarded that don't represent actual
    human beings that you're trying to model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能正在查看网络日志数据，就像我们在之前的示例中看到的那样，当我们进行数据清理时，异常值可能会告诉您，从一开始您的数据就存在很大问题。这可能是恶意流量，可能是机器人，或者其他应该被丢弃的代理，它们并不代表您试图建模的实际人类。
- en: If someone really wanted the mean average income in the United States (and not
    the median), you shouldn't just throw out Donald Trump because you don't like
    him. You know the fact is, his billions of dollars are going to push that mean
    amount up, even if it doesn't budge the median. So, don't fudge your numbers by
    throwing out outliers. But throw out outliers if it's not consistent with what
    you're trying to model in the first place.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人真的想知道美国的平均收入（而不是中位数），您不应该仅仅因为您不喜欢他而丢弃唐纳德·特朗普。事实是，即使他的数十亿美元并没有改变中位数，但它们会推高平均数。因此，不要通过丢弃异常值来篡改您的数据。但如果它与您首先尝试建模的内容不一致，那么就丢弃异常值。
- en: Now, how do we identify outliers? Well, remember our old friend standard deviation?
    We covered that very early in this book. It's a very useful tool for detecting
    outliers. You can, in a very principled matter, compute the standard deviation
    of a dataset that should have a more or less normal distribution. If you see a
    data point that's outside of one or two standard deviations, there you have an
    outlier.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何识别异常值？嗯，还记得我们的老朋友标准差吗？我们在这本书的早期就讨论过这个问题。这是一个非常有用的工具，用于检测异常值。您可以以一种非常有原则的方式计算应该具有更或多或少正态分布的数据集的标准差。如果您看到一个数据点超出了一个或两个标准差，那么您就有一个异常值。
- en: Remember, we talked earlier too about the box and whisker diagrams too, and
    those also have a built-in way of detecting and visualizing outliers. Those diagrams
    define outliers as lying outside 1.5 the interquartile range.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们之前也谈到了箱线图和须状图，它们也有一种内置的方法来检测和可视化异常值。这些图表将异常值定义为位于1.5倍四分位距之外的值。
- en: What multiple do you choose? Well, you kind of have to use common sense, you
    know, there's no hard and fast rule as to what is an outlier. You have to look
    at your data and kind of eyeball it, look at the distribution, look at the histogram.
    See if there's actual things that stick out to you as obvious outliers, and understand
    what they are before you just throw them away.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您选择什么倍数？嗯，您必须运用常识，您知道，没有硬性规定什么是异常值。您必须查看您的数据，用眼睛观察，查看分布，查看直方图。看看是否有明显的异常值，并在丢弃它们之前了解它们是什么。
- en: Dealing with outliers
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理异常值
- en: 'So, let''s take some example code, and see how you might handle outliers in
    practice. Let''s mess around with some outliers. It''s a pretty simple section.
    A little bit of review actually. If you want to follow along, we''re in `Outliers.ipynb`.
    So, go ahead and open that up if you''d like:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看一些示例代码，看看您如何在实践中处理异常值。让我们玩弄一些异常值。这是一个非常简单的部分。实际上是一点点复习。如果您想跟着做，我们在`Outliers.ipynb`中。所以，如果您愿意，请打开它：
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We did something very similar, very early in the book, where we created a fake
    histogram of income distribution in the United States. What we're going to do
    is start off with a normal distribution of incomes here that are have a mean of
    $27,000 per year, with a standard deviation of 15,000\. I'm going to create 10,000
    fake Americans that have an income in that distribution. This is totally made-up
    data, by the way, although it's not that far off from reality.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在书的早期做过非常类似的事情，那里我们创建了美国收入分布的假直方图。我们要做的是从这里开始，用一个年收入平均为27000美元，标准偏差为15000美元的正态分布收入。我将创建10000个在该分布中有收入的假美国人。顺便说一句，这完全是虚构的数据，尽管它与现实并不那么遥远。
- en: Then, I'm going to stick in an outlier - call it Donald Trump, who has a billion
    dollars. We're going to stick this guy in at the end of our dataset. So, we have
    a normally distributed dataset around $27,000, and then we're going to stick in
    Donald Trump at the end.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我要插入一个异常值 - 叫它唐纳德·特朗普，他有十亿美元。我们将把这个家伙插入到我们数据集的末尾。所以，我们有一个围绕着27000美元的正态分布数据集，然后我们要在最后插入唐纳德·特朗普。
- en: 'We''ll go ahead and plot that as a histogram:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续将其绘制为直方图：
- en: '![](img/206367db-3697-4fa7-9c12-35da52e11457.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/206367db-3697-4fa7-9c12-35da52e11457.png)'
- en: Wow! That's not very helpful! We have the entire normal distribution of everyone
    else in the country squeezed into one bucket of the histogram. On the other hand,
    we have Donald Trump out at the right side screwing up the whole thing at a billion
    dollars.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！这并不是很有帮助！我们把全国其他人的整个正态分布挤进了直方图的一个桶里。另一方面，我们把唐纳德·特朗普放在右边，以十亿美元搞乱了整个事情。
- en: 'The other problem too is that if I''m trying to answer the question how much
    money does the typical American make. If I take the mean to try and figure that
    out, it''s not going to be a very good, useful number:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，如果我试图回答典型美国人赚多少钱这个问题。如果我用平均值来尝试弄清楚这个问题，那将不会是一个很好的、有用的数字：
- en: '[PRE14]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Donald Trump has pushed that number up all by himself to $126,000 and some odd
    of change, when I know that the real mean of my normally distributed data that
    excludes Donald Trump is only $27,000\. So, the right thing to do there would
    be to use the median instead of the mean.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 唐纳德·特朗普独自把这个数字推高到了126000美元，而我知道，不包括唐纳德·特朗普的正态分布数据的真实均值只有27000美元。所以，在这种情况下，正确的做法是使用中位数而不是平均值。
- en: But, let's say we had to use the mean for some reason, and the right way to
    deal with this would be to exclude these outliers like Donald Trump. So, we need
    to figure out how do we identify these people. Well, you could just pick some
    arbitrary cutoff, and say, "I'm going to throw out all the billionaires", but
    that's not a very principled thing to do. Where did 1 billion come from?
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，假设我们不得不出于某种原因使用平均值，正确的处理方式是排除像唐纳德·特朗普这样的异常值。所以，我们需要弄清楚如何识别这些人。嗯，你可以随意选择一个截断点，然后说，“我要抛弃所有亿万富翁”，但这不是一个很有原则的做法。10亿是从哪里来的？
- en: It's just some accident of how we count numbers. So, a better thing to do would
    be to actually measure the standard deviation of your dataset, and identify outliers
    as being some multiple of a standard deviation away from the mean.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是我们如何计算数字的一些意外。所以，更好的做法是实际测量数据集的标准偏差，并将异常值定义为距离平均值的某个标准偏差的倍数。
- en: 'So, following is a little function that I wrote that does just that. It''s
    called `reject_outliers()`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我写的一个小函数，它就是`reject_outliers()`：
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It takes in a list of data and finds the median. It also finds the standard
    deviation of that dataset. So, I filter that out, so I only preserve data points
    that are within two standard deviations of the median for my data. So, I can use
    this handy dandy `reject_outliers()` function on my income data, to actually strip
    out weird outliers automatically:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 它接收一个数据列表并找到中位数。它还找到该数据集的标准偏差。所以，我对此进行了过滤，只保留了在我的数据中距离中位数两个标准偏差之内的数据点。所以，我可以在我的收入数据上使用这个方便的`reject_outliers()`函数，自动剔除奇怪的异常值：
- en: '![](img/2e37aa6a-afb1-4ae4-97a3-2af86b800d90.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e37aa6a-afb1-4ae4-97a3-2af86b800d90.png)'
- en: Sure enough, it works! I get a much prettier graph now that excludes Donald
    Trump and focuses in on the more typical dataset here in the center. So, pretty
    cool stuff!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 果然，它奏效了！现在我得到了一个更漂亮的图表，排除了唐纳德·特朗普，聚焦于中心的更典型的数据集。所以，非常酷！
- en: So, that's one example of identifying outliers, and automatically removing them,
    or dealing with them however you see fit. Remember, always do this in a principled
    manner. Don't just throw out outliers because they're inconvenient. Understand
    where they're coming from, and how they actually affect the thing you're trying
    to measure in spirit.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这是一个识别异常值并自动删除它们或以任何你认为合适的方式处理它们的例子。记住，一定要以原则的方式做这件事。不要只是因为它们不方便就抛弃异常值。要理解它们来自何处，以及它们实际上如何影响你试图在精神上衡量的事物。
- en: By the way, our mean is also much more meaningful now; much closer to 27,000
    that it should be, now that we've gotten rid of that outlier.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，现在我们的平均值也更有意义了；现在我们已经摆脱了那个异常值，它更接近应该是的27000。
- en: Activity for outliers
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常值的活动
- en: So, if you want to play around with this, you know just fiddle around with it
    like I normally ask you to do. Try different multiples of the standard deviation,
    try adding in more outliers, try adding in outliers that aren't quite as outlier-ish
    as Donald Trump. You know, just fabricate some extra fake data there and play
    around with it, see if you can identify those people successfully.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你想玩玩这个，你知道，就像我通常要求你做的那样，试着用标准偏差的不同倍数，试着添加更多的异常值，试着添加不那么像唐纳德·特朗普那样的异常值。你知道，只是编造一些额外的假数据，然后玩弄一下，看看你是否能成功地识别出这些人。
- en: So there you have it! Outliers; pretty simple concept. So, that's an example
    of identifying outliers by looking at standard deviations, and just looking at
    the number of standard deviations from the mean or median that you care about.
    Median is probably a better choice actually, given that the outliers might be
    skewing the mean in and of themselves, right? So, by using the standard deviation,
    that's a good way of identifying outliers in a more principled manner than just
    picking some arbitrary cutoff. Again, you need to decide what the right thing
    to do is with those outliers. What are you actually trying to measure? Is it appropriate
    to actually discard them or not? So, keep that in your head!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！异常值；非常简单的概念。所以，这是一个通过查看标准偏差来识别异常值的示例，只需查看与平均值或中位数相差的标准偏差的数量。实际上，中位数可能是一个更好的选择，因为异常值可能会使平均值产生偏差，对吧？因此，使用标准偏差是一种比仅仅选择一些任意截断更有原则的识别异常值的方法。再次强调，您需要决定如何处理这些异常值。您实际上想要衡量什么？是否适合实际丢弃它们？所以，请记住这一点！
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we talked about the importance of striking a balance between
    bias and variance and minimizing error. Next, we saw the concept of k-fold cross-validation
    and how to implement it in Python to prevent overfitting. We learned the importance
    of cleaning data and normalizing it before processing it. We then saw an example
    to determine the popular pages of a website. In [Chapter 9](780734b2-bb5e-4810-9d50-a0b9a944ae47.xhtml),
    *Apache Spark - Machine Learning on Big Data* we'll learn machine learning on
    big data using Apache Spark.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们谈到了在偏差和方差之间取得平衡以及最小化误差的重要性。接下来，我们了解了k折交叉验证的概念以及如何在Python中实现它以防止过拟合。我们学到了在处理数据之前清洁数据和对数据进行归一化的重要性。然后我们看到了一个示例，用于确定网站的热门页面。在[第9章](780734b2-bb5e-4810-9d50-a0b9a944ae47.xhtml)中，《Apache
    Spark - 大数据上的机器学习》，我们将学习如何使用Apache Spark进行大数据上的机器学习。
