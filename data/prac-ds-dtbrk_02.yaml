- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Overview of ML on Databricks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks 上的 ML 概述
- en: This chapter will give you a fundamental understanding of how to get started
    with ML on Databricks. The ML workspace is data scientist-friendly and allows
    rapid ML development by providing out-of-the-box support for popular ML libraries
    such as TensorFlow, PyTorch, and many more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为你提供如何在 Databricks 上开始进行 ML 操作的基本理解。ML 工作区对数据科学家非常友好，支持通过提供对流行 ML 库（如 TensorFlow、PyTorch
    等）的开箱即用支持，快速进行 ML 开发。
- en: We will cover setting up a trial Databricks account and learn about the various
    ML-specific features available at ML practitioners’ fingertips in the Databricks
    workspace. You will learn how to start a cluster on Databricks and create a new
    notebook.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖如何设置 Databricks 试用账户，并了解 Databricks 工作区中为 ML 从业者提供的各种 ML 专用功能。你将学习如何在 Databricks
    上启动集群并创建新的笔记本。
- en: 'In this chapter, we will cover these main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Setting up a Databricks trial account
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 Databricks 试用账户
- en: Introduction to the ML workspace on Databricks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 上的 ML 工作区简介
- en: Exploring the workspace
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索工作区
- en: Exploring clusters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索集群
- en: Exploring notebooks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索笔记本
- en: Exploring data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据
- en: Exploring experiments
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索实验
- en: Discovering the feature store
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索特征存储
- en: Discovering the model registry
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索模型注册表
- en: Libraries
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库
- en: These topics will cover the essential features to perform effective **ML Operations**
    (**MLOps**) on Databricks. Links to the Databricks official documentation will
    also be included at relevant places if you wish to learn about a particular feature
    in more detail.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主题将涵盖在 Databricks 上执行有效**ML 操作**（**MLOps**）所需的基本功能。如果你希望更深入地了解某个特定功能，相关的 Databricks
    官方文档链接也会包含在适当的地方。
- en: Let’s look at how we can get access to a Databricks workspace.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何获取 Databricks 工作区的访问权限。
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you’ll need access to the Databricks workspace with cluster
    creation privileges. By default, the owner of the workspace has permission to
    create clusters. We will cover clusters in more detail in the *Exploring clusters*
    sections. You can read more about the various cluster access control options here:
    [https://docs.databricks.com/security/access-control/cluster-acl.html](https://docs.databricks.com/security/access-control/cluster-acl.html).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章内容，你需要拥有 Databricks 工作区的访问权限，并且具备集群创建权限。默认情况下，工作区的拥有者具有创建集群的权限。我们将在*探索集群*部分更详细地讨论集群。你可以在这里阅读更多关于不同集群访问控制选项的内容：[https://docs.databricks.com/security/access-control/cluster-acl.html](https://docs.databricks.com/security/access-control/cluster-acl.html)。
- en: Setting up a Databricks trial account
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Databricks 试用账户
- en: At the time of writing, Databricks is available on all the major cloud platforms,
    namely **Google Cloud Platform** (**GCP**), **Microsoft Azure**, and **Amazon
    Web** **Services** (**AWS**).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Databricks 已在所有主要云平台上可用，分别是**谷歌云平台**（**GCP**）、**微软 Azure** 和 **亚马逊 Web**
    **服务**（**AWS**）。
- en: Databricks provides an easy way to either create an account within the community
    edition or start a 14-day trial with all the enterprise features available in
    the workspace.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 提供了一种简单的方式，可以在社区版中创建账户，或开始为期14天的试用，试用期内将提供工作区中的所有企业功能。
- en: To fully leverage the code examples provided in this book and explore the enterprise
    features we’ll cover, I highly recommend taking advantage of the 14-day trial
    option. This trial will grant you access to all the necessary functionalities,
    ensuring a seamless experience throughout your learning journey.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用本书中提供的代码示例并探索我们将要介绍的企业功能，我强烈建议你利用14天的试用选项。这个试用将为你提供所有必要的功能，确保你在整个学习过程中拥有流畅的体验。
- en: 'Please go through this link to sign up for trial account: [https://www.databricks.com/try-databricks?itm_data=PricingPage-Trial#account](https://www.databricks.com/try-databricks?itm_data=PricingPage-Trial#account)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请通过以下链接注册试用账户：[https://www.databricks.com/try-databricks?itm_data=PricingPage-Trial#account](https://www.databricks.com/try-databricks?itm_data=PricingPage-Trial#account)
- en: 'On filling out the introductory form, you will be redirected to a page that
    will provide you with options to start with trial deployments on either of the
    three clouds or create a Community Edition account:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 填写完介绍表格后，你将被重定向到一个页面，在该页面你可以选择开始在三个云平台中的任一云上进行试用部署，或创建一个社区版账户：
- en: '![Figure 2.1 – How to get a free Databricks trial account](img/Figure_02.01_B17875.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 如何获得免费的 Databricks 试用账户](img/Figure_02.01_B17875.jpg)'
- en: Figure 2.1 – How to get a free Databricks trial account
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 如何获得免费的 Databricks 试用账户
- en: Once your signup is successful, you will receive an email describing how to
    log in to the Databricks workspace.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦注册成功，您将收到一封电子邮件，描述如何登录 Databricks 工作区。
- en: Note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Most of the features we will cover in this chapter will be accessible with the
    14-day trial option.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将介绍的大部分功能，在 14 天的试用期内均可使用。
- en: 'Once you log into the workspace, access the persona selector tab on the left
    navigation bar. We will change our persona to **Machine Learning**:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 登录工作区后，访问左侧导航栏中的角色选择器标签。我们将把我们的角色切换为**机器学习**：
- en: '![Figure 2.2 – The persona-based workspace switcher](img/Figure_02.02_B17875.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 基于角色的工作区切换器](img/Figure_02.02_B17875.jpg)'
- en: Figure 2.2 – The persona-based workspace switcher
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 基于角色的工作区切换器
- en: Now, let’s take a look at the new Databricks ML workspace features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看新的 Databricks ML 工作区功能。
- en: Exploring the workspace
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索工作区
- en: 'The **workspace** is within a Databricks ML environment. Each user of the Databricks
    ML environment will have a workspace. Users can create notebooks and develop code
    in isolation or collaborate with other teammates through granular access controls.
    You will be working within the workspace or repos for most of your time in the
    Databricks environment. We will learn more about repos in the *Repos* section:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**工作区**位于 Databricks ML 环境内。每个 Databricks ML 环境的用户都有一个工作区。用户可以独立创建笔记本和开发代码，或通过细粒度的访问控制与其他团队成员协作。您将在工作区或仓库中度过在
    Databricks 环境的大部分时间。我们将在 *仓库* 部分深入了解更多内容：'
- en: '![Figure 2.3 – The Workspace tab](img/Figure_02.03_B17875.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 工作区标签](img/Figure_02.03_B17875.jpg)'
- en: Figure 2.3 – The Workspace tab
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 工作区标签
- en: It’s important to note that the **Workspace** area is primarily intended for
    Databricks notebooks. While the workspace does support version control for notebooks
    using Git providers within Databricks, it’s worth highlighting that this version
    control capability within workspace notebooks is now considered less recommended
    compared to using repos.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，**工作区**区域主要用于 Databricks 笔记本。虽然工作区确实支持使用 Git 提供者进行版本控制，但需要强调的是，相较于使用仓库，工作区内的版本控制功能现在被认为不太推荐。
- en: Version control, in the context of software development, is a system that helps
    track changes made to files over time. It allows you to maintain a historical
    record of modifications, enabling collaboration, bug tracking, and reverting to
    previous versions if needed. In the case of Databricks, version control specifically
    refers to tracking changes made to notebooks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制，在软件开发中，是一种帮助跟踪文件随时间变化的系统。它允许您维护修改的历史记录，支持协作、错误跟踪，并在需要时恢复到先前版本。在 Databricks
    中，版本控制专门指的是跟踪笔记本中的变更。
- en: To enhance best practices, Databricks is transitioning away from relying solely
    on the version control feature within the workspace. Instead, it emphasizes the
    use of repos, which offers improved support for both Databricks and non-Databricks-specific
    files. This strategic shift provides a more comprehensive and versatile approach
    to managing code and files within the Databricks environment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加强最佳实践，Databricks 正在逐步摆脱仅依赖工作区内的版本控制功能。相反，它强调使用仓库，这种方法为 Databricks 和非 Databricks
    特定文件提供了更好的支持。这一战略转变为在 Databricks 环境中管理代码和文件提供了更加全面和多功能的方式。
- en: By utilizing repos, you can effectively manage and track changes not only to
    notebooks but also to other file types. This includes code files, scripts, configuration
    files, and more. Repos leverage popular version control systems such as Git, enabling
    seamless collaboration, branch management, code review workflows, and integration
    with external tools and services. Let’s look at the **Repos** feature, which was
    recently added to the workspace.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用仓库，您不仅可以有效地管理和跟踪笔记本的变更，还可以管理其他文件类型的变更。这包括代码文件、脚本、配置文件等。仓库使用流行的版本控制系统，如 Git，支持无缝协作、分支管理、代码审查工作流以及与外部工具和服务的集成。让我们来看看最近加入工作区的
    **仓库** 功能。
- en: Repos
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仓库
- en: 'Repos is short for repository. This convenient feature allows you to version
    control your code in the Databricks environment. Using repos, you can store arbitrary
    files within a Git repository. At the time of writing, Databricks supports the
    following Git providers:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库是 repository 的缩写。这个方便的功能让您可以在 Databricks 环境中对代码进行版本控制。使用仓库，您可以将任意文件存储在 Git
    仓库中。写作时，Databricks 支持以下 Git 提供者：
- en: GitHub
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub
- en: Bitbucket
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bitbucket
- en: GitLab
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitLab
- en: Azure DevOps
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure DevOps
- en: Databricks repos provide a logging mechanism to track and record various user
    interactions with a Git repository. These interactions include actions such as
    committing code changes and submitting pull requests. The repo features are also
    available through the REST **application programming interface** (**API**) ([https://docs.databricks.com/dev-tools/api/latest/repos.html](https://docs.databricks.com/dev-tools/api/latest/repos.html)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 仓库提供了一种日志机制，用于跟踪和记录用户与 Git 仓库的各种交互。这些交互包括提交代码更改和提交拉取请求等操作。仓库功能也可以通过
    REST **应用程序编程接口**（**API**）使用（[https://docs.databricks.com/dev-tools/api/latest/repos.html](https://docs.databricks.com/dev-tools/api/latest/repos.html)）。
- en: '![Figure 2.4 – The Repos tab](img/Figure_02.04_B17875.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – Repos 选项卡](img/Figure_02.04_B17875.jpg)'
- en: Figure 2.4 – The Repos tab
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – Repos 选项卡
- en: You can read more about how to set up repos for your environment at https://docs.databricks.com/repos.html#configure-your-git-integration-with-databricks.
    Repositories are essential for setting up your CI/CD processes in the Databricks
    environment. The **Repos** feature allows users to version their code and also
    allows reproducibility.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 https://docs.databricks.com/repos.html#configure-your-git-integration-with-databricks
    了解更多关于如何为你的环境设置仓库的信息。仓库对于在 Databricks 环境中设置 CI/CD 流程至关重要。**Repos** 功能允许用户对代码进行版本控制，并支持可重复性。
- en: '**Continuous integration/continuous deployment** (**CI/CD**) is a software
    development approach that involves automating the processes of integrating code
    changes, testing them, and deploying them to production environments. In the last
    chapter of this book, we will discuss more about the deployment paradigms in Databricks
    and CI/CD as part of your MLOps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**持续集成/持续部署**（**CI/CD**）是一种软件开发方法，涉及自动化集成代码更改、测试它们并将其部署到生产环境的过程。在本书的最后一章，我们将讨论更多关于
    Databricks 中的部署范式以及作为 MLOps 一部分的 CI/CD：'
- en: '![Figure 2.5 – The supported Git providers](img/Figure_02.05_B17875.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 支持的 Git 提供商](img/Figure_02.05_B17875.jpg)'
- en: Figure 2.5 – The supported Git providers
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 支持的 Git 提供商
- en: Now, let’s look at clusters, the central compute units for performing model
    training in the Databricks environment.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看集群，Databricks 环境中执行模型训练的核心计算单元。
- en: Exploring clusters
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索集群
- en: '**Clusters** are the primary computing units that will do the heavy lifting
    when you’re training your ML models. The VMs associated with a cluster are provisioned
    in Databricks users’ cloud subscriptions; however, the Databricks UI provides
    an interface to control the cluster type and its settings.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**集群**是进行机器学习模型训练时执行繁重计算任务的主要计算单元。与集群相关的虚拟机是在 Databricks 用户的云订阅中配置的；然而，Databricks
    用户界面提供了控制集群类型及其设置的界面。'
- en: 'Clusters are ephemeral compute resources. No data is stored on clusters:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 集群是短暂的计算资源。集群上不存储数据：
- en: '![Figure 2.6 – The Clusters tab](img/Figure_02.06_B17875.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 集群选项卡](img/Figure_02.06_B17875.jpg)'
- en: Figure 2.6 – The Clusters tab
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 集群选项卡
- en: 'The **Pools** feature allows end users to create Databricks VM pools. One of
    the benefits of working in the cloud environment is that you can request new compute
    resources on demand. The end user pays by the second and returns the compute once
    the load on the cluster is low. This is great; however, requesting a VM from the
    cloud provider, ramping it up, and adding it to a cluster still takes some time.
    Using pools, you can pre-provision VMs to keep them in a standby state. If a cluster
    requests new nodes and has access to the pool, then if the pool has the required
    VMs available, within seconds, these nodes will be added to the cluster, helping
    reduce the cluster scale uptime. Once the cluster is done processing high load
    or is terminated, the machine borrowed from the pool is returned to the pool and
    can be used by the next cluster. More about pools can be found here: [https://docs.databricks.com/clusters/instance-pools/index.html](https://docs.databricks.com/clusters/instance-pools/index.html).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pools** 功能允许最终用户创建 Databricks 虚拟机池。在云环境中工作的一个好处是你可以按需请求新的计算资源。最终用户按秒计费，并且在集群负载较低时归还计算资源。这很好；然而，从云服务提供商请求虚拟机，启动它并将其添加到集群中仍然需要一些时间。通过使用池，你可以预先配置虚拟机并将其保持在待命状态。如果集群请求新节点并且能够访问池，那么如果池中有所需的虚拟机可用，这些节点将在几秒钟内被添加到集群中，帮助减少集群的扩展启动时间。一旦集群完成高负载处理或终止，从池中借用的虚拟机会被归还到池中，可以被下一个集群使用。更多关于池的信息，请访问：[https://docs.databricks.com/clusters/instance-pools/index.html](https://docs.databricks.com/clusters/instance-pools/index.html)。'
- en: 'Databricks **jobs** allow users to automate code execution on a particular
    schedule. It has a lot of other valuable configurations around how many times
    you can retry code execution in case of failure and can set up alerts in case
    of failure. You can read a lot more about jobs here: [https://docs.databricks.com/data-engineering/jobs/jobs-quickstart.html](https://docs.databricks.com/data-engineering/jobs/jobs-quickstart.html).
    This link is for a Databricks workspace deployed on AWS; however, you can click
    on the **Change cloud** tab to match your deployment:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks**作业**允许用户在特定的时间表上自动执行代码。它还提供了许多其他有价值的配置选项，例如在代码执行失败时可以重试的次数，并能够在失败时设置警报。你可以在这里阅读更多关于作业的信息：[https://docs.databricks.com/data-engineering/jobs/jobs-quickstart.html](https://docs.databricks.com/data-engineering/jobs/jobs-quickstart.html)。该链接适用于在AWS上部署的Databricks工作区；不过，你可以点击**更改云**标签以匹配你的部署：
- en: '![Figure 2.7 – The dropdown for selecting documentation pertinent to your cloud](img/Figure_02.07_B17875.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – 选择与您云相关文档的下拉菜单](img/Figure_02.07_B17875.jpg)'
- en: Figure 2.7 – The dropdown for selecting documentation pertinent to your cloud
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 选择与您云相关文档的下拉菜单
- en: For now, let’s focus on the **Create** **Cluster** tab.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重点关注**创建** **集群**标签。
- en: 'If you are coming from an ML background where most of the work has been done
    on your laptop or a single isolated VM, then Databricks provides an easy way to
    get started by providing a single-node mode. In this case, you will get all the
    benefits of Databricks while working on a single-node cluster. The existing non-distributed
    code should run as is on this cluster. As an example, the following code will
    run only on the driver node as is in a non-distributed manner:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你来自机器学习领域，且大部分工作都是在笔记本或单独的虚拟机上完成的，那么Databricks提供了一种简单的方式来开始，提供单节点模式。在这种情况下，你将在单节点集群上使用Databricks的所有优势。现有的非分布式代码应该能在该集群上直接运行。例如，以下代码将在驱动节点上以非分布式方式运行：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Typically, a cluster refers to a collection of machines processing data in a
    distributed fashion. In the case of a single-node cluster, a single **VM** runs
    all the processes on multiple VMs in a regular cluster.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，集群指的是一组以分布式方式处理数据的机器。在单节点集群的情况下，一台**虚拟机（VM）**运行所有进程，而在常规集群中则有多个虚拟机共同运行。
- en: 'It is straightforward to start up a cluster in the Databricks environment.
    All the code provided in this book can be run on a single-node cluster. To spin
    up a single-node cluster, follow these steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在Databricks环境中启动集群非常简单。本书中提供的所有代码都可以在单节点集群上运行。要启动单节点集群，按照以下步骤操作：
- en: Give a name to the cluster.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给集群命名。
- en: Change the cluster mode to **Single Node**.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将集群模式更改为**单节点**。
- en: Set the latest ML runtime to **Databricks** **Runtime Version**.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最新的机器学习运行时设置为**Databricks** **运行时版本**。
- en: 'Click **Create cluster**:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建集群**：
- en: '![Figure 2.8 – The New Cluster screen](img/Figure_02.08_B17875.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.8 – 新建集群屏幕](img/Figure_02.08_B17875.jpg)'
- en: Figure 2.8 – The New Cluster screen
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – 新建集群屏幕
- en: This will start the process of provisioning our cluster. There are some advanced
    settings, such as adding tags, using init scripts, and connecting through JDBC
    to this cluster, that you can read about.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动集群配置过程。你可以阅读一些高级设置，如添加标签、使用初始化脚本以及通过JDBC连接到此集群等内容。
- en: '**Databricks Runtime** is a powerful platform that enhances big data analytics
    by improving the performance, security, and usability of Spark jobs. With features
    such as optimized I/O, enhanced security, and simplified operations, it offers
    a comprehensive solution. It comes in various flavors, including **ML** and **Photon**,
    catering to specific needs. Databricks Runtime is the ideal choice for running
    big data analytics workloads effectively. Databricks Runtime is powered by Delta
    Lake, which seamlessly integrates batch and streaming data to enable near-real-time
    analytics. Delta Lake’s capability to track data versions over time is crucial
    for reproducing ML model training and experimentation. This ensures data consistency
    and empowers reproducibility in your workflows. You can read more about Databricks
    Runtime here: [https://docs.databricks.com/runtime/index.html](https://docs.databricks.com/runtime/index.html).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**Databricks Runtime**是一个强大的平台，通过提高Spark作业的性能、安全性和可用性，增强了大数据分析功能。凭借优化的I/O、增强的安全性和简化的操作等功能，它提供了一个全面的解决方案。它有多种版本，包括**ML**和**Photon**，以满足特定需求。Databricks
    Runtime是高效运行大数据分析工作负载的理想选择。Databricks Runtime由Delta Lake提供支持，它无缝地集成了批处理和流数据，支持近实时分析。Delta
    Lake能够跟踪数据的版本变更，对于重现机器学习模型训练和实验至关重要。这确保了数据的一致性，并在你的工作流程中提供了可重现性。你可以在这里阅读更多关于Databricks
    Runtime的内容：[https://docs.databricks.com/runtime/index.html](https://docs.databricks.com/runtime/index.html)。'
- en: You will be using the ML runtime as an ML practitioner on Databricks. Databricks
    Runtime ML is a pre-built ML infrastructure that’s integrated with the capabilities
    of the Databricks workspace. It provides popular ML libraries such as TensorFlow
    and PyTorch, distributed training libraries such as Horovod, and pre-configured
    GPU support. With faster cluster creation and compatibility with installed libraries,
    it simplifies scaling ML and deep learning tasks. Additionally, it offers data
    exploration, cluster management, code and environment management, automation support,
    and integrated MLflow for model development and deployment.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Databricks上的机器学习实践者，你将使用ML运行时。Databricks Runtime ML是一个预构建的机器学习基础设施，它与Databricks工作区的功能集成。它提供了流行的机器学习库，如TensorFlow和PyTorch，分布式训练库，如Horovod，以及预配置的GPU支持。通过更快的集群创建和与已安装库的兼容性，它简化了机器学习和深度学习任务的扩展。此外，它还提供了数据探索、集群管理、代码和环境管理、自动化支持，以及集成的MLflow用于模型开发和部署。
- en: Databricks provides three different cluster access modes and their specific
    recommended use case patterns. All these cluster access modes can be used either
    in a multi-node (your cluster has a dedicated driver node and one or more executor
    nodes) or a single-node fashion (your cluster has a single node; both the driver
    program and executor programs run on a single node).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks提供了三种不同的集群访问模式及其特定的推荐使用案例模式。所有这些集群访问模式都可以在多节点模式（集群有一个专用的驱动节点和一个或多个执行节点）或单节点模式（集群仅有一个节点；驱动程序和执行程序都在一个节点上运行）下使用。
- en: Single user
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单一用户
- en: This mode is recommended for single users and data applications that can be
    developed using Python, Scala, SQL, and R. Clusters are set to terminate after
    120 minutes of inactivity, and the standard cluster is the default cluster mode.
    End users can also use this cluster to execute a notebook through a Databricks
    job using a scheduled activity. It is best to segregate different data processing
    pipelines into separate standard clusters. Segregating data pipelines prevents
    the failure of one cluster from affecting another. As Databricks charges customers
    by the second, this approach is viable and widely used. A cluster with this access
    type supports ML workloads.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此模式建议用于单一用户和可以使用Python、Scala、SQL和R开发的数据应用。集群会在120分钟无活动后终止，标准集群是默认的集群模式。最终用户也可以通过Databricks作业使用调度活动执行笔记本。最好将不同的数据处理管道划分到独立的标准集群中。划分数据管道可以防止一个集群的失败影响到另一个集群。由于Databricks按秒计费，这种方法是可行的并被广泛使用。具有此访问类型的集群支持机器学习工作负载。
- en: Shared
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享
- en: This mode is ideal when multiple users try to use the same cluster. It can provide
    maximum resource utilization and has lower query latency requirements in multiuser
    scenarios. Data applications can be developed using Python and SQL but not R and
    Scala. These clusters provide user isolation and also support ML workloads.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当多个用户尝试使用同一个集群时，此模式是理想的选择。它能够提供最大化的资源利用率，并且在多用户场景下具有较低的查询延迟要求。数据应用可以使用Python和SQL开发，但不能使用R和Scala。这些集群提供用户隔离，并且也支持机器学习工作负载。
- en: No isolation shared
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无隔离共享
- en: This type of cluster is intended only for admin users. We won’t cover too much
    about this type of access as this cluster type doesn’t support ML use cases.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的集群仅面向管理员用户。我们不会过多讨论此类访问，因为该集群类型不支持 ML 用例。
- en: 'You can read more about user isolation here: [https://docs.databricks.com/notebooks/notebook-isolation.html](https://docs.databricks.com/notebooks/notebook-isolation.html).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于用户隔离的内容：[https://docs.databricks.com/notebooks/notebook-isolation.html](https://docs.databricks.com/notebooks/notebook-isolation.html)。
- en: Let’s take a look at single-node clusters as this is the type of cluster you
    will be using to run the code that’s been shared as part of this book.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看单节点集群，因为这就是你将用来运行本书中共享代码的集群类型。
- en: Single-node clusters
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单节点集群
- en: Single-node clusters do not have worker nodes, and all the Python code runs
    on the driver node. These clusters are configured to terminate after 120 minutes
    of inactivity by default and can be used to build and test small data pipelines
    and do lightweight **exploratory data analysis** (**EDA**) and ML development.
    Python, Scala, SQL, and R are supported.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 单节点集群没有工作节点，所有的 Python 代码都在驱动节点上运行。默认情况下，这些集群会在 120 分钟没有活动后自动终止，适用于构建和测试小型数据管道，以及进行轻量级的**探索性数据分析**（**EDA**）和
    ML 开发。支持 Python、Scala、SQL 和 R。
- en: If you want to use specific libraries not included in the runtime, we will explore
    the various options to install the required libraries in the *Library* section
    of this chapter.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用运行时中未包含的特定库，我们将在本章的 *库* 部分探讨安装所需库的各种选项。
- en: Exploring notebooks
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索笔记本
- en: If you are familiar with **Jupyter** and **IPython notebooks**, then Databricks
    notebooks will look very familiar. A Databricks notebook development environment
    consists of cells where end users can interactively write code in R, Python, Scala,
    or SQL.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉**Jupyter**和**IPython 笔记本**，那么 Databricks 笔记本会显得非常熟悉。Databricks 笔记本开发环境由多个单元组成，用户可以在其中互动式地编写
    R、Python、Scala 或 SQL 代码。
- en: Databricks notebooks also have additional functionalities such as integration
    with the Spark UI, powerful integrated visualizations, version control, and an
    MLflow model tracking server. We can also parameterize a notebook and pass parameters
    to it at execution time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 笔记本还具有其他功能，如与 Spark UI 的集成、强大的集成可视化、版本控制以及 MLflow 模型跟踪服务器。我们还可以对笔记本进行参数化，并在执行时向其传递参数。
- en: 'We will cover notebooks in more detail as the code examples presented to you
    in this book utilize the Databricks notebook environment. Additional details about
    notebooks can be found at [https://docs.databricks.com/notebooks/index.html](https://docs.databricks.com/notebooks/index.html):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续详细介绍笔记本，因为本书中提供的代码示例利用了 Databricks 笔记本环境。有关笔记本的更多详细信息，请访问[https://docs.databricks.com/notebooks/index.html](https://docs.databricks.com/notebooks/index.html)：
- en: '![Figure 2.9 – Databricks notebooks](img/Figure_02.09_B17875.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – Databricks 笔记本](img/Figure_02.09_B17875.jpg)'
- en: Figure 2.9 – Databricks notebooks
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – Databricks 笔记本
- en: Let’s take a look at the next feature on the **Data** tab, also called the Databricks
    metastore.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 **数据** 标签上的下一个功能，也称为 Databricks 元数据存储。
- en: Exploring data
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: 'By default, when a new workspace is deployed, it comes with a managed Hive
    **metastore**. A metastore allows you to register datasets in various formats
    such as **Comma-Separated Values** (**CSV**), **Parquet**, **Delta format**, **text**,
    or **JavaScript Object Notation** (**JSON**) as an external table ([https://docs.databricks.com/data/metastores/index.html](https://docs.databricks.com/data/metastores/index.html)).
    We will not go too much into detail about the metastore here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当部署新的工作区时，它会附带一个托管的 Hive **元数据存储**。元数据存储允许你以外部表的形式注册各种格式的数据集，如**逗号分隔值**（**CSV**）、**Parquet**、**Delta
    格式**、**文本**或**JavaScript 对象表示法**（**JSON**）（[https://docs.databricks.com/data/metastores/index.html](https://docs.databricks.com/data/metastores/index.html)）。我们在这里不会深入讨论元数据存储的细节：
- en: '![Figure 2.10 – The Data tab](img/Figure_02.10_B17875.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.10 – 数据标签](img/Figure_02.10_B17875.jpg)'
- en: Figure 2.10 – The Data tab
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 – 数据标签
- en: 'It’s all right if you are not familiar with the term metastore. In simple terms,
    it is similar to a relational database. In relational databases, there are databases
    and then table names and schemas. The end user can use SQL to interact with the
    data stored in databases and tables. Similarly, in Databricks, end users can decide
    to register datasets stored in cloud storage so that they’re available as tables.
    You can learn more here: [https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html](https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉“元存储”这个术语，也没关系。简单来说，它类似于关系数据库。在关系数据库中，有数据库，然后是表名和模式。最终用户可以使用 SQL 与存储在数据库和表中的数据交互。同样，在
    Databricks 中，最终用户可以选择注册存储在云存储中的数据集，以便它们可以作为表格使用。您可以在这里了解更多：[https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html](https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html)。
- en: The Hive metastore provides a means of implementing access control by utilizing
    table access control lists for local users within the workspace. However, to enhance
    data access governance and ensure unified control over various assets such as
    deployed models and AI assets, Databricks has introduced Unity Catalog as a best
    practice solution. This enables comprehensive management and governance across
    multiple workspaces.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 元存储通过利用工作区内本地用户的表访问控制列表提供了一种实现访问控制的方法。然而，为了增强数据访问治理并确保对各种资产（如部署的模型和 AI
    资产）进行统一控制，Databricks 引入了 Unity Catalog 作为最佳实践解决方案。这使得跨多个工作区进行全面的管理和治理成为可能。
- en: Let’s understand Unity Catalog in a bit more detail.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解 Unity Catalog。
- en: 'Unity Catalog is a unified governance solution for data and AI assets on the
    lakehouse. It provides centralized access control, auditing, lineage, and data
    discovery capabilities across Databricks workspaces:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Unity Catalog 是湖仓中数据和 AI 资产的统一治理解决方案。它提供了跨 Databricks 工作区的集中访问控制、审计、血缘分析和数据发现功能：
- en: '![Figure 2.11 – Unity Catalog’s relationship to workspaces](img/Figure_02.11_B17875.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.11 – Unity Catalog 与工作区的关系](img/Figure_02.11_B17875.jpg)'
- en: Figure 2.11 – Unity Catalog’s relationship to workspaces
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11 – Unity Catalog 与工作区的关系
- en: 'Here are some of the key features of Unity Catalog:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 Unity Catalog 的一些关键特性：
- en: '**Define once, secure everywhere**: Unity Catalog administers data access policies
    across all workspaces and personas from a single place'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义一次，处处安全**：Unity Catalog 从一个地方管理跨所有工作区和角色的数据访问策略'
- en: '**Standards-compliant security model**: Unity Catalog’s security model is based
    on standard ANSI SQL and allows administrators to grant permissions in their existing
    data lake'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**符合标准的安全模型**：Unity Catalog 的安全模型基于标准 ANSI SQL，并允许管理员在现有数据湖中授予权限'
- en: '**Built-in auditing and lineage**: Unity Catalog captures user-level audit
    logs and lineage data, tracking how data assets are created and used across all
    languages and personas'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内建审计和血缘分析**：Unity Catalog 捕获用户级审计日志和血缘数据，跟踪数据资产在所有语言和角色中的创建和使用情况'
- en: '**Data discovery**: Unity Catalog provides a search interface to help data
    consumers find data and lets users tag and document data assets'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据发现**：Unity Catalog 提供了一个搜索界面，帮助数据消费者找到数据，并允许用户标记和记录数据资产'
- en: '**System tables (Public Preview)**: Unity Catalog provides operational data,
    including audit logs, billable usage, and lineage'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统表（公共预览）**：Unity Catalog 提供操作数据，包括审计日志、可计费使用情况和血缘分析'
- en: 'Let’s understand what the Unity Catalog object model looks like:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解 Unity Catalog 对象模型的具体样子：
- en: '![Figure 2.12 – The Unity Catalog object model](img/Figure_02.12_B17875.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.12 – Unity Catalog 对象模型](img/Figure_02.12_B17875.jpg)'
- en: Figure 2.12 – The Unity Catalog object model
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12 – Unity Catalog 对象模型
- en: 'The Unity Catalog’s hierarchy of primary data objects flows from metastore
    to table:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Unity Catalog 的主要数据对象层次结构从元存储到表格流动：
- en: '**Metastore**: The top-level container for metadata. Each metastore exposes
    a three-level namespace (**catalog.schema.table**) that organizes your data.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元存储**：元数据的顶级容器。每个元存储公开一个三级命名空间（**目录.模式.表**），用于组织您的数据。'
- en: '**Catalog**: This is the first layer of the object hierarchy and is used to
    organize your data assets.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目录**：这是对象层次结构的第一层，用于组织您的数据资产。'
- en: '**Schema**: Also known as databases, schemas contain tables and views.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式**：也称为数据库，模式包含表格和视图。'
- en: '**Table**: The lowest level in the object hierarchy are tables and views.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格**：对象层次结构中最低级别是表格和视图。'
- en: As mentioned previously, doing a deep dive into Unity Catalog is a big topic
    in itself and outside the scope of this book. Unity Catalog offers centralized
    governance, auditing, and data discovery capabilities for data and AI assets across
    Databricks workspaces. It provides a secure model based on ANSI SQL, automatic
    capture of user-level audit logs and data lineage, and a hierarchical data organization
    system. It also supports a variety of data formats, advanced identity management,
    specified admin roles for data governance, and is compatible with Databricks Runtime
    11.3 LTS or above.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深入了解 Unity Catalog 是一个非常庞大的话题，超出了本书的范围。Unity Catalog 为 Databricks 工作区中的数据和
    AI 资产提供集中式治理、审计和数据发现功能。它提供基于 ANSI SQL 的安全模型、自动捕获用户级审计日志和数据血缘关系、以及层级化的数据组织系统。它还支持多种数据格式、先进的身份管理、专门的管理员角色用于数据治理，并兼容
    Databricks Runtime 11.3 LTS 或更高版本。
- en: For a more comprehensive understanding of Unity Catalog, go to [https://docs.databricks.com/data-governance/unity-catalog/index.html](https://docs.databricks.com/data-governance/unity-catalog/index.html).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 想要更全面了解 Unity Catalog，请访问 [https://docs.databricks.com/data-governance/unity-catalog/index.html](https://docs.databricks.com/data-governance/unity-catalog/index.html)。
- en: All the features we’ve covered so far are standard among all the Databricks
    persona-specific features.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所覆盖的所有功能都是 Databricks 针对特定角色的标准功能。
- en: The following three features, namely experiments, the feature store, and models,
    are critical for the ML persona.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下三个功能——实验、特征存储和模型——对于机器学习角色至关重要。
- en: Let’s take a look at them one by one.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一一查看这些功能。
- en: Exploring experiments
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索实验
- en: As the name suggests, experiments are the central location where all the model
    training pertinent to business problems can be accessed. Users can define their
    name for the experiment or a default system-generated one and use it to train
    the different ML model training runs. Experiments in the Databricks UI come from
    integrating MLflow into the platform. We will dive deeper into MLflow in the coming
    chapters to understand more details; however, it’s important to get a sense of
    what MLflow is and some of the terminology that is MLflow-specific.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，实验是所有与业务问题相关的模型训练的集中位置，用户可以访问这些实验。用户可以为实验定义自己的名称，或者使用系统自动生成的默认名称，并用它来训练不同的机器学习模型。Databricks
    UI 中的实验来自将 MLflow 集成到平台中。我们将在接下来的章节中深入探讨 MLflow，以了解更多细节；然而，首先了解 MLflow 的基本概念及一些特定术语是非常重要的。
- en: 'MLflow is an open source platform for managing the end-to-end ML life cycle.
    Here are the key components of MLflow:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 是一个开源平台，用于管理端到端的机器学习生命周期。以下是 MLflow 的关键组件：
- en: '**Tracking**: This allows you to track experiments to record and compare parameters
    and results.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪**：这允许你跟踪实验，记录和比较参数与结果。'
- en: '**Models**: This component helps manage and deploy models from various ML libraries
    to a variety of model serving and inference platforms.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：此组件有助于管理和部署来自各种机器学习库的模型，支持多种模型服务和推理平台。'
- en: '**Projects**: This allows you to package ML code in a reusable, reproducible
    form so that you can share it with other data scientists or transfer it to production.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目**：这允许你以可重用、可复制的形式打包机器学习代码，以便与其他数据科学家共享或转移到生产环境中。'
- en: '**Model Registry**: This centralizes a model store for managing models’ full
    life cycle stage transitions: from staging to production, with capabilities for
    versioning and annotating. Databricks provides a managed version of the Model
    Registry in Unity Catalog.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型注册表**：这是一个集中式的模型存储库，用于管理模型的完整生命周期阶段转换：从预发布到生产，并具备版本控制和注释功能。Databricks 在
    Unity Catalog 中提供了一个托管版本的模型注册表。'
- en: '**Model Serving**: This allows you to host MLflow models as REST endpoints.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型服务**：这允许你将 MLflow 模型作为 REST 端点进行托管。'
- en: 'There are also certain terms specific to MLflow:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些专属于 MLflow 的术语：
- en: '**Run**: A run represents a specific instance of training an ML model. It comprises
    parameters, metrics, artifacts, and metadata associated with the training process.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行**：运行表示训练机器学习模型的具体实例。它包括与训练过程相关的参数、度量、工件和元数据。'
- en: '**Experiment**: An experiment serves as a container for organizing and tracking
    the results of ML experiments. It consists of multiple runs, allowing for easy
    comparison and analysis of different approaches.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验**：实验作为一个容器，用于组织和跟踪机器学习实验的结果。它由多个运行组成，便于不同方法的比较和分析。'
- en: '**Parameter**: A parameter refers to a configurable value that can be adjusted
    during the training of an ML model. These values influence the behavior and performance
    of the model.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数**：参数是指在训练机器学习模型过程中可以调整的可配置值。这些值影响模型的行为和性能。'
- en: '**Metric**: A metric is a quantitative measure that’s used to evaluate the
    performance of an ML model. Metrics provide insights into how well the model is
    performing on specific tasks or datasets.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度量**：度量是用于评估机器学习模型性能的定量指标。度量提供了有关模型在特定任务或数据集上表现如何的洞察。'
- en: '**Artifact**: An artifact refers to any output generated during an ML experiment.
    This can include files, models, images, or other relevant data that captures the
    results or intermediate stages of the experiment.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工件**：工件是指在机器学习实验期间生成的任何输出。这可以包括文件、模型、图像或捕获实验结果或中间阶段的其他相关数据。'
- en: '**Project**: A project encompasses the code, data, and configuration necessary
    to reproduce an ML experiment. It provides a structured and organized approach
    to managing all the components required for a specific ML workflow.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目**：项目包括重现机器学习实验所需的代码、数据和配置。它提供了一种结构化和组织化的方法，用于管理特定机器学习工作流程的所有组件。'
- en: '**Model**: A model represents a trained ML model that can be utilized to make
    predictions or perform specific tasks based on the learned patterns and information
    from the training data.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：模型表示经过训练的机器学习模型，可用于基于训练数据学到的模式和信息进行预测或执行特定任务。'
- en: '**Model registry**: A model registry serves as a centralized repository for
    storing and managing ML models. It provides versioning, tracking, and collaboration
    capabilities for different model versions and their associated metadata.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型注册表**：模型注册表作为集中存储和管理机器学习模型的库。它为不同模型版本及其关联的元数据提供版本控制、跟踪和协作能力。'
- en: '**Backend store**: The backend store is responsible for storing MLflow entities
    such as runs, parameters, metrics, and tags. It provides the underlying storage
    infrastructure for managing experiment data.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后端存储**：后端存储负责存储诸如运行、参数、度量和标签等MLflow实体。它为管理实验数据提供了基础存储基础设施。'
- en: '**Artifact store**: The artifact store is responsible for storing artifacts
    produced during ML experiments. This can include files, models, images, or any
    other relevant data that’s generated throughout the experimentation process.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工件存储**：工件存储负责存储在机器学习实验期间生成的工件。这可能包括文件、模型、图像或任何在实验过程中生成的其他相关数据。'
- en: '**Flavor**: A flavor represents a standardized way of packaging an ML model,
    allowing it to be easily consumed by specific tools or platforms. Flavors provide
    flexibility and interoperability when deploying and serving models.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风味**：风味代表一种标准化的ML模型打包方式，使其可以轻松被特定工具或平台消费。风味在部署和服务模型时提供了灵活性和互操作性。'
- en: '**UI**: The UI refers to the graphical interface provided by MLflow, allowing
    users to interact with and visualize experiment results, track runs, and manage
    models through an intuitive interface.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户界面（UI）**：UI指的是MLflow提供的图形界面，允许用户通过直观界面与实验结果进行交互和可视化，跟踪运行并管理模型。'
- en: 'MLflow also employs additional terms, but the ones mentioned here are some
    of the most commonly used. For further details, please consult the MLflow documentation:
    [https://mlflow.org/docs/latest/index.html](https://mlflow.org/docs/latest/index.html).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow还采用了其他术语，但此处提到的是一些最常用的。有关详细信息，请参阅MLflow文档：[https://mlflow.org/docs/latest/index.html](https://mlflow.org/docs/latest/index.html)。
- en: 'Databricks AutoML is fully integrated with MLflow, so all the model training
    and the artifacts that are generated are automatically logged in the MLflow server:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks AutoML与MLflow完全集成，因此生成的所有模型训练和工件都会自动记录在MLflow服务器中：
- en: '![Figure 2.13 – The Experiments tab](img/Figure_02.13_B17875.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图2.13 – 实验选项卡](img/Figure_02.13_B17875.jpg)'
- en: Figure 2.13 – The Experiments tab
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 – 实验选项卡
- en: End users can also utilize Databricks AutoML to start modeling a solution for
    their ML problems. Databricks has taken a different approach with its AutoML capability,
    called **glass** **box AutoML**.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最终用户也可以利用Databricks AutoML来为其机器学习问题建模。Databricks 在其AutoML能力中采用了一种不同的方法，称为**玻璃盒AutoML**。
- en: Databricks AutoML simplifies the workflow for ML practitioners by automatically
    generating comprehensive notebooks. These notebooks encompass all the necessary
    code for feature engineering and model training, covering various combinations
    of ML models and hyperparameters. This feature allows ML practitioners to thoroughly
    inspect the generated code and gain deeper insights into the process.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks AutoML 通过自动生成全面的笔记本简化了 ML 从业者的工作流程。这些笔记本涵盖了特征工程和模型训练所需的所有代码，涵盖了多种
    ML 模型和超参数的组合。此功能使 ML 从业者能够深入检查生成的代码，并获得对过程的更深层次理解。
- en: 'Databricks AutoML currently supports classification, regression, and forecasting
    models. For a list of algorithms that AutoML can use to create models, go to [https://docs.databricks.com/applications/machine-learning/automl.html#automl-algorithms](https://docs.databricks.com/applications/machine-learning/automl.html#automl-algorithms):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks AutoML 当前支持分类、回归和预测模型。有关 AutoML 可以使用哪些算法来创建模型的完整列表，请访问 [https://docs.databricks.com/applications/machine-learning/automl.html#automl-algorithms](https://docs.databricks.com/applications/machine-learning/automl.html#automl-algorithms)：
- en: '![Figure 2.14 – The default experiment is linked to Python notebooks by default](img/Figure_02.14_B17875.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.14 – 默认实验默认与 Python 笔记本连接](img/Figure_02.14_B17875.jpg)'
- en: Figure 2.14 – The default experiment is linked to Python notebooks by default
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14 – 默认实验默认与 Python 笔记本连接
- en: MLflow was developed in-house at Databricks to ease the end-to-end ML life cycle
    and MLOps. Since the launch of MLflow, it has been widely adopted and supported
    by the open source community.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 是 Databricks 内部开发的，用于简化端到端的 ML 生命周期和 MLOps。自 MLflow 发布以来，它已经被广泛采用并得到了开源社区的支持。
- en: Now, let’s look at the feature store.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看特征存储。
- en: Discovering the feature store
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现特征存储
- en: The **feature store** is a relatively new yet stable release in the latest Databricks
    ML workspace. Many organizations that have mature ML processes in place, such
    as Uber, Facebook, DoorDash, and many more, have internally implemented their
    feature stores.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征存储**是最新 Databricks ML 工作空间中的一个相对较新但稳定的版本。许多已经拥有成熟 ML 流程的组织，如 Uber、Facebook、DoorDash
    等，已经在内部实现了他们的特征存储。'
- en: ML life cycle management and workflows are complex. Forbes conducted a survey
    ([https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says))
    with data scientists and uncovered that managing data is the most expensive and
    time-consuming operation in their day-to-day work.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ML 生命周期管理和工作流程是复杂的。Forbes 进行了一项调查（[https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says)），并与数据科学家进行了交流，发现管理数据是他们日常工作中最昂贵且最耗时的操作。
- en: Data scientists need to spend a lot of time finding the data, cleaning it, doing
    EDA, and then performing feature engineering to train their ML models. This is
    an iterative process. The effort that needs to be put in to make the process repeatable
    is an enormous challenge. This is where feature stores come in.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家需要花费大量时间查找数据、清洗数据、进行 EDA，然后进行特征工程以训练他们的 ML 模型。这是一个迭代过程。为了使这个过程可以重复，需要投入的努力是一个巨大的挑战。这就是特征存储的作用所在。
- en: Databricks Feature Store is standardized on the open source Delta format, which
    allows data scientists to govern features similar to those used to govern access
    to models, notebooks, or jobs in the Databricks environment.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 特征存储采用开源 Delta 格式进行标准化，这使得数据科学家能够像管理模型、笔记本或工作任务的访问权限一样管理特征。
- en: 'Databricks Feature Store is unique in a couple of ways:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 特征存储有几个独特之处：
- en: 'It uses Delta Lake to store feature tables. This allows end users to read data
    from any of the supported languages and connectors outside of Databricks. More
    can be read here: [https://docs.delta.io/latest/delta-intro.html](https://docs.delta.io/latest/delta-intro.html).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用 Delta Lake 存储特征表。这使得最终用户能够从 Databricks 外部的任何受支持语言和连接器中读取数据。更多信息请访问： [https://docs.delta.io/latest/delta-intro.html](https://docs.delta.io/latest/delta-intro.html)。
- en: The integrated Feature Store UI within the Databricks ML workspace provides
    end-to-end traceability and lineage of how the features were generated and which
    downstream models use it in a single unified view. We will look at this in more
    detail in [*Chapter 3*](B17875_03.xhtml#_idTextAnchor063).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks ML工作区中的集成Feature Store UI提供了端到端的可追溯性和特征生成的溯源信息，以及哪些下游模型在统一视图中使用了这些特征。我们将在[*第3章*](B17875_03.xhtml#_idTextAnchor063)中更详细地探讨这一点。
- en: Databricks Feature Store also integrates seamlessly with MLflow. This allows
    Databricks Feature Store to utilize all the great features of MLflow’s feature
    pipelines, as well as to generate features and write them out as feature tables
    in Delta format. The Feature Store has its own generic model packaging format
    that is compatible with the MLflow Models component, which lets your models know
    exactly which features were used to train the models. This integration makes it
    possible to simplify our MLOps pipeline.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks Feature Store还与MLflow无缝集成。这使得Databricks Feature Store能够利用MLflow特征管道的所有优点，并生成特征并将其以Delta格式写入特征表。Feature
    Store具有自己的通用模型包装格式，与MLflow Models组件兼容，这使得模型能够明确了解哪些特征被用于训练模型。这种集成使我们能够简化MLOps管道。
- en: A client can call the serving endpoint either in batch mode or online mode,
    and the model will automatically retrieve the latest features from the Feature
    Store and provide inference. We will see practical examples of this in the coming
    chapters.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端可以通过批处理模式或在线模式调用服务端点，模型将自动从Feature Store中获取最新的特征并提供推理服务。我们将在接下来的章节中看到相关的实际案例。
- en: 'You can also read more about the current state of Databricks Feature Store
    here: [https://docs.databricks.com/machine-learning/feature-store/index.html](https://docs.databricks.com/machine-learning/feature-store/index.html)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在这里了解更多关于Databricks Feature Store当前状态的信息：[https://docs.databricks.com/machine-learning/feature-store/index.html](https://docs.databricks.com/machine-learning/feature-store/index.html)
- en: Lastly, let’s discuss the model registry.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来讨论一下模型注册表。
- en: Discovering the model registry
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现模型注册表
- en: '**Models** is a fully managed and integrated MLflow model registry available
    to each deployed Databricks ML workspace. The registry has its own set of APIs
    and a UI to collaborate with data scientists across the organization and fully
    manage the MLflow model. Data scientists and ML engineers can develop models in
    any of the supported ML frameworks ([https://mlflow.org/docs/latest/models.html#built-in-model-flavors](https://mlflow.org/docs/latest/models.html#built-in-model-flavors))
    and package them in a generic MLfLow model format:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型**是一个完全托管且与Databricks ML工作区集成的MLflow模型注册表。该注册表拥有一套API和一个UI，方便组织中的数据科学家进行协作并全面管理MLflow模型。数据科学家和机器学习工程师可以在任何受支持的ML框架中开发模型（[https://mlflow.org/docs/latest/models.html#built-in-model-flavors](https://mlflow.org/docs/latest/models.html#built-in-model-flavors)），并将其打包成通用的MLflow模型格式：'
- en: '![Figure 2.15 – The Models tab](img/Figure_02.15_B17875.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.15 – 模型标签页](img/Figure_02.15_B17875.jpg)'
- en: Figure 2.15 – The Models tab
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15 – 模型标签页
- en: 'The model registry provides features to manage the versioning, tagging, and
    state transitioning between different environments (moving models from staging
    to production to archive):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 模型注册表提供了管理版本、标签以及在不同环境之间进行状态转换的功能（将模型从预发布阶段迁移到生产环境再到归档）：
- en: '![Figure 2.16 – The Registered Models tab](img/Figure_02.16_B17875.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.16 – 已注册模型标签页](img/Figure_02.16_B17875.jpg)'
- en: Figure 2.16 – The Registered Models tab
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16 – 已注册模型标签页
- en: 'Before we move on, there is another important feature that we need to understand:
    the **Libraries** feature of Databricks. This feature allows users to utilize
    third-party or custom code available to Databricks notebooks and jobs running
    on your cluster.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，还有一个重要的功能需要理解：Databricks的**库**功能。这个功能允许用户将第三方或自定义代码引入Databricks笔记本和在集群上运行的作业中。
- en: Libraries
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 库
- en: Libraries are fundamental building blocks of any programming ecosystem. They
    are akin to toolboxes, comprising pre-compiled routines that offer enhanced functionality
    and assist in optimizing code efficiency. In Databricks, libraries are used to
    make third-party or custom code available to notebooks and jobs running on clusters.
    These libraries can be written in various languages, including Python, Java, Scala,
    and R.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 库是任何编程生态系统中的基础构建模块。它们类似于工具箱，包含了预编译的例程，提供增强的功能并帮助优化代码效率。在Databricks中，库用于将第三方或自定义代码引入到在集群上运行的笔记本和作业中。这些库可以用多种语言编写，包括Python、Java、Scala和R。
- en: Storing libraries
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储库
- en: When it comes to storage, libraries uploaded using the library UI are stored
    in the **Databricks File System** (**DBFS**) root. However, all workspace users
    can modify data and files stored in the DBFS root. If a more secure storage option
    is desired, you can opt to store libraries in cloud object storage, use library
    package repositories, or upload libraries to workspace files.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储方面，通过库 UI 上传的库存储在**Databricks 文件系统**（**DBFS**）根目录中。然而，所有工作区用户都可以修改存储在 DBFS
    根目录中的数据和文件。如果需要更安全的存储选项，您可以选择将库存储在云对象存储中，使用库包仓库，或者将库上传到工作区文件。
- en: Managing libraries
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理库
- en: 'Library management in Databricks can be handled via three different interfaces:
    the workspace UI, the **command-line interface** (**CLI**), or the Libraries API.
    Each option caters to different workflows and user preferences, and the choice
    often depends on individual use cases or project requirements.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 中，库的管理可以通过三种不同的界面进行：工作区 UI、**命令行界面**（**CLI**）或 Libraries API。每个选项都适应不同的工作流程和用户偏好，选择通常取决于个人使用场景或项目需求。
- en: Databricks Runtime and libraries
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Databricks Runtime 和库
- en: Databricks Runtime comes equipped with many common libraries. To find out which
    libraries are included in your runtime, you can refer to the **System Environment**
    subsection of the Databricks Runtime release notes to check your specific runtime
    version.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks Runtime 配备了许多常见的库。要查找您的运行时中包含哪些库，您可以参考 Databricks Runtime 发行说明中的**系统环境**子部分，检查您的具体运行时版本。
- en: Note that Python `atexit` functions aren’t invoked by Databricks when your notebook
    or job finishes processing. If you’re utilizing a Python library that registers
    `atexit` handlers, it’s crucial to ensure your code calls the required functions
    before exiting. Also, the use of Python eggs is being phased out in Databricks
    Runtime and will eventually be removed; consider using Python wheels or installing
    packages from PyPI as alternatives.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当您的笔记本或作业完成处理时，Python的`atexit`函数不会被Databricks调用。如果您正在使用一个注册了`atexit`处理程序的Python库，确保您的代码在退出前调用所需的函数非常重要。另外，Databricks
    Runtime 正在逐步淘汰使用 Python eggs，并最终会移除它们；建议使用 Python wheels 或从 PyPI 安装包作为替代方案。
- en: Library usage modes
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库使用模式
- en: 'Databricks allows three different modes for library installation: cluster-installed,
    notebook-scoped, and workspace libraries:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 提供三种库安装模式：集群安装、笔记本范围的库和工作区库：
- en: '**Cluster libraries**: These libraries are available for use by all notebooks
    running on a particular cluster.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群库**：这些库可供所有在特定集群上运行的笔记本使用。'
- en: '**Notebook-scoped libraries**: Available for Python and R, these libraries
    create an environment scoped to a notebook session and do not affect other notebooks
    running on the same cluster. They are temporary and need to be reinstalled for
    each session.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本范围的库**：这些库适用于 Python 和 R，创建一个与笔记本会话范围相关的环境，不会影响在同一集群上运行的其他笔记本。它们是临时的，每个会话需要重新安装。'
- en: '**Workspace libraries**: These act as local repositories from which you can
    create cluster-installed libraries. They could be custom code written by your
    organization or a specific version of an open source library that your organization
    prefers.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作区库**：这些库充当本地仓库，您可以从中创建集群安装的库。它们可以是贵组织编写的自定义代码，或者是贵组织偏好的开源库的特定版本。'
- en: Let’s next move on to cover Unity Catalog limitations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们讨论一下 Unity Catalog 的限制。
- en: Unity Catalog limitations
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Unity Catalog 限制
- en: There are certain limitations when using libraries with Unity Catalog. For more
    details, you should refer to the *Cluster* *libraries* section.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Unity Catalog 时存在一些限制。有关更多细节，您应该参考*集群* *库*部分。
- en: Installation sources for libraries
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库的安装来源
- en: Cluster libraries can be installed directly from public repositories such as
    PyPI, Maven, or CRAN. Alternatively, they can be sourced from a cloud object storage
    location, a workspace library in the DBFS root, or even by uploading library files
    from your local machine. Libraries installed directly via upload are stored in
    the DBFS root.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 集群库可以直接从公共仓库安装，如 PyPI、Maven 或 CRAN。或者，它们可以来自云对象存储位置、DBFS 根目录中的工作区库，甚至可以通过从本地机器上传库文件进行安装。通过上传直接安装的库存储在
    DBFS 根目录中。
- en: For most of our use cases, we will be using notebook-scoped libraries. You can
    install notebook-scoped libraries using the `%pip` `magic` command.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数使用场景，我们将使用笔记本范围的库。您可以使用`%pip` `magic`命令安装笔记本范围的库。
- en: 'Here are some ways to utilize `%pip` in notebooks to install notebook-scoped
    libraries:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些在笔记本中使用 `%pip` 安装笔记本作用域库的方法：
- en: '`%pip install <package-name>` for notebook-scoped libraries or select PyPI
    as the source for cluster libraries'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%pip install <package-name>` 用于笔记本作用域的库，或者选择 PyPI 作为集群库的来源'
- en: '`%pip install <package-name> --index-url <mirror-url>` for notebook-scoped
    libraries'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%pip install <package-name> --index-url <mirror-url>` 用于笔记本作用域的库'
- en: '`%pip install git+https://github.com/<username>/<repo>.git` for notebook-scoped
    libraries or select PyPI as the source and specify the repository URL as the package
    name for cluster libraries'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%pip install git+https://github.com/<username>/<repo>.git` 用于笔记本作用域的库，或者选择
    PyPI 作为来源，并指定仓库 URL 作为集群库的包名'
- en: '`%pip install dbfs:/<path-to-package>` for notebook-scoped libraries or select
    DBFS/S3 as the source for cluster libraries'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%pip install dbfs:/<path-to-package>` 用于笔记本作用域的库，或者选择 DBFS/S3 作为集群库的来源'
- en: Now, let’s summarize this chapter.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结一下本章内容。
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we got a brief overview of all the components of the Databricks
    ML workspace. This will enable us to utilize these components in a more hands-on
    fashion so that we can train ML models and deploy them for various ML problems
    efficiently in the Databricks environment.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们简要了解了 Databricks ML 工作区的所有组件。这将帮助我们更实际地使用这些组件，从而在 Databricks 环境中高效地训练和部署
    ML 模型，解决各种 ML 问题。
- en: In the next chapter, we will start working on a customer churn prediction problem
    and register our first feature tables in the Databricks feature store.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始处理客户流失预测问题，并在 Databricks 特征存储中注册我们的第一个特征表。
- en: Further reading
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following topics:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多本章涉及的主题，请查阅以下内容：
- en: 'Databricks libraries: [https://docs.databricks.com/libraries/index.html](https://docs.databricks.com/libraries/index.html%20)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 库：[https://docs.databricks.com/libraries/index.html](https://docs.databricks.com/libraries/index.html%20)
- en: 'Databricks notebooks: [https://docs.databricks.com/notebooks/index.html](https://docs.databricks.com/notebooks/index.html)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 笔记本：[https://docs.databricks.com/notebooks/index.html](https://docs.databricks.com/notebooks/index.html)
- en: 'Part 2: ML Pipeline Components and Implementation'
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：ML 管道组件与实现
- en: At the end of this section, you will have a good understanding of each of the
    ML components that are available in the Databricks ML experience and will be comfortable
    using them in your projects.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节结束时，您将对 Databricks ML 体验中的每个 ML 组件有清晰的理解，并能够在项目中自如地使用它们。
- en: 'This section has the following chapters:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含以下章节：
- en: '[*Chapter 3*](B17875_03.xhtml#_idTextAnchor063), *Utilizing the Feature Store*'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 3 章*](B17875_03.xhtml#_idTextAnchor063)，*利用特征存储*'
- en: '[*Chapter 4*](B17875_04.xhtml#_idTextAnchor076), *Understanding MLflow Components
    on Databricks*'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 4 章*](B17875_04.xhtml#_idTextAnchor076)，*了解 Databricks 上的 MLflow 组件*'
- en: '[*Chapter 5*](B17875_05.xhtml#_idTextAnchor085), *Create a Baseline Model Using
    Databricks AutoML*'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 5 章*](B17875_05.xhtml#_idTextAnchor085)，*使用 Databricks AutoML 创建基准模型*'
