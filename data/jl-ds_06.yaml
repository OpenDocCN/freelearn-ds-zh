- en: Chapter 6. Supervised Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章 监督机器学习
- en: It is often believed that data science is machine learning, which means in data
    science, we only train models of machine learning. But data science is much more
    than that. Data science involves understanding data, gathering data, munging data,
    taking the meaning out of that data, and then machine learning if needed.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 人们通常认为数据科学就是机器学习，这意味着在数据科学中，我们只是训练机器学习模型。但数据科学远不止于此。数据科学涉及理解数据、收集数据、整理数据、从中获取意义，然后如果需要进行机器学习。
- en: In my opinion, machine learning is the most exciting field that exists today.
    With huge amounts of data that is readily available, we can gather invaluable
    knowledge. Lots of companies have made their machine learning libraries accessible
    and there are lots of open source alternatives that exist.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，机器学习是当今存在的最令人兴奋的领域。随着大量可用数据的出现，我们可以收集到宝贵的知识。许多公司已经使他们的机器学习库变得可访问，还有很多开源替代品存在。
- en: 'In this chapter, you will study the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下主题：
- en: What is machine learning?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Types of machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的类型
- en: What is overfitting and underfitting?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合是什么？
- en: Bias-variance trade-off
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: Feature extraction and selection
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取和选择
- en: Decision trees
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Naïve Bayes classifier
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: What is machine learning?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Generally, when we talk about machine learning, we get into the idea of us fighting
    wars with intelligent machines that we created but went out of control. These
    machines are able to outsmart the human race and become a threat to human existence.
    These theories are just created for our entertainment. We are still very far away
    from such machines.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，当我们谈论机器学习时，我们会涉及到与我们创建但失控的智能机器争斗的想法。这些机器能够智胜人类，并对人类生存构成威胁。这些理论只是为了我们的娱乐而创造的。我们离这样的机器还非常遥远。
- en: 'So, the question is: what is machine learning? Tom M. Mitchell gave a formal
    definition:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以问题是：什么是机器学习？Tom M. Mitchell给出了一个正式的定义：
- en: '*"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P if its performance at tasks in T, as
    measured by P, improves with experience E."*'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"如果一个计算机程序在任务T的表现，以性能度量P来衡量，随着经验E的增加而提高，那么它就被认为是从经验E中学习。"*'
- en: This implies that machine learning is teaching computers to generate algorithms
    using data without programming them explicitly. It transforms data into actionable
    knowledge. Machine learning has close association with statistics, probability,
    and mathematical optimization.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着机器学习是教计算机使用数据生成算法，而不是明确编程它们。它将数据转化为可操作的知识。机器学习与统计学、概率论和数学优化密切相关。
- en: As technology grows, there is one thing that grows with it exponentially—data.
    We have huge amounts of unstructured and structured data growing at a very great
    pace. Lots of data is generated by space observatories, meteorologists, biologists,
    fitness sensors, surveys, and so on. It is not possible to manually go through
    this much amount of data and find patterns or gain insights. This data is very
    important for scientists, domain experts, governments, health officials, and even
    businesses. To gain knowledge out of this data, we need self-learning algorithms
    that can help us in decision making.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着技术的发展，有一样东西呈指数增长——数据。我们有大量的结构化和非结构化数据，以非常快的速度增长。太空观测站、气象学家、生物学家、健身传感器、调查等产生了大量数据。手动处理这么多数据并找出模式或洞察力是不可能的。这些数据对科学家、领域专家、政府、卫生官员甚至企业都非常重要。为了从这些数据中获取知识，我们需要自学习算法来帮助我们做决策。
- en: Machine learning evolved as a subfield of artificial intelligence, which eliminates
    the need to manually analyze large amounts of data. Instead of using machine learning,
    we make data-driven decisions by gaining knowledge using self-learning predictive
    models. Machine learning has become important in our daily lives. Some common
    use cases include search engines, games, spam filters, and image recognition.
    Self-driving cars also uses machine learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习作为人工智能的一个子领域发展，消除了手动分析大量数据的需要。我们通过使用自学习预测模型来进行数据驱动决策。机器学习已经成为我们日常生活中的重要组成部分。一些常见的应用包括搜索引擎、游戏、垃圾邮件过滤器和图像识别。自动驾驶汽车也使用机器学习。
- en: 'Some basic terminologies used in machine learning include:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中使用的一些基本术语包括：
- en: '**Features**: Distinctive characteristics of the data point or record'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征**：数据点或记录的独特特征。'
- en: '**Training set**: This is the dataset that we feed to train the algorithm that
    helps us to find relationships or build a model'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：这是我们输入算法进行训练的数据集，帮助我们发现关系或构建模型。'
- en: '**Testing set**: The algorithm generated using the training dataset is tested
    on the testing dataset to find the accuracy'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：使用训练数据集生成的算法会在测试数据集上进行测试，以查找准确度。'
- en: '**Feature vector**: An n-dimensional vector that contains the features defining
    an object'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征向量**：包含定义对象特征的n维向量。'
- en: '**Sample**: An item from the dataset or the record'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本**：数据集中的一个项目或记录。'
- en: Uses of machine learning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习的应用
- en: 'Machine learning in one way or another is used everywhere. Its applications
    are endless. Let''s discuss some very common use cases:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在某种程度上无处不在，它的应用几乎没有边界。我们来讨论一些非常常见的使用案例：
- en: '**E-mail spam filtering**: Every major e-mail service provider uses machine
    learning to filter out spam messages from the Inbox to the Spam folder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电子邮件垃圾邮件过滤**：每个主要的电子邮件服务提供商都使用机器学习来将垃圾邮件从收件箱过滤到垃圾邮件文件夹。'
- en: '**Predicting storms and natural disasters**: Machine learning is used by meteorologists
    and geologists to predict the natural disasters using weather data, which can
    help us to take preventive measures.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测风暴和自然灾害**：气象学家和地质学家利用机器学习，通过天气数据预测自然灾害，这有助于我们采取预防措施。'
- en: '**Targeted promotions/campaigns and advertising**: On social sites, search
    engines, and maybe in mailboxes, we see advertisements that somehow suit our tastes.
    This is made feasible using machine learning on the data from our past searches,
    our social profile, or e-mail contents.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定向促销/活动和广告**：在社交网站、搜索引擎，甚至可能在邮箱中，我们看到的广告总是某种程度上符合我们的口味。这是通过对我们过去搜索记录、社交资料或电子邮件内容的数据进行机器学习来实现的。'
- en: '**Self-driving cars**: Technology giants are currently working on self-driving
    cars. This is made possible using machine learning on the feed of the actual data
    from human drivers, image and sound processing, and various other factors.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动驾驶汽车**：科技巨头目前正致力于自动驾驶汽车。这是通过对实际驾驶数据、人类驾驶员的图像和声音处理以及其他各种因素进行机器学习实现的。'
- en: Machine learning is also used by businesses to predict the market.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习也被企业用来预测市场。
- en: It can also be used to predict the outcomes of elections and the sentiment of
    voters towards a particular candidate.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还可以用来预测选举结果和选民对特定候选人的情感。
- en: Machine learning is also being used to prevent crime. By understanding the pattern
    of different criminals, we can predict a crime that can happen in the future and
    can prevent it.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习也被用来预防犯罪。通过理解不同罪犯的模式，我们可以预测未来可能发生的犯罪，并加以防范。
- en: One case that got a huge amount of attention was of a big retail chain in the
    United States using machine learning to identify pregnant women. The retailer
    thought of a strategy to give discounts on multiple maternity products, so that
    the women would become loyal customers and would baby purchase items with a high
    profit margin.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个广受关注的案例是，美国一家大型零售商利用机器学习来识别孕妇。该零售商想出了通过在多种孕妇用品上提供折扣来吸引女性顾客，让她们成为忠实客户，并购买高利润的婴儿用品。
- en: The retailer worked on the algorithm to predict the pregnancy using useful patterns
    in purchases of different products which are useful for pregnant women.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该零售商通过分析购买不同孕妇用品的模式，研究出了预测怀孕的算法。
- en: Once a man approached the retailer and asked for the reason that his teenage
    daughter is receiving discount coupons for maternity items. The retail chain offered
    an apology but later the father himself apologized when he got to know that his
    daughter was indeed pregnant.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经有一个人走到零售商面前，询问为什么他的青少年女儿会收到孕妇用品的折扣券。零售商道歉了，但后来父亲自己也道歉了，因为他了解到女儿确实怀孕了。
- en: This story may or may not be completely true, but retailers do analyze their
    customers' data routinely to find out patterns for targeted promotions, campaigns,
    and inventory management.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事可能完全真实，也可能并非完全真实，但零售商确实定期分析客户数据，以发现用于定向促销、活动和库存管理的模式。
- en: Machine learning and ethics
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习与伦理
- en: 'Let''s see where machine learning is used very frequently:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看机器学习在哪些领域被广泛应用：
- en: '**Retailers**: In the previous example, we mentioned how retail chains use
    data for machine learning to increase their revenue as well as to retain their
    customers'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零售商**：在之前的例子中，我们提到零售商如何使用数据来进行机器学习，从而增加收入并留住客户'
- en: '**Spam filtering**: E-mails are processed using various machine learning algorithms
    for spam filtering'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**垃圾邮件过滤**：电子邮件通过各种机器学习算法进行垃圾邮件过滤'
- en: '**Targeted advertisements**: In our mailbox, social sites, or search engines,
    we see advertisements of our liking'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定向广告**：在我们的邮箱、社交网站或搜索引擎中，我们会看到自己喜欢的广告'
- en: These are only some of the actual use cases that are implemented in the world
    today. One thing that is common between them is the user data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些仅是现实世界中实施的部分实际用例。它们之间的共同点是用户数据。
- en: In the first example, retailers are using the history of transactions done by
    the user for targeted promotions and campaigns and for inventory management, among
    other things. Retail giants do this by providing users a loyalty or sign-up card.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，零售商利用用户的交易历史进行定向推广和活动策划，以及库存管理等其他工作。零售巨头通过为用户提供忠诚卡或注册卡来实现这一点。
- en: In the second example, the e-mail service provider uses trained machine learning
    algorithms to detect and flag spam. It does so by going through the contents of
    e-mail/attachments and classifying the sender of the e-mail.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个例子中，电子邮件服务提供商使用经过训练的机器学习算法来检测和标记垃圾邮件。它通过检查电子邮件内容/附件，并对电子邮件发送者进行分类来实现这一点。
- en: In the third example, again the e-mail provider, social network, or search engine
    will go through our cookies, our profiles, or our mails to do the targeted advertising.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个例子中，同样是电子邮件提供商、社交网络或搜索引擎通过我们的Cookies、个人资料或邮件来进行定向广告。
- en: In all of these examples, it is mentioned in the terms and conditions of the
    agreement when we sign up with the retailer, e-mail provider, or social network
    that the user's data will be used but privacy will not be violated.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些例子中，当我们与零售商、电子邮件提供商或社交网络签约时，协议的条款和条件中都会提到将使用用户数据，但不会侵犯隐私。
- en: It is really important that before using data that is not publicly available,
    we take the required permissions. Also, our machine learning models shouldn't
    discriminate on the basis of region, race, and sex, or of any other kind. The
    data provided should not be used for purposes not mentioned in the agreement or
    if it is illegal in the region or country of existence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用未公开的数据之前，我们必须获得必要的许可。这非常重要。此外，我们的机器学习模型不应在地域、种族、性别或任何其他方面存在歧视。提供的数据不应用于协议中未提及的目的，或在所在地区或国家是非法的。
- en: Machine learning – the process
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习 – 过程
- en: Machine learning algorithms are trained in keeping with the idea of how the
    human brain works. They are somewhat similar. Let's discuss the whole process.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的训练是根据人类大脑工作的方式进行的。它们有些相似。让我们来讨论整个过程。
- en: 'The machine learning process can be described in three steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习过程可以分为三个步骤：
- en: Input
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入
- en: Abstraction
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抽象
- en: Generalization
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 泛化
- en: 'These three steps are the core of how the machine learning algorithm works.
    Although the algorithm may or may not be divided or represented in such a way,
    this explains the overall approach:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个步骤是机器学习算法工作的核心。尽管算法的表现形式可能不同，但这解释了整体方法：
- en: The first step concentrates on what data should be there and what shouldn't.
    On the basis of that, it gathers, stores, and cleans the data as per the requirements.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步集中在应该包含哪些数据以及不应包含哪些数据。根据这一点，它根据需求收集、存储并清理数据。
- en: The second step entails the data being translated to represent the bigger class
    of data. This is required as we cannot capture everything and our algorithm should
    not be applicable for only the data that we have.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步涉及将数据转换为代表更大类的数据。这是必要的，因为我们无法捕捉到所有数据，且我们的算法不应该仅适用于我们拥有的数据。
- en: The third step focuses on the creation of the model or an action that will use
    this abstracted data, which will be applicable for the broader mass.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三步关注于创建模型或行动，这些模型或行动将使用这些抽象的数据，并适用于更广泛的群体。
- en: So, what should be the flow of approaching a machine learning problem?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，接近一个机器学习问题的流程应该是什么样的呢？
- en: '![Machine learning – the process](img/B05321_06_01.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习 – 过程](img/B05321_06_01.jpg)'
- en: In this particular figure, we see that the data goes through the abstraction
    process before it can be used to create the machine learning algorithm. This process
    itself is cumbersome. We studied this process in the chapter related to data munging.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的图示中，我们看到数据在用于创建机器学习算法之前，经过了抽象处理过程。这个过程本身是繁琐的。我们在与数据清洗相关的章节中学习了这个过程。
- en: The process follows the training of the model, which is fitting the model into
    the dataset that we have. The computer does not pick up the model on its own,
    but it is dependent on the learning task. The learning task also includes generalizing
    the knowledge gained on the data that we don't have yet.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程紧随模型训练之后，模型的训练就是将模型拟合到我们拥有的数据集上。计算机并不会自主选择模型，而是依赖于学习任务。学习任务还包括将从我们尚未拥有的数据中获得的知识进行泛化。
- en: Therefore, training the model is based on the data that we currently have and
    the learning task includes generalization of the model for future data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练模型是基于我们当前拥有的数据，而学习任务包括将模型泛化到未来的数据。
- en: It depends on how our model deduces knowledge from the dataset that we currently
    have. We need to make such a model that can gather insights into something that
    wasn't known to us before can be useful and can be linked to future data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它取决于我们的模型如何从我们当前拥有的数据集中推断知识。我们需要创建一个能够从之前不为我们所知的东西中汲取洞察的模型，这样它就能对未来数据产生有用的联系。
- en: Different types of machine learning
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同类型的机器学习
- en: 'Machine learning is divided mainly into three categories:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习主要分为三类：
- en: Supervised learning
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement learning
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: In supervised learning, the model/machine is presented with inputs and the outputs
    corresponding to those inputs. The machine learns from these inputs and applies
    this learning in further unseen data to generate outputs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，模型/机器会接收输入以及与这些输入对应的输出。机器从这些输入中学习，并将这种学习应用于进一步的未见数据，以生成输出。
- en: Unsupervised learning doesn't have the required outputs; therefore it is up
    to the machine to learn and find patterns that were previously unseen.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习没有所需的输出，因此由机器来学习并寻找之前未见的模式。
- en: In reinforcement learning, the machine continuously interacts with the environment
    and learns through this process. This includes a feedback loop.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，机器与环境持续互动并通过这一过程学习。这包括一个反馈循环。
- en: What is bias-variance trade-off?
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是偏差-方差权衡？
- en: 'Let''s understand what bias and variance are. First we will go through bias
    in our model:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解一下什么是偏差和方差。首先，我们将讨论模型中的偏差：
- en: Bias is the difference between the predictions that have been generated by the
    model and the correct value that was expected or we should have received.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差是模型生成的预测结果与预期的正确值之间的差异，或者说我们应该获得的值。
- en: When we get the new data, the model will work out and give predictions. Therefore,
    it means our model has a range of predictions it can generate.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们获得新数据时，模型会进行计算并给出预测。因此，这意味着我们的模型有一个可以生成预测的范围。
- en: Bias is the correctness of this range of predictions.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差是这一预测范围的准确性。
- en: 'Now, let''s understand variance and how it affects the model:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解方差以及它如何影响模型：
- en: Variance is the variability of the model when the data points are changed or
    new data is introduced
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方差是当数据点发生变化或引入新数据时，模型的变异性
- en: It shouldn't be required to tweak the model every time new data is introduced
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不应该在每次引入新数据时都需要调整模型
- en: As per our understanding of bias and variance, we can conclude that these affect
    each other. Therefore, while creating the model, we keep this trade-off in consideration.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对偏差和方差的理解，我们可以得出结论，它们相互影响。因此，在创建模型时，我们会考虑这一权衡。
- en: Effects of overfitting and underfitting on a model
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合对模型的影响
- en: Overfitting happens when the model that we have created also starts considering
    the outliers or noise in our dataset. Therefore, it means our model is fitting
    the dataset rather too well.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合发生在我们创建的模型开始考虑数据集中的异常值或噪声时。因此，这意味着我们的模型过度拟合了数据集。
- en: The drawback of such a model is that it will not be able to generalize well.
    Such models have low bias and high variance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的缺点是无法很好地进行泛化。这类模型具有低偏差和高方差。
- en: Underfitting happens when the model that we have created is not able to find
    out the patterns or trend of the data as is desired. Therefore, it means the model
    is not fitting to the dataset well.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合发生在我们创建的模型未能找到数据的模式或趋势时。因此，这意味着模型未能很好地适应数据集。
- en: The drawback of such a model is that it is not able to give good predictions.
    Such models have high bias and low variance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的缺点是无法给出良好的预测。这些模型具有高偏差和低方差。
- en: We should try to reduce both underfitting and overfitting. This is done through
    various techniques. Ensemble models are very good in avoiding underfitting and
    overfitting. We will study ensemble models in upcoming chapters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该尽量减少欠拟合和过拟合。这可以通过各种技术来实现。集成模型在避免欠拟合和过拟合方面非常有效。我们将在接下来的章节中学习集成模型。
- en: Understanding decision trees
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解决策树
- en: A decision tree is a very good example of "divide and conquer". It is one of
    the most practical and widely used methods for inductive inference. It is a supervised
    learning method that can be used for both classification and regression. It is
    non-parametric and its aim is to learn by inferring simple decision rules from
    the data and create such a model that can predict the value of the target variable.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是“分治法”的一个很好的例子。它是最实用、最广泛使用的归纳推理方法之一。它是一种监督学习方法，可用于分类和回归。它是非参数的，目的是通过推断数据中的简单决策规则来学习，并创建一个能够预测目标变量值的模型。
- en: Before taking a decision, we analyze the probability of the pros and cons by
    weighing the different options that we have. Let's say we want to purchase a phone
    and we have multiple choices in the price segment. Each of the phones has something
    really good, and maybe better than the other. To make a choice, we start by considering
    the most important feature that we want. And as such, we create a series of features
    that it has to pass to become the ultimate choice.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在做出决策之前，我们会通过权衡不同选项来分析利弊。例如，我们想购买一部手机，并且有多个价格区间的选择。每款手机都有某些特别好的功能，可能比其他手机更好。为了做出选择，我们从考虑我们最重要的特征开始。基于此，我们创建了一系列需要满足的特征，最终选择将是最符合这些特征的那一款。
- en: 'In this section, we will learn about:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将学习：
- en: Decision trees
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Entropy measures
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵度量
- en: Random forests
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: We will also learn about famous decision tree learning algorithms such as ID3
    and C5.0.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将学习著名的决策树学习算法，如 ID3 和 C5.0。
- en: Building decision trees – divide and conquer
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建决策树 - 分治法
- en: A heuristic called recursive partitioning is used to build decision trees. In
    this approach, our data is split into similar classes of smaller and smaller subsets
    as we move along.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为递归划分的启发式方法用于构建决策树。在这种方法中，随着推进，我们将数据划分为越来越小的相似类别。
- en: A decision tree is actually an inverted tree. It starts from the root and ends
    up at the leaf nodes, which are the terminal nodes. The splitting of the node
    into branches is based on logical decisions. The whole dataset is represented
    at the root node. A feature is chosen by the algorithm that is most predictive
    of the target class. Then it partitions the examples into distinct value groups
    of this particular feature. This represents the first set of the branches of our
    tree.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树实际上是一个倒置的树。它从根节点开始，最终到达叶节点，这些叶节点是终端节点。节点的分支依据逻辑决策。整个数据集在根节点处表示。算法会选择一个对目标类别最具预测性的特征。然后，它根据这个特征将样本划分为不同的值组。这代表了我们树的第一组分支。
- en: The divide-and-conquer approach is followed until the end point is reached.
    At each step, the algorithm continues to choose the best candidate feature.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 采用分治法，直到达到终点。在每一步，算法会继续选择最佳的候选特征。
- en: 'The end point is defined when:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当以下条件满足时，定义终点：
- en: At a particular node, nearly all the examples belong to the same class
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某个节点，几乎所有的样本都属于同一类别
- en: The feature list is exhausted
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征列表已用尽
- en: A predefined size limit of the tree is reached
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达到预定义的树大小限制
- en: '![Building decision trees – divide and conquer](img/B05321_06_02.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![构建决策树 - 分治法](img/B05321_06_02.jpg)'
- en: 'The preceding image is a very famous example of decision tree. Here, a decision
    tree is made to find out whether to go out or not:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像是一个非常著名的决策树示例。在这里，决策树是用来判断是否外出：
- en: Outlook is the root node. This refers to all the possible classes of the environment
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Outlook 是根节点。这指的是环境中所有可能的类别。
- en: Sunny, overcast, and Rain are the branches.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sunny、overcast 和 Rain 是分支。
- en: Humidity and Wind are the leaf nodes, which are again split into branches, and
    a decision is taken depending on the favorable environment.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 湿度和风速是叶节点，这些节点又被分为分支，决策是根据有利的环境做出的。
- en: These trees can also be re-represented as if-then rules, which would be easily
    understandable. Decision trees are one of the very successful and popular algorithms,
    with a broad range of applications.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些树也可以重新表示为if-then规则，这样会更容易理解。决策树是非常成功且受欢迎的算法之一，应用范围广泛。
- en: 'The following are some of the applications of decision trees:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是决策树的一些应用：
- en: '**Decision of credit card / loan approval**: Credit scoring models are based
    on decision trees, where every applicant''s information is fed to decide whether
    a credit card / loan should be approved or not.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信用卡/贷款批准决策**：信用评分模型基于决策树，每个申请人的信息被输入，以决定是否批准信用卡/贷款。'
- en: '**Medical diagnosis**: Various diseases are diagnosed using well-defined and
    tested decision trees based on symptoms, measurements, and tests.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医学诊断**：许多疾病通过基于症状、测量和检测的经过充分定义和测试的决策树进行诊断。'
- en: Where should we use decision tree learning?
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们应该在什么情况下使用决策树学习？
- en: 'Although there are various decision tree learning methods that can be used
    for a variety of problems, decision trees are best suited for the following scenarios:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有多种决策树学习方法可以用于各种问题，但决策树最适合以下场景：
- en: Attribute-value pairs are scenarios where instances are described by attributes
    from a fixed set and values. In the previous example, we had the attribute as
    "Wind" and the values as "Strong" and "Weak". These disjoint possible values make
    it easy to create decision tree learning, although attributes with real values
    can also be used.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属性-值对是指通过固定集的属性和相应的值来描述实例的场景。在前面的例子中，我们有属性“风速”和值“强”和“弱”。这些互斥的可能值使得决策树学习变得容易，尽管也可以使用具有实数值的属性。
- en: The final output of the target function has a discreet value, like the previous
    example, where we had "Yes" or "No". The decision tree algorithm can be extended
    to have more than two possible target values. Decision trees can also be extended
    to have real values as outputs, but this is rarely used.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标函数的最终输出是离散值，类似于前面的例子，其中我们有“是”或“否”。决策树算法可以扩展为具有多个可能的目标值。决策树也可以扩展为具有实数值作为输出，但这种情况很少使用。
- en: The decision tree algorithm is robust to errors in the training dataset. These
    errors can be in the attribute values of the examples or the classification of
    the examples or both.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树算法对于训练数据集中的错误具有鲁棒性。这些错误可能出现在示例的属性值、分类或者两者都有。
- en: Decision tree learning is also suited for missing values in a dataset. If the
    values are missing in some examples where they are available for attributes in
    other examples, then decision trees can be used.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树学习也适用于数据集中缺失值的情况。如果某些示例中的值缺失，而其他示例中的相同属性有值，那么可以使用决策树。
- en: Advantages of decision trees
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树的优点
- en: It is easy to understand and interpret decision trees. Visualizing decision
    trees is easy too.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树容易理解和解释，决策树的可视化也很简单。
- en: In other algorithms, data normalization needs to be done before they can be
    applied. Normalization refers to the creation of dummy variables and removing
    blank values. Decision trees, on the other hand, require very less data preparation.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其他算法中，必须先进行数据归一化才能应用。归一化是指创建虚拟变量并去除空值。而决策树则需要的准备工作较少。
- en: The cost involved in prediction using a decision tree is logarithmic with respect
    to the number of examples used in training the tree.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树进行预测的成本与训练树所使用的示例数量呈对数关系。
- en: Decision trees, unlike other algorithms, can be applied to both numerical and
    categorical data. Other algorithms are generally specialized to be used for only
    one type of variable.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他算法不同，决策树可以同时应用于数值型和类别型数据。其他算法通常专门用于处理其中一种类型的变量。
- en: Decision trees can easily take care of the problems where multiple outputs is
    a possibility.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树可以轻松处理可能有多个输出的情况。
- en: Decision trees follow the white box model, which means a condition is easily
    explained using Boolean logic if the situation is observable in the model. On
    the other hand, results are comparatively difficult to interpret in a black box
    model, such as artificial neural networks.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树遵循白盒模型，这意味着如果情况在模型中是可观察的，使用布尔逻辑可以轻松解释条件。另一方面，在黑盒模型（如人工神经网络）中，结果相对难以解释。
- en: Statistical tests can be used to validate the model. Therefore, we can test
    the reliability of the model.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计测试可以用来验证模型。因此，我们可以测试模型的可靠性。
- en: It is able to perform well even if there is a violation in the assumptions from
    the true model that was the source of the data.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使数据源的真实模型假设被违反，它也能表现良好。
- en: Disadvantages of decision trees
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树的缺点
- en: 'We''ve covered where decision trees are suited and their advantages. Now we
    will go through the disadvantages of decision trees:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了决策树适用的情况及其优势。现在我们将讨论决策树的缺点：
- en: There is always a possibility of overfitting the data in decision trees. This
    generally happens when we create trees that are over-complex and are not possible
    to generalize well.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树存在过拟合数据的可能性。这通常发生在创建过于复杂且难以泛化的树时。
- en: To avoid this, various steps can be taken. One method is pruning. As the name
    suggests, it is a method where we set the maximum depth to which the tree can
    grow.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免这种情况，可以采取多种步骤。其中一种方法是修剪。顾名思义，这是一种方法，我们在其中设置树可以生长到的最大深度。
- en: Instability is always a concern with decision trees as a small variation in
    the data can result in generation of a different tree altogether.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树总是存在不稳定性的问题，因为数据的微小变化可能导致生成完全不同的树。
- en: The solution to such a scenario is ensemble learning, which we will study in
    the next chapter.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种场景的解决方案是集成学习，在下一章中我们将学习它。
- en: Decision tree learning may sometimes lead to the creation of biased trees, where
    some classes are dominant over others. The solution to such a scenario is balancing
    the dataset prior to fitting it to the decision tree algorithm.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树学习有时可能导致创建偏向某些类的树，而忽视其他类的情况。在将数据拟合到决策树算法之前，解决这种情况的方法是平衡数据集。
- en: Decision tree learning is known to be NP-complete, considering several aspects
    of optimality. This holds even for the basic concepts.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树学习被认为是 NP 完全的，考虑到优化的几个方面。即使对于基本概念也是如此。
- en: Usually, heuristic algorithms like greedy algorithms are used where a locally
    optimal decision is made at each node. This doesn't guarantee that we will have
    a decision tree that is globally optimal.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常使用贪婪算法等启发式算法，在每个节点都做出局部最优决策。这并不保证我们将得到一个全局最优的决策树。
- en: Learning can be hard for the concepts such as Parity, XOR, and multiplexer problems,
    where decision trees cannot represent them easily.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于诸如奇偶性、XOR 和多路复用器问题等概念，学习可能会很困难，决策树无法轻松表示它们。
- en: Decision tree learning algorithms
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树学习算法
- en: There are various decision tree learning algorithms that are actually variations
    of the core algorithm. The core algorithm is actually a top-down, greedy search
    through all possible trees.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种决策树学习算法，实际上是核心算法的变体。核心算法实际上是一种自顶向下的、贪婪的搜索所有可能树的方法。
- en: 'We are going to discuss two algorithms:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论两种算法：
- en: ID3
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ID3
- en: C4.5 and C5.0
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C4.5 和 C5.0
- en: The first algorithm, **ID3** (**Iterative Dichotomiser 3**), was developed by
    Ross Quinlan in 1986\. The algorithm proceeds by creating a multiway tree, where
    it uses a greedy search to find each node and the features that can yield maximum
    information gain for the categorical targets. As trees can grow to the maximum
    size, which can result in over-fitting of data, pruning is used to make the generalized
    model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个算法，**ID3**（**迭代二分器 3**），是由 Ross Quinlan 在 1986 年开发的。该算法通过创建一个多路树来进行，它使用贪婪搜索找到每个节点和可以产生最大信息增益的特征，由于树可以增长到最大尺寸，这可能导致数据过拟合，因此使用修剪来创建泛化模型。
- en: C4.5 came after ID3 and eliminated the restriction that all features must be
    categorical. It does this by defining dynamically a discrete attribute based on
    the numerical variables. This partitions into a discrete set of intervals from
    the continuous attribute value. C4.5 creates sets of if-then rules from the trained
    trees of the ID3 algorithm. C5.0 is the latest version; it builds smaller rule
    sets and uses comparatively lesser memory.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: C4.5是在ID3之后发展起来的，消除了所有特征必须是分类变量的限制。它通过基于数值变量动态定义离散特征来实现这一点。它将连续的属性值划分为一组离散区间。C4.5从ID3算法的训练树中创建if-then规则集合。C5.0是最新版本；它创建了更小的规则集，并且使用相对较少的内存。
- en: How a decision tree algorithm works
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树算法如何工作
- en: 'The decision tree algorithm constructs the top-down tree. It follows these
    steps:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法构建自顶向下的树。它遵循以下步骤：
- en: To know which element should come at the root of the tree, a statistical test
    is done on each instance of the attribute to determine how well the training examples
    can be classified using this attribute alone.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了知道哪一个元素应该出现在树的根节点，算法会对每个属性实例进行统计测试，以确定仅使用该属性时训练示例能够被多好地分类。
- en: This leads to the selection of the best attribute at the root node of the tree.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这导致在树的根节点选择最佳特征。
- en: Now at this root node, for each possible value of the attribute, the descendants
    are created.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在这个根节点上，对于每个属性的可能值，都创建后代节点。
- en: The examples in our training dataset are sorted to each of these descendant
    nodes.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练数据集中的示例会被分配到每一个这些后代节点。
- en: Now for these individual descendant nodes, all the previous steps are repeated
    for the remaining examples in our training dataset.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这些单独的后代节点，之前的所有步骤会针对训练数据集中剩余的示例重复进行。
- en: This leads to the creation of an acceptable tree for our training dataset using
    a greedy search. The algorithm never backtracks, which means it never reconsiders
    the previous choices and follows the tree downwards.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会通过贪婪搜索创建一个可接受的训练数据集决策树。算法永不回溯，这意味着它永远不会重新考虑之前的选择，而是继续向树的下方发展。
- en: Understanding and measuring purity of node
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解与衡量节点的纯度
- en: The decision tree is built top-down. It can be tough to decide on which attribute
    to split on each node. Therefore, we find the feature that best splits the target
    class. Purity is the measure of a node containing only one class.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是自顶向下构建的。每个节点选择分裂的属性可能会很困难。因此，我们寻找能够最好地分裂目标类别的特征。纯度是指一个节点只包含一个类别的度量。
- en: 'Purity in C5.0 is measured using entropy. The entropy of the sample of the
    examples is the indication of how class values are mixed across the examples:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0中的纯度是通过熵来衡量的。样本的熵指示了类别值在示例之间的混合程度：
- en: '0: The minimum value is an indication of the homogeneity in the class values
    in the sample'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '0: 最小值表示样本中类别值的同质性'
- en: '1: The maximum value is an indication that there is maximum amount of disorder
    in the class values in the sample'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1: 最大值表示样本中类别值的最大无序程度'
- en: 'Entropy is given by:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的计算公式为：
- en: '![Understanding and measuring purity of node](img/image_06_003.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![理解与衡量节点的纯度](img/image_06_003.jpg)'
- en: In the preceding formula, *S* refers to the dataset that we have and *c* refers
    to the class levels. For a given class *i*, *p* is the proportion of the values.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*S*表示我们拥有的数据集，*c*表示类别水平。对于给定类别*i*，*p*是该类别值的比例。
- en: 'When the purity measure is determined, the algorithm has to decide on which
    feature the data should be split. To decide this, the entropy measure is used
    by the algorithm to calculate how the homogeneity differs on splitting on each
    possible feature. This particular calculation done by the algorithm is the information
    gain:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当纯度度量确定后，算法必须决定数据应该根据哪个特征进行分裂。为了决定这一点，算法使用熵度量来计算在每个可能的特征上分裂时，同质性如何变化。算法进行的这种计算叫做信息增益：
- en: '![Understanding and measuring purity of node](img/image_06_004.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![理解与衡量节点的纯度](img/image_06_004.jpg)'
- en: The difference between the entropy before splitting the dataset (*S1*) and the
    resulting partitions from splitting (*S2*) is called information gain (*F*).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集分割前的熵（*S1*）与分割后得到的子集熵（*S2*）之间的差异叫做信息增益（*F*）。
- en: An example
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个示例
- en: Let's apply what we've learned to create a decision tree using Julia. We will
    be using the example available for Python on [http://scikit-learn.org/](http://scikit-learn.org/)
    and Scikitlearn.jl by Cedric St-Jean.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所学的知识应用于使用 Julia 创建决策树。我们将使用 [http://scikit-learn.org/](http://scikit-learn.org/)
    上为 Python 提供的示例和 Cedric St-Jean 开发的 Scikitlearn.jl。
- en: 'We will first have to add the required packages:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要添加所需的包：
- en: '[PRE0]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'ScikitLearn provides the interface to the much-famous library of machine learning
    for Python to Julia:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ScikitLearn 提供了一个接口，将著名的机器学习库从 Python 转换到 Julia：
- en: '[PRE1]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After adding the required packages, we will create the dataset that we will
    be using in our example:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加所需包之后，我们将创建我们将在示例中使用的数据集：
- en: '[PRE2]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will generate a 16-element `Array{Float64,1}`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个包含 16 个元素的 `Array{Float64,1}`。
- en: 'Now we will create instances of two different models. One model is where we
    will not limit the depth of the tree, and in other model, we will prune the decision
    tree on the basis of purity:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建两个不同模型的实例。一个模型不限制树的深度，另一个模型则根据纯度修剪决策树：
- en: '![An example](img/image_06_005.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/image_06_005.jpg)'
- en: We will now fit the models to the dataset that we have. We will fit both the
    models.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将对现有的数据集拟合模型。我们将拟合这两个模型。
- en: '![An example](img/image_06_006.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/image_06_006.jpg)'
- en: This is the first model. Here our decision tree has `25` leaf nodes and a depth
    of `8`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一个模型。这里我们的决策树有 `25` 个叶节点，深度为 `8`。
- en: '![An example](img/image_06_007.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/image_06_007.jpg)'
- en: This is the second model. Here we prune our decision tree. This has `6` leaf
    nodes and a depth of `4`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第二个模型。这里我们修剪了决策树。这个模型有 `6` 个叶节点，深度为 `4`。
- en: 'Now we will use the models to predict on the test dataset:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用模型在测试数据集上进行预测：
- en: '[PRE3]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This creates a 501-element `Array{Float64,1}`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个包含 501 个元素的 `Array{Float64,1}`。
- en: 'To better understand the results, let''s plot both the models on the dataset
    that we have:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解结果，让我们将这两个模型在我们拥有的数据集上进行可视化：
- en: '[PRE4]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Decision trees can tend to overfit data. It is required to prune the decision
    tree to make it more generalized. But if we do more pruning than required, then
    it may lead to an incorrect model. So, it is required that we find the most optimized
    pruning level.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可能会过拟合数据。为了使其更加通用，必须修剪决策树。但如果修剪过度，可能会导致模型不正确。因此，必须找到最优化的修剪级别。
- en: '![An example](img/image_06_008.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/image_06_008.jpg)'
- en: It is quite evident that the first decision tree overfits to our dataset, whereas
    the second decision tree model is comparatively more generalized.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，第一个决策树对我们的数据集过拟合，而第二个决策树模型则相对更加通用。
- en: Supervised learning using Naïve Bayes
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯的有监督学习
- en: Naïve Bayes is one of most famous machine learning algorithms to date. It is
    widely used in text classification techniques.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是目前最著名的机器学习算法之一，广泛应用于文本分类技术。
- en: Naïve Bayes methods come under the set of supervised learning algorithms. It
    is a probabilistic classifier and is based on Bayes' theorem. It takes the "naïve"
    assumption that every pair of features is independent of one another.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯方法属于有监督学习算法。它是一种概率分类器，基于贝叶斯定理。它假设每对特征彼此独立，这一假设被称为“朴素”假设。
- en: And in spite of these assumptions, Naïve Bayes classifiers work really well.
    Their most famous use case is spam filtering. The effectiveness of this algorithm
    is justified by the requirement of quite a small amount of training data for estimating
    the required parameters.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管做出这些假设，朴素贝叶斯分类器仍然表现得非常好。它们最著名的应用是垃圾邮件过滤。该算法的有效性体现在它对训练数据的需求非常小，能够估计出所需的参数。
- en: These classifiers and learners are quite fast when compared to other methods.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他方法相比，这些分类器和学习器的速度相当快。
- en: '![Supervised learning using Naïve Bayes](img/image_06_009.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯的有监督学习](img/image_06_009.jpg)'
- en: 'In this given formula:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中：
- en: '*A* and *B* are events.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A* 和 *B* 是事件。'
- en: '*P(A)* and *P(B)* are probabilities of *A* and *B*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(A)* 和 *P(B)* 分别是 *A* 和 *B* 的概率。'
- en: These are prior probabilities and are independent of each other.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些是先验概率，它们彼此独立。
- en: '*P(A | B)* is the probability of *A* with the condition that *B* is true. It
    is the posterior probability of class (*A*, target) given predictor (*B*, attributes).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(A | B)* 是在条件 *B* 为真的情况下，*A* 的概率。它是给定预测变量（*B*，属性）时，类别（*A*，目标）的后验概率。'
- en: '*P(B | A)* is the probability of *B* with the condition that *A* is true. It
    is the likelihood, which is the probability of the predictor given class.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(B | A)* 是在 *A* 为真时，*B* 的概率。它是预测给定类别的可能性，也就是预测器给定类别的概率。'
- en: Advantages of Naïve Bayes
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯的优点
- en: 'Following are some of the advantages of Naïve Bayes:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是朴素贝叶斯的一些优点：
- en: It is relatively simple to build and understand
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它相对简单，易于构建和理解
- en: It can be trained easily and doesn't require a huge dataset
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以很容易地训练，并且不需要庞大的数据集
- en: It is comparatively fast
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它相对较快
- en: Is not affected by irrelevant features
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不受无关特征的影响
- en: Disadvantages of Naïve Bayes
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯的缺点
- en: The disadvantage of Naïve Bayes is the "naïve" assumption that every feature
    is independent. This is not always true.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯的缺点是“朴素”假设，即每个特征都是独立的。这并非总是正确的。
- en: Uses of Naïve Bayes classification
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类的用途
- en: 'Here are a few uses of Naïve Bayes classification:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是朴素贝叶斯分类的一些应用：
- en: '**Naïve Bayes text classification**: This is used as a probabilistic learning
    method and is actually one of the most successful algorithms to classify documents.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯文本分类**：这是一种概率学习方法，实际上是最成功的文档分类算法之一。'
- en: '**Spam filtering**: This is the best known use case of Naïve Bayes. Naïve Bayes
    is used to identify spam e-mail from legitimate e-mail. Many server-side e-mail
    filtering mechanisms use this with other algorithms.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**垃圾邮件过滤**：这是朴素贝叶斯最知名的应用场景。朴素贝叶斯用于区分垃圾邮件和合法邮件。许多服务器端的邮件过滤机制与其他算法一起使用朴素贝叶斯。'
- en: '**Recommender systems**: Naïve Bayes can also be used to build recommender
    systems. Recommender systems are used to predict and suggest products the user
    may like in the future. It is based on unseen data and is used with collaborative
    filtering to do so. This method is more scalable and generally performs better
    than other algorithms.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐系统**：朴素贝叶斯也可用于构建推荐系统。推荐系统用于预测并建议用户可能在未来喜欢的产品。它基于未见过的数据，并与协同过滤结合使用。这种方法更具可扩展性，通常比其他算法表现更好。'
- en: To understand how Naïve Bayes classifiers actually work, we should understand
    the Bayesian rule. It was formed by Thomas Bayes in the 18^(th) century. He developed
    various mathematical principles, which are known to us as Bayesian methods. These
    very effectively describe probabilities of events and how the probabilities should
    be revised when we have additional information.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解朴素贝叶斯分类器如何实际工作，我们需要理解贝叶斯定理。它由托马斯·贝叶斯在18世纪提出。他发展了各种数学原理，这些原理今天被我们称为贝叶斯方法。这些方法有效地描述了事件的概率，以及当我们获得额外信息时，如何修正这些概率。
- en: Classifiers, based on Bayesian methods, use the training dataset to find out
    the observed probability of every class based on the values of all the features.
    So, when this classifier is used on unlabeled or unseen data, it makes use of
    the observed probabilities to predict to which class the new features belong.
    Although it is a very simple algorithm, its performance is comparable or better
    than most other algorithms.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯方法的分类器使用训练数据集，根据所有特征的值计算每个类别的观测概率。因此，当该分类器用于未标记或未见过的数据时，它会利用观测到的概率来预测新特征属于哪个类别。尽管这是一种非常简单的算法，但其性能可与大多数其他算法相媲美，甚至更好。
- en: 'Bayesian classifiers are best used for these cases:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类器最适用于以下情况：
- en: A dataset containing numerous attributes, where all of them should be considered
    simultaneously to calculate the probability of an outcome.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含大量属性的数据集，在计算结果的概率时需要同时考虑所有这些属性。
- en: Features with weak effects are generally ignored, but Bayesian classifiers use
    them too to generate predictions. Many such weak features can lead to a big change
    in the decision.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于影响较弱的特征，通常会被忽略，但贝叶斯分类器仍然会使用它们来生成预测。许多此类弱特征可能会导致决策的重大变化。
- en: How Bayesian methods work
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯方法的工作原理
- en: Bayesian methods are dependent on the concept that the estimation of likelihood
    of an event is based on the evidence at hand. The possible outcome of the situation
    is the event; for example, in a coin toss, we get heads or tails. Similarly, a
    mail can be "ham" or "spam". The trial refers to a single opportunity in which
    an event occurs. In our previous example, the coin toss is the trial.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法依赖于一个概念，即事件发生的可能性估计是基于现有证据的。事件的可能结果就是事件本身；例如，在抛硬币时，我们可能得到正面或反面。同样，邮件可能是“正常”邮件或“垃圾”邮件。试验是指发生事件的单次机会。在我们之前的例子中，抛硬币就是试验。
- en: Posterior probabilities
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后验概率
- en: '*posterior probability = conditional probability * prior probability/evidence*'
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*后验概率 = 条件概率 * 先验概率 / 证据*'
- en: In terms of classification, posterior probability refers to the probability
    that a particular object belongs to a class x when the observed feature values
    are given. For example, "what is the probability that it will rain given the temperature
    and percentage of humidity?"
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类中，后验概率指的是在给定观察到的特征值时，一个特定对象属于某个类x的概率。例如，“给定温度和湿度，概率它会下雨是多少？”
- en: '*P(rain | xi), xi = [45degrees, 95%humidity]*'
  id: totrans-222
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*P(rain | xi), xi = [45度, 95%湿度]*'
- en: Let *xi* be the feature vector of the sample *i*, where *i*" belongs to *{1,2,3,.....n*}
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设*xi*为样本*i*的特征向量，其中*i*属于*{1,2,3,...n}*。
- en: Let *wj* be the notation of the class *j*, where *j* belongs to *{1,2,3,......n}*
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设*wj*为类*j*的符号，其中*j*属于*{1,2,3,...n}*。
- en: '*P(xi | wi)* is the probability of the observing sample *xi* when it is given
    that it belongs to class *wj*'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(xi | wi)* 是观察样本*xi*的概率，前提是它属于类*wj*。'
- en: 'The general notation of posterior probabilities is:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 后验概率的通用表示法是：
- en: '*P(*wj *| xi) = P(xi | wj) * P(wj)/P(xi)*'
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*P(*wj* | xi) = P(xi | wj) * P(wj)/P(xi)*'
- en: The main objective of Naïve Bayes is to maximize the probability of the posterior
    probability on the given training data so that a decision rule can be formed.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Naïve Bayes的主要目标是最大化给定训练数据的后验概率，以便形成一个决策规则。
- en: Class-conditional probabilities
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类条件概率
- en: Bayesian classifiers assume that all the samples in the dataset are independent
    and identically distributed. Here, independence means that the probability of
    one observation is not affected by the probability of the other observation.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类器假设数据集中的所有样本是独立同分布的。这里的独立性意味着一个观察的概率不受另一个观察概率的影响。
- en: One very famous example that we discussed is the coin toss. Here the outcome
    of the first coin toss doesn't affect the subsequent coin tosses. The probability
    of getting the head or tail always remains 0.5 for an unbiased coin.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论过的一个非常著名的例子是抛硬币。在这里，第一次抛硬币的结果不会影响后续的抛硬币结果。对于一个公平的硬币，得到正面或反面的概率始终为0.5。
- en: An added assumption is that the features have conditional independence. This
    is another "naïve" assumption, which means that the estimation of the likelihood
    or the class-conditional probabilities can be done directly from the training
    data without needing to evaluate all the probabilities of x.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一个额外的假设是特征具有条件独立性。这是另一个“天真”的假设，意味着可以直接从训练数据中估计似然或类条件概率，而无需评估所有x的概率。
- en: Let's understand with an example. Suppose we have to create a server-side e-mail
    filtering application to decide if the mails are spam or not. Let's say we have
    around 1,000 e-mails and 100 e-mails are spam.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解。假设我们必须创建一个服务器端的电子邮件过滤应用程序，以决定邮件是否是垃圾邮件。假设我们有大约1000封电子邮件，其中100封是垃圾邮件。
- en: Now, we received a new mail with the text "Hello Friend". So, how should we
    calculate the class-conditional probability of the new message?
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们收到了一封新邮件，内容是“Hello Friend”。那么，我们应该如何计算这封新邮件的类条件概率呢？
- en: 'The pattern of the text consists of two features: "hello" and "friend". Now,
    we will calculate the class-conditional probability of the new mail.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的模式由两个特征组成：“hello”和“friend”。现在，我们将计算新邮件的类条件概率。
- en: 'Class-conditional probability is the probability of encountering "hello" when
    the mail is spam * the probability of encountering "friend" when the mail is spam:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 类条件概率是当邮件是垃圾邮件时遇到“hello”的概率 * 当邮件是垃圾邮件时遇到“friend”的概率：
- en: '*P(X=[hello,world] | w=spam) = P(hello | spam) * P(friend | spam)*'
  id: totrans-237
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*P(X=[hello, world] | w=spam) = P(hello | spam) * P(friend | spam)*'
- en: We can easily find out how many mails contained the word "hello" and how many
    mails contained the word "spam". However, we took the "naïve" assumption that
    one word doesn't influence the occurrence of the other. We know that "hello" and
    "friend" often occur together. Therefore, our assumption is violated.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松找出包含“hello”一词的邮件数量，以及包含“spam”一词的邮件数量。然而，我们做出了一个“天真”的假设，即一个单词不会影响另一个单词的出现。我们知道，“hello”和“friend”经常一起出现。因此，我们的假设被违反了。
- en: Prior probabilities
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先验概率
- en: Prior probability is the prior knowledge of the occurrence of an event. It is
    the general probability of the occurrence of the particular class. If the priors
    follow a uniform distribution, the posterior probabilities are determined using
    the class-conditional probabilities and also using the evidence term.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 先验概率是关于事件发生的先验知识。它是特定类别发生的总概率。如果先验分布为均匀分布，则后验概率是通过类别条件概率和证据项来确定的。
- en: Prior knowledge is obtained using the estimation on the training data, when
    the training data is the sample of the entire population.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 先验知识是通过对训练数据的估计获得的，当训练数据是整个群体的样本时。
- en: Evidence
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 证据
- en: There is one more required value to calculate posterior probability, and that
    is "evidence". The evidence P(x) is the probability of occurrence of the particular
    pattern x, which is independent of the class label.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 计算后验概率还需要一个值，那就是“证据”。证据P(x)是特定模式x发生的概率，它与类标签无关。
- en: The bag of words
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋模型
- en: 'In the previous example, we were doing classification of e-mails. For that,
    we classify a pattern. To classify a pattern, the most important tasks are:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们进行的是电子邮件的分类。为此，我们对一个模式进行分类。要对一个模式进行分类，最重要的任务是：
- en: Feature extraction
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取
- en: Feature selection
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择
- en: 'But how are good features recognized? There are some characteristics of good
    features:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何识别好的特征呢？好的特征有一些特征：
- en: The features must be important to the use case that we are building the classifier
    for
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征必须对我们为其构建分类器的用例有重要意义
- en: The selected features should have enough information to distinguish well between
    the different patterns and can be used to train the classifier
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择的特征应该包含足够的信息，能够很好地区分不同的模式，并可以用于训练分类器。
- en: The features should not be susceptible to distortion or scaling
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征不应容易受到失真或缩放的影响
- en: We need to first represent the e-mail text document as a feature vector before
    we can fit it to our model and apply machine learning algorithms. The classification
    of the text document uses the bag-of-words model. In this model, we create the
    vocabulary, which is a collection of different words that occur in all the e-mails
    (training set) and then count how many times each word occurred.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要先将电子邮件文本表示为特征向量，然后才能将其适配到我们的模型并应用机器学习算法。文本文件的分类使用的是词袋模型。在这个模型中，我们创建词汇表，这是一个包含所有电子邮件（训练集）中出现的不同单词的集合，然后统计每个单词出现的次数。
- en: Advantages of using Naïve Bayes as a spam filter
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯作为垃圾邮件过滤器的优点
- en: 'Here are the advantages of using Naïve Bayes as a spam filter:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用朴素贝叶斯作为垃圾邮件过滤器的优点：
- en: It can be personalized. It means that it can be trained on a per user basis.
    We sometime subscribe to newsletters or mailing lists or update about products,
    which may be spam to other users. Also, the e-mails that I receive have some words
    related to my work, which may be categorized as spam for other users. So, being
    a legitimate user, I would not like my mails going into spam. We may try to use
    the rules or filters, but Bayesian spam filtering is far more superior than these
    mechanisms.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以个性化。这意味着它可以基于每个用户进行训练。我们有时会订阅新闻简报、邮件列表或关于产品的更新，这些对于其他用户来说可能是垃圾邮件。此外，我收到的邮件中包含一些与我的工作相关的词汇，这些对其他用户来说可能被归类为垃圾邮件。所以，作为一个合法用户，我不希望我的邮件进入垃圾邮件箱。我们可以尝试使用规则或过滤器，但贝叶斯垃圾邮件过滤比这些机制更为优秀。
- en: Bayesian spam filters are effective in avoiding false positives, by which it
    is very less probable that legitimate e-mail will be classified as spam. For example,
    we all get mails with the word "Nigeria" or claiming to be from Nigeria, which
    are actually phishing scams. But there is the possibility that I have a relative
    or a friend there, or I have some business there; therefore that mail may not
    be illegitimate to me.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯垃圾邮件过滤器在避免误报方面非常有效，因此合法邮件被分类为垃圾邮件的可能性非常小。例如，我们都会收到包含“尼日利亚”一词或声称来自尼日利亚的邮件，这些邮件实际上是钓鱼诈骗。但是，我可能在那儿有亲戚或朋友，或者我在那里有生意；因此，这封邮件对我来说可能并不不合法。
- en: Disadvantages of Naïve Bayes filters
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 朴素贝叶斯过滤器的缺点
- en: Bayesian filters are vulnerable to Bayesian poisoning, which is a technique
    in which a large amount of legitimate text is sent with the spam mail. Therefore,
    the Bayesian filter fails there and marks it as "ham" or legitimate mail.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯过滤器容易受到贝叶斯中毒的影响，这是一种通过将大量合法文本与垃圾邮件一起发送的技术。因此，贝叶斯过滤器在此处失败，并将其标记为“ham”或合法邮件。
- en: Examples of Naïve Bayes
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯的例子
- en: 'Let us create some Naïve Bayes models using Julia:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Julia创建一些Naïve Bayes模型：
- en: '[PRE5]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We added the required `NaiveBayes` package.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了所需的`NaiveBayes`包。
- en: 'Now, let''s create some dummy datasets:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一些虚拟数据集：
- en: '[PRE6]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We created two arrays of `X` and `y`, where an element in `y` represents the
    column in `X`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了两个数组`X`和`y`，其中`y`中的每个元素表示`X`中的一列：
- en: '[PRE7]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We loaded an instance of MultinomialNB and fit our dataset to it:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了MultinomialNB的实例，并将我们的数据集拟合到它上：
- en: '[PRE8]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we will use it to predict it on our test dataset:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用它对我们的测试数据集进行预测：
- en: '[PRE9]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output that I got was:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到的输出是：
- en: '[PRE10]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Which means the first column is `b`, second is `a`, and third is also `a`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着第一列是`b`，第二列是`a`，第三列也是`a`。
- en: 'This example was on a dummy dataset. Let''s apply Naïve Bayes on an actual
    dataset. We will be using the famous iris dataset in this example:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子使用了一个虚拟数据集。让我们在一个实际数据集上应用Naïve Bayes。我们将在这个例子中使用著名的鸢尾花数据集：
- en: '[PRE11]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We loaded RDatasets, which contains the iris dataset. We created arrays for
    the feature vectors (sepal length, sepal width, petal length, and petal width).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了RDatasets，它包含了鸢尾花数据集。我们为特征向量（花萼长度、花萼宽度、花瓣长度和花瓣宽度）创建了数组。
- en: '![Examples of Naïve Bayes](img/image_06_010.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![Naïve Bayes 示例](img/image_06_010.jpg)'
- en: Now we will split the dataset for training and testing.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将拆分数据集进行训练和测试。
- en: '![Examples of Naïve Bayes](img/image_06_011.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![Naïve Bayes 示例](img/image_06_011.jpg)'
- en: This is quite straightforward, fitting the dataset to a Naïve Bayes classifier.
    We are also calculating the accuracy to which our model worked. We can see that
    the accuracy was 1.0, which is 100%.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相当简单的，将数据集拟合到Naïve Bayes分类器上。我们还计算了模型的准确性。我们可以看到准确率是1.0，也就是100%。
- en: Summary
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about machine learning and its uses. Providing computers
    the ability to learn and improve has far-reaching uses in this world. It is used
    in predicting disease outbreaks, predicting weather, games, robots, self-driving
    cars, personal assistants, and much more.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了机器学习及其应用。赋予计算机学习和改进的能力在这个世界上有着深远的应用。它被用于预测疾病爆发、天气预测、游戏、机器人、自动驾驶汽车、个人助手等众多领域。
- en: 'There are three different types of machine learning: supervised learning, unsupervised
    learning, and reinforcement learning.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习有三种不同的类型：监督学习、无监督学习和强化学习。
- en: In this chapter, we learned about supervised learning, especially about Naïve
    Bayes and decision trees. In further chapters, we will learn more about ensemble
    learning and unsupervised learning.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了监督学习，特别是Naïve Bayes和决策树。在接下来的章节中，我们将学习更多关于集成学习和无监督学习的内容。
- en: References
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://github.com/JuliaStats/MLBase.jl](https://github.com/JuliaStats/MLBase.jl)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/JuliaStats/MLBase.jl](https://github.com/JuliaStats/MLBase.jl)'
- en: '[http://julialang.org/](http://julialang.org/)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://julialang.org/](http://julialang.org/)'
- en: '[https://github.com/johnmyleswhite/NaiveBayes.jl](https://github.com/johnmyleswhite/NaiveBayes.jl)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/johnmyleswhite/NaiveBayes.jl](https://github.com/johnmyleswhite/NaiveBayes.jl)'
- en: '[https://github.com/bensadeghi/DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/bensadeghi/DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl)'
- en: '[https://github.com/bicycle1885/RandomForests.jl](https://github.com/bicycle1885/RandomForests.jl)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/bicycle1885/RandomForests.jl](https://github.com/bicycle1885/RandomForests.jl)'
- en: '[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)'
