- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Traditional Supervised Learning Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统的监督式学习算法
- en: Artificial intelligence is the new electricity.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人工智能是新的电力。
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Andrew Ng
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —Andrew Ng
- en: In *Chapter 7*, we will turn our attention to supervised machine learning algorithms.
    These algorithms, characterized by their reliance on labeled data for model training,
    are multifaceted and versatile in nature. Let’s consider some instances such as
    decision trees, **Support Vector Machines** (**SVMs**), and linear regression,
    to name a few, which all fall under the umbrella of supervised learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7章*，我们将关注监督式机器学习算法。这些算法的特点是依赖有标签的数据进行模型训练，其本质多样且灵活。让我们来看看一些实例，如决策树、**支持向量机**（**SVMs**）和线性回归等，它们都属于监督学习范畴。
- en: As we delve deeper into this field, it’s important to note that this chapter
    doesn’t cover neural networks, a significant category within supervised machine
    learning. Given their complexity and the rapid advancements occurring in the field,
    neural networks merit an in-depth exploration, which we will embark on in the
    following three chapters. The vast expanse of neural networks necessitates more
    than a single chapter to fully discuss their complexities and potential.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们对这个领域的深入探讨，需要指出的是，本章并未涉及神经网络，尽管它是监督式机器学习中的一个重要类别。鉴于神经网络的复杂性以及该领域的快速发展，神经网络值得深入探讨，我们将在接下来的三章中展开。神经网络的广泛内容需要不止一章来充分讨论它们的复杂性和潜力。
- en: In this chapter, we will delve into the essentials of supervised machine learning,
    featuring classifiers and regressors. We will explore their capabilities using
    real-world problems as case studies. Six distinct classification algorithms will
    be presented, followed by three regression techniques. Lastly, we’ll compare their
    results to encapsulate the key takeaways from this discussion.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨监督式机器学习的核心要素，重点介绍分类器和回归器。我们将通过实际问题作为案例，探索它们的能力。将展示六种不同的分类算法，接着介绍三种回归技术。最后，我们将比较它们的结果，总结本次讨论的关键要点。
- en: The overall objective of this chapter is for you to understand the different
    types of supervised machine learning techniques, and to know what the best supervised
    machine learning techniques are for certain classes of problems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的总体目标是帮助你理解不同类型的监督式机器学习技术，并了解哪些监督式机器学习技术最适用于特定类别的问题。
- en: 'The following concepts are discussed in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下概念：
- en: Understanding supervised machine learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解监督式机器学习
- en: Understanding classification algorithms
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类算法
- en: The methods to evaluate the performance of classifiers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类器性能的方法
- en: Understanding regression algorithms
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解回归算法
- en: The methods to evaluate the performance of regression algorithms
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估回归算法性能的方法
- en: Let’s start by looking at the basic concepts behind supervised machine learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先来了解监督式机器学习的基本概念。
- en: Understanding supervised machine learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解监督式机器学习
- en: Machine learning focuses on using data-driven approaches to create autonomous
    systems that can help us to make decisions with or without human supervision.
    In order to create these autonomous systems, machine learning uses a group of
    algorithms and methodologies to discover and formulate repeatable patterns in
    data. One of the most popular and powerful methodologies used in machine learning
    is the supervised machine learning approach. In supervised machine learning, an
    algorithm is given a set of inputs, called **features**, and their corresponding
    outputs, called **labels**. These features often comprise structured data like
    user profiles, historical sales figures, or sensor measurements, while the labels
    usually represent specific outcomes we want to predict, such as customer purchasing
    habits or product quality ratings. Using a given dataset, a supervised machine
    learning algorithm is used to train a model that captures the complex relationship
    between the features and labels represented by a mathematical formula. This trained
    model is the basic vehicle that is used for predictions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习侧重于利用数据驱动的方法来创建自主系统，这些系统可以帮助我们在有无人工监督的情况下做出决策。为了创建这些自主系统，机器学习使用一组算法和方法来发现并构建数据中的可重复模式。机器学习中最流行和最强大的方法之一是监督式机器学习方法。在监督式机器学习中，算法会接收一组输入，称为**特征**，以及它们对应的输出，称为**标签**。这些特征通常包括结构化数据，如用户资料、历史销售数据或传感器测量值，而标签通常代表我们希望预测的具体结果，如客户购买习惯或产品质量评分。通过给定的数据集，监督式机器学习算法用于训练一个模型，捕捉特征和标签之间通过数学公式表示的复杂关系。这个训练好的模型是用于预测的基本工具。
- en: The ability to learn from existing data in supervised learning is similar to
    the ability of the human brain to learn from experience. This learning ability
    in supervised learning uses one of the attributes of the human brain and is a
    fundamental way of opening the gates to bring decision-making power and intelligence
    to machines.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习中从现有数据中学习的能力类似于人类大脑从经验中学习的能力。这种学习能力利用了人类大脑的一个特性，是将决策能力和智能引入机器的基本方式。
- en: Let’s consider an example where we want to use supervised machine learning techniques
    to train a model that can categorize a set of emails into legitimate ones (called
    **legit**) and unwanted ones (called **spam**). In order to get started, we need
    examples from the past so that the machine can learn what sort of content of emails
    should be classified as spam.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个示例，我们希望使用监督式机器学习技术训练一个模型，能够将一组电子邮件分类为合法邮件（称为**合法**）和垃圾邮件（称为**垃圾邮件**）。为了开始，我们需要从过去的例子中获取数据，以便机器可以学习什么类型的邮件内容应被分类为垃圾邮件。
- en: This content-based learning task using text data is a complex process and is
    achieved through one of the supervised machine learning algorithms. Some examples
    of supervised machine learning algorithms that can be used to train the model
    in this example include decision trees and Naive Bayes classifiers, which we will
    discuss later in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本数据进行的基于内容的学习任务是一个复杂的过程，并通过其中一种监督式机器学习算法实现。在本示例中，可以用于训练模型的一些监督式机器学习算法示例包括决策树和朴素贝叶斯分类器，我们将在本章稍后讨论。
- en: For now, we will focus on how we can formulate supervised machine learning problems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将重点讨论如何构建监督式机器学习问题。
- en: Formulating supervised machine learning problems
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建监督式机器学习问题
- en: 'Before going deeper into the details of supervised machine learning algorithms,
    let’s define some of the basic supervised machine learning terminology:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论监督式机器学习算法的细节之前，让我们先定义一些基本的监督式机器学习术语：
- en: '| **Terminology** | **Explanation** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **术语** | **解释** |'
- en: '| Label | A label is the variable that our model is tasked with predicting.
    There can be only one label in a supervised machine learning model. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 标签是我们模型需要预测的变量。监督式机器学习模型中只能有一个标签。 |'
- en: '| Features | The set of input variables used to predict the label is called
    the features. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 用于预测标签的输入变量集称为特征。 |'
- en: '| Feature engineering | Transforming features to prepare them for the chosen
    supervised machine learning algorithm is called feature engineering. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 特征工程 | 转换特征以准备它们用于选择的监督式机器学习算法的过程称为特征工程。 |'
- en: '| Feature vector | Before providing an input to a supervised machine learning
    algorithm, all the features are combined in to a data structure called a feature
    vector. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 特征向量 | 在将输入提供给监督学习算法之前，所有的特征都会组合成一个叫做特征向量的数据结构。 |'
- en: '| Historical data | The data from the past that is used to formulate the relationship
    between the label and the features is called historical data. Historical data
    comes with examples. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 历史数据 | 用于构建标签与特征之间关系的数据，称为历史数据。历史数据包含实例。 |'
- en: '| Training/testing data | Historical data with examples is divided into two
    parts—a larger dataset called the training data and a smaller dataset called the
    testing data. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 训练/测试数据 | 历史数据通过实例分为两部分——较大的数据集称为训练数据，较小的数据集称为测试数据。 |'
- en: '| Model | A mathematical formulation of the patterns that best capture the
    relationship between the label and the features. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 一种数学形式，用于表达最能捕捉标签与特征之间关系的模式。 |'
- en: '| Training | Creating a model using training data. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 使用训练数据创建模型。 |'
- en: '| Testing | Evaluating the quality of the trained model using testing data.
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 使用测试数据评估训练模型的质量。 |'
- en: '| Prediction | The act of utilizing our trained model to estimate the label.
    In this context, “prediction” is the definitive output of the model, specifying
    a precise outcome. It’s crucial to distinguish this from “prediction probability,”
    which rather than providing a concrete result gives a statistical likelihood of
    each potential outcome. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | 使用我们训练过的模型来估计标签的过程。在这个语境中，“预测”是模型的最终输出，指定了一个确切的结果。与“预测概率”不同，后者不是给出一个具体的结果，而是提供每个潜在结果的统计可能性。
    |'
- en: A trained supervised machine learning model is capable of making predictions
    by estimating the label based on the features.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个训练过的监督学习模型能够通过根据特征来估计标签，从而进行预测。
- en: 'Let’s introduce the notation that we will use in this chapter to discuss the
    machine learning techniques:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们引入本章中将用于讨论机器学习技术的符号：
- en: '| **Variable** | **Meaning** |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **变量** | **含义** |'
- en: '| `y` | Actual label |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| `y` | 实际标签 |'
- en: '| `ý` | Predicted label |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `ý` | 预测标签 |'
- en: '| `d` | Total number of examples |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `d` | 实例总数 |'
- en: '| `b` | Number of training examples |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `b` | 训练实例的数量 |'
- en: '| `c` | Number of testing examples |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `c` | 测试实例的数量 |'
- en: '| `X_train` | Training feature vector |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| `X_train` | 训练特征向量 |'
- en: Note that in this context, an “example” refers to a single instance in our dataset.
    Each example comprises a set of features (input data) and a corresponding label
    (the outcome we’re predicting)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个语境中，“实例”指的是我们数据集中的单个实例。每个实例包含一组特征（输入数据）和一个对应的标签（我们要预测的结果）。
- en: Let’s delve into some practical applications of the terms we’ve introduced.
    Consider a feature vector, essentially a data structure encompassing all the features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨我们已经引入的一些术语的实际应用。考虑一个特征向量，本质上是一个数据结构，包含了所有的特征。
- en: For instance, if we have “n” features and “b” training examples, we represent
    this training feature vector as `X_train`. Hence, if our training dataset consists
    of five examples and five variables or features, `X_train` will have five rows—one
    for each example, and a total of 25 elements (5 examples x 5 features).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，如果我们有“n”个特征和“b”个训练实例，我们表示这个训练特征向量为 `X_train`。因此，如果我们的训练数据集包含五个实例和五个变量或特征，`X_train`
    将有五行——每个实例一行，总共有25个元素（5个实例 x 5个特征）。
- en: In this context, `X_train` is a specific term representing our training dataset.
    Each example in this dataset is a combination of features and its associated label.
    We use superscripts to denote a specific example’s row number. Thus, a single
    example in our dataset is given as (*X*^((1)), *y*^((1))) where *X*^((1)) refers
    to the features of the first example and *y*^((1)) is its corresponding label.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个语境中，`X_train` 是一个特定术语，表示我们的训练数据集。这个数据集中的每个实例是特征和其对应标签的组合。我们用上标来表示特定实例的行号。因此，我们数据集中的单个实例表示为
    (*X*^((1)), *y*^((1)))，其中 *X*^((1)) 表示第一个实例的特征，而 *y*^((1)) 是它的对应标签。
- en: Our complete labeled dataset, *D*, can therefore be expressed as *D = {( X*^((1))*,y*^((1))*),
    (y* ^((2))*,y*^((2))*), ….. , (X*^((d))*,y*^((d))*)}*, where *D* signifies the
    total number of examples.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的完整标注数据集 *D* 可以表示为 *D = {( X*^((1))*,y*^((1))*), (y* ^((2))*,y*^((2))*),
    ….. , (X*^((d))*,y*^((d))*)}*，其中 *D* 表示总的样本数量。
- en: We partition *D* into two subsets - the training set *D*[train] and the testing
    set *D*[test]. The training set, *D*[train], can be depicted as *D*[train] *=
    {( X*^((1))*,y*^((1))*), (X*^((2))*,y*^((2))*), ….. , (X*^((b))*,y*^((b))*)}*,
    where ‘*b*’ is the number of training examples.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将*D*划分为两个子集——训练集*D*[train]和测试集*D*[test]。训练集*D*[train]可以表示为*D*[train] *= {(X*^((1))*,
    y*^((1))*)，(X*^((2))*, y*^((2))*)，…..，(X*^((b))*, y*^((b))*)}*，其中‘*b*’是训练示例的数量。
- en: The primary goal of training a model is to ensure that the predicted target
    value `('ý')` for any *i*^(th) example in the training set aligns as closely as
    possible with the actual label `('y')`. This ensures that the model’s predictions
    reflect the true outcomes presented in the examples.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的主要目标是确保训练集中的任何第*i*个样本的预测目标值`('ý')`与实际标签`('y')`尽可能一致。这确保了模型的预测反映了样本中呈现的真实结果。
- en: Now, let’s see how some of these terminologies are formulated practically.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一些术语是如何在实际中被构建的。
- en: As we discussed, a feature vector is defined as a data structure that has all
    the features stored in it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论过的，特征向量被定义为一个数据结构，其中存储了所有特征。
- en: If the number of features is *n* and the number of training examples is *b*,
    then `X_train` represents the training feature vector.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征的数量是*n*，训练样本的数量是*b*，则`X_train`表示训练特征向量。
- en: For the training dataset, the feature vector is represented by `X_train`. If
    there are *b* examples in the training dataset, then `X_train` will have *b* rows.
    If there are *n* variables, then the training dataset will have a dimension of
    *n* x *b*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据集，特征向量由`X_train`表示。如果训练数据集中有*b*个样本，那么`X_train`将有*b*行。如果有*n*个变量，那么训练数据集的维度将是*n*
    x *b*。
- en: We will use superscript to represent the row number of a training example.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上标表示训练示例的行号。
- en: This particular example in our labeled dataset is represented by *(Features*^((1))*,label*^((1))*)
    = (X*^((1))*, y*^((1))*).*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的标签数据集中的这个特定示例由*(Features*^((1))*, label*^((1))*) = (X*^((1))*, y*^((1))*)表示。
- en: So, our labeled dataset is represented by *D = {(X*^((1))*,y*^((1))*), (X*^((2))*,y*^((2))*),
    ….. , (X*^((d))*,y*^((d))*)}*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的标签数据集表示为*D = {(X*^((1))*, y*^((1))*)，(X*^((2))*, y*^((2))*)，…..，(X*^((d))*,
    y*^((d))*)}*。
- en: We divide that into two parts—*D*[train] and *D*[test].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其分为两部分——*D*[train] 和 *D*[test]。
- en: So, our training set can be represented by *D*[train] *= {(X*^((1))*,y*^((1))*),
    (X*^((2))*,y*^((2))*), ….. , (X*^((b))*,y*^((b))*)}*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的训练集可以表示为*D*[train] *= {(X*^((1))*, y*^((1))*)，(X*^((2))*, y*^((2))*)，…..，(X*^((b))*,
    y*^((b))*)}*。
- en: 'The objective of training a model is that for any *i*^(th) example in the training
    set, the predicted value of the target value should be as close to the actual
    value in the examples as possible. In other words:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的目标是，对于训练集中的任何第*i*个样本，预测的目标值应该尽可能接近样本中的实际值。换句话说：
- en: '![](img/B18046_07_001.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_001.png)'
- en: So, our testing set can be represented by *D*[test] *= {X*^((1))*,y*^((1))*),
    (X*^((2))*,y*^((2))*), ..... , (X*^((c))*,y*^((c))*)}*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的测试集可以表示为*D*[test] *= {X*^((1))*, y*^((1))*), (X*^((2))*, y*^((2))*)，.....，(X*^((c))*,
    y*^((c))*)}*。
- en: 'The values of the label are represented by a vector, *Y*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 标签的值通过一个向量*Y*表示：
- en: '*Y ={y*^((1))*, y*^((2))*, ....., y*^((m))*}*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y ={y*^((1))*, y*^((2))*, ....., y*^((m))}*。'
- en: Let’s illustrate the concepts with an example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来说明这些概念。
- en: Let’s imagine we’re working on a project to predict house prices based on various
    features, like the number of bedrooms, the size of the house in square feet, and
    its age. Here’s how we’d apply our machine learning terminology to this real-world
    scenario.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在进行一个项目，目的是根据各种特征预测房价，比如卧室数量、房屋面积和房龄。我们将如何将机器学习术语应用到这个现实场景中呢？
- en: In this context, our “features” would be the number of bedrooms, house size,
    and age. Let’s say we have 50 examples (i.e., 50 different houses for which we
    have these details and the corresponding price). We can represent these in a training
    feature vector called `X_train`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的“特征”将是卧室数量、房屋面积和房龄。假设我们有50个样本（即50个不同的房子，并且我们拥有这些房子的一些特征以及对应的价格）。我们可以将它们表示为一个训练特征向量，叫做`X_train`。
- en: '`X_train` becomes a table with 50 rows (one for each house) and 3 columns (one
    for each feature: bedrooms, size, and age). It’s a 50 x 3 matrix holding all our
    feature data.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train`变成一个包含50行（每个房子的行）和3列（每个特征：卧室数、面积和房龄）的表格。这是一个50 x 3的矩阵，保存了我们所有的特征数据。'
- en: An individual house’s feature set and price might be represented as *((X*^((i))*,y*^((i))*))*,
    where *X*^((i)) contains the features of the *i*^(th) house and *y*^((i)) is its
    actual price.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 单个房屋的特征集和价格可能表示为*((X*^((i))*,y*^((i))*))*，其中*X*^((i))包含第*i*个房屋的特征，*y*^((i))是其实际价格。
- en: Our entire dataset *D* can then be viewed as *D =* *{(X*^((1))*,y*^((1))*),
    (X*^((2))*,y*^((2))*)), ... , ((X*^((50))*,y*^((50))*))}*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的整个数据集*D*可以表示为*D =* *{(X*^((1))*,y*^((1))*), (X*^((2))*,y*^((2))*), ... ,
    ((X*^((50))*,y*^((50))*))}*。
- en: 'Suppose we use 40 houses for training and the remaining 10 for testing. Our
    training set *D*[train] would be the first 40 examples: *{(X*^((1))*,y*^((1))*),
    (X*^((2))*,y*^((2))*)), ... , ((X*^((40))*,y*^((40))*))}*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用40个房屋进行训练，剩余的10个用于测试。我们的训练集*D*[train]将是前40个样本：*{(X*^((1))*,y*^((1))*),
    (X*^((2))*,y*^((2))*), ... , ((X*^((40))*,y*^((40))*))}*。
- en: After training our model, the goal is to predict house prices ![](img/B18046_07_002.png)
    that closely match the actual prices ![](img/B18046_07_003.png) for all houses
    in our training set.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后，我们的目标是预测房价 ![](img/B18046_07_002.png)，这些预测结果应与所有训练集中房屋的实际价格 ![](img/B18046_07_003.png)紧密匹配。
- en: 'Our testing set *D*[test] consists of the remaining 10 examples: *{(X*^((41))*,y*^((41))*),
    (X*^((42))*,y*^((42))*), ... , (X*^((50))*,y*^((50))*))}*.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试集*D*[test]由剩余的10个样本组成：*{(X*^((41))*,y*^((41))*), (X*^((42))*,y*^((42))*),
    ... , (X*^((50))*,y*^((50))*))}*。
- en: 'Lastly, we have the *Y* vector, comprising all our actual house prices: *Y
    ={ y*^((1))*, y*^((2))*, ....., y*^((50))*}*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有*Y*向量，包含了我们所有实际的房价：*Y ={ y*^((1))*, y*^((2))*, ....., y*^((50))*}*。
- en: With this concrete example, we can see how these concepts and equations translate
    into practice when predicting house prices with supervised machine learning.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个具体的例子，我们可以看到在使用监督学习预测房价时，这些概念和方程是如何转化为实践的。
- en: Understanding enabling conditions
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解启用条件
- en: 'A supervised machine learning algorithm needs certain enabling conditions to
    be met in order to perform. Enabling conditions are certain prerequisites that
    ensure the efficacy of a supervised machine learning algorithm. These enabling
    conditions are as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个监督学习算法需要满足某些条件才能执行。启用条件是确保监督学习算法有效性的某些前提条件。这些启用条件如下：
- en: '**Enough examples**: Supervised machine learning algorithms need enough examples
    to train a model. We say that we have enough examples when we have conclusive
    evidence that the pattern of interest is fully represented in our dataset.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**足够的示例**：监督学习算法需要足够的示例来训练模型。当我们有确凿证据表明兴趣模式在数据集中得到了充分表示时，我们就认为有足够的示例。'
- en: '**Patterns in historical data**: The examples used to train a model need to
    have patterns in them. The likelihood of the occurrence of our event of interest
    should be dependent on a combination of patterns, trends, and events. The label
    mathematically represents the event of interest in our model. Without these, we
    are dealing with random data that cannot be used to train a model.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**历史数据中的模式**：用于训练模型的示例需要具有某种模式。我们关注的事件发生的可能性应该依赖于模式、趋势和事件的组合。在我们的模型中，标签在数学上表示了我们关注的事件。如果没有这些，我们所处理的数据就是随机数据，无法用于训练模型。'
- en: '**Valid assumptions**: When we train a supervised machine learning model using
    examples, we expect that the assumptions that apply to the examples will also
    be valid in the future. Let’s look at an actual example. If we want to train a
    machine learning model for the government that can predict the likelihood of whether
    a visa will be granted to a student, the understanding is that the laws and policies
    will not change when the model is used for predictions. If new policies or laws
    are enforced after training the model, the model may need to be retrained to incorporate
    this new information.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有效假设**：当我们使用示例训练监督学习模型时，我们期望适用于这些示例的假设在未来也能有效。让我们看一个实际的例子。如果我们想为政府训练一个可以预测学生签证是否会被批准的机器学习模型，那么理解是，当该模型用于预测时，法律和政策不会发生变化。如果在训练模型后执行了新政策或新法律，模型可能需要重新训练以纳入这些新信息。'
- en: Let us look into how we can differentiate between a classifier and a regressor.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨如何区分分类器和回归器。
- en: Differentiating between classifiers and regressors
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分分类器和回归器
- en: 'In a machine learning model, the label can be a category variable or a continuous
    variable. Continuous variables are numeric variables that can have an infinite
    number of values between two values, while categorical variables are qualitative
    variables that are classified into distinct categories. The type of label determines
    what type of supervised machine learning model we have. Fundamentally, we have
    two types of supervised machine learning models:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型中，标签可以是类别变量或连续变量。连续变量是数值变量，可以在两个值之间具有无限多个值，而类别变量是定性变量，可以分类为不同的类别。标签的类型决定了我们使用的有监督机器学习模型的类型。从根本上讲，我们有两种类型的有监督机器学习模型：
- en: '**Classifiers**: If the label is a category variable, the machine learning
    model is called a classifier. Classifiers can be used to answer the following
    type of business questions:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器**：如果标签是类别变量，则机器学习模型被称为分类器。分类器可以用来回答以下类型的业务问题：'
- en: Is this abnormal tissue growth a malignant tumor?
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种异常的组织生长是恶性肿瘤吗？
- en: Based on the current weather conditions, will it rain tomorrow?
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据当前的天气条件，明天会下雨吗？
- en: Based on the profile of a particular applicant, should their mortgage application
    be approved?
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据某位特定申请人的资料，是否应批准他们的按揭申请？
- en: '**Regressors**: If the label is a continuous variable, we train a regressor.
    Regressors can be used to answer the following types of business questions:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归器**：如果标签是连续变量，则我们训练一个回归器。回归器可以用来回答以下类型的业务问题：'
- en: Based on the current weather condition, how much will it rain tomorrow?
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据当前的天气状况，明天会下多少雨？
- en: What will the price of a particular home be with given characteristics?
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定某一特定房屋的特征，价格会是多少？
- en: Let’s look at both classifiers and regressors in more detail.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看分类器和回归器。
- en: Understanding classification algorithms
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分类算法
- en: 'In supervised machine learning, if the label is a category variable, the model
    is categorized as a classifier. Recall that the model is essentially a mathematical
    representation learned from the training data:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在有监督机器学习中，如果标签是类别变量，则模型被归类为分类器。回忆一下，模型本质上是从训练数据中学习到的数学表示：
- en: The historical data is called **labeled data**.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史数据被称为**已标记数据**。
- en: The production data, which the label needs to be predicted for, is called **unlabeled
    data**.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要预测标签的生产数据被称为**未标记数据**。
- en: The ability to accurately label unlabeled data using a trained model is the
    real power of classification algorithms. Classifiers predict labels for unlabeled
    data to answer a particular business question.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练过的模型准确地标记未标记的数据是分类算法的真正力量。分类器预测未标记数据的标签，以回答特定的业务问题。
- en: Before we present the details of classification algorithms, let’s first present
    a business problem that we will use as a challenge for classifiers. We will then
    use six different algorithms to answer the same challenge, which will help us
    compare their methodology, approach, and performance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们展示分类算法的细节之前，让我们先展示一个业务问题，作为分类器的挑战。然后我们将使用六种不同的算法回答相同的挑战，这将帮助我们比较它们的方法论、思路和性能。
- en: Presenting the classifiers challenge
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 展示分类器挑战
- en: 'We will first present a common problem, which we will use as a challenge to
    test six different classification algorithms. This common problem is referred
    to as the classifier challenge in this chapter. Using all six classifiers to solve
    the same problem will help us in two ways:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先展示一个常见问题，作为挑战测试六种不同的分类算法。这个常见问题在本章中被称为分类器挑战。使用所有六个分类器来解决相同的问题将帮助我们从两个方面进行改进：
- en: All the input variables need to be processed and assembled as a complex data
    structure, called a feature vector. Using the same feature vector helps us avoid
    repeating data preparation for all six algorithms.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有输入变量需要处理并组装成一个复杂的数据结构，称为特征向量。使用相同的特征向量帮助我们避免为所有六种算法重复数据准备。
- en: We can accurately compare the performance of various algorithms as we use the
    same feature vector for input.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于使用相同的特征向量作为输入，我们可以准确比较各种算法的性能。
- en: The classifiers challenge is about predicting the likelihood of a person making
    a purchase. In the retail industry, one of the things that can help maximize sales
    is understanding better the behavior of the customers. This can be done by analyzing
    the patterns found in historical data. Let’s state the problem, first.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器挑战是预测一个人是否会进行购买的可能性。在零售行业，能够帮助最大化销售的一件事就是更好地理解顾客的行为。这可以通过分析历史数据中发现的模式来实现。我们先陈述问题。
- en: The problem statement
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题陈述
- en: Given the historical data, can we train a binary classifier that can predict
    whether a particular user will eventually buy a product based on their profile?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 给定历史数据，我们能否训练一个二元分类器，预测一个特定用户是否最终会根据其个人资料购买产品？
- en: 'First, let’s explore the labeled dataset available to solve this problem:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们探索可用的标记数据集来解决这个问题：
- en: '![](img/B18046_07_004.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_004.png)'
- en: Note that *x* is a member of a set of real numbers. ![](img/B18046_07_005.png)
    indicates that it is a vector with *b* real-time features. ![](img/B18046_07_006.png)
    implies that is a binary variable, as we are dealing with a binary classification
    problem. The output can be `0` or `1`, where each number represents a different
    class.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*x* 是实数集合的一个成员。![](img/B18046_07_005.png) 表示它是一个具有 *b* 个实时特征的向量。![](img/B18046_07_006.png)
    意味着它是一个二元变量，因为我们处理的是一个二分类问题。输出可以是 `0` 或 `1`，每个数字代表一个不同的类别。
- en: For this particular example, when `y = 1`, we call it a positive class, and
    when `y = 0`, we call it a negative class. To make it more tangible, when `y`
    equals `1`, we’re dealing with a positive class, meaning the user is likely to
    make a purchase. Conversely, when `y` equals `0`, it represents the negative class,
    suggesting the user isn’t likely to buy anything. This model will allow us to
    predict future user behavior based on their historical actions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个具体的示例，当 `y = 1` 时，我们称其为正类，当 `y = 0` 时，我们称其为负类。为了使其更具体，当 `y` 等于 `1` 时，我们处理的是正类，意味着用户可能会进行购买。相反，当
    `y` 等于 `0` 时，它表示负类，意味着用户不太可能购买任何东西。该模型将帮助我们根据用户的历史行为预测未来的用户行为。
- en: Although the level of the positive and negative classes can be chosen arbitrarily,
    it is a good practice to define the positive class as the event of interest. If
    we try to flag the fraudulent transaction for a bank, then the positive class
    (that is, `y = 1`) should be the fraudulent transaction, not the other way around.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管正类和负类的水平可以任意选择，但一个好的做法是将正类定义为感兴趣的事件。如果我们尝试为银行标记欺诈交易，那么正类（即 `y = 1`）应该是欺诈交易，而不是反过来。
- en: 'Now, let’s look at the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看以下内容：
- en: The actual label, denoted by *y*
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际标签，用 *y* 表示
- en: The predicted label, denoted by *ý*
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测标签，用 *ý* 表示
- en: Note that for our classifiers challenge, the actual value of the label found
    in this example is represented by *y*. If, in our example, someone has purchased
    an item, we say *y =1*. The predicted values are represented by *ý*. The input
    feature vector, *x*, will have a dimension equal to the number of input variables.
    We want to determine what the probability is that a user will make a purchase,
    given a particular input.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于我们的分类器挑战，示例中标签的实际值用 *y* 来表示。如果在我们的示例中，有人购买了一个商品，我们就说 *y = 1*。预测值用 *ý*
    来表示。输入特征向量 *x* 的维度将等于输入变量的数量。我们希望确定给定特定输入时，用户购买商品的概率。
- en: 'So, we want to determine the probability that *y = 1*, given a particular value
    of feature vector *x*. Mathematically, we can represent this as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们希望确定在给定特征向量 *x* 的特定值时，*y = 1* 的概率。从数学上讲，我们可以这样表示：
- en: '![](img/B18046_07_007.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_007.png)'
- en: Note that the expression *P*(*y* = *1*|*x*) represents the conditional probability
    of the event *y* being equal to 1, given the occurrence of event *x*. In other
    words, it represents the probability of the outcome y being true or positive,
    given the knowledge or presence of a specific condition *x*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，表达式 *P*(*y* = *1*|*x*) 表示在事件 *x* 发生的条件下，事件 *y* 等于 1 的条件概率。换句话说，它表示在特定条件
    *x* 的知识或存在下，结果 *y* 为真的概率，即 *y* 为正类的概率。
- en: Now, let’s look at how we can process and assemble different input variables
    in the feature vector, *x*. The methodology for assembling different parts of
    *x* using the processing pipeline is discussed in more detail in the following
    section.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何处理和组装特征向量 *x* 中的不同输入变量。使用处理管道组装 *x* 不同部分的方法将在下一部分中更详细地讨论。
- en: Feature engineering using a data processing pipeline
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用数据处理管道进行特征工程
- en: Preparing data for a chosen machine learning algorithm is called **feature engineering**
    and is a crucial part of the machine learning life cycle. Feature engineering
    is done in different stages or phases. The multi-stage processing code used to
    process data is collectively known as a **data pipeline**. Making a data pipeline
    using standard processing steps, wherever possible, makes it reusable and decreases
    the effort needed to train the models. By using more well-tested software modules,
    the quality of the code is also enhanced.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为所选机器学习算法准备数据的过程称为**特征工程**，是机器学习生命周期中的关键部分。特征工程可以分为不同的阶段或阶段。用于处理数据的多阶段处理代码统称为**数据管道**。尽可能使用标准处理步骤构建数据管道，可以使其可重用，并减少训练模型所需的工作量。通过使用更多经过充分测试的软件模块，代码的质量也得到了提升。
- en: In addition to feature engineering, it’s important to note that data cleaning
    is a crucial part of this process as well. This involves addressing issues like
    outlier detection and missing value treatment. For instance, outlier detection
    allows you to identify and handle anomalous data points that could negatively
    impact your model’s performance. Similarly, missing value treatment is a technique
    used to fill in or handle missing data points in your dataset, ensuring your model
    has a complete picture of the data. These are important steps to be included in
    the data pipeline, helping to improve the reliability and accuracy of your machine
    learning models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征工程，值得注意的是，数据清洗也是此过程中的一个关键部分。这涉及到解决异常值检测和缺失值处理等问题。例如，异常值检测可以帮助你识别并处理那些可能会负面影响模型性能的异常数据点。同样，缺失值处理是用来填充或处理数据集中的缺失数据点，确保模型能够获得完整的数据视图。这些步骤是数据管道中不可或缺的一部分，有助于提高机器学习模型的可靠性和准确性。
- en: Let’s design a reusable processing pipeline for the classifiers challenge. As
    mentioned, we will prepare data once and then use it for all the classifiers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为分类器挑战设计一个可重用的处理管道。如前所述，我们将一次性准备数据，然后用于所有分类器。
- en: Importing data
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 导入数据
- en: 'Let us start by importing the necessary libraries:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入必要的库：
- en: '[PRE0]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that we will use the `pandas` library in Python, which is a powerful open-source
    data manipulation and analysis tool that provides high-performance data structures
    and data analysis tools. We will also use `sklearn`, which provides a comprehensive
    suite of tools and algorithms for various machine learning tasks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将使用Python中的`pandas`库，它是一个强大的开源数据操作与分析工具，提供高性能的数据结构和数据分析工具。我们还将使用`sklearn`，它提供了一套全面的工具和算法，用于各种机器学习任务。
- en: Importing data
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 导入数据
- en: 'The labeled data for this problem containing the examples is stored in a file
    called `Social_Network_Ads.csv` in the `CSV` format. Let us start by reading this
    file:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该问题的标签数据包含了示例，存储在一个名为`Social_Network_Ads.csv`的`CSV`格式文件中。我们从读取这个文件开始：
- en: '[PRE1]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This file can be downloaded from [https://storage.googleapis.com/neurals/data/Social_Network_Ads.csv](https://storage.googleapis.com/neurals/data/Social_Network_Ads.csv).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件可以从[https://storage.googleapis.com/neurals/data/Social_Network_Ads.csv](https://storage.googleapis.com/neurals/data/Social_Network_Ads.csv)下载。
- en: Feature selection
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征选择
- en: The process of selecting features that are relevant to the context of the problem
    that we want to solve is called **feature selection**. It is an essential part
    of feature engineering.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 选择与我们想要解决的问题相关的特征的过程称为**特征选择**。这是特征工程中的一个重要部分。
- en: Once the file is imported, we drop the `User ID` column, which is used to identify
    a person and should be excluded when training a model. Generally, `User ID` is
    an identifying field that uniquely represents each person but holds no meaningful
    contribution to the patterns or trends we try to model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件导入后，我们会删除`User ID`列，该列用于识别个人，在训练模型时应该排除。通常，`User ID`是一个唯一标识每个人的字段，但对我们试图建模的模式或趋势没有实际意义。
- en: 'For this reason, it’s a common practice to drop such columns before training
    a machine learning model:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练机器学习模型之前，通常的做法是删除这些列：
- en: '[PRE2]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let’s preview the dataset using the `head` command, which will print the
    first five rows of this dataset:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`head`命令预览数据集，它将打印出该数据集的前五行：
- en: '[PRE3]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The dataset looks like this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集如下所示：
- en: '![A table with numbers and text  Description automatically generated](img/B18046_07_01.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含数字和文本的表格  自动生成的描述](img/B18046_07_01.png)'
- en: 'Figure 7.1: Example dataset'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.1: 示例数据集'
- en: Now, let’s look at how we can further process the input dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何进一步处理输入数据集。
- en: One-hot encoding
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 独热编码
- en: Several machine learning models operate best when all features are expressed
    as continuous variables. This stipulation implies that we need an approach to
    transforming categorical features into continuous ones. One common technique to
    achieve this is ‘one-hot encoding.’
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习模型在所有特征都作为连续变量表示时表现最好。这一要求意味着我们需要一种方法将类别特征转换为连续特征。一种常见的技术是‘独热编码’。
- en: In our context, the `Gender` feature is categorical, and we aim to convert it
    into a continuous variable using one-hot encoding. But what is one-hot encoding,
    exactly?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的背景下，`Gender`特征是类别型的，我们的目标是使用独热编码将其转换为连续变量。但是，究竟什么是独热编码呢？
- en: 'One-hot encoding is a process that transforms a categorical variable into a
    format that machine learning algorithms can understand better. It does so by creating
    new binary features for each category in the original feature. For example, if
    we apply one-hot encoding to ‘`Gender`, ‘ it would result in two new features:
    `Male` and `Female`. If the gender is `Male`, the ‘`Male`'' feature would be 1
    (indicating true), and ‘`Female`'' would be 0 (indicating false), and vice versa.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码是一种将类别变量转换为机器学习算法能更好理解的格式的过程。它通过为原始特征中的每个类别创建新的二进制特征来实现这一点。例如，如果我们对‘`Gender`’应用独热编码，它将生成两个新特征：`Male`和`Female`。如果性别是`Male`，则‘`Male`’特征为1（表示真），‘`Female`’特征为0（表示假），反之亦然。
- en: 'Let’s now apply this one-hot encoding process to our ‘`Gender`'' feature and
    continue our model preparation process:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这个独热编码过程应用到我们的‘`Gender`’特征上，并继续我们的模型准备过程：
- en: '[PRE4]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `drop='first'` parameter indicates that the first category in the ‘`Gender`'
    feature should be dropped.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`drop=''first''`参数表示应删除‘`Gender`’特征中的第一个类别。'
- en: 'First, let us perform one-hot encoding on ‘`Gender`'':'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们对‘`Gender`’进行独热编码：
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we use the `fit_transform` method to apply one-hot encoding to the ‘`Gender`'
    column. The `reshape(-1, 1)` function is used to ensure that the data is in the
    correct 2D format expected by the encoder. The `toarray()` function is used to
    convert the output, which is a sparse matrix, into a dense `numpy` array for easier
    manipulation later on.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`fit_transform`方法对‘`Gender`’列应用独热编码。`reshape(-1, 1)`函数用于确保数据符合编码器所期望的正确二维格式。`toarray()`函数用于将输出的稀疏矩阵转换为密集的`numpy`数组，以便后续更方便地处理。
- en: 'Next, let us add the encoded `Gender` back to the dataframe:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将编码后的`Gender`重新添加到数据框中：
- en: '[PRE6]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that this line of code adds the encoded ‘`Gender`' data back to the DataFrame.
    Since we’ve set `drop='first'`, and assuming that the ‘`Male`' category is considered
    the first category, our new column, ‘`Female`,’ will have a value of `1` if the
    gender is female, and `0` if it is male.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这行代码将编码后的‘`Gender`’数据重新添加到数据框中。由于我们设置了`drop='first'`，假设‘`Male`’类别被视为第一个类别，那么我们新添加的列‘`Female`’将具有`1`的值（表示女性），如果性别是男性，则为`0`。
- en: 'Then, we drop the original `Gender` column from the DataFrame, as it has now
    been replaced with our new `Female` column:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从数据框中删除原始的`Gender`列，因为它已经被我们的新`Female`列所替代：
- en: '[PRE7]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once it’s converted, let’s look at the dataset again:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦转换完成，让我们再次查看数据集：
- en: '[PRE8]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Table  Description automatically generated](img/B18046_07_02.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated](img/B18046_07_02.png)'
- en: 'Figure 7.2: Add a caption here….'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.2: 在这里添加标题…'
- en: Notice that in order to convert a variable from a category variable into a continuous
    variable, one-hot encoding has converted `Gender` into two separate columns—`Male`
    and `Female`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了将变量从类别变量转换为连续变量，独热编码已将`Gender`转换为两个单独的列——`Male`和`Female`。
- en: Let us look into how we can specify the features and labels.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何指定特征和标签。
- en: Specifying the features and label
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指定特征和标签
- en: 'Let’s specify the features and labels. We will use `y` through out this book
    to represent the label and `X` to represent the feature set:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们指定特征和标签。在本书中，我们将使用`y`表示标签，使用`X`表示特征集：
- en: '[PRE9]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`X` represents the feature vector and contains all the input variables that
    we need to use to train the model.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`X`表示特征向量，包含我们需要用于训练模型的所有输入变量。'
- en: Dividing the dataset into testing and training portions
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将数据集划分为测试集和训练集
- en: 'Next, we will partition our dataset into two parts: 70% for training and 30%
    for testing. The rationale behind this particular division is that, as a rule
    of thumb in machine learning practice, we want a sizable portion of the dataset
    to train our model so that it can learn effectively from various examples. This
    is where the larger 70% comes into play. However, we also need to ensure that
    our model generalizes well to unseen data and doesn’t just memorize the training
    set. To evaluate this, we will set aside 30% of the data for testing. This data
    is not used during the training process and acts as a benchmark for gauging the
    trained model’s performance and its ability to make predictions on new, unseen
    data:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把数据集分成两部分：70%用于训练，30%用于测试。这个划分的理由是，在机器学习实践中，有一个经验法则，我们希望数据集的大部分用于训练模型，使其能够从各种示例中有效学习。这里的70%便是为了这个目的。然而，我们也需要确保我们的模型能够很好地推广到未见过的数据，而不仅仅是记住训练集。为了评估这一点，我们将把30%的数据保留用于测试。这部分数据在训练过程中不会被使用，作为衡量训练模型性能和其对新、未见数据进行预测能力的基准：
- en: '[PRE10]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This has created the following four data structures:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经创建了以下四个数据结构：
- en: '`X_train`: A data structure containing the features of the training data'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_train`：包含训练数据特征的数据结构'
- en: '`X_test`: A data structure containing the features of the training test'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_test`：包含测试数据集特征的数据结构'
- en: '`y_train`: A vector containing the values of the label in the training dataset'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_train`：包含训练数据集中标签值的向量'
- en: '`y_test`: A vector containing the values of the label in the testing dataset'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_test`：包含测试数据集标签值的向量'
- en: Let us now apply feature normalization to the dataset.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对数据集应用特征归一化。
- en: Scaling the features
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征缩放
- en: As we proceed with the preparation of our dataset for our machine learning model,
    an important step is **feature normalization**, also known as scaling. In many
    machine learning algorithms, scaling the variables to a uniform range, typically
    from 0 to 1, can enhance the model’s performance by ensuring that no individual
    feature can dominate others due to its scale.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备我们的数据集以供机器学习模型使用时，一个重要步骤是**特征归一化**，也称为缩放。在许多机器学习算法中，特征变量被缩放到统一的范围，通常是从0到1，这可以通过确保没有单一特征因其尺度过大而主导其他特征，从而提高模型的性能。
- en: This process can also help the algorithm converge more quickly to the solution.
    Now, let’s apply this transformation to our dataset for optimal results.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程还可以帮助算法更快地收敛到解决方案。现在，让我们对数据集应用这一变换，以获得最佳结果。
- en: 'First, we initialize an instance of the `StandardScaler` class, which will
    be used to conduct the scaling operation:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化一个`StandardScaler`类的实例，它将用于执行缩放操作：
- en: '[PRE11]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we use the `fit_transform` method. This transformation scales the features
    such that they have a mean of `0` and a standard deviation of `1`, which is the
    essence of standard scaling. The transformed data is stored in the `X_train_scaled`
    variable:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`fit_transform`方法。这个变换会对特征进行缩放，使它们的均值为`0`，标准差为`1`，这就是标准化缩放的本质。转换后的数据存储在`X_train_scaled`变量中：
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we will apply the `transform` method, which applies the same transformation
    (as in the prior code) to the test dataset `X_test`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将应用`transform`方法，它将相同的变换（如前面的代码所示）应用于测试数据集`X_test`：
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: After we scale the data, it is ready to be used as input to the different classifiers
    that we will present in the subsequent sections.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们缩放数据后，它已经准备好作为不同分类器的输入，这些分类器将在后续章节中展示。
- en: Evaluating the classifiers
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估分类器
- en: 'Once the model is trained, we need to evaluate its performance. To do that,
    we will use the following process:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们需要评估其性能。为此，我们将使用以下过程：
- en: We will divide the labeling dataset into two parts—a training partition and
    a testing partition. We will use the testing partition to evaluate the trained
    model.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把标记数据集分为两部分——训练分区和测试分区。我们将使用测试分区来评估训练好的模型。
- en: We will use the features of our testing partition to generate labels for each
    row. This is our set of predicted labels.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用测试分区的特征来生成每一行的标签。这是我们的预测标签集。
- en: We will compare the set of predicted labels with the actual labels to evaluate
    the model.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将比较预测标签集与实际标签，以评估模型。
- en: Unless we try to solve something quite trivial, there will be some misclassifications
    when we evaluate the model. How we interpret these misclassifications to determine
    the quality of the model depends on which performance metrics we choose to use.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 除非我们试图解决一些非常简单的问题，否则在评估模型时会出现一些误分类。我们如何解读这些误分类来确定模型质量，取决于我们选择使用哪些性能指标。
- en: Once we have both the set of actual labels and the predicted labels, a bunch
    of performance metrics can be used to evaluate the models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了实际标签和预测标签，就可以使用一系列性能指标来评估模型。
- en: The best metric for quantifying the model will depend on the requirements of
    the business problem that we want to solve, as well as the characteristics of
    the training dataset.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 量化模型的最佳指标将取决于我们要解决的业务问题的需求，以及训练数据集的特点。
- en: Let us now look at the confusion matrix.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下混淆矩阵。
- en: Confusion matrices
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'A confusion matrix is used to summarize the results of the evaluation of a
    classifier. The confusion matrix for a binary classifier looks as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵用于总结分类器评估的结果。二分类器的混淆矩阵如下所示：
- en: '![Diagram  Description automatically generated](img/B18046_07_03.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图示  描述自动生成](img/B18046_07_03.png)'
- en: 'Figure 7.3: Confusion matrix'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：混淆矩阵
- en: If the label of the classifier we train has two levels, it is called a **binary
    classifier**. The first critical use case of supervised machine learning—specifically,
    a binary classifier—was during the First World War to differentiate between an
    aircraft and flying birds.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练的分类器标签有两个级别，那么它就是一个**二分类器**。监督式机器学习的第一个关键应用——特别是二分类器——是在第一次世界大战期间，用于区分飞机和飞鸟。
- en: 'The classification can be divided into the following four categories:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 分类可以分为以下四类：
- en: '**True Positives** (**TPs**): The positive classifications that were correctly
    classified'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正阳性** (**TPs**)：正确分类为正类的正类'
- en: '**True Negatives** (**TNs**): The negative classifications that were correctly
    classified'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性** (**TNs**)：正确分类为负类的负类'
- en: '**False Positives** (**FPs**): The positive classifications that were actually
    negative'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性** (**FPs**)：实际上是负类的被分类为正类'
- en: '**False Negatives** (**FNs**): The negative classifications that were actually
    positive'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性** (**FNs**)：实际上是正类的被分类为负类'
- en: Let’s see how we can use these four categories to create various performance
    metrics.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用这四个类别来创建各种性能指标。
- en: A confusion matrix provides a comprehensive snapshot of a model’s performance
    by detailing the number of correct and incorrect predictions. It enumerates TPs,
    TNs, FPs, and FNs. Among these, the correct classifications refer to the instances
    where our model correctly identified the class, i.e., TPs and TNs. The model’s
    accuracy, which signifies the proportion of these correct classifications (TPs
    and TNs) out of all the predictions made, can then be calculated directly from
    this confusion matrix. A confusion matrix gives you the number of correct classifications
    and misclassifications through a count of TPs, TNs, FPs, and FNs. The model accuracy
    is defined as the proportion of correct classifications among all predictions
    and can be easily seen from the confusion matrix as follows.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵通过详细列出正确和错误预测的数量，提供了模型性能的全面快照。它列举了TPs、TNs、FPs 和 FNs。在这些中，正确分类指的是模型正确识别类别的实例，即TPs和TNs。模型的准确率是指这些正确分类（TPs和TNs）在所有预测中所占的比例，可以直接从这个混淆矩阵中计算得到。混淆矩阵通过计算TPs、TNs、FPs和FNs，告诉你正确分类和误分类的数量。模型准确率定义为所有预测中正确分类所占的比例，可以从混淆矩阵中轻松看到，如下所示。
- en: When we have an approximately equal number of positive and negative examples
    in our data – a situation known as balanced classes – the accuracy metric can
    provide a valuable measure of our model’s performance. In other words, accuracy
    is the ratio of correct predictions made by the model to the total number of predictions.
    For example, if our model correctly identifies 90 out of 100 test instances, whether
    they are positive or negative, its accuracy will be 90%. This metric can give
    us a general understanding of how well our model performs across both classes.
    If our data has balanced classes (i.e., the total number of positive examples
    is roughly equal to the number of negative examples), than the accuracy will give
    us a good insight into the quality of our trained model. Accuracy is the proportion
    of correction classifications among all predictions.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的数据中有近似相等数量的正例和负例示例时——这种情况称为平衡类——准确率指标可以提供一个有价值的模型性能度量。换句话说，准确率是模型正确预测数量与总预测数量的比率。例如，如果我们的模型在100个测试实例中正确识别了90个，无论它们是正例还是负例，其准确率将为90%。这个度量可以让我们大致了解模型在两个类别中的整体表现。如果我们的数据具有平衡的类别（即正例总数大致等于负例总数），那么准确率将为我们提供对训练模型质量的良好洞察。准确率是在所有预测中分类正确的比例。
- en: 'Mathematically:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上：
- en: '![](img/B18046_07_008.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_008.png)'
- en: Understanding recall and precision
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解召回率和精确度
- en: While calculating accuracy, we do not differentiate between TPs and TNs. Evaluating
    a model through accuracy is straightforward, but when the data has imbalanced
    classes, it will not accurately quantify the quality of the trained model. When
    the data has imbalanced classes, two additional metrics will better quantify the
    quality of the trained model, recall and precision. We will use an example of
    a popular diamond mining process to explain the concepts of these two additional
    metrics.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算准确率时，我们不区分TP和TN。通过准确率评估模型是直接的，但是当数据存在不平衡类时，它将无法准确量化训练模型的质量。当数据存在不平衡类时，另外两个指标可以更好地量化训练模型的质量，即召回率和精确度。我们将利用一个流行的钻石采矿过程的例子来解释这两个额外指标的概念。
- en: For centuries, alluvial diamond mining has been one of the most popular ways
    of extracting diamonds from the sand of riverbeds all over the world. Erosion
    over thousands of years is known to wash diamonds from their primary deposits
    to riverbeds in different parts of the world. To mine diamonds, people have collected
    sand from the banks of rivers in a large open pit. After going though extensive
    washing, a large number of rocks are left in the pit.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 几个世纪以来，冲积钻石采矿一直是从全世界各地河床沙中提取钻石的最流行方式之一。数千年的侵蚀已知将钻石从其主要矿床冲刷到世界各地的河床中。为了采矿钻石，人们在一个大型露天矿坑中从河岸收集沙子。经过大量冲洗后，矿坑中留下大量岩石。
- en: A vast majority of these washed rocks are just ordinary stones. Identifying
    one of the rocks as a diamond is rare but a very important event. In our scenario,
    the owners of a mine are experimenting with the use of computer vision to identify
    which of the washed rocks are just ordinary rocks and which of the washed rocks
    are diamonds. They are using shape, color, and reflection to classify the washed
    rocks using computer vision.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 绝大多数这些冲刷过的岩石只是普通的石头。将其中一块岩石识别为钻石是罕见但非常重要的事件。在我们的场景中，一家矿山的所有者正在尝试使用计算机视觉来识别哪些冲刷过的岩石是普通的石头，哪些是钻石。他们正在利用形状、颜色和反射通过计算机视觉对冲刷过的岩石进行分类。
- en: 'In the context of this example:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子的背景下：
- en: '| **TP** | A washed rock correctly identified as a diamond |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **TP** | 一块正确识别为钻石的冲刷过的岩石 |'
- en: '| **TN** | A washed rock correctly identified as a stone |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **TN** | 一块正确识别为石头的冲刷过的岩石 |'
- en: '| **FP** | A stone incorrectly identified as a diamond |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **FP** | 一块错误地识别为钻石的石头 |'
- en: '| **FN** | A diamond incorrectly identified as a stone |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **FN** | 一颗错误地识别为石头的钻石 |'
- en: 'Let us explain recall and precision while keeping this diamond extraction process
    from the mine in mind:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在心中记住这个从矿山中提取钻石的过程，来解释召回率和精确度：
- en: '**Recall**: This calculates the *hit rate*, which is the proportion of identified
    events of interest in a gigantic repository of events. In other words, this metric
    rates our ability to find or “hit” most of the events of interest and leaves as
    little as possible unidentified. In the context of identifying diamonds in a pit
    of a large number of washed stones, recall is about quantifying the success of
    the treasure hunt. For a certain pit filled up with washed stones, recall will
    be the ratio of the number of diamonds identified to the total number of diamonds
    in the pit:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：这计算了*命中率*，即在大量事件库中识别出目标事件的比例。换句话说，这个指标衡量了我们找到或“命中”大部分目标事件的能力，并尽可能减少未被识别的事件。在识别一堆普通石头中的钻石的背景下，召回率是量化寻宝成功率的指标。对于某个装满了洗净石头的坑，召回率将是识别到的钻石数量与坑内钻石总数的比例：'
- en: '![](img/B18046_07_009.png)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B18046_07_009.png)'
- en: Let us assume there were 10 diamonds in the pit, each valued at $1,000\. Our
    machine learning algorithm was able to identify nine of them. So, the recall will
    be *9/10 = 0.90*.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设坑内有 10 颗钻石，每颗价值 $1,000。我们的机器学习算法能够识别出其中的九颗。那么，召回率就是 *9/10 = 0.90*。
- en: So, we are able to retrieve 90% of our treasure. In dollar cost, we were able
    to identify $9,000 of treasure out of a total value of $10,000.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，我们能够找回 90% 的宝藏。按美元计算，我们识别出了 $9,000 的宝藏，而总值为 $10,000。
- en: '**Precision**: In precision, we only focus on the data points flagged by the
    trained model as positive and discard everything else. If we filter only the events
    flagged as positive by our trained model (i.e., TPs and FPs) and then calculate
    the accuracy, this is called precision.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精准率**：精准率只关注由训练好的模型标记为正类的数据点，并丢弃其他所有数据。如果我们只筛选出模型标记为正类的事件（即 TPs 和 FPs），然后计算其准确性，这就是精准率。'
- en: Now, let us investigate precision in the context of the diamond mining example.
    Let us consider a scenario where we want to use computer vision to identify diamonds
    among a pit of washed rocks and send them to customers.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们在钻石开采的背景下探讨精准率。假设我们希望使用计算机视觉技术从一堆洗净的石头中识别出钻石，并将其发送给客户。
- en: The process is supposed to be automated. The worst-case scenario is the algorithm
    misclassifying a stone as a diamond, resulting in the end customer receiving it
    in the mail and getting charged for it. So, it should be obvious that for this
    process to be feasible, precision should be high.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个过程应该是自动化的。最坏的情况是算法错误地将一块石头分类为钻石，最终客户收到这块石头并因此被收费。因此，显然，为了使这个过程可行，精准率应该很高。
- en: 'For the diamond mining example:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 针对钻石开采的例子：
- en: '![](img/B18046_07_010.png)'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B18046_07_010.png)'
- en: Understanding the recall and precision trade-off
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解召回率和精准率之间的权衡
- en: 'Making decisions with a classifier involves a two-step process. Firstly, the
    classifier generates a decision score ranging from 0 to 1\. Then, it applies a
    decision threshold to determine the class for each data point. Data points scoring
    above the threshold are assigned a positive class, while those scoring below are
    assigned a negative class. The two steps can be explained as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分类器做出决策涉及一个两步过程。首先，分类器生成一个介于 0 到 1 之间的决策分数。然后，它应用一个决策阈值来确定每个数据点的类别。分数高于阈值的数据点被分配为正类，而分数低于阈值的数据点则被分配为负类。这两个步骤可以如下解释：
- en: The classifier generates a decision score, which is a number from 0 to 1.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类器生成一个决策分数，这是一个介于 0 到 1 之间的数字。
- en: The classifier uses the value of a parameter, called a decision threshold, to
    allocate one of the two classes to the current datapoint. Any decision (score
    > decision) threshold is predicted to be positive, and any data point having a
    decision (score < decision) threshold is predicted to be negative.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类器使用一个叫做决策阈值的参数值，将当前数据点分配到两个类别之一。任何决策（分数 > 决策阈值）的数据点被预测为正类，而任何决策（分数 < 决策阈值）的数据点被预测为负类。
- en: Envision a scenario where you operate a diamond mine. Your task is to identify
    precious diamonds in a heap of ordinary rocks. To facilitate this process, you’ve
    developed a machine learning classifier. The classifier reviews each rock, assigns
    it a decision score ranging from 0 to 1, and finally, classifies the rock based
    on this score and a predefined decision threshold.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个情境，你在经营一个钻石矿。你的任务是从一堆普通的石头中识别出珍贵的钻石。为了简化这个过程，你开发了一个机器学习分类器。该分类器检查每一块石头，分配一个介于
    0 到 1 之间的决策分数，最后根据这个分数和预设的决策阈值对石头进行分类。
- en: The decision score essentially represents the classifier’s confidence that a
    given rock is indeed a diamond, with rocks closer to 1 highly likely to be diamonds.
    The decision threshold, on the other hand, is a predefined cut-off point that
    decides the ultimate classification of a rock. Rocks scoring above the threshold
    are classified as diamonds (the positive class), while those scoring below are
    discarded as ordinary rocks (the negative class).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 决策分数本质上表示分类器对某块岩石确实是钻石的信心，分数接近1的岩石很可能是钻石。而决策阈值是一个预定义的截止点，决定岩石的最终分类。分数高于阈值的岩石被分类为钻石（正类），而低于阈值的岩石则被丢弃为普通岩石（负类）。
- en: Now, imagine all the rocks are arranged in ascending order of their decision
    scores, as shown in *Figure 7.4*. The rocks on the far left have the lowest scores
    and are least likely to be diamonds, while those on the far right have the highest
    scores, making them most likely to be diamonds. In an ideal scenario, every rock
    to the right of the decision threshold would be a diamond, and every rock to the
    left would be an ordinary stone.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设所有岩石按其决策分数升序排列，如*图7.4*所示。最左侧的岩石分数最低，最不可能是钻石，而最右侧的岩石分数最高，最可能是钻石。在理想情况下，决策阈值右侧的每块岩石都会是钻石，而左侧的每块岩石都是普通石头。
- en: Consider a situation, as depicted in *Figure 7.4*, where the decision threshold
    is at the center. On the right side of the decision boundary, we find three actual
    diamonds (TPs) and one ordinary rock wrongly flagged as a diamond (FPs). On the
    left, we have two ordinary rocks correctly identified (TNs) and two diamonds wrongly
    classified as ordinary rocks (FNs).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑如*图7.4*所示的情况，其中决策阈值处于中央。在决策边界的右侧，我们发现三个实际的钻石（TPs）和一个被错误标记为钻石的普通岩石（FPs）。在左侧，我们有两个正确识别的普通岩石（TNs）和两个被错误分类为普通岩石的钻石（FNs）。
- en: Thus, on the left-hand side of the decision threshold, you will find two correct
    classifications and two misclassifications. They are 2TNs and 2FNs.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在决策阈值的左侧，你会发现两个正确分类和两个误分类。它们分别是2个真正负类（TNs）和2个假负类（FNs）。
- en: 'Let us calculate the recall and precision for *Figure 7.4*:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算*图7.4*中的召回率和精确率：
- en: '![](img/B18046_07_011.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_011.png)'
- en: '![](img/B18046_07_012.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_012.png)'
- en: '![A comparison of diamonds and stone  Description automatically generated](img/B18046_07_04.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![钻石与石头的对比  描述自动生成](img/B18046_07_04.png)'
- en: 'Figure 7.4: Precision/recall trade-off: rocks are ranked by their classifier
    score'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：精确率/召回率权衡：岩石按分类器分数排序
- en: Those that are above the decision threshold are considered diamonds.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 处于决策阈值之上的被认为是钻石。
- en: Note that the higher the threshold, the higher the precision but the lower the
    recall.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，阈值越高，精确率越高，但召回率越低。
- en: 'Adjusting the decision threshold influences the trade-off between precision
    and recall. If we move the threshold to the right (as shown in *Figure 7.5*),
    we increase the criteria for a rock to be classified as a diamond, increasing
    precision but decreasing recall:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 调整决策阈值会影响精确率和召回率之间的权衡。如果我们将阈值向右移动（如*图7.5*所示），我们增加了将岩石分类为钻石的标准，从而提高了精确率，但降低了召回率：
- en: '![A screenshot of a computer screen  Description automatically generated](img/B18046_07_05.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成](img/B18046_07_05.png)'
- en: 'Figure 7.5: Precision/recall trade-off: rocks are ranked by their classifier
    score'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：精确率/召回率权衡：岩石按分类器分数排序
- en: Those that are above the decision threshold are considered diamonds. Note that
    the higher the threshold, the higher the precision but the lower the recall.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 处于决策阈值之上的被认为是钻石。请注意，阈值越高，精确率越高，但召回率越低。
- en: 'In *Figure 7.6*, we have decreased the decision threshold. In other words,
    we have decreased our criteria for a rock to be classified as a diamond. So, FNs
    (the treasure misses) will decrease, but FPs (the false signal) will increase
    as well. Thus, if we decrease the threshold (as shown in *Figure 7.6*), we loosen
    the criteria for diamond classification, increasing recall but decreasing precision:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.6*中，我们降低了决策阈值。换句话说，我们降低了将岩石分类为钻石的标准。因此，假负类（即漏掉的宝石）会减少，但假正类（即错误信号）会增加。因此，如果我们降低阈值（如*图7.6*所示），我们放宽了钻石分类的标准，增加了召回率，但降低了精确率：
- en: '![A screenshot of a computer screen  Description automatically generated](img/B18046_07_06.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成](img/B18046_07_06.png)'
- en: 'Figure 7.6: Precision/recall trade-off: rocks are ranked by their classifier
    score'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6：精确度/召回率权衡：岩石根据其分类器得分排序
- en: Those that are above the decision threshold are considered diamonds. Note that
    the higher the threshold, the higher the precision but the lower the recall.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 那些高于决策阈值的被视为钻石。请注意，阈值越高，精确度越高，但召回率越低。
- en: So, playing with the value of the decision boundary is about managing the trade-off
    between recall and precision. We increase the decision boundary to get better
    precision and can expect more recall, and we lower the decision boundary to get
    better recall and can expect less precision.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，调整决策边界的值就是在召回率和精确度之间权衡。我们增加决策边界以获得更好的精确度，同时可以预期更多的召回率；我们降低决策边界以获得更好的召回率，但可能会牺牲精确度。
- en: 'Let us draw a graph between precision and recall to better understand the trade-off:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制一张精确度与召回率的图表，以便更好地理解两者之间的权衡：
- en: '![Chart, line chart  Description automatically generated](img/B18046_07_07.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, line chart  Description automatically generated](img/B18046_07_07.png)'
- en: 'Figure 7.7: Precision vs. recall'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7：精确度与召回率的关系
- en: What is the right choice for recall and precision?
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率和精确度的最佳选择是什么？
- en: Increasing recall is done by decreasing the criteria we use to identify a data
    point as positive. The precision is expected to decrease, but as shown in the
    figure above, it falls sharply at around 0.8\. This is the point where we can
    choose the right value of recall and precision. In the above graph, if we choose
    0.8 as the recall, the precision is 0.75\. We can interpret it as being able to
    flag 80% of all the data points of interest. We flag these data points 75% accurately
    according to this level of precision. If there is no specific business requirement
    and it’s for a generic use case, this may be a reasonable compromise.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 增加召回率是通过降低我们用来识别数据点为正样本的标准来实现的。精确度预计会降低，但如上图所示，当精确度降到约 0.8 时，下降幅度非常明显。这是我们可以选择合适的召回率和精确度值的点。在上面的图表中，如果我们选择
    0.8 作为召回率，精确度为 0.75。我们可以将其解释为能够标记 80% 的所有目标数据点。在这个精确度水平下，我们对这些数据点的标记准确率为 75%。如果没有特定的业务需求，且只是用于通用场景，这可能是一个合理的折中方案。
- en: 'Another way to show the inherent trade-off between precision and recall is
    by using the **Receiving Operating Curve** (**ROC**). To do that, let us define
    two terms: **True Positive Rate** (**TPR**) and **False Positive Rate** (**FPR**).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 展示精确度与召回率之间固有权衡的另一种方法是使用 **接收操作特性曲线**（**ROC**）。为此，我们先定义两个术语：**真正例率**（**TPR**）和
    **假正例率**（**FPR**）。
- en: 'Let us look into the ROC curve. To calculate the TPR and FPR, we need to look
    at the diamonds in the pit:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 ROC 曲线。为了计算 TPR 和 FPR，我们需要观察坑中的钻石：
- en: '![](img/B18046_07_013.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_013.png)'
- en: '![](img/B18046_07_014.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_014.png)'
- en: '![](img/B18046_07_015.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_015.png)'
- en: 'Note that:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意：
- en: TPR is equal to the recall or hit rate.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TPR 等于召回率或命中率。
- en: TNR can be thought of as the recall or hit rate of the negative event. It determines
    our success in correctly identifying the negative event. It is also called **specificity**.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TNR 可以看作是负事件的召回率或命中率。它决定了我们正确识别负事件的成功率。它也叫做 **特异度**。
- en: FPR = 1 – TNR = 1 - Specificity.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FPR = 1 – TNR = 1 - 特异度。
- en: 'It should be obvious that TPR and FPR for these figures can be calculated as
    follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这些图中的 TPR 和 FPR 可以通过以下方式计算：
- en: '| **Figure number** | **TPR** | **FPR** |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| **图号** | **TPR** | **FPR** |'
- en: '| 7.4 | 3/5=0.6 | 1/3=0.33 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 7.4 | 3/5=0.6 | 1/3=0.33 |'
- en: '| 7.5 | 2/5=0.4 | 0/3 = 0 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 7.5 | 2/5=0.4 | 0/3 = 0 |'
- en: '| 7.6 | 5/5 = 1 | 1/3 = 0.33 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 7.6 | 5/5 = 1 | 1/3 = 0.33 |'
- en: Note that TPR or recall will be increased by lowering our decision threshold.
    In an effort to get as many diamonds as possible from the mine, we will lower
    our criterion for a washed stone being classified as a diamond. The result is
    that more stones will be incorrectly classified as diamonds, increasing FPR.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，降低我们的决策阈值会增加 TPR 或召回率。为了从矿山中尽可能多地提取钻石，我们会降低将石头分类为钻石的标准。结果是更多的石头会被错误地分类为钻石，从而增加
    FPR。
- en: 'Note that a good-quality classification algorithm should be able to provide
    the decision score for each of the rocks in the pit, which roughly matches the
    likelihood of a rock being a diamond. The output of such an algorithm is shown
    in *Figure 7.8*. Diamonds are supposed to be on the right side and stones are
    supposed to be on the left side. In the figure, as we have decreased the decision
    threshold from `0.8` to `0.2`, we expect to have a much higher increase in TRP
    and then FPR. In fact, the steep increase in TRP with a slight increase in FPR
    is one of the best indications of the quality of a binary classifier, as the classification
    algorithm was able to generate decision scores that directly relate with the likelihood
    of a rock being a diamond. If the diamonds and stones are randomly located on
    the decision score axis, it is equally likely that lowering the decision threshold
    will flag stones or diamonds. This would be the worst possible binary classifier,
    also called a randomizer:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一个高质量的分类算法应该能够为每一块岩石提供决策分数，这个分数大致对应岩石成为钻石的概率。该算法的输出如*图 7.8*所示。钻石应该位于右侧，石块应该位于左侧。在图中，随着我们将决策阈值从`0.8`降到`0.2`，我们预计TRP和FPR会有显著的提升。实际上，TRP的急剧增加与FPR的轻微增加是评估二分类器质量的最佳指标之一，因为该分类算法能够生成与岩石成为钻石的可能性直接相关的决策分数。如果钻石和石块在决策分数轴上随机分布，降低决策阈值会同样可能标记为石块或钻石。这将是最差的二分类器，也叫做随机化器：
- en: '![Chart, scatter chart  Description automatically generated](img/B18046_07_08.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, scatter chart  Description automatically generated](img/B18046_07_08.png)'
- en: 'Figure 7.8: ROC curve'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8：ROC 曲线
- en: Understanding overfitting
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解过拟合
- en: If a machine learning model performs great in a development environment but
    degrades noticeably in a production environment, we say that the model is overfitted.
    This means the trained model too closely follows the training dataset. It is an
    indication there are too many details in the rules created by the model. The trade-off
    between model variance and bias best captures the idea.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个机器学习模型在开发环境中表现优秀，但在生产环境中明显退化，我们称这个模型为过拟合。这意味着训练出的模型过于贴合训练数据集，模型创建的规则中包含了过多的细节。这表明模型的方差与偏差之间的权衡最能体现这一概念。
- en: When developing a machine learning model, we often make certain simplifying
    assumptions about the real-world phenomena that the model is supposed to capture.
    These assumptions are essential to make the modeling process manageable and less
    complex. However, the simplicity of these assumptions introduces a certain level
    of ‘bias’ in to our model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发机器学习模型时，我们通常会对模型需要捕捉的真实世界现象做出一些简化假设。这些假设对于使建模过程更可控、更简洁至关重要。然而，这些假设的简化会在我们的模型中引入一定程度的“偏差”。
- en: Let’s break this down further. Bias is a term that quantifies how much, on average,
    our predictions deviate from true values. In simple terms, if we have high bias,
    it means our model’s predictions are far off from the actual values, which leads
    to a high error rate on our training data.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步分析一下。偏差是一个量化我们预测值与真实值偏离程度的术语。简单来说，如果我们有较高的偏差，意味着我们的模型预测值与实际值相差较远，这会导致训练数据上较高的误差率。
- en: For instance, consider linear regression models. They assume a linear relationship
    between input features and output variables. However, this may not always be the
    case in real-world scenarios where relationships can be non-linear or more complex.
    This linear assumption, while simplifying our model, can lead to a high bias,
    as it may not fully capture the actual relationships between variables.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑线性回归模型。它假设输入特征与输出变量之间存在线性关系。然而，在现实世界中，关系可能是非线性的或更加复杂的，这并不总是成立。虽然这个线性假设简化了我们的模型，但它可能会导致较高的偏差，因为它可能无法完全捕捉变量之间的实际关系。
- en: Now, let’s also talk about ‘variance.’ Variance, in the context of machine learning,
    refers to the amount by which our model’s predictions would change if we used
    a different training dataset. A model with high variance pays a lot of attention
    to training data and tends to learn from the noise and the details. As a result,
    it performs very well on training data but not so well on unseen or test data.
    This difference in performance is often referred to as overfitting.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来谈谈“方差”。在机器学习中，方差是指如果我们使用不同的训练数据集，我们的模型预测会发生多大的变化。一个高方差的模型非常注重训练数据，容易从噪声和细节中学习。因此，它在训练数据上的表现非常好，但在未见过的数据或测试数据上的表现较差。这种表现差异通常被称为过拟合。
- en: We can visualize bias and variance using a bullseye diagram, as shown in *Figure
    7.9*. Note that the center of the target is a model that perfectly predicts the
    correct values. Shots that are far away from the bullseye indicate high bias,
    while shots that are dispersed widely indicate high variance. In a perfect scenario,
    we would like a low bias and low variance, where all shots hit right in the bullseye.
    However, in real-world scenarios, there is a trade-off. Lowering bias increases
    variance, and lowering variance increases bias.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过靶心图来可视化偏差和方差，如*图 7.9*所示。请注意，靶心的中心是一个完美预测正确值的模型。离靶心较远的射击表示高偏差，而分散较广的射击表示高方差。在理想的情况下，我们希望偏差低、方差低，所有射击都击中靶心。然而，在现实世界中，这是一种权衡。降低偏差会增加方差，而降低方差会增加偏差。
- en: 'This is known as the bias-variance trade-off and is a fundamental aspect of
    machine learning model design:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是偏差-方差权衡，它是机器学习模型设计中的一个基本概念：
- en: '![A diagram of different types of bias  Description automatically generated](img/B18046_07_09.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![不同类型偏差的示意图  自动生成的描述](img/B18046_07_09.png)'
- en: 'Figure 7.9: Graphical illustration of bias and variance'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9：偏差和方差的图示
- en: Balancing the right amount of generalization in a machine learning model is
    a delicate process. This balance, or sometimes imbalance, is described by the
    bias-variance trade-off. Generalization in machine learning refers to the model’s
    ability to adapt properly to new, unseen data, drawn from the same distribution
    as the one used for training. In other words, a well-generalized model can effectively
    apply learned rules from training data to new, unseen data. A more generalized
    model is achieved using simpler assumptions. These simpler assumptions result
    in more broad rules, which in turn make the model less sensitive to fluctuations
    in the training data. This means that the model will have low variance, as it
    doesn’t change much with different training sets.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型中，平衡适当的泛化程度是一个微妙的过程。这个平衡，有时也是不平衡的，正是通过偏差-方差权衡来描述的。机器学习中的泛化指的是模型在适应新数据（这些数据来源于与训练数据相同的分布）时的能力。换句话说，一个泛化良好的模型能够将从训练数据中学到的规则有效地应用于新的、未见过的数据。通过使用更简单的假设，可以实现更广泛的规则，从而使模型对训练数据的波动不那么敏感。这意味着模型的方差较低，因为它在不同训练集之间变化不大。
- en: However, there is a downside to this. Simpler assumptions mean the model might
    not fully capture all the complex relationships within the data. This results
    in a model that is consistently ‘off-target’ from the true output, leading to
    a higher bias.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这也有其负面影响。简单的假设意味着模型可能无法完全捕捉数据中的所有复杂关系。这会导致模型始终偏离真实输出，从而增加偏差。
- en: 'So, in this sense, more generalization equates to lower variance but higher
    bias. This is the essence of the bias-variance trade-off: a model with too much
    generalization (high bias) might oversimplify the problem and miss important patterns,
    while a model with too little generalization (high variance) might overfit to
    the training data, capturing noise along with the signal.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从这个角度看，更高的泛化意味着更低的方差，但更高的偏差。这就是偏差-方差权衡的本质：一个过度泛化的模型（高偏差）可能会过度简化问题，错过重要的模式，而一个泛化不足的模型（高方差）可能会对训练数据进行过拟合，同时捕捉噪声和信号。
- en: Striking a balance between these two extremes is one of the central challenges
    in machine learning, and the ability to manage this trade-off can often make the
    difference between a good model and a great one. This trade-off between bias and
    variance is determined by the choice of algorithm, the characteristics of the
    data, and various hyperparameters. It is important to achieve the right compromise
    between bias and variance, based on the requirements of the specific problem you
    try to solve.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种极端之间找到平衡是机器学习中的一个核心挑战，管理这种权衡的能力往往能够决定一个模型是好还是优秀。偏差与方差之间的这种权衡由算法的选择、数据的特征以及各种超参数决定。根据你试图解决的具体问题的需求，达到偏差与方差之间的适当妥协是非常重要的。
- en: Let us now look into how we can specify different phases of a classifier.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下如何指定分类器的不同阶段。
- en: Specifying the phases of classifiers
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指定分类器的阶段
- en: 'Once the labeled data is prepared, the development of the classifiers involves
    training, evaluation, and deployment. These three phases of implementing a classifier
    are shown in the **Cross-Industry Standard Process for Data Mining (CRISP-DM**)
    life cycle in the following diagram (the CRISP-DM life cycle was explained in
    more detail in *Chapter 5*, *Graph Algorithms*):'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦标记数据准备好，分类器的开发就包括训练、评估和部署。这三个实施分类器的阶段在下面的**跨行业数据挖掘标准流程（CRISP-DM）**生命周期图中有所展示（CRISP-DM生命周期在*第五章*，*图算法*中有更详细的解释）：
- en: '![Diagram  Description automatically generated](img/B18046_07_10.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18046_07_10.png)'
- en: 'Figure 7.10: CRISP DM life cycle'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.10: CRISP DM 生命周期'
- en: 'When implementing a classifier model, there are several crucial phases to consider,
    starting with a thorough understanding of the business problem at hand. This involves
    identifying the data needed to solve this problem and understanding the real-world
    context of the data. After gathering the relevant labeled data, the next step
    is to split this dataset into two sections: a training set and a testing set.
    The training set, typically larger, is used to train the model to understand patterns
    and relationships within the data. The testing set, on the other hand, is used
    to evaluate the model’s performance on unseen data.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施分类器模型时，需要考虑几个关键阶段，从彻底理解当前的业务问题开始。这包括识别解决此问题所需的数据，并理解数据的实际背景。收集相关的标记数据后，下一步是将数据集分成两部分：训练集和测试集。训练集通常较大，用于训练模型以理解数据中的模式和关系。另一方面，测试集用于评估模型在未见过的数据上的表现。
- en: To ensure both sets are representative of the overall data distribution, we
    will use a random sampling technique. This way, we can reasonably expect that
    patterns in the entire dataset will be reflected in both the training and testing
    partitions.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保这两个数据集能代表整体数据分布，我们将使用随机抽样技术。这样，我们可以合理地期望整个数据集中的模式在训练集和测试集两部分中都有体现。
- en: Note that, as shown in *Figure 7.10*, there is first a training phase, where
    training data is used to train a model. Once the training phase is over, the trained
    model is evaluated using the testing data. Different performance matrices are
    used to quantify the performance of the trained model. Once the model is evaluated,
    we have the model deployment phase, where the trained model is deployed and used
    for inference to solve real-world problems by labeling unlabeled data.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如*图 7.10*所示，首先是训练阶段，在该阶段使用训练数据来训练模型。训练阶段结束后，使用测试数据评估训练后的模型。通过不同的性能矩阵来量化训练模型的表现。一旦模型被评估，我们进入模型部署阶段，将训练好的模型部署并用于推断，通过标记未标记数据来解决实际问题。
- en: Now, let’s look at some classification algorithms.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一些分类算法。
- en: 'We will look at the following classification algorithms in the subsequent sections:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍以下分类算法：
- en: The decision tree algorithm
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树算法
- en: The XGBoost algorithm
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost算法
- en: The Random Forest algorithm
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林算法
- en: The logistic regression algorithm
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归算法
- en: The **SVM** algorithm
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SVM**算法'
- en: The Naive Bayes algorithm
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法
- en: Let’s start with the decision tree algorithm.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从决策树算法开始。
- en: Decision tree classification algorithm
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树分类算法
- en: A decision tree is based on a recursive partitioning approach (divide and conquer),
    which generates a set of rules that can be used to predict a label. It starts
    with a root node and splits it into multiple branches. Internal nodes represent
    a test on a certain attribute, and the result of the test is represented by a
    branch to the next level. The decision tree ends in leaf nodes, which contain
    the decisions. The process stops when partitioning no longer improves the outcome.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树基于递归划分方法（分治法），它生成一组规则用于预测标签。算法从根节点开始，并将其划分为多个分支。内部节点表示对某个属性的测试，测试结果通过分支表示到达下一层级。决策树在叶节点结束，叶节点包含决策。当划分不再改善结果时，过程停止。
- en: Let us now look into the details of the decision tree algorithm.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解决策树算法的细节。
- en: Understanding the decision tree classification algorithm
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解决策树分类算法
- en: The distinguishing feature of decision tree classification is the generation
    of a human-interpretable hierarchy of rules that is used to predict the label
    at runtime. This model’s transparency is a major advantage, as it allows us to
    understand the reasoning behind each prediction. This hierarchical structure is
    formed through a recursive algorithm, following a series of steps.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类的独特之处在于生成一组可由人类理解的规则层次结构，这些规则用于在运行时预测标签。该模型的透明性是其一个主要优势，因为它使我们能够理解每个预测背后的推理过程。这个层次结构是通过递归算法形成的，遵循一系列步骤。
- en: First, let’s illustrate this with a simplified example. Consider a decision
    tree model predicting whether a person will enjoy a specific movie. The topmost
    decision or ‘rule’ in the tree might be, ‘Is the movie a comedy or not?’ If the
    answer is yes, the tree branches to the next rule, like, ‘Does the movie star
    the person’s favorite actor?’ If no, it branches to another rule. Each decision
    point creates further subdivisions, forming a tree-like structure of rules, until
    we reach a final prediction.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过一个简化的例子来说明这一点。考虑一个预测一个人是否会喜欢某部特定电影的决策树模型。树顶端的决策或“规则”可能是，“这部电影是喜剧吗？”如果答案是“是”，树则分支到下一个规则，比如，“这部电影是否由这个人的最喜欢的演员主演？”如果答案是否，树则分支到另一个规则。每个决策点都会进一步细分，形成规则的树状结构，直到我们得到最终的预测。
- en: With this process, a decision tree guides us through a series of understandable,
    logical steps to arrive at a prediction. This clarity is what sets decision tree
    classifiers apart from other machine learning models.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一过程，决策树引导我们通过一系列易于理解的逻辑步骤，最终得出预测结果。这种清晰性使得决策树分类器区别于其他机器学习模型。
- en: 'The algorithm is recursive in nature. Creating this hierarchy of rules involves
    the following steps:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法本质上是递归的。创建这一规则层次结构涉及以下步骤：
- en: '**Find the most important feature**: Out of all of the features, the algorithm
    identifies the feature that best differentiates between the data points in the
    training dataset with respect to the label. The calculation is based on metrics
    such as information gain or Gini impurity.'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**找到最重要的特征**：在所有特征中，算法识别出最能区分训练数据集中的数据点与标签之间的特征。计算依据诸如信息增益或基尼不纯度等度量。'
- en: '**Bifurcate**: Using the most identified important feature, the algorithm creates
    a criterion that is used to divide the training dataset into two branches:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**二分**：使用最重要的特征，算法创建一个标准，用于将训练数据集分为两个分支：'
- en: Data points that pass the criterion
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符合标准的数据点
- en: Data points that fail the criterion
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不符合标准的数据点
- en: '**Check for leaf nodes**: If any resultant branch mostly contains labels of
    one class, the branch is made final, resulting in a leaf node.'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查叶节点**：如果任何结果分支中大部分标签属于同一类别，该分支将成为最终分支，从而形成叶节点。'
- en: '**Check the stopping conditions and repeat**: If the provided stopping conditions
    are not met, then the algorithm will go back to *step 1* for the next iteration.
    Otherwise, the model is marked as trained, and each node of the resultant decision
    tree at the lowest level is labeled as a leaf node. The stopping condition can
    be as simple as defining the number of iterations, or a default stopping condition
    can be used, where the algorithm stops as soon it reaches a certain homogeneity
    level for each of the leaf nodes.'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查停止条件并重复**：如果提供的停止条件未满足，算法将回到*步骤1*进行下一次迭代。否则，模型将标记为已训练，并且结果决策树中最低级别的每个节点将被标记为叶节点。停止条件可以简单地定义为迭代次数，或者使用默认的停止条件，当算法达到每个叶节点的同质性水平时便停止。'
- en: 'The decision tree algorithm can be explained by the following diagram:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法可以通过以下图示进行解释：
- en: '![Diagram  Description automatically generated](img/B18046_07_11.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图示 说明自动生成](img/B18046_07_11.png)'
- en: 'Figure 7.11: Decision Tree'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：决策树
- en: In the preceding diagram, the root contains a bunch of circles and crosses.
    They just represent two different categories of a particular feature. The algorithm
    creates a criterion that tries to separate the circles from the crosses. At each
    level, the decision tree creates partitions of the data, which are expected to
    be more and more homogeneous from level 1 upward. A perfect classifier has leaf
    nodes that only contain circles or crosses. Training perfect classifiers is usually
    difficult due to the inherent unpredictability and noise in real-world datasets.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图中，根节点包含了一些圆圈和叉号。它们仅代表某个特征的两类不同类别。该算法创建了一个标准，试图将圆圈与叉号分开。在每个层级，决策树对数据进行划分，并且这些划分期望在从第
    1 层开始越来越均匀。一个完美的分类器的叶节点只包含圆圈或叉号。由于现实世界数据集的内在不可预测性和噪声，训练完美的分类器通常是困难的。
- en: Note that the decision trees have key advantages that make them a preferred
    choice in many scenarios. The beauty of decision tree classifiers lies in their
    interpretability. Unlike many other models, they provide a clear and transparent
    set of ‘if-then’ rules, which makes the decision-making process understandable
    and auditable. This is particularly beneficial in fields like healthcare or finance,
    where comprehending the logic behind a prediction can be as important as the prediction
    itself.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，决策树具有一些关键优势，使其在许多场景中成为首选。决策树分类器的优点在于其可解释性。与许多其他模型不同，它们提供了一套清晰透明的“如果-那么”规则，使得决策过程易于理解和审计。这在医疗保健或金融等领域尤为有利，在这些领域，理解预测背后的逻辑与预测本身一样重要。
- en: Additionally, decision trees are less sensitive to the scale of the data and
    can handle a mix of categorical and numerical variables. This makes them a versatile
    tool in the face of diverse data types.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，决策树对数据的规模不太敏感，并且能够处理分类和数值变量的混合。这使得它们在面对多样化数据类型时成为一个多功能的工具。
- en: So, even though training a ‘perfect’ decision tree classifier might be difficult,
    the advantages they offer, including their simplicity, transparency, and flexibility,
    often outweigh this challenge.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管训练一个“完美”的决策树分类器可能是困难的，但它们所提供的优势，包括简单性、透明性和灵活性，通常足以克服这一挑战。
- en: We are going to use the decision tree classification algorithm for the classifiers
    challenge.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用决策树分类算法来解决分类器挑战问题。
- en: 'Now, let’s use the decision tree classification algorithm for the common problem
    that we previously defined to predict whether a customer ends up purchasing a
    product:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用决策树分类算法解决我们之前定义的常见问题，预测客户是否最终购买产品：
- en: 'First, let’s instantiate the decision tree classification algorithm and train
    a model using the training portion of the data that we prepared for our classifiers:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们实例化决策树分类算法，并使用我们为分类器准备的数据的训练部分来训练一个模型：
- en: '[PRE14]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let’s use our trained model to predict the labels for the testing portion
    of our labeled data. Let’s generate a confusion matrix that can summarize the
    performance of our trained model:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用我们训练好的模型来预测标注数据的测试部分的标签。让我们生成一个混淆矩阵，以总结我们训练模型的性能：
- en: '[PRE16]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This gives the following output:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE17]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let’s calculate the `accuracy`, `recall`, and `precision` values for the
    created classifier by using the decision tree classification algorithm:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用决策树分类算法计算所创建分类器的`准确率`、`召回率`和`精确度`值：
- en: '[PRE19]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Running the preceding code will produce the following output:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行上述代码将产生以下输出：
- en: '[PRE20]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The performance measures help us compare different training modeling techniques
    with each other.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 性能衡量标准帮助我们将不同的训练建模技术相互比较。
- en: Let us now look into the strengths and weaknesses of decision tree classifiers.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来探讨决策树分类器的优缺点。
- en: The strengths and weaknesses of decision tree classifiers
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树分类器的优缺点
- en: In this section, let’s look at the strengths and weaknesses of using the decision
    tree classification algorithm.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论使用决策树分类算法的优缺点。
- en: One of the most significant strengths of decision tree classifiers lies in their
    inherent transparency. The rules that govern their model formation are human-readable
    and interpretable, making them ideal for situations that demand a clear understanding
    of the decision-making process. This type of model, often referred to as a white-box
    model, is an essential component in scenarios where bias needs to be minimized
    and transparency maximized. This is particularly relevant in critical industries
    such as government and insurance, where accountability and traceability are paramount.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器最显著的优势之一在于其固有的透明性。支配其模型构建的规则是可读且可解释的，这使得它们非常适合那些需要清晰理解决策过程的情境。这种类型的模型通常被称为白盒模型，是需要最小化偏差并最大化透明度的场景中不可或缺的组成部分。这在政府和保险等关键行业中尤其重要，因为这些行业对问责制和可追溯性有着至关重要的要求。
- en: In addition, decision tree classifiers are well equipped to handle categorical
    variables. Their design is inherently suited to extracting information from discrete
    problem spaces, which makes them an excellent choice for datasets where most features
    fall into specific categories.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，决策树分类器能够很好地处理分类变量。它们的设计天生适合从离散问题空间中提取信息，这使得它们在大多数特征属于特定类别的数据集中特别合适。
- en: On the flip side, decision tree classifiers do exhibit certain limitations.
    Their biggest challenge is the tendency toward overfitting. When a decision tree
    delves too deep, it runs the risk of creating rules that capture an excessive
    amount of detail. This leads to models that overgeneralize from the training data
    and perform poorly on unseen data. Therefore, it’s critical to implement strategies
    such as pruning to prevent overfitting when using decision tree classifiers.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，决策树分类器也确实存在一些局限性。它们最大的挑战是容易过拟合。当决策树深入挖掘时，可能会创建捕捉过多细节的规则，这会导致模型从训练数据中过度泛化，进而在未见过的数据上表现不佳。因此，使用决策树分类器时，实施剪枝等策略以防止过拟合是至关重要的。
- en: Another limitation of decision tree classifiers is their struggle with non-linear
    relationships. Their rules are predominantly linear, and as such, they may not
    capture the nuances of relationships that aren’t straight-line in nature. Therefore,
    while decision trees bring some impressive strengths to the table, their weaknesses
    warrant careful consideration when choosing the appropriate model for your data.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器的另一个局限性是它们在处理非线性关系时的困难。它们的规则主要是线性的，因此可能无法捕捉到非直线关系的细微差别。因此，虽然决策树为解决问题带来了一些显著的优势，但它们的弱点在选择适当的模型时需要特别谨慎。
- en: Use cases
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例
- en: 'Decision trees classifiers can be used in the following use cases to classify
    data:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器可以用于以下用例来分类数据：
- en: '**Mortgage applications**: To train a binary classifier to determine whether
    an applicant is likely to default.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抵押贷款申请**：训练一个二分类器来判断申请人是否可能违约。'
- en: '**Customer segmentation**: To categorize customers into high-worth, medium-worth,
    and low-worth customers so that marketing strategies can be customized for each
    category.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户细分**：将客户分类为高价值客户、中等价值客户和低价值客户，以便根据每个类别定制营销策略。'
- en: '**Medical diagnosis**: To train a classifier that can categorize a benign or
    malignant growth.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医学诊断**：训练一个分类器来区分良性或恶性肿瘤。'
- en: '**Treatment-effectiveness analysis**: To train a classifier that can flag patients
    who have reacted positively to a particular treatment.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**治疗效果分析**：训练一个分类器来标记对某种治疗反应积极的患者。'
- en: '**Using a decision tree for feature selection**: Another aspect worth discussing
    when examining decision tree classifiers is their feature selection capability.
    In the process of rule creation, decision trees tend to choose a subset of features
    from your dataset. This inherent trait of decision trees can be beneficial, especially
    when dealing with datasets that have a large number of features.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用决策树进行特征选择**：在考察决策树分类器时，另一个值得讨论的方面是它们的特征选择能力。在规则创建过程中，决策树倾向于从数据集中选择一部分特征。决策树这一固有特性在处理特征数目庞大的数据集时尤为有益。'
- en: Why is this feature selection important, you might ask? In machine learning,
    dealing with numerous features can be a challenge. An excess of features can lead
    to models that are complex, harder to interpret, and may even result in worse
    performance due to the ‘curse of dimensionality.’
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，为什么特征选择如此重要？在机器学习中，处理大量特征可能是一个挑战。特征过多可能导致模型复杂，难以解释，甚至由于"维度诅咒"导致性能下降。
- en: By automatically selecting a subset of the most important features, decision
    trees can simplify a model and focus on the most relevant predictors.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自动选择最重要的特征子集，决策树可以简化模型，并专注于最相关的预测变量。
- en: Notably, the feature selection process within decision trees isn’t confined
    to their own model development. The outcomes of this process can also serve as
    a form of preliminary feature selection for other machine learning models. This
    can provide an initial understanding of which features are most important and
    help streamline the development of other machine learning models.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，决策树中的特征选择过程并不限于它们自身模型的开发。这个过程的结果也可以作为其他机器学习模型的初步特征选择。这可以提供哪些特征最重要的初步了解，并帮助简化其他机器学习模型的开发。
- en: Next, let us look into the ensemble methods.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入探讨集成方法。
- en: Understanding the ensemble methods
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解集成方法
- en: In the domain of machine learning, an ensemble refers to a technique where multiple
    models, each with slight variations, are created and combined to form a composite
    or aggregate model. The variations could arise from using different model parameters,
    subsets of the data, or even different machine learning algorithms.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，集成是一种技术，其中多个模型，每个模型都有细微的差异，被创建并结合成一个复合或聚合模型。这些差异可以来自使用不同的模型参数、数据子集，甚至是不同的机器学习算法。
- en: However, what does “slightly different” mean in this context? Here, each individual
    model in the ensemble is created to be unique, but not radically different. This
    can be achieved by tweaking the hyperparameters, training each model on a different
    subset of training data, or using diverse algorithms. The aim is to have each
    model capture different aspects or nuances of the data, which can help enhance
    the overall predictive power when they’re combined.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，"稍微不同"在这个上下文中是什么意思？在这里，集成中的每个单独模型都是为了独特性而创建的，但不会有根本性的差异。这可以通过调整超参数、在不同的训练数据子集上训练每个模型，或者使用不同的算法来实现。目标是让每个模型捕捉数据的不同方面或细微差别，当它们结合在一起时，可以帮助提升整体的预测能力。
- en: So, how are these models combined? The ensemble technique involves a process
    of decision-making known as aggregation, where the predictions from individual
    models are consolidated. This could be a simple average, a majority vote, or a
    more complex approach, depending on the specific ensemble technique used.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些模型是如何结合的呢？集成技术涉及一种称为聚合的决策过程，其中个别模型的预测被合并。这可能是一个简单的平均值、一个多数投票，或者是根据所使用的具体集成技术采用更复杂的方法。
- en: As for when and why ensemble methods are needed, they can be particularly useful
    when a single model isn’t sufficient to achieve a high level of accuracy. By combining
    multiple models, the ensemble can capture more complexity and often achieve better
    performance. This is because the ensemble can average out biases, reduce variance,
    and is less likely to overfit to the training data.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 至于何时以及为何需要集成方法，当单个模型无法达到较高的准确率时，它们尤其有用。通过组合多个模型，集成可以捕捉更多的复杂性，并通常能获得更好的性能。这是因为集成可以平均化偏差，减少方差，并且不容易过拟合训练数据。
- en: Finally, assessing the effectiveness of an ensemble is similar to evaluating
    a single model. Metrics such as accuracy, precision, recall, or F1-score can be
    used, depending on the nature of the problem. The key difference is that these
    metrics are applied to the aggregated predictions of the ensemble rather than
    the predictions of a single model.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，评估一个集成模型的有效性类似于评估单个模型。可以根据问题的性质使用如准确率、精确度、召回率或F1分数等指标。关键的不同在于，这些指标是应用于集成的聚合预测，而不是单一模型的预测。
- en: Let’s look at some ensemble algorithms, starting with XGBoost.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看一些集成算法，从 XGBoost 开始。
- en: Implementing gradient boosting with the XGBoost algorithm
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 XGBoost 算法实现梯度提升
- en: XGBoost, introduced in 2014, is an ensemble classification algorithm that’s
    gained widespread popularity, primarily due to its foundation on the principles
    of gradient boosting. But what does gradient boosting entail? Essentially, it’s
    a machine learning technique that involves building many models sequentially,
    with each new model attempting to correct the errors made by the previous ones.
    This progression continues until a significant reduction in error rate is achieved,
    or a pre-defined number of models has been added.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost于2014年推出，是一种集成分类算法，因其基于梯度提升原理而广受欢迎。那么，梯度提升到底是什么呢？本质上，它是一种机器学习技术，涉及顺序构建多个模型，每个新模型都试图纠正前一个模型的错误。这个过程会一直进行，直到误差率显著降低，或添加了预定义数量的模型为止。
- en: In the context of XGBoost, it employs a collection of interrelated decision
    trees and optimizes their predictions using gradient descent, a popular optimization
    algorithm that aims to find the minimum of a function – in this case, the residual
    error. In simpler terms, gradient descent iteratively adjusts the model to minimize
    the difference between its predictions and the actual values.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在XGBoost的背景下，它采用一组相互关联的决策树，并通过梯度下降优化它们的预测结果，梯度下降是一种常用的优化算法，旨在找到一个函数的最小值——在这种情况下是残差误差。简单来说，梯度下降通过不断调整模型来最小化预测值与实际值之间的差异。
- en: The design of XGBoost makes it well suited for distributed computing environments.
    This compatibility extends to Apache Spark – a platform for large-scale data processing,
    and cloud computing platforms like Google Cloud and **Amazon Web Services** (**AWS**).
    These platforms provide the computational resources needed to efficiently run
    XGBoost, especially on larger datasets.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost的设计使其非常适合分布式计算环境。这种兼容性扩展到Apache Spark——一个大规模数据处理平台，以及像Google Cloud和**Amazon
    Web Services**（**AWS**）这样的云计算平台。这些平台提供了高效运行XGBoost所需的计算资源，特别是在处理更大数据集时。
- en: 'Now, we will walk through the process of implementing gradient boosting using
    the XGBoost algorithm. Our journey includes preparing the data, training the model,
    generating predictions, and evaluating the model’s performance. Firstly, data
    preparation is key to properly utilizing the XGBoost algorithm. Raw data often
    contains inconsistencies, missing values, or variable types that might not be
    suitable for the algorithm. Therefore, it’s imperative to preprocess and clean
    the data, normalizing numerical fields and encoding categorical ones as needed.
    Once our data is appropriately formatted, we can proceed with model training.
    An instance of the XGBClassifier has been created, which we’ll use to fit our
    model. Let us look at the steps:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将走过使用XGBoost算法实现梯度提升的过程。我们的旅程包括数据准备、模型训练、生成预测和评估模型性能。首先，数据准备是正确使用XGBoost算法的关键。原始数据通常包含不一致性、缺失值或可能不适合算法的变量类型。因此，必须对数据进行预处理和清理，按需规范化数值字段并编码分类字段。一旦数据格式化正确，我们可以继续进行模型训练。已经创建了一个XGBClassifier实例，我们将用它来拟合我们的模型。让我们看看步骤：
- en: 'This process is trained using the `X_train` and `y_train` data subsets, representing
    our features and labels respectively:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程使用`X_train`和`y_train`数据子集进行训练，分别代表我们的特征和标签：
- en: '[PRE21]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we will generate predictions based on the newly trained model:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将基于新训练的模型生成预测结果：
- en: '[PRE23]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The produces the following output:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程产生如下输出：
- en: '[PRE24]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we will quantify the performance of the model:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将量化模型的性能：
- en: '[PRE26]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This gives us the following output:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE27]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now, let’s look at the Random Forest algorithm.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看随机森林算法。
- en: The Random Forest algorithm is an ensemble learning method that achieves its
    effectiveness by combining the outputs of numerous decision trees, thereby reducing
    both bias and variance. Here, let’s dive deeper into how it’s trained and how
    it generates predictions. In training, the Random Forest algorithm leverages a
    technique known as bagging, or bootstrap-aggregating. It generates `N` subsets
    from the training dataset, each created by randomly selecting some rows and columns
    from the input data. This selection process introduces randomness into the model,
    hence the name ‘Random Forest.’ Each subset of data is used to train an independent
    decision tree, resulting in a collection of trees denoted as C[1] through C[m].
    These trees can be of any type, but typically, they’re binary trees where each
    node splits the data based on a single feature.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法是一种集成学习方法，通过结合多个决策树的输出，减少偏差和方差，从而实现其效果。在这里，我们深入探讨它是如何训练的，以及它是如何生成预测的。在训练过程中，随机森林算法利用一种称为bagging（或自助聚合）的技术。它从训练数据集中生成`N`个子集，每个子集通过从输入数据中随机选择一些行和列来创建。这个选择过程引入了随机性，因此被称为“随机森林”。每个数据子集用于训练一棵独立的决策树，最终得到一组树，表示为C[1]到C[m]。这些树可以是任何类型，但通常是二叉树，每个节点根据一个特征来划分数据。
- en: In terms of predictions, the Random Forest model employs a democratic voting
    system. When a new instance of data is fed into the model for prediction, each
    decision tree in the forest generates its own label. The final prediction is determined
    by majority voting, meaning the label that received the most votes from all the
    trees becomes the overall prediction.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测方面，随机森林模型采用民主投票系统。当一个新的数据实例被输入模型进行预测时，森林中的每棵决策树都会生成自己的标签。最终预测由多数投票决定，意味着得到最多票数的标签将成为整体预测。
- en: 'It is shown in *Figure 7.12*:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图7.12*所示：
- en: '![Diagram  Description automatically generated](img/B18046_07_12.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18046_07_12.png)'
- en: 'Figure 7.12: Random Forest'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12：随机森林
- en: Note that in *Figure 7.12*, *m* trees are trained, which is represented by *C*[1]
    to *C*[m]—that is, *Trees = {C*[1]*,..,C*[m]*}*.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在*图7.12*中，训练了*m*棵树，表示为*C*[1]到*C*[m]，即*树 = {C*[1]*,..,C*[m]*}。
- en: 'Each of the trees generates a prediction, which is represented by a set:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 每棵树都会生成一个预测，表示为一个集合：
- en: '*Individual predictions = P= {P*[1]*,..., P*[m]*}*'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '*个体预测 = P= {P*[1]*,..., P*[m]*}*'
- en: 'The final prediction is represented by `Pf`. It is determined by the majority
    of the individual predictions. The `mode` function can be used to find the majority
    decision (`mode` is the number that repeats most often and is in the majority).
    The individual prediction and the final prediction are linked, as follows:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 最终预测由`Pf`表示。它由大多数个体预测决定。可以使用`mode`函数找到多数决策（`mode`是出现次数最多且占多数的数字）。个体预测与最终预测是相关的，如下所示：
- en: '[PRE28]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This ensemble technique offers several benefits. Firstly, the randomness introduced
    into both data selection and decision tree construction reduces the risk of overfitting,
    increasing model robustness. Secondly, each tree in the forest operates independently,
    making Random Forest models highly parallelizable and, hence, suitable for large
    datasets. Lastly, Random Forest models are versatile, capable of handling both
    regression and classification tasks, and dealing effectively with missing or outlier
    data.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这种集成技术提供了几个优势。首先，数据选择和决策树构建中引入的随机性降低了过拟合的风险，从而提高了模型的鲁棒性。其次，森林中的每棵树都独立运行，使得随机森林模型高度并行化，因此适用于大数据集。最后，随机森林模型具有多功能性，能够处理回归和分类任务，并有效应对缺失数据或异常数据。
- en: However, keep in mind that the effectiveness of a Random Forest model heavily
    depends on the number of trees it contains. Having too few might lead to a weak
    model, while too many could result in unnecessary computation. It’s important
    to fine-tune this parameter based on the specific needs of your application.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请记住，随机森林模型的有效性在很大程度上取决于它包含的树木数量。树木太少可能导致模型较弱，而树木太多则可能导致不必要的计算。根据应用的具体需求，调整这个参数非常重要。
- en: Differentiating the Random Forest algorithm from ensemble boosting
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分随机森林算法与集成增强方法
- en: Random Forest and ensemble boosting represent two distinct approaches to ensemble
    learning, a powerful method in machine learning that combines multiple models
    to create more robust and accurate predictions.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林和集成增强法代表了集成学习的两种不同方法，集成学习是机器学习中的一种强大方法，它通过结合多个模型来创建更稳健和准确的预测。
- en: In the Random Forest algorithm, each decision tree operates independently, uninfluenced
    by the performance or structure of the other trees in the forest. Each tree is
    built from a different subset of the data and uses a different subset of features
    for its decisions, adding to the overall diversity of the ensemble. The final
    output is determined by aggregating the predictions from all the trees, typically
    through a majority vote.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林算法中，每棵决策树独立运行，不受其他树的性能或结构的影响。每棵树都是从数据的不同子集构建的，并且在做出决策时使用不同的特征子集，这增加了集成的整体多样性。最终输出是通过对所有树的预测进行聚合来确定的，通常通过多数投票。
- en: Ensemble boosting, on the other hand, employs a sequential process where each
    model is aware of the mistakes made by its predecessors. Boosting techniques generate
    a sequence of models where each successive model aims to correct the errors of
    the previous one. This is achieved by assigning higher weights to the misclassified
    instances in the training set for the next model in the sequence. The final prediction
    is a weighted sum of the predictions made by all models in the ensemble, effectively
    giving more influence to more accurate models.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 集成增强法（ensemble boosting）采用一种顺序过程，每个模型都意识到其前一个模型的错误。增强技术生成一系列模型，其中每个后续模型旨在纠正前一个模型的错误。通过为下一模型分配更高的权重给训练集中的错误分类实例来实现这一点。最终的预测是所有模型预测的加权总和，从而有效地赋予更准确的模型更多的影响力。
- en: In essence, while Random Forest leverages the power of independence and diversity,
    ensemble boosting focuses on correcting mistakes and improving from past errors.
    Each approach has its own strengths and can be more effective, depending on the
    nature and structure of the data being modeled.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，虽然随机森林利用独立性和多样性的优势，集成增强法则侧重于纠正错误并从过去的错误中改进。每种方法都有其自身的优势，根据数据的性质和结构的不同，它们可能更有效。
- en: Using the Random Forest algorithm for the classifiers challenge
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机森林算法进行分类挑战
- en: Let’s instantiate the Random Forest algorithm and use it to train our model
    using the training data.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实例化随机森林算法，并使用训练数据来训练我们的模型。
- en: 'There are two key hyperparameters that we’ll look at here:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个关键的超参数，我们将重点关注：
- en: '`n_estimators`'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`'
- en: '`max_depth`'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`'
- en: The `n_estimators` hyperparameter determines the number of individual decision
    trees that are constructed within the ensemble. Essentially, it dictates the size
    of the ‘forest.’ A larger number of trees generally leads to more robust predictions,
    as it increases the diversity of decision paths and a model’s ability to generalize.
    However, it’s important to note that adding more trees also increases the computational
    complexity, and beyond a certain point, the improvements in accuracy may become
    negligible.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators` 超参数决定了集成中构建的单个决策树的数量。本质上，它决定了“森林”的大小。更多的树通常会导致更稳健的预测，因为它增加了决策路径的多样性和模型的泛化能力。然而，重要的是要注意，增加更多的树也会增加计算复杂性，超过某个点后，准确性的提升可能变得微不足道。'
- en: On the other hand, the `max_depth` hyperparameter specifies the maximum depth
    that each individual tree can reach. In the context of a decision tree, ‘depth’
    refers to the longest path from the root node (the starting point at the top of
    the tree) to a leaf node (the final decision outputs at the bottom). By limiting
    the maximum depth, we essentially control the complexity of the learned structures,
    balancing the trade-off between underfitting and overfitting. A tree that’s too
    shallow may miss important decision rules, while a tree that’s too deep may overfit
    to the training data, capturing noise and outliers.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`max_depth` 超参数指定了每棵树可以达到的最大深度。在决策树的上下文中，“深度”是指从根节点（树顶的起始点）到叶节点（底部的最终决策输出）之间的最长路径。通过限制最大深度，我们实际上控制了学习结构的复杂性，在欠拟合和过拟合之间取得平衡。过于浅的树可能会错过重要的决策规则，而过于深的树可能会对训练数据进行过拟合，捕捉到噪声和异常值。
- en: Fine-tuning these two hyperparameters plays a vital role in optimizing the performance
    of your decision-tree-based models, striking the right balance between predictive
    power and computational efficiency.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 微调这两个超参数在优化基于决策树的模型性能方面起着至关重要的作用，能够在预测能力和计算效率之间取得正确的平衡。
- en: 'To train a classifier using the Random Forest algorithm, we will do the following:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用随机森林算法训练分类器，我们将执行以下步骤：
- en: '[PRE29]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once the Random Forest model is trained, let’s use it for predictions:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦随机森林模型训练完成，我们就可以用它来进行预测：
- en: '[PRE31]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Which gives the output as:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出如下输出：
- en: '[PRE32]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let’s quantify how good our model is:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们量化模型的好坏：
- en: '[PRE33]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will observe the following output:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将观察到以下输出：
- en: '[PRE34]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that Random Forest is a popular and versatile machine learning method that
    can be used for both classification and regression tasks. It is renowned for its
    simplicity, robustness, and flexibility, making it applicable across a broad range
    of contexts.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，随机森林是一种流行且多用途的机器学习方法，既可以用于分类任务，也可以用于回归任务。它因其简单性、鲁棒性和灵活性而著称，使其能够应用于各种不同的场景。
- en: Next, let’s look into logistic regression.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下逻辑回归。
- en: Logistic regression
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is a classification algorithm used for binary classification.
    It uses a logistic function to formulate the interaction between the input features
    and the label. It is one of the simplest classification techniques that is used
    to model a binary dependent variable.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种用于二分类的分类算法。它使用逻辑函数来表述输入特征与标签之间的关系。它是最简单的分类技术之一，用于建模二元因变量。
- en: Assumptions
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 假设条件
- en: 'Logistic regression assumes the following:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归假设如下：
- en: The training dataset does not have a missing value.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集没有缺失值。
- en: The label is a binary category variable.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签是二元分类变量。
- en: The label is ordinal—in other words, a categorical variable with ordered values.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签是有序的——换句话说，是具有顺序值的分类变量。
- en: All features or input variables are independent of each other.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有特征或输入变量相互独立。
- en: Establishing the relationship
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立关系
- en: 'For logistic regression, the predicted value is calculated as follows:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逻辑回归，预测值的计算方法如下：
- en: '*![](img/B18046_07_035.png)*'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/B18046_07_035.png)*'
- en: 'Let’s suppose that:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 假设：
- en: '*![](img/B18046_07_036.png)*'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/B18046_07_036.png)*'
- en: 'So now:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在：
- en: '![](img/B18046_07_016.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_016.png)'
- en: 'The preceding relationship can be graphically shown as follows:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 前述关系可以图形化表示如下：
- en: '![Chart, line chart  Description automatically generated](img/B18046_07_13.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 描述自动生成](img/B18046_07_13.png)'
- en: 'Figure 7.13: Plotting the sigmoid function'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：绘制 sigmoid 函数
- en: Note that if *z* is large, ![](img/B18046_07_017.png) (*z*) will equal `1`.
    If *z* is very small or a large negative number, ![](img/B18046_07_017.png) (*z*)
    will equal `0`. Also, when *z* is 0, then ![](img/B18046_07_017.png) (*z*)=0.5\.
    Sigmoid is a natural function to use to represent probabilities, as it is strictly
    bounded between 0 and 1\. By ‘natural,’ we mean it is well suited or particularly
    effective due to its inherent properties. In this case, the sigmoid function always
    outputs a value between 0 and 1, which aligns with the probability range. This
    makes it a great tool for modeling probabilities in logistic regression. The objective
    of training a logistic regression model is to find the correct values for *w*
    and *j*.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果 *z* 很大，![](img/B18046_07_017.png) (*z*) 将等于 `1`。如果 *z* 很小或是一个很大的负数，![](img/B18046_07_017.png)
    (*z*) 将等于 `0`。另外，当 *z* 为 0 时，![](img/B18046_07_017.png) (*z*) = 0.5。sigmoid 是一个自然函数，适用于表示概率，因为它的值严格限定在
    0 到 1 之间。所谓“自然”是指它由于固有的性质而特别适用或有效。在这种情况下，sigmoid 函数始终输出一个介于 0 和 1 之间的值，这与概率范围一致。这使得它成为逻辑回归中建模概率的一个优秀工具。训练逻辑回归模型的目标是找到
    *w* 和 *j* 的正确值。
- en: Logistic regression is named after the function that is used to formulate it,
    called the **logistic** or **sigmoid function**.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归得名于用于表述它的函数，称为**逻辑函数**或**sigmoid 函数**。
- en: The loss and cost functions
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数和成本函数
- en: The `loss` function defines how we want to quantify an error for a particular
    example in our training data. The `cost` function defines how we want to minimize
    an error in our entire training dataset. So, the `loss` function is used for one
    of the examples in the training dataset and the `cost` function is used for the
    overall cost that quantifies the overall deviation of the actual and predicted
    values. It is dependent on the choice of *w* and *h*.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '`损失`函数定义了我们如何为训练数据中的某个特定示例量化误差。`成本`函数定义了我们如何在整个训练数据集上最小化误差。因此，`损失`函数用于训练数据集中的单个示例，而`成本`函数用于量化实际值和预测值之间的总体偏差。它依赖于*w*和*h*的选择。'
- en: 'The `loss` function used in logistic regression for a certain example *i* in
    the training set is as follows:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，用于训练集中特定示例*i*的`损失`函数如下：
- en: '*Loss (ý*^((i))*, y*^((i))*) = - (y*^((i)) *log ý*^((i)) *+ (1-y*^((i)) *)
    log (1-ý*^((i))*)*'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失（ý*^((i))*, y*^((i))*) = - (y*^((i)) *log ý*^((i)) *+ (1-y*^((i)) *) log
    (1-ý*^((i))*)*'
- en: Note that when *y*^((i)) *= 1, Loss(ý*^((i))*, y*^((i))*) = - logý*^((i)). Minimizing
    the loss will result in a large value of ý^((i)). Being a sigmoid function, the
    maximum value will be *1*.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当*y*^((i)) *= 1时，损失（ý*^((i))*, y*^((i))*) = - logý*^((i))。最小化损失将导致ý^((i))的值变大。作为一个sigmoid函数，它的最大值为*1*。
- en: If *y*^((i)) *= 0, Loss (ý*^((i))*, y*^((i))*) = - log (1-ý*^((i))*)*.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*y*^((i)) *= 0，损失（ý*^((i))*, y*^((i))*) = - log (1-ý*^((i))*)*。
- en: Minimizing the loss will result in *ý*^((i)) being as small as possible, which
    is *0*.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化损失将导致*ý*^((i))尽可能小，这时其值为*0*。
- en: 'The cost function of logistic regression is as follows:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的成本函数如下：
- en: '![](img/B18046_07_020.png)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_020.png)'
- en: Let us now look into the details of logistic regression.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解逻辑回归的细节。
- en: When to use logistic regression
  id: totrans-454
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用逻辑回归
- en: Logistic regression works great for binary classifiers. To clarify, binary classification
    refers to the process of predicting one of two possible outcomes. For example,
    if we try to predict whether an email is spam or not, this is a binary classification
    problem because there are only two possible results – ‘spam’ or ‘not spam.’
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归在二分类器中表现很好。为了澄清，二分类是指预测两种可能结果之一的过程。例如，如果我们尝试预测一封邮件是否为垃圾邮件，这就是一个二分类问题，因为只有两种可能的结果——‘垃圾邮件’或‘非垃圾邮件’。
- en: However, there are certain limitations to logistic regression. Particularly,
    it may struggle when dealing with large datasets of subpar quality. For instance,
    consider a dataset filled with numerous missing values, outliers, or irrelevant
    features. The logistic regression model might find it difficult to produce accurate
    predictions under these circumstances.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，逻辑回归存在一些局限性。特别是，当处理质量较差的大型数据集时，它可能会遇到困难。例如，假设有一个数据集，其中包含大量缺失值、异常值或无关特征。在这种情况下，逻辑回归模型可能会发现很难做出准确的预测。
- en: Further, while logistic regression can handle linear relationships between features
    and the target variable effectively, it can fall short when dealing with complex,
    non-linear relationships. Picture a dataset where the relationship between the
    predictor variables and the target is not a straight line but a curve; a logistic
    regression model might struggle in such scenarios.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然逻辑回归能够有效地处理特征与目标变量之间的线性关系，但在处理复杂的非线性关系时可能会显得力不从心。想象一个数据集，其中预测变量与目标之间的关系不是一条直线，而是曲线；在这种情况下，逻辑回归模型可能会遇到困难。
- en: Despite these limitations, logistic regression can often serve as a solid starting
    point for classification tasks. It provides a benchmark performance that can be
    used to compare the effectiveness of more complex models. Even if it doesn’t deliver
    the highest accuracy, it does offer interpretability and simplicity, which can
    be valuable in certain contexts.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些局限性，逻辑回归通常仍然是分类任务的一个可靠起点。它提供了一个基准性能，可以用来与更复杂的模型效果进行比较。即使它不能提供最高的准确度，但它提供了可解释性和简单性，在某些情境下是非常有价值的。
- en: Using the logistic regression algorithm for the classifiers challenge
  id: totrans-459
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用逻辑回归算法进行分类器挑战
- en: 'In this section, we will see how we can use the logistic regression algorithm
    for the classifiers challenge:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何使用逻辑回归算法来进行分类器挑战：
- en: 'First, let’s instantiate a logistic regression model and train it using the
    training data:'
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们实例化一个逻辑回归模型并使用训练数据进行训练：
- en: '[PRE35]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s predict the values of the `test` data and create a confusion matrix:'
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们预测`test`数据的值并创建混淆矩阵：
- en: '[PRE36]'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We get the following output upon running the preceding code:'
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在运行上述代码后得到以下输出：
- en: '[PRE37]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, let’s look at the performance metrics:'
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下性能指标：
- en: '[PRE38]'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We get the following output upon running the preceding code:'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在运行上述代码后得到以下输出：
- en: '[PRE39]'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Next, let’s look at **SVMs**.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看**SVM**。
- en: The SVM algorithm
  id: totrans-472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVM算法
- en: The **SVM** classifier is a robust tool in the machine learning arsenal, which
    functions by identifying an optimal decision boundary, or hyperplane, that distinctly
    segregates two classes. To further clarify, think of this ‘hyperplane’ as a line
    (in two dimensions), a surface (in three dimensions), or a manifold (in higher
    dimensions) that best separates the different classes in the feature space.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVM**分类器是机器学习工具中的一个强大工具，通过识别一个最优决策边界或超平面，来显著区分两类。为了进一步解释，可以将这个“超平面”看作是一条线（二维中），一个面（三维中），或一个流形（高维中），它能最好地分隔特征空间中的不同类别。'
- en: The key characteristic that sets SVMs apart is their optimization goal – it
    aims to maximize the margin, which is the distance between the decision boundary
    and the closest data points from either class, known as the ‘support vectors.’
    In simpler terms, the SVM algorithm doesn’t just find a line to separate the classes;
    it also tries to find the line that’s as far away as possible from the closest
    points of each class, thereby maximizing the separating gap.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 区分SVM的关键特性是其优化目标——旨在最大化边距，即决策边界与来自任一类的最近数据点（称为“支持向量”）之间的距离。简单来说，SVM算法不仅仅是找到一条分割类别的线；它还尝试找到一条尽可能远离每个类别最近点的线，从而最大化分隔间隙。
- en: Consider a basic two-dimensional example where we try to separate circles from
    crosses. Our goal with SVMs is not just to find a line that divides these two
    types of shapes but also to find the line that maintains the greatest distance
    from the circles and crosses nearest to it.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个基本的二维示例，我们尝试将圆圈与叉号分开。我们使用SVM的目标不仅仅是找到一条分开这两种形状的线，而是找到一条线，使它与最靠近它的圆圈和叉号之间的距离最大。
- en: SVMs can be extremely useful when dealing with high-dimensional data, complex
    domains, or when the classes are not easily separable by a simple straight line.
    They can perform exceptionally well where logistic regression may falter, for
    instance, in situations with non-linearly separable data.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: SVM（支持向量机）在处理高维数据、复杂领域，或当类之间无法通过简单的直线分割时，非常有用。它们在逻辑回归可能失败的地方表现尤为出色，例如，在非线性可分的数据场景中。
- en: 'The margin is defined as the distance between the separating hyperplane (the
    decision boundary) and the training samples that are closest to this hyperplane,
    called the **support vectors**. So, let’s start with a very basic example with
    only two dimensions, *X*[1] and *X*[2]. We want a line to separate the circles
    from the crosses. This is shown in the following diagram:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 边距定义为分隔超平面（决策边界）与最接近该超平面的训练样本之间的距离，这些训练样本被称为**支持向量**。因此，我们从一个非常基本的二维示例开始，*X*[1]和*X*[2]。我们希望找到一条线来分开圆圈和叉号。如下图所示：
- en: '![Chart, scatter chart  Description automatically generated](img/B18046_07_14.png)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 说明自动生成](img/B18046_07_14.png)'
- en: 'Figure 7.14: SVM algorithm'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：SVM算法
- en: 'We have drawn two lines, and both perfectly separate the crosses from the circles.
    However, there has to be an optimal line, or decision boundary, that gives us
    the best chance of correctly classifying most of the additional examples. A reasonable
    choice may be a line that is evenly spaced between these two classes to give a
    little bit of a buffer for each class, as shown here:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 我们画了两条线，两条线都能完美地将叉号和圆圈分开。然而，必须存在一条最优线或决策边界，它能为我们提供最大机会，以正确分类大多数额外的样本。一个合理的选择可能是将这两类之间均匀分隔的线，从而为每个类提供一点缓冲区，如下图所示：
- en: '![Diagram  Description automatically generated](img/B18046_07_15.png)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![图示 说明自动生成](img/B18046_07_15.png)'
- en: 'Figure 7.15: Concepts related to SVM'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15：与SVM相关的概念
- en: Moreover, unlike logistic regression, SVMs are better equipped to handle smaller,
    cleaner datasets, and they excel at capturing complex relationships without needing
    a large amount of data. However, the trade-off here is interpretability – while
    logistic regression provides easily understandable insights into the model’s decision-making
    process, SVMs, being inherently more complex, are not as straightforward to interpret.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与逻辑回归不同，SVM 更适合处理较小、更干净的数据集，并且它能够在不需要大量数据的情况下捕捉复杂的关系。然而，这里有一个权衡——可解释性——尽管逻辑回归能够提供易于理解的模型决策过程，SVM
    由于其固有的复杂性，解释起来不如逻辑回归直观。
- en: Now, let’s see how we can use SVM to train a classifier for our challenge.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用 SVM 来训练一个分类器以应对我们的挑战。
- en: Using the SVM algorithm for the classifiers challenge
  id: totrans-485
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SVM 算法解决分类器挑战
- en: 'First, let’s instantiate the SVM classifier and then use the training portion
    of the labeled data to train it. The `kernel` hyperparameter determines the type
    of transformation that is applied to the input data in order to make it linearly
    separable:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实例化 SVM 分类器，然后使用标记数据的训练部分来训练它。`kernel` 超参数决定了对输入数据应用何种变换，以使其线性可分：
- en: '[PRE40]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once trained, let’s generate some predictions and look at the confusion matrix:'
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦训练完成，让我们生成一些预测并查看混淆矩阵：
- en: '[PRE41]'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Observe the following output:'
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察以下输出：
- en: '[PRE42]'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, let’s look at the various performance metrics:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下各种性能指标：
- en: '[PRE43]'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'After running the preceding code, we get the following values as our output:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们得到以下值作为输出：
- en: '[PRE44]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Understanding the Naive Bayes algorithm
  id: totrans-496
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解朴素贝叶斯算法
- en: 'Based on probability theory, Naive Bayes is one of the simplest classification
    algorithms. If used properly, it can come up with accurate predictions. The Naive
    Bayes Algorithm is so-named for two reasons:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 基于概率论，朴素贝叶斯是最简单的分类算法之一。如果使用得当，它可以给出准确的预测。朴素贝叶斯算法得名有两个原因：
- en: It is based on a naive assumption that there is independence between the features
    and the input variable.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它基于一个朴素假设，即特征与输入变量之间是独立的。
- en: It is based on Bayes’ theorem. Note that Bayes’ theorem is employed to calculate
    the probability of a particular class or outcome, given some observed features.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它基于贝叶斯定理。请注意，贝叶斯定理用于计算在观察到某些特征的情况下，特定类别或结果的概率。
- en: This algorithm tries to classify instances based on the probabilities of the
    preceding attributes/instances, assuming complete attribute independence.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法尝试基于先前属性/实例的概率来分类实例，假设属性之间完全独立。
- en: 'There are three types of events:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 事件有三种类型：
- en: '**Independent** events do not affect the probability of another event occurring
    (for example, receiving an email offering you free entry to a tech event *and*
    a re-organization occurring in your company).'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立**事件不会影响另一个事件发生的概率（例如，收到一封提供免费入场技术活动的电子邮件 *和* 公司进行重组的事件）。'
- en: '**Dependent** events affect the probability of another event occurring; that
    is, they are linked in some way (for example, the probability of you getting to
    a conference on time could be affected by an airline staff strike or flights that
    may not run on time).'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关**事件会影响另一个事件发生的概率；也就是说，它们在某种程度上是相关的（例如，你准时到达会议的概率可能会受到航空公司员工罢工或航班可能不准时的影响）。'
- en: '**Mutually exclusive** events cannot occur simultaneously (for example, the
    probability of rolling a three and a six on a single dice roll is 0—these two
    outcomes are mutually exclusive).'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互斥**事件不能同时发生（例如，掷骰子时同时出现三和六的概率为 0——这两个结果是互斥的）。'
- en: Bayes’ theorem
  id: totrans-505
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: 'Bayes’ theorem is used to calculate the conditional probability between two
    independent events, *A* and *B*. The probability of events *A* and *B* happening
    is represented by *P*(*A*) and *P*(*B*). The conditional probability is represented
    by *P*(*B*|*A*), which is the conditional probability that event *B* will happen
    given that event *A* has occurred:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理用于计算两个独立事件 *A* 和 *B* 之间的条件概率。事件 *A* 和 *B* 发生的概率分别由 *P*(*A*) 和 *P*(*B*)
    表示。条件概率由 *P*(*B*|*A*) 表示，即在事件 *A* 发生的情况下，事件 *B* 发生的条件概率：
- en: '![](img/B18046_07_021.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_021.png)'
- en: When it comes to applying Naive Bayes, the algorithm is particularly effective
    in scenarios where the dimensionality of the inputs (number of features) is high.
    This makes it well suited for text classification tasks such as spam detection
    or sentiment analysis.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用朴素贝叶斯算法时，特别适用于输入数据维度（特征数量）较高的场景。这使得它非常适合用于文本分类任务，如垃圾邮件检测或情感分析。
- en: It can handle both continuous and discrete data, and it’s computationally efficient,
    making it useful for real-time predictions. Naive Bayes is also a good choice
    when you have limited computational resources and need a quick and easy implementation,
    but it’s worth noting that its “naive” assumption of feature independence can
    be a limitation in some cases.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以处理连续数据和离散数据，并且计算效率高，适用于实时预测。当你拥有有限的计算资源并且需要快速简单的实现时，朴素贝叶斯也是一个不错的选择。但需要注意的是，它的“朴素”假设（即特征独立性）在某些情况下可能成为限制。
- en: Calculating probabilities
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算概率
- en: 'Naive Bayes is based on probability fundamentals. The probability of a single
    event occurring (the observational probability) is calculated by taking the number
    of times the event occurred and dividing it by the total number of processes that
    could have led to that event. For example, a call center receives over 100 support
    calls per day, which is 50 times over the course of a month. You want to know
    the probability that a call is responded to in under three minutes, based on the
    previous amount of time in which it was responded to. If the call center manages
    to match this time record on 27 occasions, then the observational probability
    of 100 calls being answered in under three minutes is as follows:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯基于概率基础。单个事件发生的概率（观察概率）是通过计算事件发生的次数，并将其除以可能导致该事件发生的总次数。例如，呼叫中心每天接到超过100个支持电话，在一个月内发生了50次。你想知道在过去的时间记录下，电话能否在三分钟内得到回应的概率。如果呼叫中心在27次情况下达到了这个时间记录，那么100个电话在三分钟内得到回应的观察概率如下：
- en: '*P(100 support calls in under 3 mins) = (27 / 50) = 0.54 (54%)*'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '*P（3分钟内接到100个支持电话） = (27 / 50) = 0.54（54%）*'
- en: One hundred calls can be responded to in under three minutes in about half the
    time, based on the records of the 50 times it occurred in the past.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 基于过去50次发生的记录，100个电话可以在大约一半的时间内在三分钟内得到回应。
- en: Now, let us look into the multiplication rules for `AND` events.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看`AND`事件的乘法规则。
- en: Multiplication rules for AND events
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`AND`事件的乘法规则'
- en: 'To calculate the probability of two or more events occurring simultaneously,
    consider whether events are independent or dependent. If they are independent,
    the simple multiplication rule is used:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算两个或多个事件同时发生的概率，首先要考虑事件是独立的还是依赖的。如果它们是独立的，则使用简单的乘法规则：
- en: '*P(outcome 1 AND outcome 2) = P(outcome 1) * P(outcome 2)*'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '*P（结果1 AND 结果2）= P（结果1）* P（结果2）*'
- en: For example, to calculate the probability of receiving an email with free entry
    to a tech event *and* re-organization occurring in your workplace, this simple
    multiplication rule would be used. The two events are independent, as the occurrence
    of one does not affect the chance of the other occurring.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要计算收到一封包含免费入场券的技术活动电子邮件*并且*工作场所发生重组的概率，可以使用这个简单的乘法规则。这两个事件是独立的，因为一个事件的发生不会影响另一个事件发生的概率。
- en: 'If receiving the tech event email has a probability of 31% and the probability
    of staff re-organization is 82%, then the probability of both occurring is calculated
    as follows:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 如果收到技术活动电子邮件的概率是31%，而员工重组的概率是82%，则两者同时发生的概率如下计算：
- en: '*P(email AND re-organization) = P(email) * P(re-organization) = (0.31) * (0.82)
    = 0.2542 (25%)*'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '*P（电子邮件 AND 重组） = P（电子邮件）* P（重组） = (0.31) * (0.82) = 0.2542（25%）*'
- en: The general multiplication rule
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般乘法规则
- en: 'If two or more events are dependent, the general multiplication rule is used.
    This formula is actually valid in both cases of independent and dependent events:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个或多个事件是依赖的，则使用一般乘法规则。这个公式实际上在独立事件和依赖事件的两种情况下都是有效的：
- en: '*P(outcome 1 AND outcome 2)=P(outcome 1)*P(outcome 2 | outcome 1)*'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '*P（结果1 AND 结果2）= P（结果1）* P（结果2 | 结果1）*'
- en: Note that *P(outcome 2 | outcome 1)* refers to the conditional probability of
    `outcome 2` occurring given that `outcome 1` has already occurred. The formula
    incorporates the dependence between the events. If the events are independent,
    then the conditional probability is irrelevant as one outcome does not influence
    the chance of the other occurring, and *P(outcome 2 | outcome 1)* is simply *P(outcome
    2)*. Note that the formula in this case just becomes the simple multiplication
    rule.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*P(事件 2 | 事件 1)* 是指在事件 1 已经发生的情况下，事件 2 发生的条件概率。这个公式反映了事件之间的依赖性。如果事件是独立的，那么条件概率就不再重要，因为一个事件不会影响另一个事件发生的概率，此时
    *P(事件 2 | 事件 1)* 就简化为 *P(事件 2)*。请注意，这种情况下的公式变成了简单的乘法规则。
- en: Let’s illustrate this with a simple example. Suppose you’re drawing two cards
    from a deck, and you want to know the probability of drawing an ace first and
    then a king. The first event (drawing an ace) modifies the conditions for the
    second event (drawing a king) since we’re not replacing the ace in the deck. According
    to the general multiplication rule, we can calculate this as *P(ace) * P(king
    | ace)*, where *P(king | ace)* is the probability of drawing a King given that
    we’ve already drawn an ace.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来说明。假设你从一副牌中抽两张牌，并且你想知道先抽到一张 ace 再抽到一张 king 的概率。第一个事件（抽到 ace）会改变第二个事件（抽到
    king）的条件，因为我们并没有将 ace 放回牌堆。根据一般的乘法规则，我们可以计算为 *P(ace) * P(king | ace)*，其中 *P(king
    | ace)* 是在已抽到 ace 的情况下抽到 king 的概率。
- en: Addition rules for OR events
  id: totrans-526
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OR 事件的加法规则
- en: 'When calculating the probability of either one event or the other occurring
    (mutually exclusive), the following simple addition rule is used:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算事件 A 或事件 B 发生的概率（互斥事件）时，使用以下简单的加法规则：
- en: '*P(outcome 1 OR outcome 2) = P(outcome 1) + P(outcome 2)*'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(事件 1 或 事件 2) = P(事件 1) + P(事件 2)*'
- en: 'For example, what is the probability of rolling a 6 or a 3? To answer this
    question, first, note that both outcomes cannot occur simultaneously. The probability
    of rolling a 6 is (1 / 6) and the same can be said for rolling a 3:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，掷出 6 或 3 的概率是多少？为了解答这个问题，首先需要注意这两个结果不能同时发生。掷出 6 的概率是 (1 / 6)，同样掷出 3 的概率也是
    (1 / 6)：
- en: '*P(6 OR 3) = (1 / 6) + (1 / 6) = 0.33 (33%)*'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(6 或 3) = (1 / 6) + (1 / 6) = 0.33 (33%)*'
- en: 'If the events are not mutually exclusive and can occur simultaneously, use
    the following general addition formula, which is always valid in both cases of
    mutual exclusiveness and non-mutual exclusiveness:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 如果事件不是互斥的且可以同时发生，则使用以下通用加法公式，这在互斥和非互斥的情况下都适用：
- en: '*P(outcome 1 OR outcome 2) = P(outcome 1) + P(outcome 2) P(outcome 1 AND outcome
    2)*'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(事件 1 或 事件 2) = P(事件 1) + P(事件 2) - P(事件 1 和 事件 2)*'
- en: Using the Naive Bayes algorithm for the classifiers challenge
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯算法解决分类器挑战
- en: 'Now, let’s use the Naive Bayes algorithm to solve the classifiers challenge:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用朴素贝叶斯算法来解决分类器挑战：
- en: 'First, we will import the `GaussianNB()` function and use it to train the model:'
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将导入 `GaussianNB()` 函数并用它来训练模型：
- en: '[PRE45]'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, let’s use the trained model to predict the results. We will use it to
    predict the labels for our test partition, which is `X_test`:'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用训练好的模型来预测结果。我们将用它来预测测试数据集的标签，即 `X_test`：
- en: '[PRE47]'
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, let’s print the confusion matrix:'
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们打印混淆矩阵：
- en: '[PRE48]'
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, let’s print the performance matrices to quantify the quality of our trained
    model:'
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们打印性能矩阵，量化我们训练模型的质量：
- en: '[PRE50]'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Which gives the output as:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE51]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: For classification algorithms, the winner is...
  id: totrans-547
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对于分类算法，最终的赢家是...
- en: Let’s take a moment to compare the performance metrics of the various algorithms
    we’ve discussed. However, keep in mind that these metrics are highly dependent
    on the data we’ve used in these examples, and they can significantly vary for
    different datasets.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍作停顿，比较一下我们讨论过的各种算法的性能指标。不过，请记住，这些指标高度依赖于我们在这些例子中使用的数据，对于不同的数据集，它们可能会有显著差异。
- en: The performance of a model can be influenced by factors such as the nature of
    the data, the quality of the data, and how well the assumptions of the model align
    with the data.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型的性能可能会受到数据特性、数据质量以及模型假设与数据匹配程度等因素的影响。
- en: 'Here’s a summary of our observations:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们观察结果的总结：
- en: '| **Algorithm** | **Accuracy** | **Recall** | **Precision** |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| **算法** | **准确率** | **召回率** | **精确度** |'
- en: '| Decision tree | 0.94 | 0.93 | 0.88 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.94 | 0.93 | 0.88 |'
- en: '| `XGBoost` | 0.93 | 0.90 | 0.87 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| `XGBoost` | 0.93 | 0.90 | 0.87 |'
- en: '| `Random Forest` | 0.93 | 0.90 | 0.87 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| `随机森林` | 0.93 | 0.90 | 0.87 |'
- en: '| `Logistic regression` | 0.91 | 0.81 | 0.89 |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| `逻辑回归` | 0.91 | 0.81 | 0.89 |'
- en: '| `SVM` | 0.89 | 0.71 | 0.92 |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| `SVM` | 0.89 | 0.71 | 0.92 |'
- en: '| `Naive Bayes` | 0.92 | 0.81 | 0.92 |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| `朴素贝叶斯` | 0.92 | 0.81 | 0.92 |'
- en: From the table above, the decision tree classifier exhibits the highest performance
    in terms of both accuracy and recall in this particular context. For precision,
    we see a tie between the SVM and Naive Bayes algorithms.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 从上表来看，决策树分类器在这个特定的背景下，无论是准确度还是召回率都展现了最高的性能。就精确度而言，我们看到SVM和朴素贝叶斯算法并列。
- en: However, remember that these results are data-dependent. For instance, SVM might
    excel in scenarios where data is linearly separable or can be made so through
    kernel transformations. Naive Bayes, on the other hand, performs well when the
    features are independent. Decision trees and Random Forests might be preferred
    when we have complex non-linear relationships. Logistic regression is a solid
    choice for binary classification tasks and can serve as a good benchmark model.
    Lastly, XGBoost, being an ensemble technique, is powerful when dealing with a
    wide range of data types and often leads in terms of model performance across
    various tasks.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，记住这些结果是数据依赖的。例如，SVM可能在数据线性可分或者通过核变换使其可分的场景中表现出色。另一方面，朴素贝叶斯在特征独立时表现良好。决策树和随机森林在处理复杂的非线性关系时可能更受青睐。逻辑回归是二分类任务的稳健选择，并且可以作为一个不错的基准模型。最后，XGBoost作为一种集成技术，在处理各种数据类型时非常强大，且在许多任务中通常在模型性能上领先。
- en: So, it’s critical to understand your data and the requirements of your task
    before choosing a model. These results are merely a starting point, and deeper
    exploration and validation should be performed for each specific use case.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在选择模型之前，理解你的数据和任务需求至关重要。这些结果仅仅是一个起点，应该针对每个具体的应用场景进行更深入的探索和验证。
- en: Understanding regression algorithms
  id: totrans-561
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解回归算法
- en: A supervised machine learning model uses one of the regression algorithms if
    the label is a continuous variable. In this case, the machine learning model is
    called a regressor.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标签是连续变量，则监督式机器学习模型会使用回归算法。在这种情况下，机器学习模型称为回归器。
- en: To provide a more concrete understanding, let’s take a couple of examples. Suppose
    we want to predict the temperature for the next week based on historical data,
    or we aim to forecast sales for a retail store in the coming months.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更具体的理解，假设我们想基于历史数据预测下周的温度，或者我们希望预测零售店在未来几个月的销售额。
- en: Both temperatures and sales figures are continuous variables, which means they
    can take on any value within a specified range, as opposed to categorical variables,
    which have a fixed number of distinct categories. In such scenarios, we would
    use a regressor rather than a classifier.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 温度和销售额都是连续变量，这意味着它们可以取指定范围内的任何值，与之相对，分类变量具有固定数量的离散类别。在这种情况下，我们会使用回归器而不是分类器。
- en: In this section, we will present various algorithms that can be used to train
    a supervised machine learning regression model—or, put simply, a regressor. Before
    we go into the details of the algorithms, let’s first create a challenge for these
    algorithms to test their performance, abilities, and effectiveness.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍可以用于训练监督式机器学习回归模型的各种算法，简单来说就是回归器。在深入探讨这些算法的细节之前，我们先创建一个挑战，来测试这些算法的性能、能力和有效性。
- en: Presenting the regressors challenge
  id: totrans-566
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归器挑战展示
- en: 'Similar to the approach that we used with the classification algorithms, we
    will first present a problem to be solved as a challenge for all regression algorithms.
    We will call this common problem the regressors challenge. Then, we will use three
    different regression algorithms to address the challenge. This approach of using
    a common challenge for different regression algorithms has two benefits:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在分类算法中使用的方法，我们将首先提出一个问题，作为所有回归算法的挑战。我们将这个共同的问题称为回归器挑战。然后，我们将使用三种不同的回归算法来解决这一挑战。使用共同挑战来比较不同回归算法的方法有两个好处：
- en: We can prepare the data once and use the prepared data on all three regression
    algorithms.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以先准备数据，然后在所有三种回归算法上使用准备好的数据。
- en: We can compare the performance of three regression algorithms in a meaningful
    way, as we will use them to solve the same problem.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以以有意义的方式比较三种回归算法的性能，因为我们将使用它们来解决相同的问题。
- en: Let’s look at the problem statement of the challenge.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看挑战的题目。
- en: The problem statement of the regressors challenge
  id: totrans-571
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归算法挑战的任务描述
- en: Predicting the mileage of different vehicles is important these days. An efficient
    vehicle is good for the environment and is also cost-effective. The mileage can
    be estimated from the power of the engine and the characteristics of the vehicle.
    Let’s create a challenge for regressors to train a model that can predict the
    **Miles per Gallon** (**MPG**) of a vehicle based on its characteristics.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 预测不同车辆的油耗在今天非常重要。一辆高效的车辆对环境有益，也更具成本效益。油耗可以根据发动机的功率和车辆的特性来估算。让我们为回归算法设计一个挑战，训练一个模型，根据车辆的特性预测**每加仑的英里数**（**MPG**）。
- en: Let’s look at the historical dataset that we will use to train the regressors.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看将用于训练回归算法的历史数据集。
- en: Exploring the historical dataset
  id: totrans-574
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索历史数据集
- en: 'The following are the features of the historical dataset data that we have:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们拥有的历史数据集中的特征：
- en: '| **Name** | **Type** | **Description** |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **类型** | **描述** |'
- en: '| `NAME` | Category | Identifies a particular vehicle |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| `名称` | 类别 | 标识特定的车辆 |'
- en: '| `CYLINDERS` | Continuous | The number of cylinders (between four and eight)
    |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| `气缸数` | 连续型 | 气缸的数量（介于四个和八个之间） |'
- en: '| `DISPLACEMENT` | Continuous | The displacement of the engine in cubic inches
    |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| `排量` | 连续型 | 引擎的排量（单位：立方英寸） |'
- en: '| `HORSEPOWER` | Continuous | The horsepower of the engine |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| `马力` | 连续型 | 发动机的马力 |'
- en: '| `ACCELERATION` | Continuous | The time it takes to accelerate from 0 to 60
    mph (in seconds) |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| `加速度` | 连续型 | 从0加速到60英里每小时所需的时间（单位：秒） |'
- en: The label for this problem is a continuous variable, `MPG`, that specifies the
    MPG for each of the vehicles.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的标签是一个连续变量`MPG`，它指定每辆车的英里每加仑（MPG）值。
- en: Let’s first design the data processing pipeline for this problem.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为这个问题设计数据处理管道。
- en: Feature engineering using a data processing pipeline
  id: totrans-584
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用数据处理管道进行特征工程
- en: 'Let’s see how we can design a reusable processing pipeline to address the regressors
    challenge. As mentioned, we will prepare the data once and then use it in all
    the regression algorithms. Let’s follow these steps:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何设计一个可重用的数据处理管道来应对回归算法的挑战。如前所述，我们将一次性准备好数据，然后在所有回归算法中使用它。让我们按以下步骤进行：
- en: 'We will start by importing the dataset, as follows:'
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入数据集，如下所示：
- en: '[PRE52]'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let’s now preview the dataset:'
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们预览一下数据集：
- en: '[PRE53]'
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This is how the dataset will look:'
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就是数据集的样子：
- en: '![Table  Description automatically generated with medium confidence](img/B18046_07_16.png)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated with medium confidence](img/B18046_07_16.png)'
- en: 'Figure 7.16: Please add a caption here'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16：请在此添加标题
- en: Now, let’s proceed on to feature selection. Let’s drop the `NAME` column, as
    it is only an identifier that is needed for cars. Columns that are used to identify
    the rows in our dataset are not relevant to training the model. Let’s drop this
    column.
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们继续进行特征选择。我们将删除`NAME`列，因为它只是车辆所需的标识符。数据集中用于标识行的列与训练模型无关。让我们删除这一列。
- en: 'Let’s convert all of the input variables and impute all the `null` values:'
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们转换所有输入变量，并填充所有`null`值：
- en: '[PRE54]'
  id: totrans-595
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Imputation improves the quality of the data and prepares it to be used to train
    the model. Now, let’s see the final step.
  id: totrans-596
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 填充缺失值可以提高数据质量，并为训练模型做好准备。现在，让我们看一下最后一步。
- en: 'Let’s divide the data into testing and training partitions:'
  id: totrans-597
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将数据分为测试集和训练集：
- en: '[PRE55]'
  id: totrans-598
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This has created the following four data structures:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 这已创建以下四个数据结构：
- en: '`X_train`: A data structure containing the features of the training data'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_train`：包含训练数据特征的数据结构'
- en: '`X_test`: A data structure containing the features of the training test'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_test`：包含训练测试特征的数据结构'
- en: '`y_train`: A vector containing the values of the label in the training dataset'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_train`：包含训练数据集标签值的向量'
- en: '`y_test`: A vector containing the values of the label in the testing dataset'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_test`：包含测试数据集标签值的向量'
- en: Now, let’s use the prepared data on three different regressors so that we can
    compare their performance.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用准备好的数据在三种不同的回归算法上进行测试，以便比较它们的性能。
- en: Linear regression
  id: totrans-605
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: Among the assortment of supervised machine learning algorithms, linear regression
    is often seen as the most straightforward to grasp. Initially, we will explore
    simple linear regression and then gradually broaden our discussion to encompass
    multiple linear regression.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种监督学习算法中，线性回归通常被认为是最简单的。最初，我们将探讨简单线性回归，然后逐渐扩展讨论，涵盖多元线性回归。
- en: It’s important to note, however, that while linear regression is accessible
    and easy to implement, it is not always the ‘best’ choice in every circumstance.
    Each machine learning algorithm, including the ones we’ve discussed so far, comes
    with its unique strengths and limitations, and their effectiveness varies depending
    on the type and structure of the data at hand.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是要注意，尽管线性回归方法易于实现且易于使用，但并不总是在所有情况下都是‘最佳’选择。每种机器学习算法，包括我们到目前为止讨论过的，都有其独特的优势和局限性，并且它们的效果因数据的类型和结构而异。
- en: For instance, decision trees and Random Forests are excellent at handling categorical
    data and capturing complex non-linear relationships. SVMs can work well with high-dimensional
    data and are robust to outliers, while logistic regression is particularly effective
    for binary classification problems.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，决策树和随机森林在处理分类数据和捕捉复杂的非线性关系方面表现出色。支持向量机（SVM）在高维数据中表现良好，并且对异常值具有鲁棒性，而逻辑回归对于二分类问题特别有效。
- en: On the other hand, linear regression models are well suited to predicting continuous
    outcomes and can provide interpretability, which can be valuable in understanding
    the impact of individual features.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，线性回归模型非常适合预测连续结果，并且具有可解释性，这在理解单个特征的影响时非常有价值。
- en: Simple linear regression
  id: totrans-610
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: 'At its most basic level, linear regression establishes a relationship between
    two variables, usually represented as a single independent variable and a single
    dependent variable. Linear regression is a technique that enables us to study
    how changes in the dependent variable (plotted on the *y*-axis) are influenced
    by changes in the independent variable (plotted on the *x*-axis). It can be represented
    as follows:'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基本的层面上，线性回归建立了两个变量之间的关系，通常表示为一个自变量和一个因变量。线性回归是一种技术，可以帮助我们研究因变量（绘制在 *y* 轴上）的变化是如何受到自变量（绘制在
    *x* 轴上）变化的影响的。它可以表示如下：
- en: '![](img/B18046_07_022.png)'
  id: totrans-612
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_022.png)'
- en: 'This formula can be explained as follows:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式可以这样解释：
- en: '*y* is the dependent variable.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 是因变量。'
- en: '*X* is the independent variable.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X* 是自变量。'
- en: '![](img/B18046_07_023.png) is the slope that indicates how much the line rises
    for each increase in *X*.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18046_07_023.png) 是斜率，表示每当 *X* 增加时，直线上升的幅度。'
- en: '*α* is the intercept that indicates the value of *y* when *X* = 0.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α* 是截距，表示当 *X* = 0 时 *y* 的值。'
- en: 'Linear regression operates under these assumptions:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归基于以下假设：
- en: '**Linearity**: The relationship between independent and dependent variables
    is linear.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性关系**：自变量与因变量之间的关系是线性的。'
- en: '**Independence**: The observations are independent of each other.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立性**：观察值之间相互独立。'
- en: '**No multicollinearity**: The independent variables are not too highly correlated
    with each other.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无多重共线性**：自变量之间没有过高的相关性。'
- en: 'Some examples of relationships between a single continuous dependent variable
    and a single continuous independent variable are as follows:'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 单一连续因变量与单一连续自变量之间关系的一些例子如下：
- en: A person’s weight and their calorie intake
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个人的体重与其卡路里摄入量之间的关系
- en: The price of a house and its area in square feet in a particular neighborhood
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定社区中房屋价格与其面积（以平方英尺计）之间的关系
- en: The humidity in the air and the likelihood of rain
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空气中的湿度与降雨概率之间的关系
- en: For linear regression, both the input (independent) variable and the target
    (dependent) variable must be numeric. The best relationship is found by minimizing
    the sum of the squares of the vertical distances of each point, from a line drawn
    through all the points. It is assumed that the relationship is linear between
    the predictor variable and the label. For example, the more money invested in
    research and development, the higher the sales.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性回归，输入（自变量）和目标（因变量）必须都是数值型的。通过最小化每个数据点到通过所有点绘制的直线的垂直距离的平方和，来找到最佳的关系。假设自变量与标签之间是线性关系。例如，投入更多资金进行研发，会导致销售额的提高。
- en: 'Let’s look at a specific example. Let’s try to formulate the relationship between
    marketing expenditures and sales for a particular product. They are found to be
    directly relational to each other. The marketing expenditures and sales are drawn
    on a two-dimensional graph and are shown as blue diamonds. The relationship can
    best be approximated by drawing a straight line, as shown in the following graph:'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的例子。假设我们要建立营销支出与某一产品销售之间的关系。经过研究发现，它们是直接相关的。营销支出和销售额在二维图表上绘制出来，显示为蓝色的菱形点。我们可以通过绘制一条直线来近似表示这种关系，如下图所示：
- en: '![Chart, scatter chart  Description automatically generated](img/B18046_07_17.png)'
  id: totrans-628
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 说明自动生成](img/B18046_07_17.png)'
- en: 'Figure 7.17: Linear regression'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17：线性回归
- en: Once the linear line is drawn, we can see the mathematical relationship between
    marketing expenditure and sales.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦画出线性直线，我们就能看到营销支出与销售额之间的数学关系。
- en: Evaluating the regressors
  id: totrans-631
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归模型评估
- en: 'The linear line that we drew is an approximation of the relationship between
    the dependent and independent variables. Even the best line will have some deviation
    from the actual values, as shown here:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制的线性直线是依赖变量和自变量之间关系的近似。即便是最佳拟合线，也会与实际值有所偏差，如下所示：
- en: '![Chart, scatter chart  Description automatically generated](img/B18046_07_18.png)'
  id: totrans-633
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 说明自动生成](img/B18046_07_18.png)'
- en: 'Figure 7.18: Evaluating regressors'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18：回归模型评估
- en: 'A typical way of quantifying the performance of linear regression models is
    by using **Root Mean Square Error** (**RMSE**). This calculates the standard deviation
    of the errors made by the trained model mathematically. For example, in the training
    dataset, the `loss` function is calculated as follows:'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 量化线性回归模型性能的常见方法是使用 **均方根误差** (**RMSE**)。该方法计算训练模型的误差标准差。例如，在训练数据集中，`loss` 函数的计算如下：
- en: '*Loss (ý*^((i))*, y*^((i))*) = 1/2(ý*^((i))*- y*^((i))*)2*'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '*Loss (ý*^((i))*, y*^((i))*) = 1/2(ý*^((i))*- y*^((i))*)²*'
- en: 'This leads to the following `cost` function, which minimizes the loss of all
    of the examples in the training set:'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 这得出了以下 `cost` 函数，用于最小化训练集所有样本的损失：
- en: '![](img/B18046_07_024.png)'
  id: totrans-638
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_024.png)'
- en: Let’s try to interpret RMSE. If RMSE is $50 for our example model that predicts
    the price of a product, this means that around 68.2% of the predictions will fall
    within $50 of the true value (that is, ![](img/B18046_07_025.png)). It also means
    that 95% of the predictions will fall within $100 (that is, ![](img/B18046_07_026.png))
    of the actual value. Finally, 99.7% of the predictions will fall within $150 of
    the actual value.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着解释 RMSE。如果我们预测产品价格的示例模型的 RMSE 为 50 美元，这意味着大约 68.2% 的预测结果会落在真实值的 50 美元范围内（即
    ![](img/B18046_07_025.png)）。这也意味着 95% 的预测结果会落在真实值的 100 美元范围内（即 ![](img/B18046_07_026.png)）。最后，99.7%
    的预测结果会落在真实值的 150 美元范围内。
- en: Let us look into multiple regression.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解多元回归。
- en: Multiple regression
  id: totrans-641
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多元回归
- en: The fact is that most real-world analyses have more than one independent variable.
    Multiple regression is an extension of simple linear regression. The key difference
    is that there are additional beta coefficients for the additional predictor variables.
    When training a model, the goal is to find the beta coefficients that minimize
    the errors of the linear equation. Let’s try to mathematically formulate the relationship
    between the dependent variable and the set of independent variables (features).
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，大多数现实世界的分析都涉及多个自变量。多元回归是简单线性回归的扩展。其关键区别在于，除了原有的回归系数外，还有额外的预测变量系数。在训练模型时，目标是找到那些最小化线性方程误差的回归系数。让我们尝试用数学公式表达依赖变量与一组自变量（特征）之间的关系。
- en: For instance, in the housing market, the price of a house (the dependent variable)
    could depend on numerous factors such as its size, location, age, and more (the
    independent variables).
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，在房地产市场中，房价（依赖变量）可能受多个因素的影响，如房屋的面积、位置、年龄等（自变量）。
- en: 'Similar to a simple linear equation, the dependent variable, *y*, is quantified
    as the sum of an intercept term, plus the product of the ![](img/B18046_07_027.png)
    coefficients multiplied by the *x* value for each of the *i* features:'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于简单线性方程，依赖变量 *y* 可以量化为截距项与每个 *i* 特征的 *x* 值乘以回归系数的乘积之和：
- en: '![](img/B18046_07_028.png)'
  id: totrans-645
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_07_028.png)'
- en: The error is represented by ![](img/B18046_07_029.png) and indicates that the
    predictions are not perfect.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 误差由![](img/B18046_07_029.png)表示，表明预测并不完美。
- en: The ![](img/B18046_07_027.png) coefficients allow each feature to have a separate
    estimated effect on the value of *y* because *y* changes by an amount of ![](img/B18046_07_031.png)
    for each unit increase in *x*[i]. Moreover, the intercept (![](img/B18046_07_032.png))
    indicates the expected value of *y* when the independent variables are all 0.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B18046_07_027.png)系数使每个特征对*y*值的影响可以单独估算，因为当*x*[i]增加1个单位时，*y*变化了![](img/B18046_07_031.png)。此外，截距（![](img/B18046_07_032.png)）表示当所有自变量为0时，*y*的期望值。'
- en: Note that all the variables in the preceding equation can be represented by
    a bunch of vectors. The target and predictor variables are now vectors with a
    row, and the regression coefficients, ![](img/B18046_07_033.png), and errors,
    ![](img/B18046_07_029.png), are also vectors.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面方程中的所有变量都可以表示为一组向量。目标和预测变量现在是带有一行的向量，而回归系数![](img/B18046_07_033.png)和误差![](img/B18046_07_029.png)也都是向量。
- en: Next, let us look into how we can use linear regression for the regressors challenge.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用线性回归来解决回归者挑战。
- en: Using the linear regression algorithm for the regressors challenge
  id: totrans-650
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线性回归算法解决回归者挑战
- en: 'Now, let’s train the model using the training portion of the dataset. Note
    that we will use the same data and data engineering logic that we discussed earlier:'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用数据集的训练部分来训练模型。请注意，我们将使用之前讨论过的相同数据和数据工程逻辑：
- en: 'Let’s start by importing the linear regression package:'
  id: totrans-652
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先导入线性回归包：
- en: '[PRE56]'
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Then, let’s instantiate the linear regression model and train it using the
    training dataset:'
  id: totrans-654
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们实例化线性回归模型，并使用训练数据集对其进行训练：
- en: '[PRE57]'
  id: totrans-655
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-656
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, let’s predict the results using the test portion of the dataset:'
  id: totrans-657
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用数据集的测试部分来预测结果：
- en: '[PRE59]'
  id: totrans-658
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output generated by running the preceding code will generate the following:'
  id: totrans-659
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行前面的代码生成的输出将如下所示：
- en: '[PRE60]'
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: As discussed in the preceding section, RMSE is the standard deviation of the
    error. It indicates that 68.2% of predictions will fall within `4.36` of the value
    of the label.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，RMSE是误差的标准差。它表明68.2%的预测结果将在标签值的`4.36`范围内。
- en: Let us look into when we can use linear regression.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看什么时候可以使用线性回归。
- en: When is linear regression used?
  id: totrans-663
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么时候使用线性回归？
- en: 'Linear regression is used to solve many real-world problems, including the
    following:'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归被用来解决许多现实世界中的问题，包括以下几个：
- en: Sales forecasting
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 销售预测
- en: Predicting optimum product prices
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测最佳产品价格
- en: Quantifying the causal relationship between an event and the response, such
    as in clinical drug trials, engineering safety tests, or marketing research
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化事件与反应之间的因果关系，例如在临床药物试验、工程安全测试或市场研究中。
- en: Identifying patterns that can be used to forecast future behavior, given known
    criteria—for example, predicting insurance claims, natural disaster damage, election
    results, and crime rates
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别可以用来预测未来行为的模式，前提是已知某些标准——例如，预测保险理赔、自然灾害损失、选举结果和犯罪率。
- en: Let us next look into the weaknesses of linear regression.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看线性回归的弱点。
- en: The weaknesses of linear regression
  id: totrans-670
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归的弱点
- en: 'The weaknesses of linear regression are as follows:'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的弱点如下：
- en: It only works with numerical features.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它只适用于数值特征。
- en: Categorical data needs to be pre-processed.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别数据需要预处理。
- en: It does not cope well with missing data.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不适应缺失数据。
- en: It makes assumptions about the data.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对数据有假设。
- en: The regression tree algorithm
  id: totrans-676
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归树算法
- en: Similar to classification trees used for categorical outcomes, regression trees
    are another subset of decision trees, but they are employed when the target, or
    label, is a continuous variable instead of categorical. This distinction impacts
    how the tree algorithm processes and learns from the data.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于用于分类结果的分类树，回归树是决策树的另一个子集，但它们在目标或标签是连续变量而不是类别变量时使用。这一区别影响了树算法如何处理和学习数据。
- en: In the case of classification trees, the algorithm tries to identify the categories
    that the data points belong to. However, with regression trees, the goal is to
    predict a specific, continuous value. This might be something like the price of
    a house, a company’s future stock price, or the likely temperature tomorrow.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类树，算法试图识别数据点属于哪个类别。然而，在回归树中，目标是预测一个特定的连续值。这可能是房价、公司未来的股票价格，或者明天的温度。
- en: These variations between classification and regression trees also lead to differences
    in the algorithms used. In a classification tree, we typically use metrics such
    as Gini impurity or entropy to find the best split. In contrast, regression trees
    utilize measures like **Mean Squared Error** (**MSE**) to minimize the distance
    between the actual and predicted continuous values.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树和回归树之间的这些差异也导致了所使用算法的不同。在分类树中，我们通常使用如基尼不纯度或熵等指标来找到最佳分割点。而回归树则利用**均方误差**（**MSE**）等度量来最小化实际值和预测值之间的距离。
- en: Using the regression tree algorithm for the regressors challenge
  id: totrans-680
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用回归树算法解决回归问题挑战
- en: 'In this section, we will see how a regression tree algorithm can be used for
    the regressors challenge:'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看到如何使用回归树算法来处理回归问题挑战：
- en: 'First, we train the model using a regression tree algorithm:'
  id: totrans-682
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用回归树算法来训练模型：
- en: '[PRE61]'
  id: totrans-683
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-684
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Once the regression tree model is trained, we use the trained model to predict
    the values:'
  id: totrans-685
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦回归树模型训练完成，我们就使用训练好的模型来预测值：
- en: '[PRE63]'
  id: totrans-686
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Then, we calculate RMSE to quantify the performance of the model:'
  id: totrans-687
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们计算 RMSE 来量化模型的性能：
- en: '[PRE64]'
  id: totrans-688
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We get the following output:'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '[PRE65]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The gradient boost regression algorithm
  id: totrans-691
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升回归算法
- en: Now, let’s shift our focus to the gradient boosting regression algorithm, which
    uses an ensemble of decision trees to formulate underlying patterns within a dataset.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把焦点转向梯度提升回归算法，它通过集成多个决策树来提取数据集中的潜在模式。
- en: At its core, gradient boosting regression operates by creating a ‘team’ of decision
    trees, where each member progressively learns from the mistakes of its predecessors.
    In essence, each subsequent decision tree in the sequence attempts to correct
    the prediction errors made by the tree before it, leading to an ‘ensemble’ that
    makes a final prediction based on the collective wisdom of all the individual
    trees. What makes this algorithm truly unique is its capability to handle a broad
    spectrum of data and its resistance to overfitting. This versatility allows it
    to perform admirably across diverse datasets and problem scenarios.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，梯度提升回归通过创建一个“团队”决策树来运作，每个成员逐步从前一个成员的错误中学习。实际上，序列中的每棵决策树都试图纠正前一棵树所做的预测错误，从而形成一个“集成”模型，基于所有单个树的集体智慧做出最终预测。这个算法真正独特之处在于它能够处理广泛的数据，并且具有抗过拟合的能力。这种多功能性使得它能够在不同的数据集和问题场景中表现出色。
- en: Using the gradient boost regression algorithm for the regressors challenge
  id: totrans-694
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度提升回归算法解决回归问题挑战
- en: In this section, we will see how we can use the gradient boost regression algorithm
    for the regressors challenge, predicting a car’s MPG rating, which is a continuous
    variable and, therefore, a classic regression problem. Remember that our independent
    variables include features like ‘`CYLINDERS`,’ ‘`DISPLACEMENT`,’ ‘`HORSEPOWER`,’
    ‘`WEIGHT`,’ and ‘`ACCELERATION`.’
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看到如何使用梯度提升回归算法解决回归问题挑战，预测汽车的 MPG（每加仑英里数）评分，这是一个连续变量，因此是一个典型的回归问题。记住，我们的自变量包括像‘`CYLINDERS`’、‘`DISPLACEMENT`’、‘`HORSEPOWER`’、‘`WEIGHT`’
    和 ‘`ACCELERATION`’ 等特征。
- en: Looking closely, MPG is not as straightforward as it may seem, considering the
    multifaceted relationships between the influencing factors. For example, while
    cars with higher displacement typically consume more fuel, leading to a lower
    MPG, this relationship could be offset by factors like weight and horsepower.
    It’s these nuanced interactions that may elude simpler models like linear regression
    or a single decision tree.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察，MPG 并不像看起来那样简单，因为影响因素之间存在多方面的关系。例如，虽然具有更大排量的汽车通常消耗更多燃料，导致较低的 MPG，但这一关系可能会被像重量和马力等因素所抵消。正是这些微妙的交互作用可能会让简单的模型如线性回归或单一决策树无法捕捉到。
- en: This is where the gradient boosting regression algorithm may be useful. By building
    an ensemble of decision trees, each learning from the errors of its predecessor,
    the model will aim to discern these complex patterns in the data. Each tree contributes
    its understanding of the data, refining the predictions to be more accurate and
    reliable.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，梯度提升回归算法可能会派上用场。通过构建一个决策树的集成体，每棵树都从前一个树的错误中学习，模型将旨在辨别数据中的这些复杂模式。每棵树都为数据提供其理解，精炼预测，使其更加准确和可靠。
- en: 'For example, one decision tree might learn that cars with larger ‘`DISPLACEMENT`''
    values tend to have lower `MPG`. The next tree might then pick up on the subtlety
    that lighter cars (‘`WEIGHT`'') with the same ‘`DISPLACEMENT`'' can sometimes
    achieve higher `MPG`. Through this iterative learning process, the model unveils
    the intricate layers of relationships between the variables:'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个决策树可能会发现，拥有较大‘`DISPLACEMENT`'值的汽车往往具有较低的`MPG`。下一个决策树可能会捕捉到一个细微差别，即具有相同‘`DISPLACEMENT`'值的较轻汽车（‘`WEIGHT`'）有时可以获得更高的`MPG`。通过这种迭代学习过程，模型揭示了变量之间复杂的关系层次：
- en: 'The first step in our Python script is to import the necessary library:'
  id: totrans-699
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的Python脚本的第一步是导入必要的库：
- en: '[PRE66]'
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Here, we import the `ensemble` module from the `sklearn` library:'
  id: totrans-701
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们从`sklearn`库导入`ensemble`模块：
- en: '[PRE67]'
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-703
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-704
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Finally, we calculate RMSE to quantify the performance of the model:'
  id: totrans-705
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算RMSE以量化模型的表现：
- en: '[PRE70]'
  id: totrans-706
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Running this will give us the output value, as follows:'
  id: totrans-707
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行这个程序将给出如下输出值：
- en: '[PRE71]'
  id: totrans-708
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: For regression algorithms, the winner is...
  id: totrans-709
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对于回归算法，优胜者是……
- en: 'Let’s look at the performance of the three regression algorithms that we used
    on the same data and exactly the same use case:'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在相同数据和完全相同的用例上，我们使用的三种回归算法的表现：
- en: '| **Algorithm** | **RMSE** |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '| **算法** | **RMSE** |'
- en: '| Linear regression | 4.36214129677179 |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
  zh: '| 线性回归 | 4.36214129677179 |'
- en: '| Regression tree | 5.2771702288377 |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
  zh: '| 回归树 | 5.2771702288377 |'
- en: '| Gradient boost regression | 4.034836373089085 |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
  zh: '| 梯度提升回归 | 4.034836373089085 |'
- en: Looking at the performance of all the regression algorithms, it is obvious that
    the performance of gradient boost regression is the best, as it has the lowest
    RMSE. This is followed by linear regression. The regression tree algorithm performed
    the worst for this problem.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 从所有回归算法的表现来看，很明显梯度提升回归的表现最佳，因为它的RMSE最低。紧随其后的是线性回归。回归树算法在这个问题中的表现最差。
- en: Practical example – how to predict the weather
  id: totrans-716
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际例子——如何预测天气
- en: Now, we’ll transition from theory to application, employing the concepts we’ve
    discussed in this chapter to predict tomorrow’s rainfall, based on a year’s worth
    of weather data from a specific city. This real-world scenario aims to reinforce
    the principles of supervised learning.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从理论过渡到应用，运用本章讨论的概念，通过一整年的某城市天气数据来预测明天的降雨情况。这个实际场景旨在强化有监督学习的原理。
- en: There are numerous algorithms capable of this task, but selecting the most suitable
    one hinges on the specific characteristics of our problem and data. Each algorithm
    has unique advantages and excels in specific contexts. For example, while linear
    regression can be ideal when there’s a discernible numerical correlation, decision
    trees might be more effective when dealing with categorical variables or non-linear
    relationships.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多算法能够完成这个任务，但选择最合适的算法取决于我们问题和数据的具体特点。每种算法都有独特的优势，在特定情境下表现出色。例如，当存在明显的数值相关性时，线性回归可能是理想选择，而当处理分类变量或非线性关系时，决策树可能更有效。
- en: For this prediction challenge, we have chosen logistic regression. This choice
    is driven by the binary nature of our prediction target (i.e., will it rain tomorrow
    or not?), a situation where Logistic Regression often excels. This algorithm provides
    a probability score between 0 and 1, allowing us to make clear yes-or-no predictions,
    ideal for our rainfall forecast scenario.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个预测挑战，我们选择了逻辑回归。这个选择源于我们预测目标的二元性质（即，明天是否会下雨？），这种情况下逻辑回归往往表现优异。该算法提供一个介于0和1之间的概率分数，使我们能够做出明确的是/否预测，非常适合我们的降雨预测场景。
- en: Remember, this practical example differs from previous ones. It’s crafted to
    help you grasp how we select and apply a particular algorithm to specific real-world
    problems, offering a deeper understanding of the thought process behind algorithm
    selection.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这个实际例子与之前的例子有所不同。它旨在帮助你理解我们如何选择并应用特定的算法来解决实际问题，提供了算法选择背后的思维过程的更深入理解。
- en: 'The data available to train this model is in the CSV file called `weather.csv`:'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 训练此模型的数据位于名为`weather.csv`的CSV文件中：
- en: 'Let’s import the data as a pandas DataFrame:'
  id: totrans-722
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将数据导入为pandas DataFrame：
- en: '[PRE72]'
  id: totrans-723
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Let’s look at the columns of the DataFrame:'
  id: totrans-724
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看一下DataFrame的列：
- en: '[PRE73]'
  id: totrans-725
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-726
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now, let’s look at the header of the first 13 columns of the `weather.csv`
    data that show the typical weather of a city:'
  id: totrans-727
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下`weather.csv`数据中显示城市典型天气的前13列的表头：
- en: '[PRE75]'
  id: totrans-728
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '![A screenshot of a computer  Description automatically generated](img/B18046_07_19.png)'
  id: totrans-729
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer  Description automatically generated](img/B18046_07_19.png)'
- en: 'Figure 7.19: Data showing typical weather of a city'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19：显示城市典型天气的数据
- en: 'Now, let’s look at the last 10 columns of the `weather.csv` data:'
  id: totrans-731
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看`weather.csv`数据的最后10列：
- en: '[PRE76]'
  id: totrans-732
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '![A picture containing application  Description automatically generated](img/B18046_07_20.png)'
  id: totrans-733
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing application  Description automatically generated](img/B18046_07_20.png)'
- en: 'Figure 7.20: Last 10 columns of the weather.csv data'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20：`weather.csv`数据的最后10列
- en: 'Let’s use `x` to represent the input features. We will drop the `Date` field
    for the feature list, as it is not useful in the context of predictions. We will
    also drop the `RainTomorrow` label:'
  id: totrans-735
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用`x`来表示输入特征。我们将删除`Date`字段，因为它在预测中没有用处。我们还将删除`RainTomorrow`标签：
- en: '[PRE77]'
  id: totrans-736
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let’s use `y` to represent the label:'
  id: totrans-737
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用`y`来表示标签：
- en: '[PRE78]'
  id: totrans-738
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Now, let’s divide the data into `train_test_split`:'
  id: totrans-739
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将数据划分为`train_test_split`：
- en: '[PRE79]'
  id: totrans-740
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'As the label is a binary variable, we will train a classifier. So, logistic
    regression will be a good choice here. First, let’s instantiate the logistic regression
    model:'
  id: totrans-741
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于标签是一个二元变量，我们将训练一个分类器。因此，逻辑回归在这里是一个不错的选择。首先，让我们实例化逻辑回归模型：
- en: '[PRE80]'
  id: totrans-742
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now, we can use `train_x` and `test_x` to train the model:'
  id: totrans-743
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`train_x`和`test_x`来训练模型：
- en: '[PRE81]'
  id: totrans-744
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Once the model is trained, let’s use it for predictions:'
  id: totrans-745
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们就可以使用它进行预测：
- en: '[PRE82]'
  id: totrans-746
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Now, let’s find the accuracy of our trained model:'
  id: totrans-747
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来看看我们训练模型的准确度：
- en: '[PRE83]'
  id: totrans-748
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-749
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Now, this binary classifier can be used to predict whether it will rain tomorrow.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个二元分类器可以用来预测明天是否会下雨。
- en: Summary
  id: totrans-751
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Wrapping up, this chapter served as a comprehensive expedition into the multifaceted
    landscape of supervised machine learning. We spotlighted the primary components
    of classification and regression algorithms, dissecting their mechanics and applications.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章是一次深入监督机器学习多面性领域的全面探索。我们重点介绍了分类和回归算法的主要组成部分，剖析了它们的机制和应用。
- en: The chapter demonstrated a broad spectrum of algorithms through practical examples,
    providing an opportunity to understand the functionality of these tools in real-world
    contexts. This journey underscored the adaptability of supervised learning techniques
    and their ability to tackle varied problems.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过实际例子展示了广泛的算法，提供了在现实场景中理解这些工具功能的机会。这一旅程强调了监督学习技术的适应性及其解决各种问题的能力。
- en: By juxtaposing the performance of different algorithms, we emphasized the crucial
    role of context when selecting an optimal machine learning strategy. Factors such
    as data size, feature complexity, and prediction requirements play significant
    roles in this selection process.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 通过并列不同算法的表现，我们强调了在选择最佳机器学习策略时，上下文的重要性。数据大小、特征复杂度和预测要求等因素在这个选择过程中起着重要作用。
- en: As we transition to the upcoming chapters, the knowledge gleaned from this exploration
    serves as a robust foundation. This understanding of how to apply supervised learning
    techniques in practical scenarios is a critical skill set in the vast realm of
    machine learning. Keep these insights at your fingertips as we journey further
    into the compelling world of AI, preparing for an even deeper dive into the complex
    universe of neural networks.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入接下来的章节，从本次探索中获得的知识将作为坚实的基础。这种如何在实际场景中应用监督学习技术的理解，是机器学习广阔领域中的一项关键技能。在我们进一步探索人工智能的迷人世界时，保持这些见解在手，为深入研究神经网络的复杂世界做好准备。
- en: Learn more on Discord
  id: totrans-756
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Discord上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的Discord社区——你可以在这里分享反馈、向作者提问并了解新版本——请扫描下面的二维码：
- en: '[https://packt.link/WHLel](https://packt.link/WHLel)'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/WHLel](https://packt.link/WHLel)'
- en: '![](img/QR_Code1955211820597889031.png)'
  id: totrans-759
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1955211820597889031.png)'
