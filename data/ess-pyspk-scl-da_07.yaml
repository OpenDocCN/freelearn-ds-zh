- en: 'Chapter 5: Scalable Machine Learning with PySpark'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章：使用 PySpark 进行可扩展机器学习
- en: In the previous chapters, we have established that modern-day data is growing
    at a rapid rate, with a volume, velocity, and veracity not possible for traditional
    systems to keep pace with. Thus, we learned about distributed computing to keep
    up with the ever-increasing data processing needs and saw practical examples of
    ingesting, cleansing, and integrating data to bring it to a level that is conducive
    to business analytics using the power and ease of use of Apache Spark's unified
    data analytics platform. This chapter, and the chapters that follow, will explore
    the data science and **machine learning** (**ML**) aspects of data analytics.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们已经建立了现代数据以惊人的速度增长，且其体量、速度和准确性是传统系统无法跟上的。因此，我们学习了分布式计算，以跟上日益增长的数据处理需求，并通过实际案例了解如何摄取、清洗和整合数据，将其处理到适合商业分析的水平，充分利用
    Apache Spark 统一的数据分析平台的强大功能和易用性。本章及后续章节将探索数据科学和**机器学习**（**ML**）在数据分析中的应用。
- en: Today, the computer science disciplines of AI and ML have made a massive comeback
    and are pervasive. Businesses everywhere need to leverage these techniques to
    remain competitive, expand their customer base, introduce novel product lines,
    and stay profitable. However, traditional ML and data science techniques were
    designed to deal with limited samples of data and are not inherently scalable.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，人工智能（AI）和机器学习（ML）计算机科学领域正经历大规模复兴，并且无处不在。各行各业的企业都需要利用这些技术来保持竞争力，扩大客户群，推出新产品线，并保持盈利。然而，传统的机器学习和数据科学技术是为了处理有限的数据样本而设计的，本身并不具备扩展性。
- en: This chapter provides you with an overview of traditional ML algorithms, including
    supervised and unsupervised ML techniques and explores real-world use cases of
    ML in business applications. Then you will learn about the need for scalable ML.
    A few techniques for scaling out ML algorithms in a distributed fashion to handle
    very large data samples will be presented. Then, we will dive into the ML library
    of Apache Spark, called MLlib, along with code examples, to perform data wrangling
    using Apache Spark's MLlib to explore, clean, and manipulate data in order to
    get it ready for ML applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为你提供了传统机器学习算法的概述，包括有监督和无监督的 ML 技术，并探索了机器学习在商业应用中的实际案例。接着，你将了解可扩展机器学习的必要性。将介绍一些以分布式方式扩展
    ML 算法、处理非常大的数据样本的技术。然后，我们将深入探讨 Apache Spark 的 ML 库——MLlib，并结合代码示例，使用 Apache Spark
    的 MLlib 进行数据整理，探索、清洗并操作数据，为机器学习应用做好准备。
- en: 'This chapter covers the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主要内容：
- en: ML overview
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: Scaling out machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展机器学习
- en: Data wrangling with Apache Spark and MLlib
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 和 MLlib 进行数据整理
- en: By the end of this chapter, you will have gained an appreciation for scalable
    ML and its business applications and acquired a basic understanding of Apache
    Spark's scalable ML library, named MLlib. You will have acquired the skills necessary
    to utilize MLlib to clean and transform data and get it ready for ML applications
    at scale, helping you to reduce the time taken for data cleansing tasks, and making
    your overall ML life cycle much more efficient.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将会对可扩展的机器学习（ML）及其商业应用有所了解，并掌握 Apache Spark 的可扩展 ML 库——MLlib 的基本知识。你将掌握使用
    MLlib 清洗和转换数据的技能，为大规模机器学习应用做准备，帮助你减少数据清洗任务所需的时间，使你的整体 ML 生命周期更加高效。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using the Databricks Community Edition to run our
    code: [https://community.cloud.databricks.com](https://community.cloud.databricks.com).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 Databricks Community Edition 来运行我们的代码：[https://community.cloud.databricks.com](https://community.cloud.databricks.com)。
- en: Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册说明可以在 [https://databricks.com/try-databricks](https://databricks.com/try-databricks)
    找到。
- en: The code and data used in this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter05](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter05).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章使用的代码和数据可以从 [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter05](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter05)
    下载。
- en: ML overview
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: '**Machine Learning** is a field of AI and computer science that leverages statistical
    models and computer science algorithms for learning patterns inherent in data,
    without being explicitly programmed. ML consists of algorithms that automatically
    convert patterns within data into models. Where pure mathematical or rule-based
    models perform the same task over and over again, an ML model learns from data
    and its performance can be greatly improved by exposing it to vast amounts of
    data.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**是人工智能和计算机科学的一个领域，利用统计模型和计算机算法学习数据中固有的模式，而无需显式编程。机器学习由能够自动将数据中的模式转换为模型的算法组成。当纯数学或基于规则的模型一遍又一遍地执行相同任务时，机器学习模型则从数据中学习，并且通过暴露于大量数据，性能可以大大提高。'
- en: A typical ML process involves applying an ML algorithm to a known dataset called
    the training dataset, to generate a new ML model. This process is generally termed
    *model training* or *model fitting*. Some ML models are trained on datasets containing
    a known correct answer that we intend to predict in an unknown dataset. This known,
    correct value in the training dataset is termed the *label*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的机器学习过程涉及将机器学习算法应用于已知数据集（称为训练数据集），以生成新的机器学习模型。这个过程通常被称为*模型训练*或*模型拟合*。一些机器学习模型是在包含已知正确答案的数据集上进行训练的，目的是在未知数据集中预测这些答案。训练数据集中已知的正确值称为*标签*。
- en: "Once the model is trained, the resultant model is applied to new data in order\
    \ to \Lpredict the required values. This process is generally referred to as **model\
    \ inference** or **model scoring**."
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，生成的模型将应用于新数据以预测所需的值。这个过程通常被称为**模型推断**或**模型评分**。
- en: Tip
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Instead of training a single model, it is a best practice to train multiple
    models using various different model parameters called **hyperparameters** and
    select the best model among all the trained models based on well-defined **accuracy
    metrics**. This process of training multiple models based on different parameters
    is generally referred to as hyperparameter **tuning** or **cross-validation**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与其训练单一模型，最佳做法是使用不同的模型参数（称为**超参数**）训练多个模型，并根据定义明确的**准确性指标**从所有训练过的模型中选择最佳模型。这个基于不同参数训练多个模型的过程通常被称为超参数**调优**或**交叉验证**。
- en: Examples of ML algorithms include classification, regression, clustering, collaborative
    filtering, and dimensionality reduction.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的示例包括分类、回归、聚类、协同过滤和降维。
- en: Types of ML algorithms
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习算法的类型
- en: ML algorithms can be classified into three broad categories, namely, supervised
    learning, unsupervised learning, and reinforcement learning, as discussed in the
    following sections.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以分为三大类，即监督学习、无监督学习和强化学习，以下部分将详细讨论这些内容。
- en: Supervised learning
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习
- en: '**Supervised learning** is a type of ML where a model is trained on a dataset
    with a known value called the **label**. This label is tagged in the training
    dataset and represents the correct answer to the problem we are trying to solve.
    Our intention with supervised learning is to predict the label in an unknown dataset
    once the model is trained on a known dataset with the tagged label.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**是一种机器学习方法，其中模型在已知标签的训练数据集上进行训练。该标签是在训练数据集中标记的，并代表我们尝试解决问题的正确答案。我们进行监督学习的目的是在模型在已知数据集上经过训练后，预测未知数据集中的标签。'
- en: Examples of supervised learning algorithms include Linear Regression, Logistic
    Regression, Naive Bayes Classifiers, K-Nearest Neighbor, Decision Trees, Random
    Forest, Gradient Boosted Trees, and Support Vector Machine.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法的示例包括线性回归、逻辑回归、朴素贝叶斯分类器、K近邻、决策树、随机森林、梯度提升树和支持向量机。
- en: Supervised learning can be divided into two main classes, namely, regression
    and classification problems. While regression deals with predicting an unknown
    label, classification tries to classify the training dataset into known categories.
    Detailed implementation of supervised learning using **Apache Spark** **MLlib**
    will be introduced in [*Chapter 6*](B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107),
    *Supervised Learning*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习可以分为两大类，即回归问题和分类问题。回归问题涉及预测一个未知标签，而分类问题则尝试将训练数据集分类到已知类别中。使用**Apache Spark**
    **MLlib**进行监督学习的详细实现将在[*第六章*](B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107)，“*监督学习*”中介绍。
- en: Unsupervised learning
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: '**Unsupervised learning** is a type of ML where the training data is unknown
    to the algorithm and is not already labeled with a correct answer. Unsupervised
    learning involves learning the structure of an unknown, unlabeled dataset, without
    any guidance from the user. Here, the task of the machine is to group data into
    cohorts or groups according to certain similarities or differences without any
    prior training.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习**是机器学习的一种类型，其中训练数据对算法是未知的，且没有提前标注正确答案。无监督学习涉及学习一个未知、未标注的数据集的结构，没有用户的任何指导。在这种情况下，机器的任务是根据某些相似性或差异将数据分组或归类，而无需任何预先的训练。'
- en: Unsupervised learning can be further divided into **clustering** and **association**
    problems. Clustering deals with discovering cohorts within the training dataset,
    while association deals with discovering rules within the data that describe the
    relationship between entities. Examples of unsupervised learning include K-means
    clustering and collaborative filtering. Unsupervised learning will be explored
    at length with coding examples using Apache Spark MLlib in [*Chapter 7*](B16736_07_Final_JM_ePub.xhtml#_idTextAnchor128),
    *Unsupervised Machine Learning*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习可以进一步分为**聚类**和**关联**问题。聚类问题涉及发现训练数据集中的类别，而关联问题则涉及发现数据中描述实体关系的规则。无监督学习的例子包括K-means聚类和协同过滤。无监督学习将在[*第7章*](B16736_07_Final_JM_ePub.xhtml#_idTextAnchor128)中详细探讨，*无监督机器学习*，并通过Apache
    Spark MLlib提供编码示例。
- en: Reinforcement learning
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习
- en: '**Reinforcement learning** is employed by software systems and machines to
    find the best possible behavior or path that should be taken in a given situation.
    Unlike supervised learning, which already holds the correct answer within the
    training dataset, in reinforcement learning, there is no correct answer, but the
    reinforcement agent employs trial and error to decide the outcome and is designed
    to learn from experience. The reinforcement agent is either rewarded or penalized
    depending on the path chosen and the goal here is to maximize the reward.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**被软件系统和机器用于在给定情境下找到最优的行为或路径。与已经在训练数据集中包含正确答案的监督学习不同，在强化学习中没有固定答案，强化学习代理通过反复试验来决定结果，并且旨在从经验中学习。强化学习代理会根据选择的路径获得奖励或受到惩罚，目标是最大化奖励。'
- en: Reinforcement learning is used in applications such as self-driving cars, robotics,
    industrial automation, and natural language processing for chatbot agents. There
    aren't any out-of-the-box implementations of reinforcement learning within Apache
    Spark MLlib, so further exploration of this concept is beyond the scope of this
    book.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习应用于自驾车、机器人技术、工业自动化以及用于聊天机器人代理的自然语言处理等领域。Apache Spark MLlib中没有现成的强化学习实现，因此深入探讨这一概念超出了本书的范围。
- en: Note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Another branch of data science and ML is **deep learning**, which leverages
    advanced techniques for ML, such as neural networks, which are also becoming very
    prominent these days. Although Apache Spark does support certain deep learning
    algorithms, these concepts are too advanced to be included within the scope of
    this book.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学和机器学习的另一个分支是**深度学习**，它利用了先进的机器学习技术，如神经网络，这些技术近年来也变得非常突出。虽然Apache Spark确实支持某些深度学习算法，但这些概念过于先进，无法在本书的范围内涉及。
- en: Business use cases of ML
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习的业务应用案例
- en: So far, we have discussed various categories of ML and briefly introduced you
    to the tasks the ML models can perform. In this section, you will learn about
    some real-life applications of ML algorithms that help solve actual business problems
    across various industry verticals.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了机器学习的不同类别，并简要介绍了机器学习模型可以执行的任务。在本节中，您将了解一些机器学习算法在现实生活中如何帮助解决不同行业的实际商业问题。
- en: Customer churn prevention
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户流失预防
- en: Building customer churn models using ML can be very useful in identifying all
    those customers who are likely to stop engaging with your business and also help
    you gain insight into the factors that might lead them to churn. A churn model
    can simply be a regression model that estimates the risk score of each individual.
    Customer churn models can be very useful in identifying customers at risk of churning,
    thereby allowing businesses to implement strategies for customer retention.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习建立客户流失模型对于识别那些可能停止与您的业务互动的客户非常有用，并且还可以帮助您深入了解导致客户流失的因素。流失模型可以简单地是一个回归模型，用于估算每个个体的风险评分。客户流失模型可以帮助企业识别面临流失风险的客户，从而实施客户保持策略。
- en: Customer lifetime value modeling
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户终生价值建模
- en: Retail businesses generate a huge share of their revenue from a small cohort
    of high-value customers who provide repeat business. Customer lifetime value models
    can estimate a customer's lifetime, a period after which they might churn. They
    can also predict the total revenue that a customer would probably generate over
    their lifetime. Thus, estimating the revenue that a potential high-value customer
    might bring over their lifetime could be essential in redirecting marketing dollars
    to attracting and retaining such customers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 零售企业的收入大部分来自少数高价值客户，这些客户带来了重复购买。客户终生价值模型可以估算一个客户的生命周期，即客户可能流失的时期。它们还可以预测一个客户在其生命周期内可能带来的总收入。因此，估算潜在高价值客户在其生命周期内可能带来的收入，对于重新分配营销资金来吸引和留住这些客户至关重要。
- en: Demand forecasting
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需求预测
- en: Brick and mortar businesses, as well as online businesses, have limited storage
    space within their actual stores as well as at the warehouse. Hence, it is important
    to properly stock the available storage space with products that will actually
    be in demand. You could develop a simple model based on seasonality and the month
    of the year. However, building a sophisticated ML model that includes not just
    seasonality and historical data, but external data such as current trends on social
    media, weather forecast data, and customer sentiment on social media, could lead
    to better forecasting of demand and help maximize revenues as a result.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实体店和在线商店都有有限的实际存储空间，无论是在商店内还是在仓库里。因此，如何将这些有限的存储空间填充上实际需求的产品是非常重要的。您可以基于季节性和每年的月份开发一个简单的模型。然而，建立一个更复杂的机器学习模型，不仅包括季节性和历史数据，还包括外部数据，例如社交媒体上的当前趋势、天气预报数据和社交媒体上的客户情绪，可能会导致更准确的需求预测，并因此帮助最大化收入。
- en: Shipment lead-time prediction
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运输交货时间预测
- en: Any business that involves delivery and logistics operations, whether it be
    an online retailer or a food delivery aggregator, needs to be able to estimate
    the amount of time it would take for the order to reach the customer. Often, this
    shipment lead time is an essential decision-making factor by the customer in terms
    of doing business with you versus moving on to a competitor. Regression models
    can be used to accurately estimate the amount of time required to deliver the
    product to the customer's zip code, based on factors such as product origin and
    destination locations, weather, and other seasonal data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 任何涉及配送和物流操作的企业，无论是在线零售商还是食品配送平台，都需要能够估算订单送达客户所需的时间。通常，运输的交货时间是客户在选择是否与您合作时做决策的一个关键因素，客户可能会因此选择与竞争对手合作。回归模型可以根据产品的起始地和目的地、天气及其他季节性数据准确估算产品交付到客户邮政编码所需的时间。
- en: Market basket analysis
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 市场篮分析
- en: Market basket analysis is a technique for making product recommendations to
    customers based on items already in their basket. ML can be used to discover association
    rules within product categories by leveraging the collaborative filtering algorithm
    in order to make product recommendations to online customers based on the items
    already in their cart and their past purchases. This is a prominent use case used
    by pretty much every e-tailer.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 市场篮分析是一种根据客户购物篮中已有的商品向其推荐其他产品的技术。通过利用协同过滤算法，机器学习（ML）可以发现产品类别之间的关联规则，从而根据客户购物车中的商品和过去的购买记录向在线客户推荐产品。这是几乎所有电子零售商常用的一个重要应用。
- en: Financial fraud detection
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 财务欺诈检测
- en: ML has the inherent capability to detect patterns within data. Thus, ML can
    be leveraged to build models that can detect anomalies across financial transactions
    to flag certain transactions as being fraudulent. Traditionally, financial institutions
    have already been leveraging rule-based models for fraud detection, but incorporating
    ML models into the mix makes the fraud models even more potent, thereby helping
    to detect novel fraud patterns.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习具备从数据中检测模式的固有能力。因此，可以利用机器学习构建能够检测财务交易异常的模型，以标记某些交易为欺诈行为。传统上，金融机构已经在利用基于规则的模型进行欺诈检测，但将机器学习模型纳入其中会使欺诈检测模型更加有效，从而帮助发现新的欺诈模式。
- en: Information extraction using natural language processing
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自然语言处理进行信息提取
- en: Pharma companies and businesses that generate a large corpus of knowledge are
    faced with a unique challenge specific to their industry. Trying to identify whether
    a certain piece of knowledge has already been created by another group within
    the vast organization is not a straightforward problem at organizations with tens
    of thousands of employees. ML's natural language processing techniques can be
    applied to sort, group, classify, and label a large corpus of documents so that
    users can easily search if a similar piece of knowledge already exists.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 制药公司和产生大量知识的企业面临着一个特定于其行业的独特挑战。在拥有数万名员工的组织中，尝试识别某个知识点是否已经由另一个小组创建，并非一件简单的事情。机器学习的自然语言处理技术可以用来整理、分组、分类和标注大量文档，从而使用户能够轻松搜索是否已有类似的知识点存在。
- en: So far, you have explored the basics of ML, the different types of ML algorithms,
    and their applications in real-world business use cases. In the following section,
    we will discuss the need for scalable ML and a few techniques for scaling our
    ML algorithms, and get an introduction to Apache Spark's native, scalable ML library
    called **MLlib** and its application for performing data wrangling.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了机器学习的基本知识、不同类型的机器学习算法以及它们在实际商业案例中的应用。在接下来的部分中，我们将讨论可扩展机器学习的需求，以及扩展机器学习算法的一些技术，并介绍Apache
    Spark的原生可扩展机器学习库**MLlib**及其在数据整理中的应用。
- en: Scaling out machine learning
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展机器学习
- en: In the previous sections, we learned that ML is a set of algorithms that, instead
    of being explicitly programmed, automatically learn patterns hidden within data.
    Thus, an ML algorithm exposed to a larger dataset can potentially result in a
    better-performing model. However, traditional ML algorithms were designed to be
    trained on a limited data sample and on a single machine at a time. This means
    that the existing ML libraries are not inherently scalable. One solution to this
    problem is to down-sample a larger dataset to fit in the memory of a single machine,
    but this also potentially means that the resulting models aren't as accurate as
    they could be.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们了解到，机器学习是一套算法，而不是显式编程，它能够自动学习数据中隐藏的模式。因此，暴露给更大数据集的机器学习算法，可能会导致更好的模型表现。然而，传统的机器学习算法设计是基于有限的数据样本并在单台机器上训练的。这意味着现有的机器学习库本身并不具备扩展性。解决这一问题的一个方法是将较大的数据集下采样，以适应单台机器的内存，但这也可能意味着最终得到的模型不如可能的最优模型准确。
- en: Also, typically, several ML models are built on the same dataset, simply varying
    the parameters supplied to the algorithm. Out of these several models, the best
    model is chosen for production purposes, using a technique called **hyperparameter
    tuning**. Building several models using a single machine, one model after another,
    in a linear manner takes a very long time to arrive at the best possible model,
    leading to a longer time to production and, hence, a longer time to market.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通常会在同一数据集上构建多个机器学习模型，仅仅是改变提供给算法的参数。在这些模型中，选择最优的模型用于生产环境，这个过程叫做**超参数调优**。在单台机器上按顺序构建多个模型，需要很长时间才能得到最佳模型，这导致生产周期更长，从而也增加了推向市场的时间。
- en: Given these scalability challenges with traditional ML algorithms, there is
    a need to either scale out existing ML algorithms or new scalable ML algorithms.
    We will explore some techniques for scaling out ML algorithms in the following
    section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于传统机器学习算法的可扩展性挑战，迫切需要扩展现有的机器学习算法或开发新的可扩展机器学习算法。我们将在接下来的部分中探索一些扩展机器学习算法的技术。
- en: Techniques for scaling ML
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展机器学习的技术
- en: Two of the prominent techniques for scaling out ML algorithms are described
    in the following sections.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将介绍两种扩展机器学习算法的主要技术。
- en: Embarrassingly parallel processing
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 令人尴尬的并行处理
- en: '**Embarrassingly parallel processing** is a type of parallel computing technique
    where little to no effort is required to split a given computational problem into
    smaller parallel tasks. This is possible when the parallelized tasks do not have
    any interdependencies, and all tasks can execute completely independently of one
    another.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**令人尴尬的并行处理**是一种并行计算技术，在这种技术中，几乎不需要任何努力就可以将给定的计算问题分解成较小的并行任务。当并行化的任务之间没有任何相互依赖，且所有任务都可以完全独立执行时，这种方式就可以实现。'
- en: Now, let's try to apply this to the problem of scaling out single machine ML
    algorithms on very large datasets, and at the outset, this doesn't seem like a
    simple task at all. However, consider the problem of hyperparameter tuning or
    cross-validation. Here, we can run multiple parallel models, each with different
    model parameters, but on the same smaller dataset that can fit into a single machine's
    memory. Here, we can easily train multiple models on the same dataset by varying
    the model parameters. Thus, by leveraging the embarrassingly parallel processing
    technique, we can accelerate our model-building process by several orders of magnitude,
    helping us get to the best possible model within hours instead of several weeks
    or even months, and thereby accelerating your business time to value. You will
    learn more about applying this technique to scale out single-node Python ML libraries
    using Apache Spark in [*Chapter 10*](B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176)*,
    Scaling Out Single-Node Machine Learning Using PySpark*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将这个方法应用到在非常大的数据集上扩展单机机器学习算法的问题上，乍一看，这似乎不是一个简单的任务。然而，考虑到超参数调优或交叉验证的问题，我们可以运行多个并行模型，每个模型都有不同的参数，但它们都可以在一个单机的内存中处理相同的小数据集。在这种情况下，我们可以通过调整模型参数轻松地在相同的数据集上训练多个模型。因此，通过利用令人尴尬的并行处理技术，我们可以将模型构建过程加速数个数量级，帮助我们在数小时内而非数周或数月内找到最优的模型，从而加速你的业务价值实现。你将进一步了解如何在[*第10章*](B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176)*中应用这一技术，使用PySpark扩展单节点机器学习*。
- en: Scalable ML algorithms
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可扩展的机器学习算法
- en: While the embarrassingly parallel computing technique helps us get better models
    with greater accuracy in a faster time, it is still limited by smaller dataset
    sizes. This means that we might still be missing out on potential patterns of
    data because of down-sampling. To overcome this, we need ML algorithms that can
    inherently scale out across multiple machines and can train on a very large dataset
    in a distributed manner. Apache Spark's native ML library, called MLlib, consists
    of such inherently scalable ML algorithms, and we will explore MLlib further in
    the following section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管令人尴尬的并行计算技术帮助我们在更短时间内得到更好的模型，并提高了准确性，但它仍然受限于较小的数据集大小。这意味着我们可能会因为数据的下采样而错失潜在的数据模式。为了解决这个问题，我们需要能够天然扩展至多个机器的机器学习算法，并能够在分布式的方式下训练非常大的数据集。Apache
    Spark的原生ML库，名为MLlib，包含了这些本质上可扩展的机器学习算法，接下来的部分我们将进一步探讨MLlib。
- en: Introduction to Apache Spark's ML library
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark的ML库简介
- en: MLlib is Apache Spark's native ML library. Being a native library, MLlib has
    tight integration with the rest of Spark's APIs and libraries, including Spark
    SQL Engine, DataFrame APIs, Spark SQL API, and even Structured Streaming. This
    gives Apache Spark the unique advantage of being a truly unified data analytics
    platform that can perform all tasks pertaining to data analytics, starting from
    data ingestion to data transformation, the ad hoc analysis of data, building sophisticated
    ML models, and even leveraging those models for production use cases. In the following
    section, you will explore more of Spark MLlib and its core components.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是Apache Spark的原生机器学习库。作为一个原生库，MLlib与Spark的其他API和库紧密集成，包括Spark SQL引擎、DataFrame
    API、Spark SQL API，甚至结构化流处理。这个特性使得Apache Spark成为一个真正统一的数据分析平台，可以执行所有与数据分析相关的任务，从数据摄取到数据转换，再到数据的临时分析、构建复杂的机器学习模型，甚至将这些模型应用于生产环境中。在接下来的部分，你将更深入地了解Spark
    MLlib及其核心组件。
- en: Spark MLlib overview
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark MLlib概览
- en: In the early versions of Apache Spark, MLlib was based on Spark's RDD API. Starting
    with Spark version 2.0, a new ML library based on DataFrame APIs was introduced.
    Now, in Spark 3.0 and versions above this, the DataFrame API-based MLlib is standard,
    while the older RDD-based MLlib is in maintenance mode with no future enhancements
    planned.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Spark 的早期版本中，MLlib 基于 Spark 的 RDD API。自 Spark 2.0 版本开始，推出了一个基于 DataFrame
    API 的新 ML 库。现在，在 Spark 3.0 及更高版本中，基于 DataFrame API 的 MLlib 是标准，而旧的基于 RDD 的 MLlib
    处于维护模式，未来不会再进行扩展。
- en: The DataFrame-based MLlib closely mimics traditional single-machine Python-based
    ML libraries such as scikit-learn and consists of three major components, called
    transformers, estimators, and pipelines, as described in the following sections.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 DataFrame 的 MLlib 与传统的基于 Python 的单机 ML 库（如 scikit-learn）高度相似，包含三个主要组件，分别是转换器、估算器和管道，具体内容将在接下来的章节中介绍。
- en: Transformers
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换器
- en: 'A **transformer** is an algorithm that takes a DataFrame as input, performs
    processing on the DataFrame columns, and returns another DataFrame. An ML model
    trained using Spark MLlib is a transformer that takes a raw DataFrame and returns
    another DataFrame with the original raw data along with the new prediction columns.
    A typical transformer pipeline is shown in the following diagram:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换器** 是一种算法，它接受一个 DataFrame 作为输入，对 DataFrame 列进行处理，并返回另一个 DataFrame。使用 Spark
    MLlib 训练的 ML 模型是一个转换器，它接受一个原始 DataFrame，并返回一个包含原始数据和新预测列的 DataFrame。典型的转换器管道如下图所示：'
- en: '![Figure 5.1 – A transformer pipeline'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1 – 一个转换器管道'
- en: '](img/B16736_05_01.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_05_01.jpg)'
- en: Figure 5.1 – A transformer pipeline
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 一个转换器管道
- en: In the previous diagram, a typical transformer pipeline is depicted, where a
    series of transformer stages, including a **VectorIndexer** and an already trained
    **Linear Regression Model**, are applied to the raw DataFrame. The result is a
    new DataFrame with all the original columns, along with some new columns containing
    predicted values.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，展示了一个典型的转换器管道，其中一系列的转换器阶段，包括 **VectorIndexer** 和已训练的 **线性回归模型**，被应用到原始的
    DataFrame。结果是一个新的 DataFrame，包含了所有原始列，并新增了包含预测值的新列。
- en: Note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: The transformation DataFrame operation is a different concept compared to transformers
    within Spark's MLlib. While both transform one DataFrame into another DataFrame
    and are lazily evaluated, the former is an operation performed on a DataFrame,
    while the latter is an actual ML algorithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 转换操作与 Spark 的 MLlib 中的转换器是不同的概念。虽然两者都将一个 DataFrame 转换为另一个 DataFrame，并且都是惰性计算，但前者是对
    DataFrame 执行的操作，而后者则是一个实际的 ML 算法。
- en: Estimators
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 估算器
- en: 'An **estimator** is another algorithm that accepts a DataFrame as input and
    results in a transformer. Any ML algorithm is an estimator in that it transforms
    a DataFrame with raw data into a DataFrame with actual predictions. An estimator
    pipeline is depicted in the following diagram:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**估算器**是另一种算法，它接受一个 DataFrame 作为输入，并生成一个转换器。任何 ML 算法都是一个估算器，因为它将包含原始数据的 DataFrame
    转换为包含实际预测结果的 DataFrame。估算器管道在下图中描述：'
- en: '![Figure 5.2 – An estimator pipeline'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.2 – 一个估算器管道'
- en: '](img/B16736_05_02.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_05_02.jpg)'
- en: Figure 5.2 – An estimator pipeline
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 一个估算器管道
- en: In the preceding diagram, a **Transformer** is first applied to a DataFrame
    with raw data to result in a **Feature Vector** DataFrame. An **Estimator** in
    the form of a **Linear Regression** **Algorithm** is then applied to the DataFrame
    containing **Feature Vectors** to result in a **Transformer** in the form of a
    newly trained **Linear Regression Model**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，首先将 **Transformer** 应用于一个包含原始数据的 DataFrame，生成一个 **特征向量** DataFrame。然后，将
    **估算器**（以 **线性回归** **算法** 的形式）应用于包含 **特征向量** 的 DataFrame，生成一个新的 **线性回归模型**，该模型作为
    **Transformer** 返回。
- en: Note
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: A feature vector is a special data structure within the Spark MLlib library.
    It is a DataFrame column consisting of actual vector objects of the floating-point
    type. Since ML is based on mathematics and statistics, all ML algorithms exclusively
    operate on vectors of floating-point values. Raw data is converted into feature
    vectors using feature extraction and feature engineering techniques.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量是 Spark MLlib 库中的一种特殊数据结构。它是一个 DataFrame 列，包含实际的浮动点类型向量对象。由于 ML 基于数学和统计学，所有
    ML 算法只对浮动点值的向量进行操作。原始数据通过特征提取和特征工程技术转换为特征向量。
- en: Pipelines
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管道
- en: An ML pipeline within Spark MLlib chains together several stages of transformers
    and estimators into a DAG that performs an end-to-end ML operation ranging from
    data cleansing, to feature engineering, to actual model training. A pipeline could
    be a transformer-only pipeline or an estimator-only pipeline or a mix of the two.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib 中的 ML 管道将多个转换器和估算器的阶段链在一起，形成一个执行端到端 ML 操作的有向无环图（DAG），该操作从数据清理到特征工程，再到实际的模型训练。一个管道可以是仅包含转换器的管道，也可以是仅包含估算器的管道，或者两者的结合。
- en: Using the available transformers and estimators within Spark MLlib, an entire
    end-to-end ML pipeline can be constructed. A typical ML pipeline consists of several
    stages, starting with data wrangling, feature engineering, model training, and
    model inferencing. You will learn more about data wrangling techniques in the
    following section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark MLlib 中的可用转换器和估算器，可以构建一个完整的端到端机器学习（ML）管道。一个典型的 ML 管道由多个阶段组成，从数据清理、特征工程、模型训练到模型推理。你将在接下来的章节中学习更多关于数据清理的技术。
- en: Data wrangling with Apache Spark and MLlib
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 和 MLlib 进行数据清理
- en: '**Data wrangling**, also referred to within the data science community as **data
    munging**, or simply **data preparation**, is the first step in a typical data
    science process. Data wrangling involves sampling, exploring, selecting, manipulating,
    and cleansing data to make it ready for ML applications. Data wrangling takes
    up to 60 to 80 percent of the whole data science process and is the most crucial
    step in guaranteeing the accuracy of the ML model being built. The following sections
    explore the data wrangling process using Apache Spark and MLlib.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据清理**，在数据科学社区中也称为 **数据清洗** 或简而言之 **数据准备**，是典型数据科学过程中的第一步。数据清理涉及采样、探索、选择、操作和清洗数据，以使其准备好进行机器学习应用。数据清理占整个数据科学过程的
    60% 到 80%，是确保构建的 ML 模型准确性的最关键步骤。接下来的章节将使用 Apache Spark 和 MLlib 探讨数据清理过程。'
- en: Data preprocessing
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Data preprocessing is the first step in the data wrangling process and involves
    gathering, exploring, and selecting the data elements useful for solving the problem
    at hand. The data science process typically succeeds the data engineering process
    and the assumption here is that clean and integrated data is already available
    in the data lake. However, data that is clean enough for BI may not be good enough
    for data science. Also, data science applications require additional datasets,
    which may not be useful for other analytics use cases, and hence, may not yet
    be clean.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理是数据清理过程中的第一步，涉及收集、探索和选择对于解决当前问题有用的数据元素。数据科学过程通常继承数据工程过程，假设数据湖中已经存在干净和集成的数据。然而，足够干净的数据可能对于商业智能（BI）来说是合适的，但可能并不适合数据科学应用。同时，数据科学应用需要额外的数据集，这些数据集可能对其他分析用例无用，因此可能尚未清理。
- en: 'Before we start manipulating and wrangling our data, we need to load it into
    a Spark DataFrame and explore the data to gain some understanding of its structure.
    The following code example will make use of the integrated dataset produced toward
    the end of [*Chapter 3*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056), *Data
    Cleansing and Integration*, named `retail_silver.delta`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始操作和清理数据之前，我们需要将其加载到 Spark DataFrame 中，并探索数据以了解其结构。以下代码示例将使用在 [*第三章*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056)《数据清理与集成》末尾产生的集成数据集，名为
    `retail_silver.delta`：
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding code snippet, we perform the following operations:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们执行了以下操作：
- en: We load data from the data lake into a Spark DataFrame using the `spark.read()`
    function.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `spark.read()` 函数将数据从数据湖加载到 Spark DataFrame 中。
- en: We print its schema to check the data types of the columns using the `printSchema()`
    function.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `printSchema()` 函数打印其架构，以检查列的数据类型。
- en: We display a few columns from the DataFrame to check their values using the
    `select()` operation.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `select()` 操作显示 DataFrame 中的几个列，以检查它们的值。
- en: We generate basic statistics on the DataFrame using the `describe()` operation.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `describe()` 操作生成 DataFrame 的基本统计信息。
- en: Data cleansing
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清理
- en: 'In the code example in the previous section, you must have noticed that most
    data types are just strings types. The dataset might also contain duplicates,
    and there are also `NULL` values in the data. Let''s fix these inconsistencies
    in the dataset, as shown in the following code snippet:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节的代码示例中，你应该注意到大多数数据类型只是字符串类型。数据集可能还包含重复项，并且数据中也可能存在 `NULL` 值。让我们解决数据集中的这些不一致性，如下所示的代码片段所示：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code snippet, we perform the following operations:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，我们执行了以下操作：
- en: We de-duplicate data using the `dropduplicates()` operation using the key columns.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`dropduplicates()`操作通过键列去重数据。
- en: We then cast datetime columns into the proper timestamp type, using the `to_timestamp()`
    function, by supplying the correct format of the timestamp.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用`to_timestamp()`函数将datetime列转换为正确的时间戳类型，通过提供正确的时间戳格式。
- en: We change the data types of the DataFrame using the `CAST()` method.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`CAST()`方法更改DataFrame的数据类型。
- en: We replace missing values and `NULL` values with `0` using the `na.fill()` operation.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`na.fill()`操作将缺失值和`NULL`值替换为`0`。
- en: This section showed you how to perform basic data cleansing at scale using PySpark.
    The following section will show you how to manipulate data steps such as filtering
    and renaming.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何使用PySpark进行大规模的数据清洗。下一节将展示如何执行数据处理步骤，如过滤和重命名。
- en: Data manipulation
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据操作
- en: 'Once you have a cleaner dataset, you can perform operations to filter out any
    data not required by your use case, rename columns to follow your naming conventions,
    and drop any unwanted data columns, as shown in the following code block:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了更干净的数据集，你可以执行操作来过滤掉任何不需要的数据、重命名列以遵循你的命名约定，并删除任何不需要的数据列，如下代码块所示：
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the preceding code snippet, we perform the following operations:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，我们执行了以下操作：
- en: We filter, slice, and dice data using the `where()` function.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`where()`函数过滤、切片和处理数据。
- en: We rename columns using the `withColumnsRenamed()` function and drop unwanted
    columns using the `drop()` function.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`withColumnsRenamed()`函数重命名列，并使用`drop()`函数删除不需要的列。
- en: We convert the Spark DataFrame to a PySpark DataFrame using the `toPandas()`
    function.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`toPandas()`函数将Spark DataFrame转换为PySpark DataFrame。
- en: Sometimes, there is an ML algorithm that's not available in Spark MLlib, or
    there's a custom algorithm built using single-node Python libraries. For these
    use cases, you can convert your Spark DataFrame into a pandas DataFrame, as shown
    in *step 3* previously.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，Spark MLlib中没有可用的ML算法，或者有一个使用单节点Python库构建的自定义算法。对于这些用例，你可以将Spark DataFrame转换为pandas
    DataFrame，如前面的*步骤3*所示。
- en: Note
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Converting a Spark DataFrame to a pandas Dataframe involves collecting all the
    data from the Executors onto the Spark driver. Thus, care needs to be taken that
    this conversion is only applied to smaller datasets, otherwise this could lead
    to an `OutOfMemory` error on the Driver node.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将Spark DataFrame转换为pandas DataFrame涉及将所有数据从Executor收集到Spark驱动程序。因此，需要注意此转换仅应用于较小的数据集，否则可能会导致驱动节点的`OutOfMemory`错误。
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about the concept of ML and the different types
    of ML algorithms. You also learned about some of the real-world applications of
    ML to help businesses minimize losses and maximize revenues and accelerate their
    time to market. You were introduced to the necessity of scalable ML and two different
    techniques for scaling out ML algorithms. Apache Spark's native ML Library, MLlib,
    was introduced, along with its major components.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了机器学习（ML）的概念和不同类型的ML算法。你还学习了ML在现实世界中的一些应用，帮助企业减少损失、最大化收入并加速上市时间。你还了解了可扩展ML的必要性，以及两种不同的技术来扩展ML算法。介绍了Apache
    Spark的本地ML库MLlib及其主要组件。
- en: Finally, you learned a few techniques to perform data wrangling to clean, manipulate,
    and transform data to make it more suitable for the data science process. In the
    following chapter, you will learn about the send phase of the ML process, called
    feature extraction and feature engineering, where you will learn to apply various
    scalable algorithms to transform individual data fields to make them even more
    suitable for data science applications.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学习了一些执行数据清理、处理和转换的技术，使数据更适合数据科学流程。在接下来的章节中，你将学习机器学习（ML）流程的发送阶段，称为特征提取和特征工程，你将学习如何应用各种可扩展的算法来转换单个数据字段，使其更适合数据科学应用。
