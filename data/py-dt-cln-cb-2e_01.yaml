- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Anticipating Data Cleaning Issues When Importing Tabular Data with pandas
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预见导入表格数据时的清洗问题，使用pandas
- en: Scientific distributions of **Python** (Anaconda, WinPython, Canopy, and so
    on) provide analysts with an impressive range of data manipulation, exploration,
    and visualization tools. One important tool is pandas. Developed by Wes McKinney
    in 2008, but really gaining in popularity after 2012, pandas is now an essential
    library for data analysis in Python. The recipes in this book demonstrate how
    many common data preparation tasks can be done more easily with pandas than with
    other tools. While we work with pandas extensively in this book, we also use other
    popular packages such as Numpy, matplotlib, and scipy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 科学版的**Python**（如Anaconda、WinPython、Canopy等）为分析师提供了广泛的数据处理、探索和可视化工具。其中一个重要的工具是pandas。pandas由Wes
    McKinney于2008年开发，但真正获得广泛关注是在2012年之后，它如今已成为Python数据分析中的必备库。本书中的实例展示了许多常见的数据准备任务如何通过pandas比其他工具更加轻松地完成。虽然我们在本书中广泛使用pandas，但也使用了其他流行的软件包，如Numpy、matplotlib和scipy。
- en: A key pandas object is the **DataFrame**, which represents data as a tabular
    structure, with rows and columns. In this way, it is similar to the other data
    stores we discuss in this chapter. However, a pandas DataFrame also has indexing
    functionality that makes selecting, combining, and transforming data relatively
    straightforward, as the recipes in this book will demonstrate.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键的pandas对象是**DataFrame**，它将数据表示为一个表格结构，具有行和列。这样，它与我们在本章讨论的其他数据存储方式类似。然而，pandas的DataFrame还具有索引功能，使得选择、合并和转换数据相对简单，正如本书中的示例所展示的那样。
- en: 'Before we can make use of this great functionality, we have to import our data
    into pandas. Data comes to us in a wide variety of formats: as CSV or Excel files,
    as tables from SQL databases, from statistical analysis packages such as SPSS,
    Stata, SAS, or R, from non-tabular sources such as JSON, and from web pages.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以利用这个强大的功能之前，我们需要将数据导入到pandas中。数据以多种格式呈现给我们：作为CSV或Excel文件，从SQL数据库的表格中，来自统计分析软件包如SPSS、Stata、SAS或R，来自非表格来源如JSON，以及网页数据。
- en: 'We examine tools to import tabular data in this recipe. Specifically, we cover
    the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将介绍导入表格数据的工具。具体而言，我们将涵盖以下主题：
- en: Importing CSV files
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入CSV文件
- en: Importing Excel files
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入Excel文件
- en: Importing data from SQL databases
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从SQL数据库导入数据
- en: Importing SPSS, Stata, and SAS data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入SPSS、Stata和SAS数据
- en: Importing R data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入R数据
- en: Persisting tabular data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久化表格数据
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code and notebooks for this chapter are available on GitHub at [https://github.com/michaelbwalker/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/michaelbwalker/Python-Data-Cleaning-Cookbook-Second-Edition).
    You can use any **IDE** (**Integrated Development Environment**) of your choice
    – IDLE, Visual Studio, Sublime, Spyder, and so on – or Jupyter Notebook to work
    with any of the code in this chapter, or any chapter in this book. A good guide
    to get started with Jupyter Notebook can be found here: [https://www.dataquest.io/blog/jupyter-notebook-tutorial/](https://www.dataquest.io/blog/jupyter-notebook-tutorial/).
    I used the Spyder IDE to write the code in this chapter.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码和笔记本可以在GitHub上找到：[https://github.com/michaelbwalker/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/michaelbwalker/Python-Data-Cleaning-Cookbook-Second-Edition)。你可以使用任何你选择的**IDE**（**集成开发环境**）——如IDLE、Visual
    Studio、Sublime、Spyder等——或Jupyter Notebook来操作本章的代码，或本书中的任何一章。关于如何开始使用Jupyter Notebook的好指南可以在这里找到：[https://www.dataquest.io/blog/jupyter-notebook-tutorial/](https://www.dataquest.io/blog/jupyter-notebook-tutorial/)。我使用了Spyder
    IDE编写本章的代码。
- en: I used pandas 2.2.1 and NumPy version 1.24.3 for all of the code in this chapter
    and subsequent chapters. I have also tested all code with pandas 1.5.3.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章及后续章节的所有代码中使用了pandas 2.2.1和NumPy版本1.24.3。我也在pandas 1.5.3上测试了所有代码。
- en: Importing CSV files
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入CSV文件
- en: 'The `read_csv` method of the `pandas` library can be used to read a file with
    **comma separated values** (**CSV**) and load it into memory as a pandas DataFrame.
    In this recipe, we import a CSV file and address some common issues: creating
    column names that make sense to us, parsing dates, and dropping rows with critical
    missing data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`库的`read_csv`方法可以用来读取一个**逗号分隔值**（**CSV**）文件，并将其加载到内存中作为pandas的DataFrame。在本示例中，我们导入了一个CSV文件，并解决了一些常见问题：创建我们能理解的列名，解析日期，以及删除含有重要缺失数据的行。'
- en: Raw data is often stored as CSV files. These files have a carriage return at
    the end of each line of data to demarcate a row, and a comma between each data
    value to delineate columns. Something other than a comma can be used as the delimiter,
    such as a tab. Quotation marks may be placed around values, which can be helpful
    when the delimiter occurs naturally within certain values, which sometimes happens
    with commas.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据通常以 CSV 文件格式存储。这些文件在每行数据的末尾有一个回车符，用于区分每一行数据，并且数据值之间有逗号分隔。除了逗号外，也可以使用其他分隔符，比如制表符。值周围可能会有引号，尤其是当分隔符本身出现在某些值中时（比如逗号出现在值中）。
- en: All data in a CSV file are characters, regardless of the logical data type.
    This is why it is easy to view a CSV file, presuming it is not too large, in a
    text editor. The pandas `read_csv` method will make an educated guess about the
    data type of each column, but you will need to help it along to ensure that these
    guesses are on the mark.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件中的所有数据都是字符类型，无论其逻辑数据类型是什么。这就是为什么当 CSV 文件不太大的时候，很容易在文本编辑器中查看它。pandas 的`read_csv`方法会对每一列的数据类型进行推测，但你需要帮助它，以确保这些推测是准确的。
- en: Getting ready
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Create a folder for this chapter, and then create a new Python script or **Jupyter
    Notebook** file in that folder. Create a data subfolder, and then place the `landtempssample.csv`
    file in that subfolder. Alternatively, you could retrieve all of the files from
    the GitHub repository, including the data files. Here is a screenshot of the beginning
    of the CSV file:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为本章节创建一个文件夹，然后在该文件夹中创建一个新的 Python 脚本或**Jupyter Notebook**文件。创建一个数据子文件夹，然后将`landtempssample.csv`文件放入该子文件夹。或者，你也可以从
    GitHub 仓库中获取所有文件，包括数据文件。以下是 CSV 文件开头的截图：
- en: '![Screenshot from 2023-05-28 21-00-25](img/B18596_01_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![2023-05-28 21-00-25 的截图](img/B18596_01_01.png)'
- en: 'Figure 1.1: Land Temperatures Data'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：陆地温度数据
- en: '**Data note**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: This dataset, taken from the Global Historical Climatology Network integrated
    database, is made available for public use by the United States National Oceanic
    and Atmospheric Administration at [https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly).
    I used the data from version 4\. The data in this recipe uses a 100,000-row sample
    of the full dataset, which is also available in the repository.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自全球历史气候网络（Global Historical Climatology Network）集成数据库，由美国国家海洋和大气管理局（NOAA）在[https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly)提供公开使用。我使用的是第
    4 版的数据。本食谱中的数据使用了完整数据集中的 100,000 行样本，完整数据集也可以在仓库中找到。
- en: How to do it…
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will import a CSV file into pandas, taking advantage of some very useful
    `read_csv` options:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把 CSV 文件导入到 pandas 中，利用一些非常有用的`read_csv`选项：
- en: 'Import the `pandas` library, and set up the environment to make viewing the
    output easier:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`库，并设置环境以便更方便地查看输出：
- en: '[PRE0]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Read the data file, set new names for the headings, and parse the date column.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据文件，设置新的标题名称，并解析日期列。
- en: 'Pass an argument of `1` to the `skiprows` parameter to skip the first row,
    pass a list of columns to `parse_dates` to create a pandas datetime column from
    those columns, and set `low_memory` to `False`. This will cause pandas to load
    all of the data into memory at once, rather than in chunks. We do this so that
    pandas can identify the data type of each column automatically. In the *There’s
    more…* section, we see how to set the data type for each column manually:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将参数`1`传递给`skiprows`参数，以跳过第一行，将列的列表传递给`parse_dates`，以便从这些列创建一个 pandas 日期时间列，并将`low_memory`设置为`False`。这样，pandas
    会一次性将所有数据加载到内存中，而不是分块加载。我们这样做是为了让 pandas 自动识别每一列的数据类型。在*更多内容…*部分，我们会看到如何手动设置每列的数据类型：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Note**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**备注**'
- en: We have to use `skiprows` because we are passing a list of column names to `read_csv`.
    If we use the column names in the CSV file, we do not need to specify values for
    either `names` or `skiprows`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须使用`skiprows`，因为我们正在将列名列表传递给`read_csv`。如果我们使用 CSV 文件中的列名，则不需要为`names`或`skiprows`指定值。
- en: Get a quick glimpse of the data.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 快速浏览一下数据。
- en: 'View the first few rows. Show the data type for all columns, as well as the
    number of rows and columns:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 查看前几行。显示所有列的数据类型，以及行和列的数量：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Give the date column a more appropriate name and view the summary statistics
    for average monthly temperature:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给日期列起个更合适的名字，并查看平均月温的总结统计：
- en: '[PRE9]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Look for missing values for each column.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找每列的缺失值。
- en: 'Use `isnull`, which returns `True` for each value that is missing for each
    column, and `False` when not missing. Chain this with `sum` to count the missing
    values for each column. (When working with Boolean values, `sum` treats `True`
    as `1` and `False` as `0`. I will discuss method chaining in the *There’s more...*
    section of this recipe):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`isnull`，它会对每一列的缺失值返回`True`，对非缺失值返回`False`。将其与`sum`链式调用来计算每列的缺失值数量。（在处理布尔值时，`sum`将`True`视为`1`，将`False`视为`0`。我将在*后续内容...*部分讨论方法链式调用）：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Remove rows with missing data for `avgtemp`.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除缺失`avgtemp`数据的行。
- en: 'Use the `subset` parameter to tell `dropna` to drop rows when `avgtemp` is
    missing. Set `inplace` to `True`. Leaving `inplace` at its default value of `False`
    would display the DataFrame, but the changes we have made would not be retained.
    Use the `shape` attribute of the DataFrame to get the number of rows and columns:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`subset`参数告诉`dropna`在`avgtemp`缺失时删除行。将`inplace`设置为`True`。如果将`inplace`保持在默认值`False`，则会显示DataFrame，但我们所做的更改不会被保留。使用DataFrame的`shape`属性获取行数和列数：
- en: '[PRE15]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: That’s it! Importing CSV files into pandas is as simple as that.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！将CSV文件导入pandas就是这么简单。
- en: How it works...
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Almost all of the recipes in this book use the `pandas` library. We refer to
    it as `pd` to make it easier to reference later. This is customary. We also use
    `float_format` to display float values in a readable way and `set_option` to make
    the Terminal output wide enough to accommodate the number of variables.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的几乎所有食谱都使用`pandas`库。为了方便后续引用，我们将其称为`pd`。这是惯例。我们还使用`float_format`以可读的方式显示浮动值，并使用`set_option`使终端输出足够宽，以容纳所需的变量数量。
- en: Much of the work is done by the first line in *Step 2*. We use `read_csv` to
    load a pandas DataFrame in memory and call it `landtemps`. In addition to passing
    a filename, we set the `names` parameter to a list of our preferred column headings.
    We also tell `read_csv` to skip the first row, by setting `skiprows` to 1, since
    the original column headings are in the first row of the CSV file. If we do not
    tell it to skip the first row, `read_csv` will treat the header row in the file
    as actual data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分工作由*第2步*中的第一行完成。我们使用`read_csv`加载一个pandas DataFrame到内存中，并将其命名为`landtemps`。除了传递文件名外，我们将`names`参数设置为我们首选的列标题列表。我们还告诉`read_csv`跳过第一行，通过将`skiprows`设置为1，因为CSV文件的第一行包含了原始列标题。如果不告诉它跳过第一行，`read_csv`会将文件中的标题行当作实际数据来处理。
- en: '`read_csv` also solves a date conversion issue for us. We use the `parse_dates`
    parameter to ask it to convert the `month` and `year` columns to a date value.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv`还为我们解决了日期转换问题。我们使用`parse_dates`参数要求它将`month`和`year`列转换为日期值。'
- en: '*Step 3* runs through a few standard data checks. We use `head(7)` to print
    out all columns for the first seven rows. We use the `dtypes` attribute of the
    DataFrame to show the data type of all columns. Each column has the expected data
    type. In pandas, character data has the object data type, a data type that allows
    for mixed values. `shape` returns a tuple, whose first element is the number of
    rows in the DataFrame (100,000 in this case) and whose second element is the number
    of columns (9).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*第3步*进行了一些标准的数据检查。我们使用`head(7)`打印出前七行的所有列。我们使用DataFrame的`dtypes`属性显示所有列的数据类型。每列都具有预期的数据类型。在pandas中，字符数据具有对象数据类型，这是一个允许混合值的数据类型。`shape`返回一个元组，其第一个元素是DataFrame的行数（此例中为100,000），第二个元素是列数（9）。'
- en: When we used `read_csv` to parse the `month` and `year` columns, it gave the
    resulting column the name `month_year`. We used the `rename` method in *Step 4*
    to give that column a more appropriate name. We need to specify `inplace=True`
    to replace the old column name with the new column name in memory. The `describe`
    method provides summary statistics on the `avgtemp` column.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用`read_csv`解析`month`和`year`列时，它会将结果列命名为`month_year`。我们在*第4步*中使用`rename`方法为该列命名了更合适的名称。我们需要指定`inplace=True`，以便在内存中将旧列名替换为新列名。`describe`方法提供了`avgtemp`列的汇总统计信息。
- en: 'Notice that the count for `avgtemp` indicates that there are 85,554 rows that
    have valid values for `avgtemp`. This is out of 100,000 rows for the whole DataFrame,
    as provided by the `shape` attribute. The listing of missing values for each column
    in *Step 5* (`landtemps.isnull().sum()`) confirms this: *100,000 – 85,554 = 14,446*.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`avgtemp`的计数显示有85,554行具有有效的`avgtemp`值。这是在整个DataFrame的100,000行中，`shape`属性提供了这个信息。*第5步*中的缺失值列表（`landtemps.isnull().sum()`）确认了这一点：*100,000
    – 85,554 = 14,446*。
- en: '*Step 6* drops all rows where `avgtemp` is `NaN`. (The `NaN` value, not a number,
    is the pandas representation of missing values.) `subset` is used to indicate
    which column to check for missing values. The `shape` attribute for `landtemps`
    now indicates that there are 85,554 rows, which is what we would expect, given
    the previous count from `describe`.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*第6步*删除所有`avgtemp`值为`NaN`的行。(`NaN`值，即非数字，是pandas表示缺失值的方式。）`subset`用于指定检查缺失值的列。此时，`landtemps`的`shape`属性显示共有85,554行，这与通过`describe`获取的前一次计数一致。'
- en: There’s more...
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: If the file you are reading uses a delimiter other than a comma, such as a tab,
    this can be specified in the `sep` parameter of `read_csv`. When creating the
    pandas DataFrame, an index was also created. The numbers to the far left of the
    output when `head` was run are index values. Any number of rows can be specified
    for `head`. The default value is `5`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你读取的文件使用的是除逗号外的其他分隔符，例如制表符，可以在`read_csv`的`sep`参数中指定。当创建pandas DataFrame时，还会创建一个索引。在运行`head`时，输出最左侧的数字即为索引值。`head`可以指定任意数量的行，默认值是`5`。
- en: 'Instead of setting `low_memory` to `False`, to get pandas to make good guesses
    regarding data types, we could have set data types manually:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将`low_memory`设置为`False`，为了让pandas更好地猜测数据类型，我们可以手动设置数据类型：
- en: '[PRE17]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `landtemps.isnull().sum()` statement is an example of chaining methods.
    First, `isnull` returns a DataFrame of `True` and `False` values, resulting from
    testing whether each column value is `null`. The `sum` function takes that DataFrame
    and sums the `True` values for each column, interpreting the `True` values as
    `1` and the `False` values as `0`. We would have obtained the same result if we
    had used the following two steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`landtemps.isnull().sum()`语句是方法链的一个例子。首先，`isnull`返回一个`True`和`False`值组成的DataFrame，表示测试每一列值是否为`null`。`sum`函数对这个DataFrame进行求和，计算每列中`True`值的数量，`True`值按`1`计，`False`值按`0`计。如果我们使用以下两步操作，也能得到相同的结果：'
- en: '[PRE19]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There is no hard and fast rule for when to chain methods and when not to do
    so. I find chaining helpful when the overall operation feels like a single step,
    even if it’s two or more steps mechanically. Chaining also has the side benefit
    of not creating extra objects that I might not need.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 什么时候链式调用方法，什么时候不调用，并没有硬性规定。我发现，当整体操作看起来像是一个单一步骤时，即使它实际上是两个或多个步骤，从机械角度来看，链式调用是有帮助的。链式调用还有一个附带的好处，就是不会创建我可能不需要的额外对象。
- en: 'The dataset used in this recipe is just a sample from the full land temperatures
    database, with almost 17 million records. You can run the larger file if your
    machine can handle it, with the following code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱中使用的数据集仅为完整的土地温度数据库的一个样本，包含近1700万条记录。如果你的机器能够处理的话，可以运行更大的文件，使用以下代码：
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`read_csv` can read a compressed ZIP file. We get it to do this by passing
    the name of the ZIP file and the type of compression.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv`可以读取压缩的ZIP文件。我们通过传递ZIP文件的名称和压缩类型来实现这一功能。'
- en: See also
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: Subsequent recipes in this chapter, and in other chapters, set indexes to improve
    navigation over rows and merging.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以及其他章节的后续食谱设置了索引，以提高行和合并操作的导航效率。
- en: A significant amount of reshaping of the Global Historical Climatology Network
    raw data was done before using it in this recipe. We demonstrate this in *Chapter
    11*, *Tidying and Reshaping Data*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用全球历史气候网络原始数据之前，进行了大量的数据重塑。我们在*第11章*，*整理和重塑数据*中展示了这一过程。
- en: Importing Excel files
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入Excel文件
- en: 'The `read_excel` method of the `pandas` library can be used to import data
    from an Excel file and load it into memory as a pandas DataFrame. In this recipe,
    we import an Excel file and handle some common issues when working with Excel
    files: extraneous header and footer information, selecting specific columns, removing
    rows with no data, and connecting to particular sheets.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`库的`read_excel`方法可以用来从Excel文件中导入数据，并将其加载到内存中作为pandas DataFrame。在本食谱中，我们导入一个Excel文件，并处理一些常见问题，如多余的页眉和页脚信息、选择特定的列、删除没有数据的行以及连接到特定的工作表。'
- en: Despite the tabular structure of Excel, which invites the organization of data
    into rows and columns, spreadsheets are not datasets and do not require people
    to store data in that way. Even when some data conforms with those expectations,
    there is often additional information in rows or columns before or after the data
    to be imported. Data types are not always as clear as they are to the person who
    created the spreadsheet. This will be all too familiar to anyone who has ever
    battled with importing leading zeros. Moreover, Excel does not insist that all
    data in a column be of the same type, or that column headings be appropriate for
    use with a programming language such as Python.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Excel 的表格结构鼓励将数据组织成行和列，但电子表格并不是数据集，且不要求人们以这种方式存储数据。即使某些数据符合这些期望，通常在数据导入前后，行或列中还有其他信息。数据类型并不总是像创建电子表格的人所理解的那样清晰。这对于任何曾经与导入前导零作斗争的人来说，都是再熟悉不过的了。此外，Excel
    并不要求某一列中的所有数据类型相同，也不要求列标题适合用于像 Python 这样的编程语言。
- en: Fortunately, `read_excel` has a number of options for handling messiness in
    Excel data. These options make it relatively easy to skip rows, select particular
    columns, and pull data from a particular sheet or sheets.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`read_excel` 提供了多个选项，用于处理 Excel 数据中的杂乱问题。这些选项使得跳过行、选择特定列以及从特定工作表或多个工作表中提取数据变得相对简单。
- en: Getting ready
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You can download the `GDPpercapita22b.xlsx` file, as well as the code for this
    recipe, from the GitHub repository for this book. The code assumes that the Excel
    file is in a data subfolder. Here is a view of the beginning of the file (some
    columns were hidden for display purposes):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从本书的 GitHub 仓库下载 `GDPpercapita22b.xlsx` 文件，以及此配方的代码。代码假设 Excel 文件位于数据子文件夹中。以下是文件开头的视图（出于显示目的，某些列被隐藏）：
- en: '![](img/B18596_01_02.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18596_01_02.png)'
- en: 'Figure 1.2: View of the dataset'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：数据集视图
- en: 'And here is a view of the end of the file:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是文件末尾的视图：
- en: '![](img/B18596_01_03.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18596_01_03.png)'
- en: 'Figure 1.3: View of the dataset'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3：数据集视图
- en: '**Data note**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据注释**'
- en: This dataset, from the Organisation for Economic Co-operation and Development,
    is available for public use at [https://stats.oecd.org/](https://stats.oecd.org/).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自经济合作与发展组织（OECD），可在 [https://stats.oecd.org/](https://stats.oecd.org/)
    上公开获取。
- en: How to do it…
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We import an Excel file into pandas and do some initial data cleaning:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个 Excel 文件导入 pandas 并进行一些初步的数据清理：
- en: 'Import the `pandas` library:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 库：
- en: '[PRE21]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Read the Excel per capita GDP data.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读 Excel 人均 GDP 数据。
- en: 'Select the sheet with the data we need, but skip the columns and rows that
    we do not want. Use the `sheet_name` parameter to specify the sheet. Set `skiprows`
    to `4` and `skipfooter` to `1` to skip the first four rows (the first row is hidden)
    and the last row. We provide values for `usecols` to get data from column `A`
    and columns `C` through `W` (column `B` is blank). Use `head` to view the first
    few rows and `shape` to get the number of rows and columns:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 选择包含我们所需数据的工作表，但跳过我们不需要的列和行。使用 `sheet_name` 参数指定工作表。将 `skiprows` 设置为 `4`，将 `skipfooter`
    设置为 `1`，以跳过前四行（第一行被隐藏）和最后一行。我们为 `usecols` 提供值，从列 `A` 和列 `C` 到列 `W` 获取数据（列 `B`
    是空白的）。使用 `head` 查看前几行，使用 `shape` 获取行数和列数：
- en: '[PRE22]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Note**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: You may encounter a problem with `read_excel` if the Excel file does not use
    utf-8 encoding. One way to resolve this is to save the Excel file as a CSV file,
    reopen it, and then save it with utf-8 encoding.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Excel 文件没有使用 utf-8 编码，您可能会遇到 `read_excel` 的问题。一种解决方法是将 Excel 文件另存为 CSV 文件，重新打开并保存为
    utf-8 编码格式。
- en: 'Use the `info` method of the DataFrame to view data types and the `non-null`
    count. Notice that all columns have the `object` data type:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 DataFrame 的 `info` 方法查看数据类型和 `non-null` 数量。注意所有列的数据类型都是 `object`：
- en: '[PRE26]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Rename the `Year` column to `metro`, and remove the leading spaces.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `Year` 列重命名为 `metro`，并去除前导空格。
- en: 'Give an appropriate name to the metropolitan area column. There are extra spaces
    before the metro values in some cases. We can test for leading spaces with `startswith(''
    '')` and then use `any` to establish whether there are one or more occasions when
    the first character is blank. We can use `endswith('' '')` to examine trailing
    spaces. We use strip to remove both leading and trailing spaces. When we test
    for trailing spaces again, we see that there are none:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 给大都市区域列起一个合适的名称。在某些情况下，大都市值前面有额外的空格。我们可以使用 `startswith(' ')` 测试是否有前导空格，然后使用
    `any` 来检查是否有一个或多个前导空格的情况。我们可以使用 `endswith(' ')` 来检查尾随空格。我们使用 `strip` 去除前后空格。再次测试尾随空格时，我们看到没有：
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Convert the data columns to numeric.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据列转换为数值型。
- en: 'Iterate over all of the GDP year columns (2000–2020) and convert the data type
    from `object` to `float`. Coerce the conversion even when there is character data
    – the `..` in this example. We want character values in those columns to become
    `missing`, which is what happens. Rename the year columns to better reflect the
    data in those columns:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历所有 GDP 年度列（2000–2020），并将数据类型从 `object` 转换为 `float`。即使存在字符数据（例如这个例子中的 `..`），也要强制转换。我们希望这些列中的字符值变为
    `missing`，这正是发生的情况。将年份列重命名为更能反映列中数据的名称：
- en: '[PRE34]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Use the `describe` method to generate summary statistics for all numeric data
    in the DataFrame:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `describe` 方法生成 DataFrame 中所有数值数据的摘要统计信息：
- en: '[PRE38]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Remove rows where all of the per capita GDP values are missing.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除所有人均 GDP 值缺失的行。
- en: 'Use the `subset` parameter of `dropna` to inspect all columns, starting with
    the second column (it is zero-based) and going through to the last column. Use
    `how` to specify that we want to drop rows only if all of the columns specified
    in `subset` are missing. Use `shape` to show the number of rows and columns in
    the resulting DataFrame:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `dropna` 的 `subset` 参数检查所有列，从第二列（基于零索引）开始，到最后一列。使用 `how` 参数指定只有当 `subset`
    中所有列的值都缺失时才删除行。使用 `shape` 来显示结果 DataFrame 中的行数和列数：
- en: '[PRE40]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Set the index for the DataFrame using the metropolitan area column.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用大都市区域列设置 DataFrame 的索引。
- en: 'Confirm that there are 692 valid values for `metro` and that there are 692
    unique values, before setting the index:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 确认 `metro` 有 692 个有效值，并且这 692 个值是唯一的，然后再设置索引：
- en: '[PRE42]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We have now imported the Excel data into a pandas DataFrame and cleaned up some
    of the messiness in the spreadsheet.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经将 Excel 数据导入到 pandas DataFrame 中，并清理了一些电子表格中的杂乱数据。
- en: How it works…
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: We mostly manage to get the data we want in *Step 2* by skipping rows and columns
    we do not want, but there are still a number of issues – `read_excel` interprets
    all of the GDP data as character data, many rows are loaded with no useful data,
    and the column names do not represent the data well. In addition, the metropolitan
    area column might be useful as an index, but there are leading and trailing blanks,
    and there may be missing or duplicated values.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *步骤 2* 中通过跳过不需要的行和列，基本上获得了我们想要的数据，但仍然存在一些问题——`read_excel` 将所有的 GDP 数据解释为字符数据，许多行加载了无用的数据，而且列名未能很好地表示数据。此外，大都市区域列可能作为索引很有用，但存在前后空格，并且可能存在缺失或重复的值。
- en: '`read_excel` interprets `Year` as the column name for the metropolitan area
    data because it looks for a header above the data for that Excel column and finds
    `Year` there. We rename that column `metro` in *Step 4*. We also use `strip` to
    fix the problem with leading and trailing blanks. We could have just used `lstrip`
    to remove leading blanks, or `rstrip` if there had been trailing blanks. It is
    a good idea to assume that there might be leading or trailing blanks in any character
    data, cleaning that data shortly after the initial import.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_excel` 将 `Year` 解释为大都市区域数据的列名，因为它查找该 Excel 列数据上方的标题，并发现了 `Year`。我们在 *步骤
    4* 中将该列重命名为 `metro`。我们还使用 `strip` 修复了前后空格问题。我们本来也可以只使用 `lstrip` 去除前导空格，或者如果有尾随空格，使用
    `rstrip`。假设任何字符数据可能存在前导或尾随空格，并在初次导入后立即清理数据是一个好习惯。'
- en: The spreadsheet authors used `..` to represent missing data. Since this is actually
    valid character data, those columns get the object data type (that is how pandas
    treats columns with character or mixed data). We coerce a conversion to numeric
    type in *Step 5*. This also results in the original values of `..` being replaced
    with `NaN` (not a number), how pandas represents missing values for numbers. This
    is what we want.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 电子表格作者使用 `..` 表示缺失数据。由于这实际上是有效的字符数据，这些列获得了对象数据类型（这是 pandas 处理具有字符或混合数据的列的方式）。我们在*第
    5 步*强制转换为数值类型。这也导致将 `..` 的原始值替换为 `NaN`（不是一个数字），这是 pandas 表示数字缺失值的方式。这正是我们想要的。
- en: We can fix all of the per capita GDP columns with just a few lines because pandas
    makes it easy to iterate over the columns of a DataFrame. By specifying `[1:]`,
    we iterate from the second column to the last column. We can then change those
    columns to numeric and rename them to something more appropriate.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 能够仅通过几行代码修复所有人均 GDP 列，因为 pandas 提供了对 DataFrame 列进行迭代的简便方法。通过指定 `[1:]`，我们从第二列迭代到最后一列。然后，我们可以将这些列更改为数值类型，并重命名为更合适的名称。
- en: There are several reasons why it is a good idea to clean up the column headings
    for the annual GDP columns – it helps us to remember what the data actually is;
    if we merge it with other data by metropolitan area, we will not have to worry
    about conflicting variable names; and we can use attribute access to work with
    pandas Series based on those columns, which I will discuss in more detail in the
    *There’s more…* section of this recipe.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个原因值得考虑清理年度 GDP 列标题 – 这帮助我们记住数据的实际内容; 如果我们按都会区域与其他数据合并，我们将不必担心冲突的变量名称; 我们可以使用属性访问来处理基于这些列的
    pandas Series，我将在本配方的*还有更多…*部分详细讨论。
- en: '`describe` in *Step 6* shows us that fewer than 500 rows have valid data for
    per capita GDP for some years. When we drop all rows that have missing values
    for all per capita GDP columns in *step 7*, we end up with 692 rows in the DataFrame.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 6 步*中的 `describe` 显示我们有少于 500 行有效数据的人均 GDP 的一些年份。当我们在*第 7 步*中删除所有人均 GDP
    列中缺失值的所有行时，我们最终得到 DataFrame 中的 692 行。'
- en: There’s more…
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Once we have a pandas DataFrame, we have the ability to treat columns as more
    than just columns. We can use attribute access (such as `percapitaGPA.metro`)
    or bracket notation (`percapitaGPA['metro']`) to get the functionality of a pandas
    Series. Either method makes it possible to use Series string inspecting methods,
    such as `str.startswith`, and counting methods, such as `nunique`. Note that the
    original column names of `20##` did not allow attribute access because they started
    with a number, so `percapitaGDP.pcGDP2001.count()` works, but `percapitaGDP.2001.count()`
    returns a syntax error because `2001` is not a valid Python identifier (since
    it starts with a number).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了 pandas DataFrame，我们可以将列视为不仅仅是列。我们可以使用属性访问（例如 `percapitaGPA.metro`）或括号表示法（`percapitaGPA['metro']`）来获取
    pandas Series 的功能。任何一种方法都可以使用 Series 的字符串检查方法，如 `str.startswith`，以及计数方法，如 `nunique`。请注意，`20##`
    的原始列名不允许属性访问，因为它们以数字开头，所以 `percapitaGDP.pcGDP2001.count()` 可以工作，但 `percapitaGDP.2001.count()`
    返回语法错误，因为 `2001` 不是有效的 Python 标识符（因为它以数字开头）。
- en: pandas is rich with features for string manipulation and for Series operations.
    We will try many of them out in subsequent recipes. This recipe showed those that
    I find most useful when importing Excel data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 提供了丰富的字符串操作和 Series 操作功能。我们将在后续的示例中尝试其中许多功能。本示例展示了在导入 Excel 数据时我认为最有用的功能。
- en: See also
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: There are good reasons to consider reshaping this data. Instead of 21 columns
    of GDP per capita data for each metropolitan area, we should have 21 rows of data
    for each metropolitan area, with columns for year and GDP per capita. Recipes
    for reshaping data can be found in *Chapter 11*, *Tidying and Reshaping Data*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多理由考虑重塑这些数据。与每个都会区域的 21 列人均 GDP 数据相比，我们应该为每个都会区域有 21 行数据，年份和人均 GDP 数据的列。有关重塑数据的配方可以在*第
    11 章*，*整理和重塑数据*中找到。
- en: Importing data from SQL databases
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 SQL 数据库导入数据
- en: In this recipe, we will use `pymssql` and `mysql apis` to read data from **Microsoft
    SQL Server** and **MySQL** (now owned by **Oracle**) databases, respectively.
    Data from sources such as these tends to be well structured, since it is designed
    to facilitate simultaneous transactions by members of organizations and those
    who interact with them. Each transaction is also likely related to some other
    organizational transaction.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用`pymssql`和`mysql apis`从**Microsoft SQL Server**和**MySQL**（现在由**Oracle**拥有）数据库中读取数据。像这些来源的数据通常结构良好，因为它们旨在促进组织成员及与之互动的人员的同时交易。每一笔交易也可能与其他组织交易相关联。
- en: This means that although data tables from enterprise systems such as these are
    more reliably structured than data from CSV files and Excel files, their logic
    is less likely to be self-contained. You need to know how the data from one table
    relates to data from another table to understand its full meaning. These relationships
    need to be preserved, including the integrity of primary and foreign keys, when
    pulling data. Moreover, well-structured data tables are not necessarily uncomplicated
    data tables. There are often sophisticated coding schemes that determine data
    values, and these coding schemes can change over time. For example, codes for
    merchandise at a retail store chain might be different in 1998 than they are in
    2024\. Similarly, frequently there are codes for missing values, such as 99,999,
    that pandas will understand as valid values.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，尽管来自企业系统的数据表比CSV文件和Excel文件中的数据结构更加可靠，但它们的逻辑较少是自包含的。你需要了解一个表中的数据与另一个表中的数据如何关联，才能理解其完整含义。在提取数据时，必须保留这些关系，包括主键和外键的完整性。此外，结构良好的数据表不一定是简单的数据表。往往存在复杂的编码方案来确定数据值，这些编码方案可能会随着时间的推移而发生变化。例如，零售商店连锁的商品代码可能在1998年与2024年有所不同。同样，经常会有缺失值的代码，如99,999，pandas会将其识别为有效值。
- en: Since much of this logic is business logic, and implemented in stored procedures
    or other applications, it is lost when pulled out of this larger system. Some
    of what is lost will eventually have to be reconstructed when preparing data for
    analysis. This almost always involves combining data from multiple tables, so
    it is important to preserve the ability to do that. However, it also may involve
    adding some of the coding logic back after loading the SQL table into a pandas
    DataFrame. We explore how to do that in this recipe.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其中许多逻辑是业务逻辑，并且在存储过程或其他应用程序中实现，因此当数据从这一更大的系统中提取出来时，这些逻辑会丢失。在为分析准备数据时，丢失的部分最终需要被重建。这几乎总是涉及到将多个表中的数据合并，因此保留这种能力非常重要。然而，它也可能涉及在将SQL表加载到pandas
    DataFrame后，将一些编码逻辑重新加入。我们将在本教程中探讨如何做到这一点。
- en: Getting ready
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe assumes you have `pymssql` and `mysql apis` installed. If you do
    not, it is relatively straightforward to install them with `pip`. From the Terminal,
    or `powershell` (in Windows), enter `pip install pymssql` or `pip install mysql-connector-python`.
    We will work with data on educational attainment in this recipe.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程假设你已经安装了`pymssql`和`mysql apis`。如果没有，可以通过`pip`轻松安装。在终端或`powershell`（在Windows中）中输入`pip
    install pymssql`或`pip install mysql-connector-python`。我们将在本教程中使用有关教育水平的数据。
- en: '**Data note**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: The dataset used in this recipe is available for public use at [https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip](https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程使用的数据集可以通过[https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip](https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip)公开下载。
- en: How to do it...
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We import SQL Server and MySQL data tables into a pandas DataFrame, as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将SQL Server和MySQL的数据表导入到pandas DataFrame中，如下所示：
- en: Import `pandas`, `numpy`, `pymssql`, and `mysql`.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`、`pymssql`和`mysql`。
- en: 'This step assumes that you have installed `pymssql` and `mysql apis`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤假设你已经安装了`pymssql`和`mysql apis`：
- en: '[PRE48]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Use `pymssql api` and `read_sql` to retrieve and load data from a SQL Server
    instance.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pymssql api`和`read_sql`从SQL Server实例中检索并加载数据。
- en: 'Select the columns we want from the SQL Server data, and use SQL aliases to
    improve column names (for example, `fedu AS fathereducation`). Create a connection
    to the SQL Server data by passing database credentials to the `pymssql` `connect`
    function. Create a pandas DataFrame by passing the `SELECT` statement and connection
    object to `read_sql`. Use `close` to return the connection to the pool on the
    server:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从 SQL Server 数据中选择我们需要的列，使用 SQL 别名改善列名（例如，`fedu AS fathereducation`）。通过将数据库凭证传递给
    `pymssql` 的 `connect` 函数，建立与 SQL Server 数据的连接。通过将 `SELECT` 语句和连接对象传递给 `read_sql`
    创建 pandas DataFrame。使用 `close` 将连接返回到服务器上的连接池：
- en: '[PRE49]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '**Note**'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Although tools such as `pymssql` make connecting to a SQL Server instance relatively
    straightforward, the syntax still might take a little time to get used to if it
    is unfamiliar. The previous step shows the parameter values you will typically
    need to pass to a connection object – the name of the server, the name of a user
    with credentials on the server, the password for that user, and the name of a
    SQL database on the server.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像 `pymssql` 这样的工具使得连接到 SQL Server 实例相对简单，但如果不熟悉，语法可能还是需要一些时间适应。前一步展示了你通常需要传递给连接对象的参数值——服务器名称、具有凭证的用户名称、该用户的密码，以及服务器上
    SQL 数据库的名称。
- en: 'Check the data types and the first few rows:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据类型和前几行：
- en: '[PRE50]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Connecting to a MySQL server is not very different from connecting to a SQL
    Server instance. We can use the `connect` method of the `mysql` connector to do
    that and then use `read_sql` to load the data.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到 MySQL 服务器与连接到 SQL Server 实例没有太大不同。我们可以使用 `mysql` 连接器的 `connect` 方法来完成此操作，然后使用
    `read_sql` 加载数据。
- en: 'Create a connection to the `mysql` data, pass that connection to `read_sql`
    to retrieve the data, and load it into a pandas DataFrame (the same data file
    on student math scores was uploaded to SQL Server and MySQL, so we can use the
    same SQL `SELECT` statement we used in the previous step):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 创建与 `mysql` 数据的连接，将该连接传递给 `read_sql` 以检索数据，并将其加载到 pandas DataFrame 中（相同的学生数学成绩数据文件已经上传到
    SQL Server 和 MySQL，因此我们可以使用与上一步骤相同的 SQL `SELECT` 语句）：
- en: '[PRE54]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Rearrange the columns, set an index, and check for missing values.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排列列，设置索引，并检查缺失值。
- en: 'Move the grade data to the left of the DataFrame, just after `studentid`. Also,
    move the `freetime` column to the right after `traveltime` and `studytime`. Confirm
    that each row has an ID and that the IDs are unique, and set `studentid` as the
    index:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 将成绩数据移到 DataFrame 的左侧，紧跟在 `studentid` 后面。同时，将 `freetime` 列移到 `traveltime` 和
    `studytime` 后面。确认每行都有一个 ID 且 ID 是唯一的，并将 `studentid` 设置为索引：
- en: '[PRE55]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Use the DataFrame’s `count` function to check for missing values:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 DataFrame 的 `count` 函数检查缺失值：
- en: '[PRE60]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Replace coded data values with more informative values.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用更具信息量的值替换编码数据值。
- en: 'Create a dictionary with the replacement values for the columns, and then use
    `replace` to set those values:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个字典，包含要替换的列的值，然后使用 `replace` 设置这些值：
- en: '[PRE62]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Change the type for columns with the changed data to `category`.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将已更改数据的列类型改为 `category`。
- en: 'Check any changes in memory usage:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 检查内存使用情况的任何变化：
- en: '[PRE63]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Calculate percentages for values in the `famrel` column.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 `famrel` 列中的值的百分比。
- en: 'Run `value_counts`, and set `normalize` to `True` to generate percentages:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `value_counts`，并将 `normalize` 设置为 `True` 以生成百分比：
- en: '[PRE67]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Use `apply` to calculate percentages for multiple columns:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `apply` 计算多个列的百分比：
- en: '[PRE69]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The preceding steps retrieved a data table from a SQL database, loaded that
    data into pandas, and did some initial data checking and cleaning.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤从 SQL 数据库中检索了一个数据表，将数据加载到 pandas 中，并进行了初步的数据检查和清理。
- en: How it works…
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Since data from enterprise systems is typically better structured than CSV or
    Excel files, we do not need to do things such as skip rows or deal with different
    logical data types in a column. However, some massaging is still usually required
    before we can begin exploratory analysis. There are often more columns than we
    need, and some column names are not intuitive or not ordered in the best way for
    analysis. The meaningfulness of many data values is not stored in the data table
    to avoid entry errors and save on storage space. For example, `3` is stored for
    mother’s education rather than secondary education. It is a good idea to reconstruct
    that coding as early in the cleaning process as possible.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由于企业系统中的数据通常比CSV或Excel文件更有结构性，我们无需执行像跳过行或处理列中的不同逻辑数据类型等操作。然而，在开始探索性分析之前，通常仍然需要做一些数据处理。通常列的数量超过我们需要的，而且一些列名并不直观，或者没有以最适合分析的顺序排列。为了避免输入错误并节省存储空间，许多数据值的意义并未存储在数据表中。例如，`3`表示母亲的教育程度，而不是“中等教育”。最好在清理过程中尽早重构这种编码。
- en: To pull data from a SQL database server, we need a connection object to authenticate
    us on the server, as well as a SQL select string. These can be passed to `read_sql`
    to retrieve the data and load it into a pandas DataFrame. I usually use the SQL
    `SELECT` statement to do a bit of cleanup of column names at this point. I sometimes
    also reorder columns, but I did that later in this recipe.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要从SQL数据库服务器中提取数据，我们需要一个连接对象来进行身份验证，以及一个SQL选择字符串。这些可以传递给`read_sql`以检索数据并加载到pandas
    DataFrame中。我通常在此时使用SQL的`SELECT`语句对列名进行一些清理。有时我也会重新排列列，但在这个步骤我是在稍后的操作中做的。
- en: We set the index in *Step 5*, first confirming that every row has a value for
    `studentid` and that it is unique. This is often more important when working with
    enterprise data because we will almost always need to merge the retrieved data
    with other data files on the system. Although an index is not required for this
    merging, the discipline of setting one prepares us for the tricky business of
    merging data further down the road. It will also likely improve the speed of the
    merge.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*步骤5*中设置了索引，首先确认每行都有`studentid`的值，并且它是唯一的。当处理企业数据时，这一点通常更为重要，因为我们几乎总是需要将获取的数据与系统中的其他数据文件合并。虽然合并时不要求必须有索引，但设置索引的习惯让我们为后续更复杂的合并操作做好了准备。这也可能提高合并的速度。
- en: We use the DataFrame’s `count` function to check for missing values and that
    there are no missing values – for non-missing values, the count is 395 (the number
    of rows) for every column. This is almost too good to be true. There may be values
    that are logically missing – that is, valid numbers that nonetheless connote missing
    values, such as `-1`, `0`, `9`, or `99`. We address this possibility in the next
    step.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用DataFrame的`count`函数检查缺失值，并确认没有缺失值——对于非缺失值，每一列的计数都是395（行数）。这几乎好得令人难以置信。可能存在逻辑上缺失的值——也就是说，有效的数字却表示缺失值，比如`-1`、`0`、`9`或`99`。我们将在下一步处理中解决这个问题。
- en: '*Step 7* demonstrates a useful technique for replacing data values for multiple
    columns. We create a dictionary to map original values to new values for each
    column and then run it using `replace`. To reduce the amount of storage space
    taken up by the new verbose values, we convert the data type of those columns
    to `category`. We do this by generating a list of the keys of our `setvalues`
    dictionary – `setvalueskeys = [k for k in setvalues]` generates [`famrel`, `freetime`,
    `goout`, `mothereducation`, and `fathereducation`]. We then iterate over those
    five columns and use the `astype` method to change the data type to `category`.
    Notice that the memory usage for those columns is reduced substantially.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤7*展示了一个用于替换多个列数据值的有用技巧。我们创建一个字典，将每列的原始值映射到新值，然后使用`replace`运行它。为了减少新长格式值占用的存储空间，我们将这些列的数据类型转换为`category`。我们通过生成`setvalues`字典的键列表来实现这一点——`setvalueskeys
    = [k for k in setvalues]`生成[`famrel`、`freetime`、`goout`、`mothereducation`和`fathereducation`]。然后，我们遍历这五列，并使用`astype`方法将数据类型更改为`category`。注意，这些列的内存使用量显著减少。'
- en: Finally, we check the assignment of new values by using `value_counts` to view
    relative frequencies. We use `apply` because we want to run `value_counts` on
    multiple columns. To prevent `value_counts` sorting by frequency, we set sort
    to `False`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过使用`value_counts`查看相对频率来检查新值的分配情况。我们使用`apply`，因为我们想对多个列运行`value_counts`。为了防止`value_counts`按频率排序，我们将`sort`设置为`False`。
- en: The DataFrame `replace` method is also a handy tool for dealing with logical
    missing values that will not be recognized as missing when retrieved by `read_sql`.
    The `0` values for `mothereducation` and `fathereducation` seem to fall into that
    category. We fix this problem in the `setvalues` dictionary by indicating that
    the `0` values for `mothereducation` and `fathereducation` should be replaced
    with `NaN`. It is important to address these kinds of missing values shortly after
    the initial import because they are not always obvious and can significantly impact
    all subsequent work.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`replace`方法也是处理逻辑缺失值的便利工具，当通过`read_sql`检索时，这些值将不被识别为缺失。对于`mothereducation`和`fathereducation`的`0`值似乎属于这种情况。我们在`setvalues`字典中通过指示`mothereducation`和`fathereducation`的`0`值应替换为`NaN`来解决这个问题。在初始导入后不久解决这类缺失值问题很重要，因为它们并不总是显而易见，而且可能会显著影响后续所有工作。'
- en: Users of packages such as *SPPS*, *SAS*, and *R* will notice the difference
    between this approach and value labels in SPSS and R, as well as the `proc` format
    in SAS. In pandas, we need to change the actual data to get more informative values.
    However, we reduce how much data is actually stored by giving the column a `category`
    data type. This is similar to `factors` in R.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 像*SPPS*、*SAS*和*R*等软件包的用户会注意到这种方法与SPSS和R中的值标签以及SAS中的`proc`格式的差异。在 pandas 中，我们需要更改实际数据以获得更详细的值。然而，通过为列指定`category`数据类型，我们减少了实际存储的数据量。这类似于R中的`factors`。
- en: There’s more…
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: I moved the grade data to near the beginning of the DataFrame. I find it helpful
    to have potential target or dependent variables in the leftmost columns, keeping
    them at the forefront of your mind. It is also helpful to keep similar columns
    together. In this example, personal demographic variables (sex and age) are next
    to one another, as are family variables (`mothereducation` and `fathereducation`),
    and how students spend their time (`traveltime`, `studytime`, and `freetime`).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我将成绩数据移到了DataFrame的开头附近。我发现将潜在的目标或依赖变量放在最左边的列中是有帮助的，这样可以将它们保持在脑海中的最前沿。将类似的列放在一起也是有帮助的。在这个例子中，个人人口统计变量（性别和年龄）相邻，家庭变量（`mothereducation`和`fathereducation`）相邻，以及学生如何花时间（`traveltime`、`studytime`和`freetime`）。
- en: You could have used `map` instead of `replace` in *Step 7*. Prior to version
    19.2 of pandas, `map` was significantly more efficient. Since then, the difference
    in efficiency has been much smaller. If you are working with a very large dataset,
    the difference may still be enough to consider using `map`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Step 7*中，你本可以使用`map`而不是`replace`。在 pandas 的 19.2 版本之前，`map` 的效率显著更高。此后，效率差异已经小得多。如果你正在处理非常大的数据集，这种差异仍然足以考虑使用`map`。
- en: See also
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The recipes in *Chapter 10*, *Addressing Data Issues When Combining DataFrames*,
    go into detail on merging data. We will take a closer look at bivariate and multivariate
    relationships between variables in *Chapter 4*, *Identifying Outliers in Subsets
    of Data*. We will demonstrate how to use some of these same approaches in packages
    such as SPSS, SAS, and R in subsequent recipes in this chapter.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Chapter 10*的配方中，解决数据合并时的数据问题会详细介绍数据合并的细节。我们将在*Chapter 4*中更仔细地研究变量之间的双变量和多变量关系，以及如何在本章后续的SPSS、SAS和R中使用一些相同的方法。
- en: Importing SPSS, Stata, and SAS data
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入SPSS、Stata和SAS数据
- en: We will use `pyreadstat` to read data from three popular statistical packages
    into pandas. The key advantage of `pyreadstat` is that it allows data analysts
    to import data from these packages without losing metadata, such as variable and
    value labels.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`pyreadstat`从三个流行的统计软件包中读取数据到 pandas 中。`pyreadstat`的主要优势在于允许数据分析师导入这些软件包的数据而不丢失元数据，例如变量和值标签。
- en: The SPSS, Stata, and SAS data files we receive often come to us with the data
    issues of CSV and Excel files and SQL databases having been resolved. We do not
    typically have the invalid column names, changes in data types, and unclear missing
    values that we can get with CSV or Excel files, nor do we usually get the detachment
    of data from business logic, such as the meaning of data codes, that we often
    get with SQL data. When someone or some organization shares a data file from one
    of these packages with us, they have often added variable labels and value labels
    for categorical data. For example, a hypothetical data column called `presentsat`
    has the `overall satisfaction with presentation` variable label and `1`–`5` value
    labels, with `1` being not at all satisfied and `5` being highly satisfied.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到的SPSS、Stata和SAS数据文件通常已经解决了CSV和Excel文件以及SQL数据库中的数据问题。通常我们不会遇到CSV或Excel文件中可能出现的无效列名、数据类型变化和不明确的缺失值，也不太会遇到SQL数据中常见的数据与业务逻辑脱节的问题（比如数据代码的含义）。当某人或某个组织与我们共享来自这些软件包的数据文件时，他们通常会为分类数据添加变量标签和数值标签。例如，一个假设的数据列`presentsat`可能有`总体满意度`的变量标签和`1`到`5`的数值标签，其中`1`表示完全不满意，`5`表示非常满意。
- en: The challenge is retaining that metadata when importing data from those systems
    into pandas. There is no precise equivalent to variable and value labels in pandas,
    and built-in tools for importing SAS, Stata, and SAS data lose the metadata. In
    this recipe, we will use `pyreadstat` to load variable and value label information
    and use a couple of techniques to represent that information in pandas.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于从这些系统导入数据到pandas时如何保留元数据。pandas没有与变量和数值标签完全对应的功能，并且用于导入SAS、Stata和SAS数据的内建工具会丢失元数据。在这个教程中，我们将使用`pyreadstat`加载变量和数值标签信息，并使用一些技巧将这些信息表示在pandas中。
- en: Getting ready
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe assumes you have installed the `pyreadstat` package. If it is not
    installed, you can install it with `pip`. From the Terminal, or Powershell (in
    Windows), enter `pip install pyreadstat`. You will need the SPSS, Stata, and SAS
    data files for this recipe to run the code.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程假设你已经安装了`pyreadstat`包。如果没有安装，你可以通过`pip`来安装它。在终端或PowerShell（Windows）中输入`pip
    install pyreadstat`。运行此代码需要SPSS、Stata和SAS数据文件。
- en: We will work with data from the United States **National Longitudinal Surveys**
    (**NLS**) of Youth.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自美国**国家纵向调查**（**NLS**）青少年数据的数据。
- en: '**Data note**'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: The NLS of Youth is conducted by the United States Bureau of Labor Statistics.
    This survey started with a cohort of individuals in 1997\. Each survey respondent
    was high school age when they first completed the survey, having been born between
    1980 and 1985\. There were annual follow-up surveys each year through 2023\. For
    this recipe, I pulled 42 variables on grades, employment, income, and attitudes
    toward government, from the hundreds of data items on the survey. Separate files
    for SPSS, Stata, and SAS can be downloaded from the repository.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 青少年NLS调查由美国劳工统计局进行。该调查始于1997年，最初的受访者是在高中年龄时完成调查的，出生年份在1980至1985年之间。每年都有后续调查，一直到2023年。对于本教程，我从调查中数百个数据项中提取了42个变量，涵盖了成绩、就业、收入和对政府的态度。可以从数据仓库下载SPSS、Stata和SAS的独立文件。
- en: The original NLS data can be downloaded from [https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search),
    along with code for creating SPSS, Stata, or SAS files from the ASCII data files
    included in the download.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的NLS数据可以从[https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search)下载，同时也有用于从下载中的ASCII数据文件创建SPSS、Stata或SAS文件的代码。
- en: How to do it...
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will import data from SPSS, Stata, and SAS, retaining metadata such as value
    labels:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从SPSS、Stata和SAS中导入数据，保留元数据，如数值标签：
- en: Import `pandas`, `numpy`, and `pyreadstat`.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`和`pyreadstat`。
- en: 'This step assumes that you have installed `pyreadstat`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步假设你已经安装了`pyreadstat`：
- en: '[PRE73]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Retrieve the SPSS data.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取SPSS数据。
- en: 'Pass a path and filename to the `read_sav` method of `pyreadstat`. Display
    the first few rows and a frequency distribution. Note that the column names and
    value labels are non-descriptive, and that `read_sav` returns both a pandas DataFrame
    and a `meta` object:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 将路径和文件名传递给`pyreadstat`的`read_sav`方法。显示前几行和频率分布。注意，列名和数值标签是非描述性的，并且`read_sav`返回一个pandas
    DataFrame和一个`meta`对象：
- en: '[PRE74]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Grab the metadata to improve column labels and value labels.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取元数据以改进列标签和数值标签。
- en: 'The `metaspss` object created when we called `read_sav` has the column labels
    and the value labels from the SPSS file. Use the `variable_value_labels` dictionary
    to map values to value labels for one column (`R0536300`). (This does not change
    the data. It only improves our display when we run `value_counts`.) Use the `set_value_labels`
    method to actually apply the value labels to the DataFrame:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们调用`read_sav`时创建的`metaspss`对象包含了SPSS文件中的列标签和值标签。使用`variable_value_labels`字典将值映射到某一列（`R0536300`）的值标签。（这不会改变数据，仅在运行`value_counts`时改进显示。）使用`set_value_labels`方法将值标签实际应用于DataFrame：
- en: '[PRE80]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Use column labels in the metadata to rename the columns.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用元数据中的列标签来重命名列。
- en: 'To use the column labels from `metaspss` in our DataFrame, we can simply assign
    the column labels in `metaspss` to our DataFrame’s column names. Clean up the
    column names a bit by changing them to lowercase, changing spaces to underscores,
    and removing all remaining non-alphanumeric characters:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要在我们的DataFrame中使用`metaspss`中的列标签，我们只需将`metaspss`中的列标签分配给DataFrame的列名。稍微清理一下列名，将其转换为小写字母，将空格替换为下划线，并删除所有剩余的非字母数字字符：
- en: '[PRE85]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Simplify the process by applying the value labels from the beginning.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从一开始就应用值标签来简化这个过程。
- en: 'The data values can actually be applied in the initial call to `read_sav` by
    setting `apply_value_formats` to `True`. This eliminates the need to call the
    `set_value_labels` function later:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在初始调用`read_sav`时，就可以通过设置`apply_value_formats`为`True`来应用数据值。这消除了稍后调用`set_value_labels`函数的需求：
- en: '[PRE90]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Show the columns and a few rows:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示列和几行数据：
- en: '[PRE91]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Run frequencies on one of the columns, and set the index:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对某一列运行频率，并设置索引：
- en: '[PRE95]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: That demonstrated how to convert data from SPSS. Let’s try that with Stata data.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这展示了如何将SPSS数据转换。现在我们尝试使用Stata数据。
- en: Import the Stata data, apply value labels, and improve the column headings.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Stata数据，应用值标签，并改进列标题。
- en: 'Use the same methods for the Stata data that we used for the SPSS data:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对Stata数据使用与SPSS数据相同的方法：
- en: '[PRE98]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'View a few rows of the data and run frequencies:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看几行数据并运行频率：
- en: '[PRE100]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Fix the logical missing values that show up with the Stata data and set an
    index. We can use the `replace` method to set any value that is between `–9` and
    `–1` in any column to missing:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修复Stata数据中出现的逻辑缺失值，并设置索引。我们可以使用`replace`方法将任何列中介于`–9`和`–1`之间的值设置为缺失值：
- en: '[PRE104]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: The process is fairly similar when working with SAS data files, as the next
    few steps illustrate.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理SAS数据文件时，流程与接下来的几个步骤类似。
- en: 'Retrieve the SAS data, using the SAS catalog file for value labels:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SAS目录文件来检索SAS数据的值标签：
- en: 'The data values for SAS are stored in a catalog file. Setting the catalog file
    path and filename retrieves the value labels and applies them:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: SAS的数据值存储在目录文件中。设置目录文件路径和文件名可以检索值标签并应用它们：
- en: '[PRE109]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: This demonstrates how to import SPSS, SAS, and Stata data without losing important
    metadata.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了如何导入SPSS、SAS和Stata数据，而不丢失重要的元数据。
- en: How it works...
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `read_sav`, `read_dta`, and `read_sas7bdat` methods of `Pyreadstat`, for
    SPSS, Stata, and SAS data files, respectively, work in a similar manner. Value
    labels can be applied when reading in the data by setting `apply_value_formats`
    to `True` for SPSS and Stata files (*Steps 5 and 8*), or by providing a catalog
    file path and filename for SAS (*Step 12*).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pyreadstat`的`read_sav`、`read_dta`和`read_sas7bdat`方法，分别用于SPSS、Stata和SAS数据文件，工作方式相似。在读取数据时，可以通过设置`apply_value_formats`为`True`来应用值标签（适用于SPSS和Stata文件，见*步骤5和8*），或者通过提供SAS目录文件路径和文件名来应用值标签（见*步骤12*）。'
- en: We can set `formats_as_category` to `True` to change the data type to `category`
    for those columns where the data values will change. The meta object has the column
    names and the column labels from the statistical package, so metadata column labels
    can be assigned to pandas DataFrame column names at any point (`nls97spss.columns
    = metaspss.column_labels`). We can even revert to the original column headings
    after assigning meta column labels to them by setting pandas column names to the
    metadata column names (`nls97spss.columns = metaspss.column_names`).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`formats_as_category`设置为`True`，以便将那些数据值会变化的列的数据类型更改为`category`。元对象包含了统计软件包中的列名和列标签，因此，元数据列标签可以随时分配给pandas
    DataFrame的列名（`nls97spss.columns = metaspss.column_labels`）。我们甚至可以在将元数据列标签分配给列之后，使用元数据列名将pandas列名恢复为原始列标题（`nls97spss.columns
    = metaspss.column_names`）。
- en: In *Step 3*, we looked at some of the SPSS data before applying value labels.
    We looked at the dictionary for one variable (`metaspss.variable_value_labels['R0536300']`),
    but we could have viewed it for all variables (`metaspss.variable_value_labels`).
    When we are satisfied that the labels make sense, we can set them by calling the
    `set_value_labels` function. This is a good approach when you do not know the
    data well and want to inspect the labels before applying them.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3*中，我们在应用值标签之前查看了一些 SPSS 数据。我们查看了某个变量的字典（`metaspss.variable_value_labels['R0536300']`），但我们也可以查看所有变量的字典（`metaspss.variable_value_labels`）。当我们确认标签合理后，可以通过调用
    `set_value_labels` 函数来设置它们。这是一种好的方法，尤其是在你对数据不太了解时，想在应用标签之前先检查它们。
- en: The column labels from the meta object are often a better choice than the original
    column headings. Column headings can be quite cryptic, particularly when the SPSS,
    Stata, or SAS file is based on a large survey, as in this example. However, the
    labels are not usually ideal for column headings either. They sometimes have spaces,
    capitalization that is not helpful, and non-alphanumeric characters. We chain
    some string operations to switch to lowercase, replace spaces with underscores,
    and remove non-alphanumeric characters.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 来自元对象的列标签通常比原始列标题更为合适。列标题可能会非常晦涩，特别是当 SPSS、Stata 或 SAS 文件基于一个大型调查时，就像这个例子一样。然而，这些标签通常也不是理想的列标题。它们有时包含空格、不太有用的大写字母和非字母数字字符。我们通过一些字符串操作将其转换为小写，替换空格为下划线，并去除非字母数字字符。
- en: Handling missing values is not always straightforward with these data files,
    since there are often many reasons why data is missing. If the file is from a
    survey, the missing value may be because of a survey skip pattern, or a respondent
    failed to respond, or the response was invalid, and so on. The NLS has nine possible
    values for missing, from `–1` to `–9`. The SPSS import automatically set those
    values to `NaN`, while the Stata import retained the original values. (We could
    have gotten the SPSS import to retain those values by setting `user_missing` to
    `True`.) For the Stata data, we need to tell it to replace all values from `–1`
    to `–9` with `NaN`. We do this by using the DataFrame’s `replace` function and
    passing it a list of integers from `–9` to `–1` (`list(range(-9,0))`).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值在这些数据文件中并不总是那么简单，因为数据缺失往往有很多不同的原因。如果文件来自于一项调查，缺失值可能是因为调查跳过模式、受访者未作回应、回应无效等原因。NLS
    有九种可能的缺失值，从 `–1` 到 `–9`。SPSS 导入会自动将这些值设置为 `NaN`，而 Stata 导入则保留了原始值。（我们本可以通过将 `user_missing`
    设置为 `True`，让 SPSS 导入保留这些值。）对于 Stata 数据，我们需要告知它将所有从 `–1` 到 `–9` 的值替换为 `NaN`。我们通过使用
    DataFrame 的 `replace` 函数，并传入从 `–9` 到 `–1` 的整数列表（`list(range(-9,0))`）来实现这一点。
- en: There’s more…
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: You may have noticed similarities between this recipe and the previous one in
    terms of how value labels are set. The `set_value_labels` function is like the
    DataFrame `replace` operation we used to set value labels in that recipe. We passed
    a dictionary to `replace` that mapped columns to value labels. The `set_value_labels`
    function in this recipe essentially does the same thing, using the `variable_value_labels`
    property of the meta object as the dictionary.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，这个步骤和前面的步骤在设置值标签方面有相似之处。`set_value_labels` 函数类似于我们在上一步中用于设置值标签的 DataFrame
    `replace` 操作。我们将一个字典传递给 `replace`，该字典将列与值标签映射起来。在这个步骤中，`set_value_labels` 函数本质上做了同样的事情，它使用元对象的
    `variable_value_labels` 属性作为字典。
- en: Data from statistical packages is often not as well structured as SQL databases
    tend to be in one significant way. Since they are designed to facilitate analysis,
    they often violate database normalization rules. There is often an implied relational
    structure that might have to be *unflattened* at some point. For example, the
    data may combine individual and event-level data – a person and hospital visits,
    a brown bear and the date it emerged from hibernation. Often, this data will need
    to be reshaped for some aspects of the analysis.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 来自统计软件包的数据通常没有 SQL 数据库那样结构化，尤其在一个重要的方面。因为这些软件包的设计目的是为了促进分析，它们常常违反数据库的规范化规则。通常会有一个隐含的关系结构，可能需要在某个时刻进行*反扁平化*。例如，数据可能会将个体和事件级别的数据结合在一起——一个人和医院就诊记录，一个棕熊和它从冬眠中苏醒的日期。通常情况下，这些数据需要在某些分析方面进行重塑。
- en: See also
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The `pyreadstat` package is nicely documented at [https://github.com/Roche/pyreadstat](https://github.com/Roche/pyreadstat).
    The package has many useful options for selecting columns and handling missing
    data that space did not permit me to demonstrate in this recipe. In *Chapter 11*,
    *Tidying and Reshaping Data*, we will examine how to normalize data that may have
    been flattened for analytical purposes.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`pyreadstat`包有很好的文档，地址是[https://github.com/Roche/pyreadstat](https://github.com/Roche/pyreadstat)。这个包有许多有用的选项，可以选择列并处理缺失数据，但由于篇幅限制，我无法在本食谱中展示。我们将在*第11章*，*整理和重塑数据*中，讨论如何规范化可能已为分析目的而扁平化的数据。'
- en: Importing R data
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入R数据
- en: We will use `pyreadr` to read an R data file into pandas. Since `pyreadr` cannot
    capture the metadata, we will write code to reconstruct value labels (analogous
    to R factors) and column headings. This is similar to what we did in the *Importing
    data from SQL databases* recipe.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`pyreadr`将R数据文件读取到pandas中。由于`pyreadr`无法捕捉元数据，我们需要编写代码来重构值标签（类似于R中的因子）和列标题。这与我们在*从SQL数据库导入数据*食谱中做的类似。
- en: The R statistical package is, in many ways, similar to the combination of Python
    and pandas, at least in its scope. Both have strong tools across a range of data
    preparation and data analysis tasks. Some data scientists work with both R and
    Python, perhaps doing data manipulation in Python and statistical analysis in
    R, or vice versa, depending on their preferred packages. However, there is currently
    a scarcity of tools for reading data saved in R, as `rds` or `rdata` files, into
    Python. The analyst often saves the data as a CSV file first and then loads it
    into Python. We will use `pyreadr`, from the same author as `pyreadstat`, because
    it does not require an installation of R.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: R统计包在许多方面类似于Python和pandas的组合，至少在其功能范围上是如此。两者都在数据准备和数据分析任务中拥有强大的工具。部分数据科学家同时使用R和Python，可能在Python中进行数据处理，在R中进行统计分析，或者反过来，具体取决于他们偏好的包。然而，目前在将R中保存的数据（如`rds`或`rdata`文件）读取到Python中的工具仍然比较稀缺。分析师通常先将数据保存为CSV文件，然后再加载到Python中。我们将使用`pyreadr`，它与`pyreadstat`同一作者开发，因为它不需要安装R。
- en: When we receive an R file, or work with one we have created ourselves, we can
    count on it being fairly well structured, at least compared to CSV or Excel files.
    Each column will have only one data type, column headings will have appropriate
    names for Python variables, and all rows will have the same structure. However,
    we may need to restore some of the coding logic, as we did when working with SQL
    data.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们接收到R文件，或处理我们自己创建的R文件时，可以指望它的结构相对完善，至少比CSV或Excel文件要好。每一列只有一种数据类型，列标题会有适合Python变量的名称，所有行也会有相同的结构。然而，我们可能需要恢复一些编码逻辑，就像处理SQL数据时一样。
- en: Getting ready
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe assumes you have installed the `pyreadr` package. If it is not installed,
    you can install it with `pip`. From the Terminal, or Powershell (in Windows),
    enter `pip install pyreadr`.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱假设你已经安装了`pyreadr`包。如果尚未安装，你可以使用`pip`安装它。在终端或Windows的Powershell中输入`pip install
    pyreadr`。
- en: We will again work with the NLS in this recipe. You will need to download the
    `rds` file used in this recipe from the GitHub repository in order to run the
    code.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将再次使用NLS数据。你需要从GitHub仓库下载本食谱中使用的`rds`文件，以便运行代码。
- en: How to do it…
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We will import data from R without losing important metadata:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将导入R数据，同时不会丢失重要的元数据：
- en: 'Load `pandas`, `numpy`, `pprint`, and the `pyreadr` package:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`pandas`，`numpy`，`pprint`和`pyreadr`包：
- en: '[PRE114]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Get the R data.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取R数据。
- en: 'Pass the path and filename to the `read_r` method to retrieve the R data, and
    load it into memory as a pandas DataFrame. `read_r` can return one or more objects.
    When reading an `rds` file (as opposed to an `rdata` file), it will return one
    object, having the key `None`. We indicate `None` to get the pandas DataFrame:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 将路径和文件名传递给`read_r`方法，以检索R数据，并将其加载为pandas DataFrame。`read_r`可以返回一个或多个对象。当读取`rds`文件（与`rdata`文件相对）时，它会返回一个对象，键为`None`。我们通过指定`None`来获取pandas
    DataFrame：
- en: '[PRE115]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Set up dictionaries for value labels and column headings.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置值标签和列标题的字典。
- en: 'Load a dictionary that maps columns to the value labels and create a list of
    preferred column names as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 加载一个将列映射到值标签的字典，并创建如下的首选列名列表：
- en: '[PRE119]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Set value labels and missing values, and change selected columns to the `category`
    data type.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置值标签和缺失值，并将选定的列更改为`category`数据类型。
- en: 'Use the `setvalues` dictionary to replace existing values with value labels.
    Replace all values from `–9` to `–1` with `NaN`:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `setvalues` 字典将现有值替换为值标签。用 `NaN` 替换所有从 `–9` 到 `–1` 的所有值：
- en: '[PRE121]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Set meaningful column headings:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置有意义的列标题：
- en: '[PRE125]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: This shows how R data files can be imported into pandas and value labels assigned.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了如何将 R 数据文件导入 pandas 并分配值标签。
- en: How it works…
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理是……
- en: Reading R data into pandas with `pyreadr` is fairly straightforward. Passing
    a filename to the `read_r` function is all that is required. Since `read_r` can
    return multiple objects with one call, we need to specify which object. When reading
    an `rds` file (as opposed to an `rdata` file), only one object is returned. It
    has the key `None`.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pyreadr` 将 R 数据读入 pandas 相当简单。将文件名传递给 `read_r` 函数即可。由于 `read_r` 可以一次返回多个对象，我们需要指定哪个对象。在读取
    `rds` 文件时（而不是 `rdata` 文件），只返回一个对象。它的键为 `None`。
- en: In *Step 3*, we loaded a dictionary that maps our variables to value labels,
    and a list for our preferred column headings. In *Step 4* we applied the value
    labels. We also changed the data type to `category` for the columns where we applied
    the values. We did this by generating a list of the keys in our `setvalues` dictionary
    with `[k for k in setvalues]` and then iterating over those columns.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 3 步* 中，我们加载了一个将变量映射到值标签的字典，并为我们首选的列标题加载了一个列表。在 *第 4 步* 中，我们应用了值标签。我们还将应用值的数据类型更改为
    `category`，用 `[k for k in setvalues]` 生成了 `setvalues` 字典中键的列表，然后迭代这些列。
- en: We change the column headings in *Step 5* to ones that are more intuitive. Note
    that the order matters here. We need to set the value labels before changing the
    column names, since the `setvalues` dictionary is based on the original column
    headings.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 *第 5 步* 中的列标题更改为更直观的标题。请注意这里顺序很重要。在更改列名之前，我们需要设置值标签，因为 `setvalues` 字典基于原始列标题。
- en: The main advantage of using `pyreadr` to read R files directly into pandas is
    that we do not have to convert the R data into a CSV file first. Once we have
    written our Python code to read the file, we can just rerun it whenever the R
    data changes. This is particularly helpful when we do not have R on the machine
    where we work.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pyreadr` 将 R 文件直接读入 pandas 的主要优势是我们无需首先将 R 数据转换为 CSV 文件。一旦编写了读取文件的 Python
    代码，只需在 R 数据更改时重新运行它即可。当我们在没有安装 R 的机器上工作时，这尤其有帮助。
- en: There’s more…
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: '`Pyreadr` is able to return multiple DataFrames. This is useful when we save
    several data objects in R as an `rdata` file. We can return all of them with one
    call.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pyreadr` 能够返回多个数据帧。当我们将多个数据对象保存为 `rdata` 文件时，这将非常有用。我们可以一次性返回它们所有。'
- en: '`Pprint` is a handy tool for improving the display of Python dictionaries.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pprint` 是改善 Python 字典显示的一个方便工具。'
- en: 'We could have used `rpy2` instead of `pyreadr` to import R data. `rpy2` requires
    that R also be installed, but it is more powerful than `pyreadr`. It will read
    R factors and automatically set them to pandas DataFrame values. See the following
    code:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用 `rpy2` 而不是 `pyreadr` 导入 R 数据。`rpy2` 要求也安装了 R，但它比 `pyreadr` 更强大。它将读取
    R 因子并自动将它们设置为 pandas DataFrame 值。参见以下代码：
- en: '[PRE127]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: This generates unusual *–2147483648* values. This is what happened when `readRDS`
    interpreted missing data in numeric columns. A global replacement of that number
    with `NaN`, after confirming that that is not a valid value, would be a good next
    step.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成异常的 *–2147483648* 值。这是当 `readRDS` 解释数值列中的缺失数据时发生的情况。确认该数值不是有效值后，将该数字全局替换为
    `NaN`，是一个不错的下一步。
- en: See also
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Clear instructions and examples for `pyreadr` are available at [https://github.com/ofajardo/pyreadr](https://github.com/ofajardo/pyreadr).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`pyreadr` 的清晰说明和示例可在 [https://github.com/ofajardo/pyreadr](https://github.com/ofajardo/pyreadr)
    查看。'
- en: Feather files, a relatively new format, can be read by both R and Python. I
    discuss those files in the next recipe.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: Feather 文件是一种相对较新的格式，可以被 R 和 Python 都读取。我将在下一个示例中讨论这些文件。
- en: Persisting tabular data
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化表格数据
- en: 'We persist data, copy it from memory to local or remote storage, for several
    reasons: to be able to access the data without having to repeat the steps we used
    to generate it, to share the data with others, or to make it available for use
    with different software. In this recipe, we save data that we have loaded into
    a pandas DataFrame as different file types (CSV, Excel, Pickle, and Feather).'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们持久化数据，将其从内存复制到本地或远程存储，有几个原因：为了能够在不重复生成数据的步骤的情况下访问数据，便于与他人共享数据，或者为了使数据能被不同的软件使用。在本食谱中，我们将加载到
    pandas DataFrame 中的数据保存为不同的文件类型（CSV、Excel、Pickle 和 Feather）。
- en: Another important, but sometimes overlooked, reason to persist data is to preserve
    some segment of our data that needs to be examined more closely; perhaps it needs
    to be scrutinized by others before our analysis can be completed. For analysts
    who work with operational data in medium- to large-sized organizations, this process
    is part of the daily data-cleaning workflow.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的，但有时被忽视的，持久化数据的原因是保存我们需要更仔细检查的某个数据片段；也许它需要在我们完成分析之前被其他人审查。对于在中型到大型组织中处理操作数据的分析师来说，这一过程是日常数据清理工作流程的一部分。
- en: 'In addition to these reasons for persisting data, our decisions about when
    and how to serialize data are shaped by several other factors: where we are in
    terms of our data analysis projects, the hardware and software resources of the
    machine(s) saving and reloading the data, and the size of our dataset. Analysts
    end up having to be much more intentional when saving data than they are when
    pressing *Ctrl* + *S* in their word-processing application.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些持久化数据的原因外，我们关于何时和如何序列化数据的决策还受到其他几个因素的影响：我们处于数据分析项目的哪个阶段，保存和重新加载数据的机器的硬件和软件资源，以及我们的数据集大小。分析师在保存数据时往往比在字处理应用程序中按
    *Ctrl* + *S* 时更需要谨慎。
- en: Once we persist data, it is stored separately from the logic that we used to
    create it. I find this to be one of the most important threats to the integrity
    of our analysis. Often, we end up loading data that we saved some time in the
    past (a week ago? A month ago? A year ago?) and forget how a variable was defined
    and how it relates to other variables. If we are in the middle of a data-cleaning
    task, it is best not to persist our data, so long as our workstation and network
    can easily handle the burden of regenerating the data. It is a good idea to persist
    data only once we have reached milestones in our work.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们持久化数据，它就会与我们用于创建它的逻辑分开存储。我认为这是对我们分析完整性构成的最重要威胁之一。我们常常会加载那些我们在过去保存的数据（一个星期前？一个月前？一年前？），却忘记了一个变量是如何定义的，它与其他变量的关系是什么。如果我们正处于数据清理的过程中，最好不要持久化数据，只要我们的工作站和网络能够轻松地处理数据的重新生成。只有在我们完成了工作的里程碑时，才应该持久化数据。
- en: Beyond the question of *when* to persist data, there is the question of *how*.
    If we are persisting it for our own reuse with the same software, it is best to
    save it in a binary format native to that software. That is pretty straightforward
    for tools such as SPSS, SAS, Stata, and R, but not so much for pandas. But that
    is good news in a way. We have lots of choices, from CSV and Excel to Pickle and
    Feather. We save as all these file types in this recipe.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*何时*持久化数据的问题，还有*如何*持久化数据的问题。如果我们是为了自己在相同软件中重新使用数据，最好将其保存为该软件本地的二进制格式。对于 SPSS、SAS、Stata
    和 R 等工具来说，这很简单，但对于 pandas 来说就没有那么容易了。不过从某种意义上来说，这也是好消息。我们有很多选择，从 CSV 和 Excel 到
    Pickle 和 Feather。在本食谱中，我们会将数据保存为所有这些文件类型。
- en: '**Note**'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Pickle and Feather are binary file formats that can be used to store pandas
    DataFrames.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: Pickle 和 Feather 是可以用来存储 pandas DataFrame 的二进制文件格式。
- en: Getting ready
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to install Feather if you do not have it on your system. You can
    do that by entering `pip install pyarrow` in a Terminal window or `powershell`
    (in Windows). If you do not already have a subfolder named `Views` in your `chapter
    1` folder, you will need to create it in order to run the code for this recipe.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统中没有安装 Feather，你需要安装它。可以通过在终端窗口或 Windows 的 `powershell` 中输入 `pip install
    pyarrow` 来安装。如果在你的 `chapter 1` 文件夹中没有名为 `Views` 的子文件夹，你需要创建一个，以便运行本食谱中的代码。
- en: '**Data note**'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: This dataset, taken from the Global Historical Climatology Network integrated
    database, is made available for public use by the United States National Oceanic
    and Atmospheric Administration at [https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly).
    I used the data from version 4\. The data in this recipe uses a 100,000-row sample
    of the full dataset, which is also available in the repository.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集来自全球历史气候网络集成数据库，由美国国家海洋和大气管理局提供公开使用，网址为[https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly)。我使用的是版本4的数据。本例中的数据使用的是完整数据集中的100,000行样本，完整数据集也可在仓库中获得。
- en: How to do it…
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We will load a CSV file into pandas and then save it as a Pickle and a Feather
    file. We will also save subsets of the data to the CSV and Excel formats:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将CSV文件加载到pandas中，然后将其保存为Pickle和Feather格式。我们还将数据的子集保存为CSV和Excel格式：
- en: Import `pandas` and `pyarrow`.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`和`pyarrow`。
- en: '`pyarrow` needs to be imported in order to save pandas to Feather:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 需要导入`pyarrow`才能将pandas数据保存为Feather格式：
- en: '[PRE129]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Load the land temperatures CSV file into pandas, drop rows with missing data,
    and set an index:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将土地温度的CSV文件加载到pandas中，删除包含缺失数据的行，并设置索引：
- en: '[PRE130]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: Write extreme values for `temperature` to CSV and Excel files.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`temperature`的极端值写入CSV和Excel文件。
- en: 'Use the `quantile` method to select outlier rows, which are those at the 1
    in 1,000 level at each end of the distribution:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`quantile`方法选择异常值行，这些行位于分布两端的千分之一水平：
- en: '[PRE133]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: Save to Pickle and Feather files.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存为Pickle和Feather文件。
- en: 'The index needs to be reset in order to save a Feather file:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保存Feather文件，需要重置索引：
- en: '[PRE138]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: Load the Pickle and Feather files we just saved.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载我们刚保存的Pickle和Feather文件。
- en: 'Note that our index was preserved when saving and loading the Pickle file:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当保存和加载Pickle文件时，我们的索引得到了保留：
- en: '[PRE139]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: The previous steps demonstrated how to serialize pandas DataFrames using two
    different formats, Pickle and Feather.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤展示了如何使用两种不同的格式，Pickle和Feather，序列化pandas的DataFrame。
- en: How it works...
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Persisting pandas data is quite straightforward. DataFrames have the `to_csv`,
    `to_excel`, `to_pickle`, and `to_feather` methods. Pickling preserves our index.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 持久化pandas数据非常简单。DataFrame有`to_csv`、`to_excel`、`to_pickle`和`to_feather`方法。Pickle格式能保留我们的索引。
- en: There’s more...
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: The advantage of storing data in CSV files is that saving it uses up very little
    additional memory. The disadvantage is that writing CSV files is quite slow, and
    we lose important metadata, such as data types. (`read_csv` can often figure out
    the data type when we reload the file, but not always.) Pickle files keep that
    data but can burden a system that is low on resources when serializing. Feather
    is easier on resources and can be easily loaded in R as well as Python, but we
    have to sacrifice our index in order to serialize. Also, the authors of Feather
    make no promises regarding long-term support.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 存储数据到CSV文件的优点是保存时几乎不占用额外的内存。缺点是写入CSV文件的速度比较慢，而且我们会失去重要的元数据，例如数据类型。(`read_csv`在重新加载文件时通常能识别数据类型，但并不总是如此。)
    Pickle文件能保留这些数据，但在序列化时可能会给资源较少的系统带来负担。Feather格式对资源的要求较低，且可以在R和Python中轻松加载，但为了序列化我们必须牺牲索引。此外，Feather的作者未对长期支持做出承诺。
- en: You may have noticed that I do not make a global recommendation about what to
    use for data serialization – other than to limit your persistence of full datasets
    to project milestones. This is definitely one of those “right tools for the right
    job” kind of situations. I use CSV or Excel files when I want to share a segment
    of a file with colleagues for discussion. I use Feather for ongoing Python projects,
    particularly when I am using a machine with sub-par RAM and an outdated chip and
    also using R. When I am wrapping up a project, I pickle the DataFrames.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我没有就数据序列化提出全球性的建议——除非是在项目里程碑时限制持久化完整数据集。这绝对是一个“对的工具用于对的工作”的情形。当我想与同事分享文件的一个片段进行讨论时，我使用CSV或Excel文件。当我进行持续的Python项目时，尤其是当我使用内存较小、处理器较旧并且还要使用R的机器时，我使用Feather格式。当我完成一个项目时，我会使用Pickle格式保存DataFrames。
- en: Summary
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Our Python data projects typically start with raw data stored in a range of
    formats and exported from a variety of software tools. Among the most popular
    tabular formats and tools are CSV and Excel files, SQL tables, and SPSS, Stata,
    SAS, and R datasets. We converted data from all of these sources into a pandas
    DataFrame in this chapter, and addressed the most common challenges. We also explored
    approaches to persisting tabular data. We will work with data in other formats
    in the next chapter.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Python数据项目通常从存储在各种格式中的原始数据开始，这些数据由多种软件工具导出。最常见的表格格式和工具包括CSV和Excel文件、SQL表格，以及SPSS、Stata、SAS和R数据集。在本章中，我们将所有这些来源的数据转换为pandas
    DataFrame，并解决了最常见的挑战。我们还探索了持久化表格数据的方法。在下一章中，我们将处理其他格式的数据。
- en: Join our community on Discord
  id: totrans-394
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://discord.gg/p8uSgEAETX](https://discord.gg/p8uSgEAETX )'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://discord.gg/p8uSgEAETX](https://discord.gg/p8uSgEAETX )'
- en: '![](img/QR_Code10336218961138498953.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code10336218961138498953.png)'
