- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: The continued growth in data coupled with the need to make increasingly complex
    decisions against that data is creating massive hurdles that prevent organizations
    from deriving insights in a timely manner using traditional analytical approaches.
    The field of big data has become so related to these frameworks that its scope
    is defined by what these frameworks can handle. Whether you're scrutinizing the
    clickstream from millions of visitors to optimize online ad placements, or sifting
    through billions of transactions to identify signs of fraud, the need for advanced
    analytics, such as machine learning and graph processing, to automatically glean
    insights from enormous volumes of data is more evident than ever.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据持续增长，加上对这些数据进行越来越复杂的决策的需求，正在创造巨大的障碍，阻止组织利用传统的分析方法及时获取洞察力。大数据领域与这些框架密切相关，其范围由这些框架能处理的内容来定义。无论您是在审查数百万访问者的点击流以优化在线广告位置，还是在筛选数十亿交易以识别欺诈迹象，对于从海量数据中自动获取洞察力的高级分析（如机器学习和图处理）的需求比以往任何时候都更加明显。
- en: Apache Spark, the de facto standard for big data processing, analytics, and
    data sciences across all academia and industries, provides both machine learning
    and graph processing libraries, allowing companies to tackle complex problems
    easily with the power of highly scalable and clustered computers. Spark's promise
    is to take this a little further to make writing distributed programs using Scala
    feel like writing regular programs for Spark. Spark will be great in giving ETL
    pipelines huge boosts in performance and easing some of the pain that feeds the
    MapReduce programmer's daily chant of despair to the Hadoop gods.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark，作为大数据处理、分析和数据科学在所有学术界和行业中的事实标准，提供了机器学习和图处理库，使公司能够轻松应对复杂问题，利用高度可扩展和集群化的计算机的强大能力。Spark的承诺是进一步推动使用Scala编写分布式程序感觉像为Spark编写常规程序。Spark将在提高ETL管道性能和减轻一些痛苦方面做得很好，这些痛苦来自MapReduce程序员每天对Hadoop神明的绝望呼唤。
- en: In this book, we used Spark and Scala for the endeavor to bring state-of-the-art
    advanced data analytics with machine learning, graph processing, streaming, and
    SQL to Spark, with their contributions to MLlib, ML, SQL, GraphX, and other libraries.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们使用Spark和Scala进行努力，将最先进的高级数据分析与机器学习、图处理、流处理和SQL引入Spark，并将它们贡献给MLlib、ML、SQL、GraphX和其他库。
- en: We started with Scala and then moved to the Spark part, and finally, covered
    some advanced topics for big data analytics with Spark and Scala. In the appendix,
    we will see how to extend your Scala knowledge for SparkR, PySpark, Apache Zeppelin,
    and in-memory Alluxio. This book isn't meant to be read from cover to cover. Skip
    to a chapter that looks like something you're trying to accomplish or that simply
    ignites your interest.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Scala开始，然后转向Spark部分，最后，涵盖了一些关于使用Spark和Scala进行大数据分析的高级主题。在附录中，我们将看到如何扩展您的Scala知识，以用于SparkR、PySpark、Apache
    Zeppelin和内存中的Alluxio。本书不是要从头到尾阅读的。跳到一个看起来像您要完成的任务或简单激起您兴趣的章节。
- en: Happy reading!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 祝阅读愉快！
- en: What this book covers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容
- en: '[Chapter 1](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c), *Introduction
    to Scala*, will teach big data analytics using the Scala-based APIs of Spark.
    Spark itself is written with Scala and naturally, as a starting point, we will
    discuss a brief introduction to Scala, such as the basic aspects of its history,
    purposes, and how to install Scala on Windows, Linux, and Mac OS. After that,
    the Scala web framework will be discussed in brief. Then, we will provide a comparative
    analysis of Java and Scala. Finally, we will dive into Scala programming to get
    started with Scala.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c)，*Scala简介*，将教授使用基于Scala的Spark
    API进行大数据分析。Spark本身是用Scala编写的，因此作为起点，我们将讨论Scala的简要介绍，例如其历史、目的以及如何在Windows、Linux和Mac
    OS上安装Scala。之后，将简要讨论Scala web框架。然后，我们将对Java和Scala进行比较分析。最后，我们将深入Scala编程，开始使用Scala。'
- en: '[Chapter 2](part0058.html#1NA0K1-21aec46d8593429cacea59dbdcd64e1c), *Object-Oriented
    Scala*, says that the object-oriented programming (OOP) paradigm provides a whole
    new layer of abstraction. In short, this chapter discusses some of the greatest
    strengths of OOP languages: discoverability, modularity, and extensibility. In
    particular, we will see how to deal with variables in Scala; methods, classes,
    and objects in Scala; packages and package objects; traits and trait linearization;
    and Java interoperability.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[第2章](part0058.html#1NA0K1-21aec46d8593429cacea59dbdcd64e1c)，*面向对象的Scala*，说道面向对象编程（OOP）范式提供了全新的抽象层。简而言之，本章讨论了面向对象编程语言的一些最大优势：可发现性、模块化和可扩展性。特别是，我们将看到如何处理Scala中的变量；Scala中的方法、类和对象；包和包对象；特征和特征线性化；以及Java互操作性。'
- en: '[Chapter 3](part0093.html#2OM4A1-21aec46d8593429cacea59dbdcd64e1c), *Functional
    Programming Concepts*, showcases the functional programming concepts in Scala.
    More specifically, we will learn several topics, such as why Scala is an arsenal
    for the data scientist, why it is important to learn the Spark paradigm, pure
    functions, and higher-order functions (HOFs). A real-life use case using HOFs
    will be shown too. Then, we will see how to handle exceptions in higher-order
    functions outside of collections using the standard library of Scala. Finally,
    we will look at how functional Scala affects an object''s mutability.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](part0093.html#2OM4A1-21aec46d8593429cacea59dbdcd64e1c)，*函数式编程概念*，展示了Scala中的函数式编程概念。更具体地，我们将学习几个主题，比如为什么Scala是数据科学家的武器库，为什么学习Spark范式很重要，纯函数和高阶函数（HOFs）。还将展示使用HOFs的实际用例。然后，我们将看到如何在Scala的标准库中处理高阶函数在集合之外的异常。最后，我们将看看函数式Scala如何影响对象的可变性。'
- en: '[Chapter4](part0117.html#3FIHQ1-21aec46d8593429cacea59dbdcd64e1c), *Collection
    APIs*, introduces one of the features that attract most Scala users--the Collections
    API. It''s very powerful and flexible, and has lots of operations coupled. We
    will also demonstrate the capabilities of the Scala Collection API and how it
    can be used in order to accommodate different types of data and solve a wide range
    of different problems. In this chapter, we will cover Scala collection APIs, types
    and hierarchy, some performance characteristics, Java interoperability, and Scala
    implicits.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章《集合API》介绍了吸引大多数Scala用户的功能之一——集合API。它非常强大和灵活，并且具有许多相关操作。我们还将演示Scala集合API的功能以及如何使用它来适应不同类型的数据并解决各种不同的问题。在本章中，我们将涵盖Scala集合API、类型和层次结构、一些性能特征、Java互操作性以及Scala隐式。
- en: '[Chapter 5](part0148.html#4D4J81-21aec46d8593429cacea59dbdcd64e1c), *Tackle
    Big Data - Spark Comes to the Party,* outlines data analysis and big data; we
    see the challenges that big data poses, how they are dealt with by distributed
    computing, and the approaches suggested by functional programming. We introduce
    Google''s MapReduce, Apache Hadoop, and finally, Apache Spark, and see how they
    embraced this approach and these techniques. We will look into the evolution of
    Apache Spark: why Apache Spark was created in the first place and the value it
    can bring to the challenges of big data analytics and processing.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章《应对大数据 - Spark加入派对》概述了数据分析和大数据；我们看到大数据带来的挑战，以及它们是如何通过分布式计算来处理的，以及函数式编程提出的方法。我们介绍了谷歌的MapReduce、Apache
    Hadoop，最后是Apache Spark，并看到它们是如何采纳这种方法和这些技术的。我们将探讨Apache Spark的演变：为什么首先创建了Apache
    Spark以及它如何为大数据分析和处理的挑战带来价值。
- en: '[Chapter 6](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c), *Start
    Working with Spark - REPL and RDDs,* covers how Spark works; then, we introduce
    RDDs, the basic abstractions behind Apache Spark, and see that they are simply
    distributed collections exposing Scala-like APIs. We will look at the deployment
    options for Apache Spark and run it locally as a Spark shell. We will learn the
    internals of Apache Spark, what RDDs are, DAGs and lineages of RDDs, Transformations,
    and Actions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章《开始使用Spark - REPL和RDDs》涵盖了Spark的工作原理；然后，我们介绍了RDDs，这是Apache Spark背后的基本抽象，看到它们只是暴露类似Scala的API的分布式集合。我们将研究Apache
    Spark的部署选项，并在本地运行它作为Spark shell。我们将学习Apache Spark的内部工作原理，RDD是什么，RDD的DAG和谱系，转换和操作。
- en: '[Chapter 7](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c), *Special
    RDD Operations,* focuses on how RDDs can be tailored to meet different needs,
    and how these RDDs provide new functionalities (and dangers!) Moreover, we investigate
    other useful objects that Spark provides, such as broadcast variables and Accumulators.
    We will learn aggregation techniques, shuffling.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第7章《特殊RDD操作》着重介绍了如何定制RDD以满足不同的需求，以及这些RDD提供了新的功能（和危险！）此外，我们还研究了Spark提供的其他有用对象，如广播变量和累加器。我们将学习聚合技术、洗牌。
- en: '[Chapter 8](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c), *Introduce
    a Little Structure - SparkSQL,* teaches how to use Spark for the analysis of structured
    data as a higher-level abstraction of RDDs and how Spark SQL''s APIs make querying
    structured data simple yet robust. Moreover, we introduce datasets and look at
    the differences between datasets, DataFrames, and RDDs. We will also learn to
    join operations and window functions to do complex data analysis using DataFrame
    APIs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章《引入一点结构 - SparkSQL》教您如何使用Spark分析结构化数据，作为RDD的高级抽象，以及Spark SQL的API如何使查询结构化数据变得简单而健壮。此外，我们介绍数据集，并查看数据集、数据框架和RDD之间的区别。我们还将学习使用数据框架API进行复杂数据分析的连接操作和窗口函数。
- en: '[Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c), *Stream
    Me Up, Scotty - Spark Streaming,* takes you through Spark Streaming and how we
    can take advantage of it to process streams of data using the Spark API. Moreover,
    in this chapter, the reader will learn various ways of processing real-time streams
    of data using a practical example to consume and process tweets from Twitter.
    We will look at integration with Apache Kafka to do real-time processing. We will
    also look at structured streaming, which can provide real-time queries to your
    applications.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第9章《带我上流 - Spark Streaming》带您了解Spark Streaming以及我们如何利用它来使用Spark API处理数据流。此外，在本章中，读者将学习使用实际示例处理实时数据流的各种方法，以消费和处理来自Twitter的推文。我们将研究与Apache
    Kafka的集成以进行实时处理。我们还将研究结构化流，它可以为您的应用程序提供实时查询。
- en: '[Chapter 10](part0326.html#9MSNC1-21aec46d8593429cacea59dbdcd64e1c), *Everything
    is Connected - GraphX,* in this chapter, we learn how many real-world problems
    can be modeled (and resolved) using graphs. We will look at graph theory using
    Facebook as an example, Apache Spark''s graph processing library GraphX, VertexRDD
    and EdgeRDDs, graph operators, aggregateMessages, TriangleCounting, the Pregel
    API, and use cases such as the PageRank algorithm.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第10章《一切都相连 - GraphX》中，我们将学习许多现实世界的问题可以使用图来建模（和解决）。我们将以Facebook为例看图论，Apache Spark的图处理库GraphX，VertexRDD和EdgeRDDs，图操作符，aggregateMessages，TriangleCounting，Pregel
    API以及PageRank算法等用例。
- en: '[Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c), *Learning
    Machine Learning - Spark MLlib and ML*, the purpose of this chapter is to provide
    a conceptual introduction to statistical machine learning. We will focus on Spark''s
    machine learning APIs, called Spark MLlib and ML. We will then discuss how to
    solve classification tasks using decision trees and random forest algorithms and
    regression problem using linear regression algorithm. We will also show how we
    could benefit from using one-hot encoding and dimensionality reductions algorithms
    in feature extraction before training a classification model. In later sections,
    we will show a step-by-step example of developing a collaborative filtering-based
    movie recommendation system.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第11章，“学习机器学习-Spark MLlib和ML”，本章的目的是提供统计机器学习的概念介绍。我们将重点介绍Spark的机器学习API，称为Spark
    MLlib和ML。然后我们将讨论如何使用决策树和随机森林算法解决分类任务，以及使用线性回归算法解决回归问题。我们还将展示在训练分类模型之前如何从使用独热编码和降维算法在特征提取中受益。在后面的部分，我们将逐步展示开发基于协同过滤的电影推荐系统的示例。
- en: '[Chapter 12](part0383.html#BD87E1-21aec46d8593429cacea59dbdcd64e1c), *Advanced
    Machine Learning Best Practices*, provides theoretical and practical aspects of
    some advanced topics of machine learning with Spark. We will see how to tune machine
    learning models for optimized performance using grid search, cross-validation,
    and hyperparameter tuning. In a later section, we will cover how to develop a
    scalable recommendation system using ALS, which is an example of a model-based
    recommendation algorithm. Finally, a topic modelling application will be demonstrated
    as a text clustering technique'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第12章，“高级机器学习最佳实践”，提供了一些关于使用Spark进行机器学习的高级主题的理论和实践方面。我们将看到如何使用网格搜索、交叉验证和超参数调整来调整机器学习模型以获得最佳性能。在后面的部分，我们将介绍如何使用ALS开发可扩展的推荐系统，这是一个基于模型的推荐算法的示例。最后，将演示主题建模应用作为文本聚类技术。
- en: '[Chapter 13](part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c), *My Name
    is Bayes, Naive Bayes,* states that machine learning in big data is a radical
    combination that has created great impact in the field of research, in both academia
    and industry. Big data imposes great challenges on ML, data analytics tools, and
    algorithms to find the real value. However, making a future prediction based on
    these huge datasets has never been easy. Considering this challenge, in this chapter,
    we will dive deeper into ML and find out how to use a simple yet powerful method
    to build a scalable classification model and concepts such as multinomial classification,
    Bayesian inference, Naive Bayes, decision trees, and a comparative analysis of
    Naive Bayes versus decision trees.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第13章，“我的名字是贝叶斯，朴素贝叶斯”，指出大数据中的机器学习是一个革命性的组合，对学术界和工业界的研究领域产生了巨大影响。大数据对机器学习、数据分析工具和算法提出了巨大挑战，以找到真正的价值。然而，基于这些庞大数据集进行未来预测从未容易。考虑到这一挑战，在本章中，我们将深入探讨机器学习，了解如何使用简单而强大的方法构建可扩展的分类模型，以及多项式分类、贝叶斯推断、朴素贝叶斯、决策树和朴素贝叶斯与决策树的比较分析等概念。
- en: '[Chapter 14](part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c), *Time
    to Put Some Order - Cluster Your Data with Spark MLlib,* gets you started on how
    Spark works in cluster mode with its underlying architecture. In previous chapters,
    we saw how to develop practical applications using different Spark APIs. Finally,
    we will see how to deploy a full Spark application on a cluster, be it with a
    pre-existing Hadoop installation or without.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第14章，“整理数据的时候到了-Spark MLlib对数据进行聚类”，让您了解Spark在集群模式下的工作原理及其基础架构。在之前的章节中，我们看到了如何使用不同的Spark
    API开发实际应用程序。最后，我们将看到如何在集群上部署完整的Spark应用程序，无论是使用现有的Hadoop安装还是不使用。
- en: '[Chapter 15](part0458.html#DKP1K1-21aec46d8593429cacea59dbdcd64e1c), *Text
    Analytics Using Spark ML,* outlines the wonderful field of text analytics using
    Spark ML. Text analytics is a wide area in machine learning and is useful in many
    use cases, such as sentiment analysis, chat bots, email spam detection, natural
    language processing, and many many more. We will learn how to use Spark for text
    analysis with a focus on use cases of text classification using a 10,000 sample
    set of Twitter data. We will also look at LDA, a popular technique to generate
    topics from documents without knowing much about the actual text, and will implement
    text classification on Twitter data to see how it all comes together.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第15章，“使用Spark ML进行文本分析”，概述了使用Spark ML进行文本分析的广泛领域。文本分析是机器学习中的一个广泛领域，在许多用例中非常有用，例如情感分析、聊天机器人、电子邮件垃圾邮件检测、自然语言处理等。我们将学习如何使用Spark进行文本分析，重点关注使用包含1万个样本的Twitter数据集进行文本分类的用例。我们还将研究LDA，这是一种从文档中生成主题的流行技术，而不需要了解实际文本内容，并将在Twitter数据上实现文本分类，以了解所有内容是如何结合在一起的。
- en: '[Chapter 16](part0480.html#E9OE01-21aec46d8593429cacea59dbdcd64e1c), *Spark
    Tuning,* digs deeper into Apache Spark internals and says that while Spark is
    great in making us feel as if we are using just another Scala collection, we shouldn''t
    forget that Spark actually runs in a distributed system. Therefore, throughout
    this chapter, we will cover how to monitor Spark jobs, Spark configuration, common
    mistakes in Spark app development, and some optimization techniques.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第16章，“Spark调优”，深入挖掘Apache Spark内部，并表示虽然Spark在让我们感觉好像只是使用另一个Scala集合方面做得很好，但我们不应忘记Spark实际上是在分布式系统中运行。因此，在本章中，我们将介绍如何监视Spark作业、Spark配置、Spark应用程序开发中的常见错误以及一些优化技术。
- en: '[Chapter 17](part0511.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c), *Time
    to Go to ClusterLand - Deploying Spark on a Cluster,* explores how Spark works
    in cluster mode with its underlying architecture. We will see Spark architecture
    in a cluster, the Spark ecosystem and cluster management, and how to deploy Spark
    on standalone, Mesos, Yarn, and AWS clusters. We will also see how to deploy your
    app on a cloud-based AWS cluster.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[第17章](part0511.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c)，*去集群之旅-在集群上部署Spark*，探讨了Spark在集群模式下的工作方式及其基础架构。我们将看到集群中的Spark架构，Spark生态系统和集群管理，以及如何在独立、Mesos、Yarn和AWS集群上部署Spark。我们还将看到如何在基于云的AWS集群上部署您的应用程序。'
- en: '[Chapter 18](part0550.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c), *Testing
    and Debugging Spark,* explains how difficult it can be to test an application
    if it is distributed; then, we see some ways to tackle this. We will cover how
    to do testing in a distributed environment, and testing and debugging Spark applications.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[第18章](part0550.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c)，*测试和调试Spark*，解释了在分布式环境中测试应用程序有多么困难；然后，我们将看到一些解决方法。我们将介绍如何在分布式环境中进行测试，以及测试和调试Spark应用程序。'
- en: '[Chapter 19](part0571.html#H0HH61-21aec46d8593429cacea59dbdcd64e1c), *PySpark
    & SparkR,* covers the other two popular APIs for writing Spark code using R and
    Python, that is, PySpark and SparkR. In particular, we will cover how to get started
    with PySpark and interacting with DataFrame APIs and UDFs with PySpark, and then
    we will do some data analytics using PySpark. The second part of this chapter
    covers how to get started with SparkR. We will also see how to do data processing
    and manipulation, and how to work with RDD and DataFrames using SparkR, and finally,
    some data visualization using SparkR.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[第19章](part0571.html#H0HH61-21aec46d8593429cacea59dbdcd64e1c)，*PySpark和SparkR*，涵盖了使用R和Python编写Spark代码的另外两种流行API，即PySpark和SparkR。特别是，我们将介绍如何开始使用PySpark并与PySpark交互DataFrame
    API和UDF，然后我们将使用PySpark进行一些数据分析。本章的第二部分涵盖了如何开始使用SparkR。我们还将看到如何进行数据处理和操作，以及如何使用SparkR处理RDD和DataFrames，最后，使用SparkR进行一些数据可视化。'
- en: '[Appendix A](part0593.html#HLGTI1-21aec46d8593429cacea59dbdcd64e1c), *Accelerating
    Spark with Alluxio*, shows how to use Alluxio with Spark to increase the speed
    of processing. Alluxio is an open source distributed memory storage system useful
    for increasing the speed of many applications across platforms, including Apache
    Spark. We will explore the possibilities of using Alluxio and how Alluxio integration
    will provide greater performance without the need to cache the data in memory
    every time we run a Spark job.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[附录A](part0593.html#HLGTI1-21aec46d8593429cacea59dbdcd64e1c)，*使用Alluxio加速Spark*，展示了如何使用Alluxio与Spark来提高处理速度。Alluxio是一个开源的分布式内存存储系统，可用于提高跨平台的许多应用程序的速度，包括Apache
    Spark。我们将探讨使用Alluxio的可能性以及Alluxio集成如何在运行Spark作业时提供更高的性能而无需每次都将数据缓存到内存中。'
- en: '[Appendix B](part0612.html#I7KO81-21aec46d8593429cacea59dbdcd64e1c), *Interactive
    Data Analytics with Apache Zepp**e**lin*, says that from a data science perspective,
    interactive visualization of your data analysis is also important. Apache Zeppelin
    is a web-based notebook for interactive and large-scale data analytics with multiple
    backends and interpreters. In this chapter, we will discuss how to use Apache
    Zeppelin for large-scale data analytics using Spark as the interpreter in the
    backend.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[附录B](part0612.html#I7KO81-21aec46d8593429cacea59dbdcd64e1c)，*使用Apache Zeppelin进行交互式数据分析*，从数据科学的角度来看，交互式可视化数据分析也很重要。Apache
    Zeppelin是一个基于Web的笔记本，用于具有多个后端和解释器的交互式和大规模数据分析。在本章中，我们将讨论如何使用Apache Zeppelin进行大规模数据分析，使用Spark作为后端的解释器。'
- en: What you need for this book
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书所需的内容
- en: 'All the examples have been implemented using Python version 2.7 and 3.5 on
    an Ubuntu Linux 64 bit, including the TensorFlow library version 1.0.1\. However,
    in the book, we showed the source code with only Python 2.7 compatible. Source
    codes that are Python 3.5+ compatible can be downloaded from the Packt repository.
    You will also need the following Python modules (preferably the latest versions):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有示例都是在Ubuntu Linux 64位上使用Python版本2.7和3.5实现的，包括TensorFlow库版本1.0.1。然而，在本书中，我们只展示了与Python
    2.7兼容的源代码。与Python 3.5+兼容的源代码可以从Packt存储库下载。您还需要以下Python模块（最好是最新版本）：
- en: Spark 2.0.0 (or higher)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0.0（或更高）
- en: Hadoop 2.7 (or higher)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 2.7（或更高）
- en: Java (JDK and JRE) 1.7+/1.8+
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java（JDK和JRE）1.7+/1.8+
- en: Scala 2.11.x (or higher)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala 2.11.x（或更高）
- en: Python 2.7+/3.4+
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 2.7+/3.4+
- en: R 3.1+ and RStudio 1.0.143 (or higher)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 3.1+和RStudio 1.0.143（或更高）
- en: Eclipse Mars, Oxygen, or Luna (latest)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eclipse Mars，Oxygen或Luna（最新）
- en: Maven Eclipse plugin (2.9 or higher)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maven Eclipse插件（2.9或更高）
- en: Maven compiler plugin for Eclipse (2.3.2 or higher)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eclipse的Maven编译器插件（2.3.2或更高）
- en: Maven assembly plugin for Eclipse (2.4.1 or higher)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eclipse的Maven汇编插件（2.4.1或更高）
- en: '**Operating system:** Linux distributions are preferable (including Debian,
    Ubuntu, Fedora, RHEL, and CentOS) and to be more specific, for Ubuntu it is recommended
    to have a complete 14.04 (LTS) 64-bit (or later) installation, VMWare player 12,
    or Virtual box. You can run Spark jobs on Windows (XP/7/8/10) or Mac OS X (10.4.7+).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**操作系统：**首选Linux发行版（包括Debian，Ubuntu，Fedora，RHEL和CentOS），更具体地说，对于Ubuntu，建议安装完整的14.04（LTS）64位（或更高版本），VMWare
    player 12或Virtual box。您可以在Windows（XP/7/8/10）或Mac OS X（10.4.7+）上运行Spark作业。'
- en: '**Hardware configuration:** Processor Core i3, Core i5 (recommended), or Core
    i7 (to get the best results). However, multicore processing will provide faster
    data processing and scalability. You will need least 8-16 GB RAM (recommended)
    for a standalone mode and at least 32 GB RAM for a single VM--and higher for cluster.
    You will also need enough storage for running heavy jobs (depending on the dataset
    size you will be handling), and preferably at least 50 GB of free disk storage
    (for standalone word missing and for an SQL warehouse).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**硬件配置：**处理器Core i3，Core i5（推荐）或Core i7（以获得最佳结果）。然而，多核处理将提供更快的数据处理和可伸缩性。您至少需要8-16
    GB RAM（推荐）以独立模式运行，至少需要32 GB RAM以单个VM运行-并且对于集群来说需要更高。您还需要足够的存储空间来运行繁重的作业（取决于您处理的数据集大小），最好至少有50
    GB的免费磁盘存储空间（用于独立的单词丢失和SQL仓库）。'
- en: Who this book is for
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书适合谁
- en: Anyone who wishes to learn how to perform data analysis by harnessing the power
    of Spark will find this book extremely useful. No knowledge of Spark or Scala
    is assumed, although prior programming experience (especially with other JVM languages)
    will be useful in order to pick up the concepts quicker. Scala has been observing
    a steady rise in adoption over the past few years, especially in the fields of
    data science and analytics. Going hand in hand with Scala is Apache Spark, which
    is programmed in Scala and is widely used in the field of analytics. This book
    will help you leverage the power of both these tools to make sense of big data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 任何希望通过利用Spark的力量来学习数据分析的人都会发现这本书非常有用。我们不假设您具有Spark或Scala的知识，尽管先前的编程经验（特别是使用其他JVM语言）将有助于更快地掌握这些概念。在过去几年中，Scala的采用率一直在稳步上升，特别是在数据科学和分析领域。与Scala齐头并进的是Apache
    Spark，它是用Scala编程的，并且在分析领域被广泛使用。本书将帮助您利用这两种工具的力量来理解大数据。
- en: Conventions
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 约定
- en: 'In this book, you will find a number of text styles that distinguish between
    different kinds of information. Here are some examples of these styles and an
    explanation of their meaning. Code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles are shown as follows: "The next lines of code read the link and assign
    it to the to the `BeautifulSoup` function."'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，您将找到一些区分不同信息类型的文本样式。以下是一些这些样式的示例及其含义的解释。文本中的代码词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟URL、用户输入和Twitter句柄显示如下：“下一行代码读取链接并将其分配给`BeautifulSoup`函数。”
- en: 'A block of code is set as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块设置如下：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望引起您对代码块的特定部分的注意时，相关的行或项目将以粗体显示：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出都以以下方式编写：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**New terms**and **important words** are shown in bold. Words that you see
    on the screen, for example, in menus or dialog boxes, appear in the text like
    this: "Clicking the Next button moves you to the next screen."'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**新术语**和**重要单词**以粗体显示。您在屏幕上看到的单词，例如菜单或对话框中的单词，会以这种方式出现在文本中：“单击“下一步”按钮将您移至下一个屏幕。”'
- en: Warnings or important notes appear like this.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要说明会以这种方式出现。
- en: Tips and tricks appear like this.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 提示和技巧会以这种方式出现。
- en: Reader feedback
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读者反馈
- en: Feedback from our readers is always welcome. Let us know what you think about
    this book-what you liked or disliked. Reader feedback is important for us as it
    helps us develop titles that you will really get the most out of. To send us general
    feedback, simply e-mail `feedback@packtpub.com`, and mention the book's title
    in the subject of your message. If there is a topic that you have expertise in
    and you are interested in either writing or contributing to a book, see our author
    guide at [www.packtpub.com/authors](http://www.packtpub.com/authors).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。让我们知道您对本书的看法-您喜欢或不喜欢什么。读者的反馈对我们很重要，因为它可以帮助我们开发您真正能够充分利用的标题。要向我们发送一般反馈，只需发送电子邮件至`feedback@packtpub.com`，并在主题中提及书名。如果您在某个专题上有专业知识，并且有兴趣撰写或为书籍做出贡献，请参阅我们的作者指南[www.packtpub.com/authors](http://www.packtpub.com/authors)。
- en: Customer support
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户支持
- en: Now that you are the proud owner of a Packt book, we have a number of things
    to help you to get the most from your purchase.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您是Packt书籍的自豪所有者，我们有一些事情可以帮助您充分利用您的购买。
- en: Downloading the example code
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码
- en: 'You can download the example code files for this book from your account at
    [http://www.packtpub.com](http://www.packtpub.com). If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you. You can download the
    code files by following these steps:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[http://www.packtpub.com](http://www.packtpub.com)的帐户中下载本书的示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，文件将直接通过电子邮件发送给您。您可以按照以下步骤下载代码文件：
- en: Log in or register to our website using your e-mail address and password.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您的电子邮件地址和密码登录或注册到我们的网站。
- en: Hover the mouse pointer on the SUPPORT tab at the top.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将鼠标指针悬停在顶部的SUPPORT标签上。
- en: Click on Code Downloads & Errata.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“代码下载和勘误”。
- en: Enter the name of the book in the Search box.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索框中输入书名。
- en: Select the book for which you're looking to download the code files.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您要下载代码文件的书籍。
- en: Choose from the drop-down menu where you purchased this book from.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从下拉菜单中选择您购买此书的地点。
- en: Click on Code Download.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“代码下载”。
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下载文件后，请确保使用以下最新版本的解压缩或提取文件夹：
- en: WinRAR / 7-Zip for Windows
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WinRAR / 7-Zip for Windows
- en: Zipeg / iZip / UnRarX for Mac
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zipeg / iZip / UnRarX for Mac
- en: 7-Zip / PeaZip for Linux
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 7-Zip / PeaZip for Linux
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Scala-and-Spark-for-Big-Data-Analytics](https://github.com/PacktPublishing/Scala-and-Spark-for-Big-Data-Analytics).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码包也托管在GitHub上，网址为[https://github.com/PacktPublishing/Scala-and-Spark-for-Big-Data-Analytics](https://github.com/PacktPublishing/Scala-and-Spark-for-Big-Data-Analytics)。我们还有其他丰富的图书和视频代码包，可在[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)获取。快去看看吧！
- en: Downloading the color images of this book
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载本书的彩色图片
- en: We also provide you with a PDF file that has color images of the screenshots/diagrams
    used in this book. The color images will help you better understand the changes
    in the output. You can download this file from [https://www.packtpub.com/sites/default/files/downloads/ScalaandSparkforBigDataAnalytics_ColorImages.pdf](https://www.packtpub.com/sites/default/files/downloads/ScalaandSparkforBigDataAnalytics_ColorImages.pdf)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为您提供了一个PDF文件，其中包含本书中使用的截图/图表的彩色图片。彩色图片将帮助您更好地理解输出中的变化。您可以从[https://www.packtpub.com/sites/default/files/downloads/ScalaandSparkforBigDataAnalytics_ColorImages.pdf](https://www.packtpub.com/sites/default/files/downloads/ScalaandSparkforBigDataAnalytics_ColorImages.pdf)下载此文件。
- en: Errata
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 勘误
- en: Although we have taken every care to ensure the accuracy of our content, mistakes
    do happen. If you find a mistake in one of our books-maybe a mistake in the text
    or the code-we would be grateful if you could report this to us. By doing so,
    you can save other readers from frustration and help us improve subsequent versions
    of this book. If you find any errata, please report them by visiting [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details of your errata. Once your errata are verified, your submission will
    be accepted and the errata will be uploaded to our website or added to any list
    of existing errata under the Errata section of that title. To view the previously
    submitted errata, go to [https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)
    and enter the name of the book in the search field. The required information will
    appear under the Errata section.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经非常注意确保内容的准确性，但错误还是会发生。如果您在我们的书中发现错误——可能是文本或代码中的错误，我们将不胜感激，如果您能向我们报告。通过这样做，您可以帮助其他读者避免挫折，并帮助我们改进本书的后续版本。如果您发现任何勘误，请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)报告，选择您的书，点击勘误提交表单链接，并输入您的勘误详情。一旦您的勘误经过验证，您的提交将被接受，并且勘误将被上传到我们的网站或添加到该标题的勘误部分下的任何现有勘误列表中。要查看以前提交的勘误，请转到[https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)，并在搜索框中输入书名。所需的信息将出现在勘误部分下。
- en: Piracy
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 盗版
- en: Piracy of copyrighted material on the Internet is an ongoing problem across
    all media. At Packt, we take the protection of our copyright and licenses very
    seriously. If you come across any illegal copies of our works in any form on the
    Internet, please provide us with the location address or website name immediately
    so that we can pursue a remedy. Please contact us at `copyright@packtpub.com`
    with a link to the suspected pirated material. We appreciate your help in protecting
    our authors and our ability to bring you valuable content.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网上的版权材料盗版是所有媒体的持续问题。在Packt，我们非常重视版权和许可的保护。如果您在互联网上发现我们作品的任何非法副本，请立即向我们提供位置地址或网站名称，以便我们采取补救措施。请通过`copyright@packtpub.com`与我们联系，并附上涉嫌盗版材料的链接。感谢您帮助我们保护作者和我们提供有价值内容的能力。
- en: Questions
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: If you have a problem with any aspect of this book, you can contact us at `questions@packtpub.com`,
    and we will do our best to address the problem.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对本书的任何方面有问题，可以通过`questions@packtpub.com`与我们联系，我们将尽力解决问题。
