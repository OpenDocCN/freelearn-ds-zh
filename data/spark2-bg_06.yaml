- en: Chapter 6.  Spark Stream Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 Spark流处理
- en: Data processing use cases can be mainly divided into two types. The first type
    is the use cases where the data is static and processing is done in its entirety
    as one unit of work, or by dividing it into smaller batches. While doing the data
    processing, the underlying data set does not change nor do new data sets get added
    to the processing units. This is batch processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理用例主要可以分为两种类型。第一种类型是数据静态，处理作为一个工作单元或分成更小的批次进行。在数据处理过程中，底层数据集不会改变，也不会有新的数据集添加到处理单元中。这是批处理。
- en: The second type is the use cases where the data is getting generated like a
    stream, and the processing is done as and when the data is generated. This is
    stream processing. In the previous chapters of this book, all the data processing
    use cases were pertaining to the former type. This chapter is going to focus on
    the latter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种类型是数据像流水一样生成，处理随着数据生成而进行。这就是流处理。在本书的前几章中，所有数据处理用例都属于前一种类型。本章将关注后者。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Data stream processing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据流处理
- en: Micro batch data processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微批数据处理
- en: A log event processor
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志事件处理器
- en: Windowed data processing and other options
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口数据处理及其他选项
- en: Kafka stream processing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka流处理
- en: Streaming jobs with Spark
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行流作业
- en: Data stream processing
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据流处理
- en: Data sources generate data like a stream, and many real-world use cases require
    them to be processed in real time. The meaning of *real time* can change from
    use case to use case. The main parameter that defines what is meant by real time
    for a given use case is how soon the ingested data or the frequent interval in
    which all the data ingested since the last interval needs to be processed. For
    example, when a major sports event is happening, the application that consumes
    the score events and sends them to the subscribed users should be processing the
    data as fast as it can. The faster can be sent, the better it is.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源生成数据如同流水，许多现实世界的用例要求它们实时处理。*实时*的含义因用例而异。定义特定用例中实时含义的主要参数是，从上次间隔以来摄取的数据或频繁间隔需要多快处理。例如，当重大体育赛事进行时，消费比分事件并将其发送给订阅用户的应用程序应尽可能快地处理数据。发送得越快，效果越好。
- en: But what is the definition of *fast* here? Is it fine to process the score data
    within, say, an hour of the score event happening? Probably not. Is it fine to
    process the data within a minute of the score event happening? It is definitely
    better than processing after an hour. Is it fine to process the data within a
    second of the score event happening? Probably yes, and much better than the earlier
    data processing time intervals.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但这里的*快*是什么定义呢？在比分事件发生后一小时内处理比分数据是否可以？可能不行。在比分事件发生后一分钟内处理数据是否可以？这肯定比一小时内处理要好。在比分事件发生后一秒内处理数据是否可以？可能可以，并且比之前的数据处理时间间隔要好得多。
- en: In any data stream processing use cases, this time interval is very important.
    The data processing framework should have the capability to process the data stream
    in an appropriate time interval of choice to deliver good business value.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何数据流处理用例中，这个时间间隔都非常重要。数据处理框架应具备在自选的适当时间间隔内处理数据流的能力，以提供良好的商业价值。
- en: When processing stream data in regular intervals of choice, the data is collected
    from the beginning of the time interval to the end of the time interval, grouped
    in a micro batch, and data processing is done on that batch of data. Over an extended
    period of time, the data processing application would have processed many such
    micro batches of data. In this type of processing, the data processing application
    will have visibility of only the specific micro batch that is getting processed
    at a given point in time. In other words, the application will not have any visibility
    or access to the already processed micro batches of data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当以自选的常规间隔处理流数据时，数据从时间间隔的开始收集到结束，分组为微批，并对该批数据进行数据处理。在较长时间内，数据处理应用程序将处理许多这样的微批数据。在这种类型的处理中，数据处理应用程序在给定时间点只能看到正在处理的特定微批。换句话说，应用程序对已经处理的微批数据没有任何可见性或访问权限。
- en: Now, there is another dimension to this type of processing. Suppose a given
    use case mandates the need to process the data every minute, but at the same time,
    while processing the data of a given micro batch, there is a need to peek into
    the data that was already processed in the last 15 minutes. A fraud detection
    module of a retail banking transaction processing application is a good example
    of this particular business requirement. There is no doubt that the retail banking
    transactions are to be processed within milliseconds of their occurrence. When
    processing an ATM cash withdrawal transaction, it is a good idea to see whether
    somebody is trying to continuously withdraw cash and, if found, send the proper
    alert. For this, when processing a given cash withdrawal transaction, the application
    checks whether there are any other cash withdrawals from the same ATM using the
    same card that was used in the last 15 minutes. The business rule is to send an
    alert when such transactions are more than two in the last 15 minutes. In this
    use case, the fraud detection application should have visibility of all the transactions
    that happened in a window of 15 minutes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这种处理类型还有另一个维度。假设给定的用例要求每分钟处理数据，但在处理给定的微批数据时，需要查看过去15分钟内已处理的数据。零售银行交易处理应用程序的欺诈检测模块是这种特定业务需求的良好示例。毫无疑问，零售银行交易应在发生后的毫秒内进行处理。在处理ATM现金提取交易时，查看是否有人试图连续提取现金，如果发现，发送适当的警报是一个好主意。为此，在处理给定的现金提取交易时，应用程序检查在过去15分钟内是否从同一ATM使用同一张卡进行了任何其他现金提取。业务规则是在过去15分钟内此类交易超过两次时发送警报。在此用例中，欺诈检测应用程序应该能够查看过去15分钟内发生的所有交易。
- en: A good stream data processing framework should have the ability to processing
    the data in any given interval of time, as well as the ability to peek into the
    data ingested within a sliding window of time. The Spark Streaming library that
    is working on top of Spark is one of the best data stream processing frameworks
    that has both of these capabilities.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的流数据处理框架应该具有在任何给定时间间隔内处理数据的能力，以及在滑动时间窗口内查看已摄取数据的能力。在Spark之上工作的Spark Streaming库是具有这两种能力的最佳数据流处理框架之一。
- en: Look again at the bigger picture of the Spark library stack as given in *Figure
    1* to set the context and see what is being discussed here before getting into
    and taking up the use cases.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看*图1*中给出的Spark库堆栈的全貌，以设置上下文并了解正在讨论的内容，然后再深入探讨和处理用例。
- en: '![Data stream processing](img/image_06_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![数据流处理](img/image_06_001.jpg)'
- en: Figure 1
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: Micro batch data processing
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微批处理数据处理
- en: Every Spark Streaming data processing application will be running continuously
    till it is terminated. This application will be constantly *listening* to the
    data source to receive the incoming stream of data. The Spark Streaming data processing
    application would have a configured batch interval. At the end of every batch
    interval, it will produce a data abstraction named **Discretized Stream** (**DStream**)
    which works very similar to Spark's RDD. Just like RDD, a DStream supports an
    equivalents method for the commonly used Spark transformations and Spark actions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Spark Streaming数据处理应用程序将持续运行，直到被终止。该应用程序将不断*监听*数据源以接收传入的数据流。Spark Streaming数据处理应用程序将有一个配置的批处理间隔。在每个批处理间隔结束时，它将产生一个名为**离散流**（**DStream**）的数据抽象，该抽象与Spark的RDD非常相似。与RDD一样，DStream支持常用Spark转换和Spark操作的等效方法。
- en: Tip
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Just like RDD, a DStream is also immutable and distributed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如RDD一样，DStream也是不可变的和分布式的。
- en: '*Figure 2* shows how DStreams are being produced in a Spark Streaming data
    processing application.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2*展示了在Spark Streaming数据处理应用程序中DStreams是如何产生的。'
- en: '![Micro batch data processing](img/image_06_004.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![微批处理数据处理](img/image_06_004.jpg)'
- en: Figure 2
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: '*Figure 2* depicts the most important elements of a Spark Streaming application.
    For the configured batch interval, the application produces one DStream. Each
    DStream is a collection of RDDs consisting of the data collected within that batch
    interval. The number of RDDs within a DStream for a given batch interval will
    vary.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2*描绘了Spark Streaming应用程序最重要的元素。对于配置的批处理间隔，应用程序产生一个DStream。每个DStream是一个由该批处理间隔内收集的数据组成的RDD集合。对于给定的批处理间隔，DStream中的RDD数量会有所不同。'
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Since Spark Streaming applications are continuously running applications collecting
    data, in this chapter, rather than running the code in REPL, the complete application
    is discussed, including the instructions to compile, package and run.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark Streaming应用程序是持续运行的应用程序，用于收集数据，本章中，我们不再通过REPL运行代码，而是讨论完整的应用程序，包括编译、打包和运行的指令。
- en: The Spark programming model was discussed in [Chapter 2](ch02.html "Chapter 2. Spark
    Programming Model"), *Spark Programming Model*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark编程模型已在[第二章](ch02.html "第二章 Spark编程模型")，*Spark编程模型*中讨论。
- en: Programming with DStreams
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DStreams进行编程
- en: Programming with DStreams in a Spark Streaming data processing application also
    follows a very similar model, as DStreams consist of one or more RDDs. When methods
    such as Spark transformations or Spark actions are invoked on a DStream, the equivalent
    operation is applied to all the RDDs that constitute the DStream.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark Streaming数据处理应用程序中使用DStreams也遵循非常相似的模式，因为DStreams由一个或多个RDD组成。当对DStream调用诸如Spark转换或Spark操作等方法时，相应的操作将应用于构成DStream的所有RDD。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: An important point to note here is that not all the Spark transformations and
    Spark actions that work on RDD are unsupported on DStreams. The other notable
    change is the differences in capability across programming languages.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是，并非所有适用于RDD的Spark转换和Spark操作都适用于DStreams。另一个显著的变化是不同编程语言之间的能力差异。
- en: The Scala and Java APIs for Spark Streaming are ahead of the Python API in terms
    of the number of features supported for Spark Streaming data processing application
    development.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming的Scala和Java API在支持Spark Streaming数据处理应用程序开发的特性数量上领先于Python API。
- en: '*Figure 3* depicts how methods applied on a DStream are applied on the underlying
    RDDs. The Spark Streaming programming guide is to be consulted before using any
    of the methods on DStreams. The Spark Streaming programming guide is marked with
    special callouts containing the text *Python API* wherever the Python API deviates
    from its Scala or Java counterparts.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3*展示了应用于DStream的方法如何应用于底层RDDs。在使用DStreams上的任何方法之前，应查阅Spark Streaming编程指南。Spark
    Streaming编程指南在Python API与其Scala或Java对应部分存在差异的地方，用特殊标注包含文本*Python API*。'
- en: 'Assume that, for a given batch interval in a Spark Streaming data processing
    application, a DStream is generated consisting of multiple RDDs. When a filter
    method is applied on that DStream, here is how it gets translated into the underlying
    RDDs. *Figure 3* shows a filter transformation applied on a DStream with two RDDs,
    resulting in another DStream containing only one RDD because of the filter condition:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在Spark Streaming数据处理应用程序的给定批次间隔内，生成一个包含多个RDD的DStream。当对该DStream应用过滤方法时，以下是其如何转换为底层RDDs的过程。*图3*显示了对包含两个RDD的DStream应用过滤转换，由于过滤条件，结果生成仅包含一个RDD的另一个DStream。
- en: '![Programming with DStreams](img/image_06_003.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![使用DStreams进行编程](img/image_06_003.jpg)'
- en: Figure 3
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3
- en: A log event processor
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志事件处理器
- en: These days, it is very common to have a central repository of application log
    events in many enterprises. Also, the log events are streamed live to data processing
    applications in order to monitor the performance of the running applications on
    a real-time basis so that timely remediation measures can be taken. Such a use
    case is discussed here to demonstrate the real-time processing of log events using
    a Spark Streaming data processing application. In this use case, the live application
    log events are written to a TCP socket. The Spark Streaming data processing application
    constantly listens to a given port on a given host to collect the stream of log
    events.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，许多企业普遍拥有一个中央应用程序日志事件存储库。此外，这些日志事件被实时流式传输到数据处理应用程序，以便实时监控运行应用程序的性能，从而及时采取补救措施。本节将讨论这样一个用例，以展示使用Spark
    Streaming数据处理应用程序对日志事件进行实时处理。在此用例中，实时应用程序日志事件被写入TCP套接字。Spark Streaming数据处理应用程序持续监听给定主机上的特定端口，以收集日志事件流。
- en: Getting ready with the Netcat server
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备Netcat服务器
- en: 'The Netcat utility that comes with most UNIX installations is used here as
    the data server. To make sure that Netcat is installed in the system, type the
    manual command as given in the following scripts, and, after coming out of it,
    run it and make sure that there is no error message. Once the server is up and
    running, whatever is typed in the standard input of the Netcat server console
    is considered as the application logs events for simplicity and demonstration
    purposes. The following commands run from a terminal prompt will start the Netcat
    data server on localhost port `9999`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用大多数UNIX安装附带的Netcat实用程序作为数据服务器。为了确保系统中安装了Netcat，请按照以下脚本中的手动命令操作，退出后运行它，并确保没有错误消息。一旦服务器启动并运行，在Netcat服务器控制台的标准输入中输入的内容将被视为应用程序日志事件，以简化演示目的。从终端提示符运行的以下命令将在localhost端口`9999`上启动Netcat数据服务器：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once the preceding steps are completed, the Netcat server is ready and the Spark
    Streaming data processing application will process all the lines that are typed
    in the previous console window. Leave this console window alone; all the following
    shell commands will be run in a different terminal window.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 完成上述步骤后，Netcat服务器就绪，Spark Streaming数据处理应用程序将处理在前一个控制台窗口中输入的所有行。不要关闭此控制台窗口；所有后续的shell命令将在另一个终端窗口中运行。
- en: Since there is a lack of parity of Spark Streaming features between different
    programming languages, the Scala code is used to explain all the Spark Streaming
    concepts and use cases. After that, the Python code is given, and, if there is
    a lack of support for any of the features being discussed in Python, that is also
    captured.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不同编程语言之间Spark Streaming特性的不一致，使用Scala代码来解释所有Spark Streaming概念和用例。之后，给出Python代码，如果Python中讨论的任何特性缺乏支持，也会记录下来。
- en: The Scala and Python code are organized in the way demonstrated in *Figure 4*.
    For the compilation, packaging and running of the code, bash scripts are used
    so that it is easy for the readers to run them to produce consistent results.
    Each of these script file contents are discussed here.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图4*所示，Scala和Python代码的组织方式。为了编译、打包和运行代码，使用了Bash脚本，以便读者可以轻松运行它们以产生一致的结果。这些脚本文件的内容在此讨论。
- en: Organizing files
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件组织
- en: 'In the following folder tree, the `project`and `target`folders are created
    at runtime. The source code that comes with this book can be copied directly to
    a convenient folder in the system:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的文件夹树中，`project`和`target`文件夹在运行时创建。本书附带的源代码可以直接复制到系统中方便的文件夹中：
- en: '![Organizing files](img/image_06_007.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![文件组织](img/image_06_007.jpg)'
- en: Figure 4
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4*'
- en: 'For compiling and packaging, the **Scala build tool** (**sbt**) is used. In
    order to make sure that sbt is working properly, run the following commands from
    the `Scala` folder of the tree in *Figure 4* in the terminal window. This is to
    make sure that sbt is working fine and the code is compiling:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 编译和打包使用**Scala构建工具**(**sbt**)。为了确保sbt正常工作，请从*图4*中树的`Scala`文件夹中在终端窗口中运行以下命令。这是为了确保sbt运行正常，代码编译无误：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The following table captures the representative sample list of files and the
    purpose of each of them in the context of the Spark Streaming data processing
    application that is being discussed here.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下表概述了正在讨论的Spark Streaming数据处理应用程序中文件的代表性样本列表及其各自用途。
- en: '| **File Name** | **Purpose** |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **文件名** | **用途** |'
- en: '| `README.txt` | Instructions to run the application. One for the Scala application
    and the other one for the Python application. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `README.txt` | 运行应用程序的说明。一份针对Scala应用程序，另一份针对Python应用程序。 |'
- en: '| `submitPy.sh` | Bash script to submit the Python job to the Spark cluster.
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `submitPy.sh` | 向Spark集群提交Python作业的Bash脚本。 |'
- en: '| `compile.sh` | Bash script to compile the Scala code. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `compile.sh` | 编译Scala代码的Bash脚本。 |'
- en: '| `submit.sh` | Bash script to submit the Scala job to the Spark cluster. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `submit.sh` | 向Spark集群提交Scala作业的Bash脚本。 |'
- en: '| `config.sbt` | The sbt configuration file. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `config.sbt` | sbt配置文件。 |'
- en: '| `*.scala` | Spark Streaming data processing application code in Scala. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `*.scala` | Scala中的Spark Streaming数据处理应用程序代码。 |'
- en: '| `*.py` | Spark Streaming data processing application code in Python. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `*.py` | Python中的Spark Streaming数据处理应用程序代码。 |'
- en: '| `*.jar` | The Spark Streaming and Kafka integration JAR file that needs to
    be downloaded and placed under the `lib` folder for the proper functioning of
    the applications. This is being used in `submit.sh` as well as in `submitPy.sh`
    for submitting the job to the cluster. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `*.jar` | 需要下载并放置在`lib`目录下的Spark Streaming和Kafka集成JAR文件，以确保应用程序正常运行。这在`submit.sh`和`submitPy.sh`中用于向集群提交作业。
    |'
- en: Submitting the jobs to the Spark cluster
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向Spark集群提交作业
- en: 'To properly run the application, some of the configurations depend on the system
    in which it is being run. They are to be edited in the `submit.sh` file and the
    `submitPy.sh` file. Wherever such edits are required, comments are given with
    the `[FILLUP]` tag. The most important of these are the setting of the Spark installation
    directory and the Spark master configuration, which can differ from system to
    system. The source of the preceding script `submit.sh` file is given as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确运行应用程序，其中一些配置取决于它运行的系统。它们需要在`submit.sh`文件和`submitPy.sh`文件中进行编辑。无论何处需要此类编辑，都会使用`[FILLUP]`标签给出注释。其中最重要的是设置Spark安装目录和Spark主配置，这可能因系统而异。前面脚本`submit.sh`文件的源代码如下：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The source of the preceding script file `submitPy.sh` is given as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 前面脚本文件`submitPy.sh`的源代码如下：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Monitoring running applications
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控正在运行的应用程序
- en: As described in [Chapter 2](ch02.html "Chapter 2. Spark Programming Model"),
    *Spark Programming Model*, Spark installation comes with a powerful Spark web
    UI for monitoring the Spark applications that are running.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第二章](ch02.html "第二章 Spark编程模型")所述，*Spark编程模型*，Spark安装自带一个强大的Spark Web UI，用于监控正在运行的Spark应用程序。
- en: There are additional visualizations available specifically for the Spark Streaming
    jobs that are running.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正在运行的Spark Streaming作业，还有额外的可视化工具可用。
- en: 'The following scripts start the Spark master and workers, and enable monitoring.
    The assumption here is that the reader has made all the configuration changes
    suggested in [Chapter 2](ch02.html "Chapter 2. Spark Programming Model"), *Spark
    Programming Model* to enable Spark application monitoring. If that is not done,
    the applications can still be run. The only change to be made is to put the cases
    in the `submit.sh` file and the `submitPy.sh` file to make sure that instead of
    the Spark master URL, something like `local[4]` is used. Run the following commands
    on the terminal window:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本启动Spark主节点和工作者，并启用监控。这里的假设是读者已经按照[第二章](ch02.html "第二章 Spark编程模型")，*Spark编程模型*中的建议进行了所有配置更改，以启用Spark应用程序监控。如果没有这样做，应用程序仍然可以运行。唯一需要做的更改是将`submit.sh`文件和`submitPy.sh`文件中的情况更改为确保使用`local[4]`之类的内容，而不是Spark主URL。在终端窗口中运行以下命令：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Make sure that the Spark web UI is up and running by visiting `http://localhost:8080/`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问`http://localhost:8080/`确保Spark Web UI已启动并运行。
- en: Implementing the application in Scala
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Scala中实现应用程序
- en: 'The following code snippet is the Scala code for the log event processing application:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是用于日志事件处理应用程序的Scala代码：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the previous code snippet, there are two Scala objects. One is for setting
    the proper logging levels, to make sure that unwanted messages are not displayed
    on the console. The `StreamingApps` Scala object holds the logic of the stream
    processing. The following list captures the essence of the functionality:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，有两个Scala对象。一个是设置适当的日志级别，以确保控制台上不显示不需要的消息。`StreamingApps` Scala对象包含流处理的逻辑。以下列表捕捉了功能的本质：
- en: A Spark configuration is created with the application name.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用应用程序名称创建Spark配置。
- en: A Spark `StreamingContext` object is created, which is the heart of the stream
    processing. The second parameter of the `StreamingContext` constructor is the
    batch interval, which is 10 seconds. The line containing `ssc.socketTextStream`
    creates DStreams at every batch interval, which is 10 seconds here, containing
    the lines typed in the Netcat console.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建了一个Spark `StreamingContext`对象，这是流处理的中心。`StreamingContext`构造函数的第二个参数是批处理间隔，这里是10秒。包含`ssc.socketTextStream`的行在每个批处理间隔（此处为10秒）创建DStreams，其中包含在Netcat控制台中输入的行。
- en: A filter transformation is applied next on the DStream, to have only the lines
    containing the word `ERROR`. The filter transformation creates new DStreams containing
    only the filtered lines.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来对DStream应用过滤转换，只包含包含单词`ERROR`的行。过滤转换创建仅包含过滤行的新DStreams。
- en: The next line prints the DStream contents to the console. In other words, for
    every batch interval, if there are lines containing the word `ERROR`, that get
    displayed in the console.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一行将DStream内容打印到控制台。换句话说，对于每个批处理间隔，如果存在包含单词`ERROR`的行，则会在控制台上显示。
- en: At the end of this data processing logic, the given `StreamingContext` is started
    and will run until it is terminated.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此数据处理逻辑结束时，给定的`StreamingContext`启动并运行，直到被终止。
- en: In the previous code snippet, there is no loop construct telling the application
    to repeat till the running application is terminated. This is achieved by the
    Spark Streaming library itself. From the beginning till the termination of the
    data processing application, all the statements are run once. All the operations
    on the DStreams are repeated (internally) for every batch. If the output of the
    previous application is closely examined, the output from the println() statements
    are seen only once in the console, even though these statements are between the
    initialization and termination of the `StreamingContext`. That is because the
    *magic loop* is repeating only for the statements containing original and derived
    DStreams.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，没有循环结构告诉应用程序重复直到运行应用程序被终止。这是由Spark Streaming库本身实现的。从数据处理应用程序开始到终止，所有语句都运行一次。对DStreams的所有操作都会重复（内部）每个批次。如果仔细检查前一个应用程序的输出，尽管这些语句位于`StreamingContext`的初始化和终止之间，但只能在控制台上看到println()语句的输出一次。这是因为*魔法循环*仅对包含原始和派生DStreams的语句重复。
- en: Because of the special nature of the looping implemented in Spark Streaming
    applications, it is futile to give print statements and log statements within
    the streaming logic in the application code, like the one that is given in the
    code snippet. If that is a must, then these logging statements are to be instrumented
    within the functions that are passed to DStreams for the transformations and actions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark Streaming应用程序中实现的循环的特殊性，在应用程序代码的流逻辑中给出打印语句和日志语句是徒劳的，就像代码片段中给出的那样。如果必须这样做，那么这些日志语句应该在传递给DStreams进行转换和操作的函数中进行设置。
- en: Tip
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If persistence is required for the processed data, there are many output operations
    available for DStreams, just as there are for RDDs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要对处理后的数据进行持久化，DStreams提供了多种输出操作，就像RDDs一样。
- en: Compiling and running the application
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编译和运行应用程序
- en: The following commands are run on the terminal window to compile and run the
    application. Instead of using `./compile.sh`, a simple sbt compile command can
    also be used.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令在终端窗口中运行以编译和运行应用程序。可以使用简单的sbt编译命令，而不是使用`./compile.sh`。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that, as discussed previously, the Netcat server must be running before
    these commands are executed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如前所述，在执行这些命令之前，Netcat服务器必须正在运行。
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If no error messages are shown, and the results are showing in line with the
    previous output, the Spark Streaming data processing application has started properly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有显示错误消息，并且结果与之前的输出一致，则Spark Streaming数据处理应用程序已正确启动。
- en: Handling the output
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理输出
- en: Note that the output of the print statements comes before the DStream output
    print. So far, nothing has been typed in the Netcat console and, therefore, there
    is nothing to process.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，打印语句的输出在DStream输出打印之前。到目前为止，还没有在Netcat控制台中输入任何内容，因此没有要处理的内容。
- en: 'Now go to the Netcat console that was started earlier and enter the following
    lines of log event messages by giving a gap of few seconds to make sure that the
    output goes to more than one batch, where the batch size is 10 seconds:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转到之前启动的Netcat控制台，输入以下几行日志事件消息，间隔几秒钟，以确保输出到多个批次，其中批处理大小为10秒：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once the log event messages are entered into the Netcat console window, the
    following results will start showing up in the Spark Streaming data processing
    application, filtering only the log event messages containing the keyword ERROR.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦日志事件消息输入到Netcat控制台窗口，以下结果将开始显示在Spark Streaming数据处理应用程序中，仅过滤包含关键字ERROR的日志事件消息。
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The Spark web UI (`http://localhost:8080/`) was already enabled, and Figures
    5 and 6 show the Spark applications and statistics.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Web UI（`http://localhost:8080/`）已启用，图5和图6显示了Spark应用程序和统计信息。
- en: From the main page (after visiting the URL `http://localhost:8080/`), click
    the running Spark Streaming data processing application's name link to bring up
    the regular monitoring page. From that page, click the **Streaming** tab, to reveal
    the page containing the streaming statistics.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从主页（访问URL `http://localhost:8080/`后），点击正在运行的Spark Streaming数据处理应用程序的名称链接，以调出常规监控页面。从该页面，点击**Streaming**标签，以显示包含流统计信息的页面。
- en: 'The link and tab to be clicked are circled in red:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 需要点击的链接和标签以红色圆圈标出：
- en: '![Handling the output](img/image_06_008.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![处理输出](img/image_06_008.jpg)'
- en: Figure 5
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图5
- en: 'From the page shown in *Figure 5*, click on the circled application link; it
    will take you to the relevant page. From that page, once the **Streaming** tab
    is clicked, the page containing the streaming statistics will show up as captured
    in *Figure 6*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图5*所示页面中，点击圆圈内的应用程序链接；这将带您到相关页面。从该页面，一旦点击**Streaming**标签，将显示包含流统计信息的页面，如*图6*所示：
- en: '![Handling the output](img/image_06_009.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![处理输出](img/image_06_009.jpg)'
- en: Figure 6
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图6
- en: There are a whole lot of application statistics available from these Spark web
    UI pages, and exploring them extensively is a good idea to gain a deeper understanding
    of the behavior of the Spark Streaming data processing applications submitted.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些Spark网页界面提供了大量的应用程序统计信息，深入探索它们有助于更深入地理解提交的Spark Streaming数据处理应用程序的行为。
- en: Tip
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Care must be taken while enabling the monitoring of streaming applications as
    it should not affect the performance of the application itself.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在启用流应用程序监控时必须小心，以确保不影响应用程序本身的性能。
- en: Implementing the application in Python
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中实现应用程序
- en: 'The same use case is implemented in Python, and the following code snippet
    saved in `StreamingApps.py`is used to do this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的用例在Python中实现，以下代码片段保存在`StreamingApps.py`中用于执行此操作：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The following commands are run on the terminal window to run the Python Spark
    Streaming data processing application from the directory where the code is downloaded.
    Before running the application, in the same way that the modifications are made
    to the script that is used to run the Scala application, the `submitPy.sh` file
    also has to be changed to point to the right Spark installation directory and
    configure the Spark master. If monitoring is enabled, and if the submission is
    pointing to the right Spark master, the same Spark web UI will capture the statistics
    of the Python Spark Streaming data processing applications as well.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令在终端窗口中运行Python Spark Streaming数据处理应用程序，该目录是代码下载的位置。在运行应用程序之前，如同对用于运行Scala应用程序的脚本进行修改一样，`submitPy.sh`文件也需要更改，以指向正确的Spark安装目录并配置Spark主节点。如果启用了监控，并且提交指向了正确的Spark主节点，则相同的Spark网页界面也将捕获Python
    Spark Streaming数据处理应用程序的统计信息。
- en: 'The following commands are run on the terminal window to run the Python application:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令在终端窗口中运行Python应用程序：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once the same log event messages used in the Scala implementation are entered
    into the Netcat console window, the following results will start showing up in
    the streaming application, filtering only the log event messages containing the
    keyword `ERROR`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将用于Scala实现中的相同日志事件消息输入到Netcat控制台窗口中，以下结果将开始显示在流应用程序中，仅过滤包含关键字`ERROR`的日志事件消息：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you look at the outputs from both the Scala and Python programs, you can
    clearly see whether there are any log event messages containing the word `ERROR`
    in a given batch interval. Once the data is processed, the application discards
    the processed data without retaining them for any future use.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看Scala和Python程序的输出，可以清楚地看到在给定的批次间隔内是否存在包含单词`ERROR`的日志事件消息。一旦数据被处理，应用程序会丢弃已处理的数据，不保留它们以供将来使用。
- en: In other words, the application never retains or remembers any of the log event
    messages from the previous batch intervals. If there is a need to capture the
    number of error messages, say in the last 5 minutes or so, then the previous approach
    will not work. We will discuss that in the next section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 换言之，该应用程序不会保留或记忆任何来自先前批次间隔的日志事件消息。如果需要捕获错误消息的数量，例如在过去5分钟左右，那么先前的方法将不适用。我们将在下一节讨论这一点。
- en: Windowed data processing
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口化数据处理
- en: 'In the Spark Streaming data processing application discussed in the previous
    section, assume that there is a need to count the number of log event messages
    containing the keyword ERROR in the previous three batches. In other words, there
    should be the ability to count the number of such event messages across a window
    of three batches. At any given point in time, the window should be sliding along
    with time as and when a new batch of data is available. Three important terms
    have been discussed here, and *Figure 7* explains them. They are:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节讨论的Spark Streaming数据处理应用程序中，假设需要统计前三个批次中包含关键字ERROR的日志事件消息的数量。换句话说，应该能够跨三个批次的窗口统计此类事件消息的数量。在任何给定时间点，随着新数据批次的可用，窗口应随时间滑动。这里讨论了三个重要术语，*图7*解释了它们。它们是：
- en: 'Batch interval: The time interval at which a DStream is produced'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理间隔：生成DStream的时间间隔
- en: 'Window length: The duration of the number of batch intervals where there is
    a need to peek into all the DStreams produced in those batch intervals'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口长度：需要查看在那些批处理间隔中生成的所有DStreams的批处理间隔的持续时间
- en: Sliding interval:  The interval at which the window operation, such as counting
    the event messages, is performed
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滑动间隔：执行窗口操作（如统计事件消息）的时间间隔
- en: '![Windowed data processing](img/image_06_011.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![窗口化数据处理](img/image_06_011.jpg)'
- en: Figure 7
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图7
- en: In *Figure 7*, at a given point in time, the DStreams used for the operation
    to be performed are enclosed in a rectangle.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7*中，在某一特定时间点，用于执行操作的DStreams被包含在一个矩形内。
- en: In every batch interval, a new DStream is generated. Here, the window length
    is three and the operation to be performed in a window is counting the number
    of event messages in that window. The sliding interval is kept the same as the
    batch interval so that the counting operation is done as and when a new DStream
    is generated, so that the count is correct all the time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个批处理间隔中，都会生成一个新的DStream。这里，窗口长度为三，窗口内要执行的操作是统计该窗口内的事件消息数量。滑动间隔保持与批处理间隔相同，以便在新DStream生成时执行计数操作，从而始终确保计数的准确性。
- en: At time **t2**, the counting operation is done on the DStreams generated at
    times **t0**, **t1**, and **t2**. At time **t3**, the counting operation is done
    again since the sliding window is kept the same as the batch interval, and this
    time counting the events is done on the DStreams generated at time **t1**, **t2**,
    and **t3**. At time **t4**, the counting operation is done again, counting the
    events done on the DStreams generated at time **t2**, **t3**, and **t4**. The
    operations continue in that fashion till the application is terminated.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间**t2**，计数操作针对在时间**t0**、**t1**和**t2**生成的DStreams执行。在时间**t3**，由于滑动窗口保持与批处理间隔相同，计数操作再次执行，这次针对在时间**t1**、**t2**和**t3**生成的DStreams进行事件计数。在时间**t4**，计数操作再次执行，针对在时间**t2**、**t3**和**t4**生成的DStreams进行事件计数。操作以此类推，直到应用程序终止。
- en: Counting the number of log event messages processed in Scala
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Scala中统计已处理的日志事件消息数量
- en: 'In the preceding section, the processing of the log event messages is discussed.
    In the same application code after the printing of the log event messages containing
    the word `ERROR`, include the following lines of code in the Scala application:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述部分，讨论了日志事件消息的处理。在同一应用程序代码中，在打印包含单词`ERROR`的日志事件消息之后，在Scala应用程序中包含以下代码行：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The first parameter is the window length and the second one is the sliding
    window interval. This single magic line will print a count of log event messages
    processed once the following lines are typed in the Netcat console:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是窗口长度，第二个参数是滑动窗口间隔。这条神奇的代码行将在Netcat控制台输入以下行后，打印出已处理的日志事件消息的计数：
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The same Spark Streaming data processing application in Scala, with the additional
    lines of code, produces the following output:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中运行的相同的Spark Streaming数据处理应用程序，加上额外的代码行，会产生以下输出：
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If the output is studied properly, it can be noticed that, in the first batch
    interval, one log event message is processed. Obviously the count displayed is
    `1` for that batch interval. In the next batch interval, one more log event message
    is processed. The count displayed for that batch interval is `2`. In the next
    batch interval, no log event message is processed. But the count for that window
    is still `2`. For one more window, the count is displayed as `2`. Then it reduces
    to `1`, and then 0.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细研究输出，可以注意到，在第一个批处理间隔中，处理了一个日志事件消息。显然，该批处理间隔显示的计数为`1`。在下一个批处理间隔中，又处理了一个日志事件消息。该批处理间隔显示的计数为`2`。在下一个批处理间隔中，没有处理日志事件消息。但该窗口的计数仍然是`2`。对于另一个窗口，计数显示为`2`。然后它减少到`1`，然后是0。
- en: 'The most important point to be noted here is that, in the application codes
    for both Scala and Python, immediately after StreamingContext creation, the following
    line of code needs to be inserted to specify the checkpoint directory:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是，在Scala和Python的应用程序代码中，在创建StreamingContext之后，需要立即插入以下代码行来指定检查点目录：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Counting the number of log event messages processed in Python
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中统计处理日志事件消息的数量
- en: 'In the Python application code, after the printing of the log event messages
    containing the word ERROR, include the following lines of code in the Scala application:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python应用程序代码中，在打印包含单词ERROR的日志事件消息之后，在Scala应用程序中包含以下代码行：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first parameter is the window length and the second one is the sliding
    window interval. This single magic line will print a count of log event messages
    processed once the following lines are typed in the Netcat console:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是窗口长度，第二个参数是滑动窗口间隔。这条神奇的代码行将在Netcat控制台输入以下行后，打印出处理的日志事件消息的计数：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The same Spark Streaming data processing application in Python, with the additional
    lines of code, produces the following output:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中使用相同的Spark Streaming数据处理应用程序，添加额外的代码行，产生以下输出：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output pattern of the Python application is also very similar to the Scala
    application.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Python应用程序的输出模式与Scala应用程序也非常相似。
- en: More processing options
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多处理选项
- en: Apart from the count operation in a window, there are more operations that can
    be done on DStreams in conjunction with windowing. The following table captures
    the important transformations. All these transformations are acting on the selected
    window and return a DStream.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 除了窗口中的计数操作外，还可以在DStreams上进行更多操作，并与窗口化结合。下表捕捉了重要的转换。所有这些转换都作用于选定的窗口并返回一个DStream。
- en: '| **Transformation** | **Description** |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **转换** | **描述** |'
- en: '| `window(windowLength, slideInterval)` | Returns DStreams computed in the
    window |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `window(windowLength, slideInterval)` | 返回在窗口中计算的DStreams |'
- en: '| `countByWindow(windowLength, slideInterval)` | Returns the count of elements
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `countByWindow(windowLength, slideInterval)` | 返回元素的计数 |'
- en: '| `reduceByWindow(func, windowLength, slideInterval)` | Returns one element
    by applying the aggregation function |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByWindow(func, windowLength, slideInterval)` | 通过应用聚合函数返回一个元素 |'
- en: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | Returns
    one key/value pair per key after applying the aggregation function over  multiple
    values per key |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | 对每个键应用多个值的聚合函数后，返回每个键的一对键/值
    |'
- en: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | Returns
    one key/count pair per key after applying the count of multiple values per key
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | 对每个键应用多个值的计数后，返回每个键的一对键/计数
    |'
- en: One of the most important steps of stream processing is the persisting of the
    stream data into secondary storage. Since the velocity of the data in Spark Streaming
    data processing applications is going to be very high, any kind of persistence
    mechanism that introduces additional latency into the whole process is not an
    advisable solution.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理中最关键的步骤之一是将流数据持久化到辅助存储中。由于Spark Streaming数据处理应用程序中的数据速度将非常高，任何引入额外延迟的持久化机制都不是一个可取的解决方案。
- en: In batch processing scenarios, it is fine to write to the HDFS and other file
    system based storage. But when it comes to the storage of stream output, depending
    on the use case, an ideal stream data storage mechanism should be chosen.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在批处理场景中，向HDFS和其他基于文件系统的存储写入数据是可行的。但涉及到流输出存储时，应根据用例选择理想的流数据存储机制。
- en: NoSQL data stores such as Cassandra support fast writes of temporal data. It
    is also ideal to read the data that is stored for any further analysis purposes.
    Spark Streaming library supports many output methods on DStreams. They include
    options to save the stream data as text file, object file, Hadoop files, and so
    on. As well as this, there are many third-party drivers available to save the
    data into various data stores.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: NoSQL 数据存储如 Cassandra 支持快速写入时间序列数据。它也非常适合读取存储的数据以供进一步分析。Spark Streaming 库支持
    DStreams 上的多种输出方法。它们包括将流数据保存为文本文件、对象文件、Hadoop 文件等的选项。此外，还有许多第三方驱动程序可用于将数据保存到各种数据存储中。
- en: Kafka stream processing
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka 流处理
- en: The log event processor example covered in this chapter was listening to a TCP
    socket for the stream of messages to be processed by the Spark Streaming data
    processing application. But in real-world use cases, this is not going to be the
    case.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的日志事件处理器示例正在监听 TCP 套接字，以接收 Spark Streaming 数据处理应用程序将要处理的消息流。但在现实世界的用例中，情况并非如此。
- en: Message queueing systems with publish-subscribe capability are generally used
    for processing messages. The traditional message queueing systems failed to perform
    because of the huge volume of messages to be processed per second for the needs
    of large-scale data processing applications.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 具有发布-订阅功能的消息队列系统通常用于处理消息。传统的消息队列系统因每秒需要处理大量消息以满足大规模数据处理应用的需求而表现不佳。
- en: 'Kafka is a publish-subscribe messaging system used by many IoT applications
    to process a huge number of messages. The following capabilities of Kafka made
    it one of the most widely used messaging systems:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 是一种发布-订阅消息系统，被许多物联网应用用于处理大量消息。以下 Kafka 的功能使其成为最广泛使用的消息系统之一：
- en: 'Extremely fast: Kafka can process huge amounts of data by handling reading
    and writing in short intervals of time from many application clients'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极速处理：Kafka 能够通过在短时间内处理来自许多应用程序客户端的读写操作来处理大量数据
- en: 'Highly scalable: Kafka is designed to scale up and scale out to form a cluster
    using commodity hardware'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可扩展：Kafka 设计用于通过使用商品硬件向上和向外扩展以形成集群
- en: 'Persists a huge number of messages: Messages reaching Kafka topics are persisted
    into the secondary storage, while at the same time it is handling huge number
    of messages flowing through'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久化大量消息：到达 Kafka 主题的消息被持久化到辅助存储中，同时处理大量流经的消息
- en: Note
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A detailed treatment of Kafka is outside the scope of this book. It is assumed
    that the reader is familiar with and has working knowledge of Kafka. From a Spark
    Streaming data processing application perspective, it doesn't really make a difference
    whether it is a TCP socket or Kafka that is being used as a message source. But
    working on a teaser use case with Kafka as the message producer will give a good
    appreciation of the toolsets enterprises are using heavily. *Learning Apache Kafka*
    *- Second Edition* by *Nishant Garg* ([https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition))
    is a good reference book to learn more about Kafka.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 的详细介绍超出了本书的范围。假设读者熟悉并具有 Kafka 的实际操作知识。从 Spark Streaming 数据处理应用程序的角度来看，无论是使用
    TCP 套接字还是 Kafka 作为消息源，实际上并没有什么区别。但是，通过使用 Kafka 作为消息生产者的预告用例，可以很好地了解企业广泛使用的工具集。*《学习
    Apache Kafka》第二版*由*Nishant Garg*编写（[https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition)）是学习
    Kafka 的优秀参考书。
- en: 'The following are some of the important elements of Kafka, and are terms to
    be understood before proceeding further:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Kafka 的一些重要元素，也是进一步了解之前需要理解的术语：
- en: 'Producer: The real source of the messages, such as weather sensors or mobile
    phone network'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产者：消息的实际来源，如气象传感器或移动电话网络
- en: 'Broker: The Kafka cluster, which receives and persists the messages published
    to its topics by various producers'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理：Kafka 集群，接收并持久化由各种生产者发布到其主题的消息
- en: 'Consumer: The data processing applications subscribed to the Kafka topics that
    consume the messages published to the topics'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者：数据处理应用程序订阅了 Kafka 主题，这些主题消费了发布到主题的消息
- en: The same log event processing application use case discussed in the preceding
    section is used again here to elucidate the usage of Kafka with Spark Streaming.
    Instead of collecting the log event messages from the TCP socket, here the Spark
    Streaming data processing application will act as a consumer of a Kafka topic
    and the messages published to the topic will be consumed.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中讨论的相同日志事件处理应用程序用例再次用于阐明Kafka与Spark Streaming的使用。这里，Spark Streaming数据处理应用程序将作为Kafka主题的消费者，而发布到该主题的消息将被消费。
- en: 'The Spark Streaming data processing application uses the version 0.8.2.2 of
    Kafka as the message broker, and the assumption is that the reader has already
    installed Kafka, at least in a standalone mode. The following activities are to
    be performed to make sure that Kafka is ready to process the messages produced
    by the producers and that the Spark Streaming data processing application can
    consume those messages:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming数据处理应用程序使用Kafka作为消息代理的0.8.2.2版本，假设读者已经至少在独立模式下安装了Kafka。以下活动是为了确保Kafka准备好处理生产者产生的消息，并且Spark
    Streaming数据处理应用程序可以消费这些消息：
- en: Start the Zookeeper that comes with Kafka installation.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动随Kafka安装一起提供的Zookeeper。
- en: Start the Kafka server.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Kafka服务器。
- en: Create a topic for the producers to send the messages to.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为生产者创建一个主题以发送消息。
- en: Pick up one Kafka producer and start publishing log event messages to the newly
    created topic.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个Kafka生产者，开始向新创建的主题发布日志事件消息。
- en: Use the Spark Streaming data processing application to process the log events
    published to the newly created topic.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Spark Streaming数据处理应用程序处理发布到新创建主题的日志事件。
- en: Starting Zookeeper and Kafka
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动Zookeeper和Kafka
- en: 'The following scripts are run from separate terminal windows in order to start
    Zookeeper and the Kafka broker, and to create the required Kafka topics:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本在单独的终端窗口中运行，以启动Zookeeper和Kafka代理，并创建所需的Kafka主题：
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Tip
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Make sure that the environment variable `$KAFKA_HOME` is pointing to the directory
    where Kafka is installed. Also, it is very important to start Zookeeper, Kafka
    server, Kafka producer, and Spark Streaming log event data processing application
    in separate terminal windows.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 确保环境变量`$KAFKA_HOME`指向Kafka安装的目录。同时，在单独的终端窗口中启动Zookeeper、Kafka服务器、Kafka生产者和Spark
    Streaming日志事件数据处理应用程序非常重要。
- en: The Kafka message producer can be any application capable of publishing messages
    to the Kafka topics. Here, the `kafka-console-producer` that comes with Kafka
    is used as the producer of choice. Once the producer starts running, whatever
    is typed into its console window will be treated as a message that is published
    to the chosen Kafka topic. The Kafka topic is given as a command line argument
    when starting the `kafka-console-producer`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka消息生产者可以是任何能够向Kafka主题发布消息的应用程序。这里，使用随Kafka一起提供的`kafka-console-producer`作为首选生产者。一旦生产者开始运行，在其控制台窗口中输入的任何内容都将被视为发布到所选Kafka主题的消息。启动`kafka-console-producer`时，Kafka主题作为命令行参数给出。
- en: 'The submission of the Spark Streaming data processing application that consumes
    log event messages produced by the Kafka producer is slightly different from the
    application covered in the preceding section. Here, many Kafka jar files are required
    for the data processing. Since they are not part of the Spark infrastructure,
    they have to be submitted to the Spark cluster. The following jar files are required
    for the successful running of this application:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 提交消费由Kafka生产者产生的日志事件消息的Spark Streaming数据处理应用程序与前一节中介绍的应用程序略有不同。这里，数据处理需要许多Kafka
    jar文件。由于它们不是Spark基础设施的一部分，因此必须提交给Spark集群。以下jar文件是成功运行此应用程序所必需的：
- en: '`$KAFKA_HOME/libs/kafka-clients-0.8.2.2.jar`'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$KAFKA_HOME/libs/kafka-clients-0.8.2.2.jar`'
- en: '`$KAFKA_HOME/libs/kafka_2.11-0.8.2.2.jar`'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$KAFKA_HOME/libs/kafka_2.11-0.8.2.2.jar`'
- en: '`$KAFKA_HOME/libs/metrics-core-2.2.0.jar`'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$KAFKA_HOME/libs/metrics-core-2.2.0.jar`'
- en: '`$KAFKA_HOME/libs/zkclient-0.3.jar`'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$KAFKA_HOME/libs/zkclient-0.3.jar`'
- en: '`Code/Scala/lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Code/Scala/lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`'
- en: '`Code/Python/lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Code/Python/lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`'
- en: In the preceding list of jar files, the maven repository co-ordinate for `spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`
    is `"org.apache.spark" %% "spark-streaming-kafka-0-8" % "2.0.0-preview"`. This
    particular jar file has to be downloaded and placed in the lib folder of the directory
    structure given in Figure 4\. It is being used in the `submit.sh` and the `submitPy.sh`
    scripts, which  submit the application to the Spark cluster. The download URL
    for this jar file is given in the reference section of this chapter.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述的jar文件列表中，`spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`的Maven仓库坐标是`"org.apache.spark"
    %% "spark-streaming-kafka-0-8" % "2.0.0-preview"`。这个特定的jar文件必须下载并放置在图4所示的目录结构的lib文件夹中。它被用于`submit.sh`和`submitPy.sh`脚本中，这些脚本将应用程序提交给Spark集群。该jar文件的下载URL在本章的参考部分给出。
- en: In the `submit.sh` and `submitPy.sh` files, the last few lines contain a conditional
    statement looking for the second parameter value of 1 to identify this application
    and ship the required jar files to the Spark cluster.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在`submit.sh`和`submitPy.sh`文件中，最后几行包含一个条件语句，查找第二个参数值为1以识别此应用程序，并将所需的jar文件发送到Spark集群。
- en: Tip
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Instead of shipping these individual jar files separately to the Spark cluster
    when the job is submitted, an assembly jar can be used by creating it using sbt.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与其在提交作业时单独将这些jar文件发送到Spark集群，不如使用sbt创建的程序集jar。
- en: Implementing the application in Scala
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Scala中实现应用程序
- en: 'The following code snippet is the Scala code for the log event processing application
    that processes the messages produced by the Kafka producer. The use case of this
    application is the same as the one discussed in the preceding section concerning
    windowing operations:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是用于处理由Kafka生产者产生的消息的日志事件处理应用程序的Scala代码。该应用程序的使用案例与前一节讨论的关于窗口操作的使用案例相同：
- en: '[PRE20]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Compared to the Scala code in the preceding section, the major difference is
    in the way the stream is created.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一节中的Scala代码相比，主要区别在于流创建的方式。
- en: Implementing the application in Python
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中实现应用程序
- en: 'The following code snippet is the Python code for the log event processing
    application that processes the message produced by the Kafka producer. The use
    case of this application is also the same as the one discussed in the preceding
    section concerning windowing operations:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是用于处理由Kafka生产者产生的消息的日志事件处理应用程序的Python代码。该应用程序的使用案例与前一节讨论的关于窗口操作的使用案例相同：
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following commands are run on the terminal window to run the Scala application:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令是在终端窗口中运行Scala应用程序的命令：
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following commands are run on the terminal window to run the Python application:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令是在终端窗口中运行Python应用程序的命令：
- en: '[PRE23]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When both of the preceding programs are running, whatever log event messages
    are typed into the console window of the Kafka console producer, and invoked using
    the following command and inputs, will be processed by the application. The outputs
    of this program will be very similar to the ones that are given in the preceding
    section:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当上述两个程序都在运行时，无论在Kafka控制台生产者的控制台窗口中输入什么日志事件消息，并通过以下命令和输入调用，都将由应用程序处理。该程序的输出将与前一节给出的输出非常相似：
- en: '[PRE24]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Spark provides two approaches to process Kafka streams. The first one is the
    receiver-based approach that was discussed previously and the second one is the
    direct approach.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供两种处理Kafka流的方法。第一种是之前讨论过的基于接收器的方法，第二种是直接方法。
- en: This direct approach to processing Kafka messages is a simplified method in
    which Spark Streaming is using all the possible capabilities of Kafka just like
    any of the Kafka topic consumers, and polls for the messages in the specific topic,
    and the partition by the offset number of the messages. Depending on the batch
    interval of the Spark Streaming data processing application, it picks up a certain
    number of offsets from the Kafka cluster, and this range of offsets is processed
    as a batch. This is highly efficient and ideal for processing messages with a
    requirement to have exactly-once processing. This method also reduces the Spark
    Streaming library's need to do additional work to implement the exactly-once semantics
    of the message processing and delegates that responsibility to Kafka. The programming
    constructs of this approach are slightly different in the APIs used for the data
    processing. Consult the appropriate reference material for the details.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这种直接处理Kafka消息的方法是一种简化方式，其中Spark Streaming利用Kafka的所有可能功能，就像任何Kafka主题消费者一样，通过偏移量号在特定主题和分区中轮询消息。根据Spark
    Streaming数据处理应用程序的批处理间隔，它从Kafka集群中选择一定数量的偏移量，并将此范围内的偏移量作为一批处理。这种方法高效且非常适合需要精确一次处理的消息。此方法还减少了Spark
    Streaming库实现消息处理精确一次语义的需求，并将该责任委托给Kafka。此方法的编程构造在用于数据处理的API中略有不同。请查阅相关参考资料以获取详细信息。
- en: The preceding sections introduced the concept of a Spark Streaming library and
    discussed some of the real-world use cases. There is a big difference between
    Spark data processing applications developed to process static batch data and
    those developed to process dynamic stream data in a deployment perspective. The
    availability of data processing applications to process a stream of data must
    be constant. In other words, such applications should not have components that
    are single points of failure. The following section is going to discuss this topic.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 前述章节介绍了Spark Streaming库的概念，并讨论了一些实际应用案例。从部署角度来看，开发用于处理静态批处理数据的Spark数据处理应用程序与开发用于处理动态流数据的应用程序之间存在很大差异。处理数据流的数据处理应用程序的可用性必须持续不断。换句话说，此类应用程序不应具有单点故障组件。下一节将讨论此主题。
- en: Spark Streaming jobs in production
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming作业在生产环境中
- en: When a Spark Streaming application is processing the incoming data, it is very
    important to have uninterrupted data processing capability so that all the data
    that is getting ingested is processed. In business-critical streaming applications,
    most of the time missing even one piece of data can have a huge business impact.
    To deal with such situations, it is important to avoid single points of failure
    in the application infrastructure.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当Spark Streaming应用程序处理传入数据时，确保数据处理不间断至关重要，以便所有正在摄取的数据都能得到处理。在关键业务流应用程序中，大多数情况下，即使遗漏一条数据也可能产生巨大的业务影响。为应对这种情况，避免应用程序基础设施中的单点故障至关重要。
- en: From a Spark Streaming application perspective, it is good to understand how
    the underlying components in the ecosystem are laid out so that the appropriate
    measures can be taken to avoid single points of failure.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark Streaming应用程序的角度来看，了解生态系统中底层组件的布局是有益的，以便采取适当措施避免单点故障。
- en: 'A Spark Streaming application deployed in a cluster such as Hadoop YARN, Mesos
    or Spark Standalone mode has two main components very similar to any other type
    of Spark application:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 部署在Hadoop YARN、Mesos或Spark独立模式等集群中的Spark Streaming应用程序，与其他类型的Spark应用程序一样，主要包含两个相似的组件：
- en: '**Spark driver**: This contains the application code written by the user'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark驱动程序**：包含用户编写的应用程序代码'
- en: '**Executors**: The executors that execute the jobs submitted by the Spark driver'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行器**：执行由Spark驱动程序提交的作业的执行器'
- en: But the executors have an additional component called a receiver that receives
    the data getting ingested as a stream and saves it as blocks of data in memory.
    When one receiver is receiving the data and forming the data blocks, they are
    replicated to another executor for fault-tolerance. In other words, in-memory
    replication of the data blocks is done onto a different executor. At the end of
    every batch interval, these data blocks are combined to form a DStream and sent
    out for further processing downstream.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 但执行器有一个额外的组件，称为接收器，它接收作为流输入的数据并将其保存为内存中的数据块。当一个接收器正在接收数据并形成数据块时，它们会被复制到另一个执行器以实现容错。换句话说，数据块的内存复制是在不同的执行器上完成的。在每个批处理间隔结束时，这些数据块被合并以形成DStream，并发送出去进行下游进一步处理。
- en: '*Figure 8* depicts the components working together in a Spark Streaming application
    infrastructure deployed in a cluster:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8*描绘了在集群中部署的Spark Streaming应用基础设施中协同工作的组件：'
- en: '![Spark Streaming jobs in production](img/image_06_013.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![生产环境中的Spark Streaming作业](img/image_06_013.jpg)'
- en: Figure 8
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图8
- en: In *Figure 8*, there are two executors. The receiver component is deliberately
    not displayed in the second executor to show that it is not using the receiver
    and instead just collects the replicated data blocks from the other executor.
    But when needed, such as on the failure of the first executor, the receiver in
    the second executor can start functioning.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8*中展示了两个执行器。为了表明第二个执行器并未使用接收器，而是直接从另一个执行器收集复制的块数据，故意未显示其接收器组件。但在需要时，例如第一个执行器发生故障时，第二个执行器的接收器可以开始工作。'
- en: Implementing fault-tolerance in Spark Streaming data processing applications
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Spark Streaming数据处理应用中实现容错机制
- en: Spark Streaming data processing application infrastructure has many moving parts.
    Failures can happen to any one of them, resulting in the interruption of the data
    processing. Typically failures can happen to the Spark driver or the executors.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming数据处理应用的基础设施包含多个动态部分。任何一部分都可能发生故障，导致数据处理中断。通常，故障可能发生在Spark驱动程序或执行器上。
- en: Note
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This section is not intended to give detailed treatment to running the Spark
    Streaming applications in production with fault-tolerance. The intention is to
    make the reader appreciate the precautions to be taken when deploying Spark Streaming
    data processing applications in production.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 本节并非旨在详细介绍在生产环境中运行具有容错能力的Spark Streaming应用。其目的是让读者了解在生产环境中部署Spark Streaming数据处理应用时应采取的预防措施。
- en: When an executor fails, since the replication of data is happening on a regular
    basis, the task of receiving the data stream will be taken over by the executor
    on which the data was getting replicated. There is a situation in which when an
    executor fails, all the data that is unprocessed will be lost. To circumvent this
    problem, there is a way to persist the data blocks into HDFS or Amazon S3 in the
    form of write-ahead logs.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当某个执行器发生故障时，由于数据复制是定期进行的，接收数据流的任务将由数据正在被复制的执行器接管。存在一种情况，即当执行器失败时，所有未处理的数据都将丢失。为规避此问题，可将数据块以预写日志的形式持久化到HDFS或Amazon
    S3中。
- en: Tip
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: There is no need to have both the in-memory replication of the data blocks and
    write-ahead logs together in one infrastructure. Keep only one of them, depending
    on the need.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 无需在同一基础设施中同时保留数据块的内存复制和预写日志。根据需求，只保留其中之一即可。
- en: When the Spark driver fails, the driven program is stopped, all the executors
    lose connection, and they stop functioning. This is the most dangerous situation.
    To deal with this situation, some configuration and code changes are necessary.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当Spark驱动程序失败时，驱动程序停止运行，所有执行器失去连接并停止工作。这是最危险的情况。为应对这种情况，需要进行一些配置和代码更改。
- en: 'The Spark driver has to be configured to have an automatic driver restart,
    which is supported by the cluster managers. This includes a change in the Spark
    job submission method to have the cluster mode in whichever may be the cluster
    manager. When a restart of the driver happens, to start from the place when it
    crashed, a checkpointing mechanism has to be implemented in the driver program.
    This has already been done in the code samples that are used. The following lines
    of code do that job:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Spark驱动程序必须配置为支持集群管理器的自动驱动程序重启。这包括更改Spark作业提交方法，以在任何集群管理器中具有集群模式。当驱动程序重新启动时，为了从崩溃的地方开始，必须在驱动程序程序中实现检查点机制。这在使用的代码示例中已经完成。以下代码行完成了这项工作：
- en: '[PRE25]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Tip
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In a sample application, it is fine to use a local system directory as the checkpoint
    directory. But in a production environment, it is better to keep this checkpoint
    directory as an HDFS location in the case of Hadoop or an S3 location in the case
    of an Amazon cloud.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例应用中，使用本地系统目录作为检查点目录是可以的。但在生产环境中，最好将此检查点目录保持为Hadoop情况下的HDFS位置，或亚马逊云情况下的S3位置。
- en: From an application coding perspective, the way the `StreamingContext`is created
    is slightly different. Instead of creating a new `StreamingContext`every time,
    the factory method `getOrCreate`of the `StreamingContext`is to be used with a
    function, as shown in the following code segment. If that is done, when the driver
    is restarted, the factory method will check the checkpoint directory to see whether
    an earlier `StreamingContext`was in use, and, if found in the checkpoint data,
    it is created. Otherwise, a new `StreamingContext`is created.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用编码的角度来看，创建`StreamingContext`的方式略有不同。不应每次都创建新的`StreamingContext`，而应使用函数与`StreamingContext`的工厂方法`getOrCreate`一起使用，如下面的代码段所示。如果这样做，当驱动程序重新启动时，工厂方法将检查检查点目录，以查看是否正在使用早期的`StreamingContext`，如果检查点数据中找到，则创建它。否则，将创建一个新的`StreamingContext`。
- en: 'The following code snippet gives the definition of a function that can be used
    with the `getOrCreate`factory method of the `StreamingContext`. As mentioned earlier,
    a detailed treatment of these aspects is beyond the scope of this book:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段给出了一个函数的定义，该函数可与`StreamingContext`的`getOrCreate`工厂方法一起使用。如前所述，这些方面的详细讨论超出了本书的范围：
- en: '[PRE26]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: At a data source level, it is a good idea to build parallelism for faster data
    processing and, depending on the source of data, this can be accomplished in different
    ways. Kafka inherently supports partition at the topic level, and that kind of
    scaling out mechanism supports a good amount of parallelism. As a consumer of
    Kafka topics, the Spark Streaming data processing application can have multiple
    receivers by creating multiple streams, and the data generated by those streams
    can be combined by the union operation on the Kafka streams.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据源级别，构建并行性以加快数据处理是一个好主意，并且根据数据源的不同，这可以通过不同的方式实现。Kafka本身支持主题级别的分区，这种扩展机制支持大量的并行性。作为Kafka主题的消费者，Spark
    Streaming数据处理应用可以通过创建多个流来拥有多个接收器，并且这些流生成的数据可以通过对Kafka流的联合操作来合并。
- en: The production deployment of Spark Streaming data processing applications is
    to be done purely based on the type of application that is being used. Some of
    the guidelines given previously are just introductory and conceptual in nature.
    There is no silver bullet approach to solving production deployment problems,
    and they have to evolve along with the application development.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming数据处理应用的生产部署应完全基于所使用的应用类型。之前给出的一些指导原则仅具有介绍性和概念性。解决生产部署问题没有一劳永逸的方法，它们必须随着应用开发而发展。
- en: Structured streaming
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化流
- en: In the data streaming use cases that have been covered so far, there are many
    developer tasks in terms of building of the structure data and implementing fault
    tolerance for the application. The data that has been dealt with so far in data
    streaming applications is unstructured data. Just like the batch data processing
    use cases, even in streaming use cases, if there is the capability to process
    structured data, that is a great advantage, and lots of pre-processing can be
    avoided. Data stream processing applications are continuously running applications
    and they are bound to develop failures or interruptions. In such situations, it
    is imperative to build fault tolerance in the data streaming applications.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前所讨论的数据流应用案例中，涉及众多开发者任务，包括构建结构化数据以及为应用程序实现容错机制。迄今为止在数据流应用中处理的数据均为非结构化数据。正如批量数据处理案例一样，即便在流式处理案例中，若能处理结构化数据，亦是一大优势，可避免大量预处理工作。数据流处理应用是持续运行的应用，必然会遭遇故障或中断。在此类情况下，构建数据流应用的容错机制至关重要。
- en: In any data streaming application, the data is getting ingested continuously,
    and if there is a need to interrogate the data received at any given point in
    time, the application developers have to persist the data processed into a data
    store that supports querying. In Spark 2.0, the structured streaming concept is
    built around these aspects, and the whole idea behind building this brand new
    feature from the ground up is to relieve application developers of these pain
    areas. There is a feature with the reference number SPARK-8360 being built at
    the time of writing this chapter, and its progress can be monitored by visiting
    the corresponding page.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何数据流应用中，数据持续被导入，若需在任意时间点查询接收到的数据，应用开发者必须将已处理的数据持久化至支持查询的数据存储中。在Spark 2.0中，结构化流处理概念围绕这些方面构建，全新特性自底层打造旨在减轻应用开发者在这些问题上的困扰。撰写本章时，一项编号为SPARK-8360的特性正在开发中，其进展可通过访问相应页面进行监控。
- en: The structured streaming concept can be explained using a real-world use case,
    such as the banking transaction use case we looked at before. Assume that the
    comma-separated transaction records containing the account number and transaction
    amount are coming in a stream. In the structured stream processing method, all
    these data items get ingested into an unbounded table or DataFrame that supports
    querying using Spark SQL. In other words, since the data is accumulated in a DataFrame,
    whatever data processing is possible using a DataFrame will be possible with the
    stream data as well. This reduces the burden on application developers and they
    can focus on the business logic of the application rather than the infrastructure
    related-aspects.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理概念可通过实际案例加以阐述，例如我们之前探讨的银行业务交易案例。假设以逗号分隔的交易记录（包含账号及交易金额）正以流的形式传入。在结构化流处理方法中，所有这些数据项均被导入至一个支持使用Spark
    SQL进行查询的无界表或DataFrame。换言之，由于数据累积于DataFrame中，任何可通过DataFrame实现的数据处理同样适用于流数据，从而减轻了应用开发者的负担，使其能够专注于业务逻辑而非基础设施相关方面。
- en: References
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'For more information, visit the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请访问以下链接：
- en: '[https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)'
- en: '[http://kafka.apache.org/](http://kafka.apache.org/)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://kafka.apache.org/](http://kafka.apache.org/)'
- en: '[http://spark.apache.org/docs/latest/streaming-kafka-integration.html](http://spark.apache.org/docs/latest/streaming-kafka-integration.html)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/streaming-kafka-integration.html](http://spark.apache.org/docs/latest/streaming-kafka-integration.html)'
- en: '[https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition)'
- en: '[http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar](http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar](http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar)'
- en: '[https://issues.apache.org/jira/browse/SPARK-836](https://issues.apache.org/jira/browse/SPARK-836)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://issues.apache.org/jira/browse/SPARK-836](https://issues.apache.org/jira/browse/SPARK-836)'
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'Spark provides a very powerful library on top of the Spark core to process
    the stream of data getting ingested at a high velocity. This chapter introduced
    the basics of the Spark Streaming library, and a simple log event message processing
    system has been developed with two types of data source: one uses a TCP data server
    and the other uses Kafka. At the end of the chapter, a brief look at the production
    deployment of Spark Streaming data processing applications is provided and the
    possible ways of implementing fault-tolerance in Spark Streaming data processing
    applications as discussed.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 在其核心之上提供了一个非常强大的库，用于处理高速摄取的数据流。本章介绍了 Spark Streaming 库的基础知识，并开发了一个简单的日志事件消息处理系统，该系统使用了两种类型的数据源：一种使用
    TCP 数据服务器，另一种使用 Kafka。在本章末尾，简要介绍了 Spark Streaming 数据处理应用程序的生产部署，并讨论了在 Spark Streaming
    数据处理应用程序中实现容错的可能方法。
- en: Spark 2.0 brings the capability to process and query structured data in streaming
    applications, and the concept has been introduced, which relieves application
    developers from pre-processing the unstructured data, building fault-tolerance
    and querying the data that is being ingested on a near-real-time basis.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0 引入了在流式应用程序中处理和查询结构化数据的能力，这一概念的引入减轻了应用程序开发人员对非结构化数据进行预处理、构建容错性和近乎实时地查询正在摄取的数据的负担。
- en: 'Applied mathematicians and statisticians have come up with ways and means to
    answer questions related to a new piece of data based on the *learning* that has
    already been done on an existing bank of data. Typically these questions include,
    but are not limited to: does this piece of data fit a given model, can this piece
    of data be classified in a certain way, and does this piece of data belong to
    any group or cluster?'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 应用数学家和统计学家已经提出了各种方法来回答与新数据片段相关的问题，这些问题基于对现有数据集的*学习*。通常，这些问题包括但不限于：这个数据片段是否符合给定模型，这个数据片段是否可以以某种方式分类，以及这个数据片段是否属于任何组或集群？
- en: There are lots of algorithms available to *train* a data model and ask questions
    to this *model* about the new piece of data. This rapidly evolving branch of data
    science has huge applicability in data processing, and is popularly known as machine
    learning. The next chapter is going to discuss the machine learning library of
    Spark.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多算法可用于*训练*数据模型，并向该*模型*询问有关新数据片段的问题。这一快速发展的数据科学分支在数据处理中具有巨大的适用性，并被广泛称为机器学习。下一章将讨论
    Spark 的机器学习库。
