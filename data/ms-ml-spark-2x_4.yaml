- en: Predicting Movie Reviews Using NLP and Spark Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NLP和Spark流处理预测电影评论
- en: In this chapter, we will take an in-depth look at the field of **Natural Language
    Processing** (**NLP**), not to be confused with Neuro-Linguistic Programming!
    NLP helps analyze raw textual data and extract useful information such as sentence
    structure, sentiment of text, or even translation of text between languages. Since
    many sources of data contain raw text, (for example, reviews, news articles, and
    medical records). NLP is getting more and more popular, thanks to providing an
    insight into the text and helps make automatized decisions easier.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究**自然语言处理**（**NLP**）领域，不要与神经语言编程混淆！NLP有助于分析原始文本数据并提取有用信息，如句子结构、文本情感，甚至不同语言之间的翻译。由于许多数据源包含原始文本（例如评论、新闻文章和医疗记录），NLP变得越来越受欢迎，因为它提供了对文本的洞察，并有助于更轻松地做出自动化决策。
- en: Under the hood, NLP is often using machine-learning algorithms to extract and
    model the structure of text. The power of NLP is much more visible if it is applied
    in the context of another machine method, where, for example, text can represent
    one of the input features.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，NLP通常使用机器学习算法来提取和建模文本的结构。如果将NLP应用于另一个机器方法的背景下，例如文本可以代表输入特征之一，NLP的力量就更加明显。
- en: In this chapter, we will apply NLP to analyze the *sentiment* of movie reviews.
    Based on annotated training data, we will build a classification model that is
    going to distinguish between positive and negative movie reviews. It is important
    to mention that we do not extract sentiment directly from the text (based on words
    such as love, hate, and so on), but utilize a binary classification that we have
    already explored in the previous chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将应用NLP来分析电影评论的*情感*。基于标注的训练数据，我们将构建一个分类模型，用于区分正面和负面的电影评论。重要的是要提到，我们不直接从文本中提取情感（基于诸如爱、恨等词语），而是利用我们在上一章中已经探讨过的二元分类。
- en: 'In order to accomplish this, we will take raw movie reviews that have been
    manually scored beforehand and train an ensemble-a set of models-which are as
    follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们将采用事先手动评分的原始电影评论，并训练一个集成模型-一组模型-如下所示：
- en: Process the movie reviews to synthesize the features for our model.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理电影评论以合成我们模型的特征。
- en: Here, we will explore the various features we can create with text data ranging
    from a bag-of-words approach to a weighted bag-of-words (for example, TF-IDF)
    and then briefly explore the word2vec algorithm, which we will explore in detail
    in [Chapter 5](part0101.html#30A8Q0-d18ba71168a441bd917775fac13ca893), Word2vec
    for Prediction and Clustering.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探讨使用文本数据创建各种特征的方法，从词袋模型到加权词袋模型（例如TF-IDF），然后简要探讨word2vec算法，我们将在[第5章](part0101.html#30A8Q0-d18ba71168a441bd917775fac13ca893)中详细探讨，即预测和聚类的Word2vec。
- en: Alongside this, we will look at some basic ways of feature selection/omission,
    which include removing stopwords and punctuation, or stemming.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，我们将研究一些基本的特征选择/省略方法，包括去除停用词和标点，或者词干提取。
- en: 'Using the generated features, we will run a variety of supervised, binary classification
    algorithms to help us classify positive and negative reviews, which include the
    following:'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用生成的特征，我们将运行各种监督的二元分类算法，帮助我们对正面和负面的评论进行分类，其中包括以下内容：
- en: Classification decision tree
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类决策树
- en: Naive Bayes
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Random forest
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Gradient boosted trees
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: Leveraging the combined predictive power of the four different learning algorithms,
    we will create a super-learner model, which takes all four "guesses" of the models
    as meta-features to train a deep neural network to output a final prediction.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用四种不同学习算法的综合预测能力，我们将创建一个超级学习模型，该模型将四种模型的所有“猜测”作为元特征，训练一个深度神经网络输出最终预测。
- en: 'Finally, we will create a Spark machine learning pipeline for this process,
    which does the following:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将为此过程创建一个Spark机器学习管道，该管道执行以下操作：
- en: Extracts features from new movie reviews
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从新的电影评论中提取特征
- en: Comes up with a prediction
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出一个预测
- en: Outputs this prediction inside of a Spark streaming application (yes, you will
    build your first machine learning application in every chapter for the remainder
    of this book!)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark流应用程序中输出这个预测（是的，你将在本书的剩余章节中构建你的第一个机器学习应用程序！）
- en: If this sounds a tad ambitious, take heart! We will step through each one of
    these tasks in a manner that is both methodical and purposeful so that you can
    have the confidence to build your own NLP application; but first, a little background
    history and some theory behind this exciting field.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来有点雄心勃勃，那就放心吧！我们将以一种有条理和有目的的方式逐步完成这些任务，这样你就可以有信心构建自己的NLP应用；但首先，让我们简要了解一下这个令人兴奋的领域的一些背景历史和理论。
- en: NLP - a brief primer
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP - 简要介绍
- en: 'Just like artificial neural networks, NLP is a relatively "old" subject, but
    one that has garnered a massive amount of attention recently due to the rise of
    computing power and various applications of machine learning algorithms for tasks
    that include, but are not limited to, the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 就像人工神经网络一样，NLP是一个相对“古老”的主题，但最近由于计算能力的提升和机器学习算法在包括但不限于以下任务中的各种应用，它引起了大量关注：
- en: '**Machine translation** (**MT**): In its simplest form, this is the ability
    of machines to translate one language of words to another language of words. Interestingly,
    proposals for machine translation systems pre-date the creation of the digital
    computer. One of the first NLP applications was created during World War II by
    an American scientist named Warren Weaver whose job was to try and crack German
    code. Nowadays, we have highly sophisticated applications that can translate a
    piece of text into any number of different languages we desire!'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译（MT）：在其最简单的形式中，这是机器将一种语言的词翻译成另一种语言的词的能力。有趣的是，机器翻译系统的提议早于数字计算机的创建。第一个自然语言处理应用之一是在二战期间由美国科学家沃伦·韦弗（Warren
    Weaver）创建的，他的工作是试图破译德国密码。如今，我们有高度复杂的应用程序，可以将一段文本翻译成我们想要的任意数量的不同语言！
- en: '**Speech recognition** (**SR**): These methodologies and technologies attempt
    to recognize and translate spoken words into text using machines. We see these
    technologies in smartphones nowadays that use SR systems in tasks ranging from
    helping us find directions to the nearest gas station to querying Google for the
    weekend''s weather forecast. As we speak into our phones, a machine is able to
    recognize the words we are speaking and then translate these words into text that
    the computer can recognize and perform some task if need be.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别（SR）：这些方法和技术试图利用机器识别和翻译口语到文本。我们现在在智能手机中看到这些技术，这些手机使用语音识别系统来帮助我们找到最近的加油站的方向，或者查询谷歌周末的天气预报。当我们对着手机说话时，机器能够识别我们说的话，然后将这些话翻译成计算机可以识别并执行某些任务的文本。
- en: '**Information retrieval** (**IR**): Have you ever read a piece of text, such
    as an article on a news website, for example, and wanted to see similar news articles
    like the one you just read? This is but one example of an information retrieval
    system that takes a piece of text as an "input" and seeks to obtain other relevant
    pieces of text similar to the input text. Perhaps the easiest and most recognizable
    example of an IR system is doing a search on a web-based search engine. We give
    some words that we want to "know" more about (this is the "input"), and the output
    are the search results, which are hopefully relevant to our input search query.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息检索（IR）：你是否曾经阅读过一篇文章，比如新闻网站上的一篇文章，然后想看看与你刚刚阅读的文章类似的新闻文章？这只是信息检索系统的一个例子，它以一段文本作为“输入”，并寻求获取与输入文本类似的其他相关文本。也许最简单和最常见的信息检索系统的例子是在基于网络的搜索引擎上进行搜索。我们提供一些我们想要“了解更多”的词（这是“输入”），输出是搜索结果，希望这些结果与我们的输入搜索查询相关。
- en: '**Information extraction** (**IE**): This is the task of extracting structured
    bits of information from unstructured data such as text, video and pictures. For
    example, when you read a blog post on some website, often, the post is tagged
    with a few keywords that describe the general topics about this posting, which
    can be classified using information extraction systems. One extremely popular
    avenue of IE is called *Visual Information Extraction,* which attempts to identify
    complex entities from the visual layout of a web page, for example, which would
    not be captured in typical NLP approaches.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息提取（IE）：这是从非结构化数据（如文本、视频和图片）中提取结构化信息的任务。例如，当你阅读某个网站上的博客文章时，通常会给这篇文章打上几个描述这篇文章一般主题的关键词，这可以使用信息提取系统进行分类。信息提取的一个极其受欢迎的领域是称为*视觉信息提取*，它试图从网页的视觉布局中识别复杂实体，这在典型的自然语言处理方法中无法捕捉到。
- en: '**Text summarization** (**darn, no acronym here!**): This is a hugely popular
    area of interest. This is the task of taking pieces of text of various length
    and summarizing them by identifying topics, for example. In the next chapter,
    we will explore two popular approaches to text summarization via topic models
    such as **Latent Dirichlet Allocation** (**LDA**) and **Latent Semantic Analysis**
    (**LSA**).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本摘要（该项没有缩写！）：这是一个非常受欢迎的研究领域。这是通过识别主题等方式，对各种长度的文本进行摘要的任务。在下一章中，我们将通过主题模型（如潜在狄利克雷分配（LDA）和潜在语义分析（LSA））来探讨文本摘要的两种流行方法。
- en: In this chapter, we will use NLP techniques to help us solve a binary classification
    problem for rating movie reviews from **International Movie Database** (**IMDb**).
    Let's now shift our attention to the dataset we will use and learn more about
    feature extraction techniques with Spark.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用自然语言处理技术来帮助我们解决来自国际电影数据库（IMDb）的电影评论的二元分类问题。现在让我们将注意力转移到我们将使用的数据集，并学习更多关于使用Spark进行特征提取的技术。
- en: The dataset
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: The Large Movie Review Database, originally published in the paper, *Learning
    Word Vectors for Sentiment Analysis,* by Andrew L. Maas et al, can be downloaded
    from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最初发表在Andrew L. Maas等人的论文《为情感分析学习词向量》中的《大型电影评论数据库》可以从[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)下载。
- en: The downloaded archive contains two folders labeled *train* and *test*. For
    train, there are 12,500 positive reviews and 12,500 negative reviews that we will
    train a classifier on. The test dataset contains the same amount of positive and
    negative reviews for a grand total of 50,000 positive and negative reviews amongst
    the two files.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下载的存档包含两个标记为*train*和*test*的文件夹。对于训练，有12,500条正面评价和12,500条负面评价，我们将在这些上训练一个分类器。测试数据集包含相同数量的正面和负面评价，总共有50,000条正面和负面评价在这两个文件中。
- en: 'Let''s look at an example of one review to see what the data looks like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个评论的例子，看看数据是什么样子的：
- en: '"Bromwell High is nothing short of brilliant. Expertly scripted and perfectly
    delivered, this searing parody of  students and teachers at a South London Public
    School leaves you literally rolling with laughter. It''s vulgar, provocative,
    witty and sharp. The characters are a superbly caricatured cross-section of British
    society (or to be more accurate, of any society). Following the escapades of Keisha,
    Latrina, and Natella, our three "protagonists", for want of a better term, the
    show doesn''t shy away from parodying every imaginable subject. Political correctness
    flies out the window in every episode. If you enjoy shows that aren''t afraid
    to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!"'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: “Bromwell High”简直太棒了。剧本写得精彩，表演完美，这部对南伦敦公立学校的学生和老师进行讽刺的喜剧让你捧腹大笑。它粗俗、挑衅、机智而敏锐。角色们是对英国社会（或者更准确地说，是对任何社会）的绝妙夸张。跟随凯莎、拉特丽娜和娜特拉的冒险，我们的三位“主角”，这部节目毫不避讳地对每一个可以想象的主题进行了讽刺。政治正确在每一集中都被抛在了窗外。如果你喜欢那些不怕拿每一个禁忌话题开玩笑的节目，那么《布朗韦尔高中》绝对不会让你失望！
- en: It appears that the only thing we have to work with is the raw text from the
    movie review and review sentiment; we know nothing about the date posted, who
    posted the review, and other data that may/may not be helpful to us aside from
    the text.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们唯一需要处理的是来自电影评论的原始文本和评论情感；除了文本之外，我们对发布日期、评论者以及其他可能有用的数据一无所知。
- en: Dataset preparation
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集准备
- en: 'Before running any data manipulation, we need to prepare the Spark environment
    as we did in the previous chapters. Let''s start the Spark shell and request enough
    memory to process the downloaded dataset:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行任何数据操作之前，我们需要像在前几章中那样准备好Spark环境。让我们启动Spark shell，并请求足够的内存来处理下载的数据集：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To avoid too much logging output from Spark, it is possible to control logging
    level at runtime directly by calling `setLogLevel` on SparkContext:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免Spark产生过多的日志输出，可以通过在SparkContext上调用`setLogLevel`来直接控制运行时的日志级别：
- en: '`sc.setLogLevel("WARN")`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`sc.setLogLevel("WARN")`'
- en: The command decreases the verbosity of the Spark output.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令减少了Spark输出的冗长程度。
- en: The next challenge is to read in the training dataset, which is composed of
    25,000 positive and negative movie reviews. The following lines of code will read
    in these files and then create our binary labels of 0 for a negative review and
    1 for a positive review.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个挑战是读取训练数据集，它由25,000条积极和消极的电影评论组成。以下代码将读取这些文件，然后创建我们的二进制标签，0表示消极评论，1表示积极评论。
- en: 'We directly utilize the exposed Spark `sqlContext` method, `textFile`, that
    allows for reading multiple files and returns Dataset[String]. This is the difference
    from the method mentioned in the previous chapters, which were using the method
    called `wholeTextFiles` and producing RDD[String]:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直接利用了暴露的Spark `sqlContext`方法`textFile`，它允许读取多个文件并返回Dataset[String]。这与前几章提到的方法不同，前几章使用的是`wholeTextFiles`方法，产生的是RDD[String]：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can directly show the first five lines using the dataset method `show` (you
    can modify the truncate parameter to show the full text of the review):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接使用数据集方法`show`来显示前五行（您可以修改截断参数以显示评论的完整文本）：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/00092.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00092.jpeg)'
- en: 'Next, we will do the same thing for the negative reviews:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对消极评论做同样的处理：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Take a look at the following screenshot:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下的截图：
- en: '![](img/00093.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00093.jpeg)'
- en: Now, each of the *positiveReview* and *negativeReviews* variables represents
    RDD of loaded reviews. Each row of dataset contains a string representing a single
    review. However, we still need to generate corresponding labels and merge both
    loaded datasets together.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，*positiveReview*和*negativeReviews*变量分别表示加载的评论的RDD。数据集的每一行包含一个表示单个评论的字符串。然而，我们仍然需要生成相应的标签，并将加载的两个数据集合并在一起。
- en: 'The labeling is easy, since we loaded negative and positive reviews as separated
    Spark DataFrames. We can directly append a constant column representing the label
    0 for negative reviews and 1 for positive reviews:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 标记很容易，因为我们将消极和积极的评论加载为分开的Spark数据框。我们可以直接添加一个表示消极评论的标签0和表示积极评论的标签1的常量列：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Take a look at the following screenshot:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下的截图：
- en: '![](img/00094.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00094.jpeg)'
- en: In this case, we used the `withColumn` method, which appends a new column to
    an existing dataset. The definition of a new column `lit(1.0)` means a constant
    column defined by a numeric literal *1.0*. We need to use a real number to define
    the target value, since the Spark API expects it. Finally, we merged both datasets
    together using the `union` method.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用了`withColumn`方法，它会在现有数据集中添加一个新列。新列`lit(1.0)`的定义意味着一个由数字文字*1.0*定义的常量列。我们需要使用一个实数来定义目标值，因为Spark
    API需要它。最后，我们使用`union`方法将这两个数据集合并在一起。
- en: We also appended the magic column `row_id`, which uniquely identifies each row
    in the dataset. This trick simplifies our workflow later when we need to join
    the output of several algorithms.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还添加了魔术列`row_id`，它唯一标识数据集中的每一行。这个技巧简化了我们在需要合并多个算法的输出时的工作流程。
- en: Why did we use a double value instead of a string label representation? In the
    code labeling individual reviews, we defined a constant column with numeric literals
    representing double numbers. We could use also *lit("positive")* to label positive
    reviews, but using plain text labels would force us to transform the string value
    into numeric value in the later steps anyway. Hence, in this example, we will
    make our life easier using double value labels directly. Furthermore, we used
    double values directly since it is required by the Spark API.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们使用双精度值而不是字符串标签表示？在代码标记单个评论时，我们使用了表示双精度数字的数字文字来定义一个常量列。我们也可以使用*lit("positive")*来标记积极的评论，但是使用纯文本标签会迫使我们在后续步骤中将字符串值转换为数值。因此，在这个例子中，我们将直接使用双精度值标签来简化我们的生活。此外，我们直接使用双精度值，因为Spark
    API要求这样做。
- en: Feature extraction
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取
- en: At this stage, we have only a raw text representing reviews, which is not sufficient
    to run any machine learning algorithm. We need to transform the text into a numeric
    format, aka perform the so-called "feature extraction" (it is as it sounds; we
    are taking the input data and extracting features which we will use to train a
    model). The method generates some new features based on input feature(s). There
    are many methods regarding how the text can be transformed into numeric features.
    We can count the number of words, length of text, or number of punctuations. However,
    to represent text in a systematic way that would reflect a text structure, we
    need more elaborate methods.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们只有一个代表评论的原始文本，这不足以运行任何机器学习算法。我们需要将文本转换为数字格式，也就是进行所谓的“特征提取”（就像它听起来的那样；我们正在提取输入数据并提取特征，这些特征将用于训练模型）。该方法基于输入特征生成一些新特征。有许多方法可以将文本转换为数字特征。我们可以计算单词的数量、文本的长度或标点符号的数量。然而，为了以一种系统化的方式表示文本，反映文本结构，我们需要更复杂的方法。
- en: Feature extraction method– bag-of-words model
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取方法-词袋模型
- en: 'Now that we have ingested our data and created our labels, it''s time to extract
    our features to build our binary classification model. As its name suggests, the
    bag-of-words approach is a very common feature-extraction technique whereby we
    take a piece of text, in this case a movie review, and represent it as a bag (aka
    multiset) of its words and grammatical tokens. Let''s look at an example using
    a few movie reviews:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们已经摄取了我们的数据并创建了我们的标签，是时候提取我们的特征来构建我们的二元分类模型了。顾名思义，词袋模型方法是一种非常常见的特征提取技术，我们通过这种方法将一段文本，比如一部电影评论，表示为它的单词和语法标记的袋子（也称为多重集）。让我们通过几个电影评论的例子来看一个例子： '
- en: '**Review 1:** *Jurassic World was such a flop!*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论1：** *《侏罗纪世界》真是个失败！*'
- en: '**Review 2:** *Titanic ... an instant classic. Cinematography was as good as
    the acting!!*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论2：** *《泰坦尼克号》……一个经典。摄影和表演一样出色！*'
- en: 'For each token (can be a word and/or punctuation), we will create a feature
    and then count the occurrence of that token throughout the document. Here''s what
    our bag-of-words dataset would look like for the first review:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个标记（可以是一个单词和/或标点符号），我们将创建一个特征，然后计算该标记在整个文档中的出现次数。我们的词袋数据集对于第一条评论将如下所示：
- en: '| **Review ID** | **a** | **Flop** | **Jurassic** | **such** | **World** |
    **!** |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **评论ID** | **a** | **失败** | **侏罗纪** | **如此** | **世界** | **!** |'
- en: '| Review 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 评论1 | 1 | 1 | 1 | 1 | 1 | 1 |'
- en: 'First, notice the arrangement of this dataset, often called a *document-term
    matrix* (each document [row] is composed of a certain set of words [terms] that
    make up this two-dimensional matrix). We can also arrange this differently and
    transpose the rows and columns to create-you guessed it-a *term-document matrix*
    whereby the columns now show the documents that have that particular term and
    the numbers inside the cells are the counts. Also, realize that the order of the
    words is alphabetical, which means we lose any sense of word order. This implies
    that the word "flop" is equidistant in similarity to the word "Jurassic," and
    while we know this is not true, this highlights one of the limitations of the
    bag-of-words approach: *the* *word order is lost, and sometimes, different documents
    can have the same representation but mean totally different things.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意到这个数据集的排列方式，通常称为*文档-术语矩阵*（每个文档[行]由一定的一组单词[术语]组成，构成了这个二维矩阵）。我们也可以以不同的方式排列它，并转置行和列，创建-你猜对了-一个*术语-文档矩阵*，其中列现在显示具有该特定术语的文档，单元格内的数字是计数。还要意识到单词的顺序是按字母顺序排列的，这意味着我们失去了任何单词顺序的意义。这意味着“失败”一词与“侏罗纪”一词的相似度是相等的，虽然我们知道这不是真的，但这突显了词袋模型方法的一个局限性：*单词顺序丢失了，有时，不同的文档可以有相同的表示，但意思完全不同。*
- en: In the next chapter, you will learn about an extremely powerful learning algorithm
    pioneered at Google and included in Spark called  **word-to-vector** (**word2vec**),
    which essentially digitizes terms to "encode" their meaning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将了解到一种在谷歌开发并包含在Spark中的极其强大的学习算法，称为**word-to-vector**（**word2vec**），它本质上是将术语数字化以“编码”它们的含义。
- en: Second, notice that for our given review of six tokens (including punctuation),
    we have six columns. Suppose we added the second review to our document-term-matrix;
    how would our original bag-of-words change?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，注意到对于我们给定的包括标点符号在内的六个标记的评论，我们有六列。假设我们将第二条评论添加到我们的文档-术语-矩阵中；我们原始的词袋模型会如何改变？
- en: '| **Review ID** | **a** | **acting** | **an** | **as** | **Cinematography**
    | **classic** | **flop** | **good** | **instant** | **Jurassic** | **such** |
    **Titanic** | **was** | **World** | **.** | **!** |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| **评论ID** | **a** | **表演** | **一个** | **和** | **摄影** | **经典** | **失败** | **出色**
    | **瞬间** | **侏罗纪** | **如此** | **泰坦尼克号** | **是** | **世界** | **.** | **!** |'
- en: '| Review 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 1
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 评论1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 1 |'
- en: '| Review 2 | 0 | 1 | 1 | 2 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 1 | 1 | 0 | 1 | 2
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 评论2 | 0 | 1 | 1 | 2 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 1 | 1 | 0 | 1 | 2 |'
- en: We tripled our original number of features from five to 16 tokens, which brings
    us to another consideration with this approach. Given that we must create a feature
    for every token, it's not difficult to see we will soon have an extremely wide
    and very sparse matrix representation (sparse because one document will certainly
    not contain every word/symbol/emoticon, and so on, and therefore, most of the
    cell inputs will be zero). This poses some interesting problems with respect to
    the dimensionality for our algorithms.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们原始的特征数量从五个增加到16个标记，这带来了这种方法的另一个考虑。鉴于我们必须为每个标记创建一个特征，很容易看出我们很快将拥有一个非常宽且非常稀疏的矩阵表示（稀疏是因为一个文档肯定不会包含每个单词/符号/表情符号等，因此大多数单元格输入将为零）。这对于我们的算法的维度来说提出了一些有趣的问题。
- en: Consider the situation where we are trying to train a random forest using a
    bag-of-words approach on a text document that has +200k tokens, whereby most of
    the inputs will be zero. Recall that in a tree-based learner, it is making determinations
    to "go left or go right", which is dependent on the feature type. In a bag-of-words
    example, we can count features as true or false (that is, the document has the
    term or not) or the occurrence of a term (that is, how many times does the document
    have this term). For each successive branch in our tree, the algorithm must consider
    all these features (or at least the square root of the number of features in the
    case of a random forest), which can be extremely wide and sparse, and make a decision
    that influences the overall outcome.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一种情况，我们试图在文本文档上使用词袋方法训练一个随机森林，其中有200,000多个标记，其中大多数输入将为零。请记住，在基于树的学习器中，它要做出“向左还是向右”的决定，这取决于特征类型。在词袋示例中，我们可以将特征计数为真或假（即，文档是否具有该术语）或术语的出现次数（即，文档具有该术语的次数）。对于我们树中的每个后续分支，算法必须考虑所有这些特征（或者至少考虑特征数量的平方根，例如在随机森林的情况下），这可能是非常宽泛和稀疏的，并且做出影响整体结果的决定。
- en: Luckily, you are about to learn how Spark deals with this type of dimensionality
    and sparsity along with some steps we can take to reduce the number of features
    in the next section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，您将要学习Spark如何处理这种类型的维度和稀疏性，以及我们可以在下一节中采取的一些步骤来减少特征数量。
- en: Text tokenization
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本标记化
- en: To perform feature extraction, we still need to provide individual words-tokens
    that are composing the original text. However, we do not need to consider all
    the words or characters. We can, for example, directly skip punctuations or unimportant
    words such as prepositions or articles, which mostly do not bring any useful information.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行特征提取，我们仍然需要提供组成原始文本的单词标记。但是，我们不需要考虑所有的单词或字符。例如，我们可以直接跳过标点符号或不重要的单词，如介词或冠词，这些单词大多不会带来任何有用的信息。
- en: Furthermore, a common practice is to regularize tokens to a common representation.
    This can include methods such as unification of characters (for example, using
    only lowercase characters, removing diacritics, using common character encoding
    such as utf8, and so on) or putting words into a common form (so-called stemming,
    for example, "cry"/"cries"/"cried" is represented by "cry").
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，常见做法是将标记规范化为通用表示。这可以包括诸如统一字符（例如，仅使用小写字符，删除变音符号，使用常见字符编码，如utf8等）或将单词放入通用形式（所谓的词干提取，例如，“cry”/“cries”/“cried”表示为“cry”）的方法。
- en: 'In our example, we will perform this process using the following steps:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将使用以下步骤执行此过程：
- en: Lowercase all words ("Because" and "because" are the same word).
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有单词转换为小写（“Because”和“because”是相同的单词）。
- en: Remove punctuation symbols with a regular expression function.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正则表达式函数删除标点符号。
- en: Remove stopwords. These are essentially injunctions and conjunctions such as
    *in*, *at*, *the*, *and*, *etc*, and so on, that add no contextual meaning to
    the review that we want to classify.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除停用词。这些基本上是没有上下文意义的禁令和连接词，例如*in*，*at*，*the*，*and*，*etc*，等等，这些词对我们想要分类的评论没有任何上下文意义。
- en: Find "rare tokens" that have a total number of occurrences less than three times
    in our corpus of reviews.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找在我们的评论语料库中出现次数少于三次的“稀有标记”。
- en: Finally, remove all "rare tokens."
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，删除所有“稀有标记”。
- en: Each of the steps in the preceding sequence represent our best practices when
    doing sentiment classification on text. For your situation, you may not want to
    lowercase all words (for example, "Python", the language and "python", the snake
    type, is an important distinction!). Furthermore, your stopwords list-if you choose
    to include one-may be different and incorporate more business logic given your
    task. One website that has done a fine job in collecting lists of stopwords is
    [http://www.ranks.nl/stopwords](http://www.ranks.nl/stopwords).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 前述序列中的每个步骤都代表了我们在对文本进行情感分类时的最佳实践。对于您的情况，您可能不希望将所有单词转换为小写（例如，“Python”语言和“python”蛇类是一个重要的区别！）。此外，您的停用词列表（如果选择包含）可能会有所不同，并且会根据您的任务融入更多的业务逻辑。一个收集停用词列表做得很好的网站是[http://www.ranks.nl/stopwords](http://www.ranks.nl/stopwords)。
- en: Declaring our stopwords list
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声明我们的停用词列表
- en: 'Here, we can directly reuse the list of generic English stopwords provided
    by Spark. However, we can enrich it by our specific stopwords:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以直接重用Spark提供的通用英语停用词列表。但是，我们可以通过我们特定的停用词来丰富它：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As stated earlier, this is an extremely delicate task and highly dependent on
    the business problem you are looking to solve. You may wish to add to this list
    terms that are relevant to your domain that will not help the prediction task.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所述，这是一项非常微妙的任务，严重依赖于您要解决的业务问题。您可能希望在此列表中添加与您的领域相关的术语，这些术语不会帮助预测任务。
- en: 'Declare a tokenizer that tokenizes reviews and omits all stopwords and words
    that are too short:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 声明一个标记器，对评论进行标记，并省略所有停用词和长度太短的单词：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s take a look at this function step by step to see what it''s doing. It
    accepts a single review as an input and then calls the following functions:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步查看这个函数，看看它在做什么。它接受单个评论作为输入，然后调用以下函数：
- en: '`.split("""\W+""")`: This splits movie review text into tokens that are represented
    by alphanumeric characters only.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.split("""\W+""")`：这将电影评论文本拆分为仅由字母数字字符表示的标记。'
- en: '`.map(_.toLowerCase.replaceAll("[^\\p{IsAlphabetic}]", ""))`: As a best practice,
    we lowercase the tokens so that at index time, *Java = JAVA = java*. However,
    this unification is not always the case, and it''s important that you are aware
    of the implications lowercasing your text data can have on the model. As an example,
    "Python," the computing language would lowercase to "python," which is also a
    snake. Clearly, the two tokens are not the same; however, lowercasing would make
    it so! We will also filter out all numeric characters.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.map(_.toLowerCase.replaceAll("[^\\p{IsAlphabetic}]", ""))`: 作为最佳实践，我们将标记转换为小写，以便在索引时*Java
    = JAVA = java*。然而，这种统一并不总是成立，你需要意识到将文本数据转换为小写可能会对模型产生的影响。例如，计算语言"Python"转换为小写后是"python"，这也是一种蛇。显然，这两个标记不相同；然而，转换为小写会使它们相同！我们还将过滤掉所有的数字字符。'
- en: '`.filter(w =>w.length>minTokenLen)`: Only keep those tokens whose length is
    greater than a specified limit (in our case, three characters).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.filter(w =>w.length>minTokenLen)`: 只保留长度大于指定限制的标记（在我们的例子中，是三个字符）。'
- en: '`.filter(w => !stopWords.contains(w))`: Using the stopwords list that we declared
    beforehand, we can remove these terms from our tokenized data.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.filter(w => !stopWords.contains(w))`: 使用之前声明的停用词列表，我们可以从我们的标记化数据中删除这些术语。'
- en: 'We can now directly apply the defined function on the corpus of reviews:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以直接将定义的函数应用于评论的语料库：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this case, we are marking the function `toTokens` as a Spark user-defined
    function by calling the `udf` marker, which exposes a common Scala function to
    be used in the context of the Spark DataFrame. After that, we can directly apply
    the defined `udf` function on the `reviewText` column in the loaded dataset. The
    output from the function creates a new column called `reviewTokens`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们通过调用`udf`标记将函数`toTokens`标记为Spark用户定义的函数，这将公共Scala函数暴露给在Spark DataFrame上下文中使用。之后，我们可以直接将定义的`udf`函数应用于加载的数据集中的`reviewText`列。函数的输出创建了一个名为`reviewTokens`的新列。
- en: We separated `toTokens` and `toTokensUDF` definitions since it would be easier
    to define them in one expression. This is a common practice that allows you to
    test the `toTokens` method in separation without the need of using and knowing
    Spark infrastructure.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`toTokens`和`toTokensUDF`的定义分开，因为在一个表达式中定义它们会更容易。这是一个常见的做法，可以让你在不使用和了解Spark基础设施的情况下单独测试`toTokens`方法。
- en: Furthermore, you can reuse the defined `toTokens` method among different projects,
    which do not necessarily need to be Spark-based.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以在不一定需要基于Spark的不同项目中重用定义的`toTokens`方法。
- en: 'The following code finds all the rare tokens:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码找到了所有的稀有标记：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Rare tokens computation is a complex operation. In our example, the input is
    represented by rows containing a list of tokens. However, we need to compute all
    the unique tokens and their occurrences.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 稀有标记的计算是一个复杂的操作。在我们的例子中，输入由包含标记列表的行表示。然而，我们需要计算所有唯一标记及其出现次数。
- en: Therefore, we flatten the structure into a new dataset where each row represents
    a token by using the `flatMap` method.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用`flatMap`方法将结构展平为一个新的数据集，其中每行表示一个标记。
- en: Then, we can use the same strategy that we used in the previous chapters. We
    can generate key-value pairs *(word, 1)* for each word.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用在前几章中使用的相同策略。我们可以为每个单词生成键值对*(word, 1)*。
- en: The pair is expressing the number of occurrences of the given word. Then, we
    will just group all the pairs with the same word together (the `groupByKey` method)
    and compute the total number of occurrences of the word representing a group (`reduceGroups`).
    The following steps just filter out all too frequent words and finally collect
    the result as a list of words.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这对表示了给定单词的出现次数。然后，我们只需将所有具有相同单词的对分组在一起（`groupByKey`方法），并计算代表一组的单词的总出现次数（`reduceGroups`）。接下来的步骤只是过滤掉所有太频繁的单词，最后将结果收集为单词列表。
- en: 'The next goal is to find rare tokens. In our example, we will consider each
    token with occurrences less than three as rare:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个目标是找到稀有标记。在我们的例子中，我们将考虑出现次数少于三次的每个标记：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00095.jpeg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00095.jpeg)'
- en: 'Now that we have our tokenization function, it is time to filter out rare tokens
    by defining another Spark UDF, which we will directly apply on the `reviewTokens` input
    data column:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的标记化函数，是时候通过定义另一个Spark UDF来过滤出稀有标记了，我们将直接应用于`reviewTokens`输入数据列：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Movie reviews tokens are as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 电影评论的标记如下：
- en: '![](img/00096.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00096.jpeg)'
- en: Depending on your particular task, you may wish to add or perhaps delete some
    stopwords or explore different regular expression patterns (teasing out email
    addresses using regular expressions, for example, is quite common). For now, we
    will take the tokens that we have to and use it to build our dataset.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的特定任务，你可能希望添加或删除一些停用词，或者探索不同的正则表达式模式（例如，使用正则表达式挖掘电子邮件地址是非常常见的）。现在，我们将使用我们拥有的标记构建我们的数据集。
- en: Stemming and lemmatization
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还原和词形还原
- en: 'One extremely popular step in NLP is to stem words back to their root form.
    For example, "accounts" and "accounting" would both be stemmed to "account," which
    at first blush seems very reasonable. However, stemming falls prey to the following
    two areas, which you should be aware of:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中一个非常流行的步骤是将单词还原为它们的词根形式。例如，"accounts"和"accounting"都会被还原为"account"，乍一看似乎非常合理。然而，还原会出现以下两个问题，你应该注意：
- en: '1\. **Over-stemming**: This is when stemming fails to keep two words with distinct
    meanings separate. For example, stem ("general," "genetic") = "gene".'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **过度还原**：这是指还原未能将具有不同含义的两个单词区分开。例如，还原("general," "genetic") = "gene"。
- en: '2\. **Under-stemming**: This is the inability to reduce words with the same
    meaning to their root forms. For example, stem ("jumping," "jumpiness") = *jumpi* but
    stem ("jumped," "jumps") = "jump." In this example, we know that each of the preceding
    terms are simply an inflection of the root word "jump;" however, depending on
    the stemmer you choose to employ (the two most common stemmers are Porter [oldest
    and most common] and Lancaster), you may fall into this error.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 2. **欠词干化**：这是无法将具有相同含义的单词减少到它们的根形式的能力。例如，stem（"jumping"，"jumpiness"）= *jumpi*，但stem（"jumped"，"jumps"）=
    "jump"。在这个例子中，我们知道前面的每个术语只是根词"jump"的一个变形；然而，根据您选择使用的词干提取器（最常见的两种词干提取器是Porter [最古老和最常见]和Lancaster），您可能会陷入这种错误。
- en: Given the possibilities of over and under-stemming words in your corpus, NLP
    practitioners cooked up the notion of lemmatization to help combat these known
    issues. The word "Lemming," is taking the canonical (dictionary) form of a *set
    of related words* based on the context of the word. For example, lemma ("paying,"
    "pays, "paid") = "pay." Like stemming, lemmatization tries to group related words,
    but goes one step further by trying to group words by their word sense because,
    after all, the same two words can have entirely different meanings depending on
    the context! Given the depth and complexity of this chapter already, we will refrain
    from performing any lemmatization techniques, but interested parties can read
    further about this topic at [http://stanfordnlp.github.io/CoreNLP/](http://stanfordnlp.github.io/CoreNLP/).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到语料库中单词的过度和不足词干化的可能性，自然语言处理从业者提出了词形还原的概念来帮助解决这些已知问题。单词"lemming"是根据单词的上下文，以*一组相关单词*的规范（词典）形式。例如，lemma（"paying"，"pays"，"paid"）=
    "pay"。与词干提取类似，词形还原试图将相关单词分组，但它进一步尝试通过它们的词义来分组单词，因为毕竟，相同的两个单词在不同的上下文中可能有完全不同的含义！考虑到本章已经很深入和复杂，我们将避免执行任何词形还原技术，但感兴趣的人可以在[http://stanfordnlp.github.io/CoreNLP/](http://stanfordnlp.github.io/CoreNLP/)上进一步阅读有关这个主题的内容。
- en: Featurization - feature hashing
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征化-特征哈希
- en: Now, it is time to transform string representation into a numeric one. We adopt
    a bag-of-words approach; however, we use a trick called feature hashing. Let's
    look in more detail at how Spark employs this powerful technique to help us construct
    and access our tokenized dataset efficiently. We use feature hashing as a time-efficient
    implementation of a bag-of-words, as explained earlier.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候将字符串表示转换为数字表示了。我们采用词袋方法；然而，我们使用了一个叫做特征哈希的技巧。让我们更详细地看一下Spark如何使用这种强大的技术来帮助我们高效地构建和访问我们的标记数据集。我们使用特征哈希作为词袋的时间高效实现，正如前面所解释的。
- en: 'At its core, feature hashing is a fast and space-efficient method to deal with
    high-dimensional data-typical in working with text-by converting arbitrary features
    into indices within a vector or matrix. This is best described with an example
    text. Suppose we have the following two movie reviews:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，特征哈希是一种快速和空间高效的方法，用于处理高维数据-在处理文本时很典型-通过将任意特征转换为向量或矩阵中的索引。这最好用一个例子来描述。假设我们有以下两条电影评论：
- en: '*The movie Goodfellas was well worth the money spent. Brilliant acting!*'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*电影《好家伙》物有所值。演技精湛！*'
- en: '*Goodfellas is a riveting movie with a great cast and a brilliant plot-a must
    see for all movie lovers!*'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*《好家伙》是一部扣人心弦的电影，拥有一流的演员阵容和精彩的情节-所有电影爱好者必看！*'
- en: 'For each token in these reviews, we can apply a "hashing trick," whereby we
    assign the distinct tokens a number. So, the set of unique tokens (after lowercasing
    + text processing) in the preceding two reviews would be in alphabetical order:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些评论中的每个标记，我们可以应用"哈希技巧"，从而为不同的标记分配一个数字。因此，前面两条评论中唯一标记的集合（在小写+文本处理后）将按字母顺序排列：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will then apply the hashes to create the following matrix:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将应用哈希来创建以下矩阵：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The matrix from the feature hashing is constructed as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 特征哈希的矩阵构造如下：
- en: Rows *represent* the movie review numbers.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行*代表*电影评论编号。
- en: Columns *represent* the features (not the actual words!). The feature space
    is represented by a range of used hash functions. Note that for each row, there
    is the same number of columns and not just one ever-growing, wide matrix.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列*代表*特征（而不是实际单词！）。特征空间由一系列使用的哈希函数表示。请注意，对于每一行，列的数量是相同的，而不仅仅是一个不断增长的宽矩阵。
- en: Thus, every entry in the matrix (*i, j)* = *k* means that in row *i,* feature
    *j,* appears *k* times. So, for example, the token "movie" which is hashed on
    feature 9, appears twice in the second review; therefore, matrix (2, 9) = 2.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，矩阵中的每个条目（*i，j*）= *k*表示在第*i*行，特征*j*出现*k*次。例如，标记"movie"被哈希到特征9上，在第二条评论中出现了两次；因此，矩阵（2，9）=
    2。
- en: The used hashing function makes gaps. If the hashing function hashes a small
    set of words into large numeric space, the resulting matrix will have high sparsity.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用的哈希函数会产生间隙。如果哈希函数将一小组单词哈希到大的数字空间中，得到的矩阵将具有很高的稀疏性。
- en: One important consideration to think about is the notion of hashing collisions,
    which is where two different features (tokens, in this case) are hashed into the
    same index number in our feature matrix. A way to guard against this is to choose
    a large number of features to hash, which is a parameter we can control in Spark
    (the default setting for this in Spark is 2^20 ~ 1 million features).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的一点是要考虑的是哈希碰撞的概念，即两个不同的特征（在这种情况下是标记）被哈希到我们的特征矩阵中的相同索引号。防范这种情况的方法是选择大量要哈希的特征，这是我们可以在Spark中控制的参数（Spark中的默认设置是2^20〜100万个特征）。
- en: Now, we can employ Spark's hashing function, which will map each token to a
    hash index that will make up our feature vector/matrix. As always, we will start
    with our imports of the necessary classes we will need and then change the default
    value for the number of features to create hashes against to roughly 4096 (2^12).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用Spark的哈希函数，它将每个标记映射到一个哈希索引，这将组成我们的特征向量/矩阵。与往常一样，我们将从我们需要的类的导入开始，然后将创建哈希的特征的默认值更改为大约4096（2^12）。
- en: 'In the code, we will use the `HashingTF` transformer from the Spark ML package
    (you will learn more about transformations later in this chapter). It requires
    the names of input and output columns. For our dataset `movieReviews`, the input
    column is `reviewTokens`, which holds the tokens created in the previous steps.
    The result of the transformation is stored in a new column called `tf`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们将使用Spark ML包中的`HashingTF`转换器（您将在本章后面学习更多关于转换的内容）。它需要输入和输出列的名称。对于我们的数据集`movieReviews`，输入列是`reviewTokens`，其中包含在前面步骤中创建的标记。转换的结果存储在一个名为`tf`的新列中：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00097.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.jpeg)'
- en: After invoking the transformation, the resulting `tfTokens` dataset contains
    alongside original data a new column called `tf`, which holds an instance of `org.apache.spark.ml.linalg`.
    Vector for each input row. The vector in our case is a sparse vector (because
    the hash space is much larger than the number of unique tokens).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 调用转换后，生成的`tfTokens`数据集中除了原始数据之外，还包含一个名为`tf`的新列，该列保存了每个输入行的`org.apache.spark.ml.linalg`实例。向量。在我们的情况下，向量是稀疏向量（因为哈希空间远大于唯一标记的数量）。
- en: Term Frequency - Inverse Document Frequency (TF-IDF) weighting scheme
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语频率-逆文档频率（TF-IDF）加权方案
- en: 'We will now use Spark ML to apply a very common weighting scheme called a TF-IDF
    to convert our tokenized reviews into vectors, which will be inputs to our machine
    learning models. The math behind this transformation is relatively straightforward:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用Spark ML应用一个非常常见的加权方案，称为TF-IDF，将我们的标记化评论转换为向量，这将成为我们机器学习模型的输入。这种转换背后的数学相对简单：
- en: '![](img/00098.jpeg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00098.jpeg)'
- en: 'For each token:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个标记：
- en: Find the term frequency within a given document (in our case, a movie review).
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到给定文档（在我们的情况下是电影评论）内的术语频率。
- en: Multiply this count by the log of the inverse document frequency that looks
    at how common the token occurs among all of the documents (commonly referred to
    as the corpus).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此计数乘以查看标记在所有文档中出现的频率的对数的逆文档频率（通常称为语料库）。
- en: Taking the inverse is useful, in that it will penalize tokens that occur too
    frequently in the document (for example, "movie") and boost those tokens that
    do not appear as frequently.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取逆是有用的，因为它将惩罚在文档中出现太频繁的标记（例如，“电影”），并提升那些不太频繁出现的标记。
- en: 'Now, we can scale terms based on the inverse term document frequency formula
    explained earlier. First, we need to compute a model-a prescription about how
    to scale term frequencies. In this case, we use the Spark `IDF` estimator to create
    a model based on the input data produced by the previous step `hashingTF`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以根据先前解释的逆文档频率公式来缩放术语。首先，我们需要计算一个模型-关于如何缩放术语频率的规定。在这种情况下，我们使用Spark `IDF`
    估计器基于前一步`hashingTF`生成的输入数据创建模型：
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we will build a Spark estimator that we trained (fitted) on the input
    data (= output of transformation in the previous step). The IDF estimator computes
    weights of individual tokens. Having the model, it is possible to apply it on
    any data that contains a column defined during fitting:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将构建一个Spark估计器，该估计器在输入数据（=上一步转换的输出）上进行了训练（拟合）。IDF估计器计算单个标记的权重。有了模型，就可以将其应用于包含在拟合期间定义的列的任何数据：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/00099.jpeg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpeg)'
- en: 'Let''s look in more detail at a single row and the difference between `hashingTF` and
    `IDF` outputs. Both operations produced a sparse vector the same length. We can
    look at non-zero elements and verify that both rows contain non-zero values at
    the same locations:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下单个行和`hashingTF`和`IDF`输出之间的差异。这两个操作都产生了相同长度的稀疏向量。我们可以查看非零元素，并验证这两行在相同位置包含非零值：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can also print a few non-zero values:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印一些非零值：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00100.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.jpeg)'
- en: You can directly see that tokens with the same frequency in the sentence can
    have different resulting scores based on their frequencies over all the sentences.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接看到，在句子中具有相同频率的标记根据它们在所有句子中的频率而产生不同的分数。
- en: Let's do some (model) training!
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们进行一些（模型）训练！
- en: 'At this point, we have a numeric representation of textual data, which captures
    the structure of reviews in a simple way. Now, it is time for model building.
    First, we will select columns that we need for training and split the resulting
    dataset. We will keep the generated `row_id` column in the dataset. However, we
    will not use it as an input feature, but only as a simple unique row identifier:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经对文本数据进行了数值表示，以简单的方式捕捉了评论的结构。现在是建模的时候了。首先，我们将选择需要用于训练的列，并拆分生成的数据集。我们将保留数据集中生成的`row_id`列。但是，我们不会将其用作输入特征，而只会将其用作简单的唯一行标识符：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Notice that we have created four different subsets of our data: a training
    dataset, testing dataset, transfer dataset, and a final validation dataset. The
    transfer dataset will be explained later on in the chapter, but everything else
    should appear very familiar to you already from the previous chapters.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经创建了数据的四个不同子集：训练数据集、测试数据集、转移数据集和最终验证数据集。转移数据集将在本章后面进行解释，但其他所有内容应该已经非常熟悉了。
- en: Also, the cache call is important since the majority of the algorithms are going
    to iteratively query the dataset data, and we want to avoid repeated evaluation
    of all the data preparation operations.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，缓存调用很重要，因为大多数算法将迭代地查询数据集数据，我们希望避免重复评估所有数据准备操作。
- en: Spark decision tree model
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark决策树模型
- en: 'First, let''s start with a simple decision tree and perform a grid search over
    a few of the hyper-parameters. We will follow the code from [Chapter 2](part0038.html#147LC0-d18ba71168a441bd917775fac13ca893), *Detecting
    Dark Matter: The Higgs-Boson Particle* to build our models that are trained to
    maximize the AUC statistic. However, instead of using models from the MLlib library,
    we will adopt models from the Spark ML package. The motivation of using the ML
    package will be clearer later when we will need to compose the models into a form
    of pipeline. Nevertheless, in the following code, we will use `DecisionTreeClassifier`,
    which we fit to `trainData`, generate prediction for `testData`, and evaluate
    the model''s AUC performance with the help of `BinaryClassificationEvaluato`:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从一个简单的决策树开始，并对一些超参数进行网格搜索。我们将遵循[第2章](part0038.html#147LC0-d18ba71168a441bd917775fac13ca893)中的代码，*探测暗物质：希格斯玻色子粒子*来构建我们的模型，这些模型经过训练以最大化AUC统计量。然而，我们将不再使用MLlib库中的模型，而是采用Spark
    ML包中的模型。在后面需要将模型组合成管道时，使用ML包的动机将更加清晰。然而，在下面的代码中，我们将使用`DecisionTreeClassifier`，将其拟合到`trainData`，为`testData`生成预测，并借助`BinaryClassificationEvaluato`评估模型的AUC性能：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/00101.jpeg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.jpeg)'
- en: 'After selecting the best model, we will write it into a file. This is a useful
    trick since model training can be time and resource expensive, and the next time,
    we can load the model directly from the file instead of retraining it again:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择最佳模型之后，我们将把它写入文件。这是一个有用的技巧，因为模型训练可能会耗费时间和资源，下一次，我们可以直接从文件中加载模型，而不是重新训练它：
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Spark Naive Bayes model
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark朴素贝叶斯模型
- en: Next, let's look to employ Spark's implementation of Naive Bayes. As a reminder,
    we purposely stay away from going into the algorithm itself as this has been covered
    in many machine learning books; instead, we will focus on the parameters of the
    model and ultimately, how we can "deploy" these models in a Spark streaming application
    later on in this chapter.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来使用Spark的朴素贝叶斯实现。作为提醒，我们故意避免深入算法本身，因为这在许多机器学习书籍中已经涵盖过；相反，我们将专注于模型的参数，最终，我们将在本章后面的Spark流应用中“部署”这些模型。
- en: 'Spark''s implementation of Naive Bayes is relatively straightforward, with
    just a few parameters we need to keep in mind. They are mainly as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Spark对朴素贝叶斯的实现相对简单，我们只需要记住一些参数。它们主要如下：
- en: '**getLambda**: Sometimes referred to as "additive smoothing" or "laplace smoothing,"
    this parameter allows us to smooth out the observed proportions of our categorical
    variables to create a more uniform distribution. This parameter is especially
    important when the number of categories you are trying to predict is very low
    and you don''t want entire categories to be missed due to low sampling. Enter
    the lambda parameter that "helps" you combat this by introducing some minimal
    representation of some of the categories.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**getLambda**：有时被称为“加法平滑”或“拉普拉斯平滑”，这个参数允许我们平滑观察到的分类变量的比例，以创建更均匀的分布。当你尝试预测的类别数量非常低，而你不希望由于低采样而错过整个类别时，这个参数尤为重要。输入lambda参数可以通过引入一些类别的最小表示来“帮助”你解决这个问题。'
- en: '**getModelType**: There are two options here: "*multinomial"* (default) or
    "*Bernoulli."* The *Bernoulli *model type would assume that our features are binary,
    which in our text example would be "*does review have word _____? Yes or no?*"
    The *multinomial* model type, however, takes discrete word counts. One other model
    type that is not currently implemented in Spark for Naive Bayes but is appropriate
    for you to know is a Gaussian model type. This gives our model features the freedom
    to come from a normal distribution.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**getModelType**：这里有两个选项：“*multinomial*”（默认）或“*Bernoulli*”。*Bernoulli*模型类型会假设我们的特征是二进制的，在我们的文本示例中将是“*评论中是否有单词_____？是或否？*”然而，*multinomial*模型类型采用离散的词频。另一个目前在Spark中朴素贝叶斯中没有实现但你需要知道的模型类型是高斯模型类型。这使我们的模型特征可以来自正态分布。'
- en: 'Given that we only have one hyper-parameter to deal with in this case, we will
    simply go with the default value for our lamda, but you are encouraged to try
    a grid search approach as well for optimal results:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到在这种情况下我们只有一个超参数要处理，我们将简单地使用我们的lamda的默认值，但是你也可以尝试网格搜索方法以获得最佳结果：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/00102.jpeg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.jpeg)'
- en: 'It is interesting to compare the performance of different models for the same
    input dataset. Often, it turns out that even a simple Naive Bayes algorithm lends
    itself very well to text classification tasks. The reason partly has to do with
    the first adjective of this algorithm: "naive." Specifically, this particular
    algorithm assumes that our features-which in this case are globally weighted term
    frequencies-are mutually independent. Is this true in the real world? More often,
    this assumption is often violated; however, this algorithm still could perform
    just as well, if not better, than more complex models.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同模型在相同输入数据集上的性能是很有趣的。通常情况下，即使是简单的朴素贝叶斯算法也非常适合文本分类任务。部分原因在于该算法的第一个形容词：“朴素”。具体来说，这个特定的算法假设我们的特征——在这种情况下是全局加权的词项频率——是相互独立的。在现实世界中这是真的吗？更常见的情况是这个假设经常被违反；然而，这个算法仍然可以表现得和更复杂的模型一样好，甚至更好。
- en: Spark random forest model
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark随机森林模型
- en: 'Next, we will move on to our random forest algorithm, which, as you will recall
    from the previous chapters, is an ensemble of various decision trees whereby we
    perform a grid search again alternating between various depths and other hyper-parameters,
    which will be familiar:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转向我们的随机森林算法，正如你从前面的章节中记得的那样，它是各种决策树的集成，我们将再次进行网格搜索，交替使用不同的深度和其他超参数，这将是熟悉的：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/00103.jpeg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.jpeg)'
- en: From our grid search, the highest AUC we are seeing is `0.769`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的网格搜索中，我们看到的最高AUC是`0.769`。
- en: Spark GBM model
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark GBM模型
- en: 'Finally, we will move on to our **gradient boosting machine** (**GBM**), which
    will be the final model in our ensemble of models. Note that in the previous chapters,
    we used H2O''s version of GBM, but now, we will stick with Spark and use Spark''s
    implementation of GBM as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将继续使用**梯度提升机**（**GBM**），这将是我们模型集成中的最终模型。请注意，在之前的章节中，我们使用了H2O的GBM版本，但现在，我们将坚持使用Spark，并使用Spark的GBM实现如下：
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/00104.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00104.jpeg)'
- en: 'So now, we have trained up four different learning algorithms: a (single) decision
    tree, a random forest, Naive Bayes, and a gradient boosted machine. Each provides
    a different AUC as summarized in the table here. We can see that the best performing
    model is RandomForest followed by GBM. However, it is fair to say that we did
    not perform any exhausted search for the GBM model nor did we use a high number
    of iterations as is usually recommended:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经训练了四种不同的学习算法：（单个）决策树、随机森林、朴素贝叶斯和梯度提升机。每个模型提供了不同的AUC，如表中所总结的。我们可以看到表现最好的模型是随机森林，其次是GBM。然而，公平地说，我们并没有对GBM模型进行详尽的搜索，也没有使用通常建议的高数量的迭代：
- en: '| Decision tree | 0.659 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.659 |'
- en: '| Naive Bayes | 0.484 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.484 |'
- en: '| Random forest | 0.769 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 0.769 |'
- en: '| GBM | 0.755 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| GBM | 0.755 |'
- en: Super-learner model
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超级学习者模型
- en: 'Now, we will combine the prediction power for all of these algorithms to generate
    a "super-learner" with the help of a neural network, which takes each model''s
    prediction as input and then, tries to come up with a better prediction, given
    the guesses of the individually trained models. At a high level, the architecture
    would look something like this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将结合所有这些算法的预测能力，借助神经网络生成一个“超级学习者”，该神经网络将每个模型的预测作为输入，然后尝试给出更好的预测，考虑到各个单独训练模型的猜测。在高层次上，架构会看起来像这样：
- en: '![](img/00105.jpeg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.jpeg)'
- en: We will explain further the intuition behind building a "super-learner" and
    the benefits of this approach, and teach you how to build your Spark streaming
    application, which will take in your text (that is, a movie review that you will
    write) and run it through the prediction engine of each of your models. Using
    these predictions as input into your neural network, we will yield a positive
    or negative sentiment using the combined power of the various algorithms.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进一步解释构建“超级学习者”的直觉和这种方法的好处，并教您如何构建您的Spark流应用程序，该应用程序将接收您的文本（即，您将写的电影评论）并将其通过每个模型的预测引擎。使用这些预测作为输入到您的神经网络，我们将利用各种算法的综合能力产生积极或消极的情绪。
- en: Super learner
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超级学习者
- en: 'In the preceding sections, we trained several models. Now, we will compose
    them into an ensemble called a super learner using a deep learning model. The
    process to build a super learner is straightforward (see the preceding figure):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们训练了几个模型。现在，我们将使用深度学习模型将它们组合成一个称为超级学习者的集成。构建超级学习者的过程很简单（见前面的图）：
- en: Select base algorithms (for example, GLM, random forest, GBM, and so on).
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择基本算法（例如，GLM、随机森林、GBM等）。
- en: Select a meta-learning algorithm (for example, deep learning).
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个元学习算法（例如，深度学习）。
- en: Train each of the base algorithms on the training set.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上训练每个基本算法。
- en: Perform K-fold cross-validation on each of these learners and collect the cross-validated
    predicted values from each of the base algorithms.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对这些学习者进行K折交叉验证，并收集每个基本算法的交叉验证预测值。
- en: The N cross-validated predicted values from each of the L-base algorithms can
    be combined to form a new NxL matrix. This matrix, along with the original response
    vector, is called the "level-one" data.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个L基本算法中交叉验证预测的N个值可以组合成一个新的NxL矩阵。这个矩阵连同原始响应向量被称为“一级”数据。
- en: Train the meta-learning algorithm on the level-one data.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一级数据上训练元学习算法。
- en: The super learner (or so-called "ensemble model") consists of the L-base learning
    models and the meta-learning model, which can then be used to generate predictions
    on a test set.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超级学习者（或所谓的“集成模型”）由L个基本学习模型和元学习模型组成，然后可以用于在测试集上生成预测。
- en: The key trick of ensembles is to combine a diverse set of strong learners together.
    We already discussed a similar trick in the context of the random forest algorithm.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 集成的关键技巧是将一组不同的强学习者组合在一起。我们已经在随机森林算法的上下文中讨论了类似的技巧。
- en: The PhD thesis of Erin LeDell contains much more detailed information about
    super learners and their scalability. You can find it at [http://www.stat.berkeley.edu/~ledell/papers/ledell-phd-thesis.pdf](http://www.stat.berkeley.edu/~ledell/papers/ledell-phd-thesis.pdf).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Erin LeDell的博士论文包含了关于超级学习者及其可扩展性的更详细信息。您可以在[http://www.stat.berkeley.edu/~ledell/papers/ledell-phd-thesis.pdf](http://www.stat.berkeley.edu/~ledell/papers/ledell-phd-thesis.pdf)找到它。
- en: In our example, we will simplify the whole process by skipping cross-validation
    but using a single hold-out dataset. It is important to mention that this is not
    the recommended approach!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将通过跳过交叉验证但使用单个留出数据集来简化整个过程。重要的是要提到，这不是推荐的方法！
- en: As the first step, we use trained models and a transfer dataset to get predictions
    and compose them into a new dataset, augmenting it by the actual labels.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们使用训练好的模型和一个转移数据集来获得预测，并将它们组合成一个新的数据集，通过实际标签来增强它。
- en: This sounds easy; however, we cannot use the *DataFrame#withColumn* method directly
    and create a new `DataFrame` from multiple columns from different datasets, since
    the method accepts columns only from the left-hand side `DataFrame` or constant
    columns.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很容易；然而，我们不能直接使用*DataFrame#withColumn*方法并从不同数据集的多个列创建一个新的`DataFrame`，因为该方法只接受左侧`DataFrame`或常量列的列。
- en: 'Nevertheless, we have already prepared the dataset for this situation by assigning
    a unique ID to each row. In this case, we will use it and join individual model
    predictions based on `row_id`. We also need to rename each model prediction column
    to uniquely identify the model prediction inside the dataset:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们已经通过为每一行分配一个唯一的ID来为这种情况准备了数据集。在这种情况下，我们将使用它，并根据`row_id`来合并各个模型的预测。我们还需要重命名每个模型预测列，以便在数据集中唯一标识模型预测：
- en: '[PRE24]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/00106.jpeg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00106.jpeg)'
- en: The table is composed of the models' prediction and annotated by the actual
    label. It is interesting to see how individual models agree/disagree on the predicted
    value.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 该表由模型的预测组成，并由实际标签注释。看到个体模型在预测值上的一致性/不一致性是很有趣的。
- en: 'We can use the same transformation to prepare a validation dataset for our
    super learner:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的转换来准备超级学习器的验证数据集：
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we can build our meta-learner algorithm. In this case, we will use the
    deep learning algorithm provided by the H2O machine learning library. However,
    it needs a little bit of preparation-we need to publish the prepared train and
    test data as H2O frames:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建我们的元学习算法。在这种情况下，我们将使用H2O机器学习库提供的深度学习算法。但是，它需要一点准备-我们需要将准备好的训练和测试数据发布为H2O框架：
- en: '[PRE26]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We also need to transform the `label` column into a categorical column. This
    is necessary; otherwise, the H2O deep learning algorithm would perform regression
    since the `label` column is numeric:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将`label`列转换为分类列。这是必要的；否则，H2O深度学习算法将执行回归，因为`label`列是数值型的：
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we can build an H2O deep learning model. We can directly use the Java
    API of the algorithm; however, since we would like to compose all the steps into
    a single Spark pipeline, we will utilize a wrapper exposing the Spark estimator
    API:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建一个H2O深度学习模型。我们可以直接使用该算法的Java API；但是，由于我们希望将所有步骤组合成一个单独的Spark管道，因此我们将利用一个暴露Spark估计器API的包装器：
- en: '[PRE28]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Since we directly specified the validation dataset, we can explore the performance
    of the model:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们直接指定了验证数据集，我们可以探索模型的性能：
- en: '![](img/00107.jpeg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00107.jpeg)'
- en: 'Alternatively, we can open the H2O Flow UI (by calling `hc.openFlow`) and explore
    its performance in the visual form:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以打开H2O Flow UI（通过调用`hc.openFlow`）并以可视化形式探索其性能：
- en: '![](img/00108.jpeg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.jpeg)'
- en: You can easily see that the AUC for this model on the validation dataset is
    0.868619 - the value higher than all the AUC values of the individual models.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以轻松地看到该模型在验证数据集上的AUC为0.868619-高于所有个体模型的AUC值。
- en: Composing all transformations together
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有转换组合在一起
- en: In the previous section, we developed individual steps using Spark primitives,
    that is, UDFs, native Spark algorithms, and H2O algorithms. However, to invoke
    all these transformation on unseen data requires a lot of manual effort. Hence,
    Spark introduces the concept of pipelines, mainly motivated by Python scikit pipelines
    ([http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们使用了Spark原语（即UDF、本地Spark算法和H2O算法）开发了个别步骤。但是，要在未知数据上调用所有这些转换需要大量的手动工作。因此，Spark引入了管道的概念，主要受到Python
    scikit管道的启发（[http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)）。
- en: 'To learn more about the design decision behind Python, we recommend that you
    read the excellent paper "API design for machine learning software: experiences
    from the scikit-learn project" by Lars Buitinck et al ([https://arxiv.org/abs/1309.0238](https://arxiv.org/abs/1309.0238)).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '要了解Python背后的设计决策更多信息，我们建议您阅读 Lars Buitinck等人的优秀论文"API design for machine learning
    software: experiences from the scikit-learn project"（[https://arxiv.org/abs/1309.0238](https://arxiv.org/abs/1309.0238)）。'
- en: 'The pipeline is composed of stages that are represented by estimators and transformations:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 管道由由估计器和转换器表示的阶段组成：
- en: '**Estimators**: These are the core elements that expose a fit method that creates
    a model. Most of the classification and regression algorithms are represented
    as an estimator.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**估计器**：这些是核心元素，公开了一个创建模型的fit方法。大多数分类和回归算法都表示为估计器。'
- en: '**Transformers**: These transform an input dataset into a new dataset. The
    transformers expose the method `transform`, which implements the logic of transformation.
    The transformers can produce single on multiple vectors. Most of the models produced
    by estimators are transformers-they transform an input dataset into a new dataset
    representing the prediction. Another example can be the TF transformer used in
    this section.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换器**：这些将输入数据集转换为新数据集。转换器公开了`transform`方法，该方法实现了转换的逻辑。转换器可以生成单个或多个向量。大多数估计器生成的模型都是转换器-它们将输入数据集转换为表示预测的新数据集。本节中使用的TF转换器就是一个例子。'
- en: The pipeline itself exposes the same interface as the estimator. It has the
    fit method, so it can be trained and produces a "pipeline model", which can be
    used for data transformation (it has the same interface as transformers). Hence,
    the pipelines can be combined hierarchically together. Furthermore, the individual
    pipeline stages are invoked in a sequential order; however, they can still represent
    a directed acyclic graph (for example, a stage can have two input columns, each
    produced by a different stage). In this case, the sequential order has to follow
    the topological ordering of the graph.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 管道本身公开了与估计器相同的接口。它有fit方法，因此可以进行训练并生成"管道模型"，该模型可用于数据转换（它具有与转换器相同的接口）。因此，管道可以按层次结合在一起。此外，单个管道阶段按顺序调用；但是，它们仍然可以表示有向无环图（例如，一个阶段可以有两个输入列，每个列由不同的阶段产生）。在这种情况下，顺序必须遵循图的拓扑排序。
- en: In our example, we will compose all the transformation together. However, we
    will not define a training pipeline (that is, a pipeline that will train all the
    models), but we will use the already trained models to set up the pipeline stages.
    Our motivation is to define a pipeline that we can use to score a new movie review.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将把所有的转换组合在一起。然而，我们不会定义一个训练管道（即，一个将训练所有模型的管道），而是使用已经训练好的模型来设置管道阶段。我们的动机是定义一个可以用来对新的电影评论进行评分的管道。
- en: '![](img/00109.jpeg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.jpeg)'
- en: 'So, let''s start from the beginning of our example-the first operation that
    we applied on the input data was a simple tokenizer. It was defined by a Scala
    function that we wrapped into a form of Spark UDF. However, to use it as a part
    of the pipeline we need to wrap the defined Scala function into a transformation.
    Spark does not provide any simple wrapper to do that, so it is necessary to define
    a generic transformation from scratch. We know that we will transform a single
    column into a new column. In this case, we can use `UnaryTransformer`, which exactly
    defines one-to-one column transformation. We can be a little bit more generic
    and define a generic wrapper for Scala functions (aka Spark UDFs):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从我们示例的开始开始-我们在输入数据上应用的第一个操作是一个简单的分词器。它是由一个Scala函数定义的，我们将其包装成了Spark UDF的形式。然而，为了将其作为管道的一部分使用，我们需要将定义的Scala函数包装成一个转换。Spark没有提供任何简单的包装器来做到这一点，因此需要从头开始定义一个通用的转换。我们知道我们将把一个列转换成一个新列。在这种情况下，我们可以使用`UnaryTransformer`，它确切地定义了一对一的列转换。我们可以更加通用一些，定义一个Scala函数（也就是Spark
    UDFs）的通用包装器：
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `UDFTransformer` class wraps a function `f`, which accepts a generic type
    `T`, and produces type `U`. At the Spark dataset level, it transforms an input
    column (see `UnaryTransformer`) of type `inType` into a new output column (again,
    the field is defined by `UnaryTransformer`) of the `outType` type. The class also
    has a dummy implementation of the trait `MLWritable`, which supports serialization
    of the transformer into a file.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`UDFTransformer`类包装了一个函数`f`，该函数接受一个通用类型`T`，并产生类型`U`。在Spark数据集级别上，它将一个输入列（参见`UnaryTransformer`）的类型`inType`转换为一个新的输出列（同样，该字段由`UnaryTransformer`定义）的`outType`类型。该类还具有特质`MLWritable`的虚拟实现，支持将转换器序列化到文件中。'
- en: 'Now, we just need to define our tokenizer transformer:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要定义我们的分词器转换器：
- en: '[PRE30]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The defined transformer accepts a string column (that is, a movie review) and
    produces a new column that contains an array of strings representing movie review
    tokens. The transformer is directly using the `toTokens` function, which we used
    at the beginning of the chapter.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 定义的转换器接受一个字符串列（即电影评论），并产生一个包含表示电影评论标记的字符串数组的新列。该转换器直接使用了我们在本章开头使用的`toTokens`函数。
- en: 'The next transformation should remove rare words. In this case, we will use
    a similar approach as in the previous step and utilize the defined `UDFTransformer` function:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的转换应该是删除稀有单词。在这种情况下，我们将使用与上一步类似的方法，并利用定义的`UDFTransformer`函数：
- en: '[PRE31]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This transformer accepts a column containing an array of tokens and produces
    a new column containing a filtered array of tokens. It is using the already defined
    `rareTokensFilter` Scala function.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换器接受一个包含标记数组的列，并产生一个包含过滤后标记数组的新列。它使用了已经定义的`rareTokensFilter` Scala函数。
- en: So far, we have not specified any input data dependencies, including names of
    input columns. We will keep it for the final pipeline definition.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有指定任何输入数据依赖关系，包括输入列的名称。我们将把它留到最终的管道定义中。
- en: The next steps include vectorization with the help of the `TF` method hashing
    string tokens into a large numeric space and followed by a transformation based
    on the built `IDF` model. Both transformations are already defined in the expected
    form-the first `hashingTF` transformation is already a transformer translating
    a set of tokens into numeric vectors, the second one `idfModel` accepts the numeric
    vector and scales it based on the computed coefficients.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤包括使用`TF`方法进行向量化，将字符串标记哈希成一个大的数字空间，然后基于构建的`IDF`模型进行转换。这两个转换已经以期望的形式定义好了-第一个`hashingTF`转换已经是一个将一组标记转换为数值向量的转换器，第二个`idfModel`接受数值向量并根据计算的系数对其进行缩放。
- en: These steps provide input for the trained binomial models. Each base model represents
    a transformer producing several new columns such as prediction, raw prediction,
    and probabilities. However, it is important to mention that not all models provide
    the full set of columns. For example, Spark GBM currently (Spark version 2.0.0)
    provides only the prediction column. Nevertheless, it is good enough for our example.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤为训练好的二项模型提供了输入。每个基础模型代表一个产生多个新列的转换器，例如预测、原始预测和概率。然而，重要的是要提到，并非所有模型都提供完整的列集。例如，Spark
    GBM目前（Spark版本2.0.0）只提供预测列。尽管如此，对于我们的示例来说已经足够了。
- en: 'After generating predictions, our dataset contains many columns; for example,
    input columns, columns with tokens, transformed tokens, and so on. However, to
    apply the generated meta-learner, we need only columns with prediction generated
    by the base models. Hence, we will define a column selector transformation, which
    drops all the unnecessary columns. In this case, we have a transformation-accepting
    dataset with N-columns and producing a new dataset with M-columns. Therefore,
    we cannot use `UnaryTransformer` defined earlier, and we need to define a new
    ad-hoc transformation called `ColumnSelector`:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 生成预测后，我们的数据集包含许多列；例如，输入列、带有标记的列、转换后的标记等等。然而，为了应用生成的元学习器，我们只需要基础模型生成的预测列。因此，我们将定义一个列选择器转换，删除所有不必要的列。在这种情况下，我们有一个接受N列并产生一个新的M列数据集的转换。因此，我们不能使用之前定义的`UnaryTransformer`，我们需要定义一个名为`ColumnSelector`的新的特定转换：
- en: '[PRE32]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`ColumnSelector` represents a generic transformer that selects only the given
    columns from the input dataset. It is important to mention the overall two-stages
    concept-the first stage transforms the schema (that is, the metadata associated
    with each dataset) and the second transforms the actual dataset. The separation
    allows Spark to invoke early checks on transformers to find incompatibilities
    before invoking actual data transformations.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`ColumnSelector`表示一个通用的转换器，它从输入数据集中仅选择给定的列。重要的是要提到整体的两阶段概念-第一阶段转换模式（即，与每个数据集相关联的元数据）和第二阶段转换实际数据集。这种分离允许Spark在调用实际数据转换之前对转换器进行早期检查，以查找不兼容之处。'
- en: 'We need to define the actual column selector transformer by creating an instance
    of `columnSelector`-be aware of specifying the right columns to keep:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通过创建`columnSelector`的实例来定义实际的列选择器转换器-请注意指定要保留的正确列：
- en: '[PRE33]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'At this point, our transformers are ready to be composed into the final "super-learning"
    pipeline. The API of the pipeline is straightforward-it accepts individual stages
    that are invoked sequentially. However, we still need to specify dependencies
    between individual stages. Mostly the dependency is described by input and output
    column names:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们的转换器已经准备好组成最终的“超级学习”管道。管道的API很简单-它接受按顺序调用的单个阶段。然而，我们仍然需要指定单个阶段之间的依赖关系。大多数情况下，依赖关系是由输入和输出列名描述的：
- en: '[PRE34]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'There are a few important concepts worth mentioning:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些值得一提的重要概念：
- en: The `tokenizerTransformer` and `rareTokensFilterTransformer` are connected via
    the column `allReviewTokens`-the first one is the column producer, and the second
    one is the column consumer.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizerTransformer`和`rareTokensFilterTransformer`通过列`allReviewTokens`连接-第一个是列生产者，第二个是列消费者。'
- en: The `dtModel`, `nbModel`, `rfModel`, and `gbmModel` models have the same input
    column defined as `idf.getOutputColumn`. In this case, we have effectively used
    computation DAG, which is topologically ordered into a sequence
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtModel`、`nbModel`、`rfModel`和`gbmModel`模型都将相同的输入列定义为`idf.getOutputColumn`。在这种情况下，我们有效地使用了计算DAG，它是按拓扑顺序排列成一个序列'
- en: All the models have the same output columns (with some exceptions, in the case
    of GBM), which cannot be appended into the resulting dataset all together since
    the pipeline expects unique names of columns. Hence, we need to rename the output
    columns of the models by calling `setPredictionCol`, `setRawPredictionCol`, and
    `setProbabilityCol`. It is important to mention that the GBM does not produce
    raw prediction and probabilities columns right now.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有模型都具有相同的输出列（在GBM的情况下有一些例外），由于管道期望列的唯一名称，因此不能将所有模型的输出列一起追加到结果数据集中。因此，我们需要通过调用`setPredictionCol`、`setRawPredictionCol`和`setProbabilityCol`来重命名模型的输出列。重要的是要提到，GBM目前不会产生原始预测和概率列。
- en: 'Now, we can fit the pipeline to get the pipeline model. This is, in fact, an
    empty operation, since our pipeline is composed only of transformers. However,
    we still need to call the `fit` method:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以拟合管道以获得管道模型。实际上，这是一个空操作，因为我们的管道只由转换器组成。然而，我们仍然需要调用`fit`方法：
- en: '[PRE35]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Voila, we have our super-learner model, composed of multiple Spark models and
    orchestrated by the H2O deep learning model. It is time to use the model to make
    a prediction!
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，我们有了我们的超级学习模型，由多个Spark模型组成，并由H2O深度学习模型编排。现在是使用模型进行预测的时候了！
- en: Using the super-learner model
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用超级学习模型
- en: 'The usage of the model is easy-we need to provide a dataset with a single column
    called `reviewText` and transform it with `superLearnerModel`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的使用很简单-我们需要提供一个名为`reviewText`的单列数据集，并用`superLearnerModel`进行转换：
- en: '[PRE36]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The returned prediction `reviewPrediction` is a dataset with the following
    structure:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的预测`reviewPrediction`是一个具有以下结构的数据集：
- en: '[PRE37]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/00110.jpeg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00110.jpeg)'
- en: The first column contains the predicted value, which was decided based on the
    F1 threshold. The columns `p0` and `p1` represent probabilities of individual
    prediction classes.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列包含基于F1阈值决定的预测值。列`p0`和`p1`表示各个预测类别的概率。
- en: 'If we explore the content of the returned dataset, it contains a single row:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们探索返回的数据集的内容，它包含一行：
- en: '[PRE38]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/00111.jpeg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00111.jpeg)'
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter demonstrated three powerful concepts: the processing of text,
    Spark pipelines, and super learners.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本章演示了三个强大的概念：文本处理、Spark管道和超级学习者。
- en: The text processing is a powerful concept that is waiting to be largely adopted
    by the industry. Hence, we will go deeper into the topic in the following chapters
    and look at other approaches of natural language processing.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 文本处理是一个强大的概念，正在等待被行业广泛采用。因此，我们将在接下来的章节中深入探讨这个主题，并看看自然语言处理的其他方法。
- en: The same holds for Spark pipelines, which have become an inherent part of Spark
    and the core of the Spark ML package. They offer an elegant way of reusing the
    same concepts during training and scoring time. Hence, we would like to use the
    concept in the upcoming chapters as well.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Spark管道也是一样，它们已经成为Spark的固有部分和Spark ML包的核心。它们提供了一种优雅的方式，在训练和评分时重复使用相同的概念。因此，我们也希望在接下来的章节中使用这个概念。
- en: Finally, with super learners, aka ensembles, you learned the basic concept of
    how to benefit from ensembling multiple models together with the help of a meta-learner.
    This offers a simple but powerful way of building strong learners, which are still
    simple enough to understand.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过超级学习者，也就是集成学习，您学会了如何通过元学习器的帮助从多个模型中获益的基本概念。这提供了一种简单但强大的方式来构建强大的学习者，这些学习者仍然足够简单易懂。
