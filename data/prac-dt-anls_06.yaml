- en: Gathering and Loading Data in Python
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中收集和加载数据
- en: This chapter will explain what SQL is and why it is important for data analysis
    by teaching you how to use and access databases using SQLite for our examples.
    An overview of relational database technology will be provided along with insightful
    information on database systems to help to improve your data literacy when communicating
    with experts. You will also learn how to run SQL `SELECT` queries from the Jupyter
    Notebook and how to load them into DataFrames. Basic statistics, data lineage,
    and metadata (data about data) will be explained using the `pandas` library.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过教授您如何使用SQLite数据库来使用和访问数据库，解释SQL是什么以及为什么它对数据分析很重要。将提供关系数据库技术的概述，以及关于数据库系统的深入信息，以帮助提高您与专家沟通时的数据素养。您还将学习如何在Jupyter
    Notebook中运行SQL `SELECT`查询，以及如何将它们加载到DataFrames中。基本统计、数据血缘和元数据（关于数据的数据）将使用`pandas`库进行解释。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Introduction to SQL and relational databases
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL和关系数据库简介
- en: From SQL to pandas DataFrames
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从SQL到pandas DataFrames
- en: Data about your data explained
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释您数据的数据
- en: The importance of data lineage
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据血缘的重要性
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Here's the GitHub repository of this book: [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter05](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter05).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的GitHub仓库：[https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter05](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter05)。
- en: You can download and install the required software from the following link: [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从以下链接下载和安装所需的软件：[https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)。
- en: Introduction to SQL and relational databases
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL和关系数据库简介
- en: We are now at a point in this book where my professional career started, working
    with databases and SQL. **Structured Query Language** (**SQL**) was created decades
    ago as a means to communicate with structured data stored in tables. Over the
    years, SQL has evolved from multiple variations that were specific to the underlining
    database technology. For example, IBM, Oracle, and Sybase all had variations in
    their SQL commands, which built loyalty in their customers but required changes
    when switching vendors. The adoption of the **International Organization for Standardization **(**ISO**)
    and **American National Standards Institute** (**ANSI**) standards helped to define
    what is commonly used today.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在处于本书的这样一个阶段，我的职业生涯开始于与数据库和SQL一起工作。**结构化查询语言**（**SQL**）是几十年前作为一种与存储在表中的结构化数据进行通信的手段而创建的。多年来，SQL已经从针对底层数据库技术的多种变体中演变而来。例如，IBM、Oracle和Sybase都对其SQL命令有所变体，这在其客户中建立了忠诚度，但在更换供应商时需要做出改变。**国际标准化组织**（**ISO**）和**美国国家标准协会**（**ANSI**）标准的采用有助于定义今天普遍使用的标准。
- en: 'So far in this book, all of the examples of structured data focused on one
    table or file. Relational databases solve the problem of storing data together
    in multiple tables while keeping consistency across them using the concept of
    a primary and foreign key:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书中的所有结构化数据示例都集中在单个表或文件上。关系数据库通过使用主键和外键的概念，解决了在多个表中存储数据并保持它们之间一致性的问题：
- en: A primary key is the unique value (typically an integer) used to represent a
    single distinct record or tuple in each table.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主键是用于表示每个表中单个不同记录或元组的唯一值（通常是整数）。
- en: A foreign key would be a field in one table that references the primary key
    from another.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外键将是一个表中的字段，它引用另一个表的主键。
- en: This relationship defines integrity between one or more tables for consistency
    for all of the data. Since the concept of storing and joining the data is abstract,
    this allows it to be applied to many different data subjects. For example, you
    can create a database to store sales from a manufacturing company, user hits from
    a website, or stock purchases in a financial services company. Because of this
    versatility, SQL remains a top programming language and a must-have skill for
    data analysis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种关系定义了在所有数据中保持一致性的一个或多个表之间的完整性。由于存储和连接数据的概念是抽象的，这使得它可以应用于许多不同的数据主题。例如，您可以创建一个数据库来存储制造公司的销售数据，网站的用户点击，或金融服务公司的股票购买。正因为这种多功能性，SQL仍然是顶级编程语言，也是数据分析必备的技能。
- en: SQL was created to communicate with data stored in database tables that have
    a defined schema. A database schema is like a blueprint that defines a structure
    for storing data before the data is loaded. This definition includes rules, conditions,
    and specific data types for each field in a table. The foundation for the database
    technology was created by Dr. EF Codd back in 1970 and was a milestone of the *Evolution
    of Data Analysis*, which I defined in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml), *Fundamentals
    of Data Analysis*. The concept of persisting data in defined columns and rows
    as tables in a structured relationship showcases the legacy of Dr. Codd's contribution
    to this technology and data. His contributions to the technology along with others
    such as Ralph Kimball and Bill Inmon has created new industries and careers over
    the years. If you come across an **Enterprise Data Warehouse** (**EDW**), you
    can bet money it uses the Kimball or Inmon methods as a standard. Their influence, which
    defined new skills to work with data, cannot be understated. I have immense gratitude
    for the people who have evolved technologies and concepts supporting all things
    data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 是为了与存储在具有定义模式的数据库表中的数据进行通信而创建的。数据库模式就像一个蓝图，在数据加载之前定义了存储数据的结构。这个定义包括规则、条件和表中每个字段的特定数据类型。数据库技术的基石是由
    Codd 博士在 1970 年创立的，它是 *数据分析演变* 的重要里程碑，我在 [第一章](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml)，*数据分析基础*
    中定义了这一点。将数据持久化在定义的列和行中作为结构化关系中的表，展示了 Codd 博士对这一技术和数据的贡献的遗产。他与 Ralph Kimball 和
    Bill Inmon 等人的贡献在多年中创造了新的行业和职业。如果你遇到一个 **企业数据仓库** (**EDW**)，你可以肯定它使用 Kimball 或
    Inmon 方法作为标准。他们的影响，定义了与数据工作的新技能，不容小觑。我对那些推动支持所有数据技术和概念的技术和概念发展的人怀有深深的感激。
- en: What is defined as a relational database is a vast subject so I'm going to focus
    on what is relevant for building your data literacy and the analysis of data from
    consuming data using SQL. The key concepts to focus on behind working with any
    **Relational Database Management System** (**RDBMS**) begin with how to communicate
    with the system or servers that host the database. Most of them support using
    an ODBC driver, which handles authentication and communication over a network.
    **Open Database Connectivity** (**ODBC**) is a common standard used to send and
    receive data between your analysis tool, for example, the Jupyter Notebook, and
    where the data is stored. Most large-scale, enterprise relational database systems
    support ODBC connections to communicate with the database.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 关系数据库的定义是一个庞大的主题，所以我将专注于构建你的数据素养和从使用 SQL 消费数据进行分析的相关内容。在处理任何 **关系数据库管理系统** (**RDBMS**)
    时，关键概念始于如何与托管数据库的系统或服务器进行通信。大多数系统支持使用 ODBC 驱动程序，该驱动程序处理网络上的身份验证和通信。**开放数据库连接**
    (**ODBC**) 是一个用于在分析工具（例如 Jupyter Notebook）和数据存储之间发送和接收数据的常用标准。大多数大型企业关系数据库系统支持
    ODBC 连接以与数据库进行通信。
- en: Traditionally, this would be known as a client-server architecture, where your
    local computer is known as the client and the location of the database would be
    managed by one or more servers. When I was a consultant, the most common enterprise
    RDBMSes I worked with were Microsoft SQL Server, Oracle, IBM DB2, MySQL, and PostgreSQL.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，这被称为客户端-服务器架构，其中你的本地计算机被称为客户端，数据库的位置由一个或多个服务器管理。当我是一名顾问时，我工作中遇到的最常见的企业关系数据库管理系统是
    Microsoft SQL Server、Oracle、IBM DB2、MySQL 和 PostgreSQL。
- en: An ODBC driver may need to be installed and configured on your workstation to
    communicate with a client-server architecture.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要在你的工作站上安装和配置 ODBC 驱动程序，以便与客户端-服务器架构进行通信。
- en: Today, other flavors of both open source and vendor database products exist
    but many do and should support SQL or a variation of it. For example, Apache's
    HiveQL is very similar to ASCI SQL but runs on top of the **Hadoop Distributed
    File System** (**HDFS**) instead of a database. For our examples, we will be using
    SQLite, which is a file-based database you can install locally or connect with
    via ODBC. SQLite is open source and cross-platform, which means we can install
    it on any operating system, and it is touted as the *most widely deployed and
    used database engine in the world*according to their download page, which you
    can find in the *Further reading* section.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开源和供应商数据库产品都有其他版本，但许多确实应该支持SQL或其变体。例如，Apache的HiveQL与ASCI SQL非常相似，但它在**Hadoop分布式文件系统**（**HDFS**）上运行，而不是在数据库上。在我们的示例中，我们将使用SQLite，这是一个基于文件的数据库，你可以本地安装或通过ODBC连接。SQLite是开源的，跨平台的，这意味着我们可以在任何操作系统上安装它，根据他们的下载页面，它被誉为*世界上部署和使用最广泛的数据库引擎*，你可以在*进一步阅读*部分找到它。
- en: Once a connection has been established, a user ID and password are commonly
    required, which control what actions you can perform and which tables you can
    access. If you installed the database yourself, you are the owner of the database
    and probably have system administrator rights, which gives you full access to
    create, delete, and read any table. If you are a client, the **Database Administrator**
    (**DBA**) would be responsible for setting up access and permission for your user
    ID.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦建立了连接，通常需要用户ID和密码，这些控制你可以执行的操作以及你可以访问的表。如果你自己安装了数据库，你就是数据库的所有者，可能拥有系统管理员权限，这让你可以完全访问创建、删除和读取任何表。如果你是客户端，**数据库管理员**（**DBA**）将负责为你的用户ID设置访问和权限。
- en: I find what makes the SQL a popular language even today is the learning curve
    required to use it. In my experience, many business users and data analysts find
    the syntax intuitively obvious even without a background in computer science.
    SQL code is easy to read and it's quickly understood what the expected results are.
    It also supports instant gratification where a few commands can produce results
    in less than one second even with large volumes of data once it's been optimized
    for performance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为即使到现在，SQL之所以成为一门流行的语言，是因为使用它所需的曲线学习。根据我的经验，许多商业用户和数据分析师即使没有计算机科学背景，也会觉得其语法直观易懂。SQL代码易于阅读，并且可以迅速理解预期的结果。它还支持即时满足，一旦优化性能，即使数据量很大，几个命令也能在不到一秒内产生结果。
- en: 'For example, let''s say I want to know the highest closing stock price of Apple
    stock in all of 2018\. Even without really understanding all of the details behind
    how or where that data is stored, the syntax for this one line of code is easy
    to interpret:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我想知道2018年苹果股票的最高收盘价。即使没有真正理解这些数据是如何或在哪里存储的所有细节，这一行代码的语法也容易理解：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s walk through this code and break out the key components:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分析这段代码，并分解其关键组件：
- en: First, I capitalized the reserved words, which are universal across any RDBMS
    that supports ISO standard/ASCI SQL.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我将保留词大写了，这些保留词在支持ISO标准/ASCI SQL的任何关系数据库管理系统（RDBMS）中都是通用的。
- en: The `SELECT` command instructs the code to retrieve data in the form of rows
    and columns from a table defined after the `FROM` statement.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SELECT`命令指示代码从`FROM`语句之后定义的表中以行和列的形式检索数据。'
- en: Between the `SELECT` and the `FROM` reserved words is the `max(closing_price)` command.
    This is using the `max()` function that is available in SQL to retrieve the maximum
    or largest value from the `closing_price` field. The max function will only return
    one row and one value regardless of whether duplicate values exist in the data.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`SELECT`和`FROM`保留词之间是`max(closing_price)`命令。这是使用SQL中可用的`max()`函数从`closing_price`字段检索最大或最大值。无论数据中是否存在重复值，max函数都只返回一行和一个值。
- en: The `FROM` section of the code lets the SQL interpreter know a table or object
    is being referenced immediately afterward. For this example, we are looking for
    records from the `tbl_stock_price` table.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码中的`FROM`部分让SQL解释器知道紧随其后的是引用一个表或对象。在这个例子中，我们正在寻找`tbl_stock_price`表中的记录。
- en: The `WHERE` clause from the `SELECT` SQL statement restricts the data by reducing
    the number of rows to a specific condition, which is defined by a specific field
    of `year` and `value` to the right of the equals sign of `2018`.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SELECT` SQL语句中的`WHERE`子句通过减少行数到特定的条件来限制数据，该条件由等号右侧的特定字段`year`和`value`定义，即`2018`。'
- en: '`SELECT` is the most common SQL command and has many different use cases and
    levels of complexity. We are just scratching the surface but you can find more
    resources in the *Further reading* section.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`SELECT` 是最常用的 SQL 命令，具有许多不同的使用场景和复杂度级别。我们只是触及了表面，你可以在*进一步阅读*部分找到更多资源。'
- en: SQL is not case sensitive but the tables and fields referenced might be, depending
    on which RDBMS is being used. Spaces are important between reserve words but you
    typically won't find spaces in the table or field names. Rather, underscores or
    dashes are common.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 对大小写不敏感，但所引用的表和字段可能因所使用的 RDBMS 而异。保留字之间需要有空格，但通常在表或字段名称中找不到空格。相反，下划线或破折号更为常见。
- en: From SQL to pandas DataFrames
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 SQL 到 pandas DataFrame
- en: Now that we have some background on SQL and relational databases, let's download
    a local copy of an SQLite database file, set up a connection, and load some data
    into a `pandas` DataFrame. For this example, I have provided the database file
    named `customer_sales.db` so be sure to download it from the GitHub repository
    beforehand.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 SQL 和关系数据库有了背景知识，让我们下载 SQLite 数据库文件的本地副本，设置连接，并将一些数据加载到 `pandas` DataFrame
    中。为此示例，我提供了名为 `customer_sales.db` 的数据库文件，所以请确保在 GitHub 存储库中事先下载它。
- en: 'To give you some context about this database file and support the **Know Your
    Data** (**KYD**) concept that we learned in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml),
    *Fundamentals* *of Data Analysis*, we have three tables named `tbl_customers`,
    `tbl_products`, and `tbl_sales`. This would be a simple example of any company
    that has customers who purchase products that generate sales over any period of
    time. A visual representation of how the data is stored and joined together, which
    is commonly known as an **ERD** (short for **Entity Relationship Diagram**), is
    shown in the following diagram:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你了解这个数据库文件，并支持我们在[第1章](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml)“数据分析基础”中学到的**了解你的数据**（**KYD**）概念，我们创建了三个名为
    `tbl_customers`、`tbl_products` 和 `tbl_sales` 的表格。这将是任何拥有客户购买产品并在任何时间段内产生销售的公司的一个简单示例。数据存储和连接的视觉表示，通常称为**ERD**（实体关系图），如下所示：
- en: '![](img/349bdfe8-02e1-43dd-9926-6fc1bbe4084b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/349bdfe8-02e1-43dd-9926-6fc1bbe4084b.png)'
- en: In the preceding diagram, we have a visual of three tables with the column name
    defined on the left side of each box and the data type of each column immediately
    to the right. The primary key for each table is identified with a suffix in the
    name of `_ID`, along with bolding the text in the first row of each table. The primary
    key commonly has a data type of integer, which is also the case here.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到三个表格的视觉表示，每个表格的列名定义在每个框的左侧，而每个列的数据类型则紧挨着其右侧。每个表格的主键通过名称后缀`_ID`来标识，并在每个表格的第一行文本上使用粗体表示。主键通常具有整数数据类型，这里也是如此。
- en: The `tbl_sales` table includes two of those fields, `Customer_ID` and `Product_ID`, which
    means they are classified as foreign keys. The lines between the tables reinforce
    the relationship between them, which also indicates how to join them together.
    The small lines that look like *crow's feet* tell the consumer these tables are
    defined with a one-to-many relationship. In this example, `tbl_sales` will have
    many customers and many products but a record in `tbl_customers` will only have
    one value assigned per `Customer_ID` and `tbl_products` will only have one value
    assigned per `Product_ID`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`tbl_sales` 表包括两个这样的字段，`Customer_ID` 和 `Product_ID`，这意味着它们被分类为外键。表之间的线条加强了它们之间的关系，这也指示了如何将它们连接起来。看起来像*鸟爪*的小线条告诉消费者这些表是以一对一的关系定义的。在这个例子中，`tbl_sales`
    将会有许多客户和许多产品，但每个 `Customer_ID` 在 `tbl_customers` 中只会分配一个值，而 `tbl_products` 将只会为每个
    `Product_ID` 分配一个值。'
- en: 'Now that we have more information about the data, let''s launch a new Jupyter
    notebook and name it `retrieve_sql_and_create_dataframe`. To create a connection
    and use SQLite, we have to import a new library using code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对数据有了更多了解，让我们启动一个新的 Jupyter 笔记本，并将其命名为 `retrieve_sql_and_create_dataframe`。为了创建连接并使用
    SQLite，我们必须使用代码导入一个新的库：
- en: 'To load an SQLite database connection, you just need to add the following command
    in your Jupyter notebook and run the cell. Feel free to follow along by creating
    your own notebook (I have placed a copy in GitHub for reference):'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要加载 SQLite 数据库连接，你只需在你的 Jupyter 笔记本中添加以下命令并运行该单元格。你可以自由地跟随操作，创建自己的笔记本（我已经在 GitHub
    上放置了一个副本以供参考）：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `sqlite3` module comes with the Anaconda distribution installed. Refer to
    [Chapter 2](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml), *Overview of Python and
    Installing Jupyter Notebook*, for help with setting up your environment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`sqlite3` 模块是 Anaconda 分发版自带安装的。有关设置环境的帮助，请参阅 [第 2 章](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml)，*Python
    和 Jupyter Notebook 安装概述*。'
- en: 'Next, we need to assign a connection to a variable named `conn` and point to
    the location of the database file, which is named `customer_sales.db`. Since we
    already imported the `sqlite3` library in the prior `In[]` line, we can use this
    built-in function to communicate with the database:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将一个名为 `conn` 的连接分配给变量，并指向数据库文件的位置，该文件名为 `customer_sales.db`。由于我们在之前的
    `In[]` 行中已经导入了 `sqlite3` 库，我们可以使用这个内置函数与数据库通信：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Be sure to copy the `customer_sales.db` file to the correct Jupyter folder directory
    to avoid errors with the connection.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保将 `customer_sales.db` 文件复制到正确的 Jupyter 文件夹目录，以避免连接错误。
- en: 'The next library to import should be very familiar, which allows us to use
    `pandas` so the code will be as follows:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个需要导入的库应该是非常熟悉的，这样我们就可以使用 `pandas`，从而使代码如下所示：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To run a SQL statement and assign the results to a DataFrame, we have to run
    this one line of code. The `pandas` library includes a `read_sql_query()` function
    to make it easier to communicate with databases using SQL. It requires a connection
    parameter, which we named `conn` in the previous steps. We assign the results
    to a new DataFrame as `df_sales` to make it easier to identify:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要运行一个 SQL 语句并将结果分配给一个 DataFrame，我们必须运行这一行代码。`pandas` 库包含一个 `read_sql_query()`
    函数，这使得使用 SQL 与数据库通信变得更容易。它需要一个连接参数，我们在前面的步骤中将其命名为 `conn`。我们将结果分配给一个新的 DataFrame
    作为 `df_sales` 以便更容易识别：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have the results in a DataFrame, we can use all of the available
    `pandas` library commands against this data without going back to the database.
    To view the results, we can just run the `head()` command against this DataFrame
    using this code:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将结果放入 DataFrame 中，我们可以使用所有可用的 `pandas` 库命令来处理这些数据，而无需返回到数据库。要查看结果，我们只需运行
    `head()` 命令针对这个 DataFrame 并使用此代码：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output will look like the following screenshot where the `tbl_sales` table has
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中 `tbl_sales` 表已经被加载到一个带有标签的标题行 DataFrame 中，索引列位于左侧，起始值为 `0`：
- en: '![](img/c32c6615-5457-4e20-8630-ffcfbdc6d11e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c32c6615-5457-4e20-8630-ffcfbdc6d11e.png)'
- en: 'To sort the values in the DataFrame, we can use the `sort_values()` function
    and include a parameter of the field name, which will default to ascending order.
    Let''s begin by sorting the results by date to see when the first sale was recorded
    in the database by using this command:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要对 DataFrame 中的值进行排序，我们可以使用 `sort_values()` 函数并包含一个字段名称参数，它将默认为升序。让我们首先按日期排序结果，以查看数据库中首次记录的销售日期：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output would look like the following screenshot where the DataFrame output
    is now sorted by the `Sale_Date` field from `1/15/2015` to `6/9/2019`. Notice
    the difference in `Sale_ID`, which is out of sequence:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中 DataFrame 输出现在按 `Sale_Date` 字段从 `1/15/2015` 到 `6/9/2019` 排序。注意
    `Sale_ID` 的差异，它是不连续的：
- en: '![](img/9169e662-b2d9-4523-8f1a-c531c5104c27.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9169e662-b2d9-4523-8f1a-c531c5104c27.png)'
- en: 'To limit the data displayed, we can use the `DataFrame.loc` command to isolate
    specific rows or columns based on how it is labeled by the header row. To retrieve
    the first row available, we simply run this command against our DataFrame and
    reference the index value, which begins with `0`:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要限制显示的数据，我们可以使用 `DataFrame.loc` 命令根据标题行的标签来隔离特定的行或列。要检索第一行可用的数据，我们只需运行这个命令针对我们的
    DataFrame 并引用索引值，它从 `0` 开始：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output would look like the following screenshot where a single record is
    displayed as a series with the rows transposed from multiple columns to multiple
    rows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中一条记录以序列的形式显示，多列转置为多行：
- en: '![](img/4ded9bc9-f8e9-4e4c-8cb4-17bd6eb75751.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ded9bc9-f8e9-4e4c-8cb4-17bd6eb75751.png)'
- en: Using this method, you must know which specific record you are looking for by
    index, which reflects how the data was loaded from the SQL statement. To ensure
    consistency between the database tables, you may want to include an `ORDER BY`
    command when loading the data into the DataFrame.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，你必须知道你正在通过索引查找哪个特定记录，这反映了数据是如何从 SQL 语句中加载的。为了确保数据库表之间的一致性，你可能想在将数据加载到
    DataFrame 中时包含一个 `ORDER BY` 命令。
- en: 'To restrict the data displayed, we can use a nested command to isolate specific
    rows based on a condition. A business task you could address using this data would
    be to *identify customers with high sales so we can thank them personally*. To
    do this, we can filter the sales by a specific value and display only the rows
    that meet or exceed that condition. For this example, we assigned `high` to an
    arbitrary number so any `Sales_Amount` over 100 will be displayed using this command:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要限制显示的数据，我们可以使用嵌套命令根据条件隔离特定的行。你可以使用这些数据解决的业务任务包括*识别销售量高的客户，以便我们可以亲自感谢他们*。为此，我们可以通过特定值过滤销售数据，并仅显示满足或超过该条件的行。在这个例子中，我们将`high`分配给一个任意数字，所以任何`Sales_Amount`超过100的记录都将使用此命令显示：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output would look like the following screenshot where a single record is
    displayed based on the condition because there is only one record where `Sales_Amount`
    is greater than `100`, which is `Sale_ID` equal to `4`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中根据条件显示单个记录，因为只有一个记录的`Sales_Amount`大于`100`，即`Sale_ID`等于`4`：
- en: '![](img/0d0d28d3-e4f3-4bb9-934d-ba765dd9a288.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d0d28d3-e4f3-4bb9-934d-ba765dd9a288.png)'
- en: 'Another example of how to restrict results would be looking for a specific
    value assigned to a specific field in the DataFrame. If we wanted to better understand
    this data, we could do so by looking at the `Sales_Quantity` field and seeing
    which records only had one product purchased:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 限制结果的一个例子是查找分配给 DataFrame 中特定字段的特定值。如果我们想更好地理解这些数据，我们可以通过查看`Sales_Quantity`字段来查看哪些记录只购买了一个产品：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output would look like the following screenshot, where multiple records
    are displayed based on the condition where `Sales_Quantity` is equal to `1`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中根据条件显示多个记录，其中`Sales_Quantity`等于`1`：
- en: '![](img/07b0a5ba-380a-40ac-a8a7-25996b2f9da3.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07b0a5ba-380a-40ac-a8a7-25996b2f9da3.png)'
- en: The steps define a best practice for an analysis workflow. Retrieving SQL results,
    storing them in one or more DataFrames, and then performing analysis in your notebook
    is common and encouraged. Migrating data between sources (from database to Jupyter
    Notebook) can take high compute resources depending on the volume of data, so
    be conscious of how much memory you have available and how large the databases
    you are working with are.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤定义了分析工作流程的最佳实践。检索 SQL 结果，将它们存储在一个或多个 DataFrame 中，然后在你的笔记本中进行分析是常见且被鼓励的。根据数据量，在数据库之间迁移数据可能需要大量的计算资源，所以请留意你有多少可用内存以及你正在处理的数据库有多大。
- en: Data about your data explained
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据关于数据解释
- en: Now that we have a better understanding of how to work with SQL sourced data
    using Python and pandas, let's explore some fundamental statistics along with
    practical usage for data analysis. So far, we have focused on descriptive statistics
    versus predictive statistics. However, I recommend not proceeding with any data
    science predictive analytics without a firm understanding of descriptive analytics
    first.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经更好地理解了如何使用 Python 和 pandas 处理 SQL 数据源，让我们探索一些基本统计量以及数据分析的实际应用。到目前为止，我们主要关注描述性统计与预测性统计。然而，我建议在充分理解描述性分析之前，不要进行任何数据科学的预测性分析。
- en: Fundamental statistics
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本统计量
- en: Descriptive analytics is based on what has already happened in the past by analyzing
    the digital footprint of data to gain insights, analyze trends, and identify patterns.
    Using SQL to read data from one or more tables supports this effort, which should
    include basic statistics and arithmetic. Having the data structured and conformed,
    which includes defined data types per column, makes this type of analysis easier
    once you understand some key concepts and commands.There are many statistical
    functions available in both SQL and Python.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性分析基于过去已经发生的事情，通过分析数据的数字足迹来获得洞察力，分析趋势，并识别模式。使用 SQL 从一个或多个表中读取数据支持这一努力，这应该包括基本的统计和算术。数据结构化和规范，包括每列定义的数据类型，一旦你理解了一些关键概念和命令，这种类型的分析就会变得更容易。SQL
    和 Python 中都有许多统计函数可用。
- en: 'I have summarized a few that are fundamental to your data analysis in this
    table:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在这个表中总结了几个对您的数据分析至关重要的函数：
- en: '| **Statistical Measure** | **Description** | **Best For/Use Case** | **SQL
    Syntax** | **pandas Function** |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **统计量** | **描述** | **最佳用途/用例** | **SQL 语法** | **pandas 函数** |'
- en: '| Count | The number of occurrences of a value regardless of data type | Finding
    out the size of a table/number of records | `SELECT Count(*) FROM table_name`
    | `df.count()` |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 计数 | 不论数据类型，一个值出现的次数 | 查找表的大小/记录数 | `SELECT Count(*) FROM table_name` | `df.count()`
    |'
- en: '| Count Distinct | The number of distinct occurrences of a value regardless
    of data type | Removing duplicate values/verify distinct values used for categories
    of data | `SELECT Count(distinct field_name) FROM table_name` | `df.nunique()`
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 计数（不重复） | 不论数据类型，一个值出现的不同次数 | 移除重复值/验证用于数据类别中的唯一值 | `SELECT Count(distinct
    field_name) FROM table_name` | `df.nunique()` |'
- en: '| Sum | The aggregation of values as a whole or total against numeric data
    types | Finding the total population or measuring the amount of money | `SELECT
    Sum(field_name) FROM table_name` | `df.sum()` |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 求和 | 对数值数据类型的值的整体或总计聚合 | 查找总人口或衡量金钱的数量 | `SELECT Sum(field_name) FROM table_name`
    | `df.sum()` |'
- en: '| Mean | The arithmetic average from a set of two or more numeric data types
    | Sum of values divided by the count of values | `SELECT AVG(field_name) FROM
    table_name` | `df.mean()` |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 来自两个或更多数值数据类型的算术平均值 | 值的总和除以值的计数 | `SELECT AVG(field_name) FROM table_name`
    | `df.mean()` |'
- en: '| Min | The lowest numeric value of a value in a field | Finding the lowest
    value  | `SELECT MIN(field_name) FROM table_name` | `df.min()` |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 最小值 | 字段中值的最低数值 | 查找最低值 | `SELECT MIN(field_name) FROM table_name` | `df.min()`
    |'
- en: '| Max | The highest numeric value of a value in a field | Finding the highest
    value  | `SELECT MAX(field_name) FROM table_name` | `df.max()` |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 最大值 | 字段中值的最高数值 | 查找最高值 | `SELECT MAX(field_name) FROM table_name` | `df.max()`
    |'
- en: 'The most common statistical measure I use in SQL is *Count* where you are counting
    the number of records per table. Using this function helps to validate that the
    volume of data you are working with is in line with the source system, producers
    of data, and business sponsors. For example, you are told by the business sponsor
    that they use a database to store customers, products, and sales and they have
    over 30,000 customers. Let''s say you run the following SQL query:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SQL 中，我最常用的统计度量是 *计数*，即统计每个表中记录的数量。使用此函数有助于验证你正在处理的数据量与源系统、数据生产者和业务赞助者一致。例如，业务赞助者告诉你他们使用数据库来存储客户、产品和销售，并且他们有超过
    30,000 名客户。让我们假设你运行以下 SQL 查询：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are 90,000 results. Why is there such a dramatic difference? The first
    question would be: are you using the correct table? Any database is flexible so
    it can be organized by the DBA to manage relationships based on application and
    business needs, so active customers (customers who purchased a product and created
    sales data) could be stored in a different table, such as `active_customers`.
    Another question would be: is there a field used to identify whether the record
    is active or not? If so, that field should be included in the `WHERE` section
    of your `SELECT` statement, for example, `SELECT count(*) from customers where
    active_flag = true`.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有 90,000 个结果。为什么会有如此大的差异？首先的问题可能是：你是否使用了正确的表？任何数据库都是灵活的，因此它可以由 DBA 根据应用和业务需求进行组织，因此活跃客户（购买了产品并创建了销售数据的客户）可能存储在不同的表中，例如
    `active_customers`。另一个问题是：是否有字段用于标识记录是否活跃？如果有，那么该字段应该包含在 `SELECT` 语句的 `WHERE`
    部分中，例如，`SELECT count(*) from customers where active_flag = true`。
- en: A second advantage of using the `count()` function for analysis is to set expectations
    for yourself as to how much time it takes for each query to return results. If
    you run a `count(*)` on products, customers, and sales tables, the amount of time
    taken to retrieve the results will vary depending on the volume of data and how
    the DBA has optimized the performance. Tables have shapes, which means the number
    of rows and columns will vary between them. They also can grow or shrink depending
    on their intended use. A table such as `sales` is transactional so the number
    of rows will dramatically increase over time. We can classify transaction tables
    as deep because the number of columns is minimal, but the number of rows will
    grow. Tables such as `customers` and `products` are known as reference tables,
    which are wide in shape because they could have dozens of columns with significantly
    fewer rows compared to transaction tables.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`count()`函数进行分析的第二个优点是为自己设定预期，了解每个查询返回结果所需的时间。如果你在产品、客户和销售表上运行`count(*)`，检索结果所需的时间将根据数据量以及DBA如何优化性能而变化。表格有形状，这意味着行数和列数在它们之间会有所不同。它们也可以根据其预期用途而增长或缩小。例如，`sales`表是事务性的，因此行数会随着时间的推移而显著增加。我们可以将事务表归类为深度，因为列数最少，但行数会增长。例如，`customers`和`products`表被称为参考表，它们在形状上较宽，因为它们可能有数十个列，但行数与事务表相比要少得多。
- en: Tables with high numbers of rows and columns and densely-populated distinct
    values take up more disk space and require more memory and CPU to process. If
    the `sales` table has billons of rows, counting the number of rows could take
    hours waiting for the response from `SELECT count(*) from sales` and would be
    discouraged by the administrators/IT support team. I worked with a data engineering
    team that was able to retrieve SQL results in less than 10 seconds against a 100
    billion record table. That kind of response time requires developer expertise
    and administrative access to configure the table to support a super-fast response
    time.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 行数和列数众多且具有密集独特值的表格会占用更多磁盘空间，并且处理时需要更多的内存和CPU资源。如果`sales`表有数十亿行，那么通过`SELECT count(*)
    from sales`来计算行数可能需要数小时等待响应，并且会被管理员/IT支持团队所劝阻。我曾与一个数据工程团队合作，他们能够在100亿条记录的表中在10秒内检索SQL结果。这种响应时间需要开发者的专业知识和对表进行配置以支持超快响应时间的管理权限。
- en: Another valid point when dealing with the `count()` function is knowing the
    difference between frequency versus distinct values. Depending on which table
    you are performing a counting function against, you may be just counting the number
    of occurrences, or frequency of records. For the 30,000 customers example, if
    there is a difference in the results between `count(customer_id)` and `count(distinct
    customer_id)`, we know counting the records includes duplicate customers. This
    may not be an issue depending on the analysis you are performing. If you wanted
    to know how often a customer buys any product, then `counting(customer_id)` will
    answer that question. If you wanted to know how many customers are buying each
    product, using `distinct` would provide more accurate information.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理`count()`函数时，另一个有效的观点是了解频率与独特值之间的区别。根据你针对哪个表执行计数函数，你可能只是在计算记录出现的次数，或者记录的频率。以30,000个客户为例，如果`count(customer_id)`和`count(distinct
    customer_id)`的结果之间有差异，我们知道计数记录包括了重复的客户。这取决于你进行的分析，可能不是问题。如果你想知道客户购买任何产品的频率，那么`count(customer_id)`将回答这个问题。如果你想知道有多少客户购买每种产品，使用`distinct`将提供更准确的信息。
- en: The `sum()` function, which is short for summation, is another common measure
    used for statistical analysis in descriptive analytics. One key difference between
    counting versus summing would be that sum requires a number value to calculate
    accurate results whereas counting can be done against any data type. For example,
    you cannot and should not sum the `customer_name` field in the `customers` table
    because the data type is defined as a string. You can technically sum the `customer_id`
    field if it's defined as an integer, however, that would give you misleading information
    because that is not the intended use of the field. Like `count`, `sum` is an aggregate
    measure used to add together all of the values found in a specific field such
    as `sales_amount` or quantity from a `sales` table.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`sum()`函数，简称求和，是描述性分析中用于统计分析的另一个常用度量。计数与求和之间的一个关键区别是，求和需要一个数值来计算准确的结果，而计数可以对任何数据类型进行。例如，你不能也不应该对`customers`表中的`customer_name`字段求和，因为数据类型被定义为字符串。如果你将其定义为整数，技术上可以对`customer_id`字段求和，但这会提供误导性的信息，因为这不是该字段的预期用途。像`count`一样，`sum`是一个聚合度量，用于将特定字段（如`sales_amount`或销售表中的数量）中找到的所有值相加。'
- en: 'To use the `sum()` function in SQL is easy. If you want to know the sum for
    all time with no constraints or conditions, use the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中使用`sum()`函数很简单。如果你想知道所有时间的总和而没有约束或条件，可以使用以下语法：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can then add a condition such as only active customers by including the
    `WHERE` clause with the `flag` field, which has the following syntax: `SELECT
    sum(field_name) from table_name WHERE active_flg = TRUE`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以添加一个条件，例如只包括活跃客户，通过包含带有`flag`字段的`WHERE`子句，其语法如下：`SELECT sum(field_name)
    from table_name WHERE active_flg = TRUE`。
- en: We will uncover more advanced features such as aggregation using SQL in [Chapter
    8](9bdac090-8534-480e-8154-a854115c0b7a.xhtml), *Understanding Joins, Relationships,
    and Aggregates*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第8章[理解连接、关系和聚合](9bdac090-8534-480e-8154-a854115c0b7a.xhtml)中揭示更多高级功能，例如使用SQL进行聚合。
- en: The mean or average function is another common statistical function very useful
    for data analysis, and it's easy to write the command using SQL. average is the
    sum of all values divided by the count with the syntax of `SELECT avg(field_name)
    from table_name`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 均值或平均函数是另一个非常常用的统计函数，对于数据分析非常有用，并且使用SQL编写命令很容易。平均数是所有值的总和除以计数，语法为`SELECT avg(field_name)
    from table_name`。
- en: The denominator of counting values is using the frequency/number of occurrences
    versus distinct values so you should understand how the table is populated before
    running the SQL command. For example, a sales table is transaction-based with
    many customers and products so the average would be different from the average
    against the product or customer table because each record would be distinct.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 计数值的分母是使用频率/发生次数与不同值相比，因此在运行SQL命令之前你应该理解表是如何填充的。例如，一个销售表是基于交易的，有多个客户和产品，所以平均数会与产品表或客户表的平均数不同，因为每条记录都是唯一的。
- en: 'The `min` and `max` functions are also useful and easy to interpret using SQL.
    The built-in functions are `min()` and `max()`, which return the minimum numeric
    value from a population of data along with the maximum or highest value. A good
    business question to understand from your table would be what is the lowest and
    highest sales amount for 2018? The syntax in SQL would be as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`min`和`max`函数在SQL中也非常有用且易于解释。内置函数是`min()`和`max()`，它们从数据集中返回最小数值以及最大或最高值。从你的表中理解的一个好业务问题是2018年的最低和最高销售额是多少？SQL中的语法如下：'
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This information would be useful to know to understand the range of sales per
    customer and product across all periods of time.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 了解每个客户和产品在所有时间段内的销售范围，这些信息将非常有用。
- en: An important factor to recognize when running these statistical functions against
    your data is to understand when values are blank or what is commonly known as
    null. In SQL, `NULL` represents nothing and the nonexistence of a value. In RDBMS,
    null values are a rule when a DBA defines the schema for each table. During that
    process of creating columns by defining the data type for each field, there is
    an option to allow null values. The reasons vary by use case whether to allow
    nulls during the design of a database table, but what's important to understand
    for analysis is whether they exist in your data and how they should be treated.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据运行这些统计函数时，一个重要的因素是要理解值是空的还是通常所说的空值。在SQL中，`NULL`代表无物和值的不存在。在RDBMS中，空值是DBA为每个表定义架构时的一个规则。在通过为每个字段定义数据类型创建列的过程中，有一个选项允许空值。根据用例的不同，允许在数据库表设计时使用空值的原因也各不相同，但对于分析来说，重要的是要了解它们是否存在于你的数据中以及它们应该如何被处理。
- en: Let's start with an example from our `customers` table where one of the fields
    such as the second address line allows `NULL`, which is common. Why is this common?
    Because a second address field is optional and is not even used in many cases,
    but what if you are a company that needs to physically mail marketing materials
    or invoices to customers? If the data entry always required a value, it would
    unnecessarily populate a value in that second address field in the database, which
    is inefficient because it takes more time to enter a value for each customer and
    takes more storage space. In most cases, forcing a value in large-scale enterprise
    systems creates poor data quality, which then requires time to fix the data or
    creates confusion working with the data, especially working with millions of customers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的`customers`表中的一个例子开始，其中一个字段，如第二行地址允许`NULL`，这是常见的。为什么这是常见的？因为第二行地址字段是可选的，而且在许多情况下甚至没有被使用，但如果你是一家需要物理邮寄营销材料或发票给客户的公司呢？如果数据录入总是需要值，那么在数据库中的第二行地址字段中不必要地填充值，这是低效的，因为它需要更多的时间为每个客户输入值，并且需要更多的存储空间。在大多数情况下，在大规模企业系统中强制输入值会创建较差的数据质量，这随后需要时间来修复数据，或者在处理数据时造成混淆，尤其是在处理数百万客户的情况下。
- en: Metadata explained
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 元数据解释
- en: 'Metadata is commonly known as descriptive information about the data source.
    A key concept exposed in metadata analysis is related to understanding that nulls
    exist in databases. From a data analysis perspective, we need to make sure we
    understand how it impacts our analysis. In Python and other coding languages such
    as Java, you may see the word `NaN` returned. This is an acronym for *Not a Number*
    and helps you to understand that you may not be able to perform statistical calculations
    or functions against those values. In other cases such as Python, `NaN` values
    will have special functions to handle them, such as the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据通常是指关于数据源描述性信息。在元数据分析中，一个关键概念与理解数据库中存在空值相关。从数据分析的角度来看，我们需要确保我们理解它对我们分析的影响。在Python和其他编程语言如Java中，你可能看到返回的单词`NaN`。这是“非数字”（Not
    a Number）的缩写，有助于你理解你可能无法对这些值执行统计计算或函数。在其他情况下，例如Python，`NaN`值将具有特殊函数来处理它们，如下所示：
- en: In NumPy, use the `nansum()` function
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在NumPy中，使用`nansum()`函数
- en: Use pandas with the `isnull()` function
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pandas的`isnull()`函数
- en: In SQL, use `is null` or `isnull` depending on the RDBMS you are working with
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在SQL中，根据你使用的RDBMS使用`is null`或`isnull`
- en: Since you are testing for a condition to exist, you can also include the keyword
    of `NOT` to test for the opposite, for example, `Select * from customer_table
    where customer_name is NOT null`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你正在测试一个条件是否存在，你也可以包括`NOT`关键字来测试相反的情况，例如，`Select * from customer_table where
    customer_name is NOT null`。
- en: Understanding nulls and `NaN` boils down to KYD and metadata about the source
    datasets you are working with. If you don't have access to the database system
    to see the metadata and underlining schema, we can use pandas and DataFrames to
    gain some insights about SQL data. Let's walk through an example, by loading a
    single table from the database into a DataFrame in a notebook and run some metadata
    functions to gain more information.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 理解空值和`NaN`归结为KYD以及你正在处理的数据源元数据。如果你无法访问数据库系统以查看元数据和底层架构，我们可以使用pandas和DataFrames来获取关于SQL数据的一些见解。让我们通过一个例子来操作，将单个表从数据库加载到笔记本中的DataFrame，并运行一些元数据函数以获取更多信息。
- en: 'To begin, create a new Jupyter notebook and name it `test_for_nulls_using_sql_and_pandas`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个新的Jupyter笔记本，并将其命名为`test_for_nulls_using_sql_and_pandas`：
- en: 'Similar to the prior example, to load an SQLite database connection, you just
    need to add the following command in your Jupyter notebook and run the cell:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与先前的例子类似，要加载 SQLite 数据库连接，你只需在你的 Jupyter 笔记本中添加以下命令并运行该单元格：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we need to assign a connection to a variable named `conn` and point to
    the location of the database file, which is named `customer_sales.db`. Since we
    already imported the `sqlite3` library in the prior `In[]` line, we can use this
    built-in function to communicate with the database:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将一个连接分配给名为 `conn` 的变量，并指向数据库文件的位置，该文件名为 `customer_sales.db`。由于我们在先前的
    `In[]` 行中已经导入了 `sqlite3` 库，我们可以使用这个内置函数与数据库进行通信：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Import the `pandas` library as shown in the following code:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如以下代码所示导入 `pandas` 库：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Using the `read_sql_query()` function, we assign the results to a new DataFrame
    as `df_customers` to make it easier to identify:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `read_sql_query()` 函数，我们将结果分配给一个新的 DataFrame，命名为 `df_customers`，以便更容易识别：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To view the results, we can just run the `head()` command against this DataFrame
    using this code:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看结果，我们只需运行以下代码中的 `head()` 命令来针对这个 DataFrame：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output would look like the following screenshot where the `tbl_customers` table has
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中 `tbl_customers` 表已被加载到 DataFrame 中，带有标签的标题行，索引列位于左侧，起始值为 `0`：
- en: '![](img/d95da4e6-ed39-443c-94bc-0895a5548000.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d95da4e6-ed39-443c-94bc-0895a5548000.png)'
- en: 'We can profile the DataFrame and easily identify any `NULL` values using the
    following command. The `isnull()` pandas function tests for null values across
    the entire DataFrame:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令来分析 DataFrame 并轻松地识别任何 `NULL` 值。`isnull()` pandas 函数在整个 DataFrame
    中测试空值：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output would look like the following screenshot where the DataFrame will
    return a `True` or `False` value rather than the actual value by the cell for
    each row and column:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中 DataFrame 将返回每行和每列的 `True` 或 `False` 值，而不是单元格中的实际值：
- en: '![](img/b9fa1c24-fe8c-4668-aaa6-a5f90b5b4c7e.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b9fa1c24-fe8c-4668-aaa6-a5f90b5b4c7e.png)'
- en: With a few commands, we learned how to communicate with databases and identify
    some important metadata about the data stored in tables. To continue improving
    our data literacy, let's understand how the data was populated into the database
    by understanding data lineage.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 几条命令，我们就学会了如何与数据库进行通信，并识别存储在表中的数据的一些重要元数据。为了继续提高我们的数据素养，让我们通过了解数据血缘来理解数据是如何填充到数据库中的。
- en: The importance of data lineage
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据血缘的重要性
- en: '**Data lineage** is the ability to trace back the source of a dataset to how
    it was created. It is a fun topic for me because it typically requires investigating
    the history of how systems generate data, identifying how it was processed, and
    working with the people who produce and consume the data. This process helps to
    improve your data literacy, which is the ability to read, write, analyze, and
    argue with data because you can learn how the data impacts the organization. Is
    the data critical to business functions such as generating sales or was it created
    for compliance purposes? These types of questions should be answered by learning
    more about the lineage of the data.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据血缘**是指追踪数据集来源以及其创建方式的能力。对我来说，这是一个有趣的话题，因为它通常需要调查系统生成数据的历史，识别其处理方式，并与生产和使用数据的个人合作。这个过程有助于提高你的数据素养，即阅读、编写、分析和用数据辩论的能力，因为你能够了解数据对组织的影响。数据对于业务功能，如生成销售是否至关重要，或者它是为了合规目的而创建的？这些问题应该通过更多地了解数据的血缘来回答。'
- en: From experience, this process of tracing data lineage involves working sessions
    directly with the people who are responsible for the data and uncovering any documentation
    like an ERD demonstrated in the *From SQL to pandas DataFrames* section or help
    guides. In many cases, the documentation available for enterprise systems that
    have matured over time will not reflect the nuances that you will see when analyzing
    the data. For example, if a new field was created on an existing table that is
    populated from a web form that did not exist before, historical data will have
    `NULL` or `NaN` values until the point in time when the data entry started.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从经验来看，追踪数据血缘的过程涉及到直接与负责数据的人员进行工作会话，并揭示任何如*从SQL到pandas DataFrames*部分中展示的ERD（实体关系图）或帮助指南等文档。在许多情况下，随着时间的推移而成熟的企业的系统可用的文档可能不会反映你在分析数据时看到的细微差别。例如，如果在一个之前不存在表单的现有表中创建了一个新字段，那么历史数据将会有`NULL`或`NaN`值，直到数据录入开始的时间点。
- en: Data lineage can quickly become complex, which takes time to unwind and multiple
    resources to expose the details when not properly documented. When multiple systems
    are involved, working with **Subject Matter Experts** (**SMEs**) will fast track
    the process so you don't have to reverse engineer all of the steps in the data
    flow.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据血缘可能会迅速变得复杂，如果不进行适当的记录，则需要时间来解开并需要多个资源来揭示细节。当涉及多个系统时，与**主题专家**（**SMEs**）合作将加速流程，这样你就不必逆向工程数据流中的所有步骤。
- en: Data flow
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据流
- en: '**Data flow** is a subset of data lineage that is typically part of a larger
    data governance strategy within large organizations so there may be existing tools
    or systems already in place that visually represent how the data is processed,
    which is commonly known as data flow. A hypothetical example of a data flow diagram
    would be the following diagram where we look at some of the data we have been
    working with in our exercises so far. In this diagram, we have a logical representation
    of how the `tbl_customers` table is populated from our SQLite database. I have
    documented the inputs and outputs as stages one to four:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据流**是数据血缘的一个子集，通常是大组织内部更大数据治理策略的一部分，因此可能已经存在一些工具或系统，这些工具或系统能够直观地表示数据是如何处理的，这通常被称为数据流。一个假设的数据流图示例如下所示，其中我们查看迄今为止我们在练习中处理的一些数据。在这个图中，我们有`tbl_customers`表从我们的SQLite数据库中填充的逻辑表示。我已经将输入和输出记录为第一阶段到第四阶段：'
- en: '![](img/41e59c35-0b8b-4de8-8e0a-47bbee0751b7.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/41e59c35-0b8b-4de8-8e0a-47bbee0751b7.png)'
- en: The input stage
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入阶段
- en: First, we have the **Input** stage, which is identified as the **Mobile App**,
    **Web App,** and **Client PC** systems. These systems have created feeds out into
    multiple file formats. In our example, this data is batch processed, where the
    data files are saved and sent out for the next stage.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有**输入**阶段，它被标识为**移动应用**、**Web应用**和**客户端PC**系统。这些系统已经创建了输出到多种文件格式的数据。在我们的例子中，这些数据是批量处理的，其中数据文件被保存并发送至下一阶段。
- en: The data ingestion stage
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据摄入阶段
- en: The **Data Ingestion** stage is where multiple files such as `customers.json` and
    `customers.xml` are processed. Because this is a logical diagram rather than a
    highly technical one, the details behind what technologies are used to process
    the data ingestion are omitted. Data ingestion is also known as **ETL**, which
    is an acronym for **Extract, Transform, and Load**, which is automated and maintained
    by data engineering teams or developers.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据摄入**阶段是处理多个文件，如`customers.json`和`customers.xml`的地方。因为这个图是逻辑图而不是高度技术性的图，所以省略了处理数据摄入背后使用的技术细节。数据摄入也被称为**ETL**，即**提取、转换和加载**的缩写，这是由数据工程团队或开发者自动化和维护的。'
- en: We can see an intermediary step called `tbl_stage_customers` during this ETL,
    which is a landing table for processing the data between the source files and
    the target table in the database. Also included in this stage is an `ODBC` connection
    where the **Client PC** system has direct access to insert, update, and delete
    records from the `tbl_customers` table.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个ETL过程中，我们可以看到一个中间步骤称为`tbl_stage_customers`，这是一个在源文件和数据库中的目标表之间处理数据的中间表。此阶段还包括一个`ODBC`连接，其中**客户端PC**系统可以直接访问`tbl_customers`表以插入、更新和删除记录。
- en: During the process of learning more about the data flow, be sure to ask whether
    the tables are defined with logical delete versus the physical deleting of rows.
    In most cases, the direct removal of rows in a table is not supported, so Boolean
    data type columns are used to indicate whether the record is active or flagged
    for deletion by the system or user.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习更多关于数据流的过程中，务必询问表是否使用逻辑删除而不是物理删除行。在大多数情况下，不支持直接从表中删除行，因此使用布尔数据类型列来指示记录是否由系统或用户激活或标记为删除。
- en: The data source stage
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据源阶段
- en: 'The third stage is named **Data Source**, which is defined as the `tbl_customers` table.
    Some questions to ask the developer or DBA are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 第三阶段被称为**数据源**，定义为`tbl_customers`表。向开发人员或数据库管理员提出的一些问题如下：
- en: What is your retention policy for this data/how long is the data preserved?
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你对这个数据的保留策略是什么/数据保留多长时间？
- en: What is the average daily volume of records being populated in this table?
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个表中每天平均填充了多少条记录？
- en: Can they provide some metadata such as how many rows, columns, and data types
    for each field?
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们能否提供一些元数据，例如每个字段的行数、列数和数据类型？
- en: What are the dependencies/joins to this table including primary and foreign
    keys?
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个表包括主键和外键在内的依赖/连接是什么？
- en: How often is this table backed up and is there system downtime we should be
    aware of that would impact analysis?
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个表多久备份一次，我们是否应该注意可能影响分析的系统停机时间？
- en: Does a data dictionary exist for this table/database?
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个表/数据库是否存在数据字典？
- en: The data target stage
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据目标阶段
- en: The fourth stage, named **Data Target**, helps a data analyst to understand
    downstream dependencies from the source table. In this example, we have a **Sales
    Report**, the `compliance_feed.json` file, and **Jupyter Notebook**. Some useful
    information to uncover would be the frequency of how often that compliance feed
    is sent and who the consumers of that data are.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第四阶段被称为**数据目标**，它帮助数据分析师理解从源表的下游依赖关系。在这个例子中，我们有一个**销售报告**、`compliance_feed.json`文件和**Jupyter
    Notebook**。需要揭示的有用信息包括合规数据发送的频率以及数据消费者是谁。
- en: This may become important if the timing of your analysis is not in line with
    data feeds from the **Data Source** stage. Trust in your analysis and the ability
    to argue that your analysis is complete and accurate comes from understanding
    timing issues and your ability to reconcile and match counts between multiple
    data-target outputs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的分析时间与**数据源**阶段的 数据馈送时间不一致，这可能变得很重要。对分析结果的信任以及能够论证你的分析是完整和准确的，来自于理解时间问题以及你解决和匹配多个数据目标输出之间计数的能力。
- en: Business rules
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 业务规则
- en: Another important point about data lineage is to uncover business rules, lookup
    values, or mapping reference sources. A business rule is an abstract concept that
    helps you to understand software code that is applied during data processing.
    An example would be when the user of the **Mobile App** clicks a **Submit** button,
    a new `customers.json` file is created. Business rules can also be more complex,
    such as `tbl_stage_customers` table does not populate records in the `tbl_customers` until
    all source files are received and a batch process runs at 12 A.M. EST daily. Business
    rules may be explicitly defined in the database during the creation of a database
    table such as the rule to define a primary key on a column, coded on a web form
    or mobile application.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 数据血缘的另一个重要点是揭示业务规则、查找值或映射参考源。业务规则是一个抽象概念，有助于你理解在数据处理过程中应用到的软件代码。例如，当**移动应用**的用户点击**提交**按钮时，会创建一个新的`customers.json`文件。业务规则也可以更复杂，例如`tbl_stage_customers`表不会在`tbl_customers`中填充记录，直到所有源文件接收完毕并且每天凌晨12点（东部标准时间）运行批处理。业务规则可以在创建数据库表时明确定义，例如在列上定义主键的规则，在网页表单或移动应用程序中编码。
- en: Documenting these business rules should be included in your methodology to support
    your analysis. This helps you to argue insights from your data analysis by either
    verifying the existence of the business rule or identifying outliers that contradict
    assumptions made about the source data. For example, if you were told a database
    table was created to not allow `NULL` in specific fields but you end up finding
    it, you can review your findings with the DBA to uncover how this occurred. It
    could have easily been a business exception that was created or that the enforcement
    of the business rule was implemented after the table was already populated.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 应该将记录这些业务规则包括在你的方法论中，以支持你的分析。这有助于你通过验证业务规则的存在或识别与关于源数据的假设相矛盾的反常值来从数据分析中得出见解。例如，如果你被告知一个数据库表被创建为不允许特定字段中的`NULL`，但你最终发现它存在，你可以与数据库管理员（DBA）一起审查你的发现，以揭示这种情况是如何发生的。这可能是创建了一个业务异常，或者是在表已经填充之后才实施了业务规则的执行。
- en: Understanding business rules helps to identify data gaps and verifies accuracy
    during analysis. If the average daily volume of records for this table drops to
    zero records for multiple consecutive days, there might be an issue in stage 2
    during the **Data Ingestion** or it just might be a holiday where no customer
    records were received and processed.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 理解业务规则有助于识别数据缺口并在分析过程中验证准确性。如果这个表的平均每日记录量连续多日降至零记录，那么可能是在**数据摄取**阶段的第2阶段出现了问题，或者可能只是假日，没有收到和处理客户记录。
- en: In either case, learning how to ask these questions of the subject matter experts
    and verifying the data lineage will build confidence in your analysis and trust
    with both producers and consumers of the data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，学习如何向主题专家提出这些问题并验证数据血缘将增强你对分析的信心，并赢得数据生产者和消费者双方的信任。
- en: 'Now that you understand all of the concepts, let''s walk through the data lineage
    of the data we are working with in this chapter—`customer_sales.db`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了所有这些概念，让我们回顾一下本章中我们正在处理的数据血缘——`customer_sales.db`：
- en: In the **Input** stage for this database, three source CSV files were manually
    created for example purposes. Each source table has a one-for-one match with a
    CSV file named `tbl_customers`, `tbl_products`, and `tbl_sales`.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个数据库的**输入**阶段，为了示例目的，手动创建了三个源CSV文件。每个源表都与一个名为`tbl_customers`、`tbl_products`和`tbl_sales`的CSV文件一一对应。
- en: In the **Data Ingestion** stage, each file was imported using a few SQL commands,
    which created the schema for each table (the field names, defined data types,
    and join relationships). This process is commonly known as an ETL where the source
    data is ingested and persisted as tables in the database. If any changes between
    the source files and the target database table are required, a business rule should
    be documented to help to provide transparency between the producers and consumers
    of the data. For this example, the source and target match.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**数据摄取**阶段，每个文件都是通过几个SQL命令导入的，这为每个表创建了模式（字段名称、定义的数据类型和连接关系）。这个过程通常被称为ETL，其中源数据被摄取并持久化为数据库中的表。如果需要修改源文件和目标数据库表之间的任何变化，应该记录业务规则以帮助提供数据生产者和消费者之间的透明度。对于这个例子，源数据和目标数据是一致的。
- en: The **Data Source** stage in this example would be `customer_sales.db`. This
    now becomes the golden copy for data flowing out of the database for analysis
    and any reporting.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，**数据源**阶段将是`customer_sales.db`。现在，这成为从数据库流向分析以及任何报告的数据的黄金副本。
- en: The **Target** stage in our example would be the Jupyter notebook and the creation
    of DataFrames for analysis.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的例子中，**目标**阶段将是Jupyter笔记本和为分析创建DataFrame。
- en: While this is a small example with only a few steps, the concepts apply to large-scale
    enterprise solutions with many more data sources and technologies used to automate
    the data flow. I commonly sketch out the stages for data lineage before doing
    any data analysis to ensure I understand the complete process. This helps to communicate
    with stakeholders and SMEs to ensure accuracy in the insights you gain from data
    sources.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个只有几个步骤的小例子，但这些概念适用于具有更多数据源和用于自动化数据流的技术的大规模企业解决方案。我通常在开始任何数据分析之前绘制数据血缘的阶段，以确保我理解整个流程。这有助于与利益相关者和SMEs沟通，以确保从数据源中获得见解的准确性。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We have covered a few key topics in this chapter to help you to improve your
    data literacy by learning about working with databases and using SQL. We learned
    about the history of SQL and the people who created the foundation for storing
    structured data in databases. We walked through some examples and how to insert
    records from a SQL `SELECT` statement into a `pandas` DataFrame for analysis.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了一些关键主题，帮助你通过学习与数据库交互和使用 SQL 来提高数据素养。我们了解了 SQL 的历史以及为在数据库中存储结构化数据奠定基础的人。我们通过一些示例介绍了如何将
    SQL `SELECT` 语句中的记录插入到 `pandas` DataFrame 中进行分析。
- en: By using the `pandas` library, we learned about how to sort, limit, and restrict
    data along with fundamental statistical functions such as counting, summing, and
    average. We covered how to identify and work with `NaN` (that is, nulls) in datasets
    along with the importance of data lineage during analysis.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `pandas` 库，我们学习了如何排序、限制和限制数据，以及基本的统计函数，如计数、求和和平均值。我们介绍了如何在数据集中识别和处理 `NaN`（即空值），以及分析过程中数据来源的重要性。
- en: In our next chapter, we will explore time series data and learn how to visualize
    your data using additional Python libraries to help to improve your data literacy
    skills.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们接下来的章节中，我们将探讨时间序列数据，并学习如何使用额外的 Python 库来可视化你的数据，以帮助提高你的数据素养技能。
- en: Further reading
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Here are some links that you can refer to for more information on the relative
    topics of this chapter:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些链接，你可以参考这些链接以获取本章相关主题的更多信息：
- en: Historical details about how SQL was created: [http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt ](http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 SQL 创建的历史细节：[http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt](http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt)
- en: Handling `NULL` values: [https://codeburst.io/understanding-null-undefined-and-nan-b603cb74b44c](https://codeburst.io/understanding-null-undefined-and-nan-b603cb74b44c)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理 `NULL` 值：[https://codeburst.io/understanding-null-undefined-and-nan-b603cb74b44c](https://codeburst.io/understanding-null-undefined-and-nan-b603cb74b44c)
- en: Handling duplicate values with pandas: [https://www.python-course.eu/dealing_with_NaN_in_python.php](https://www.python-course.eu/dealing_with_NaN_in_python.php)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pandas 处理重复值：[https://www.python-course.eu/dealing_with_NaN_in_python.php](https://www.python-course.eu/dealing_with_NaN_in_python.php)
- en: About SQLite databases: [https://www.sqlite.org/about.html](https://www.sqlite.org/about.html)
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 SQLite 数据库：[https://www.sqlite.org/about.html](https://www.sqlite.org/about.html)
- en: Data modeling techniques: [https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据建模技术：[https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/)
- en: pandas DataFrame functions: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas DataFrame 函数：[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)
