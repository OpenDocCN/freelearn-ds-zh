- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: End-to-End View of a Time Series Analysis Project
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列分析项目的端到端视角
- en: Building on the foundation set in the previous chapters in which we were introduced
    to time series analysis, its multiple use cases, and Apache Spark, a key tool
    for such analysis, this chapter guides us through the entire process of a time
    series analysis project. Starting with use cases, we will move on to the end-to-end
    approach with DataOps, ModelOps, and DevOps. We will cover key stages such as
    data processing, feature engineering, model selection, and evaluation, offering
    practical insights into building a time series analysis pipeline with Spark and
    other tools.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们介绍了时间序列分析及其多个应用场景，以及 Apache Spark——作为这种分析的关键工具——的基础，本章将引导我们完成整个时间序列分析项目的过程。从用例出发，我们将过渡到涵盖
    DataOps、ModelOps 和 DevOps 的端到端方法。我们将涵盖关键阶段，如数据处理、特征工程、模型选择和评估，并提供关于如何使用 Spark
    和其他工具构建时间序列分析管道的实用见解。
- en: This holistic view of a time series analysis project will equip us with a structured
    approach to handling real-world projects, enhancing our ability to implement end-to-end
    solutions. The information here will guide us as practitioners through a framework
    for using Spark in a cohesive manner and ensuring the successful execution of
    time series analysis projects. We will conclude with two approaches for implementation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这种全面的时间序列分析项目视角将为我们提供一种结构化的方法，帮助我们处理现实世界的项目，增强我们实施端到端解决方案的能力。这里的信息将为我们提供一个框架，帮助我们以一致的方式使用
    Spark，并确保时间序列分析项目的成功执行。我们将以两种实施方法作为结尾。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Driven by use cases
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由用例驱动
- en: From DataOps to ModelOps to DevOps
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 DataOps 到 ModelOps 再到 DevOps
- en: Implementation examples and tools
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施示例与工具
- en: Let’s get started!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The hands-on part of this chapter will be to implement end-to-end examples
    for a time series analysis project. The code for this chapter can be found in
    the `ch4` folder of the GitHub repository at the following link:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的实操部分将是实现时间序列分析项目的端到端示例。该章节的代码可以在 GitHub 仓库中的 `ch4` 文件夹找到，链接如下：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4)'
- en: The hands-on section of this chapter (*Implementation examples and tools*) will
    go into further detail. This requires some skills in building an open source environment.
    If you do not intend to build your own Apache Spark environment and your focus
    is instead on time series and using but not deploying Spark and other tools, you
    can skip the hands-on section of this chapter. You can use a managed platform
    such as Databricks, which comes pre-built with Spark, MLflow, and tools for workflows
    and notebooks, as we will do in future chapters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的实操部分（*实施示例与工具*）将进一步详细说明。这需要一些构建开源环境的技能。如果你不打算构建自己的 Apache Spark 环境，且关注点仅在于时间序列分析并使用
    Spark 和其他工具，而不是部署它们，那么你可以跳过本章的实操部分。你可以使用像 Databricks 这样的托管平台，它预装了 Spark、MLflow
    以及用于工作流和笔记本的工具，正如我们在未来章节中所做的那样。
- en: Driven by use cases
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 由用例驱动
- en: 'Before we get into the *how* of doing an end-to-end time series analysis project,
    as always, it is good to start with the *why*. There can be many reasons, often
    a combination, to justify the inception of a time series analysis project. Some
    of the reasons are as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论如何进行端到端时间序列分析项目之前，像往常一样，先从*为什么*开始总是一个好的选择。可能有许多原因，通常是多种原因的组合，来证明启动时间序列分析项目的必要性。以下是一些原因：
- en: '**Technology refresh**: The emphasis can be on the technology due to an aging
    platform needing replacement and not being able to meet requirements anymore,
    or when a new technology is available, offering better performance, lower costs,
    or more capabilities, such as advanced machine learning models or scalable cloud-based
    resources.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术更新**：强调技术的原因可能是由于老化的平台需要替换，且无法再满足需求，或当有新技术出现，提供更好的性能、更低的成本或更多的功能，如高级机器学习模型或可扩展的云资源。'
- en: '**Research on methods**: For organizations or departments focused on research,
    the main driver is finding new and better methods such as developing and testing
    new algorithms for analyzing time series.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法研究**：对于专注于研究的组织或部门，主要驱动力是寻找新的、更好的方法，比如开发和测试用于分析时间序列的新算法。'
- en: '**Data exploration**: Similar to research in its nature, but requiring to be
    nearer to the data, this is usually embedded within businesses’ data teams. The
    need here is to understand time series data without necessarily a predefined end
    application. The objective is to uncover patterns, trends, and anomalies in the
    data.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据探索**：与研究类似，但需要更接近数据，这通常嵌入在企业的数据团队中。这里的需求是理解时间序列数据，而不一定需要预先定义的应用目标。其目的是发现数据中的模式、趋势和异常。'
- en: '**Use case**: With this approach, we begin with the end in mind, first identifying
    specific needs and expectations of the end users or stakeholders. We then set
    the project to answer those needs based on the analysis of time series data.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用例**：在这种方法中，我们从结果出发，首先识别最终用户或相关方的具体需求和期望。然后，我们基于时间序列数据分析来设置项目，以回答这些需求。'
- en: While all the preceding reasons have their merit and are certainly valid, over
    the years, I have seen the business-driven use case approach as the one with the
    highest return on investment. We started the discussion on time series-based use
    cases across various industries in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044),
    such as inventory forecasting, predicting energy usage, financial market trend
    analysis, or anomaly detection in sensor data; here, we will focus on this use
    case-driven approach and take it further.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前提到的所有理由都有其合理性，并且无疑是有效的，但多年来，我发现以业务驱动的用例方法在投资回报方面是最高的。我们在[*第2章*](B18568_02.xhtml#_idTextAnchor044)中已经开始讨论基于时间序列的用例，涵盖了各个行业的应用场景，如库存预测、能源使用预测、金融市场趋势分析或传感器数据中的异常检测；在这里，我们将重点关注这种用例驱动的方法，并进一步探讨。
- en: The use case approach first identifies and defines real-world specific business
    applications or challenges. It then chooses the technical solution best fit to
    address these requirements. At first glance, this does not sound very different
    from any project in a business setup. The key difference here is highlighted by
    the word “specific” in that the use case approach is about a specific, measurable
    business outcome. This follows a lean approach in the sense that we want to avoid
    features that do not contribute to the business outcome.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 用例方法首先识别并定义现实世界中具体的业务应用或挑战。接着，选择最适合解决这些需求的技术方案。乍一看，这与任何商业环境中的项目似乎并无太大不同。这里的关键区别在于“具体”一词，用例方法强调的是具体、可衡量的业务成果。这遵循精益方法，因为我们希望避免那些无法为业务成果做出贡献的功能。
- en: Use cases can be compared to user stories in the agile approach to software
    development. As a matter of fact, the agile methodology is often how the use cases
    are implemented, with a streamlined iterative development process involving users
    all the way.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 用例可以与敏捷软件开发方法中的用户故事进行对比。事实上，敏捷方法通常是实现用例的方式，通过简化的迭代开发过程，始终涉及用户的参与。
- en: The following *Figure 4**.1* gives an overview of the use case driven approach,
    based on what has been discussed so far, including their key characteristics.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下*图 4.1*概述了基于迄今为止讨论内容的用例驱动方法，包括它们的关键特征。
- en: '![](img/B18568_04_1.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_1.jpg)'
- en: 'Figure 4.1: Use case driven approach'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：用例驱动方法
- en: 'Now that we have defined the use case-driven approach, we will look at the
    key characteristics of this approach, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了用例驱动的方法，接下来我们将介绍这一方法的关键特征，如下所示：
- en: '**Business outcome**: Project success is measured by the outcome in terms of
    business metrics on higher revenue, cost reduction, efficiency gain, and better
    and quicker decision-making.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务成果**：项目的成功通过业务指标来衡量，具体包括提高收入、降低成本、提升效率以及更好、更快速的决策制定。'
- en: '**User-focused**: Working from the start with the end users and stakeholders
    to identify their specific needs, the project’s objectives include answering those
    needs, in addition to the preceding business outcomes.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以用户为中心**：从一开始就与最终用户和相关方合作，明确他们的具体需求，项目目标除了回答这些需求外，还包括前述的业务成果。'
- en: '**Specific**: We’ve discussed this term a couple of times. The specificity
    of a project provides a focused direction to its scope, making its execution more
    agile. We want to address a specific need, for example, sales forecasting, and
    this can be even more granular, such as forecasting for a specific line of product
    or region.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具体**：我们已经讨论过这个词几次。项目的具体性为其范围提供了明确的方向，使得执行更加灵活。我们希望解决一个具体的需求，例如销售预测，这个需求甚至可以更为细化，比如为特定产品线或区域进行预测。'
- en: '**Iterative**: Feedback and refinement loops involving end users and stakeholders
    ensure the project remains on track to meet the expected business outcomes. This
    again highlights the similarity to an agile approach with its short development
    cycles, incremental delivery, continuous feedback, and adaptability.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代性**：涉及最终用户和利益相关者的反馈和改进循环确保项目保持正轨，满足预期的商业成果。这再次强调了与敏捷方法的相似性，其短开发周期、增量交付、持续反馈和适应性。'
- en: 'Adhering to these characteristics, the use cases are scoped small enough to
    be achievable and bring value within a few months, if not even weeks. These smaller
    use cases usually mean that there are several of them competing in parallel for
    development resources. This requires prioritization to ensure that resources are
    well invested. The following criteria are commonly used to prioritize use cases:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循这些特征，确保用例范围足够小，能够在几个月内（如果不是几周的话）实现并带来价值。这些较小的用例通常意味着它们在开发资源上并行竞争。这就需要优先排序，以确保资源得到合理投资。以下标准通常用于优先排序用例：
- en: '**Impact**: This is a measurement of the expected business impact of the use
    case, preferably calculated in monetary value. If the outcome is a reduction in
    time, the equivalent monetary value of the time saving is estimated.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**影响力**：这是用例预期商业影响的衡量标准，最好用货币价值来计算。如果结果是时间的减少，则需估算时间节省的等值货币价值。'
- en: '**Cost**: We need to account for all costs related to the use case from the
    moment of use case ideation to the time the use case is live in production and
    bringing value to the business. Costs can be related to development, infrastructure,
    migration, training, support, and production operations.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：我们需要计算与用例相关的所有成本，从用例构想到用例上线并为业务带来价值的整个过程。成本可能与开发、基础设施、迁移、培训、支持和生产运营相关。'
- en: '**Return on investment** (**ROI**): This can be simply estimated by dividing
    the impact by the cost. As an example, for a retailer who wants to better forecast
    stocks in stores, if the total cost to get the stock forecasting use case in production
    is $50k, and the improvement in stock forecasting is estimated to bring $200k
    in savings over 3 years, the ROI is 4x over this period.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投资回报率**（**ROI**）：这可以通过将影响力除以成本来简单估算。例如，对于一家希望更好地预测店铺库存的零售商，如果将库存预测用例投入生产的总成本为50万美元，而库存预测改进预计将带来三年200万美元的节省，则在此期间ROI为4倍。'
- en: '**Technical feasibility**: The technical solution for the use case exists and
    can be achieved within time and budget.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术可行性**：用例的技术解决方案存在，并且能够在时间和预算内实现。'
- en: '**Data availability and accessibility**: Data is available and accessible to
    build the use case and then operationalize it.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可用性和可访问性**：数据可用且可访问，用以构建用例并将其投入运营。'
- en: Using the preceding criteria, in the case of competing need for resources, a
    use case with a high impact and an ROI of 10x that is feasible and has data available
    is done before another use case with a lower impact and an ROI of 3x, or before
    one that does not have data access.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前述标准，在资源竞争的情况下，具有高影响力、投资回报率（ROI）为10倍、可行且已有数据的用例，应优先于另一个影响较小、ROI为3倍或没有数据访问的用例。
- en: In summary, starting with a clear understanding of the user’s needs, a use case-based
    project ensures applicability and relevance to the business, closely aligning
    with the stakeholders’ objectives, with measurable impact. Having a good use case
    is only the start, though. We will now deep-dive into the next steps, from the
    use case to the successful completion of a time series analysis project that delivers
    business outcomes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，从清晰理解用户需求开始，基于用例的项目确保了与业务的适用性和相关性，紧密对接利益相关者的目标，并能量化影响。然而，拥有一个好的用例仅仅是开始。接下来，我们将深入探讨从用例到成功完成时间序列分析项目、并实现商业成果的下一步。
- en: From DataOps to ModelOps to DevOps
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从DataOps到ModelOps再到DevOps
- en: Once a significant use case has been identified, a few phases play a crucial
    role, starting from **data operations** (**DataOps**) to **model operations**
    (**ModelOps**) and finally to **deployment** (**DevOps**) to a live business environment
    delivering value. A solid end-to-end process covering these phases ensures that
    we can consistently deliver from one use case to the next while ensuring that
    the results are reproducible. *Figure 4**.2* gives an overview of these phases,
    which will be detailed in this section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了一个重要的用例，一些阶段发挥着至关重要的作用，从**数据操作**（**DataOps**）到**模型操作**（**ModelOps**），最后到**部署**（**DevOps**），将业务环境中的价值传递到实际应用中。一个覆盖这些阶段的完整端到端过程确保了我们可以持续地从一个用例交付到下一个，同时确保结果的可重复性。*图4.2*概述了这些阶段，接下来将在本节中详细介绍。
- en: '![](img/B18568_04_2.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_2.jpg)'
- en: 'Figure 4.2: DataOps, ModelOps, and DevOps'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：DataOps、ModelOps和DevOps
- en: DataOps
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataOps
- en: DataOps for a time series analysis project encompasses best practices and processes
    to ensure the flow, quality, and access to time series data as part of its life
    cycle. The aim is for timely, efficient, and accurate time series analysis and
    modeling to derive actionable business insights.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: DataOps在时间序列分析项目中的应用包括最佳实践和过程，确保时间序列数据在生命周期中的流动、质量和访问。其目标是及时、高效且准确地进行时间序列分析和建模，从而得出可操作的业务洞察。
- en: DataOps practices follow the complete data and metadata life cycle and can be
    broken down broadly into data source integration, data processing, data governance,
    and data sharing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: DataOps实践涵盖完整的数据和元数据生命周期，通常可以分为数据源集成、数据处理、数据治理和数据共享。
- en: Source integration
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 来源集成
- en: Data source integration involves first identifying the data source and gaining
    access, and then ingesting data from the source.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源集成首先涉及识别数据源并获取访问权限，然后从源中摄取数据。
- en: '**Data sources** can be internal or external. Internal sources are primarily
    databases such as transactional records, system logs, or sensor data for telemetry.
    External sources include examples such as market data, weather data, or social
    media, which is now becoming predominant. Sources vary greatly across industries
    in volume, frequency of updates, and data format. Once the sources have been identified
    and accessed, **data ingestion** is the process of bringing the data into the
    platform for processing. This is usually achieved with automated ingestion pipelines,
    running in batches at specific frequencies (hourly, daily, etc.) or streaming
    continuously. Mechanisms for ingestion include database connections, API calls,
    or web-based scraping, among others.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据来源**可以是内部的或外部的。内部数据来源主要是数据库，例如交易记录、系统日志或用于遥测的传感器数据。外部数据来源包括市场数据、天气数据或社交媒体数据，而后者现在正成为主流。不同领域的数据来源在数据量、更新频率和数据格式上差异巨大。一旦确定并访问了数据来源，**数据摄取**就是将数据引入平台进行处理的过程。通常通过自动化的数据摄取管道来实现，按照特定的频率（每小时、每天等）批量运行或持续流式传输。数据摄取机制包括数据库连接、API调用或基于Web的抓取等。'
- en: Processing and storage
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理与存储
- en: Data processing includes cleaning up the data, transforming it into the right
    format, and storing it for analysis. A recommended approach is the medallion approach,
    as illustrated in *Figure 4**.3*, which involves multiple stages of processing
    from raw data to curated to report-ready data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理包括清理数据、将其转化为正确的格式，并为分析存储。推荐的方法是奖牌方法，如*图4.3*所示，包含多个处理阶段，从原始数据到精加工数据，再到准备报告的数据。
- en: '![](img/B18568_04_3.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_3.jpg)'
- en: 'Figure 4.3: Medallion stages of processing'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：数据处理的奖牌阶段
- en: Medallion approach
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 奖牌方法
- en: 'The medallion approach to data processing organizes data into three stages:
    Bronze, Silver, and Gold. This is often used in data lakes and Delta Lake architecture.
    Raw data is ingested from various sources without transformation in the Bronze
    stage. The Silver stage results from data cleaning, enrichment, and transformation
    to create a curated dataset. Finally, the Gold stage represents the highest quality
    data, cleansed, aggregated, and read-optimized for advanced analytics, reporting,
    and business intelligence. This multi-tiered structure augments data quality and
    facilitates data management.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理的奖牌方法将数据组织为三个阶段：青铜、白银和黄金。这通常用于数据湖和Delta Lake架构。在青铜阶段，原始数据从不同来源摄取而不进行转化。白银阶段是通过数据清理、增强和转化来创建一个精加工的数据集。最后，黄金阶段代表最高质量的数据，已清洗、汇总并优化以进行高级分析、报告和商业智能。这种多层结构提高了数据质量，并便于数据管理。
- en: Once the data has been ingested from sources, **data quality checks and cleaning**
    are the first steps to building trustworthiness in the data. These involve the
    handling of missing values, detecting and correcting errors, removing duplicates,
    and filtering outliers. These improve the data quality and give confidence that
    the analysis is built on a solid foundation. The specific requirement at this
    stage for time series data is to verify and maintain temporal integrity due to
    the sequential nature of the data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据从源端摄取，**数据质量检查与清洗**是构建数据可信度的第一步。这包括处理缺失值、检测并修正错误、去除重复项以及筛选异常值。这些操作能提升数据质量，并为分析提供坚实的基础。对于时间序列数据，特定要求是在此阶段验证并维护时间一致性，以确保数据的顺序性。
- en: The raw data from sources is usually not apt for direct analysis use and requires
    several **transformations** to be appropriate for time series analysis. This involves,
    among other transformations, changing semi-structured data to a structured format
    for quicker access. Data at granular or irregular intervals is aggregated to a
    higher-level interval such as from every minute to hourly, from hourly to daily,
    and so on. Date and time fields may require special processing to be in a sortable
    format and used to set a time index for faster retrieval. Different time zones
    need to be handled accordingly.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 来源的原始数据通常不适合直接用于分析，需要经过多次**转换**，使其适合时间序列分析。此过程包括将半结构化数据转换为结构化格式以便于更快速的访问等多项转换操作。粒度较小或间隔不规则的数据需要聚合到更高层级的时间间隔，例如将每分钟的数据聚合为每小时数据、每小时数据聚合为每日数据，依此类推。日期和时间字段可能需要特别处理，以确保其为可排序格式，并用于设置时间索引，以便更快速地检索。不同的时区需要相应处理。
- en: Frequently disregarded in smaller projects, **metadata** is an essential requirement
    in an enterprise environment to enable data traceability and lineage for governance.
    This data about the data captures, for example, source identifier, ingestion and
    update times, changes made, as well as historical versions. Metadata is captured
    as part of the data ingestion and transformation pipelines, and natively with
    storage protocols such as Delta.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在较小的项目中常被忽视，**元数据**在企业环境中是确保数据可追溯性和数据血缘治理的重要要求。例如，这些关于数据的信息包括源标识符、数据摄取和更新的时间、所做的更改以及历史版本。元数据作为数据摄取和转化管道的一部分进行捕获，并且原生支持诸如Delta这样的存储协议。
- en: While all the data processing described so far can be done in memory, there
    is a requirement for longer-term **storage** and retrieval for analysis over time.
    This storage needs to be cost-effective, scalable, and secure while providing
    the high performance required for timely analysis. Based on the volume and velocity
    of data, options include dedicated time series databases such as InfluxDB, or
    cloud-based storage in combination with a storage protocol such as Delta.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管迄今为止描述的所有数据处理可以在内存中完成，但对数据的长期**存储**和检索仍有需求，以便进行时间跨度较长的分析。此存储需要具有成本效益、可扩展性、安全性，并且能够提供进行及时分析所需的高性能。根据数据的体积和流动速度，可选择的存储方式包括专门的时间序列数据库如InfluxDB，或结合使用Delta等存储协议的云端存储。
- en: We will delve deeper into data processing in [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103)
    on data preparation. For now, let’s shift our focus to governance and security,
    which are among the most critical considerations for DataOps from a risk perspective.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第5章*](B18568_05.xhtml#_idTextAnchor103)中更深入探讨数据处理，特别是数据准备的内容。现在，我们将焦点转向治理和安全，这些是从风险角度来看，DataOps中最关键的考虑因素之一。
- en: Monitoring, security, and governance
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控、安全性与治理
- en: 'Data monitoring, security, and governance encompass several overlapping data
    practice areas, including data quality, privacy, access control, compliance, and
    policies. To appreciate the importance of these practices, let’s consider the
    following in the news at the time of this writing:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据监控、安全性和治理涵盖了多个交叉的数据实践领域，包括数据质量、隐私、访问控制、合规性以及政策。为了理解这些实践的重要性，让我们来看看在撰写本文时新闻中的以下内容：
- en: A cybersecurity breach just impacted major organizations, including Ticketmaster,
    Banco Santander, and Ticketek. A hacking group called ShinyHunters gained access
    to Ticketmaster’s database, and in doing so, compromised the personal information
    of 560 million users. This includes names, addresses, phone numbers, email addresses,
    and payment details. Reports are that this data is being sold on hacking forums
    for substantial amounts. Banco Santander had a similar breach, affecting customers
    and employees.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一起网络安全事件影响了多个主要组织，包括Ticketmaster、Banco Santander和Ticketek。一个名为ShinyHunters的黑客组织获取了Ticketmaster的数据库，并因此泄露了5.6亿用户的个人信息。泄露的内容包括姓名、地址、电话号码、电子邮件地址和支付详情。据报道，这些数据正在黑客论坛上以高价出售。Banco
    Santander也发生了类似的泄露，影响了客户和员工。
- en: '[Source: [https://www.wired.com/story/snowflake-breach-ticketmaster-santander-ticketek-hacked/](https://www.wired.com/story/snowflake-breach-ticketmaster-santander-ticketek-hacked/)]'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[来源：[https://www.wired.com/story/snowflake-breach-ticketmaster-santander-ticketek-hacked/](https://www.wired.com/story/snowflake-breach-ticketmaster-santander-ticketek-hacked/)]'
- en: These breaches, linked to a third-party cloud data warehouse service, highlight
    the challenges in cybersecurity and the need for strong measures for monitoring,
    security, and governance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些与第三方云数据仓库服务相关的数据泄露突显了网络安全的挑战，以及对监控、安全和治理强有力措施的需求。
- en: Monitoring
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 监控
- en: The goal here is to promptly identify issues and be able to take corrective
    measures, ideally before it has a negative consequence. The monitoring is of the
    data itself, of the execution status of transformation pipelines, as well as for
    security and governance breaches. For data monitoring, this means tracking the
    data quality by measuring its accuracy and completeness while catching data gaps
    and anomalies. One way to achieve this is by comparing against a range or specific
    time series pattern, as we have seen in the anomaly detection example in [*Chapter
    2*](B18568_02.xhtml#_idTextAnchor044). As for the data pipeline monitoring, these
    are tracked for performance to ensure data freshness, **service-level agreements**
    (**SLAs**) are honored, and lineage to track provenance and integrity. From a
    security point of view, we want to catch any attempt at a data breach in time
    to act upon it. The monitoring should be an automated process, with alerting in
    place.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是及时识别问题，并能够采取纠正措施，理想情况下在其产生负面后果之前。监控内容包括数据本身、转换管道的执行状态，以及安全和治理漏洞。对于数据监控，这意味着通过衡量数据的准确性和完整性来跟踪数据质量，同时捕捉数据缺口和异常。实现这一目标的一种方法是与一系列特定的时间序列模式进行比较，正如我们在[*第二章*](B18568_02.xhtml#_idTextAnchor044)的异常检测示例中所看到的。至于数据管道监控，主要跟踪其性能，以确保数据的新鲜度、**服务级别协议**（**SLA**）得到遵守，以及通过数据溯源和完整性跟踪其血统。从安全角度来看，我们希望及时发现任何数据泄露的尝试，并采取相应的行动。监控应为自动化过程，并具备警报功能。
- en: Security
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安全
- en: Related to both data at rest and in transit, we need to define roles and related
    permissions to ensure access control. Some time series data is sensitive and only
    authorized personnel should be able to view or manipulate the data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是静态数据还是传输中的数据，我们都需要定义角色和相关权限，以确保访问控制。某些时间序列数据具有敏感性，只有授权人员才能查看或操作这些数据。
- en: In regulated industries, when handling personal data, we need to ensure that
    data handling and storage practices comply with relevant regulations (HIPAA, GDPR,
    etc.). This also involves ensuring privacy and managing consent for personal data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在受监管行业中，处理个人数据时，我们需要确保数据处理和存储实践符合相关法规（如HIPAA、GDPR等）。这还涉及确保隐私并管理个人数据的同意。
- en: Governance
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 治理
- en: In addition to the preceding, the data governance practice is responsible for
    assigning roles and responsibilities to manage data. As part of this, the data
    stewards oversee data quality, compliance, and policies.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前述内容，数据治理实践负责分配角色和责任以管理数据。作为其中的一部分，数据管理员负责监督数据质量、合规性和政策。
- en: By establishing the right processes, people, and tools in place, we can ensure
    the prevention of data breaches and effective mitigation if they do occur.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过建立正确的流程、人员和工具，我们可以确保预防数据泄露，并在发生时有效减轻其影响。
- en: We have now covered the process of ingesting and transforming data into trustworthy
    and useful data in a governed and secure way. The step left now as part of DataOps
    is to share the data for analysis and consumption by users or other systems.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了以可信和有用的方式将数据摄取并转化为受治理和安全的数据的过程。作为DataOps的一部分，剩下的步骤就是将数据共享给用户或其他系统进行分析和消费。
- en: Sharing and consumption
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共享与消费
- en: After ingestion and processing the data, we want the curated data and outcome
    of analytics to be visible and available to users. A centralized **data catalog**,
    complete with descriptions and usage guidelines, allows users to easily discover
    and access available datasets.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据摄取和处理之后，我们希望经过整理的数据和分析结果能够对用户可见并可访问。一个集中的**数据目录**，包括描述和使用指南，可以让用户轻松发现并访问可用的数据集。
- en: Finally, as part of the DataOps phase, we want data scientists, analysts, and
    other users to be able to consume the data for exploration, analysis, and reporting.
    Ideally, we want to tie this to governance to ensure that the right set of users
    are accessing and consuming permitted datasets only. Access and methods for consumption
    include file-based access, database connection, and APIs, among others.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作为DataOps阶段的一部分，我们希望数据科学家、分析师和其他用户能够使用数据进行探索、分析和报告。理想情况下，我们希望将其与治理结合，确保只有授权的用户访问和使用允许的数据集。访问方式和使用方法包括基于文件的访问、数据库连接和API等。
- en: As discussed in this section, DataOps is the set of processes to ensure that
    data is available, accessible, and usable. It is iterative, with feedback from
    consumers and continuous improvement to data, pipelines, and practices. By establishing
    a scalable and flexible infrastructure with Apache Spark’s processing power and
    versatility at its core, DataOps ensures that data scientists and analysts have
    the high-quality data they need when they need it, to derive insights and drive
    decisions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节所讨论的，DataOps是一系列确保数据可用、可访问和可用的过程。它是迭代的，通过来自消费者的反馈以及对数据、管道和实践的持续改进来实现。通过建立一个可扩展且灵活的基础设施，并以Apache
    Spark的处理能力和多功能性为核心，DataOps确保数据科学家和分析师能够在需要时获得高质量的数据，以获取洞察并推动决策。
- en: We will cover the practical considerations for DataOps in [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103),
    *Data Preparation*. For now, let’s focus on ModelOps, which is the next phase
    after DataOps.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第5章*](B18568_05.xhtml#_idTextAnchor103)《数据准备》中讨论DataOps的实际考虑。目前，让我们专注于ModelOps，它是继DataOps之后的下一个阶段。
- en: ModelOps
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ModelOps
- en: While DataOps is about the data life cycle, ModelOps is about the model life
    cycle – more specifically, statistical and machine learning models. The objective
    is to manage the models from development to deployment, ensuring that they are
    reliable, accurate, and scalable while delivering actionable insights based on
    the use cases’ requirements.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然DataOps关注的是数据生命周期，ModelOps关注的是模型生命周期——更具体地说，是统计和机器学习模型。其目标是从开发到部署管理模型，确保模型可靠、准确且可扩展，同时根据用例要求提供可操作的洞察。
- en: ModelOps, MLOps, and LLMOps
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ModelOps、MLOps和LLMOps
- en: These terms have overlapping definitions and are sometimes used interchangeably.
    In this book, we will refer to ModelOps as the broader life cycle management practice
    for different types of models, including simulations, statistical, and machine
    learning models. We will use **machine learning operations** (**MLOps**) more
    specifically for machine learning models and **large language model operations**
    (**LLMOps**) for the specific considerations that apply to the life cycle of LLMs.
    As such, ModelOps will refer to the superset of practices.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语具有重叠的定义，有时可以互换使用。在本书中，我们将“ModelOps”作为不同类型模型（包括仿真模型、统计模型和机器学习模型）的更广泛生命周期管理实践来使用。我们将更具体地使用**机器学习运维**（**MLOps**）来指代机器学习模型，并将**大语言模型运维**（**LLMOps**）用于特指大语言模型生命周期中的相关考虑。因此，ModelOps将指代这一系列实践的总和。
- en: ModelOps practices can be categorized broadly into model development and testing,
    and model deployment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ModelOps实践大致可以分为模型开发与测试，以及模型部署。
- en: Model development and testing
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型开发与测试
- en: Model development and testing involves creating and fine-tuning time series
    analysis models based on historical data. This process starts with feature engineering,
    selecting appropriate algorithms, such as Autoregressive Integrated Moving Average
    (ARIMA) or Long Short-Term Memory (LSTM), and splitting the data into training
    and testing sets. Then, models are iteratively trained and evaluated using performance
    metrics. This ensures accuracy. After that, by testing the model on unseen data,
    we can ensure that the model can generalize well to new, real-world scenarios.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 模型开发与测试涉及基于历史数据创建和微调时间序列分析模型。这个过程从特征工程开始，选择合适的算法，如自回归综合滑动平均（ARIMA）或长短期记忆（LSTM），并将数据划分为训练集和测试集。然后，模型通过反复训练和评估性能指标来确保准确性。接下来，通过在未见过的数据上测试模型，我们可以确保模型能够很好地推广到新的、真实的场景中。
- en: 'We will now detail further each of these steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将进一步详细介绍每个步骤：
- en: '**Feature engineering**: Overlapping with the DataOps phase, feature engineering
    is the initial stage of model development, concerned with the identification of
    existing and creation of new features from the time series data. These include
    creating lags and rolling averages features, where information from previous time
    steps is used to calculate new features, and creating temporal features that capture
    the time-based characteristics such as specific time of day, day of the week,
    month, or holidays. In addition, the feature engineering stage covers transformations
    to make time series stationary, such as differencing or log transformation, or
    resampling to make time series regular, as discussed in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016).
    We will see how Apache Spark can be used for feature engineering in [*Chapter
    8*](B18568_08.xhtml#_idTextAnchor151), on model development.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征工程**：与DataOps阶段重叠，特征工程是模型开发的初始阶段，关注的是从时间序列数据中识别现有特征并创建新特征。这包括创建滞后和滚动平均特征，其中使用前一时间步的数据计算新特征，以及创建捕捉时间相关特征的时间特征，如一天中的特定时间、星期几、月份或假期。此外，特征工程阶段还包括对时间序列进行平稳化的转换，如差分或对数变换，或通过重采样使时间序列变得规律化，如在[*第1章*](B18568_01.xhtml#_idTextAnchor016)中所讨论的那样。我们将在[*第8章*](B18568_08.xhtml#_idTextAnchor151)中看到如何使用Apache
    Spark进行特征工程，涉及模型开发。'
- en: '**Model selection**: The model to choose is from an ever-growing list of candidate
    models for time series: ARIMA, Prophet, machine learning, deep learning models
    such as LSTM, and many others. The right time series model depends on the data
    available and the use case we are implementing, as we have seen with the use case
    examples in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044). **Exploratory data
    analysis** (**EDA**), detailed in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116),
    guides us in this process by providing an understanding of the data’s trends,
    seasonality, and underlying patterns. Finding the best model, however, is part
    of an iterative process, refined by model validation, which we will now present
    as the next step.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型选择**：选择模型是从一个不断增长的时间序列候选模型列表中挑选：ARIMA、Prophet、机器学习、深度学习模型（如LSTM）等。正确的时间序列模型取决于可用的数据和我们要实现的用例，正如我们在[*第2章*](B18568_02.xhtml#_idTextAnchor044)中的用例示例中看到的那样。**探索性数据分析**（**EDA**），在[*第6章*](B18568_06.xhtml#_idTextAnchor116)中详细介绍，帮助我们理解数据的趋势、季节性和潜在模式，从而指导我们完成这一过程。然而，找到最佳模型是一个迭代过程，通过模型验证来不断完善，我们将在下一步中介绍这个过程。'
- en: '**Dataset splitting**: Once we have candidate models, the first step before
    training the models is to split the historical data into training, validation,
    and test datasets. The specific consideration in doing so for time series data
    is twofold: to preserve the chronological order within datasets, and to ensure
    that there is no data leakage between the splits.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据集划分**：一旦我们有了候选模型，训练模型之前的第一步是将历史数据划分为训练集、验证集和测试集。在时间序列数据中进行此操作的具体考虑因素有两个：一是保持数据集内的时间顺序，二是确保在划分之间没有数据泄漏。'
- en: '**Training**: During this phase, the model is fitted to the training dataset
    by adjusting its parameters. This can be supervised with predefined labels or
    actual outcomes, or unsupervised, as explained in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044).
    In the case of supervised training, the model parameters are adjusted using a
    process such as gradient descent to minimize the difference, using a loss function,
    between model prediction and actual outcome. For unsupervised training, the model
    is adjusted until a stopping criterion is met, such as the number of runs or the
    number of categorization classes.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练**：在这一阶段，模型通过调整其参数来拟合训练数据集。这可以是有监督的，使用预定义的标签或实际结果，或者是无监督的，如在[*第2章*](B18568_02.xhtml#_idTextAnchor044)中解释的那样。在有监督训练的情况下，模型参数通过诸如梯度下降的过程进行调整，以最小化模型预测与实际结果之间的差异，使用损失函数进行优化。对于无监督训练，模型会进行调整，直到满足停止标准，如运行次数或分类类别数。'
- en: '**Validation**: As part of training iterations, model validation uses unseen
    validation datasets with techniques such as time-based cross-validation. This
    is to check that there is no overfitting and that the trained model can generalize
    to unseen data with acceptable accuracy. The model is evaluated for accuracy using
    metrics such as **mean absolute percentage error** (**MAPE**) or **mean absolute
    error** (**MAE**). As an iterative process, this stage includes hyperparameter
    tuning, where models with different settings are trained and validated to find
    the best model configuration. Techniques such as grid search or Bayesian optimization
    are used to search for the optimal hyperparameters.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**验证**：作为训练迭代的一部分，模型验证使用未见过的验证数据集，并采用如基于时间的交叉验证等技术。这是为了检查是否存在过拟合，并确保训练后的模型能够以可接受的准确性对未见过的数据进行泛化。模型的准确性通过如**平均绝对百分比误差**（**MAPE**）或**平均绝对误差**（**MAE**）等指标进行评估。作为一个迭代过程，这一阶段包括超参数调优，在此过程中，不同设置的模型被训练和验证，以找到最佳的模型配置。技术如网格搜索或贝叶斯优化被用来寻找最优的超参数。'
- en: Parameters and hyperparameters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 参数与超参数
- en: Note the difference between parameters and hyperparameters. These terms are
    often confused. Model **parameters** are learned from the data as part of a training
    run, such as a neural network’s weights and biases. **Hyperparameters** define
    the configuration of the model prior to model training, which, in the case of
    a neural network, can be the number of nodes and layers defining its architecture.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意参数和超参数之间的区别。这些术语常常被混淆。模型**参数**是通过训练过程从数据中学习得来的，例如神经网络的权重和偏差。**超参数**则是在模型训练之前定义的模型配置，举例来说，在神经网络中，超参数可以是定义其架构的节点数和层数。
- en: '**Testing** – As a final step in model development, the model is evaluated
    against the unseen testing dataset and compared with different algorithms or types
    of models. Testing can also include other criteria beyond model accuracy, such
    as response time, and integration testing with the application code used with
    the model.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试** – 作为模型开发的最后一步，模型会在未见过的测试数据集上进行评估，并与不同的算法或模型类型进行比较。测试还可以包括超出模型准确度的其他标准，如响应时间，以及与模型配合使用的应用代码的集成测试。'
- en: Model training, validation, and testing will be covered in detail in [*Chapter
    7*](B18568_07.xhtml#_idTextAnchor133).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练、验证和测试将在[*第7章*](B18568_07.xhtml#_idTextAnchor133)中详细讨论。
- en: Model deployment and monitoring
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型部署与监控
- en: Model deployment and monitoring involves the transition of time series analysis
    models from development to a live production environment, together with continuous
    oversight of their performance. This ongoing monitoring allows retraining and
    updating the model to adapt to changes in the data patterns or in the behavior
    of the underlying system being analyzed.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署与监控涉及将时间序列分析模型从开发环境过渡到生产环境，并持续监控其性能。这种持续的监控使得模型能够在数据模式或被分析的底层系统行为发生变化时，进行重新训练和更新。
- en: 'We will now detail further each of these steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将进一步详细说明这些步骤：
- en: '**Deployment**: Models are deployed for production into a model-serving framework.
    This can be containerized with tools such as Kubernetes and Docker or deployed
    to a cloud-based solution such as Databricks Model Serving, Amazon SageMaker,
    Azure Machine Learning, or Google Vertex AI. Once deployed, the model can be used
    for batch inferencing, scheduled at recurring intervals, or real-time inferencing
    based on continuously streaming data sources or in response to API requests.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署**：模型被部署到生产环境中的模型服务框架中。这可以通过像Kubernetes和Docker这样的工具进行容器化，或者部署到基于云的解决方案中，例如Databricks模型服务、Amazon
    SageMaker、Azure机器学习或Google Vertex AI。一旦部署，模型可以用于批量推理，按定期间隔安排，或基于持续流数据源进行实时推理，或响应API请求。'
- en: '**Monitoring**: Once the model is deployed in production, monitoring is required
    to ensure that it remains fit for purpose and of value. With **data drift** (the
    change in the characteristics of the data over time) and **concept drift**, where
    the model’s representation of reality worsens over time, model accuracy decreases.
    This can be detected with model monitoring, and alerts sent accordingly.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监控**：一旦模型在生产环境中部署，就需要进行监控，以确保模型持续适配目的并有价值。随着**数据漂移**（数据特征随时间的变化）和**概念漂移**（模型对现实的表征随时间恶化），模型的准确性会下降。这可以通过模型监控来检测，并根据情况发送警报。'
- en: '**Retraining**: When the monitoring alerts about a drift, and if the drift
    is significant enough, retraining the model is the next step. This can be manually
    launched or automated. If retraining does not yield a sufficiently accurate model,
    then we will have to go back to the model development cycle to find another model
    fit for purpose.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**再训练**：当监控警报提示出现漂移时，如果漂移足够显著，下一步是对模型进行再训练。这可以手动启动，也可以自动化。如果再训练未能产生足够准确的模型，我们将不得不回到模型开发周期，寻找适合目的的其他模型。'
- en: '**Governance**: This includes several key considerations. We need to keep track
    of model versions and life cycle stages throughout the model’s life cycle and
    associated processes. In addition, for auditability purposes, logs of training,
    deployment, and accuracy metrics are kept, and in some cases, requests to and
    responses from model inference are also saved. Additional considerations include
    access control to the model and ensuring it meets all legal and regulatory compliance
    requirements, including when dealing with personal or sensitive data.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**治理**：这包括几个关键的考虑因素。我们需要在模型的整个生命周期和相关过程中跟踪模型版本和生命周期阶段。此外，为了审计目的，会保存训练、部署和准确性指标的日志，在某些情况下，还会保存模型推理的请求和响应。其他考虑因素包括模型的访问控制，并确保它符合所有法律和合规要求，尤其是在处理个人或敏感数据时。'
- en: In summary, ModelOps for a time series analysis project covers the end-to-end
    process of developing, deploying, and maintaining models, while overlapping with
    DataOps for its data requirements. ModelOps ensures continuous improvement, reproducibility,
    collaboration, and fitness for purpose with respect to business objectives. It
    also maintains the model’s effectiveness and ensures that it keeps delivering
    value over time.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，时间序列分析项目的ModelOps涵盖了从开发、部署到维护模型的端到端过程，同时与DataOps的相关数据需求有所交集。ModelOps确保持续改进、可复现性、协作和与业务目标的适配性。它还维持模型的有效性，并确保模型随时间持续提供价值。
- en: We will cover the practical considerations for ModelOps in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133),
    *Building and Testing Models*. The next phase is DevOps, which we will detail
    now.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第7章*](B18568_07.xhtml#_idTextAnchor133)中详细介绍ModelOps的实际考虑因素，*构建和测试模型*。接下来的阶段是DevOps，我们现在将详细介绍。
- en: DevOps
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DevOps
- en: Next after ModelOps, DevOps is a set of practices and tools that smoothen the
    handover between **development** (**Dev**) and **operations** (**Ops**). This
    is for both the model and related application code. By automating the building,
    testing, deployment, and monitoring of time series applications, DevOps ensures
    that they are reliable, scalable, and continuously deliver value to the business.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 紧接着ModelOps之后，DevOps是一组实践和工具，旨在平滑**开发**（**Dev**）与**运维**（**Ops**）之间的交接。这适用于模型及其相关的应用程序代码。通过自动化时间序列应用的构建、测试、部署和监控，DevOps确保它们是可靠的、可扩展的，并能持续为业务提供价值。
- en: The DevOps practices can be broadly broken down into **continuous integration/continuous
    deployment** (**CI/CD**), infrastructure management, and monitoring and governance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps实践大致可以分为**持续集成/持续部署**（**CI/CD**）、基础设施管理、监控和治理。
- en: CI/CD
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: CI/CD
- en: CI/CD involves automating the integration and deployment of time series analysis
    models for seamless updates to production.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: CI/CD涉及自动化时间序列分析模型的集成和部署，以便对生产环境进行无缝更新。
- en: 'This includes the following steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括以下步骤：
- en: '**Code and model versioning and repository**: Code and model changes require
    tracking, with the possibility of rolling back to previous versions if needed.
    This means that the code and models need to be version-controlled and stored in
    a repository from where the different versions can be accessed.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**代码和模型版本管理与仓库**：代码和模型的变化需要进行跟踪，并且如果需要，可以回滚到以前的版本。这意味着代码和模型需要进行版本控制，并存储在一个仓库中，便于访问不同的版本。'
- en: '**Testing**: It is crucial that there is no regression whenever changes are
    made to the time series model and associated code. One way to ensure this is through
    automated testing, with unit and integration testing, which can be kicked off
    either when production monitoring detects a degradation or when there are model
    or associated code changes in development.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试**：每当时间序列模型和相关代码发生变化时，确保没有回归是至关重要的。确保这一点的一种方法是通过自动化测试，进行单元测试和集成测试，这些测试可以在生产监控检测到性能下降时启动，或者在开发环境中模型或相关代码发生变化时启动。'
- en: '**Deployment**: Once the time series model and code are ready in development,
    the next steps are deployment to staging and production. It is recommended to
    automate this deployment with CI/CD pipelines to minimize the risks of errors
    due to manual steps and make this a seamless, repeatable, and scalable process.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署**：一旦时间序列模型和代码在开发环境中准备好，接下来的步骤就是部署到预生产和生产环境。推荐使用CI/CD管道自动化此部署，以最小化由于手动步骤引起的错误风险，并使该过程成为无缝、可重复和可扩展的。'
- en: In summary, CI/CD pipelines ensure that new features, improvements, and bug
    fixes are consistently integrated, tested, and deployed while minimizing downtime
    and enhancing the efficiency of new code rollout.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，CI/CD管道确保新功能、改进和漏洞修复能够持续集成、测试和部署，同时最大程度地减少停机时间，提高新代码发布的效率。
- en: Infrastructure management
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础设施管理
- en: '**Infrastructure as code** (**IaC**) is a recommended approach to provisioning
    as it enables the infrastructure configurations to be version-controlled, self-documented,
    reproducible, and scalable. This is how compute, storage, and networking configurations
    can be set consistently. In a virtual environment such as a cloud environment,
    the infrastructure itself is, in a sense, version-controlled as it is software-defined
    in nature.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**基础设施即代码**（**IaC**）是一种推荐的配置方法，因为它使基础设施配置可以进行版本控制、自我文档化、可重现并且可扩展。这是设置计算、存储和网络配置一致性的一种方式。在云环境等虚拟环境中，基础设施本身在某种意义上是版本控制的，因为它是软件定义的。'
- en: In addition to the previous core resources, security-specific configurations
    require provisioning for access controls, encryption, and firewalls for network
    security.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前述核心资源外，安全特定的配置还需要为访问控制、加密和网络安全防火墙提供配置。
- en: As demand for the application changes, the corresponding workload changes with
    a requirement for additional or fewer infrastructure resources. A scalable infrastructure
    management process ensures that the infrastructure is automatically scaled based
    on demand.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用需求的变化，相应的工作负载也会变化，可能需要更多或更少的基础设施资源。一个可扩展的基础设施管理流程确保基础设施根据需求自动进行扩展。
- en: Monitoring, security, and governance
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控、安全性和治理
- en: DevOps has similar requirements for monitoring, security, and governance to
    DataOps and ModelOps. The scope for DevOps encompasses everything that is deployed
    to the production environment, including models, code, and configurations. This
    is typically fulfilled via processes such as application, security, and compliance
    monitoring, logging and alerting, and incident management.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps在监控、安全性和治理方面的要求与DataOps和ModelOps类似。DevOps的范围涵盖了部署到生产环境中的一切，包括模型、代码和配置。这通常通过诸如应用程序、安全性和合规性监控、日志记录和警报、以及事件管理等流程来实现。
- en: In summary, DevOps ensures that applications, including time series analysis,
    are highly available and scalable by automating their deployment, management,
    and scaling. The key here is to make the transition from *Dev* to *Ops* seamless
    by facilitating collaboration and using automation to ensure that a time series
    analysis project can evolve from a use case concept to its technical implementation
    to a fully operational system that drives significant business impact and value.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，DevOps 通过自动化部署、管理和扩展，确保应用程序（包括时间序列分析）具有高可用性和可扩展性。关键在于通过促进协作和使用自动化，使得从 *Dev*
    到 *Ops* 的过渡无缝化，确保时间序列分析项目能够从用例概念演变为技术实现，再到驱动显著业务影响和价值的全面运营系统。
- en: Now that we understand the end-to-end phases of a time series analysis project,
    the next section will provide practical examples and tools for implementing what
    we have learned so far in this chapter.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了时间序列分析项目的端到端阶段，接下来的部分将提供实际示例和工具，以实施我们在本章中所学的内容。
- en: Implementation examples and tools
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施示例和工具
- en: 'With the end-to-end phases defined, this section will examine two implementation
    examples: a notebook-based approach and an orchestrator-based approach.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了端到端阶段后，本节将探讨两种实施示例：基于笔记本的方法和基于协调器的方法。
- en: Note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you do not intend to build your own end-to-end environment, you can skip
    the practical part of this section and use a managed platform such as Databricks,
    as we will do in future chapters.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不打算构建自己的端到端环境，可以跳过本节的实践部分，使用像 Databricks 这样的托管平台，正如我们将在后续章节中所做的那样。
- en: Let’s start by setting up the environment required to run the examples.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从设置运行示例所需的环境开始。
- en: Environment setup
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境设置
- en: We will be using Docker containers, as in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063),
    for the platform infrastructure. Refer to the *Using a container for deployment*
    section in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063) for instructions on
    installing Docker.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Docker 容器，正如在 [*第 3 章*](B18568_03.xhtml#_idTextAnchor063) 中所示，用于平台基础设施。有关安装
    Docker 的说明，请参考 [*第 3 章*](B18568_03.xhtml#_idTextAnchor063) 中的 *使用容器进行部署* 部分。
- en: Alternative to Docker
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 的替代方案
- en: 'You can use Podman as an open source alternative to Docker. You can find more
    information here: [https://podman.io/](https://podman.io/).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Podman 作为 Docker 的开源替代方案。你可以在这里找到更多信息：[https://podman.io/](https://podman.io/)。
- en: Before we can deploy the Docker containers, we will validate in the next section
    that there is no conflict with the network ports that will be used by the containers.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以部署 Docker 容器之前，我们将在下一部分验证容器将使用的网络端口是否存在冲突。
- en: Network ports
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络端口
- en: 'The following network ports need to be available on your local machine or development
    environment:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下网络端口需要在你的本地机器或开发环境中可用：
- en: 'Apache Spark: `7077`, `8070`, and `8081`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Apache Spark: `7077`、`8070` 和 `8081`'
- en: 'Jupyter Notebook: `4040`, `4041`, `4042`, and `8888`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jupyter Notebook: `4040`、`4041`、`4042` 和 `8888`'
- en: 'MLflow: `5001`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'MLflow: `5001`'
- en: 'Airflow: `8080`'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Airflow: `8080`'
- en: 'You can check for the current use of these ports by existing applications with
    the following command, run from the command line or terminal:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下命令检查现有应用程序是否正在使用这些端口，从命令行或终端运行：
- en: '[PRE0]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you see the required ports in the list of ports already in use, you must
    either stop the application using that port or change the `docker-compose` file
    to use another port.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到所需的端口已在使用的端口列表中，你必须停止使用该端口的应用程序，或者修改 `docker-compose` 文件以使用其他端口。
- en: As an example, let’s assume that the output of the preceding `netstat` command
    reveals that port `8080` is already in use on your local machine or development
    environment, and you are not able to stop the existing application using this
    port.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，假设前面的 `netstat` 命令输出显示本地机器或开发环境中端口 `8080` 已被占用，且你无法停止使用该端口的现有应用程序。
- en: 'In this case, you will need to change port `8080` (meant for the Airflow web
    server) in the `docker-compose.yaml` file to another, unused port. Just search
    and replace `8080` on the left of the colon (`:`) to say `8090` if this port is
    free, as per the following example:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你需要在 `docker-compose.yaml` 文件中将端口 `8080`（用于 Airflow Web 服务器）更改为另一个未使用的端口。只需搜索并将冒号（`:`）左边的
    `8080` 替换为 `8090`，如果该端口未被占用，如下所示：
- en: 'From this:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自此：
- en: '[PRE1]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To this:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE2]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Keep note of the new port and use this instead of the existing one whenever
    you need to type the corresponding URL. In this example, port `8080` is changed
    to `8090`, and the matching URL change for the Airflow web server is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 记下新端口，并在需要输入对应URL时使用该端口。在此示例中，端口`8080`已更改为`8090`，Airflow Web服务器的匹配URL更改如下：
- en: 'From this:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从此开始：
- en: '`http://localhost:8080/`'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`http://localhost:8080/`'
- en: 'To this:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改为：
- en: '`http://localhost:8090/`'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`http://localhost:8090/`'
- en: Note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You will need to change the network port in all URLs in the following sections
    that you had to modify as per this section.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在以下部分中的所有URL中更改网络端口，按照本节的说明进行修改。
- en: Environment startup
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境启动
- en: 'Once Docker is installed and running, and the network port configuration is
    validated, the following instructions guide you to set up and start the environment:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Docker安装并运行，且网络端口配置已验证，以下指令将指导你设置和启动环境：
- en: 'We first download the deployment script from the Git repository for this chapter,
    which is at the following URL:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从本章的Git仓库下载部署脚本，仓库地址如下：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4)'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4)'
- en: 'We will be using the `git clone`-friendly URL, which is as follows:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将使用`git clone`友好的URL，具体如下：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git)'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git)'
- en: 'To do this, start a terminal or command line and run the following commands:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为此，启动终端或命令行并运行以下命令：
- en: '[PRE3]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools),
    missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'xcrun: 错误：无效的活动开发者路径（/Library/Developer/CommandLineTools），缺少xcrun，路径为：/Library/Developer/CommandLineTools/usr/bin/xcrun'
- en: '[PRE4]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: xcode-select --install
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: xcode-select --install
- en: '[PRE5]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now start the container build and startup. A makefile is provided to
    simplify the process of starting and stopping the containers. The following command
    builds the Docker images for the containers and then starts them:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以开始构建和启动容器了。提供了一个makefile以简化启动和停止容器的过程。以下命令构建容器的Docker镜像并启动它们：
- en: '[PRE6]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Windows environment
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Windows环境
- en: 'If you are using a Windows environment, you can install a Windows version of
    `make`, as per the following documentation: [https://gnuwin32.sourceforge.net/packages/make.htm](https://gnuwin32.sourceforge.net/packages/make.htm)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Windows环境，你可以按照以下文档安装Windows版本的`make`：[https://gnuwin32.sourceforge.net/packages/make.htm](https://gnuwin32.sourceforge.net/packages/make.htm)
- en: 'The `make up` command will give the following or equivalent output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`make up`命令将输出以下或等效的内容：'
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You may see the following error when you run the preceding `make` `up` command:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你运行上面的`make` `up`命令时，你可能会看到以下错误：
- en: '[PRE8]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You may get an error if you have `bash` instead of`sh` in your environment and
    the script cannot locate the `sh` file. In this case, change the last line in
    makefile from "`sh prep-airflow.sh`" to "`bash prep-airflow.sh`" and then run
    the `make` `up` command again.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你的环境中使用的是`bash`而不是`sh`，且脚本无法找到`sh`文件，你可能会遇到错误。在这种情况下，将makefile中的最后一行从"`sh
    prep-airflow.sh`"更改为"`bash prep-airflow.sh`"，然后再次运行`make` `up`命令。
- en: 'By the end of the process, as in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063),
    you will have a running Spark cluster and a separate node for Jupyter Notebook.
    In addition, we have deployed the following components here:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 经过该过程的结束，正如[*第3章*](B18568_03.xhtml#_idTextAnchor063)所述，你将拥有一个运行中的Spark集群和一个独立的Jupyter
    Notebook节点。此外，我们在这里已部署以下组件：
- en: '**MLflow** – An open source platform, originally developed by Databricks, for
    managing the end-to-end machine learning life cycle. With features for experimentation
    and deployment, MLflow is designed to work with any machine learning library and
    programming language. This makes it flexible for various environments and use
    cases, which explains its broad adoption.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow** – 一个开源平台，最初由Databricks开发，用于管理端到端的机器学习生命周期。MLflow具有实验和部署功能，旨在与任何机器学习库和编程语言兼容。这使得它在各种环境和用例中具有灵活性，也解释了它被广泛采用的原因。'
- en: 'You can find more information here: [https://mlflow.org/](https://mlflow.org/).'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在这里找到更多信息：[https://mlflow.org/](https://mlflow.org/)。
- en: '**Apache Airflow** – Created by Airbnb, Airflow is an open source platform
    for orchestrating data processing pipelines and computational workflows. With
    the ability to programmatically define, schedule, and monitor workflows at scale,
    Airflow is widely adopted, including by data engineers and data scientists, for
    diverse types of workflows.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Airflow** – 由Airbnb创建，Airflow是一个开源平台，用于协调数据处理管道和计算工作流。通过能够以编程方式定义、调度和监控大规模的工作流，Airflow被广泛采用，包括数据工程师和数据科学家在内，用于各种类型的工作流。'
- en: 'You can find more information here: [https://airflow.apache.org/](https://airflow.apache.org/).'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在这里找到更多信息：[https://airflow.apache.org/](https://airflow.apache.org/)。
- en: '**Postgres** – This is the relational database used in the backend by Airflow.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Postgres** – 这是Airflow后台使用的关系型数据库。'
- en: Let’s now validate the environment that we have just deployed.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们验证刚刚部署的环境。
- en: Accessing the UIs
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问用户界面
- en: 'We will now access the **user interfaces** (**UIs**) of the different components
    as a quick way to validate the deployment:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将访问不同组件的**用户界面**（**UIs**），作为快速验证部署的一种方式：
- en: 'Follow the instructions in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063)
    to validate the deployment of Jupyter Notebook and the Apache Spark cluster. Note
    that due to the Airflow web server using port `8080`, which is the same port we
    used in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063) for Apache Spark, we have
    changed the Spark master node to the following local URL:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请按照[*第三章*](B18568_03.xhtml#_idTextAnchor063)中的指示验证Jupyter Notebook和Apache Spark集群的部署。请注意，由于Airflow
    Web服务器使用端口`8080`，这是我们在[*第三章*](B18568_03.xhtml#_idTextAnchor063)中为Apache Spark使用的相同端口，因此我们已将Spark主节点更改为以下本地网址：
- en: '`http://localhost:8070/`'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`http://localhost:8070/`'
- en: 'MLflow is accessible at the following local URL:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MLflow可以通过以下本地网址访问：
- en: '`http://localhost:5001/`'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`http://localhost:5001/`'
- en: This will open the web page as per *Figure 4**.4*.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打开如*图4.4*所示的网页。
- en: '![](img/B18568_04_4.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_4.jpg)'
- en: 'Figure 4.4: MLflow'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：MLflow
- en: 'The next UI, *Figure 4**.5*, is for Airflow, accessible via the following local
    URL:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个UI，*图4.5*，是Airflow界面，可以通过以下本地网址访问：
- en: '`http://localhost:8080/`'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`http://localhost:8080/`'
- en: The default username and password are `airflow`, which is highly recommended
    to change.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 默认的用户名和密码是`airflow`，强烈建议更改。
- en: '![](img/B18568_04_5.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_5.jpg)'
- en: 'Figure 4.5: Airflow'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：Airflow
- en: We now have our environment set up, which we will use next.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经设置好环境，接下来将使用这个环境。
- en: Notebook approach
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Notebook方法
- en: We have used notebooks from [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016),
    where we started with the Databricks Community edition. In [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063),
    we deployed our own notebook environment with Jupyter, which is an open source
    implementation. As we have seen by now, notebooks give us a feature-rich document-type
    interface where we can combine executable code, visualizations, and text. This
    makes notebooks popular for interactive and collaborative work in data science
    and machine learning. Notebooks can also be built for execution in a non-interactive
    way, which, together with the fact that they have already been used in the earlier
    data science and experimentation phases, makes them readily adaptable to an end-to-end
    notebook.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了[*第一章*](B18568_01.xhtml#_idTextAnchor016)中的笔记本，我们从Databricks Community版本开始。在[*第三章*](B18568_03.xhtml#_idTextAnchor063)中，我们部署了自己的Jupyter笔记本环境，这是一个开源实现。正如我们到目前为止所看到的，笔记本提供了一个功能丰富的文档类型界面，在这里我们可以结合可执行代码、可视化和文本。这使得笔记本在数据科学和机器学习的互动和协作工作中非常流行。笔记本也可以构建为非交互式执行，这与它们已经在早期的数据科学和实验阶段中使用过的事实结合，使得它们能够方便地适应端到端的笔记本。
- en: 'In this first example, we will use an all-in-one notebook based on the Prophet-based
    code introduced in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016). If you followed
    the instructions in the earlier *Environment setup* section, the example notebook
    should be accessible directly within the Jupyter Notebook UI, at the `work / notebooks`
    location on the left folder navigation panel, as shown in *Figure 4**.6*, at the
    following URL: `http://localhost:8888/lab`.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个示例中，我们将使用基于[*第一章*](B18568_01.xhtml#_idTextAnchor016)中介绍的基于Prophet的代码的全功能笔记本。如果你按照之前的*环境设置*部分中的指示操作，示例笔记本应该可以直接在Jupyter
    Notebook UI中访问，在左侧文件夹导航面板的`work / notebooks`位置，如*图4.6*所示，网址为：`http://localhost:8888/lab`。
- en: '![](img/B18568_04_6.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_6.jpg)'
- en: 'Figure 4.6: Notebook'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：Notebook
- en: 'The notebook is also downloadable from the following GitHub location:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本也可以从以下GitHub位置下载：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/notebooks/ts-spark_ch4_data-ml-ops.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/notebooks/ts-spark_ch4_data-ml-ops.ipynb)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/notebooks/ts-spark_ch4_data-ml-ops.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/notebooks/ts-spark_ch4_data-ml-ops.ipynb)'
- en: 'With a focus more on structure here than on the code itself, which does not
    change much from [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016), we structured
    the notebook into the following sections:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点更多地放在结构上，而非代码本身，代码内容与[*第1章*](B18568_01.xhtml#_idTextAnchor016)变化不大，我们将笔记本分成了以下几个部分：
- en: Config
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置
- en: DataOps
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataOps
- en: Ingest data from source
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从源获取数据
- en: Transform data
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换数据
- en: ModelOps
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ModelOps
- en: Train and log model
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练并记录模型
- en: Forecast with model
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型进行预测
- en: The notable addition to the code from [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016),
    in addition to the structure explained previously, is the MLOps part, which we
    will detail next.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面解释的结构外，从[*第1章*](B18568_01.xhtml#_idTextAnchor016)引入的代码中还有MLOps部分，接下来我们将详细说明。
- en: MLOps with MLflow
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用MLflow进行MLOps
- en: 'In this notebook example, we use MLflow as the tool to implement several MLOps
    requirements. The following code extract focuses on this specific part:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在此笔记本示例中，我们使用MLflow作为工具来实现多个MLOps需求。以下代码片段专注于这一部分：
- en: '[PRE9]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The MLflow functionalities used in the preceding code are the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中使用的MLflow功能如下：
- en: '`set_tracking_uri` – This sets the URI of the tracking server where MLflow
    will store model-related information. This centralizes model data and facilitates
    collaboration among team members. The tracking server can be a remote server or
    a local file path.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`set_tracking_uri` – 这个函数设置跟踪服务器的URI，MLflow将在该服务器上存储与模型相关的信息。这样可以集中管理模型数据，促进团队成员之间的协作。跟踪服务器可以是远程服务器，也可以是本地文件路径。'
- en: '`set_experiment` – This creates a new experiment or uses an existing one. An
    experiment is a logical grouping of runs (separate model training or trials),
    useful to organize and compare different trials.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`set_experiment` – 这个函数会创建一个新实验，或者使用现有的实验。实验是运行（单独的模型训练或试验）的逻辑分组，有助于组织和比较不同的试验。'
- en: '`start_run` – This starts a new MLflow run, which can be within a given experiment.
    As a representation of a single training or trial, `run` groups related artifacts
    such as parameters, metrics, and models.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_run` – 这会启动一个新的MLflow运行，可以在给定的实验中进行。作为单次训练或试验的表示，`run`会将相关的工件（如参数、指标和模型）归类。'
- en: '`prophet.log_model` – This function logs a Prophet model as an artifact in
    the current MLflow run.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prophet.log_model` – 该函数将一个Prophet模型作为工件记录到当前的MLflow运行中。'
- en: '`log_params` – This logs key-value pairs of parameters used during the run.
    Parameters are model configurations.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_params` – 该函数记录运行过程中使用的参数的键值对。参数是模型配置。'
- en: '`log_metrics` – This logs key-value pairs of metrics evaluated during the run.
    Metrics are numerical values about the model’s performance (e.g., Mean Squared
    Error, accuracy).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_metrics` – 该函数记录运行过程中评估的指标的键值对。指标是关于模型性能的数值（例如均方误差、准确率）。'
- en: 'The outcome of this can then be accessed via the MLflow UI at the following
    URL: `http://localhost:5001/`.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以通过MLflow UI在以下网址访问：`http://localhost:5001/`。
- en: This will open the UI to a similar page as per *Figure 4**.4*, from where you
    can navigate on the left panel to the experiment named `ts-spark_ch4_data-ml-ops_time_series_prophet_notebook`.
    This experiment name seen in the UI comes from the code, which is highlighted
    in the preceding code.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开一个类似于*图4.4*的UI页面，在左侧面板中，你可以导航到名为`ts-spark_ch4_data-ml-ops_time_series_prophet_notebook`的实验。UI中看到的实验名称来自于前面的代码中的标记。
- en: The **Overview** tab for the experiment, shown in *Figure 4**.7*, has information
    about the experiment such as the creator, creation date, status, source code creating
    the experiment, and model logged from the experiment. It also shows the model
    parameters and metrics as logged in the code.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的**概览**标签，如*图4.7*所示，包含实验的信息，如创建者、创建日期、状态、创建实验的源代码，以及从实验中记录的模型。它还显示了代码中记录的模型参数和指标。
- en: '![](img/B18568_04_7.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_7.jpg)'
- en: 'Figure 4.7: MLflow experiment overview'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：MLflow实验概览
- en: The **Model metrics** tab, shown in *Figure 4**.8*, allows one to search and
    view the metrics graph.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型指标**标签，如*图4.8*所示，可以用来搜索并查看指标图。'
- en: '![](img/B18568_04_8.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_8.jpg)'
- en: 'Figure 4.8: MLflow model metrics'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：MLflow模型指标
- en: The initial screen of the **Artifacts** tab, shown in *Figure 4**.9*, shows
    the model schema, which we logged in the code as the signature. It also gives
    code examples of how to use the model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**Artifacts**标签页的初始界面，见*图 4.9*，显示了我们在代码中作为签名记录的模型架构。它还提供了如何使用模型的代码示例。'
- en: '![](img/B18568_04_9.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_9.jpg)'
- en: 'Figure 4.9: MLflow model schema'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：MLflow 模型架构
- en: The **MLmodel** section of the **Artifacts** tab, shown in *Figure 4**.10*,
    shows the model artifact with its path.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLmodel**部分位于**Artifacts**标签页中，显示了模型工件及其路径，见*图 4.10*。'
- en: '![](img/B18568_04_10.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_10.jpg)'
- en: 'Figure 4.10: MLflow model artifact'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：MLflow 模型工件
- en: This is as far as we will go with MLflow in this example. We will use MLflow
    in a similar way in the next example with an orchestrator and expand into further
    use of MLflow in [*Chapter 9*](B18568_09.xhtml#_idTextAnchor169), *Going to Production*.
    For now, we will be looking at other considerations with the notebook approach.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用MLflow到此为止。我们将在下一个示例中使用编排器和MLflow类似的方式，并在[**第9章**](B18568_09.xhtml#_idTextAnchor169)《走向生产》中深入探讨MLflow的更多应用。现在，我们将关注笔记本方法的其他考虑因素。
- en: Multiple notebooks
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多个笔记本
- en: 'The notebook example here is only a starting point that can be adapted and
    extended based on the requirements of your own use case, as well as the techniques
    that will be discussed in the following chapters. For more complex requirements,
    it is recommended to use separate notebooks for the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的笔记本示例仅是一个起点，可以根据自身用例的需求以及后续章节将讨论的技术进行适配和扩展。对于更复杂的需求，建议为以下内容使用单独的笔记本：
- en: Exploratory data analysis and data science
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索性数据分析与数据科学
- en: Feature engineering
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Model development, selection, and deployment of the best model
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型开发、选择及最佳模型的部署
- en: Production data pipeline, potentially including feature engineering as well
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产数据管道，可能还包括特征工程
- en: Production model inferencing
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产模型推理
- en: Monitoring
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控
- en: Model retraining
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型重训练
- en: While notebooks are great for their interactivity, collaborative ease, relative
    simplicity, and versatility, they have limitations, which we will cover in the
    next section.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管笔记本因其交互性、协作简便性、相对简单性和多功能性而备受青睐，但它们也存在限制，接下来的章节将对此进行讲解。
- en: Limitations
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制
- en: 'However good they are, notebooks for end-to-end time series analysis present
    several challenges. These are as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 无论笔记本有多优秀，端到端时间序列分析的笔记本都面临若干挑战，具体如下：
- en: No scheduling and orchestration capabilities. This makes it hard to go beyond
    simple sequential workflows and develop complex workflows.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏调度和编排功能。这使得它难以超越简单的顺序工作流，开发复杂的工作流。
- en: Scalability issue. The notebook code runs in the notebook kernel, which is limited
    to the resource of the single machine where it is located. Note that this can
    be resolved by submitting the task from the notebook to run on the Apache Spark
    cluster, as we have done in our example.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性问题。笔记本代码运行在笔记本内核中，受限于所在机器的资源。请注意，可以通过将任务从笔记本提交到Apache Spark集群中运行来解决此问题，正如我们在示例中所做的那样。
- en: Lack of error handling. If the code in a notebook cell fails, the whole workflow
    execution stops. It is, of course, possible to write error-handling code, but
    this adds additional coding effort.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏错误处理。如果笔记本单元格中的代码失败，整个工作流执行将停止。当然，可以编写错误处理代码，但这会增加额外的编码工作量。
- en: To answer these challenges, we will be considering another approach next, using
    an orchestrator.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，我们接下来将考虑另一种方法，使用编排器。
- en: Orchestrator approach
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编排器方法
- en: Before diving into the approach, let’s first understand what an orchestrator
    means. Airflow, which we will use here, was mentioned earlier as an example.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解这种方法之前，首先让我们理解什么是编排器。我们将在此使用的Airflow，之前作为示例提到过。
- en: An **orchestrator** plays a central role in managing workflows, including data
    engineering and processing. A workflow or pipeline is a set of computing tasks
    executed together in a certain order, in parallel or sequentially, usually with
    dependency on the outcome of the preceding task or tasks. In addition to scheduling
    the workflows, an orchestrator usually has features to author them before and
    monitor their execution post-scheduling.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**编排器**在管理工作流（包括数据工程和处理）中扮演着核心角色。工作流或管道是一组计算任务，这些任务按一定顺序并行或顺序执行，通常依赖于前一个任务或多个任务的结果。除了调度工作流外，编排器通常还具备在调度前创建工作流和在调度后监控其执行的功能。'
- en: Benefits of an orchestrator
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编排器的好处
- en: 'Using an orchestrator provides the following benefits over the limitations
    of the notebook-only approach:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 使用调度器相较于仅使用笔记本的方法提供了以下优点：
- en: Scheduling the tasks within the workflows, considering their dependencies and
    parallel or sequential execution requirements. This also includes conditional
    logic for task execution.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在工作流中调度任务，考虑任务的依赖关系以及并行或顺序执行的要求。这还包括任务执行的条件逻辑。
- en: Scalable and distributed task execution.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展和分布式任务执行。
- en: Monitoring and logging workflow execution, including performance and errors.
    This is crucial for production environments.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和记录工作流执行，包括性能和错误。这对于生产环境至关重要。
- en: Error handling and alerting with possibilities to retry, skip to the next task,
    or fail the entire pipeline. This is also a key requirement for production environments.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误处理和警报，提供重试、跳过到下一个任务或失败整个管道的可能性。这也是生产环境中的关键要求。
- en: Integration with other systems and tools. This is required to build end-to-end
    workflows, covering DataOps, ModelOps, and DevOps, which usually means working
    with different specialized tools.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他系统和工具的集成。这是构建端到端工作流的必要条件，涵盖DataOps、ModelOps和DevOps，这通常意味着需要使用不同的专业工具。
- en: Now that we have seen the benefits and have the environment set up with Airflow
    as the orchestrator, let’s get into the practice.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了这些优点，并且环境已设置好，Airflow作为调度器，现在让我们开始实践。
- en: Authoring the workflow
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建工作流
- en: The first step is to create the workflow or **direct acyclic graph** (**DAG**)
    as it is also called.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建工作流或**有向无环图**（**DAG**），也叫做DAG。
- en: 'If you followed the instructions in the earlier *Environment setup* section,
    the example DAG is already loaded and accessible directly within the Airflow UI,
    as shown in *Figure 4**.5*, at the following URL: `http://localhost:8080/`. At
    this point, you can jump to the next section to run the DAG or continue here for
    details on the DAG code.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遵循了之前*环境设置*部分中的说明，示例DAG已经加载并可以直接通过Airflow UI访问，如*图4.5*所示，网址为：`http://localhost:8080/`。此时，你可以跳到下一部分来运行DAG，或者继续查看DAG代码的详细信息。
- en: 'The DAG definition is in a Python code file in the `dags` folder and is also
    downloadable from the following GitHub location:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: DAG定义位于`dags`文件夹中的Python代码文件中，也可以从以下GitHub位置下载：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/dags/ts-spark_ch4_airflow-dag.py](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/dags/ts-spark_ch4_airflow-dag.py)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/dags/ts-spark_ch4_airflow-dag.py](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/dags/ts-spark_ch4_airflow-dag.py)'
- en: The core of the code is very similar to what we saw in the previous notebook
    example. This section focuses on integrating with Airflow and defining the DAG’s
    tasks, which are the individual steps of the DAG.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的核心与我们在之前笔记本示例中看到的非常相似。本节重点介绍与Airflow的集成以及定义DAG的任务，这些任务是DAG的各个步骤。
- en: Task definition – Python code
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务定义 – Python代码
- en: 'When the orchestrator runs the tasks, it calls the following corresponding
    Python functions as the underlying code that needs to be executed. Note the function
    parameters that are passed in and the return values. These are aligned with the
    task’s definition, which we will see next:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 当调度器运行任务时，它调用以下相应的Python函数作为需要执行的底层代码。注意传入的函数参数和返回值。这些与任务的定义对齐，我们将在接下来的部分看到：
- en: '`ingest_data` – for task `t1`. Note that `spark.read` will run on the Spark
    cluster:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ingest_data` – 对应任务`t1`。注意，`spark.read`将在Spark集群上运行：'
- en: '[PRE10]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`transform_data` – for task `t2`:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`transform_data` – 对应任务`t2`：'
- en: '[PRE11]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`train_and_log_model` – for task `t3`. Note that the MLflow functions, such
    as `mlflow.set_experiment` and `mlflow.prophet.log_model`, make calls to the MLflow
    server. A partial extract of the code is shown here:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train_and_log_model` – 对应任务`t3`。注意，MLflow函数，如`mlflow.set_experiment`和`mlflow.prophet.log_model`，会调用MLflow服务器。这里展示了代码的部分摘录：'
- en: '[PRE12]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`forecast` – for task `t4`. Note that `mlflow.prophet.load_model` loads the
    model from the MLflow server. This is done in this way here only to show how to
    retrieve the model from an MLflow server. It is not strictly required here as
    we could have kept the reference to the model locally:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`forecast` – 对应任务`t4`。注意，`mlflow.prophet.load_model`从MLflow服务器加载模型。这里只是为了展示如何从MLflow服务器检索模型。实际上并非严格需要，因为我们本可以在本地保留对模型的引用：'
- en: '[PRE13]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: These tasks are referenced by the DAG, which we will define next.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务由DAG引用，我们将在接下来定义它们。
- en: DAG definition
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DAG定义
- en: 'Overarching the preceding task definitions, we have the high-level Airflow
    DAG, which is defined as per the following:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的任务定义之上，我们有一个高层次的Airflow DAG，它按照以下定义：
- en: '[PRE14]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This points to `default_args`, which contains the following DAG parameters.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这指向`default_args`，其中包含以下DAG参数。
- en: '[PRE15]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Further information on these is available in the following Airflow documentation:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这些的更多信息可以在以下Airflow文档中找到：
- en: '[https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html#airflow.models.baseoperator.BaseOperator](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html#airflow.models.baseoperator.BaseOperator)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html#airflow.models.baseoperator.BaseOperator](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html#airflow.models.baseoperator.BaseOperator)'
- en: We have not set `schedule_interval` as we want to trigger the DAG manually from
    the Airflow UI.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有设置`schedule_interval`，因为我们希望通过Airflow UI手动触发DAG。
- en: DAG tasks
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DAG任务
- en: 'The DAG tasks are defined as per the following. Note the reference to `dag`
    and to the underlying Python function defined previously. The use of `PythonOperator`
    means that the tasks will be calling Python functions:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: DAG任务按以下方式定义。请注意引用了`dag`和先前定义的底层Python函数。使用`PythonOperator`意味着任务将调用Python函数：
- en: '`t1`:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`t1`：'
- en: '[PRE16]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`t2`:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`t2`：'
- en: '[PRE17]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notable for task `t2` is how the output from task `t1`, `t1.output`, is passed
    as input, `pdf`, to task `t2`.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务`t2`的一个显著特点是，任务`t1`的输出`t1.output`作为输入`pdf`传递给任务`t2`。
- en: '`t3`:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`t3`：'
- en: '[PRE18]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output from task `t2`, `t2.output`, is passed as input, `pdf`, to task `t3`.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务`t2`的输出`t2.output`作为输入`pdf`传递给任务`t3`。
- en: '`t4`:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`t4`：'
- en: '[PRE19]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The output from task `t3`, `t3.output`, is passed as input, `model_uri`, to
    task `t4`.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务`t3`的输出`t3.output`作为输入`model_uri`传递给任务`t4`。
- en: 'These tasks are then configured with the following code to be orchestrated
    sequentially by Airflow:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些任务通过以下代码配置，以便由Airflow按顺序进行协调：
- en: '[PRE20]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This concludes the workflow definition as a DAG in Airflow. The example here
    is only a starting point, with a simple sequential workflow that can be adapted
    and extended based on your specific requirements and the additional time series
    analysis tasks that will be discussed in the following chapters.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了在Airflow中定义工作流作为DAG的过程。这里的示例只是一个起点，展示了一个简单的顺序工作流，可以根据您的具体需求以及接下来章节中讨论的时间序列分析任务进行调整和扩展。
- en: Orchestrating notebooks
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 协调笔记本
- en: 'Note that it is also possible to combine the orchestrator and notebook approach
    by calling notebooks from Airflow tasks using the `PapermillOperator` operator.
    You can find more information on this operator here: [https://airflow.apache.org/docs/apache-airflow-providers-papermill/stable/operators.html](https://airflow.apache.org/docs/apache-airflow-providers-papermill/stable/operators.html).'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，还可以通过使用`PapermillOperator`操作符从Airflow任务调用笔记本，将协调器和笔记本方法结合起来。有关此操作符的更多信息，请访问：[https://airflow.apache.org/docs/apache-airflow-providers-papermill/stable/operators.html](https://airflow.apache.org/docs/apache-airflow-providers-papermill/stable/operators.html)。
- en: Once the DAG is written and placed in Airflow’s `dags` folder, it will be automatically
    picked up by Airflow, checked for syntax errors in the Python definition file,
    and then listed in the list of DAGs available to run, which we will cover next.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦DAG编写完成并放置在Airflow的`dags`文件夹中，它将被Airflow自动识别，检查Python定义文件中的语法错误，然后列出可以运行的DAG列表，接下来我们将介绍这一部分内容。
- en: Running the workflow
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行工作流
- en: The workflow can be launched by clicking on the run button (>) on the right
    of the DAG, as seen in *Figure 4**.5* of the *Accessing the UIs* section. By clicking
    on the DAG name on the left panel, the details and graph of the DAG can be viewed
    in the Airflow UI, as shown in *Figure 4**.11*.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过点击DAG右侧的运行按钮(>)来启动工作流，正如*图 4.5*中所示的*访问UI*部分。通过点击左侧面板中的DAG名称，可以在Airflow UI中查看DAG的详细信息和图形，如*图
    4.11*所示。
- en: '![](img/B18568_04_11.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_04_11.jpg)'
- en: 'Figure 4.11: Airflow DAG'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：Airflow DAG
- en: To view information on a specific run and task of the DAG, select the run on
    the left, and then the task from the graph. This will provide the option to view
    the execution log of the task.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看DAG中特定运行和任务的信息，请在左侧选择运行，然后从图形中选择任务。这将提供查看任务执行日志的选项。
- en: Another interesting piece of information is the execution time of the different
    tasks, which can be viewed from the **Gantt** tab on the same screen.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的信息是不同任务的执行时间，可以通过同一界面中的**甘特图**标签查看。
- en: We are only exploring the surface of Airflow here, which is a feature-rich tool
    beyond the scope of this book. Refer to the Airflow documentation for more information.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里只是在探索Airflow的表面，它是一个功能丰富的工具，超出了本书的范围。请参阅Airflow文档以获取更多信息。
- en: 'As was mentioned earlier, some of the code runs on the Apache Spark cluster.
    This can be visualized from the Spark master node, as per *Figure 3**.6* in [*Chapter
    3*](B18568_03.xhtml#_idTextAnchor063). The URL is the following: `http://localhost:8070/`.
    The Spark UI will show a running application if it is still running. This application
    is the Spark code launched from the Airflow tasks.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，部分代码在Apache Spark集群上运行。根据[*第3章*](B18568_03.xhtml#_idTextAnchor063)中的*图3.6*，可以从Spark主节点可视化这一过程。URL为：`http://localhost:8070/`。如果应用仍在运行，Spark
    UI将显示一个运行中的应用程序。这个应用程序是从Airflow任务中启动的Spark代码。
- en: 'As for MLflow, you can view the outcome in the MLflow UI at the following URL:
    `http://localhost:5001/`.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MLflow，你可以在以下URL中通过MLflow UI查看结果：`http://localhost:5001/`。
- en: From the MLflow UI page, similar to *Figure 4**.4*, you can navigate on the
    left panel to the experiment named `ts-spark_ch4_data-ml-ops_time_series_prophet`.
    This experiment name seen in the UI comes from the code, which is highlighted
    in the code for `train_and_log_model` previously.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLflow UI页面上，类似于*图4.4*，你可以在左侧面板中导航到名为`ts-spark_ch4_data-ml-ops_time_series_prophet`的实验。UI中看到的实验名称来自代码，在之前的`train_and_log_model`代码中有所高亮显示。
- en: '`This concludes the second approach discussed in this chapter. We will build
    on this orchestrator example using the concepts we learn in the upcoming chapters.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '`这就结束了本章讨论的第二种方法。我们将在接下来的章节中，使用我们学到的概念，基于这个编排器示例进行构建。'
- en: Environment shutdown
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境关闭
- en: 'We can now stop the container environment. The makefile provided simplifies
    the process with the following command:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以停止容器环境。提供的makefile简化了这个过程，使用以下命令：
- en: '[PRE21]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will give the following or equivalent output:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生如下或等效的输出：
- en: '[PRE22]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If you do not intend to use it further, you can go ahead and delete the Docker
    containers created with the `Delete` action, as explained here: [https://docs.docker.com/desktop/use-desktop/container/#container-actions](https://docs.docker.com/desktop/use-desktop/container/#container-actions).'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不打算进一步使用它，可以按照此处的说明，删除通过`Delete`操作创建的Docker容器：[https://docs.docker.com/desktop/use-desktop/container/#container-actions](https://docs.docker.com/desktop/use-desktop/container/#container-actions)。
- en: Summary
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we detailed the important phases of a time series analysis
    project, starting with the choice of a use case corresponding to a business requirement.
    The use case was then mapped to the technical solution, with DataOps, ModelOps,
    and DevOps components. We finally looked at two approaches for implementation,
    including examples of baseline implementations with an all-in-one notebook and
    with an orchestrator, which will be further extended in the rest of this book.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细介绍了时间序列分析项目的关键阶段，从选择与业务需求相对应的用例开始。接着，将该用例映射到技术解决方案，包括DataOps、ModelOps和DevOps组件。最后，我们探讨了两种实现方式，包括带有一体化笔记本和带有编排器的基准实现示例，这将在本书接下来的部分进一步扩展。
- en: In the following chapter, we will do just that, focusing on DataOps with data
    preparation.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将专注于此，重点是通过数据准备进行DataOps。
- en: Join our community on Discord
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/ds](https://packt.link/ds)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/ds](https://packt.link/ds)'
- en: '![](img/ds_(1).jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ds_(1).jpg)'
