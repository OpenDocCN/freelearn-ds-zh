- en: Using Spark SQL in Streaming Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在流应用中使用Spark SQL
- en: In this chapter, we will present typical use cases for using Spark SQL in streaming
    applications. Our focus will be on structured streaming using the Dataset/DataFrame
    APIs introduced in Spark 2.0\. Additionally, we will introduce and work with Apache
    Kafka, as it is an integral part of many web-scale streaming application architectures.
    Streaming applications typically involve real-time, context-aware responses to
    incoming data or messages. We will use several examples to illustrate the key
    concepts and techniques to build such applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍在流应用中使用Spark SQL的典型用例。我们的重点将放在使用Spark 2.0中引入的Dataset/DataFrame API进行结构化流处理上。此外，我们还将介绍并使用Apache
    Kafka，因为它是许多大规模网络流应用架构的重要组成部分。流应用通常涉及对传入数据或消息进行实时、上下文感知的响应。我们将使用几个示例来说明构建此类应用的关键概念和技术。
- en: 'In this chapter, we will learn these topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: What is a streaming data application?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是流数据应用？
- en: Typical streaming use cases
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型的流应用用例
- en: Using Spark SQL DataFrame/Dataset APIs to build streaming applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL DataFrame/Dataset API构建流应用
- en: Using Kafka in Structured Streaming applications
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在结构化流应用中使用Kafka
- en: Creating a receiver for a custom data source
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为自定义数据源创建接收器
- en: Introducing streaming data applications
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍流数据应用
- en: Traditional batch applications typically ran for hours, processing all or most
    of the data stored in relational databases. More recently, Hadoop-based systems
    have been used to support MapReduce-based batch jobs to process very large volumes
    of distributed data. In contrast, stream processing occurs on streaming data that
    is continuously generated. Such processing is used in a wide variety of analytics
    applications that compute correlations between events, aggregate values, sample
    incoming data, and so on.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的批处理应用通常运行数小时，处理存储在关系数据库中的所有或大部分数据。最近，基于Hadoop的系统已被用于支持基于MapReduce的批处理作业，以处理非常大量的分布式数据。相比之下，流处理发生在持续生成的流数据上。这种处理在各种分析应用中被使用，用于计算事件之间的相关性、聚合值、对传入数据进行抽样等。
- en: Stream processing typically ingests a sequence of data and incrementally computes
    statistics and other functions on a record-by-record/event-by-event basis, or
    over sliding time windows, on the fly.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理通常会逐步计算统计数据和其他功能，以记录/事件为基础，或者在滑动时间窗口上进行实时计算。
- en: Increasingly, streaming data applications are applying machine learning algorithms
    and **Complex Event Processing** (**CEP**) algorithms to provide strategic insights
    and the ability to quickly and intelligently react to rapidly changing business
    conditions. Such applications can scale to handle very high volumes of streaming
    data and respond appropriately on a real-time basis. Additionally, many organizations
    are implementing architectures containing both a real-time layer and a batch layer.
    In such implementations, it is very important to maintain a single code base,
    as far as possible, for these two layers (for examples such architectures, refer
    [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c), *Spark SQL
    in Large-Scale Application Architectures*). Spark Structured Streaming APIs helps
    us achieve such objectives in a scalable, reliable, and fault-tolerant manner.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的流数据应用正在应用机器学习算法和复杂事件处理（CEP）算法，以提供战略洞察和快速、智能地对快速变化的业务条件做出反应。这类应用可以扩展以处理非常大量的流数据，并能够实时做出适当的响应。此外，许多组织正在实施包含实时层和批处理层的架构。在这种实现中，尽可能地保持这两个层的单一代码库非常重要（有关此类架构的示例，请参阅[第12章](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c)，*大规模应用架构中的Spark
    SQL*）。Spark结构化流API可以帮助我们以可扩展、可靠和容错的方式实现这些目标。
- en: Some of the real-world uses cases for streaming applications include processing
    of sensor data in IoT applications, stock market applications such as risk management
    and algorithmic trading, network monitoring, surveillance applications, in-the-moment
    customer engagement in e-commerce applications, fraud detection, and so on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 流应用的一些真实用例包括处理物联网应用中的传感器数据、股票市场应用（如风险管理和算法交易）、网络监控、监视应用、电子商务应用中的即时客户参与、欺诈检测等。
- en: As a result, many platforms have emerged that provide the infrastructure needed
    to build streaming data applications, including Apache Kafka, Apache Spark Streaming,
    Apache Storm, Amazon Kinesis Streams, and others.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，许多平台已经出现，提供了构建流数据应用所需的基础设施，包括Apache Kafka、Apache Spark Streaming、Apache Storm、Amazon
    Kinesis Streams等。
- en: In this chapter, we will explore stream processing using Apache Spark and Apache
    Kafka. Over the next few sections, we will explore Spark Structured Streaming
    in detail using Spark SQL DataFrame/Dataset APIs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨使用Apache Spark和Apache Kafka进行流处理。在接下来的几节中，我们将使用Spark SQL DataFrame/Dataset
    API详细探讨Spark结构化流。
- en: Building Spark streaming applications
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建Spark流应用
- en: In this section, we will primarily focus on the newly introduced structured
    streaming feature (in Spark 2.0). Structured streaming APIs are GA with Spark
    2.2 and using them is the preferred method for building streaming Spark applications.
    Several updates to Kafka-based processing components including performance improvements
    have also been released in Spark 2.2\. We introduced structured streaming in [Chapter
    1](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c), *Getting Started with
    Spark SQL*. In this chapter, we will get deeper into the topic and present several
    code examples to showcase its capabilities.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将主要关注新引入的结构化流特性（在Spark 2.0中）。结构化流API在Spark 2.2中已经是GA，并且使用它们是构建流式Spark应用的首选方法。Spark
    2.2还发布了对基于Kafka的处理组件的多个更新，包括性能改进。我们在[第1章](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c)，*开始使用Spark
    SQL*中介绍了结构化流，本章中我们将深入探讨这个主题，并提供几个代码示例来展示其能力。
- en: As a quick recap, structured streaming provides a fast, scalable, fault-tolerant,
    end-to-end exactly-once stream processing without the developer having to reason
    about the underlying streaming mechanisms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，结构化流提供了一种快速、可扩展、容错、端到端的精确一次流处理，而开发人员无需考虑底层的流处理机制。
- en: It is built on the Spark SQL engine, and the streaming computations can be expressed
    in the same way batch computations are expressed on static data. It provides several
    data abstractions including Streaming Query, Streaming Source, and Streaming Sink
    to simplify streaming applications without getting into the underlying complexities
    of data streaming. Programming APIs are available in Scala, Java, and Python,
    and you can use the familiar Dataset / DataFrame API to implement your applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它建立在Spark SQL引擎上，流计算可以以与静态数据上的批处理计算相同的方式来表达。它提供了几种数据抽象，包括流查询、流源和流接收器，以简化流应用程序，而不涉及数据流的底层复杂性。编程API在Scala、Java和Python中都可用，您可以使用熟悉的Dataset
    / DataFrame API来实现您的应用程序。
- en: In [Chapter 1](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c), *Getting
    Started with Spark SQL*, we used the IPinYou Dataset to create a streaming DataFrame
    and then defined a streaming query on it. We showed the results getting updated
    in each time interval. Here, we recreate our streaming DataFrame, and then execute
    various functions on it to showcase the types of computations possible on the
    streaming input data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c)中，*开始使用Spark SQL*，我们使用IPinYou数据集创建了一个流DataFrame，然后在其上定义了一个流查询。我们展示了结果在每个时间间隔内得到更新。在这里，我们重新创建我们的流DataFrame，然后在其上执行各种函数，以展示在流输入数据上可能的计算类型。
- en: 'First, we start the Spark shell and import the necessary classes required for
    the hands-on part of this chapter. We will be using file sources to simulate the
    incoming data in most of our examples:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们启动Spark shell，并导入本章实际操作所需的必要类。在我们的大多数示例中，我们将使用文件源来模拟传入的数据：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we define the schema for the bid records in our source files, as shown:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为源文件中的出价记录定义模式，如下所示：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the next step, we will define a streaming data source based on the input
    CSV file. We specify the schema defined in the previous step and other required
    parameters (using options). We also limit the number of files processed to one
    per batch:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将基于输入的CSV文件定义一个流数据源。我们指定在上一步中定义的模式和其他必需的参数（使用选项）。我们还将每批处理的文件数量限制为一个：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can print the schema of the streaming DataFrame as you would in the case
    of a static one:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像在静态数据的情况下一样打印流DataFrame的模式：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Implementing sliding window-based functionality
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现基于滑动窗口的功能
- en: In this subsection, we will cover the sliding window operation on streaming
    data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将介绍对流数据进行滑动窗口操作。
- en: 'As the timestamp data is not in the correct format, we will define a new column
    and convert the input timestamp string to the right format and type for our processing:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于时间戳数据格式不正确，我们将定义一个新列，并将输入时间戳字符串转换为适合我们处理的正确格式和类型：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we will define a streaming query that writes the output to the standard
    output. We will define aggregations over a sliding window where we group the data
    by window and city ID, and compute the count of each group.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个流查询，将输出写入标准输出。我们将在滑动窗口上定义聚合，其中我们按窗口和城市ID对数据进行分组，并计算每个组的计数。
- en: For a more detailed description of structured streaming programming, refer to [http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html.](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有关结构化流编程的更详细描述，请参阅[http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html.](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- en: 'Here, we count the number of bids within 10-minute windows updating every five
    minutes, that is, bids received in 10-minutes windows sliding every five minutes.
    The streaming query using a window is as shown:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算在10分钟的窗口内的出价数量，每五分钟更新一次，也就是说，在每五分钟滑动一次的10分钟窗口内收到的出价。使用窗口的流查询如下所示：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is written to the standard output because we used a `Console Sink`
    as specified by the `console` keyword in the format parameter. The output contains
    columns for the window, the city ID, and the computed counts, as follows. We see
    two batches as we have placed two files in our input directory:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出写入标准输出，因为我们在格式参数中使用了`console`关键字指定了`Console Sink`。输出包含窗口、城市ID和计算的计数列，如下所示。我们看到了两批数据，因为我们在输入目录中放置了两个文件：
- en: '![](img/00170.jpeg)![](img/00171.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00170.jpeg)![](img/00171.jpeg)'
- en: Joining a streaming Dataset with a static Dataset
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将流数据集与静态数据集进行连接
- en: 'In this subsection, we will give an example of joining a streaming Dataset
    with a static one. We will join Datasets based on the `cityID` to achieve user-friendly
    output that contains the city name instead of the `cityID`. First, we define a
    schema for our city records and create static DataFrame from the CSV file containing
    the city IDs and their corresponding city names:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将举例说明如何将流数据集与静态数据集进行连接。我们将基于`cityID`来连接数据集，以实现包含城市名称而不是`cityID`的用户友好输出。首先，我们为我们的城市记录定义一个模式，并从包含城市ID及其对应城市名称的CSV文件创建静态DataFrame：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will join the streaming and the static DataFrames, as shown:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将连接流和静态DataFrame，如下所示：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will execute our previous streaming query with the column for city names
    specified instead of city IDs in the joined DataFrame:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行我们之前的流查询，指定城市名称的列，而不是连接的DataFrame中的城市ID：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The results are as follows. Here, we see a single batch of output data, as
    we have removed one of the input files from our source directory. For the rest
    of this chapter, we will limit processing to a single input file to conserve space:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下。在这里，我们看到了一批输出数据，因为我们已经从源目录中删除了一个输入文件。在本章的其余部分，我们将限制处理为单个输入文件，以节省空间：
- en: '![](img/00172.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00172.jpeg)'
- en: 'Next, we create a new  DataFrame with a timestamp column and a few selected
    columns from a previously created DataFrame:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个带有时间戳列和从先前创建的DataFrame中选择的几列的新DataFrame：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we are not computing aggregations, and simply want the streaming bids to
    be appended to the results, we use the `outputMode` "append" instead of "complete",
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不计算聚合，并且只是希望将流式出价附加到结果中，因此我们使用`outputMode`"append"而不是"complete"，如下所示：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/00173.jpeg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00173.jpeg)'
- en: Using the Dataset API in Structured Streaming
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用结构化流中的数据集API
- en: So far, we have used untyped APIs with DataFrames. In order to use typed APIs,
    we can switch from using DataFrames to Datasets. Most streaming operations are
    supported by the DataFrame/Dataset APIs; however, a few operations such as multiple
    streaming aggregations and distinct operations not supported, yet. And others
    such as outer JOINs and sorting are conditionally supported.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用了与DataFrame不同的未类型化API。为了使用类型化API，我们可以从使用DataFrame切换到使用数据集。大多数流式操作都受到DataFrame/Dataset
    API的支持；但是，一些操作，如多个流式聚合和不支持的不同操作，尚不受支持。而其他操作，如外连接和排序，是有条件支持的。
- en: For a complete list of unsupported and conditionally supported operations, refer
    to [http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有关不受支持和有条件支持的操作的完整列表，请参阅[http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)。
- en: Here, we present a few examples of using typed APIs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了一些使用类型化API的示例。
- en: 'First, we will define a `case` class called `Bid`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个名为`Bid`的`case`类：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can define a streaming Dataset from a streaming DataFrame using the `case`
    class defined in the previous step:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在前一步中定义的`case`类，从流式DataFrame中定义一个流式数据集：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Using output sinks
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用输出sink
- en: You can direct the streaming output data to various output sinks including File,
    Foreach, Console, and Memory sinks. Typically, the Console and Memory sinks are
    used for debugging purposes. As we have already used Console sink in earlier sections;
    here we will discuss the usage of other sinks in more detail.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将流式输出数据定向到各种输出sink，包括文件、Foreach、控制台和内存sink。通常，控制台和内存sink用于调试目的。由于我们已经在之前的部分中使用了控制台sink，因此我们将更详细地讨论其他sink的用法。
- en: Using the Foreach Sink for arbitrary computations on output
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Foreach Sink进行输出上的任意计算
- en: 'If you want to perform arbitrary computations on the output, then you can use
    the `Foreach` Sink. For this purpose, you will need to implement the `ForeachWriter`
    interface, as shown. In our example, we simply print the record, but you can also
    perform other computations, as per your requirements:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想对输出执行任意计算，那么可以使用`Foreach` Sink。为此，您需要实现`ForeachWriter`接口，如所示。在我们的示例中，我们只是打印记录，但您也可以根据您的要求执行其他计算：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the next step, we will implement an example using the `Foreach` sink defined
    in the previous step. Specify the `ForeachWriter` implemented in the preceding
    step as shown:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将实现一个示例，使用在上一步中定义的`Foreach` sink。如下所示，指定在前一步中实现的`ForeachWriter`：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As a result, the user-agent information is displayed as shown:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将显示用户代理信息，如下所示：
- en: '![](img/00174.gif)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00174.gif)'
- en: Using the Memory Sink to save output to a table
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用内存Sink将输出保存到表
- en: 'If you want to save the output data as a table, you can use the Memory Sink;
    this can be useful for interactive querying. We define a streaming DataFrame as
    before. However, we specify the format parameter as `memory` and the table name.
    Finally, we execute a SQL query on our table, as shown:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想将输出数据保存为表，可以使用内存Sink；这对于交互式查询很有用。我们像以前一样定义一个流式DataFrame。但是，我们将格式参数指定为`memory`，并指定表名。最后，我们对我们的表执行SQL查询，如下所示：
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/00175.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00175.jpeg)'
- en: Using the File Sink to save output to a partitioned table
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用文件sink将输出保存到分区表
- en: 'We can also save the output as partitioned tables. For instance, we can partition
    the output by time and store them as Parquet files on HDFS. Here, we show an example
    of storing our output to Parquet files using a File Sink. It is mandatory to specify
    the checkpoint directory location in the given command:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将输出保存为分区表。例如，我们可以按时间对输出进行分区，并将其存储为HDFS上的Parquet文件。在这里，我们展示了使用文件sink将输出存储为Parquet文件的示例。在给定的命令中，必须指定检查点目录位置：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can check the HDFS filesystem for the output Parquet files and the checkpoint
    files, as shown:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查HDFS文件系统，查看输出Parquet文件和检查点文件，如下所示：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00176.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00176.jpeg)'
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/00177.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00177.jpeg)'
- en: In the next section, we will explore some useful features for managing and monitoring
    streaming queries.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探索一些有用的功能，用于管理和监视流式查询。
- en: Monitoring streaming queries
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监视流式查询
- en: 'At this stage, if you list the active streaming queries in the system, you
    should see the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，如果您列出系统中的活动流查询，您应该会看到以下输出：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can also monitor and manage a specific streaming query, for example, the
    `windowedCounts` query (a `StreamingQuery` object), as shown:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以监视和管理特定的流式查询，例如`windowedCounts`查询（一个`StreamingQuery`对象），如下所示：
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/00178.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00178.jpeg)'
- en: 'To stop the streaming query execution, you can execute the `stop()` command,
    as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止流式查询执行，您可以执行`stop()`命令，如下所示：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the next section, we will shift our focus using Kafka as the source of incoming
    data streams in our structured streaming applications.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把重点转移到使用Kafka作为结构化流应用程序中传入数据流的来源。
- en: Using Kafka with Spark Structured Streaming
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kafka与Spark结构化流
- en: Apache Kafka is a distributed streaming platform. It enables you to publish
    and subscribe to data streams, and process and store them as they get produced.
    Kafka’s widespread adoption by the industry for web-scale applications is because
    of its high throughput, low latency, high scalability, high concurrency, reliability,
    and fault-tolerance features.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka是一个分布式流平台。它使您能够发布和订阅数据流，并在其产生时处理和存储它们。Kafka被业界广泛采用于面向Web规模应用程序，因为它具有高吞吐量、低延迟、高可伸缩性、高并发性、可靠性和容错特性。
- en: Introducing Kafka concepts
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Kafka概念
- en: Kafka is typically used to build real-time streaming data pipelines to move
    data between systems, reliably, and also to transform and react to the streams
    of data. Kafka is run as a cluster on one or more servers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka通常用于构建实时流数据管道，可在系统之间可靠地移动数据，还可对数据流进行转换和响应。Kafka作为一个或多个服务器上的集群运行。
- en: 'Some of the key concepts of Kafka are described here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述了Kafka的一些关键概念：
- en: '**Topic**: High-level abstraction for a category or stream name to which messages
    are published. A topic can have `0`, `1`, or many consumers who subscribe to the
    messages published to it. Users define a new topic for each new category of messages.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题**：用于发布消息的类别或流名称的高级抽象。一个主题可以有`0`、`1`或多个订阅其发布的消息的消费者。用户为每个新类别的消息定义一个新主题。'
- en: '**Producers**: Clients that publish messages to a topic.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产者**：向主题发布消息的客户端。'
- en: '**Consumers**: Clients that consume messages from a topic.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者**：从主题中消费消息的客户端。'
- en: '**Brokers**: One or more servers where message data is replicated and persisted.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Broker**：一个或多个服务器，用于复制和持久化消息数据。'
- en: Additionally, the producers and consumers can simultaneously write to and read
    from multiple topics. Each Kafka topic is partitioned and messages written to
    each partition are sequential. The messages in the partitions have an offset that
    uniquely identifies each message within the partition.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，生产者和消费者可以同时写入和读取多个主题。每个Kafka主题都被分区，写入每个分区的消息是顺序的。分区中的消息具有唯一标识每条消息的偏移量。
- en: The reference site for Apache Kafka installation, tutorials, and examples is
    [https://kafka.apache.org/](https://kafka.apache.org/).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka安装、教程和示例的参考网站是[https://kafka.apache.org/](https://kafka.apache.org/)。
- en: The partitions of a topic are distributed and each Broker handles requests for
    a share of the partitions. Each partition is replicated across a configurable
    number of Brokers. The Kafka cluster retains all published messages for a configurable
    period of time. Apache Kafka uses Apache ZooKeeper as a coordination service for
    its distributed processes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 主题的分区是分布的，每个Broker处理一部分分区的请求。每个分区在可配置数量的Broker上复制。Kafka集群保留所有发布的消息一段可配置的时间。Apache
    Kafka使用Apache ZooKeeper作为其分布式进程的协调服务。
- en: Introducing ZooKeeper concepts
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍ZooKeeper概念
- en: ZooKeeper is a distributed, open-source coordination service for distributed
    applications. It relieves the developer from having to implement coordination
    services from scratch. It uses a shared hierarchical namespace to allow distributed
    processes to coordinate with each other and avoid errors related to race conditions
    and deadlocks.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper是一个分布式的开源协调服务，用于分布式应用程序。它使开发人员不必从头开始实现协调服务。它使用共享的分层命名空间，允许分布式进程相互协调，并避免与竞争条件和死锁相关的错误。
- en: The reference site for Apache ZooKeeper installation and tutorials is [https://zookeeper.apache.org/](https://zookeeper.apache.org/).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Apache ZooKeeper安装和教程的参考网站是[https://zookeeper.apache.org/](https://zookeeper.apache.org/)。
- en: ZooKeeper data is kept in memory and, hence, it has very high throughput and
    low latency. It is replicated over a set of hosts to provide high availability.
    ZooKeeper provides a set of guarantees, including sequential consistency and atomicity.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper数据保存在内存中，因此具有非常高的吞吐量和低延迟。它在一组主机上复制，以提供高可用性。ZooKeeper提供一组保证，包括顺序一致性和原子性。
- en: Introducing Kafka-Spark integration
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Kafka-Spark集成
- en: 'We present a simple example here to familiarize you with Kafka-Spark integration.
    The environment for this section uses: Apache Spark 2.1.0 and Apache Kafka 0.10.1.0
    (Download file: `kafka_2.11-0.10.1.0.tgz)` .'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提供一个简单的示例，以使您熟悉Kafka-Spark集成。本节的环境使用：Apache Spark 2.1.0和Apache Kafka 0.10.1.0（下载文件：`kafka_2.11-0.10.1.0.tgz)`。
- en: 'First, we start a single-node ZooKeeper using the script provided with Apache
    Kafka distribution, as shown:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用Apache Kafka分发提供的脚本启动单节点ZooKeeper，如下所示：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After the Zookeeper node is up and running, we start the Kafka server using
    the script available in the Apache Kafka distribution, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Zookeeper节点启动后，我们使用Apache Kafka分发中提供的脚本启动Kafka服务器，如下所示：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we create a topic called `test`, to which we will send messages for Spark
    streaming to consume. For our simple example, we specify both the replication
    factor and the number of partitions as `1`. We can use the utility script available
    for this purpose, as shown:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个名为“test”的主题，我们将向其发送消息以供Spark流处理。对于我们的简单示例，我们将复制因子和分区数都指定为“1”。我们可以使用为此目的提供的实用脚本，如下所示：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can see the list of topics (including “test”) using this script:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此脚本查看主题列表（包括“test”）：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we start a command line-based producer to send messages to Kafka, as
    follows. Here, each line is sent as a separate message. You should see each line
    appear in your Spark streaming query (running in a different window) as you type
    it and hit enter (as illustrated):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们启动一个基于命令行的生产者来向Kafka发送消息，如下所示。在这里，每行都作为单独的消息发送。当您输入并按下回车时，您应该在Spark流查询中看到每行出现（在不同的窗口中运行）。
- en: '[PRE26]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In a separate window, start Spark shell with the appropriate Kafka packages
    specified in the command line, as shown:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个单独的窗口中，启动Spark shell，并在命令行中指定适当的Kafka包，如下所示：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After Spark shell starts up, we will create a streaming Dataset with the format
    specified as "kafka". In addition, we will also specify the Kafka server and the
    port it’s running on, and explicitly subscribe to the topic we created earlier,
    as follows. The key and value fields are cast to String type to make the output
    human-readable:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Spark shell启动后，我们将创建一个格式指定为"kafka"的流式数据集。此外，我们还将指定Kafka服务器和其运行的端口，并明确订阅我们之前创建的主题，如下所示。键和值字段被转换为字符串类型，以使输出易于阅读。
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we will start a streaming query that outputs the streaming Dataset to
    the standard output, as shown:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将启动一个流式查询，将流式数据集输出到标准输出，如下所示：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You should see the following output as you type sentences in the Kafka producer
    window:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在Kafka生产者窗口中输入句子时，您应该看到以下输出：
- en: '![](img/00179.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00179.jpeg)'
- en: Introducing Kafka-Spark Structured Streaming
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Kafka-Spark结构化流
- en: 'Then, we will provide another example of Kafka-Spark Structured Streaming,
    where we direct the contents of a iPinYou bids file to a producer, as demonstrated:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将提供另一个Kafka-Spark结构化流的示例，其中我们将iPinYou竞价文件的内容定向到生产者，如下所示：
- en: '[PRE30]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We will also create a new topic, called `connect-test`, a new streaming Dataset
    that contains the records from the file, and a new streaming query that lists
    them on the screen, as shown:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将创建一个名为`connect-test`的新主题，一个包含文件记录的新流式数据集，以及一个在屏幕上列出它们的新流式查询，如下所示：
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The truncated output is given here. The records are spread across multiple
    batches as they stream in:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 截断的输出如下所示。记录分布在多个批次中，因为它们在流中传输：
- en: '![](img/00180.gif)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00180.gif)'
- en: In the next section, we will create a receiver for accessing an arbitrary streaming
    data source.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将创建一个用于访问任意流式数据源的接收器。
- en: Writing a receiver for a custom data source
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为自定义数据源编写接收器
- en: So far, we have worked with data sources that have built-in support available
    in Spark. However, Spark Streaming can receive data from any arbitrary source,
    but we will need to implement a receiver for receiving data from the custom data
    source.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用了在Spark中内置支持的数据源。但是，Spark流式处理可以从任意源接收数据，但我们需要实现一个接收器来从自定义数据源接收数据。
- en: In this section, we will define a custom data source for public APIs available
    from the **Transport for London** (**TfL**) site. This site makes a unified API
    available for each mode of transportation in London. These APIs provide access
    to real-time data, for instance, rail arrivals. The output is available in the
    XML and JSON formats. We will use the APIs for current arrival predictions of
    London underground on a specific line.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为来自**伦敦交通**（TfL）网站提供的公共API定义一个自定义数据源。该网站为伦敦的每种交通方式提供了统一的API。这些API提供对实时数据的访问，例如，铁路到达情况。输出以XML和JSON格式提供。我们将使用API来获取伦敦地铁特定线路的当前到达预测。
- en: The reference site for TfL is [https://tfl.gov.uk](https://tfl.gov.uk); register
    on this site to generate an application key for accessing the APIs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: TfL的参考网站是[https://tfl.gov.uk](https://tfl.gov.uk); 在该网站上注册以生成用于访问API的应用程序密钥。
- en: 'We will start by extending the abstract class `Receiver` and implementing the
    `onStart()` and `onStop()` methods. In the `onStart()` method, we start the threads
    responsible for receiving the data and in `onStop()`, we stop these threads. The
    `receive` method receives the stream of data using a HTTP client, as shown:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先扩展抽象类`Receiver`并实现`onStart()`和`onStop()`方法。在`onStart()`方法中，我们启动负责接收数据的线程，在`onStop()`中，我们停止这些线程。`receive`方法使用HTTP客户端接收数据流，如下所示：
- en: '[PRE32]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The following object creates the `StreamingContext` and starts the application.
    The `awaitTermination()` method ensures that the application runs continuously.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下对象创建了`StreamingContext`并启动了应用程序。`awaitTermination()`方法确保应用程序持续运行。
- en: 'You can terminate the application using *Ctrl *+ *C *:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用*Ctrl *+ *C *来终止应用程序：
- en: '[PRE33]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `sbt` file used for compiling and packing the application is listed here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 用于编译和打包应用程序的`sbt`文件如下所示：
- en: '[PRE34]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We use the `spark-submit` command to execute our application, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`spark-submit`命令来执行我们的应用程序，如下所示：
- en: '[PRE35]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output from the streaming program is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 流式程序的输出如下所示：
- en: '![](img/00181.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00181.jpeg)'
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced streaming data applications. We provided a few
    examples of using Spark SQL DataFrame/Dataset APIs to build streaming applications.
    Additionally, we showed the use of Kafka in structured streaming applications.
    Finally, we presented an example of creating a receiver for a custom data source.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了流式数据应用程序。我们提供了使用Spark SQL DataFrame/Dataset API构建流式应用程序的几个示例。此外，我们展示了Kafka在结构化流应用程序中的使用。最后，我们提供了一个为自定义数据源创建接收器的示例。
- en: In the next chapter, we will shift our focus to using Spark SQL in machine learning
    applications. Specifically, we will explore key concepts of feature engineering
    and machine learning pipelines.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把重点转移到在机器学习应用中使用Spark SQL。具体来说，我们将探索特征工程和机器学习流水线的关键概念。
