- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Introduction to Transfer Learning and Pre-Trained Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习与预训练模型简介
- en: Just as one wouldn’t try to reinvent the wheel, in the world of data science
    and **machine learning** (**ML**), it’s often more efficient to build upon existing
    knowledge. This is where the concepts of **transfer learning** (**TL**) and pre-trained
    models come into play, two incredibly important tools in a data scientist’s repertoire.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 就像没人会重新发明轮子一样，在数据科学和**机器学习**（**ML**）领域，通常更高效的做法是基于现有知识进行构建。这就是**迁移学习**（**TL**）和预训练模型的概念发挥作用的地方，这两者是数据科学家工具库中不可或缺的强大工具。
- en: TL is almost like a shortcut in ML. Instead of taking a model architecture that
    has never seen data before, such as a Logistic Regression model or a Random Forest
    model, imagine being able to take a model trained on one task and then repurposing
    it for a different, yet related task. That’s TL in a nutshell – leveraging existing
    knowledge to learn new things more efficiently. It’s a concept that echoes throughout
    many facets of life and is a key technique in data science.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TL几乎像是机器学习中的快捷方式。与其使用一个从未接触过数据的模型架构，例如逻辑回归模型或随机森林模型，不如想象一下能够将一个在某个任务上训练过的模型，重新用于另一个不同但相关的任务。这就是TL的本质——利用现有的知识以更高效的方式学习新事物。这一概念贯穿于生活的许多方面，是数据科学中的一项关键技术。
- en: Pre-trained models are off-the-shelf components, ready to be used right out
    of the box. They’re ML models that have been trained on large datasets, capturing
    an immense amount of information about the task they were trained on. When it
    comes to tackling new tasks, these pre-trained models provide a substantial head
    start. The importance of TL and pre-trained models cannot be overstated in modern
    ML. They are fundamental to many cutting-edge applications in data science, from
    **computer vision** (**CV**) tasks such as image recognition to **natural language
    processing** (**NLP**) tasks such as **sentiment analysis** (**SA**). By leveraging
    these techniques, we can achieve impressive results even with limited data or
    resources.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型是现成的组件，可以立即使用。它们是已在大数据集上进行训练的机器学习模型，捕获了关于它们所训练任务的海量信息。面对新任务时，这些预训练模型提供了一个巨大的起步优势。在现代机器学习中，TL和预训练模型的重要性不容忽视。它们是数据科学中许多前沿应用的基础，从**计算机视觉**（**CV**）任务（如图像识别）到**自然语言处理**（**NLP**）任务（如**情感分析**（**SA**））。通过利用这些技术，我们即使在数据或资源有限的情况下，也能取得令人印象深刻的成果。
- en: 'In the following sections, we will investigate these concepts, exploring their
    nuances, applications, and potential:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨这些概念，探索它们的细微差别、应用和潜力：
- en: Understanding pre-trained models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解预训练模型
- en: Different types of TL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的TL
- en: TL with **Bidirectional Encoder Representations from Transformers** (**BERT**)
    and **Generative Pre-trained** **Transformer** (**GPT**)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TL与**双向编码器表示的转换器**（**BERT**）和**生成预训练****变换器**（**GPT**）
- en: We’ll also walk through practical examples that highlight their power in real-world
    scenarios. By the end of this chapter, you’ll have a solid understanding of TL
    and pre-trained models and be equipped to harness their potential in your own
    projects.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将通过实际示例来讲解它们在实际场景中的强大作用。通过本章内容，您将深入理解TL和预训练模型，并能够在自己的项目中利用它们的潜力。
- en: Understanding pre-trained models
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解预训练模型
- en: Pre-trained models are like learning from the experience of others. These models
    have been trained on extensive datasets, learning patterns, and features that
    make them adept at their tasks. Think of it as if a model has been reading thousands
    of books on a subject, absorbing all that information. When we use a pre-trained
    model, we’re leveraging all that prior knowledge.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型就像是从他人的经验中学习。这些模型已在大量数据集上进行过训练，学习了使其能够胜任任务的模式和特征。可以把它想象成一个模型读了成千上万本关于某一主题的书籍，吸收了所有这些信息。当我们使用预训练模型时，我们实际上是在利用这些先前的知识。
- en: In general, pre-training steps are not necessarily “useful” to a human, but
    it is crucial to a model to simply learn about a domain and about a medium. Pre-training
    helps models learn how language works in general but not how to classify sentiments
    or detect an object.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，预训练步骤对人类来说并不“有用”，但对于模型而言，它是学习领域和媒介的关键。预训练帮助模型理解语言是如何运作的，但并不涉及如何分类情感或检测物体。
- en: Benefits of using pre-trained models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练模型的好处
- en: The benefits of using pre-trained models are numerous. For starters, they save
    us a lot of time. Training a model from scratch can be a time-consuming process,
    but using a pre-trained model gives us a head start. Furthermore, these models
    often lead to better performance, especially when our dataset is relatively small.
    The reason? Pre-trained models have seen much more data than we usually have at
    our disposal, and they’ve learned a lot from it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型的好处有很多。首先，它们能为我们节省大量时间。从头开始训练一个模型可能是一个耗时的过程，但使用预训练模型可以让我们提前开始。此外，这些模型通常能带来更好的性能，尤其是在数据集相对较小的情况下。原因是，预训练模型已经看过比我们通常可以使用的更多的数据，并从中学到了很多。
- en: '*Figure 12**.1* shows the result of a study done on **large language models**
    (**LLMs**) such as BERT where one of the goals was to show that pre-training was
    leading to some obvious patterns in how BERT was recognizing basic grammatical
    constructs. The study visualized that models post pre-training were able to recognize
    what we would consider as obvious grammatical patterns, such as pronoun-antecedent
    relationships and direct object/verb relationships:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.1* 显示了一项关于 **大型语言模型** (**LLMs**) 的研究结果，例如 BERT，其中一个目标是展示预训练如何导致 BERT
    在识别基本语法结构时出现一些明显的模式。该研究可视化地表明，经过预训练的模型能够识别我们认为是显而易见的语法模式，如代词-先行词关系和直接宾语/动词关系：'
- en: '![Figure 12.1 – A study visualizing how BERT’s pre-training allowed it to pick
    up on common grammatical constructs without ever being told what they were](img/B19488_12_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.1 – 一项研究，展示了 BERT 预训练如何让它识别常见的语法结构，尽管它从未被告知这些结构是什么](img/B19488_12_01.jpg)'
- en: Figure 12.1 – A study visualizing how BERT’s pre-training allowed it to pick
    up on common grammatical constructs without ever being told what they were
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – 一项研究，展示了 BERT 预训练如何让它识别常见的语法结构，尽管它从未被告知这些结构是什么
- en: BERT, of course, is not the only model that undergoes pre-training, and this
    practice is not even limited to text-based models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 当然不是唯一一个进行预训练的模型，这种做法甚至不限于基于文本的模型。
- en: Commonly used pre-trained models
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常用的预训练模型
- en: Pre-trained models come in all shapes and sizes, each tailored to different
    types of data and tasks. Let’s talk about some of the most popular ones.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型有各种形状和尺寸，每种都针对不同类型的数据和任务量身定制。让我们来谈谈其中一些最受欢迎的模型。
- en: Image-based models
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于图像的模型
- en: 'For tasks related to images, models such as the Vision Transformer (more on
    this one later in this chapter), models from the **Visual Geometry Group** (**VGG**)
    (as seen in *Figure 12**.2*), and ResNet are some common options to choose from.
    Models from these families have been trained on tens of thousands of images, learning
    to recognize everything from shapes and textures to complex objects. They’re incredibly
    versatile and can be fine-tuned for a wide array of image-based tasks:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与图像相关的任务，像 Vision Transformer（本章稍后会介绍）这样的模型，**视觉几何组**（**VGG**）的模型（如在 *图 12.2*
    中所示），以及 ResNet 是一些常见的选择。这些模型已经在成千上万的图像上进行过训练，学会了从形状和纹理到复杂物体的识别。它们非常通用，可以为各种基于图像的任务进行微调：
- en: "![Figure 12.2 – The VGG16 model (from the VGG) is a convolutional neural network\
    \ (CNN) that can be pre-trained on ima\uFEFFge data](img/B19488_12_02.jpg)"
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.2 – VGG16 模型（来自 VGG）是一个卷积神经网络（CNN），可以在图像数据上进行预训练](img/B19488_12_02.jpg)'
- en: Figure 12.2 – The VGG16 model (from the VGG) is a convolutional neural network
    (CNN) that can be pre-trained on image data
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – VGG16 模型（来自 VGG）是一个卷积神经网络（CNN），可以在图像数据上进行预训练
- en: Text-based models
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于文本的模型
- en: 'When it comes to text, models such as BERT (*Figure 12**.3*) and GPT are among
    the most used language models. They were first originally architected in 2018
    (only 1 year after the primary Transformer architecture that both GPT and BERT
    are based on was even proposed or mentioned), and they’ve, as with their image
    counterparts, been trained on vast amounts of text data, learning the intricacies
    of human language. Whether it’s understanding the sentiment behind a tweet or
    answering questions about a piece of text, these models are up to the task. As
    we move forward, we’ll see how these pre-trained models can be combined with TL
    to tackle new tasks with impressive efficiency:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到文本时，像BERT（*图12.3*）和GPT这样的模型是最常用的语言模型之一。它们最初是在2018年设计的（仅在GPT和BERT都基于的基础Transformer架构被提出或提及后1年），并且像它们的图像对应物一样，已经在大量文本数据上进行训练，学习了人类语言的复杂性。无论是理解推文背后的情感，还是回答关于某段文本的问题，这些模型都能胜任任务。随着我们不断前进，我们将看到这些预训练模型如何与TL结合，凭借出色的效率应对新的任务：
- en: '![Figure 12.3 – A figure from the original blog post from Google open sourcing
    BERT in 2018 calls out OpenAI’s GPT-1 model](img/B19488_12_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3 – 来自谷歌2018年开源BERT原始博客文章中的一张图片，强调了OpenAI的GPT-1模型](img/B19488_12_03.jpg)'
- en: Figure 12.3 – A figure from the original blog post from Google open sourcing
    BERT in 2018 calls out OpenAI’s GPT-1 model
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – 来自谷歌2018年开源BERT原始博客文章中的一张图片，强调了OpenAI的GPT-1模型
- en: This came out a few months prior, highlighting BERT’s ability to process more
    relationships between tokens with the relatively same number of parameters than
    GPT.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片是在几个月前发布的，突出了BERT在相对相同数量的参数下，比GPT处理更多词汇之间关系的能力。
- en: Decoding BERT’s pre-training
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码BERT的预训练
- en: 'One of the most impressive feats of TL can be observed in BERT, a pre-trained
    model that revolutionized the NLP landscape. Two fundamental training tasks drive
    BERT’s robust understanding of language semantics and relationships: **masked
    language modeling** (**MLM**) and **next sentence prediction** (**NSP**). Let’s
    break them down and see how each one contributes to BERT’s language processing
    abilities.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: TL最令人印象深刻的成就之一可以从BERT中看到，BERT是一个革命性地改变了NLP领域的预训练模型。两个基本的训练任务推动了BERT对语言语义和关系的深刻理解：**掩码语言建模**（**MLM**）和**下一句预测**（**NSP**）。让我们分解一下，看看每个任务如何促进BERT的语言处理能力。
- en: MLM
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLM
- en: '**MLM** (visualized in *Figure 12**.4*) is a key component of BERT’s pre-training
    process. In essence, MLM works by randomly replacing approximately 15% of the
    words in the input data with a special (*MASK*) token. It’s then up to BERT to
    figure out which word was replaced, essentially filling in the blank. Think of
    it as a sophisticated game of *Mad Libs* that BERT plays during its training.
    If we were to take a sentence such as “Stop at the light,” MLM might replace “light”
    with (*MASK*), prompting BERT to predict the missing word:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLM**（*见图12.4*）是BERT预训练过程中的关键组成部分。本质上，MLM通过随机将输入数据中大约15%的单词替换为一个特殊的（*MASK*）标记来工作。然后，BERT需要弄清楚哪个单词被替换了，实质上是填补空白。可以把它看作是BERT在训练过程中玩的一种复杂的*填词游戏*。举个例子，假设我们有一句话“Stop
    at the light”，MLM可能会将“light”替换为（*MASK*），并促使BERT预测缺失的单词：'
- en: '![Figure 12.4 – The MLM pre-training task has BERT filling in missing tokens
    from a sequence of tokens](img/B19488_12_04.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图12.4 – MLM预训练任务让BERT从一个词序列中填补缺失的标记](img/B19488_12_04.jpg)'
- en: Figure 12.4 – The MLM pre-training task has BERT filling in missing tokens from
    a sequence of tokens
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 – MLM预训练任务让BERT从一个词序列中填补缺失的标记
- en: This process of MLM helps BERT understand the context around each word and construct
    meaningful relationships between different parts of a sentence.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MLM这一过程帮助BERT理解每个单词周围的上下文，并构建句子不同部分之间的有意义关系。
- en: NSP
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NSP
- en: '**NSP** (visualized in *Figure 12**.5*) is the second crucial part of BERT’s
    pre-training regimen. NSP is a binary classification problem, where BERT is tasked
    with determining whether a provided sentence *B* follows sentence *A* in the original
    text. In the grand scheme of language understanding, this is like asking BERT
    to understand the logical flow of sentences in a piece of text. This ability to
    predict whether sentence *B* logically follows sentence *A* allows BERT to understand
    more nuanced, higher-level linguistic structures and narrative flows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**NSP**（在*图12.5*中可视化）是BERT预训练方案的第二个关键部分。NSP是一个二分类问题，BERT的任务是判断提供的句子*B*是否在原文中紧跟句子*A*。从语言理解的宏观角度来看，这就像是在要求BERT理解一段文本中句子的逻辑流。这种预测句子*B*是否合乎逻辑地跟随句子*A*的能力，使得BERT能够理解更加微妙的、更高层次的语言结构和叙事流：'
- en: '![Figure 12.5 – In the NSP pre-training task, BERT is looking at two thoughts
    and deciding if the second phrase would come directly after the first phrase](img/B19488_12_05.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图12.5 – 在NSP预训练任务中，BERT正在查看两个思想，并判断第二个短语是否紧接着第一个短语](img/B19488_12_05.jpg)'
- en: Figure 12.5 – In the NSP pre-training task, BERT is looking at two thoughts
    and deciding if the second phrase would come directly after the first phrase
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 – 在NSP预训练任务中，BERT正在查看两个思想，并判断第二个短语是否紧接着第一个短语
- en: 'Put another way, MLM helps BERT get a grasp on intricate connections between
    words and their contexts, while NSP equips BERT with an understanding of the relationships
    between sentences. It’s this combination that makes BERT such a powerful tool
    for a range of NLP tasks. Between NSP and MLM (*Figure 12**.6*), BERT’s training
    is meant to give it a sense of how tokens affect phrase meanings (MLM) and how
    phrases work together to form larger thoughts (NSP):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，MLM帮助BERT理解单词及其上下文之间复杂的关系，而NSP则使BERT能够理解句子之间的关系。正是这种组合使得BERT成为执行各种NLP任务的强大工具。在NSP和MLM（*图12.6*）之间，BERT的训练旨在让它理解词元如何影响短语含义（MLM），以及短语如何协同工作以形成更大的思想（NSP）：
- en: "![Figure 12.6 – BERT’s pre-training helps it to learn about la\uFEFFnguage\
    \ in general](img/B19488_12_06.jpg)"
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图12.6 – BERT的预训练帮助它学习语言的基本知识](img/B19488_12_06.jpg)'
- en: Figure 12.6 – BERT’s pre-training helps it to learn about language in general
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 – BERT的预训练帮助它学习语言的基本知识
- en: TL
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TL
- en: 'Now that we have a strong grasp of pre-trained models, let’s shift our attention
    toward the other compelling facet of this equation: **TL**. In essence, TL is
    the application of knowledge gained from one problem domain (source) to a different
    but related problem domain (target).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对预训练模型有了较强的理解，让我们把注意力转向这个方程式的另一个引人注目的方面：**TL**。本质上，TL是将从一个问题领域（源）获得的知识应用到另一个但相关的问题领域（目标）。
- en: The process of TL
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TL的过程
- en: TL is all about adaptability. It takes a model that’s been trained on one task
    and adapts it to perform a different but related task. They might not have solved
    that particular mystery before, but their skills and experience can be adapted
    to the task at hand. TL is particularly useful when we have a small amount of
    data for our specific task or when our task is very similar to the one the original
    model was trained on. In these situations, TL can save time and resources while
    boosting our model’s performance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: TL的核心在于适应性。它将一个已经在某个任务上训练过的模型，调整到执行一个不同但相关的任务。它们可能之前没有解决过这个特定的谜题，但它们的技能和经验可以被调整到当前的任务上。TL在我们为特定任务拥有少量数据，或者我们的任务与原始模型训练时的任务非常相似时特别有用。在这些情况下，TL可以节省时间和资源，同时提高我们模型的性能。
- en: Different types of TL
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同类型的TL
- en: Let’s take a moment to get familiar with the diverse landscape of TL. It’s not
    a single monolithic idea, but rather a collection of varied strategies that fall
    under one umbrella term. There’s a type of TL for just about every scenario you
    might come across.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间了解一下TL的多样性。它不是一个单一的整体概念，而是多种不同策略的集合，这些策略都属于同一个umbrella term。几乎每种你可能遇到的场景都有一种TL类型。
- en: Inductive TL
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归纳式TL
- en: First up, we have **inductive TL** (**ITL**). This is all about using what’s
    already been learned and applying it in new, but related, scenarios. The key here
    is the generalization of learned features from one task—let’s call this the source
    task—and then fine-tuning them to perform well on another task—the target task.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有**归纳式TL**（**ITL**）。这完全是关于利用已经学到的知识，并将其应用到新的但相关的场景中。这里的关键是将一个任务中学到的特征进行泛化——我们称之为源任务——然后对它们进行微调，以便在另一个任务——目标任务上表现良好。
- en: 'Imagine a model that’s spent its virtual lifetime learning from a broad text
    corpus, getting to grips with the complexities of language, grammar, and context.
    Now, we have a different task on our hands: SA on product reviews. With ITL, our
    model can use what it’s already learned about language and fine-tune itself to
    become a pro at detecting sentiment. *Figure 12**.7* visualizes how we will approach
    ITL in a later section. The main idea is to take a pre-trained model from a model
    repository such as `HuggingFace` and perform any potential model modifications
    for our task, throw some labeled data at it (like we would any other ML model),
    and watch it learn:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，一个模型在广泛的文本语料库中度过了它的虚拟一生，掌握了语言、语法和语境的复杂性。现在，我们有一个新的任务：对产品评论进行情感分析（SA）。通过迁移学习（ITL），我们的模型可以利用它已经学到的语言知识，并通过微调使自己成为一个检测情感的专家。*图12.7*展示了我们如何在后续部分应用ITL。其核心思想是，从模型库（如`HuggingFace`）中取出一个预训练模型，对其进行任何可能的任务调整，投入一些带标签的数据（像我们对待任何其他机器学习模型一样），然后观察它如何学习：
- en: "![Figure 12.7 – An ITL process might involve training BERT on\uFEFF supervised\
    \ labeled data](img/B19488_12_07.jpg)"
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图12.7 – ITL过程可能涉及在带监督标签数据上训练BERT](img/B19488_12_07.jpg)'
- en: Figure 12.7 – An ITL process might involve training BERT on supervised labeled
    data
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 – ITL过程可能涉及在带监督标签数据上训练BERT
- en: Transductive TL
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传导性迁移学习（TTL）
- en: Our second type of TL is called **transductive TL** (**TTL**) and is a bit more
    nebulous in its task. Rather than being given a concrete second task to perform
    (such as classification), our model instead is asked to adapt to new data without
    losing its grounding in the original task. It’s a good choice when we have a bunch
    of unlabeled data for our target task.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二种迁移学习类型称为**传导性迁移学习（TTL）**，它的任务相对模糊。我们的模型并不是被要求执行一个具体的第二任务（例如分类），而是被要求在不失去对原任务基础的理解的情况下，适应新数据。当我们有大量未标注的目标任务数据时，这种方法是一个不错的选择。
- en: For example, if a model was trained on one image dataset to identify different
    objects and we have a new, unlabeled image dataset, we could ask the model to
    use its knowledge from the source task to label the new dataset, even without
    any explicit labels provided.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个模型在一个图像数据集上进行了训练，以识别不同的物体，而我们有一个新的未标注的图像数据集，我们可以要求模型利用从源任务中获得的知识来标注新的数据集，即使没有提供任何显式标签。
- en: Unsupervised TL – feature extraction
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督迁移学习（TL）——特征提取
- en: Pre-trained models aren’t just useful for their predictive abilities. They’re
    also treasure troves of features, ripe for extraction. Using **unsupervised TL**
    (**UTL**), our model, which was trained on a vast text corpus, can use its understanding
    of language to find patterns and help us divide the text into meaningful categories.
    Feature extraction with pre-trained models involves using a pre-trained model
    to transform raw data into a more useful format—one that highlights important
    features and patterns.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型不仅对其预测能力有用，它们也是特征的宝库，等待我们去挖掘。通过使用**无监督迁移学习（UTL）**，我们已经在庞大的文本语料库上训练过的模型，可以利用它对语言的理解，发现模式并帮助我们将文本划分为有意义的类别。使用预训练模型进行特征提取，涉及使用预训练模型将原始数据转化为更有用的格式——突出重要特征和模式。
- en: These are the three main flavors of TL, each with its own approach and ideal
    use cases. In the wide world of ML, it’s all about picking the right tool for
    the job, and TL definitely gives us a whole toolbox to choose from. With this
    newfound understanding of TL, we’re well equipped to start putting it into practice.
    In the next section, we’ll see how TL and pre-trained models can come together
    to conquer new tasks with ease.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种迁移学习方法是TL的主要类型，每种方法都有其独特的方式和理想的应用场景。在广阔的机器学习世界中，关键在于选择合适的工具，而迁移学习无疑为我们提供了一个完整的工具箱供我们选择。通过对迁移学习的这些新的理解，我们已做好充分准备，开始将其应用到实践中。在下一节中，我们将看到迁移学习和预训练模型如何结合在一起，轻松征服新任务。
- en: TL with BERT and GPT
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于BERT和GPT的迁移学习（TL）
- en: 'Having grasped the fundamental concepts of pre-trained models and TL, it’s
    time to put theory into practice. It’s one thing to know the ingredients; it’s
    another to know how to mix them into a delicious dish with them. In this section,
    we will take some models that have already learned a lot from their pre-training
    and fine-tune them to perform a new, related task. This process involves adjusting
    the model’s parameters to better suit the new task, much like fine-tuning a musical
    instrument:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了预训练模型和迁移学习（TL）的基本概念后，是时候将理论付诸实践了。知道食材是什么是一回事；知道如何将它们混合成一道美味的菜肴又是另一回事。在这一部分，我们将采用一些已经从预训练中学到了很多的模型，并对其进行微调，使其能够执行一个新的相关任务。这个过程涉及调整模型的参数，以更好地适应新任务，类似于调整乐器的音调：
- en: '![Figure 12.8 – ITL](img/B19488_12_08.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.8 – ITL](img/B19488_12_08.jpg)'
- en: Figure 12.8 – ITL
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8 – ITL
- en: ITL takes a pre-trained model that was generally trained on a semi-supervised
    (or unsupervised) task and then is given labeled data to learn a specific task.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ITL采用一个通常在半监督（或无监督）任务上训练的预训练模型，然后用标记数据学习一个特定任务。
- en: Examples of TL
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TL示例
- en: Let’s take a look at some examples of TL with specific pre-trained models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些具体预训练模型的TL示例。
- en: Example – Fine-tuning a pre-trained model for text classification
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 – 微调预训练模型进行文本分类
- en: Consider a simple text classification problem. Suppose we need to analyze customer
    reviews and determine whether they’re positive or negative. We have a dataset
    of reviews, but it’s not nearly large enough to train a **deep learning** (**DL**)
    model from scratch. We will fine-tune BERT on a text classification task, allowing
    the model to adapt its existing knowledge to our specific problem.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个简单的文本分类问题。假设我们需要分析客户评论，并判断它们是正面还是负面。我们有一个评论数据集，但它远远不足以从头开始训练一个**深度学习**（**DL**）模型。我们将微调BERT进行文本分类任务，使模型能够将其现有知识适应我们的特定问题。
- en: We will have to move away from the popular scikit-learn library to another popular
    library called `transformers`, which was created by HuggingFace (the pre-trained
    model repository I mentioned earlier) as `scikit-learn` does not (yet) support
    Transformer models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要从流行的scikit-learn库转向另一个流行的库，名为`transformers`，它是由HuggingFace（我之前提到的预训练模型库）创建的，因为`scikit-learn`（目前）不支持Transformer模型。
- en: '*Figure 12**.9* shows how we will have to take the original BERT model and
    make some minor modifications to it to perform text classification. Luckily, the
    `transformers` package has a built-in class to do this for us called `BertForSequenceClassification`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.9*展示了我们如何需要对原始BERT模型进行一些小的修改，以执行文本分类。幸运的是，`transformers`包有一个内置的类，名为`BertForSequenceClassification`，可以为我们完成这项工作：'
- en: '![Figure 12.9 – Simplest text classification case](img/B19488_12_09.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.9 – 最简单的文本分类情况](img/B19488_12_09.jpg)'
- en: Figure 12.9 – Simplest text classification case
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9 – 最简单的文本分类情况
- en: In many TL cases, we need to architect additional layers. In the simplest text
    classification case, we add a classification layer on top of a pre-trained BERT
    model so that it can perform the kind of classification we want.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多TL案例中，我们需要架构额外的层。在最简单的文本分类情况下，我们在预训练的BERT模型上添加一个分类层，以便它可以执行我们想要的分类任务。
- en: 'The following code block shows an end-to-end code example of fine-tuning BERT
    on a text classification task. Note that we are also using a package called `datasets`,
    also made by `HuggingFace`, to load a sentiment classification task from IMDb
    reviews. Let’s begin by loading up the dataset:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了在文本分类任务中微调BERT的端到端代码示例。请注意，我们还使用了一个名为`datasets`的包，它也是由`HuggingFace`开发的，用于从IMDb评论中加载情感分类任务。让我们首先加载数据集：
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With our dataset loaded up, we can run some training code to update our BERT
    model on our labeled data:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集后，我们可以运行一些训练代码，以在我们的标记数据上更新BERT模型：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once we have our saved model, we can use the following code to run the model
    against unseen data:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们保存了模型，我们可以使用以下代码对模型进行未见数据的推理：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Example – TL for image classification
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 – 图像分类的TL
- en: 'We could take a pre-trained model such as ResNet or the Vision Transformer
    (shown in *Figure 12**.10*), initially trained on a large-scale image dataset
    such as ImageNet. This model has already learned to detect various features from
    images, from simple shapes to complex objects. We can take advantage of this knowledge,
    fine-tuning the model on a custom image classification task:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用像ResNet或视觉Transformer（如*图 12.10*所示）这样的预训练模型，它最初是在像ImageNet这样的规模较大的图像数据集上训练的。这个模型已经学会了从图像中检测各种特征，从简单的形状到复杂的物体。我们可以利用这个知识，在自定义的图像分类任务上对模型进行微调：
- en: '![Figure 12.10 – The Vision Transformer](img/B19488_12_10.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.10 – 视觉Transformer](img/B19488_12_10.jpg)'
- en: Figure 12.10 – The Vision Transformer
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10 – 视觉Transformer
- en: The Vision Transformer is like a BERT model for images. It relies on many of
    the same principles, except instead of text tokens, it uses segments of images
    as “tokens” instead.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉Transformer就像是图像领域的BERT模型。它依赖于许多相同的原理，只不过它使用图像的片段作为“令牌”来代替文本令牌。
- en: The following code block shows an end-to-end code example of fine-tuning the
    Vision Transformer on an image classification task. The code should look very
    similar to the BERT code from the previous section because the aim of the `transformers`
    library is to standardize training and usage of modern pre-trained models so that
    no matter what task you are performing, they can offer a relatively unified training
    and inference experience.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了一个端到端的代码示例，用于在图像分类任务中微调Vision Transformer。该代码应该与上一节的BERT代码非常相似，因为`transformers`库的目标是标准化现代预训练模型的训练和使用，使得无论你执行什么任务，它们都能提供相对统一的训练和推理体验。
- en: Let’s begin by loading up our data and taking a look at the kinds of images
    we have (seen in *Figure 12**.11*). Note that we are only going to use 1% of the
    dataset to show that you really don’t need that much data to get a lot out of
    pre-trained models!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加载数据并查看我们所拥有的图像类型（见*图12.11*）。请注意，我们只会使用数据集的1%来证明其实你并不需要那么多数据就能从预训练模型中获得很大的收益！
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can similarly use the model using the following code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以类似地使用以下代码来使用模型：
- en: '![Figure 12.11 – A single example from CIFAR10 showing an airplane](img/B19488_12_11.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图12.11 – 来自CIFAR10的单一示例，显示一架飞机](img/B19488_12_11.jpg)'
- en: Figure 12.11 – A single example from CIFAR10 showing an airplane
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11 – 来自CIFAR10的单一示例，显示一架飞机
- en: 'Now, we can train our pre-trained Vision Transformer:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始训练我们预训练的Vision Transformer：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Our final model has about 95% accuracy on 1% of the test set. We can now use
    our new classifier on unseen images, as in this next code block:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终的模型在测试集的1%上达到了大约95%的准确率。现在我们可以在未见过的图像上使用我们的新分类器，如下一个代码块所示：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure 12**.12* shows the result of this single classification, and it looks
    like it did pretty well:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.12* 显示了这次单一分类的结果，看起来效果相当不错：'
- en: '![Figure 12.12 – Our classifier predicting a stock image of a plane correctly](img/B19488_12_12.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图12.12 – 我们的分类器正确预测了一张飞机的库存图像](img/B19488_12_12.jpg)'
- en: Figure 12.12 – Our classifier predicting a stock image of a plane correctly
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12 – 我们的分类器正确预测了一张飞机的库存图像
- en: With minimal labeled data, we can leverage TL to turn models off the shelf into
    powerhouse predictive models.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在最小化标注数据的情况下，我们可以利用迁移学习（TL）将现成的模型转化为强大的预测模型。
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Our exploration of pre-trained models gave us insight into how these models,
    trained on extensive data and time, provide a solid foundation for us to build
    upon. They help us overcome constraints related to computational resources and
    data availability. Notably, we familiarized ourselves with image-based models
    such as VGG16 and ResNet, and text-based models such as BERT and GPT, adding them
    to our repertoire.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对预训练模型的探索让我们了解了这些经过大量数据和时间训练的模型是如何为我们提供坚实的基础，以供进一步开发的。它们帮助我们克服了计算资源和数据可用性相关的限制。特别是，我们熟悉了像VGG16和ResNet这样的图像模型，以及像BERT和GPT这样的文本模型，进一步丰富了我们的工具库。
- en: Our voyage continued into the domain of TL, where we learned its fundamentals,
    recognized its versatile applications, and acknowledged its different forms—inductive,
    transductive, and unsupervised. Each type, with its unique characteristics, adds
    a different dimension to our ML toolbox. Through practical examples, we saw these
    concepts in action, applying a BERT model for text classification and a Vision
    Transformer for image classification.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的探索继续进入了迁移学习（TL）领域，在那里我们学习了其基本原理，认识到其多样化的应用，并了解了它的不同形式——归纳式、传导式和无监督式。每种类型都有其独特的特性，为我们的机器学习工具箱增添了不同的维度。通过实际的例子，我们看到了这些概念的实际应用，使用BERT模型进行文本分类，使用Vision
    Transformer进行图像分类。
- en: But, as we’ve come to appreciate, TL and pre-trained models, while powerful,
    are not the solution to all data science problems. They shine in the right circumstances,
    and it’s our role as data scientists to discern when and how to deploy these powerful
    methods effectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，正如我们已经认识到的那样，虽然迁移学习和预训练模型很强大，但并不是所有数据科学问题的解决方案。它们在合适的情况下大放异彩，而作为数据科学家的我们，需要判断何时以及如何有效地部署这些强大的方法。
- en: 'As this chapter concludes, the understanding we’ve gleaned of TL and pre-trained
    models not only equips us to handle real-world ML tasks but also paves the way
    for tackling more advanced concepts and techniques. By introducing more steps
    into our ML process (such as pre-training), we are opening ourselves up to more
    potential errors. Our next chapter will also begin to investigate a hidden side
    of TL: transferring bias and tackling drift.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章的结束，我们对迁移学习（TL）和预训练模型的理解，不仅使我们能够处理实际的机器学习（ML）任务，还为我们应对更高级的概念和技术铺平了道路。通过在我们的机器学习过程中引入更多步骤（如预训练），我们也在为更多潜在的错误敞开了大门。下一章将开始探讨迁移学习的一个隐藏面：偏差传递和漂移应对。
- en: Moving forward, consider diving deeper into other pre-trained models, investigating
    other variants of TL, or even attempting to create your own pre-trained models.
    Whichever path you opt for, remember—the world of ML is vast and always evolving.
    Stay curious and keep learning!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 向前看，可以考虑深入研究其他预训练模型，探索迁移学习的其他变种，甚至尝试创建你自己的预训练模型。无论你选择哪条路径，记住——机器学习的世界广阔且不断发展。保持好奇，持续学习！
