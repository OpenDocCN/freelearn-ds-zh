- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Algorithms and How to Apply Them
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法及其应用
- en: In this book, we have already looked at a variety of ways to create pandas data
    structures and select/assign data within them and have subsequently seen how to
    store those structures in common formats. These features alone can make pandas
    a powerful tool in the realm of data exchange, but we are still just scratching
    the surface of what pandas can offer.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经看过了多种创建pandas数据结构、选择/赋值数据以及将这些结构存储为常见格式的方法。这些功能单独来看已经足以使pandas在数据交换领域成为一个强大的工具，但我们仍然只是触及了pandas所能提供的一小部分。
- en: A core component of data analysis and computing in general is the application
    of *algorithms*, which describe a sequence of steps the computer should take to
    process data. In their simplistic form, common data algorithms build upon basic
    arithmetic (for example, “sum this column”), but scale out to any sequence of
    steps that you may need for your custom calculations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析和计算的核心组成部分是*算法*的应用，它描述了计算机处理数据时应采取的步骤序列。在简单的形式下，常见的数据算法基于基本的算术运算（例如，“对这一列求和”），但它们也可以扩展到你可能需要的任何步骤序列，以进行自定义计算。
- en: As you will see in this chapter, pandas provides many common data algorithms
    out of the box, but also gives you a robust framework through which you can compose
    and apply your own algorithms. The algorithms pandas provides out of the box would
    be faster than anything you can write by hand in Python, and as you progress in
    your data journey, you will usually find that clever use of these algorithms can
    cover a vast amount of data processing needs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在本章中看到的，pandas提供了许多常见的数据算法，但同时也为你提供了一个强大的框架，通过它你可以构建和应用自己的算法。pandas提供的这些算法通常比你在Python中手动编写的任何算法都要快，随着你在数据处理的旅程中不断进步，你会发现这些算法的巧妙应用可以涵盖大量的数据处理需求。
- en: 'We are going to cover the following recipes in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下几种方法：
- en: Basic `pd.Series` arithmetic
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的`pd.Series`算术运算
- en: Basic `pd.DataFrame` arithmetic
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的`pd.DataFrame`算术运算
- en: Aggregations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合
- en: Transformations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换
- en: Map
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 映射
- en: Apply
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用
- en: Summary statistics
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要统计
- en: Binning algorithms
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分箱算法
- en: One-hot encoding with `pd.get_dummies`
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pd.get_dummies`进行独热编码
- en: Chaining with `.pipe`
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`.pipe`进行链式操作
- en: Selecting the lowest-budget movies from the top 100
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从前100部电影中选择预算最低的电影
- en: Calculating a trailing stop order price
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算尾部止损订单价格
- en: Finding the baseball players best at…
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找棒球运动员最擅长…
- en: Understanding which position scores the most per team
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解每个团队中得分最高的位置
- en: Basic pd.Series arithmetic
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的pd.Series算术运算
- en: The easiest place to start when exploring pandas algorithms is with a `pd.Series`,
    given it is also the most basic structure provided by the pandas library. Basic
    arithmetic will cover the operations of addition, subtraction, multiplication,
    and division, and, as you will see in this section, pandas offers two ways to
    perform these. The first approach allows pandas to work with the `+`, `-`, `*`,
    and `/` operators built into the Python language, which is an intuitive way for
    new users coming to the library to pick up the tool. However, to cover features
    specific to data analysis not covered by the Python language, and to support the
    *Chaining with .pipe* approach that we will cover later in this chapter, pandas
    also offers `pd.Series.add`, `pd.Series.sub`, `pd.Series.mul`, and `pd.Series.div`,
    respectively.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 探索pandas算法的最佳起点是使用`pd.Series`，因为它是pandas库提供的最基本的数据结构。基本的算术运算包括加法、减法、乘法和除法，正如你将在本节中看到的，pandas提供了两种执行这些操作的方式。第一种方法允许pandas使用Python语言内置的`+`、`-`、`*`和`/`运算符，这对于初次接触该库的新用户来说是一种直观的学习工具。然而，为了涵盖Python语言未涵盖的数据分析特定功能，并支持稍后在本章中将介绍的*使用.pipe进行链式操作*方法，pandas还提供了`pd.Series.add`、`pd.Series.sub`、`pd.Series.mul`和`pd.Series.div`方法，分别对应着这些运算符。
- en: The pandas library goes to great lengths to keep its API consistent across all
    data structures, so you will see that the knowledge from this section can be easily
    transferred over to the `pd.DataFrame` structure, with the only difference being
    that a `pd.Series` is one-dimensional while a `pd.DataFrame` is two-dimensional.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: pandas库极力保持其API在所有数据结构中的一致性，因此你将会看到本节中的知识可以轻松地转移到`pd.DataFrame`结构中，唯一的区别是`pd.Series`是一维的，而`pd.DataFrame`是二维的。
- en: How to do it
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'Let’s create a simple `pd.Series` from a Python `range` expression:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Python的`range`表达式创建一个简单的`pd.Series`：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To establish terminology, let’s briefly consider an expression like `a + b`.
    In such an expression, we are using a *binary operator* (`+`). The term *binary*
    refers to the fact that you need to add two things together for this expression
    to make sense, that is, it wouldn’t make sense to just have an expression like
    `a +`. Those two “things” are technically considered *operands*; so, with `a +
    b`, we have a left operand of `a` and a right operand of `b`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确立术语，让我们简单地考虑一个像 `a + b` 这样的表达式。在这种表达式中，我们使用了一个 *二元操作符*（`+`）。术语 *二元* 是指你需要将两个东西加在一起才能使这个表达式有意义，也就是说，像
    `a +` 这样的表达式是不合逻辑的。这两个“东西”在技术上被视为 *操作数*；因此，在 `a + b` 中，我们有一个左操作数 `a` 和一个右操作数 `b`。
- en: With one of the operands being a `pd.Series`, the most basic algorithmic expression
    in pandas would encompass the other operand being a *scalar*, that is to say,
    just a single value. When that occurs, the scalar value is *broadcast* to each
    element of the `pd.Series` to apply the algorithm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当其中一个操作数是 `pd.Series` 时，pandas 中最基本的算法表达式会包含另一个操作数是 *标量*，也就是说，只有一个值。当发生这种情况时，标量值会被
    *广播* 到 `pd.Series` 的每个元素上，从而应用该算法。
- en: 'For example, if we wanted to add the number 42 to each and every element of
    our `pd.Series`, we could simply express that as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想将数字 42 加到 `pd.Series` 中的每一个元素，我们可以简单地这样表达：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The pandas library is able to take the addition expression and apply it to our
    `pd.Series` in a *vectorized* manner (i.e., the number 42 gets applied to all
    values at once without requiring users to resort to a `for` loop in Python).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 库能够以 *向量化* 方式处理加法表达式（即数字 42 会一次性应用到所有值上，而无需用户在 Python 中使用 `for` 循环）。
- en: 'Subtraction may be expressed naturally using the `-` operator:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 减法可以自然地用 `-` 操作符来表示：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Similarly, multiplication may be expressed with the `*` operator:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，乘法可以通过 `*` 操作符来表示：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'By now, you can probably surmise that division is expressed with the `/` operator:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能已经猜到，除法是用 `/` 操作符来表示的：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It is also perfectly valid for the two operands to be a `pd.Series`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 两个操作数都是 `pd.Series` 也是完全有效的：
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As mentioned in the introduction of this section, while the built-in Python
    operators are commonly used and viable in most cases, pandas still offers dedicated
    methods for `pd.Series.add`, `pd.Series.sub`, `pd.Series.mul`, and `pd.Series.div`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节介绍中所提到的，虽然内置的 Python 操作符在大多数情况下是常用且可行的，pandas 仍然提供了专门的方法，如 `pd.Series.add`、`pd.Series.sub`、`pd.Series.mul`
    和 `pd.Series.div`：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The advantage of `pd.Series.add` over the built-in operator is that it accepts
    an optional `fill_value=` argument to handle missing data:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.Series.add` 相较于内置操作符的优势在于，它接受一个可选的 `fill_value=` 参数来处理缺失数据：'
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Later in this chapter, you will also be introduced to chaining with `.pipe`,
    which chains most naturally with the pandas methods and not with the built-in
    Python operators.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本章稍后你还将接触到使用 `.pipe` 进行链式操作，这与 pandas 方法链式操作最为自然，而不是与内置的 Python 操作符链式操作。
- en: There’s more…
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: When both operands in your expression are `pd.Series` objects together, it is
    important to note that pandas will align on the row labels. This alignment behavior
    is considered a feature, but can also be surprising to newcomers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当表达式中的两个操作数都是 `pd.Series` 对象时，重要的是要注意，pandas 会对齐行标签。这种对齐行为被视为一种特性，但对新手来说可能会令人惊讶。
- en: 'To see why this matters, let’s start with two `pd.Series` objects that have
    an identical row index. When we try to add these together, we get a rather unsurprising
    result:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解为什么这很重要，我们先从两个具有相同行索引的 `pd.Series` 对象开始。当我们尝试将它们相加时，结果并不令人意外：
- en: '[PRE16]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'But what happens when the row index values are not identical? A simple case
    may involve adding two `pd.Series` objects together, where one `pd.Series` uses
    a row index that is a subset of the other. You can see this with `ser3` in the
    following code, which only has 2 values and uses the default `pd.RangeIndex` with
    values of `[0, 1]`. When added together with `ser1`, we still get a 3-element
    `pd.Series` in return, but values are only added where the row index labels can
    be aligned from both `pd.Series` objects:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 那么当行索引值不相同时，会发生什么呢？一个简单的例子是将两个 `pd.Series` 对象相加，其中一个 `pd.Series` 使用的行索引是另一个的子集。你可以通过以下代码中的
    `ser3` 来看到这一点，它只有两个值，并且使用默认的 `pd.RangeIndex`，值为 `[0, 1]`。当与 `ser1` 相加时，我们仍然得到一个包含三个元素的
    `pd.Series`，但只有当两个 `pd.Series` 对象的行索引标签能够对齐时，值才会被相加：
- en: '[PRE18]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now let’s take a look at what happens when two `pd.Series` objects of the same
    length get added together, but the row index values are different:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看当两个相同长度的`pd.Series`对象相加时会发生什么，但它们的行索引值不同：
- en: '[PRE20]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For an even more extreme case, let’s consider the situation where one `pd.Series`
    has row index values that are non-unique:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个更极端的例子，让我们考虑一个情况，其中一个`pd.Series`的行索引值是非唯一的：
- en: '[PRE22]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If you have a background in SQL, the behavior of pandas here is akin to a `FULL
    OUTER JOIN` in a database. Every label from each row index gets included in the
    output, with pandas matching up the labels that can be seen in both `pd.Series`
    objects. This can be directly replicated in a database like PostgreSQL:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有SQL的背景，pandas在这里的行为类似于数据库中的`FULL OUTER JOIN`。每个行索引的标签都会被包含在输出中，pandas会将可以在两个`pd.Series`对象中看到的标签进行匹配。这可以在像PostgreSQL这样的数据库中直接复制：
- en: '[PRE24]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you were to run this snippet directly in PostgreSQL, you would get back
    the following result:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你直接在PostgreSQL中运行这段代码，你将得到以下结果：
- en: '[PRE25]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Ignoring the ordering difference, you can see that the database gives us back
    all of the unique `index` values from the combinations of `[0, 1, 2]` and `[0,
    1, 1]`, alongside any associated `val1` and `val2` values. Even though `ser1`
    only had one `index` value of `1`, that same value appeared twice in the `index`
    column in `ser5`. The `FULL OUTER JOIN` therefore shows both `val2` values from
    `ser5` (`4` and `8`), while duplicating the `val1` value originating from `ser1`
    (`1`).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略顺序差异，你可以看到数据库返回了从`[0, 1, 2]`和`[0, 1, 1]`的组合中得到的所有唯一`index`值，以及任何相关的`val1`和`val2`值。尽管`ser1`只有一个`index`值为`1`，但这个值在`ser5`的`index`列中出现了两次。因此，`FULL
    OUTER JOIN`显示了来自`ser5`的两个`val2`值（`4`和`8`），同时重复了源自`ser1`的`val1`值（`1`）。
- en: 'If you were to subsequently add `val1` and `val2` together in the database,
    you would get back a result that matches the output of `ser1 + ser5`, sparing
    the fact that the database may choose a different order for its output:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你接着在数据库中将`val1`和`val2`相加，你将得到一个结果，该结果与`ser1 + ser5`的输出相匹配，唯一的区别是数据库可能会选择不同的输出顺序：
- en: '[PRE26]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Basic pd.DataFrame arithmetic
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的`pd.DataFrame`算术运算
- en: Having now covered basic `pd.Series` arithmetic, you will find that the corresponding
    `pd.DataFrame` arithmetic operations are practically identical, with the lone
    exception being that our algorithms now work in two dimensions instead of just
    one. In doing so, the pandas API makes it easy to interpret data regardless of
    its shape, and without requiring users to write loops to interact with data. This
    helps significantly reduce developer effort and helps you write faster code –
    a win-win for developers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了基本的`pd.Series`算术运算后，你会发现，相应的`pd.DataFrame`算术运算几乎是完全相同的，唯一的区别是我们的算法现在在二维数据上工作，而不仅仅是单维数据。这样，pandas
    API使得无论数据的形状如何，都能轻松地解释数据，而且无需用户编写循环来与数据交互。这大大减少了开发人员的工作量，帮助你编写更快的代码——对开发人员来说是双赢。
- en: How it works
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'Let’s create a small 3x3 `pd.DataFrame` using random numbers:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用随机数创建一个小的3x3`pd.DataFrame`：
- en: '[PRE28]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Much like a `pd.Series`, a `pd.DataFrame` also supports built-in binary operators
    with a scalar argument. Here is a simplistic addition operation:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`pd.Series`一样，`pd.DataFrame`也支持带有标量参数的内置二进制运算符。这里是一个简化的加法操作：
- en: '[PRE30]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And here is a simplistic multiplication operation:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简化的乘法操作：
- en: '[PRE32]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You can also perform arithmetic with a `pd.Series`. By default, each row label
    in the `pd.Series` is searched for and aligned against the columns of the `pd.DataFrame`.
    To illustrate, let’s create a small `pd.Series` whose index labels match the column
    labels of `df`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以对`pd.Series`执行算术运算。默认情况下，`pd.Series`中的每一行标签都会被查找并与`pd.DataFrame`的列进行对齐。为了说明这一点，让我们创建一个小的`pd.Series`，它的索引标签与`df`的列标签匹配：
- en: '[PRE34]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'If you were to try and add this to our `pd.DataFrame`, it would take the value
    of `col1` in the `pd.Series` and add it to every element in the `col1` column
    of the `pd.DataFrame`, repeating for each index entry:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试将其添加到我们的`pd.DataFrame`中，它将取`pd.Series`中的`col1`值并将其添加到`pd.DataFrame`中`col1`列的每个元素，针对每个索引条目重复执行：
- en: '[PRE36]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In cases where the row labels of the `pd.Series` do not match the column labels
    of the `pd.DataFrame`, you may end up with missing data:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pd.Series`的行标签与`pd.DataFrame`的列标签不匹配的情况下，你可能会遇到缺失数据：
- en: '[PRE38]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: If you would like to control how `pd.Series` and `pd.DataFrame` align, you can
    use the `axis=` parameter of methods like `pd.DataFrame.add`, `pd.DataFrame.sub`,
    `pd.DataFrame.mul`, and `pd.DataFrame.div`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望控制`pd.Series`和`pd.DataFrame`的对齐方式，可以使用像`pd.DataFrame.add`、`pd.DataFrame.sub`、`pd.DataFrame.mul`和`pd.DataFrame.div`等方法的`axis=`参数。
- en: 'Let’s see this in action by creating a new `pd.Series` using row labels that
    align better with the row labels of our `pd.DataFrame`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过创建一个新的`pd.Series`来查看这个过程，使用的行标签与我们`pd.DataFrame`的行标签更好地对齐：
- en: '[PRE40]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Specifying `df.add(ser, axis=0)` will match up the row labels from both the
    `pd.Series` and `pd.DataFrame`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 指定`df.add(ser, axis=0)`将会匹配`pd.Series`和`pd.DataFrame`中的行标签：
- en: '[PRE42]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You can also use two `pd.DataFrame` arguments as the operands of addition,
    subtraction, multiplication, and division. Here is how to multiply two `pd.DataFrame`
    objects together:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以将两个`pd.DataFrame`作为加法、减法、乘法和除法的操作数。以下是如何将两个`pd.DataFrame`对象相乘：
- en: '[PRE44]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Of course, when doing this, you still need to be aware of the index alignment
    rules – items are always aligned by label and not by position!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在执行此操作时，你仍然需要注意索引对齐规则——项目总是按标签对齐，而不是按位置对齐！
- en: 'Let’s create a new 3x3 `pd.DataFrame` with different row and column labels
    to show this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的3x3 `pd.DataFrame`，具有不同的行和列标签，以展示这一点：
- en: '[PRE46]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Attempting to add this to our previous `pd.DataFrame` will generate a row index
    with labels `["row1", "row2", "row3", 0, 1, 2]` and a column index with labels
    `["col1", "col2", "col3", 0, 1, 2]`. Because no labels could be aligned, everything
    comes back as a missing value:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试将其添加到我们之前的`pd.DataFrame`中，将生成一个行索引，标签为`["row1", "row2", "row3", 0, 1, 2]`，列索引，标签为`["col1",
    "col2", "col3", 0, 1, 2]`。因为无法对齐标签，所有数据都会返回缺失值：
- en: '[PRE48]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Aggregations
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合
- en: Aggregations (also referred to as *reductions*) help you to reduce multiple
    values from a series of values down to a single value. Even if the technical term
    is new to you, you have no doubt encountered many aggregations in your data journey.
    Things like the *count* of records, the *sum* or sales, or the *average* price
    are all very common aggregations.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合（也称为*归约*）帮助你将多个值从一个值的序列中减少为单个值。即使这个技术术语对你来说比较新，你无疑在数据过程中已经遇到过许多聚合。诸如记录的*计数*、*总和*或销售额、*平均*价格等，都是非常常见的聚合。
- en: In this recipe, we will explore many of the aggregations built into pandas,
    while also forming an understanding of how these aggregations are applied. Most
    analysis you will do throughout your data journey involves taking large datasets
    and aggregating the values therein into results that your audience can consume.
    Executives at most companies are not interested in receiving a data dump of transactions,
    they just want to know the sum, min, max, mean, and so on of values within those
    transactions. As such, effective use and application of aggregations is a key
    component to converting your complex data transformation pipelines into simple
    outputs that others can use and act upon.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将探索pandas内置的许多聚合方法，同时形成对这些聚合如何应用的理解。在你的数据旅程中，大多数分析都涉及将大型数据集进行聚合，转化为你的观众可以理解的结果。大多数公司高层并不感兴趣接收一大堆事务数据，他们只关心这些事务中数值的总和、最小值、最大值、平均值等。因此，有效地使用和应用聚合方法是将复杂的数据转换管道转化为他人可以使用和采取行动的简单输出的关键组成部分。
- en: How to do it
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: Many basic aggregations are implemented as methods directly on the `pd.Series`
    object, which makes it trivial to calculate commonly desired outputs like the
    `count`, `sum`, `max`, and so on.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基础聚合作为方法直接实现于`pd.Series`对象，这使得计算常见的输出（如`count`、`sum`、`max`等）变得非常简单。
- en: 'To kick off this recipe, let’s once again start with a `pd.Series` containing
    random numbers:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始这个食谱，我们再次从一个包含随机数的`pd.Series`开始：
- en: '[PRE50]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The pandas library provides methods for many commonly used aggregations, like
    `pd.Series.count`, `pd.Series.mean`, `pd.Series.std`, `pd.Series.min`, `pd.Series.max`,
    and `pd.Series.sum`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: pandas库提供了许多常用的聚合方法，如`pd.Series.count`、`pd.Series.mean`、`pd.Series.std`、`pd.Series.min`、`pd.Series.max`和`pd.Series.sum`：
- en: '[PRE51]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Instead of calling those methods directly, a more generic way to invoke these
    aggregations would be to use `pd.Series.agg`, providing the name of the aggregation
    you would like to perform as a string:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接调用这些方法不同，调用这些聚合方法的一个更通用的方式是使用`pd.Series.agg`，并将你想执行的聚合名称作为字符串传递：
- en: '[PRE53]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'An advantage using `pd.Series.agg` is that it can perform multiple aggregations
    for you. For example, if you wanted to calculate the minimum and maximum of a
    field in one step, you could do this by providing a list to `pd.Series.agg`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pd.Series.agg`的一个优点是它可以为你执行多个聚合操作。例如，如果你想要在一步中计算一个字段的最小值和最大值，你可以通过将一个列表传递给`pd.Series.agg`来实现：
- en: '[PRE55]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Aggregating a `pd.Series` is straightforward because there is only one dimension
    to be aggregated. With a `pd.DataFrame`, there are two possible dimensions to
    aggregate along, so you have a few more considerations as an end user of the library.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合`pd.Series`是直接的，因为只有一个维度需要聚合。对于`pd.DataFrame`来说，有两个可能的维度需要聚合，因此作为库的最终用户，你需要考虑更多因素。
- en: 'To walk through this, let’s go ahead and create a `pd.DataFrame` with random
    numbers:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这一点，让我们创建一个包含随机数的`pd.DataFrame`：
- en: '[PRE57]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'By default, invoking an aggregation using a built-in method like `pd.DataFrame.sum`
    will apply *along the columns*, meaning each column is individually aggregated.
    After that, pandas will display the result of each column’s aggregation as an
    entry in a `pd.Series`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，使用像`pd.DataFrame.sum`这样的内置方法进行聚合时，会*沿着列*进行操作，也就是说，每一列都会单独进行聚合。然后，pandas会将每一列的聚合结果显示为`pd.Series`中的一项：
- en: '[PRE59]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'If you would like to aggregate data in each row, you can specify the `axis=1`
    argument, with the caveat being that pandas is way more optimized for `axis=0`
    operations, so this has a chance of being *significantly slower* than aggregating
    columns. Even still, it is a rather unique feature of pandas that can be useful
    when performance is not the main concern:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要对每一行的数据进行聚合，可以指定`axis=1`参数，值得注意的是，pandas在`axis=0`操作上进行了更多优化，因此这可能比聚合列要*显著慢*。尽管如此，这是pandas的一个独特功能，当性能不是主要关注点时，它还是非常有用的：
- en: '[PRE61]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Much like a `pd.Series`, a `pd.DataFrame` has a `.agg` method, which can be
    used to apply multiple aggregations at once:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`pd.Series`一样，`pd.DataFrame`也有一个`.agg`方法，可以用于一次性应用多个聚合操作：
- en: '[PRE63]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: There’s more…
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'In the examples covered in the *How to do it* section, we passed functions
    as strings like `min` and `max` to `.agg`. This is great for simple functions,
    but for more complex cases, you can also pass in callable arguments. Each callable
    should accept a single argument `pd.Series` and reduce down to a scalar:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在*如何做到这一点*部分的例子中，我们将像`min`和`max`这样的函数作为字符串传递给`.agg`。对于简单的函数来说，这很好用，但对于更复杂的情况，你也可以传入可调用的参数。每个可调用对象应该接受一个`pd.Series`作为参数，并将其归约为标量：
- en: '[PRE65]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Transformations
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换
- en: Contrary to *aggregations*, transformations do not reduce an array of values
    to a single value but, rather, maintain the shape of the calling object. This
    particular recipe may seem rather mundane coming from the previous section on
    aggregations, but transformations and aggregations will end up being very complementary
    tools to calculate things like the “% total of group” later in the cookbook.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与*聚合*不同，变换不会将一组值压缩为单一值，而是保持调用对象的形状。这个特定的例子可能看起来很平凡，因为它来自于前一节的聚合内容，但变换和聚合最终会成为非常互补的工具，用于像“群体的百分比总和”之类的计算，这些将在后续的手册中展示。
- en: How to do it
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'Let’s create a small `pd.Series`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个小的`pd.Series`：
- en: '[PRE67]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Much like we saw with `pd.Series.agg` before, `pd.Series.transform` can accept
    a list of functions to apply. However, whereas `pd.Series.agg` expected these
    functions to return a single value, `pd.Series.transform` expects these functions
    to return a `pd.Series` with the same index and shape:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前在`pd.Series.agg`中看到的那样，`pd.Series.transform`也可以接受一个要应用的函数列表。然而，`pd.Series.agg`期望这些函数返回一个单一值，而`pd.Series.transform`期望这些函数返回一个具有相同索引和形状的`pd.Series`：
- en: '[PRE68]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Much like `pd.DataFrame.agg` would *aggregate* each column by default, `pd.DataFrame.transform`
    will *transform* each column by default. Let’s create a small `pd.DataFrame` to
    see this in action:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`pd.DataFrame.agg`默认会*聚合*每一列一样，`pd.DataFrame.transform`默认会*变换*每一列。让我们创建一个小的`pd.DataFrame`来看看这个过程：
- en: '[PRE70]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Sparing implementation details, calling something like `df.transform("abs")`
    will apply the absolute value function to each column individually before piecing
    back together the result as a `pd.DataFrame`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 抛开实现细节，像`df.transform("abs")`这样的调用将对每一列单独应用绝对值函数，然后将结果拼接回一个`pd.DataFrame`：
- en: '[PRE72]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'If you were to pass multiple transformation functions to `pd.DataFrame.transform`,
    you will end up with a `pd.MultiIndex`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将多个变换函数传递给`pd.DataFrame.transform`，你将得到一个`pd.MultiIndex`：
- en: '[PRE74]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: There’s more…
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: As mentioned in the introduction to this recipe, transformations and aggregations
    can work naturally together alongside the `GroupBy` concept, which will be covered
    in *Chapter 8*, *Group By*. In particular, our *Group by basics* recipe will be
    helpful to compare/contrast aggregations to transformations and will highlight
    how transformations can be used to expressively and succinctly calculate “percent
    of group” calculations.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本食谱介绍中提到的，转换和聚合可以与`GroupBy`概念自然地结合使用，这将在*第8章*中介绍，*分组方法*。特别是，我们的*分组基础*食谱将有助于比较和对比聚合与转换，并强调如何使用转换来简洁而富有表现力地计算“分组百分比”。
- en: Map
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 映射
- en: The `.agg` and `.transform` methods we have seen so far apply to an entire *sequence*
    of values at once. Generally, in pandas, this is a good thing; it allows pandas
    to perform *vectorized* operations that are fast and computationally efficient.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的`.agg`和`.transform`方法一次性作用于整个*值序列*。通常在pandas中，这是一个好事；它允许pandas执行*向量化*操作，速度快且计算高效。
- en: Still, sometimes, you as an end user may decide that you want to trade performance
    for customization or finer-grained control. This is where the `.map` methods can
    come into the picture; `.map` helps you apply functions individually to each element
    of your pandas object.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，有时，作为最终用户，你可能决定愿意牺牲性能以换取定制或更细粒度的控制。这时，`.map`方法可以派上用场；`.map`帮助你将函数逐一应用到pandas对象的每个元素。
- en: How to do it
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'Let’s assume we have a `pd.Series` of data that mixes together both numbers
    and lists of numbers:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含数字和数字列表混合的数据`pd.Series`：
- en: '[PRE76]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '`.agg` or `.transform` are not suitable here because we do not have a uniform
    data type – we really have to inspect each element to make a decision on how to
    handle it.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`.agg`或`.transform`在这里不适用，因为我们没有统一的数据类型——我们实际上需要检查每个元素，决定如何处理它。'
- en: 'For our analysis, let’s assume that when we encounter a number, we are happy
    to return the value as is. If we encounter a list of values, we want to average
    out all of the values within that list and return that. A function implementing
    this feature would look as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的分析，假设当我们遇到一个数字时，我们愿意直接返回该值。如果我们遇到一个值的列表，我们希望计算该列表中的所有值的平均值并返回它。实现这个功能的函数如下所示：
- en: '[PRE78]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We can then apply this to each element of our `pd.Series` using `pd.Series.map`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用`pd.Series.map`将其应用到`pd.Series`的每个元素：
- en: '[PRE79]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'If we had a `pd.DataFrame` containing this type of data, `pd.DataFrame.map`
    would be able to apply this function just as well:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个包含这种数据类型的`pd.DataFrame`，那么`pd.DataFrame.map`也能够很好地应用这个函数：
- en: '[PRE81]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: There’s more…
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'In the above example, instead of using `pd.Series.map`, you could have also
    used `pd.Series.transform`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，你也可以使用`pd.Series.transform`，而不是使用`pd.Series.map`：
- en: '[PRE85]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'However, you would *not* get the same results with `pd.DataFrame.transform`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你*不会*得到与`pd.DataFrame.transform`相同的结果：
- en: '[PRE87]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Why is this? Remember that `.map` explicitly applies a function to each element,
    regardless of if you are working with a `pd.Series` or `pd.DataFrame`. `pd.Series.transform`
    is also happy to apply a function to each element that it contains, but `pd.DataFrame.transform`
    essentially loops over each column and passes that column as an argument to the
    callable arguments.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样呢？记住，`.map`会明确地对每个元素应用一个函数，无论你是操作`pd.Series`还是`pd.DataFrame`。`pd.Series.transform`也很乐意对它包含的每个元素应用一个函数，但`pd.DataFrame.transform`本质上是遍历每一列，并将该列作为参数传递给可调用的函数。
- en: 'Because our function is implemented as:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的函数是这样实现的：
- en: '[PRE89]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'the `isinstance(value, list)` check fails when passed a `pd.Series` and you
    end up just returning the `pd.Series` itself. If we tweak our function slightly:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当传入一个`pd.Series`时，`isinstance(value, list)`检查会失败，结果你只是返回了`pd.Series`本身。如果我们稍微调整一下我们的函数：
- en: '[PRE90]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'then the behavior of `pd.DataFrame.transform` becomes more clear:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 那么`pd.DataFrame.transform`的行为就更清晰了：
- en: '[PRE91]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: While there may be conceptual overlap, generally, in your code, you should think
    of `.map` as working element-wise, whereas `.agg` and `.transform` will try as
    best as they can to work with larger sequences of data at once.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能存在概念上的重叠，但通常，在代码中，你应该把`.map`看作是逐元素操作，而`.agg`和`.transform`则尽可能一次性处理更大范围的数据序列。
- en: Apply
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: Apply is a commonly used method, to the point that I would argue it is *overused*.
    The `.agg`, `.transform`, and `.map` methods seen so far have relatively clear
    semantics (`.agg` reduces, `.transform` maintains shape, `.map` applies functions
    element-wise), but when you reach for `.apply`, you can mirror any of these. That
    flexibility may seem nice at first, but because `.apply` leaves it up to pandas
    to *do the right thing*, you are typically better off picking the most explicit
    methods to avoid surprises.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply`是一个常用的方法，我甚至认为它被*过度使用*。到目前为止，我们看到的`.agg`、`.transform`和`.map`方法都有相对明确的语义（`.agg`用于汇总，`.transform`保持形状不变，`.map`逐元素应用函数），但是当你使用`.apply`时，它几乎可以模拟所有这些功能。一开始，这种灵活性可能看起来很不错，但由于`.apply`让pandas来“做正确的事情”，通常你最好选择最明确的方法，以避免意外结果。'
- en: Even still, you will see a lot of code out in the wild (especially from users
    who did not read this book); so, understanding what it does and what its limitations
    are can be invaluable.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 即使如此，你仍然会在实际代码中看到很多代码（尤其是那些没有阅读这本书的用户写的代码）；因此，理解它的功能和局限性是非常有价值的。
- en: How to do it
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: Calling `pd.Series.apply` will make `.apply` act like `.map` (i.e., the function
    gets applied to each individual element of the `pd.Series`).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`pd.Series.apply`会使`.apply`像`.map`一样工作（即，函数应用于`pd.Series`的每个单独元素）。
- en: 'Let’s take a look at a rather contrived function that prints out each element:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个稍微有点牵强的函数，它会打印出每个元素：
- en: '[PRE93]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Funneling this through `.apply`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`.apply`来传递：
- en: '[PRE94]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'gives exactly the same behavior as `pd.Series.map`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 会得到与`pd.Series.map`完全相同的行为：
- en: '[PRE96]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '`pd.Series.apply` works like a Python loop, calling the function for each element.
    Because our function returns nothing, our resulting `pd.Series` is a like-indexed
    array of `None` values.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.Series.apply`的工作方式类似于Python循环，对每个元素调用函数。因为我们的函数没有返回任何值，所以我们得到的`pd.Series`是一个索引相同的`None`值数组。'
- en: 'Whereas `pd.Series.apply` works element-wise, `pd.DataFrame.apply` works across
    each column as a `pd.Series`. Let’s see this in action with a `pd.DataFrame` of
    shape `(3, 2)`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 而`pd.Series.apply`是逐元素应用的，`pd.DataFrame.apply`则按列工作，将每一列视为一个`pd.Series`。让我们用一个形状为`(3,
    2)`的`pd.DataFrame`来看看它的实际应用：
- en: '[PRE98]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: As you can see in the above output, the function was only called twice given
    the two columns of data, but it was applied three times with the `pd.Series` that
    had three rows.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，在给定的两列数据中，函数仅被调用了两次，但在包含三行的`pd.Series`中，它被应用了三次。
- en: 'Aside from how many times `pd.DataFrame.apply` actually applies the function,
    the shape of the return value can vary between mirroring `.agg` and `.transform`
    functionality. Our preceding example is closer to a `.agg` because it returns
    a single `None` value, but if we returned the element we printed, we would get
    behavior more like a `.transform`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`pd.DataFrame.apply`实际应用函数的次数外，返回值的形状也可能有所不同，可能与`.agg`和`.transform`的功能相似。我们之前的例子更接近`.agg`，因为它返回了一个单一的`None`值，但如果我们返回我们打印的元素，那么行为更像是`.transform`：
- en: '[PRE102]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: If you find this confusing, you are not alone. Trusting pandas to *do the right
    thing* with `.apply` can be a risky proposition; I strongly advise users exhaust
    all options with `.agg`, `.transform`, or `.map` before reaching for `.apply`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这很困惑，你不是一个人。相信pandas在使用`.apply`时“做正确的事情”可能是一个有风险的选择；我强烈建议用户在使用`.apply`之前，先尝试使用`.agg`、`.transform`或`.map`，直到这些方法无法满足需求为止。
- en: Summary statistics
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汇总统计
- en: 'Summary statistics provide a quick way to understand the basic properties and
    distribution of the data. In this section, we introduce two powerful pandas methods:
    `pd.Series.value_counts` and `pd.Series.describe`, which can serve as useful starting
    points for exploration.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总统计提供了一种快速了解数据基本属性和分布的方式。在这一部分，我们介绍了两个强大的pandas方法：`pd.Series.value_counts`和`pd.Series.describe`，它们可以作为探索的有用起点。
- en: How to do it
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'The `pd.Series.value_counts` method attaches frequency counts to each distinct
    data point, making it easy to see how often each value occurs. This is particularly
    useful for discrete data:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.Series.value_counts`方法为每个不同的数据点附加了频率计数，使得查看每个值出现的频率变得简单。这对离散数据特别有用：'
- en: '[PRE104]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'For continuous data, `pd.Series.describe` is a heap of calculations packaged
    together into one method call. Through invocation of this particular method, you
    can easily see the count, mean, minimum, and maximum, alongside a high-level distribution
    of your data:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续数据，`pd.Series.describe`是将一堆计算打包成一个方法调用。通过调用这个方法，你可以轻松查看数据的计数、均值、最小值、最大值，以及数据的高层次分布：
- en: '[PRE106]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'By default, we will see our distribution summarized through the 25%, 50%, 75%,
    and max (or 100%) quartiles. If your data analysis was focused on a more particular
    part of the distribution, you could control what this method presents back by
    providing a `percentiles=` argument:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，我们会看到通过 25%、50%、75% 和最大值（或 100%）四分位数来概述我们的分布。如果您的数据分析集中在分布的某一特定部分，您可以通过提供
    `percentiles=` 参数来控制此方法返回的内容：
- en: '[PRE108]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Binning algorithms
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分箱算法
- en: Binning is the process of taking a continuous variable and categorizing it into
    discrete buckets. It can be useful to turn a potentially infinite amount of values
    into a finite amount of “bins” for your analysis.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 分箱（Binning）是将连续变量分成离散区间的过程。这对于将可能是无限的数值转化为有限的“区间”以进行分析是非常有用的。
- en: How to do it
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'Let’s imagine we have collected survey data from users of a system. One of
    the survey questions asks users for their age, producing data that looks like:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经从一个系统的用户那里收集了调查数据。调查中的一个问题询问用户的年龄，产生的数据如下所示：
- en: '[PRE110]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Rather than treating each age as an individual number, we will use `pd.cut`
    to place each record into an age group. As a first attempt, let’s pass our `pd.Series`
    and the number of bins we would like to generate as arguments:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不打算将每个年龄当作一个独立的数值，而是使用 `pd.cut` 将每条记录分到一个年龄组。作为第一次尝试，我们将 `pd.Series` 和我们想要生成的区间数量作为参数传递：
- en: '[PRE112]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: This produces a `pd.CategoricalDtype` with 4 distinct intervals – `(17.952,
    30.0]`, `(30.0, 42.0]`, `(42.0, 54.0]`, and `(54.0, 66.0]`. Save some unexpected
    decimal places on the first bin, which starts at `17.952`, these bins all cover
    an equidistant range of 12 years, which was derived from the fact that the maximum
    value (`66`) minus the lowest value (`18`) yields a total age gap of 48 years,
    which, when divided equally by 4, gives us the 12-year range for each bin.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成一个具有 4 个不同区间的 `pd.CategoricalDtype` —— `(17.952, 30.0]`、`(30.0, 42.0]`、`(42.0,
    54.0]` 和 `(54.0, 66.0]`。除了第一个区间从 `17.952` 开始有一些意外的小数位外，这些区间都覆盖了 12 年的等距范围，得出这个范围的原因是最大值（`66`）减去最小值（`18`）得到的年龄差为
    48 年，然后将其平分成 4 个区间，每个区间的跨度为 12 年。
- en: 'The age `17.952` we see in the first bin may make sense to pandas internally
    for whatever algorithm it chose to determine the buckets, but it is ultimately
    uninteresting to us since we know we are dealing with whole numbers. Fortunately,
    this can be controlled via the `precision=` keyword argument to remove any decimal
    places:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一个区间看到的年龄`17.952`可能对于 pandas 内部使用的任何算法在确定区间时有意义，但对于我们来说并不重要，因为我们知道我们处理的是整数。幸运的是，可以通过
    `precision=` 关键字参数来控制，去除任何小数部分：
- en: '[PRE114]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '`pd.cut` does not limit us to producing equally sized bins like this. If, instead,
    we wanted to place each person into 10-year age buckets, we could provide those
    ranges as the second argument:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.cut` 并不限于生成像上面那样大小相等的区间。如果我们想要将每个人按 10 年一组进行分箱，可以将这些范围作为第二个参数提供：'
- en: '[PRE116]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'However, this is a little too strict because it would not account for users
    over the age of 70\. To handle that, we could change our last bin edge from `70`
    to `999` and treat it as a catch-all:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方式有点过于严格，因为它没有考虑到 70 岁以上的用户。为了解决这个问题，我们可以将最后一个区间的边界从 `70` 改为 `999`，并将其视为“其他”区间：
- en: '[PRE118]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'In turn, this produced a label of `(60, 999)`, which leaves something to be
    desired from a display perspective. If we are not happy with the default labels
    produced, we can control their output with the `labels=` argument:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，这生成了标签 `(60, 999)`，从显示角度来看，这并不令人满意。如果我们对默认生成的标签不满意，可以通过 `labels=` 参数控制它们的输出：
- en: '[PRE120]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: However, our labels above are not *quite* right. Note that we provided both
    `30-40` and `40-50`, but what happens if someone is exactly 40 years old? What
    bin are they placed in?
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上面的标签并不*完全*正确。请注意，我们提供了 `30-40` 和 `40-50`，但是如果某人恰好是 40 岁呢？他们会被放入哪个区间？
- en: 'Fortunately, we can see this in our data already from `Steve`, who perfectly
    matches this criteria. If you inspect the default bin he is placed in, it appears
    as `(30, 40]`:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以通过数据中 `Steve` 的记录看到这一点，他恰好符合这个标准。如果查看他所在的默认区间，它显示为 `(30, 40]`：
- en: '[PRE122]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'Binning, by default, is *right inclusive*, meaning each bin can be thought
    of as *up to and including* a particular value. If we wanted behavior that was
    *up to but not including*, we could control this with the `right` argument:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，分箱是*右闭*的，这意味着每个区间可以被认为是*包括*特定值。如果我们想要的是*不包括*特定值的行为，可以通过 `right` 参数来控制：
- en: '[PRE124]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: This changed the bin for `Steve` from `(30, 40]` to `[40, 50)`. In the default
    string representation, the square bracket signifies the edge being *inclusive*
    of a particular value, whereas the parenthesis is *exclusive*.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这将 `Steve` 的区间从 `(30, 40]` 改为 `[40, 50)`。在默认的字符串表示中，方括号表示该边界是*包含*某个特定值的，而圆括号则是*不包含*的。
- en: One-hot encoding with pd.get_dummies
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `pd.get_dummies` 进行独热编码
- en: It is not uncommon in data analysis and machine learning applications to take
    data that is categorical in nature and convert it into a sequence of `0/1` values,
    as the latter can be more easily interpreted by numeric algorithms. This process
    is often called *one-hot encoding*, and the outputs are typically referred to
    as *dummy indicators*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析和机器学习应用中，将类别型数据转换为 `0/1` 值的情况并不罕见，因为后者能更容易地被数值算法解释。这一过程通常称为 *独热编码*，输出通常被称为
    *虚拟指示符*。
- en: How to do it
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'Let’s start with a small `pd.Series` containing a discrete set of colors:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个包含离散颜色集的小 `pd.Series` 开始：
- en: '[PRE126]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'Passing this as an argument to `pd.get_dummies` will create a like-indexed
    `pd.DataFrame` with a Boolean column for each color. Each row has one column with
    `True` that maps it back to its original value; all other columns in the same
    row will be `False`:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 将其作为参数传递给 `pd.get_dummies` 将创建一个具有布尔列的 `pd.DataFrame`，每个颜色对应一列。每一行会有一个 `True`
    的列，将其映射回原始值；该行中的其他所有列将是 `False`：
- en: '[PRE128]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'If we are not satisfied with the default column names, we can modify them by
    adding a prefix. A common convention in data modeling is to prefix a Boolean column
    with `is_`:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不满意默认的列名，可以通过添加前缀来修改它们。在数据建模中，一个常见的约定是将布尔列的前缀加上 `is_`：
- en: '[PRE130]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: Chaining with .pipe
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `.pipe` 链式调用
- en: 'When writing pandas code, there are two major stylistic forms that developers
    follow. The first approach makes liberal use of variables throughout a program,
    whether that means creating new variables like:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写 pandas 代码时，开发者通常遵循两种主要的风格形式。第一种方法是大量使用变量，无论是创建新的变量，像这样：
- en: '[PRE132]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'or simply reassigning to the same variable repeatedly:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 或者只是反复重新赋值给同一个变量：
- en: '[PRE133]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'The alternative approach is to express your code as a *pipeline*, where each
    step accepts and returns a `pd.DataFrame`:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将代码表达为 *管道*，每个步骤接受并返回一个 `pd.DataFrame`：
- en: '[PRE134]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: With the variable-based approach, you must create multiple variables in your
    program, or change the state of a `pd.DataFrame` at every reassignment. The pipeline
    approach, by contrast, does not create any new variables, nor does it change the
    state of your `pd.DataFrame`.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于变量的方法，你必须在程序中创建多个变量，或者在每次重新赋值时改变 `pd.DataFrame` 的状态。相比之下，管道方法不会创建任何新变量，也不会改变
    `pd.DataFrame` 的状态。
- en: While the pipeline approach could theoretically be better handled by a query
    optimizer, pandas does not offer such a feature as of the time of writing, and
    it is hard to guess what that may look like in the future. As such, the choice
    between the two approaches makes almost no difference for performance; it is truly
    a matter of style.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然管道方法从理论上讲可以更好地由查询优化器处理，但截至写作时，pandas 并没有提供此类功能，且很难猜测未来可能会是什么样子。因此，选择这两种方法几乎对性能没有影响；这真的是风格上的问题。
- en: I encourage you to familiarize yourself with both approaches. You may at times
    find it easier to express your code as a pipeline; at other times, that may feel
    burdensome. There is no hard requirement to use one or the other, so you can mix
    and match the styles freely throughout your code.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你熟悉这两种方法。你可能会发现有时将代码表达为管道更容易；而其他时候，可能会觉得那样做很繁琐。没有强制要求使用其中任何一种方法，因此你可以在代码中自由混合和匹配这两种风格。
- en: How to do it
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'Let’s start with a very basic `pd.DataFrame`. The columns and their contents
    are not important for now:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个非常基础的 `pd.DataFrame` 开始。列及其内容暂时不重要：
- en: '[PRE135]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Now let’s create some sample functions that will change the content of the
    columns. These functions should accept and return a `pd.DataFrame`, which you
    can see from the code annotations:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一些示例函数，这些函数将改变列的内容。这些函数应该接受并返回一个 `pd.DataFrame`，你可以从代码注释中看到这一点：
- en: '[PRE137]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'As mentioned in the introduction to this recipe, one of the most common ways
    to apply these functions would be to list them out as separate steps in our program,
    assigning the results of each step to a new variable along the way:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在本教程开头提到的，应用这些函数的最常见方法之一是将它们列为程序中的单独步骤，并将每个步骤的结果分配给一个新变量：
- en: '[PRE138]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'If we wanted to avoid the use of intermediate variables altogether, we could
    have also tried to nest the function calls inside of one another:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望完全避免使用中间变量，我们也可以尝试将函数调用嵌套在一起：
- en: '[PRE140]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: However, that doesn’t make the code any more readable, especially given the
    fact that `change_col1` is executed before `change_col2`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不会使代码更加易读，特别是考虑到`change_col1`在`change_col2`之前执行。
- en: 'By expressing this as a pipeline, we can avoid the use of variables and more
    easily express the order of operations being applied. To achieve this, we are
    going to reach for the `pd.DataFrame.pipe` method:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这个过程表达为管道，我们可以避免使用变量，并更容易表达应用的操作顺序。为了实现这一点，我们将使用`pd.DataFrame.pipe`方法：
- en: '[PRE142]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: As you can see, we have gotten back the same result as before, but without the
    use of variables and in a way that is arguably more readable.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们得到了与之前相同的结果，但没有使用变量，而且以一种可以说更易读的方式呈现。
- en: 'In case any of the functions you want to apply in a pipeline need to accept
    more arguments, `pd.DataFrame.pipe` is able to forward them along for you. For
    instance, let’s see what happens if we add a new `str_case` parameter to our `change_col2`
    function:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在管道中应用的某些函数需要接受更多参数，`pd.DataFrame.pipe`可以将它们转发给你。例如，让我们看看如果我们向`change_col2`函数添加一个新的`str_case`参数会发生什么：
- en: '[PRE144]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'As you can see with `pd.DataFrame.pipe`, you can simply pass that argument
    along as either a positional or keyword argument, just as if you were invoking
    the `change_col2` function directly:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在`pd.DataFrame.pipe`中看到的，你可以简单地将参数作为位置参数或关键字参数传递，就像你直接调用`change_col2`函数一样：
- en: '[PRE145]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: To reiterate what we mentioned in the introduction to this recipe, there is
    little to no functional difference between these styles. I encourage you to learn
    them both as you will inevitably see code written both ways. For your own development,
    you may even find that mixing and matching the approaches works best.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 重申一下我们在本食谱介绍中提到的，这些风格之间几乎没有功能差异。我鼓励你同时学习这两种风格，因为你不可避免地会看到代码同时以这两种方式编写。为了你自身的开发，你甚至可能会发现混合使用这两种方法是最有效的。
- en: Selecting the lowest-budget movies from the top 100
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从前100部电影中选择最低预算的电影
- en: Now that we have covered many of the core pandas algorithms from a theoretical
    level, we can start looking at more “real world” datasets and touch on common
    ways to explore them.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从理论层面覆盖了许多核心的pandas算法，我们可以开始看看一些“真实世界”的数据集，并探讨常见的探索方法。
- en: Top N analysis is a common technique whereby you filter your data based on how
    your data performs when measured by a single variable. Most analytics tools have
    the capability to help you filter your data to answer questions like *What are
    the top 10 customers by sales?* or, *What are the 10 products with the lowest
    inventory?*. When chained together, you can even form catchy news headlines such
    as *Out of the Top 100 Universities, These 5 Have the Lowest Tuition Fees*, or
    *From the Top 50 Cities to Live, These 10 Are the Most Affordable*.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Top N分析是一种常见技术，通过该技术，你可以根据数据在单一变量上的表现来筛选数据。大多数分析工具都能帮助你筛选数据，以回答类似*销售额最高的前10个客户是哪些？*
    或者 *库存最少的10个产品是哪些？* 这样的问题。当这些方法链式调用时，你甚至可以形成引人注目的新闻标题，比如*在前100所大学中，这5所学费最低*，或*在前50个宜居城市中，这10个最实惠*。
- en: Given how common these types of analyses are, pandas offers built-in functionality
    to help you easily perform them. In this recipe, we will take a look at `pd.DataFrame.nlargest`
    and `pd.DataFrame.nsmallest` and see how we can use them together to answer a
    question like *From the top 100 movies, which had the lowest budget?*.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些类型的分析非常常见，pandas提供了内置功能来帮助你轻松执行这些分析。在本食谱中，我们将查看`pd.DataFrame.nlargest`和`pd.DataFrame.nsmallest`，并看看如何将它们结合使用，以回答类似*从前100部电影中，哪些电影预算最低？*的问题。
- en: How to do it
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做
- en: 'Let’s start by reading in the movie dataset and selecting the columns `movie_title`,
    `imdb_score`, `budget`, and `gross`:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从读取电影数据集并选择`movie_title`、`imdb_score`、`budget`和`gross`列开始：
- en: '[PRE147]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'The `pd.DataFrame.nlargest` method can be used to select the top 100 movies
    by `imdb_score`:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.DataFrame.nlargest`方法可以用来选择按`imdb_score`排序的前100部电影：'
- en: '[PRE149]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Now that we have the top 100 selected, we can chain in a call to `pd.DataFrame.nsmallest`
    to return the five lowest-budget movies among those:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经选择了前100部电影，我们可以链式调用`pd.DataFrame.nsmallest`，从中返回五部预算最低的电影：
- en: '[PRE151]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: There’s more…
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: It is possible to pass a list of column names as the `columns=` parameter of
    the `pd.DataFrame.nlargest` and `pd.DataFrame.nsmallest` methods. This would only
    be useful to break ties in the event that there were duplicate values sharing
    the *nth* ranked spot in the first column in the list.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将列名列表作为`columns=`参数传递给`pd.DataFrame.nlargest`和`pd.DataFrame.nsmallest`方法。这在遇到第一列中有重复值共享*第n名*排名时才有用，以此来打破平局。
- en: 'To see where this matters, let’s try to just select the top 10 movies by `imdb_score`:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这一点的重要性，让我们尝试按`imdb_score`选择前10部电影：
- en: '[PRE153]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: 'As you can see, the lowest `imdb_score` from the top 10 is `8.9`. However,
    there are more than 10 movies that have a score of `8.9` and above:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，前10名中的最低`imdb_score`是`8.9`。然而，实际上有超过10部电影的评分为`8.9`或更高：
- en: '[PRE155]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'The movies that were a part of the top 10 just happened to be the first two
    movies pandas came across with that score. However, you can use the `gross` column
    as the tiebreaker:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 那些出现在前10名的电影，恰好是pandas遇到的前两部评分为该分数的电影。但是，你可以使用`gross`列作为平局的决胜者：
- en: '[PRE157]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: With that, you see that Pulp Fiction replaced Schindler’s List in our top 10
    analysis, given that it grossed higher.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 因为《低俗小说》票房更高，所以你可以看到它取代了《辛德勒的名单》成为我们前10名分析中的一部分。
- en: Calculating a trailing stop order price
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算追踪止损单价格
- en: There are many strategies to trade stocks. One basic type of trade that many
    investors employ is the *stop order*. A stop order is an order placed by an investor
    to buy or sell a stock that executes whenever the market price reaches a certain
    point. Stop orders are useful to both prevent huge losses and protect gains.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多股票交易策略。许多投资者采用的一种基本交易类型是*止损单*。止损单是投资者在市场价格达到某个点时执行的买入或卖出股票的订单。止损单有助于防止巨大的亏损并保护收益。
- en: In a typical stop order, the price does not change throughout the lifetime of
    the order. For instance, if you purchased a stock for $100 per share, you might
    want to set a stop order at $90 per share to limit your downside to 10%.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的止损单中，价格在整个订单生命周期内不会变化。例如，如果你以每股$100的价格购买了一只股票，你可能希望在每股$90设置止损单，以限制你的最大损失为10%。
- en: A more advanced strategy would be to continually modify the sale price of the
    stop order to track the value of the stock if it increases in value. This is called
    a *trailing stop order*. Concretely, if the same $100 stock increases to $120,
    then a trailing stop order 10% below the current market value would move the sale
    price to $108.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更高级的策略是不断调整止损单的售价，以跟踪股票价值的变化，如果股票上涨。这被称为*追踪止损单*。具体来说，如果一只价格为$100的股票上涨到$120，那么一个低于当前市场价10%的追踪止损单会将售价调整为$108。
- en: The trailing stop order never moves down and is always tied to the maximum value
    since the time of purchase. If the stock fell from $120 to $110, the stop order
    would still remain at $108\. It would only increase if the price moved above $120.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 追踪止损单从不下调，总是与购买时的最高价值挂钩。如果股票从$120跌到$110，止损单仍然会保持在$108。只有当价格超过$120时，止损单才会上调。
- en: This recipe determines the trailing stop order price given an initial purchase
    price for any stock using the `pd.Series.cummax` method and how `pd.Series.cummin`
    could instead be used to handle short positions. We will also see how the `pd.Series.idxmax`
    method can be used to identify the day the stop order would have been triggered.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码通过`pd.Series.cummax`方法，根据任何股票的初始购买价格来确定追踪止损单价格，并展示了如何使用`pd.Series.cummin`来处理短仓头寸。我们还将看到如何使用`pd.Series.idxmax`方法来识别止损单被触发的那一天。
- en: How to do it
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤
- en: 'To get started, we will work with Nvidia (NVDA) stock and assume a purchase
    on the first trading day of 2020:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，我们将以Nvidia（NVDA）股票为例，假设在2020年第一交易日进行购买：
- en: '[PRE159]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'In the pandas 2.2 series, there is a bug that prevents the preceding code block
    from running, instead throwing a `ValueError`. If affected by this bug, you can
    alternatively run `pd.read_csv` without the `dtype_backend` argument, and add
    in a call to `pd.DataFrame.convert_dtypes` instead:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas 2.2版本中，存在一个bug，导致前面的代码块无法运行，反而抛出`ValueError`错误。如果遇到这个问题，可以选择不使用`dtype_backend`参数运行`pd.read_csv`，然后改为调用`pd.DataFrame.convert_dtypes`：
- en: '[PRE161]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: For more information, see pandas bug issue `#57930` ([https://github.com/pandas-dev/pandas/issues/57930](https://github.com/pandas-dev/pandas/issues/57930)).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息请参见pandas的bug问题`#57930`（[https://github.com/pandas-dev/pandas/issues/57930](https://github.com/pandas-dev/pandas/issues/57930)）。
- en: 'Regardless of which path you took, be aware that `pd.read_csv` returns a `pd.DataFrame`,
    but for this analysis we will only need a `pd.Series`. To perform that conversion,
    you can call `pd.DataFrame.squeeze`, which will reduce the object from two to
    one dimension, if possible:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您采取了哪条路径，请注意，`pd.read_csv`返回一个`pd.DataFrame`，但对于本分析，我们只需要一个`pd.Series`。为了进行转换，您可以调用`pd.DataFrame.squeeze`，如果可能的话，它将把对象从二维减少到一维：
- en: '[PRE163]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'With that, we can use the `pd.Series.cummax` method to track the highest closing
    price seen to date:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以使用`pd.Series.cummax`方法来跟踪到目前为止观察到的最高收盘价：
- en: '[PRE165]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'To create a trailing stop order that limits our downside to 10%, we can chain
    in a multiplication by `0.9`:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个将下行风险限制在10%的止损订单，我们可以链式操作，将其乘以`0.9`：
- en: '[PRE167]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: The `pd.Series.cummax` method works by retaining the maximum value encountered
    up to and including the current value. Multiplying this series by 0.9, or whatever
    cushion you would like to use, creates the trailing stop order. In this particular
    example, NVDA increased in value, and thus, its trailing stop has also increased.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.Series.cummax`方法通过保留截至当前值为止遇到的最大值来工作。将该系列乘以0.9，或者使用您希望的任何保护系数，即可创建止损订单。在这个特定的例子中，NVDA的价值增加，因此其止损也随之上升。'
- en: On the flip side, let’s say we were pessimistic about NVDA stock during this
    timeframe, and we wanted to short the stock. However, we still wanted to put a
    stop order in place to limit the downside to a 10% increase in value.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，假设我们对NVDA股票在这段时间内持悲观看法，并且我们想要做空该股票。然而，我们仍然希望设置一个止损订单，以限制下跌幅度不超过10%。
- en: 'For this, we can simply replace our usage of `pd.Series.cummax` with `pd.Series.cummin`
    and multiply by `1.1` instead of `0.9`:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们只需将`pd.Series.cummax`的使用替换为`pd.Series.cummin`，并将`0.9`改为`1.1`即可：
- en: '[PRE169]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '[PRE170]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: There’s more…
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'With our trailing stop orders calculated, we can easily determine the days
    where we would have fallen off of the cumulative maximum by more than our threshold:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算我们的止损订单，我们可以轻松地确定在哪些日子我们会跌破累计最大值，超过我们的设定阈值。
- en: '[PRE171]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'If we only cared to identify the very first day where we fell below the cumulative
    maximum, we could use the `pd.Series.idxmax` method. This method works by first
    calculating the maximum value within a `pd.Series`, and then returns the first-row
    index where that maximum was encountered:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只关心找出我们第一次跌破累计最大值的那一天，我们可以使用`pd.Series.idxmax`方法。该方法通过首先计算`pd.Series`中的最大值，然后返回出现该最大值的第一行索引：
- en: '[PRE173]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: The expression `ser <= stop_prices` gives back a Boolean `pd.Series` containing
    `True=/=False` values, with each `True` record indicating where the stock price
    is at or below the stop price we already calculated. `pd.Series.idxmax` will consider
    `True` to be the maximum value in that `pd.Series`; so, by returning the first
    index label where `True` was seen as a value, it tells us the first day that our
    trailing stop order should have been triggered.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式`ser <= stop_prices`会返回一个布尔类型的`pd.Series`，其中包含`True/False`值，每个`True`记录表示股票价格在我们已经计算出的止损价格或以下。`pd.Series.idxmax`会将`True`视为该`pd.Series`中的最大值；因此，返回第一次遇到`True`的索引标签，它告诉我们应该触发止损订单的第一天。
- en: This recipe gives us just a taste of how useful pandas may be for trading securities.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例让我们初步了解了pandas在证券交易中的应用价值。
- en: Finding the baseball players best at…
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找最擅长的棒球选手...
- en: The American sport of baseball has long been a subject of intense analytical
    research, with data collection dating back to the early 1900s. For Major League
    baseball teams, advanced data analysis helps answer questions like *How much should
    I pay for X player?* and *What should I do in the game given the current state
    of things*?, For fans, that same data can be used as fodder for endless debates
    around *who is the greatest player ever*.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 美国棒球运动长期以来一直是激烈分析研究的对象，数据收集可以追溯到20世纪初。对于美国职业棒球大联盟的球队，先进的数据分析帮助回答诸如*我应该为X球员支付多少薪水？*以及*在当前局势下，我应该在比赛中做什么？*这样的问题。对于球迷来说，同样的数据也可以作为无尽辩论的素材，讨论*谁是历史上最伟大的球员*。
- en: 'For this recipe, we are going to use data that was collected from [retrosheet.org](https://www.retrosheet.org).
    Per the Retrosheet licensing requirements, you should be aware of the following
    legal disclaimer:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将使用从[retrosheet.org](https://www.retrosheet.org)收集的数据。根据Retrosheet的许可要求，您应注意以下法律免责声明：
- en: The information used here was obtained free of charge and is copyrighted by
    Retrosheet. Interested parties may contact Retrosheet at [www.retrosheet.org](https://www.retrosheet.org).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 此处使用的信息是免费获取的，并由Retrosheet版权所有。有兴趣的各方可以联系Retrosheet，网址是[www.retrosheet.org](https://www.retrosheet.org)。
- en: From its raw form, the data was summarized to show the common baseball metrics
    for at bat (`ab`), hits (`h`), runs scored (`r`), and home runs (`hr`) for professional
    players in the years 2020–2023.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始形式中，这些数据被汇总以显示2020年至2023年间职业球员的常见棒球统计指标，包括打数（`ab`）、安打（`h`）、得分（`r`）和全垒打（`hr`）。
- en: How to do it
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'Let’s start by reading in our summarized data and setting the `id` column (which
    represents a unique player) as the index:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从读取我们的汇总数据和将`id`列（表示唯一球员）设置为索引开始：
- en: '[PRE175]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: 'In baseball, it is rather rare for a player to dominate all statistical categories.
    Oftentimes, a player with a lot of home runs (`hr`) will be more powerful and
    can hit the ball farther, but may do so less frequently than a player more specialized
    to collect a lot of hits (`h`). With pandas, we are fortunate to not have to dive
    into each metric individually; a simple call to `pd.DataFrame.idxmax` will look
    at each column, find the maximum value, and return the row index value associated
    with that maximum value for you:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在棒球中，一个球员很少能在所有统计类别中占据主导地位。通常情况下，一位具有许多全垒打（`hr`）的球员更具威力，能够将球打得更远，但可能比一个更专注于收集大量安打（`h`）的球员频率较低。有了pandas，我们幸运地不必深入研究每个指标；只需简单调用`pd.DataFrame.idxmax`就可以查看每一列，找到最大值，并返回与该最大值关联的行索引值：
- en: '[PRE177]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: As you can see, player `semim001` (Marcus Semien) had the most at bats, `freef001`
    (Freddie Freeman) had the most runs and hits, and `judga001` (Aaron Judge) hit
    the most home runs in this timeframe.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，球员`semim001`（Marcus Semien）在上场次数方面表现最佳，`freef001`（Freddie Freeman）在得分和安打方面表现最佳，而`judga001`（Aaron
    Judge）在这段时间内敲出了最多的全垒打。
- en: 'If you wanted to look deeper into how these great players performed across
    all categories, you could take the output of `pd.DataFrame.idxmax`, subsequently
    call `pd.Series.unique` on the values, and use that as a mask for the overall
    `pd.DataFrame`:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想深入了解这些优秀球员在所有类别中的表现，可以使用`pd.DataFrame.idxmax`的输出，随后对这些值调用`pd.Series.unique`作为整体`pd.DataFrame`的掩码：
- en: '[PRE179]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: There’s more…
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这还不止于此…
- en: 'For a nice visual enhancement to this data, you can use `pd.DataFrame.style.highlight_max`
    to very specifically show which category these players were the best at:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 要为这些数据提供良好的视觉增强效果，您可以使用`pd.DataFrame.style.highlight_max`来非常具体地显示这些球员在哪个类别表现最佳：
- en: '[PRE181]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '![](img/B31091_05_01.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_05_01.png)'
- en: 'Figure 5.1: Jupyter Notebook output of a DataFrame highlighting max value per
    column'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：Jupyter Notebook 输出的DataFrame，突出显示每列的最大值
- en: Understanding which position scores the most per team
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解哪个位置在每支球队中得分最高
- en: In baseball, teams are allowed 9 batters in a “lineup,” with 1 representing
    the first person to bat and 9 representing the last. Over the course of a game,
    teams cycle through batters in order, starting over with the first batter after
    the last has batted.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在棒球中，每支球队可以有9名“打击阵容”球员，1代表第一个击球手，9代表最后一个。在比赛过程中，球队按顺序循环轮换击球手，第一位击球手在最后一位击球手击球后重新开始。
- en: Typically, teams place some of their best hitters toward the “top of the lineup”
    (i.e., lower number positions) to maximize the opportunity for them to come around
    and score. However, this does not always mean that the person who bats in position
    1 will always be the first to score.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，球队会把一些最佳击球手放在“阵容的前面”（即较低的位置），以最大化他们的得分机会。然而，这并不总是意味着第一位击球手总是第一个得分的人。
- en: In this recipe, we are going to look at all Major League baseball teams from
    2000–2023 and find the position that scored the most runs for a team over each
    season.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将查看从2000年到2023年的所有大联盟棒球队，并找出在每个赛季中为球队得分最多的位置。
- en: How to do it
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'Much like we did in the *Finding the baseball players best at…* recipe, we
    are going to use data taken from [retrosheet.org](https://www.retrosheet.org).
    For this particular dataset, we are going to set the `year` and `team` columns
    in the row index, leaving the remaining columns to show the position in the batting
    order:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在*寻找棒球运动员最擅长…*的示例中所做的那样，我们将使用从[retrosheet.org](https://www.retrosheet.org)获取的数据。对于这个特定的数据集，我们将把`year`和`team`列设置为行索引，剩余的列用于显示击球顺序中的位置：
- en: '[PRE182]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'With `pd.DataFrame.idxmax`, we can see for every year and team which position
    scored the most runs. However, with this dataset, the index label we would like
    `pd.DataFrame.idxmax` to identify is actually in the columns and not the rows.
    Fortunately, pandas can still calculate this easily with the `axis=1` argument:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pd.DataFrame.idxmax`，我们可以查看每年和每个队伍中哪个位置得分最高。然而，在这个数据集中，我们希望`pd.DataFrame.idxmax`识别的索引标签实际上是在列中，而不是行中。幸运的是，pandas仍然可以通过`axis=1`参数轻松计算这个：
- en: '[PRE184]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: 'From there, we can use `pd.Series.value_counts` to understand the number of
    times a given position in the order represented the most runs scored for a team.
    We are also going to use the `normalize=True` argument, which will give us a frequency
    instead of a total:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我们可以使用`pd.Series.value_counts`来了解在队伍中，某一位置代表得分最多的次数。我们还将使用`normalize=True`参数，它将为我们提供频率而不是总数：
- en: '[PRE186]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: Unsurprisingly, the first batter scored most frequently accounted for the most
    runs, doing so for 48% of the teams.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，得分最多的首位打者通常会占据得分最多的位置，约48%的队伍如此。
- en: There’s more…
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'We might want to explore more and answer the question: *For teams where the
    first batter scored the most runs, who scored the second-most*?'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想深入探索并回答这个问题：*对于首位打者得分最多的队伍，谁得分第二多*？
- en: 'To calculate this, we can create a mask to filter on teams where the first
    batter scored the most, drop that column from our dataset, and then repeat with
    the same `pd.DataFrame.idxmax` approach to identify the position next in line:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个，我们可以创建一个掩码来筛选出首位打者得分最多的队伍，然后从数据集中删除该列，然后重复相同的`pd.DataFrame.idxmax`方法来识别下一个位置：
- en: '[PRE188]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: As you can see, if a team’s first batter does not lead the team in runs scored,
    the second batter ends up being the leader almost 50% of the time.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，如果一个队伍的首位打者并没有得分最多，第二位打者几乎50%的情况下会成为得分最多的人。
- en: Join our community on Discord
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/pandas](https://packt.link/pandas)'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/pandas](https://packt.link/pandas)'
- en: '![](img/QR_Code5040900042138312.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5040900042138312.png)'
