- en: Supervised Learning - Classification Techniques
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习 - 分类技术
- en: 'Most real-world machine learning problems use supervised learning. In supervised
    learning, the model will learn from a labeled training dataset. A label is a target
    variable that we want to predict. It is an extra piece of information that helps
    in making decisions or predictions, for example, which loan application is safe
    or risky, whether a patient suffers from a disease or not, house prices, and credit
    eligibility scores. These labels act as a supervisor or teacher for the learning
    process. Supervised learning algorithms can be of two types: classification or
    regression. A classification problem has a categorical target variable, such as
    a loan application status as safe or risky, whether a patient suffers from a "disease"
    or "not disease," or whether a customer is "potential" or "not potential."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现实世界中的机器学习问题都使用监督学习。在监督学习中，模型将从一个带标签的训练数据集中学习。标签是我们想要预测的目标变量。它是一个额外的信息，帮助做出决策或预测，例如，哪个贷款申请是安全的或有风险的，患者是否患有某种疾病，房价，以及信用资格分数。这些标签作为学习过程中的监督者或老师。监督学习算法可以分为两种类型：分类或回归。分类问题有一个分类目标变量，例如，贷款申请状态是安全的还是有风险的，患者是否患有“疾病”或“无疾病”，或顾客是否是“潜在的”或“非潜在的”。
- en: This chapter focuses on supervised machine learning, and specifically covers
    classification techniques. This chapter will mostly be using scikit-learn. It
    will delve into basic techniques of classification, such as naive Bayes, **Support
    Vector Machines** (**SVMs**), **K-Nearest Neighbor** (**KNN**), and decision trees.
    Also, it focuses on train-test split strategies and model evaluation methods and
    parameters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍监督学习，特别是分类技术。本章将主要使用 scikit-learn。将深入讨论分类的基本技术，如朴素贝叶斯、**支持向量机**（**SVM**）、**K-最近邻**（**KNN**）和决策树。此外，还重点介绍训练集与测试集的划分策略以及模型评估方法和参数。
- en: 'The topics of this chapter are listed as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主题如下：
- en: Classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Naive Bayes classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类
- en: Decision tree classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树分类
- en: KNN classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNN 分类
- en: SVM classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM 分类
- en: Splitting training and testing sets
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 划分训练集和测试集
- en: Evaluating the classification model performance
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类模型的表现
- en: ROC curve and AUC
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC 曲线与 AUC
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术要求如下：
- en: 'You can find the code and the datasets at the following GitHub link: [https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter10](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter10).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在以下 GitHub 链接找到代码和数据集：[https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter10](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter10)。
- en: All the code blocks are available in the `ch10.ipynb` file.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有代码块都在 `ch10.ipynb` 文件中。
- en: This chapter uses only one CSV file (`diabetes.csv`) for practice purposes.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章仅使用一个 CSV 文件（`diabetes.csv`）进行练习。
- en: In this chapter, we will use the `pandas` and `scikit-learn` Python libraries.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 `pandas` 和 `scikit-learn` 这两个 Python 库。
- en: Classification
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: 'As a healthcare data analyst, your job is to identify patients or sufferers
    that have a higher chance of a particular disease, for example, diabetes or cancer.
    These predictions will help you to treat patients before the disease occurs. Similarly,
    a sales and marketing manager wants to predict potential customers who have more
    of a chance of buying a product. This is the process of categorizing customers
    into two or more categories known as classification. The classification model
    predicts the categorical class label, such as whether the customer is potential
    or not. In the classification process, the model is trained on available data,
    makes predictions, and evaluates the model performance. Developed models are called
    classifiers. This means it has three stages: training, prediction, and evaluation.
    The trained model is evaluated using parameters such as accuracy, precision, recall,
    F1-score, and **Area Under Curve** (**AUC**). Classification has a variety of
    applications in various domains, such as banking, finance, citizen services, healthcare,
    text analysis, image identification, and object detection.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名医疗数据分析师，你的工作是识别那些患某种特定疾病（例如糖尿病或癌症）可能性较高的患者或病人。这些预测将帮助你在疾病发生之前对患者进行治疗。同样，销售和市场经理也希望预测那些有更高可能性购买产品的潜在客户。这就是将客户分类为两个或多个类别的过程，称为分类。分类模型预测类别标签，例如客户是否是潜在客户。在分类过程中，模型基于现有数据进行训练，做出预测，并评估模型的表现。开发出来的模型称为分类器。这意味着分类模型有三个阶段：训练、预测和评估。训练好的模型通过准确率、精确度、召回率、F1分数和**曲线下面积**（**AUC**）等参数来评估。分类在许多领域中有广泛的应用，如银行、金融、公共服务、医疗保健、文本分析、图像识别和物体检测等。
- en: 'As an analyst, you have to first define the problem that you want to solve
    using classification and then identify the potential features that predict the
    labels accurately. Features are the columns or attributes that are responsible
    for prediction. In diabetes prediction problems, health analysts will collect
    patient information, such as age, exercise routine, junk food-eating habits, alcohol
    consumption, and smoking habit characteristics or features. These features will
    be used to predict whether the patient will suffer from diabetes. You can see
    in the following diagram how data can be classified into two classes using a line:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为分析师，你首先需要定义你想通过分类解决的问题，然后确定能够准确预测标签的潜在特征。特征是负责预测的列或属性。在糖尿病预测问题中，健康分析师会收集患者的信息，例如年龄、锻炼习惯、垃圾食品摄入习惯、酒精消费和吸烟习惯等特征。这些特征将用于预测患者是否会患上糖尿病。你可以在下面的图示中看到，如何通过一条线将数据分类为两个类别：
- en: '![](img/1640f8cc-3ce7-47e7-99b5-f52cd572fd96.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1640f8cc-3ce7-47e7-99b5-f52cd572fd96.png)'
- en: 'Machine learning and data mining processes have various steps: data collection,
    data preprocessing, train-test split, model generation, and evaluation. We have
    seen data analysis models such as KDD, SEMMA, and CRISP-DM. In classification,
    we only focus on the train-test split, model generation, and evaluation.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和数据挖掘过程有多个步骤：数据收集、数据预处理、训练-测试拆分、模型生成和评估。我们已经看到了像KDD、SEMMA和CRISP-DM这样的数据分析模型。在分类中，我们只关注训练-测试拆分、模型生成和评估这几个步骤。
- en: 'The classification model has three stages: train-test split, model generation,
    and model evaluation. In the train-test split stage, data is divided into two
    parts: training and testing sets. In training, the training set is used to generate
    the model, and testing is used in the model evaluation stage to assess the model''s
    performance using evaluation metrics such as accuracy, error, precision, and recall.
    You can see the classification process in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型有三个阶段：训练-测试拆分、模型生成和模型评估。在训练-测试拆分阶段，数据被分为两部分：训练集和测试集。在训练阶段，使用训练集来生成模型，而在评估阶段，使用测试集来评估模型的表现，通过准确率、误差、精确度和召回率等评估指标。你可以在以下图示中看到分类过程：
- en: '![](img/e0deb2e7-0dd2-4bf4-a2c1-162c675ad21f.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0deb2e7-0dd2-4bf4-a2c1-162c675ad21f.png)'
- en: In the preceding diagram, steps for the classification process are presented.
    Now that we understand the classification process, it's time to learn the classification
    techniques. In the next section, we will focus on the naive Bayes classification
    algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，展示了分类过程的各个步骤。现在我们已经理解了分类过程，接下来是学习分类技术。在下一节中，我们将重点介绍朴素贝叶斯分类算法。
- en: Naive Bayes classification
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类
- en: Naive Bayes is a classification method based on the Bayes theorem. Bayes' theorem
    is named after its inventor, the statistician Thomas Bayes. It is a fast, accurate,
    robust, easy-to-understand, and interpretable technique. It can also work faster
    on large datasets. Naive Bayes is effectively deployed in text mining applications
    such as document classification, predicting sentiments of customer reviews, and
    spam filtering.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一种基于贝叶斯定理的分类方法。贝叶斯定理以其发明者、统计学家托马斯·贝叶斯命名。它是一种快速、准确、稳健、易于理解和解释的技术。它还可以在大数据集上更快速地工作。朴素贝叶斯广泛应用于文本挖掘，如文档分类、客户评论情感预测和垃圾邮件过滤等。
- en: 'The naive Bayes classifier is called naive because it assumes class conditional
    independence. Class conditional independence means each feature column is independent
    of the remaining other features. For example, in the case of determining whether
    a person has diabetes or not, it depends upon their eating habits, their exercise
    routine, the nature of their profession, and their lifestyle. Even if features
    are correlated or depend on each other, naive Bayes will still assume they are
    independent. Let''s understand the Bayes theorem formula:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器被称为“朴素”，因为它假设类条件独立。类条件独立意味着每个特征列独立于其他特征。例如，在确定一个人是否患有糖尿病时，可能依赖于他们的饮食习惯、锻炼习惯、职业性质和生活方式。即使特征之间存在相关性或依赖关系，朴素贝叶斯仍然假设它们是独立的。让我们理解贝叶斯定理的公式：
- en: '![](img/eab7c66a-04d9-4a17-8d6c-80cf6afd72b5.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eab7c66a-04d9-4a17-8d6c-80cf6afd72b5.png)'
- en: 'Here, *y* is the target and *X* is the set of features. *p(y)* and *p(X)* are
    the prior probabilities regardless of evidence. This means the probability of
    events before evidence is seen. *p(y|X)* is the posterior probability of event
    *X* after evidence is seen. It is the probability of *y* given evidence *X*. *p(X|y)*
    is the posterior probability of event *y* after evidence is seen. It is the probability
    of *X* given evidence *y*. Let''s take an example of the preceding equation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*y* 是目标，*X* 是特征集。*p(y)* 和 *p(X)* 是不考虑证据的先验概率。这意味着在看到证据之前事件的概率。*p(y|X)* 是在看到证据
    *X* 后事件 *X* 的后验概率。它是给定证据 *X* 的 *y* 的概率。*p(X|y)* 是在看到证据后事件 *y* 的后验概率。它是给定证据 *y*
    的 *X* 的概率。我们来看看前面的方程示例：
- en: '![](img/d1307850-e64d-4428-b376-8fdad1ece1da.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1307850-e64d-4428-b376-8fdad1ece1da.png)'
- en: Here, we are finding the probability of a patient who will suffer from diabetes
    based on their smoking frequency using Bayes' theorem.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用贝叶斯定理找出一个患者是否会因吸烟频率而患糖尿病的概率。
- en: 'Let''s see the working of the naive Bayes classification algorithm. Assume
    that dataset *D* has *X* features and label *y*. Features can be n-dimensional,
    *X*=*X*1, *X*2, *X*3... *Xn*. Label *y* may have *m* classes, *C*1, *C*2, *C*3...*Cm*.
    It will work as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看朴素贝叶斯分类算法的工作原理。假设数据集 *D* 具有 *X* 特征和标签 *y*。特征可以是 n 维的，*X* = *X*1, *X*2,
    *X*3... *Xn*。标签 *y* 可能有 *m* 个类，*C*1, *C*2, *C*3... *Cm*。它将按以下方式工作：
- en: Calculate the prior probabilities,![](img/8fe3120f-0421-4f46-9e43-dbd9d4abfc96.png)
    and ![](img/36a92b16-c79e-467c-9b08-19c72ff86086.png), for the given class labels.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算给定类标签的先验概率，![](img/8fe3120f-0421-4f46-9e43-dbd9d4abfc96.png) 和 ![](img/36a92b16-c79e-467c-9b08-19c72ff86086.png)。
- en: 'Calculate the posterior probabilities, ![](img/8480cb8f-da61-418f-ac55-39bb30687dc2.png)and
    ![](img/51fa1b1c-b657-4692-93d1-bf6d84a2ca3a.png), with each attribute for each
    class:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算后验概率，![](img/8480cb8f-da61-418f-ac55-39bb30687dc2.png) 和 ![](img/51fa1b1c-b657-4692-93d1-bf6d84a2ca3a.png)，每个类的每个属性：
- en: '![](img/1b29185a-39c7-4cb6-b2dd-ed851326b4ad.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b29185a-39c7-4cb6-b2dd-ed851326b4ad.png)'
- en: 'Multiply the same class posterior probability,![](img/706c0f20-4bb2-46ba-a418-b7b8b050295c.png):'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 乘以相同类的后验概率，![](img/706c0f20-4bb2-46ba-a418-b7b8b050295c.png)：
- en: '![](img/d8bd2e11-eeb9-4840-b8ec-b914b3c7f6b2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8bd2e11-eeb9-4840-b8ec-b914b3c7f6b2.png)'
- en: If the attribute is categorical then there should be several records of class
    ![](img/5a8777da-f995-4a0b-8b00-46bc99b9d197.png) in with ![](img/98048ac3-e469-462f-8485-d38627764f62.png)value,
    divided by ![](img/c87ea333-1e06-47fd-bdc0-abfe136958fc.png)records in the dataset.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果属性是分类的，那么应该有多个类 ![](img/5a8777da-f995-4a0b-8b00-46bc99b9d197.png) 在具有 ![](img/98048ac3-e469-462f-8485-d38627764f62.png)
    值的记录中，除以数据集中的 ![](img/c87ea333-1e06-47fd-bdc0-abfe136958fc.png) 记录数。
- en: 'If the attribute is continuous, then it is calculated using Gaussian distribution:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果属性是连续的，则使用高斯分布进行计算：
- en: '![](img/92d65d23-c06e-42f8-910a-19e5c8471c8e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92d65d23-c06e-42f8-910a-19e5c8471c8e.png)'
- en: 'Multiply the prior probability, *p*(*y*), by the posterior probability from
    *step 3*:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将先验概率 *p*(*y*) 乘以*步骤 3*中的后验概率：
- en: '![](img/052d16b7-760b-4974-bcd1-9df8053f9b06.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/052d16b7-760b-4974-bcd1-9df8053f9b06.png)'
- en: Find the class with the maximum probability for the given input feature set.
    This class will be our final prediction.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到给定输入特征集的最大概率类。这个类将作为我们的最终预测。
- en: 'Now, let''s create a model using naive Bayes classification in Python:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Python中的朴素贝叶斯分类创建一个模型：
- en: 'Load the Pima Indians Diabetes dataset ([https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter09/diabetes.csv](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter09/diabetes.csv))
    using the following lines of code:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码行加载Pima印度糖尿病数据集（[https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter09/diabetes.csv](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter09/diabetes.csv)）：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This results in the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/947371ae-5c7c-4092-9ca9-d99c27df1772.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/947371ae-5c7c-4092-9ca9-d99c27df1772.png)'
- en: We have thus imported `pandas` and read the dataset. In the preceding example,
    we are reading the Pima Indians Diabetes dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经导入了`pandas`并读取了数据集。在之前的示例中，我们正在读取Pima印度糖尿病数据集。
- en: 'We will now split the dataset into two parts, as follows:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将数据集拆分成两部分，如下所示：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After loading the dataset, we divide the dataset into a dependent or label column
    (`target`) and independent or feature columns (`feature_set`). After this, the
    dataset will be broken up into train and test sets. Now, both the dependent and
    independent columns are broken up into train and test sets (`feature_train`, `feature_test`,
    `target_train`, and `target_test`) using `train_test_split()`. `train_test_split()`
    takes dependent and independent DataFrames, `test_size` and `random_state`. Here,
    `test_size` will decide the ratio of the train-test split (that is, `test_size
    0.3` means 30% is the testing set and the remaining 70% of data will be the training
    set), and `random_state` is used as a seed value for reproducing the same data
    split each time. If `random_state` is `None`, then it will randomly split the
    records each time, which will give different performance measures.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集后，我们将数据集分为依赖列或标签列（`target`）和独立列或特征列（`feature_set`）。之后，数据集将被拆分为训练集和测试集。现在，依赖列和独立列将通过`train_test_split()`被拆分为训练集和测试集（`feature_train`、`feature_test`、`target_train`和`target_test`）。`train_test_split()`接受依赖和独立的DataFrame，`test_size`和`random_state`。其中，`test_size`决定了训练集和测试集的比例（例如，`test_size
    0.3`意味着30%是测试集，剩余的70%数据是训练集），`random_state`用作种子值，用于每次重新生成相同的数据拆分。如果`random_state`为`None`，则每次都会随机拆分记录，这会导致不同的性能度量。
- en: 'We will now build the naive Bayes classification model:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将构建朴素贝叶斯分类模型：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we have created a naive Bayes model. First, we will import the `GaussianNB`
    class and create its object or model. This model will fit on the training dataset
    (`feature_train`, `target_train`). After training, the model is ready to make
    predictions using the `predict()` method.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个朴素贝叶斯模型。首先，我们将导入`GaussianNB`类并创建其对象或模型。这个模型将根据训练数据集（`feature_train`、`target_train`）进行拟合。训练完成后，模型准备好使用`predict()`方法进行预测。
- en: 'Finally, we will evaluate the model''s performance:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将评估模型的性能：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This results in the following output:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: scikit-learn's `metrics` class offers various methods for performance evaluation,
    for example, accuracy, precision, recall, and F1-score metrics. These methods
    will take actual target labels (`target_test`) and predicted labels (`predictions`).
    We will understand these metrics in detail in the *Evaluating the classification
    model performance* section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的`metrics`类提供了多种性能评估方法，例如准确率、精确率、召回率和F1分数。这些方法将使用实际目标标签（`target_test`）和预测标签（`predictions`）。我们将在*评估分类模型性能*部分详细了解这些指标。
- en: Naive Bayes is a simple, fast, accurate, and easy-to-understand method for prediction.
    It has a lower computation cost and can work with large datasets. Naive Bayes
    can also be employed in multi-class classification problems. The naive Bayes classifier
    performs better compared to logistic regression when data has a class independence
    assumption.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一种简单、快速、准确且易于理解的预测方法。它具有较低的计算成本，可以处理大型数据集。朴素贝叶斯还可以用于多类分类问题。当数据具有类独立性假设时，朴素贝叶斯分类器比逻辑回归表现更好。
- en: Naive Bayes suffers from the **zero frequency problem**. Zero frequency means
    that if any category in the feature is missing, then it will have a zero frequency
    count. This problem is solved by Laplacian correction. Laplacian correction (or
    Laplace transformation) is a kind of smoothing technique that will add one record
    for each class so that the frequency count for the missing class will become 1,
    thus probabilities of Bayes' theorem will not be affected. Another issue with
    naive Bayes is its assumption of class conditional independence, as it is practically
    impossible for all the predictors to be fully independent. In this section, we
    have learned about naive Bayes classification. Now it's time to learn about the
    decision tree classification algorithm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯存在 **零频率问题**。零频率意味着如果某个特征的类别缺失，则该类别的频率计数为零。这个问题可以通过拉普拉斯修正来解决。拉普拉斯修正（或拉普拉斯变换）是一种平滑技术，会为每个类别添加一条记录，使得缺失类别的频率计数变为
    1，从而不会影响贝叶斯定理中的概率。朴素贝叶斯的另一个问题是其假设类条件独立性，因为在实际情况中，所有预测因子完全独立几乎是不可能的。在这一部分，我们已经学习了朴素贝叶斯分类法。现在是时候学习决策树分类算法了。
- en: Decision tree classification
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树分类
- en: A decision tree is one of the most well-known classification techniques. It
    can be employed for both types of supervised learning problems (classification
    and regression problems). It is a flowchart-like tree structure and mimics human-level
    thinking, which makes it easier to understand and interpret. It also makes you
    see the logic behind the prediction unlike black-box algorithms such as SVMs and
    neural networks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是最著名的分类技术之一。它可以应用于两种类型的监督学习问题（分类问题和回归问题）。决策树是一种类似流程图的树形结构，模仿人类思维方式，这使得它更容易理解和解释。与支持向量机（SVM）和神经网络等“黑箱”算法不同，决策树能让你看到预测背后的逻辑。
- en: 'The decision tree has three basic components: the internal node, the branch,
    and leaf nodes. Here, each terminal node represents a feature, the link represents
    the decision rule or split rule, and the leaf provides the result of the prediction.
    The first starting or master node in the tree is the root node. It partitions
    the data based on features or attributes values. Here, we divide the data and
    again divide the remaining data recursively until all the items refer to the same
    class or there are no more columns left. Decision trees can be employed in both
    types of problems: classification and regression. There are lots of decision tree
    algorithms available, for example, CART, ID3, C4.5, and CHAID. But here, we are
    mainly focusing on CART and ID3 because in scikit-learn, these are the two that
    are available. Let''s see the decision tree classifier generation process in the
    following figure:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有三个基本组件：内部节点、分支和叶子节点。在这里，每个终端节点代表一个特征，链接代表决策规则或分割规则，叶子节点提供预测结果。树中的第一个起始节点或主节点是根节点。它根据特征或属性值来划分数据。在这里，我们分割数据，然后递归地再次划分剩余数据，直到所有项属于同一类别或没有剩余列。决策树可以应用于分类和回归问题。市面上有许多决策树算法，例如
    CART、ID3、C4.5 和 CHAID。但在这里，我们主要关注 CART 和 ID3，因为在 scikit-learn 中，这两种算法是可用的。让我们在下图中看看决策树分类器的生成过程：
- en: '![](img/58f77656-6f19-4456-a98b-9595cf104458.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58f77656-6f19-4456-a98b-9595cf104458.png)'
- en: '**CART** stands for **Classification and Regression Tree**. CART utilizes the
    Gini index for selecting the best column. The Gini index is the difference between
    the sum of the squared probabilities of each class from 1\. The feature or column
    with the minimum Gini index value is selected as the splitting or partition feature.
    The value of the Gini index lies in the range of 0 and 1\. If the Gini index value
    is 0, it indicates that all items belong to one class, and if the Gini index value
    is exactly 1, it indicates that all the elements are randomly distributed. A 0.5
    value of the Gini index indicates the equal distribution of items into some classes:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**CART** 代表 **分类与回归树**。CART 使用基尼指数来选择最佳列。基尼指数是每个类别的概率平方和与 1 的差值。具有最小基尼指数值的特征或列被选为分割或划分特征。基尼指数的值范围在
    0 和 1 之间。如果基尼指数值为 0，表示所有项属于同一类；如果基尼指数值为 1，则表示所有元素是随机分布的。基尼指数值为 0.5 表示项在多个类别中均匀分布：'
- en: '![](img/92f2aebd-a624-4a83-b54d-b065530bfbff.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92f2aebd-a624-4a83-b54d-b065530bfbff.png)'
- en: '**ID3** stands for **Iterative Dichotomiser 3**. It uses information gain or
    entropy as an attribute selection measure. Entropy was invented by Shannon, and
    it measures the amount of impurity or randomness in a dataset. Information gain
    measures the variations between entropy before partition and mean entropy after
    the partition of the dataset for a specific column. The feature or attribute with
    the largest value of information gain will be selected as a splitting feature
    or attribute. If entropy is 0, it indicates that there exists only a single class,
    and if entropy is 1, it indicates that items are equally distributed:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**ID3** 代表 **Iterative Dichotomiser 3（迭代二分法 3）**。它使用信息增益或熵作为属性选择度量。熵由香农提出，用于衡量数据集中的杂乱度或随机性。信息增益衡量在特定列的分割前后，熵的变化量。具有最大信息增益值的特征或属性将被选择为分裂特征或属性。如果熵为
    0，表示只有一个类；如果熵为 1，则表示项目均匀分布：'
- en: '![](img/4797209b-540d-4f34-91e9-daf873cbfb9c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4797209b-540d-4f34-91e9-daf873cbfb9c.png)'
- en: The decision tree is very intuitive and easy to understand, interpret, and explain
    to stakeholders. There is no need to normalize features and distribution-free
    algorithms. Decision trees are also used to predict missing values. They have
    the capability to capture non-linear patterns. Decision trees can overfit and
    are sensitive to noisy data. Decision trees are biased with imbalanced data, which
    is why before applying decision trees, we should balance out the dataset. Decision
    trees are more expensive in terms of time and complexity.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树非常直观，易于理解、解释并向利益相关者讲解。无需对特征进行标准化，并且是无分布假设的算法。决策树也可以用来预测缺失值。它们具备捕捉非线性模式的能力。决策树可能会过拟合，并且对噪声数据敏感。决策树在数据不平衡时有偏差，这也是在应用决策树之前，我们需要平衡数据集的原因。决策树在时间和复杂度上更为昂贵。
- en: 'Let''s work on a decision tree using scikit-learn and perform a prediction
    dataset. After this, we will be ready for the model building:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 scikit-learn 构建决策树并执行预测数据集。之后，我们将准备好构建模型：
- en: First, you need to import `pandas` and load the Pimas dataset using the `read_csv()`
    method that we already saw in the last section.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要导入 `pandas` 并使用我们在上一部分中看到的 `read_csv()` 方法加载 Pimas 数据集。
- en: After this, we need to divide the dataset into training and testing datasets
    similar to what we performed in the preceding section.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将数据集划分为训练集和测试集，类似于我们在前一部分中执行的操作。
- en: 'Now, we will build the decision tree classification model:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将构建决策树分类模型：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we have created a decision tree model. First, we will import the `DecisionTreeClassifier`
    class and create its object or model. This model will fit on the training dataset
    (`feature_train`, `target_train`). After training, the model is ready to make
    predictions using the `predict()` method.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们创建了一个决策树模型。首先，我们将导入 `DecisionTreeClassifier` 类并创建其对象或模型。该模型将在训练数据集（`feature_train`，`target_train`）上进行拟合。训练完成后，模型已准备好使用
    `predict()` 方法进行预测。
- en: 'We will now evaluate the model''s performance:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将评估模型的性能：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This results in the following output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下结果：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding example, model performance is assessed using accuracy, precision,
    recall, and F1-score.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，模型性能通过准确度、精确度、召回率和 F1 分数进行评估。
- en: After getting a full understanding of decision trees, let's move on to the KNN
    classification.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全理解决策树之后，让我们继续进行 KNN 分类。
- en: KNN classification
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN 分类
- en: KNN is a simple, easy-to-comprehend, and easy-to-implement classification algorithm.
    It can also be used for regression problems. KNN can be employed in lots of use
    cases, such as item recommendations and classification problems. Specifically,
    it can suggest movies on Netflix, articles on Medium, candidates on naukari.com,
    products on eBay, and videos on YouTube. In classification, it can be used to
    classify instances such as, for example, banking institutes that can classify
    the loan of risky candidates, or political scientists can classify potential voters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 是一种简单、易于理解和实现的分类算法。它也可以用于回归问题。KNN 可用于许多应用场景，如项目推荐和分类问题。具体来说，它可以在 Netflix
    上推荐电影，在 Medium 上推荐文章，在 naukari.com 上推荐候选人，在 eBay 上推荐产品，在 YouTube 上推荐视频。在分类中，它可以用于分类实例，例如银行机构可以对风险较大的贷款候选人进行分类，或者政治学家可以对潜在选民进行分类。
- en: KNN has three basic properties, which are non-parametric, lazy learner, and
    instance-based learning. Non-parametric means the algorithm is distribution-free
    and there is no need for parameters such as mean and standard deviation. Lazy
    learner means KNN does not train the model; that is, the model is trained in the
    testing phase. This makes for faster training but slower testing. It is also more
    time- and memory-consuming. Instance-based learning means the predicted outcome
    is based on the similarity with its nearest neighbors. It does not create any
    abstract equations or rules for prediction; instead, it stores all the data and
    queries each record.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 有三个基本属性，分别是非参数性、懒惰学习者和基于实例的学习。非参数性意味着算法不依赖于分布，因此不需要像均值和标准差这样的参数。懒惰学习者意味着
    KNN 不进行模型训练；也就是说，模型是在测试阶段进行训练的。这使得训练速度更快，但测试速度较慢，并且也更加耗时和占用内存。基于实例的学习意味着预测结果是基于与其最近邻的相似度。它不会为预测创建抽象的方程或规则，而是存储所有数据并查询每个记录。
- en: 'The KNN classification algorithm finds the *k* most similar instances from
    the training dataset and the majority decides the predicted label of the given
    input features. The following steps will be performed by the KNN classifier to
    make predictions:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 分类算法从训练数据集中找到 *k* 个最相似的实例，且多数决定了给定输入特征的预测标签。KNN 分类器将执行以下步骤来进行预测：
- en: Compute the distance for an input observation with all the observations in the
    training dataset.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输入观察与训练数据集中所有观察的距离。
- en: Find the *K* top closest neighbors by sorting the distance with all the instances
    in ascending order.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过按距离对所有实例进行升序排序，找到 *K* 个最接近的邻居。
- en: Perform voting on the *K* top closest neighbors and predict the label with the
    majority of votes.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 *K* 个最接近的邻居进行投票，并预测获得最多票数的标签。
- en: 'This is better represented using the following diagram:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程可以通过以下图示来更好地展示：
- en: '![](img/90a682db-5fe7-44c1-88cb-918b3096f997.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90a682db-5fe7-44c1-88cb-918b3096f997.png)'
- en: 'Let''s work on a KNN classifier using scikit-learn and perform a prediction
    on a dataset:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 scikit-learn 构建一个 KNN 分类器，并对数据集进行预测：
- en: Load the Pima Indians Diabetes dataset.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载皮马印第安糖尿病数据集。
- en: First, you need to import `pandas` and load the dataset using the `read_csv()`
    method that we have already seen in the *Naive Bayes classification* session.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要导入 `pandas` 并使用我们在 *朴素贝叶斯分类* 课程中已经看到的 `read_csv()` 方法加载数据集。
- en: Split the dataset.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 划分数据集。
- en: After this, we need to break down the dataset into two sets – a training and
    a testing set – as we did in the *Naive Bayes classification* section.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们需要将数据集分成两部分——训练集和测试集——就像我们在 *朴素贝叶斯分类* 部分所做的那样。
- en: Build the KNN classification model.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建 KNN 分类模型。
- en: 'Now, we are ready for the model building:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备开始构建模型：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code block, we imported the `KNeighborsClassifier` class and
    created its object or model. Here, we have taken 3 neighbors as an input parameter
    to the model. If we do not specify the number of neighbors as an input parameter,
    then the model will choose 5 neighbors by default. This model will fit on the
    training dataset (`feature_train`, `target_train`). After training, the model
    is ready to make predictions using the `predict()` method.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们导入了 `KNeighborsClassifier` 类并创建了它的对象或模型。在这里，我们将 3 个邻居作为模型的输入参数。如果我们没有指定邻居的数量，模型将默认选择
    5 个邻居。该模型将拟合训练数据集（`feature_train`, `target_train`）。训练完成后，模型已准备好使用 `predict()`
    方法进行预测。
- en: 'Evaluate the model''s performance:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型性能：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This results in the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding example, model performance is assessed using accuracy, precision,
    recall, and F1-score.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，模型性能通过准确率、精确度、召回率和 F1 分数进行评估。
- en: After understanding the KNN classification algorithm, it's time to learn about
    the SVM classification algorithm.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了 KNN 分类算法之后，接下来是学习 SVM 分类算法。
- en: SVM classification
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVM 分类
- en: SVMs are the most preferred and favorite machine learning algorithms by many
    data scientists due to their accuracy with less computation power. They are employed
    for both regression and classification problems. They also offer a kernel trick
    to model non-linear relationships. SVM has a variety of use cases, such as intrusion
    detection, text classification, face detection, and handwriting recognition.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其较少的计算资源需求和较高的准确性，SVM是许多数据科学家最喜欢的机器学习算法。它们被用于回归和分类问题，并提供了核技巧来建模非线性关系。SVM有多种应用场景，例如入侵检测、文本分类、人脸识别和手写识别。
- en: SVM is a discriminative model that generates optimal hyperplanes with a large
    margin in n-dimensional space to separate data points. The basic idea is to discover
    the **Maximum Marginal Hyperplane** (**MMH**) that perfectly separates data into
    given classes. The maximum margin means the maximum distance between data points
    of both classes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是一个判别模型，它在n维空间中生成具有大间隔的最优超平面来分隔数据点。其基本思想是发现**最大间隔超平面**（**MMH**），该超平面完美地将数据分成不同类别。最大间隔意味着两类数据点之间的最大距离。
- en: Terminology
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语
- en: 'We will now explore some of the terminology that goes into SVM classification:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨一些与SVM分类相关的术语：
- en: '**Hyperplane**: Hyperplane is a decision boundary used to distinguish between
    two classes. Hyperplane dimensionality is decided by the number of features. It
    is also known as a decision plane.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超平面**：超平面是用来区分两类的决策边界。超平面的维度由特征的数量决定。它也被称为决策平面。'
- en: '**Support** **vectors**: Support vectors are the closest points to the hyperplane
    and help in the orientation of the hyperplane by maximizing the margin.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量**：支持向量是距离超平面最近的点，它们通过最大化间隔来帮助确定超平面的方向。'
- en: '**Margin**: Margin is the maximum gap between the closest points. The larger
    the margin, the better the classification is considered. The margin can be calculated
    by the perpendicular distance from the support vector line.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**间隔**：间隔是距离最近点的最大距离。间隔越大，分类效果越好。间隔可以通过从支持向量线到超平面的垂直距离来计算。'
- en: 'The core objective of an SVM is to choose the hyperplane with the largest possible
    boundary between support vectors. The SVM finds the MMH in the following two stages:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的核心目标是选择具有最大可能边界的超平面，边界由支持向量之间的距离决定。SVM通过以下两个阶段来找到MMH：
- en: Create hyperplanes that separate the data points in the best possible manner.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建超平面，以最佳方式分隔数据点。
- en: 'Select the hyperplane with maximum margin hyperplane:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最大间隔的超平面：
- en: '![](img/18fb04d2-0af9-45d5-a9b4-5e492df75aa5.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18fb04d2-0af9-45d5-a9b4-5e492df75aa5.png)'
- en: The SVM algorithm is a faster and more accurate classifier compared to naive
    Bayes. It performs better with a larger margin of separation. SVM is not favorable
    for large datasets. Its performance also depends upon the type of kernel used.
    It performs poorly with overlapping classes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与朴素贝叶斯相比，SVM算法是一种更快、更准确的分类器。它在较大的分隔间隔下表现更好。SVM不适用于大规模数据集。其性能也依赖于所使用的核函数类型。对于重叠类，它的表现较差。
- en: 'Let''s work on support vector classifiers using `scikit-learn` and perform
    a prediction dataset. After this, we will divide the dataset into two sets of
    training and testing sets as we did in the *Naive Bayes classification* section.
    After this, we are ready with the model building:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`scikit-learn`进行支持向量分类器的工作，并执行一个预测数据集。完成后，我们将数据集划分为训练集和测试集，就像我们在*朴素贝叶斯分类*部分所做的那样。接下来，我们准备好进行模型构建：
- en: Load the Pima Indians Diabetes dataset.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载皮马印第安人糖尿病数据集。
- en: First, you need to import `pandas` and load the dataset using the `read_csv()`
    method that we already saw in the *Naive Bayes classification* session.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要导入`pandas`并使用我们在*朴素贝叶斯分类*环节中已经看到的`read_csv()`方法加载数据集。
- en: Split the dataset.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 划分数据集。
- en: After this, we need to break the dataset up into two sets – a training and testing
    set – as we did in the *naive Bayes classification* section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们需要将数据集拆分为两个集合——训练集和测试集——就像我们在*朴素贝叶斯分类*部分所做的那样。
- en: Build the SVM classification model.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建SVM分类模型。
- en: 'Now, we are ready with the model building:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好开始模型构建：
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code block, we will import the `svm` module and create its
    `svm.SVC()` object or model. Here, we have passed the `linear` kernel. You can
    also pass another kernel, such as **`poly`**, **`rbf`**, or `sigmoid`. If we don't
    specify the kernel, then it will select `rbf` by default as the kernel. The linear
    kernel will create a linear hyperplane to separate diabetic and non-diabetic patients.
    This model will fit on the training dataset (`feature_train`, `target_train`).
    After training, the model is ready to make predictions using the `predict()` method.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码块中，我们将导入 `svm` 模块并创建其 `svm.SVC()` 对象或模型。 这里我们传递了 `linear` 核。 您还可以传递另一个核，例如
    **`poly`**，**`rbf`** 或 `sigmoid`。 如果我们不指定核心，则默认选择 `rbf` 作为核心。 线性核将创建一个线性超平面来区分糖尿病患者和非糖尿病患者。
    该模型将适合于训练数据集（`feature_train`，`target_train`）。 训练后，模型已准备使用 `predict()` 方法进行预测。
- en: 'Evaluate the model''s performance:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型的性能：
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the following output:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding example, model performance will be assessed using metrics such
    as accuracy, precision, recall, and F1-score. After understanding all these classifiers,
    it's time to see the training and testing set splitting strategies.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，将使用准确性、精确度、召回率和 F1 分数等指标评估模型的性能。 理解所有这些分类器后，现在是时候看看训练和测试集分割策略了。
- en: Splitting training and testing sets
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割训练和测试集
- en: 'Data scientists need to assess the performance of a model, overcome overfitting,
    and tune the hyperparameters. All these tasks require some hidden data records
    that were not used in the model development phase. Before model development, the
    data needs to be divided into some parts, such as train, test, and validation
    sets. The training dataset is used to build the model. The test dataset is used
    to assess the performance of a model that was trained on the train set. The validation
    set is used to find the hyperparameters. Let''s look at the following strategies
    for the train-test split in the upcoming subsections:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家需要评估模型的性能，克服过拟合并调整超参数。 所有这些任务都需要一些未在模型开发阶段使用的隐藏数据记录。 在模型开发之前，数据需要分成一些部分，例如训练、测试和验证集。
    训练数据集用于构建模型。 测试数据集用于评估在训练集上训练的模型的性能。 验证集用于找到超参数。 让我们看看下面的训练-测试分割策略：
- en: Holdout method
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留样本法
- en: K-fold cross-validation
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K 折交叉验证
- en: Bootstrap method
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bootstrap 方法
- en: Holdout
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保留样本法
- en: 'In this method, the dataset is divided randomly into two parts: a training
    and testing set. Generally, this ratio is 2:1, which means 2/3 for training and
    1/3 for testing. We can also split it into different ratios, such as 6:4, 7:3,
    and 8:2:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在此方法中，数据集被随机分成两部分：训练集和测试集。 通常，这个比例是2:1，即2/3 用于训练，1/3 用于测试。 我们还可以将其分割成不同的比例，如6:4、7:3
    和 8:2：
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding example, `test_size=0.3` represents 30% for the testing set
    and 70% for the training set. `train_test_split()` splits the dataset into 7:3.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，`test_size=0.3` 表示测试集占30%，训练集占70%。 `train_test_split()` 将数据集分割为7:3。
- en: K-fold cross-validation
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K 折交叉验证
- en: 'In this approach, the data is split into *k* partitions of approximately equal
    size. It will train *k* models and evaluate them using each partition. In each
    iteration, one partition will hold for testing, and the remaining *k* partitions
    are collectively used for training purposes. Classification accuracy will be the
    average of all accuracies. It also ensures that the model is not overfitting:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在此方法中，数据分为大致相同大小的 *k* 个分区。 它将训练 *k* 个模型，并使用每个分区进行评估。 在每次迭代中，一个分区将保留用于测试，其余的
    *k* 个分区将集体用于训练。 分类准确度将是所有准确度的平均值。 它还确保模型不会过拟合：
- en: '![](img/0203d2e5-87ec-4e32-981f-1a9c10575719.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0203d2e5-87ec-4e32-981f-1a9c10575719.png)'
- en: In stratified cross-validation, *k* partitions are divided with approximately
    the same class distribution. This means it preserves the percentages of each class
    in each partition.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在分层交叉验证中，*k* 个分区大致相同的类分布。 这意味着它保留每个分区中每个类的百分比。
- en: Bootstrap method
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bootstrap 方法
- en: 'Bootstrap is a resampling technique. It performs a sampling iteratively from
    the dataset with replacement. Sampling with replacement will make random selections.
    It requires the size of the sample and the number of iterations. In each iteration,
    it uniformly selects the records. Each record has equal chances of being selected
    again. The samples that are not selected are known as "out-of-bag" samples. Let''s
    understand bootstrap using the following diagram:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 自助法是一种重采样技术。它从数据集中反复进行有放回的抽样。有放回的抽样会进行随机选择。它需要样本的大小和迭代次数。在每次迭代中，它会均匀选择记录。每条记录被选择的机会相同。未被选择的样本称为“袋外”样本。我们可以通过以下图表来理解自助法：
- en: '![](img/26693f60-98b4-4383-b4bf-49da50df23f1.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26693f60-98b4-4383-b4bf-49da50df23f1.png)'
- en: In the preceding diagram, we can see that each element has an equal chance of
    selection in each bootstrap sample. Let's jump to another important topic of classification,
    which is classification model evaluation. The next topic helps us to assess the
    performance of the classification model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图表中，我们可以看到每个元素在每次自助采样中都有相同的选择机会。接下来，我们将跳到分类的另一个重要话题——分类模型评估。下一个话题帮助我们评估分类模型的表现。
- en: Evaluating the classification model performance
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估分类模型的表现
- en: Up to now, we have learned how to create classification models. Creating a machine
    learning classification model is not enough; as a business or data analyst, you
    also want to assess its performance so that you can deploy it in live projects.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何创建分类模型。创建机器学习分类模型还不够；作为业务或数据分析师，你还需要评估它的表现，以便可以将其部署到实际项目中。
- en: scikit-learn offers various metrics, such as a confusion matrix, accuracy, precision,
    recall, and F1-score, to evaluate the performance of a model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了各种度量标准，如混淆矩阵、准确率、精确度、召回率和 F1 分数，用于评估模型的表现。
- en: Confusion matrix
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'A confusion matrix is an approach that gives a brief statement of prediction
    results on a binary and multi-class classification problem. Let''s assume we have
    to find out whether a person has diabetes or not. The concept behind the confusion
    matrix is to find the number of right and mistaken forecasts, which are further
    summarized and separated into each class. It clarifies all the confusion related
    to the performance of our classification model. This 2x2 matrix not only shows
    the error being made by our classifier but also represents what sort of mistakes
    are being made. A confusion matrix is used to make a complete analysis of statistical
    data faster and also make the results more readable and understandable through
    clear data visualization. It contains two rows and columns, as shown in the following
    list. Let''s understand the basic terminologies of the confusion matrix:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一种简要说明二分类和多分类问题预测结果的方法。假设我们要找出一个人是否患有糖尿病。混淆矩阵的概念是找出正确预测和错误预测的数量，然后将它们进一步总结并分开到每个类别中。它阐明了与我们分类模型表现相关的所有混淆信息。这个
    2x2 矩阵不仅显示了分类器所犯的错误，还展示了犯的是什么类型的错误。混淆矩阵用于更快速地完成统计数据分析，并通过清晰的数据可视化使结果更具可读性和易理解性。它包含两行两列，如下所示。让我们理解混淆矩阵的基本术语：
- en: '**True Positive** (**TP**): This represents cases that are forecasted as `Yes`
    and in reality, the cases are `Yes`; for example, we have forecasted them as fraudulent
    cases and in reality, they are fraudulent.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性** (**TP**)：这代表预测为`是`，且实际情况也是`是`的案例；例如，我们预测它们为欺诈案例，实际上它们确实是欺诈案例。'
- en: '**True Negative** (**TN**): This represents cases that are forecasted as `No`
    and in reality, the cases are `No`; for example, we have forecasted them as non-fraudulent
    cases and in reality, they are non-fraudulent.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性** (**TN**)：这代表预测为`不是`，且实际情况也是`不是`的案例；例如，我们预测它们为非欺诈案例，实际上它们确实是非欺诈案例。'
- en: '**False Positive** (**FP**): This represents cases that are forecasted as `Yes`
    and in reality, the cases are `No`; for example, we have forecasted them as fraudulent
    cases and in reality, they are not fraudulent. This type of incident class represents
    a Type I error.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性** (**FP**)：这代表预测为`是`，但实际情况是`不是`的案例；例如，我们预测它们为欺诈案例，但实际上它们不是欺诈案例。这种类型的事件类别表示类型
    I 错误。'
- en: '**False Negative** (**FN**): This represents cases that are forecasted as `No`
    and in reality, the cases are `No`; for example, we have forecasted them as non-fraudulent
    cases and in reality, they are fraudulent. This type of incident class represents
    a Type II error.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**（**FN**）：这表示那些预测为`否`，但实际上是`否`的案例；例如，我们预测它们为非欺诈案件，实际上它们是欺诈案件。这种类型的事件类别表示的是第二类错误。'
- en: 'Let''s take an example of a fraud detection problem:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个欺诈检测问题为例：
- en: '![](img/db102069-a0c6-4c33-ab1a-c02bfef8f809.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db102069-a0c6-4c33-ab1a-c02bfef8f809.png)'
- en: 'In the preceding example, we have taken two classes of fraud: Yes and No. Yes
    indicates fraudulent activity and No indicates non-fraudulent activity. The total
    number of predicted records is 825, which means 825 transactions were tested.
    In all these 825 cases, the model or classifier forecasted 550 times Yes and 275
    times No. In reality, actual fraudulent cases are 525 and non-fraudulent cases
    are 300.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们考虑了两类欺诈行为：是和否。是表示欺诈行为，否表示非欺诈行为。预测记录的总数为825，这意味着测试了825笔交易。在这825个案例中，模型或分类器预测了550次是，275次否。实际上，实际的欺诈案件为525个，非欺诈案件为300个。
- en: 'Let''s create a confusion matrix in Python using scikit-learn:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用scikit-learn在Python中创建一个混淆矩阵：
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This results in the following output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/2d68507c-9909-4826-9122-cc0af782847c.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d68507c-9909-4826-9122-cc0af782847c.png)'
- en: 'In the preceding example, we have loaded the data and divided the data into
    two parts: training and testing sets. After this, we performed model training
    using logistic regression as we did in the previous chapter. Here, to plot the
    confusion matrix, we have used the `plot_confusion_matrix()` method with the model
    object, testing feature set, testing label set, and `values_format` parameters.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们加载了数据并将其分为两个部分：训练集和测试集。之后，我们使用逻辑回归进行了模型训练，正如在前一章中所做的那样。在这里，为了绘制混淆矩阵，我们使用了`plot_confusion_matrix()`方法，传入了模型对象、测试特征集、测试标签集和`values_format`参数。
- en: Accuracy
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准确率
- en: 'Now, we will find the accuracy of the model calculated from the confusion matrix.
    It tells us how accurate our predictive model is:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算从混淆矩阵得出的模型准确率。它告诉我们预测模型的准确性：
- en: '![](img/7e728bf1-6dd0-48b2-b96b-f24ff72cb7fb.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e728bf1-6dd0-48b2-b96b-f24ff72cb7fb.png)'
- en: '![](img/8c6821d9-986d-4396-bf62-0b9a4a4ba929.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c6821d9-986d-4396-bf62-0b9a4a4ba929.png)'
- en: Precision
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确度
- en: 'When the model predicted `Yes`, how often was it correct? This is the percentage
    of positive cases out of the total predicted cases in the dataset. In simple terms,
    we can understand precision as "Up to what level our model is right when it says
    it''s right":'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型预测为`是`时，它的正确率有多高？这是数据集中总预测案例中正类案例的百分比。简而言之，我们可以将精确度理解为“当模型说它正确时，它到底有多正确”：
- en: '![](img/03e9f997-f47f-44cd-86d2-7f40f66a4e11.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03e9f997-f47f-44cd-86d2-7f40f66a4e11.png)'
- en: '![](img/e43eede2-9848-4347-83c5-a83c8acb6087.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e43eede2-9848-4347-83c5-a83c8acb6087.png)'
- en: Recall
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 召回率
- en: 'When it is actually `Yes`, how often did the model predict `Yes`? This is also
    known as sensitivity. This is the percentage of positive cases out of all the
    total actual cases present in the dataset:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当实际为`是`时，模型预测`是`的频率是多少？这也被称为灵敏度。它是数据集中所有实际正类案例中，预测为正类的比例：
- en: '![](img/a9bdced7-b936-4517-88b1-f0bf3592e0c0.png)![](img/ad712a56-df69-4cae-bcf3-6a1044ab7692.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9bdced7-b936-4517-88b1-f0bf3592e0c0.png)![](img/ad712a56-df69-4cae-bcf3-6a1044ab7692.png)'
- en: F-measure
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F-measure（F值）
- en: 'F-measure is considered as one of the better ways to assess the model. In lots
    of areas of data science, competition model performance is assessed using F-measure.
    It is a harmonic mean of precision and recall. The higher the value of the F1-score,
    the better the model is considered. F1-score provides equal weightage to precision
    and recall, which means it indicates a balance between both:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: F-measure被认为是评估模型的较好方式之一。在数据科学的许多领域，竞争模型的性能都是通过F-measure来评估的。它是精确度和召回率的调和均值。F1分数的值越高，模型越好。F1分数为精确度和召回率赋予相等的权重，这意味着它表示两者之间的平衡：
- en: '![](img/f3139cdd-3edf-4e89-8059-487ef59d4b49.png)![](img/6caa7b33-dbe8-4d9d-b552-2054cb9cd1e4.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3139cdd-3edf-4e89-8059-487ef59d4b49.png)![](img/6caa7b33-dbe8-4d9d-b552-2054cb9cd1e4.png)'
- en: One drawback of F-measure is that it assigns equal weightage to precision and
    recall but in some examples, one needs to be higher than the other, which is the
    reason why the F1-score may not be an exact metric.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: F-measure的一个缺点是它对精确度和召回率赋予相同的权重，但在某些情况下，需要其中一个高于另一个，这也是为什么F1分数可能不是一个准确的度量标准。
- en: In the preceding sections, we have seen classification algorithms such as naive
    Bayes, decision trees, KNN, and SVMs. We have assessed the model performance using
    scikit-learn's `accuracy_score()` for model accuracy, `precision_score()` for
    model precision, `recall_score()` for model recall, and `f1_score()` for model
    F1-score.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了诸如朴素贝叶斯、决策树、KNN 和 SVM 等分类算法。我们通过使用 scikit-learn 的 `accuracy_score()`
    来评估模型准确率，`precision_score()` 来评估模型精度，`recall_score()` 来评估模型召回率，以及 `f1_score()`
    来评估模型的 F1 分数。
- en: 'We can also print the classification report to dig down into the details to
    understand the classification model. Let''s create the confusion report:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印分类报告，深入了解分类模型的细节。让我们创建混淆报告：
- en: '[PRE16]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following output:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/dc2c28e2-df40-4598-9a31-ff995bca8173.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc2c28e2-df40-4598-9a31-ff995bca8173.png)'
- en: In the preceding code, we have printed the confusion matrix report using the
    `confusion_report()` method with test set labels, prediction set or predicted
    labels, and target value list parameters.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 `confusion_report()` 方法打印了混淆矩阵报告，传入了测试集标签、预测集或预测标签以及目标值列表参数。
- en: ROC curve and AUC
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC 曲线和 AUC
- en: 'AUC-ROC curve is a tool to measure and assess the performance of classification
    models. **ROC** (**Receiver Operating Characteristics**) is a pictorial visualization
    of model performance. It plots a two-dimensional probability plot between the
    FP rate (or 1-specificity) and the TP rate (or sensitivity). We can also represent
    the area covered by a model with a single number using AUC:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: AUC-ROC 曲线是衡量和评估分类模型性能的工具。**ROC**（**接收者操作特性**）是模型性能的图示化表示。它绘制了 FP 率（或 1-特异性）与
    TP 率（或灵敏度）之间的二维概率图。我们还可以使用 AUC 用一个数字表示模型所覆盖的区域：
- en: '![](img/2eadc2fe-ce50-4b8f-b875-d0ffe2e3eef1.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2eadc2fe-ce50-4b8f-b875-d0ffe2e3eef1.png)'
- en: 'Let''s create the ROC curve using the scikit-learn module:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 scikit-learn 模块创建 ROC 曲线：
- en: '[PRE17]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This results in the following output:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/bf293445-39ad-44e0-b00a-236e8908669f.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf293445-39ad-44e0-b00a-236e8908669f.png)'
- en: In the preceding example, We have drawn the ROC plot `plot_roc_curve()` method
    with model object, testing feature set, and testing label set parameters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们使用模型对象、测试特征集和测试标签集参数绘制了 `plot_roc_curve()` 方法的 ROC 图。
- en: 'In the ROC curve, the AUC is a measure of divisibility. It tells us about the
    model''s class distinction capability. The higher the AUC value, the better the
    model is at distinguishing between "fraud" and "not fraud." For an ideal classifier,
    the AUC is equal to 1:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ROC 曲线中，AUC 是一种可分性度量。它告诉我们模型的类区分能力。AUC 值越高，模型在区分“欺诈”和“非欺诈”方面就越好。对于理想的分类器，AUC
    等于 1：
- en: '![](img/6dcb60fc-8600-4651-8ef5-ae66d3c78f63.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6dcb60fc-8600-4651-8ef5-ae66d3c78f63.png)'
- en: 'Let''s compute an AUC score as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算 AUC 分数，如下所示：
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This results in the following output:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: scikit-learn's `metrics` class offers an AUC performance evaluation measure.
    `roc_auc_score()` methods will take actual labels (`y_test`) and predicted probability
    (`y_pred_prob`).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的 `metrics` 类提供了 AUC 性能评估度量。`roc_auc_score()` 方法将接受实际标签（`y_test`）和预测概率（`y_pred_prob`）。
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discovered classification, its techniques, the train-test
    split strategy, and performance evaluation measures. This will benefit you in
    gaining an important skill for predictive data analysis. You have seen how to
    develop linear and non-linear classifiers for predictive analytics using scikit-learn.
    In the earlier topics of the chapter, you got an understanding of the basics of
    classification and machine learning algorithms, such as naive Bayes classification,
    decision tree classification, KNN, and SVMs. In later sections, you saw data splitting
    approaches and model performance evaluation measures such as accuracy score, precision
    score, recall score, F1-score, ROC curve, and AUC score.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了分类及其技术、训练-测试划分策略和性能评估指标。这将帮助你掌握预测数据分析的重要技能。你已经学会了如何使用 scikit-learn
    开发线性和非线性分类器进行预测分析。在本章的前面部分，你了解了分类基础和机器学习算法，例如朴素贝叶斯分类、决策树分类、KNN 和 SVM。后续章节中，你看到了数据划分方法和模型性能评估指标，例如准确度分数、精度分数、召回率、F1
    分数、ROC 曲线和 AUC 分数。
- en: The next chapter, [Chapter 11](cb4ebee8-1420-48f2-ad5d-6e49f241e9e2.xhtml),
    *Unsupervised Learning – PCA and Clustering*, will concentrate on the important
    topics of unsupervised machine learning techniques and dimensionality reduction
    techniques in Python. The chapter starts with dimension reduction and principal
    component analysis. In the later sections of the chapter, the focus will be on
    clustering methods such as k-means, hierarchical, DBSCAN, and spectral clustering.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章，[第11章](cb4ebee8-1420-48f2-ad5d-6e49f241e9e2.xhtml)，*无监督学习——主成分分析与聚类*，将重点讲解无监督机器学习技术和降维技术在
    Python 中的应用。章节开始介绍降维和主成分分析。在接下来的部分中，将聚焦于聚类方法，如 k-means、层次聚类、DBSCAN 和谱聚类。
