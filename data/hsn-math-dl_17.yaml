- en: Geometric Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 几何深度学习
- en: Throughout this book, we have learned about various types of neural networks
    that are used in deep learning, such as convolutional neural networks and recurrent
    neural networks, and they have achieved some tremendous results in a variety of
    tasks, such as computer vision, image reconstruction, synthetic data generation,
    speech recognition, language translation, and so on. All of the models we have
    looked at so far have been trained on Euclidean data, that is, data that can be
    represented in grid (matrix) format—images, text, audio, and so on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们学习了多种用于深度学习的神经网络类型，如卷积神经网络和循环神经网络，并且它们在各种任务中取得了惊人的成果，如计算机视觉、图像重建、合成数据生成、语音识别、语言翻译等。我们迄今为止研究的所有模型都在欧几里得数据上进行训练，即可以以网格（矩阵）格式表示的数据——图像、文本、音频等。
- en: However, many of the tasks that we would like to apply deep learning to use
    non-Euclidean data (more on this shortly) – the kind that the neural networks
    we have come across so far are unable to process and deal with. This includes
    dealing with sensor networks, mesh surfaces, point clouds, objects (the kind used
    in computer graphics), social networks, and so on. In general, geometric deep
    learning is designed to help deep neural networks generalize to graphs and manifolds
    (we learned about graphs in [Chapter 5](758f1209-7a1d-474c-b494-bbf905a25afd.xhtml),
    *Graph Theory*, and we will learn about manifolds shortly in this chapter).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们希望将深度学习应用于的许多任务使用的是非欧几里得数据（稍后会详细讨论）——这类数据是我们到目前为止接触到的神经网络无法处理的。这包括处理传感器网络、网格表面、点云、物体（计算机图形学中使用的那种）、社交网络等等。一般来说，几何深度学习旨在帮助深度神经网络在图形和流形上进行泛化（我们在[第5章](758f1209-7a1d-474c-b494-bbf905a25afd.xhtml)中学习了图论，接下来在本章中我们将学习流形）。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将覆盖以下主题：
- en: Comparing Euclidean and non-Euclidean data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较欧几里得数据与非欧几里得数据
- en: Graph neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图神经网络
- en: Spectral graph CNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谱图CNNs
- en: Mixture model networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合模型网络
- en: Facial recognition in 3D
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3D人脸识别
- en: Let's get started!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Comparing Euclidean and non-Euclidean data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较欧几里得数据与非欧几里得数据
- en: Before we learn about geometric deep learning techniques, it is important for
    us to understand the differences between Euclidean and non-Euclidean data, and
    why we need a separate approach to deal with it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习几何深度学习技术之前，理解欧几里得数据和非欧几里得数据之间的差异以及为什么我们需要一种单独的方法来处理它非常重要。
- en: Deep learning architectures such as FNNs, CNNs, and RNNs have proven successful
    for a variety of tasks, such as speech recognition, machine translation, image
    reconstruction, object recognition and segmentation, and motion tracking, in the
    last 8 years. This is because of their ability to exploit and use the local statistical
    properties that exist within data. These properties include stationarity, locality,
    and compositionality. In the case of CNNs, the data they take as input can be
    represented in a grid form (such as images, which can be represented by matrices
    and tensors).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习架构，如FNNs、CNNs和RNNs，在过去的8年中已证明在各种任务中取得了成功，如语音识别、机器翻译、图像重建、物体识别与分割以及运动追踪。这是因为它们能够利用和使用数据中存在的局部统计属性。这些属性包括平稳性、局部性和组合性。以CNNs为例，它们所接受的输入数据可以以网格形式表示（例如图像，可以用矩阵和张量表示）。
- en: 'The stationarity, in this case (images), comes from the fact that CNNs have
    the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下（图像），平稳性来自于CNNs具备以下特性：
- en: Shift invariance, owing to the use of convolutions.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移位不变性，归功于卷积的使用。
- en: The locality can be attributed to local connectivity since the kernels are observing
    not just singular pixels but neighboring pixels as well.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部性可归因于局部连接性，因为卷积核不仅观察单个像素，还会观察相邻像素。
- en: The compositionality comes from it being made up of multiple scales (or hierarchies)
    where simpler structures are combined to represent more abstract structures.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合性来自于它由多个尺度（或层次）组成，其中简单的结构被组合在一起以表示更抽象的结构。
- en: However, not all data can be expressed in the format required for deep neural
    networks, and if it can be contorted into grid form, this means that we have had
    to sacrifice a lot of the relationships that existed in the complex data in favor
    of a much more simple representation that our neural networks can take as input.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有数据都能以深度神经网络所需的格式表达，如果数据能够被扭曲成网格形态，这意味着我们不得不牺牲复杂数据中存在的许多关系，转而使用一种更简单的表示方式，以便神经网络能够将其作为输入。
- en: These three properties limit what neural networks can learn and the kinds of
    problems we can use them for.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个属性限制了神经网络能够学习的内容，以及我们可以使用它们解决的各种问题。
- en: A lot of the data that exists in the real world, as you may have guessed, cannot
    be properly captured in a grid. This kind of data can, however, be represented
    using graphs or manifolds. Examples of data that can be represented by graphs
    include social networks, academic paper citation networks, communication networks,
    knowledge graphs, molecules, and road maps. On the other hand, we can make use
    of Riemannian manifolds (more on this in the next section) to represent three-dimensional
    objects (which are volumetric) such as animals, human bodies, faces, airplanes,
    chairs, and so on. In a nutshell, both are methods of capturing the relationships
    that may exist between nodes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经猜到，现实世界中许多数据无法被恰当地捕捉到网格中。然而，这种数据可以通过图或流形来表示。可以通过图表示的数据的例子包括社交网络、学术论文引用网络、通信网络、知识图谱、分子和道路地图。另一方面，我们可以利用黎曼流形（下一节将详细介绍）来表示三维物体（即体积物体），如动物、人体、面孔、飞机、椅子等。简而言之，两者都是捕捉节点之间可能存在关系的方法。
- en: This type of data is difficult for neural networks to deal with because it lacks
    the structure they are used to being fed during training. For example, we may
    want to use the weight (or strength) between two nodes to represent the closeness
    of two people in a social network. In this scenario, we would do this to make
    a suggestion regarding a new friend for a user to add to their existing network.
    However, there is no straightforward method we can use to represent this information
    in a feature vector.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的数据对神经网络来说很难处理，因为它缺乏神经网络在训练过程中习惯处理的结构。例如，我们可能想通过两个节点之间的权重（或强度）来表示社交网络中两个人的亲密程度。在这种情况下，我们可能会这样做，以便为用户提供一个新朋友的建议，供他们添加到现有的网络中。然而，我们没有简单的方法来将这些信息表示为特征向量。
- en: Before we learn about the methods used in geometric deep learning, let's learn
    what exactly a manifold is.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习几何深度学习中使用的方法之前，首先让我们了解一下什么是流形。
- en: Manifolds
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流形
- en: A **manifold** is any topological space where, in the neighborhood of any point, (*p*),
    it is topologically equivalent (or homeomorphic) to a *k*-dimensional Euclidean
    space. We encountered the term manifold earlier in this book, but we didn't define
    it properly, so we will do that now. The preceding definition probably sounds
    a bit daunting, but it will make a lot more sense in a moment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**流形** 是任何一个拓扑空间，其中在任何点（*p*）的邻域内，它在拓扑上等价于（或同胚于）一个 *k* 维欧几里得空间。我们在本书的早些时候遇到过流形这个词，但当时并没有正确地定义它，所以现在我们将进行定义。前面的定义可能听起来有些令人生畏，但稍后它会变得更容易理解。'
- en: Suppose we have a one-dimensional manifold. For simplicity, we will work with
    a circle, or a disk, (which we denote as *S¹*) that exists in [![](img/4e3146ed-66b4-497c-9b63-51b8c8ba347e.png)] (there
    are other one-dimensional manifolds, as well, such as parabolas, hyperbolas, and
    cubic curves, but that doesn't concern us right now).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个一维流形。为了简化，我们将使用一个圆圈或圆盘（我们用*S¹*表示），它存在于[![](img/4e3146ed-66b4-497c-9b63-51b8c8ba347e.png)]（还有其他一维流形，比如抛物线、双曲线和立方曲线，但目前我们不讨论这些）。
- en: 'Let''s suppose we have the following manifold:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下流形：
- en: '![](img/f19d5c8b-4fee-44ad-b004-1007d3b26c0b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f19d5c8b-4fee-44ad-b004-1007d3b26c0b.png)'
- en: 'Now, if we were to zoom into the curve of the circle on the top-left quadrant
    a bunch of times, eventually, we would arrive at a magnification where the curve
    appears to look like a straight line (sort of like a tangent at that point):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们反复放大左上角象限中的圆弧，最终我们会到达一个放大程度，在这个程度下，圆弧看起来像一条直线（有点像那个点的切线）：
- en: '![](img/7fbac093-a8ff-4737-8fa9-a4501890b4f5.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fbac093-a8ff-4737-8fa9-a4501890b4f5.png)'
- en: As shown in the preceding image, we have zoomed into the top left and have drawn
    a tangent at a point, and at that point, the curve is almost parallel to the tangent
    line. The more we zoom in, the more the line appears to be straight.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们已放大至左上角，并在某点画了切线，在该点，曲线几乎与切线平行。我们越放大，线条看起来就越像是直线。
- en: Similarly, if we have a two-dimensional manifold such as a sphere (which we
    denote as *S²*) that exists in ![](img/a88efc3f-540c-4329-bfa0-7e4913d7e181.png) and
    we zoom into it, then at any point on the surface, it will appear to look like
    a flat disk. A manifold does not necessarily have to be a sphere; it can be any
    topological space that has the characteristics of a manifold. To visualize this,
    consider the Earth to be a manifold. From anywhere you stand and look, the Earth
    appears to look flat (or planar), even though it is curved.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果我们有一个二维流形，例如一个球面（我们表示为*S²*），它存在于 ![](img/a88efc3f-540c-4329-bfa0-7e4913d7e181.png) 中，并且我们对其进行放大，那么在表面上的任何一点，都会看起来像一个平坦的圆盘。流形不一定非得是球面；它可以是任何具有流形特性的拓扑空间。为了形象地理解这一点，可以将地球视为一个流形。从你站立的任何地方向四周看，地球看起来是平的（或平面），尽管它实际上是弯曲的。
- en: 'We can write the unit circle and unit sphere, respectively, as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以分别如下表示单位圆和单位球：
- en: '[![](img/776c47dd-f6f4-41e6-98e1-c4892f3174cd.png)]'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/776c47dd-f6f4-41e6-98e1-c4892f3174cd.png)]'
- en: '[![](img/0b27b163-fc14-4890-8e3c-66dae3f3650e.png)]'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/0b27b163-fc14-4890-8e3c-66dae3f3650e.png)]'
- en: 'Similarly, we can also write higher-dimensional manifolds as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们也可以如下表示更高维度的流形：
- en: '![](img/861681f0-6438-4b7e-ae51-e1d119888371.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/861681f0-6438-4b7e-ae51-e1d119888371.png)'
- en: 'Formally, a (differentiable) *k*-dimensional manifold M in ![](img/4abead96-71b5-459c-a8e5-acfec1e0956a.png) is
    a set of points in ![](img/199aafe4-8acd-4773-ad40-ce8a6cd522b2.png) where for
    every point, *p ∈ M*, there is a small open neighborhood, *U,* of *p*, a vector-valued
    differentiable function, [![](img/1edc1871-73ae-4a40-bca9-c45f1df0357e.png)],
    and an open set, [![](img/21facf0b-47d6-4497-82f8-d02f52982a8a.png),] with the
    following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地说，*k*维可微流形M在 ![](img/4abead96-71b5-459c-a8e5-acfec1e0956a.png) 中是 ![](img/199aafe4-8acd-4773-ad40-ce8a6cd522b2.png) 中的一组点，对于每一个点，*p
    ∈ M*，存在一个*p*的小开放邻域*U*，一个向量值可微函数，[![](img/1edc1871-73ae-4a40-bca9-c45f1df0357e.png)]，以及一个开集，[![](img/21facf0b-47d6-4497-82f8-d02f52982a8a.png)]，满足以下条件：
- en: '[![](img/6380669c-017a-478c-b300-7e3fdc3000b3.png)]'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/6380669c-017a-478c-b300-7e3fdc3000b3.png)]'
- en: 'The Jacobian of *F* has rank, *k*, at each of the points in *V*, where the
    Jacobian of *F* is an *n* × *k* matrix that looks as follows:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F*的雅可比矩阵在*V*中的每个点都有秩*k*，其中*F*的雅可比矩阵是一个*n* × *k*的矩阵，形如下所示：'
- en: '![](img/f82e32d1-226f-48be-be39-b66f6fec5249.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f82e32d1-226f-48be-be39-b66f6fec5249.png)'
- en: Here, *F* is the local parameterization of the manifold.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*F*是流形的局部参数化。
- en: We can also use existing topological spaces to create new ones using Cartesian
    products. Suppose we have two topological spaces, *X* and *Y*, and that their
    Cartesian product is [![](img/446e52b9-f6a2-45c0-b788-49434486063a.png)], where
    each [![](img/e9301b2e-c144-432d-988a-149cac1d8795.png)] and [![](img/4cb05171-f9d3-42f6-986e-69cf0ccc8d2f.png) ]generates
    a point, [![](img/9f7ce26e-9a4d-4863-860f-0e93f4940ce7.png)]. A familiar Cartesian
    product is a three-dimensional Euclidean space, where [![](img/a80f3506-bf84-4d76-a727-cd55644d4777.png)]. However,
    it is important to note that [![](img/5d7e1519-bb3d-436d-b728-55c34a6487b8.png)] (it
    is actually equal to *T²*, which is a ring torus, but we will avoid going into
    why since that goes beyond the scope of this book).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用现有的拓扑空间通过笛卡尔积构建新的拓扑空间。假设我们有两个拓扑空间，*X*和*Y*，它们的笛卡尔积是 [![](img/446e52b9-f6a2-45c0-b788-49434486063a.png)]，其中每个 [![](img/e9301b2e-c144-432d-988a-149cac1d8795.png)] 和 [![](img/4cb05171-f9d3-42f6-986e-69cf0ccc8d2f.png)] 生成一个点， [![](img/9f7ce26e-9a4d-4863-860f-0e93f4940ce7.png)]。一个熟悉的笛卡尔积是三维欧几里得空间，其中
    [![](img/a80f3506-bf84-4d76-a727-cd55644d4777.png)]。然而，需要注意的是 [![](img/5d7e1519-bb3d-436d-b728-55c34a6487b8.png)] （它实际上等于*T²*，即一个环面，但我们将避免探讨其原因，因为那超出了本书的范围）。
- en: In computer graphics, we use two-dimensional manifolds embedded in [![](img/1ffa25ed-6992-4650-8e18-355dbcec494c.png)] to
    represent boundary surfaces of three-dimensional objects.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机图形学中，我们使用嵌入在 [![](img/1ffa25ed-6992-4650-8e18-355dbcec494c.png)] 中的二维流形来表示三维物体的边界表面。
- en: 'Since these parametric manifolds are objects, they will have an orientation,
    and to define this, we will need what is known as the tangent space to the manifold.
    However, before we''re able to define the tangent space, we need to clarify some
    concepts. Suppose again that we have a *k*-dimensional manifold, *M*, defined
    in [![](img/8300eb70-f0bd-4f7f-8163-b3c2bd63edd0.png)] (where *k < n*). Here,
    for each *p ∈ M,* there is an open set, *U*, containing *p* and (*n-k*) real-valued
    functions, [![](img/3bd04522-57a3-423e-93b8-d97d860897f2.png),] defined on *U* such
    that [![](img/d7a5df0f-d5ab-4472-ad06-83d34857fd5c.png)] and at each point, [![](img/8e18cbb8-f7f5-48a1-a29d-b25b44b2e14e.png)],
    we have the following linearly independent vectors:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些参数化流形是对象，它们会有一个方向性，而为了定义这一点，我们需要所谓的流形的切空间。然而，在我们能够定义切空间之前，我们需要先澄清一些概念。假设我们有一个
    *k* 维流形 *M*，它被定义在 [![](img/8300eb70-f0bd-4f7f-8163-b3c2bd63edd0.png)] 上（其中 *k
    < n*）。在这里，对于每个 *p ∈ M*，都有一个包含 *p* 的开集 *U* 和 (*n-k*) 个实值函数 [![](img/3bd04522-57a3-423e-93b8-d97d860897f2.png)]，这些函数在
    *U* 上定义，并且满足 [![](img/d7a5df0f-d5ab-4472-ad06-83d34857fd5c.png)]，在每一点 [![](img/8e18cbb8-f7f5-48a1-a29d-b25b44b2e14e.png)]
    处，我们有以下线性无关的向量：
- en: '![](img/33f61fd5-e37b-467f-b9fb-3a0876e93f1b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33f61fd5-e37b-467f-b9fb-3a0876e93f1b.png)'
- en: 'Now, the normal space to *M* at *p* is written as *N[p](M)* and is spanned
    by the following vectors:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，流形 *M* 在点 *p* 处的法空间表示为 *N[p](M)*，并由以下向量张成：
- en: '![](img/f5fd8010-61bd-4488-98bf-92a57ae81a58.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5fd8010-61bd-4488-98bf-92a57ae81a58.png)'
- en: 'As we already know, tangents are perpendicular to normal vectors, and so the
    tangent space, *T[p](M)*, to the manifold at *p* consists of all the vectors, ![](img/450ca9f7-273b-4656-acec-40ac7e64f512.png), that
    are perpendicular to each of the normal vectors, *N[p](M)*. However, [![](img/1a41d505-069e-4531-9f1b-66b0f2f2944a.png)] is
    only in *T[p](M)* if—and only if—for all of [![](img/7087577e-51ef-47d4-a38e-50c920de691c.png)],
    we have the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，切向量是垂直于法向量的，因此流形上点 *p* 处的切空间 *T[p](M)* 包含所有与每个法向量 *N[p](M)* 垂直的向量，![](img/450ca9f7-273b-4656-acec-40ac7e64f512.png)。然而，[![](img/1a41d505-069e-4531-9f1b-66b0f2f2944a.png)]
    只有在对于所有的 [![](img/7087577e-51ef-47d4-a38e-50c920de691c.png)]，我们有如下条件时，才能属于 *T[p](M)*：
- en: '![](img/6858642f-5757-42d6-8301-82d56dac604e.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6858642f-5757-42d6-8301-82d56dac604e.png)'
- en: 'Once we have our tangent space, we can use it to define a Riemannian metric,
    which is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了切空间，就可以用它来定义黎曼度量，具体定义如下：
- en: '![](img/19f564e1-1bf7-4322-bd3b-fbd180f0995e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19f564e1-1bf7-4322-bd3b-fbd180f0995e.png)'
- en: This metric allows us to perform local measurements of angles, distances, and
    volumes, as well as any manifold that this metric is defined on. This is known
    as a **Riemannian manifold**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该度量使我们能够执行角度、距离和体积的局部测量，以及度量所定义的任何流形。这被称为 **黎曼流形**。
- en: 'Before we move on, there are two terms that are important for us to become
    familiar with:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，有两个重要的术语需要我们熟悉：
- en: '**Isometry**: Metric preserving shape deformation'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**等距性**：度量保持形状变换'
- en: '**Geodesic**: Shortest path on *M* between *p* and *p''*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测地线**：在 *M* 上 *p* 和 *p''* 之间的最短路径'
- en: 'Interestingly, we can define manifolds using scalar fields and vector fields,
    which means we can extend calculus to manifolds. In this case, we need to introduce
    three new concepts, as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们可以通过标量场和向量场来定义流形，这意味着我们可以将微积分扩展到流形中。在这种情况下，我们需要引入以下三个新概念：
- en: '**Scalar field**: [![](img/13707b05-a444-466e-a624-7ba9a72ba18d.png)]'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标量场**： [![](img/13707b05-a444-466e-a624-7ba9a72ba18d.png)]'
- en: '**Vector field**: [![](img/a24fd592-73ac-44d9-90a5-fae76e0a026f.png)]'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量场**： [![](img/a24fd592-73ac-44d9-90a5-fae76e0a026f.png)]'
- en: '**Hilbert space with inner products**:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带内积的希尔伯特空间**：'
- en: '![](img/ddf57b4a-3a0f-45fc-855d-8045cd32f95a.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ddf57b4a-3a0f-45fc-855d-8045cd32f95a.png)'
- en: A Hilbert space is an abstract vector space that merely generalizes the concept
    of Euclidean space. So, the methods we learned about regarding vector algebra
    and calculus can be extended from two-dimensional and three-dimensional Euclidean
    space to an arbitrary or infinite number of dimensions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 希尔伯特空间是一个抽象的向量空间，它仅仅是对欧几里得空间概念的推广。因此，我们关于向量代数和微积分的方法可以从二维和三维欧几里得空间扩展到任意维数甚至无限维数。
- en: 'Naturally, if we are going to define calculus on manifolds, we want to be able
    to take derivatives, but this isn''t as clear as it is for curves. For manifolds,
    we make use of the tangent space, such that [![](img/dfbb68ad-04ce-428b-a4a5-6ab651fad1e1.png)] and
    the directional derivative is [![](img/8c001bcc-abf2-4a66-b5d9-68aa26063bba.png)],
    which tells us how much *f* changes at point *p* in the direction *F(p)*. And
    the intrinsic gradient operator, which tells us the direction that the change
    of *f* is most steep in, is calculated from the following equation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，如果我们要在流形上定义微积分，我们希望能够求导，但这不像曲线那样清晰。对于流形，我们利用切空间，使得[![](img/dfbb68ad-04ce-428b-a4a5-6ab651fad1e1.png)]，方向导数是[![](img/8c001bcc-abf2-4a66-b5d9-68aa26063bba.png)]，它告诉我们*f*在点*p*沿方向*F(p)*的变化量。而内在梯度算子，告诉我们*f*变化最陡的方向，可以通过以下方程计算：
- en: '![](img/44cc4c31-e751-4ac4-aea9-d263cee4c7c8.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44cc4c31-e751-4ac4-aea9-d263cee4c7c8.png)'
- en: 'The intrinsic divergence operator calculates the net flow of the field, *F*,
    at point *p* through the following equation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 内在散度算子通过以下方程计算场*F*在点*p*的净流量：
- en: '![](img/f2fc8c34-7107-4889-b4b6-3dfc89a8995f.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2fc8c34-7107-4889-b4b6-3dfc89a8995f.png)'
- en: 'Using this, we can find the formal adjoint of the gradient, as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个，我们可以找到梯度的正式伴随算子，如下所示：
- en: '![](img/28e372d9-aa55-4837-8c63-1b2388d06eba.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28e372d9-aa55-4837-8c63-1b2388d06eba.png)'
- en: 'Now, we can find the Laplacian, [![](img/fb8fa0f4-d4f7-4cb7-8500-e8d2508781da.png)],
    which calculates the difference between *f(x)* and the average value of *f* in
    the vicinity of point *p* using [![](img/55683a44-1593-4b5f-89fb-5b861a470aa2.png)], which
    tells us that the Laplacian is isometry-invariant (an isometry is a distance preserving
    transformation between metric spaces. However, in geometry, we sometimes want
    the shape of an object to be defined in a way that is invariant to isometries.
    This means the object can be deformed so that it gets bent but not stretched,
    thus not affecting the intrinsic distances), positive-definite, and symmetric,
    as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以找到拉普拉斯算子[![](img/fb8fa0f4-d4f7-4cb7-8500-e8d2508781da.png)]，它通过[![](img/55683a44-1593-4b5f-89fb-5b861a470aa2.png)]计算*f(x)*与点*p*附近*f*的平均值之间的差异，这告诉我们拉普拉斯算子是等距不变的（等距是度量空间之间保持距离的变换。然而，在几何学中，我们有时希望对象的形状以不受等距影响的方式定义。这意味着对象可以被变形，使其弯曲但不被拉伸，从而不影响内在的距离），是正定的，且是对称的，如下所示：
- en: '![](img/49e07a88-52ec-4e0f-9872-5e8e436dc0c8.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49e07a88-52ec-4e0f-9872-5e8e436dc0c8.png)'
- en: Discrete manifolds
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离散流形
- en: For a moment, let's think back to [Chapter 5](758f1209-7a1d-474c-b494-bbf905a25afd.xhtml),
    *Graph Theory*, where we learned about graph theory. As a quick refresher, a graph, *G*,
    is made up of vertices, [![](img/0a1c8e80-d4db-4a6a-badb-f1371002406b.png),] and
    edges, [![](img/5cb513ce-a51c-49db-bf2e-ab7019b8f509.png)], and the undirected
    edge, [![](img/cd412fdb-b4bc-4c0b-87a7-230dc8429bdd.png)] iff [![](img/601bcf06-f0b0-4119-8b19-383ed43fcd20.png)].
    The edges of weighted graphs have weights, [![](img/89368c78-3fdf-47b5-bd43-efd3ac74d9e0.png),] for
    all [![](img/0cfbdd02-0a0f-4646-a981-359137ab4984.png)], and vertexes can have
    weights as well for all [![](img/ab2e1bf0-8a77-4c4c-a50f-29110105a27f.png)], the
    vertex weight, [![](img/abaac773-d23c-4cf4-95d6-32291fcaee00.png)].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时回顾一下[第5章](758f1209-7a1d-474c-b494-bbf905a25afd.xhtml)，*图论*，我们在其中学习了图论。简单回顾一下，图*G*由顶点[![](img/0a1c8e80-d4db-4a6a-badb-f1371002406b.png)]和边[![](img/5cb513ce-a51c-49db-bf2e-ab7019b8f509.png)]组成，且无向边[![](img/cd412fdb-b4bc-4c0b-87a7-230dc8429bdd.png)]当且仅当[![](img/601bcf06-f0b0-4119-8b19-383ed43fcd20.png)]。加权图的边具有权重[![](img/89368c78-3fdf-47b5-bd43-efd3ac74d9e0.png)]，对于所有[![](img/0cfbdd02-0a0f-4646-a981-359137ab4984.png)]，顶点也可以有权重[![](img/ab2e1bf0-8a77-4c4c-a50f-29110105a27f.png)]，即顶点权重[![](img/abaac773-d23c-4cf4-95d6-32291fcaee00.png)]。
- en: The reason we care about graphs here is because we can also do calculus on graphs.
    To do this, we will need to define a vertex field, [![](img/8bafb317-9f00-4700-9e6d-a1cf8920e7d5.png),] and
    an edge field, [![](img/eb51847d-1030-4244-84a8-80793d6bb397.png)] (we also assume
    that [![](img/9944d77e-4518-475c-9257-714271e21334.png)]). The Hilbert spaces
    with inner products are [![](img/8a3ff850-eddd-433e-b3d3-63c8d49c012d.png)] and [![](img/850965d1-3be6-4a1d-a6af-aaf6d661b825.png)].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以关心图，是因为我们也可以在图上做微积分。为此，我们需要定义一个顶点场[![](img/8bafb317-9f00-4700-9e6d-a1cf8920e7d5.png)]和一个边场[![](img/eb51847d-1030-4244-84a8-80793d6bb397.png)]（我们还假设[![](img/9944d77e-4518-475c-9257-714271e21334.png)]）。带有内积的希尔伯特空间是[![](img/8a3ff850-eddd-433e-b3d3-63c8d49c012d.png)]和[![](img/850965d1-3be6-4a1d-a6af-aaf6d661b825.png)]。
- en: As we no doubt know by now, in calculus, we are very fond of gradients, and
    naturally, we can define a gradient operator for graphs, [![](img/9ce89fe2-09e2-4577-992b-10308d259484.png),] giving
    us ![](img/bfc5e81e-880b-42a0-800e-5b3f3ad591be.png) and a divergence operator, [![](img/53c83825-69c9-4fe1-850d-9925c5285dd1.png),]
    that produces [![](img/f23beb13-655a-4be1-8e47-73f7e9e85dc7.png)] and is adjoint
    to the gradient operator, [![](img/e7617edc-54ce-4289-808d-4375bb76c66a.png)].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们现在所知道的，在微积分中，我们非常喜欢梯度，因此我们自然可以为图定义一个梯度算子，[![](img/9ce89fe2-09e2-4577-992b-10308d259484.png)，]，给我们带来！[](img/bfc5e81e-880b-42a0-800e-5b3f3ad591be.png)，以及一个散度算子，[![](img/53c83825-69c9-4fe1-850d-9925c5285dd1.png)，]，它生成[![](img/f23beb13-655a-4be1-8e47-73f7e9e85dc7.png)]，并且是梯度算子的伴随算子，[![](img/e7617edc-54ce-4289-808d-4375bb76c66a.png)]。
- en: 'The graph Laplacian operator, [![](img/7c808823-0000-4369-bce7-7c2fe4eb49b0.png),] is
    defined as [![](img/66cf8822-52ab-4c0e-81e2-7055ae9dbdf9.png).] By combining the
    preceding two equations, we can obtain the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图拉普拉斯算子，[![](img/7c808823-0000-4369-bce7-7c2fe4eb49b0.png)，]定义为[![](img/66cf8822-52ab-4c0e-81e2-7055ae9dbdf9.png)]。通过结合前面两个方程，我们可以得到如下结果：
- en: '![](img/c3e3ae6f-5340-4553-a51e-537e2bc123bb.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3e3ae6f-5340-4553-a51e-537e2bc123bb.png)'
- en: 'As in the case of manifolds, this calculates the difference between *f* and
    its local average (that is, of the neighboring nodes). We can rewrite this as
    a positive semi-definite square matrix:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如同流形的情况，这计算了*f*与其局部平均值之间的差异（即邻近节点的平均值）。我们可以将其重新写成一个正半定的方阵：
- en: '![](img/d5733be0-8ff7-43d1-b0b1-c08ab2952149.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5733be0-8ff7-43d1-b0b1-c08ab2952149.png)'
- en: 'We can also write this as an unnormalized Laplacian, where *A = I*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将其写为一个未归一化的拉普拉斯算子，其中*A = I*：
- en: '![](img/321c0d87-c9d6-468b-9852-cceb336cbc52.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/321c0d87-c9d6-468b-9852-cceb336cbc52.png)'
- en: 'Finally, we can write it for the random walk Laplacian, where *A = D*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将其写为随机游走拉普拉斯算子，其中*A = D*：
- en: '![](img/59f6a2cf-cbf7-4e41-a938-d142ede11f5a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59f6a2cf-cbf7-4e41-a938-d142ede11f5a.png)'
- en: Here, [![](img/0b9ed71c-07d1-454e-a493-89023d5fa1a0.png)], [![](img/952608f4-edec-4d4f-82a4-216605174c18.png)],
    and [![](img/b086c116-ae2f-44c1-bb3c-6d6730bb8bed.png)].
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/0b9ed71c-07d1-454e-a493-89023d5fa1a0.png)]，[![](img/952608f4-edec-4d4f-82a4-216605174c18.png)]，和[![](img/b086c116-ae2f-44c1-bb3c-6d6730bb8bed.png)]。
- en: Using graphs, we can formulate discrete manifolds, that is, describe three-dimensional
    objects using vertices, [![](img/acc21909-6103-47d4-9ae4-9717090bd894.png)], edges, [![](img/38592001-96db-499a-ab82-d609512a506a.png)],
    and faces, [![](img/bf61c7d8-0a94-4d8a-b729-c82f43f57073.png)]. This is generally
    referred to as a triangular mesh. In a manifold mesh, each edge is shared by two
    faces and each vertex has one loop.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图形，我们可以构建离散流形，即使用顶点，[![](img/acc21909-6103-47d4-9ae4-9717090bd894.png)]，边，[![](img/38592001-96db-499a-ab82-d609512a506a.png)]，和面，[![](img/bf61c7d8-0a94-4d8a-b729-c82f43f57073.png)]来描述三维物体。这通常被称为三角网格。在一个流形网格中，每条边由两个面共享，每个顶点有一个循环。
- en: Before we move on, let's redefine the Laplacian on triangular meshes using the
    cotangent formula, which can be defined for an embedded mesh that has the coordinates [![](img/f782098f-6a3d-4d2d-be2f-8442e5a12995.png)] and
    in terms of the lengths of the edges.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们使用余切公式重新定义三角网格上的拉普拉斯算子，这可以为具有坐标[![](img/f782098f-6a3d-4d2d-be2f-8442e5a12995.png)]的嵌入网格定义，并且是基于边长的。
- en: 'The cotangent Laplacian for the embedded mesh is as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入网格的余切拉普拉斯算子如下：
- en: '![](img/3206731b-bcb1-4047-bcfb-cfb7ce49ac99.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3206731b-bcb1-4047-bcfb-cfb7ce49ac99.png)'
- en: 'Here, [![](img/2efc8d27-83ab-4ef6-970c-8c5c64fdaaba.png)] (since we''re dealing
    with triangles) and the cotangent Laplacian, in terms of edge length, is as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/2efc8d27-83ab-4ef6-970c-8c5c64fdaaba.png)]（因为我们处理的是三角形），且余切拉普拉斯算子在边长的定义下如下：
- en: '![](img/3f7581b2-cf32-4657-9d61-176d9bf04457.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f7581b2-cf32-4657-9d61-176d9bf04457.png)'
- en: Here, [![](img/6fc42cf4-83f1-4a16-8488-cd68a1d545ee.png)], *s* is a semi-perimeter,
    and [![](img/82a358ae-fa52-49b7-9ae1-78af8887afcb.png)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/6fc42cf4-83f1-4a16-8488-cd68a1d545ee.png)]，*s*是半周长，且[![](img/82a358ae-fa52-49b7-9ae1-78af8887afcb.png)]。
- en: Spectral decomposition
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谱分解
- en: 'To understand spectral analysis, we must first define the Laplacian on a compact
    manifold, *M*, which has countably many eigenfunctions:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解谱分析，我们必须首先定义紧致流形*M*上的拉普拉斯算子，它有可数多个特征函数：
- en: '![](img/0f5fde60-90ab-4929-9505-1af686dd75b5.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f5fde60-90ab-4929-9505-1af686dd75b5.png)'
- en: This is for [![](img/c53cd608-af09-493a-b23f-c809d837db39.png)].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是针对[![](img/c53cd608-af09-493a-b23f-c809d837db39.png)]的。
- en: 'Due to the symmetry property, the eigenfunctions will be both real and orthonormal,
    which gives us the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对称性特性，特征函数将是实数且正交归一化的，这给我们带来以下结果：
- en: '![](img/1661df2c-231f-4d4c-a10a-b47cf6e725e0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1661df2c-231f-4d4c-a10a-b47cf6e725e0.png)'
- en: Here, the eigenvalues are non-negative, that is, [![](img/505fa5fe-6d00-46c0-beff-8daf8488477f.png)].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，特征值是非负的，也就是说，[![](img/505fa5fe-6d00-46c0-beff-8daf8488477f.png)]。
- en: 'For two-dimensional manifolds, we often use Weyl''s law to describe the asymptotic
    behavior of eigenvalues, which looks as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二维流形，我们通常使用Weyl定律来描述特征值的渐近行为，形式如下：
- en: '![](img/1a1f15cf-eac4-4868-a918-83e64e3d1a5c.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a1f15cf-eac4-4868-a918-83e64e3d1a5c.png)'
- en: 'Using the eigenfunctions and eigenvalues, we can eigendecompose the graph Laplacian
    into the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征函数和特征值，我们可以将图拉普拉斯算子特征分解为以下形式：
- en: '![](img/1d8f0606-c775-4e79-a77d-38a4a17757b5.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d8f0606-c775-4e79-a77d-38a4a17757b5.png)'
- en: Here, [![](img/02cc3698-f8f9-4a6f-bcca-bd18087b2405.png)] and [![](img/5edc0284-56d8-4286-abae-9bdf61bc7eb1.png)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/02cc3698-f8f9-4a6f-bcca-bd18087b2405.png)] 和 [![](img/5edc0284-56d8-4286-abae-9bdf61bc7eb1.png)]。
- en: 'Now, if we were to pose this as a generalized eigenproblem, we would obtain
    the following from the preceding equation with *A*-orthogonal eigenvectors, [![](img/6f210cae-ee66-4c91-bdcf-3a166c9b7f62.png)]:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们将其作为一个广义特征问题来提出，我们可以从前面的方程中得到以下结果，假设有*A*正交特征向量，[![](img/6f210cae-ee66-4c91-bdcf-3a166c9b7f62.png)]：
- en: '![](img/e9d25719-3a6f-48ba-b112-5e66782042d2.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9d25719-3a6f-48ba-b112-5e66782042d2.png)'
- en: 'If we were to change the variables through substituting, [![](img/bcd2494e-d333-4f40-b72e-43e2de78a1cd.png)],
    we would find ourselves with the standard eigenproblem:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过代入来改变变量，[![](img/bcd2494e-d333-4f40-b72e-43e2de78a1cd.png)]，我们将得到标准特征问题：
- en: '![](img/e36ee789-095f-48f7-812a-55d614c60d48.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e36ee789-095f-48f7-812a-55d614c60d48.png)'
- en: Here, the eigenvectors are orthogonal, that is, [![](img/59101960-e8af-445c-8201-70d96e883deb.png)].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，特征向量是正交的，也就是说，[![](img/59101960-e8af-445c-8201-70d96e883deb.png)]。
- en: Graph neural networks
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图神经网络
- en: Graph neural networks are the quintessential neural network for geometric deep
    learning, and, as the name suggests, they work particularly well on graph-based
    data such as meshes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络是几何深度学习的典型神经网络，正如其名所示，它们在基于图的数据（如网格）上表现尤为出色。
- en: Now, let's assume we have a graph, *G*, that has a binary adjacency matrix, *A*.
    Then, we have another matrix, *X*, that contains all the node features. These
    features could be text, images, or categorical, node degrees, clustering coefficients,
    indicator vectors, and so on. The goal here is to generate node embeddings using
    local neighborhoods.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有一个图*G*，它有一个二进制邻接矩阵*A*。然后，我们有另一个矩阵*X*，其中包含所有节点特征。这些特征可以是文本、图像、类别、节点度、聚类系数、指示向量等。这里的目标是通过局部邻域生成节点嵌入。
- en: As we know, nodes on graphs have neighboring nodes, and, in this case, each
    node tries to aggregate the information from its neighbors using a neural network.
    We can think of the network neighborhood as a computation graph. Since each node
    has edges with different nodes, each node has a unique computation graph.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，图上的节点有邻居节点，在这种情况下，每个节点试图通过神经网络从其邻居处聚合信息。我们可以将网络邻域视为计算图。由于每个节点与不同节点之间有边，因此每个节点都有一个独特的计算图。
- en: If we think back to convolutional neural networks, we learned that convolutions
    are a window of sorts and that we can slide across the input and summarize the
    data into a reduced form. The aggregator operation works similarly to how the
    convolution operation works.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾卷积神经网络，我们会发现卷积是一种窗口操作，我们可以滑动窗口并将输入数据汇总为简化的形式。聚合操作与卷积操作的工作原理类似。
- en: Let's dive right in and see how they work mathematically.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一下，看看它们在数学上的运作原理。
- en: 'At each layer, nodes have embeddings, and initial embeddings are equivalent
    to the node features:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层，节点都有嵌入，初始嵌入等同于节点特征：
- en: '![](img/e7b67efb-7660-446a-9d62-0e2e18464a0e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7b67efb-7660-446a-9d62-0e2e18464a0e.png)'
- en: 'The embedding of the *k^(th)* layer embedding of *v* is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*k^(th)*层的* v *嵌入如下所示：'
- en: '![](img/f4872f56-34d3-4f48-bdfd-0468eaadcd6d.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4872f56-34d3-4f48-bdfd-0468eaadcd6d.png)'
- en: 'This is done for all *k > 0*, where [![](img/1e5ab5b8-1515-4fd3-bd4f-f87612b4224f.png)] is
    the previous layer embedding of *v* and [![](img/6564f28d-06a8-44c6-a80c-583780c690e9.png)] is
    the average of the neighbor''s previous layer embeddings. During training, the
    model learns *W[k]* and *B[k]*, and the output embeddings for each node after
    *K* layers of neighborhood aggregation is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这对所有 *k > 0* 都适用，其中[![](img/1e5ab5b8-1515-4fd3-bd4f-f87612b4224f.png)]是* v *的前一层嵌入，[![](img/6564f28d-06a8-44c6-a80c-583780c690e9.png)]是邻居的前一层嵌入的平均值。在训练过程中，模型学习*W[k]*和*B[k]*，并且每个节点在经过*K*层邻域聚合后的输出嵌入如下所示：
- en: '![](img/7688317c-813f-4f07-b5ef-fc48d5629605.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7688317c-813f-4f07-b5ef-fc48d5629605.png)'
- en: To generate embeddings that are of a high quality, we define a loss function
    on *z[v]* and feed the embeddings into it, after which we perform gradient descent
    to train the aggregation parameters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成高质量的嵌入，我们在*z[v]*上定义一个损失函数，并将嵌入传递给它，之后我们执行梯度下降以训练聚合参数。
- en: 'In the case of a supervised task, we can define the loss as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督任务的情况下，我们可以定义损失函数如下：
- en: '![](img/7f31732f-a34e-48ca-ba39-1a72c6072c06.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f31732f-a34e-48ca-ba39-1a72c6072c06.png)'
- en: 'Let''s suppose we have the following undirected graph:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下无向图：
- en: '![](img/7288fb23-dcf2-40f1-aaf8-c4e7437e3dd7.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7288fb23-dcf2-40f1-aaf8-c4e7437e3dd7.png)'
- en: 'With this, we want to calculate the update for the solid node, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们想要计算实心节点的更新，如下所示：
- en: '![](img/23adde1e-0ec8-4996-9df8-7874b2d043e7.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23adde1e-0ec8-4996-9df8-7874b2d043e7.png)'
- en: 'To calculate the update, we can use the following equation:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算更新，我们可以使用以下方程：
- en: '![](img/cd9a53b0-5789-4a3c-9412-0f150a1baad4.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd9a53b0-5789-4a3c-9412-0f150a1baad4.png)'
- en: Here, *N^((i))* is the neighbor of node *i* and *c[i,j]* is the normalized constant.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N^((i))*是节点*i*的邻居，*c[i,j]*是归一化常数。
- en: Spectral graph CNNs
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 频谱图卷积神经网络（Spectral Graph CNNs）
- en: 'Spectral graph CNNs, as the name suggests, use a spectral convolution, which
    we defined as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，频谱图卷积神经网络（Spectral Graph CNNs）使用频谱卷积，我们定义如下：
- en: '![](img/555b3fcf-9882-4616-8db5-947a927c9dc1.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/555b3fcf-9882-4616-8db5-947a927c9dc1.png)'
- en: 'Here, [![](img/881e0f2d-f672-4c70-b7c0-30cb18409376.png)]. We can rewrite this
    in matrix form as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/881e0f2d-f672-4c70-b7c0-30cb18409376.png)]。我们可以将其重新写为矩阵形式如下：
- en: '![](img/30588bf4-df9a-460b-8c6c-3ae4d0a072dc.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30588bf4-df9a-460b-8c6c-3ae4d0a072dc.png)'
- en: This is not shift-invariant since *G* does not have a circulant structure.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是平移不变的，因为*G*没有循环结构。
- en: 'Now, in the spectral domain, we define a convolutional layer as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在频谱域中，我们定义卷积层如下：
- en: '![](img/cfdaee09-4d66-424d-a3fe-281545536dc1.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfdaee09-4d66-424d-a3fe-281545536dc1.png)'
- en: Here, [![](img/4391da57-4c66-4a1b-abe8-f3e61ecf7cfb.png)], [![](img/fdfac598-12fa-4c13-b13a-9d3c65a4c6a5.png)],
    and [![](img/7b94b285-1f1f-41de-b4dd-b55642df8f5a.png)] is an n×n diagonal matrix
    of spectral filter coefficients (which are basis-dependent, meaning that they
    don't generalize over different graphs and are limited to a single domain), and
    ξ is the nonlinearity that's applied to the vertex-wise function values.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/4391da57-4c66-4a1b-abe8-f3e61ecf7cfb.png)]，[![](img/fdfac598-12fa-4c13-b13a-9d3c65a4c6a5.png)]，和[![](img/7b94b285-1f1f-41de-b4dd-b55642df8f5a.png)]是一个n×n的频谱滤波器系数对角矩阵（这些系数是基依赖的，意味着它们不能跨图之间推广，仅限于单一域），ξ是应用于顶点函数值的非线性。
- en: What this means is that if we learn a convolutional filter with the basis Φ
    on one domain, it will not be transferable or applicable to another task that
    has the basis Ψ. This isn't to say we can't create bases that can be used for
    different domains—we can—however, this requires using a joint diagonalizable procedure.
    But doing this would require having prior knowledge of both domains and how they
    relate to one another.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果我们在一个域上使用基Φ学习卷积滤波器，它将不能转移或应用于具有基Ψ的另一个任务。这并不意味着我们不能创建可以用于不同域的基——我们可以——然而，这需要使用联合对角化过程。但这样做需要事先了解两个域以及它们如何相互关联。
- en: 'We can also define the pooling function in non-Euclidean domains. We refer
    to this as graph coarsening, and in it, only a fraction, *α < 1*, of the vertices
    of the graph are left. If we were to have the eigenvectors of graph Laplacians
    at different resolutions, then they would be related through the following equation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在非欧几里得域中定义池化函数。我们称之为图粗化，在其中，只有一部分*α < 1*的图节点被保留下来。如果我们在不同分辨率下拥有图拉普拉斯算子的特征向量，那么它们将通过以下方程关联：
- en: '![](img/4b9e24c2-18d5-40fa-b091-07ad85aab625.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b9e24c2-18d5-40fa-b091-07ad85aab625.png)'
- en: Here, Φ is an *n* × *n* matrix, ![](img/d639d1fb-3de6-4144-a008-3c103bf91d09.png) is
    an α*n* × α*n* matrix, and *P* is an α*n* × *n* binary matrix representing the
    position of the *i^(th)* vertex of the coarsened graph on the original graph.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，Φ是一个*n* × *n*矩阵，![](img/d639d1fb-3de6-4144-a008-3c103bf91d09.png)是一个α*n*
    × α*n*矩阵，*P*是一个α*n* × *n*的二进制矩阵，表示粗化图中第*i*个顶点在原图中的位置。
- en: 'Graph neural networks, like the neural networks we learned about in previous
    chapters, can also overfit, and in an effort to avoid this from happening, we
    adapt the learning complexity to try and reduce the total number of free parameters
    in the model. For this, we use spatial localization of the filters in the frequency
    domain. In the Euclidean domain, we can write this as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs），就像我们在前几章学习的神经网络一样，也可能发生过拟合。为了避免这种情况发生，我们通过适应学习复杂度来尽量减少模型中自由参数的总数。为此，我们使用滤波器在频域中的空间局部化。在欧几里得领域中，我们可以将其写为：
- en: '![](img/8ed46abf-c8c2-4839-8b0d-5ad99412e8d7.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ed46abf-c8c2-4839-8b0d-5ad99412e8d7.png)'
- en: 'What this tells us is that in order to learn a layer in which the features
    aren''t just well localized in the original domain but also shared across other
    locations, we must learn smooth spectral multipliers. Spectral multipliers are
    parameterized as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，为了学习一个特征不仅在原始领域中局部化良好，而且在其他位置也能共享的层，我们必须学习平滑的谱乘子。谱乘子的参数化如下：
- en: '![](img/352c53df-02f7-4e61-a97c-28e5176de009.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/352c53df-02f7-4e61-a97c-28e5176de009.png)'
- en: Here, [![](img/8129fb07-25c2-4b3e-b15e-f5e84cb7eb1f.png)], which is a fixed
    interpolation matrix of size *k *× *q*, and α is a vector of interpolation coefficients
    of size *q*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/8129fb07-25c2-4b3e-b15e-f5e84cb7eb1f.png)] 是一个固定大小为 *k* × *q* 的插值矩阵，α
    是大小为 *q* 的插值系数向量。
- en: Mixture model networks
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合模型网络
- en: Now that we've seen a few examples of how GNNs work, let's go a step further
    and see how we can apply neural networks to meshes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了一些 GNN 工作的示例，让我们更进一步，看看如何将神经网络应用于网格。
- en: First, we use a patch that is defined at each point in a local system of *d*-dimensional
    pseudo-coordinates, [![](img/852595ac-4a06-4289-8881-9734f439fc26.png)], around
    *x*. This is referred to as a geodesic polar. On each of these coordinates, we
    apply a set of parametric kernels, [![](img/b931854e-0df3-4d86-b9ce-c381e4da12a5.png)], that
    produces local weights.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用一个在局部 *d* 维伪坐标系统中定义的补丁， [![](img/852595ac-4a06-4289-8881-9734f439fc26.png)]，围绕
    *x*。这被称为大地测量极坐标。在这些坐标上，我们应用一组参数化核， [![](img/b931854e-0df3-4d86-b9ce-c381e4da12a5.png)]，产生局部权重。
- en: 'The kernels here differ in that they are Gaussian and not fixed, and are produced
    using the following equation:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的核不同之处在于它们是高斯型的，而不是固定的，并且是通过以下方程式产生的：
- en: '![](img/02710616-5143-41f4-8c69-626624399c19.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02710616-5143-41f4-8c69-626624399c19.png)'
- en: These parameters ([![](img/8c0f473f-4df8-4e71-9886-4550d1a91e12.png)] and [![](img/55e50983-bcd7-46b4-bbfa-4b49178fe023.png)])
    are trainable and learned.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数（[![](img/8c0f473f-4df8-4e71-9886-4550d1a91e12.png)] 和 [![](img/55e50983-bcd7-46b4-bbfa-4b49178fe023.png)])是可训练的并且是学习得到的。
- en: 'A spatial convolution with a filter, *g*, can be defined as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 可以定义带有滤波器 *g* 的空间卷积如下：
- en: '![](img/010c1e88-6c90-4c7d-84a6-d3830cf9eeae.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/010c1e88-6c90-4c7d-84a6-d3830cf9eeae.png)'
- en: Here, [![](img/c4f2eccd-9e5d-4d57-96f3-0b38c60923df.png)] is a feature at vertex
    *i*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/c4f2eccd-9e5d-4d57-96f3-0b38c60923df.png)] 是顶点 *i* 处的特征。
- en: 'Previously, we mentioned geodesic polar coordinates, but what are they? Let''s
    define them and find out. We can write them as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们提到了大地测量极坐标，但它们是什么？让我们定义它们并了解一下。我们可以将它们写成如下：
- en: '![](img/865efff9-b4bd-4c07-9af3-5b72ec0e4ab6.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/865efff9-b4bd-4c07-9af3-5b72ec0e4ab6.png)'
- en: Here, [![](img/4dbbfce0-4f81-40b1-ab02-727159b94024.png)] is the geodesic distance
    between *i* and *j* and [![](img/e3cfa0d7-1143-4932-9d73-672ac80c7eac.png)] is
    the direction of the geodesic from *i*to *j*. However, here, the orientation is
    somewhat ambiguous.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/4dbbfce0-4f81-40b1-ab02-727159b94024.png)] 是 *i* 和 *j* 之间的大地测量距离，[![](img/e3cfa0d7-1143-4932-9d73-672ac80c7eac.png)]
    是从 *i* 到 *j* 的大地测量方向。然而，在这里，方向是有些模糊的。
- en: 'Now, we can define angular max-pooling (which is a rotating filter), as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义角度最大池化（即旋转滤波器），如下所示：
- en: '![](img/af8f824f-5de0-49e4-bdbb-6fc54072190a.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af8f824f-5de0-49e4-bdbb-6fc54072190a.png)'
- en: Facial recognition in 3D
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3D 人脸识别
- en: Let's go ahead and see how this translates to a real-world problem such as 3D
    facial recognition, which is used in phones, security, and so on. In 2D images,
    this would be largely dependent on the pose and illumination, and we don't have
    access to depth information. Because of this limitation, we use 3D faces instead
    so that we don't have to worry about lighting conditions, head orientation, and
    various facial expressions. For this task, the data we will be using is meshes.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续看看这如何应用于现实世界中的问题，例如 3D 人脸识别，它被广泛应用于手机、安全等领域。在 2D 图像中，这通常依赖于姿态和光照，并且我们无法获取深度信息。由于这个限制，我们改用
    3D 人脸，以便不用担心光照条件、头部朝向和各种面部表情。对于这个任务，我们将使用网格数据。
- en: In this case, our meshes make up an undirected, connected graph, *G = (V, E,
    A)*, where |*V*| = *n* is the vertices, *E* is a set of edges, and [![](img/46f079cd-8c6f-4247-b332-e8f5faf6a32a.png)] contains
    the *d*-dimensional pseudo-coordinates, [![](img/aaa202b8-4c72-412e-8882-c1922f6eff73.png)], where [![](img/586d29f9-43f2-438d-9a7f-b4aac0535a11.png)].
    The node feature matrix is denoted as [![](img/dc24f964-bf5d-4855-8bc2-47c52e9dc607.png)], where
    each of the nodes contains *d*-dimensional features. We then define the *l^(th)*
    channel of the feature map as *f[l]*, of which the *i^(th)* node is denoted as
    *f[l](i)*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的网格构成了一个无向的连通图，*G = (V, E, A)*，其中 |*V*| = *n* 是顶点，*E* 是边的集合，且[![](img/46f079cd-8c6f-4247-b332-e8f5faf6a32a.png)]包含了*d*-维伪坐标，[![](img/aaa202b8-4c72-412e-8882-c1922f6eff73.png)]，其中[![](img/586d29f9-43f2-438d-9a7f-b4aac0535a11.png)]。节点特征矩阵表示为[![](img/dc24f964-bf5d-4855-8bc2-47c52e9dc607.png)]，其中每个节点包含*d*-维特征。我们接着定义特征图的
    *l^(th)* 通道为 *f[l]*，其中 *i^(th)* 节点表示为 *f[l](i)*。
- en: 'The pseudo-coordinates, *u(i, j)*, determine how the features in the mesh are
    aggregated, and since, as we know, meshes are constructed from smaller triangles,
    we can compute the pseudo-coordinates from all the nodes, *i* to node *j*. Here,
    we will use the globally normalized Cartesian coordinates:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 伪坐标 *u(i, j)* 决定了网格中如何聚合特征，正如我们所知道的，网格是由更小的三角形构成的，我们可以从所有节点 *i* 到节点 *j* 计算伪坐标。在这里，我们将使用全局归一化的笛卡尔坐标：
- en: '![](img/cc244183-d0f7-43f5-a536-397fc0482bb6.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc244183-d0f7-43f5-a536-397fc0482bb6.png)'
- en: This gives us the ability to map spatial relations to fixed regions.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们能够将空间关系映射到固定区域。
- en: We initialize the weights using [![](img/b8488374-61e5-4ae6-8974-c5c709150765.png)], where
    *l[in]* is the dimensions of the input feature for the *k^(th)* layer.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用[![](img/b8488374-61e5-4ae6-8974-c5c709150765.png)]初始化权重，其中 *l[in]* 是 *k^(th)*
    层输入特征的维度。
- en: 'Now, we can compute the feature aggregation into node *i* from the neighboring
    nodes, [![](img/5dd098db-522a-4b76-aedc-b08dcc5c9d79.png)], as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以从相邻节点计算特征聚合到节点 *i*，如图所示[![](img/5dd098db-522a-4b76-aedc-b08dcc5c9d79.png)]，具体如下：
- en: '![](img/6e1304dc-c7c4-4854-97c6-538187ba6b0d.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e1304dc-c7c4-4854-97c6-538187ba6b0d.png)'
- en: Here, [![](img/1297f9df-8e57-4f75-ae87-7d9bfe58f1fc.png)].[![](img/ef3c4505-4a70-4d3b-a0a0-4187e40cc1f8.png)] is
    the basis of the *B*-spline over degree *m*, and [![](img/98d3364d-4e49-4884-974b-03b8a649a4cc.png)] are
    parameters that can be learned.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/1297f9df-8e57-4f75-ae87-7d9bfe58f1fc.png)]。[![](img/ef3c4505-4a70-4d3b-a0a0-4187e40cc1f8.png)]
    是 *B*-样条的基础，阶数为 *m*，而[![](img/98d3364d-4e49-4884-974b-03b8a649a4cc.png)] 是可以学习的参数。
- en: 'This being a classification task, we will use cross-entropy as our loss function.
    We do this as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个分类任务，我们将使用交叉熵作为我们的损失函数。具体如下：
- en: '![](img/e2e14f1e-3f8a-4682-b534-3d2a8e3d537d.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2e14f1e-3f8a-4682-b534-3d2a8e3d537d.png)'
- en: Here, [![](img/eeec0a71-7683-4bce-bcc6-a3bc69142b08.png)] and *Y* is the label
    matrix.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/eeec0a71-7683-4bce-bcc6-a3bc69142b08.png)] 和 *Y* 是标签矩阵。
- en: And with that, we can conclude this chapter on geometric deep learning.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们可以结束这一章关于几何深度学习的内容。
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about some important mathematical topics, such as
    the difference between Euclidean and non-Euclidean data and manifolds. We then
    went on to learn about a few fascinating and emerging topics in the field of deep
    learning that have widespread applications in a plethora of domains in which traditional
    deep learning algorithms have proved to be ineffective. This new class of neural
    networks, known as graph neural networks, greatly expand on the usefulness of
    deep learning by extending it to work on non-Euclidean data. Toward the end of
    this chapter, we saw an example use case for graph neural networks—facial recognition
    in 3D.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了一些重要的数学主题，如欧几里得数据与非欧几里得数据及流形之间的区别。接着，我们学习了深度学习领域中一些引人入胜且新兴的主题，这些主题在许多传统深度学习算法已证明无效的领域中有着广泛的应用。这类新的神经网络，被称为图神经网络，极大地扩展了深度学习的实用性，使其能够在非欧几里得数据上工作。章末，我们还看到了图神经网络的一个实际应用案例——三维人脸识别。
- en: This brings us to the end of this book. Congratulations on successfully completing
    the lessons that were provided!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本书到此结束，恭喜你成功完成了所提供的课程！
