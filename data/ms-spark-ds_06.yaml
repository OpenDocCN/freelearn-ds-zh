- en: Chapter 6. Scraping Link-Based External Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。抓取基于链接的外部数据
- en: This chapter aims to explain a common pattern for enhancing local data with
    external content found at URLs or over APIs. Examples of this are when URLs are
    received from GDELT or Twitter. We offer readers a tutorial using the GDELT news
    index service as a source of news URLs, demonstrating how to build a web scale
    news scanner that scrapes global breaking news of interest from the Internet.
    We explain how to build this specialist web scraping component in a way that overcomes
    the challenges of scale. In many use cases, accessing the raw HTML content is
    not sufficient enough to provide deeper insights into emerging global events.
    An expert data scientist must be able to extract entities out of that raw text
    content to help build the context needed track broader trends.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在解释一种增强本地数据的常见模式，该模式使用从URL或API获取的外部内容。例如，当从GDELT或Twitter接收到URL时。我们为读者提供了一个使用GDELT新闻索引服务作为新闻URL来源的教程，演示如何构建一个从互联网上抓取感兴趣的全球突发新闻的网络规模新闻扫描器。我们解释了如何构建这个专门的网络抓取组件，以克服规模的挑战。在许多用例中，访问原始HTML内容是不足以提供对新兴全球事件的更深入洞察的。专业的数据科学家必须能够从原始文本内容中提取实体，以帮助构建跟踪更广泛趋势所需的上下文。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Create a scalable web content fetcher using the *Goose* library
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*Goose*库创建可扩展的网络内容获取器
- en: Leverage the Spark framework for Natural Language Processing (NLP)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用Spark框架进行自然语言处理（NLP）
- en: De-duplicate names using the double metaphone algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用双重音标算法去重名字
- en: Make use of GeoNames dataset for geographic coordinates lookup
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用GeoNames数据集进行地理坐标查找
- en: Building a web scale news scanner
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个网络规模的新闻扫描器
- en: What makes data science different from statistics is the emphasis on scalable
    processing to overcome complex issues surrounding the quality and variety of the
    collected data. While statisticians work on samples of clean datasets, perhaps
    coming from a relational database, data scientists in contrast, work at scale
    with unstructured data coming from a variety of sources. While the former focuses
    on building models having high degrees of precision and accuracy, the latter often
    focuses on constructing rich integrated datasets that offer the discovery of less
    strictly defined insights. The data science journey usually involves torturing
    the initial sources of data, joining datasets that were theoretically not meant
    to be joined, enriching content with publicly available information, experimenting,
    exploring, discovering, trying, failing, and trying again. No matter the technical
    or mathematical skills, the main difference between an average and an expert data
    scientist is the level of curiosity and creativity employed in extracting the
    value latent in the data. For instance, you could build a simple model and provide
    business teams with the minimum they asked for, or you could notice and leverage
    all these URLs mentioned in your data, then scrape that content, and use these
    extended results to discover new insights that exceed the original questions business
    teams asked.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学与统计学的不同之处在于强调可扩展处理以克服围绕收集数据的质量和多样性的复杂问题。而统计学家处理干净数据集的样本，可能来自关系数据库，数据科学家相反，处理来自各种来源的大规模非结构化数据。前者专注于构建具有高精度和准确性的模型，而后者通常专注于构建丰富的集成数据集，提供发现不那么严格定义的见解。数据科学之旅通常涉及折磨初始数据源，连接理论上不应该连接的数据集，丰富内容与公开可用信息，实验，探索，发现，尝试，失败，再次尝试。无论技术或数学技能如何，普通数据科学家与专业数据科学家之间的主要区别在于在提取数据中的潜在价值时所使用的好奇心和创造力的水平。例如，你可以构建一个简单的模型，并为业务团队提供他们要求的最低要求，或者你可以注意并利用数据中提到的所有这些URL，然后抓取这些内容，并使用这些扩展结果来发现超出业务团队最初问题的新见解。
- en: Accessing the web content
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问网络内容
- en: 'Unless you have been working really hard in early 2016, you will have heard
    about the death of the singer *David Bowie*, aged 69, on January 10, 2016\. This
    news has been widely covered by all media publishers, relayed on social networks,
    and followed by lots of tributes paid from the greatest artists around the world.
    This sadly is a perfect use case for the content of this book, and a good illustration
    for this chapter. We will use the following article from the BBC as a reference
    in this section:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你在2016年初非常努力地工作，否则你一定听说过歌手*大卫·鲍伊*于2016年1月10日去世，享年69岁。这一消息被所有媒体发布商广泛报道，在社交网络上传播，并得到了世界各地最伟大艺术家的致敬。这可悲地成为了本书内容的一个完美用例，并且是本章的一个很好的例证。我们将使用BBC的以下文章作为本节的参考：
- en: '![Accessing the web content](img/image_06_001.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![访问网络内容](img/image_06_001.jpg)'
- en: 'Figure 1: BBC article about David Bowie, Source: http://www.bbc.co.uk/news/entertainment-arts-35278872'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：关于大卫·鲍伊的BBC文章，来源：http://www.bbc.co.uk/news/entertainment-arts-35278872
- en: Looking at the HTML source code behind this article, the first thing to notice
    is that most of the content does not contain any valuable information. This includes
    the header, footer, navigation panels, sidebar, and all the hidden JavaScript
    code. While we are only interested in the title, some references (such as the
    publishing date), and at most, really only a dozens lines for the article itself,
    analyzing the page will require parsing more than 1500 lines of HTML code. Although
    we can find plenty of libraries designed for parsing HTML file content, creating
    a parser generic enough that can work with unknown HTML structures from random
    articles might become a real challenge on its own.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这篇文章背后的HTML源代码，首先要注意的是大部分内容都不包含任何有价值的信息。这包括标题、页脚、导航面板、侧边栏和所有隐藏的JavaScript代码。虽然我们只对标题、一些参考（如发布日期）感兴趣，最多只对文章本身的几十行感兴趣，但分析页面将需要解析超过1500行的HTML代码。虽然我们可以找到许多用于解析HTML文件内容的库，但创建一个足够通用的解析器，可以处理来自随机文章的未知HTML结构，可能会成为一个真正的挑战。
- en: The Goose library
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Goose图书馆
- en: 'We delegate this logic to the excellent Scala library **Goose** ([https://github.com/GravityLabs/goose](https://github.com/GravityLabs/goose)).
    This library opens a URL connection, downloads the HTML content, cleanses it from
    all its junk, scores the different paragraphs using some clustering of English
    stop words, and finally returns the pure text content stripped of any of the underlying
    HTML code. With a proper installation of *imagemagick*, this library can even
    detect the most representative picture of a given website (out of the scope here).
    The `goose` dependency is available on Maven central:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个逻辑委托给优秀的Scala库**Goose**（[https://github.com/GravityLabs/goose](https://github.com/GravityLabs/goose)）。该库打开一个URL连接，下载HTML内容，清理掉所有的垃圾，使用一些英文停用词的聚类对不同的段落进行评分，最后返回剥离了任何底层HTML代码的纯文本内容。通过正确安装*imagemagick*，该库甚至可以检测给定网站的最具代表性的图片（这里不在讨论范围内）。`goose`依赖项可在Maven中央库中找到：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Interacting with the Goose API is as pleasant as the library itself. We create
    a new Goose configuration, disable the image fetching, modify some optional settings
    such as the user agent and time out options, and create a new `Goose` object:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与Goose API交互就像使用库本身一样愉快。我们创建一个新的Goose配置，禁用图像获取，修改一些可选设置，如用户代理和超时选项，并创建一个新的`Goose`对象：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Calling the `extractContent` method returns an Article class with the following
    values:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`extractContent`方法返回一个具有以下值的Article类：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Using such a library, opening a connection and parsing the HTML content did
    not take us more than a dozen lines of code, and the technique can be applied
    to a random list of articles' URLs regardless of their source or HTML structure.
    The final output is a cleanly parsed dataset that is consistent, and highly useable
    in downstream analysis.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这样一个库，打开连接并解析HTML内容不会花费我们超过十几行的代码，这种技术可以应用于任意来源或HTML结构的文章URL列表。最终的输出是一个干净解析的数据集，一致，并且在下游分析中非常有用。
- en: Integration with Spark
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与Spark集成
- en: The next logical step is to integrate such a library and make its API available
    within a scalable Spark application. Once integrated, we will explain how to efficiently
    retrieve the remote content from a large collection of URLs and how to make use
    of non-serializable classes inside of a Spark transformation, and in a way that
    is performant.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个逻辑步骤是集成这样一个库，并在可扩展的Spark应用程序中提供其API。一旦集成，我们将解释如何有效地从大量URL中检索远程内容，以及如何在Spark转换中使用不可序列化的类，并且以高性能的方式。
- en: Scala compatibility
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scala兼容性
- en: 'The Goose library on Maven has been compiled for Scala 2.9, and therefore is
    not compatible with Spark distribution (requires Scala 2.11 for version 2.0+ of
    Spark). To use it, we had to recompile the Goose distribution for Scala 2.11 and,
    for your convenience, we made it available on our main GitHub repository. This
    can be quickly installed using the commands below:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Maven上的Goose库已经编译为Scala 2.9，因此与Spark分发不兼容（Spark 2.0+需要Scala 2.11）。为了使用它，我们不得不为Scala
    2.11重新编译Goose分发，并为了您的方便，我们将其放在了我们的主GitHub存储库中。可以使用以下命令快速安装：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note, you will have to modify your project `pom.xml` file using this new dependency.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您将需要修改您的项目`pom.xml`文件以使用这个新的依赖项。
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Serialization issues
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列化问题
- en: Any Spark developer working with third-party dependencies should have experienced
    a `NotSerializableException` at least once. Although it might be challenging to
    find the exact root cause on a large project with lots of transformations, the
    reason is quite simple. Spark tries to serialize all its transformations before
    sending them to the appropriate executors. Since the `Goose` class is not serializable,
    and since we built an instance outside of a closure, this code is a perfect example
    of a `NotSerializableException` being thrown.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 任何与第三方依赖项一起工作的Spark开发人员至少应该遇到过`NotSerializableException`。尽管在一个有很多转换的大型项目中找到确切的根本原因可能是具有挑战性的，但原因是非常简单的。Spark试图在将它们发送到适当的执行器之前序列化所有的转换。由于`Goose`类不可序列化，并且由于我们在闭包外部构建了一个实例，这段代码是`NotSerializableException`的一个完美例子。
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We simply overcome this constraint by creating an instance of a `Goose` class
    inside of a `map` transformation. By doing so, we avoid passing any reference
    to a non-serializable object we may have created. Spark will be able to send the
    code *as-is* to each of its executors without having to serialize any referenced
    object.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在`map`转换中创建一个`Goose`类的实例来简单地克服了这个限制。通过这样做，我们避免了传递任何我们可能创建的非可序列化对象的引用。Spark将能够将代码*原样*发送到每个执行器，而无需序列化任何引用的对象。
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Creating a scalable, production-ready library
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个可扩展的、生产就绪的库
- en: Improving performance of a simple application that runs on a single server is
    sometimes not easy; but doing so on a distributed application running on several
    nodes that processes a large amount of data in parallel is often vastly more difficult,
    as there are so many additional factors to consider that affect performance. We
    show next, the principles we used to tune the content fetching library, so it
    can be confidently run on clusters at any scale without issues.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 改进简单应用程序的性能在单个服务器上运行有时并不容易；但在并行处理大量数据的分布式应用程序上进行这样的改进通常更加困难，因为有许多其他因素会影响性能。接下来，我们将展示我们用来调整内容获取库的原则，以便它可以在任何规模的集群上自信地运行而不会出现问题。
- en: Build once, read many
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建一次，多次读取
- en: It is worth mentioning that in the previous example, a new Goose instance was
    created for each URL, making our code particularly inefficient when running at
    scale. As a naive example to illustrate this point, it may take around 30 ms to
    create a new instance of a `Goose` class. Doing so on each of our millions of
    records would require 1 hour on a 10 node clusters, not to mention the garbage
    collection performance that would be significantly impacted. This process can
    be significantly improved using a `mapPartitions` transformation. This closure
    will be sent to the Spark executors (just like a `map` transformation would be)
    but this pattern allows us to create a single Goose instance per executor and
    call its `extractContent` method for each of the executor's records.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，在前面的示例中，为每个URL创建了一个新的Goose实例，这使得我们的代码在大规模运行时特别低效。举个简单的例子来说明这一点，创建一个`Goose`类的新实例可能需要大约30毫秒。在我们数百万条记录中的每一条上都这样做将需要在一个10节点集群上花费1小时，更不用说垃圾回收性能将受到显著影响。使用`mapPartitions`转换可以显著改善这个过程。这个闭包将被发送到Spark执行器（就像`map`转换一样），但这种模式允许我们在每个执行器上创建一个单独的Goose实例，并为每个执行器的记录调用其`extractContent`方法。
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Exception handling
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异常处理
- en: Exception handling is a cornerstone of proper software engineering. This is
    especially true in distributed computing, where we are potentially interacting
    with a large number of external resources and services that are out of our direct
    control. If we were not handling exceptions properly, for instance, any error
    occurring while fetching external website content would make Spark reschedule
    the entire task on other nodes several times before throwing a final exception
    and aborting the job. In a production-grade, lights-out web scraping operation,
    this type of issue could compromise the whole service. We certainly do not want
    to abort our whole web scraping content handling process because of a simple 404
    error.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 异常处理是正确软件工程的基石。这在分布式计算中尤其如此，因为我们可能与大量直接不受我们控制的外部资源和服务进行交互。例如，如果我们没有正确处理异常，那么在获取外部网站内容时发生的任何错误都会使Spark在抛出最终异常并中止作业之前多次重新安排整个任务在其他节点上。在生产级别的、无人值守的网络爬虫操作中，这种问题可能会危及整个服务。我们当然不希望因为一个简单的404错误而中止整个网络爬虫内容处理过程。
- en: 'To harden our code against these potential issues, any exceptions should be
    properly caught, and we should ensure that all returned objects should consistently
    be made optional, being undefined for all the failed URLs. In this respect, the
    only bad thing that could be said about the Goose library is the inconsistency
    of its returned values: null can be returned for titles and dates, while an empty
    string is returned for missing descriptions and bodies. Returning null is a really
    bad practice in Java/Scala as it usually leads to `NullPointerException` – despite
    the fact most developers usually write a This should not happen comment next to
    it. In Scala, it is advised to return an option instead of null. In our example
    code, any field we harvest from the remote content should be returned optionally,
    as it may not exist on the original source page. Additionally, we should address
    other areas of consistency too when we harvest data, for example we can convert
    dates into strings as it might lead to serialization issues when calling an action
    (such as **collect**). For all these reasons, we should redesign our `mapPartitions`
    transformation as follows.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加强我们的代码对这些潜在问题的防范，任何异常都应该被正确捕获，并且我们应该确保所有返回的对象都应该一致地被设置为可选的，对于所有失败的URL来说都是未定义的。在这方面，关于Goose库唯一不好的一点是其返回值的不一致性：标题和日期可能返回null，而缺少描述和正文的情况下会返回空字符串。在Java/Scala中返回null是一个非常糟糕的做法，因为它通常会导致`NullPointerException`，尽管大多数开发人员通常会在旁边写上"This
    should not happen"的注释。在Scala中，建议返回一个选项而不是null。在我们的示例代码中，我们从远程内容中获取的任何字段都应该以可选的方式返回，因为它可能在原始源页面上不存在。此外，当我们获取数据时，我们还应该处理其他方面的一致性，例如我们可以将日期转换为字符串，因为在调用操作（如**collect**）时可能会导致序列化问题。因为这些原因，我们应该按照以下方式重新设计我们的`mapPartitions`转换。
- en: We test for the existence of each object and return optional results
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们测试每个对象的存在并返回可选结果
- en: We wrap the article content into a serializable case class `Content`
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将文章内容封装到一个可序列化的`Content`类中
- en: We catch any exception and return a default object with undefined values
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们捕获任何异常并返回一个具有未定义值的默认对象
- en: 'The revised code is shown as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的代码如下所示：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Performance tuning
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能调优
- en: Although most of the time, the performance of a Spark application can greatly
    be improved from changes to the code itself (we have seen the concept of using
    `mapPartitions` instead of a `map` function for that exact same purpose), you
    may also have to find the right balance between the total number of executors,
    the number of cores per executor, and the memory allocated to each of your containers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数情况下，Spark应用程序的性能可以通过对代码本身的更改大大改善（我们已经看到了使用`mapPartitions`而不是`map`函数来实现完全相同目的的概念），但您可能还需要找到总执行器数量、每个执行器的核心数量以及分配给每个容器的内存之间的正确平衡。
- en: When doing this second kind of application tuning, the first question to ask
    yourself is whether your application is I/O bound (lots of read/write access),
    network bound (lots of transfer between nodes), memory, or CPU bound (your tasks
    usually take too much time to complete).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行这种第二种类型的应用程序调优时，首先要问自己的问题是，您的应用程序是I/O绑定（大量读/写访问）、网络绑定（节点之间大量传输）、内存绑定还是CPU绑定（您的任务通常需要太长时间才能完成）。
- en: It is easy to spot the main bottleneck in our web scraper application. It takes
    around 30 ms to create a `Goose` instance, and fetching the HTML of a given URL
    takes around 3 seconds to complete. We basically spend 99% of our time waiting
    for a chunk of content to be retrieved, mainly because of the Internet connectivity
    and website availability. The only way to overcome this issue is to drastically
    increase the number of executors used in our Spark job. Note that since executors
    usually sit on different nodes (assuming a correct Hadoop setup), a higher degree
    of parallelism will not hit the network limit in terms of bandwidth (as it would
    certainly do on a single node with multiple threads).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易发现我们的网络爬虫应用程序中的主要瓶颈。创建一个`Goose`实例大约需要30毫秒，获取给定URL的HTML大约需要3秒才能完成。基本上，我们花费了99%的时间等待内容块被检索，主要是因为互联网连接和网站的可用性。克服这个问题的唯一方法是大幅增加我们Spark作业中使用的执行者数量。请注意，由于执行者通常位于不同的节点上（假设正确的Hadoop设置），更高的并行度不会在带宽方面受到网络限制（就像在单个节点上使用多个线程时肯定会发生的那样）。
- en: 'Furthermore, it is key to note that no reduce operation (no shuffle) is involved
    at any stage of this process as this application is a *map-only* job, making it
    linearly scalable by nature. Logically speaking, two times more executors would
    make our scraper two times more performant. To reflect these settings on our application,
    we need to make sure our data set is partitioned evenly with at least as many
    partitions as the number of executors we have defined. If our dataset were to
    fit on a single partition only, only one of our many executors would be used,
    making our new Spark setup both inadequate and highly inefficient. Repartitioning
    our collection is a one-off operation (albeit an expensive one) assuming we properly
    cache and materialize our RDD. We use parallelism of `200` here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，关键要注意的是，在这个过程的任何阶段都没有涉及减少操作（没有洗牌），因为这个应用是一个仅映射的作业，因此天然具有线性可扩展性。从逻辑上讲，两倍的执行者将使我们的爬虫性能提高两倍。为了反映这些设置在我们的应用程序上，我们需要确保我们的数据集被均匀地分区，至少有与我们定义的执行者数量一样多的分区。如果我们的数据集只能适应一个分区，那么我们的许多执行者中只有一个会被使用，使我们的新Spark设置既不足够又高度低效。重新分区我们的集合是一个一次性的操作（尽管是一个昂贵的操作），假设我们正确地缓存和实现我们的RDD。我们在这里使用了`200`的并行性：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The last thing to remember is to thoroughly cache the returned RDD, as this
    eliminates the risk that all its lazily defined transformations (including the
    HTML content fetching) might be re-evaluated on any further action we might call.
    To stay on the safe side, and because we absolutely do not want to fetch HTML
    content over the internet twice, we force this caching to take place explicitly
    by persisting the returned dataset to `DISK_ONLY`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要记住的一件事是彻底缓存返回的RDD，因为这样可以消除所有懒惰定义的转换（包括HTML内容获取）可能在我们调用任何进一步的操作时重新评估的风险。为了保险起见，因为我们绝对不想两次从互联网获取HTML内容，我们强制这种缓存明确地发生，通过将返回的数据集持久化到`DISK_ONLY`。
- en: Named entity recognition
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: Building a web scraper that enriches an input dataset containing URLs with external
    web-based HTML content is of great business value within a big data ingestion
    service. But while an average data scientist should be able to study the returned
    content by using some basic clustering and classification techniques, an expert
    data scientist will bring this data enrichment process to the next level, by further
    enriching and adding value to it in post processes. Commonly, these value-added,
    post processes include disambiguating the external text content, extracting entities
    (like People, Places, and Dates), and converting raw text into its simplest grammatical
    form. We will explain in this section how to leverage the Spark framework in order
    to create a reliable **Natural Language Processing** (**NLP**) pipeline that includes
    these valuable post-processed outputs, and which handles English language-based
    content at any scale.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个网络爬虫，用外部基于网页的HTML内容丰富包含URL的输入数据集，在大数据摄入服务中具有很大的商业价值。但是，虽然普通的数据科学家应该能够使用一些基本的聚类和分类技术来研究返回的内容，但专业的数据科学家将把这个数据丰富过程提升到下一个级别，通过进一步丰富和增加价值来进行后续处理。通常，这些增值的后续处理包括消除外部文本内容的歧义，提取实体（如人物、地点和日期），以及将原始文本转换为最简单的语法形式。我们将在本节中解释如何利用Spark框架来创建一个可靠的自然语言处理（NLP）管道，其中包括这些有价值的后处理输出，并且可以处理任何规模的英语内容。
- en: Scala libraries
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scala库
- en: '**ScalaNLP** ([http://www.scalanlp.org/](http://www.scalanlp.org/)) is the
    parent project of breeze (among others), and is a numerical computational framework
    heavily used in Spark MLlib. This library would have been the perfect candidate
    for NLP on Spark if it was not causing such a number of dependency issues between
    the different versions of breeze and epic. To overcome these core dependency mismatches,
    we would have to recompile either the entire Spark distribution or the full ScalaNLP
    stack, neither of them being a walk in the park. Instead, our preferred candidate
    is thus a suite of Natural Language processors from the Computational Language
    Understanding Lab ([https://github.com/clulab/processors](https://github.com/clulab/processors)).
    Written in Scala 2.11, it provides three different APIs: A Stanford **CoreNLP**
    processor, a fast processor, and one for processing biomedical text. Within this
    library, we can use `FastNLPProcessor` that is both accurate enough for basic
    **Named Entity Recognition** (**NER**) functionalities and licensed under Apache
    v2.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ScalaNLP（http://www.scalanlp.org/）是breeze（等等）的父项目，并且是在Spark MLlib中广泛使用的数值计算框架。如果它没有在不同版本的breeze和epic之间引起这么多依赖问题，这个库本来是Spark上NLP的完美候选者。为了克服这些核心依赖不匹配，我们要么重新编译整个Spark分发版，要么重新编译整个ScalaNLP堆栈，这两者都不是易事。因此，我们更倾向于使用来自计算语言理解实验室的一套自然语言处理器（https://github.com/clulab/processors）。它是用Scala
    2.11编写的，提供了三种不同的API：斯坦福CoreNLP处理器、快速处理器和用于处理生物医学文本的处理器。在这个库中，我们可以使用`FastNLPProcessor`，它对于基本的命名实体识别功能来说足够准确，并且在Apache
    v2许可下。
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: NLP walkthrough
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP演练
- en: A NLP processor annotates a document and returns a list of lemma (words in their
    simplest grammatical form), a list of named entities types such as `[ORGANIZATION]`,
    `[LOCATION]`, `[PERSON]` and a list of normalized entities (such as actual date
    values).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: NLP处理器注释文档并返回词形的列表（以其最简单的语法形式呈现的单词），命名实体类型的列表，如`[ORGANIZATION]`，`[LOCATION]`，`[PERSON]`，以及标准化实体的列表（如实际日期值）。
- en: Extracting entities
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取实体
- en: In the following example, we initialize a `FastNLPProcessor` object, annotate
    and tokenize the document into a list of `Sentence`, zip both the lemma and NER
    types, and finally return an array of recognized entities for each given sentence.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，我们初始化一个`FastNLPProcessor`对象，注释并标记文档为一个`Sentence`列表，将词形和NER类型进行压缩，最后返回每个给定句子的识别实体数组。
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Looking at the above output, you may notice that all the retrieved entities
    are not linked together, both `David` and `Bowie` being two distinct entities
    of a type `[PERSON]`. We recursively aggregate consecutive similar entities using
    the following methods.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的输出中，您可能会注意到所有检索到的实体都没有链接在一起，`David`和`Bowie`都是类型为`[PERSON]`的两个不同实体。我们使用以下方法递归聚合连续相似的实体。
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Printing out the same content now gives us a much more consistent output.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打印相同的内容会给我们一个更一致的输出。
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In a functional programming context, try to limit the use of any mutable object
    (such as using `var`). As a rule of thumb, any mutable object can always be avoided
    using preceding recursive functions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数式编程环境中，尽量限制使用任何可变对象（如使用`var`）。作为一个经验法则，可以通过使用前置递归函数来避免任何可变对象。
- en: Abstracting methods
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 抽象方法
- en: 'We appreciate that working on an array of sentences (sentences being themselves
    an array of entities) might sound quite blurry. By experience, this will be much
    more confusing when running at scale, when several `flatMap` functions will be
    required for a simple transformation on a RDD. We wrap the results into a class
    `Entities` and expose the following methods:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们意识到在一组句子（句子本身是一个实体数组）上工作可能听起来很模糊。根据经验，当在大规模运行时，对RDD进行简单转换将需要多个`flatMap`函数，这将更加令人困惑。我们将结果封装到一个`Entities`类中，并公开以下方法：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Building a scalable code
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建可扩展的代码
- en: 'We have now defined our NLP framework and abstracted most of the complex logic
    into a set of methods and convenient classes. The next step is to integrate this
    code within a Spark context and to start processing text content at scale. In
    order to write scalable code, one needs to take extra care addressing the following
    points:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经定义了我们的NLP框架，并将大部分复杂逻辑抽象成一组方法和方便的类。下一步是将这段代码集成到Spark环境中，并开始大规模处理文本内容。为了编写可扩展的代码，需要特别注意以下几点：
- en: Any use of a non-serializable class within a Spark job must be carefully declared
    inside of a closure in order to avoid a `NotSerializableException` being raised.
    Please refer to the Goose library serialization issues we have been discussing
    in the previous section.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark作业中使用非可序列化类时，必须在闭包内仔细声明，以避免引发`NotSerializableException`。请参考我们在前一节中讨论的Goose库序列化问题。
- en: Whenever we create a new instance of `FastNLPProcessor` (whenever we first hit
    its `annotate` method because of lazy defined), all the required models will be
    retrieved from classpath, deserialized, and loaded into memory. This process takes
    around 10 seconds to complete.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每当我们创建一个`FastNLPProcessor`的新实例（每当我们首次调用其`annotate`方法时，因为它是懒惰定义的），所有所需的模型将从类路径中检索、反序列化并加载到内存中。这个过程大约需要10秒钟才能完成。
- en: In addition to the instantiation process being quite slow, it is worth mentioning
    that the models can be very large (around a gigabyte), and that keeping all these
    models in memory will be incrementally consuming our available Heap space.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了实例化过程相当缓慢之外，值得一提的是模型可能非常庞大（大约1GB），并且将所有这些模型保留在内存中将逐渐消耗我们可用的堆空间。
- en: Build once, read many
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一次构建，多次读取
- en: For all these reasons, embedding our code *as-is* within a `map` function would
    be terribly inefficient (and would probably blow all our available heap space).
    As per the below example, we leverage the `mapPartitions` pattern in order to
    optimize both the overhead time of loading and deserializing the models, as well
    as reducing the amount of memory used by our executors. Using `mapPartitions`
    forces the processing of the first record of each partition to evaluate the models
    inducing the model loading and deserializing process, and all subsequent calls
    on that executor will reuse those models within that partition, helping to limit
    the expensive model transfer and initialization costs to once per executor.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 出于以上所有原因，将我们的代码*原样*嵌入`map`函数中将非常低效（并且可能会耗尽我们所有的可用堆空间）。如下例所示，我们利用`mapPartitions`模式来优化加载和反序列化模型的开销时间，以及减少执行器使用的内存量。使用`mapPartitions`强制处理每个分区的第一条记录以评估引导模型加载和反序列化过程，并且在该执行器上的所有后续调用将重用该分区内的模型，有助于将昂贵的模型传输和初始化成本限制为每个执行器一次。
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The ultimate goal of this NLP scalability problem is to load the least possible
    number of models while processing as many records as possible. With one executor,
    we would load the models only once but would totally lose the point of parallel
    computing. With lots of executors, we will spend much more time deserializing
    models than actually processing our text content. This is discussed in the performance
    tuning section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个NLP可扩展性问题的最终目标是在处理尽可能多的记录时加载尽可能少的模型。对于一个执行器，我们只加载一次模型，但完全失去了并行计算的意义。对于大量的执行器，我们将花费更多的时间反序列化模型，而不是实际处理我们的文本内容。这在性能调优部分有所讨论。
- en: Scalability is also a state of mind
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可扩展性也是一种思维状态
- en: Because we designed our code locally before integrating it into Spark, we kept
    in mind writing things in the most convenient way. It is important because scalability
    is not only how fast you code works in a big data environment, but also how people
    feel about it, and how efficiently developers interact with your API. As a developer,
    if you need to chain nested `flatMap` functions in order to perform what should
    be a simple transformation, your code simply does not scale! Thanks to our data
    structure being totally abstracted inside of an `Entities` class, deriving the
    different RDDs from our NLP extraction can be done from a simple map function.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在将代码集成到Spark之前在本地设计了我们的代码，我们一直记得以最方便的方式编写代码。这很重要，因为可扩展性不仅体现在大数据环境中代码运行的速度上，还体现在人们对其感觉如何，以及开发人员与您的API的交互效率如何。作为开发人员，如果你需要链接嵌套的`flatMap`函数来执行本应该是简单转换的操作，那么你的代码根本不具备可扩展性！由于我们的数据结构完全抽象在一个`Entities`类中，从我们的NLP提取中派生出不同的RDD可以通过一个简单的映射函数完成。
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Tip
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: It is key to note the use of `persist` here. As previously done on the HTML
    fetcher process, we thoroughly cache the returned RDD to avoid situations where
    all its underlying transformations will be re-evaluated on any further action
    we might be calling. NLP processing being quite an expensive process, you have
    to make sure it won't be executed twice, hence the `DISK_ONLY` cache here.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 关键要注意这里使用了`persist`。与之前在HTML获取器过程中所做的一样，我们彻底缓存返回的RDD，以避免在调用任何进一步的操作时重新评估其所有基础转换的情况。NLP处理是一个非常昂贵的过程，你必须确保它不会被执行两次，因此这里使用了`DISK_ONLY`缓存。
- en: Performance tuning
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能调优
- en: 'In order to bring this application to scale, you need to ask yourself the same
    key questions: Is this job I/O, memory, CPU, or network bound? NLP extraction
    is an expensive task, and loading a model is memory intensive. We may have to
    reduce the number of executors while allocating much more memory to each of them.
    To reflect these settings, we need to make sure our dataset will be evenly partitioned
    using at least as many partitions as the number of executors. We also need to
    enforce this repartitioning by caching our RDD and calling a simple `count` action
    that will evaluate all our previous transformations (including the partitioning
    itself).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个应用程序扩展，你需要问自己同样关键的问题：这个作业是I/O、内存、CPU还是网络绑定的？NLP提取是一个昂贵的任务，加载模型需要大量内存。我们可能需要减少执行器的数量，同时为每个执行器分配更多的内存。为了反映这些设置，我们需要确保我们的数据集将被均匀分区，使用至少与执行器数量相同的分区。我们还需要通过缓存我们的RDD并调用一个简单的`count`操作来强制进行这种重新分区，这将评估我们所有先前的转换（包括分区本身）。
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: GIS lookup
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GIS查找
- en: In the previous section, we were covering an interesting use case, how to extract
    location entities from unstructured data. In this section, we will make our enrichment
    process even smarter by trying to retrieve the actual geographical coordinate
    information (such as latitude and longitude) based on the locations of entities
    we were able to identify. Given an input string `London`, can we detect the city
    of London - UK together with its relative latitude and longitude? We will be discussing
    how to build an efficient geo lookup system that does not rely on any external
    API and which can process location data of any scale by leveraging the Spark framework
    and the *Reduce-Side-Join* pattern. When building this lookup service, we will
    have to bear in mind many places around the world might be sharing the same name
    (there are around 50 different places called Manchester in the US alone), and
    that an input record may not use the official name of the place it would be referring
    to (the official name of commonly used Geneva/Switzerland is Geneva)**.**
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们涵盖了一个有趣的用例，即如何从非结构化数据中提取位置实体。在本节中，我们将通过尝试根据我们能够识别的实体的位置来检索实际的地理坐标信息（如纬度和经度），使我们的丰富过程变得更加智能。给定一个输入字符串`伦敦`，我们能否检测到伦敦-英国的城市以及其相对纬度和经度？我们将讨论如何构建一个高效的地理查找系统，该系统不依赖于任何外部API，并且可以通过利用Spark框架和*Reduce-Side-Join*模式处理任何规模的位置数据。在构建此查找服务时，我们必须牢记世界上许多地方可能共享相同的名称（仅在美国就有大约50个名为曼彻斯特的地方），并且输入记录可能不使用所指的地方的官方名称（通常使用的瑞士日内瓦的官方名称是日内瓦）。
- en: GeoNames dataset
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GeoNames数据集
- en: '**GeoNames** ([http://www.geonames.org/](http://www.geonames.org/)) is a geographical
    database that covers all countries, contains over 10 million place names with
    geographic coordinates, and is available for download free of charge. In this
    example, we will be using the `AllCountries.zip` dataset (1.5 GB) together with
    `admin1CodesASCII.txt` reference data in order to turn our location strings into
    valuable location objects with geo coordinates. We will be keeping only the records
    related to continents, countries, states, districts, and cities together with
    major oceans, seas, rivers, lakes, and mountains, thus reducing by half, the entire
    dataset. Although the admin codes dataset easily fits in memory, the Geo names
    must be processed within an RDD and need to be converted into the following case
    classes:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**GeoNames** ([http://www.geonames.org/](http://www.geonames.org/))是一个涵盖所有国家的地理数据库，包含超过1000万个地名和地理坐标，并可免费下载。在这个例子中，我们将使用`AllCountries.zip`数据集（1.5
    GB），以及`admin1CodesASCII.txt`参考数据，将我们的位置字符串转换为具有地理坐标的有价值的位置对象。我们将仅保留与大洲、国家、州、地区和城市以及主要海洋、海洋、河流、湖泊和山脉相关的记录，从而将整个数据集减少一半。尽管管理代码数据集很容易放入内存中，但Geo名称必须在RDD中处理，并且需要转换为以下案例类：'
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will not be describing the process of parsing a flat file into a `geoNameRDD`
    here. The parser itself is quite straightforward, processing a tab delimited records
    file and converting each value as per the above case class definition. We expose
    the following static method instead:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不在这里描述将平面文件解析为`geoNameRDD`的过程。解析器本身非常简单，处理制表符分隔的记录文件，并根据上述案例类定义转换每个值。相反，我们将公开以下静态方法：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Building an efficient join
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建高效的连接
- en: 'The main lookup strategy will rely on a `join` operation to be executed against
    both our Geo names and our input data. In order to maximize the chance of getting
    a location match, we will be expanding our initial data using a `flatMap` function
    over all the possible alternative names, hence drastically increasing the initial
    size of 5 million to approximately 20 million records. We also make sure to clean
    names from any accents, dashes, or fuzzy characters they might contain:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的查找策略将依赖于对我们的地理名称和输入数据执行的`join`操作。为了最大限度地提高获取位置匹配的机会，我们将使用`flatMap`函数扩展我们的初始数据，以涵盖所有可能的替代名称，因此将初始大小从500万条记录大幅增加到约2000万条记录。我们还确保从名称中清除任何可能包含的重音符号、破折号或模糊字符：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'And voila, the remaining process is a simple `join`  operation between both
    a cleaned input and a cleaned `geoNameRDD`. Finally, we can group all the matching
    places into a simple set of `GeoName` objects:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，剩下的过程是在清理后的输入和清理后的`geoNameRDD`之间进行简单的`join`操作。最后，我们可以将所有匹配的地点分组成一组简单的`GeoName`对象：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'An interesting pattern can be discussed here. How does Spark perform a `join`
    operation on large datasets? Called the *Reduce-Side-Join* pattern in legacy MapReduce,
    it requires the framework to hash all the keys from both RDDs and send all elements
    with a same key (same hash) on a dedicated node in order to locally `join` their
    values. The principle of *Reduce-Side-Join* is illustrated in *Figure 2* as follows.
    Because a *Reduce-Side-Join* is an expensive task (network bound), we must take
    special care addressing the following two points:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可以讨论一个有趣的模式。Spark如何在大型数据集上执行`join`操作？在传统的MapReduce中称为*Reduce-Side-Join*模式，它要求框架对来自两个RDD的所有键进行哈希，并将具有相同键（相同哈希）的所有元素发送到专用节点，以便在本地`join`它们的值。*Reduce-Side-Join*的原则如下图2所示。由于*Reduce-Side-Join*是一项昂贵的任务（受网络限制），我们必须特别注意解决以下两个问题：
- en: '*GeoNames* dataset is much larger than our input RDD*.*We will be wasting lots
    of effort shuffling data that wouldn''t match anyway, making our `join` not only
    inefficient, but mainly useless.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*GeoNames*数据集比我们的输入RDD要大得多。我们将浪费大量精力洗牌数据，而这些数据无论如何都不会匹配，使我们的`join`不仅效率低下，而且主要是无用的。'
- en: '*GeoNames* dataset does not change over time*.*It wouldn''t make sense to re-shuffle
    this immutable dataset on a pseudo real-time system (such as Spark Streaming)
    where location events are received in batch.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*GeoNames*数据集随时间不会改变。在伪实时系统（如Spark Streaming）中接收位置事件的批处理中，重新洗牌这个不可变的数据集是没有意义的。'
- en: We can build two different strategies, an offline and an online strategy. The
    former will make use of a *Bloom filter* to drastically reduce the amount of data
    to be shuffled while the latter will partition our RDD by key in order to reduce
    the network cost associated to a `join` operation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建两种不同的策略，一种是离线策略，一种是在线策略。前者将利用*布隆过滤器*大大减少要洗牌的数据量，而后者将按键对我们的RDD进行分区，以减少与`join`操作相关的网络成本。
- en: '![Building an efficient join](img/image_06_002.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![构建高效的`join`](img/image_06_002.jpg)'
- en: 'Figure 2: The Reduce-Side-Join'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Reduce-Side-Join
- en: Offline strategy - Bloom filtering
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离线策略-布隆过滤
- en: '**Bloom filter** is a space efficient probabilistic data structure that is
    used to test whether an element is a member of a set with a limited probability
    of false positives. Heavily used in legacy MapReduce, some implementations have
    been compiled for Scala. We will use the Bloom filter of breeze library, available
    on maven central (breeze itself can be used without much of dependency mismatches
    compared to the ScalaNLP models we were discussing earlier).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**布隆过滤器**是一种空间高效的概率数据结构，用于测试元素是否是有限概率的假阳性成员。在传统的MapReduce中被广泛使用，一些实现已经编译为Scala。我们将使用breeze库的布隆过滤器，该库可在maven中心获得（与我们之前讨论的ScalaNLP模型相比，breeze本身可以在很大程度上避免依赖不匹配）。'
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Because our input dataset is much smaller than our `geoNameRDD`, we will train
    a Bloom filter against the former by leveraging the `mapPartitions` function.
    Each executor will build its own Bloom filter that we can aggregate, thanks to
    its associative property, into a single object using a bitwise operator within
    a `reduce` function:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的输入数据集比`geoNameRDD`要小得多，所以我们将通过利用`mapPartitions`函数对前者训练一个布隆过滤器。每个执行器将构建自己的布隆过滤器，我们可以通过其关联属性将其聚合成一个单一对象，使用`reduce`函数内的位运算符：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We test our filter against the full `geoNameRDD` in order to remove the places
    we know will not match, and finally execute our same `join` operation, but this
    time with much less data:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们针对完整的`geoNameRDD`测试我们的过滤器，以删除我们知道不会匹配的地点，最后执行相同的`join`操作，但这次处理的数据要少得多：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'By reducing the size of our `geoNameRDD`, we have been able to release a lot
    of pressure from the shuffling process, making our `join` operation much more
    efficient. The resulting *Reduce-Side-Join* is reported on following *Figure 3*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少`geoNameRDD`的大小，我们已经成功地减轻了洗牌过程的压力，使我们的`join`操作更加高效。产生的*Reduce-Side-Join*如下图3所示：
- en: '![Offline strategy - Bloom filtering](img/image_06_003.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![离线策略-布隆过滤](img/image_06_003.jpg)'
- en: 'Figure 3: Reduce-Side-Join with Bloom filter'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用布隆过滤器的Reduce-Side-Join
- en: Online strategy - Hash partitioning
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线策略-哈希分区
- en: 'In an offline process, we were reducing the amount of data to be shuffled by
    pre-processing our `geoNameRDD`. In a streaming process, because any new batch
    of data is different, it wouldn''t be worth filtering our reference data over
    and over. In such a scenario, we can greatly improve the `join` performance by
    pre-partitioning our `geoNameRDD` data by key, using a `HashPartitioner` with
    the number of partitions being at least the number of executors. Because the Spark
    framework knows about the repartitioning used, only the input RDD would be sent
    to the shuffle, making our lookup service significantly faster. This is illustrated
    in *Figure 4*. Note the `cache` and `count` methods used to enforce the partitioning.
    Finally, we can safely execute our same `join` operation, this time with much
    less pressure on the network:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在离线过程中，我们通过预处理我们的`geoNameRDD`来减少要洗牌的数据量。在流处理过程中，因为任何新的数据批次都是不同的，所以不值得一遍又一遍地过滤我们的参考数据。在这种情况下，我们可以通过使用`HashPartitioner`按键预分区我们的`geoNameRDD`数据，使用的分区数至少是执行器的数量，从而大大提高`join`性能。因为Spark框架知道重新分区的使用，只有输入RDD将被发送到洗牌，使我们的查找服务显着更快。这在*图4*中有所说明。请注意，使用`cache`和`count`方法来强制分区。最后，我们可以安全地执行我们相同的`join`操作，这次对网络的压力要小得多：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Online strategy - Hash partitioning](img/image_06_004.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![在线策略-哈希分区](img/image_06_004.jpg)'
- en: 'Figure 4: Reduce-Side-Join with Hash partitioning'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用哈希分区的减少端连接
- en: Content deduplication
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内容去重
- en: With cities like Manchester being found 100 times in our dataset, we need to
    work on a deduplication strategy for similar names, taking into account some cities
    might not be as important as others in terms of probability of being found within
    a random text content.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 像曼彻斯特这样的城市在我们的数据集中被发现100次，我们需要为类似名称制定去重策略，考虑到一些城市在随机文本内容中被发现的概率可能不如其他城市重要。
- en: Context learning
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文学习
- en: The most accurate method for de-duplicating locations content would probably
    be to study location records in their contexts, similar to Apple – the company
    – being to Google and Yahoo! what Apple – the fruit – is to banana and orange.
    By machine learning locations in their context, we would probably discover that
    words *beavers* and *bears* are contextually close to the city of London in Ontario,
    Canada. As far as we know, the risk of bumping into a wild bear in London, UK
    is pretty small. Assuming one can access the text content, training a model shouldn't
    be difficult, but accessing the geo coordinates would require building an indexed
    dictionary of every single place with both its geographical values and its most
    describing topics. Because we do not have access to such a dataset (we could be
    scraping *Wikipedia* though), and that we do not want to assume one gets access
    to text content, we will simply be ranking places as an order of importance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于去重地点内容最准确的方法可能是研究地点记录在其上下文中的情况，类似于苹果公司对谷歌和雅虎的关系，苹果水果对香蕉和橙子的关系。通过机器学习地点在其上下文中，我们可能会发现单词*海狸*和*熊*在加拿大安大略省伦敦市的上下文中是相关的。据我们所知，在英国伦敦遇到野生熊的风险是非常小的。假设可以访问文本内容，训练模型不应该很困难，但访问地理坐标将需要建立一个带有每个地方的地理值和最能描述的主题的索引字典。因为我们没有访问这样的数据集（尽管我们可以从*维基百科*上获取），并且我们不想假设有人可以访问文本内容，所以我们将简单地将地点排名为重要性的顺序。
- en: Location scoring
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 地点评分
- en: Given the different codes we pulled from the GeoNames website, we assume a continent
    will be more important than a country, a country will be more important than a
    state or a capital, and so on. This naive approach will make sense for 80% of
    the time, but might return irrelevant results in some edge cases. Given the Manchester
    example, we will find Manchester as being the parish of Manchester, a major state
    in Jamaica, instead of Manchester city, a *simple* city in the UK. We can fix
    this issue by being less restrictive in term of scoring and by sorting places
    of a same score by descending order of population. Returning the most important
    and relevant place makes sense, and such an approach is done by most online APIs
    anyway, but is that fair for the less important cities? We improve our scoring
    engine by adding a unique reference ID to a context where several locations may
    be mentioned together. If a document is only focused on cities in Canada, and
    if nothing is mentioning the United Kingdom, then *London* would most likely be
    the place in Canada. If no country or state is mentioned, or if both Canada and
    United Kingdom are found, we take the most important city of London in our dataset
    being London in the UK. The de-duplication occurs by sorting all our matching
    records by similar continent/country/states mentioned in the context, then by
    importance, and finally by population. The first result will be returned as our
    best candidate.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们从GeoNames网站获取的不同代码，我们假设一个大陆比一个国家更重要，一个国家比一个州或一个首都更重要，依此类推。这种天真的方法在80%的时间内是有意义的，但在一些边缘情况下可能会返回不相关的结果。以曼彻斯特为例，我们会发现曼彻斯特是牙买加的一个重要州的教区，而不是英国的一个简单的城市。我们可以通过在评分方面放宽限制并按人口数量降序排序相同评分的地点来解决这个问题。返回最重要和相关的地点是有意义的，大多数在线API都是这样做的，但对于不太重要的城市来说公平吗？我们通过向上下文添加唯一的参考ID来改进我们的评分引擎，在那里可能会提到几个地点。如果一个文档只关注加拿大的城市，而没有提到英国，那么伦敦很可能是加拿大的地方。如果没有提到国家或州，或者加拿大和英国都被提到，我们将在我们的数据集中将伦敦作为英国的伦敦。通过按照上下文中提到的相似大陆/国家/州进行排序，然后按重要性，最后按人口进行去重。第一个结果将作为我们最佳候选返回。
- en: Names de-duplication
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 名称去重
- en: As we were pulling entities from an NLP extraction process without any validation,
    the name we were able to retrieve may be written in many different ways. They
    can be written in different order, might contain middle names or initials, a salutation
    or a nobility title, nicknames, or even some typos and spelling mistakes. Although
    we do not aim to fully de-duplicate the content (such as learning that both *Ziggy
    Stardust* and *David Bowie* stand for the same person), we will be introducing
    two simple techniques used to de-duplicate a large amount of data at a minimal
    cost by combining the concept MapReduce paradigm and functional programming.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们从NLP提取过程中提取实体而没有任何验证，我们能够检索到的名称可能以许多不同的方式书写。它们可以按不同的顺序书写，可能包含中间名或缩写，称谓或贵族头衔，昵称，甚至一些拼写错误和拼写错误。尽管我们不打算完全去重内容（比如学习到*Ziggy
    Stardust*和*David Bowie*代表同一个人），但我们将介绍两种简单的技术，通过结合MapReduce范式和函数式编程的概念，以最小的成本去重大量数据。
- en: Functional programming with Scalaz
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Scalaz进行函数式编程
- en: 'This section is all about enriching data as part of an ingestion pipeline.
    We are therefore less interested in building the most accurate system using advanced
    machine learning techniques, but rather the most scalable and efficient one. We
    want to keep a dictionary of alternative names for each record, to merge and update
    them really fast, with the least possible code, and at very large scale. We want
    these structures to behave like monoids, algebraic associative structures properly
    supported on **Scalaz** ([https://github.com/scalaz/scalaz](https://github.com/scalaz/scalaz)),
    a library used for doing pure functional programming:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本节主要是关于作为摄入管道的一部分丰富数据。因此，我们对使用先进的机器学习技术构建最准确的系统不太感兴趣，而是对构建最可扩展和高效的系统感兴趣。我们希望保留每条记录的替代名称字典，以便快速合并和更新它们，代码尽可能少，并且规模非常大。我们希望这些结构表现得像单子，代数上的可结合结构，适当地支持**Scalaz**（[https://github.com/scalaz/scalaz](https://github.com/scalaz/scalaz)）上的纯函数式编程库：
- en: '[PRE26]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Our de-duplication strategy
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的去重策略
- en: 'We use a simple example below to justify the need of using Scalaz programming
    for building a scalable, deduplication pipeline made of multiple transformations.
    Using a RDD of person, `personRDD`, as a test dataset shown as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面使用一个简单的示例来证明使用Scalaz编程构建可扩展的去重管道的需求，该管道由多个转换组成。使用人员的RDD，`personRDD`，作为下面显示的测试数据集：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here, we first count the number of occurrences for each entry. This is in fact
    a simple Wordcount algorithm, the *101* of MapReduce programming:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先计算每个条目的出现次数。实际上，这是一个简单的Wordcount算法，MapReduce编程的*101*：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, we apply a first transformation, such as `lowercase`, and produce an
    updated report:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们应用第一个转换，比如`lowercase`，并生成一个更新的报告：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, we then apply a second transformation that removes any special character:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们然后应用第二个转换，删除任何特殊字符：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We now have reduced our list of six entries to only two, but since we've lost
    the original records across our transformations, we cannot build a dictionary
    in the form of [original value] -> [new value].
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经将我们的六个条目减少到只有两个，但由于我们在转换过程中丢失了原始记录，我们无法构建一个形式为[原始值]->[新值]的字典。
- en: Using the mappend operator
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用mappend运算符
- en: 'Instead, using the Scalaz API, we initialize a names'' frequency dictionary
    (as a Map, initialized to 1) upfront for each original record and merge these
    dictionaries using the `mappend` function (accessed through the `|+|` operator).
    The merge occurs after each transformation, within a `reduceByKey` function, taking
    the result of the transformation as a key and the term frequency map as a value:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用Scalaz API，我们预先初始化每个原始记录的名称频率字典（作为Map，初始化为1），并使用`mappend`函数（通过`|+|`运算符访问）合并这些字典。在每个转换之后，合并发生在`reduceByKey`函数中，将转换的结果作为键，术语频率映射作为值：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For each de-duplication entry, we find the most frequent item and build our
    dictionary RDD as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个去重条目，我们找到最频繁的项目，并构建我们的字典RDD如下：
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In order to fully de-duplicate our person RDD, one needs to replace all `david
    bowie` and `david#bowie` occurrences with `David Bowie`. Now that we have explained
    the de-duplication strategy itself, let us dive deeply into the set of transformations.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完全去重我们的人员RDD，需要将所有`david bowie`和`david#bowie`的出现替换为`David Bowie`。现在我们已经解释了去重策略本身，让我们深入研究一下转换集。
- en: Simple clean
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单清理
- en: 'The first deduplication transformation is obviously to clean names from all
    their fuzzy characters or extra spaces. We replace accents with their matching
    ASCII characters, handle camel case properly, and remove any stop words such as
    [mr, miss, sir]. Applying this function to the prime minister of Tonga, [Mr. Sialeʻataongo
    Tuʻivakanō], we return [siale ataongo tu ivakano], a much cleaner version of it,
    at least in the context of string deduplication. Executing the deduplication itself
    will be as simple as a few lines of code using both the MapReduce paradigm and
    the monoids concept introduced earlier:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个去重转换显然是从所有模糊字符或额外空格中清理名称。我们用它们匹配的ASCII字符替换重音符号，正确处理驼峰大小写，并删除任何停用词，例如[mr,
    miss, sir]。将此函数应用于汤加总理，[Mr. Sialeʻataongo Tuʻivakanō]，我们返回[siale ataongo tu ivakano]，这是一个更干净的版本，至少在字符串去重的情况下是这样。执行去重本身将是使用MapReduce范式和早期引入的单子概念的几行代码：
- en: '[PRE33]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: DoubleMetaphone
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DoubleMetaphone
- en: '**DoubleMetaphone** is a useful algorithm that can index names by their English
    pronunciation. Although it does not produce an exact phonetic representation of
    a name, it creates a simple hash function that can be used to group names with
    similar phonemes.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**DoubleMetaphone**是一种有用的算法，可以根据其英语发音索引名称。尽管它不能产生一个名字的精确音标表示，但它创建了一个简单的哈希函数，可以用于将具有相似音素的名称分组。'
- en: Note
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information about DoubleMetaphone algorithm, please refer to: *Philips,
    L. (1990). Hanging on the Metaphone (Vol. 7). Computer Language.)*'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 有关DoubleMetaphone算法的更多信息，请参阅：*Philips，L.（1990）。Hanging on the Metaphone（Vol.
    7）。计算机语言。）*
- en: We turn to this algorithm for performance reasons, as finding potential typos
    and spelling mistakes in large dictionaries is usually an expensive operation;
    it often requires a candidate name to be compared with each of the others we are
    tracking. This type of comparison is challenging in a big data environment as
    it usually requires a Cartesian `join` which can generate excessively large intermediate
    datasets. The metaphone algorithm offers a greater, and much faster alternative.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 出于性能原因，我们转向这种算法，因为在大型词典中查找潜在的拼写错误和拼写错误通常是一项昂贵的操作；通常需要将候选姓名与我们正在跟踪的每个其他姓名进行比较。这种类型的比较在大数据环境中是具有挑战性的，因为它通常需要进行笛卡尔`join`，这可能会生成过大的中间数据集。metaphone算法提供了一个更大、更快的替代方案。
- en: 'Using the `DoubleMetaphone` class from the Apache commons package, we simply
    leverage the MapReduce paradigm by grouping names sharing a same pronunciation.
    `[david bowie]`, `[david bowi]` and `[davide bowie]`, for example, are all sharing
    the same code `[TFT#P]` and will all be grouped together. In the example below,
    we compute the double metaphone hash for each record and call a `reduceByKey`
    that merges and updates all our names'' frequency maps:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Apache commons包中的`DoubleMetaphone`类，我们简单地利用MapReduce范式，将发音相同的姓名分组。例如，`[david
    bowie]`、`[david bowi]`和`[davide bowie]`都共享相同的代码`[TFT#P]`，将被分组在一起。在下面的示例中，我们计算每条记录的双元音哈希，并调用`reduceByKey`来合并和更新所有我们姓名的频率映射：
- en: '[PRE34]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can also greatly improve this simple technique by keeping a list of common
    English nicknames (bill, bob, will, beth, al, and so on) and their associated
    primary names, so that we can match across non-phonetic synonyms. We can do this
    by pre-processing our name RDD by replacing the hash codes for known nicknames
    with the hash codes of the associated primary names, and then we can run the same
    deduplication algorithm to resolve duplicates across both phonetic and synonym
    based matches. This will detect both spelling mistakes and alternative nicknames
    as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过保留常见英文昵称（比如bill、bob、will、beth、al等）及其对应的主要名称的列表，极大地改进这种简单的技术，这样我们就可以在非音标同义词之间进行匹配。我们可以通过预处理我们的姓名RDD，将已知昵称的哈希码替换为相关主要名称的哈希码，然后我们可以运行相同的去重算法来解决基于音标和同义词的重复。这将检测拼写错误和替代昵称，如下所示：
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Once again, we want to highlight the fact this algorithm (and the simple cleansing
    routine shown above) will not be as accurate as a proper, fuzzy string matching
    approach that would, for example, compute a *Levenshtein* distance between each
    possible pair of names. By sacrificing accuracy, we do however create a method
    that is highly scalable, and that finds most common spelling mistakes at a minimal
    cost, especially spelling mistakes made on silent consonants. Once all the alternative
    names have been grouped on the resulting hash codes, we can output the best alternative
    to the presented name as the most frequent name we return from our term frequency
    objects. This best alternate is applied through a `join` with the initial name
    RDD in order to replace any record with its preferred alternative (if any):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这种算法（以及上面显示的简单清洗例程）将不像适当的模糊字符串匹配方法那样准确，例如计算每对可能的姓名之间的*Levenshtein*距离。然而，通过牺牲准确性，我们创造了一种高度可扩展的方法，以最小的成本找到大多数常见的拼写错误，特别是在无声辅音上的拼写错误。一旦所有替代名称都已根据生成的哈希码分组，我们可以将最佳替代名称输出为我们从我们的词频对象返回的最频繁的名称。通过`join`将这个最佳替代应用于初始名称RDD，以替换任何记录为其首选替代（如果有的话）：
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: News index dashboard
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新闻索引仪表板
- en: Since we were able to enrich the content found at input URLs with valuable information,
    the natural next step is to start visualizing our data. Although the different
    techniques of Exploratory Data Analysis have been thoroughly discussed within
    [Chapter 4](ch04.xhtml "Chapter 4. Exploratory Data Analysis"), *Exploratory Data
    Analysis*, we believe it is worth wrapping up what we have covered so far using
    a simple dashboard in Kibana. From around 50,000 articles, we were able to fetch
    and analyze on January 10-11, we filter any record mentioning *David Bowie* as
    a NLP entity and containing the word *death.* Because all our text content is
    properly indexed in Elasticsearch, we can pull 209 matching articles with their
    content in just a few seconds.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们能够丰富输入URL中找到的内容，我们自然的下一步是开始可视化我们的数据。虽然探索性数据分析的不同技术已经在[第4章](ch04.xhtml "第4章。探索性数据分析")中进行了详细讨论，*探索性数据分析*，我们认为值得用Kibana中的简单仪表板总结到目前为止我们所涵盖的内容。从大约50,000篇文章中，我们能够在1月10日至11日获取并分析，我们过滤掉任何提到*David
    Bowie*作为NLP实体并包含*death*一词的记录。因为我们所有的文本内容都被正确索引在Elasticsearch中，我们可以在几秒钟内提取209篇匹配的文章及其内容。
- en: '![News index dashboard](img/B05261_06_5.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![新闻索引仪表板](img/B05261_06_5.jpg)'
- en: 'Figure 5: News Index Dashboard'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：新闻索引仪表板
- en: We can quickly get the top ten persons mentioned alongside **David Bowie**,
    including his stage name *Ziggy Stardust*, his son *Duncan Jones*, his former
    producer *Tony Visconti*, or the British prime minister *David Cameron*. Thanks
    to the *GeoLookup* service we built, we display all the different places mentioned,
    discovering a clique around the Vatican City state where the cardinal **Gianfranco
    Ravasi**, head of the pontifical council of culture, tweeted about *David Bowie's*
    famous lyrics of *Space Oddity*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以快速获取与**David Bowie**一起提到的前十位人物，包括他的艺名*Ziggy Stardust*、他的儿子*Duncan Jones*、他的前制作人*Tony
    Visconti*，或者英国首相*David Cameron*。由于我们建立的*GeoLookup*服务，我们展示了所有提到的不同地点，发现了梵蒂冈城国家周围的一个圈子，那里的红衣主教**Gianfranco
    Ravasi**，文化部主席，发推特提到*David Bowie*的著名歌词*Space Oddity*。
- en: '![News index dashboard](img/image_06_006.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![新闻索引仪表板](img/image_06_006.jpg)'
- en: 'Figure 6: Vatican paid tribute from Twitter'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：梵蒂冈向推特致敬
- en: Finally, in the race to be the first news publishing company covering breaking
    news, finding the first one who published about *David Bowie's* death is as easy
    as a simple click!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在争先发布关于*David Bowie*去世的新闻的竞赛中，找到第一个报道的人就像简单的点击一样容易！
- en: Summary
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Data science is not just about machine learning. In fact, machine learning is
    only a small portion of it. In our understanding of what modern data science is,
    the science often happens exactly here, at the data enrichment process. The real
    magic occurs when one can transform a meaningless dataset into a valuable set
    of information and get new insights out of it. In this section, we have been describing
    how to build a fully functional data insight system using nothing more than a
    simple collection of URLs (and a bit of elbow grease).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学不仅仅是关于机器学习。事实上，机器学习只是其中的一小部分。在我们对现代数据科学的理解中，科学往往恰好发生在数据丰富化的过程中。真正的魔力发生在当一个人能够将一个无意义的数据集转化为有价值的信息集，并从中获得新的见解。在本节中，我们已经描述了如何使用简单的URL集合（和一点点努力）构建一个完全功能的数据洞察系统。
- en: In this chapter, we demonstrated how to create an efficient web scraper with
    Spark using the Goose library and how to extract and de-duplicate features out
    of raw text using NLP techniques and the GeoNames database. We also covered some
    interesting design patterns such as *mapPartitions* and *Bloom filters* that will
    be discussed further in [Chapter 14](ch14.xhtml "Chapter 14. Scalable Algorithms"),
    *Scalable Algorithms*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们演示了如何使用Goose库在Spark中创建高效的网络爬虫，以及如何使用NLP技术和GeoNames数据库从原始文本中提取和去重特征。我们还涵盖了一些有趣的设计模式，如*mapPartitions*和*Bloom
    filters*，这些将在[第14章](ch14.xhtml "第14章. 可扩展算法") *可扩展算法*中进一步讨论。
- en: In the next chapter, we will be focusing on the people we were able to extract
    from all these news articles. We will be describing how to create connections
    among them using simple contact chaining techniques, how to efficiently store
    and query a large graph from a Spark context, and how to use *GraphX* and *Pregel*
    to detect communities.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将专注于从所有这些新闻文章中提取出来的人们。我们将描述如何使用简单的联系链技术在它们之间建立联系，如何在Spark环境中高效存储和查询大型图表，以及如何使用*GraphX*和*Pregel*来检测社区。
