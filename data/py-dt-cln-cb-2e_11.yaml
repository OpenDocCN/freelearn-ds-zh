- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Tidying and Reshaping Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整理和重塑数据
- en: Echoing Leo Tolstoy’s wisdom (“Happy families are all alike; every unhappy family
    is unhappy in its own way.”), Hadley Wickham tells us, all tidy data is fundamentally
    alike, but all untidy data is messy in its own special way. How many times have
    we all stared at some rows of data and thought, *“What… how... why did they do
    that?”* This overstates the case somewhat. Although there are many ways that data
    can be poorly structured, there are limits to human creativity in this regard.
    It is possible to categorize the most frequent ways in which datasets deviate
    from normalized or tidy forms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 引用列夫·托尔斯泰的智慧（“幸福的家庭都是相似的；每个不幸的家庭都有其不幸的方式。”），哈德利·威克姆告诉我们，所有整洁的数据本质上是相似的，但所有不整洁的数据都有其独特的混乱方式。我们多少次盯着某些数据行，心里想，*“这…怎么回事…为什么他们这么做？”*
    这有点夸张。尽管数据结构不良的方式有很多，但在人类创造力方面是有限的。我们可以将数据集偏离标准化或整洁形式的最常见方式进行分类。
- en: 'This was Hadley Wickham’s observation in his seminal work on tidy data. We
    can lean on that work, and our own experiences with oddly structured data, to
    prepare for the reshaping we have to do. Untidy data often has one or more of
    the following characteristics: a lack of clarity about merge-by column relationships;
    data redundancy on the *one* side of one-to-many relationships; data redundancy
    due to many-to-many relationships; values stored in column names; multiple values
    stored in one variable value; and data not being structured at the unit of analysis.
    (Although the last category is not necessarily a case of untidy data, some of
    the techniques we will review in the next few recipes are applicable to common
    unit-of-analysis problems.)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是哈德利·威克姆在他关于整洁数据的开创性著作中的观察。我们可以依赖这项工作，以及我们自己在处理结构奇特的数据时的经验，为我们需要进行的重塑做好准备。不整洁的数据通常具有以下一种或多种特征：缺乏明确的按列合并关系；一对多关系中的*一方*数据冗余；多对多关系中的数据冗余；列名中存储值；将多个值存储在一个变量值中；数据未按分析单位进行结构化。（尽管最后一种情况不一定是数据不整洁的表现，但我们将在接下来的几个菜谱中回顾的某些技术也适用于常见的分析单位问题。）
- en: 'We use powerful tools in this chapter to deal with the challenges of data cleaning
    like the preceding. Specifically, we’ll go over the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用强大的工具来应对像前面那样的数据清理挑战。具体而言，我们将讨论以下内容：
- en: Removing duplicated rows
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除重复行
- en: Fixing many-to-many relationships
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修复多对多关系
- en: Using `stack` and `melt` to reshape data from wide to long format
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`stack`和`melt`将数据从宽格式重塑为长格式
- en: Melting multiple groups of columns
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多组列的合并
- en: Using `unstack` and `pivot` to reshape data from long to wide format
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`unstack`和`pivot`将数据从长格式重塑为宽格式
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need pandas, NumPy, and Matplotlib to complete the recipes in this
    chapter. I used pandas 2.1.4, but the code will run on pandas 1.5.3 or later.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章的任务，您将需要pandas、NumPy和Matplotlib。我使用的是pandas 2.1.4，但该代码可以在pandas 1.5.3或更高版本上运行。
- en: The code in this chapter can be downloaded from the book’s GitHub repository,
    [https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码可以从本书的GitHub仓库下载，[https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition)。
- en: Removing duplicated rows
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除重复行
- en: 'There are several reasons why we might have data duplicated at the unit of
    analysis:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在分析单位上的重复有几种原因：
- en: The existing DataFrame may be the result of a one-to-many merge, and the one
    side is the unit of analysis.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前的DataFrame可能是一次一对多合并的结果，其中一方是分析单位。
- en: The DataFrame is repeated measures or panel data collapsed into a flat file,
    which is just a special case of the first situation.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该DataFrame是反复测量或面板数据被压缩为平面文件，这只是第一种情况的特殊情况。
- en: We may be working with an analysis file where multiple one-to-many relationships
    have been flattened, creating many-to-many relationships.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能正在处理一个分析文件，其中多个一对多关系已被展平，形成多对多关系。
- en: When the *one* side is the unit of analysis, data on the *many* side may need
    to be collapsed in some way. For example, if we are analyzing outcomes for a cohort
    of students at a college, students are the unit of analysis; but we may also have
    course enrollment data for each student. To prepare the data for analysis, we
    might need to first count the number of courses, sum the total credits, or calculate
    the GPA for each student, before ending up with one row per student. To generalize
    from this example, we often need to aggregate the information on the *many* side
    before removing duplicated data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当*单一*侧面是分析单元时，*多重*侧面的数据可能需要以某种方式进行合并。例如，如果我们在分析一组大学生的结果，学生是分析单元；但我们也可能拥有每个学生的课程注册数据。为了准备数据进行分析，我们可能需要首先统计每个学生的课程数量、总学分或计算GPA，最后得到每个学生的一行数据。通过这个例子，我们可以概括出，在去除重复数据之前，我们通常需要对*多重*侧面的信息进行聚合。
- en: In this recipe, we look at the pandas techniques for removing duplicate rows,
    and consider when we do and don’t need to do aggregation during that process.
    We address duplication in many-to-many relationships in the next recipe.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们查看了pandas处理去除重复行的技巧，并考虑了在这个过程中什么时候需要进行聚合，什么时候不需要。在下一示例中，我们将解决多对多关系中的重复问题。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will work with the COVID-19 daily case data in this recipe. It has one row
    per day per country, each row having the number of new cases and new deaths for
    that day. There are also demographic data for each country, and running totals
    for cases and deaths, so the last row for each country provides total cases and
    total deaths.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将处理COVID-19每日病例数据。该数据集为每个国家提供每一天的一行数据，每行记录当天的新病例和新死亡人数。每个国家还有人口统计数据和病例死亡的累计总数，因此每个国家的最后一行提供了总病例数和总死亡人数。
- en: '**Data note**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: Our World in Data provides COVID-19 public use data at [https://ourworldindata.org/covid-cases](https://ourworldindata.org/covid-cases).
    The dataset includes total cases and deaths, tests administered, hospital beds,
    and demographic data such as median age, gross domestic product, and diabetes
    prevalence. The dataset used in this recipe was downloaded on March 3, 2024.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Our World in Data提供了COVID-19公共使用数据，网址是[https://ourworldindata.org/covid-cases](https://ourworldindata.org/covid-cases)。该数据集包括总病例和死亡人数、已进行的测试、医院床位以及诸如中位年龄、国内生产总值和糖尿病患病率等人口统计数据。本示例中使用的数据集是2024年3月3日下载的。
- en: How to do it…
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We use `drop_duplicates` to remove duplicated demographic data for each country
    in the COVID-19 daily data. We explore `groupby` as an alternative to `drop_duplicates`
    when we need to do some aggregation before removing duplicated data:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`drop_duplicates`去除每个国家在COVID-19每日数据中的重复人口统计数据。当我们需要先进行一些聚合再去除重复数据时，我们也可以探索`groupby`作为`drop_duplicates`的替代方法：
- en: 'Import `pandas` and the COVID-19 daily cases data:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`和COVID-19每日病例数据：
- en: '[PRE0]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create lists for the daily cases and deaths columns, case total columns, and
    demographic columns (the `total_cases` and `total_deaths` columns are the running
    totals for cases and deaths respectively for that country):'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每日病例和死亡列、病例总数列以及人口统计列创建列表（`total_cases`和`total_deaths`列分别是该国家的病例和死亡的累计总数）：
- en: '[PRE1]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a DataFrame with just the daily data:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个仅包含每日数据的DataFrame：
- en: '[PRE3]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Select one row per country.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个国家选择一行。
- en: 'Check to see how many countries (location) to expect by getting the number
    of unique locations. Sort by location and casedate. Then use `drop_duplicates`
    to select one row per location, and use the keep parameter to indicate that we
    want the last row for each country:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 检查预计有多少个国家（位置），方法是获取唯一位置的数量。按位置和病例日期排序。然后使用`drop_duplicates`选择每个位置的一个行，并使用keep参数指示我们希望为每个国家选择最后一行：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Sum the values for each group.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个组的值进行求和。
- en: 'Use the pandas DataFrame groupby method to sum total cases and deaths for each
    country. (We calculate sums for cases and deaths here rather than using the running
    total of cases and deaths already in the DataFrame.) Also, get the last value
    for some of the columns that are duplicated across all rows for each country:
    `median_age`, `gdp_per_capita`, `region`, and `casedate`. (We select only a few
    columns from the DataFrame.) Notice that the numbers match those from *step 4*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pandas的DataFrame groupby方法来计算每个国家的病例和死亡总数。（我们在这里计算病例和死亡的总和，而不是使用DataFrame中已存在的病例和死亡的累计总数。）同时，获取一些在每个国家的所有行中都重复的列的最后一个值：`median_age`、`gdp_per_capita`、`region`和`casedate`。（我们只选择DataFrame中的少数几列。）请注意，数字与*第4步*中的一致：
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The choice of `drop_duplicates` or `groupby` to eliminate data redundancy comes
    down to whether we need to do any aggregation before collapsing the *many* side.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 选择使用`drop_duplicates`还是`groupby`来消除数据冗余，取决于我们是否需要在压缩*多*方之前进行任何聚合。
- en: How it works...
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The COVID-19 data has one row per country per day, but very little of the data
    is actually daily data. Only `casedate`, `new_cases`, and `new_deaths` can be
    considered daily data. The other columns show cumulative cases and deaths, or
    demographic data. The cumulative data is redundant since we have the actual values
    for `new_cases` and `new_deaths`. The demographic data has the same values for
    each country across all days.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: COVID-19数据每个国家每天有一行，但实际上很少有数据是每日数据。只有`casedate`、`new_cases`和`new_deaths`可以视为每日数据。其他列则显示累计病例和死亡人数，或是人口统计数据。累计数据是冗余的，因为我们已有`new_cases`和`new_deaths`的实际值。人口统计数据在所有日期中对于每个国家来说值是相同的。
- en: There is an implied one-to-many relationship between the country (and its associated
    demographic data) on the *one* side and the daily data on the *many* side. We
    can recover that structure by creating a DataFrame with the daily data, and another
    DataFrame with the demographic data. We do that in *steps 3* and *4*. When we
    need totals across countries we can generate those ourselves, rather than storing
    redundant data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 国家（及其相关人口统计数据）与每日数据之间有一个隐含的一对多关系，其中*一*方是国家，*多*方是每日数据。我们可以通过创建一个包含每日数据的DataFrame和另一个包含人口统计数据的DataFrame来恢复这种结构。我们在*步骤3*和*4*中做到了这一点。当我们需要跨国家的总数时，我们可以自己生成，而不是存储冗余数据。
- en: The running totals variables are not completely useless, however. We can use
    them to check our calculations of total cases and total deaths. *Step 5* shows
    how we can use `groupby` to restructure data when we need to do more than drop
    duplicates. In this case, we want to summarize `new_cases` and `new_deaths` for
    each country.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，运行总计变量并非完全没有用处。我们可以使用它们来检查我们关于病例总数和死亡总数的计算。*步骤5*展示了如何在需要执行的不仅仅是去重时，使用`groupby`来重构数据。在这种情况下，我们希望对每个国家的`new_cases`和`new_deaths`进行汇总。
- en: There’s more...
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: I can sometimes forget a small detail. When changing the structure of data,
    the meaning of certain columns can change. In this example, `casedate` becomes
    the date for the last row for each country. We rename that column `lastdate`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我有时会忘记一个小细节。在改变数据结构时，某些列的含义可能会发生变化。在这个例子中，`casedate`变成了每个国家最后一行的日期。我们将该列重命名为`lastdate`。
- en: See also
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: We explore `groupby` in more detail in *Chapter 9*, *Fixing Messy Data When
    Aggregating*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第9章*《聚合时修复混乱数据》中更详细地探讨了`groupby`。
- en: Hadley Wickham’s *Tidy Data* paper is available at [https://vita.had.co.nz/papers/tidy-data.pdf](https://vita.had.co.nz/papers/tidy-data.pdf).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Hadley Wickham的*整洁数据*论文可以在[https://vita.had.co.nz/papers/tidy-data.pdf](https://vita.had.co.nz/papers/tidy-data.pdf)找到。
- en: Fixing many-to-many relationships
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修复多对多关系
- en: We sometimes have to work with a data table that was created from a many-to-many
    merge. This is a merge where merge-by column values are duplicated on both the
    left and right sides. As we discussed in the previous chapter, many-to-many relationships
    in a data file often represent multiple one-to-many relationships where the *one*
    side has been removed. There is a one-to-many relationship between dataset A and
    dataset B, and also a one-to-many relationship between dataset A and dataset C.
    The problem we sometimes have is that we receive a data file with B and C merged
    but with A excluded.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们必须处理从多对多合并创建的数据表。这是一种在左侧和右侧的合并列值都被重复的合并。正如我们在前一章中讨论的那样，数据文件中的多对多关系通常代表多个一对多关系，其中*一*方被移除。数据集A和数据集B之间有一对多关系，数据集A和数据集C之间也有一对多关系。我们有时面临的问题是，收到的数据文件将B和C合并在一起，而将A排除在外。
- en: The best way to work with data structured in this way is to recreate the implied
    one-to-many relationships, if possible. We do this by first creating a dataset
    structured like A; that is, how A is likely structured given the many-to-many
    relationship we see between B and C. The key to being able to do this is to identify
    a good merge-by column for the data on both sides of the many-to-many relationship.
    This column, or these columns, will be duplicated in both the B and C datasets,
    but will be unduplicated in the theoretical A dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这种结构的数据的最佳方式是重新创建隐含的一对多关系，若可能的话。我们通过首先创建一个类似 A 的数据集来实现；也就是说，假设有一个多对多关系，我们可以推测
    A 的结构是怎样的。能够做到这一点的关键是为数据两边的多对多关系识别出一个合适的合并列。这个列或这些列将在 B 和 C 数据集中重复，但在理论上的 A 数据集中不会重复。
- en: The data we use in this recipe is a good example. We have data from the Cleveland
    Museum of Art on its collections. We have multiple rows for every item in the
    museum’s collection. Those rows have data on the collection item (including title
    and creation date); the creator (including years of birth and death); and citations
    of the work in the press. When there are multiple creators and multiple citations,
    which is often, rows are duplicated. More precisely, the number of rows for each
    collection item is the Cartesian product of the number of citations and creations.
    So, if there are 5 citations and 2 creators, we see 10 rows for that item.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中使用的数据就是一个很好的例子。我们使用克利夫兰艺术博物馆的收藏数据。每个博物馆收藏品都有多行数据。这些数据包括收藏品的信息（如标题和创作日期）；创作者的信息（如出生和死亡年份）；以及该作品在媒体中的引文。当有多个创作者和多个引文时（这通常发生），行数会重复。更精确地说，每个收藏品的行数是引文和创作者数量的笛卡尔积。所以，如果有5条引文和2个创作者，我们将看到该项目有10行数据。
- en: What we want instead is a collections file with one row (and a unique identifier)
    for each item in the collection, a creators file with one row per creator for
    each item, and a citations file with one row per citation of each item. We will
    create those files in this recipe.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要的是一个收藏品文件，每个收藏品一行（并且有一个唯一标识符），一个创作者文件，每个创作者对应一行，和一个引文文件，每个引文对应一行。在本教程中，我们将创建这些文件。
- en: Some of you will have noticed that there is still more tidying up to do here.
    We ultimately want a separate creator file with one row for every creator, and
    another file with just a creator id and a collection item id. We need this structure
    because a creator may be the creator for multiple items. We ignore that added
    complication in this recipe.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的一些人可能已经注意到，这里还有更多的整理工作要做。我们最终需要一个单独的创作者文件，每个创作者一行，另一个文件只包含创作者 ID 和收藏品 ID。我们需要这种结构，因为一个创作者可能会为多个项目创作。我们在这个教程中忽略了这个复杂性。
- en: I should add that this situation is not the fault of the Cleveland Museum of
    Art, which generously provides an API that returns collections data as a JSON
    file. It is the responsibility of individuals who use the API to create data files
    that are most appropriate for their research purposes. It is also possible, and
    often a good choice, to work directly from the more flexible structure of a JSON
    file. We demonstrate how to do that in *Chapter 12*, *Automate Data Cleaning with
    User-Defined Functions, Classes, and Pipelines*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该补充的是，这种情况并不是克利夫兰艺术博物馆的错，该博物馆慷慨地提供了一个 API，可以返回作为 JSON 文件的收藏数据。使用 API 的个人有责任创建最适合自己研究目的的数据文件。直接从更灵活的
    JSON 文件结构中工作也是可能的，而且通常是一个不错的选择。我们将在 *第12章*，*使用用户定义的函数、类和管道自动清理数据* 中演示如何操作。
- en: Getting ready
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will work with data on the Cleveland Museum of Art’s collections. The CSV
    file has data on both creators and citations, merged by an `itemid` column that
    identifies the collection item. There are one or many rows for citations and creators
    for each item.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用克利夫兰艺术博物馆收藏的数据。CSV 文件包含有关创作者和引文的数据，这些数据通过 `itemid` 列合并，`itemid` 用来标识收藏品。每个项目可能有一行或多行关于引文和创作者的数据。
- en: '**Data note**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: 'The Cleveland Museum of Art provides an API for public access to this data:
    [https://openaccess-api.clevelandart.org/](https://openaccess-api.clevelandart.org/).
    Much more than the citations and creators data is available in the API. The data
    in this recipe were downloaded in April 2024.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 克利夫兰艺术博物馆提供了一个公共访问数据的 API：[https://openaccess-api.clevelandart.org/](https://openaccess-api.clevelandart.org/)。API
    提供的数据远不止引文和创作者的数据。本文中的数据是2024年4月下载的。
- en: How to do it…
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We handle many-to-many relationships between DataFrames by recovering the multiple
    implied one-to-many relationships in the data:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过恢复数据中隐含的多个一对多关系来处理 DataFrame 之间的多对多关系：
- en: 'Import `pandas` and the museum’s `collections` data. Let’s also limit the length
    of values in the `collection` and `title` columns for easier display:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 和博物馆的 `collections` 数据。为了更方便显示，我们还将限制 `collection` 和 `title` 列中值的长度：
- en: '[PRE15]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Show some of the museum’s `collections` data. Notice that almost all of the
    data values are redundant, except for `citation`.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示博物馆的一些 `collections` 数据。注意，几乎所有的数据值都是冗余的，除了 `citation`。
- en: 'Also, show the number of unique `itemid`, `citation`, and `creator` values.
    There are 986 unique collection items, 12,941 citations, and 1,062 item/creator
    combinations:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，显示唯一的 `itemid`、`citation` 和 `creator` 值的数量。有 986 个独特的集合项，12,941 个引用，以及 1,062
    对 item/creator 组合：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Show a collection item with duplicated citations and creators.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示一个包含重复引用和创作者的集合项。
- en: 'Only show the first 6 rows (there are actually 28 in total). Notice that the
    citation data is duplicated for every creator:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 只显示前 6 行（实际上共有 28 行）。注意，引用数据对于每个创作者都是重复的：
- en: '[PRE26]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create a collections DataFrame. `title`, `category`, and `creation_date` should
    be unique to a collection item, so we create a DataFrame with just those columns,
    along with the `itemid` index. We get the expected number of rows, `986`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个集合 DataFrame。`title`、`category` 和 `creation_date` 应该是每个集合项唯一的，因此我们创建一个仅包含这些列的
    DataFrame，并带有 `itemid` 索引。我们得到预期的行数 `986`：
- en: '[PRE28]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s look at the row in the new DataFrame, `cmacollections`, for the same
    item we displayed in *step 3*:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看新 DataFrame `cmacollections` 中的同一项，该项在 *步骤 3* 中已经展示过：
- en: '[PRE32]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Create a citations DataFrame.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个引用（citations）DataFrame。
- en: 'This will just have `itemid` and `citation`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将只包含 `itemid` 和 `citation`：
- en: '[PRE34]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create a creators DataFrame:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个创作者 DataFrame：
- en: '[PRE36]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Count the number of collection items with a creator born after 1950.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统计出生在 1950 年后创作者的集合项数量。
- en: 'First, convert the `birth_year` values from string to numeric. Then, create
    a DataFrame with just young artists:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将 `birth_year` 值从字符串转换为数字。然后，创建一个只包含年轻艺术家的 DataFrame：
- en: '[PRE38]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, we can merge the `youngartists` DataFrame with the collections DataFrame
    to create a flag for collection items that have at least one creator born after
    1950:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将 `youngartists` DataFrame 与集合 DataFrame 合并，创建一个标记，用于标识至少有一个创作者出生在 1950
    年后 的集合项：
- en: '[PRE42]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now we have three DataFrames—collection items (`cmacollections`), citations
    (`cmacitations`), and creators (`cmacreators`)—instead of one. `cmacollections`
    has a one-to-many relationship with both `cmacitations` and `cmacreators`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了三个 DataFrame——集合项（`cmacollections`）、引用（`cmacitations`）和创作者（`cmacreators`）——而不是一个。`cmacollections`
    与 `cmacitations` 和 `cmacreators` 都存在一对多关系。
- en: How it works...
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: If you mainly work directly with enterprise data, you probably rarely see a
    file with this kind of structure, but many of us are not so lucky. If we requested
    data from the museum on both the media citations and creators of their collections,
    it would not be completely surprising to get a data file similar to this one,
    with duplicated data for citations and creators. But the presence of what looks
    like a unique identifier of collection items gives us some hope of recovering
    the one-to-many relationships between a collection item and its citations, and
    a collection item and its creators.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你主要直接处理企业数据，你可能很少会看到这种结构的文件，但许多人并没有这么幸运。如果我们从博物馆请求关于其收藏的媒体引用和创作者的数据，得到类似这样的数据文件并不完全令人惊讶，其中引用和创作者的数据是重复的。但看起来像是集合项唯一标识符的存在，让我们有希望恢复集合项与其引用之间、一对多的关系，以及集合项与创作者之间、一对多的关系。
- en: '*Step 2* shows that there are 986 unique `itemid` values. This suggests that
    there are probably only 986 collection items represented in the 17,001 rows of
    the DataFrame. There are 12,941 unique `itemid` and `citation` pairs, or about
    13 citations per collection item on average. There are 1,062 `itemid` and `creator`
    pairs.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 2* 显示有 986 个独特的 `itemid` 值。这表明在 17,001 行的 DataFrame 中，可能只包含 986 个集合项。共有
    12,941 对独特的 `itemid` 和 `citation`，即每个集合项平均有约 13 条引用。共有 1,062 对 `itemid` 和 `creator`。'
- en: '*Step 3* shows the duplication of collection item values such as `title`. The
    number of rows returned is equal to the Cartesian product of the merge-by values
    on the left and right sides of a merge. For the *Dead Blue Roller* item, there
    are 28 rows (we only show six of them in *step 3*), since there were 14 citations
    and 2 creators. The row for each creator is duplicated 14 times; once for each
    citation. Each citation is there twice; once for each creator. There are very
    few use cases for which it makes sense to leave the data in this state.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 3* 展示了集合项目值（如 `title`）的重复情况。返回的行数等于左右合并条件的笛卡尔积。对于 *Dead Blue Roller* 项目，共有28行（我们在*步骤
    3*中只展示了其中6行），因为它有14个引用和2个创作者。每个创作者的行会被重复14次；每个引用重复一次，针对每个创作者。每个引用会出现两次；一次针对每个创作者。对于非常少的用例，保留这种状态的数据是有意义的。'
- en: Our North Star to guide us in getting this data into better shape is the `itemid`
    column. We use it to create a collections DataFrame in *step 4*. We keep only
    one row for each value of `itemid`, and get other columns associated with a collection
    item, rather than a citation or creator—`title`, `category`, and `creation_date`
    (since `itemid` is the index, we need to first reset the index before dropping
    duplicates).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的“北极星”是 `itemid` 列，它帮助我们将数据转化为更好的结构。在*步骤 4*中，我们利用它来创建集合 DataFrame。我们仅保留每个
    `itemid` 值的一行，并获取与集合项目相关的其他列，而非引用或创作者——`title`、`category` 和 `creation_date`（因为
    `itemid` 是索引，我们需要先重置索引，然后再删除重复项）。
- en: 'We follow the same procedure to create `citations` and `creators` DataFrames
    in *steps 6* and *7*. We use `drop_duplicates` to keep unique combinations of
    `itemid` and `citation`, and unique combinations of `itemid` and `creator`, respectively.
    This gives us the expected number of rows in the example case: 14 `citations`
    rows and 2 `creators` rows.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照相同的程序，在*步骤 6* 和 *步骤 7* 中分别创建 `citations` 和 `creators` DataFrame。我们使用 `drop_duplicates`
    保留 `itemid` 和 `citation` 的唯一组合，和 `itemid` 和 `creator` 的唯一组合。这让我们得到了预期的行数：14行 `citations`
    数据和2行 `creators` 数据。
- en: '*Step 8* demonstrates how we can now work with these DataFrames to construct
    new columns and do analysis. We want the number of collection items that have
    at least one creator born after 1950\. The unit of analysis is the collection
    items, but we need information from the creators DataFrame for the calculation.
    Since the relationship between `cmacollections` and `cmacreators` is one-to-many,
    we make sure that we are only retrieving one row per `itemid` in the creators
    DataFrame, even if more than one creator for an item was born after 1950:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 8* 展示了我们如何使用这些 DataFrame 来构建新列并进行分析。我们想要计算至少有一个创作者出生在1950年之后的集合项目数量。分析的单位是集合项目，但我们需要从创作者
    DataFrame 中获取信息来进行计算。由于 `cmacollections` 和 `cmacreators` 之间是多对一的关系，我们确保在创作者 DataFrame
    中每个 `itemid` 只获取一行数据，即使某个项目有多个创作者出生在1950年之后：'
- en: '[PRE46]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: There’s more...
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The duplication that occurs with many-to-many merges is most problematic when
    we are working with quantitative data. If the original file had the assessed value
    of each item in the collection, it would be duplicated in much the same way as
    `title` is duplicated. Any descriptive statistics we generated on the assessed
    value would be off by a fair bit. For example, if the *Dead Blue Roller* item
    had an assessed value of $1,000,000, we would get $28,000,000 when summarizing
    the assessed value, since there are 28 duplicated values.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理定量数据时，多对多合并所产生的重复数据最为棘手。如果原始文件中包含了每个集合项目的评估价值，这些值将像 `title` 一样被重复。如果我们对评估价值生成描述性统计信息，结果会偏差很大。例如，如果
    *Dead Blue Roller* 项目的评估价值为1,000,000美元，在汇总评估价值时，我们将得到28,000,000美元，因为有28个重复值。
- en: This shows the importance of normalized and tidy data. If there were an assessed
    value column, we would have included it in the `cmacollections` DataFrame we created
    in *step 4*. This value would be unduplicated and we would be able to generate
    summary statistics for collections.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这突显了标准化和整洁数据的重要性。如果有评估价值列，我们会将其包含在*步骤 4*中创建的 `cmacollections` DataFrame 中。这个值将不会被重复，并且我们能够为集合生成汇总统计数据。
- en: I find it helpful to always return to the unit of analysis, which overlaps with
    the tidy data concept but is different in some ways. The approach in *step 8*
    would have been very different if we were just interested in the number of creators
    born after 1950, instead of the number of collection items with a creator born
    after 1950\. In that case, the unit of analysis would be the creator and we would
    just use the creators DataFrame.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现始终回到分析单位是非常有帮助的，它与整洁数据的概念有重叠，但在某些方面有所不同。如果我们只关心1950年后出生的创作者的数量，而不是1950年后出生的创作者所对应的收藏项数量，*第
    8 步*中的方法会完全不同。在这种情况下，分析单位将是创作者，我们只会使用创作者数据框。
- en: See also
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: We examine many-to-many merges in the *Doing many-to-many merges* recipe in
    *Chapter 10*, *Addressing Data Issues When Combining DataFrames*.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第 10 章*的*处理合并数据框时的数据问题*部分中讨论了多对多合并。
- en: We demonstrate a very different way to work with data structured in this way
    in *Chapter 12*, *Automate Data Cleaning with User-Defined Functions, Classes
    and Pipelines*, in the *Classes that handle non-tabular data structures* recipe.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第 12 章*的*使用用户定义函数、类和管道自动化数据清理*部分中的*处理非表格数据结构的类*示例中，展示了处理这种结构数据的完全不同方式。
- en: Using stack and melt to reshape data from wide to long format
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 stack 和 melt 将数据从宽格式转换为长格式
- en: One type of untidiness that Wickham identified is variable values embedded in
    column names. Although this rarely happens with enterprise or relational data,
    it is fairly common with analytical or survey data. Variable names might have
    suffixes that indicate a time period, such as a month or year. Or similar variables
    on a survey might have similar names, such as `familymember1age` and `familymember2age`,
    because that is convenient and consistent with the survey designers’ understanding
    of the variable.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Wickham 确定的一种不整洁数据类型是将变量值嵌入列名中。虽然在企业或关系型数据中这种情况很少发生，但在分析数据或调查数据中却相当常见。变量名可能会有后缀，指示时间段，如月份或年份。或者，调查中相似的变量可能有类似的名称，比如
    `familymember1age` 和 `familymember2age`，因为这样便于使用，并且与调查设计者对变量的理解一致。
- en: One reason why this messiness happens relatively frequently with survey data
    is that there can be multiple units of analysis on one survey instrument. An example
    is the United States decennial census, which asks both household and personal
    questions. Survey data is also sometimes made up of repeated measures or panel
    data, but nonetheless often has only one row per respondent. When this is the
    case, new measurements or responses are stored in new columns rather than new
    rows, and the column names will be similar to column names for responses from
    earlier periods, except for a change in suffix.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 调查数据中这种混乱相对频繁发生的一个原因是，一个调查工具上可能有多个分析单位。一个例子是美国的十年一次人口普查，它既询问家庭问题，也询问个人问题。调查数据有时还包括重复测量或面板数据，但通常每个受访者只有一行数据。在这种情况下，新测量值或新回答会存储在新列中，而不是新行中，列名将与早期时期的响应列名相似，唯一的区别是后缀的变化。
- en: The United States **National Longitudinal Survey of Youth** (**NLS**) is a good
    example of this. It is panel data, where each individual is surveyed each year.
    However, there is just one row of data per respondent in the analysis file provided.
    Responses to questions such as the number of weeks worked in a given year are
    placed in new columns. Tidying the NLS data means converting columns such as `weeksworked17`
    through `weeksworked21` (for weeks worked in 2017 through 2021) to just one column
    for weeks worked, another column for year, and five rows for each person (one
    for each year) rather than one. This is sometimes referred to as converting data
    from *wide to long* format.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**美国青年纵向调查**（**NLS**）是一个很好的例子。它是面板数据，每个个体每年都进行调查。然而，分析文件中每个受访者只有一行数据。类似“在某一年工作了多少周”这样的问题的回答会放入新的列中。整理
    NLS 数据意味着将如 `weeksworked17` 到 `weeksworked21`（即2017年到2021年间的工作周数）等列，转换成仅有一列表示工作周数，另一列表示年份，且每个人有五行数据（每年一行），而不是一行数据。这有时被称为将数据从*宽格式转换为长格式*。'
- en: 'Amazingly, pandas has several functions that make transformations like this
    relatively easy: `stack`, `melt`, and `wide_to_long`. We use `stack` and `melt`
    in this recipe, and explore `wide_to_long` in the next.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，pandas 有几个函数使得像这样的转换相对容易：`stack`、`melt` 和 `wide_to_long`。我们在这个示例中使用 `stack`
    和 `melt`，并在接下来的部分探讨 `wide_to_long`。
- en: Getting ready
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will work with the NLS data on the number of weeks worked and college enrollment
    status for each year. The DataFrame has one row per survey respondent.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将处理NLS中每年工作周数和大学入学状态的数据。DataFrame中每行对应一位调查参与者。
- en: '**Data note**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: The **National Longitudinal Surveys** (**NLS**), administered by the United
    States Bureau of Labor Statistics, are longitudinal surveys of individuals who
    were in high school in 1997 when the surveys started. Participants were surveyed
    each year through 2023\. The surveys are available for public use at [nlsinfo.org](https://nlsinfo.org).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**国家纵向调查**（**NLS**），由美国劳工统计局管理，是对1997年开始时在高中的个体进行的纵向调查。参与者每年接受一次调查，直到2023年。调查数据可供公众使用，网址为[nlsinfo.org](https://nlsinfo.org)。'
- en: How to do it…
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will use `stack` and `melt` to transform the NLS weeks worked data from
    wide to long, pulling out year values from the column names as we do so:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`stack`和`melt`将NLS工作周数据从宽格式转换为长格式，同时提取列名中的年份值：
- en: 'Import `pandas` and the NLS data:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`和NLS数据：
- en: '[PRE47]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: View some of the values for the number of weeks worked.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看一些工作周数的值。
- en: 'First, set the index:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，设置索引：
- en: '[PRE48]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Use `stack` to transform the data from wide to long.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`stack`将数据从宽格式转换为长格式。
- en: 'First, select only the `weeksworked##` columns. Use stack to move each column
    name in the original DataFrame into the index and move the `weeksworked##` values
    into the associated row. Reset the `index` so that the `weeksworked##` column
    names become the values for the `level_1` column (which we rename `year`), and
    the `weeksworked##` values become the values for the 0 column (which we rename
    `weeksworked`):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，仅选择`weeksworked##`列。使用stack将原始DataFrame中的每个列名移入索引，并将`weeksworked##`的值移入相应的行。重置`index`，使得`weeksworked##`列名成为`level_1`列（我们将其重命名为`year`）的值，`weeksworked##`的值成为0列（我们将其重命名为`weeksworked`）的值：
- en: '**Data note**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: For future upgrades to `pandas 3.0`, we would have to mention keyword argument
    as `(future_stack=True)` in the `stack` function.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于未来升级到`pandas 3.0`，我们需要在`stack`函数中提到关键字参数`(future_stack=True)`。
- en: '[PRE52]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Fix the `year` values.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修正`year`值。
- en: 'Get the last digits of the year values, convert them to integers, and add 2,000:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 获取年份值的最后几位数字，将其转换为整数，并加上2,000：
- en: '[PRE54]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Alternatively, use `melt` to transform the data from wide to long.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，使用`melt`将数据从宽格式转换为长格式。
- en: 'First, reset the `index` and select the `originalid` and `weeksworked##` columns.
    Use the `id_vars` and `value_vars` parameters of `melt` to specify `originalid`
    as the `ID` variable and the `weeksworked##` columns as the columns to be rotated,
    or melted. Use the `var_name` and `value_name` parameters to rename the columns
    as `year` and `weeksworked` respectively. The column names in `value_vars` become
    the values for the new year column (which we convert to an integer using the original
    suffix). The values for the `value_vars` columns are assigned to the new `weeksworked`
    column for the associated row:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，重置`index`并选择`originalid`和`weeksworked##`列。使用`melt`的`id_vars`和`value_vars`参数，指定`originalid`作为`ID`变量，并将`weeksworked##`列作为要旋转或熔化的列。使用`var_name`和`value_name`参数将列名重命名为`year`和`weeksworked`。`value_vars`中的列名成为新`year`列的值（我们使用原始后缀将其转换为整数）。`value_vars`列的值被分配到新`weeksworked`列中的相应行：
- en: '[PRE58]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Reshape the college enrollment columns with `melt`.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`melt`重塑大学入学列。
- en: 'This works the same way as the `melt` function for the weeks worked columns:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这与`melt`函数在处理工作周数列时的作用相同：
- en: '[PRE60]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Merge the weeks worked and college enrollment data:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并工作周数和大学入学数据：
- en: '[PRE62]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: This gives us one DataFrame from the melting of both the weeks worked and the
    college enrollment columns.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这将通过熔化工作周数和大学入学列，生成一个DataFrame。
- en: How it works...
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: We can use `stack` or `melt` to reshape data from wide to long format, but `melt`
    provides more flexibility. `stack` will move all of the column names into the
    index. We see in *step 4* that we get the expected number of rows after stacking,
    `44920`, which is 5*8984, the number of rows in the initial data.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`stack`或`melt`将数据从宽格式重塑为长格式，但`melt`提供了更多的灵活性。`stack`会将所有列名移动到索引中。我们在*第4步*中看到，堆叠后得到了预期的行数`44920`，这等于5*8984，即初始数据中的行数。
- en: With `melt`, we can rotate the column names and values based on an `ID` variable
    other than the index. We do this with the `id_vars` parameter. We specify which
    variables to melt by using the `value_vars` parameter.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`melt`，我们可以根据不同于索引的`ID`变量旋转列名和值。我们通过`id_vars`参数来实现这一点。我们使用`value_vars`参数指定要旋转的变量。
- en: In *step 6*, we also reshape the college enrollment columns. To create one DataFrame
    with the reshaped weeks worked and college enrollment data, we merge the two DataFrames
    we created in *steps 5* and *6*. We will see in the next recipe how to accomplish
    what we did in *steps 5* through *7* in one step.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 6* 中，我们还重新塑造了大学入学的列。为了将重新塑造后的工作周和大学入学数据合并为一个 DataFrame，我们合并了 *步骤 5* 和
    *步骤 6* 中创建的两个 DataFrame。我们将在下一个配方中看到如何一步完成 *步骤 5* 到 *步骤 7* 的工作。
- en: Melting multiple groups of columns
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 融合多个列组
- en: When we needed to melt multiple groups of columns in the previous recipe, we
    used `melt` twice and then merged the resulting DataFrames. That worked fine,
    but we can accomplish the same tasks in one step with the `wide_to_long` function.
    `wide_to_long` has more functionality than `melt`, but is a bit more complicated
    to use.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个配方中，当我们需要融合多个列组时，我们使用了两次 `melt` 然后合并了结果的 DataFrame。那样也可以，但我们可以用 `wide_to_long`
    函数在一步内完成相同的任务。`wide_to_long` 的功能比 `melt` 强大，但使用起来稍微复杂一些。
- en: Getting ready
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will work with the weeks worked and college enrollment data from the NLS
    in this recipe.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本配方中使用 NLS 的工作周和大学入学数据。
- en: How to do it…
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will transform multiple groups of columns at once using `wide_to_long`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `wide_to_long` 一次性转换多个列组：
- en: 'Import `pandas` and load the NLS data:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 并加载 NLS 数据：
- en: '[PRE66]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'View some of the weeks worked and college enrollment data:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看一些工作周和大学入学的数据：
- en: '[PRE67]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Run the `wide_to_long` function.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `wide_to_long` 函数。
- en: 'Pass a list to stubnames to indicate the column groups that are wanted. (All
    columns starting with the same characters as each item in the list will be selected
    for melting.) Use the `i` parameter to indicate the ID variable (`originalid`),
    and use the `j` parameter to name the column (`year`) that is based on the column
    suffixes—`17`, `18`, and so on:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个列表传递给 `stubnames` 以指示所需的列组。（所有列名以列表中每一项的相同字符开头的列都会被选中进行转换。）使用 `i` 参数指示 ID
    变量（`originalid`），并使用 `j` 参数指定基于列后缀（如 `17`、`18` 等）命名的列（`year`）：
- en: '[PRE69]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '`wide_to_long` accomplishes in one step what it took us several steps to accomplish
    in the previous recipe using `melt`.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`wide_to_long` 一步完成了我们在前一个配方中使用 `melt` 需要多个步骤才能完成的工作。'
- en: How it works...
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The `wide_to_long` function does almost all of the work for us, though it takes
    more effort to set it up than for `stack` or `melt`. We need to provide the function
    with the characters (`weeksworked` and `colenroct` in this case) of the column
    groups. Since our variables are named with suffixes indicating the year, `wide_to_long`
    translates the suffixes into values that make sense and melts them into the column
    that is named with the `j` parameter. It’s almost magic!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`wide_to_long` 函数几乎为我们完成了所有工作，尽管它的设置比 `stack` 或 `melt` 要复杂一些。我们需要向函数提供列组的字符（在这个例子中是
    `weeksworked` 和 `colenroct`）。由于我们的变量名称带有表示年份的后缀，`wide_to_long` 会将这些后缀转换为有意义的值，并将它们融合到用
    `j` 参数命名的列中。这几乎就像魔法一样！'
- en: There’s more...
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The suffixes of the `stubnames` columns in this recipe are the same: 17 through
    21\. But that does not have to be the case. When suffixes are present for one
    column group, but not for another, the values for the latter column group for
    that suffix will be missing. We can see that by excluding `weeksworked17` from
    the DataFrame and adding `weeksworked16`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方中 `stubnames` 列的后缀是相同的：17 到 21。但这不一定是必然的。当某个列组有后缀，而另一个没有时，后者列组对应后缀的值将会缺失。通过排除
    DataFrame 中的 `weeksworked17` 并添加 `weeksworked16`，我们可以看到这一点：
- en: '[PRE71]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The `weeksworked` values for 2017 are now missing, as are the `colenroct` values
    for 2016.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，2017 年的 `weeksworked` 值缺失了，2016 年的 `colenroct` 值也缺失了。
- en: Using unstack and pivot to reshape data from long to wide format
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `unstack` 和 `pivot` 将数据从长格式转换为宽格式
- en: Sometimes, we actually have to move data from a tidy to an untidy structure.
    This is often because we need to prepare the data for analysis with software packages
    that do not handle relational data well, or because we are submitting data to
    some external authority that has requested it in an untidy format. `unstack` and
    `pivot` can be helpful when we need to reshape data from long to wide format.
    `unstack` does the opposite of what we did with `stack`, and `pivot` does the
    opposite of `melt`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们实际上需要将数据从整洁格式转换为杂乱格式。这通常是因为我们需要将数据准备为某些不擅长处理关系型数据的软件包分析，或者因为我们需要提交数据给某个外部机构，而对方要求以杂乱格式提供数据。`unstack`
    和 `pivot` 在需要将数据从长格式转换为宽格式时非常有用。`unstack` 做的是与我们使用 `stack` 的操作相反的事，而 `pivot` 做的则是与
    `melt` 相反的操作。
- en: Getting ready
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We continue to work with the NLS data on weeks worked and college enrollment
    in this recipe.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本食谱中继续处理关于工作周数和大学入学的 NLS 数据。
- en: How to do it…
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'We use `unstack` and `pivot` to return the melted NLS DataFrame to its original
    state:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`unstack`和`pivot`将融化的 NLS 数据框恢复到其原始状态：
- en: 'Import `pandas` and load the stacked and melted NLS data:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`并加载堆叠和融化后的 NLS 数据：
- en: '[PRE73]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Stack the data again.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次堆叠数据。
- en: 'This repeats the stack from an earlier recipe in this chapter:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这重复了本章早期食谱中的堆叠操作：
- en: '[PRE74]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Melt the data again.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次融化数据。
- en: 'This repeats the `melt` from an earlier recipe in this chapter:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这重复了本章早期食谱中的`melt`操作：
- en: '[PRE76]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Use `unstack` to convert the stacked data from long to wide:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`unstack`将堆叠的数据从长格式转换为宽格式：
- en: '[PRE78]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Use `pivot` to convert the melted data from long to wide.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pivot`将融化的数据从长格式转换为宽格式。
- en: 'pivot is slightly more complicated than unstack. We need to pass arguments
    to do the reverse of melt, telling pivot the column to use for the column name
    suffixes (`year`) and where to grab the values to be unmelted (from the `weeksworked`
    columns, in this case):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`pivot`比`unstack`稍微复杂一点。我们需要传递参数来执行 melt 的反向操作，告诉 pivot 使用哪个列作为列名后缀（`year`），并从哪里获取要取消融化的值（在本例中来自`weeksworked`列）：'
- en: '[PRE80]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: This returns the NLS data back to its original untidy form.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这将 NLS 数据返回到其原始的无序形式。
- en: How it works...
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: We first do a `stack` and a `melt` in *steps 2* and *3* respectively. This rotates
    the DataFrames from wide to long format. We then `unstack` (*step 4*) and `pivot`
    (*step 5*) those DataFrames to rotate them back from long to wide.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在*步骤 2*和*步骤 3*分别执行`stack`和`melt`。这将数据框从宽格式转换为长格式。然后我们使用`unstack`（*步骤 4*）和`pivot`（*步骤
    5*）将数据框从长格式转换回宽格式。
- en: '`unstack` uses the multi-index that is created by the `stack` to figure out
    how to rotate the data.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`unstack`使用由`stack`创建的多重索引来确定如何旋转数据。'
- en: The `pivot` function needs us to indicate the index column (`originalid`), the
    column whose values will be appended to the column names (`year`), and the name
    of the columns with the values to be unmelted (`weeksworked`). `Pivot` will return
    multilevel column names. We fix that by pulling from the second level with `[col[1]
    for col in weeksworked.columns[1:]]`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`pivot`函数需要我们指定索引列（`originalid`），将附加到列名中的列（`year`），以及包含要取消融化值的列名称（`weeksworked`）。`pivot`将返回多级列名。我们通过从第二级提取`[col[1]
    for col in weeksworked.columns[1:]]`来修复这个问题。'
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We explored key tidy data topics in this chapter. These topics included handling
    duplicated data, either by dropping rows where the data are redundant, or aggregating
    by group. We also restructured data stored in a many-to-many format into a tidy
    format. Finally, we stepped through several ways of converting data from wide
    to long format, and back to wide when necessary. Up next is the final chapter
    of the book, where we will learn to automate data cleaning with user-defined functions,
    classes and pipelines.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们探讨了关键的 tidy 数据主题。这些主题包括处理重复数据，可以通过删除冗余数据的行或按组聚合来处理。我们还将以多对多格式存储的数据重构为 tidy
    格式。最后，我们介绍了将数据从宽格式转换为长格式的几种方法，并在必要时将其转换回宽格式。接下来是本书的最后一章，我们将学习如何使用用户定义的函数、类和管道来自动化数据清理。
- en: Join our community on Discord
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的社区，参与 Discord 讨论
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://discord.gg/p8uSgEAETX](https://discord.gg/p8uSgEAETX )'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://discord.gg/p8uSgEAETX](https://discord.gg/p8uSgEAETX)'
- en: '![](img/QR_Code10336218961138498953.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code10336218961138498953.png)'
