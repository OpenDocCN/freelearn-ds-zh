- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Deploying, Maintaining, and Administrating DLT Pipelines Using Terraform
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Terraform 部署、维护和管理 DLT 数据管道
- en: In this chapter, we’re going to explore how an automation tool such as Terraform
    can be used to express data pipelines as code, commonly referred to as **Infrastructure
    as Code** ( **IAC** ), in Databricks. We’ll look at how to set up a local Terraform
    development environment using popular code editors such as VS Code so that we
    can experiment with deploying different resources to a Databricks workspace. Next,
    we’ll dive into how to represent data pipelines using Terraform and how to configure
    different aspects of a **Delta Live Tables** ( **DLT** ) pipeline. We’ll also
    look at how we can automate the validation and deployment of IaC to different
    Databricks workspaces, including a production workspace. Lastly, we’ll examine
    industry best practices and future considerations along the way.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何使用像 Terraform 这样的自动化工具将数据管道以代码的形式表达，这通常被称为**基础设施即代码**（**IAC**），并应用于
    Databricks。我们将学习如何使用流行的代码编辑器，如 VS Code，设置本地 Terraform 开发环境，以便我们可以尝试将不同的资源部署到 Databricks
    工作区。接下来，我们将深入探讨如何使用 Terraform 表示数据管道，并如何配置 **Delta Live Tables**（**DLT**）管道的不同方面。我们还将学习如何自动化
    IaC 的验证和部署到不同的 Databricks 工作区，包括生产工作区。最后，我们将讨论行业最佳实践和未来的考虑事项。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Introducing the Databricks provider for Terraform
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Terraform 的 Databricks 提供程序
- en: Setting up a local environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置本地环境
- en: Configuring DLT pipelines using Terraform
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Terraform 配置 DLT 数据管道
- en: Automating DLT pipeline deployment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化 DLT 数据管道部署
- en: Hands-on exercise – deploying a DLT pipeline using VS Code
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践操作 - 使用 VS Code 部署 DLT 数据管道
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To follow along with the examples provided in this chapter, you’ll need Databricks
    workspace permissions to create and start an all-purpose cluster so that you can
    import and execute the chapter’s accompanying notebooks. All code samples can
    be downloaded from this chapter’s GitHub repository located at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08)
    . This chapter will create and run several new notebooks, as well as run a new
    DLT pipeline using the product’s **Advanced** edition, estimated to consume around
    10—15 **Databricks** **Units** ( **DBUs** ).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本章提供的示例，你需要拥有 Databricks 工作区权限，以创建并启动通用集群，这样你才能导入并执行本章附带的笔记本。所有的代码示例可以从本章的
    GitHub 仓库下载，地址为：[https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08)。本章将创建并运行几个新的笔记本，还将使用产品的**高级**版运行一个新的
    DLT 数据管道，预计会消耗大约 10—15 **Databricks** **Units**（**DBUs**）。
- en: Introducing the Databricks provider for Terraform
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Terraform 的 Databricks 提供程序
- en: 'Terraform is an open source deployment automation tool that can be used to
    automate the deployment of cloud infrastructure in a repeatable and predictable
    manner. One reason Terraform is such a popular deployment tool is that it supports
    deploying infrastructure to the three major cloud providers: **Amazon Web Services**
    ( **AWS** ), **Azure** , and **Google Cloud Platform** ( **GCP** ). Terraform
    is centered around the concept of defining IaC where, rather than manually deploying
    cloud components such as network objects, virtual machines, or storage containers,
    they are expressed using code files. Furthe rmore, Terraform files are configuration-driven.
    Rather than expressing *how* to deploy the infrastructure, cloud administrators
    focus on expressing *what* changes between environments through configuration.
    Lastly, Terraform maintains the state of your architecture, meaning that the tool
    will keep track of the state of cloud resources and will update the state accordingly
    for each new execution of a Terraform configuration file.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 是一个开源部署自动化工具，可以用于以可重复和可预测的方式自动化部署云基础设施。Terraform 成为如此受欢迎的部署工具的原因之一是，它支持将基础设施部署到三个主要云提供商：**Amazon
    Web Services** (**AWS**)、**Azure** 和 **Google Cloud Platform** (**GCP**) 。Terraform
    的核心概念是定义基础设施即代码（IaC），即通过代码文件表达云组件（如网络对象、虚拟机或存储容器）的配置，而不是手动部署这些组件。此外，Terraform
    文件是配置驱动的。云管理员专注于通过配置表达环境之间的*变化*，而不是表达*如何*部署基础设施。最后，Terraform 维护着你的架构状态，意味着该工具将跟踪云资源的状态，并在每次执行
    Terraform 配置文件时相应地更新状态。
- en: Lastly, Terraform files can be executed directly from your local machine, allowing
    you to interact with cloud resources remotely.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Terraform 文件可以直接从本地机器执行，允许你远程与云资源进行交互。
- en: '![Figure 8.1 – Terraform will reflect environment changes using configuration
    files](img/B22011_08_1.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – Terraform 将通过配置文件反映环境变化](img/B22011_08_1.jpg)'
- en: Figure 8.1 – Terraform will reflect environment changes using configuration
    files
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – Terraform 将通过配置文件反映环境变化
- en: Terraform configuration files define the cloud infrastructure that is applied
    in the cloud provider, and the infrastructure state is synced back to the local
    environment. Furthermore, Databricks provides a Terraform provider for deploying
    Databricks workspaces and workspace objects to the major cloud providers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 配置文件定义了在云提供商中应用的云基础设施，基础设施状态会同步回本地环境。此外，Databricks 提供了一个 Terraform
    提供者，用于将 Databricks 工作区和工作区对象部署到主要云提供商中。
- en: A Terraform provider is a plugin for the Terraform tool that enables users to
    interact with specific APIs. In this case, the Terraform provider interacts with
    the Databricks REST API, allowing workspace administrators to automate the deployment
    of even the most complex data processing environments.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 提供者是 Terraform 工具的插件，使用户能够与特定的 API 进行交互。在这个案例中，Terraform 提供者与 Databricks
    REST API 交互，允许工作区管理员自动化部署即便是最复杂的数据处理环境。
- en: 'There are many advantages to using Terraform to automate the deployment of
    data pipelines within your organization, including the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Terraform 自动化部署组织内的数据管道有许多优势，包括以下几点：
- en: It is easy to deploy infrastructure between the major cloud providers, making
    it trivial to migrate between clouds if need be.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主要云提供商之间部署基础设施非常容易，使得如果需要，迁移云平台变得轻而易举。
- en: It is easy to scale to hundreds of data pipelines by focusing on defining configuration
    rather than manually deploying and maintaining data pipelines.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过专注于定义配置而不是手动部署和维护数据管道，轻松实现数百个数据管道的扩展。
- en: Pipeline definition is concise, allowing cloud administrators to focus on expressing
    what should change, rather than how to deploy infrastructure.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道定义简洁，允许云管理员专注于表达需要更改的内容，而不是如何部署基础设施。
- en: Let’s look at how easy it is to get started defining Databricks resources and
    applying them to targeted workspaces.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何轻松开始定义 Databricks 资源并将其应用于目标工作区。
- en: Setting up a local Terraform environment
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置本地 Terraform 环境
- en: Before we can begin deploying data pipeline objects to our Databricks workspace,
    we need to install the Terraform **command-line interface** ( **CLI** ) tool.
    If you haven’t already done so, you will need to download the Terraform CLI, which
    can be downloaded for free from the HashiCorp website ( [https://developer.hashicorp.com/terraform/install](https://developer.hashicorp.com/terraform/install)
    ).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始将数据管道对象部署到 Databricks 工作区之前，我们需要安装 Terraform **命令行界面**（**CLI**）工具。如果你还没有安装，你需要下载
    Terraform CLI，可以从 HashiCorp 网站免费获取（[https://developer.hashicorp.com/terraform/install](https://developer.hashicorp.com/terraform/install)）。
- en: Next, we want to organize the Terraform configuration files into a single directory.
    Let’s create a new directory called **chp8_databricks_terraform** .
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将 Terraform 配置文件组织到一个目录中。我们可以创建一个新的目录，命名为 **chp8_databricks_terraform**。
- en: Within the newly created directory, let’s create a brand new Terraform configuration
    file where we will define our data pipeline and other related workspace objects.
    Create a new file, naming it **main.tf** .
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在新创建的目录中，我们将创建一个全新的 Terraform 配置文件，在该文件中定义我们的数据管道和其他相关的工作区对象。创建一个新文件，命名为 **main.tf**。
- en: Important note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Terraform configuration files use the Terraform language and end with the **.**
    **tf** extension.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 配置文件使用 Terraform 语言，并以 **.** **tf** 扩展名结尾。
- en: Importing the Databricks Terraform provider
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入 Databricks Terraform 提供程序
- en: The first step to using Terraform to deploy Databricks workspace objects is
    to import the Databricks Terraform provider. If it’s your first time using the
    Terraf orm provider, Terraform will take care of downloading the Databricks provider
    from the Terraform Registry. The Terraform Registry is a public hub for downloading
    third-party providers, modules, and security policies that aid in developing Terraform
    configuration files to deploy your cloud infrastructure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Terraform 部署 Databricks 工作区对象的第一步是导入 Databricks Terraform 提供程序。如果你第一次使用 Terraform
    提供程序，Terraform 将自动从 Terraform 注册表下载 Databricks 提供程序。Terraform 注册表是一个公共中心，用于下载第三方提供程序、模块和安全策略，帮助开发
    Terraform 配置文件以部署云基础设施。
- en: 'Add the following code snippet at the top of the new Terraform configuration
    file, **main.tf** :'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下代码片段添加到新创建的 Terraform 配置文件 **main.tf** 的顶部：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code snippet will instruct the Terraform CLI tool to download and import
    a Terraform provider called **databricks** that has been published to the Terraform
    Registry by the Databricks organization.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将指示 Terraform CLI 工具下载并导入一个名为 **databricks** 的 Terraform 提供程序，该提供程序已经由 Databricks
    组织发布到 Terraform 注册表中。
- en: Now that we’ve imported the Databricks Terraform provider, we can begin deploying
    data pipeline objects to our Databricks workspace. But before we can do that,
    we must first authenticate with our Databricks workspace to make changes, such
    as creating a new DLT pipeline.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经导入了 Databricks Terraform 提供程序，可以开始将数据管道对象部署到 Databricks 工作区。但在此之前，我们必须先与
    Databricks 工作区进行身份验证，以便进行更改，例如创建新的 DLT 管道。
- en: Configuring workspace authentication
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置工作区身份验证
- en: If you recall, the Databricks Terraform provider will interact with the Databricks
    REST API behind the scenes. As a result, the same authentication mechanisms that
    are used to authenticate with the Databricks REST API and make workspace changes
    can be applied using Terraform.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，Databricks Terraform 提供程序将在后台与 Databricks REST API 交互。因此，用于与 Databricks
    REST API 进行身份验证并进行工作区更改的相同身份验证机制也可以通过 Terraform 使用。
- en: 'In total, there are about nine supported methods for authenticating with a
    Databricks workspace using the Terraform provider (the latest list can be found
    here: [https://registry.terraform.io/providers/databricks/databricks/latest/docs#authentication](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authentication)
    ). A few of the popular authentication methods include the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，使用 Terraform 提供程序进行 Databricks 工作区身份验证的支持方法大约有九种（最新的列表可以在此找到：[https://registry.terraform.io/providers/databricks/databricks/latest/docs#authentication](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authentication)）。其中一些常见的身份验证方法包括：
- en: Using a workspace administrator username and password
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用工作区管理员用户名和密码
- en: Using a Databricks **Personal Access** **Token** ( **PAT** )
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Databricks **个人访问** **令牌**（**PAT**）
- en: Using the Azure CLI or Google Cloud CLI
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Azure CLI 或 Google Cloud CLI
- en: If using the Azure cloud provider, using a service principal or managed service
    identity
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果使用 Azure 云提供商，需使用服务主体或托管服务标识
- en: Using the Databricks CLI ( user-to-machine authentication)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Databricks CLI（用户对机器身份验证）
- en: Important note
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Since we are doing local development and testing, in the following example,
    we’ll be generating an OAuth token using the Databricks CLI and logging in to
    our Databricks workspace manually. However, for production deployments, it’s recommended
    to securely store workspace credentials in a secrets manager such as Azure Key
    Vault, AWS Secrets Manager, or HashiCorp Vault, to name a few.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在进行本地开发和测试，在以下示例中，我们将使用 Databricks CLI 生成一个 OAuth 令牌，并手动登录到我们的 Databricks
    工作区。然而，对于生产部署，建议将工作区凭据安全地存储在像 Azure Key Vault、AWS Secrets Manager 或 HashiCorp
    Vault 等秘密管理器中。
- en: There are a couple of options for storing authentication tokens that are used
    with Terraform – directly within the Terraform configuration file as a part of
    the Databricks provider import, or on the local machine within a configuration
    file. We would recommend the latter option to avoid accidental exposure of credentials
    when checking in code artifacts to your code repository. The easiest method for
    populating this configuratio n file is by using the Databricks CLI.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种选项可以存储与 Terraform 一起使用的认证令牌——直接存储在 Terraform 配置文件中，作为 Databricks 提供程序导入的一部分，或者存储在本地机器中的配置文件中。我们建议选择后者，以避免在将代码文件提交到代码库时意外暴露凭证。填充此配置文件的最简单方法是使用
    Databricks CLI。
- en: 'The Databricks CLI supports Windows, Linux, or macOS operating systems, making
    it a cross-platform-compatible and versatile tool. If your local machine uses
    macOS or Linux operating systems, you can download the Databricks CLI using the
    Homebrew package manager using a shell prompt. Or you can easily upgrade the version
    of an existing Databricks CLI installation. For example, the following commands
    will install or upgrade an existing Databricks CLI installation using Homebrew
    on a Mac:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks CLI 支持 Windows、Linux 或 macOS 操作系统，使其成为跨平台兼容的多功能工具。如果你的本地机器使用的是 macOS
    或 Linux 操作系统，你可以通过命令行提示符使用 Homebrew 包管理器下载 Databricks CLI。或者，你也可以轻松地升级现有 Databricks
    CLI 安装的版本。例如，以下命令将在 Mac 上使用 Homebrew 安装或升级现有的 Databricks CLI 安装：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'On a Windows machine, you can install the Databricks CLI using the popular
    package manager, **winget** ( [https://learn.microsoft.com/windows/package-manager/winget/](https://learn.microsoft.com/windows/package-manager/winget/)
    ). The following commands will download and install the Databricks CLI using the
    **winget** utility:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 机器上，你可以使用流行的包管理器**winget**（[https://learn.microsoft.com/windows/package-manager/winget/](https://learn.microsoft.com/windows/package-manager/winget/)）安装
    Databricks CLI。以下命令将使用 **winget** 工具下载并安装 Databricks CLI：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once downloaded, you can configure authentication by executing the **configure**
    command in the Databricks CLI:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，你可以通过执行 **configure** 命令在 Databricks CLI 中配置认证：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When applying a Terraform configuration file to a target environment, the Terraform
    CLI will first check to see whether authentication details are provided directly
    within the configuration file. Otherwise, the Terraform CLI will look for the
    local Databricks configuration file, which gets stored in a special hidden file
    called **.databrickscfg** under your user’s home folder.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当将 Terraform 配置文件应用于目标环境时，Terraform CLI 会首先检查是否在配置文件中直接提供了认证信息。否则，Terraform
    CLI 会查找本地的 Databricks 配置文件，该文件存储在名为 **.databrickscfg** 的特殊隐藏文件中，位于用户的主文件夹下。
- en: 'You can also specify a profile name, which is helpful when you have multiple
    Databricks workspaces and you need to deploy infrastructure components between
    the different workspaces. Using the profiles, you can store authentication details
    separately and easily reference them during deployment. You can learn more about
    creating/testing a profile here: [https://docs.databricks.com/dev-tools/cli/profiles.html](https://docs.databricks.com/dev-tools/cli/profiles.html)
    .'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以指定一个配置文件名称，这在你有多个 Databricks 工作区，并且需要在不同工作区之间部署基础设施组件时非常有用。使用配置文件，你可以单独存储认证信息，并在部署期间轻松引用它们。你可以在此了解更多关于创建/测试配置文件的信息：[https://docs.databricks.com/dev-tools/cli/profiles.html](https://docs.databricks.com/dev-tools/cli/profiles.html)。
- en: Defining a DLT pipeline source notebook
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义 DLT 管道源笔记本
- en: In the next example, we’re going to define a notebook that will contain the
    start of a simple DLT pipeline and deploy the notebook to your user’s workspace
    directory in a target Databricks workspace.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将定义一个笔记本，包含一个简单 DLT 管道的开始部分，并将该笔记本部署到目标 Databricks 工作区中的用户工作区目录。
- en: 'To construct the workspace location of where to deploy the notebook, we’ll
    need to get your current user in Databricks. Rather than hardcoding this value,
    we can use the **databricks_current_user** data source, which retrieves the current
    user’s Databricks username at deployment time. Add the following configuration
    block to the **main.tf** file:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建部署笔记本的工作区位置，我们需要获取你当前在 Databricks 中的用户。为了避免硬编码此值，我们可以使用 **databricks_current_user**
    数据源，它在部署时获取当前用户的 Databricks 用户名。将以下配置块添加到 **main.tf** 文件中：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we’ll use the **databricks_notebook** resource to define a new Python
    notebook, using the previous data source to construct the notebook path. Since
    the notebook is fairly simple, containing only a single DLT dataset definition,
    we’ll define the notebook contents inline. Add the following configuration block
    to the **main.tf** file:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 **databricks_notebook** 资源定义一个新的 Python 笔记本，使用之前的数据源构造笔记本路径。由于笔记本相当简单，仅包含一个
    DLT 数据集定义，我们将在其中内联定义笔记本内容。将以下配置块添加到 **main.tf** 文件中：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, let’s add one last block to the **main.tf** configuration that prints
    the URL to the deployed notebook:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们向 **main.tf** 配置文件添加最后一个块，打印已部署笔记本的 URL：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Click **Save** to save the configuration file. In a terminal window, navigate
    to the directory containing the **main.tf** configuration file.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 **Save** 保存配置文件。在终端窗口中，导航到包含 **main.tf** 配置文件的目录。
- en: Applying workspace changes
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用工作区更改
- en: 'The first command that should be run is the **terraform init** command, which
    executes several initialization steps to prepare the current working directory
    to deploy cloud resources using Terraform. Execute the following command from
    a terminal window or shell prompt:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 应该运行的第一个命令是 **terraform init** 命令，该命令执行多个初始化步骤，以准备当前工作目录以便使用 Terraform 部署云资源。从终端窗口或
    shell 提示符执行以下命令：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, the Terraform CLI provides a way for us to validate the effects of a
    Terraform configuration file before applying the changes. Execute the **validate**
    com mand:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，Terraform CLI 提供了一种方法，让我们在应用更改之前验证 Terraform 配置文件的效果。执行 **validate** 命令：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Lastly, we can view the proposed infrastructure changes by listing all of the
    planned changes in the Terraform plan. Execute the following command to view the
    proposed Terraform plan:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过列出 Terraform 计划中的所有预定更改来查看拟议的基础设施更改。执行以下命令查看拟议的 Terraform 计划：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You’ll notice that there will be a single resource defined in the plan. In this
    case, it will be the new Databricks notebook containing our DLT dataset definition.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到计划中将定义一个单一的资源。在这种情况下，它将是包含我们 DLT 数据集定义的新 Databricks 笔记本。
- en: 'Once we validate that the plan looks good, we can then apply the changes to
    the target Databricks workspace. Apply the Terraform plan by executing the **apply**
    command:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们验证计划看起来没有问题，就可以将更改应用到目标 Databricks 工作区。通过执行 **apply** 命令应用 Terraform 计划：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output will be the full notebook URL to the newly created notebook. Copy
    the output URL and paste it into a browser window. Verify that there is a new
    notebook with Python set as the default programming language, containing a single
    notebook cell with the definition of a single DLT dataset, **yellow_taxi_raw**
    .
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是新创建笔记本的完整 URL。复制输出的 URL 并粘贴到浏览器窗口中。验证是否有一个新的笔记本，Python 被设置为默认编程语言，且包含一个包含单一
    DLT 数据集定义的笔记本单元，**yellow_taxi_raw**。
- en: Congratulations! You’ve written your first Terraform configuration file and
    you are well on your way to automating the deployment of your Databricks assets
    across environments. In the next section, we’ll expand on the previous example
    to see how the Databricks Terraform provider can be used to deploy DLT pipelines
    to workspaces.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经编写了第一个 Terraform 配置文件，并且在自动化部署 Databricks 资产到各个环境的道路上迈出了坚实的步伐。在下一节中，我们将基于前面的示例展开，看看
    Databricks Terraform 提供者如何用于将 DLT 管道部署到工作区。
- en: Configuring DLT pipelines using Terraform
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Terraform 配置 DLT 管道
- en: We will use the **databricks_pipeline** resource in the Databricks Terraform
    provider to deploy a DLT pipeline to a target Databricks workspace. The **databricks_pipeline**
    resource is the main building block for our Terraform configuration files. Within
    this Terraform resource, we can specify many different configuration options that
    will affect the deployment of our DLT pipeline. For example, we can configure
    the DLT production edition, a target Unity Catalog location, library dependencies,
    update cluster sizes, and more. Let’s dive into the exact configurations to get
    a better idea of the type of control you have over the DLT pipeline that gets
    deployed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Databricks Terraform 提供程序中的 **databricks_pipeline** 资源，将 DLT 管道部署到目标 Databricks
    工作区。**databricks_pipeline** 资源是我们 Terraform 配置文件的主要构建块。在此 Terraform 资源中，我们可以指定许多不同的配置选项，这些选项将影响
    DLT 管道的部署。例如，我们可以配置 DLT 生产版、目标 Unity Catalog 位置、库依赖项、更新集群大小等。让我们深入探讨这些具体配置，以便更好地了解您对部署的
    DLT 管道的控制。
- en: 'There are several arguments used to define the configuration and behavior of
    a DLT pipeline using the Databricks provider for Terraform. To get a better picture
    of the types of arguments, the following sections cover all the available arguments
    in the Databricks provider for Terraform (the latest version can be found here:
    [https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/pipeline](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/pipeline)
    ).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个参数用于定义使用 Databricks 提供程序进行 Terraform 配置的 DLT 管道的配置和行为。为了更好地了解这些参数，以下部分涵盖了
    Databricks 提供程序中 Terraform 的所有可用参数（最新版本可以在此找到：[https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/pipeline](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/pipeline)）。
- en: 'Generally speaking, the **databricks_pipeline** arguments can be thought about
    as falling into one of three possible categories:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，**databricks_pipeline** 参数可以被看作是属于以下三类之一：
- en: '**Runtime configuration** : **name** , **channel** , **development** , **continuous**
    , **edition** , **photon** , **configuration** , and **library**'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时配置**：**名称**，**频道**，**开发**，**持续**，**版本**，**光子**，**配置**，和 **库**'
- en: '**Pipeline compute** **configuration** : **cluster**'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道计算** **配置**：**集群**'
- en: '**Pipeline dataset storage configuration** : **catalog** , **target** , and
    **storage**'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道数据集存储配置**：**目录**，**目标**，和 **存储**'
- en: Let’s go through each argument in greater detail to get a better understanding
    of the effect that our Terraform configuration can have on a target DLT pipeline.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解每个参数，以更好地理解我们的 Terraform 配置对目标 DLT 管道的影响。
- en: name
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: name
- en: The **name** argument assigns an alphanumeric name to identify a DLT pipeline.
    The **name** argument should be a String that can contain mixed-case characters,
    numbers, spaces, and special characters (including emoji characters). Furthermore,
    the pipeline **name** argument doesn’t necessarily need to be unique; name uniqueness
    is not enforced by the Databricks Terraform provider. Upon creation of a DLT pipeline,
    the Databricks Data Intelligence Platform will assign a unique pipeline identifier
    to each pipeline, so the **name** argument is solely used as a convenient way
    for data engineers to distinguish their DLT pipelines from other pipelines from
    the DLT UI.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**name** 参数为 DLT 管道分配一个字母数字名称，以便标识该管道。**name** 参数应为一个字符串，可以包含大小写字母、数字、空格和特殊字符（包括表情符号字符）。此外，管道
    **name** 参数不一定需要唯一；Databricks Terraform 提供程序并不强制名称唯一。在创建 DLT 管道时，Databricks 数据智能平台会为每个管道分配一个唯一的管道标识符，因此
    **name** 参数仅作为数据工程师区分其 DLT 管道与其他管道的一种方便方式。'
- en: notification
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: notification
- en: The **notification** argument is used to specify a list of email recipients
    who will receive an email notification during specific pipeline events. The types
    of DLT pipeline events that will trigger a notification include **on-update-success**
    , **on-update-failure** , **on-update-fatal-failure** , and **on-flow-failure**
    .
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**notification** 参数用于指定在特定管道事件期间将收到电子邮件通知的收件人列表。触发通知的 DLT 管道事件类型包括 **on-update-success**，**on-update-failure**，**on-update-fatal-failure**
    和 **on-flow-failure**。'
- en: channel
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: channel
- en: 'The **channel** argument controls the type of Databricks runtime a DLT pipeline
    update cluster should use. There are only two options to choose from: **CURRENT**
    and **PREVIEW** . **CURRENT** selects the latest stable Databricks runtime release
    and is the default option. You may want to choose **PREVIEW** if your DLT pipeline
    is operating in a development environment, and you’d like to experiment with upcoming
    performance features and optimizations that haven’t made their way into the current
    Databricks runtime yet.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**通道**参数控制DLT管道更新集群应使用的Databricks运行时类型。只有两个选项可供选择：**CURRENT**和**PREVIEW**。**CURRENT**选择最新的稳定Databricks运行时版本，并且是默认选项。如果您的DLT管道在开发环境中运行，并且您想尝试尚未进入当前Databricks运行时的未来性能功能和优化，您可能希望选择**PREVIEW**。'
- en: development
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: development
- en: The **development** argument is a Boolean flag that controls whether you want
    to execute your DLT pipeline in Development mode or not. When set to **true**
    , Terraform will deploy a DLT pipeline, with the pipeline mode set to **Development**
    . This will also be reflected on the DLT UI by a toggle button at the top right
    of the DLT UI.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**开发**参数是一个布尔标志，用于控制是否希望以开发模式执行DLT管道。当设置为**true**时，Terraform将部署一个DLT管道，并将管道模式设置为**开发**模式。这也会通过DLT
    UI右上角的切换按钮在DLT UI中显示出来。'
- en: '![Figure 8.2 – Development mode is visible from the DLT UI as a toggle button](img/B22011_8_2.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 在DLT UI中，开发模式可以通过切换按钮显示](img/B22011_8_2.jpg)'
- en: Figure 8.2 – Development mode is visible from the DLT UI as a toggle button
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 在DLT UI中，开发模式可以通过切换按钮显示
- en: Similarly, when this argument is set to **false** , Terraform will set the pipeline
    mode to **Production** . If you recall from [*Chapter 2*](B22011_02.xhtml#_idTextAnchor052)
    , we mentioned that in Development mode, DLT will not retry pipeline updates in
    the event of a runtime exception and will also keep the update cluster up and
    running to help data engineers triage and fix bugs, thereby shortening debugging
    cycles.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当该参数设置为**false**时，Terraform将把管道模式设置为**生产**模式。如果您还记得[*第2章*](B22011_02.xhtml#_idTextAnchor052)，我们提到过在开发模式下，DLT在运行时异常发生时不会重试管道更新，并且还会保持更新集群的运行状态，以帮助数据工程师排查和修复错误，从而缩短调试周期。
- en: continuous
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: continuous
- en: The **continuous** argument is a Boolean flag that controls the frequency of
    pipeline update executions. When set to **true** , Terraform will deploy a DLT
    pipeline that will continuously update datasets within a DLT pipeline. Similarly,
    when set to **false** , the DLT pipeline will be deployed with a triggered execution
    mode. In this type of execution mode, a data engineer will need to trigger the
    start of a pipeline update either by clicking the **Start** button on the DLT
    UI or by invoking the Pipelines REST API to start a pipeline update.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**持续**参数是一个布尔标志，用于控制管道更新执行的频率。当设置为**true**时，Terraform将部署一个DLT管道，持续更新管道中的数据集。同样，当设置为**false**时，DLT管道将以触发执行模式进行部署。在这种执行模式下，数据工程师需要通过点击DLT
    UI上的**开始**按钮或通过调用Pipelines REST API来触发管道更新的开始。'
- en: edition
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: edition
- en: 'The **edition** argument selects which product edition you would like to use
    when deploying a DLT pipeline. There are only three options to choose from: **CORE**
    , **PRO** , and **ADVANCED** . If you recall from [*Chapter 2*](B22011_02.xhtml#_idTextAnchor052)
    , the product edition selects the feature set that you would like to enable when
    running a DLT pipeline. As a result, the pipeline pricing is reflected in the
    number of features enabled with an edition. For example, the **PRO** product edition
    will enable data engineers to use expectations to enforce data quality but will
    also incur the highest operational pricing. On the other hand, the **CORE** product
    edition may be used to append incoming data to streaming tables and will incur
    the least amount of operation charges to update.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**版本**参数选择您在部署DLT管道时希望使用的产品版本。可以选择的选项只有三个：**CORE**，**PRO**，和**ADVANCED**。如果您还记得[*第2章*](B22011_02.xhtml#_idTextAnchor052)，产品版本选择了您在运行DLT管道时希望启用的功能集。因此，管道定价反映了通过版本启用的功能数量。例如，**PRO**产品版本将允许数据工程师使用期望来强制执行数据质量，但也会产生最高的操作费用。另一方面，**CORE**产品版本可用于将传入数据追加到流表中，并将产生最少的操作费用来进行更新。'
- en: photon
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: photon
- en: The **photon** argument is a Boolean flag that controls whether to use a Photon
    processing engine to update a DLT pipeline. When set to **true** , Terraform will
    deploy a DLT pipeline with an update cluster having the Photon engine enabled.
    During the dataset updates, your DLT pipeline can take advantage of this fast,
    vectorized processing engine that makes joins, aggregations, windows, and sorting
    much faster than the default cluster. When set to **false** , DLT will create
    an update cluster using the traditional Catalyst engine in Spark. Due to faster
    processing and improved performance, enabling Photon execution will incur higher
    DBU pricing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**Photon**参数是一个布尔标志，控制是否使用Photon处理引擎来更新DLT管道。当设置为**true**时，Terraform将部署一个启用Photon引擎的更新集群。在数据集更新过程中，您的DLT管道可以利用这个快速的向量化处理引擎，使连接、聚合、窗口和排序的速度比默认集群快。设置为**false**时，DLT将创建一个使用传统Catalyst引擎的更新集群。由于更快的处理速度和更好的性能，启用Photon执行将导致更高的DBU定价。'
- en: configuration
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: The **configuration** argument allows data engineers to deploy a DLT pipeline
    with optional runtime configuration. The configuration argument is an optional
    list of key-value pairs. This argument can be used to populate environment variables,
    cloud storage locations, or cluster shutdown settings, for example.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**配置**参数允许数据工程师部署一个具有可选运行时配置的DLT管道。配置参数是一个可选的键值对列表。例如，可以使用该参数来填充环境变量、云存储位置或集群关闭设置等。'
- en: library
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库
- en: 'The **library** argument can be used to install cluster libraries that a DLT
    pipeline update might depend on in order to apply updates to a pipeline. The **library**
    argument also adds support for referencing local notebook or arbitrary file dependencies,
    if data engineers wish to include dependent code artifacts using local files as
    opposed to build artifacts. For example, the following **library** block could
    be used to include a custom date utility defined as a Python file in the user’s
    home directory in their workspace:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**库**参数可用于安装DLT管道更新可能依赖的集群库，以便将更新应用于管道。**库**参数还支持引用本地笔记本或任意文件依赖项，如果数据工程师希望使用本地文件而不是构建工件来包含依赖的代码文件。例如，以下**库**块可以用于包含一个在用户工作区的主目录中定义的自定义日期实用程序，作为Python文件：'
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: cluster
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群
- en: 'The **cluster** argument controls what cluster is used by the pipeline during
    an update, maintenance activities, or the default cluster type to use in both
    update and maintenance tasks. If no **cluster** block is specified, DLT will create
    a default cluster to use to apply updates to a pipeline’s datasets. Furthermore,
    the **cluster** argument will also contain a **mode** parameter where you can
    specify what type of autoscaling to use. If you recall, in [*Chapter 2*](B22011_02.xhtml#_idTextAnchor052)
    , we described two autoscaling modes in DLT: **LEGACY** and **ENHANCED** . For
    example, the following configuration will create an update cluster that will autoscale
    from a minimum of three worker nodes to a maximum of eight nodes using the **ENHANCED**
    autoscaling algorithm:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**集群**参数控制在更新、维护活动期间或在更新和维护任务中使用的默认集群类型。如果没有指定**集群**块，DLT将创建一个默认集群，用于应用更新到管道的数据集。此外，**集群**参数还将包含一个**模式**参数，您可以在其中指定使用哪种类型的自动扩展。如果您还记得，在[*第2章*](B22011_02.xhtml#_idTextAnchor052)中，我们描述了DLT中的两种自动扩展模式：**传统模式**和**增强模式**。例如，以下配置将创建一个更新集群，使用**增强模式**自动扩展，从最小的三台工作节点扩展到最多八个节点：'
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: catalog
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: 'The **catalog** argument determines the catalog to store the output datasets
    of a DLT pipeline in Unity Catalog. As a DLT pipeline executes the definitions
    for datasets outlined in a DLT pipeline, these datasets need to have some destination
    location specified. You can specify a combination of a catalog and a schema (covered
    in the next section, *target* ) or you can specify a cloud storage location –
    but not both. This argument is mutually exclusive to the following argument, the
    **storage** argument. Alternatively, data engineers can continue to store a DLT
    pipeline’s datasets in the legacy Hive Metastore specifying the following configuration:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**目录**参数确定用于在Unity Catalog中存储DLT管道输出数据集的目录。随着DLT管道执行数据集的定义，这些数据集需要指定一些目标位置。您可以指定目录和模式的组合（在下节*目标*中介绍），或者可以指定一个云存储位置——但不能同时指定两者。此参数与下一个参数**存储**参数是互斥的。或者，数据工程师可以继续将DLT管道的数据集存储在传统的Hive元数据存储中，指定以下配置：'
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Important note
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If either the **catalog** or **storage** arguments are changed in a Terraform
    configuration file and the changes are applied, Terraform will recreate the entire
    DLT pipeline with the new changes. These values cannot be updated in the original
    DLT pipeline once deployed.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在 Terraform 配置文件中更改了 **catalog** 或 **storage** 参数，并且应用了这些更改，Terraform 将会重新创建整个
    DLT 流水线并应用新的更改。这些值在原始 DLT 流水线部署后无法更新。
- en: target
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: target
- en: The **target** argument specifies the schema in which to store the output datasets
    defined in a DLT pipeline. This argument, combined with the previous **catalog**
    argument, specifies a fully qualified schema in Unity Catalog or the legacy Hive
    Metastore. Data engineers may choose to use the values set in the **catalog**
    and **target** arguments as a convenient means for querying the intermediate datasets
    of a DLT pipeline. This may be for common tasks such as data validation, debugging,
    or general data wrangling.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**target** 参数指定了在 DLT 流水线中存储输出数据集的架构。这个参数与前面的 **catalog** 参数一起，指定了在 Unity Catalog
    或传统 Hive Metastore 中的完全限定架构。数据工程师可以选择使用 **catalog** 和 **target** 参数中设置的值，作为查询
    DLT 流水线中间数据集的便捷方式。这可以用于常见的任务，例如数据验证、调试或一般的数据整理。'
- en: storage
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: storage
- en: 'The **storage** argument can be used to specify a cloud storage location to
    store the output datasets and other related metadata for a DLT pipeline. It’s
    important to keep in mind that this argument is mutually exclusive to the preceding
    argument, the **catalog** argument. The **storage** argument may contain a fully
    qualified storage location path, a volumes location, or a location in the **Databricks
    File System** ( **DBFS** ). For example, the following configuration block would
    create a DLT pipeline whose output datasets would be stored in the DBFS:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**storage** 参数可用于指定一个云存储位置，用于存储 DLT 流水线的输出数据集和其他相关元数据。请注意，这个参数与前面的 **catalog**
    参数互斥。**storage** 参数可以包含完全限定的存储位置路径、卷位置，或 **Databricks 文件系统**（**DBFS**）中的位置。例如，以下配置块将创建一个
    DLT 流水线，其输出数据集将存储在 DBFS 中：'
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Important note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The **storage** and **catalog** arguments are mutually exclusive to one another.
    You may only specify one when authoring a Terraform configuration file.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**storage** 和 **catalog** 参数是互斥的。在编写 Terraform 配置文件时，您只能指定其中一个。'
- en: By now, you should feel confident in using the **databricks_pipeline** resource
    to declare DLT pipelines using the Databricks provider for Terraform. You should
    also have a greater understanding of the different types of configuration options
    available to customize a target DLT pipeline. In the next section, we’ll look
    at how we can automate the deployment of DLT pipelines using existing version
    control systems so that the latest changes are synchronized across target workspaces
    as soon as they are made available.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该已经能够自信地使用 **databricks_pipeline** 资源，通过 Terraform 的 Databricks 提供程序声明
    DLT 流水线。您还应该对可用的不同类型的配置选项有更深入的理解，以便自定义目标 DLT 流水线。在接下来的部分中，我们将探讨如何使用现有的版本控制系统自动化
    DLT 流水线的部署，以便最新的更改能够在发布后尽快在目标工作区中同步。
- en: Automating DLT pipeline deployment
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化 DLT 流水线部署
- en: 'Terraform can be combined with automated **Continuous Integration/Continuous
    Deployment** ( **CI/CD** ) tools, such as GitHub Actions or Azure DevOps Pipelines
    to automatically deploy code changes to your Databricks workspaces. Since Terraform
    is cross-platform, the target Databricks workspace can be in one of the major
    cloud providers: GCP, AWS, or Azure. This allows your development team to maintain
    infrastructure in a single set of code artifacts while also being agile enough
    to apply the same resources to alternate cloud providers.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 可以与自动化的 **持续集成/持续部署**（**CI/CD**）工具结合使用，例如 GitHub Actions 或 Azure DevOps
    Pipelines，自动将代码更改部署到您的 Databricks 工作区。由于 Terraform 是跨平台的，目标 Databricks 工作区可以位于主要的云服务提供商之一：GCP、AWS
    或 Azure。这使得您的开发团队能够在一组代码工件中维护基础设施，同时又能灵活地将相同的资源应用于其他云提供商。
- en: Let’s walk through a typical CI/CD process that uses the Databricks provider
    for Terraform to deploy Databricks resources to target workspaces. The CI/CD process
    will contain two automated build pipelines – one that will be used to validate
    changes made in feature branches, and a second that will be used to deploy changes
    that have been approved and merged into the main code branch to Databricks workspaces.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一个典型的 CI/CD 流程，该流程使用 Databricks 提供的 Terraform 提供程序将 Databricks 资源部署到目标工作区。CI/CD
    流程将包含两个自动化构建流水线——一个用于验证在功能分支中所做的更改，另一个用于将已批准并合并到主代码分支的更改部署到 Databricks 工作区。
- en: First, a team member creates a new feature branch to track changes to their
    organization’s IaC code base. Once finished, the engineer will open a pull request,
    requesting one or more peers from their team to review the changes, leave feedback,
    and approve or reject the changes. Upon opening a pull request, the build pipeline
    will be triggered to run, which will check out the feature branch, initialize
    the current working directory using the Terraform **init** command, validate the
    Terraform plan using the Terraform **validate** command, and generate an output
    in the form of a Terraform plan. Optionally, this Terraform plan can be automatically
    included in the pull request as a comment for peers to review.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，一名团队成员创建一个新的功能分支，用于跟踪他们组织的基础设施即代码（IaC）代码库的更改。完成后，工程师将打开一个拉取请求，要求一个或多个团队成员审查更改，留下反馈，并批准或拒绝更改。打开拉取请求后，构建流水线将被触发运行，流水线将检出功能分支，使用
    Terraform **init** 命令初始化当前工作目录，使用 Terraform **validate** 命令验证 Terraform 计划，并生成一个
    Terraform 计划的输出。可选地，这个 Terraform 计划可以作为评论自动包含在拉取请求中，供团队成员审查。
- en: When the pull request has been approved by their team members, the feature branch
    can be merged into the main code repository branch – or the **main** branch, for
    short.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当拉取请求获得团队成员的批准后，功能分支可以合并到主代码库分支——简称为**主**分支。
- en: Once the feature branch has been merged into the **main** branch, the build
    release pipeline is triggered to run. The build release pipeline will check out
    the latest copy of the **main** branch and apply the changes using the Terraform
    **apply** command. Upon applying the Terraform plan, new changes to the organization’s
    infrastructure will be reflected in the target Databricks workspace.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦功能分支合并到**主**分支，构建发布流水线就会被触发运行。构建发布流水线将检出最新的**主**分支副本，并使用 Terraform **apply**
    命令应用更改。在应用 Terraform 计划后，组织基础设施的更新将反映在目标 Databricks 工作区中。
- en: '![Figure 8.3 – Automatic deployment of Databricks resources using build tools](img/B22011_08_3.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 使用构建工具自动部署 Databricks 资源](img/B22011_08_3.jpg)'
- en: Figure 8.3 – Automatic deployment of Databricks resources using build tools
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 使用构建工具自动部署 Databricks 资源
- en: By now, you should have a complete understanding of how to design an automatic
    Databricks deployment using tools such as Azure DevOps to synchronize infrastructure
    changes through Terraform. Let’s combine everything that we’ve learned in the
    preceding sections to deploy our very own DLT pipeline to a target Databricks
    workspace using a typical development environment.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该完全理解如何使用像 Azure DevOps 这样的工具设计自动化的 Databricks 部署，借助 Terraform 同步基础设施的更改。让我们将前面各节中学到的内容结合起来，使用典型的开发环境将我们自己的
    DLT 流水线部署到目标 Databricks 工作区。
- en: Hands-on exercise – deploying a DLT pipeline using VS Code
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践练习 – 使用 VS Code 部署 DLT 流水线
- en: In this hands-on exercise, we’ll be using the popular code editor, **Visual
    Studio Code** ( **VS Code** ), to author new Terraform configuration files for
    deploying a DLT pipeline to a target Databricks workspace. VS Code has gained
    immense popularity over the years due to its ease of use, light memory footprint,
    friendly code navigation, syntax highlighting, and code refactoring, as well as
    a great community of extensions. Plus, VS Code is built around an open source
    community, meaning it’s free to download and use. Furthermore, VS Code is a cross-platform
    code editor, supporting Windows, macOS, and Linux operating systems. In this hands-on
    exercise, we’ll be using one of the community extensions, the Terraform plugin
    for VS Code, which is authored by HashiCorp to help assist in the development
    of Terraform configuration files. For example, the Terraform plugin for VS Code
    features Terraform syntax highlighting, auto-completion, code formatting, access
    to Terraform commands from the VS Code command palette, and overall, provides
    an easy experience navigating Terraform configuration files for deploying Databricks
    workspace objects.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个动手实践中，我们将使用流行的代码编辑器 **Visual Studio Code** (**VS Code**) 来编写新的 Terraform
    配置文件，部署 DLT 流水线到目标 Databricks 工作区。多年来，VS Code 因其易用性、轻便的内存占用、友好的代码导航、语法高亮和代码重构功能，以及丰富的扩展社区而获得了极大的欢迎。此外，VS
    Code 是围绕开源社区构建的，意味着它可以免费下载安装并使用。更重要的是，VS Code 是一个跨平台的代码编辑器，支持 Windows、macOS 和
    Linux 操作系统。在这个动手实践中，我们将使用一个社区扩展——HashiCorp 编写的 Terraform 插件，用于帮助开发 Terraform 配置文件。例如，VS
    Code 的 Terraform 插件提供了 Terraform 语法高亮、自动补全、代码格式化、通过 VS Code 命令面板访问 Terraform 命令，并且总的来说，提供了一个轻松的体验，帮助我们导航
    Terraform 配置文件，以便部署 Databricks 工作区对象。
- en: Setting up VS Code
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 VS Code
- en: VS C [ode can be downloaded from its websit](https://code.visualstudio.com/download)
    e located at [https://code.visualstudio.com/download](https://code.visualstudio.com/download)
    . If you haven’t installed VS Code yet, select the installer download for the
    operating system that matches your local machine. The installer may take a few
    minutes to download, depending on your network connection speed. Once the installer
    has been downloaded, unzip the ZIP file to reveal the downloaded contents. Next,
    double-click the application file, **Visual Studio Code** , to launch the code
    editor. Alternatively, you can move the application file to the **Applications**
    directory of your local operating system.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: VS C[ode 可以从其官方网站](https://code.visualstudio.com/download)下载，网址为 [https://code.visualstudio.com/download](https://code.visualstudio.com/download)。如果你还没有安装
    VS Code，选择适合本地操作系统的安装程序下载。下载过程可能需要几分钟，具体取决于你的网络连接速度。下载完成后，解压 ZIP 文件以查看下载的内容。接下来，双击应用程序文件，**Visual
    Studio Code**，启动代码编辑器。或者，你也可以将应用程序文件移动到本地操作系统的 **Applications** 目录中。
- en: Next, let’s install the Terraform extension by HashiCorp. In a web browser window,
    navigate to the Terraform extension in the Visual Studio Marketplace website located
    at [https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform](https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform)
    . Or you can search for the extension in the Marketplace search box in VS Code.
    Click the **Install** button to download and install the Terraform extension for
    VS Code.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们安装 HashiCorp 提供的 Terraform 扩展。在一个网页浏览器窗口中，访问 Visual Studio Marketplace
    上的 Terraform 扩展，网址为 [https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform](https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform)。或者，你可以在
    VS Code 中的 Marketplace 搜索框中搜索该扩展。点击 **安装** 按钮，下载并安装适用于 VS Code 的 Terraform 扩展。
- en: '![Figure 8.4 – The Terraform extension for VS Code by HashiCorp can be installed
    from the Visual Studio Marketplace](img/B22011_08_4.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – HashiCorp 提供的 Terraform 扩展可以从 Visual Studio Marketplace 安装](img/B22011_08_4.jpg)'
- en: Figure 8.4 – The Terraform extension for VS Code by HashiCorp can be installed
    from the Visual Studio Marketplace
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – HashiCorp 提供的 Terraform 扩展可以从 Visual Studio Marketplace 安装。
- en: You may be prompted to allow your web browser to open the VS Code application
    on your local machine. If so, click the **Allow** button to open VS Code and install
    the extension. The extension will be downloaded and installed in just a few minutes.
    Once the installation has been completed, you should now see menu items for HashiCorp
    Terraform on the left-hand side navigation bar of VS Code.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会被提示允许浏览器打开本地的 VS Code 应用程序。如果是这样，点击 **允许** 按钮以打开 VS Code 并安装扩展。扩展将在几分钟内下载并安装完成。安装完成后，你应该可以在
    VS Code 左侧导航栏中看到 HashiCorp Terraform 的菜单项。
- en: '![Figure 8.5 – The HashiCorp Terraform extension will create new menu items
    in the left-hand side navigation bar](img/B22011_08_5.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – HashiCorp Terraform 扩展将在左侧导航栏中创建新的菜单项](img/B22011_08_5.jpg)'
- en: Figure 8.5 – The HashiCorp Terraform extension will create new menu items in
    the left-hand side navigation bar
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – HashiCorp Terraform 扩展将在左侧导航栏中创建新的菜单项
- en: Now that the Terraform extension has been successfully installed, the extension
    will automatically be activated when the code editor detects a Terraform file.
    You can verify that the extension is activated by a Terraform logo, which will
    appear in the bottom right-hand corner of the opened Terraform file.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Terraform 扩展已经成功安装，当代码编辑器检测到 Terraform 文件时，扩展会自动激活。你可以通过在打开的 Terraform 文件右下角看到
    Terraform 的标志来确认扩展是否已激活。
- en: Creating a new Terraform project
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个新的 Terraform 项目
- en: 'Let’s create a new directory for our hands-on exercise:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的动手练习创建一个新的目录：
- en: '[PRE15]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create an empty Terraform configuration file, titled **main.tf** , either from
    a shell prompt or using VS Code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个空的 Terraform 配置文件，命名为 **main.tf**，可以从命令行提示符或者使用 VS Code 创建：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Optionally, you can clone the sample project from this chapter’s GitHub repo,
    located at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08)
    . Next, open the directory in VS Code by selecting **File** | **Open Folder**
    and navigating to the directory’s location.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: （可选）你可以从本章的 GitHub 仓库克隆示例项目，仓库地址为 [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08)。接下来，通过选择
    **文件** | **打开文件夹**，并导航到目录的位置，在 VS Code 中打开该目录。
- en: Defining the Terraform resources
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义 Terraform 资源
- en: Let’s start by expanding the Terraform example introduced in the *Setting up
    a local Terraform environment* section at the beginning of this chapter. Either
    copy the existing **main.tf** file or feel free to directly edit the body of the
    existing **main.tf** configuration file.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从扩展本章开始时 *设置本地 Terraform 环境* 部分的 Terraform 示例入手。你可以复制现有的 **main.tf** 文件，或者直接编辑现有
    **main.tf** 配置文件的内容。
- en: 'First, let’s begin by adding a second dataset to the DLT pipeline definition
    in the **databricks_notebook** resource definition (the code from the *Defining
    a DLT pipeline source notebook* section has been truncated for brevity in the
    following code block). We will now have a data pipeline containing two datasets
    – a bronze layer followed by a silver layer. Update the **databricks_notebook**
    resource definition in the **main.tf** file with the following definition:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过在 **databricks_notebook** 资源定义中添加第二个数据集来开始定义 DLT 流水线（出于简洁性考虑，*定义 DLT
    流水线源笔记本*部分的代码在下面的代码块中已被省略）。现在我们将有一个包含两个数据集的数据流水线——一个铜层，接着是银层。请在 **main.tf** 文件中更新
    **databricks_notebook** 资源定义，内容如下：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, before we can create a new DLT pipeline, we’ll want to define a location
    in Unity Catalog in which to store the pipeline datasets. Add the following catalog
    and schema resource definitions to the bottom of the **main.tf** file:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在我们创建新的 DLT 流水线之前，我们需要在 Unity Catalog 中定义一个位置来存储流水线数据集。请将以下目录和架构资源定义添加到
    **main.tf** 文件的底部：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that we have an updated source notebook containing the definition of our
    DLT pipeline, as well as a location to store the pipeline datasets, we can define
    a DLT pipeline. Add the following pipeline definition to the **main.tf** file:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了包含 DLT 流水线定义的更新源笔记本，以及一个存储流水线数据集的位置，我们可以定义一个 DLT 流水线。请将以下流水线定义添加到 **main.tf**
    文件：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You’ll notice that we’ve defined the location for the notebook containing the
    DLT pipeline definition, a default cluster to use for pipeline updates and maintenance
    tasks, as well as other runtime settings such as the Development mode, product
    edition, channel, and more.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，我们已经定义了包含DLT管道定义的笔记本的位置、用于管道更新和维护任务的默认集群，以及其他运行时设置，如开发模式、产品版本、频道等。
- en: 'Next, we’ll want to orchestrate the updates to our DLT pipeline so that we
    can trigger runs on a repeated schedule, configure alert notifications, or set
    timeout thresholds. Add the following workflow definition to the bottom of the
    **main.tf** file:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要协调对DLT管道的更新，以便我们可以在重复的时间表上触发运行，配置警报通知或设置超时阈值。将以下工作流定义添加到**main.tf**文件的底部：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Lastly, we’ll want to output the workflow URL of the deployed resource so that
    we can open the workflow UI easily from a browser. Add the following output definition
    to the **main.tf** file:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要输出已部署资源的工作流URL，以便我们可以轻松地从浏览器打开工作流UI。将以下输出定义添加到**main.tf**文件中：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Deploying the Terraform project
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署Terraform项目
- en: 'Before we can begin deploying new resources, the first step is to initialize
    the Terraform project. Execute the Terraform **init** command in the parent directory
    either from the VS Code command palette or from a shell prompt:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始部署新资源之前，第一步是初始化Terraform项目。在父目录中执行Terraform的**init**命令，可以通过VS Code命令面板或Shell提示符来完成：
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, preview the changes in the Terraform file by executing the Terraform
    **plan** command to view the proposed infrastructure changes:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过执行Terraform的**plan**命令来预览Terraform文件中的更改，以查看拟议的基础设施更改：
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In total, there should be five new resources created, including the **databricks_notebook**
    resource, which represents the notebook containing the DLT pipeline definition,
    the target Unity Catalog’s catalog, the target Unity Catalog schema, the **databricks_pipeline**
    resource, which represents our DLT pipeline, and the **databricks_job** resource,
    which represents the workflow that will trigger pipeline updates.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有五个新的资源会被创建，包括**databricks_notebook**资源，它代表了包含DLT管道定义的笔记本、目标Unity Catalog的目录、目标Unity
    Catalog架构、**databricks_pipeline**资源，它代表我们的DLT管道，以及**databricks_job**资源，它代表将触发管道更新的工作流。
- en: 'After we’ve validated the plan, we can now deploy our DLT pipeline to a Databricks
    workspace. Next, execute the Terraform **apply** command to deploy the new infrastructure
    changes to our workspace:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证计划之后，我们现在可以将我们的DLT管道部署到Databricks工作区。接下来，执行Terraform的**apply**命令，将新的基础设施更改部署到我们的工作区：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Once all resource changes have been applied, you should expect Terraform to
    output the URL to the Databricks workflow.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有资源更改应用完毕，您应该期待Terraform输出Databricks工作流的URL。
- en: Copy and paste the workflow URL into a browser window and ensure that the address
    resolves to the newly created workflow in the target workspace. You’ll notice
    that the new workflow contains a single task for updating the DLT pipeline. The
    workflow is paused, as outlined in the Terraform configuration. Optionally, you
    can click the blue **Run now** button to trigger a new, immediate run of the workflow.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 将工作流URL复制并粘贴到浏览器窗口中，并确保该地址解析到目标工作区中新创建的工作流。您会注意到，新工作流包含一个用于更新DLT管道的任务。该工作流已暂停，如Terraform配置所示。您也可以点击蓝色的**立即运行**按钮来触发工作流的一个新的立即运行。
- en: "![Figure 8.6 – Terraform will output the \uFEFFworkflow URL for updating the\
    \ DLT pipeline](img/B22011_08_6.jpg)"
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – Terraform将输出用于更新DLT管道的工作流URL](img/B22011_08_6.jpg)'
- en: Figure 8.6 – Terraform will output the workflow URL for updating the DLT pipeline
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – Terraform将输出用于更新DLT管道的工作流URL
- en: 'As simple as it was to deploy our changes to the target Databricks workspace,
    it’s just as easy to undeploy the changes. Execute the following command to remove
    all the resource changes from the target Databricks workspace:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管将更改部署到目标Databricks工作区很简单，但撤销这些更改同样容易。执行以下命令以从目标Databricks工作区中删除所有资源更改：
- en: '[PRE25]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Confirm the decision by entering the word **yes** . It may take a few minutes
    to fully undeploy all of the resources from your Databricks workspace.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过输入**yes**确认该决定。完全撤销所有资源从您的Databricks工作区中可能需要几分钟。
- en: As you saw, with just a few keystrokes and a few clicks of the button, it was
    fast and easy to provision and deprovision resources in a Databricks workspace
    using the Databricks Terraform provider. Rather than instructing Terraform how
    to deploy the resources to our target Databricks workspace, we focused on what
    changes to make through configuration and let the Terraform tool handle the heavy
    lifting for us.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，通过几个按键和几次点击，使用 Databricks Terraform 提供者在 Databricks 工作区中配置和去配置资源变得既快速又简便。我们没有指示
    Terraform 如何将资源部署到目标 Databricks 工作区，而是专注于通过配置做出哪些更改，让 Terraform 工具为我们处理繁重的工作。
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered how to use the Databricks provider for Terraform
    to implement a CI/CD process for deploying data pipelines across workspaces. We
    saw how easy it was to set up a local development environment for working with
    Terraform configuration files and how easy it was to test our Terraform plans
    before applying them to a target environment. We also installed the Databricks
    Terraform provider from the Terraform Registry and imported the provider into
    Terraform configuration files. Next, we dove into the details of the **databricks_pipeline**
    resource, which is used by the Databricks Terraform provider to deploy a DLT pipeline
    to a target workspace. We inspected each argument in the resource specification
    and saw how we coul d control the DLT pipeline runtime configuration, the compute
    settings, and even the location of the output datasets from our pipeline. Lastly,
    we saw how easy it was to automate our Terraform configuration files by storing
    them in a version control system such as GitHub and automating the deployment
    using a build tool such as Azure DevOps Pipelines. We concluded the chapter with
    a hands-on example of using the Terraform extension with the popular code editor
    VS Code to deploy a DLT pipeline from your local development environment.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何使用 Databricks Terraform 提供者实现 CI/CD 过程，以便在工作区之间部署数据管道。我们看到，如何轻松设置本地开发环境来处理
    Terraform 配置文件，以及在将配置应用于目标环境之前，如何轻松测试我们的 Terraform 计划。我们还从 Terraform 注册中心安装了 Databricks
    Terraform 提供者，并将其导入到 Terraform 配置文件中。接下来，我们深入了解了 **databricks_pipeline** 资源，它由
    Databricks Terraform 提供者用于将 DLT 管道部署到目标工作区。我们检查了资源规范中的每个参数，并了解了如何控制 DLT 管道的运行时配置、计算设置，甚至管道输出数据集的位置。最后，我们看到，通过将
    Terraform 配置文件存储在 GitHub 等版本控制系统中并利用 Azure DevOps Pipelines 等构建工具自动化部署，自动化配置文件变得如此简单。我们通过一个实际操作示例来结束本章，展示了如何使用
    Terraform 扩展和流行的代码编辑器 VS Code 从本地开发环境部署 DLT 管道。
- en: However, Terraform isn’t for everyone and it may be the case that it’s too complex
    or too difficult to use for your use case. In the next chapter, we’ll dive into
    **Databricks Asset Bundles** ( **DABs** ), which is another CI/CD tool that makes
    it simple to package and deploy Databricks code artifacts for data and machine
    learning workloads.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Terraform 并不适合所有人，可能对你的用例来说，它过于复杂或难以使用。在下一章中，我们将深入探讨**Databricks 资源包**（**DABs**），这是另一个
    CI/CD 工具，可以简化将 Databricks 代码工件打包并部署到数据和机器学习工作负载的过程。
