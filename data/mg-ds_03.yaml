- en: Testing Your Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试你的模型
- en: Coming up with a perfect machine learning model is not simple if you do not
    use a good testing methodology. This seemingly perfect model will fail the moment
    you deploy it. Testing the model's performance is not an easy task, but it is
    an essential part of every data science project. Without proper testing, you can't
    be sure whether your models will work as expected, and you can't choose the best
    approach to solve the task at hand.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不使用好的测试方法，提出一个完美的机器学习模型并不简单。这个看似完美的模型会在你部署它的那一刻失败。测试模型的性能并非易事，但它是每个数据科学项目的必不可少的一部分。如果没有适当的测试，你无法确定模型是否能按预期工作，也无法选择最合适的方法来解决当前任务。
- en: This chapter will explore various approaches for model testing and look at different
    types of metrics, using mathematical functions that evaluate the quality of predictions.
    We will also go through a set of methods for testing classifier models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨各种模型测试方法，并查看不同类型的度量标准，使用数学函数来评估预测的质量。我们还将介绍一套用于测试分类器模型的方法。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Offline model testing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离线模型测试
- en: Online model testing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线模型测试
- en: Offline model testing
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离线模型测试
- en: Offline model testing encompasses all the model-evaluation processes that are
    performed before the model is deployed. Before discussing online testing in detail,
    we must first define model errors and ways to calculate them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 离线模型测试包括所有在模型部署之前进行的模型评估过程。在详细讨论在线测试之前，我们必须首先定义模型误差及其计算方法。
- en: Understanding model errors
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解模型误差
- en: Every model can make mistakes because collected data, and the model itself,
    introduces implications about the nature of your problem. The best example of
    a good working model is inside your brain. You use modeling in real time—the brain
    renders everything you see by interpreting electromagnetic impulses recorded by
    your eyes. While this picture of the world is imperfect, it is useful as we receive
    over 90% of information through the visual channel. The last 10% comes from hearing,
    touch, and our other senses. Thus, each model, **M**, tries to predict real value,
    **Y**, by making a guess, [![](img/c76546bd-25e3-4ecf-912a-d99afa7820eb.png)].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都可能出错，因为收集的数据和模型本身都对问题的本质产生了影响。一个优秀模型的最佳例子就是你的大脑。你在实时使用建模——大脑通过解读眼睛记录的电磁脉冲来渲染你所看到的一切。虽然这种世界的图像并不完美，但它很有用，因为我们通过视觉通道接收到超过90%的信息。剩下的10%来自听觉、触觉和其他感觉。因此，每个模型**M**都试图通过猜测来预测真实值**Y**，[![](img/c76546bd-25e3-4ecf-912a-d99afa7820eb.png)]。
- en: 'The difference between the real value and model''s approximation makes up the
    model error:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 真实值与模型近似值之间的差异构成了模型误差：
- en: '![](img/91dce14c-f063-4766-9b5c-6f57446e1e70.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91dce14c-f063-4766-9b5c-6f57446e1e70.png)'
- en: For regression problems, we can measure the error in quantities that the model
    predicts. For example, if we predict house prices using a machine learning model
    and get a prediction of $300,000 for a house with a real price of $350,000, we
    can say that the error is $350,000 - $300,000 = $50,000.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，我们可以衡量模型预测的量值误差。例如，如果我们使用机器学习模型预测房价，并且预测结果为$300,000，而实际价格为$350,000，我们可以说误差为$350,000
    - $300,000 = $50,000。
- en: For classification problems in the simplest setting, we can measure the error
    as 0 for a guess, and 1 for a wrong answer. For example, for a cat/dog recognizer,
    we give an error of 1 if the model predicts that there is a cat in a dog photo,
    and 0 if it gives a correct answer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，在最简单的设定下，我们可以将猜测的错误值设为0，错误答案设为1。例如，对于一个猫/狗识别器，如果模型预测一张狗的照片里有猫，则错误值为1；如果给出了正确答案，则错误值为0。
- en: Decomposing errors
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分解误差
- en: 'You won''t find a machine learning model that perfectly solves your problems
    without making even a single mistake, no matter how small. Since every model makes
    mistakes, it is critical to understand their nature. Suppose that our model makes
    a prediction and we know the real value. If this prediction is incorrect, then there
    is some difference between the prediction and the true value:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你不会找到一个能够完美解决问题且没有任何错误的机器学习模型，无论这个错误多么微小。由于每个模型都会出错，理解这些错误的本质至关重要。假设我们的模型进行了预测，并且我们知道真实值。如果这个预测不正确，那么预测值与真实值之间就存在差异：
- en: '![](img/d24ad2f9-c167-4252-a586-8eab3b33ffc4.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d24ad2f9-c167-4252-a586-8eab3b33ffc4.png)'
- en: The other part of this error will come from imperfections in our data, and some
    from imperfections in our model. No matter how complex our model is, it can only
    reduce the modeling error. Irreducible error is out of our control, hence its
    name.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误差的另一部分来源于我们数据的缺陷，还有一些来自模型的缺陷。无论我们的模型多复杂，它只能减少建模误差。不可约误差超出了我们的控制范围，因此得名。
- en: 'Let''s look at it in the following formula:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在以下公式中来看它：
- en: '![](img/a9aa3c92-9622-4e74-a604-6389a73e0725.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9aa3c92-9622-4e74-a604-6389a73e0725.png)'
- en: 'Not all reducible errors are the same. We can decompose reducible errors further.
    For example, look at the following diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有可约错误都是相同的。我们可以进一步分解可约错误。例如，看看下面的图：
- en: '![](img/737bbad2-ca32-44e0-918e-4821c09f8c73.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/737bbad2-ca32-44e0-918e-4821c09f8c73.png)'
- en: 'The red center of each target represents our goal (real value), and the blue
    shots represent the model predictions. In the target, the model''s aim is off—all
    predictions are close together, but they are far away from the target. This kind
    of error is called **bias**. The simpler our model is, the more bias it will have.
    For a simple model, the bias component can become prevailing:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每个目标的红色中心表示我们的目标（真实值），而蓝色的点表示模型的预测值。在目标中，模型的瞄准偏差——所有预测值都很接近，但远离目标。这样的误差被称为**偏差**。我们的模型越简单，偏差越大。对于简单的模型，偏差成分可能占主导地位：
- en: '![](img/d5bf457f-5afb-4cf4-8109-4be8f3003775.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5bf457f-5afb-4cf4-8109-4be8f3003775.png)'
- en: In the preceding plot, we try to model a complex relationship between variables
    with a simple line. This kind of model has a high bias.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们尝试通过一条简单的直线来建模变量之间的复杂关系。这种模型具有较高的偏差。
- en: 'The second component of the model error is **variance**:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 模型误差的第二个成分是**方差**：
- en: '![](img/3226d01c-6805-4da1-b48b-d87d7faa061c.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3226d01c-6805-4da1-b48b-d87d7faa061c.png)'
- en: All predictions appear to be clustered around the true target, but the spread
    is too high. The source of this error comes from the model's sensitivity to fluctuations
    in data. If the model has high variance, randomness in measurements can lead to
    very different predictions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的预测值似乎都聚集在真实目标附近，但散布太广。这种误差的来源是模型对数据波动的敏感性。如果模型具有高方差，测量中的随机性会导致非常不同的预测结果。
- en: 'So far, we have decomposed model error into the three following numbers:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将模型误差分解成了以下三个部分：
- en: '![](img/d46b3109-8245-44da-95f4-62f6017375bc.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d46b3109-8245-44da-95f4-62f6017375bc.png)'
- en: It is not a coincidence that bias and variance sit close together in the same
    formula. There is a relationship between them. Predictive models show a property
    called the **bias-variance** **tradeoff**—the more biased a model, the lower the
    variance component of the error. And in reverse, the more variance it has, the
    lower its bias will be. This important fact will be a game-changer for building
    ensemble models, which we will explore in [Chapter 3](eb2995e4-1a9a-43d9-b162-557a4664069b.xhtml), *Understanding
    AI*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差出现在同一公式中并非偶然，它们之间存在关系。预测模型展示了一种叫做**偏差-方差** **权衡**的特性——模型的偏差越大，误差中的方差成分就越小；反之，方差越大，偏差就越小。这个重要的事实将对构建集成模型产生深远影响，我们将在[第3章](eb2995e4-1a9a-43d9-b162-557a4664069b.xhtml)
    *理解AI*中进行探讨。
- en: 'Typically, models that impose some kind of structure in the data have a high
    bias (they assume certain laws that the data conforms to). Biased models will
    work well, as long as the data does not contradict the underlying logic of the
    model. To give you an example of such a model, think of a simple line. For example,
    we will predict a housing price as a linear function of its size in square feet:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，施加某种数据结构的模型具有较高的偏差（它们假设数据遵循某些规律）。偏差模型表现良好，只要数据不与模型的基本逻辑相矛盾。举个例子，想象一下一个简单的直线模型。例如，我们将房价预测为其平方英尺大小的线性函数：
- en: '![](img/c9e0c82d-5039-401e-a129-8dcf13e4c6d5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9e0c82d-5039-401e-a129-8dcf13e4c6d5.png)'
- en: 'Notice that if we change the square footage by a little, say 0.1, then the
    prediction won''t change by much. Thus, this model has low variance. When the
    model is sensitive to changes in its input, its variance will outgrow the bias.
    The variance component will grow with your model increases in complexity and the
    total number of parameters grows. In the following plot, you can see how two different
    models fit the same dataset. The first simple model has low variance, and the
    second complex model has high variance:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果我们稍微改变一下面积，比如 0.1，那么预测结果不会有太大变化。因此，这个模型的方差很低。当模型对输入的变化敏感时，其方差会超过偏差。随着模型复杂度的增加和参数总数的增多，方差成分会增加。在下图中，你可以看到两个不同的模型如何拟合相同的数据集。第一个简单模型具有低方差，第二个复杂模型具有高方差：
- en: '![](img/84bb9ab9-a0d4-4e34-a62c-4ef303df7ff7.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84bb9ab9-a0d4-4e34-a62c-4ef303df7ff7.png)'
- en: In the preceding plot, slight changes in **X** can lead to large fluctuations
    of **Y**. Models with high variance are robust and imply that the data is much
    less structured.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，**X** 的微小变化可能导致 **Y** 的大幅波动。具有高方差的模型是鲁棒的，并且意味着数据结构较少。
- en: Understanding overfitting
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解过拟合
- en: The bias-variance trade-off goes hand in hand with a very important problem
    in machine learning called **overfitting**. If your model is too simple, it will
    cause large errors. If it is too complex, it will memorize the data too well.
    An overfitted model remembers data too well and acts like a database. Suppose
    that our housing dataset contains some lucky deals where previous houses had a
    low price because of circumstances not captured in the data. An overfit model
    will memorize those examples too closely and predict incorrect price values on
    unseen data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差-方差权衡与机器学习中的一个非常重要的问题密切相关，叫做**过拟合**。如果你的模型太简单，它会导致较大的误差。如果它过于复杂，它会过于记忆数据。一个过拟合的模型记住数据太好，像一个数据库一样工作。假设我们的住房数据集包含一些幸运的交易，其中之前的房屋因某些数据未捕捉的情况而价格较低。一个过拟合的模型会过于紧密地记住这些例子，并在未见数据上预测错误的价格值。
- en: Now, having understood the error decomposition, can we use it as a stepping
    stone to design a model-testing pipeline?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，理解了误差分解后，我们能否将其作为设计模型测试管道的基石？
- en: 'We need to determine how to measure model error in such a way that it will
    correspond to the real model performance on unseen data. The answer comes from
    the question itself. We will split all the available data into two sets: a training
    set and a test set, as shown in the following screenshot:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确定如何测量模型误差，以使其能够反映模型在未见数据上的实际表现。答案来自问题本身。我们将把所有可用数据分成两组：训练集和测试集，如下图所示：
- en: '![](img/a6fd01ba-178a-4d6f-aace-c1717aa27156.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6fd01ba-178a-4d6f-aace-c1717aa27156.png)'
- en: 'We will use data in the training set to train our model. The test set acts
    as unseen data and you should not use the test set in the training process. When
    the model''s training is finished, you can feed the test data into your model.
    Now you can calculate errors for all predictions. The model did not use the test
    data during training, so the test set error represents the model error on unseen
    data. The drawback to this approach is that you take a significant amount of data,
    usually up to 30%, to use for testing. This means less training data and lower
    model quality. There is also a caveat – if you use your test set too much, error
    metrics will start to lie. For example, suppose that you did the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用训练集中的数据来训练我们的模型。测试集作为未见数据，你不应该在训练过程中使用测试集。当模型训练完成后，你可以将测试数据输入到模型中。现在你可以计算所有预测的误差。由于模型在训练过程中没有使用测试数据，因此测试集误差代表了模型在未见数据上的误差。这种方法的缺点是，你需要花费大量的数据，通常是多达
    30%，来用于测试。这意味着训练数据减少，模型质量也会降低。还有一个警告——如果你过度使用测试集，误差度量可能会开始失真。例如，假设你做了以下操作：
- en: Trained a model
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个模型
- en: Measured the error on the test data
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测量误差
- en: Changed your model to improve the metrics
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改进模型以提高指标
- en: Repeated steps 1-3 ten times
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 1-3 十次
- en: Deployed the model to production
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署模型到生产环境
- en: It is likely that the quality of your model will be much lower than expected.
    Why did this happen? Let's look more closely *step 3*. You looked at a score,
    and changed your model or data processing code several consecutive times. In fact,
    you did several learning iterations by hand. By repeatedly improving the test
    score, you indirectly disclosed information about the test data to your model.
    When the metric values measured on a test set deviate from the metrics measured
    on the real data, we say that the test data has leaked into our model. Data leaks
    are notoriously hard to detect before they cause damage. To avoid them, you should
    always be mindful of the possibility of a leak, think critically, and follow best
    practices.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型质量可能远低于预期。这是为什么呢？让我们更仔细地看看*第3步*。您查看了一个评分，并多次更改了您的模型或数据处理代码。事实上，您进行了几次手动学习迭代。通过反复提高测试得分，您间接地将测试数据的某些信息泄漏到了您的模型中。当在测试集上测得的指标值与实际数据上的指标值偏离时，我们称测试数据已经泄漏到我们的模型中。数据泄漏在造成损害之前往往很难察觉。为了避免数据泄漏，您应该时刻关注泄漏的可能性，批判性地思考，并遵循最佳实践。
- en: We can use a separate piece of data to fight test set leakage. Data scientists
    use validation sets to tune model parameters and compare different models before
    choosing the best one. Then, the test data is used only as a final check that
    informs you about model quality on unseen data. After you have measured the test
    metric scores, the only decision left is to make is whether the model will proceed
    to testing in a real-world scenario.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用独立的数据来防止测试集泄漏。数据科学家使用验证集来调整模型参数，并在选择最佳模型之前比较不同的模型。然后，测试数据仅作为最终检查，告知您模型在未见过的数据上的质量。在您测量了测试指标得分之后，唯一剩下的决定就是是否将模型投入真实世界的测试场景。
- en: 'In the following screenshot, you can see an example of a train/validation/test
    split of the dataset:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，您可以看到数据集的训练/验证/测试划分示例：
- en: '![](img/b0e5b92e-1785-463b-bb97-2e720e6c2af0.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0e5b92e-1785-463b-bb97-2e720e6c2af0.png)'
- en: 'Unfortunately, the following two problems persist when we use this approach:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当我们使用这种方法时，以下两个问题仍然存在：
- en: The information about our test set might still leak into our solution after
    many iterations. Test-set leakage does not disappear completely when you use the
    validation set, it just becomes slower. To overcome this, change your test data
    from time to time. Ideally, make a new test set for every model-deployment cycle.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的测试集信息在多次迭代后可能仍然会泄漏到我们的解决方案中。即使使用了验证集，测试集泄漏也不会完全消失，它只是变得更慢。为了解决这个问题，您应该定期更换测试数据。理想情况下，每次模型部署周期时都应创建一个新的测试集。
- en: You might overfit your validation data quickly, because of the train-measure-change
    feedback cycle for tuning your models.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于训练-评估-修改的反馈循环，您可能很快会对验证数据过拟合，以调整您的模型。
- en: To prevent overfitting, you can randomly select train and validation sets from
    your data for each experiment. Randomly shuffle all available data, then select
    random train and validation datasets by splitting the data into three parts according
    to proportions you have chosen.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止过拟合，您可以为每次实验随机选择训练集和验证集。将所有可用数据随机打乱，然后按照您选择的比例将数据划分为三部分，随机选择训练集和验证集。
- en: There is no general rule for how much training, validation, and testing data
    you should use. Often, more training data means a more accurate model, but it
    means that you will have less data to assess the model's performance. The typical
    split for medium-sized datasets (up to 100,000 data points) is to use 60-80% of
    the data to train the model and use the rest for validation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练、验证和测试数据的使用量没有通用规则。通常，更多的训练数据意味着更准确的模型，但这也意味着您将拥有更少的数据来评估模型的性能。对于中等大小的数据集（最多100,000个数据点），典型的划分是将60-80%的数据用于训练模型，其余数据用于验证。
- en: The situation changes for large datasets. If you have a dataset with 10,000,000
    rows, using 30% for testing would comprise 3,000,000 rows. It is likely that this
    amount would be overkill. Increasing test and validation test sizes will yield
    diminishing returns. For some problems, you will get good results with 100,000
    examples for testing, which would amount for a 1% test size. The more data you
    have, the lower the proportion you should use for testing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型数据集，情况有所不同。如果您的数据集有10,000,000行，使用30%作为测试集将包含3,000,000行数据。这个数量可能会过多。增加测试和验证集的大小将带来边际效益递减。对于某些问题，使用100,000个测试样本就能获得良好的结果，这相当于1%的测试比例。数据越多，您用于测试的比例应该越低。
- en: 'Often, there is too little data. In those situations, taking from 30%-40% data
    for testing and validation might severely decrease the model''s accuracy. You
    can apply a technique called cross-validation in data-scarce situations. With
    cross-validation, there''s no need to create a separate validation or test set.
    Cross-validation proceeds in the following way:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 经常会遇到数据过少的情况。在这些情况下，使用 30%-40% 的数据进行测试和验证可能会显著降低模型的准确性。你可以在数据稀缺的情况下使用一种叫做交叉验证的技术。使用交叉验证时，不需要创建单独的验证集或测试集。交叉验证按以下方式进行：
- en: You choose some fixed number of iterations—three, for example.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你选择一个固定的迭代次数——例如，选择三次。
- en: Split the dataset into three parts.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分成三部分。
- en: For each iteration, cross-validation uses 2/3 of the dataset as a training data
    and 1/3 as validation data.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每次迭代，交叉验证使用 2/3 的数据集作为训练数据，1/3 作为验证数据。
- en: Train model for each of the three train-validation set pairs.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每一对训练-验证集训练模型。
- en: Calculate the metric values using each validation set.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用每个验证集计算指标值。
- en: Aggregate the metrics into a single number by averaging all metric values.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有指标值取平均，汇总成一个单一的数值。
- en: 'The following screenshot explains cross-validation visually:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图以可视化的方式解释了交叉验证：
- en: '![](img/029fc396-016c-4c6d-b0aa-0cc072c3b1ed.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/029fc396-016c-4c6d-b0aa-0cc072c3b1ed.png)'
- en: 'Cross-validation has one main drawback: it requires significantly more computational
    resources to assess model quality. In our example, to make a single assessment
    we needed to fit three models. With a regular train/test split, we would train
    only one model. In addition, cross-validation accuracy will grow with the number
    of iterations you use (also called folds). So cross-validation allows you to use
    more data for training, while requiring more computational resources. How do we
    choose between cross-validation and train-validation-test splits for projects?'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证有一个主要缺点：它需要显著更多的计算资源来评估模型质量。在我们的例子中，为了做一次评估，我们需要训练三个模型。而使用常规的训练/测试拆分，我们只需要训练一个模型。此外，交叉验证的准确性会随着迭代次数（也叫折数）的增加而提高。所以，交叉验证使你能够使用更多的数据进行训练，同时需要更多的计算资源。那么，我们如何在交叉验证和训练-验证-测试拆分之间做选择呢？
- en: In cross-validation, [![](img/50985a49-a4a0-4825-850d-cde48295079d.png)] is
    a variable parameter that is set up by a data scientist. The lowest possible value
    is 1, which is equivalent to a simple train/test split. The largest extreme is [![](img/14ec3d61-d29f-4a58-8f3c-cdf28e242428.png)] equal
    to the number of data points in the dataset. This means that if we have [![](img/94f8ab10-6399-4c76-a3ce-7f21586c2418.png)]
    points in the dataset, the model will be trained and tested [![](img/f3838149-cf5b-433b-8eea-3ee7c78648a5.png)] times.
    This special case of cross-validation is called **leave-one-out cross-validation**.
    In theory, a larger number of folds means that the cross-validation will return
    more accurate metric values. While leave-one-out cross-validation is the most
    theoretically accurate method, it is seldom used in practice because of the large
    computational requirements. In practice, the values of [![](img/50985a49-a4a0-4825-850d-cde48295079d.png)] range
    from 3 to 15 folds, depending on the dataset size. Your project may need to use
    more, so take this as advice and not as a rule.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证中， [![](img/50985a49-a4a0-4825-850d-cde48295079d.png)] 是由数据科学家设置的一个变量参数。最小值为
    1，相当于一个简单的训练/测试拆分。最大值是 [![](img/14ec3d61-d29f-4a58-8f3c-cdf28e242428.png)]，即数据集中数据点的数量。这意味着如果我们有
    [![](img/94f8ab10-6399-4c76-a3ce-7f21586c2418.png)] 个数据点，模型将被训练和测试 [![](img/f3838149-cf5b-433b-8eea-3ee7c78648a5.png)]
    次。这个特殊的交叉验证情况叫做**留一交叉验证**。理论上，更多的折数意味着交叉验证会返回更准确的指标值。虽然留一交叉验证是最理论上准确的方法，但由于巨大的计算需求，它在实践中很少使用。实际上， [![](img/50985a49-a4a0-4825-850d-cde48295079d.png)]
    的值通常在 3 到 15 折之间，具体取决于数据集的大小。你的项目可能需要使用更多的折数，所以把这个作为建议，而不是规则。
- en: 'The following table sums up a general way of thinking:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了一种常见的思考方式：
- en: '|  | **Model training requires low to moderate computational resources and
    time** | **Model training requires large computational resources and takes a long
    time** |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | **模型训练需要低到中等的计算资源和时间** | **模型训练需要大量计算资源并且耗时较长** |'
- en: '| **Small to medium dataset** | Cross-validation | Either |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **小到中型数据集** | 交叉验证 | 任意 |'
- en: '| **Large dataset** | Either | Train/validation/test split |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **大数据集** | 任意 | 训练/验证/测试拆分 |'
- en: Another important aspect related to model testing is how to split the data.
    A slight error in your splitting logic can mean all your testing efforts were
    in vain. Splitting is easy, if all observations in your dataset are independent.
    Then you can use random data splits. But what if we are solving the stock-price
    prediction problem? When our data rows are tied to time, we can't look at them
    as independent values. Today's stock prices depend on their past values. If this
    wasn't true, the prices would randomly jump from $0 to $1,000\. In this situation,
    suppose we have two years' worth of stock data, from January 2017 to December
    2018\. If we use random splits, it is possible that our model will train in September
    2018 and test on February 2017\. This makes no sense. We must always think about
    causal relationships and dependencies between your observations and be sure to
    check whether your validation procedure is correct.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与模型测试相关的重要方面是如何拆分数据。稍有错误的拆分逻辑可能会导致你所有的测试工作都白费。如果数据集中的所有观测值都是独立的，那么拆分就很简单。你可以使用随机数据拆分。但如果我们正在解决股票价格预测问题呢？当我们的数据行与时间相关时，我们不能将它们视为独立的值。今天的股价依赖于过去的股价。如果这不成立，股价就会随机从$0跳到$1,000。在这种情况下，假设我们有两年的股票数据，从2017年1月到2018年12月。如果我们使用随机拆分，可能会出现模型在2018年9月训练，而在2017年2月进行测试的情况。这是毫无意义的。我们必须始终考虑观测值之间的因果关系和依赖性，并确保验证过程是正确的。
- en: Next, we will learn about metrics, which are formulas we can use to summarize
    validation and test errors. Metrics will allow us to compare different models
    and choose the best candidates for production use.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习关于指标的知识，它是我们用来总结验证和测试误差的公式。指标将使我们能够比较不同的模型，并选择最适合生产使用的候选模型。
- en: Using technical metrics
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用技术指标
- en: Each model, no matter how complex and accurate, makes mistakes. It is natural
    to expect that some models will be better than others when solving a specific
    problem. Currently, we can measure errors by comparing individual model predictions
    with the ground truth. It would be useful to summarize them into a single number
    for measuring the model's performance. We can use a metric to do this. There are
    many kinds of metrics that are suitable for different machine learning problems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型，无论多么复杂和精确，都不可避免地会犯错。我们自然会期望一些模型在解决特定问题时比其他模型表现更好。目前，我们可以通过将各个模型的预测与真实值进行比较来衡量误差。将这些误差总结成一个数字来衡量模型的表现会很有用。我们可以使用一个指标来实现这一点。不同的机器学习问题适合使用不同种类的指标。
- en: 'In particular, for regression problems the most common metric is the **root
    mean square error**, or **RMSE**:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，对于回归问题，最常见的指标是**均方根误差**，或称**RMSE**：
- en: '![](img/04394a71-3018-4d3e-82d0-2bb8f07996be.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04394a71-3018-4d3e-82d0-2bb8f07996be.png)'
- en: 'Let''s examine the elements of this formula:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个公式的各个元素：
- en: '*N* is the total number of data points.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* 是数据点的总数。'
- en: '*predicted - actual* measures the error between ground truth and model prediction.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测值 - 实际值* 衡量了地面真相和模型预测之间的误差。'
- en: The Sigma sign at the start of the formula means sum.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公式开头的Sigma符号表示求和。
- en: 'Another popular way to measure regression errors is **mean absolute error**
    (**MAE**):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常用的回归误差衡量方法是**平均绝对误差**（**MAE**）：
- en: '![](img/6f81be96-3130-428c-9cac-d5bab44c92ff.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f81be96-3130-428c-9cac-d5bab44c92ff.png)'
- en: Note that MAE is very similar to RMSE. Compared to MAE, RMSE has a square root
    instead of absolute value and it squares errors. While MAE and RMSE may seem identical,
    there are some technical differences between them. Data scientists can choose
    best metrics for a problem, knowing their trade-offs and shortcomings. You don't
    need to learn them all, but I would like to highlight one difference to give you
    a general feel of the thought process. RMSE penalizes large errors more than MAE.
    This property comes from the fact that RMSE uses squared errors, while MAE uses
    absolute values. To illustrate, an error of 4 would be 4 in MAE, but in RMSE it
    will turn into 16 because of the square.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，MAE与RMSE非常相似。与MAE相比，RMSE的区别在于它使用平方根而不是绝对值，并且它会对误差进行平方。尽管MAE和RMSE看起来很相似，但它们之间还是存在一些技术性差异。数据科学家可以根据问题的需求选择最合适的指标，了解各个指标的权衡和局限性。你不需要学习所有的指标，但我希望强调一个差异，以便让你大致理解思考过程。RMSE对大误差的惩罚比MAE更严重。这一特性来自于RMSE使用平方误差，而MAE使用绝对值。举个例子，MAE中的误差为4，而RMSE中的误差则会变成16，因为它要进行平方计算。
- en: 'For classification problems, the metric-calculation process is more involved.
    Let''s imagine that we are building a binary classifier that estimates the probability
    of a person having pneumonia. To calculate how accurate the model is, we may just
    divide the total of correct answers by the number of rows in the dataset:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，指标计算过程更为复杂。假设我们正在构建一个二分类器，用于估计一个人是否患有肺炎。为了计算模型的准确性，我们可能只需将正确答案的总数除以数据集中的行数：
- en: '![](img/6aabef97-73f4-434d-8301-331290a05a0b.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6aabef97-73f4-434d-8301-331290a05a0b.png)'
- en: Here, [![](img/a63ff15c-921b-49d1-a89b-55c00b1d2ac7.png) ]is the amount of correct
    predictions, and [![](img/4b8b3e08-085a-41d7-8772-0d9defe1ec0a.png)] is the total
    number of predictions. Accuracy is simple to understand and calculate, but it
    has a major flaw. Let's assume the average probability of having pneumonia is
    0.001%. That is, one person out of 100,000 has the illness. If you had collected
    data on 200,000 people, it is feasible that your dataset would contain only two
    positive cases. Imagine you have asked a data scientist to build a machine learning
    model that estimates pneumonia probability based on a patient's data. You have
    said that you would only accept an accuracy of no less than 99.9%. Suppose that
    someone created a dummy algorithm that always outputs zeros.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，[![](img/a63ff15c-921b-49d1-a89b-55c00b1d2ac7.png)]是正确预测的数量，而[![](img/4b8b3e08-085a-41d7-8772-0d9defe1ec0a.png)]是总的预测数量。准确率简单易懂且容易计算，但它有一个重大缺陷。假设患有肺炎的平均概率是0.001%。也就是说，每10万人中只有一个人得病。如果你收集了20万人数据，那么数据集中可能只有两个阳性病例。假设你要求数据科学家构建一个基于患者数据来估计肺炎概率的机器学习模型，并且你要求准确率不低于99.9%。假设某人创建了一个虚拟算法，始终输出零。
- en: 'This model has no real value, but its accuracy on our data will be high as
    it will make only two errors:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型没有实际价值，但它在我们的数据上的准确性会很高，因为它只会犯两个错误：
- en: '![](img/300e6318-20d0-4d3a-814e-6cb59bd9947f.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/300e6318-20d0-4d3a-814e-6cb59bd9947f.png)'
- en: The problem is that accuracy considers only global fraction of answers. When
    one class outnumbers the others, accuracy outputs misleading values.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于准确率仅考虑答案的全局比例。当某个类别的数量远超其他类别时，准确率会输出误导性的值。
- en: 'Let''s look at model predictions in more detail by constructing a confusion
    table:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过构建混淆矩阵来更详细地查看模型预测：
- en: '|  | **Model prediction:****Has pneumonia** | **Model prediction:****Does not
    have pneumonia** |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | **模型预测:****有肺炎** | **模型预测:****没有肺炎** |'
- en: '| **Real outcome:****Has pneumonia** | 0 | 2 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **实际结果:****有肺炎** | 0 | 2 |'
- en: '| **Real outcome:****Does not have pneumonia** | 0 | 199,998 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **实际结果:****没有肺炎** | 0 | 199,998 |'
- en: After looking at this table, we can see that the dummy model won't be helpful
    to anyone. It didn't identify two people with the condition as positive. We call
    those errors **False Negatives** (**FN**). The model also correctly identified
    all patients with no pneumonia, or **True Negatives** (**TN**), but it has failed
    to diagnose ill patients correctly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看这个表格，我们可以看到，虚拟模型对任何人都没有帮助。它没有将两个有病的人判定为阳性。我们称这些错误为**假阴性**（**FN**）。该模型也正确地识别了所有没有肺炎的患者，或者说是**真阴性**（**TN**），但它未能正确诊断出生病的患者。
- en: 'Now, suppose that your team has built a real model and got the following results:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你的团队已经构建了一个真实的模型，并得到了以下结果：
- en: '|  | **Model prediction:****Has pneumonia** | **Model prediction:****Does not
    have pneumonia** |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | **模型预测:****有肺炎** | **模型预测:****没有肺炎** |'
- en: '| **Real outcome:****Has pneumonia** | 2 | 0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **实际结果:****有肺炎** | 2 | 0 |'
- en: '| **Real outcome:****Does not have pneumonia** | 30 | 199,968 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **实际结果:****没有肺炎** | 30 | 199,968 |'
- en: This model correctly identified two cases, making two **True Positive** (**TP**)
    predictions. This is a clear improvement over the previous iteration. However,
    the model also identified 30 people as having pneumonia, while they were not ill
    in reality. We call such an error a **False Positive** (**FP**) prediction. Is
    having 30 false positives a significant disadvantage? That depends on how physicians
    will use the model. If all subjects will be automatically prescribed with heavy
    medication with side-effects, false positives can be critical.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型正确地识别了两个病例，做出了两个**真阳性**（**TP**）预测。这相较于之前的版本有了明显的改进。然而，模型也将30个实际没有肺炎的人误判为有肺炎。我们称这种错误为**假阳性**（**FP**）预测。那么，30个假阳性是否是一个显著的缺点呢？这取决于医生如何使用这个模型。如果所有被诊断的人都会被自动开具带有副作用的重药，假阳性就可能是一个严重问题。
- en: 'It may be less severe if we consider a positive model only as a possibility
    of having a disease. If a positive model answer only signals that the patient
    must go through a specific set of diagnostic procedures, then we can see a benefit:
    to achieve the same level of pneumonia identification, therapists will diagnose
    only 32 patients, where previously they had to investigate 200,000 cases. If we
    had not used the confusion table, we might have missed dangerous model behavior
    that would negatively affect people''s health.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅将正向模型视为患病的可能性，情况可能没有那么严重。如果正向模型的回答仅表示患者必须经过一组特定的诊断程序，那么我们就能看到一个好处：为了达到相同的肺炎识别水平，治疗师只需要诊断32个患者，而之前需要检查200,000个案例。如果我们没有使用混淆矩阵，可能会错过模型的不良行为，这可能对人们的健康造成负面影响。
- en: 'Next, your team has done another experiment and created a new model:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你的团队进行了另一个实验，并创建了一个新模型：
- en: '|  | **Model prediction:****Has pneumonia** | **Model prediction:****Does not
    have pneumonia** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | **模型预测：** **有肺炎** | **模型预测：** **没有肺炎** |'
- en: '| **Real outcome:****Has pneumonia** | 0 | 2 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **实际结果：** **有肺炎** | 0 | 2 |'
- en: '| **Real outcome:****Does not have pneumonia** | 100,000 | 99,998 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **实际结果：** **没有肺炎** | 100,000 | 99,998 |'
- en: Does this model perform better? The model would have missed one patient that
    needed therapy and assigned 100,000 healthy people to a treatment group, making
    physicians do unnecessary work. In truth, you can make the final decision only
    after presenting results to the people who will use the model. They may have a
    different opinion on what is best. It would be best to define this at the first
    stages of the project by creating a model-testing methodology document by collaborating
    with experts in the field.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型表现得更好吗？该模型可能会漏掉一个需要治疗的患者，同时将100,000个健康人分配到治疗组，导致医生做不必要的工作。实际上，只有在将结果展示给最终使用该模型的人之后，你才能做出最终决策。他们可能对什么是最佳选择有不同的看法。最好在项目的初期阶段通过与该领域的专家合作，创建一个模型测试方法文档来定义这一点。
- en: You will face binary classification problems everywhere, thus having a good
    understanding of terminology is important.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在各个地方遇到二分类问题，因此对术语的良好理解非常重要。
- en: 'You can see all new concepts summed up in the following table:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的表格中看到所有新概念的总结：
- en: '|  | **Model prediction:****1 (positive case)** | **Model prediction:****0
    (negative case)** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | **模型预测：** **1（阳性案例）** | **模型预测：** **0（阴性案例）** |'
- en: '| **Real outcome:****1 (positive case)** | TP | FN |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **实际结果：** **1（阳性案例）** | 真阳性（TP） | 假阴性（FN） |'
- en: '| **Real outcome:****0 (negative case)** | FP | TN |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| **实际结果：** 0（阴性案例） | 假阳性（FP） | 真阴性（TN） |'
- en: It is crucial to note that you can control the amount of false positive and
    false negative responses for a single model. Classifiers output a probability
    of a data point belonging to a class. That is, the model prediction is a number
    between 0 and 1\. You can decide whether a prediction belongs to a positive or
    negative class by comparing it with a threshold. For example, if the threshold
    is 0.5, then any model prediction greater than 0.5 will belong to class 1 and
    to 0 otherwise.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 需要特别注意的是，你可以控制单个模型的假阳性和假阴性反应的数量。分类器输出一个数据点属于某一类别的概率。也就是说，模型的预测是一个介于0和1之间的数字。你可以通过将其与阈值进行比较来决定一个预测是否属于正类或负类。例如，如果阈值是0.5，则任何大于0.5的模型预测都属于类别1，否则属于类别0。
- en: By changing the threshold, you can change the proportions between the cells
    in the confusion table. By choosing a large threshold, like 0.9, the volume of
    false positive responses will decrease, but false negative responses will increase.
    Threshold selection is essential for binary classification problems. Some environments,
    such as digital advertising, will be more forgiving to false positives, while
    in others, such as healthcare or insurance, you may find them unacceptable.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变阈值，你可以改变混淆矩阵中各单元格之间的比例。选择一个较大的阈值，比如0.9，假阳性反应的数量会减少，但假阴性反应的数量会增加。阈值选择对于二分类问题至关重要。一些环境，比如数字广告，对于假阳性会更宽容，而在其他环境中，比如医疗保健或保险，假阳性可能是无法接受的。
- en: Confusion tables provide deep insights into classification problems but require
    your attention and time. This can be limiting when you want to do numerous experiments
    and compare many models. To simplify the process, statisticians and data scientists
    have designed many metrics that sum up classifier performance without suffering
    from problems like accuracy metrics do. First, let's examine some ways to summarize
    confusion table rows and columns. From there, we will explore how to condense
    it into a single statistic.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵为分类问题提供了深刻的洞察，但需要你投入时间和精力。这在你希望进行大量实验并比较许多模型时可能成为限制。为了简化这个过程，统计学家和数据科学家设计了许多度量方法，可以在不遇到像准确率度量那样的问题的情况下总结分类器的性能。首先，让我们检查一些总结混淆矩阵行和列的方法。然后，我们将探索如何将其浓缩为一个单一的统计量。
- en: 'In the following table, you can see two new metrics for summarizing different
    kinds of errors, precision and recall:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，你可以看到两种新的度量，用于总结不同类型的错误，精确度和召回率：
- en: '|  | **Model prediction:****1** (**positive case**) | **Model prediction:****0**
    (**negative case**) | **Combined metric** |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | **模型预测：**1**（**正类**） | **模型预测：**0**（**负类**） | **综合度量** |'
- en: '| **Real outcome:****1** (**positive case**) | True Positive | False Negative
    | ![](img/7714af33-14b7-42ca-a21b-8ed301f8b76c.png) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| **实际结果：**1**（**正类**） | 真正例 | 假负例 | ![](img/7714af33-14b7-42ca-a21b-8ed301f8b76c.png)
    |'
- en: '| **Real outcome:****0** (**negative case**) | False Positive | True Negative
    |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| **实际结果：**0**（**负类**） | 假正例 | 真负例 |  |'
- en: '| **Combined metric** | ![](img/9e75cead-a17d-444b-804a-6e25c5aa57f9.png),
    also called **True Positive Rate** (**TPR**) |  |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| **综合度量** | ![](img/9e75cead-a17d-444b-804a-6e25c5aa57f9.png)，也称为**真正例率**（**TPR**）
    |  |  |'
- en: Precision measures a proportion of positive (relevant) cases that your model
    has identified. If your model predicted 10 positive cases and 2 positive predictions
    turned out to be negative in reality, then its precision would be 0.8\. Recall
    represents a probability of correctly predicting a positive case. If out of 10
    positive cases, the model had predicted all 10 correctly (10 true positives) and
    marked 5 negative cases as positive (5 false positives), then its recall would
    be 0.67\. A recall of 0.67 means that if our model predicts a positive case, it
    will be correct 67 times out of 100.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度衡量你的模型识别出的正类（相关）案例的比例。如果模型预测了10个正类案例，并且其中2个预测为正类的实际情况为负类，那么它的精确度为0.8。召回率表示正确预测正类案例的概率。如果在10个正类案例中，模型正确预测了全部10个（10个真正例），并将5个负类案例错误标记为正类（5个假正例），那么它的召回率为0.67。召回率为0.67意味着，如果我们的模型预测正类案例，它将在100次预测中有67次是正确的。
- en: 'For binary classification, precision and recall diminish the amount of metrics
    we must work with to two. This is better, but not ideal. We can sum up everything
    into a single number by using a metric called **F1-score**. You can calculate
    F1 using the following formula:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类，精确度和召回率将我们必须处理的度量缩减为两个。这是更好的，但仍不是理想的。我们可以通过使用一种称为**F1-score**的度量，将所有内容汇总为一个单一的数值。你可以使用以下公式计算F1：
- en: '![](img/692dbd05-b1da-4d50-b59d-f082413b7b83.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/692dbd05-b1da-4d50-b59d-f082413b7b83.png)'
- en: F1 is 1 for a perfect classifier and 0 for the worst classifier. Because it
    considers both precision and recall, it does not suffer from the same problem
    as accuracy and is a better default metric for classification problems.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: F1对于完美分类器为1，对于最差的分类器为0。由于它同时考虑了精确度和召回率，它不会像准确率那样存在相同的问题，因此它是分类问题的更好默认度量。
- en: More about imbalanced classes
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多关于不平衡类别的信息
- en: In the preceding examples, you might have noticed that many prediction problems
    suffer from a phenomenon where one class occurs much more frequently than the
    others. Identifying diseases such as cancer, estimating probabilities of credit
    default, or detecting fraud in financial transactions are all examples of imbalanced
    problems – positive cases are much less frequent than the negative ones. In such
    situations, estimating classifier performance becomes tricky. Metrics such as
    accuracy start to show an overly optimistic picture, so you need to resort to
    more advanced technical metrics. The F1 score gives much more realistic values
    in this setting. However, the F1 score is calculated from class assignments (0
    or 1 in the case of binary classification) rather than class probabilities (0.2
    and 0.95 in the case of binary classification).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，你可能已经注意到，许多预测问题存在一种现象，即某一类出现的频率远高于其他类。识别癌症等疾病、估算信用违约的概率或检测金融交易中的欺诈行为都是不平衡问题的例子——正例远少于负例。在这种情况下，评估分类器性能变得很棘手。像准确率这样的指标开始显得过于乐观，因此你需要依赖更先进的技术指标。在这种情境下，F1得分提供了更为真实的数值。然而，F1得分是从类别分配（在二分类情况下为0或1）计算得出的，而不是类别概率（在二分类情况下为0.2和0.95）。
- en: 'Most machine learning models output a probability of an example belonging to
    a certain class, rather than direct class assignment. In particular, a cancer-detection
    model could output a 0.32 (32%) disease probability based on the incoming data.
    Then we must decide whether the patient will be labeled as having cancer or not.
    To do this, we can use a threshold: all values lower than or equal to this threshold
    will be labeled as 0 (does not have cancer), and all values greater than this
    threshold will be considered as 1 (has cancer). The threshold can greatly affect
    the resulting model''s quality, especially for imbalanced datasets. For example,
    lower threshold values of will likely result in more 0 labels, however, the relationship
    is not linear.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习模型输出的是某个样本属于某个类别的概率，而不是直接的类别分配。特别是，癌症检测模型可能会基于输入数据输出一个0.32（32%）的疾病概率。然后，我们必须决定是否将患者标记为患有癌症。为此，我们可以使用阈值：所有低于或等于该阈值的值将被标记为0（没有癌症），而所有大于该阈值的值将被标记为1（患有癌症）。阈值会极大地影响结果模型的质量，尤其是对于不平衡数据集。例如，较低的阈值可能会导致更多的0标签，然而这种关系并不是线性的。
- en: 'To illustrate this, let''s take a trained model and generate predictions for
    the test dataset. If we calculate class assignments by taking lots of different
    thresholds, and then calculate the precision, recall, and F1 score for each of
    those assignments, we could depict each precision and recall value in a single
    plot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们可以使用一个训练好的模型并为测试数据集生成预测。如果我们通过设置不同的阈值来计算类别分配，然后为每个类别分配计算精度、召回率和F1得分，我们可以在单一图表中描绘出每个精度和召回率值：
- en: '![](img/c11524ea-c914-4a1d-97b1-75b79b2d55e8.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c11524ea-c914-4a1d-97b1-75b79b2d55e8.png)'
- en: The preceding plot was made using the `yellowbrick` library, which contains
    many useful visualization tools for model selection and interpretation. You can
    see the capabilities of this library here: [https://www.scikit-yb.org/en/latest/index.html](https://www.scikit-yb.org/en/latest/index.html).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表是使用`yellowbrick`库制作的，该库包含许多用于模型选择和解释的有用可视化工具。你可以在这里看到这个库的功能：[https://www.scikit-yb.org/en/latest/index.html](https://www.scikit-yb.org/en/latest/index.html)。
- en: In the preceding plot, you can see the precision (blue), recall (green), and
    F1 (red) values for each threshold between 0 and 1\. Based on this plot, we can
    see that 0.5, which is the default in many machine learning libraries, might not
    be the best choice and that something like 0.45 would yield more optimal metric
    values.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图表中，你可以看到每个阈值（从0到1之间）对应的精度（蓝色）、召回率（绿色）和F1得分（红色）。根据这个图表，我们可以看到0.5（这是许多机器学习库中的默认值）可能不是最好的选择，像0.45这样的阈值可能会产生更为优化的指标值。
- en: Another useful concept shown in the plot is the queue rate (depicted in magenta),
    which shows the proportion of instances in the test dataset labeled as positive.
    For the 0.45 threshold (identified in the plot as a dashed line), you can see
    that the queue rate is 0.4\. This means that approximately 40% of all cases will
    be labeled as fraudulent. Depending on the business process in which the model
    will be used, positive cases might need to be further investigated by humans.
    In some cases, manual checking takes a lot of time or resources, but it is OK
    to misclassify a few positive instances for a much lower queue rate. In such cases,
    you might want to choose models with lower queue rates even if their performance
    is lower.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图中展示的另一个有用概念是队列率（以品红色显示），它表示在测试数据集中被标记为正类的实例所占比例。对于0.45的阈值（图中以虚线表示），你可以看到队列率为0.4。也就是说，大约40%的所有案例将被标记为欺诈。根据模型将被使用的业务流程，正类案例可能需要进一步由人工调查。在某些情况下，人工检查会消耗大量时间或资源，但对于更低的队列率，误判一些正类实例是可以接受的。在这种情况下，即使模型性能较低，你可能也会选择队列率更低的模型。
- en: All information about precision, recall, and thresholds can be further summarized
    into a single number called the **area under precision-recall curve** (**PR AUC**).
    This metric can be used to make quick judgments over a large number of different
    models without making manual evaluations of model quality on different thresholds.
    Another metric that is frequently used for binary classifier evaluations is called
    the **area under the** **receiver operating characteristic curve** (**ROC AUC**).
    In general, you will want to use PR AUC for imbalanced datasets and ROC AUC for
    balanced datasets.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 所有关于精确度、召回率和阈值的信息可以进一步总结为一个叫做**精确度-召回率曲线下的面积**（**PR AUC**）的单一数值。这个指标可以用来快速判断大量不同模型，而无需手动评估模型在不同阈值下的质量。另一个常用于二分类器评估的指标是**受试者工作特征曲线下的面积**（**ROC
    AUC**）。通常情况下，你会希望在数据集不平衡时使用PR AUC，在数据集平衡时使用ROC AUC。
- en: The difference arises from the ways in which those metrics are calculated, but
    we will omit the technical details here for the sake of brevity. Calculating AUC
    metrics is a bit more involved than the other metrics presented in this chapter.
    For more information, check out [https://www.chioka.in/differences-between-roc-auc-and-pr-auc/](https://www.chioka.in/differences-between-roc-auc-and-pr-auc/) and
    [https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异来自于这些指标的计算方式，但为了简洁起见，我们将在这里省略技术细节。计算AUC指标比本章介绍的其他指标要复杂一些。欲了解更多信息，请查看[https://www.chioka.in/differences-between-roc-auc-and-pr-auc/](https://www.chioka.in/differences-between-roc-auc-and-pr-auc/)和[https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)。
- en: 'There is no single rule for choosing the right balance for the precision, recall,
    F1, and queue rate. Those values should be thoughtfully investigated with respect
    to the business process. Relying solely on technical metrics for model selection
    can result in a disaster, as models that are the best for your customers are not
    always the most accurate models. In some cases, high precision might be more important
    than recall, while for others, the queue rate will be most important. At this
    point, we need to introduce another kind of metric that will act as a bridge between
    technical metrics and business requirements: business metrics.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 选择精确度、召回率、F1值和队列率的合适平衡并没有固定的规则。这些值应该根据业务流程进行深入的调查。单纯依赖技术指标来选择模型可能会导致灾难，因为最适合你的客户的模型不一定是最准确的模型。在某些情况下，高精确度可能比召回率更为重要，而在其他情况下，队列率可能是最重要的。在这种情况下，我们需要引入另一种指标，它将充当技术指标和业务需求之间的桥梁：业务指标。
- en: Applying business metrics
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用业务指标
- en: While technical metrics may be essential in the model-development process, they
    do not speak the language of business. A bunch of confusion tables with the F1
    score will rarely impress your customers or stakeholders. They are more concerned
    with the problem that the model will solve than with its internals. They won't
    be interested in the false positive rate, but they will listen when you will talk
    about the money that the model would save them in the next quarter. Therefore,
    designing a business metric is important. Your project will need a quality measure
    that is crystal clear for all key stakeholders, with or without experience in
    data science. If you are in the business environment, a good start would be to
    look at the **key performance indicators** (**KPI**) of business processes you
    are trying to improve using machine learning. It is likely that you will find
    a ready-to-use business metric.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管技术指标在模型开发过程中至关重要，但它们并不适用于商业语言。充满混淆表格和 F1 分数的数据，通常难以打动你的客户或利益相关者。他们更关心的是模型能解决什么问题，而不是其内部细节。他们不会对假阳性率感兴趣，但如果你提到模型在下个季度能为他们节省多少资金，他们会愿意听。因此，设计商业指标非常重要。你的项目需要一个对所有关键利益相关者（无论是否具备数据科学经验）都非常清晰的质量衡量标准。如果你身处商业环境，开始的好方法是看看你通过机器学习试图改进的业务流程的**关键绩效指标**（**KPI**）。你很可能会找到一个现成的商业指标。
- en: At this point, we conclude an introduction to technical metrics. There are many
    more ways to test classification and regression models, all with pros and cons.
    Enumerating and describing them all would take a book in itself and would be unnecessary
    because we have already achieved our goal. Armed with new concepts from this chapter,
    you now understand the general flow of how to evaluate a machine learning model
    before testing it in real-world conditions. Now you can use offline model testing
    to check the model's quality before deploying the model. Next, we will explore
    online testing to complete your understanding of a model's quality assessment.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们总结了技术指标的介绍。测试分类和回归模型的方法有很多，每种方法都有其优缺点。列举并描述所有这些方法将需要一本书的篇幅，而且没有必要，因为我们已经达到了目标。通过本章的新概念，你现在已经理解了如何在实际条件下测试之前评估机器学习模型的一般流程。现在，你可以使用离线模型测试来检查模型的质量，然后再进行部署。接下来，我们将探索在线测试，以完善你对模型质量评估的理解。
- en: Online model testing
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线模型测试
- en: 'Even a great offline model testing pipeline won''t guarantee that the model
    will perform exactly the same in production. There are always risks that can affect
    your model performance, such as the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是一个优秀的离线模型测试流程，也无法保证模型在生产环境中的表现完全相同。总是有一些风险会影响模型的性能，比如以下几种：
- en: '**Humans**: We can make mistakes and leave bugs in the code.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类**：我们会犯错误，并可能在代码中留下 bug。'
- en: '**Data collection**: Selection bias and incorrect data-collection procedures
    may disrupt true metric values.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集**：选择性偏差和不正确的数据收集程序可能会破坏真实的指标值。'
- en: '**Changes**: Real-world data may change and deviate from your training dataset,
    leading to unexpected model behavior.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变化**：现实世界中的数据可能会发生变化，偏离你的训练数据集，从而导致模型出现意外的行为。'
- en: The only way to be certain about model performance in the near future is to
    perform a live test. Depending on the environment, such test may introduce big
    risks. For example, models that assess airplane engine quality or patient health
    would be unsuitable for real-world testing before we become confident in their
    performance.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在不久的将来，确认模型性能的唯一方法是进行实时测试。根据环境的不同，此类测试可能会带来很大的风险。例如，评估飞机发动机质量或病人健康状况的模型，在我们对其性能没有足够信心之前，是不适合进行实际测试的。
- en: 'When the time for a live test comes, you will want to minimize risks while
    making statistically valid conclusions. Thankfully, there is a statistical framework
    for that purpose known as hypothesis testing. When performing a hypothesis test,
    you check the validity of some idea (hypothesis) by collecting data and executing
    a statistical test. Imagine you need to check whether your new advertising model
    increases revenues from the ad service. To do this, you randomly split all your
    clients into two groups: one group uses the old advertising algorithm, while the
    others see ads recommended by a new algorithm. After you have collected a sufficient
    volume of data, you compare two groups and measure differences between them. Why
    do we need to bother with statistics, you may ask?'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行现场测试时，你将希望在得出统计有效结论的同时，最小化风险。幸运的是，为此目的存在一种统计框架，称为假设检验。在进行假设检验时，你通过收集数据并执行统计测试来检查某个想法（假设）的有效性。假设你需要检查你的新广告模型是否增加了广告服务的收入。为此，你将所有客户随机分成两组：一组使用旧的广告算法，另一组则看到由新算法推荐的广告。收集了足够的数据后，你会比较这两组并衡量它们之间的差异。你可能会问，为什么我们需要去麻烦统计学呢？
- en: 'Because we can answer the following questions only with the help of stats:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们只能借助统计学回答以下问题：
- en: How should I sort (sample) individuals into each group? Can my sampling process
    distort results of the test?
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该如何将个体排序（抽样）到每个组别中？我的抽样过程会扭曲测试结果吗？
- en: What is the minimum number of clients in each group? Can random fluctuations
    in the data affect my measurements?
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个组别的最小客户数量是多少？数据中的随机波动会影响我的测量结果吗？
- en: How long should I run the test for to get a confident answer?
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该运行测试多长时间才能得出一个有信心的答案？
- en: What formula should I use to compare results in each group?
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该使用什么公式来比较每个组别中的结果？
- en: 'The experiment setup for a hypothesis test splits test targets into two groups
    on purpose. We can try to use a single group instead. For instance, we can take
    one set of measurements with the old model. After the first part of the experiment
    is finished, we can deploy the new algorithm and measure its effect. Then, we
    compare two measurements made one after another. What could go wrong? In fact,
    the results we get wouldn''t mean anything. Many things could have changed in
    between our measurements, such as the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设检验的实验设置故意将目标分为两组。我们也可以尝试只使用单组。举例来说，我们可以先用旧模型做一组测量。第一部分实验结束后，再部署新算法并测量其效果。然后，我们比较两个连续测量之间的差异。可能会出现什么问题？事实上，我们得到的结果并没有什么意义。我们的测量之间可能发生了许多变化，比如以下这些：
- en: User preferences
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户偏好
- en: General user mood
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户的整体情绪
- en: Popularity of our service
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们服务的受欢迎程度
- en: Average user profile
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均用户画像
- en: Any other attribute of users or businesses
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户或企业的其他任何属性
- en: 'All these hidden effects could affect our measurements in unpredictable ways,
    which is why we need two groups: test and control. We must select these groups
    in such a way that the only difference between them is our hypothesis. It should
    be present in the test group and missing from the control group. To illustrate,
    in medical trials, control groups are the ones who get the placebo. Suppose we
    want to test the positive effect of a new painkiller. Here are some examples of
    bad test setups:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些隐藏的影响因素可能以不可预测的方式影响我们的测量结果，这就是为什么我们需要两个组别：实验组和对照组。我们必须以这样的方式选择这些组别，确保它们之间的唯一区别就是我们的假设。假设应该出现在实验组中，而在对照组中缺失。例如，在医学试验中，对照组是那些接受安慰剂的人。假设我们想测试一种新止痛药的正面效果。以下是一些不良测试设置的例子：
- en: The control group consists only of women.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对照组仅由女性组成。
- en: The test and control groups are in different geographical locations.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验组和对照组位于不同的地理位置。
- en: You use biased interviews to preselect people for an experiment.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你使用偏见性采访来预先选择实验参与者。
- en: The easiest way to create groups is random selection. Truly random selection
    may be hard to do in the real world, but is easy if you deal with internet services.
    There, you may just randomly decide which version of your algorithm to use for
    each active user. Be sure to always design experiment setups with an experienced
    statistician or data scientist, as correct tests are notoriously hard to execute,
    especially in offline settings.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 创建组别的最简单方法是随机选择。真正的随机选择在现实世界中可能很难实现，但如果你处理的是互联网服务，这就很容易了。在那里，你可以随机决定每个活跃用户使用哪一版本的算法。务必始终与经验丰富的统计学家或数据科学家共同设计实验设置，因为正确的测试执行起来非常困难，尤其是在离线环境中。
- en: 'Statistical tests check the validity of a null hypothesis, that is, that the
    results you got are by chance. The opposite result is called an alternative hypothesis.
    For instance, here is the hypothesis set for our ad model test:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 统计检验验证原假设的有效性，即你获得的结果是否是偶然的。相反的结果称为备择假设。例如，这是我们广告模型测试的假设设置：
- en: '**Null hypothesis**: The new model does not affect the ad service revenue.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原假设**：新模型不会影响广告服务收入。'
- en: '**Alternative hypothesis**: The new model affects the ad service revenue.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**备择假设**：新模型会影响广告服务收入。'
- en: Typically, a statistical test measures the probability of a null hypothesis
    being true. If the chances are low, then the alternative hypothesis is true. Otherwise,
    we accept the null hypothesis. If, according to a statistical test, the probability
    that the new model does not affect service revenue would be 5%, we would say that
    we accept the alternative hypothesis at a 95% confidence level. This means the
    model affects the ad service revenue with a 95% probability. The significance
    level for rejecting the null hypothesis depends on the level of risk you want
    to take. For an ad model, a 95% significance may be enough, while no less than
    a 99% significance is satisfactory for a model that tests patient health conditions.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，统计检验衡量原假设为真的概率。如果这种概率很低，那么备择假设为真。否则，我们接受原假设。如果根据统计检验，新的模型不会影响服务收入的概率为5%，那么我们会在95%的置信水平下接受备择假设。这意味着模型以95%的概率影响广告服务收入。拒绝原假设的显著性水平取决于你愿意承担的风险级别。对于广告模型，95%的显著性可能足够，而对于测试患者健康状况的模型，则需要不低于99%的显著性。
- en: The most typical hypothesis test is comparing two means. If we use this test
    in our ad model example, we would measure average revenues with and without the
    new ranking algorithm. We may accept or reject the null hypothesis using a test
    statistic when the experiment is finished.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最典型的假设检验是比较两个均值。如果我们在广告模型示例中使用此检验，我们将测量新排名算法前后的平均收入。实验结束后，我们可以通过检验统计量来接受或拒绝原假设。
- en: 'The amount of data you need to collect for conducting a hypothesis test depends
    on several factors:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 进行假设检验所需收集的数据量取决于多个因素：
- en: '**Confidence level**: The more statistical confidence you need, the more data
    is required to support the evidence.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**置信水平**：你需要的统计置信度越高，所需的数据量就越大，以支持证据。'
- en: '**Statistical power**: This measures the probability of detecting a significant
    difference, if one exists. The more statistical power your test has, the lower
    the chance of false negative responses.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计功效**：这衡量了在存在显著差异时检测到该差异的概率。你的测试统计功效越高，出现假阴性结果的概率越低。'
- en: '**Hypothesized difference and population variance**: If your data has large
    variance, you need to collect more data to detect a significant difference. If
    the difference between the two means is smaller than population variance, you
    would need even more data.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假设差异和总体方差**：如果你的数据方差很大，你需要收集更多的数据以检测显著差异。如果两个均值之间的差异小于总体方差，你则需要更多的数据。'
- en: 'You can see how different test parameters determine their data hunger in the
    following table:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下表格看到不同测试参数如何决定它们对数据的需求：
- en: '| **Confidence level** | **Statistical power** | **Hypothesized difference**  |
    **Population variance** | **Recommended sample size** |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| **置信水平** | **统计功效** | **假设差异**  | **总体方差** | **推荐样本量** |'
- en: '| 95% | 90% | $10 | $100 | 22 ad demonstrations to clients |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 95% | 90% | $10 | $100 | 22次广告展示给客户 |'
- en: '| 99% | 90% | $10 | $100 | 30 ad demonstrations to clients |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 99% | 90% | $10 | $100 | 30次广告展示给客户 |'
- en: '| 99% | 90% | $1 | $100 | 2,976 ad demonstrations to clients |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 99% | 90% | $1 | $100 | 2,976次广告展示给客户 |'
- en: 'While powerful, hypothesis tests have limitations: you need to wait until the
    experiment ends before you can apply its results. If your model is bad, you won''t
    be able to reduce damage without compromising the test procedure. Another limitation
    is that you can test only one model at a time with a single hypothesis test.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管假设检验功能强大，但它也有局限性：你需要等到实验结束后才能应用其结果。如果你的模型很差，就无法在不妥协测试过程的情况下减少损害。另一个局限是，每次只能用单一的假设检验来测试一个模型。
- en: 'In situations where you can trade off statistical rigor for speed and risk-aversion,
    there is an alternative approach called **Multi-Armed Bandits** (**MABs**). To
    understand how MABs work, imagine yourself inside a casino with lots of slot machines.
    You know that some of those machines yield better returns than others. Your task
    is to find the best slot machine with a minimal number of trials. Thus, you try
    different (multi) arms of slot machines (bandits) to maximize your reward. You
    can extend this situation to testing multiple ad models: for each user, you must
    find a model that is most likely to increase your ad revenue.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在可以权衡统计严谨性、速度和风险规避的情况下，有一种替代方法叫做 **多臂老虎机**（**MABs**）。为了理解 MABs 的工作原理，假设你身处赌场中，那里有许多老虎机。你知道其中一些机器的回报比其他机器更好。你的任务是通过最少的试验次数找到最好的老虎机。因此，你尝试不同的老虎机（多臂）的手臂（bandits），以最大化你的奖励。你可以将这个情境扩展到测试多个广告模型：对于每个用户，你必须找到最有可能增加广告收入的模型。
- en: 'The most popular MAB algorithm is called an epsilon-greedy bandit. Despite
    the name, the inner workings of the method are simple:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的 MAB 算法叫做 epsilon-贪婪 bandit。尽管名字如此，这种方法的内部机制其实很简单：
- en: Select a small number called **epsilon**. Suppose we have chosen 0.01.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个小的数字，称为 **epsilon**。假设我们选择了 0.01。
- en: Choose a random number between 0 and 1\. This number will determine whether
    MAB will explore or exploit a possible set of choices.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个介于 0 和 1 之间的随机数。这个数字将决定 MAB 是进行探索还是利用可能的选择集。
- en: If the number is lower or equal to epsilon, make a choice at random and record
    a reward after making an action tied to your choice. We call this process exploration
    – MAB tries different actions at random with a low probability to find out their
    mean reward.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果数字小于或等于 epsilon，则随机做出选择，并在执行与选择相关的行为后记录奖励。我们称这个过程为探索（exploration）——MAB 以较低的概率随机尝试不同的行为，以发现它们的平均奖励。
- en: If your number is greater than epsilon, make the best choice according to the
    data you have collected. We call this process exploitation – MAB exploits knowledge
    it has collected to execute an action that has the best expected reward. MAB selects
    the best action by averaging all recorded rewards for each choice and selecting
    a choice with the greatest reward expectation.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你的数字大于 epsilon，根据你收集的数据做出最佳选择。我们称这个过程为利用（exploitation）——MAB 利用它所收集的知识执行预期奖励最好的行动。MAB
    通过对每个选择的所有记录奖励进行平均，选择期望奖励最大的选项。
- en: Frequently, we start with large values of epsilon and decrease it to smaller
    values. In this way, MAB explores lots of random choices at the start and exploits
    the most profitable actions toward the end. The exploration frequency is gradually
    diminishing, becoming closer to zero.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们从较大的 epsilon 值开始，然后逐渐将其减小到更小的值。这样，MAB 在开始时会探索大量的随机选择，并在最后利用最有利的行动。探索频率逐渐减少，接近零。
- en: When you first launch MAB, it collects rewards from random actions. As time
    passes, you will see that average rewards for all choices converge to their true
    values. The major benefit of MABs is that they change their behavior in real time.
    While someone is waiting for a hypothesis test results, MAB gives you a changing
    picture while covering to the best choice. Bandits are one of the most basic reinforcement
    learning algorithms. Despite their simplicity, they can provide good results.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当你首次启动 MAB 时，它从随机行为中收集奖励。随着时间的推移，你会看到所有选择的平均奖励趋向于它们的真实值。MAB 的主要优势在于它可以实时改变行为。当有人等待假设检验结果时，MAB
    给你呈现的是一个不断变化的画面，同时不断向最佳选择收敛。Bandits 是最基本的强化学习算法之一。尽管它们很简单，但可以提供良好的结果。
- en: 'We now have two new testing approaches to use. How do we choose between them?
    Unfortunately, there is no simple answer. Hypothesis tests and MABs pose different
    constraints on data, sampling processes, and experiment conditions. It is better
    to consult an experienced statistician or a data scientist before deciding. Mathematical
    constraints are not the only things that affect the choice; the environment is
    also important. MABs are easy to apply in situations where you can test different
    choices on random individuals from the whole population. This may be very convenient
    when testing models for a large online retailer, but is impossible for clinical
    trials, where you are better to apply hypothesis testing. Let''s see a rule of
    thumb for choosing between MABs and hypothesis tests:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两种新的测试方法可供使用。我们该如何选择其中之一呢？不幸的是，这没有简单的答案。假设检验和MAB（多臂老虎机）对数据、采样过程和实验条件提出了不同的约束。在做决定之前，最好请教经验丰富的统计学家或数据科学家。数学约束并不是唯一影响选择的因素；环境也很重要。MABs在可以从整个群体的随机个体中测试不同选择的情况下容易应用。当测试大型在线零售商的模型时，这可能非常方便，但在临床试验中是不可能的，那里你最好使用假设检验。让我们看看选择MABs和假设检验的经验法则：
- en: MABs are better suited to environments where you need to test many alternatives
    with a limited set of resources. You trade off statistical rigor for efficiency
    when using MABs. MABs can take a lot of time to converge, gradually improving
    over time.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MABs更适合在需要用有限资源测试多个备选方案的环境中使用。使用MAB时，你在统计严谨性和效率之间做出权衡。MABs可能需要很长时间才能收敛，随着时间推移逐渐改进。
- en: You should apply hypothesis tests if you have only one alternative, if your
    trial involves great risks, or if you need a statistically-rigorous answer. Hypothesis
    tests take a fixed amount of time and resources to complete, but impose larger
    risks than MABs.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你只有一个备选方案，或者你的试验涉及较大风险，或者你需要统计学严谨的答案，你应该使用假设检验。假设检验需要固定的时间和资源来完成，但比MABs带来更大的风险。
- en: While testing models in an online setting is extremely important for making
    sure that your offline test results stay true after the deployment stage, there
    is still a danger zone that we have not covered. Abrupt and unexpected changes
    in data can severely affect or even break the deployed model, so it is also important
    to monitor incoming data quality.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线环境中测试模型对于确保离线测试结果在部署阶段之后保持有效至关重要，但仍然存在我们尚未覆盖的危险区域。数据的突然和意外变化可能会严重影响甚至破坏已部署的模型，因此监控输入数据质量也非常重要。
- en: Online data testing
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线数据测试
- en: Even after performing successful online tests, you are not fully guarded against
    unexpected issues with model operation. Machine learning models are sensitive
    to incoming data. Good models have a certain degree of generalization, but significant
    changes in data or underlying processes that generate data can lead the model
    predictions astray. If online data significantly diverges from test data, you
    can't be certain about model performance before performing online tests. If the
    test data differs from the training data, then your model won't work as expected.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 即使成功地进行了在线测试，你也无法完全防范模型操作中出现的意外问题。机器学习模型对输入数据非常敏感。好的模型具有一定的泛化能力，但数据或生成数据的基本过程发生显著变化时，模型预测可能会失准。如果在线数据与测试数据有显著偏差，在进行在线测试之前，你无法确定模型的性能。如果测试数据与训练数据不同，那么你的模型就无法按预期工作。
- en: 'To overcome this, your system needs to monitor all incoming data and check
    its quality on the fly. Here are some typical checks:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，你的系统需要实时监控所有输入数据并检查其质量。以下是一些典型的检查：
- en: Missing values in mandatory data fields
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必填数据字段中的缺失值
- en: Minimum and maximum values
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小值和最大值
- en: Acceptable values of categorical data fields
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类数据字段的可接受值
- en: String data formats (dates, addresses)
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串数据格式（日期、地址）
- en: Target variable statistics (distribution checks, averages)
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标变量统计（分布检查、平均值）
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we answered a very important question: what does it mean for
    a model to work correctly? We explored the nature of errors and studied metrics
    that can quantify and measure model errors. We drew a line between offline and
    online model testing and defined testing procedures for both types. We can perform
    offline model testing using train/validation/test data splits and cross-validation.
    For online testing, we can choose between hypothesis tests and MABs.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回答了一个非常重要的问题：什么才是模型正确工作的标准？我们探讨了错误的性质，并研究了可以量化和衡量模型错误的指标。我们区分了离线和在线模型测试，并为两者定义了测试流程。我们可以通过训练/验证/测试数据集划分和交叉验证来进行离线模型测试。对于在线测试，我们可以选择假设检验和多臂老虎机（MABs）。
- en: In the next chapter, we will look into the inner workings of data science. We
    will dive into the main concepts behind machine learning and deep learning, giving
    an intuitive understanding of how machines learn.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨数据科学的内部运作。我们将深入了解机器学习和深度学习背后的主要概念，帮助读者直观地理解机器是如何学习的。
