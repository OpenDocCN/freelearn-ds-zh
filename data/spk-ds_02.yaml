- en: Chapter 2. The Spark Programming Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。Spark 编程模型
- en: Large-scale data processing using thousands of nodes with built-in fault tolerance
    has become widespread due to the availability of open source frameworks, with
    Hadoop being a popular choice. These frameworks are quite successful in executing
    specific tasks such as **Extract, Transform, and Load** (**ETL**) and storage
    applications that deal with web-scale data. However, developers were left with
    a myriad of tools to work with, along with the well-established Hadoop ecosystem.
    There was a need for a single, general-purpose development platform that caters
    to batch, streaming, interactive, and iterative requirements. This was the motivation
    behind Spark.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 由于开源框架的普及，大规模数据处理已成为常见做法，借助成千上万节点和内建的容错能力，Hadoop 成为了一个流行的选择。这些框架在执行特定任务（如 **提取、转换和加载**（**ETL**）以及处理
    Web 规模数据的存储应用）上非常成功。然而，开发人员需要面对大量工具的选择，以及已经建立完善的 Hadoop 生态系统。业界急需一个单一的、通用的开发平台，能满足批处理、流处理、交互式和迭代式的需求。这正是
    Spark 的动机所在。
- en: The previous chapter outlined the big data analytics challenges and how Spark
    addressed most of them at a very high level. In this chapter, we will examine
    the design goals and choices involved in the making of Spark to get a clearer
    understanding of its suitability as a data science platform for big data. We will
    also cover the core abstraction **Resilient Distributed Dataset** (**RDD**) in
    depth with examples.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章概述了大数据分析面临的挑战，以及 Spark 如何在高层次上解决其中的大部分问题。本章将深入探讨 Spark 的设计目标与选择，以更清晰地了解其作为大数据数据科学平台的适用性。我们还将深入讲解核心抽象
    **弹性分布式数据集**（**RDD**）并通过示例进行说明。
- en: 'As a prerequisite for this chapter, a basic understanding of Python or Scala
    along with elementary understanding of Spark is needed. The topics covered in
    this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前提是需要具备基本的 Python 或 Scala 知识，以及对 Spark 的初步理解。本章涵盖的主题如下：
- en: The programming paradigm - language support and design benefits
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程范式 - 语言支持与设计优势
- en: Supported programming languages
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持的编程语言
- en: Choosing the right language
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的语言
- en: The Spark engine - Spark core components and their implications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 引擎 - Spark 核心组件及其含义
- en: Driver program
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序
- en: Spark shell
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Shell
- en: SparkContext
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkContext
- en: Worker nodes
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点
- en: Executors
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行器
- en: Shared variables
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享变量
- en: Flow of execution
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行流程
- en: The RDD API - understanding the RDD fundamentals
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD API - 理解 RDD 基础
- en: RDD basics
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 基础
- en: Persistence
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久化
- en: RDD operations - let's get your hands dirty
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 操作 - 让我们动手试试
- en: Getting started with the shell
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用 Shell
- en: Creating RDDs
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 RDD
- en: Transformations on normal RDDs
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对普通 RDD 的转换
- en: Transformations on pair RDDs
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对配对 RDD 的转换
- en: Actions
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作
- en: The programming paradigm
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编程范式
- en: For Spark to address the big data challenges and serve as a platform for data
    science and other scalable applications, it was built with well-thought-out design
    considerations and language support.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决大数据挑战并作为数据科学及其他可扩展应用的平台，Spark 在设计时考虑了周密的设计因素和语言支持。
- en: There are Spark APIs designed for varieties of application developers to create
    Spark-based applications using standard API interfaces. Spark provides APIs for
    Scala, Java, R and Python programming languages, as explained in the following
    sections.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了适用于各种应用开发者的 API，以便开发者使用标准的 API 接口创建基于 Spark 的应用程序。Spark 提供了适用于 Scala、Java、R
    和 Python 编程语言的 API，详细内容在以下章节中讲解。
- en: Supported programming languages
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的编程语言
- en: With built-in support for so many languages, Spark can be used interactively
    through a shell, which is otherwise known as **Read-Evaluate-Print-Loop** (**REPL**),
    in a way that will feel familiar to developers of any language. The developers
    can use the language of their choice, leverage existing libraries, and seamlessly
    interact with Spark and its ecosystem. Let us see the ones supported on Spark
    and how they fit into the Spark ecosystem.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 内建支持多种语言，可以通过 Shell 进行交互式使用，这种方式称为 **读取-评估-打印-循环**（**REPL**），对任何语言的开发者来说都非常熟悉。开发者可以选择自己熟悉的语言，利用现有的库，轻松与
    Spark 及其生态系统进行交互。接下来，我们将介绍 Spark 支持的语言以及它们如何融入 Spark 生态系统。
- en: Scala
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scala
- en: Spark itself is written in Scala, a **Java Virtual Machine** (**JVM**) based
    functional programming language. The Scala compiler generates byte code that executes
    on the JVM. So, it can seamlessly integrate with any other JVM-based systems such
    as HDFS, Cassandra, HBase, and so on. Scala was the language of choice because
    of its concise programming interface, an interactive shell, and its ability to
    capture functions and efficiently ship them across the nodes in a cluster. Scala
    is an extensible (scalable, hence the name), statically typed, efficient multi-paradigm
    language that supports functional and object-oriented language features.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 本身是用 Scala 编写的，Scala 是一种基于 **Java 虚拟机** (**JVM**) 的函数式编程语言。Scala 编译器生成的字节码在
    JVM 上执行。因此，它可以与其他基于 JVM 的系统（如 HDFS、Cassandra、HBase 等）无缝集成。选择 Scala 作为编程语言，是因为它简洁的编程接口、交互式命令行以及能够捕获函数并高效地将其传输到集群中的各个节点。Scala
    是一种可扩展的（因此得名）、静态类型、有效率的多范式语言，支持函数式和面向对象的语言特性。
- en: Apart from the full-blown applications, Scala also supports shell (Spark shell)
    for interactive data analysis on Spark.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了完全成熟的应用程序，Scala 还支持 Shell（Spark shell）用于在 Spark 上进行交互式数据分析。
- en: Java
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Java
- en: Since Spark is JVM based, it naturally supports Java. This helps existing Java
    developers to develop data science applications along with other scalable applications.
    Almost all the built-in library functions are accessible from Java. Coding in
    Java for data science assignments is comparatively difficult in Spark, but someone
    very hands-on with Java might find it easy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Spark 是基于 JVM 的，它自然支持 Java。这有助于现有的 Java 开发者开发数据科学应用程序以及其他可扩展的应用程序。几乎所有内置的库函数都可以通过
    Java 访问。在 Spark 中用 Java 编写数据科学任务相对较难，但如果非常熟悉 Java 的人，可能会觉得很容易。
- en: This Java API only lacks a shell-based interface for interactive data analysis
    on Spark.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Java API 唯一缺少的是用于 Spark 上交互式数据分析的基于 Shell 的接口。
- en: Python
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python
- en: Python is supported on Spark through PySpark, which is built on top of Spark's
    Java API (using Py4J). From now on, we will be using the term **PySpark** to refer
    to the Python environment on Spark. Python was already very popular amongst developers
    for data wrangling, data munging, and other data science related tasks. Support
    for Python on Spark became even more popular as Spark could address the scalable
    computation challenge.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Python 通过 PySpark 在 Spark 上得到支持，PySpark 是构建在 Spark 的 Java API 之上的（使用 Py4J）。从现在起，我们将使用
    **PySpark** 这个术语来指代 Spark 上的 Python 环境。Python 已经在开发者中因数据处理、数据清洗和其他数据科学相关任务而广受欢迎。随着
    Spark 可以解决可扩展计算的挑战，Python 在 Spark 上的支持变得更加流行。
- en: Through Python's interactive shell on Spark (PySpark), interactive data analysis
    at scale is possible.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Python 在 Spark 上的交互式命令行（PySpark），可以在大规模数据上进行交互式数据分析。
- en: R
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R
- en: R is supported on Spark through SparkR, an R package through which Spark's scalability
    is accessible through R. SparkR empowered R to address its limitation of single-threaded
    runtime, because of which computation was limited only to a single node.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: R 通过 SparkR 在 Spark 上得到支持，SparkR 是一个 R 包，借此包 Spark 的可扩展性可以通过 R 来访问。SparkR 使得
    R 克服了单线程运行时的局限性，这也是计算仅限于单个节点的原因。
- en: Since R was originally designed only for statistical analysis and machine learning,
    it was already enriched with most of the packages. Data scientists can now work
    on huge data at scale with a minimal learning curve. R is still a default choice
    for many data scientists.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 R 最初仅为统计分析和机器学习设计，因此它已经包含了大多数包。数据科学家现在可以在极大数据量下工作，并且几乎不需要学习曲线。R 仍然是许多数据科学家的首选。
- en: Choosing the right language
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择正确的语言
- en: 'Apart from the developer''s language preference, at times there are other constraints
    that may draw attention. The following aspects could supplement your development
    experience while choosing one language over the other:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了开发者的语言偏好之外，有时还有其他一些约束条件可能会引起关注。以下几点可能会在选择语言时，补充你的开发体验：
- en: An interactive shell comes in handy when developing complex logic. All languages
    supported by Spark except Java have an interactive shell.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开发复杂逻辑时，交互式命令行非常有用。Spark 支持的所有语言中，除了 Java 外，其他都提供了交互式命令行。
- en: R is the lingua franca of data scientists. It is definitely more suitable for
    pure data analytics because of its richer set of libraries. R support was added
    in Spark 1.4.0 so that Spark reaches out to data scientists working on R.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 是数据科学家的通用语言。由于其拥有更丰富的库集，它显然更适合纯数据分析。Spark 1.4.0 中加入了对 R 的支持，这使得 Spark 可以覆盖在
    R 上工作的数据科学家。
- en: Java has a broader base of developers. Java 8 has included lambda expressions
    and hence the functional programming aspect. Nevertheless, Java tends to be verbose.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 拥有更广泛的开发者基础。Java 8 引入了 Lambda 表达式，从而支持了函数式编程。然而，Java 往往显得冗长。
- en: Python is gradually gaining more popularity in the data science space. The availability
    of Pandas and other data processing libraries, and its simple and expressive nature,
    make Python a strong candidate. Python gives more flexibility than R in scenarios
    such as data aggregation from different sources, data cleaning, natural language
    processing, and so on.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 在数据科学领域的受欢迎程度逐渐上升。Pandas 等数据处理库的可用性以及 Python 简单而富有表现力的特性，使其成为一个强有力的候选语言。在数据聚合、数据清理、自然语言处理等场景中，Python
    比 R 更具灵活性。
- en: Scala is perhaps the best choice for real-time analytics because this is the
    closest to Spark. The initial learning curve for developers coming from other
    languages should not be a deterrent for serious production systems. The latest
    inclusions to Spark are usually first available in Scala. Its static typing and
    sophisticated type inference improve efficiency as well as compile-time checks.
    Scala can draw from Java's libraries as Scala's own library base is still at an
    early stage, but catching up.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala 可能是进行实时分析的最佳选择，因为它与 Spark 最为接近。对于来自其他语言的开发者而言，初期的学习曲线不应成为开发生产系统的障碍。Spark
    的最新功能通常会首先在 Scala 中发布。Scala 的静态类型和复杂的类型推断提高了效率，并且增强了编译时的检查。Scala 可以利用 Java 的库，因为
    Scala 自身的库基础仍处于早期阶段，但正在逐步追赶。
- en: The Spark engine
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 引擎
- en: To program with Spark, a basic understanding of Spark components is needed.
    In this section, some of the important Spark components along with their execution
    mechanism will be explained so that developers and data scientists can write programs
    and build applications.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Spark 编程，首先需要对 Spark 组件有基本的理解。在本节中，我们将解释一些重要的 Spark 组件及其执行机制，以帮助开发者和数据科学家编写程序并构建应用程序。
- en: 'Before getting into the details, we suggest you take a look at the following
    diagram so that the descriptions of the Spark gears are more comprehensible as
    you read further:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解细节之前，我们建议你先查看以下图表，以便在进一步阅读时能够更好地理解 Spark 各部分的描述：
- en: '![The Spark engine](img/image_02_001.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 引擎](img/image_02_001.jpg)'
- en: Driver program
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 驱动程序
- en: The Spark shell is an example of a driver program. A driver program is a process
    that executes in the JVM and runs the user's *main* function on it. It has a SparkContext
    object which is a connection to the underlying cluster manager. A Spark application
    is initiated when the driver starts and it completes when the driver stops. The
    driver, through an instance of SparkContext, coordinates all processes within
    a Spark application.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Shell 是驱动程序的一个示例。驱动程序是一个在 JVM 中执行并运行用户 *main* 函数的进程。它拥有一个 SparkContext
    对象，该对象是与底层集群管理器的连接。当驱动程序启动时，Spark 应用程序开始；当驱动程序停止时，Spark 应用程序完成。驱动程序通过 SparkContext
    实例来协调 Spark 应用程序中的所有进程。
- en: Primarily, an RDD lineage **Directed Acyclic Graph** (**DAG**) is built on the
    driver side with data sources (which may be RDDs) and transformations. This DAG
    is submitted to the DAG scheduler when an *action* method is encountered. The
    DAG scheduler then splits the DAG into logical units of work (for example, map
    or reduce) called stages. Each stage, in turn, is a set of tasks, and each task
    is assigned to an executor (worker) by the task scheduler. Jobs may be executed
    in FIFO order or round robin, depending on the configuration.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，一个 RDD 的谱系 **有向无环图**（**DAG**）在驱动程序端构建，包含数据源（可能是 RDD）和转换操作。当遇到 *action* 方法时，这个
    DAG 会被提交给 DAG 调度器。DAG 调度器随后将 DAG 分割成逻辑工作单元（例如 map 或 reduce），称为阶段（stages）。每个阶段又由一组任务组成，每个任务由任务调度器分配给一个执行器（worker）。任务可以按
    FIFO 顺序或轮询顺序执行，具体取决于配置。
- en: Tip
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Inside a single Spark application, multiple parallel jobs can run simultaneously
    if they were submitted from separate threads.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个 Spark 应用程序中，如果多个任务是从不同的线程提交的，它们可以并行执行。
- en: The Spark shell
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark Shell
- en: The Spark shell is none other than the interface provided by Scala and Python.
    It looks very similar to any other interactive shell. It has a SparkContext object
    (created by default for you) that lets you leverage the distributed cluster. An
    interactive shell is quite useful for exploratory or ad hoc analysis. You can
    develop your complex scripts step by step through the shell without going through
    the compile-build-execute cycle.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Spark shell 其实就是由 Scala 和 Python 提供的接口。它看起来与其他任何交互式 shell 非常相似。它有一个 SparkContext
    对象（默认为你创建）让你能够利用分布式集群。交互式 shell 对于探索性或临时分析非常有用。你可以通过 shell 一步步地开发复杂的脚本，而不需要经历编译-构建-执行的循环过程。
- en: SparkContext
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SparkContext
- en: SparkContext is the entry point to the Spark core engine. This object is required
    to create and manipulate RDDs and create shared variables on a cluster. The SparkContext
    object connects to a cluster manager, which is responsible for resource allocation.
    Spark comes with its own standalone cluster manager. Since the cluster manager
    is a pluggable component in Spark, it can be managed through external cluster
    managers such as Apache Mesos or YARN.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: SparkContext 是进入 Spark 核心引擎的入口点。此对象用于创建和操作 RDD，以及在集群上创建共享变量。SparkContext 对象连接到集群管理器，集群管理器负责资源分配。Spark
    自带独立的集群管理器。由于集群管理器是 Spark 中的可插拔组件，因此可以通过外部集群管理器（如 Apache Mesos 或 YARN）来管理它。
- en: When you start a Spark shell, a SparkContext object is created by default for
    you. You can also create it by passing a SparkConf object that is used to set
    various Spark configuration parameters as key value pairs. Please note that there
    can be only one SparkContext object in a JVM.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当你启动 Spark shell 时，默认会为你创建一个 SparkContext 对象。你也可以通过传递一个 SparkConf 对象来创建它，该对象用于设置各种
    Spark 配置参数，以键值对的形式传递。请注意，JVM 中只能有一个 SparkContext 对象。
- en: Worker nodes
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作节点
- en: Worker nodes are the nodes that run the application code in a cluster, obeying
    the driver program. The real work is actually executed by the worker nodes. Each
    machine in the cluster may have one or more worker instances (default one). A
    worker node executes one or more executors that belong to one or more Spark applications.
    It consists of a *block manager* component, which is responsible for managing
    data blocks. The blocks can be cached RDD data, intermediate shuffled data, or
    broadcast data. When the available RAM is not sufficient, it automatically moves
    some data blocks to disk. Data replication across nodes is another responsibility
    of block manager.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点是集群中运行应用程序代码的节点，听从驱动程序的指令。实际的工作是由工作节点执行的。集群中的每台机器可能有一个或多个工作实例（默认为一个）。一个工作节点执行一个或多个属于一个或多个
    Spark 应用程序的执行进程。它由一个 *块管理器* 组件组成，负责管理数据块。数据块可以是缓存的 RDD 数据、中间的洗牌数据或广播数据。当可用的 RAM
    不足时，它会自动将一些数据块移到磁盘。数据在节点之间的复制是块管理器的另一项职责。
- en: Executors
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行进程
- en: Each application has a set of executor processes. Executors reside on worker
    nodes and communicate directly with the driver once the connection is made by
    the cluster manager. All executors are managed by SparkContext. An executor is
    a single JVM instance that serves a single Spark application. An executor is responsible
    for managing computation through tasks, storage, and caching on each worker node.
    It can run multiple tasks concurrently.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个应用程序都有一组执行进程。执行进程驻留在工作节点上，并在集群管理器建立连接后与驱动程序直接通信。所有执行进程都由 SparkContext 管理。执行进程是一个独立的
    JVM 实例，服务于一个 Spark 应用程序。执行进程负责通过任务在每个工作节点上管理计算、存储和缓存。它可以并行运行多个任务。
- en: Shared variables
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享变量
- en: Normally, the code is shipped to partitions along with separate copies of variables.
    These variables cannot be used to propagate results (for example, intermediate
    work counts) back to the driver program. Shared variables are used for this purpose.
    There are two kinds of shared variables, **broadcast variables** and **accumulators**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，代码会随着变量的独立副本一起发送到各个分区。这些变量不能用于将结果（例如中间工作计数）传递回驱动程序。共享变量用于这个目的。有两种共享变量，**广播变量**
    和 **累加器**。
- en: Broadcast variables enable the programmers to retain a read-only copy cached
    on each node rather than shipping a copy of it with tasks. If large, read-only
    data is used in multiple operations, it can be designated as broadcast variables
    and shipped only once to all worker nodes. The data broadcast in this way is cached
    in serialized form and is deserialized before running each task. Subsequent operations
    can access these variables along with the local variables moved along with the
    code. Creating broadcast variables is not necessary in all cases, except the ones
    where tasks across multiple stages need the same read-only copy of the data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量使程序员能够在每个节点上保留只读副本，而不是将其与任务一起传输。如果在多个操作中使用大型的只读数据，可以将其指定为广播变量，并只将其传输一次到所有工作节点。以这种方式广播的数据是以序列化形式缓存的，在运行每个任务之前会被反序列化。后续操作可以访问这些变量以及与代码一起传输的本地变量。并非所有情况都需要创建广播变量，只有在跨多个阶段的任务需要相同的只读数据副本时，才需要使用广播变量。
- en: Accumulators are variables that are always incremented, such as counters or
    cumulative sums. Spark natively supports accumulators of numeric types, but allows
    programmers to add support for new types. Please note that the worker nodes cannot
    read the value of accumulators; they can only modify their values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 累加器是始终递增的变量，例如计数器或累计和。Spark 本地支持数值类型的累加器，但允许程序员为新类型添加支持。请注意，工作节点不能读取累加器的值；它们只能修改其值。
- en: Flow of execution
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行流程
- en: A Spark application consists of a set of processes with one *driver* program
    and multiple *worker* (*executor*) programs. The driver program contains the application's
    *main* function and a SparkContext object, which represents a connection to the
    Spark cluster. Coordination between driver and the other processes happens through
    the SparkContext object.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Spark 应用程序由一组进程组成，其中有一个*驱动程序*程序和多个*工作程序*（*执行器*）程序。驱动程序包含应用程序的*main*函数和一个
    SparkContext 对象，后者表示与 Spark 集群的连接。驱动程序和其他进程之间的协调是通过 SparkContext 对象完成的。
- en: 'A typical Spark client program performs the following steps:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的 Spark 客户端程序执行以下步骤：
- en: When a program is run on a Spark shell, it is called the driver program with
    the user's `main` method in it. It gets executed in the JVM of the system where
    you are running the driver program.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当程序在 Spark shell 上运行时，它被称为驱动程序，包含用户的`main`方法。它在运行驱动程序的系统的 JVM 中执行。
- en: The first step is to create a SparkContext object with the required configuration
    parameters. When you run the PySpark or Spark shell, it is instantiated by default,
    but for other applications, you have to create it explicitly. SparkContext is
    actually the gateway to Spark.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是使用所需的配置参数创建一个 SparkContext 对象。当你运行 PySpark 或 Spark shell 时，它会默认实例化，但对于其他应用程序，你需要显式地创建它。SparkContext
    实际上是访问 Spark 的入口。
- en: The next step is to define one or more RDDs, either by loading a file or programmatically
    by passing an array of items, referred to parallel collection
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义一个或多个 RDD，方法是加载文件或通过传递项的数组（称为并行集合）以编程方式定义。
- en: Then more RDDs can be defined by a sequence of transformations, which are tracked
    and managed by a **lineage graph**. These RDD transformations may be viewed as
    piped UNIX commands where the output of one command becomes the input to the next
    command and so on. Each resulting RDD of a *transformation* step has a pointer
    to its parent RDD and also has a function for calculating its data. The RDD is
    acted on only after encountering an *action* statement. So, the *transformations*
    are lazy operations used to define new RDDs and *actions* launch a computation
    to return a value to the program or write data to external storage. We will discuss
    this aspect a little more in the following sections.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后可以通过一系列转换定义更多的 RDD，这些转换通过**祖先图**进行追踪和管理。这些 RDD 转换可以看作是 UNIX 命令管道，其中一个命令的输出作为下一个命令的输入，以此类推。每个*转换*步骤的结果
    RDD 都有指向其父 RDD 的指针，并且有一个用于计算其数据的函数。RDD 只有在遇到*动作*语句后才会被操作。所以，*转换*是懒操作，用于定义新的 RDD，而*动作*启动计算并返回值给程序或将数据写入外部存储。我们将在以下部分进一步讨论这一方面。
- en: At this stage, Spark creates an execution graph where nodes represent the RDDs
    and edges represent the transformation steps. Spark breaks the job into multiple
    tasks to run on separate machines. This is how Spark sends the **compute** to
    the data across the nodes in a cluster, rather than getting all the data together
    and computing it.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个阶段，Spark 创建一个执行图，其中节点表示 RDD，边表示转换步骤。Spark 将作业分解为多个任务，在不同的机器上运行。这是 Spark 如何将计算发送到集群中的节点上的一种方法，而不是将所有数据一起获取并进行计算。
- en: The RDD API
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD API
- en: The RDD is a read-only, partitioned, fault-tolerant collection of records. From
    a design perspective, there was a need for a single data structure abstraction
    that hides the complexity of dealing with a wide variety of data sources, be it
    HDFS, filesystems, RDBMS, NOSQL data structures, or any other data source. The
    user should be able to define the RDD from any of these sources. The goal was
    to support a wide array of operations and let users compose them in any order.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 是一个只读、分区的、容错的记录集合。从设计的角度来看，需要一个单一的数据结构抽象，隐藏处理各种数据源（如 HDFS、文件系统、RDBMS、NOSQL
    数据结构或任何其他数据源）的复杂性。用户应能够从这些源中定义 RDD。目标是支持广泛的操作，并允许用户以任何顺序组合它们。
- en: RDD basics
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD 基础知识
- en: Each dataset is represented as an object in Spark's programming interface called
    RDD. Spark provides two ways for creating RDDs. One way is to parallelize an existing
    collection. The other way is to reference a dataset in an external storage system
    such as a filesystem.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集在 Spark 编程接口中表示为一个名为 RDD 的对象。Spark 提供了两种创建 RDD 的方式。一种方式是并行化现有集合。另一种方式是引用外部存储系统（如文件系统）中的数据集。
- en: An RDD is composed of one or more data sources, maybe after performing a series
    of transformations including several operators. Every RDD or RDD partition knows
    how to recreate itself in case of failure. It has the log of transformations,
    or a *lineage* that is required to recreate itself from stable storage or another
    RDD. Thus, any program using Spark can be assured of built-in fault tolerance,
    regardless of the underlying data source and the type of RDD.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 由一个或多个数据源组成，可能在执行一系列包括多个运算符的转换后。每个 RDD 或 RDD 分区都知道如何在失败时重新创建自己。它具有转换的日志或*血统*，从稳定存储或另一个
    RDD 中重新创建自己所需。因此，使用 Spark 的任何程序都可以确保具有内置的容错性，无论底层数据源和 RDD 的类型如何。
- en: 'There are two kinds of methods available on RDDs: transformations, and actions.
    Transformations are the methods that are used to create RDDs. Actions are the
    methods that utilize RDDs. RDDs are usually partitioned. Users may choose to persist
    RDDs that may be reused in their programs.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 上有两种方法可用：转换和操作。转换是用于创建 RDD 的方法。操作是利用 RDD 的方法。RDD 通常被分区。用户可以选择持久化 RDD，以便在其程序中重复使用。
- en: RDDs are immutable (read-only) data structures, so any transformation results
    in the creation of a new RDD. The transformations are applied lazily, only when
    any action is applied on them, and not when an RDD is defined. An RDD is recomputed
    every time it is used in an action unless the user explicitly persists the RDD
    in memory. Saving in memory saves a lot of time. If the memory is not sufficient
    to accommodate the RDD fully, the remaining portion of that RDD will be stored
    (spilled) on the hard disk automatically. One advantage of lazy transformations
    is that it is possible to optimize the transformation steps. For example, if the
    action is to return the first line, Spark computes only a single partition and
    skips the rest.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs 是不可变（只读）数据结构，因此任何转换都会导致新的 RDD 的创建。这些转换是惰性应用的，只有在应用任何操作时才会应用它们，而不是在定义 RDD
    时。每次在操作中使用 RDD 时，RDD 都会重新计算，除非用户显式将 RDD 持久化到内存中。将数据保存在内存中可以节省大量时间。如果内存不足以完全容纳
    RDD，则该 RDD 的剩余部分将自动存储（溢写）到硬盘上。惰性转换的一个优点是可以优化转换步骤。例如，如果操作是返回第一行，则 Spark 只计算单个分区并跳过其余部分。
- en: An RDD may be viewed as a set of partitions (splits) with a list of dependencies
    on parent RDDs and a function to compute a partition given its parents. Sometimes,
    each partition of a parent RDD is used by a single child RDD. This is called *narrow
    dependency*. Narrow dependency is desirable because when a parent RDD partition
    is lost, only a single child partition needs to be recomputed. On the other hand,
    computing a single child RDD partition that involves operations such as *group-by-keys*
    depends on several parent RDD partitions. Data from each parent RDD partition
    in turn is required in creating data in several child RDD partitions. Such a dependency
    is called *wide dependency*. In the case of narrow dependency, it is possible
    to keep both parent and child RDD partitions on a single node (co-partition).
    But this is not possible in the case of wide dependency because parent data is
    scattered across several partitions. In such cases, data should be *shuffled*
    across partitions. Data shuffling is a resource-intensive operation that should
    be avoided to the extent possible. Another issue with wide dependency is that
    all child RDD partitions need to be recomputed even when a single parent RDD partition
    is lost.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 RDD 可以视为一组分区（拆分），它具有一份父 RDD 的依赖列表，以及一个给定父 RDD 的函数，用于计算该分区。有时，每个父 RDD 的分区只会被一个子
    RDD 使用。这叫做 *窄依赖*。窄依赖是理想的，因为当父 RDD 分区丢失时，只需要重新计算单个子分区。另一方面，计算一个包含诸如 *按键分组* 等操作的单个子
    RDD 分区时，依赖于多个父 RDD 分区。每个父 RDD 分区中的数据在生成多个子 RDD 分区数据时都需要用到。这样的依赖叫做 *宽依赖*。在窄依赖的情况下，父
    RDD 和子 RDD 分区可以保持在同一个节点上（共同分区）。但在宽依赖的情况下，由于父数据分散在多个分区中，这是不可能的。在这种情况下，数据应该在分区间进行
    *洗牌*。数据洗牌是一个资源密集型操作，应尽可能避免。宽依赖的另一个问题是，即使只有一个父 RDD 分区丢失，所有子 RDD 分区也需要重新计算。
- en: Persistence
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久化
- en: RDDs are computed on the fly every time they are acted upon through an action
    method. The developer has the ability to override this default behavior and instruct
    to *persist* or *cache* a dataset in memory across partitions. If this dataset
    is required to participate in several actions, then persisting saves a significant
    amount of time, CPU cycles, disk I/O, and network bandwidth. The fault-tolerance
    mechanism is applicable to the cached partitions too. When any partition is lost
    due to node failure, it is recomputed using a lineage graph. If the available
    memory is insufficient, Spark gracefully spills the persisted partitions on to
    the disk. The developer may remove unwanted RDDs using *unpersist*. Nevertheless,
    Spark automatically monitors the cache and removes old partitions using **Least
    Recently Used** (**LRU**) algorithms.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 每当 RDD 被操作时，它都会即时计算。开发者可以覆盖这个默认行为，指示要在分区之间 *持久化* 或 *缓存* 数据集。如果该数据集需要参与多个操作，那么持久化可以节省大量时间、CPU
    周期、磁盘 I/O 和网络带宽。容错机制同样适用于缓存的分区。当任何分区由于节点故障丢失时，会通过谱系图重新计算该分区。如果可用内存不足，Spark 会优雅地将持久化的分区溢出到磁盘。开发者可以使用
    *unpersist* 移除不需要的 RDD。然而，Spark 会自动监控缓存，并使用 **最近最少使用**（**LRU**）算法移除旧的分区。
- en: Tip
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '`Cache()` is the same as `persist()` or `persist (MEMORY_ONLY)`. While the
    `persist()` method can have many other arguments for different levels of persistence,
    such as only memory, memory and disk, only disk, and so on, the `cache()` method
    is designed only for persistence in the memory.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cache()` 与 `persist()` 或 `persist (MEMORY_ONLY)` 相同。虽然 `persist()` 方法可以接受许多其他参数来指定不同的持久化级别，例如仅内存、内存和磁盘、仅磁盘等，但
    `cache()` 方法仅用于内存中的持久化。'
- en: RDD operations
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD 操作
- en: Spark programming usually starts by choosing a suitable interface that you are
    comfortable with. If you intend to do interactive data analysis, then a shell
    prompt would be the obvious choice. However, choosing a Python shell (PySpark)
    or Scala shell (Spark-Shell) depends on your proficiency with these languages
    to some extent. If you are building a full-blown scalable application then proficiency
    matters a great deal, so you should develop the application in your language of
    choice between Scala, Java, and Python, and submit it to Spark. We will discuss
    this aspect in more detail later in the book.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 编程通常从选择一个你熟悉的界面开始。如果你打算进行交互式数据分析，那么 shell 提示符显然是一个不错的选择。然而，选择 Python shell（PySpark）或
    Scala shell（Spark-Shell）在一定程度上取决于你对这些语言的熟练程度。如果你正在构建一个完整的可扩展应用程序，那么熟练度就显得尤为重要，因此你应该根据自己擅长的语言（Scala、Java
    或 Python）来开发应用，并提交到 Spark。我们将在本书后面详细讨论这一方面。
- en: Creating RDDs
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 RDD
- en: In this section, we will use both a Python shell (PySpark) and a Scala shell
    (Spark-Shell) to create an RDD. Both of these shells have a predefined, interpreter-aware
    SparkContext that is assigned to a variable `sc`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Python shell（PySpark）和 Scala shell（Spark-Shell）来创建 RDD。两个 shell 都具有预定义的、能够感知解释器的
    SparkContext，并将其分配给变量 `sc`。
- en: 'Let us get started with some simple code examples. Note that the code assumes
    the current working directory is Spark''s home directory. The following code snippet
    initiates the Spark interactive shell, reads a file from the local filesystem,
    and prints the first line from that file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些简单的代码示例开始。请注意，代码假定当前工作目录是 Spark 的主目录。以下代码片段启动 Spark 交互式 shell，从本地文件系统读取文件，并打印该文件的第一行：
- en: '**Python**:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Scala**:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In both the preceding examples, the first line has invoked the interactive shell.
    The SparkContext variable `sc` is already defined as expected. We have created
    an RDD by the name `fileRDD` that points to a file `RELEASE`. This statement is
    just a transformation and will not be executed until an action is encountered.
    You can try giving a nonexistent filename but you will not get any error until
    you execute the next statement, which happens to be an *action* statement.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，第一行已经调用了交互式 shell。SparkContext 变量 `sc` 已按预期定义。我们创建了一个名为 `fileRDD` 的
    RDD，指向文件 `RELEASE`。此语句只是一个转换，直到遇到操作时才会执行。你可以尝试给一个不存在的文件名，但在执行下一个语句之前不会报错，而该语句恰好是一个
    *action* 操作。
- en: We have completed the whole cycle of initiating a Spark application (shell),
    creating an RDD, and consuming it. Since RDDs are recomputed every time an action
    is executed, `fileRDD` is not persisted in the memory or hard disk. This allows
    Spark to optimize the sequence of steps and execute intelligently. In fact, in
    the previous example, the optimizer would have just read one partition of the
    input file because `first()` does not require a complete file scan.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了启动 Spark 应用程序（shell）、创建 RDD 并消费它的整个过程。由于 RDD 每次执行操作时都会重新计算，`fileRDD`
    并不会持久化到内存或硬盘中。这使得 Spark 可以优化步骤顺序并智能地执行。事实上，在之前的示例中，优化器只会读取输入文件的一个分区，因为 `first()`
    不需要进行完整的文件扫描。
- en: 'Recall that there are two ways to create an RDD: one way is to create a pointer
    to a data source and the other is to parallelize an existing collection. The previous
    examples covered one way, by loading a file from a storage system. We will now
    see the second way, which is parallelizing an existing collection. RDD creation
    by passing in-memory collections is simple but may not work very well for large
    collections, because the input collection should fit completely in the driver
    node''s memory.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，创建 RDD 有两种方式：一种是创建指向数据源的指针，另一种是并行化现有的集合。之前的示例展示了第一种方式，即从存储系统加载文件。接下来我们将看到第二种方式，即并行化现有的集合。通过传递内存中的集合来创建
    RDD 非常简单，但对于大型集合可能效果不佳，因为输入集合必须完全适应驱动节点的内存。
- en: 'The following example creates an RDD by passing a Python/Scala list with the
    `parallelize` function:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例通过使用 `parallelize` 函数将 Python/Scala 列表传递来创建 RDD：
- en: '**Python**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: A lambda function is an unnamed function, typically used as function arguments
    to other functions. A Python lambda function can be a single expression only.
    If your logic requires multiple steps, create a separate function and use it in
    the lambda expression.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: lambda 函数是一个没有名称的函数，通常作为其他函数的参数传递给函数。Python 中的 lambda 函数只能是一个单一的表达式。如果你的逻辑需要多个步骤，可以创建一个独立的函数，并在
    lambda 表达式中使用它。
- en: '**Scala**:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we saw in the previous example, we were able to pass a Scala/Python collection
    to create an RDD and we also had the liberty to specify the number of partitions
    to cut those collections into. Spark runs one task for each partition of the cluster,
    so it has to be carefully decided to optimize the computation effort. Though Spark
    sets the number of partitions automatically based on the cluster, we have the
    liberty to set it manually by passing it as a second argument to the `parallelize`
    function (for example, `sc.parallelize(data, 3)`). The following is a diagrammatic
    representation of an RDD which is created with a dataset with, say, 14 records
    (or tuples) and is partitioned into 3, distributed across 3 nodes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的示例中所看到的，我们能够传递一个 Scala/Python 集合来创建一个 RDD，同时我们也可以自由指定将这些集合切分成多少个分区。Spark
    为集群中的每个分区运行一个任务，因此必须仔细决定如何优化计算工作量。虽然 Spark 会根据集群自动设置分区数，但我们可以通过将其作为第二个参数传递给 `parallelize`
    函数来手动设置分区数（例如，`sc.parallelize(data, 3)`）。以下是一个 RDD 的示意图，该 RDD 是通过一个包含 14 条记录（或元组）的数据集创建的，并被分成
    3 个分区，分布在 3 个节点上：
- en: '![Creating RDDs](img/1-1.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![创建 RDD](img/1-1.jpg)'
- en: 'Writing a Spark program usually consists of transformations and actions. Transformations
    are lazy operations that define how to build an RDD. Most of the transformations
    accept a single function argument. All these methods convert one data source to
    another. Every time you perform a transformation on any RDD, a new RDD will be
    generated, even if it is a small change as shown in the following diagram:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 编写 Spark 程序通常包括转换和操作。转换是延迟操作，用于定义如何构建 RDD。大多数转换接受一个函数作为参数。所有这些方法将一种数据源转换为另一种数据源。每次对任何
    RDD 执行转换时，即使只是一个小的改变，也会生成一个新的 RDD，如下图所示：
- en: '![Creating RDDs](img/image_02_003.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![创建 RDD](img/image_02_003.jpg)'
- en: This is because the RDDs are immutable (read-only) abstractions by design. The
    resulting output from an action can either be written back to the storage system
    or it can be returned to the driver program for local computation if needed to
    produce the final output.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为 RDD 是不可变（只读）抽象设计的。操作的结果可以写回存储系统，或者在需要生成最终输出时返回给驱动程序进行本地计算。
- en: So far, we have seen some simple transformations that define RDDs and some actions
    to process them and generate some output. Let us go on a quick tour of some handy
    transformations and actions followed by transformations on pair RDDs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了一些简单的转换操作，它们定义了 RDD 以及一些操作来处理它们并生成输出。接下来，让我们快速浏览一些常用的转换和操作，随后再看看对成对
    RDD 的转换操作。
- en: Transformations on normal RDDs
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 普通 RDD 的转换操作
- en: The Spark API includes a rich set of transformation operators, and developers
    can compose them in arbitrary ways. Try out the following examples on the interactive
    shell to gain a better understanding of these operations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Spark API 包含一套丰富的转换操作符，开发人员可以将它们以任意方式组合。请在交互式 shell 中尝试以下示例，以更好地理解这些操作。
- en: The filter operation
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: filter 操作
- en: The `filter` operation returns an RDD with only those elements that satisfy
    a `filter` condition, similar to the `WHERE` condition in SQL.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter` 操作返回一个 RDD，其中只包含满足 `filter` 条件的元素，类似于 SQL 中的 `WHERE` 条件。'
- en: '**Python**:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Scala**:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The distinct operation
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: distinct 操作
- en: 'The distinct (`[numTasks]`) operation returns an RDD with a new dataset after
    eliminating duplicates:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: distinct (`[numTasks]`) 操作返回一个经过去重的新数据集的 RDD：
- en: '**Python**:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Scala**:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The intersection operation
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交集操作
- en: 'The intersection operation takes another dataset as input. It returns a dataset
    that contains common elements:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 交集操作接受另一个数据集作为输入。它返回一个包含共同元素的数据集：
- en: '**Python**:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Scala**:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The union operation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: union 操作
- en: 'The union operation takes another dataset as input. It returns a dataset that
    contains elements of itself and the input dataset supplied to it. If there are
    common values in both sets, then they will appear as duplicate values in the resulting
    set after union:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: union 操作接受另一个数据集作为输入。它返回一个包含自身和输入数据集元素的数据集。如果两个集合中有共同的值，则它们会在联合后的结果集中作为重复值出现：
- en: '**Python**:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Scala**:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The map operation
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: map 操作
- en: 'The map operation returns a distributed dataset formed by executing an input
    function on each of the elements in the input dataset:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: map 操作通过在输入数据集的每个元素上执行一个输入函数，返回一个分布式数据集：
- en: '**Python**:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Scala**:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The flatMap operation
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: flatMap 操作
- en: 'The flatMap operation is similar to the `map` operation. While `map` returns
    one element per input element, `flatMap` returns a list of zero or more elements
    for each input element:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: flatMap 操作类似于 `map` 操作。虽然 `map` 对每个输入元素返回一个元素，`flatMap` 对每个输入元素返回一个零个或多个元素的列表：
- en: '**Python**:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Scala**:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The keys operation
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: keys 操作
- en: 'The keys operation returns an RDD with the key of each tuple:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: keys 操作返回一个 RDD，其中包含每个元组的键：
- en: '**Python**:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Scala**:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The cartesian operation
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 笛卡尔积操作
- en: 'The `cartesian` operation takes another dataset as argument and returns the
    Cartesian product of both datasets. This can be an expensive operation, returning
    a dataset of size `m` x `n` where `m` and `n` are the sizes of input datasets:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`cartesian` 操作接受另一个数据集作为参数，返回两个数据集的笛卡尔积。这是一个可能比较昂贵的操作，会返回一个大小为 `m` x `n` 的数据集，其中
    `m` 和 `n` 是输入数据集的大小：'
- en: '**Python**:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Scala**:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Transformations on pair RDDs
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对配对 RDD 的变换
- en: Some Spark operations are available only on RDDs of key value pairs. Note that
    most of these operations, except counting operations, usually involve shuffling,
    because the data related to a key may not always reside on a single partition.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 Spark 操作仅适用于键值对类型的 RDD。请注意，除了计数操作之外，这些操作通常涉及洗牌，因为与某个键相关的数据可能并不总是驻留在同一个分区中。
- en: The groupByKey operation
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: groupByKey 操作
- en: 'Similar to the SQL `groupBy` operation, this groups input data based on the
    key and you can use `aggregateKey` or `reduceByKey` to perform aggregate operations:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 SQL 中的 `groupBy` 操作，它根据键将输入数据分组，你可以使用 `aggregateKey` 或 `reduceByKey` 来执行聚合操作：
- en: '**Python**:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Scala**:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The join operation
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: join 操作
- en: 'The join operation takes another dataset as input. Both datasets should be
    of the key value pairs type. The resulting dataset is yet another key value dataset
    having keys and values from both datasets:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: join 操作接受另一个数据集作为输入。两个数据集应该是键值对类型。结果数据集是另一个键值对数据集，包含两个数据集的键和值：
- en: '**Python**:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE22]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Scala**:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The reduceByKey operation
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: reduceByKey 操作
- en: 'The reduceByKey operation merges the values for each key using an associative
    reduce function. This will also perform the merging locally on each mapper before
    sending results to a reducer and producing hash-partitioned output:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: reduceByKey 操作使用一个结合性减少函数来合并每个键的值。这也将在每个映射器上本地执行合并，然后将结果发送到减少器并生成哈希分区输出：
- en: '**Python**:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Scala**:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The aggregate operation
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚合操作
- en: 'The aggregrate operation returns an RDD with the keys of each tuple:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: aggregrate 操作返回一个 RDD，其中包含每个元组的键：
- en: '**Python**:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Scala**:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that in the preceding aggregate examples, the resultant strings (for example,
    `abcd`, `xxabxcd`, `53`, `01`) you get need not match the output shown here exactly.
    It depends on the order in which the individual tasks return their output.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的聚合示例中，结果字符串（例如，`abcd`，`xxabxcd`，`53`，`01`）不需要与这里显示的输出完全匹配。它取决于各个任务返回其输出的顺序。
- en: Actions
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Actions
- en: Once an RDD has been created, the various transformations get executed only
    when an *action* is performed on it. The result of an action can either be data
    written back to the storage system or returned to the driver program that initiated
    this for further computation locally to produce the final result.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了 RDD，各种变换只有在对其执行 *action* 操作时才会被执行。一个 action 操作的结果可以是写回存储系统的数据，或者返回给发起该操作的驱动程序，以便进一步在本地计算产生最终结果。
- en: We have already covered some of the action functions in the previous examples
    of transformations. The following are a few more, but there are a lot more that
    you have to explore.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在前面的变换示例中涵盖了一些 action 函数。下面是更多的示例，但还有很多你需要探索的。
- en: The collect() function
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: collect() 函数
- en: The `collect()` function returns all the results of an RDD operation as an array
    to the driver program. This is usually useful for operations that produce sufficiently
    small datasets. Ideally, the result should easily fit in the memory of the system
    that's hosting the driver program.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`collect()` 函数将 RDD 操作的所有结果作为数组返回给驱动程序。通常，针对生成足够小的数据集的操作，这个函数非常有用。理想情况下，结果应该能够轻松适应承载驱动程序的系统的内存。'
- en: The count() function
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: count() 函数
- en: This returns the number of elements in a dataset or the resulting output of
    an RDD operation.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 该操作返回数据集中的元素数量或 RDD 操作的结果输出。
- en: The take(n) function
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: take(n) 函数
- en: The `take(n)` function returns the first (`n`) elements of a dataset or the
    resulting output of an RDD operation.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`take(n)`函数返回数据集的前`n`个元素或RDD操作的结果输出。'
- en: The first() function
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: first()函数
- en: The `first()` function returns the first element of the dataset or the resulting
    output of an RDD operation. It works similarly to the `take(1)` function.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`first()`函数返回数据集的第一个元素或RDD操作的结果输出。它的工作方式与`take(1)`函数类似。'
- en: The takeSample() function
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: takeSample()函数
- en: 'The `takeSample(withReplacement, num, [seed])` function returns an array with
    a random sample of elements from a dataset. It has three arguments as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`takeSample(withReplacement, num, [seed])`函数返回一个包含数据集随机样本元素的数组。它有三个参数，如下所示：'
- en: '`withReplacement`/`withoutReplacement`: This indicates sampling with or without
    replacement (while taking multiple samples, it indicates whether to replace the
    old sample back to the set and then take a fresh sample or sample without replacing).
    For `withReplacement`, argument should be `True` and `False` otherwise.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`withReplacement`/`withoutReplacement`：表示是否进行有放回或无放回的抽样（在多次抽样时，表示是否将旧样本放回集合中再抽取新的样本，或者是直接不放回）。对于`withReplacement`，参数应为`True`，否则为`False`。'
- en: '`num`: This indicates the number of elements in the sample.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num`：表示样本中的元素数量。'
- en: '`Seed`: This is a random number generator seed (optional).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Seed`：这是一个随机数生成器种子（可选）。'
- en: The countByKey() function
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: countByKey()函数
- en: The `countByKey()` function is available only on RDDs of type key value. It
    returns a table of (`K`, `Int`) pairs with the count of each key.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`countByKey()`函数仅在键值类型的RDD上可用。它返回一个包含（`K`，`Int`）对的表，表中记录了每个键的计数。'
- en: 'The following are some example code snippets on Python and Scala:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些Python和Scala的示例代码：
- en: '**Python**:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Scala**:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Summary
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we touched upon the supported programming languages, their
    advantages and when to choose one language over the other. We discussed the design
    of the Spark engine along with its core components and their execution mechanism.
    We saw how Spark sends the data to be computed across many cluster nodes. We then
    discussed some RDD concepts. We learnt how to create RDDs and perform transformations
    and actions on them through both Scala and Python. We also discussed some advanced
    operations on RDDs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涉及了支持的编程语言、它们的优缺点以及何时选择一种语言而不是另一种语言。我们讨论了Spark引擎的设计以及它的核心组件和执行机制。我们了解了Spark如何将待处理的数据发送到多个集群节点。接着我们讨论了一些RDD概念，学习了如何通过Scala和Python创建RDD并对其进行转换和操作。我们还讨论了一些RDD的高级操作。
- en: In the next chapter, we will learn about DataFrames in detail and how they justify
    their suitability for all sorts of data science requirements.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将详细学习DataFrame及其如何证明适用于各种数据科学需求。
- en: References
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Scala language:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Scala语言：
- en: '[http://www.scala-lang.org](http://www.scala-lang.org)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.scala-lang.org](http://www.scala-lang.org)'
- en: 'Apache Spark architecture:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark架构：
- en: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
- en: 'The Spark programming guide is the primary resource for concepts; refer to
    the language-specific API documents for a complete list of operations available:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Spark编程指南是学习概念的主要资源；有关可用操作的完整列表，请参阅语言特定的API文档：
- en: '[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)'
- en: 'Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory
    Cluster Computing by Matei Zaharia and others is the original source for RDD basics:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory
    Cluster Computing》由Matei Zaharia等人编写，是RDD基础的原始来源：'
- en: '[https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf](https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf](https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf)'
- en: '[http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf](http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf](http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf)'
- en: 'Spark Summit, the official event series of Apache Spark, has a wealth of the
    latest information. Check out past events'' presentations and videos:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Summit，Apache Spark的官方活动系列，提供了丰富的最新信息。查看过去活动的演示文稿和视频：
- en: '[https://spark-summit.org/2016/](https://spark-summit.org/2016/)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark-summit.org/2016/](https://spark-summit.org/2016/)'
