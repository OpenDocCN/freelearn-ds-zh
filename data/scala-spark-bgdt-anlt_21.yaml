- en: Interactive Data Analytics with Apache Zeppelin
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Zeppelin进行交互式数据分析
- en: From a data science perspective, interactive visualization of your data analysis
    is also important. Apache Zeppelin, is a web-based notebook for interactive and
    large-scale data analytics with multiple backends and interpreters, such as Spark,
    Scala, Python, JDBC, Flink, Hive, Angular, Livy, Alluxio, PostgreSQL, Ignite,
    Lens, Cassandra, Kylin, Elasticsearch, JDBC, HBase, BigQuery, Pig, Markdown, Shell,
    and even more.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据科学的角度来看，交互式可视化数据分析也很重要。Apache Zeppelin是一个基于Web的笔记本，用于交互式和大规模数据分析，具有多个后端和解释器，如Spark、Scala、Python、JDBC、Flink、Hive、Angular、Livy、Alluxio、PostgreSQL、Ignite、Lens、Cassandra、Kylin、Elasticsearch、JDBC、HBase、BigQuery、Pig、Markdown、Shell等等。
- en: 'There is no doubt about Spark''s ability to handle large-scale datasets in
    a scalable and fast way. However, one thing in Spark is missing--there is no real-time
    or interactive visualization support with it. Considering the aforementioned exciting
    features of Zeppelin, in this chapter, we will discuss how to use Apache Zeppelin
    for large-scale data analytics using Spark as the interpreter in the backend.
    In summary, the following topics will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，Spark有能力以可扩展和快速的方式处理大规模数据集。但是，Spark中缺少一件事--它没有实时或交互式的可视化支持。考虑到Zeppelin的上述令人兴奋的功能，在本章中，我们将讨论如何使用Apache
    Zeppelin进行大规模数据分析，使用Spark作为后端的解释器。总之，将涵盖以下主题：
- en: Introduction to Apache Zeppelin
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Zeppelin简介
- en: Installation and getting started
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和入门
- en: Data ingestion
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄入
- en: Data analytics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析
- en: Data visualization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Data collaboration
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据协作
- en: Introduction to Apache Zeppelin
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Zeppelin简介
- en: Apache Zeppelin is a web-based notebook that enables you to do data analytics
    in an interactive way. Using Zeppelin, you can make beautiful data-driven, interactive,
    and collaborative documents with SQL, Scala, and more. The Apache Zeppelin interpreter
    concept allows any language/data-processing backend to be plugged into Zeppelin.
    Currently, Apache Zeppelin supports many interpreters, such as Apache Spark, Python,
    JDBC, Markdown, and Shell. Apache Zeppelin is a relatively new technology from
    the Apache Software Foundation, which enables data scientists, engineers, and
    practitioners to take the advantage of the data exploration, visualization, sharing,
    and collaboration features.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Zeppelin是一个基于Web的笔记本，可以让您以交互方式进行数据分析。使用Zeppelin，您可以使用SQL、Scala等制作美丽的数据驱动、交互式和协作文档。Apache
    Zeppelin解释器概念允许将任何语言/数据处理后端插入Zeppelin。目前，Apache Zeppelin支持许多解释器，如Apache Spark、Python、JDBC、Markdown和Shell。Apache
    Zeppelin是Apache软件基金会的一个相对较新的技术，它使数据科学家、工程师和从业者能够利用数据探索、可视化、共享和协作功能。
- en: Installation and getting started
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和入门
- en: 'Since using the other interpreters is not the goal of this book, but using
    Spark on Zeppelin is, all the code will be written using Scala. In this section,
    therefore, we will show how to configure Zeppelin using the binary package that
    contains only the Spark interpreter. Apache Zeppelin officially supports and is
    tested on the following environments:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用其他解释器不是本书的目标，而是在Zeppelin上使用Spark，所有代码都将使用Scala编写。因此，在本节中，我们将展示如何使用仅包含Spark解释器的二进制包配置Zeppelin。Apache
    Zeppelin官方支持并在以下环境上进行了测试：
- en: '| **Requirements** | **Value/Version** | **Other Requirements** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **要求** | **值/版本** | **其他要求** |'
- en: '| Oracle JDK | 1.7 or higher | Set `JAVA_HOME` |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| Oracle JDK | 1.7或更高版本 | 设置`JAVA_HOME` |'
- en: '| OS | macOS 10.X+ Ubuntu 14.X+'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '| 操作系统 | macOS 10.X+ Ubuntu 14.X+'
- en: CentOS 6.X+
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CentOS 6.X+
- en: Windows 7 Pro SP1+ | - |
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 7 Pro SP1+ | - |
- en: Installation and configuration
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和配置
- en: As shown in the preceding table, Java is required to execute Spark codes on
    Zeppelin. Therefore, if not set up, install and set up Java on any of the aforementioned
    platforms. Alternatively, you can refer to [Chapter 1](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduction to Scala*, to learn how to set up Java on your machine.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如前表所示，要在Zeppelin上执行Spark代码，需要Java。因此，如果尚未设置，请在上述任何平台上安装和设置Java。或者，您可以参考[第1章](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c)，*Scala简介*，了解如何在计算机上设置Java。
- en: 'The latest release of Apache Zeppelin can be downloaded from [https://zeppelin.apache.org/download.html](https://zeppelin.apache.org/download.html).
    Each release comes with three options:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从[https://zeppelin.apache.org/download.html](https://zeppelin.apache.org/download.html)下载最新版本的Apache
    Zeppelin。每个版本都有三个选项：
- en: '**Binary package with all the interpreters**: It contains support for many
    interpreters. For example, Spark, JDBC, Pig, Beam, Scio, BigQuery, Python, Livy,
    HDFS, Alluxio, Hbase, Scalding, Elasticsearch, Angular, Markdown, Shell, Flink,
    Hive, Tajo, Cassandra, Geode, Ignite, Kylin, Lens, Phoenix, and PostgreSQL are
    currently supported in Zeppelin.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**带有所有解释器的二进制包**：它包含对许多解释器的支持。例如，Spark、JDBC、Pig、Beam、Scio、BigQuery、Python、Livy、HDFS、Alluxio、Hbase、Scalding、Elasticsearch、Angular、Markdown、Shell、Flink、Hive、Tajo、Cassandra、Geode、Ignite、Kylin、Lens、Phoenix和PostgreSQL目前在Zeppelin中得到支持。'
- en: '**Binary package with the Spark interpreter**: It usually contains only the
    Spark interpreter. It also contains the interpreter net-install script.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**带有Spark解释器的二进制包**：通常只包含Spark解释器。它还包含解释器的网络安装脚本。'
- en: '**Source**: You can also build Zeppelin with all the latest changes from the
    GitHub repo (more to follow).'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**源代码**：您还可以从GitHub存储库构建Zeppelin（更多内容将在后续介绍）。'
- en: 'To show how to install and configure Zeppelin, we have downloaded the binary
    package from the following site mirror:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示如何安装和配置Zeppelin，我们从以下站点镜像下载了二进制包：
- en: '[http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz](http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz](http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz)'
- en: Once you have downloaded it, unzip it somewhere in your machine. Suppose that
    the path where you have unzipped the file is `/home/Zeppelin/`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下载后，在计算机上的某个位置解压缩它。假设您解压缩文件的路径是`/home/Zeppelin/`。
- en: Building from source
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从源代码构建
- en: 'You can also build Zeppelin with all the latest changes from the GitHub repo.
    If you want to build from source, you must first install the following tools:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以从GitHub存储库中构建所有最新更改的Zeppelin。如果要从源代码构建，必须首先安装以下工具：
- en: 'Git: any version'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Git：任何版本
- en: 'Maven: 3.1.x or higher'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maven：3.1.x或更高版本
- en: 'JDK: 1.7 or higher'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDK：1.7或更高版本
- en: 'npm: the latest version'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: npm：最新版本
- en: 'libfontconfig: the latest version'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: libfontconfig：最新版本
- en: 'If you haven''t installed Git and Maven yet, check the build requirement instructions
    from [http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements](http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements).
    However, due to page limitation, we have not discussed all the steps in detail.
    If you''re interested, you should refer to this URL for more details: [http://zeppelin.apache.org/docs/snapshot/install/build.html](http://zeppelin.apache.org/docs/snapshot/install/build.html).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未安装Git和Maven，请从[http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements](http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements)检查构建要求说明。但是，由于页面限制，我们没有详细讨论所有步骤。如果您感兴趣，可以参考此URL获取更多详细信息：[http://zeppelin.apache.org/docs/snapshot/install/build.html](http://zeppelin.apache.org/docs/snapshot/install/build.html)。
- en: Starting and stopping Apache Zeppelin
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动和停止Apache Zeppelin
- en: 'On all Unix-like platforms (for example, Ubuntu, macOS, and so on), use the
    following command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有类Unix平台（例如Ubuntu、macOS等）上，使用以下命令：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If the preceding command is successfully executed, you should observe the following
    logs on the terminal:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的命令成功执行，您应该在终端上看到以下日志：
- en: '![](img/00061.jpeg)**Figure 1**: Starting Zeppelin from the Ubuntu terminal'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00061.jpeg)**图1**：从Ubuntu终端启动Zeppelin'
- en: 'If you are on Windows, use the following command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用Windows，使用以下命令：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After Zeppelin has started successfully, go to `http://localhost:8080` with
    your web browser and you will see that Zeppelin is running. More specifically,
    you''ll see the following view on your browser:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Zeppelin成功启动后，使用您的网络浏览器转到`http://localhost:8080`，您将看到Zeppelin正在运行。更具体地说，您将在浏览器上看到以下视图：
- en: '![](img/00069.jpeg)**Figure 2**: Zeppelin is running on http://localhost:8080'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00069.jpeg)**图2**：Zeppelin正在http://localhost:8080上运行'
- en: Congratulations; you have successfully installed Apache Zeppelin! Now, let's
    move on to Zeppelin and get started with our data analytics once we have configured
    the preferred interpreter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功安装了Apache Zeppelin！现在，让我们继续使用Zeppelin，并在配置了首选解释器后开始我们的数据分析。
- en: 'Now, to stop Zeppelin from the command line, issue the following command:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要从命令行停止Zeppelin，请发出以下命令：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Creating notebooks
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建笔记本
- en: Once you are on `http://localhost:8080/`, you can explore different options
    and menus that help you understand how to get familiar with Zeppelin. You can
    find more on Zeppelin and its user-friendly UI at [https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html](https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html)
    (you can refer to the latest quick start documentation too, based on the available
    versions).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在`http://localhost:8080/`上，您可以探索不同的选项和菜单，以帮助您了解如何熟悉Zeppelin。您可以在[https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html](https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html)上找到更多关于Zeppelin及其用户友好的UI的信息（您也可以根据可用版本参考最新的快速入门文档）。
- en: 'Now, let''s first create a sample notebook and get started. As shown in the
    following figure, you can create a new notebook by clicking on the Create new
    note option:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们首先创建一个示例笔记本并开始。如下图所示，您可以通过单击“创建新笔记”选项来创建一个新的笔记本：
- en: '![](img/00082.jpeg)**Figure 3**: Creating a sample Zeppelin notebook'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00082.jpeg)**图3**：创建一个示例Zeppelin笔记本'
- en: As shown in the previous figure, the default interpreter is selected as Spark.
    In the drop-down list, you will also see only Spark, since we have download the
    Spark-only binary package for Zeppelin.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，默认解释器选择为Spark。在下拉列表中，您还将只看到Spark，因为我们已经为Zeppelin下载了仅包含Spark的二进制包。
- en: Configuring the interpreter
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置解释器
- en: Every interpreter belongs to an interpreter group. An interpreter group is a
    unit of start/stop interpreters. By default, every interpreter belongs to a single
    group, but the group might contain more interpreters. For example, the Spark interpreter
    group includes Spark support, pySpark, Spark SQL, and the dependency loader. If
    you want to execute an SQL statement on Zeppelin, you should specify the interpreter
    type using the `%` sign; for example, for using SQL, you should use `%sql`; for
    mark-down, use `%md`, and so on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 每个解释器都属于一个解释器组。解释器组是启动/停止解释器的单位。默认情况下，每个解释器属于一个单一组，但该组可能包含更多的解释器。例如，Spark解释器组包括Spark支持、pySpark、Spark
    SQL和依赖项加载器。如果您想在Zeppelin上执行SQL语句，应该使用`%`符号指定解释器类型；例如，要使用SQL，应该使用`%sql`；要使用标记，使用`%md`，依此类推。
- en: 'For more information, refer to the following image:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参考以下图片：
- en: '![](img/00086.jpeg)**Figure 4**: The interpreter properties for using Spark
    on Zeppelin Data ingestion'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00086.jpeg)**图4**：在Zeppelin上使用Spark的解释器属性数据摄入'
- en: 'Well, once you have created the notebook, you can start writing Spark code
    directly in the code section. For this simple example, we will use the bank dataset,
    which is publicly available for research and can be downloaded from [https://archive.ics.uci.edu/ml/machine-learning-databases/00222/](https://archive.ics.uci.edu/ml/machine-learning-databases/00222/),
    courtesy of S. Moro, R. Laureano, and P. Cortez, Using Data Mining for Bank Direct
    Marketing: An Application of the CRISP-DM Methodology. The dataset contains data
    such as age, job title, marital status, education, if s/he is a defaulter, bank
    balance, housing, if borrower loaned from the bank, and so on, about the customer
    of the bank in a CSV format. A sample of the dataset is given as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，一旦您创建了笔记本，就可以直接在代码部分编写Spark代码。对于这个简单的例子，我们将使用银行数据集，该数据集可供研究使用，并可从[https://archive.ics.uci.edu/ml/machine-learning-databases/00222/](https://archive.ics.uci.edu/ml/machine-learning-databases/00222/)下载，由S.
    Moro、R. Laureano和P. Cortez提供，使用数据挖掘进行银行直接营销：CRISP-DM方法的应用。数据集包含诸如年龄、职业头衔、婚姻状况、教育、是否为违约者、银行余额、住房、是否从银行借款等客户的信息，以CSV格式提供。以下是数据集的样本：
- en: '![](img/00094.jpeg)**Figure 5**: A sample of the bank dataset'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00094.jpeg)**图5**：银行数据集样本'
- en: 'Now, let''s first load the data on the Zeppelin notebook:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们首先在Zeppelin笔记本上加载数据：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Upon the execution of this line of code, create a new paragraph and name it
    as the data ingestion paragraph:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码行后，创建一个新的段落，并将其命名为数据摄入段落：
- en: '![](img/00098.jpeg)**Figure 6**: Data ingesting paragraph'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00098.jpeg)**图6**：数据摄入段落'
- en: If you see the preceding image carefully, the code worked and we did not need
    to define the Spark context. The reason is that it is already defined there as
    `sc`. You don't even need to define Scala implicitly. We will see an example of
    this later.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细观察前面的图像，代码已经运行，我们不需要定义Spark上下文。原因是它已经在那里定义为`sc`。甚至不需要隐式定义Scala。稍后我们将看到一个例子。
- en: Data processing and visualization
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理和可视化
- en: 'Now, let''s create a case class that will tell us how to pick the selected
    fields from the dataset:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个案例类，告诉我们如何从数据集中选择所需的字段：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, split each line, filter out the header (starts with `age`), and map it
    into the `Bank` case class, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将每行拆分，过滤掉标题（以`age`开头），并将其映射到`Bank`案例类中，如下所示：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, convert to DataFrame and create a temporal table:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，转换为DataFrame并创建临时表：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following screenshot shows that all the code snippets were executed successfully
    without showing any errors:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示所有代码片段都成功执行，没有显示任何错误：
- en: '![](img/00102.jpeg)**Figure 7**: Data process paragraph'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00102.jpeg)**图7**：数据处理段落'
- en: 'To make it more transparent, let''s see the status marked in green color (in
    the top-right corner of the image), as follows, after the code has been executed
    for each case:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更加透明，让我们在代码执行后查看标记为绿色的状态（在图像右上角），如下所示：
- en: '![](img/00114.jpeg)**Figure 8**: A successful execution of Spark code in each
    paragraph'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00114.jpeg)**图8**：每个段落中Spark代码的成功执行'
- en: 'Now let''s load some data to play with the following SQL command:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们加载一些数据，以便使用以下SQL命令进行操作：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that the preceding line of code is a pure SQL statement that selects the
    names of all the customers whose age is greater than or equal to 45 (that is,
    age distribution). Finally, it counts the number for the same customer group.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述代码行是一个纯SQL语句，用于选择年龄大于或等于45岁的所有客户的姓名（即年龄分布）。最后，它计算了同一客户组的数量。
- en: 'Now let''s see how the preceding SQL statement works on the temp view (that
    is, `bank`):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看前面的SQL语句在临时视图（即`bank`）上是如何工作的：
- en: '![](img/00126.jpeg)**Figure 9**: SQL query that selects the names of all the
    customers with age distribution [Tabular]'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00126.jpeg)**图9**：选择所有年龄分布的客户姓名的SQL查询[表格]'
- en: Now you can select graph options, such as histogram, pie-chart, bar chart, and
    so on, from the tab near the table icon (in the result section). For example,
    using histogram, you can see the corresponding count for `age group >=45`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以从结果部分附近的选项卡中选择图形选项，例如直方图、饼图、条形图等。例如，使用直方图，您可以看到`年龄组>=45`的相应计数。
- en: '![](img/00092.jpeg)**Figure 10**: SQL query that selects the names of all the
    customers with age distribution [Histogram]'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00092.jpeg)**图10**：选择所有年龄分布的客户姓名的SQL查询[直方图]'
- en: 'This is how it looks using a pie-chart:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用饼图的效果：
- en: '![](img/00328.jpeg)**Figure 11**: SQL query that selects the names all the
    customers with age distribution [pie-chart]'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00328.jpeg)**图11**：选择所有年龄分布的客户姓名的SQL查询[饼图]'
- en: Fantastic! We are now almost ready to do more complex data analytics problems
    using Zeppelin.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在我们几乎可以使用Zeppelin进行更复杂的数据分析问题了。
- en: Complex data analytics with Zeppelin
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Zeppelin进行复杂数据分析
- en: In this section, we will see how to perform more complex analytics using Zeppelin.
    At first, we will formalize the problem, and then, will explore the dataset that
    will be used. Finally, we will apply some visual analytics and machine learning
    techniques.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何使用Zeppelin执行更复杂的分析。首先，我们将明确问题，然后将探索将要使用的数据集。最后，我们将应用一些可视化分析和机器学习技术。
- en: The problem definition
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题定义
- en: 'In this section, we will build a spam classifier for classifying the raw text
    as spam or ham. We will also show how to evaluate such a model. We will try to
    focus using and working with the DataFrame API. In the end, the spam classifier
    model will help you distinguish between spam and ham messages. The following image
    shows a conceptual view of two messages (spam and ham respectively):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个垃圾邮件分类器，用于将原始文本分类为垃圾邮件或正常邮件。我们还将展示如何评估这样的模型。我们将尝试专注于使用和处理DataFrame
    API。最终，垃圾邮件分类器模型将帮助您区分垃圾邮件和正常邮件。以下图像显示了两条消息的概念视图（分别为垃圾邮件和正常邮件）：
- en: '[![](img/00333.jpeg)](https://blog.codecentric.de/files/2016/06/ham-vs-spam.png)**Figure
    12**: Spam and Ham example'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/00333.jpeg)](https://blog.codecentric.de/files/2016/06/ham-vs-spam.png)**图12**：垃圾邮件和正常邮件示例'
- en: We power some basic machine learning techniques to build and evaluate such a
    classifier for this kind of problem. In particular, the logistic regression algorithm
    will be used for this problem.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一些基本的机器学习技术来构建和评估这种类型问题的分类器。具体来说，逻辑回归算法将用于解决这个问题。
- en: Dataset descripting and exploration
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集描述和探索
- en: 'The spam data set that we downloaded from [https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)
    consists of 5,564 SMS, which have been classified by hand as either ham or spam.
    Only 13.4% of these SMSes are spam. This means that the dataset is skewed and
    provides only a few examples of spam. This is something to keep in mind, as it
    can introduce bias while training models:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)下载的垃圾数据集包含5,564条短信，已经被手动分类为正常或垃圾。这些短信中只有13.4%是垃圾短信。这意味着数据集存在偏斜，并且只提供了少量垃圾短信的示例。这是需要记住的一点，因为它可能在训练模型时引入偏差：
- en: '![](img/00336.jpeg)**Figure 13**: A snap of the SMS dataset'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00336.jpeg)**图13**：短信数据集的快照'
- en: So, what does this data look like? As you might have seen, social media text
    can really get dirty, containing slang words, misspelled words, missing whitespaces,
    abbreviated words, such as *u*, *urs*, *yrs*, and so on, and, often, a violation
    of grammar rules. It sometimes even contains trivial words in the messages. Thus,
    we need to take care of these issues as well. In the following steps, we will
    encounter these issues for a better interpretation of the analytics.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些数据是什么样子的呢？您可能已经看到，社交媒体文本可能会非常肮脏，包含俚语、拼写错误、缺少空格、缩写词，比如*u*、*urs*、*yrs*等等，通常违反语法规则。有时甚至包含消息中的琐碎词语。因此，我们需要处理这些问题。在接下来的步骤中，我们将遇到这些问题，以更好地解释分析结果。
- en: '**Step 1\. Load the required packages and APIs on Zeppelin** - Let''s load
    the required packages and APIs and create the first paragraph, before we ingest
    the dataset on Zeppelin:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步。在Zeppelin上加载所需的包和API - 让我们加载所需的包和API，并在Zeppelin上创建第一个段落，然后再将数据集导入：
- en: '![](img/00345.jpeg)**Figure 14**: Package/APIs load paragraph'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00345.jpeg)**图14**：包/ API加载段落'
- en: '**Step 2\. Load and parse the dataset** - We''ll use the CSV parsing library
    by Databricks (that is, `com.databricks.spark.csv`) to read the data into the
    DataFrame:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步。加载和解析数据集 - 我们将使用Databricks的CSV解析库（即`com.databricks.spark.csv`）将数据读入DataFrame：
- en: '![](img/00351.jpeg)**Figure 15**: Data ingesting/load paragraph'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00351.jpeg)**图15**：数据摄取/加载段落'
- en: '**Step 3\. Using** `StringIndexer` **to create numeric labels** - Since the
    labels in the original DataFrame are categorical, we will have to convert them
    back so that we can feed them to or use them in the machine learning models:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步。使用`StringIndexer`创建数字标签 - 由于原始DataFrame中的标签是分类的，我们需要将它们转换回来，以便在机器学习模型中使用：
- en: '![](img/00357.jpeg)**Figure 16**: The StringIndexer paragraph, and the output
    shows the raw labels, original texts, and corresponding labels.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00357.jpeg)**图16**：StringIndexer段落，输出显示原始标签、原始文本和相应标签。'
- en: '**Step 4\. Using** `RegexTokenizer` **to create a bag of words** - We''ll use
    `RegexTokenizer` to remove unwanted words and create a bag of words:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步。使用`RegexTokenizer`创建词袋 - 我们将使用`RegexTokenizer`去除不需要的单词并创建一个词袋：
- en: '![](img/00363.jpeg)**Figure 17**: The RegexTokenizer paragraph, and the output
    shows the raw labels, original texts, corresponding labels, and tokens'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00363.jpeg)**图17**：RegexTokenizer段落，输出显示原始标签、原始文本、相应标签和标记'
- en: '**Step 5\. Removing stop words and creating a filtered** **DataFrame** - We''ll
    remove stop words and create a filtered DataFrame for visual analytics. Finally,
    we show the DataFrame:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步。删除停用词并创建一个经过筛选的DataFrame - 我们将删除停用词并创建一个经过筛选的DataFrame以进行可视化分析。最后，我们展示DataFrame：
- en: '![](img/00329.jpeg)**Figure 18**: StopWordsRemover paragraph and the output
    shows the raw labels, original texts, corresponding labels, tokens, and filtered
    tokens without the stop words'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00329.jpeg)**图18**：StopWordsRemover段落，输出显示原始标签、原始文本、相应标签、标记和去除停用词的筛选标记'
- en: '**Step 6\. Finding spam messages/words and their frequency** - Let''s try to
    create a DataFrame containing only the spam words, along with their respective
    frequency, to understand the context of the messages in the dataset. We can create
    a paragraph on Zeppelin:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步。查找垃圾消息/单词及其频率 - 让我们尝试创建一个仅包含垃圾单词及其相应频率的DataFrame，以了解数据集中消息的上下文。我们可以在Zeppelin上创建一个段落：
- en: '![](img/00349.jpeg)**Figure 19**: Spam tokens with a frequency paragraph'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00349.jpeg)**图19**：带有频率段落的垃圾邮件标记'
- en: 'Now, let''s see them in the graph using SQL queries. The following query selects
    all the tokens with frequencies of more than 100\. Then, we sort the tokens in
    a descending order of their frequency. Finally, we use the dynamic forms to limit
    the number of records. The first one is just a raw tabular format:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过SQL查询在图表中查看它们。以下查询选择所有频率超过100的标记。然后，我们按照它们的频率降序排序标记。最后，我们使用动态表单来限制记录的数量。第一个是原始表格格式：
- en: '![](img/00128.jpeg)**Figure 20**: Spam tokens with a frequency visualization
    paragraph [Tabular]'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00128.jpeg)**图20**：带有频率可视化段落的垃圾邮件标记[表格]'
- en: 'Then, we''ll use a bar diagram, which provides more visual insights. We can
    now see that the most frequent words in the spam messages are call and free, with
    a frequency of 355 and 224 respectively:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用条形图，这提供了更多的视觉洞察。现在我们可以看到，垃圾短信中最频繁出现的单词是call和free，分别出现了355次和224次：
- en: '![](img/00096.jpeg)**Figure 21**: Spam tokens with a frequency visualization
    paragraph [Histogram]'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00096.jpeg)**图21**：带有频率可视化段落的垃圾邮件标记[直方图]'
- en: 'Finally, using the pie chart provides much better and wider visibility, especially
    if you specify the column range:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用饼图提供了更好更广泛的可见性，特别是如果您指定了列范围：
- en: '![](img/00145.jpeg)**Figure 22**: Spam tokens with a frequency visualization
    paragraph [Pie chart]'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00145.jpeg)**图22**：带有频率可视化段落的垃圾邮件标记[饼图]'
- en: '**Step 7\. Using HashingTF for term frequency** - Use `HashingTF` to generate
    the term frequency of each filtered token, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第7步。使用HashingTF进行词频- 使用`HashingTF`生成每个过滤标记的词频，如下所示：
- en: '![](img/00251.jpeg)**Figure 23**: HashingTF paragraph, and the output shows
    the raw labels, original texts, corresponding labels, tokens, filtered tokens,
    and corresponding term-frequency for each row'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图23：HashingTF段落，输出显示原始标签、原始文本、相应标签、标记、过滤后的标记和每行的相应词频
- en: '**Step 8\. Using IDF for Term frequency-inverse document frequency (TF-IDF)**
    - TF-IDF is a feature vectorization method widely used in text mining to reflect
    the importance of a term to a document in the corpus:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第8步。使用IDF进行词频-逆文档频率（TF-IDF）- TF-IDF是一种在文本挖掘中广泛使用的特征向量化方法，用于反映术语对语料库中文档的重要性：
- en: '![](img/00085.jpeg)**Figure 24**: IDF paragraph, and the output shows the raw
    labels, original texts, corresponding labels, tokens, filtered tokens, term-frequency,
    and the corresponding IDFs for each row'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：IDF段落，输出显示原始标签、原始文本、相应标签、标记、过滤后的标记、词频和每行的相应IDF
- en: '**Bag of words:** The bag of words assigns a value of `1` for every occurrence
    of a word in a sentence. This is probably not ideal, as each category of the sentence,
    most likely, has the same frequency of *the*, *and*, and other words; whereas
    words such as *viagra* and *sale* probably should have an increased importance
    in figuring out whether or not the text is spam.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋：词袋为句子中每个单词的出现赋予值`1`。这可能不是理想的，因为句子的每个类别很可能具有相同的*the*、*and*等词的频率；而*viagra*和*sale*等词可能在确定文本是否为垃圾邮件方面应该具有更高的重要性。
- en: '**TF-IDF:** This is the acronym for Text Frequency – Inverse Document Frequency.
    This term is essentially the product of text frequency and inverse document frequency
    for each word. This is commonly used in the bag of words methodology in NLP or
    text analytics.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF：这是文本频率-逆文档频率的缩写。这个术语本质上是每个词的文本频率和逆文档频率的乘积。这在NLP或文本分析中的词袋方法中常用。
- en: '**Using TF-IDF:** Let''s take a look at word frequency. Here, we consider the
    frequency of a word in an individual entry, that is, term. The purpose of calculating
    text frequency (TF) is to find terms that appear to be important in each entry.
    However, words such as *the* and *and* may appear very frequently in every entry.
    We want to downweigh the importance of these words, so we can imagine that multiplying
    the preceding TF by the inverse of the whole document frequency might help find
    important words. However, since a collection of texts (a corpus) may be quite
    large, it is common to take the logarithm of the inverse document frequency. In
    short, we can imagine that high values of TF-IDF might indicate words that are
    very important to determining what a document is about. Creating the TF-IDF vectors
    requires us to load all the text into memory and count the occurrences of each
    word before we can start training our model.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TF-IDF：让我们来看看词频。在这里，我们考虑单个条目中单词的频率，即术语。计算文本频率（TF）的目的是找到在每个条目中似乎重要的术语。然而，诸如“the”和“and”之类的词在每个条目中可能出现得非常频繁。我们希望降低这些词的重要性，因此我们可以想象将前述TF乘以整个文档频率的倒数可能有助于找到重要的词。然而，由于文本集合（语料库）可能相当大，通常会取逆文档频率的对数。简而言之，我们可以想象TF-IDF的高值可能表示对确定文档内容非常重要的词。创建TF-IDF向量需要我们将所有文本加载到内存中，并在开始训练模型之前计算每个单词的出现次数。
- en: '**Step 9\. Using VectorAssembler to generate raw features for the Spark ML
    pipeline** - As you saw in the previous step, we have only the filtered tokens,
    labels, TF, and IDF. However, there are no associated features that can be fed
    into any ML models. Thus, we need to use the Spark VectorAssembler API to create
    features based on the properties in the previous DataFrame, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步。使用VectorAssembler生成Spark ML管道的原始特征- 正如您在上一步中看到的，我们只有过滤后的标记、标签、TF和IDF。然而，没有任何可以输入任何ML模型的相关特征。因此，我们需要使用Spark
    VectorAssembler API根据前一个DataFrame中的属性创建特征，如下所示：
- en: '![](img/00101.jpeg)**Figure 25**: The VectorAssembler paragraph that shows
    using VectorAssembler for feature creations'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图25：VectorAssembler段落，显示使用VectorAssembler进行特征创建
- en: '**Step 10\. Preparing the training and test set** - Now we need to prepare
    the training and test set. The training set will be used to train the Logistic
    Regression model in Step 11*,* and the test set will be used to evaluate the model
    in Step 12\. Here, I make it 75% for the training and 25% for the test. You can
    adjust it accordingly:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第10步。准备训练和测试集- 现在我们需要准备训练和测试集。训练集将用于在第11步中训练逻辑回归模型，测试集将用于在第12步中评估模型。在这里，我将其设置为75%用于训练，25%用于测试。您可以根据需要进行调整：
- en: '![](img/00117.jpeg)**Figure 26**: Preparing training/test set paragraph'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图26：准备训练/测试集段落
- en: '**Step 11\. Training binary logistic regression model** - Since, the problem
    itself is a binary classification problem, we can use a binary logistic regression
    classifier, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 第11步。训练二元逻辑回归模型- 由于问题本身是一个二元分类问题，我们可以使用二元逻辑回归分类器，如下所示：
- en: '![](img/00133.jpeg)**Figure 27**: LogisticRegression paragraph that shows how
    to train the logistic regression classifier with the necessary labels, features,
    regression parameters, elastic net param, and maximum iterations'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图27：LogisticRegression段落，显示如何使用必要的标签、特征、回归参数、弹性网参数和最大迭代次数训练逻辑回归分类器
- en: Note that, here, for better results, we have iterated the training for 200 times.
    We have set the regression parameter and elastic net params a very low -i.e. 0.0001
    for making the training more intensive.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了获得更好的结果，我们已经迭代了200次的训练。我们已经将回归参数和弹性网参数设置得非常低-即0.0001，以使训练更加密集。
- en: '**Step 12\. Model evaluation** - Let''s compute the raw prediction for the
    test set. Then, we instantiate the raw prediction using the binary classifier
    evaluator, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤12. 模型评估** - 让我们计算测试集的原始预测。然后，我们使用二元分类器评估器来实例化原始预测，如下所示：'
- en: '**![](img/00335.jpeg)****Figure 28**: Model evaluator paragraph'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/00335.jpeg)****图28**: 模型评估段落'
- en: 'Now let''s compute the accuracy of the model for the test set, as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算模型在测试集上的准确性，如下所示：
- en: '![](img/00346.jpeg)**Figure 29**: Accuracy calculation paragraph'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00346.jpeg)**图29**: 准确性计算段落'
- en: 'This is pretty impressive. However, if you were to go with the model tuning
    using cross-validation, for example, you could gain even higher accuracy. Finally,
    we will compute the confusion matrix to get more insight:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当令人印象深刻。然而，如果您选择使用交叉验证进行模型调优，例如，您可能会获得更高的准确性。最后，我们将计算混淆矩阵以获得更多见解：
- en: '![](img/00350.jpeg)**Figure 30**: Confusion paragraph shows the number of correct
    and incorrect predictions summarized with count values and broken down by each
    class'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '图30: 混淆段落显示了正确和错误预测的数量，以计数值总结，并按每个类别进行了分解'
- en: Data and results collaborating
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据和结果协作
- en: 'Furthermore, Apache Zeppelin provides a feature for publishing your notebook
    paragraph results. Using this feature, you can show the Zeppelin notebook paragraph
    results on your own website. It''s very straightforward; just use the `<iframe>`
    tag on your page. If you want to share the link of your Zeppelin notebook, the
    first step to publish your paragraph result is Copy a paragraph link. After running
    a paragraph in your Zeppelin notebook, click the gear button located on the right-hand.
    Then, click Link this paragraph in the menu, as shown in the following image:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Apache Zeppelin提供了一个功能，用于发布您的笔记本段落结果。使用此功能，您可以在自己的网站上展示Zeppelin笔记本段落结果。非常简单；只需在您的页面上使用`<iframe>`标签。如果您想分享Zeppelin笔记本的链接，发布段落结果的第一步是复制段落链接。在Zeppelin笔记本中运行段落后，单击位于右侧的齿轮按钮。然后，在菜单中单击链接此段落，如下图所示：
- en: '![](img/00355.jpeg)**Figure 31**: Linking the paragraph'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00355.jpeg)**图31**: 链接段落'
- en: 'Then, just copy the provided link, as shown here:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，只需复制提供的链接，如下所示：
- en: '![](img/00358.jpeg)**Figure 32**: Getting the link for paragraph sharing with
    collaborators'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00358.jpeg)**图32**: 获取与协作者共享段落的链接'
- en: 'Now, even if you want to publish the copied paragraph, you may use the `<iframe>`
    tag on your website. Here is an example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，即使您想发布复制的段落，您也可以在您的网站上使用`<iframe>`标签。这是一个例子：
- en: '[PRE8]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, you can show off your beautiful visualization results on your website.
    This is more or less the end of our data analytics journey with Apache Zeppelin.
    For more inforamtion and related updates, you should visit the official website
    of Apache Zeppelin at [https://zeppelin.apache.org/](https://zeppelin.apache.org/);
    you can even subscribe to Zeppelin users at [users-subscribe@zeppelin.apache.org](mailto:users-subscribe@zeppelin.apache.org).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以在您的网站上展示您美丽的可视化结果。这更多或少是我们使用Apache Zeppelin进行数据分析旅程的结束。有关更多信息和相关更新，您应该访问Apache
    Zeppelin的官方网站[https://zeppelin.apache.org/](https://zeppelin.apache.org/)；您甚至可以订阅Zeppelin用户[users-subscribe@zeppelin.apache.org](mailto:users-subscribe@zeppelin.apache.org)。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Apache Zeppelin is a web-based notebook that enables you to do data analytics
    in an interactive way. Using Zeppelin, you can make beautiful data-driven, interactive,
    and collaborative documents with SQL, Scala, and more. It is gaining more popularity
    by the day, since more features are being added to recent releases. However, due
    to page limitations, and to make you more focused on using Spark only, we have
    shown examples that are only suitable for using Spark with Scala. However, you
    can write your Spark code in Python and test your notebook with similar ease.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Zeppelin是一个基于Web的笔记本，可以让您以交互方式进行数据分析。使用Zeppelin，您可以使用SQL、Scala等制作美丽的数据驱动、交互式和协作文档。它正在日益受到欢迎，因为最近的版本中添加了更多功能。然而，由于页面限制，并且为了让您更专注于仅使用Spark，我们展示了仅适用于使用Scala的Spark的示例。但是，您可以用Python编写您的Spark代码，并以类似的轻松方式测试您的笔记本。
- en: In this chapter, we discussed how to use Apache Zeppelin for large-scale data
    analytics using Spark in the backend as the interpreter. We saw how to install
    and get started with Zeppelin. We then saw how to ingest your data and parse and
    analyse it for better visibility. Then, we saw how to visualize it for better
    insights. Finally, we saw how to share the Zeppelin notebook with collaborators.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使用Apache Zeppelin进行后端使用Spark进行大规模数据分析。我们看到了如何安装和开始使用Zeppelin。然后，我们看到了如何摄取您的数据并解析和分析以获得更好的可见性。然后，我们看到了如何将其可视化以获得更好的见解。最后，我们看到了如何与协作者共享Zeppelin笔记本。
