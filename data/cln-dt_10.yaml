- en: Chapter 10. Twitter Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章. Twitter 项目
- en: 'As with the project we completed in [Chapter 9](part0059.xhtml#aid-1O8H61 "Chapter 9. Stack
    Overflow Project"), *Stack Overflow Project*, our next full-length chapter dinner
    party is designed to show off our data cleaning skills in particular, while still
    completing each stage of the overall data science process. Our previous project
    used Stack Overflow data, a combination of MySQL and PHP to clean it, and JavaScript
    D3 to visualize it. In this chapter, we will use Twitter data, MySQL and Python
    to collect and clean it, and JavaScript and D3 to visualize it. As with our previous
    project, in this project, the data science process is still the same:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在[第9章](part0059.xhtml#aid-1O8H61 "第9章. Stack Overflow 项目")完成的*Stack Overflow
    项目*一样，我们下一个完整章节的晚宴派对旨在特别展示我们的数据清理技能，同时仍完成整个数据科学过程的每个阶段。我们之前的项目使用了 Stack Overflow
    数据，结合 MySQL 和 PHP 进行清理，并使用 JavaScript D3 进行可视化。在本章中，我们将使用 Twitter 数据，MySQL 和 Python
    收集和清理数据，并使用 JavaScript 和 D3 进行可视化。与我们之前的项目一样，本项目中的数据科学过程仍然是相同的：
- en: Decide what kind of problem we are trying to solve — why do we need to examine
    this data?
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定我们要解决的问题类型——我们为什么需要检查这些数据？
- en: Collect and store our data, which consists of downloading and extracting a publicly-available
    set of tweet identification numbers, and then using a program to redownload the
    original tweets. This step also includes creating smaller sets of data for testing
    purposes.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集和存储数据，包括下载和提取一组公开的推文识别号码，然后使用程序重新下载原始推文。此步骤还包括为测试目的创建较小的数据集。
- en: Clean data by extracting and storing only the parts that we need. In this step,
    we write a quick Python program to load in the data, pull out the fields we want,
    and write those to a small set of database tables.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理数据，通过提取和存储我们需要的部分来完成。在这一步中，我们编写一个快速的 Python 程序来加载数据，提取我们想要的字段，并将它们写入一小组数据库表中。
- en: Analyze the data. Do we need to perform any calculations on the data? Should
    we write some aggregate functions to count or sum the data? Do we need to transform
    the data in some way?
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析数据。我们是否需要对数据进行任何计算？我们是否需要编写一些聚合函数来计数或求和数据？我们是否需要以某种方式转换数据？
- en: Provide visualizations of the data, if possible.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果可能，提供数据的可视化。
- en: Resolve the problem we set out to investigate. Did our process work? Were we
    successful?
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决我们要调查的问题。我们的过程是否有效？我们是否成功了？
- en: As with our previous project, the bulk of the work will be in the collection,
    storage, and cleaning tasks. In this project, we will notice that we store different
    versions of our data several times, first as a text file, then as a hydrated JSON
    file, then as a MySQL database. Each of these different formats results from a
    different collection or cleaning process and feeds into the next. In this way,
    we start to see how the collection, storage, and cleaning steps can be **iterative**
    — feeding back to one another — rather than just linear.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的项目一样，工作的大部分内容将集中在数据的收集、存储和清理任务上。在本项目中，我们会注意到我们将数据存储为不同的版本几次，首先是文本文件，然后是已填充的
    JSON 文件，接着是 MySQL 数据库。这些不同的格式是通过不同的收集或清理过程产生的，并将数据传递到下一个步骤。通过这种方式，我们开始看到收集、存储和清理步骤是如何**迭代**的——彼此反馈，而不仅仅是线性的。
- en: Step one – posing a question about an archive of tweets
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步——提出一个关于推文档案的问题
- en: Twitter is a popular **microblogging** platform used by millions of people around
    the world to share their thoughts or communicate about current events. Because
    of Twitter's relative ease of access for both posting and reading, especially
    from mobile devices, it has emerged as an important platform for sharing information
    during public events, such as political crises and protests, or to track the emergence
    of an issue in the public consciousness. Saved tweets become a sort of time capsule,
    providing a wealth of insight into public sentiment at the time of the event.
    Frozen in time, the tweets themselves are unaffected by memory lapses or subsequent
    reversals in public opinion. Scholars and media experts can collect and study
    these topical **tweet archives** to attempt to learn more about what the public
    opinion was at the time, or about how information traveled, or even about what
    happened in an event, when it happened, and why.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter是一个受欢迎的**微博**平台，全球数百万人使用它分享想法或讨论时事。由于Twitter在发布和阅读方面相对易于访问，尤其是通过移动设备，它已成为在公共事件（如政治危机和抗议）期间分享信息的重要平台，或用来追踪一个问题在公众意识中的出现。保存的推文成为一种时间胶囊，为我们提供了关于事件发生时公共情绪的丰富见解。定格在时间中的推文不会受到记忆丧失或公众舆论后续反转的影响。学者和媒体专家可以收集并研究这些专题**推文档案**，以试图了解当时的公众舆论，或者了解信息是如何传播的，甚至了解某个事件发生时发生了什么，何时发生，为什么发生。
- en: 'Many people have now started making their tweet collections public. Some of
    the tweet archives that are now available for public download include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在许多人已经开始公开他们的推文集合。一些现在可以公开下载的推文档案包括：
- en: Tweets that are in reference to protest and unrest following events in Ferguson,
    Missouri, USA, between August 10 and August 27, 2014\. These are available at
    [https://archive.org/details/ferguson-tweet-ids](https://archive.org/details/ferguson-tweet-ids).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2014年8月10日至8月27日期间，关于美国密苏里州费格森发生抗议和骚乱的推文。这些推文可以在[https://archive.org/details/ferguson-tweet-ids](https://archive.org/details/ferguson-tweet-ids)下载。
- en: Tweets from a number of countries during the Arab Spring events of 2011, including
    Libya, Bahrain, Egypt, and others. These are available at [http://dfreelon.org/2012/02/11/arab-spring-twitter-data-now-available-sort-of/](http://dfreelon.org/2012/02/11/arab-spring-twitter-data-now-available-sort-of/).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2011年阿拉伯之春期间来自多个国家的推文，包括利比亚、巴林、埃及等。这些推文可以在[http://dfreelon.org/2012/02/11/arab-spring-twitter-data-now-available-sort-of/](http://dfreelon.org/2012/02/11/arab-spring-twitter-data-now-available-sort-of/)下载。
- en: Tweets that mention **#YesAllWomen** hashtag and the related hashtag **#NotAllMen**,
    between May and June 2014\. These are available at [http://digital.library.unt.edu/ark:/67531/metadc304853/](http://digital.library.unt.edu/ark:/67531/metadc304853/).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提到**#YesAllWomen**标签和相关标签**#NotAllMen**的推文，时间为2014年5月至6月。这些推文可以在[http://digital.library.unt.edu/ark:/67531/metadc304853/](http://digital.library.unt.edu/ark:/67531/metadc304853/)下载。
- en: In the next step, we will need to choose one of these to download and begin
    to work with. Since the Ferguson tweets are the newest and the most complete of
    those three example data sets, I have designed the rest of this chapter around
    it. However, no matter what set you use, the concepts and basic procedures here
    will apply.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们需要从这些数据集中选择一个进行下载并开始使用。由于费格森推文是这三个示例数据集中最新且最完整的，因此我设计了本章的其余内容围绕它进行。不过，无论你使用哪个数据集，这里的概念和基本流程都适用。
- en: 'The basic question we want to ask in this chapter is quite simple: when people
    tweeted about Ferguson, which Internet domains did they tweet about most? Compared
    to our set of questions in [Chapter 9](part0059.xhtml#aid-1O8H61 "Chapter 9. Stack
    Overflow Project"), *Stack Overflow Project*, this is a very simple question,
    but the data collection process is somewhat different here, so a simple question
    is enough to motivate our data science dinner party.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中要提出的基本问题非常简单：当人们关于费格森发表推文时，他们最常提到的是哪些互联网域名？与我们在[第9章](part0059.xhtml#aid-1O8H61
    "第9章. Stack Overflow项目")中的问题集相比，*Stack Overflow项目*，这是一个非常简单的问题，但数据收集过程在这里略有不同，因此一个简单的问题足以激励我们的数据科学晚宴。
- en: Step two – collecting the data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二步 – 收集数据
- en: Unlike the small sentiment tweet archive that we studied in [Chapter 7](part0045.xhtml#aid-1AT9A1
    "Chapter 7. RDBMS Cleaning Techniques"), *RDBMS Cleaning Techniques*, the newer
    tweet archives, like those mentioned, no longer contain the actual text of the
    tweets themselves. Twitter's **Terms of Service** (**ToS**) have changed as of
    2014, and distributing other people's tweets is now a violation of this ToS. Instead,
    what you will find in a newer tweet archive is actually just the tweet identification
    (ID) numbers. Using these numbers, we will have to collect the actual tweets individually.
    At that point, we can store and analyze the tweets ourselves. Note that at any
    point during or after this process, we cannot redistribute the tweet text or their
    metadata, only the tweet identification numbers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在[第7章](part0045.xhtml#aid-1AT9A1 "第7章. RDBMS清理技术")中研究的小型情感推文档案不同，*RDBMS清理技术*，更新的推文档案，如上文提到的，已不再包含推文的实际文本。Twitter的**服务条款**（**ToS**）自2014年起发生了变化，分发他人的推文现在违反了这些条款。相反，在较新的推文档案中，你将找到的仅仅是推文的标识符（ID）号码。使用这些号码，我们必须单独收集实际的推文。到那时，我们可以自行存储和分析这些推文。请注意，在此过程的任何时刻或之后，我们不能重新分发推文的文本或其元数据，只能分发推文的标识符。
- en: Note
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Even though it is inconvenient for the researcher who wants to collect the tweets,
    the stated reason for Twitter's change to their ToS is to honor the notion of
    copyright by the original person who posted the tweet. This is especially important
    in terms of deleting a tweet. If a tweet has been copied and redistributed all
    around the Web and stored in data files by third parties, the tweet cannot really
    be deleted. By asking that researchers copy only the tweet ID, Twitter attempts
    to protect the user's ability to delete his or her own content. A request for
    a deleted tweet ID will return no result.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于想要收集推文的研究人员来说，这样做不太方便，但Twitter更改其服务条款（ToS）的原因是为了尊重最初发布推文的人的版权观念。尤其是在删除推文时非常重要。如果一条推文已经被复制并在网络上广泛传播，并且被第三方存储在数据文件中，那么推文实际上是无法被删除的。通过要求研究人员仅复制推文ID，Twitter试图保护用户删除自己内容的能力。如果请求已删除的推文ID，将不会返回任何结果。
- en: Download and extract the Ferguson file
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载并提取费格森文件
- en: The Ferguson-related tweets are available at the Internet Archive as a set of
    archived, compressed text files. Point your browser to [https://archive.org/details/ferguson-tweet-ids](https://archive.org/details/ferguson-tweet-ids)
    and download the 147 MB ZIP file.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与费格森相关的推文可以在互联网档案馆找到，作为一组归档的压缩文本文件。请将浏览器指向[https://archive.org/details/ferguson-tweet-ids](https://archive.org/details/ferguson-tweet-ids)并下载147
    MB的ZIP文件。
- en: 'We can extract the files as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式提取文件：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `ls` command shows that a directory is created called `ferguson-tweet-ids`,
    and there are two zipped files inside that directory as well: `ferguson-indictment-tweet-ids.zip`
    and `ferguson-tweet-ids.zip`. We really only need to unzip one of these to perform
    this project, so I chose this one:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`ls`命令显示创建了一个名为`ferguson-tweet-ids`的目录，并且该目录内也有两个压缩文件：`ferguson-indictment-tweet-ids.zip`和`ferguson-tweet-ids.zip`。我们实际上只需要解压其中一个文件来执行这个项目，因此我选择了这个：'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Unzipping this file exposes several manifest text files, as well as a data
    folder. Inside the `data` folder is a gzipped file. Unzip it as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 解压此文件后，可以看到多个清单文本文件以及一个数据文件夹。`data`文件夹中包含一个gzipped文件。按以下方式解压：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This yields a file called `ids.txt`. This is the file we are actually after.
    Let's explore this file.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生一个名为`ids.txt`的文件。这个文件就是我们实际需要的文件。让我们来探索一下这个文件。
- en: 'To see the size of the file, we can run the `wc` command. When run from the
    command prompt as shown here, `wc` shows how many lines, words, and characters
    are in the file, in that order:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看文件的大小，我们可以运行`wc`命令。从命令提示符运行时，如这里所示，`wc`命令会显示文件中的行数、单词数和字符数，顺序如下：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The first number indicates how many lines are in the `ids.txt` file, just over
    13 million. Next, we can peek inside the file with the `head` command:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数字表示`ids.txt`文件中的行数，总共有1300多万行。接下来，我们可以使用`head`命令查看文件内容：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `head` command shows the first ten lines of the file so we can see that
    each line is comprised of an 18-digit tweet ID.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`head`命令显示文件的前十行，我们可以看到每行由一个18位的推文ID组成。'
- en: Create a test version of the file
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建文件的测试版本
- en: In this stage, we will create a small test file to work with for the remainder
    of this project. The reasons we want to do this are the same reasons why we worked
    with test tables in the [Chapter 9](part0059.xhtml#aid-1O8H61 "Chapter 9. Stack
    Overflow Project"), *Stack Overflow Project*. Because the original files are extremely
    large, we want to work with a subset of the data in case we make a mistake. We
    also want to be able to test our code without taking too long to complete each
    step.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，我们将创建一个小的测试文件，供本项目其余部分使用。我们想这么做的原因与我们在[第 9 章](part0059.xhtml#aid-1O8H61
    "第 9 章。Stack Overflow 项目")中处理测试表格的原因相同，*Stack Overflow 项目*。因为原始文件非常大，我们希望在出错时能够处理数据的一个子集。我们还希望在测试代码时，每一步不会花费太长时间。
- en: 'Unlike in previous exercises, it probably does not matter if we select our
    test data lines randomly in this case. Just by looking at the result of the `head`
    command we ran earlier, we can see that the lines are not really in low-to-high
    order. In fact, we have no information about what order the original set of lines
    are in. Therefore, let''s just grab the first 1,000 tweet IDs and save them to
    a file. This will become our test set:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的练习不同，在这种情况下，我们选择测试数据行的顺序可能并不重要。仅通过查看我们之前运行的 `head` 命令的结果，我们可以看到这些行并不完全是按低到高的顺序排列。事实上，我们对原始数据行的顺序一无所知。因此，我们只需抓取前
    1,000 个推文 ID 并将其保存到一个文件中。这个文件将成为我们的测试集：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Hydrate the tweet IDs
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充推文 ID
- en: We will now use this test set of 1,000 tweet IDs to test our procedure for collecting
    the original tweets based on their identification number. This process is called
    **hydrating** the tweets. To do this, we will use a handy Python tool called **twarc**,
    which was written by Ed Summers, the same person who archived all the Ferguson
    tweets. It works by taking a list of tweet IDs and fetching each original tweet
    from the Twitter API one by one. To do anything with the Twitter API, we must
    have a Twitter developer account already set up. Let's make our Twitter account
    first, and then we can install twarc and use it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用这组 1,000 个推文 ID 来测试我们基于推文 ID 收集原始推文的流程。这个过程称为**填充**推文。为此，我们将使用一个非常方便的
    Python 工具，叫做**twarc**，它是由 Ed Summers 编写的，Ed 同时也是存档了所有 Ferguson 推文的人。它的工作原理是通过获取推文
    ID 列表，逐个从 Twitter API 获取原始推文。要使用 Twitter API，必须先设置好 Twitter 开发者帐户。我们先创建一个 Twitter
    账户，然后再安装 twarc 并使用它。
- en: Setting up a Twitter developer account
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Twitter 开发者账户
- en: To set up a Twitter developer account, go to [https://apps.twitter.com](https://apps.twitter.com)
    and log in with your Twitter account. If you do not already have a Twitter account,
    you will need to make one of those in order to complete the rest of these steps.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置 Twitter 开发者账户，请访问 [https://apps.twitter.com](https://apps.twitter.com)，并使用你的
    Twitter 账户登录。如果你还没有 Twitter 账户，你需要先创建一个，才能完成后续步骤。
- en: 'Once you are logged in with your Twitter account, from the [http://apps.twitter.com](http://apps.twitter.com)
    page, click on **Create New App**. Fill in the required details to create your
    application (give it a name, perhaps something like `My Tweet Test`; a short description;
    and a URL, which does not have to be permanent). My filled-in app creation form
    is shown here for your reference:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你用 Twitter 账户登录，从 [http://apps.twitter.com](http://apps.twitter.com) 页面，点击**创建新应用**。填写必要的详细信息来创建你的应用（为它命名，可以是类似
    `My Tweet Test` 的名字；提供简短的描述；以及 URL，URL 不必是永久性的）。我的应用创建表单已填写如下，供你参考：
- en: '![Setting up a Twitter developer account](img/image00307.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![设置 Twitter 开发者账户](img/image00307.jpeg)'
- en: Twitter's Create New App form, filled in with sample data
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 填写示例数据的 Twitter 创建新应用表单
- en: Click on the checkbox indicating that you agree to the developer agreement,
    and you will be returned to the screen listing all your Twitter apps, with your
    new app at the top of the list.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 点击表示同意开发者协议的复选框，然后你将返回到显示所有 Twitter 应用的屏幕，新的应用会显示在列表的最上方。
- en: 'Next, to use this app, we will need to get a few key pieces of information
    that are required for it to work. Click on the app you just created, and you will
    see four tabs across the top of the next screen. We are interested in the one
    that says **Keys and Access Tokens**. It looks like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，要使用这个应用程序，我们需要获取一些必要的关键信息。点击你刚创建的应用，你将看到下一屏幕顶部有四个标签。我们感兴趣的是标有**密钥和访问令牌**的那个。它看起来是这样的：
- en: '![Setting up a Twitter developer account](img/image00308.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![设置 Twitter 开发者账户](img/image00308.jpeg)'
- en: The tab interface within your newly created Twitter app.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你新创建的 Twitter 应用中的标签界面。
- en: 'There are a lot of numbers and secret codes on this screen, but there are four
    items that we need to pay particular attention to:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个页面上有很多数字和密钥，但有四个项目我们需要特别关注：
- en: '**CONSUMER_KEY**'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CONSUMER_KEY**'
- en: '**CONSUMER_SECRET**'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CONSUMER_SECRET**'
- en: '**ACCESS_TOKEN**'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ACCESS_TOKEN**'
- en: '**ACCESS_TOKEN_SECRET**'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ACCESS_TOKEN_SECRET**'
- en: No matter what kind of Twitter API programming you are doing, at least for the
    time being, you will always need these same four items. This is true whether you
    are using twarc or some other tool. These credentials are how Twitter authenticates
    you and makes sure that you have the access rights needed to see whatever it is
    that you are requesting.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你在进行哪种类型的 Twitter API 编程，至少目前为止，你总是需要这四项凭证。无论你使用的是 twarc 还是其他工具，这些凭证都是 Twitter
    用来验证你并确保你有权限查看你请求的内容的方式。
- en: Note
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: These API credentials are also how Twitter limits how many requests you can
    send and how quickly you can make your requests. The twarc tool handles all of
    this on our behalf, so we do not have to worry too much about exceeding our rate
    limits. For more information on Twitter's rate limits, check out their developer
    documentation at [https://dev.twitter.com/rest/public/rate-limiting](https://dev.twitter.com/rest/public/rate-limiting).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 API 凭证也是 Twitter 限制你可以发送多少请求以及请求速度的依据。twarc 工具会代我们处理这些问题，所以我们不需要过多担心超过请求频率限制。欲了解更多关于
    Twitter 限制的信息，请查看他们的开发者文档，[https://dev.twitter.com/rest/public/rate-limiting](https://dev.twitter.com/rest/public/rate-limiting)。
- en: Installing twarc
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 twarc
- en: Now that we have access to our Twitter credentials, we can install twarc.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 Twitter 凭证，就可以安装 twarc 了。
- en: The twarc download page is available on GitHub at [https://github.com/edsu/twarc](https://github.com/edsu/twarc).
    On that page, there is documentation about how to use the tool and what options
    are available.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: twarc 下载页面可以在 GitHub 上找到，[https://github.com/edsu/twarc](https://github.com/edsu/twarc)。在该页面上，有关于如何使用这个工具以及可用选项的文档。
- en: To install twarc in your Canopy Python environment, start up Canopy and then
    choose **Canopy Command Prompt** from the **Tools** menu.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要在你的 Canopy Python 环境中安装 twarc，启动 Canopy 后从**工具**菜单中选择**Canopy 命令提示符**。
- en: 'At the command prompt, type in:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令提示符中输入：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This command will install twarc and make it available to be called as a command-line
    program, or from within your own Python programs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这条命令将安装 twarc 并使其可以作为命令行程序调用，或者在你的 Python 程序中使用。
- en: Running twarc
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行 twarc
- en: 'We can now use twarc from the command line to hydrate the `ids_1000.txt` file
    that we created earlier. The command line for this is very long, since we have
    to pass in those four long secret tokens that we created earlier on the Twitter
    site. To save myself from making errors, I used my text editor, Text Wrangler,
    to create the command line first, and then I pasted it into my Command Prompt.
    Your final command line will look like the one that follows, except everywhere
    that it says `abcd`, you should instead fill in your appropriate secret token
    or secret key:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用命令行通过 twarc 工具来获取我们之前创建的`ids_1000.txt`文件。由于我们需要输入之前在 Twitter 网站上创建的四个长的密钥令牌，命令行非常长。为了避免出错，我先使用文本编辑器
    Text Wrangler 创建了命令行，然后将其粘贴到命令提示符中。你最终的命令行会像下面这样，但每当出现`abcd`时，你应该用你对应的密钥令牌或密钥来替代：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that this command line will redirect its output to a JSON file called
    `tweets_1000.json`. Inside that file is a JSON representation of each tweet that
    we only had the ID for previously. Let''s check how long the new file is:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这条命令行将把输出重定向到一个名为`tweets_1000.json`的 JSON 文件。该文件中包含了我们之前只有 ID 的每条推文的 JSON
    表示。让我们检查一下新文件的长度：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `wc` utility indicates that my file is 894 lines long, which indicates that
    some of the tweets were unable to be found (since I originally had 1,000 tweets
    in my dataset). If tweets have been deleted in the time since I have written this,
    your file might be even smaller.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`wc` 工具显示我的文件长达 894 行，这表示一些推文无法找到（因为我原本的数据集中有 1,000 条推文）。如果这些推文在我编写本文后被删除，你的文件可能会更小。'
- en: 'Next, we can also peek inside the file:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们还可以打开文件查看它的内容：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We could also open it in a text editor to view it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在文本编辑器中打开它查看。
- en: 'Each line in the JSON file represents a single tweet, each of which looks like
    the example that follows. However, this example tweet is not from the Ferguson
    dataset, since I do not have the rights to distribute those tweets. Instead, I
    used one of the tweets I created back in [Chapter 2](part0020.xhtml#aid-J2B82
    "Chapter 2. Fundamentals – Formats, Types, and Encodings"), *Fundamentals – Formats,
    Types, and Encodings*, for our discussion on UTF-8 encoding. Since this tweet
    was created just for this book, and I own the content, I can show you the JSON
    format without violating Twitter''s ToS. Here is what my tweet looked like through
    the Twitter web interface:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 文件中的每一行代表一条推文，每条推文的结构类似于以下示例。然而，这个示例推文并不来自 Ferguson 数据集，因为我没有权利分发这些推文。相反，我使用了我在[第
    2 章](part0020.xhtml#aid-J2B82 "第 2 章. 基础知识 – 格式、类型和编码")中创建的推文，*基础知识 – 格式、类型和编码*，用于讨论
    UTF-8 编码。由于这条推文是专门为本书创建的，而且我拥有内容的所有权，因此我可以在不违反 Twitter 使用条款的情况下展示其 JSON 格式。以下是我的推文在
    Twitter 网页界面上的显示方式：
- en: '![Running twarc](img/image00309.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![运行 twarc](img/image00309.jpeg)'
- en: An example of a tweet, as shown in Twitter's web interface.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个推文的示例，如 Twitter 网页界面所示。
- en: 'The following is what the tweet looks like in its JSON representation after
    twarc hydrates it. I have added newlines in between each JSON element so we can
    see what attributes are available in each tweet more easily:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是推文在经过 twarc 水合后的 JSON 表示形式。我在每个 JSON 元素之间添加了换行符，这样我们可以更容易地看到每个推文中可用的属性：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Not only does each JSON object include facts about the tweet itself, for example,
    the text, the date, and the time that it was sent out, it also includes a wealth
    of information about the person who tweeted it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 JSON 对象不仅包括关于推文本身的事实，例如文本、日期和发送时间，它还包括关于发布推文的人的大量信息。
- en: Tip
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The hydration process results in a *lot* of information about a single individual
    tweet — and this is a dataset with 13 million tweets in all. Keep this in mind
    when you get ready to hydrate the entire Ferguson dataset at the end of this chapter.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 水合过程会生成关于单条推文的*大量*信息——而且这是一个包含 1300 万条推文的数据集。在你准备在本章末尾水合整个 Ferguson 数据集时，请记住这一点。
- en: Step three – data cleaning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三步 – 数据清洗
- en: At this point, we are ready to begin cleaning the JSON file, extracting the
    details of each tweet that we want to keep in our long-term storage.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们已经准备好开始清理 JSON 文件，提取我们希望保存在长期存储中的每条推文的详细信息。
- en: Creating database tables
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据库表
- en: 'Since our motivating question only asks about URLs, we really only need to
    extract those, along with the tweet IDs. However, for the sake of practice in
    cleaning, and so that we can compare this exercise to what we did earlier in [Chapter
    7](part0045.xhtml#aid-1AT9A1 "Chapter 7. RDBMS Cleaning Techniques"), *RDBMS Cleaning
    Techniques*, with the `sentiment140` data set, let''s design a small set of database
    tables as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的问题只关心 URL，因此我们实际上只需要提取这些 URL 和推文 ID。然而，为了练习数据清洗，并且为了能够将此练习与我们在[第 7 章](part0045.xhtml#aid-1AT9A1
    "第 7 章. RDBMS 清洗技术")中使用 `sentiment140` 数据集时做的工作进行对比，*RDBMS 清洗技术*，我们设计了一组小型数据库表，如下所示：
- en: A `tweet` table, which only holds information about the tweets
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `tweet` 表，专门存储推文的信息
- en: A `hashtag` table, which holds information about which tweets referenced which
    hashtags
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `hashtag` 表，存储哪些推文引用了哪些话题标签的信息
- en: A `URL` table, which holds information about which tweets referenced which URLs
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `URL` 表，存储哪些推文引用了哪些 URL 的信息
- en: A `mentions` table, which holds information about which tweets mentioned which
    users
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `mentions` 表，包含有关哪些推文提到了哪些用户的信息
- en: This is similar to the structure we designed in [Chapter 7](part0045.xhtml#aid-1AT9A1
    "Chapter 7. RDBMS Cleaning Techniques"), *RDBMS Cleaning Techniques*, except in
    that case we had to parse out our own list of hashtags and URLs and user mentions
    from the tweet text. The twarc tool has definitely saved us some effort as we
    complete this project in this chapter.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在[第 7 章](part0045.xhtml#aid-1AT9A1 "第 7 章. RDBMS 清洗技术")中设计的结构类似，*RDBMS 清洗技术*，不同的是，在那个例子中我们必须从推文文本中解析出自己的话题标签、URL
    和用户提及。使用 twarc 工具无疑为我们完成本章的工作节省了很多精力。
- en: 'The `CREATE` statements to make our four tables are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这四个表的 `CREATE` 语句如下：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: One thing to note about the tweets table is that it was created with the **utf8mb4**
    collation. This is because the tweets themselves may include characters that are
    very high in the UTF-8 range. In fact, some characters in these tweets will require
    more space than the 3-byte limit that the native MySQL UTF-8 character set can
    hold. Therefore, we designed the main tweet table to hold data in MySQL's utf8mb4
    collation, which is included in MySQL 5.5 or higher. If you are working on a MySQL
    version older than that, or for some other reason, you do not have access to the
    utf8mb4 collation, you can use MySQL's older UTF-8-general collation, but be aware
    that you may generate encoding errors with an emoji here or there. If you do run
    into this error, MySQL will likely yield a message about Error 1366 and *incorrect
    string value* when you are trying to `INSERT` the record.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有一点需要注意，推文表是使用**utf8mb4**字符集创建的。这是因为推文中可能包含UTF-8范围内非常高的字符。实际上，这些推文中的某些字符将需要比MySQL原生UTF-8字符集所能容纳的3字节限制更多的空间。因此，我们设计了主要的推文表，使其能够使用MySQL的utf8mb4字符集，该字符集在MySQL
    5.5或更高版本中可用。如果你正在使用比这个版本更旧的MySQL，或者由于其他原因无法访问utf8mb4字符集，你可以使用MySQL较旧的UTF-8-general字符集，但需要注意，某些表情符号可能会导致编码错误。如果确实遇到此错误，MySQL可能会显示关于错误1366和*不正确的字符串值*的消息，当你尝试`INSERT`记录时。
- en: Now that each table is created, we can begin to select and load the data in.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每个表格都已创建，我们可以开始选择并加载数据了。
- en: Populating the new tables in Python
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中填充新表格
- en: The Python script that follows will load in the JSON file, extract the values
    from the fields we are interested in, and populate the four tables described previously.
    There are a few additional, important notes about this script, which I will go
    through now.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python脚本将加载JSON文件，提取我们感兴趣的字段的值，并填充之前描述的四个表格。关于此脚本，还有一些额外的重要事项，我将一一讲解。
- en: 'This script does require the `MySQLdb` Python modules to be installed. As a
    Canopy Python user, these modules are easy to install through the package manager.
    Simply run a search for `MySQLdb` in the **Package Manager** and click to install:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本确实需要安装`MySQLdb` Python模块。作为Canopy Python用户，这些模块可以通过包管理器轻松安装。只需在**包管理器**中搜索`MySQLdb`，然后点击安装：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Tip
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: For more information on each of the fields provided in the JSON representation
    of a tweet, the Twitter API documentation is very helpful. The sections on Users,
    Entities, and Entities in Tweets are particularly instructive when planning which
    fields to extract from the JSON tweet. You can get started with the documentation
    at [https://dev.twitter.com/overview/api/](https://dev.twitter.com/overview/api/).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 关于JSON表示的推文中每个字段的更多信息，Twitter API文档非常有帮助。有关用户、实体以及推文中的实体部分的内容，尤其是在规划从JSON推文中提取哪些字段时，文档中的这些部分特别有指导意义。你可以通过[https://dev.twitter.com/overview/api/](https://dev.twitter.com/overview/api/)开始查阅文档。
- en: Once this script is run, the four tables are populated with data. On my MySQL
    instance, after running the preceding script against my `ids_1000.txt` file, I
    ended up with 893 rows in the tweets table; 1,048 rows in the hashtags table;
    896 rows in the user mentions table; and 371 rows in the URLs table. If you have
    fewer rows here or there, check to see whether it is because tweets have been
    deleted.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行此脚本，四个表格将填充数据。在我的MySQL实例中，运行上述脚本并对`ids_1000.txt`文件进行操作后，我在推文表中得到了893行数据；在标签表中得到了1,048行数据；在用户提及表中得到了896行数据；在URL表中得到了371行数据。如果你的某些表格行数较少，检查是否是因为某些推文被删除了。
- en: Step four – simple data analysis
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四步 —— 简单的数据分析
- en: Suppose we want to learn which web domains were linked the most in the Ferguson
    dataset. We can answer this question by extracting just the domain portion of
    the URL stored in the `tdisplay` column in our `ferguson_tweets_urls` table. For
    our purposes, we will consider everything before the first slash (`/`) as the
    interesting part of the URL.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要了解在Ferguson数据集中，哪些网页域名被链接得最多。我们可以通过提取存储在`ferguson_tweets_urls`表中`tdisplay`列的URL域名部分来回答这个问题。对于我们的目的，我们将URL中第一个斜杠(`/`)之前的部分视为感兴趣的部分。
- en: 'The following SQL query gives us the domain and count of posts that reference
    that domain:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下SQL查询为我们提供了域名和引用该域名的帖子数：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The result of this query is a dataset that looks something like the following
    (run on the sample set of 1,000 rows):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询的结果是一个数据集，看起来大致如下（在1,000行样本数据上运行）：
- en: '| url | num |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| url | num |'
- en: '| --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| bit.ly | 47 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| bit.ly | 47 |'
- en: '| wp.me | 32 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| wp.me | 32 |'
- en: '| dlvr.it | 18 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| dlvr.it | 18 |'
- en: '| huff.to | 13 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| huff.to | 13 |'
- en: '| usat.ly | 9 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| usat.ly | 9 |'
- en: '| ijreview.com | 8 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| ijreview.com | 8 |'
- en: '| latimes.com | 7 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| latimes.com | 7 |'
- en: '| gu.com | 7 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| gu.com | 7 |'
- en: '| ift.tt | 7 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ift.tt | 7 |'
- en: This snippet of the dataset shows just the first few rows, but we can already
    see some of the more popular results are URL-shortening services, such as `bit.ly`.
    We can also see that we are able to remove all those shortened URLs created by
    Twitter's own shortener, `t.co`, simply by using the display column rather than
    the main URL column.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这段数据集片段仅展示了前几行，但我们已经可以看到一些更受欢迎的结果是URL缩短服务，比如`bit.ly`。我们还可以看到，我们可以通过使用显示列而不是主URL列，轻松去除所有由Twitter自有的短链接服务`t.co`创建的缩短URL。
- en: In the next section, we can use these counts to build a bar graph, in a similar
    manner to the way we built a simple graph in [Chapter 9](part0059.xhtml#aid-1O8H61
    "Chapter 9. Stack Overflow Project"), *Stack Overflow Project*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们可以使用这些计数来构建一个条形图，方式与我们在[第9章](part0059.xhtml#aid-1O8H61 "第9章. Stack
    Overflow项目")中构建简单图表的方式类似，*Stack Overflow项目*。
- en: Step five – visualizing the data
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五步 - 可视化数据
- en: To build a little D3-enabled graph, we could follow the same procedure we used
    in [Chapter 9](part0059.xhtml#aid-1O8H61 "Chapter 9. Stack Overflow Project"),
    *Stack Overflow Project*, in which we made a PHP script that queries the database,
    and then our JavaScript uses the results as the live input to a bar graph. Alternatively,
    we could generate a CSV file with Python and let D3 generate its graph from those
    results. Since we already performed the PHP method in the previous chapter, let's
    use the CSV file method here, just for variety. This is also a good excuse to
    continue on with Python in this chapter, since this is already the language we
    have been using.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个小的D3启用图表，我们可以遵循与[第9章](part0059.xhtml#aid-1O8H61 "第9章. Stack Overflow项目")中*Stack
    Overflow项目*类似的流程，在那个章节里我们制作了一个PHP脚本来查询数据库，然后我们的JavaScript使用结果作为条形图的实时输入。或者，我们可以使用Python生成CSV文件，让D3从这些结果中生成图表。由于我们在上一章已经使用过PHP方法，这里就用CSV文件方法，换个口味。这也是一个继续使用Python的好理由，因为我们已经在本章中使用了这个语言。
- en: 'The following script connects to the database, selects out the top 15 most-used
    URLs and their counts, and writes the entire thing to a CSV file:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本连接到数据库，选择出使用最多的前15个URL及其计数，并将整个内容写入CSV文件：
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once we have this CSV file, we can feed it into a stock D3 bar graph template,
    just to see what it looks like. The following can be called `buildBarGraph.html`
    or the like:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了这个CSV文件，我们就可以将它输入到一个标准的D3条形图模板中，看看它的效果。以下内容可以命名为`buildBarGraph.html`之类的：
- en: Note
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure you have the D3 libraries in your local folder, just as you did in
    previous chapters, along with the CSV file you just made.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的本地文件夹中有D3库，就像在前几章中一样，另外还要有你刚刚创建的CSV文件。
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The resulting bar graph looks like the one shown here. Again, remember that
    we are using the test dataset so the numbers are quite small:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的条形图看起来就像这里展示的那样。再强调一次，记得我们使用的是测试数据集，因此这些数字非常小：
- en: '![Step five – visualizing the data](img/image00310.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![第五步 - 可视化数据](img/image00310.jpeg)'
- en: A simple bar graph drawn in D3 using our CSV file.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的CSV文件在D3中绘制的简单条形图。
- en: Step six – problem resolution
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六步 - 问题解决
- en: Since data visualization is not the main purpose of this book, we are not overly
    concerned with how sophisticated the diagram from the section is, and suffice
    it to say that there are many, many more interesting patterns to be uncovered
    in the Ferguson data set than just which URLs were pointed to the most. Now that
    you know how to easily download and clean this massive data set, perhaps you can
    let your imagination work to uncover some of these patterns. Remember that when
    you release your findings to your adoring public, you must not release the tweets
    themselves or their metadata. But you can release the tweet IDs, or a subset of
    them, if that is what your question required.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据可视化不是本书的主要目的，我们并不太关注这一部分的图表有多复杂，简单来说，费格森数据集里还有很多有趣的模式等着我们去发现，不仅仅是哪些URL被点击得最多。既然你已经知道如何轻松地下载和清理这个庞大的数据集，也许你可以发挥想象力，去揭示这些模式。当你向公众发布你的发现时，记得不要发布推文本身或其元数据。但如果你的问题需要，你可以发布推文的ID，或者它们的一个子集。
- en: Moving this process into full (non-test) tables
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将此过程转移到完整（非测试）表格中
- en: 'Just like in [Chapter 9](part0059.xhtml#aid-1O8H61 "Chapter 9. Stack Overflow
    Project"), *Stack Overflow Project*, we made test tables so that we could develop
    our project in a stress-free environment with a manageable number of tweets to
    collect. When you are ready to collect the full list of tweets, be ready to spend
    some time doing so. Twitter''s rate limits will kick in, and twarc will take a
    long time to run. Ed Summers indicates on this blog post that it will take about
    one week to run the Ferguson tweets: [http://inkdroid.org/journal/2014/11/18/on-forgetting/](http://inkdroid.org/journal/2014/11/18/on-forgetting/).
    Of course, if you are careful, you will only have to run it once.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在[第9章](part0059.xhtml#aid-1O8H61 "第9章。堆栈溢出项目")中提到的*堆栈溢出项目*一样，我们制作了测试表，以便在一个无压力的环境中开发我们的项目，并收集可管理数量的推文。当你准备好收集完整的推文列表时，做好花费一些时间的准备。Twitter的速率限制将会生效，twarc也会运行很长时间。Ed
    Summers在这篇博客文章中指出，运行Ferguson推文大约需要一周的时间：[http://inkdroid.org/journal/2014/11/18/on-forgetting/](http://inkdroid.org/journal/2014/11/18/on-forgetting/)。当然，如果你小心操作，你只需要运行一次。
- en: Another thing you could do to speed up the time it takes to hydrate the tweet
    IDs is to work as a team with someone else. You can divide the tweet ID file in
    half and each work on your portion of the tweets. During the data cleaning process,
    make sure you `INSERT` both into the same final database table.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做的另一件事来加速推文ID的水合作用时间是与其他人合作。你可以将推文ID文件分成两半，各自处理自己部分的推文。在数据清理过程中，确保你将两者都`INSERT`到同一个最终数据库表中。
- en: 'Here are the steps we will follow to change our project to collect the full
    set of tweets rather than the 1,000-tweet sample:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将遵循的步骤，以便将我们的项目从收集1,000条推文样本更改为收集完整的推文集合：
- en: 'Empty the `ferguson_tweets`, `ferguson_tweets_hashtags`, `ferguson_tweets_mentions`,
    and `ferguson_tweets_urls` tables as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式清空`ferguson_tweets`、`ferguson_tweets_hashtags`、`ferguson_tweets_mentions`和`ferguson_tweets_urls`表：
- en: '[PRE16]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Run twarc on the full `ids.txt` file rather than the `ids_1000.txt` file as
    follows:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式在完整的`ids.txt`文件上运行twarc，而不是在`ids_1000.txt`文件上运行：
- en: '[PRE17]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Re-run the `jsonTweetCleaner.py` Python script.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新运行`jsonTweetCleaner.py` Python脚本。
- en: At this point, you will have a cleaned database full of tweets, hashtags, mentions,
    and URLs, ready for analysis and visualization. Since there are so many more rows
    now in each table, be aware that each of the visualization steps could take much
    longer to run, depending on what kind of queries you are running.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到这时，你将拥有一个已清理的数据库，里面充满了推文、标签、提及和网址，准备进行分析和可视化。由于现在每个表中有更多的行，请注意，每个可视化步骤的运行时间可能会更长，这取决于你执行的查询类型。
- en: Summary
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this project, we learned how to reconstruct a list of tweets based on their
    identification numbers. First, we located high-quality archived tweet data that
    conforms to Twitter's latest ToS. We learned how to split it into a set small
    enough for testing purposes. Then, we learned how to hydrate each tweet into a
    full JSON representation of itself, using the Twitter API and the twarc command-line
    tool. Next, we learned how to extract pieces of the JSON entities in Python, saving
    the fields to a new set of tables in our MySQL database. We then ran some simple
    queries to count the most common URLs, and we drew a bar graph using D3.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们学会了如何根据推文的ID重建推文列表。首先，我们找到符合Twitter最新服务条款的高质量存档推文数据。我们学会了如何将其拆分成一个足够小的集合，适用于测试目的。接着，我们学会了如何使用Twitter
    API和twarc命令行工具将每条推文水合成其完整的JSON表示。然后，我们学会了如何在Python中提取JSON实体的部分内容，并将字段保存到MySQL数据库中的新表中。最后，我们运行了一些简单的查询来计算最常见的网址，并使用D3绘制了一个条形图。
- en: In this book, we have learned how to perform a variety of data cleaning tasks,
    both simple and complex. We used a variety of languages, tools, and techniques
    to get the job done, and along the way, I hope you were able to perfect your existing
    skills while learning many new ones.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经学习了如何执行各种数据清理任务，包括简单的和复杂的。我们使用了多种语言、工具和技术来完成任务，在这个过程中，我希望你能够完善现有技能，并学到许多新技能。
- en: At this point, our final dinner party is complete, and you are now ready to
    begin your own cleaning projects in your fully stocked — and very clean — data
    science kitchen. Where should you begin?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们的最终晚宴已圆满结束，你现在准备好在你充实而又非常干净的数据科学厨房中开始自己的清理项目了。你应该从哪里开始？
- en: Do you like contests and prizes? Kaggle hosts frequent data analysis competitions
    at their website, [http://kaggle.com](http://kaggle.com). You can work alone or
    as part of a team. Many teams need clean data, so that is a fantastic way to pitch
    in.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你喜欢比赛和奖品吗？Kaggle 在他们的网站上举办频繁的数据分析比赛，[http://kaggle.com](http://kaggle.com)。你可以单独工作，也可以作为团队的一部分。许多团队需要干净的数据，这也是你贡献力量的绝佳方式。
- en: If you are more of a public service kind of person, may I suggest School of
    Data? Their website is at [http://schoolofdata.org](http://schoolofdata.org),
    and they host courses and *Data Expeditions* where experts and amateurs from around
    the world get together and solve real-world problems using data.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你是一个更偏向于公共服务的人，我建议你去看看数据学院（School of Data）？他们的网址是 [http://schoolofdata.org](http://schoolofdata.org)，他们提供课程和*数据探险*活动，来自世界各地的专家和业余爱好者共同解决使用数据的现实问题。
- en: 'To extend your data cleaning practice, I highly recommend getting your hands
    dirty with some of the many publicly-available data sets out there. KDnuggets
    has a nice list of them here, including some lists of lists: [http://www.kdnuggets.com/datasets/](http://www.kdnuggets.com/datasets/).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了进一步扩展你的数据清洗实践，我强烈推荐你亲自去摸索一些公共可用的数据集。KDnuggets 提供了一个很好的数据集列表，包含一些列表中的列表，网址是：[http://www.kdnuggets.com/datasets/](http://www.kdnuggets.com/datasets/)。
- en: Did you like the Stack Overflow examples in [Chapter 9](part0059.xhtml#aid-1O8H61
    "Chapter 9. Stack Overflow Project"), *Stack Overflow Project*? The *Meta* Stack
    Exchange, available at [http://meta.stackexchange.com](http://meta.stackexchange.com),
    is a site just for discussing the way StackExchange sites work. Users discuss
    hundreds of amazing ideas for how to query Stack Overflow data and what to do
    with what they find. Or, you can always contribute to the large body of questions
    on Stack Overflow itself that are related to data cleaning. Finally, there are
    several other Stack Exchange sites that are also related to data cleaning. One
    useful site is the Open Data Stack Exchange, available at [http://opendata.stackexchange.com](http://opendata.stackexchange.com).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你喜欢[第 9 章](part0059.xhtml#aid-1O8H61 "第 9 章. Stack Overflow 项目")，*Stack Overflow
    项目*中的 Stack Overflow 示例吗？*Meta* Stack Exchange 是一个专门讨论 StackExchange 网站工作方式的网站，网址是
    [http://meta.stackexchange.com](http://meta.stackexchange.com)。用户们讨论了数百个关于如何查询
    Stack Overflow 数据以及如何处理这些数据的精彩想法。或者，你也可以贡献自己的一份力量，参与 Stack Overflow 上与数据清洗相关的大量问题讨论。最后，还有一些与数据清洗相关的其他
    Stack Exchange 网站，其中一个有用的网站是 Open Data Stack Exchange，网址是 [http://opendata.stackexchange.com](http://opendata.stackexchange.com)。
- en: Twitter data is extremely popular right now. If you liked working with Twitter
    data, consider taking our [Chapter 10](part0067.xhtml#aid-1VSLM1 "Chapter 10. Twitter
    Project"), *Twitter Project*, project to the next level by asking and answering
    your own questions about one of those publicly-available tweet collections. Or,
    how about collecting and curating a new set of tweet IDs of your own? If you build
    clean collections of tweet IDs on some topic of interest, you can distribute those
    and researchers and other data scientists will be extremely grateful.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter 数据目前非常受欢迎。如果你喜欢处理 Twitter 数据，可以考虑将我们的[第 10 章](part0067.xhtml#aid-1VSLM1
    "第 10 章. Twitter 项目")，*Twitter 项目*，提升到一个新的层次，通过提出和回答你自己的问题，分析那些公开可用的推文集合。或者，你也可以收集和整理一组属于你自己的推文
    ID。如果你在某个感兴趣的话题上建立干净的推文 ID 集合，你可以将其分发给研究人员和其他数据科学家，他们会非常感激。
- en: Good luck in your data cleaning adventures, and *bon apetit!*
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你在数据清洗的冒险中好运，*祝好胃口！*
