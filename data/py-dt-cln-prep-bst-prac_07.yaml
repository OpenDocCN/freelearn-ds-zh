- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Data Sinks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据接收端
- en: 'In the world of modern data processing, crucial decisions about data management,
    storage, and processing will determine successful outcomes. In this chapter, we
    will deep dive into three important pillars that underpin effective data processing
    pipelines: selecting the right **data sink**, choosing the optimal file type,
    and mastering partitioning strategies. By discussing these critical elements and
    their real-world applications, this chapter will equip you with the insights and
    strategies needed to architect data solutions that optimize efficiency, scalability,
    and performance within the complicated landscape of data processing technologies.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代数据处理的世界中，关于数据管理、存储和处理的关键决策将决定成功的结果。在本章中，我们将深入探讨支撑高效数据处理管道的三大重要支柱：选择正确的**数据接收端**、选择最优的文件类型，以及掌握分区策略。通过讨论这些关键要素及其在实际应用中的体现，本章将为你提供所需的洞察和策略，帮助你在复杂的数据处理技术领域内设计优化效率、可扩展性和性能的数据解决方案。
- en: 'In this chapter, we will discuss the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Choosing the right data sink for your use case
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的使用案例选择正确的数据接收端
- en: Choosing the right file type for your use case
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的使用案例选择正确的文件类型
- en: Navigating partitioning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导航分区
- en: Designing an online retail data platform
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一个在线零售数据平台
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, we will need to install the following libraries:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们需要安装以下库：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As always, you can find all the code for this chapter in this book’s GitHub
    repository: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter07](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter07).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，你可以在本书的GitHub仓库中找到本章的所有代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter07](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter07)。
- en: Each section is followed by a script with a similar naming convention, so feel
    free to execute the scripts and/or follow along by reading this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每个部分后面都有一个类似命名规则的脚本，因此可以执行这些脚本或通过阅读本章进行跟进。
- en: Choosing the right data sink for your use case
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为你的使用案例选择正确的数据接收端
- en: A data sink refers to a destination or endpoint where data is directed or stored.
    The term “sink” is used metaphorically to convey the idea of data flowing into
    and being absorbed by a designated location. Data sinks are commonly used as storage
    locations where data can be permanently or temporarily stored. This storage can
    be in the form of databases, files, or other data structures.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据接收端指的是数据流向或存储的目标位置。术语“接收端”是用来比喻数据流入并被指定位置吸收的概念。数据接收端通常作为存储位置，数据可以在此永久或临时存储。这些存储可以是数据库、文件或其他数据结构的形式。
- en: Data engineers and data scientists often work with a variety of data sinks,
    depending on their specific tasks and use cases. Let’s look at some common data
    sinks, along with code examples, while considering the pros and cons of each type.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师和数据科学家通常根据其特定的任务和使用场景，使用多种数据接收端。让我们看看一些常见的数据接收端，并附带代码示例，同时考虑每种类型的优缺点。
- en: Relational databases
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关系型数据库
- en: Relational databases are a type of **database management system** (**DBMS**)
    that organizes data into tables with rows and columns, where each row represents
    a record and each column represents a field. The relationships between tables
    are established using keys. The primary key uniquely identifies each record in
    a table, and foreign keys create links between tables.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库是一种**数据库管理系统**（**DBMS**），它将数据组织成具有行和列的表格，每行代表一条记录，每列代表一个字段。表格之间的关系通过键建立。主键唯一标识表格中的每一条记录，外键则在表格之间创建链接。
- en: Overview of relational databases
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关系型数据库概述
- en: 'The following is a quick overview of the key components of relational databases:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关系型数据库的关键组件的简要概述：
- en: '**Tables**: Data is organized into tables, where each table represents a specific
    entity or concept. For example, in a database for a library, there might be tables
    for books, authors, and borrowers.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格**：数据被组织成表格，每个表格代表特定的实体或概念。例如，在一个图书馆的数据库中，可能会有关于书籍、作者和借阅者的表格。'
- en: '**Rows and columns**: Each table consists of rows and columns. A row represents
    a specific record (e.g., a book), and each column represents a specific attribute
    or field of that record (e.g., title, author, and publication year).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行和列**：每个表由行和列组成。行代表特定的记录（例如，一本书），每列代表该记录的特定属性或字段（例如，书名、作者和出版年份）。'
- en: '**Keys**: Keys are used to establish relationships between tables. The primary
    key uniquely identifies each record in a table, and foreign keys in related tables
    create links between them.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键**：键用于建立表之间的关系。主键唯一标识表中的每一条记录，而相关表中的外键则在它们之间创建连接。'
- en: '**Structured Query Language** (**SQL**): Relational databases use SQL for querying
    and manipulating data. SQL allows users to retrieve, insert, update, and delete
    data, as well as define and modify the database’s structure.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化查询语言** (**SQL**)：关系型数据库使用SQL进行数据查询和操作。SQL允许用户检索、插入、更新和删除数据，同时定义和修改数据库的结构。'
- en: 'In the data field, we usually find relational databases in the following scenarios:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据领域，我们通常在以下场景中看到关系型数据库：
- en: '**Structured data**: If your data has a well-defined structure with clear relationships
    between entities, a relational database is a suitable choice.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化数据**：如果您的数据具有明确的结构，并且实体之间有清晰的关系，那么关系型数据库是一个合适的选择。'
- en: '**Data integrity requirements**: If maintaining data integrity is critical
    for your application (e.g., in financial systems or healthcare applications),
    a relational database provides mechanisms to enforce integrity constraints.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据完整性要求**：如果您的应用对于数据完整性有严格要求（例如，在金融系统或医疗应用中），关系型数据库提供机制来强制执行完整性约束。'
- en: '**Atomicity, consistency, isolation, and durability** **(ACID) properties**:
    **Atomicity** ensures a transaction is an all-or-nothing operation: either all
    changes are committed, or none are. For instance, in transferring money between
    accounts, atomicity guarantees both balances are updated together or not at all.
    **Consistency** means transactions move the database from one valid state to another
    while adhering to integrity constraints. If a rule such as unique customer IDs
    is violated, the transaction is rolled back to maintain consistency. **Isolation**
    ensures transactions execute independently, preventing interference and visibility
    of uncommitted changes between concurrent transactions. This avoids issues such
    as dirty reads. Finally, **durability** guarantees that once committed, changes
    persist, even after system failures, ensuring the permanence of updates such as
    contact information in an online application. If your application demands adherence
    to ACID properties, relational databases are designed to meet these requirements.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原子性、一致性、隔离性和持久性** **(ACID) 特性**：**原子性**确保事务是“全有或全无”的操作：要么所有更改都提交，要么都不提交。例如，在账户之间转账时，原子性保证两个账户的余额要么都更新，要么都不更新。**一致性**意味着事务将数据库从一个有效状态转移到另一个有效状态，同时遵守完整性约束。如果违反了唯一客户ID等规则，事务将回滚以保持一致性。**隔离性**确保事务独立执行，防止并发事务之间的干扰和未提交更改的可见性，避免了脏读等问题。最后，**持久性**保证一旦事务提交，更改就会永久保留，即使系统发生故障，也能确保更新（如在线应用中的联系人信息）的持久性。如果您的应用需要遵守ACID特性，关系型数据库专门设计来满足这些需求。'
- en: '**Complex queries**: If your application involves complex queries and reporting
    needs, relational databases, with their SQL querying capabilities, are well-suited
    for such scenarios.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂查询**：如果您的应用涉及复杂的查询和报告需求，关系型数据库凭借其SQL查询功能，非常适合此类场景。'
- en: There are many different options for building relational databases, as we will
    see in the following section.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 市面上有许多不同的构建关系型数据库的选项，接下来我们将看到这些选项。
- en: Different options for relational database management systems
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关系型数据库管理系统的不同选项
- en: 'There are many different **relational database management systems** (**RDBMSs**)
    out there. We’ve summarized the main ones in the following table:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 市面上有许多不同的**关系型数据库管理系统** (**RDBMSs**) 。我们在下表中总结了主要的几种：
- en: '| **Database** | **Description** |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **数据库** | **描述** |'
- en: '| MySQL | An open source RDBMS known for its speed, reliability, and wide usage
    in web development |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| MySQL | 一个以速度、可靠性著称的开源关系型数据库管理系统，广泛应用于网页开发 |'
- en: '| PostgreSQL | An open source RDBMS with advanced features, extensibility,
    and support for complex queries |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| PostgreSQL | 一个开源关系型数据库管理系统，具备高级功能、可扩展性，并支持复杂查询 |'
- en: '| Oracle Database | A commercial RDBMS known for its scalability, security,
    and comprehensive set of data management features |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| Oracle 数据库 | 一款商业 RDBMS，以其可扩展性、安全性以及全面的数据管理功能而著称 |'
- en: '| Microsoft SQL Server | A commercial RDBMS by Microsoft that integrates with
    Microsoft technologies and has business intelligence support |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| Microsoft SQL Server | 微软推出的商业 RDBMS，集成了微软技术并支持商业智能 |'
- en: '| SQLite | A lightweight, embedded, serverless RDBMS suitable for applications
    with low to moderate database requirements |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| SQLite | 一款轻量级、嵌入式、无服务器的 RDBMS，适用于数据库需求较低或中等的应用 |'
- en: '| MariaDB | An open source RDBMS forked from MySQL that aims for compatibility
    while introducing new features |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| MariaDB | 一款从 MySQL 派生的开源 RDBMS，旨在兼容性同时引入新特性 |'
- en: Table 7.1 – Summary of RDBMSs
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.1 – RDBMS 概述
- en: Now, let’s see an example of how to quickly set up a local relational database,
    connect to it, and write a new table.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何快速设置本地关系型数据库、连接到它并创建一个新表的示例。
- en: An example of a PostgreSQL database
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个 PostgreSQL 数据库示例
- en: 'First, we need to install and set up PostgreSQL. This differs depending on
    the **operating system** (**OS**), but the logic remains the same. The following
    script automates the process of installing and setting up PostgreSQL on macOS
    or Debian-based Linux systems: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/setup_postgres.sh](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/setup_postgres.sh).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装并设置 PostgreSQL。这根据**操作系统**（**OS**）有所不同，但逻辑保持一致。以下脚本自动化了在 macOS 或基于
    Debian 的 Linux 系统上安装和设置 PostgreSQL 的过程：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/setup_postgres.sh](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/setup_postgres.sh)。
- en: 'First, it detects the OS using the `uname` command:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它使用 `uname` 命令检测操作系统：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If macOS is detected, it uses Homebrew to update package lists, install PostgreSQL,
    and start the PostgreSQL service. If a Debian-based Linux OS is detected, it uses
    `apt-get` to update package lists, install PostgreSQL and its `contrib` package,
    and start the PostgreSQL service. Here’s the code for installing macOS:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检测到 macOS，它将使用 Homebrew 更新软件包列表，安装 PostgreSQL 并启动 PostgreSQL 服务。如果检测到基于 Debian
    的 Linux 操作系统，它将使用 `apt-get` 更新软件包列表，安装 PostgreSQL 及其 `contrib` 包，并启动 PostgreSQL
    服务。以下是安装 macOS 的代码：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If your OS isn’t supported by this script, then the following error message
    will be shown:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的操作系统不被该脚本支持，则会显示以下错误消息：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this case, you will need to install PostgreSQL *manually and start the service*.
    Once you’ve done that, you can continue to the second part of the script. The
    script then switches to the default `postgres` user to execute SQL commands that
    create a new database user if it doesn’t already exist, create a new database
    owned by this user, and grant all privileges on the database to the user, as shown
    here:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你需要*手动安装 PostgreSQL 并启动服务*。完成后，你可以继续执行脚本的第二部分。然后，脚本切换到默认的 `postgres`
    用户，以执行 SQL 命令，在该用户尚未存在时创建新数据库用户，创建一个由该用户拥有的新数据库，并授予该用户对该数据库的所有权限，如下所示：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To execute the preceding code, do the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行上述代码，请按照以下步骤操作：
- en: Make sure you pull the repository to your local laptop.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保将仓库拉取到本地笔记本电脑。
- en: Go to the folder where the repository is located.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到存放仓库的文件夹。
- en: Open a terminal in the repository folder location.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在仓库文件夹位置打开终端。
- en: 'Execute the following commands to navigate to the right folder:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令以导航到正确的文件夹：
- en: '[PRE5]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: maria.zevrou@FVFGR3ANQ05P chapter7 % cd setup
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: maria.zevrou@FVFGR3ANQ05P chapter7 % cd setup
- en: maria.zevrou@FVFGR3ANQ05P set up % ls
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: maria.zevrou@FVFGR3ANQ05P set up % ls
- en: setup_postgres.sh
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: setup_postgres.sh
- en: '[PRE6]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Make the script executable by running the following command:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令使脚本可执行：
- en: '[PRE7]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, run the actual script using the following command:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下命令运行实际的脚本：
- en: '[PRE8]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After executing the script, you should see a confirmation message that the
    PostgreSQL setup, including the database and user creation, has been completed:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 执行脚本后，你应该看到一条确认消息，表示 PostgreSQL 设置（包括数据库和用户创建）已完成：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we’re ready to execute the script so that we can write our incoming data
    to the database we created in the previous step. You can find this script here:
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/1.postgressql.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/1.postgressql.py).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备执行脚本，以便将传入的数据写入我们在前一步中创建的数据库。你可以在这里找到这个脚本：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/1.postgressql.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/1.postgressql.py)。
- en: 'This script connects to the PostgreSQL database that we created previously
    and manages a table within it. Let’s get started:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本连接到我们之前创建的 PostgreSQL 数据库并管理其中的表。让我们开始吧：
- en: 'Let’s start by importing the necessary libraries:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入必要的库：
- en: '[PRE10]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we must define several functions, starting with `table_exists`. This
    function checks whether a specified table already exists in the database:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们需要定义几个函数，从 `table_exists` 开始。该函数检查指定的表是否已存在于数据库中：
- en: '[PRE11]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The next function we need is the `create_table` function, which creates a new
    table if it doesn’t already exist within a specific schema. In our case, it will
    have three columns: `id` as the primary key, then `name` and `age`:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要的下一个函数是 `create_table` 函数，如果表在特定模式下不存在，它将创建一个新表。在我们的例子中，它将有三列：`id` 作为主键，`name`
    和 `age`：
- en: '[PRE12]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, we must define the `insert_data` function, which inserts rows of data
    into the table:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们必须定义 `insert_data` 函数，该函数用于向表中插入数据行：
- en: '[PRE13]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we must use the following function to display the retrieved data:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须使用以下函数来显示检索到的数据：
- en: '[PRE14]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'At this point, the script will create a mock DataFrame containing sample data
    (names and ages):'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此时，脚本将创建一个包含示例数据（名称和年龄）的模拟 DataFrame：
- en: '[PRE15]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It establishes a connection to a PostgreSQL database using the specified connection
    parameters (database name, user, password, host, and port). These are the details
    we used in the previous step when we set up the database, so no change is required
    on your side:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它使用指定的连接参数（数据库名称、用户名、密码、主机和端口）建立与 PostgreSQL 数据库的连接。这些正是我们在之前的步骤中设置数据库时使用的详细信息，因此你这边无需做任何更改：
- en: '[PRE16]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, it checks whether a table named `example_table` exists, creates it
    if necessary, and then inserts the mock data into the table. After committing
    the changes to the database, the script fetches and prints the data from the table
    to confirm the successful insertion before finally closing the database connection:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它会检查名为 `example_table` 的表是否存在，如有必要会创建它，然后将模拟数据插入到表中。在提交更改到数据库后，脚本从表中获取数据并打印，以确认成功插入，最后关闭数据库连接：
- en: '[PRE17]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To execute the preceding script, just execute the following command in the
    `chapter7` folder:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行前面的脚本，只需在 `chapter7` 文件夹中执行以下命令：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Important Note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Remember to always close the connections as it helps you avoid performance issues
    and ensures that new connections can be established when needed. It allows the
    database to free up resources associated with the connection, and it ensures that
    any uncommitted transactions are handled properly. Closing a connection returns
    it to the connection pool, making it available for reuse by other parts of your
    application.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 记得始终关闭连接，因为这有助于避免性能问题，并确保在需要时能够建立新连接。它使数据库能够释放与连接相关的资源，并确保任何未提交的事务得到适当处理。关闭连接将其返回到连接池，使其可以被应用程序的其他部分重用。
- en: 'To see the table that was created in the database, you can open the PSQL process
    in your terminal and connect to the `learn_sql` database by executing the following
    command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看在数据库中创建的表，可以打开终端中的 PSQL 进程，并通过执行以下命令连接到 `learn_sql` 数据库：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, run the following command to list all the available tables:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行以下命令以列出所有可用的表：
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should see something similar to the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到类似如下内容：
- en: '![ Figure 7.1 – List tables in the database](img/B19801_07_1.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 列出数据库中的表](img/B19801_07_1.jpg)'
- en: Figure 7.1 – List tables in the database
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 列出数据库中的表
- en: 'You can also interact now with the table by executing the following SQL commands:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你还可以通过执行以下 SQL 命令与表进行交互：
- en: '![Figure 7.2 – Showing all the rows in the table](img/B19801_07_2.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 显示表中的所有行](img/B19801_07_2.jpg)'
- en: Figure 7.2 – Showing all the rows in the table
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 显示表中的所有行
- en: 'If you rerun the same Python script without dropping the existing table first,
    you won’t see a new table being created; instead, new rows will be added to the
    same table:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在不先删除现有表的情况下重新运行相同的Python脚本，你不会看到创建一个新表；相反，新的行会被添加到相同的表中：
- en: '![Figure 7.3 – Showing all the rows in the table once the script has been rerun](img/B19801_07_3.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 在脚本重新运行后显示表中的所有行](img/B19801_07_3.jpg)'
- en: Figure 7.3 – Showing all the rows in the table once the script has been rerun
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 在脚本重新运行后显示表中的所有行
- en: Having understood how to set up a relational database and use it as a sink by
    writing new data in it, let’s deep dive into the advantages and disadvantages
    of relational databases.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解如何设置关系型数据库并通过写入新数据将其用作存储后，我们深入探讨关系型数据库的优缺点。
- en: Advantages and disadvantages of relational databases
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关系型数据库的优缺点
- en: In this section, we’ll summarize the advantages and disadvantages of using RDBMSs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将总结使用关系型数据库管理系统（RDBMS）的优缺点。
- en: 'The advantages are as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 优点如下：
- en: RDBMS systems have ACID properties, providing a robust framework for reliable
    and secure transactions
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDBMS系统具有ACID属性，提供了一个强大的框架来保证可靠和安全的事务
- en: RDBMS technology has been around for decades, resulting in mature and well-established
    systems with extensive documentation and community support
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDBMS技术已经存在了几十年，产生了成熟且完善的系统，拥有丰富的文档和社区支持
- en: 'However, they also have various disadvantages:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们也有各种缺点：
- en: The rigid schema of RDBMS can be a limitation when dealing with evolving or
    dynamic data structures as they require schema modifications. Schema changes might
    be required for new data.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDBMS的严格模式在处理不断变化或动态数据结构时可能成为一种限制，因为它们需要模式修改。新数据可能需要进行模式更改。
- en: RDBMSs are primarily designed for structured data and may not be the best choice
    for handling unstructured or semi-structured data.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDBMS主要设计用于结构化数据，可能不适合处理非结构化或半结构化数据。
- en: If you’re wondering in what file type the data in the relational databases is
    written, then you’ll find the following subsection fascinating.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想关系型数据库中的数据是以什么文件类型存储的，那么接下来的子部分会让你觉得很有趣。
- en: Relational database file types
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关系型数据库文件类型
- en: In relational databases, the choice of file types for storing data is typically
    *abstracted* from users and developers, and it is not common to interact with
    the underlying files directly. Relational databases manage data storage and retrieval
    through their internal mechanisms, which often involve *proprietary* *file formats.*
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系型数据库中，存储数据的文件类型通常是*抽象*的，用户和开发人员不常直接与底层文件交互。关系型数据库通过其内部机制管理数据存储和检索，这些机制通常涉及*专有的*
    *文件格式*。
- en: The process of storing and organizing data within relational databases is managed
    by the DBMS, and users interact with the data using SQL or other query languages.
    The DBMS abstracts the physical storage details from users, providing a logical
    layer that allows for data manipulation and retrieval without direct concern for
    the underlying file formats.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系型数据库中，存储和组织数据的过程由数据库管理系统（DBMS）管理，用户使用SQL或其他查询语言与数据进行交互。DBMS将物理存储细节从用户中抽象出来，提供一个逻辑层，允许数据操作和检索，而无需直接关注底层文件格式。
- en: 'Let’s discuss the key points regarding file types in relational databases:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下关系型数据库中文件类型的关键点：
- en: Relational database vendors often use proprietary file formats for their data
    storage. Each database management system may have *its own internal structure
    and mechanisms for* *managing data*.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系型数据库供应商通常使用专有的文件格式来存储数据。每个数据库管理系统可能有*它自己的内部结构和机制来* *管理数据*。
- en: Relational databases typically organize data into *tablespaces*, which are logical
    storage containers. These tablespaces consist of pages or blocks where data is
    stored. The organization and structure of these pages are determined by the specific
    DBMS.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系型数据库通常将数据组织成*表空间*，这是逻辑存储容器。这些表空间由存储数据的页或块组成。页的组织和结构由特定的数据库管理系统（DBMS）决定。
- en: Relational databases prioritize ACID properties to ensure data integrity and
    reliability. The internal file formats are designed to support these transactional
    guarantees.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系型数据库优先考虑ACID属性，以确保数据完整性和可靠性。内部文件格式被设计用来支持这些事务性保证。
- en: Relational databases use various indexing and optimization techniques to enhance
    query performance. The internal file structures, including B-trees or other indexing
    structures, are optimized for efficient data retrieval.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系数据库使用各种索引和优化技术来提升查询性能。包括 B 树或其他索引结构在内的内部文件结构被优化以实现高效的数据检索。
- en: Users interact with relational databases using SQL commands.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户使用 SQL 命令与关系数据库进行交互。
- en: While users typically don’t interact directly with the underlying file formats,
    understanding the concepts of tablespaces, pages, and how the DBMS manages data
    storage can be useful for database administrators and developers when they’re
    optimizing performance or troubleshooting issues.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然用户通常不会直接与底层文件格式交互，但理解表空间、页面及数据库管理系统（DBMS）如何管理数据存储的概念，对于数据库管理员和开发人员在优化性能或排查问题时是非常有用的。
- en: Transitioning from an RDBMS to a **not only SQL** (**NoSQL**) database involves
    a shift in data modeling, schema design, and querying approaches. We’ll explore
    the differences in the following section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从关系数据库管理系统（RDBMS）迁移到**不仅仅是 SQL**（**NoSQL**）数据库涉及数据建模、模式设计和查询方法的转变。我们将在接下来的部分中探讨这些差异。
- en: NoSQL databases
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NoSQL 数据库
- en: NoSQL databases, also known as **not only SQL** or **non-SQL** databases, are
    a class of database systems that provide a flexible and scalable approach to handling
    and storing data. Unlike traditional relational databases, which enforce a structured
    schema with predefined tables, columns, and relationships, NoSQL databases are
    designed to handle various data models, accommodating different ways of structuring
    and organizing data and providing a more dynamic and adaptable approach to data
    modeling.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: NoSQL 数据库，也称为**不仅仅是 SQL**或**非 SQL**数据库，是一类提供灵活和可扩展的数据存储与处理方法的数据库系统。与传统的关系数据库不同，后者强制使用具有预定义表格、列和关系的结构化模式，NoSQL
    数据库旨在处理多种数据模型，适应不同的数据结构和组织方式，并提供更加动态和灵活的数据建模方法。
- en: Overview of NoSQL databases
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NoSQL 数据库概述
- en: 'Here’s a quick overview of the key components of NoSQL databases:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 NoSQL 数据库关键组件的快速概述：
- en: NoSQL databases often use a schemaless or schema-flexible approach, allowing
    data to be stored without the need for a predefined schema. This flexibility is
    particularly useful in cases where the data structure is evolving or not well-known
    in advance.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NoSQL 数据库通常采用无模式或灵活模式的方法，允许数据在没有预定义模式的情况下进行存储。这种灵活性在数据结构不断变化或无法预先确定的情况下尤其有用。
- en: NoSQL databases come in different types, each with its own data model, such
    as document-oriented (such as MongoDB), key-value stores (such as Redis), column-family
    stores (such as Apache Cassandra), and graph databases (such as Neo4j). Data models
    vary to accommodate different types of data and use cases. The document-oriented
    data model stores data as JSON documents, allowing each document to have a different
    structure, which is ideal for semi-structured or unstructured data. The key-value
    data model stores data as key-value pairs, with the value being a simple type
    or a complex structure, offering fast data retrieval but limited query capabilities.
    The column-family data model organizes data into columns instead of rows, enabling
    efficient storage and retrieval of large datasets. Lastly, the graph data model
    represents data as nodes and edges, making it perfect for applications focused
    on relationships, such as social networks and network analysis.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NoSQL 数据库有不同类型，每种类型都有自己的数据模型，如面向文档的（如 MongoDB）、键值存储（如 Redis）、列族存储（如 Apache Cassandra）和图数据库（如
    Neo4j）。数据模型因应不同类型的数据和使用场景而有所不同。面向文档的数据模型将数据存储为 JSON 文档，允许每个文档具有不同的结构，适用于半结构化或非结构化数据。键值数据模型将数据存储为键值对，其中值可以是简单类型或复杂结构，提供快速的数据检索，但查询能力有限。列族数据模型将数据按列而非行组织，使得存储和检索大规模数据集更加高效。最后，图数据模型将数据表示为节点和边，非常适合关注关系的应用，如社交网络和网络分析。
- en: NoSQL databases are typically designed for horizontal scalability, meaning they
    can efficiently distribute data across multiple nodes.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NoSQL 数据库通常设计为横向扩展，这意味着它们能够高效地将数据分布到多个节点。
- en: NoSQL databases often adhere to the **consistency, availability, and partition
    tolerance** (**CAP**) theorem, which states that a distributed system can provide
    – at most – two out of three of the guarantees. NoSQL databases may prioritize
    availability and partition tolerance over strict consistency.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NoSQL 数据库通常遵循 **一致性、可用性和分区容忍性**（**CAP**）定理，该定理表明分布式系统最多只能提供三项保证中的两项。NoSQL 数据库可能会优先考虑可用性和分区容忍性，而不是严格的一致性。
- en: 'In the data field, we usually find NoSQL databases as sinks in the following
    cases:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据领域，我们通常会发现 NoSQL 数据库作为以下情况下的“数据存储”：
- en: When we’re dealing with data models that may change frequently or aren’t well-defined
    in advance.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们处理的数据模型可能经常变化或事先定义不明确时。
- en: When an application anticipates or experiences rapid growth. In this case, horizontal
    scalability is essential.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个应用程序预见到或经历快速增长时，在这种情况下，水平可扩展性至关重要。
- en: When the data can’t be put into tables with fixed relationships. In this case,
    a more flexible storage model is needed.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据无法放入具有固定关系的表中时，这时需要一种更灵活的存储模型。
- en: When rapid development and iteration are critical. In this case, we need to
    modify the data model on the fly.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当快速开发和迭代至关重要时，在这种情况下，我们需要随时修改数据模型。
- en: When the specific features and capabilities of a particular type of NoSQL database
    align with the requirements of the application (e.g., document-oriented for content-heavy
    applications, key-value stores for caching, and graph databases for relationship-centric
    data).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当某一特定类型的 NoSQL 数据库的具体特性和功能与应用程序的需求相符时（例如，面向内容的应用程序使用面向文档的数据库，缓存使用键值存储，关系数据使用图数据库）。
- en: Let’s see an example of how to connect to a NoSQL database and write a new table.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个如何连接到 NoSQL 数据库并写入新表的示例。
- en: An example of a MongoDB database
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个 MongoDB 数据库的示例
- en: 'Before diving into the code, we’ll take some time to explain MongoDB and some
    important concepts related to it:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码之前，我们先花些时间来解释 MongoDB 以及与之相关的一些重要概念：
- en: '**Document**: This is the basic unit of data in MongoDB and is represented
    as a **Binary JSON** (**BSON**) object. Documents are like rows in a relational
    database but can have varying structures. Documents are composed of fields (key-value
    pairs). Each field can contain different data types, such as strings, numbers,
    arrays, or nested documents.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档**：这是 MongoDB 中的数据基本单元，以 **二进制 JSON**（**BSON**）对象的形式表示。文档类似于关系数据库中的行，但可以具有不同的结构。文档由字段（键值对）组成。每个字段可以包含不同的数据类型，例如字符串、数字、数组或嵌套文档。'
- en: '**Collection**: A grouping of MongoDB documents, analogous to a table in a
    relational database. Collections contain documents and serve as the primary method
    of organizing data. Collections don’t require a predefined schema, allowing documents
    within the same collection to have different structures.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合**：MongoDB 文档的集合，类似于关系数据库中的表。集合包含文档，并作为组织数据的主要方法。集合不需要预定义的架构，这使得同一集合中的文档可以具有不同的结构。'
- en: '**Database**: A container for collections. MongoDB databases hold collections
    and serve as the highest level of data organization. Each database is isolated
    from others, meaning operations in one database don’t affect others.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据库**：集合的容器。MongoDB 数据库包含集合，并作为数据组织的最高层级。每个数据库与其他数据库隔离，这意味着一个数据库中的操作不会影响其他数据库。'
- en: 'Now that we have a better understanding, let’s go through the code. To run
    this example, set up MongoDB locally by following the documentation for your OS.
    For mac instructions, go here: [https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/](https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/).
    The following code example shows how to create a database and then write data
    in MongoDB using `pymongo`. Note that `pymongo` is the official Python driver
    for MongoDB and provides a Python interface for connecting to MongoDB databases,
    executing queries, and manipulating data using Python scripts.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对这些概念有了更清晰的理解，让我们来看看代码。要运行这个示例，请按照操作系统的文档在本地设置 MongoDB。对于 Mac 的安装说明，请访问这里：[https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/](https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/)。以下代码示例展示了如何创建数据库并使用
    `pymongo` 在 MongoDB 中写入数据。请注意，`pymongo` 是 MongoDB 的官方 Python 驱动程序，提供了一个 Python
    接口来连接 MongoDB 数据库，执行查询，并通过 Python 脚本操作数据。
- en: 'Let’s get started:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧：
- en: 'After installing MongoDB, open your Terminal and start the service. The commands
    presented here are for Mac; follow the commands in the documentation for your
    OS:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完 MongoDB 后，打开你的终端并启动服务。这里提供的命令适用于 Mac；请根据你的操作系统，参考文档中的命令：
- en: '[PRE21]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Validate whether the service is running by executing the following command:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令验证服务是否正在运行：
- en: '[PRE22]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Having finished with the installations, let’s set up a MongoDB database.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安装完成后，我们来设置一个 MongoDB 数据库。
- en: 'In your Terminal, type the following command to go into the MongoDB editor:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的终端中，输入以下命令进入 MongoDB 编辑器：
- en: '[PRE23]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'best_collection_ever:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'best_collection_ever:'
- en: '[PRE24]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You should see a response similar to the following:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到类似以下的响应：
- en: '[PRE25]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'At this point, we’re ready to switch to Python and start adding data to this
    collection. You can find the code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/2.pymongo.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/2.pymongo.py).
    Follow these steps:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经准备好切换到 Python，并开始向这个集合中添加数据。你可以在这里找到代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/2.pymongo.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/2.pymongo.py)。请按照以下步骤操作：
- en: 'First, let’s import the required libraries:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库：
- en: '[PRE27]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Every time we connect to a NoSQL database, we need to provide the connection
    details. Update all the values for the parameters in the `mongo_params` dictionary,
    which contains the MongoDB server host, port, username, password, and authentication
    source:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次连接到 NoSQL 数据库时，我们需要提供连接详情。更新 `mongo_params` 字典中的所有参数值，字典包含了 MongoDB 服务器主机、端口、用户名、密码和认证源：
- en: '[PRE28]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s look at the different functions we will use in this example to insert
    the documents into the MongoDB database. The first function checks whether a collection
    exists in the database before creating a new one:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看一下在本例中用于将文档插入 MongoDB 数据库的不同函数。第一个函数在创建新集合之前，会检查集合是否存在于数据库中：
- en: '[PRE29]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following function takes a database and collection name as arguments and
    creates a collection with the name we pass (in our case, we provided `collection_name`
    as the name):'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下函数将数据库和集合名称作为参数，并创建一个我们传递的名称的集合（在我们的例子中，我们提供了`collection_name`作为名称）：
- en: '[PRE30]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we will take the collection we created in the previous step, which
    is just a placeholder for now, and insert some data into it:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用之前创建的集合，它现在只是一个占位符，并向其中插入一些数据：
- en: '[PRE31]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s create some data to be inserted into the collection:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一些要插入到集合中的数据：
- en: '[PRE32]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let’s specify the parameters that are required for the connection and create
    it:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们指定连接所需的参数并创建连接：
- en: '[PRE33]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let’s check whether the collection with the provided name exists. If the
    collection exists, use the existing collection; if not, create a new collection
    with the provided name:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们检查是否存在具有提供名称的集合。如果集合存在，则使用现有集合；如果不存在，则创建一个新的集合并使用提供的名称：
- en: '[PRE34]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, take the collection and insert the provided data:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，获取集合并插入提供的数据：
- en: '[PRE35]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, close the MongoDB connection:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，关闭 MongoDB 连接：
- en: '[PRE36]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After executing this script, you should be able to see the records being added
    to the collection:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此脚本后，你应该能够看到记录被添加到集合中：
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This script demonstrates how to interact with MongoDB databases and collections.
    Unlike relational databases, MongoDB does not require *tables or schemas to be
    created upfront*. Instead, you work directly with databases and collections. As
    an exercise to understand more about this flexible data model, try inserting data
    into the collection of a different structure. This contrasts with relational databases,
    where you insert rows into a *table with a fixed schema*. You can find an example
    here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/3.pymongo_expand.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/3.pymongo_expand.py).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本脚本演示了如何与 MongoDB 数据库和集合进行交互。与关系型数据库不同，MongoDB 不需要*预先创建表或模式*。相反，你直接与数据库和集合进行操作。作为练习，为了更好地理解这种灵活的数据模型，尝试将不同结构的数据插入集合中。这与关系型数据库不同，后者是向*具有固定模式的表*中插入行。你可以在这里找到一个示例：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/3.pymongo_expand.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/3.pymongo_expand.py)。
- en: Finally, the script uses the `find` method to query and retrieve documents from
    a collection. MongoDB queries are more flexible compared to SQL queries, especially
    for nested data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，脚本使用`find`方法查询并从集合中检索文档。与SQL查询相比，MongoDB的查询更加灵活，特别是在处理嵌套数据时。
- en: Don’t delete the MongoDB database
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 不要删除MongoDB数据库
- en: Please don’t clean the Mongo resources we created as we will use them in the
    streaming sink example. We will clean up all the resources at the end of this
    chapter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要清理我们创建的Mongo资源，因为我们将在流式接收示例中使用它们。我们将在本章结束时清理所有资源。
- en: In the next section, we will discuss the advantages and disadvantages that NoSQL
    databases offer.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论NoSQL数据库所提供的优缺点。
- en: Advantages and disadvantages of NoSQL databases
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NoSQL数据库的优缺点
- en: Let’s summarize the advantages and disadvantages of using NoSQL systems.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下使用NoSQL系统的优缺点。
- en: 'The advantages of NoSQL databases are as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: NoSQL数据库的优点如下：
- en: '**Scalability**: NoSQL databases are designed for horizontal scalability, allowing
    them to handle large volumes of data by distributing it across multiple servers.
    This makes them particularly suitable for big data applications and cloud environments.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：NoSQL数据库设计时考虑了水平扩展性，可以通过将数据分布到多个服务器来处理大量数据。这使得它们特别适用于大数据应用和云环境。'
- en: '**Flexibility**: Unlike SQL databases, which require a fixed schema, NoSQL
    databases offer flexible schemas. This allows structured, semi-structured, and
    unstructured data to be stored, making it easier to adapt to changing data models
    without significant restructuring.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：与需要固定模式的SQL数据库不同，NoSQL数据库提供灵活的模式。这使得结构化、半结构化和非结构化数据可以存储，从而更容易适应不断变化的数据模型，而无需进行大规模的重构。'
- en: '**Performance**: NoSQL databases can offer superior performance for certain
    types of queries, especially when dealing with large datasets. They’re often optimized
    for high-speed data retrieval and can handle large volumes of transactions per
    second.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：NoSQL数据库在处理某些类型的查询时，尤其是处理大数据集时，可能提供更优的性能。它们通常针对高速数据检索进行了优化，并能处理每秒大量的事务。'
- en: '**Cost-effectiveness**: Many NoSQL databases are open source and can be scaled
    using commodity hardware, which reduces costs compared to the expensive hardware
    often required for scaling SQL databases.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：许多NoSQL数据库是开源的，并且可以使用普通硬件进行扩展，这相比于通常需要昂贵硬件来扩展SQL数据库的成本要低。'
- en: '**Developer agility**: The flexibility in schema and data models allows developers
    to iterate quickly and adapt to new requirements without the need for extensive
    database administration'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发者灵活性**：模式和数据模型的灵活性使得开发者可以快速迭代，适应新的需求，而无需进行大量的数据库管理。'
- en: 'However, they also have their disadvantages:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们也有一些缺点：
- en: '**Lack of standardization**: NoSQL databases don’t have a standardized query
    language such as SQL. This can lead to a steeper learning curve and make it challenging
    to switch between different NoSQL systems.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏标准化**：NoSQL数据库没有像SQL那样的标准化查询语言。这可能导致学习曲线较陡，并且使得在不同NoSQL系统之间切换变得具有挑战性。'
- en: '**Limited support for complex queries**: NoSQL databases generally lack the
    advanced querying capabilities of SQL databases, such as joins and complex transactions,
    which can limit their use in applications that require complex data relationships.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的复杂查询支持**：NoSQL数据库通常缺乏SQL数据库的高级查询功能，如连接和复杂事务，这可能限制它们在需要复杂数据关系的应用中的使用。'
- en: '**Data consistency**: Many NoSQL databases prioritize availability and partition
    tolerance over consistency (as per the CAP theorem). This can lead to eventual
    consistency models, which may not be suitable for applications that require strict
    data integrity.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据一致性**：许多NoSQL数据库优先考虑可用性和分区容忍性，而不是一致性（根据CAP定理）。这可能导致最终一致性模型，这对于需要严格数据完整性的应用可能不适用。'
- en: '**Maturity and community support**: NoSQL databases are relatively newer compared
    to SQL databases, which means they may have less mature ecosystems and smaller
    communities. This can make finding support and resources more challenging.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成熟度和社区支持**：与SQL数据库相比，NoSQL数据库相对较新，这意味着它们可能拥有不那么成熟的生态系统和较小的社区。这可能使得寻找支持和资源变得更加困难。'
- en: '**Complex maintenance**: The distributed nature of NoSQL databases can lead
    to complex maintenance tasks, such as data distribution and load balancing, which
    require specialized knowledge'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂的维护**：NoSQL数据库的分布式特性可能导致复杂的维护任务，如数据分布和负载均衡，这些任务需要专业的知识。'
- en: Now, let’s discuss the file formats we may encounter when we’re working with
    NoSQL databases.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论在使用 NoSQL 数据库时可能遇到的文件格式。
- en: NoSQL database file types
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NoSQL 数据库文件类型
- en: 'The most prevalent file formats are JSON and BSON. JSON is a lightweight, human-readable
    data interchange format that uses a key-value pair structure and supports nested
    data structures. It’s widely adopted for web-based data exchange due to its simplicity
    and ease of parsing. JSON is language-agnostic, making it suitable for various
    programming languages. JSON’s flexible and schemaless nature aligns well with
    the flexible schema approach of many NoSQL databases and it allows for easy handling
    of evolving data structures. NoSQL databases often deal with semi-structured or
    unstructured data, and JSON’s hierarchical structure accommodates such data well.
    Here’s an example of a JSON data file:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的文件格式是 JSON 和 BSON。JSON 是一种轻量级的、易于人类阅读的数据交换格式，采用键值对结构，并支持嵌套数据结构。由于其简洁性和易于解析，JSON
    被广泛应用于基于 Web 的数据交换。JSON 是语言无关的，适用于各种编程语言。JSON 的灵活性和无模式特性与许多 NoSQL 数据库的灵活模式方法相契合，允许轻松处理不断变化的数据结构。NoSQL
    数据库通常处理半结构化或非结构化数据，JSON 的层级结构能够很好地适应这种数据。以下是一个 JSON 数据文件的示例：
- en: '[PRE38]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'BSON is a binary-encoded serialization for JSON-like documents and is designed
    to be efficient for storage and traversal. It adds additional data types not present
    in JSON, such as `date` and `binary`. BSON files are encoded before they’re stored
    and decoded before they’re displayed. BSON’s binary format is more efficient for
    storage and serialization, making it suitable for scenarios where data needs to
    be represented compactly. BSON is the primary data format that’s used in MongoDB.
    Let’s have a look at the BSON representation of the file presented previously:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: BSON 是一种二进制编码的序列化格式，用于类似 JSON 的文档，旨在提高存储和遍历的效率。它增加了 JSON 中没有的数据类型，如 `date` 和
    `binary`。BSON 文件在存储前会被编码，在显示前会被解码。BSON 的二进制格式更适合存储和序列化，因此在需要紧凑表示数据的场景中非常合适。BSON
    是 MongoDB 中使用的主要数据格式。让我们来看一下之前展示的文件的 BSON 表示：
- en: '[PRE39]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In NoSQL databases, the choice between JSON and BSON often depends on the specific
    requirements of the database and the use case. While JSON is more human-readable
    and easy to work with in many scenarios, BSON’s binary efficiency is beneficial
    in certain contexts, particularly where storage and serialization efficiency are
    critical.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NoSQL 数据库中，选择 JSON 还是 BSON 通常取决于数据库的具体需求和使用场景。虽然 JSON 在许多场景下更易于人类阅读和操作，但 BSON
    的二进制效率在某些情况下，特别是在存储和序列化效率至关重要时，具有优势。
- en: In the next section, we will discuss data warehouses, what challenges they solve,
    and which use cases you should consider implementing when using them.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论数据仓库，它们解决了哪些挑战，以及在使用时应该考虑实施的应用场景。
- en: Data warehouses
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库
- en: Transitioning to a data warehouse becomes important when the volume and complexity
    of data, as well as the need for advanced analytics, surpass the capabilities
    of your existing relational or NoSQL databases. If your relational database struggles
    with large data volumes, complex queries, or performance issues during analytical
    processing, a data warehouse can offer optimized storage and query performance
    for such workloads. Similarly, NoSQL databases, while excellent for handling unstructured
    or semi-structured data and scaling horizontally, may lack the sophisticated query
    capabilities and performance needed for in-depth analytics and reporting. Data
    warehouses are designed to integrate data from multiple sources, including both
    relational and NoSQL databases, facilitating comprehensive analysis and reporting.
    They provide robust support for historical data analysis, complex queries, and
    data governance, making them an ideal solution when you need to enhance your data
    integration, analytics, and reporting capabilities beyond what traditional databases
    offer.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据的体积和复杂性以及对高级分析的需求超出了现有关系型或 NoSQL 数据库的能力时，转向数据仓库变得非常重要。如果您的关系型数据库在处理大量数据、复杂查询或在分析处理期间遇到性能问题，数据仓库可以为此类工作负载提供优化的存储和查询性能。同样，NoSQL
    数据库虽然在处理非结构化或半结构化数据以及横向扩展方面表现优异，但可能缺乏深度分析和报告所需的复杂查询能力和性能。数据仓库旨在整合来自多个来源的数据，包括关系型和
    NoSQL 数据库，促进全面分析和报告。它们为历史数据分析、复杂查询和数据治理提供强有力的支持，使其成为在需要提升数据整合、分析和报告能力时的理想解决方案，超越了传统数据库的能力。
- en: Overview of data warehouses
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据仓库概览
- en: A data warehouse is a specialized database system designed for storing, organizing,
    and retrieving *large volumes of data* that are used for business intelligence
    and analytics efficiently. Unlike transactional databases, which are optimized
    for quick data updates and individual record retrievals, data warehouses are structured
    to support complex queries, aggregations, and reporting on historical and current
    data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库是一种专门的数据库系统，旨在高效地存储、组织和检索*大量数据*，这些数据用于商业智能和分析。与优化用于快速数据更新和单个记录检索的事务性数据库不同，数据仓库的结构是为了支持复杂的查询、聚合和对历史及当前数据的报告。
- en: 'Here’s a quick overview of the key components of data warehouses:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据仓库关键组件的快速概述：
- en: Various data sources contribute to a data warehouse, including transactional
    databases, external files, logs, and more.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种数据源为数据仓库提供数据，包括事务性数据库、外部文件、日志等。
- en: '**Extract, transform, and load** (**ETL**) processes are used to gather data
    from source systems, transform it into a consistent format, and load it into the
    data warehouse.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取、转换和加载**（**ETL**）过程用于从源系统收集数据，将其转换为一致的格式，并将其加载到数据仓库中。'
- en: Data warehouses employ optimized storage methods, such as columnar storage,
    to store large volumes of data efficiently.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库采用优化的存储方法，例如列式存储，以高效地存储大量数据。
- en: Indexes and pre-aggregated tables are used to optimize query performance. In
    a data warehouse, indexes play a crucial role in optimizing query performance.
    An index is a data structure that enhances the speed and efficiency of data retrieval
    from a table by creating a separate, organized subset of the data. Indexes are
    typically created on one or more columns to facilitate faster querying. Without
    indexes, the database must scan the entire table to locate relevant rows. Indexes
    help the database quickly find rows that meet query conditions. Common candidates
    for indexing include columns used in `WHERE` clauses, `JOIN` conditions, and `ORDER
    BY` clauses. However, over-indexing can lead to diminishing returns and increased
    maintenance overhead. While indexes improve query performance, they also consume
    additional storage and can slow down write operations due to the need to maintain
    the index.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索引和预聚合表用于优化查询性能。在数据仓库中，索引在优化查询性能方面起着至关重要的作用。索引是一种数据结构，通过创建数据的独立、有序子集来提高从表中检索数据的速度和效率。索引通常在一个或多个列上创建，以便加速查询。没有索引时，数据库必须扫描整个表才能定位相关行。索引帮助数据库快速找到符合查询条件的行。常见的索引候选列包括`WHERE`子句、`JOIN`条件和`ORDER
    BY`子句中使用的列。然而，过度索引可能导致收益递减并增加维护负担。虽然索引提高了查询性能，但它们也消耗额外的存储空间，并且由于需要维护索引，可能会减慢写操作的速度。
- en: Techniques such as parallel processing and indexing are employed to enhance
    the speed of analytical queries.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如并行处理和索引等技术被用来提高分析查询的速度。
- en: Integration with business intelligence tools allows users to create reports
    and dashboards and perform data analysis.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与商业智能工具的集成允许用户创建报告和仪表板，并执行数据分析。
- en: Data is organized using multidimensional models, often in the form of star or
    snowflake schemas, to support analytics and reporting.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据通过多维模型进行组织，通常以星型或雪花型模式的形式出现，以支持分析和报告。
- en: Let’s expand on dimensional modeling. It’s a design technique that’s used in
    data warehousing to structure data so that it supports efficient retrieval for
    analytical queries and reporting. Unlike traditional relational models, dimensional
    models are optimized for query performance and ease of use. In the next section,
    we will present the main schema types in dimensional models.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们扩展一下维度建模。它是一种在数据仓库中用于结构化数据的设计技术，使得数据能够支持高效的分析查询和报告。与传统的关系模型不同，维度模型经过优化，专注于查询性能和易用性。在接下来的部分中，我们将介绍维度模型中的主要模式类型。
- en: Schema types in dimensional models
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度模型中的模式类型
- en: 'Dimensional modeling primarily involves two types of schemas: the star schema
    and the snowflake schema. The **star schema** is the simplest form of dimensional
    modeling and is where a central fact table is directly connected to multiple dimension
    tables, forming a star-like structure. This schema type is highly intuitive and
    easy to navigate, making it ideal for straightforward queries and reporting. Each
    dimension table in a star schema contains a primary key that maps to a foreign
    key in the fact table, providing descriptive context to the quantitative data
    in the fact table. For instance, a sales star schema might include a central sales
    fact table with foreign keys linking to dimension tables for products, customers,
    time, and stores, thereby simplifying complex queries by reducing the number of
    joins:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 维度建模主要涉及两种模式：星型模式和雪花模式。**星型模式**是最简单的维度建模形式，其中一个中心事实表直接连接到多个维度表，形成类似星星的结构。这种模式直观易懂，便于导航，非常适合简单的查询和报告。每个星型模式中的维度表包含一个主键，与事实表中的外键相关联，为事实表中的定量数据提供描述性上下文。例如，销售星型模式可能包括一个中心销售事实表，外键链接到产品、客户、时间和商店等维度表，从而通过减少连接的数量简化复杂查询：
- en: '![Figure 7.4 – Star schema](img/B19801_07_4.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 星型模式](img/B19801_07_4.jpg)'
- en: Figure 7.4 – Star schema
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 星型模式
- en: 'On the other hand, the **snowflake schema** is a more normalized version of
    the star schema and is where dimension tables are further broken down into related
    sub-tables, resembling a snowflake pattern. This structure reduces data redundancy
    and can save storage space, although it introduces more complexity in query design
    due to the additional joins required. For example, a product dimension table in
    a snowflake schema might be normalized into separate tables for product categories
    and brands, creating a multi-layered structure that ensures higher data integrity
    and reduces update anomalies. While the snowflake schema may be slightly more
    complex to query, it offers benefits in terms of data maintenance and scalability,
    especially in environments where data consistency and storage optimization are
    critical:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**雪花模式**是星型模式的一个更规范化版本，在这种模式下，维度表会进一步拆分成相关的子表，形成类似雪花的结构。这种结构减少了数据冗余，可以节省存储空间，尽管由于需要额外的连接，它在查询设计上引入了更多的复杂性。例如，雪花模式中的产品维度表可能被规范化为独立的产品类别和品牌表，从而形成多层次结构，确保更高的数据完整性并减少更新异常。虽然雪花模式在查询时可能稍显复杂，但在数据维护和可扩展性方面提供了优势，尤其是在数据一致性和存储优化至关重要的环境中：
- en: '![Figure 7.5 – Snowflake schema](img/B19801_07_5.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 雪花模式](img/B19801_07_5.jpg)'
- en: Figure 7.5 – Snowflake schema
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 雪花模式
- en: Star schemas are often preferred for simpler hierarchies and when query performance
    is a higher priority, while snowflake schemas may be chosen when more efficient
    use of storage and achieving a higher level of normalization is essential.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 星型模式通常用于较简单的层次结构，并且当查询性能优先时，而雪花模式则可能在更高效地使用存储和实现更高程度的规范化时被选用。
- en: Transitioning from understanding schema types in dimensional modeling, it’s
    essential to explore the diverse options available for implementing and leveraging
    data warehouses in various organizational contexts.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解维度建模中的模式类型之后，探索在各种组织环境中实施和利用数据仓库的多种选择至关重要。
- en: Data warehouse solutions
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据仓库解决方案
- en: 'There are many different data warehouse options out there. We’ve summarized
    the main ones in the following table:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上有许多不同的数据仓库选择。我们在下表中总结了主要的选项：
- en: '| **Data Warehouse** | **Description** |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| **数据仓库** | **描述** |'
- en: '| Databricks SQL | A cloud-based, serverless data warehouse that brings warehouse
    capabilities to data lakes. It’s also known for its scalability, performance,
    and parallel processing, as well as its built-in machine learning capabilities.
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Databricks SQL | 一种基于云的、无服务器的数据仓库，将仓库功能带入数据湖。它以其可扩展性、性能和并行处理能力而闻名，并且具备内建的机器学习功能。
    |'
- en: '| Amazon Redshift | A fully managed, scalable data warehouse service in the
    cloud. It’s optimized for high-performance analytics. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Amazon Redshift | 一项完全托管的、可扩展的云数据仓库服务，优化用于高性能分析。 |'
- en: '| Snowflake | A cloud-based data warehouse with a multi-cluster, shared architecture
    that supports diverse workloads. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Snowflake | 一种基于云的数据仓库，具有多集群共享架构，支持多种工作负载。 |'
- en: '| Google BigQuery | A serverless, highly scalable data warehouse with built-in
    machine learning capabilities. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Google BigQuery | 一种无服务器、高度可扩展的数据仓库，具有内置的机器学习功能。 |'
- en: '| **Data Warehouse** | **Description** |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| **数据仓库** | **描述** |'
- en: '| Teradata | An on-premises or cloud-based data warehouse known for its scalability,
    performance, and parallel processing. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Teradata | 一种支持扩展性、性能和并行处理的本地或基于云的数据仓库。 |'
- en: '| Microsoft Azure Synapse Analytics | A cloud-based analytics service that
    offers both on-demand and provisioned resources. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Microsoft Azure Synapse Analytics | 一种基于云的分析服务，提供按需和预配资源。 |'
- en: Table 7.2 – Data warehousing solutions
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.2 – 数据仓库解决方案
- en: In the next section, we will look at an example of creating a BigQuery table
    to illustrate the practical application of data warehouses.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将查看创建 BigQuery 表的示例，以说明数据仓库的实际应用。
- en: An example of a data warehouse
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据仓库示例
- en: 'Let’s learn how to create a new table in BigQuery. Google Cloud provides a
    client library for various programming languages, including Python, to interact
    with BigQuery. To get the ready so that you can run the following example, go
    to the BigQuery documentation: [https://cloud.google.com/python/docs/reference/bigquery/latest](https://cloud.google.com/python/docs/reference/bigquery/latest).
    Let’s deep dive into the example. We will follow the same patterns that have been
    presented so far in this chapter. Let’s get started:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来学习如何在 BigQuery 中创建一个新表。Google Cloud 为包括 Python 在内的多种编程语言提供了与 BigQuery 交互的客户端库。为了准备好并运行以下示例，请访问
    BigQuery 文档：[https://cloud.google.com/python/docs/reference/bigquery/latest](https://cloud.google.com/python/docs/reference/bigquery/latest)。让我们深入研究这个示例。我们将按照本章至今介绍的相同模式进行操作。让我们开始吧：
- en: Note
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To run this example, you need to have a **Google Cloud Platform** (**GCP**)
    account and a Google Storage bucket ready.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，您需要拥有一个**Google Cloud Platform**（**GCP**）账户，并准备好一个 Google Storage 存储桶。
- en: 'Import the required libraries:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE40]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'First, we’ll set up the project ID. Replace `your_project_id` with your actual
    values:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将设置项目 ID。将 `your_project_id` 替换为你的实际值：
- en: '[PRE41]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define the dataset and table name and update the following fields:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据集和表名称，并更新以下字段：
- en: '[PRE42]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Check whether the dataset and the table exist:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据集和表是否存在：
- en: '[PRE43]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the table schema (replace it with your schema if you wish to update
    the data). In this example, we will create a table with two columns (`column1`
    and `column2`). The first column will be of the `STRING` type, while the second
    one will be of the `INTEGER` type. The first column can’t contain missing values,
    whereas the second one can:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义表的模式（如果你希望更新数据，可以替换为你的模式）。在本例中，我们将创建一个包含两列（`column1` 和 `column2`）的表。第一列将是
    `STRING` 类型，第二列将是 `INTEGER` 类型。第一列不能包含缺失值，而第二列可以：
- en: '[PRE44]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let’s perform the checks for the existence of a table with the same name. If
    the table doesn’t exist, then we create it with the provided name:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查是否存在同名的表。如果表不存在，则使用提供的名称创建它：
- en: '[PRE45]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let’s create some mock data that will be inserted into the table:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一些将被插入到表中的模拟数据：
- en: '[PRE46]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Construct the data to be inserted:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建要插入的数据：
- en: '[PRE47]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Insert the data and check for errors. If everything works as expected, close
    the connection:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入数据并检查是否有错误。如果一切按预期工作，关闭连接：
- en: '[PRE48]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If any insertion errors occur, print them out:'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果发生任何插入错误，打印出错误信息：
- en: '[PRE49]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Close the BigQuery client:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭 BigQuery 客户端：
- en: '[PRE50]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: What’s the difference between a container and a table?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 容器和表有什么区别？
- en: A container (called different things in different systems, such as database,
    dataset, or schema) is a logical grouping mechanism that’s used to organize and
    manage data objects such as tables, views, and related metadata. Containers provide
    a way to partition and structure data based on specific needs, such as access
    control, data governance, or logical separation of data domains. On the other
    hand, a table is a fundamental data structure that stores the actual data records,
    organized into rows and columns. Tables define the schema (column names and data
    types) and hold the data values.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 容器（在不同系统中有不同的名称，如数据库、数据集或模式）是一种逻辑分组机制，用于组织和管理数据对象，例如表、视图和相关的元数据。容器提供了一种基于特定需求（如访问控制、数据治理或数据域的逻辑分离）分区和结构化数据的方式。另一方面，表是一种基本的数据结构，用于存储实际的数据记录，按行和列组织。表定义了模式（列名称和数据类型），并存储数据值。
- en: At this point, let’s transition from understanding the fundamental components
    of a data warehouse environment to discussing the advantages and disadvantages
    that data warehouses offer in managing and analyzing large volumes of data efficiently.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们从了解数据仓库环境的基本组件转向讨论数据仓库在高效管理和分析大量数据时所提供的优缺点。
- en: Advantages and disadvantages of data warehouses
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据仓库的优缺点
- en: Let’s summarize the advantages and disadvantages of using data warehouses.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下使用数据仓库的优缺点。
- en: 'The advantages are as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 优点如下：
- en: Optimized for analytical queries and large-scale data processing
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对分析查询和大规模数据处理进行了优化
- en: Can handle massive amounts of data
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理海量数据
- en: Integration with other data tools and services
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他数据工具和服务的集成
- en: 'Here are their disadvantages:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是它们的缺点：
- en: Higher costs for storage and querying compared to traditional databases
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比传统数据库，存储和查询的成本更高
- en: Might have limitations regarding real-time data processing
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能在实时数据处理方面存在限制
- en: File types in data warehouses
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据仓库中的文件类型
- en: In terms of file formats, it’s accurate to say that many modern data warehouses
    use **proprietary** internal storage formats for writing data. These proprietary
    formats are usually columnar storage formats that are optimized for efficient
    querying and analytics.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 就文件格式而言，可以准确地说，许多现代数据仓库使用**专有**的内部存储格式来写入数据。这些专有格式通常是列式存储格式，针对高效查询和分析进行了优化。
- en: 'Let’s have a look at the differences we may find in these proprietary formats:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些专有格式可能带来的差异：
- en: Data warehouses often use columnar storage formats such as Parquet, ORC, or
    Avro. While these formats are open and widely adopted, each data warehouse might
    have its internal optimizations or extensions.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库通常使用如 Parquet、ORC 或 Avro 等列式存储格式。虽然这些格式是开放且广泛采用的，但每个数据仓库可能有其内部优化或扩展。
- en: The actual implementations of these columnar storage formats may have vendor-specific
    optimizations or features. For example, how a specific data warehouse handles
    compression, indexing, and metadata might be specific to the vendor.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些列式存储格式的实际实现可能具有供应商特定的优化或功能。例如，某个特定数据仓库如何处理压缩、索引和元数据可能是特定于该供应商的。
- en: Users interact with data warehouses through standard interfaces, such as SQL
    queries. The choice of storage format often doesn’t affect users, so long as the
    data warehouse supports common data interchange formats for import and export.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户通过标准接口与数据仓库进行交互，例如 SQL 查询。存储格式的选择通常不会影响用户，只要数据仓库支持常见的数据交换格式用于导入和导出。
- en: So, while the internal storage mechanisms may have vendor-specific optimizations,
    the use of well-established, open, and widely adopted columnar storage formats
    ensures a degree of interoperability and flexibility. Users typically interact
    with data warehouses using standard SQL queries or data interchange formats such
    as CSV, JSON, or Avro for data import/export, which adds a layer of standardization
    to the external-facing aspects of these systems.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管内部存储机制可能有供应商特定的优化，但使用公认的、开放的且广泛采用的列式存储格式，确保了一定程度的互操作性和灵活性。用户通常使用标准 SQL
    查询或数据交换格式（如 CSV、JSON 或 Avro）与数据仓库交互进行数据的导入/导出，这为这些系统的外部接口提供了一层标准化。
- en: Transitioning from traditional data warehouses to data lakes represents a strategic
    shift that embraces a more flexible and scalable paradigm. In the next section,
    we will deep dive into data lakes.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 从传统数据仓库过渡到数据湖代表了一种战略转变，拥抱更灵活和可扩展的范式。在接下来的章节中，我们将深入探讨数据湖。
- en: Data lakes
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据湖
- en: The transition from traditional data warehouses to data lakes represents a shift
    in how organizations handle and analyze their data. Traditional data warehouses
    are designed to store structured data, which is highly organized and formatted
    according to a predefined schema, such as tables with rows and columns in relational
    databases. Structured data is easy to query and analyze using SQL. However, data
    warehouses struggle with handling unstructured data, which lacks a predefined
    format or organization. Examples of unstructured data include text documents,
    emails, images, videos, and social media posts. Data lakes, on the other hand,
    offer a more flexible and scalable solution by storing both structured and unstructured
    data in its native format. This allows organizations to ingest and store vast
    amounts of data without the need for immediate structuring. This transition addresses
    the limitations of data warehouses, offering a more versatile and future-proof
    approach to data management.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 从传统的数据仓库到数据湖的过渡，代表了组织处理和分析数据方式的转变。传统的数据仓库设计用于存储结构化数据，这些数据高度组织，并根据预定义的模式格式化，例如关系数据库中的表格和行列。结构化数据便于使用
    SQL 进行查询和分析。然而，数据仓库在处理非结构化数据时遇到困难，非结构化数据没有预定义的格式或组织方式。非结构化数据的例子包括文本文件、电子邮件、图像、视频和社交媒体帖子。而数据湖则提供了一种更灵活、可扩展的解决方案，通过存储结构化和非结构化数据的原始格式，使组织能够摄取和存储大量数据，而无需立即进行结构化。这一过渡解决了数据仓库的局限性，提供了一种更具多样性和未来可扩展性的数据管理方法。
- en: Overview of data lakes
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据湖概述
- en: 'A data lake is a centralized repository that allows organizations to store
    vast amounts of raw and unstructured data in any format needed. They’re designed
    to handle diverse data types, such as structured, semi-structured, and unstructured
    data, and enable data exploration, analytics, and machine learning. Data lakes
    solve several problems: they enable the consolidation of diverse data sources,
    break down silos by providing a unified data storage solution, and support advanced
    analytics by making all types of data easily accessible.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是一个集中式存储库，允许组织存储大量原始和非结构化数据，且可以存储任何所需格式的数据。它们被设计用来处理多种数据类型，如结构化、半结构化和非结构化数据，并支持数据探索、分析和机器学习。数据湖解决了多个问题：它们使得不同数据源能够统一存储，打破了信息孤岛，并通过提供易于访问的所有类型数据支持高级分析。
- en: Remember
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住
- en: Think of data lakes as a filesystem, such as for storing data in a file location
    on your laptop, just on a much larger scale.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将数据湖视为文件系统，就像在你的笔记本电脑上存储数据位置一样，不过规模要大得多。
- en: Data lake solutions
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据湖解决方案
- en: 'Here’s a summary of the available data lake solutions in the data space:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据空间中可用的数据湖解决方案的摘要：
- en: '| **Data Lake** | **Description** |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| **数据湖** | **描述** |'
- en: '| Amazon S3 | A cloud-based object storage service by **Amazon Web Services**
    (**AWS**). It’s commonly used as a foundation for data lakes. |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Amazon S3 | **亚马逊网络服务** (**AWS**) 提供的基于云的对象存储服务，通常作为数据湖的基础设施。 |'
- en: '| Azure Data Lake Storage | A scalable and secure cloud-based storage solution
    by Microsoft Azure that’s designed to support big data analytics and data lakes.
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Azure 数据湖存储 | 微软 Azure 提供的可扩展且安全的基于云的存储解决方案，旨在支持大数据分析和数据湖。 |'
- en: '| **Hadoop Distributed File** **System** (**HDFS**) | A distributed filesystem
    that forms the storage layer of Apache Hadoop, an open source big data processing
    framework. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| **Hadoop 分布式文件** **系统** (**HDFS**) | 作为 Apache Hadoop 的存储层，HDFS 是一个分布式文件系统，Hadoop
    是一个开源的大数据处理框架。 |'
- en: '| Google Cloud Storage | Google Cloud’s object storage service, often used
    as part of a data lake architecture in the GCP. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Google Cloud Storage | Google Cloud 提供的对象存储服务，通常作为 GCP 中数据湖架构的一部分使用。 |'
- en: Table 7.3 – Data lake solutions
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.3 – 数据湖解决方案
- en: 'The shift from traditional data warehouses to data lakes represents a fundamental
    transformation in how organizations manage and analyze data. This shift was driven
    by several factors, including the need for greater flexibility, scalability, and
    the ability to handle diverse and unstructured data types. The following table
    highlights the key differences between traditional data warehouses and data lakes:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 从传统的数据仓库到数据湖的转变，代表了组织管理和分析数据方式的根本性变化。这一转变是由多个因素推动的，包括对更大灵活性、可扩展性以及处理多样化和非结构化数据类型的需求。下表突出显示了传统数据仓库和数据湖之间的主要区别：
- en: '|  | **Data Warehouses** | **Data Lakes** |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | **数据仓库** | **数据湖** |'
- en: '| Data variety and flexibility | Traditional data warehouses are designed to
    handle structured data and are less adaptable to handle diverse data types or
    unstructured data. | Data lakes emerged as a response to the increasing volume
    and variety of data. They provide a storage repository for raw, unstructured,
    and diverse data types, allowing organizations to store large volumes of data
    without the need for predefined schemas. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 数据种类和灵活性 | 传统的数据仓库设计用于处理结构化数据，对于处理多样化数据类型或非结构化数据的适应性较差。 | 数据湖的出现是为了应对数据量和数据种类的快速增长。它们为原始、非结构化和多样化的数据类型提供存储库，允许组织存储大量数据，而无需预定义的模式。
    |'
- en: '| Scalability | Traditional data warehouses often face scalability challenges
    when dealing with massive amounts of data. Scaling up a data warehouse can be
    costly and may have limitations. | Data lakes, particularly cloud-based solutions,
    offer scalable storage and computing resources. They can efficiently scale horizontally
    to handle growing datasets and processing demands. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性 | 传统的数据仓库在处理海量数据时通常面临可扩展性挑战。扩展数据仓库可能会很昂贵，并且可能存在限制。 | 数据湖，尤其是基于云的解决方案，提供可扩展的存储和计算资源。它们可以有效地水平扩展，以应对日益增长的数据集和处理需求。
    |'
- en: '|  | **Data Warehouses** | **Data Lakes** |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  | **数据仓库** | **数据湖** |'
- en: '| Cost-efficiency | Traditional data warehouses can be expensive to scale,
    and the cost structure may not be conducive to storing large volumes of raw or
    less structured data. | Cloud-based data lakes often follow a pay-as-you-go model,
    allowing organizations to manage costs more efficiently by paying for the resources
    they use. This is particularly beneficial for storing large amounts of raw data.
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 成本效益 | 传统的数据仓库在扩展时可能成本高昂，并且其成本结构可能不适合存储大量原始或结构较差的数据。 | 基于云的数据湖通常采用按需付费模式，允许组织通过按使用的资源付费来更有效地管理成本。这对于存储大量原始数据特别有利。
    |'
- en: '| Schema-on-dead versus schema-on-write | Follow a schema-on-write approach,
    where data is structured and transformed before being loaded into the warehouse.
    | Follow a schema-on-read approach, allowing for the storage of raw, untransformed
    data. The schema is applied at the time of data analysis, providing more flexibility
    in data exploration. |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 写时模式与读时模式 | 遵循写时模式（schema-on-write），数据在加载到仓库之前被结构化和转换。 | 遵循读时模式（schema-on-read），允许存储原始、未经转换的数据。模式在数据分析时应用，提供了更多的数据探索灵活性。
    |'
- en: Table 7.4 – Data warehouses versus data lakes
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.4 – 数据仓库与数据湖
- en: 'The emergence of the Lakehouse architecture further refined the shift away
    from data warehouses by solving some key challenges associated with data lakes
    and by bringing features traditionally associated with data warehouses into the
    data lake environment. Here’s a breakdown of the key aspects of this evolution:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Lakehouse架构的出现通过解决数据湖相关的关键挑战，并将传统上与数据仓库相关的特性引入数据湖环境，从而进一步完善了摆脱数据仓库的转变。以下是这一演变的关键方面概述：
- en: The Lakehouse integrates ACID transactions into the data lake, providing transactional
    capabilities that were traditionally associated with data warehouses. This ensures
    data consistency and reliability.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakehouse将ACID事务集成到数据湖中，提供了传统上与数据仓库相关的事务处理能力。这确保了数据的一致性和可靠性。
- en: The Lakehouse supports schema evolution, allowing changes to be made to data
    schemas over time without requiring a full transformation of existing data. This
    enhances flexibility and reduces the impact of schema changes on existing processes.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakehouse支持模式演变，允许随时间对数据模式进行更改，而无需对现有数据进行完全转换。这提高了灵活性，并减少了模式更改对现有流程的影响。
- en: The Lakehouse introduces features for managing data quality, including schema
    enforcement and constraints, ensuring that data stored in the lake meets specified
    standards.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakehouse引入了管理数据质量的特性，包括模式强制执行和约束，确保存储在数据湖中的数据符合指定的标准。
- en: The Lakehouse aims to provide a unified platform for analytics by combining
    the strengths of data lakes and data warehouses. It allows organizations to perform
    analytics on both structured and semi-structured data in a centralized repository.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakehouse旨在通过结合数据湖和数据仓库的优势，提供一个统一的分析平台。它允许组织在一个集中式存储库中对结构化和半结构化数据进行分析。
- en: The Lakehouse enhances the metadata catalog, providing a comprehensive view
    of data lineage, quality, and transformations. This facilitates better governance
    and understanding of the data stored in the lake.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakehouse 增强了元数据目录，提供了数据血缘、质量和转换的全面视图。这有助于更好的治理和对湖中数据的理解。
- en: The Lakehouse concept has evolved through discussions in the data and analytics
    community, with various companies contributing to the development and adoption
    of Lakehouse principles.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: Lakehouse 概念通过数据和分析社区的讨论不断发展，多个公司为 Lakehouse 原则的发展和采用做出了贡献。
- en: An example of a data lake
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据湖示例
- en: 'Let’s explore an example of how to write some Parquet files on S3, AWS’s cloud
    storage. To get everything set up, go to the AWS documentation: [https://docs.aws.amazon.com/code-library/latest/ug/python_3_s3_code_examples.html](https://docs.aws.amazon.com/code-library/latest/ug/python_3_s3_code_examples.html).
    Now, follow these steps:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一个如何在 S3 上写入 Parquet 文件的示例，S3 是 AWS 的云存储。要设置一切，访问 AWS 文档：[https://docs.aws.amazon.com/code-library/latest/ug/python_3_s3_code_examples.html](https://docs.aws.amazon.com/code-library/latest/ug/python_3_s3_code_examples.html)。现在，按照以下步骤进行操作：
- en: Note
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To run this example, you need to have an AWS account and an S3 bucket ready.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，您需要拥有一个 AWS 账户并准备好 S3 存储桶。
- en: 'We’ll start by importing the required libraries:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将开始导入所需的库：
- en: '[PRE51]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, we’re going to create some mock data so that we can write it to S3:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一些模拟数据，以便将其写入 S3：
- en: '[PRE52]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, we must convert the DataFrame into Parquet format:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须将 DataFrame 转换为 Parquet 格式：
- en: '[PRE53]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Update your authentication key and the bucket name in which we’re going to
    write the data:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新您的认证密钥和我们将要写入数据的存储桶名称：
- en: '[PRE54]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Connect to the S3 bucket using the connection details from the previous step:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前一步的连接信息连接到 S3 存储桶：
- en: '[PRE55]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Upload the Parquet file to S3:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Parquet 文件上传到 S3：
- en: '[PRE56]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As discussed previously, Lakehouses come with their own set of advantages and
    disadvantages.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Lakehouse 具有自己的优缺点。
- en: 'The advantages are as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 其优势如下：
- en: Lakehouses provide a unified platform that integrates the strengths of both
    data lakes and data warehouses. This allows organizations to leverage the flexibility
    of data lakes and the transactional capabilities of data warehouses in a single
    environment.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakehouse 提供了一个统一的平台，结合了数据湖和数据仓库的优势。这使得组织能够在一个环境中同时利用数据湖的灵活性和数据仓库的事务能力。
- en: Lakehouses follow a schema-on-read approach, allowing for the storage of raw,
    untransformed data.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakehouse 遵循 schema-on-read 方法，允许存储原始的、未转换的数据。
- en: Lakehouses support diverse data types, including structured, semi-structured,
    and unstructured data.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakehouse 支持多种数据类型，包括结构化、半结构化和非结构化数据。
- en: Lakehouses integrate ACID transactions, providing transactional capabilities
    that ensure data consistency and reliability. This is particularly important for
    use cases where data integrity is critical.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakehouse 集成了 ACID 事务，提供了事务能力，确保数据的一致性和可靠性。这对于数据完整性至关重要的使用场景尤为重要。
- en: Many Lakehouse solutions offer time travel capabilities, allowing users to query
    data at specific points in time. Versioning of data provides historical context
    and supports audit requirements.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多 Lakehouse 解决方案提供时间旅行功能，允许用户在特定时间点查询数据。数据的版本控制提供了历史背景并支持审计要求。
- en: Lakehouses often implement optimized storage formats (e.g., Delta and Iceberg)
    that contribute to storage efficiency and improved query performance, especially
    for large-scale analytical workloads.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakehouse 通常实现优化的存储格式（例如 Delta 和 Iceberg），这有助于提高存储效率和查询性能，特别是对于大规模分析工作负载。
- en: 'The disadvantages are as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 其缺点如下：
- en: Users and administrators may need to adapt to a new way of working with data
    while considering both schema-on-read and schema-on-write paradigms. This may
    require training and education.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户和管理员可能需要适应一种新的数据处理方式，同时考虑 schema-on-read 和 schema-on-write 模式。这可能需要培训和教育。
- en: Depending on the implementation and cloud provider, costs associated with storing,
    processing, and managing data in a Lakehouse architecture may vary. Organizations
    need to carefully manage costs to ensure efficiency.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据实现和云服务提供商的不同，存储、处理和管理 Lakehouse 架构中数据的成本可能会有所不同。组织需要仔细管理成本，以确保效率。
- en: As we’ve discussed, Lakehouses have the amazing advantage of allowing any data
    type to be ingested and stored, from structured to semi-structured to unstructured.
    This means we can find any file type in the ingestion part of the process, from
    CSVs to Parquets and Avros. While in the ingestion part, we can see any file type,
    on the write part, we can take advantage of the flexibility the Lakehouse offers
    and store the data in optimized open table file formats. Open table format is
    a file format that’s used to store tabular data in a way that’s easily accessible
    and interoperable across various data processing and analytics tools.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所讨论的，Lakehouse 拥有一个惊人的优势——它可以让任何类型的数据从结构化到半结构化再到非结构化数据都可以被摄取和存储。这意味着在摄取过程中我们可以看到任何文件类型，从
    CSV 文件到 Parquet 和 Avro 文件。而在写入部分，我们可以利用 Lakehouse 提供的灵活性，将数据存储为优化过的开放表格文件格式。开放表格格式是一种用于存储表格数据的文件格式，能够让数据在各种数据处理和分析工具之间轻松访问和互操作。
- en: File types in data lakes
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据湖中的文件类型
- en: 'In Lakehouse architecture, we have three prominent formats: Delta, Apache Iceberg,
    and Apache Hudi. These formats provide features such as ACID transactions, schema
    evolution, incremental data processing, and read and write optimizations. Here’s
    a brief overview of these formats:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Lakehouse 架构中，我们有三种突出格式：Delta、Apache Iceberg 和 Apache Hudi。这些格式提供了 ACID 事务、模式演变、增量数据处理以及读写优化等特性。以下是这些格式的简要概述：
- en: Delta Lake is an open source storage layer designed to enhance the reliability
    and performance of data processing in data lakes. It is well-suited for building
    data lakes on infrastructure such as S3 or Azure Storage and has strong support
    for ACID transactions and data versioning.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Lake 是一个开源存储层，旨在提高数据湖中数据处理的可靠性和性能。它非常适合在 S3 或 Azure 存储等基础设施上构建数据湖，并且对
    ACID 事务和数据版本控制有很强的支持。
- en: Apache Iceberg is another open source table format that’s optimized for fast
    query performance. It’s a good choice when query efficiency is required and it
    has excellent support for schema evolution and versioning.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Iceberg 是另一种开源表格格式，专为快速查询性能优化。当需要查询效率时，它是一个不错的选择，并且它对模式演变和版本控制有出色的支持。
- en: Apache Hudi (Hadoop Upserts, Deletes, and Incrementals) is another open source
    data lake storage format that provides great support for real-time data processing
    and streaming features. While it may not be as widely known as Delta Lake or Apache
    Iceberg, Hudi is gaining traction, especially in Apache Spark and Hadoop ecosystems.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Hudi（Hadoop 更新、删除和增量处理）是另一种开源数据湖存储格式，它为实时数据处理和流处理特性提供了很好的支持。尽管它可能不像 Delta
    Lake 或 Apache Iceberg 那样广为人知，但在 Apache Spark 和 Hadoop 生态系统中，Hudi 正在逐渐获得关注。
- en: 'In general, all these formats were built to solve the same challenges, which
    is why they have a lot of common features. Thus, before choosing the best one
    for your workload, there are a couple of things you should consider to ensure
    you’re going in the right direction:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这些格式都是为了解决相同的挑战而构建的，这也是它们有许多共同特性的原因。因此，在选择最适合你工作负载的格式之前，有几个因素你需要考虑，以确保你走在正确的方向：
- en: Consider the compatibility of each technology with your existing data processing
    ecosystem and tools
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑每种技术与现有数据处理生态系统和工具的兼容性。
- en: Evaluate the level of community support, ongoing development, and adoption within
    the data community for each technology
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估每种技术在数据社区中的社区支持、持续开发和采用程度。
- en: Assess the performance characteristics of each technology concerning your specific
    use case, especially in terms of read and write operations
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估每种技术在特定使用案例中的性能特征，尤其是在读写操作方面。
- en: Ultimately, the choice between Delta Lake, Apache Iceberg, and Apache Hudi should
    be driven by the specific requirements and priorities of your data lake or lakehouse
    environment. It’s also beneficial to experiment and benchmark each solution with
    your data and workloads to make an informed decision.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，Delta Lake、Apache Iceberg 和 Apache Hudi 之间的选择应该由你的数据湖或湖仓环境的具体需求和优先事项驱动。通过实验和基准测试每个解决方案，结合你的数据和工作负载，可以做出更明智的决策。
- en: The last sink technology we’re going to discuss is streaming data sinks.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的最后一种接收器技术是流数据接收器。
- en: Streaming data sinks
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流数据接收器
- en: The transition from batch and micro-batch processing to streaming technologies
    marks a significant evolution in data processing and analytics. Batch processing
    involves collecting and processing data in large, discrete chunks at scheduled
    intervals, which can lead to delays in data availability and insights. Micro-batch
    processing improves on this by handling smaller batches at more frequent intervals,
    reducing latency but still not achieving real-time data processing. Streaming
    technologies, however, enable the continuous ingestion and processing of data
    in real-time, allowing organizations to immediately analyze and act on data as
    it arrives. This shift to streaming technologies addresses the growing need for
    real-time analytics and decision-making in today’s fast-paced business environments,
    providing a more dynamic and responsive approach to data management.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 从批处理和微批处理到流处理技术的过渡，标志着数据处理和分析的重大进步。批处理涉及在预定的时间间隔内收集和处理大量离散数据，这可能导致数据可用性和洞察力的延迟。微批处理通过在更频繁的时间间隔内处理较小的批次来改进这一点，减少了延迟，但仍未实现实时数据处理。而流处理技术则能够实现数据的实时摄取和处理，使组织能够在数据到达时立即进行分析并采取行动。这一向流处理技术的转变，解决了当今快速变化的商业环境中对实时分析和决策的日益增长的需求，为数据管理提供了更加动态和响应迅速的方法。
- en: Overview of streaming data sinks
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流数据接收端概述
- en: 'Streaming data sinks are components or services that consume and store streaming
    data in real time. They act as the endpoint where streaming data is ingested,
    processed, and persisted for further analysis or retrieval. Here’s an overview
    of streaming data sinks and their main components:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据接收端是实时消费和存储流数据的组件或服务。它们作为流数据被摄取、处理并持久化以供进一步分析或检索的终端。以下是流数据接收端及其主要组件的概述：
- en: '**Ingestion component**: This is responsible for receiving and accepting incoming
    data streams'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摄取组件**：它负责接收和接纳传入的数据流'
- en: '**Processing logic**: This is a bespoke logic that may include components for
    data enrichment, transformation, or aggregation'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理逻辑**：这是一种定制的逻辑，可能包括数据丰富、转化或聚合的组件'
- en: '**Storage component**: This persists streaming data for future analysis or
    retrieval'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储组件**：它用于持久化流数据，便于未来分析或检索'
- en: '**Connectors**: Their main role is to interact with various data processing
    or storage systems'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接器**：它们的主要作用是与各种数据处理或存储系统进行交互'
- en: 'We usually implement streaming sinks in the following areas:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常在以下领域实施流数据接收端：
- en: In real-time analytics systems. This allows organizations to gain insights into
    their data as events occur.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实时分析系统中，允许组织在事件发生时实时获取数据洞察。
- en: In systems monitoring, where streaming data sinks capture and process real-time
    metrics, logs, or events, enabling immediate alerting and responses to issues
    or anomalies.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在系统监控中，流数据接收端捕获和处理实时指标、日志或事件，从而实现对问题或异常的即时告警和响应。
- en: In financial transactions or e-commerce. Here, streaming data sinks can be used
    for real-time fraud detection by analyzing patterns and anomalies in transaction
    data.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在金融交易或电子商务中，流数据接收端可用于通过分析交易数据中的模式和异常实现实时欺诈检测。
- en: In the **Internet of Things** (**IoT**) scenarios, streaming data sinks handle
    the continuous flow of data from sensors and devices, supporting real-time monitoring
    and control.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**物联网**（**IoT**）场景中，流数据接收端处理来自传感器和设备的持续数据流，支持实时监控和控制。
- en: Now, let’s have a look at the available options we have for streaming sinks.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看可用于流数据接收端的可选项。
- en: Streaming sinks solutions
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流数据接收端解决方案
- en: 'Many cloud platforms offer managed streaming data services that act as sinks,
    such as Amazon Kinesis, Azure Event Hubs, and Google Cloud Dataflow, as shown
    in the following table:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 许多云平台提供托管的流数据服务，这些服务充当数据接收端，如亚马逊 Kinesis、Azure Event Hubs 和 Google Cloud Dataflow，如下表所示：
- en: '| **Streaming** **Data Sink** | **Description** |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| **流数据接收端** | **描述** |'
- en: '| Amazon Kinesis | A fully managed service for real-time stream processing
    in AWS. It supports data streams, analytics, and storage. |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 亚马逊 Kinesis | AWS 中用于实时流处理的完全托管服务。它支持数据流、分析和存储。 |'
- en: '| Azure Event Hub | A cloud-based real-time analytics service in Azure for
    processing and analyzing streaming data. |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| Azure Event Hub | Azure 中用于处理和分析流数据的基于云的实时分析服务。 |'
- en: '| Google Cloud Dataflow | A fully managed stream and batch processing service
    on GCP. |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| Google Cloud Dataflow | GCP 上的完全托管流处理和批处理服务。 |'
- en: '| Apache Kafka | A distributed streaming platform that can act as both a source
    and a sink for streaming data. |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Apache Kafka | 一个分布式流平台，既可以作为流数据的源，也可以作为流数据的接收器。 |'
- en: '| Apache Spark Streaming | A real-time data processing framework that’s part
    of the Apache Spark ecosystem. |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Apache Spark Streaming | 一个实时数据处理框架，是 Apache Spark 生态系统的一部分。 |'
- en: '| Apache Flink | A stream processing framework that supports event time processing
    and various sink connectors. |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| Apache Flink | 一种流处理框架，支持事件时间处理和各种接收器连接器。 |'
- en: Table 7.5 – Different streaming data services
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.5 – 不同的流数据服务
- en: In the next section, we will use one of the most popular streaming sinks, Kafka,
    to get an idea of what writing in a streaming sink looks like.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用最流行的流数据接收器之一 Kafka，了解在流接收器中写入数据的过程。
- en: An example of a streaming data sink
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流数据接收器示例
- en: 'First things first, let’s get an initial understanding of the main components
    of Apache Kafka:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们对 Apache Kafka 的主要组件有一个初步了解：
- en: '**Brokers** are the core servers that make up a Kafka cluster. They handle
    the storage and management of messages. Each broker is identified by a unique
    ID. Brokers are responsible for replicating data across the cluster for fault
    tolerance.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**是构成 Kafka 集群的核心服务器。它们处理消息的存储和管理。每个代理都有一个唯一的 ID。代理负责在集群中复制数据，以确保容错能力。'
- en: '**Topics** are the primary abstractions in Kafka for organizing and categorizing
    messages. They are like tables in a database or folders in a filesystem. Messages
    are published to and read from specific topics. Topics can be partitioned for
    scalability and parallel processing.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题**是 Kafka 中用于组织和分类消息的主要抽象。它们就像数据库中的表或文件系统中的文件夹。消息被发布到特定的主题，并从这些主题中读取。主题可以进行分区，以实现可扩展性和并行处理。'
- en: '**Partitions** are the units of parallelism in Kafka. Each topic is divided
    into one or more partitions, which allow for distributed storage and processing
    of data. Messages within a partition are ordered and immutable.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区**是 Kafka 中的并行单元。每个主题被划分为一个或多个分区，这样可以实现数据的分布式存储和处理。分区中的消息是有序且不可变的。'
- en: '**Producers** are client applications that publish (write) messages to Kafka
    topics. They can choose which partition to send messages to or use a partitioning
    strategy. Producers are responsible for serializing, compressing, and load balancing
    data among partitions.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产者**是将消息发布（写入）到 Kafka 主题的客户端应用程序。它们可以选择将消息发送到哪个分区，或使用分区策略。生产者负责序列化、压缩数据，并在各个分区之间进行负载均衡。'
- en: '**Consumers** are client applications that subscribe to (read) messages from
    Kafka topics. They can read from one or more partitions of a topic and keep track
    of which messages they have already consumed.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者**是从 Kafka 主题中订阅（读取）消息的客户端应用程序。它们可以从主题的一个或多个分区读取消息，并跟踪它们已经消费的消息。'
- en: '**ZooKeeper** is used for managing and coordinating Kafka brokers. It maintains
    metadata about the Kafka cluster. Newer versions of Kafka are moving toward removing
    the ZooKeeper dependency.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ZooKeeper**用于管理和协调 Kafka 代理。它维护关于 Kafka 集群的元数据。Kafka 的新版本正逐步去除对 ZooKeeper
    的依赖。'
- en: 'Now that we have a better understanding of the main Kafka components, let’s
    start with our step-by-step guide. We will need to install a couple of components
    for this example, so stay with me as we go through the process. To simplify this,
    we will use Docker as it allows you to define the entire environment in a `docker-compose.yml`
    file, making it easy to set up Kafka and Zookeeper with minimal configuration.
    This eliminates the need to manually install and configure each component on your
    local machine. Follow these steps:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对 Kafka 的主要组件有了更好的理解，接下来我们将开始我们的逐步指南。为了完成这个示例，我们需要安装几个组件，请跟随我一起完成整个过程。为了简化这个过程，我们将使用
    Docker，因为它可以让你通过 `docker-compose.yml` 文件定义整个环境，轻松地设置 Kafka 和 ZooKeeper，并进行最小的配置。这避免了手动在本地机器上安装和配置每个组件的需要。请按照以下步骤操作：
- en: 'Download Docker by following the public documentation: [https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/).'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照公共文档下载 Docker：[https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/)。
- en: Next, set up Kafka with Docker. For this, let’s have a look at the `docker-compose.yml`
    file at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/docker-compose.yml](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/docker-compose.yml).
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用 Docker 设置 Kafka。为此，让我们查看位于 [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/docker-compose.yml](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/docker-compose.yml)
    的 `docker-compose.yml` 文件。
- en: 'This Docker Compose configuration sets up a simple Kafka and Zookeeper environment
    using version 3 of the Docker Compose file format. The configuration defines two
    services: `zookeeper` and `kafka`.'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此 Docker Compose 配置使用 Docker Compose 文件格式的第 3 版设置了一个简单的 Kafka 和 Zookeeper 环境。该配置定义了两个服务：`zookeeper`
    和 `kafka`。
- en: 'Zookeeper uses the `confluentinc/cp-zookeeper:latest` image. It maps the host
    machine’s port, `2181`, to the container’s port, `2181`, for client connections.
    The `ZOOKEEPER_CLIENT_PORT` environment variable is set to `2181`, which specifies
    the port Zookeeper will listen on for client requests:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zookeeper 使用 `confluentinc/cp-zookeeper:latest` 镜像。它将主机机器的端口 `2181` 映射到容器的端口
    `2181`，用于客户端连接。`ZOOKEEPER_CLIENT_PORT` 环境变量设置为 `2181`，指定 Zookeeper 将侦听客户端请求的端口：
- en: '[PRE57]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Kafka uses the `confluentinc/cp-kafka:latest` image. It maps the host machine’s
    port, `9092`, to the container’s port, `9092`, for external client connections:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kafka 使用 `confluentinc/cp-kafka:latest` 镜像。它将主机机器的端口 `9092` 映射到容器的端口 `9092`，用于外部客户端连接：
- en: '[PRE58]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Here are some key environment variables that configure Kafka:'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里有一些配置 Kafka 的关键环境变量：
- en: '`KAFKA_BROKER_ID` is set to `1`, identifying this broker uniquely in a Kafka
    cluster'
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_BROKER_ID` 设置为 `1`，在 Kafka 集群中唯一标识此代理。'
- en: '`KAFKA_ZOOKEEPER_CONNECT` points to the Zookeeper service (`zookeeper:2181`),
    allowing Kafka to connect to Zookeeper for managing cluster metadata'
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_ZOOKEEPER_CONNECT` 指向 Zookeeper 服务 (`zookeeper:2181`)，允许 Kafka 连接到 Zookeeper
    管理集群元数据。'
- en: '`KAFKA_ADVERTISED_LISTENERS` advertises two listeners:'
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_ADVERTISED_LISTENERS` 广告两个监听器：'
- en: '`PLAINTEXT://kafka:29092` for internal Docker network communication'
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PLAINTEXT://kafka:29092` 用于 Docker 内部网络通信。'
- en: '`PLAINTEXT_HOST://localhost:9092` for connections from outside the Docker network
    (e.g., from the host machine)'
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PLAINTEXT_HOST://localhost:9092` 用于来自 Docker 网络外部的连接（例如，来自主机机器）'
- en: '`KAFKA_LISTENER_SECURITY_PROTOCOL_MAP` ensures both advertised listeners use
    the `PLAINTEXT` protocol, meaning no encryption or authentication'
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_LISTENER_SECURITY_PROTOCOL_MAP` 确保两个公布的监听器使用 `PLAINTEXT` 协议，意味着没有加密或认证。'
- en: '`KAFKA_INTER_BROKER_LISTENER_NAME` is set to `PLAINTEXT`, specifying which
    listener Kafka brokers will use to communicate with each other'
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_INTER_BROKER_LISTENER_NAME` 设置为 `PLAINTEXT`，指定 Kafka 代理之间将使用哪个监听器进行通信。'
- en: '`KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR` is set to `1`, meaning the offsets
    topic (used for storing consumer group offsets) will not be replicated across
    multiple brokers, which is typical for a single-broker setup'
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR` 设置为 `1`，表示偏移主题（用于存储消费者组偏移量）不会跨多个代理复制，这在单代理设置中很典型。'
- en: This setup is ideal for local development or testing, where you need a simple,
    single-node Kafka environment without the complexities of a multi-node, production-grade
    cluster. Now, we’re ready to run the container.
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种设置非常适合本地开发或测试，您需要一个简单的单节点 Kafka 环境，而无需多节点生产级集群的复杂性。现在，我们准备运行容器。
- en: 'Let’s run the Docker container to start Kafka and Zookeeper. In your terminal,
    enter the following command:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们运行 Docker 容器以启动 Kafka 和 Zookeeper。在您的终端中，输入以下命令：
- en: '[PRE59]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This will take the Kafka and Zookeeper images and install them in your environment.
    You should see something similar to the following printed on your terminal:'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将获取 Kafka 和 Zookeeper 镜像，并将它们安装在您的环境中。您应该在终端上看到类似以下的输出：
- en: '[PRE60]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Kafka producer
  id: totrans-410
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kafka 生产者
- en: 'Now, let’s go back to our Python IDE and look at how we can push data to a
    Kafka producer. For this, we’re going to read the data written in MongoDB and
    produce it in Kafka. You can find the code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4a.kafka_producer.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4a.kafka_producer.py).
    Let’s get started:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到Python IDE，看看如何将数据推送到Kafka生产者。为此，我们将从MongoDB读取数据，并将其传递到Kafka。你可以在此处找到代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4a.kafka_producer.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4a.kafka_producer.py)。让我们开始吧：
- en: 'First, let’s import the necessary libraries:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入所需的库：
- en: '[PRE61]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Next, define the MongoDB connection:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义MongoDB连接：
- en: '[PRE62]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Here, `MongoClient('mongodb://localhost:27017')` connects to a MongoDB instance
    running on localhost at the default port of `27017`. This creates a client object
    that allows interaction with the database. Then, `db = mongo_client['no_sql_db']`
    selects the `no_sql_db` database from the MongoDB instance. Finally, `collection
    = db['best_collection_ever']` selects the `best_collection_ever` collection from
    the `no_sql_db` database.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`MongoClient('mongodb://localhost:27017')`连接到本地运行的MongoDB实例，默认端口为`27017`。这将创建一个客户端对象，允许与数据库进行交互。然后，`db
    = mongo_client['no_sql_db']`从MongoDB实例中选择`no_sql_db`数据库。最后，`collection = db['best_collection_ever']`从`no_sql_db`数据库中选择`best_collection_ever`集合。
- en: 'Let’s perform the Kafka producer configuration that creates a Kafka producer
    object with the specified configuration. This producer will be used to send messages
    (in this case, MongoDB documents) to a Kafka topic:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们进行Kafka生产者配置，创建一个带有指定配置的Kafka生产者对象。该生产者将用于将消息（在此案例中为MongoDB文档）发送到Kafka主题：
- en: '[PRE63]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The following function is a callback function that will be called when the
    Kafka producer finishes sending a message. It checks whether there was an error
    during the message delivery and prints a message indicating success or failure.
    This function provides feedback on whether messages were successfully sent to
    Kafka, which is useful for debugging and monitoring:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下函数是一个回调函数，当Kafka生产者完成发送消息时将被调用。它检查消息传递过程中是否出现错误，并打印指示成功或失败的消息。此函数提供了有关消息是否成功发送到Kafka的反馈，对于调试和监控非常有用：
- en: '[PRE64]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Read from MongoDB and produce to Kafka for the document in `collection.find()`:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从MongoDB读取并将文档通过`collection.find()`发送到Kafka：
- en: '[PRE65]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The preceding code iterates over each document in the `best_collection_ever`
    collection. The `find()` method retrieves all documents from the collection. Then,
    `message = json.dumps(document, default=str)` converts each MongoDB document (a
    Python dictionary) into a JSON string. The `default=str` parameter handles data
    types that aren’t JSON serializable by converting them into strings. Next, `producer.produce('mongodb_topic',
    value=message.encode('utf-8'), callback=delivery_report)` sends the JSON string
    as a message to the `mongodb_topic` Kafka topic. The message is encoded in UTF-8,
    and the `delivery_report` function is set as a callback to handle delivery confirmation.
    Finally, `producer.poll(0)` ensures that the Kafka producer processes delivery
    reports and other events. This is necessary to keep the producer active and responsive.
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码遍历了`best_collection_ever`集合中的每个文档。`find()`方法从集合中检索所有文档。然后，`message = json.dumps(document,
    default=str)`将每个MongoDB文档（一个Python字典）转换为JSON字符串。`default=str`参数通过将无法序列化为JSON的数据类型转换为字符串，处理这些数据类型。接下来，`producer.produce('mongodb_topic',
    value=message.encode('utf-8'), callback=delivery_report)`将JSON字符串作为消息发送到`mongodb_topic`
    Kafka主题。消息采用UTF-8编码，`delivery_report`函数作为回调函数，用于处理投递确认。最后，`producer.poll(0)`确保Kafka生产者处理投递报告和其他事件。这是保持生产者活跃和响应所必需的。
- en: 'This ensures that all messages in the producer’s queue are sent to Kafka before
    the script exits. Without this step, there might be unsent messages remaining
    in the queue:'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这确保生产者队列中的所有消息在脚本退出前都被发送到Kafka。如果没有这一步骤，可能会有未发送的消息残留在队列中：
- en: '[PRE66]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'After running this script, you should see the following print statements:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此脚本后，你应该会看到以下打印语句：
- en: '[PRE67]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: So far, we’ve connected to a MongoDB database, read the documents from a collection,
    and sent these documents as messages to the Kafka topic.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经连接到MongoDB数据库，读取了集合中的文档，并将这些文档作为消息发送到Kafka主题。
- en: Kafka consumer
  id: totrans-429
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kafka消费者
- en: 'Next, let’s run the consumer so that it can consume the messages from the Kafka
    producer. The full code can be found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4b.kafka_consumer.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4b.kafka_consumer.py):'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们运行消费者，以便它能够从 Kafka 生产者消费消息。完整代码可以在 [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4b.kafka_consumer.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4b.kafka_consumer.py)
    找到：
- en: 'Let’s start by importing the libraries:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先导入所需的库：
- en: '[PRE68]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Next, we must create the Kafka consumer configuration that specifies the Kafka
    broker(s) to connect to. Here, it’s connecting to a Kafka broker running on localhost
    at port `9092`. In this case, `group.id` sets the consumer group ID, which allows
    multiple consumers to coordinate and share the work of processing messages from
    a topic. Messages will be distributed among consumers in the same group. Next,
    `auto.offset.reset` defines the behavior when there is no initial offset in Kafka
    or if the current offset doesn’t exist. Setting this to `earliest` means the consumer
    will start reading from the earliest available message in the topic:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须创建 Kafka 消费者配置，指定要连接的 Kafka broker。这里，它连接到运行在本地主机上的 Kafka broker，端口为
    `9092`。在这种情况下，`group.id` 设置消费者组 ID，这使得多个消费者能够协调并共享处理来自主题消息的工作。消息将在同一组的消费者之间分发。接下来，`auto.offset.reset`
    定义了在 Kafka 中没有初始偏移量或当前偏移量不存在时的行为。将此设置为 `earliest` 表示消费者将从主题中最早的可用消息开始读取：
- en: '[PRE69]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now, we will instantiate a Kafka consumer with the configuration specified
    earlier. Here, `consumer.subscribe([''mongodb_topic''])` subscribes the consumer
    to the `mongodb_topic` Kafka topic. This means the consumer will receive messages
    from this topic:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将实例化一个 Kafka 消费者，并使用之前指定的配置。这里，`consumer.subscribe(['mongodb_topic'])`
    将消费者订阅到 `mongodb_topic` Kafka 主题。这意味着消费者将接收来自此主题的消息：
- en: '[PRE70]'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Set the duration for which the consumer should run (in seconds):'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置消费者运行的时长（以秒为单位）：
- en: '[PRE71]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The following code begins an infinite loop that will run until it’s explicitly
    broken out of. This loop continuously polls Kafka for new messages. Here, `if
    time.time() - start_time > run_duration` checks whether the consumer has been
    running for longer than the specified `run_duration`. If so, it prints a message
    and breaks out of the loop, stopping the consumer:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码开始一个无限循环，直到显式中断。这个循环不断地轮询 Kafka 是否有新消息。这里，`if time.time() - start_time >
    run_duration` 检查消费者是否已经运行超过了指定的 `run_duration`。如果是，它会打印一条消息并退出循环，停止消费者：
- en: '[PRE72]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'After running the preceding code, you should see the following print statements:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您应该会看到以下打印输出：
- en: '[PRE73]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The goal of this example was to show you how data can be continuously read from
    a NoSQL database such as MongoDB and then streamed in real-time to other systems
    using Kafka. Kafka acts as a messaging system that allows data producers (e.g.,
    MongoDB) to be decoupled from data consumers. This example also illustrates how
    data can be processed in stages, allowing for scalable and flexible data pipelines.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的目标是向您展示如何从 MongoDB 等 NoSQL 数据库中持续读取数据，并通过 Kafka 实时流式传输到其他系统。Kafka 充当消息系统，允许数据生产者（例如
    MongoDB）与数据消费者解耦。此示例还说明了如何分阶段处理数据，从而实现可扩展和灵活的数据管道。
- en: In terms of a real use case scenario, imagine that we are building a ride-sharing
    app. Handling events such as ride requests, cancellations, and driver statuses
    in real-time is crucial for efficiently matching riders with drivers. MongoDB
    stores this event data, such as ride requests and driver locations, while Kafka
    streams the events to various microservices. These microservices then process
    the events to make decisions, such as assigning a driver to a rider. By using
    Kafka, the system becomes highly responsive, scalable, and resilient as it decouples
    event producers (such as ride requests) from consumers (such as the driver assignment
    logic).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际应用场景来看，假设我们正在构建一个拼车应用。实时处理事件，例如乘车请求、取消请求和司机状态，对于高效地将乘客与司机匹配至关重要。MongoDB 存储这些事件数据，如乘车请求和司机位置，而
    Kafka 将事件流式传输到各种微服务。这些微服务随后处理这些事件以做出决策，例如将司机分配给乘客。通过使用 Kafka，系统变得高度响应、可扩展且具备弹性，因为它将事件生产者（例如乘车请求）与消费者（例如司机分配逻辑）解耦。
- en: To summarize what we have seen so far, in contrast to relational sinks, which
    involve structured data with defined schemas, Kafka can serve as a buffer or intermediary
    for data ingestion, allowing for decoupled and scalable data pipelines. NoSQL
    sinks often handle unstructured or semi-structured data, similar to Kafka’s flexibility
    with message formats. Kafka’s ability to handle high-throughput data streams complements
    NoSQL databases’ scalability and flexibility.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 总结我们迄今为止所看到的内容，与涉及具有定义模式的结构化数据的关系型接收端不同，Kafka 可以作为数据摄取的缓冲区或中介，允许解耦和可扩展的数据管道。NoSQL
    接收端通常处理非结构化或半结构化数据，类似于 Kafka 对消息格式的灵活性。Kafka 处理高吞吐量数据流的能力与 NoSQL 数据库的可扩展性和灵活性相辅相成。
- en: 'To clean all the resources that have been used so far, execute the cleaning
    script: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/cleanup_script.sh](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/cleanup_script.sh).'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清理到目前为止使用的所有资源，请执行清理脚本：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/cleanup_script.sh](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/cleanup_script.sh)。
- en: In the next section, we will deep dive into the file format seen in streaming
    data sinks.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将深入探讨流式数据接收端所看到的文件格式。
- en: File types in streaming data sinks
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流式数据接收端的文件类型
- en: Streaming data sinks primarily deal with messages or events rather than traditional
    file storage. The data that’s transmitted through streaming data sinks is often
    in formats such as JSON, Avro, or binary. These formats are commonly used for
    serializing and encoding data in streaming scenarios. They are efficient and support
    schema evolution. In the *NoSQL databases* section of this chapter, we deep-dived
    in the JSON file format. Here, we’ll look at Avro and binary.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 流式数据接收端主要处理消息或事件，而不是传统的文件存储。通过流式数据接收端传输的数据通常采用 JSON、Avro 或二进制等格式。这些格式在流式场景中被广泛用于数据的序列化和编码。它们高效并且支持模式演化。在本章的*NoSQL数据库*部分，我们深入探讨了
    JSON 文件格式。在这里，我们将讨论 Avro 和二进制格式。
- en: 'Apache Avro is a binary serialization format developed within the Apache Hadoop
    project. It uses a schema to define data structures, allowing for efficient serialization
    and deserialization. Avro is known for its compact binary representation, providing
    fast serialization and efficient storage. In streaming scenarios, minimizing data
    size is crucial for efficient transmission over the network. Avro’s compact binary
    format reduces data size, improving bandwidth utilization. Avro also supports
    schema evolution, allowing for changes in data structures over time without requiring
    all components to be updated simultaneously. Avro’s schema-based approach enables
    interoperability between different systems and languages, making it suitable for
    diverse ecosystems. Let’s see an example of an Avro file:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Avro 是一个在 Apache Hadoop 项目中开发的二进制序列化格式。它使用模式来定义数据结构，从而实现高效的序列化和反序列化。Avro
    以其紧凑的二进制表示而著称，提供快速的序列化和高效的存储。在流式场景中，最小化数据大小对于高效的网络传输至关重要。Avro 的紧凑二进制格式减少了数据大小，提升了带宽利用率。Avro
    还支持模式演化，允许数据结构随时间变化而不需要同时更新所有组件。Avro 的基于模式的方法使得不同系统和语言之间能够实现互操作性，适用于多样的生态系统。我们来看一个
    Avro 文件示例：
- en: '[PRE74]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Binary formats use a compact binary representation of data, resulting in efficient
    storage and transmission. Various binary protocols can be employed based on specific
    requirements, such as Google’s **Protocol Buffers** (**protobuf**) or Apache Thrift.
    Binary formats minimize the size of the transmitted data, reducing bandwidth usage
    in streaming scenarios. Binary serialization and deserialization are generally
    faster than text-based formats, which is crucial in high-velocity streaming environments.
    Let’s have a look at a binary file in protobuf:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制格式使用紧凑的二进制数据表示方式，从而实现高效的存储和传输。可以根据具体需求使用各种二进制协议，例如 Google 的**协议缓冲区**（**protobuf**）或
    Apache Thrift。二进制格式通过最小化传输数据的大小，在流式场景中减少带宽使用。二进制序列化和反序列化通常比基于文本的格式更快，这在高吞吐量流式环境中至关重要。我们来看一个
    protobuf 的二进制文件示例：
- en: '[PRE75]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: In streaming sinks, the choice between JSON, Avro, or binary depends on the
    specific requirements of the streaming use case, including factors such as interoperability,
    schema evolution, and data size considerations.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在流式数据接收端，选择 JSON、Avro 还是二进制格式取决于流式使用场景的具体需求，包括互操作性、模式演化和数据大小等因素。
- en: So far, we’ve discussed the most common data sinks used by data engineers and
    data scientists, as well as the different file types we usually encounter with
    those sinks. In the following sections, we will provide a summary of all the discussed
    data sinks and file types, as well as their pros and cons and when to best use
    them.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了数据工程师和数据科学家使用的最常见数据存储系统，以及我们通常会遇到的不同文件类型。在接下来的章节中，我们将总结所有讨论过的数据存储系统和文件类型，以及它们的优缺点和最佳使用时机。
- en: Which sink is the best for my use case?
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哪种数据存储系统最适合我的使用场景？
- en: 'Let’s summarize what we’ve learned regarding the different data sinks and get
    a deeper understanding of when to use which:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下关于不同数据存储系统的知识，并深入理解何时使用哪种系统：
- en: '| **Technology** | **Pros** | **Cons** | **When** **to Choose** | **Use Case**
    |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **优点** | **缺点** | **何时选择** | **使用场景** |'
- en: '| Relational database | ACID properties ensure data consistency.Mature query
    languages (SQL) for complex queries.Support for complex transactions and joins.
    | Limited scalability for read-heavy workloads.Schema changes may be challenging
    and downtime-prone.May not scale well horizontally. | Structured data with a well-defined
    schema.When you’re maintaining relationships between data entities. | Transactional
    applicationsEnterprise applications with structured data. |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| 关系型数据库 | ACID属性确保数据一致性。成熟的查询语言（SQL）支持复杂查询。支持复杂的事务和连接操作。 | 对于读密集型工作负载的可扩展性有限。架构更改可能具有挑战性，并容易导致停机。可能无法水平扩展。
    | 具有明确定义架构的结构化数据。需要维护数据实体之间关系的情况。 | 事务应用程序。处理结构化数据的企业应用程序。 |'
- en: '| NoSQL database | Flexible schema, suitable for semi-structured or unstructured
    data.Scalability – horizontal scaling is often easier.High write throughput for
    certain workloads. | Lack of standardized query language may require learning
    a specific API.May lack ACID compliance in favor of eventual consistency.Limited
    support for complex transactions. | Dynamic or evolving data schema.Rapid development
    and iteration.Handling large volumes of data with varying structures. | Document
    databases for content management.Real-time a pplications with variable schema.JSON
    data storage for web applications |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| NoSQL数据库 | 灵活的架构，适用于半结构化或非结构化数据。可扩展性——水平扩展通常更容易。某些工作负载下具有高写入吞吐量。 | 缺乏标准化的查询语言，可能需要学习特定的API。可能缺乏ACID一致性，而偏向最终一致性。对复杂事务的支持有限。
    | 动态或不断变化的数据架构。快速开发和迭代。处理结构各异的大量数据。 | 用于内容管理的文档数据库。具有可变架构的实时应用程序。用于Web应用程序的JSON数据存储。
    |'
- en: '| Data warehouse | Optimized for complex analytics and reporting.Efficient
    data compression and indexing.Scalability for read-heavy analytical workloads.
    | May not be cost-effective for high-volume transactional workloads.May have higher
    latency for real-time queries.May require specialized skills for maintenance and
    optimization. | Analytical processing on large datasets.Aggregating and analyzing
    historical data. | Business intelligence and reporting tools.Running complex queries
    on terabytes of data. |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| 数据仓库 | 针对复杂分析和报表优化。高效的数据压缩和索引。适用于读密集型分析工作负载的可扩展性。 | 对于高吞吐量事务工作负载可能成本较高。实时查询可能存在较高延迟。可能需要专门的技能来维护和优化。
    | 对大数据集进行分析处理。聚合和分析历史数据。 | 商业智能和报表工具。在TB级数据上运行复杂查询。 |'
- en: '| **Technology** | **Pros** | **Cons** | **When** **to Choose** | **Use Case**
    |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **优点** | **缺点** | **何时选择** | **使用场景** |'
- en: '| Lakehouse | Unified platform combining data lake and data warehouse features.Offers
    scalable storage and computing resources.It can efficiently scale horizontally
    to handle growing datasets and processing demands.Pay-as-you-go model, allowing
    organizations to manage costs more efficiently by paying for the resources they
    use. This is particularly beneficial for storing large amounts of raw data.Follow
    a schema-on-read approach, allowing for the storage of raw, untransformed data.
    | Complexity in managing both schema-on-read and schema-on-write.Depending on
    the implementation and cloud provider, costs associated with storage, processing,
    and managing data in a Lakehouse architecture may vary. Organizations need to
    carefully manage costs to ensure efficiency. | A balance between flexibility and
    transactional capabilities. | Real-time analytics with long-term storage.Any engineering,
    machine learning, and analytics use case |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| Lakehouse | 结合了数据湖和数据仓库特征的统一平台。提供可扩展的存储和计算资源。可以高效地横向扩展，以应对不断增长的数据集和处理需求。按需付费模式，使组织能够通过为所用资源付费来更有效地管理成本。这对于存储大量原始数据尤其有利。采用按需读取模式，允许存储未经转换的原始数据。
    | 管理按需读取和按需写入模式的复杂性。根据实现方式和云服务提供商，Lakehouse 架构中存储、处理和管理数据的成本可能有所不同。组织需要仔细管理成本，以确保高效性。
    | 在灵活性和事务能力之间的平衡。 | 实时分析与长期存储。任何工程、机器学习和分析用例 |'
- en: '| Streaming sinks | Enables real-time processing and analysis of streaming
    data.Scales horizontally to handle high volumes of incoming data.Integral to building
    event-driven architectures. | Implementing and managing streaming data sinks can
    be complex.The processing and persistence of streaming data introduces some latency.Depending
    on the chosen solution, infrastructure costs may be a consideration. | Continuous
    ingestion and processing of data in real-time | IoTReal-time analytical use casesSystems
    monitoring |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 流数据存储 | 实现实时处理和流数据分析。水平扩展以处理大量的输入数据。构建事件驱动架构的核心组成部分。 | 实现和管理流数据存储可能很复杂。流数据的处理和持久化会引入一些延迟。根据所选解决方案，基础设施成本可能是一个考虑因素。
    | 实时持续摄取和处理数据 | 物联网、实时分析应用场景、系统监控 |'
- en: Table 7.6 – Summary table of all the data sinks, as well as their pros, cons,
    and use cases
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.6 – 所有数据存储的总结表，包括它们的优缺点和使用场景
- en: In *Table 7.8*, the **Use Case** column provides more context and practical
    examples of how each data sink technology can be effectively applied in real-world
    scenarios.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在*表7.8*中，**用例**列提供了更多的背景信息和实际示例，说明如何在实际场景中有效地应用每种数据存储技术。
- en: Moving from selecting the right data sink technology to choosing the appropriate
    file type is a crucial step in designing an effective data processing pipeline.
    Once you’ve determined where your data will be stored (data sink), you need to
    consider how it will be stored (file type). The choice of file type can impact
    data storage efficiency, query performance, data integrity, and interoperability
    with other systems.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 从选择合适的数据存储技术到选择适当的文件类型，这是设计有效数据处理管道的关键步骤。一旦确定了数据存储的位置（数据存储），接下来就需要考虑数据如何存储（文件类型）。文件类型的选择会影响数据存储效率、查询性能、数据完整性以及与其他系统的互操作性。
- en: Decoding file types for optimal usage
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码文件类型以实现最佳使用
- en: Choosing the right file type when selecting a data sink is crucial for optimizing
    data storage, processing, and retrieval. One of the file types that we haven’t
    discussed so far but is very important since it’s used as an underline format
    for other file formats is the Parquet file.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择数据存储时，选择合适的文件类型对优化数据存储、处理和检索至关重要。我们至今未讨论过的一种文件类型，但它非常重要，因为它作为其他文件格式的底层格式使用，那就是
    Parquet 文件。
- en: Parquet is a columnar storage file format designed for efficient data storage
    and processing in big data and analytics environments. It is an open standard
    file format that provides benefits such as high compression ratios, columnar storage,
    and support for complex data structures. Parquet is widely adopted in the Apache
    Hadoop ecosystem and is supported by various data processing frameworks.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是一种列式存储文件格式，旨在大数据和分析环境中实现高效的数据存储和处理。它是一个开放标准文件格式，提供高压缩比、列式存储和对复杂数据结构的支持等优点。Parquet
    在 Apache Hadoop 生态系统中得到了广泛应用，并且被各种数据处理框架所支持。
- en: Parquet stores data in a columnar format, which means values from the same column
    are stored together. This design is advantageous for analytics workloads where
    queries often involve selecting a subset of columns. Parquet also supports different
    compression algorithms, allowing users to choose the one that best suits their
    requirements. This contributes to reduced storage space and improved query performance.
    Parquet files can handle schema evolution as well, making it possible to add or
    remove columns without requiring a complete rewrite of the dataset. This feature
    is essential for scenarios where the data schema evolves. Due to its advantages,
    Parquet has become a widely adopted and standardized file format in the big data
    ecosystem, forming the basis for other optimized formats, such as Delta and Iceberg.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 以列式格式存储数据，这意味着来自同一列的值会一起存储。这种设计对于分析工作负载非常有利，尤其是查询通常涉及选择一部分列时。Parquet
    还支持多种压缩算法，用户可以选择最适合自己需求的算法，从而减少存储空间并提升查询性能。Parquet 文件还能处理模式演化，使得可以在不完全重写数据集的情况下添加或删除列。这一特性在数据模式演化的场景中尤为重要。由于其诸多优势，Parquet
    已成为大数据生态系统中广泛采用并标准化的文件格式，为 Delta 和 Iceberg 等其他优化格式奠定了基础。
- en: 'Having discussed Parquet files, we can now compare the common file types, along
    with their pros and cons, and provide guidance on when to choose each type for
    different data sinks:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论完 Parquet 文件后，我们现在可以比较常见的文件类型，以及它们的优缺点，并为不同数据存储提供选择建议：
- en: '| **File Type** | **Pros** | **Cons** | **When** **to Choose** |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| **文件类型** | **优点** | **缺点** | **何时选择** |'
- en: '| JSON | Human-readable | Larger file size compared to binary formatsSlower
    serialization/ deserialization | Semi-structured or human-readable data is required
    |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| JSON | 人类可读 | 与二进制格式相比文件较大，序列化/反序列化速度较慢 | 需要半结构化或人类可读数据 |'
- en: '| BSON | Compact binary formatSupports richer data types | May not be as human-readable
    as JSON, with limited adoption outside MongoDB | Efficiency in storage and transmission
    is crucial |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| BSON | 紧凑的二进制格式，支持更丰富的数据类型 | 可能不像 JSON 那样易于人类阅读，并且在 MongoDB 以外的地方采用有限 | 存储和传输效率至关重要时选择
    |'
- en: '| Parquet | Columnar storage, which is efficient for analyticsCompression and
    encoding lead to smaller file sizes | Not as human-readable as JSONYou can’t update
    tables – you need to rewrite them | Analytical processing, data warehousing |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| Parquet | 列式存储，适合分析，压缩和编码减少文件大小 | 不像 JSON 那样易于人类阅读，不能直接更新表格 – 需要重写 | 分析处理、数据仓储
    |'
- en: '| Avro | Compact binary serializationSchema-based and supports schema evolutionInteroperable
    across different systems | Slightly less human-readable compared to JSON | Bandwidth-efficient
    streaming and diverse language support |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| Avro | 紧凑的二进制序列化基于模式，支持模式演化，跨不同系统互操作 | 与 JSON 相比，稍微不那么易于人类阅读 | 带宽高效的流处理和多语言支持
    |'
- en: '| Delta | ACID transactions for data consistencyEfficient storage format for
    data lakesSchema evolution and time-travel queries | Larger size than Parquet
    | Real-time analytics with long-term storage |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| Delta | 提供 ACID 事务以确保数据一致性，适用于数据湖的高效存储格式，支持模式演化和时间旅行查询 | 文件比 Parquet 大 |
    实时分析与长期存储 |'
- en: '| Hudi | Efficient incremental data processingACID transactions for real-time
    data | Larger size than Parquet | Streaming data applications and change data
    capture |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| Hudi | 高效的增量数据处理，支持 ACID 事务以实现实时数据 | 文件比 Parquet 大 | 流数据应用和变更数据捕获 |'
- en: '| Iceberg | Schema evolution, ACID transactionsOptimized storage formats such
    as Parquet | Larger size than Parquet | Time-travel queries and evolving schemas
    |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| Iceberg | 支持模式演化、ACID 事务，优化存储格式如 Parquet | 文件比 Parquet 大 | 时间旅行查询和模式演化 |'
- en: '| **File Type** | **Pros** | **Cons** | **When** **to Choose** |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| **文件类型** | **优点** | **缺点** | **何时选择** |'
- en: '| Binary format | Compact and efficient storageFast serialization and deserialization
    | Not human-readableLimited support for schema evolution | Efficiency is crucial
    in bandwidth usage and processing speed |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 二进制格式 | 紧凑且高效的存储，快速序列化和反序列化 | 不易于人类阅读，支持的模式演化有限 | 在带宽使用和处理速度方面对效率要求高时选择 |'
- en: Table 7.7 – A summary table of all the file formats, as well as their pros,
    cons, and use cases
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.7 – 所有文件格式的汇总表，包括它们的优缺点和使用案例
- en: In the next section, we’re going to discuss **partitioning**, an important concept
    in the context of data storage, especially in distributed storage systems. While
    the concept of partitioning itself is more closely associated with data lakes,
    data warehouses, and distributed filesystems, its relevance extends to the broader
    discussion of data sinks.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论**分区**，这是数据存储中一个重要的概念，尤其是在分布式存储系统的上下文中。尽管分区这一概念与数据湖、数据仓库和分布式文件系统密切相关，但它在更广泛的数据存储讨论中也具有重要意义。
- en: Navigating partitioning
  id: totrans-485
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导航分区
- en: '**Data partitioning** is a technique that’s used to divide and organize large
    datasets into smaller, more manageable subsets called partitions. When writing
    data to sinks, such as databases or distributed storage systems, employing appropriate
    data partitioning strategies is crucial for optimizing query performance, data
    retrieval, and storage efficiency. Partitioning in data storage systems, including
    time-based, geographic, and hybrid partitioning, offers several benefits in terms
    of read operations, updates, and writes:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据分区**是一种技术，用于将大型数据集划分并组织成更小、更易管理的子集，称为分区。当将数据写入存储系统时，例如数据库或分布式存储系统，采用适当的数据分区策略对优化查询性能、数据检索和存储效率至关重要。数据存储系统中的分区，包括基于时间、地理位置和混合分区，在读操作、更新和写操作方面提供了多个好处：'
- en: When querying the data, partitioning allows the system to skip irrelevant data
    quickly. For example, in **time-based partitioning**, if you’re interested in
    data for a specific date, the system can directly access the partition corresponding
    to that date, leading to faster query times. It ensures that only the necessary
    partitions are scanned, reducing the amount of data to process.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在查询数据时，分区使得系统能够迅速跳过无关数据。例如，在**基于时间的分区**中，如果你只关心某一特定日期的数据，系统可以直接访问与该日期对应的分区，从而提高查询速度。它确保只扫描必要的分区，减少了需要处理的数据量。
- en: Partitioning can simplify updates, especially when the updates are concentrated
    in specific partitions. For example, if you need to update data for a specific
    date or region, the system can isolate the affected partition, reducing the scope
    of the update operation.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区可以简化更新，尤其是当更新集中在特定分区时。例如，如果你需要更新特定日期或地区的数据，系统可以隔离受影响的分区，从而减少更新操作的范围。
- en: Partitioning can enhance the efficiency of write operations, particularly when
    appending data. New data can be written to the appropriate partition without affecting
    the existing data, leading to a more straightforward and faster write process.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区可以提高写操作的效率，特别是在附加数据时。新数据可以写入适当的分区，而不影响现有数据，从而使写入过程更简单、更快速。
- en: Partitioning supports parallel processing. Different partitions can be read
    or written concurrently, enabling better utilization of resources and faster overall
    processing times.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区支持并行处理。不同的分区可以并行读取或写入，从而更好地利用资源并加快整体处理速度。
- en: Partitioning provides a logical organization of data. It simplifies data management
    tasks such as archiving old data, deleting obsolete records, or migrating specific
    partitions to different storage tiers based on access patterns.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区提供了数据的逻辑组织方式。它简化了数据管理任务，如归档旧数据、删除过时记录或根据访问模式将特定分区迁移到不同的存储层次。
- en: With partitioning, you can optimize storage based on usage patterns. For example,
    frequently accessed partitions can be stored in high-performance storage, while
    less frequently accessed partitions can be stored in lower-cost storage.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分区，你可以根据使用模式优化存储。例如，频繁访问的分区可以存储在高性能存储中，而不常访问的分区则可以存储在低成本存储中。
- en: Partitioning supports pruning, where the system can eliminate entire partitions
    from consideration during query execution. This pruning mechanism further accelerates
    query performance.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区支持修剪，系统可以在查询执行过程中排除整个分区的考虑。这种修剪机制进一步加速了查询性能。
- en: Let’s have a closer look at the different partitioning strategies.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解不同的分区策略。
- en: Horizontal versus vertical partitioning
  id: totrans-495
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 水平分区与垂直分区
- en: 'When discussing partitioning strategies in the context of databases or distributed
    systems, we generally refer to two main types: **horizontal partitioning** and
    **vertical partitioning**. Each approach organizes data differently to improve
    performance, scalability, or manageability. Let’s start with horizontal partitioning.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论数据库或分布式系统中的分区策略时，我们通常提到两种主要类型：**水平分区**和**垂直分区**。每种方法以不同的方式组织数据，以提高性能、可扩展性或可管理性。让我们先从水平分区开始。
- en: Horizontal partitioning, or sharding, involves dividing a table’s rows into
    multiple partitions, each containing a subset of the data. This approach is commonly
    used to scale out databases by distributing data across multiple servers, where
    each shard maintains the same schema but holds different rows. For example, a
    user table in a large application could be sharded by user IDs, with IDs 1 to
    10,000 in one partition and IDs 10,001 to 20,000 in another. This strategy enables
    the system to handle larger datasets than a single machine could manage, enhancing
    performance in large-scale applications.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 水平分区，也称为分片，涉及将表的行分割成多个分区，每个分区包含数据的一个子集。这种方法通常用于通过将数据分布到多个服务器上来扩展数据库，其中每个分片保持相同的模式，但包含不同的行。例如，一个大型应用中的用户表可以根据用户ID进行分片，ID
    1到10,000在一个分区中，ID 10,001到20,000在另一个分区中。这种策略使得系统能够处理比单台机器更大的数据集，从而提升大规模应用中的性能。
- en: Vertical partitioning, on the other hand, involves splitting a table’s columns
    into different partitions, where each partition contains a subset of columns but
    includes all rows. This strategy is effective when different columns are accessed
    or updated at varying frequencies as it optimizes performance by minimizing the
    amount of data that’s processed during a query. For example, in a user profiles
    table, basic information such as name and email could be stored in one partition,
    while a large binary data column, such as a profile picture, is stored in another.
    This allows queries targeting specific columns to access a smaller, more efficient
    dataset, thereby enhancing performance.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，垂直分区涉及将表的列分割成不同的分区，每个分区包含一部分列，但包含所有行。当不同的列以不同的频率被访问或更新时，这种策略是有效的，因为它通过最小化查询过程中处理的数据量来优化性能。例如，在用户资料表中，基本信息如姓名和电子邮件可以存储在一个分区中，而像用户头像这样的二进制大数据列则存储在另一个分区中。这使得针对特定列的查询可以访问更小、更高效的数据集，从而提升性能。
- en: Both strategies can be used in combination to meet the specific needs of a database
    system, depending on the data structure and access patterns. The reality is that
    in the data field, **horizontal partitioning** is more commonly seen and widely
    adopted than vertical partitioning. This is particularly true in large-scale,
    distributed databases and applications that need to handle vast amounts of data,
    high traffic, or geographically dispersed users. In the next section, we will
    see some examples of horizontal partitioning.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种策略可以结合使用，以满足数据库系统的特定需求，具体取决于数据结构和访问模式。实际上，在数据领域，**水平分区**比垂直分区更常见和广泛采用。这在需要处理大量数据、高流量或地理分散的用户的大规模分布式数据库和应用程序中特别如此。在接下来的部分，我们将看到一些水平分区的例子。
- en: Time-based partitioning
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于时间的分区
- en: Time-based partitioning involves organizing data based on timestamps. Each partition
    represents a specific time interval, such as a day, hour, or minute. It allows
    for efficient retrieval of historical data and time-based aggregations. It also
    facilitates data retention and archiving policies.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时间的分区涉及根据时间戳来组织数据。每个分区代表一个特定的时间区间，例如一天、一小时或一分钟。它有助于高效地检索历史数据和进行基于时间的聚合操作，同时还便于实施数据保留和归档策略。
- en: 'In this example, you’ll learn how to create time-based partitioning on your
    local laptop using Parquet files. You can find the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/5.time_based_partitioning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/5.time_based_partitioning.py).
    Follow these steps:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你将学习如何使用Parquet文件在本地笔记本电脑上创建基于时间的分区。你可以在这里找到完整的代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/5.time_based_partitioning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/5.time_based_partitioning.py)。按照以下步骤操作：
- en: 'Import the required libraries:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE76]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Define a sample dataset with two columns: `timestamp` and `value`. This dataset
    represents time series data with timestamps and corresponding values:'
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个示例数据集，包含两列：`timestamp`和`value`。该数据集表示带有时间戳和相应值的时间序列数据：
- en: '[PRE77]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Create a pandas DataFrame:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个pandas DataFrame：
- en: '[PRE78]'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Convert the `timestamp` column into a `datetime` type. This ensures that the
    timestamps are treated as datetime objects for accurate time-based operations:'
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`timestamp`列转换为`datetime`类型。这确保了时间戳作为日期时间对象进行处理，以便进行准确的基于时间的操作：
- en: '[PRE79]'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Update the path to store the data. Use an existing path:'
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新路径以存储数据。使用现有路径：
- en: '[PRE80]'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Iterate through the DataFrame, grouping rows by the `date` component of the
    `timestamp` column. Convert each group into a PyArrow table and write it to the
    corresponding partition path in Parquet format:'
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历DataFrame，通过`timestamp`列的`date`部分对行进行分组。将每个组转换为PyArrow表，并将其写入相应的分区路径，以Parquet格式存储：
- en: '[PRE81]'
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Create the directory if it doesn’t exist:'
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果目录不存在，创建该目录：
- en: '[PRE82]'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'After executing this script, you’ll see two Parquet files being created in
    your base directory – one for each day of the week:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此脚本后，你将在基础目录中看到两个Parquet文件被创建——每周的每一天对应一个文件：
- en: '![Figure 7.6 – Time-based partitioning output](img/B19801_07_6.jpg)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 基于时间的分区输出](img/B19801_07_6.jpg)'
- en: Figure 7.6 – Time-based partitioning output
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 基于时间的分区输出
- en: Let’s have a look at another common partitioning strategy, known as geographic
    partitioning.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下另一种常见的分区策略，称为地理分区（geographic partitioning）。
- en: Geographic partitioning
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 地理分区
- en: '**Geographic partitioning** involves dividing data based on geographical attributes
    such as regions, countries, or cities. This strategy is valuable when you’re dealing
    with geospatial data or location-based analytics. It enables fast and targeted
    retrieval of data related to specific geographic areas, thus supporting spatial
    queries and analysis.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '**地理分区**涉及根据地理属性（如地区、国家或城市）对数据进行划分。这种策略在处理地理空间数据或基于位置的分析时非常有价值。它能够快速、精准地检索与特定地理区域相关的数据，从而支持空间查询和分析。'
- en: 'Here’s an example of how you can create geographic-based partitioning in your
    local laptop using Parquet files. You can find the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/6.geo_partitioning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/6.geo_partitioning.py).
    Follow these steps:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例，展示了如何在本地笔记本电脑上使用Parquet文件创建基于地理位置的分区。你可以在这里找到完整的代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/6.geo_partitioning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/6.geo_partitioning.py)。按照以下步骤操作：
- en: 'Create a base directory for storing partitioned data:'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个用于存储分区数据的基础目录：
- en: '[PRE83]'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Convert each group (region-specific data) into a PyArrow table. Then, write
    the tables to the corresponding paths:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个组（特定地区的数据）转换为PyArrow表。然后，将表写入相应的路径：
- en: '[PRE84]'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Create a directory for each region within the base directory:'
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基础目录中为每个区域创建一个目录：
- en: '[PRE85]'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Convert the group into a PyArrow table and write it to the partition path:'
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将组转换为PyArrow表，并将其写入分区路径：
- en: '[PRE86]'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'After executing this script, you will see three Parquet files being created
    in your base directory – one for each geographic location available in the data:'
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行此脚本后，你会在基础目录中看到三个Parquet文件被创建——每个文件对应数据中一个地理位置：
- en: '![Figure 7.7 – Geographic-based partitioning output](img/B19801_07_7.jpg)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 基于地理位置的分区输出](img/B19801_07_7.jpg)'
- en: Figure 7.7 – Geographic-based partitioning output
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 基于地理位置的分区输出
- en: Let’s have a look at the last common partitioning strategy, known as hybrid
    partitioning.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下最后一种常见的分区策略，称为混合分区。
- en: Note
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Geographic partitioning is a specialized form of category partitioning that
    organizes data based on geographical attributes or spatial criteria. **Category
    partitioning** is a fundamental strategy in data organization that involves grouping
    data based on specific categories or attributes, such as customer demographics,
    product types, or transactional characteristics.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 地理分区是一种基于类别分区的专门形式，它根据地理属性或空间标准来组织数据。**类别分区**是数据组织中的基本策略，它涉及根据特定的类别或属性对数据进行分组，例如客户人口统计、产品类型或交易特征。
- en: Hybrid partitioning
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合分区
- en: '**Hybrid partitioning** involves combining multiple partitioning strategies
    to optimize data organization for specific use cases. For instance, you might
    partition data first by time and then further partition each time interval by
    a key or geographic location. It offers flexibility for addressing complex querying
    patterns and diverse data access requirements.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合分区**是指将多种分区策略结合起来，以优化特定使用场景的数据组织。例如，您可以先按时间进行分区，然后再根据某个键或地理位置进一步对每个时间段进行分区。它为处理复杂的查询模式和多样化的数据访问需求提供了灵活性。'
- en: 'Here’s an example of how you can create hybrid partitioning on your local laptop
    using Parquet files. You can find the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/7.hybrid_partitioning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/7.hybrid_partitioning.py).
    Follow these steps:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个如何在本地笔记本电脑上使用 Parquet 文件创建混合分区的示例。您可以在这里找到完整的代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/7.hybrid_partitioning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/7.hybrid_partitioning.py)。请按照以下步骤操作：
- en: 'Create a base directory for storing partitioned data:'
  id: totrans-541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个基础目录来存储分区数据：
- en: '[PRE87]'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Perform hybrid partitioning:'
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行混合分区：
- en: '[PRE88]'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Create a directory for each timestamp and region combination within the base
    directory:'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基础目录中为每个时间戳和地区组合创建一个目录：
- en: '[PRE89]'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Convert the group into a PyArrow table and write it to the partition path:'
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分组转换为 PyArrow 表并写入分区路径：
- en: '[PRE90]'
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'After executing this script, you will see three Parquet files being created
    in your base directory – two locations for January 1, 2022, and one for January
    2, 2022:'
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行此脚本后，您将在基础目录中看到创建了三个 Parquet 文件——两个是 2022 年 1 月 1 日的文件，一个是 2022 年 1 月 2 日的文件：
- en: '![Figure 7.8 – Hybrid partitioning output](img/B19801_07_8.jpg)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 混合分区输出](img/B19801_07_8.jpg)'
- en: Figure 7.8 – Hybrid partitioning output
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 混合分区输出
- en: Remember
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住
- en: So far, we’ve explored various types of partitioning, such as time-based and
    geographic. However, remember you can use any column that makes sense in your
    data, your use case, and the query patterns for the table as partitioning column(s).
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了多种分区类型，例如基于时间和地理位置的分区。然而，请记住，您可以根据数据、使用场景以及表的查询模式，选择任何有意义的列作为分区列。
- en: Now that we’ve discussed different partitioning strategies, it’s time to talk
    about how to choose the column you will partition your data on.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了不同的分区策略，接下来我们要讨论如何选择用于分区的数据列。
- en: Considerations for choosing partitioning strategies
  id: totrans-555
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择分区策略的考虑因素
- en: 'Choosing the right partitioning strategy for your data involves considering
    various factors to optimize performance, query efficiency, and data management.
    Here are some key considerations for choosing partitioning strategies:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 为数据选择合适的分区策略需要考虑多种因素，以优化性能、查询效率和数据管理。以下是选择分区策略时的一些关键考虑因素：
- en: '**Query patterns**: Select partitioning strategies based on the types of queries
    your application or analytics platform will perform most frequently.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询模式**：根据您的应用程序或分析平台最常执行的查询类型来选择分区策略。'
- en: '**Data distribution**: Ensure partitions are distributed evenly to prevent
    data hotspots and resource contention.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分布**：确保分区均匀分布，以防止数据热点和资源争用。'
- en: '**Data size**: Consider the volume of data that will be stored in each partition.
    Smaller partitions can improve query performance, but too many small partitions
    might impact management overhead.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据大小**：考虑每个分区中存储的数据量。较小的分区可以提高查询性能，但过多的小分区可能会增加管理开销。'
- en: '**Query complexity**: Some queries might benefit from hybrid partitioning,
    especially if they involve multiple attributes.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询复杂性**：某些查询可能会从混合分区中受益，特别是那些涉及多个属性的查询。'
- en: '**Scalability**: Partitioning should allow for future scalability and accommodate
    data growth over time.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：分区应支持未来的可扩展性，并能够随着时间推移适应数据增长。'
- en: Data partitioning is a key architectural decision that can significantly impact
    the efficiency and performance of your data processing pipeline. By employing
    appropriate data partitioning strategies, you can ensure that your data is organized
    in a way that aligns with your querying patterns and maximizes the benefits of
    your chosen data sink technology.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分区是一个关键的架构决策，它可能会显著影响数据处理管道的效率和性能。通过采用适当的数据分区策略，可以确保数据按照查询模式进行组织，从而最大化所选数据接收技术的好处。
- en: In the next section, we’re going to put everything we’ve learned in this chapter
    into practice by describing a real-world case scenario and going through all the
    logical steps for defining the best strategy associated with data sinks and file
    types.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将通过描述一个实际案例并逐步讲解所有定义与数据接收和文件类型相关的最佳策略，将本章所学内容付诸实践。
- en: Designing an online retail data platform
  id: totrans-564
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计一个在线零售数据平台
- en: An online retailer wants to create an analytics platform to collect and analyze
    all the data generated by their e-commerce website. This platform aims to provide
    capabilities that allow for real-time data processing and analytics to improve
    customer experiences, optimize business operations, and drive strategic decision-making
    for the online retail business.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 一家在线零售商希望创建一个分析平台，用于收集和分析其电子商务网站生成的所有数据。该平台旨在提供实时数据处理和分析能力，以改善客户体验、优化业务运营并推动在线零售业务的战略决策。
- en: 'After long discussions with the team, we identified four main requirements
    to consider:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 在与团队的长时间讨论后，我们确定了四个主要需求需要考虑：
- en: '**Handle large volumes of transaction data**: The platform needs to efficiently
    ingest and transform large volumes of transaction data. This needs to be done
    by accounting for scalability, high throughput, and cost-effectiveness.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理大量交易数据**：平台需要高效地摄取和转换大量交易数据。这需要考虑可扩展性、高吞吐量和成本效益。'
- en: '**Provide real-time insights**: Business analysts require immediate access
    to real-time insights derived from transaction data. The platform should support
    real-time data processing and analytics to enable timely decision-making.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供实时洞察**：业务分析师需要即时访问从交易数据中得出的实时洞察。平台应支持实时数据处理和分析，以支持及时决策。'
- en: There’s a need to combine batch and streaming data ingestion to handle both
    the real-time website data and the batch customer data, which is updated slowly.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要结合批处理和流数据摄取，以处理实时网站数据和慢速更新的批量客户数据。
- en: '**Use AWS as the cloud provider**: The choice of the cloud provider (AWS) comes
    from the fact that the retailer is currently using other AWS services and wants
    to stick with the same provider.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 AWS 作为云服务提供商**：选择 AWS 作为云服务提供商，源于该零售商目前正在使用其他 AWS 服务，且希望继续使用同一提供商。'
- en: 'Let’s have a quick look at how we can solve these requirements:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下如何解决这些需求：
- en: 'Choose the right data sink technology:'
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择合适的数据接收技术：
- en: '**Thinking process**: A Lakehouse architecture is an ideal solution for the
    data platform requirements due to its ability to handle large volumes of data
    with scalability, high throughput, and cost-effectiveness. It leverages distributed
    storage and compute resources, allowing for efficient data ingestion and transformation.
    Additionally, the architecture supports real-time data processing and analytics,
    enabling business analysts to access immediate insights from transaction data
    for timely decision-making. By combining batch and streaming data ingestion, the
    Lakehouse seamlessly integrates real-time website data with batch-updated customer
    data.'
  id: totrans-573
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**思考过程**：由于 Lakehouse 架构能够处理大规模数据，具备可扩展性、高吞吐量和成本效益，因此它是满足数据平台需求的理想解决方案。它利用分布式存储和计算资源，实现高效的数据摄取和转换。此外，该架构支持实时数据处理和分析，使业务分析师能够即时访问交易数据，以便及时决策。通过结合批处理和流数据摄取，Lakehouse
    可以将实时网站数据与批量更新的客户数据无缝集成。'
- en: '**Choice**: A Lakehouse solution on AWS is selected for its scalability, cost-effectiveness,
    and seamless integration with other AWS services. AWS is compatible with a Lakehouse
    architecture.'
  id: totrans-574
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择**：选择 AWS 上的 Lakehouse 解决方案，因为其具有可扩展性、成本效益，并能够与其他 AWS 服务无缝集成。AWS 与 Lakehouse
    架构兼容。'
- en: 'Evaluate and choose the data file format:'
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估并选择数据文件格式：
- en: '**Data characteristics**: The customer data consists of structured transaction
    records, including customer IDs, product IDs, purchase amounts, timestamps, and
    geolocation. The streaming data includes customer IDs and other web metrics, such
    as what each customer is currently browsing on the website.'
  id: totrans-576
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据特征**：客户数据包括结构化的交易记录，包括客户ID、产品ID、购买金额、时间戳和地理位置。流数据包括客户ID和其他网站指标，例如每个客户当前在网站上浏览的内容。'
- en: '**Choice**: Delta file format is selected for its transactional capabilities
    and ACID compliance. It also supports batch and streaming workloads.'
  id: totrans-577
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择**：选择Delta文件格式是因为其事务能力和ACID合规性。它还支持批量和流式工作负载。'
- en: 'Implement data ingestion for batch and streaming data:'
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现批量和流数据的摄取：
- en: '**Data ingestion**: ETL processes are designed to transform incoming transaction
    data into Delta files. Real-time transaction data is streamed from AWS Kinesis
    for immediate processing and stored as Delta files while batch data coming from
    different other systems is integrated.'
  id: totrans-579
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据摄取**：ETL过程旨在将传入的交易数据转换为Delta文件。实时交易数据通过AWS Kinesis进行流式传输，以便即时处理，并作为Delta文件存储，同时来自不同系统的批量数据也被整合。'
- en: '**Partitioning logic**: Batch and streaming data are being processed and stored
    in Delta files. The streaming data is *partitioned by date when written out*.
    Next, transformations and data consolidation happen before it’s stored as the
    final analytical tables.'
  id: totrans-580
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区逻辑**：批量和流数据正在处理并存储为Delta文件。流数据在写入时按日期进行*分区*。接下来，进行转换和数据整合，然后将其存储为最终的分析表。'
- en: 'Define a partitioning strategy for analytical tables:'
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为分析表定义分区策略：
- en: '**Query patterns**: Analysts often query data based on certain periods of time
    and some tables based on product categories.'
  id: totrans-582
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询模式**：分析师通常基于特定时间段或基于产品类别的某些表进行数据查询。'
- en: '**Choice**: As we learned in the *Considerations for choosing partitioning
    strategies* section, we need to take into account the way users are querying the
    table. To get the best read performance out of the queries, time-based and category-based
    partitioning must be implemented. Data is partitioned by date and further partitioned
    by product category in analytics tables that the users query often.'
  id: totrans-583
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择**：正如我们在*选择分区策略的考虑因素*一节中所学，我们需要考虑用户查询表的方式。为了获得最佳的查询读取性能，必须实现基于时间和类别的分区。在用户经常查询的分析表中，数据按日期分区，并进一步按产品类别进行分区。'
- en: 'Monitor and optimize:'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控与优化：
- en: '**Performance monitoring**: Regularly monitor query performance, streaming
    throughput, and resource utilization using AWS monitoring and logging services'
  id: totrans-585
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能监控**：定期使用AWS监控和日志服务监控查询性能、流式传输吞吐量和资源利用率。'
- en: '**Optimization**: Continuously optimize both batch and streaming components
    based on observed performance and changing data patterns'
  id: totrans-586
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：根据观察到的性能和变化的数据模式，持续优化批量和流组件。'
- en: '**Schema evolution**: Ensure that the Delta schema accommodates streaming data
    changes and maintains compatibility with existing batch data'
  id: totrans-587
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构演变**：确保Delta架构能够适应流数据的变化，并与现有的批量数据保持兼容。'
- en: With this architecture, the online retail analytics platform gains the capability
    to process both batch and real-time data in an effective and cost-optimized way.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种架构，在线零售分析平台能够以有效且具有成本优化的方式处理批量和实时数据。
- en: Summary
  id: totrans-589
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Throughout this chapter, we focused on the components of designing and optimizing
    data write operations. We discussed how to choose the right data sink technology,
    how file formats significantly impact storage efficiency and query performance,
    and why it matters to choose the right one for your use case. Finally, we discussed
    why data partitioning is crucial for optimizing query performance and resource
    utilization.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们重点讨论了数据写入操作的设计和优化组件。我们讨论了如何选择合适的数据接收技术，文件格式如何显著影响存储效率和查询性能，以及为什么选择正确的文件格式对你的使用案例至关重要。最后，我们讨论了为什么数据分区对于优化查询性能和资源利用率至关重要。
- en: In the next chapter, we will start transforming the data that’s been written
    on the data sink to better prepare it for downstream analytics by detecting and
    handling outliers and missing values.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始转换已写入数据接收器的数据，通过检测和处理异常值和缺失值，为下游分析做好准备。
- en: 'Part 2: Downstream Data Cleaning – Consuming Structured Data'
  id: totrans-592
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2部分：下游数据清洗——消费结构化数据
- en: This part delves into the processes required for cleaning and preparing structured
    data for analysis, focusing on handling common data challenges that occur in more
    refined datasets. It provides practical techniques for managing missing values
    and outliers, ensuring data consistency through normalization and standardization,
    and effectively processing categorical features. Additionally, it introduces specialized
    methods for working with time series data, a common yet complex data type. By
    mastering these downstream cleaning and preparation techniques, readers will be
    well-equipped to turn structured data into actionable insights for advanced analytics.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分深入探讨了为分析准备结构化数据所需的清理过程，重点处理更精细数据集中常见的数据挑战。它提供了处理缺失值和离群值的实用技巧，通过归一化和标准化确保数据的一致性，并有效处理类别特征。此外，还介绍了处理时间序列数据的专业方法，这是一种常见但复杂的数据类型。通过掌握这些下游清理和准备技巧，读者将能够将结构化数据转化为可操作的见解，供高级分析使用。
- en: 'This part has the following chapters:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 8*](B19801_08.xhtml#_idTextAnchor195)*, Detecting and Handling Missing
    Values and Outliers*'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B19801_08.xhtml#_idTextAnchor195)*，检测和处理缺失值与离群值*'
- en: '[*Chapter 9*](B19801_09.xhtml#_idTextAnchor213)*, Normalization and Standardization*'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B19801_09.xhtml#_idTextAnchor213)*，归一化与标准化*'
- en: '[*Chapter 10*](B19801_10.xhtml#_idTextAnchor223)*, Handling Categorical Features*'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B19801_10.xhtml#_idTextAnchor223)*，处理类别特征*'
- en: '[*Chapter 11*](B19801_11.xhtml#_idTextAnchor246)*, Consuming Time Series Data*'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B19801_11.xhtml#_idTextAnchor246)*，处理时间序列数据*'
