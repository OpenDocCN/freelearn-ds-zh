- en: Structured Streaming with PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PySpark进行结构化流处理
- en: 'In this chapter, we will cover how to work with Apache Spark Structured Streaming
    within PySpark. You will learn the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何在PySpark中使用Apache Spark结构化流处理。您将学习以下内容：
- en: Understanding DStreams
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解DStreams
- en: Understanding global aggregations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解全局聚合
- en: Continuous aggregations with structured streaming
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用结构化流进行连续聚合
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: With the prevalence of machine-generated *real-time data*, including but not
    limited to IoT sensors, devices, and beacons, it is increasingly important to
    gain insight into this fire hose of data as quickly as it is being created. Whether
    you are detecting fraudulent transactions, real-time detection of sensor anomalies,
    or sentiment analysis of the next cat video, streaming analytics is an increasingly
    important differentiator and business advantage.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器生成的*实时数据*的普及，包括但不限于物联网传感器、设备和信标，迅速获得这些数据的洞察力变得越来越重要。无论您是在检测欺诈交易、实时检测传感器异常，还是对下一个猫视频的情感分析，流分析都是一个越来越重要的差异化因素和商业优势。
- en: As we progress through these recipes, we will be combining the constructs of
    *batch* and *real-time* processing for the creation of continuous applications.
    With Apache Spark, data scientists and data engineers can analyze their data using
    Spark SQL in batch and in real time, train machine learning models with MLlib,
    and score these models via Spark Streaming.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们逐步学习这些内容，我们将结合*批处理*和*实时*处理的构建来创建连续应用。使用Apache Spark，数据科学家和数据工程师可以使用Spark
    SQL在批处理和实时中分析数据，使用MLlib训练机器学习模型，并通过Spark Streaming对这些模型进行评分。
- en: 'An important reason for the rapid adoption of Apache Spark is that it unifies
    all of these disparate data processing paradigms (machine learning via ML and
    MLlib, Spark SQL, and streaming). As note, in *Spark Streaming: What is It and
    Who’s Using it* ([https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/](https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/)),
    companies such as Uber, Netflix, and Pinterest often showcase their uses case
    through Spark Streaming:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'Apache Spark迅速被广泛采用的一个重要原因是它统一了所有这些不同的数据处理范式（通过ML和MLlib进行机器学习，Spark SQL和流处理）。正如在*Spark
    Streaming: What is It and Who’s Using it*（[https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/](https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/)）中所述，像Uber、Netflix和Pinterest这样的公司经常通过Spark
    Streaming展示他们的用例：'
- en: '*How Uber Uses Spark and Hadoop to Optimize Customer Experience* at [https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/](https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*How Uber Uses Spark and Hadoop to Optimize Customer Experience*在[https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/](https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/)'
- en: Spark and Spark Streaming at Netflix at [https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/](https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Netflix的Spark和Spark Streaming在[https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/](https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/)
- en: Can Spark Streaming survive Chaos Monkey? at [http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html](http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming能否经受Chaos Monkey的考验？在[http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html](http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html)
- en: Real-time analytics at Pinterest at [https://engineering.pinterest.com/blog/real-time-analytics-pinterest](https://engineering.pinterest.com/blog/real-time-analytics-pinterest)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinterest的实时分析在[https://engineering.pinterest.com/blog/real-time-analytics-pinterest](https://engineering.pinterest.com/blog/real-time-analytics-pinterest)
- en: Understanding Spark Streaming
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark Streaming
- en: 'For real-time processing in Apache Spark, the current focus is on structured
    streaming, which is built on top of the DataFrame/dataset infrastructure. The
    use of DataFrame abstraction allows streaming, machine learning, and Spark SQL
    to be optimized in the Spark SQL Engine Catalyst Optimizer and its regular improvements
    (for example, Project Tungsten). Nevertheless, to more easily understand Spark
    Streaming, it is worthwhile to understand the fundamentals of its Spark Streaming
    predecessor. The following diagram represents a Spark Streaming application data
    flow involving the Spark driver, workers, streaming sources, and streaming targets:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Apache Spark中的实时处理，当前的重点是结构化流，它是建立在DataFrame/数据集基础设施之上的。使用DataFrame抽象允许在Spark
    SQL引擎Catalyst Optimizer中对流处理、机器学习和Spark SQL进行优化，并且定期进行改进（例如，Project Tungsten）。然而，为了更容易地理解Spark
    Streaming，值得了解其Spark Streaming前身的基本原理。以下图表代表了涉及Spark驱动程序、工作程序、流源和流目标的Spark Streaming应用程序数据流：
- en: '![](img/00164.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00164.jpeg)'
- en: 'The description of the preceding diagram is as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图表的描述如下：
- en: Starting with the **Spark Streaming Context** (**SSC**), the driver will execute
    long-running tasks on the executors (that is, the Spark workers).
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**Spark Streaming Context**（**SSC**）开始，驱动程序将在执行程序（即Spark工作程序）上执行长时间运行的任务。
- en: The code defined within the driver (starting `ssc.start()`), the **Receiver**
    on the executors (**Executor 1** in this diagram) receives a data stream from
    the **Streaming Sources**. Spark Streaming can receive **Kafka** or **Twitter**,
    and/or you can build your own custom receiver. With the incoming data stream,
    the receiver divides the stream into blocks and keeps these blocks in memory.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在驱动程序中定义的代码（从`ssc.start()`开始），执行程序（在此图中为**Executor 1**）从**流源**接收数据流。Spark Streaming可以接收**Kafka**或**Twitter**，或者您可以构建自己的自定义接收器。接收器将数据流分成块并将这些块保存在内存中。
- en: These data blocks are replicated to another executor for high availability.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些数据块被复制到另一个执行程序以实现高可用性。
- en: The block ID information is transmitted to the block manager master on the driver,
    thus ensuring that each block of data in memory is tracked and accounted for.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 块ID信息被传输到驱动程序上的块管理器主节点，从而确保内存中的每个数据块都被跟踪和记录。
- en: For every batch interval configured within SSC (commonly, this is every 1 second),
    the driver will launch Spark tasks to process the blocks. Those blocks are then
    persisted to any number of target data stores, including cloud storage (for example,
    S3, WASB), relational data stores (for example, MySQL, PostgreSQL, and so on),
    and NoSQL stores.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于SSC中配置的每个批处理间隔（通常是每1秒），驱动程序将启动Spark任务来处理这些块。这些块然后被持久化到任意数量的目标数据存储中，包括云存储（例如S3、WASB）、关系型数据存储（例如MySQL、PostgreSQL等）和NoSQL存储。
- en: In the following sections, we will review recipes with **Discretized Streams**
    or **DStreams** (the fundamental streaming building block) and then perform global
    aggregations by performing stateful calculations on DStreams. We will then simplify
    our streaming application by using structured streaming while at the same time
    gaining performance optimizations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将回顾**离散流**或**DStreams**（基本的流构建块）的示例，然后通过对DStreams进行有状态的计算来执行全局聚合。然后，我们将通过使用结构化流简化我们的流应用程序，同时获得性能优化。
- en: Understanding DStreams
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解DStreams
- en: 'Before we dive into structured streaming, let''s start by talking about DStreams.
    DStreams are built on top of RDDs and represent a stream of data divided into
    small chunks. The following figure represents these data chunks in micro-batches
    of milliseconds to seconds. In this example, the lines of DStream is micro-batched
    into seconds where each square represents a micro-batch of events that occurred
    within that second window:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论结构化流之前，让我们先谈谈DStreams。DStreams是建立在RDDs之上的，表示被分成小块的数据流。下图表示这些数据块以毫秒到秒的微批次形式存在。在这个例子中，DStream的行被微批次到秒中，每个方块代表在那一秒窗口内发生的一个微批次事件：
- en: At time interval 1 second, there were five occurrences of the event **blue**
    and three occurrences of the event **green**
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在1秒的时间间隔内，事件**blue**出现了五次，事件**green**出现了三次
- en: At time interval 2 seconds, there is a single occurrence of **gohawks**
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在2秒的时间间隔内，事件**gohawks**出现了一次
- en: At time interval 4 seconds, there are two occurrences of the event **green**
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在4秒的时间间隔内，事件**green**出现了两次
- en: '![](img/00165.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00165.jpeg)'
- en: Because DStreams are built on top of RDDs, Apache Spark's core data abstraction,
    this allows Spark Streaming to easily integrate with other Spark components such
    as MLlib and Spark SQL.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因为DStreams是建立在RDDs之上的，Apache Spark的核心数据抽象，这使得Spark Streaming可以轻松地与其他Spark组件（如MLlib和Spark
    SQL）集成。
- en: Getting ready
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: For these Apache Spark Streaming examples, we will be creating and executing
    a console application via the bash terminal. To make things easier, you will want
    to have two terminal windows open.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些Apache Spark Streaming示例，我们将通过bash终端创建和执行一个控制台应用程序。为了简化操作，你需要打开两个终端窗口。
- en: How to do it...
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'As noted in the previous section, we will use two terminal windows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，我们将使用两个终端窗口：
- en: One terminal window to transmit an event
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个终端窗口传输一个事件
- en: Another terminal to receive those events
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个终端接收这些事件
- en: 'Note that the source code for this can be found in the Apache Spark 1.6 Streaming
    Programming Guide at: [https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此代码的源代码可以在Apache Spark 1.6 Streaming编程指南中找到：[https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html)。
- en: Terminal 1 – Netcat window
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 终端1 - Netcat窗口
- en: 'For the first window, we will use Netcat (or nc) to manually send events such
    as blue, green, and gohawks. To start Netcat, use the following command; we will
    direct our events to port `9999`, where our Spark Streaming job will detect:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个窗口，我们将使用Netcat（或nc）手动发送事件，如blue、green和gohawks。要启动Netcat，请使用以下命令；我们将把我们的事件定向到端口`9999`，我们的Spark
    Streaming作业将会检测到：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To match the previous diagram, we will type in our events so that the console
    screen looks like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了匹配上一个图表，我们将输入我们的事件，使得控制台屏幕看起来像这样：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Terminal 2 – Spark Streaming window
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 终端2 - Spark Streaming窗口
- en: 'We will create a simple PySpark Streaming application using the following code
    called `streaming_word_count.py`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码创建一个简单的PySpark Streaming应用程序，名为`streaming_word_count.py`：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To run this PySpark Streaming application, execute the following command from
    your `$SPARK_HOME` folder:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这个PySpark Streaming应用程序，请在`$SPARK_HOME`文件夹中执行以下命令：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In terms of how you time this, you should:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间上的安排，你应该：
- en: First start with `nc -lk 9999`.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先使用`nc -lk 9999`。
- en: 'Then, start your PySpark Streaming application: `/bin/spark-submit streaming_word_count.py
    localhost 9999`.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，启动你的PySpark Streaming应用程序：`/bin/spark-submit streaming_word_count.py localhost
    9999`。
- en: 'Then, start typing your events, for example:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，开始输入你的事件，例如：
- en: For the first second, type `blue blue blue blue blue green green green`
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第一秒，输入`blue blue blue blue blue green green green`
- en: For the second second, type `gohawks`
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二秒时，输入`gohawks`
- en: Wait a second; for the fourth second, type `green green`
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等一下，在第四秒时，输入`green green`
- en: 'The console output from your PySpark streaming application will look something
    similar to this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你的PySpark流应用程序的控制台输出将类似于这样：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To end the streaming application (and the `nc` window, for that matter), execute
    a termination command (for example, *Ctrl* + *C*).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要结束流应用程序（以及`nc`窗口），执行终止命令（例如，*Ctrl* + *C*）。
- en: How it works...
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As noted in the previous subsections, this recipe is comprised of one terminal
    window transmitting event data using `nc`. The second window runs our Spark Streaming
    application, reading from the port that the first window is transmitting to.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的小节所述，这个示例由一个终端窗口组成，用`nc`传输事件数据。第二个窗口运行我们的Spark Streaming应用程序，从第一个窗口传输到的端口读取数据。
- en: 'The important call outs for this code are noted here:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的重要调用如下所示：
- en: We're creating a Spark context using two working threads, hence the use of `local[2]`.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用两个工作线程创建一个Spark上下文，因此使用`local[2]`。
- en: As noted in the Netcat window, we're using `ssc.socketTextStream` to listen
    to the local socket of the `localhost`, port `9999`.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如Netcat窗口中所述，我们使用`ssc.socketTextStream`来监听`localhost`的本地套接字，端口为`9999`。
- en: Recall that for each 1-second batch, we're not only reading a single line (for
    example, `blue blue blue blue blue green green green`), but also splitting it
    up into individual `words` via `split`.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请记住，对于每个1秒批处理，我们不仅读取一行（例如`blue blue blue blue blue green green green`），还通过`split`将其拆分为单独的`words`。
- en: We're using a Python `lambda` function and PySpark `map` and `reduceByKey` functions
    to quickly count the occurrences of words within the 1-second batch. For example,
    in the case of `blue blue blue blue blue green green green`, there are five blue
    and three green events, as reported at *2018-06-21 23:00:30* of our streaming
    application.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用Python的`lambda`函数和PySpark的`map`和`reduceByKey`函数来快速计算1秒批处理中单词的出现次数。例如，在`blue
    blue blue blue blue green green green`的情况下，有五个蓝色和三个绿色事件，如我们的流应用程序的*2018-06-21
    23:00:30*报告的那样。
- en: '`ssc.start()` is in reference to the application starting the Spark Streaming
    context.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssc.start()`是指应用程序启动Spark Streaming上下文。'
- en: '`ssc.awaitTermination()` is waiting for a termination command to stop the streaming
    application (for example, *Ctrl* + *C*); otherwise, the application will continue
    to run.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssc.awaitTermination()`正在等待终止命令来停止流应用程序（例如*Ctrl* + *C*）；否则，应用程序将继续运行。'
- en: There's more...
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'When using the PySpark console, often there are a lot of messages that are
    sent out to the console that can make it difficult to read the streaming output.
    To make it easier to read, ensure that you have created and modified the `log4j.properties`
    file within the `$SPARK_HOME/conf` folder. To do this, follow these steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用PySpark控制台时，通常会有很多消息发送到控制台，这可能会使流输出难以阅读。为了更容易阅读，请确保您已经创建并修改了`$SPARK_HOME/conf`文件夹中的`log4j.properties`文件。要做到这一点，请按照以下步骤操作：
- en: Go to the `$SPARK_HOME/conf` folder.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`$SPARK_HOME/conf`文件夹。
- en: 'By default, there is a `log4j.properties.template` file. Copy it with the same
    name, removing the `.template`, that is:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，有一个`log4j.properties.template`文件。将其复制为相同的名称，删除`.template`，即：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Edit the `log4j.properties` in your favorite editor (for example, sublime,
    vi, and so on). In line 19 of the file, change this line:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您喜欢的编辑器（例如sublime、vi等）中编辑`log4j.properties`。在文件的第19行，更改此行：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 改为：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This way, instead of all log information (that is, `INFO`) being directed to
    the console, only errors (that is, `ERROR`) will be directed to the console.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，不是所有的日志信息（即`INFO`）都被定向到控制台，只有错误（即`ERROR`）会被定向到控制台。
- en: Understanding global aggregations
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解全局聚合
- en: 'In the previous section, our recipe provided a snapshot count of events. That
    is, it provided the count of events at the point in time. But what if you want
    to understand a sum of events for some time window? This is the concept of global
    aggregations:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们的示例提供了事件的快照计数。也就是说，它提供了在某一时间点的事件计数。但是，如果您想要了解一段时间窗口内的事件总数呢？这就是全局聚合的概念：
- en: '![](img/00166.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00166.jpeg)'
- en: 'If we wanted global aggregations, the same example as before (Time 1: 5 blue,
    3 green, Time 2: 1 gohawks, Time 4: 2 greens) would be calculated as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要全局聚合，与之前相同的示例（时间1：5蓝色，3绿色，时间2：1 gohawks，时间4：2绿色）将被计算为：
- en: 'Time 1: 5 blue, 3 green'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间1：5蓝色，3绿色
- en: 'Time 2: 5 blue, 3 green, 1 gohawks'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间2：5蓝色，3绿色，1 gohawks
- en: 'Time 4: 5 blue, 5 green, 1 gohawks'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间4：5蓝色，5绿色，1 gohawks
- en: Within the traditional batch calculations, this would be similar to a `groupbykey`
    or `GROUP BY` statement. But in the case of streaming applications, this calculation
    needs to be done within milliseconds, which is typically too short of a time window
    to perform a `GROUP BY` calculation. However, with Spark Streaming global aggregations,
    this calculation can be completed quickly by performing a stateful streaming calculation.
    That is, using the Spark Streaming framework, all of the information to perform
    the aggregation is kept in memory (that is, keeping the data in *state*) so that
    it can be calculated in its small time window.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的批处理计算中，这将类似于`groupbykey`或`GROUP BY`语句。但是在流应用程序的情况下，这个计算需要在毫秒内完成，这通常是一个太短的时间窗口来执行`GROUP
    BY`计算。然而，通过Spark Streaming全局聚合，可以通过执行有状态的流计算来快速完成这个计算。也就是说，使用Spark Streaming框架，执行聚合所需的所有信息都保存在内存中（即保持数据在*state*中），以便在其小时间窗口内进行计算。
- en: Getting ready
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: For these Apache Spark Streaming examples, we will be creating and executing
    a console application via the bash terminal. To make things easier, you will want
    to have two terminal windows open.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些Apache Spark Streaming示例，我们将通过bash终端创建和执行一个控制台应用程序。为了简化操作，您需要打开两个终端窗口。
- en: How to do it...
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'As noted in the previous section, we will use two terminal windows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，我们将使用两个终端窗口：
- en: One terminal window to transmit an event
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个终端窗口用于传输事件
- en: Another terminal to receive those events
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个终端接收这些事件
- en: 'The source code for this can be found in the Apache Spark 1.6 Streaming Programming
    Guide at: [https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的源代码可以在Apache Spark 1.6 Streaming编程指南中找到：[https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html)。
- en: Terminal 1 – Netcat window
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 终端1 - Netcat窗口
- en: 'For the first window, we will use Netcat (or `nc`) to manually send events
    such as blue, green, and gohawks. To start Netcat, use the following command;
    we will direct our events to port `9999` where our Spark Streaming job will detect:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个窗口，我们将使用Netcat（或`nc`）手动发送事件，如蓝色、绿色和gohawks。要启动Netcat，请使用以下命令；我们将把我们的事件定向到端口`9999`，我们的Spark
    Streaming作业将检测到：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To match the previous diagram, we will type in our events so that the console
    screen looks like this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了匹配前面的图表，我们将输入我们的事件，以便控制台屏幕看起来像这样：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Terminal 2 – Spark Streaming window
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 终端2 - Spark Streaming窗口
- en: 'We will create a simple PySpark Streaming application using the following code
    called `streaming_word_count.py`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码创建一个简单的PySpark Streaming应用程序，名为`streaming_word_count.py`：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To run this PySpark Streaming application, execute the following command from
    your `$SPARK_HOME` folder:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此PySpark Streaming应用程序，请从您的`$SPARK_HOME`文件夹执行以下命令：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In terms of how you time this, you should:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在计时方面，您应该：
- en: First start with `nc -lk 9999`.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先使用`nc -lk 9999`。
- en: Then, start your PySpark Streaming application: `./bin/spark-submit stateful_streaming_word_count.py
    localhost 9999`.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，启动您的PySpark Streaming应用程序：`./bin/spark-submit stateful_streaming_word_count.py
    localhost 9999`。
- en: 'Then, start typing your events, for example:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，开始输入您的事件，例如：
- en: For the first second, type `blue blue blue blue blue green green green`
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一秒，输入`blue blue blue blue blue green green green`
- en: For the second second, type `gohawks`
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二秒，输入`gohawks`
- en: Wait a second; for the fourth second, type `green green`
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等一秒；第四秒，输入`green green`
- en: 'The console output from your PySpark streaming application will look something
    similar to the following output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您的PySpark流应用程序的控制台输出将类似于以下输出：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To end the streaming application (and the `nc` window, for that matter), execute
    a termination command (for example, *Ctrl* + *C*).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要结束流应用程序（以及`nc`窗口），执行终止命令（例如，*Ctrl* + *C*）。
- en: How it works...
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As noted in the previous subsections, this recipe is comprised of one terminal
    window transmitting event data using `nc`. The second window runs our Spark Streaming
    application reading from the port that the first window is transmitting to.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几节所述，这个示例由一个终端窗口传输事件数据使用`nc`组成。第二个窗口运行我们的Spark Streaming应用程序，从第一个窗口传输到的端口读取数据。
- en: 'The important call outs for this code are noted here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的重要调用如下所示：
- en: We're creating a Spark context using two working threads, hence the use of `local[2]`.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用两个工作线程创建一个Spark上下文，因此使用`local[2]`。
- en: As noted in the Netcat window, we're using `ssc.socketTextStream` to listen
    to the local socket of the `localhost`, port `9999`.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如Netcat窗口中所述，我们使用`ssc.socketTextStream`来监听`localhost`的本地套接字，端口为`9999`。
- en: We have created a `updateFunc`, which performs the task of aggregating the previous
    value with the currently aggregated value.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建了一个`updateFunc`，它执行将先前的值与当前聚合值进行聚合的任务。
- en: Recall that for each 1-second batch, we're not only reading a single line (for
    example, `blue blue blue blue blue green green green`) but also splitting it up
    into individual `words` via `split`.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请记住，对于每个1秒批处理，我们不仅仅是读取一行（例如，`blue blue blue blue blue green green green`），还要通过`split`将其拆分为单独的`words`。
- en: We're using a Python `lambda` function and PySpark `map` and `reduceByKey` functions
    to quickly count the occurrences of words within the 1-second batch. For example,
    in the case of `blue blue blue blue blue green green green`, there are 5 blue
    and 3 green events, as reported at *2018-06-21 23:00:30* of our streaming application.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用Python的`lambda`函数和PySpark的`map`和`reduceByKey`函数来快速计算1秒批处理中单词的出现次数。例如，在`blue
    blue blue blue blue green green green`的情况下，有5个蓝色和3个绿色事件，如我们的流应用程序的*2018-06-21
    23:00:30*报告的那样。
- en: The difference between the previous streaming application vs. the current stateful
    version is that we're calculating running counts (`running_counts`) with the current
    aggregation (for example, five blue and three green events) with `updateStateByKey`.
    This allows Spark Streaming to keep the state of the current aggregation within
    the context of the previously defined `updateFunc`.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与以前的流应用程序相比，当前的有状态版本计算了当前聚合（例如，五个蓝色和三个绿色事件）的运行计数（`running_counts`），并使用`updateStateByKey`。这使得Spark
    Streaming可以在先前定义的`updateFunc`的上下文中保持当前聚合的状态。
- en: '`ssc.start()` is in reference to the application starting the Spark Streaming
    context.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssc.start()`是指应用程序启动Spark Streaming上下文。'
- en: '`ssc.awaitTermination()` is waiting for a termination command to stop the streaming
    application (for example, *Ctrl* + *C*); otherwise, the application will continue
    to run.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssc.awaitTermination()`正在等待终止命令以停止流应用程序（例如，*Ctrl* + *C*）；否则，应用程序将继续运行。'
- en: Continuous aggregation with structured streaming
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用结构化流进行连续聚合
- en: 'As noted in earlier chapters, the execution of Spark SQL or DataFrame queries revolves
    around building a logical plan, choosing a physical plan (of the many generated
    physical plans) based on its cost optimizer, and then generating the code (that
    is, code gen) via the Spark SQL Engine Catalyst Optimizer. What structured streaming
    introduces is the concept of an *incremental* execution plan. That is, structured
    streaming repeatedly applies the execution plan for every new block of data it
    receives. This way, the Spark SQL engine can take advantage of the optimizations
    included within Spark DataFrames and apply them to an incoming data stream. Because
    structured streaming is built on top of Spark DataFrames, this means it will also
    be easier to integrate other DataFrame-optimized components, including MLlib,
    GraphFrames, TensorFrames, and so on:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，Spark SQL或DataFrame查询的执行围绕着构建逻辑计划，选择一个基于成本优化器的物理计划（从生成的物理计划中选择一个），然后通过Spark
    SQL引擎Catalyst优化器生成代码（即代码生成）。结构化流引入的概念是*增量*执行计划。也就是说，结构化流会针对每个新的数据块重复应用执行计划。这样，Spark
    SQL引擎可以利用包含在Spark DataFrames中的优化，并将其应用于传入的数据流。因为结构化流是构建在Spark DataFrames之上的，这意味着它也将更容易地集成其他DataFrame优化的组件，包括MLlib、GraphFrames、TensorFrames等等：
- en: '![](img/00167.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00167.jpeg)'
- en: Getting ready
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: For these Apache Spark Streaming examples, we will be creating and executing
    a console application via the bash terminal. To make things easier, you will want
    to have two terminal windows open.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些Apache Spark Streaming示例，我们将通过bash终端创建和执行控制台应用程序。为了使事情变得更容易，您需要打开两个终端窗口。
- en: How to do it...
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'As noted in the previous section, we will use two terminal windows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，我们将使用两个终端窗口：
- en: One terminal window to transmit an event
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个终端窗口传输一个事件
- en: Another terminal to receive those events
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个终端接收这些事件
- en: 'The source code for this can be found in the Apache Spark 2.3.1 Structured
    Streaming Programming Guide at: [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此源代码可以在Apache Spark 2.3.1结构化流编程指南中找到：[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)。
- en: Terminal 1 – Netcat window
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 终端1-Netcat窗口
- en: 'For the first window, we will use Netcat (or `nc`) to manually send events
    such as blue, green, and gohawks. To start Netcat, use this command; we will direct
    our events to port `9999`, where our Spark Streaming job will detect:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个窗口，我们将使用Netcat（或`nc`）手动发送事件，例如blue、green和gohawks。要启动Netcat，请使用此命令；我们将把我们的事件定向到端口`9999`，我们的Spark
    Streaming作业将检测到：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To match the previous diagram, we will type in our events so that the console
    screen looks like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了匹配之前的图表，我们将输入我们的事件，以便控制台屏幕看起来像这样：
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Terminal 2 – Spark Streaming window
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 终端2-Spark Streaming窗口
- en: 'We will create a simple PySpark Streaming application using the following code
    called `structured_streaming_word_count.py`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码创建一个简单的PySpark Streaming应用程序，名为`structured_streaming_word_count.py`：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To run this PySpark Streaming application, execute the following command from
    your `$SPARK_HOME` folder:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此PySpark Streaming应用程序，请从您的`$SPARK_HOME`文件夹执行以下命令：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In terms of how you time this, you should:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在计时方面，您应该：
- en: First start with `nc -lk 9999`.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先从`nc -lk 9999`开始。
- en: Then, start your PySpark Streaming application: `./bin/spark-submit stateful_streaming_word_count.py
    localhost 9999`.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，启动您的PySpark Streaming应用程序：`./bin/spark-submit stateful_streaming_word_count.py
    localhost 9999`。
- en: 'Then, start typing your events, for example:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，开始输入您的事件，例如：
- en: For the first second, type `blue blue blue blue blue green green green`
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第一秒，输入`blue blue blue blue blue green green green`
- en: For the second second, type `gohawks`
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第二秒，输入`gohawks`
- en: Wait a second; for the fourth second, type `green green`
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等一下；在第四秒，输入`green green`
- en: 'The console output from your PySpark streaming application will look something
    similar to the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 您的PySpark流应用程序的控制台输出将类似于以下内容：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To end the streaming application (and the `nc` window, for that matter), execute
    a termination command (for example, *Ctrl* + *C*).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要结束流应用程序（以及`nc`窗口），执行终止命令（例如，*Ctrl* + *C*）。
- en: Similar to global aggregations with DStreams, with structured streaming, you
    can easily perform stateful global aggregations within the context of a DataFrame.
    Another optimization you'll notice with structured streaming is that the streaming
    aggregations will only appear whenever there are new events. Specifically notice
    how when we delayed between time = 2s and time = 4s, there is not an extra batch
    being reported to the console.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与DStreams的全局聚合类似，使用结构化流，您可以在DataFrame的上下文中轻松执行有状态的全局聚合。您还会注意到结构化流的另一个优化是，只有在有新事件时，流聚合才会出现。特别注意当我们在时间=2秒和时间=4秒之间延迟时，控制台没有额外的批次报告。
- en: How it works...
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As noted in the previous subsections, this recipe is comprised of one terminal
    window transmitting event data using `nc`. The second window runs our Spark Streaming
    application, reading from the port that the first window is transmitting to.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文所述，此示例由一个终端窗口组成，该窗口使用`nc`传输事件数据。第二个窗口运行我们的Spark Streaming应用程序，从第一个窗口传输到的端口读取。
- en: 'The important callouts for this code are noted here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的重要部分在这里标出：
- en: Instead of creating a Spark context, we're creating a `SparkSession`
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建一个`SparkSession`而不是创建一个Spark上下文
- en: With the SparkSession, we can use `readStream` to specify the `socket` *format*
    to specify that we're listening to `localhost` at port `9999`
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有了SparkSession，我们可以使用`readStream`指定`socket` *format*来指定我们正在监听`localhost`的端口`9999`
- en: We use the PySpark SQL functions `split` and `explode` to take our `line` and
    break it down to `words`
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用PySpark SQL函数`split`和`explode`来获取我们的`line`并将其拆分为`words`
- en: To generate our running word count, we need only to create `wordCounts` to run
    a `groupBy` statement and `count()` on `words`
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要生成我们的运行词计数，我们只需要创建`wordCounts`来运行`groupBy`语句和`count()`在`words`上
- en: Finally, we will use `writeStream` to write out the `complete` set of `query`
    data to the `console` (as opposed to some other data sink)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将使用`writeStream`将`query`数据的`complete`集写入`console`（而不是其他数据汇）
- en: Because we're using a Spark session, the application is waiting for a termination
    command to stop the streaming application (for example, <Ctrl><C>) via `query.awaitTermination()`
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为我们正在使用一个Spark会话，该应用程序正在等待终止命令来停止流应用程序（例如，<Ctrl><C>）通过`query.awaitTermination()`
- en: Because structured streaming is using DataFrames, it is simpler and easier to
    read because we're using the familiar DataFrame abstraction while also gaining
    all the performance optimizations of DataFrames.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因为结构化流使用DataFrames，所以它更简单、更容易阅读，因为我们使用熟悉的DataFrame抽象，同时也获得了所有DataFrame的性能优化。
