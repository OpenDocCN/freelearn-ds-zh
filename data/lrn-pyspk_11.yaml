- en: Chapter 11. Packaging Spark Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 11 章. 打包 Spark 应用程序
- en: So far we have been working with a very convenient way of developing code in
    Spark - the Jupyter notebooks. Such an approach is great when you want to develop
    a proof of concept and document what you do along the way.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用一种非常方便的方式来在 Spark 中开发代码 - Jupyter 笔记本。当您想开发一个概念验证并记录您所做的工作时，这种方法非常出色。
- en: However, Jupyter notebooks will not work if you need to schedule a job, so it
    runs every hour. Also, it is fairly hard to package your application as it is
    not easy to split your script into logical chunks with well-defined APIs - everything
    sits in a single notebook.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您需要安排作业，Jupyter 笔记本将无法工作，因此它每小时运行一次。此外，打包您的应用程序相当困难，因为很难将脚本分割成具有良好定义的 API
    的逻辑块 - 所有的内容都位于单个笔记本中。
- en: In this chapter, we will learn how to write your scripts in a reusable form
    of modules and submit jobs to Spark programmatically.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何以模块化的形式编写您的脚本，并编程提交作业到 Spark。
- en: 'Before you begin, however, you might want to check out the *Bonus Chapter 2,
    Free Spark Cloud Offering* where we provide instructions on how to subscribe and
    use either Databricks'' Community Edition or Microsoft''s HDInsight Spark offerings;
    the instructions on how to do so can be found here: [https://www.packtpub.com/sites/default/files/downloads/FreeSparkCloudOffering.pdf](https://www.packtpub.com/sites/default/files/downloads/FreeSparkCloudOffering.pdf).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，您可能想查看 *Bonus Chapter 2, Free Spark Cloud Offering*，其中我们提供了如何订阅和使用 Databricks
    的社区版或 Microsoft 的 HDInsight Spark 提供的说明；如何做到这一点的说明可以在此处找到：[https://www.packtpub.com/sites/default/files/downloads/FreeSparkCloudOffering.pdf](https://www.packtpub.com/sites/default/files/downloads/FreeSparkCloudOffering.pdf)。
- en: 'In this chapter you will learn:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习：
- en: What the `spark-submit` command is
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark-submit` 命令是什么'
- en: How to package and deploy your app programmatically
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何以编程方式打包和部署您的应用程序
- en: How to modularize your Python code and submit it along with PySpark script
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何模块化您的 Python 代码并将其与 PySpark 脚本一起提交
- en: The spark-submit command
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`spark-submit` 命令'
- en: The entry point for submitting jobs to Spark (be it locally or on a cluster)
    is the `spark-submit` script. The script, however, allows you not only to submit
    the jobs (although that is its main purpose), but also kill jobs or check their
    status.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提交作业到 Spark（无论是本地还是集群）的入口点是 `spark-submit` 脚本。然而，该脚本不仅允许您提交作业（尽管这是其主要目的），还可以终止作业或检查其状态。
- en: Note
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Under the hood, the `spark-submit` command passes the call to the `spark-class`
    script that, in turn, starts a launcher Java application. For those interested,
    you can check the GitHub repository for Spark: [https://github.com/apache/spark/blob/master/bin/sparksubmit](https://github.com/apache/spark/blob/master/bin/spark-submit)t.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，`spark-submit` 命令将调用 `spark-class` 脚本，该脚本反过来启动一个启动器 Java 应用程序。对于感兴趣的人来说，可以查看
    Spark 的 GitHub 仓库：[https://github.com/apache/spark/blob/master/bin/sparksubmit](https://github.com/apache/spark/blob/master/bin/spark-submit)。
- en: The `spark-submit` command provides a unified API for deploying apps on a variety
    of Spark supported cluster managers (such as Mesos or Yarn), thus relieving you
    from configuring your application for each of them separately.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark-submit` 命令为在多种 Spark 支持的集群管理器（如 Mesos 或 Yarn）上部署应用程序提供了一个统一的 API，从而让您无需分别对每个应用程序进行配置。'
- en: 'On the general level, the syntax looks as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般层面上，语法如下所示：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will go through the list of all the options soon. The `app arguments` are
    the parameters you want to pass to your application.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就会查看所有选项的列表。`app arguments` 是您想要传递给应用程序的参数。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can either parse the parameters from the command line yourself using `sys.argv`
    (after `import sys`) or you can utilize the `argparse` module for Python.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `sys.argv`（在 `import sys` 之后）自行解析命令行参数，或者可以使用 Python 的 `argparse` 模块。
- en: Command line parameters
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命令行参数
- en: You can pass a host of different parameters for Spark engine when using `spark-submit`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `spark-submit` 时，您可以传递大量针对 Spark 引擎的参数。
- en: Note
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In what follows we will cover only the parameters specific for Python (as `spark-submit`
    can also be used to submit applications written in Scala or Java and packaged
    as `.jar` files).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下内容中，我们将仅介绍针对 Python 的特定参数（因为 `spark-submit` 也可以用于提交用 Scala 或 Java 编写的应用程序，并打包为
    `.jar` 文件）。
- en: 'We will now go through the parameters one-by-one so you have a good overview
    of what you can do from the command line:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐一介绍参数，以便您对从命令行可以执行的操作有一个良好的概述：
- en: '`--master`: Parameter used to set the URL of the master (head) node. Allowed
    syntax is:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--master`：用于设置主（头）节点 URL 的参数。允许的语法是：'
- en: '`local`: Used for executing your code on your local machine. If you pass `local`,
    Spark will then run in a single thread (without leveraging any parallelism). On
    a multi-core machine you can specify either, the exact number of cores for Spark
    to use by stating `local[n]` where `n` is the number of cores to use, or run Spark
    spinning as many threads as there are cores on the machine using `local[*]`.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local`: 用于在你的本地机器上执行你的代码。如果你传递`local`，Spark将随后在单个线程中运行（不利用任何并行性）。在多核机器上，你可以指定Spark要使用的确切核心数，通过指定`local[n]`，其中`n`是要使用的核心数，或者使用`local[*]`运行Spark，使其以机器上的核心数创建尽可能多的线程。'
- en: '`spark://host:port`: It is a URL and a port for the Spark standalone cluster
    (that does not run any job scheduler such as Mesos or Yarn).'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark://host:port`: 这是一个Spark独立集群的URL和端口号（不运行任何作业调度器，如Mesos或Yarn）。'
- en: '`mesos://host:port`: It is a URL and a port for the Spark cluster deployed
    over Mesos.'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mesos://host:port`: 这是一个部署在Mesos上的Spark集群的URL和端口号。'
- en: '`yarn`: Used to submit jobs from a head node that runs Yarn as the workload
    balancer.'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`yarn`: 用于从运行Yarn作为工作负载均衡器的头节点提交作业。'
- en: '`--deploy-mode`: Parameter that allows you to decide whether to launch the
    Spark driver process locally (using `client`) or on one of the worker machines
    inside the cluster (using the `cluster` option). The default for this parameter
    is `client`. Here''s an excerpt from Spark''s documentation that explains the
    differences with more specificity (source: [http://bit.ly/2hTtDVE](http://bit.ly/2hTtDVE)):'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--deploy-mode`: 参数允许你决定是否在本地（使用`client`）或集群中的某个工作机器上（使用`cluster`选项）启动Spark驱动程序进程。此参数的默认值为`client`。以下是Spark文档的摘录，它更具体地解释了差异（来源：[http://bit.ly/2hTtDVE](http://bit.ly/2hTtDVE)）：'
- en: A common deployment strategy is to submit your application from [a screen session
    on] a gateway machine that is physically co-located with your worker machines
    (e.g. Master node in a standalone EC2 cluster). In this setup, client mode is
    appropriate. In client mode, the driver is launched directly within the spark-submit
    process which acts as a client to the cluster. The input and output of the application
    is attached to the console. Thus, this mode is especially suitable for applications
    that involve the REPL (e.g. Spark shell).
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一种常见的部署策略是从与你的工作机器物理上位于同一位置的门控机器上的[屏幕会话](https://example.org)提交你的应用程序（例如，独立EC2集群中的主节点）。在这种配置中，客户端模式是合适的。在客户端模式下，驱动程序直接在spark-submit过程中启动，该过程作为集群的客户端。应用程序的输入和输出连接到控制台。因此，这种模式特别适合涉及REPL（例如Spark
    shell）的应用程序。
- en: ''
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alternatively, if your application is submitted from a machine far from the
    worker machines (e.g. locally on your laptop), it is common to use cluster mode
    to minimize network latency between the drivers and the executors. Currently,
    standalone mode does not support cluster mode for Python applications.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 或者，如果你的应用程序是从远离工作机器的机器（例如，在你的笔记本电脑上本地）提交的，那么通常使用集群模式以最小化驱动程序和执行器之间的网络延迟。目前，独立模式不支持Python应用程序的集群模式。
- en: '`--name`: Name of your application. Note that if you specified the name of
    your app programmatically when creating `SparkSession` (we will get to that in
    the next section) then the parameter from the command line will be overridden.
    We will explain the precedence of parameters shortly when discussing the `--conf`
    parameter.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--name`: 你的应用程序的名称。请注意，如果你在创建`SparkSession`时以编程方式指定了应用程序的名称（我们将在下一节中介绍），则命令行参数将覆盖该参数。我们将在讨论`--conf`参数时简要解释参数的优先级。'
- en: '`--py-files`: Comma-delimited list of `.py`, `.egg` or `.zip` files to include
    for Python apps. These files will be delivered to each executor for use. Later
    in this chapter we will show you how to package your code into a module.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--py-files`: 要包含的`.py`、`.egg`或`.zip`文件的逗号分隔列表，用于Python应用程序。这些文件将被发送到每个执行器以供使用。在本章的后面部分，我们将向你展示如何将你的代码打包成模块。'
- en: '`--files`: Command gives a comma-delimited list of files that will also be
    delivered to each executor to use.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--files`: 该命令给出一个以逗号分隔的文件列表，这些文件也将被发送到每个执行器以供使用。'
- en: '`--conf`: Parameter to change a configuration of your app dynamically from
    the command line. The syntax is `<Spark property>=<value for the property>`. For
    example, you can pass `--conf spark.local.dir=/home/SparkTemp/` or `--conf spark.app.name=learningPySpark`;
    the latter would be an equivalent of submitting the `--name` property as explained
    previously.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--conf`: 参数允许你从命令行动态更改应用程序的配置。语法是`<Spark属性>=<属性值>`。例如，你可以传递`--conf spark.local.dir=/home/SparkTemp/`或`--conf
    spark.app.name=learningPySpark`；后者相当于之前解释的提交`--name`属性。'
- en: Note
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Spark uses the configuration parameters from three places: the parameters from
    the `SparkConf` you specify when creating `SparkContext` within your app take
    the highest precedence, then any parameter that you pass to the `spark-submit`
    script from the command line, and lastly, any parameter that is specified in the
    `conf/spark-defaults.conf` file.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Spark从三个地方使用配置参数：在创建`SparkContext`时，你在应用程序中指定的`SparkConf`参数具有最高优先级，然后是任何从命令行传递给`spark-submit`脚本的参数，最后是`conf/spark-defaults.conf`文件中指定的任何参数。
- en: '`--properties-file`: File with a configuration. It should have the same set
    of properties as the `conf/spark-defaults.conf` file as it will be read instead
    of it.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--properties-file`：包含配置的文件。它应该具有与`conf/spark-defaults.conf`文件相同的属性集，因为它将被读取而不是它。'
- en: '`--driver-memory`: Parameter that specifies how much memory to allocate for
    the application on the driver. Allowed values have a syntax similar to the 1,000M,
    2G. The default is 1,024M.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--driver-memory`：指定为驱动程序分配多少内存的应用程序参数。允许的值具有类似于1,000M、2G的语法。默认值为1,024M。'
- en: '`--executor-memory`: Parameter that specifies how much memory to allocate for
    the application on each of the executors. The default is 1G.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--executor-memory`：指定为每个执行器分配多少内存的应用程序参数。默认值为1G。'
- en: '`--help`: Shows the help message and exits.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--help`：显示帮助信息并退出。'
- en: '`--verbose`: Prints additional debug information when running your app.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--verbose`：在运行应用程序时打印额外的调试信息。'
- en: '`--version`: Prints the version of Spark.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--version`：打印Spark的版本。'
- en: 'In a Spark standalone with `cluster` deploy mode only, or on a cluster deployed
    over Yarn, you can use the `--driver-cores` that allows specifying the number
    of cores for the driver (default is 1). In a Spark standalone or Mesos with `cluster`
    deploy mode only you also have the opportunity to use either of these:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在Spark独立和`cluster`部署模式中，或在Yarn上部署的集群中，你可以使用`--driver-cores`来指定驱动程序的核心数（默认为1）。在Spark独立或Mesos的`cluster`部署模式中，你还有机会使用以下任何一个：
- en: '`--supervise`: Parameter that, if specified, will restart the driver if it
    is lost or fails. This also can be set in Yarn by setting the `--deploy-mode`
    to `cluster`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--supervise`：如果指定，当驱动程序丢失或失败时，将重新启动驱动程序。这也可以通过将`--deploy-mode`设置为`cluster`在Yarn中设置。'
- en: '`--kill`: Will finish the process given its `submission_id`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--kill`：将根据其`submission_id`结束进程'
- en: '`--status`: If this command is specified, it will request the status of the
    specified app'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--status`：如果指定此命令，它将请求指定应用程序的状态。'
- en: In a Spark standalone and Mesos only (with the `client` deploy mode) you can
    also specify the `--total-executor-cores`, a parameter that will request the number
    of cores specified for all executors (not each). On the other hand, in a Spark
    standalone and YARN, only the `--executor-cores` parameter specifies the number
    of cores per executor (defaults to 1 in YARN mode, or to all available cores on
    the worker in standalone mode).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark独立和Mesos（仅使用`client`部署模式）中，你也可以指定`--total-executor-cores`，这是一个将请求所有执行器（而不是每个执行器）指定的核心数的参数。另一方面，在Spark独立和YARN中，只有`--executor-cores`参数指定了每个执行器的核心数（在YARN模式下默认为1，或在独立模式下为工作节点上的所有可用核心）。
- en: 'In addition, when submitting to a YARN cluster you can specify:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当提交到YARN集群时，你可以指定：
- en: '`--queue`: This parameter specifies a queue on YARN to submit the job to (default
    is `default`)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--queue`：此参数指定一个队列，将作业提交到YARN（默认为`default`）。'
- en: '`--num-executors`: Parameter that specifies how many executor machines to request
    for the job. If dynamic allocation is enabled, the initial number of executors
    will be at least the number specified.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--num-executors`：指定为作业请求多少个执行器机器的参数。如果启用了动态分配，初始执行器数量至少为指定的数量。'
- en: Now that we have discussed all the parameters it is time to put it into practice.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了所有参数，是时候将其付诸实践了。
- en: Deploying the app programmatically
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以编程方式部署应用程序
- en: Unlike the Jupyter notebooks, when you use the `spark-submit` command, you need
    to prepare the `SparkSession` yourself and configure it so your application runs
    properly.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与Jupyter笔记本不同，当你使用`spark-submit`命令时，你需要自己准备`SparkSession`并配置它，以确保应用程序正常运行。
- en: In this section, we will learn how to create and configure the `SparkSession`
    as well as how to use modules external to Spark.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何创建和配置`SparkSession`，以及如何使用Spark外部模块。
- en: Note
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you have not created your free account with either Databricks or Microsoft
    (or any other provider of Spark) do not worry - we will be still using your local
    machine as this is easier to get us started. However, if you decide to take your
    application to the cloud it will literally only require changing the `--master`
    parameter when you submit the job.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有在Databricks或Microsoft（或任何Spark的提供者）上创建你的免费账户，不要担心——我们仍然会使用你的本地机器，因为这更容易让我们开始。然而，如果你决定将你的应用程序迁移到云端，实际上只需要在提交作业时更改`--master`参数。
- en: Configuring your SparkSession
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置你的SparkSession
- en: The main difference between using Jupyter and submitting jobs programmatically
    is the fact that you have to create your Spark context (and Hive, if you plan
    to use HiveQL), whereas when running Spark with Jupyter the contexts are automatically
    started for you.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Jupyter和通过编程方式提交作业之间的主要区别在于，你必须创建你的Spark上下文（如果你计划使用HiveQL，还包括Hive），而当你使用Jupyter运行Spark时，上下文会自动为你启动。
- en: In this section, we will develop a simple app that will use public data from
    Uber with trips made in the NYC area in June 2016; we downloaded the dataset from
    [https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-06.csv](https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-06.csv)
    (beware as it is an almost 3GB file). The original dataset contains 11 million
    trips, but for our example we retrieved only 3.3 million and selected only a subset
    of all available columns.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开发一个简单的应用程序，该应用程序将使用Uber的公共数据，这些数据是在2016年6月的纽约地区完成的行程；我们从[https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-06.csv](https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-06.csv)（注意，它是一个几乎3GB的文件）下载的数据集。原始数据集包含1100万次行程，但为了我们的示例，我们只检索了330万次，并且只选择了所有可用列的一个子集。
- en: Note
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The transformed dataset can be downloaded from [http://www.tomdrabas.com/data/LearningPySpark/uber_data_nyc_2016-06_3m_partitioned.csv.zip](http://www.tomdrabas.com/data/LearningPySpark/uber_data_nyc_2016-06_3m_partitioned.csv.zip).
    Download the file and unzip it to the `Chapter13` folder from GitHub. The file
    might look strange as it is actually a directory containing four files inside
    that, when read by Spark, will form one dataset.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的数据集可以从[http://www.tomdrabas.com/data/LearningPySpark/uber_data_nyc_2016-06_3m_partitioned.csv.zip](http://www.tomdrabas.com/data/LearningPySpark/uber_data_nyc_2016-06_3m_partitioned.csv.zip)下载。下载文件并将其解压到GitHub的`Chapter13`文件夹中。文件可能看起来很奇怪，因为它实际上是一个包含四个文件的目录，当Spark读取时，将形成一个数据集。
- en: So, let's get to it!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧！
- en: Creating SparkSession
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建SparkSession
- en: 'Things with Spark 2.0 have become slightly simpler than with previous versions
    when it comes to creating `SparkContext`. In fact, instead of creating a `SparkContext`
    explicitly, Spark currently uses `SparkSession` to expose higher-level functionality.
    Here''s how you do it:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的版本相比，Spark 2.0在创建`SparkContext`方面变得稍微简单一些。实际上，Spark目前使用`SparkSession`来暴露高级功能，而不是显式创建`SparkContext`。以下是这样做的方法：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding code is all that you need!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码就是你需要的一切！
- en: Tip
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you want to use RDD API you still can. However, you do not need to create
    a `SparkContext` anymore as `SparkSession` starts one under the hood. To get the
    access you can simply call (borrowing from the preceding example): `sc = spark.SparkContext`.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然想使用RDD API，你仍然可以。然而，你不再需要创建一个`SparkContext`，因为`SparkSession`在底层会自动启动一个。为了获取访问权限，你可以简单地调用（借鉴前面的示例）：`sc
    = spark.SparkContext`。
- en: In this example, we first create the `SparkSession` object and call its `.builder`
    internal class. The `.appName(...)` method allows us to give our application a
    name, and the `.getOrCreate()` method either creates or retrieves an already created
    `SparkSession`. It is a good convention to give your application a meaningful
    name as it helps to (1) find your application on a cluster and (2) creates less
    confusion for everyone.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们首先创建`SparkSession`对象并调用其`.builder`内部类。`.appName(...)`方法允许我们给我们的应用程序一个名字，而`.getOrCreate()`方法要么创建一个，要么检索一个已经创建的`SparkSession`。给应用程序一个有意义的名字是一个好习惯，因为它有助于（1）在集群上找到你的应用程序，并且（2）减少每个人的困惑。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Under the hood, the Spark session creates a `SparkContext` object. When you
    call `.stop()` on `SparkSession` it actually terminates the `SparkContext` within.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，Spark会话创建一个`SparkContext`对象。当你对`SparkSession`调用`.stop()`时，它实际上会终止内部的`SparkContext`。
- en: Modularizing code
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码模块化
- en: Building your code in such a way so it can be reused later is always a good
    thing. The same can be done with Spark - you can modularize your methods and then
    reuse them at a later point. It also aids readability of your code and its maintainability.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式构建您的代码以便以后可以重用始终是一件好事。Spark 也可以这样做 - 您可以将方法模块化，然后在以后某个时间点重用它们。这也有助于提高代码的可读性和可维护性。
- en: 'In this example, we will build a module that would do some calculations on
    our dataset: It will compute the *as-the-crow-flies* distance (in miles) between
    the pickup and drop-off locations (using the Haversine formula), and also will
    convert the calculated distance from miles into kilometers.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将构建一个模块，它将对我们的数据集进行一些计算：它将计算从接货点到卸货点的“直线距离”（以英里为单位）（使用 Haversine 公式），并将计算出的距离从英里转换为公里。
- en: Note
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'More on the Haversine formula can be found here: [http://www.movable-type.co.uk/scripts/latlong.html](http://www.movable-type.co.uk/scripts/latlong.html).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Haversine 公式的更多信息可以在这里找到：[http://www.movable-type.co.uk/scripts/latlong.html](http://www.movable-type.co.uk/scripts/latlong.html)。
- en: So, first, we will build a module.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先，我们将构建一个模块。
- en: Structure of the module
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模块结构
- en: We put the code for our extraneous methods inside the `additionalCode` folder.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们额外方法的代码放在了 `additionalCode` 文件夹中。
- en: Tip
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Check out the GitHub repository for this book if you have not done so already
    [https://github.com/drabastomek/learningPySpark/tree/master/Chapter11](https://github.com/drabastomek/learningPySpark/tree/master/Chapter11).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请查看此书的 GitHub 仓库 [https://github.com/drabastomek/learningPySpark/tree/master/Chapter11](https://github.com/drabastomek/learningPySpark/tree/master/Chapter11)。
- en: 'The tree for the folder looks as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹的树状结构如下：
- en: '![Structure of the module](img/B05793_11_01.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![模块结构](img/B05793_11_01.jpg)'
- en: 'As you can see, it has a structure of a somewhat normal Python package: At
    the top we have the `setup.py` file so we can package up our module, and then
    inside we have our code.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，它具有某种正常 Python 包的结构：在最上面我们有 `setup.py` 文件，这样我们就可以打包我们的模块，然后内部包含我们的代码。
- en: 'The `setup.py` file in our case looks as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，`setup.py` 文件如下所示：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will not delve into details here on the structure (which on its own is fairly
    self-explanatory): You can read more about how to define `setup.py` files for
    other projects here [https://pythonhosted.org/an_example_pypi_project/setuptools.html](https://pythonhosted.org/an_example_pypi_project/setuptools.html).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会深入探讨结构（它本身相当直观）：您可以在以下链接中了解更多关于如何为其他项目定义 `setup.py` 文件的信息 [https://pythonhosted.org/an_example_pypi_project/setuptools.html](https://pythonhosted.org/an_example_pypi_project/setuptools.html)。
- en: 'The `__init__.py` file in the utilities folder has the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 工具文件夹中的 `__init__.py` 文件包含以下代码：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It effectively exposes the `geoCalc.py` and `converters` (more on these shortly).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 它有效地暴露了 `geoCalc.py` 和 `converters`（稍后将有更多介绍）。
- en: Calculating the distance between two points
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算两点之间的距离
- en: The first method we mentioned uses the Haversine formula to calculate the direct
    distance between any two points on a map (Cartesian coordinates). The code that
    does this lives in the `geoCalc.py` file of the module.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到的第一个方法使用 Haversine 公式来计算地图上任意两点之间的直接距离（笛卡尔坐标）。执行此操作的代码位于模块的 `geoCalc.py`
    文件中。
- en: The `calculateDistance(...)` is a static method of the `geoCalc` class. It takes
    two geo-points, expressed as either a tuple or a list with two elements (latitude
    and longitude, in that order), and uses the Haversine formula to calculate the
    distance. The Earth's radius necessary to calculate the distance is expressed
    in miles so the distance calculated will also be in miles.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculateDistance(...)` 是 `geoCalc` 类的一个静态方法。它接受两个地理点，这些点以元组或包含两个元素（按顺序为纬度和经度）的列表的形式表示，并使用
    Haversine 公式来计算距离。计算距离所需的地球半径以英里表示，因此计算出的距离也将以英里为单位。'
- en: Converting distance units
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换距离单位
- en: We build the utilities package so it can be more universal. As a part of the
    package we expose methods to convert between various units of measurement.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了工具包，使其更加通用。作为包的一部分，我们公开了用于在各个测量单位之间进行转换的方法。
- en: Note
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: At this time we limit it to the distance only, but the functionality can be
    further extended to other domains such as area, volume, or temperature.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们只限制距离，但功能可以进一步扩展到其他领域，如面积、体积或温度。
- en: 'For ease of use, any class implemented as a `converter` should expose the same
    interface. That is why it is advised that such a class derives from our `BaseConverter`
    class (see `base.py`):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于使用，任何实现为`converter`的类都应该公开相同的接口。这就是为什么建议这样的类从我们的`BaseConverter`类派生（参见`base.py`）：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It is a purely abstract class that cannot be instantiated: Its sole purpose
    is to force the derived classes to implement the `convert(...)` method. See the
    `distance.py` file for details of the implementation. The code should be self-explanatory
    for someone proficient in Python so we will not be going through it step-by-step
    here.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个纯抽象类，不能被实例化：它的唯一目的是强制派生类实现`convert(...)`方法。有关实现细节，请参阅`distance.py`文件。对于熟悉Python的人来说，代码应该是自解释的，所以我们不会一步一步地解释它。
- en: Building an egg
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建一个蛋
- en: 'Now that we have all our code in place we can package it. The documentation
    for PySpark states that you can pass `.py` files (using the `--py-files` switch)
    to the `spark-submit` script separated by commas. However, it is much more convenient
    to package our module into a `.zip` or an `.egg`. This is when the `setup.py`
    file comes handy - all you have to do is to call this inside the `additionalCode`
    folder:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有代码放在一起，我们可以打包它。PySpark的文档指出，你可以使用`--py-files`开关将`.py`文件传递给`spark-submit`脚本，并用逗号分隔。然而，将我们的模块打包成`.zip`或`.egg`会更方便。这时`setup.py`文件就派上用场了——你只需要在`additionalCode`文件夹中调用它：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If all goes well you should see three additional folders: `PySparkUtilities.egg-info`,
    `build`, and `dist` - we are interested in the file that sits in the `dist` folder:
    The `PySparkUtilities-0.1.dev0-py3.5.egg`.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该看到三个额外的文件夹：`PySparkUtilities.egg-info`、`build`和`dist`——我们感兴趣的是位于`dist`文件夹中的文件：`PySparkUtilities-0.1.dev0-py3.5.egg`。
- en: Tip
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: After running the preceding command, you might find that the name of your `.egg`
    file is slightly different as you might have a different Python version. You can
    still use it in your Spark jobs, but you will have to adapt the `spark-submit`
    command to reflect the name of your `.egg` file.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的命令后，你可能发现你的`.egg`文件名略有不同，因为你可能有不同的Python版本。你仍然可以在Spark作业中使用它，但你需要调整`spark-submit`命令以反映你的`.egg`文件名。
- en: User defined functions in Spark
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark中的用户定义函数
- en: 'In order to do operations on `DataFrame`s in PySpark you have two options:
    Use built-in functions to work with data (most of the time it will be sufficient
    to achieve what you need and it is recommended as the code is more performant)
    or create your own user-defined functions.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在PySpark中对`DataFrame`进行操作时，你有两种选择：使用内置函数来处理数据（大多数情况下这足以实现所需的功能，并且推荐这样做，因为代码性能更好）或创建自己的用户定义函数。
- en: 'To define a UDF you have to wrap the Python function within the `.udf(...)`
    method and define its return value type. This is how we do it in our script (check
    the `calculatingGeoDistance.py` file):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义一个用户定义函数（UDF），你必须将Python函数包装在`.udf(...)`方法中，并定义其返回值类型。这就是我们在脚本中这样做的方式（检查`calculatingGeoDistance.py`文件）：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can then use such functions to calculate the distance and convert it to
    miles:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这样的函数来计算距离并将其转换为英里：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using the `.withColumn(...)` method we create additional columns with the values
    of interest to us.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`.withColumn(...)`方法，我们创建额外的列，包含我们感兴趣的价值。
- en: Note
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'A word of caution needs to be stated here. If you use the PySpark built-in
    functions, even though you call them Python objects, underneath that call is translated
    and executed as Scala code. If, however, you write your own methods in Python,
    it is not translated into Scala and, hence, has to be executed on the driver.
    This causes a significant performance hit. Check out this answer from Stack Overflow
    for more details: [http://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python](http://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要提醒一点。如果你使用PySpark内置函数，即使你调用它们为Python对象，底层调用会被转换并执行为Scala代码。然而，如果你在Python中编写自己的方法，它不会被转换为Scala，因此必须在驱动程序上执行。这会导致性能显著下降。查看Stack
    Overflow上的这个答案以获取更多详细信息：[http://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python](http://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python)。
- en: Let's now put all the puzzles together and finally submit our job.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们把所有的拼图放在一起，最终提交我们的工作。
- en: Submitting a job
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提交一个工作
- en: 'In your CLI type the following (we assume you keep the structure of the folders
    unchanged from how it is structured on GitHub):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的CLI中输入以下内容（我们假设你保持文件夹结构与GitHub上的结构不变）：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We owe you some explanation for the `launch_spark_submit.sh` shell script.
    In Bonus [Chapter 1](ch01.html "Chapter 1. Understanding Spark"), *Installing
    Spark*, we configured our Spark instance to run Jupyter (by setting the `PYSPARK_DRIVER_PYTHON`
    system variable to `jupyter`). If you were to simply use `spark-submit` on a machine
    configured in such a way, you would most likely get some variation of the following
    error:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对`launch_spark_submit.sh`shell脚本进行一些解释。在Bonus [第1章](ch01.html "第1章。理解Spark")，*安装Spark*中，我们配置了Spark实例以运行Jupyter（通过设置`PYSPARK_DRIVER_PYTHON`系统变量为`jupyter`）。如果你在这样配置的机器上简单地使用`spark-submit`，你很可能会遇到以下错误的一些变体：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Thus, before running the `spark-submit` command we first have to unset the
    variable and then run the code. This would quickly become extremely tiring so
    we automated it with the `launch_spark_submit.sh` script:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在运行`spark-submit`命令之前，我们首先必须取消设置该变量，然后运行代码。这会迅速变得极其繁琐，所以我们通过`launch_spark_submit.sh`脚本自动化了它：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, this is nothing more than a wrapper around the `spark-submit`
    command.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这不过是`spark-submit`命令的一个包装器。
- en: 'If all goes well, you will see the following *stream of consciousness* appearing
    in your CLI:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你将在CLI中看到以下*意识流*：
- en: '![Submitting a job](img/B05793_11_02.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![提交作业](img/B05793_11_02.jpg)'
- en: 'There''s a host of useful things that you can get from reading the output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中你可以获得许多有用的信息：
- en: 'Current version of Spark: 2.1.0'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前Spark版本：2.1.0
- en: Spark UI (what will be useful to track the progress of your job) is started
    successfully on `http://localhost:4040`
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark UI（用于跟踪作业进度的工具）已成功在`http://localhost:4040`启动
- en: Our `.egg` file was added successfully to the execution
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的成功添加了`.egg`文件到执行
- en: The `uber_data_nyc_2016-06_3m_partitioned.csv` was read successfully
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`uber_data_nyc_2016-06_3m_partitioned.csv`已成功读取'
- en: Each start and stop of jobs and tasks are listed
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个作业和任务的启动和停止都被列出
- en: 'Once the job finishes, you will see something similar to the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 作业完成后，你将看到以下类似的内容：
- en: '![Submitting a job](img/B05793_11_03.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![提交作业](img/B05793_11_03.jpg)'
- en: From the preceding screenshot, we can read that the distances are reported correctly.
    You can also see that the Spark UI process has now been stopped and all the clean
    up jobs have been performed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图，我们可以看到距离被正确报告。你还可以看到Spark UI进程现在已经停止，并且所有清理工作都已执行。
- en: Monitoring execution
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控执行
- en: 'When you use the `spark-submit` command, Spark launches a local server that
    allows you to track the execution of the job. Here''s what the window looks like:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用`spark-submit`命令时，Spark会启动一个本地服务器，允许你跟踪作业的执行情况。以下是窗口的外观：
- en: '![Monitoring execution](img/B05793_11_04.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![监控执行](img/B05793_11_04.jpg)'
- en: At the top you can switch between the **Jobs** or **Stages** view; the **Jobs**
    view allows you to track the distinct jobs that are executed to complete the whole
    script, while the **Stages** view allows you to track all the stages that are
    executed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，你可以切换到**作业**或**阶段**视图；**作业**视图允许你跟踪执行整个脚本的独立作业，而**阶段**视图允许你跟踪所有执行的阶段。
- en: 'You can also peak inside each stage execution profile and track each task execution
    by clicking on the link of the stage. In the following screenshot, you can see
    the execution profile for Stage 3 with four tasks running:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过点击阶段的链接来查看每个阶段的执行配置文件，并跟踪每个任务的执行。在以下截图中，你可以看到Stage 3的执行配置文件，其中运行了四个任务：
- en: '![Monitoring execution](img/B05793_11_05.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![监控执行](img/B05793_11_05.jpg)'
- en: Tip
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: In a cluster setup instead of **driver/localhost** you would see the driver
    number and host's IP address.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群设置中，你将看到**driver/localhost**而不是**驱动器/本地主机**，而是驱动器编号和主机的IP地址。
- en: 'Inside a job or a stage, you can click on the DAG Visualization to see how
    your job or stage gets executed (the following chart on the left shows the **Job**
    view, while the one on the right shows the **Stage** view):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个作业或阶段内部，你可以点击DAG可视化来查看你的作业或阶段是如何执行的（左边的以下图表显示了**作业**视图，而右边的显示了**阶段**视图）：
- en: '![Monitoring execution](img/B05793_11_06.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![监控执行](img/B05793_11_06.jpg)'
- en: Databricks Jobs
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks作业
- en: 'If you are using the Databricks product, an easy way to go from development
    from your Databricks notebooks to production is to use the Databricks Jobs feature.
    It will allow you to:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Databricks产品，从Databricks笔记本的开发到生产的一个简单方法就是使用Databricks作业功能。它将允许你：
- en: Schedule your Databricks notebook to run on an existing or new cluster
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安排Databricks笔记本在现有或新集群上运行
- en: Schedule at your desired frequency (from minutes to months)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按您希望的频率（从分钟到月份）安排
- en: Schedule time out and retries for your job
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的作业安排超时和重试
- en: Be alerted when the job starts, completes, and/or errors out
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当作业开始、完成或出错时收到警报
- en: View historical job runs as well as review the history of the individual notebook
    job runs
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看历史作业运行以及审查单个笔记本作业运行的记录
- en: This capability greatly simplifies the scheduling and production workflow of
    your job submissions. Note that you will need to upgrade your Databricks subscription
    (from Community edition) to use this feature.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这种功能极大地简化了您作业提交的调度和生产工作流程。请注意，您需要将您的 Databricks 订阅（从社区版）升级才能使用此功能。
- en: 'To use this feature, go to the Databricks **Jobs** menu and click on **Create
    Job**. From here, fill out the job name and then choose the notebook that you
    want to turn into a job, as shown in the following screenshot:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此功能，请转到 Databricks **作业**菜单并点击**创建作业**。从这里，填写作业名称，然后选择您想要转换为作业的笔记本，如图所示：
- en: '![Databricks Jobs](img/B05793_11_07.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![Databricks 作业](img/B05793_11_07.jpg)'
- en: 'Once you have chosen your notebook, you can also choose whether to use an existing
    cluster that is running or have the job scheduler launch a **New Cluster** specifically
    for this job, as shown in the following screenshot:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您选择了笔记本，您还可以选择是否使用正在运行的现有集群，或者让作业调度器为该作业启动一个**新集群**，如图所示：
- en: '![Databricks Jobs](img/B05793_11_08.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![Databricks 作业](img/B05793_11_08.jpg)'
- en: 'Once you have chosen your notebook and cluster; you can set the schedule, alerts,
    timeout, and retries. Once you have completed setting up your job, it should look
    something similar to the **Population vs. Price Linear Regression Job**, as noted
    in the following screenshot:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您选择了笔记本和集群；您可以设置计划、警报、超时和重试。一旦您完成设置作业，它应该看起来类似于以下截图中的**人口与价格线性回归作业**：
- en: '![Databricks Jobs](img/B05793_11_09.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![Databricks 作业](img/B05793_11_09.jpg)'
- en: You can test the job by clicking on the **Run Now** link under **Active runs**
    to test your job.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过点击**活动运行**下方的**立即运行**链接来测试作业。
- en: 'As noted in the **Meetup Streaming RSVPs** Job, you can view the history of
    your completed runs; as shown in the screenshot, for this notebook there are **50**
    completed job runs:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如**Meetup Streaming RSVPs 作业**中所述，您可以查看您已完成运行的记录；如图所示，对于这个笔记本，有**50**个完成的作业运行：
- en: '![Databricks Jobs](img/B05793_11_10.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![Databricks 作业](img/B05793_11_10.jpg)'
- en: 'By clicking on the job run (in this case, **Run 50**), you can see the results
    of that job run. Not only can you view the start time, duration, and status, but
    also the results for that specific job:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击作业运行（在这种情况下，**运行 50**），您可以查看该作业运行的结果。您不仅可以查看开始时间、持续时间和服务状态，还可以查看该特定作业的结果：
- en: '![Databricks Jobs](img/B05793_11_11.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![Databricks 作业](img/B05793_11_11.jpg)'
- en: Note
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: '**REST Job Server**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**REST 作业服务器**'
- en: A popular way to run jobs is to also use REST APIs. If you are using Databricks,
    you can run your jobs using the Databricks REST APIs. If you prefer to manage
    your own job server, a popular open source REST Job Server is `spark-jobserver`
    - a RESTful interface for submitting and managing Apache Spark jobs, jars, and
    job contexts. The project recently (at the time of writing) was updated so it
    can handle PySpark jobs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 运行作业的一种流行方式是使用 REST API。如果您使用 Databricks，您可以使用 Databricks REST API 运行作业。如果您更喜欢管理自己的作业服务器，一个流行的开源
    REST 作业服务器是 `spark-jobserver` - 一个用于提交和管理 Apache Spark 作业、jar 和作业上下文的 RESTful
    接口。该项目最近（在撰写本文时）进行了更新，以便它可以处理 PySpark 作业。
- en: For more information, please refer to [https://github.com/spark-jobserver/spark-jobserver](https://github.com/spark-jobserver/spark-jobserver).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅 [https://github.com/spark-jobserver/spark-jobserver](https://github.com/spark-jobserver/spark-jobserver)。
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we walked you through the steps on how to submit applications
    written in Python to Spark from the command line. The selection of the `spark-submit`
    parameters has been discussed. We also showed you how you can package your Python
    code and submit it alongside your PySpark script. Furthermore, we showed you how
    you can track the execution of your job.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了如何从命令行将用 Python 编写的应用程序提交到 Spark 的步骤。我们讨论了 `spark-submit` 参数的选择。我们还向您展示了如何打包您的
    Python 代码，并将其与 PySpark 脚本一起提交。此外，我们还向您展示了如何跟踪作业的执行。
- en: In addition, we also provided a quick overview of how to run Databricks notebooks
    using the Databricks Jobs feature. This feature simplifies the transition from
    development to production, allowing you to take your notebook and execute it as
    an end-to-end workflow.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还提供了一个关于如何使用Databricks Jobs功能运行Databricks笔记本的快速概述。此功能简化了从开发到生产的过渡，允许您将笔记本作为一个端到端工作流程执行。
- en: This brings us to the end of this book. We hope you enjoyed the journey, and
    that the material contained herein will help you start working with Spark using
    Python. Good luck!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的内容到此结束。我们希望您享受了这次旅程，并且书中包含的材料能帮助您开始使用Python与Spark进行工作。祝您好运！
