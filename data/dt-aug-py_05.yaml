- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Text Augmentation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本增强
- en: Text augmentation is a technique that is used in **Natural Language Processing**
    (**NLP**) to generate additional data by modifying or creating new text from existing
    text data. Text augmentation involves techniques such as character swapping, noise
    injection, synonym replacement, word deletion, word insertion, and word swapping.
    Image and text augmentation have the same goal. They strive to increase the size
    of the training dataset and improve AI prediction accuracy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 文本增强是一种在**自然语言处理**（**NLP**）中使用的技术，通过修改或从现有文本数据创建新文本来生成额外数据。文本增强涉及的技术包括字符交换、噪声注入、同义词替换、单词删除、单词插入和单词交换。图像和文本增强的目标相同，它们都旨在增加训练数据集的大小并提高AI预测的准确性。
- en: Text augmentation is relatively more challenging to evaluate because it is not
    as intuitive as image augmentation. The intent of an image augmentation technique
    is clear, such as flipping a photo, but a character-swapping technique will be
    disorienting to the reader. Therefore, readers might perceive the benefits as
    subjective.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 文本增强相对较难评估，因为它不像图像增强那样直观。图像增强技术的意图很明确，比如翻转照片，但字符交换技术可能会让读者感到困惑。因此，读者可能会将其益处视为主观的。
- en: The effectiveness of text augmentation depends on the quality of the generated
    data and the specific NLP task being performed. It can be challenging to determine
    the appropriate *safe* level of text augmentation that is required for a given
    dataset, and it often requires experimentation and testing to achieve the desired
    results.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 文本增强的效果取决于生成数据的质量以及正在执行的具体NLP任务。确定给定数据集所需的适当*安全*级别的文本增强可能会非常具有挑战性，这通常需要实验和测试才能获得预期的结果。
- en: Customer feedback or social media chatter is fair game for text augmentation
    because the writing is messy and, predominantly, contains grammatical errors.
    Conversely, legal documents or written medical communications, such as doctor’s
    prescriptions or reports, are off-limits because the message is precise. In other
    words, error injections, synonyms, or even AI-generated text might change the
    legal or medical meaning beyond the *safe* level.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 客户反馈或社交媒体上的聊天内容是文本增强的合理素材，因为这些内容通常杂乱无章，并且大多数包含语法错误。相反，法律文件或书面医疗通讯，如医生处方或报告，则不适合进行文本增强，因为这些内容要求精确表达。换句话说，错误注入、同义词替换甚至AI生成的文本可能会改变法律或医学的含义，超出*安全*水平。
- en: The biases in text augmentation are equally difficult to discern. For example,
    adding noise by purposely misspelling words using the Keyboard augmentation method
    might introduce bias against real-world tweets, which typically contain misspelled
    words. There are no generalized rules to follow, and the answer only becomes evident
    after thoroughly studying the data and reviewing the AI forecasting objective.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 文本增强中的偏差同样很难察觉。例如，通过故意拼写错误单词来使用键盘增强方法添加噪声，可能会引入对现实世界推文的偏见，而这些推文通常包含拼写错误的单词。没有普遍适用的规则可循，答案只有在深入研究数据并审查AI预测目标后才能显现出来。
- en: Fun fact
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: As generative AI becomes more widely available, you can use **OpenAI’s GPT-3**,
    **Google Bard**, or **Facebook’s Roberta** system to generate original articles
    for text augmentation. For example, you can ask generative AI to create positive
    or negative reviews about a company product, then use the AI-written articles
    to train predictive AI on sentiment analysis.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成型AI的普及，你可以使用**OpenAI的GPT-3**、**Google Bard**或**Facebook的Roberta**系统生成原创文章用于文本增强。例如，你可以让生成型AI创建关于公司产品的正面或负面评论，然后使用AI编写的文章来训练情感分析预测AI。
- en: 'In [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101), you will learn about text
    augmentation and how to code the methods in Python notebooks. In particular, we
    will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第5章*](B17990_05.xhtml#_idTextAnchor101)中，你将学习文本增强，并如何在Python笔记本中编写相关方法。特别是，我们将涵盖以下主题：
- en: Character augmenting
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符增强
- en: Word augmenting
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词增强
- en: Sentence and flow augmenting
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子和流程增强
- en: Text augmentation libraries
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本增强库
- en: Real-world text datasets
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实世界文本数据集
- en: Reinforcing learning through Python Notebook
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Python笔记本强化学习
- en: Let’s get started with the simplest topic, character augmentation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最简单的主题开始，字符增强。
- en: Character augmenting
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符增强
- en: Character augmentation substitutes or injects characters into the text. In other
    words, it creates typing errors. Therefore, the method seems counterintuitive.
    Still, just like noise injection in image augmentation, scholarly published papers
    illustrate the benefit of character augmentation in improving AI forecasting accuracy,
    such as *Effective Character-Augmented Word Embedding for Machine Reading Comprehension*
    by *Zhuosheng Zhang, Yafang Huang, Pengfei Zhu, and Hai Zhao*, from the 2018 *CCF
    International Conference on Natural* *Language Processing*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 字符增强会替换或注入字符到文本中。换句话说，它会制造打字错误。因此，这种方法似乎是违反直觉的。然而，就像图像增强中的噪声注入一样，学术论文展示了字符增强在提高AI预测准确性方面的好处，例如*Zhuosheng
    Zhang, Yafang Huang, Pengfei Zhu 和 Hai Zhao*于2018年在《*CCF国际自然语言处理会议*》上发表的《*有效的字符增强词嵌入用于机器阅读理解*》。
- en: 'The three standard methods for character augmentation are listed as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 三种标准的字符增强方法如下所示：
- en: The **Optical Character Recognition (OCR) augmenting** function substitutes
    frequent errors in OCR by converting images into text, such as the letter *o*
    into the number *0* or the capital letter *I* into the number *1*.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**光学字符识别（OCR）增强**功能通过将图像转换为文本来替代OCR中常见的错误，例如将字母*o*转换为数字*0*，或将大写字母*I*转换为数字*1*。'
- en: The **Keyboard augmenting** method replaces a character with other characters
    that are adjacent to it. For example, a typical typing error for character *b*
    is hitting key *v* or key *n* instead.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键盘增强**方法将字符替换为其相邻的其他字符。例如，字符*b*的常见打字错误是按下*v*键或*n*键。'
- en: The **Random character** function randomly swaps, inserts, or deletes characters
    within the piece of text.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机字符**功能会在文本中随机交换、插入或删除字符。'
- en: Fun fact
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: 'Computer encoding text was very different from 1963 to 1970; for example, a
    computer would encode the letter *A* as an integer 64 or Hexidecimal 41\. This
    originated from the **American National Standards Institute** (**ANSI**) in 1964,
    and the **International Organization for Standardization** (**ISO**) adopted the
    standard around 1970\. In 1980, the **Unification Code** (**Unicode**) subsumed
    the ISO standard for all international languages. However, if you come across
    computer text from around 1964, it could be encoded in **Extended Binary Coded
    Decimal Interchange Code** (**EBCDIC**), which encodes the letter *A* as 193 or
    Hexidecimal C1\. As a programmer, you might have to answer this question: *does
    your website or mobile app* *support Unicode?*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机编码文本在1963年至1970年期间有很大不同；例如，计算机会将字母*A*编码为整数64或十六进制41。这一做法起源于1964年的**美国国家标准学会**（**ANSI**），并在1970年左右被**国际标准化组织**（**ISO**）采纳。1980年，**统一码**（**Unicode**）取代了ISO标准，适用于所有国际语言。然而，如果你遇到来自1964年左右的计算机文本，它可能采用**扩展二进制编码十进制交换码**（**EBCDIC**），它将字母*A*编码为193或十六进制C1。作为程序员，你可能需要回答这样一个问题：*你的网站或移动应用是否支持Unicode？*
- en: After character augmenting, the next category is word augmenting.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在字符增强之后，接下来的类别是单词增强。
- en: Word augmenting
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词增强
- en: 'Word augmentations carry the same bias and *safe* level warning as character
    augmentations. Over half of these augmentation methods inject errors into the
    text, but other functions generate new text using synonyms or a pretrained AI
    model. The standard word augmentation functions are listed as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 单词增强与字符增强具有相同的偏差和*安全*级别警告。超过一半的这些增强方法会向文本中注入错误，但其他功能则使用同义词或预训练的 AI 模型生成新文本。标准的单词增强功能如下所示：
- en: The **Misspell augmentation** function uses a predefined dictionary to simulate
    spelling mistakes. It is based on the scholarly paper *Text Data Augmentation
    Made Simple By Leveraging NLP Cloud APIs* by **Claude Coulombe**, which was published
    in 2018.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拼写错误增强**功能使用预定义的字典模拟拼写错误。它基于**Claude Coulombe**在2018年发布的学术论文《*通过利用NLP云API简化文本数据增强*》。'
- en: The **Split augmentation** function splits words into two tokens randomly.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分割增强**功能会将单词随机分成两个词元。'
- en: 'The **Random word** augmentation method applies random behavior to the text
    with four parameters: **substitute**, **swap**, **delete**, and **crop**. It is
    based on two scholarly papers: *Synthetic and Natural Noise Both Break Neural
    Machine Translation* by **Yonatan Belinkov and Yonatan Bisk**, published in 2018,
    and *Data Augmentation via Dependency Tree Morphing for Low-Resource Languages*
    by **Gozde Gul Sahin and** **Mark Steedman**.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机词**增强方法通过四个参数对文本应用随机行为：**替换**、**交换**、**删除**和**裁剪**。它基于两篇学术论文：**Yonatan
    Belinkov和Yonatan Bisk**于2018年发表的《*Synthetic and Natural Noise Both Break Neural
    Machine Translation*》以及**Gozde Gul Sahin**和**Mark Steedman**的《*Data Augmentation
    via Dependency Tree Morphing for Low-Resource Languages*》。'
- en: The **Synonym augmentation** function substitutes words with synonyms from a
    predefined database. The first option is to use **WordNet**. WordNet is an extensive
    lexical database of English from *Princeton University*. The database groups nouns,
    verbs, adjectives, and adverbs into sets of cognitive synonyms. The second option
    is to use a **Paraphrase Database** (**PPDB**). A PPDB is an automatically extracted
    database containing millions of paraphrases in 16 languages. A PPDB aims to improve
    language processing by making systems more robust to language variability and
    unseen words. The entire PPDB resource is freely available under the United States
    **Creative Commons Attribution** **3.0** license.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同义词增强**功能用来自预定义数据库的同义词替换单词。第一个选项是使用**WordNet**。WordNet是普林斯顿大学提供的一个广泛的英语词汇数据库。该数据库将名词、动词、形容词和副词分组为认知同义词集。第二个选项是使用**同义词数据库**（**PPDB**）。PPDB是一个自动提取的数据库，包含16种语言的数百万个同义词。PPDB旨在通过增强系统对语言变异性和未见过单词的鲁棒性来改善语言处理。整个PPDB资源可在美国**创意共享署名****3.0**许可证下免费获取。'
- en: The **Antonym augmentation** function replaces words with antonyms. It is based
    on the scholarly paper, *Adversarial Over-Sensitivity and Over-Stability Strategies
    for Dialogue Models*, by **Tong Niu and Mohit Bansal**, which was published in
    2018.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反义词增强**功能用反义词替换单词。它基于**Tong Niu**和**Mohit Bansal**于2018年发表的学术论文《*Adversarial
    Over-Sensitivity and Over-Stability Strategies for Dialogue Models*》。'
- en: The **Reserved Word augmentation** method swaps target words where you define
    a word list. It is the same as synonyms, except the terms are created manually.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保留字增强**方法在你定义的单词列表中交换目标单词。它与同义词相似，只是这些术语是手动创建的。'
- en: Fun challenge
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战
- en: 'Here is a thought experiment: can you think of a new character or word augmentation
    technique? A hint is to think about how a dyslexic person reads.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个思想实验：你能想出一种新的字符或单词增强技术吗？一个提示是考虑一下阅读障碍者是如何阅读的。
- en: Next, we will look at sentence augmentation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看句子增强。
- en: Sentence augmentation
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子增强
- en: Sentence augmenting uses generative AI to create new texts. Examples of AI models
    are BERT, Roberta, GPT-2, and others.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 句子增强使用生成式AI来创建新文本。AI模型的例子有BERT、Roberta、GPT-2等。
- en: 'The three sentence augmentation methods are listed as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 三种句子增强方法如下所示：
- en: '**Contextual Word Embeddings** uses GPT-2, Distilled-GPT-2, and XLNet.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文词嵌入**使用GPT-2、Distilled-GPT-2和XLNet。'
- en: '**Abstractive Summarization** uses Facebook Robertaand T5-Large.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽象总结**使用Facebook Roberta和T5-Large。'
- en: '**Top-n Similar Word** uses LAMBADA.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Top-n 相似词**使用LAMBADA。'
- en: Before Pluto explains the code in the Python Notebook, let’s review the text
    augmentation libraries.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pluto解释Python笔记本中的代码之前，我们先来回顾一下文本增强库。
- en: Text augmentation libraries
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本增强库
- en: There are many more Python open source image augmentation libraries than text
    augmentation libraries. Some libraries are more adaptable to a particular category
    than others, but in general, it is a good idea to pick one or two and become proficient
    in them.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 比起文本增强库，Python中开源的图像增强库更多。有些库对特定类别的适应性比其他库更强，但通常来说，选择一到两个库并精通它们是一个不错的选择。
- en: 'The well-known libraries are **Nlpaug**, **Natural Language Toolkit** (**NLTK**),
    **Generate Similar** (**Gensim**), **TextBlob**, **TextAugment**, and **AugLy**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 知名的库有**Nlpaug**、**自然语言工具包**（**NLTK**）、**生成相似**（**Gensim**）、**TextBlob**、**TextAugment**和**AugLy**：
- en: '**Nlpaug** is a library used for textual augmentation for DL. The goal is to
    improve DL model performance by generating textual data. The GitHub link is [https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nlpaug**是一个用于深度学习文本增强的库。其目标是通过生成文本数据来提高深度学习模型的表现。GitHub链接是[https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug)。'
- en: '**NLTK** is a platform used for building Python programs to work with human
    language data. It provides interfaces to over 50 corpora and lexical resources,
    such as WordNet. NLTK contains text-processing libraries for classification, tokenization,
    stemming, tagging, parsing, and semantic reasoning. The GitHub link is [https://github.com/nltk/nltk](https://github.com/nltk/nltk).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NLTK** 是一个用于构建 Python 程序以处理人类语言数据的平台。它提供了对超过 50 种语料库和词汇资源的接口，例如 WordNet。NLTK
    包含用于分类、分词、词干提取、标注、解析和语义推理的文本处理库。GitHub 链接是 [https://github.com/nltk/nltk](https://github.com/nltk/nltk)。'
- en: '**Gensim** is a popular open source NLP library used for unsupervised topic
    modeling. It uses academic models and modern statistical machine learning to perform
    word vectors, corpora, topic identification, document comparison, and analyzing
    plain-text documents. The GitHub link is [https://github.com/RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gensim** 是一个流行的开源 NLP 库，用于无监督主题建模。它利用学术模型和现代统计机器学习来执行词向量、语料库、主题识别、文档比较和分析纯文本文档。GitHub
    链接是 [https://github.com/RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim)。'
- en: '**TextBlob** is a library that is used for processing textual data. It provides
    a simple API for diving into typical NLP tasks such as part-of-speech tagging,
    noun phrase extraction, sentiment analysis, classification, and translation. The
    GitHub link is [https://github.com/sloria/TextBlob](https://github.com/sloria/TextBlob).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TextBlob** 是一个用于处理文本数据的库。它提供了一个简单的 API，方便处理典型的 NLP 任务，如词性标注、名词短语提取、情感分析、分类和翻译。GitHub
    链接是 [https://github.com/sloria/TextBlob](https://github.com/sloria/TextBlob)。'
- en: '**TextAugment** is a library that is used for augmenting text in NLP applications.
    It uses and combines the NLTK, Gensim, and TextBlob libraries. The GitHub link
    is [https://github.com/dsfsi/textaugment](https://github.com/dsfsi/textaugment).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TextAugment** 是一个用于增强 NLP 应用中文本的库。它使用并结合了 NLTK、Gensim 和 TextBlob 库。GitHub
    链接是 [https://github.com/dsfsi/textaugment](https://github.com/dsfsi/textaugment)。'
- en: '**AugLy** is a data augmentation library from Facebook that supports audio,
    image, text, and video modules and over 100 augmentations. The augmentation of
    each modality is categorized into sub-libraries. The GitHub link is [https://github.com/facebookresearch/AugLy](https://github.com/facebookresearch/AugLy'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AugLy** 是 Facebook 提供的一个数据增强库，支持音频、图像、文本和视频模块，并提供超过 100 种增强方法。每种模态的增强被分类到不同的子库中。GitHub
    链接是 [https://github.com/facebookresearch/AugLy](https://github.com/facebookresearch/AugLy)'
- en: )
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Similarly to image augmentation wrapper functions, Pluto will write wrapper
    functions that use the library under the hood. You can pick more than one library
    for a project, but Pluto will use the **Nlpaug** library to power the wrapper
    functions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像增强包装函数类似，Pluto 将编写包装函数，通过库实现底层功能。你可以为一个项目选择多个库，但 Pluto 将使用 **Nlpaug** 库来支持这些包装函数。
- en: Let’s start by downloading the real-world text datasets from the *Kaggle* website.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 *Kaggle* 网站下载真实世界的文本数据集。
- en: Real-world text datasets
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真实世界的文本数据集
- en: The *Kaggle* website is an online community platform for data scientists and
    machine learning enthusiasts. The Kaggle website has thousands of real-world datasets;
    Pluto found a little over 2,900 **NLP** datasets and has selected two NLP datasets
    for this chapter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kaggle* 网站是一个面向数据科学家和机器学习爱好者的在线社区平台。Kaggle 网站有成千上万的真实世界数据集；Pluto 在其中找到了约 2,900
    个 **NLP** 数据集，并选择了两个 NLP 数据集用于本章内容。'
- en: In [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038), Pluto uses the **Netflix**
    and **Amazon** datasets as examples with which to understand biases. Pluto keeps
    the **Netflix** NLP dataset because the movie reviews are curated . There are
    a few syntactical errors, but overall, the input texts are of high quality.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第二章*](B17990_02.xhtml#_idTextAnchor038) 中，Pluto 以 **Netflix** 和 **Amazon**
    数据集为例，帮助理解偏差问题。Pluto 保留了 **Netflix** 的 NLP 数据集，因为电影评论是经过筛选的。虽然存在一些语法错误，但整体上输入文本质量较高。
- en: 'The second **NLP** dataset is **Twitter Sentiment Analysis** (**TSA**). The
    29,530 real-world tweets contain many grammatical errors and misspelled words.
    The challenge is to classify the tweets into two categories: (1) normal or (2)
    racist and sexist.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个 **NLP** 数据集是 **Twitter 情感分析** (**TSA**)。29,530 条真实世界的推文包含许多语法错误和拼写错误。挑战在于将推文分类为两类：
    (1) 正常，或 (2) 种族歧视和性别歧视。
- en: 'The dataset was published in 2021 by **Mayur Dalvi**, and the license is **CC0:
    Public** **Domain**, [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集由 **Mayur Dalvi** 于 2021 年发布，许可证为 **CC0: 公共领域**，[https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)。'
- en: 'After selecting the two NLP datasets, you can use the same four steps to begin
    the process of practical learning through a Python Notebook. If you need clarification,
    review *Chapters 2* and *3*. The steps are listed as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 选择两个 NLP 数据集后，您可以使用相同的四个步骤，通过 Python Notebook 开始实际学习的过程。如果需要澄清，可以参考*第 2 章*和*第
    3 章*。步骤如下：
- en: Retrieve Python Notebook and Pluto.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取 Python Notebook 和 Pluto。
- en: Download real-world data.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载真实世界数据。
- en: Import into pandas.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入到 pandas。
- en: View data.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看数据。
- en: Let’s start with Pluto.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 Pluto 开始。
- en: The Python Notebook and Pluto
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python Notebook 和 Pluto
- en: Start by loading the `data_augmentation_with_python_chapter_5.ipynb` file into
    **Google Colab** or your chosen Jupyter notebook or JupyterLab environment. From
    this point onward, the code snippets are from the Python Notebook, which contains
    the complete code.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从将 `data_augmentation_with_python_chapter_5.ipynb` 文件加载到 **Google Colab** 或您选择的
    Jupyter notebook 或 JupyterLab 环境开始。从此开始，代码片段来自 Python Notebook，其中包含完整代码。
- en: 'The next step is to clone the repository. Pluto will reuse the code from [*Chapter
    2*](B17990_02.xhtml#_idTextAnchor038) because it has the downloading Kaggle data
    methods and not the image augmentation functions. The `!git` and `%run` statements
    are used to start up Pluto. The command is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是克隆代码库。Pluto 将重用来自[*第 2 章*](B17990_02.xhtml#_idTextAnchor038)的代码，因为它包含下载
    Kaggle 数据的方法，而不包含图像增强功能。`!git` 和 `%run` 语句用于启动 Pluto。命令如下：
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We need one more check to ensure Pluto has been loaded satisfactorily. The
    following command asks Pluto to say his status:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要进行最后一次检查，以确保 Pluto 已经成功加载。以下命令让 Pluto 报告他的状态：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output should be as follows or similar, depending on your system:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示，或根据您的系统可能有所不同：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, Pluto reported that he is from [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038),
    which is also known as **version 2.0**. This is what we wanted because we don’t
    need any image augmentation functions from *Chapters 3* and *4*. The next step
    is to download the real-world **Netflix** and **Twitter** NLP datasets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Pluto 报告说他来自[*第 2 章*](B17990_02.xhtml#_idTextAnchor038)，也就是**版本 2.0**。这正是我们所需要的，因为我们不需要来自*第
    3 章*和*第 4 章*的任何图像增强功能。下一步是下载真实世界的 **Netflix** 和 **Twitter** NLP 数据集。
- en: Real-world NLP datasets
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实世界的 NLP 数据集
- en: 'There has yet to be any new code written for this chapter. Pluto reuses the
    `fetch_kaggle_dataset()` method to download the **Netflix** dataset, as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 目前尚未为本章编写新代码。Pluto 重用 `fetch_kaggle_dataset()` 方法下载 **Netflix** 数据集，具体如下：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `dataset-netflix-shows.zip` file is 1.34 MB, and the function automatically
    unzips in the **kaggle** directory.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset-netflix-shows.zip` 文件大小为 1.34 MB，功能会自动解压到 **kaggle** 目录中。'
- en: 'The method for fetching the Twitter dataset is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 Twitter 数据集的方法如下：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `twitter-sentiments-analysis-nlp.zip` file is 1.23 MB, and the function
    automatically unzips in the **kaggle** directory.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`twitter-sentiments-analysis-nlp.zip` 文件大小为 1.23 MB，功能会自动解压到 **kaggle** 目录中。'
- en: Fun challenge
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战
- en: 'The challenge is to search and download two additional real-world NLP datasets
    from the Kaggle website. Hint: use the `pluto.fetch_kaggle_dataset()` method.
    Pluto is an imaginary digital Siberian Husky. Therefore, he will happily fetch
    data until your disk space is full.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战是从 Kaggle 网站搜索并下载两个额外的真实世界 NLP 数据集。提示：使用 `pluto.fetch_kaggle_dataset()` 方法。Pluto
    是一只虚拟的数字西伯利亚哈士奇犬。因此，他会一直开心地获取数据，直到你的磁盘空间用尽。
- en: The next step is to load the data into pandas.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据加载到 pandas 中。
- en: Pandas
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pandas
- en: '`fetch_df()` method. Note that `df` is a typical shorthand for the pandas **DataFrame**
    class.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`fetch_df()` 方法。请注意，`df` 是 pandas **DataFrame** 类的典型缩写。'
- en: 'For the **Netflix** data, Pluto uses the following two commands for importing
    to pandas and printing out the data batch:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **Netflix** 数据，Pluto 使用以下两个命令将数据导入 pandas 并打印出数据批次：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.1 – Netflix movie descriptions](img/B17990_05_01.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – Netflix 电影描述](img/B17990_05_01.jpg)'
- en: Figure 5.1 – Netflix movie descriptions
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – Netflix 电影描述
- en: Fun fact
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: The `fetch_df()` method randomly selects several records to display in the data
    batch. The number of records, or batch size, is the `bsize` parameter. The default
    is 10 records.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`fetch_df()` 方法随机选择若干记录显示在数据批次中。记录的数量或批次大小是 `bsize` 参数，默认是 10 条记录。'
- en: The **Netflix** movie-reviewed data is curated; therefore, it is clean. Pluto
    doesn’t have to scrub the data. However, the **Twitter** data is another story.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**Netflix** 电影评论数据经过整理，因此是干净的。Pluto 不需要清洗这些数据。然而，**Twitter** 数据就另当别论了。'
- en: 'The commands for cleaning, importing, and batch-displaying the **Twitter**
    data to pandas are as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗、导入并批量显示 **Twitter** 数据到 pandas 的命令如下：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.2 – Twitter tweets](img/B17990_05_02.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – Twitter 推文](img/B17990_05_02.jpg)'
- en: Figure 5.2 – Twitter tweets
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – Twitter 推文
- en: Since the real-world tweets from **Twitter** have been written by the public,
    they contain misspelled words, bad words, and all sorts of shenanigans.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于来自 **Twitter** 的现实世界推文是由公众编写的，它们包含拼写错误、脏话以及各种胡闹。
- en: The goal is to predict regular versus racist or sexist tweets. Pluto focuses
    on learning text argumentation; therefore, he prefers to have tweets with printable
    characters, no HTML tags, and no words of profanity.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是预测普通推文与种族主义或性别歧视推文。Pluto 重点学习文本论证，因此他希望推文中包含可打印字符，没有 HTML 标签，也没有脏话。
- en: 'Pluto writes two simple helper methods to clean the text and remove the words
    of profanity. The `_clean_text()` function uses the `regex` library, and the one
    line of code is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 编写了两个简单的辅助方法来清洗文本并去除脏话。`_clean_text()` 函数使用了 `regex` 库，代码如下：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `_clean_bad_word()` helper function uses the `filter-profanity` library,
    and the one line of code is as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`_clean_bad_word()` 辅助函数使用了 `filter-profanity` 库，代码如下：'
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `clean_text()` method uses the two helper functions with pandas’ powerful
    `apply` function. Using pandas’ built-in functions, Pluto writes the `clean_text()`
    function with two code lines instead of a dozen lines using standard `if-else`
    and `for`-loop construct. The code is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`clean_text()` 方法利用 pandas 强大的 `apply` 函数结合这两个辅助函数。利用 pandas 内建的函数，Pluto 用两行代码编写了
    `clean_text()` 函数，而不是用十几行标准的 `if-else` 和 `for` 循环构造。代码如下：'
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The commands for the clean tweets and showing the data batch are as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 清洁推文和显示数据批次的命令如下：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.3 – Clean Twitter tweets](img/B17990_05_03.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 清洁后的 Twitter 推文](img/B17990_05_03.jpg)'
- en: Figure 5.3 – Clean Twitter tweets
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 清洁后的 Twitter 推文
- en: Fun fact
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 趣味事实
- en: Who would have known that a dog and a panda could work together well? The next
    *Kung Fu Panda* movie is about **Po** and **Pluto** teaming up to defend and augment
    the city wall against the storm of the century, which has been caused by global
    warming.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 谁能想到狗和熊猫能合作得这么好？下一部 *功夫熊猫* 电影讲述的是 **Po** 和 **Pluto** 联手抵御世纪风暴的故事，这场风暴是由全球变暖引起的，它正摧毁着城市墙。
- en: Let’s use pandas and some other libraries to visualize the NLP dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 pandas 和其他一些库来可视化 NLP 数据集。
- en: Visualizing NLP data
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化 NLP 数据
- en: '[*Chapter 2*](B17990_02.xhtml#_idTextAnchor038) uses the `draw_word_count()`
    method to display the average word per record and the shortest and longest movie
    reviews. The right-hand side of the graph shows the histogram of the movie review
    word counts. The pandas library generates beautiful word count charts. Pluto reuses
    the function to display the **Netflix** NLP data, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第二章*](B17990_02.xhtml#_idTextAnchor038) 使用 `draw_word_count()` 方法显示每条记录的平均单词数，以及最短和最长的电影评论。图表的右侧显示了电影评论单词计数的直方图。pandas
    库生成了漂亮的单词计数图表。Pluto 重用了这个函数来显示 **Netflix** NLP 数据，具体如下：'
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.4 – Netflix word counts](img/B17990_05_04.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – Netflix 单词计数](img/B17990_05_04.jpg)'
- en: Figure 5.4 – Netflix word counts
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – Netflix 单词计数
- en: 'The Netflix movie description mean is 23.88 words, with a minimum of 10 words
    and a maximum of 48 words. Pluto does the same for the **Twitter** NLP data, as
    follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Netflix 电影描述的平均字数为 23.88 字，最少 10 字，最多 48 字。Pluto 对 **Twitter** NLP 数据做了同样的处理，具体如下：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.5 – Twitter word counts](img/B17990_05_05.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – Twitter 单词计数](img/B17990_05_05.jpg)'
- en: Figure 5.5 – Twitter word counts
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – Twitter 单词计数
- en: The average word count of the Twitter tweets is 12.78 words, with a minimum
    of 1 word and a maximum of 33 words.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 推文的平均字数为 12.78 字，最少 1 字，最多 33 字。
- en: 'Pluto writes the `draw_text_null_data()` method to check whether there is any
    missing data, also known as a `Missingno` library generates the graph with the
    following key line of code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 编写了 `draw_text_null_data()` 方法来检查是否存在缺失数据，也就是使用 `Missingno` 库生成图表，关键代码行如下：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Pluto draws the `null` data graph for the Netflix data, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 为 Netflix 数据绘制了 `null` 数据图表，如下所示：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.6 – Netflix missing data](img/B17990_05_06.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – Netflix 缺失数据](img/B17990_05_06.jpg)'
- en: Figure 5.6 – Netflix missing data
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – Netflix 缺失数据
- en: There is missing data in the **director**, **cast**, and **country** categories
    for the **Netflix** data, but the **description** category, also known as the
    movie review, has no missing data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**Netflix** 数据中的 **director**、**cast** 和 **country** 分类存在缺失数据，但 **description**
    分类，也就是电影评论，没有缺失数据。'
- en: 'Pluto does the same for the **Twitter** data, as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 对 **Twitter** 数据做了相同的操作，如下所示：
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.7 – Twitter missing data](img/B17990_05_07.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – Twitter 缺失数据](img/B17990_05_07.jpg)'
- en: Figure 5.7 – Twitter missing data
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – Twitter 缺失数据
- en: There is no missing data in the **Twitter** data.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**Twitter** 数据中没有缺失数据。'
- en: Fun fact
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Many multimillion-dollar AI systems have failed primarily because of a lack
    of control over the input data. For example, the *Amazon Recruiting* system in
    2020 failed because there was no diversity in the dataset, and the most egregious
    debacle was *Microsoft’s Chatbot Tay* in 2016\. It was corrupted by Twitter users
    inputting sexist and racist tweets.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 许多价值数百万美元的 AI 系统失败的主要原因是缺乏对输入数据的控制。例如，2020 年的 *Amazon Recruiting* 系统失败了，因为数据集中缺乏多样性，最严重的失败是
    2016 年的 *Microsoft Chatbot Tay*。它被 Twitter 用户输入的性别歧视和种族主义推文所破坏。
- en: 'The next chart is the word cloud infographic diagram. This is an extraordinary
    method for visualizing the NLP text. The most commonly used words are displayed
    in a large font, while the least used terms are displayed in a smaller font. The
    **WordCloud** library generates the infographic chart, and the essential code
    snippet is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图表是字词云信息图。这是一种极好的 NLP 文本可视化方法。最常用的词语以大字号显示，而最少使用的词语则以较小字号显示。**WordCloud**
    库生成了信息图表，关键代码片段如下：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Pluto uses the `_draw_text_wordcloud()` helper function and the `draw_text_wordcloud()`
    method to display the infographic chart for real-world **Netflix** data, as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 使用 `_draw_text_wordcloud()` 辅助函数和 `draw_text_wordcloud()` 方法，展示了真实世界的
    **Netflix** 数据的字词云信息图，如下所示：
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.8 – Netflix word cloud, with approximately 246,819 words](img/B17990_05_08.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – Netflix 字词云，包含大约 246,819 个单词](img/B17990_05_08.jpg)'
- en: Figure 5.8 – Netflix word cloud, with approximately 246,819 words
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – Netflix 字词云，包含大约 246,819 个单词
- en: 'Pluto does the same for the real-world **Twitter** data, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 对真实世界的 **Twitter** 数据也做了相同的操作，如下所示：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.9 – Twitter    word cloud, with approximately 464,992 words](img/B17990_05_09.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – Twitter 字词云，包含大约 464,992 个单词](img/B17990_05_09.jpg)'
- en: Figure 5.9 – Twitter word cloud, with approximately 464,992 words
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – Twitter 字词云，包含大约 464,992 个单词
- en: Fun fact
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Here is a fun fact about the history of word cloud graphs. The word cloud, also
    known as a tag cloud, Wordle, or weighted list, was first used in print by **Douglas
    Coupland** in the book *Microserfs*. It was published in 1995, but not until 2004
    did the word clouds exist in digital format on the *Flickr* website. Today, word
    cloud infographics are widespread on the web and in academic papers.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个关于字词云图的有趣小知识。字词云，又称标签云、Wordle 或加权列表，最早由 **Douglas Coupland** 在 1995 年的书籍
    *Microserfs* 中以印刷形式使用。但直到 2004 年，字词云才在 *Flickr* 网站上以数字格式存在。今天，字词云信息图在互联网上和学术论文中广泛应用。
- en: So far, Pluto has discussed character, word, and sentence augmentation theories,
    chosen the **Nlpaug** text augmentation library, and downloaded the real-world
    **Netflix** and **Twitter** NLP datasets. It is time for Pluto to reinforce his
    learning by performing text augmentation with Python code.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，Pluto 已经讨论了字符、单词和句子增强理论，选择了 **Nlpaug** 文本增强库，并下载了真实世界的 **Netflix** 和 **Twitter**
    NLP 数据集。现在是时候通过 Python 代码执行文本增强来强化他的学习了。
- en: Reinforcing learning through Python Notebook
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 Python Notebook 强化学习
- en: Pluto uses the Python Notebook to reinforce our understanding of text augmentation.
    He uses the batch function to display text in batches. This works similarly to
    the batch functions for images. In other words, it randomly selects new records
    and transforms them using the augmentation methods.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 使用 Python Notebook 加深我们对文本增强的理解。他使用批处理函数以批次的形式显示文本。它的工作方式类似于图像的批处理函数。换句话说，它会随机选择新的记录，并使用增强方法进行转换。
- en: Fun fact
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Pluto recommends running the batch functions repeatedly to gain a deeper insight
    into the text augmentation methods. There are thousands of text records in the
    **Twitter** and **Amazon** datasets. Each time you run the batch functions, it
    displays different records from the dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 推荐重复运行批处理函数，以深入了解文本增强方法。在 **Twitter** 和 **Amazon** 数据集中有成千上万的文本记录。每次运行批处理函数时，它会显示来自数据集的不同记录。
- en: As with the image augmentation implementation, the wrapper functions use the
    **Nlpaug** library under the hood. The wrapper function allows you to focus on
    the text transformation concepts and not be distracted by the library implementation.
    You can use another text augmentation library, and the wrapper function input
    and output will remain the same.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像增强实现类似，包装函数在幕后使用 **Nlpaug** 库。这个包装函数让你可以专注于文本转换的概念，而不被库的实现所分心。你也可以使用其他文本增强库，包装函数的输入和输出将保持不变。
- en: Pluto could write one complex function that contains all the text transformation
    techniques, and it may be more efficient, but that is not the goal of this book.
    After reading this book, you can choose to rewrite or hack the Python Notebook
    to suit your style with confidence.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 可以编写一个复杂的函数，包含所有文本转换技术，这可能更高效，但这并不是本书的目标。阅读完本书后，你可以自信地选择重写或修改 Python Notebook
    以适应你的风格。
- en: 'In this chapter, Pluto uses an opening line from the book *A Tale of Two Cities*
    by **Charles Dickens** as the control text. Pluto paraphrases the text by substituting
    the commas between the phrases with periods because this makes it easier for the
    text augmentation process. The control text is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，Pluto 使用 **查尔斯·狄更斯** 的《双城记》中的开篇句子作为控制文本。Pluto 通过将短语之间的逗号替换为句号来改写文本，因为这样更有利于文本增强过程。控制文本如下：
- en: '*“It was the best of times. It was the worst of times. It was the age of wisdom.
    It was the age of foolishness. It was the epoch of belief. It was the epoch* *of
    incredulity.”*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*“这是最好的时代，也是最坏的时代。是智慧的时代，也是愚蠢的时代。是信仰的纪元，也是怀疑的纪元。”*'
- en: 'The Python Notebook covers the following topics:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Python Notebook 涵盖以下主题：
- en: Character augmentation
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符增强
- en: Word augmentation
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词增强
- en: Let’s start with the three character augmentation techniques.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从三种字符增强技术开始。
- en: Character augmentation
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字符增强
- en: Character augmentation involves injecting errors into the text. The process
    is counterintuitive because it purposely adds errors to the data. In other words,
    it makes the text harder for humans to understand. In contrast, computers use
    deep learning algorithms to predict the outcome, particularly the **Convolutional
    Neural Network** (**CNN**) and the **Recurrent Neural Network** (**RNN**) algorithms.
    For example, sentiment classification for tweets does not affect by misspelled
    words.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 字符增强是指向文本中注入错误。这个过程是违反直觉的，因为它故意向数据中添加错误。换句话说，它使得文本对于人类来说更难理解。相反，计算机使用深度学习算法来预测结果，尤其是
    **卷积神经网络**（**CNN**）和 **递归神经网络**（**RNN**）算法。例如，推文的情感分类不会受到拼写错误的影响。
- en: 'In particular, Pluto will explain the following three methods:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，Pluto 将解释以下三种方法：
- en: OCR augmenting
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OCR 增强
- en: Keyboard augmenting
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键盘增强
- en: Random augmenting
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机增强
- en: Let’s start with OCR.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 OCR 开始。
- en: OCR augmenting
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OCR 增强
- en: The OCR process converts an image into a piece of text, with frequent errors
    such as mixing *0* and *o* during the conversion.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: OCR 过程将图像转换为文本，在转换过程中常出现错误，如将 *0* 和 *o* 混淆。
- en: 'Pluto writes the `_print_aug_batch()` helper function to randomly select sample
    records from the NLP data, apply the text augmenting method, and print it using
    pandas. The input or method definition is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 编写了 `_print_aug_batch()` 辅助函数，随机选择 NLP 数据中的样本记录，应用文本增强方法，并使用 pandas 打印。输入或方法定义如下：
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, `df` is the pandas DataFrame, `aug_function` is the augmentation method
    from the wrapper function, `col_dest` is the chosen column destination, `bsize`
    is the number of samples in the batch with a default of three, and `title` is
    the optional title for the chart.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`df` 是 pandas DataFrame，`aug_function` 是包装函数中的增强方法，`col_dest` 是选择的目标列，`bsize`
    是批次中的样本数，默认为三，`title` 是图表的可选标题。
- en: 'The OCR wrapper function is elementary. The two lines of code are the `aug_func`)
    and the helper function. The entire code is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: OCR 包装函数是基础的。两行代码分别是 `aug_func` 和辅助函数。整个代码如下：
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Pluto uses the `print_aug_ocr()` method with the **Netflix** data, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 使用 `print_aug_ocr()` 方法处理 **Netflix** 数据，如下所示：
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.10 – Netflix OCR augmenting](img/B17990_05_10.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – Netflix OCR 增强](img/B17990_05_10.jpg)'
- en: Figure 5.10 – Netflix OCR augmenting
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – Netflix OCR 增强
- en: In *Figure 5**.10*, the first line is **Dickens’** control text, with the augmented
    text on the left-hand side and the original text on the right-hand side. The following
    three rows are randomly sampled from the **Netflix** NLP data. Pluto recommends
    that you read the left-hand augmented text first. Stop and try to decipher the
    meaning before reading the original text.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.10* 中，第一行是 **狄更斯** 的控制文本，左侧是增强后的文本，右侧是原始文本。接下来的三行是从 **Netflix** NLP 数据中随机抽取的。Pluto
    推荐先阅读左侧的增强文本。停下来并试着解读其含义，然后再阅读原始文本。
- en: Fun fact
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 趣味事实
- en: Pluto recommends repeatedly running the `print_aug_ocr()` method to see other
    movie descriptions. You can increase `bsize` to see more than two records at a
    time.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 推荐反复运行 `print_aug_ocr()` 方法以查看其他电影描述。你可以增加 `bsize` 来查看一次超过两条记录。
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 同样对 **Twitter** 的 NLP 数据进行处理，如下所示：
- en: '[PRE23]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.11 – Twitter OCR augmenting](img/B17990_05_11.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – Twitter OCR 增强](img/B17990_05_11.jpg)'
- en: Figure 5.11 – Twitter OCR augmenting
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – Twitter OCR 增强
- en: Next, Pluto moves on from the OCR method to the keyboard technique.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，Pluto 从 OCR 方法转向键盘技术。
- en: Keyboard augmenting
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 键盘增强
- en: 'The keyboard augmenting method replaces a character with a close-distance key
    on a keyboard. For example, a typical typing error for character *b* is using
    key *v* or key *n*. The augmentation variable defines as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 键盘增强方法通过用键盘上相近的键替换字符。例如，字符 *b* 的典型打字错误是按了键 *v* 或键 *n*。增强变量定义如下：
- en: '[PRE24]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Pluto uses the `print_aug_keyboard()` wrapper function with the **Netflix**
    NLP data, as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 使用 `print_aug_keyboard()` 包装函数处理 **Netflix** 的 NLP 数据，如下所示：
- en: '[PRE25]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.12 – Netflix keyboard augmenting](img/B17990_05_12.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – Netflix 键盘增强](img/B17990_05_12.jpg)'
- en: Figure 5.12 – Netflix keyboard augmenting
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – Netflix 键盘增强
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 同样对 **Twitter** 的 NLP 数据进行处理，如下所示：
- en: '[PRE26]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.13 – Twitter keyboard augmenting](img/B17990_05_13.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13 – Twitter 键盘增强](img/B17990_05_13.jpg)'
- en: Figure 5.13 – Twitter keyboard augmenting
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – Twitter 键盘增强
- en: The last of the three text augmentation methods is the random technique.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 三种文本增强方法中的最后一种是随机技术。
- en: Random augmenting
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机增强
- en: 'The random character function randomly swaps, inserts, or deletes characters
    in the text. The four modes for the random process are **inserting**, **deleting**,
    **substituting**, and **swapping**. The augmentation variable defines as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 随机字符功能会随机交换、插入或删除文本中的字符。随机过程的四种模式是 **插入**、**删除**、**替换** 和 **交换**。增强变量定义如下：
- en: '[PRE27]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Pluto uses the `print_aug_random()` wrapper function with `action` set to `insert`
    in the **Netflix** NLP data, as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 在 **Netflix** 的 NLP 数据中使用 `print_aug_random()` 包装函数，并将 `action` 设置为 `insert`，如下所示：
- en: '[PRE28]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.14 – Netflix random insert augmenting](img/B17990_05_14.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 – Netflix 随机插入增强](img/B17990_05_14.jpg)'
- en: Figure 5.14 – Netflix random insert augmenting
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – Netflix 随机插入增强
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 同样对 **Twitter** 的 NLP 数据进行处理，如下所示：
- en: '[PRE29]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.15 – Twitter random insert augmenting](img/B17990_05_15.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – Twitter 随机插入增强](img/B17990_05_15.jpg)'
- en: Figure 5.15 – Twitter random insert augmenting
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – Twitter 随机插入增强
- en: 'Pluto uses the `print_aug_random()` wrapper function with `action` set to `delete`
    for the **Netflix** NLP data, as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 在 **Netflix** NLP 数据上使用 `print_aug_random()` 包装函数，`action` 设置为 `delete`，具体如下：
- en: '[PRE30]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.16 – Netflix random delete augmenting](img/B17990_05_16.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – Netflix 随机删除增强](img/B17990_05_16.jpg)'
- en: Figure 5.16 – Netflix random delete augmenting
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – Netflix 随机删除增强
- en: 'Pluto does the same for the Twitter NLP data, as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 对 Twitter 的 NLP 数据也进行了相同处理，具体如下：
- en: '[PRE31]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.17 – Twitter random delete augmenting](img/B17990_05_17.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – Twitter 随机删除增强](img/B17990_05_17.jpg)'
- en: Figure 5.17 – Twitter random delete augmenting
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – Twitter 随机删除增强
- en: 'Pluto uses the `print_aug_random()` wrapper function with `actio`n set to `substitute`
    for the **Netflix** NLP data, as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 在 **Netflix** NLP 数据上使用 `print_aug_random()` 包装函数，`action` 设置为 `substitute`，具体如下：
- en: '[PRE32]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.18 – Netflix random substitute augmenting](img/B17990_05_18.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18 – Netflix 随机替换增强](img/B17990_05_18.jpg)'
- en: Figure 5.18 – Netflix random substitute augmenting
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 – Netflix 随机替换增强
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 对 **Twitter** NLP 数据也进行了相同处理，具体如下：
- en: '[PRE33]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.19 – Twitter random substitute augmenting](img/B17990_05_19.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.19 – Twitter 随机替换增强](img/B17990_05_19.jpg)'
- en: Figure 5.19 – Twitter random substitute augmenting
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 – Twitter 随机替换增强
- en: 'Pluto uses the `print_aug_random()` wrapper function with `action` set to `swap`
    for the **Netflix** NLP data, as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 在 **Netflix** NLP 数据上使用 `print_aug_random()` 包装函数，`action` 设置为 `swap`，具体如下：
- en: '[PRE34]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.20 – Netflix random swap augmenting](img/B17990_05_20.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.20 – Netflix 随机交换增强](img/B17990_05_20.jpg)'
- en: Figure 5.20 – Netflix random swap augmenting
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20 – Netflix 随机交换增强
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 对 **Twitter** NLP 数据也进行了相同处理，具体如下：
- en: '[PRE35]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.21 – Twitter random swap augmenting](img/B17990_05_21.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.21 – Twitter 随机交换增强](img/B17990_05_21.jpg)'
- en: Figure 5.21 – Twitter random swap augmenting
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.21 – Twitter 随机交换增强
- en: Fun challenge
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战
- en: 'Here is a thought experiment: if the input text contains misspelled words and
    bad grammar, such as tweets, could correcting the spelling and grammar be a valid
    augmentation method?'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个思维实验：如果输入文本包含拼写错误和语法错误，比如推文，那么纠正拼写和语法是否可以作为有效的增强方法？
- en: Pluto has covered the **OCR**, **Keyboard**, and four modes of **Random** character
    augmentation techniques. The next step is augmenting words.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 已涵盖 **OCR**、**键盘** 和四种模式的 **随机** 字符增强技术。下一步是增强单词。
- en: Word augmenting
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词增强
- en: At this point in the book, Pluto might think text augmentation is effortless,
    and it is true. We built a solid foundation layer in [*Chapter 1*](B17990_01.xhtml#_idTextAnchor016)
    with an object-oriented class and learned how to extend the object as we learned
    about new augmentation techniques. In [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038),
    Pluto added the functions for downloading any *Kaggle* real-world dataset, and
    *Chapters 3* and *4* gave us the wrapper function pattern. Therefore, at this
    point, Pluto reuses the methods and patterns to make the Python code concise and
    easy to understand.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，Pluto 可能会觉得文本增强毫不费力，实际上这是真的。我们在 [*第一章*](B17990_01.xhtml#_idTextAnchor016)
    中打下了坚实的基础，使用面向对象的类并学习如何在学习新增强技术时扩展对象。在 [*第二章*](B17990_02.xhtml#_idTextAnchor038)
    中，Pluto 添加了用于下载任何 *Kaggle* 真实世界数据集的函数，*第三章* 和 *第四章* 给我们提供了包装函数模式。因此，到了这一点，Pluto
    复用了这些方法和模式，使 Python 代码简洁易懂。
- en: The word augmentation process is similar to character augmentation. Pluto uses
    the same `_print_aug_batch()` helper method. In particular, Pluto will cover the
    **Misspell**, **Split**, **Random**, **Synonyms**, **Antonyms**, and **Reserved**
    word augmenting techniques.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 单词增强过程类似于字符增强。Pluto 使用相同的 `_print_aug_batch()` 辅助方法。特别是，Pluto 将涵盖 **拼写错误**、**分割**、**随机**、**同义词**、**反义词**
    和 **保留** 单词增强技术。
- en: Let’s start with the misspell augmentation technique.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从拼写错误增强技术开始。
- en: Misspell augmenting
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拼写错误增强
- en: 'The misspell augmentation function uses a predefined dictionary to simulate
    spelling mistakes. The augmentation variable defines this as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写错误增强函数使用预定义字典模拟拼写错误。增强变量定义如下：
- en: '[PRE36]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Pluto uses the `print_aug_word_misspell()` wrapper function on the **Netflix**
    NLP data, as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 在 **Netflix** NLP 数据上使用 `print_aug_word_misspell()` 包装函数，具体如下：
- en: '[PRE37]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.22 – Netflix misspell word augmenting](img/B17990_05_22.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.22 – Netflix 拼写错误单词增强](img/B17990_05_22.jpg)'
- en: Figure 5.22 – Netflix misspell word augmenting
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22 – Netflix 拼写错误单词增强
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 对**Twitter** NLP 数据执行相同操作，如下所示：
- en: '[PRE38]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.23 – Twitter misspell word augmenting](img/B17990_05_23.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.23 – Twitter 拼写错误单词增强](img/B17990_05_23.jpg)'
- en: Figure 5.23 – Twitter misspell word augmenting
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.23 – Twitter 拼写错误单词增强
- en: Similar to **Misspell** is the **Split** word augmentation technique.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 与**拼写错误**类似的是**分割**单词增强技术。
- en: Split augmenting
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分割增强
- en: 'The split augmentation function randomly splits words into two tokens. The
    augmentation variable defines this as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 分割增强函数随机将单词拆分为两个标记。增强变量定义如下：
- en: '[PRE39]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Pluto uses the `print_aug_word_split()` wrapper function on the **Netflix**
    NLP data, as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 在**Netflix** NLP 数据上使用 `print_aug_word_split()` 包装函数，如下所示：
- en: '[PRE40]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.24 – Netflix split word augmenting](img/B17990_05_24.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.24 – Netflix 分割单词增强](img/B17990_05_24.jpg)'
- en: Figure 5.24 – Netflix split word augmenting
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24 – Netflix 分割单词增强
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 对**Twitter** NLP 数据执行相同操作，如下所示：
- en: '[PRE41]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.25 – Twitter split word augmenting](img/B17990_05_25.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.25 – Twitter 分割单词增强](img/B17990_05_25.jpg)'
- en: Figure 5.25 – Twitter split word augmenting
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25 – Twitter 分割单词增强
- en: After the split word method, Pluto presents the random word augmenting method.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割单词方法之后，Pluto 展示了随机单词增强方法。
- en: Random augmenting
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机增强
- en: 'The random word augmentation method applies random behavior to the text with
    four parameters: **Swap**, **Crop**, **Substitute**, or **Delete**. The augmentation
    variable defines this as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 随机单词增强方法对文本应用随机行为，具有四个参数：**交换**、**裁剪**、**替换**或**删除**。增强变量定义如下：
- en: '[PRE42]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Pluto uses the `print_aug_word_random()` wrapper function for swapping mode
    on the **Netflix** NLP data, as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 在**Netflix** NLP 数据上使用 `print_aug_word_random()` 包装函数进行交换模式，如下所示：
- en: '[PRE43]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.26 – Netflix random swap word augmenting](img/B17990_05_26.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.26 – Netflix 随机交换单词增强](img/B17990_05_26.jpg)'
- en: Figure 5.26 – Netflix random swap word augmenting
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.26 – Netflix 随机交换单词增强
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 对**Twitter** NLP 数据执行相同操作，如下所示：
- en: '[PRE44]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.27 – Twitter random swap word augmenting](img/B17990_05_27.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.27 – Twitter 随机交换单词增强](img/B17990_05_27.jpg)'
- en: Figure 5.27 – Twitter random swap word augmenting
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.27 – Twitter 随机交换单词增强
- en: 'Pluto uses the `print_aug_word_random()` wrapper function for cropping mode
    on the **Netflix** NLP data, as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 在**Netflix** NLP 数据上使用 `print_aug_word_random()` 包装函数进行裁剪模式，如下所示：
- en: '[PRE45]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.28 – Netflix random crop word augmenting](img/B17990_05_28.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.28 – Netflix 随机裁剪单词增强](img/B17990_05_28.jpg)'
- en: Figure 5.28 – Netflix random crop word augmenting
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.28 – Netflix 随机裁剪单词增强
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 对**Twitter** NLP 数据执行相同操作，如下所示：
- en: '[PRE46]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.29 – Twitter random crop word augmenting](img/B17990_05_29.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.29 – Twitter 随机裁剪单词增强](img/B17990_05_29.jpg)'
- en: Figure 5.29 – Twitter random crop word augmenting
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.29 – Twitter 随机裁剪单词增强
- en: So, Pluto has described the **Swap** and **Crop** word augmentation methods
    but not the **Substitute** and **Delete** ones. This is because they are similar
    to the character augmenting functions and are in the Python Notebook. Next on
    the block is synonym augmenting.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，Pluto 描述了**交换**和**裁剪**单词增强方法，但没有涉及**替换**和**删除**方法。这是因为它们类似于字符增强函数，并且已经在 Python
    Notebook 中。接下来的内容是同义词增强。
- en: Synonym augmenting
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 同义词增强
- en: 'The synonym augmentation function substitutes words with synonyms from a predefined
    database. **WordNet** and **PPBD** are two optional databases. The augmentation
    variable defines this process as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 同义词增强函数将单词替换为预定义数据库中的同义词。**WordNet** 和 **PPBD** 是两个可选的数据库。增强变量定义该过程如下：
- en: '[PRE47]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Pluto uses the `print_aug_word_synonym()` wrapper function on the **Netflix**
    NLP data, as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 在**Netflix** NLP 数据上使用 `print_aug_word_synonym()` 包装函数，如下所示：
- en: '[PRE48]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.30 – Netflix synonym word augmenting](img/B17990_05_30.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.30 – Netflix 同义词单词增强](img/B17990_05_30.jpg)'
- en: Figure 5.30 – Netflix synonym word augmenting
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.30 – Netflix 同义词单词增强
- en: 'It is interesting and funny that the synonym of *It* is **Information Technology**
    for the control text. Mr. Dickens, who wrote Tale of Two Cities in 1859, could
    never have known that IT is a popular acronym for information technology. Pluto
    does the same for the **Twitter** NLP data, as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣且好笑的是，*It* 的同义词对于控制文本来说是 **信息技术**。写下《双城记》的狄更斯先生，1859 年时肯定无法预见到 IT 是信息技术的流行缩写。Pluto
    在 **Twitter** NLP 数据上做了相同的处理，如下所示：
- en: '[PRE49]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.31 – Twitter synonym word augmenting](img/B17990_05_31.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.31 – Twitter 同义词增强](img/B17990_05_31.jpg)'
- en: Figure 5.31 – Twitter synonym word augmenting
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.31 – Twitter 同义词增强
- en: When there are synonyms, you will also find antonyms.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 当有同义词时，你也会找到反义词。
- en: Antonym augmenting
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反义词增强
- en: 'The antonym augmentation function randomly replaces words with antonyms. The
    augmentation variable defines this as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 反义词增强功能会随机用反义词替换词汇。增强变量定义了这个过程，如下所示：
- en: '[PRE50]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Pluto uses the `print_aug_word_antonym()` wrapper function on the **Netflix**
    NLP data, as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 使用 `print_aug_word_antonym()` 包装函数处理 **Netflix** NLP 数据，如下所示：
- en: '[PRE51]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output is as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.32 – Netflix antonym word augmenting](img/B17990_05_32.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.32 – Netflix 反义词增强](img/B17990_05_32.jpg)'
- en: Figure 5.32 – Netflix antonym word augmenting
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.32 – Netflix 反义词增强
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 在 **Twitter** NLP 数据上做了相同的处理，如下所示：
- en: '[PRE52]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output is as follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.33 – Twitter antonym word augmenting](img/B17990_05_33.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.33 – Twitter 反义词增强](img/B17990_05_33.jpg)'
- en: Figure 5.33 – Twitter antonym word augmenting
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.33 – Twitter 反义词增强
- en: After synonyms and antonyms, which are automated, reserved word augmentation
    requires a manual word list.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在同义词和反义词增强（自动化处理）之后，保留字增强需要手动词汇表。
- en: Reserved word augmenting
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保留字增强
- en: 'The reserved word augmentation method swaps target words where you define a
    word list. It is the same as synonyms, except the terms are created manually.
    Pluto uses the **Netflix** and **Twitter** word cloud diagrams, *Figures 5.8*
    and *5.9*, to select the top three reoccurring words in the NLP datasets. The
    augmentation variable defines this process as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 保留字增强方法通过定义一个词汇表来交换目标词汇。它与同义词增强相同，只不过这些词是手动创建的。Pluto 使用 **Netflix** 和 **Twitter**
    的词云图，*图 5.8* 和 *5.9*，来选择 NLP 数据集中排名前三的重复词汇。增强变量定义了这个过程，如下所示：
- en: '[PRE53]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Pluto uses the `print_aug_word_reserved()` wrapper function on the **Netflix**
    NLP data, as follows:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 使用 `print_aug_word_reserved()` 包装函数处理 **Netflix** NLP 数据，如下所示：
- en: '[PRE54]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.34 – Netflix reserved word augmenting](img/B17990_05_34.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.34 – Netflix 保留字增强](img/B17990_05_34.jpg)'
- en: Figure 5.34 – Netflix reserved word augmenting
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.34 – Netflix 保留字增强
- en: 'Notice the words **wisdom** and **foolishness** are substituted with **Intelligence**
    and **idiocy**, **life** with **existance**, and **family** with **brood**. Pluto
    does the same for the **Twitter** NLP data, as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**wisdom** 和 **foolishness** 被替换为 **Intelligence** 和 **idiocy**，**life**
    被替换为 **existance**，**family** 被替换为 **brood**。Pluto 在 **Twitter** NLP 数据上做了相同的处理，如下所示：
- en: '[PRE55]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output is as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.35 – Twitter reserved word augmenting](img/B17990_05_35.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.35 – Twitter 保留字增强](img/B17990_05_35.jpg)'
- en: Figure 5.35 – Twitter reserved word augmenting
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.35 – Twitter 保留字增强
- en: Notice the words **wisdom** and **foolishness** are substituted with **sagacity**
    and **idiocy**, and **user** with **people** and **customer**.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**wisdom** 和 **foolishness** 被替换为 **sagacity** 和 **idiocy**，**user** 被替换为
    **people** 和 **customer**。
- en: '**Reserved Word** augmenting is the last word augmentation method of this chapter.
    Pluto has covered **Misspell**, **Split**, **Random**, **Synonym**, **Antonym**,
    and **Reserved Word** augmentation, but these are only some of the possible word
    augmentation techniques you can use.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '**保留字** 增强是本章的最后一种词汇增强方法。Pluto 涵盖了 **拼写错误**、**拆分**、**随机**、**同义词**、**反义词**、**保留字**增强，但这些只是你可以使用的一些词汇增强技术。'
- en: Fun challenge
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战
- en: The challenge is to use the Augly library or the NLTK, Gensim, or Textblob libraries
    to write a new wrapper function. It is relatively easy. The first step is to copy
    a wrapper function, such as the `print_aug_keyboard()` function. The second and
    last step is to replace `aug_func = nlpaug.augmenter.char.KeyboardAug()` with
    `aug_func = augly.text.functional.simulate_typos()`. There are more parameters
    in the Augly function. A hint is to use the `augly.text.functional.simulate_typos?`
    command to display the function documentation.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战是使用 Augly 库或 NLTK、Gensim 或 Textblob 库编写一个新的包装函数。这相对简单。第一步是复制一个包装函数，比如 `print_aug_keyboard()`
    函数。第二步也是最后一步是将 `aug_func = nlpaug.augmenter.char.KeyboardAug()` 替换为 `aug_func
    = augly.text.functional.simulate_typos()`。Augly 函数中有更多参数。一个提示是使用 `augly.text.functional.simulate_typos?`
    命令来显示该函数的文档。
- en: The **Nlpaug** library and other text augmentation libraries, such as **NLTK**,
    **Gensim**, **Textblob**, and **Augly**, have additional text augmentation methods.
    In addition, newly published scholarly papers are an excellent source in which
    to discover new text augmentation techniques.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**Nlpaug** 库和其他文本增强库，如 **NLTK**、**Gensim**、**Textblob** 和 **Augly**，提供了更多的文本增强方法。此外，最新发布的学术论文也是发现新文本增强技术的绝佳来源。'
- en: Let’s summarize this chapter.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下本章内容。
- en: Summary
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: At first glance, text augmentation seems counterintuitive and problematic because
    the techniques inject errors into the text. Still, DL based on CNNs or RNNs recognizes
    patterns regardless of a few misspellings or synonym replacements. Furthermore,
    many published scholarly papers have described the benefits of text augmentation
    to increase prediction or forecast accuracy.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，文本增强似乎违反直觉且有问题，因为这些技术会向文本中注入错误。然而，基于 CNN 或 RNN 的深度学习仍然能够识别模式，不管是拼写错误还是同义词替换。此外，许多已发布的学术论文都描述了文本增强在提高预测或预报准确性方面的好处。
- en: In [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101), you learned about three
    **Character** augmentation techniques, **OCR, Keyboard**, and **Random**. In addition,
    the six **Word** augmentation techniques are the **Misspell**, **Split**, **Random**,
    **Synonyms**, **Antonyms**, and **Reserved** words. There are more text augmentation
    methods in the Nlgaug, NLTK, Gensim, TextBlob, and Augly libraries.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第5章*](B17990_05.xhtml#_idTextAnchor101)中，你学习了三种**字符**增强技术，分别是**OCR**、**键盘**和**随机**。此外，还有六种**单词**增强技术，分别是**拼写错误**、**分割**、**随机**、**同义词**、**反义词**和**保留**词。在
    Nlgaug、NLTK、Gensim、TextBlob 和 Augly 库中还有更多文本增强方法。
- en: Implementing the text augmentation methods using a Python Notebook is deceptively
    simple. This is because Pluto built a solid foundation layer in [*Chapter 1*](B17990_01.xhtml#_idTextAnchor016)
    with an object-oriented class and learned how to extend the object with **decorator**
    as he discovered new augmentation techniques. In [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038),
    Pluto added the functions for downloading any *Kaggle* real-world dataset, and
    *Chapters 3* and *4* gave us the wrapper function pattern. Therefore, in this
    chapter, Pluto reused the methods and patterns to make the Python Notebook code
    concise and easy to understand.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python Notebook 实现文本增强方法看似简单。因为 Pluto 在[*第1章*](B17990_01.xhtml#_idTextAnchor016)中构建了一个面向对象的类，并学习了如何使用**装饰器**扩展对象，从而在发现新的增强技术时能够加以利用。在[*第2章*](B17990_02.xhtml#_idTextAnchor038)中，Pluto
    添加了用于下载任何*Kaggle*真实世界数据集的功能，*第3章*和*第4章*为我们提供了包装函数模式。因此，在本章中，Pluto 复用了这些方法和模式，使得
    Python Notebook 代码简洁易懂。
- en: Throughout the chapter, there are *Fun facts* and *Fun challenges*. Pluto hopes
    you will take advantage of them and expand your experience beyond the scope of
    this chapter.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个章节中，包含了*有趣的事实*和*有趣的挑战*。Pluto 希望你能够利用这些内容，拓展你的经验，超越本章的范围。
- en: The next chapter will delve deeper into text augmentation using machine learning
    methods. Ironically, the goal of text augmentation is to make machine learning
    and DL predict and forecast accurately, and we will use the same AI system to
    increase the efficiency of text augmentation. It is a circular logic or cyclical
    process.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将深入探讨使用机器学习方法的文本增强。具有讽刺意味的是，文本增强的目标是使机器学习和深度学习能够准确预测和预报，而我们将使用相同的 AI 系统来提高文本增强的效率。这是一个循环逻辑或周期性过程。
- en: Pluto is waiting for you in the next chapter, *Text Augmentation with* *Machine
    Learning*.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 正在下一章等你，*使用机器学习进行文本增强*。
