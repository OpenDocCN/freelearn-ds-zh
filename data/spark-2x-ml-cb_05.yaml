- en: Practical Machine Learning with Regression and Classification in Spark 2.0 -
    Part I
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中进行回归和分类的实用机器学习-第一部分
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Fitting a linear regression line to data the old-fashioned way
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将线性回归线拟合到数据的传统方法
- en: Generalized linear regression in Spark 2.0
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0中的广义线性回归
- en: Linear regression API with Lasso and L-BFGS in Spark 2.0
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0中具有Lasso和L-BFGS的线性回归API
- en: Linear regression API with Lasso and auto optimization selection in Spark 2.0
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0中具有Lasso和自动优化选择的线性回归API
- en: Linear regression API with ridge regression and auto optimization selection
    in Spark 2.0
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0中具有岭回归和自动优化选择的线性回归API
- en: Isotonic regression in Apache Spark 2.0
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 2.0中的等保回归
- en: Multilayer perceptron classifier in Apache Spark 2.0
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 2.0中的多层感知器分类器
- en: One versus Rest classifier (One-vs-All) in Apache Spark 2.0
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 2.0中的一对多分类器（One-vs-All）
- en: Survival regression - parametric AFT model in Apache Spark 2.0
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 2.0中的生存回归-参数AFT模型
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This chapter, along with the next chapter, covers the fundamental techniques
    for regression and classification available in Spark 2.0 ML and MLlib library.
    Spark 2.0 highlights a new direction by moving the RDD-based regressions (see
    the next chapter) to maintenance mode while emphasizing **Linear Regression**
    and **Generalized Regression** going forward.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章与下一章一起，涵盖了Spark 2.0 ML和MLlib库中可用的回归和分类的基本技术。Spark 2.0通过将基于RDD的回归（见下一章）移动到维护模式来突显新的方向，同时强调**线性回归**和**广义回归**。
- en: At a high level, the new API design favors parameterization of elastic net to
    produce the ridge versus Lasso regression and everything in between, as opposed
    to a named API (for example, `LassoWithSGD`). The new API approach is a much cleaner
    design and forces you to learn elastic net and its power when it comes to feature
    engineering that remains an art in data science. We provide adequate examples,
    variations, and notes to guide you through the complexities in these techniques.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，新的API设计更倾向于对弹性网的参数化，以产生岭回归与Lasso回归以及两者之间的一切，而不是命名API（例如，`LassoWithSGD`）。新的API方法是一个更清晰的设计，并迫使您学习弹性网及其在特征工程中的作用，这在数据科学中仍然是一门艺术。我们提供充分的例子、变化和注释，以指导您应对这些技术中的复杂性。
- en: 'The following figure depicts the regression and classification coverage (part
    1) in this chapter:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了本章中回归和分类覆盖范围（第一部分）：
- en: '![](img/00104.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00104.jpeg)'
- en: First, you will learn how to implement linear regression using algebraic equations
    via just Scala code and RDDs from scratch to get an insight for the math and why
    we need an iterative optimization method to estimate the solution for a large
    system of regressions. Second, we explore the **generalized linear model** (**GLM**)
    and its various statistical distribution families and link functions while stressing
    its limitation to 4,096 parameters only in the current implementation. Third,
    we tackle the **linear regression model** (**LRM**) and how to use the elastic
    net parameterization to mix and match L1 and L2 penalty functions to achieve logistic,
    ridge, Lasso, and everything in between. We also explore the solver (that is,
    optimizer) method and how to set it to use L-BFGS optimization, auto optimizer
    selection, and so on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将学习如何使用代数方程通过Scala代码和RDD从头开始实现线性回归，以便了解数学和为什么我们需要迭代优化方法来估计大规模回归系统的解决方案。其次，我们探讨**广义线性模型**（**GLM**）及其各种统计分布家族和链接函数，同时强调当前实现中仅限于4,096个参数的限制。第三，我们解决**线性回归模型**（**LRM**）以及如何使用弹性网参数化来混合和匹配L1和L2惩罚函数，以实现逻辑回归、岭回归、Lasso等。我们还探讨了求解器（即优化器）方法以及如何设置它以使用L-BFGS优化、自动优化选择等。
- en: After exploring the GLM and linear regression recipes, we proceed to provide
    recipes for more exotic regression/classification methods such as isotonic regression,
    multilayer perceptron (that is, form of neuron net), One-vs-Rest, and survival
    regression to demonstrate Spark 2.0's power and completeness to deal with cases
    that are not addressed by linear techniques. With the increased risks in the financial
    world in the early 21^(st) century and new advancements in genome, Spark 2.0 also
    pulls together four important methods (isotonic regression, multilayer perceptron,
    One-vs-Rest, and survival regression or parametric ATF) in an easy to use machine
    learning library. The parametric ATF method at scale should be of particular interest
    to financial, data scientist, or actuarial professionals alike.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索GLM和线性回归配方之后，我们继续提供更多外来的回归/分类方法的配方，例如等保回归、多层感知器（即神经元网络的形式）、一对多和生存回归，以展示Spark
    2.0处理线性技术无法解决的情况的能力和完整性。随着21世纪初金融世界风险的增加和基因组的新进展，Spark 2.0还将四种重要方法（等保回归、多层感知器、一对多和生存回归或参数AFT）整合到一个易于使用的机器学习库中。规模化的参数AFT方法应该特别受到金融、数据科学家或精算专业人士的关注。
- en: Even though some of these methods such as `LinearRegression()` API, have theoretically
    been available since 1.3x+, it is important to note that Spark 2.0 pulls all of
    them together in an easy-to-use and maintainable API (that is, backward compatibility)
    in a glmnet R-like manner as they move the RDD-based regression API into maintenance
    mode. The L-BFGS optimizer and normal equations take a front seat while SGD is
    available in RDD-based APIs for backward compatibility.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些方法，比如`LinearRegression()` API，从理论上来说自1.3x+版本就已经可用，但重要的是要注意，Spark 2.0将它们全部整合到一个易于使用和可维护的API中（即向后兼容），以glmnet
    R的方式移动基于RDD的回归API到维护模式。L-BFGS优化器和正规方程占据主导地位，而SGD在RDD-based APIs中可用以实现向后兼容。
- en: 'Elastic net is the preferred method that can not only deal with L1 (Lasso regression)
    and L2 (ridge regression) in absolute terms prefered method for regularization,
    but also provide a dial-like mechanism that enables the user to fine-tune the
    penalty function (parameter shrinkage versus selection). While we recall using
    the elastic net function in 1.4.2, Spark 2.0 pulls it all together without the
    need to deal with each individual API for parameter tuning (important when selecting
    a model dynamically based on the latest data). As we start diving into the recipes,
    we strongly encourage the user to explore various parameter settings `setElasticNetParam()`
    and `setSolver()` configurations to master these powerful APIs. It is important
    not to mix the penalty function `setElasticNetParam(value: Double)` (L1 , L2,
    OLs, elastic net: linearly mixed L1/L2), which are regular or model penalty schemes
    with optimization (normal, L-BFGS, auto, and so on) techniques that are related
    to cost function optimization techniques.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '弹性网络是首选方法，不仅可以绝对处理L1（Lasso回归）和L2（岭回归）的正则化方法，还可以提供类似拨盘的机制，使用户能够微调惩罚函数（参数收缩与选择）。虽然我们在1.4.2中使用了弹性网功能，但Spark
    2.0将所有内容整合在一起，无需处理每个单独的API进行参数调整（根据最新数据动态选择模型时很重要）。当我们开始深入研究这些配方时，我们强烈鼓励用户探索各种参数设置`setElasticNetParam()`和`setSolver()`配置，以掌握这些强大的API。重要的是不要混淆惩罚函数`setElasticNetParam(value:
    Double)`（L1，L2，OLs，弹性网：线性混合L1/L2），这些是正则化或模型惩罚方案与与成本函数优化技术相关的优化（正常，L-BFGS，自动等）技术。'
- en: It is critical to note that the RDD-based regressions are still very important
    since there are a lot of current ML implementation systems that rely heavily on
    the previous API regime and its SGD optimizer. Please see the next chapter for
    complete treatment with teaching notes covering RDD-based regressions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，基于RDD的回归仍然非常重要，因为有很多当前的ML实现系统严重依赖于以前的API体系及其SGD优化器。请参阅下一章，了解基于RDD的回归的完整处理和教学笔记。
- en: Fitting a linear regression line to data the old fashioned way
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用传统的方法将线性回归线拟合到数据
- en: In this recipe, we use RDDs and a closed form formula to code a simple linear
    equation from scratch. The reason we use this as the first recipe is to demonstrate
    that you can always implement any given statistical learning algorithm via the
    RDDs to achieve computational scale using Apache Spark.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们使用RDD和封闭形式公式从头开始编写一个简单的线性方程。我们之所以将这个作为第一个配方，是为了演示您可以始终通过RDD实现任何给定的统计学习算法，以实现使用Apache
    Spark的计算规模。
- en: How to do it...
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包，以便`SparkSession`获得对集群的访问，以及`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Initialize a `SparkSession` specifying configurations with the builder pattern
    thus making an entry point available for the Spark cluster:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建模式初始化`SparkSession`，指定配置，从而使Spark集群的入口点可用：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Set output level to `ERROR` to reduce Spark''s output:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的输出：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We create two arrays representing the dependent (that is, `y`) and an independent
    variable (that is, `x`):'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建两个数组，表示因变量（即`y`）和自变量（即`x`）：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We use `sc.parallelize(x)` to transform the two arrays to RDDs:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`sc.parallelize(x)`将两个数组转换为RDD：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this step, we demonstrate the `zip()` method of an RDD that creates pairs
    of dependent/independent tuples *(y,x)* from the two RDDs. We introduce this function
    since you must often learn to work with pairs in machine learning algorithms:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们演示了RDD的`zip()`方法，它从两个RDD中创建因变量/自变量元组*(y,x)*。我们介绍这个函数，因为您经常需要学习如何在机器学习算法中使用成对工作：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To make sure we understand the `zip()` functionality, let''s take a look at
    the output, but make sure you include the `collect()` or some other form of action
    to make sure the data is presented in order. If we do not use an action method,
    the output from RDDs will be random:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保我们理解`zip()`功能，让我们来看一下输出，但一定要包括`collect()`或其他形式的操作，以确保数据按顺序呈现。如果我们不使用操作方法，RDD的输出将是随机的：
- en: '![](img/00105.gif)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.gif)'
- en: 'This is an important step that demonstrates how to iterate, access, and compute
    on each individual member of the pair. In order to compute the regression line,
    we need to compute sum, product, and averages (that is, *sum(x)*, *sum(y)*, and
    *sum (x * y)*). The `map(_._1).sum()` function is a mechanism in which the RDD
    pairs are iterated upon, but only the first elements are considered:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一个重要的步骤，演示了如何迭代、访问和计算每个成员。为了计算回归线，我们需要计算和、乘积和平均值（即*sum(x)*、*sum(y)*和*sum (x
    * y)*）。`map(_._1).sum()`函数是一种机制，RDD对被迭代，但只考虑第一个元素：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this step we continue with computing the averages of individual RDD''s pair
    member along with their product. These individual computations (that is, *mean(x)*,
    *mean(y)*, and *mean(x*y)*), along with mean squared, will be used to compute
    the slope and intercept of the regression line. While we could have computed the
    mean from the previous statistics in the previous steps manually, we should make
    sure we are familiar with methods that are available intrinsically via an RDD:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们继续计算每个RDD对成员的平均值以及它们的乘积。这些单独的计算（即*mean(x)*、*mean(y)*和*mean(x*y)*），以及平均平方，将用于计算回归线的斜率和截距。虽然我们可以从前面的统计数据中手动计算平均值，但我们应该确保熟悉RDD内在可用的方法：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is the final step, in which we compute the mean of `x` and `y` squared:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是最后一步，我们计算`x`和`y`的平方的平均值：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We print the statistic for completeness:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印统计信息以供参考：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We compute the `numerator` and `denominator` for the formula:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算公式的`分子`和`分母`：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We finally compute the slope of the regression line:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们最终计算回归线的斜率：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We now calculate the intercept and print. If you do not want the intercept
    (intercept to be set to `0`), then the formula for the slope needs to be slightly
    modified. You can look for more details in other sources such as the internet
    and find the required equation:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们计算截距并打印。如果你不想要截距（截距设置为`0`），那么斜率的公式需要稍作修改。你可以在其他来源（如互联网）中寻找更多细节并找到所需的方程：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using the slope and intercept, we write the regression line equation as follows:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用斜率和截距，我们将回归线方程写成如下形式：
- en: '[PRE14]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We declared two Scala arrays, parallelized them into two RDDs that are separate
    vectors of `x()` and `y()`. We then used the `zip()` method from the RDD API to
    produce a paired (that is, zipped) RDD. It results in an RDD in which each member
    is an *(x , y)* pair. We then proceed to calculate the mean, sum, and so on, and
    apply the closed form formula as described to find the intercept and slope for
    the regression line.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声明了两个Scala数组，将它们并行化为两个分开的`x()`和`y()`的RDD，然后使用RDD API中的`zip()`方法产生了一个成对的（即，压缩的）RDD。它产生了一个RDD，其中每个成员都是一个*(x，y)*对。然后我们继续计算均值，总和等，并应用上述封闭形式的公式来找到回归线的截距和斜率。
- en: In Spark 2.0, the alternative would have been to use the GLM API out of the
    box. It is worth mentioning that the maximum number of parameters for a closed
    normal form scheme supported by GLM is limited to 4,096.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0中，另一种选择是直接使用GLM API。值得一提的是，GLM支持的封闭正态形式方案的最大参数数量限制为4,096。
- en: 'We used a closed form formula to demonstrate that a regression line associated
    with a set of numbers *(Y1, X1), ..., (Yn, Xn)* is simply the line that minimizes
    the sum of the square errors. In a simple regression equation, the line is as
    follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了封闭形式的公式来证明与一组数字（*Y1，X1），...，（Yn，Xn）*相关联的回归线简单地是最小化平方误差和的线。在简单的回归方程中，该线如下：
- en: Slope of the regression line ![](img/00106.jpeg)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归线的斜率 ![](img/00106.jpeg)
- en: Offset of the regression line ![](img/00107.jpeg)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归线的偏移 ![](img/00107.jpeg)
- en: The equation for the regression line ![](img/00108.jpeg)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归线的方程 ![](img/00108.jpeg)
- en: A regression line is simply the best fit line that minimizes the sum of the
    square error. For a set of points (dependent variable, independent variable),
    there are many lines that can pass through these points and capture the general
    linear relationship, but only one of those lines is the line that minimizes all
    the errors from such a fit.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 回归线简单地是最小化平方误差和的最佳拟合线。对于一组点（因变量，自变量），有许多直线可以穿过这些点并捕捉到一般的线性关系，但只有其中一条线是最小化所有拟合误差的线。
- en: 'For the example, we presented the line *Y = 1.21 + .9153145 * X*. Shown in
    the following figure is such a line and we computed the slope and the offset with
    a closed form formula. The linear model depicted by the linear equation of a line
    represents our best linear model (*slope=.915345*, *intercept= 1.21*) for the
    given data using closed form formulas:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们呈现了线*Y = 1.21 + .9153145 * X*。下图显示了这样一条线，我们使用封闭形式的公式计算了斜率和偏移。线性模型由线性方程表示，代表了我们使用封闭形式公式得到的给定数据的最佳线性模型（*斜率=.915345*，*截距=1.21*）：
- en: '![](img/00109.gif)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.gif)'
- en: 'The data points plotted in the preceding figure are as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中绘制的数据点如下：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There's more...
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: It should be noted that not all regression forms have a closed form formula
    or become very inefficient (that is, impractical) with a large number of parameters
    on large datasets - this is the reason we use optimization techniques such as
    SGD or L-BFGS.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，并非所有的回归形式都有封闭形式的公式，或者在大型数据集上变得非常低效（即，不切实际）-这就是我们使用SGD或L-BFGS等优化技术的原因。
- en: It is critical to recall from the previous recipes that you should make sure
    you cache any RDD or data structure associated with machine learning algorithms
    to avoid lazy instantiation due to the way Spark optimizes and maintains lineage
    (that is, lazy instantiation).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的教程中，你应该确保缓存与机器学习算法相关的任何RDD或数据结构，以避免由于Spark优化和维护血统（即，延迟实例化）的方式而导致的延迟实例化。
- en: See also
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'We recommend a book from Stanford University, which can be downloaded from
    the following site for free. It is a classic and a must-read whether you are a
    new or advanced practitioner in the field:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推荐一本来自斯坦福大学的书，可以从以下网站免费下载。无论你是新手还是高级从业者，这都是一本经典必读的书：
- en: The Elements of Statistical Learning, Data Mining, Inference, and Prediction,
    Second Edition, by Hastie, Tibshirani, and Friedman (2009). Springer-Verlag ([http://web.stanford.edu/~hastie/ElemStatLearn/](http://web.stanford.edu/~hastie/ElemStatLearn/)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 《统计学习的要素，数据挖掘，推断和预测，第二版》，Hastie，Tibshirani和Friedman（2009）。Springer-Verlag ([http://web.stanford.edu/~hastie/ElemStatLearn/](http://web.stanford.edu/~hastie/ElemStatLearn/))。
- en: Generalized linear regression in Spark 2.0
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中的广义线性回归
- en: This recipe covers the **generalized regression model** (**GLM**) implementation
    in Spark 2.0\. There is a great parallel between this `GeneralizedLinearRegression`
    in Spark 2.0 and `glmnet` implementation in R. This API is a welcome addition
    that allows you to select and set both distribution family (for example, Gaussian)
    and link functions (for example, inverse log) with a coherent and well-designed
    API.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程涵盖了Spark 2.0中的**广义回归模型**（**GLM**）实现。Spark 2.0中的`GeneralizedLinearRegression`与R中的`glmnet`实现之间存在很大的相似性。这个API是一个受欢迎的补充，允许你选择和设置分布族（例如，高斯）和链接函数（例如，反对数）的API。
- en: How to do it...
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: We use a housing dataset from the UCI machine library depository.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用了UCI机器库存储中的房屋数据集。
- en: 'Download the entire dataset from the following URLs:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下网址下载整个数据集：
- en: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
- en: The dataset is comprised of 14 columns with the first 13 columns being the independent
    variables (that is, features) that try to explain the median price (that is, last
    column) of an owner-occupied house in Boston, USA.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由14列组成，前13列是自变量（即特征），试图解释美国波士顿自住房的中位价格（即最后一列）。
- en: 'We have chosen and cleaned the first eight columns as features. We use the
    first 200 rows to train and predict the median price:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经选择并清理了前八列作为特征。我们使用前200行来训练和预测房价的中位数：
- en: '**CRIM**: Per capita crime rate by town'
  id: totrans-84
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CRIM**：按城镇人均犯罪率'
- en: '**ZN**: Proportion of residential land zoned for lots over 25,000 sq.ft'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ZN**：超过25,000平方英尺的住宅用地比例'
- en: '**INDUS**: Proportion of non-retail business acres per town'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**INDUS**：每个城镇的非零售业务面积比例'
- en: '**CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CHAS**：查尔斯河虚拟变量（如果地块与河流相接则为1；否则为0）'
- en: '**NOX**: Nitric oxide concentration (parts per 10 million)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NOX**：一氧化氮浓度（每千万份之一）'
- en: '**RM**: Average number of rooms per dwelling'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RM**：每个住宅的平均房间数'
- en: '**AGE**: Proportion of owner-occupied units built prior to 1940'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AGE**：1940年前建成的自住单位比例'
- en: 'Please use the `housing8.csv` file and make sure you move it to the following
    directory:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请使用`housing8.csv`文件，并确保将其移动到以下目录：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中开始一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`SparkSession`所需的必要包，以便访问集群和`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Initialize a `SparkSession` specifying configurations to gain access to the
    Spark cluster:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`SparkSession`指定配置以访问Spark集群：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We need to import implicits for data conversion routines:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要导入数据转换例程的隐式：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we load housing data into a dataset:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将住房数据加载到数据集中：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let''s parse the housing data and convert it into label points:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们解析住房数据并将其转换为标签点：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now display the loaded data using the following code:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用以下代码显示加载的数据：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as shown here:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/00110.jpeg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00110.jpeg)'
- en: 'Next, we configure a generalized linear regression algorithm for generating
    a new model:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们为生成一个新模型配置了一个广义线性回归算法：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Feel free to experiment with different parameters for better fit.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请随意尝试不同的参数以获得更好的拟合效果。
- en: 'We fit the model to the housing data:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将模型拟合到住房数据：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we retrieve the summary data to judge the accuracy of the model:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们检索摘要数据以判断模型的准确性：
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we print out the summary statistics:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印出摘要统计信息：
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We close the program by stopping the `SparkSession`:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止`SparkSession`来关闭程序：
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How it works...
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we showed a generalized linear regression algorithm in action.
    We began by loading and parsing a CSV file into a dataset. Next, we created a
    generalized linear regression algorithm and generated a new model by passing our
    dataset to the `fit()` method. Once the fit operation was completed, we retrieved
    summary statistics from the model and displayed computed values to reconcile accuracy.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们展示了广义线性回归算法的运行情况。我们首先将CSV文件加载和解析为数据集。接下来，我们创建了一个广义线性回归算法，并通过将数据集传递给`fit()`方法来生成一个新模型。完成拟合操作后，我们从模型中检索摘要统计信息，并显示计算出的值以调整准确性。
- en: In this example, we explored fitting the data with a *Gaussian* distribution
    and *Identity*, but there are many more configurations that we can use to solve
    a specific regression fit, which are explained in the next section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们探索了使用*高斯*分布和*身份*拟合数据，但还有许多其他配置可以用来解决特定的回归拟合问题，这些将在下一节中解释。
- en: There's more...
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The GLM in Spark 2.0 is a general-purpose regression model that can support
    many configurations. We are impressed with the number of families available as
    of the initial release of Spark 2.0.0.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0中的GLM是一个通用的回归模型，可以支持许多配置。我们对Spark 2.0.0初始版本提供的众多系列印象深刻。
- en: 'It is important to note as of Spark 2.0.2:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，截至Spark 2.0.2：
- en: The maximum number of parameters for a regression is currently limited to 4,096
    max.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前回归的最大参数数量限制为4,096个。
- en: The only optimization (that is, solver) currently supported is **iteratively
    reweighted least squares** (**IRLS**), which is also the default solve.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前唯一支持的优化（即求解器）是**迭代重新加权最小二乘法**（**IRLS**），这也是默认求解器。
- en: When you set the solver to *auto*, it defaults to IRLS.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您将求解器设置为*auto*时，它默认为IRLS。
- en: The `setRegParam()` sets the regularization parameter for L2 regularization.
    The regularization term is *0.5 * regParam * L2norm(coefficients)^2* per Spark
    2.0 documentation - make sure you understand the implications.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setRegParam()`设置L2正则化的正则化参数。根据Spark 2.0文档，正则化项为*0.5 * regParam * L2norm(coefficients)^2*
    - 请确保您理解其影响。'
- en: If you are not sure how to handle the distribution fitting, we highly recommend
    one of our favorite books, *Handbook of Fitting Statistical Distributions with
    R*, that has served us well when modeling agricultural commodities such as CBOT
    Wheat, which has a reverse volatility smile curve (very different from equities).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定如何处理分布拟合，我们强烈推荐我们最喜欢的书之一，*用R拟合统计分布手册*，在建模芝加哥期货交易所小麦等农产品时为我们提供了很好的帮助，它具有反向波动率笑曲线（与股票非常不同）。
- en: 'Configuration and available options are as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 配置和可用选项如下：
- en: '![](img/00111.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00111.jpeg)'
- en: Be sure to experiment with different families and link functions to make sure
    your assumption of the underlying distribution is correct.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要尝试不同的族和链接函数，以确保您对基础分布的假设是正确的。
- en: See also
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for `GeneralizedLinearRegression()` is available at the following
    link:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`GeneralizedLinearRegression()`的文档可在以下链接找到：'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.GeneralizedLinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.GeneralizedLinearRegression)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.GeneralizedLinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.GeneralizedLinearRegression)'
- en: 'Some of the important API calls within `GeneralizedLinearRegression`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`GeneralizedLinearRegression`中的一些重要API调用：'
- en: '`def **setFamily**(value: String): GeneralizedLinearRegression.this.type`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setFamily**(value: String): GeneralizedLinearRegression.this.type`'
- en: '`def **setLink**(value: String): GeneralizedLinearRegression.this.type`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setLink**(value: String): GeneralizedLinearRegression.this.type`'
- en: '`def **setMaxIter**(value: Int): GeneralizedLinearRegression.this.type`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setMaxIter**(value: Int): GeneralizedLinearRegression.this.type`'
- en: '`def **setRegParam**(value: Double): GeneralizedLinearRegression.this.type`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setRegParam**(value: Double): GeneralizedLinearRegression.this.type`'
- en: '`def **setSolver**(value: String): GeneralizedLinearRegression.this.type`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setSolver**(value: String): GeneralizedLinearRegression.this.type`'
- en: '`def **setFitIntercept**(value: Boolean): GeneralizedLinearRegression.this.type`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setFitIntercept**(value: Boolean): GeneralizedLinearRegression.this.type`'
- en: 'The solver is currently IRLS; a quick reference can be found at the following
    link:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 求解器目前是IRLS；快速参考可在以下链接找到：
- en: '[https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares)'
- en: 'For complete understanding of the new approach with GLM and linear regression
    in Spark 2.0+, please be sure to consult and understand CRAN glmnet implementation
    in R:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全理解Spark 2.0+中GLM和线性回归的新方法，请务必参考并了解R中CRAN glmnet的实现：
- en: Main page is available at [https://cran.r-project.org/web/packages/glmnet/index.html](https://cran.r-project.org/web/packages/glmnet/index.html)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主页可在[https://cran.r-project.org/web/packages/glmnet/index.html](https://cran.r-project.org/web/packages/glmnet/index.html)找到
- en: User guide is available at [https://cran.r-project.org/web/packages/glmnet/glmnet.pdf](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户指南可在[https://cran.r-project.org/web/packages/glmnet/glmnet.pdf](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf)找到
- en: Linear regression API with Lasso and L-BFGS in Spark 2.0
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中带有Lasso和L-BFGS的线性回归API
- en: In this recipe, we will demonstrate the use of Spark 2.0's `LinearRegression()`
    API to showcase a unified/parameterized API to tackle the linear regression in
    a comprehensive way capable of extension without backward-compatibility issues
    of an RDD-based named API. We show how to use the `setSolver()` to set the optimization
    method to first-order memory-efficient L-BFGS, which can deal with numerous amount
    of parameters (that is, especially in sparse configuration) with ease.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将演示如何使用Spark 2.0的`LinearRegression()` API来展示一个统一/参数化的API，以全面的方式处理线性回归，能够在不会出现RDD命名API的向后兼容问题的情况下进行扩展。我们展示如何使用`setSolver()`将优化方法设置为一阶内存高效的L-BFGS，它可以轻松处理大量参数（即在稀疏配置中）。
- en: In this recipe, the `.setSolver()` is set to `lbgfs`, which makes the L-BFGS
    (see RDD-based regression for more detail) the selected optimization method. The
    `.setElasticNetParam()` is not set, so the default of `0` remains in effect, which
    makes this a Lasso regression.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，`.setSolver()`设置为`lbgfs`，这使得L-BFGS（详见基于RDD的回归）成为选择的优化方法。`.setElasticNetParam()`未设置，因此默认值`0`仍然有效，这使得这是一个Lasso回归。
- en: How to do it...
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: We use a housing dataset from the UCI machine library depository.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用UCI机器库存储中的住房数据集。
- en: 'Download the entire data set from the following URLs:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下链接下载整个数据集：
- en: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
- en: The dataset is comprised of 14 columns with the first 13 columns being independent
    variables (that is, features) that try to explain the median price (that is, last
    column) of an owner-occupied house in Boston, USA.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由14列组成，前13列是独立变量（即特征），试图解释美国波士顿自住房的中位价格（即最后一列）。
- en: 'We have chosen and cleaned the first eight columns as features. We use the
    first 200 rows to train and predict the median price:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择并清理了前八列作为特征。我们使用前200行来训练和预测中位价格：
- en: '**CRIM**: Per capita crime rate by town'
  id: totrans-162
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CRIM**：按城镇划分的人均犯罪率'
- en: '**ZN**: Proportion of residential land zoned for lots over 25,000 sq.ft.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ZN**：用于25,000平方英尺以上地块的住宅用地比例'
- en: '**INDUS**: Proportion of non-retail business acres per town'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**INDUS**：每个城镇的非零售业务土地比例'
- en: '**CHAS**: Charles River dummy variable (`= 1` if tract bounds river; `0` otherwise)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CHAS**：查尔斯河虚拟变量（如果地块与河流相接，则为`1`；否则为`0`）'
- en: '**NOX**: Nitric oxide concentration (parts per 10 million)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NOX**：一氧化氮浓度（每千万分之一）'
- en: '**RM**: Average number of rooms per dwelling'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RM**：每个住宅的平均房间数'
- en: '**AGE**: Proportion of owner-occupied units built prior to 1940'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AGE**：1940年前建成的自住单位比例'
- en: 'Please use the `housing8.csv` file and make sure you move it to the following
    directory:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请使用`housing8.csv`文件，并确保将其移动到以下目录：
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Import the necessary packages for the `SparkSession` to gain access to the
    cluster and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包，以便`SparkSession`访问集群和`log4j.Logger`减少Spark产生的输出量：
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Initialize a `SparkSession` specifying configurations to gain access to the
    Spark cluster:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`SparkSession`，指定配置以访问Spark集群：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We need to import implicits for data conversion routines:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要导入数据转换例程的隐式：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we load the housing data into a dataset:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将房屋数据加载到数据集中：
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s parse the housing data and convert it into label points:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们解析房屋数据并将其转换为标签点：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now display the loaded data:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在显示加载的数据：
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as shown here:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/00112.jpeg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00112.jpeg)'
- en: 'Next, we configure a linear regression algorithm for generating a model:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们配置线性回归算法以生成模型：
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now we fit the model to the housing data:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将模型拟合到房屋数据中：
- en: '[PRE40]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we retrieve summary data to reconcile the accuracy of the model:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们检索摘要数据以调和模型的准确性：
- en: '[PRE41]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, we print out the summary statistics:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印出摘要统计信息：
- en: '[PRE42]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We close the program by stopping the `SparkSession`:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止`SparkSession`来关闭程序：
- en: '[PRE43]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works...
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we use the housing data again to demonstrate the Spark 2.0 `LinearRegression()` API
    using the L-BFGS optimization option. We read the file in, parse the data, and
    select specific columns for the regression. We keep the recipe short by accepting
    default parameters, but set number of iterations (for convergence to a solution)
    and optimization method to `lbfgs` before running the `.fit()` method. We then
    proceed to output a couple of quick metrics (that is, MSE and RMSE) for demonstration
    only. We show how to implement/compute these metrics yourself with RDD. Using
    Spark 2.0 native facilities/metrics and RDDs-based regression recipes, we show
    how Spark can do these metrics out of the box now, which is testimony to how far
    we have come from Spark 1.0.1!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们再次使用房屋数据来演示Spark 2.0的`LinearRegression()`API，使用L-BFGS优化选项。我们读取文件，解析数据，并选择回归的特定列。我们通过接受默认参数来保持示例简短，但在运行`.fit()`方法之前，将迭代次数（用于收敛到解决方案）和优化方法设置为`lbfgs`。然后，我们继续输出一些快速指标（即MSE和RMSE）仅用于演示。我们展示了如何使用RDD自己实现/计算这些指标。使用Spark
    2.0的本机功能/指标和基于RDD的回归示例，我们展示了Spark现在可以直接完成这些指标，这证明了我们从Spark 1.0.1走过了多远！
- en: Using Newton's optimization technique (for example, `lbfgs`) for small number
    of columns is an overkill, which will be demo later in this book to enable the
    readers to use these recipes on large datasets in real-world settings (for example,
    typical cancer/genome data readily available from sources mentioned in [Chapter
    1](part0027.html#PNV60-4d291c9fed174a6992fd24938c2f9c77), *Practical Machine Learning
    with Spark Using Scala*).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于少量列使用牛顿优化技术（例如`lbfgs`）是一种过度，稍后在本书中进行演示，以便读者能够在实际环境中的大型数据集上使用这些示例（例如，从[第1章](part0027.html#PNV60-4d291c9fed174a6992fd24938c2f9c77)中提到的典型癌症/基因组数据）。
- en: There's more...
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Elastic net (contributed by DB Tsai and others) and evangelized by Alpine Labs
    showed up in our radar starting with Spark 1.4 and 1.5, which is now the de facto
    technique in Spark 2.0.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网（由DB Tsai和其他人贡献）和由Alpine Labs推广的技术从Spark 1.4和1.5开始引起了我们的关注，现在已成为Spark 2.0中的事实标准技术。
- en: To level set, elastic net is a linear combination of L1 and L2 penalty. It can
    be modeled conceptually as a dial that can decide how much of L1 and how much
    of L2 to include in the penalty (Shrinkage versus Selection).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了水平设置，弹性网是L1和L2惩罚的线性组合。它可以在概念上被建模为一个可以决定在惩罚中包含多少L1和多少L2的旋钮（收缩与选择）。
- en: We want to stress that we can now select between the type of regression via
    the parameter setting rather than named APIs. This is an important departure from
    RDD-based APIs (that is, now in maintenance mode) that we demonstrate later in
    this chapter.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想强调的是，现在我们可以通过参数设置来选择回归类型，而不是命名的API。这是与基于RDD的API（即现在处于维护模式）的重要分歧，我们稍后在本章中进行演示。
- en: The following table provides a quick cheat sheet for setting parameters to select
    between Lasso, Ridge, OLS, and elastic net.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了一个快速设置参数的备忘单，以在Lasso、Ridge、OLS和弹性网之间进行选择。
- en: 'Please see the following table `setElasticNetParam(value: Double)`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '请参阅以下表格`setElasticNetParam(value: Double)`：'
- en: '| **Regression Type** | **Penalty** | **Parameter** |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **回归类型** | **惩罚** | **参数** |'
- en: '| Lasso | L1 | 0 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Lasso | L1 | 0 |'
- en: '| Ridge | L2 | 1 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Ridge | L2 | 1 |'
- en: '| Elastic net | L1 + L2 | 0.0 < alpha < 1.0 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 弹性网 | L1 + L2 | 0.0 < alpha < 1.0 |'
- en: '| OLS | Ordinary least square | None |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| OLS | 普通最小二乘法 | 无 |'
- en: 'It is crucial to understand how regularization is controlled via an elastic
    net parameter (corresponding to Alpha) described in this following brief treatment:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下简要处理来了解正则化是如何通过弹性网参数（对应于Alpha）来控制的：
- en: Simple: [https://en.wikipedia.org/wiki/Elastic_net_regularization](https://en.wikipedia.org/wiki/Elastic_net_regularization)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单：[https://en.wikipedia.org/wiki/Elastic_net_regularization](https://en.wikipedia.org/wiki/Elastic_net_regularization)
- en: Complete: [http://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf](http://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整：[http://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf](http://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf)
- en: Using genome data: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3232376/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3232376/)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基因组数据：[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3232376/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3232376/)
- en: See also
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `LinearRegression()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinearRegression()`的文档：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)'
- en: Be sure to consult the actual source code since it extends the *Regressor* itself: [https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/regression/LinearRegression.scala](https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/regression/LinearRegression.scala)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一定要查看实际的源代码，因为它扩展了* Regressor*本身：[https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/regression/LinearRegression.scala](https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/regression/LinearRegression.scala)
- en: 'Some of the important API calls within `LinearRegression`:'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinearRegression`中一些重要的API调用：'
- en: '`def setElasticNetParam(value: Double): LinearRegression.this.type`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def setElasticNetParam(value: Double): LinearRegression.this.type`'
- en: '`def **setRegParam**(value: Double): LinearRegression.this.type`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setRegParam**(value: Double): LinearRegression.this.type`'
- en: '`def **setSolver**(value: String): LinearRegression.this.type`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setSolver**(value: String): LinearRegression.this.type`'
- en: '`def **setMaxIter**(value: Int): LinearRegression.this.type`'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setMaxIter**(value: Int): LinearRegression.this.type`'
- en: '`def **setFitIntercept**(value: Boolean): LinearRegression.this.type`'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setFitIntercept**(value: Boolean): LinearRegression.this.type`'
- en: An important aspect of Spark ML is its simple yet exceptionally powerful API
    set that allows scaling to billions of rows at very little extra effort by the
    developer on an existing cluster. You will be surprised the scale at which Lasso
    can be used to discover relevant feature sets while L-BFGS optimization (that
    does not require a direct hessian matrix presence) can handle a tremendous number
    of features with ease. The details of the `updater` implementation for LBFGS in
    Spark 2.0 source code is beyond the scope of this book.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML的一个重要方面是其简单而异常强大的API集，它允许开发人员在现有集群上轻松扩展到数十亿行，而几乎不需要额外的工作。您会惊讶于Lasso可以用于发现相关特征集的规模，而L-BFGS优化（不需要直接的海森矩阵存在）可以轻松处理大量特征。Spark
    2.0源代码中LBFGS的`updater`实现的细节超出了本书的范围。
- en: We will cover the optimizations that relate to these ML algorithms in a follow-up
    chapter due to their complexity.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其复杂性，我们将在后续章节中介绍与这些ML算法相关的优化。
- en: Linear regression API with Lasso and 'auto' optimization selection in Spark
    2.0
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中具有Lasso和'auto'优化选择的线性回归API
- en: In this recipe, we build on the previous recipe `LinearRegression` by selecting
    LASSO regression explicitly via the `setElasticNetParam(0.0)` while letting Spark
    2.0 pick the optimization on its own using `setSolver('auto')`*. *We remind again
    that the RDD-based regression API is now in maintenance mode and this is the preferred
    method going forward.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们通过显式选择LASSO回归`setElasticNetParam(0.0)`来构建上一个配方`LinearRegression`，同时让Spark
    2.0使用`setSolver('auto')`自行选择优化。*我们再次提醒，基于RDD的回归API现在处于维护模式，这是未来的首选方法。
- en: How to do it...
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: We use a housing data set from the UCI machine library depository..
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用UCI机器库存储的住房数据集。
- en: 'Download the entire data set from the following URLs:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下网址下载整个数据集：
- en: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
- en: The dataset is comprised of 14 columns with the first 13 columns being the independent
    variables (that is, features) that try to explain the median price (that is, last
    column) of an owner-occupied house in Boston, USA.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由14列组成，前13列是独立变量（即特征），试图解释美国波士顿自有住房的中位价格（即最后一列）。
- en: 'We have chosen and cleaned the first eight columns as features. We use the
    first 200 rows to train and predict the median price:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经选择并清理了前八列作为特征。我们使用前200行来训练和预测中位数价格：
- en: '**CRIM:** Per capita crime rate by town'
  id: totrans-238
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CRIM:** 按城镇划分的人均犯罪率'
- en: '**ZN:** Proportion of residential land zoned for lots over 25,000 sq.ft.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ZN:** 用于超过25,000平方英尺的住宅用地比例'
- en: '**INDUS:** Proportion of non-retail business acres per town'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**INDUS:** 每个城镇的非零售业务面积比例'
- en: '**CHAS:** Charles River dummy variable (`= 1` if tract bounds river; `0` otherwise)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CHAS:** 查尔斯河虚拟变量（如果地块边界河流，则为`1`；否则为`0`）'
- en: '**NOX:** Nitric oxide concentration (parts per 10 million)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NOX:** 一氧化氮浓度（每1000万份之一）'
- en: '**RM:** Average number of rooms per dwelling'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RM:** 每个住宅的平均房间数'
- en: '**AGE**: Proportion of owner-occupied units built prior to 1940'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AGE**: 1940年前建造的自有住房的比例'
- en: 'Please use the `housing8.csv` file and make sure you move it to the following
    directory:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请使用`housing8.csv`文件，并确保将其移动到以下目录：
- en: '[PRE44]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中开始一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE45]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Import the necessary packages for the `SparkSession` to gain access to the
    cluster and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包，以便`SparkSession`访问集群和`Log4j.Logger`减少Spark产生的输出量：
- en: '[PRE46]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE47]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Initialize a `SparkSession` specifying configurations to gain access to the
    Spark cluster:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个`SparkSession`，指定配置以访问Spark集群：
- en: '[PRE48]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We need to import implicits for data conversion routines:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要导入数据转换例程的隐式：
- en: '[PRE49]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we load the housing data into a dataset:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将住房数据加载到数据集中：
- en: '[PRE50]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let''s parse the housing data and convert it into label points:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们解析住房数据并将其转换为标签点：
- en: '[PRE51]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now display the loaded data:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在显示加载的数据：
- en: '![](img/00113.jpeg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00113.jpeg)'
- en: 'Next, we configure a linear regression algorithm for generating a model:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们配置一个线性回归算法来生成模型：
- en: '[PRE52]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now we fit the model to the housing data:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将模型拟合到住房数据中：
- en: '[PRE53]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, we retrieve summary data to reconcile the accuracy of the model:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们检索摘要数据以调和模型的准确性：
- en: '[PRE54]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Finally, we print out the summary statistics:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印摘要统计信息：
- en: '[PRE55]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We close the program by stopping the `SparkSession`:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止`SparkSession`来关闭程序：
- en: '[PRE56]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: How it works...
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We read the housing data and load selected columns and use them to predict
    the price of a housing unit. We use the following code snippet to select the regression
    as LASSO and let Spark pick up the optimization on its own:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取住房数据并加载选定的列，并使用它们来预测住房单位的价格。我们使用以下代码片段选择回归为LASSO，并让Spark自行选择优化：
- en: '[PRE57]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We change the `setMaxIter()` to `1000` for demonstration purposes. The default
    setting is `100` out of the box.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`setMaxIter()`更改为`1000`以进行演示。默认设置为`100`。
- en: There's more...
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'While Spark does a very nice implementation of L-BFGS, please see the following
    links for a quick understanding of BFGS and its inner workings as it relates to
    this recipe:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Spark对L-BFGS有很好的实现，请参阅以下链接，快速了解BFGS及其内部工作原理，因为它与这个示例相关：
- en: Simple treatment of BFGS: [https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BFGS的简单处理：[https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)
- en: From the *Journal of Machine Learning Research* and also nice limited-memory
    BGFS treatment from a mathematical programming viewpoint: [http://www.jmlr.org/papers/volume14/hennig13a/hennig13a.pdf](http://www.jmlr.org/papers/volume14/hennig13a/hennig13a.pdf)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自*机器学习研究杂志*，也是一个很好的有限内存BGFS处理的数学编程视角：[http://www.jmlr.org/papers/volume14/hennig13a/hennig13a.pdf](http://www.jmlr.org/papers/volume14/hennig13a/hennig13a.pdf)
- en: Also see the RDD-based regression recipes for more details on LBGFS. The following
    links provide implementation details if you need to understand the BFGS techniques.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以查看基于RDD的回归示例，以了解有关LBGFS的更多详细信息。如果您需要了解BFGS技术的实现细节，请参考以下链接。
- en: This implementation in C language helps us to develop a solid understanding
    of the first-order optimization at code level: [http://www.chokkan.org/software/liblbfgs/](http://www.chokkan.org/software/liblbfgs/)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这个C语言实现帮助我们在代码级别开发对一阶优化的扎实理解：[http://www.chokkan.org/software/liblbfgs/](http://www.chokkan.org/software/liblbfgs/)
- en: The *cctbx* also provides good implementation details if you need to see more: [http://cctbx.sourceforge.net](http://cctbx.sourceforge.net)
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cctbx*还提供了很好的实现细节，如果您需要了解更多：[http://cctbx.sourceforge.net](http://cctbx.sourceforge.net)'
- en: Pretty good treatment from Harvard University on L-BFGS in R: [https://cran.r-project.org/web/packages/lbfgs/vignettes/Vignette.pdf](https://cran.r-project.org/web/packages/lbfgs/vignettes/Vignette.pdf)
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈佛大学关于R中L-BFGS的很好的处理：[https://cran.r-project.org/web/packages/lbfgs/vignettes/Vignette.pdf](https://cran.r-project.org/web/packages/lbfgs/vignettes/Vignette.pdf)
- en: See also
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `LinearRegression()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinearRegression()`的文档：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)'
- en: 'Documentation for BFGS and L-BFGS:'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BFGS和L-BFGS的文档：
- en: '[https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)'
- en: '[https://en.wikipedia.org/wiki/Limited-memory_BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Limited-memory_BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)'
- en: Linear regression API with ridge regression and 'auto' optimization selection
    in Spark 2.0
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中具有岭回归和“自动”优化选择的线性回归API
- en: In this recipe, we implement ridge regression using the `LinearRegression` interface.
    We use the elastic net parameter to set the appropriate value to a full L2 penalty,
    which in turn selects the ridge regression accordingly.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用`LinearRegression`接口实现岭回归。我们使用弹性网参数来设置适当的值以进行完整的L2惩罚，从而相应地选择岭回归。
- en: How to do it...
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: We use a housing data set from the UCI machine library depository.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用UCI机器库存储的住房数据集。
- en: 'Download the entire data set from the following URLs:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下网址下载整个数据集：
- en: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
- en: The dataset is comprised of 14 columns with the first 13 columns being the independent
    variables (that is, features) that try to explain the median price (that is, last
    column) of an owner-occupied house in Boston, USA.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由14列组成，前13列是独立变量（即特征），试图解释美国波士顿自住房的中位价格（即最后一列）。
- en: 'We have chosen and cleaned the first eight columns as features. We use the
    first 200 rows to train and predict the median price:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择并清理了前八列作为特征。我们使用前200行来训练和预测中位价格：
- en: '**CRIM**: Per capita crime rate by town'
  id: totrans-300
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
  zh: CRIM：按城镇计算的人均犯罪率
- en: '**ZN**: Proportion of residential land zoned for lots over 25,000 sq.ft.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZN：用于超过25,000平方英尺的地块的住宅用地比例
- en: '**INDUS**: Proportion of non-retail business acres per town'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: INDUS：每个城镇非零售业务英亩的比例
- en: '**CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CHAS：查尔斯河虚拟变量（如果地区与河流相接则为1；否则为0）
- en: '**NOX**: Nitric oxide concentration (parts per 10 million)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NOX：一氧化氮浓度（每千万分之一）
- en: '**RM**: Average number of rooms per dwelling'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RM：每个住宅的平均房间数
- en: '**AGE**: Proportion of owner-occupied units built prior to 1940'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AGE：1940年前建成的自住单位比例
- en: 'Please use the `housing8.csv` file and make sure you move it to the following
    directory:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请使用`housing8.csv`文件，并确保将其移动到以下目录：
- en: '[PRE58]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up package location where the program will reside:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE59]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以便`SparkSession`访问集群和`Log4j.Logger`减少Spark产生的输出量：
- en: '[PRE60]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE61]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Initialize a `SparkSession` specifying configurations to gain access to the
    Spark cluster:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`SparkSession`，指定配置以访问Spark集群：
- en: '[PRE62]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We need to import implicits for data conversion routines:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要导入数据转换例程的隐式：
- en: '[PRE63]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Next, we load the housing data into a dataset:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将房屋数据加载到数据集中：
- en: '[PRE64]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Let''s parse the housing data and convert it into label points:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们解析房屋数据并将其转换为标签点：
- en: '[PRE65]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now display the loaded data:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在显示加载的数据：
- en: '![](img/00114.jpeg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00114.jpeg)'
- en: 'Next, we configure a linear regression algorithm for generating a model:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们配置线性回归算法以生成模型：
- en: '[PRE66]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now, we fit the model to the housing data:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将模型拟合到房屋数据：
- en: '[PRE67]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, we retrieve the summary data to reconcile the accuracy of the model:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们检索摘要数据以调和模型的准确性：
- en: '[PRE68]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Finally, we print out the summary statistics:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印出摘要统计信息：
- en: '[PRE69]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We close the program by stopping the `SparkSession`:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止`SparkSession`来关闭程序：
- en: '[PRE70]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: How it works...
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We loaded the data by reading the housing data and loading the appropriate
    columns. We then proceeded to set the parameters that will force the `LinearRegression()`
    to perform a Ridge regression while keeping the optimization to ''auto''. The
    following code shows how the linear regression API can be used to set the desired
    type of regression to ridge:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过读取房屋数据并加载适当的列来加载数据。然后，我们继续设置将强制`LinearRegression()`执行岭回归的参数，同时保持优化为'auto'。以下代码显示了如何使用线性回归API来设置所需的回归类型为岭回归：
- en: '[PRE71]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: We then used `.fit()` to fit the model to the data. We finally used `.summary`
    to extract the model summary and print the MSE and RMSE for the model.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用`.fit()`将模型拟合到数据。最后，我们使用`.summary`提取模型摘要并打印模型的MSE和RMSE。
- en: There's more...
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'To make sure we are clear on the difference between ridge and Lasso regression,
    we must first highlight the difference between parameter shrinkage (that is, we
    squash the weight using a square root function, but never set it to zero) and
    feature engineering or parameter selection (that is, we shrink the parameters
    all the way to `0`, thereby causing some of the parameters to disappear altogether
    from the model):'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们清楚岭回归和Lasso回归之间的区别，我们必须首先强调参数收缩（即，我们使用平方根函数压缩权重，但从不将其设置为零）和特征工程或参数选择之间的区别（即，我们将参数收缩到`0`，从而导致一些参数从模型中完全消失）：
- en: Ridge regression: [https://en.wikipedia.org/wiki/Tikhonov_regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization)
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 岭回归：[https://en.wikipedia.org/wiki/Tikhonov_regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization)
- en: Lasso regression: [https://en.wikipedia.org/wiki/Lasso_(statistics)](https://en.wikipedia.org/wiki/Lasso_(statistics))
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lasso回归：[https://en.wikipedia.org/wiki/Lasso_(statistics)](https://en.wikipedia.org/wiki/Lasso_(statistics))
- en: Elastic net - Stanford University: [http://web.stanford.edu/~hastie/TALKS/enet_talk.pdf](http://web.stanford.edu/~hastie/TALKS/enet_talk.pdf)
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性网络 - 斯坦福大学：[http://web.stanford.edu/~hastie/TALKS/enet_talk.pdf](http://web.stanford.edu/~hastie/TALKS/enet_talk.pdf)
- en: See also
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation on Linear regression: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归文档：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)
- en: Isotonic regression in Apache Spark 2.0
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 2.0中的等渗回归
- en: In this recipe, we demonstrate the `IsotonicRegression()` function in Spark
    2.0\. The isotonic or monotonic regression is used when order is expected in the
    data and we want to fit an increasing ordered line (that is, manifest itself as
    a step function) to a series of observations. The terms **isotonic regression**
    (**IR**) and **monotonic regression** (**MR**) are synonymous in literature and
    can be used interchangeably.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们演示了Spark 2.0中的`IsotonicRegression()`函数。当数据中期望有顺序并且我们想要将递增的有序线（即，表现为阶梯函数）拟合到一系列观察中时，使用等渗或单调回归。术语**等渗回归**（**IR**）和**单调回归**（**MR**）在文献中是同义的，可以互换使用。
- en: In short, what we are trying to do with the `IsotonicRegression()` recipe is
    to provide a better fit versus some of the shortcomings of Naive Bayes and SVM.
    While they are both powerful classifiers, Naive Bayes lacks a good estimate of
    P (C | X) and **Support Vector Machines** (**SVM**) at best provides only a proxy
    (can use hyperplane distance), which is not an accurate estimator in some cases.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们尝试使用`IsotonicRegression()`配方提供比朴素贝叶斯和SVM的一些缺点更好的拟合。虽然它们都是强大的分类器，但朴素贝叶斯缺乏P（C
    | X）的良好估计，**支持向量机**（**SVM**）最多只提供代理（可以使用超平面距离），在某些情况下并不是准确的估计器。
- en: How to do it...
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Go to the website to download the file and save the file into the data path
    mentioned in the following code blocks. We use the famous Iris data and fit a
    step line to the observation. We use the Iris data in `LIBSVM` format from the
    library to demonstrate the IR.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到网站下载文件并将文件保存到以下代码块中提到的数据路径。我们使用著名的鸢尾花数据并将一步线拟合到观察中。我们使用库中以`LIBSVM`格式的鸢尾花数据来演示IR。
- en: The filename we choose is `iris.scale.txt` [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的文件名是`iris.scale.txt` [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale)。
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE72]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包，以便`SparkSession`可以访问集群和`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE73]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE74]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Initialize a `SparkSession` specifying configurations with the builder pattern
    thus making an entry point available for the Spark cluster:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建模式初始化`SparkSession`，从而使Spark集群的入口点可用：
- en: '[PRE75]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We then read the data file in, print out the data schema, and display the data
    in the console:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们读入数据文件，打印出数据模式，并在控制台中显示数据：
- en: '[PRE76]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We get the following console output:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下控制台输出：
- en: '![](img/00115.jpeg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00115.jpeg)'
- en: 'We then split the data set to training and test set in a ratio of *0.7:0.3*:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将数据集分割为训练集和测试集，比例为*0.7:0.3*：
- en: '[PRE77]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Next, we create the `IsotonicRegression` object and fit it in the training
    data:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建`IsotonicRegression`对象并将其拟合到训练数据中：
- en: '[PRE78]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Now we print out the model boundary and predictions in the console:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们在控制台中打印出模型边界和预测：
- en: '[PRE79]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We get the following console output:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下控制台输出：
- en: '[PRE80]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'We let the model transform the test data and display the result:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们让模型转换测试数据并显示结果：
- en: '[PRE81]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'We get the following console output:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下控制台输出：
- en: '![](img/00116.jpeg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00116.jpeg)'
- en: 'We close the program by stopping the `SparkSession`:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止`SparkSession`来关闭程序：
- en: '[PRE82]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: How it works...
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we explored the features of the Isotonic Regress model. We
    first read the dataset file into Spark in a `libsvm` format. We then split the
    data (*70/30*) and proceeded. Next, we displayed the DataFrame in the console
    by calling the `.show()` function. We then created the `IsotonicRegression()`
    object and let the model run for itself by calling the `fit(data)` function. In
    this recipe, we kept it simple and did not change any default parameters, but
    the readers should experiment and use the JChart package to graph the line and
    see the effect on the increasing and stepped line result in action.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们探索了等距回归模型的特性。我们首先以`libsvm`格式将数据集文件读入Spark。然后我们分割数据（*70/30*）并进行下一步。接下来，我们通过调用`.show()`函数在控制台中显示DataFrame。然后，我们创建了`IsotonicRegression()`对象，并通过调用`fit(data)`函数让模型自行运行。在这个示例中，我们保持简单，没有改变任何默认参数，但读者应该进行实验，并使用JChart包来绘制线条，看看增长和阶梯线条结果的影响。
- en: Finally, we displayed the model boundary and predictions in the console and
    used the model to transform the test dataset and displayed the result DataFrame
    in the console with the prediction field included. All Spark ML algorithms are
    sensitive to hyper parameter value. While there are no hard and fast rules for
    setting these parameters, a good amount of experimentation using the scientific
    method is required before going to production.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在控制台中显示了模型边界和预测，并使用模型转换测试数据集，并在控制台中显示包含预测字段的结果DataFrame。所有Spark ML算法对超参数值都很敏感。虽然设置这些参数没有硬性规定，但在投入生产之前需要进行大量的科学方法实验。
- en: We have covered a good number of model evaluation facilities provided by Spark
    in previous chapters and have discussed evaluation metrics throughout the book
    without being redundant. Spark provides the following model evaluation methods.
    A developer must pick and choose the specific evaluation metric facility based
    on the type of algorithm being evaluated (for example, discrete, continuous, binary,
    multiclass, and so on).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中介绍了Spark提供的许多模型评估设施，并在整本书中讨论了评估指标，而没有重复。Spark提供以下模型评估方法。开发人员必须根据正在评估的算法类型（例如，离散、连续、二进制、多类等）选择特定的评估指标设施。
- en: We will cover the evaluation metrics individually using recipes, but please
    see the following link for Spark's model evaluation coverage: [http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html](http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将单独使用配方来覆盖评估指标，但请参阅以下链接，了解Spark的模型评估覆盖范围：[http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html](http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html)。
- en: There's more...
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The Spark 2.0 implementation has the following restrictions at the time of
    writing:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Spark 2.0实现具有以下限制：
- en: 'Only single feature (that is, univariate) algorithms are supported:'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅支持单特征（即单变量）算法：
- en: '[PRE83]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The implementation is currently set to **parallelized pool adjacent violators
    algorithm** (**PAVA**):'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前的实现设置为**并行池相邻违反者算法**（**PAVA**）：
- en: As of Spark 2.1.0, it is a univariate monotonic implementation
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截至Spark 2.1.0，它是单变量单调实现
- en: See CRAN implementation like Spark 2.0: [https://cran.r-project.org/web/packages/isotone/vignettes/isotone.pdf](https://cran.r-project.org/web/packages/isotone/vignettes/isotone.pdf)
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看CRAN实现，如Spark 2.0：[https://cran.r-project.org/web/packages/isotone/vignettes/isotone.pdf](https://cran.r-project.org/web/packages/isotone/vignettes/isotone.pdf)
- en: See UCLA paper (PAVA): [http://gifi.stat.ucla.edu/janspubs/2009/reports/deleeuw_hornik_mair_R_09.pdf](http://gifi.stat.ucla.edu/janspubs/2009/reports/deleeuw_hornik_mair_R_09.pdf)
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参见UCLA论文（PAVA）：[http://gifi.stat.ucla.edu/janspubs/2009/reports/deleeuw_hornik_mair_R_09.pdf](http://gifi.stat.ucla.edu/janspubs/2009/reports/deleeuw_hornik_mair_R_09.pdf)
- en: See University of Wisconsin: [https://www.biostat.wisc.edu/sites/default/files/tr_116.pdf](https://www.biostat.wisc.edu/sites/default/files/tr_116.pdf)
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参见威斯康星大学：[https://www.biostat.wisc.edu/sites/default/files/tr_116.pdf](https://www.biostat.wisc.edu/sites/default/files/tr_116.pdf)
- en: 'Documentation for Isotonic regression:'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等距回归的文档：
- en: '[https://spark.apache.org/docs/latest/ml-classification-regression.html#isotonic-regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#isotonic-regression)'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/ml-classification-regression.html#isotonic-regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#isotonic-regression)'
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegression](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegression)'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegression](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegression)'
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegressionModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegressionModel)'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegressionModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegressionModel)'
- en: See also
  id: totrans-398
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'More information about Isotonic Regression can be found at:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 有关保序回归的更多信息，请参见：
- en: '[https://en.wikipedia.org/wiki/Isotonic_regression](https://en.wikipedia.org/wiki/Isotonic_regression)'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Isotonic_regression](https://en.wikipedia.org/wiki/Isotonic_regression)'
- en: 'The isotonic regression line ends up being a step function as opposed to a
    linear regression, which is a straight line. The following figure (source: Wikipedia)
    provides a good reference:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 保序回归线最终成为一个阶梯函数，而不是线性回归，即一条直线。以下图（来源：维基百科）提供了一个很好的参考：
- en: '![](img/00117.jpeg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.jpeg)'
- en: Multilayer perceptron classifier in Apache Spark 2.0
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 2.0中的多层感知器分类器
- en: In this recipe, we explore Spark's 2.0 **multilayer perceptron classifier**
    (**MLPC**), which is another name for feed-forward neural networks. We use the
    iris data set to predict a binary outcome for the feature vectors that describes
    the input. The key point to remember is that, even though the name sounds a bit
    complicated, at its core the MLP is just a non-linear classifier for data that
    cannot be separated via a simple linear line or hyperplane.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探索了Spark 2.0的**多层感知器分类器**（**MLPC**），这是前馈神经网络的另一个名称。我们使用鸢尾花数据集来预测描述输入的特征向量的二元结果。要记住的关键点是，即使名称听起来有点复杂，MLP本质上只是用于无法通过简单的线性线或超平面分离的数据的非线性分类器。
- en: How to do it...
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Go to the `LIBSVM` Data: Classification (Multi-class) Repository and download
    the file from the following URL: [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale)'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`LIBSVM`数据：分类（多类）存储库，并从以下URL下载文件：[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale)
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE84]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`SparkSession`所需的包，以访问集群，并导入`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE85]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`，以减少Spark的日志输出：
- en: '[PRE86]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Initialize a `SparkSession` specifying configurations to gain access to the
    Spark cluster:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`SparkSession`，指定配置以访问Spark集群：
- en: '[PRE87]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'We begin by loading the `libsvm` formatted data file into memory:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将`libsvm`格式的数据文件加载到内存中：
- en: '[PRE88]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Now display the loaded data:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在显示加载的数据：
- en: 'From the console, this is the output:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台，这是输出：
- en: '[PRE89]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '![](img/00118.jpeg)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.jpeg)'
- en: 'Next, we utilize the Datasets `randomSplit` method to divide data into two
    buckets with allocations of 80% and 20% for each bucket:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们利用数据集的`randomSplit`方法将数据分成两个桶，每个桶分配80%和20%的数据：
- en: '[PRE90]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The `randomSplit` method returns an array with two sets of data, the training
    set being the 80% portion and testing set being the 20% portion:'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`randomSplit`方法返回一个包含两组数据的数组，其中训练集占80%，测试集占20%：'
- en: '[PRE91]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Next, we configure the multilayer perceptron classifier with an input layer
    of four nodes, a hidden layer of five nodes, and a four-node layer for output:'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们配置多层感知器分类器，输入层为四个节点，隐藏层为五个节点，输出为四个节点：
- en: '[PRE92]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '**Blocksize**: Block size for stacking input data in matrices to speed up the
    computation. This is more of an efficiency parameter with a recommended size between
    `10` and `1000`. This parameter concerns the total amount of data that is shoved
    into a partition for efficiency reasons.'
  id: totrans-428
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Blocksize**：用于将输入数据堆叠在矩阵中以加快计算速度的块大小。这更多是一个效率参数，推荐的大小在`10`和`1000`之间。该参数涉及将数据推入分区以提高效率的总量。'
- en: '**MaxIter**: Maximum number of iterations to run the model.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MaxIter**：运行模型的最大迭代次数。'
- en: '**Seed**: Set the seed for weight initialization if weights are not set.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Seed**：如果未设置权重，则设置权重初始化的种子。'
- en: 'The following two lines from the Spark source code on GitHub reveals the default
    set within the code:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub上Spark源代码的以下两行显示了代码中的默认设置：
- en: '[PRE93]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: To understand the parameters and seeding better, see the MLP source code at [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解参数和种子，请查看MLP源代码[https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala)。
- en: 'We generate a model by invoking the fit method:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过调用fit方法生成模型：
- en: '[PRE94]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Next, we put the trained model to use transforming the test data and displaying
    the predicted results:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们利用训练好的模型对测试数据进行转换，并显示预测结果：
- en: '[PRE95]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The result will be displayed in console like the following:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将像以下内容一样显示在控制台上：
- en: '![](img/00119.gif)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.gif)'
- en: 'Finally, we extract predictions and labels from the results and pass them to
    a Multi Class Classification Evaluator to generate an accuracy value:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们从结果中提取预测和标签，并将它们传递给多类分类评估器以生成准确度值：
- en: '[PRE96]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'We close the program by stopping the `SparkSession`:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止`SparkSession`来关闭程序：
- en: '[PRE97]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: How it works...
  id: totrans-444
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we demonstrated usage of a multilayer perceptron classifier.
    We began by loading the classic Iris dataset in `libsvm` format. Next, we split
    the dataset with a ratio of 80% for training set data and 20% for test set data.
    In our definition phase, we configured the multilayer perceptron classifier with
    an input layer of four nodes, a hidden layer of five nodes, and a four-node layer
    for output. We generated a trained model by invoking the `fit()` method, and then
    produced predictions utilizing the trained model.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们演示了多层感知器分类器的用法。我们首先加载了经典的鸢尾花数据集，格式为`libsvm`。接下来，我们将数据集分成80%的训练集数据和20%的测试集数据。在定义阶段，我们配置了一个输入层有四个节点，一个隐藏层有五个节点，一个输出层有四个节点的多层感知器分类器。我们通过调用`fit()`方法生成了一个训练模型，然后利用训练模型进行预测。
- en: Finally, we retrieved predictions and labels, passing them to the multi-class
    classification evaluator that computes an accuracy value.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们获取了预测和标签，并将它们传递给多类分类评估器，计算准确度值。
- en: 'A simple visual inspection of predicted versus actual without much experimentation
    and fitting seems very impressive and acts a testimony as to why neural networks
    (much different than the early 1990s version) are back in favor. They do a great
    job in capturing non-linear surfaces. Here are some examples of non-linear surfaces
    (source: Graphing Calculator 4 on Mac App Store).'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有太多实验和拟合的情况下，对预测与实际情况进行简单的目测似乎非常令人印象深刻，并且证明了为什么神经网络（与上世纪90年代的版本大不相同）重新受到青睐。它们在捕捉非线性表面方面做得很好。以下是一些非线性表面的例子（来源：Mac
    App Store上的Graphing Calculator 4）。
- en: 'The following figure shows a 2D depiction of a sample non-linear case:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了一个样本非线性情况的二维描述：
- en: '![](img/00120.jpeg)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.jpeg)'
- en: The following figure shows a 3D depiction of a sample non-linear case.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了一个样本非线性情况的三维描述。
- en: '![](img/00121.jpeg)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00121.jpeg)'
- en: 'Generally speaking, a neural network is defined first by a code sample as follows:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，神经网络首先由以下代码示例定义：
- en: '[PRE98]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: That defines the physical configuration of the network. In this case, we have
    a *4 x 5 x 4* MLP, meaning four input layers, five hidden layers, and four output
    layers. The `BlockSize` is set to 110 by using the `setBlockSize(110)` method
    for demonstration purposes, but 128 is the default out of the box. It is important
    to have a good random function to initialize the weights, which in this case is
    the current system time `setSeed(System.*currentTimeMillis*()`. The `setMaxIter(145)`
    is the maximum number of iterations used by the `setSolver()` method, which is
    `l-bfgs` solver by default.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了网络的物理配置。在这种情况下，我们有一个*4 x 5 x 4* MLP，意思是四个输入层，五个隐藏层和四个输出层。通过使用`setBlockSize(110)`方法将`BlockSize`设置为110，但默认值为128。重要的是要有一个良好的随机函数来初始化权重，在这种情况下是当前系统时间`setSeed(System.*currentTimeMillis*()`。`setMaxIter(145)`是`setSolver()`方法使用的最大迭代次数，默认为`l-bfgs`求解器。
- en: There's more...
  id: totrans-455
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '**Multilayer Perceptron** (**MLP**) or **Feed Forward Network** (**FFN**) is
    usually the first type of neuron network that one comes to understand before graduating
    to **Restricted Boltzmann Machines** (**RBM**) and **Recurrent Neural Network**
    (**RRN**) that are common in deep learning. While MLP technically can be configured/referred
    to as deep network, one must investigate a bit and understand as to why it is
    considered a first step (only) in a journey toward deep learning networks.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）或**前馈网络**（**FFN**）通常是人们在毕业于**受限玻尔兹曼机**（**RBM**）和**循环神经网络**（**RRN**）之前首先了解的神经网络类型，这些类型在深度学习中很常见。虽然MLP在技术上可以被配置/称为深度网络，但必须进行一些调查并了解为什么它被认为是通往深度学习网络的第一步（仅此而已）。'
- en: In Spark's 2.0 implementation, sigmoid function (non-linear activation) is used
    in a deep stackable network configuration (more than three layer) to map the output
    to a `Softmax` function to create a non-trivial mapping surface that can capture
    extreme non-linear behavior of the data.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0的实现中，Sigmoid函数（非线性激活）用于深度可堆叠网络配置（超过三层），将输出映射到`Softmax`函数，以创建一个能够捕捉数据极端非线性行为的非平凡映射表面。
- en: Spark uses a sigmoid function to achieve non-linear mapping in a stackable configuration
    via an easy-to-use API. The following image depicts a Sigmoid function and its
    graph using a graphing calculator software on Mac
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: Spark使用Sigmoid函数通过易于使用的API在可堆叠的配置中实现非线性映射。以下图显示了Sigmoid函数及其在Mac上的图形计算器软件上的图形。
- en: '![](img/00122.jpeg)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00122.jpeg)'
- en: See also
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for Spark's 2.0 MLP: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.MultilayerPerceptronClassifier](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.MultilayerPerceptronClassifier)
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0 MLP的文档：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.MultilayerPerceptronClassifier](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.MultilayerPerceptronClassifier)
- en: 'For a quick introduction to MLP, see the following:'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关MLP的快速介绍，请参阅以下内容：
- en: '[https://en.wikipedia.org/wiki/Multilayer_perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Multilayer_perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)'
- en: '[https://en.wikipedia.org/wiki/Perceptron](https://en.wikipedia.org/wiki/Perceptron)'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Perceptron](https://en.wikipedia.org/wiki/Perceptron)'
- en: '[http://www.di.ubi.pt/~lfbaa/pubs/NN2008.pdf](http://www.di.ubi.pt/~lfbaa/pubs/NN2008.pdf)'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.di.ubi.pt/~lfbaa/pubs/NN2008.pdf](http://www.di.ubi.pt/~lfbaa/pubs/NN2008.pdf)'
- en: See Spark MLP source code at [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala)
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参阅Spark MLP源代码：[https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala)
- en: 'Classic papers needed to understand deep belief networks (absolute minimum)
    and its contrast to a simple MLP:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 理解深度信念网络（绝对最低限度）及其与简单MLP的对比所需的经典论文：
- en: Deep belief networks: [https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度置信网络：[https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
- en: Stacked auto encoders: [http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf](http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf)
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠自动编码器：[http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf](http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf)
- en: Sparse representation: [http://www.cs.nyu.edu/~ranzato/publications/ranzato-nips06.pdf](http://www.cs.nyu.edu/~ranzato/publications/ranzato-nips06.pdf)
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏表示：[http://www.cs.nyu.edu/~ranzato/publications/ranzato-nips06.pdf](http://www.cs.nyu.edu/~ranzato/publications/ranzato-nips06.pdf)
- en: Some of the important API calls within `MultilayerPerceptronClassifier`**:**
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultilayerPerceptronClassifier`中的一些重要的API调用：**'
- en: 'The `BlockSize` by default is set to 128 - you should only start adjusting
    this parameter when you feel you have mastered the MLP in full:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '`BlockSize`默认设置为128 - 只有在您完全掌握MLP时才应开始调整此参数：'
- en: '`def **setLayers**(value: Array[Int]): MultilayerPerceptronClassifier.this.type`'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setLayers**(value: Array[Int]): MultilayerPerceptronClassifier.this.type`'
- en: '`def **setFeaturesCol**(value: String): MultilayerPerceptronClassifier`'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setFeaturesCol**(value: String): MultilayerPerceptronClassifier`'
- en: '`def **setLabelCol**(value: String): MultilayerPerceptronClassifier`'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setLabelCol**(value: String): MultilayerPerceptronClassifier`'
- en: '``def **setSeed**(value: Long): MultilayerPerceptronClassifier.this.type``'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '``def **setSeed**(value: Long): MultilayerPerceptronClassifier.this.type``'
- en: '`def **setBlockSize**(value: Int): MultilayerPerceptronClassifier.this.type`'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setBlockSize**(value: Int): MultilayerPerceptronClassifier.this.type`'
- en: '`def **setSolver**(value: String): MultilayerPerceptronClassifier.this.type`'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def **setSolver**(value: String): MultilayerPerceptronClassifier.this.type`'
- en: One-vs-Rest classifier (One-vs-All) in Apache Spark 2.0
  id: totrans-479
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 2.0中的One-vs-Rest分类器（One-vs-All）
- en: In this recipe, we demonstrate One-vs-Rest in Apache Spark 2.0\. What we are
    trying to achieve with the `OneVsRest()` classifier is to make a binary logistic
    regression to work for a multi-class / multi-label classification problem. The
    recipe is a two-step approach in which we first configure a `LogisticRegression()`
    object and then use it in a `OneVsRest()` classifier to solve a multi-class classification
    problem using logistic regression.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们演示了Apache Spark 2.0中的One-vs-Rest。我们尝试通过`OneVsRest()`分类器使二元逻辑回归适用于多类/多标签分类问题。该示例是一个两步方法，首先我们配置一个`LogisticRegression()`对象，然后在`OneVsRest()`分类器中使用它来解决使用逻辑回归的多类分类问题。
- en: How to do it...
  id: totrans-481
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Go to the `LIBSVM` Data: Classification (Multi-class) Repository and download
    the file: [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale)'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`LIBSVM`数据：分类（多类）存储库，并下载文件：[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale)
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE99]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Import the necessary packages for the `SparkSession` to gain access to the
    cluster and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包，以便`SparkSession`可以访问集群，`Log4j.Logger`可以减少Spark产生的输出量：
- en: '[PRE100]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`，以减少Spark的日志输出：
- en: '[PRE101]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Initialize a `SparkSession` specifying configurations constructing an entry
    point to the Spark cluster:'
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个`SparkSession`，指定配置，构建一个Spark集群的入口点：
- en: '[PRE102]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We begin by loading into memory the `libsvm` formatted data file:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将`libsvm`格式的数据文件加载到内存中：
- en: '[PRE103]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Now display the loaded data:'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在显示加载的数据：
- en: '[PRE104]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '![](img/00123.jpeg)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.jpeg)'
- en: 'Next, we utilize the datasets `randomSplit` method splitting the dataset with
    a ratio of 80% training data and 20% test data:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们利用数据集的`randomSplit`方法，将数据集按80%的训练数据和20%的测试数据进行分割：
- en: '[PRE105]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Let''s configure a logistics regression algorithm to use as a classifier for
    the One-vs-Rest algorithm:'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们配置一个逻辑回归算法，用作One-vs-Rest算法的分类器：
- en: '[PRE106]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Next, we create the one versus rest object passing our newly created logistic
    regression object as an argument:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个one versus rest对象，将我们新创建的逻辑回归对象作为参数传递：
- en: '[PRE107]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'We generated the model by invoking the fit method on our one-vs-rest object:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在我们的one-vs-rest对象上调用fit方法生成模型：
- en: '[PRE108]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Now, we will use the trained model to generate predictions for the test data
    and display the results:![](img/00124.jpeg)
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用训练好的模型为测试数据生成预测并显示结果：![](img/00124.jpeg)
- en: 'Finally, we pass predictions to the Multi Class Classification Evaluator to
    generate an accuracy value:'
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将预测传递给多类分类评估器，生成准确度值：
- en: '[PRE109]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'We close the program by stopping the `SparkSession`:'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止`SparkSession`来关闭程序：
- en: '[PRE110]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: How it works...
  id: totrans-510
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'In this example, we demonstrated usage of the One-vs-Rest classifier. We began
    by loading the classic Iris dataset in `libsvm` format. Next, we split the dataset
    with a ratio of 80% for a training dataset and 20% for a test dataset. We draw
    the users'' attention to how we use system time for randomness in a split as follows:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们演示了One-vs-Rest分类器的用法。我们首先加载了经典的Iris数据集，格式为`libsvm`。接下来，我们将数据集按80%的比例分割为训练数据集和20%的测试数据集。我们提醒用户注意，我们如何使用系统时间来进行分割的随机性如下：
- en: '[PRE111]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'The algorithm can be best described as a three-step process:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法最好可以描述为一个三步过程：
- en: 'We first configure the regression object without having to have a base logistic
    model at hand so it can be fed into our classifier:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先配置回归对象，而无需手头上有基本的逻辑模型，以便将其输入到我们的分类器中：
- en: '[PRE112]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'In the next step, we feed the configured regression model into our classifier
    and call the `fit()` function to finish the job accordingly:'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，我们将配置好的回归模型输入到我们的分类器中，并调用`fit()`函数来完成相应的工作：
- en: '[PRE113]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: We generate a trained model and transform the test data by way of the model.
    Finally, we pass predictions to the multi class classification evaluator, generating
    an accuracy value.
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们生成了一个训练模型，并通过该模型转换了测试数据。最后，我们将预测传递给多类分类评估器，生成一个准确度值。
- en: There's more...
  id: totrans-519
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The typical usage of this algorithm is for tagging and bagging different news
    items of interest about a person into various categories (such as friendly versus
    hostile, lukewarm versus elated, and so on). Another usage in medical billing
    could be the classification of patient diagnostic into different medical codes
    used for automated billing and revenue cycle maximization.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 这种算法的典型用法是将关于一个人的不同新闻项目标记和打包到各种类别中（例如友好与敌对、温和与欢欣等）。在医疗账单中的另一个用途可能是将患者诊断分类为用于自动结算和收入循环最大化的不同医疗编码。
- en: 'One-vs-Rest: As shown in the following figure, this solves an *n*-label classification
    problem by binary logistic regression:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 一对多：如下图所示，这通过二元逻辑回归解决了一个*n*标签分类问题：
- en: '![](img/00125.jpeg)'
  id: totrans-522
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00125.jpeg)'
- en: See also
  id: totrans-523
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Spark''s 2.0 documentation for `OneVsRest()` can be found at:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0关于`OneVsRest()`的文档可以在以下找到：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.OneVsRest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.OneVsRest)'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.OneVsRest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.OneVsRest)'
- en: 'Another way to visualize this is, to assess for a given binary classifier can
    we break down an n-class input into *N* number of logistic regressions and then
    pick the one that describes the data the best. There are numerous examples of
    this classifier using Scikit Learn library in Python as follows:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化方法是，评估给定二元分类器是否可以将n类输入分解为*N*个逻辑回归，然后选择最能描述数据的那个。以下是使用Python中Scikit Learn库的此分类器的众多示例：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier)'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier)'
- en: 'But we recommend you do a quick scan of the actual Scala source code (less
    than 400 lines only) at GitHub:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们建议您在GitHub上对实际的Scala源代码进行快速扫描（仅不到400行）：
- en: '[https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/classification/OneVsRest.scala](https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/classification/OneVsRest.scala)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/classification/OneVsRest.scala](https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/classification/OneVsRest.scala)'
- en: Survival regression – parametric AFT model in Apache Spark 2.0
  id: totrans-530
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生存回归 - 参数AFT模型在Apache Spark 2.0中
- en: In this recipe, we explore Spark 2.0's implementation for Survival regression,
    which is not the typical proportional hazard model, but the **Accelerated Failure
    Time** (**AFT**) model instead. This is an important distinction that should be
    kept in mind while running this recipe otherwise the results would not make sense.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们探索了Spark 2.0对生存回归的实现，这不是典型的比例危险模型，而是**加速失效时间**（**AFT**）模型。这是一个重要的区别，应该在运行这个教程时牢记，否则结果将毫无意义。
- en: The survival regression analysis considers itself with models of *time to an
    event* nature, which are common in medicine, insurance, and anytime survivability
    of the subject is of interest. One of my coauthors happen to be a fully trained
    medical doctor (in addition to being a computer scientist), so we use a real dataset
    HMO-HIM+ study from a well-respected book in the field so we can obtain a reasonable
    output.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 生存回归分析关注的是*事件发生时间*的模型，这在医学、保险和任何时候对主题的生存能力感兴趣的情况下都很常见。我的一位合著者碰巧是一位受过全面训练的医生（除了是计算机科学家），所以我们使用了该领域一本备受尊敬的书中的真实数据集HMO-HIM+研究，以便获得合理的输出。
- en: Currently, we are using this technique to do drought modeling at scale to predict
    price impact on agricultural commodities in long-range time frames and forecasts.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们正在使用这种技术来进行干旱建模，以预测农产品在长期时间范围内的价格影响和预测。
- en: How to do it...
  id: totrans-534
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Go to the UCLA website to download the file:'
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往UCLA网站下载文件：
- en: '[https://stats.idre.ucla.edu/stat/r/examples/asa/hmohiv.csv](https://stats.idre.ucla.edu/stat/r/examples/asa/hmohiv.csv)'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://stats.idre.ucla.edu/stat/r/examples/asa/hmohiv.csv](https://stats.idre.ucla.edu/stat/r/examples/asa/hmohiv.csv)'
- en: 'The dataset we used is the actual data that is in the book *Applied Survival
    Analysis: Regression Modeling of Time to Event Data* by David W Hosmer and Stanley
    Lemeshow (1999). The data came from a HMO-HIM+ study and the data contains the
    following fields:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的数据集是David W Hosmer和Stanley Lemeshow（1999）的书《应用生存分析：事件发生时间数据的回归建模》中的实际数据。数据来自HMO-HIM+研究，数据包含以下字段：
- en: '![](img/00126.jpeg)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00126.jpeg)'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE114]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`SparkSession`所需的包，以便访问集群，以及`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE115]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`，以减少Spark的日志输出：
- en: '[PRE116]'
  id: totrans-545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Initialize a `SparkSession` specifying configurations with the builder pattern
    thus making an entry point available for the Spark Cluster:'
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化`SparkSession`，指定配置，从而为Spark集群提供入口点：
- en: '[PRE117]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: We then read the `csv` file in, skipping the first line (header).
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们读取`csv`文件，跳过第一行（标题）。
- en: 'Note: there are multiple ways to read in the `csv` files to a Spark DataFrame:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有多种方法可以将`csv`文件读入Spark DataFrame：
- en: '[PRE118]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'We convert the field from string to double. We are only interested in the ID,
    time, age, and censor fields. The four fields then form a DataFrame:'
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将字段从字符串转换为双精度。我们只对ID、时间、年龄和审查字段感兴趣。然后这四个字段形成一个DataFrame：
- en: '[PRE119]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: The new `features` fields is a vector composed of `time` and `age` fields.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 新的`features`字段是由`time`和`age`字段组成的向量。
- en: 'Next, we display the DataFrame in the console:'
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在控制台中显示了DataFrame：
- en: '[PRE120]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'From the console, this is the output:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台，这是输出：
- en: '![](img/00127.jpeg)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00127.jpeg)'
- en: Now we will create the `AFTSurvivalRegression` object, and set the parameters.
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将创建`AFTSurvivalRegression`对象，并设置参数。
- en: The quantile probabilities are set to 0.3 and 0.6 for this particular recipe.
    The values depict the boundaries of quantile, which are numerical vectors of probabilities
    with values in the range of *0.0* to *1.0* [*0.0,1.0*]. For example, using (0.25,
    0.5, 0.75) for quantile probability vector is a common theme.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的配方，分位数概率设置为0.3和0.6。这些值描述了分位数的边界，它们是概率的数值向量，其值范围在*0.0*到*1.0* [*0.0,1.0*]之间。例如，使用(0.25,
    0.5, 0.75)作为分位数概率向量是一个常见的主题。
- en: The quantiles column name is set to *quantiles*.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 分位数列名设置为*quantiles*。
- en: In the following code, we create the `AFTSurvivalRegression()` object and set
    the column name and quantile probability vector.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们创建了`AFTSurvivalRegression()`对象，并设置了列名和分位数概率向量。
- en: 'The following code from Spark''s source code on GitHub shows default values:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Spark在GitHub上的源代码的以下代码显示了默认值：
- en: '[PRE121]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: To understand parameterization and seeding, *Spark Source Code for Survival
    Regression* can be referenced on GitHub at [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala).
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解参数化和种子，可以在GitHub上参考*Spark Source Code for Survival Regression* [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala)。
- en: '[PRE122]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'We let the model run:'
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们让模型运行：
- en: '[PRE123]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'We print out the model data into the console:'
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将模型数据打印到控制台：
- en: '[PRE124]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'The following output will be seen in the console:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台中将看到以下输出：
- en: '[PRE125]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'We use the preceding model to transform the dataset and display the result
    in the console:'
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用前面的模型转换了数据集，并在控制台中显示了结果：
- en: '[PRE126]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'The following output will be seen in the console:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台中将看到以下输出：
- en: '![](img/00128.gif)'
  id: totrans-575
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00128.gif)'
- en: 'We close the program by stopping the `SparkSession`:'
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止`SparkSession`来关闭程序：
- en: '[PRE127]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: How it works...
  id: totrans-578
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We explored features of the **Accelerated Failure time** (**AFT**) model. We
    first read the dataset file into Spark using the `sparkContext.textFile()`. There
    are multiple ways to read a `csv` format file. We just picked one showing more
    detailed steps.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了**加速失效时间**（**AFT**）模型的特征。我们首先使用`sparkContext.textFile()`将数据集文件读入Spark。有多种方法可以读取`csv`格式文件。我们只选择了一个显示更详细步骤的方法。
- en: Next, we filtered out the head row, and converted the interested fields from
    string to double, and then converted the double dataset into a new DataFrame with
    a new `features` field.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们过滤了头行，并将感兴趣的字段从字符串转换为双精度，然后将双精度数据集转换为具有新`features`字段的新DataFrame。
- en: We then created the `AFTSurvivalRegression` object and set the quantile parameters,
    and let the model run for itself by calling the `fit(data)` function.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建了`AFTSurvivalRegression`对象并设置了分位数参数，并通过调用`fit(data)`函数让模型自行运行。
- en: Finally, we displayed the model summary and used the model to transform the
    dataset and displayed the resulting DataFrame with prediction and quantiles fields
    included.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们显示了模型摘要，并使用模型转换了数据集，并显示了包括预测和分位数字段的结果DataFrame。
- en: There's more...
  id: totrans-583
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Spark implementation of survival regression (`AFTSurvivalRegression`)*:*
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: Spark实现的生存回归（`AFTSurvivalRegression`）*：*
- en: '**Model**: Accelerated Failure Time (AFT).'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型：**加速失效时间（AFT）。'
- en: '**Parametric:** Using Weibull distribution.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数化：**使用威布尔分布。'
- en: '**Optimization:** Spark chooses AFT because it is easier to parallelize and
    views the problem as a convex optimization problem with L-BFGS being the method
    of choice as optimization method.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化：**Spark选择AFT是因为它更容易并行化，并将问题视为具有L-BFGS作为优化方法的凸优化问题。'
- en: '**R/SparkR users:** When fitting `AFTSurvivalRegressionModel` without intercept
    on dataset with constant nonzero column, Spark MLlib outputs zero coefficients
    for constant nonzero columns. This behavior is different from R `survival::survreg`.
    (from Spark 2.0.2 documentation)'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R/SparkR用户：**在没有拦截器的情况下拟合`AFTSurvivalRegressionModel`到具有常数非零列的数据集时，Spark
    MLlib会对常数非零列输出零系数。这种行为与R `survival::survreg`不同。（来自Spark 2.0.2文档）'
- en: You should think of the outcome as the time until the occurrence of an event
    of interest occurs, such as occurrence of a disease, winning, losing, time to
    mortgage default, marriage, divorce, landing a job after graduation, and so on.
    What is unique about these models is that the *time event* is a duration and does
    not necessarily have an explanatory variable (that is, it is just a duration in
    days, months, or years).
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该将结果视为发生感兴趣事件的时间，比如疾病的发生、赢得或失去、按揭违约时间、婚姻、离婚、毕业后找到工作等。这些模型的独特之处在于*时间事件*是一个持续时间，并不一定有解释变量（也就是说，它只是一段时间，以天、月或年为单位）。
- en: 'The reasons you might use survival models as opposed to simple regression (that
    is, tempting) are as follows:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能使用生存模型而不是简单回归（即诱人）的原因如下：
- en: Need to model the outcome variable as a time event
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要将结果变量建模为时间事件
- en: Censoring - not all the data is known or used (common when using long-range
    commodity data from past centuries)
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查-并非所有数据都是已知的或使用的（在使用来自过去几个世纪的长期商品数据时很常见）
- en: Non-normality distributed outcome - often the case with time
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非正态分布的结果-通常是时间的情况
- en: It might or might not be a case of multivariate analysis
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可能是多变量分析的情况，也可能不是。
- en: 'While there are two approaches to survival regression as outlined here, at
    the time of writing, Spark 2.0 only supports the AFT model and not the most-talked-about
    version, which is the proportional hazard model:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在此处概述了生存回归的两种方法，但在撰写本文时，Spark 2.0仅支持AFT模型，而不支持最广为人知的比例风险模型：
- en: '**Proportional hazard model** (**PH**):'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比例风险模型（PH）：
- en: Proportionality assumed over time
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比例性假设随时间而定
- en: Multiplying a constant by covariance over consideration time
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在考虑时间内通过协方差乘以常数
- en: 'Example: Cox Proportional Hazard Model'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：Cox比例风险模型
- en: '**hx(y) = h0(y)*g(X)**'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**hx(y) = h0(y)*g(X)**'
- en: 'Accelerated Time Failure (ATF) **- **Spark 2.0 implementation:'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速时间故障（ATF）- Spark 2.0实施：
- en: Proportionality assumption can be assumed or is violated
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以假设或违反比例性假设
- en: 'The constant value multiplied by covariance to get the regression coefficient
    values may be:'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过协方差乘以常数值以获得回归系数值可能是：
- en: Accelerated
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速
- en: Decelerated
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减速
- en: 'Allows for stages in unfolding of the regression:'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许回归展开的阶段：
- en: Stages of disease
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 疾病的阶段
- en: Stages of survivability
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生存能力的阶段
- en: '*Yx * g(X) = Y0*'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Yx * g(X) = Y0*'
- en: '*Sx(y) = S0(yg(X))*'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sx(y) = S0(yg(X))*'
- en: '*where,*'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '*其中，*'
- en: '*Y*: survival time,'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y*：生存时间，'
- en: '*X*: covariate vector,'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '*X*：协变量向量，'
- en: '*hx(y)*: the hazard function,'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '*hx(y)*：危险函数，'
- en: '*Sx(y)*: the survival function of *Y* given *X*,'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sx(y)*：给定*X*的*Y*的生存函数，'
- en: '*Yx*: *Y* given *X*'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '*Yx*：给定*X*的*Y*'
- en: 'Parametric modeling - underlying distribution of time variable:'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数建模 - 时间变量的基础分布：
- en: Exponential
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数
- en: Weibull - Spark 2.0 implementation
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weibull - Spark 2.0实施
- en: Log Logistic
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数逻辑
- en: Normal
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正态
- en: Gamma
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伽马
- en: 'See also - very popular in R - we have used these two packages:'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅 - 在R中非常受欢迎 - 我们使用了这两个软件包：
- en: 'Library(survival): Standard Survival Analysis'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Library(survival)：标准生存分析
- en: 'Library(eha): For AFT modeling'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Library(eha)：用于AFT建模
- en: 'Documentation for `SurvivalRegression` is available at the following URLs:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '`SurvivalRegression`的文档可在以下网址找到：'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel)'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel)'
- en: '[https://spark.apache.org/docs/latest/ml-classification-regression.html#survival-regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#survival-regression)'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/ml-classification-regression.html#survival-regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#survival-regression)'
- en: 'The original format of the `HMOHIV` data set can be found at - connect as guest:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '`HMOHIV`数据集的原始格式可在以下网址找到 - 以访客身份连接：'
- en: '[ftp://ftp.wiley.com/public/sci_tech_med/survival](ftp://ftp.wiley.com/public/sci_tech_med/survival)'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '[ftp://ftp.wiley.com/public/sci_tech_med/survival](ftp://ftp.wiley.com/public/sci_tech_med/survival)'
- en: 'A deep and complete comparison of Proportional versus AFT (Spark 2.0) hazard
    models can be found at:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在以下网址找到Proportional与AFT（Spark 2.0）风险模型的深入完整比较：
- en: '[https://ecommons.usask.ca/bitstream/handle/10388/etd-03302009-140638/JiezhiQiThesis.pdf](https://ecommons.usask.ca/bitstream/handle/10388/etd-03302009-140638/JiezhiQiThesis.pdf)'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://ecommons.usask.ca/bitstream/handle/10388/etd-03302009-140638/JiezhiQiThesis.pdf](https://ecommons.usask.ca/bitstream/handle/10388/etd-03302009-140638/JiezhiQiThesis.pdf)'
- en: 'End-to-end real-world medical study with charts:'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 具有图表的端到端真实世界医学研究：
- en: '[https://www.researchgate.net/profile/Richard_Kay2/publication/254087561_On_the_Use_of_the_Accelerated_Failure_Time_Model_as_an_Alternative_to_the_Proportional_Hazards_Model_in_the_Treatment_of_Time_to_Event_Data_A_Case_Study_in_Influenza/links/548ed67e0cf225bf66a710ce.pdf](https://www.researchgate.net/profile/Richard_Kay2/publication/254087561_On_the_Use_of_the_Accelerated_Failure_Time_Model_as_an_Alternative_to_the_Proportional_Hazards_Model_in_the_Treatment_of_Time_to_Event_Data_A_Case_Study_in_Influenza/links/548ed67e0cf225bf66a710ce.pdf)'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.researchgate.net/profile/Richard_Kay2/publication/254087561_On_the_Use_of_the_Accelerated_Failure_Time_Model_as_an_Alternative_to_the_Proportional_Hazards_Model_in_the_Treatment_of_Time_to_Event_Data_A_Case_Study_in_Influenza/links/548ed67e0cf225bf66a710ce.pdf](https://www.researchgate.net/profile/Richard_Kay2/publication/254087561_On_the_Use_of_the_Accelerated_Failure_Time_Model_as_an_Alternative_to_the_Proportional_Hazards_Model_in_the_Treatment_of_Time_to_Event_Data_A_Case_Study_in_Influenza/links/548ed67e0cf225bf66a710ce.pdf)'
- en: See also
  id: totrans-635
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for AFT survival implementation:'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AFT生存实施的文档：
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler)'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler)'
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegression](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegression)'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegression](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegression)'
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel)'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel)'
- en: "Spark source code for survival regression can be referenced on GitHub: [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala\uFEFF\
    ](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala)"
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: "Spark生存回归的源代码可在GitHub上找到：[https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala\uFEFF\
    ](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala)"
