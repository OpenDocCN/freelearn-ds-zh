- en: Chapter 2. Data Acquisition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。数据获取
- en: As a data scientist, one of the most important tasks is to load data into your
    data science platform. Rather than having uncontrolled, ad hoc processes, this
    chapter explains how a general data ingestion pipeline in Spark can be constructed
    that serves as a reusable component across many feeds of input data. We walk through
    a configuration and demonstrate how it delivers vital feed management information
    under a variety of running conditions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，将数据加载到数据科学平台是最重要的任务之一。本章解释了如何构建Spark中的通用数据摄入管道，而不是采用无控制的临时过程，这个管道可以作为跨多个输入数据源的可重用组件。我们将配置并演示它如何在各种运行条件下提供重要的数据管理信息。
- en: Readers will learn how to construct a *content register* and use it to track
    all input loaded to the system and to deliver metrics on ingestion pipelines,
    so that these flows can be reliably run as an automated, lights-out process.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 读者将学习如何构建*内容注册*并使用它来跟踪加载到系统中的所有输入，并提供摄入管道的指标，以便这些流可以可靠地作为自动化的、无人值守的过程运行。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduce the **Global Database of Events, Language, and Tone** (**GDELT**)
    dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍**全球事件、语言和情绪数据库**（**GDELT**）数据集
- en: Data pipelines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管道
- en: Universal ingestion framework
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用摄入框架
- en: Real-time monitoring for new data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时监控新数据
- en: Receiving streaming data via Kafka
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Kafka接收流数据
- en: Registering new content and vaulting for tracking purposes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册新内容和保险库以进行跟踪
- en: Visualization of content metrics in Kibana to monitor ingestion processes and
    data health
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kibana中可视化内容指标，以监控摄入过程和数据健康状况
- en: Data pipelines
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管道
- en: 'Even with the most basic of analytics, we always require some data. In fact,
    finding the *right data* is probably among the hardest problems to solve in data
    science (but that''s a whole topic for another book!). We have already seen in
    the last chapter that the way in which we obtain our data can be as simple or
    complicated as is needed. In practice, we can break this decision down into two
    distinct areas: *ad hoc* and *scheduled*.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最基本的分析，我们总是需要一些数据。事实上，找到*正确的数据*可能是数据科学中最难解决的问题之一（但这是另一本书的整个主题！）。我们已经在上一章中看到，我们获取数据的方式可以简单或复杂，视情况而定。实际上，我们可以将这个决定分解为两个不同的领域：*临时*和*定期*。
- en: '**Ad hoc data acquisition**: is the most common method during prototyping and
    small scale analytics as it usually doesn''t require any additional software to
    implement. The user acquires some data and simply downloads it from source as
    and when required. This method is often a matter of clicking on a web link and
    storing the data somewhere convenient, although the data may still need to be
    versioned and secure.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**临时数据获取**：在原型设计和小规模分析中通常是最常见的方法，因为它通常不需要额外的软件来实现。用户获取一些数据，只需在需要时从源头下载。这种方法通常是点击一个网页链接并将数据存储在方便的地方，尽管数据可能仍然需要进行版本控制和安全保护。'
- en: '**Scheduled data acquisition**: is used in more controlled environments for
    large scale and production analytics; there is also an excellent case for ingesting
    a dataset into a data lake for possible future use. With the **Internet of Things**
    (**IoT**) on the increase, huge volumes of data are being produced in many cases,
    if the data is not ingested immediately it is lost forever. Much of this data
    may not have an apparent use today, but could have in the future; so the mindset
    is to gather all of the data in case it is needed and delete it later when we
    are sure it is not.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定期数据获取**：在大规模和生产分析的受控环境中使用；还有一个很好的理由将数据集摄入数据湖以备将来使用。随着**物联网**（**IoT**）的增长，在许多情况下产生了大量数据，如果数据不立即摄入，就会永远丢失。其中许多数据今天可能没有明显的用途，但将来可能会有用；因此，心态是收集所有数据以防需要时使用，并在确定不需要时稍后删除它。'
- en: It's clear we need a flexible approach to data acquisition that supports a variety
    of procurement options.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们需要一种灵活的数据获取方法，支持各种采购选项。
- en: Universal ingestion framework
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用摄入框架
- en: There are many ways to approach data acquisition, ranging from home-grown bash
    scripts through to high-end commercial tools. The aim of this section is to introduce
    a highly-flexible framework that we can use for small-scale data ingest, and then
    grow as our requirements change all the way through to a full, corporately-managed
    workflow if needed. That framework will be built using **Apache NiFi**. NiFi enables
    us to build large-scale, integrated data pipelines that move data around the planet.
    In addition, it's also incredibly flexible and easy to build simple pipelines
    usually quicker even than using bash or any other traditional scripting method.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以处理数据获取，从自制的bash脚本到高端商业工具。本节的目的是介绍一个高度灵活的框架，我们可以用于小规模数据摄入，然后根据需求的变化扩展到完整的公司管理工作流程。这个框架将使用**Apache
    NiFi**构建。NiFi使我们能够构建大规模的集成数据管道，将数据传输到全球各地。此外，它也非常灵活，易于构建简单的管道，通常甚至比使用bash或任何其他传统脚本方法更快。
- en: Note
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If an ad hoc approach is taken to source the same dataset on a number of occasions,
    then some serious thought should be given as to whether it falls into the scheduled
    category, or at least whether a more robust storage and versioning setup should
    be introduced.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在多次采集同一数据集时采用了临时方法，那么应认真考虑是否应将其归类为定期类别，或者至少是否应引入更健壮的存储和版本控制设置。
- en: We have chosen to use Apache NiFi as it offers a solution that provides the
    ability to create many pipelines of varying complexity that can be scaled to truly
    big data and IoT levels, and it also provides a great drag and drop interface
    (using what's known as *flow-based programming*  [*https://en.wikipedia.org/wiki/Flow-based_programming*](https://en.wikipedia.org/wiki/Flow-based_programming)
    ). With patterns, templates, and modules for workflow production, it automatically
    takes care of many of the complex features that traditionally plague developers
    such as multithreading, connection management, and scalable processing. For our
    purposes, it will enable us to quickly build simple pipelines for prototyping,
    and scale these to full production where required.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用Apache NiFi，因为它提供了一个解决方案，可以创建许多不同复杂度的管道，并且可以扩展到真正的大数据和物联网级别，并且还提供了一个出色的拖放界面（使用所谓的*基于流的编程*
    [*https://en.wikipedia.org/wiki/Flow-based_programming*](https://en.wikipedia.org/wiki/Flow-based_programming)）。通过工作流生产的模式、模板和模块，它自动处理了许多传统上困扰开发人员的复杂功能，如多线程、连接管理和可扩展处理。对于我们的目的，它将使我们能够快速构建简单的原型管道，并在需要时将其扩展到完整的生产环境。
- en: 'It''s pretty well documented and easy to get running by following the information
    on [https://nifi.apache.org/download.html](https://nifi.apache.org/download.html).
    It runs in a browser and looks like this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 它已经被很好地记录下来，并且可以通过遵循[https://nifi.apache.org/download.html](https://nifi.apache.org/download.html)上的信息来轻松运行。它在浏览器中运行，看起来像这样：
- en: '![Universal ingestion framework](img/image_02_001.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![通用摄入框架](img/image_02_001.jpg)'
- en: We leave the installation of NiFi as an exercise for the reader, which we would
    encourage you to do as we will be using it in the following section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将NiFi的安装留给读者自己去练习，我们鼓励您这样做，因为我们将在接下来的部分中使用它。
- en: Introducing the GDELT news stream
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍GDELT新闻流
- en: 'Hopefully, we have NiFi up and running now and can start to ingest some data.
    So, let''s start with some global news media data from GDELT. Here''s our brief,
    taken from the GDELT website, [http://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/](http://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 希望现在我们已经启动并运行了NiFi，并且可以开始摄入一些数据。因此，让我们从GDELT获取一些全球新闻媒体数据。以下是我们从GDELT网站[http://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/](http://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/)中摘取的简要信息：
- en: '*"Within 15 minutes of GDELT monitoring a news report breaking anywhere the
    world, it has translated it, processed it to identify all events, counts, quotes,
    people, organizations, locations, themes, emotions, relevant imagery, video, and
    embedded social media posts, placed it into global context, and made all of this
    available via a live open metadata firehose enabling open research on the planet
    itself.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在GDELT监控世界各地的新闻报道发布后的15分钟内，它已经将其翻译并加工处理，以识别所有事件、计数、引用、人物、组织、地点、主题、情感、相关图像、视频和嵌入式社交媒体帖子，并将其放入全球背景中，并通过实时开放的元数据火线提供所有这些内容，从而实现对地球本身的开放研究。
- en: '*[As] the single largest deployment in the world of sentiment analysis, we
    hope that by bringing together so many emotional and thematic dimensions crossing
    so many languages and disciplines, and applying all of it in realtime to breaking
    news from across the planet, that this will spur an entirely new era in how we
    think about emotion and the ways in which it can help us better understand how
    we contextualize, interpret, respond to, and understand global events."*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为世界上最大的情感分析部署，我们希望通过将跨越多种语言和学科的许多情感和主题维度实时应用于来自全球各地的突发新闻，从而激发我们对情感的全新时代，以及它如何帮助我们更好地理解我们如何情境化、解释、回应和理解全球事件的方式。
- en: Quite a challenging remit I think you'd agree! Therefore, rather than delay,
    pausing to specify the details here, let's get going straight away. We'll introduce
    the aspects of GDELT as we use them throughout the coming chapters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是一个相当具有挑战性的任务！因此，与其拖延，暂停在这里指定细节，不如立即开始。我们将在接下来的章节中使用GDELT的各个方面。
- en: In order to start consuming this open data, we'll need to hook into that metadata
    firehose and ingest the news streams onto our platform. How do we do this? Let's
    start by finding out what data is available.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始使用这些开放数据，我们需要连接到元数据火线并将新闻流引入我们的平台。我们该如何做呢？让我们首先找出可用的数据。
- en: Discovering GDELT in real-time
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时发现GDELT
- en: GDELT publishes a list of the latest files on their website. This list is updated
    every 15 minutes. In NiFi, we can set up a dataflow that will poll the GDELT website,
    source a file from this list, and save it to HDFS so we can use it later.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GDELT在其网站上发布了最新文件的列表。该列表每15分钟更新一次。在NiFi中，我们可以设置一个数据流，该数据流将轮询GDELT网站，从此列表中获取文件，并将其保存到HDFS，以便我们以后使用。
- en: Inside the NiFi dataflow designer, create a HTTP connector by dragging a processor
    onto the canvas and selecting `GetHTTP` function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在NiFi数据流设计师中，通过将处理器拖放到画布上并选择`GetHTTP`功能来创建一个HTTP连接器。
- en: '![Discovering GDELT in real-time](img/image_02_002.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![实时发现GDELT](img/image_02_002.jpg)'
- en: 'To configure this processor, you''ll need to enter the URL of the file list
    as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置此处理器，您需要输入文件列表的URL，如下所示：
- en: '[http://data.gdeltproject.org/gdeltv2/lastupdate.txt](http://data.gdeltproject.org/gdeltv2/lastupdate.txt)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://data.gdeltproject.org/gdeltv2/lastupdate.txt](http://data.gdeltproject.org/gdeltv2/lastupdate.txt)'
- en: Also, provide a temporary filename for the file list you will download. In the
    example below, we've used NiFi's expression language to generate a universally
    unique key so that files are not overwritten (`UUID()`).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 还要为您将要下载的文件列表提供一个临时文件名。在下面的示例中，我们使用了NiFi的表达式语言来生成一个通用唯一键，以便不会覆盖文件（`UUID()`）。
- en: '![Discovering GDELT in real-time](img/image_02_003.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![实时发现GDELT](img/image_02_003.jpg)'
- en: It's worth noting that with this type of processor (`GetHTTP` method), NiFi
    supports a number of scheduling and timing options for the polling and retrieval.
    For now, we're just going to use the default options and let NiFi manage the polling
    intervals for us.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，对于这种类型的处理器（`GetHTTP`方法），NiFi支持多种调度和定时选项，用于轮询和检索。目前，我们将使用默认选项，让NiFi为我们管理轮询间隔。
- en: 'An example of the latest file list from GDELT is shown as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的GDELT文件列表示例如下：
- en: '![Discovering GDELT in real-time](img/ch-02-image.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![实时发现GDELT](img/ch-02-image.jpg)'
- en: Next, we will parse the URL of the GKG news stream so that we can fetch it in
    a moment. Create a regular expression parser by dragging a processor onto the
    canvas and selecting `ExtractText`. Now, position the new processor underneath
    the existing one and drag a line from the top processor to the bottom one. Finish
    by selecting the `success` relationship in the connection dialog that pops up.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解析GKG新闻流的URL，以便稍后可以获取它。通过将处理器拖放到画布上并选择`ExtractText`来创建一个正则表达式解析器。现在，将新处理器放在现有处理器下面，并从顶部处理器向底部处理器拖动一条线。最后，在弹出的连接对话框中选择`success`关系。
- en: 'This is shown in the following example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下示例中显示：
- en: '![Discovering GDELT in real-time](img/image_02_005.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![实时发现GDELT](img/image_02_005.jpg)'
- en: 'Next, let''s configure the `ExtractText` processor to use a regular expression
    that matches only the relevant text of the file list, for example:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们配置`ExtractText`处理器，使用一个只匹配文件列表相关文本的正则表达式，例如：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: From this regular expression, NiFi will create a new property (in this case,
    called `url`) associated with the flow design, which will take on a new value
    as each particular instance goes through the flow. It can even be configured to
    support multiple threads.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个正则表达式中，NiFi将创建一个新的属性（在本例中称为`url`），与流程设计相关联，每个特定实例通过流程时都会采用新值。它甚至可以配置为支持多个线程。
- en: 'Again, this is example is shown as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这个示例如下所示：
- en: '![Discovering GDELT in real-time](img/image_02_006.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![实时发现GDELT](img/image_02_006.jpg)'
- en: It's worth noting here, that while this is a fairly specific example, the technique
    is deliberately general purpose and can be used in many situations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，虽然这是一个相当具体的例子，但这种技术是故意通用的，可以在许多情况下使用。
- en: Our first GDELT feed
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的第一个GDELT数据源
- en: Now that we have the URL of the GKG feed, we fetch it by configuring an `InvokeHTTP`
    processor to use the `url` property we previously created as it's remote endpoint,
    and dragging the line as before.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了GKG数据源的URL，通过配置`InvokeHTTP`处理器来使用我们之前创建的`url`属性作为其远程端点，并像以前一样拖动线。
- en: '![Our first GDELT feed](img/image_02_007.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![我们的第一个GDELT数据源](img/image_02_007.jpg)'
- en: 'All that remains is to decompress the zipped content with an `UnpackContent`
    processor (using the basic `.zip` format) and save to HDFS using a `PutHDFS` processor,
    like so:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的就是使用`UnpackContent`处理器（使用基本的`.zip`格式）解压缩压缩内容，并使用`PutHDFS`处理器保存到HDFS，如下所示：
- en: '![Our first GDELT feed](img/image_02_008.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![我们的第一个GDELT数据源](img/image_02_008.jpg)'
- en: Improving with publish and subscribe
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改进发布和订阅
- en: 'So far, this flow looks very *point-to-point*, meaning that if we were to introduce
    a new consumer of data, for example, a Spark-streaming job, the flow must be changed.
    For example, the flow design might have to change to look like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这个流程看起来非常*点对点*，这意味着如果我们要引入新的数据消费者，例如Spark-streaming作业，流程必须更改。例如，流程设计可能必须更改为如下所示：
- en: '![Improving with publish and subscribe](img/image_02_009.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![改进发布和订阅](img/image_02_009.jpg)'
- en: If we add yet another, the flow must change again. In fact, each time we add
    a new consumer, the flow gets a little more complicated, particularly when all
    the error handling is added. This is clearly not always desirable, as introducing
    or removing consumers (or producers) of data, might be something we want to do
    often, even frequently. Plus, it's also a good idea to try to keep your flows
    as simple and reusable as possible.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再添加另一个，流程必须再次更改。事实上，每次添加新的消费者时，流程都会变得更加复杂，特别是当所有错误处理都添加进去时。显然，这并不总是可取的，因为引入或移除数据的消费者（或生产者）可能是我们经常甚至频繁想要做的事情。此外，尽可能保持流程简单和可重用也是一个好主意。
- en: Therefore, for a more flexible pattern, instead of writing directly to HDFS,
    we can publish to *Apache Kafka*. This gives us the ability to add and remove
    consumers at any time without changing the data ingestion pipeline. We can also
    still write to HDFS from Kafka if needed, possibly even by designing a separate
    NiFi flow, or connect directly to Kafka using the Spark-streaming.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了更灵活的模式，我们可以将数据发布到*Apache Kafka*，而不是直接写入HDFS。这使我们能够随时添加和移除消费者，而不必更改数据摄入管道。如果需要，我们还可以从Kafka向HDFS写入，甚至可以通过设计一个单独的NiFi流程，或者直接使用Spark-streaming连接到Kafka。
- en: To do this, we create a Kafka writer by dragging a processor onto the canvas
    and selecting `PutKafka`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们通过将处理器拖放到画布上并选择`PutKafka`来创建一个Kafka写入器。
- en: '![Improving with publish and subscribe](img/image_02_010.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![改进发布和订阅](img/image_02_010.jpg)'
- en: We now have a simple flow that continuously polls for an available file list,
    routinely retrieving the latest copy of a new stream over the web as it becomes
    available, decompressing the content, and streaming it record-by-record into Kafka,
    a durable, fault-tolerant, distributed message queue, for processing by the Spark-streaming
    or storage in HDFS. And what's more, without writing a single line of bash!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一个简单的流程，不断轮询可用文件列表，定期检索新流的最新副本，解压内容，并将其逐条记录流入Kafka，这是一个持久的、容错的、分布式消息队列，用于Spark-streaming处理或在HDFS中存储。而且，这一切都不需要写一行bash代码！
- en: Content registry
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容注册
- en: We have seen in this chapter that data ingestion is an area that is often overlooked,
    and that its importance cannot be underestimated. At this point, we have a pipeline
    that enables us to ingest data from a source, schedule that ingest, and direct
    the data to our repository of choice. But the story does not end there. Now we
    have the data, we need to fulfil our data management responsibilities. Enter the
    *content registry*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中看到，数据摄入是一个经常被忽视的领域，它的重要性不容小觑。在这一点上，我们有一个管道，使我们能够从源头摄入数据，安排摄入，并将数据定向到我们选择的存储库。但故事并不会在这里结束。现在我们有了数据，我们需要履行我们的数据管理责任。进入*内容注册*。
- en: We're going to build an index of metadata related to that data we have ingested.
    The data itself will still be directed to storage (HDFS, in our example) but,
    in addition, we will store metadata about the data, so that we can track what
    we've received and understand basic information about it, such as, when we received
    it, where it came from, how big it is, what type it is, and so on.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将建立一个与我们摄入的数据相关的元数据索引。数据本身仍将被定向到存储（在我们的示例中是HDFS），但另外，我们将存储有关数据的元数据，以便我们可以跟踪我们收到的数据，并了解一些基本信息，例如我们何时收到它，它来自哪里，它有多大，它是什么类型，等等。
- en: Choices and more choices
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择和更多选择
- en: 'The choice of which technology we use to store this metadata is, as we have
    seen, one based upon knowledge and experience. For metadata indexing, we will
    require at least the following attributes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的存储元数据的技术选择是基于知识和经验的。对于元数据索引，我们至少需要以下属性：
- en: Easily searchable
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于搜索
- en: Scalable
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的
- en: Parallel write ability
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行写入能力
- en: Redundancy
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冗余
- en: There are many ways to meet these requirements, for example we could write the
    metadata to Parquet, store in HDFS, and search using Spark SQL. However, here
    we will use *Elasticsearch* as it meets the requirements a little better, most
    notably because it facilitates low latency queries of our metadata over a REST
    API, very useful for creating dashboards. In fact, Elasticsearch has the advantage
    of integrating directly with **Kibana**, meaning it can quickly produce rich visualizations
    of our content registry. For this reason, we will proceed with Elasticsearch in
    mind.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多满足这些要求的方法，例如我们可以将元数据写入Parquet，存储在HDFS中，并使用Spark SQL进行搜索。然而，在这里，我们将使用*Elasticsearch*，因为它更好地满足了要求，特别是因为它通过REST
    API方便地进行低延迟查询我们的元数据，这对于创建仪表板非常有用。事实上，Elasticsearch具有与**Kibana**直接集成的优势，这意味着它可以快速生成我们内容注册的丰富可视化。因此，出于这个原因，我们将考虑使用Elasticsearch。
- en: Going with the flow
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随波逐流
- en: 'Using our current NiFi pipeline flow, let''s fork the output from "Fetch GKG
    files from URL" to add an additional set of steps to allow us to capture and store
    this metadata in Elasticsearch. These are:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们当前的NiFi管道流，让我们从“从URL获取GKG文件”中分叉输出，以添加一组额外的步骤，以允许我们在Elasticsearch中捕获和存储这些元数据。这些是：
- en: Replace the flow content with our metadata model.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用我们的元数据模型替换流内容。
- en: Capture the metadata.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 捕获元数据。
- en: Store directly in Elasticsearch.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接存储在Elasticsearch中。
- en: 'Here''s what this looks like in NiFi:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在NiFi中的样子如下：
- en: '![Going with the flow](img/image_02_011.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![随波逐流](img/image_02_011.jpg)'
- en: Metadata model
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 元数据模型
- en: 'So, the first step here is to define our metadata model. And there are many
    areas we could consider, but let''s select a set that helps tackle a few key points
    from earlier discussions. This will provide a good basis upon which further data
    can be added in the future, if required. So, let''s keep it simple and use the
    following three attributes:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里的第一步是定义我们的元数据模型。有许多方面可以考虑，但让我们选择一组可以帮助解决之前讨论中的一些关键问题的集合。如果需要，这将为将来进一步添加数据提供一个良好的基础。因此，让我们保持简单，使用以下三个属性：
- en: File size
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件大小
- en: Date ingested
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄取日期
- en: File name
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件名
- en: These will provide basic registration of received files.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些将提供接收文件的基本注册。
- en: 'Next, inside the NiFi flow, we''ll need to replace the actual data content
    with this new metadata model. An easy way to do this, is to create a JSON template
    file from our model. We''ll save it to local disk and use it inside a `FetchFile`
    processor to replace the flow''s content with this skeleton object. This template
    will look something like:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在NiFi流程中，我们需要用这个新的元数据模型替换实际的数据内容。一个简单的方法是，从我们的模型中创建一个JSON模板文件。我们将它保存到本地磁盘，并在`FetchFile`处理器中使用它，以用这个骨架对象替换流的内容。这个模板会看起来像这样：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note the use of placeholder names (`SIZE, FILENAME, DATE`) in place of the attribute
    values. These will be substituted, one-by-one, by a sequence of `ReplaceText`
    processors, that swap the placeholder names for an appropriate flow attribute
    using regular expressions provided by the NiFi Expression Language, for example
    `DATE` becomes `${now()}`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意在属性值的位置上使用了占位符名称（`SIZE, FILENAME, DATE`）。这些将逐个被一系列`ReplaceText`处理器替换，这些处理器使用NiFi表达式语言提供的正则表达式，例如`DATE`变成`${now()}`。
- en: The last step is to output the new metadata payload to Elasticsearch. Once again,
    NiFi comes ready with a processor for this; the `PutElasticsearch` processor.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将新的元数据负载输出到Elasticsearch。同样，NiFi已经准备了一个处理器来实现这一点；`PutElasticsearch`处理器。
- en: 'An example metadata entry in Elasticsearch:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch中的一个元数据条目示例：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have added the ability to collect and interrogate metadata, we
    now have access to more statistics that can be used for analysis. This includes:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经添加了收集和查询元数据的能力，我们现在可以访问更多可用于分析的统计数据。这包括：
- en: Time-based analysis, for example, file sizes over time
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于时间的分析，例如，随时间变化的文件大小
- en: Loss of data, for example, are there data holes in the timeline?
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据丢失，例如，时间轴上是否有数据空缺？
- en: If there is a particular analytic that is required, the NIFI metadata component
    can be adjusted to provide the relevant data points. Indeed, an analytic could
    be built to look at historical data and update the index accordingly if the metadata
    does not exist in current data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要特定的分析，NIFI元数据组件可以进行调整以提供相关的数据点。事实上，可以构建一个分析来查看历史数据，并根据需要更新索引，如果当前数据中不存在元数据。
- en: Kibana dashboard
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kibana 仪表板
- en: 'We have mentioned Kibana a number of times in this chapter. Now that we have
    an index of metadata in Elasticsearch, we can use the tool to visualize some analytics.
    The purpose of this brief section is to demonstrate that we can immediately start
    to model and visualize our data. To see Kibana used in a more complex scenario,
    have a look at [Chapter 9](ch09.xhtml "Chapter 9.  News Dictionary and Real-Time
    Tagging System") *, News Dictionary and Real-Time Tagging System*. In this simple
    example, we have completed the following steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们多次提到了Kibana。现在我们在Elasticsearch中有了元数据索引，我们可以使用该工具来可视化一些分析。这个简短的部分的目的是为了演示我们可以立即开始对数据进行建模和可视化。要查看Kibana在更复杂的场景中的使用，请参阅[第9章](ch09.xhtml
    "第9章。新闻词典和实时标记系统")*，新闻词典和实时标记系统*。在这个简单的示例中，我们完成了以下步骤：
- en: Added the Elasticsearch index for our GDELT metadata to the **Settings** tab.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**设置**选项卡中添加了我们的GDELT元数据的Elasticsearch索引。
- en: Selected file size under the **Discover** tab.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**发现**选项卡下选择文件大小。
- en: Selected **Visualize** for file size.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**可视化**以文件大小。
- en: Changed the `Aggregation` field to `Range`.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`聚合`字段更改为`范围`。
- en: Entered values for the ranges.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入范围的值。
- en: 'The resulting graph displays the file size distribution:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图显示了文件大小的分布：
- en: '![Kibana dashboard](img/image_02_012.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![Kibana 仪表板](img/image_02_012.jpg)'
- en: From here, we are free to create new visualizations or even a fully-featured
    dashboard that can be used to monitor the status of our file ingest. By increasing
    the variety of metadata written to Elasticsearch from NiFi, we can make more fields
    available in Kibana and even start our data science journey right here with some
    ingest-based actionable insights.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们可以自由地创建新的可视化，甚至是一个功能齐全的仪表板，用于监控我们文件摄取的状态。通过增加从NiFi写入Elasticsearch的元数据的多样性，我们可以在Kibana中提供更多字段，甚至可以从这里开始我们的数据科学之旅，获得一些基于摄取的可操作见解。
- en: Now that we have a fully-functioning data pipeline delivering us real-time feeds
    of data, how do we ensure data quality of the payload we are receiving? Let's
    take a look at the options.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个完全运行的数据管道，为我们提供实时数据源，那么我们如何确保接收到的数据质量呢？让我们看看有哪些选项。
- en: Quality assurance
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 质量保证
- en: With an initial data ingestion capability implemented, and data streaming onto
    your platform, you will need to decide how much quality assurance is required
    at the "front door". It's perfectly viable to start with no initial quality controls
    and build them up over time (retrospectively scanning historical data as time
    and resources allow). However, it may be prudent to install a basic level of verification
    to begin with. For example, basic checks such as file integrity, parity checking,
    completeness, checksums, type checking, field counting, overdue files, security
    field pre-population, denormalization, and so on.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实施了初始数据摄取能力，并将数据流入平台后，您需要决定“前门”需要多少质量保证。可以完全没有初始质量控制，并随着时间和资源的允许逐渐建立起来（对历史数据进行回顾扫描）。但是，最好从一开始就安装基本的验证。例如，基本检查，如文件完整性、奇偶校验、完整性、校验和、类型检查、字段计数、过期文件、安全字段预填充、去规范化等。
- en: You should take care that your up-front checks do not take too long. Depending
    on the intensity of your examinations and the size of your data, it's not uncommon
    to encounter a situation where there is not enough time to perform all processing
    before the next dataset arrives. You will always need to monitor your cluster
    resources and calculate the most efficient use of time.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该注意，您的前期检查不要花费太长时间。根据您的检查强度和数据的大小，遇到无法在下一个数据集到达之前完成所有处理的情况并不罕见。您始终需要监视您的集群资源，并计算最有效的时间利用方式。
- en: 'Here are some examples of the types of rough capacity planning calculations
    you can perform:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些粗略容量规划计算的示例：
- en: Example 1 - Basic quality checking, no contending users
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例1 - 基本质量检查，没有竞争用户
- en: Data is ingested every 15 minutes and takes 1 minute to pull from the source
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据每15分钟摄取一次，从源头拉取需要1分钟
- en: Quality checking (integrity, field count, field pre-population) takes 4 minutes
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质量检查（完整性、字段计数、字段预填充）需要4分钟
- en: There are no other users on the compute cluster
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算集群上没有其他用户
- en: '*There are 10 minutes of resources available for other tasks.*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*其他任务*有10分钟的资源可用。'
- en: As there are no other users on the cluster, this is satisfactory - no action
    needs to be taken.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于集群上没有其他用户，这是令人满意的-不需要采取任何行动。
- en: Example 2 - Advanced quality checking, no contending users
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例2 - 高级质量检查，没有竞争用户
- en: Data is ingested every 15 minutes and takes 1 minute to pull from the source
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据每15分钟摄取一次，从源头拉取需要1分钟
- en: Quality checking (integrity, field count, field pre-population, denormalization,
    sub dataset building) takes 13 minutes
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质量检查（完整性、字段计数、字段预填充、去规范化、子数据集构建）需要13分钟
- en: There are no other users on the compute cluster
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算集群上没有其他用户
- en: '*There is only 1 minute of resource available* for *other tasks.*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*其他任务*只有1分钟的资源可用。'
- en: 'You probably need to consider, either:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要考虑：
- en: Configuring a resource scheduling policy
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置资源调度策略
- en: Reducing the amount of data ingested
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少摄取的数据量
- en: Reducing the amount of processing we undertake
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少我们进行的处理量
- en: Adding additional compute resources to the cluster
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向集群添加额外的计算资源
- en: Example 3 - Basic quality checking, 50% utility due to contending users
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例3 - 基本质量检查，由于竞争用户，效用度为50%
- en: Data is ingested every 15 minutes and takes 1 minute to pull from the source
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据每15分钟摄取一次，从源头拉取需要1分钟
- en: Quality checking (integrity, field count, field pre-population) takes 4 minutes
    (100% utility)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质量检查（完整性、字段计数、字段预填充）需要4分钟（100%效用）
- en: There are other users on the compute cluster
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算集群上有其他用户
- en: '*There are 6 minutes of resources available for other tasks (15 - 1 - (4 *
    (100 / 50))). Since there are other users, there is a danger that, at least some
    of the time, we will not be able to complete our processing and a backlog of jobs
    will occur.*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*其他任务有6分钟的资源可用（15-1-(4 *（100/50）)）。由于还有其他用户，存在无法完成处理并出现作业积压的危险。*'
- en: 'When you run into timing issues, you have a number of options available to
    you in order to circumvent any backlog:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当遇到时间问题时，您有多种选择可以避免任何积压：
- en: Negotiating sole use of the resources at certain times
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些时候协商独占资源的使用权
- en: 'Configuring a resource scheduling policy, including:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置资源调度策略，包括：
- en: 'YARN fair scheduler: allows you to define queues with differing priorities
    and target your Spark jobs by setting the `spark.yarn.queue` property on start-up
    so your job always takes precedence'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YARN公平调度程序：允许您定义具有不同优先级的队列，并通过在启动时设置`spark.yarn.queue`属性来定位您的Spark作业，以便始终优先考虑您的作业
- en: 'Dynamic resource allocation: allows concurrently running jobs to automatically
    scale to match their utilization'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态资源分配：允许同时运行的作业自动扩展以匹配它们的利用率
- en: 'Spark scheduler pool: allows you to define queues when sharing a `SparkContext`
    using a multithreading model, and target your Spark job by setting the `spark.scheduler.pool`
    property per execution thread so your thread takes precedence'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark调度程序池：允许您在使用多线程模型共享`SparkContext`时定义队列，并通过设置`spark.scheduler.pool`属性来定位您的Spark作业，以便每个执行线程都优先考虑
- en: Running processing jobs overnight when the cluster is quiet
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群安静时过夜运行处理作业
- en: In any case, you will eventually get a good idea of how the various parts to
    your jobs perform and will then be in a position to calculate what changes could
    be made to improve efficiency. There's always the option of throwing more resources
    at the problem, especially when using a cloud provider, but we would certainly
    encourage the intelligent use of existing resources - this is far more scalable,
    cheaper, and builds data expertise.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，您最终会对作业的各个部分的表现有一个很好的了解，然后就能够计算出可以提高效率的改变。在使用云提供商时，总是有增加更多资源的选项，但我们当然鼓励对现有资源进行智能利用-这样更具可扩展性，更便宜，并且建立数据专业知识。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we walked through the full setup of an Apache NiFi GDELT ingest
    pipeline, complete with metadata forks and a brief introduction to visualizing
    the resulting data. This section is particularly important as GDELT is used extensively
    throughout the book and the NiFi method is a highly effective way to source data
    in a scalable and modular way.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细介绍了Apache NiFi GDELT摄取管道的完整设置，包括元数据分支和对生成数据的简要介绍。本节非常重要，因为GDELT在整本书中被广泛使用，而NiFi方法是一种可扩展和模块化的数据来源方法。
- en: In the next chapter, we will get to grips with what to do with the data once
    it's landed, by looking at schemas and formats.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习一旦数据到达后该如何处理数据，包括查看模式和格式。
