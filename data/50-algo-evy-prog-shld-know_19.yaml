- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Practical Considerations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际考虑因素
- en: There are a bunch of algorithms presented in this book that can be used to solve
    real-world problems. In this chapter, we’ll examine the practicality of the algorithms
    presented in this book. Our focus will be on their real-world applicability, potential
    challenges, and overarching themes, including utility and ethical implications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中介绍了许多可用于解决现实世界问题的算法。在这一章中，我们将探讨本书中算法的实用性。我们的重点将放在它们的现实世界适用性、潜在挑战和整体主题上，包括效用和伦理影响。
- en: This chapter is organized as follows. We will start with an introduction. Then,
    we will present the issues around the explainability of an algorithm, which is
    the degree to which the internal mechanics of an algorithm can be explained in
    understandable terms. Then, we will present the ethics of using an algorithm and
    the possibility of creating biases when implementing them. Next, the techniques
    for handling NP-hard problems will be discussed. Finally, we will investigate
    factors that should be considered before choosing an algorithm.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的组织结构如下：我们将从引言开始。接着，我们将介绍算法可解释性的问题，即算法的内部机制能够以可理解的术语进行解释的程度。然后，我们将讨论使用算法的伦理问题以及实施算法时可能产生的偏见。接下来，我们将讨论处理
    NP-hard 问题的技术。最后，我们将研究在选择算法之前应考虑的因素。
- en: By the end of this chapter, you will have learned about the practical considerations
    that are important to keep in mind when using algorithms to solve real-world problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解在使用算法解决现实世界问题时，必须牢记的实际考虑因素。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing practical considerations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入实际考虑因素
- en: The explainability of an algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法的可解释性
- en: Understanding ethics and algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解伦理与算法
- en: Reducing bias in models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型中的偏差
- en: When to use algorithms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时使用算法
- en: Let’s start with some of the challenges facing algorithmic solutions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些算法解决方案面临的挑战开始。
- en: Challenges facing algorithmic solutions
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法解决方案面临的挑战
- en: In addition to designing, developing, and testing an algorithm, in many cases,
    it is important to consider certain practical aspects of starting to rely on a
    machine to solve a real-world problem. For certain algorithms, we may need to
    consider ways to reliably incorporate new, important information that is expected
    to keep changing even after we have deployed our algorithm. For example, the unexpected
    disruption of global supply chains may negate some of the assumptions we used
    to train a model predicting the profit margins for a product. We need to carefully
    consider whether incorporating this new information will change the quality of
    our well-tested algorithm in any way. If so, how is our design going to handle
    it?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设计、开发和测试算法外，在许多情况下，考虑开始依赖机器来解决现实世界问题时的某些实际因素也是非常重要的。对于某些算法，我们可能需要考虑如何可靠地加入新信息，而这些信息预计在我们部署算法之后仍会持续变化。例如，全球供应链的突发中断可能会使我们用于训练模型以预测产品利润率的某些假设失效。我们需要仔细考虑是否加入这些新信息会以某种方式改变我们经过充分测试的算法的质量。如果会，那么我们的设计如何处理这种变化？
- en: Expecting the unexpected
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预见意外情况
- en: Most solutions to real-world problems developed using algorithms are based on
    some assumptions. These assumptions may unexpectedly change after the model has
    been deployed. Some algorithms use assumptions that may be affected by changing
    global geo-political situations. For example, consider a trained model that predicts
    the financial profit for an international company with offices all over the world.
    An unexpected disruptive event like a war or the spread of a sudden deadly virus
    may result in fundamentally changing the assumptions for this model and the quality
    of predictions. For such use cases, the advice is to “expect the unexpected” and
    strategize for surprises. For certain data-driven models, the surprise may come
    from changes in the regulatory policies after the solution has been deployed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多使用算法开发的现实世界问题解决方案都是基于某些假设的。这些假设在模型部署后可能会意外地发生变化。一些算法使用的假设可能会受到全球地缘政治情况变化的影响。例如，考虑一个训练好的模型，它预测一家在全球各地都有办事处的国际公司的财务利润。像战争或突如其来的致命病毒蔓延这样的意外破坏事件，可能会根本性地改变这个模型的假设和预测的质量。对于这种应用场景，建议是“预见意外”并为意外情况做好应对策略。对于某些数据驱动的模型，意外可能来自于解决方案部署后法规政策的变化。
- en: When we are using algorithms to solve a real-world problem, we are, in a way,
    relying on machines for problem-solving. Even the most sophisticated algorithms
    are based on simplification and assumptions and cannot handle surprises. We are
    still not even close to fully handing over critical decision-making to algorithms
    we’ve designed ourselves.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用算法来解决现实世界的问题时，某种程度上，我们是在依赖机器来进行问题解决。即使是最复杂的算法也都建立在简化和假设的基础上，无法应对突发情况。我们仍然离将关键决策完全交给我们自己设计的算法的目标相距甚远。
- en: For example, Google’s recommendation engine algorithms have recently faced the
    European Union’s regulatory restrictions due to privacy concerns. These algorithms
    may be some of the most advanced in their field but if banned, these algorithms
    may turn out to be useless as they won’t be used to solve the problems they were
    supposed to tackle.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，谷歌的推荐引擎算法由于隐私问题，最近面临欧盟的监管限制。这些算法可能是该领域最先进的技术之一，但如果被禁止，这些算法可能会变得毫无用处，因为它们将无法解决它们原本应解决的问题。
- en: But, the truth of the matter is that, unfortunately, the practical considerations
    of an algorithm are still after-thoughts that are not usually considered at the
    initial design phase.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，事实是，遗憾的是，算法的实际考虑往往是事后的思考，通常在初期设计阶段并未得到充分的考虑。
- en: For many use cases, once an algorithm is deployed and the short-term excitement
    of providing the solution is over, the practical aspects and implications of using
    an algorithm will be discovered over time and will define the success or failure
    of the project.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多用例来说，一旦算法被部署，短期内的解决方案带来的兴奋感过去后，使用算法的实际问题和影响将在时间的推移中被发现，并最终决定项目的成败。
- en: Let’s look into a practical example where not paying attention to the practical
    considerations failed a high-profile project designed by one of the best IT companies
    in the world.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一个实际的例子，分析一下没有关注实际考虑导致一个全球顶尖IT公司设计的高调项目失败的原因。
- en: Failure of Tay, the Twitter AI bot
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tay的失败，推特AI机器人
- en: Let’s present the classical example of Tay, which was presented as the first-ever
    AI Twitter bot created by Microsoft in 2016\. Using an AI algorithm, Tay was trained
    as an automated Twitter bot capable of responding to tweets about a particular
    topic. To achieve that, it had the capability of constructing simple messages
    using its existing vocabulary by sensing the context of the conversation. Once
    deployed, it was supposed to keep learning from real-time online conversations
    and by augmenting its vocabulary of the words used often in important conversations.
    After living in cyberspace for a couple of days, Tay started learning new words.
    In addition to some new words, unfortunately, Tay picked up some words from the
    racism and rudeness of ongoing tweets. It soon started using newly learned words
    to generate tweets of its own. A tiny minority of these tweets were offensive
    enough to raise a red flag. Although it exhibited intelligence and quickly learned
    how to create customized tweets based on real-time events, as designed, at the
    same time, it seriously offended people. Microsoft took it offline and tried to
    re-tool it, but that did not work. Microsoft had to eventually kill the project.
    That was the sad end of an ambitious project.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个经典的例子——Tay，这是微软在2016年创建的首个AI推特机器人。通过使用AI算法，Tay被训练成一个能够根据特定话题回复推文的自动化推特机器人。为了实现这一目标，它具备根据对话上下文构建简单消息的能力，利用现有的词汇。一旦部署，它本应从实时的在线对话中不断学习，并通过增强其词汇库，吸收在重要对话中频繁使用的单词。然而，Tay在网络空间中生活了几天后，开始学习新词汇。除了学习了一些新词，遗憾的是，Tay还从正在进行的推文中学到了一些种族主义和粗俗的词语。它很快开始使用新学到的词语发布自己的推文。尽管其中绝大多数推文并无恶意，但其中少数推文足够冒犯人们，迅速引发了警报。尽管它展现出了智能，并且迅速学会了如何根据实时事件创作定制的推文，但与此同时，它也严重冒犯了人们。微软将其下线并尝试进行重新调整，但未能成功。最终，微软不得不终止了这个项目。这是一个雄心勃勃的项目的悲惨结局。
- en: Note that although the intelligence built into it by Microsoft was impressive,
    the company ignored the practical implications of deploying a self-learning Twitter
    bot. The NLP and machine learning algorithms may have been best in class, but
    due to the obvious shortcomings, it was practically a useless project. Today,
    Tay has become a textbook example of a failure due to ignoring the practical implications
    of allowing algorithms to learn on the fly. The lessons learned by the failure
    of Tay definitely influenced the AI projects of later years. Data scientists also
    started paying more attention to the transparency of algorithms.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管微软在其内置的人工智能方面取得了令人印象深刻的成绩，但公司忽视了部署自学习Twitter机器人的实际影响。尽管自然语言处理和机器学习算法可能是业内最先进的，但由于显而易见的缺陷，该项目几乎是毫无用处的。如今，Tay已经成为一个由于忽视让算法实时学习的实际后果而导致失败的教科书式例子。Tay的失败所带来的教训无疑影响了后来的AI项目。数据科学家们也开始更加关注算法的透明性。
- en: 'To delve deeper, here’s a comprehensive study on Tay: [https://spectrum.ieee.org/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of
    -online-conversation](https://spectrum.ieee.org/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入了解，以下是关于Tay的全面研究：[https://spectrum.ieee.org/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of
    -online-conversation](https://spectrum.ieee.org/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation)。
- en: That brings us to the next topic, which explores the need for and ways to make
    algorithms transparent.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了下一个话题，探讨了为何需要让算法透明以及如何实现这一点。
- en: The explainability of an algorithm
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法的可解释性
- en: 'First, let us differentiate between a black box and a white box algorithm:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们区分黑盒算法和白盒算法：
- en: A black box algorithm is one whose logic is not interpretable by humans either
    due to its complexity or due to its logic being represented in a convoluted manner.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑盒算法是指其逻辑无法被人类解释的算法，原因可能是其复杂性或其逻辑以复杂的方式呈现。
- en: A white box algorithm is one whose logic is visible and understandable to a
    human.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白盒算法是指其逻辑对人类可见且可以理解的算法。
- en: Explainability in the context of machine learning refers to our capacity to
    grasp and articulate the reasons behind an algorithm’s specific outputs. In essence,
    it gauges how comprehensible an algorithm’s inner workings and decision pathways
    are to human cognition.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，可解释性指的是我们理解和表达算法特定输出背后原因的能力。从本质上讲，它衡量的是一个算法的内部工作原理和决策路径对于人类认知的可理解程度。
- en: Many algorithms, especially within the machine learning sphere, are often termed
    “black box” due to their opaque nature. For instance, consider neural networks,
    which we delve into in *Chapter 8*, *Neural Network Algorithms*. These algorithms,
    which underpin many deep learning applications, are quintessential examples of
    black box models. Their complexity and multi-layered structures make them inherently
    non-intuitive, rendering their inner decision-making processes enigmatic to human
    understanding.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法，特别是在机器学习领域，由于其不透明性，通常被称为“黑盒”。例如，考虑我们在*第8章*中讨论的神经网络算法，*神经网络算法*。这些算法是许多深度学习应用的基础，是典型的黑盒模型。它们的复杂性和多层结构使得它们本质上不直观，导致其内部决策过程对人类理解来说是谜一样的。
- en: However, it’s crucial to note that the terms “black box” and “white box” are
    definitive categorizations, indicating either complete opacity or transparency,
    respectively. It’s not a gradient or spectrum where an algorithm can be somewhat
    black or somewhat white. Current research is fervently aimed at rendering these
    black box algorithms, like neural networks, more transparent and explainable.
    Yet, due to their intricate architecture, they remain predominantly in the black
    box category.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须注意，“黑盒”和“白盒”这两个术语是明确的分类，分别表示完全不透明和完全透明。它们不是一个渐变或光谱，一个算法不可能是“有点黑”或“有点白”。当前的研究正致力于让这些黑盒算法（如神经网络）变得更加透明和可解释。然而，由于其复杂的架构，它们仍然主要属于黑盒类别。
- en: If algorithms are used for critical decision-making, it may be important to
    understand the reasons behind the results generated by the algorithm. Avoiding
    black box algorithms and using white box ones instead also provides better insights
    into the inner workings of the model. The decision tree algorithm discussed in
    *Chapter 7*, *Traditional Supervised Learning Algorithms*, is an example of such
    white box algorithms. For example, an explainable algorithm will guide doctors
    as to which features were actually used to classify patients as sick or not. If
    the doctor has any doubts about the results, they can go back and double-check
    those particular features for accuracy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果算法用于关键决策中，理解算法生成结果的原因可能是非常重要的。避免使用黑箱算法，转而使用白箱算法，也能更好地洞察模型的内部运作。*第七章*中讨论的决策树算法，*传统监督学习算法*，就是这种白箱算法的一个例子。例如，一个可解释的算法将指导医生了解哪些特征实际被用来将病人分类为生病或未生病。如果医生对结果有任何疑问，他们可以回去重新检查这些特征的准确性。
- en: Machine learning algorithms and explainability
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习算法和可解释性
- en: In the realm of machine learning, the concept of explainability is paramount.
    But what exactly do we mean by explainability? At its core, explainability refers
    to the clarity with which we can understand and interpret a machine learning model’s
    decisions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，可解释性的概念至关重要。那么，到底我们指的是什么是可解释性呢？从本质上讲，可解释性指的是我们能够理解和解释机器学习模型决策的清晰度。
- en: It’s about pulling back the curtain on a model’s predictions and understanding
    the “why” behind them.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着揭开模型预测背后的面纱，理解其背后的“原因”。
- en: When leveraging machine learning, especially in decision-making scenarios, individuals
    often need to place trust in a model’s output. This trust can be significantly
    amplified if the model’s processes and decisions are transparent and justifiable.
    To illustrate the significance of explainability, let’s consider a real-world
    scenario.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在利用机器学习，特别是在决策场景中，个人往往需要信任模型的输出。如果模型的过程和决策是透明的并且可以解释的，那么这种信任可以得到显著增强。为了说明可解释性的重要性，让我们考虑一个现实场景。
- en: Let’s assume that we want to use machine learning to predict the prices of houses
    in the Boston area based on their characteristics. Let’s also assume that local
    city regulations will allow us to use machine learning algorithms only if we can
    provide detailed information for the justification of any predictions whenever
    needed. This information is needed for audit purposes to make sure that certain
    segments of the housing market are not artificially manipulated. Making our trained
    model explainable will provide this additional information.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想利用机器学习根据房屋的特征预测波士顿地区的房价。假设当地的城市规定允许我们使用机器学习算法，但前提是每当需要时，我们必须提供详细的信息来证明任何预测的合理性。这些信息是审计的需要，以确保住房市场的某些细分不会被人为操控。让我们的训练模型具有可解释性将提供这些额外的信息。
- en: Let’s look into different options that are available for implementing the explainability
    of our trained model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解实现已训练模型可解释性的不同选项。
- en: Presenting strategies for explainability
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释性策略展示
- en: 'For machine learning, there are fundamentally two strategies to provide explainability
    to algorithms:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习，提供算法可解释性的基本策略有两种：
- en: '**A global explainability strategy**: This is to provide the details of the
    formulation of a model as a whole. For example, let us consider the case of a
    machine learning model used to approve or refuse loans for individuals for a major
    bank. A global explainability strategy can be used to quantify the transparency
    of the decisions of this model. The global explainability strategy is not about
    the transparency of individual decisions but about the transparency of aggregated
    trends. Let us say that if there is speculation in the press about gender bias
    in this model, a global explainability strategy will provide the necessary information
    to validate or negate the speculation.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全球可解释性策略**：这是指提供整个模型制定过程的细节。例如，我们可以考虑一个用于批准或拒绝个人贷款的机器学习模型的案例。可以使用全球可解释性策略来量化该模型决策的透明度。全球可解释性策略并不是针对单个决策的透明度，而是关于整体趋势的透明度。假设媒体对该模型中的性别偏见进行猜测，全球可解释性策略将提供必要的信息来验证或否定这一猜测。'
- en: '**A local explainability strategy**: This is to provide the rationale for a
    single individual prediction made by our trained model. The aim is to provide
    transparency for each individual decision. For instance, consider our earlier
    example of predicting house prices in the Boston area. If a homeowner questions
    why their house was valued at a specific price by the model, a local explainability
    strategy would provide the detailed reasoning behind that specific valuation,
    offering clarity on the various factors and weights that contributed to the estimate.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部可解释策略**：这是为了提供由我们的训练模型做出的单一预测的依据。其目的是为每个单独的决策提供透明度。例如，考虑我们之前的例子，预测波士顿地区的房价。如果一位房主质疑为什么他们的房子被模型评估为特定价格，那么局部可解释策略将提供有关该具体估价的详细推理，明确指出各种因素及其权重，帮助理解模型如何做出该预测。'
- en: For global explainability, we have techniques such as **Testing with Concept
    Activation Vectors** (**TCAV**), which is used to provide explainability for image
    classification models. TCAV depends on calculating directional derivatives to
    quantify the degree of the relationship between a user-defined concept and the
    classification of pictures. For example, it will quantify how sensitive a prediction
    of classifying a person as male is to the presence of facial hair in the picture.
    There are other global explainability strategies, such as partial dependence plots
    and calculating the permutation importance, which can help explain the formulations
    in our trained model. Both global and local explainability strategies can either
    be model-specific or model-agnostic. Model-specific strategies apply to certain
    types of models, whereas model-agnostic strategies can be applied to a wide variety
    of models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于全局可解释性，我们有一些技术，如**使用概念激活向量进行测试**（**TCAV**），它用于为图像分类模型提供可解释性。TCAV通过计算方向导数来量化用户定义的概念与图像分类之间的关系程度。例如，它会量化分类一个人是男性的预测对面部毛发存在的敏感度。还有其他全局可解释性策略，例如部分依赖图和计算排列重要性，它们有助于解释我们训练模型中的公式。全局和局部可解释性策略可以是模型特定的，也可以是模型无关的。模型特定策略适用于某些类型的模型，而模型无关策略可以应用于各种不同的模型。
- en: 'The following diagram summarizes the different strategies available for machine
    learning explainability:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示总结了可用于机器学习可解释性的不同策略：
- en: '![Diagram  Description automatically generated](img/B18046_16_01.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B18046_16_01.png)'
- en: 'Figure 16.1: Machine learning explainability approaches'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.1：机器学习可解释性方法
- en: Now, let’s look at how we can implement explainability using one of these strategies.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看如何使用这些策略之一实现可解释性。
- en: Implementing explainability
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现可解释性
- en: '**Local Interpretable Model-Agnostic Explanations** (**LIME**) is a model-agnostic
    approach that can explain individual predictions made by a trained model. Being
    model-agnostic, it can explain the predictions of most types of trained machine
    learning models.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**局部可解释模型无关解释**（**LIME**）是一种模型无关的方法，可以解释经过训练的模型所做出的单个预测。由于其模型无关的特性，它可以解释大多数类型训练过的机器学习模型的预测。'
- en: LIME explains decisions by inducing small changes to the input for each instance.
    It can gather the effects on the local decision boundary for that instance. It
    iterates over the loop to provide details for each variable. Looking at the output,
    we can see which variable has the most influence on that instance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LIME通过对每个实例的输入进行小幅度变化来解释决策。它可以收集这些变化对该实例的局部决策边界的影响。它会遍历循环，提供每个变量的详细信息。通过查看输出，我们可以看到哪个变量对该实例的影响最大。
- en: 'Let’s see how we can use LIME to make the individual predictions of our house
    price model explainable:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用LIME使我们的房价预测模型的个别预测变得可解释：
- en: 'If you have never used `LIME` before, you need to install the package using
    `pip`:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你之前从未使用过`LIME`，你需要使用`pip`安装该软件包：
- en: '[PRE0]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, let’s import the Python packages that we need:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们导入我们需要的Python包：
- en: '[PRE1]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will train a model that can predict housing prices in a particular city.
    For that, we will first import the dataset that is stored in the `housing.pkl`
    file. Then, we will explore the features it has:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将训练一个模型，该模型可以预测某个特定城市的房价。为此，我们首先将导入存储在`housing.pkl`文件中的数据集。然后，我们将探索它所包含的特征：
- en: '[PRE2]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Based on these features, we need to predict the price of a home.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于这些特征，我们需要预测一套房屋的价格。
- en: 'Now, let’s train the model. We will be using a random forest regressor to train
    the model. First, we divide the data into testing and training partitions and
    then we use it to train the model:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们训练模型。我们将使用随机森林回归器来训练模型。首先，我们将数据分为测试集和训练集，然后使用它来训练模型：
- en: '[PRE4]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, let us identify the category columns:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们识别类别列：
- en: '[PRE6]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let’s instantiate the LIME explainer with the required configuration parameters.
    Note that we are specifying that our label is `''price''`, representing the prices
    of houses in Boston:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用所需的配置参数来实例化 LIME 解释器。请注意，我们指定我们的标签是`'price'`，表示波士顿的房价：
- en: '[PRE7]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let us try to look into the details of predictions. For that, first, let us
    import `pyplot` as the plotter from `matplotlib`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试深入了解预测的细节。为此，首先让我们从`matplotlib`导入`pyplot`作为绘图工具：
- en: '[PRE8]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/B18046_16_02.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_16_02.png)'
- en: 'Figure 16.2: Feature-wise explanation of a Housing Price Prediction'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.2：房价预测的特征逐项解释
- en: 'As the LIME explainer works on individual predictions, we need to choose the
    predictions we want to analyze. We have asked the explainer for its justification
    of the predictions indexed as `1` and `35`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 LIME 解释器是针对个别预测工作的，我们需要选择要分析的预测。我们已要求解释器对索引为`1`和`35`的预测提供理由：
- en: '[PRE9]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Chart, waterfall chart  Description automatically generated](img/B18046_16_03.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图表，瀑布图 描述自动生成](img/B18046_16_03.png)'
- en: 'Figure 16.3: Highlighting key features: Dissecting predictions for test instances
    1 and 35'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.3：突出显示关键特征：解读测试实例 1 和 35 的预测
- en: 'Let’s try to analyze the preceding explanation by LIME, which tells us the
    following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试分析 LIME 之前的解释，它告诉我们以下内容：
- en: '**The list of features used in the individual predictions**: They are indicated
    on the *y*-axis in the preceding screenshot.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在个别预测中使用的特征列表**：它们在前面的截图中标示在*y*轴上。'
- en: '**The relative importance of the features in determining the decision**: The
    larger the bar line, the greater the importance is. The value of the number is
    on the *x*-axis.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征在决定中的相对重要性**：条形线越大，重要性越大。数字的值显示在*x*轴上。'
- en: '**The positive or negative influence of each of the input features on the label**:
    Red bars show a negative influence and green bars show the positive influence
    of a particular feature.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个输入特征对标签的正负影响**：红色条表示负面影响，绿色条表示特定特征的正面影响。'
- en: Understanding ethics and algorithms
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解伦理学与算法
- en: 'Algorithmic ethics, also known as computational ethics, delves into the moral
    dimensions of algorithms. This crucial domain aims to ensure that machines operating
    on these algorithms uphold ethical standards. The development and deployment of
    algorithms may inadvertently foster unethical outcomes or biases. Crafting algorithms
    poses a challenge in anticipating their full range of ethical repercussions. When
    we discuss large-scale algorithms in this context, we refer to those processing
    vast volumes of data. However, the complexity is magnified when multiple users
    or designers contribute to an algorithm, introducing varied human biases. The
    overarching goal of algorithmic ethics is to spotlight and address concerns arising
    in these areas:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 算法伦理学，也称为计算伦理学，探讨算法的道德维度。这个关键领域的目标是确保基于这些算法运行的机器遵守伦理标准。算法的开发和部署可能无意中导致不道德的结果或偏见。设计算法时，预测其所有道德后果是一项挑战。当我们在这个背景下讨论大规模算法时，指的是那些处理大量数据的算法。然而，当多个用户或设计者共同参与算法的设计时，复杂性进一步增加，因为这会引入不同的人为偏见。算法伦理学的总体目标是突出并解决这些领域中出现的问题：
- en: '**Bias and discrimination**: There are many factors that can affect the quality
    of the solutions created by algorithms. One of the major concerns is unintentional
    and algorithmic bias. The reason may be the design of the algorithm resulting
    in giving some data more importance than other data. Or, the reason may be in
    the data collection and selection of data. It may result in data points that should
    be computed by the algorithm being omitted, or data that shouldn’t be involved
    being included. An example is the use of an algorithm by an insurance company
    to calculate risk. It may be using data about car accidents that includes the
    gender of drivers involved. Based on the data available, the algorithm might decide
    that women are involved in more crashes and therefore women drivers automatically
    receive higher-cost insurance quotes.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏见与歧视**：有许多因素会影响算法所创建的解决方案的质量。一个主要的关注点是无意中的算法偏见。其原因可能在于算法的设计导致某些数据比其他数据更重要。或者，原因可能出在数据的收集和选择上。这可能会导致应由算法计算的数据被遗漏，或者本不应包含的数据被纳入。例如，一家保险公司使用算法计算风险时，可能会使用包含驾驶者性别的车祸数据。根据现有的数据，算法可能会认为女性驾驶员涉及更多的事故，因此女性驾驶员自动获得更高费用的保险报价。'
- en: '**Privacy**: The data used by algorithms may have personal information and
    may be used in ways that could intrude on the privacy of individuals. For example,
    the algorithms that enable facial recognition are one example of the privacy issues
    that are caused by the use of algorithms. Currently, many cities and airports
    are using facial recognition systems around the world. The challenge is to use
    these algorithms in a way that protects individuals against any privacy breaches.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私**：算法使用的数据可能包含个人信息，并且可能以侵犯个人隐私的方式被使用。例如，启用面部识别的算法就是隐私问题的一个例子。目前，全球许多城市和机场都在使用面部识别系统。挑战在于如何以保护个人隐私不受侵犯的方式使用这些算法。'
- en: More and more companies are making the ethical analysis of an algorithm part
    of its design. But the truth is that problems may not become apparent until we
    find a problematic use case.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的公司将算法的伦理分析纳入其设计过程中。但事实是，问题可能直到我们发现某个有问题的应用场景时才会显现出来。
- en: Problems with learning algorithms
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习算法的问题
- en: Algorithms capable of fine-tuning themselves according to changing data patterns
    are called learning algorithms. They are in learning mode in real time, but this
    real-time learning capability may have ethical implications. This creates the
    possibility that their learning could result in decisions that may have problems
    from an ethical point of view. As they are created to be in a continuous evolutionary
    phase, it is almost impossible to continuously perform an ethical analysis of
    them.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 能够根据变化的数据模式进行自我调整的算法被称为学习算法。它们在实时的学习模式中工作，但这种实时学习能力可能带来伦理上的问题。这就有可能导致它们的学习过程做出从伦理角度来看有问题的决策。由于它们被设计为处于持续进化的阶段，几乎不可能对它们进行持续的伦理分析。
- en: For example, let us study the problem that Amazon discovered in the learning
    algorithm they designed to hire people. Amazon started using an AI algorithm for
    hiring employees in 2015\. Before it was deployed, it went through stringent testing
    to make sure that it met both functional and non-functional requirements and had
    no bias or any other ethical issue. As it was designed as a learning algorithm,
    it was constantly fine-tuning itself with new data as it became available. A couple
    of weeks after it was deployed, Amazon discovered that the AI algorithm had surprisingly
    developed a gender bias. Amazon took the algorithm offline and investigated. It
    was found that gender bias was introduced due to some specific patterns in the
    new data. Specifically, in recent data, there were far more men than women. And
    the men in the recent data happened to have a more relevant background for the
    job posted. The real-time fine-tuning learning had some unintentional consequences
    and resulted in the algorithm starting to favor men over women, thus introducing
    bias. The algorithm started using gender as one of the deciding factors for hiring.
    The model was re-trained and necessary safety guardrails were added to make sure
    that gender bias was not re-introduced.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们研究一下亚马逊在其招聘算法中发现的问题。亚马逊从2015年开始使用AI算法来招聘员工。在部署之前，它经过了严格的测试，以确保其满足功能和非功能要求，并且没有任何偏见或其他伦理问题。由于它是一个学习算法，因此随着新数据的不断出现，它会不断地自我调优。部署几周后，亚马逊发现AI算法意外地发展出了性别偏见。亚马逊将算法下线并进行了调查。结果发现，性别偏见是由于新数据中的一些特定模式引入的。具体而言，最近的数据中男性数量远多于女性，而这些男性恰好有更相关的背景适合该职位。实时的自我调优学习带来了一些无意的后果，导致算法开始偏向男性而非女性，从而引入了偏见。该算法开始将性别作为招聘的决定因素之一。之后，模型重新训练，并添加了必要的安全防护措施，确保不会再次引入性别偏见。
- en: As the complexity of algorithms grows, it is becoming more and more difficult
    to fully understand their long-term implications for individuals and groups within
    society.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 随着算法复杂性的增加，全面理解它们对社会中个人和群体的长期影响变得越来越困难。
- en: Understanding ethical considerations
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解伦理考量
- en: Algorithmic solutions are mathematical formulations. It is the responsibility
    of the people responsible for developing algorithms to ensure that they conform
    to ethical sensitivities around the problem we are trying to solve. Once the solutions
    are deployed, they may need to be periodically monitored to make sure that they
    do not start creating ethical issues as new data becomes available and underlying
    assumptions shift.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 算法解决方案是数学公式。开发算法的人员有责任确保其符合我们试图解决问题的伦理敏感性。一旦解决方案部署，它们可能需要定期监控，以确保随着新数据的到来和基础假设的变化，算法不会开始产生伦理问题。
- en: 'These ethical considerations of algorithms depend on the type of the algorithm.
    For example, let’s look into the following algorithms and their ethical considerations.
    Some examples of powerful algorithms for which careful ethical considerations
    are needed are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法的伦理考量取决于算法的类型。例如，让我们来看一下以下算法及其伦理考量。需要仔细考虑伦理问题的一些强大算法示例如下：
- en: Both classification and regression algorithms serve distinct purposes in machine
    learning. Classification algorithms categorize data into predefined classes and
    can be directly instrumental in decision-making processes. For instance, they
    might determine visa approvals or identify specific demographics in a city. On
    the other hand, regression algorithms predict numerical values based on input
    data, and these predictions can indeed be utilized in decision-making. For example,
    a regression model might predict the optimal price for listing a house on the
    market. In essence, while classification offers categorical outcomes, regression
    provides quantitative predictions; both are valuable for informed decision-making
    in varied scenarios.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类算法和回归算法在机器学习中各自有不同的用途。分类算法将数据分为预定义的类别，并且可以直接用于决策过程。例如，它们可能决定签证审批或识别城市中的特定人群。另一方面，回归算法基于输入数据预测数值，这些预测确实可以用于决策。例如，回归模型可能预测在市场上列出房子的最佳价格。本质上，分类提供了类别结果，而回归提供了定量预测；两者在不同场景下都对知情决策有价值。
- en: Algorithms, when used in recommendation engines, can match resumes to job seekers,
    both for individuals and groups. For such use cases, the algorithms should implement
    the explainability at both a local and global level. Local-level explainability
    will provide traceability for a particular individual resume when matched to available
    jobs. Global-level explainability will provide transparency of the overall logic
    being used to match resumes to jobs.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法在推荐引擎中的应用可以将简历与求职者匹配，无论是针对个人还是群体。对于这种使用场景，算法应该在局部和全局层面都实现可解释性。局部层面的可解释性会提供特定个人简历与可用职位匹配时的可追溯性。全局层面的可解释性则提供了匹配简历和职位所使用的整体逻辑的透明度。
- en: Data mining algorithms can be used to mine information about individuals from
    various data sources that may be used by governments for decision-making. For
    example, the Chicago police department uses data mining algorithms to identify
    criminal hotspots and high-risk individuals in the city. Making sure these data
    mining algorithms are designed and used in a way that satisfies all requirements
    related to ethics is done through careful design and constant monitoring.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据挖掘算法可以用来从各种数据源中挖掘关于个人的信息，这些信息可能被政府用来进行决策。例如，芝加哥警察局使用数据挖掘算法来识别城市中的犯罪热点和高风险个体。确保这些数据挖掘算法的设计和使用符合所有伦理要求，需要通过精心设计和持续监控来实现。
- en: Hence, the ethical consideration of algorithms will depend on the particular
    use case they are used in and the entities they directly or indirectly affect.
    Careful analysis is needed from an ethical point of view before starting to use
    an algorithm for critical decision-making. These ethical considerations should
    be part of the design process.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，算法的伦理考量将取决于其使用的具体场景以及它们直接或间接影响的实体。在开始使用算法进行关键决策之前，需要从伦理角度进行仔细分析。这些伦理考量应当是设计过程的一部分。
- en: Factors affecting algorithmic solutions
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响算法解决方案的因素
- en: The following are the factors that we should keep in mind while performing an
    analysis of how good algorithmic solutions are.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在分析算法解决方案优劣时应当牢记的因素。
- en: Considering inconclusive evidence
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 考虑不确定证据
- en: In machine learning, the quality and breadth of your dataset play crucial roles
    in the accuracy and reliability of your model’s outcomes. Often, data might appear
    limited or may lack the comprehensive depth required to provide a conclusive result.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，数据集的质量和广度在模型结果的准确性和可靠性中起着至关重要的作用。通常，数据可能显得有限，或者缺乏提供决定性结果所需的全面深度。
- en: 'For instance, let’s consider clinical trials: if a new drug is tested on a
    small group of people, the results might not comprehensively reflect its efficacy.
    Similarly, if we examine patterns of fraud in a particular postal code of a city,
    limited data might suggest a trend that isn’t necessarily accurate on a broader
    scale.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑临床试验：如果一款新药在一小部分人群中进行测试，结果可能无法全面反映其疗效。同样，如果我们在某个城市的特定邮政编码区域检查欺诈模式，有限的数据可能会暗示一个趋势，但这个趋势在更广泛的范围内并不一定准确。
- en: It’s essential to differentiate between “limited data” and “inconclusive evidence.”
    While most datasets are inherently limited (no dataset can capture every single
    possibility), the term ‘inconclusive evidence’ refers to data that doesn’t provide
    a clear or definitive trend or outcome. This distinction is vital as basing decisions
    on inconclusive patterns could lead to errors in judgment. Always approach decision-making
    with a critical eye, especially when working with algorithms trained on such data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于区分“有限数据”和“不确定证据”。虽然大多数数据集本质上都是有限的（没有数据集能捕捉到所有可能性），但“不确定证据”是指数据未能提供明确或决定性的趋势或结果。这一区分至关重要，因为基于不确定模式做出的决策可能会导致判断错误。特别是在使用基于此类数据训练的算法时，决策时必须保持批判性的眼光。
- en: Decisions that are based on inconclusive evidence are prone to lead to unjustified
    actions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 基于不确定证据做出的决策容易导致不合理的行动。
- en: Traceability
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可追溯性
- en: Machine learning algorithms typically have separate development and production
    environments. This can potentially create a disconnection between the training
    phase and the inference phase. It means that if there is some harm caused by an
    algorithm, it is very hard to trace and debug. Also, when a problem is found in
    an algorithm, it is difficult to actually determine the people who were affected
    by it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法通常有单独的开发和生产环境。这可能导致训练阶段与推理阶段之间的脱节。这意味着，如果算法造成了某种伤害，追踪和调试非常困难。而且，当算法发现问题时，实际上很难确定受到影响的人群。
- en: Misguided evidence
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 误导性证据
- en: Algorithms are data-driven formulations. The **Garbage-in, Garbage-out** (**GIGO**)
    principle means that results from algorithms can only be as reliable as the data
    on which they are based. If there are biases in the data, they will be reflected
    in the algorithms as well.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 算法是数据驱动的公式。**垃圾进，垃圾出**（**GIGO**）原则意味着算法的结果只会与其所基于的数据一样可靠。如果数据中存在偏见，那么这些偏见也会反映在算法中。
- en: Unfair outcomes
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不公平的结果
- en: The use of algorithms may result in harming vulnerable communities and groups
    that are already at a disadvantage.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的使用可能会对已经处于不利地位的弱势群体造成伤害。
- en: Additionally, the use of algorithms to distribute research funding has been
    proven on more than one occasion to be biased toward the male population. Algorithms
    used for granting immigration are sometimes unintentionally biased toward vulnerable
    population groups.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用算法来分配研究资金已经被证明多次对男性群体存在偏见。用于移民审批的算法有时无意间对弱势群体存在偏见。
- en: Despite using high-quality data and complex mathematical formulations, if the
    result is an unfair outcome, the whole effort may bring more harm than benefit.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用高质量的数据和复杂的数学公式，如果结果是不公平的，整个努力可能带来的伤害大于收益。
- en: Let us look into how we can reduce the bias in models.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何减少模型中的偏差。
- en: Reducing bias in models
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少模型中的偏差
- en: As we have discussed, the bias in a model is about certain attributes of a particular
    algorithm that cause it to create unfair outcomes. In the current world, there
    are known, well-documented general biases based on gender, race, and sexual orientation.
    It means that the data we collect is expected to exhibit those biases unless we
    are dealing with an environment where an effort has been made to remove them before
    collecting the data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所讨论的，模型中的偏见是指特定算法的某些属性导致它产生不公平的结果。在当今世界，基于性别、种族和性取向的偏见是已知的，并且有文献记载。这意味着我们收集的数据可能会表现出这些偏见，除非我们在收集数据之前做出了努力，去消除这些偏见。
- en: 'Most of the time, bias in algorithms is directly or indirectly introduced by
    humans. Humans introduce bias either unintentionally through negligence or intentionally
    through subjectivity. One of the reasons for human bias is the fact that the human
    brain is vulnerable to cognitive bias, which reflects a person’s own subjectivity,
    beliefs, and ideology in both the data process and logic creation process of an
    algorithm. Human bias can be reflected either in data used by the algorithm or
    in the formulation of the algorithm itself. For a typical machine learning project
    following the **CRISP-DM** (short for **Cross-Industry Standard Process**) lifecycle,
    which was explained in *Chapter 5*, *Graph Algorithms*, the bias looks like the
    following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，算法中的偏见是由人类直接或间接引入的。人类通过疏忽无意中引入偏见，或者通过主观性故意引入偏见。人类偏见的一个原因是人类大脑容易受到认知偏见的影响，这种偏见反映了一个人在数据处理和算法逻辑创建过程中的主观性、信仰和意识形态。人类偏见可以反映在算法使用的数据中，也可以反映在算法本身的制定中。对于一个典型的机器学习项目，遵循**CRISP-DM**（即**跨行业标准过程**）生命周期，正如在*第5章*中所解释的，*图算法*，偏见通常呈现如下：
- en: '![Diagram  Description automatically generated](img/B18046_16_04.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18046_16_04.png)'
- en: 'Figure 16.4: Bias can be introduced in different phases of the CRISP-DM lifecycle'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：偏见可以在CRISP-DM生命周期的不同阶段被引入
- en: The trickiest part of reducing bias is to first identify and locate unconscious
    bias.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 减少偏差最棘手的部分是首先识别和定位潜在的无意识偏见。
- en: Let us look into when to use algorithms.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看何时使用算法。
- en: When to use algorithms
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用算法
- en: 'Algorithms are like tools in a practitioner’s toolbox. First, we need to understand
    which tool is the best one to use under the given circumstances. Sometimes, we
    need to ask ourselves whether we even have a solution for the problem we are trying
    to solve and when the right time to deploy our solution is. We need to determine
    whether the use of an algorithm can provide a solution to a real problem that
    is actually useful, rather than the alternative. We need to analyze the effect
    of using the algorithm in terms of three aspects:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 算法就像是从业者工具箱中的工具。首先，我们需要了解在给定的情况下，哪种工具是最合适的。有时，我们需要问自己，是否已经有解决我们正在尝试解决的问题的方案，何时是部署解决方案的最佳时机。我们需要确定，使用算法是否能提供一个实际有用的解决方案，而不是替代方案。我们需要从三个方面分析使用算法的效果：
- en: '**Cost**: Can use justify the cost related to the effort of implementing the
    algorithm?'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：使用算法的成本是否值得？'
- en: '**Time**: Does our solution make the overall process more efficient than simpler
    alternatives?'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间**：我们的解决方案是否使整体过程比更简单的替代方案更高效？'
- en: '**Accuracy**: Does our solution produce more accurate results than simpler
    alternatives?'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：我们的解决方案是否比更简单的替代方案产生更准确的结果？'
- en: 'To choose the right algorithm, we need to find the answers to the following
    questions:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的算法时，我们需要找到以下问题的答案：
- en: Can we simplify the problem by making assumptions?
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否通过做出假设来简化问题？
- en: How will we evaluate our algorithm?
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将如何评估我们的算法？
- en: What are the key metrics?
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键的度量标准是什么？
- en: How will it be deployed and used?
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将如何被部署和使用？
- en: Does it need to be explainable?
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要被解释吗？
- en: Do we understand the three important non-functional requirements—security, performance,
    and availability?
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否理解三项重要的非功能性需求——安全性、性能和可用性？
- en: Is there any expected deadline?
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有预期的截止日期？
- en: Upon selecting an algorithm based on the above-mentioned criteria, it’s worth
    considering that while most events or challenges can be anticipated and addressed,
    there are exceptions that defy our traditional understanding and predictive capabilities.
    Let us look into this in more detail.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在根据上述标准选择算法后，值得考虑的是，尽管大多数事件或挑战都可以预见并加以解决，但仍然有一些例外事件，它们违背了我们传统的理解和预测能力。让我们更详细地探讨这一点。
- en: Understanding black swan events and their implications on algorithms
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解黑天鹅事件及其对算法的影响
- en: In the realm of data science and algorithmic solutions, certain unpredictable
    and rare events can present unique challenges. Coined by Nassim Taleb in “Fooled
    by Randomness” (2001), the term “black swan event” metaphorically represents such
    rare and unpredictable occurrences.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学和算法解决方案领域，一些不可预测且罕见的事件可能会带来独特的挑战。“黑天鹅事件”这一术语由纳西姆·塔勒布在《随机的愚弄》（2001）中提出，形象地代表了那些罕见且不可预测的事件。
- en: 'To qualify as a black swan event, it must satisfy the following criteria:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要被认为是黑天鹅事件，它必须满足以下标准：
- en: '**Unexpectedness**: The event surprises most observers, like the atomic bomb
    drop on Hiroshima.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**意外性**：该事件令大多数观察者感到惊讶，就像广岛原子弹轰炸那样。'
- en: '**Magnitude**: The event is disruptive and significant, akin to the Spanish
    flu outbreak.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件的重大性**：该事件具有颠覆性和重大意义，就像西班牙流感的爆发。'
- en: '**Post-event predictability**: After the event, it becomes apparent that if
    clues had been noted, the event could’ve been anticipated, like overlooked signs
    before the Spanish flu became a pandemic.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件后可预测性**：在事件发生后，很明显，如果之前注意到某些线索，事件是可以预见的，就像在西班牙流感成为大流行之前被忽视的迹象。'
- en: '**Not a surprise to all**: Some individuals might’ve anticipated the event,
    as the scientists involved in the Manhattan Project did with the atomic bomb.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并非所有人都感到惊讶**：一些人可能早已预见到事件，就像参与曼哈顿计划的科学家们预见到原子弹的爆炸一样。'
- en: Before black swans were first discovered in the wild, for centuries, they were
    used to represent something that could not happen. After their discovery, the
    term remained popular but there was a change in what it represented. It now represents
    something so rare that it cannot be predicted.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在黑天鹅事件首次在野外被发现之前，几个世纪以来，它们一直用来代表一些不可能发生的事情。发现之后，这个词依然流行，但其代表的意义发生了变化。它现在代表的是一些极其罕见，无法预测的事件。
- en: '**Challenges and opportunities for algorithms with black swan events**:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**黑天鹅事件对算法的挑战和机会**：'
- en: '**Forecasting dilemmas**: While there are numerous forecasting algorithms,
    from ARIMA to deep learning methodologies, predicting a black swan event remains
    elusive. Using standard techniques might provide a false sense of security. Predicting
    the exact timing of another event, like COVID-19, for example, is fraught with
    challenges due to insufficient historical data.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测困境**：虽然有许多预测算法，从 ARIMA 到深度学习方法，但预测黑天鹅事件仍然是一个难题。使用标准技术可能会给人一种虚假的安全感。例如，预测像
    COVID-19 这样的事件的确切发生时间，由于历史数据不足，面临着许多挑战。'
- en: '**Predicting implications**: Once a black swan event occurs, foreseeing its
    broad societal impacts is complex. We may lack both the relevant data for the
    algorithms and an understanding of societal interrelations affected by the event.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测影响**：一旦黑天鹅事件发生，预测其广泛的社会影响就变得复杂。我们可能缺乏相关的数据和对事件影响下的社会关系的理解。'
- en: '**Predictive potential**: While black swan events appear random, they result
    from overlooked complex precursors. Herein lies an opportunity for algorithms:
    devising strategies to predict and detect these precursors might help anticipate
    a potential black swan event.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测潜力**：虽然黑天鹅事件看似随机，但它们通常是由于被忽视的复杂前兆所引起的。算法在此提供了机会：制定预测和检测这些前兆的策略，可能有助于预见潜在的黑天鹅事件。'
- en: '**The relevance of a practical application**:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**实际应用的相关性**：'
- en: Let’s consider the recent COVID-19 pandemic, a prime black swan event. A potential
    practical application might involve leveraging data on prior pandemics, global
    travel patterns, and local health metrics. An algorithm could then monitor for
    unusual spikes in illness or other potential early indicators, signaling a potential
    global health threat. However, the uniqueness of black swan events makes this
    challenging.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑最近的 COVID-19 大流行，这是一个典型的黑天鹅事件。一种潜在的实际应用可能涉及利用先前大流行的相关数据、全球旅行模式和当地健康指标。然后，一个算法可以监控疾病的异常激增或其他潜在的早期迹象，提示可能的全球健康威胁。然而，黑天鹅事件的独特性使得这一过程变得更加困难。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the practical aspects that should be considered
    while designing algorithms. We looked into the concept of algorithmic explainability
    and the various ways we can provide it at different levels. We also looked into
    the potential ethical issues in algorithms. Finally, we described which factors
    to consider while choosing an algorithm.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了设计算法时应考虑的实际方面。我们探讨了算法可解释性的概念，以及如何在不同层次上提供可解释性。我们还探讨了算法中可能出现的伦理问题。最后，我们描述了在选择算法时需要考虑的因素。
- en: Algorithms are engines in the new automated world that we are witnessing today.
    It is important to learn about, experiment with, and understand the implications
    of using algorithms. Understanding their strengths and limitations and the ethical
    implications of using algorithms will go a long way in making this world a better
    place to live in, and this book is an effort to achieve this important goal in
    this ever-changing and evolving world.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 算法是我们今天所见证的这个新自动化世界中的引擎。了解、实验和理解使用算法的影响至关重要。理解它们的优点和局限性，以及使用算法的伦理影响，将对改善我们生活的世界产生深远的影响，本书的目标就是在这个不断变化和发展的世界中实现这一重要目标。
- en: Learn more on Discord
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的 Discord 社区——在这里你可以分享反馈、向作者提问并了解新版本——请扫描下面的二维码：
- en: '[https://packt.link/WHLel](https://packt.link/WHLel)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/WHLel](https://packt.link/WHLel)'
- en: '![](img/QR_Code1955211820597889031.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1955211820597889031.png)'
