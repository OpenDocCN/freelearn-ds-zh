- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Data Profiling – Understanding Data Structure, Quality, and Distribution
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析 – 理解数据结构、质量和分布
- en: '**Data profiling** refers to scrutinizing, understanding, and validating datasets
    to learn more about their underlying structure, patterns, and quality. It is a
    critical step in data management and ingestion as it can enhance data quality
    and accuracy and ensure compliance with regulatory standards. In this chapter,
    you will learn how to perform profiling with different tools and how to change
    your tactics as the data volume increases.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据分析**指的是对数据集进行细致检查、理解和验证，从中了解其潜在的结构、模式和质量。这是数据管理和数据摄取中的关键步骤，因为它能提高数据质量和准确性，并确保符合监管标准。在本章中，你将学习如何使用不同的工具进行数据分析，并了解如何随着数据量的增加调整策略。'
- en: 'In this chapter, we will deep dive into the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨以下主题：
- en: Understanding data profiling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据分析
- en: Data profiling with the pandas profiler
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pandas profiler 进行数据分析
- en: Data validation with Great Expectations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用《远大前程》进行数据验证
- en: Comparing Great Expectations and the pandas profiler – when to use what
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较《远大前程》和 pandas profiler – 何时使用哪一个
- en: How to profile big data volumes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何分析大数据量
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need to install a Python interpreter that can be
    downloaded and installed using the instructions given here: [https://www.python.org/downloads/](https://www.python.org/downloads/).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要安装 Python 解释器，可以通过以下链接下载并按照说明进行安装：[https://www.python.org/downloads/](https://www.python.org/downloads/)。
- en: 'You can find all the code for the chapter in the following GitHub repository:
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下 GitHub 仓库找到本章的所有代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03)。
- en: Understanding data profiling
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据分析
- en: If you have never heard of **data profiling** before starting this chapter,
    it is a comprehensive process that involves analyzing and examining data from
    various sources to gain insights into the structure, quality, and overall characteristics
    of a dataset. Let’s start by describing the main goals of data profiling.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在开始本章之前，你从未听说过 **数据分析**，它是一个全面的过程，涉及分析和检查来自不同来源的数据，以深入了解数据集的结构、质量和整体特征。让我们从描述数据分析的主要目标开始。
- en: Identifying goals of data profiling
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分析目标识别
- en: Data profiling helps us understand the structure and quality of the data. As
    a result, we can get a better idea of the best way to organize the different datasets,
    identify potential data integration challenges, assess data quality, and identify
    and address issues that may affect the reliability and trustworthiness of the
    data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析帮助我们理解数据的结构和质量。因此，我们可以更好地了解如何组织不同的数据集，识别潜在的数据整合问题，评估数据质量，并识别和解决可能影响数据可靠性和可信度的问题。
- en: Let’s deep dive into the three main goals of data profiling.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨数据分析的三个主要目标。
- en: Data structure
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据结构
- en: One of the main goals of data profiling is to understand the data’s structure.
    This entails examining the data types, formats, and relationships between different
    data fields.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析的主要目标之一是理解数据的结构。这包括检查数据类型、格式和不同数据字段之间的关系。
- en: 'Here’s an example of a simple table structure. Consider a table named `Employee`
    that stores information about employees in a company:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的表格结构示例。考虑一个名为 `Employee` 的表格，存储公司员工的信息：
- en: '| `EmployeeID` | `FirstName` | `LastName` | `Position` | `Department` | `Salary`
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `EmployeeID` | `FirstName` | `LastName` | `Position` | `Department` | `Salary`
    |'
- en: '| `1` | `John` | `Doe` | `Software Eng` | `IT` | `75000` |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `John` | `Doe` | `软件工程师` | `IT` | `75000` |'
- en: '| `2` | `Jane` | `Smith` | `Data Analyst` | `Analytics` | `60000` |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `2` | `Jane` | `Smith` | `数据分析师` | `分析` | `60000` |'
- en: '| `3` | `Bob` | `Johnson` | `Project Manager` | `Project Management` | `85000`
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `3` | `Bob` | `Johnson` | `项目经理` | `项目管理` | `85000` |'
- en: 'Let’s break this table down:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分析一下这个表格：
- en: '`EmployeeID`: A unique identifier for each employee'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EmployeeID`: 每个员工的唯一标识符'
- en: '`FirstName` and `LastName` are columns storing the first and last names of
    employees'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FirstName` 和 `LastName` 是存储员工姓名的字段'
- en: '`Position`: The job title or position of the employee'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Position`: 员工的职位或职称'
- en: '`Department`: The department in which the employee works'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Department`: 员工所在的部门'
- en: '`Salary`: The salary of the employee'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Salary`：员工的薪资'
- en: This table structure is organized into rows and columns. Each row represents
    a specific employee, and each column represents a different attribute or piece
    of information about the employee. The table structure allows for easy querying,
    filtering, and joining of data. The values in each column adhere to a specific
    data type (e.g., integer, string, etc.), and relationships between tables can
    be established using keys.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格结构按行和列进行组织。每一行代表一个特定的员工，每一列代表员工的不同属性或信息。表格结构便于查询、过滤和连接数据。每一列中的值遵循特定的数据类型（例如整数、字符串等），并且可以通过键建立表格之间的关系。
- en: This is a simplified example, but in real-world scenarios, tables can have more
    columns and complex relationships.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简化的示例，但在实际应用中，表格可能包含更多列和复杂的关系。
- en: Data quality
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据质量
- en: '**Data quality** involves evaluating the overall reliability and trustworthiness
    of the data. Through data profiling, we can identify various data quality problems,
    including duplicate records, incorrect or inconsistent values, missing values,
    and outliers. By quantifying these issues, organizations gain an understanding
    of the extent to which the data can be trusted and relied upon for analysis.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据质量**涉及评估数据的整体可靠性和可信度。通过数据分析，我们可以识别出各种数据质量问题，包括重复记录、错误或不一致的值、缺失值和异常值。通过量化这些问题，组织能够了解数据在分析中可以信赖和依赖的程度。'
- en: Data distribution
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据分布
- en: Understanding data distribution within each field or column is another key objective
    of data profiling. By analyzing data distribution, organizations gain insights
    into patterns, frequencies, and anomalies present in the data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 理解每个字段或列中的数据分布是数据分析的另一个关键目标。通过分析数据分布，组织可以深入了解数据中的模式、频率和异常。
- en: 'Let’s imagine that we are working for an e-commerce company and we are collecting
    daily sales revenue data. By examining the distribution, we can identify trends
    in sales:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们为一家电子商务公司工作，收集每天的销售收入数据。通过检查数据分布，我们可以识别出销售趋势：
- en: '![Figure 3.1 – Distribution of daily sales revenue](img/B19801_03_1.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 每日销售收入分布](img/B19801_03_1.jpg)'
- en: Figure 3.1 – Distribution of daily sales revenue
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 每日销售收入分布
- en: In this histogram, we can see that the sales data follows a normal distribution,
    indicating that data near the mean is more frequent in occurrence than data far
    from the mean. In this way, we can understand the mean daily sales we can expect
    on a regular day.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个直方图中，我们可以看到销售数据呈正态分布，说明靠近均值的数据比远离均值的数据出现的频率更高。通过这种方式，我们可以了解在正常情况下每天的平均销售额。
- en: Now that we understand what challenges data profiling can deal with, let’s see
    the different ways you can go about performing data profiling.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了数据分析可以解决的挑战，让我们来看看你可以通过哪些不同的方式进行数据分析。
- en: Exploratory data analysis options – profiler versus manual
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析选项 – 分析工具与手动方法
- en: When performing **exploratory data analysis** (**EDA**), there are different
    approaches you can take to understand your data, including conducting manual analysis
    or using a profiler.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行**探索性数据分析（EDA）**时，你可以采取不同的方法来理解你的数据，包括进行手动分析或使用分析工具。
- en: '**Manual EDA** involves writing custom code or using general-purpose data analysis
    libraries (e.g., pandas in Python) to explore the data. It gives you more flexibility
    and control over the analysis process. You can customize the analysis based on
    your specific requirements and questions. Manual EDA allows for more in-depth
    exploration, including custom calculations, feature engineering, and advanced
    visualizations. It can be beneficial when dealing with complex data or when you
    have specific domain knowledge that you want to apply to the analysis.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**手动探索性数据分析（EDA）**涉及编写自定义代码或使用通用数据分析库（例如 Python 中的 pandas）来探索数据。它给予你更多的灵活性和对分析过程的控制。你可以根据具体需求和问题定制分析。手动EDA允许更深入的探索，包括自定义计算、特征工程和高级可视化。当处理复杂数据或你拥有特定领域知识时，手动EDA会很有帮助。'
- en: A **profiler** is a tool or library specifically designed for analyzing and
    summarizing data. It automates many EDA tasks and provides quick insights into
    the data’s structure, summary statistics, missing values, data types, and distributions.
    It can save you time by automating repetitive tasks and providing a comprehensive
    overview of the dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**分析工具**是专门用于分析和总结数据的工具或库。它自动化了许多EDA任务，并提供数据结构、汇总统计、缺失值、数据类型和分布的快速洞察。它通过自动化重复性任务并提供数据集的全面概览，节省了时间。'
- en: 'Let’s see more in detail when to use what:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解何时使用什么：
- en: '|  | **Manual EDA** | **Data Profiling** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | **手动EDA** | **数据分析工具** |'
- en: '| **Pros** | Flexibility to explore data based on specific needs | Automated
    process for quick insights |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | 根据特定需求灵活探索数据 | 自动化流程，快速洞察 |'
- en: '|  | In-depth understanding of the data through custom code | Consistent and
    standardized analysis across datasets |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | 通过自定义代码深入理解数据 | 跨数据集的一致性和标准化分析 |'
- en: '|  | Greater control over analysis techniques and visuals | Automated visualizations
    and summary statistics |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | 更大控制权的分析技术和可视化 | 自动化的可视化和汇总统计 |'
- en: '|  | Identification of data quality issues and anomalies |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据质量问题和异常的识别 |'
- en: '| **Cons** | A time-consuming process requiring manual effort and repetitive
    | Limited customization options in generated reports |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| **缺点** | 需要大量时间且需人工干预，过程重复 | 生成的报告自定义选项有限 |'
- en: '|  | Higher likelihood of human errors or biases | May not capture complex
    relationships or patterns |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | 更高的人工错误或偏见的可能性 | 可能无法捕捉复杂的关系或模式 |'
- en: '|  | Lack of standardization across analysts or teams | Less flexibility compared
    to manual analysis |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | 分析师或团队之间缺乏标准化 | 与手动分析相比，灵活性较差 |'
- en: '|  | **Manual EDA** | **Data Profiling** |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | **手动EDA** | **数据分析工具** |'
- en: '|  |  | Reliance on predefined algorithms and techniques |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 依赖预定义的算法和技术 |'
- en: Table 3.1 – Comparison between Manual EDA versus using a profiler tool
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1 – 手动EDA与使用分析工具的比较
- en: As the data grows, manual EDA becomes increasingly time-consuming and prone
    to human error, leading to inconsistent results and potentially overlooked data
    issues. Manual efforts also lack scalability and reproducibility, making it difficult
    to handle large datasets and collaborate effectively. That is why for the rest
    of the chapter, we will focus on how you can use different profiling tools to
    perform EDA on the data; however, in practice, a combined approach is often implemented.
    We will also provide some insights on how to change your tools given the size
    of your data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量的增加，手动EDA变得越来越耗时，并容易出错，导致结果不一致，并可能忽视数据问题。手动方法也缺乏可扩展性和可重复性，处理大型数据集和有效协作变得困难。这就是为什么在本章其余部分，我们将重点介绍如何使用不同的分析工具对数据进行EDA；但在实践中，通常会采用结合的方法。我们还将提供一些关于如何根据数据量改变工具的见解。
- en: Profiling data with pandas’ ydata_profiling
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pandas的ydata_profiling进行数据分析
- en: Let’s see an example in Python that showcases data profiling using the `ProfileReport`
    class from the `ydata-profiling` library.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个Python示例来展示使用`ydata-profiling`库中的`ProfileReport`类进行数据分析。
- en: 'Let’s start with installing a few libraries first:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从安装一些库开始：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the following code example, we will use the `iris` dataset from the `seaborn`
    library, which is an open source dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们将使用`seaborn`库中的`iris`数据集，它是一个开源数据集。
- en: Next, we are going to read the dataset and perform some initial EDA with *minimal
    code*!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将读取数据集并使用*最少的代码*进行初步的EDA！
- en: 'We’ll start by importing the libraries and loading the dataset directly from
    its URL using the `read_csv()` function from pandas:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过导入库并使用`pandas`的`read_csv()`函数直接从URL加载数据集来开始：
- en: '[PRE1]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Load the `iris` dataset from the `seaborn` library:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`seaborn`库加载`iris`数据集：
- en: '[PRE2]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we’ll perform data profiling by creating a profile report using the `ProfileReport()`
    function from `pandas_profiling`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将通过使用`pandas_profiling`中的`ProfileReport()`函数来执行数据分析：
- en: '[PRE3]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We’ll generate an HTML report using the `to_file()` method, which exports the
    profiling results to an HTML file for easy sharing and further analysis:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`to_file()`方法生成HTML报告，将分析结果导出为HTML文件，便于分享和进一步分析：
- en: '[PRE4]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Optionally, we can embed the report in the notebook:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，我们可以将报告嵌入到笔记本中：
- en: '[PRE5]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Writing the report to a JSON file is optional, but a best practice:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将报告写入JSON文件是可选的，但这是一个最佳实践：
- en: '[PRE6]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s explore the results of the profiler one by one.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一探索分析工具的结果。
- en: Overview
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概览
- en: 'The first section in the profiling report is the **Overview** section. In the
    **Overview** section, you have multiple tabs, as shown in the following figure:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 分析报告中的第一部分是**概览**部分。在**概览**部分，你可以看到多个标签，如下图所示：
- en: '![Figure 3.2 – Overview of pandas profiler results](img/B19801_03_2.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – pandas 分析器结果概览](img/B19801_03_2.jpg)'
- en: Figure 3.2 – Overview of pandas profiler results
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – pandas 分析器结果概览
- en: 'In the **Overview** tab of the profiler results, we can see the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析器结果的**概览**标签中，我们可以看到以下内容：
- en: '**Number of variables**: The iris dataset has five variables – sepal length,
    sepal width, petal length, petal width, and species'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量数量**：鸢尾花数据集包含五个变量——花萼长度、花萼宽度、花瓣长度、花瓣宽度和物种'
- en: '**Number of observations**: The dataset contains 150 rows'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观测数**：数据集包含150行'
- en: '**Missing cells**: No missing values are present in the iris dataset'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失单元格**：鸢尾花数据集中没有缺失值'
- en: '**Duplicate rows**: There is one duplicate row'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复行**：有一行重复数据'
- en: 'Then, we have the **Alerts** tab, as shown in the following screenshot:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以看到**警告**标签，如下图所示：
- en: '![Figure 3.3 – The Alerts tab of the ydata_profiling profiler](img/B19801_03_3.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – ydata_profiling 分析器的警告标签](img/B19801_03_3.jpg)'
- en: Figure 3.3 – The Alerts tab of the ydata_profiling profiler
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – ydata_profiling 分析器的警告标签
- en: 'In the `sepal_length`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sepal_length`部分：
- en: '![Figure 3.4 – Numerical feature profiling](img/B19801_03_4.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 数值特征分析](img/B19801_03_4.jpg)'
- en: Figure 3.4 – Numerical feature profiling
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 数值特征分析
- en: In the `sepal_length` part of the profile page, we can get more details about
    the specific numeric feature. A similar analysis is performed for all the other
    numeric features in the dataset. We can see that this feature has 35 different
    values and there are no missing values in the dataset for this feature. All the
    values are positive, which makes sense as this feature represents the length of
    the sepal and these values can range from 4.3 to 7.9\. The histogram shows the
    distribution for the feature.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sepal_length`部分的分析页面中，我们可以获取更多关于特定数值特征的详细信息。对数据集中的所有其他数值特征也进行了类似的分析。我们可以看到该特征有35个不同的值，且数据集中没有缺失值。所有值都是正数，这很合理，因为该特征代表花萼的长度，值的范围应在4.3到7.9之间。直方图展示了该特征的分布情况。
- en: '![Figure 3.5 – Categorical feature profiling](img/B19801_03_5.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 类别特征分析](img/B19801_03_5.jpg)'
- en: Figure 3.5 – Categorical feature profiling
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 类别特征分析
- en: In the `species` part of the profile page, we can get more details about the
    specific categorical feature. A similar analysis is performed for all the other
    categorical features in the dataset. We can see that this feature has three different
    values (`sectosa`, `versicolor`, and `virginica`) and there are no missing values
    in the data for this feature. From the graph, we can see that we have the same
    number of records for each value of the feature (50).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在`species`部分的分析页面中，我们可以获取更多关于特定类别特征的详细信息。对数据集中的所有其他类别特征也进行了类似的分析。我们可以看到该特征有三个不同的值（`sectosa`、`versicolor`和`virginica`），且数据集中没有缺失值。从图表中可以看出，每个特征值的记录数相同（50条）。
- en: Interactions
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互作用
- en: Another section in the profiling report is an **Interactions** section, which
    visualizes the relationships and potential interactions between different columns
    in the dataset. These charts are particularly useful for identifying potential
    correlations or dependencies between variables presented as *scatter plots*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 分析报告中的另一个部分是**交互作用**部分，展示了数据集中不同列之间的关系和潜在的交互作用。这些图表特别有助于识别变量之间的潜在相关性或依赖性，通常以*散点图*的形式呈现。
- en: The following figure shows the interactions between different variables. This
    chart can be created for every different combination of *numeric* variables. Let’s
    see an example for petal length and petal width.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了不同变量之间的交互作用。该图表可以针对所有不同的*数值*变量组合进行创建。我们来看一下花瓣长度与花瓣宽度的示例。
- en: '![Figure 3.6 – Interaction chart between petal length and petal width](img/B19801_03_6.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 花瓣长度与花瓣宽度的交互作用图](img/B19801_03_6.jpg)'
- en: Figure 3.6 – Interaction chart between petal length and petal width
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 花瓣长度与花瓣宽度的交互作用图
- en: In the interaction chart, we can see how one variable influences the other.
    For example, as the petal length increases, the petal width increases as well.
    So, there is a linear relationship between the two. Since there is a strong interaction
    between these two variables, it is a good idea to deep dive into this interaction
    and examine in more detail the correlation plots for this pair.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在交互图中，我们可以看到一个变量如何影响另一个变量。例如，随着花瓣长度的增加，花瓣宽度也增加。因此，这两者之间存在着线性关系。由于这两个变量之间有强烈的相互作用，因此深入研究这种相互作用并详细检查这一对变量的相关性图表是一个好主意。
- en: Correlations
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关性
- en: A **correlation matrix** also depicts the interactions between variables where
    each cell represents the relationship between two columns. The cells are color-coded
    based on the strength or type of interaction detected between the corresponding
    column pairs. This helps in identifying how strongly two variables are related.
    Positive correlations are typically shown in one color (e.g., blue), while negative
    correlations are shown in another (e.g., red), with the intensity of the color
    indicating the strength of the correlation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关矩阵**还描绘了变量之间的相互作用，每个单元格表示两个列之间的关系。单元格的颜色根据检测到的相互作用的强度或类型进行编码。这有助于识别两个变量之间的关系强度。正相关通常以一种颜色显示（例如，蓝色），而负相关则以另一种颜色显示（例如，红色），颜色的深浅表示相关性的强度。'
- en: For example, there might be a positive correlation between petal length and
    petal width, indicating that as the length of the petal increases, so does the
    width.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，花瓣长度和花瓣宽度之间可能存在正相关，表明花瓣长度增加时，花瓣宽度也会增加。
- en: '![Figure 3.7 – Correlation chart between numeric variables](img/B19801_03_7.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 数值变量之间的相关性图](img/B19801_03_7.jpg)'
- en: Figure 3.7 – Correlation chart between numeric variables
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 数值变量之间的相关性图
- en: As we can see from the chart, the darker the color blue is, the stronger the
    correlation between the variables. Petal length and petal width have more than
    0.75 positive correlation, showing that as one increases the other increases too.
    This is something we need to be aware of before proceeding to any modeling exercise
    as we may not need to keep both variables in the dataset, as having one can predict
    the other. For instance, if two variables are highly correlated, you might drop
    one of them or create a new feature that encapsulates the information from both.
    In some cases, removing highly correlated features can lead to faster training
    times for some machine learning algorithms, as the algorithm doesn’t need to deal
    with redundant information; also, we can simplify models, making them easier to
    understand and less prone to overfitting.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中可以看到，蓝色越深，变量之间的相关性越强。花瓣长度和花瓣宽度的正相关性超过 0.75，表明其中一个增加时另一个也会增加。在进行任何建模工作之前，我们需要注意这一点，因为我们可能不需要在数据集中保留这两个变量，因为拥有其中一个就能预测另一个。例如，如果两个变量高度相关，你可以删除其中一个，或者创建一个新特征来包含这两个变量的信息。在某些情况下，移除高度相关的特征可以加快某些机器学习算法的训练时间，因为算法不需要处理冗余信息；此外，简化模型也能让模型更容易理解，减少过拟合的风险。
- en: Note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**High correlation threshold**: Set a threshold for high correlation (e.g.,
    0.8 or 0.9). Variables with correlation coefficients above this threshold are
    considered highly correlated.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**高相关性阈值**：设置高相关性的阈值（例如，0.8 或 0.9）。相关系数高于此阈值的变量被视为高度相关。'
- en: Missing values
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失值
- en: Another key aspect of data quality is **missing values**. It refers to the absence
    of data in specific entries or variables within a dataset. They can occur for
    various reasons, such as data entry errors, sensor malfunctions, or errors in
    the ingestion process. If ignored, it can lead to biased and inaccurate results.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量的另一个关键方面是**缺失值**。它指的是数据集中特定条目或变量中缺少数据。缺失值可能由于多种原因出现，例如数据输入错误、传感器故障或数据获取过程中的错误。如果忽视这些缺失值，可能导致偏倚和不准确的结果。
- en: 'The following figure shows the percentage of non-missing values for each column
    in the data:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了数据中每一列非缺失值的百分比：
- en: '![Figure 3.8 – Percentage of non-missing values in the data](img/B19801_03_8.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – 数据中非缺失值的百分比](img/B19801_03_8.jpg)'
- en: Figure 3.8 – Percentage of non-missing values in the data
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 数据中非缺失值的百分比
- en: In the present example, we can see that all the values in the dataset are complete
    and that all the features have 150 non-null values. So, that is good news for
    us and we can proceed to the next check.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前示例中，我们可以看到数据集中的所有值都是完整的，且所有特征都有 150 个非空值。因此，这对我们来说是个好消息，我们可以继续进行下一步检查。
- en: Duplicate rows
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重复行
- en: A **duplicate row** in a dataset refers to a row that is identical to another
    row in every column. This means that for every column in the dataset, the values
    in the duplicate row are the same as those in the row it duplicates. Surfacing
    the presence and extent of duplicate rows helps us quickly identify potential
    data quality issues. As we’ve said, duplicate rows can arise due to various reasons,
    such as data integration problems, incorrect deduplication processes, or simply
    the nature of the data collection process.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的**重复行**是指每一列的值都与另一行相同的行。这意味着数据集中的每一列，在重复行中的值与它所重复的行的值完全一致。揭示重复行的存在和范围有助于我们快速识别潜在的数据质量问题。正如我们所说，重复行可能是由于各种原因产生的，例如数据整合问题、错误的去重过程，或只是数据收集过程的性质。
- en: In a profiling report, we can see the duplicate rows under **Most frequently
    occurring**, where a sample of the duplicates in the dataset is presented.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析报告中，我们可以在**最常见的重复项**下看到重复行，其中展示了数据集中重复项的一个示例。
- en: '![Figure 3.9 – Duplicate rows in the data](img/B19801_03_9.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – 数据中的重复行](img/B19801_03_9.jpg)'
- en: Figure 3.9 – Duplicate rows in the data
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 数据中的重复行
- en: In general, to find the duplicate rows, you need to identify key columns or
    a combination of columns that should be unique. Typically, we identify duplicates
    based on all columns in the dataset. If there are duplicates, it indicates we
    have duplicate rows. There are only two duplicated rows in the dataset in our
    example, as shown in the preceding figure.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，要查找重复行，您需要识别应当唯一的关键列或列的组合。通常，我们会根据数据集中的所有列来识别重复项。如果存在重复项，说明我们有重复的行。我们的示例数据集中只有两行重复数据，如前图所示。
- en: At this stage of the analysis, we are not handling duplicates since our focus
    is on understanding the data’s structure and characteristics. However, we will
    need to investigate the nature and source of these duplicates. Given that they
    represent a small proportion of the data, we could simply drop one of each pair
    of identical rows.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的分析阶段，我们并不处理重复项，因为我们的重点是理解数据的结构和特征。然而，我们需要调查这些重复项的性质和来源。鉴于它们在数据中所占比例很小，我们可以简单地删除每对相同的行中的一行。
- en: Sample dataset
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例数据集
- en: Sampling refers to the process of selecting a subset of data from a larger dataset
    instead of working with the entire dataset. In the EDA step, we usually work on
    a sample of data as it can provide initial insights and help in formulating hypotheses
    before committing resources to a full analysis.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样是指从较大的数据集中选择一个数据子集的过程，而不是使用整个数据集。在探索性数据分析（EDA）步骤中，我们通常会使用数据的一个样本，因为它可以提供初步的洞见，并帮助在进行全面分析之前提出假设。
- en: '![Figure 3.10 – Sample dataset](img/B19801_03_10.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 示例数据集](img/B19801_03_10.jpg)'
- en: Figure 3.10 – Sample dataset
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 示例数据集
- en: Now that we’ve understood how to build data profiles with the `ydata_profiling`
    library, let’s have a closer look at a very popular but similar profiler called
    the **pandas** **data profiler**.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用 `ydata_profiling` 库构建数据分析报告，接下来让我们更详细地看看一个非常流行且类似的分析工具——**pandas**
    **数据分析工具**。
- en: Profiling high volumes of data with the pandas data profiler
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pandas 数据分析工具分析大量数据
- en: '**Pandas profiling** is a powerful library for generating detailed reports
    on datasets. However, for large datasets, the profiling process can become time-consuming
    and memory-intensive. When dealing with large datasets, you may need to consider
    a few strategies to optimize the profiling process:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pandas Profiling** 是一个强大的库，用于生成数据集的详细报告。然而，对于大数据集来说，分析过程可能会变得耗时且占用大量内存。处理大数据集时，您可能需要考虑一些优化分析过程的策略：'
- en: '**Sampling**: Instead of profiling the entire dataset, you can take a random
    sample of the data to generate the report. This can significantly reduce the computation
    time and memory requirements while still providing a representative overview of
    the dataset:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽样**：与其对整个数据集进行分析，不如对数据进行随机抽样来生成报告。这可以显著减少计算时间和内存需求，同时仍然提供数据集的代表性概览：'
- en: '[PRE7]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Subset selection**: If you’re interested in specific columns or subsets of
    the dataset, you can select only those columns for profiling. This reduces the
    computational load and narrows down the focus to the variables of interest:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子集选择**：如果你对数据集中的特定列或子集感兴趣，可以仅选择这些列进行分析。这会减少计算负载，并将关注点集中在感兴趣的变量上：'
- en: '[PRE8]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Configuring profiler options**: The pandas profiling library provides several
    configuration options that allow you to fine-tune the profiling process. You can
    adjust these options to limit the depth of analysis, reduce computations, or skip
    certain time-consuming tasks if they are not necessary for your analysis:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置分析器选项**：pandas profiling 库提供了多个配置选项，允许你精细调整分析过程。你可以调整这些选项，以限制分析的深度、减少计算量，或者跳过某些在分析中不必要的耗时任务：'
- en: '[PRE9]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Parallel processing**: If your system supports parallel processing, you can
    leverage it to speed up the profiling process. By distributing the workload across
    multiple cores or machines, you can potentially reduce the time required for profiling
    large datasets:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行处理**：如果你的系统支持并行处理，你可以利用它加速分析过程。通过将工作负载分配到多个核心或机器上，你可以可能减少分析大数据集所需的时间：'
- en: '[PRE10]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Incremental profiling**: If your dataset is too large to fit in memory, you
    can consider performing incremental profiling by splitting the data into smaller
    chunks and profiling them individually. You can then combine the profiling results
    to get an overview of the entire dataset:'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增量分析**：如果你的数据集过大，无法完全加载到内存中，可以考虑通过将数据拆分为更小的块并分别进行分析来执行增量分析。然后，你可以将分析结果合并，获得整个数据集的概览：'
- en: '[PRE11]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Some of these strategies aim to optimize the profiling process for large datasets,
    but they may result in some loss of granularity and detail compared to profiling
    the entire dataset. It’s essential to strike a balance between computational efficiency
    and the level of insight required for your analysis.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些策略中的一些旨在优化大数据集的分析过程，但与对整个数据集进行分析相比，可能会导致一些粒度和细节的丧失。必须在计算效率和所需的分析深度之间找到平衡。
- en: The next tool we are going to review is usually used in data-engineering heavy
    workflows as it provides a lot of flexibility, automation, and easy integration
    with other tools.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要审查的工具通常用于数据工程密集型的工作流程，因为它提供了大量的灵活性、自动化功能，并且可以与其他工具轻松集成。
- en: Data validation with the Great Expectations library
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Great Expectations 库进行数据验证
- en: '**Great Expectations** is an open source Python library that facilitates data
    validation and documentation. It provides a framework for defining, managing,
    and executing data quality checks, making it easier to ensure data integrity and
    reliability throughout the data pipeline. Quality checks can be executed at different
    stages of the data life cycle, as shown in the following diagram:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**Great Expectations** 是一个开源的 Python 库，旨在促进数据验证和文档化。它提供了一个框架，用于定义、管理和执行数据质量检查，从而使得在整个数据管道中更容易确保数据的完整性和可靠性。质量检查可以在数据生命周期的不同阶段执行，如下图所示：'
- en: '![Figure 3.11 – Quality checks at different stages of the data life cycle](img/B19801_03_11.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.11 – 数据生命周期不同阶段的质量检查](img/B19801_03_11.jpg)'
- en: Figure 3.11 – Quality checks at different stages of the data life cycle
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 – 数据生命周期不同阶段的质量检查
- en: 'Let’s discuss each of the touch points in the data life cycle where quality
    checks can be applied, as illustrated in the preceding figure:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论数据生命周期中每个可以应用质量检查的接触点，如前面的图所示：
- en: '**Data entry**: During data entry or data collection, checks are conducted
    to ensure that the data is accurately captured and recorded. This can involve
    verifying the format, range, and type of data, as well as performing validation
    checks against predefined rules or standards.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据输入**：在数据输入或数据收集过程中，会进行检查以确保数据的准确捕获和记录。这可能涉及验证数据的格式、范围和类型，以及根据预定义规则或标准进行验证检查。'
- en: '**Data transformation**: If data undergoes any transformations or conversions,
    such as data cleansing or data normalization, quality checks are performed to
    validate the accuracy of the transformed data. This helps ensure that the data
    retains its integrity throughout the process.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换**：如果数据经历了任何转换或变换，如数据清洗或数据标准化，则需要执行质量检查以验证转换后数据的准确性。这有助于确保数据在整个过程中保持完整性。'
- en: '**Data integration**: When combining data from different sources or systems,
    data quality checks are necessary to identify any inconsistencies or discrepancies.
    This may involve checking for duplicate records, resolving missing or mismatched
    data, and reconciling any conflicting information.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集成**：在将来自不同来源或系统的数据合并时，必须进行数据质量检查，以识别任何不一致或差异。这可能包括检查重复记录、解决缺失或不匹配的数据，以及调和任何冲突的信息。'
- en: '**Data consumption**: Prior to performing any data analysis or generating reports,
    it is essential to run data quality checks to ensure the integrity of the data.
    This involves validating the data against predefined criteria, checking for outliers
    or anomalies, and verifying the overall quality of the dataset.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据消费**：在进行任何数据分析或生成报告之前，必须运行数据质量检查以确保数据的完整性。这包括根据预定义的标准验证数据，检查异常值或不一致，并验证数据集的整体质量。'
- en: Great Expectations allows you to set Expectations or rules for your data and
    then validate your data against these Expectations at any point in the data life
    cycle. *Figure 3**.12* illustrates the features of this library in more detail.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Great Expectations允许你为数据设定期望或规则，并在数据生命周期的任何阶段验证数据是否符合这些期望。*图 3.12*更详细地展示了该库的功能。
- en: '![Figure 3.12 – Great Expectations process from data collection to data quality
    results](img/B19801_03_12.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.12 – Great Expectations从数据收集到数据质量结果的流程](img/B19801_03_12.jpg)'
- en: Figure 3.12 – Great Expectations process from data collection to data quality
    results
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 – Great Expectations从数据收集到数据质量结果的流程
- en: 'As you can see, there are three main steps to be aware of when working with
    Great Expectations:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用Great Expectations时，需要注意三个主要步骤：
- en: Bringing/collecting all the data you want to apply Expectations on
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集/整理所有你希望应用期望的数据
- en: Writing the Expectations and applying them to the different data
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写期望并将其应用于不同的数据
- en: Enjoying the benefits of clean, high-quality, and trustworthy data coming to
    life
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 享受干净、高质量和可信数据带来的好处
- en: In the next section, we will go through how to configure Great Expectations
    to validate the dataset.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何配置Great Expectations来验证数据集。
- en: Configuring Great Expectations for your project
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Great Expectations以适应你的项目
- en: You can validate your data against the defined Expectations using Great Expectations.
    The library provides functions to execute these validations and identify any inconsistencies
    or issues in the data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Great Expectations对照定义的期望来验证数据。该库提供了执行这些验证的功能，并帮助识别数据中的任何不一致或问题。
- en: 'You will need to install the `great-expectations` library for data profiling.
    You can use the following command to install the library in any IDE or a terminal:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要安装`great-expectations`库以进行数据分析。你可以使用以下命令在任何IDE或终端中安装该库：
- en: '[PRE12]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This should install the library. We are going to use the same dataset as before
    so that we can showcase the difference between the tools:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会安装该库。我们将使用与之前相同的数据集，以便展示工具之间的差异：
- en: 'Let’s start by setting up your project. Open your terminal, navigate to the
    desired location where you want to set up your new project, and then set up a
    new folder by running the following command:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从设置项目开始。打开终端，导航到你希望设置新项目的位置，然后运行以下命令来创建一个新文件夹：
- en: '[PRE13]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we will go into the newly created folder by typing the following:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将进入新创建的文件夹，输入以下命令：
- en: '[PRE14]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we will create some folders for our data and the code we will need to
    use to run our example. Make sure you are in the `great_expectations` directory
    and run the following:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一些文件夹来存储我们的数据和运行示例所需的代码。确保你在`great_expectations`目录下，并运行以下命令：
- en: '[PRE15]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should have created the following project structure:'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该已经创建了以下项目结构：
- en: '![Figure 3.13 – Great Expectations project initialization](img/B19801_03_13.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.13 – Great Expectations项目初始化](img/B19801_03_13.jpg)'
- en: Figure 3.13 – Great Expectations project initialization
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – Great Expectations项目初始化
- en: 'Next, we’ll run the following command to initialize a new Great Expectations
    project. Make sure you are in the `great_expectations` folder:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将运行以下命令来初始化一个新的Great Expectations项目。确保你在`great_expectations`文件夹中：
- en: '[PRE16]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Figure 3.14 – Great Expectations project initialization](img/B19801_03_14.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.14 – Great Expectations项目初始化](img/B19801_03_14.jpg)'
- en: Figure 3.14 – Great Expectations project initialization
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14 – Great Expectations项目初始化
- en: 'Hit *Y* when prompted and Great Expectations will go ahead and build a project
    structure for us in the `great_expectations` folder. The folder structure will
    look like the following:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当提示时，按 *Y*，Great Expectations 将继续在 `great_expectations` 文件夹中为我们构建项目结构。文件夹结构将如下所示：
- en: '![Figure 3.15 – Great Expectations folder structure](img/B19801_03_15.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.15 – Great Expectations 文件夹结构](img/B19801_03_15.jpg)'
- en: Figure 3.15 – Great Expectations folder structure
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 – Great Expectations 文件夹结构
- en: 'The folder structure of Great Expectations follows a specific convention to
    organize the configuration, Expectations, and data documentation related to your
    data pipeline. Let’s learn a little more about the structure:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Great Expectations 的文件夹结构遵循特定的约定，用于组织与数据管道相关的配置、期望和数据文档。让我们来进一步了解一下结构：
- en: '`/uncommitted/`: This directory contains all the uncommitted configuration
    and validation files. It is where you define and modify Expectations, validations,
    and data documentation.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/uncommitted/`：该目录包含所有未提交的配置和验证文件。在这里，你定义和修改期望、验证以及数据文档。'
- en: '`/checkpoints/`: This directory stores checkpoint files, which hold the sets
    of Expectations to be validated against specific data batches. Checkpoints are
    useful for running validations on specific portions or subsets of your data.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/checkpoints/`：该目录存储检查点文件，这些文件包含一组期望（Expectations），用于与特定数据批次进行验证。检查点对于在数据的特定部分或子集上运行验证非常有用。'
- en: '`/expectations/`: This directory holds the Expectation Suites and Expectation
    files. An Expectation Suite is a collection of related Expectations, while Expectation
    files contain individual Expectations. You can create subdirectories within this
    folder to organize your Expectations based on a data source or data asset.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/expectations/`：该目录存储期望套件（Expectation Suites）和期望文件（Expectation files）。期望套件是相关期望的集合，而期望文件包含单个期望。你可以在此文件夹内创建子目录，以根据数据源或数据资产组织你的期望。'
- en: '`/plugins/`: This folder is used to store custom plugins and extensions that
    you may develop to extend the functionality of Great Expectations.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/plugins/`：此文件夹用于存储你可能开发的自定义插件和扩展，用以扩展 Great Expectations 的功能。'
- en: '`great_expectations.yml`: This configuration file stores the deployment settings
    for Great Expectations. It contains essential information and parameters that
    define how Great Expectations operates within your deployment environment.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`great_expectations.yml`：该配置文件存储 Great Expectations 的部署设置。它包含定义 Great Expectations
    如何在你的部署环境中运行的必要信息和参数。'
- en: Now that we’ve set up and initialized a Great Expectations project, let’s create
    our first data source using Great Expectations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置并初始化了一个 Great Expectations 项目，接下来让我们使用 Great Expectations 创建我们的第一个数据源。
- en: Create your first Great Expectations data source
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建你的第一个 Great Expectations 数据源
- en: 'So far, we have created the project structure to create our Expectations. The
    next step is to get some data to build Expectations on. In order to retrieve the
    dataset, go to the repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03/great_expectations/code](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03/great_expectations/code),
    get the `1.data_set_up.py` script, and save it under the `great_expectations/code/`
    folder. Now, let’s write some test data to our folder by running the following
    Python script: `great_expectations/code/1.data_set_up.py`. Here’s what the script
    looks like:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经创建了项目结构来构建我们的期望（Expectations）。下一步是获取一些数据来构建期望。为了获取数据集，请访问 [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03/great_expectations/code](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03/great_expectations/code)，获取
    `1.data_set_up.py` 脚本，并将其保存在 `great_expectations/code/` 文件夹中。现在，让我们通过运行以下 Python
    脚本将一些测试数据写入文件夹：`great_expectations/code/1.data_set_up.py`。下面是该脚本的样子：
- en: '[PRE17]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In your terminal, in the `great_expectations/code/` directory, execute the
    following command:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中，进入 `great_expectations/code/` 目录，执行以下命令：
- en: '[PRE18]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This script performs a simple task of loading the `iris` dataset from a remote
    source, from the `seaborn` library’s GitHub repository, using the pandas library.
    It then saves this dataset to a local file named `iris_data.csv` in the `great_expectations/data`
    directory. Finally, it prints a confirmation message to indicate that the file
    has been successfully saved.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本执行了一个简单的任务：使用 pandas 库从远程源加载 `iris` 数据集，该数据集位于 `seaborn` 库的 GitHub 仓库中。然后，它将此数据集保存为
    `iris_data.csv` 文件，保存在 `great_expectations/data` 目录下。最后，它打印确认信息，表示文件已成功保存。
- en: 'Now, we need to tell Great Expectations which data we want to use to build
    Great Expectations and where to find this data. In your terminal, execute the
    following command:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要告诉Great Expectations我们想使用哪些数据来构建Great Expectations，以及在哪里找到这些数据。在终端中执行以下命令：
- en: '[PRE19]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will display the following prompt:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下提示：
- en: '![Figure 3.16 – Great Expectations file configuration](img/B19801_03_16.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图3.16 – Great Expectations 文件配置](img/B19801_03_16.jpg)'
- en: Figure 3.16 – Great Expectations file configuration
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – Great Expectations 文件配置
- en: Follow the steps in the terminal, as shown in *Figure 3**.16*, making sure you
    choose option `1` as we are going to work with files and not SQL databases. Since
    our datasets are small enough to fit in memory, we can manipulate them with pandas.
    So, we’ll choose option `1` again. It will then prompt you to enter the path to
    the dataset file and since we saved our dataset in the `data` folder, enter `../data`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照终端中的步骤操作，如*图3.16*所示，确保选择`1`选项，因为我们将处理文件而不是SQL数据库。由于我们的数据集足够小，可以放入内存，因此我们可以使用pandas进行操作。所以，我们再次选择选项`1`。接下来，它会提示您输入数据集文件的路径，由于我们将数据集保存在`data`文件夹中，请输入`../data`。
- en: After this step, Great Expectations automatically creates a Jupyter Notebook
    for us to explore! This notebook is stored at the `great_expectations/gx/uncommitted/datasource_new.ipynb`
    path and after the execution, you can just delete it if you don’t want to maintain
    unnecessary code. The purpose of this notebook is to help you create a pandas
    data source configuration and avoid any manual mistakes.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此步骤后，Great Expectations会自动为我们创建一个Jupyter笔记本以供探索！这个笔记本存储在`great_expectations/gx/uncommitted/datasource_new.ipynb`路径下，执行后，如果您不想维护不必要的代码，可以直接删除它。这个笔记本的目的是帮助您创建一个pandas数据源配置，避免任何人工错误。
- en: Let’s open the notebook, update `datasource_name`, as shown in the following
    screenshot, and execute all the cells in the notebook.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 打开笔记本，更新`datasource_name`，如以下截图所示，并执行笔记本中的所有单元格。
- en: '![Figure 3.17 – Great Expectations – customizing data source name](img/B19801_03_017.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图3.17 – Great Expectations – 自定义数据源名称](img/B19801_03_017.jpg)'
- en: Figure 3.17 – Great Expectations – customizing data source name
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – Great Expectations – 自定义数据源名称
- en: We can give it any name we want at this point, but to be consistent with the
    incoming data, let’s name it `iris_data`. From now on, when we refer to `iris_data`,
    we know we are working on the Expectations for the `iris` data source we created
    in the previous step.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此时我们可以给它任何名称，但为了与传入的数据保持一致，我们将其命名为`iris_data`。从现在开始，当我们提到`iris_data`时，我们知道我们正在处理在前一步中创建的`iris`数据源的Expectations。
- en: Note
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Maintaining consistent and clear naming between Expectation validations and
    data sources enhances readability, reduces errors, and simplifies maintenance
    and debugging!
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在Expectation验证和数据源之间保持一致和清晰的命名，可以提高可读性，减少错误，并简化维护和调试！
- en: Creating your first Great Expectations suite
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建您的第一个Great Expectations套件
- en: Now that we have declared which data source we want to build an Expectation
    for, let’s go ahead and build the first suite for our `iris` dataset.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经声明了要为其构建Expectation的数据源，接下来让我们为`iris`数据集构建第一个套件。
- en: 'Open your terminal and execute the following command:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 打开终端并执行以下命令：
- en: '[PRE20]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There are multiple ways to create your Expectation Suite, as you can see from
    the following figure.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，您可以通过多种方式创建Expectation套件。
- en: '![Figure 3.18 – Great Expectations – options for creating your suite](img/B19801_03_18.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图3.18 – Great Expectations – 创建您的套件的选项](img/B19801_03_18.jpg)'
- en: Figure 3.18 – Great Expectations – options for creating your suite
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 – Great Expectations – 创建您的套件的选项
- en: 'Let’s explore each of the options:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨每个选项：
- en: '`Manually, without interacting with a sample batch of data (default)`: This
    approach involves manually defining Expectations and configuring the suite without
    directly interacting with a sample batch of data. Expectations are typically based
    on your knowledge of the data and the specific requirements of your project. You
    define Expectations by specifying conditions, ranges, patterns, and other criteria
    that you expect the data to meet. This approach requires a thorough understanding
    of the data and domain knowledge to define accurate Expectations.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`手动操作，不与样本批数据交互（默认）`：这种方法涉及手动定义Expectations并配置套件，而不直接与样本批数据交互。Expectations通常基于您对数据的了解和项目的具体要求。通过指定条件、范围、模式和其他您期望数据满足的标准来定义Expectations。此方法需要对数据和领域知识有透彻的了解，以定义准确的Expectations。'
- en: '`Interactively, with a sample batch of data`: In this approach, you load a
    small representative batch of data into Great Expectations and use it to interactively
    define Expectations. This allows you to visually inspect the data, identify patterns,
    and explore various data statistics. You can iteratively build and refine Expectations
    based on your observations and understanding of the data.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`交互式，使用样本数据批次`: 在这种方法中，你将一个小的代表性数据批次加载到 Great Expectations 中，并使用它交互式地定义期望值。这使你能够直观地检查数据，识别模式，并探索各种数据统计信息。你可以基于对数据的观察和理解，迭代地构建和完善期望值。'
- en: '`Automatically, using a Data Assistant`: Great Expectations provides a Data
    Assistant feature that automatically suggests Expectations based on the data.
    The Data Assistant analyzes the data and generates a set of suggested Expectations,
    which you can review and customize. This approach can be helpful when you have
    limited knowledge about the data or want to quickly generate a starting point
    for your Expectations. You can leverage the suggested Expectations as a foundation
    and further refine them based on your domain knowledge and specific requirements.
    The Data Assistant accelerates the process of building a suite by automating the
    initial Expectation generation.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`自动化，通过数据助手`: Great Expectations 提供了一个数据助手功能，该功能根据数据自动建议期望值（Expectations）。数据助手分析数据并生成一组建议的期望值，您可以查看并自定义这些期望值。当你对数据了解有限或希望快速生成期望值的起始点时，这个方法特别有帮助。你可以利用建议的期望值作为基础，并根据自己的领域知识和具体要求进一步优化它们。数据助手通过自动生成初始期望值来加速构建期望值套件的过程。'
- en: In this example, we will use the third option to build the suite automatically.
    This functionality is similar to the one that pandas profiling offers and we have
    explored it before in the *Profiling data with pandas’ ydata_profiling* section.
    So, go ahead and choose option `3` in the terminal, as shown in *Figure 3**.19*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用第三个选项自动构建套件。此功能类似于 pandas profiling 提供的功能，我们之前已经在*使用 pandas’ ydata_profiling
    进行数据分析*部分中探讨过。所以，请继续在终端中选择选项`3`，如*图 3.19*所示。
- en: '![Figure 3.19 – Great Expectations – the Data Assistant option](img/B19801_03_19.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.19 – Great Expectations – 数据助手选项](img/B19801_03_19.jpg)'
- en: Figure 3.19 – Great Expectations – the Data Assistant option
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19 – Great Expectations – 数据助手选项
- en: 'As a next step, you will be asked to choose which data source you want to create
    the suite for, which is the output from the previous step. Type `1` for the `iris_data`
    source we built before and then input the name of the new Expectation Suite: `expect_iris`.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，你将被要求选择希望为其创建套件的数据源，这将是上一步的输出。键入`1`以选择我们之前构建的`iris_data`源，然后输入新期望值套件的名称：`expect_iris`。
- en: After executing the preceding command, a new notebook will be created automatically
    at `great_expectations/gx/uncommitted/edit_expect_iris.ipynb`. Open and read the
    notebook to understand the logic of the code; in summary, this notebook helps
    you choose columns and other factors from the data that you care about and lets
    the profiler create some Expectations for you that you can adjust later.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的命令后，一个新的笔记本将会自动创建在`great_expectations/gx/uncommitted/edit_expect_iris.ipynb`。打开并阅读该笔记本以理解代码的逻辑；总的来说，这个笔记本帮助你从数据中选择你关心的列和其他因素，并让分析器为你创建一些可以稍后调整的期望值（Expectations）。
- en: You have the option to create Expectations for all the columns in your dataset
    or a subset of them, as shown in *Figure 3**.20*.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择为数据集中的所有列或其中的一部分创建期望值，如*图 3.20*所示。
- en: '![Figure 3.20 – Great Expectations – columns included in the Suite](img/B19801_03_20.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.20 – Great Expectations – 套件中包含的列](img/B19801_03_20.jpg)'
- en: Figure 3.20 – Great Expectations – columns included in the Suite
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20 – Great Expectations – 套件中包含的列
- en: You can add all the column names for which you do *not* want to create Expectations
    in the `exclude_column_name` list. For any columns not added to the list, `great_expectations`
    will build Expectations for you. In our case, we want to create Expectations for
    all the columns, so we will leave the list empty, as shown in *Figure 3**.21*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将所有你不希望创建期望值的列名称添加到`exclude_column_name`列表中。对于没有添加到该列表中的任何列，`great_expectations`将为你构建期望值。在我们的例子中，我们希望为所有列创建期望值，因此我们将列表留空，如*图
    3.21*所示。
- en: '![Figure 3.21 – Great Expectations – excluding columns from the Suite](img/B19801_03_21.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.21 – Great Expectations – 从套件中排除列](img/B19801_03_21.jpg)'
- en: Figure 3.21 – Great Expectations – excluding columns from the Suite
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.21 – Great Expectations – 从套件中排除列
- en: Remember to execute all the cells in the notebook and let’s have a look at all
    the different Expectations built automatically by `great_expectations` for us.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 记得执行笔记本中的所有单元格，让我们来看看`great_expectations`为我们自动构建的所有不同期望。
- en: Great Expectations Suite report
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Great Expectations 套件报告
- en: Let’s have a look at the profiling created by `great_expectations`. As you can
    see from *Figure 3**.22*, 52 Expectations were created and all have successfully
    passed. We can monitor the success percentage in the **Overview** tab to get a
    quick idea of how many Expectations pass every time a new data feed is coming
    to your pipeline.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看由`great_expectations`创建的分析结果。如图*3.22*所示，已创建52个期望，且都已成功通过。我们可以在**概览**选项卡中监控成功百分比，以便快速了解每当新的数据流入您的数据管道时，有多少期望通过。
- en: '![Figure 3.22 – Report overview statistics](img/B19801_03_22.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.22 – 报告概览统计](img/B19801_03_22.jpg)'
- en: Figure 3.22 – Report overview statistics
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.22 – 报告概览统计
- en: 'Let’s have a closer look at what Expectations we are validating our data against.
    The first thing to consider is across-the-table or table-level Expectations, as
    shown in the following screenshot:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看我们正在验证数据的期望。首先要考虑的是跨表或表级期望，如下图所示：
- en: '![Figure 3.23 – Table-level expectations](img/B19801_03_23.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.23 – 表级期望](img/B19801_03_23.jpg)'
- en: Figure 3.23 – Table-level expectations
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.23 – 表级期望
- en: These Expectations check if the columns in the dataset match a given set of
    column names and if the dataset has the expected number of columns. It can be
    useful for ensuring all expected columns are present in the incoming data. If
    the incoming data does not contain all the columns shown in the Expectations,
    then the process will fail.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这些期望检查数据集中的列是否与给定的列名集匹配，并且数据集是否具有预期的列数。这对于确保传入的数据包含所有预期列非常有用。如果传入的数据未包含期望中的所有列，则该过程将失败。
- en: '![Figure 3.24 – Column-level Expectations](img/B19801_03_24.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.24 – 列级期望](img/B19801_03_24.jpg)'
- en: Figure 3.24 – Column-level Expectations
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.24 – 列级期望
- en: The next set of Expectations is created for each of the columns in the table
    and we will refer to them as feature Expectations.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 下一组期望是为表格中的每一列创建的，我们将其称为特征期望。
- en: '![Figure 3.25 – Feature-level Expectations](img/B19801_03_25.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.25 – 特征级期望](img/B19801_03_25.jpg)'
- en: Figure 3.25 – Feature-level Expectations
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.25 – 特征级期望
- en: These Expectations are checked separately for each column, and they can contain
    min and max values for the feature, whether we accept null values in the column
    or not, and many others. Remember, up to this point, all the Expectations were
    built automatically by us using a tool that does not understand the business context
    of the data. So, remember to check the Expectations and update them based on the
    business understanding of the data, as we will show in the next part.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这些期望会针对每一列分别检查，它们可以包含特征的最小值和最大值、是否接受该列的空值以及其他许多内容。记住，到目前为止，所有的期望都是通过我们使用的工具自动生成的，这些工具并不理解数据的业务上下文。所以，记得根据对数据的业务理解来检查并更新期望，正如我们将在下一部分展示的那样。
- en: Manually edit Great Expectations
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动编辑 Great Expectations
- en: While automatically generated Expectations provide a good starting point, they
    may not be sufficient for production-ready data validation. At this stage, it
    is important to further refine and customize the suite. You have the option to
    edit the suite manually or interactively. In general, manual editing is preferred
    when you have a clear understanding of the expected data properties and want to
    define Expectations efficiently and precisely. Since we’ve already done a basic
    automatic profiling of the data, we will proceed with the manual editing approach.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自动生成的期望提供了一个很好的起点，但它们可能不足以满足生产环境下的数据验证需求。在这一阶段，进一步精炼和定制期望套件非常重要。您可以选择手动或交互式编辑期望套件。通常，当您清楚理解预期的数据属性，并希望高效、准确地定义期望时，手动编辑是首选。由于我们已经完成了数据的基本自动分析，因此我们将选择手动编辑方法。
- en: 'Open the terminal and execute the following command:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 打开终端并执行以下命令：
- en: '[PRE21]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You will be prompted to choose how you want to update the suite, either manually
    or interactively. We will proceed with it manually.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 您将被提示选择如何更新期望套件，可以选择手动或交互式更新。我们将选择手动进行更新。
- en: 'Upon providing the necessary input, Great Expectations opens the Jupyter Notebook
    available at the following location: `great_expectations/gx/uncommitted/edit_expect_iris.ipynb`.
    The notebook includes a comprehensive display of all the Expectations that were
    automatically generated. This allows you to review and examine the Expectations
    in detail, providing you with a clear overview of the validation rules that Great
    Expectations has inferred from the data. Have a look at all the Expectations we
    created and update them as necessary. In case you don’t want to use notebooks,
    you can go open the `great_expectations/gx/expectations/expect_iris.json` file
    and update it there.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 提供必要的输入后，Great Expectations 会在以下位置打开可用的 Jupyter Notebook：`great_expectations/gx/uncommitted/edit_expect_iris.ipynb`。该
    Notebook 显示了所有自动生成的 Expectations 的完整列表。这使你能够详细查看和检查 Expectations，清晰地了解 Great Expectations
    从数据中推断出的验证规则。查看我们创建的所有 Expectations，并根据需要更新它们。如果你不想使用 Notebook，可以直接打开 `great_expectations/gx/expectations/expect_iris.json`
    文件并在其中更新。
- en: Checkpoints
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查点
- en: 'So far, we have established a connection to our training dataset and defined
    our Expectations based on the training data. The next step is to apply these Expectations
    to our new stream of data in order to validate the new dataset and make sure it
    passes the checks. So, we need to create the connection between the Great Expectation
    suite and the new data to validate. We can do this with a checkpoint. To achieve
    this, we will first mock some test data to apply the Expectations. You can find
    the script at the following location: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter03/great_expectations/code/2.mock_test_dataset.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter03/great_expectations/code/2.mock_test_dataset.py).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经建立了与训练数据集的连接，并根据训练数据定义了 Expectations。下一步是将这些 Expectations 应用到新的数据流上，以验证新数据集，并确保其通过检查。因此，我们需要创建
    Great Expectation 套件与新数据之间的连接以进行验证。我们可以通过检查点来实现这一点。为此，我们将首先模拟一些测试数据来应用 Expectations。你可以在以下位置找到脚本：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter03/great_expectations/code/2.mock_test_dataset.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter03/great_expectations/code/2.mock_test_dataset.py)。
- en: Save it under the `great_expectations/code/` folder. The script takes care of
    saving the test file in the required location, which is `great_expectations/data/`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 将其保存在 `great_expectations/code/` 文件夹中。脚本会自动将测试文件保存到所需的位置，即 `great_expectations/data/`。
- en: 'In your terminal, in the `great_expectations/code/` directory, execute the
    following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中，在 `great_expectations/code/` 目录下，执行以下命令：
- en: '[PRE22]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s have a closer look at the code we just executed, starting with the import
    statements:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看一下我们刚刚执行的代码，从导入语句开始：
- en: '[PRE23]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Load the `iris` dataset from the `seaborn` library:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `seaborn` 库加载 `iris` 数据集：
- en: '[PRE24]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We will do some transformations that will fail the Expectations and, in this
    case, we will update the `sepal_length` values to `60`, which will break our Expectations:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将做一些转换，这些转换会导致 Expectations 失败，在这种情况下，我们将把 `sepal_length` 的值更新为 `60`，这将打破我们的
    Expectations：
- en: '[PRE25]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will also rename columns to showcase the change in column names and by extension
    to the expected schema of the data:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将重命名列名，以展示列名的更改，并进一步展示数据预期的模式：
- en: '[PRE26]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We will write DataFrame that will work as a new data feed to test our Expectations
    against:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写一个 DataFrame，作为新的数据源来测试我们的 Expectations：
- en: '[PRE27]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, we need to create a checkpoint that will execute the Great Expectation
    Suite we created on the test dataset. To initiate the checkpoint, you can run
    the following command in your terminal:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要创建一个检查点，执行我们在测试数据集上创建的 Great Expectation Suite。要启动检查点，你可以在终端中运行以下命令：
- en: '[PRE28]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Upon execution, Great Expectations automatically generates a Jupyter Notebook
    that provides valuable information about the checkpoint here: `/great_expectations/gx/uncommitted/edit_checkpoint_expect_iris_ckpnt.ipynb`.
    This includes details about the data to which the checkpoint will be applied.
    Before executing the notebook, we need to update the file name and point it to
    the test file, as shown here:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 执行后，Great Expectations 会自动生成一个 Jupyter Notebook，提供有关检查点的有用信息，位置在：`/great_expectations/gx/uncommitted/edit_checkpoint_expect_iris_ckpnt.ipynb`。其中包含有关应用检查点的数据的详细信息。在执行
    Notebook 之前，我们需要更新文件名，并指向测试文件，如下所示：
- en: '[PRE29]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Uncomment the last two lines and then execute all the cells of the notebook:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 取消注释最后两行代码，然后执行 Notebook 中的所有单元格：
- en: '[PRE30]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The preceding notebook will apply the checkpoint to the new dataset and create
    a report of all the Expectations that have passed or failed. Let’s see the results!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 上述笔记本将把检查点应用于新的数据集，并创建一份报告，列出所有通过或失败的期望。让我们看看结果吧！
- en: '![Figure 3.26 – Expectations results](img/B19801_03_26.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.26 – 期望结果](img/B19801_03_26.jpg)'
- en: Figure 3.26 – Expectations results
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.26 – 期望结果
- en: As expected, our Expectations failed on the column names and on the petal width
    as it cannot find the right column names because of schema changes.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们的期望在列名和花瓣宽度上失败，因为它由于架构变化无法找到正确的列名。
- en: '![Figure 3.27 – Expectations failures because of schema changes](img/B19801_03_27.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.27 – 由于架构变化导致的期望失败](img/B19801_03_27.jpg)'
- en: Figure 3.27 – Expectations failures because of schema changes
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.27 – 由于架构变化导致的期望失败
- en: It also alerted us about the `sepal_length` variable as all the values are unexpected
    and outside of the accepted range of values it has seen so far!
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 它还提醒了我们 `sepal_length` 变量，因为所有值都不符合预期，超出了它所看到的可接受范围！
- en: '![Figure 3.28 – Expectations failures because of out-of-range values](img/B19801_03_28.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.28 – 由于超出范围的值导致的期望失败](img/B19801_03_28.jpg)'
- en: Figure 3.28 – Expectations failures because of out-of-range values
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.28 – 由于超出范围的值导致的期望失败
- en: Can you see how many problems it could save us from? If this data was not checked
    and had been ingested, the subsequent processes and integration pipelines would
    fail, and a lot of work would be needed to try and identify which process failed
    and why. In our case, we know exactly where the problem started, and we have a
    clear idea of what we need to do to fix it.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你能看到它能为我们节省多少问题吗？如果这批数据没有经过检查并直接被导入，后续的处理和集成管道会失败，并且需要大量工作来确定哪个流程失败以及原因。在我们的案例中，我们清楚地知道问题从哪里开始，并且有明确的修复方法。
- en: Note
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Checkpoints are designed to be reusable, so you can run the same checkpoint
    configuration against multiple batches of data as they arrive. This allows you
    to consistently validate incoming data against the same set of Expectations. Additionally,
    checkpoints can be enhanced with various actions, such as sending notifications,
    updating data documentation (Data Docs), or triggering downstream processes based
    on the validation results.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点设计为可重用的，因此你可以在多个数据批次到达时，使用相同的检查点配置来运行。这使得你能够始终如一地验证传入数据是否符合相同的期望集。此外，检查点可以通过各种操作进行增强，例如发送通知、更新数据文档（Data
    Docs），或根据验证结果触发下游流程。
- en: Now, if you are impressed by the automation that Great Expectations provides
    and you wish to see how you can migrate all the pandas profiling you’ve been doing
    so far to Great Expectations Suites, then we’ve got you covered. Just keep reading.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你对 Great Expectations 提供的自动化印象深刻，并希望了解如何将你迄今为止使用 pandas profiling 的所有内容迁移到
    Great Expectations Suites 中，那么我们为你准备了相关内容。继续阅读吧。
- en: Using pandas profiler to build your Great Expectations Suite
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 pandas profiler 构建你的 Great Expectations Suite
- en: 'The pandas profiler has a functionality that allows you to build Expectation
    Suites out of a pandas profiling exercise. Let’s look at the following example
    `great_expectations/code/3.with_pandas_profiler.py`:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: pandas profiler 具有一个功能，允许你通过 pandas profiling 过程构建 Expectation Suites。让我们看一下以下示例
    `great_expectations/code/3.with_pandas_profiler.py`：
- en: '[PRE31]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In this code snippet, we have taken our data and have created a pandas profiling.
    Then, we obtained an Expectation Suite from the report created previously. We
    can use this suite to further validate and check another batch of data.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码示例中，我们获取了数据并创建了一个 pandas profiling。接着，我们从之前创建的报告中获得了一个 Expectation Suite。我们可以使用这个套件进一步验证并检查另一批数据。
- en: So far, we have reviewed different profiling tools and how they work. The next
    step is to get a better understanding of when to use which tool and where to start.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回顾了不同的分析工具及其工作原理。接下来的步骤是更好地理解何时使用哪种工具以及从哪里开始。
- en: Comparing Great Expectations and pandas profiler – when to use what
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 Great Expectations 和 pandas profiler – 何时使用哪个
- en: Pandas profiling and Great Expectations are both valuable tools for data profiling
    and analysis, but they have different strengths and use cases. Here’s a comparison
    between the two tools.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas profiling 和 Great Expectations 都是数据分析和数据概况分析中有价值的工具，但它们各自有不同的优势和应用场景。以下是对这两种工具的比较。
- en: '|  | **Pandas Profiler** | **Great Expectations** |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | **Pandas Profiler** | **Great Expectations** |'
- en: '| **Data Exploration** | Provides quick insights and exploratory data summaries
    | Focuses on data validation and documentation |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| **数据探索** | 提供快速的洞察和探索性数据总结 | 专注于数据验证和文档编制 |'
- en: '| **Data Validation** | Limited data validation capabilities | Advanced data
    validation with explicit Expectations and rules |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| **数据验证** | 限制的数据验证能力 | 高级数据验证，具有明确的期望和规则 |'
- en: '| **Customization** | Limited customization options | Extensive customization
    for defining Expectations and rules |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| **定制化** | 限制的定制选项 | 提供广泛的定制选项，用于定义期望和规则 |'
- en: '| **Learning Curve** | Relatively easy to use | A steeper learning curve for
    defining Expectations and configuration |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| **学习曲线** | 相对容易使用 | 定义期望和配置时具有较陡的学习曲线 |'
- en: '| **Scalability** | Suitable for small- to medium-scale data | Scalable for
    big data environments with distributed processing |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展性** | 适用于小型到中型数据 | 可扩展至大数据环境，支持分布式处理 |'
- en: '| **Visualizations** | Generates interactive visualizations | Focuses more
    on validating and documenting data rather than visuals |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| **可视化** | 生成交互式可视化 | 更注重数据验证和文档编制，而非可视化 |'
- en: '| **Use Case** | Quick data exploration and initial insights | Data quality
    control and enforcing data consistency |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| **使用案例** | 快速数据探索和初步洞察 | 数据质量控制和强制数据一致性 |'
- en: Table 3.2 – Great Expectations and pandas profiler comparison
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 – Great Expectations与pandas profiler比较
- en: Pandas profiling is well suited for quick data exploration and initial insights,
    while Great Expectations excels in data validation, documentation, and enforcing
    data quality rules. Pandas profiling is more beginner-friendly and provides immediate
    insights, while Great Expectations offers more advanced customization options
    and scalability for larger datasets. The choice between the two depends on the
    specific requirements of the project and the level of data quality control needed.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas profiling非常适合快速的数据探索和初步洞察，而Great Expectations则在数据验证、文档编制和执行数据质量规则方面表现突出。Pandas
    profiling更适合初学者，能提供即时的洞察，而Great Expectations则提供更多的定制选项，并且能够扩展到更大的数据集。选择两者之间的工具，取决于项目的具体需求以及所需的数据质量控制级别。
- en: As the volume of data increases, we need to make sure that the choice of tools
    we’ve made can scale as well. Let’s have a look at how we can do this with Great
    Expectations.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量的增加，我们需要确保所选择的工具也能够进行扩展。让我们看看如何使用Great Expectations实现这一点。
- en: Great Expectations and big data
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Great Expectations与大数据
- en: 'While Great Expectations can be used effectively with smaller datasets, it
    also provides mechanisms to address the challenges associated with scaling data
    validation and documentation for big data environments. Here are some considerations
    for scaling Great Expectations as data size increases:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然《远大前程》可以有效地用于较小的数据集，但它也提供了机制来解决在大数据环境中扩展数据验证和文档编制的挑战。以下是随着数据量增加，扩展Great Expectations的一些注意事项：
- en: '**Distributed processing frameworks**: Great Expectations integrates seamlessly
    with popular distributed processing frameworks, such as Apache Spark. By leveraging
    the parallel processing capabilities of these frameworks, Great Expectations can
    distribute the data validation workload across a cluster, allowing for efficient
    processing and scalability.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式处理框架**：Great Expectations与流行的分布式处理框架（如Apache Spark）无缝集成。通过利用这些框架的并行处理能力，Great
    Expectations可以将数据验证工作负载分布到集群中，从而实现高效的处理和扩展性。'
- en: '**Partitioning and sampling**: Great Expectations simplifies the process of
    partitioning and sampling large datasets and enhancing performances and scalability.
    Unlike the manual partitioning required in tools such as pandas profiling, Great
    Expectations automates the creation of data subsets or partitions for profiling
    and validation. This feature allows you to validate specific subsets or partitions
    of the data, rather than processing the entire dataset at once. By automating
    the partitioning process, Great Expectations streamlines the profiling workflow
    and eliminates the need for manual chunk creation, saving time and effort.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区和采样**：Great Expectations简化了分区和采样大数据集的过程，并提高了性能和可扩展性。与需要在诸如pandas profiling等工具中手动进行分区不同，Great
    Expectations自动创建数据子集或分区以供分析和验证。此功能使您能够验证数据的特定子集或分区，而无需一次处理整个数据集。通过自动化分区过程，Great
    Expectations简化了分析流程，并消除了手动创建数据块的需求，节省了时间和精力。'
- en: '**Incremental validation**: Instead of revalidating the entire big dataset
    every time, Great Expectations supports incremental validation. This means that
    as new data is ingested or processed, only the relevant portions or changes need
    to be validated, reducing the overall validation time and effort. This is a great
    trick to reduce the time it takes to check the whole data and optimize for cost!'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增量验证**：Great Expectations 支持增量验证，而不是每次都重新验证整个大数据集。这意味着当新数据被摄入或处理时，只需要验证相关部分或变化，从而减少整体验证的时间和精力。这是减少检查全部数据所需时间并优化成本的绝佳技巧！'
- en: '**Caching and memoization**: Great Expectations incorporates caching and memoization
    techniques to optimize performance when repeatedly executing the same validations.
    This can be particularly beneficial when working with large datasets, as previously
    computed results can be stored and reused, minimizing redundant computations.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存和记忆化**：Great Expectations 采用缓存和记忆化技术，以优化在重复执行相同验证时的性能。当处理大数据集时，特别有益，因为先前计算的结果可以存储并重复使用，从而最小化冗余计算。'
- en: '**Cloud-based infrastructure**: Leveraging cloud-based infrastructure and services
    can enhance scalability for Great Expectations. By leveraging cloud computing
    platforms, such as AWS or Azure, you can dynamically scale resources to handle
    increased data volumes and processing demands'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于云的基础设施**：利用基于云的基础设施和服务可以提升 Great Expectations 的可扩展性。通过使用云计算平台，如 AWS 或 Azure，你可以动态地扩展资源，以应对增加的数据量和处理需求。'
- en: '**Efficient data storage**: Choosing appropriate data storage technologies
    optimized for big data, such as distributed file systems or columnar databases,
    can improve the performance and scalability of Great Expectations. These technologies
    are designed to handle large-scale data efficiently and provide faster access
    for validation and processing tasks.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效数据存储**：选择适合大数据的优化数据存储技术，如分布式文件系统或列式数据库，可以提升 Great Expectations 的性能和可扩展性。这些技术旨在高效处理大规模数据，并为验证和处理任务提供更快的访问速度。'
- en: Note
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While Great Expectations offers scalability options, the specific scalability
    measures may depend on the underlying infrastructure, data storage systems, and
    distributed processing frameworks employed in your big data environment.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Great Expectations 提供了可扩展性选项，但具体的可扩展性措施可能取决于底层基础设施、数据存储系统和你所使用的大数据环境中的分布式处理框架。
- en: Summary
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter detailed how data profiling is crucial for ensuring the quality,
    integrity, and reliability of datasets. The process involves in-depth analysis
    to understand the structure, patterns, and potential issues within the data. For
    effective profiling, tools such as pandas profiling and Great Expectations offer
    powerful solutions. Pandas profiling automates the generation of comprehensive
    reports, providing valuable insights into data characteristics. Great Expectations,
    on the other hand, facilitates the creation of data quality Expectations and allows
    for systematic validation. While these tools excel in smaller datasets, scaling
    profiling to big data requires specialized approaches. Learning the tips and tricks,
    such as data sampling and parallel processing, enables efficient and scalable
    profiling on large datasets.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细说明了数据分析在确保数据集质量、完整性和可靠性方面的重要性。该过程涉及对数据的深入分析，以了解数据结构、模式和潜在问题。为了进行有效的分析，诸如
    pandas profiling 和 Great Expectations 等工具提供了强大的解决方案。Pandas profiling 自动生成综合报告，提供有关数据特征的宝贵见解。而
    Great Expectations 则便于创建数据质量预期并允许系统化验证。虽然这些工具在小型数据集上表现出色，但将分析扩展到大数据需要专门的方法。学习数据抽样和并行处理等技巧，有助于在大数据集上进行高效且可扩展的分析。
- en: In the next chapter, we will focus on how to clean and manipulate data to make
    sure it is in the right format to pass Expectations and be successfully ingested.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点讨论如何清理和处理数据，确保数据格式正确，能够通过预期验证并成功摄入。
