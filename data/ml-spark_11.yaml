- en: Real-Time Machine Learning with Spark Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark Streaming进行实时机器学习
- en: So far in this book, we have focused on batch data processing. That is, all
    our analysis, feature extraction, and model training has been applied to a fixed
    set of data that does not change. This fits neatly into Spark's core abstraction
    of RDDs, which are immutable distributed datasets. Once created, the data underlying
    the RDD does not change, although we might create new RDDs from the original RDD
    through Spark's transformation and action operators.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们专注于批量数据处理。也就是说，我们所有的分析、特征提取和模型训练都应用于一个不变的数据集。这与Spark的RDD的核心抽象非常契合，RDD是不可变的分布式数据集。一旦创建，RDD的底层数据不会改变，尽管我们可能通过Spark的转换和操作符创建新的RDD。
- en: Our attention has also been on batch machine learning models where we train
    a model on a fixed batch of training data that is usually represented as an RDD
    of feature vectors (and labels, in the case of supervised learning models).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的关注也集中在批量机器学习模型上，我们在固定的批量训练数据集上训练模型，通常表示为特征向量的RDD（在监督学习模型的情况下还有标签）。
- en: 'In this chapter, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: Introduce the concept of online learning, where models are trained and updated
    on new data as it becomes available
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍在线学习的概念，即在新数据可用时训练和更新模型
- en: Explore stream processing using Spark Streaming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索使用Spark Streaming进行流处理
- en: See how Spark Streaming fits together with the online learning approach
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Spark Streaming如何与在线学习方法结合
- en: Introduce the concept of Structured Streaming
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍结构化流处理的概念
- en: The following sections use RDD as the distributed dataset. In a similar way,
    we can use DataFrame or SQL operations on streaming data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分使用RDD作为分布式数据集。类似地，我们可以在流数据上使用DataFrame或SQL操作。
- en: See [https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html](https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html)
    for more details on DataFrame and SQL operations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有关DataFrame和SQL操作的更多详细信息，请参见[https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html](https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html)。
- en: Online learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线学习
- en: The batch machine learning methods that we have applied in this book focus on
    processing an existing fixed set of training data. Typically, these techniques
    are also iterative, and we have performed multiple passes over our training data
    in order to converge to an optimal model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中应用的批量机器学习方法侧重于处理现有的固定训练数据集。通常，这些技术也是迭代的，我们对训练数据进行多次通过以收敛到最佳模型。
- en: By contrast, online learning is based on performing only one sequential pass
    through the training data in a fully incremental fashion (that is, one training
    example at a time). After seeing each training example, the model makes a prediction
    for this example and then receives the true outcome (for example, the label for
    classification or real target for regression). The idea behind online learning
    is that the model continually updates as new information is received, instead
    of being retrained periodically in batch training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在线学习是基于以完全增量的方式对训练数据进行一次顺序通过（即一次处理一个训练样本）。在看到每个训练样本后，模型对该样本进行预测，然后接收真实结果（例如，分类的标签或回归的真实目标）。在线学习的理念是，模型在接收到新信息时不断更新，而不是定期进行批量训练。
- en: In some settings, when the data volume is very large or the process that generates
    the data is changing rapidly, online learning methods can adapt more quickly and
    in near real time, without needing to be retrained in an expensive batch process.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，当数据量非常大或生成数据的过程变化迅速时，在线学习方法可以更快地适应并几乎实时地进行，而无需在昂贵的批处理过程中重新训练。
- en: However, online learning methods do not have to be used in a purely online manner.
    In fact, we have already seen an example of using an online learning model in
    the batch setting when we used **Stochastic gradient descent** (**SGD**) optimization
    to train our classification and regression models. SGD updates the model after
    each training example. However, we still made use of multiple passes over the
    training data in order to converge to a better result.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在纯在线方式下，并不一定非要使用在线学习方法。事实上，当我们使用随机梯度下降（SGD）优化来训练分类和回归模型时，我们已经看到了在批处理设置中使用在线学习模型的例子。SGD在每个训练样本后更新模型。然而，为了收敛到更好的结果，我们仍然对训练数据进行了多次通过。
- en: In the pure online setting, we do not (or perhaps cannot) make multiple passes
    over the training data; hence, we need to process each input as it arrives. Online
    methods also include mini-batch methods where, instead of processing one input
    at a time, we process a small batch of training data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在纯在线设置中，我们不会（或者可能无法）对训练数据进行多次通过；因此，我们需要在输入到达时处理每个输入。在线方法还包括小批量方法，其中我们不是一次处理一个输入，而是处理一小批训练数据。
- en: Online and batch methods can also be combined in real-world situations. For
    example, we can periodically retrain our models offline (say, every day) using
    batch methods. We can then deploy the trained model to production and update it
    using online methods in real time (that is, during the day, in between batch retraining)
    to adapt to any changes in the environment. This is very similar to lambda architecture
    which is a data processing architecture supporting both batch and streaming methods.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在线和批量方法也可以在现实世界的情况下结合使用。例如，我们可以使用批量方法定期（比如每天）对模型进行离线重新训练。然后，我们可以将训练好的模型部署到生产环境，并使用在线方法实时更新（即在批量重新训练之间的白天）以适应环境的任何变化。这与lambda架构非常相似，lambda架构是支持批量和流处理方法的数据处理架构。
- en: As we will see in this chapter, the online learning setting can fit neatly into
    stream processing and the Spark Streaming framework.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本章中看到的，在线学习设置可以很好地适应流处理和Spark Streaming框架。
- en: See [http://en.wikipedia.org/wiki/Online_machine_learning](http://en.wikipedia.org/wiki/Online_machine_learning)
    for more details on online machine learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在线机器学习的更多详细信息，请参见[http://en.wikipedia.org/wiki/Online_machine_learning](http://en.wikipedia.org/wiki/Online_machine_learning)。
- en: Stream processing
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流处理
- en: Before covering online learning with Spark, we will first explore the basics
    of stream processing and introduce the Spark Streaming library.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍使用Spark进行在线学习之前，我们将首先探讨流处理的基础知识，并介绍Spark Streaming库。
- en: In addition to the core Spark API and functionality, the Spark project contains
    another major library (in the same way as MLlib is a major project library) called
    **Spark Streaming**, which focuses on processing data streams in real time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了核心Spark API和功能之外，Spark项目还包含另一个主要库（就像MLlib是一个主要项目库一样）称为**Spark Streaming**，它专注于实时处理数据流。
- en: A data stream is a continuous sequence of records. Common examples include activity
    stream data from a web or mobile application, time-stamped log data, transactional
    data, and event streams from sensor or device networks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流是连续的记录序列。常见的例子包括来自网络或移动应用的活动流数据，时间戳日志数据，交易数据，以及来自传感器或设备网络的事件流。
- en: The batch processing approach typically involves saving the data stream to an
    intermediate storage system (for example, HDFS or a database) and running a batch
    process on the saved data. In order to generate up-to-date results, the batch
    process must be run periodically (for example, daily, hourly, or even every few
    minutes) on the latest data available.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理方法通常涉及将数据流保存到中间存储系统（例如HDFS或数据库），并在保存的数据上运行批处理。为了生成最新的结果，批处理必须定期运行（例如每天，每小时，甚至每几分钟）以处理最新可用的数据。
- en: By contrast, the stream-based approach applies processing to the data stream
    as it is generated. This allows near real-time processing (of the order of a subsecond
    to a few tenths of a second time-frames rather than minutes, hours, days, or even
    weeks with typical batch processing).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于流的方法将处理应用于生成的数据流。这允许几乎实时处理（与典型的批处理相比，处理时间为亚秒到几分之一秒的时间范围，而不是分钟，小时，天甚至几周）。
- en: An introduction to Spark Streaming
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming简介
- en: 'There are a few different general techniques to deal with stream processing.
    Two of the most common ones are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 处理流处理的一些不同的一般技术。其中最常见的两种如下：
- en: Treat each record individually and process it as soon as it is seen.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个记录进行单独处理，并在看到时立即处理。
- en: Combine multiple records into **mini-batches**. These mini-batches can be delineated
    either by time or by the number of records in a batch.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多个记录合并成**小批次**。这些小批次可以根据时间或批次中的记录数量来划分。
- en: 'Spark Streaming takes the second approach. The core primitive in Spark Streaming
    is the **discretized stream**, or **DStream**. A DStream is a sequence of mini-batches,
    where each mini-batch is represented as a Spark RDD:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming采用第二种方法。Spark Streaming的核心原语是**离散流**或**DStream**。DStream是一系列小批次，其中每个小批次都表示为Spark
    RDD：
- en: '![](img/image_11_001.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_11_001.png)'
- en: The discretized stream abstraction
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 离散流抽象
- en: A DStream is defined by its input source and a time window called the **batch
    interval**. The stream is broken up into time periods equal to the batch interval
    (beginning from the starting time of the application). Each RDD in the stream
    will contain the records that are received by the Spark Streaming application
    during a given batch interval. If no data is present in a given interval, the
    RDD will simply be empty.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DStream由其输入源和称为**批处理间隔**的时间窗口定义。流被分成与批处理间隔相等的时间段（从应用程序的起始时间开始）。流中的每个RDD将包含由Spark
    Streaming应用程序在给定批处理间隔期间接收的记录。如果在给定间隔中没有数据，则RDD将为空。
- en: Input sources
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入源
- en: Spark Streaming **receivers** are responsible for receiving data from an **input
    source** and converting the raw data into a DStream made up of Spark RDDs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming **接收器**负责从**输入源**接收数据，并将原始数据转换为由Spark RDD组成的DStream。
- en: Spark Streaming supports various input sources, including file-based sources
    (where the receiver watches for new files arriving at the input location and creates
    the DStream from the contents read from each new file) and network-based sources
    (such as receivers that communicate with socket-based sources, the Twitter API
    stream, Akka actors, or message queues and distributed stream and log transfer
    frameworks, such as Flume, Kafka, and Amazon Kinesis).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming支持各种输入源，包括基于文件的源（其中接收器监视到达输入位置的新文件并从每个新文件中读取的内容创建DStream）和基于网络的源（例如与基于套接字的源，Twitter
    API流，Akka actors或消息队列和分布式流和日志传输框架通信的接收器，如Flume，Kafka和Amazon Kinesis）。
- en: See the documentation on input sources at [http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams](http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams)
    for more details and for links to various advanced sources.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有关输入源的文档，请参阅[http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams](http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams)以获取更多详细信息和链接到各种高级源。
- en: Transformations
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: As we saw in [Chapter 1](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml), *Getting
    Up and Running with Spark*, and throughout this book, Spark allows us to apply
    powerful transformations to RDDs. As DStreams are made up of RDDs, Spark Streaming
    provides a set of transformations available on DStreams; these transformations
    are similar to those available on RDDs. These include `map`, `flatMap`, `filter`,
    `join`, and `reduceByKey`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml)中看到的，*使用Spark快速入门*，以及本书中的其他地方，Spark允许我们对RDD应用强大的转换。由于DStreams由RDD组成，Spark
    Streaming提供了一组可用于DStreams的转换；这些转换类似于RDD上可用的转换。这些包括`map`，`flatMap`，`filter`，`join`和`reduceByKey`。
- en: Spark Streaming transformations, such as those applicable to RDDs, operate on
    each element of a DStream's underlying data. That is, the transformations are
    effectively applied to each RDD in the DStream, which, in turn, applies the transformation
    to the elements of the RDD.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming转换，如适用于RDD的转换，对DStream底层数据的每个元素进行操作。也就是说，转换实际上应用于DStream中的每个RDD，进而将转换应用于RDD的元素。
- en: Spark Streaming also provides operators such as reduce and count. These operators
    return a DStream made up of a single element (for example, the count value for
    each batch). Unlike the equivalent operators on RDDs, these do not trigger computation
    on DStreams directly. That is, they are not actions, but they are still transformations,
    as they return another DStream.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming还提供了诸如reduce和count之类的操作符。这些操作符返回由单个元素组成的DStream（例如，每个批次的计数值）。与RDD上的等效操作符不同，这些操作不会直接触发DStreams上的计算。也就是说，它们不是操作，但它们仍然是转换，因为它们返回另一个DStream。
- en: Keeping track of state
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪状态
- en: When we were dealing with batch processing of RDDs, keeping and updating a state
    variable was relatively straightforward. We could start with a certain state (for
    example, a count or sum of values) and then use broadcast variables or accumulators
    to update this state in parallel. Usually, we would then use an RDD action to
    collect the updated state to the driver and, in turn, update the global state.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理RDD的批处理时，保持和更新状态变量相对简单。我们可以从某个状态开始（例如，值的计数或总和），然后使用广播变量或累加器并行更新此状态。通常，我们会使用RDD操作将更新后的状态收集到驱动程序，并更新全局状态。
- en: With DStreams, this is a little more complex, as we need to keep track of states
    across batches in a fault-tolerant manner. Conveniently, Spark Streaming provides
    the `updateStateByKey` function on a DStream of key-value pairs, which takes care
    of this for us, allowing us to create a stream of arbitrary state information
    and update it with each batch of data seen. For example, the state could be a
    global count of the number of times each key has been seen. The state could, thus,
    represent the number of visits per web page, clicks per advert, tweets per user,
    or purchases per product, for example.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DStreams，这会更加复杂，因为我们需要以容错的方式跨批次跟踪状态。方便的是，Spark Streaming在键值对的DStream上提供了`updateStateByKey`函数，它为我们处理了这一点，允许我们创建任意状态信息的流，并在看到每个批次数据时更新它。例如，状态可以是每个键被看到的次数的全局计数。因此，状态可以表示每个网页的访问次数，每个广告的点击次数，每个用户的推文数，或者每个产品的购买次数，等等。
- en: General transformations
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一般的转换
- en: The Spark Streaming API also exposes a general `transform` function that gives
    us access to the underlying RDD for each batch in the stream. That is, where the
    higher-level functions such as `map` transform a DStream to another DStream, `transform`
    allows us to apply functions from an RDD to another RDD. For example, we can use
    the RDD `join` operator to join each batch of the stream to an existing RDD that
    we computed separately from our streaming application (perhaps, in Spark or some
    other system).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming API还公开了一个通用的`transform`函数，它允许我们访问流中每个批次的基础RDD。也就是说，高级函数如`map`将DStream转换为另一个DStream，而`transform`允许我们将RDD中的函数应用于另一个RDD。例如，我们可以使用RDD
    `join`运算符将流的每个批次与我们在流应用程序之外单独计算的现有RDD进行连接（可能是在Spark或其他系统中）。
- en: The full list of transformations and further information on each of them is
    provided in the Spark documentation at [http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams](http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的转换列表和有关每个转换的更多信息在Spark文档中提供，网址为[http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams](http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams)。
- en: Actions
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作
- en: 'While some of the operators we have seen in Spark Streaming, such as count,
    are not actions as in the batch RDD case, Spark Streaming has the concept of actions
    on DStreams. Actions are output operators that, when invoked, trigger computation
    on the DStream. They are as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在Spark Streaming中看到的一些操作符，如count，在批处理RDD的情况下不是操作，但Spark Streaming具有DStreams上的操作概念。操作是输出运算符，当调用时，会触发DStream上的计算。它们如下：
- en: '`print`: This prints the first 10 elements of each batch to the console and
    is typically used for debugging and testing.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`print`：这将每个批次的前10个元素打印到控制台，通常用于调试和测试。'
- en: '`saveAsObjectFile`, `saveAsTextFiles`, and `saveAsHadoopFiles`: These functions
    output each batch to a Hadoop-compatible filesystem with a filename (if applicable)
    derived from the batch start timestamp.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`saveAsObjectFile`、`saveAsTextFiles`和`saveAsHadoopFiles`：这些函数将每个批次输出到与Hadoop兼容的文件系统，并使用从批次开始时间戳派生的文件名（如果适用）。'
- en: '`forEachRDD`: This operator is the most generic and allows us to apply any
    arbitrary processing to the RDDs within each batch of a DStream. It is used to
    apply side effects, such as saving data to an external system, printing it for
    testing, exporting it to a dashboard, and so on.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forEachRDD`：此运算符是最通用的，允许我们对DStream的每个批次中的RDD应用任意处理。它用于应用副作用，例如将数据保存到外部系统，为测试打印数据，将数据导出到仪表板等。'
- en: Note that, like batch processing with Spark, DStream operators are **lazy**.
    In the same way in which we need to call an action, such as `count`, on an RDD
    to ensure that processing takes place, we need to call one of the preceding action
    operators in order to trigger computation on a DStream. Otherwise, our streaming
    application will not actually perform any computation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与Spark的批处理一样，DStream操作符是**惰性**的。就像我们需要在RDD上调用`count`等操作来确保处理发生一样，我们需要调用前面的操作符之一来触发DStream上的计算。否则，我们的流应用实际上不会执行任何计算。
- en: Window operators
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口操作符
- en: As Spark Streaming operates on time-ordered batched streams of data, it introduces
    a new concept, which is that of **windowing**. A `window` function computes a
    transformation over a sliding window applied to the stream.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark Streaming操作的是按时间顺序排列的批处理数据流，因此引入了一个新概念，即**窗口**。`窗口`函数计算应用于流的滑动窗口的转换。
- en: A window is defined by the length of the window and the sliding interval. For
    example, with a 10-second window and a 5-second sliding interval, we will compute
    results every 5 seconds, based on the latest 10 seconds of data in the DStream.
    For example, we might wish to calculate the top websites by page view numbers
    over the last 10 seconds and recompute this metric every 5 seconds using a sliding
    window.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口由窗口的长度和滑动间隔定义。例如，使用10秒的窗口和5秒的滑动间隔，我们将每5秒计算一次结果，基于DStream中最新的10秒数据。例如，我们可能希望计算过去10秒内页面浏览次数最多的网站，并使用滑动窗口每5秒重新计算这个指标。
- en: 'The following figure illustrates a windowed DStream:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了一个窗口化的DStream：
- en: '![](img/image_11_002.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_11_002.png)'
- en: A windowed DStream
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口化的DStream
- en: Caching and fault tolerance with Spark Streaming
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark Streaming进行缓存和容错处理
- en: Like Spark RDDs, DStreams can be cached in memory. The use cases for caching
    are similar to those for RDDs-if we expect to access the data in a DStream multiple
    times (perhaps performing multiple types of analysis or aggregation or outputting
    to multiple external systems), we will benefit from caching the data. Stateful
    operators, which include `window` functions and `updateStateByKey`, do this automatically
    for efficiency.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与Spark RDD类似，DStreams可以被缓存在内存中。缓存的用例与RDD的用例类似-如果我们希望多次访问DStream中的数据（可能执行多种类型的分析或聚合，或者输出到多个外部系统），那么缓存数据将会有所好处。状态操作符，包括`window`函数和`updateStateByKey`，会自动进行这种操作以提高效率。
- en: Recall that RDDs are immutable datasets and are defined by their input data
    source and **lineage**-that is, the set of transformations and actions that are
    applied to the RDD. Fault tolerance in RDDs works by recreating the RDD (or partition
    of an RDD) that is lost due to the failure of a worker node.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，RDD是不可变的数据集，并且由其输入数据源和**血统**（即应用于RDD的一系列转换和操作）来定义。RDD的容错性是通过重新创建由于工作节点故障而丢失的RDD（或RDD的分区）来实现的。
- en: As DStreams are themselves batches of RDDs, they can also be recomputed as required
    to deal with worker node failure. However, this depends on the input data still
    being available. If the data source itself is fault-tolerant and persistent (such
    as HDFS or some other fault-tolerant data store), then the DStream can be recomputed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DStreams本身是RDD的批处理，它们也可以根据需要重新计算以处理工作节点的故障。然而，这取决于输入数据是否仍然可用。如果数据源本身是容错和持久的（例如HDFS或其他容错的数据存储），那么DStream可以被重新计算。
- en: If data stream sources are delivered over a network (which is a common case
    with stream processing), Spark Streaming's default persistence behavior is to
    replicate data to two worker nodes. This allows network DStreams to be recomputed
    in the case of failure. Note, however, that any data received by a node but *not
    yet replicated* might be lost when a node fails.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据流源通过网络传输（这在流处理中很常见），Spark Streaming的默认持久化行为是将数据复制到两个工作节点。这允许在发生故障时重新计算网络DStreams。然而，请注意，当一个节点失败时，任何接收到但*尚未复制*的数据可能会丢失。
- en: Spark Streaming also supports recovery of the driver node in the event of failure.
    However, currently, for network-based sources, data in the memory of worker nodes
    will be lost in this case. Hence, Spark Streaming is not fully fault-tolerant
    in the face of failure of the driver node or application. Instead lambda architecture
    can be used here. For example, nightly batch can come through and correct things
    in the case of a failure.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming还支持在驱动节点发生故障时进行恢复。然而，目前对于基于网络的数据源，工作节点内存中的数据在这种情况下将会丢失。因此，Spark
    Streaming在面对驱动节点或应用程序故障时并不完全容错。而是可以使用lambda架构。例如，夜间批处理可以通过并在发生故障时纠正事情。
- en: See [http://spark.apache.org/docs/latest/streaming-programming-guide.html#caching-persistence](http://spark.apache.org/docs/latest/streaming-programming-guide.html#caching-persistence)
    and [http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-properties](http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-properties)
    for more details.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见[http://spark.apache.org/docs/latest/streaming-programming-guide.html#caching-persistence](http://spark.apache.org/docs/latest/streaming-programming-guide.html#caching-persistence)和[http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-properties](http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-properties)。
- en: Creating a basic streaming application
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建基本的流应用程序
- en: We will now work through creating our first Spark Streaming application to illustrate
    some of the basic concepts around Spark Streaming that we introduced earlier.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过创建我们的第一个Spark Streaming应用程序来说明我们之前介绍的Spark Streaming的一些基本概念。
- en: We will expand on the example applications used in [Chapter 1](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml),
    *Getting Up and Running with Spark*, where we used a small example dataset of
    product purchase events. For this example, instead of using a static set of data,
    we will create a simple producer application that will randomly generate events
    and send them over a network connection. We will then create a few Spark Streaming
    consumer applications that will process this event stream.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将扩展[第1章](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml)中使用的示例应用程序，*使用Spark快速上手*，在那里我们使用了一个小型的产品购买事件示例数据集。在这个例子中，我们将创建一个简单的生产者应用程序，随机生成事件并通过网络连接发送。然后，我们将创建一些Spark
    Streaming消费者应用程序来处理这个事件流。
- en: The sample project for this chapter contains the code you will need. It is called
    `scala-spark-streaming-app`. It consists of a Scala SBT project definition file,
    the example application source code, and a `srcmainresources` directory that contains
    a file called `names.csv`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例项目包含您需要的代码。它被称为`scala-spark-streaming-app`。它包括一个Scala SBT项目定义文件，示例应用程序源代码，以及一个包含名为`names.csv`的文件的`srcmainresources`目录。
- en: 'The `build.sbt` file for the project contains the following project definition:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的`build.sbt`文件包含以下项目定义：
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that we added a dependency on Spark MLlib and Spark Streaming, which includes
    the dependency on the Spark core.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们添加了对Spark MLlib和Spark Streaming的依赖，其中包括对Spark核心的依赖。
- en: 'The `names.csv` file contains a set of 20 randomly generated user names. We
    will use these names as part of our data generation function in our producer application:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`names.csv`文件包含一组20个随机生成的用户名。我们将使用这些名称作为我们生产者应用程序中数据生成函数的一部分：'
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The producer application
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产者应用程序
- en: 'Our producer needs to create a network connection and generate some random
    purchase event data to send over this connection. First, we will define our object
    and main method definition. We will then read the random names from the `names.csv`
    resource and create a set of products with prices, from which we will generate
    our random product events:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的生产者需要创建一个网络连接，并生成一些随机购买事件数据发送到这个连接上。首先，我们将定义我们的对象和主方法定义。然后，我们将从`names.csv`资源中读取随机名称，并创建一组带有价格的产品，从中我们将生成我们的随机产品事件：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using the list of names and map of product name to price, we will create a
    function that will randomly pick a product and name from these sources, generating
    a specified number of product events:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用名称列表和产品名称到价格的映射，我们将创建一个函数，该函数将从这些来源中随机选择产品和名称，生成指定数量的产品事件：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we will create a network socket and set our producer to listen on
    this socket. As soon as a connection is made (which will come from our consumer
    streaming application), the producer will start generating random events at a
    random rate between 0 and 5 per second:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将创建一个网络套接字，并设置我们的生产者监听此套接字。一旦建立连接（这将来自我们的消费者流应用程序），生产者将开始以每秒0到5个之间的随机速率生成随机事件：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This producer example is based on the `PageViewGenerator` example in the Spark
    Streaming examples.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个生产者示例是基于Spark Streaming示例中的`PageViewGenerator`示例。
- en: 'The producer can be run by changing into the base directory of `scala-spark-streaming-app`
    and using SBT to run the application, as we did in [Chapter 1](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml),
    *Getting Up and Running with Spark*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过转到`scala-spark-streaming-app`的基本目录并使用SBT运行应用程序来运行生产者，就像我们在[第1章](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml)中所做的那样，*使用Spark快速启动*：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Use the `run` command to execute the application:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`run`命令来执行应用程序：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should see output similar to the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似以下的输出：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Select the `StreamingProducer` option. The application will start running,
    and you should see the following output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 选择`StreamingProducer`选项。应用程序将开始运行，您应该看到以下输出：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can see that the producer is listening on port `9999`, waiting for our consumer
    application to connect.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到生产者正在端口`9999`上监听，等待我们的消费者应用连接。
- en: Creating a basic streaming application
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建基本的流式应用程序
- en: 'Next, we will create our first streaming program. We will simply connect to
    the producer and print out the contents of each batch. Our streaming code looks
    like this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建我们的第一个流式程序。我们将简单地连接到生产者并打印出每个批次的内容。我们的流式代码如下：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It looks fairly simple, and it is mostly due to the fact that Spark Streaming
    takes care of all the complexity for us. First, we initialized a `StreamingContext`
    (which is the streaming equivalent of the `SparkContext` we have used so far),
    specifying similar configuration options that are used to create a `SparkContext`.
    Notice, however, that here we are required to provide the batch interval, which
    we set to 10 seconds.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来相当简单，这主要是因为Spark Streaming为我们处理了所有复杂性。首先，我们初始化了一个`StreamingContext`（这是我们迄今为止使用的`SparkContext`的流式等价物），指定了用于创建`SparkContext`的类似配置选项。但是请注意，这里我们需要提供批处理间隔，我们将其设置为10秒。
- en: We then created our data stream using a predefined streaming source, `socketTextStream`,
    which reads text from a socket host and port and creates a `DStream[String]`.
    We then called the `print` function on the DStream; this function prints out the
    first few elements of each batch.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用预定义的流式源`socketTextStream`创建了我们的数据流，该流从套接字主机和端口读取文本，并创建了一个`DStream[String]`。然后我们在DStream上调用`print`函数；这个函数打印出每个批次的前几个元素。
- en: Calling `print` on a DStream is similar to calling `take` on an RDD. It displays
    only the first few elements.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在DStream上调用`print`类似于在RDD上调用`take`。它只显示前几个元素。
- en: 'We can run this program using SBT. Open a second terminal window, leaving the
    producer program running, and run `sbt`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用SBT运行此程序。打开第二个终端窗口，保持生产者程序运行，并运行`sbt`：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Again, you should see a few options to select:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您应该看到几个选项可供选择：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Run the `SimpleStreamingApp` main class. You should see the streaming program
    start up, displaying output similar to the one shown here:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`SimpleStreamingApp`主类。您应该看到流式程序启动，并显示类似于此处显示的输出：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At the same time, you should see that the terminal window running the producer
    displays something like the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，您应该看到运行生产者的终端窗口显示类似以下内容：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After about 10 seconds, which is the time of our streaming batch interval,
    Spark Streaming will trigger a computation on the stream due to our use of the
    `print` operator. This should display the first few events in the batch, which
    will look something like the following output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 大约10秒后，这是我们流式批处理间隔的时间，由于我们使用了`print`运算符，Spark Streaming将在流上触发计算。这应该显示批次中的前几个事件，看起来类似以下输出：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that you might see different results, as the producer generates a random
    number of random events each second.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可能会看到不同的结果，因为生产者每秒生成随机数量的随机事件。
- en: You can terminate the streaming app by pressing *Ctrl* + *C*. If you want to,
    you can also terminate the producer (if you do, you will need to restart it again
    before starting the next streaming programs that we will create).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过按*Ctrl* + *C*来终止流式应用程序。如果您愿意，您也可以终止生产者（如果这样做，您将需要在创建我们将创建的下一个流式程序之前重新启动它）。
- en: Streaming analytics
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式分析
- en: Next, we will create a slightly more complex streaming program. In [Chapter
    1](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml), *Getting Up and Running with Spark*,
    we calculated a few metrics on our dataset of product purchases. These included
    the total number of purchases, the number of unique users, the total revenue,
    and the most popular product (together with its number of purchases and total
    revenue).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个稍微复杂一些的流式处理程序。在[第1章](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml)中，*使用Spark快速上手*，我们对产品购买数据集计算了一些指标。这些指标包括购买总数、唯一用户数、总收入以及最受欢迎的产品（以及其购买数量和总收入）。
- en: In this example, we will compute the same metrics on our stream of purchase
    events. The key difference is that these metrics will be computed per batch and
    printed out.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将在购买事件流上计算相同的指标。关键区别在于这些指标将按批次计算并打印出来。
- en: 'We will define our streaming application code here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里定义我们的流应用程序代码：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: First, we created exactly the same `StreamingContext` and socket stream as we
    did earlier. Our next step is to apply a `map` transformation to the raw text,
    where each record is a comma-separated string representing the purchase event.
    The `map` function splits the text and creates a tuple of `(user, product, price)`.
    This illustrates the use of `map` on a DStream and how it is the same as if we
    had been operating on an RDD.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建了完全相同的`StreamingContext`和套接字流，就像我们之前做的那样。我们的下一步是对原始文本应用`map`转换，其中每条记录都是表示购买事件的逗号分隔字符串。`map`函数将文本拆分并创建一个`(用户，产品，价格)`元组。这说明了在DStream上使用`map`以及它与在RDD上操作时的相同之处。
- en: 'Next, we will use `foreachRDD` to apply arbitrary processing on each RDD in
    the stream to compute our desired metrics and print them to the console:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`foreachRDD`在流中的每个RDD上应用任意处理，以计算我们需要的指标并将其打印到控制台：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you compare the code operating on the RDDs inside the preceding `foreachRDD`
    block with that used in [Chapter 1](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml),
    *Getting Up and Running with Spark*, you will notice that it is virtually the
    same code. This shows that we can apply any RDD-related processing we wish within
    the streaming setting by operating on the underlying RDDs, as well as using the
    built-in higher level streaming operations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您比较一下在前面的`foreachRDD`块中操作RDD的代码与[第1章](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml)中使用的代码，*使用Spark快速上手*，您会注意到它们几乎是相同的代码。这表明我们可以通过操作底层RDD以及使用内置的更高级别的流操作，在流设置中应用任何与RDD相关的处理。
- en: Let's run the streaming program again by calling `sbt run` and selecting `StreamingAnalyticsApp`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`sbt run`并选择`StreamingAnalyticsApp`来再次运行流处理程序。
- en: Remember that you might also need to restart the producer if you previously
    terminated the program. This should be done before starting the streaming application.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，如果之前终止了程序，您可能还需要重新启动生产者。这应该在启动流应用程序之前完成。
- en: 'After about 10 seconds, you should see output from the streaming program similar
    to the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 大约10秒后，您应该会看到与以下类似的流处理程序输出：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can again terminate the streaming program using *Ctrl* + *C*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以再次使用*Ctrl* + *C*终止流处理程序。
- en: Stateful streaming
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态的流式处理
- en: 'As a final example, we will apply the concept of **stateful** streaming using
    the `updateStateByKey` function to compute a global state of revenue and number
    of purchases per user, which will be updated with new data from each 10-second
    batch. Our `StreamingStateApp` app is shown here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`updateStateByKey`函数应用**有状态**流式处理的概念，以计算每个用户的全局收入和购买数量的状态，并将其与每个10秒批次的新数据进行更新。我们的`StreamingStateApp`应用程序如下所示：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We will first define an `updateState` function that will compute the new state
    from the running state value and the new data in the current batch. Our state,
    in this case, is a tuple of `(number of products, revenue)` pairs, which we will
    keep for each user. We will compute the new state given the set of `(product,
    revenue)` pairs for the current batch and the accumulated state at the current
    time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个`updateState`函数，该函数将根据当前批次的新数据和运行状态值计算新状态。在这种情况下，我们的状态是一个`(产品数量，收入)`对的元组，我们将为每个用户保留这些状态。我们将根据当前批次的`(产品，收入)`对集合和当前时间的累积状态计算新状态。
- en: 'Notice that we will deal with an `Option` value for the current state, as it
    might be empty (which will be the case for the first batch), and we need to define
    a default value, which we will do using `getOrElse` as shown here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将处理当前状态的`Option`值，因为它可能为空（这将是第一个批次的情况），我们需要定义一个默认值，我们将使用`getOrElse`来实现，如下所示：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After applying the same string split transformation we used in our previous
    example, we called `updateStateByKey` on our DStream, passing in our defined `updateState`
    function. We then printed the results to the console.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用了与之前示例中相同的字符串拆分转换后，我们在DStream上调用了`updateStateByKey`，传入了我们定义的`updateState`函数。然后我们将结果打印到控制台。
- en: Start the streaming example using `sbt run` and by selecting `[4] StreamingStateApp`
    (also restart the producer program if necessary).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sbt run`启动流处理示例，并选择`[4] StreamingStateApp`（如果需要，也重新启动生产者程序）。
- en: 'After around 10 seconds, you will start to see the first set of state outputs.
    We will wait another 10 seconds to see the next set of outputs. You will see the
    overall global state being updated:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 大约10秒后，您将开始看到第一组状态输出。我们将再等待10秒钟以查看下一组输出。您将看到整体全局状态正在更新：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can see that the number of purchases and revenue totals for each user are
    added to with each batch of data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个用户的购买数量和收入总额在每个数据批次中都会增加。
- en: Now, see if you can adapt this example to use Spark Streaming's `window` functions.
    For example, you can compute similar statistics per user over the past minute,
    sliding every 30 seconds.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看您是否可以将此示例调整为使用Spark Streaming的`window`函数。例如，您可以每隔30秒滑动一次，在过去一分钟内计算每个用户的类似统计信息。
- en: Online learning with Spark Streaming
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark Streaming进行在线学习
- en: As we have seen, Spark Streaming makes it easy to work with data streams in
    a way that should be familiar to us from working with RDDs. Using Spark's stream
    processing primitives combined with the online learning capabilities of ML Library
    SGD-based methods, we can create real-time machine learning models that we can
    update on new data in the stream as it arrives.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，Spark Streaming使得以与使用RDD类似的方式处理数据流变得容易。使用Spark的流处理原语结合ML Library SGD-based方法的在线学习能力，我们可以创建实时的机器学习模型，并在流中的新数据到达时更新它们。
- en: Streaming regression
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式回归
- en: Spark provides a built-in streaming machine learning model in the `StreamingLinearAlgorithm`
    class. Currently, only a linear regression implementation is available-`StreamingLinearRegressionWithSGD`-but
    future versions will include classification.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在`StreamingLinearAlgorithm`类中提供了内置的流式机器学习模型。目前，只有线性回归实现可用-`StreamingLinearRegressionWithSGD`-但未来版本将包括分类。
- en: 'The streaming regression model provides two methods for usage:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 流式回归模型提供了两种使用方法：
- en: '`trainOn`: This takes `DStream[LabeledPoint]` as its argument. This tells the
    model to train on every batch in the input DStream. It can be called multiple
    times to train on different streams.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainOn`：这需要`DStream[LabeledPoint]`作为其参数。这告诉模型在输入DStream的每个批次上进行训练。可以多次调用以在不同的流上进行训练。'
- en: '`predictOn`: This also takes `DStream[LabeledPoint]`. This tells the model
    to make predictions on the input DStream, returning a new `DStream[Double]` that
    contains the model predictions.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predictOn`：这也接受`DStream[LabeledPoint]`。这告诉模型对输入DStream进行预测，返回一个包含模型预测的新`DStream[Double]`。'
- en: Under the hood, the streaming regression model uses `foreachRDD` and `map` to
    accomplish this. It also updates the model variable after each batch and exposes
    the latest trained model, which allows us to use this model in other applications
    or save it to an external location.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，流式回归模型使用`foreachRDD`和`map`来完成这一点。它还在每个批次后更新模型变量，并公开最新训练的模型，这使我们可以在其他应用程序中使用这个模型或将其保存到外部位置。
- en: The streaming regression model can be configured with parameters for step size
    and number of iterations in the same way as standard batch regression-the model
    class used is the same. We can also set the initial model weight vector.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 流式回归模型可以像标准批处理回归一样配置步长和迭代次数的参数-使用的模型类是相同的。我们还可以设置初始模型权重向量。
- en: When we first start training a model, we can set the initial weights to a zero
    vector, or a random vector, or perhaps load the latest model from the result of
    an offline batch process. We can also decide to save the model periodically to
    an external system and use the latest model state as the starting point (for example,
    in the case of a restart after a node or application failure).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们首次开始训练模型时，可以将初始权重设置为零向量，或随机向量，或者从离线批处理过程的结果中加载最新模型。我们还可以决定定期将模型保存到外部系统，并使用最新模型状态作为起点（例如，在节点或应用程序故障后重新启动时）。
- en: A simple streaming regression program
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的流式回归程序
- en: To illustrate the use of streaming regression, we will create a simple example
    similar to the preceding one, which uses simulated data. We will write a producer
    program that generates random feature vectors and target variables, given a fixed,
    known weight vector, and writes each training example to a network stream.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明流式回归的使用，我们将创建一个类似于前面的简单示例，使用模拟数据。我们将编写一个生成器程序，它生成随机特征向量和目标变量，给定一个已知的固定权重向量，并将每个训练示例写入网络流。
- en: Our consumer application will run a streaming regression model, training and
    then testing on our simulated data stream. Our first example consumer will simply
    print its predictions to the console.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的消费应用程序将运行一个流式回归模型，对我们的模拟数据流进行训练和测试。我们的第一个示例消费者将简单地将其预测打印到控制台上。
- en: Creating a streaming data producer
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建流式数据生成器
- en: The data producer operates in a manner similar to our product event producer
    example. Recall from [Chapter 5](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml),
    *Building a Recommendation Engine with Spark*, that a linear model is a linear
    combination (or vector dot product) of a weight vector, *w*, and a feature vector,
    *x* (that is, *wTx*). Our producer will generate synthetic data using a fixed,
    known weight vector and randomly generated feature vectors. This data fits the
    linear model formulation exactly, so we will expect our regression model to learn
    the true weight vector fairly easily.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成器的操作方式类似于我们的产品事件生成器示例。回想一下[第5章](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml)中的*使用Spark构建推荐引擎*，线性模型是权重向量*w*和特征向量*x*（即*wTx*）的线性组合（或向量点积）。我们的生成器将使用固定的已知权重向量和随机生成的特征向量生成合成数据。这些数据完全符合线性模型的制定，因此我们期望我们的回归模型能够很容易地学习到真实的权重向量。
- en: 'First, we will set up a maximum number of events per second (say, 100) and
    the number of features in our feature vector (also 100 in this example):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将设置每秒的最大事件数（比如100）和特征向量中的特征数（在本例中也是100）：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `generateRandomArray` function creates an array of the specified size where
    the entries are randomly generated from a normal distribution. We will use this
    function initially to generate our known weight vector, `w`, which will be fixed
    throughout the life of the producer. We will also create a random `intercept`
    value that will also be fixed. The weight vector and `intercept` will be used
    to generate each data point in our stream:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`generateRandomArray`函数创建指定大小的数组，其中的条目是从正态分布中随机生成的。我们将首先使用这个函数来生成我们已知的固定权重向量`w`，它将在生成器的整个生命周期内保持不变。我们还将创建一个随机的`intercept`值，它也将是固定的。权重向量和`intercept`将用于生成我们流中的每个数据点：'
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will also need a function to generate a specified number of random data
    points. Each event is made up of a random feature vector and the target that we
    get from computing the dot product of our known weight vector with the random
    feature vector and adding the `intercept` value:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个函数来生成指定数量的随机数据点。每个事件由一个随机特征向量和我们通过计算已知权重向量与随机特征向量的点积并添加`intercept`值得到的目标组成：
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we will use code similar to our previous producer to instantiate a
    network connection and send a random number of data points (between 0 and 100)
    in text format over the network each second:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用类似于之前生产者的代码来实例化网络连接，并每秒以文本格式通过网络发送随机数量的数据点（介于0和100之间）：
- en: '[PRE24]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can start the producer using `sbt run`, followed by choosing to execute
    the `StreamingModelProducer` main method. This should result in the following
    output, thus indicating that the producer program is waiting for connections from
    our streaming regression application:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`sbt run`启动生产者，然后选择执行`StreamingModelProducer`的主方法。这应该会导致以下输出，从而表明生产者程序正在等待来自我们流式回归应用程序的连接：
- en: '[PRE25]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Creating a streaming regression model
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建流式回归模型
- en: 'In the next step in our example, we will create a streaming regression program.
    The basic layout and setup is the same as our previous streaming analytics examples:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例的下一步中，我们将创建一个流式回归程序。基本布局和设置与我们之前的流式分析示例相同：
- en: '[PRE26]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, we will set up the number of features to match the records in our input
    data stream. We will then create a zero vector to use as the initial weight vector
    of our streaming regression model. Finally, we will select the number of iterations
    and step size:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将设置特征数量，以匹配输入数据流中的记录。然后，我们将创建一个零向量，用作流式回归模型的初始权重向量。最后，我们将选择迭代次数和步长：
- en: '[PRE27]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we will again use the `map` function to transform the input DStream,
    where each record is a string representation of our input data, into a `LabeledPoint`
    instance that contains the target value and feature vector:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将再次使用`map`函数将输入DStream转换为`LabeledPoint`实例，其中每个记录都是我们输入数据的字符串表示，包含目标值和特征向量：
- en: '[PRE28]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The final step is to tell the model to train and test on our transformed DStream
    and also to print out the first few elements of each batch in the DStream of predicted
    values:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是告诉模型在转换后的DStream上进行训练和测试，并打印出每个批次中前几个元素的预测值DStream：
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that because we are using the same MLlib model classes for streaming as
    we did for batch processing, we can, if we choose, perform multiple iterations
    over the training data in each batch (which is just an RDD of `LabeledPoint` instances).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，因为我们在流式处理中使用了与批处理相同的MLlib模型类，如果选择，我们可以对每个批次的训练数据进行多次迭代（这只是`LabeledPoint`实例的RDD）。
- en: Here, we will set the number of iterations to `1` to simulate purely online
    learning. In practice, you can set the number of iterations higher, but note that
    the training time per batch will go up. If the training time per batch is much
    higher than the batch interval, the streaming model will start to lag behind the
    velocity of the data stream.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将把迭代次数设置为`1`，以模拟纯在线学习。在实践中，您可以将迭代次数设置得更高，但请注意，每批训练时间会增加。如果每批训练时间远远高于批间隔时间，流式模型将开始落后于数据流的速度。
- en: This can be handled by decreasing the number of iterations, increasing the batch
    interval, or increasing the parallelism of our streaming program by adding more
    Spark workers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过减少迭代次数、增加批间隔时间或通过添加更多Spark工作节点来增加流式程序的并行性来处理。
- en: 'Now, we''re ready to run `SimpleStreamingModel` in our second terminal window
    using `sbt run` in the same way as we did for the producer (remember to select
    the correct main method for SBT to execute). Once the streaming program starts
    running, you should see the following output in the producer console:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备在第二个终端窗口中使用`sbt run`运行`SimpleStreamingModel`，方式与我们为生产者所做的方式相同（记住选择正确的主方法以供SBT执行）。一旦流式程序开始运行，您应该在生产者控制台中看到以下输出：
- en: '[PRE30]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After about 10 seconds, you should start seeing the model predictions being
    printed to the streaming application console, similar to those shown here:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 大约10秒后，您应该开始看到模型预测被打印到流式应用程序控制台，类似于这里显示的内容：
- en: '[PRE31]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Congratulations! You've created your first streaming online learning model!
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经创建了您的第一个流式在线学习模型！
- en: You can shut down the streaming application (and, optionally, the producer)
    by pressing *Ctrl* + *C* in each terminal window.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在每个终端窗口中按下*Ctrl* + *C*来关闭流式应用程序（以及可选地关闭生产者）。
- en: Streaming K-means
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式K均值
- en: MLlib also includes a streaming version of K-means clustering; this is called
    `StreamingKMeans`. This model is an extension of the mini-batch K-means algorithm
    where the model is updated with each batch based on a combination between the
    cluster centers computed from the previous batches and the cluster centers computed
    for the current batch.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib还包括K均值聚类的流式版本；这被称为`StreamingKMeans`。该模型是小批量K均值算法的扩展，其中模型根据前几批计算的簇中心和当前批计算的簇中心的组合进行更新。
- en: '`StreamingKMeans` supports a *forgetfulness* parameter *alpha* (set using the
    `setDecayFactor` method); this controls how aggressive the model is in giving
    weight to newer data. An alpha value of 0 means the model will only use new data,
    while with an alpha value of `1`, all data since the beginning of the streaming
    application will be used.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingKMeans`支持*遗忘*参数*alpha*（使用`setDecayFactor`方法设置）；这控制模型在给予新数据权重时的侵略性。alpha值为0意味着模型只使用新数据，而alpha值为`1`时，自流应用程序开始以来的所有数据都将被使用。'
- en: We will not cover streaming K-means further here (the Spark documentation at
    [http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-clustering](http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-clustering)
    contains further detail and an example). However, perhaps you could try to adapt
    the preceding streaming regression data producer to generate input data for a
    `StreamingKMeans` model. You could also adapt the streaming regression application
    to use `StreamingKMeans`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不在这里进一步介绍流式K均值（Spark文档[http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-clustering](http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-clustering)中包含更多细节和示例）。但是，也许您可以尝试将前面的流式回归数据生成器调整为生成`StreamingKMeans`模型的输入数据。您还可以调整流式回归应用程序以使用`StreamingKMeans`。
- en: 'You can create the clustering data producer by first selecting a number of
    clusters, *K*, and then generating each data point by:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过首先选择簇数*K*，然后通过以下方式生成每个数据点来创建聚类数据生成器：
- en: Randomly selecting a cluster index.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择一个簇索引。
- en: Generating a random vector using specific normal distribution parameters for
    each cluster. That is, each of the *K* clusters will have a mean and variance
    parameter, from which the random vectors will be generated using an approach similar
    to our preceding `generateRandomArray` function.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特定正态分布参数生成随机向量以用于每个簇。也就是说，每个*K*簇将具有均值和方差参数，从中将使用类似于我们前面的`generateRandomArray`函数的方法生成随机向量。
- en: In this way, each data point that belongs to the same cluster will be drawn
    from the same distribution, so our streaming clustering model should be able to
    learn the correct cluster centers over time.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，属于同一簇的每个数据点将从相同的分布中抽取，因此我们的流式聚类模型应该能够随着时间学习正确的簇中心。
- en: Online model evaluation
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线模型评估
- en: Combining machine learning with Spark Streaming has many potential applications
    and use cases, including keeping a model or set of models up to date on new training
    data as it arrives, thus enabling them to adapt quickly to changing situations
    or contexts.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习与Spark Streaming结合使用有许多潜在的应用和用例，包括使模型或一组模型随着新的训练数据的到来保持最新，从而使它们能够快速适应不断变化的情况或背景。
- en: Another useful application is to track and compare the performance of multiple
    models in an online manner and, possibly, also perform model selection in real
    time so that the best performing model is always used to generate predictions
    on live data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的应用是以在线方式跟踪和比较多个模型的性能，并可能实时执行模型选择，以便始终使用性能最佳的模型来生成实时数据的预测。
- en: This can be used to do real-time "A/B testing" of models, or combined with more
    advanced online selection and learning techniques, such as Bayesian update approaches
    and bandit algorithms. It can also be used simply to monitor model performance
    in real time, thus being able to respond or adapt if performance degrades for
    some reason.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用于对模型进行实时的“A/B测试”，或与更高级的在线选择和学习技术结合使用，例如贝叶斯更新方法和赌博算法。它也可以简单地用于实时监控模型性能，从而能够在某些情况下做出响应或调整。
- en: In this section, we will walk through a simple extension to our streaming regression
    example. In this example, we will compare the evolving error rate of two models
    with different parameters as they see more and more data in our input stream.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍对我们的流式回归示例的简单扩展。在这个例子中，我们将比较两个具有不同参数的模型随着在输入流中看到更多数据而不断变化的误差率。
- en: Comparing model performance with Spark Streaming
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark Streaming比较模型性能
- en: As we have used a known weight vector and intercept to generate the training
    data in our producer application, we would expect our model to eventually learn
    this underlying weight vector (in the absence of random noise, which we do not
    add for this example).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在生产者应用程序中使用已知的权重向量和截距生成训练数据，我们期望我们的模型最终能够学习到这个潜在的权重向量（在本例中我们没有添加随机噪声）。
- en: Therefore, we should see the model's error rate decrease over time, as it sees
    more and more data. We can also use standard regression error metrics to compare
    the performance of multiple models.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应该看到模型的误差率随着时间的推移而减少，因为它看到越来越多的数据。我们还可以使用标准的回归误差指标来比较多个模型的性能。
- en: In this example, we will create two models with different learning rates, training
    them both on the same data stream. We will then make predictions for each model
    and measure the **mean-squared error** (**MSE**) and **root mean-squared error**
    (**RMSE**) metrics for each batch.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将创建两个具有不同学习率的模型，同时在相同的数据流上对它们进行训练。然后我们将为每个模型进行预测，并测量每个批次的均方误差（MSE）和均方根误差（RMSE）指标。
- en: 'Our new monitored streaming model code is shown here:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新监控流模型代码如下：
- en: '[PRE32]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Note that most of the preceding setup code is the same as our simple streaming
    model example. However, we created two instances of `StreamingLinearRegressionWithSGD`:
    one with a learning rate of `0.01` and one with the learning rate set to `1.0`.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面大部分的设置代码与我们简单的流模型示例相同。但是，我们创建了两个`StreamingLinearRegressionWithSGD`的实例：一个学习率为`0.01`，另一个学习率设置为`1.0`。
- en: 'Next, we will train each model on our input stream, and using Spark Streaming''s
    `transform` function, we will create a new DStream that contains the error rates
    for each model:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在输入流上训练每个模型，并使用Spark Streaming的`transform`函数创建一个包含每个模型的误差率的新DStream：
- en: '[PRE33]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we will use `foreachRDD` to compute the MSE and RMSE metrics for each
    model and print them to the console:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`foreachRDD`来计算每个模型的MSE和RMSE指标，并将它们打印到控制台上：
- en: '[PRE34]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If you terminated the producer earlier, start it again by executing `sbt run`
    and selecting `StreamingModelProducer`. Once the producer is running again, in
    your second terminal window, execute `sbt run` and choose the main class for `MonitoringStreamingModel`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您之前终止了生产者，请通过执行`sbt run`并选择`StreamingModelProducer`来重新启动它。一旦生产者再次运行，在第二个终端窗口中，执行`sbt
    run`并选择`MonitoringStreamingModel`的主类。
- en: 'You should see the streaming program startup, and after about 10 seconds, the
    first batch will be processed, printing output similar to the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到流式程序启动，大约10秒后，第一批数据将被处理，打印出类似以下的输出：
- en: '[PRE35]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Since both models start with the same initial weight vector, we see that they
    both make the same predictions on this first batch and, therefore, have the same
    error.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两个模型都从相同的初始权重向量开始，我们看到它们在第一批次上都做出了相同的预测，因此具有相同的误差。
- en: 'If we leave the streaming program running for a few minutes, we should eventually
    see that one of the models has started converging, leading to a lower and lower
    error, while the other model has tended to diverge to a poorer model due to the
    overly high learning rate:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们让流式程序运行几分钟，我们应该最终会看到其中一个模型已经开始收敛，导致误差越来越低，而另一个模型由于过高的学习率而趋于发散，变得更差：
- en: '[PRE36]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If you leave the program running for a number of minutes, you should eventually
    see the first model''s error rate becoming quite small:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您让程序运行几分钟，最终应该会看到第一个模型的误差率变得非常小：
- en: '[PRE37]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note again, that due to random data generation, you might see different results,
    but the overall result should be the same-in the first batch, the models will
    have the same error, and subsequently, the first model should start to generate
    to a smaller and smaller error.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，由于随机数据生成，您可能会看到不同的结果，但总体结果应该是相同的-在第一批次中，模型将具有相同的误差，随后，第一个模型应该开始生成越来越小的误差。
- en: Structured Streaming
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: With Spark version 2.0 we have structured streaming which states that the output
    of the application is equal to executing a batch job on a prefix of the data.
    Structured Streaming handles consistency and reliability within the engine and
    in interactions with external systems. Structured Stream is a simple data frame
    and dataset API.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark 2.0版本，我们有结构化流处理，它表示应用程序的输出等同于在数据的前缀上执行批处理作业。结构化流处理处理引擎内部的一致性和可靠性以及与外部系统的交互。结构化流是一个简单的数据框架和数据集API。
- en: Users provide the query they want to run along with the input and output locations.
    The system then executes the query incrementally, maintaining enough state to
    recover from failure, keeping the results consistent in external storage, and
    so on.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提供他们想要运行的查询以及输入和输出位置。然后系统逐渐执行查询，保持足够的状态以从故障中恢复，在外部存储中保持结果的一致性等。
- en: Structured Streaming promises a much simpler model for building real-time applications,
    built on the features that work best in Spark Streaming. However Structured Streaming
    is in alpha for Spark 2.0.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理承诺构建实时应用程序的模型更简单，建立在Spark Streaming中最有效的功能上。然而，结构化流处理在Spark 2.0中处于alpha阶段。
- en: Summary
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we connected some of the dots between online machine learning
    and streaming data analysis. We introduced the Spark Streaming library and API
    for continuous processing of data streams based on familiar RDD functionality
    and we worked through examples of streaming analytics applications that illustrate
    this functionality.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们连接了在线机器学习和流数据分析之间的一些关键点。我们介绍了Spark Streaming库和API，用于基于熟悉的RDD功能进行数据流的连续处理，并通过示例演示了流分析应用程序，说明了这种功能。
- en: Finally, we used ML Library's streaming regression model in a streaming application
    that involves computing and comparing model performance on a stream of input feature
    vectors.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在涉及计算和比较输入特征向量流上的模型性能的流应用程序中使用了ML库的流回归模型。
