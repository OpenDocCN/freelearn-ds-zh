- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Optimization in Multiple Variables
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量优化
- en: Hey! We are at the last checkpoint of our calculus study. What’s missing? Gradient
    descent, of course.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿！我们已经到了微积分学习的最后一个检查点。还缺什么？当然是梯度下降。
- en: In the previous eight chapters, we lined up all of our ducks in a row, and now
    it’s time to take that shot. First, we’ll put multivariable functions to code.
    Previously, we built a convenient interface in the form of our Function class
    to represent differentiable functions. After the lengthy setup in the previous
    chapter, we can easily extend it, and with the power of vectorization, we don’t
    even have to change that much. Let’s go!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的八章中，我们将所有内容都整理好了，现在是时候下手了。首先，我们将多变量函数代码化。之前，我们构建了一个方便的接口形式——Function 类，用来表示可微函数。在上一章进行了较长的设置后，我们现在可以轻松扩展它，并且利用向量化的强大功能，我们甚至不需要做太多修改。开始吧！
- en: 17.1 Multivariable functions in code
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.1 多变量函数的代码实现
- en: It’s been a long time since we put theory into code. So, let’s take a look at
    multivariable functions!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我们将理论转化为代码以来已经有一段时间了。那么，让我们来看看多变量函数吧！
- en: 'Last time, we built a Function base class with two main methods: one for computing
    the derivative (Function.prime) and one for getting the dictionary of parameters
    (Function.parameters).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 上一次，我们构建了一个函数基类，包含两个主要方法：一个用于计算导数（Function.prime），另一个用于获取参数字典（Function.parameters）。
- en: 'This won’t be much of a surprise: the multivariate function base class is not
    much different. For clarity, we’ll appropriately rename the prime method to grad.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不会让人感到惊讶：多变量函数基类与此没有太大区别。为了清晰起见，我们将适当重命名主方法为 grad。
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s see a few examples right away. The simplest one is the squared Euclidean
    norm f(x) = ∥x∥², a close relative to the mean squared error function. Its gradient
    is given by
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们立即来看几个例子。最简单的一个是平方欧几里得范数 f(x) = ∥x∥²，它与均方误差函数非常相似。它的梯度由下式给出：
- en: '![∇f (x) = 2x, ](img/file1575.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![∇f (x) = 2x, ](img/file1575.png)'
- en: thus everything is ready to implement it. As we’ve used NumPy arrays to represent
    vectors, we’ll use them as the input as well.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一切准备就绪，可以进行实现。由于我们使用 NumPy 数组来表示向量，我们也将它们作为输入。
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that SquaredNorm is different from f(x) = ∥x∥² in a mathematical sense,
    as it accepts any NumPy array, not just an n-dimensional vector. This is not a
    problem now, but will be later, so keep that in mind.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，SquaredNorm 与 f(x) = ∥x∥² 在数学意义上是不同的，因为它接受任何 NumPy 数组，而不仅仅是一个 n 维向量。现在这不是问题，但稍后会有问题，所以请记住这一点。
- en: Another example can be given with the parametric linear function
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子可以通过参数线性函数给出
- en: '![g(x,y) = ax + by, ](img/file1578.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![g(x, y) = ax + by, ](img/file1578.png)'
- en: where a,b ∈ℝ are arbitrary parameters. Let’s see how g(x,y) is implemented!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 a, b ∈ ℝ 是任意参数。让我们看看如何实现 g(x, y)！
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To check if our implementation works correctly, we can quickly test it out on
    a simple example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的实现是否正确，我们可以在一个简单的例子中快速测试一下。
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Perhaps we might have overlooked this question until now, but trust me, specifying
    the input and output shapes is of crucial importance. When doing mathematics,
    we can be flexible in our notation and treat any vector x ∈ℝ^n as a column or
    row vector, but painfully, this is not the case in practice.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 或许我们直到现在才开始关注这个问题，但相信我，指定输入和输出的形状至关重要。在做数学时，我们可以灵活地使用符号，将任何向量 x ∈ ℝ^n 视为列向量或行向量，但令人痛苦的是，实践中情况并非如此。
- en: Correctly keeping track of array shapes is of utmost importance and can save
    you hundreds of hours. No joke.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正确地跟踪数组形状至关重要，它可以为你节省数百小时的时间。绝不是开玩笑。
- en: 17.2 Minima and maxima, revisited
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.2 最小值和最大值，再次探讨
- en: In a single variable, we have successfully used the derivatives to find the
    local optima of differentiable functions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在单变量的情况下，我们成功地使用导数找到了可微函数的局部最优解。
- en: 'Recall that if f : ℝ →ℝ is differentiable everywhere, then Theorem [87](ch021.xhtml#x1-214004r87)
    gives that'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '回顾一下，如果 f : ℝ → ℝ 在处处可微，那么定理 [87](ch021.xhtml#x1-214004r87) 给出的结论是'
- en: (a) f^′(a) = 0 and f^(′′)(a)/span>0 implies a local minimum. (b) f^′(a) = 0
    and f^(′′)(a)/span>0 implies a local maximum.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (a) f^′(a) = 0 且 f^(′′)(a) > 0 表示局部最小值。(b) f^′(a) = 0 且 f^(′′)(a) < 0 表示局部最大值。
- en: (A simple f^′(a) = 0 is not enough, as the example f(x) = x³ shows at 0.)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (简单的 f^′(a) = 0 不足以确定最值，正如 f(x) = x³ 在 0 处的例子所示。)
- en: Can we do something similar in multiple variables?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能在多变量的情况下做类似的事情吗？
- en: 'Right from the start, there seems to be an issue: the derivative is not a scalar
    (thus, we can’t equate it to 0).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从一开始似乎就有一个问题：导数不是标量（因此，我们不能将其等于 0）。
- en: 'This is easy to solve: the analogue of the condition f^′(a) = 0 is ∇f(a) =
    (0,0,…,0). For simplicity, the zero vector (0,0,…,0) will also be denoted by 0\.
    Don’t worry, this won’t be confusing; it’s all clear from the context. Introducing
    a new notation for the zero vector would just add more complexity.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易解决：条件 f^′(a) = 0 的类似条件是 ∇f(a) = (0,0,…,0)。为了简便，零向量 (0,0,…,0) 也可以表示为 0\。别担心，这不会让人困惑；从上下文中可以清楚地理解。引入零向量的新符号只是增加了复杂性。
- en: 'We can visualize what happens with the tangent plane at critical points. In
    a single variable, we have already seen this: as Figure [17.1](#) illustrates,
    f^′(a) = 0 implies that the tangent line is horizontal.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过切平面来直观地理解临界点的情况。在单变量情况下，我们已经看到过这个：如图 [17.1](#) 所示，f^′(a) = 0 表明切线是水平的。
- en: '![PIC](img/file1579.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1579.png)'
- en: 'Figure 17.1: Local extrema in a single variable'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.1：单变量的局部极值
- en: 'In multiple variables, the situation is similar: ∇f(a) = 0 implies that the
    best local linear approximation ([16.3](ch026.xhtml#x1-257003r68)) is constant;
    that is, the tangent plane is horizontal. (As visualized by Figure [17.2](#).)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在多变量情况下，情况类似：∇f(a) = 0 表示最佳的局部线性逼近（[16.3](ch026.xhtml#x1-257003r68)）是常数；即切平面是水平的。（如图
    [17.2](#) 所示。）
- en: '![PIC](img/file1580.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1580.png)'
- en: 'Figure 17.2: Local extrema in multiple variables'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.2：多变量的局部极值
- en: 'So, what does ∇f(a) = 0 imply? Similarly to the single-variable case, an a
    ∈ℝ^n is called a critical point of f if ∇f(a) = 0 holds. The similarity doesn’t
    stop at the level of terminologies. We also have three options in multiple variables
    as well: a is'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，∇f(a) = 0 表示什么呢？类似于单变量的情况，a ∈ℝ^n 如果满足 ∇f(a) = 0，则称为 f 的临界点。这种相似性不仅体现在术语上，我们在多变量情况下也有三种选择：a
    是
- en: a local minimum,
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 局部最小值，
- en: a local maximum,
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 局部最大值，
- en: or neither.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者都不是。
- en: In multiple variables, a non-extremal critical point is called a saddle point,
    because the two-dimensional case bears a striking resemblance to an actual horse
    saddle, as you are about to see. Saddle points are the high-dimensional analogues
    of the one-dimensional inflection points. The functions
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在多变量情况下，非极值的临界点称为鞍点，因为二维情况下的形状与实际的马鞍非常相似，正如你将要看到的那样。鞍点是高维空间中单变量拐点的类比。函数
- en: '![ 2 2 f (x,y) = x + y , 2 2 g(x,y) = − (x + y ), h (x,y) = x2 − y2 ](img/file1581.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 2 f (x,y) = x + y , 2 2 g(x,y) = − (x + y ), h (x,y) = x2 − y2 ](img/file1581.png)'
- en: at (0,0) provide an example for all three, as Figure [17.3](#), Figure [17.4](#),
    and Figure [17.5](#) show. (Keep in mind that a local extremum might be global.)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 (0,0) 处提供了所有三种情况的示例，如图 [17.3](#)、图 [17.4](#) 和图 [17.5](#) 所示。（记住，局部极值可能是全局极值。）
- en: '![PIC](img/file1582.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1582.png)'
- en: 'Figure 17.3: A (local) maximum'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.3：一个（局部）最大值
- en: '![PIC](img/file1583.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1583.png)'
- en: 'Figure 17.4: A (local) minima'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.4：一个（局部）最小值
- en: '![PIC](img/file1584.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1584.png)'
- en: 'Figure 17.5: A saddle point'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.5：一个鞍点
- en: To put things into order, let’s start formulating definitions and theorems.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理清思路，我们从定义和定理开始制定。
- en: Definition 73\. (Critical points)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 73.（临界点）
- en: 'Let f : ℝ^n →ℝ be an arbitrary vector-scalar function. We say that a ∈ℝ^n is
    a critical point of f if either'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ 为任意向量-标量函数。我们说 a ∈ℝ^n 是 f 的临界点，如果满足以下条件之一：'
- en: '![∇f (a) = 0 ](img/file1585.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![∇f (a) = 0 ](img/file1585.png)'
- en: holds, or f is not partially differentiable at a in at least one variable.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 保持成立，或者 f 在至少一个变量上不可偏微分。
- en: The second case (where f is not differentiable at a) is there to handle situations
    like f(x,y) = jxj + jyj.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种情况（f 在 a 处不可微）是为了处理像 f(x,y) = jxj + jyj 这样的情况。
- en: For the sake of precision, let’s define local extrema in multiple dimensions
    as well.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了精确起见，我们也来定义多维的局部极值。
- en: Definition 74\. (Local minima and maxima)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 74.（局部最小值和最大值）
- en: 'Let f : ℝ^n →ℝ be an arbitrary vector-scalar function and let a ∈ℝ^n be an
    arbitrary point.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ 为任意向量-标量函数，a ∈ℝ^n 为任意点。'
- en: (a) a is a local minimum if there exists an 𝜀/span>0 such that
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 如果存在 𝜀/span>0，使得
- en: '![f(a) ≤ f(x), x ∈ B(𝜀,a). ](img/file1586.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![f(a) ≤ f(x), x ∈ B(𝜀,a). ](img/file1586.png)'
- en: (b) a is a strict local minimum if there exists an 𝜀/span>0 such that
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (b) a 是严格局部最小值，如果存在 𝜀/span>0，使得
- en: '![f(a) <f(x), x ∈ B(𝜀,a). ](img/file1587.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![f(a) <f(x), x ∈ B(𝜀,a). ](img/file1587.png)'
- en: (c) a is a local maximum if there exists an 𝜀/span>0 such that
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (c) a 是局部最大值，如果存在 𝜀/span>0，使得
- en: '![f(a) ≥ f(x), x ∈ B (𝜀,a) ∖{a}. ](img/file1588.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![f(a) ≥ f(x), x ∈ B (𝜀,a) ∖{a}. ](img/file1588.png)'
- en: (d) a is a strict local maximum if there exists an 𝜀/span>0 such that
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (d) a 是严格局部最大值，如果存在 𝜀/span>0，使得
- en: '![f(a) >f(x), x ∈ B (𝜀,a) ∖{a}. ](img/file1589.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![f(a) >f(x), x ∈ B (𝜖,a) ∖{a}. ](img/file1589.png)'
- en: As the example of x² −y² shows, a critical point is not necessarily a local
    extremum, but a local extremum is always a critical point. The next result, which
    is the analogue of Definition [73](ch027.xhtml#x1-270018r73), makes this mathematically
    precise.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 x² −y² 的例子所示，临界点不一定是局部极值，但局部极值总是临界点。接下来的结果，类似于定义 [73](ch027.xhtml#x1-270018r73)，对此进行了数学上的精确描述。
- en: Theorem 106\.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 106\。
- en: 'Let f : ℝ^n →ℝ be an arbitrary vector-scalar function, and suppose that f is
    partially differentiable with respect to all variables at some a ∈ℝ^n.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '令 f : ℝ^n →ℝ 为一个任意的向量-标量函数，假设 f 在某个 a ∈ℝ^n 处对所有变量部分可微。'
- en: If f has a local extremum at a, then ∇f(a) = 0.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 f 在 a 处有局部极值，则 ∇f(a) = 0。
- en: Proof. This is a direct consequence of Theorem [86](ch021.xhtml#x1-213005r86),
    as if a = (a[1],…,a[n]) is a local extremum of the vector-scalar function f, then
    it is a local extremum of the single-variable functions h→f(a + he[i]), where
    e[i] is the vector whose i-th component is 1, while the others are 0.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：这是定理 [86](ch021.xhtml#x1-213005r86) 的直接结果，因为如果 a = (a[1],…,a[n]) 是向量-标量函数
    f 的局部极值，那么它也是单变量函数 h→f(a + he[i]) 的局部极值，其中 e[i] 是一个向量，其第 i 个分量为 1，其余分量为 0。
- en: According to the very definition of the partial derivative given by Definition [66](ch026.xhtml#x1-254009r66),
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义 [66](ch026.xhtml#x1-254009r66) 给出的偏导数定义，
- en: '![-d-f(a + hei) =-∂f-(a). dh ∂xi ](img/file1591.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![-d-f(a + hei) =-∂f-(a). dh ∂xi ](img/file1591.png)'
- en: Thus, Theorem [86](ch021.xhtml#x1-213005r86) gives that
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，定理 [86](ch021.xhtml#x1-213005r86) 给出了
- en: '![ ∂f ∂x--(a ) = 0 i ](img/file1592.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![ ∂f ∂x--(a ) = 0 i ](img/file1592.png)'
- en: for all i = 1,…,n, giving that ∇f(a) = 0.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 i = 1,…,n，给定 ∇f(a) = 0。
- en: So, how can we find the local extrema with the derivative? As we have already
    suggested, studying the second derivative will help us pinpoint the extrema among
    critical points. Unfortunately, things are much more complicated in n variables,
    so let’s focus on the two-variable case first.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何通过导数来找到局部极值呢？正如我们之前所建议的，研究二阶导数将帮助我们在临界点中定位极值。不幸的是，n 变量的情况要复杂得多，所以我们首先集中讨论二元情况。
- en: Theorem 107\. (The two-variable second derivative test)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 107\。（二元函数的二阶导数检验）
- en: 'Let f : ℝ² →ℝ be an arbitrary vector-scalar function, and suppose that f is
    partially differentiable at some a ∈ℝ². Also suppose that a is a critical point,
    that is, ∇f(a) = 0.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '令 f : ℝ² →ℝ 为一个任意的向量-标量函数，假设 f 在某个 a ∈ℝ² 处部分可微。并且假设 a 是一个临界点，即 ∇f(a) = 0。'
- en: (a) If detH[f](a)/span>0 and ![∂2f- ∂x22](img/file1593.png)/span>0, then a is
    a local minimum.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 如果 detH[f](a)/span>0 且 ![∂2f- ∂x22](img/file1593.png)/span>0，则 a 是局部最小值。
- en: (b) If detH[f](a)/span>0 and ![∂2f- ∂x22](img/file1594.png)/span>0, then a is
    a local maximum.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 如果 detH[f](a)/span>0 且 ![∂2f- ∂x22](img/file1594.png)/span>0，则 a 是局部最大值。
- en: (c) If detH[f](a)/span>0, then a is a saddle point.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 如果 detH[f](a)/span>0，则 a 是一个鞍点。
- en: We will not prove this, but some remarks are in order. First, as the determinant
    of the Hessian can be 0, Theorem [107](ch027.xhtml#x1-270025r107) does not cover
    all possible cases.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会证明这一点，但有一些说明是必要的。首先，由于 Hessian 行列式可能为 0，定理 [107](ch027.xhtml#x1-270025r107)
    并未涵盖所有可能的情况。
- en: It’s probably best to see a few examples, so let’s revisit the previously seen
    functions
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最好看几个例子，因此让我们回顾一下之前看到的函数
- en: '![f (x,y) = x2 + y2, g(x,y) = − (x2 + y2), 2 2 h (x,y) = x − y . ](img/file1595.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![f (x,y) = x2 + y2, g(x,y) = − (x2 + y2), 2 2 h (x,y) = x − y . ](img/file1595.png)'
- en: All three have a critical point at 0, so the Hessians can provide a clearer
    picture. The Hessians are given by the matrices
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个函数在 0 处都有一个临界点，因此 Hessian 可以提供更清晰的图像。Hessian 的矩阵为
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ ⌈2 0⌉ ⌈− 2 0 ⌉ ⌈2 0 ⌉ Hf (x,y) = 0 2 , Hg(x,y) = 0 − 2 , Hh
    (x,y) = 0 − 2 . ](img/file1596.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ ⌈2 0⌉ ⌈− 2 0 ⌉ ⌈2 0 ⌉ Hf (x,y) = 0 2 , Hg(x,y) = 0 − 2 , Hh
    (x,y) = 0 − 2 . ](img/file1596.png)'
- en: For functions of two variables, Theorem [107](ch027.xhtml#x1-270025r107) says
    that it is enough to study detH[f](a) and ![∂2f ∂y2](img/file1597.png)(a).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元函数，定理 [107](ch027.xhtml#x1-270025r107) 说明，研究 detH[f](a) 和 ![∂2f ∂y2](img/file1597.png)(a)
    就足够了。
- en: In the case of f(x,y) = x² + y², we have H[f](0,0) = 4 and ![∂2f ∂y2](img/file1598.png)(0,0)
    = 2, giving that 0 is a local minimum of f(x,y) = x² + y².
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 f(x,y) = x² + y²，我们有 H[f](0,0) = 4 且 ![∂2f ∂y2](img/file1598.png)(0,0) =
    2，说明 0 是 f(x,y) = x² + y² 的局部最小值。
- en: Similarly, we can conclude that 0 is a local maximum of g(x,y) = −(x² + y²)
    (which shouldn’t surprise you, as g = −f).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以得出结论，0 是 g(x,y) = −(x² + y²) 的局部最大值（这并不令人惊讶，因为 g = −f）。
- en: Finally, for h(x,y) = x² −y², the second derivative test confirms that 0 is
    indeed a saddle point.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于 h(x,y) = x² − y²，二阶导数检验确认 0 确实是一个鞍点。
- en: 'So, what’s up with the general case? Unfortunately, just studying the determinant
    of the Hessian matrix is not enough. We need to bring in the heavy-hitters: eigenvalues.
    (See Definition [23](ch012.xhtml#x1-105003r23).) Here is the second derivative
    test in its full glory.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，通用情况如何呢？不幸的是，仅仅研究Hessian矩阵的行列式是不够的。我们需要引入重磅人物：特征值。（见定义[23](ch012.xhtml#x1-105003r23)）。这就是二阶导数检验的完整形式。
- en: Theorem 108\. (The multivariable second derivative test)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 108. （多变量二阶导数检验）
- en: 'Let f : ℝ^n →ℝ be an arbitrary vector-scalar function, and suppose that f is
    partially differentiable with respect to all variables at some a ∈ℝ^n. Also suppose
    that a is a critical point, that is, ∇f(a) = 0.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ 是一个任意的向量-标量函数，假设 f 对所有变量在某个 a ∈ℝ^n 处部分可微。还假设 a 是一个临界点，即 ∇f(a)
    = 0。'
- en: (a) If all the eigenvalues of H[f](a) are positive, then a is a local minimum.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 如果 H[f](a) 的所有特征值都是正数，则 a 是局部最小值。
- en: (b) If all the eigenvalues of H[f](a) are negative, then a is a local maximum.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 如果 H[f](a) 的所有特征值都是负数，则 a 是局部最大值。
- en: (c) If all the eigenvalues of H[f](a) are either positive or negative, then
    a is a saddle point.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 如果 H[f](a) 的所有特征值都是正数或负数，则 a 是鞍点。
- en: 'That’s right: if any of the eigenvalues are 0, then the test is inconclusive.
    You might recall from linear algebra that in practice, computing the eigenvalues
    is not as fast as computing the second-order derivatives, but there are plenty
    of numerical methods (like the QR-algorithm, as we saw in Section [7.5](ch013.xhtml#computing-eigenvalues)).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 没错：如果任何特征值为0，则该检验无法得出结论。你可能还记得在线性代数中，实际上计算特征值的速度不如计算二阶导数快，但有很多数值方法（比如我们在第[7.5](ch013.xhtml#computing-eigenvalues)节中看到的QR算法）。
- en: 'To sum it up, the method of optimizing (differentiable) multivariable functions
    is a simple two-step process:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，优化（可微）多变量函数的方法是一个简单的两步过程：
- en: find the critical points by solving the equation ∇f(x) = 0,
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过解方程 ∇f(x) = 0 来找到临界点，
- en: then use the second derivative test to determine which critical points are extrema.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用二阶导数检验来确定哪些临界点是极值点。
- en: Do we use this method in practice to optimize functions? No. Why? Most importantly,
    because computing the eigenvalues of the Hessian for a vector-scalar function
    with millions of variables is extremely hard. Why is the second derivative test
    so important? Because understanding the behavior of functions around their extremal
    points is essential to truly understand gradient descent. Believe it or not, this
    is the key behind the theoretical guarantees for convergence.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实际中使用这种方法来优化函数吗？不使用。为什么？最重要的原因是，对于具有数百万变量的向量-标量函数，计算Hessian矩阵的特征值是非常困难的。为什么二阶导数检验如此重要？因为理解函数在极值点附近的行为是深入理解梯度下降的关键。信不信由你，这就是理论上保证收敛性的关键。
- en: Speaking of gradient descent, now is the time to dig deep into the algorithm
    that powers neural networks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 说到梯度下降，现在是时候深入探讨驱动神经网络的算法了。
- en: 17.3 Gradient descent in its full form
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3 梯度下降的完整形式
- en: Gradient descent is one of the most important algorithms in machine learning.
    We have talked about this a lot, although up until this point, we have only seen
    it for single-variable functions (which is, I admit, not the most practical use
    case).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是机器学习中最重要的算法之一。我们已经讨论过很多次，尽管到目前为止，我们仅仅看到它用于单变量函数（我承认，这并不是最实际的应用案例）。
- en: However, now we have all the tools we need to talk about gradient descent in
    its general form. Let’s get to it!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在我们已经掌握了所有讨论梯度下降一般形式所需的工具。让我们开始吧！
- en: 'Suppose that we have a differentiable vector-scalar function f : ℝ^n →ℝ that
    we want to maximize. This can describe the return on investment of an investing
    strategy, or any other quantity. Calculating the gradient and finding the critical
    points is often not an option, as solving the equation ∇f(x) = 0 can be computationally
    unfeasible. Thus, we resort to an iterative solution.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '假设我们有一个可微的向量-标量函数 f : ℝ^n →ℝ ，我们想要最大化它。这可以描述投资策略的回报，或任何其他量。计算梯度并找到临界点通常不可行，因为解方程
    ∇f(x) = 0 在计算上可能不可行。因此，我们采用迭代解法。'
- en: 'The algorithm is the same as for single-variable functions (as seen in Section [13.2](ch021.xhtml#the-basics-of-gradient-descent)):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法与单变量函数的情况相同（如第[13.2](ch021.xhtml#the-basics-of-gradient-descent)节所见）：
- en: Start from a random point.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个随机点开始。
- en: Calculate its gradient.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算它的梯度。
- en: Take a step towards its direction.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向它的方向迈出一步。
- en: Repeat until convergence.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到收敛。
- en: This is called gradient ascent. We can formalize it in the following way.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为梯度上升。我们可以按照以下方式进行形式化。
- en: (The gradient ascent algorithm) Step 1\. Initialize the starting point x[0]
    ∈ℝ^n and select a learning rate h ∈ (0,∞). Step 2\. Let![xn+1 := xn + h∇f (xn
    ). ](img/file1599.png)Step 3\. Repeat Step 2\. until convergence.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: （梯度上升算法）步骤1：初始化起始点x[0] ∈ℝ^n并选择学习率h ∈ (0,∞)。步骤2：令![xn+1 := xn + h∇f (xn )](img/file1599.png)。步骤3：重复步骤2，直到收敛。
- en: If we want to minimize f, we might as well maximize −f. The only effect of this
    is a sign change for the gradient. In this form, the algorithm is called gradient
    descent, and this is the version that’s widely used to train neural networks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想最小化f，那么不妨最大化−f。这样做的唯一影响是梯度的符号变化。以这种形式，算法被称为梯度下降法，这也是广泛用于训练神经网络的版本。
- en: (The gradient descent algorithm) Step 1\. Initialize the starting point x[0]
    ∈ℝ^n and select a learning rate h ∈ (0,∞). Step 2\. Let![xn+1 := xn − h∇f (xn
    ). ](img/file1600.png)Step 3\. Repeat Step 2\. until convergence.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: （梯度下降算法）步骤1：初始化起始点x[0] ∈ℝ^n并选择学习率h ∈ (0,∞)。步骤2：令![xn+1 := xn − h∇f (xn )](img/file1600.png)。步骤3：重复步骤2，直到收敛。
- en: After all of this setup, implementing gradient descent is straightforward.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有这些设置后，实现梯度下降法非常直接。
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice that it is almost identical to the single-variable version in Section [13.2](ch021.xhtml#the-basics-of-gradient-descent).
    To see if it works correctly, let’s test it out on the squared Euclidean norm
    function, implemented by SquaredNorm earlier!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这与第[13.2](ch021.xhtml#the-basics-of-gradient-descent)节中的单变量版本几乎完全相同。为了验证它是否有效，让我们在之前由SquaredNorm实现的平方欧几里得范数函数上进行测试！
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is nothing special to it, really. The issues with multivariable gradient
    descent are the same as what we discussed with the single-variable version: it
    can get stuck in local minima, it is sensitive to our choice of learning rate,
    and the gradient can be computationally hard to calculate in high dimensions.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上并没有什么特别之处。多变量梯度下降法的问题与我们在单变量版本中讨论的相同：它可能会陷入局部最小值，对学习率的选择非常敏感，而且在高维度下，梯度的计算可能非常困难。
- en: 17.4 Summary
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.4 小结
- en: 'Although this chapter was short and sweet, we took quite a big step by dissecting
    the fine details of gradient descent in high dimensions. The chapter’s brevity
    is a testament to the power of vectorization: same formulas, code, and supercharged
    functionality. It’s quite unbelievable, but the simple algorithm'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章简短而精炼，但通过详细剖析高维度梯度下降法的细节，我们迈出了一个很大的步伐。本章的简洁是向量化强大功能的见证：相同的公式、代码和超级充能的功能。这几乎难以置信，但这个简单的算法
- en: '![xn+1 = xn − h∇f (xn) ](img/file1601.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![xn+1 = xn − h∇f (xn)](img/file1601.png)'
- en: is behind most of the neural network models. Yes, even state-of-the-art ones.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是大多数神经网络模型背后的原理。是的，甚至是最先进的模型。
- en: This lies on the same theoretical foundations as the univariate case, but instead
    of checking the positivity of the second derivatives, we have to study the full
    Hessian matrix H[f]. To be more precise, we have learned that a critical point
    ∇f(a) = 0 is
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这与单变量情况有相同的理论基础，但我们不再检查二阶导数的正性，而是需要研究完整的Hessian矩阵H[f]。更准确地说，我们已经了解到，临界点∇f(a)
    = 0是
- en: a local minimum if all the eigenvalues of H[f](a) are positive,
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果H[f](a)的所有特征值都是正数，则为局部最小值，
- en: and a local maximum if all the eigenvalues of H[f](a) are negative.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果H[f](a)的所有特征值都是负数，则为局部最大值。
- en: Deep down, this is the reason why gradient descent works. And with this, we
    have finished our study of calculus, both in single and multiple variables.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，这就是梯度下降法有效的原因。有了这个，我们已经完成了对微积分的学习，包括单变量和多变量情况。
- en: 'Take a deep breath and relax a bit. We are approaching the final stretch of
    our adventure: our last stop is the theory of probability, the thinking paradigm
    that is behind predictive modeling. For instance, the most famous loss functions,
    like the mean-squared error or the cross-entropy, are founded upon probabilistic
    concepts. Understanding and taming uncertainty is one of the biggest intellectual
    feats of science, and we are about to undertake this journey ourselves.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 深呼吸，放松一下。我们即将进入冒险的最后阶段：我们最后的目的地是概率论，这是预测建模背后的思维范式。例如，最著名的损失函数，如均方误差或交叉熵，都是基于概率概念的。理解和驾驭不确定性是科学领域中最大的智力成就之一，我们即将开始这段旅程。
- en: See you in the next chapter!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 下章见！
- en: 17.5 Problems
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.5 问题
- en: Problem 1\. Let y ∈ℝ^n be an arbitrary vector. The general version of the famous
    mean-squared error is defined by
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 1. 设 y ∈ ℝ^n 是一个任意向量。著名的均方误差的一般版本定义为
- en: '![ n MSE (x) = 1-∑ (x − y )2\. n i i i=1 ](img/file1602.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![ n MSE (x) = 1-∑ (x − y )². n i i i=1 ](img/file1602.png)'
- en: Compute its gradient and implement it using the MultivariateFunction base class!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 计算其梯度，并使用 MultivariateFunction 基类实现它！
- en: 'Problem 2\. Let f : ℝ² →ℝ be the function defined by'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '问题 2. 设 f : ℝ² → ℝ 是由以下方式定义的函数'
- en: '![ 2 2 f(x,y) = (2x − y)(y − x ). ](img/file1603.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 2 f(x,y) = (2x − y)(y − x )². ](img/file1603.png)'
- en: Does f have a local extremum in x = (0,0)?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: f 在 x = (0,0) 处有局部极值吗？
- en: Problem 3\. Use the previously implemented gradient_descent function to find
    the minimum of
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 3. 使用之前实现的 gradient_descent 函数来找到以下函数的最小值
- en: '![ 2 2 f(x,y) = sin(x+ y)+ x y . ](img/file1604.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 2 f(x,y) = sin(x+ y)+ x y . ](img/file1604.png)'
- en: Experiment with various learning rates and initial values!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的学习率和初始值！
- en: 'Problem 4\. In the problem section of Chapter 13, we saw the improved version
    of gradient descent, called gradient descent with momentum. We can do the same
    in multiple variables: define'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 4. 在第 13 章的问题部分，我们看到了梯度下降的改进版本，称为带动量的梯度下降。我们可以在多变量的情况下做相同的事情：定义
- en: '![dn+1 = αdn − hf ′(xn), xn+1 = xn + dn, ](img/file1605.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![dn+1 = αdn − hf ′(xn), xn+1 = xn + dn, ](img/file1605.png)'
- en: where d[0] = 0 and x[0] is arbitrary. Implement it!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 d[0] = 0 且 x[0] 是任意的。实现它！
- en: Join our community on Discord
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他用户、机器学习专家以及作者本人一起阅读这本书。提问、为其他读者提供解决方案、通过“问我任何问题”环节与作者互动，等等。扫描二维码或访问链接加入社区。
    [https://packt.link/math](https://packt.link/math)
- en: '![PIC](img/file1.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
