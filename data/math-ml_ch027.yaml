- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Optimization in Multiple Variables
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå˜é‡ä¼˜åŒ–
- en: Hey! We are at the last checkpoint of our calculus study. Whatâ€™s missing? Gradient
    descent, of course.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å˜¿ï¼æˆ‘ä»¬å·²ç»åˆ°äº†å¾®ç§¯åˆ†å­¦ä¹ çš„æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹ã€‚è¿˜ç¼ºä»€ä¹ˆï¼Ÿå½“ç„¶æ˜¯æ¢¯åº¦ä¸‹é™ã€‚
- en: In the previous eight chapters, we lined up all of our ducks in a row, and now
    itâ€™s time to take that shot. First, weâ€™ll put multivariable functions to code.
    Previously, we built a convenient interface in the form of our Function class
    to represent differentiable functions. After the lengthy setup in the previous
    chapter, we can easily extend it, and with the power of vectorization, we donâ€™t
    even have to change that much. Letâ€™s go!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰çš„å…«ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹éƒ½æ•´ç†å¥½äº†ï¼Œç°åœ¨æ˜¯æ—¶å€™ä¸‹æ‰‹äº†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†å¤šå˜é‡å‡½æ•°ä»£ç åŒ–ã€‚ä¹‹å‰ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–¹ä¾¿çš„æ¥å£å½¢å¼â€”â€”Function ç±»ï¼Œç”¨æ¥è¡¨ç¤ºå¯å¾®å‡½æ•°ã€‚åœ¨ä¸Šä¸€ç« è¿›è¡Œäº†è¾ƒé•¿çš„è®¾ç½®åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥è½»æ¾æ‰©å±•å®ƒï¼Œå¹¶ä¸”åˆ©ç”¨å‘é‡åŒ–çš„å¼ºå¤§åŠŸèƒ½ï¼Œæˆ‘ä»¬ç”šè‡³ä¸éœ€è¦åšå¤ªå¤šä¿®æ”¹ã€‚å¼€å§‹å§ï¼
- en: 17.1 Multivariable functions in code
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.1 å¤šå˜é‡å‡½æ•°çš„ä»£ç å®ç°
- en: Itâ€™s been a long time since we put theory into code. So, letâ€™s take a look at
    multivariable functions!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä»æˆ‘ä»¬å°†ç†è®ºè½¬åŒ–ä¸ºä»£ç ä»¥æ¥å·²ç»æœ‰ä¸€æ®µæ—¶é—´äº†ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹å¤šå˜é‡å‡½æ•°å§ï¼
- en: 'Last time, we built a Function base class with two main methods: one for computing
    the derivative (Function.prime) and one for getting the dictionary of parameters
    (Function.parameters).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šä¸€æ¬¡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå‡½æ•°åŸºç±»ï¼ŒåŒ…å«ä¸¤ä¸ªä¸»è¦æ–¹æ³•ï¼šä¸€ä¸ªç”¨äºè®¡ç®—å¯¼æ•°ï¼ˆFunction.primeï¼‰ï¼Œå¦ä¸€ä¸ªç”¨äºè·å–å‚æ•°å­—å…¸ï¼ˆFunction.parametersï¼‰ã€‚
- en: 'This wonâ€™t be much of a surprise: the multivariate function base class is not
    much different. For clarity, weâ€™ll appropriately rename the prime method to grad.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¹¶ä¸ä¼šè®©äººæ„Ÿåˆ°æƒŠè®¶ï¼šå¤šå˜é‡å‡½æ•°åŸºç±»ä¸æ­¤æ²¡æœ‰å¤ªå¤§åŒºåˆ«ã€‚ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œæˆ‘ä»¬å°†é€‚å½“é‡å‘½åä¸»æ–¹æ³•ä¸º gradã€‚
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Letâ€™s see a few examples right away. The simplest one is the squared Euclidean
    norm f(x) = âˆ¥xâˆ¥Â², a close relative to the mean squared error function. Its gradient
    is given by
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç«‹å³æ¥çœ‹å‡ ä¸ªä¾‹å­ã€‚æœ€ç®€å•çš„ä¸€ä¸ªæ˜¯å¹³æ–¹æ¬§å‡ é‡Œå¾—èŒƒæ•° f(x) = âˆ¥xâˆ¥Â²ï¼Œå®ƒä¸å‡æ–¹è¯¯å·®å‡½æ•°éå¸¸ç›¸ä¼¼ã€‚å®ƒçš„æ¢¯åº¦ç”±ä¸‹å¼ç»™å‡ºï¼š
- en: '![âˆ‡f (x) = 2x, ](img/file1575.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![âˆ‡f (x) = 2x, ](img/file1575.png)'
- en: thus everything is ready to implement it. As weâ€™ve used NumPy arrays to represent
    vectors, weâ€™ll use them as the input as well.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥è¿›è¡Œå®ç°ã€‚ç”±äºæˆ‘ä»¬ä½¿ç”¨ NumPy æ•°ç»„æ¥è¡¨ç¤ºå‘é‡ï¼Œæˆ‘ä»¬ä¹Ÿå°†å®ƒä»¬ä½œä¸ºè¾“å…¥ã€‚
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that SquaredNorm is different from f(x) = âˆ¥xâˆ¥Â² in a mathematical sense,
    as it accepts any NumPy array, not just an n-dimensional vector. This is not a
    problem now, but will be later, so keep that in mind.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼ŒSquaredNorm ä¸ f(x) = âˆ¥xâˆ¥Â² åœ¨æ•°å­¦æ„ä¹‰ä¸Šæ˜¯ä¸åŒçš„ï¼Œå› ä¸ºå®ƒæ¥å—ä»»ä½• NumPy æ•°ç»„ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ª n ç»´å‘é‡ã€‚ç°åœ¨è¿™ä¸æ˜¯é—®é¢˜ï¼Œä½†ç¨åä¼šæœ‰é—®é¢˜ï¼Œæ‰€ä»¥è¯·è®°ä½è¿™ä¸€ç‚¹ã€‚
- en: Another example can be given with the parametric linear function
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªä¾‹å­å¯ä»¥é€šè¿‡å‚æ•°çº¿æ€§å‡½æ•°ç»™å‡º
- en: '![g(x,y) = ax + by, ](img/file1578.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![g(x, y) = ax + by, ](img/file1578.png)'
- en: where a,b âˆˆâ„ are arbitrary parameters. Letâ€™s see how g(x,y) is implemented!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ a, b âˆˆ â„ æ˜¯ä»»æ„å‚æ•°ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å®ç° g(x, y)ï¼
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To check if our implementation works correctly, we can quickly test it out on
    a simple example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ£€æŸ¥æˆ‘ä»¬çš„å®ç°æ˜¯å¦æ­£ç¡®ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸€ä¸ªç®€å•çš„ä¾‹å­ä¸­å¿«é€Ÿæµ‹è¯•ä¸€ä¸‹ã€‚
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Perhaps we might have overlooked this question until now, but trust me, specifying
    the input and output shapes is of crucial importance. When doing mathematics,
    we can be flexible in our notation and treat any vector x âˆˆâ„^n as a column or
    row vector, but painfully, this is not the case in practice.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è®¸æˆ‘ä»¬ç›´åˆ°ç°åœ¨æ‰å¼€å§‹å…³æ³¨è¿™ä¸ªé—®é¢˜ï¼Œä½†ç›¸ä¿¡æˆ‘ï¼ŒæŒ‡å®šè¾“å…¥å’Œè¾“å‡ºçš„å½¢çŠ¶è‡³å…³é‡è¦ã€‚åœ¨åšæ•°å­¦æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çµæ´»åœ°ä½¿ç”¨ç¬¦å·ï¼Œå°†ä»»ä½•å‘é‡ x âˆˆ â„^n è§†ä¸ºåˆ—å‘é‡æˆ–è¡Œå‘é‡ï¼Œä½†ä»¤äººç—›è‹¦çš„æ˜¯ï¼Œå®è·µä¸­æƒ…å†µå¹¶éå¦‚æ­¤ã€‚
- en: Correctly keeping track of array shapes is of utmost importance and can save
    you hundreds of hours. No joke.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£ç¡®åœ°è·Ÿè¸ªæ•°ç»„å½¢çŠ¶è‡³å…³é‡è¦ï¼Œå®ƒå¯ä»¥ä¸ºä½ èŠ‚çœæ•°ç™¾å°æ—¶çš„æ—¶é—´ã€‚ç»ä¸æ˜¯å¼€ç©ç¬‘ã€‚
- en: 17.2 Minima and maxima, revisited
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.2 æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œå†æ¬¡æ¢è®¨
- en: In a single variable, we have successfully used the derivatives to find the
    local optima of differentiable functions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å•å˜é‡çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æˆåŠŸåœ°ä½¿ç”¨å¯¼æ•°æ‰¾åˆ°äº†å¯å¾®å‡½æ•°çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚
- en: 'Recall that if f : â„ â†’â„ is differentiable everywhere, then TheoremÂ [87](ch021.xhtml#x1-214004r87)
    gives that'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›é¡¾ä¸€ä¸‹ï¼Œå¦‚æœ f : â„ â†’ â„ åœ¨å¤„å¤„å¯å¾®ï¼Œé‚£ä¹ˆå®šç† [87](ch021.xhtml#x1-214004r87) ç»™å‡ºçš„ç»“è®ºæ˜¯'
- en: (a) f^â€²(a) = 0 and f^(â€²â€²)(a)/span>0 implies a local minimum. (b) f^â€²(a) = 0
    and f^(â€²â€²)(a)/span>0 implies a local maximum.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (a) f^â€²(a) = 0 ä¸” f^(â€²â€²)(a) > 0 è¡¨ç¤ºå±€éƒ¨æœ€å°å€¼ã€‚(b) f^â€²(a) = 0 ä¸” f^(â€²â€²)(a) < 0 è¡¨ç¤ºå±€éƒ¨æœ€å¤§å€¼ã€‚
- en: (A simple f^â€²(a) = 0 is not enough, as the example f(x) = xÂ³ shows at 0.)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (ç®€å•çš„ f^â€²(a) = 0 ä¸è¶³ä»¥ç¡®å®šæœ€å€¼ï¼Œæ­£å¦‚ f(x) = xÂ³ åœ¨ 0 å¤„çš„ä¾‹å­æ‰€ç¤ºã€‚)
- en: Can we do something similar in multiple variables?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½åœ¨å¤šå˜é‡çš„æƒ…å†µä¸‹åšç±»ä¼¼çš„äº‹æƒ…å—ï¼Ÿ
- en: 'Right from the start, there seems to be an issue: the derivative is not a scalar
    (thus, we canâ€™t equate it to 0).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸€å¼€å§‹ä¼¼ä¹å°±æœ‰ä¸€ä¸ªé—®é¢˜ï¼šå¯¼æ•°ä¸æ˜¯æ ‡é‡ï¼ˆå› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½å°†å…¶ç­‰äº 0ï¼‰ã€‚
- en: 'This is easy to solve: the analogue of the condition f^â€²(a) = 0 is âˆ‡f(a) =
    (0,0,â€¦,0). For simplicity, the zero vector (0,0,â€¦,0) will also be denoted by 0\.
    Donâ€™t worry, this wonâ€™t be confusing; itâ€™s all clear from the context. Introducing
    a new notation for the zero vector would just add more complexity.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆå®¹æ˜“è§£å†³ï¼šæ¡ä»¶ f^â€²(a) = 0 çš„ç±»ä¼¼æ¡ä»¶æ˜¯ âˆ‡f(a) = (0,0,â€¦,0)ã€‚ä¸ºäº†ç®€ä¾¿ï¼Œé›¶å‘é‡ (0,0,â€¦,0) ä¹Ÿå¯ä»¥è¡¨ç¤ºä¸º 0\ã€‚åˆ«æ‹…å¿ƒï¼Œè¿™ä¸ä¼šè®©äººå›°æƒ‘ï¼›ä»ä¸Šä¸‹æ–‡ä¸­å¯ä»¥æ¸…æ¥šåœ°ç†è§£ã€‚å¼•å…¥é›¶å‘é‡çš„æ–°ç¬¦å·åªæ˜¯å¢åŠ äº†å¤æ‚æ€§ã€‚
- en: 'We can visualize what happens with the tangent plane at critical points. In
    a single variable, we have already seen this: as FigureÂ [17.1](#) illustrates,
    f^â€²(a) = 0 implies that the tangent line is horizontal.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ‡å¹³é¢æ¥ç›´è§‚åœ°ç†è§£ä¸´ç•Œç‚¹çš„æƒ…å†µã€‚åœ¨å•å˜é‡æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°è¿‡è¿™ä¸ªï¼šå¦‚å›¾ [17.1](#) æ‰€ç¤ºï¼Œf^â€²(a) = 0 è¡¨æ˜åˆ‡çº¿æ˜¯æ°´å¹³çš„ã€‚
- en: '![PIC](img/file1579.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1579.png)'
- en: 'FigureÂ 17.1: Local extrema in a single variable'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 17.1ï¼šå•å˜é‡çš„å±€éƒ¨æå€¼
- en: 'In multiple variables, the situation is similar: âˆ‡f(a) = 0 implies that the
    best local linear approximation ([16.3](ch026.xhtml#x1-257003r68)) is constant;
    that is, the tangent plane is horizontal. (As visualized by FigureÂ [17.2](#).)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šå˜é‡æƒ…å†µä¸‹ï¼Œæƒ…å†µç±»ä¼¼ï¼šâˆ‡f(a) = 0 è¡¨ç¤ºæœ€ä½³çš„å±€éƒ¨çº¿æ€§é€¼è¿‘ï¼ˆ[16.3](ch026.xhtml#x1-257003r68)ï¼‰æ˜¯å¸¸æ•°ï¼›å³åˆ‡å¹³é¢æ˜¯æ°´å¹³çš„ã€‚ï¼ˆå¦‚å›¾
    [17.2](#) æ‰€ç¤ºã€‚ï¼‰
- en: '![PIC](img/file1580.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1580.png)'
- en: 'FigureÂ 17.2: Local extrema in multiple variables'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 17.2ï¼šå¤šå˜é‡çš„å±€éƒ¨æå€¼
- en: 'So, what does âˆ‡f(a) = 0 imply? Similarly to the single-variable case, an a
    âˆˆâ„^n is called a critical point of f if âˆ‡f(a) = 0 holds. The similarity doesnâ€™t
    stop at the level of terminologies. We also have three options in multiple variables
    as well: a is'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œâˆ‡f(a) = 0 è¡¨ç¤ºä»€ä¹ˆå‘¢ï¼Ÿç±»ä¼¼äºå•å˜é‡çš„æƒ…å†µï¼Œa âˆˆâ„^n å¦‚æœæ»¡è¶³ âˆ‡f(a) = 0ï¼Œåˆ™ç§°ä¸º f çš„ä¸´ç•Œç‚¹ã€‚è¿™ç§ç›¸ä¼¼æ€§ä¸ä»…ä½“ç°åœ¨æœ¯è¯­ä¸Šï¼Œæˆ‘ä»¬åœ¨å¤šå˜é‡æƒ…å†µä¸‹ä¹Ÿæœ‰ä¸‰ç§é€‰æ‹©ï¼ša
    æ˜¯
- en: a local minimum,
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å±€éƒ¨æœ€å°å€¼ï¼Œ
- en: a local maximum,
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å±€éƒ¨æœ€å¤§å€¼ï¼Œ
- en: or neither.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ–è€…éƒ½ä¸æ˜¯ã€‚
- en: In multiple variables, a non-extremal critical point is called a saddle point,
    because the two-dimensional case bears a striking resemblance to an actual horse
    saddle, as you are about to see. Saddle points are the high-dimensional analogues
    of the one-dimensional inflection points. The functions
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šå˜é‡æƒ…å†µä¸‹ï¼Œéæå€¼çš„ä¸´ç•Œç‚¹ç§°ä¸ºéç‚¹ï¼Œå› ä¸ºäºŒç»´æƒ…å†µä¸‹çš„å½¢çŠ¶ä¸å®é™…çš„é©¬ééå¸¸ç›¸ä¼¼ï¼Œæ­£å¦‚ä½ å°†è¦çœ‹åˆ°çš„é‚£æ ·ã€‚éç‚¹æ˜¯é«˜ç»´ç©ºé—´ä¸­å•å˜é‡æ‹ç‚¹çš„ç±»æ¯”ã€‚å‡½æ•°
- en: '![ 2 2 f (x,y) = x + y , 2 2 g(x,y) = âˆ’ (x + y ), h (x,y) = x2 âˆ’ y2 ](img/file1581.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 2 f (x,y) = x + y , 2 2 g(x,y) = âˆ’ (x + y ), h (x,y) = x2 âˆ’ y2 ](img/file1581.png)'
- en: at (0,0) provide an example for all three, as FigureÂ [17.3](#), FigureÂ [17.4](#),
    and FigureÂ [17.5](#) show. (Keep in mind that a local extremum might be global.)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ (0,0) å¤„æä¾›äº†æ‰€æœ‰ä¸‰ç§æƒ…å†µçš„ç¤ºä¾‹ï¼Œå¦‚å›¾ [17.3](#)ã€å›¾ [17.4](#) å’Œå›¾ [17.5](#) æ‰€ç¤ºã€‚ï¼ˆè®°ä½ï¼Œå±€éƒ¨æå€¼å¯èƒ½æ˜¯å…¨å±€æå€¼ã€‚ï¼‰
- en: '![PIC](img/file1582.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1582.png)'
- en: 'FigureÂ 17.3: A (local) maximum'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 17.3ï¼šä¸€ä¸ªï¼ˆå±€éƒ¨ï¼‰æœ€å¤§å€¼
- en: '![PIC](img/file1583.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1583.png)'
- en: 'FigureÂ 17.4: A (local) minima'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 17.4ï¼šä¸€ä¸ªï¼ˆå±€éƒ¨ï¼‰æœ€å°å€¼
- en: '![PIC](img/file1584.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1584.png)'
- en: 'FigureÂ 17.5: A saddle point'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 17.5ï¼šä¸€ä¸ªéç‚¹
- en: To put things into order, letâ€™s start formulating definitions and theorems.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†æ¸…æ€è·¯ï¼Œæˆ‘ä»¬ä»å®šä¹‰å’Œå®šç†å¼€å§‹åˆ¶å®šã€‚
- en: Definition 73\. (Critical points)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ 73.ï¼ˆä¸´ç•Œç‚¹ï¼‰
- en: 'Let f : â„^n â†’â„ be an arbitrary vector-scalar function. We say that a âˆˆâ„^n is
    a critical point of f if either'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'è®¾ f : â„^n â†’â„ ä¸ºä»»æ„å‘é‡-æ ‡é‡å‡½æ•°ã€‚æˆ‘ä»¬è¯´ a âˆˆâ„^n æ˜¯ f çš„ä¸´ç•Œç‚¹ï¼Œå¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€ï¼š'
- en: '![âˆ‡f (a) = 0 ](img/file1585.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![âˆ‡f (a) = 0 ](img/file1585.png)'
- en: holds, or f is not partially differentiable at a in at least one variable.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿æŒæˆç«‹ï¼Œæˆ–è€… f åœ¨è‡³å°‘ä¸€ä¸ªå˜é‡ä¸Šä¸å¯åå¾®åˆ†ã€‚
- en: The second case (where f is not differentiable at a) is there to handle situations
    like f(x,y) = jxj + jyj.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç§æƒ…å†µï¼ˆf åœ¨ a å¤„ä¸å¯å¾®ï¼‰æ˜¯ä¸ºäº†å¤„ç†åƒ f(x,y) = jxj + jyj è¿™æ ·çš„æƒ…å†µã€‚
- en: For the sake of precision, letâ€™s define local extrema in multiple dimensions
    as well.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç²¾ç¡®èµ·è§ï¼Œæˆ‘ä»¬ä¹Ÿæ¥å®šä¹‰å¤šç»´çš„å±€éƒ¨æå€¼ã€‚
- en: Definition 74\. (Local minima and maxima)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ 74.ï¼ˆå±€éƒ¨æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼‰
- en: 'Let f : â„^n â†’â„ be an arbitrary vector-scalar function and let a âˆˆâ„^n be an
    arbitrary point.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'è®¾ f : â„^n â†’â„ ä¸ºä»»æ„å‘é‡-æ ‡é‡å‡½æ•°ï¼Œa âˆˆâ„^n ä¸ºä»»æ„ç‚¹ã€‚'
- en: (a) a is a local minimum if there exists an ğœ€/span>0 such that
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (a) å¦‚æœå­˜åœ¨ ğœ€/span>0ï¼Œä½¿å¾—
- en: '![f(a) â‰¤ f(x), x âˆˆ B(ğœ€,a). ](img/file1586.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![f(a) â‰¤ f(x), x âˆˆ B(ğœ€,a). ](img/file1586.png)'
- en: (b) a is a strict local minimum if there exists an ğœ€/span>0 such that
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (b) a æ˜¯ä¸¥æ ¼å±€éƒ¨æœ€å°å€¼ï¼Œå¦‚æœå­˜åœ¨ ğœ€/span>0ï¼Œä½¿å¾—
- en: '![f(a) <f(x), x âˆˆ B(ğœ€,a). ](img/file1587.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![f(a) <f(x), x âˆˆ B(ğœ€,a). ](img/file1587.png)'
- en: (c) a is a local maximum if there exists an ğœ€/span>0 such that
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (c) a æ˜¯å±€éƒ¨æœ€å¤§å€¼ï¼Œå¦‚æœå­˜åœ¨ ğœ€/span>0ï¼Œä½¿å¾—
- en: '![f(a) â‰¥ f(x), x âˆˆ B (ğœ€,a) âˆ–{a}. ](img/file1588.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![f(a) â‰¥ f(x), x âˆˆ B (ğœ€,a) âˆ–{a}. ](img/file1588.png)'
- en: (d) a is a strict local maximum if there exists an ğœ€/span>0 such that
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (d) a æ˜¯ä¸¥æ ¼å±€éƒ¨æœ€å¤§å€¼ï¼Œå¦‚æœå­˜åœ¨ ğœ€/span>0ï¼Œä½¿å¾—
- en: '![f(a) >f(x), x âˆˆ B (ğœ€,a) âˆ–{a}. ](img/file1589.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![f(a) >f(x), x âˆˆ B (ğœ–,a) âˆ–{a}. ](img/file1589.png)'
- en: As the example of xÂ² âˆ’yÂ² shows, a critical point is not necessarily a local
    extremum, but a local extremum is always a critical point. The next result, which
    is the analogue of DefinitionÂ [73](ch027.xhtml#x1-270018r73), makes this mathematically
    precise.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ xÂ² âˆ’yÂ² çš„ä¾‹å­æ‰€ç¤ºï¼Œä¸´ç•Œç‚¹ä¸ä¸€å®šæ˜¯å±€éƒ¨æå€¼ï¼Œä½†å±€éƒ¨æå€¼æ€»æ˜¯ä¸´ç•Œç‚¹ã€‚æ¥ä¸‹æ¥çš„ç»“æœï¼Œç±»ä¼¼äºå®šä¹‰ [73](ch027.xhtml#x1-270018r73)ï¼Œå¯¹æ­¤è¿›è¡Œäº†æ•°å­¦ä¸Šçš„ç²¾ç¡®æè¿°ã€‚
- en: Theorem 106\.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å®šç† 106\ã€‚
- en: 'Let f : â„^n â†’â„ be an arbitrary vector-scalar function, and suppose that f is
    partially differentiable with respect to all variables at some a âˆˆâ„^n.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä»¤ f : â„^n â†’â„ ä¸ºä¸€ä¸ªä»»æ„çš„å‘é‡-æ ‡é‡å‡½æ•°ï¼Œå‡è®¾ f åœ¨æŸä¸ª a âˆˆâ„^n å¤„å¯¹æ‰€æœ‰å˜é‡éƒ¨åˆ†å¯å¾®ã€‚'
- en: If f has a local extremum at a, then âˆ‡f(a) = 0.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ f åœ¨ a å¤„æœ‰å±€éƒ¨æå€¼ï¼Œåˆ™ âˆ‡f(a) = 0ã€‚
- en: Proof. This is a direct consequence of TheoremÂ [86](ch021.xhtml#x1-213005r86),
    as if a = (a[1],â€¦,a[n]) is a local extremum of the vector-scalar function f, then
    it is a local extremum of the single-variable functions hâ†’f(a + he[i]), where
    e[i] is the vector whose i-th component is 1, while the others are 0.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ˜ï¼šè¿™æ˜¯å®šç† [86](ch021.xhtml#x1-213005r86) çš„ç›´æ¥ç»“æœï¼Œå› ä¸ºå¦‚æœ a = (a[1],â€¦,a[n]) æ˜¯å‘é‡-æ ‡é‡å‡½æ•°
    f çš„å±€éƒ¨æå€¼ï¼Œé‚£ä¹ˆå®ƒä¹Ÿæ˜¯å•å˜é‡å‡½æ•° hâ†’f(a + he[i]) çš„å±€éƒ¨æå€¼ï¼Œå…¶ä¸­ e[i] æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå…¶ç¬¬ i ä¸ªåˆ†é‡ä¸º 1ï¼Œå…¶ä½™åˆ†é‡ä¸º 0ã€‚
- en: According to the very definition of the partial derivative given by DefinitionÂ [66](ch026.xhtml#x1-254009r66),
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®å®šä¹‰ [66](ch026.xhtml#x1-254009r66) ç»™å‡ºçš„åå¯¼æ•°å®šä¹‰ï¼Œ
- en: '![-d-f(a + hei) =-âˆ‚f-(a). dh âˆ‚xi ](img/file1591.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![-d-f(a + hei) =-âˆ‚f-(a). dh âˆ‚xi ](img/file1591.png)'
- en: Thus, TheoremÂ [86](ch021.xhtml#x1-213005r86) gives that
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå®šç† [86](ch021.xhtml#x1-213005r86) ç»™å‡ºäº†
- en: '![ âˆ‚f âˆ‚x--(a ) = 0 i ](img/file1592.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![ âˆ‚f âˆ‚x--(a ) = 0 i ](img/file1592.png)'
- en: for all i = 1,â€¦,n, giving that âˆ‡f(a) = 0.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ‰€æœ‰ i = 1,â€¦,nï¼Œç»™å®š âˆ‡f(a) = 0ã€‚
- en: So, how can we find the local extrema with the derivative? As we have already
    suggested, studying the second derivative will help us pinpoint the extrema among
    critical points. Unfortunately, things are much more complicated in n variables,
    so letâ€™s focus on the two-variable case first.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•é€šè¿‡å¯¼æ•°æ¥æ‰¾åˆ°å±€éƒ¨æå€¼å‘¢ï¼Ÿæ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€å»ºè®®çš„ï¼Œç ”ç©¶äºŒé˜¶å¯¼æ•°å°†å¸®åŠ©æˆ‘ä»¬åœ¨ä¸´ç•Œç‚¹ä¸­å®šä½æå€¼ã€‚ä¸å¹¸çš„æ˜¯ï¼Œn å˜é‡çš„æƒ…å†µè¦å¤æ‚å¾—å¤šï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆé›†ä¸­è®¨è®ºäºŒå…ƒæƒ…å†µã€‚
- en: Theorem 107\. (The two-variable second derivative test)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å®šç† 107\ã€‚ï¼ˆäºŒå…ƒå‡½æ•°çš„äºŒé˜¶å¯¼æ•°æ£€éªŒï¼‰
- en: 'Let f : â„Â² â†’â„ be an arbitrary vector-scalar function, and suppose that f is
    partially differentiable at some a âˆˆâ„Â². Also suppose that a is a critical point,
    that is, âˆ‡f(a) = 0.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä»¤ f : â„Â² â†’â„ ä¸ºä¸€ä¸ªä»»æ„çš„å‘é‡-æ ‡é‡å‡½æ•°ï¼Œå‡è®¾ f åœ¨æŸä¸ª a âˆˆâ„Â² å¤„éƒ¨åˆ†å¯å¾®ã€‚å¹¶ä¸”å‡è®¾ a æ˜¯ä¸€ä¸ªä¸´ç•Œç‚¹ï¼Œå³ âˆ‡f(a) = 0ã€‚'
- en: (a) If detH[f](a)/span>0 and ![âˆ‚2f- âˆ‚x22](img/file1593.png)/span>0, then a is
    a local minimum.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (a) å¦‚æœ detH[f](a)/span>0 ä¸” ![âˆ‚2f- âˆ‚x22](img/file1593.png)/span>0ï¼Œåˆ™ a æ˜¯å±€éƒ¨æœ€å°å€¼ã€‚
- en: (b) If detH[f](a)/span>0 and ![âˆ‚2f- âˆ‚x22](img/file1594.png)/span>0, then a is
    a local maximum.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (b) å¦‚æœ detH[f](a)/span>0 ä¸” ![âˆ‚2f- âˆ‚x22](img/file1594.png)/span>0ï¼Œåˆ™ a æ˜¯å±€éƒ¨æœ€å¤§å€¼ã€‚
- en: (c) If detH[f](a)/span>0, then a is a saddle point.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (c) å¦‚æœ detH[f](a)/span>0ï¼Œåˆ™ a æ˜¯ä¸€ä¸ªéç‚¹ã€‚
- en: We will not prove this, but some remarks are in order. First, as the determinant
    of the Hessian can be 0, TheoremÂ [107](ch027.xhtml#x1-270025r107) does not cover
    all possible cases.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä¼šè¯æ˜è¿™ä¸€ç‚¹ï¼Œä½†æœ‰ä¸€äº›è¯´æ˜æ˜¯å¿…è¦çš„ã€‚é¦–å…ˆï¼Œç”±äº Hessian è¡Œåˆ—å¼å¯èƒ½ä¸º 0ï¼Œå®šç† [107](ch027.xhtml#x1-270025r107)
    å¹¶æœªæ¶µç›–æ‰€æœ‰å¯èƒ½çš„æƒ…å†µã€‚
- en: Itâ€™s probably best to see a few examples, so letâ€™s revisit the previously seen
    functions
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¥½çœ‹å‡ ä¸ªä¾‹å­ï¼Œå› æ­¤è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ä¹‹å‰çœ‹åˆ°çš„å‡½æ•°
- en: '![f (x,y) = x2 + y2, g(x,y) = âˆ’ (x2 + y2), 2 2 h (x,y) = x âˆ’ y . ](img/file1595.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![f (x,y) = x2 + y2, g(x,y) = âˆ’ (x2 + y2), 2 2 h (x,y) = x âˆ’ y . ](img/file1595.png)'
- en: All three have a critical point at 0, so the Hessians can provide a clearer
    picture. The Hessians are given by the matrices
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸‰ä¸ªå‡½æ•°åœ¨ 0 å¤„éƒ½æœ‰ä¸€ä¸ªä¸´ç•Œç‚¹ï¼Œå› æ­¤ Hessian å¯ä»¥æä¾›æ›´æ¸…æ™°çš„å›¾åƒã€‚Hessian çš„çŸ©é˜µä¸º
- en: '![ âŒŠ âŒ‹ âŒŠ âŒ‹ âŒŠ âŒ‹ âŒˆ2 0âŒ‰ âŒˆâˆ’ 2 0 âŒ‰ âŒˆ2 0 âŒ‰ Hf (x,y) = 0 2 , Hg(x,y) = 0 âˆ’ 2 , Hh
    (x,y) = 0 âˆ’ 2 . ](img/file1596.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![ âŒŠ âŒ‹ âŒŠ âŒ‹ âŒŠ âŒ‹ âŒˆ2 0âŒ‰ âŒˆâˆ’ 2 0 âŒ‰ âŒˆ2 0 âŒ‰ Hf (x,y) = 0 2 , Hg(x,y) = 0 âˆ’ 2 , Hh
    (x,y) = 0 âˆ’ 2 . ](img/file1596.png)'
- en: For functions of two variables, TheoremÂ [107](ch027.xhtml#x1-270025r107) says
    that it is enough to study detH[f](a) and ![âˆ‚2f âˆ‚y2](img/file1597.png)(a).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºäºŒå…ƒå‡½æ•°ï¼Œå®šç† [107](ch027.xhtml#x1-270025r107) è¯´æ˜ï¼Œç ”ç©¶ detH[f](a) å’Œ ![âˆ‚2f âˆ‚y2](img/file1597.png)(a)
    å°±è¶³å¤Ÿäº†ã€‚
- en: In the case of f(x,y) = xÂ² + yÂ², we have H[f](0,0) = 4 and ![âˆ‚2f âˆ‚y2](img/file1598.png)(0,0)
    = 2, giving that 0 is a local minimum of f(x,y) = xÂ² + yÂ².
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº f(x,y) = xÂ² + yÂ²ï¼Œæˆ‘ä»¬æœ‰ H[f](0,0) = 4 ä¸” ![âˆ‚2f âˆ‚y2](img/file1598.png)(0,0) =
    2ï¼Œè¯´æ˜ 0 æ˜¯ f(x,y) = xÂ² + yÂ² çš„å±€éƒ¨æœ€å°å€¼ã€‚
- en: Similarly, we can conclude that 0 is a local maximum of g(x,y) = âˆ’(xÂ² + yÂ²)
    (which shouldnâ€™t surprise you, as g = âˆ’f).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·åœ°ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œ0 æ˜¯ g(x,y) = âˆ’(xÂ² + yÂ²) çš„å±€éƒ¨æœ€å¤§å€¼ï¼ˆè¿™å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œå› ä¸º g = âˆ’fï¼‰ã€‚
- en: Finally, for h(x,y) = xÂ² âˆ’yÂ², the second derivative test confirms that 0 is
    indeed a saddle point.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¯¹äº h(x,y) = xÂ² âˆ’ yÂ²ï¼ŒäºŒé˜¶å¯¼æ•°æ£€éªŒç¡®è®¤ 0 ç¡®å®æ˜¯ä¸€ä¸ªéç‚¹ã€‚
- en: 'So, whatâ€™s up with the general case? Unfortunately, just studying the determinant
    of the Hessian matrix is not enough. We need to bring in the heavy-hitters: eigenvalues.
    (See DefinitionÂ [23](ch012.xhtml#x1-105003r23).) Here is the second derivative
    test in its full glory.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œé€šç”¨æƒ…å†µå¦‚ä½•å‘¢ï¼Ÿä¸å¹¸çš„æ˜¯ï¼Œä»…ä»…ç ”ç©¶HessiançŸ©é˜µçš„è¡Œåˆ—å¼æ˜¯ä¸å¤Ÿçš„ã€‚æˆ‘ä»¬éœ€è¦å¼•å…¥é‡ç£…äººç‰©ï¼šç‰¹å¾å€¼ã€‚ï¼ˆè§å®šä¹‰[23](ch012.xhtml#x1-105003r23)ï¼‰ã€‚è¿™å°±æ˜¯äºŒé˜¶å¯¼æ•°æ£€éªŒçš„å®Œæ•´å½¢å¼ã€‚
- en: Theorem 108\. (The multivariable second derivative test)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å®šç† 108. ï¼ˆå¤šå˜é‡äºŒé˜¶å¯¼æ•°æ£€éªŒï¼‰
- en: 'Let f : â„^n â†’â„ be an arbitrary vector-scalar function, and suppose that f is
    partially differentiable with respect to all variables at some a âˆˆâ„^n. Also suppose
    that a is a critical point, that is, âˆ‡f(a) = 0.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'è®¾ f : â„^n â†’â„ æ˜¯ä¸€ä¸ªä»»æ„çš„å‘é‡-æ ‡é‡å‡½æ•°ï¼Œå‡è®¾ f å¯¹æ‰€æœ‰å˜é‡åœ¨æŸä¸ª a âˆˆâ„^n å¤„éƒ¨åˆ†å¯å¾®ã€‚è¿˜å‡è®¾ a æ˜¯ä¸€ä¸ªä¸´ç•Œç‚¹ï¼Œå³ âˆ‡f(a)
    = 0ã€‚'
- en: (a) If all the eigenvalues of H[f](a) are positive, then a is a local minimum.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (a) å¦‚æœ H[f](a) çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯æ­£æ•°ï¼Œåˆ™ a æ˜¯å±€éƒ¨æœ€å°å€¼ã€‚
- en: (b) If all the eigenvalues of H[f](a) are negative, then a is a local maximum.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (b) å¦‚æœ H[f](a) çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯è´Ÿæ•°ï¼Œåˆ™ a æ˜¯å±€éƒ¨æœ€å¤§å€¼ã€‚
- en: (c) If all the eigenvalues of H[f](a) are either positive or negative, then
    a is a saddle point.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (c) å¦‚æœ H[f](a) çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯æ­£æ•°æˆ–è´Ÿæ•°ï¼Œåˆ™ a æ˜¯éç‚¹ã€‚
- en: 'Thatâ€™s right: if any of the eigenvalues are 0, then the test is inconclusive.
    You might recall from linear algebra that in practice, computing the eigenvalues
    is not as fast as computing the second-order derivatives, but there are plenty
    of numerical methods (like the QR-algorithm, as we saw in SectionÂ [7.5](ch013.xhtml#computing-eigenvalues)).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡é”™ï¼šå¦‚æœä»»ä½•ç‰¹å¾å€¼ä¸º0ï¼Œåˆ™è¯¥æ£€éªŒæ— æ³•å¾—å‡ºç»“è®ºã€‚ä½ å¯èƒ½è¿˜è®°å¾—åœ¨çº¿æ€§ä»£æ•°ä¸­ï¼Œå®é™…ä¸Šè®¡ç®—ç‰¹å¾å€¼çš„é€Ÿåº¦ä¸å¦‚è®¡ç®—äºŒé˜¶å¯¼æ•°å¿«ï¼Œä½†æœ‰å¾ˆå¤šæ•°å€¼æ–¹æ³•ï¼ˆæ¯”å¦‚æˆ‘ä»¬åœ¨ç¬¬[7.5](ch013.xhtml#computing-eigenvalues)èŠ‚ä¸­çœ‹åˆ°çš„QRç®—æ³•ï¼‰ã€‚
- en: 'To sum it up, the method of optimizing (differentiable) multivariable functions
    is a simple two-step process:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œä¼˜åŒ–ï¼ˆå¯å¾®ï¼‰å¤šå˜é‡å‡½æ•°çš„æ–¹æ³•æ˜¯ä¸€ä¸ªç®€å•çš„ä¸¤æ­¥è¿‡ç¨‹ï¼š
- en: find the critical points by solving the equation âˆ‡f(x) = 0,
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡è§£æ–¹ç¨‹ âˆ‡f(x) = 0 æ¥æ‰¾åˆ°ä¸´ç•Œç‚¹ï¼Œ
- en: then use the second derivative test to determine which critical points are extrema.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åä½¿ç”¨äºŒé˜¶å¯¼æ•°æ£€éªŒæ¥ç¡®å®šå“ªäº›ä¸´ç•Œç‚¹æ˜¯æå€¼ç‚¹ã€‚
- en: Do we use this method in practice to optimize functions? No. Why? Most importantly,
    because computing the eigenvalues of the Hessian for a vector-scalar function
    with millions of variables is extremely hard. Why is the second derivative test
    so important? Because understanding the behavior of functions around their extremal
    points is essential to truly understand gradient descent. Believe it or not, this
    is the key behind the theoretical guarantees for convergence.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å®é™…ä¸­ä½¿ç”¨è¿™ç§æ–¹æ³•æ¥ä¼˜åŒ–å‡½æ•°å—ï¼Ÿä¸ä½¿ç”¨ã€‚ä¸ºä»€ä¹ˆï¼Ÿæœ€é‡è¦çš„åŸå› æ˜¯ï¼Œå¯¹äºå…·æœ‰æ•°ç™¾ä¸‡å˜é‡çš„å‘é‡-æ ‡é‡å‡½æ•°ï¼Œè®¡ç®—HessiançŸ©é˜µçš„ç‰¹å¾å€¼æ˜¯éå¸¸å›°éš¾çš„ã€‚ä¸ºä»€ä¹ˆäºŒé˜¶å¯¼æ•°æ£€éªŒå¦‚æ­¤é‡è¦ï¼Ÿå› ä¸ºç†è§£å‡½æ•°åœ¨æå€¼ç‚¹é™„è¿‘çš„è¡Œä¸ºæ˜¯æ·±å…¥ç†è§£æ¢¯åº¦ä¸‹é™çš„å…³é”®ã€‚ä¿¡ä¸ä¿¡ç”±ä½ ï¼Œè¿™å°±æ˜¯ç†è®ºä¸Šä¿è¯æ”¶æ•›æ€§çš„å…³é”®ã€‚
- en: Speaking of gradient descent, now is the time to dig deep into the algorithm
    that powers neural networks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´åˆ°æ¢¯åº¦ä¸‹é™ï¼Œç°åœ¨æ˜¯æ—¶å€™æ·±å…¥æ¢è®¨é©±åŠ¨ç¥ç»ç½‘ç»œçš„ç®—æ³•äº†ã€‚
- en: 17.3 Gradient descent in its full form
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3 æ¢¯åº¦ä¸‹é™çš„å®Œæ•´å½¢å¼
- en: Gradient descent is one of the most important algorithms in machine learning.
    We have talked about this a lot, although up until this point, we have only seen
    it for single-variable functions (which is, I admit, not the most practical use
    case).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€é‡è¦çš„ç®—æ³•ä¹‹ä¸€ã€‚æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡å¾ˆå¤šæ¬¡ï¼Œå°½ç®¡åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä»…ä»…çœ‹åˆ°å®ƒç”¨äºå•å˜é‡å‡½æ•°ï¼ˆæˆ‘æ‰¿è®¤ï¼Œè¿™å¹¶ä¸æ˜¯æœ€å®é™…çš„åº”ç”¨æ¡ˆä¾‹ï¼‰ã€‚
- en: However, now we have all the tools we need to talk about gradient descent in
    its general form. Letâ€™s get to it!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç°åœ¨æˆ‘ä»¬å·²ç»æŒæ¡äº†æ‰€æœ‰è®¨è®ºæ¢¯åº¦ä¸‹é™ä¸€èˆ¬å½¢å¼æ‰€éœ€çš„å·¥å…·ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: 'Suppose that we have a differentiable vector-scalar function f : â„^n â†’â„ that
    we want to maximize. This can describe the return on investment of an investing
    strategy, or any other quantity. Calculating the gradient and finding the critical
    points is often not an option, as solving the equation âˆ‡f(x) = 0 can be computationally
    unfeasible. Thus, we resort to an iterative solution.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¯å¾®çš„å‘é‡-æ ‡é‡å‡½æ•° f : â„^n â†’â„ ï¼Œæˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–å®ƒã€‚è¿™å¯ä»¥æè¿°æŠ•èµ„ç­–ç•¥çš„å›æŠ¥ï¼Œæˆ–ä»»ä½•å…¶ä»–é‡ã€‚è®¡ç®—æ¢¯åº¦å¹¶æ‰¾åˆ°ä¸´ç•Œç‚¹é€šå¸¸ä¸å¯è¡Œï¼Œå› ä¸ºè§£æ–¹ç¨‹
    âˆ‡f(x) = 0 åœ¨è®¡ç®—ä¸Šå¯èƒ½ä¸å¯è¡Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨è¿­ä»£è§£æ³•ã€‚'
- en: 'The algorithm is the same as for single-variable functions (as seen in SectionÂ [13.2](ch021.xhtml#the-basics-of-gradient-descent)):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®—æ³•ä¸å•å˜é‡å‡½æ•°çš„æƒ…å†µç›¸åŒï¼ˆå¦‚ç¬¬[13.2](ch021.xhtml#the-basics-of-gradient-descent)èŠ‚æ‰€è§ï¼‰ï¼š
- en: Start from a random point.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ä¸€ä¸ªéšæœºç‚¹å¼€å§‹ã€‚
- en: Calculate its gradient.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—å®ƒçš„æ¢¯åº¦ã€‚
- en: Take a step towards its direction.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‘å®ƒçš„æ–¹å‘è¿ˆå‡ºä¸€æ­¥ã€‚
- en: Repeat until convergence.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡å¤ç›´åˆ°æ”¶æ•›ã€‚
- en: This is called gradient ascent. We can formalize it in the following way.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¢«ç§°ä¸ºæ¢¯åº¦ä¸Šå‡ã€‚æˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è¿›è¡Œå½¢å¼åŒ–ã€‚
- en: (The gradient ascent algorithm) Step 1\. Initialize the starting point x[0]
    âˆˆâ„^n and select a learning rate h âˆˆ (0,âˆ). Step 2\. Let![xn+1 := xn + hâˆ‡f (xn
    ). ](img/file1599.png)Step 3\. Repeat Step 2\. until convergence.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆæ¢¯åº¦ä¸Šå‡ç®—æ³•ï¼‰æ­¥éª¤1ï¼šåˆå§‹åŒ–èµ·å§‹ç‚¹x[0] âˆˆâ„^nå¹¶é€‰æ‹©å­¦ä¹ ç‡h âˆˆ (0,âˆ)ã€‚æ­¥éª¤2ï¼šä»¤![xn+1 := xn + hâˆ‡f (xn )](img/file1599.png)ã€‚æ­¥éª¤3ï¼šé‡å¤æ­¥éª¤2ï¼Œç›´åˆ°æ”¶æ•›ã€‚
- en: If we want to minimize f, we might as well maximize âˆ’f. The only effect of this
    is a sign change for the gradient. In this form, the algorithm is called gradient
    descent, and this is the version thatâ€™s widely used to train neural networks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³æœ€å°åŒ–fï¼Œé‚£ä¹ˆä¸å¦¨æœ€å¤§åŒ–âˆ’fã€‚è¿™æ ·åšçš„å”¯ä¸€å½±å“æ˜¯æ¢¯åº¦çš„ç¬¦å·å˜åŒ–ã€‚ä»¥è¿™ç§å½¢å¼ï¼Œç®—æ³•è¢«ç§°ä¸ºæ¢¯åº¦ä¸‹é™æ³•ï¼Œè¿™ä¹Ÿæ˜¯å¹¿æ³›ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œçš„ç‰ˆæœ¬ã€‚
- en: (The gradient descent algorithm) Step 1\. Initialize the starting point x[0]
    âˆˆâ„^n and select a learning rate h âˆˆ (0,âˆ). Step 2\. Let![xn+1 := xn âˆ’ hâˆ‡f (xn
    ). ](img/file1600.png)Step 3\. Repeat Step 2\. until convergence.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼‰æ­¥éª¤1ï¼šåˆå§‹åŒ–èµ·å§‹ç‚¹x[0] âˆˆâ„^nå¹¶é€‰æ‹©å­¦ä¹ ç‡h âˆˆ (0,âˆ)ã€‚æ­¥éª¤2ï¼šä»¤![xn+1 := xn âˆ’ hâˆ‡f (xn )](img/file1600.png)ã€‚æ­¥éª¤3ï¼šé‡å¤æ­¥éª¤2ï¼Œç›´åˆ°æ”¶æ•›ã€‚
- en: After all of this setup, implementing gradient descent is straightforward.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆæ‰€æœ‰è¿™äº›è®¾ç½®åï¼Œå®ç°æ¢¯åº¦ä¸‹é™æ³•éå¸¸ç›´æ¥ã€‚
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice that it is almost identical to the single-variable version in SectionÂ [13.2](ch021.xhtml#the-basics-of-gradient-descent).
    To see if it works correctly, letâ€™s test it out on the squared Euclidean norm
    function, implemented by SquaredNorm earlier!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œè¿™ä¸ç¬¬[13.2](ch021.xhtml#the-basics-of-gradient-descent)èŠ‚ä¸­çš„å•å˜é‡ç‰ˆæœ¬å‡ ä¹å®Œå…¨ç›¸åŒã€‚ä¸ºäº†éªŒè¯å®ƒæ˜¯å¦æœ‰æ•ˆï¼Œè®©æˆ‘ä»¬åœ¨ä¹‹å‰ç”±SquaredNormå®ç°çš„å¹³æ–¹æ¬§å‡ é‡Œå¾—èŒƒæ•°å‡½æ•°ä¸Šè¿›è¡Œæµ‹è¯•ï¼
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is nothing special to it, really. The issues with multivariable gradient
    descent are the same as what we discussed with the single-variable version: it
    can get stuck in local minima, it is sensitive to our choice of learning rate,
    and the gradient can be computationally hard to calculate in high dimensions.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šå¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ã€‚å¤šå˜é‡æ¢¯åº¦ä¸‹é™æ³•çš„é—®é¢˜ä¸æˆ‘ä»¬åœ¨å•å˜é‡ç‰ˆæœ¬ä¸­è®¨è®ºçš„ç›¸åŒï¼šå®ƒå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼ï¼Œå¯¹å­¦ä¹ ç‡çš„é€‰æ‹©éå¸¸æ•æ„Ÿï¼Œè€Œä¸”åœ¨é«˜ç»´åº¦ä¸‹ï¼Œæ¢¯åº¦çš„è®¡ç®—å¯èƒ½éå¸¸å›°éš¾ã€‚
- en: 17.4 Summary
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.4 å°ç»“
- en: 'Although this chapter was short and sweet, we took quite a big step by dissecting
    the fine details of gradient descent in high dimensions. The chapterâ€™s brevity
    is a testament to the power of vectorization: same formulas, code, and supercharged
    functionality. Itâ€™s quite unbelievable, but the simple algorithm'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ¬ç« ç®€çŸ­è€Œç²¾ç‚¼ï¼Œä½†é€šè¿‡è¯¦ç»†å‰–æé«˜ç»´åº¦æ¢¯åº¦ä¸‹é™æ³•çš„ç»†èŠ‚ï¼Œæˆ‘ä»¬è¿ˆå‡ºäº†ä¸€ä¸ªå¾ˆå¤§çš„æ­¥ä¼ã€‚æœ¬ç« çš„ç®€æ´æ˜¯å‘é‡åŒ–å¼ºå¤§åŠŸèƒ½çš„è§è¯ï¼šç›¸åŒçš„å…¬å¼ã€ä»£ç å’Œè¶…çº§å……èƒ½çš„åŠŸèƒ½ã€‚è¿™å‡ ä¹éš¾ä»¥ç½®ä¿¡ï¼Œä½†è¿™ä¸ªç®€å•çš„ç®—æ³•
- en: '![xn+1 = xn âˆ’ hâˆ‡f (xn) ](img/file1601.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![xn+1 = xn âˆ’ hâˆ‡f (xn)](img/file1601.png)'
- en: is behind most of the neural network models. Yes, even state-of-the-art ones.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¤§å¤šæ•°ç¥ç»ç½‘ç»œæ¨¡å‹èƒŒåçš„åŸç†ã€‚æ˜¯çš„ï¼Œç”šè‡³æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚
- en: This lies on the same theoretical foundations as the univariate case, but instead
    of checking the positivity of the second derivatives, we have to study the full
    Hessian matrix H[f]. To be more precise, we have learned that a critical point
    âˆ‡f(a) = 0 is
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸å•å˜é‡æƒ…å†µæœ‰ç›¸åŒçš„ç†è®ºåŸºç¡€ï¼Œä½†æˆ‘ä»¬ä¸å†æ£€æŸ¥äºŒé˜¶å¯¼æ•°çš„æ­£æ€§ï¼Œè€Œæ˜¯éœ€è¦ç ”ç©¶å®Œæ•´çš„HessiançŸ©é˜µH[f]ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬å·²ç»äº†è§£åˆ°ï¼Œä¸´ç•Œç‚¹âˆ‡f(a)
    = 0æ˜¯
- en: a local minimum if all the eigenvalues of H[f](a) are positive,
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœH[f](a)çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯æ­£æ•°ï¼Œåˆ™ä¸ºå±€éƒ¨æœ€å°å€¼ï¼Œ
- en: and a local maximum if all the eigenvalues of H[f](a) are negative.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœH[f](a)çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯è´Ÿæ•°ï¼Œåˆ™ä¸ºå±€éƒ¨æœ€å¤§å€¼ã€‚
- en: Deep down, this is the reason why gradient descent works. And with this, we
    have finished our study of calculus, both in single and multiple variables.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ ¹æœ¬ä¸Šè®²ï¼Œè¿™å°±æ˜¯æ¢¯åº¦ä¸‹é™æ³•æœ‰æ•ˆçš„åŸå› ã€‚æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†å¯¹å¾®ç§¯åˆ†çš„å­¦ä¹ ï¼ŒåŒ…æ‹¬å•å˜é‡å’Œå¤šå˜é‡æƒ…å†µã€‚
- en: 'Take a deep breath and relax a bit. We are approaching the final stretch of
    our adventure: our last stop is the theory of probability, the thinking paradigm
    that is behind predictive modeling. For instance, the most famous loss functions,
    like the mean-squared error or the cross-entropy, are founded upon probabilistic
    concepts. Understanding and taming uncertainty is one of the biggest intellectual
    feats of science, and we are about to undertake this journey ourselves.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±å‘¼å¸ï¼Œæ”¾æ¾ä¸€ä¸‹ã€‚æˆ‘ä»¬å³å°†è¿›å…¥å†’é™©çš„æœ€åé˜¶æ®µï¼šæˆ‘ä»¬æœ€åçš„ç›®çš„åœ°æ˜¯æ¦‚ç‡è®ºï¼Œè¿™æ˜¯é¢„æµ‹å»ºæ¨¡èƒŒåçš„æ€ç»´èŒƒå¼ã€‚ä¾‹å¦‚ï¼Œæœ€è‘—åçš„æŸå¤±å‡½æ•°ï¼Œå¦‚å‡æ–¹è¯¯å·®æˆ–äº¤å‰ç†µï¼Œéƒ½æ˜¯åŸºäºæ¦‚ç‡æ¦‚å¿µçš„ã€‚ç†è§£å’Œé©¾é©­ä¸ç¡®å®šæ€§æ˜¯ç§‘å­¦é¢†åŸŸä¸­æœ€å¤§çš„æ™ºåŠ›æˆå°±ä¹‹ä¸€ï¼Œæˆ‘ä»¬å³å°†å¼€å§‹è¿™æ®µæ—…ç¨‹ã€‚
- en: See you in the next chapter!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ç« è§ï¼
- en: 17.5 Problems
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.5 é—®é¢˜
- en: Problem 1\. Let y âˆˆâ„^n be an arbitrary vector. The general version of the famous
    mean-squared error is defined by
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ 1. è®¾ y âˆˆ â„^n æ˜¯ä¸€ä¸ªä»»æ„å‘é‡ã€‚è‘—åçš„å‡æ–¹è¯¯å·®çš„ä¸€èˆ¬ç‰ˆæœ¬å®šä¹‰ä¸º
- en: '![ n MSE (x) = 1-âˆ‘ (x âˆ’ y )2\. n i i i=1 ](img/file1602.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![ n MSE (x) = 1-âˆ‘ (x âˆ’ y )Â². n i i i=1 ](img/file1602.png)'
- en: Compute its gradient and implement it using the MultivariateFunction base class!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—å…¶æ¢¯åº¦ï¼Œå¹¶ä½¿ç”¨ MultivariateFunction åŸºç±»å®ç°å®ƒï¼
- en: 'Problem 2\. Let f : â„Â² â†’â„ be the function defined by'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 'é—®é¢˜ 2. è®¾ f : â„Â² â†’ â„ æ˜¯ç”±ä»¥ä¸‹æ–¹å¼å®šä¹‰çš„å‡½æ•°'
- en: '![ 2 2 f(x,y) = (2x âˆ’ y)(y âˆ’ x ). ](img/file1603.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 2 f(x,y) = (2x âˆ’ y)(y âˆ’ x )Â². ](img/file1603.png)'
- en: Does f have a local extremum in x = (0,0)?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: f åœ¨ x = (0,0) å¤„æœ‰å±€éƒ¨æå€¼å—ï¼Ÿ
- en: Problem 3\. Use the previously implemented gradient_descent function to find
    the minimum of
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ 3. ä½¿ç”¨ä¹‹å‰å®ç°çš„ gradient_descent å‡½æ•°æ¥æ‰¾åˆ°ä»¥ä¸‹å‡½æ•°çš„æœ€å°å€¼
- en: '![ 2 2 f(x,y) = sin(x+ y)+ x y . ](img/file1604.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 2 f(x,y) = sin(x+ y)+ x y . ](img/file1604.png)'
- en: Experiment with various learning rates and initial values!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•ä¸åŒçš„å­¦ä¹ ç‡å’Œåˆå§‹å€¼ï¼
- en: 'Problem 4\. In the problem section of Chapter 13, we saw the improved version
    of gradient descent, called gradient descent with momentum. We can do the same
    in multiple variables: define'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ 4. åœ¨ç¬¬ 13 ç« çš„é—®é¢˜éƒ¨åˆ†ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ¢¯åº¦ä¸‹é™çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç§°ä¸ºå¸¦åŠ¨é‡çš„æ¢¯åº¦ä¸‹é™ã€‚æˆ‘ä»¬å¯ä»¥åœ¨å¤šå˜é‡çš„æƒ…å†µä¸‹åšç›¸åŒçš„äº‹æƒ…ï¼šå®šä¹‰
- en: '![dn+1 = Î±dn âˆ’ hf â€²(xn), xn+1 = xn + dn, ](img/file1605.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![dn+1 = Î±dn âˆ’ hf â€²(xn), xn+1 = xn + dn, ](img/file1605.png)'
- en: where d[0] = 0 and x[0] is arbitrary. Implement it!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ d[0] = 0 ä¸” x[0] æ˜¯ä»»æ„çš„ã€‚å®ç°å®ƒï¼
- en: Join our community on Discord
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„ Discord ç¤¾åŒº
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»–ç”¨æˆ·ã€æœºå™¨å­¦ä¹ ä¸“å®¶ä»¥åŠä½œè€…æœ¬äººä¸€èµ·é˜…è¯»è¿™æœ¬ä¹¦ã€‚æé—®ã€ä¸ºå…¶ä»–è¯»è€…æä¾›è§£å†³æ–¹æ¡ˆã€é€šè¿‡â€œé—®æˆ‘ä»»ä½•é—®é¢˜â€ç¯èŠ‚ä¸ä½œè€…äº’åŠ¨ï¼Œç­‰ç­‰ã€‚æ‰«æäºŒç»´ç æˆ–è®¿é—®é“¾æ¥åŠ å…¥ç¤¾åŒºã€‚
    [https://packt.link/math](https://packt.link/math)
- en: '![PIC](img/file1.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
