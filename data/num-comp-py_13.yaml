- en: Grouping for Aggregation, Filtration, and Transformation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合、过滤和转换的分组
- en: One of the most fundamental tasks during a data analysis involves splitting
    data into independent groups before performing a calculation on each group. This
    methodology has been around for quite some time but has more recently been referred
    to as **split-apply-combine**. This chapter covers the powerful `groupby` method,
    which allows you to group your data in any way imaginable and apply any type of
    function independently to each group before returning a single dataset.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析中的一项最基本任务是将数据拆分成独立的组，然后对每个组执行计算。这种方法已经存在很长时间了，但最近被称为**split-apply-combine**。本章将介绍强大的
    `groupby` 方法，它允许你以任何想象得到的方式对数据进行分组，并独立地对每个组应用任何类型的函数，然后返回一个单一的数据集。
- en: Hadley Wickham coined the term **split-apply-combine** to describe the common
    data analysis pattern of breaking up data into independent manageable chunks,
    independently applying functions to these chunks, and then combining the results
    back together. More details can be found in his paper ([http://bit.ly/2isFuL9](http://www.stat.wvu.edu/~jharner/courses/stat623/docs/plyrJSS.pdf)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Hadley Wickham 创造了**split-apply-combine**这个术语，用来描述常见的数据分析模式：将数据拆分成独立且可管理的块，独立地对这些块应用函数，然后将结果重新组合。更多细节可以在他的论文中找到（[http://bit.ly/2isFuL9](http://www.stat.wvu.edu/~jharner/courses/stat623/docs/plyrJSS.pdf)）。
- en: 'Before we get started with the recipes, we will need to know just a little
    terminology. All basic groupby operations have **grouping columns**, and each
    unique combination of values in these columns represents an independent grouping
    of the data. The syntax looks as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始具体的操作之前，我们需要了解一些基本的术语。所有基本的 `groupby` 操作都有**分组列**，这些列中每种独特的值组合代表数据的一个独立分组。语法如下：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The result of this operation returns a groupby object. It is this groupby object
    that will be the engine that drives all the calculations for this entire chapter.
    Pandas actually does very little when creating this groupby object, merely validating
    that grouping is possible. You will have to chain methods on this groupby object
    in order to unleash its powers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作的结果返回一个 `groupby` 对象。正是这个 `groupby` 对象将作为驱动整个章节所有计算的引擎。实际上，Pandas 在创建这个
    `groupby` 对象时几乎不做任何事情，只是验证分组是否可能。你需要在这个 `groupby` 对象上链式调用方法，以释放它的强大功能。
- en: Technically, the result of the operation will either be a `DataFrameGroupBy`
    or `SeriesGroupBy` but for simplicity, it will be referred to as the groupby object
    for the entire chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，操作的结果将是 `DataFrameGroupBy` 或 `SeriesGroupBy`，但为了简便起见，本章将统一称之为 `groupby`
    对象。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Defining an aggregation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义聚合
- en: Grouping and aggregating with multiple columns and functions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个列和函数进行分组和聚合
- en: Removing the MultiIndex after grouping
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分组后移除 MultiIndex
- en: Customizing an aggregation function
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义聚合函数
- en: Customizing aggregating functions with `*args` and `**kwargs`
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `*args` 和 `**kwargs` 自定义聚合函数
- en: Examining the `groupby` object
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查 `groupby` 对象
- en: Filtering for states with a minority majority
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤拥有少数族裔多数的州
- en: Transforming through a weight loss bet
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减肥赌注进行转换
- en: Calculating weighted mean SAT scores per state with apply
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 apply 计算每个州的加权平均 SAT 分数
- en: Grouping by continuous variables
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按连续变量分组
- en: Counting the total number of flights between cities
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算城市之间的航班总数
- en: Finding the longest streak of on-time flights
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找最长的准时航班连续记录
- en: Defining an aggregation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义聚合
- en: The most common use of the `groupby` method is to perform an aggregation. What
    actually is an aggregation? In our data analysis world, an aggregation takes place
    when a sequence of many inputs get summarized or combined into a single value
    output. For example, summing up all the values of a column or finding its maximum
    are common aggregations applied on a single sequence of data. An aggregation simply
    takes many values and converts them down to a single value.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupby` 方法最常见的用途是执行聚合操作。那么，什么是聚合呢？在我们的数据分析领域，当许多输入的序列被总结或合并成一个单一的输出值时，就发生了聚合。例如，对某一列的所有值求和或找出其最大值，是对单一数据序列应用的常见聚合操作。聚合操作就是将多个值转换为一个单一的值。'
- en: In addition to the grouping columns defined during the introduction, most aggregations
    have two other components, the **aggregating columns** and **aggregating functions**.
    The aggregating columns are those whose values will be aggregated. The aggregating
    functions define how the aggregation takes place. Major aggregation functions
    include `sum`, `min`, `max`, `mean`, `count`, `variance`, `std`, and so on.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了引言中定义的分组列外，大多数聚合操作还有两个其他组件，**聚合列**和**聚合函数**。聚合列是那些其值将被聚合的列。聚合函数定义了聚合的方式。常见的聚合函数包括`sum`、`min`、`max`、`mean`、`count`、`variance`、`std`等。
- en: Getting ready
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we examine the flights dataset and perform the simplest possible
    aggregation involving only a single grouping column, a single aggregating column,
    and a single aggregating function. We will find the average arrival delay for
    each airline. Pandas has quite a few different syntaxes to produce an aggregation
    and this recipe covers them.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们查看航班数据集，并执行最简单的聚合操作，涉及一个分组列、一个聚合列和一个聚合函数。我们将计算每个航空公司的平均到达延迟。Pandas
    提供了多种不同的语法来执行聚合，本示例涵盖了这些语法。
- en: How to do it...
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Read in the flights dataset, and define the grouping columns (`AIRLINE`), aggregating
    columns (`ARR_DELAY`), and aggregating functions (`mean`):'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取航班数据集，并定义分组列（`AIRLINE`）、聚合列（`ARR_DELAY`）和聚合函数（`mean`）：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/50f4d53d-fcb0-4e12-9a69-0d39d5ea04d4.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50f4d53d-fcb0-4e12-9a69-0d39d5ea04d4.png)'
- en: 'Place the grouping column in the `groupby` method and then call the `agg` method
    with a dictionary pairing the aggregating column with its aggregating function:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分组列放入`groupby`方法中，然后使用一个字典，将聚合列与其聚合函数配对，接着调用`agg`方法：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/fec1f726-3a08-44eb-97aa-a134f638a217.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fec1f726-3a08-44eb-97aa-a134f638a217.png)'
- en: 'Alternatively, you may place the aggregating column in the indexing operator
    and then pass the aggregating function as a string to `agg`:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，你也可以将聚合列放入索引操作符中，然后将聚合函数作为字符串传递给`agg`：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The string names used in the previous step are a convenience pandas offers
    you to refer to a particular aggregation function. You can pass any aggregating
    function directly to the `agg` method such as the NumPy `mean` function. The output
    is the same as the previous step:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前一步中使用的字符串名称是 pandas 提供的方便方式，用于引用特定的聚合函数。你也可以将任何聚合函数直接传递给`agg`方法，例如 NumPy 的`mean`函数。输出结果与前一步相同：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It''s possible to skip the `agg` method altogether in this case and use the
    `mean` method directly. This output is also the same as step 3:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，完全可以跳过`agg`方法，直接使用`mean`方法。这个输出结果与步骤 3 相同：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The syntax for the `groupby` method is not as straightforward as other methods.
    Let's intercept the chain of methods in step 2 by storing the result of the `groupby`
    method as its own variable
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupby`方法的语法并不像其他方法那样直接。让我们通过将`groupby`方法的结果存储为一个变量，来中断步骤 2 中的方法链。'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A completely new intermediate object is first produced with its own distinct
    attributes and methods. No calculations take place at this stage. Pandas merely
    validates the grouping columns. This groupby object has an `agg` method to perform
    aggregations. One of the ways to use this method is to pass it a dictionary mapping
    the aggregating column to the aggregating function, as done in step 2.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先会生成一个全新的中间对象，具有自己独特的属性和方法。在此阶段没有任何计算发生。Pandas 仅验证分组列。这个 groupby 对象有一个`agg`方法用于执行聚合操作。使用这种方法的一种方式是将一个字典传递给它，将聚合列与聚合函数进行映射，正如步骤
    2 所示。
- en: There are several different flavors of syntax that produce a similar result,
    with step 3 showing an alternative. Instead of identifying the aggregating column
    in the dictionary, place it inside the indexing operator just as if you were selecting
    it as a column from a DataFrame. The function string name is then passed as a
    scalar to the `agg` method.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的语法可以实现相似的结果，步骤 3 展示了一种替代方法。无需在字典中指定聚合列，可以像从 DataFrame 中选择列一样将其放入索引操作符内。然后，将函数的字符串名称作为标量传递给`agg`方法。
- en: You may pass any aggregating function to the `agg` method. Pandas allows you
    to use the string names for simplicity but you may also explicitly call an aggregating
    function as done in step 4\. NumPy provides many functions that aggregate values.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将任何聚合函数传递给`agg`方法。Pandas 为了简化操作，允许使用字符串名称，但你也可以像步骤 4 中那样显式地调用聚合函数。NumPy 提供了许多聚合函数。
- en: 'Step 5 shows one last syntax flavor. When you are only applying a single aggregating
    function as in this example, you can often call it directly as a method on the
    groupby object itself without `agg`. Not all aggregation functions have a method
    equivalent but many basic ones do. The following is a list of several aggregating
    functions that may be passed as a string to `agg` or chained directly as a method
    to the groupby object:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步展示了最后一种语法风格。当你只应用单个聚合函数时，通常可以直接作为方法调用到groupby对象本身，而无需使用`agg`。并非所有聚合函数都有相应的方法，但许多基本的聚合函数是有的。以下是一些可以作为字符串传递给`agg`或直接作为方法链调用到groupby对象的聚合函数：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There's more...
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'If you do not use an aggregating function with `agg`, pandas raises an exception.
    For instance, let''s see what happens when we apply the square root function to
    each group:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在使用`agg`时没有使用聚合函数，pandas会抛出异常。例如，让我们看看当我们对每个组应用平方根函数时会发生什么：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: See also
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: Pandas official documentation on *Aggregation* ([http://bit.ly/2iuf1Nc](http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation))
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas官方文档关于*聚合*的说明 ([http://bit.ly/2iuf1Nc](http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation))
- en: Grouping and aggregating with multiple columns and functions
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多个列和函数进行分组和聚合
- en: 'It is possible to do grouping and aggregating with multiple columns. The syntax
    is only slightly different than it is for grouping and aggregating with a single
    column. As usual with any kind of grouping operation, it helps to identify the
    three components: the grouping columns, aggregating columns, and aggregating functions.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用多个列进行分组和聚合。语法与使用单个列进行分组和聚合仅有轻微不同。像任何分组操作一样，识别三个组成部分会有所帮助：分组列、聚合列和聚合函数。
- en: Getting ready
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'In this recipe, we showcase the flexibility of the `groupby` DataFrame method
    by answering the following queries:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们通过回答以下查询展示了`groupby` DataFrame方法的灵活性：
- en: Finding the number of cancelled flights for every airline per weekday
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找每家航空公司在每个工作日取消的航班数量
- en: Finding the number and percentage of cancelled and diverted flights for every
    airline per weekday
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找每家航空公司在每个工作日取消和转机航班的数量和百分比
- en: For each origin and destination, finding the total number of flights, the number
    and percentage of cancelled flights, and the average and variance of the airtime
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个出发地和目的地，查找航班的总数、取消航班的数量和百分比，以及空中时间的平均值和方差
- en: How to do it...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Read in the flights dataset, and answer the first query by defining the grouping
    columns (`AIRLINE, WEEKDAY`), the aggregating column (`CANCELLED`), and the aggregating
    function (`sum`):'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取航班数据集，并通过定义分组列（`AIRLINE, WEEKDAY`）、聚合列（`CANCELLED`）和聚合函数（`sum`）来回答第一个查询：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Answer the second query by using a list for each pair of grouping and aggregating
    columns. Also, use a list for the aggregating functions:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用每对分组列和聚合列的列表来回答第二个查询。同时，为聚合函数使用一个列表：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/5f4fb3bf-8d5c-43ec-adfa-712255f9496e.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f4fb3bf-8d5c-43ec-adfa-712255f9496e.png)'
- en: 'Answer the third query using a dictionary in the `agg` method to map specific
    aggregating columns to specific aggregating functions:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用字典在`agg`方法中映射特定的聚合列到特定的聚合函数来回答第三个查询：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/14de92a8-0578-44fd-93d4-149460e03eef.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14de92a8-0578-44fd-93d4-149460e03eef.png)'
- en: How it works...
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: To group by multiple columns as in step 1, we pass a list of the string names
    to the `groupby` method. Each unique combination of `AIRLINE` and `WEEKDAY` forms
    an independent group. Within each of these groups, the sum of the cancelled flights
    is found and then returned as a Series.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如第1步所示，要按多个列分组，我们将字符串列名列表传递给`groupby`方法。`AIRLINE`和`WEEKDAY`的每种唯一组合都形成一个独立的组。在这些组内，找到取消航班的总和并作为一个Series返回。
- en: Step 2, again groups by both `AIRLINE` and `WEEKDAY`, but this time aggregates
    two columns. It applies each of the two aggregation functions, `sum` and `mean`,
    to each column resulting in four returned columns per group.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步，再次按`AIRLINE`和`WEEKDAY`分组，但这次对两列进行聚合。它将`sum`和`mean`两个聚合函数应用于每一列，从而每个组返回四列结果。
- en: Step 3 goes even further, and uses a dictionary to map specific aggregating
    columns to different aggregating functions. Notice that the `size` aggregating
    function returns the total number of rows per group. This is different than the
    `count` aggregating function, which returns the number of non-missing values per
    group.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步更进一步，使用字典将特定的聚合列映射到不同的聚合函数。注意，`size`聚合函数返回每个组的总行数。这与`count`聚合函数不同，后者返回每个组的非缺失值数量。
- en: There's more...
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'There are a few main flavors of syntax that you will encounter when performing
    an aggregation. The following four blocks of pseudocode summarize the main ways
    you can perform an aggregation with the `groupby` method:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行聚合时，你将遇到几种主要的语法类型。以下四个伪代码块总结了使用`groupby`方法进行聚合的主要方式：
- en: 'Using `agg` with a dictionary is the most flexible and allows you to specify
    the aggregating function for each column:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`agg`与字典结合是最灵活的，它允许你为每一列指定聚合函数：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Using `agg` with a list of aggregating functions applies each of the functions
    to each of the aggregating columns:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`agg`与聚合函数列表结合，应用每个函数到每个聚合列：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Directly using a method following the aggregating columns instead of `agg`,
    applies just that method to each aggregating column. This way does not allow for
    multiple aggregating functions:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接在聚合列后使用方法，而不是`agg`，将该方法仅应用于每个聚合列。这种方法不允许多个聚合函数：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If you do not specify the aggregating columns, then the aggregating method
    will be applied to all the non-grouping columns:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有指定聚合列，那么聚合方法将应用于所有非分组列：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding four code blocks it is possible to substitute a string for
    any of the lists when grouping or aggregating by a single column.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前四个代码块中，当按单列进行分组或聚合时，任何列表都可以替换为字符串。
- en: Removing the MultiIndex after grouping
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分组后移除多重索引
- en: Inevitably, when using `groupby`, you will likely create a MultiIndex in the
    columns or rows or both. DataFrames with MultiIndexes are more difficult to navigate
    and occasionally have confusing column names as well.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 不可避免地，在使用`groupby`时，你很可能会在列或行，甚至两者中创建多重索引。具有多重索引的DataFrame更难以操作，偶尔还会有令人困惑的列名。
- en: Getting ready
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we perform an aggregation with the `groupby` method to create
    a DataFrame with a MultiIndex for the rows and columns and then manipulate it
    so that the index is a single level and the column names are descriptive.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用`groupby`方法进行聚合，创建一个具有行和列多重索引的DataFrame，然后对其进行操作，使得索引变为单层，并且列名具有描述性。
- en: How to do it...
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Read in the flights dataset; write a statement to find the total and average
    miles flown; and the maximum and minimum arrival delay for each airline for each
    weekday:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取航班数据集；写一个语句，找出每个航空公司在每个工作日的飞行总里程和平均里程，以及最大和最小的到达延误：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/000fc6a3-3bb0-4fd1-8c48-339fdbbc16ce.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/000fc6a3-3bb0-4fd1-8c48-339fdbbc16ce.png)'
- en: 'Both the rows and columns are labeled by a MultiIndex with two levels. Let''s
    squash it down to just a single level. To address the columns, we use the MultiIndex
    method, `get_level_values`. Let''s display the output of each level and then concatenate
    both levels before setting it as the new column values:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行和列都由两个层次的多重索引标记。我们将其压缩为单一层次。为了处理列，我们使用多重索引方法`get_level_values`。我们将显示每个层次的输出，然后将这两个层次合并，最后将其设为新的列名：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/776af29d-5e5c-4c22-8d65-e75693161af6.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/776af29d-5e5c-4c22-8d65-e75693161af6.png)'
- en: 'Return the row labels to a single level with `reset_index`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`reset_index`将行标签恢复为单一层次：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/d2a0fb0c-945c-478e-a7a7-8835ff925b68.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2a0fb0c-945c-478e-a7a7-8835ff925b68.png)'
- en: How it works...
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: When using the `agg` method to perform an aggregation on multiple columns, pandas
    creates an index object with two levels. The aggregating columns become the top
    level and the aggregating functions become the bottom level. Pandas displays MultiIndex
    levels differently than single-level columns. Except for the **innermost** levels,
    repeated index values do not get displayed on the screen. You can inspect the
    DataFrame from step 1 to verify this. For instance, the `DIST` column shows up
    only once but it refers to both of the first two columns.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `agg` 方法对多个列执行聚合操作时，pandas 会创建一个具有两级的索引对象。聚合列成为顶级，聚合函数成为底级。pandas 会以不同于单级列的方式显示
    MultiIndex 级别。除了 **最内层** 的级别外，重复的索引值不会显示在屏幕上。你可以查看步骤 1 中的 DataFrame 来验证这一点。例如，`DIST`
    列只会显示一次，但它指代的是前两列的内容。
- en: The innermost MultiIndex level is the one closest to the data. This would be
    the bottom-most column level and the right-most index level.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最内层的 MultiIndex 级别是最接近数据的级别。这通常是最底层的列级别和最右侧的索引级别。
- en: Step 2 defines new columns by first retrieving the underlying values of each
    of the levels with the MultiIndex method `get_level_values.` This method accepts
    an integer identifying the index level. They are numbered beginning with zero
    from the top/left. Indexes support vectorized operations, so we concatenate both
    levels together with a separating underscore. We assign these new values to the
    `columns` attribute.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2 通过首先使用 MultiIndex 方法 `get_level_values` 检索每个级别的基础值来定义新列。该方法接受一个整数，表示索引级别。索引级别从上/左开始编号，编号从零开始。索引支持向量化操作，因此我们将两个级别连接起来，并用下划线分隔。然后将这些新值分配给
    `columns` 属性。
- en: In step 3, we make both index levels as columns with `reset_index`. We could
    have concatenated the levels together like we did in step 2, but it makes more
    sense to keep them as separate columns.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 3 中，我们使用 `reset_index` 将两个索引级别转为列。我们本可以像步骤 2 中那样将这些级别连接在一起，但将它们保留为单独的列更有意义。
- en: There's more...
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'By default, at the end of a groupby operation, pandas puts all of the grouping
    columns in the index. The `as_index` parameter in the `groupby` method can be
    set to `False` to avoid this behavior. You can chain the `reset_index` method
    after grouping to get the same effect as done in step 3\. Let''s see an example
    of this by finding the average distance traveled per flight from each airline:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在 `groupby` 操作结束时，pandas 会将所有分组列放入索引中。`groupby` 方法中的 `as_index` 参数可以设置为
    `False`，以避免这种行为。你也可以在分组之后链式调用 `reset_index` 方法，达到与步骤 3 相同的效果。我们通过找出每个航空公司每次航班的平均行程距离来看看这个例子：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/a196e45e-4c13-4ed3-9efe-5d64e7dbc121.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a196e45e-4c13-4ed3-9efe-5d64e7dbc121.png)'
- en: Take a look at the order of the airlines in the previous result. By default,
    pandas sorts the grouping columns. The `sort` parameter exists within the `groupby`
    method and is defaulted to `True`. You may set it to `False` to keep the order
    of the grouping columns the same as how they are encountered in the dataset. You
    also get a small performance improvement by not sorting your data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下前面结果中航空公司的排序。默认情况下，pandas 会对分组列进行排序。`sort` 参数存在于 `groupby` 方法中，默认值为 `True`。你可以将其设置为
    `False`，保持分组列的顺序与数据集中出现的顺序相同。通过不对数据进行排序，你还可以获得小幅的性能提升。
- en: Customizing an aggregation function
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义聚合函数
- en: Pandas provides a number of the most common aggregation functions for you to
    use with the groupby object. At some point, you will need to write your own customized
    user-defined functions that don't exist in pandas or NumPy.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 提供了许多最常用的聚合函数，供你在 `groupby` 对象上使用。在某些情况下，你可能需要编写自己定制的用户定义函数，这些函数在 pandas
    或 NumPy 中并不存在。
- en: Getting ready
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we use the college dataset to calculate the mean and standard
    deviation of the undergraduate student population per state. We then use this
    information to find the maximum number of standard deviations from the mean that
    any single population value is per state.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用大学数据集来计算每个州本科生人口的均值和标准差。然后，我们使用这些信息找到每个州中某个人口值距离均值的标准差最大值。
- en: How to do it...
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Read in the college dataset, and find the mean and standard deviation of the
    undergraduate population by state:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取大学数据集，并按州计算本科生人口的均值和标准差：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/da651e10-a471-434f-a5e1-e5e0f7ae4307.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da651e10-a471-434f-a5e1-e5e0f7ae4307.png)'
- en: 'This output isn''t quite what we desire. We are not looking for the mean and
    standard deviations of the entire group but the maximum number of standard deviations
    away from the mean for any one institution. In order to calculate this, we need
    to subtract the mean undergraduate population by state from each institution''s
    undergraduate population and then divide by the standard deviation. This standardizes
    the undergraduate population for each group. We can then take the maximum of the
    absolute value of these scores to find the one that is farthest away from the
    mean. Pandas does not provide a function capable of doing this. Instead, we will
    need to create a custom function:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个输出结果并不是我们想要的。我们并不寻找整个组的均值和标准差，而是寻找任何一个机构与均值之间离得最远的标准差数值。为了计算这个值，我们需要从每个机构的本科生人口中减去按州划分的本科生人口均值，再除以标准差。这将标准化每个组的本科生人口。然后我们可以取这些分数的绝对值的最大值，找到与均值最远的那个值。Pandas并没有提供能够实现这一点的函数。因此，我们需要创建一个自定义函数：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After defining the function, pass it directly to the `agg` method to complete
    the aggregation:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义函数后，将其直接传递给`agg`方法以完成聚合：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How it works...
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: There does not exist a predefined pandas function to calculate the maximum number
    of standard deviations away from the mean. We were forced to construct a customized
    function in step 2\. Notice that this custom function `max_deviation` accepts
    a single parameter, `s`. Looking ahead at step 3, you will notice that the function
    name is placed inside the `agg` method without directly being called. Nowhere
    is the parameter `s` explicitly passed to `max_deviation`. Instead, pandas implicitly
    passes the `UGDS` column as a Series to `max_deviation`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 没有预定义的Pandas函数可以计算离均值最远的标准差数值。我们被迫在步骤2中构建一个自定义函数。注意，这个自定义函数`max_deviation`接受一个参数`s`。看一下步骤3，你会注意到函数名被放在`agg`方法内，而没有直接调用。`s`参数没有明确传递给`max_deviation`，相反，Pandas隐式地将`UGDS`列作为Series传递给`max_deviation`。
- en: The `max_deviation` function is called once for each group. As `s` is a Series,
    all normal Series methods are available. It subtracts the mean of that particular
    grouping from each of the values in the group before dividing by the standard
    deviation in a process called **standardization**.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_deviation`函数会对每个组调用一次。由于`s`是一个Series，因此所有常规的Series方法都可以使用。它会从该组中每个值减去该组的均值，然后除以标准差，这一过程称为**标准化**。'
- en: Standardization is a common statistical procedure to understand how greatly
    individual values vary from the mean. For a normal distribution, 99.7% of the
    data lies within three standard deviations of the mean.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化是一个常见的统计程序，用于了解个体值与均值之间的差异有多大。对于正态分布，99.7%的数据位于均值的三个标准差以内。
- en: As we are interested in absolute deviation from the mean, we take the absolute
    value from all the standardized scores and return the maximum. The `agg` method
    necessitates that a single scalar value must be returned from our custom function, or
    else an exception will be raised. Pandas defaults to using the sample standard
    deviation which is undefined for any groups with just a single value. For instance,
    the state abbreviation *AS* (American Samoa) has a missing value returned as it
    has only a single institution in the dataset.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们关注的是与均值的绝对偏差，因此我们从所有标准化分数中取绝对值，并返回最大值。`agg`方法要求我们的自定义函数必须返回一个标量值，否则会抛出异常。Pandas默认使用样本标准差，而对于只有一个值的组，标准差是未定义的。例如，州缩写*AS*（美属萨摩亚）返回缺失值，因为数据集中只有一个机构。
- en: There's more...
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'It is possible to apply our customized function to multiple aggregating columns.
    We simply add more column names to the indexing operator. The `max_deviation`
    function only works with numeric columns:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将自定义函数应用于多个聚合列。只需将更多的列名添加到索引操作符中。`max_deviation`函数仅适用于数值列：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/aba24502-a192-411c-bc6b-61e69a37abab.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aba24502-a192-411c-bc6b-61e69a37abab.png)'
- en: 'You can also use your customized aggregation function along with the prebuilt
    functions. The following does this and groups by state and religious affiliation:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以将自定义的聚合函数与预构建的函数一起使用。以下示例将这两者结合，并按州和宗教信仰进行分组：
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/e89efc68-2bf0-4338-9180-1c2b0efbbc56.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e89efc68-2bf0-4338-9180-1c2b0efbbc56.png)'
- en: 'Notice that pandas uses the name of the function as the name for the returned
    column. You can change the column name directly with the rename method or you
    can modify the special function attribute `__name__`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Pandas使用函数的名称作为返回列的名称。你可以通过`rename`方法直接更改列名，或者修改特殊函数属性`__name__`：
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/25d346b9-cf76-4eb2-a6db-d0fa8725a4d2.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25d346b9-cf76-4eb2-a6db-d0fa8725a4d2.png)'
- en: Customizing aggregating functions with *args and **kwargs
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`*args`和`**kwargs`自定义聚合函数
- en: 'When writing your own user-defined customized aggregation function, pandas
    implicitly passes it each of the aggregating columns one at a time as a Series.
    Occasionally, you will need to pass more arguments to your function than just
    the Series itself. To do so, you need to be aware of Python''s ability to pass
    an arbitrary number of arguments to functions. Let''s take a look at the signature
    of the groupby object''s `agg` method with help from the `inspect` module:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当编写自己的自定义聚合函数时，Pandas会隐式地将每个聚合列逐一作为Series传递给它。偶尔，你需要向函数传递的不仅仅是Series本身的参数。为此，你需要了解Python可以向函数传递任意数量的参数。让我们借助`inspect`模块来看一下`groupby`对象的`agg`方法签名：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The argument `*args` allow you to pass an arbitrary number of non-keyword arguments
    to your customized aggregation function. Similarly, `**kwargs` allows you to pass
    an arbitrary number of keyword arguments.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`*args`允许你将任意数量的非关键字参数传递给自定义的聚合函数。同样，`**kwargs`允许你将任意数量的关键字参数传递给函数。
- en: Getting ready
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备好了吗
- en: In this recipe, we build a customized function for the college dataset that
    finds the percentage of schools by state and religious affiliation that have an
    undergraduate population between two values.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们为大学数据集构建了一个自定义函数，计算按州和宗教信仰分类的学校百分比，这些学校的本科生人数在两个值之间。
- en: How to do it...
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Define a function that returns the percentage of schools with an undergraduate
    population between 1,000 and 3,000:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，返回本科生人数在1,000到3,000之间的学校百分比：
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Calculate this percentage grouping by state and religious affiliation:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按州和宗教信仰计算百分比分组：
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This function works fine but it doesn''t give the user any flexibility to choose
    the lower and upper bound. Let''s create a new function that allows the user to
    define these bounds:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个函数工作正常，但它没有给用户提供选择上下限的灵活性。我们来创建一个新函数，允许用户定义这些上下限：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Pass this new function to the `agg` method along with lower and upper bounds:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个新函数与上下限一起传递给`agg`方法：
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works...
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Step 1 creates a function that doesn't accept any extra arguments. The upper
    and lower bounds must be hardcoded into the function itself, which isn't very
    flexible. Step 2 shows the results of this aggregation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步创建了一个不接受任何额外参数的函数。上下限必须硬编码到函数中，这样不够灵活。第2步展示了此聚合的结果。
- en: We create a more flexible function in step 3 that allows users to define both
    the lower and upper bounds dynamically. Step 4 is where the magic of `*args` and
    `**kwargs` come into play. In this particular example, we pass two non-keyword
    arguments, 1,000 and 10,000, to the `agg` method. Pandas passes these two arguments
    respectively to the `low` and `high` parameters of `pct_between`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第3步创建了一个更灵活的函数，允许用户动态定义上下限。第4步是`*args`和`**kwargs`发挥作用的地方。在这个例子中，我们向`agg`方法传递了两个非关键字参数，1,000和10,000，Pandas将这两个参数分别传递给`pct_between`的`low`和`high`参数。
- en: 'There are a few ways we could achieve the same result in step 4\. We could
    have explicitly used the parameter names with the following command to produce
    the same result:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过几种方式在第4步中实现相同的结果。我们可以明确地使用参数名称，并通过以下命令产生相同的结果：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The order of the keyword arguments doesn''t matter as long as they come after
    the function name. Further still, we can mix non-keyword and keyword arguments
    as long as the keyword arguments come last:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 关键字参数的顺序并不重要，只要它们位于函数名后面。进一步说，我们可以将非关键字参数和关键字参数混合使用，只要关键字参数在最后：
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: For ease of understanding, it's probably best to include all the parameter names
    in the order that they are defined in the function signature.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于理解，最好按照函数签名中定义的顺序包括所有参数名称。
- en: Technically, when `agg` is called, all the non-keyword arguments get collected
    into a tuple named `args` and all the keyword arguments get collected into a dictionary
    named `kwargs`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，当调用`agg`时，所有非关键字参数都会被收集到一个名为`args`的元组中，所有关键字参数则会被收集到一个名为`kwargs`的字典中。
- en: There's more...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Unfortunately, pandas does not have a direct way to use these additional arguments
    when using multiple aggregation functions together. For example, if you wish to
    aggregate using the `pct_between` and `mean` functions, you will get the following
    exception:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，pandas没有直接的方法来在使用多个聚合函数时传递这些额外的参数。例如，如果你希望同时使用`pct_between`和`mean`函数进行聚合，你将遇到以下异常：
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Pandas is incapable of understanding that the extra arguments need to be passed
    to `pct_between`. In order to use our custom function with other built-in functions
    and even other custom functions, we can define a special type of nested function
    called a **closure**. We can use a generic closure to build all of our customized
    functions:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas无法理解需要将额外的参数传递给`pct_between`。为了将我们的自定义函数与其他内建函数甚至其他自定义函数一起使用，我们可以定义一种特殊类型的嵌套函数，称为**闭包**。我们可以使用通用的闭包来构建所有的定制化函数：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/0185d893-b8f3-47d9-847b-ea7838685247.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0185d893-b8f3-47d9-847b-ea7838685247.png)'
- en: The `make_agg_func` function acts as a factory to create customized aggregation
    functions. It accepts the customized aggregation function that you already built
    (`pct_between` in this case), a `name` argument, and an arbitrary number of extra
    arguments. It returns a function with the extra arguments already set. For instance,
    `my_agg1` is a specific customized aggregating function that finds the percentage
    of schools with an undergraduate population between one and three thousand. The
    extra arguments (`*args` and `**kwargs`) specify an exact set of parameters for
    your customized function (`pct_between` in this case). The `name` parameter is
    very important and must be unique each time `make_agg_func` is called. It will
    eventually be used to rename the aggregated column.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_agg_func`函数充当工厂，创建定制化的聚合函数。它接受你已经构建的定制化聚合函数（此例中为`pct_between`）、`name`参数和任意数量的额外参数。它返回一个已经设置了额外参数的函数。例如，`my_agg1`是一个特定的定制化聚合函数，用于查找本科生人数在一千到三千之间的学校的百分比。额外的参数（`*args`和`**kwargs`）为你的定制函数（此例中为`pct_between`）指定了一组精确的参数。`name`参数非常重要，每次调用`make_agg_func`时都必须是唯一的，最终它会用于重命名聚合后的列。'
- en: A closure is a function that contains a function inside of it (a nested function)
    and returns this nested function. This nested function must refer to variables
    in the scope of the outer function in order to be a closure. In this example,
    `make_agg_func` is the outer function and returns the nested function `wrapper`,
    which accesses the variables `func`, `args`, and `kwargs` from the outer function.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 闭包是一个包含内部函数（嵌套函数）并返回这个嵌套函数的函数。这个嵌套函数必须引用外部函数作用域中的变量，才能成为闭包。在这个例子中，`make_agg_func`是外部函数，并返回嵌套函数`wrapper`，后者访问外部函数中的`func`、`args`和`kwargs`变量。
- en: See also
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: '*Arbitrary Argument Lists* from the official Python documentation ([http://bit.ly/2vumbTE](https://docs.python.org/3/tutorial/controlflow.html#arbitrary-argument-lists))'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自官方Python文档的*任意参数列表*（[http://bit.ly/2vumbTE](https://docs.python.org/3/tutorial/controlflow.html#arbitrary-argument-lists)）
- en: A tutorial on *Python Closures* ([http://bit.ly/2xFdYga](http://www.geeksforgeeks.org/python-closures/))
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python闭包*的教程（[http://bit.ly/2xFdYga](http://www.geeksforgeeks.org/python-closures/)）'
- en: Examining the groupby object
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查groupby对象
- en: The immediate result from using the `groupby` method on a DataFrame will be
    a groupby object. Usually, we continue operating on this object to do aggregations
    or transformations without ever saving it to a variable. One of the primary purposes
    of examining this groupby object is to inspect individual groups.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`groupby`方法对DataFrame进行操作的即时结果将是一个groupby对象。通常，我们继续对该对象进行操作，进行聚合或转换，而不会将其保存到变量中。检查这个groupby对象的主要目的是检查单个分组。
- en: Getting ready
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we examine the groupby object itself by directly calling methods
    on it as well as iterating through each of its groups.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们通过直接调用`groupby`对象上的方法以及迭代其每个分组来检查该对象本身。
- en: How to do it...
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s get started by grouping the state and religious affiliation columns
    from the college dataset, saving the result to a variable and confirming its type:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从对大学数据集中的州和宗教归属列进行分组开始，将结果保存到一个变量中，并确认其类型：
- en: '[PRE35]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Use the `dir` function to discover all its available functionality:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`dir`函数来发现它所有可用的功能：
- en: '[PRE36]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Find the number of groups with the `ngroups` attribute:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ngroups`属性查找分组的数量：
- en: '[PRE37]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To find the uniquely identifying labels for each group, look in the `groups`
    attribute, which contains a dictionary of each unique group mapped to all the
    corresponding index labels of that group:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查找每个组的唯一标识标签，请查看`groups`属性，该属性包含一个字典，其中每个唯一组都映射到该组的所有对应索引标签：
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Retrieve a single group with the `get_group` method by passing it a tuple of
    an exact group label. For example, to get all the religiously affiliated schools
    in the state of Florida, do the following:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过传递一个精确的组标签元组，可以使用`get_group`方法检索单个组。例如，要获取佛罗里达州所有宗教附属学校，可以按以下步骤操作：
- en: '[PRE39]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/eacc01ed-8742-4326-b264-9d7ef642eb55.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eacc01ed-8742-4326-b264-9d7ef642eb55.png)'
- en: 'You may want to take a peek at each individual group. This is possible because
    groupby objects are iterable:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能想查看每个单独的组。因为groupby对象是可迭代的，所以这是可能的：
- en: '[PRE40]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](img/abafebc3-f01c-4644-8ff4-dd569a6c390b.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abafebc3-f01c-4644-8ff4-dd569a6c390b.png)'
- en: You can also call the head method on your groupby object to get the first rows
    of each group together in a single DataFrame.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以在groupby对象上调用head方法，将每个组的前几行放在一个单独的DataFrame中。
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](img/72cd2228-11a4-4845-90d3-401a079270ee.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72cd2228-11a4-4845-90d3-401a079270ee.png)'
- en: How it works...
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Step 1 formally creates our groupby object. It is useful to display all the
    public attributes and methods to reveal all the possible functionality as was
    done in step 2\. Each group is uniquely identified by a tuple containing a unique
    combination of the values in the grouping columns. Pandas allows you to select
    a specific group as a DataFrame with the `get_group` method shown in step 5.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步正式创建我们的groupby对象。显示所有公共属性和方法很有用，这样可以揭示所有可能的功能，如第二步所示。每个组通过包含分组列中值的唯一组合的元组来唯一标识。Pandas允许你使用第五步中展示的`get_group`方法选择特定的组作为DataFrame。
- en: It is rare that you will need to iterate through your groups and in general,
    you should avoid doing so if necessary, as it can be quite slow. Occasionally,
    you will have no other choice. When iterating through a groupby object, you are
    given a tuple containing the group name and the DataFrame without the grouping
    columns. This tuple is unpacked into the variables `name` and `group` in the for-loop
    in step 6.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你通常不需要遍历你的组，通常如果不是必要的话应该避免这样做，因为这样可能非常慢。偶尔，你可能别无选择。当遍历groupby对象时，你会得到一个元组，其中包含组名称和不包含分组列的DataFrame。这个元组在第六步的for循环中被解包到`name`和`group`变量中。
- en: One interesting thing you can do while iterating through your groups is to display
    a few of the rows from each group directly in the notebook. To do this, you can
    either use the print function or the `display` function from the `IPython.display`
    module. Using the `print` function results in DataFrames that are in plain text
    without any nice HTML formatting. Using the `display` function will produce DataFrames
    in their normal easy-to-read format.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在遍历组时，你可以做的有趣的事情之一是直接在笔记本中显示每个组的几行数据。为此，你可以使用`print`函数或来自`IPython.display`模块的`display`函数。使用`print`函数时，结果是没有任何漂亮HTML格式的纯文本DataFrame。而使用`display`函数则会以正常且易于阅读的格式显示DataFrame。
- en: There's more...
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'There are several useful methods that were not explored from the list in step
    2\. Take for instance the `nth` method, which, when given a list of integers,
    selects those specific rows from each group. For example, the following operation
    selects the first and last rows from each group:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步中的列表中有几个有用的方法没有被探索。以`nth`方法为例，当给定一个整数列表时，它会从每个组中选择这些特定的行。例如，以下操作选择每个组的第一行和最后一行：
- en: '[PRE42]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/35964ec0-13c2-4fc8-a4e1-2c49eeeb7735.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35964ec0-13c2-4fc8-a4e1-2c49eeeb7735.png)'
- en: See also
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: Official documentation of the `display` function from IPython ([http://bit.ly/2iAIogC](http://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display))
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`display`函数的官方文档来自IPython ([http://bit.ly/2iAIogC](http://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display))'
- en: Filtering for states with a minority majority
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 筛选具有少数派多数的州
- en: In [Chapter 10](3b938362-1f65-406c-ba9d-3bf735543ca8.xhtml), *Selecting Subsets
    of Data*, we marked every row as `True` or `False` before filtering out the `False`
    rows. In a similar fashion, it is possible to mark entire groups of data as either
    `True` or `False` before filtering out the `False` groups. To do this, we first
    form groups with the `groupby` method and then apply the `filter` method. The
    `filter` method accepts a function that must return either `True` or `False` to
    indicate whether a group is kept or not.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](3b938362-1f65-406c-ba9d-3bf735543ca8.xhtml)，*选择数据子集*，我们在过滤掉`False`行之前，将每一行标记为`True`或`False`。以类似的方式，也可以在过滤掉`False`组之前，将整个数据组标记为`True`或`False`。为此，我们首先使用`groupby`方法对数据进行分组，然后应用`filter`方法。`filter`方法接受一个必须返回`True`或`False`的函数，用来指示是否保留某个组。
- en: This `filter` method applied after a call to the `groupby` method is completely
    different than the DataFrame `filter` method.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用`groupby`方法后应用的这个`filter`方法与数据框（DataFrame）的`filter`方法是完全不同的。
- en: Getting ready
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we use the college dataset to find all the states that have
    more non-white undergraduate students than white. As this is a dataset from the
    US, whites form the majority and therefore, we are looking for states with a minority
    majority.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用大学数据集来查找所有少数族裔本科生数量超过白人数量的州。由于这是一个来自美国的数据集，而白人是多数群体，因此我们寻找的是少数族裔多数的州。
- en: How to do it...
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Read in the college dataset, group by state, and display the total number of
    groups. This should equal the number of unique states retrieved from the `nunique`
    Series method:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取大学数据集，按州分组，并显示组的总数。这应该等于从`nunique`系列方法中检索到的唯一州的数量：
- en: '[PRE43]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `grouped` variable has a `filter` method, which accepts a custom function
    that determines whether a group is kept or not. The custom function gets implicitly
    passed a DataFrame of the current group and is required to return a boolean. Let''s
    define a function that calculates the total percentage of minority students and
    returns `True` if this percentage is greater than a user-defined threshold:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`grouped`变量有一个`filter`方法，该方法接受一个自定义函数来决定是否保留某个组。自定义函数会隐式地接收到当前组的一个数据框，并需要返回一个布尔值。让我们定义一个函数，计算少数族裔学生的总百分比，如果这个百分比大于用户定义的阈值，则返回`True`：'
- en: '[PRE44]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Use the `filter` method passed with the `check_minority` function and a threshold
    of 50% to find all states that have a minority majority:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用传入`check_minority`函数和50%阈值的`filter`方法来查找所有有少数族裔多数的州：
- en: '[PRE45]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](img/3f8be7b3-9a86-4710-b751-fbe63a2b8cd5.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f8be7b3-9a86-4710-b751-fbe63a2b8cd5.png)'
- en: 'Just looking at the output may not be indicative of what actually happened.
    The DataFrame starts with state Arizona (AZ) and not Alaska (AK) so we can visually
    confirm that something changed. Let''s compare the `shape` of this filtered DataFrame
    with the original. Looking at the results, about 60% of the rows have been filtered,
    and only 20 states remain that have a minority majority:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅仅查看输出结果可能无法反映实际发生的情况。数据框（DataFrame）从亚利桑那州（Arizona，简称AZ）开始，而不是阿拉斯加州（Alaska，简称AK），所以我们可以直观地确认某些内容发生了变化。让我们将这个过滤后的数据框的`shape`与原始数据框进行对比。从结果来看，大约60%的行被过滤掉了，剩下的只有20个州有少数族裔多数：
- en: '[PRE46]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: How it works...
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: This recipe takes a look at the total population of all the institutions on
    a state-by-state basis. The goal is to keep all the rows from the states, as a
    whole, that have a minority majority. This requires us to group our data by state,
    which is done in step 1\. We find that there are 59 independent groups.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例通过逐州查看所有院校的总人口情况。目标是保留所有那些有少数族裔多数的州的所有行。这需要我们按照州对数据进行分组，这在步骤1中完成。我们发现共有59个独立组。
- en: The `filter` groupby method either keeps all the rows in a group or filters
    them out. It does not change the number of columns. The `filter` groupby method
    performs this gatekeeping through a user-defined function, for example, `check_minority`
    in this recipe. A very important aspect to filter is that it passes the entire
    DataFrame for that particular group to the user-defined function and returns a
    single boolean for each group.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter`的分组方法要么保留组中的所有行，要么将其过滤掉。它不会改变列的数量。`filter`的分组方法通过一个用户定义的函数执行这一筛选工作，例如本示例中的`check_minority`。一个非常重要的过滤方面是，它会将整个数据框传递给该组的用户定义函数，并返回每个组的一个布尔值。'
- en: Inside of the `check_minority` function, the percentage and the total number
    of non-white students for each institution are first calculated and then the total
    number of all students is found. Finally, the percentage of non-white students
    for the entire state is checked against the given threshold, which produces a
    boolean.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在`check_minority`函数内部，首先计算每个机构的少数族裔学生百分比和非白人学生的总数，然后计算所有学生的总数。最后，检查整个州的非白人学生百分比是否超过给定的阈值，结果为布尔值。
- en: The final result is a DataFrame with the same columns as the original but with
    the rows from the states that don't meet the threshold filtered out. As it is
    possible that the head of the filtered DataFrame is the same as the original,
    you need to do some inspection to ensure that the operation completed successfully.
    We verify this by checking the number of rows and number of unique states.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是一个与原始数据框（DataFrame）具有相同列数的数据框，但其中的行已过滤掉不符合阈值的州。由于过滤后的数据框头部可能与原始数据框相同，因此需要进行检查，以确保操作成功完成。我们通过检查行数和独特州的数量来验证这一点。
- en: There's more...
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Our function, `check_minority`, is flexible and accepts a parameter to lower
    or raise the percentage of minority threshold. Let''s check the shape and number
    of unique states for a couple of other thresholds:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的函数`check_minority`是灵活的，接受一个参数以降低或提高少数群体的阈值百分比。让我们检查一下其他几个阈值下的数据框形状和独特州的数量：
- en: '[PRE47]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: See also
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: Pandas official documentation on *Filtration* ([http://bit.ly/2xGUoA7](https://pandas.pydata.org/pandas-docs/stable/groupby.html#filtration))
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas官方文档关于*过滤*（[http://bit.ly/2xGUoA7](https://pandas.pydata.org/pandas-docs/stable/groupby.html#filtration)）
- en: Transforming through a weight loss bet
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过体重减轻比赛进行转变
- en: One method to increase motivation to lose weight is to make a bet with someone
    else. The scenario in this recipe will track weight loss from two individuals
    over the course of a four-month period and determine a winner.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 增加减肥动力的一种方法是和别人打赌。在这个配方中，我们将追踪两个人在四个月期间的体重减轻情况，并确定谁是最终的赢家。
- en: Getting ready
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: In this recipe, we use simulated data from two individuals to track the percentage
    of weight loss over the course of four months. At the end of each month, a winner
    will be declared based on the individual who lost the highest percentage of body
    weight for that month. To track weight loss, we group our data by month and person,
    then call the `transform` method to find the percentage weight loss at each week
    from the start of the month.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们使用了两个人的模拟数据来追踪他们在四个月内的体重减轻百分比。在每个月结束时，根据该月体重减轻百分比最多的人来宣布赢家。为了追踪体重减轻情况，我们按月和人物对数据进行分组，然后调用`transform`方法来找出每周体重减轻的百分比，从而得出每月开始时的变化情况。
- en: How to do it...
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Read in the raw weight_loss dataset, and examine the first month of data from
    the two people, `Amy` and `Bob`. There are a total of four weigh-ins per month:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取原始的体重减轻数据集，并检查`Amy`和`Bob`两个人的第一个月数据。每个月共有四次体重测量：
- en: '[PRE48]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/5efff218-dbf7-48a9-b831-f6052ddb4fef.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5efff218-dbf7-48a9-b831-f6052ddb4fef.png)'
- en: 'To determine the winner for each month, we only need to compare weight loss
    from the first week to the last week of each month. But, if we wanted to have
    weekly updates, we can also calculate weight loss from the current week to the
    first week of each month.  Let''s create a function that is capable of providing
    weekly updates:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确定每个月的赢家，我们只需比较每个月从第一周到最后一周的体重减轻情况。但如果我们想要每周更新，也可以计算每个月从当前周到第一周的体重减轻情况。让我们创建一个能够提供每周更新的函数：
- en: '[PRE49]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Let's test out this function for Bob during the month of January.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们测试一下Bob在1月期间使用这个函数的结果。
- en: '[PRE50]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: You should ignore the index values in the last output. 0, 2, 4 and 6 simply
    refer to the original row labels of the DataFrame and have no relation to the
    week.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该忽略最后输出中的索引值。0、2、4和6仅仅是原始数据框的行标签，与周次无关。
- en: 'After the first week, Bob lost 1% of his body weight. He continued losing weight
    during the second week but made no progress during the last week.  We can apply
    this function to every single combination of person and week to get the weight
    loss per week in relation to the first week of the month. To do this, we need
    to group our data by `Name` and `Month` , and then use the `transform` method
    to apply this custom function:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一周后，Bob减轻了1%的体重。在第二周，他继续减肥，但在最后一周没有进展。我们可以将此函数应用于每个人和每周的所有组合，以获取每周相对于月初的体重减轻情况。为此，我们需要按`Name`和`Month`对数据进行分组，然后使用`transform`方法应用此自定义函数：
- en: '[PRE51]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `transform` method must return an object with the same number of rows as
    the calling DataFrame. Let''s append this result to our original DataFrame as
    a new column. To help shorten the output, we will select Bob''s first two months
    of data:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`transform`方法必须返回与调用的DataFrame具有相同数量行的对象。让我们将这个结果作为新列附加到原始DataFrame中。为了缩短输出，我们将选择Bob的前两个月的数据：'
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/998f0234-1f22-4853-8f8c-3c1576a352ba.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/998f0234-1f22-4853-8f8c-3c1576a352ba.png)'
- en: 'Notice that the percentage weight loss resets after the new month. With this
    new column, we can manually determine a winner but let''s see if we can find a
    way to do this automatically. As the only week that matters is the last week,
    let''s select week 4:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，百分比体重减轻在每个月开始时会重置。通过这个新列，我们可以手动确定每个月的获胜者，但让我们看看能否找到自动执行此操作的方法。由于唯一重要的是最后一周的数据，我们来选择第4周：
- en: '[PRE53]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![](img/5024a731-2dbe-4fc1-9e27-d76065bb7859.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5024a731-2dbe-4fc1-9e27-d76065bb7859.png)'
- en: 'This narrows down the weeks but still doesn''t automatically find out the winner
    of each month. Let''s reshape this data with the `pivot` method so that Bob''s
    and Amy''s percent weight loss is side-by-side for each month:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将缩小数据范围，但仍无法自动找出每个月的获胜者。让我们使用`pivot`方法重新整理这些数据，这样Bob和Amy的百分比体重减轻就能并排显示在每个月：
- en: '[PRE54]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![](img/efdc2ed3-5b3b-4a62-b399-7da0b37d2cec.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/efdc2ed3-5b3b-4a62-b399-7da0b37d2cec.png)'
- en: 'This output makes it clearer who has won each month, but we can still go a
    couple steps farther. NumPy has a vectorized if-then-else function called `where`,
    which can map a Series or array of booleans to other values. Let''s create a column
    for the name of the winner and highlight the winning percentage for each month:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个输出让每个月的获胜者更加清晰，但我们仍然可以进一步优化。NumPy有一个矢量化的if-then-else函数，叫做`where`，它可以将布尔值的Series或数组映射到其他值。让我们创建一个列，记录获胜者的名字，并突出显示每个月的获胜百分比：
- en: '[PRE55]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](img/452143c1-f8f8-4aa6-a8e2-98749075c3b3.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/452143c1-f8f8-4aa6-a8e2-98749075c3b3.png)'
- en: 'Use the `value_counts` method to return the final score as the number of months
    won:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`value_counts`方法返回最终得分，即获胜的月份数量：
- en: '[PRE56]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: How it works...
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Throughout this recipe, the `query` method is used to filter data instead of
    boolean indexing. Refer to the *Improving readability of Boolean indexing with
    the query method* recipe from [Chapter 11](9f721370-ae04-4425-aab9-d525335b96b3.xhtml),
    *Boolean Indexing*, for more information*.*
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，使用`query`方法来过滤数据，而不是使用布尔索引。有关更多信息，请参考[第11章](9f721370-ae04-4425-aab9-d525335b96b3.xhtml)中的*通过query方法提高布尔索引的可读性*一节，*布尔索引*。
- en: Our goal is to find the percentage weight loss for each month for each person.
    One way to accomplish this task is to calculate each week's weight loss relative
    to the start of each month. This specific task is perfectly suited to the `transform`
    groupby method. The `transform` method accepts a function as its one required
    parameter. This function gets implicitly passed each non-grouping column (or only
    the columns specified in the indexing operator as was done in this recipe with
    `Weight`). It must return a sequence of values the same length as the passed group
    or else an exception will be raised. In essence, all values from the original
    DataFrame are transforming. No aggregation or filtration takes place.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找出每个人每个月的百分比体重减轻。一种实现此任务的方法是计算每周的体重减轻相对于每个月开始时的情况。这个任务非常适合使用`transform`的groupby方法。`transform`方法接受一个函数作为其唯一的必需参数。这个函数会隐式传递每个非分组列（或者只传递在索引操作符中指定的列，如本节中对`Weight`的处理）。它必须返回与传入分组相同长度的值序列，否则会引发异常。从本质上讲，原始DataFrame中的所有值都会被转换。没有进行聚合或筛选操作。
- en: Step 2 creates a function that subtracts the first value of the passed Series
    from all of its values and then divides this result by the first value. This calculates
    the percent loss (or gain) relative to the first value. In step 3 we test this
    function on one person during one month.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2 创建了一个函数，它从传入的 Series 的所有值中减去第一个值，然后将结果除以第一个值。这计算了相对于第一个值的百分比损失（或增益）。在步骤
    3 中，我们在一个人和一个月的数据上测试了这个函数。
- en: In step 4, we use this function in the same manner over every combination of
    person and week. In some literal sense, we are *transforming* the `Weight` column
    into the percentage of weight lost for the current week. The first month of data
    is outputted for each person. Pandas returns the new data as a Series. This Series
    isn't all that useful by itself and makes more sense appended to the original
    DataFrame as a new column. We complete this operation in step 5.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 4 中，我们以相同的方式在每个人和每周的所有组合上使用这个函数。从字面意义上讲，我们正在*将* `Weight` 列转换为当前周的体重减轻百分比。每个人的第一个月数据会被输出。Pandas
    将新数据作为一个 Series 返回。这个 Series 本身并不是特别有用，最好是作为一个新列追加到原始的 DataFrame 中。我们在步骤 5 中完成这个操作。
- en: To determine the winner, only week 4 of each month is necessary. We could stop
    here and manually determine the winner but pandas supplies us functionality to
    automate this. The `pivot` function in step 7 reshapes our dataset by pivoting
    the unique values of one column into new column names. The `index` parameter is
    used for the column that you do not want to pivot. The column passed to the `values`
    parameter gets tiled over each unique combination of the columns in the `index`
    and `columns` parameters.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定赢家，只需要每个月的第 4 周的数据。我们可以在这里停下来手动确定赢家，但 pandas 为我们提供了自动化的功能。步骤 7 中的 `pivot`
    函数通过将一个列的唯一值转换为新的列名来重新塑形数据集。`index` 参数用于指定不需要透视的列。传递给 `values` 参数的列将在 `index`
    和 `columns` 参数的每个唯一组合上铺开。
- en: The `pivot` method only works if there is just a single occurrence of each unique
    combination of the columns in the `index` and `columns` parameters. If there is
    more than one unique combination, an exception will be raised. You can use the
    `pivot_table` method in that situation which allows you to aggregate multiple
    values together.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`pivot` 方法仅在 `index` 和 `columns` 参数中的每个唯一组合只出现一次时才有效。如果有多个唯一组合，将会抛出异常。在这种情况下，你可以使用
    `pivot_table` 方法，它允许你聚合多个值。'
- en: After pivoting, we utilize the highly effective and fast NumPy `where` function,
    whose first argument is a condition that produces a Series of booleans. `True`
    values get mapped to *Amy* and `False` values get mapped to *Bob.* We highlight
    the winner of each month and tally the final score with the `value_counts` method.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在透视之后，我们利用高效且快速的 NumPy `where` 函数，其第一个参数是一个条件，返回一个布尔值的 Series。`True` 值会映射到*Amy*，`False`
    值会映射到*Bob*。我们标出每个月的赢家，并用 `value_counts` 方法统计最终得分。
- en: There's more...
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'Take a look at the DataFrame output from step 7\. Did you notice that the months
    are in alphabetical and not chronological order? Pandas unfortunately, in this
    case at least, orders the months for us alphabetically. We can solve this issue
    by changing the data type of `Month` to a categorical variable. Categorical variables
    map all the values of each column to an integer. We can choose this mapping to
    be the normal chronological order for the months. Pandas uses this underlying
    integer mapping during the `pivot` method to order the months chronologically:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下步骤 7 输出的 DataFrame。你是否注意到月份的顺序是按字母顺序排列的，而不是按时间顺序？遗憾的是，至少在这种情况下，Pandas 是按字母顺序排列月份的。我们可以通过将
    `Month` 列的数据类型更改为分类变量来解决这个问题。分类变量会将每个列中的所有值映射到整数。我们可以选择这种映射，使月份按正常的时间顺序排列。Pandas
    在 `pivot` 方法中使用这个整数映射来按时间顺序排列月份：
- en: '[PRE57]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![](img/0c4324b4-8516-4239-bba6-9cd5b45d1893.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c4324b4-8516-4239-bba6-9cd5b45d1893.png)'
- en: To convert the `Month` column, use the `Categorical` constructor. Pass it the
    original column as a Series and a unique sequence of all the categories in the
    desired order to the `categories` parameter. As the `Month` column is already
    in chronological order, we can simply use the `unique` method, which preserves
    order to get the array that we desire. In general, to sort columns of object data
    type by something other than alphabetical, convert them to categorical.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要转换`Month`列，使用`Categorical`构造函数。将原始列作为Series传递给它，并将所需顺序中的所有类别的唯一序列传递给`categories`参数。由于`Month`列已经按时间顺序排列，我们可以直接使用`unique`方法，该方法保留顺序，从而获取所需的数组。一般来说，要按字母顺序以外的方式对对象数据类型的列进行排序，可以将其转换为分类数据类型。
- en: See also
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: Pandas official documentation on `groupby` *Transformation* ([http://bit.ly/2vBkpA7](http://pandas.pydata.org/pandas-docs/stable/groupby.html#transformation))
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas官方文档中的`groupby` *转换*（[http://bit.ly/2vBkpA7](http://pandas.pydata.org/pandas-docs/stable/groupby.html#transformation)）
- en: NumPy official documentation on the `where` function ([http://bit.ly/2weT21l](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html))
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy官方文档中的`where`函数（[http://bit.ly/2weT21l](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html)）
- en: Calculating weighted mean SAT scores per state with apply
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用apply计算按州加权的SAT数学成绩平均值
- en: The groupby object has four methods that accept a function (or functions) to perform
    a calculation on each group. These four methods are `agg`, `filter`, `transform`,
    and `apply`. Each of the first three of these methods has a very specific output
    that the function must return. `agg` must return a scalar value, `filter` must
    return a boolean, and `transform` must return a Series with the same length as
    the passed group. The `apply` method, however, may return a scalar value, a Series,
    or even a DataFrame of any shape, therefore making it very flexible. It is also
    called only once per group, which contrasts with `transform` and `agg` that get
    called once for each non-grouping column. The `apply` method's ability to return
    a single object when operating on multiple columns at the same time makes the
    calculation in this recipe possible.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupby`对象有四个方法可以接受一个函数（或多个函数），对每个组进行计算。这四个方法分别是`agg`、`filter`、`transform`和`apply`。这些方法中的前三个都有非常具体的输出要求，函数必须返回特定的值。`agg`必须返回一个标量值，`filter`必须返回一个布尔值，而`transform`必须返回一个与传入组长度相同的Series。然而，`apply`方法可以返回标量值、Series，甚至是任意形状的DataFrame，因此非常灵活。它每次只调用一次每个组，这与`transform`和`agg`每次都调用每个非分组列不同。`apply`方法在同时操作多个列时能够返回单一对象，这使得本例中的计算成为可能。'
- en: Getting ready
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we calculate the weighted average of both the math and verbal
    SAT scores per state from the college dataset. We weight the scores by the population
    of undergraduate students per school.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们计算了每个州的数学和语言SAT成绩的加权平均值，数据来源于大学数据集。我们根据每所学校的本科生人数对成绩进行加权。
- en: How to do it...
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Read in the college dataset, and drop any rows that have missing values in
    either the `UGDS`, `SATMTMID`, or `SATVRMID` columns. We must have non-missing
    values for each of these three columns:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取大学数据集，并删除任何在`UGDS`、`SATMTMID`或`SATVRMID`列中有缺失值的行。我们必须确保这三列中每一列都没有缺失值：
- en: '[PRE58]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The vast majority of institutions do not have data for our three required columns,
    but this is still more than enough data to continue. Next, create a user-defined
    function to calculate the weighted average of just the SAT math scores:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绝大多数学校没有我们要求的三列数据，但这些数据仍然足够用来继续。接下来，创建一个用户定义的函数来计算SAT数学成绩的加权平均值：
- en: '[PRE59]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Group by state and pass this function to the `apply` method:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按州分组，并将此函数传递给`apply`方法：
- en: '[PRE60]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We successfully returned a scalar value for each group. Let''s take a small
    detour and see what the outcome would have been by passing the same function to
    the `agg` method:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们成功地为每个组返回了一个标量值。让我们稍作绕道，看看如果将相同的函数传递给`agg`方法，结果会是什么样的：
- en: '[PRE61]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](img/d02a23f5-aa1b-4a02-b5e6-5449ba33f6fe.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d02a23f5-aa1b-4a02-b5e6-5449ba33f6fe.png)'
- en: 'The `weighted_math_average` function gets applied to each non-aggregating column
    in the DataFrame. If you try and limit the columns to just `SATMTMID`, you will
    get an error as you won''t have access to `UGDS`. So, the best way to complete
    operations that act on multiple columns is with `apply`:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`weighted_math_average`函数应用于DataFrame中的每个非聚合列。如果你尝试将列限制为仅`SATMTMID`，你会遇到错误，因为你无法访问`UGDS`。因此，完成对多个列进行操作的最佳方法是使用`apply`：'
- en: '[PRE62]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'A nice feature of `apply` is that you can create multiple new columns by returning
    a Series. The index of this returned Series will be the new column names. Let''s
    modify our function to calculate the weighted and arithmetic average for both
    SAT scores along with the count of the number of institutions from each group.
    We return these five values in a Series:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`apply`的一个好功能是，你可以通过返回一个Series来创建多个新列。这个返回的Series的索引将成为新列的名称。让我们修改我们的函数，以计算两个SAT分数的加权平均值和算术平均值，并统计每个组中院校的数量。我们将这五个值以Series的形式返回：'
- en: '[PRE63]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '![](img/3267d64e-37a3-4e90-9c9f-2aabdf1b0d26.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3267d64e-37a3-4e90-9c9f-2aabdf1b0d26.png)'
- en: How it works...
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In order for this recipe to complete properly, we need to first filter for institutions
    that do not have missing values for `UGDS`, `SATMTMID`, and `SATVRMID`. By default,
    the `dropna` method drops rows that have one or more missing values. We must use
    the `subset` parameter to limit the columns it looks at for missing values.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这个操作顺利完成，我们首先需要筛选出没有`UGDS`、`SATMTMID`和`SATVRMID`缺失值的院校。默认情况下，`dropna`方法会删除包含一个或多个缺失值的行。我们必须使用`subset`参数，限制它检查缺失值的列。
- en: In step 2, we define a function that calculates the weighted average for just
    the `SATMTMID` column. The weighted average differs from an arithmetic mean in
    that each value is multiplied by some weight. This quantity is then summed and
    divided by the sum of the weights. In this case, our weight is the undergraduate
    student population.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步中，我们定义了一个函数，用来计算`SATMTMID`列的加权平均值。加权平均与算术平均的不同之处在于，每个值会乘以一个权重。然后将这些加权值相加，并除以权重的总和。在这个例子中，我们的权重是本科生人数。
- en: In step 3, we pass this function to the `apply` method. Our function `weighted_math_average`
    gets passed a DataFrame of all the original columns for each group. It returns
    a single scalar value, the weighted average of `SATMTMID`. At this point, you
    might think that this calculation is possible using the `agg` method. Directly
    replacing `apply` with `agg` does not work as `agg` returns a value for each of
    its aggregating columns.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们将这个函数传递给`apply`方法。我们的函数`weighted_math_average`会接收每个组的所有原始列的DataFrame，并返回一个标量值，即`SATMTMID`的加权平均值。此时，你可能会认为可以使用`agg`方法来进行此计算。直接用`agg`替换`apply`是行不通的，因为`agg`会为每个聚合列返回一个值。
- en: It actually is possible to use `agg` indirectly by precomputing the multiplication
    of `UGDS` and `SATMTMID`.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，可以通过先计算`UGDS`和`SATMTMID`的乘积，间接使用`agg`方法。
- en: Step 6 really shows the versatility of `apply`. We build a new function that
    calculates the weighted and arithmetic average of both SAT columns as well as
    the number of rows for each group. In order for `apply` to create multiple columns,
    you must return a Series. The index values are used as column names in the resulting
    DataFrame. You can return as many values as you want with this method.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步真正展示了`apply`的多功能性。我们构建了一个新函数，计算两个SAT列的加权平均值、算术平均值以及每个组的行数。为了让`apply`创建多个列，你必须返回一个Series。索引值将作为结果DataFrame中的列名。你可以用这种方法返回任意数量的值。
- en: Notice that the `OrderedDict` class was imported from the `collections` module,
    which is part of the standard library. This ordered dictionary is used to store
    the data. A normal Python dictionary could not have been used to store the data
    since it does not preserve insertion order.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`OrderedDict`类是从`collections`模块导入的，这个模块是标准库的一部分。这个有序字典用来存储数据。普通的Python字典不能用来存储这些数据，因为它不能保持插入顺序。
- en: The constructor, `pd.Series`, does have an index parameter that you can use
    to specify order but using an `OrderedDict` is cleaner.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 构造器`pd.Series`确实有一个`index`参数，你可以用它来指定顺序，但使用`OrderedDict`会更简洁。
- en: There's more...
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'In this recipe, we returned a single row as a Series for each group. It''s
    possible to return any number of rows and columns for each group by returning
    a DataFrame. In addition to finding just the arithmetic and weighted means, let''s
    also find the geometric and harmonic means of both SAT columns and return the
    results as a DataFrame with rows as the name of the type of mean and columns as
    the SAT type. To ease the burden on us, we use the NumPy function `average` to
    compute the weighted average and the SciPy functions `gmean` and `hmean` for geometric
    and harmonic means:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们返回了每个组的单行数据作为一个Series。通过返回一个DataFrame，可以为每个组返回任意数量的行和列。除了计算算术和加权平均数之外，我们还要计算两个SAT列的几何平均数和调和平均数，并将结果返回为一个DataFrame，其中行是平均数的类型名称，列是SAT类型。为了减轻我们的负担，我们使用了NumPy的`average`函数来计算加权平均数，使用SciPy的`gmean`和`hmean`函数来计算几何平均数和调和平均数：
- en: '[PRE64]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![](img/e4bf0560-11c6-4599-b1d5-e8f47cd2a215.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4bf0560-11c6-4599-b1d5-e8f47cd2a215.png)'
- en: See also
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Pandas official documentation of the `apply` groupby method ([http://bit.ly/2wmG9ki](http://pandas.pydata.org/pandas-docs/stable/groupby.html#flexible-apply))
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas官方文档的`apply` groupby方法（[http://bit.ly/2wmG9ki](http://pandas.pydata.org/pandas-docs/stable/groupby.html#flexible-apply)）
- en: Python official documentation of the `OrderedDict` class ([http://bit.ly/2xwtUCa](https://docs.python.org/3/library/collections.html#collections.OrderedDict))
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python官方文档的`OrderedDict`类（[http://bit.ly/2xwtUCa](https://docs.python.org/3/library/collections.html#collections.OrderedDict)）
- en: SciPy official documentation of its stats module ([http://bit.ly/2wHtQ4L](https://docs.scipy.org/doc/scipy/reference/stats.html))
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SciPy官方文档的统计模块（[http://bit.ly/2wHtQ4L](https://docs.scipy.org/doc/scipy/reference/stats.html)）
- en: Grouping by continuous variables
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按连续变量分组
- en: When grouping in pandas, you typically use columns with discrete repeating values.
    If there are no repeated values, then grouping would be pointless as there would
    only be one row per group. Continuous numeric columns typically have few repeated
    values and are generally not used to form groups. However, if we can transform
    columns with continuous values into a discrete column by placing each value into
    a bin, rounding them, or using some other mapping, then grouping with them makes
    sense.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中进行分组时，通常使用具有离散重复值的列。如果没有重复值，那么分组就没有意义，因为每个组只有一行。连续的数字列通常重复值较少，通常不会用来分组。然而，如果我们能通过将每个值放入一个区间、四舍五入或使用其他映射，将连续值列转换为离散列，那么使用它们进行分组是有意义的。
- en: Getting ready
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we explore the flights dataset to discover the distribution
    of airlines for different travel distances. This allows us, for example, to find
    the airline that makes the most flights between 500 and 1,000 miles. To accomplish
    this, we use the pandas `cut` function to discretize the distance of each flight
    flown.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探索了航班数据集，以发现不同旅行距离下航空公司的分布。例如，这使我们能够找到在500到1000英里之间飞行次数最多的航空公司。为此，我们使用pandas的`cut`函数来离散化每个航班的距离。
- en: How to do it...
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Read in the flights dataset, and output the first five rows:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取航班数据集，并输出前五行：
- en: '[PRE65]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '![](img/ed49e291-ded0-4c48-9451-d0f0cf9c47f8.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed49e291-ded0-4c48-9451-d0f0cf9c47f8.png)'
- en: 'If we want to find the distribution of airlines over a range of distances,
    we need to place the values of the `DIST` column into discrete bins. Let''s use
    the pandas `cut` function to split the data into five bins:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们想要找到在不同距离范围内的航空公司分布，我们需要将`DIST`列的值放入离散的区间中。让我们使用pandas的`cut`函数将数据划分为五个区间：
- en: '[PRE66]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'An ordered categorical Series is created. To help get an idea of what happened,
    let''s count the values of each category:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了一个有序分类Series。为了帮助理解发生了什么，让我们统计一下每个类别的值：
- en: '[PRE67]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The `cuts` Series can now be used to form groups. Pandas allows you to form
    groups in any way you wish. Pass the `cuts` Series to the `groupby` method and
    then call the `value_counts` method on the `AIRLINE` column to find the distribution
    for each distance group. Notice that SkyWest (*OO*) makes up 33% of flights less
    than 200 miles but only 16% of those between 200 and 500 miles:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在可以使用`cuts` Series来形成组。Pandas允许您以任何方式形成组。将`cuts` Series传递给`groupby`方法，然后调用`value_counts`方法来查找每个距离组的分布。注意，SkyWest
    (*OO*)在200英里以下的航班中占比33%，但在200到500英里的航班中仅占16%：
- en: '[PRE68]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: How it works...
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 2, the `cut` function places each value of the `DIST` column into one
    of five bins. The bins are created by a sequence of six numbers defining the edges.
    You always need one more edge than the number of bins. You can pass the `bins`
    parameter an integer, which automatically creates that number of equal-width bins.
    Negative infinity and positive infinity objects are available in NumPy and ensure
    that all values get placed in a bin. If you have values that are outside of the
    bin edges, they will be made missing and not be placed in a bin.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，`cut`函数将`DIST`列的每个值放入五个箱子之一。箱子的边界是通过一组六个数字定义的。你总是需要比箱子数量多一个边界。你可以将`bins`参数设置为一个整数，自动创建该数量的等宽箱子。负无穷和正无穷对象在NumPy中可用，确保所有值都会被放入箱子中。如果有值超出了箱子的边界，它们将被标记为缺失并不会放入箱子。
- en: The `cuts` variable is now a Series of five ordered categories. It has all the
    normal Series methods and in step 3, the `value_counts` method is used to get
    a sense of its distribution.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuts`变量现在是一个包含五个有序类别的Series。它拥有所有常规Series方法，并且在步骤3中，使用`value_counts`方法来了解其分布情况。'
- en: Very interestingly, pandas allows you to pass the `groupby` method any object.
    This means that you are able to form groups from something completely unrelated
    to the current DataFrame. Here, we group by the values in the `cuts` variable.
    For each grouping, we find the percentage of flights per airline with `value_counts` by
    setting `normalize` to `True`.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 非常有趣的是，pandas允许你将任何对象传递给`groupby`方法。这意味着你可以从与当前DataFrame完全无关的东西中创建分组。在这里，我们根据`cuts`变量中的值进行分组。对于每个分组，我们通过将`normalize`设置为`True`来使用`value_counts`找出每个航空公司的航班百分比。
- en: Some interesting insights can be drawn from this result. Looking at the full
    result, SkyWest is the leading airline for under 200 miles but has no flights
    over 2,000 miles. In contrast, American Airlines has the fifth highest total for
    flights under 200 miles but has by far the most flights between 1,000 and 2,000
    miles.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果中可以得出一些有趣的见解。查看完整结果，SkyWest是200英里以下航程的领先航空公司，但没有超过2,000英里的航班。相比之下，美国航空在200英里以下的航班数量排名第五，但在1,000到2,000英里之间的航班数量遥遥领先。
- en: There's more...
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can find more results when grouping by the `cuts` variable. For instance,
    we can find the 25th, 50th, and 75th percentile airtime for each distance grouping.
    As airtime is in minutes, we can divide by 60 to get hours:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对`cuts`变量进行分组，获得更多的结果。例如，我们可以找到每个距离分组的第25、第50和第75百分位的飞行时间。由于飞行时间以分钟为单位，我们可以除以60来得到小时：
- en: '[PRE69]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We can use this information to create informative string labels when using
    the `cut` function. These labels replace the interval notation. We can also chain
    the `unstack` method which transposes the inner index level to column names:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些信息来创建信息性字符串标签，当使用`cut`函数时，这些标签将替代区间表示法。我们还可以链式调用`unstack`方法，将内部索引级别转置为列名：
- en: '[PRE70]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '![](img/a44ef2f0-739b-4970-abba-c3e147aa0031.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a44ef2f0-739b-4970-abba-c3e147aa0031.png)'
- en: See also
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: Pandas official documentation on the `cut` function ([http://bit.ly/2whcUkJ](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html))
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas官方文档中的`cut`函数（[http://bit.ly/2whcUkJ](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html)）
- en: Refer to [Chapter 14](a833ecb6-8487-4648-8632-860640490e01.xhtml), *Re**structuring
    Data into a Tidy For**m*, for many more recipes with unstack
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参阅[第14章](a833ecb6-8487-4648-8632-860640490e01.xhtml)，*将数据重构为整洁的形式*，了解更多使用unstack的技巧
- en: Counting the total number of flights between cities
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计两座城市之间的航班总数
- en: In the flights dataset, we have data on the origin and destination airport.
    It is trivial to count the number of flights originating in Houston and landing
    in Atlanta, for instance. What is more difficult is counting the total number
    of flights between the two cities, regardless of which one is the origin or destination.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在航班数据集中，我们有关于起点和目的地机场的数据。例如，统计从休斯顿出发并降落在亚特兰大的航班数量是微不足道的。更困难的是统计两座城市之间的航班总数，而不考虑哪座城市是起点或目的地。
- en: Getting ready
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we count the total number of flights between two cities regardless
    of which one is the origin or destination. To accomplish this, we sort the origin
    and destination airports alphabetically so that each combination of airports always
    occurs in the same order. We can then use this new column arrangement to form
    groups and then to count.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们统计了两座城市之间的航班总数，而不考虑哪一个是起点或目的地。为此，我们按字母顺序对起点和目的地机场进行排序，使得每一对机场的组合总是按照相同的顺序出现。然后，我们可以使用这种新的列排列方式来形成分组并进行计数。
- en: How to do it...
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Read in the flights dataset, and find the total number of flights between each
    origin and destination airport:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取航班数据集，并找到每个起始和目的地机场之间的总航班数：
- en: '[PRE71]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Select the total number of flights between Houston (*IAH*) and Atlanta (*ATL*)
    in both directions:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择休斯顿（*IAH*）和亚特兰大（*ATL*）之间两个方向的总航班数：
- en: '[PRE72]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We could simply sum these two numbers together to find the total flights between
    the cities but there is a more efficient and automated solution that can work
    for all flights. Let''s independently sort the origin and destination cities for
    each row in alphabetical order:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以简单地将这两个数字相加以找到城市之间的总航班数，但有一种更有效和自动化的解决方案可以适用于所有航班。让我们独立地按字母顺序对每一行的起始和目的地城市进行排序：
- en: '[PRE73]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '![](img/bacd986b-9e18-46ee-b46c-caf49b2b8385.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bacd986b-9e18-46ee-b46c-caf49b2b8385.png)'
- en: 'Now that each row has been independently sorted, the column names are not correct.
    Let''s rename them to something more generic and then again find the total number
    of flights between all cities:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在每行都被独立排序，列名不正确。让我们将它们重命名为更通用的名称，然后再次找到所有城市之间的总航班数：
- en: '[PRE74]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Let''s select all the flights between Atlanta and Houston and verify that it
    matches the sum of the values in step 2:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们选择所有亚特兰大和休斯顿之间的航班，并验证它是否与第 2 步中值的总和相匹配：
- en: '[PRE75]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'If we try and select flights with Houston followed by Atlanta, we get an error:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们尝试选择休斯顿后面的亚特兰大航班，我们会收到一个错误：
- en: '[PRE76]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: How it works...
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In step 1, we form groups by the origin and destination airport columns and
    then apply the `size` method to the groupby object, which simply returns the total
    number of rows for each group. Notice that we could have passed the string `size`
    to the `agg` method to achieve the same result. In step 2, the total number of
    flights for each direction between Atlanta and Houston are selected. The Series
    `flights_count` has a MultiIndex with two levels. One way to select rows from
    a MultiIndex is to pass the `loc` indexing operator a tuple of exact level values.
    Here, we actually select two different rows, `('ATL', 'HOU')` and `('HOU', 'ATL')`.
    We use a list of tuples to do this correctly.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们通过起始和目的地机场列形成分组，然后将 `size` 方法应用于 groupby 对象，它简单地返回每个组的总行数。请注意，我们可以将字符串
    `size` 传递给 `agg` 方法以达到相同的结果。在第二步中，选择了亚特兰大和休斯顿之间每个方向的总航班数。Series `flights_count`
    具有两个级别的 MultiIndex。从 MultiIndex 中选择行的一种方法是向 `loc` 索引运算符传递一个确切级别值的元组。在这里，我们实际上选择了两行，`('ATL',
    'HOU')` 和 `('HOU', 'ATL')`。我们使用一个元组列表来正确执行此操作。
- en: Step 3 is the most pertinent step in the recipe. We would like to have just
    one label for all flights between Atlanta and Houston and so far we have two.
    If we alphabetically sort each combination of origin and destination airports,
    we would then have a single label for flights between airports. To do this, we
    use the DataFrame `apply` method. This is different from the groupby `apply` method.
    No groups are formed in step 3.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 步是这个步骤中最相关的步骤。我们希望对亚特兰大和休斯顿之间的所有航班只有一个标签，到目前为止我们有两个标签。如果我们按字母顺序对每个起始和目的地机场的组合进行排序，那么我们将有一个单一的标签用于机场之间的航班。为此，我们使用
    DataFrame `apply` 方法。这与 groupby `apply` 方法不同。在第 3 步中不形成组。
- en: 'The DataFrame `apply` method must be passed a function. In this case, it''s
    the built-in `sorted` function. By default, this function gets applied to each
    column as a Series. We can change the direction of computation by using `axis=1`
    (or `axis=''index''`). The `sorted` function has each row of data passed to it
    implicitly as a Series. It returns a list of sorted airport codes. Here is an
    example of passing the first row as a Series to the sorted function:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame `apply` 方法必须传递一个函数。在这种情况下，它是内置的 `sorted` 函数。默认情况下，此函数将应用于每一列作为一个 Series。我们可以通过使用
    `axis=1`（或 `axis='index'`）来改变计算的方向。`sorted` 函数将每一行数据隐式地作为一个 Series 传递给它。它返回一个排序后的机场代码列表。这里是将第一行作为一个
    Series 传递给 sorted 函数的示例：
- en: '[PRE77]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The `apply` method iterates over all rows using `sorted` in this exact manner.
    After completion of this operation, each row is independently sorted. The column
    names are now meaningless. We rename the column names in the next step and then
    perform the same grouping and aggregating as was done in step 2\. This time, all
    flights between Atlanta and Houston fall under the same label.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply` 方法以这种确切的方式使用 `sorted` 迭代所有行。完成此操作后，每行都会被独立排序。列名现在毫无意义。我们在下一步中重命名列名，然后执行与第
    2 步相同的分组和聚合操作。这次，所有亚特兰大和休斯顿之间的航班都归为同一标签。'
- en: There's more...
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: You might be wondering why we can't use the simpler `sort_values` Series method.
    This method does not sort independently and instead, preserves the row or column
    as a single record as one would expect while doing a data analysis. Step 3 is
    a very expensive operation and takes several seconds to complete. There are only
    about 60,000 rows so this solution would not scale well to larger data. Calling
    the
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我们不能使用更简单的`sort_values`系列方法。这个方法不能独立排序，而是保留每一行或每一列作为一个完整的记录，正如我们在进行数据分析时所期望的那样。步骤3是一个非常耗时的操作，完成需要几秒钟。虽然只有大约60,000行数据，但这个解决方案不适合处理更大的数据集。调用
- en: Step 3 is a very expensive operation and takes several seconds to complete.
    There are only about 60,000 rows so this solution would not scale well to larger
    data. Calling the `apply` method with `axis=1` is one of the least performant
    operations in all of pandas. Internally, pandas loops over each row and does not
    provide any speed boosts from NumPy. If possible, avoid using `apply` with `axis=1`.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤3是一个非常耗时的操作，完成需要几秒钟。虽然只有大约60,000行数据，但这个解决方案不适合处理更大的数据集。调用`apply`方法并使用`axis=1`是所有pandas操作中性能最差的之一。在内部，pandas会对每一行进行循环操作，而无法借助NumPy的速度提升。如果可能，尽量避免使用`apply`和`axis=1`。
- en: 'We can get a massive speed increase with the NumPy `sort` function. Let''s
    go ahead and use this function and analyze its output. By default, it sorts each
    row independently:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过NumPy的`sort`函数显著提高速度。让我们使用这个函数并分析它的输出。默认情况下，它会独立排序每一行：
- en: '[PRE78]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'A two-dimensional NumPy array is returned. NumPy does not easily do grouping
    operations so let''s use the DataFrame constructor to create a new DataFrame and
    check whether it equals the `flights_sorted` DataFrame from step 3:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的是一个二维的NumPy数组。NumPy不容易进行分组操作，因此我们可以使用DataFrame构造函数来创建一个新的DataFrame，并检查它是否等于步骤3中的`flights_sorted`
    DataFrame：
- en: '[PRE79]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'As the DataFrames are the same, you can replace step 3 with the previous faster
    sorting routine. Let''s time the difference between each of the different sorting
    methods:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DataFrame是相同的，你可以用之前更快的排序方法替代步骤3。让我们来对比每种排序方法的时间差异：
- en: '[PRE80]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: The NumPy solution is an astounding 700 times faster than using `apply` with
    pandas.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy解决方案比使用pandas的`apply`快了惊人的700倍。
- en: See also
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: NumPy official documentation on the `sort` function ([http://bit.ly/2vtRt0M](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sort.html))
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy官方文档关于`sort`函数的说明（[http://bit.ly/2vtRt0M](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sort.html)）
- en: Finding the longest streak of on-time flights
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找准时航班的最长连续段
- en: One of the most important metrics for airlines is their on-time flight performance.
    The Federal Aviation Administration considers a flight delayed when it arrives
    at least 15 minutes later than its scheduled arrival time. Pandas has direct methods
    to calculate the total and percentage of on-time flights per airline. While these
    basic summary statistics are an important metric, there are other non-trivial
    calculations that are interesting, such as finding the length of consecutive on-time
    flights for each airline at each of its origin airports.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 航空公司最重要的指标之一就是它们的准时航班表现。联邦航空管理局（FAA）将航班视为延误航班，如果它比计划到达时间晚了至少15分钟。Pandas提供了直接的方法来计算每个航空公司准时航班的总数和百分比。虽然这些基本的总结统计数据是一个重要的指标，但也有一些其他的非平凡计算很有意思，比如查找每个航空公司在每个起始机场的连续准时航班长度。
- en: Getting ready
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: In this recipe, we find the longest consecutive streak of on-time flights for
    each airline at each origin airport. This requires each value in a column to be
    aware of the value immediately following it. We make clever use of the `diff`
    and `cumsum` methods in order to find streaks before applying this methodology
    to each of the groups.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们找到每个航空公司在每个起始机场的最长连续准时航班段。这要求每个列中的值能够察觉到紧跟其后的值。我们巧妙地使用了`diff`和`cumsum`方法来找到连续段，在将这种方法应用于每个组之前。
- en: How to do it...
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Before we get started with the actual flights dataset, let''s practice counting
    streaks of ones with a small sample Series:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们开始处理实际的航班数据集之前，先练习使用一个小样本Series来计数连续的1：
- en: '[PRE81]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Our final representation of the streaks of ones will be a Series of the same
    length as the original with an independent count beginning from one for each streak.
    To get started, let''s use the `cumsum` method:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们最终得到的连续1的表示将是一个与原始数据长度相同的Series，每一段连续1的计数从1开始。为了开始，我们使用`cumsum`方法：
- en: '[PRE82]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'We have now accumulated all the ones going down the Series. Let''s multiply
    this Series by the original:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在已经累积了所有沿着序列向下的 1。让我们将这个序列与原始序列相乘：
- en: '[PRE83]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'We have only non-zero values where we originally had ones. This result is fairly
    close to what we desire. We just need to restart each streak at one instead of
    where the cumulative sum left off. Let''s chain the `diff` method, which subtracts
    the previous value from the current:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们只有在原始数据中为 1 的地方出现非零值。这个结果与我们期望的非常接近。我们只需要让每个 streak 从 1 重新开始，而不是从累积和的结果开始。让我们连接`diff`方法，它会将当前值减去前一个值：
- en: '[PRE84]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'A negative value represents the end of a streak. We need to propagate the negative
    values down the Series and use them to subtract away the excess accumulation from
    step 2\. To do this, we will make all non-negative values missing with the `where`
    method:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 负值表示 streak 的结束。我们需要将负值向下传播，并用它们来从步骤 2 中减去多余的累积。为此，我们将使用`where`方法将所有非负值设为缺失：
- en: '[PRE85]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'We can now propagate these values down with the `ffill` method:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以使用`ffill`方法将这些值向下传播：
- en: '[PRE86]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Finally, we can add this Series back to `s1` to clear out the excess accumulation:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以将这个序列加回到`s1`中，清除多余的累积：
- en: '[PRE87]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now that we have a working consecutive streak finder, we can find the longest
    streak per airline and origin airport. Let''s read in the flights dataset and
    create a column to represent on-time arrival:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了一个可以工作的连续 streak 查找器，我们可以找到每个航空公司和起始机场的最长 streak。让我们读取航班数据集，并创建一列来表示准时到达：
- en: '[PRE88]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '![](img/6e668d00-e72b-4851-a2ab-dfe2ecbd2739.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e668d00-e72b-4851-a2ab-dfe2ecbd2739.png)'
- en: 'Use our logic from the first seven steps to define a function that returns
    the maximum streak of ones for a given Series:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们前七步中的逻辑，定义一个函数来返回给定序列中的最大连续 1：
- en: '[PRE89]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Find the maximum streak of on-time arrivals per airline and origin airport
    along with the total number of flights and percentage of on-time arrivals. First,
    sort the day of the year and scheduled departure time:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到每个航空公司和起始机场的最大准时到达 streak，以及航班总数和准时到达的百分比。首先，按照年份中的日期和计划的起飞时间排序：
- en: '[PRE90]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '![](img/3bd814fd-2141-4ccc-9b1b-334f35c85196.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bd814fd-2141-4ccc-9b1b-334f35c85196.png)'
- en: How it works...
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Finding streaks in the data is not a straightforward operation in pandas and
    requires methods that look ahead or behind, such as `diff` or `shift`, or those
    that remember their current state, such as `cumsum`. The final result from the
    first seven steps is a Series the same length as the original that keeps track
    of all consecutive ones. Throughout these steps, we use the `mul` and `add` methods
    instead of their operator equivalents (`*`) and (`+`). In my opinion, this allows
    for a slightly cleaner progression of calculations from left to right. You, of
    course, can replace these with the actual operators.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas 中找到 streak 并不是一个简单的操作，需要使用一些前瞻或回溯的方法，比如`diff`或`shift`，或者那些能记住当前状态的方法，比如`cumsum`。前七步的最终结果是一个与原始序列长度相同的序列，记录了所有连续的
    1。在这些步骤中，我们使用了`mul`和`add`方法，而不是其运算符等价物（`*`）和（`+`）。我认为，这样做可以让计算的过程从左到右更加简洁。你当然可以用实际的运算符来替换它们。
- en: Ideally, we would like to tell pandas to apply the `cumsum` method to the start
    of each streak and reset itself after the end of each one. It takes many steps
    to convey this message to pandas. Step 2 accumulates all the ones in the Series
    as a whole. The rest of the steps slowly remove any excess accumulation. In order
    to identify this excess accumulation, we need to find the end of each streak and
    subtract this value from the beginning of the next streak.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望告诉 pandas 在每个 streak 开始时应用`cumsum`方法，并在每个 streak 结束后重置它。这需要许多步骤来传达给
    pandas。步骤 2 将序列中的所有 1 累积在一起。接下来的步骤则逐渐去除多余的累积。为了识别这些多余的累积，我们需要找到每个 streak 的结束位置，并从下一个
    streak 的开始位置减去这个值。
- en: To find the end of each streak, we cleverly make all values not part of the
    streak zero by multiplying `s1` by the original Series of zeros and ones in step
    3\. The first zero following a non-zero, marks the end of a streak. That's good,
    but again, we need to eliminate the excess accumulation. Knowing where the streak
    ends doesn't exactly get us there.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到每个 streak 的结束，我们巧妙地通过将`s1`与第 3 步中的零和一的原始序列相乘，来将所有不属于 streak 的值变为零。跟随第一个零的非零值标志着一个
    streak 的结束。这个方法不错，但我们还需要消除多余的累积。知道 streak 的结束位置并不能完全解决问题。
- en: In step 4, we use the `diff` method to find this excess. The `diff` method takes
    the difference between the current value and any value located at a set number
    of rows away from it. By default, the difference between the current and the immediately
    preceding value is returned.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，我们使用`diff`方法来找出这些多余的值。`diff`方法计算当前值与距离它一定行数的任何值之间的差异。默认情况下，它返回当前值与紧接着的前一个值之间的差异。
- en: Only negative values are meaningful in step 4\. Those are the ones immediately
    following the end of a streak. These values need to be propagated down until the
    end of the following streak. To eliminate (make missing) all the values we don't
    care about, we use the `where` method, which takes a Series of conditionals of
    the same size as the calling Series. By default, all the `True` values remain
    the same, while the `False` values become missing. The `where` method allows you
    to use the calling Series as part of the conditional by taking a function as its
    first parameter. An anonymous function is used, which gets passed the calling
    Series implicitly and checks whether each value is less than zero. The result
    of step 5 is a Series where only the negative values are preserved with the rest
    changed to missing.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 只有负值在第4步中才是有意义的。这些值位于连续序列的末尾。需要将这些值向下传播，直到下一个连续序列的结束。为了消除（使其缺失）我们不关心的所有值，我们使用`where`方法，它接受一个与调用的Series大小相同的条件Series。默认情况下，所有`True`值保持不变，而`False`值则变为缺失。`where`方法允许你通过将一个函数作为其第一个参数来使用调用的Series作为条件的一部分。这里使用了一个匿名函数，它隐式地接受调用的Series并检查每个值是否小于零。第5步的结果是一个Series，其中只有负值被保留，其余的都变成缺失值。
- en: The `ffill` method in step 6 replaces missing values with the last non-missing
    value going forward/down a Series. As the first three values don't follow a non-missing
    value, they remain missing. We finally have our Series that removes the excess
    accumulation. We add our accumulation Series to the result of step 6 to get the
    streaks all beginning from zero. The `add` method allows us to replace the missing
    values with the `fill_value` parameter. This completes the process of finding
    streaks of ones in the dataset. When doing complex logic like this, it is a good
    idea to use a small dataset where you know what the final output will be. It would
    be quite a difficult task to start at step 8 and build this streak-finding logic
    while grouping.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步中的`ffill`方法将缺失值替换为向前（或向下）传播的最后一个非缺失值。由于前面三个值没有跟随任何非缺失值，因此它们保持为缺失值。最终，我们得到了一个移除多余累积的Series。我们将这个累积Series与第6步的结果相加，从而得到所有从零开始的连续序列。`add`方法允许我们使用`fill_value`参数替换缺失值。这个过程完成了在数据集中查找连续的1值序列。当做复杂逻辑处理时，最好使用一个小数据集，这样你可以预知最终结果。如果从第8步开始并在分组时构建这个查找连续序列的逻辑，那将是一个非常困难的任务。
- en: In step 8, we create the `ON_TIME` column. One item of note is that the cancelled
    flights have missing values for `ARR_DELAY`, which do not pass the boolean condition
    and therefore result in a zero for the `ON_TIME` column. Canceled flights are
    treated the same as delayed.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8步中，我们创建了`ON_TIME`列。需要注意的一点是，取消的航班在`ARR_DELAY`列中有缺失值，这些缺失值无法通过布尔条件，因此在`ON_TIME`列中会显示为零。取消的航班与延误航班一样处理。
- en: Step 9 turns our logic from the first seven steps into a function and chains
    the `max` method to return the longest streak. As our function returns a single
    value, it is formally an aggregating function and can be passed to the `agg` method
    as done in step 10\. To ensure that we are looking at actual consecutive flights,
    we use the `sort_values` method to sort by date and scheduled departure time.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步将我们前七步的逻辑转化为一个函数，并链式调用`max`方法以返回最长的连续序列。由于我们的函数返回单一值，它正式成为一个聚合函数，并可以像第10步中那样传递给`agg`方法。为了确保我们正在查看实际的连续航班，我们使用`sort_values`方法按日期和预定出发时间进行排序。
- en: There's more...
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'Now that we have found the longest streaks of on-time arrivals, we can easily
    find the opposite--the longest streak of delayed arrivals. The following function
    returns two rows for each group passed to it. The first row is the start of the
    streak, and the last row is the end of the streak. Each row contains the month
    and day that the streak started/ended, along with the total streak length:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了最长的按时到达连续序列，我们可以轻松地找到相反的情况——最长的延误到达连续序列。以下函数将返回传递给它的每个组的两行。第一行是连续序列的开始，最后一行是结束。每一行都包含了该序列开始/结束的月份和日期，以及连续序列的总长度：
- en: '[PRE91]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '![](img/2f18b944-147e-4743-aa76-2e1ee9800842.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f18b944-147e-4743-aa76-2e1ee9800842.png)'
- en: As we are using the `apply` groupby method, a DataFrame of each group is passed
    to the `max_delay_streak` function. Inside this function, the index of the DataFrame
    is dropped and replaced by a `RangeIndex` in order for us to easily find the first
    and last row of the streak. The `ON_TIME` column is inverted and then the same
    logic is used to find streaks of delayed flights. The index of the first and last
    rows of the streak are stored as variables. These indexes are then used to select
    the month and day when the streaks ended. We use a DataFrame to return our results.
    We label and name the index to make the final result clearer.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用`apply`的groupby方法时，每个组的DataFrame会被传递给`max_delay_streak`函数。在这个函数内部，DataFrame的索引会被删除，并用`RangeIndex`替换，这样我们可以轻松找到连续记录的第一行和最后一行。接着，`ON_TIME`列会被反转，然后使用相同的逻辑来查找延误航班的连续记录。连续记录的第一行和最后一行的索引会被存储为变量。这些索引随后用于选择连续记录结束的月份和日期。我们使用DataFrame来返回结果，并为索引添加标签和名称，以便让最终结果更清晰。
- en: Our final results show the longest delayed streaks accompanied by the first
    and last date. Let's investigate to see if we can find out why these delays happened.
    Inclement weather is a common reason for delayed or canceled flights. Looking
    at the first row, American Airlines (AA) started a streak of 38 delayed flights
    in a row from the Dallas Fort-Worth (DFW) airport beginning February 26 until
    March 1 of 2015\. Looking at historical weather data from February 27, 2015, two
    inches of snow fell, which was a record for that day ([http://bit.ly/2iLGsCg](http://bit.ly/2iLGsCg)).
    This was a major weather event for DFW and caused massive problems for the entire
    city ([http://bit.ly/2wmsHPj](http://bit.ly/2wmsHPj)). Notice that DFW makes another
    appearance as the third longest streak but this time a few days earlier and for
    a different airline.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终结果显示了最长的延迟连续记录，并伴随有起始日期和结束日期。让我们来调查一下，看看是否能找出这些延迟发生的原因。恶劣天气是航班延误或取消的常见原因。查看第一行，来自美国航空公司（AA）的航班从2015年2月26日开始，从达拉斯沃斯堡机场（DFW）起飞，连续38个航班发生延误，直到3月1日。根据2015年2月27日的历史天气数据，当天降雪达到两英寸，这也是当天的降雪记录（[http://bit.ly/2iLGsCg](http://bit.ly/2iLGsCg)）。这场大规模的天气事件对DFW造成了严重影响，并给整个城市带来了巨大的麻烦（[http://bit.ly/2wmsHPj](http://bit.ly/2wmsHPj)）。请注意，DFW再次出现，成为第三长的延误记录，但这次发生的时间稍早一些，并且是由另一家航空公司造成的。
- en: See also
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: Pandas official documentation of `ffill` ( [http://bit.ly/2gn5zGU](http://bit.ly/2gn5zGU)
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas官方文档的`ffill`（[http://bit.ly/2gn5zGU](http://bit.ly/2gn5zGU)）
