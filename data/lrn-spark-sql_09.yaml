- en: Developing Applications with Spark SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL开发应用
- en: In this chapter, we will present several examples of developing applications
    using Spark SQL. We will primarily focus on text analysis-based applications,
    including preprocessing pipelines, bag-of-words techniques, computing readability
    metrics for financial documents, identifying themes in document corpuses, and
    using Naive Bayes classifiers. Additionally, we will describe the implementation
    of a machine learning example.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍使用Spark SQL开发应用的几个示例。我们将主要关注基于文本分析的应用，包括预处理管道、词袋技术、计算财务文件的可读性指标、识别文档语料中的主题以及使用朴素贝叶斯分类器。此外，我们将描述一个机器学习示例的实现。
- en: 'More specifically, you will learn about the following in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，您将在本章中了解以下内容：
- en: Spark SQL-based application's development
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Spark SQL的应用开发
- en: Preprocessing textual data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理文本数据
- en: Building preprocessing data pipelines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建预处理数据管道
- en: Identifying themes in document corpuses
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文档语料中识别主题
- en: Using Naive Bayes classifiers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类器
- en: Developing a machine learning application
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发机器学习应用
- en: Introducing Spark SQL applications
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Spark SQL应用
- en: Machine learning, predictive analytics, and related data science topics are
    becoming increasingly popular for solving real-world problems across business
    domains. These applications are driving mission-critical business decision making
    in many organizations. Examples of such applications include recommendation engines,
    targeted advertising, speech recognition, fraud detection, image recognition and
    categorization, and so on. Spark (and Spark SQL) is increasingly becoming the
    platform of choice for these large-scale distributed applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习、预测分析和相关的数据科学主题正变得越来越受欢迎，用于解决商业领域的实际问题。这些应用正在推动许多组织中至关重要的业务决策。这些应用的例子包括推荐引擎、定向广告、语音识别、欺诈检测、图像识别和分类等。Spark（以及Spark
    SQL）越来越成为这些大规模分布式应用的首选平台。
- en: With the availability of online data sources for financial news, earning conference
    calls, regulatory filings, social media, and so on, interest in the automated
    and intelligent analysis of textual and other unstructured data available in various
    formats, including text, audio, and video, has proliferated. These applications
    include sentiment analysis from regulatory filings, large-scale automated analysis
    of news articles and stories, twitter analysis, stock price prediction applications,
    and so on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着在线数据源（如财经新闻、收益电话会议、监管文件、社交媒体等）的可用性，人们对对各种格式的文本、音频和视频等非结构化数据进行自动化和智能化分析的兴趣日益增加。这些应用包括从监管文件中进行情感分析、对新闻文章和故事进行大规模自动化分析、Twitter分析、股价预测应用等。
- en: In this chapter, we will present some approaches and techniques for dealing
    with textual data. Additionally, we will present some examples of applying machine
    learning models on textual data to classify documents, derive insights from document
    corpuses, and process textual information for sentiment analysis.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些处理文本数据的方法和技术。此外，我们将介绍一些应用机器学习模型对文本数据进行分类、从文档语料中得出见解以及为情感分析处理文本信息的示例。
- en: In the next section, we start our coverage with a few methods that help in converting
    regulatory filings into collections of words. This step allows the use of domain-specific
    dictionaries to classify the tone of the documents, train algorithms to identify
    document characteristics or identify hidden structures as common topics across
    a collection of documents.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将从几种方法开始介绍如何将监管文件转换为词语集合。这一步允许使用领域特定的词典来对文件的语气进行分类，训练算法来识别文件特征或识别作为一组文件的共同主题的隐藏结构。
- en: 'For a more detailed survey of textual analysis methods in accounting and finance,
    refer to *Textual Analysis in Accounting and Finance: A Survey* by Tim Loughran
    and Bill McDonald, at [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2504147](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2504147).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有关会计和金融领域文本分析方法的更详细调查，请参阅Tim Loughran和Bill McDonald的《会计和金融中的文本分析：一项调查》，网址为[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2504147](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2504147)。
- en: We will also examine typical issues, challenges, and limitations present in
    implementing textual analysis applications, for example, converting tokens to
    words, disambiguating sentences, and cleansing embedded tags, documents, and other
    noisy elements present in financial disclosure documents. Also, note that the
    use of HTML formatting is a leading source of errors while parsing documents.
    Such parsing depends on the consistency in the structure of the text and the related
    markup language, often resulting in significant errors. Additionally, it is important
    to understand that we are often interested in both the intended and the unintended
    information conveyed by text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将研究在实施文本分析应用中存在的典型问题、挑战和限制，例如将标记转换为词语、消除歧义句子以及清理财务披露文件中存在的嵌入式标签、文档和其他嘈杂元素。此外，请注意，使用HTML格式是解析文件时出现错误的主要来源。这种解析依赖于文本结构和相关标记语言的一致性，通常会导致重大错误。此外，重要的是要理解，我们通常对文本传达的意图和非意图信息都感兴趣。
- en: Understanding text analysis applications
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解文本分析应用
- en: The inherent nature of language and writing leads to problems of high dimensionality
    while analyzing documents. Hence, some of the most widely used textual methods
    rely on the critical assumption of independence, where the order and direct context
    of a word are not important. Methods, where word sequence is ignored, are typically
    labeled as "bag-of-words" techniques.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 语言和书写的固有特性导致在分析文档时出现高维度的问题。因此，一些最广泛使用的文本方法依赖于独立性的关键假设，即单词的顺序和直接上下文并不重要。在忽略单词顺序的方法通常被标记为“词袋”技术。
- en: Textual analysis  is a lot more imprecise compared to quantitative analysis.
    Textual data requires an additional step of translating the text into quantitative
    measures, which are then used as inputs for various text-based analytics or ML
    methods. Many of these methods are based on deconstructing a document into a term-document
    matrix consisting of rows of words and columns of word counts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与定量分析相比，文本分析更加不精确。文本数据需要额外的步骤将文本转化为定量指标，然后作为各种基于文本的分析或机器学习方法的输入。其中许多方法是基于将文档解构为术语-文档矩阵，其中包含单词行和单词计数列。
- en: In applications using a bag of words, the approach to normalizing the word counts
    is important as the raw counts directly dependent on the document length. A simple
    use of proportions can solve this problem, however, we might also want to adjust
    a word's weight. Typically, these approaches are based on the rarity of a given
    term in the document, for example, **term frequency-inverse document frequency** (**tf-idf**).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用词袋模型的应用中，规范化词频是很重要的，因为原始计数直接依赖于文档长度。简单地使用比例可以解决这个问题，但是我们可能也想要调整单词的权重。通常，这些方法是基于文档中给定术语的稀有程度，例如**词频-逆文档频率**（**tf-idf**）。
- en: In the next section, we will explore using Spark SQL for the textual analysis
    of financial documents.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨使用Spark SQL进行财务文件的文本分析。
- en: Using Spark SQL for textual analysis
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL进行文本分析
- en: In this section, we will present a detailed example of typical preprocessing
    required for preparing data for textual analysis (from the accounting and finance
    domain). We will also compute a few metrics for readability (a measure of whether
    the receiver of information can accurately reconstruct the intended message).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示一个典型的预处理示例，用于准备文本分析所需的数据（来自会计和金融领域）。我们还将计算一些可读性指标（接收信息的人是否能准确重构预期的消息）。
- en: Preprocessing textual data
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本数据预处理
- en: In this section, we will develop a set of functions for preprocessing a `10-K`
    statement. We will be using the "complete submission text file" for a `10-K` filing
    on the EDGAR website as the input text in our example.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开发一组用于预处理`10-K`报告的函数。我们将使用EDGAR网站上的“完整提交文本文件”作为我们示例中的输入文本。
- en: 'For more details on the `Regex` expressions used to preprocess `10-K` filings
    refer to *The Annual Report Algorithm: Retrieval of Financial Statements and Extraction
    of Textual Information*, by Jorg Hering at [http://airccj.org/CSCP/vol7/csit76615.pdf](http://airccj.org/CSCP/vol7/csit76615.pdf).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有关用于预处理`10-K`报告的`Regex`表达式的更多细节，请参阅Jorg Hering的*年度报告算法：财务报表检索和文本信息提取*，链接为[http://airccj.org/CSCP/vol7/csit76615.pdf](http://airccj.org/CSCP/vol7/csit76615.pdf)。
- en: 'First, we import all the packages required in this chapter:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入本章中所需的所有包：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we read the input file and convert the input rows into a single string
    for our processing. You can download the input file for the following example
    from [https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/0001193125-14-383437-index.html](https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/0001193125-14-383437-index.html):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们读取输入文件，并将输入行转换为一个字符串以供我们处理。您可以从以下链接下载以下示例的输入文件：[https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/0001193125-14-383437-index.html](https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/0001193125-14-383437-index.html)：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As the pre-processing functions are executed, the length of the input string
    progressively reduces as a lot of extraneous or unrelated text/tags are removed
    at each step. We compute the starting length of the original string to track the
    impact of applying specific functions in each of the processing steps:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 随着预处理函数的执行，输入字符串的长度逐渐减少，因为在每个步骤中都删除了很多无关的文本/标签。我们计算原始字符串的起始长度，以跟踪在每个处理步骤中应用特定函数的影响：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In addition, `10-K` files are composed of several exhibits--XBRL, graphics,
    and other documents (file) types embedded in the financial statements; these include
    Microsoft Excel files (file extension `*.xlsx`), ZIP files (file extension `*.zip`),
    and encoded PDF files (file extension `*.pdf`).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`10-K`文件由几个附件组成--XBRL、图形和其他文档（文件）类型嵌入在财务报表中；这些包括Microsoft Excel文件（文件扩展名`*.xlsx`）、ZIP文件（文件扩展名`*.zip`）和编码的PDF文件（文件扩展名`*.pdf`）。
- en: 'In the following step, we apply additional rules for deleting these embedded
    documents:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将应用额外的规则来删除这些嵌入式文档：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we delete all the metadata included in the core document and the exhibits,
    as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们删除核心文档和附件中包含的所有元数据，如下所示：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Before deleting all the HTML elements and their corresponding attributes, we
    delete the tables in the document since they normally contain non-textual (quantitative)
    information.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在删除所有HTML元素及其对应的属性之前，我们先删除文档中的表格，因为它们通常包含非文本（定量）信息。
- en: 'The following function uses a set of regular expressions applied to delete
    tables and HTML elements embedded in the financial statement:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数使用一组正则表达式来删除嵌入在财务报表中的表格和HTML元素：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, we extract the text in the body section of each of the HTML-formatted
    documents. As the EDGAR system accepts submissions with extended character sets,
    such as `&nbsp; &amp;`, `&reg;`, and so on--they will need to be decoded and/or
    appropriately replaced for textual analysis.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提取每个HTML格式文档的正文部分的文本。由于EDGAR系统接受包含扩展字符集的提交，比如`&nbsp; &amp;`，`&reg;`等等--它们需要被解码和/或适当替换以进行文本分析。
- en: 'We show a few examples of these in this function:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们展示了一些例子：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we define a function that cleans up excess blanks, line feed, and carriage
    returns:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，清除多余的空格、换行和回车：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the next code block, we define a function to illustrate the removal of a
    user-specified set of strings. These strings can be read from an input file or
    a database. This step is not required if you can implement `Regex` for extraneous
    text that are, typically, present throughout the document (and vary from document
    to document), but do not have any additional value in textual analysis:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个代码块中，我们定义一个函数来说明删除用户指定的一组字符串。这些字符串可以从输入文件或数据库中读取。如果您可以实现`Regex`来处理文档中通常存在的多余文本（并且从文档到文档可能会有所不同），但在文本分析中没有任何附加值，则不需要这一步：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the next step, we remove all the URLs, filenames, digits, and punctuation
    (except the periods) from the document string. The periods are retained at this
    stage for computing the number of periods (representing the number of sentences)
    in the text (as shown in the following section):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将从文档字符串中删除所有的URL、文件名、数字和标点符号（除了句号）。在这个阶段，句号被保留下来用于计算文本中句子的数量（如下一节所示）：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the next section, we will discuss some metrics typically used to measure
    readability, that is, whether the textual information contained in a `10-K` filing
    is accessible to the user.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论一些通常用于衡量可读性的指标，即`10-K`申报中包含的文本信息是否对用户可访问。
- en: Computing readability
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算可读性
- en: 'The Fog Index and the number of words contained in the annual report have been
    widely used as measures of readability for annual reports (that is, `Form 10-Ks`).
    The Fog Index is a function of two variables: average sentence length (in words)
    and complex words (defined as the percentage of words with more than two syllables):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 雾指数和年度报告中包含的单词数量已被广泛用作年度报告（即`Form 10-Ks`）的可读性指标。雾指数是两个变量的函数：平均句子长度（以单词计）和复杂单词（定义为超过两个音节的单词的百分比）：
- en: '*Fog Index = 0.4 * (average number of words per sentence + percentage of complex
    words)*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*雾指数 = 0.4 *（每句平均单词数 + 复杂单词的百分比）*'
- en: The Fog Index equation estimates the number of years of education needed to
    understand the text on a first reading. Thus, a Fog Index value of `16` implies
    that the reader needs sixteen years of education--essentially a college degree--to
    comprehend the text on a first reading. Generally, documents with a Fog Index
    of above eighteen are considered unreadable since more than a master's degree
    is needed to understand the text.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 雾指数方程估计了阅读文本所需的教育年限。因此，雾指数值为`16`意味着读者需要十六年的教育——基本上是大学学位——才能在第一次阅读时理解文本。一般来说，雾指数超过十八的文件被认为是不可读的，因为需要超过硕士学位才能理解文本。
- en: Parsing the `10-K` for computing the average number of words per sentence is
    typically a difficult and error-prone process because these documents contain
    a variety of abbreviations, and use periods to delineate section identifiers or
    as spacers. Additionally, real-world systems will also need to identify the many
    lists contained in such filings (based on punctuation and line spacing). For example,
    such an application will need to avoid counting the periods in section headers,
    ellipses, or other cases where a period is likely not terminating a sentence,
    and then assume that the remaining periods are sentence terminations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 解析`10-K`以计算每个句子的平均单词数通常是一个困难且容易出错的过程，因为这些文件包含各种缩写，并使用句号来界定部分标识符或作为空格。此外，真实世界的系统还需要识别这些申报中包含的许多列表（基于标点和行间距）。例如，这样的应用程序需要避免计算部分标题、省略号或其他情况中的句号，并假定剩下的句号是句子的结束。
- en: The average words-per-sentence metric is determined by the number of words divided
    by the number of sentence terminations. This is typically done by removing abbreviations
    and other spurious sources of periods and then counting the number of sentence
    terminators and the number of words.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 平均每句单词数的度量是由单词数除以句子终止符的数量确定的。这通常是通过删除缩写和其他虚假的句号源，然后计算句子终止符和单词的数量来完成的。
- en: 'We compute the number of periods left in the text, as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算文本中剩余的句号数量，如下所示：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we remove all the periods (and any other non-alphabetic characters remaining
    in our text) to arrive at an initial set of words contained in our original document.
    Note that all of these words may still not be legitimate words:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将删除文本中剩余的所有句号（和任何其他非字母字符），以得到包含在我们原始文档中的初始单词集。请注意，所有这些单词可能仍然不是合法的单词：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/00269.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00269.jpeg)'
- en: 'In the following step, we convert the string of words into a DataFrame and
    use the `explode()` function to create a row for each of the words:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将把单词字符串转换为DataFrame，并使用`explode()`函数为每个单词创建一行：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, we read in a dictionary (preferably a domain-specific dictionary). We
    will match our list of words against this dictionary to arrive at our final list
    of words (they should all be legitimate words after this stage).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将读取一个词典（最好是领域特定的词典）。我们将把我们的单词列表与这个词典进行匹配，以得到我们最终的单词列表（在这个阶段它们应该都是合法的单词）。
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For our purposes, we have used the Loughran & McDonold's Master Dictionary as
    it contains words typically found in 10-K statements. You can download the `LoughranMcDonald_MasterDictionary_2014.csv` file
    and the associated documentation from [https://www3.nd.edu/~mcdonald/Word_Lists.html](https://www3.nd.edu/~mcdonald/Word_Lists.html).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们的目的，我们使用了Loughran & McDonold的主词典，因为它包含在10-K报告中通常找到的单词。您可以从[https://www3.nd.edu/~mcdonald/Word_Lists.html](https://www3.nd.edu/~mcdonald/Word_Lists.html)下载`LoughranMcDonald_MasterDictionary_2014.csv`文件和相关文档。
- en: 'In the next step, we join our word list DataFrame with the dictionary, and
    compute the number of words in our final list:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将我们的单词列表DataFrame与词典连接，并计算我们最终列表中的单词数量：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The average words per sentence are computed by dividing the number of words
    by the number of periods computed earlier:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 平均每句单词数是通过将单词数除以先前计算的句号数来计算的：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We use the `Syllables` column in the dictionary to compute the number of words
    in our word list that have more than two syllables, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用词典中的“音节”列来计算我们的单词列表中有多少单词有两个以上的音节，具体如下：
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we plug in the parameters into our equation to compute the Fog Index,
    as illustrated:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将参数插入到我们的方程中，计算Fog指数，如下所示：
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The argument against the use of readability measures, such as the Fog Index,
    in financial documents is the observation that a majority of these documents are
    not distinguishable based on the writing style used. Additionally, even though
    the percentage of complex words in these documents may be high, such words, or
    industry jargon, are easily understood by the audience for such documents (for
    example, the investor community).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 反对使用可读性指标（例如Fog指数）在财务文件中的观点是，这些文件中的大多数并不能根据所使用的写作风格来区分。此外，即使这些文件中复杂词语的比例可能很高，这些词语或行业术语对这些文件的受众（例如投资者社区）来说是容易理解的。
- en: As a simple proxy for readability of annual reports, Loughran and McDonald suggest
    using the natural log of gross `10-K` file size (complete submission text file).
    Compared to the Fog Index, this measure is a lot easier to obtain and does not
    involve complicated parsing of `10-K` documents.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作为年度报告可读性的简单指标，Loughran和McDonald建议使用“10-K”文件大小的自然对数（完整提交文本文件）。与Fog指数相比，这个指标更容易获得，不需要复杂的解析“10-K”文件。
- en: 'In the following step, we present a function to compute the file (or, more
    specifically, the RDD) size:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们提供一个计算文件（或者更具体地说，RDD）大小的函数：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The file size (in MB) and the log of the file sizes can be calculated as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 文件大小（以MB为单位）和文件大小的对数可以如下计算：
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Although file size is a good proxy for the readability of documents such as
    the `10-K` filings, it may be less suitable for text from press releases, newswire
    stories, and earnings conference calls. In such cases, as the length of the text
    does not vary significantly, other approaches that focus more on the content may
    be more suitable.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文件大小是衡量文档可读性的一个很好的指标，比如“10-K”备案，但对于新闻稿、新闻稿件和盈利电话会议等文本可能不太合适。在这种情况下，由于文本长度变化不大，更适合采用更注重内容的其他方法。
- en: In the next section, we will discuss using word lists in textual analysis.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论在文本分析中使用单词列表。
- en: Using word lists
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用单词列表
- en: In measuring the tone or sentiment of a financial document, practitioners typically
    count the number of words associated with a particular sentiment scaled by the
    total number of words in the document. Thus, for example, higher proportions of
    negative words in a document indicate a more pessimistic tone.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在衡量财务文件的语气或情绪时，从业者通常计算与特定情绪相关的单词数量，按照文档中的总单词数进行比例缩放。因此，例如，文档中更高比例的负面词语表明更悲观的语气。
- en: The use of dictionaries to measure tone has several important advantages. Apart
    from the convenience in computing sentiment at scale, the use of such dictionaries
    promotes standardization by eliminating individual subjectivity. An important
    component of classifying words is identifying the most frequently occurring words
    within each classification.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词典来衡量语气具有几个重要优势。除了在大规模计算情绪时的便利性外，使用这种词典还通过消除个体主观性来促进标准化。对于分类单词的重要组成部分是识别每个分类中最常出现的单词。
- en: 'In the following step, we use the word-sentiment indicators contained in the
    dictionary to get a sense of the sentiment or tone of the `10-K` filing. This
    can be computed relative to the past filings by the same organization, or against
    other organizations in the same or different sector(s):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们使用词典中包含的单词情感指标来了解“10-K”备案的情感或语气。这可以相对于同一组织以往的备案进行计算，或者与同一或不同行业的其他组织进行比较。
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Typically, the use of modal words is also important in such analyses. For example,
    the use of weaker modal words (for example, may, could, and might) could possibly
    signal issues at the firm:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在这种分析中，情态词的使用也很重要。例如，使用较弱的情态词（例如，may，could和might）可能会暗示公司存在问题：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the following code, we count the number of words for each category of modal
    words. As per the reference documentation of the dictionary used here, a `1` indicates
    "strong modal" (for example, words such as "always", "definitely", and "never"),
    a `2` indicates "moderate modal" (for example, words such as "can", "generally",
    and "usually"), and a `3` indicates "weak modal" (for example, words such as "almost",
    "could", "might", and "suggests"):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们计算每个情态词类别的单词数量。根据此处使用的词典的参考文档，“1”表示“强情态”（例如，“always”，“definitely”和“never”等词），“2”表示“中等情态”（例如，“can”，“generally”和“usually”等词），而“3”表示“弱情态”（例如，“almost”，“could”，“might”和“suggests”等词）：
- en: '[PRE23]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the next section, we will use some of the functions defined in this section
    to create a data preprocessing pipeline for `10-K` filings.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用本节定义的一些函数来为“10-K”备案创建数据预处理流水线。
- en: Creating data preprocessing pipelines
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据预处理流水线
- en: In this section, we will convert some of the data processing functions from
    the previous sections into custom Transformers. These Transformer objects map
    an input DataFrame to an output DataFrame and are typically used to prepare DataFrames
    for machine learning applications.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将一些先前部分的数据处理函数转换为自定义Transformer。这些Transformer对象将输入DataFrame映射到输出DataFrame，并通常用于为机器学习应用程序准备DataFrame。
- en: We create the following classes as `UnaryTransformer` objects that apply transformations
    to one input DataFrame column and produce another by appending a new column (containing
    the processing results of the applied function) to it. These custom Transformer
    objects can then be a part of a processing pipeline.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建以下类作为“UnaryTransformer”对象，将转换应用于一个输入DataFrame列，并通过将新列（包含应用函数的处理结果）附加到其中来生成另一个列。然后，这些自定义Transformer对象可以成为处理流水线的一部分。
- en: 'First, we create the four custom `UnaryTransformer` classes that we will use
    in our example, as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建四个自定义的`UnaryTransformer`类，我们将在示例中使用，如下所示：
- en: '**TablesNHTMLElemCleaner.scala**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**TablesNHTMLElemCleaner.scala**'
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**AllURLsFileNamesDigitsPunctuationExceptPeriodCleaner.scala**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**AllURLsFileNamesDigitsPunctuationExceptPeriodCleaner.scala**'
- en: '[PRE25]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**OnlyAlphasCleaner.scala**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**OnlyAlphasCleaner.scala**'
- en: '[PRE26]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**ExcessLFCRWSCleaner.scala**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**ExcessLFCRWSCleaner.scala**'
- en: '[PRE27]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create the following `build.sbt` file for compiling and packaging the target
    classes:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 创建以下`build.sbt`文件来编译和打包目标类：
- en: '[PRE28]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Use the following `SBT` command to compile and package the classes into a JAR
    file:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下`SBT`命令编译并打包类成一个JAR文件：
- en: '[PRE29]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, restart Spark shell with the preceding JAR file included in the session:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重新启动Spark shell，并将前面的JAR文件包含在会话中：
- en: '[PRE30]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The Dataset for the following example, Reuters-21578, Distribution 1.0, can
    be downloaded from [https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例的数据集，Reuters-21578，Distribution 1.0，可以从[https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection)下载。
- en: Here, we will take one of the entries demarcated by `<Reuters>...</Reuters>`
    tags in the downloaded SGML files to create a new input file containing a single
    story. This roughly simulates a new story coming into our pipeline. More specifically,
    this story can be coming in via a Kafka queue, and we can create a continuous
    Spark SQL application to process the incoming story text.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用下载的SGML文件中由`<Reuters>...</Reuters>`标记分隔的条目之一，创建一个包含单个故事的新输入文件。这大致模拟了一个新故事进入我们的管道。更具体地说，这个故事可能是通过Kafka队列进入的，我们可以创建一个连续的Spark
    SQL应用程序来处理传入的故事文本。
- en: 'First, we read in the newly created file into a DataFrame, as illustrated:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将新创建的文件读入DataFrame中，如下所示：
- en: '[PRE31]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we create instances of Transformers using the classes we defined earlier
    in this section. The Transformers are chained together in the pipeline by specifying
    the output columns for each of the Transformer as the input column to the next
    Transformer in the chain:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用本节中之前定义的类创建Transformer的实例。通过将每个Transformer的输出列指定为链中下一个Transformer的输入列，将Transformer在管道中链接在一起：
- en: '[PRE32]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'After processing the text through our cleansing components, we add two more
    stages in order to tokenize and remove stop words from our text, as demonstrated.
    We use the generic stop words list file available at [https://www3.nd.edu/~mcdonald/Word_Lists.html](https://www3.nd.edu/~mcdonald/Word_Lists.html):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过我们的清洗组件处理文本后，我们添加了另外两个阶段，以对我们的文本进行标记化和去除停用词，如示例所示。我们使用可在[https://www3.nd.edu/~mcdonald/Word_Lists.html](https://www3.nd.edu/~mcdonald/Word_Lists.html)上找到的通用停用词列表文件：
- en: '[PRE33]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](img/00270.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00270.jpeg)'
- en: '[PRE34]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: At this stage, the components for all our processing stages are ready to be
    assembled into a pipeline.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，所有处理阶段的组件都准备好被组装成一个管道。
- en: For more details on Spark pipelines, refer to [https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Spark管道的更多详细信息，请参阅[https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)。
- en: 'We create a pipeline and chain all the Transformers to specify the pipeline
    stages, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个管道，并将所有Transformer链接在一起，以指定管道阶段，如下所示：
- en: '[PRE35]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The `pipeline.fit()` method is called on the original DataFrame containing
    the raw text document:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始包含原始文本文档的DataFrame上调用`pipeline.fit()`方法：
- en: '[PRE36]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can use the pipeline model from the preceding step to transform our original
    Dataset to the form required to feed other downstream textual applications. We
    also drop the columns from intermediate processing steps to clean up our DataFrame:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用前面步骤的管道模型来转换我们原始的数据集，以满足其他下游文本应用程序的要求。我们还删除了中间处理步骤的列，以清理我们的DataFrame：
- en: '[PRE37]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Furthermore, we can clean up the final output column by removing any rows containing
    empty strings or spaces, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过删除包含空字符串或空格的任何行来清理最终输出列，如下所示：
- en: '[PRE38]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The rest of the processing is similar to what we presented earlier. The following
    steps explode the column containing our words into separate rows, join our final
    list of words with the dictionary, and then compute the sentiment and modal word''s
    usage:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其余处理步骤与我们之前介绍的类似。以下步骤将包含我们单词的列分解为单独的行，将我们最终的单词列表与字典连接，然后计算情感和情态词的使用：
- en: '[PRE39]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/00271.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00271.jpeg)'
- en: '[PRE40]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The next set of steps illustrates the use of the preceding pipeline for processing
    another story from our corpus. We can then compare these results to get a relative
    sense of pessimism (reflected by measures of negative sentiment) in the stories:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 下一组步骤演示了使用前面的管道处理我们语料库中的另一个故事。然后我们可以比较这些结果，以获得故事中悲观情绪的相对感知：
- en: '[PRE41]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](img/00272.jpeg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00272.jpeg)'
- en: '[PRE42]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Based on the following negative sentiment computation, we can conclude that
    this story is, relatively, more pessimistic than the previous one analyzed:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以下负面情感计算，我们可以得出结论，这个故事相对来说比之前分析的更悲观：
- en: '[PRE43]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the next section, we will shift our focus to identifying the major themes
    within a corpus of documents.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将把重点转移到识别文档语料库中的主要主题。
- en: Understanding themes in document corpuses
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解文档语料库中的主题
- en: Bag-of-words-based techniques can also be used to classify common themes in
    documents or to identify themes within a corpus of documents. Broadly, these techniques,
    like most, are attempting to reduce the dimensionality of the term-document matrix,
    based on each word's relation to latent variables in this case.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词袋技术也可以用于对文档中的常见主题进行分类，或者用于识别文档语料库中的主题。广义上讲，这些技术，像大多数技术一样，试图基于每个词与潜在变量的关系来减少术语-文档矩阵的维度。
- en: One of the earliest approaches to this type of classification was **Latent Semantic
    Analysis** (**LSA**). LSA can avoid the limitations of count-based methods associated
    with synonyms and terms with multiple meanings. Over the years, the concept of
    LSA has evolved into another model called **Latent Dirichlet Allocation** (**LDA**).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类的最早方法之一是**潜在语义分析**（**LSA**）。LSA 可以避免与同义词和具有多重含义的术语相关的基于计数的方法的限制。多年来，LSA
    的概念演变成了另一个称为**潜在狄利克雷分配**（**LDA**）的模型。
- en: LDA allows us to identify latent thematic structure within a collection of documents.
    Both LSA and LDA use the term-document matrix for reducing the dimensionality
    of the term space and for producing the topic weights. A constraint of both the
    LSA and LDA techniques is that they work best when applied to large documents.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 允许我们在一系列文档中识别潜在的主题结构。LSA 和 LDA 都使用术语-文档矩阵来降低术语空间的维度，并生成主题权重。LSA 和 LDA 技术的一个限制是它们在应用于大型文档时效果最佳。
- en: For more detailed explanation of LDA, refer to *Latent Dirichlet Allocation*,by
    David M. Blei, Andrew Y. Ng, and Michael I. Jordan, at [http://ai.stanford.edu/~ang/papers/jair03-lda.pdf](http://ai.stanford.edu/~ang/papers/jair03-lda.pdf).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 LDA 的更详细解释，请参阅 David M. Blei、Andrew Y. Ng 和 Michael I. Jordan 的*潜在狄利克雷分配*，网址为[http://ai.stanford.edu/~ang/papers/jair03-lda.pdf](http://ai.stanford.edu/~ang/papers/jair03-lda.pdf)。
- en: Now, we present an example of using LDA over a corpus of XML documents.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们展示了在 XML 文档语料库上使用 LDA 的示例。
- en: 'Start the Spark shell with the package for reading XML documents as we will
    be reading an XML-based corpus in this section:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用包含用于读取 XML 文档的包启动 Spark shell，因为我们将在本节中读取基于 XML 的语料库：
- en: '[PRE44]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, we define a few constants for the number of topics, maximum number of
    iterations, and the vocabulary size, as shown:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了一些常量，包括主题数量、最大迭代次数和词汇量大小，如下所示：
- en: '[PRE45]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `PERMISSIVE` mode, used as follows, allows us to continue creating a DataFrame
    even while encountering corrupt records during parsing. The `rowTag` parameter
    specifies the XML node to be read. Here, we are interested in the sentences present
    in the document for our topic's analysis using LDA.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`PERMISSIVE` 模式如下所示，允许我们在解析过程中遇到损坏记录时继续创建 DataFrame。`rowTag` 参数指定要读取的 XML 节点。在这里，我们对使用
    LDA 进行主题分析时文档中的句子感兴趣。'
- en: The Dataset for this example contains 4,000 Australian legal cases from the
    **Federal Court of Australia** (**FCA**), and can be downloaded from [https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports](https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的数据集包含来自**澳大利亚联邦法院**（**FCA**）的 4,000 个法律案例，可从[https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports](https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports)下载。
- en: 'We read in all the case files, as demonstrated:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取所有案例文件，如下所示：
- en: '[PRE46]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we generate document IDs for each of the legal cases, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为每个法律案例生成文档 ID，如下所示：
- en: '[PRE47]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We use a `CountVectorizer` (and `CountVectorizerModel`) to convert our collection
    of legal documents into vectors of token counts. Here, we do not have a priori
    dictionary available, so the `CountVectorizer` is used as an Estimator to extract
    the vocabulary and generate a `CountVectorizerModel`. The model produces sparse
    representations for the documents over the vocabulary, which is then passed to
    the LDA algorithm. During the fitting process, the `CountVectorizer` will select
    the top `vocabSize` words ordered by term frequency across the corpus:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`CountVectorizer`（和`CountVectorizerModel`）将我们的法律文件集转换为标记计数的向量。在这里，我们没有预先的词典可用，因此`CountVectorizer`被用作估计器来提取词汇并生成`CountVectorizerModel`。该模型为文档生成了在词汇表上的稀疏表示，然后传递给
    LDA 算法。在拟合过程中，`CountVectorizer`将选择跨语料库按词项频率排序的前`vocabSize`个词：
- en: '[PRE48]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We compute the log-likelihood and the log-perplexity of the LDA model, as shown.
    A model with higher likelihood implies a better model. Similarly, lower perplexity
    represents a better model:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算 LDA 模型的对数似然和对数困惑度，如下所示。具有更高似然的模型意味着更好的模型。同样，更低的困惑度代表更好的模型：
- en: '[PRE49]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we use the `describeTopics()` function to display the topics described
    by their top-weighted terms, as illustrated:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`describeTopics()`函数显示由其权重最高的术语描述的主题，如下所示：
- en: '[PRE50]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](img/00273.jpeg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00273.jpeg)'
- en: '[PRE51]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![](img/00274.jpeg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00274.jpeg)'
- en: 'We can display the results containing the actual terms corresponding to the
    term indices, as follows. It is quite evident from the words displayed here that
    we are dealing with a legal corpus:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显示包含与术语索引对应的实际术语的结果，如下所示。从这里显示的词语中很明显，我们正在处理一个法律语料库。
- en: '[PRE52]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The next main topic in textual analysis is collocated words. For some words,
    much of their meaning is derived from their collocation with other words. Predicting
    word meaning based on collocation is generally one of the most common extensions
    beyond the simple bag-of-words approach.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析的下一个主要主题是搭配词。对于一些词来说，它们的意义很大程度上来自于与其他词的搭配。基于搭配来预测词义通常是简单词袋方法之外最常见的扩展之一。
- en: In the next section, we will examine the use of Naive Bayes classifier on n-gram.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将研究在 n-gram 上使用朴素贝叶斯分类器。
- en: Using Naive Bayes classifiers
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类器
- en: Naive Bayes classifiers are a family of probabilistic classifiers based on applying
    the Bayes' conditional probability theorem. These classifiers assume independence
    between the features. Naive Bayes is often the baseline method for text categorization
    with word frequencies as the feature set. Despite the strong independence assumptions,
    the Naive Bayes classifiers are fast and easy to implement; hence, they are used
    very commonly in practice.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器是一类基于贝叶斯条件概率定理的概率分类器。这些分类器假设特征之间相互独立。朴素贝叶斯通常是文本分类的基准方法，使用词频作为特征集。尽管存在强烈的独立性假设，朴素贝叶斯分类器快速且易于实现；因此，在实践中它们被广泛使用。
- en: While Naive Bayes is very popular, it also suffers from errors that can lead
    to favoring of one class over the other(s). For example, skewed data can cause
    the classifier to favor one class over another. Similarly, the independence assumption
    can lead to erroneous classification weights that favor one class over another.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管朴素贝叶斯非常受欢迎，但它也存在可能导致偏向某一类别的错误。例如，倾斜的数据可能导致分类器偏向某一类别。同样，独立性假设可能导致错误的分类权重，偏向某一类别。
- en: For specific heuristics for dealing with problems associated with Naive Bayes
    classifers, refer to *Tackling the Poor Assumptions of Naive Bayes Text Classifiers*,
    by Rennie, Shih, et al at [https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 有关处理朴素贝叶斯分类器相关问题的具体启发式方法，请参阅Rennie，Shih等人的《解决朴素贝叶斯文本分类器的错误假设》[https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf)。
- en: One of the main advantages of the Naive Bayes approach is that it does not require
    a large training Dataset to estimate the parameters necessary for classification.
    Among the various approaches for word classification using supervised machine
    learning, the Naive Bayes method is extremely popular; for example, which sentences
    in an annual report can be classified as being "negative," "positive," or "neutral".
    Naive Bayes method is also most commonly used with n-grams and **Support Vector
    Machines** (**SVMs**).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯方法的主要优点之一是它不需要大量的训练数据集来估计分类所需的参数。在使用监督机器学习进行单词分类的各种方法中，朴素贝叶斯方法非常受欢迎；例如，年度报告中哪些句子可以被分类为“负面”、“正面”或“中性”。朴素贝叶斯方法也是最常与n-grams和支持向量机（SVMs）一起使用的。
- en: N-grams are used for a variety of different tasks. For example, n-grams can
    be used for developing features for supervised machine learning models such as
    SVMs, Maximum Entropy models, and Naive Bayes. When the value of `N` is `1`, the
    n-grams are referred to as unigrams (essentially, the individual words in a sentence;
    when the value of `N` is `2`, they are called bigrams, when `N` is `3`, they are
    called trigrams, and so on. The main idea here is to use tokens such as bigrams
    in the feature space instead of individual words or unigrams.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: N-grams用于各种不同的任务。例如，n-grams可用于为监督机器学习模型（如SVMs，最大熵模型和朴素贝叶斯）开发特征。当`N`的值为`1`时，n-grams被称为unigrams（实质上是句子中的单个词；当`N`的值为`2`时，它们被称为bigrams，当`N`为`3`时，它们被称为trigrams，依此类推。这里的主要思想是在特征空间中使用bigrams等标记，而不是单个词或unigrams。
- en: The Dataset for this example contains approximately 1.69 million Amazon reviews
    for the electronics category, and can be downloaded from [http://jmcauley.ucsd.edu/data/amazon/](http://jmcauley.ucsd.edu/data/amazon/).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例的数据集包含大约169万条电子产品类别的亚马逊评论，可从[http://jmcauley.ucsd.edu/data/amazon/](http://jmcauley.ucsd.edu/data/amazon/)下载。
- en: For a more detailed explanation of steps used in this example, check out Natural
    Language Processing with Apache Spark ML and Amazon Reviews (Parts 1 & 2), Mike
    Seddon, at [http://mike.seddon.ca/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/](http://mike.seddon.ca/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此示例中使用的步骤的更详细解释，请查看Mike Seddon的《使用Apache Spark ML和亚马逊评论进行自然语言处理》（第1和第2部分）[http://mike.seddon.ca/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/](http://mike.seddon.ca/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/)。
- en: 'First, we read the input JSON file to create our input DataFrame:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们读取输入的JSON文件以创建我们的输入DataFrame：
- en: '[PRE53]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![](img/00275.jpeg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00275.jpeg)'
- en: 'You can print the schema, as shown:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以打印模式，如下所示：
- en: '[PRE54]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, we print out the count of records for each of the rating values, as follows.
    Note that the number of records is highly skewed in favor of rating five. This
    skew can impact our results in favor of rating five versus the other ratings:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打印出每个评分值的记录数，如下所示。请注意，记录数在评分五方面严重倾斜。这种倾斜可能会影响我们的结果，使评分五相对于其他评分更有利：
- en: '[PRE55]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We create a view from the DataFrame, as follows. This step can conveniently
    help us create a more balanced training DataFrame containing an equal number of
    records from each of the rating categories:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从DataFrame创建一个视图，如下所示。这一步可以方便地帮助我们创建一个更平衡的训练DataFrame，其中包含每个评分类别的相等数量的记录：
- en: '[PRE56]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, we create our training and test Datasets using the row numbers:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用行号创建我们的训练和测试数据集：
- en: '[PRE57]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'In the given steps, we tokenize our text, remove stop words, and create bigrams
    and trigrams:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的步骤中，我们对文本进行标记化，去除停用词，并创建bigrams和trigrams：
- en: '[PRE58]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'In the next steps, we define `HashingTF` instances for the unigrams, the bigrams,
    and the trigrams:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们为unigrams、bigrams和trigrams定义`HashingTF`实例：
- en: '[PRE59]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Then, we create an instance of the Naive Bayes classifier:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个朴素贝叶斯分类器的实例：
- en: '[PRE60]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We assemble our processing pipeline, as illustrated:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们组装我们的处理管道，如图所示：
- en: '[PRE61]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We create a parameter grid to be used for cross-validation to arrive at the
    best set of parameters for our model, as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个参数网格，用于交叉验证以得到我们模型的最佳参数集，如下所示：
- en: '[PRE62]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In the next step, note that a k-fold cross-validation performs model selection
    by splitting the Dataset into a set of non-overlapping, randomly partitioned folds
    that are used as separate training and test Datasets; for example, with `k=3`
    folds, K-fold cross-validation will generate three (training, test) Dataset pairs,
    each of which uses `2/3` of the data for training and `1/3` for testing.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，请注意，k折交叉验证通过将数据集分割为一组不重叠的、随机分区的折叠来执行模型选择；例如，对于`k=3`折叠，k折交叉验证将生成三个（训练、测试）数据集对，每个数据集对使用数据的`2/3`进行训练和`1/3`进行测试。
- en: 'Each fold is used as the test set exactly once. To evaluate a particular `ParamMap`,
    `CrossValidator` computes the average evaluation metric for the three models produced
    by fitting the Estimator on the two different (training, test) Dataset pairs.
    After identifying the best `ParamMap`, `CrossValidator` finally refits the Estimator
    using the best `ParamMap` on the entire Dataset:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 每个折叠都恰好被用作测试集一次。为了评估特定的`ParamMap`，`CrossValidator`计算在两个不同（训练，测试）数据集对上拟合估计器产生的三个模型的平均评估指标。在识别出最佳的`ParamMap`后，`CrossValidator`最终使用整个数据集使用最佳的`ParamMap`重新拟合估计器：
- en: '[PRE63]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The prediction results obtained are not good after executing the previous set
    of steps. Let''s check whether we can improve the results by reducing the number
    of review categories and increase the number of records in our training set, as
    illustrated:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行前一组步骤后，获得的预测结果并不理想。让我们检查是否可以通过减少评论类别的数量并增加训练集中的记录数量来改善结果，如图所示：
- en: '[PRE64]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![](img/00276.jpeg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00276.jpeg)'
- en: '[PRE65]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The label converter can be used to recover the original labels'' text for improved
    readability in case of textual labels:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 标签转换器可用于恢复原始标签的文本，以提高可读性，如果是文本标签的话：
- en: '[PRE66]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, we create an instance of our Naive Bayes classifier:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建我们朴素贝叶斯分类器的一个实例：
- en: '[PRE67]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We assemble our pipeline with all the Transformers and the Naive Bayes estimator,
    as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用所有转换器和朴素贝叶斯估计器组装我们的管道，如下所示：
- en: '[PRE68]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We use cross-validation to select the best parameters for our model, as shown:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用交叉验证来选择模型的最佳参数，如图所示：
- en: '[PRE69]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Note the significant improvement in our prediction results as a result of clubbing
    ratings into fewer categories and increasing the number of records for training
    our model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意将评级合并为较少类别并增加训练模型的记录数量后，我们的预测结果显著改善。
- en: In the next section, we will present an example of machine learning on textual
    data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍一个关于文本数据的机器学习示例。
- en: Developing a machine learning application
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发一个机器学习应用程序
- en: In this section, we will present a machine learning example for textual analysis.
    Refer to [Chapter 6](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL in Machine Learning Applications*, for more details about the machine
    learning code presented in this section.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一个文本分析的机器学习示例。有关本节中提供的机器学习代码的更多详细信息，请参阅[第6章](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c)，*在机器学习应用中使用Spark
    SQL*。
- en: The Dataset used in the following example contains 1,080 documents of free text
    business descriptions of Brazilian companies categorized into a subset of nine
    categories. You can download this Dataset from [https://archive.ics.uci.edu/ml/datasets/CNAE-9](https://archive.ics.uci.edu/ml/datasets/CNAE-9).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例中使用的数据集包含巴西公司的自由文本业务描述的1,080个文档，分类为九个子类别。您可以从[https://archive.ics.uci.edu/ml/datasets/CNAE-9](https://archive.ics.uci.edu/ml/datasets/CNAE-9)下载此数据集。
- en: '[PRE70]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next, we define a schema for the input records:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为输入记录定义一个模式：
- en: '[PRE71]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We then use the schema to convert the RDD to a DataFrame, as illustrated:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用模式将RDD转换为DataFrame，如图所示：
- en: '[PRE72]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '![](img/00277.jpeg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00277.jpeg)'
- en: 'Next, we add an index column to the DataFrame using the `monotonically_increasing_id()`
    function, as shown:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`monotonically_increasing_id()`函数向DataFrame添加索引列，如图所示：
- en: '[PRE73]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '![](img/00278.jpeg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00278.jpeg)'
- en: 'In the following step, we assemble the feature vector:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们组装特征向量：
- en: '[PRE74]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We split the input Dataset into the training (90 percent of the records) and
    test (10 percent of the records) Datasets:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入数据集分为训练数据集（记录的90％）和测试数据集（记录的10％）：
- en: '[PRE75]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'In the following step, we create and fit the logistic regression model on our
    training Dataset:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们在训练数据集上创建并拟合逻辑回归模型：
- en: '[PRE76]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We can list the parameters of the model, as illustrated:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以列出模型的参数，如图所示：
- en: '[PRE77]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Next, we display the coefficients and intercept values for our logistic regression
    model:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们显示逻辑回归模型的系数和截距值：
- en: '[PRE78]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Next, we use the model to make predictions on the test Dataset, as demonstrated:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用模型对测试数据集进行预测，如图所示：
- en: '[PRE79]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Select example rows to display the key columns in the predictions DataFrame,
    as shown:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 选择示例行以显示预测DataFrame中的关键列，如图所示：
- en: '[PRE80]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '![](img/00279.jpeg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00279.jpeg)'
- en: 'We use an evaluator to compute the test error, as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用评估器来计算测试错误，如下所示：
- en: '[PRE81]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The results of applying a logistic regression model to our Dataset are not
    particularly good. Now, we define parameter grid to explore whether a better set
    of parameters can improve the overall prediction results for our model:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 将逻辑回归模型应用于我们的数据集的结果并不特别好。现在，我们定义参数网格，以探索是否更好的参数集可以改善模型的整体预测结果：
- en: '[PRE82]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Here, we show the use of a `CrossValidator` to pick the better parameters for
    our model:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了使用`CrossValidator`来选择模型的更好参数：
- en: '[PRE83]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Next, we run the cross-validation to choose the best set of parameters:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们运行交叉验证以选择最佳的参数集：
- en: '[PRE84]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'We make predictions on the test Dataset. Here, `cvModel` uses the best model
    found (`lrModel`):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对测试数据集进行预测。这里，`cvModel`使用找到的最佳模型（`lrModel`）：
- en: '[PRE85]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '![](img/00280.jpeg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00280.jpeg)'
- en: '[PRE86]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '![](img/00281.jpeg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00281.jpeg)'
- en: '[PRE87]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Note the significant improvement in the prediction accuracy as a result of
    cross-validation:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 注意交叉验证结果中预测准确率的显著提高：
- en: '[PRE88]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Finally, save the model to the filesystem. We will retrieve it from the filesystem
    in the program later in this section:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将模型保存到文件系统。我们将在本节的程序中从文件系统中检索它：
- en: '[PRE89]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Next, we build an application, compile, package, and execute it using the code
    and the saved model and test data from our spark-shell session, as shown:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建一个应用程序，编译，打包，并使用来自我们spark-shell会话的代码和保存的模型和测试数据来执行它，如图所示：
- en: '**LRExample.scala**'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**LRExample.scala**'
- en: '[PRE90]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Create a `lib` folder inside your root `SBT` directory (the same as the directory
    containing the `build.sbt` file) and copy `scopt_2.11-3.3.0.jar` and `spark-examples_2.11-2.2.1-SNAPSHOT.jar`
    from spark distribution's `examples` directory into it.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在根`SBT`目录（包含`build.sbt`文件的目录）内创建一个`lib`文件夹，并将spark分发的`examples`目录中的`scopt_2.11-3.3.0.jar`和`spark-examples_2.11-2.2.1-SNAPSHOT.jar`复制到其中。
- en: 'Next, compile and package the source using the same `build.sbt` file used in
    an earlier section, as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用与前一节中相同的`build.sbt`文件编译和打包源代码，如下所示：
- en: '[PRE91]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Finally, execute your Spark Scala program using spark-submit, as shown:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用spark-submit执行您的Spark Scala程序，如下所示：
- en: '[PRE92]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced a few Spark SQL applications in the textual analysis
    space. Additionally, we provided detailed code examples, including building a
    data preprocessing pipeline, implementing sentiment analysis, using Naive Bayes
    classifier with n-grams, and implementing an LDA application to identify themes
    in a document corpus. Additionally, we worked through the details of implementing
    an example of machine learning.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些在文本分析领域中使用Spark SQL的应用程序。此外，我们提供了详细的代码示例，包括构建数据预处理流水线，实现情感分析，使用朴素贝叶斯分类器与n-gram，并实现一个LDA应用程序来识别文档语料库中的主题。另外，我们还详细介绍了实现一个机器学习示例的细节。
- en: In the next chapter, we will focus on use cases for using Spark SQL in deep
    learning applications. We will explore a few of the emerging deep learning libraries
    and present examples of implementing deep learning related applications.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将专注于在深度学习应用中使用Spark SQL的用例。我们将探索一些新兴的深度学习库，并提供实现深度学习相关应用程序的示例。
