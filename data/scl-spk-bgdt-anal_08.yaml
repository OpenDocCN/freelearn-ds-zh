- en: Introduce a Little Structure - Spark SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介结构 - Spark SQL
- en: '"One machine can do the work of fifty ordinary men. No machine can do the work
    of one extraordinary man."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “一台机器可以做五十个普通人能做的工作，但没有一台机器能做一个非凡人能做的工作。”
- en: '- Elbert Hubbard'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 埃尔伯特·哈伯德'
- en: 'In this chapter, you will learn how to use Spark for the analysis of structured
    data (unstructured data, such as a document containing arbitrary text or some
    other format has to be transformed into a structured form); we will see how DataFrames/datasets
    are the corner stone here, and how Spark SQL''s APIs make querying structured
    data simple yet robust. Moreover, we introduce datasets and see the difference
    between datasets, DataFrames, and RDDs. In a nutshell, the following topics will
    be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用 Spark 来分析结构化数据（如需将无结构数据，比如包含任意文本或其他格式的文档，转换为结构化形式）；我们将看到 DataFrames/datasets
    在这里是基础，Spark SQL 的 API 如何使查询结构化数据既简单又强大。此外，我们还会介绍 datasets，并探讨 datasets、DataFrames
    和 RDDs 之间的区别。总的来说，本章将涵盖以下主题：
- en: Spark SQL and DataFrames
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL 和 DataFrames
- en: DataFrame and SQL API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame 和 SQL API
- en: DataFrame schema
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame 模式
- en: datasets and encoders
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集和编码器
- en: Loading and saving data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据加载和保存
- en: Aggregations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合操作
- en: Joins
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接操作
- en: Spark SQL and DataFrames
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL 和 DataFrames
- en: Before Apache Spark, Apache Hive was the go-to technology whenever anyone wanted
    to run an SQL-like query on a large amount of data. Apache Hive essentially translated
    SQL queries into MapReduce-like, like logic, automatically making it very easy
    to perform many kinds of analytics on big data without actually learning to write
    complex code in Java and Scala.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Spark 之前，Apache Hive 是每当需要对大量数据运行 SQL 类似查询时的首选技术。Apache Hive 实质上将 SQL
    查询转换为类似 MapReduce 的逻辑，从而自动使得执行各种大数据分析变得非常简单，而不需要学习编写复杂的 Java 和 Scala 代码。
- en: With the advent of Apache Spark, there was a paradigm shift in how we can perform
    analysis on big data scale. Spark SQL provides an easy-to-use SQL-like layer on
    top of Apache Spark's distributed computation abilities. In fact, Spark SQL can
    be used as an online analytical processing database.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Apache Spark 的出现，我们在大数据规模上执行分析的方式发生了范式转变。Spark SQL 提供了一个易于使用的 SQL 类似层，建立在
    Apache Spark 的分布式计算能力之上。实际上，Spark SQL 可以作为一个在线分析处理数据库使用。
- en: '![](img/00297.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00297.jpeg)'
- en: Spark SQL works by parsing the SQL-like statement into an **Abstract Syntax
    Tree** (**AST**), subsequently converting that plan to a logical plan and then
    optimizing the logical plan into a physical plan that can be executed. The final
    execution uses the underlying DataFrame API, making it very easy for anyone to
    use DataFrame APIs by simply using an SQL-like interface rather than learning
    all the internals. Since this book dives into technical details of various APIs,
    we will primarily cover the DataFrame APIs, showing Spark SQL API in some places
    to contrast the different ways of using the APIs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 的工作原理是通过将 SQL 类似语句解析为**抽象语法树**（**AST**），随后将该计划转换为逻辑计划，并优化逻辑计划为可执行的物理计划。最终执行使用底层的
    DataFrame API，这使得任何人都可以通过使用类似 SQL 的接口而不是学习所有内部实现，轻松使用 DataFrame API。由于本书深入探讨了各种
    API 的技术细节，我们将主要介绍 DataFrame API，并在某些地方展示 Spark SQL API，以对比不同的 API 使用方式。
- en: Thus, DataFrame API is the underlying layer beneath Spark SQL. In this chapter,
    we will show you how to create DataFrames using various techniques, including
    SQL queries and performing operations on the DataFrames.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DataFrame API 是 Spark SQL 底层的基础层。在本章中，我们将展示如何使用各种技术创建 DataFrames，包括 SQL 查询和对
    DataFrame 执行操作。
- en: A DataFrame is an abstraction of the **Resilient Distributed dataset** (**RDD**),
    dealing with higher level functions optimized using catalyst optimizer and also
    highly performant via the Tungsten Initiative. You can think of a dataset as an
    efficient table of an RDD with heavily optimized binary representation of the
    data. The binary representation is achieved using encoders, which serializes the
    various objects into a binary structure for much better performance than RDD representation.
    Since DataFrames uses the RDD internally anyway, a DataFrame/dataset is also distributed
    exactly like an RDD, and thus is also a distributed dataset. Obviously, this also
    means datasets are immutable.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 是 **弹性分布式数据集**（**RDD**）的抽象，处理通过 Catalyst 优化器优化的更高级功能，并且通过 Tungsten
    Initiative 实现高性能。你可以将数据集看作是 RDD 的高效表格，并具有经过优化的二进制数据表示。二进制表示是通过编码器实现的，编码器将各种对象序列化为二进制结构，从而提供比
    RDD 表示更好的性能。由于 DataFrame 内部无论如何都使用 RDD，因此 DataFrame/数据集也像 RDD 一样分布式，因此它也是一个分布式数据集。显然，这也意味着数据集是不可变的。
- en: 'The following is an illustration of the binary representation of data:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据的二进制表示示意图：
- en: '![](img/00037.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00037.jpeg)'
- en: datasets were added in Spark 1.6 and provide the benefits of strong typing on
    top of DataFrames. In fact, since Spark 2.0, the DataFrame is simply an alias
    of a dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集（datasets）在 Spark 1.6 中被添加，并在 DataFrame 之上提供强类型的好处。事实上，从 Spark 2.0 开始，DataFrame
    实际上是数据集的别名。
- en: '`org.apache.spark.sql` defines type `DataFrame` as a `dataset[Row]`, which
    means that most of the APIs will work well with both datasets and `DataFrames`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`org.apache.spark.sql` 将类型 `DataFrame` 定义为 `dataset[Row]`，这意味着大多数 API 可以很好地与数据集和
    `DataFrame` 配合使用。'
- en: '**type DataFrame = dataset[Row]**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**type DataFrame = dataset[Row]**'
- en: A DataFrame is conceptually similar to a table in a Relational Database. Hence,
    a DataFrame contains rows of data, with each row comprised of several columns.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 在概念上类似于关系数据库中的表。因此，DataFrame 包含数据行，每行由多个列组成。
- en: One of the first things we need to keep in mind is that, just like RDDs, DataFrames
    are immutable. This property of DataFrames being immutable means every transformation
    or action creates a new DataFrame.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要记住的第一件事是，像 RDD 一样，DataFrame 也是不可变的。DataFrame 的这种不可变性意味着每个转换或操作都会创建一个新的 DataFrame。
- en: '![](img/00034.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00034.jpeg)'
- en: Let's start looking more into DataFrames and how they are different from RDDs.
    RDD's, as seen before, represent a low-level API of data manipulation in Apache
    Spark. The DataFrames were created on top of RDDs to abstract the low-level inner
    workings of RDDs and expose high-level APIs, which are easier to use and provide
    a lot of functionality out-of-the-box. DataFrame was created by following similar
    concepts found in the Python pandas package, R language, Julia language, and so
    on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始深入了解 DataFrame 以及它们如何与 RDD 不同。正如之前所见，RDD 是 Apache Spark 中的数据操作的低级 API。DataFrame
    是在 RDD 之上创建的，目的是抽象化 RDD 的低级内部工作原理，并暴露出更高层次的 API，这些 API 更易于使用，并且提供了大量开箱即用的功能。DataFrame
    的创建遵循了类似于 Python pandas 包、R 语言、Julia 语言等中的概念。
- en: As we mentioned before, DataFrames translate the SQL code and domain specific
    language expressions into optimized execution plans to be run on top of Spark
    Core APIs in order for the SQL statements to perform a wide variety of operations.
    DataFrames support many different types of input data sources and many types of
    operations. These includes all types of SQL operations, such as joins, group by,
    aggregations, and window functions, as most of the databases. Spark SQL is also
    quite similar to the Hive query language, and since Spark provides a natural adapter
    to Apache Hive, users who have been working in Apache Hive can easily transfer
    their knowledge, applying it to Spark SQL, thus minimizing the transition time.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，DataFrame 将 SQL 代码和特定领域语言表达式转换为优化执行计划，这些计划将在 Spark Core API 上运行，以便
    SQL 语句能够执行各种操作。DataFrame 支持多种不同类型的输入数据源和操作类型。包括所有类型的 SQL 操作，如连接、分组、聚合和窗口函数，类似于大多数数据库。Spark
    SQL 也非常类似于 Hive 查询语言，并且由于 Spark 提供了与 Apache Hive 的自然适配器，已经在 Apache Hive 中工作的用户可以轻松地将他们的知识转移到
    Spark SQL，从而最大限度地减少过渡时间。
- en: DataFrames essentially depend on the concept of a table, as seen previously.
    The table can be operated on very similar to how Apache Hive works. In fact, many
    of the operations on the tables in Apache Spark are similar to how Apache Hive
    handles tables and operates on those tables. Once you have a table that is the
    DataFrame, the DataFrame can be registered as a table and you can operate on the
    data using Spark SQL statements in lieu of DataFrame APIs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames 本质上依赖于表的概念，正如之前所见。可以非常类似于 Apache Hive 的方式操作表。事实上，Apache Spark 中对表的许多操作与
    Apache Hive 处理表及操作表的方式非常相似。一旦有了作为 DataFrame 的表，可以将 DataFrame 注册为表，并且可以使用 Spark
    SQL 语句操作数据，而不是使用 DataFrame API。
- en: DataFrames depend on the catalyst optimizer and the Tungsten performance improvements,
    so let's briefly examine how catalyst optimizer works. A catalyst optimizer creates
    a parsed logical plan from the input SQL and then analyzes the logical plan by
    looking at all the various attributes and columns used in the SQL statement. Once
    the analyzed logical plan is created, catalyst optimizer further tries to optimize
    the plan by combining several operations and also rearranging the logic to get
    better performance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames 依赖于催化剂优化器和钨丝性能改进，因此让我们简要地看一下催化剂优化器是如何工作的。催化剂优化器从输入的 SQL 创建一个解析后的逻辑计划，然后通过查看
    SQL 语句中使用的所有各种属性和列来分析这个逻辑计划。一旦分析完逻辑计划，催化剂优化器进一步尝试通过合并多个操作和重新排列逻辑来优化计划，以获得更好的性能。
- en: In order to understand the catalyst optimizer, think about it as a common sense
    logic Optimizer which can reorder operations such as filters and transformations,
    sometimes grouping several operations into one so as to minimize the amount of
    data that is shuffled across the worker nodes. For example, catalyst optimizer
    may decide to broadcast the smaller datasets when performing joint operations
    between different datasets. Use explain to look at the execution plan of any data
    frame. The catalyst optimizer also computes statistics of the DataFrame's columns
    and partitions, improving the speed of execution.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解催化剂优化器，可以将其看作是一个常识逻辑优化器，它可以重新排序诸如过滤器和转换等操作，有时将多个操作组合成一个操作，以尽量减少在工作节点之间传输的数据量。例如，催化剂优化器可能决定在不同数据集之间执行联合操作时广播较小的数据集。使用
    explain 查看任何数据框的执行计划。催化剂优化器还计算 DataFrame 的列和分区的统计信息，提高执行速度。
- en: For example, if there are transformations and filters on the data partitions,
    then the order in which we filter data and apply transformations matters a lot
    to the overall performance of the operations. As a result of all the optimizations,
    the optimized logical plan is generated, which is then converted into a physical
    plan. Obviously, several physical plans are possibilities to execute the same
    SQL statement and generate the same result. The cost optimization logic determines
    and picks a good physical plan, based on cost optimizations and estimations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果数据分区上有转换和过滤操作，则过滤数据和应用转换的顺序对操作的整体性能至关重要。由于所有优化，生成了优化的逻辑计划，然后将其转换为物理计划。显然，有几个物理计划可以执行相同的
    SQL 语句并生成相同的结果。成本优化逻辑根据成本优化和估算选择一个好的物理计划。
- en: Tungsten performance improvements are another key ingredient in the secret sauce
    behind the phenomenal performance improvements offered by Spark 2.x compared to
    the previous releases, such as Spark 1.6 and older. Tungsten implements a complete
    overhaul of memory management and other performance improvements. Most important
    memory management improvements use binary encoding of the objects and referencing
    them in both off-heap and on-heap memory. Thus, Tungsten allows the usage of office
    heap memory using the binary encoding mechanism to encode all the objects. Binary
    encoded objects take up much less memory. Project Tungsten also improves shuffle
    performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 钨丝性能改进是 Spark 2.x 提供的显著性能改进背后的秘密武器之一，与之前的版本如 Spark 1.6 和更早的版本相比。钨丝实现了对内存管理和其他性能改进的完全改造。最重要的内存管理改进使用对象的二进制编码，并在堆外和堆内存中引用它们。因此，钨丝允许使用二进制编码机制来编码所有对象以使用办公堆内存。二进制编码的对象占用的内存要少得多。项目钨丝还改进了洗牌性能。
- en: The data is typically loaded into DataFrames through the `DataFrameReader`,
    and data is saved from DataFrames through `DataFrameWriter`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通常通过 `DataFrameReader` 将数据加载到 DataFrames 中，并通过 `DataFrameWriter` 从 DataFrames
    中保存数据。
- en: DataFrame API and SQL API
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame API 和 SQL API
- en: 'The creation of a DataFrame can be done in several ways:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据框可以通过多种方式进行：
- en: By executing SQL queries
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过执行SQL查询
- en: Loading external data such as Parquet, JSON, CSV, text, Hive, JDBC, and so on
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载外部数据，如Parquet、JSON、CSV、文本、Hive、JDBC等
- en: Converting RDDs to data frames
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将RDD转换为数据框
- en: A DataFrame can be created by loading a CSV file. We will look at a CSV `statesPopulation.csv`,
    which is being loaded as a DataFrame.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过加载CSV文件来创建一个数据框。我们将查看一个名为`statesPopulation.csv`的CSV文件，它被加载为一个数据框。
- en: The CSV has the following format of US states populations from years 2010 to
    2016.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该CSV文件包含2010年到2016年间美国各州的人口数据格式。
- en: '| **State** | **Year** | **Population** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **州** | **年份** | **人口** |'
- en: '| Alabama | 2010 | 4785492 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 阿拉巴马州 | 2010 | 4785492 |'
- en: '| Alaska | 2010 | 714031 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 阿拉斯加州 | 2010 | 714031 |'
- en: '| Arizona | 2010 | 6408312 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 亚利桑那州 | 2010 | 6408312 |'
- en: '| Arkansas | 2010 | 2921995 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 阿肯色州 | 2010 | 2921995 |'
- en: '| California | 2010 | 37332685 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 加利福尼亚州 | 2010 | 37332685 |'
- en: Since this CSV has a header, we can use it to quickly load into a DataFrame
    with an implicit schema detection.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此CSV文件包含标题行，我们可以使用它快速加载到数据框中，并进行隐式的模式检测。
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the DataFrame is loaded, it can be examined for the schema:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据框加载完成，就可以检查其模式：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`option("header", "true").option("inferschema", "true").option("sep", ",")`
    tells Spark that the CSV has a `header`; a comma separator is used to separate
    the fields/columns and also that schema can be inferred implicitly.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`option("header", "true").option("inferschema", "true").option("sep", ",")`告诉Spark，CSV文件包含`header`，使用逗号作为分隔符，并且可以隐式推断模式。'
- en: DataFrame works by parsing the logical plan, analyzing the logical plan, optimizing
    the plan, and then finally executing the physical plan of execution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框通过解析逻辑计划、分析逻辑计划、优化计划，然后最终执行物理执行计划来工作。
- en: 'Using explain on DataFrame shows the plan of execution:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`explain`命令对数据框进行查看，可以显示执行计划：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A DataFrame can also be registered as a table name (shown as follows), which
    will then allow you to type SQL statements like a relational Database.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框也可以注册为表名（如下所示），这将允许你像关系型数据库一样编写SQL语句。
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once we have the DataFrame as a structured DataFrame or a table, we can run
    commands to operate on the data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了结构化的数据框或表格，我们就可以运行命令来对数据进行操作：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you see in the preceding piece of code, we have written an SQL-like statement
    and executed it using `spark.sql` API.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看前面的代码片段，我们已经编写了一个类似SQL的语句，并使用`spark.sql` API执行它。
- en: Note that the Spark SQL is simply converted to the DataFrame API for execution
    and the SQL is only a DSL for ease of use.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Spark SQL实际上是转换为数据框API进行执行，SQL只是一个简化使用的领域特定语言（DSL）。
- en: Using the `sort` operation on the DataFrame, you can order the rows in the DataFrame
    by any column. We see the effect of descending `sort` using the `Population` column
    as follows. The rows are ordered by the Population in a descending order.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据框上的`sort`操作，你可以按任何列对数据框中的行进行排序。以下是使用`Population`列进行降序`sort`操作的效果。行将按照人口的降序进行排序。
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Using `groupBy` we can group the DataFrame by any column. The following is the
    code to group the rows by `State` and then add up the `Population` counts for
    each `State`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`groupBy`我们可以按任何列对数据框进行分组。以下是按`State`分组行，然后为每个`State`的`Population`数量求和的代码。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using the `agg` operation, you can perform many different operations on columns
    of the DataFrame, such as finding the `min`, `max`, and `avg` of a column. You
    can also perform the operation and rename the column at the same time to suit
    your use case.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`agg`操作，你可以对数据框中的列执行多种不同的操作，例如查找列的`min`、`max`和`avg`。你还可以同时执行操作并重命名列，以适应你的用例。
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Naturally, the more complicated the logic gets, the execution plan also gets
    more complicated. Let''s look at the plan for the preceding operation of `groupBy`
    and `agg` API invocations to better understand what is really going on under the
    hood. The following is the code showing the execution plan of the group by and
    summation of population per `State`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，逻辑越复杂，执行计划也越复杂。让我们查看之前`groupBy`和`agg` API调用的执行计划，以更好地理解背后发生了什么。以下是显示按`State`分组并对人口进行求和的执行计划的代码：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: DataFrame operations can be chained together very well so that the execution
    takes advantage of the cost optimization (Tungsten performance improvements and
    catalyst optimizer working together).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框操作可以很好地链式连接，这样执行就能利用成本优化（Tungsten性能改进和Catalyst优化器协同工作）。
- en: 'We can also chain the operations together in a single statement, as shown as
    follows, where we not only group the data by `State` column and then sum the `Population`
    value, but also sort the DataFrame by the summation column:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将操作链式连接在一个语句中，如下所示，其中我们不仅按 `State` 列对数据进行分组并汇总 `Population` 值，还按汇总列对 DataFrame
    进行排序：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding chained operation consists of multiple transformations and actions,
    which can be visualized using the following diagram:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上述链式操作包含了多个转换和操作，可以通过以下图表来可视化：
- en: '![](img/00042.jpeg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00042.jpeg)'
- en: 'It''s also possible to create multiple aggregations at the same time, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以同时创建多个聚合，如下所示：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Pivots
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据透视
- en: Pivoting is a great way of transforming the table to create a different view,
    more suitable to doing many summarizations and aggregations. This is accomplished
    by taking the values of a column and making each of the values an actual column.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据透视是一种很好的方法，可以将表格转化为不同的视图，更适合进行多项汇总和聚合。这是通过获取列的值，并将每个值转化为一个实际的列来实现的。
- en: To understand this better, let's pivot the rows of the DataFrame by `Year` and
    examine the result, which shows that, now, the column `Year` created several new
    columns by converting each unique value into an actual column. The end result
    of this is that, now, instead of just looking at year columns, we can use the
    per year columns created to summarize and aggregate by `Year`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，让我们按 `Year` 对 DataFrame 的行进行数据透视，并查看结果。这显示现在 `Year` 列通过将每个唯一值转换为一个实际的列，创建了几个新的列。最终结果是，现在我们不仅仅查看年份列，而是可以使用每年的列来按
    `Year` 进行汇总和聚合。
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Filters
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过滤器
- en: DataFrame also supports Filters, which can be used to quickly filter the DataFrame
    rows to generate new DataFrames. The Filters enable very important transformations
    of the data to narrow down the DataFrame to our use case. For example, if all
    you want is to analyze the state of California, then using `filter` API performs
    the elimination of non-matching rows on every partition of data, thus improving
    the performance of the operations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 还支持过滤器（Filters），可以快速过滤 DataFrame 行并生成新的 DataFrame。过滤器使数据的转换变得非常重要，可以将
    DataFrame 精简到我们的使用场景。例如，如果你只想分析加利福尼亚州的情况，那么使用 `filter` API 可以在每个数据分区上执行非匹配行的删除，从而提高操作的性能。
- en: Let's look at the execution plan for the filtering of the DataFrame to only
    consider the state of California.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看过滤 DataFrame 的执行计划，以仅考虑加利福尼亚州的情况。
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that we can seen the execution plan, let''s now execute the `filter` command,
    as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到执行计划，接下来执行 `filter` 命令，如下所示：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: User-Defined Functions (UDFs)
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户自定义函数（UDFs）
- en: UDFs define new column-based functions that extend the functionality of Spark
    SQL. Often, the inbuilt functions provided in Spark do not handle the exact need
    we have. In such cases, Apache Spark supports the creation of UDFs, which can
    be used.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: UDF 定义了基于列的新的函数，扩展了 Spark SQL 的功能。通常，Spark 中内置的函数无法处理我们所需的精确功能。在这种情况下，Apache
    Spark 支持创建 UDF，我们可以使用它们。
- en: '`udf()` internally calls a case class User-Defined Function, which itself calls
    ScalaUDF internally.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`udf()` 内部调用了一个案例类用户自定义函数，它内部又调用了 ScalaUDF。'
- en: Let's go through an example of an UDF which simply converts State column values
    to uppercase.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来了解一个简单的用户自定义函数（UDF），该函数将 State 列的值转换为大写。
- en: First, we create the function we need in Scala.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在 Scala 中创建所需的函数。
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Then, we have to encapsulate the created function inside the `udf` to create
    the UDF.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须将创建的函数封装在 `udf` 中，以创建 UDF。
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that we have created the `udf`, we can use it to convert the State column
    to uppercase.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了 `udf`，可以使用它将 State 列转换为大写。
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Schema   structure of data
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构  数据的结构
- en: A schema is the description of the structure of your data and can be either
    Implicit or Explicit.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 架构是数据结构的描述，可以是隐式的或显式的。
- en: Since the DataFrames are internally based on the RDD, there are two main methods
    of converting existing RDDs into datasets. An RDD can be converted into a dataset
    by using reflection to infer the schema of the RDD. A second method for creating
    datasets is through a programmatic interface, using which you can take an existing
    RDD and provide a schema to convert the RDD into a dataset with schema.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DataFrame 内部基于 RDD，因此将现有 RDD 转换为数据集有两种主要方法。可以通过反射推断 RDD 的架构，从而将 RDD 转换为数据集。创建数据集的第二种方法是通过编程接口，使用该接口可以提供现有的
    RDD 和架构，将 RDD 转换为带架构的数据集。
- en: In order to create a DataFrame from an RDD by inferring the schema using reflection,
    the Scala API for Spark provides case classes which can be used to define the
    schema of the table. The DataFrame is created programmatically from the RDD, because
    the case classes are not easy to use in all cases. For instance, creating a case
    classes on a 1000 column table is time consuming.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过反射推断模式从 RDD 创建 DataFrame，Spark 的 Scala API 提供了案例类，可以用来定义表的模式。DataFrame 是通过程序化方式从
    RDD 创建的，因为在所有情况下使用案例类并不容易。例如，在一个有 1000 列的表上创建案例类会非常耗时。
- en: Implicit schema
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐式模式
- en: Let us look at an example of loading a **CSV** (c**omma-separated Values**)
    file into a DataFrame. Whenever a text file contains a header, read API can infer
    the schema by reading the header line. We also have the option to specify the
    separator to be used to split the text file lines.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个将 **CSV**（逗号分隔值）文件加载到 DataFrame 中的例子。每当文本文件包含表头时，读取 API 可以通过读取表头行来推断模式。我们也可以指定用于分隔文本文件行的分隔符。
- en: We read the `csv` inferring the schema from the header line and uses comma (`,`)
    as the separator. We also show use of `schema` command and `printSchema` command
    to verify the schema of the input file.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取 `csv` 文件，通过表头推断模式，并使用逗号（`,`) 作为分隔符。我们还展示了 `schema` 命令和 `printSchema` 命令的使用，来验证输入文件的模式。
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Explicit schema
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显式模式
- en: A schema is described using `StructType`, which is a collection of `StructField`
    objects.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 模式通过 `StructType` 描述，它是 `StructField` 对象的集合。
- en: '`StructType` and `StructField` belong to the `org.apache.spark.sql.types` package.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`StructType` 和 `StructField` 属于 `org.apache.spark.sql.types` 包。'
- en: DataTypes such as `IntegerType`, `StringType` also belong to the `org.apache.spark.sql.types`
    package.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`IntegerType`、`StringType` 等数据类型也属于 `org.apache.spark.sql.types` 包。'
- en: Using these imports, we can define a custom explicit schema.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些导入，我们可以定义一个自定义的显式模式。
- en: 'First, import the necessary classes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入必要的类：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Define a schema with two columns/fields-an `Integer` followed by a `String`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个包含两列/字段的模式——一个 `Integer` 字段，后跟一个 `String` 字段：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It''s easy to print the newly created `schema`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 打印新创建的 `schema` 非常简单：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'There is also an option to print JSON, which is as follows, using `prettyJson`
    function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以选择打印 JSON，方法如下，使用 `prettyJson` 函数：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'All the data types of Spark SQL are located in the package `org.apache.spark.sql.types`.
    You can access them by doing:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 的所有数据类型都位于 `org.apache.spark.sql.types` 包中。你可以通过以下方式访问它们：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Encoders
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器
- en: Spark 2.x supports a different way of defining schema for complex data types.
    First, let's look at a simple example.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.x 支持为复杂数据类型定义模式的另一种方式。首先，我们来看一个简单的例子。
- en: 'Encoders must be imported using the import statement in order for you to use
    Encoders:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 必须通过 import 语句导入编码器，以便使用编码器：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s look at a simple example of defining a tuple as a data type to be used
    in the dataset APIs:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个简单的例子，定义一个元组作为数据类型，并在数据集 API 中使用它：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding code looks complicated to use all the time, so we can also define
    a case class for our need and then use it. We can define a case class `Record`
    with two fields-an `Integer` and a `String`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码看起来总是很复杂，因此我们也可以为需求定义一个案例类，然后使用它。我们可以定义一个名为 `Record` 的案例类，包含两个字段——一个 `Integer`
    和一个 `String`：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using `Encoders` , we can easily create a `schema` on top of the case class,
    thus allowing us to use the various APIs with ease:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Encoders`，我们可以轻松地在案例类上创建一个 `schema`，从而轻松使用各种 API：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'All the data types of Spark SQL are located in the package **`org.apache.spark.sql.types`**.
    You can access them by doing:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 的所有数据类型都位于包 **`org.apache.spark.sql.types`** 中。你可以通过以下方式访问它们：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should use the `DataTypes` object in your code to create complex Spark
    SQL types such as arrays or maps, as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在代码中使用 `DataTypes` 对象来创建复杂的 Spark SQL 类型，例如数组或映射，如下所示：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following are the data types supported in Spark SQL APIs:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Spark SQL API 中支持的数据类型：
- en: '| **Data type** | **Value type in Scala** | **API to access or create a data
    type** |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **数据类型** | **Scala 中的值类型** | **访问或创建数据类型的 API** |'
- en: '| `ByteType` | `Byte` | `ByteType` |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| `ByteType` | `Byte` | `ByteType` |'
- en: '| `ShortType` | `Short` | `ShortType` |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| `ShortType` | `Short` | `ShortType` |'
- en: '| `IntegerType` | `Int` | `IntegerType` |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| `IntegerType` | `Int` | `IntegerType` |'
- en: '| `LongType` | `Long` | `LongType` |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| `LongType` | `Long` | `LongType` |'
- en: '| `FloatType` | `Float` | `FloatType` |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| `FloatType` | `Float` | `FloatType` |'
- en: '| `DoubleType` | `Double` | `DoubleType` |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| `DoubleType` | `Double` | `DoubleType` |'
- en: '| `DecimalType` | `java.math.BigDecimal` | `DecimalType` |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| `DecimalType` | `java.math.BigDecimal` | `DecimalType` |'
- en: '| `StringType` | `String` | `StringType` |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| `StringType` | `String` | `StringType` |'
- en: '| `BinaryType` | `Array[Byte]` | `BinaryType` |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| `BinaryType` | `Array[Byte]` | `BinaryType` |'
- en: '| `BooleanType` | `Boolean` | `BooleanType` |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| `BooleanType` | `Boolean` | `BooleanType` |'
- en: '| `TimestampType` | `java.sql.Timestamp` | `TimestampType` |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| `TimestampType` | `java.sql.Timestamp` | `TimestampType` |'
- en: '| `DateType` | `java.sql.Date` | `DateType` |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| `DateType` | `java.sql.Date` | `DateType` |'
- en: '| `ArrayType` | `scala.collection.Seq` | `ArrayType(elementType, [containsNull])`
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| `ArrayType` | `scala.collection.Seq` | `ArrayType(elementType, [containsNull])`
    |'
- en: '| `MapType` | `scala.collection.Map` | `MapType(keyType, valueType, [valueContainsNull])`
    Note: The default value of `valueContainsNull` is `true`. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| `MapType` | `scala.collection.Map` | `MapType(keyType, valueType, [valueContainsNull])`
    注意：`valueContainsNull` 的默认值为 `true`。 |'
- en: '| `StructType` | `org.apache.spark.sql.Row` | `StructType(fields)` Note: fields
    is a `Seq` of `StructFields`. Also, two fields with the same name are not allowed.
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `StructType` | `org.apache.spark.sql.Row` | `StructType(fields)` 注意：fields
    是 `Seq` 类型的 `StructFields`。此外，不允许有两个同名的字段。 |'
- en: Loading and saving datasets
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和保存数据集
- en: We need to have data read into the cluster as input and output or results written
    back to the storage to do anything practical with our code. Input data can be
    read from a variety of datasets and sources such as Files, Amazon S3 storage,
    Databases, NoSQLs, and Hive, and the output can similarly also be saved to Files,
    S3, Databases, Hive, and so on.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将数据读取到集群作为输入，并将输出或结果写回存储，这样才能对代码做实际操作。输入数据可以从多种数据集和来源读取，如文件、Amazon S3 存储、数据库、NoSQL
    和 Hive，输出也可以保存到文件、S3、数据库、Hive 等。
- en: Several systems have support for Spark via a connector, and this number is growing
    day by day as more systems are latching onto the Spark processing framework.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接器，多个系统已经支持 Spark，随着越来越多的系统接入 Spark 处理框架，这一数字正在日益增长。
- en: Loading datasets
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Spark SQL can read data from external storage systems such as files, Hive tables,
    and JDBC databases through the `DataFrameReader` interface.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 可以通过 `DataFrameReader` 接口从外部存储系统（如文件、Hive 表和 JDBC 数据库）读取数据。
- en: The format of the API call is `spark.read.inputtype`
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: API 调用的格式是 `spark.read.inputtype`
- en: Parquet
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet
- en: CSV
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV
- en: Hive Table
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive 表
- en: JDBC
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDBC
- en: ORC
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORC
- en: Text
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本
- en: JSON
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: 'Let''s look at a couple of simple examples of reading CSV files into DataFrames:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看几个简单的例子，展示如何将 CSV 文件读取到 DataFrame 中：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Saving datasets
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存数据集
- en: Spark SQL can save data to external storage systems such as files, Hive tables
    and JDBC databases through `DataFrameWriter` interface.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 可以通过 `DataFrameWriter` 接口将数据保存到外部存储系统，如文件、Hive 表和 JDBC 数据库。
- en: The format of the API call is `dataframe``.write.outputtype`
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: API 调用的格式是 `dataframe``.write.outputtype`
- en: Parquet
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet
- en: ORC
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORC
- en: Text
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本
- en: Hive table
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive 表
- en: JSON
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: CSV
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV
- en: JDBC
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDBC
- en: 'Let''s look at a couple of examples of writing or saving a DataFrame to a CSV
    file:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看几个将 DataFrame 写入或保存为 CSV 文件的示例：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Aggregations
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合
- en: Aggregation is the method of collecting data based on a condition and performing
    analytics on the data. Aggregation is very important to make sense of data of
    all sizes, as just having raw records of data is not that useful for most use
    cases.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合是根据某个条件收集数据并对其进行分析的方法。聚合对于理解各种规模的数据非常重要，因为仅仅拥有原始数据记录对于大多数用例来说并不那么有用。
- en: For example, if you look at the following table and then the aggregated view,
    it is obvious that just raw records do not help you understand the data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，如果你查看下面的表格和它的聚合视图，显然仅仅是原始记录并不能帮助你理解数据。
- en: Imagine a table containing one temperature measurement per day for every city
    in the world for five years.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，一个表格，其中记录了世界上每个城市五年内每天的气温测量数据。
- en: 'Shown in the following is a table containing records of average temperature
    per day per city:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了一个包含每天每个城市的平均气温记录的表格：
- en: '| **City** | **Date** | **Temperature** |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| **城市** | **日期** | **气温** |'
- en: '| Boston | 12/23/2016 | 32 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 波士顿 | 2016/12/23 | 32 |'
- en: '| New York | 12/24/2016 | 36 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 纽约 | 2016/12/24 | 36 |'
- en: '| Boston | 12/24/2016 | 30 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 波士顿 | 2016/12/24 | 30 |'
- en: '| Philadelphia | 12/25/2016 | 34 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 费城 | 2016/12/25 | 34 |'
- en: '| Boston | 12/25/2016 | 28 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 波士顿 | 2016/12/25 | 28 |'
- en: 'If we want to compute the average temperature per city for all the days we
    have measurements for in the above table, we can see results which look similar
    to the following table:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想计算上表中每个城市的日均气温，我们可以看到的结果将类似于下面的表格：
- en: '| **City** | **Average Temperature** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **城市** | **平均气温** |'
- en: '| Boston | 30 - *(32 + 30 + 28)/3* |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 波士顿 | 30 - *(32 + 30 + 28)/3* |'
- en: '| New York | 36 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 纽约 | 36 |'
- en: '| Philadelphia | 34 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 费城 | 34 |'
- en: Aggregate functions
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合函数
- en: Most aggregations can be done using functions that can be found in the `org.apache.spark.sql.functions`
    package. In addition, custom aggregation functions can also be created, also known
    as **User Defined Aggregation Functions** (**UDAF**).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数聚合操作可以使用在`org.apache.spark.sql.functions`包中找到的函数来完成。此外，还可以创建自定义聚合函数，称为**用户定义的聚合函数**（**UDAF**）。
- en: Each grouping operation returns a `RelationalGroupeddataset`, on which you can
    specify aggregations.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分组操作返回一个`RelationalGroupeddataset`，你可以在其上指定聚合操作。
- en: 'We will load the sample data to illustrate all the different types of aggregate
    functions in this section:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载示例数据，以说明本节中所有不同类型的聚合函数：
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Count
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数
- en: Count is the most basic aggregate function, which simply counts the number of
    rows for the column specified. An extension is the `countDistinct`, which also
    eliminates duplicates.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`count`是最基本的聚合函数，它简单地计算指定列的行数。其扩展版本是`countDistinct`，它还会去除重复项。'
- en: 'The `count` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`count` API有多种实现方式，具体使用哪个API取决于特定的使用场景：'
- en: '[PRE32]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s look at examples of invoking `count` and `countDistinct` on the DataFrame
    to print the row counts:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些在DataFrame上调用`count`和`countDistinct`的示例，来打印行数：
- en: '[PRE33]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: First
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一行
- en: Gets the first record in the `RelationalGroupeddataset.`
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 获取`RelationalGroupeddataset`中的第一条记录。
- en: 'The `first` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`first` API有多种实现方式，具体使用哪个API取决于特定的使用场景：'
- en: '[PRE34]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s look at an example of invoking `first` on the DataFrame to output the
    first row:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`first`的示例，来输出第一行：
- en: '[PRE35]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Last
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后一行
- en: Gets the last record in the `RelationalGroupeddataset`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 获取`RelationalGroupeddataset`中的最后一条记录。
- en: 'The `last` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`last` API有多种实现方式，具体使用哪个API取决于特定的使用场景：'
- en: '[PRE36]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Let's look at an example of invoking `last` on the DataFrame to output the last
    row.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`last`的示例，来输出最后一行。
- en: '[PRE37]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: approx_count_distinct
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: approx_count_distinct
- en: Approximate distinct count is much faster at approximately counting the distinct
    records rather than doing an exact count, which usually needs a lot of shuffles
    and other operations. While the approximate count is not 100% accurate, many use
    cases can perform equally well even without an exact count.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 近似的不同计数比进行精确计数要快得多，因为精确计数通常需要很多数据重分区和其他操作。虽然近似计数不是100%准确，但在许多使用场景下，即使没有精确计数，表现也能一样好。
- en: The `approx_count_distinct` API has several implementations, as follows. The
    exact API used depends on the specific use case.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`approx_count_distinct` API有多种实现方式，具体使用哪个API取决于特定的使用场景。'
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s look at an example of invoking `approx_count_distinct` on the DataFrame
    to print the approximate count of the DataFrame:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`approx_count_distinct`的示例，来打印DataFrame的近似计数：
- en: '[PRE39]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Min
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小值
- en: The minimum of the column value of one of the columns in the DataFrame. An example
    is if you want to find the minimum temperature of a city.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame中某一列的最小值。例如，如果你想找出某个城市的最低温度。
- en: 'The `min` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`min` API有多种实现方式，具体使用哪个API取决于特定的使用场景：'
- en: '[PRE40]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s look at an example of invoking `min` on the DataFrame to print the minimum
    Population:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`min`的示例，来打印最小人口：
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Max
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大值
- en: The maximum of the column value of one of the columns in the DataFrame. An example
    is if you want to find the maximum temperature of a city.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame中某一列的最大值。例如，如果你想找出某个城市的最高温度。
- en: The `max` API has several implementations, as follows. The exact API used depends
    on the specific use case.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`max` API有多种实现方式，具体使用哪个API取决于特定的使用场景。'
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s look at an example of invoking `max` on the DataFrame to print the maximum
    Population:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`max`的示例，来打印最大人口：
- en: '[PRE43]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Average
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均值
- en: The average of the values is calculated by adding the values and dividing by
    the number of values.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 值的平均值通过将所有值相加然后除以值的个数来计算。
- en: Average of 1,2,3 is (1 + 2 + 3) / 3 = 6/3 = 2
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 1、2、3的平均值是(1 + 2 + 3) / 3 = 6 / 3 = 2
- en: 'The `avg` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`avg` API有多种实现方式，具体使用哪个API取决于特定的使用场景：'
- en: '[PRE44]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s look at an example of invoking `avg` on the DataFrame to print the average
    population:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`avg`的示例，来打印平均人口：
- en: '[PRE45]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Sum
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 求和
- en: Computes the sum of the values of the column. Optionally, `sumDistinct` can
    be used to only add up distinct values.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 计算列中值的总和。可以选择使用 `sumDistinct` 来仅计算不同值的总和。
- en: 'The `sum` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`sum` API 有几种实现方式，具体使用哪个 API 取决于特定的使用场景：'
- en: '[PRE46]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Let's look at an example of invoking `sum` on the DataFrame to print the summation
    (total) `Population`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个调用 `sum` 的例子，计算 DataFrame 中 `Population` 的总和：
- en: '[PRE47]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Kurtosis
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 峰度
- en: Kurtosis is a way of quantifying differences in the shape of distributions,
    which may look very similar in terms of means and variances, yet are actually
    different. In such cases, kurtosis becomes a good measure of the weight of the
    distribution at the tail of the distribution, as compared to the middle of the
    distribution.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 峰度是量化分布形状差异的一种方式，即使均值和方差看起来非常相似，它们的形状实际上却是不同的。在这种情况下，峰度成为衡量分布尾部相对于分布中部的权重的良好指标。
- en: The `kurtosis` API has several implementations, as follows. The exact API used
    depends on the specific use case.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`kurtosis` API 有几种实现方式，具体使用哪个 API 取决于特定的使用场景。'
- en: '[PRE48]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s look at an example of invoking `kurtosis` on the DataFrame on the `Population`
    column:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个调用 `kurtosis` 的例子，针对 `Population` 列的 DataFrame：
- en: '[PRE49]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Skewness
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏度
- en: Skewness measures the asymmetry of the values in your data around the average
    or mean.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 偏度衡量数据中各值围绕平均值或均值的非对称性。
- en: The `skewness` API has several implementations, as follows. The exact API used
    depends on the specific use case.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`skewness` API 有几种实现方式，具体使用哪个 API 取决于特定的使用场景。'
- en: '[PRE50]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let''s look at an example of invoking `skewness` on the DataFrame on the Population
    column:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个调用 `skewness` 的例子，针对 `Population` 列的 DataFrame：
- en: '[PRE51]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Variance
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方差
- en: Variance is the average of the squared differences of each of the values from
    the mean.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 方差是每个值与均值的平方差的平均值。
- en: 'The `var` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`var` API 有几种实现方式，具体使用哪个 API 取决于特定的使用场景：'
- en: '[PRE52]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, let''s look at an example of invoking `var_pop` on the DataFrame measuring
    variance of `Population`:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个调用 `var_pop` 的例子，计算 `Population` 的方差：
- en: '[PRE53]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Standard deviation
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准差
- en: Standard deviation is the square root of the variance (see previously).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差是方差的平方根（参见前文）。
- en: 'The `stddev` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`stddev` API 有几种实现方式，具体使用哪个 API 取决于特定的使用场景：'
- en: '[PRE54]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s look at an example of invoking `stddev` on the DataFrame printing the
    standard deviation of `Population`:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个调用 `stddev` 的例子，打印 `Population` 的标准差：
- en: '[PRE55]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Covariance
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协方差
- en: Covariance is a measure of the joint variability of two random variables. If
    the greater values of one variable mainly corresponds with the greater values
    of the other variable, and the same holds for the lesser values, then the variables
    tend to show similar behavior and the covariance is positive. If the opposite
    is true, and the greater values of one variable correspond with the lesser values
    of the other variable, then the covariance is negative.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差是衡量两个随机变量联合变异性的指标。如果一个变量的较大值与另一个变量的较大值主要对应，而较小值也同样如此，那么这两个变量趋向于表现出相似的行为，协方差为正。如果情况相反，一个变量的较大值与另一个变量的较小值对应，那么协方差为负。
- en: The `covar` API has several implementations, as follows. The exact API used
    depends on the specific use case.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`covar` API 有几种实现方式，具体使用哪个 API 取决于特定的使用场景。'
- en: '[PRE56]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s look at an example of invoking `covar_pop` on the DataFrame to calculate
    the covariance between the year and population columns:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个调用 `covar_pop` 的例子，计算年份与人口列之间的协方差：
- en: '[PRE57]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: groupBy
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: groupBy
- en: A common task seen in data analysis is to group the data into grouped categories
    and then perform calculations on the resultant groups of data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，常见的任务之一是将数据分组到不同的类别中，然后对结果数据进行计算。
- en: A quick way to understand grouping is to imagine being asked to assess what
    supplies you need for your office very quickly. You could start looking around
    you and just group different types of items, such as pens, paper, staplers, and
    analyze what you have and what you need.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 理解分组的一种快速方法是想象自己被要求快速评估办公室所需的用品。你可以开始环顾四周，按不同类型的物品进行分组，如笔、纸张、订书机，并分析你有什么和需要什么。
- en: 'Let''s run `groupBy` function on the `DataFrame` to print aggregate counts
    of each State:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对`DataFrame`运行`groupBy`函数，以打印每个州的聚合计数：
- en: '[PRE58]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'You can also `groupBy` and then apply any of the aggregate functions seen previously,
    such as `min`, `max`, `avg`, `stddev`, and so on:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以先`groupBy`，然后应用之前看到的任何聚合函数，如`min`、`max`、`avg`、`stddev`等：
- en: '[PRE59]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Rollup
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汇总
- en: 'Rollup is a multi-dimensional aggregation used to perform hierarchical or nested
    calculations. For example, if we want to show the number of records for each State+Year
    group, as well as for each State (aggregating over all years to give a grand total
    for each `State` irrespective of the `Year`), we can use `rollup` as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总是用于执行层次或嵌套计算的多维聚合。例如，如果我们想显示每个`State+Year`组的记录数，以及每个`State`的记录数（汇总所有年份，给出每个州的总数，不考虑`Year`），我们可以按如下方式使用`rollup`：
- en: '[PRE60]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The `rollup` calculates the count for state and year, such as California+2014,
    as well as California state (adding up all years).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`rollup`计算州和年份的计数，例如加利福尼亚州+2014年，以及加利福尼亚州（汇总所有年份）。'
- en: Cube
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 立方体
- en: 'Cube is a multi-dimensional aggregation used to perform hierarchical or nested
    calculations just like rollup, but with the difference that cube does the same
    operation for all dimensions. For example, if we want to show the number of records
    for each `State` and `Year` group, as well as for each `State` (aggregating over
    all Years to give a grand total for each State irrespective of the `Year`), we
    can use rollup as follows. In addition, `cube` also shows a grand total for each
    Year (irrespective of the `State`):'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 立方体是用于执行层次或嵌套计算的多维聚合，类似于汇总，但不同的是立方体对所有维度执行相同的操作。例如，如果我们想显示每个`State`和`Year`组的记录数，以及每个`State`的记录数（汇总所有年份，给出每个州的总数，不考虑`Year`），我们可以按如下方式使用汇总。此外，`cube`还会显示每年的总计（不考虑`State`）：
- en: '[PRE61]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Window functions
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口函数
- en: 'Window functions allow you to perform aggregations over a window of data rather
    than entire data or some filtered data. The use cases of such window functions
    are:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口函数允许您在数据窗口而非整个数据或某些过滤数据上执行聚合操作。这类窗口函数的使用案例有：
- en: Cumulative sum
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累计和
- en: Delta from previous value for same key
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与前一个值的差异（对于相同的键）
- en: Weighted moving average
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权移动平均
- en: 'The best way to understand window functions is to imagine a sliding window
    over the larger dataset universe. You can specify a window looking at three rows
    T-1, T, and T+1, and by performing a simple calculation. You can also specify
    a window of latest/most recent ten values:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 理解窗口函数的最好方法是想象在更大的数据集宇宙中有一个滑动窗口。您可以指定一个窗口，查看三行T-1、T和T+1，并通过执行一个简单的计算。您还可以指定一个包含最新/最近十个值的窗口：
- en: '![](img/00047.jpeg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00047.jpeg)'
- en: The API for the window specification requires three properties, the `partitionBy()`,
    `orderBy()`, and the `rowsBetween()`. The `partitionBy` chunks the data into the
    partitions/groups as specified by `partitionBy()`. `orderBy()` is used to order
    the data within each partition of data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口规范的API需要三个属性：`partitionBy()`、`orderBy()`和`rowsBetween()`。`partitionBy`将数据分割成由`partitionBy()`指定的分区/组。`orderBy()`用于对每个数据分区中的数据进行排序。
- en: The `rowsBetween()` specifies the window frame or the span of the sliding window
    to perform the calculations.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`rowsBetween()`指定窗口帧或滑动窗口的跨度，以进行计算。'
- en: 'To try out the windows function, there are certain packages that are needed.
    You can import the necessary packages using import directives, as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试窗口函数，需要一些特定的包。您可以通过以下导入指令导入所需的包：
- en: '[PRE62]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now, you are ready to write some code to learn about the window functions. Let's
    create a window specification for the partitions sorted by `Population` and partitioned
    by `State`. Also, specify that we want to consider all rows until the current
    row as part of the `Window`.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已准备好编写代码来了解窗口函数。让我们创建一个窗口规范，对按`Population`排序并按`State`分区的分区进行排序。同时，指定我们希望将所有行视为`Window`的一部分，直到当前行。
- en: '[PRE63]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Compute the `rank` over the window specification. The result will be a rank
    (row number) added to each row, as long as it falls within the `Window` specified.
    In this example, we chose to partition by `State` and then order the rows of each
    `State` further by descending order. Hence, all State rows have their own rank
    numbers assigned.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 计算窗口规范上的`rank`。结果将是为每一行添加一个排名（行号），只要它落在指定的`Window`内。在这个例子中，我们选择按`State`进行分区，然后进一步按降序排列每个`State`的行。因此，每个州的行都有自己的排名号。
- en: '[PRE64]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: ntiles
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分位数
- en: The ntiles is a popular aggregation over a window and is commonly used to divide
    input dataset into n parts. For example, in predictive analytics, deciles (10
    parts) are often used to first group the data and then divide it into 10 parts
    to get a fair distribution of data. This is a natural function of the window function
    approach, hence ntiles is a good example of how window functions can help.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ntiles是窗口聚合中常见的一种方法，通常用于将输入数据集分为n个部分。例如，在预测分析中，十等分（10个部分）通常用于首先对数据进行分组，然后将其分为10个部分，以获得公平的数据分布。这是窗口函数方法的自然功能，因此ntiles是窗口函数如何提供帮助的一个很好的例子。
- en: 'For example, if we want to partition the `statesPopulationDF` by `State` (window
    specification was shown previously), order by population, and then divide into
    two portions, we can use `ntile` over the `windowspec`:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想按`State`对`statesPopulationDF`进行分区（如前所示的窗口规范），按人口排序，然后将其分为两部分，我们可以在`windowspec`上使用`ntile`：
- en: '[PRE65]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: As shown previously, we have used `Window` function and `ntile()` together to
    divide the rows of each `State` into two equal portions.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所示，我们已使用`Window`函数和`ntile()`一起将每个`State`的行分为两等份。
- en: A popular use of this function is to compute deciles used in data science Models.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 该功能的常见用途之一是计算数据科学模型中使用的十等分（deciles）。
- en: Joins
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接
- en: In traditional databases, joins are used to join one transaction table with
    another lookup table to generate a more complete view. For example, if you have
    a table of online transactions by customer ID and another table containing the
    customer city and customer ID, you can use join to generate reports on the transactions
    by city.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统数据库中，连接用于将一张交易表与另一张查找表连接，以生成更完整的视图。例如，如果你有一张按客户ID分类的在线交易表和另一张包含客户城市及客户ID的表，你可以使用连接来生成按城市分类的交易报告。
- en: '**Transactions table**: The following table has three columns, the **CustomerID**,
    the **Purchased item,** and how much the customer paid for the item:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**交易表**：以下表格包含三列，**客户ID**、**购买商品**以及客户为商品支付的价格：'
- en: '| **CustomerID** | **Purchased item** | **Price paid** |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| **客户ID** | **购买商品** | **支付价格** |'
- en: '| 1 | Headphone | 25.00 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 耳机 | 25.00 |'
- en: '| 2 | Watch | 100.00 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 手表 | 100.00 |'
- en: '| 3 | Keyboard | 20.00 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 键盘 | 20.00 |'
- en: '| 1 | Mouse | 10.00 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 鼠标 | 10.00 |'
- en: '| 4 | Cable | 10.00 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 电缆 | 10.00 |'
- en: '| 3 | Headphone | 30.00 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 耳机 | 30.00 |'
- en: '**Customer Info table:** The following table has two columns, the **CustomerID**
    and the **City** the customer lives in:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户信息表**：以下表格包含两列，**客户ID**和客户所在的**城市**：'
- en: '| **CustomerID** | **City** |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| **客户ID** | **城市** |'
- en: '| 1 | Boston |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 波士顿 |'
- en: '| 2 | New York |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 纽约 |'
- en: '| 3 | Philadelphia |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 费城 |'
- en: '| 4 | Boston |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 波士顿 |'
- en: 'Joining the transaction table with the customer info table will generate a
    view as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 将交易表与客户信息表连接将生成如下视图：
- en: '| **CustomerID** | **Purchased item** | **Price paid** | **City** |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| **客户ID** | **购买商品** | **支付价格** | **城市** |'
- en: '| 1 | Headphone | 25.00 | Boston |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 耳机 | 25.00 | 波士顿 |'
- en: '| 2 | Watch | 100.00 | New York |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 手表 | 100.00 | 纽约 |'
- en: '| 3 | Keyboard | 20.00 | Philadelphia |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 键盘 | 20.00 | 费城 |'
- en: '| 1 | Mouse | 10.00 | Boston |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 鼠标 | 10.00 | 波士顿 |'
- en: '| 4 | Cable | 10.00 | Boston |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 电缆 | 10.00 | 波士顿 |'
- en: '| 3 | Headphone | 30.00 | Philadelphia |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 耳机 | 30.00 | 费城 |'
- en: 'Now, we can use this joined view to generate a report of **Total sale price**
    by **City**:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个连接后的视图来生成按**城市**分类的**总销售价格**报告：
- en: '| **City** | **#Items** | **Total sale price** |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| **城市** | **#商品** | **总销售价格** |'
- en: '| Boston | 3 | 45.00 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 波士顿 | 3 | 45.00 |'
- en: '| Philadelphia | 2 | 50.00 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 费城 | 2 | 50.00 |'
- en: '| New York | 1 | 100.00 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 纽约 | 1 | 100.00 |'
- en: Joins are an important function of Spark SQL, as they enable you to bring two
    datasets together, as seen previously. Spark, of course, is not only meant to
    generate reports, but is used to process data on a petabyte scale to handle real-time
    streaming use cases, machine learning algorithms, or plain analytics. In order
    to accomplish these goals, Spark provides the API functions needed.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 连接是Spark SQL的重要功能，因为它允许你将两个数据集结合起来，如前所示。当然，Spark不仅仅用于生成报告，它还用于处理PB级别的数据，以应对实时流处理、机器学习算法或普通的分析任务。为了实现这些目标，Spark提供了所需的API函数。
- en: A typical join between two datasets takes place using one or more keys of the
    left and right datasets and then evaluates a conditional expression on the sets
    of keys as a Boolean expression. If the result of the Boolean expression returns
    true, then the join is successful, else the joined DataFrame will not contain
    the corresponding join.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的数据集连接通过使用左侧和右侧数据集的一个或多个键来完成，然后在键的集合上评估条件表达式作为布尔表达式。如果布尔表达式的结果为真，则连接成功，否则连接后的DataFrame将不包含相应的连接数据。
- en: 'The join API has 6 different implementations:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: Join API有6种不同的实现方式：
- en: '[PRE66]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We will use one of the APIs to understand how to use join APIs ; however, you
    can choose to use other APIs depending on the use case:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用其中一个API来理解如何使用join API；不过，您也可以根据使用场景选择其他API：
- en: '[PRE67]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Note that joins will be covered in detail in the next few sections.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，连接的详细内容将在接下来的几部分中讨论。
- en: Inner workings of join
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Join的内部工作原理
- en: Join works by operating on the partitions of a DataFrame using the multiple
    executors. However, the actual operations and the subsequent performance depends
    on the type of `join` and the nature of the datasets being joined. In the next
    section, we will look at the types of joins.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Join通过在多个执行器上操作DataFrame的分区来工作。然而，实际的操作和随后的性能取决于`join`的类型和所连接数据集的性质。在下一部分中，我们将讨论各种连接类型。
- en: Shuffle join
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Shuffle连接
- en: 'Join between two big datasets involves shuffle join where partitions of both
    left and right datasets are spread across the executors. Shuffles are expensive
    and it''s important to analyze the logic to make sure the distribution of partitions
    and shuffles is done optimally. The following is an illustration of how shuffle
    join works internally:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个大数据集之间进行连接时，涉及到shuffle join，其中左侧和右侧数据集的分区被分布到各个执行器中。Shuffle操作是非常昂贵的，因此必须分析逻辑，确保分区和shuffle的分布是最优的。以下是shuffle
    join如何在内部工作的示意图：
- en: '![](img/00166.jpeg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00166.jpeg)'
- en: Broadcast join
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播连接
- en: 'A join between one large dataset and a smaller dataset can be done by broadcasting
    the smaller dataset to all executors where a partition from the left dataset exists.
    The following is an illustration of how a broadcast join works internally:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个大数据集和一个小数据集之间进行连接时，可以通过将小数据集广播到所有执行器来完成，前提是左侧数据集的分区存在。以下是广播连接如何在内部工作的示意图：
- en: '![](img/00194.jpeg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00194.jpeg)'
- en: Join types
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接类型
- en: The following is a table of the different types of joins. This is important,
    as the choice made when joining two datasets makes all the difference in the output,
    and also the performance.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是不同类型的连接表。这非常重要，因为在连接两个数据集时所做的选择对结果和性能有着决定性影响。
- en: '| **Join type** | **Description** |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| **Join type** | **Description** |'
- en: '| **inner** | The inner join compares each row from *left* to rows from *right*
    and combines matched pair of rows from *left* and *right* datasets only when both
    have non-NULL values. |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| **inner** | 内连接将*left*中的每一行与*right*中的每一行进行比较，并且只有当两者都有非NULL值时，才会将匹配的*left*和*right*行组合在一起。
    |'
- en: '| **cross** | The cross join matches every row from *left* with every row from
    *right* generating a Cartesian cross product. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| **cross** | cross join将*left*中的每一行与*right*中的每一行进行匹配，生成一个笛卡尔积。 |'
- en: '| **outer, full, fullouter** | The full outer Join gives all rows in *left*
    and *right* filling in NULL if only in *right* or *left*. |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| **outer, full, fullouter** | 全外连接返回*left*和*right*中的所有行，如果某一侧没有数据，则填充NULL。
    |'
- en: '| **leftanti** | The leftanti Join gives only rows in *left* based on non-existence
    on *right* side. |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| **leftanti** | leftanti Join仅返回*left*中的行，基于*right*侧的不存在。 |'
- en: '| **left, leftouter** | The leftouter Join gives all rows in *left* plus common
    rows of *left* and *right* (inner join). Fills in NULL if not in *right*. |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| **left, leftouter** | leftouter Join返回*left*中的所有行以及*left*和*right*的共同行（内连接）。如果不在*right*中，则填充NULL。
    |'
- en: '| **leftsemi** | The leftsemi Join gives only rows in *left* based on existence
    on *right* side. The does not include *right-*side values. |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| **leftsemi** | leftsemi Join仅返回*left*中的行，基于*right*侧的存在。它不包括*right*侧的值。 |'
- en: '| **right, rightouter** | The rightouter Join gives all rows in *right* plus
    common rows of *left* and *right* (inner join). Fills in NULL if not in *left*.
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| **right, rightouter** | rightouter Join返回*right*中的所有行以及*left*和*right*的共同行（内连接）。如果不在*left*中，则填充NULL。
    |'
- en: We will examine how the different join types work by using the sample datasets.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用示例数据集来研究不同的连接类型是如何工作的。
- en: '[PRE68]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Inner join
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内连接
- en: Inner join results in rows from both `statesPopulationDF` and `statesTaxRatesDF`
    when state is non-NULL in both datasets.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 内连接（Inner join）会返回来自`statesPopulationDF`和`statesTaxRatesDF`的数据行，前提是两个数据集中`state`列的值都非空。
- en: '![](img/00095.jpeg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00095.jpeg)'
- en: 'Join the two datasets by the state column as follows:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 按照`state`列连接这两个数据集，如下所示：
- en: '[PRE69]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You can run the `explain()` on the `joinDF` to look at the execution plan:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对`joinDF`运行`explain()`，查看执行计划：
- en: '[PRE70]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Left outer join
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 左外连接（Left outer join）
- en: Left outer join results in all rows from `statesPopulationDF`, including any
    common in `statesPopulationDF` and `statesTaxRatesDF`.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 左外连接（Left outer join）返回`statesPopulationDF`中的所有行，包括`statesPopulationDF`和`statesTaxRatesDF`中共同的行。
- en: '![](img/00273.jpeg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00273.jpeg)'
- en: 'Join the two datasets by the state column, shown as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 按照`state`列连接这两个数据集，如下所示：
- en: '[PRE71]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Right outer join
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 右外连接
- en: Right outer join results in all rows from `statesTaxRatesDF`, including any
    common in `statesPopulationDF` and `statesTaxRatesDF`.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 右外连接（Right outer join）返回`statesTaxRatesDF`中的所有行，包括`statesPopulationDF`和`statesTaxRatesDF`中共同的行。
- en: '![](img/00319.jpeg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00319.jpeg)'
- en: 'Join the two datasets by the `State` column as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 按照`State`列连接这两个数据集，如下所示：
- en: '[PRE72]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Outer join
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外连接
- en: Outer join results in all rows from `statesPopulationDF` and `statesTaxRatesDF`.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 外连接（Outer join）返回`statesPopulationDF`和`statesTaxRatesDF`中的所有行。
- en: '![](img/00245.jpeg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00245.jpeg)'
- en: 'Join the two datasets by the `State` column as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 按照`State`列连接这两个数据集，如下所示：
- en: '[PRE73]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Left anti join
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 左反连接
- en: Left anti join results in rows from only `statesPopulationDF` if, and only if,
    there is NO corresponding row in `statesTaxRatesDF`.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 左反连接（Left anti join）只会返回来自`statesPopulationDF`的数据行，前提是`statesTaxRatesDF`中没有对应的行。
- en: '![](img/00072.jpeg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.jpeg)'
- en: 'Join the two datasets by the `State` column as follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 按照`State`列连接这两个数据集，如下所示：
- en: '[PRE74]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Left semi join
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 左半连接
- en: Left semi join results in rows from only `statesPopulationDF` if, and only if,
    there is a corresponding row in `statesTaxRatesDF`.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 左半连接（Left semi join）只会返回来自`statesPopulationDF`的数据行，前提是`statesTaxRatesDF`中有对应的行。
- en: '![](img/00097.jpeg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.jpeg)'
- en: 'Join the two datasets by the state column as follows:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 按照`state`列连接这两个数据集，如下所示：
- en: '[PRE75]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Cross join
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉连接
- en: Cross join matches every row from *left* with every row from *right,* generating
    a Cartesian cross product.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉连接（Cross join）会将*左*表的每一行与*右*表的每一行匹配，生成一个笛卡尔积。
- en: '![](img/00312.jpeg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00312.jpeg)'
- en: 'Join the two datasets by the `State` column as follows:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 按照`State`列连接这两个数据集，如下所示：
- en: '[PRE76]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: You can also use join with cross jointype instead of calling the cross join
    API. `statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State").isNotNull,
    "cross").count`.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`cross`连接类型来代替调用交叉连接（cross join）API。`statesPopulationDF.join(statesTaxRatesDF,
    statesPopulationDF("State").isNotNull, "cross").count`。
- en: Performance implications of join
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接的性能影响
- en: The join type chosen directly impacts the performance of the join. This is because
    joins require the shuffling of data between executors to execute the tasks, hence
    different joins, and even the order of the joins, need to be considered when using
    join.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的连接类型直接影响连接的性能。这是因为连接操作需要在执行器之间对数据进行洗牌，因此在使用连接时，需要考虑不同的连接类型，甚至连接的顺序。
- en: 'The following is a table you could use to refer to when writing `Join` code:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你在编写`Join`代码时可以参考的表格：
- en: '| **Join type** | **Performance considerations and tips** |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| **Join type** | **性能考虑和提示** |'
- en: '| **inner** | Inner join requires the left and right tables to have the same
    column. If you have duplicate or multiple copies of the keys on either the left
    or right side, the join will quickly blow up into a sort of a Cartesian join,
    taking a lot longer to complete than if designed correctly to minimize the multiple
    keys. |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| **inner** | 内连接要求左表和右表具有相同的列。如果左表或右表中有重复或多个键值，连接操作会迅速膨胀成一种笛卡尔连接，导致执行时间远长于正确设计的连接，后者能够最小化多个键的影响。
    |'
- en: '| **cross** | Cross Join matches every row from *left* with every row from
    *right,* generating a Cartesian cross product. This is to be used with caution,
    as this is the worst performant join, to be used in specific use cases only. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| **cross** | 交叉连接将*左*表的每一行与*右*表的每一行匹配，生成一个笛卡尔积。使用时需谨慎，因为这是性能最差的连接类型，应该仅在特定用例中使用。
    |'
- en: '| **outer, full, fullouter** | Fullouter Join gives all rows in *left* and
    *right* filling in NULL if only in *right* or *left*. If used on tables with little
    in common, can result in very large results and thus slow performance. |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| **outer, full, fullouter** | 完全外连接（Fullouter Join）返回*左*表和*右*表中的所有行，如果某行仅存在于*右*表或*左*表中，则填充为NULL。如果用于表之间的公共部分很少的情况，可能会导致结果非常大，从而影响性能。
    |'
- en: '| **leftanti** | Leftanti Join gives only rows in *left* based on non-existence
    on *right* side. Very good performance, as only one table is fully considered
    and the other is only checked for the join condition. |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| **leftanti** | Leftanti 连接仅根据*right*侧的不存不存在返回*left*中的行。性能非常好，因为只有一个表被完全考虑，另一个表只是根据连接条件进行检查。
    |'
- en: '| **left, leftouter** | Leftouter Join gives all rows in *left* plus common
    rows of *left* and *right* (inner join). Fills in NULL if not in *right*. If used
    on tables with little in common, can result in very large results and thus slow
    performance. |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| **left, leftouter** | Leftouter 连接返回所有来自*left*的行，并加上*left*和*right*的公共行（内连接）。如果在*right*中不存在，则填充
    NULL。如果在两个表之间的共同部分很少，可能会导致结果非常庞大，从而导致性能变慢。 |'
- en: '| **leftsemi** | Leftsemi Join gives only rows in *left* based on existence
    on *right* side. Does not include *right* side values. Very good performance,
    as only one table is fully considered and other is only checked for the join condition.
    |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| **leftsemi** | Leftsemi 连接仅根据*right*侧的存在返回*left*中的行。不会包括*right*侧的值。性能非常好，因为只有一个表被完全考虑，另一个表只是根据连接条件进行检查。
    |'
- en: '| **right, rightouter** | Rightouter Join gives all rows in *right* plus common
    rows of *left* and *right* (inner join). Fills in NULL if not in *left*. Performance
    is similar to the leftouter join mentioned previously in this table. |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| **right, rightouter** | Rightouter 连接提供了所有来自*right*的行，并加上*left*和*right*的公共行（内连接）。如果在*left*中不存在，则填充
    NULL。性能与前面表格中的左外连接相似。 |'
- en: Summary
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the origin of DataFrames and how Spark SQL provides
    the SQL interface on top of DataFrames. The power of DataFrames is such that execution
    times have decreased manyfold over original RDD-based computations. Having such
    a powerful layer with a simple SQL-like interface makes them all the more powerful.
    We also looked at various APIs to create, and manipulate DataFrames, as well as
    digging deeper into the sophisticated features of aggregations, including `groupBy`,
    `Window`, `rollup`, and `cubes`. Finally, we also looked at the concept of joining
    datasets and the various types of joins possible, such as inner, outer, cross,
    and so on.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了 DataFrame 的起源以及 Spark SQL 如何在 DataFrame 上提供 SQL 接口。DataFrame 的强大之处在于，执行时间相比原始的
    RDD 计算大幅缩短。拥有这样一个强大的层次结构，并且具有简单的 SQL 类似接口，使其更加高效。我们还探讨了各种 API 来创建和操作 DataFrame，同时深入了解了聚合操作的复杂特性，包括
    `groupBy`、`Window`、`rollup` 和 `cubes`。最后，我们还研究了数据集连接的概念以及可能的各种连接类型，如内连接、外连接、交叉连接等。
- en: In the next chapter, we will explore the exciting world of real-time data processing
    and analytics in the [Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c),
    *Stream Me Up, Scotty - Spark Streaming*.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探索实时数据处理和分析的精彩世界，内容见[第9章](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c)，*Stream
    Me Up, Scotty - Spark Streaming*。
