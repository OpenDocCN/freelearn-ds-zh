- en: 14 Outlier Detection Using Unsupervised Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 无监督机器学习中的异常值检测
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区，参与Discord讨论
- en: '![](img/file0.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/file0.png)'
- en: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
- en: In *Chapter 8*, *Outlier Detection Using Statistical Methods*, you explored
    parametric and non-parametric statistical techniques to spot potential outliers.
    The methods were simple, interpretable, and yet quite effective.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8章*，*使用统计方法的异常值检测*中，你探讨了使用参数和非参数统计技术来识别潜在的异常值。这些方法简单、可解释，而且相当有效。
- en: Outlier detection is not straightforward, mainly due to the ambiguity surrounding
    the definition of what an outlier is, specific to your data or the problem that
    you are trying to solve. For example, though common, some of the thresholds used
    in *Chapter 8*, *Outlier Detection Using Statistical Methods*, are still arbitrary
    and not a rule that you must follow. Therefore, having domain knowledge or access
    to **Subject Matter Experts** (**SMEs**) is vital to making the proper judgment
    when spotting outliers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值检测并非简单直观，主要是由于异常值的定义存在模糊性，这种定义取决于你的数据或你试图解决的问题。例如，尽管常见，但*第8章*中使用的某些阈值，*使用统计方法的异常值检测*，仍然是任意的，并非你必须遵循的规则。因此，拥有领域知识或能够访问**主题专家**（**SMEs**）对于正确判断异常值至关重要。
- en: In this chapter, you will be introduced to a handful of machine learning-based
    methods for outlier detection. Most of the machine learning techniques for outlier
    detection are considered *unsupervised* outlier detection methods, such as **Isolation
    Forests** (**iForest**), unsupervised **K-Nearest Neighbors** (**KNN**), **Local
    Outlier Factor** (**LOF**), and **Copula-Based Outlier Detection** (**COPOD**),
    to name a few.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将会接触到基于机器学习的几种异常值检测方法。大多数机器学习异常值检测技术被视为*无监督*异常值检测方法，例如**孤立森林**（**iForest**）、无监督**K-近邻算法**（**KNN**）、**局部异常因子**（**LOF**）以及**基于Copula的异常值检测**（**COPOD**）等。
- en: Generally, outliers (or anomalies) are considered a rare occurrence (later in
    the chapter, you will see this referenced as the contamination percentage). In
    other words, you would assume a small fraction of your data are outliers in a
    large data set. For example, 1% of the data may be potential outliers. However,
    this complexity requires methods designed to find patterns in the data. Unsupervised
    outlier detection techniques are great at finding patterns in rare occurrences.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，异常值（或异常情况）被认为是一种罕见现象（在本章稍后你会看到，这被称为污染率）。换句话说，你会假设在一个大型数据集中，只有一小部分数据是异常值。例如，1%的数据可能是潜在的异常值。然而，这种复杂性需要设计出能够发现数据模式的方法。无监督的异常值检测技术擅长于发现罕见现象中的模式。
- en: After investigating outliers, you will have a historical set of labeled data,
    allowing you to leverage semi-supervised outlier detection techniques. This chapter
    focuses on unsupervised outlier detection.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在调查异常值之后，你将拥有一个历史标签数据集，允许你使用半监督的异常值检测技术。本章重点讲解无监督异常值检测。
- en: 'In this chapter, you will be introduced to the **PyOD** library, described
    as *"a comprehensive and scalable Python toolkit for detecting outlying objects
    in multivariate data."* The library offers an extensive collection of implementations
    for popular and emerging algorithms in the field of outlier detection, which you
    can read about here: [https://github.com/yzhao062/pyod](https://github.com/yzhao062/pyod).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将接触到**PyOD**库，它被描述为*“一个全面且可扩展的Python工具包，用于检测多变量数据中的异常对象。”*该库提供了一个广泛的实现集合，涵盖了异常值检测领域的流行算法和新兴算法，你可以在此阅读更多内容：[https://github.com/yzhao062/pyod](https://github.com/yzhao062/pyod)。
- en: You will be using the same New York taxi dataset to make it easier to compare
    the results between the different machine learning methods in this chapter and
    the statistical methods from *Chapter 8*, *Outlier Detection Using Statistical
    Methods*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用相同的纽约出租车数据集，这样可以更方便地比较本章不同机器学习方法和*第8章*，*使用统计方法的异常值检测*中的统计方法的结果。
- en: 'The recipes that you will encounter in this chapter are as follow*s*:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中你将遇到的内容如下*：
- en: Detecting outliers using **KNN**
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**KNN**检测异常值
- en: Detecting outliers using **LOF**
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**LOF**检测异常值
- en: Detecting outliers using **iForest**
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**iForest**检测异常值
- en: Detecting outliers using **One-Class Support Vector Machine** (**OCSVM**)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**一类支持向量机**（**OCSVM**）检测异常值
- en: Detecting outliers using **COPOD**
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**COPOD**检测异常值
- en: Detecting outliers with **PyCaret**
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**PyCaret**检测异常值
- en: Technical requirements
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can download the Jupyter notebooks and datasets required from the GitHub
    repository:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 GitHub 仓库下载所需的 Jupyter notebooks 和数据集：
- en: 'Jupyter notebooks: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch14/Chapter%2014.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch14/Chapter%2014.ipynb)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter notebooks：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch14/Chapter%2014.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch14/Chapter%2014.ipynb)
- en: 'Datasets: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch14](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch14)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch14](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch14)
- en: 'You can install PyOD with either `pip` or Conda. For a `pip` install, run the
    following command:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `pip` 或 Conda 安装 PyOD。对于 `pip` 安装，运行以下命令：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For a `Conda` install, run the following command:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `Conda` 安装，运行以下命令：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To prepare for the outlier detection recipes, start by loading the libraries
    that you will be using throughout the chapter:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备异常值检测方法，首先加载你将在整个章节中使用的库：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Load the `nyc_taxi.csv` data into a pandas DataFrame as it will be used throughout
    the chapter:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `nyc_taxi.csv` 数据加载到 pandas DataFrame 中，因为它将在整个章节中使用：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can store the known dates containing outliers, also known as ground truth
    labels:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以存储包含异常值的已知日期，也称为真实标签：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create the `plot_outliers` function that you will use throughout the recipes:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 创建你将在整个方法中使用的 `plot_outliers` 函数：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you proceed with the outlier detection recipes, the goal is to see how the
    different techniques capture outliers and compare them to the ground truth labels,
    as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行异常值检测时，目标是观察不同技术如何捕捉异常值，并将其与真实标签进行比较，如下所示：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code should produce a time series plot with `X` markers for the
    known outliers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应生成一个时间序列图，其中标记 `X` 表示已知的异常值：
- en: '![Figure 14.1: Plotting the NYC taxi data after downsampling with ground truth
    labels (outliers)](img/file278.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.1：对下采样后的纽约出租车数据进行绘图，并附上真实标签（异常值）](img/file278.jpg)'
- en: 'Figure 14.1: Plotting the NYC taxi data after downsampling with ground truth
    labels (outliers)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1：对下采样后的纽约出租车数据进行绘图，并附上真实标签（异常值）
- en: PYOD'S METHODS FOR TRAINING AND MAKING PREDICTIONS
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PYOD 的训练与预测方法
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Like scikit-learn, PyOD offers familiar methods for training your model and
    making predictions by providing three methods: `model.fit()`, `model.predict()`,
    and `model.fit_predict()`.'
  id: totrans-41
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 像 scikit-learn 一样，PyOD 提供了熟悉的方法来训练模型并进行预测，方法包括：`model.fit()`、`model.predict()`
    和 `model.fit_predict()`。
- en: ''
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the recipes, we will break down the process into two steps by first fitting
    the model (training) using `.fit()` and then making a prediction using `.predict()`.
  id: totrans-43
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这些方法中，我们将过程分为两步，首先使用 `.fit()` 拟合模型（训练），然后使用 `.predict()` 进行预测。
- en: 'In addition to the `predict` method, PyOD provides two additional methods:
    `predict_proba` and `predict_confidence`.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `predict` 方法，PyOD 还提供了两个附加方法：`predict_proba` 和 `predict_confidence`。
- en: In the first recipe, you will explore how PyOD works behind the scenes and introduce
    fundamental concepts, for example, the concept of `contamination` and how `threshold_`
    and `decision_scores_` are used to generate the binary labels (*abnormal* or *normal*).
    These concepts will be covered in more depth in the following recipe.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个方法中，你将探索 PyOD 如何在幕后工作，并介绍基本概念，例如 `contamination` 的概念，以及如何使用 `threshold_`
    和 `decision_scores_` 来生成二元标签（*异常* 或 *正常*）。这些概念将在接下来的方法中详细讨论。
- en: Detecting outliers using KNN
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 KNN 检测异常值
- en: The KNN algorithm is typically used in a supervised learning setting where prior
    results or outcomes (labels) are known.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 算法通常用于监督学习环境，其中先前的结果或输出（标签）是已知的。
- en: It can be used to solve classification or regression problems. The idea is simple;
    for example, you can classify a new data point, Y, based on its nearest neighbors.
    For instance, if k=5, the algorithm will find the five nearest data points (neighbors)
    by distance to the point Y and determine its class based on the majority. If there
    are three blue and two red nearest neighbors, Y is classified as blue. The K in
    KNN is a parameter you can modify to find the optimal value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用来解决分类或回归问题。这个思想很简单；例如，你可以根据最近邻来分类一个新的数据点Y。例如，如果k=5，算法会通过与点Y的距离找到五个最近的邻居，并根据多数邻居的类别来确定Y的类别。如果五个邻居中有三个是蓝色，两个是红色，那么Y将被分类为蓝色。KNN中的K是一个参数，你可以修改它以找到最佳值。
- en: In the case of outlier detection, the algorithm is used differently. Since we
    do not know the outliers (labels) in advance, KNN is used in an *unsupervised*
    learning manner. In this scenario, the algorithm finds the closest *K* nearest
    neighbors for every data point and measures the average distance. The points with
    the most significant distance from the population will be considered outliers,
    and more specifically, they are considered *global* outliers. In this case, the
    distance becomes the score to determine which points are outliers among the population,
    and hence KNN is a **proximity-based algorithm**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在离群点检测的情况下，算法的使用方式有所不同。由于我们事先不知道离群点（标签），KNN以*无监督*的方式进行学习。在这种情况下，算法会为每个数据点找到最近的*K*个邻居，并计算它们的平均距离。与数据集其余部分的距离最远的点将被视为离群点，更具体地说，它们被认为是*全局*离群点。在这种情况下，距离成为确定哪些点是离群点的评分依据，因此KNN是一种**基于邻近的算法**。
- en: Generally, proximity-based algorithms rely on the distance or proximity between
    an outlier point and its nearest neighbors. In KNN, the number of nearest neighbors,
    *k*, is a parameter you need to determine. There are other variants of the KNN
    algorithm supported by PyOD, for example, **Average KNN** (**AvgKNN**), which
    uses the average distance to the KNN for scoring, and **Median KNN** (**MedKNN**),
    which uses the median distance for scoring.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，基于邻近的算法依赖于离群点与其最近邻之间的距离或接近度。在KNN算法中，最近邻的数量，*k*，是你需要确定的一个参数。PyOD支持KNN算法的其他变种，例如，**平均KNN**（**AvgKNN**），它使用与KNN的平均距离进行评分；以及**中位数KNN**（**MedKNN**），它使用中位数距离进行评分。
- en: How to do it...
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this recipe, you will continue to work with the `tx` DataFrame, created
    in the *Technical requirements* section, to detect outliers using the `KNN` class
    from PyOD:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，你将继续使用在*技术要求*部分创建的`tx`数据框，通过PyOD的`KNN`类来检测离群点：
- en: 'Start by loading the `KNN` class:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先加载`KNN`类：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You should be familiar with a few parameters to control the algorithm's behavior.
    The first parameter is `contamination`, a numeric (float) value representing the
    dataset's fraction of outliers. This is a common parameter across all the different
    classes (algorithms) in PyOD. For example, a `contamination` value of `0.1` indicates
    that you expect 10% of the data to be outliers. The default value is `contamination=0.1`.
    The contamination value can range from `0` to `0.5` (or 50%). You will need to
    experiment with the contamination value, since the value influences the scoring
    threshold used to determine potential outliers, and how many of these potential
    outliers are to be returned. You will learn more about this in the *How it works...*
    section of this chapter.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要熟悉一些参数来控制算法的行为。第一个参数是`contamination`，一个数字（浮动）值，表示数据集中离群点的比例。这是PyOD中所有不同类别（算法）通用的参数。例如，`contamination`值为`0.1`表示你期望数据中10%是离群点。默认值为`contamination=0.1`。`contamination`的值可以在`0`到`0.5`（即50%）之间变化。你需要通过实验来调整`contamination`值，因为该值会影响用于确定潜在离群点的评分阈值，以及返回多少潜在离群点。你将在本章的*它是如何工作的...*部分深入了解这一点。
- en: For example, if you suspect the proportion of outliers in your data at 3%, then
    you can use that as the contamination value. You could experiment with different
    contamination values, inspect the results, and determine how to adjust the contamination
    level. We already know that there are 5 known outliers out of the 215 observations
    (around 2.3%), and in this recipe, you will use 0.03 (or 3%).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你怀疑数据中的离群点比例为3%，你可以将其作为`contamination`值。你可以尝试不同的`contamination`值，检查结果，并确定如何调整`contamination`水平。我们已经知道在215个观测值中有5个已知的离群点（约2.3%），在这个示例中，你将使用0.03（或3%）。
- en: The second parameter, specific to KNN, is `method`, which defaults to `method='largest'`.
    In this recipe, you will change it to the `mean` (the average of all *k* neighbor
    distances). The third parameter, also specific to KNN, is `metric`, which tells
    the algorithm how to compute the distances. The default is the `minkowski` distance
    but it can take any distance metrics from scikit-learn or the SciPy library. Finally,
    you need to provide the number of neighbors, which defaults to `n_neighbors=5`.
    Ideally, you will want to run for different KNN models with varying values of
    *k* and compare the results to determine the optimal number of neighbors.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数，特定于KNN的是`method`，其默认值为`method='largest'`。在本教程中，你将把它更改为`mean`（所有*k*邻居距离的平均值）。第三个参数，亦为KNN特有的是`metric`，它告诉算法如何计算距离。默认值是`minkowski`距离，但它可以接受来自scikit-learn或SciPy库的任何距离度量。最后，你需要提供邻居的数量，默认值为`n_neighbors=5`。理想情况下，你将希望使用不同的KNN模型并比较不同*k*值的结果，以确定最佳邻居数量。
- en: 'Instantiate KNN with the updated parameters and then train (fit) the model:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用更新后的参数实例化KNN，然后训练（拟合）模型：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `predict` method will generate binary labels, either `1` or `0`, for each
    data point. A value of `1` indicates an outlier. Store the results in a pandas
    Series:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`predict`方法将为每个数据点生成二进制标签，`1`或`0`。值为`1`表示是异常值。将结果存储在pandas Series中：'
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Filter the `predicted` Series to only show the outlier values:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤`predicted` Series，仅显示异常值：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Overall, the results look promising; four out of the five known dates have been
    identified. Additionally, the algorithm identified the day after Christmas as
    well as January 26, 2015, which was when all vehicles were ordered off the street
    due to the North American blizzard.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来看，结果很有前景；已识别出五个已知日期中的四个。此外，算法还识别了圣诞节后的那一天，以及2015年1月26日，那一天由于北美暴风雪，所有车辆都被命令驶离街道。
- en: 'Use the `plot_outliers` function created in the *Technical requirements* section
    to visualize the output to gain better insight:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在*技术要求*部分创建的`plot_outliers`函数来可视化输出，以获得更好的洞察：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code should produce a plot similar to that in *Figure 14.1*,
    except the `x` markers are based on the outliers identified using the KNN algorithm:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应生成类似于*图 14.1*的图表，不同之处在于`x`标记是基于使用KNN算法识别的异常值：
- en: '![Figure 14.2: Markers showing the identified potential outliers using the
    KNN algorithm](img/file279.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.2：使用KNN算法识别的潜在异常值标记](img/file279.jpg)'
- en: 'Figure 14.2: Markers showing the identified potential outliers using the KNN
    algorithm'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2：使用KNN算法识别的潜在异常值标记
- en: 'To print the labels (dates) along with the markers, just call the `plot_outliers`
    function again, but this time with `labels=True`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要打印标签（日期）和标记，只需再次调用`plot_outliers`函数，但这次要设置`labels=True`：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding code should produce a similar plot to the one in *Figure 14.2*
    with the addition of text labels.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应生成一个类似于*图 14.2*的图表，并附加文本标签。
- en: How it works...
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The unsupervised approach to the KNN algorithm calculates the distance of an
    observation to other neighboring observations. The default distance used in PyOD
    for KNN is the Minkowski distance (the p-norm distance). You can change to different
    distance measures, such as the Euclidean distance with `euclidean` or `l2` or
    the Manhattan distance with `manhattan` or `l1`. This can be accomplished using
    the `metric` parameter, which can take a string value, for example, `metric='l2'`
    or `metric='euclidean'`, or a callable function from scikit-learn or SciPy. This
    is a parameter that you experiment with as it influences how the distance is calculated,
    which is what the outlier scores are based on.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: KNN算法的无监督方法计算一个观测值与其他邻近观测值的距离。PyOD中KNN的默认距离是闵可夫斯基距离（p-范数距离）。你可以更改为不同的距离度量，例如使用`euclidean`或`l2`表示欧几里得距离，或使用`manhattan`或`l1`表示曼哈顿距离。可以使用`metric`参数来实现这一点，`metric`参数可以接受字符串值，例如`metric='l2'`或`metric='euclidean'`，也可以是来自scikit-learn或SciPy的可调用函数。这是一个需要实验的参数，因为它影响距离的计算方式，而异常值分数正是基于此进行计算的。
- en: 'Traditionally, when people hear KNN, they immediately assume it is only a supervised
    learning algorithm. For unsupervised KNN, there are three popular algorithms:
    ball tree, KD tree, and brute-force search. The PyOD library supports all three
    as `ball_tree`, `kd_tree`, and `brute`, respectively. The default value is set
    to `algorithm="auto"`.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，当人们听到 KNN 时，他们立即认为它仅仅是一个监督学习算法。对于无监督 KNN，有三种常见的算法：ball tree、KD tree 和暴力搜索。PyOD
    库支持这三种算法，分别为 `ball_tree`、`kd_tree` 和 `brute`。默认值设置为 `algorithm="auto"`。
- en: 'PyOD uses an internal score specific to each algorithm, scoring each observation
    in the training set. The `decision_scores_` attribute will show these scores for
    each observation. Higher scores indicate a higher potential of being an abnormal
    observation:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: PyOD 使用特定于每个算法的内部评分，对训练集中的每个观测值进行评分。`decision_scores_` 属性将显示每个观测值的这些评分。较高的评分表示该观测值更有可能是异常值：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can convert this into a DataFrame:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其转换为 DataFrame：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Since all the data points are scored, PyOD will determine a threshold to limit
    the number of outliers returned. The threshold value depends on the *contamination*
    value you provided earlier (the proportion of outliers you suspect). The higher
    the contamination value, the lower the threshold, and hence more outliers are
    returned. A lower contamination value will increase the threshold.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有数据点都被评分，PyOD 会确定一个阈值来限制返回的异常值数量。阈值的大小取决于你之前提供的 *污染* 值（你怀疑的异常值比例）。污染值越高，阈值越低，因此返回的异常值越多。污染值越低，阈值会提高。
- en: 'You can get the threshold value using the `threshold_` attribute from the model
    after fitting it to the training data. Here is the threshold for KNN based on
    a 3% contamination rate:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用模型在拟合训练数据后，从 `threshold_` 属性中获取阈值。以下是基于 3% 污染率的 KNN 阈值：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is the value used to filter out the significant outliers. Here is an example
    of how you reproduce that:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用来过滤显著异常值的值。以下是如何重现这一点的示例：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 14.3: Showing the decision scores from PyOD](img/file280.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.3：显示来自 PyOD 的决策评分](img/file280.jpg)'
- en: 'Figure 14.3: Showing the decision scores from PyOD'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3：显示来自 PyOD 的决策评分
- en: 'Notice the last observation on `2014-09-27` is slightly above the threshold,
    but it was not returned when you used the `predict` method. If you use the contamination
    threshold, you can get a better cutoff:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最后一个观测值 `2014-09-27` 稍微高于阈值，但在你使用 `predict` 方法时并没有返回。如果使用污染阈值，你可以得到一个更好的截止点：
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Another helpful method is `predict_proba`, which returns the probability of
    being normal and the probability of being abnormal for each observation. PyOD
    provides two methods for determining these percentages: `linear` or `unify`. The
    two methods scale the outlier scores before calculating the probabilities. For
    example, in the case of `linear`, the implementation uses `MinMaxScaler` from
    scikit-learn to scale the scores before calculating the probabilities. The `unify`
    method uses the z-score (standardization) and the Gaussian error function (`erf`)
    from the SciPy library (`scipy.special.erf`).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的方法是 `predict_proba`，它返回每个观测值是正常值和异常值的概率。PyOD 提供了两种方法来确定这些概率：`linear` 或
    `unify`。这两种方法都会在计算概率之前对异常值评分进行缩放。例如，在 `linear` 方法的实现中，使用了 scikit-learn 的 `MinMaxScaler`
    来缩放评分，然后再计算概率。`unify` 方法则使用 z-score（标准化）和 SciPy 库中的高斯误差函数（`erf`）（`scipy.special.erf`）。
- en: 'You can compare the two approaches. First, start using the `linear` method
    to calculate the prediction probability, you can use the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以比较这两种方法。首先，使用 `linear` 方法计算预测概率，你可以使用以下代码：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: For the `unify` method, you can just update `method='unify'`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `unify` 方法，你只需将 `method='unify'` 更新即可。
- en: 'To save any PyOD model, you can use the `joblib` Python library:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存任何 PyOD 模型，你可以使用 `joblib` Python 库：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There's more...
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'Earlier in the recipe, when instantiating the `KNN` class, you changed the
    value of `method` for calculating the outlier *score* to be `mean`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的步骤中，当实例化 `KNN` 类时，你将计算异常值 *评分* 的 `method` 值更改为 `mean`：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s create a function for the KNN algorithm to train the model on different
    scoring methods by updating the `method` parameter to either `mean`, `median`,
    or `largest` to examine the impact on the decision scores:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为 KNN 算法创建一个函数，通过更新 `method` 参数为 `mean`、`median` 或 `largest`，以训练模型并检查这些方法对决策评分的影响：
- en: '`largest` uses the largest distance to the *k*th neighbor as the outlier score.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`largest` 使用到 *k* 邻居的最大距离作为异常值评分。'
- en: '`mean` uses the average of the distances to the *k* neighbors as the outlier
    score.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean` 使用与 *k* 邻居的距离的平均值作为异常值分数。'
- en: '`median` uses the median of the distances to the *k* neighbors as the outlier
    score.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`median` 使用与 *k* 邻居的距离的中位数作为异常值分数。'
- en: 'Create the `knn_anomaly` function with the following parameters: `data`, `method`,
    `contamination`, and `k`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `knn_anomaly` 函数，包含以下参数：`data`、`method`、`contamination` 和 `k`：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can run the function using different methods, contamination, and *k* values
    to experiment.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过不同的方法、污染度和 *k* 值来运行该函数进行实验。
- en: 'Explore how the different methods produce a different threshold, which impacts
    the outliers being detected:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 探索不同方法如何生成不同的阈值，这会影响异常值的检测：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code should print out the top 10 outliers for each method (with
    contamination at 5%):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码应该会打印出每种方法的前 10 个异常值（污染度为 5%）：
- en: '![Figure 14.4: Comparing decision scores using different KNN distance metrics](img/file281.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.4：使用不同 KNN 距离度量比较决策分数](img/file281.jpg)'
- en: 'Figure 14.4: Comparing decision scores using different KNN distance metrics'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：使用不同 KNN 距离度量比较决策分数
- en: Notice the top six (representing the 3% contamination) are identical for all
    three methods. The order may vary and the decision scores are different between
    the methods. Do notice the difference between the methods is more apparent beyond
    the top six, as shown in *Figure 14.4*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前六个（表示 3% 污染度）对于三种方法是相同的。顺序可能会有所不同，各方法的决策分数也不同。请注意，各方法之间的差异在前六个之后更为明显，如 *图
    14.4* 所示。
- en: See also
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'Check out the following resources:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下资源：
- en: 'To learn more about unsupervised KNN, the scikit-learn library has a great
    explanation about its implementation: [https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors](https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于无监督 KNN 的内容，scikit-learn 库提供了关于其实现的很棒的解释：[https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors](https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors)。
- en: 'To learn more about PyOD KNN and the different parameters, visit the official
    documentation here: [https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=knn#module-pyod.models.knn](https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=knn#module-pyod.models.knn).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于 PyOD KNN 和不同参数的内容，请访问官方文档：[https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=knn#module-pyod.models.knn](https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=knn#module-pyod.models.knn)。
- en: Detecting outliers using LOF
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LOF 检测异常值
- en: In the previous recipe, *Detecting outliers using KNN*, in the KNN algorithm,
    the decision scoring for detecting outliers was based on the distance between
    observations. A data point far from its KNN can be considered an outlier. Overall,
    the algorithm does a good job of capturing global outliers, but those far from
    the surrounding points may not do well with identifying local outliers.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个实例中，*使用 KNN 检测异常值*，KNN 算法通过观测点之间的距离来为检测异常值计算决策分数。与 KNN 距离较远的数据点可以被认为是异常值。总体来说，该算法在捕捉全局异常值方面表现不错，但对于那些远离周围点的数据点，可能无法很好地识别局部异常值。
- en: This is where the LOF (Local Outlier Factor) comes in to solve this limitation.
    Instead of using the distance between neighboring points, it uses density as a
    basis for scoring data points and detecting outliers. The LOF is considered a
    **density-based algorithm**. The idea behind the LOF is that outliers will be
    further from other data points and more isolated, and thus will be in low-density
    regions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，LOF（局部异常因子）就能解决这个问题。LOF 不使用邻近点之间的距离，而是通过密度作为基础来为数据点评分并检测异常值。LOF 被认为是一个**基于密度的算法**。LOF
    的理念是，异常值会离其他数据点较远且更加孤立，因此会出现在低密度区域。
- en: 'It is easier to illustrate this with an example: imagine a person standing
    in line in a small but busy Starbucks, and everyone is pretty much close to each
    other; then, we can say the person is in a high-density area and, more specifically,
    **high local density**. If the person decides to wait in their car in the parking
    lot until the line eases up, they are isolated and in a **low-density** area,
    thus being considered an outlier. From the perspective of the people standing
    in line, who are probably not aware of the person in the car, that person is considered
    not reachable even though that person in the vehicle can see all of the individuals
    standing in line. So, we say that the person in the car is not reachable from
    their perspective. Hence, we sometimes refer to this as **inverse reachability**
    (how far you are from the neighbors'' perspective, not just yours).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 用一个例子来说明这个概念会更清楚：假设一个人站在小而忙碌的星巴克队伍中，每个人几乎都靠得很近；那么，我们可以说这个人处于高密度区域，更具体地说是**高局部密度**。如果这个人决定在停车场里等着，直到队伍稍微缓解，他就被孤立了，处于**低密度**区域，因此被认为是异常值。从排队的人角度来看，他们可能不知道车里的人，但车里的人能看到所有排队的人。所以，从他们的角度来看，车里的人被认为是不可接近的。我们称之为**逆向可达性**（从邻居的角度看你有多远，而不仅仅是你自己的视角）。
- en: Like KNN, you still need to define the *k* parameter for the number of nearest
    neighbors. The nearest neighbors are identified based on the distance measured
    between the observations (think KNN), then the **Local Reachability Density**
    (**LRD** or **local density** for short) is measured for each neighboring point.
    This local density is the score used to compare the *k*th neighboring observations
    and those with lower local densities than their *k*th neighbors are considered
    outliers (they are further from the reach of their neighbors).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 和KNN一样，你仍然需要定义用于最近邻居数量的*k*参数。最近邻居是基于观察值之间的距离进行识别的（想想KNN），然后对每个邻近点计算**局部可达密度**（**LRD**或简称**局部密度**）。这个局部密度是用于比较第*k*个邻近观察值的评分，那些局部密度低于第*k*个邻居的点被视为异常值（它们离邻居的范围更远）。
- en: How to do it...
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'In this recipe, you will continue to work with the `tx` DataFrame, created
    in the *Technical requirements* section, to detect outliers using the **LOF**
    class from PyOD:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，你将继续使用在*技术要求*部分创建的`tx` DataFrame，使用PyOD中的**LOF**类来检测异常值：
- en: 'Start by loading the `LOF` class:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先加载`LOF`类：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You should be familiar with a few parameters to control the algorithm's behavior.
    The first parameter is `contamination`, a numeric (float) value representing the
    dataset's fraction of outliers. For example, a value of `0.1` indicates that you
    expect 10% of the data to be outliers. The default value is *contamination=0.1*.
    In this recipe, you will use `0.03` (3%).
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该熟悉几个控制算法行为的参数。第一个参数是`contamination`，它是一个数值（浮动型），表示数据集中异常值的比例。例如，`0.1`表示你预计10%的数据是异常值。默认值是*contamination=0.1*。在本例中，你将使用`0.03`（3%）。
- en: The second parameter is the number of neighbors, which defaults to `n_neighbors=5`,
    similar to the KNN algorithm. Ideally, you will want to run different models with
    varying values of *k* (`n_neighbors`) and compare the results to determine the
    optimal number of neighbors. Lastly, the `metric` parameter specifies which metric
    to use to calculate the distance. This can be any distance metrics from the scikit-learn
    or SciPy libraries (for example, **Euclidean** or **Manhattan** distance). The
    default value is the **Minkowski** distance with `metric='minkowski'`. Since the
    Minkowski distance is a generalization for both the Euclidean (![](img/file282.png))
    and Manhattan distances (![](img/file283.png)), you will notice a `p` parameter.
    By default, `p=2` indicates Euclidean distance, while a value of `p=1` indicates
    Manhattan distance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是邻居的数量，默认为`n_neighbors=5`，类似于KNN算法。理想情况下，你会使用不同的*k*（`n_neighbors`）值运行不同的模型，并比较结果以确定最佳的邻居数。最后，`metric`参数指定用来计算距离的度量。可以使用scikit-learn或SciPy库中的任何距离度量（例如，**欧几里得**距离或**曼哈顿**距离）。默认值是**闵可夫斯基**距离，`metric='minkowski'`。由于闵可夫斯基距离是欧几里得距离（![](img/file282.png)）和曼哈顿距离（![](img/file283.png)）的推广，你会看到一个`p`参数。默认情况下，`p=2`表示欧几里得距离，而`p=1`表示曼哈顿距离。
- en: 'Instantiate LOF by updating `n_neighbors=5` and `contamination=0.03` while
    keeping the rest of the parameters with the default values. Then, train (fit)
    the model:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过更新`n_neighbors=5`和`contamination=0.03`来实例化LOF，同时保持其他参数为默认值。然后，训练（拟合）模型：
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `predict` method will output either `1` or `0` for each data point. A value
    of `1` indicates an outlier. Store the results in a pandas Series:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`predict`方法将为每个数据点输出`1`或`0`。值为`1`表示异常值。将结果存储在一个pandas Series中：'
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Filter the predicted Series to only show the outlier values:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤预测的序列，仅显示异常值：
- en: '[PRE26]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Interestingly, it captured three out of the five known dates but managed to
    identify the day after Thanksgiving and the day after Christmas as outliers. Additionally,
    October 31 was on a Friday, and it was Halloween night.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，它捕获了五个已知日期中的三个，但成功识别了感恩节后的那一天和圣诞节后的那一天为异常值。此外，10月31日是星期五，那天是万圣节夜晚。
- en: 'Use the `plot_outliers` function created in the *Technical requirements* section
    to visualize the output to gain better insight:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在*技术要求*部分创建的`plot_outliers`函数来可视化输出，以获得更好的洞察：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code should produce a plot similar to that in *Figure 14.1*,
    except the `x` markers are based on the outliers identified using the LOF algorithm:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应生成一个类似于*图14.1*的图表，只不过`x`标记是基于使用LOF算法识别的异常值：
- en: '![Figure 14.5: Markers showing the identified potential outliers using the
    LOF algorithm](img/file284.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图14.5：使用LOF算法识别的潜在异常值标记](img/file284.jpg)'
- en: 'Figure 14.5: Markers showing the identified potential outliers using the LOF
    algorithm'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：使用LOF算法识别的潜在异常值标记
- en: 'To print the labels (dates) with the markers, just call the `plot_outliers`
    function again but this time with `labels=True`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 若要打印带有标记的标签（日期），只需再次调用`plot_outliers`函数，但这次传入`labels=True`：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding code should produce a similar plot to the one in *Figure 14.5*
    with the addition of text labels.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应生成一个类似于*图14.5*的图表，并附加文本标签。
- en: How it works...
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The **LOF** is a **density-based algorithm** that assumes that outlier points
    are more isolated and have lower local density scores compared to their neighbors.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**LOF**是一种**基于密度的算法**，它假设异常点比邻居更加孤立，且具有较低的局部密度得分。'
- en: 'LOF is like KNN in that we measure the distances between the neighbors before
    calculating the local density. The local density is the basis of the decision
    scores, which you can view using the `decision_scores_` attribute:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: LOF类似于KNN，因为我们在计算局部密度之前，测量邻居之间的距离。局部密度是决策得分的基础，你可以通过`decision_scores_`属性查看这些得分：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The scores are very different from those in *Figure 14.3* for KNN.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些得分与*图14.3*中的KNN得分非常不同。
- en: For more insight into `decision_`scores_, threshold_, or predict_proba, please
    review the first recipe of this chapter, Detecting outliers using KNN.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 若要更深入了解`decision_`得分、`threshold_`或`predict_proba`，请查看本章的第一篇教程——使用KNN检测异常值。
- en: There's more...
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Like the LOF, another extension of the algorithm is the **Cluster-Based Local
    Outlier Factor (CBLOF).** The CBLOF is similar to LOF in concept as it relies
    on cluster size and distance when calculating the scores to determine outliers.
    So, instead of the number of neighbors (`n_neighbors` like in LOF), we now have
    a new parameter, which is the number of clusters (`n_clusters`).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于LOF，算法的另一个扩展是**基于聚类的局部异常因子（CBLOF）**。CBLOF在概念上与LOF相似，因为它在计算得分以确定异常值时依赖于聚类大小和距离。因此，除了LOF中的邻居数（`n_neighbors`），我们现在有了一个新的参数，即聚类数（`n_clusters`）。
- en: The default clustering estimator, `clustering_estimator`, in PyOD is the k-means
    clustering algorithm.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: PyOD中的默认聚类估计器`clustering_estimator`是K均值聚类算法。
- en: 'You will use the CBLOF class from PyOD and keep most parameters at the default
    values. Change the `n_clusters=8` and `contamination=0.03` parameters:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用PyOD中的CBLOF类，并保持大部分参数为默认值。更改`n_clusters=8`和`contamination=0.03`参数：
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The preceding code should produce a plot similar to that in *Figure 14.1* except
    the `x` markers are based on the outliers identified using the CBLOF algorithm:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应生成一个类似于*图14.1*的图表，只不过`x`标记是基于使用CBLOF算法识别的异常值：
- en: '![Figure 14.6: Markers showing the identified potential outliers using the
    CBLOF algorithm](img/file285.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图14.6：使用CBLOF算法识别的潜在异常值标记](img/file285.jpg)'
- en: 'Figure 14.6: Markers showing the identified potential outliers using the CBLOF
    algorithm'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：使用CBLOF算法识别的潜在异常值标记
- en: Compare *Figure 14.6* with *Figure 14.5* (LOF) and notice the similarity.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 将*图14.6*与*图14.5*（LOF）进行比较，注意它们之间的相似性。
- en: See also
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about the LOF and CBLOF algorithms, you can visit the PyOD documentation:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于LOF和CBLOF算法的信息，你可以访问PyOD文档：
- en: 'LOF: [https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LOF: [https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof)'
- en: 'CBLOF: [https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CBLOF: [https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof)'
- en: Detecting outliers using iForest
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用iForest检测异常值
- en: '**iForest** has similarities with another popular algorithm known as **Random
    Forests**. Random Forests is a **tree-based supervised learning** algorithm. In
    supervised learning, you have existing labels (classification) or values (regression)
    representing the target variable. This is how the algorithm learns (it is supervised).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**iForest** 与另一个流行的算法**随机森林**有相似之处。随机森林是一种**基于树的监督学习**算法。在监督学习中，你有现有的标签（分类）或值（回归）来表示目标变量。这就是算法学习的方式（它是监督学习）。'
- en: The name *forest* stems from the underlying mechanism of how the algorithm works.
    For example, in classification, the algorithm randomly samples the data to build
    multiple weak classifiers (smaller decision trees) that collectively make a prediction.
    In the end, you get a forest of smaller trees (models). This technique outperforms
    a single complex classifier that may overfit the data. Ensemble learning is the
    concept of multiple weak learners collaborating to produce an optimal solution.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*森林*这个名称来源于算法工作原理的底层机制。例如，在分类中，算法随机采样数据来构建多个弱分类器（较小的决策树），这些分类器共同做出预测。最终，你得到一个由较小树（模型）组成的森林。这种技术优于单个可能会过拟合数据的复杂分类器。集成学习是多个弱学习者协作产生最优解的概念。'
- en: iForest, also an **ensemble learning** method, is the unsupervised learning
    approach to Random Forests. The iForest algorithm isolates anomalies by randomly
    partitioning (splitting) a dataset into multiple partitions. This is performed
    recursively until all data points belong to a partition. The number of partitions
    required to isolate an anomaly is typically smaller than the number of partitions
    needed to isolate a regular point. The idea is that an anomaly data point is further
    from other points and thus easier to separate (isolate).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: iForest，作为一种**集成学习**方法，是随机森林的无监督学习方法。iForest算法通过随机划分（拆分）数据集成多个分区来隔离异常值。这个过程是递归进行的，直到所有数据点都属于一个分区。隔离一个异常值所需的分区数量通常比隔离常规数据点所需的分区数量要少。这个思路是，异常数据点距离其他点较远，因此更容易被分离（隔离）。
- en: In contrast, a normal data point is probably clustered closer to the larger
    set and, therefore, will require more partitions (splits) to isolate that point.
    Hence the name, isolation forest, since it identifies outliers through isolation.
    Once all the points are isolated, the algorithm will create an outlier score.
    You can think of these splits as creating a decision tree path. The shorter the
    path length to a point, the higher the chances of an anomaly.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，正常的数据点可能会更接近较大的数据集，因此需要更多的分区（拆分）来隔离该点。因此，称之为隔离森林，因为它通过隔离来识别异常值。一旦所有的点都被隔离，算法会生成一个异常值评分。你可以把这些分区看作是创建了一个决策树路径。到达某个点的路径越短，异常的可能性越大。
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this recipe, you will continue to work with the `nyc_taxi` DataFrame to
    detect outliers using the `IForest` class from the PyOD library:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，你将继续使用`nyc_taxi`数据框，利用PyOD库中的`IForest`类来检测异常值：
- en: 'Start by loading the `IForest` class:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始加载`IForest`类：
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: There are a few parameters that you should be familiar with to control the algorithm's
    behavior. The first parameter is `contamination`. The default value is `contamination=0.1`
    but in this recipe, you will use `0.03` (3%).
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一些参数是你应该熟悉的，以控制算法的行为。第一个参数是`contamination`。默认值为`contamination=0.1`，但在本例中，你将使用`0.03`（3%）。
- en: The second parameter is `n_estimators`, which defaults to `n_estimators=100`.
    This is the number of random trees generated. Depending on the complexity of your
    data, you may want to increase this value to a higher range, such as `500` or
    more. Start with the default smaller value to understand how the baseline model
    works—finally, `random_state` defaults to `None`. Since the iForest algorithm
    randomly generates partitions for the data, it is good to set a value to ensure
    that your work is reproducible. This way, you can get consistent results back
    when you rerun the code. Of course, this could be any integer value.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是 `n_estimators`，默认为 `n_estimators=100`，即生成的随机树的数量。根据数据的复杂性，您可能希望将该值增大到更高的范围，如
    `500` 或更高。从默认的小值开始，以了解基准模型的工作原理——最后，`random_state` 默认为 `None`。由于 iForest 算法会随机生成数据的划分，因此设置一个值有助于确保工作结果的可复现性。这样，当您重新运行代码时，能够获得一致的结果。当然，这个值可以是任何整数。
- en: 'Instantiate `IForest` and update the `contamination` and `random_state` parameters.
    Then, fit the new instance of the class (`iforest`) on the resampled data to train
    the model:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化 `IForest` 并更新 `contamination` 和 `random_state` 参数。然后，将该类的新实例（`iforest`）拟合到重采样数据上，以训练模型：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Use the `predict` method to identify outliers. The method will output either
    `1` or `0` for each data point. For example, a value of `1` indicates an outlier.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `predict` 方法来识别异常值。该方法会为每个数据点输出 `1` 或 `0`。例如，值为 `1` 表示是一个异常值。
- en: 'Let''s store the results in a pandas Series:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将结果存储在一个 pandas Series 中：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Interestingly, unlike the previous recipe, *Detecting outliers using KNN*, iForest
    detected `7` outliers while the KNN algorithm detected `6`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，与之前的算法 *使用 KNN 检测异常值* 不同，iForest 检测到了 `7` 个异常值，而 KNN 算法检测到了 `6` 个异常值。
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Overall, iForest captured four out of the five known outliers. There are additional
    but interesting dates identified that should trigger an investigation to determine
    whether these data points are outliers. For example, November 8, 2014, was detected
    as a potential outlier by the algorithm, which was not considered in the data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，iForest 捕获了已知的五个异常值中的四个。还有一些其他有趣的日期被识别出来，这些日期应触发调查，以确定这些数据点是否为异常值。例如，2014年11月8日被算法检测为潜在异常值，而该日期并未被考虑在数据中。
- en: 'Use the `plot_outliers` function created in the *Technical requirements* section
    to visualize the output to gain better insight:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *技术要求* 部分中创建的 `plot_outliers` 函数来可视化输出，以便更好地理解：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding code should produce a plot similar to that in *Figure 14.1* except
    the `x` markers are based on the outliers identified using the iForest algorithm:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应生成类似于 *图 14.1* 中的图表，唯一不同的是 `x` 标记基于使用 iForest 算法识别的异常值：
- en: '![Figure 14.7: Markers showing the identified potential outliers using the
    iForest algorithm](img/file286.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.7：使用 iForest 算法标识潜在异常值的标记](img/file286.jpg)'
- en: 'Figure 14.7: Markers showing the identified potential outliers using the iForest
    algorithm'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：使用 iForest 算法标识潜在异常值的标记
- en: 'To print the labels (dates) with the markers, just call the `plot_outliers`
    function again but this time with `labels=True`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要打印带有标记的标签（日期），只需再次调用 `plot_outliers` 函数，但这次将 `labels=True`：
- en: '[PRE36]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding code should produce a similar plot as the one in *Figure 14.7*
    with the addition of text labels.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应生成类似于 *图 14.7* 的图表，并且增加了文本标签。
- en: How it works...
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Since iForest is an ensemble method, you will be creating multiple models (tree
    learners). The default value of `n_estimators` is `100`. Increasing the number
    of base estimators may improve model performance up to a certain level before
    the computational performance takes a hit. So, for example, think of the number
    of estimators as trained models. For instance, for 100 estimators, you are essentially
    creating 100 decision tree models.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 iForest 是一种集成方法，您将创建多个模型（决策树学习器）。`n_estimators` 的默认值是 `100`。增加基础估计器的数量可能会提高模型的性能，但在某个程度上可能会影响计算性能。因此，您可以将估计器数量视为训练好的模型。例如，对于
    100 个估计器，您实际上是创建了 100 个决策树模型。
- en: 'There is one more parameter worth mentioning, which is the `bootstrap` parameter.
    It is a Boolean set to `False` by default. Since iForest randomly samples the
    data, you have two options: random sampling with replacement (known as *bootstrapping*)
    or random sampling without replacement. The default behavior is sampling without
    replacement.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个值得一提的参数是`bootstrap`参数。默认值为`False`，它是一个布尔值。由于iForest会随机抽样数据，你有两个选择：带替换的随机抽样（称为*自助抽样*）或不带替换的随机抽样。默认行为是没有替换的抽样。
- en: There's more...
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The iForest algorithm from PyOD (the `IForest` class) is a wrapper to scikit-learn's
    `IsolationForest` class. This is also true for the KNN used in the previous recipe,
    *Detecting outliers using KNN*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: PyOD中的iForest算法（`IForest`类）是scikit-learn中`IsolationForest`类的封装。这对上一食谱中使用的KNN也是如此，*使用KNN检测异常值*。
- en: 'Let''s explore this further and use scikit-learn to implement the iForest algorithm.
    You will use the `fit_predict()` method as a single step to train and predict,
    which is also available in PyOD''s implementations across the various algorithms:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步探索，使用scikit-learn实现iForest算法。你将使用`fit_predict()`方法作为一步训练和预测，这个方法也可以在PyOD的各种算法实现中找到：
- en: '[PRE37]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The results are the same. But do notice that, unlike PyOD, the identified outliers
    were labeled as `-1`, while in PyOD, outliers were labeled with `1`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一样的。但请注意，与PyOD不同，识别出的异常值在标记时为`-1`，而在PyOD中，异常值被标记为`1`。
- en: See also
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参见
- en: 'The PyOD iForest implementation is actually a wrapper to the `IsolationForest`
    class from scikit-learn:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: PyOD的iForest实现实际上是scikit-learn中`IsolationForest`类的封装：
- en: 'To learn more about PyOD iForest and the different parameters, visit their
    official documentation here: [https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=knn#module-pyod.models.iforest](https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=knn#module-pyod.models.iforest).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于PyOD iForest和不同参数的信息，请访问他们的官方文档：[https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=knn#module-pyod.models.iforest](https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=knn#module-pyod.models.iforest)。
- en: 'To learn more about the `IsolationForest` class from scikit-learn, you can
    visit their official documentation page here: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn-ensemble-isolationforest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn-ensemble-isolationforest).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于scikit-learn中`IsolationForest`类的信息，你可以访问他们的官方文档页面：[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn-ensemble-isolationforest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn-ensemble-isolationforest)。
- en: Detecting outliers using One-Class Support Vector Machine (OCSVM)
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用单类支持向量机（OCSVM）检测异常值
- en: '**Support Vector Machine (SVM)** is a popular supervised machine learning algorithm
    that is mainly known for classification but can also be used for regression. The
    popularity of SVM comes from the use of kernel functions (sometimes referred to
    as the **kernel trick**), such as linear, polynomial, **Radius-Based Function**
    (**RBF**), and the sigmoid function.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机（SVM）** 是一种流行的有监督机器学习算法，主要用于分类，但也可以用于回归。SVM的流行源于使用核函数（有时称为**核技巧**），例如线性、多项式、**基于半径的函数**（**RBF**）和sigmoid函数。'
- en: In addition to classification and regression, SVM can also be used for outlier
    detection in an unsupervised manner, similar to KNN, which is mostly known as
    a supervised machine learning technique but was used in an unsupervised manner
    for outlier detection, as seen in the *Outlier detection using KNN* recipe.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分类和回归，SVM还可以以无监督的方式用于异常值检测，类似于KNN。KNN通常被认为是一种有监督的机器学习技术，但在异常值检测中它是以无监督的方式使用的，正如在*使用KNN进行异常值检测*食谱中所见。
- en: How to do it...
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this recipe, you will continue to work with the `tx` DataFrame, created
    in the *Technical requirements* section, to detect outliers using the `ocsvm`
    class from PyOD:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，你将继续使用在*技术要求*部分创建的`tx`数据框，利用PyOD中的`ocsvm`类检测异常值：
- en: 'Start by loading the `OCSVM` class:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先加载`OCSVM`类：
- en: '[PRE38]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: There are a few parameters that you should be familiar with to control the algorithm's
    behavior. The first parameter is `contamination`. The default value is `contamination=0.1`
    and in this recipe, you will use `0.03` (3%).
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一些参数你应该了解，以控制算法的行为。第一个参数是`contamination`。默认值为`contamination=0.1`，在这个食谱中，你将使用`0.03`（即3%）。
- en: The second parameter is `kernel`, which is set to `rbf`, which you will keep
    as is.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是 `kernel`，其值设为 `rbf`，你将保持其不变。
- en: 'Instantiate OCSVM by updating `contamination=0.03` while keeping the rest of
    the parameters with the default values. Then, train (fit) the model:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过更新 `contamination=0.03` 来实例化 OCSVM，同时保持其余参数为默认值。然后，训练（拟合）模型：
- en: '[PRE39]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `predict` method will output either `1` or `0` for each data point. A value
    of `1` indicates an outlier. Store the results in a pandas Series:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`predict` 方法将为每个数据点输出 `1` 或 `0`。值为 `1` 表示异常值。将结果存储在一个 pandas Series 中：'
- en: '[PRE40]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Filter the predicted Series to only show the outlier values:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 筛选预测的 Series，仅显示异常值：
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Interestingly, it captured one out of the five known dates.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，它捕捉到了五个已知日期中的一个。
- en: 'Use the `plot_outliers` function created in the *Technical requirements* section
    to visualize the output to gain better insight:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*技术要求*部分中创建的 `plot_outliers` 函数可视化输出，以便获得更好的洞察：
- en: '[PRE42]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The preceding code should produce a plot similar to that in *Figure 14.1* except
    the `x` markers are based on the outliers identified using the OCSVM algorithm:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应产生一个与*图 14.1*类似的图表，唯一不同的是 `x` 标记是基于 OCSVM 算法识别的异常值：
- en: '![Figure 14.8: Line plot with markers for each outlying point using OCSVM](img/file287.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.8：使用 OCSVM 标记每个异常点的折线图](img/file287.jpg)'
- en: 'Figure 14.8: Line plot with markers for each outlying point using OCSVM'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：使用 OCSVM 标记每个异常点的折线图
- en: When examining the plot in *Figure 14.8*, it is not clear why OCSVM picked up
    on those dates as being outliers. The RBF kernel can capture non-linear relationships,
    so you would expect it to be a robust kernel.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看*图 14.8*中的图表时，不清楚为什么 OCSVM 识别出这些日期为异常值。RBF 核函数可以捕捉非线性关系，因此它应该是一个强健的核函数。
- en: The reason for this inaccuracy is that SVM is sensitive to data scaling. To
    get better results, you will need to standardize (scale) your data first.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这个不准确的原因在于 SVM 对数据缩放敏感。为了获得更好的结果，您需要首先对数据进行标准化（缩放）。
- en: 'Let''s fix this issue and standardize the data and then rerun the algorithm
    again:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们解决这个问题，先对数据进行标准化，然后再次运行算法：
- en: '[PRE43]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Interestingly, now the model identified four out of the five known outlier dates.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，现在模型识别出了五个已知异常日期中的四个。
- en: 'Use the `plot_outliers` function on the new result set:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新的结果集上使用 `plot_outliers` 函数：
- en: '[PRE44]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The preceding code should produce a more reasonable plot, as shown in the following
    figure:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应生成一个更合理的图表，如下图所示：
- en: '![Figure 14.9: OCSVM after scaling the data using the standardizer function](img/file288.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.9：使用标准化函数缩放数据后的 OCSVM](img/file288.jpg)'
- en: 'Figure 14.9: OCSVM after scaling the data using the standardizer function'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：使用标准化函数缩放数据后的 OCSVM
- en: Compare the results from *Figure 14.9* and *Figure 14.8* to see how scaling
    made a big difference in how the OCSVM algorithm identified outliers.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 比较*图 14.9*和*图 14.8*中的结果，看看缩放如何在 OCSVM 算法识别异常值方面产生了显著差异。
- en: How it works...
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The PyOD implementation for OCSVM is a wrapper to scikit-learn's **OneClassSVM**
    implementation.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: PyOD 对 OCSVM 的实现是对 scikit-learn 中 **OneClassSVM** 实现的封装。
- en: Similar to SVM, OneClassSVM is sensitive to outliers and also the scaling of
    the data. In order to get reasonable results, it is important to standardize (scale)
    your data before training your model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SVM 相似，OneClassSVM 对异常值以及数据的缩放非常敏感。为了获得合理的结果，在训练模型之前标准化（缩放）数据非常重要。
- en: There's more...
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Let''s explore how the different kernels perform on the same dataset. In the
    following code, you test four kernels: `''linear''`, `''poly''`, `''rbf''`, and
    `''sigmoid''`.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨不同核函数在相同数据集上的表现。在下面的代码中，您将测试四种核函数：`'linear'`、`'poly'`、`'rbf'` 和 `'sigmoid'`。
- en: 'Recall that when working with SVM, you will need to scale your data. You will
    use the scaled dataset created earlier:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，当使用 SVM 时，您需要缩放数据。您将使用之前创建的缩放数据集：
- en: '[PRE45]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The preceding code should produce a plot for each kernel so you can visually
    inspect and compare the difference between them:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应生成每个核函数的图表，以便您可以直观地检查并比较它们之间的差异：
- en: '![Figure 14.10: Comparing the different kernels with OCSVM](img/file289.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.10：比较不同核函数与 OCSVM 的效果](img/file289.jpg)'
- en: 'Figure 14.10: Comparing the different kernels with OCSVM'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10：比较不同核函数与 OCSVM 的效果
- en: Interestingly, each kernel method captured slightly different outliers. You
    can rerun the previous code to print out the labels (dates) for each marker by
    passing the `labels=True` parameter.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，每种核方法捕获的异常值略有不同。您可以重新运行之前的代码，通过传递 `labels=True` 参数来打印每个标记（日期）的标签。
- en: See also
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参见
- en: 'To learn more about the OCSVM implementation, visit the official documentation
    here: [https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关 OCSVM 实现的更多信息，请访问官方文档：[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm)。
- en: Detecting outliers using COPOD
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 COPOD 检测异常值
- en: 'COPOD is an exciting algorithm based on a paper published in September 2020,
    which you can read here: [https://arxiv.org/abs/2009.09463](https://arxiv.org/abs/2009.09463).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: COPOD 是一个令人兴奋的算法，基于 2020 年 9 月发布的一篇论文，你可以在这里阅读：[https://arxiv.org/abs/2009.09463](https://arxiv.org/abs/2009.09463)。
- en: The PyOD library offers many algorithms based on the latest research papers,
    which can be broken down into linear models, proximity-based models, probabilistic
    models, ensembles, and neural networks.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: PyOD 库提供了许多基于最新研究论文的算法，这些算法可以分为线性模型、基于邻近的模型、概率模型、集成模型和神经网络。
- en: COPOD falls under probabilistic models and is labeled as a *parameter-free*
    algorithm. The only parameter it takes is the *contamination* factor, which defaults
    to `0.1`. The COPOD algorithm is inspired by statistical methods, making it a
    fast and highly interpretable model. The algorithm is based on copula, a function
    generally used to model dependence between independent random variables that are
    not necessarily normally distributed. In time series forecasting, copulas have
    been used in univariate and multivariate forecasting, which is popular in financial
    risk modeling. The term copula stems from the copula function joining (coupling)
    univariate marginal distributions to form a uniform multivariate distribution
    function.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: COPOD 属于概率模型，并被标记为 *无参数* 算法。它唯一的参数是 *contamination* 因子，默认为 `0.1`。COPOD 算法受统计方法启发，使其成为一个快速且高度可解释的模型。该算法基于
    copula，一种通常用于建模相互独立的随机变量之间的依赖关系的函数，这些变量不一定服从正态分布。在时间序列预测中，copula 已被应用于单变量和多变量预测，这在金融风险建模中非常流行。copula
    这个术语源自 copula 函数，它将单变量的边际分布连接（耦合）在一起，形成一个统一的多变量分布函数。
- en: How to do it...
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, you will continue to work with the `tx` DataFrame to detect
    outliers using the `COPOD` class from the PyOD library:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，您将继续使用 `tx` DataFrame，通过 PyOD 库中的 `COPOD` 类来检测异常值：
- en: 'Start by loading the `COPOD` class:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先加载 `COPOD` 类：
- en: '[PRE46]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The only parameter you need to consider is `contamination`. Generally, think
    of this parameter (used in all the outlier detection implementations) as a threshold
    to control the model's sensitivity and minimize the false positives. Since it
    is a parameter you control, ideally, you want to run several models to experiment
    with the ideal threshold rate that works for your use cases.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要考虑的唯一参数是 `contamination`。通常，将该参数（用于所有异常值检测实现）视为一个阈值，用于控制模型的敏感性并最小化假阳性。由于这是一个由您控制的参数，理想情况下，您希望运行多个模型，实验出适合您用例的理想阈值。
- en: For more insight into `decision_scores_`, `threshold_`, or `predict_proba`,
    please review the first recipe, *Detecting outliers using KNN*, of this chapter.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于 `decision_scores_`、`threshold_` 或 `predict_proba` 的信息，请查看本章的第一个食谱，*使用
    KNN 检测异常值*。
- en: 'Instantiate `COPOD` and update `contamination` to `0.03`. Then, fit on the
    resampled data to train the model:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`COPOD`并将`contamination`更新为`0.03`。然后，在重新采样的数据上进行拟合，以训练模型：
- en: '[PRE47]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Use the `predict` method to identify outliers. The method will output either
    `1` or `0` for each data point. For example, a value of `1` indicates an outlier.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `predict` 方法识别异常值。该方法将为每个数据点输出 `1` 或 `0`。例如，`1` 表示异常值。
- en: 'Store the results in a pandas Series:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 将结果存储在 pandas Series 中：
- en: '[PRE48]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The number of outliers matches the number you got using iForest.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值的数量与使用 iForest 获得的数量相匹配。
- en: 'Filter the predicted Series only to show the outlier values:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅筛选预测的 Series，以显示异常值：
- en: '[PRE49]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Compared with other algorithms you have explored so far, you will notice some
    interesting outliers captured with COPOD that were not identified before. For
    example, COPOD identified July 4, a national holiday in the US (Independence Day).
    It happens to fall on a weekend (Friday being off). The COPOD model captured anomalies
    throughout the weekend for July 4 and July 6\. It happens that July 6 was an interesting
    day due to a baseball game in New York.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 与你迄今为止探索的其他算法相比，你会注意到使用COPOD捕获了一些有趣的异常点，这些异常点之前没有被识别出来。例如，COPOD识别了7月4日——美国的国庆节（独立日）。那天恰好是周末（星期五是休息日）。COPOD模型在7月4日和7月6日的整个周末期间捕获了异常。恰巧7月6日是由于纽约的一场棒球赛而成为一个有趣的日子。
- en: 'Use the `plot_outliers` function created in the *Technical requirements* section
    to visualize the output to gain better insights:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*技术要求*部分中创建的`plot_outliers`函数来可视化输出，以便获得更好的洞察：
- en: '[PRE50]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The preceding code should produce a plot similar to that in *Figure 14.1*,
    except the `x` markers are based on the outliers identified using the COPOD algorithm:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应该生成一个类似于*图14.1*的图表，唯一的区别是`x`标记基于使用COPOD算法识别的异常点：
- en: '![Figure 14.11: Markers showing the identified potential outliers using the
    COPOD algorithm](img/file290.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图14.11：使用COPOD算法识别的潜在异常点的标记](img/file290.jpg)'
- en: 'Figure 14.11: Markers showing the identified potential outliers using the COPOD
    algorithm'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：使用COPOD算法识别的潜在异常点的标记
- en: 'To print the labels (dates) with the markers, just call the `plot_outliers`
    function again, but this time with `labels=True`:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 要打印带有标记的标签（日期），只需再次调用`plot_outliers`函数，但这次需要将`labels=True`：
- en: '[PRE51]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The preceding code should produce a similar plot to the one in *Figure 14.11*
    with the addition of text labels.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应该生成一个类似于*图14.11*的图表，并加上文本标签。
- en: How it works...
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: COPOD is an advanced algorithm, but it is still based on probabilistic modeling
    and finding statistically significant extremes within the data. Several tests
    using COPOD have demonstrated its superb performance against benchmark datasets.
    The appeal of using COPOD is that it is parameter-free (aside from the contamination
    factor). So, as a user, you do not have to worry about hyperparameter tuning.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: COPOD是一个先进的算法，但它仍然基于概率建模和在数据中找到统计学上显著的极端值。使用COPOD的几项测试已经证明它在基准数据集上的卓越表现。使用COPOD的一个吸引力是它不需要调参（除了污染因子）。因此，作为用户，你无需担心超参数调优。
- en: There's more...
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Another simple and popular probabilistic algorithm is the **Median Absolute
    Deviation** (**MAD**). We explored MAD in *Chapter 8*, *Outlier Detection Using
    Statistical Methods*, in the *Outlier detection using modified z-score* recipe,
    in which you built the algorithm from scratch.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个简单且流行的概率算法是**中位绝对偏差**（**MAD**）。我们在*第8章*，*使用统计方法进行异常值检测*中探讨了MAD，具体是在*使用修改过的z-score进行异常值检测*的食谱中，你是从零开始构建该算法的。
- en: This is a similar implementation provided by PyOD and takes one parameter, the
    threshold. If you recall from *Chapter 8*, *Outlier Detection Using Statistical
    Methods*, the threshold is based on the standard deviation.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这是PyOD提供的一个类似实现，只有一个参数：阈值。如果你还记得*第8章*，*使用统计方法进行异常值检测*，阈值是基于标准差的。
- en: 'The following code shows how we can implement MAD with PyOD. You will use `threshold=3`
    to replicate what you did in *Chapter 8*, *Outlier Detection Using Statistical
    Methods*:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用PyOD实现MAD。你将使用`threshold=3`来复现你在*第8章*，*使用统计方法进行异常值检测*中的操作：
- en: '[PRE52]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This should match the results you obtained in *Chapter 8*, *Outlier Detection
    Using Statistical Methods*, with the modified z-score implementation.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该与你在*第8章*，*使用统计方法进行异常值检测*中，修改过的z-score实现的结果一致。
- en: See also
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about COPOD and its implementation in PyOD, visit the official
    documentation here: [https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=copod#pyod.models.copod.COPOD](https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=copod#pyod.models.copod.COPOD).'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于COPOD及其在PyOD中的实现，请访问官方文档：[https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=copod#pyod.models.copod.COPOD](https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=copod#pyod.models.copod.COPOD)。
- en: 'If you are interested in reading the research paper for *COPOD: Copula-Based
    Outlier Detection* (published in September 2020), visit the arXiv.org page here:
    [https://arxiv.org/abs/2009.09463](https://arxiv.org/abs/2009.09463).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你有兴趣阅读*COPOD: 基于Copula的异常值检测*（2020年9月发布）的研究论文，请访问arXiv.org页面：[https://arxiv.org/abs/2009.09463](https://arxiv.org/abs/2009.09463)。'
- en: Detecting outliers with PyCaret
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PyCaret 检测异常值
- en: In this recipe, you will explore **PyCaret** for outlier detection. PyCaret
    ([https://pycaret.org](https://pycaret.org)) is positioned as "an open-source,
    low-code machine learning library in Python that automates machine learning workflows".
    PyCaret acts as a wrapper for PyOD, which you used earlier in the previous recipes
    for outlier detection. What PyCaret does is simplify the entire process for rapid
    prototyping and testing with a minimal amount of code.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，你将探索**PyCaret**用于异常检测。PyCaret ([https://pycaret.org](https://pycaret.org))
    被定位为“一个开源的、低代码的 Python 机器学习库，自动化机器学习工作流”。PyCaret 是 PyOD 的封装，你之前在食谱中使用过它进行异常检测。PyCaret
    的作用是简化整个过程，用最少的代码进行快速原型设计和测试。
- en: You will use PyCaret to examine multiple outlier detection algorithms, similar
    to the ones you used in earlier recipes, and see how PyCaret simplifies the process
    for you.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用 PyCaret 来检查多种异常检测算法，类似于你在之前的食谱中使用过的算法，并查看 PyCaret 如何简化这个过程。
- en: Getting ready
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'The recommended way to explore PyCaret is to create a new virtual Python environment
    just for PyCaret so it can install all the required dependencies without any conflicts
    or issues with your current environment. If you need a quick refresher on how
    to create a virtual Python environment, check out the *Development environment
    setup* recipe, from *Chapter 1*, *Getting Started with Time Series Analysis*.
    The chapter covers two methods: using `conda` and `venv`.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 探索 PyCaret 的推荐方式是为 PyCaret 创建一个新的虚拟 Python 环境，这样它可以安装所有所需的依赖项，而不会与当前环境发生任何冲突或问题。如果你需要快速回顾如何创建虚拟
    Python 环境，请参考*开发环境设置*食谱，见*第 1 章*，*时间序列分析入门*。本章介绍了两种方法：使用`conda`和`venv`。
- en: 'The following instructions will show the process using `conda`. You can call
    the environment any name you like; for the following example, we will name our
    environment `pycaret`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下说明将展示使用`conda`的过程。你可以为环境命名任何你喜欢的名字；在以下示例中，我们将命名为`pycaret`：
- en: '[PRE53]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In order to make the new `pycaret` environment visible within Jupyter, you
    can run the following code:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使新的`pycaret`环境在 Jupyter 中可见，你可以运行以下代码：
- en: '[PRE54]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'There is a separate Jupyter notebook for this recipe, which you can download
    from the GitHub repository:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱有一个单独的 Jupyter notebook，你可以从 GitHub 仓库下载：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch14/Chapter%2014-pycaret.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch14/Chapter%2014-pycaret.ipynb)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch14/Chapter%2014-pycaret.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch14/Chapter%2014-pycaret.ipynb)'
- en: How to do it...
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, you will not be introduced to any new concepts. The focus is
    to demonstrate how PyCaret can be a great starting point when you are experimenting
    and want to quickly evaluate different models. You will load PyCaret and run it
    for different outlier detection algorithms:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，你将不会接触到任何新概念。重点是展示如何使用 PyCaret 作为实验的起点，并快速评估不同的模型。你将加载 PyCaret 并运行不同的异常检测算法：
- en: 'Start by loading all the available functions from the `pycaret.anomaly` module:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`pycaret.anomaly`模块加载所有可用的函数：
- en: '[PRE55]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The preceding code should produce a table summary as show in Figure 14.12
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应该生成一个表格摘要，如图 14.12 所示
- en: '![Figure 14.12 – PyCaret summary output](img/file291.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.12 – PyCaret 摘要输出](img/file291.png)'
- en: Figure 14.12 – PyCaret summary output
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.12 – PyCaret 摘要输出
- en: 'To print a list of available outlier detection algorithms, you can run `models()`:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要打印可用的异常检测算法列表，可以运行`models()`：
- en: '[PRE56]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'This should display a pandas DataFrame, as follows:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会显示一个 pandas DataFrame，如下所示：
- en: '![Figure 14.14: Available outlier detection algorithms from PyCaret](img/file292.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.14: PyCaret 可用的异常检测算法](img/file292.png)'
- en: 'Figure 14.14: Available outlier detection algorithms from PyCaret'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14.14: PyCaret 可用的异常检测算法'
- en: Notice these are all sourced from the PyOD library. As stated earlier, PyCaret
    is a wrapper on top of PyOD and other libraries, such as scikit-learn.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些算法都来自 PyOD 库。如前所述，PyCaret 是 PyOD 和其他库（如 scikit-learn）的封装。
- en: 'Let''s store the names of the first eight algorithms in a list to use later:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将前八个算法的名称存储在列表中，以便稍后使用：
- en: '[PRE57]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Loop through the list of algorithms and store the output in a dictionary so
    you can reference it later for your analysis. To create a model in PyCaret, you
    simply use the `create_model()` function. This is similar to the `fit()` function
    in scikit-learn and PyOD for training the model. Once the model is created, you
    can use the model to predict (identify) the outliers using the `predict_model()`
    function. PyCaret will produce a DataFrame with three columns: the original `value`
    column, a new column, `Anomaly`, which stores the outcome as either `0` or `1`,
    where `1` indicates an outlier, and another new column, `Anomaly_Score`, which
    stores the score used (the higher the score, the higher the chance it is an anomaly).'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历算法列表并将输出存储在字典中，以便稍后在分析中引用。要在PyCaret中创建模型，你只需使用`create_model()`函数。这类似于scikit-learn和PyOD中的`fit()`函数，用于训练模型。一旦模型创建完成，你可以使用该模型通过`predict_model()`函数预测（识别）离群值。PyCaret将生成一个包含三列的DataFrame：原始的`value`列，一个新的`Anomaly`列，存储结果为`0`或`1`，其中`1`表示离群值，另一个新的`Anomaly_Score`列，存储使用的得分（得分越高，表示越有可能是离群值）。
- en: 'You will only change the contamination parameter to match earlier recipes using
    PyOD. In PyCaret, the contamination parameter is called `fraction` and to be consistent,
    you will set that to `0.03` or 3% with `fraction=0.03`:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要改变污染参数，以便与之前使用PyOD的配方匹配。在PyCaret中，污染参数被称为`fraction`，为了保持一致性，你需要将其设置为`0.03`或者3%，即`fraction=0.03`：
- en: '[PRE58]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The `results` dictionary contains the output (a DataFrame) from each model.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`results`字典包含每个模型的输出（一个DataFrame）。'
- en: 'To print out the outliers from each model, you can simply loop through the
    dictionary:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要打印每个模型的离群值，你可以简单地遍历字典：
- en: '[PRE59]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This should print the results for each of the eight models. The following are
    the first two models from the list as an example:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出每个模型的结果。以下是列表中的前两个模型作为示例：
- en: '[PRE60]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: How it works...
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'PyCaret is a great library for automated machine learning, and recently they
    have been expanding their capabilities around time series analysis and forecasting
    and anomaly (outlier) detection. PyCaret is a wrapper over PyOD, the same library
    you used in earlier recipes of this chapter. *Figure 14.14* shows the number of
    PyOD algorithms supported by PyCaret, which is a subset of the more extensive
    list from PyOD: [https://pyod.readthedocs.io/en/latest/index.html#implemented-algorithms](https://pyod.readthedocs.io/en/latest/index.html#implemented-algorithms).'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: PyCaret是一个出色的自动化机器学习库，最近他们在时间序列分析、预测和离群值（异常值）检测方面不断扩展其功能。PyCaret是PyOD的封装库，你在本章之前的配方中也使用了PyOD。*图14.14*展示了PyCaret支持的PyOD算法数量，这是PyOD更广泛算法列表的一个子集：[https://pyod.readthedocs.io/en/latest/index.html#implemented-algorithms](https://pyod.readthedocs.io/en/latest/index.html#implemented-algorithms)。
- en: See also
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'To learn more about PyCaret''s outlier detection, please visit the official
    documentation here: [https://pycaret.gitbook.io/docs/get-started/quickstart#anomaly-detection](https://pycaret.gitbook.io/docs/get-started/quickstart#anomaly-detection).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于PyCaret离群值检测的信息，请访问官方文档：[https://pycaret.gitbook.io/docs/get-started/quickstart#anomaly-detection](https://pycaret.gitbook.io/docs/get-started/quickstart#anomaly-detection)。
