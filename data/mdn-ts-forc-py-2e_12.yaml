- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Global Forecasting Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全球预测模型
- en: In previous chapters, we saw how we can use modern machine learning models on
    time series forecasting problems, essentially replacing traditional models such
    as ARIMA or exponential smoothing. However, before now, we were looking at the
    different time series in any dataset (such as households in the London Smart Meters
    dataset) in isolation, just as the traditional models did.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经看到如何使用现代机器学习模型解决时间序列预测问题，本质上是替代了传统的模型，如 ARIMA 或指数平滑。然而，直到现在，我们一直将数据集中的不同时间序列（例如伦敦智能电表数据集中的家庭数据）单独分析，这就像传统模型所做的那样。
- en: However, we will now explore a different paradigm of modeling where we use a
    single machine learning model to forecast a bunch of time series together. As
    we will learn in the chapter, this paradigm brings many benefits with it, from
    the perspective of both computation and accuracy.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在我们将探讨一种不同的建模范式，其中我们使用单一的机器学习模型一起预测多个时间序列。正如本章所学，这种范式在计算和准确性方面都带来了许多好处。
- en: 'In this chapter, we will be covering these main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主要内容：
- en: Why Global Forecasting Models?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择全球预测模型？
- en: Creating GFMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建全球预测模型（GFMs）
- en: Strategies to improve GFMs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善全球预测模型的策略
- en: Interpretability
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need to set up the **Anaconda** environment following the instructions
    in the *Preface* of the book to get a working environment with all the libraries
    and datasets required for the code in this book. Any additional library will be
    installed while running the notebooks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要按照书中*前言*中的说明设置 **Anaconda** 环境，以获得一个包含所有所需库和数据集的工作环境。任何额外的库将在运行笔记本时自动安装。
- en: 'You will need to run the following notebooks before using the code in this
    chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用本章代码之前，您需要运行以下笔记本：
- en: '`02-Preprocessing_London_Smart_Meter_Dataset.ipynb` in `Chapter02`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-Preprocessing_London_Smart_Meter_Dataset.ipynb`（第2章）'
- en: '`01-Setting_up_Experiment_Harness.ipynb` in `Chapter04`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-Setting_up_Experiment_Harness.ipynb`（第4章）'
- en: 'From the `Chapter06` and `Chapter07` folders:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 `Chapter06` 和 `Chapter07` 文件夹：
- en: '`01-Feature_Engineering.ipynb`'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-Feature_Engineering.ipynb`'
- en: '`02-Dealing_with_Non-Stationarity.ipynb`'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-Dealing_with_Non-Stationarity.ipynb`'
- en: '`02a-Dealing_with_Non-Stationarity-Train+Val.ipynb`'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02a-Dealing_with_Non-Stationarity-Train+Val.ipynb`'
- en: 'From the `Chapter08` folder:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 `Chapter08` 文件夹：
- en: '`00-Single_Step_Backtesting_Baselines.ipynb`'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`00-Single_Step_Backtesting_Baselines.ipynb`'
- en: '`01-Forecasting_with_ML.ipynb`'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-Forecasting_with_ML.ipynb`'
- en: '`01a-Forecasting_with_ML_for_Test_Dataset.ipynb`'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01a-Forecasting_with_ML_for_Test_Dataset.ipynb`'
- en: '`02-Forecasting_with_Target_Transformation.ipynb`'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-Forecasting_with_Target_Transformation.ipynb`'
- en: '`02a-Forecasting_with_Target_Transformation(Test).ipynb`'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02a-Forecasting_with_Target_Transformation(Test).ipynb`'
- en: The associated code for the chapter can be found at [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter10](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter10).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的相关代码可以在 [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter10](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter10)
    找到。
- en: Why Global Forecasting Models?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择全球预测模型？
- en: We talked about global models briefly in *Chapter 5*, *Time Series Forecasting
    as Regression*, where we mentioned related datasets. We can think of many scenarios
    where we would encounter related time series. We may need to forecast the sales
    for all the products of a retailer, the number of rides requested for a cab service
    across different areas of a city, or the energy consumption of all the households
    in a particular area (which is what the London Smart Meters dataset does). We
    call these related time series because all the different time series in the dataset
    can have a lot of factors in common with each other. For instance, the yearly
    seasonality that might occur in retail products might be present for a large section
    of products, or the way an external factor such as temperature affects energy
    consumption may be similar for a large number of households. Therefore, one way
    or the other, the different time series in a related time series dataset share
    attributes between them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第五章*中简要讨论了全球模型，*时间序列预测作为回归*，并提到了相关数据集。我们可以想到很多场景，在这些场景中我们会遇到相关的时间序列。例如，我们可能需要预测零售商所有产品的销售量，城市不同地区出租车服务的请求数量，或者某个特定区域所有家庭的能源消耗（这正是伦敦智能电表数据集的用途）。我们称这些为相关时间序列，因为数据集中的所有不同时间序列可能具有许多共同的因素。例如，零售产品可能会出现的年度季节性现象可能会出现在大部分产品上，或者温度等外部因素对能源消耗的影响可能对大量家庭相似。因此，不管是以何种方式，相关时间序列数据集中的不同时间序列之间共享一些特征。
- en: Traditionally, we used to consider each time series an independent time series;
    in other words, each time series was assumed to be generated using a different
    data-generating process. Classical models such as ARIMA and exponential smoothing
    are trained for each time series. However, we can also consider all the time series
    in the dataset as being generated from a single data-generating process, and the
    subsequent modeling approach would be to train a single model to forecast all
    the time series in the dataset. The latter is what we refer to as **Global Forecasting
    Models** (**GFMs**). **GFMs** are models that are designed to handle multiple
    related time series, allowing for shared learning across those time series. In
    contrast, the traditional approach is referred to as **Local Forecasting Models**
    (**LFMs**).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，我们通常认为每个时间序列是独立的时间序列；换句话说，每个时间序列被假设是由不同的数据生成过程生成的。像ARIMA和指数平滑等经典模型是针对每个时间序列进行训练的。然而，我们也可以认为数据集中的所有时间序列是由单一的数据生成过程生成的，随之而来的是一种建模方法，即训练一个单一的模型来预测数据集中的所有时间序列。后者就是我们所称的**全球预测模型**（**GFMs**）。**GFMs**是旨在处理多个相关时间序列的模型，允许这些时间序列之间进行共享学习。相比之下，传统方法被称为**局部预测模型**（**LFMs**）。
- en: Although we briefly talked about the drawbacks of LFMs in *Chapter 5*, *Time
    Series Forecasting as Regression*, let’s summarize them in a more concrete fashion
    and see why GFMs help us smooth over a lot of those drawbacks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在*第五章*中简要讨论了LFMs的缺点，*时间序列预测作为回归*，但我们可以以更具体的方式总结这些缺点，看看为什么GFMs有助于克服这些问题。
- en: Sample size
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本大小
- en: In most real-world applications (especially in business forecasting), the time
    series we have to forecast is not very long. Adopting a completely data-driven
    approach to modeling such a small time series is problematic. Training a highly
    flexible model with a handful of data points will lead to the model memorizing
    the training data, resulting in an overfit.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实际应用中（尤其是在商业预测中），我们需要预测的时间序列并不长。采用完全以数据为驱动的建模方法来处理这样一个较短的时间序列是有问题的。使用少量数据点训练一个高度灵活的模型会导致模型记住训练数据，从而出现过拟合。
- en: Traditionally, this has been overcome by placing strong priors or inductive
    bias into the models we use for forecasting. Inductive bias loosely refers to
    a set of assumptions or restrictions that are built into a model that should help
    the model predict feature combinations it has not encountered while training.
    For instance, double exponential smoothing has strong assumptions about seasonality
    and trend. The model does not allow any other more complicated patterns to be
    learned from the data. Therefore, using these strong assumptions, we are restricting
    the model search to a small section of the hypothesis space. While this helps
    in low-data regimes, the flip side is that these assumptions may limit accuracy.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，这个问题通过在我们用于预测的模型中加入强先验或归纳偏差来克服。归纳偏差大致指的是一组假设或限制，这些假设或限制被内置到模型中，应该帮助模型预测在训练过程中没有遇到的特征组合。例如，双指数平滑法对季节性和趋势有强烈的假设。该模型不允许从数据中学习任何其他更复杂的模式。因此，使用这些强假设，我们将模型的搜索范围限制在假设空间的一个小部分。虽然在数据较少的情况下这有助于提高准确性，但其反面是这些假设可能限制了模型的准确性。
- en: Recent developments in the field of machine learning have shown us without a
    doubt that using a data-driven approach (with much fewer assumptions or priors)
    on large training sets will lead to us training better models. However, conventional
    statistical wisdom tells us that the number of data points needs to be at least
    10 to 100 times the number of parameters that we are trying to learn from those
    data points.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在机器学习领域的进展无疑向我们展示了，使用数据驱动的方法（假设或先验较少）在大规模训练集上能训练出更好的模型。然而，传统的统计学观点告诉我们，数据点的数量需要至少是我们尝试从这些数据点中学习的参数数量的10到100倍。
- en: So, if we stick to LFMs, scenarios in which we can adopt a completely data-driven
    approach will be very rare. This is where GFMs shine. A GFM is able to use the
    history of *all* the time series in a dataset to train the model and learn a single
    set of parameters that work for all the time series in the dataset. Borrowing
    the terminology introduced in *Chapter 5*, *Time Series Forecasting as Regression*,
    we increase the *width* of the dataset, keeping the *length* the same (refer back
    to *Figure 5.2*). This explosion of historical information available to a single
    model lets us use completely data-driven techniques on time series datasets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们坚持使用LFMs（局部因果模型），完全数据驱动的方法能被采纳的场景将非常少见。这正是GFMs的优势所在。GFM能够利用数据集中*所有*时间序列的历史数据来训练模型，并学习一组适用于数据集中所有时间序列的参数。借用在*第5章*《时间序列预测作为回归》中引入的术语，我们增加了数据集的*宽度*，而保持*长度*不变（参见*图5.2*）。这种为单一模型提供的大量历史信息让我们可以对时间序列数据集采用完全数据驱动的技术。
- en: Cross-learning
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨领域学习
- en: GFMs, by design, promote cross-learning across different time series in a dataset.
    Imagine we have a time series that is quite new and does not have a history rich
    enough for teaching the model—for instance, the sales of a newly introduced retail
    product or the electricity consumption of a new household in a region. If we consider
    these time series in isolation, it will be a while before we start to get reasonable
    forecasts from the models we train on them, but GFMs make that process easier
    by enabling cross-learning. GFMs have an implicit sense of similarity between
    different time series and they will be able to use patterns they have seen in
    similar time series with a rich history to come up with a forecast on the new
    time series.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: GFMs（广义因果模型）从设计上促进了数据集中不同时间序列之间的跨领域学习。假设我们有一个相对较新的时间序列，且其历史数据不足以有效地训练模型——例如，新推出的零售产品的销售数据或某地区新家庭的电力消费数据。如果我们将这些时间序列单独考虑，可能需要一段时间才能从我们训练的模型中得到合理的预测，但GFMs通过跨领域学习使得这个过程变得更加简单。GFMs具有不同时间序列之间的隐性相似性，它们能够利用在历史数据丰富的类似时间序列中观察到的模式，来为新的时间序列生成预测。
- en: Another way cross-learning helps is by acting like a regularizer while estimating
    common parameters such as seasonality. For instance, the seasonality exhibited
    by similar products in a retail scenario is best estimated at an aggregate level,
    because each individual time series will have some sort of noise that can creep
    into the seasonality extraction. By enforcing common seasonality across multiple
    products, we are essentially regularizing the seasonality estimation and, in the
    process, making the seasonality estimate more robust. The good thing about GFMs
    is that they take a data-driven approach to define the seasonality of which products
    should be estimated together and which ones have different patterns. If you have
    different seasonality patterns in different products, a GFM may struggle to model
    them together. However, when provided with enough information on how to distinguish
    between different products, the GFM will be able to learn that difference too.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉学习的另一种帮助方式是，在估算共同参数（如季节性）时，它起到一种正则化作用。例如，在零售场景中，类似产品所表现出的季节性最好在汇总层面进行估算，因为每个单独的时间序列都可能会有一些噪声，这些噪声可能会影响季节性提取。通过在多个产品之间强制统一季节性，我们实际上是在正则化季节性估算，并且在这个过程中，使季节性估算更加稳健。GFMs的优点在于，它们采用数据驱动的方法来定义哪些产品的季节性应该一起估算，哪些产品有不同的季节性模式。如果不同产品之间有不同的季节性模式，GFM可能难以将它们一起建模。然而，当提供足够的区分不同产品的信息时，GFM也能学会这种差异。
- en: Multi-task learning
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多任务学习
- en: 'GFMs can be considered multi-task learning paradigms where a single model is
    trained to learn multiple tasks (as forecasting each time series is a separate
    task). Multi-task learning is an active area of research, and there are many benefits
    to using multi-task models:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: GFMs可以视为多任务学习范式，其中一个模型被训练用来学习多个任务（因为预测每个时间序列是一个独立的任务）。多任务学习是一个活跃的研究领域，使用多任务模型有许多好处：
- en: When the model is learning from noisy, high-dimensional data, it becomes harder
    for the model to distinguish between useful and non-useful features. When we train
    the model on a multi-task paradigm, the model can understand useful features by
    looking at features that are useful for other tasks as well, thus providing the
    model with an additional perspective for discerning useful features.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型从嘈杂的高维数据中学习时，模型区分有用特征和无用特征的难度加大。当我们在多任务框架下训练模型时，模型可以通过观察对其他任务也有用的特征，理解有用特征，从而为模型提供额外的视角来识别有用特征。
- en: Sometimes, features such as seasonality might be hard to learn from a particularly
    noisy time series. However, under a multi-task framework, the model can learn
    the difficult features using other time series in the dataset.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时候，像季节性这样的特征可能很难从特别嘈杂的时间序列中学习。然而，在多任务框架下，模型可以利用数据集中的其他时间序列来学习这些困难的特征。
- en: Finally, multi-task learning introduces a kind of regularization that forces
    the model to find a model that works well on all tasks, thus reducing the risk
    of overfitting.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，多任务学习引入了一种正则化方法，它迫使模型找到一个在所有任务上都表现良好的模型，从而减少过拟合的风险。
- en: Engineering complexity
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工程复杂度
- en: LFMs pose a challenge from the engineering side as well for large datasets.
    If we have thousands or millions of time series to forecast, it becomes increasingly
    difficult to both train and manage the life cycle of these LFMs. In *Chapter 8*,
    *Forecasting Time Series with Machine Learning Models*, we trained LFMs for just
    a subset of households in the dataset. It took almost 20 to 30 minutes to train
    a machine learning model for all 150 households and we ran them with the default
    hyperparameters. In a normal machine learning workflow, we train multiple machine
    learning models and do hyperparameter tuning to find the best configuration of
    the model. However, carrying out all these steps for thousands of time series
    in a dataset becomes increasingly complex and time-consuming.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大规模数据集，LFMs也带来了工程上的挑战。如果我们需要预测数千个甚至数百万个时间序列，训练和管理这些LFMs的生命周期变得越来越困难。在*第8章*，*使用机器学习模型预测时间序列*中，我们仅对数据集中一部分家庭进行了LFM训练。我们花了大约20到30分钟来训练150个家庭的机器学习模型，并且使用的是默认的超参数。在常规的机器学习工作流程中，我们需要训练多个机器学习模型并进行超参数调优，以找到最佳的模型配置。然而，对于数据集中的成千上万的时间序列执行所有这些步骤，变得越来越复杂和耗时。
- en: Equally, then there is the issue of managing the life cycle of these models.
    All these individual models need to be deployed to production, their performance
    needs to be monitored to check for model and data drift, and they need to be retrained
    at a set frequency. This becomes increasingly complex as we have more and more
    time series to forecast.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这就涉及到如何管理这些模型的生命周期。所有这些单独的模型都需要部署到生产环境中，需要监控它们的表现以检查模型和数据的漂移，并且需要在设定的频率下重新训练。随着我们需要预测的时间序列数量越来越多，这变得愈加复杂。
- en: However, by shifting to a GFM paradigm, we drastically reduce the time and effort
    required to train and manage a machine learning model throughout its life cycle.
    As we will see in this chapter, training a GFM on these 150 households takes only
    a fraction of the time it takes to train LFMs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过转向GFM范式，我们大大减少了在模型整个生命周期中训练和管理机器学习模型所需的时间和精力。正如我们将在本章中看到的，在这些150个家庭上训练GFM的时间只是训练LFM所需时间的一小部分。
- en: Despite all the advantages of GFMs, they are not without some drawbacks. The
    main drawback is that we are assuming that all the time series in a dataset are
    generated by a single **Data Generating Process** (**DGP**). This might not be
    a valid assumption and this can lead to the GFM underfitting some specific types
    of time series patterns that are underrepresented in the dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GFMs有许多优点，但它们也并非没有缺点。主要的缺点是我们假设数据集中的所有时间序列都是由单一的**数据生成过程**（**DGP**）生成的。这可能并不是一个有效的假设，这可能导致GFM无法拟合数据集中某些特定类型的时间序列模式，这些模式在数据集中出现得较少。
- en: Another open issue is whether a GFM is good for use with unrelated tasks or
    time series. The jury is out on this one, but Montero-Manso et al. proved that
    there are also gains in modeling unrelated time series with a GFM. The same finding
    has been put forward, although from another perspective, by Oreshkin et al., who
    trained a global model on the M4 dataset (a set of unrelated datasets) and obtained
    state-of-the-art performance. They attributed it to the meta-learning capabilities
    of the model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个未解的问题是，GFM是否适用于处理无关的任务或时间序列。这个问题仍在争论中，但Montero-Manso等人证明了使用GFM对无关时间序列建模也可以带来收益。Oreshkin等人从另一个角度提出了相同的发现，他们在M4数据集（一个无关的数据集）上训练了一个全局模型，并取得了最先进的表现。他们将这一成果归因于模型的元学习能力。
- en: That being said, relatedness does help the GFM, as the learning task becomes
    easier this way. We will see a practical application of this in upcoming sections
    of this chapter as well.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，相关性确实有助于GFM，因为这样学习任务变得更简单。我们将在本章的后续部分看到这一点的实际应用。
- en: In the larger scheme of things, the benefits we derive from a GFM paradigm far
    outweigh the drawbacks. On most tasks, the GFMs either perform on par with or
    better than local models. It has been proven theoretically as well, by Montero-Manso
    et al., that a GFM, in a worst-case scenario, learns the same function as a local
    model. We will see this clearly in the models we are going to train in the upcoming
    sections. Finally, the training time and engineering complexity drop drastically
    as you move to a GFM paradigm.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从更大的角度来看，我们从GFM范式中获得的好处远大于其缺点。在大多数任务中，GFMs的表现与局部模型相当，甚至更好。Montero-Manso等人也从理论上证明了，在最坏的情况下，GFM学到的函数与局部模型相同。我们将在接下来的部分中清楚地看到这一点。最后，随着你转向GFM范式，训练时间和工程复杂性都会大幅降低。
- en: Now that we have explained why a GFM is a worthwhile paradigm to adopt, let’s
    see how we can train one.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解释了为什么GFM是一个值得采用的范式，让我们看看如何训练一个GFM。
- en: Creating GFMs
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建GFMs
- en: Training a GFM is very straightforward. While we were training LFMs in *Chapter
    8*, *Forecasting Time Series with Machine Learning Models*, we were looping over
    different households in the London Smart Meters dataset and training a model for
    each household. However, if we just take all the households into a single dataframe
    (our dataset is already that way) and train a single model on it, we get a GFM.
    One thing we want to keep in mind is to make sure that all the time series in
    the dataset have the same frequency. In other words, if we mix daily time series
    with weekly ones while training these models, the performance drop will be noticeable—especially
    if we are using time-varying features and other time-based information. For a
    purely autoregressive model, mixing time series in this way is much less of a
    problem.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GFM非常简单。在*第8章*《使用机器学习模型进行时间序列预测》中，我们训练LFM时，是在伦敦智能电表数据集中循环处理不同家庭，并为每个家庭训练一个模型。然而，如果我们将所有家庭的数据放入一个单一的数据框（我们的数据集本来就是这样的），并在其上训练一个单一的模型，我们就得到了一个GFM。需要记住的一点是，确保数据集中的所有时间序列具有相同的频率。换句话说，如果我们在训练这些模型时将日常时间序列与每周时间序列混合，性能下降是显而易见的——尤其是在使用时间变化特征和其他基于时间的信息时。对于纯自回归模型来说，以这种方式混合时间序列问题要小得多。
- en: '**Notebook alert:**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒：**'
- en: To follow along with the complete code, use the notebook named `01-Global_Forecasting_Models-ML.ipynb`
    in the `Chapter10` folder.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随完整的代码，请使用`Chapter10`文件夹中的`01-Global_Forecasting_Models-ML.ipynb`笔记本。
- en: The standard framework we developed in *Chapter 8*, *Forecasting Time Series
    with Machine Learning Models*, is general enough to work for GFMs as well. So,
    as we did in that chapter, we define `FeatureConfig` and `MissingValueConfig`
    in the `01-Global_Forecasting_Models-ML.ipynb` notebook. We also slightly tweaked
    the Python function to train and evaluate the machine learning to make it work
    for all households. The details and exact functions can be found in the notebook.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第8章*《使用机器学习模型进行时间序列预测》中开发的标准框架足够通用，也适用于GFM。因此，正如我们在该章节中所做的，我们在`01-Global_Forecasting_Models-ML.ipynb`笔记本中定义了`FeatureConfig`和`MissingValueConfig`。我们还稍微调整了Python函数，用于训练和评估机器学习模型，使其适用于所有家庭。详细信息和确切的函数可以在该笔记本中找到。
- en: 'Now, instead of looping over different households, we input the entire training
    dataset into the `get_X_y` function:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，代替循环处理不同的家庭，我们将整个训练数据集输入到`get_X_y`函数中：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have the data, we need to train the model. Training the model is
    also exactly the same as we saw in *Chapter 8*, *Forecasting Time Series with
    Machine Learning Models*. We will just choose LightGBM, which was the best-performing
    LFM model, and use functions we defined earlier to train the model and evaluate
    the results:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了数据，接下来需要训练模型。训练模型也和我们在*第8章*《使用机器学习模型进行时间序列预测》中看到的一模一样。我们只需选择LightGBM，这是表现最佳的LFM模型，并使用之前定义的函数来训练模型并评估结果：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, in `y_pred`, we will have the forecast for all the households and `feat_df`
    will have the feature importance. `agg_metrics` will have the aggregated metric
    for all the selected households.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在`y_pred`中，我们将获得所有家庭的预测值，而`feat_df`将包含特征重要性。`agg_metrics`将包含所有选定家庭的汇总指标。
- en: 'Let’s look at how well our GFM model did:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下我们的GFM模型表现如何：
- en: '![Figure 10.1 – Aggregate metrics with the baseline GFM ](img/B22389_10_01.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 基准GFM的汇总指标](img/B22389_10_01.png)'
- en: 'Figure 10.1: Aggregate metrics with the baseline GFM'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：基准GFM的汇总指标
- en: We are not doing better than the best LFM (in the first row) in terms of the
    metrics. However, one thing we should note is the time taken to train the model—~30
    seconds. The LFM for all the selected households was taking ~30 minutes. This
    huge reduction in time taken gives us a lot of flexibility to iterate faster with
    different features and techniques.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 就指标而言，我们的表现不如最佳的LFM（第一行）。然而，有一点我们应该注意的是训练模型所花费的时间——大约30秒。所有选定家庭的LFM训练时间大约需要30分钟。这个时间的大幅减少为我们提供了更多的灵活性，可以更快速地迭代不同的特征和技术。
- en: With that said, let’s now look at a few techniques with which we can improve
    the accuracy of the GFMs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们现在看看一些可以提高GFM准确性的技术。
- en: Strategies to improve GFMs
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改善GFM的策略
- en: 'GFMs have been in use in many forecasting competitions in Kaggle and outside
    of it. They have been battle-tested empirically, although very little work has
    gone into examining why they work so well from a theoretical point of view. Montero-Manso
    and Hyndman (2020) have a working paper titled *Principles and Algorithms for
    Forecasting Groups of Time Series: Locality and Globality*, which is an in-depth
    investigation, both theoretical and empirical, of GFMs and the many techniques
    that have been developed by the data science community collectively. In this section,
    we will try to include strategies to improve GFMs and, wherever possible, try
    to give theoretical justifications for why they would work.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: GFMs在许多Kaggle和其他预测竞赛中被使用。它们已经经过实证检验，尽管很少有研究从理论角度探讨它们为何表现如此出色。Montero-Manso和Hyndman（2020）有一篇工作论文，题为《时间序列分组预测的原理与算法：局部性与全局性》，该论文对GFMs以及数据科学社区集体开发的许多技术进行了深入的理论和实证研究。在本节中，我们将尝试提出改进GFMs的策略，并尽可能地给出理论依据，解释为何这些策略有效。
- en: '**Reference check:**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查：**'
- en: The Montero-Manso and Hyndman (2020) research paper is cited in *References*
    as reference *1*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Montero-Manso和Hyndman（2020）的研究论文在*参考文献*中作为参考文献*1*被引用。
- en: 'In the paper, Montero-Manso and Hyndman use a basic result in machine learning
    about generalization error to carry out the theoretical analysis, and it is worth
    spending a bit of time understanding that, at least at a high level. `Generalization
    error`, as we know, is the difference between out-of-sample error and in-sample
    error. Yaser S Abu-Mostafa has a free, online **Massive Open Online Course** (**MOOC**)
    and an associated book (both of which are linked in the *Further reading* section).
    It is a short course on machine learning and is a course that I would recommend
    to anyone in the machine learning field for developing a stronger theoretical
    and conceptual basis for what we do. One of the important concepts the course
    and book put forward is the use of Hoeffding’s inequality from probability theory
    to derive bounds on a learning problem. Let’s quickly look at the result to develop
    our understanding:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，Montero-Manso和Hyndman利用机器学习中关于泛化误差的基本结果进行理论分析，值得花些时间理解这个概念，至少在高层次上是这样。`泛化误差`，我们知道，是样本外误差与样本内误差之间的差异。Yaser
    S Abu-Mostafa有一个免费的在线**大规模开放在线课程**（**MOOC**）和一本相关的书（两者都可以在*进一步阅读*部分找到）。这是一个关于机器学习的简短课程，我推荐给任何希望在机器学习领域建立更强理论和概念基础的人。课程和书籍提出的一个重要概念是使用概率理论中的Hoeffding不等式来推导学习问题的界限。让我们快速看一下这个结果，以加深理解：
- en: '![](img/B22389_10_001.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_10_001.png)'
- en: It has a probability of at least 1**-![](img/B22389_10_002.png).**
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它的概率至少为1**-![](img/B22389_10_002.png).**
- en: '*E*[in] is the in-sample average error and *E*[out] is the expected out-of-sample
    error. *N* is the total number of samples in the dataset from which we are learning
    and *H* is the hypothesis class of models. It is a finite set of functions that
    can potentially fit the data. The size of *H*, denoted by |*H*|, is the complexity
    of *H*. Although the formula of the bound looks intimidating, let’s simplify the
    way we look at it to develop the necessary understanding.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*E*[in]是样本内平均误差，*E*[out]是预期的样本外误差。*N*是我们从中学习的数据集的总样本数，*H*是模型的假设类。它是一个有限的函数集合，可能适配数据。*H*的大小，记作|*H*|，表示*H*的复杂度。虽然这个界限的公式看起来让人害怕，但让我们简化一下它的表达方式，以便发展我们对它的必要理解。'
- en: We want *E*[out] to be as close to *E*[in] as possible, and for that, we need
    the terms in the square root to be as small as possible. There are two terms under
    the square root that are in our *control*, so to speak—*N* and |*H*|. Therefore,
    to make the generalization error (*E*[in]- *E*[out]) as small as possible, we
    either need to increase *N* (have more data) or decrease |*H*| (have a less complex
    model). This is a result that is applicable to all machine learning but Montero-Manso
    and Hyndman, with a few assumptions, made this applicable to time series models
    as well. It is this result that they used to give theoretical backing to the arguments
    put forward in their working paper.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望*E*[out]尽可能接近*E*[in]，为此，我们需要使平方根中的项尽可能小。平方根下有两个项在我们的*控制*之中，可以这么说——*N*和|*H*|。因此，为了使泛化误差（*E*[in]
    - *E*[out]）尽可能小，我们要么需要增加*N*（拥有更多数据），要么需要减小|*H*|（采用更简单的模型）。这是一个适用于所有机器学习的结果，但Montero-Manso和Hyndman在一些假设条件下，将这一结果也适用于时间序列模型。他们使用这一结果为他们工作论文中的论点提供了理论支持。
- en: 'Montero-Manso and Hyndman have taken Hoeffding’s inequality and applied it
    to LFMs and GFMs to compare them. We can see the result here (for a full mathematical
    and statistical understanding, refer to the original paper under *References*):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Montero-Manso 和 Hyndman 将 Hoeffding 不等式应用于 LFM 和 GFM 进行比较。我们可以在这里看到结果（有关完整的数学和统计理解，请参见*参考文献*中的原始论文）：
- en: '![](img/B22389_10_003.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_10_003.png)'
- en: '![](img/B22389_10_004.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_10_004.png)'
- en: '![](img/B22389_10_005.png) and **![](img/B22389_10_006.png)** are the average
    in-sample errors across all the time series using the local and global approaches,
    respectively. **![](img/B22389_10_006.png)** and **![](img/B22389_10_008.png)**
    are the out-of-sample expectations under the local and global approaches, respectively.
    *H*[i] is the hypothesis class for the *i*-th time series and *J* is the hypothesis
    class for the global approach (the global approach only fits a single function
    and hence, has just a single hypothesis class).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B22389_10_005.png) 和 **![](img/B22389_10_006.png)** 分别是使用局部方法和全局方法时所有时间序列的平均样本内误差。**![](img/B22389_10_006.png)**
    和 **![](img/B22389_10_008.png)** 分别是局部方法和全局方法下的样本外期望值。*H*[i] 是第 *i* 个时间序列的假设类，*J*
    是全局方法的假设类（全局方法只拟合一个函数，因此只有一个假设类）。'
- en: One of the most interesting results that comes out of this is that the complexity
    term for LFMs (![](img/B22389_10_009.png)) grows the size of the dataset. The
    greater the number of time series we have in the dataset, the more complexity
    and the worse the generalization error, whereas with GFMs, the complexity term
    (*log*(|*J*|)) stays constant. Therefore, for a dataset of moderate size, the
    overall complexity of LFMs (such as exponential smoothing) can be much higher
    than a single GFM, no matter how complex the GFM is. As a corollary, we can also
    think that with the available dataset (*NK*), we can afford to train a model with
    much higher complexity than a model for LFMs. There are many ways to increase
    the complexity of the model, which we will see in the following section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个有趣的结果是，LFM 的复杂性项（![](img/B22389_10_009.png)）会随着数据集大小的增长而增加。数据集中时间序列的数量越多，复杂性越高，泛化误差越大；而对于
    GFM，复杂性项（*log*(|*J*|)）保持不变。因此，对于中等大小的数据集，LFM（如指数平滑法）的整体复杂性可能远高于单一的 GFM，无论 GFM
    如何复杂。作为一个推论，我们还可以认为，在可用数据集（*NK*）的情况下，我们可以训练一个具有更高复杂度的模型，而不仅仅是 LFM 的模型。增加模型复杂度有很多方法，我们将在下一节中看到。
- en: Now, let’s return to the GFMs we were training. We saw that the performance
    of the GFM we trained was not up to the mark when we compared it with the best
    LFM (LightGBM), but it is better than the baseline and other models we tried,
    so right off the bat, we know the GFM we trained is not terrible. Now, let’s look
    at a few ways to improve the performance of the model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到我们正在训练的 GFM。我们看到，当我们将训练的 GFM 与最好的 LFM（LightGBM）进行比较时，GFM 的性能未达到预期，但它比基准和我们尝试的其他模型更好，因此，我们一开始就知道我们训练的
    GFM 还不算差。现在，让我们来看一些提高模型性能的方法。
- en: Increasing memory
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加记忆
- en: As we discussed in *Chapter 5*, *Time Series Forecasting as Regression*, the
    machine learning models that we discuss in this book are finite memory models
    or Markov models. A model such as exponential smoothing takes into account the
    entire history of a time series while forecasting, but models such as any of the
    machine learning models we discussed only take in finite memory to make their
    predictions. In a finite memory model, the amount of memory we allow the model
    to access is called the size of the memory (*M*) or order of autoregression (from
    econometrics).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第5章*《*时间序列预测作为回归*》中讨论的那样，本书讨论的机器学习模型是有限记忆模型或马尔可夫模型。像指数平滑法这样的模型在预测时会考虑时间序列的整个历史，而我们讨论的任何机器学习模型仅使用有限的记忆来进行预测。在有限记忆模型中，允许模型访问的记忆量被称为记忆大小（*M*）或自回归阶数（经济计量学中的概念）。
- en: Providing a greater amount of memory to the model increases the complexity of
    the model. Therefore, one of the ways to increase the performance of the GFM is
    to increase the amount of memory the model has access to. There are many ways
    to increase the amount of memory.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为模型提供更大的记忆量会增加模型的复杂性。因此，提高 GFM 性能的一个方法是增加模型可访问的记忆量。增加记忆量有很多方式。
- en: Adding more lag features
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增加更多的滞后特征
- en: If you have prior exposure to ARIMA models, you will know that the number of
    **Autoregressive** (**AR**) terms are sparingly used. We usually see AR models
    with single-digit lags. There is nothing stopping us from running ARIMA models
    with larger lags, but since we do run ARIMA in the LFM paradigm, the model has
    to learn the parameters of all the lags using limited data and therefore, in practice,
    practitioners commonly choose smaller lags. However, when we are moving to GFMs,
    we can afford to have much larger lags. Montero-Manso and Hyndman empirically
    showed the benefits of adding more lags to GFMs. For highly seasonal time series,
    a peculiar phenomenon was observed. The accuracy improves with an increase in
    lags, but it then saturates and suddenly worsens when the lag becomes equal to
    the seasonal cycle. On further increasing the lags beyond the seasonal cycle,
    the accuracy shows huge gains. This may be because of the overfitting that happens
    because of seasonality. It becomes very easy for the model to favor the seasonal
    lag because it works very well in a sample, so it’s better to add a few more lags
    on the plus side of the seasonal cycle.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前接触过 ARIMA 模型，你会知道**自回归**（**AR**）项通常使用得很少。我们通常看到的 AR 模型滞后项为个位数。虽然没有什么可以阻止我们使用更大的滞后项来运行
    ARIMA 模型，但由于我们是在 LFM 范式下运行 ARIMA，模型必须使用有限的数据来学习所有滞后项的参数，因此，在实践中，实践者通常选择较小的滞后项。然而，当我们转向
    GFM 时，我们可以承受使用更大的滞后项。Montero-Manso 和 Hyndman 经验性地展示了将更多滞后项添加到 GFM 中的好处。对于高度季节性的时间序列，观察到了一种特殊现象：随着滞后项的增加，准确性提高，但在滞后项等于季节周期时准确性突然饱和并变差。当滞后项超过季节周期时，准确性则有了巨大的提升。这可能是因为季节性引发的过拟合现象。由于季节性，模型容易偏向季节性滞后项，因为它在样本中表现很好，因此最好在季节周期的加号一侧再增加一些滞后项。
- en: Adding rolling features
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加滚动特征
- en: Another way to increase the memory of the model is to include rolling averages
    as features. Rolling averages take information from extended windows on memory
    and encode that information by way of descriptive statistics (such as the mean
    or max). This is an efficient way of including the memory because we can take
    very large windows for memory and include the information as a single feature
    in the model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 增加模型记忆的另一种方法是将滚动平均值作为特征。滚动平均值通过描述性统计（如均值或最大值）对来自较大记忆窗口的信息进行编码。这是一种有效的包含记忆的方式，因为我们可以采用非常大的窗口来进行记忆，并将这些信息作为单一特征包含在模型中。
- en: Adding EWMA features
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加 EWMA 特征
- en: An **Exponentially Weighted Moving Average** (**EWMA**) is a way to include
    infinite memory in a finite memory model. The EWMA essentially takes the average
    of the entire history but is weighted according to the ![](img/B22389_04_009.png)
    that we set. Therefore, with different values of ![](img/B22389_04_009.png), we
    get different kinds of memory, again encoded as a single feature. Including different
    EWMA features has also empirically proved beneficial.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**指数加权移动平均**（**EWMA**）是一种在有限记忆模型中引入无限记忆的方法。EWMA 本质上计算整个历史的平均值，但根据我们设置的![](img/B22389_04_009.png)加权。因此，通过不同的![](img/B22389_04_009.png)值，我们可以获得不同种类的记忆，这些记忆又被编码为单一特征。包括不同的
    EWMA 特征也在经验上证明是有益的。'
- en: We have already included these kinds of features in our feature engineering
    (*Chapter 6*, *Feature Engineering for Time Series Forecasting*), and they are
    part of the baseline GFM we trained, so let’s move on to the next strategy for
    improving the accuracy of GFMs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在特征工程中包含了这些类型的特征（*第 6 章*，*时间序列预测的特征工程*），它们也是我们训练的基线 GFM 的一部分，因此让我们继续进行下一种提高
    GFM 准确性的策略。
- en: Using time series meta-features
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用时间序列元特征
- en: The baseline GFM we trained earlier in the *Creating Global Forecasting Models
    (GFMs)* section had lag features, rolling features, and EWMA features, but we
    have given no feature that helps the model distinguish between different time
    series in the dataset. The baseline GFM model learned a generalized function that
    generates a forecast provided the features. This might work well enough for homogenous
    datasets where all the time series are very similar in nature, but for heterogenous
    datasets, the information with which the model can distinguish each time series
    comes in handy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*创建全球预测模型（GFM）*部分中训练的基线 GFM 模型包含滞后特征、滚动特征和 EWMA 特征，但我们并没有提供任何帮助模型区分数据集中不同时间序列的特征。基线
    GFM 模型学到的是一个通用的函数，该函数在给定特征的情况下生成预测。对于所有时间序列非常相似的同质数据集，这可能工作得足够好，但对于异质数据集，模型能够区分每个时间序列的信息就变得非常有用。
- en: So, information about the time series itself is what we call meta-features.
    In a retail context, it can be the product ID, the category of products, the store
    number, and so on. In our dataset, we have features such as `stdorToU`, `Acorn`,
    `Acorn_grouped`, and `LCLid`, which give some information about the time series
    itself. Including these meta-features in the GFM will improve the performance
    of the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关于时间序列本身的信息就是我们所说的元特征。在零售环境中，这些元特征可以是产品ID、产品类别、商店编号等。在我们的数据集中，我们有像`stdorToU`、`Acorn`、`Acorn_grouped`和`LCLid`这样的特征，它们提供了一些关于时间序列本身的信息。将这些元特征包含在GFM中将提高模型的性能。
- en: However, there is just one problem—more often than not, these meta-features
    are categorical in nature. A feature is categorical when the values in the feature
    can only take discrete values. For instance, `Acorn_grouped` can only have one
    of three values—`Affluent`, `Comfortable`, or `Adversity`. Most machine learning
    models do not work well with categorical features. All the models in scikit-learn,
    the most popular machine learning library in the Python ecosystem, do not allow
    categorical features at all. To include categorical features in machine learning
    models, we need to encode them into numerical form, and there are many ways to
    encode categorical columns. Let’s review a few popular options.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个问题——往往这些元特征是类别型的。当特征中的值只能取离散的值时，该特征就是类别型的。例如，`Acorn_grouped`只能有三个值之一——`Affluent`、`Comfortable`或`Adversity`。大多数机器学习模型处理类别型特征的效果不好。Python生态系统中最流行的机器学习库scikit-learn中的所有模型都完全不支持类别型特征。为了将类别型特征纳入机器学习模型，我们需要将它们编码成数值形式，并且有许多方法可以编码类别列。让我们回顾几种常见的选项。
- en: Ordinal encoding and one-hot encoding
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有序编码和独热编码
- en: The most popular ways of encoding categorical features are ordinal encoding
    and one-hot encoding, but they are not always the best choices. Let’s quickly
    review what these techniques are and when they are suitable.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 编码类别特征的最常见方法是有序编码和独热编码，但它们并不总是最好的选择。让我们快速回顾一下这些技术是什么，以及何时适用它们。
- en: Ordinal encoding is the simplest of them all. We simply assign a numerical code
    to the unique values of a category and then replace the categorical value with
    the numerical code. To encode the `Acorn_grouped` feature from our dataset, all
    we need to do is assign codes, say `1` for `Affluent`, `2` for `Comfortable`,
    and `3` for `Adversity`, and replace all instances of the categorical values with
    the code we assigned. While this is really easy, this kind of encoding introduces
    meanings to the categorical values that we may or may not intend. When we assign
    numerical codes, we are implicitly saying that the categorical value that gets
    assigned `2` as a code is better than the categorical value with `1` as the code.
    This kind of encoding only works for ordinal features (features whose categorical
    values have an intrinsic sense of rank in their meaning) and should be sparingly
    used. Another way we can think about the problem is in terms of distance. When
    we do ordinal encoding, the distance between `Comfortable` and `Affluent` can
    be higher than the distance between `Comfortable` and `Adversity`, depending on
    the way we encode.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有序编码是其中最简单的一种。我们只需为类别的唯一值分配一个数值代码，然后用数值代码替换类别值。为了编码我们数据集中的`Acorn_grouped`特征，我们所需要做的就是分配代码，比如为`Affluent`分配`1`，为`Comfortable`分配`2`，为`Adversity`分配`3`，然后将所有类别值替换为我们分配的代码。虽然这非常简单，但这种编码方法会引入我们可能并不打算赋予类别值的含义。当我们分配数值代码时，我们隐含地表示，类别值为`2`的特征比类别值为`1`的特征更好。这种编码方式只适用于有序特征（即类别值在意义上有固有排序的特征），并且应该谨慎使用。我们还可以从距离的角度考虑这个问题。当我们进行有序编码时，`Comfortable`与`Affluent`之间的距离可能比`Comfortable`与`Adversity`之间的距离更大，这取决于我们如何编码。
- en: 'One-hot encoding is a better way of representing categorical features with
    no ordinal meaning. It essentially encodes the categorical features in a higher
    dimension, placing the categorical values equally distant in that space. The size
    of the dimension it requires to encode the categorical values is equal to the
    cardinality of the categorical variable. Cardinality is the number of unique values
    in the categorical feature. Let’s see how sample data would be encoded in a one-hot
    encoding scheme:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码是一种更好的表示没有顺序意义的类别特征的方法。它本质上将类别特征编码到一个更高维度的空间，将类别值在该空间中等距离地分布。编码类别值所需的维度大小等于类别变量的基数。基数是类别特征中唯一值的数量。让我们看看如何在一热编码方案中编码示例数据：
- en: '![Figure 10.2 – One-hot encoding of categorical features ](img/B22389_10_02.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – 类别特征的一热编码](img/B22389_10_02.png)'
- en: 'Figure 10.2: One-hot encoding of categorical features'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：类别特征的一热编码
- en: We can see that the resulting encoding will have a column for each unique value
    in the categorical feature and the value is indicated by `1` in the column. For
    instance, the first row is `Comfortable`, and therefore, every other column except
    the `Comfortable` column will have `0` and the `Comfortable` column will have
    `1`. If we calculate the Euclidean distance between any two categorical values,
    we can see that they are the same.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，结果编码将为类别特征中的每个唯一值设置一列，且该列中的值用 `1` 表示。例如，第一行是 `舒适`，因此，除 `舒适` 列之外的每一列都会是
    `0`，而 `舒适` 列会是 `1`。如果我们计算任何两个类别值之间的欧几里得距离，我们可以看到它们是相同的。
- en: 'However, there are three main issues with this encoding, all of which become
    a problem with high cardinality categorical variables:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种编码存在三个主要问题，所有这些问题在高基数类别变量中都会变得更加严重：
- en: The embedding is inherently sparse and many machine learning models (for instance,
    tree-based models and neural networks) do not really work well with sparse data
    (sparse data is when a majority of values in the data are zeros). When the cardinality
    is just 5 or 10, the sparsity introduced may not be that much of a problem, but
    when we consider a cardinality of 100 or 500, the encoding becomes really sparse.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入本质上是稀疏的，许多机器学习模型（例如基于树的模型和神经网络）在处理稀疏数据时表现不佳（稀疏数据是指数据中大多数值为零）。当基数只有 5 或 10
    时，引入的稀疏性可能不是太大问题，但当我们考虑基数为 100 或 500 时，编码变得非常稀疏。
- en: Another issue is the explosion of dimensions of the problem. When we increase
    the total number of features of a problem due to the large number of new features
    that are created through one-hot encoding, we make the problem harder to solve.
    This can be explained by the curse of dimensionality. The *Further reading* section
    has a link with more information on the curse of dimensionality.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个问题是问题维度的爆炸。当我们由于一热编码生成大量新特征而增加问题的总特征数量时，问题变得更难以解决。这可以通过维度灾难来解释。*进一步阅读*部分有一个链接，提供了更多关于维度灾难的信息。
- en: The last problem is related to practical concerns. For a large dataset, if we
    one-hot encode a categorical value with hundreds or thousands of unique values,
    the resulting dataframe is not going to be easy to work with because it will not
    fit in the computer memory.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个问题与实际操作有关。对于一个大型数据集，如果我们对具有数百或数千个唯一值的类别值进行一热编码，生成的数据框将难以使用，因为它将无法适应计算机内存。
- en: There is a slightly different way of one-hot encoding where we drop one of these
    dimensions, called **dummy variable encoding**. This has the added benefit of
    making the encoding linearly independent, which, in turn, has some advantages,
    especially for vanilla linear regression. The *Further reading* section has a
    link if you want to know more.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种稍微不同的一热编码方法，其中我们会丢弃其中一个维度，这称为**虚拟变量编码**。这样做的额外好处是使得编码线性独立，这反过来带来一些优势，特别是对于普通线性回归。如果你想了解更多内容，*进一步阅读*部分有一个链接。
- en: Since the categorical columns that we must encode have high cardinality (at
    least a few of them), we will not be doing this encoding. Instead, let’s look
    at a few encoding techniques that can handle high cardinality categorical variables
    better.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们必须编码的类别列具有较高的基数（至少其中有几个），我们将不会执行这种编码。相反，让我们看看一些可以更好处理高基数类别变量的编码技术。
- en: Frequency encoding
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 频率编码
- en: Frequency encoding is an encoding schema that does not increase the dimensions
    of the problem. It takes a single categorical array and returns a single numeric
    array. The logic is very simple—it replaces the categorical values with the number
    of times the value occurs in the training dataset. Although it’s not perfect,
    this works pretty well, as it lets the model distinguish between different categories
    based on how frequently they occur.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 频率编码是一种不增加问题维度的编码方案。它接受一个单一的分类数组，并返回一个单一的数字数组。逻辑非常简单——它用该值在训练数据集中出现的次数来替换分类值。虽然它并不完美，但效果相当好，因为它让模型能够基于类别出现的频率来区分不同的类别。
- en: There is a popular library, `category_encoders`, that implements a lot of different
    encoding schemes in a standard scikit-learn style estimator, and we will be using
    that in our experiments as well. The standard framework we developed in *Chapter
    8*, *Forecasting Time Series with Machine Learning Models*, also had a couple
    of functionalities that we didn’t use—`encode_categorical` and `categorical_encoder`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个流行的库，`category_encoders`，它实现了许多不同的编码方案，采用标准的scikit-learn样式估算器，我们在实验中也会使用它。我们在*第8章：使用机器学习模型预测时间序列*中开发的标准框架，也有一些我们没有使用的功能——`encode_categorical`和`categorical_encoder`。
- en: 'So, let’s use them and train our model now:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们现在使用它们并训练我们的模型：
- en: '[PRE2]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The rest of the process is the same as what we saw in the *Creating Global
    Forecasting Models (GFMs)* section and we get the forecast using the encoded meta-features:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的过程与我们在*创建全局预测模型（GFM）*部分看到的相同，我们通过编码后的元特征来获取预测结果：
- en: '![Figure 10.3 – Aggregate metrics with the GFM with meta-features (frequency
    encoding) ](img/B22389_10_03.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – 使用带元特征的GFM的聚合指标（频率编码）](img/B22389_10_03.png)'
- en: 'Figure 10.3: Aggregate metrics with the GFM with meta-features (frequency encoding)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：使用带元特征的GFM的聚合指标（频率编码）
- en: Right away, we can see that there is a reduction in error, although it is minimal.
    We can also see that the training time has almost doubled. This may be because
    now we have an additional step of encoding the categorical features in addition
    to training the machine learning model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立刻可以看到，虽然误差减少的幅度很小，但还是有减少。我们也可以看到训练时间几乎翻倍了。这可能是因为现在我们除了训练机器学习模型之外，还增加了对分类特征的编码步骤。
- en: The main issue with frequency encoding is that it doesn’t work with features
    that are uniformly distributed in the dataset. For instance, the `LCLid` feature,
    which is just a unique code for each household, is uniformly distributed in the
    dataset and when we use frequency encoding, all the `LCLid` features will come
    to almost the same frequency, and hence the machine learning model considers them
    almost the same.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 频率编码的主要问题是它不适用于数据集中均匀分布的特征。例如，`LCLid`特征，它是每个家庭的唯一代码，在数据集中是均匀分布的，当我们使用频率编码时，所有的`LCLid`特征几乎会达到相同的频率，因此机器学习模型几乎会认为它们是相同的。
- en: Let’s now look at a slightly different approach.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个稍微不同的方法。
- en: Target mean encoding
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标均值编码
- en: 'Target mean encoding, in its most vanilla form, is a very simple concept. It
    is a *supervised* approach that uses the target in the training dataset to encode
    the categorical columns. Let’s look at an example:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 目标均值编码，在其最基本的形式下，是一个非常简单的概念。它是一种*监督式*方法，利用训练数据集中的目标来编码分类列。让我们来看一个例子：
- en: '![Figure 10.4 – Target mean encoding ](img/B22389_10_04.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – 目标均值编码](img/B22389_10_04.png)'
- en: 'Figure 10.4: Target mean encoding'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：目标均值编码
- en: The vanilla target mean encoding has a few limitations. It increases the chance
    of overfitting the training data because we are using the mean targets directly
    and thereby leaking the target into the model in a way. Another problem with the
    approach is that when the categorical values are unevenly distributed, there may
    be a few categorical values with very small sample sizes, and therefore, the mean
    estimate becomes noisy. Extending this problem to the extreme, we get another
    case where an unseen categorical value comes up in test data. This is also not
    supported in the vanilla version. Therefore, in practice, this simple version
    is almost never used, but slightly more sophisticated versions of this concept
    are widely used and are an effective strategy for encoding categorical features.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 基础的目标均值编码有一些限制。它增加了过拟合训练数据的可能性，因为我们直接使用了均值目标，从而以某种方式将目标信息泄漏到模型中。该方法的另一个问题是，当类别值分布不均时，可能会有一些类别值的样本量非常小，因此均值估计会变得很嘈杂。将这个问题推向极端，我们会遇到测试数据中出现未知的类别值，这在基础版本中也是不支持的。因此，在实践中，这种简单版本几乎不会被使用，而稍微复杂一点的变种广泛应用，并且是编码类别特征的有效策略。
- en: In `category_encoders`, there are many variations of this concept, but let’s
    look at two popular and effective ones here.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在`category_encoders`中，存在许多这种概念的变种，但这里我们来看看其中两种流行且有效的版本。
- en: In 2001, Daniele Micci-Barreca proposed a variant of mean encoding. If we consider
    the target as a binary variable, say 1 and 0, the mean (which is the number of
    1s or number of samples) is also the probability of having 1\. Using this interpretation
    of the means, Daniele proposed blending two probabilities—prior and posterior
    probabilities—as the final encoding for the categorical features.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在2001年，Daniele Micci-Barreca提出了一种均值编码的变体。如果我们将目标视为一个二进制变量，例如1和0，则均值（即1的数量或样本的数量）也可以看作是1的概率。基于这个均值的解释，Daniele提出将先验概率和后验概率融合，作为类别特征的最终编码。
- en: '**Reference check:**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查：**'
- en: The research paper by Daniele Micci-Barreca is cited in *References* as reference
    *2*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Daniele Micci-Barreca的研究论文在*参考文献*中被引用为参考文献*2*。
- en: 'The prior probability is defined as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 先验概率定义如下：
- en: '![](img/B22389_10_012.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_10_012.png)'
- en: Here, *n*[y] is the number of cases such that *target* = 1, and *n*[TR] is the
    number of samples in the training data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*n*[y]是*target* = 1的案例数，而*n*[TR]是训练数据中样本的数量。
- en: 'The posterior probability is defined for category *i* as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 后验概率对于类别*i*的定义如下：
- en: '![](img/B22389_10_013.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_10_013.png)'
- en: Here, *n*[iY] is the number of samples in the dataset where *category* = *i*
    and `Y = 1`, and *n*[i] is the number of samples in the dataset where *category*
    = *i*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*n*[iY]是数据集中*category* = *i*且`Y = 1`的样本数量，而*n*[i]是数据集中*category* = *i*的样本数量。
- en: 'Now, the final encoding for category *i* is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，类别*i*的最终编码如下：
- en: '![](img/B22389_10_014.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_10_014.png)'
- en: Here, ![](img/B22389_05_008.png) is the weighting factor, which is a monotonically
    increasing function on *n*[i] that is bounded between 0 and 1\. So, this function
    gives a larger weight to the posterior probability as the number of samples increases.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B22389_05_008.png)是加权因子，它是一个关于*n*[i]的单调递增函数，且其值被限制在0和1之间。所以，当样本数量增加时，这个函数会给后验概率更大的权重。
- en: 'Adapting this to the regression setting, the probabilities change to expected
    values so that the formula becomes the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 将其调整到回归设置中，概率变为期望值，因此公式变为以下形式：
- en: '![](img/B22389_10_016.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_10_016.png)'
- en: Here, *TR*[i] is all the rows where *category* = 1 and ![](img/B22389_10_017.png)
    is the sum of *Y* for *TR*[i].![](img/B22389_10_018.png) is the sum of *Y* for
    all the rows in the training dataset. As with the binary variable, we are mixing
    the expected value of *Y*, given *category* = *i* (*E*[*Y*|*category* = *i*])
    and the expected value of *Y* (*E*[*Y*]) for the final categorical encoding.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*TR*[i]是所有*category* = 1的行，而![](img/B22389_10_017.png)是*TR*[i]中*Y*的总和。![](img/B22389_10_018.png)是训练数据集中所有行的*Y*的总和。与二进制变量类似，我们混合了*category*
    = *i*时的*Y*的期望值（*E*[*Y*|*category* = *i*]）和*Y*的期望值（*E*[*Y*]），得到最终的类别编码。
- en: 'There are many functions that we can use for ![](img/B22389_05_008.png). Daniele
    mentions a very common functional form (sigmoid):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多函数来处理![](img/B22389_05_008.png)。Daniele提到了一种非常常见的函数形式（sigmoid）：
- en: '![](img/B22389_10_020.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_10_020.png)'
- en: 'Here, *n*[i] is the number of samples in the dataset where, *category* = *i*
    and *k* and *f* are tunable hyperparameters. *k* determines half of the minimal
    sample size for which we completely trust the estimate. If `k = 1`, what we are
    saying is that we trust the posterior estimate from a category that has only two
    samples. *f* determines how fast the sigmoid transitions between the two extremes.
    As *f* tends to infinity, the transition becomes a hard threshold between prior
    and posterior probabilities. `TargetEncoder` from `category_encoders` has implemented
    this ![](img/B22389_05_008.png). The *k* parameter is called `min_samples_leaf`
    with a default value of 1, and the *f* parameter is called `smoothing` with a
    default value of 1\. Let’s see how this encoding works on our problem. Using a
    different encoder in the framework we are working on is as simple as passing a
    different `cat_encoder` (the initialized categorical encoder) to `ModelConfig`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*n*[i]是数据集中样本数量，其中*category* = *i*，*k*和*f*是可调的超参数。*k*决定了我们完全信任估计值的最小样本量的一半。如果`k
    = 1`，我们所说的是我们信任来自只有两个样本的类别的后验估计值。*f*决定了sigmoid在两个极值之间的过渡速度。当*f*趋近于无穷大时，过渡变成了先验概率和后验概率之间的硬阈值。`category_encoders`中的`TargetEncoder`实现了这一点！[](img/B22389_05_008.png)。*k*参数被称为`min_samples_leaf`，默认值为1，而*f*参数被称为`smoothing`，默认值为1。让我们看看这种编码在我们的任务中如何工作。在我们正在使用的框架中，使用不同的编码器只需将不同的`cat_encoder`（已初始化的分类编码器）传递给`ModelConfig`：
- en: '[PRE3]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The rest of the code is exactly the same. We can find the full code in the
    corresponding notebook. Let’s see how well the new encoding has done:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 其余代码完全相同。我们可以在相应的笔记本中找到完整的代码。让我们看看新编码的效果如何：
- en: '![Figure 10.5 – Aggregate metrics with the GFM with meta-features (target encoding)
    ](img/B22389_10_05.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图10.5 – 使用元特征的GFM聚合指标（目标编码）](img/B22389_10_05.png)'
- en: 'Figure 10.5: Aggregate metrics with the GFM with meta-features (target encoding)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：使用元特征的GFM聚合指标（目标编码）
- en: It’s not doing that well, is it? As with machine learning models, the **No Free
    Lunch Theorem** (**NFLT**) applies to categorical encoding as well. There is no
    one encoding scheme that works well all the time. Although not directly related
    to the topic, if you want to know more about the NFLT, head to the *Further reading*
    section.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 结果并不是很好，对吧？与机器学习模型一样，**没有免费午餐定理**（**NFLT**）同样适用于分类编码。没有一种编码方案可以始终表现良好。虽然这与主题直接无关，但如果你想了解更多关于NFLT的信息，可以前往*进一步阅读*部分。
- en: With all these *supervised* categorical encoding techniques, such as target
    mean encoding, we have to be really careful not to induce data leakage. The encoder
    should be fit using training data and not using the validation or test data. Another
    very popular technique is to generate categorical encoding using cross-validation
    and use the out-of-sample encodings to absolutely avoid data leakage or overfitting.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有这些*有监督*的分类编码技术，如目标均值编码，我们必须非常小心，以避免数据泄露。编码器应使用训练数据来拟合，而不是使用验证或测试数据。另一种非常流行的技术是使用交叉验证生成分类编码，并使用样本外编码来完全避免数据泄露或过拟合。
- en: There are many more encoding schemes, such as `MEstimateEncoder` (which uses
    additive smoothing as the ![](img/B22389_05_008.png)), `HashingEncoder`, and so
    on, in `category_encoders`. Another very effective way of encoding categorical
    features is using embedding from deep learning. The *Further reading* section
    has a link to a tutorial for doing this.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他编码方案，例如`MEstimateEncoder`（它使用加法平滑，如 ![](img/B22389_05_008.png)），`HashingEncoder`等等，均在`category_encoders`中实现。另一种非常有效的编码分类特征的方法是使用深度学习的嵌入。*进一步阅读*部分提供了一个关于如何进行这种编码的教程链接。
- en: Before now, all this categorical encoding was a separate step before the modeling.
    Now, let’s look at a technique that considers categorical features natively for
    model training.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，所有这些分类编码是建模之前的一个单独步骤。现在，让我们来看一种将分类特征作为模型训练的原生处理技术。
- en: LightGBM’s native handling of categorical features
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LightGBM对分类特征的本地处理
- en: A few machine learning model implementations handle categorical features natively,
    especially gradient-boosting models. CatBoost and LightGBM, two of the most popular
    GBM implementations, handle categorical features out of the box. CatBoost has
    a unique way of encoding categorical features into numerical ones internally using
    something similar to additive smoothing. The *Further reading* section has links
    to further information on how this encoding is done. `category_encoders` has implemented
    this logic as `CatBoostEncoder` so that we can use this type of encoding for any
    machine learning model as well.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 几种机器学习模型的实现可以原生地处理分类特征，特别是梯度提升模型。CatBoost 和 LightGBM 是最流行的 GBM 实现之一，可以直接处理分类特征。CatBoost
    有一种独特的方式将分类特征内部转换为数值特征，类似于加法平滑。*进一步阅读*部分有关于如何进行这种编码的详细信息。`category_encoders` 已经实现了这种逻辑，称为
    `CatBoostEncoder`，这样我们也可以为任何机器学习模型使用这种编码方式。
- en: While CatBoost handles this internal conversion into numerical features, LightGBM
    takes a more native approach to dealing with categorical features. LightGBM considers
    the categorical features as is while growing and splitting the trees. For a categorical
    feature with *k* unique values (cardinality of *k*), there are 2^k^(-1)-1 possible
    partitions. This soon becomes intractable, but for regression trees, Walter D.
    Fisher proposed a technique back in 1958 that makes the complexity of finding
    an optimal split much less. The essence of the method is to use average target
    statistics for each categorical value to order them and then find the optimal
    split in the ordered categorical values.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CatBoost 处理了这种内部转换为数值特征的问题，LightGBM 更本地化地处理分类特征。LightGBM 在生长和分割树时将分类特征视为原样。对于具有
    *k* 个唯一值（*k* 的基数）的分类特征，有 2^k^(-1)-1 种可能的分区。这很快变得难以处理，但是对于回归树，Walter D. Fisher
    在 1958 年提出了一种技术，使得找到最优分割的复杂性大大降低。该方法的核心是使用每个分类值的平均目标统计数据进行排序，然后在排序后的分类值中找到最优的分割点。
- en: '**Reference check:**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查：**'
- en: The research paper by Fisher is cited in *References* as reference *3*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Fisher 的研究论文被引用在*参考文献*中，作为第*3*条参考文献。
- en: 'LightGBM’s `scikit-learn` API supports this feature by taking an argument,
    `categorical_feature`, which has a list of categorical feature names, during `fit`.
    We can use the `fit_kwargs` argument in the fit of our `MLModel` that we defined
    in *Chapter 8*, *Forecasting Time Series with Machine Learning Models*, to pass
    in this parameter. Let’s see how we can do this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 的 `scikit-learn` API 支持这一功能，可以在 `fit` 过程中传入一个参数 `categorical_feature`，其中包含分类特征的名称列表。我们可以在我们在
    *第 8 章* 中定义的 `MLModel` 的 `fit` 中使用 `fit_kwargs` 参数来传递这个参数。让我们看看如何做到这一点：
- en: '[PRE4]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`y_pred` has the forecasts, which we evaluate as usual. Let’s also see the
    results:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`y_pred` 包含了预测结果，我们按照惯例进行评估。让我们也看看结果：'
- en: '![Figure 10.6 – Aggregate metrics with the GFM with meta-features (native LightGBM)
    ](img/B22389_10_06.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 使用 GFM 和元特征的聚合指标（原生 LightGBM）](img/B22389_10_06.png)'
- en: 'Figure 10.6: Aggregate metrics with the GFM with meta-features (native LightGBM)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.6: 使用 GFM 和元特征的聚合指标（原生 LightGBM）'
- en: We can observe a good reduction in `MAE` as well as `meanMASE` with the native
    handling of categorical features. We can also see a reduction in the total training
    time because we don’t have a separate step for encoding the categorical feature.
    Empirically, the native handling of categorical features works better most of
    the time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到在原生处理分类特征时，`MAE` 和 `meanMASE` 有显著的降低。我们还可以看到总体训练时间的缩短，因为不需要单独的步骤来编码分类特征。经验上，原生处理分类特征大多数情况下效果更好。
- en: Now that we have encoded the categorical features, let’s look at another way
    to improve accuracy.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对分类特征进行了编码，让我们看看另一种提高准确率的方法。
- en: Tuning hyperparameters
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整超参数
- en: A hyperparameter is a setting that controls how a machine learning model is
    trained but is not learned from the data. In contrast, model parameters are learned
    from the data during training. For example, in **Gradient Boosting Decision Trees**
    (**GBDT**), model parameters are the *decision thresholds in each tree*, learned
    from the data. Hyperparameters, like the *number of trees*, *learning rate*, and
    *tree depth*, are set before training and control the model’s structure and how
    it learns. While parameters adjust based on the data, hyperparameters must be
    tuned externally.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是控制机器学习模型如何训练的设置，但不是从数据中学习的。相比之下，模型参数是在训练过程中从数据中学习的。例如，在**梯度提升决策树**（**GBDT**）中，模型参数是*每棵树中的决策阈值*，从数据中学习得出。超参数，如*树的数量*、*学习率*和*树的深度*，在训练前设定，并控制模型的结构及其学习方式。虽然参数是基于数据调整的，但超参数必须外部调优。
- en: Although hyperparameter tuning is common practice in machine learning, we haven’t
    been able to do so because of the sheer number of models we had under the LFM
    paradigm. Now that we have a GFM that finishes training in 30 seconds, hyperparameter
    tuning becomes feasible. From a theoretical perspective, we also saw that GFMs
    can afford a larger complexity and can therefore evaluate a greater number of
    functions to pick the best without overfitting.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管超参数调优在机器学习中是常见的做法，但由于在LFM范式下我们有大量模型，过去我们无法进行调优。现在，我们有一个可以在30秒内完成训练的GFM，超参数调优变得可行。从理论角度来看，我们还看到，GFM可以承受更大的复杂度，因此能够评估更多的函数，选择最佳的而不会发生过拟合。
- en: Mathematical optimization is defined as the selection of a best element, with
    regard to some criterion, from some set of available alternatives. In most cases,
    this involves finding the maximum or minimum value of some function (an **objective
    function**) from a set of alternatives (the **search space**) subject to some
    conditions (**constraints**). The search space can be discrete variables, continuous
    variables, or a mixture of both, and the objective function can be differentiable
    or non-differentiable. There is a large body of research that tackles these variations.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 数学优化被定义为根据某些标准从一组可用的备选方案中选择最佳元素。在大多数情况下，这涉及从一组备选方案（**搜索空间**）中找到某个函数（**目标函数**）的最大值或最小值，并满足一些条件（**约束**）。搜索空间可以是离散变量、连续变量，或两者的混合，目标函数可以是可微的或不可微的。针对这些变种，已有大量研究。
- en: You may be wondering why we are talking about mathematical optimization now,
    right? Hyperparameter tuning is a mathematical optimization problem. The objective
    function here is non-differentiable and returns the metric for which we are optimizing—for
    instance, the **Mean Absolute Error** (**MAE**).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们现在要讨论数学优化？超参数调优是一个数学优化问题。这里的目标函数是不可微的，并返回我们优化的度量——例如，**平均绝对误差**（**MAE**）。
- en: The search space comprises the different hyperparameters we are tuning—say,
    the number of trees or depth of the trees. It could be a mixture of continuous
    and discrete variables and the constraints would be any restriction on the search
    space we impose—for instance, a particular hyperparameter cannot be negative,
    or a particular combination of hyperparameters cannot occur. Therefore, being
    aware of the terms used in mathematical optimization will help us in our discussion.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间包括我们要调优的不同超参数——比如，树的数量或树的深度。它可能是连续变量和离散变量的混合，约束条件是我们对搜索空间施加的任何限制——例如，某个特定的超参数不能为负，或者某些超参数的特定组合不能出现。因此，了解数学优化中使用的术语将有助于我们的讨论。
- en: Even though hyperparameter tuning is a standard machine learning concept, we
    will quickly review three main techniques (besides manual trial and error) for
    doing hyperparameter tuning.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管超参数调优是一个标准的机器学习概念，我们将简要回顾三种主要的技术（除了手动的反复试错法）来进行超参数调优。
- en: Grid search
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格搜索
- en: Grid search can be thought of as a brute-force method where we define a discrete
    grid over the search space, check the objective function at each point in the
    grid, and pick the best point in that grid. The grid is defined as a set of discrete
    points for each of the hyperparameters we choose to tune. Once the grid is defined,
    all the intersections of the grid are evaluated to search for the best objective
    value. If we are tuning 5 hyperparameters and the grid has 20 discrete values
    for each parameter, the total number of trials for a grid search would be 3,200,000
    (20⁵). This means training a model 3.2 million times and evaluating it. This becomes
    quite limiting because most modern machine learning models have many hyperparameters.
    For instance, LightGBM has more than 100, and out of those, at least 20 are highly
    impactful parameters when tuned. So, using a brute force approach such as grid
    search forces us to make the search space quite small so that it becomes feasible
    to carry out the tuning in a reasonable amount of time.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索可以被看作是一种暴力方法，在这种方法中，我们定义一个离散的网格覆盖搜索空间，在网格中的每个点上检查目标函数，并选择网格中最优的点。网格是为我们选择调优的每个超参数定义的一组离散点。一旦网格定义完成，所有网格交点都会被评估，以寻找最优的目标值。如果我们要调优5个超参数，并且每个参数的网格有20个离散值，那么网格搜索的总试验次数将是3,200,000次（20⁵）。这意味着要训练模型3.2百万次并对其进行评估。这会成为一个相当大的限制，因为大多数现代机器学习模型有许多超参数。例如，LightGBM有超过100个超参数，其中至少20个超参数在调优时具有较大的影响力。因此，使用像网格搜索这样的暴力方法迫使我们将搜索空间限制得很小，以便在合理的时间内完成调优。
- en: 'For our case, we have defined a very small grid of just 27 trials by limiting
    ourselves to a really small search space. Let’s see how we do that:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的案例，我们通过将搜索空间限制得非常小，定义了一个只有27次试验的小网格。让我们看看我们是如何做到的：
- en: '[PRE5]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We just tune three hyperparameters (`num_leaves`, `objective`, and `colsample_bytree`),
    and with just three options for each parameter. Performing the grid search after
    this is just about looping over the parameter space and evaluating the model at
    each combination of hyperparameters:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只调优三个超参数（`num_leaves`、`objective` 和 `colsample_bytree`），每个参数只有三个选项。在这种情况下，执行网格搜索就相当于遍历参数空间，并在每个超参数组合下评估模型：
- en: '[PRE6]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This takes about 15 minutes to complete and gives us the best MAE of `0.73454`,
    which is already a great improvement from our untuned GFM.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程大约需要15分钟完成，并且给我们带来了最好的MAE值 `0.73454`，这相比未调优的GFM已经是一个很大的改进。
- en: However, this makes us wonder whether there is an even better solution that
    we haven’t covered in the grid we defined. One option is to expand the grid and
    run the grid search again. This increases the number of trials exponentially and
    soon becomes infeasible.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这使我们想知道是否有一个更好的解决方案，是我们在定义的网格中没有涵盖的。一个选择是扩展网格并再次运行网格搜索。这会指数级增加试验次数，并很快变得不可行。
- en: Let’s look at a different method where we can explore a larger search space
    with the same number of trials.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一种方法，我们可以在相同数量的试验下探索更大的搜索空间。
- en: Random search
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机搜索
- en: Random search takes a slightly different route. In random search, we also define
    the search space, but instead of discretely defining specific points in the space,
    we define probability distributions over the range we want to explore. These probability
    distributions can be anything from a uniform distribution (which says any point
    in the range is equally likely) to a Gaussian distribution (which has the familiar
    peak in the middle), or any other esoteric distributions, such as gamma or beta
    distributions. As long as we can sample from the distribution, we can use it for
    random search. Once we define the search space, we can sample points from the
    distribution and evaluate each of the points to find the best hyperparameter.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索采取了稍微不同的路线。在随机搜索中，我们同样定义搜索空间，但不是离散地定义空间中的具体点，而是定义我们希望探索的范围上的概率分布。这些概率分布可以是均匀分布（表示范围内的每个点出现的概率相同），也可以是高斯分布（在中间有一个熟悉的峰值），或者任何其他特殊的分布，如伽马分布或贝塔分布。只要我们能够从分布中抽样，就可以使用它进行随机搜索。一旦我们定义了搜索空间，就可以从分布中抽取点并评估每个点，找到最佳超参数。
- en: While the number of trials is a function of the defined search space for grid
    search, it is a user input for random search, so we get to decide how much time
    or computational budget we need to use for hyperparameter tuning and, because
    of that, we can also search over a larger search space.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网格搜索，试验次数是定义搜索空间的函数，而对于随机搜索，试验次数是用户输入的参数，因此我们可以决定用于超参数调优的时间或计算预算，因此我们也可以在更大的搜索空间中进行搜索。
- en: 'With this new flexibility, let’s define a larger search space for our problem
    and use random search:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这种新的灵活性，让我们为我们的问题定义一个更大的搜索空间，并使用随机搜索：
- en: '[PRE7]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This also runs for about 15 minutes, but we have explored a larger search space.
    However, the best MAE reported was just `0.73752`, which is lower than with grid
    search. Maybe if we run the search for a greater number of iterations, we will
    get a better score, but that is just a shot in the dark. Ironically, that is pretty
    much what random search also does. It closes its eyes and throws a dart at random
    places on the dartboard and hopes it hits the bull’s eye.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程大约运行了15分钟，但我们探索了更大的搜索空间。然而，报告的最佳MAE值仅为`0.73752`，低于网格搜索的结果。也许如果我们运行更多次迭代，可能会得到更好的分数，但那只是一个瞎猜。具有讽刺意味的是，这实际上也是随机搜索所做的。它闭上眼睛，随意在飞镖靶上投掷飞镖，希望它能击中靶心。
- en: There are two terms in mathematical optimization called exploration and exploitation.
    Exploration ensures the optimization algorithm reaches different regions of the
    search space, whereas exploitation makes sure we search more in regions that are
    giving us better results. Random search is purely explorative and is unaware of
    what is happening as it evaluates different trials.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 数学优化中有两个术语，分别是探索（exploration）和利用（exploitation）。探索确保优化算法能够到达搜索空间的不同区域，而利用则确保我们在获得更好结果的区域进行更多的搜索。随机搜索完全是探索性的，它在评估不同的试验时并不关心发生了什么。
- en: Let’s look at one last technique that tries to balance between exploration and
    exploitation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下最后一种技术，它尝试在探索与利用之间找到平衡。
- en: Bayesian optimization
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: Bayesian optimization has a lot of similarities with random search. Both define
    their search space as probability distributions, and in both techniques, the user
    decides how many trials it needs to evaluate, but where they differ is the key
    advantage of Bayesian optimization. While random search is randomly sampling from
    the search space, Bayesian optimization is doing it intelligently. Bayesian optimization
    is aware of its past trials and the objective values that came out of those trials
    so that it can adapt future trials to exploit the regions where better objective
    values were seen. At a high level, it does this by building a probability model
    of the objective function and using it to focus trials on promising areas. The
    details of the algorithm are worth knowing and we have linked to a couple of resources
    in *Further reading* to help you along the way.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化与随机搜索有许多相似之处。两者都将搜索空间定义为概率分布，而且在这两种技术中，用户决定需要评估多少次试验，但它们的关键区别是贝叶斯优化的主要优势。随机搜索是从搜索空间中随机采样，而贝叶斯优化则是智能地进行采样。贝叶斯优化知道它的过去试验以及从这些试验中得到的目标值，这样它就可以调整未来的试验，利用那些曾经得到更好目标值的区域。从高层次来看，它是通过构建目标函数的概率模型并利用它来将试验集中在有希望的区域。算法的细节值得了解，我们在*进一步阅读*部分提供了一些资源链接，帮助你深入了解。
- en: Now, let’s use a popular library, `optuna`, to implement Bayesian optimization
    for hyperparameter tuning on the GFM we have been training.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用一个流行的库`optuna`来实现贝叶斯优化，用于我们训练的GFM模型的超参数调优。
- en: 'The process is quite simple. We need to define a function that takes in a parameter
    called `trial`. Inside the function, we sample the different parameters we want
    to tune from the `trial` object, train the model, evaluate the forecast, and return
    the metric we want to optimize (the MAE). Let’s quickly do that:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程非常简单。我们需要定义一个函数，该函数接受一个名为`trial`的参数。在函数内部，我们从`trial`对象中采样我们想调优的不同参数，训练模型，评估预测结果，并返回我们希望优化的度量（MAE）。让我们快速实现一下：
- en: '[PRE8]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once we have defined the objective function, we need to initialize a sampler.
    `optuna` has many samplers, such as `GridSampler`, `RandomSampler`, and `TPESampler`.
    For all standard use cases, `TPESampler` is the one to use. `GridSampler` does
    grid search and `RandomSampler` does random search. When defining a **Tree Parzen
    Estimator** (**TPE**) sampler, there are two parameters that we should pay attention
    to:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了目标函数，我们需要初始化一个采样器。`optuna` 提供了多种采样器，如 `GridSampler`、`RandomSampler` 和 `TPESampler`。对于所有标准用例，应该使用
    `TPESampler`。`GridSampler` 进行网格搜索，`RandomSampler` 进行随机搜索。在定义 **树形帕尔岑估计器**（**TPE**）采样器时，有两个参数我们需要特别关注：
- en: '`seed`: This sets the seed for the random sampling. This makes the process
    reproducible.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed`：设置随机抽样的种子。这使得该过程具有可重复性。'
- en: '`n_startup_trials`: This is the number of trials that are purely exploratory.
    This is done to understand the search space before the exploitation kicks in.
    The default value is `10`. We can reduce or increase this depending on how large
    our sample space is and how many trials we are planning to do.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_startup_trials`：这是完全探索性的试验次数。此操作是为了在开始利用之前理解搜索空间。默认值为 `10`。根据样本空间的大小和计划进行的试验数量，我们可以减少或增加此值。'
- en: The rest of the parameters are best left untouched for the most common use cases.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的参数最好保持不变，以适应最常见的使用情况。
- en: 'Now, we create a study, which is the object that runs the trials and stores
    all the details about the trials:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建一个研究对象，它负责运行试验并存储所有关于试验的细节：
- en: '[PRE9]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we define the direction of optimization, and we pass in the sampler we
    initialized earlier. Once the study is defined, we need to call the `optimize`
    method and pass the objective function we defined, the number of trials we need
    to run, and some other parameters. A full list of parameters for the `optimize`
    method is available here—[https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了优化方向，并传入了我们之前初始化的采样器。一旦定义了研究对象，我们需要调用 `optimize` 方法，并传入我们定义的目标函数、需要运行的试验次数以及一些其他参数。`optimize`
    方法的完整参数列表可以在这里查看—[https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize)。
- en: This runs slightly longer, maybe because of the additional computation required
    to generate new trials, but still only takes about 20 minutes for the 27 trials.
    As expected, this has come up with another combination of hyperparameters for
    which the objective value is `0.72838` (the lowest before now).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这运行时间稍长，可能是因为生成新试验所需的额外计算，但仍然只需要大约 20 分钟来完成 27 次试验。正如预期的那样，这又得到了一个新的超参数组合，其目标值为
    `0.72838`（截至目前为止的最低值）。
- en: 'To fully illustrate the difference between the three, let’s compare how the
    three techniques spent their computational budget:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分说明三者之间的区别，让我们比较一下三种技术如何分配它们的计算预算：
- en: '![Figure 10.7 – Distribution of computational effort (grid versus random versus
    Bayesian optimization) ](img/B22389_10_07.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 计算工作量的分布（网格搜索 vs 随机搜索 vs 贝叶斯优化）](img/B22389_10_07.png)'
- en: 'Figure 10.7: Distribution of computational effort (grid versus random versus
    Bayesian optimization)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：计算工作量的分布（网格搜索 vs 随机搜索 vs 贝叶斯优化）
- en: We can see that the Bayesian optimization has a fat tail on the lower side,
    indicating that it spent most of its computational budget evaluating and exploiting
    the optimal regions in the search space.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，贝叶斯优化在较低值处有一个厚尾，表明它将大部分计算预算用于评估和利用搜索空间中的最优区域。
- en: Let’s look at how the different trials with these techniques fared as the optimization
    procedure progressed.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在优化过程进行的过程中，这些不同的技术如何表现。
- en: The notebook has a more detailed comparison and commentary on the three techniques.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本中有对三种技术的更详细比较和评论。
- en: The bottom line is that if we have unlimited computation, grid search with a
    well-defined and fine-grained grid is the best option, but if we value the efficiency
    of our computational effort, we should go for Bayesian optimization.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结论是，如果我们有无限的计算资源，使用定义良好的精细网格进行网格搜索是最佳选择，但如果我们重视计算效率，我们应该选择贝叶斯优化。
- en: 'Let’s see how the new parameters worked out for us:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看新参数的效果如何：
- en: '![Figure 10.8 – Aggregate metrics with the tuned GFM with meta-features ](img/B22389_10_08.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8 – 使用调整过的 GFM 和元特征的聚合指标](img/B22389_10_08.png)'
- en: 'Figure 10.8: Aggregate metrics with the tuned GFM with meta-features'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8：使用元特征调整后的GFM的聚合指标
- en: We have had huge improvements in `MAE` and `meanMASE`, mostly because we were
    optimizing for the MAE when hyperparameter tuning. The MAE and MSE have slightly
    different priorities and we will spend more time on that in *Part 4*, *Mechanics
    of Forecasting*. The runtime also increased because the new parameters build more
    leaves for a tree than before and are more complex than the default parameters.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`MAE`和`meanMASE`上取得了巨大的改善，主要是因为在超参数调整时我们优化的是MAE。MAE和MSE的侧重点略有不同，接下来在*第4部分*，*预测的机制*中我们将更多地讨论这一点。运行时间也有所增加，因为新参数为树构建了比默认参数更多的叶子，模型也比默认参数更复杂。
- en: Now, let’s look at another strategy for improving the performance of a GFM.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下另一种提高GFM性能的策略。
- en: Partitioning
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分区
- en: Out of all the strategies we have discussed so far, this is the most counter-intuitive,
    especially if you are coming from a standard machine learning or statistics background.
    Normally, we would expect the model to do well with more data, but partitioning
    or splitting the dataset into multiple, almost equal parts has been shown (empirically)
    to improve the accuracy of the model. While this has been seen empirically, why
    this happens is something that is still not quite clear. One explanation is that
    the GFMs have a slightly simpler job of learning when trained on a subset of similar
    entities and hence, can learn specific functions to subsets of similar entities.
    Another explanation for the phenomenon has been put forward by Montero-Manso and
    Hyndman (Reference *1*). They put forward that partitioning the data is another
    form of increasing the complexity because instead of having *log*(|*J*|) as the
    complexity term, we have
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止讨论的所有策略中，这个是最不直观的，特别是如果你来自标准的机器学习或统计学背景。通常，我们会期望模型在更多数据的情况下表现更好，但将数据集分区或拆分为多个几乎相等的部分，已经在经验上被证明能提高模型的准确性。虽然这一点已被经验验证，但其背后的原因仍不完全清楚。一个解释是，GFMs在训练时面对更简单的任务，当它们训练在相似实体的子集上时，因此能学习到特定实体子集的函数。Montero-Manso和Hyndman（参考文献*1*）提出了另一个解释。他们认为，数据分区是增加复杂性的另一种形式，因为我们不再将*log*(|*J*|)作为复杂性项，而是
- en: '![](img/B22389_10_023.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_10_023.png)'
- en: where *P* is the number of partitions. With this rationale, the LFMs are special
    cases where *P* is equal to the number of time series in the dataset.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*P*是分区的数量。按照这个逻辑，LFM是特例，其中*P*等于数据集中的时间序列数量。
- en: There are many ways we can partition the data, each with varying degrees of
    complexity.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用多种方式来划分数据集，每种方式的复杂度不同。
- en: Random partition
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机分区
- en: 'The simplest method is to randomly split the dataset into *P*-equal partitions
    and train separate models for each partition. This method faithfully follows the
    explanation that Montero-Manso and Hyndman provide because we are splitting the
    dataset randomly, with no concern for the similarity of the different households.
    Let’s see how we can do that:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是将数据集随机划分为*P*个相等的分区，并为每个分区训练独立的模型。这个方法忠实地遵循了Montero-Manso和Hyndman的解释，因为我们是随机划分数据集的，不考虑不同家庭之间的相似性。让我们看看如何操作：
- en: '[PRE10]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we just loop over these partitions and train separate models for each
    partition. The exact code can be found in the notebook. Let’s see how well the
    random partition does:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只需遍历这些分区，为每个分区训练独立的模型。具体代码可以在笔记本中找到。让我们看看随机分区效果如何：
- en: '![Figure 10.9 – Aggregate metrics with the tuned GFM with meta-features and
    random partitioning ](img/B22389_10_09.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.9 – 使用元特征和随机分区调整后的GFM的聚合指标](img/B22389_10_09.png)'
- en: 'Figure 10.9: Aggregate metrics with the tuned GFM with meta-features and random
    partitioning'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9：使用元特征和随机分区调整后的GFM的聚合指标
- en: We can see a decrease in `MAE` and `meanMASE` even with a random partition.
    There is even a decrease in runtime because the individual models are working
    on less data and hence, train faster.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是随机分区，我们也能看到`MAE`和`meanMASE`的下降。运行时间也有所减少，因为每个独立的模型处理的数据较少，因此训练速度更快。
- en: Now, let’s see another way of partitioning, keeping the similarity of different
    time series in mind.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看一下另一种分区方法，同时考虑不同时间序列的相似性。
- en: Judgmental partitioning
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 判断性分区
- en: Judgmental partitioning is when we use some attribute of the time series to
    split the dataset, and this is called judgmental because, usually, this depends
    on the judgment of the person who is working on the model. There are many ways
    of doing this. We can use some meta-feature, or we can use some characteristics
    of the time series (such as volume, variability, intermittency, or a combination
    of them) to partition the dataset.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 判断性分割是指我们使用时间序列的某些属性来划分数据集，这种方法被称为判断性分割，因为通常这取决于正在处理模型的人的判断。实现这一目标的方法有很多种。我们可以使用一些元特征，或者使用时间序列的某些特征（例如，量、变异性、间歇性，或它们的组合）来划分数据集。
- en: 'Let’s use a meta-feature called `Acorn_grouped` to partition the dataset. Again,
    we will just loop over the unique values in `Acorn_grouped` and train a model
    for each value. We will also not use `Acorn_grouped` as a feature. The exact code
    is in the notebook. Let’s see how well this partitioning does:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个元特征，叫做`Acorn_grouped`，来划分数据集。同样，我们将只遍历`Acorn_grouped`中的唯一值，并为每个值训练一个模型。我们也不会将`Acorn_grouped`作为一个特征。具体代码在笔记本中。让我们看看这种分割方法的效果如何：
- en: '![Figure 10.10 – Aggregate metrics with the tuned GFM with meta-features and
    Acorn_grouped partitioning ](img/B22389_10_10.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图10.10 – 使用调优后的GFM与元特征和Acorn_grouped分割的汇总指标](img/B22389_10_10.png)'
- en: 'Figure 10.10: Aggregate metrics with the tuned GFM with meta-features and Acorn_grouped
    partitioning'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10：使用调优后的GFM与元特征和Acorn_grouped分割的汇总指标
- en: This does even better than random partitioning. We can assume each of the partitions
    (`Affluent`, `Comfortable`, and `Adversity`) has some kind of similarity, which
    makes the learning easier, and hence, we get better accuracy.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法比随机分割表现得更好。我们可以假设每个分区（`Affluent`、`Comfortable` 和 `Adversity`）都有某种相似性，这使得学习变得更加容易，因此我们得到了更好的准确性。
- en: Now, let’s look at another way to partition the dataset, again, using similarity.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下另一种划分数据集的方法，同样是基于相似性。
- en: Algorithmic partitioning
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法分割
- en: In judgmental partitioning, we pick some meta-features or time series characteristics
    for partitioning the dataset. We pick a handful of dimensions to partition the
    dataset because we are doing it in our minds and our mental faculties cannot handle
    more than two or three dimensions well, but we can see this partitioning as an
    unsupervised clustering approach and this approach is called algorithmic partitioning.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在判断性分割中，我们选择一些元特征或时间序列特征来划分数据集。我们选择少数几个维度来划分数据集，因为我们是在脑海中进行操作的，而我们的思维能力通常无法处理超过两三个维度，但我们可以将这种分割视为一种无监督的聚类方法，这种方法称为算法分割。
- en: 'There are two ways we can cluster time series:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类时间序列有两种方法：
- en: Extracting features for each time series and using those features to form clusters
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个时间序列提取特征，并使用这些特征形成聚类
- en: Using time series clustering techniques using the **Dynamic Time Warping** (**DTW**)
    distance
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于**动态时间规整**（**DTW**）距离的时间序列聚类技术
- en: '`tslearn` is an open source Python library that has implemented a few time
    series clustering approaches based on the distances between time series. There
    is a link in *Further reading* for more information on the library and how it
    can be used for time series clustering.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`tslearn`是一个开源Python库，已经实现了基于时间序列之间距离的几种时间序列聚类方法。在*进一步阅读*中有一个链接，提供了有关该库以及如何使用它进行时间序列聚类的更多信息。'
- en: In our example, we are going to use the first method, where we derive a few
    time series characteristics and use them for clustering. There are many features
    from statistical and temporal literature, such as autocorrelation, mean, variance,
    entropy, and peak-to-peak distance, that we can extract from the time series.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将使用第一种方法，即我们提取一些时间序列特征并用于聚类。从统计和时间文献中，有许多特征可以提取，例如自相关、均值、方差、熵和峰值间距等，这些都可以从时间序列中提取。
- en: We can use another open source Python library called the **Time Series Feature
    Extraction Library** (`tsfel`) to make the process easier.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用另一个开源Python库，叫做**时间序列特征提取库**（`tsfel`），来简化这个过程。
- en: 'The library has many classes of features—statistical, temporal, and spectral
    domains—that we can choose from, and the rest is handled by the library. Let’s
    see how we can generate these features and create a dataframe to perform clustering:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 该库有许多类别的特征——统计、时间、和频谱域——我们可以从中选择，剩下的由库来处理。让我们看看如何生成这些特征并创建一个数据框以执行聚类：
- en: '[PRE11]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The dataframe looks something like this:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框的样子大概是这样的：
- en: '![Figure 10.11 – Features extracted from different time series ](img/B22389_10_11.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.11 – 从不同时间序列中提取的特征](img/B22389_10_11.png)'
- en: 'Figure 10.11: Features extracted from different time series'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11：从不同时间序列中提取的特征
- en: Now that we have the dataframe with each row representing a time series with
    different features, we can ideally apply any clustering method, such as k-means,
    k-medoids, or HDBSCAN, and find clusters. However, in high dimensions, a lot of
    the distance metrics (including Euclidean) do not work as well as they are supposed
    to. There is a seminal paper on the topic by Charu C. Agarwal et al. from 2001
    that explores the topic. When we increase the dimensionality of the space, our
    common sense (which conceptualizes three dimensions) does not work as well and,
    as a consequence, common distance metrics such as Euclidean distance do not work
    very well with high dimensions. We have linked to a blog summarizing the paper
    (in *Further reading*) and the paper itself (Reference *5*), which make the concept
    clearer. So, a common way of handling high-dimensional clustering is by performing
    dimensionality reduction first and then using normal clustering.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个数据框，每一行代表一个具有不同特征的时间序列，理论上我们可以应用任何聚类方法，如k-means、k-medoids或HDBSCAN来找到聚类。然而，在高维空间中，许多距离度量（包括欧几里得距离）并不像预期的那样工作。有一篇由Charu
    C. Agarwal等人于2001年发布的开创性论文，探讨了这个问题。当我们增加空间的维度时，我们的常识（它概念化了三维空间）就不那么适用了，因此常见的距离度量，如欧几里得距离，在高维度中效果并不好。我们已经链接了一篇总结该论文的博客（在*进一步阅读*中）和该论文本身（参考文献*5*），它们使这个概念更加清晰。因此，处理高维聚类的常见方法是先进行降维，然后再使用普通的聚类方法。
- en: '**Principal Component Analysis** (**PCA**) was the go-to tool in the field,
    but since PCA only captures and details linear relationships while reducing the
    dimensions, nowadays, another class of techniques is starting to become more popular—manifold
    learning.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析** (**PCA**) 是该领域常用的工具，但由于PCA在降维时仅捕捉和详细描述线性关系，现在，另一类技术开始变得更受欢迎——流形学习。'
- en: '**t-distributed Stochastic Neighbor Embeddings** (**t-SNE**) is a popular technique
    from this category, which is really popular for high-dimensional visualization.
    It is a really clever technique where we project the points from a high-dimensional
    space to a lower dimension, keeping the distribution of distance in the original
    space as close as possible to the one in lower dimensions. There is a lot to learn
    here that is beyond the scope of this book. There are links in the *Further reading*
    section that can help you get started.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**t-分布随机邻居嵌入** (**t-SNE**) 是这一类别中流行的技术，尤其适用于高维可视化。这是一种非常巧妙的技术，我们将点从高维空间投影到低维空间，同时尽可能保持原始空间中的距离分布与低维空间中的距离分布相似。这里有很多内容需要学习，超出了本书的范围。*进一步阅读*部分中有一些链接可以帮助你入门。'
- en: 'To cut a long story short, we will be using t-SNE to reduce the dimensions
    of the dataset we have and then cluster the dataset with the reduced dimensions.
    If you really want to cluster time series and use those clusters in some other
    way, I would not suggest using t-SNE because it doesn’t preserve the distance
    between points and the density of points. The distil.pub article in *Further reading*
    throws more light on the issue. But in our case, we are using the clusters just
    as a grouping for training another model, so this approximation can do well. Let’s
    see how we do that:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 长话短说，我们将使用t-SNE将数据集的维度减少，然后使用降维后的数据集进行聚类。如果你真的想对时间序列进行聚类并以其他方式使用这些聚类，我不建议使用t-SNE，因为它不能保持点之间的距离和点的密度。*进一步阅读*中的distil.pub文章更详细地阐述了这个问题。但在我们的案例中，我们仅将聚类用作训练另一个模型的分组，因此这个近似方法是可以的。让我们看看我们是如何做到的：
- en: '[PRE12]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Since we reduced the dimensions to two, we can also visualize the clusters
    formed:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将维度降至二维，因此我们还可以可视化形成的聚类：
- en: '![Figure 10.12 – Clustered time series after t-SNE dimensionality reduction
    ](img/B22389_10_12.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.12 – t-SNE降维后的聚类时间序列](img/B22389_10_12.png)'
- en: 'Figure 10.12: Clustered time series after t-SNE dimensionality reduction'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12：t-SNE降维后的聚类时间序列
- en: 'We have three well-defined clusters formed and now we are just going to use
    these clusters to train a model for each cluster. As usual, we loop over the three
    clusters and train the models. Let’s see how we did so:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经形成了三个定义明确的聚类，现在我们将使用这些聚类来训练每个聚类的模型。像往常一样，我们对三个聚类进行循环并训练模型。让我们看看我们是如何做的：
- en: '![Figure 10.13 – Aggregate metrics with the tuned GFM with meta-features and
    clustered partitioning ](img/B22389_10_13.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.13 – 使用元特征和集群分区调优的GFM汇总指标](img/B22389_10_13.png)'
- en: 'Figure 10.13: Aggregate metrics with the tuned GFM with meta-features and clustered
    partitioning'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13：使用元特征和集群分区调优的GFM汇总指标
- en: 'It looks as though this is the best MAE we have seen in all our experiments,
    but the three partition techniques have very similar MAEs. We can’t see whether
    any one is better than the other just by looking at a single hold-out set. For
    good measure, we can run these forecasts with a test dataset using the `01a-Global_Forecasting_Models-ML-test.ipynb`
    notebook in the `Chapter08` folder. Let’s see how the aggregate metrics are on
    the test dataset:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这是我们所有实验中看到的最佳MAE，但三种分区技术的MAE非常相似。仅凭一个保留集，我们无法判断哪一种优于另一种。为了进一步验证，我们可以使用`Chapter08`文件夹中的`01a-Global_Forecasting_Models-ML-test.ipynb`笔记本在测试数据集上运行这些预测。让我们看看测试数据集上的汇总指标：
- en: '![Figure 10.14 – Aggregate metrics on test data ](img/B22389_10_14.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.14 – 测试数据上的汇总指标](img/B22389_10_14.png)'
- en: 'Figure 10.14: Aggregate metrics on test data'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14：测试数据上的汇总指标
- en: As expected, the clustered partition is still the methodology that performs
    the best in this case.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，集群分区在这种情况下仍然是表现最好的方法。
- en: In *Chapter 8*, *Forecasting Time Series with Machine Learning Models*, it took
    us 8 minutes and 20 seconds to train an LFM for all the households in our dataset.
    Now, with the GFM paradigm, we finished training a model in 57 seconds (in the
    worst-case scenario). That’s 777% less training time and this comes with an 8.78%
    decrease in the MAE.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8章*，*使用机器学习模型进行时间序列预测*中，我们花了8分钟20秒训练了一个LFM来处理数据集中所有家庭的预测。现在，采用GFM范式，我们在57秒内完成了模型训练（在最坏情况下）。这比训练时间减少了777%，同时MAE也减少了8.78%。
- en: We chose to do these experiments with LightGBM. This does not mean that LightGBM
    or any other gradient-boosting model is the only choice for GFMs, but they are
    a pretty good default. A well-tuned gradient-boosted trees model is a very difficult
    baseline to beat, but as always in machine learning, we should check what works
    best using well-defined experiments.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用LightGBM进行这些实验。这并不意味着LightGBM或其他任何梯度提升模型是GFMs的唯一选择，但它们是一个相当不错的默认选择。一个经过精调的梯度提升树模型是一个非常难以超越的基准，但和机器学习中的所有情况一样，我们应该通过明确的实验来检查什么方法效果最好。
- en: Although there are no hard and fast rules or cutoffs for when a GFM makes more
    sense than an LFM, as the number of time series in a dataset increases, the GFM
    becomes more favorable, both from the perspective of accuracy and computation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有硬性规定或界限来确定何时GFM比LFM更合适，但随着数据集中时间序列的数量增加，从准确性和计算角度来看，GFM变得更为有利。
- en: Although we have achieved good results using GFMs, typically the complex models
    that do well in this paradigm are black boxes. Let’s look at some ways to open
    the black box and understand and explain the model better.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用GFM取得了良好的结果，但通常在这种范式下表现好的复杂模型是黑盒模型。让我们看看一些打开黑盒、理解和解释模型的方式。
- en: Interpretability
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性
- en: Interpretability can be defined as the degree to which a human can understand
    the cause of a decision. In machine learning and artificial intelligence, that
    translates to the degree to which someone can understand the how and why of an
    algorithm and its predictions. There are two ways to look at interpretability—transparency
    and post hoc interpretation.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性可以定义为人类能够理解决策原因的程度。在机器学习和人工智能中，这意味着一个人能够理解一个算法及其预测的“如何”和“为什么”的程度。可解释性有两种看法——透明性和事后解释。
- en: '*Transparency* is when the model is inherently simple and can be simulated
    or thought about using human cognition. A human should be able to fully understand
    the inputs and the process a model takes to convert these inputs to outputs. This
    is a very stringent condition that almost none of the model machine learning or
    deep learning models satisfy.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '*透明性*是指模型本身简单，能够通过人类认知来模拟或思考。人类应该能够完全理解模型的输入以及模型如何将这些输入转换为输出的过程。这是一个非常严格的条件，几乎没有任何机器学习或深度学习模型能够满足。'
- en: This is where *post hoc interpretation* techniques shine. There is a wide variety
    of techniques that use the inputs and outputs of a model to understand why a model
    has made the predictions it has.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是*事后解释*技术大显身手的地方。有多种技术可以利用模型的输入和输出，帮助理解模型为何做出它的预测。
- en: There are many popular techniques such as *permutation feature importance*,
    *Shapley values*, and *LIME*. All of these are general-purpose interpretation
    techniques that can be used on any machine learning model and that includes the
    GFMs we were discussing. Let’s talk about a few of them at a high level.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多流行的技术，如*排列特征重要性*、*夏普利值*和*LIME*。所有这些都是通用的解释技术，可以用于任何机器学习模型，包括我们之前讨论的GFM。让我们高层次地谈谈其中的一些。
- en: '**Mean decrease in impurity:**'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '**杂质减少的均值：**'
- en: This is the regular “feature importance” that we get out of the box from tree-based
    models. This technique measures how much a feature reduces impurity (such as Gini
    impurity in classification or variance in regression) when used to split nodes
    in a decision tree. The higher the reduction in impurity, the more important the
    feature is considered. However, it’s biased towards continuous features or those
    with high cardinality. It is fast and readily available in libraries like scikit-learn
    but may give misleading results if features have varying scales or many categories.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们从基于树的模型中直接获得的常规“特征重要性”。该技术衡量一个特征在用于决策树节点分裂时，减少杂质（例如分类中的基尼杂质或回归中的方差）多少。杂质减少得越多，特征就越重要。然而，它对连续特征或高基数特征有偏见。该方法快速且在像scikit-learn这样的库中易于使用，但如果特征具有不同的尺度或多个类别，它可能会给出误导性的结果。
- en: '**Drop column importance (Leave One Covariate Out** (**LOCO**)**):**'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**删除列重要性（Leave One Covariate Out，**LOCO**）：**'
- en: This method assesses feature importance by iteratively removing one feature
    at a time and retraining the model. The drop in performance from the baseline
    model indicates the importance of that feature. It is model-agnostic and captures
    interactions between features, but is computationally expensive since it requires
    retraining the model for each feature removed. It can also give misleading results
    if collinear features exist, as the model may compensate for the removed feature.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法通过逐个移除特征并重新训练模型来评估特征的重要性。与基线模型的性能下降表明该特征的重要性。它是模型无关的，并捕获特征之间的交互作用，但由于每次移除特征都需要重新训练模型，因此计算开销较大。如果存在共线性特征，模型可能会补偿已删除的特征，从而导致误导性结果。
- en: '**Permutation importance:**'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**排列重要性：**'
- en: Permutation importance measures the drop in model performance when the values
    of a single feature are randomly shuffled, disrupting its relationship with the
    target. This technique is intuitive and model-agnostic, and it doesn’t require
    retraining the model, making it computationally efficient. However, it can inflate
    the importance of correlated features, as models can rely on related features
    to compensate for the permuted one.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 排列重要性衡量当一个特征的值被随机打乱，从而破坏它与目标的关系时模型性能的下降。这种技术直观且与模型无关，并且不需要重新训练模型，使其在计算上效率较高。然而，它可能会夸大相关特征的重要性，因为模型可以依赖相关特征来弥补被打乱的特征。
- en: '**Partial Dependence Plots (PDPs) and Individual Conditional Expectation (ICE)
    plots:**'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**部分依赖图（PDP）和个体条件期望（ICE）图：**'
- en: PDPs visualize the average effect of a feature on the model’s predictions, showing
    how the target variable changes as the feature’s values change, while ICE plots
    show the effect of a feature for individual instances. These plots help understand
    the feature-target relationship but assume independence between features, which
    can lead to misleading interpretations in the presence of correlated variables.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: PDP（部分依赖图）可视化特征对模型预测的平均影响，展示当特征值变化时目标变量如何变化，而ICE（个体条件期望）图则展示单个实例的特征效果。这些图有助于理解特征与目标之间的关系，但假设特征之间是独立的，这在存在相关变量时可能导致误导性的解释。
- en: '**Local Interpretable Model-agnostic Explanations (LIME):**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**局部可解释模型无关解释（LIME）：**'
- en: LIME is a model-agnostic technique that explains individual predictions by approximating
    a complex model locally using simpler, interpretable models, like linear regression.
    It works by generating perturbations of the data point in question and fitting
    a local model to these samples. This method is intuitive and widely applicable
    to both structured and unstructured data (text and images), but defining the right
    locality for perturbations can be challenging, especially for tabular data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: LIME是一种模型无关的技术，它通过使用更简单、可解释的模型（如线性回归）在局部近似复杂模型来解释单个预测。它通过生成数据点的扰动并为这些样本拟合一个局部模型来工作。这种方法直观且广泛适用于结构化和非结构化数据（文本和图像），但定义扰动的正确局部性可能会很具挑战性，尤其是对表格数据而言。
- en: '**SHapley Additive exPlanations (SHAP):**'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**SHapley 加性解释（SHAP）**：'
- en: SHAP unifies several interpretation methods, including Shapley values and LIME,
    into a single framework that attributes feature importance in a model-agnostic
    way. SHAP provides both local and global interpretations and benefits from fast
    implementations for tree-based models (TreeSHAP). It combines the theoretical
    strength of Shapley values with practical efficiency, although it can still be
    computationally intensive for large datasets.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 将多种解释方法（包括 Shapley 值和 LIME）统一为一个框架，能够以模型无关的方式归因特征重要性。SHAP 提供了局部和全局解释，并通过快速实现支持树基模型（TreeSHAP）。它结合了
    Shapley 值的理论优势和实践中的高效性，尽管对于大规模数据集而言，它仍然可能计算密集。
- en: Each technique has its strengths and trade-offs, but SHAP stands out due to
    its strong theoretical foundation and ability to connect local and global interpretations
    effectively. For more extensive coverage of such techniques, I have included a
    few links in *Further reading*. The blog series by yours truly and the free book
    by Christoper Molnar are excellent resources for you to get up to speed (more
    on interpretability in *Chapter 17*).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 每种技术都有其优点和折衷，但 SHAP 因其强大的理论基础和有效连接局部与全局解释的能力而脱颖而出。关于此类技术的更广泛介绍，我在*进一步阅读*中提供了一些链接。由我本人编写的博客系列和
    Christopher Molnar 的免费书籍是非常好的资源，能够帮助你更快掌握相关知识（更多关于可解释性的内容见*第17章*）。
- en: Congratulations on finishing the second part of the book! It has been quite
    an intensive part where we went over quite a bit of theory and practical lessons,
    and we hope you are now comfortable with using machine learning for time series
    forecasting.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了本书的第二部分！这一部分内容相当密集，我们讲解了不少理论和实践课程，希望你现在已经能够熟练运用机器学习进行时间序列预测。
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: To round up the second part of the book nicely, we explored GFMs in detail and
    saw why they are important and why they are an exciting new direction in time
    series forecasting. We saw how we can use a GFM using machine learning models
    and also reviewed many techniques to make GFMs perform better, most of which are
    quite frequently used in competitions and industry use cases alike. We also took
    a high-level look at the interpretability techniques. Now that we have wrapped
    up the machine learning section of the book, we will move on to a specific type
    of machine learning that has become well-known over the past few years—**deep
    learning**—in the next chapter.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了很好地总结本书的第二部分，我们详细探讨了 GFMs，了解了它们为何重要以及为何它们在时间序列预测中是一个令人兴奋的新方向。我们看到如何利用机器学习模型来使用
    GFM，并回顾了许多技术，这些技术大多在竞赛和行业应用中频繁使用。我们还简要回顾了可解释性技术。现在我们已经完成了机器学习部分的内容，接下来将进入本书的下一章，专注于近年来广为人知的一种机器学习类型——**深度学习**。
- en: References
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The following are sources that we have referenced throughout the chapter:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在本章中引用的来源：
- en: 'Montero-Manso, P., Hyndman, R.J. (2020), *Principles and algorithms for forecasting
    groups of time series: Locality and globality*. arXiv:2008.00444[cs.LG]: [https://arxiv.org/abs/2008.00444](https://arxiv.org/abs/2008.00444).'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Montero-Manso, P., Hyndman, R.J. (2020)，*预测时间序列群体的原理与算法：局部性与全局性*。arXiv:2008.00444[cs.LG]：[https://arxiv.org/abs/2008.00444](https://arxiv.org/abs/2008.00444)。
- en: 'Micci-Barreca, D. (2001), *A preprocessing scheme for high-cardinality categorical
    attributes in classification and prediction problems*. *SIGKDD Explor. Newsl*.
    3, 1 (July 2001), 27–32: [https://doi.org/10.1145/507533.507538](https://doi.org/10.1145/507533.507538).'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Micci-Barreca, D. (2001)，*分类与预测问题中高基数分类属性的预处理方案*。*SIGKDD Explor. Newsl.* 3,
    1（2001年7月），27–32：[https://doi.org/10.1145/507533.507538](https://doi.org/10.1145/507533.507538)。
- en: 'Fisher, W. D. (1958). *On Grouping for Maximum Homogeneity*. *Journal of the
    American Statistical Association*, 53(284), 789–798: [https://doi.org/10.2307/2281952](https://doi.org/10.2307/2281952).'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Fisher, W. D. (1958). *群体最大同质性分组研究*。*美国统计学会期刊*，53(284)，789–798：[https://doi.org/10.2307/2281952](https://doi.org/10.2307/2281952)。
- en: Fisher, W.D. (1958), *A preprocessing scheme for high-cardinality categorical
    attributes in classification and prediction problems*. *SIGKDD Explor. Newsl.*
    3, 1 (July 2001), 27–32.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Fisher, W.D. (1958)，*分类与预测问题中高基数分类属性的预处理方案*。*SIGKDD Explor. Newsl.* 3, 1（2001年7月），27–32。
- en: 'Aggarwal, C. C., Hinneburg, A., and Keim, D. A. (2001). *On the Surprising
    Behavior of Distance Metrics in High Dimensional Spaces.* In *Proceedings of the
    8th International Conference on Database Theory* (ICDT ‘01). Springer-Verlag,
    Berlin, Heidelberg, 420–434: [https://dl.acm.org/doi/10.5555/645504.656414](https://dl.acm.org/doi/10.5555/645504.656414).'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Aggarwal, C. C., Hinneburg, A., 和Keim, D. A. (2001). *高维空间中距离度量的惊人行为*。在*第八届国际数据库理论会议论文集*（ICDT
    ‘01）中，Springer-Verlag, Berlin, Heidelberg, 420-434：[https://dl.acm.org/doi/10.5555/645504.656414](https://dl.acm.org/doi/10.5555/645504.656414).
- en: 'Oreshkin, B. N., Carpov D., Chapados N., and Bengio Y. (2020). *N-BEATS: Neural
    basis expansion analysis for interpretable time series forecasting*. *8th International
    Conference on Learning Representations, ICLR 2020*: [https://openreview.net/forum?id=r1ecqn4YwB](https://openreview.net/forum?id=r1ecqn4YwB).'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Oreshkin, B. N., Carpov D., Chapados N., 和Bengio Y. (2020). *N-BEATS: 具有可解释性的时间序列预测的神经基础扩展分析*，*第八届国际学习表示大会，ICLR
    2020*：[https://openreview.net/forum?id=r1ecqn4YwB](https://openreview.net/forum?id=r1ecqn4YwB).'
- en: Further reading
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'The following are a few resources that you can explore for a detailed study:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些资源，您可以进一步探索以进行详细学习：
- en: '*Learning From Data* by Yaser Abu-Mostafa: [https://work.caltech.edu/lectures.html](https://work.caltech.edu/lectures.html)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据学习* 由Yaser Abu-Mostafa编写：[https://work.caltech.edu/lectures.html](https://work.caltech.edu/lectures.html)'
- en: '*Curse of Dimensionality*—Georgia Tech: [https://www.youtube.com/watch?v=OyPcbeiwps8](https://www.youtube.com/watch?v=OyPcbeiwps8)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*维度灾难*—乔治亚理工学院：[https://www.youtube.com/watch?v=OyPcbeiwps8](https://www.youtube.com/watch?v=OyPcbeiwps8)'
- en: '*Dummy Variable Trap*: [https://www.learndatasci.com/glossary/dummy-variable-trap/](https://www.learndatasci.com/glossary/dummy-variable-trap/)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*虚拟变量陷阱*：[https://www.learndatasci.com/glossary/dummy-variable-trap/](https://www.learndatasci.com/glossary/dummy-variable-trap/)'
- en: 'Using deep learning to learn categorical embeddings: [https://pytorch-tabular.readthedocs.io/en/latest/tutorials/03-Neural%20Embedding%20in%20Scikit-Learn%20Workflows/](https://pytorch-tabular.readthedocs.io/en/latest/tutorials/03-Neural%20Embedding%20in%20Scikit-Learn%20Workflows/)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习学习类别嵌入：[https://pytorch-tabular.readthedocs.io/en/latest/tutorials/03-Neural%20Embedding%20in%20Scikit-Learn%20Workflows/](https://pytorch-tabular.readthedocs.io/en/latest/tutorials/03-Neural%20Embedding%20in%20Scikit-Learn%20Workflows/)
- en: 'Handling categorical features—CatBoost: [https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理类别特征—CatBoost：[https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic)
- en: '*Exploring Bayesian Optimization*—from Distil.pub: [https://distill.pub/2020/bayesian-optimization/](https://distill.pub/2020/bayesian-optimization/)'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*探索贝叶斯优化*—来自Distil.pub：[https://distill.pub/2020/bayesian-optimization/](https://distill.pub/2020/bayesian-optimization/)'
- en: 'Frazier, P.I. (2018). *A Tutorial on Bayesian Optimization*. arXiv:1807.02811
    [stat.ML]: [https://arxiv.org/abs/1807.02811](https://arxiv.org/abs/1807.02811)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frazier, P.I. (2018). *贝叶斯优化教程*。arXiv:1807.02811 [stat.ML]：[https://arxiv.org/abs/1807.02811](https://arxiv.org/abs/1807.02811)
- en: 'Time series clustering using `tslearn`: [https://tslearn.readthedocs.io/en/stable/user_guide/clustering.html](https://tslearn.readthedocs.io/en/stable/user_guide/clustering.html)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`tslearn`进行时间序列聚类：[https://tslearn.readthedocs.io/en/stable/user_guide/clustering.html](https://tslearn.readthedocs.io/en/stable/user_guide/clustering.html)
- en: '*The Surprising Behaviour of Distance Metrics in High Dimensions*: [https://towardsdatascience.com/the-surprising-behaviour-of-distance-metrics-in-high-dimensions-c2cb72779ea6](https://towardsdatascience.com/the-surprising-behaviour-of-distance-metrics-in-high-dimensions-c2cb72779ea6)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高维空间中距离度量的惊人行为*：[https://towardsdatascience.com/the-surprising-behaviour-of-distance-metrics-in-high-dimensions-c2cb72779ea6](https://towardsdatascience.com/the-surprising-behaviour-of-distance-metrics-in-high-dimensions-c2cb72779ea6)'
- en: '*An illustrated introduction to the t-SNE algorithm*: [https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/](https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t-SNE算法的图解介绍*：[https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/](https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/)'
- en: '*How to Use t-SNE Effectively*—from Distil.pub: [https://distill.pub/2016/misread-tsne/](https://distill.pub/2016/misread-tsne/)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何有效使用t-SNE*—来自Distil.pub：[https://distill.pub/2016/misread-tsne/](https://distill.pub/2016/misread-tsne/)'
- en: 'The NFLT: [https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFLT：[https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)
- en: '*Interpretability: Cracking open the black box* – parts I, II, and III by Manu
    Joseph: [https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-i/](https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-i/)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释性：破解黑箱* – 第一、二、三部分，作者 Manu Joseph: [https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-i/](https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-i/)'
- en: '*Interpretable Machine Learning: A Guide for Making Black Box Models Explainable*
    by Christoph Molnar: [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释机器学习：使黑箱模型可解释的指南*，作者 Christoph Molnar: [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)'
- en: '*Global models for time series forecasting*: [https://www.sciencedirect.com/science/article/abs/pii/S0031320321006178](https://www.sciencedirect.com/science/article/abs/pii/S0031320321006178)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全球时间序列预测模型*：[https://www.sciencedirect.com/science/article/abs/pii/S0031320321006178](https://www.sciencedirect.com/science/article/abs/pii/S0031320321006178)'
- en: Join our community on Discord
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
- en: Leave a Review!
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留下您的评论！
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoy it!
    Your feedback is invaluable and helps us improve and grow. Once you’ve completed
    reading it, please take a moment to leave an Amazon review; it will only take
    a minute, but it makes a big difference for readers like you.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您购买了本书——我们希望您喜欢它！您的反馈对我们非常重要，能够帮助我们不断改进和成长。阅读完本书后，请花点时间留下亚马逊评论；这只需要一分钟，但对像您这样的读者来说意义重大。
- en: Scan the QR or visit the link to receive a free ebook of your choice.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描二维码或访问链接，领取您选择的免费电子书。
- en: '[https://packt.link/NzOWQ](Chapter_10.xhtml)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/NzOWQ](Chapter_10.xhtml)'
- en: '![A qr code with black squares  Description automatically generated](img/review1.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![一个带有黑色方块的二维码，描述自动生成](img/review1.jpg)'
