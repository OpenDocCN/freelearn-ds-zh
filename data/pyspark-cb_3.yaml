- en: Abstracting Data with DataFrames
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataFrame抽象数据
- en: 'In this chapter, you will learn about the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下示例：
- en: Creating DataFrames
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建DataFrame
- en: Accessing underlying RDDs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问底层RDD
- en: Performance optimizations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能优化
- en: Inferring the schema using reflection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反射推断模式
- en: Specifying the schema programmatically
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以编程方式指定模式
- en: Creating a temporary table
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建临时表
- en: Using SQL to interact with DataFrames
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SQL与DataFrame交互
- en: Overview of DataFrame transformations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame转换概述
- en: Overview of DataFrame actions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame操作概述
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we will explore the current fundamental data structure—DataFrames.
    DataFrames take advantage of the developments in the tungsten project and the
    Catalyst Optimizer. These two improvements bring the performance of PySpark on
    par with that of either Scala or Java.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索当前的基本数据结构——DataFrame。DataFrame利用了钨项目和Catalyst Optimizer的发展。这两个改进使PySpark的性能与Scala或Java的性能相媲美。
- en: 'Project tungsten is a set of improvements to Spark Engine aimed at bringing
    its execution process closer to the *bare metal*. The main deliverables include:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Project tungsten是针对Spark引擎的一系列改进，旨在将其执行过程更接近于*裸金属*。主要成果包括：
- en: '**Code generation at runtime**: This aims at leveraging the optimizations implemented
    in modern compilers'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在运行时生成代码**：这旨在利用现代编译器中实现的优化'
- en: '**Taking advantage of the memory hierarchy**: The algorithms and data structures
    exploit memory hierarchy for fast execution'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用内存层次结构**：算法和数据结构利用内存层次结构进行快速执行'
- en: '**Direct-memory management**: Removes the overhead associated with Java garbage
    collection and JVM object creation and management'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接内存管理**：消除了与Java垃圾收集和JVM对象创建和管理相关的开销'
- en: '**Low-level programming**: Speeds up memory access by loading immediate data
    to CPU registers'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低级编程**：通过将立即数据加载到CPU寄存器中加快内存访问'
- en: '**Virtual function dispatches elimination**: This eliminates the necessity
    of multiple CPU calls'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟函数调度消除**：这消除了多个CPU调用的必要性'
- en: Check this blog from Databricks for more information: [https://www.databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://www.databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 查看Databricks的博客以获取更多信息：[https://www.databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://www.databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)。
- en: The Catalyst Optimizer sits at the core of Spark SQL and powers both the SQL
    queries executed against the data and DataFrames. The process starts with the
    query being issued to the engine. The logical plan of execution is first being
    optimized. Based on the optimized logical plan, multiple physical plans are derived
    and pushed through a cost optimizer. The selected, most cost-efficient plan is
    then translated (using code generation optimizations implemented as part of the
    tungsten project) into an optimized RDD-based execution code.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst Optimizer位于Spark SQL的核心，并驱动对数据和DataFrame执行的SQL查询。该过程始于向引擎发出查询。首先优化执行的逻辑计划。基于优化的逻辑计划，派生多个物理计划并通过成本优化器推送。然后选择最具成本效益的计划，并将其转换（使用作为钨项目的一部分实施的代码生成优化）为优化的基于RDD的执行代码。
- en: Creating DataFrames
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建DataFrame
- en: A Spark DataFrame is an immutable collection of data distributed within a cluster.
    The data inside a DataFrame is organized into named columns that can be compared
    to tables in a relational database.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame是在集群中分布的不可变数据集合。DataFrame中的数据组织成命名列，可以与关系数据库中的表进行比较。
- en: In this recipe, we will learn how to create Spark DataFrames.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将学习如何创建Spark DataFrame。
- en: Getting ready
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark 2.3 environment. If
    you do not have one, you might want to go back to [Chapter 1](part0026.html#OPEK0-dc04965c02e747b9b9a057725c821827), *Installing
    and Configuring Spark*, and follow the recipes you find there.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的Spark 2.3环境。如果没有，请返回[第1章](part0026.html#OPEK0-dc04965c02e747b9b9a057725c821827)，*安装和配置Spark*，并按照那里找到的示例进行操作。
- en: All the code that you will need for this chapter can be found in the GitHub
    repository we set up for the book: [http://bit.ly/2ArlBck](http://bit.ly/2ArlBck);
    go to `Chapter 3` and open the `3\. Abstracting data with DataFrames.ipynb` notebook.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您在本章中需要的所有代码都可以在我们为本书设置的GitHub存储库中找到：[http://bit.ly/2ArlBck](http://bit.ly/2ArlBck)；转到`第3章`并打开`3.
    使用DataFrame抽象数据.ipynb`笔记本。
- en: There are no other requirements.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 没有其他要求。
- en: How to do it...
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'There are many ways to create a DataFrame, but the simplest way is to create
    an RDD and convert it into a DataFrame:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多创建DataFrame的方法，但最简单的方法是创建一个RDD并将其转换为DataFrame：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How it works...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: If you have read the previous chapter, you probably already know how to create
    RDDs. In this example, we simply call the `sc.parallelize(...)` method.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经阅读了上一章，您可能已经知道如何创建RDD。在这个示例中，我们只需调用`sc.parallelize(...)`方法。
- en: Our sample dataset contains just a handful of records of the relatively recent
    Apple computers. However, as with all RDDs, it is hard to figure out what each
    element of the tuple stands for since RDDs are schema-less structures.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例数据集只包含了一些相对较新的苹果电脑的记录。然而，与所有RDD一样，很难弄清楚元组的每个元素代表什么，因为RDD是无模式的结构。
- en: Therefore, when using the `.createDataFrame(...)` method of `SparkSession`,
    we pass a list of column names as the second argument; the first argument is the
    RDD we wish to transform into a DataFrame.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当使用`SparkSession`的`.createDataFrame(...)`方法时，我们将列名列表作为第二个参数传递；第一个参数是我们希望转换为DataFrame的RDD。
- en: 'Now, if we peek inside the `sample_data` RDD using `sample_data.take(1)`, we
    will retrieve the first record:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们使用`sample_data.take(1)`来查看`sample_data` RDD的内容，我们将检索到第一条记录：
- en: '![](img/00034.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00034.jpeg)'
- en: 'To compare the content of a DataFrame, we can run `sample_data_df.take(1)`
    to get the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要比较DataFrame的内容，我们可以运行`sample_data_df.take(1)`来获取以下内容：
- en: '![](img/00035.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00035.jpeg)'
- en: As you can now see, a DataFrame is a collection of `Row(...)` objects. A `Row(...)`
    object consists of data that is named, unlike an RDD.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以看到，DataFrame是`Row(...)`对象的集合。`Row(...)`对象由命名的数据组成，与RDD不同。
- en: If the preceding `Row(...)` object looks similar to a dictionary to you, you
    are not wrong. Any `Row(...)` object can be converted into a dictionary using
    the `.asDict(...)` method. For more information, check out [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的`Row(...)`对象对您来说看起来类似于字典，那么您是正确的。任何`Row(...)`对象都可以使用`.asDict(...)`方法转换为字典。有关更多信息，请查看[http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row)。
- en: 'If, however, we were to have a look at the data within the `sample_data_df`
    DataFrame, using the `.show(...)` method, we would see the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们要查看`sample_data_df` DataFrame中的数据，使用`.show(...)`方法，我们会看到以下内容：
- en: '![](img/00036.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00036.jpeg)'
- en: 'Since DataFrames have schema, let''s see the schema of our `sample_data_df`
    using the `.printSchema()` method:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DataFrames具有模式，让我们使用`.printSchema()`方法查看我们的`sample_data_df`的模式：
- en: '![](img/00037.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00037.jpeg)'
- en: As you can see, the columns in our DataFrame have the datatypes matching the
    datatypes of the original `sample_data` RDD.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们DataFrame中的列具有与原始`sample_data` RDD的数据类型匹配的数据类型。
- en: Even though Python is not a strongly-typed language, DataFrames in PySpark are.
    Unlike RDDs, every element of a DataFrame column has a specified type (these are
    all listed in the `pyspark.sql.types` submodule) and all the data must conform
    to the specified schema.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Python不是一种强类型语言，但PySpark中的DataFrames是。与RDD不同，DataFrame列的每个元素都有指定的类型（这些都列在`pyspark.sql.types`子模块中），并且所有数据必须符合指定的模式。
- en: There's more...
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多信息...
- en: When you use the `.read` attribute of `SparkSession`, it returns a `DataFrameReader`
    object. `DataFrameReader` is an interface to read data into a DataFrame.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用`SparkSession`的`.read`属性时，它会返回一个`DataFrameReader`对象。`DataFrameReader`是一个用于将数据读入DataFrame的接口。
- en: From JSON
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从JSON
- en: 'To read data from a JSON-formatted file, you can simply do the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要从JSON格式文件中读取数据，您只需执行以下操作：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The only drawback (although a minor one) of reading the data from a JSON-formatted
    file is the fact that all the columns will be ordered alphabetically. See for
    yourself by running `sample_data_json_df.show()`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从JSON格式文件中读取数据的唯一缺点（尽管是一个小缺点）是所有列将按字母顺序排序。通过运行`sample_data_json_df.show()`来自己看看：
- en: '![](img/00038.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00038.jpeg)'
- en: 'The datatypes, however, remain unchanged: `sample_data_json_df.printSchema()`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但数据类型保持不变：`sample_data_json_df.printSchema()`
- en: '![](img/00039.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00039.jpeg)'
- en: From CSV
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从CSV
- en: 'Reading from a CSV file is equally simple:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从CSV文件中读取同样简单：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The only additional parameters passed make sure that the method treats the first
    row as column names (the `header` parameter) and that it will attempt to assign
    the right datatype to each column based on the content (the `inferSchema` parameter
    assigns strings by default).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 传递的唯一附加参数确保该方法将第一行视为列名（`header`参数），并且它将尝试根据内容为每列分配正确的数据类型（`inferSchema`参数默认分配字符串）。
- en: In contrast to reading the data from a JSON-formatted file, reading from a CSV
    file preserves the order of columns.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与从JSON格式文件中读取数据不同，从CSV文件中读取可以保留列的顺序。
- en: See also
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Check Spark's documentation for a full list of supported data formats: [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请查看Spark的文档，了解支持的数据格式的完整列表：[http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)
- en: Accessing underlying RDDs
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问底层RDD
- en: Switching to using DataFrames does not mean we need to completely abandon RDDs.
    Under the hood, DataFrames still use RDDs, but of `Row(...)` objects, as explained
    earlier. In this recipe, we will learn how to interact with the underlying RDD
    of a DataFrame.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到使用DataFrames并不意味着我们需要完全放弃RDD。在底层，DataFrames仍然使用RDD，但是`Row(...)`对象，如前所述。在本示例中，我们将学习如何与DataFrame的底层RDD交互。
- en: Getting ready
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark 2.3 environment. Also,
    you should have already gone through the previous recipe as we will reuse the
    data we created there.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的Spark 2.3环境。此外，您应该已经完成了上一个示例，因为我们将重用我们在那里创建的数据。
- en: There are no other requirements.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 没有其他要求。
- en: How to do it...
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this example, we will extract the size of the HDD and its type into separate
    columns, and will then calculate the minimum volume needed to put each computer
    in boxes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将把HDD的大小和类型提取到单独的列中，然后计算放置每台计算机所需的最小容量：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works...
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'As pointed out earlier, each element of the RDD inside the DataFrame is a `Row(...)`
    object. You can check it by running these two statements:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面指出的，DataFrame中的RDD的每个元素都是一个`Row(...)`对象。您可以通过运行以下两个语句来检查它：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 还有：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first one produces a single-item list where the element is `Row(...)`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个产生一个单项列表，其中元素是`Row(...)`：
- en: '![](img/00040.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00040.jpeg)'
- en: 'The other also produces a single-item list, but the item is a tuple:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个也产生一个单项列表，但项目是一个元组：
- en: '![](img/00041.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00041.jpeg)'
- en: The `sample_data` RDD is the first RDD we created in the previous recipe.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample_data` RDD是我们在上一个示例中创建的第一个RDD。'
- en: With that in mind, let's now turn our attention to the code.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个想法，现在让我们把注意力转向代码。
- en: 'First, we load the necessary modules: to work with the `Row(...)` objects,
    we need `pyspark.sql`, and we will use the `.round(...)` method later, so we need
    the `pyspark.sql.functions` submodule.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载必要的模块：要使用`Row(...)`对象，我们需要`pyspark.sql`，稍后我们将使用`.round(...)`方法，因此我们需要`pyspark.sql.functions`子模块。
- en: Next, we extract `.rdd` from `sample_data_df`. Using the `.map(...)` transformation,
    we first add the `HDD_size` column to the schema.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从`sample_data_df`中提取`.rdd`。使用`.map(...)`转换，我们首先将`HDD_size`列添加到模式中。
- en: Since we are working with RDDs, we want to retain all the other columns. Thus,
    we first convert the row (which is a `Row(...)` object) into a dictionary using
    the `.asDict()` method, so then we can later unpack it using `**`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在使用RDD，我们希望保留所有其他列。因此，我们首先使用`.asDict()`方法将行（即`Row(...)`对象）转换为字典，然后我们可以稍后使用`**`进行解包。
- en: In Python, the single `*` preceding a list of tuples, if passed as a parameter
    to a function, passes each element of a list as a separate argument to the function.
    The double `**` takes the first element and turns it into a keyword parameter,
    and uses the second element as the value to be passed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，单个`*`在元组列表之前，如果作为函数的参数传递，将列表的每个元素作为单独的参数传递给函数。双`**`将第一个元素转换为关键字参数，并使用第二个元素作为要传递的值。
- en: 'The second argument follows a simple convention: we pass the name of the column
    we want to create (the `HDD_size`), and set it to the desired value. In our first
    example, we split the `.HDD` column and extract the first element since it is `HDD_size`.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数遵循一个简单的约定：我们传递要创建的列的名称（`HDD_size`），并将其设置为所需的值。在我们的第一个示例中，我们拆分了`.HDD`列并提取了第一个元素，因为它是`HDD_size`。
- en: 'We repeat this step twice more: first, to create the `HDD_type` column, and
    second, to create the `Volume` column.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重复此步骤两次：首先创建`HDD_type`列，然后创建`Volume`列。
- en: Next, we use the `.toDF(...)` method to convert our RDD back to a DataFrame.
    Note that you can still use the `.toDF(...)` method to convert a regular RDD (that
    is, where each element is not a `Row(...)` object) to a DataFrame, but you will
    you need to pass a list of column names to the `.toDF(...)` method or you end
    up with unnamed columns.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`.toDF(...)`方法将我们的RDD转换回DataFrame。请注意，您仍然可以使用`.toDF(...)`方法将常规RDD（即每个元素不是`Row(...)`对象的情况）转换为DataFrame，但是您需要将列名的列表传递给`.toDF(...)`方法，否则您将得到未命名的列。
- en: Finally, we `.select(...)` the columns so we can `.round(...)` the newly created
    `Volume` column. The `.alias(...)` method produces a different name for the resulting
    column.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们`.select(...)`列，以便我们可以`.round(...)`新创建的`Volume`列。`.alias(...)`方法为生成的列产生不同的名称。
- en: 'The resulting DataFrame looks as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的DataFrame如下所示：
- en: '![](img/00042.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00042.jpeg)'
- en: Unsurprisingly, the desktop iMac would require the biggest box.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，台式iMac需要最大的盒子。
- en: Performance optimizations
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能优化
- en: 'Starting with Spark 2.0, the performance of PySpark using DataFrames was on
    apar with that of Scala or Java. However, there was one exception: using **User
    Defined Functions** (**UDFs**); if a user defined a pure Python method and registered
    it as a UDF, under the hood, PySpark would have to constantly switch runtimes
    (Python to JVM and back). This was the main reason for an enormous performance
    hit compared with Scala, which does not need to convert the JVM object to a Python
    object.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark 2.0开始，使用DataFrame的PySpark性能与Scala或Java相当。但是，有一个例外：使用**用户定义函数**（**UDFs**）；如果用户定义了一个纯Python方法并将其注册为UDF，在幕后，PySpark将不断切换运行时（Python到JVM再到Python）。这是与Scala相比性能巨大下降的主要原因，Scala不需要将JVM对象转换为Python对象。
- en: Things have changed significantly in Spark 2.3\. First, Spark started using
    the new Apache project. Arrow creates a single memory space used by all environments,
    thus removing the need for constant copying and converting between objects.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.3中，情况发生了显著变化。首先，Spark开始使用新的Apache项目。Arrow创建了一个所有环境都使用的单一内存空间，从而消除了不断复制和转换对象的需要。
- en: '![](img/00043.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00043.jpeg)'
- en: Source: https://arrow.apache.org/img/shared.png
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：https://arrow.apache.org/img/shared.png
- en: For an overview of Apache Arrow, go to [https://arrow.apache.org](https://arrow.apache.org).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Apache Arrow的概述，请访问[https://arrow.apache.org](https://arrow.apache.org)。
- en: Second, Arrow stores columnar objects in memory giving a big performance boost.
    Thus, in order to further leverage that, portions of the PySpark code have been
    refactored and that brought us vectorized UDFs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，Arrow将列对象存储在内存中，从而大大提高了性能。因此，为了进一步利用这一点，PySpark代码的部分已经进行了重构，这为我们带来了矢量化UDF。
- en: 'In this recipe, we will learn how to use them and test the performance of both:
    the old, row-by-row UDFs, and the new vectorized ones.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将学习如何使用它们，并测试旧的逐行UDF和新的矢量化UDF的性能。
- en: Getting ready
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark 2.3 environment.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要有一个可用的Spark 2.3环境。
- en: There are no other requirements.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 没有其他要求。
- en: How to do it...
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this example, we will use SciPy to return a value of a normal probability
    distribution function (PDF) for a set of 1,000,000 random numbers between 0 and
    1:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将使用SciPy返回在0到1之间的100万个随机数集的正态概率分布函数（PDF）的值。
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How it works...
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'First, as always, we import all the modules we will need to run this example:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，像往常一样，我们导入我们将需要运行此示例的所有模块：
- en: '`pyspark.sql.functions` gives us access to PySpark SQL functions. We will use
    it to create our DataFrame with random numbers.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pyspark.sql.functions`为我们提供了访问PySpark SQL函数的途径。我们将使用它来创建带有随机数字的DataFrame。'
- en: The `pandas` framework will give us access to the `.Series(...)` datatype so
    we can return a column from our UDF.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`框架将为我们提供`.Series(...)`数据类型的访问权限，以便我们可以从我们的UDF返回一个列。'
- en: '`scipy.stats` give us access to statistical methods. We will use it to calculate
    the normal PDF for our random numbers.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scipy.stats`为我们提供了访问统计方法的途径。我们将使用它来计算我们的随机数字的正态PDF。'
- en: Next, our `big_df`. `SparkSession` has a convenience method, `.range(...)`,
    which allows us to create a range of numbers within specified bounds; in this
    example, we simply create a DataFrame with one million records.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的`big_df`。`SparkSession`有一个方便的方法`.range(...)`，允许我们在指定的范围内创建一系列数字；在这个示例中，我们只是创建了一个包含一百万条记录的DataFrame。
- en: In the next line, we add another column to our DataFrame using the `.withColumn(...)`
    method; the column's name is `val` and it will contain one million `.rand()` numbers.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一行中，我们使用`.withColumn(...)`方法向DataFrame添加另一列；列名为`val`，它将包含一百万个`.rand()`数字。
- en: The `.rand()` method returns pseudo-random numbers drawn from a uniform distribution
    that ranges between 0 and 1.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`.rand()`方法返回从0到1之间的均匀分布中抽取的伪随机数。'
- en: Finally, we `.cache()` the DataFrame so it all remains fully in memory (for
    speeding up the process).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`.cache()`方法缓存DataFrame，以便它完全保留在内存中（以加快速度）。
- en: Next, we define the `pandas_cdf(...)` method. Note the `@f.pandas_udf` decorator
    preceding the method's declaration as this is key to registering a vectorized
    UDF in PySpark and has only became available in Spark 2.3.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义`pandas_cdf(...)`方法。请注意`@f.pandas_udf`装饰器在方法声明之前，因为这是在PySpark中注册矢量化UDF的关键，并且仅在Spark
    2.3中才可用。
- en: Note that we did not have to decorate our method; we could have instead registered
    our vectorized method as `f.pandas_udf(f=pandas_pdf, returnType='double', functionType=f.PandasUDFType.SCALAR)`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不必装饰我们的方法；相反，我们可以将我们的矢量化方法注册为`f.pandas_udf(f=pandas_pdf, returnType='double',
    functionType=f.PandasUDFType.SCALAR)`。
- en: The first parameter to the decorator method is the return type of the UDF, in
    our case a `double`. This can be either a DDL-formatted type string or `pyspark.sql.types.DataType`.
    The second parameter is the function type; if we return a single column from our
    method (such as pandas' `.Series(...)` in our example), it will be `.PandasUDFType.SCALAR`
    (by default). If, on the other hand, we operate on multiple columns (such as pandas'
    `DataFrame(...)`), we would define `.PandasUDFType.GROUPED_MAP`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 装饰器方法的第一个参数是UDF的返回类型，在我们的例子中是`double`。这可以是DDL格式的类型字符串，也可以是`pyspark.sql.types.DataType`。第二个参数是函数类型；如果我们从我们的方法返回单列（例如我们的示例中的pandas'`.Series(...)`），它将是`.PandasUDFType.SCALAR`（默认情况下）。另一方面，如果我们操作多列（例如pandas'`DataFrame(...)`），我们将定义`.PandasUDFType.GROUPED_MAP`。
- en: Our `pandas_pdf(...)` method simply accepts a single column and returns a pandas'
    `.Series(...)` object with values of normal CDF-corresponding numbers.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`pandas_pdf(...)`方法简单地接受一个单列，并返回一个带有正态CDF对应数字值的pandas'`.Series(...)`对象。
- en: 'Finally, we simply use the new method to transform our data. Here''s what the
    top five records look like (yours most likely will look different since we are
    creating one million random numbers):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简单地使用新方法来转换我们的数据。以下是前五条记录的样子（您的可能看起来不同，因为我们正在创建一百万个随机数）：
- en: '![](img/00044.jpeg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00044.jpeg)'
- en: There's more...
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Let''s now compare the performance of the two approaches:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们比较这两种方法的性能：
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `test_pandas_pdf()` method simply uses the `pandas_pdf(...)` method to retrieve
    the PDF from the normal distribution, performs the `.count(...)` operation, and
    prints out the results using the `.show(...)` method. The `test_pdf()` method
    does the same but uses the `pdf(...)` method instead, which is the row-by-row
    way of using the UDFs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_pandas_pdf()`方法简单地使用`pandas_pdf(...)`方法从正态分布中检索PDF，执行`.count(...)`操作，并使用`.show(...)`方法打印结果。`test_pdf()`方法也是一样，但是使用`pdf(...)`方法，这是使用UDF的逐行方式。'
- en: 'The `%timeit` decorator simply runs the `test_pandas_pdf()` or the `test_pdf()`
    methods seven times, multiplied by each execution. Here''s a sample output (abbreviated
    as it is, as you might have expected, highly repetitive) for running the `test_pandas_pdf()`
    method:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`%timeit`装饰器简单地运行`test_pandas_pdf()`或`test_pdf()`方法七次，每次执行都会乘以。这是运行`test_pandas_pdf()`方法的一个示例输出（因为它是高度重复的，所以缩写了）：'
- en: '![](img/00045.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00045.jpeg)'
- en: 'The timings for the `test_pdf()` method are quoted as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_pdf()`方法的时间如下所示：'
- en: '![](img/00046.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00046.jpeg)'
- en: As you can see, the vectorized UDFs provide ~100x performance improvements!
    Don't get too excited, as such speedups are only expected for more complex queries,
    such as the one we used previously.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，矢量化UDF提供了约100倍的性能改进！不要太激动，因为只有对于更复杂的查询才会有这样的加速，就像我们之前使用的那样。
- en: See also
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: To learn more, check out this blog post from Databricks announcing the vectorized
    UDFs: [https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多，请查看Databricks发布的关于矢量化UDF的博客文章：[https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)
- en: Inferring the schema using reflection
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用反射推断模式
- en: DataFrames have schema, RDDs don't. That is, unless RDDs are composed of `Row(...)`
    objects.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame有模式，RDD没有。也就是说，除非RDD由`Row(...)`对象组成。
- en: In this recipe, we will learn how to create DataFrames by inferring the schema
    using reflection.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将学习如何使用反射推断模式创建DataFrames。
- en: Getting ready
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark 2.3 environment.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要拥有一个可用的Spark 2.3环境。
- en: There are no other requirements.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 没有其他要求。
- en: How to do it...
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this example, we will first read our CSV sample data into an RDD and then
    create a DataFrame from it. Here''s the code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们首先将CSV样本数据读入RDD，然后从中创建一个DataFrame。以下是代码：
- en: '[PRE8]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How it works...
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we load the SQL module of PySpark.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载PySpark的SQL模块。
- en: Next, we read the `DataFrames_sample.csv` file using the `.textFile(...)` method
    of SparkContext.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用SparkContext的`.textFile(...)`方法读取`DataFrames_sample.csv`文件。
- en: Review the previous chapter if you do not yet know how to read data into RDDs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还不知道如何将数据读入RDD，请查看前一章。
- en: 'The resulting RDD looks as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的RDD如下所示：
- en: '![](img/00047.jpeg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00047.jpeg)'
- en: As you can see, the RDD still contains the row with column names. In order to
    get rid of it, we first extract it using the `.first()` method and then later
    using the `.filter(...)` transformation to remove any row that is equal to the
    header.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，RDD仍然包含具有列名的行。为了摆脱它，我们首先使用`.first()`方法提取它，然后使用`.filter(...)`转换来删除与标题相等的任何行。
- en: Next, we split each row with a comma and create a `Row(...)` object for each
    observation. Note here that we convert all of the fields to the proper datatypes.
    For example, the `Id` column should be an integer, the `Model` name is a string,
    and `W` (width) is a float.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们用逗号分割每一行，并为每个观察创建一个`Row(...)`对象。请注意，我们将所有字段转换为适当的数据类型。例如，`Id`列应该是整数，`Model`名称是字符串，`W`（宽度）是浮点数。
- en: 'Finally, we simply call the `.createDataFrame(...)` method of SparkSession
    to convert our RDD of `Row(...)` objects into a DataFrame. Here''s the final result:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只需调用SparkSession的`.createDataFrame(...)`方法，将我们的`Row(...)`对象的RDD转换为DataFrame。这是最终结果：
- en: '![](img/00048.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00048.jpeg)'
- en: See also
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Check out Spark's documentation to learn more: [https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection](https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看Spark的文档以了解更多信息：[https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection](https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection)
- en: Specifying the schema programmatically
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以编程方式指定模式
- en: In the previous recipe, we learned how to infer the schema of a DataFrame using
    reflection.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们学习了如何使用反射推断DataFrame的模式。
- en: In this recipe, we will learn how to specify the schema programmatically.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将学习如何以编程方式指定模式。
- en: Getting ready
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark 2.3 environment.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的Spark 2.3环境。
- en: There are no other requirements.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 没有其他要求。
- en: How to do it...
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this example, we will learn how to specify the schema programmatically:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将学习如何以编程方式指定模式：
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we create a list of `.StructField(...)` objects. `.StructField(...)`
    is a programmatic way of adding a field to a schema in PySpark. The first parameter
    is the name of the column we want to add.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个`.StructField(...)`对象的列表。`.StructField(...)`是在PySpark中以编程方式向模式添加字段的方法。第一个参数是我们要添加的列的名称。
- en: The second parameter is the datatype of the data we want to store in the column;
    some of the types  available include `.LongType()`, `.StringType()`, `.DoubleType()`,
    `.BooleanType()`, `.DateType()`, and `.BinaryType()`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是我们想要存储在列中的数据的数据类型；一些可用的类型包括`.LongType()`、`.StringType()`、`.DoubleType()`、`.BooleanType()`、`.DateType()`和`.BinaryType()`。
- en: For a full list of available datatypes in PySpark, go to [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types.](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types.)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 有关PySpark中可用数据类型的完整列表，请转到[http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types.](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types.)
- en: The last parameter of `.StructField(...)` indicates whether the column can contain
    null values or not; if set to `True`, it means it can.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`.StructField(...)`的最后一个参数指示列是否可以包含空值；如果设置为`True`，则表示可以。'
- en: Next, we read in the `DataFrames_sample.csv` file using the `.textFile(...)`
    method of SparkContext. We filter out the header, as we will specify the schema
    explicitly and we do not need the name columns that are stored in the first row.
    Next, we split each row with a comma and impose the right datatypes on each element
    so it conforms to the schema we just specified.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用SparkContext的`.textFile(...)`方法读取`DataFrames_sample.csv`文件。我们过滤掉标题，因为我们将明确指定模式，不需要存储在第一行的名称列。接下来，我们用逗号分割每一行，并对每个元素施加正确的数据类型，使其符合我们刚刚指定的模式。
- en: 'Finally, we call the `.createDataFrame(...)` method but this time, along with
    the RDD, we also pass `schema`. The resulting DataFrame looks as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们调用`.createDataFrame(...)`方法，但这次，除了RDD，我们还传递`schema`。生成的DataFrame如下所示：
- en: '![](img/00049.jpeg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00049.jpeg)'
- en: See also
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Check out Spark's documentation for more: [https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看Spark的文档以了解更多信息：[https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema)
- en: Creating a temporary table
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建临时表
- en: DataFrames can easily be manipulated with SQL queries in Spark.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，可以很容易地使用SQL查询来操作DataFrame。
- en: In this recipe, we will learn how to create a temporary view so you can access
    the data within DataFrame using SQL.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将学习如何创建临时视图，以便您可以使用SQL访问DataFrame中的数据。
- en: Getting ready
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark 2.3 environment. You
    should have gone through the previous recipe, as we will be using the `sample_data_schema`
    DataFrame we created there.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的Spark 2.3环境。您应该已经完成了上一个示例，因为我们将使用那里创建的`sample_data_schema` DataFrame。
- en: There are no other requirements.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 没有其他要求。
- en: How to do it...
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We simply use the `.createTempView(...)` method of a DataFrame:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需使用DataFrame的`.createTempView(...)`方法：
- en: '[PRE10]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How it works...
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `.createTempView(...)` method is the simplest way to create a temporary
    view that later can be used to query the data. The only required parameter is
    the name of the view.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`.createTempView(...)`方法是创建临时视图的最简单方法，稍后可以用来查询数据。唯一需要的参数是视图的名称。'
- en: 'Let''s see how such a temporary view can now be used to extract data:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这样的临时视图现在如何被用来提取数据：
- en: '[PRE11]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We simply use the `.sql(...)` method of SparkSession, which allows us to write
    ANSI-SQL code to manipulate data within a DataFrame. In this example, we simply
    extract four columns. Here''s what we get back:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需使用SparkSession的`.sql(...)`方法，这使我们能够编写ANSI-SQL代码来操作DataFrame中的数据。在这个例子中，我们只是提取了四列。这是我们得到的：
- en: '![](img/00050.jpeg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00050.jpeg)'
- en: There's more...
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Once you have created a temporary view, you cannot create another view with
    the same name. However, Spark provides another method that allows us to either
    create or update a view: `.createOrReplaceTempView(...)`. As the name suggests,
    by calling this method, we either create a new view if one does not exist, or
    we replace an already existing one with the new one:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了临时视图，就不能再创建具有相同名称的另一个视图。但是，Spark提供了另一种方法，允许我们创建或更新视图：`.createOrReplaceTempView（...）`。顾名思义，通过调用此方法，我们要么创建一个新视图（如果不存在），要么用新视图替换已经存在的视图：
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As before, we can now use it to interact with the data using SQL queries:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，我们现在可以使用它来使用SQL查询与数据交互：
- en: '[PRE13]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here''s what we get back:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的：
- en: '![](img/00051.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00051.jpeg)'
- en: Using SQL to interact with DataFrames
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SQL与DataFrame交互
- en: In the previous recipe, we learned how to create or replace temporary views.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们学习了如何创建或替换临时视图。
- en: In this recipe, we will learn how to play with the data within a DataFrame using
    SQL queries.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将学习如何使用SQL查询在DataFrame中处理数据。
- en: Getting ready
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark 2.3 environment. You
    should have gone through the *Specifying the schema programmatically* recipe,
    as we will be using the `sample_data_schema` DataFrame we created there.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要具有工作的Spark 2.3环境。您应该已经通过*以编程方式指定模式*的示例，因为我们将使用在那里创建的`sample_data_schema`
    DataFrame。
- en: There are no other requirements.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 没有其他要求。
- en: How to do it...
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this example, we will extend our original data with the form factor for
    each model of Apple''s computer:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将扩展我们原始的数据，为苹果电脑的每个型号添加形式因子：
- en: '[PRE14]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we create a simple DataFrame with two columns: `Model` and `FormFactor`.
    In this example, we use the `.toDF(...)` method of an RDD to quickly convert it
    into a DataFrame. The list that we pass is simply a list of column names and the
    schema will be inferred automatically.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个简单的DataFrame，其中包含两列：“Model”和“FormFactor”。在这个例子中，我们使用RDD的`.toDF（...）`方法，快速将其转换为DataFrame。我们传递的列表只是列名的列表，模式将自动推断。
- en: Next, we create the model's view and replace `sample_data_view`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建模型视图并替换`sample_data_view`。
- en: Finally, to append `FormFactor` to our original data, we simply join the two
    views on the `Model` column. As the `.sql(...)` method accepts regular SQL expressions,
    we also use the `ORDER BY` clause so we can order by weight.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要将`FormFactor`附加到我们的原始数据，我们只需在`Model`列上连接两个视图。由于`.sql（...）`方法接受常规SQL表达式，因此我们还使用`ORDER
    BY`子句，以便按权重排序。
- en: 'Here''s what we get back:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的：
- en: '![](img/00052.jpeg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00052.jpeg)'
- en: There's more...
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The SQL queries are not limited to extracting data only. We can also run some
    aggregations:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: SQL查询不仅限于仅提取数据。我们还可以运行一些聚合：
- en: '[PRE15]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this simple example, we will count how many different computers of different
    FormFactors we have. The `COUNT(*)` operator counts how many computers we have
    and works in conjunction with the `GROUP BY` clause that specifies the aggregation
    columns.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，我们将计算不同FormFactors的不同计算机数量。`COUNT（*）`运算符计算我们有多少台计算机，并与指定聚合列的`GROUP
    BY`子句一起工作。
- en: 'Here''s what we get from this query:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个查询中我们得到了什么：
- en: '![](img/00053.jpeg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00053.jpeg)'
- en: Overview of DataFrame transformations
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame转换概述
- en: Just like RDDs, DataFrames have both transformations and actions. As a reminder,
    transformations convert one DataFrame into another, while actions perform some
    computation on a DataFrame and normally return the result to the driver. Also,
    just like the RDDs, transformations in DataFrames are lazy.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 就像RDD一样，DataFrame既有转换又有操作。作为提醒，转换将一个DataFrame转换为另一个DataFrame，而操作对DataFrame执行一些计算，并通常将结果返回给驱动程序。而且，就像RDD一样，DataFrame中的转换是惰性的。
- en: In this recipe, we will review the most common transformations.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将回顾最常见的转换。
- en: Getting ready
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark 2.3 environment. You
    should have gone through the *Specifying schema programmatically* recipe, as we
    will be using the `sample_data_schema` DataFrame we created there.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要具有工作的Spark 2.3环境。您应该已经通过*以编程方式指定模式*的示例，因为我们将使用在那里创建的`sample_data_schema`
    DataFrame。
- en: There are no other requirements.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 没有其他要求。
- en: How to do it...
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: In this section, we will list some of the most common transformations available
    for DataFrames. The purpose of this list is not to provide a comprehensive enumeration
    of all available transformations, but to give you some intuition behind the most
    common ones.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将列出一些可用于DataFrame的最常见转换。此列表的目的不是提供所有可用转换的全面枚举，而是为您提供最常见转换背后的一些直觉。
- en: The .select(...) transformation
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.select（...）`转换'
- en: The `.select(...)` transformation allows us to extract column or columns from
    a DataFrame. It works the same way as `SELECT` found in SQL.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`.select（...）`转换允许我们从DataFrame中提取列。它的工作方式与SQL中的`SELECT`相同。'
- en: 'Look at the following code snippet:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下代码片段：
- en: '[PRE16]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It produces the following output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生以下输出：
- en: '![](img/00054.jpeg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00054.jpeg)'
- en: 'In SQL syntax, this would look like the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL语法中，这将如下所示：
- en: '[PRE17]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The .filter(...) transformation
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.filter（...）`转换'
- en: The `.filter(...)` transformation, in contrast to `.select(...)`, selects only
    rows that pass the condition specified. It can be compared with the `WHERE` statement
    from SQL.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`.filter（...）`转换与`.select（...）`相反，仅选择满足指定条件的行。它可以与SQL中的`WHERE`语句进行比较。'
- en: 'Look at the following code snippet:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下代码片段：
- en: '[PRE18]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'It produces the following output:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生以下输出：
- en: '![](img/00055.jpeg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00055.jpeg)'
- en: 'In SQL syntax, the preceding would be equivalent to:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL语法中，前面的内容相当于：
- en: '[PRE19]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The .groupBy(...) transformation
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.groupBy（...）`转换'
- en: The `.groupBy(...)` transformation performs data aggregation based on the value
    (or values) from a column (or multiple columns). In SQL syntax, this equates to
    `GROUP BY`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`.groupBy（...）`转换根据列（或多个列）的值执行数据聚合。在SQL语法中，这相当于`GROUP BY`。'
- en: 'Look at the following code:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下代码：
- en: '[PRE20]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It produces this result:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生此结果：
- en: '![](img/00056.jpeg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpeg)'
- en: 'In SQL syntax, this would be:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL语法中，这将是：
- en: '[PRE21]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The .orderBy(...) transformation
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.orderBy(...)` 转换'
- en: The `.orderBy(...)` transformation sorts the results given the columns specified.
    An equivalent from the SQL world would also be `ORDER BY`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`.orderBy(...)` 转换根据指定的列对结果进行排序。 SQL世界中的等效项也将是`ORDER BY`。'
- en: 'Look at the following code snippet:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下代码片段：
- en: '[PRE22]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It produces the following output:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生以下输出：
- en: '![](img/00057.jpeg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00057.jpeg)'
- en: 'The SQL equivalent would be:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: SQL等效项将是：
- en: '[PRE23]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can also change the order of sorting to descending by using the `.desc()`
    switch of a column (the `.col(...)` method). Look at the following snippet:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用列的`.desc()`开关（`.col(...)`方法）将排序顺序更改为降序。看看以下片段：
- en: '[PRE24]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It produces the following output:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生以下输出：
- en: '![](img/00058.jpeg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00058.jpeg)'
- en: 'Put in SQL syntax, the preceding expression would be:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 以SQL语法表示，前面的表达式将是：
- en: '[PRE25]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The .withColumn(...) transformation
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.withColumn(...)` 转换'
- en: The `.withColumn(...)` transformation applies a function to some other columns
    and/or literals (using the `.lit(...)` method) and stores it as a new function.
    In SQL, this could be any method that applies any transformation to any of the
    columns and uses `AS` to assign a new column name. This transformation extends
    the original DataFrame.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`.withColumn(...)` 转换将函数应用于其他列和/或文字（使用`.lit(...)`方法）并将其存储为新函数。在SQL中，这可以是应用于任何列的任何转换的任何方法，并使用`AS`分配新列名。此转换扩展了原始数据框。'
- en: 'Look at the following code snippet:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下代码片段：
- en: '[PRE26]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'It produces the following output:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生以下输出：
- en: '![](img/00059.jpeg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00059.jpeg)'
- en: 'You could achieve the same result with the `.select(...)` transformation. The
    following code will produce the same result:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`.select(...)`转换来实现相同的结果。以下代码将产生相同的结果：
- en: '[PRE27]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The SQL (T-SQL) equivalent would be:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: SQL（T-SQL）等效项将是：
- en: '[PRE28]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The .join(...) transformation
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.join(...)` 转换'
- en: The `.join(...)` transformation allow us to join two DataFrames. The first parameter
    is the other DataFrame we want to join with, while the second parameter specifies
    the columns on which to join, and the final parameter specifies the nature of
    the join. Available types are `inner`, `cross`, `outer`, `full`, `full_outer`,
    `left`, `left_outer`, `right`, `right_outer`, `left_semi`, and `left_anti`. In
    SQL, the equivalent is the `JOIN` statement.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`.join(...)` 转换允许我们连接两个数据框。第一个参数是我们要连接的另一个数据框，而第二个参数指定要连接的列，最后一个参数指定连接的性质。可用类型为`inner`，`cross`，`outer`，`full`，`full_outer`，`left`，`left_outer`，`right`，`right_outer`，`left_semi`和`left_anti`。在SQL中，等效项是`JOIN`语句。'
- en: If you're not familiar with the `ANTI` and `SEMI` joins, check out this blog: [https://blog.jooq.org/2015/10/13/semi-join-and-anti-join-should-have-its-own-syntax-in-sql/](https://blog.jooq.org/2015/10/13/semi-join-and-anti-join-should-have-its-own-syntax-in-sql/).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉`ANTI`和`SEMI`连接，请查看此博客：[https://blog.jooq.org/2015/10/13/semi-join-and-anti-join-should-have-its-own-syntax-in-sql/](https://blog.jooq.org/2015/10/13/semi-join-and-anti-join-should-have-its-own-syntax-in-sql/)。
- en: 'Look at the following code as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如下查看以下代码：
- en: '[PRE29]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It produces the following output:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生以下输出：
- en: '![](img/00060.jpeg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00060.jpeg)'
- en: 'In SQL syntax, this would be:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL语法中，这将是：
- en: '[PRE30]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If we had a DataFrame that would not list every `Model` (note that the `MacBook`
    is missing), then the following code is:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个数据框，不会列出每个`Model`（请注意`MacBook`缺失），那么以下代码是：
- en: '[PRE31]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This will generate a table with some missing values:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个带有一些缺失值的表：
- en: '![](img/00061.jpeg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00061.jpeg)'
- en: 'The `RIGHT` join keeps only the records that are matched with the records in
    the right DataFrame. Thus, look at the following code:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`RIGHT`连接仅保留与右数据框中的记录匹配的记录。因此，看看以下代码：'
- en: '[PRE32]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This produces a table as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下表：
- en: '![](img/00062.jpeg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00062.jpeg)'
- en: 'The `SEMI` and `ANTI` joins are somewhat recent additions. The `SEMI` join
    keeps all the records from the left DataFrame that are matched with the records
    in the right DataFrame (as with the `RIGHT` join) but *only keeps the columns
    from the left DataFrame*; the `ANTI` join is the opposite of the `SEMI` join—it keeps
    only the records that are not found in the right DataFrame. So, the following
    example of a `SEMI` join is:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`SEMI`和`ANTI`连接是相对较新的添加。`SEMI`连接保留与右数据框中的记录匹配的左数据框中的所有记录（与`RIGHT`连接一样），但*仅保留左数据框中的列*；`ANTI`连接是`SEMI`连接的相反，它仅保留在右数据框中找不到的记录。因此，`SEMI`连接的以下示例是：'
- en: '[PRE33]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will produce the following result:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '![](img/00063.jpeg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00063.jpeg)'
- en: 'Whereas the example of an `ANTI` join is:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 而`ANTI`连接的示例是：
- en: '[PRE34]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will generate the following:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下内容：
- en: '![](img/00064.jpeg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00064.jpeg)'
- en: The .unionAll(...) transformation
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.unionAll(...)` 转换'
- en: The `.unionAll(...)` transformation appends values from another DataFrame. An
    equivalent in SQL syntax is `UNION ALL`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '`.unionAll(...)` 转换附加来自另一个数据框的值。 SQL语法中的等效项是`UNION ALL`。'
- en: 'Look at the following code:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下代码：
- en: '[PRE35]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It produces the following result:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生以下结果：
- en: '![](img/00065.jpeg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00065.jpeg)'
- en: 'In SQL syntax, the preceding would read as:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL语法中，前面的内容将读作：
- en: '[PRE36]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The .distinct(...) transformation
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.distinct(...)` 转换'
- en: The `.distinct(...)` transformation returns a list of distinct values from a
    column. An equivalent in SQL would be `DISTINCT`.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '`.distinct(...)` 转换返回列中不同值的列表。 SQL中的等效项将是`DISTINCT`。'
- en: 'Look at the following code:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下代码：
- en: '[PRE37]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'It produces the following result:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生以下结果：
- en: '![](img/00066.jpeg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00066.jpeg)'
- en: 'In SQL syntax, this would be:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL语法中，这将是：
- en: '[PRE38]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The .repartition(...) transformation
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.repartition(...)` 转换'
- en: The `.repartition(...)` transformation shuffles the data around the cluster
    and combines it into a specified number of partitions. You can also specify the
    column or columns you want to use to perform the partitioning on. There is no
    direct equivalent in the SQL world.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`.repartition(...)` 转换在集群中移动数据并将其组合成指定数量的分区。您还可以指定要在其上执行分区的列。在SQL世界中没有直接等效项。'
- en: 'Look at the following code:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下代码：
- en: '[PRE39]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'It produces (as expected) this result:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生了（预期的）这个结果：
- en: '[PRE40]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The .fillna(...) transformation
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.fillna(...)` 转换'
- en: The `.fillna(...)` transformation fills in the missing values in a DataFrame.
    You can either specify a single value and all the missing values will be filled
    in with it, or you can pass a dictionary where each key is the name of the column,
    and the values are to fill the missing values in the corresponding column. No
    direct equivalent exists in the SQL world.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`.fillna(...)` 转换填充 DataFrame 中的缺失值。您可以指定一个单个值，所有缺失的值都将用它填充，或者您可以传递一个字典，其中每个键是列的名称，值是要填充相应列中的缺失值。在
    SQL 世界中没有直接的等价物。'
- en: 'Look at the following code:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 看下面的代码：
- en: '[PRE41]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'It produces the following output:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生了以下输出：
- en: '![](img/00067.jpeg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00067.jpeg)'
- en: 'We could also specify the dictionary, as the `21.4` value does not really fit
    the `A` column. In the following code, we first calculate averages for each of
    the columns:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以指定字典，因为 `21.4` 值实际上并不适合 `A` 列。在下面的代码中，我们首先计算每列的平均值：
- en: '[PRE42]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `.toPandas()` method is an action (that we will cover in the next recipe)
    and it returns a pandas DataFrame. The `.to_dict(...)` method of the pandas DataFrame
    converts it into a dictionary, where the `records` parameter produces a regular
    dictionary where each column is the key and each value is the record.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '`.toPandas()` 方法是一个操作（我们将在下一个示例中介绍），它返回一个 pandas DataFrame。pandas DataFrame
    的 `.to_dict(...)` 方法将其转换为字典，其中 `records` 参数产生一个常规字典，其中每个列是键，每个值是记录。'
- en: 'The preceding code produces the following result:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下结果：
- en: '![](img/00068.jpeg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00068.jpeg)'
- en: The .dropna(...) transformation
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.dropna(...)` 转换'
- en: The `.dropna(...)` transformation removes records that have missing values.
    You can specify the threshold that translates to a minimum number of missing observations
    in the record that qualifies it to be removed. As with `.fillna(...)`, there is
    no direct equivalent in the SQL world.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '`.dropna(...)` 转换删除具有缺失值的记录。您可以指定阈值，该阈值转换为记录中的最少缺失观察数，使其符合被删除的条件。与 `.fillna(...)`
    一样，在 SQL 世界中没有直接的等价物。'
- en: 'Look at the following code:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 看下面的代码：
- en: '[PRE43]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'It produces the following result:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生了以下结果：
- en: '![](img/00069.jpeg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00069.jpeg)'
- en: 'Specifying `thresh=2`:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 指定 `thresh=2`：
- en: '[PRE44]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'It retains the first and the fourth records:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 它保留了第一条和第四条记录：
- en: '![](img/00070.jpeg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00070.jpeg)'
- en: The .dropDuplicates(...) transformation
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.dropDuplicates(...)` 转换'
- en: The `.dropDuplicates(...)` transformation, as the name suggests, removes duplicated
    records. You can also specify a subset parameter as a list of column names; the
    method will remove duplicated records based on the values found in those columns.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '`.dropDuplicates(...)` 转换，顾名思义，删除重复的记录。您还可以指定一个子集参数作为列名的列表；该方法将根据这些列中找到的值删除重复的记录。'
- en: 'Look at the following code:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 看下面的代码：
- en: '[PRE45]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: It produces the following result
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生了以下结果
- en: '![](img/00071.jpeg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00071.jpeg)'
- en: The .summary() and .describe() transformations
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.summary()` 和 `.describe()` 转换'
- en: The `.summary()` and `.describe()` transformations produce similar descriptive
    statistics, with the `.summary()` transformation additionally producing quartiles.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`.summary()` 和 `.describe()` 转换产生类似的描述性统计数据，`.summary()` 转换另外还产生四分位数。'
- en: 'Look at the following code:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 看下面的代码：
- en: '[PRE46]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'It produces the following result:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生了以下结果：
- en: '![](img/00072.jpeg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.jpeg)'
- en: The .freqItems(...) transformation
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.freqItems(...)` 转换'
- en: The `.freqItems(...)` transformation returns a list of frequent items from a
    column. You can also specify a `minSupport` parameter that will throw away items
    that are below a certain threshold.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`.freqItems(...)` 转换返回列中频繁项的列表。您还可以指定 `minSupport` 参数，该参数将丢弃低于某个阈值的项。'
- en: 'Look at the following code:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 看下面的代码：
- en: '[PRE47]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'It produces this result:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生了这个结果：
- en: '![](img/00073.jpeg)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00073.jpeg)'
- en: See also
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Refer to Spark's documentation for more transformations: [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多转换，请参阅 Spark 文档：[http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)
- en: Overview of DataFrame actions
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame 操作概述
- en: Transformations listed in the previous recipe transform one DataFrame into another.
    However, they only get executed once an action is called on a **DataFrame**.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个示例中列出的转换将一个 DataFrame 转换为另一个 DataFrame。但是，只有在对 **DataFrame** 调用操作时才会执行它们。
- en: In this recipe, we will provide an overview of the most popular actions.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将概述最常见的操作。
- en: Getting ready
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark 2.3 environment. You
    should have gone through the previous recipe, *Specifying schema programmatically*,
    as we will be using the `sample_data_schema` DataFrame we created there.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的 Spark 2.3 环境。您应该已经完成了上一个示例，*以编程方式指定模式*，因为我们将使用在那里创建的 `sample_data_schema`
    DataFrame。
- en: There are no other requirements.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 没有其他要求。
- en: How to do it...
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: In this section, we will list some of the most common actions available for
    DataFrames. The purpose of this list is not to provide a comprehensive enumeration
    of all available transformations, but to give you some intuition behind the most
    common ones.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将列出一些可用于 DataFrame 的最常见操作。此列表的目的不是提供所有可用转换的全面枚举，而是为您提供对最常见转换的直觉。
- en: The .show(...) action
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.show(...)` 操作'
- en: The `.show(...)` action, by default, shows the top five rows in tabular form.
    You can specify how many records to retrieve by passing an integer as a parameter.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`.show(...)` 操作默认显示表格形式的前五行记录。您可以通过传递整数作为参数来指定要检索的记录数。'
- en: 'Look at the following code:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 看下面的代码：
- en: '[PRE48]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'It produces this result:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生了这个结果：
- en: '![](img/00074.jpeg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00074.jpeg)'
- en: The .collect() action
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.collect()` 操作'
- en: The `.collect()` action, as the name suggests, collects all the results from
    all the worker nodes, and returns them to the driver. Beware of using this method
    on a big dataset as your driver will most likely break if you try to return the
    whole DataFrame of billions of records; use this method only to return small,
    aggregated data.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '`.collect()` 操作，顾名思义，从所有工作节点收集所有结果，并将它们返回给驱动程序。在大型数据集上使用此方法时要小心，因为如果尝试返回数十亿条记录的整个DataFrame，驱动程序很可能会崩溃；只能用此方法返回小的、聚合的数据。'
- en: 'Look at the following code:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的代码：
- en: '[PRE49]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'It produces the following result:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生了以下结果：
- en: '![](img/00075.jpeg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00075.jpeg)'
- en: The .take(...) action
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.take(...)` 操作'
- en: 'The `.take(...)` action works in the same as in RDDs–it returns the specified
    number of records to the driver node:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '`.take(...)` 操作的工作方式与RDDs中的相同–它将指定数量的记录返回给驱动节点：'
- en: '[PRE50]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'It produces this result:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生了这个结果：
- en: '![](img/00076.jpeg)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00076.jpeg)'
- en: The .toPandas() action
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`.toPandas()` 操作'
- en: The `.toPandas()` action, as the name suggests, converts the Spark DataFrame
    into a pandas DataFrame. The same warning needs to be issued here as with the
    `.collect()` action – the `.toPandas()` action collects all the records from all
    the workers, returns them to the driver, and then converts the results into a
    pandas DataFrame.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '`.toPandas()` 操作，顾名思义，将Spark DataFrame转换为pandas DataFrame。与`.collect()` 操作一样，需要在这里发出相同的警告–`.toPandas()`
    操作从所有工作节点收集所有记录，将它们返回给驱动程序，然后将结果转换为pandas DataFrame。'
- en: 'Since our sample data is tiny, we can do this without any problems:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的样本数据很小，我们可以毫无问题地做到这一点：
- en: '[PRE51]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This is what the results look like:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是结果的样子：
- en: '![](img/00077.jpeg)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00077.jpeg)'
- en: See also
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Refer to Spark's documentation for more actions: [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考Spark的文档以获取更多操作：[http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)
