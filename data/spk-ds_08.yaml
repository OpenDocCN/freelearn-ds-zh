- en: Chapter 8.  Analyzing Unstructured Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章：分析非结构化数据
- en: In this Big Data era, the proliferation of unstructured data is overwhelming.
    Numerous methods such as data mining, **Natural Language Processing** (**NLP**),
    information retrieval, and so on, exist for analyzing unstructured data. Due to
    the rapid growth of unstructured data in all kinds of businesses, scalable solutions
    have become the need of the hour. Apache Spark is equipped with out of the box
    algorithms for text analytics, and it also supports custom development of algorithms
    that are not available by default.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据时代，非结构化数据的激增让人不堪重负。存在多种方法（如数据挖掘、**自然语言处理**（**NLP**）、信息检索等）来分析非结构化数据。由于各行各业中非结构化数据的快速增长，具备可扩展性的解决方案已经成为当务之急。Apache
    Spark 配备了现成的文本分析算法，同时也支持自定义开发默认不提供的算法。
- en: In the previous chapter we have shown how SparkR, an R API to Spark for R programmers
    can harness the power of Spark, without learning a new language .  In this chapter,
    we are going to step into a whole new dimension and explore algorithms and techniques
    to extract information out of unstructured data by leveraging Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们展示了如何通过SparkR（一个为R程序员提供的Spark R API）利用Spark的强大功能，而无需学习一门新语言。在这一章中，我们将进入一个全新的维度，探索利用Spark从非结构化数据中提取信息的算法和技术。
- en: 'As a prerequisite for this chapter, a basic understanding of programming in
    Python or Scala and an overall understanding of text analytics and machine learning
    are nice to have. However, we have covered some theoretical basics with the right
    set of practical examples to make those more comprehendible and easy to implement.
    The topics covered in this chapter are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的前提条件，具备Python或Scala编程的基本知识，以及对文本分析和机器学习的整体理解将是非常有帮助的。不过，我们已经通过合适的实践示例覆盖了一些理论基础，使得这些内容更易于理解和实施。本章涵盖的主题包括：
- en: Sources of unstructured data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非结构化数据的来源
- en: Processing unstructured data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理非结构化数据
- en: Count vectorizer
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数向量化
- en: TF-IDF
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Stop-word removal
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词去除
- en: Normalization/scaling
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化/缩放
- en: Word2Vec
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec
- en: n-gram modeling
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: n-gram建模
- en: Text classification
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Naive Bayes classifier
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: Text clustering
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本聚类
- en: K-means
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值算法
- en: Dimensionality reduction
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维
- en: Singular value decomposition
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: Principal component analysis
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Summary
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小结
- en: Sources of unstructured data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非结构化数据的来源
- en: Data analytics has come very far since the spreadsheets and the BI tools in
    the eighties and nineties. Tremendous improvements in computing power, sophisticated
    algorithms, and an open source culture fueled unprecedented growth in data analytics,
    as well as in other fields. These advances in technologies paved the way for new
    opportunities and new challenges. Businesses started looking at generating insights
    from hitherto impossible to handle data sources such as internal memos, emails,
    customer satisfaction surveys, and the like. Data analytics now encompass this
    unstructured, usually text based data along with traditional rows and columns
    of data. Between the highly structured data stored in RDBMS table and completely
    unstructured plain text, we have semi-structured data sources in NoSQL data stores,
    XML or JSON documents, and graph or network data sources. As per current estimates,
    unstructured data forms about 80 percent of enterprise data and is growing rapidly.
    Satellite images, atmospheric data, social networks, blogs and other web pages,
    patient records and physicians' notes, companies' internal communications, and
    so on - all these combined are just a subset of unstructured data sources.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析自八十年代和九十年代的电子表格和BI工具以来，已经取得了长足进展。计算能力的巨大提升、复杂的算法以及开源文化推动了数据分析和其他领域的前所未有的增长。这些技术进步为新的机遇和挑战铺平了道路。企业开始着眼于从以往无法处理的数据源（如内部备忘录、电子邮件、客户满意度调查等）中生成洞察。如今，数据分析不仅仅局限于传统的行列数据，还涵盖了这种非结构化的、通常以文本为基础的数据。存储在关系数据库管理系统（RDBMS）中的高度结构化数据与完全非结构化的纯文本之间，我们有半结构化数据源，如NoSQL数据存储、XML或JSON文档，以及图形或网络数据源。据当前估计，非结构化数据约占企业数据的80%，且正在迅速增长。卫星图像、大气数据、社交网络、博客及其他网页、病历和医生记录、公司内部通讯等——这些只是非结构化数据来源的一部分。
- en: We have already been seeing successful data products that leverage unstructured
    data along with structured data. Some of the companies leverage the power of social
    networks to provide actionable insights to their customers. New fields such as
    **Sentiment Analysis** and **Multimedia Analytics** are emerging to draw insights
    from unstructured data. However, analyzing unstructured data is still a daunting
    feat. For example, contemporary text analytics tools and techniques cannot identify
    sarcasm. However, the potential benefits undoubtedly outweigh the limitations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了成功的数据产品，它们将非结构化数据与结构化数据相结合。一些公司利用社交网络的力量，为客户提供可操作的见解。像**情感分析**和**多媒体分析**这样的新领域正在涌现，以从非结构化数据中提取见解。然而，分析非结构化数据仍然是一项艰巨的任务。例如，现代的文本分析工具和技术无法识别讽刺。然而，潜在的好处无疑大于这些局限性。
- en: Processing unstructured data
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理非结构化数据
- en: 'Unstructured data does not lend itself to most of the programming tasks. It
    has to be processed in various different ways as applicable, to be able to serve
    as an input to any machine learning algorithm or for visual analysis. Broadly,
    the unstructured data analysis can be viewed as a series of steps as shown in
    the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据不适合大多数编程任务。必须根据不同情况以多种方式处理它，才能作为任何机器学习算法的输入或进行可视化分析。大致而言，非结构化数据分析可以视为一系列步骤，如下图所示：
- en: '![Processing unstructured data](img/image_08_001.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![处理非结构化数据](img/image_08_001.jpg)'
- en: Data pre-processing is the most vital step in any unstructured data analysis.
    Fortunately, there have been several proven techniques accumulated over time that
    come in handy. Spark offers most of these techniques out of the box through the
    `ml.features` package. Most of the techniques aim to convert text data to concise
    numerical vectors that can be easily consumed by machine learning algorithms.
    Developers should understand the specific requirements of their organizations
    to arrive at the best pre-processing workflow. Remember that better, relevant
    data is the key to generate better insights.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理是任何非结构化数据分析中最关键的步骤。幸运的是，随着时间的推移，已经积累了多种经过验证的技术，这些技术非常实用。Spark通过`ml.features`包提供了大部分这些技术。大多数技术的目的是将文本数据转换为简洁的数值向量，这些向量可以被机器学习算法轻松处理。开发人员应该理解其组织的具体需求，从而制定最佳的预处理工作流程。请记住，更好、更相关的数据是生成更好见解的关键。
- en: Let us explore a couple of examples that process raw text and convert them into
    data frames. First example takes some text as input and extracts all date-like
    strings whereas the second example extracts tags from twitter text. First example
    is just a warm-up, using a simple, regex (regular expression) tokenizer feature
    transformer without using any spark-specific libraries. It also draws your attention
    to the possibility of misinterpretation. For example, a product code of the form
    1-11-1111 may be interpreted as a date. The second example illustrates a non-trivial,
    multi-step extraction process that resulted in just the required tags. **User
    defined functions** (**udf**) and ML pipelines come in handy in developing such
    multi-step extraction processes. Remaining part of this section describes some
    more handy tools supplied out of box in apache Spark.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索几个处理原始文本并将其转换为数据框的示例。第一个示例将一些文本作为输入，提取所有类似日期的字符串，而第二个示例则从Twitter文本中提取标签。第一个示例只是一个热身，使用一个简单的正则表达式（regex）标记器特征转换器，而没有使用任何Spark特定的库。它还引起你对误解可能性的关注。例如，格式为1-11-1111的产品代码可能被解释为日期。第二个示例展示了一个非平凡的、多步骤的提取过程，最终只提取了所需的标签。**用户定义函数**（**udf**）和机器学习管道在开发这种多步骤提取过程中非常有用。本节的剩余部分介绍了Apache
    Spark中提供的一些其他方便的工具。
- en: '**Example-1:** **Extract date like strings from text**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例-1:** **从文本中提取类似日期的字符串**'
- en: '**Scala:**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Python:**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding example defined a regular expression pattern to recognize date
    strings. The regex pattern and the sample text DataFrame are passed to the `RegexTokenizer`
    to extract matching, date like strings. The `gaps=False` option picks matching
    strings and a value of `False` would use the given pattern as a separator. Note
    that `1-21-1111`, which is obviously not a date, is also selected.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例定义了一个正则表达式模式来识别日期字符串。正则表达式模式和示例文本数据框被传递到`RegexTokenizer`中以提取匹配的类似日期的字符串。`gaps=False`选项选择匹配的字符串，`False`值会将给定的模式用作分隔符。注意，`1-21-1111`，显然不是日期，也被选中。
- en: Next example extracts tags from twitter text and identifies most popular tags.
    You can use the same approach to collect hash (`#`) tags too.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例从Twitter文本中提取标签，并识别最流行的标签。你也可以使用相同的方法收集哈希（`#`）标签。
- en: This example uses a built in function `explode`, which converts a single row
    with an array of values into multiple rows, one value per array element.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例使用了内建函数`explode`，它将一个包含数组值的单行数据转换为多行，每行包含一个数组元素的值。
- en: "**Example-2: Extract tags from twitter \"\x80\x9Ctext\"\x80\x9D**"
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例-2：从Twitter文本中提取标签**'
- en: '**Scala:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Python**:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Count vectorizer
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计数向量化
- en: Count vectorizer extracts vocabulary (tokens) from documents and generates a
    `CountVectorizerModel` model when a dictionary is not available priori. As the
    name indicates, a text document is converted into a vector of tokens and counts.
    The model produces a sparse representation of the documents over the vocabulary.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 计数向量化从文档中提取词汇（词元），并在没有预定义字典的情况下生成`CountVectorizerModel`模型。顾名思义，文本文档被转换为包含词元和计数的向量。该模型产生词汇表上文档的稀疏表示。
- en: You can fine tune the behavior to limit the vocabulary size, minimum token count,
    and much more as applicable in your business case.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据业务需求精细调整行为，限制词汇表大小、最小词元计数等。
- en: '//Example 3: Count Vectorizer example'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: //示例3：计数向量化示例
- en: '**Scala**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Python**:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Input**:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Output**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding example demonstrates how `CountVectorizer` works as an estimator
    to extract the vocabulary and generate a `CountVectorizerModel`. Note that the
    features vector order corresponds to vocabulary and not the input sequence. Let's
    also look at how the same can be achieved by building a dictionary a-priori. However,
    keep in mind that they have their own use cases.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例演示了`CountVectorizer`如何作为估计器提取词汇并生成`CountVectorizerModel`模型。请注意，特征向量的顺序对应于词汇表而非输入序列。我们还可以看看如何通过预先构建字典来实现相同的功能。然而，请记住，它们各自有不同的使用场景。
- en: 'Example 4: define CountVectorizerModel with a-priori vocabulary'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 示例4：使用预先定义的词汇表定义CountVectorizerModel
- en: '**Scala:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Python**:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: Not available as of Spark 2.0.0
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0.0版本中不可用
- en: TF-IDF
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: The **Term Frequency-Inverse Document Frequency** (**TF-IDF**) is perhaps one
    of the most popular measures in text analytics. This metric indicates the importance
    of a given term in a given document within a set of documents. This consists two
    measurements, **Term Frequency** (**TF**) and **Inverse Document Frequency** (**IDF**).
    Let us discuss them one by one and then see their combined effect.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频-逆文档频率**（**TF-IDF**）可能是文本分析中最常用的度量之一。该度量表示某一术语在一组文档中的重要性。它由两个度量组成，**词频**（**TF**）和**逆文档频率**（**IDF**）。让我们逐一讨论它们，然后看看它们的结合效果。'
- en: TF is a measure of the relative importance of a term in a document, which is
    usually the frequency of that term divided by the number of terms in that document.
    Consider a text document containing 100 words wherein the word *apple* appears
    eight times. The TF for *apple* would be *TF = (8 / 100) = 0.08*. So, the more
    frequently a term occurs in a document, the larger is its TF coefficient.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TF是衡量术语在文档中相对重要性的指标，通常是该术语在文档中出现的频率除以文档中的词数。假设一个文本文档包含100个单词，其中词*apple*出现了8次。则*apple*的TF为*TF
    = (8 / 100) = 0.08*。因此，术语在文档中出现的频率越高，它的TF系数越大。
- en: IDF is a measure of the importance of a particular term in the entire collection
    of documents, that is, how infrequently the word occurs across all the documents.
    The importance of a term is inversely proportional to its frequency. Spark provides
    two separate methods to perform these tasks. Assume we have 6 million documents
    and the word *apple* appears in 6000 of these. Then, IDF is calculated as *IDF
    = Log(6,000,000 / 6,000) = 3*. If you observe this carefully, the lower the denominator,
    the higher is the IDF value. This means that the fewer the number of documents
    containing a particular word, the higher would be its importance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: IDF是衡量特定术语在整个文档集合中重要性的指标，即该词在所有文档中出现的频率。术语的重要性与其出现频率成反比。Spark提供了两种不同的方法来执行这些任务。假设我们有600万个文档，词*apple*出现在其中6000个文档中。那么，IDF可以计算为*IDF
    = Log(6,000,000 / 6,000) = 3*。仔细观察可以发现，分母越小，IDF值越高。这意味着包含特定词汇的文档越少，它的重要性越高。
- en: Thus, the TF-IDF score would be *TF * IDF = 0.08 * 3 = 0.24*. Note that it would
    penalize the words that are more frequent across documents and less important,
    such as *the*, *this*, *a*, and so on, and give more weight to the ones that are
    important.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，TF-IDF 分数将是*TF * IDF = 0.08 * 3 = 0.24*。请注意，它会对那些在文档中出现频率较高但不太重要的单词（如 *the*、*this*、*a*
    等）进行惩罚，而给那些重要的单词赋予更高的权重。
- en: In Spark, TF is implemented as HashingTF. It takes a sequence of terms (often
    the output of a tokenizer) and produces a fixed length features vector. It performs
    feature hashing to convert the terms into fixed length indices. IDF then takes
    that features vector (the output of HashingTF) as input and scales it based on
    the term frequency in the set of documents. The previous chapter has an example
    of this transformation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，TF 实现为 HashingTF。它接受一个术语序列（通常是分词器的输出），并生成一个固定长度的特征向量。它通过特征哈希将术语转换为固定长度的索引。然后，IDF
    会将该特征向量（HashingTF 的输出）作为输入，并根据文档集中的术语频率对其进行缩放。上一章有这个转换的示例。
- en: Stop-word removal
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停用词移除
- en: Common words such as *is*, *was*, and *the* are called stop-words. They do not
    usually add value to analysis and should be dropped during the data preparation
    step. Spark provides `StopWordsRemover` transformer, which does just that. It
    takes a sequence of tokens as a series of string inputs, such as the output of
    a tokenizer, and removes all the stop words. Spark has a stop-words list by default
    that you may override by providing your own stop-words list as a parameter. You
    may optionally turn on `caseSensitive` match which is off by default.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的单词，如 *is*、*was* 和 *the*，被称为停用词。它们通常不会为分析增加价值，应在数据准备步骤中删除。Spark 提供了 `StopWordsRemover`
    转换器，专门做这件事。它接受一系列字符串输入的标记（如分词器的输出），并移除所有停用词。Spark 默认提供一个停用词列表，你可以通过提供自己的停用词列表来覆盖它。你还可以选择启用
    `caseSensitive` 匹配，默认情况下该选项为关闭状态。
- en: 'Example 5: Stopword Remover'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 5：停用词移除器
- en: '**Scala:**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Python:**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python：**'
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Assume that we have the following DataFrame with columns `id` and `raw_text`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有如下的 DataFrame，其中包含 `id` 和 `raw_text` 列：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After applying `StopWordsRemover` with `raw_text` as the input column and `processed_text`
    as the output column for the preceding example, we should get the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用 `StopWordsRemover` 时，以 `raw_text` 作为输入列，`processed_text` 作为输出列，针对上述示例，我们应该得到以下输出：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Normalization/scaling
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归一化/缩放
- en: Normalization is a common and preliminary step in data preparation. Most of
    the machine learning algorithms work better when all features are on the same
    scale. For example, if there are two features where the value of one is about
    100 times greater than the other, bringing them to the same scale reflects meaningful
    relative activity between the two variables. Any non-numeric values, such as high,
    medium, and low, should ideally be converted to appropriate numerical quantification
    as a best practice. However, you need to be careful in doing so as it may require
    domain expertise. For example, if you assign 3, 2, and 1 for high, medium, and
    low respectively, then it should be checked that these three units are equidistant
    from each other.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化是数据准备中的常见和初步步骤。当所有特征处于相同尺度时，大多数机器学习算法效果更好。例如，如果有两个特征，其中一个值比另一个大约高出 100 倍，将它们调整到相同的尺度可以反映这两个变量之间有意义的相对活动。任何非数值类型的值，如高、中、低，理想情况下应该转换为适当的数值量化，这是最佳实践。然而，在进行转换时需要小心，因为这可能需要领域专业知识。例如，如果你为高、中、低分别分配
    3、2 和 1，那么应该检查这三个单位是否等距。
- en: The common methods of feature normalization are *scaling*, *mean subtraction*,
    and *feature standardization*, just to name a few. In scaling, each numerical
    feature vector is rescaled such that its value range is between *-1* to *+1* or
    *0* to *1* or something similar. In mean subtraction, you compute mean of a numerical
    feature vector and subtract that mean from each of the values. We are interested
    in the relative deflection from the mean, while the absolute value could be immaterial.
    Feature standardization refers to setting the data to zero mean and unit (1) variance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 特征归一化的常见方法有*缩放*、*均值减法*和*特征标准化*，这里只列举几个。缩放中，每个数值特征向量都会被重新缩放，使其值的范围介于*-1* 到 *+1*
    或 *0* 到 *1* 之间，或类似的范围。在均值减法中，你计算一个数值特征向量的均值，并从每个值中减去这个均值。我们关注的是相对于均值的偏差，而绝对值可能不重要。特征标准化指的是将数据设置为零均值和单位（1）方差。
- en: Spark provides a `Normalizer` feature transformer to normalize each vector to
    have unit norm; `StandardScaler` to have unit norm and zero mean; and `MinMaxScaler`
    to rescale each feature to a specific range of values. By default, min and max
    are 0 and 1 but you may set the value parameters yourself as per the data requirement.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了一个 `Normalizer` 特征转换器，用于将每个向量规范化为单位范数；`StandardScaler` 用于规范化为单位范数且均值为零；`MinMaxScaler`
    用于将每个特征缩放到特定的值范围。默认情况下，最小值和最大值为 0 和 1，但你可以根据数据需求自行设置值参数。
- en: Word2Vec
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec
- en: The Word2Vec is a type of PCA (you will find out more about this shortly) that
    takes a sequence of words and produces a map (of string, vector). The string is
    the word and the vector is a unique fixed size vector. The resulting word vector
    representation is useful in many machine learning and NLP applications, such as
    named entity recognition and tagging. Let us look at an example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 是一种主成分分析（PCA）（稍后你会更多了解）方法，它接收一个单词序列并生成一个映射（字符串，向量）。字符串是单词，向量是唯一的固定大小向量。生成的单词向量表示在许多机器学习和自然语言处理应用中非常有用，例如命名实体识别和标注。让我们来看一个示例。
- en: '**Example 6: Word2Vec**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 6：Word2Vec**'
- en: '**Scala**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Python:**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python：**'
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: n-gram modelling
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n-gram 建模
- en: 'An n-gram is a contiguous sequence of *n* items from a given sequence of text
    or speech. An n-gram of size *1* is referred to as a *unigram*, size *2* is a
    *bigram*, and size *3* is a *trigram*. Alternatively, they can be referred to
    by the value of *n*, for example, four-gram, five-gram, and so on. Let us take
    a look at an example to understand the possible outcomes of this model:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram 是一个由给定文本或语音序列中的 *n* 项连续组成的序列。大小为 *1* 的 n-gram 称为 *unigram*，大小为 *2* 的称为
    *bigram*，大小为 *3* 的称为 *trigram*。或者，它们也可以按 *n* 的值进行命名，例如四元组、五元组，依此类推。让我们看一个示例，以了解该模型可能的输出：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is an example of words to n-gram letters. The same is the case for sentence
    (or tokenized words) to n-gram words. For example, the 2-gram equivalent of the
    sentence *Kids love to eat chocolates* is:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个将单词转换为 n-gram 字母的示例。同样的情况也适用于将句子（或分词后的单词）转换为 n-gram 单词。例如，句子 *孩子们喜欢吃巧克力*
    的 2-gram 等效形式是：
- en: '''Kids love'', ''love to'', ''to eat'', ''eat chocolates''.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '''孩子们喜欢''，''喜欢''，''吃''，''吃巧克力''。'
- en: There are various applications of n-gram modelling in text mining and NLP. One
    of the examples is predicting the probability of each word occurring given a prior
    context (conditional probability).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram 建模在文本挖掘和自然语言处理中的应用非常广泛。一个例子是根据先前的上下文预测每个单词出现的概率（条件概率）。
- en: In Spark, `NGram` is a feature transformer that converts the input array (for
    example, the output of a Tokenizer) of strings into an array of n-grams. Null
    values in the input array are ignored by default. It returns an array of n-grams
    where each n-gram is represented by a space-separated string of words.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，`NGram` 是一个特征转换器，它将输入数组（例如，Tokenizer 的输出）中的字符串转换为一个 n-gram 数组。默认情况下，输入数组中的空值会被忽略。它返回一个由
    n-gram 组成的数组，其中每个 n-gram 是由空格分隔的单词字符串表示的。
- en: '**Example 7: NGram**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 7：NGram**'
- en: '**Scala**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Python:**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python：**'
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Text classification
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类
- en: Text classification is about assigning a topic, subject category, genre, or
    something similar to the text blob. For example, spam filters assign spam or not
    spam to an email.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是将一个主题、学科类别、类型或类似内容分配给文本块。例如，垃圾邮件过滤器会将邮件标记为垃圾邮件或非垃圾邮件。
- en: Apache Spark supports various classifiers through MLlib and ML packages. The
    SVM classifier and Naive Bayes classifier are popular classifiers, and the former
    was already covered in the previous chapter. Let's take a look at the latter now.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 通过 MLlib 和 ML 包支持各种分类器。SVM 分类器和朴素贝叶斯分类器是常用的分类器，前者已在前一章中讲解过。现在让我们来看后者。
- en: Naive Bayes classifier
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: The **Naive Bayes** (**NB**) classifier is a multiclass probabilistic classifier
    and is one of the best classification algorithms. It assumes strong independence
    between every pair of features. It computes the conditional probability distribution
    of each feature and a given label, and then applies Bayes' theorem to compute
    the conditional probability of a label given an observation. In terms of document
    classification, an observation is a document to be classified into some class.
    Despite its strong assumptions on data, it is quite popular. It works with small
    amount of training data - whether real or discrete. It works very efficiently
    because it takes a single pass through the training data; one constraint is that
    the feature vectors must be non-negative. By default, ML package supports multinomial
    NB. However, you may set the parameter `modelType` to `Bernoulli` if bernoulli
    NB is required.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯** (**NB**) 分类器是一种多类别的概率分类器，是最好的分类算法之一。它假设每对特征之间具有强独立性。它计算每个特征和给定标签的条件概率分布，然后应用贝叶斯定理计算给定观察值下标签的条件概率。在文档分类中，一个观察值就是待分类的文档。尽管它对数据有较强的假设，它仍然非常流行。它适用于少量训练数据——无论是真实数据还是离散数据。它工作非常高效，因为它只需要通过训练数据进行一次遍历；唯一的限制是特征向量必须是非负的。默认情况下，机器学习包支持多项式朴素贝叶斯。然而，如果需要伯努利朴素贝叶斯，可以将参数
    `modelType` 设置为 `Bernoulli`。'
- en: The **laplace smoothing** technique may be applied by specifying the smoothing
    parameters and is extremely useful in situations where you want to assign a small
    non-zero probability to a rare word or new word so that the posterior probabilities
    do not suddenly drop to zero.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**拉普拉斯平滑** 技术可以通过指定平滑参数来应用，在需要为稀有词或新词分配一个小的非零概率的情况下，它非常有用，以避免后验概率突然降至零。'
- en: 'Spark also provides some other hyper parameters such as `thresholds` also to
    gain fine grain control. Here is an example that categorizes twitter text. This
    example contains some hand-coded rules that assign a category to the train data.
    A particular category is assigned if any of the corresponding words are found
    in the text. For example, the category is "survey" if text contains "survey" or
    "poll". The model is trained based on this train data and evaluated on a different
    text sample collected at a different time:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 还提供了一些其他的超参数，如 `thresholds`，以便获得更细粒度的控制。以下是一个分类推特文本的示例。该示例包含一些手工编写的规则，用于为训练数据分配类别。如果文本中包含对应的词语，则会分配特定类别。例如，如果文本包含“survey”或“poll”，则类别为“survey”。模型基于这些训练数据进行训练，并在不同时间收集的不同文本样本上进行评估：
- en: '**Example 8: Naive Bayes**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 8：朴素贝叶斯**'
- en: '**Scala:**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Python:**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python：**'
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Once this is done, a model can be trained with the output of this step, which
    can classify a text blob or file.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此步骤后，可以使用该步骤的输出训练一个模型，该模型可以对文本块或文件进行分类。
- en: Text clustering
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本聚类
- en: Clustering is an unsupervised learning technique. Intuitively, clustering groups
    objects into disjoint sets. We do not know how many groups exist in the data,
    or what might be the commonality within these groups (clusters).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督学习技术。直观地说，聚类将对象分组到不相交的集合中。我们不知道数据中有多少组，也不知道这些组（簇）之间可能有什么共同之处。
- en: Text clustering has several applications. For example, an organizational entity
    may want to organize its internal documents into similar clusters based on some
    similarity measure. The notion of similarity or distance is central to the clustering
    process. Common measures used are TF-IDF and cosine similarity. Cosine similarity,
    or the cosine distance, is the cos product of the word frequency vectors of two
    documents. Spark provides a variety of clustering algorithms that can be effectively used
    in text analytics.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 文本聚类有多种应用。例如，一个组织实体可能希望根据某种相似性度量将其内部文档组织成相似的簇。相似性或距离的概念是聚类过程的核心。常用的度量方法有 TF-IDF
    和余弦相似度。余弦相似度或余弦距离是两个文档的词频向量的余弦乘积。Spark 提供了多种聚类算法，可在文本分析中有效使用。
- en: K-means
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means
- en: Perhaps K-means is the most intuitive of all the clustering algorithms. The
    idea is to segregate data points as *K* different clusters based on some similarity
    measure, say cosine distance or Euclidean distance. This algorithm that starts
    with *K* random single point clusters, and each of the remaining data points are
    assigned to nearest cluster. Then cluster centers are recomputed and the algorithm
    loops through the data points once again. This process continues iteratively until
    there are no re-assignments or when pre-defined iteration count is reached.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 也许 K-means 是所有聚类算法中最直观的一种。其思路是根据某些相似性度量方法（如余弦距离或欧几里得距离）将数据点划分为 *K* 个不同的簇。该算法从
    *K* 个随机的单点簇开始，然后将每个剩余的数据点分配到最近的簇中。接着重新计算簇的中心，并再次遍历数据点。这个过程会反复进行，直到没有重新分配数据点，或达到预定义的迭代次数。
- en: How to fix the number of clusters (*K*) is not obvious. Identifying the initial
    cluster centers is also not obvious. Sometimes the business requirement may dictate
    the number of clusters; for example, partition all existing documents into 10
    different sections. But in most of the real world scenarios, we need to find *K*
    through trial and error. One way is to progressively increase the *K* value and
    compute the cluster quality, such as cluster variance. The quality ceases to improve
    significantly beyond a certain value of *K,* which could be your ideal *K*. There
    are various other techniques, such as the elbow method, **Akaike information criterion**
    (**AIC**), and **Bayesian information criterion** (**BIC**).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如何确定簇的数量（*K*）并不是显而易见的。确定初始簇中心也不是显而易见的。有时业务需求可能会决定簇的数量；例如，将所有现有文档划分为 10 个不同的部分。但在大多数实际场景中，我们需要通过反复试验来确定
    *K*。一种方法是逐步增加 *K* 值并计算簇的质量，例如簇的方差。当 *K* 值超过某个特定值时，簇的质量不会显著提升，这个 *K* 值可能就是理想的 *K*。还有各种其他技术，如肘部法则、**赤池信息量准则**（**AIC**）和
    **贝叶斯信息量准则**（**BIC**）。
- en: Likewise, start with different starting points until the cluster quality is
    satisfactory. Then you may wish to validate your result using techniques such
    as Silhouette Score. However, these activities are computationally intensive.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，使用不同的起始点，直到簇的质量令人满意为止。然后你可能希望使用如轮廓系数等技术来验证结果。然而，这些活动计算量很大。
- en: Spark provides K-means from MLlib as well as ml packages. You may specify maximum
    iterations or convergence tolerance to fine tune algorithm performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了来自 MLlib 和 ml 包的 K-means。你可以指定最大迭代次数或收敛容忍度来优化算法的性能。
- en: Dimensionality reduction
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Imagine a large matrix with many rows and columns. In many matrix applications,
    this large matrix can be represented by some narrow matrices with small number
    of rows and columns that still represents the original matrix. Then processing
    this smaller matrix may yield similar results as that of the original matrix.
    This can be computationally efficient.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个拥有许多行和列的大矩阵。在许多矩阵应用中，这个大矩阵可以通过一些行列较少的狭窄矩阵来表示，而这些矩阵仍能代表原始矩阵。然后，处理这个较小的矩阵可能会产生与原始矩阵相似的结果。这种方法在计算上可能更高效。
- en: Dimensionality reduction is about finding that small matrix. MLLib supports
    two algorithms, SVD and PCA for dimensionality reduction on RowMatrix class. Both
    of these  algorithms allow us to specify the number of dimensions we are interested
    in retaining. Let us look at example first and then delve into the underlying
    theory .
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是关于寻找那个小矩阵的。MLlib 支持两种算法：SVD 和 PCA，用于 RowMatrix 类的降维。这两种算法都允许我们指定感兴趣的维度数量。让我们先看一个例子，然后深入探讨其中的理论。
- en: '**Example 9: Dimensionality reduction**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 9：降维**'
- en: '**Scala:**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Python:**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python：**'
- en: Not available in Python as of Spark 2.0.0
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 2.0.0 版本中，Python 不支持此功能。
- en: Singular Value Decomposition
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: The **Singular Value Decomposition** (**SVD**) is one of the centerpieces of
    linear algebra and is widely used for many real-world modeling requirements. It
    provides a convenient way of breaking a matrix into simpler, smaller matrices.
    This leads to a low-dimensional representation of a high-dimensional matrix. It
    helps us eliminate less important parts of the matrix to produce an approximate
    representation. This technique is useful in dimensionality reduction and data
    compression.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）是线性代数的核心内容之一，广泛应用于许多实际的建模需求。它提供了一种将矩阵分解成更简单、更小的矩阵的便捷方法。这导致了高维矩阵的低维表示。它帮助我们消除矩阵中不重要的部分，从而生成一个近似的表示。此技术在降维和数据压缩中非常有用。'
- en: Let *M* be a matrix of size m-rows and n-columns. The rank of a matrix is the
    number of rows that are linearly independent. A row is considered independent
    if it has at least one non-zero element and it is not a linear combination of
    one or more rows. The same rank will be obtained if we considered columns instead
    of rows - as in linear algebra.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *M* 为一个大小为m行n列的矩阵。矩阵的秩是指矩阵中线性无关的行数。如果一行包含至少一个非零元素，并且它不是一行或多行的线性组合，那么该行被认为是独立的。如果我们考虑列而非行（如线性代数中的定义），则会得到相同的秩。
- en: 'If the elements of one row are the sum of two rows, then that row is not independent.
    Then as a result of SVD, we find three matrices, *U*, *∑*, and *V* that satisfy
    the following equation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一行的元素是两行的和，那么该行不是独立的。随后，通过SVD分解，我们得到三个矩阵 *U*、*∑* 和 *V*，它们满足以下方程：
- en: '*M = U∑VT*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*M = U∑VT*'
- en: 'These three matrices have the following properties:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个矩阵具有以下特性：
- en: '**U**: This is a column-orthonormal matrix with m rows and r columns. An orthonormal
    matrix implies that each of the columns is a unit vector and the pairwise dot
    product between any two columns is 0.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**U**：这是一个具有m行和r列的列正交规范矩阵。正交规范矩阵意味着每一列都是单位向量，且任意两列的点积为0。'
- en: '**V**: This is a column-orthonormal matrix with *n* rows and *r* columns.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V**：这是一个具有 *n* 行和 *r* 列的列正交规范矩阵。'
- en: '**∑**: This is an *r* x *r* diagonal matrix with non-negative real numbers
    as principal diagonal values in descending order. In a diagonal matrix, all elements
    except the ones on the principal diagonal are zero.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**∑**：这是一个 *r* x *r* 的对角矩阵，主对角线上的值是按降序排列的非负实数。在对角矩阵中，除了主对角线上的元素外，其它元素都为零。'
- en: 'The principal diagonal values in the *∑* matrix are called singular values.
    They are considered as the underlying *concepts* or *components* that connect
    the rows and columns of the matrix. Their magnitude represents the strength of
    the corresponding components. For example, imagine that the matrix in the previous
    example contains ratings of five books by six readers. SVD allows us to split
    them into three matrices: *∑* containing the singular values representing the
    *strength* of underlying topics; *U* connecting people to concepts; and *V* connecting
    concepts to books.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*∑* 矩阵中的主对角线值称为奇异值。它们被视为连接矩阵行和列的基础*概念*或*成分*。它们的大小表示对应成分的强度。例如，假设前面提到的矩阵包含六个读者对五本书的评分。SVD
    可以将其分解为三个矩阵：*∑* 包含代表基础主题强度的奇异值；*U* 连接人和概念；*V* 连接概念和书籍。'
- en: In a large matrix, we can replace the lower magnitude singular values to zero
    and thereby reduce the corresponding rows in the remaining two matrices. Note
    that if we re-compute the matrix product on the right hand side and compare the
    value with the original matrix on the left hand side, they will be almost similar.
    We can use this technique to retain the desired number of dimensions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在大矩阵中，我们可以将较小的奇异值替换为零，从而减少剩余两个矩阵中相应行的维度。注意，如果我们重新计算右边的矩阵乘积并将其与左边的原矩阵进行比较，它们将几乎相同。我们可以使用这种方法来保留所需的维度数。
- en: Principal Component Analysis
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析
- en: "**Principal Component Analysis** (**PCA**) is a technique that takes n-dimensional\
    \ data points and project onto a smaller (fewer dimensions) subspace with minimum\
    \ loss of information. A set of data points in a high dimensional space find the\
    \ directions along which these tuples line up best. In other words, we need to\
    \ find a rotation such that the first coordinate has the largest variance possible,\
    \ and each succeeding coordinate in turn has the largest variance possible. The\
    \ idea is to treat the set of tuples as a matrix *M* and fi\x81nd the eigenvectors\
    \ for MMT."
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析** (**PCA**) 是一种将n维数据点投影到一个较小（维度更少）的子空间的技术，同时尽量减少信息的丢失。高维空间中的一组数据点会找到使这些数据点排列最佳的方向。换句话说，我们需要找到一种旋转方式，使得第一个坐标具有可能的最大方差，每个后续坐标依次具有最大的方差。这个思路是将数据集作为一个矩阵
    *M*，并找到 *MMT* 的特征向量。'
- en: If *A* is a square matrix, *e* is a column matrix with the same number of rows
    as *A*, and *λ* is a constant such that *Me = λe*, then *e* is called the eigenvector
    of *M* and *λ* is called the eigenvalue of *M*. In terms of n-dimensional plane,
    the eigenvector is the direction and the eigenvalue is a measure of variance along
    that direction. We can drop the dimensions with a low eigenvalue, thereby finding
    a smaller subspace without loss of information.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果* A *是一个方阵，* e *是一个列矩阵，行数与* A *相同，且* λ *是一个常数，使得* Me = λe *，那么* e *被称为* M
    *的特征向量，* λ *被称为* M *的特征值。在n维平面上，特征向量是方向，特征值是沿该方向的方差度量。我们可以丢弃特征值较低的维度，从而找到一个较小的子空间而不会丢失信息。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we examined the sources of unstructured data and the motivation
    behind analyzing the unstructured data. We explained various techniques that are
    required in pre-processing unstructured data and how Spark provides most of these
    tools out of the box. We also covered some of the algorithms supported by Spark
    that can be used in text analytics.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了非结构化数据的来源以及分析非结构化数据背后的动机。我们解释了在预处理非结构化数据时需要的各种技术，以及Spark如何提供大部分这些工具。我们还介绍了Spark支持的一些可以用于文本分析的算法。
- en: In the next chapter, we will go through different types of visualization techniques
    that are insightful in different stages of data analytics lifecycle.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍不同类型的可视化技术，这些技术在数据分析生命周期的不同阶段提供了深刻的见解。
- en: 'References:'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: 'The following are the references:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是参考文献：
- en: '[http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf](http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf](http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf)'
- en: '[https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf](https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf](https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf)'
- en: '[https://web.stanford.edu/class/cs124/lec/naivebayes.pdf](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://web.stanford.edu/class/cs124/lec/naivebayes.pdf](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)'
- en: '[http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)'
- en: '[http://www.mmds.org/](http://www.mmds.org/)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.mmds.org/](http://www.mmds.org/)'
- en: '[http://sebastianraschka.com/Articles/2014_pca_step_by_step.html](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://sebastianraschka.com/Articles/2014_pca_step_by_step.html](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)'
- en: '[http://arxiv.org/pdf/1404.1100.pdf](http://arxiv.org/pdf/1404.1100.pdf)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://arxiv.org/pdf/1404.1100.pdf](http://arxiv.org/pdf/1404.1100.pdf)'
- en: '[http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html)'
- en: 'Count Vectorizer:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 计数向量化：
- en: '[https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html)'
- en: 'n-gram modeling:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram建模：
- en: '[https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)'
