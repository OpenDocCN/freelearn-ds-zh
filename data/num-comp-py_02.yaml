- en: Tree-Based Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于树的机器学习模型
- en: 'The goal of tree-based methods is to segment the feature space into a number
    of simple rectangular regions, to subsequently make a prediction for a given observation
    based on either mean or mode (mean for regression and mode for classification,
    to be precise) of the training observations in the region to which it belongs.
    Unlike most other classifiers, models produced by decision trees are easy to interpret.
    In this chapter, we will be covering the following decision tree-based models
    on HR data examples for predicting whether a given employee will leave the organization
    in the near future or not. In this chapter, we will learn the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的方法的目标是将特征空间划分为多个简单的矩形区域，从而根据所属区域的训练样本的均值或众数（回归问题使用均值，分类问题使用众数）对给定的观测值进行预测。与大多数其他分类器不同，决策树生成的模型易于解释。在本章中，我们将通过人力资源数据的示例，介绍以下基于决策树的模型，用于预测某个员工是否会在近期离开公司。本章将学习以下内容：
- en: Decision trees - simple model and model with class weight tuning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树 - 简单模型及带类别权重调优的模型
- en: Bagging (bootstrap aggregation)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自助聚合（Bagging）
- en: Random Forest - basic random forest and application of grid search on hyperparameter
    tuning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林 - 基础随机森林及超参数调优的网格搜索应用
- en: Boosting (AdaBoost, gradient boost, extreme gradient boost - XGBoost)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升方法（AdaBoost、梯度提升、极端梯度提升 - XGBoost）
- en: Ensemble of ensembles (with heterogeneous and homogeneous models)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成的集成（包括异质和同质模型）
- en: Introducing decision tree classifiers
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍决策树分类器
- en: Decision tree classifiers produce rules in simple English sentences, which can
    easily be interpreted and presented to senior management without any editing.
    Decision trees can be applied to either classification or regression problems.
    Based on features in data, decision tree models learn a series of questions to
    infer the class labels of samples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器生成的规则是简单的英语句子，易于解释，并可直接呈现给高级管理层，无需编辑。决策树可以应用于分类或回归问题。根据数据中的特征，决策树模型学习一系列问题，以推断样本的类别标签。
- en: In the following figure, simple recursive decision rules have been asked by
    a programmer himself to do relevant actions. The actions would be based on the
    provided answers for each question, whether yes or no.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，程序员自己设计了简单的递归决策规则，以根据每个问题的回答（是或否）执行相应的操作。
- en: '![](img/0e2511d1-0a30-4f0b-83cb-9b5822697197.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e2511d1-0a30-4f0b-83cb-9b5822697197.png)'
- en: Terminology used in decision trees
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树中使用的术语
- en: 'Decision Trees do not have much machinery as compared with logistic regression.
    Here we have a few metrics to study. We will majorly focus on impurity measures;
    decision trees split variables recursively based on set impurity criteria until
    they reach some stopping criteria (minimum observations per terminal node, minimum
    observations for split at any node, and so on):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑回归相比，决策树的复杂度较低。这里有一些指标需要研究。我们将主要关注杂乱度度量；决策树基于设定的杂乱度标准递归地划分变量，直到达到某些停止标准（每个终端节点的最小观测数、任意节点的最小划分观测数等）。
- en: '**Entropy:** Entropy came from information theory and is the measure of impurity
    in data. If the sample is completely homogeneous, the entropy is zero, and if
    the sample is equally divided, it has entropy of one. In decision trees, the predictor
    with most heterogeneousness will be considered nearest to the root node to classify
    the given data into classes in a greedy mode. We will cover this topic in more
    depth in this chapter:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**熵：** 熵来源于信息论，是数据杂乱度的度量。如果样本完全同质，则熵为零；如果样本均匀分布，则熵为一。在决策树中，具有最大异质性的预测变量会被视为最接近根节点的特征，用贪婪方式将给定数据分类。我们将在本章中更深入地讨论这一主题：'
- en: '![](img/c53d01d9-81b5-4d64-8d55-96510a38f5b3.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c53d01d9-81b5-4d64-8d55-96510a38f5b3.jpg)'
- en: Where n = number of classes. Entropy is maximum in the middle, with a value
    of *1* and minimum at the extremes with a value of *0*. The low value of entropy
    is desirable, as it will segregate classes better.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其中n = 类别数。熵在中间最大，值为*1*，在极端情况下最小，值为*0*。低熵值是理想的，因为它能够更好地区分类别。
- en: '**Information Gain:** Information gain is the expected reduction in entropy
    caused by partitioning the examples according to a given attribute. The idea is
    to start with mixed classes and to continue partitioning until each node reaches
    its observations of purest class. At every stage, the variable with maximum information
    gain is chosen in a greedy fashion.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息增益：** 信息增益是根据给定属性对示例进行分区所导致的熵减少的期望值。这个想法是从混合类开始，并继续分区，直到每个节点达到其最纯净类别的观察值。在每个阶段，以贪婪的方式选择具有最大信息增益的变量。'
- en: '*Information Gain = Entropy of Parent - sum (weighted % * Entropy of Child)*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*信息增益 = 父节点熵 - 总和（加权% * 子节点熵）*'
- en: '*Weighted % = Number of observations in particular child/sum (observations
    in all child nodes)*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*加权% = 特定子节点中的观察次数/所有子节点中的观察次数之和*'
- en: '**Gini:** Gini impurity is a measure of misclassification, which applies in
    a multi-class classifier context. Gini works similar to entropy, except Gini is
    quicker to calculate:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基尼：** 基尼不纯度是一种多类分类器上的误分类度量。基尼的工作方式类似于熵，只是基尼的计算速度更快：'
- en: '![](img/17db81ae-57f3-4898-9b29-6b92262d8793.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17db81ae-57f3-4898-9b29-6b92262d8793.jpg)'
- en: 'Where *i = Number of classes*. The similarity between Gini and entropy is shown
    in the following figure:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *i = 类别数*。Gini和熵之间的相似性如下图所示：
- en: '![](img/65f8a2ff-dbcb-45b3-9f21-0f6fc05025bf.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65f8a2ff-dbcb-45b3-9f21-0f6fc05025bf.png)'
- en: Decision tree working methodology from first principles
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树的工作方法从第一原则开始
- en: 'In the following example, the response variable has only two classes: whether
    to play tennis or not. But the following table has been compiled based on various
    conditions recorded on various days. Now, our task is to find out which output
    the variables are resulting in most significantly: YES or NO.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，响应变量只有两类：是否打网球。但是下表是基于记录在不同天气条件下的各种情况编制的。现在，我们的任务是找出哪些输出变量最显著地导致了：是或否。
- en: 'This example comes under the Classification tree:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该示例属于分类树：
- en: '| **Day** | **Outlook** | **Temperature** | **Humidity** | **Wind** | **Play
    tennis** |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **日期** | **外观** | **温度** | **湿度** | **风** | **打网球** |'
- en: '| D1 | Sunny | Hot | High | Weak | No |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| D1 | 晴天 | 炎热 | 高 | 弱 | 否 |'
- en: '| D2 | Sunny | Hot | High | Strong | No |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| D2 | 晴天 | 炎热 | 高 | 强 | 否 |'
- en: '| D3 | Overcast | Hot | High | Weak | Yes |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| D3 | 阴天 | 炎热 | 高 | 弱 | 是 |'
- en: '| D4 | Rain | Mild | High | Weak | Yes |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| D4 | 下雨 | 温和 | 高 | 弱 | 是 |'
- en: '| D5 | Rain | Cool | Normal | Weak | Yes |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| D5 | 下雨 | 凉爽 | 正常 | 弱 | 是 |'
- en: '| D6 | Rain | Cool | Normal | Strong | No |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| D6 | 下雨 | 凉爽 | 正常 | 强 | 否 |'
- en: '| D7 | Overcast | Cool | Normal | Strong | Yes |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| D7 | 阴天 | 凉爽 | 正常 | 强 | 是 |'
- en: '| D8 | Sunny | Mild | High | Weak | No |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| D8 | 晴天 | 温和 | 高 | 弱 | 否 |'
- en: '| D9 | Sunny | Cool | Normal | Weak | Yes |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| D9 | 晴天 | 凉爽 | 正常 | 弱 | 是 |'
- en: '| D10 | Rain | Mild | Normal | Weak | Yes |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| D10 | 下雨 | 温和 | 正常 | 弱 | 是 |'
- en: '| D11 | Sunny | Mild | Normal | Strong | Yes |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| D11 | 晴天 | 温和 | 正常 | 强 | 是 |'
- en: '| D12 | Overcast | Mild | High | Strong | Yes |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| D12 | 阴天 | 温和 | 高 | 强 | 是 |'
- en: '| D13 | Overcast | Hot | Normal | Weak | Yes |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| D13 | 阴天 | 炎热 | 正常 | 弱 | 是 |'
- en: '| D14 | Rain | Mild | High | Strong | No |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| D14 | 下雨 | 温和 | 高 | 强 | 否 |'
- en: 'Taking the Humidity variable as an example to classify the Play Tennis field:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以湿度变量为例来对Play Tennis字段进行分类：
- en: '**CHAID:** Humidity has two categories and our expected values should be evenly
    distributed in order to calculate how distinguishing the variable is:'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CHAID：** 湿度有两个类别，我们的期望值应该均匀分布，以便计算变量的区分度：'
- en: '![](img/0853cd9c-7533-48da-88b5-4ad9287d3272.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0853cd9c-7533-48da-88b5-4ad9287d3272.jpg)'
- en: 'Calculating *x²* (Chi-square) value:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 计算*x²*（卡方）值：
- en: '![](img/46a70084-0231-46b4-a8d7-bb2497f161fa.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46a70084-0231-46b4-a8d7-bb2497f161fa.jpg)'
- en: '*Calculating degrees of freedom = (r-1) * (c-1)*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*计算自由度 = (r-1) * (c-1)*'
- en: Where r = number of row components/number of variable categories, C = number
    of response variables.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 r = 行组件数/变量类别数，C = 响应变量数。
- en: Here, there are two row categories (High and Normal) and two column categories
    (No and Yes).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个行类别（高和正常）和两个列类别（否和是）。
- en: Hence = *(2-1) * (2-1) = 1*
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 = *(2-1) * (2-1) = 1*
- en: p-value for Chi-square 2.8 with 1 d.f = 0.0942
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方检验的p值为2.8，自由度为1 = 0.0942
- en: 'p-value can be obtained with the following Excel formulae: *= CHIDIST (2.8,
    1) = 0.0942*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下Excel公式获得p值：*= CHIDIST (2.8, 1) = 0.0942*
- en: In a similar way, we will calculate the *p-value* for all variables and select
    the best variable with a low p-value.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们将计算所有变量的*p值*，并选择具有较低p值的最佳变量。
- en: '**ENTROPY**:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**熵：**'
- en: Entropy = - Σ p * log [2] p
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 熵 = - Σ p * log [2] p
- en: '![](img/2d60403c-6574-4fec-9eb5-760ca7cc1add.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d60403c-6574-4fec-9eb5-760ca7cc1add.png)'
- en: '![](img/b78a09fc-92e5-4213-a650-2b0faab40b9b.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b78a09fc-92e5-4213-a650-2b0faab40b9b.jpg)'
- en: '![](img/78599af8-1208-4e50-a737-4cefced3788c.jpg)![](img/ad1509db-8136-49bf-805c-7f39fbad70a9.jpg)![](img/282a23e5-2e0e-4b69-a044-e12f4cb82e38.jpg)![](img/1c2922a8-38c8-4b82-b30b-e150ca90fd9f.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78599af8-1208-4e50-a737-4cefced3788c.jpg)![](img/ad1509db-8136-49bf-805c-7f39fbad70a9.jpg)![](img/282a23e5-2e0e-4b69-a044-e12f4cb82e38.jpg)![](img/1c2922a8-38c8-4b82-b30b-e150ca90fd9f.jpg)'
- en: In a similar way, we will calculate *information gain* for all variables and
    select the best variable with the *highest information gain.*
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们将计算所有变量的*信息增益*，并选择具有*最高信息增益*的最佳变量。
- en: '**GINI**:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GINI：**'
- en: '*Gini = 1- Σp²*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*基尼 = 1- Σp²*'
- en: '![](img/e8dad0ee-8f1a-44b2-8464-e17aa9dfcd66.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8dad0ee-8f1a-44b2-8464-e17aa9dfcd66.png)'
- en: '![](img/da771f33-f6a8-44cf-9684-5e8a42a8407b.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da771f33-f6a8-44cf-9684-5e8a42a8407b.jpg)'
- en: '![](img/786732ae-2efc-4ae4-a84a-c7a9f4fcdb8e.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/786732ae-2efc-4ae4-a84a-c7a9f4fcdb8e.jpg)'
- en: '![](img/02ac841a-7b0b-4b73-82c4-d6ec41a5c7d7.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02ac841a-7b0b-4b73-82c4-d6ec41a5c7d7.jpg)'
- en: In a similar way, we will calculate *Expected Gini* for all variables and select
    the best with the *lowest expected value***.**
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们将计算所有变量的*期望基尼值*，并选择具有*最低期望值*的最佳变量。
- en: 'For the purpose of a better understanding, we will also do similar calculations
    for the Wind variable:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们也将对风速变量进行类似的计算：
- en: '**CHAID:** Wind has two categories and our expected values should be evenly
    distributed in order to calculate how distinguishing the variable is:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CHAID：** 风速有两个类别，我们的期望值应该均匀分布，以便计算该变量的区分度：'
- en: '![](img/771f6483-31db-49ab-b694-294ac635db7d.jpg)![](img/499cc86e-2728-4d27-833e-9f95e49d385e.jpg)![](img/1859ff9c-69c4-4515-a8c3-a8a3d5a16c80.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/771f6483-31db-49ab-b694-294ac635db7d.jpg)![](img/499cc86e-2728-4d27-833e-9f95e49d385e.jpg)![](img/1859ff9c-69c4-4515-a8c3-a8a3d5a16c80.jpg)'
- en: '**ENTROPY**:'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ENTROPY：**'
- en: '![](img/8a213ea5-6d1a-49d6-968b-ae26b41ff13a.png)![](img/09b3a3a8-b83b-4048-ae99-1c15c46cdde5.jpg)![](img/5856ac10-a258-4e0f-a058-bee5ed73bd2e.jpg)![](img/39bac9cf-cbe4-4815-892a-c1e1b9e0e9ec.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a213ea5-6d1a-49d6-968b-ae26b41ff13a.png)![](img/09b3a3a8-b83b-4048-ae99-1c15c46cdde5.jpg)![](img/5856ac10-a258-4e0f-a058-bee5ed73bd2e.jpg)![](img/39bac9cf-cbe4-4815-892a-c1e1b9e0e9ec.jpg)'
- en: '**GINI**:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GINI：**'
- en: '![](img/8646761f-f269-446b-acc8-63511d734c7c.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8646761f-f269-446b-acc8-63511d734c7c.jpg)'
- en: '![](img/ed77db34-70a2-434a-a6c2-a09158776786.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed77db34-70a2-434a-a6c2-a09158776786.jpg)'
- en: '![](img/7e42d1c1-3b19-41e2-8258-1ef9531d2b8f.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e42d1c1-3b19-41e2-8258-1ef9531d2b8f.jpg)'
- en: Now we will compare both variables for all three metrics so that we can understand
    them better.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将比较这两个变量在所有三个指标中的表现，以便更好地理解它们。
- en: '| Variables | CHAID (p-value) | Entropy information gain | Gini expected value
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | CHAID（p值） | 熵信息增益 | 基尼期望值 |'
- en: '| Humidity | 0.0942 | 0.1518 | 0.3669 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 湿度 | 0.0942 | 0.1518 | 0.3669 |'
- en: '| Wind | 0.2733 | 0.0482 | 0.4285 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 风速 | 0.2733 | 0.0482 | 0.4285 |'
- en: '| Better | Low value | High value | Low value |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 更好 | 低价值 | 高价值 | 低价值 |'
- en: For all three calculations, Humidity is proven to be a better classifier than
    Wind. Hence, we can confirm that all methods convey a similar story.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有三个计算，湿度被证明比风速更适合作为分类器。因此，我们可以确认所有方法传达了类似的信息。
- en: Comparison between logistic regression and decision trees
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归与决策树的比较
- en: Before we dive into the coding details of decision trees, here, we will quickly
    compare the differences between logistic regression and decision trees, so that
    we will know which model is better and in what way.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解决策树的编码细节之前，我们将快速比较逻辑回归和决策树之间的差异，以便了解哪个模型更好，以及为什么更好。
- en: '| Logistic regression | Decision trees |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 决策树 |'
- en: '| Logistic regression model looks like an equation between independent variables
    with respect to its dependent variable. | Tree classifiers produce rules in simple
    English sentences, which can be easily explained to senior management. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归模型看起来像是独立变量与其因变量之间的方程式。 | 决策树生成的是简单的英语句子的规则，可以轻松向高层管理人员解释。 |'
- en: '| Logistic regression is a parametric model, in which the model is defined
    by having parameters multiplied by independent variables to predict the dependent
    variable. | Decision Trees are a non-parametric model, in which no pre-assumed
    parameter exists. Implicitly performs variable screening or feature selection.
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归是一种参数模型，在这种模型中，通过将参数乘以独立变量来预测因变量。 | 决策树是一种非参数模型，不存在预先假定的参数。隐式地执行变量筛选或特征选择。
    |'
- en: '| Assumptions are made on response (or dependent) variable, with binomial or
    Bernoulli distribution. | No assumptions are made on the underlying distribution
    of the data. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 对响应（或因变量）做出假设，采用二项分布或伯努利分布。 | 不对数据的潜在分布做任何假设。 |'
- en: '| Shape of the model is predefined (logistic curve). | Shape of the model is
    not predefined; model fits in best possible classification based on the data instead.
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 模型的形状是预定义的（逻辑曲线）。 | 模型的形状不是预定义的；模型根据数据进行最佳分类。 |'
- en: '| Provides very good results when independent variables are continuous in nature,
    and also linearity holds true. | Provides best results when most of the variables
    are categorical in nature. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 当自变量是连续的且满足线性关系时，提供非常好的结果。 | 当大多数变量是类别型时，提供最佳结果。 |'
- en: '| Difficult to find complex interactions among variables (non-linear relationships
    between variables). | Non-linear relationships between parameters do not affect
    tree performance. Often uncover complex interactions. Trees can handle numerical
    data with highly skewed or multi-modal, as well as categorical predictors with
    either ordinal or non-ordinal structure. |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 很难找到变量之间的复杂交互（变量之间的非线性关系）。 | 参数之间的非线性关系不会影响树的性能。决策树常常能够揭示复杂的交互关系。决策树能够处理具有高度偏态或多峰分布的数值数据，以及具有序数或非序数结构的类别预测变量。
    |'
- en: '| Outliers and missing values deteriorate the performance of logistic regression.
    | Outliners and missing values are dealt with grace in decision trees. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 异常值和缺失值会降低逻辑回归的性能。 | 决策树优雅地处理异常值和缺失值。 |'
- en: Comparison of error components across various styles of models
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较不同模型样式下的误差组成
- en: 'Errors need to be evaluated in order to measure the effectiveness of the model
    in order to improve the model''s performance further by tuning various knobs.
    Error components consist of a bias component, variance component, and pure white
    noise:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 需要评估误差，以衡量模型的有效性，并通过调整不同的参数进一步提高模型的性能。误差组成包括偏差成分、方差成分和纯白噪声：
- en: '![](img/38da4c33-d21a-4098-bd1c-13e9b1274166.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38da4c33-d21a-4098-bd1c-13e9b1274166.jpg)'
- en: 'Out of the following three regions:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下三个区域中：
- en: The first region has high bias and low variance error components. In this region,
    models are very robust in nature, such as linear regression or logistic regression.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个区域具有高偏差和低方差的误差组成。在这个区域，模型非常健壮，例如线性回归或逻辑回归。
- en: Whereas the third region has high variance and low bias error components, in
    this region models are very wiggly and vary greatly in nature, similar to decision
    trees, but due to the great amount of variability in the nature of their shape,
    these models tend to overfit on training data and produce less accuracy on test
    data.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而第三个区域具有高方差和低偏差的误差组成，在该区域，模型表现出高度的不稳定性和变化，类似于决策树，但由于形状变化的巨大变异性，这些模型往往会在训练数据上过拟合，并在测试数据上表现较差。
- en: Last but not least, the middle region, also called the second region, is the
    ideal sweet spot, in which both bias and variance components are moderate, causing
    it to create the lowest total errors.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，中间区域，也叫做第二区域，是理想的最佳区域，在这个区域，偏差和方差成分适中，导致它产生最低的总误差。
- en: '![](img/b1e7525f-15f4-4e4f-8600-07c0646be1b1.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1e7525f-15f4-4e4f-8600-07c0646be1b1.png)'
- en: Remedial actions to push the model towards the ideal region
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采取补救措施，将模型推向理想区域
- en: 'Models with either high bias or high variance error components do not produce
    the ideal fit. Hence, some makeovers are required to do so. In the following diagram,
    the various methods applied are shown in detail. In the case of linear regression,
    there would be a high bias component, meaning the model is not flexible enough
    to fit some non-linearities in data. One turnaround is to break the single line
    into small linear pieces and fit them into the region by constraining them at
    knots, also called **Linear Spline**. Whereas decision trees have a high variance
    problem, meaning even a slight change in *X* values leads to large changes in
    its corresponding *Y* values, this issue can be resolved by performing an ensemble
    of the decision trees:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 具有高偏差或高方差误差成分的模型无法产生理想的拟合。因此，必须进行一些改进。在以下图示中，详细展示了所采用的各种方法。以线性回归为例，其会有一个高偏差成分，意味着模型没有足够的灵活性去拟合数据中的一些非线性关系。解决方法之一是将单一的线性模型分解为多个小线性段，并通过在节点处进行约束来拟合这些线段，这也叫做**线性样条**。而决策树则面临高方差问题，意味着即使是*X*值的微小变化也会导致*Y*值的巨大变化，这个问题可以通过决策树的集成方法来解决：
- en: '![](img/f13d344e-600a-4c0c-b8dd-b354271975f8.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f13d344e-600a-4c0c-b8dd-b354271975f8.png)'
- en: In practice, implementing splines would be a difficult and not so popular method,
    due to the involvement of the many equations a practitioner has to keep tabs on,
    in addition to checking the linearity assumption and other diagnostic KPIs (p-values,
    AIC, multi-collinearity, and so on) of each separate equation. Instead, performing
    ensemble on decision trees is most popular in the data science community, similar
    to bagging, random forest, and boosting, which we will be covering in depth in
    later parts of this chapter. Ensemble techniques tackle variance problems by aggregating
    the results from highly variable individual classifiers such as decision trees.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，实施样条函数是一种复杂且不太流行的方法，因为它涉及到许多方程，实践者需要时刻关注这些方程，还需要检查线性假设和其他诊断KPIs（如p值、AIC、多重共线性等）。相反，决策树的集成方法在数据科学界最为流行，类似于装袋法（bagging）、随机森林（random
    forest）和提升法（boosting），这些内容将在本章的后续部分深入讲解。集成技术通过聚合来自高度变量的单一分类器（如决策树）的结果来解决方差问题。
- en: HR attrition data example
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HR流失数据示例
- en: 'In this section, we will be using IBM Watson''s HR Attrition data (the data
    has been utilized in the book after taking prior permission from the data administrator)
    shared in Kaggle datasets under open source license agreement [https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset) to
    predict whether employees would attrite or not based on independent explanatory
    variables:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用IBM Watson的HR流失数据（在获得数据管理员的事先许可后，本书中使用了该数据），该数据已在Kaggle数据集中以开源许可证共享，链接为[https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset)，以预测员工是否会流失，依据的自变量是解释性变量：
- en: '[PRE0]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'There are about 1470 observations and 35 variables in this data, the top five
    rows are shown here for a quick glance of the variables:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据包含大约1470个观测值和35个变量，下面显示了前五行数据，以便快速浏览这些变量：
- en: '![](img/0c3e832c-2172-439c-809a-5e3b39697062.png)![](img/a5fced1b-56eb-4b4d-a8e5-a49da3e29e3f.png)![](img/51cfc8bd-f18c-4cca-8a57-b2a87239a318.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c3e832c-2172-439c-809a-5e3b39697062.png)![](img/a5fced1b-56eb-4b4d-a8e5-a49da3e29e3f.png)![](img/51cfc8bd-f18c-4cca-8a57-b2a87239a318.png)'
- en: 'The following code is used to convert Yes or No categories into 1 and 0 for
    modeling purposes, as scikit-learn does not fit the model on character/categorical
    variables directly, hence dummy coding is required to be performed for utilizing
    the variables in models:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于将“是”或“否”类别转换为1和0，以便进行建模，因为scikit-learn不能直接在字符/类别变量上拟合模型，因此需要执行虚拟编码以便在模型中使用这些变量：
- en: '[PRE1]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Dummy variables are created for all seven categorical variables (shown here
    in alphabetical order), which are `Business Travel`, `Department`, `Education
    Field`, `Gender`, `Job Role`, `Marital Status`, and `Overtime`. We have ignored
    four variables from the analysis, as they do not change across the observations,
    which are `Employee count`, `Employee number`, `Over18`, and `Standard Hours`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为所有七个类别变量（按字母顺序排列）创建了虚拟变量，这些类别变量包括`Business Travel`（商务旅行）、`Department`（部门）、`Education
    Field`（教育领域）、`Gender`（性别）、`Job Role`（职位）、`Marital Status`（婚姻状况）和`Overtime`（加班）。我们忽略了分析中的四个变量，因为它们在所有观测值中没有变化，分别是`Employee
    count`（员工数量）、`Employee number`（员工编号）、`Over18`（超过18岁）和`Standard Hours`（标准工作时间）：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Continuous variables are separated and will be combined with the created dummy
    variables later:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 连续变量被分离，并将在稍后与创建的虚拟变量合并：
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the following step, both derived dummy variables from categorical variables
    and straight continuous variables are combined:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，将从类别变量中衍生出的虚拟变量与直接的连续变量进行合并：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we have not removed one extra derived dummy variable for each categorical
    variable due to the reason that multi-collinearity does not create a problem in
    decision trees as it does in either logistic or linear regression, hence we can
    simply utilize all the derived variables in the rest of the chapter, as all the
    models utilize decision trees as an underlying model, even after performing ensembles
    of it.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们没有去除每个类别变量中多余的衍生虚拟变量，因为多重共线性在决策树中并不会像在逻辑回归或线性回归中那样产生问题，因此我们可以简单地利用本章其余部分中的所有衍生变量，因为所有模型都使用决策树作为基础模型，即使是在对其进行集成后也是如此。
- en: 'Once basic data has been prepared, it needs to be split by 70-30 for training
    and testing purposes:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦基本数据准备好后，需要按70-30的比例进行训练和测试数据的划分：
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'R Code for Data Preprocessing on HR Attrition Data:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: HR流失数据的R语言数据预处理代码：
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Decision tree classifier
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树分类器
- en: 'The `DecisionTtreeClassifier` from scikit-learn has been utilized for modeling
    purposes, which is available in the `tree` submodule:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 用于建模的`DecisionTreeClassifier`来自scikit-learn，位于`tree`子模块中：
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The parameters selected for the DT classifier are in the following code with
    splitting criterion as Gini, Maximum depth as 5, the minimum number of observations
    required for qualifying split is 2, and the minimum samples that should be present
    in the terminal node is 1:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码中为决策树分类器选择的参数包括：分割标准为Gini，最大深度为5，进行分割所需的最小观测值为2，终端节点应包含的最小样本数为1：
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/038fabe1-a81c-4dff-8be0-040bbd1e4e2b.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/038fabe1-a81c-4dff-8be0-040bbd1e4e2b.png)'
- en: By carefully observing the results, we can infer that, even though the test
    accuracy is high (84.6%), the precision and recall of one category (*Attrition
    = Yes*) is low (*precision = 0.39* and *recall = 0.20*). This could be a serious
    issue when management tries to use this model to provide some extra benefits proactively
    to the employees with a high chance of attrition prior to actual attrition, as
    this model is unable to identify the real employees who will be leaving. Hence,
    we need to look for other modifications; one way is to control the model by using
    class weights. By utilizing class weights, we can increase the importance of a
    particular class at the cost of an increase in other errors.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细观察结果，我们可以推断，尽管测试准确率较高（84.6%），但“流失 = 是”这一类别的精度和召回率较低（*精度 = 0.39* 和 *召回率 =
    0.20*）。当管理层尝试利用该模型在实际流失发生之前，为高流失概率的员工提供额外福利时，这可能会是一个严重问题，因为该模型无法准确识别真正会离开的员工。因此，我们需要寻找其他调整方法；一种方法是通过使用类权重来控制模型。通过使用类权重，我们可以提高特定类别的重要性，代价是增加其他错误的发生。
- en: For example, by increasing class weight to category *1*, we can identify more
    employees with the characteristics of actual attrition, but by doing so, we will
    mark some of the non-potential churner employees as potential attriters (which
    should be acceptable).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过将类权重增加到类别*1*，我们可以识别出更多具有实际流失特征的员工，但这样做会将一些非潜在流失员工标记为潜在流失者（这应当是可以接受的）。
- en: 'Another classic example of the important use of class weights is, in banking
    scenarios. When giving loans, it is better to reject some good applications than
    accepting bad loans. Hence, even in this case, it is a better idea to use higher
    weight to defaulters over non-defaulters:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 类权重的重要应用的另一个经典例子是银行场景。当发放贷款时，拒绝一些好的申请总比接受坏的贷款要好。因此，即使在这种情况下，将违约者的权重大于非违约者也是一个更好的选择：
- en: 'R Code for Decision Tree Classifier Applied on HR Attrition Data:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在人力资源流失数据上应用决策树分类器的R代码：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Tuning class weights in decision tree classifier
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整决策树分类器中的类权重
- en: 'In the following code, class weights are tuned to see the performance change
    in decision trees with the same parameters. A dummy DataFrame is created to save
    all the results of various precision-recall details of combinations:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，调整了类权重，以观察决策树在相同参数下性能的变化。创建了一个虚拟数据框，以保存各种精度-召回细节组合的结果：
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Metrics to be considered for capture are weight for zero and one category (for
    example, if the weight for zero category given is 0.2, then automatically, weight
    for the one should be 0.8, as total weight should be equal to 1), training and
    testing accuracy, precision for zero category, one category, and overall. Similarly,
    recall for zero category, one category, and overall are also calculated:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的指标包括零类别和一类别的权重（例如，如果零类别的权重为0.2，则一类别的权重应自动为0.8，因为总权重应等于1）、训练和测试准确率、零类别和一类别的精度，以及整体精度。同样，也计算零类别和一类别的召回率，以及整体召回率：
- en: '[PRE11]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Weights for the zero category are verified from 0.01 to 0.5, as we know we
    do not want to explore cases where the zero category will be given higher weightage
    than one category:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 零类别的权重范围从0.01到0.5进行验证，因为我们知道我们不想探索将零类别的权重设置得比一类别更高的情况：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/6d71e76b-50e7-4fcb-93b3-7eacb2a96bc0.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d71e76b-50e7-4fcb-93b3-7eacb2a96bc0.png)'
- en: 'From the preceding screenshot, we can see that at class weight values of 0.3
    (for zero) and 0.7 (for one) it is identifying a higher number of attriters (25
    out of 61) without compromising test accuracy 83.9% using decision trees methodology:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的截图中，我们可以看到，在类权重值为0.3（对于零类）和0.7（对于一类）时，它能识别出更多的流失者（61个中有25个），且不影响测试准确率（83.9%），使用的是决策树方法：
- en: 'R Code for Decision Tree Classifier with class weights Applied on HR Attrition
    Data:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在人力资源流失数据上应用类权重的决策树分类器R代码：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Bagging classifier
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging 分类器
- en: As we have discussed already, decision trees suffer from high variance, which
    means if we split the training data into two random parts separately and fit two
    decision trees for each sample, the rules obtained would be very different. Whereas
    low variance and high bias models, such as linear or logistic regression, will
    produce similar results across both samples. Bagging refers to bootstrap aggregation
    (repeated sampling with replacement and perform aggregation of results to be precise),
    which is a general purpose methodology to reduce the variance of models. In this
    case, they are decision trees.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论过的，决策树存在较高的方差，这意味着如果我们将训练数据随机分成两部分，并为每个样本拟合两棵决策树，那么得到的规则将会非常不同。而低方差和高偏差的模型，例如线性回归或逻辑回归，在两个样本上会产生相似的结果。Bagging
    是自助聚合（带替换的重复采样并执行结果聚合，准确来说），它是一种通用的方法，旨在减少模型的方差。在这个例子中，所使用的模型是决策树。
- en: 'Aggregation reduces the variance, for example, when we have n independent observations
    *x[1], x[2 ],..., x[n]* each with variance *σ²* and the variance of the mean *x̅* of
    the observations is given by *σ²/n*, which illustrates by averaging a set of observations
    that it reduces variance. Here, we are reducing variance by taking many samples
    from training data (also known as bootstrapping), building a separate decision
    tree on each sample separately, averaging the predictions for regression, and
    calculating mode for classification problems in order to obtain a single low-variance
    model that will have both low bias and low variance:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合能够减少方差，例如，当我们有 n 个独立观测值 *x[1], x[2], ..., x[n]* 每个观测值的方差为 *σ²* 时，所有观测值均值 *x̅*
    的方差为 *σ²/n*，这表明通过平均一组观测值可以减少方差。在这里，我们通过从训练数据中抽取多个样本（也称为自助采样），为每个样本分别构建决策树，对回归问题取平均，对分类问题计算众数，从而得到一个低方差的单一模型，既有低偏差又有低方差：
- en: '![](img/a0e4f294-1efc-45f2-bcfa-d32a18f025d0.jpg)![](img/5b837eae-9191-42fa-ae93-57220340d1da.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0e4f294-1efc-45f2-bcfa-d32a18f025d0.jpg)![](img/5b837eae-9191-42fa-ae93-57220340d1da.jpg)'
- en: 'In a bagging procedure, rows are sampled while selecting all the columns/variables
    (whereas, in a random forest, both rows and columns would be sampled, which we
    will cover in the next section) and fitting individual trees for each sample.
    In the following diagram, two colors (pink and blue) represent two samples, and
    for each sample, a few rows are sampled, but all the columns (variables) are selected
    every time. One issue that exists due to the selection of all columns is that
    most of the trees will describe the same story, in which the most important variable
    will appear initially in the split, and this repeats in all the trees, which will
    not produce de-correlated trees, so we may not get better performance when applying
    variance reduction. This issue will be avoided in random forest (we will cover
    this in the next section of the chapter), in which we will sample both rows and
    columns as well:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Bagging 过程中，行是被抽样的，而所有列/变量都被选择（而在随机森林中，行和列都会被抽样，我们将在下一节中讲解）。在下图中，两种颜色（粉色和蓝色）代表两种样本，对于每个样本，一些行被抽取，但每次都选择所有的列（变量）。由于选择了所有列，存在一个问题，即大多数树会描述相同的故事，其中最重要的变量会最初出现在分裂中，并在所有树中重复出现，这样就无法产生去相关的树，因此在应用方差减少时可能无法得到更好的性能。在随机森林中（我们将在下一节讲解），我们将同时抽取行和列，避免了这个问题：
- en: '![](img/a3dd53ad-e3cd-45c5-81a2-dfe75cec6ffe.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3dd53ad-e3cd-45c5-81a2-dfe75cec6ffe.png)'
- en: 'In the following code, the same HR data has been used to fit the bagging classifier
    in order to compare the results apple to apple with respect to decision trees:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们使用相同的 HR 数据来拟合 Bagging 分类器，以便在决策树的比较中做到公正：
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The base classifier used here is Decision Trees with the same parameter setting
    that we used in the decision tree example:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的基础分类器是决策树，参数设置与我们在决策树示例中使用的一致：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Parameters used in bagging are, `n_estimators` to represent the number of individual
    decision trees used as 5,000, maximum samples and features selected are 0.67 and
    1.0 respectively, which means it will select 2/3^(rd) of observations for each
    tree and all the features. For further details, please refer to the scikit-learn
    manual [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging中使用的参数为，`n_estimators`表示使用的单个决策树数量为5,000，最大样本和特征分别选择0.67和1.0，这意味着每棵树将选择2/3的观测值和所有特征。有关更多详细信息，请参考scikit-learn手册：[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)：
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/9e97501c-06fe-4fc4-b346-0e0fa32889c6.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e97501c-06fe-4fc4-b346-0e0fa32889c6.png)'
- en: 'After analyzing the results from bagging, the test accuracy obtained was 87.3%,
    whereas for decision tree it was 84.6%. Comparing the number of actual attrited
    employees identified, there were 13 in bagging, whereas in decision tree there
    were 12, but the number of 0 classified as 1 significantly reduced to 8 compared
    with 19 in DT. Overall, bagging improves performance over the single tree:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析了Bagging的结果后，测试准确率为87.3%，而决策树的准确率为84.6%。比较实际流失员工的识别数量，Bagging方法识别了13名，而决策树识别了12名，但将0分类为1的数量显著减少，Bagging为8，而决策树为19。总体而言，Bagging方法提高了单棵树的性能：
- en: 'R Code for Bagging Classifier Applied on HR Attrition Data:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于HR流失数据的Bagging分类器的R代码：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Random forest classifier
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林分类器
- en: Random forests provide an improvement over bagging by doing a small tweak that
    utilizes de-correlated trees. In bagging, we build a number of decision trees
    on bootstrapped samples from training data, but the one big drawback with the
    bagging technique is that it selects all the variables. By doing so, in each decision
    tree, the order of candidate/variable chosen to split remains more or less the
    same for all the individual trees, which look correlated with each other. Variance
    reduction on correlated individual entities does not work effectively while aggregating
    them.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通过一个小的调整，利用去相关的树，提供了对Bagging的改进。在Bagging中，我们基于训练数据的自助抽样构建了多个决策树，但Bagging方法的一个大缺点是它选择了所有变量。通过这样做，在每棵决策树中，选择用于划分的候选变量的顺序对于所有个体树来说基本相同，因此它们看起来相互之间相关。在聚合它们时，相关个体的方差减少效果不明显。
- en: In random forest, during bootstrapping (repeated sampling with replacement),
    samples were drawn from training data; not just simply the second and third observations
    randomly selected, similar to bagging, but it also selects the few predictors/columns
    out of all predictors (m predictors out of total p predictors).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林中，进行自助抽样（带替换的重复采样）时，从训练数据中抽取样本；不仅仅是像Bagging那样随机选择第二和第三个观测值，它还从所有预测变量中选择少量预测变量/列（从总p个预测变量中选择m个预测变量）。
- en: The thumb rule for variable selection of m variables out of total variables
    *p* is *m = sqrt(p)* for classification and *m = p/3* for regression problems
    randomly to avoid correlation among the individual trees. By doing so, significant
    improvement in the accuracy can be achieved. This ability of RF makes it one of
    the favorite algorithms used by the data science community, as a winning recipe
    across various competitions or even for solving practical problems in various
    industries.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从总变量中选择*m*个变量的拇指法则是，分类问题中*m = sqrt(p)*，回归问题中*m = p/3*，目的是随机选择以避免个体树之间的相关性。通过这样做，可以显著提高准确性。这种RF的能力使其成为数据科学社区最喜欢的算法之一，作为跨各种竞赛的获胜秘诀，甚至用于解决各行业的实际问题。
- en: 'In the following diagram, different colors represent different bootstrap samples.
    In the first sample, the 1^(st), 3^(rd), 4^(th,) and 7^(th) columns are selected,
    whereas, in the second bootstrap sample, the 2^(nd), 3^(rd), 4^(th,) and 5^(th)
    columns are selected respectively. In this way, any columns can be selected at
    random, whether they are adjacent to each other or not. Though the thumb rules
    of *sqrt (p)* or *p/3* are given, readers are encouraged to tune the number of
    predictors to be selected:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图示中，不同的颜色代表不同的自助样本。在第一个样本中，选择了第1、第3、第4和第7列，而在第二个自助样本中，选择了第2、第3、第4和第5列。通过这种方式，可以随机选择任何列，无论它们是否相邻。虽然给出了*sqrt
    (p)*或*p/3*的拇指法则，但建议读者调整要选择的预测变量数量：
- en: '![](img/2f1f70f5-eeea-43f7-9e8b-4751f7cd44e9.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f1f70f5-eeea-43f7-9e8b-4751f7cd44e9.png)'
- en: 'The sample plot shows the impact of a test error change while changing the
    parameters selected, and it is apparent that a *m = sqrt(p)* scenario gives better
    performance on test data compared with *m =p* (we can call this scenario bagging):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 样本图显示了在改变所选参数时，测试误差变化的影响，可以明显看出，*m = sqrt(p)*的情形在测试数据上的表现优于*m = p*（我们可以称之为袋装法）：
- en: '![](img/2ab1c7e8-50f3-451c-b1cc-d21901c3ceb6.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ab1c7e8-50f3-451c-b1cc-d21901c3ceb6.png)'
- en: 'Random forest classifier has been utilized from the `scikit-learn` package
    here for illustration purposes:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里为了说明，使用了来自`scikit-learn`包的随机森林分类器：
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The parameters used in random forest are: `n_estimators` representing the number
    of individual decision trees used is 5000, maximum features selected are *auto*,
    which means it will select *sqrt(p)* for classification and *p/3* for regression
    automatically. Here is the straightforward classification problem though. Minimum
    samples per leaf provide the minimum number of observations required in the terminal
    node:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林中使用的参数有：`n_estimators`表示使用的单个决策树数量为5000，选择的最大特征为*auto*，这意味着它将自动选择*sqrt(p)*用于分类，*p/3*用于回归。但这里是一个直接的分类问题。每个叶子的最小样本数提供了终端节点中所需的最小观察值数：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/e668098b-6e9d-45f6-90be-9fc73710f1a1.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e668098b-6e9d-45f6-90be-9fc73710f1a1.png)'
- en: 'Random forest classifier produced 87.8% test accuracy compared with bagging
    87.3%, and also identifies 14 actually attrited employees in contrast with bagging,
    for which 13 attrited employees have been identified:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器产生了87.8%的测试准确度，而袋装法为87.3%，同时识别出14名实际离职的员工，而袋装法则识别出了13名离职员工：
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/dcd3b9c0-2961-4cb9-acdc-e29dc119fcaa.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcd3b9c0-2961-4cb9-acdc-e29dc119fcaa.png)'
- en: 'From the variable importance plot, it seems that the monthly income variable
    seems to be most significant, followed by overtime, total working years, stock
    option levels, years at company, and so on. This provides us with some insight
    into what are major contributing factors that determine whether the employee will
    remain with the company or leave the organization:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从变量重要性图来看，月收入变量似乎是最重要的，其次是加班、总工作年限、股票期权等级、公司年限等等。这为我们提供了一些洞察，帮助我们了解哪些主要因素决定了员工是否会留在公司或离开组织：
- en: 'R Code for Random Forest Classifier Applied on HR Attrition Data:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于HR离职数据的随机森林分类器的R代码：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Random forest classifier - grid search
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林分类器 - 网格搜索
- en: 'Tuning parameters in a machine learning model play a critical role. Here, we
    are showing a grid search example on how to tune a random forest model:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型中，调整参数起着至关重要的作用。这里展示了一个网格搜索示例，演示如何调整随机森林模型：
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Tuning parameters are similar to random forest parameters apart from verifying
    all the combinations using the pipeline function. The number of combinations to
    be evaluated will be *(3 x 3 x 2 x 2) *5 =36*5 = 180* combinations. Here 5 is
    used in the end, due to the cross-validation of five-fold:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 调整参数与随机森林的参数类似，除了使用管道函数验证所有组合。需要评估的组合数量将是*(3 x 3 x 2 x 2) *5 =36*5 = 180*个组合。这里最终使用5，是因为五折交叉验证：
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/544ea2c9-c18a-45a9-80bf-246993602bf0.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/544ea2c9-c18a-45a9-80bf-246993602bf0.png)'
- en: 'In the preceding results, grid search seems to not provide many advantages
    compared with the already explored random forest result. But, practically, most
    of the times, it will provide better and more robust results compared with a simple
    exploration of models. However, by carefully evaluating many different combinations,
    it will eventually discover the best parameters combination:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的结果中，网格搜索似乎没有提供比已经探索过的随机森林结果更多的优势。但是，实际上，大多数情况下，它会提供比简单模型探索更好、更强大的结果。然而，通过仔细评估多种不同的组合，它最终会发现最佳的参数组合：
- en: 'R Code for random forest classifier with grid search applied on HR attrition
    data:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于HR离职数据的带有网格搜索的随机森林分类器的R代码：
- en: '[PRE25]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: AdaBoost classifier
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost分类器
- en: 'Boosting is another state-of-the-art model that is being used by many data
    scientists to win so many competitions. In this section, we will be covering the
    **AdaBoost** algorithm, followed by **gradient boost** and **extreme gradient
    boost** (**XGBoost**). Boosting is a general approach that can be applied to many
    statistical models. However, in this book, we will be discussing the application
    of boosting in the context of decision trees. In bagging, we have taken multiple
    samples from the training data and then combined the results of individual trees
    to create a single predictive model; this method runs in parallel, as each bootstrap
    sample does not depend on others. Boosting works in a sequential manner and does
    not involve bootstrap sampling; instead, each tree is fitted on a modified version
    of an original dataset and finally added up to create a strong classifier:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting 是一种当前许多数据科学家在多个竞赛中获胜的前沿模型。在本节中，我们将介绍 **AdaBoost** 算法，随后介绍 **梯度提升**
    和 **极端梯度提升**（**XGBoost**）。Boosting 是一种通用方法，可以应用于许多统计模型。然而，在本书中，我们将讨论 Boosting
    在决策树中的应用。在 Bagging 中，我们从训练数据中提取多个样本，并将各个树的结果结合起来创建一个单一的预测模型；这种方法是并行运行的，因为每个自助样本不依赖于其他样本。Boosting
    是顺序进行的，它不涉及自助抽样；相反，每棵树都是在原始数据集的修改版上拟合的，最终这些树的结果被加起来形成一个强分类器：
- en: '![](img/d8dbad23-3a24-4337-a7f2-46d70806d109.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8dbad23-3a24-4337-a7f2-46d70806d109.jpg)'
- en: '![](img/d67d7eae-1176-4fe6-8a40-00c74a987933.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d67d7eae-1176-4fe6-8a40-00c74a987933.png)'
- en: The preceding figure is the sample methodology on how AdaBoost works. We will
    cover step-by-step procedures in detail in the following algorithm description.
    Initially, a simple classifier has been fitted on the data (also called a decision
    stump, which splits the data into just two regions) and whatever the classes correctly
    classified will be given less weightage in the next iteration (iteration 2) and
    higher weightage for misclassified classes (observer + blue icons), and again
    another decision stump/weak classifier will be fitted on the data and will change
    the weights again for the next iteration (iteration 3, here check the - symbols
    for which weight has been increased). Once it finishes the iterations, these are
    combined with weights (weights automatically calculated for each classifier at
    each iteration based on error rate) to come up with a strong classifier, which
    predicts the classes with surprising accuracy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了 AdaBoost 工作原理的示例方法论。我们将在接下来的算法描述中详细介绍逐步流程。最初，一个简单的分类器已经拟合在数据上（也称为决策树桩，它将数据划分为两个区域），在下一次迭代（迭代
    2）中，正确分类的类将给予较低的权重，而错误分类的类（观察者 + 蓝色图标）将给予较高的权重，然后再次拟合另一个决策树桩/弱分类器，并且会改变权重以进行下一次迭代（迭代
    3，在这里检查 - 符号以查看哪个权重被增加）。一旦完成所有迭代，这些将与权重（根据错误率自动计算的每个分类器的权重）结合，得到一个强分类器，能够以惊人的准确度预测类别。
- en: '**Algorithm for AdaBoost consists of the following steps:**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaBoost 算法包括以下步骤：**'
- en: Initialize the observation weights *w[i] = 1/N, i=1, 2, …, N*. Where *N = Number
    of observations.*
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化观察值的权重 *w[i] = 1/N, i=1, 2, …, N*。其中 *N = 观察值的数量*。
- en: 'For m = 1 to M:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 m = 1 到 M：
- en: Fit a classifier *Gm(x)* to the training data using weights *w[i]*
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用权重 *w[i]* 将分类器 *Gm(x)* 拟合到训练数据上
- en: 'Compute:'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算：
- en: '![](img/cf136375-279e-47a3-9584-fd97f3841f66.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf136375-279e-47a3-9584-fd97f3841f66.jpg)'
- en: 'Compute:'
  id: totrans-199
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算：
- en: '![](img/9d970c77-ab57-497e-aaa9-10feed4790c5.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d970c77-ab57-497e-aaa9-10feed4790c5.jpg)'
- en: 'Set:'
  id: totrans-201
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置：
- en: '![](img/10e696b5-b9c1-4029-8e6e-3a679166bfaf.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10e696b5-b9c1-4029-8e6e-3a679166bfaf.jpg)'
- en: 'Output:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出：
- en: '![](img/af921fd3-a852-4c86-b988-abc62a45c382.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af921fd3-a852-4c86-b988-abc62a45c382.jpg)'
- en: All the observations are given equal weight.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 所有观察值赋予相同的权重。
- en: In bagging and random forest algorithms, we deal with the columns of the data;
    whereas, in boosting, we adjust the weights of each observation and don't elect
    a few columns.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Bagging 和随机森林算法中，我们处理的是数据的列；而在 Boosting 中，我们调整的是每个观察值的权重，并不会选择一些列。
- en: We fit a classifier on the data and evaluate overall errors. The error used
    for calculating weight should be given for that classifier in the final additive
    model (**α**) evaluation. The intuitive sense is that the higher weight will be
    given for the model with fewer errors. Finally, weights for each observation will
    be updated. Here, weight will be increased for incorrectly classified observations
    in order to give more focus to the next iterations, and weights will be reduced
    for correctly classified observations.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据上拟合一个分类器并评估整体错误。用于计算权重的误差应在最终的加法模型中给予该分类器（**α**）评估。直观上，错误较少的模型会给予更高的权重。最后，每个观察值的权重会被更新。在这里，错误分类的观察值会增加权重，以便在接下来的迭代中给予更多关注，而正确分类的观察值的权重会减少。
- en: 'All the weak classifiers are combined with their respective weights to form
    a strong classifier. In the following figure, a quick idea is shared on how weights
    changed in the last iteration compared with the initial iteration:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 所有弱分类器与各自的权重相结合，形成一个强分类器。在下图中，展示了与初始迭代相比，在最后一次迭代中权重变化的快速示意：
- en: '![](img/154913f0-cfe9-41dc-b813-cecc75c0a0fa.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/154913f0-cfe9-41dc-b813-cecc75c0a0fa.png)'
- en: '[PRE26]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Decision stump is used as a base classifier for AdaBoost. If we observe the
    following code, the depth of the tree remains as 1, which has decision taking
    ability only once (also considered a weak classifier):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树桩被用作AdaBoost的基础分类器。如果我们观察以下代码，树的深度保持为1，表示只能进行一次决策（也视为弱分类器）：
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In AdaBoost, decision stump has been used as a base estimator to fit on whole
    datasets and then fits additional copies of the classifier on the same dataset
    up to 5000 times. The learning rate shrinks the contribution of each classifier
    by 0.05\. There is a trade-off between the learning rate and number of estimators.
    By carefully choosing a low learning rate and a long number of estimators, one
    can converge optimum very much, however at the expense of computing power:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在AdaBoost中，决策树桩被用作基本估计器，拟合整个数据集，然后将分类器的附加副本拟合到同一数据集上，最多重复5000次。学习率将每个分类器的贡献缩小0.05。学习率和估计器数量之间存在权衡。通过仔细选择较低的学习率和较长的估计器数量，可以更好地收敛到最优解，但代价是计算能力的消耗：
- en: '[PRE28]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](img/2da58a78-f500-42c4-b9d3-e2c77f56cfb6.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2da58a78-f500-42c4-b9d3-e2c77f56cfb6.png)'
- en: 'The result of the AdaBoost seems to be much better than the known best random
    forest classifiers in terms of the recall of 1 value. Though there is a slight
    decrease in accuracy to 86.8% compared with the best accuracy of 87.8%, the number
    of 1''s predicted is 23 from the RF, which is 14 with some expense of an increase
    in 0''s, but it really made good progress in terms of identifying actual attriters:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 与已知的最佳随机森林分类器相比，AdaBoost的结果似乎在召回1值的表现上要好得多。尽管与最佳准确率87.8%相比，准确率略微下降至86.8%，但是随机森林预测出的1的数量为23，而AdaBoost为14，尽管增加了0的预测数量，但它在识别实际流失者方面确实取得了显著进展：
- en: 'R Code for AdaBoost classifier applied on HR attrition data:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在HR流失数据上应用的AdaBoost分类器的R代码：
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Gradient boosting classifier
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升分类器
- en: Gradient boosting is one of the competition-winning algorithms that work on
    the principle of boosting weak learners iteratively by shifting focus towards
    problematic observations that were difficult to predict in previous iterations
    and performing an ensemble of weak learners, typically decision trees. It builds
    the model in a stage-wise fashion as other boosting methods do, but it generalizes
    them by allowing optimization of an arbitrary differentiable loss function.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是一个在竞争中获胜的算法，它的原理是通过迭代地增强弱学习器，重点关注在前几次迭代中难以预测的有问题的观察数据，并执行弱学习器的集成，通常是决策树。它像其他提升方法一样分阶段构建模型，但通过允许优化任意可微分的损失函数来对这些方法进行泛化。
- en: 'Let''s start understanding Gradient Boosting with a simple example, as GB challenges
    many data scientists in terms of understanding the working principle:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来理解梯度提升，因为GB在工作原理的理解上挑战了许多数据科学家：
- en: 'Initially, we fit the model on observations producing 75% accuracy and the
    remaining unexplained variance is captured in the ***error*** term:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，我们在观察数据上拟合模型，得到75%的准确率，其余的未解释方差被捕捉在***错误***项中：
- en: '![](img/6cc1f87f-915a-4141-baa9-b7b829f34fe5.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6cc1f87f-915a-4141-baa9-b7b829f34fe5.jpg)'
- en: 'Then we will fit another model on the error term to pull the extra explanatory
    component and add it to the original model, which should improve the overall accuracy:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将拟合另一个模型来处理误差项，提取额外的解释性成分，并将其加入到原始模型中，这应该会提高整体的准确性：
- en: '![](img/2d7f49d3-84b7-41ec-8314-42b706ff263d.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d7f49d3-84b7-41ec-8314-42b706ff263d.jpg)'
- en: 'Now, the model is providing 80% accuracy and the equation looks as follows:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，模型提供了80%的准确率，方程如下：
- en: '![](img/635f7efa-5e62-4cc8-aa89-898d720a2556.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/635f7efa-5e62-4cc8-aa89-898d720a2556.jpg)'
- en: 'We continue this method one more time to fit a model on the **error2** component
    to extract a further explanatory component:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再继续使用这种方法一次，拟合一个模型到**error2**成分，以提取进一步的解释性成分：
- en: '![](img/4b7f8e8d-a0aa-4a58-9e8f-42948e8b620c.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b7f8e8d-a0aa-4a58-9e8f-42948e8b620c.jpg)'
- en: 'Now, model accuracy is further improved to 85% and the final model equation
    looks as follows:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，模型的准确性进一步提升至85%，最终的模型方程如下：
- en: '![](img/749e5e90-0a8a-4e88-9292-4abfb987f360.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/749e5e90-0a8a-4e88-9292-4abfb987f360.jpg)'
- en: Here, if we use weighted average (higher importance given to better models that
    predict results with greater accuracy than others) rather than simple addition,
    it will improve the results further. In fact, this is what the gradient boosting
    algorithm does!
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，如果我们使用加权平均（给予预测准确性更高的模型更高的重要性），而不是简单的相加，它将进一步改善结果。事实上，这正是梯度提升算法所做的！
- en: '![](img/558d6b99-dc6b-4627-a674-e61e99137f73.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/558d6b99-dc6b-4627-a674-e61e99137f73.jpg)'
- en: After incorporating weights, the name of the error changed from **error3** to
    **error4**, as both errors may not be exactly the same. If we find better weights,
    we will probably get an accuracy of 90% instead of simple addition, where we have
    only got 85%.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入权重后，误差的名称从**error3**变为**error4**，因为这两个误差可能并不完全相同。如果我们找到更好的权重，可能会得到90%的准确率，而不是简单的相加方法，在相加的情况下我们仅获得85%的准确率。
- en: '**Gradient boosting involves three elements:**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升包括三个要素：**'
- en: '**Loss function to be optimized:** Loss function depends on the type of problem
    being solved. In the case of regression problems, mean squared error is used,
    and in classification problems, the logarithmic loss will be used. In boosting,
    at each stage, unexplained loss from prior iterations will be optimized rather
    than starting from scratch.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要优化的损失函数：** 损失函数取决于所解决问题的类型。在回归问题中，使用均方误差，而在分类问题中，将使用对数损失。在提升算法中，每个阶段都会优化之前迭代中的未解释损失，而不是从头开始。'
- en: '**Weak learner to make predictions:** Decision trees are used as a weak learner
    in gradient boosting.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弱学习器进行预测：** 决策树被用作梯度提升中的弱学习器。'
- en: '**Additive model to add weak learners to minimize the loss function:** Trees
    are added one at a time and existing trees in the model are not changed. The gradient
    descent procedure is used to minimize the loss when adding trees.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加法模型通过加入弱学习器来最小化损失函数：** 树是逐一加入的，模型中已有的树不会被更改。梯度下降过程用于最小化添加树时的损失。'
- en: '**The algorithm for Gradient boosting consists of the following steps:**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升算法包含以下步骤：**'
- en: 'Initialize:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化：
- en: '![](img/2ddc97ad-e0da-44a7-b79d-c449607be372.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ddc97ad-e0da-44a7-b79d-c449607be372.jpg)'
- en: 'For *m = 1* to M:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *m = 1* 到 M：
- en: 'a) For *i = 1, 2, …, N* compute:'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: a) 对于 *i = 1, 2, …, N*，计算：
- en: '![](img/95586651-da5d-4724-aa5b-66e0d87c8136.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95586651-da5d-4724-aa5b-66e0d87c8136.jpg)'
- en: b) Fit a regression tree to the targets r[im] giving terminal regions R[jm],
    j = 1, 2, …, J[m],
  id: totrans-245
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: b) 拟合回归树到目标 r[im]，得到终端区域 R[jm]，其中 j = 1, 2, …, J[m]，
- en: 'c) For j = 1, 2, …, J[m], compute:'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: c) 对于 j = 1, 2, …, J[m]，计算：
- en: '![](img/7c070b8d-1742-4565-9c69-c730b94ae07d.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c070b8d-1742-4565-9c69-c730b94ae07d.jpg)'
- en: 'd) Update:'
  id: totrans-248
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: d) 更新：
- en: '![](img/f3a737c5-daa7-4843-bfa9-93a634fc461b.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3a737c5-daa7-4843-bfa9-93a634fc461b.jpg)'
- en: 'Output:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出：
- en: '![](img/c8ab0b80-4bb0-4e32-a52f-7a8a6d311fe1.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8ab0b80-4bb0-4e32-a52f-7a8a6d311fe1.jpg)'
- en: Initializes the constant optimal constant model, which is just a single terminal
    node that will be utilized as a starting point to tune it further in the next
    steps. *(2a)*, calculates the residuals/errors by comparing actual outcome with
    predicted results, followed by (*2b* and *2c*) in which the next decision tree
    will be fitted on error terms to bring in more explanatory power to the model,
    and in (*2d*) add the extra component to the model at last iteration. Finally,
    ensemble all weak learners to create a strong learner.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化常量最优常数模型，该模型只是一个单一的终端节点，将作为起始点，在接下来的步骤中进一步调优。*(2a)*，通过比较实际结果与预测结果来计算残差/误差，接着是(*2b*和*2c*)，在这些步骤中，下一棵决策树将在误差项上拟合，以增加模型的解释能力，最后在(*2d*)步骤中，在最后一次迭代中将额外的组件加入模型。最后，将所有弱学习器进行集成，创造出强学习器。
- en: Comparison between AdaBoosting versus gradient boosting
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost与梯度提升的比较
- en: After understanding both AdaBoost and gradient boost, readers may be curious
    to see the differences in detail. Here, we are presenting exactly that to quench
    your thirst!
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了AdaBoost和梯度提升之后，读者可能会好奇两者的具体差异。这里，我们正是展示了这些内容来解答你的疑问！
- en: '![](img/6ca41b37-732c-4c61-8225-0bc5962acab3.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ca41b37-732c-4c61-8225-0bc5962acab3.jpg)'
- en: 'The gradient boosting classifier from the scikit-learn package has been used
    for computation here:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了来自scikit-learn包的梯度提升分类器进行计算：
- en: '[PRE30]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Parameters used in the gradient boosting algorithms are as follows. Deviance
    has been used for loss, as the problem we are trying to solve is 0/1 binary classification.
    The learning rate has been chosen as 0.05, number of trees to build is 5000 trees,
    minimum sample per leaf/terminal node is 1, and minimum samples needed in a bucket
    for qualification for splitting is 2:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法中使用的参数如下。由于我们正在解决的是0/1二分类问题，因此使用了偏差作为损失函数。学习率选定为0.05，构建的树的数量为5000棵，叶节点/终端节点的最小样本数为1，桶中进行分割的最小样本数为2：
- en: '[PRE31]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](img/96bd6b48-620a-4932-ba93-231c6acff064.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96bd6b48-620a-4932-ba93-231c6acff064.png)'
- en: If we analyze the results, Gradient boosting has given better results than AdaBoost
    with the highest possible test accuracy of 87.5% with most 1's captured as 24,
    compared with AdaBoost with which the test accuracy obtained was 86.8%. Hence,
    it has been proven that it is no wonder why every data scientist tries to use
    this algorithm to win competitions!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析结果，梯度提升比AdaBoost取得了更好的结果，最高测试准确率为87.5%，捕获的1的数量最多为24，而AdaBoost的测试准确率为86.8%。因此，这证明了为什么每个数据科学家都试图使用这一算法来赢得竞赛，毫不奇怪！
- en: 'The R code for gradient boosting classifier applied on HR attrition data:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于HR流失数据的梯度提升分类器的R代码：
- en: '[PRE33]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Extreme gradient boosting - XGBoost classifier
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 极限梯度提升 - XGBoost分类器
- en: 'XGBoost is the new algorithm developed in 2014 by *Tianqi Chen* based on the
    Gradient boosting principles. It has created a storm in the data science community
    since its inception. XGBoost has been developed with both deep consideration in
    terms of system optimization and principles in machine learning. The goal of the
    library is to push the extremes of the computation limits of machines to provide
    scalable, portable, and accurate results:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 是由 *Tianqi Chen* 基于梯度提升原理于2014年开发的新算法。自其问世以来，它在数据科学界掀起了风暴。XGBoost 在系统优化和机器学习原理方面都进行了深刻的思考。该库的目标是推动机器计算极限，为用户提供可扩展、可移植且准确的结果：
- en: '[PRE34]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/9c1aa18d-2475-4698-8275-1048a2e9405a.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c1aa18d-2475-4698-8275-1048a2e9405a.png)'
- en: The results obtained from **XGBoost** are almost similar to gradient boosting.
    The test accuracy obtained was 87.1%, whereas boosting got 87.5%, and also the
    number of 1's identified is 23 compared with 24 in gradient boosting. The greatest
    advantage of XGBoost over Gradient boost is in terms of performance and the options
    available to control model tune. By changing a few of them, makes XGBoost even
    beat gradient boost as well!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 从**XGBoost**获得的结果几乎与梯度提升相似。获得的测试准确率为87.1%，而梯度提升为87.5%，并且识别到的1的数量为23，而梯度提升为24。XGBoost相对于梯度提升的最大优势在于性能和可用于控制模型调优的选项。通过更改其中的一些参数，使XGBoost甚至超过了梯度提升！
- en: 'The R code for xtreme gradient boosting (XGBoost) classifier applied on HR
    attrition data:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于HR流失数据的极限梯度提升（XGBoost）分类器的R代码：
- en: '[PRE35]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Ensemble of ensembles - model stacking
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成集成 - 模型堆叠
- en: 'Ensemble of ensembles or model stacking is a method to combine different classifiers
    into a meta-classifier that has a better generalization performance than each
    individual classifier in isolation. It is always advisable to take opinions from
    many people when you are in doubt, when dealing with problems in your personal
    life too! There are two ways to perform ensembles on models:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 集成集成或模型堆叠是一种将不同分类器组合成一个元分类器的方法，这个元分类器的泛化性能优于单独的每个分类器。无论是在处理生活中的个人问题时，还是在感到疑虑时，向多人请教总是一个好主意！执行模型集成有两种方法：
- en: '**Ensemble with different types of classifiers:** In this methodology, different
    types of classifiers (for example, logistic regression, decision trees, random
    forest, and so on) are fitted on the same training data and results are combined
    based on either majority voting or average, based on if it is classification or
    regression problems.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用不同类型分类器的集成：** 在这种方法中，使用不同类型的分类器（例如，逻辑回归、决策树、随机森林等）在相同的训练数据上进行拟合，结果根据是否为分类问题或回归问题，采用多数投票或平均值的方式进行合并。'
- en: '**Ensemble with a single type of classifiers, but built separately on various
    bootstrap samples:** In this methodology, bootstrap samples are drawn from training
    data and, each time, separate models will be fitted (individual models could be
    decision trees, random forest, and so on) on the drawn sample, and all these results
    are combined at the end to create an ensemble. This method suits dealing with
    highly flexible models where variance reduction still improves performance.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用相同类型的分类器集成，但在不同的自助采样上分别构建：** 在这种方法中，从训练数据中抽取自助采样，并且每次在抽取的样本上拟合独立的模型（这些模型可以是决策树、随机森林等），然后将所有这些结果合并在一起，形成集成。这种方法适用于处理高度灵活的模型，在这种情况下，减少方差仍然能提高性能。'
- en: Ensemble of ensembles with different types of classifiers
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用不同类型分类器的集成集成
- en: 'As briefly mentioned in the preceding section, different classifiers will be
    applied on the same training data and the results ensembled either taking majority
    voting or applying another classifier (also known as a meta-classifier) fitted
    on results obtained from individual classifiers. This means, for meta-classifier
    *X*, variables would be model outputs and Y variable would be an actual 0/1 result.
    By doing this, we will obtain the weightage that should be given for each classifier
    and those weights will be applied accordingly to classify unseen observations.
    All three methods of application of ensemble of ensembles are shown here:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一部分简要提到的那样，使用不同的分类器在相同的训练数据上进行拟合，并根据多数投票或应用另一个分类器（也称为元分类器），该分类器是基于从各个分类器得到的结果进行拟合的。也就是说，对于元分类器*X*，变量将是模型输出，而Y变量将是实际的0/1结果。通过这样做，我们将获得每个分类器应该赋予的权重，并据此对未见数据进行分类。这三种集成方法的应用如下：
- en: '**Majority voting or average:** In this method, a simple mode function (classification
    problem) is applied to select the category with the major number of appearances
    out of individual classifiers. Whereas, for regression problems, an average will
    be calculated to compare against actual values.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多数投票或平均值：** 在这种方法中，应用一个简单的众数函数（分类问题）来选择出现次数最多的类别，作为各个分类器的输出。而对于回归问题，将计算平均值并与实际值进行比较。'
- en: '**Method of application of meta-classifiers on outcomes:** Predict actual outcome
    either 0 or 1 from individual classifiers and apply a meta-classifier on top of
    0''s and 1''s. A small problem with this type of approach is that the meta-classifier
    will be a bit brittle and rigid. I mean 0''s and 1''s just gives the result, rather
    than providing exact sensibility (such as probability).'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元分类器在结果上的应用方法：** 从各个分类器预测实际结果0或1，然后在这些0和1的基础上应用元分类器。这个方法的小问题是，元分类器可能会变得有些脆弱和僵化。我的意思是，0和1只给出结果，而没有提供准确的敏感度（比如概率）。'
- en: '**Method of application of meta-classifiers on probabilities:** In this method,
    probabilities are obtained from individual classifiers instead of 0''s and 1''s.
    Applying meta-classifier on probabilities makes this method a bit more flexible
    than the first method. Though users can experiment with both methods to see which
    one performs better. After all, machine learning is all about exploration and
    trial and error methodologies.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元分类器应用于概率的方法：** 在此方法中，从各个分类器获得概率，而不是0和1。应用元分类器于概率，使得这种方法比第一种方法更加灵活。尽管用户可以尝试这两种方法，看看哪种表现更好。毕竟，机器学习就是关于探索和试错的过程。'
- en: 'In the following diagram, the complete flow of model stacking has been described
    with various stages:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图表中，描述了模型堆叠的完整流程，并展示了各个阶段：
- en: '![](img/d4230cd2-6bd4-49ff-b898-787f83c39c09.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4230cd2-6bd4-49ff-b898-787f83c39c09.png)'
- en: '**Steps in the following ensemble with multiple classifiers example**:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**以下是多个分类器集成示例的步骤**：'
- en: Four classifiers have been used separately on training data (logistic regression,
    decision tree, random forest, and AdaBoost)
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四个分类器已分别在训练数据上应用（逻辑回归、决策树、随机森林和AdaBoost）。
- en: Probabilities have been determined for all four classifiers, however, only the
    probability for category 1 has been utilized in meta-classifier due to the reason
    that the probability of class 0 + probability of class 1 = 1, hence only one probability
    is good enough to represent, or else multi-collinearity issues appearing
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经为所有四个分类器确定了概率，然而，只有类别1的概率被用于元分类器，因为类别0的概率加上类别1的概率等于1，因此只取一个概率就足以表示，否则会出现多重共线性问题。
- en: Logistic regression has been used as a meta-classifier to model the relationship
    between four probabilities (obtained from each individual classifier) with respect
    to a final 0/1 outcome
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归被用作元分类器，以建模四个概率（从每个独立分类器获得）与最终0/1结果之间的关系。
- en: 'Coefficients have been calculated for all four variables used in meta-classifier
    and applied on new data for calculating the final aggregated probability for classifying
    observations into the respective categories:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有四个变量的系数已被计算，并在新数据上应用，用于计算最终的聚合概率，将观测值分类到各自的类别中：
- en: '[PRE36]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In the following step, we perform an ensemble of classifiers:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们执行了分类器的集成：
- en: '[PRE37]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In the following step, we take probabilities only for category 1, as it gives
    intuitive sense for high probability and indicates the value towards higher class
    1\. But this should not stop someone if they really want to fit probabilities
    on a 0 class instead. In that case, low probability values are preferred for category
    1, which gives us a little bit of a headache!
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们只取类别1的概率，因为它给出了直观的高概率，并指示了朝向更高类别1的值。但如果有人真的想拟合类别0的概率，也不应因此而止步。在这种情况下，类别1的低概率值更为优选，这让我们有点头疼！
- en: '[PRE38]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/403279b7-d323-47be-bbdd-6db25232febb.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/403279b7-d323-47be-bbdd-6db25232febb.png)'
- en: 'Though code prints **Train**, **Test accuracies**, **Confusion Matrix**, and
    **Classification Reports**, we have not shown them here due to space constraints.
    Users are advised to run and check the results on their computers. Test accuracy
    came as *87.5%*, which is the highest value (the same as gradient boosting results).
    However, by careful tuning, ensembles do give much better results based on adding
    better models and removing models with low weights:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码会打印出**训练**、**测试准确率**、**混淆矩阵**和**分类报告**，但由于篇幅限制，我们在此未显示它们。建议用户在自己的计算机上运行并检查结果。测试准确率为*87.5%*，这是最高值（与梯度提升结果相同）。然而，通过仔细调优，集成方法确实能提供更好的结果，这得益于加入更好的模型并去除低权重模型：
- en: '[PRE39]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/e972efd9-6953-40fa-bba5-075c99e48b1c.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e972efd9-6953-40fa-bba5-075c99e48b1c.png)'
- en: 'It seems that, surprisingly, AdaBoost is dragging down performance of the ensemble.
    A tip is to either change the parameters used in AdaBoost and rerun the entire
    exercise, or remove the AdaBoost classifier from the ensemble and rerun the ensemble
    step to see if there is any improvement in ensemble test accuracy, precision,
    and recall values:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，AdaBoost似乎在拖慢集成的性能。一个建议是要么更改AdaBoost中的参数并重新运行整个过程，要么从集成中移除AdaBoost分类器，再次运行集成步骤，看看集成测试准确率、精确度和召回值是否有所改善：
- en: 'R Code for Ensemble of Ensembles with different Classifiers Applied on HR Attrition
    Data:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在HR流失数据上应用不同分类器的集成方法的R代码：
- en: '[PRE40]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Ensemble of ensembles with bootstrap samples using a single type of classifier
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用单一类型分类器的集成集成，结合自助样本
- en: 'In this methodology, bootstrap samples are drawn from training data and, each
    time, separate models will be fitted (individual models could be decision trees,
    random forest, and so on) on the drawn sample, and all these results are combined
    at the end to create an ensemble. This method suits dealing with highly flexible
    models where variance reduction will still improve performance:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在该方法中，从训练数据中抽取自助样本，每次将会为抽取的样本拟合单独的模型（单个模型可以是决策树、随机森林等），并且所有这些结果最终会被组合成一个集成模型。这种方法适用于处理高度灵活的模型，在这些模型中，减少方差仍然可以提高性能：
- en: '![](img/7349014a-3a2e-47b1-a9d7-5209f4ee1256.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7349014a-3a2e-47b1-a9d7-5209f4ee1256.png)'
- en: 'In the following example, AdaBoost is used as a base classifier and the results
    of individual AdaBoost models are combined using the bagging classifier to generate
    final outcomes. Nonetheless, each AdaBoost is made up of decision trees with a
    depth of 1 (decision stumps). Here, we would like to show that classifier inside
    classifier inside classifier is possible (sounds like the Inception movie though!):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，AdaBoost作为基础分类器，个别AdaBoost模型的结果通过袋装分类器结合生成最终结果。尽管如此，每个AdaBoost都是由深度为1的决策树（决策树桩）组成。在这里，我们想展示的是分类器内的分类器内的分类器是可能的（听起来像电影《盗梦空间》！）：
- en: '[PRE41]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following is the base classifier (decision stump) used in the AdaBoost
    classifier:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是AdaBoost分类器中使用的基本分类器（决策树桩）：
- en: '[PRE42]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Each AdaBoost classifier consists of 500 decision trees with a learning rate
    of 0.05:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 每个AdaBoost分类器由500棵决策树组成，学习率为0.05：
- en: '[PRE43]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The bagging classifier consists of 50 AdaBoost classifiers to ensemble the
    ensembles:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装分类器由50个AdaBoost分类器组成，以集成这些集成模型：
- en: '[PRE44]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](img/cc16a73e-c924-4552-b7f6-5ffa61b480bd.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc16a73e-c924-4552-b7f6-5ffa61b480bd.png)'
- en: The results of the ensemble on AdaBoost have shown some improvements, in which
    the test accuracy obtained is 87.1%, which is almost to that of gradient boosting
    at 87.5%, which is the best value we have seen so far. However, the number of
    1's identified is 25 here, which is greater than Gradient Boosting. Hence, it
    has been proven that an ensemble of ensembles does work! Unfortunately, these
    types of functions are not available in R software, hence we are not writing the
    equivalent R-code here.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在AdaBoost的集成结果中，显示出了一些改进，其中获得的测试准确率为87.1%，几乎与梯度提升的87.5%相当，这是我们目前见过的最佳值。然而，识别出的1的数量为25，超过了梯度提升的数量。因此，已经证明集成集成是有效的！不幸的是，这些类型的功能在R软件中不可用，因此我们在此不写相应的R代码。
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have learned the complete details about tree-based models,
    which are currently the most used in the industry, including individual decision
    trees with grid search and an ensemble of trees such as bagging, random forest,
    boosting (including AdaBoost, gradient boost and XGBoost), and finally, ensemble
    of ensembles, also known as model stacking, for further improving accuracy by
    reducing variance errors by aggregating results further. In model stacking, you
    have learned how to determine the weights for each model, so that decisions can
    be made as to which model to keep in the final results to obtain the best possible
    accuracy.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已学习了关于树基模型的完整细节，这些模型目前是行业中最常用的，包括使用网格搜索的单棵决策树，以及集成树模型，如袋装法（bagging）、随机森林、提升法（包括AdaBoost、梯度提升和XGBoost），最后是集成集成方法，也称为模型堆叠，通过进一步聚合结果来减少方差误差，从而提高准确性。在模型堆叠中，你已经学会了如何为每个模型确定权重，从而做出决策，选择哪些模型保留在最终结果中，以获得最佳的准确性。
- en: In the next chapter, you will be learning k-nearest neighbors and Naive Bayes,
    which are less computationally intensive than tree-based models. The Naive Bayes
    model will be explained with an NLP use case. In fact, Naive Bayes and SVM are
    often used where variables (number of dimensions) are very high in number to classify.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习k近邻算法和朴素贝叶斯，它们的计算开销比树基模型小。朴素贝叶斯模型将在一个NLP用例中进行解释。事实上，朴素贝叶斯和支持向量机（SVM）通常用于变量（维度数）非常高的分类问题。
