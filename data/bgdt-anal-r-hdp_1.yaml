- en: Chapter 1. Getting Ready to Use R and Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 章：开始使用 R 和 Hadoop
- en: 'The first chapter has been bundled with several topics on R and Hadoop basics
    as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 第一章已经包含了 R 和 Hadoop 基础的多个主题，如下所示：
- en: R Installation, features, and data modeling
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 安装、功能和数据建模
- en: Hadoop installation, features, and components
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 安装、功能和组件
- en: In the preface, we introduced you to R and Hadoop. This chapter will focus on
    getting you up and running with these two technologies. Until now, R has been
    used mainly for statistical analysis, but due to the increasing number of functions
    and packages, it has become popular in several fields, such as machine learning,
    visualization, and data operations. R will not load all data (Big Data) into machine
    memory. So, Hadoop can be chosen to load the data as Big Data. Not all algorithms
    work across Hadoop, and the algorithms are, in general, not R algorithms. Despite
    this, analytics with R have several issues related to large data. In order to
    analyze the dataset, R loads it into the memory, and if the dataset is large,
    it will fail with exceptions such as "cannot allocate vector of size x". Hence,
    in order to process large datasets, the processing power of R can be vastly magnified
    by combining it with the power of a Hadoop cluster. Hadoop is very a popular framework
    that provides such parallel processing capabilities. So, we can use R algorithms
    or analysis processing over Hadoop clusters to get the work done.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前言中，我们向你介绍了 R 和 Hadoop。本章将重点帮助你开始使用这两项技术。到目前为止，R 主要用于统计分析，但由于功能和包的不断增加，它在多个领域变得流行，例如机器学习、可视化和数据操作。R
    不会将所有数据（大数据）加载到机器内存中，因此，可以选择 Hadoop 来加载数据作为大数据。并非所有算法都能在 Hadoop 上运行，而且这些算法通常不是
    R 算法。尽管如此，使用 R 进行分析时，仍然存在与大数据相关的若干问题。为了分析数据集，R 会将其加载到内存中，而如果数据集很大，通常会因“无法分配大小为
    x 的向量”等异常而失败。因此，为了处理大数据集，结合 R 与 Hadoop 集群的计算能力，可以大大增强 R 的处理能力。Hadoop 是一个非常流行的框架，提供了并行处理能力。因此，我们可以在
    Hadoop 集群上使用 R 算法或分析处理来完成工作。
- en: '![Getting Ready to Use R and Hadoop](img/3282OS_01_00.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![开始使用 R 和 Hadoop](img/3282OS_01_00.jpg)'
- en: If we think about a combined RHadoop system, R will take care of data analysis
    operations with the preliminary functions, such as data loading, exploration,
    analysis, and visualization, and Hadoop will take care of parallel data storage
    as well as computation power against distributed data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑将 R 和 Hadoop 结合使用，R 将负责数据分析操作，包括数据加载、探索、分析和可视化等初步功能，而 Hadoop 将负责并行数据存储及处理分布式数据的计算能力。
- en: Prior to the advent of affordable Big Data technologies, analysis used to be
    run on limited datasets on a single machine. Advanced machine learning algorithms
    are very effective when applied to large datasets, and this is possible only with
    large clusters where data can be stored and processed with distributed data storage
    systems. In the next section, we will see how R and Hadoop can be installed on
    different operating systems and the possible ways to link R and Hadoop.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在可负担得起的大数据技术出现之前，分析通常是在单台计算机上的有限数据集上运行的。当应用到大数据集时，高级机器学习算法非常有效，而这仅能在数据可以通过分布式数据存储系统进行存储和处理的大型集群中实现。在接下来的章节中，我们将介绍如何在不同操作系统上安装
    R 和 Hadoop，以及将 R 和 Hadoop 链接的可能方式。
- en: Installing R
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 R
- en: You can download the appropriate version by visiting the official R website.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以访问 R 官方网站下载合适的版本。
- en: Here are the steps provided for three different operating systems. We have considered
    Windows, Linux, and Mac OS for R installation. Download the latest version of
    R as it will have all the latest patches and resolutions to the past bugs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供了三种不同操作系统的步骤。我们已考虑了 Windows、Linux 和 Mac OS 来安装 R。下载最新版本的 R，因为它将包含所有最新的修补程序和解决过去
    bug 的版本。
- en: 'For Windows, follow the given steps:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Windows，按照以下步骤进行操作：
- en: Navigate to [www.r-project.org](http://www.r-project.org).
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [www.r-project.org](http://www.r-project.org)。
- en: Click on the **CRAN** section, select **CRAN mirror**, and select your Windows
    OS (stick to Linux; Hadoop is almost always used in a Linux environment).
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **CRAN** 部分，选择 **CRAN 镜像**，然后选择你的 Windows 操作系统（坚持使用 Linux；Hadoop 几乎总是在 Linux
    环境中使用）。
- en: Download the latest R version from the mirror.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从镜像站点下载最新的 R 版本。
- en: Execute the downloaded `.exe` to install R.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行下载的 `.exe` 文件以安装 R。
- en: 'For Linux-Ubuntu, follow the given steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Linux-Ubuntu，按照以下步骤进行操作：
- en: Navigate to [www.r-project.org](http://www.r-project.org).
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [www.r-project.org](http://www.r-project.org)。
- en: Click on the **CRAN** section, select **CRAN mirror**, and select your OS.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**CRAN**部分，选择**CRAN 镜像**，然后选择你的操作系统。
- en: In the `/etc/apt/sources.list` file, add the CRAN `<mirror>` entry.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `/etc/apt/sources.list` 文件中添加 CRAN `<mirror>` 条目。
- en: Download and update the package lists from the repositories using the `sudo
    apt-get update` command.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `sudo apt-get update` 命令从仓库下载并更新包列表。
- en: Install R system using the `sudo apt-get install r-base` command.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `sudo apt-get install r-base` 命令安装 R 系统。
- en: 'For Linux-RHEL/CentOS, follow the given steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Linux-RHEL/CentOS，请按照以下步骤操作：
- en: Navigate to [www.r-project.org](http://www.r-project.org).
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [www.r-project.org](http://www.r-project.org)。
- en: Click on **CRAN**, select **CRAN mirror**, and select Red Hat OS.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**CRAN**，选择**CRAN 镜像**，然后选择 Red Hat 操作系统。
- en: Download the `R-*core-*.rpm` file.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 `R-*core-*.rpm` 文件。
- en: Install the `.rpm` package using the `rpm -ivh R-*core-*.rpm` command.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `rpm -ivh R-*core-*.rpm` 命令安装 `.rpm` 包。
- en: Install R system using `sudo yum install R`.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `sudo yum install R` 命令安装 R 系统。
- en: 'For Mac, follow the given steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Mac，请按照以下步骤操作：
- en: Navigate to [www.r-project.org](http://www.r-project.org).
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [www.r-project.org](http://www.r-project.org)。
- en: Click on **CRAN**, select **CRAN mirror**, and select your OS.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**CRAN**，选择**CRAN 镜像**，然后选择你的操作系统。
- en: 'Download the following files: `pkg`, `gfortran-*.dmg`, and `tcltk-*.dmg`.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载以下文件：`pkg`、`gfortran-*.dmg` 和 `tcltk-*.dmg`。
- en: Install the `R-*.pkg` file.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `R-*.pkg` 文件。
- en: Then, install the `gfortran-*.dmg` and `tcltk-*.dmg` files.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，安装 `gfortran-*.dmg` 和 `tcltk-*.dmg` 文件。
- en: After installing the base R package, it is advisable to install RStudio, which
    is a powerful and intuitive **Integrated Development Environment** (**IDE**) for
    R.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装基础 R 包后，建议安装 RStudio，这是一个功能强大且直观的**集成开发环境**（**IDE**）用于 R 语言。
- en: Tip
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: We can use R distribution of Revolution Analytics as a Modern Data analytics
    tool for statistical computing and predictive analytics, which is available in
    free as well as premium versions. Hadoop integration is also available to perform
    Big Data analytics.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Revolution Analytics 提供的 R 发行版作为现代数据分析工具，进行统计计算和预测分析，该工具提供免费版和付费版。还支持
    Hadoop 集成，可进行大数据分析。
- en: Installing RStudio
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 RStudio
- en: 'To install RStudio, perform the following steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 RStudio，请执行以下步骤：
- en: Navigate to [http://www.rstudio.com/ide/download/desktop](http://www.rstudio.com/ide/download/desktop.).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [http://www.rstudio.com/ide/download/desktop](http://www.rstudio.com/ide/download/desktop)。
- en: Download the latest version of RStudio for your operating system.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载适用于你的操作系统的最新版本 RStudio。
- en: Execute the installer file and install RStudio.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行安装程序文件并安装 RStudio。
- en: The RStudio organization and user community has developed a lot of R packages
    for graphics and visualization, such as `ggplot2`, `plyr`, `Shiny`, `Rpubs`, and
    `devtools`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: RStudio 组织和用户社区已经开发了许多用于图形和可视化的 R 包，如 `ggplot2`、`plyr`、`Shiny`、`Rpubs` 和 `devtools`。
- en: Understanding the features of R language
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 R 语言的特性
- en: There are over 3,000 R packages and the list is growing day by day. It would
    be beyond the scope of any book to even attempt to explain all these packages.
    This book focuses only on the key features of R and the most frequently used and
    popular packages.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有超过 3,000 个 R 包，并且这个数量每天都在增长。试图在任何一本书中解释所有这些包将超出其范围。本书只关注 R 的关键特性以及最常用和最受欢迎的包。
- en: Using R packages
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 R 包
- en: R packages are self-contained units of R functionality that can be invoked as
    functions. A good analogy would be a `.jar` file in Java. There is a vast library
    of R packages available for a very wide range of operations ranging from statistical
    operations and machine learning to rich graphic visualization and plotting. Every
    package will consist of one or more R functions. An R package is a re-usable entity
    that can be shared and used by others. R users can install the package that contains
    the functionality they are looking for and start calling the functions in the
    package. A comprehensive list of these packages can be found at [http://cran.r-project.org/](http://cran.r-project.org/)
    called **Comprehensive R Archive Network** (**CRAN**).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: R 包是 R 功能的自包含单元，可以作为函数调用。一个很好的类比是 Java 中的 `.jar` 文件。R 包库庞大，涵盖了各种操作，从统计运算、机器学习到丰富的图形可视化和绘图。每个包将包含一个或多个
    R 函数。R 包是一个可重用的实体，可以被他人共享和使用。R 用户可以安装包含所需功能的包并开始调用包中的函数。一个全面的包列表可以在 [http://cran.r-project.org/](http://cran.r-project.org/)
    中找到，称为**综合 R 档案网络**（**CRAN**）。
- en: Performing data operations
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行数据操作
- en: 'R enables a wide range of operations. Statistical operations, such as mean,
    min, max, probability, distribution, and regression. Machine learning operations,
    such as linear regression, logistic regression, classification, and clustering.
    Universal data processing operations are as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: R 可以进行广泛的操作。统计操作，如均值、最小值、最大值、概率、分布和回归。机器学习操作，如线性回归、逻辑回归、分类和聚类。通用数据处理操作如下：
- en: '**Data cleaning**: This option is to clean massive datasets'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清理**：此选项用于清理大量数据集。'
- en: '**Data exploration**: This option is to explore all the possible values of
    datasets'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据探索**：此选项用于探索数据集的所有可能值。'
- en: '**Data analysis**: This option is to perform analytics on data with descriptive
    and predictive analytics data visualization, that is, visualization of analysis
    output programming'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分析**：此选项用于对数据进行描述性和预测性分析数据可视化，即分析输出编程的可视化。'
- en: To build an effective analytics application, sometimes we need to use the online
    **Application Programming Interface** (**API**) to dig up the data, analyze it
    with expedient services, and visualize it by third-party services. Also, to automate
    the data analysis process, programming will be the most useful feature to deal
    with.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个有效的分析应用程序，有时我们需要使用在线 **应用程序编程接口**（**API**）来挖掘数据，通过便捷的服务进行分析，并通过第三方服务进行可视化。此外，要自动化数据分析过程，编程将是最有用的功能。
- en: R has its own programming language to operate data. Also, the available package
    can help to integrate R with other programming features. R supports object-oriented
    programming concepts. It is also capable of integrating with other programming
    languages, such as Java, PHP, C, and C++. There are several packages that will
    act as middle-layer programming features to aid in data analytics, which are similar
    to `sqldf`, `httr`, `RMongo`, `RgoogleMaps`, `RGoogleAnalytics`, and `google-prediction-api-r-client`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: R 拥有自己的编程语言来处理数据。此外，可用的包可以帮助将 R 与其他编程功能集成。R 支持面向对象编程概念。它还能够与其他编程语言（如 Java、PHP、C
    和 C++）进行集成。有多个包作为中间层编程功能，帮助进行数据分析，这些包类似于 `sqldf`、`httr`、`RMongo`、`RgoogleMaps`、`RGoogleAnalytics`
    和 `google-prediction-api-r-client`。
- en: Increasing community support
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增强社区支持
- en: As the number of R users are escalating, the groups related to R are also increasing.
    So, R learners or developers can easily connect and get their uncertainty solved
    with the help of several R groups or communities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 R 用户数量的增加，相关的 R 小组也在不断增多。因此，R 学习者或开发者可以轻松地与其他人连接，并借助多个 R 小组或社区解决他们的疑惑。
- en: 'The following are many popular sources that can be found useful:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以找到有用的流行资源：
- en: '**R mailing list**: This is an official R group created by R project owners.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R 邮件列表**：这是 R 项目所有者创建的官方 R 小组。'
- en: '**R blogs**: R has countless bloggers who are writing on several R applications.
    One of the most popular blog websites is [http://www.r-bloggers.com/](http://www.r-bloggers.com/)
    where all the bloggers contribute their blogs.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R 博客**：R 拥有无数的博客作者，他们在多个 R 应用领域进行写作。最受欢迎的博客网站之一是 [http://www.r-bloggers.com/](http://www.r-bloggers.com/)，所有的博客作者都会在该网站贡献自己的博客。'
- en: '**Stack overflow**: This is a great technical knowledge sharing platform where
    the programmers can post their technical queries and enthusiast programmers suggest
    a solution. For more information, visit [http://stats.stackexchange.com/](http://stats.stackexchange.com/).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Stack Overflow**：这是一个很棒的技术知识分享平台，程序员可以在这里发布技术问题，热心的程序员会提供解决方案。欲了解更多信息，请访问
    [http://stats.stackexchange.com/](http://stats.stackexchange.com/)。'
- en: '**Groups**: There are many other groups existing on LinkedIn and Meetup where
    professionals across the world meet to discuss their problems and innovative ideas.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小组**：在 LinkedIn 和 Meetup 上还存在许多其他小组，全球的专业人士会聚集在一起讨论他们的问题和创新想法。'
- en: '**Books**: There are also lot of books about R. Some of the popular books are
    *R in Action*, by *Rob Kabacoff*, *Manning Publications*, *R in a Nutshell*, by
    *Joseph Adler*, *O''Reilly Media*, *R and Data Mining*, by *Yanchang Zhao*, *Academic
    Press*, and *R Graphs Cookbook*, by *Hrishi Mittal*, *Packt Publishing*.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**书籍**：关于 R 的书籍也有很多。其中一些受欢迎的书籍包括 *《R 实战》*，作者 *Rob Kabacoff*，*Manning 出版社*；*《R
    速查手册》*，作者 *Joseph Adler*，*O''Reilly Media*；*《R 与数据挖掘》*，作者 *Yanchang Zhao*，*Academic
    Press*；以及 *《R 图形 cookbook》*，作者 *Hrishi Mittal*，*Packt Publishing*。'
- en: Performing data modeling in R
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 R 中进行数据建模
- en: Data modeling is a machine learning technique to identify the hidden pattern
    from the historical dataset, and this pattern will help in future value prediction
    over the same data. This techniques highly focus on past user actions and learns
    their taste. Most of these data modeling techniques have been adopted by many
    popular organizations to understand the behavior of their customers based on their
    past transactions. These techniques will analyze data and predict for the customers
    what they are looking for. Amazon, Google, Facebook, eBay, LinkedIn, Twitter,
    and many other organizations are using data mining for changing the definition
    applications.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据建模是一种机器学习技术，用于从历史数据集中识别隐藏的模式，这些模式将有助于在相同数据上进行未来值预测。这些技术高度关注过去用户的行为并学习他们的偏好。许多流行的组织已经采纳了这些数据建模技术，以根据客户的过去交易来了解他们的行为。这些技术将分析数据并预测客户在寻找什么。Amazon、Google、Facebook、eBay、LinkedIn、Twitter等众多组织正在使用数据挖掘来改变定义应用程序。
- en: 'The most common data mining techniques are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的数据挖掘技术如下：
- en: '**Regression**: In statistics, regression is a classic technique to identify
    the scalar relationship between two or more variables by fitting the state line
    on the variable values. That relationship will help to predict the variable value
    for future events. For example, any variable y can be modeled as linear function
    of another variable x with the formula *y = mx+c*. Here, x is the predictor variable,
    y is the response variable, m is slope of the line, and c is the intercept. Sales
    forecasting of products or services and predicting the price of stocks can be
    achieved through this regression. R provides this regression feature via the `lm`
    method, which is by default present in R.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：在统计学中，回归是一种经典技术，用于通过拟合变量值的状态线来识别两个或多个变量之间的标量关系。这种关系将帮助预测未来事件中变量的值。例如，任何变量y可以被建模为另一个变量x的线性函数，公式为*y
    = mx+c*。其中，x是预测变量，y是响应变量，m是直线的斜率，c是截距。产品或服务的销售预测以及股票价格预测可以通过这种回归方法实现。R通过`lm`方法提供了这一回归功能，默认情况下，该方法已存在于R中。'
- en: '**Classification**: This is a machine-learning technique used for labeling
    the set of observations provided for training examples. With this, we can classify
    the observations into one or more labels. The likelihood of sales, online fraud
    detection, and cancer classification (for medical science) are common applications
    of classification problems. Google Mail uses this technique to classify e-mails
    as spam or not. Classification features can be served by `glm`, `glmnet`, `ksvm`,
    `svm`, and `randomForest` in R.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：这是一种机器学习技术，用于标记提供的训练示例集的观察结果。通过这种方法，我们可以将观察结果分类为一个或多个标签。销售可能性、在线欺诈检测和癌症分类（医学科学）是分类问题的常见应用。Google
    Mail使用此技术将电子邮件分类为垃圾邮件或非垃圾邮件。在R中，`glm`、`glmnet`、`ksvm`、`svm`和`randomForest`方法可以用于分类功能。'
- en: '**Clustering**: This technique is all about organizing similar items into groups
    from the given collection of items. User segmentation and image compression are
    the most common applications of clustering. Market segmentation, social network
    analysis, organizing the computer clustering, and astronomical data analysis are
    applications of clustering. Google News uses these techniques to group similar
    news items into the same category. Clustering can be achieved through the `knn`,
    `kmeans`, `dist`, `pvclust`, and `Mclust` methods in R.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：这种技术的核心是将相似的项目从给定的集合中组织成组。用户细分和图像压缩是聚类技术的最常见应用。市场细分、社交网络分析、计算机集群组织以及天文数据分析都是聚类的应用。Google
    新闻利用这些技术将相似的新闻项分组到同一类别中。在R中，可以通过`knn`、`kmeans`、`dist`、`pvclust`和`Mclust`方法实现聚类。'
- en: '**Recommendation**: The recommendation algorithms are used in recommender systems
    where these systems are the most immediately recognizable machine learning techniques
    in use today. Web content recommendations may include similar websites, blogs,
    videos, or related content. Also, recommendation of online items can be helpful
    for cross-selling and up-selling. We have all seen online shopping portals that
    attempt to recommend books, mobiles, or any items that can be sold on the Web
    based on the user''s past behavior. Amazon is a well-known e-commerce portal that
    generates 29 percent of sales through recommendation systems. Recommender systems
    can be implemented via `Recommender()`with the `recommenderlab` package in R.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐**：推荐算法用于推荐系统，而这些系统是当今最容易识别的机器学习技术。Web 内容推荐可能包括类似的网站、博客、视频或相关内容。此外，在线物品的推荐对于交叉销售和追加销售也非常有帮助。我们都见过在线购物门户，尝试根据用户的过去行为推荐书籍、手机或任何可以在网上销售的商品。Amazon
    是一个知名的电子商务门户，通过推荐系统生成了 29% 的销售额。推荐系统可以通过 `Recommender()` 和 R 中的 `recommenderlab`
    包来实现。'
- en: Installing Hadoop
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 Hadoop
- en: Now, we presume that you are aware of R, what it is, how to install it, what
    it's key features are, and why you may want to use it. Now we need to know the
    limitations of R (this is a better introduction to Hadoop). Before processing
    the data; R needs to load the data into **random access memory** (**RAM**). So,
    the data needs to be smaller than the available machine memory. For data that
    is larger than the machine memory, we consider it as Big Data (only in our case
    as there are many other definitions of Big Data).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们假设你已经了解了 R，它是什么，如何安装，主要特点是什么，以及为什么你可能想要使用它。接下来，我们需要了解 R 的局限性（这也是介绍 Hadoop
    的一个更好的方式）。在处理数据之前，R 需要将数据加载到 **随机存取存储器** (**RAM**) 中。因此，数据需要小于可用的机器内存。对于大于机器内存的数据，我们将其视为大数据（仅在我们的情况下，因为大数据有很多其他定义）。
- en: To avoid this Big Data issue, we need to scale the hardware configuration; however,
    this is a temporary solution. To get this solved, we need to get a Hadoop cluster
    that is able to store it and perform parallel computation across a large computer
    cluster. Hadoop is the most popular solution. Hadoop is an open source Java framework,
    which is the top level project handled by the Apache software foundation. Hadoop
    is inspired by the Google filesystem and MapReduce, mainly designed for operating
    on Big Data by distributed processing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免大数据问题，我们需要扩展硬件配置；然而，这只是一个临时解决方案。要解决这个问题，我们需要一个能够存储数据并在大型计算机集群上执行并行计算的 Hadoop
    集群。Hadoop 是最流行的解决方案。Hadoop 是一个开源的 Java 框架，是由 Apache 软件基金会处理的顶级项目。Hadoop 的灵感来自
    Google 文件系统和 MapReduce，主要设计用于通过分布式处理操作大数据。
- en: Hadoop mainly supports Linux operating systems. To run this on Windows, we need
    to use VMware to host Ubuntu within the Windows OS. There are many ways to use
    and install Hadoop, but here we will consider the way that supports R best. Before
    we combine R and Hadoop, let us understand what Hadoop is.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 主要支持 Linux 操作系统。在 Windows 上运行 Hadoop，我们需要使用 VMware 在 Windows 操作系统中托管
    Ubuntu。有许多使用和安装 Hadoop 的方法，但在这里我们将考虑最适合 R 的方式。在结合 R 和 Hadoop 之前，我们先来了解一下 Hadoop
    是什么。
- en: Tip
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Machine learning contains all the data modeling techniques that can be explored
    with the web link [http://en.wikipedia.org/wiki/Machine_learning](http://en.wikipedia.org/wiki/Machine_learning).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习包含所有数据建模技术，可以通过这个网页链接 [http://en.wikipedia.org/wiki/Machine_learning](http://en.wikipedia.org/wiki/Machine_learning)
    进行探索。
- en: The structure blog on Hadoop installation by Michael Noll can be found at [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Michael Noll 撰写的 Hadoop 安装结构博客可以在 [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/)
    上找到。
- en: Understanding different Hadoop modes
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解不同的 Hadoop 模式
- en: 'Hadoop is used with three different modes:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 有三种不同的模式：
- en: '**The standalone mode**: In this mode, you do not need to start any Hadoop
    daemons. Instead, just call `~/Hadoop-directory/bin/hadoop` that will execute
    a Hadoop operation as a single Java process. This is recommended for testing purposes.
    This is the default mode and you don''t need to configure anything else. All daemons,
    such as NameNode, DataNode, JobTracker, and TaskTracker run in a single Java process.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立模式**：在此模式下，您无需启动任何 Hadoop 守护进程。只需调用 `~/Hadoop-directory/bin/hadoop`，它将作为单一
    Java 进程执行 Hadoop 操作。此模式推荐用于测试。它是默认模式，您无需配置任何其他内容。所有守护进程，如 NameNode、DataNode、JobTracker
    和 TaskTracker 都在一个 Java 进程中运行。'
- en: '**The pseudo mode**: In this mode, you configure Hadoop for all the nodes.
    A separate **Java Virtual Machine** (**JVM**) is spawned for each of the Hadoop
    components or daemons like mini cluster on a single host.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伪模式**：在此模式下，您为所有节点配置 Hadoop。为每个 Hadoop 组件或守护进程（如单主机上的迷你集群）启动单独的 **Java 虚拟机**（**JVM**）。'
- en: '**The full distributed mode**: In this mode, Hadoop is distributed across multiple
    machines. Dedicated hosts are configured for Hadoop components. Therefore, separate
    JVM processes are present for all daemons.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全分布式模式**：在此模式下，Hadoop 会分布到多台机器上。为 Hadoop 组件配置专用主机。因此，每个守护进程都有独立的 JVM 进程。'
- en: Understanding Hadoop installation steps
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Hadoop 安装步骤
- en: Hadoop can be installed in several ways; we will consider the way that is better
    to integrate with R. We will choose Ubuntu OS as it is easy to install and access
    it.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 可以通过多种方式安装；我们将考虑与 R 集成更好的方式。我们将选择 Ubuntu 操作系统，因为它易于安装和访问。
- en: Installing Hadoop on Linux, Ubuntu flavor (single and multinode cluster).
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Linux 上安装 Hadoop，Ubuntu 版本（单节点和多节点集群）。
- en: Installing Cloudera Hadoop on Ubuntu.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Ubuntu 上安装 Cloudera Hadoop。
- en: Installing Hadoop on Linux, Ubuntu flavor (single node cluster)
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Linux 上安装 Hadoop，Ubuntu 版本（单节点集群）
- en: 'To install Hadoop over Ubuntu OS with the pseudo mode, we need to meet the
    following prerequisites:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Ubuntu 操作系统上以伪模式安装 Hadoop，我们需要满足以下先决条件：
- en: Sun Java 6
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun Java 6
- en: Dedicated Hadoop system user
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专用 Hadoop 系统用户
- en: Configuring SSH
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 SSH
- en: Disabling IPv6
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁用 IPv6
- en: Tip
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The provided Hadoop installation will be supported with Hadoop MRv1.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的 Hadoop 安装将支持 Hadoop MRv1。
- en: 'Follow the given steps to install Hadoop:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤安装 Hadoop：
- en: Download the latest Hadoop sources from the Apache software foundation. Here
    we have considered Apache Hadoop 1.0.3, whereas the latest version is 1.1.x.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Apache 软件基金会下载最新的 Hadoop 源代码。这里我们考虑的是 Apache Hadoop 1.0.3，而最新版本是 1.1.x。
- en: '[PRE0]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add the `$JAVA_HOME` and `$HADOOP_HOME` variables to the`.bashrc` file of Hadoop
    system user and the updated `.bashrc` file looks as follows:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `$JAVA_HOME` 和 `$HADOOP_HOME` 变量添加到 Hadoop 系统用户的 `.bashrc` 文件中，更新后的 `.bashrc`
    文件如下所示：
- en: '[PRE1]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Update the Hadoop configuration files with the `conf/*-site.xml` format.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `conf/*-site.xml` 格式更新 Hadoop 配置文件。
- en: 'Finally, the three files will look as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，三个文件将如下所示：
- en: '`conf/core-site.xml`:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conf/core-site.xml`：'
- en: '[PRE2]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`conf/mapred-site.xml`:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conf/mapred-site.xml`：'
- en: '[PRE3]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`conf/hdfs-site.xml`:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conf/hdfs-site.xml`：'
- en: '[PRE4]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After completing the editing of these configuration files, we need to set up
    the distributed filesystem across the Hadoop clusters or node.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑完这些配置文件后，我们需要在 Hadoop 集群或节点之间设置分布式文件系统。
- en: 'Format **Hadoop Distributed File System** (**HDFS**) via NameNode by using
    the following command line:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过以下命令格式化 **Hadoop 分布式文件系统**（**HDFS**）：
- en: '[PRE5]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Start your single node cluster by using the following command line:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令行启动您的单节点集群：
- en: '[PRE6]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Tip
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Downloading the example code**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载示例代码**'
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从您的账户在 [http://www.packtpub.com](http://www.packtpub.com) 下载所有您购买的 Packt
    图书的示例代码文件。如果您在其他地方购买了此书，您可以访问 [http://www.packtpub.com/support](http://www.packtpub.com/support)
    并注册，将文件直接通过电子邮件发送给您。
- en: Installing Hadoop on Linux, Ubuntu flavor (multinode cluster)
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Linux 上安装 Hadoop，Ubuntu 版本（多节点集群）
- en: We learned how to install Hadoop on a single node cluster. Now we will see how
    to install Hadoop on a multinode cluster (the full distributed mode).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何在单节点集群上安装 Hadoop。现在我们将看到如何在多节点集群上安装 Hadoop（完全分布式模式）。
- en: For this, we need several nodes configured with a single node Hadoop cluster.
    To install Hadoop on multinodes, we need to have that machine configured with
    a single node Hadoop cluster as described in the last section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要多个节点配置一个单节点 Hadoop 集群。要在多节点上安装 Hadoop，我们需要按照上一节中描述的方式先配置好单节点 Hadoop 集群。
- en: 'After getting the single node Hadoop cluster installed, we need to perform
    the following steps:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装完单节点 Hadoop 集群后，我们需要执行以下步骤：
- en: In the networking phase, we are going to use two nodes for setting up a full
    distributed Hadoop mode. To communicate with each other, the nodes need to be
    in the same network in terms of software and hardware configuration.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网络配置阶段，我们将使用两个节点来设置完整的分布式 Hadoop 模式。为了彼此通信，这些节点需要在软件和硬件配置上处于同一网络中。
- en: Among these two, one of the nodes will be considered as master and the other
    will be considered as slave. So, for performing Hadoop operations, master needs
    to be connected to slave. We will enter `192.168.0.1` in the master machine and
    `192.168.0.2` in the slave machine.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这两个节点中，一个将被视为主节点，另一个将被视为从节点。因此，为了执行 Hadoop 操作，主节点需要连接到从节点。我们将在主机上输入 `192.168.0.1`，在从机上输入
    `192.168.0.2`。
- en: Update the `/etc/hosts` directory in both the nodes. It will look as `192.168.0.1
    master` and `192.168.0.2 slave`.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新两个节点中的`/etc/hosts`目录。它将显示为 `192.168.0.1 master` 和 `192.168.0.2 slave`。
- en: Tip
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: You can perform the **Secure Shell** (**SSH**) setup similar to what we did
    for a single node cluster setup. For more details, visit [http://www.michael-noll.com](http://www.michael-noll.com).
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以像我们在单节点集群设置中做的那样进行**安全外壳**（**SSH**）设置。欲了解更多详情，请访问 [http://www.michael-noll.com](http://www.michael-noll.com)。
- en: 'Updating `conf/*-site.xml`: We must change all these configuration files in
    all of the nodes.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 `conf/*-site.xml`：我们必须在所有节点中更改这些配置文件。
- en: '`conf/core-site.xml` and `conf/mapred-site.xml`: In the single node setup,
    we have updated these files. So, now we need to just replace `localhost` by `master`
    in the value tag.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conf/core-site.xml` 和 `conf/mapred-site.xml`：在单节点设置中，我们已经更新了这些文件。所以现在我们只需要将值标签中的
    `localhost` 替换为 `master`。'
- en: '`conf/hdfs-site.xml`: In the single node setup, we have set the value of `dfs.replication`
    as `1`. Now we need to update this as `2`.'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conf/hdfs-site.xml`：在单节点设置中，我们将 `dfs.replication` 的值设置为 `1`。现在我们需要将其更新为 `2`。'
- en: 'In the formatting HDFS phase, before we start the multinode cluster, we need
    to format HDFS with the following command (from the master node):'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在格式化 HDFS 阶段，在我们启动多节点集群之前，我们需要使用以下命令格式化 HDFS（从主节点执行）：
- en: '[PRE7]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we have completed all the steps to install the multinode Hadoop cluster.
    To start the Hadoop clusters, we need to follow these steps:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经完成了安装多节点 Hadoop 集群的所有步骤。要启动 Hadoop 集群，我们需要遵循以下步骤：
- en: 'Start HDFS daemons:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 HDFS 守护进程：
- en: '[PRE8]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Start MapReduce daemons:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 MapReduce 守护进程：
- en: '[PRE9]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Alternatively, we can start all the daemons with a single command:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，我们可以通过一个命令启动所有守护进程：
- en: '[PRE10]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To stop all these daemons, fire:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要停止所有这些守护进程，执行：
- en: '[PRE11]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: These installation steps are reproduced after being inspired by the blogs ([http://www.michael-noll.com](http://www.michael-noll.com))
    of Michael Noll, who is a researcher and Software Engineer based in Switzerland,
    Europe. He works as a Technical lead for a large scale computing infrastructure
    on the Apache Hadoop stack at VeriSign.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这些安装步骤是受启发于 Michael Noll 的博客（[http://www.michael-noll.com](http://www.michael-noll.com)）的，他是位于欧洲瑞士的研究员和软件工程师。他在
    VeriSign 担任 Apache Hadoop 堆栈的大规模计算基础设施技术负责人。
- en: Now the Hadoop cluster has been set up on your machines. For the installation
    of the same Hadoop cluster on single node or multinode with extended Hadoop components,
    try the Cloudera tool.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Hadoop 集群已经在你的机器上设置完成。要在单节点或多节点上安装相同的 Hadoop 集群并扩展 Hadoop 组件，可以尝试使用 Cloudera
    工具。
- en: Installing Cloudera Hadoop on Ubuntu
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Ubuntu 上安装 Cloudera Hadoop
- en: '**Cloudera Hadoop** (**CDH**) is Cloudera''s open source distribution that
    targets enterprise class deployments of Hadoop technology. Cloudera is also a
    sponsor of the Apache software foundation. CDH is available in two versions: CDH3
    and CDH4\. To install one of these, you must have Ubuntu with either 10.04 LTS
    or 12.04 LTS (also, you can try CentOS, Debian, and Red Hat systems). Cloudera
    manager will make this installation easier for you if you are installing a Hadoop
    on cluster of computers, which provides GUI-based Hadoop and its component installation
    over a whole cluster. This tool is very much recommended for large clusters.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**Cloudera Hadoop** (**CDH**) 是 Cloudera 的开源发行版，旨在面向企业级部署 Hadoop 技术。Cloudera
    也是 Apache 软件基金会的赞助商。CDH 提供两个版本：CDH3 和 CDH4\. 要安装其中之一，您必须使用带有 10.04 LTS 或 12.04
    LTS 的 Ubuntu（此外，您还可以尝试 CentOS、Debian 和 Red Hat 系统）。如果您在多台计算机的集群上安装 Hadoop，Cloudera
    管理器将使安装过程变得更加容易，它提供基于 GUI 的 Hadoop 和其组件的安装。强烈建议在大型集群中使用该工具。'
- en: 'We need to meet the following prerequisites:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要满足以下先决条件：
- en: Configuring SSH
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 SSH
- en: 'OS with the following criteria:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统应符合以下标准：
- en: Ubuntu 10.04 LTS or 12.04 LTS with 64 bit
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ubuntu 10.04 LTS 或 12.04 LTS（64 位）
- en: Red Hat Enterprise Linux 5 or 6
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Red Hat Enterprise Linux 5 或 6
- en: CentOS 5 or 6
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CentOS 5 或 6
- en: Oracle Enterprise Linux 5
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oracle Enterprise Linux 5
- en: SUSE Linux Enterprise server 11 (SP1 or lasso)
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SUSE Linux Enterprise Server 11（SP1 或 Lasso）
- en: Debian 6.0
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Debian 6.0
- en: 'The installation steps are as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 安装步骤如下：
- en: 'Download and run the Cloudera manager installer: To initialize the Cloudera
    manager installation process, we need to first download the `cloudera-manager-installer.bin`
    file from the download section of the Cloudera website. After that, store it at
    the cluster so that all the nodes can access this. Allow ownership for execution
    permission of `cloudera-manager-installer.bin` to the user. Run the following
    command to start execution.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并运行 Cloudera 管理器安装程序：要初始化 Cloudera 管理器安装过程，首先需要从 Cloudera 网站的下载区下载 `cloudera-manager-installer.bin`
    文件。下载后，将其存储在集群中，以便所有节点都可以访问此文件。允许用户执行 `cloudera-manager-installer.bin` 文件。运行以下命令开始执行。
- en: '[PRE12]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Read the Cloudera manager **Readme** and then click on **Next**.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读 Cloudera 管理器 **Readme** 文件，然后点击 **下一步**。
- en: 'Start the Cloudera manager admin console: The Cloudera manager admin console
    allows you to use Cloudera manager to install, manage, and monitor Hadoop on your
    cluster. After accepting the license from the Cloudera service provider, you need
    to traverse to your local web browser by entering `http://localhost:7180` in your
    address bar. You can also use any of the following browsers:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Cloudera 管理器管理员控制台：Cloudera 管理器管理员控制台允许您使用 Cloudera 管理器在集群上安装、管理和监控 Hadoop。接受
    Cloudera 服务提供商的许可后，您需要通过在地址栏中输入 `http://localhost:7180` 来转到本地网页浏览器。您还可以使用以下任何浏览器：
- en: Firefox 11 or higher
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Firefox 11 或更高版本
- en: Google Chrome
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Chrome
- en: Internet Explorer
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Internet Explorer
- en: Safari
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Safari
- en: Log in to the Cloudera manager console with the default credentials using `admin`
    for both the username and password. Later on you can change it as per your choice.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `admin` 作为用户名和密码登录 Cloudera 管理器控制台。以后可以根据需要更改密码。
- en: 'Use the Cloudera manager for automated CDH3 installation and configuration
    via browser: This step will install most of the required Cloudera Hadoop packages
    from Cloudera to your machines. The steps are as follows:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Cloudera 管理器通过浏览器进行自动化的 CDH3 安装和配置：此步骤将从 Cloudera 向您的计算机安装大部分所需的 Cloudera
    Hadoop 包。步骤如下：
- en: Install and validate your Cloudera manager license key file if you have chosen
    a full version of software.
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并验证您的 Cloudera 管理器许可证密钥文件（如果您选择了完整版本的软件）。
- en: Specify the hostname or IP address range for your CDH cluster installation.
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 CDH 集群安装指定主机名或 IP 地址范围。
- en: Connect to each host with SSH.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 SSH 连接到每个主机。
- en: Install the **Java Development Kit** (**JDK**) (if not already installed), the
    Cloudera manager agent, and CDH3 or CDH4 on each cluster host.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 **Java 开发工具包** (**JDK**)（如果尚未安装），Cloudera 管理器代理，以及 CDH3 或 CDH4 在每个集群主机上。
- en: Configure Hadoop on each node and start the Hadoop services.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个节点上配置 Hadoop 并启动 Hadoop 服务。
- en: 'After running the wizard and using the Cloudera manager, you should change
    the default administrator password as soon as possible. To change the administrator
    password, follow these steps:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行向导并使用 Cloudera 管理器后，您应该尽快更改默认管理员密码。更改管理员密码，请按照以下步骤操作：
- en: Click on the icon with the gear sign to display the administration page.
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击带有齿轮标志的图标，显示管理页面。
- en: Open the **Password** tab.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 **密码** 标签。
- en: Enter a new password twice and then click on **Update**.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入新密码两次，然后单击**Update**。
- en: 'Test the Cloudera Hadoop installation: You can check the Cloudera manager installation
    on your cluster by logging into the Cloudera manager admin console and by clicking
    on the **Services** tab. You should see something like the following screenshot:![Installing
    Cloudera Hadoop on Ubuntu](img/3282OS_01_01.jpg)'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试Cloudera Hadoop的安装：您可以通过登录Cloudera管理器管理控制台并单击**Services**选项卡来检查您集群上的Cloudera管理器安装。您应该看到类似下面这样的屏幕截图：![在Ubuntu上安装Cloudera
    Hadoop](img/3282OS_01_01.jpg)
- en: Cloudera manager admin console
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Cloudera管理器管理控制台
- en: You can also click on each service to see more detailed information. For example,
    if you click on the **hdfs1** link, you might see something like the following
    screenshot:![Installing Cloudera Hadoop on Ubuntu](img/3282OS_01_02.jpg)
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以单击每个服务以查看更详细的信息。例如，如果您单击**hdfs1**链接，您可能会看到类似下面这样的屏幕截图：![在Ubuntu上安装Cloudera
    Hadoop](img/3282OS_01_02.jpg)
- en: Cloudera manger admin console—HDFS service
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Cloudera管理器管理控制台——HDFS服务
- en: Tip
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To avoid these installation steps, use preconfigured Hadoop instances with Amazon
    Elastic MapReduce and MapReduce.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要避免这些安装步骤，请使用预配置的Amazon Elastic MapReduce和MapReduce实例。
- en: If you want to use Hadoop on Windows, try the HDP tool by Hortonworks. This
    is 100 percent open source, enterprise grade distribution of Hadoop. You can download
    the HDP tool at [http://hortonworks.com/download/](http://hortonworks.com/download/).
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想在Windows上使用Hadoop，请尝试由Hortonworks提供的HDP工具。这是完全开源的、企业级的Hadoop发行版。你可以在[http://hortonworks.com/download/](http://hortonworks.com/download/)下载HDP工具。
- en: Understanding Hadoop features
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Hadoop的特性
- en: 'Hadoop is specially designed for two core concepts: HDFS and MapReduce. Both
    are related to distributed computation. MapReduce is believed as the heart of
    Hadoop that performs parallel processing over distributed data.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop专为两个核心概念而设计：HDFS和MapReduce。两者都与分布式计算相关。MapReduce被认为是Hadoop的核心，它执行分布式数据上的并行处理。
- en: 'Let us see more details on Hadoop''s features:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Hadoop的特性的更多细节：
- en: HDFS
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS
- en: MapReduce
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce
- en: Understanding HDFS
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解HDFS
- en: HDFS is Hadoop's own rack-aware filesystem, which is a UNIX-based data storage
    layer of Hadoop. HDFS is derived from concepts of Google filesystem. An important
    characteristic of Hadoop is the partitioning of data and computation across many
    (thousands of) hosts, and the execution of application computations in parallel,
    close to their data. On HDFS, data files are replicated as sequences of blocks
    in the cluster. A Hadoop cluster scales computation capacity, storage capacity,
    and I/O bandwidth by simply adding commodity servers. HDFS can be accessed from
    applications in many different ways. Natively, HDFS provides a Java API for applications
    to use.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS是Hadoop自带的机架感知文件系统，是Hadoop的基于UNIX的数据存储层。HDFS源自于Google文件系统的概念。Hadoop的一个重要特性是数据和计算的分区跨多个（数千个）主机，并在靠近其数据的地方并行执行应用程序计算。在HDFS上，数据文件被复制为集群中的块序列。通过简单添加商品服务器，Hadoop集群可以扩展计算能力、存储能力和I/O带宽。HDFS可以通过多种不同的方式从应用程序访问。本地，HDFS为应用程序提供了一个Java
    API。
- en: The Hadoop clusters at Yahoo! span 40,000 servers and store 40 petabytes of
    application data, with the largest Hadoop cluster being 4,000 servers. Also, one
    hundred other organizations worldwide are known to use Hadoop.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Yahoo!的Hadoop集群覆盖40,000台服务器，存储40PB的应用程序数据，其中最大的Hadoop集群为4,000台服务器。此外，全球还有100多个组织使用Hadoop。
- en: Understanding the characteristics of HDFS
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解HDFS的特性
- en: 'Let us now look at the characteristics of HDFS:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看HDFS的特性：
- en: Fault tolerant
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容错
- en: Runs with commodity hardware
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用商品硬件运行
- en: Able to handle large datasets
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够处理大型数据集
- en: Master slave paradigm
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主从范式
- en: Write once file access only
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一次文件访问权限
- en: Understanding MapReduce
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解MapReduce
- en: MapReduce is a programming model for processing large datasets distributed on
    a large cluster. MapReduce is the heart of Hadoop. Its programming paradigm allows
    performing massive data processing across thousands of servers configured with
    Hadoop clusters. This is derived from Google MapReduce.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce是一种用于处理分布在大型集群上的大型数据集的编程模型。MapReduce是Hadoop的核心。其编程范式允许在配置有Hadoop集群的数千台服务器上执行大规模数据处理。这源自于Google
    MapReduce。
- en: 'Hadoop MapReduce is a software framework for writing applications easily, which
    process large amounts of data (multiterabyte datasets) in parallel on large clusters
    (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.
    This MapReduce paradigm is divided into two phases, Map and Reduce that mainly
    deal with key and value pairs of data. The Map and Reduce task run sequentially
    in a cluster; the output of the Map phase becomes the input for the Reduce phase.
    These phases are explained as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop MapReduce是一个软件框架，便于编写应用程序，能够在大型集群（数千个节点）的商品硬件上以可靠、容错的方式并行处理大量数据（多TB数据集）。该MapReduce范式分为两个阶段，Map和Reduce，主要处理数据的键值对。Map和Reduce任务在集群中顺序执行；Map阶段的输出成为Reduce阶段的输入。以下是这些阶段的解释：
- en: '**Map phase**: Once divided, datasets are assigned to the task tracker to perform
    the Map phase. The data functional operation will be performed over the data,
    emitting the mapped key and value pairs as the output of the Map phase.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Map阶段**：数据集一旦被划分，就会分配给任务跟踪器以执行Map阶段。数据操作将应用于数据，并将映射的键值对作为Map阶段的输出。'
- en: '**Reduce phase**: The master node then collects the answers to all the subproblems
    and combines them in some way to form the output; the answer to the problem it
    was originally trying to solve.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reduce阶段**：然后，主节点收集所有子问题的答案，并以某种方式将它们组合成最终输出；即最初试图解决的问题的答案。'
- en: 'The five common steps of parallel computing are as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 并行计算的五个常见步骤如下：
- en: 'Preparing the `Map()` input: This will take the input data row wise and emit
    key value pairs per rows, or we can explicitly change as per the requirement.'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备`Map()`输入：这将逐行读取输入数据，并为每行发出键值对，或者我们可以根据需求显式地进行更改。
- en: 'Map input: list (k1, v1)'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 映射输入：list（k1，v1）
- en: Run the user-provided `Map()` code
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行用户提供的`Map()`代码
- en: 'Map output: list (k2, v2)'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Map输出：list（k2，v2）
- en: Shuffle the Map output to the Reduce processors. Also, shuffle the similar keys
    (grouping them) and input them to the same reducer.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Map输出数据混排到Reduce处理器中。同时，将相似的键混排（将其分组），并输入到同一个reducer中。
- en: 'Run the user-provided `Reduce()` code: This phase will run the custom reducer
    code designed by developer to run on shuffled data and emit key and value.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行用户提供的`Reduce()`代码：该阶段将运行开发人员设计的自定义reducer代码，在混排的数据上运行并发出键值对。
- en: 'Reduce input: (k2, list(v2))'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少输入：（k2, list(v2)）
- en: 'Reduce output: (k3, v3)'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少输出：（k3, v3）
- en: 'Produce the final output: Finally, the master node collects all reducer output
    and combines and writes them in a text file.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 产生最终输出：最终，主节点收集所有reducer输出，并将其组合后写入文本文件。
- en: Tip
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The reference links to review on Google filesystem can be found at [http://research.google.com/archive/gfs.html](http://research.google.com/archive/gfs.html)
    and Google MapReduce can be found at [http://research.google.com/archive/mapreduce.html](http://research.google.com/archive/mapreduce.html).
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关Google文件系统的参考链接可以在[http://research.google.com/archive/gfs.html](http://research.google.com/archive/gfs.html)找到，Google
    MapReduce的链接可以在[http://research.google.com/archive/mapreduce.html](http://research.google.com/archive/mapreduce.html)找到。
- en: Learning the HDFS and MapReduce architecture
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习HDFS和MapReduce架构
- en: Since HDFS and MapReduce are considered to be the two main features of the Hadoop
    framework, we will focus on them. So, let's first start with HDFS.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 由于HDFS和MapReduce被认为是Hadoop框架的两个主要特性，我们将重点关注这两者。因此，首先让我们从HDFS开始。
- en: Understanding the HDFS architecture
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解HDFS架构
- en: HDFS can be presented as the master/slave architecture. HDFS master is named
    as NameNode whereas slave as DataNode. NameNode is a sever that manages the filesystem
    namespace and adjusts the access (open, close, rename, and more) to files by the
    client. It divides the input data into blocks and announces which data block will
    be store in which DataNode. DataNode is a slave machine that stores the replicas
    of the partitioned dataset and serves the data as the request comes. It also performs
    block creation and deletion.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS可以呈现为主/从架构。HDFS的主节点被称为NameNode，而从节点为DataNode。NameNode是一个服务器，管理文件系统的命名空间，并控制客户端对文件的访问（打开、关闭、重命名等）。它将输入数据分割成块，并公告哪个数据块将存储在哪个DataNode中。DataNode是一个从节点，存储分区数据集的副本，并根据请求提供数据。它还执行数据块的创建和删除操作。
- en: The internal mechanism of HDFS divides the file into one or more blocks; these
    blocks are stored in a set of data nodes. Under normal circumstances of the replication
    factor three, the HDFS strategy is to place the first copy on the local node,
    second copy on the local rack with a different node, and a third copy into different
    racks with different nodes. As HDFS is designed to support large files, the HDFS
    block size is defined as 64 MB. If required, this can be increased.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS的内部机制将文件分为一个或多个数据块，这些数据块存储在一组数据节点中。在正常情况下，复制因子为三时，HDFS策略是将第一份副本放在本地节点上，第二份副本放在同一机架上的不同节点上，第三份副本放入不同机架的不同节点上。由于HDFS旨在支持大文件，HDFS的块大小定义为64
    MB。如果需要，可以增加块大小。
- en: Understanding HDFS components
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解HDFS组件
- en: 'HDFS is managed with the master-slave architecture included with the following
    components:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS通过主从架构进行管理，包含以下组件：
- en: '**NameNode**: This is the master of the HDFS system. It maintains the directories,
    files, and manages the blocks that are present on the DataNodes.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NameNode**：这是HDFS系统的主节点。它维护目录、文件，并管理存储在DataNode上的数据块。'
- en: '**DataNode**: These are slaves that are deployed on each machine and provide
    actual storage. They are responsible for serving read-and-write data requests
    for the clients.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataNode**：这些是部署在每台机器上的从节点，提供实际的存储。它们负责为客户端提供读写数据请求的服务。'
- en: '**Secondary NameNode**: This is responsible for performing periodic checkpoints.
    So, if the NameNode fails at any time, it can be replaced with a snapshot image
    stored by the secondary NameNode checkpoints.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Secondary NameNode**：负责执行定期的检查点。因此，如果NameNode发生故障，可以使用由Secondary NameNode检查点存储的快照图像进行替换。'
- en: Understanding the MapReduce architecture
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解MapReduce架构
- en: MapReduce is also implemented over master-slave architectures. Classic MapReduce
    contains job submission, job initialization, task assignment, task execution,
    progress and status update, and job completion-related activities, which are mainly
    managed by the JobTracker node and executed by TaskTracker. Client application
    submits a job to the JobTracker. Then input is divided across the cluster. The
    JobTracker then calculates the number of map and reducer to be processed. It commands
    the TaskTracker to start executing the job. Now, the TaskTracker copies the resources
    to a local machine and launches JVM to map and reduce program over the data. Along
    with this, the TaskTracker periodically sends update to the JobTracker, which
    can be considered as the heartbeat that helps to update JobID, job status, and
    usage of resources.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce也实现了主从架构。经典的MapReduce包含作业提交、作业初始化、任务分配、任务执行、进度和状态更新以及作业完成相关活动，这些活动主要由JobTracker节点管理，并由TaskTracker执行。客户端应用程序向JobTracker提交作业。然后，输入数据在集群中分配。JobTracker计算需要处理的map和reduce的数量，命令TaskTracker开始执行作业。现在，TaskTracker将资源复制到本地机器并启动JVM来执行map和reduce程序。与此同时，TaskTracker定期向JobTracker发送更新，这可以看作是心跳信号，帮助更新JobID、作业状态和资源使用情况。
- en: Understanding MapReduce components
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解MapReduce组件
- en: 'MapReduce is managed with master-slave architecture included with the following
    components:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce通过主从架构进行管理，包含以下组件：
- en: '**JobTracker**: This is the master node of the MapReduce system, which manages
    the jobs and resources in the cluster (TaskTrackers). The JobTracker tries to
    schedule each map as close to the actual data being processed on the TaskTracker,
    which is running on the same DataNode as the underlying block.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JobTracker**：这是MapReduce系统的主节点，管理集群中的作业和资源（TaskTracker）。JobTracker尝试将每个map任务调度到与正在处理数据的TaskTracker尽可能接近的位置，而TaskTracker又运行在与底层数据块相同的DataNode上。'
- en: '**TaskTracker**: These are the slaves that are deployed on each machine. They
    are responsible for running the map and reducing tasks as instructed by the JobTracker.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TaskTracker**：这些是部署在每台机器上的从节点，负责按照JobTracker的指示运行map和reduce任务。'
- en: Understanding the HDFS and MapReduce architecture by plot
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过图示理解HDFS和MapReduce架构
- en: In this plot, both HDFS and MapReduce master and slave components have been
    included, where NameNode and DataNode are from HDFS and JobTracker and TaskTracker
    are from the MapReduce paradigm.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，包含了HDFS和MapReduce的主从组件，其中NameNode和DataNode来自HDFS，而JobTracker和TaskTracker来自MapReduce范式。
- en: 'Both paradigms consisting of master and slave candidates have their own specific
    responsibility to handle MapReduce and HDFS operations. In the next plot, there
    is a plot with two sections: the preceding one is a MapReduce layer and the following
    one is an HDFS layer.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种由主从候选者组成的范式各自有其特定的职责，用于处理 MapReduce 和 HDFS 操作。在下图中，图像分为两个部分：前一部分是 MapReduce
    层，后一部分是 HDFS 层。
- en: '![Understanding the HDFS and MapReduce architecture by plot](img/3282OS_01_03.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![通过图示理解 HDFS 和 MapReduce 架构](img/3282OS_01_03.jpg)'
- en: The HDFS and MapReduce architecture
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 和 MapReduce 架构
- en: Hadoop is a top-level Apache project and is a very complicated Java framework.
    To avoid technical complications, the Hadoop community has developed a number
    of Java frameworks that has added an extra value to Hadoop features. They are
    considered as Hadoop subprojects. Here, we are departing to discuss several Hadoop
    components that can be considered as an abstraction of HDFS or MapReduce.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 是一个顶级的 Apache 项目，是一个非常复杂的 Java 框架。为避免技术复杂性，Hadoop 社区开发了多个 Java 框架，这些框架为
    Hadoop 的功能增添了额外的价值。它们被视为 Hadoop 的子项目。在这里，我们将讨论几个可以视为 HDFS 或 MapReduce 抽象的 Hadoop
    组件。
- en: Understanding Hadoop subprojects
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Hadoop 子项目
- en: '**Mahout** is a popular data mining library. It takes the most popular data
    mining scalable machine learning algorithms for performing clustering, classification,
    regression, and statistical modeling to prepare intelligent applications. Also,
    it is a scalable machine-learning library.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**Mahout** 是一个流行的数据挖掘库。它采用最流行的数据挖掘可扩展机器学习算法，执行聚类、分类、回归和统计建模，以准备智能应用程序。同时，它是一个可扩展的机器学习库。'
- en: Apache Mahout is distributed under a commercially friendly Apache software license.
    The goal of Apache Mahout is to build a vibrant, responsive, and diverse community
    to facilitate discussions not only on the project itself but also on potential
    use cases.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Mahout 是在商业友好的 Apache 软件许可下分发的。Apache Mahout 的目标是建立一个充满活力、响应迅速且多样化的社区，促进不仅是项目本身的讨论，还包括潜在的使用案例。
- en: 'The following are some companies that are using Mahout:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些使用 Mahout 的公司：
- en: '**Amazon**: This a shopping portal for providing personalization recommendation'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon**：这是一个购物门户网站，提供个性化推荐服务。'
- en: '**AOL**: This is a shopping portal for shopping recommendations'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AOL**：这是一个购物门户网站，用于购物推荐。'
- en: '**Drupal**: This is a PHP content management system using Mahout for providing
    open source content-based recommendation'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Drupal**：这是一个 PHP 内容管理系统，使用 Mahout 提供开源基于内容的推荐服务。'
- en: '**iOffer**: This is a shopping portal, which uses Mahout''s Frequent Pattern
    Set Mining and collaborative filtering to recommend items to users'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**iOffer**：这是一个购物门户网站，使用 Mahout 的频繁模式集挖掘和协同过滤技术向用户推荐商品。'
- en: '**LucidWorks Big Data**: This is a popular analytics firm, which uses Mahout
    for clustering, duplicate document detection, phase extraction, and classification'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LucidWorks Big Data**：这是一家流行的分析公司，使用 Mahout 进行聚类、重复文档检测、短语提取和分类。'
- en: '**Radoop**: This provides a drag-and-drop interface for Big Data analytics,
    including Mahout clustering and classification algorithms'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Radoop**：提供一个拖拽式界面，用于大数据分析，包括 Mahout 聚类和分类算法。'
- en: '**Twitter**: This is a social networking site, which uses Mahout''s **Latent
    Dirichlet Allocation** (**LDA**) implementation for user interest modeling and
    maintains a fork of Mahout on GitHub.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Twitter**：这是一个社交网络网站，使用 Mahout 的 **潜在狄利克雷分配**（**LDA**）实现进行用户兴趣建模，并在 GitHub
    上维护 Mahout 的一个分支。'
- en: '**Yahoo**!: This is the world''s most popular web service provider, which uses
    Mahout''s Frequent Pattern Set Mining for Yahoo! Mail'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Yahoo!**：这是全球最受欢迎的网页服务提供商，使用 Mahout 的频繁模式集挖掘技术用于 Yahoo! Mail。'
- en: Tip
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The reference links on the Hadoop ecosystem can be found at [http://www.revelytix.com/?q=content/hadoop-ecosystem](http://www.revelytix.com/?q=content/hadoop-ecosystem).
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hadoop 生态系统的参考链接可以在 [http://www.revelytix.com/?q=content/hadoop-ecosystem](http://www.revelytix.com/?q=content/hadoop-ecosystem)
    找到。
- en: '**Apache HBase** is a distributed Big Data store for Hadoop. This allows random,
    real-time read/write access to Big Data. This is designed as a column-oriented
    data storage model innovated after inspired by Google BigTable.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache HBase** 是一个分布式大数据存储系统，适用于 Hadoop。它允许对大数据进行随机、实时的读写访问。该系统采用了受 Google
    BigTable 启发创新的列式数据存储模型。'
- en: 'The following are the companies using HBase:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 HBase 的公司：
- en: '**Yahoo!**: This is the world''s popular web service provider for near duplicate
    document detection'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Yahoo!**：这是全球知名的网页服务提供商，用于近重复文档检测。'
- en: '**Twitter**: This is a social networking site for version control storage and
    retrieval'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Twitter**：这是一个用于版本控制存储和检索的社交网络网站。'
- en: '**Mahalo**: This is a knowledge sharing service for similar content recommendation'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mahalo**：这是一个用于相似内容推荐的知识共享服务。'
- en: '**NING**: This is a social network service provider for real-time analytics
    and reporting'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NING**：这是一个用于实时分析和报告的社交网络服务提供商。'
- en: '**StumbleUpon**: This is a universal personalized recommender system, real-time
    data storage, and data analytics platform'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**StumbleUpon**：这是一个通用个性化推荐系统，实时数据存储和数据分析平台。'
- en: '**Veoh**: This is an online multimedia content sharing platform for user profiling
    system'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Veoh**：这是一个在线多媒体内容共享平台，用于用户档案系统。'
- en: Tip
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: For Google Big Data, distributed storage system for structured data, refer the
    link [http://research.google.com/archive/bigtable.html](http://research.google.com/archive/bigtable.html).
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 Google 大数据，结构化数据的分布式存储系统，请参考链接 [http://research.google.com/archive/bigtable.html](http://research.google.com/archive/bigtable.html)。
- en: '**Hive** is a Hadoop-based data warehousing like framework developed by Facebook.
    It allows users to fire queries in SQL-like languages, such as HiveQL, which are
    highly abstracted to Hadoop MapReduce. This allows SQL programmers with no MapReduce
    experience to use the warehouse and makes it easier to integrate with business
    intelligence and visualization tools for real-time query processing.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hive** 是一个基于 Hadoop 的数据仓库框架，由 Facebook 开发。它允许用户使用类似 SQL 的语言（如 HiveQL）进行查询，这些查询会高度抽象为
    Hadoop MapReduce。这使得没有 MapReduce 经验的 SQL 程序员也能使用该数据仓库，并使其更容易与商业智能和可视化工具进行集成，以便进行实时查询处理。'
- en: '**Pig** is a Hadoop-based open source platform for analyzing the large scale
    datasets via its own SQL-like language: Pig Latin. This provides a simple operation
    and programming interface for massive, complex data-parallelization computation.
    This is also easier to develop; it''s more optimized and extensible. Apache Pig
    has been developed by Yahoo!. Currently, Yahoo! and Twitter are the primary Pig
    users.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pig** 是一个基于 Hadoop 的开源平台，用于通过其自己的类似 SQL 的语言 Pig Latin 来分析大规模数据集。它为海量复杂的数据并行计算提供了简单的操作和编程接口。这也更容易开发，更优化且更具扩展性。Apache
    Pig 是由 Yahoo! 开发的。目前，Yahoo! 和 Twitter 是主要的 Pig 用户。'
- en: For developers, the direct use of Java APIs can be tedious or error-prone, but
    also limits the Java programmer's use of Hadoop programming's flexibility. So,
    Hadoop provides two solutions that enable making Hadoop programming for dataset
    management and dataset analysis with MapReduce easier—these are Pig and Hive,
    which are always confusing.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开发者而言，直接使用 Java API 可能既繁琐又容易出错，而且还限制了 Java 程序员在 Hadoop 编程中灵活性的发挥。因此，Hadoop
    提供了两种解决方案，使得数据集管理和数据集分析的 Hadoop 编程变得更容易——这些解决方案是 Pig 和 Hive，这两者经常让人混淆。
- en: '**Apache Sqoop** provides Hadoop data processing platform and relational databases,
    data warehouse, and other non-relational databases quickly transferring large
    amounts of data in a new way. Apache Sqoop is a mutual data tool for importing
    data from the relational databases to Hadoop HDFS and exporting data from HDFS
    to relational databases.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Sqoop** 提供了一种 Hadoop 数据处理平台，可以快速以一种全新的方式将大量数据从关系型数据库、数据仓库和其他非关系型数据库传输到
    Hadoop。Apache Sqoop 是一个用于将数据从关系型数据库导入 Hadoop HDFS，或将数据从 HDFS 导出到关系型数据库的互通数据工具。'
- en: It works together with most modern relational databases, such as MySQL, PostgreSQL,
    Oracle, Microsoft SQL Server, and IBM DB2, and enterprise data warehouse. Sqoop
    extension API provides a way to create new connectors for the database system.
    Also, the Sqoop source comes up with some popular database connectors. To perform
    this operation, Sqoop first transforms the data into Hadoop MapReduce with some
    logic of database schema creation and transformation.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 它与大多数现代关系型数据库一起工作，如 MySQL、PostgreSQL、Oracle、Microsoft SQL Server 和 IBM DB2，以及企业数据仓库。Sqoop
    扩展 API 提供了一种为数据库系统创建新连接器的方法。此外，Sqoop 源代码还提供了一些流行的数据库连接器。为了执行此操作，Sqoop 首先将数据转换为
    Hadoop MapReduce，并通过一些数据库模式创建和转换的逻辑来进行处理。
- en: '**Apache Zookeeper** is also a Hadoop subproject used for managing Hadoop,
    Hive, Pig, HBase, Solr, and other projects. Zookeeper is an open source distributed
    applications coordination service, which is designed with Fast Paxos algorithm-based
    synchronization and configuration and naming services such as maintenance of distributed
    applications. In programming, Zookeeper design is a very simple data model style,
    much like the system directory tree structure.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Zookeeper** 也是一个 Hadoop 子项目，用于管理 Hadoop、Hive、Pig、HBase、Solr 和其他项目。Zookeeper
    是一个开源的分布式应用程序协调服务，设计基于 Fast Paxos 算法的同步、配置和命名服务，如分布式应用程序的维护。在编程中，Zookeeper 的设计采用非常简单的数据模型风格，类似于系统目录树结构。'
- en: 'Zookeeper is divided into two parts: the server and client. For a cluster of
    Zookeeper servers, only one acts as a leader, which accepts and coordinates all
    rights. The rest of the servers are read-only copies of the master. If the leader
    server goes down, any other server can start serving all requests. Zookeeper clients
    are connected to a server on the Zookeeper service. The client sends a request,
    receives a response, accesses the observer events, and sends a heartbeat via a
    TCP connection with the server.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Zookeeper 分为两部分：服务器和客户端。对于 Zookeeper 服务器集群，只有一个服务器充当领导者，接受并协调所有权限。其余的服务器是主服务器的只读副本。如果领导者服务器宕机，任何其他服务器都可以开始处理所有请求。Zookeeper
    客户端连接到 Zookeeper 服务中的服务器。客户端发送请求，接收响应，访问观察事件，并通过与服务器的 TCP 连接发送心跳。
- en: For a high-performance coordination service for distributed applications, Zookeeper
    is a centralized service for maintaining configuration information, naming, and
    providing distributed synchronization and group services. All these kinds of services
    are used in some form or another by distributed applications. Each time they are
    implemented, there is a lot of work that goes into fixing the bugs and race conditions
    that are inevitable. These services lead to management complexity when the applications
    are deployed.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分布式应用程序的高性能协调服务，Zookeeper 是一个集中式服务，用于维护配置信息、命名以及提供分布式同步和组服务。所有这些服务以某种形式被分布式应用程序使用。每次实现时，都需要大量的工作来修复不可避免的
    bug 和竞态条件。这些服务在应用程序部署时导致管理复杂性。
- en: Apache Solr is an open source enterprise search platform from the Apache license
    project. Apache Solr is highly scalable, supporting distributed search and index
    replication engine. This allows building web application with powerful text search,
    faceted search, real-time indexing, dynamic clustering, database integration,
    and rich document handling.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Solr 是一个开源企业级搜索平台，来自 Apache 许可项目。Apache Solr 具有高度可扩展性，支持分布式搜索和索引复制引擎。这使得构建具有强大文本搜索、分面搜索、实时索引、动态聚类、数据库集成和丰富文档处理的
    Web 应用程序成为可能。
- en: Apache Solr is written in Java, which runs as a standalone server to serve the
    search results via REST-like HTTP/XML and JSON APIs. So, this Solr server can
    be easily integrated with an application, which is written in other programming
    languages. Due to all these features, this search server is used by Netflix, AOL,
    CNET, and Zappos.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Solr 用 Java 编写，作为独立服务器运行，通过类似 REST 的 HTTP/XML 和 JSON API 提供搜索结果。因此，这个
    Solr 服务器可以很容易地与用其他编程语言编写的应用程序集成。由于这些特点，这个搜索服务器被 Netflix、AOL、CNET 和 Zappos 等公司使用。
- en: '**Ambari** is very specific to Hortonworks. Apache Ambari is a web-based tool
    that supports Apache Hadoop cluster supply, management, and monitoring. Ambari
    handles most of the Hadoop components, including HDFS, MapReduce, Hive, Pig, HBase,
    Zookeeper, Sqoop, and HCatlog as centralized management.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ambari** 是非常特定于 Hortonworks 的。Apache Ambari 是一个基于 Web 的工具，支持 Apache Hadoop
    集群的供应、管理和监控。Ambari 处理大部分 Hadoop 组件，包括 HDFS、MapReduce、Hive、Pig、HBase、Zookeeper、Sqoop
    和 HCatlog，作为集中管理。'
- en: In addition, Ambari is able to install security based on the Kerberos authentication
    protocol over the Hadoop cluster. Also, it provides role-based user authentication,
    authorization, and auditing functions for users to manage integrated LDAP and
    Active Directory.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Ambari 能够基于 Kerberos 身份验证协议在 Hadoop 集群上安装安全性。同时，它还为用户提供基于角色的用户身份验证、授权和审计功能，帮助用户管理集成的
    LDAP 和 Active Directory。
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned what is R, Hadoop, and their features, and how to
    install them before going on to analyzing the datasets with R and Hadoop. In the
    next chapter, we are going to learn what MapReduce is and how to develop MapReduce
    programs with Apache Hadoop.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了什么是R、Hadoop及其特点，并了解了如何安装它们，然后才能使用R和Hadoop进行数据集分析。在下一章中，我们将学习什么是MapReduce，以及如何使用Apache
    Hadoop开发MapReduce程序。
