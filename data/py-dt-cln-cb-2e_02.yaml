- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Anticipating Data Cleaning Issues When Working with HTML, JSON, and Spark Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测处理HTML、JSON和Spark数据时的数据清理问题
- en: This chapter continues our work on importing data from a variety of sources
    and the initial checks we should do on the data after importing it. Over the last
    25 years, data analysts have found that they increasingly need to work with data
    in non-tabular, semi-structured forms. Sometimes, they even create and persist
    data in those forms. We will work with a common alternative to traditional tabular
    datasets in this chapter, JSON, but the general concepts can be extended to XML
    and NoSQL data stores such as MongoDB. We will also go over common issues that
    occur when scraping data from websites.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章继续讨论从各种来源导入数据的工作，以及导入数据后需要进行的初步检查。在过去的25年里，数据分析师发现他们越来越需要处理非表格型的半结构化数据。有时，他们甚至会在这些形式中创建和持久化数据。本章将以JSON这一传统表格数据集的常见替代方案为例，但这些一般概念也可以扩展到XML和NoSQL数据存储，如MongoDB。此外，我们还将讨论从网站抓取数据时常遇到的问题。
- en: Data analysts have also been finding that increases in the volume of data to
    be analyzed have been even greater than improvements in machine processing power,
    at least those computing resources that are available locally. Working with big
    data sometimes requires us to rely on technology like Apache Spark, which can
    take advantage of distributed resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析师还发现，分析数据的量的增加速度超过了机器处理能力的提升，至少是在本地可用的计算资源方面。处理大数据有时需要依赖像Apache Spark这样的技术，它可以利用分布式资源。
- en: 'In this chapter, we will work through the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将依次解决以下几种方法：
- en: Importing simple JSON data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入简单的JSON数据
- en: Importing more complicated JSON data from an API
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从API导入更复杂的JSON数据
- en: Importing data from web pages
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网页导入数据
- en: Working with Spark data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark数据
- en: Persisting JSON data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久化JSON数据
- en: Versioning data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据版本管理
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need pandas, NumPy, and Matplotlib to complete the recipes in this
    chapter. I used pandas 2.1.4, but the code will run on pandas 1.5.3 or later.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章中的方法，您需要使用pandas、NumPy和Matplotlib。我使用的是pandas 2.1.4，但该代码也适用于pandas 1.5.3及以后的版本。
- en: The code in this chapter can be downloaded from the book’s GitHub repository,
    [https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码可以从本书的GitHub仓库下载，链接：[https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition)。
- en: Importing simple JSON data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入简单的JSON数据
- en: '**JavaScript Object Notation** (**JSON**) has turned out to be an incredibly
    useful standard for transferring data from one machine, process, or node to another.
    Often, a client sends a data request to a server, upon which that server queries
    the data in local storage and then converts it from something like an SQL Server,
    MySQL, or PostgreSQL table or tables into JSON, which the client can consume.
    This is sometimes complicated further by the first server (say, a web server)
    forwarding the request to a database server. JSON facilitates this, as does XML,
    by doing the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**JavaScript对象表示法**（**JSON**）已成为一种极其有用的标准，用于在不同的机器、进程或节点之间传输数据。通常，客户端向服务器发送数据请求，服务器随后查询本地存储中的数据，并将其从SQL
    Server、MySQL或PostgreSQL表等格式转换为JSON，客户端可以使用该数据。这有时会被进一步复杂化，例如，第一个服务器（比如一个Web服务器）将请求转发给数据库服务器。JSON和XML都能方便地完成这一过程，具体方式如下：'
- en: Being readable by humans
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可被人类阅读
- en: Being consumable by most client devices
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于大多数客户端设备
- en: Not being limited in structure
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不受结构限制
- en: JSON is quite flexible, which means that it can accommodate just about anything,
    no matter how unwise. The structure can even change within a JSON file, so different
    keys might be present at different points. For example, the file might begin with
    some explanatory keys that have a very different structure than the remaining
    *data* keys or some keys might be present in some cases but not others. We will
    go over some approaches for dealing with that messiness (uh, flexibility).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: JSON具有相当高的灵活性，这意味着它几乎可以容纳任何内容，无论这些内容多么不明智。结构甚至可以在JSON文件内发生变化，因此在不同的地方可能会出现不同的键。例如，文件可能以一些解释性键开始，而这些键的结构与其余的*数据*键截然不同，或者某些键在某些情况下可能会出现，而在其他情况下则不会出现。我们将讨论处理这种混乱（呃，灵活性）的一些方法。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We are going to work with data on news stories about political candidates in
    this recipe. This data is made available for public use at [dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0ZLHOK](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0ZLHOK).
    I have combined the JSON files there into one file and randomly selected 60,000
    news stories from the combined data. This sample (`allcandidatenewssample.json`)
    is available in the GitHub repository of this book.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将处理有关政治候选人的新闻故事数据。这些数据可以在 [dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0ZLHOK](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0ZLHOK)
    上公开使用。我已将该处的 JSON 文件合并为一个文件，并从合并的数据中随机选择了 60,000 条新闻故事。这些样本（`allcandidatenewssample.json`）可以在本书的
    GitHub 仓库中找到。
- en: We will do a little work with list and dictionary comprehension in this recipe.
    *DataCamp* has good guides on list comprehension ([https://www.datacamp.com/community/tutorials/python-list-comprehension](https://www.datacamp.com/community/tutorials/python-list-comprehension))
    and dictionary comprehension ([https://www.datacamp.com/community/tutorials/python-dictionary-comprehension](https://www.datacamp.com/community/tutorials/python-dictionary-comprehension))
    if you are feeling a little rusty or have limited or no experience with list and
    dictionary comprehension.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将进行一些列表和字典推导式的操作。如果你对列表推导式或字典推导式感到生疏，或者没有经验，可以参考 *DataCamp* 提供的很好的教程：[Python
    列表推导式](https://www.datacamp.com/community/tutorials/python-list-comprehension)
    和 [Python 字典推导式](https://www.datacamp.com/community/tutorials/python-dictionary-comprehension)。
- en: How to do it…
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤…
- en: 'We will import a JSON file into pandas after doing some data checking and cleaning:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在做一些数据检查和清理后，将 JSON 文件导入到 pandas 中：
- en: Import the `json` and `pprint` libraries.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `json` 和 `pprint` 库。
- en: '`pprint` improves the display of the lists and dictionaries that are returned
    when we load JSON data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`pprint` 改善了加载 JSON 数据时返回的列表和字典的显示方式：'
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load the JSON data and look for potential issues.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 JSON 数据并查找潜在的问题。
- en: 'Use the `json load` method to return data on news stories about political candidates.
    `load` returns a list of dictionaries. Use `len` to get the size of the list,
    which is the total number of news stories in this case. (Each list item is a dictionary
    with keys for the title, source, and so on, and their respective values.) Use
    `pprint` to display the first two dictionaries. Get the value from the source
    key for the first list item:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `json load` 方法返回关于政治候选人的新闻故事数据。`load` 返回一个字典列表。使用 `len` 获取列表的大小，即新闻故事的总数。在这个例子中，每个列表项都是一个字典，包含标题、来源等键值对。使用
    `pprint` 显示前两个字典。获取第一个列表项中 `source` 键的值：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Check for differences in the structure of the dictionaries.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查字典结构的差异。
- en: 'Use `Counter` to check for any dictionaries in the list with fewer or more
    than the 9 keys that are normal. Look at a few of the dictionaries with almost
    no data (those with just two keys) before removing them. Confirm that the remaining
    list of dictionaries has the expected length – `60000-2382=57618`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Counter` 检查列表中是否有字典的键数少于或多于正常的 9 个。查看一些几乎没有数据的字典（只有两个键）然后再将其删除。确认剩余字典列表的长度符合预期——`60000-2382=57618`：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Generate counts from the JSON data.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 JSON 数据中生成统计数据。
- en: 'Get the dictionaries just for *Politico* (a website that covers political news)
    and display a couple of dictionaries:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 获取仅针对 *Politico*（一个报道政治新闻的网站）的字典，并显示其中的几个字典：
- en: '[PRE17]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Get the `source` data and confirm that it has the anticipated length.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取 `source` 数据，并确认其长度符合预期。
- en: 'Show the first few items in the new `sources` list. Generate a count of news
    stories by source and display the 10 most popular sources. Notice that stories
    from *The Hill* can have `TheHill` (without a space) or `The Hill` as the value
    for `source`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 显示新 `sources` 列表中的前几个项目。生成按来源统计的新闻故事数量，并展示 10 个最受欢迎的来源。注意，来自 *The Hill* 的故事可能会在
    `source` 中显示为 `TheHill`（没有空格）或 `The Hill`：
- en: '[PRE21]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Fix any errors in the values in the dictionary.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修正字典中值的错误。
- en: 'Fix the `source` values for `The Hill`. Notice that `The Hill` is now the most
    frequent source for news stories:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 修正 `The Hill` 的 `source` 值。注意，现在 `The Hill` 是新闻故事中最常见的来源：
- en: '[PRE29]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Create a pandas DataFrame.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 pandas DataFrame。
- en: 'Pass the JSON data to the pandas `DataFrame` method. Convert the `date` column
    to a `datetime` data type:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将 JSON 数据传递给 pandas 的 `DataFrame` 方法。将 `date` 列转换为 `datetime` 数据类型：
- en: '[PRE31]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Confirm that we are getting the expected values for `source`.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认我们得到了预期的 `source` 值。
- en: 'Also, rename the `date` column:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，重命名 `date` 列：
- en: '[PRE33]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We now have a pandas DataFrame with only the news stories where there is meaningful
    data and with the values for `source` fixed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个 pandas DataFrame，只包含那些具有有意义数据且 `source` 值已固定的新闻故事。
- en: How it works…
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理……
- en: 'The `json.load` method returns a list of dictionaries. This makes it possible
    to use a number of familiar tools when working with this data: list methods, slicing,
    list comprehensions, dictionary updates, and so on. There are times (maybe when
    you just have to populate a list or count the number of individuals in a given
    category) when there is no need to use pandas.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`json.load` 方法返回一个字典列表。这使得在处理这些数据时可以使用许多熟悉的工具：列表方法、切片、列表推导、字典更新等等。有时候（也许当你只需要填充一个列表或统计给定类别中的个体数量时）根本不需要使用
    pandas。'
- en: In *Steps 2* to *6*, we use list methods to do many of the same checks we have
    done with pandas in previous recipes. In *Step* *3*, we use `Counter` with a list
    comprehension (`Counter([len(item) for item in candidatenews])`) to get the number
    of keys in each dictionary. This tells us that there are 2,382 dictionaries with
    just 2 keys and 416 with 10\. We use `next` to look for an example of dictionaries
    with fewer or more than 9 keys to get a sense of the structure of those items.
    We use slicing to show 2 dictionaries with 2 keys to see if there is any data
    in those dictionaries. We then select only those dictionaries with more than 2
    keys.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 2* 到 *6* 中，我们使用列表方法执行许多与前面的 pandas 配方相同的检查。在 *步骤* *3* 中，我们使用 `Counter`
    与列表推导式（`Counter([len(item) for item in candidatenews])`）来获取每个字典中的键的数量。这告诉我们有 2,382
    个字典只有 2 个键，416 个字典有 10 个键。我们使用 `next` 查找一个字典示例，看看是否有少于或多于 9 个键的字典，以了解这些项的结构。我们使用切片展示
    2 个只有 2 个键的字典，看看这些字典中是否有数据。然后，我们仅选择那些有超过 2 个键的字典。
- en: In *Step 4*, we create a subset of the list of dictionaries, one that just has
    `source` equal to `Politico`, and take a look at a couple of items. We then create
    a list with just the source data and use `Counter` to list the 10 most common
    sources in *Step 5*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 4* 中，我们创建了字典列表的一个子集，仅包含 `source` 等于 `Politico` 的项，并查看了其中的几项。然后，我们创建了一个仅包含源数据的列表，并在
    *步骤 5* 中使用 `Counter` 来列出 10 个最常见的来源。
- en: '*Step 6* demonstrates how to replace key values conditionally in a list of
    dictionaries. In this case, we update the key value to `The Hill` whenever `key
    (k)` is `source` and `value (v)` is `TheHill`. The `for k, v in newsdict.items()`
    section is the unsung hero of this line. It loops through all key/value pairs
    for all dictionaries in `candidatenews`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 6* 演示了如何在字典列表中有条件地替换关键值。在这种情况下，当 `key (k)` 为 `source` 且 `value (v)` 为 `TheHill`
    时，我们将键值更新为 `The Hill`。`for k, v in newsdict.items()` 这一部分是此行代码的无名英雄。它遍历了 `candidatenews`
    中所有字典的所有键/值对。'
- en: It is easy to create a pandas DataFrame by passing the list of dictionaries
    to the pandas `DataFrame` method. We do this in *Step 7*. The main complication
    is that we need to convert the date column from a string to a date since dates
    are just strings in JSON.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将字典列表传递给 pandas 的 `DataFrame` 方法，很容易创建一个 pandas DataFrame。我们在 *步骤 7* 中执行了这一操作。主要的复杂性在于我们需要将日期列从字符串转换为日期格式，因为在
    JSON 中日期实际上只是字符串。
- en: There’s more…
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: In *Step* *5* and *6*, we use `item.get('source')` instead of `item['source']`.
    This is handy when there might be missing keys in a dictionary. `get` returns
    `None` when the key is missing, but we can use an optional second argument to
    specify a value to return.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤* *5* 和 *6* 中，我们使用 `item.get('source')` 而不是 `item['source']`。当字典中可能缺少键时，这非常方便。`get`
    方法在键缺失时返回 `None`，但我们可以使用可选的第二个参数指定返回的值。
- en: I renamed the `date` column `storydate` in *Step 8*. This is not necessary but
    is a good idea. Not only does `date` not tell you anything about what the dates
    actually represent but it is also so generic a column name that it is bound to
    cause problems at some point.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 *步骤 8* 中将 `date` 列重命名为 `storydate`。这不是必须的，但这是一个好主意。`date` 列不仅不能告诉你日期实际代表什么，而且它作为列名太过通用，最终会在某些情况下引起问题。
- en: The news stories data fits nicely into a tabular structure. It makes sense to
    represent each list item as one row and the key/value pairs as columns and column
    values for that row. There are no significant complications, such as key values
    that are themselves lists of dictionaries. Imagine an `authors` key for each story
    with a list item for each author as the key value, and that list item is a dictionary
    of information about the author. This is not at all unusual when working with
    JSON data in Python. The next recipe shows how to work with data structured in
    this way.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻故事数据非常适合表格结构。将每个列表项表示为一行，键值对作为该行的列和值，逻辑上是合理的。没有复杂的情况，例如键值本身是字典列表。假设每个故事的 `authors`
    键包含一个列表，列表中的每个项代表一位作者，而该项是关于作者的字典信息。在使用 Python 处理 JSON 数据时，这种情况并不少见。下一个食谱将展示如何处理这种结构的数据。
- en: Importing more complicated JSON data from an API
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 API 导入更复杂的 JSON 数据
- en: In the previous recipe, we discussed one significant advantage (and challenge)
    of working with JSON data – its flexibility. A JSON file can have just about any
    structure its authors can imagine. This often means that this data does not have
    the tabular structure of the data sources we have discussed so far and that pandas
    DataFrames have. Often, analysts and application developers use JSON precisely
    because it does not insist on a tabular structure. I know I do!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的食谱中，我们讨论了处理 JSON 数据的一个重要优势（也是挑战）——它的灵活性。一个 JSON 文件可以拥有几乎任何其作者能想象的结构。这通常意味着这些数据并不像我们到目前为止讨论过的数据源那样具有表格结构，也不像
    pandas DataFrame 那样有明确的行列结构。分析师和应用程序开发者之所以使用 JSON，正是因为它不强制要求表格结构。我知道我就是这么做的！
- en: Retrieving data from multiple tables often requires us to do a one-to-many merge.
    Saving that data to one table or file means duplicating data on the “one” side
    of the one-to-many relationship. For example, student demographic data is merged
    with data on the courses studied, and the demographic data is repeated for each
    course. With JSON, duplication is not required to capture these items of data
    in one file. We can have data on the courses studied nested within the data for
    each student.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从多个表中检索数据通常需要我们进行一对多合并。将这些数据保存到一个表或文件中意味着在“一”方重复数据。例如，学生的人口统计数据与所学课程的数据合并，人口统计数据会为每个课程重复。在
    JSON 中，不需要重复数据即可在一个文件中捕获这些数据项。我们可以将所学课程的数据嵌套在每个学生的数据中。
- en: But doing analysis with JSON structured in this way will eventually require
    us to either manipulate the data in a very different way than we are used to doing
    or convert the JSON to a tabular form. We examine the first approach in the *Classes
    that handle non-tabular data structures* recipe in *Chapter 12*, *Automate Data
    Cleaning with User-Defined Functions and Classes*. This recipe takes the second
    approach. It uses a very handy tool for converting selected nodes of JSON to a
    tabular structure – `json_normalize`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，以这种方式结构化的 JSON 数据进行分析，最终会要求我们以不同于习惯的方式操作数据，或者将 JSON 转换为表格形式。我们将在 *第 12 章：使用用户定义函数和类自动化数据清理*
    的 *处理非表格数据结构的类* 食谱中探讨第一种方法。本食谱采用第二种方法，使用一个非常方便的工具 `json_normalize` 将 JSON 的指定节点转换为表格结构。
- en: We first use an API to get JSON data because that is how JSON is frequently
    consumed. One advantage of retrieving the data with an API, rather than working
    from a file we have saved locally, is that it is easier to rerun our code when
    the source data is refreshed.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 API 获取 JSON 数据，因为这正是 JSON 数据常见的使用方式。通过 API 获取数据的一个优势是，相比于从本地保存的文件中读取数据，当源数据更新时，我们的代码可以更容易地重新运行。
- en: Getting ready
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe assumes you have the `requests` and `pprint` libraries already installed.
    If they are not installed, you can install them with pip. From the terminal (or
    PowerShell in Windows), enter `pip` `install` `requests` and `pip` `install` `pprint`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱假设你已经安装了 `requests` 和 `pprint` 库。如果没有安装，你可以通过 pip 安装它们。在终端（或 Windows 中的 PowerShell）中，输入
    `pip install requests` 和 `pip install pprint`。
- en: 'The following is the structure of the JSON file that is created when using
    the Collections API of the Cleveland Museum of Art. There is a helpful *info*
    section at the beginning, but we are interested in the *data* section. This data
    does not fit nicely into a tabular data structure. There may be several `citations`
    objects and several `creators` objects for each collection object. I have abbreviated
    the JSON file to save space:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用克利夫兰艺术博物馆Collections API时生成的JSON文件结构。文件开头有一个有用的*info*部分，但我们关注的是*data*部分。由于这些数据不适合直接放入表格结构中，所以我们需要展平数据。每个收藏项可能有多个`citations`对象和多个`creators`对象。我已简化了JSON文件以节省空间：
- en: '[PRE37]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '**Data note**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: The API used in this recipe is provided by the Cleveland Museum of Art. It is
    available for public use at [https://openaccess-api.clevelandart.org/](https://openaccess-api.clevelandart.org/).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方使用的API由克利夫兰艺术博物馆提供，可以在[https://openaccess-api.clevelandart.org/](https://openaccess-api.clevelandart.org/)上公开访问。
- en: Since the call to the API retrieves real-time data, you may get different output
    from running the code in this recipe.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于调用API会检索实时数据，运行此代码时可能会得到不同的输出。
- en: How to do it...
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'Create a DataFrame from the museum’s collections data with one row for each
    `citation`, and the `title` and `creation_date` duplicated:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用博物馆的收藏数据创建一个DataFrame，每个`citation`对应一行，`title`和`creation_date`会被重复：
- en: Import the `json`, `requests`, and `pprint` libraries.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`json`、`requests`和`pprint`库。
- en: 'We need the `requests` library to use an API to retrieve JSON data. `pprint`
    improves the display of lists and dictionaries:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要`requests`库来使用API获取JSON数据。`pprint`可以改善列表和字典的显示效果：
- en: '[PRE38]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Use an API to load the JSON data.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用API加载JSON数据。
- en: 'Make a `get` request to the Collections API of the Cleveland Museum of Art.
    Use the query string to indicate that you just want collections from African-American
    artists. Display the first collection item. I have truncated the output for the
    first item to save space:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 向克利夫兰艺术博物馆的Collections API发送`get`请求。使用查询字符串来表示只获取非裔美国艺术家的收藏。显示第一个收藏项。为了节省空间，我已截断了第一个项目的输出：
- en: '[PRE39]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Flatten the JSON data.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展平JSON数据。
- en: 'Create a DataFrame from the JSON data using the `json_normalize` method. Indicate
    that the number of citations will determine the number of rows, and that `accession_number`,
    `title`, `creation_date`, `collection`, `creators`, and `type` will be repeated.
    Observe that the data has been flattened by displaying the first two observations,
    transposing them with the `.T` option to make it easier to view:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`json_normalize`方法从JSON数据创建DataFrame。需要说明的是，引用的数量将决定行数，而`accession_number`、`title`、`creation_date`、`collection`、`creators`和`type`将被重复显示。可以通过显示前两条记录并使用`.T`选项转置它们，来观察数据已经被展平：
- en: '[PRE43]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Pull the `birth_year` value from `creators`:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`creators`中提取`birth_year`值：
- en: '[PRE45]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This gives us a pandas DataFrame with one row for each `citation` for each collection
    item, with the collection information (`title`, `creation_date`, and so on) duplicated.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个pandas DataFrame，其中每个`citation`都有一行，并且每个收藏项的信息（`title`、`creation_date`等）会被重复。
- en: How it works…
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: We work with a much more *interesting* JSON file in this recipe than in the
    previous one. Each object in the JSON file is an item in the collection of the
    Cleveland Museum of Art. Nested within each collection item are one or more citations.
    The only way to capture this information in a tabular DataFrame is to flatten
    it. There are also one or more dictionaries for the creators of the collection
    item (the artist or artists). That dictionary (or dictionaries) contains the `birth_year`
    value that we want.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这道配方中，我们使用的JSON文件比前一个更为*有趣*。JSON文件中的每个对象都是克利夫兰艺术博物馆收藏中的一项物品。在每个收藏项内，包含一个或多个引用。唯一能够将这些信息以表格形式存储的方法是将其展平。每个收藏项还有一个或多个关于创作者（艺术家）的字典。这些字典包含我们所需的`birth_year`值。
- en: We want one row for every citation for all collection items. To understand this,
    imagine that we are working with relational data and have a collections table
    and a citations table and that we are doing a one-to-many merge from collections
    to citations. We do something similar with `json_normalize` by using *citations*
    as the second parameter. That tells `json_normalize` to create one row for each
    citation and use the key values in each citation dictionary – for `citation`,
    `page_number`, and `url` – as data values.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望每个集合项的每个引用都对应一行。为了理解这一点，可以想象我们正在处理关系数据，拥有一个集合表和一个引用表，我们正在进行集合到引用的一个对多合并。我们通过将*citations*作为第二个参数，使用`json_normalize`做类似的事情。这告诉`json_normalize`为每个引用创建一行，并使用每个引用字典中的键值——`citation`、`page_number`和`url`——作为数据值。
- en: The third parameter in the call to `json_normalize` has the list of column names
    for the data that will be repeated with each citation. Notice that `access_number`,
    `title`, `creation_date`, `collection`, `creators`, and `type` are repeated in
    the first two observations. `Citation` and `page_number` change. (`url` is the
    same value for the first and second citations. Otherwise, it would also change.)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`json_normalize`时的第三个参数包含了列名列表，这些列将在每个引用中重复。注意到`access_number`、`title`、`creation_date`、`collection`、`creators`和`type`在前两个观测值中是重复的。`Citation`和`page_number`发生变化。（`url`在第一个和第二个引用中是相同的值。否则，它也会发生变化。）
- en: This still leaves us with the problem of the `creators` dictionaries (there
    can be more than one creator). When we ran `json_normalize`, it grabbed the value
    for each key we indicated (in the third parameter) and stored it in the data for
    that column and row, whether that value was simple text or a list of dictionaries,
    as is the case for creators. We take a look at the first (and in this case, only)
    `creators` item for the first collections row in *Step 4*, naming it `creator`.
    (Note that the `creators` list is duplicated across all `citations` for a collection
    item, just as the values for `title`, `creation_date`, and so on are.)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然给我们留下了`creators`字典的问题（可能有多个创作者）。当我们运行`json_normalize`时，它会抓取我们指示的每个键的值（在第三个参数中），并将其存储在该列和行的数据中，无论该值是简单文本还是字典列表，如`creators`的情况。我们查看*步骤4*中第一个集合行的第一个（也是唯一的）`creators`项，将其命名为`creator`。（注意，`creators`列表在所有集合项的`citations`中是重复的，正如`title`、`creation_date`等值也是一样。）
- en: 'We want the birth year of the first creator for each collection item, which
    can be found at `creator[0][''birth_year'']`. To create a `birthyear` series using
    this, we use `apply` and a `lambda` function:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要每个集合项的第一个创作者的出生年份，可以在`creator[0]['birth_year']`中找到。为了使用这个值创建`birthyear`系列，我们使用`apply`和`lambda`函数：
- en: '[PRE51]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We take a closer look at lambda functions in *Chapter 6*, *Cleaning and Exploring
    Data with Series Operations*. Here, it is helpful to think of the `x` as representing
    the `creators` series, so `x[0]` gives us the list item we want, `creators[0]`.
    We grab the value from the `birth_year` key.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仔细研究了*第6章*，*使用系列操作清理和探索数据*中的lambda函数。在这里，将`x`视为表示`creators`系列是有帮助的，因此`x[0]`给我们带来我们想要的列表项`creators[0]`。我们从`birth_year`键中提取值。
- en: There’s more…
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: You may have noticed that we left out some of the JSON returned by the API in
    our call to `json_normalize`. The first parameter that we passed to `json_normalize`
    was `camcollections['data']`. Effectively, we ignore the info object at the beginning
    of the JSON data. The information we want does not start until the data object.
    This is not very different conceptually from the `skiprows` parameter in the second
    recipe of the previous chapter. There is sometimes metadata like this at the beginning
    of JSON files.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，我们在调用`json_normalize`时省略了一些API返回的JSON。我们传递给`json_normalize`的第一个参数是`camcollections['data']`。实际上，我们忽略了JSON数据开头的`info`对象。我们需要的信息直到`data`对象才开始。这在概念上与上一章第二个食谱中的`skiprows`参数没有太大区别。JSON文件开头有时会包含类似的元数据。
- en: See also
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The preceding recipe demonstrates some useful techniques for doing data integrity
    checks without pandas, including list operations and comprehensions. Those are
    all relevant to the data in this recipe as well.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上述食谱展示了一些有用的技术，用于在没有pandas的情况下进行数据完整性检查，包括列表操作和推导式。这些对于本食谱中的数据也是相关的。
- en: Importing data from web pages
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从网页导入数据
- en: We use **Beautiful Soup** in this recipe to scrape data from a web page and
    load that data into pandas. **Web scraping** is very useful when there is data
    on a website that is updated regularly but there is no API. We can rerun our code
    to generate new data whenever the page is updated.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本教程中使用**Beautiful Soup**从网页抓取数据，并将这些数据加载到pandas中。**网页抓取**在网站上有定期更新的数据而没有API时非常有用。每当页面更新时，我们可以重新运行代码生成新的数据。
- en: Unfortunately, the web scrapers we build can be broken when the structure of
    the targeted page changes. That is less likely to happen with APIs because they
    are designed for data exchange and carefully curated with that end in mind. The
    priority for most web designers is the quality of the display of information,
    not the reliability and ease of data exchange. This causes data cleaning challenges
    that are unique to web scraping, including HTML elements that house the data in
    surprising and changing locations, formatting tags that obfuscate the underlying
    data, and explanatory text that aid data interpretation being difficult to retrieve.
    In addition to these challenges, scraping presents data cleaning issues that are
    familiar, such as changing data types in columns, less-than-ideal headings, and
    missing values. We will deal with data issues that occur most frequently in this
    recipe.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当目标页面的结构发生变化时，我们构建的网络爬虫可能会被破坏。而API则不太可能发生这种情况，因为它们是为数据交换设计的，并且在设计时就考虑到了这一点。大多数网页设计师的优先考虑事项是信息展示的质量，而非数据交换的可靠性和便捷性。这就导致了独特的网页抓取数据清理挑战，包括HTML元素将数据存放在不同且不断变化的位置、格式化标签使得底层数据变得模糊不清，以及解释性文本帮助数据解读但难以提取。除了这些挑战，抓取还会遇到一些常见的数据清理问题，比如列中数据类型的变化、不理想的标题以及缺失值。在本教程中，我们将处理这些最常见的数据问题。
- en: Getting ready
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need Beautiful Soup installed to run the code in this recipe. You can
    install it with pip by entering `pip install beautifulsoup4` in a terminal window
    or Windows PowerShell.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本教程中的代码，你需要安装Beautiful Soup。你可以通过在终端窗口或Windows PowerShell中输入`pip install
    beautifulsoup4`来安装它。
- en: 'We will scrape data from a web page, find the following table on that page,
    and load it into a pandas DataFrame:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个网页中抓取数据，找到该页面上的以下表格，并将其加载到pandas DataFrame中：
- en: '![](img/Image1956.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image1956.png)'
- en: 'Figure 2.1: COVID-19 data from countries with the lowest cases per million
    in population'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：COVID-19数据来自病例数每百万人最少的国家
- en: '**Data** **note**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据** **备注**'
- en: I created this web page, [http://www.alrb.org/datacleaning/highlowcases.html](http://www.alrb.org/datacleaning/highlowcases.html),
    based on COVID-19 data for public use from *Our World in Data*, available at [https://ourworldindata.org/covid-cases](https://ourworldindata.org/covid-cases).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了这个网页，[http://www.alrb.org/datacleaning/highlowcases.html](http://www.alrb.org/datacleaning/highlowcases.html)，基于来自*Our
    World in Data*的COVID-19数据，供公众使用，数据可在[https://ourworldindata.org/covid-cases](https://ourworldindata.org/covid-cases)找到。
- en: How to do it…
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We scrape the COVID-19 data from the website and do some routine data checks:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从网站抓取COVID-19数据并进行一些常规的数据检查：
- en: 'Import the `pprint`, `requests`, and `BeautifulSoup` libraries:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pprint`、`requests`和`BeautifulSoup`库：
- en: '[PRE52]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parse the web page and get the header row of the table.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析网页并获取表格的表头行。
- en: 'Use Beautiful Soup’s `find` method to get the table we want and then use `find_all`
    to retrieve the elements nested within the `th` elements for that table. Create
    a list of column labels based on the text of the `th` rows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Beautiful Soup的`find`方法获取我们需要的表格，然后使用`find_all`方法提取该表格中`th`元素内的内容。根据`th`行的文本创建列标签列表：
- en: '[PRE53]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Get the data from the table cells.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从表格单元格中获取数据。
- en: Find all of the table rows for the table we want. For each table row, find the
    `th` element and retrieve the text. We will use that text for our row labels.
    Also, for each row, find all the `td` elements (the table cells with the data)
    and save text from all of them in a list.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 找到我们需要的表格的所有表格行。对于每一行表格，找到`th`元素并提取文本。我们将使用这些文本作为行标签。同时，对于每一行，找到所有的`td`元素（表格单元格中的数据），并将它们的文本存入一个列表中。
- en: 'This gives us `datarows`, which has all the numeric data in the table. (You
    can confirm that it matches the table from the web page.) We then insert the `labelrows`
    list (which has the row headings) at the beginning of each list in `datarows`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了`datarows`，其中包含表格中的所有数字数据。（你可以确认它与网页中的表格匹配。）然后，我们将`labelrows`列表（包含行标题）插入到`datarows`中的每个列表的开头：
- en: '[PRE57]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Load the data into pandas.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据加载到pandas中。
- en: 'Pass the `datarows` list to the `DataFrame` method of pandas. Notice that all
    data is read into pandas with the object data type and that some data has values
    that cannot be converted into numeric values in their current form (due to the
    commas):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `datarows` 列表传递给 pandas 的 `DataFrame` 方法。请注意，所有数据都以对象数据类型读取到 pandas 中，并且一些数据的当前格式无法转换为数值（由于逗号的存在）：
- en: '[PRE63]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Fix the column names and convert the data to numeric values.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修复列名并将数据转换为数值。
- en: 'Remove spaces from the column names. Remove all non-numeric data from the first
    columns with data, including the commas (`str.replace("[^0-9]",""`). Convert to
    numeric values, handling most columns as integers, the `last_date` column as a
    datetime, `median_age` as float, and leaving `rowheadings` as an object:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 删除列名中的空格。删除第一列数据中的所有非数字数据，包括逗号（`str.replace("[^0-9]","")`）。将数据转换为数值，处理大多数列为整数，将
    `last_date` 列转换为日期时间格式，将 `median_age` 列转换为浮动数值，并将 `rowheadings` 保留为对象类型：
- en: '[PRE67]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: We have now created a pandas DataFrame from an `html` table.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经从 `html` 表格创建了一个 pandas DataFrame。
- en: How it works…
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Beautiful Soup is a very useful tool for finding specific HTML elements in a
    web page and retrieving text from them. You can get one HTML element with `find`
    and get one or more with `find_all`. The first argument for both `find` and `find_all`
    is the HTML element to get. The second argument takes a Python dictionary of attributes.
    You can retrieve text from all of the HTML elements you find with `get_text`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup 是一个非常有用的工具，用于在网页中查找特定的 HTML 元素并从中提取文本。你可以使用 `find` 获取一个 HTML
    元素，使用 `find_all` 获取一个或多个元素。`find` 和 `find_all` 的第一个参数是要获取的 HTML 元素，第二个参数是一个 Python
    字典，包含属性。你可以使用 `get_text` 从所有找到的 HTML 元素中提取文本。
- en: 'Some amount of looping is usually necessary to process the elements and text,
    as with *Step 2* and *Step 3*. These two statements in *Step 2* are fairly typical:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 处理元素和文本通常需要一定的循环操作，就像在 *步骤 2* 和 *步骤 3* 中一样。*步骤 2* 中的这两条语句是比较典型的：
- en: '[PRE69]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The first statement finds all the `th` elements we want and creates a Beautiful
    Soup result set called `theadrows` from the elements it found. The second statement
    iterates over the `theadrows` Beautiful Soup result set using the `get_text` method
    to get the text from each element and then stores it in the `labelcols` list.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个语句查找我们想要的所有 `th` 元素，并从找到的元素创建一个名为 `theadrows` 的 Beautiful Soup 结果集。第二个语句使用
    `get_text` 方法遍历 `theadrows` 结果集，从每个元素中提取文本，并将其存储在 `labelcols` 列表中。
- en: '*Step 3* is a little more involved but makes use of the same Beautiful Soup
    methods. We find all of the table rows (`tr`) in the target table (`rows = bs.find(''table'',
    {''id'':''tblLowCases''}).tbody.find_all(''tr'')`). We then iterate over each
    of those rows, finding the `th` element and getting the text in that element (`rowlabels
    = row.find(''th'').get_text()`). We also find all of the table cells (`td`) for
    each row (`cells = row.find_all(''td'', {''class'':''data''}`) and get the text
    from all table cells (`cellvalues = [j.get_text() for j in cells]`). Note that
    this code is dependent on the class of the `td` elements being `data`.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 3* 稍微复杂一些，但它使用了相同的 Beautiful Soup 方法。我们在目标表格中查找所有的表格行 (`tr`)（`rows = bs.find(''table'',
    {''id'':''tblLowCases''}).tbody.find_all(''tr'')`）。然后我们遍历每一行，查找 `th` 元素并获取该元素中的文本（`rowlabels
    = row.find(''th'').get_text()`）。我们还查找每行的所有表格单元格 (`td`)（`cells = row.find_all(''td'',
    {''class'':''data''})`）并提取所有表格单元格的文本（`cellvalues = [j.get_text() for j in cells]`）。请注意，这段代码依赖于
    `td` 元素的类名为 `data`。'
- en: 'Finally, we insert the row labels we get from the `th` elements at the beginning
    of each list in `datarows`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将从 `th` 元素中获取的行标签插入到 `datarows` 中每个列表的开头：
- en: '[PRE70]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: In *step 4*, we use the `DataFrame` method to load the list we created in *Steps
    2* and *3* into pandas. We then do some cleaning similar to what we have done
    in previous recipes in this chapter. We use `string replace` to remove spaces
    from column names and to remove all non-numeric data, including commas, from what
    are otherwise valid numeric values. We convert all columns, except for the `rowheadings`
    column, to numeric.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 4* 中，我们使用 `DataFrame` 方法将我们在 *步骤 2* 和 *步骤 3* 中创建的列表加载到 pandas 中。然后，我们进行一些清理工作，类似于本章之前的配方。我们使用
    `string replace` 移除列名中的空格，并删除所有非数字数据（包括逗号），以便这些原本有效的数字值能正确转换。我们将所有列转换为数值，除了 `rowheadings`
    列保留为对象类型。
- en: There’s more…
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'Our scraping code is dependent on several aspects of the web page’s structure
    not changing: the ID of the table of interest, the presence of `th` tags with
    column and row labels, and the `td` elements continuing to have their class equal
    to data. The good news is that if the structure of the web page does change, this
    will likely only affect the `find` and `find_all` calls. The rest of the code
    would not need to change.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的爬取代码依赖于网页结构的几个方面不发生变化：感兴趣的表格的ID、包含列和行标签的`th`标签的存在，以及`td`元素继续拥有其类名等于data。好消息是，如果网页结构确实发生变化，通常只会影响`find`和`find_all`调用。其余的代码无需更改。
- en: Working with Spark data
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark数据
- en: When working with large datasets, we sometimes need to rely on distributed resources
    to clean and manipulate our data. With Apache Spark, analysts can take advantage
    of the combined processing power of many machines. We will use PySpark, a Python
    API for working with Spark, in this recipe. We will also go over how to use PySpark
    tools to take a first look at our data, select parts of our data, and generate
    some simple summary statistics.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大型数据集时，我们有时需要依赖分布式资源来清理和操作数据。使用Apache Spark，分析师可以利用多台计算机的处理能力。本教程中我们将使用PySpark，它是一个用于处理Spark数据的Python
    API。我们还将介绍如何使用PySpark工具来初步查看数据、选择数据的某些部分，并生成一些简单的汇总统计信息。
- en: Getting ready
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To run the code in this section, you need to get Spark running on your computer.
    If you have installed Anaconda, you can follow these steps to work with Spark:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本节中的代码，你需要在计算机上运行Spark。如果你已经安装了Anaconda，可以按照以下步骤操作来使用Spark：
- en: Install `Java` with `conda install openjdk`.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`conda install openjdk`安装`Java`。
- en: Install `PySpark` with `conda install pyspark` or `conda install -c conda forge
    pyspark`.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`conda install pyspark`或`conda install -c conda-forge pyspark`安装`PySpark`。
- en: Install `findspark` with `conda install -c conda-forge findspark`.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`conda install -c conda-forge findspark`安装`findspark`。
- en: '**Note**'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Installation of PySpark can be tricky, particularly setting the necessary environment
    variables. While `findspark` helps with this, a common problem is that the Java
    installation is not recognized when running PySpark commands. If you get the dreaded
    `JAVA_HOME is not set` error when attempting to run the code in this recipe, then
    there is a good chance that that is your problem.
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安装PySpark可能会有些棘手，特别是在设置必要的环境变量时。虽然`findspark`有助于此，但一个常见问题是，当运行PySpark命令时，Java安装可能无法被识别。如果在尝试运行本教程中的代码时出现`JAVA_HOME
    is not set`错误，那么很可能是这个问题。
- en: '*Step 3* at the following link shows you how to set the environment variables
    for Linux, macOS, and Windows machines: [https://www.dei.unipd.it/~capri/BDC/PythonInstructions.html](https://www.dei.unipd.it/~capri/BDC/PythonInstructions.html).'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下链接中的*步骤3*展示了如何为Linux、macOS和Windows机器设置环境变量：[https://www.dei.unipd.it/~capri/BDC/PythonInstructions.html](https://www.dei.unipd.it/~capri/BDC/PythonInstructions.html)。
- en: We will work with the land temperature data from *Chapter 1*, *Anticipating
    Data Cleaning Issues When Importing Tabular Data with pandas*, and the candidate
    news data from this chapter. All data and the code we will be running in this
    recipe are available in the GitHub repository for this book.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自*第1章*的陆地温度数据、*使用pandas导入表格数据时预期的数据清理问题*以及本章的候选新闻数据。所有数据和我们将在本教程中运行的代码都可以在本书的GitHub仓库中找到。
- en: '**Data** **note**'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据** **注意**'
- en: This dataset, taken from the Global Historical Climatology Network integrated
    database, is made available for public use by the United States National Oceanic
    and Atmospheric Administration at [https://www.ncei.noaa.gov/data/global-historical-climatology-network-monthly/v4/](https://www.ncei.noaa.gov/data/global-historical-climatology-network-monthly/v4/).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自全球历史气候网络集成数据库，由美国国家海洋和大气管理局提供，供公众使用，网址：[https://www.ncei.noaa.gov/data/global-historical-climatology-network-monthly/v4/](https://www.ncei.noaa.gov/data/global-historical-climatology-network-monthly/v4/)。
- en: We will use PySpark in this recipe to read data we have in local storage.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中我们将使用PySpark来读取存储在本地的数据。
- en: How to do it...
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'To read and explore the data, follow these steps:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取和探索数据，请按照以下步骤操作：
- en: 'Let’s start a Spark session and load the land temperature data. We can use
    the read method of the session object to create a Spark DataFrame. We indicate
    that the first row of the CSV file we are importing has a header:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始一个Spark会话并加载陆地温度数据。我们可以使用会话对象的read方法来创建一个Spark DataFrame。我们需要指出，导入的CSV文件的第一行是标题：
- en: '[PRE71]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Notice that the `read` method returns a Spark DataFrame, not a pandas DataFrame.
    We will need to use different methods to view our data than those we have used
    so far.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`read`方法返回的是Spark DataFrame，而不是pandas DataFrame。我们将需要使用不同的方法来查看数据，而不是我们目前为止使用的方法。
- en: We load the full dataset, not just a 100,000-row sample as we did in the first
    chapter. If your system is low on resources, you can import the `landtempssample.csv`
    file instead.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了完整的数据集，而不仅仅是第一章中我们使用的10万行样本。如果你的系统资源有限，可以改为导入`landtempssample.csv`文件。
- en: 'We should take a look at the number of rows and the column names and data types
    that were imported. The `temp` column was read as a string. It should be a float.
    We will fix that in a later step:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该查看导入的行数、列名以及数据类型。`temp`列被读取为字符串类型，但它应该是浮动类型。我们将在后续步骤中修正这一点：
- en: '[PRE73]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Let’s look at the data for a few rows. We can choose a subset of the columns
    by using the `select` method:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看几行数据。我们可以通过使用`select`方法选择部分列：
- en: '[PRE77]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We should fix the data type of the `temp` column. We can use the `withColumn`
    function to do a range of column operations in Spark. Here, we use it to cast
    the `temp` column to `float`:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该修正`temp`列的数据类型。我们可以使用`withColumn`函数在Spark中进行一系列列操作。在这里，我们使用它将`temp`列转换为`float`类型：
- en: '[PRE79]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now we can run summary statistics on the `temp` variable. We can use the `describe`
    method for that:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以对`temp`变量运行摘要统计信息。我们可以使用`describe`方法来实现：
- en: '[PRE81]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The Spark session’s read method can import a variety of different data files,
    not just CSV files. Let’s try that with the `allcandidatenews` JSON file that
    we worked with earlier in this chapter:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark会话的read方法可以导入各种不同的数据文件，而不仅仅是CSV文件。让我们尝试使用之前在本章中处理过的`allcandidatenews` JSON文件：
- en: '[PRE83]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'We can use the `count` and `printSchema` methods again to look at our data:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以再次使用`count`和`printSchema`方法来查看数据：
- en: '[PRE85]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'We can also generate some summary statistics on the `story_position` variable:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以对`story_position`变量生成一些摘要统计信息：
- en: '[PRE89]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: These steps demonstrate how to import data files into a Spark DataFrame, view
    the structure of the data, and generate summary statistics.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤展示了如何将数据文件导入到Spark DataFrame，查看数据结构，并生成摘要统计信息。
- en: How it works...
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The PySpark API significantly reduces the amount of work Python programmers
    have to do to use Apache Spark to handle large data files. We get methods to work
    with that are not very different from the methods we use with pandas DataFrames.
    We can see the number of rows and columns, examine and change data types, and
    get summary statistics.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark API显著减少了Python程序员使用Apache Spark处理大数据文件时需要做的工作。我们可以使用的操作方法与我们在pandas
    DataFrame中使用的并没有太大区别。我们可以查看行列数、检查并更改数据类型，生成摘要统计信息。
- en: There’s more...
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: At some point in our analysis, we might want to convert the Spark DataFrame
    into a pandas DataFrame. This is a fairly expensive process, and we will lose
    the benefits of working with Spark, so we typically will not do that unless we
    are at the point of our analysis when we require the pandas library, or a library
    that depends on pandas. But when we need to move to pandas, it is very easy to
    do – though if you are working with a lot of data and your machine’s processor
    and RAM are not exactly top of the line, you might want to start the conversion
    and then go have some tea or coffee.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们分析的某个阶段，我们可能需要将Spark DataFrame转换为pandas DataFrame。这是一个相对昂贵的过程，并且我们会失去使用Spark的优势，所以通常只有在分析阶段需要pandas库或依赖pandas的库时，我们才会这样做。但当我们需要转换到pandas时，操作非常简单——不过如果你处理的是大量数据，而且你的计算机处理器和内存并不算顶级，可能最好先开始转换，然后去喝个茶或咖啡。
- en: 'The following code converts the `allcandidatenews` Spark DataFrame that we
    created to a pandas DataFrame and displays the resulting DataFrame structure:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将我们创建的`allcandidatenews` Spark DataFrame转换为pandas DataFrame，并显示结果DataFrame的结构：
- en: '[PRE91]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We have been largely working with non-traditional data stores in this chapter:
    JSON files, data from HTML pages, and Spark files. We often reach a point in our
    data cleaning work where it makes sense to preserve the results of that cleaning
    by persisting data. At the end of *Chapter 1*, *Anticipating Data Cleaning Issues
    When Importing Tabular Data with pandas*, we examined how to persist tabular data.
    That works fine in cases where our data can be captured well with columns and
    rows. When it cannot (say, when we are working with a JSON file that has complicated
    subdocuments), we might want to preserve that structure when persisting data.
    In the next recipe, we go over persisting JSON data.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们主要使用了非传统数据存储：JSON 文件、HTML 页面中的数据和 Spark 文件。在我们的数据清理工作中，通常会遇到一个阶段，在这个阶段保存数据清理结果是有意义的。我们在
    *第一章* 的结尾——*通过 pandas 导入表格数据时预见数据清理问题* 中，已经探讨过如何持久化表格数据。在数据能很好地通过列和行来捕捉的情况下，这种方法效果很好。但当数据无法用表格结构表示时（例如，当我们处理的是包含复杂子文档的
    JSON 文件时），我们可能希望在持久化数据时保留其原始结构。在接下来的食谱中，我们将讲解如何持久化 JSON 数据。
- en: Persisting JSON data
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化 JSON 数据
- en: 'There are several reasons why we might want to serialize a JSON file:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能有几个原因希望序列化 JSON 文件：
- en: We may have retrieved the data with an API but need to keep a snapshot of the
    data.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能通过 API 获取了数据，但需要保存数据的快照。
- en: The data in the JSON file is relatively static and informs our data cleaning
    and analysis over multiple phases of a project.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON 文件中的数据相对静态，并且在项目的多个阶段中为我们的数据清理和分析提供信息。
- en: We might decide that the flexibility of a schema-less format such as JSON helps
    us solve many data cleaning and analysis problems.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能会决定，像 JSON 这样的无模式格式的灵活性有助于解决许多数据清理和分析问题。
- en: It is worth highlighting this last reason to use JSON – that it can solve many
    data problems. Although tabular data structures clearly have many benefits, particularly
    for operational data, they are often not the best way to store data for analysis
    purposes. In preparing data for analysis, a substantial amount of time is spent
    either merging data from different tables or dealing with data redundancy when
    working with flat files. Not only are these processes time-consuming but every
    merge or reshaping leaves the door open to a data error of broad scope. This can
    also mean that we end up paying too much attention to the mechanics of manipulating
    data and too little to the conceptual issues at the core of our work.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 值得特别提到的是使用 JSON 的最后一个原因——它可以解决许多数据问题。尽管表格数据结构显然有很多优点，尤其对于操作性数据，它们往往不是存储分析数据的最佳方式。在准备数据进行分析时，往往会花费大量时间将不同表格的数据合并，或者在处理平面文件时应对数据冗余问题。这些过程不仅耗时，而且每次合并或重塑都会让我们面临广泛的数据错误风险。这也意味着我们可能会过于关注数据操作的技巧，而忽视了工作核心的概念问题。
- en: We return to the Cleveland Museum of Art collections data in this recipe. There
    are at least three possible units of analysis for this data file – the collection
    item level, the creator level, and the citation level. JSON allows us to nest
    citations and creators within collections. (You can examine the structure of the
    JSON file in the *Getting ready* section of this recipe.) This data cannot be
    persisted in a tabular structure without flattening the file, which we did in
    the *Importing more complicated JSON data from an API* recipe earlier in this
    chapter. In this recipe, we will use two different methods to persist JSON data,
    each with its own advantages and disadvantages.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇食谱中，我们回到克利夫兰艺术博物馆的藏品数据。该数据文件至少有三个可能的分析单位——藏品项目级别、创作者级别和引用级别。JSON 允许我们在藏品中嵌套引用和创作者。（你可以在本食谱的*准备工作*部分查看
    JSON 文件的结构。）如果不扁平化文件，这些数据无法以表格结构持久化，这一点我们在本章前面 *从 API 导入更复杂的 JSON 数据* 食谱中已经做过了。在本食谱中，我们将使用两种不同的方法来持久化
    JSON 数据，每种方法都有其优缺点。
- en: Getting ready
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will be working with data on the Cleveland Museum of Art’s collection of
    works by African-American artists. The following is the structure of the JSON
    data returned by the API. It has been abbreviated to save space:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自克利夫兰艺术博物馆的关于非裔美国艺术家作品的收藏数据。以下是 API 返回的 JSON 数据的结构。为了节省空间，数据已被简化：
- en: '[PRE93]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: How to do it...
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'We will serialize the JSON data using two different methods:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用两种不同的方法来序列化 JSON 数据：
- en: 'Load the `pandas`, `json`, `pprint`, `requests`, and `msgpack` libraries:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 `pandas`、`json`、`pprint`、`requests` 和 `msgpack` 库：
- en: '[PRE94]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Load the JSON data from an API. I have abbreviated the JSON output:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 API 加载 JSON 数据。我已经将 JSON 输出进行了简化：
- en: '[PRE95]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Save and reload the JSON file using Python’s `json` library.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Python的`json`库保存并重新加载JSON文件。
- en: 'Persist the JSON data in human-readable form. Reload it from the saved file
    and confirm that it worked by retrieving the `creators` data from the first `collections`
    item:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 以人类可读的形式持久化JSON数据。从保存的文件重新加载数据，并通过从第一个`collections`项中检索`creators`数据来确认它是否有效：
- en: '[PRE99]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Save and reload the JSON file using `msgpack`:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`msgpack`保存并重新加载JSON文件：
- en: '[PRE101]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: How it works…
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: We use the Cleveland Museum of Art’s Collections API to retrieve collections
    items. The `african_american_artists` flag in the query string indicates that
    we just want collections for those creators. `json.loads` returns a dictionary
    called `info` and a list of dictionaries called `data`. We check the length of
    the `data` list. This tells us that there are 778 items in collections. We then
    display the first item of collections to get a better look at the structure of
    the data. (I have abbreviated the JSON output.)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用克利夫兰艺术博物馆的收藏API来获取收藏品。查询字符串中的`african_american_artists`标志表示我们只想要这些创作者的收藏。`json.loads`返回一个名为`info`的字典和一个名为`data`的字典列表。我们检查`data`列表的长度。这告诉我们收藏中有778个项目。然后，我们展示第一个收藏品以更好地了解数据的结构。（我已经简化了JSON输出。）
- en: 'We save and then reload the data using Python’s JSON library in *Step 3*. The
    advantage of persisting the data in this way is that it keeps the data in human-readable
    form. Unfortunately, it has two disadvantages: saving takes longer than alternative
    serialization methods, and it uses more storage space.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*步骤 3*中使用Python的JSON库保存并重新加载数据。以这种方式持久化数据的优点是它保持数据以人类可读的形式。不幸的是，它有两个缺点：保存的时间比其他序列化方法要长，而且它使用更多的存储空间。
- en: In *Step 4*, we use `msgpack` to persist our data. This is faster than Python’s
    `json` library, and the saved file uses less space. Of course, the disadvantage
    is that the resulting JSON is binary rather than text-based.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们使用`msgpack`来持久化数据。这比Python的`json`库更快，而且保存的文件占用更少的空间。当然，缺点是生成的JSON是二进制格式，而不是基于文本的。
- en: There’s more…
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: I use both methods for persisting JSON data in my work. When I am working with
    small amounts of data, and that data is relatively static, I prefer human-readable
    JSON. A great use case for this is the recipes in the previous chapter where we
    needed to create value labels.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的工作中，我同时使用这两种方法来持久化JSON数据。当我处理较少的数据，并且这些数据相对静态时，我更倾向于使用人类可读的JSON。这在前一章中的食谱案例中非常适用，我们需要为其创建值标签。
- en: I use `msgpack` when I am working with large amounts of data, where that data
    changes regularly. `msgpack` files are also great when you want to take regular
    snapshots of key tables in enterprise databases.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 当我处理大量数据且这些数据经常变化时，我使用`msgpack`。`msgpack`文件在你想要定期获取企业数据库中关键表的快照时也非常适用。
- en: The Cleveland Museum of Art’s collections data is similar in at least one important
    way to the data we work with every day. The unit of analysis frequently changes.
    Here, we are looking at collections, citations, and creators. In our work, we
    might have to simultaneously look at students and courses, or households and deposits.
    An enterprise database system for the museum data would likely have separate collections,
    citations, and creators tables that we would eventually need to merge. The resulting
    merged file would have data redundancy issues that we would need to account for
    whenever we changed the unit of analysis.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 克利夫兰艺术博物馆的收藏数据在至少一个重要方面与我们每天使用的数据相似。分析单元经常变化。在这里，我们查看的是收藏品、引文和创作者。在我们的工作中，我们可能需要同时查看学生和课程，或家庭和存款。博物馆数据的企业数据库系统可能会有单独的收藏品、引文和创作者表，最终我们需要将它们合并。合并后的文件将存在数据冗余问题，我们需要在每次改变分析单元时考虑到这些问题。
- en: When we alter our data cleaning process to work directly from JSON or parts
    of it, we end up eliminating a major source of errors. We do more data cleaning
    with JSON in the *Classes that handle non-tabular data structures* recipe in *Chapter
    12*, *Automate Data Cleaning with User-Defined Functions, Classes and Pipelines*.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们修改数据清理过程，使其直接处理JSON或其部分内容时，我们最终消除了一个主要的错误源。在*第12章*的*处理非表格数据结构的类*食谱中，我们用JSON进行了更多的数据清理，*自动化数据清理与用户定义的函数、类和管道*。
- en: Versioning data
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 版本控制数据
- en: There may be times when we want to persist data without overwriting a prior
    version of the data file. This can be accomplished by appending a time stamp to
    a filename or a unique identifier. However, there are more elegant solutions available.
    One such solution is the Delta Lake library, which we will explore in this recipe.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们希望在不覆盖数据文件的先前版本的情况下持久化数据。这可以通过将时间戳或唯一标识符附加到文件名来实现。然而，还有更优雅的解决方案。一种这样的解决方案就是
    Delta Lake 库，我们将在本节中进行探索。
- en: We will work with the land temperature data again in this recipe. We will load
    the data, save it to a data lake, and then save an altered version to the same
    data lake.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将再次使用陆地温度数据。我们将加载数据，保存到数据湖中，然后将更改过的版本保存到同一个数据湖。
- en: Getting ready
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备中
- en: We will be using the Delta Lake library in this recipe, which can be installed
    with `pip install deltalake`. We will also need the `os` library so that we can
    make a directory for the data lake.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将使用 Delta Lake 库，可以通过`pip install deltalake`进行安装。我们还需要`os`库，以便为数据湖创建目录。
- en: How to do it...
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'You can get started with the data and version it as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按以下方式开始使用数据并对其进行版本管理：
- en: 'We start by importing the Delta Lake library. We also create a folder called
    `temps_lake` for our data versions:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入 Delta Lake 库。然后，我们创建一个名为`temps_lake`的文件夹，用于存储我们的数据版本：
- en: '[PRE105]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Now, let’s load the land temperature data:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们加载陆地温度数据：
- en: '[PRE106]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'We save the landtemps DataFrame to the data lake:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将 landtemps DataFrame 保存到数据湖中：
- en: '[PRE108]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Let’s now retrieve the data we just saved. We specify that we want the first
    version, though that is not necessary as the most recent version will be retrieved
    when the version is not indicated. This returns a `DeltaTable` type, which we
    can convert into a pandas DataFrame:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们获取刚刚保存的数据。我们指定要获取第一个版本，尽管这并不是必要的，因为如果没有指定版本，将会获取最新版本。返回的是一个`DeltaTable`类型，我们可以将其转换为
    pandas DataFrame：
- en: '[PRE109]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Let’s persist only the first 1,000 rows of the land temperature data to the
    data lake, without replacing the existing data. We pass a value of `overwrite`
    to the `mode` parameter. This saves a new dataset to the data lake. It does not
    replace the previous one. The `overwrite` parameter value is a little confusing
    here. This will be clearer when we use the `append` parameter value later:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将前1,000行的陆地温度数据持久化到数据湖中，而不替换现有数据。我们将`overwrite`作为`mode`参数的值传入。这将保存一个新的数据集到数据湖，而不是替换之前的版本。`overwrite`参数的值在这里有点令人困惑。当我们稍后使用`append`参数值时，这一点会更清楚：
- en: '[PRE113]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Let’s now retrieve this latest version of our data. Notice that this only has
    1,000 rows:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们提取数据的最新版本。请注意，这个版本只有1,000行：
- en: '[PRE114]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'If we specify `append` instead, we add the rows of the DataFrame in the second
    argument of `write_deltalake` to the rows of the previous version:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们改为指定`append`，则会将`write_deltalake`第二个参数中的 DataFrame 行添加到先前版本的行中：
- en: '[PRE116]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Let’s confirm that our first version of the dataset in the data lake is still
    accessible and has the number of rows we expect:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们确认数据湖中第一个版本的数据集仍然可以访问，并且包含我们预期的行数：
- en: '[PRE118]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: How it works...
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The nomenclature regarding overwrite and append is a little confusing, but it
    might make more sense if you think of overwrite as a logical deletion of the previous
    dataset, not a physical deletion. The most recent version has all new data, but
    the previous versions are still stored.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 关于覆盖（overwrite）和追加（append）的术语有点令人困惑，但如果你把覆盖看作是对先前数据集的逻辑删除，而不是物理删除，可能会更容易理解。最新版本包含所有新数据，但之前的版本仍然存储着。
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The recipes in this chapter examined importing and data preparation of non-tabular
    data in a variety of forms, including JSON and HTML. We introduced Spark for working
    with big data and discussed how to persist tabular and non-tabular data. We also
    examined how to create a data lake for versioning. We will learn how to take the
    measure of our data in the next chapter.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的食谱探讨了导入和准备各种形式的非表格数据，包括 JSON 和 HTML。我们介绍了用于处理大数据的 Spark，并讨论了如何持久化表格和非表格数据。我们还研究了如何为版本管理创建数据湖。下一章，我们将学习如何评估我们的数据。
- en: Join our community on Discord
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们在 Discord 上的社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://discord.gg/p8uSgEAETX](https://discord.gg/p8uSgEAETX )'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://discord.gg/p8uSgEAETX](https://discord.gg/p8uSgEAETX )'
- en: '![](img/QR_Code10336218961138498953.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code10336218961138498953.png)'
