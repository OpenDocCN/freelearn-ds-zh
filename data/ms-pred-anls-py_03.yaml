- en: Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 在噪声中寻找模式 – 聚类和无监督学习
- en: One of the natural questions to ask about a dataset is if it contains groups.
    For example, if we examine financial market data consisting of stock price fluctuations
    over time, are there groups of stocks that fall and rise with a similar pattern?
    Similarly, for a set of customer transactions from an e-commerce business we might
    ask if are there groups of user accounts distinguished by patterns of similar
    purchasing activity? By identifying groups of related items using the methods
    described in this chapter, we can understand data as a set of general patterns
    rather than just individual points. These patterns can help in making high-level
    summaries at the outset of a predictive modeling project, or as an ongoing way
    to report on the shape of the data we are modeling. The groupings produced can
    serve as insights themselves, or they can provide starting points for the models
    we will cover in later chapters. For example, the group to which a datapoint is
    assigned can become a feature of this observation, adding additional information
    beyond its individual values. Additionally, we can potentially calculate statistics
    (such as mean and standard deviation) item features within these groups, which
    may be more robust as model features than individual entries.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据集的一个自然问题是它是否包含组。例如，如果我们检查由随时间变化的股票价格波动组成的金融市场数据，是否存在下跌和上升模式相似的股票组？同样，对于来自电子商务业务的客户交易集，我们可能会问是否存在由相似购买活动模式区分的用户账户组？通过使用本章中描述的方法识别相关项目的组，我们可以将数据视为一组通用模式，而不仅仅是单个点。这些模式可以帮助在预测建模项目的初期进行高级总结，或者作为持续报告我们正在建模的数据形状的一种方式。产生的分组本身可以成为洞察，或者可以作为我们在后续章节中讨论的模型的起点。例如，数据点所属的组可以成为此观察的特征，为其个别值添加额外的信息。此外，我们可以在这些组内潜在地计算统计量（如均值和标准差），这些统计量可能比单个条目作为模型特征更稳健。
- en: In contrast to the methods we will discuss in later chapters, grouping or *clustering*
    algorithms are known as **unsupervised learning**, meaning we have no response
    value, such as a sale price or click-through rate, which is used to determine
    the optimal parameters of the algorithm. Rather, we identify similar datapoints
    using only the features, and as a secondary analysis might ask whether the clusters
    we identify share a common pattern in their responses (and thus suggest the cluster
    is useful in finding groups associated with the outcome we are interested in).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在后续章节中将要讨论的方法相比，分组或*聚类*算法被称为**无监督学习**，这意味着我们没有用于确定算法最佳参数的响应值，例如销售价格或点击率。相反，我们仅使用特征来识别相似数据点，作为次级分析可能会问我们识别的聚类是否在其响应中共享一个共同的模式（从而表明该聚类在寻找与我们感兴趣的输出相关的组时是有用的）。
- en: 'The task of finding these groups, or *clusters*, has a few common ingredients
    that vary between algorithms. One is a notion of distance or similarity between
    items in the dataset, which will allow us to quantitatively compare them. A second
    is the number of groups we wish to identify; this can be specified initially using
    domain knowledge, or determined by running an algorithm with different numbers
    of clusters. We can then identify the best number of clusters that describes a
    dataset through statistics, such as examining numerical variance within the groups
    determined by the algorithm, or by visual inspection. In this chapter we will
    dive into:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 找到这些组或*聚类*的任务在算法之间有一些共同的成分，这些成分有所不同。一个是数据集中项目之间的距离或相似性的概念，这将使我们能够定量比较它们。第二个是我们希望识别的组数；这可以通过领域知识初始指定，或者通过运行具有不同聚类数量的算法来确定。然后，我们可以通过统计方法，如检查算法确定的组内的数值方差，或通过视觉检查，来识别描述数据集的最佳聚类数量。在本章中，我们将深入探讨：
- en: How to normalize data for use in a clustering algorithm and to compute similarity
    measurements for both categorical and numerical data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何对数据进行归一化以用于聚类算法并计算分类数据和数值数据的相似度测量
- en: How to use k-means clustering to identify an optimal number of clusters by examining
    the squared error function
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过检查平方误差函数来使用k均值聚类识别最佳聚类数量
- en: How to use agglomerative clustering to identify clusters at different scales
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用层次聚类来识别不同尺度的聚类
- en: Using affinity propagation to automatically identify the number of clusters
    in a dataset
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用亲和传播自动识别数据集中聚类的数量
- en: How to use spectral methods to cluster data with nonlinear relationships between
    points
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用谱方法聚类具有非线性关系的点
- en: Similarity and distance metrics
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相似性和距离度量
- en: The first step in clustering any new dataset is to decide how to compare the
    similarity (or dissimilarity) between items. Sometimes the choice is dictated
    by what kinds of similarity we are most interested in trying to measure, in others
    it is restricted by the properties of the dataset. In the following sections we
    illustrate several kinds of distance for numerical, categorical, time series,
    and set-based data—while this list is not exhaustive, it should cover many of
    the common use cases you will encounter in business analysis. We will also cover
    normalizations that may be needed for different data types prior to running clustering
    algorithms.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对任何新的数据集进行聚类的第一步是决定如何比较项目之间的相似性（或差异性）。有时选择是由我们最感兴趣的相似性类型决定的，在其他情况下，它受到数据集特性的限制。在以下章节中，我们将说明几种用于数值、分类、时间序列和基于集合的数据的距离类型——虽然这个列表不是详尽的，但它应该涵盖了你在商业分析中遇到的大多数常见用例。我们还将介绍在运行聚类算法之前可能需要的归一化方法。
- en: Numerical distance metrics
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值距离度量
- en: 'Let us begin by exploring the data in the `wine.data` file. It contains a set
    of chemical measurements describing the properties of different kinds of wines,
    and the quality level (I-III) to which the wines are assigned (Forina, M., et
    al. *PARVUS An Extendible Package for Data Exploration*. Classification and Correla
    (1988)). Open the file in an iPython notebook and look at the first few rows with
    the following commands:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从探索`wine.data`文件中的数据开始。它包含一组描述不同种类葡萄酒特性的化学测量值，以及分配给葡萄酒的质量水平（I-III）（Forina,
    M.，等. *PARVUS An Extendible Package for Data Exploration*. 分类和相关性（1988））。在iPython笔记本中打开文件，并使用以下命令查看前几行：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Numerical distance metrics](img/B04881_03_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![数值距离度量](img/B04881_03_01.jpg)'
- en: 'Notice that in this dataset we have no column descriptions, which makes the
    data hard to understand since we do not know what the features are. We need to
    parse the column names from the dataset description file `wine.names`, which in
    addition to the column names contains additional information about the dataset.With
    the following code, we generate a regular expression that will match a column
    name (using a pattern where a number followed by a parenthesis has a column name
    after it, as you can see in the list of column names in the file):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在这个数据集中我们没有列描述，这使得数据难以理解，因为我们不知道特征是什么。我们需要从数据集描述文件`wine.names`中解析列名，该文件除了列名外还包含有关数据集的附加信息。以下代码生成一个正则表达式，该表达式将匹配列名（使用一个模式，其中数字后面跟着一个括号，括号后面跟着列名，正如你在文件中列名列表中看到的那样）：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We then create an array where the first element is the class label of the wine
    (whether it belongs to quality category I-III).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个数组，其中第一个元素是葡萄酒的类别标签（它是否属于质量类别I-III）。
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Iterating through the lines in the file, we extract those that match our regular
    expression:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代文件中的行，我们提取那些与我们的正则表达式匹配的行：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then assign this list to the dataframe columns property, which contains
    the names of the columns:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将此列表分配给数据框的列属性，该属性包含列名：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have appended the column names, we can look at a summary of the
    dataset using the `df.describe()` method:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经附加了列名，我们可以使用`df.describe()`方法查看数据集的摘要：
- en: '![Numerical distance metrics](img/B04881_03_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![数值距离度量](img/B04881_03_02.jpg)'
- en: 'Having performed some cleanup on the data, how can we calculate a similarity
    measurement between wines based on the information in each row? One option would
    be to consider each of the wines as a point in a thirteen-dimensional space specified
    by its dimensions (each of the columns other than `Class`). Since the resulting
    space has thirteen dimensions, we cannot directly visualize the datapoints using
    a scatterplot to see if they are nearby, but we can calculate distances just the
    same as with a more familiar 2- or 3-dimensional space using the Euclidean distance
    formula, which is simply the length of the straight line between two points. This
    formula for this length can be used whether the points are in a 2-dimensional
    plot or a more complex space such as this example, and is given by:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据进行一些清理之后，我们如何根据每行的信息计算基于葡萄酒的相似度测量？一个选项是将每款葡萄酒视为一个由其维度（除了 `Class` 之外的所有列）指定的十三维空间中的点。由于生成的空间有十三维，我们无法直接使用散点图来可视化数据点以查看它们是否接近，但我们可以使用欧几里得距离公式来计算距离，该公式简单地是两点之间直线段的长度。这个长度的公式可以用于点在二维图或更复杂的空间（如本例所示）中，公式如下：
- en: '![Numerical distance metrics](img/B04881_03_04.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![数值距离度量](img/B04881_03_04.jpg)'
- en: Here *x* and *y* are rows of the dataset and *n* is the number of columns. One
    important aspect of the Euclidean distance formula is that columns whose scale
    is much different from others can dominate the overall result of the calculation.
    In our example, the values describing the magnesium content of each wine are `~100x`
    greater than the magnitude of features describing the alcohol content or ash percentage.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 和 *y* 是数据集的行，*n* 是列数。欧几里得距离公式的一个重要方面是，尺度与其他列差异很大的列可能会主导计算的整体结果。在我们的例子中，描述每款葡萄酒中镁含量的值是描述酒精含量或灰分百分比的特性的
    `~100x` 倍。
- en: 'If we were to calculate the distance between these datapoints, it would largely
    be determined by the magnesium concentration (as even small differences on this
    scale overwhelmingly determine the value of the distance calculation), rather
    than any of its other properties. While this might sometimes be desirable (for
    example, if the column with the largest numerical value is the one we most care
    about for judging similarity), in most applications we do not favor one feature
    over another and want to give equal weight to all columns. To get a fair distance
    comparison between these points, we need to normalize the columns so that they
    fall into the same numerical range (have similar maximal and minimal values).
    We can do so using the `scale()` function in scikit-learn and the following commands,
    which uses the array `header_names` we constructed previously to access all columns
    but the class label (the first element of the array):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要计算这些数据点之间的距离，它将主要取决于镁浓度（因为在这个尺度上的微小差异会压倒性地决定距离计算的值），而不是其任何其他属性。虽然这有时可能是可取的（例如，如果数值最大的列是我们最关心的相似性判断的列），但在大多数应用中，我们不倾向于一个特征而忽略另一个，并希望对所有列给予相同的权重。为了获得这些点之间公平的距离比较，我们需要对列进行归一化，使它们落入相同的数值范围（具有相似的极大值和极小值）。我们可以使用
    scikit-learn 中的 `scale()` 函数和以下命令来实现这一点，该命令使用我们之前构建的 `header_names` 数组来访问所有列，但不是类标签（数组的第一个元素）：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Numerical distance metrics](img/B04881_03_03.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![数值距离度量](img/B04881_03_03.jpg)'
- en: This function will subtract the mean value of a column from each element and
    then divide each point by the standard deviation of the column. This normalization
    centers the data in each column at mean 0 with variance 1, and in the case of
    normally distributed data this results in a standard normal distribution. Also,
    note that the `scale()` function returns a `numpy array`, which is why we must
    call `dataframe` on the output to use the pandas function `describe()`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将从每一列的均值中减去每个元素，然后将每个点除以该列的标准差。这种归一化方法将每个列中的数据中心化到均值为0，方差为1，对于正态分布的数据，这会导致标准正态分布。此外，请注意，`scale()`
    函数返回一个 `numpy array`，这就是为什么我们必须在输出上调用 `dataframe` 来使用 pandas 函数 `describe()`。
- en: 'Now that we have scaled the data, we can calculate Euclidean distances between
    the rows using the following commands:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对数据进行归一化，我们可以使用以下命令计算行之间的欧几里得距离：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can verify that this command produces a square matrix of dimension 178
    x 178 (the number of rows in the original dataset by the following command:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下命令验证该命令生成一个 178 x 178 的方阵（原始数据集的行数）：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We have now converted our dataset of 178 rows and 13 columns into a square matrix,
    giving the distance between each of these rows. In other words, row i, column
    j in this matrix represents the Euclidean distance between rows i and j in our
    dataset. This 'distance matrix' is the input we will use for clustering inputs
    in the following section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已将178行13列的数据集转换成一个方阵，给出了这些行之间的距离。换句话说，这个矩阵中的第i行第j列表示我们数据集中第i行和第j行之间的欧几里得距离。这个“距离矩阵”是我们将在下一节中用于聚类输入的输入。
- en: 'If want to get a sense of how the points are distributed relative to one another
    using a given distance metric, we can use **multidimensional scaling** (**MDS**)—(Borg,
    Ingwer, and Patrick JF Groenen. Modern multidimensional scaling: Theory and applications.
    Springer Science & Business Media, 2005; Kruskal, Joseph B. *Nonmetric multidimensional
    scaling: a numerical method.* Psychometrika 29.2 (1964): 115-129; Kruskal, Joseph
    B. *Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis.*
    Psychometrika 29.1 (1964): 1-27) to create a visualization. MDS attempts to find
    the set of lower dimensional coordinates (here, we will use two dimensions) that
    best represents the distances between points in the original, higher dimensions
    of a dataset (here, the pairwise Euclidean distances we calculated from the 13
    dimensions). MDS finds the optimal 2-d coordinates according to the strain function:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '如果想了解点相对于彼此的分布情况，可以使用给定的距离度量，我们可以使用**多维尺度分析**（**MDS**）——（Borg, Ingwer, 和 Patrick
    JF Groenen. 现代多维尺度分析：理论与应用. Springer Science & Business Media, 2005；Kruskal, Joseph
    B. 《非度量多维尺度分析：一种数值方法》。Psychometrika 29.2 (1964): 115-129；Kruskal, Joseph B. 《通过优化非度量假设的拟合优度进行多维尺度分析》。Psychometrika
    29.1 (1964): 1-27）来创建可视化。MDS试图找到一组低维坐标（在这里，我们将使用两个维度），以最好地表示数据集原始、更高维度的点之间的距离（在这里，是我们从13个维度计算出的成对欧几里得距离）。MDS根据应变函数找到最优的2维坐标：'
- en: '![Numerical distance metrics](img/B04881_03_09.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![数值距离度量](img/B04881_03_09.jpg)'
- en: 'Where `D` are the distances we calculated between points. The 2-d coordinates
    that minimize this function are found using **Singular Value Decomposition** (**SVD**),
    which we will discuss in more detail in [Chapter 6](ch06.html "Chapter 6. Words
    and Pixels – Working with Unstructured Data"), *Words and Pixels – Working with
    Unstructured Data*. After obtaining the coordinates from MDS, we can then plot
    the results using the `wine` class to color points in the diagram. Note that the
    coordinates themselves have no interpretation (in fact, they could change each
    time we run the algorithm due to numerical randomness in the algorithm). Rather,
    it is the relative position of points that we are interested in:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`D`代表我们计算点之间的距离。通过**奇异值分解**（**SVD**）找到最小化此函数的二维坐标，我们将在[第6章](ch06.html "第6章。文字与像素
    - 处理非结构化数据")“文字与像素 - 处理非结构化数据”中更详细地讨论它。从MDS获得坐标后，我们可以使用`wine`类来着色图中的点，从而绘制结果。请注意，坐标本身没有解释（实际上，由于算法中的数值随机性，它们每次运行算法时都可能改变）。相反，我们感兴趣的是点的相对位置：'
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Numerical distance metrics](img/B04881_03_08.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![数值距离度量](img/B04881_03_08.jpg)'
- en: Given that there are many ways we could have calculated the distance between
    datapoints, is the Euclidean distance a good choice here? Visually, based on the
    multidimensional scaling plot, we can see there is separation between the classes
    based on the features we have used to calculate distance, so conceptually it appears
    that this is a reasonable choice in this case. However, the decision also depends
    on what we are trying to compare. If we are interested in detecting wines with
    similar attributes in absolute values, then it is a good metric. However, what
    if we're not interested so much in the absolute composition of the wine, but whether
    its variables follow similar trends among wines with different alcohol contents?
    In this case, we would not be interested in the absolute difference in values,
    but rather the *correlation* between the columns. This sort of comparison is common
    for time series, which we consider next.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们可以以多种方式计算数据点之间的距离，欧几里得距离在这里是一个好的选择吗？从多维尺度图上直观来看，我们可以看到基于我们用来计算距离的特征，类别之间存在分离，因此从概念上讲，这似乎是一个合理的选项。然而，这个决定也取决于我们试图比较的内容。如果我们对检测具有相似绝对属性的葡萄酒感兴趣，那么这是一个好的度量标准。然而，如果我们对葡萄酒的绝对组成不太感兴趣，而是想知道其变量是否在不同酒精含量的葡萄酒中遵循相似的趋势，那么情况就不同了。在这种情况下，我们不会对值的绝对差异感兴趣，而是对列之间的*相关性*感兴趣。这种比较在时间序列分析中很常见，我们将在下一部分讨论。
- en: Correlation similarity metrics and time series
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关性相似度指标和时间序列
- en: 'For time series data, we are often concerned with whether the patterns between
    series exhibit the same variation over time, rather than their absolute differences
    in value. For example, if we were to compare stocks, we might want to identify
    groups of stocks whose prices move up and down in similar patterns over time.
    The absolute price is of less interest than this pattern of increase and decrease.
    Let us look at an example using the variation in prices of stocks in the **Dow
    Jones Industrial Average** (**DJIA**) over time (Brown, Michael Scott, Michael
    J. Pelosi, and Henry Dirska. *Dynamic-radius species-conserving genetic algorithm
    for the financial forecasting of Dow Jones index stocks.* Machine Learning and
    Data Mining in Pattern Recognition. Springer Berlin Heidelberg, 2013\. 27-41.).
    Start by importing the data and examining the first rows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于时间序列数据，我们通常关心的是系列之间的模式是否在时间上表现出相同的变异，而不是它们在价值上的绝对差异。例如，如果我们比较股票，我们可能希望识别出随着时间的推移价格上下波动模式相似的股票组合。与这种增减模式相比，绝对价格不太重要。让我们通过查看**道琼斯工业平均指数**（**DJIA**）股票价格随时间的变化来举例（Brown,
    Michael Scott, Michael J. Pelosi, and Henry Dirska. *Dynamic-radius species-conserving
    genetic algorithm for the financial forecasting of Dow Jones index stocks.* Machine
    Learning and Data Mining in Pattern Recognition. Springer Berlin Heidelberg, 2013\.
    27-41.）。首先，导入数据并检查前几行：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Correlation similarity metrics and time series](img/B04881_03_10.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![相关性相似度指标和时间序列](img/B04881_03_10.jpg)'
- en: This data contains the daily stock price (for 6 months) for a set of 30 stocks.
    Because all of the numerical values (the prices) are on the same scale, we won't
    normalize this data as we did with the wine dimensions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这份数据包含了30只股票的每日股价（为期6个月）。由于所有数值（价格）都在相同的尺度上，我们不会像处理葡萄酒维度那样对这份数据进行归一化。
- en: We notice two things about this data. First, the closing price per week (the
    variable we will use to calculate correlation) is presented as a string. Second,
    the date is not in the correct format for plotting. We will process both columns
    to fix this, converting these columns to a `float` and `datetime` object, respectively,
    using the following commands.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到关于这份数据的两个问题。首先，每周的收盘价（我们将用它来计算相关性）被表示为一个字符串。其次，日期的格式对于绘图来说是不正确的。我们将处理这两个列以解决这个问题，分别将这些列转换为`float`和`datetime`对象，使用以下命令。
- en: To convert the closing price to a number, we apply an anonymous function that
    takes all characters but the dollar sign and casts it as a float.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将收盘价转换为数字，我们应用一个匿名函数，该函数接受所有字符，但不包括美元符号，并将其转换为浮点数。
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To convert the date, we also use an anonymous function on each row of the date
    column, splitting the string in to year, month, and day elements and casting them
    as integers to form a tuple input for a `datetime` object:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了转换日期，我们在日期列的每一行上使用一个匿名函数，将字符串分割成年、月和日元素，并将它们转换为整数以形成一个用于`datetime`对象的元组输入：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With this transformation, we can now make a pivot table (as we covered in [Chapter
    2](ch02.html "Chapter 2. Exploratory Data Analysis and Visualization in Python"),
    *Exploratory Data Analysis and Visualization in Python*) to place the closing
    prices for each week as columns and individual stocks as rows using the following
    commands:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种转换，我们现在可以创建一个交叉表（正如我们在[第2章](ch02.html "第2章. 使用Python进行探索性数据分析和可视化")，*使用Python进行探索性数据分析和可视化*中所述），将每周的收盘价作为列，个别股票作为行，使用以下命令：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Correlation similarity metrics and time series](img/B04881_03_11.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![相关相似度指标和时间序列](img/B04881_03_11.jpg)'
- en: 'As we can see, we only need columns after 2 to calculate correlations between
    rows, as the first two columns are the index and stock ticker symbol. Let us now
    calculate the correlation between these time series of stock prices by selecting
    the second column to end columns of the data frame for each row, calculating the
    pairwise correlations distance metric, and visualizing it using MDS, as before:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们只需要计算行之间的相关性的列，从第2列开始，因为前两列是索引和股票代码。现在，让我们通过选择每行的数据框的第二列到末尾列，计算成对的相关性距离指标，并使用MDS进行可视化，就像之前一样：
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Correlation similarity metrics and time series](img/B04881_03_12.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![相关相似度指标和时间序列](img/B04881_03_12.jpg)'
- en: 'It is important to note that the Pearson coefficient, which we have calculated
    here, is a measure of linear correlation between these time series. In other words,
    it captures the linear increase (or decrease) of the trend in one price relative
    to another, but won''t necessarily capture nonlinear trends (such as a parabola
    or sigmoidal pattern). We can see this by looking at the formula for the Pearson
    correlation, which is given by:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们在这里计算的皮尔逊相关系数是衡量这些时间序列之间线性相关性的指标。换句话说，它捕捉了相对于另一个价格的趋势的线性增加（或减少），但并不一定能捕捉非线性趋势（如抛物线或S形模式）。我们可以通过查看皮尔逊相关系数的公式来看到这一点，该公式如下：
- en: '![Correlation similarity metrics and time series](img/B04881_03_17.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![相关相似度指标和时间序列](img/B04881_03_17.jpg)'
- en: 'Here μ and σ represent the mean and standard deviation of series *a* and *b*.
    This value varies from 1 (highly correlated) to -1 (inversely correlated), with
    0 representing no correlation (such as a spherical cloud of points). You might
    recognize the numerator of this equation as the **covariance**, which is a measure
    of how much two datasets, *a* and *b*, vary in synch with one another. You can
    understand this by considering that the numerator is maximized when corresponding
    points in both datasets are above or below their mean value. However, whether
    this accurately captures the similarity in the data depends upon the scale. In
    data that is distributed in regular intervals between a maximum and minimum, with
    roughly the same difference between consecutive values it captures this pattern
    well. However, consider a case in which the data is exponentially distributed,
    with orders of magnitude differences between the minimum and maximum, and the
    difference between consecutive datapoints also varying widely. Here, the Pearson
    correlation would be numerically dominated by only the largest values in the series,
    which might or might not represent the overall similarity in the data. This numerical
    sensitivity also occurs in the denominator, which represents the product of the
    standard deviations of both datasets. The value of the correlation is maximized
    when the variation in the two datasets is roughly explained by the product of
    their individual variations; there is no left-over variation between the datasets
    that is not explained by their respective standard deviations. By extracting data
    for the first two stocks in this collection and plotting their pairwise values,
    we see that this assumption of linearity appears to be a valid one for comparing
    datapoints:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 μ 和 σ 分别代表序列 *a* 和 *b* 的平均值和标准差。这个值从 1（高度相关）到 -1（反向相关），0 代表没有相关性（例如球形点云）。你可能认出这个方程的分子是
    **协方差**，它是衡量两个数据集，*a* 和 *b*，如何同步变化的度量。你可以通过考虑分子在两个数据集中的对应点都高于或低于它们的平均值时达到最大值来理解这一点。然而，这个是否准确地捕捉了数据中的相似性取决于尺度。在数据在最大值和最小值之间以相同间隔分布，并且连续值之间差异大致相同的情况下，它很好地捕捉了这种模式。然而，考虑一个数据呈指数分布的情况，最小值和最大值之间有数量级差异，连续数据点的差异也很大。在这种情况下，皮尔逊相关系数将仅由序列中的最大值在数值上主导，这可能或可能不代表数据的整体相似性。这种数值敏感性也出现在分母中，它代表两个数据集标准差的乘积。相关性的值在两个数据集的变化大致由它们各自变化的乘积解释时达到最大；没有剩余的变异在数据集中没有被它们各自的标准差解释。通过提取这个集合中前两只股票的数据并绘制它们的成对值，我们看到这个线性假设对于比较数据点似乎是有效的：
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Correlation similarity metrics and time series](img/B04881_03_13.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![相关性相似度指标和时间序列](img/B04881_03_13.jpg)'
- en: In addition to verifying that these stocks have a roughly linear correlation,
    this command introduces some new functions in pandas you may find useful. The
    first is `iloc`, which allows you to select indexed rows from a dataframe. The
    second is `transpose`, which inverts the rows and columns. Here, we select the
    first two rows, transpose, and then select all rows (prices) after the second
    (since the first is the index and the second is the *Ticker* symbol).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了验证这些股票具有大致的线性相关性之外，此命令还引入了一些在 pandas 中你可能觉得有用的新函数。第一个是 `iloc`，它允许你从数据框中选择索引行。第二个是
    `transpose`，它反转行和列。在这里，我们选择了前两行，进行了转置，然后选择了第二个之后的所有行（价格）（因为第一个是索引，第二个是 *股票代码*
    符号）。
- en: 'Despite the trend we see in this example, we could imagine there might a nonlinear
    trend between prices. In these cases, it might be better to measure not the linear
    correlation of the prices themselves, but whether the high prices for one stock
    coincide with another. In other words, the rank of market days by price should
    be the same, even if the prices are nonlinearly related. We can also calculate
    this rank correlation, also known as the Spearman''s Rho, using SciPy, with the
    following formula:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这个例子中我们看到了这种趋势，我们可能会想象价格之间可能存在非线性趋势。在这些情况下，可能更好的是测量价格本身的线性相关性，而不是一个股票的高价格是否与另一个股票的高价格一致。换句话说，按价格对市场日进行排名应该相同，即使价格是非线性相关的。我们也可以使用
    SciPy 计算这种排名相关性，也称为 Spearman's Rho，以下公式：
- en: '![Correlation similarity metrics and time series](img/B04881_03_20.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![相关性相似度指标和时间序列](img/B04881_03_20.jpg)'
- en: Note
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that this formula assumes that the ranks are distinct (no ties); in the
    case of ties, we can instead calculate the Pearson correlation using the ranks
    of the datasets instead of their raw values.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个公式假设排名是不同的（没有平局）；在出现平局的情况下，我们可以使用数据集的排名而不是原始值来计算皮尔逊相关系数。
- en: 'Where *n* is the number of datapoints in each of two sets *a* and *b*, and
    *d* is the difference in ranks between each pair of datapoints *ai* and *bi*.
    Because we only compare the ranks of the data, not their actual values, this measure
    can capture variations between two datasets, even if their numerical value vary
    over wide ranges. Let us see if plotting the results using the Spearman correlation
    metric generates any differences in the pairwise distance of the stocks computed
    from MDS, using the following commands:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n* 是两个集合 *a* 和 *b* 中每个集合的数据点数量，*d* 是每对数据点 *ai* 和 *bi* 之间排名的差异。因为我们只比较数据的排名，而不是它们的实际值，所以这个度量可以捕捉到两个数据集之间的变化，即使它们的数值在广泛的范围内变化。让我们看看使用斯皮尔曼相关系数指标绘制结果是否会在从
    MDS 计算的股票的成对距离中产生任何差异，使用以下命令：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Correlation similarity metrics and time series](img/B04881_03_14.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![相关相似度指标和时间序列](img/B04881_03_14.jpg)'
- en: The Spearman correlation distances, based on the *x* and *y* axes, appear closer
    to each other than the Pearson distances, suggesting from the perspective of rank
    correlation that the time series are more similar.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 基于坐标轴 *x* 和 *y* 的斯皮尔曼相关系数距离似乎比皮尔逊距离更接近，这表明从排名相关的角度来看，时间序列更相似。
- en: 'Though they differ in their assumptions about how two datasets are distributed
    numerically, Pearson and Spearman correlations share the requirement that the
    two sets are of the same length. This is usually a reasonable assumption, and
    will be true of most of the examples we consider in this book. However, for cases
    where we wish to compare time series of unequal lengths, we can use **Dynamic
    Time Warping** (**DTW**). Conceptually, the idea of DTW is to *warp* one time
    series to align with a second, by allowing us to open gaps in either dataset so
    that it becomes the same size as the second. What the algorithm needs to resolve
    is where the most similar points of the two series are, so that gaps can be places
    in the appropriate locations. In the simplest implementation, DTW consists of
    the following steps (see the following diagram):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们在关于两个数据集如何数值分布的假设上有所不同，皮尔逊和斯皮尔曼相关系数都要求两个集合长度相同。这通常是一个合理的假设，并且在我们考虑的大多数例子中都是正确的。然而，对于我们要比较长度不等的时间序列的情况，我们可以使用**动态时间规整**（**DTW**）。从概念上讲，DTW
    的想法是通过允许我们在任一数据集中打开间隙，使其与第二个数据集大小相同，来将一个时间序列“扭曲”以与第二个时间序列对齐。算法需要解决的是两个系列中最相似点的位置，以便可以在适当的位置放置间隙。在最简单的实现中，DTW
    由以下步骤组成（见以下图表）：
- en: '![Correlation similarity metrics and time series](img/B04881_03_15.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![相关相似度指标和时间序列](img/B04881_03_15.jpg)'
- en: For dataset *a* of length *n* and a dataset *b* of length *m*, construct a matrix
    of size *n* by *m*.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于长度为 *n* 的数据集 *a* 和长度为 *m* 的数据集 *b*，构建一个大小为 *n* 乘 *m* 的矩阵。
- en: Set the top row and the leftmost column of this matrix to both be infinity (left,
    in figure above).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个矩阵的顶部行和最左边的列都设置为无穷大（如图所示）。
- en: For each point *i* in set *a*, and point *j* in set *b*, compare their similarity
    using a cost function. To this cost function, add the minimum of the element (*i-1,
    j-1*), (*i-1, j*), and (*j-1, i*)—that is, from moving up and left, left, or up
    in the matrix). These conceptually represent the costs of opening a gap in one
    of the series, versus aligning the same element in both (middle, above).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于集合 *a* 中的每个点 *i* 和集合 *b* 中的每个点 *j*，使用成本函数比较它们的相似性。将这个成本函数与元素 (*i-1, j-1*)、(*i-1,
    j*) 和 (*j-1, i*) 中的最小值相加——即从矩阵中向上和向左移动、向左或向上移动）。这些在概念上代表了在其中一个系列中打开间隙的成本，与在两个系列中对齐相同元素的成本（如图中上方中间所示）。
- en: At the end of step 2, we will have traced the minimum cost path to align the
    two series, and the DTW distance will be represented by the bottommost corner
    of the matrix, (*n.m*) (right, above).
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在步骤 2 结束时，我们将追踪到对齐两个系列的最小成本路径，DTW 距离将由矩阵的底部角落 (*n.m*) 表示（如图所示）。
- en: 'A negative aspect of this algorithm is that step 3 involves computing a value
    for every element of series *a* and *b*. For large time series or large datasets,
    this can be computationally prohibitive. While a full discussion of algorithmic
    improvements is beyond the scope of our present examples, we refer interested
    readers to FastDTW (which we will use in our example) and SparseDTW as examples
    of improvements that can be evaluated using many fewer calculations (Al-Naymat,
    Ghazi, Sanjay Chawla, and Javid Taheri. *Sparsedtw: A novel approach to speed
    up dynamic time warping*. Proceedings of the Eighth Australasian Data Mining Conference-Volume
    101\. Australian Computer Society, Inc., 2009\. Salvador, Stan, and Philip Chan.
    *Toward accurate dynamic time warping in linear time and space*. Intelligent Data
    Analysis 11.5 (2007): 561-580.).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '此算法的一个负面方面是第3步涉及到为序列*a*和*b*的每个元素计算一个值。对于大型时间序列或大型数据集，这可能计算上难以承受。虽然关于算法改进的全面讨论超出了我们当前示例的范围，但我们建议感兴趣的读者参考FastDTW（我们将在示例中使用）和SparseDTW作为改进的例子，这些改进可以通过更少的计算进行评估（Al-Naymat,
    Ghazi, Sanjay Chawla, 和 Javid Taheri. *Sparsedtw: 一种加快动态时间规整的新方法*。第八届澳大利亚和新西兰数据挖掘会议-第101卷。澳大利亚计算机协会，2009年。Salvador,
    Stan, 和 Philip Chan. *向线性时间和空间中的准确动态时间规整迈进*。智能数据分析11.5（2007年）：561-580.）。'
- en: 'We can use the FastDTW algorithm to compare the stocks data and plot the results
    again using MDS. First we will compare pairwise each pair of stocks and record
    their DTW distance in a matrix:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用FastDTW算法来比较股票数据，并再次使用MDS绘制结果。首先，我们将成对比较每一对股票，并将它们的DTW距离记录在一个矩阵中：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This function is found in the fastdtw library, which you can install using pip
    or `easy_install`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能位于fastdtw库中，您可以使用pip或`easy_install`进行安装。
- en: For computational efficiency (because the distance between `i` and `j` equals
    the distance between stocks `j` and `i`), we calculate only the upper triangle
    of this matrix. We then add the transpose (for example, the lower triangle) to
    this result to get the full distance matrix.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算效率（因为`i`和`j`之间的距离等于股票`j`和`i`之间的距离），我们只计算这个矩阵的上三角。然后我们将转置（例如，下三角）添加到这个结果中，以获得完整的距离矩阵。
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we can use MDS again to plot the results:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以再次使用MDS来绘制结果：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Correlation similarity metrics and time series](img/B04881_03_16.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![相关相似度指标和时间序列](img/B04881_03_16.jpg)'
- en: Compared to the distribution of coordinates along the *x* and *y* axis for Pearson
    correlation and rank correlation, the DTW distances appear to span a wider range,
    picking up more nuanced differences between the time series of stock prices.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与Pearson相关和秩相关的坐标分布相比，DTW距离似乎跨越了更广泛的范围，捕捉到股票价格时间序列之间更细微的差异。
- en: Now that we have looked at numerical and time series data, as a last example
    let us examine calculating similarity measurements for categorical datasets.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经研究了数值和时间序列数据，作为一个最后的例子，让我们来考察计算分类数据集的相似度测量。
- en: Similarity metrics for categorical data
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类数据的相似度指标
- en: 'Text represents one class of categorical data: for instance, we might have
    use a vector to represent the presence or absence of a given keyword for a set
    of papers submitted to an academic conference, as in our example dataset (Moran,
    Kelly H., Byron C. Wallace, and Carla E. Brodley. *Discovering Better AAAI Keywords
    via Clustering with Community-sourced Constraints*. Twenty-Eighth AAAI Conference
    on Artificial Intelligence. 2014.). If we open the data, we see that the keywords
    are represented as a string in one column, which we will need to convert into
    a binary vector:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 文本代表一类分类数据：例如，我们可能使用一个向量来表示提交给学术会议的一组论文中给定关键词的存在或不存在，正如我们的示例数据集（Moran, Kelly
    H., Byron C. Wallace, 和 Carla E. Brodley. *通过社区来源的约束进行聚类以发现更好的AAAI关键词*。第二十八届AAAI人工智能会议。2014年。）所示。如果我们打开数据，我们会看到关键词被表示为一列中的字符串，我们需要将其转换为二进制向量：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Similarity metrics for categorical data](img/B04881_03_18.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![分类数据的相似度指标](img/B04881_03_18.jpg)'
- en: 'While in [Chapter 6](ch06.html "Chapter 6. Words and Pixels – Working with
    Unstructured Data"), *Words and Pixels – Working with Unstructured Data*, we will
    examine special functions to do this conversion from text to vector, for illustrative
    purposes we will now code the solution ourselves. We need to gather all the unique
    keywords, and assign a unique index to each of them to generate a new column name
    `''keword_n`'' for each keyword:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然[第6章](ch06.html "第6章。文字和像素 – 处理非结构化数据")，“文字和像素 – 处理非结构化数据”，我们将检查特殊函数来完成从文本到向量的转换，为了说明目的，我们现在将亲自编写代码。我们需要收集所有独特的关键词，并为每个关键词分配一个唯一的索引，为每个关键词生成一个新的列名`'keyword_n'`：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We then generate a new set of columns using this keyword to column name mapping,
    to set a 1 in each row where the keyword appears in that article''s keywords:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用这个关键字到列名的映射生成一组新的列，在每个行中，如果该文章的关键词中包含该关键字，则在该行设置1：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'These columns will be appended to the right of the existing columns and we
    can select out these binary indicators using the `iloc` command, as before:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些列将被附加到现有列的右侧，我们可以使用`iloc`命令选择这些二进制指示符，就像之前一样：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Similarity metrics for categorical data](img/B04881_03_19.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![分类数据的相似性度量](img/B04881_03_19.jpg)'
- en: In this case, a Euclidean distance between articles could be computed, but because
    each coordinate is either 0 or 1, it does not provide the continuous distribution
    of distances we would like (we will get many ties, since there are only so many
    ways to add and subtract 1 and 0). Similarly, measurements of correlation between
    these binary vectors are less than ideal because the values can only be identical
    or non-identical, again leading to many duplicate correlation values.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可以计算文章之间的欧几里得距离，但由于每个坐标只能是0或1，它并不能提供我们想要的连续距离分布（由于只有有限种加法和减法1和0的方式，我们将会得到很多重复值）。
- en: 'What kinds of similarity metric could we use instead? One is the Jaccard coefficient:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用哪些类型的相似性度量来代替？一个是Jaccard系数：
- en: '![Similarity metrics for categorical data](img/B04881_03_32.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![分类数据的相似性度量](img/B04881_03_32.jpg)'
- en: 'This is the number of intersecting items (positions where both *a* and *b*
    are set to *1* in our example) divided by the union (the total number of positions
    where either *a* or *b* are set to 1).This measure could be biased, however, if
    the articles have very different numbers of keywords, as a larger set of words
    will have a greater probability of being similar to another article. If we are
    concerned about such bias, we could use the cosine similarity, which measure the
    angle between vectors and is sensitive to the number of elements in each:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相交项的数量（在我们的例子中，*a*和*b*都设置为*1*的位置）除以并集（*a*或*b*中任一设置为1的总位置数）。然而，如果文章的关键词数量差异很大，这个度量可能会存在偏差，因为更大的词集有更大的概率与另一篇文章相似。如果我们担心这种偏差，可以使用余弦相似度，它测量向量之间的角度，并且对每个向量的元素数量敏感：
- en: '![Similarity metrics for categorical data](img/B04881_03_34.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![分类数据的相似性度量](img/B04881_03_34.jpg)'
- en: 'Where:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![Similarity metrics for categorical data](img/B04881_03_35.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![分类数据的相似性度量](img/B04881_03_35.jpg)'
- en: 'We could also use the Hamming distance (Hamming, Richard W. *Error detecting
    and error correcting codes*. Bell System technical journal 29.2 (1950): 147-160.),
    which simply sums whether the elements of two sets are identical or not:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还可以使用汉明距离（Hamming, Richard W. *Error detecting and error correcting codes*.
    Bell System technical journal 29.2 (1950): 147-160.），它简单地计算两个集合的元素是否相同：'
- en: '![Similarity metrics for categorical data](img/B04881_03_37.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![分类数据的相似性度量](img/B04881_03_37.jpg)'
- en: 'Clearly, this measure will be best if we are primarily looking for matches
    and mismatches. It is also, like the Jaccard coefficient, sensitive to the number
    of items in each set, as simply increasing the number of elements increases the
    possible upper bound of the distance. Similar to Hamming is the Manhattan distance,
    which does not require the data to be binary:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，如果我们主要寻找匹配和不匹配，这个度量将是最优的。它也像Jaccard系数一样，对每个集合中的项目数量敏感，因为简单地增加元素数量会增加距离的上限。与汉明距离相似的是曼哈顿距离，它不需要数据是二进制的：
- en: '![Similarity metrics for categorical data](img/B04881_03_38.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![分类数据的相似性度量](img/B04881_03_38.jpg)'
- en: 'If we use the Manhattan distance as an example, we can use MDS again to plot
    the arrangement of the documents in keyword space using the following commands:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以曼哈顿距离为例，我们可以再次使用MDS来绘制以下命令中关键词空间中文档的排列：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Similarity metrics for categorical data](img/B04881_03_23.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![分类数据的相似性度量](img/B04881_03_23.jpg)'
- en: We see a number of groups of papers, suggesting that a simple comparison of
    common keywords provides a way to distinguish between articles.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到许多论文组，它们提出通过简单比较常见关键词可以提供一种区分文章的方法。
- en: The diagram below provides a summary of the different distance methods we have
    discussed and the decision process for choosing one over another for a particular
    problem. While it is not exhaustive, we hope it provides a starting point for
    your own clustering applications.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表总结了我们讨论的不同距离方法，以及在选择特定问题时选择一种方法而不是另一种方法的决策过程。虽然它并不详尽，但我们希望它能为你自己的聚类应用提供一个起点。
- en: '![Similarity metrics for categorical data](img/B04881_03_24.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![分类数据的相似性度量](img/B04881_03_24.jpg)'
- en: Tip
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**Aside: Normalizing categorical data**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**旁白：归一化分类数据**'
- en: As you may have noted, we don't normalize categorical data in the same way that
    we used the `scale()` function for numerical data. The reason for this is twofold.
    First, with categorical data we are usually dealing with data in the range `[0,1]`,
    so the problem of one column of the dataset containing wildly larger values that
    overwhelm the distance metric is minimized. Secondly, the notion of the scale()
    function is that the data in the column is biased, and we are removing that bias
    by subtracting the mean. For categorical data, the 'mean' has a less clear interpretation,
    as the data can only take the value of 0 or 1, while the mean might be somewhere
    between the two (for example, 0.25). Subtracting this value doesn't make sense
    as it would make the data elements no longer binary indicators.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，我们不会像使用`scale()`函数处理数值数据那样对分类数据进行归一化。原因有两点。首先，对于分类数据，我们通常处理的是范围在`[0,1]`之间的数据，因此数据集中某一列包含的极端较大值压倒距离度量的问题最小化了。其次，`scale()`函数的概念是数据列中的数据是有偏的，我们通过减去均值来消除这种偏差。对于分类数据，'均值'的含义不太明确，因为数据只能取0或1的值，而均值可能在两者之间（例如，0.25）。减去这个值没有意义，因为它会使数据元素不再是二进制指示器。
- en: '**Aside: Blending distance metrics**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**旁白：混合距离度量**'
- en: In the examples considered so far in this chapter, we have dealt with data that
    may be described as either numerical, time series, or categorical. However, we
    might easily find examples where this is not true. For instance, we could have
    a dataset of stock values over time that also contains categorical information
    about which industry the stock belongs to and numerical information about the
    size and revenue of the company. In this case, it would be difficult to choose
    a single distance metric that adequately handles all of these features. Instead,
    we could calculate a different distance metric for each of the three sets of features
    (time-series, categorical, and numerical) and blend them (by taking their average,
    sum, or product, for instance). Since these distances might cover very different
    numerical scales, we might need to normalize them, for instance using the `scale()`
    function discussed above to convert each distance metric into a distribution with
    mean 0, standard deviation 1 before combining them.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章迄今为止考虑的例子中，我们处理的数据可能被描述为数值型、时间序列型或分类型。然而，我们很容易找到这种情况并不成立的例子。例如，我们可能有一个包含随时间变化的股票价值数据集，其中还包含有关股票所属行业和公司规模及收入的分类信息。在这种情况下，选择一个能够充分处理所有这些特征的单一距离度量将变得困难。相反，我们可以为这三组特征（时间序列、分类和数值）中的每一组计算不同的距离度量，并将它们混合（例如，通过取平均值、总和或乘积）。由于这些距离可能覆盖非常不同的数值范围，我们可能需要对这些距离进行归一化，例如使用上面讨论的`scale()`函数将每个距离度量转换为具有均值0、标准差1的分布，然后再将它们组合起来。
- en: Now that we have some ways to compare the similarity of items in a dataset,
    let us start implementing some clustering pipelines.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一些比较数据集中项目相似性的方法，让我们开始实现一些聚类流程。
- en: K-means clustering
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means聚类
- en: 'K-means clustering is the classical divisive clustering algorithm. The idea
    is relatively simple: the **k** in the title comes from the number of clusters
    we wish to identify, which we need to decide before running the algorithm, while
    **means** denotes the fact that the algorithm attempts to assign datapoints to
    clusters where they are closest to the average value of the cluster. Thus a given
    datapoint chooses among k different means in order to be assigned to the most
    appropriate cluster. The basic steps of the simplest version of this algorithm
    are (MacKay, David. *An example inference task: clustering*. Information theory,
    inference and learning algorithms (2003): 284-292):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类是经典的划分聚类算法。其思想相对简单：标题中的**k**代表我们希望识别的簇的数量，在运行算法之前我们需要决定这个数量，而**means**表示算法试图将数据点分配到它们最接近簇平均值的簇中。因此，一个给定的数据点会在k个不同的均值中选择，以便被分配到最合适的簇中。该算法最简单版本的基本步骤如下（MacKay,
    David. *一个推理任务示例：聚类*. 信息理论、推理和学习算法（2003年）：284-292）：
- en: Choose a desired number of groups *k*.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个期望的组数 *k*。
- en: Assign k cluster centers; these could simply be k random points from the dataset,
    which is known as the Forgy method ( Hamerly, Greg, and Charles Elkan. *Alternatives
    to the k-means algorithm that find better clusterings*. Proceedings of the eleventh
    international conference on Information and knowledge management. ACM, 2002.).
    Alternatively, we could assign a random cluster to each datapoint, and compute
    the average of the datapoints assigned to the same cluster as the k centers, a
    method called Random Partitioning (Hamerly, Greg, and Charles Elkan. *Alternatives
    to the k-means algorithm that find better clusterings*. Proceedings of the eleventh
    international conference on Information and knowledge management. ACM, 2002).
    More sophisticated methods are also possible, as we will see shortly.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分配k个簇中心；这些可以简单地是数据集中的k个随机点，这被称为Forgy方法（Hamerly, Greg, 和 Charles Elkan. *替代k-means算法以找到更好的聚类*.
    第十一届国际信息与知识管理会议论文集. ACM, 2002年）。或者，我们可以将一个随机簇分配给每个数据点，并计算分配给同一簇的k个中心的平均数据点，这种方法称为随机划分（Hamerly,
    Greg, 和 Charles Elkan. *替代k-means算法以找到更好的聚类*. 第十一届国际信息与知识管理会议论文集. ACM, 2002年）。也可能有更复杂的方法，我们很快就会看到。
- en: Assign any remaining datapoints to the nearest cluster, based on some similarity
    measure (such as the squared Euclidean distance).
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据某些相似度度量（例如平方欧几里得距离），将任何剩余的数据点分配到最近的簇中。
- en: Recalculate the center of each of the *k* groups by averaging the points assigned
    to them. Note that at this point the center may no longer represent the location
    of a single datapoint, but is the weighted center of mass of all points in the
    group.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算分配给每个 *k* 个组的点的平均值来重新计算每个组的中心。请注意，在此点，中心可能不再代表单个数据点的位置，而是该组中所有点的加权质心。
- en: Repeat 3 and 4 until no points change cluster assignment or the maximum number
    of iterations is reached.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复3和4，直到没有点改变簇分配或达到最大迭代次数。
- en: Tip
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**K-means ++**'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**K-means++**'
- en: 'In the initialization of the algorithm in step 2 above, there are two potential
    problems. If we simple choose random points as cluster centers, they may not be
    optimally distributed within the data (particularly if the size of the clusters
    is unequal). The k points may not actually end up in the k clusters in the data
    (for example, multiple random points may reside within the largest cluster in
    the dataset, as in the figure below, top middle panel), which means the algorithm
    may not converge to the ''correct'' solution or may take a long time to do so.
    Similarly, the Random Partitioning method will tend to place all the centers near
    the greatest mass of datapoints (see figure below, top right panel), as any random
    set of points will be dominated by the largest cluster. To improve the initial
    choice of parameters, we can use instead the k++ initialization proposed in 2007
    (Arthur, David, and Sergei Vassilvitskii. "k-means++: The advantages of careful
    seeding." Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
    algorithms. Society for Industrial and Applied Mathematics, 2007.). In this algorithm,
    we choose an initial datapoint at random to be the center of the first cluster.
    We then calculate the squared distance from every other datapoint to the chosen
    datapoint, and choose the next cluster center with probability proportional to
    this distance. Subsequently, we choose the remaining clusters by calculating this
    squared distance to the previously selected centers for a given datapoint. Thus,
    this initialization will choose points with higher probability that are far from
    any previously chosen point, and spread the initial centers more evenly in the
    space. This algorithm is the default used in scikit-learn.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在上述步骤2中算法的初始化中，存在两个潜在问题。如果我们简单地选择随机点作为聚类中心，它们可能不会在数据中优化分布（尤其是如果聚类的大小不等）。k个点可能实际上不会最终落在数据中的k个聚类中（例如，如图下顶部中间面板所示，多个随机点可能位于数据集最大的聚类中），这意味着算法可能不会收敛到“正确”的解，或者可能需要很长时间才能做到。同样，随机划分方法倾向于将所有中心放置在数据点最大质量附近（见下图中顶部右侧面板），因为任何随机点集都将被最大的聚类主导。为了改进参数的初始选择，我们可以使用2007年提出的k++初始化（Arthur,
    David, 和 Sergei Vassilvitskii. "k-means++: The advantages of careful seeding."
    Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms.
    Society for Industrial and Applied Mathematics, 2007.）。在这个算法中，我们随机选择一个初始数据点作为第一个聚类的中心。然后我们计算每个其他数据点到所选数据点的平方距离，并选择下一个聚类中心，其概率与这个距离成比例。随后，我们通过计算给定数据点到先前选定的中心的平方距离来选择剩余的聚类。因此，这种初始化将以更高的概率选择远离任何先前选择点的点，并在空间中更均匀地分布初始中心。这个算法是scikit-learn中默认使用的算法。'
- en: '![K-means clustering](img/B04881_03_25.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![K-means聚类](img/B04881_03_25.jpg)'
- en: 'Kmeans++ clustering. (Top, left): Example data with three clusters of unequal
    size. (Top, middle). Random choice of cluster centers is biased towards points
    in the largest underlying cluster. (Top, right): Random partitioning results in
    center of mass for all three random clusters near the bottom of the plot. (Bottom
    panels). Kmeans++ results in sequential selection of three cluster centers that
    are evenly spaced across the dataset.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Kmeans++聚类。 (顶部，左侧)：具有三个大小不等聚类的示例数据。 (顶部，中间)：随机选择聚类中心倾向于最大的基础聚类中的点。 (顶部，右侧)：随机划分导致所有三个随机聚类的质心接近图表底部。
    (底部面板)：Kmeans++导致在数据集中均匀选择三个聚类中心。
- en: 'Let''s think for a moment about why this works; even if we start with random
    group centers, once we assign points to these groups the centers are pulled towards
    the average position of observations in our dataset. The updated center is nearer
    to this average value. After many iterations, the center of each group will be
    dominated by the average position of datapoints near the randomly chosen starting
    point. If the center was poorly chosen to begin with, it will be dragged towards
    this average, while datapoints that were perhaps incorrectly assigned to this
    group will gradually be reassigned. During this process, the overall value that
    is minimized is typically the sum of squared errors (when we are using the Euclidean
    distance metric), given by:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下为什么这有效；即使我们以随机分组中心开始，一旦我们将点分配到这些组，中心就会被拉向我们的数据集中观察的平均位置。更新后的中心更接近这个平均值。经过多次迭代后，每个组的中心将由随机选择起始点附近的平均数据点的位置主导。如果起始点选择得不好，它将被拉向这个平均值，而可能被错误分配到这个组的点将逐渐被重新分配。在这个过程中，通常最小化的整体值通常是平方误差之和（当我们使用欧几里得距离度量时），由以下公式给出：
- en: '![K-means clustering](img/B04881_03_44.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![K-means聚类](img/B04881_03_44.jpg)'
- en: Where D is the Euclidean distance and c is the cluster center for the cluster
    to which a point is assigned. This value is also sometimes referred to as the
    inertia. If we think about this for a moment, we can see that this has the effect
    that the algorithm works best for data that is composed of circles (or spheres
    in higher dimensions); the overall SSE for a cluster is minimized when points
    are uniformly distant from it in a spherical cloud. In contrast, a non-uniform
    shape (such as an ellipse) will tend to have higher SSE values, and the algorithm
    will be optimized by splitting the data into two clusters, even if visually we
    can see that they appear to be represented well by one. This fact reinforces why
    normalization is often beneficial (as the 0 mean, 1 standard deviation normalization
    attempts to approximate the shape of a normal distribution for all dimensions,
    thus forming circles of spheres of data), and the important role of data visualization
    in addition to numerical statistics in judging the quality of clusters.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中D是欧几里得距离，c是分配给点的簇的中心。这个值有时也被称为惯性。如果我们稍微思考一下，我们可以看到这会产生这样的效果：该算法最适合由圆形（或更高维度的球体）组成的数据；当点在球形云中均匀地远离簇时，簇的整体SSE最小化。相比之下，非均匀形状（如椭圆）往往会具有更高的SSE值，算法将通过将数据分成两个簇来优化，即使从视觉上看，它们似乎可以用一个簇很好地表示。这一事实强化了为什么标准化通常是有益的（因为0均值，1标准差标准化试图近似所有维度的正态分布形状，从而形成数据圆或球体），以及在判断簇的质量时，数据可视化在除了数值统计之外的重要作用。
- en: It is also important to consider the implications of this minimization criteria
    for step 3\. The SSE is equivalent to the summed squared Euclidean distance between
    cluster points and their centroid. Thus, using the squared Euclidean distance
    as the metric for comparison, we guarantee that the cluster assignment is also
    optimizing the minimization criteria. We could use other distance metrics, but
    then this will not be guaranteed. If we are using Manhattan or Hamming distance,
    we could instead make our minimization criteria the sum of distances to the cluster
    center, which we term the k-median, since the value that optimizes this statistic
    is the cluster median (Jain, Anil K., and Richard C. Dubes. Algorithms for clustering
    data. Prentice-Hall, Inc., 1988.). Alternatively, we could use an arbitrary distance
    metric with an algorithm such as k-medoids (see below).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一最小化标准对步骤3的影响也很重要。SSE等于簇点与其质心之间的欧几里得距离的平方和。因此，使用平方欧几里得距离作为比较的度量，我们保证了簇分配也在优化最小化标准。我们可以使用其他距离度量，但这样就不能保证这一点。如果我们使用曼哈顿或汉明距离，我们可以将最小化标准改为到簇中心的距离之和，我们称之为k-中位数，因为优化这个统计量的值是簇的中位数（Jain,
    Anil K. 和 Richard C. Dubes. 数据聚类算法。Prentice-Hall, Inc.，1988年）。或者，我们可以使用任意距离度量，例如k-medoids算法（见下文）。
- en: Clearly, this method will be sensitive to our initial choice of group centers,
    so we will usually run the algorithm many times and use the best result.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种方法将对我们初始选择的组中心的选择敏感，因此我们通常会多次运行算法，并使用最佳结果。
- en: 'Let''s look at an example: type the following commands in the notebook to read
    in a sample dataset.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子：在笔记本中输入以下命令以读取样本数据集。
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![K-means clustering](img/B04881_03_26.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![K-means聚类](img/B04881_03_26.jpg)'
- en: By visual inspection, this dataset clearly has a number of clusters in it. Let's
    try clustering with *k=5*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过视觉检查，这个数据集明显包含多个簇。让我们尝试使用*k=5*进行聚类。
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![K-means clustering](img/B04881_03_27.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![K-means聚类](img/B04881_03_27.jpg)'
- en: 'You will notice that we use the slice operators ''`[]`'' to index a `numpy`
    array that we create from the input dataframe, and select all rows and the columns
    after the first (the first contains the label, so we don''t need it as it isn''t
    part of the data being used for clustering). We call the KMeans model using a
    pattern that will become familiar for many algorithms in scikit-learn and PySpark:
    we create model object (KMeans) with parameters (here, 5, which is the number
    of clusters), and call ''fit_predict'' to both calibrate the model parameters
    and apply the model to the input data. Here, applying the model generates cluster
    centers, while in regression or classification models that we will discuss in
    [Chapters 4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression
    Methods"), *Connecting the Dots with Models – Regression Methods* and [Chapter
    5](ch05.html "Chapter 5. Putting Data in its Place – Classification Methods and
    Analysis"), *Putting Data in its Place – Classification Methods and Analysis*,
    ''predict'' will yield an estimated continuous response or class label, respectively,
    for each data point. We could also simply call the `fit` method for KMeans, which
    would simply return an object describing the cluster centers and the statistics
    resulting from fitting the model, such as the inertia measure we describe above.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们使用切片操作符 '`[]`' 来索引从输入数据框创建的 `numpy` 数组，并选择所有行以及第一个之后的列（第一个包含标签，因此我们不需要它，因为它不是用于聚类的数据的一部分）。我们使用在
    scikit-learn 和 PySpark 中许多算法都会熟悉的模式来调用 KMeans 模型：我们用参数（在这里是 5，这是簇的数量）创建模型对象（KMeans），并调用
    'fit_predict' 来校准模型参数并将模型应用于输入数据。在这里，应用模型会生成簇中心，而在我们将在 [第 4 章](ch04.html "第 4
    章。通过模型连接点 – 回归方法")、*通过模型连接点 – 回归方法* 和 [第 5 章](ch05.html "第 5 章。将数据放在合适的位置 – 分类方法和分析")、*将数据放在合适的位置
    – 分类方法和分析* 中讨论的回归或分类模型中，'predict' 将分别对每个数据点产生估计的连续响应或类标签。我们也可以简单地调用 KMeans 的 `fit`
    方法，这将简单地返回一个描述簇中心和拟合模型产生的统计信息的对象，例如我们上面描述的惯性度量。
- en: Is this a good number of clusters to fit to the data or not? We can explore
    this question if we cluster using several values of `k` and plot the inertia at
    each. In Python we can use the following commands.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个簇的数量适合数据吗？我们可以通过使用几个 `k` 的值进行聚类并绘制每个的惯性来探索这个问题。在 Python 中，我们可以使用以下命令。
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Recall that the inertia is defined as the sum of squared distance points in
    a cluster to the center of the cluster to which they are assigned, which is the
    objective we are trying to optimize in k-means. By visualizing this inertia value
    at each cluster number *k*, we can get a feeling for the number of clusters that
    best fits the data:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，惯性被定义为簇中点到其分配的簇中心的平方距离之和，这是我们试图在 k-means 中优化的目标。通过在每个簇编号 *k* 处可视化这个惯性值，我们可以对最适合数据的簇数量有一个感觉：
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![K-means clustering](img/B04881_03_28.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![K-means 聚类](img/B04881_03_28.jpg)'
- en: We notice there is an *elbow* around the five-cluster mark, which fortunately
    was the value we initially selected. This elbow indicates that after five clusters
    we do not significantly decrease the inertia as we add more clusters, suggesting
    that at k=5 we have captured the important group structure in the data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到在五个簇的标记处有一个 *肘部*，幸运的是，这是我们最初选择的值。这个肘部表明，在五个簇之后，我们增加更多簇时惯性并没有显著减少，这表明在 k=5
    时我们已经捕捉到了数据中的重要组结构。
- en: 'This exercise also illustrates some problems: as you can see from the plot,
    some of our clusters might be formed from what appear to be overlapping segments,
    forming a cross shape. Is this a single cluster or two mixed clusters? Unfortunately,
    without a specification in our cluster model of what shape the clusters should
    conform to, the results are driven entirely by distance metrics, not pattern which
    you might notice yourself visually. This underscores the importance of visualizing
    the results and examining them with domain experts to judge whether the obtained
    clusters makes sense. In the absence of a domain expert, we could also see whether
    the obtained clusters contain all points labeled with a known assignment—if a
    high percentage of the clusters are enriched for a single label, this indicates
    the clusters are of good conceptual quality as well as minimizing our distance
    metric.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习也说明了某些问题：正如您从图中可以看到的，我们的某些聚类可能是由看似重叠的段形成的，形成一个十字形状。这是一个单独的聚类还是两个混合的聚类？遗憾的是，如果没有在我们的聚类模型中指定聚类应遵循的形状，结果将完全由距离度量驱动，而不是您可能通过视觉注意到的模式。这强调了可视化结果并使用领域专家检查它们以判断获得的聚类是否有意义的重要性。在没有领域专家的情况下，我们还可以查看获得的聚类是否包含所有用已知分配标记的点——如果一个高比例的聚类富含单一标签，这表明聚类不仅概念质量良好，而且最小化了我们的距离度量。
- en: We could also try using a method that will automatically calculate the best
    number of clusters for a dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试使用一种方法来自动计算数据集的最佳聚类数量。
- en: Affinity propagation – automatically choosing cluster numbers
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亲和传播 – 自动选择聚类数量
- en: 'One of the weaknesses of the k-means algorithm is that we need to define upfront
    the number of clusters we expect to find in the data. When we are not sure what
    an appropriate choice is, we may need to run many iterations to find a reasonable
    value. In contrast, the Affinity Propagation algorithm (Frey, Brendan J., and
    Delbert Dueck. *Clustering by passing messages between data points*. science 315.5814
    (2007): 972-976.) finds the number of clusters automatically from a dataset. The
    algorithm takes a similarity matrix as input (S) (which might be the inverse Euclidean
    distance, for example – thus, closer points have larger values in S), and performs
    the following steps after initializing a matrix of *responsibilit*y and *availability*
    values with all zeroes. It calculates the *responsibility* for one datapoint *k*
    to be the cluster center for another datapoint *i*. This is represented numerically
    by the similarity between the two datapoints. Since all availabilities begin at
    zero, in the first round we simply subtract the highest similarity to any other
    point (k'') for *i*. Thus, a high responsibility score occurs when point k is
    much more similar to *i* than any other point.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 'k-means算法的一个弱点是我们需要事先定义预期在数据中找到的聚类数量。当我们不确定合适的选项时，我们可能需要运行多次迭代来找到一个合理的值。相比之下，亲和传播算法（Frey,
    Brendan J. 和 Delbert Dueck. *通过数据点间传递消息进行聚类*. science 315.5814 (2007): 972-976）能够从数据集中自动找到聚类数量。该算法以相似性矩阵（S）作为输入（例如，可能是欧几里得距离的逆矩阵——因此，在S中，更接近的点具有更大的值），并在初始化一个包含所有零值的责任和可用性矩阵后执行以下步骤。它计算一个数据点k作为另一个数据点i的聚类中心的责任。这通过两个数据点之间的相似性数值表示。由于所有可用性都从零开始，在第一轮中，我们只需从i中减去任何其他点（k''）的最高相似性。因此，当点k比任何其他点与i更相似时，就会产生一个高的责任分数。'
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_52.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![亲和传播 – 自动选择聚类数量](img/B04881_03_52.jpg)'
- en: 'Where *i* is the point for which we are trying to find the cluster center,
    *k* is a potential cluster center to which point *i* might be assigned, s is their
    similarity, and a is the ''availability'' described below. In the next step, the
    algorithm calculates the availability of the datapoint *k* as a cluster center
    for point *i*, which represents how appropriate k is as a cluster center for *i*
    by judging if it is the center for other points as well. Points which are chosen
    with high responsibility by many other points have high availability, as per the
    formula:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*i*是我们试图找到的聚类中心的数据点，*k*是可能被分配给数据点*i*的潜在聚类中心，s是它们的相似性，a是下面描述的'可用性'。在下一步中，算法计算数据点*k*作为数据点*i*的聚类中心时的可用性，这表示k作为*i*的聚类中心是否合适，通过判断它是否也是其他点的中心。被许多其他点选择为具有高责任度的点具有高可用性，如公式所示：
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_53.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![亲和传播 – 自动选择簇数量](img/B04881_03_53.jpg)'
- en: 'Where *r* is the responsibility given above. If *i=k*, then this formula is:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*r*是上面给出的责任。如果*i=k*，则此公式为：
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_54.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![亲和传播 – 自动选择簇数量](img/B04881_03_54.jpg)'
- en: These steps are sometimes described as **message passing**, as they represent
    information being *exchanged* by the two datapoints about the relative probability
    of one being a cluster center for another. Looking at steps 1 and 2, you can see
    that as the algorithm proceeds the responsibility will drop for many of the datapoints
    to a negative number (as we subtract not only the highest similarity of other
    datapoints, but also the availability score of these points, leaving only a few
    positives that will determine the cluster centers. At the end of the algorithm
    (once the responsibilities and availabilities are no longer changing by an appreciable
    numerical amount), each datapoint points at another as a cluster center, meaning
    the number of clusters is automatically determined from the data. This method
    has the advantage that we don't need to know the number of clusters in advance,
    but does not scale as well as other methods, since in the simplest implantation
    we need an n-by-n similarity matrix as the input. If we fit this algorithm on
    our dataset from before, we see that it detects far more clusters than our elbow
    plot suggests, since running the following commands gives a cluster number of
    309.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤有时被称为**消息传递**，因为它们代表了两个数据点之间关于一个数据点作为另一个数据点簇中心的相对概率的信息交换。查看步骤1和2，你可以看到随着算法的进行，许多数据点的责任将降低到负数（因为我们不仅减去了其他数据点的最高相似度，还减去了这些点的可用性得分，只留下少数几个正数来确定簇中心。在算法结束时（一旦责任和可用性不再以可观的数值变化），每个数据点都将指向另一个数据点作为簇中心，这意味着簇的数量会自动从数据中确定。这种方法的优势在于我们不需要事先知道簇的数量，但与其他方法相比，其扩展性并不好，因为在最简单的实现中，我们需要一个n-by-n的相似度矩阵作为输入。如果我们将此算法应用于之前的数据集，我们会看到它检测到的簇数量远多于我们的肘图所暗示的，因为运行以下命令给出簇数量为309。
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'However, if we look at a histogram of the number of datapoints in each cluster,
    using the command:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们查看每个簇中数据点的数量直方图，使用以下命令：
- en: '[PRE29]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can see that only a few clusters are large, while many points are identified
    as belonging to their own group:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到只有少数簇很大，而许多点被标识为属于它们自己的组：
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_29.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![亲和传播 – 自动选择簇数量](img/B04881_03_29.jpg)'
- en: 'Where K-Means Fails: Clustering Concentric Circles'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在K-Means失败的地方：聚类同心圆
- en: 'So far, our data has been well clustered using k-means or variants such as
    affinity propagation. What examples might this algorithm perform poorly on? Let''s
    take one example by loading our second example dataset and plotting it using the
    commands:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的数据已经很好地使用k-means或亲和传播等变体进行了聚类。这个算法可能在哪些情况下表现不佳？让我们通过加载我们的第二个示例数据集并使用以下命令进行绘图来举一个例子：
- en: '[PRE30]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_30.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![亲和传播 – 自动选择簇数量](img/B04881_03_30.jpg)'
- en: 'By eye alone, you can clearly see that there are two groups: two circles nested
    one within the other. However, if we try to run k-means clustering on this data,
    we get an unsatisfactory result, as you can see from running the following commands
    and plotting the result:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭肉眼，你可以清楚地看到有两个组：两个嵌套在一起的圆。然而，如果我们尝试对此数据进行k-means聚类，我们会得到一个不满意的结果，正如你从以下命令的运行和结果图中可以看到：
- en: '[PRE31]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_31.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![亲和传播 – 自动选择簇数量](img/B04881_03_31.jpg)'
- en: In this case, the algorithm was unable to identify the two natural clusters
    in the data—because the center ring of data is the same distance to the outer
    ring at many points, the randomly assigned cluster center (which is more likely
    to land somewhere in the outer ring) is a mathematically sound choice for the
    nearest cluster. This example suggests that, in some circumstances, we may need
    to change our strategy and use a conceptually different algorithm. Maybe our objective
    of squared error (inertia) is incorrect, for example. In this case, we might try
    k-medoids.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，算法无法识别数据中的两个自然聚类——因为数据中心环在许多点上的距离与外环相同，随机分配的聚类中心（更有可能落在外环的某个地方）对于最近的聚类是一个数学上合理的选择。这个例子表明，在某些情况下，我们可能需要改变我们的策略，并使用概念上不同的算法。也许我们的目标——平方误差（惯性）是不正确的，例如。在这种情况下，我们可能会尝试使用k-medoids。
- en: k-medoids
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-medoids
- en: 'As we have described earlier, the k-means (medians) algorithm is best suited
    to particular distance metrics, the squared Euclidean and Manhattan distance (respectively),
    since these distance metrics are equivalent to the optimal value for the statistic
    (such as total squared distance or total distance) that these algorithms are attempting
    to minimize. In cases where we might have other distance metrics (such as correlations),
    we might also use the k-medoid method (Theodoridis, Sergios, and Konstantinos
    Koutroumbas. *Pattern recognition.* (2003).), which consists of the following
    steps:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所描述的，k-means（中位数）算法最适合特定的距离度量，分别是平方欧几里得距离和曼哈顿距离，因为这些距离度量等同于这些算法试图最小化的统计量的最优值（如总平方距离或总距离）。在可能具有其他距离度量（如相关性）的情况下，我们也可以使用k-medoid方法（Theodoridis,
    Sergios, and Konstantinos Koutroumbas. *Pattern recognition.* (2003).），它包括以下步骤：
- en: Select *k* initial points as the initial cluster centers.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择*k*个初始点作为初始聚类中心。
- en: Calculate the nearest cluster center for each datapoint by any distance metric
    and assign it to that cluster.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过任何距离度量计算每个数据点的最近聚类中心，并将其分配给该聚类。
- en: For each point and each cluster center, swap the cluster center with the point
    and calculate the reduction in overall distances to the cluster center across
    all cluster members using this swap. If it doesn't improve, undo it. Iterate step
    3 for all points.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个点和每个聚类中心，将聚类中心与点交换，并计算使用此交换在整个聚类成员中到聚类中心的总体距离的减少。如果它没有改进，则撤销。对所有点重复步骤3。
- en: 'This is obviously not an exhaustive search (since we don''t repeat step 1),
    but has the advantage that the optimality criterion is not a specific optimization
    function but rather improving the compactness of the clusters by a flexible distance
    metric. Can k-medoids improve our clustering of concentric circles? Let''s try
    running using the following commands and plotting the result:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然不是一个穷举搜索（因为我们没有重复步骤1），但它的优点是，最优性标准不是一个特定的优化函数，而是通过灵活的距离度量来提高聚类的紧凑性。k-medoids能否改善我们同心圆的聚类？让我们尝试使用以下命令运行并绘制结果：
- en: '[PRE32]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that k-medoids is not included in sci-kit learn, so you will need to install
    the pyclust library using `easy_install` or `pip`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，k-medoids不包括在sci-kit learn中，所以您需要使用`easy_install`或`pip`安装pyclust库。
- en: '![k-medoids](img/B04881_03_33.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![k-medoids](img/B04881_03_33.jpg)'
- en: There isn't much improvement over k-means, so perhaps we need to change our
    clustering algorithm entirely. Perhaps instead of generating a similarity between
    datapoints in a single stage, we could examine hierarchical measures of similarity
    and clustering, which is the goal of the agglomerative clustering algorithms we
    will examine next.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与k-means相比，改进不大，所以我们可能需要完全改变我们的聚类算法。也许我们不应该在单阶段生成数据点之间的相似性，而应该检查相似性和聚类的层次度量，这正是我们将要检查的层次聚类算法的目标。
- en: Agglomerative clustering
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: 'In contrast to algorithms, such as k-means, where the dataset is partitioned
    into individual groups, **agglomerative** or **hierarchical** clustering techniques
    start by considering each datapoint as its own cluster and merging them together
    into larger groups from the bottom up (Maimon, Oded, and Lior Rokach, eds. *Data
    mining and knowledge discovery handbook*. Vol. 2\. New York: Springer, 2005).
    The classical application of this idea is in phylogenetic trees in evolution,
    where common ancestors connect individual organisms. Indeed, these methods organize
    the data into tree diagrams, known as **dendrograms**, which visualize how the
    data is sequentially merged into larger groups.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与将数据集划分为单个组的算法（如k-means）不同，**聚合**或**层次**聚类技术首先将每个数据点视为其自己的聚类，并从底部向上将它们合并成更大的组（Maimon,
    Oded, 和 Lior Rokach 编著 *数据挖掘与知识发现手册*。第2卷。纽约：Springer，2005）。这一想法的经典应用是在进化中的系统发育树，其中共同的祖先连接了单个生物体。确实，这些方法将数据组织成树状图，称为**树状图**，以可视化数据如何按顺序合并成更大的组。
- en: 'The basic steps of an agglomerative algorithm are (diagrammed visually in the
    figure below):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合算法的基本步骤如下（如图所示）：
- en: Start with each point in its own cluster.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个点在其自己的聚类开始。
- en: Compare each pair of datapoints using a distance metric. This could be any of
    the methods discussed above.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用距离度量比较每对数据点。这可以是上述讨论的任何方法。
- en: 'Use a linkage criterion to merge datapoints (at the first stage) or clusters
    (in subsequent phases), where linkage is represented by a function such as:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用连接标准合并数据点（在第一阶段）或聚类（在后续阶段），其中连接由如下函数表示：
- en: The maximum (also known as complete linkage) distance between two sets of points.The
    minimum (also known as single linkage) distance between two sets of points.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两组点之间的最大距离（也称为完全连接），或最小距离（也称为单连接）。
- en: The average distance between two sets of points, also known as **Unweighted
    Pair Group Method with Arithmetic Mean** (**UPGMA**). The points in each group
    could also be weighted to give a weighted average, or WUPGMA.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两组点之间的平均距离，也称为**未加权配对组平均法**（**UPGMA**）。每个组中的点也可以加权，以给出加权平均值，或WUPGMA。
- en: The difference between centroids (centers of mass), or UPGMC.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中心点（质量中心）之间的差异，或UPGMC。
- en: 'The squared Euclidean distance between two sets of points, or Ward''s Method
    (Ward Jr, Joe H. *Hierarchical grouping to optimize an objective function*. *Journal
    of the American statistical association* 58.301 (1963): 236-244).'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '两组点之间的欧几里得距离的平方，或Ward的方法（Ward Jr, Joe H. *通过优化目标函数进行层次分组*。*美国统计协会杂志* 58.301
    (1963): 236-244）。'
- en: Repeat steps 2-3 until there is only a single cluster containing all data points.
    Note that following the first round, the first stage of clusters become a new
    point to compare with all other clusters, and at each stage the clusters formed
    become larger as the algorithm proceeds. Along the way, we will construct a tree
    diagram as we sequentially merge clusters from the prior steps together.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复步骤2-3，直到只剩下一个包含所有数据点的单一聚类。注意，在第一轮之后，聚类的第一阶段成为一个新的点，与其他所有聚类进行比较，并且随着算法的进行，每个阶段的聚类形成会越来越大。在这个过程中，我们将构建一个树状图，因为我们按顺序将前一步骤中的聚类合并在一起。
- en: '![Agglomerative clustering](img/B04881_03_40.jpg)'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![聚合聚类](img/B04881_03_40.jpg)'
- en: 'Agglomerative clustering: From top to bottom, example of tree construction
    (right) from a dataset (left) by sequentially merging closest points'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聚合聚类：从上到下，从数据集（左）通过顺序合并最近的点构建树形结构（右）的示例。
- en: Note that we could also run this process in reverse, taking an initial dataset
    and splitting it into individual points, which would also construct a tree diagram.
    In either case, we could then find clusters at several levels of resolution by
    choosing a cutoff depth of the tree and assigning points to the largest cluster
    they have been assigned to below that cutoff. This depth is often calculated using
    the linkage score given in step 3, allowing us conceptually to choose an appropriate
    distance between groups to consider a cluster (either relatively close or far
    away as we move up the tree).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们也可以反向运行此过程，从一个初始数据集开始，将其拆分为单个点，这也会构建一个树状图。在两种情况下，我们都可以通过选择树的截止深度并将点分配给它们在截止深度以下分配的最大聚类来找到多个分辨率的聚类。这个深度通常使用步骤3中给出的连接分数来计算，使我们能够在概念上选择一个合适的组间距离来考虑聚类（随着我们向上移动树，要么相对接近要么相对远离）。
- en: Where agglomerative clustering fails
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类分析中的不足
- en: 'Agglomerative algorithms have many of the same ingredients as k-means; we choose
    a number of clusters (which will determine where we cut the tree generated by
    clustering—in the most extreme case, all points become members of a single cluster)
    and a similarity metric. We also need to choose a **linkage metric** for step
    3, which determines the rules for merging individual branches of our tree. Can
    agglomerative clustering succeed where k-means failed? Trying this approach on
    the circular data suggests otherwise, as shown by plotting results of the following
    commands:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法与k-means算法有许多相同的成分；我们选择一个聚类数（这将决定我们如何切割聚类生成的树——在最极端的情况下，所有点都成为单个聚类的成员）和一个相似性度量。我们还需要为第3步选择一个**链接度量**，它决定了合并树中单个分支的规则。层次聚类能否在k-means失败的地方成功？尝试这种方法在圆形数据上似乎不然，如下面的命令结果所示：
- en: '[PRE33]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![Where agglomerative clustering fails](img/B04881_03_41.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![聚类失败的地方](img/B04881_03_41.jpg)'
- en: In order to correctly group the inner and outer circle, we can attempt to modify
    our notion of similarity using connectivity, a concept taken from graph analysis
    in which a set of nodes are connected by edges, and connectivity refers to whether
    two nodes share an edge between them. Here, we essentially construct a graph between
    pairs of points by thresholding the number of points that can be considered similar
    to one another, rather than measuring a continuous distance metric between each
    pair of points. This potentially reduces our difficulties with the concentric
    circles data, since if we set a very small value (say 10 nearby points), the uniform
    distance from the middle to the outer ring is no longer problematic because the
    central points will always be closer to each other than to the periphery. To construct
    this connectivity-based similarity, we could take a distance matrix such as those
    we've already calculated, and threshold it for some value of similarity by which
    we think points are connected, giving us a binary matrix of 0 and 1\. This kind
    of matrix, representing the presence or absence of an edge between nodes in a
    graph, is also known as an **adjacency matrix**. We could choose this value through
    inspecting the distribution of pairwise similarity scores, or based on prior knowledge.
    We can then supply this matrix as an argument to our agglomerative clustering
    routine, providing a neighborhood of points to be considered when comparing datapoints,
    which gives an initial structure to the clusters. We can see that this makes a
    huge difference to the algorithms results after running the following commands.
    Note that when we generate the adjacency matrix L, we may end up with an asymmetric
    matrix since we threshold the ten most similar points for each member of the data.
    This could lead to situations where two points are not mutually closest to each
    other, leading to an edge represented in only the top or bottom triangle of the
    adjacency matrix. To generate a symmetric input for our clustering algorithm,
    we take the average of the matrix L added to its transpose, which effectively
    adds edges in both directions between two points.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确地将内圈和外圈分组，我们可以尝试通过连通性来修改我们对相似性的理解，连通性是一个从图分析中借用的概念，其中一组节点通过边连接，连通性指的是两个节点之间是否共享一条边。在这里，我们通过阈值化可以相互视为相似点的点的数量，而不是测量每对点之间的连续距离度量，从而在点对之间构建一个图。这可能会减少我们在同心圆数据上的困难，因为如果我们设置一个非常小的值（比如说10个附近的点），从中心到外环的均匀距离就不再成问题，因为中心点总是彼此比外围点更近。为了构建这种基于连通性的相似性，我们可以取一个距离矩阵，例如我们之前已经计算过的那些，并对其进行阈值化，以某个我们认为点之间相连的相似性值，得到一个由0和1组成的二进制矩阵。这种表示图中节点之间边存在与否的矩阵也被称为**邻接矩阵**。我们可以通过检查成对相似度分数的分布或基于先验知识来选择这个值。然后，我们可以将这个矩阵作为参数提供给我们的层次聚类程序，提供在比较数据点时要考虑的点邻域，这为聚类提供了初始结构。我们可以看到，在运行以下命令后，这会对算法的结果产生巨大影响。注意，当我们生成邻接矩阵L时，我们可能会得到一个非对称矩阵，因为我们为数据中的每个成员阈值化了最相似的十个点。这可能导致两个点不是彼此最近的，导致在邻接矩阵的上三角或下三角中只表示一个边。为了生成聚类算法的对称输入，我们取矩阵L及其转置的平均值，这实际上在两点之间添加了双向边。
- en: '[PRE34]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, as you can see, this algorithm can correctly identify and separate two
    clusters:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，正如你所看到的，这个算法可以正确地识别和分离两个聚类：
- en: 'Interestingly, constructing this neighborhood graph and partitioning it into
    sub graphs (splitting the whole graph into a set of nodes and edges that are primarily
    connected to each other, rather than to other elements of the network) is equivalent
    to performing k-means clustering on a transformed distance matrix, an approach
    known as Spectral Clustering (Von Luxburg, Ulrike. *A tutorial on spectral clustering.*
    Statistics and computing 17.4 (2007): 395-416). The transformation here is from
    taking the Euclidean distance D we calculated earlier and calculating the kernel
    score—the Gaussian kernel given by:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '有趣的是，构建这个邻域图并将其划分为子图（将整个图划分为一组节点和边，这些节点和边主要相互连接，而不是与其他网络元素连接），这与在转换后的距离矩阵上执行
    k-means 聚类等价，这种方法被称为谱聚类（Von Luxburg, Ulrike. *谱聚类的教程.* 统计与计算 17.4 (2007): 395-416）。这里的转换是将我们之前计算的欧几里得距离
    D 转换为核分数——由以下高斯核给出：'
- en: '![Where agglomerative clustering fails](img/B04881_03_70.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![聚合聚类失败的地方](img/B04881_03_70.jpg)'
- en: 'between each pair of points i and j, with a bandwidth γ instead of making a
    hard threshold as when we constructed the neighborhood before. Using the pairwise
    kernel matrix K calculated from all points i and j, we can then construct the
    Laplacian matrix of a graph, which is given by:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在每对点 i 和 j 之间，用带宽 γ 代替我们在构建邻域之前所做的硬阈值。使用从所有点 i 和 j 计算出的成对核矩阵 K，然后我们可以构建一个图的拉普拉斯矩阵，它由以下公式给出：
- en: '![Where agglomerative clustering fails](img/B04881_03_71.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![聚合聚类失败的地方](img/B04881_03_71.jpg)'
- en: 'Here, *I* is the identity matrix (with a one along the diagonal and zero elsewhere),
    and *D* is the diagonal matrix whose elements are :'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*I* 是单位矩阵（对角线上的元素为1，其他地方为0），而 *D* 是对角矩阵，其元素为：
- en: '![Where agglomerative clustering fails](img/B04881_03_72.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![聚合聚类失败的地方](img/B04881_03_72.jpg)'
- en: Giving the number of neighbors for each point *i*. In essence by calculating
    L we now represent the dataset as a series of nodes (points) connected by edges
    (the elements of this matrix), which have been normalized so that the total value
    of all edges for each node sums to 1\. Since the Gaussian kernel score is continuous,
    in this normalization divides pairwise distances between a given point and all
    others into a probability distribution where the distances (edges) sum to 1.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个点 *i* 提供邻居的数量。本质上，通过计算 L，我们现在将数据集表示为一系列通过边（这个矩阵的元素）连接的节点（点），这些边的值已经被归一化，使得每个节点的所有边的总和为
    1。由于高斯核分数是连续的，在这个归一化过程中，将给定点与所有其他点之间的成对距离划分为一个概率分布，其中距离（边）的总和为 1。
- en: 'You may recall from linear algebra that Eigenvectors of a matrix A are vectors
    v for which, if we multiply the matrix by the eigenvector v, we get the same result
    as if we had multiplied the vector by a constant amount λ: ![Where agglomerative
    clustering fails](img/B04881_03_73.jpg). Thus, the matrix here represents a kind
    of operation on the vector. For example, the identity matrix gives an eigenvalue
    of 1, since multiplying v by the identity gives v itself. We could also have a
    matrix such as:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，从线性代数中，矩阵 A 的特征向量 v 是这样的向量，如果我们用矩阵乘以特征向量 v，我们会得到与用常数 λ 乘以向量相同的结果：![聚合聚类失败的地方](img/B04881_03_73.jpg)。因此，这里的矩阵代表对向量的某种操作。例如，单位矩阵给出特征值
    1，因为用单位矩阵乘以 v 得到的是 v 本身。我们也可以有如下矩阵：
- en: '![Where agglomerative clustering fails](img/B04881_03_74.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![聚合聚类失败的地方](img/B04881_03_74.jpg)'
- en: 'which doubles the value of the vector with which it is multiplied, suggesting
    the matrix acts as a ''stretching'' operation on the vector. From this perspective,
    larger eigenvalues correspond to greater stretching of the vector, while the eigenvector
    gives the direction in which the stretching occurs. This is useful because it
    gives us the primary axes along which the matrix operation acts. In our example,
    if we take the two eigenvectors with the largest eigenvalues (in essence, the
    directions in which the matrix represents the greatest transformation of a vector),
    we are extracting the two greatest axes of variation in the matrix. We will return
    to this concept in more detail when we discuss *Principal components* in [Chapter
    6](ch06.html "Chapter 6. Words and Pixels – Working with Unstructured Data"),
    *Words and Pixels – Working with Unstructured Data*, but suffice to say that if
    we run `run-kmeans` on these eigenvectors (an approach known as **spectral clustering**,
    since the eigenvalues of a matrix that we cluster are known as the spectrum of
    a matrix), we get a result very similar to the previous agglomerative clustering
    approach using neighborhoods, as we can see from executing the following commands:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 它将与之相乘的向量的值加倍，表明矩阵在向量上执行了“拉伸”操作。从这个角度来看，较大的特征值对应于向量更大的拉伸，而特征向量给出了拉伸发生的方向。这很有用，因为它给出了矩阵操作作用的主要轴。在我们的例子中，如果我们取具有最大特征值的两个特征向量（本质上，矩阵表示向量最大变换的方向），我们正在提取矩阵中两个最大的变化轴。当我们讨论[第6章](ch06.html
    "第6章。文字和像素 - 处理非结构化数据")中的*主成分*时，我们将更详细地回到这个概念，*文字和像素 - 处理非结构化数据*，但简而言之，如果我们在这两个特征向量上运行`run-kmeans`（这种方法被称为**谱聚类**，因为聚类的矩阵的特征值被称为矩阵的谱），我们得到的结果与之前使用邻域的聚合聚类方法非常相似，正如我们从以下命令的执行中可以看到：
- en: '[PRE35]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Where agglomerative clustering fails](img/B04881_03_42.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![聚类失败的地方](img/B04881_03_42.jpg)'
- en: 'We can successfully capture this nonlinear separation boundary because we''ve
    represented the points in the space of the greatest variation in pairwise distance,
    which is the difference between the inner and outermost circle in the data:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以成功地捕捉到这个非线性分离边界，因为我们已经将点表示在成对距离最大的空间中，这是数据中内圈和外圈之间的差异：
- en: 'The preceding examples should have given you a number of approaches you can
    use to tackle clustering problems, and as a rule of thumb guide, the following
    diagram illustrates the decision process for choosing between them:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例应该已经给了你许多可以用来解决聚类问题的方法，并且作为一个经验法则指南，以下图表说明了选择它们之间的决策过程：
- en: '![Where agglomerative clustering fails](img/B04881_03_46.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![聚类失败的地方](img/B04881_03_46.jpg)'
- en: For the last part of our exploration of clustering, let's look at an example
    application utilizing Spark Streaming and k-means, which will allow us to incrementally
    update our clusters as they are received.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索聚类的最后部分，让我们看看一个利用Spark Streaming和k-means的示例应用，这将允许我们随着接收到的数据逐步更新我们的聚类。
- en: Streaming clustering in Spark
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的流式聚类
- en: Up to this point, we have mainly demonstrated examples for ad hoc exploratory
    analysis. In building up analytical applications, we need to begin putting these
    into a more robust framework. As an example, we will demonstrate the use of a
    streaming clustering pipeline using PySpark. This application will potentially
    scale to very large datasets, and we will compose the pieces of the analysis in
    such a way that it is robust to failure in the case of malformed data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要展示了用于即席探索性分析的示例。在构建分析应用时，我们需要开始将这些内容放入一个更健壮的框架中。作为一个例子，我们将演示使用PySpark的流式聚类管道的使用。这个应用有可能扩展到非常大的数据集，我们将以这种方式组合分析的部分，使其在数据格式错误的情况下具有容错性。
- en: 'As we will be using similar examples with PySpark in the following chapters,
    let''s review the key ingredients we need in such application, some of which we
    already saw in [Chapter 2](ch02.html "Chapter 2. Exploratory Data Analysis and
    Visualization in Python"), *Exploratory Data Analysis and Visualization in Python*.
    Most PySpark jobs we will create in this book consist of the following steps:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在接下来的章节中使用与PySpark类似的示例，让我们回顾一下此类应用中我们需要的关键组成部分，其中一些我们在[第2章](ch02.html
    "第2章。Python中的探索性数据分析和可视化")中已经看到，*Python中的探索性数据分析和可视化*。我们将在本书中创建的大多数PySpark作业都由以下步骤组成：
- en: Construct a Spark context. The context contains information about the name of
    the application, and parameters such as memory and number of tasks.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个Spark上下文。该上下文包含有关应用程序名称以及内存和任务数量等参数的信息。
- en: The Spark context may be used to construct secondary context objects, such as
    the streaming context we will use in this example. This context object contains
    parameters specifically about a particular kind of task, such as a streaming dataset,
    and inherits all the information we have previously initialized in the base Spark
    context.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark上下文可以用来构建二级上下文对象，例如我们将在此示例中使用的流上下文。此上下文对象包含特定于特定类型任务的参数，例如流数据集，并继承我们在基本Spark上下文中先前初始化的所有信息。
- en: Construct a dataset, which is represented in Spark as a **Resilient Distributed
    Dataset** (**RDD**). While from a programmatic standpoint we can operate on this
    RDD just as we do with, for example, a pandas dataframe, under the hood it is
    potentially parallelized across many machines during analysis. We may parallelize
    the data after reading it from a source file, or reading from parallel file systems
    such as Hadoop. Ideally we don't want to fail our whole job if one line of data
    is erroneous, so we would like to place an error handling mechanism here that
    will alert us to a failure to parse a row without blocking the whole job.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个数据集，在Spark中表示为**弹性分布式数据集**（**RDD**）。从程序的角度来看，我们可以像操作例如pandas数据框一样操作这个RDD，但实际上在分析过程中它可能被并行化到多台机器上。我们可能在从源文件读取后或从Hadoop等并行文件系统中读取数据后并行化数据。理想情况下，我们不希望因为一行数据错误而导致整个作业失败，因此我们希望在错误处理机制中放置一个将提醒我们解析行失败而不会阻塞整个作业的机制。
- en: We frequently need to transform our input dataset into a subclass of RDD known
    as a **Labeled RDD**. Labeled RDDs contain a label (such as the cluster label
    for the clustering algorithms we have been studying in this chapter) and a set
    of features. For our clustering problems, we will only perform this transformation
    when we predict (as we usually don't know the cluster ahead of time), but for
    the regression and classification models we will look at in [Chapter 4](ch04.html
    "Chapter 4. Connecting the Dots with Models – Regression Methods"), *Connecting
    the Dots with Models – Regression Methods*, and [Chapter 5](ch05.html "Chapter 5. Putting
    Data in its Place – Classification Methods and Analysis"), *Putting Data in its
    Place – Classification Methods and Analysis*, the label is used as a part of fitting
    the model.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们经常需要将我们的输入数据集转换为RDD的子类，称为**标记RDD**。标记RDD包含一个标签（例如，本章中我们研究过的聚类算法的聚类标签）和一组特征。对于我们的聚类问题，我们将在预测时执行此转换（因为我们通常不知道聚类的时间），但对于我们在[第4章](ch04.html
    "第4章。用模型连接点——回归方法")、“用模型连接点——回归方法”和[第5章](ch05.html "第5章。将数据放在合适的位置——分类方法和分析")、“将数据放在合适的位置——分类方法和分析”中将要查看的回归和分类模型，标签用作拟合模型的一部分。
- en: We'll frequently want a way to save the output of our modeling to be used by
    downstream applications, either on disk or in a database, where we can later query
    models indexed by history.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们经常希望有一种方法将我们的建模输出保存下来，以便由下游应用程序使用，无论是在磁盘上还是在数据库中，我们可以稍后通过历史记录查询索引模型。
- en: 'Let''s look at some of these components using the Python notebook. Assuming
    we have Spark installed on our system, we''ll start by importing the required
    dependencies:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Python笔记本查看这些组件。假设我们已经在系统上安装了Spark，我们将首先导入所需的依赖项：
- en: '[PRE36]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can then test starting the `SparkContext`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以测试启动`SparkContext`：
- en: '[PRE37]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Recall that the first argument gives the URL for our Spark master, the machine
    that coordinates execution of Spark jobs and distributes tasks to the worker machines
    in a cluster. In this case, we will run it locally, so give this argument as `localhost`,
    but otherwise this could be the URL of a remote machine in our cluster. The second
    argument is just the name we give to our application. With a context running,
    we can also generate the streaming context, which contains information about our
    streaming application, using the following:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，第一个参数提供了我们的Spark主机的URL，即协调Spark作业执行并将任务分配给集群中工作机的机器。在这种情况下，我们将本地运行它，因此将此参数指定为`localhost`，但否则这可能是我们集群中远程机器的URL。第二个参数只是我们为应用程序指定的名称。在上下文运行后，我们还可以使用以下方式生成流上下文，该上下文包含有关我们的流应用程序的信息：
- en: '[PRE38]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The first argument is simply the `SparkContext` used as a parent of the `StreamingContext`:
    the second is the frequency in seconds at which we will check our streaming data
    source for new data. If we expect regularly arriving data we could make this lower,
    or make it higher if we expect new data to be available less frequently.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数简单地是作为`StreamingContext`父类的`SparkContext`：第二个是我们将检查流数据源新数据的频率（以秒为单位）。如果我们期望数据定期到达，我们可以将其设置得更低，或者如果预期新数据不太频繁地可用，我们可以将其设置得更高。
- en: 'Now that we have a `StreamingContext`, we can add data sources. Let''s assume
    for now we''ll have two sources for training data (which could be historical).
    We want the job not to die if we give one line of bad data, and so we use a `Parser`
    class that gives this flexibility:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了`StreamingContext`，我们可以添加数据源。假设现在我们将有两个训练数据源（可能是历史数据）。我们希望作业在给出一条错误数据时不会死亡，因此我们使用一个提供这种灵活性的`Parser`类：
- en: '[PRE39]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We log error lines to a file with the name of our job ID, which will allow
    us to locate them later if we need to. We can then use this parser to train and
    evaluate the model. To train the model, we move files with three columns (a label
    and the data to be clustered) into the training directory. We can also add to
    the test data directory files with two columns only the coordinate features:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将错误行记录到以我们的作业ID命名的文件中，这样如果需要的话，我们可以稍后定位它们。然后我们可以使用这个解析器来训练和评估模型。为了训练模型，我们将具有三列（标签和要聚类的数据）的文件移动到训练目录中。我们还可以向测试数据目录添加只有两列的文件，仅包含坐标特征：
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The decay factor in the parameters gives the recipe for combining current cluster
    centers and old ones. For parameter 1.0, we use an equal weight between old and
    new, while for the other extreme, at 0, we only use the new data. If we stop the
    model at any point we, can inspect it using the `lastestModel()` function:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 参数中的衰减因子给出了结合当前聚类中心和旧聚类中心的配方。对于参数1.0，我们使用新旧数据之间的相等权重，而在另一个极端，即0时，我们只使用新数据。如果我们在任何时候停止模型，我们可以使用`lastestModel()`函数来检查它：
- en: '[PRE41]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We could also predict using the `predict()` function on an appropriately sized
    vector:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用`predict()`函数在适当大小的向量上预测：
- en: '[PRE42]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this section, we learned how to identify groups of similar items in a dataset,
    an exploratory analysis that we might frequently use as a first step in deciphering
    new datasets. We explored different ways of calculating the similarity between
    datapoints and described what kinds of data these metrics might best apply to.
    We examined both divisive clustering algorithms, which split the data into smaller
    components starting from a single group, and agglomerative methods, where every
    datapoint starts as its own cluster. Using a number of datasets, we showed examples
    where these algorithms will perform better or worse, and some ways to optimize
    them. We also saw our first (small) data pipeline, a clustering application in
    PySpark using streaming data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何在数据集中识别相似项的组，这是一种探索性分析，我们可能会在解读新数据集时频繁使用它作为第一步。我们探讨了计算数据点之间相似性的不同方法，并描述了这些指标可能最适合应用的数据类型。我们考察了两种聚类算法：一种是从单个组开始将数据分割成更小组件的划分聚类算法，另一种是每个数据点最初都是其自己的聚类的聚合方法。使用多个数据集，我们展示了这些算法表现好坏的例子，以及一些优化它们的方法。我们还看到了我们的第一个（小型）数据处理管道，这是一个使用流数据的PySpark聚类应用。
