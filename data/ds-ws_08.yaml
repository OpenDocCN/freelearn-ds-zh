- en: '8'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '8'
- en: 8\. Hyperparameter Tuning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. è¶…å‚æ•°è°ƒèŠ‚
- en: Overview
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚è§ˆ
- en: In this chapter, each hyperparameter tuning strategy will be first broken down
    into its key steps before any high-level scikit-learn implementations are demonstrated.
    This is to ensure that you fully understand the concept behind each of the strategies
    before jumping to the more automated methods.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæ¯ç§è¶…å‚æ•°è°ƒèŠ‚ç­–ç•¥å°†é¦–å…ˆåˆ†è§£ä¸ºå…¶å…³é”®æ­¥éª¤ï¼Œç„¶åå†å±•ç¤ºä»»ä½•é«˜çº§çš„scikit-learnå®ç°ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿åœ¨è·³åˆ°æ›´è‡ªåŠ¨åŒ–çš„æ–¹æ³•ä¹‹å‰ï¼Œä½ èƒ½å®Œå…¨ç†è§£æ¯ç§ç­–ç•¥èƒŒåçš„æ¦‚å¿µã€‚
- en: By the end of this chapter, you will be able to find further predictive performance
    improvements via the systematic evaluation of estimators with different hyperparameters.
    You will successfully deploy manual, grid, and random search strategies to find
    the optimal hyperparameters. You will be able to parameterize **k-nearest neighbors**
    (**k-NN**), **support vector machines** (**SVMs**), ridge regression, and random
    forest classifiers to optimize model performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ç»“æŸæ—¶ï¼Œä½ å°†èƒ½å¤Ÿé€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°å…·æœ‰ä¸åŒè¶…å‚æ•°çš„ä¼°ç®—å™¨ï¼Œè¿›ä¸€æ­¥æå‡é¢„æµ‹æ€§èƒ½ã€‚ä½ å°†æˆåŠŸåœ°éƒ¨ç½²æ‰‹åŠ¨æœç´¢ã€ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢ç­–ç•¥ï¼Œæ‰¾åˆ°æœ€ä¼˜è¶…å‚æ•°ã€‚ä½ å°†èƒ½å¤Ÿå¯¹**kæœ€è¿‘é‚»**ï¼ˆ**k-NN**ï¼‰ã€**æ”¯æŒå‘é‡æœº**ï¼ˆ**SVMs**ï¼‰ã€å²­å›å½’å’Œéšæœºæ£®æ—åˆ†ç±»å™¨è¿›è¡Œå‚æ•°åŒ–ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•è¨€
- en: In previous chapters, we discussed several methods to arrive at a model that
    performs well. These include transforming the data via preprocessing, feature
    engineering and scaling, or simply choosing an appropriate estimator (algorithm)
    type from the large set of possible estimators made available to the users of
    scikit-learn.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰å‡ ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å‡ ç§æ–¹æ³•æ¥å¾—åˆ°ä¸€ä¸ªè¡¨ç°è‰¯å¥½çš„æ¨¡å‹ã€‚è¿™äº›æ–¹æ³•åŒ…æ‹¬é€šè¿‡é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹å’Œç¼©æ”¾æ¥è½¬æ¢æ•°æ®ï¼Œæˆ–ä»å¤§é‡å¯ç”¨çš„scikit-learnä¼°ç®—å™¨ä¸­é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ä¼°ç®—å™¨ï¼ˆç®—æ³•ï¼‰ç±»å‹ã€‚
- en: Depending on which estimator you eventually select, there may be settings that
    can be adjusted to improve overall predictive performance. These settings are
    known as hyperparameters, and deriving the best hyperparameters is known as tuning
    or optimizing. Properly tuning your hyperparameters can result in performance
    improvements well into the double-digit percentages, so it is well worth doing
    in any modeling exercise.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æœ€ç»ˆé€‰æ‹©çš„ä¼°ç®—å™¨ï¼Œå¯èƒ½æœ‰ä¸€äº›è®¾ç½®å¯ä»¥è°ƒæ•´ï¼Œä»¥æé«˜æ•´ä½“é¢„æµ‹æ€§èƒ½ã€‚è¿™äº›è®¾ç½®è¢«ç§°ä¸ºè¶…å‚æ•°ï¼Œæ¨å¯¼å‡ºæœ€ä½³è¶…å‚æ•°çš„è¿‡ç¨‹ç§°ä¸ºè°ƒå‚æˆ–ä¼˜åŒ–ã€‚æ­£ç¡®è°ƒèŠ‚è¶…å‚æ•°å¯ä»¥å¸¦æ¥æ€§èƒ½çš„æ˜¾è‘—æå‡ï¼Œç”šè‡³è¾¾åˆ°ä¸¤ä½æ•°çš„ç™¾åˆ†æ¯”ï¼Œå› æ­¤åœ¨ä»»ä½•å»ºæ¨¡ä»»åŠ¡ä¸­éƒ½éå¸¸å€¼å¾—è¿›è¡Œè°ƒå‚ã€‚
- en: This chapter will discuss the concept of hyperparameter tuning and will present
    some simple strategies that you can use to help find the best hyperparameters
    for your estimators.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†è®¨è®ºè¶…å‚æ•°è°ƒèŠ‚çš„æ¦‚å¿µï¼Œå¹¶ä»‹ç»ä¸€äº›ç®€å•çš„ç­–ç•¥ï¼Œå¸®åŠ©ä½ ä¸ºä¼°ç®—å™¨æ‰¾åˆ°æœ€ä½³è¶…å‚æ•°ã€‚
- en: In previous chapters, we have seen some exercises that use a range of estimators,
    but we haven't conducted any hyperparameter tuning. After reading this chapter,
    we recommend you revisit these exercises, apply the techniques taught, and see
    if you can improve the results.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰å‡ ç« ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›ä½¿ç”¨å„ç§ä¼°ç®—å™¨çš„ç»ƒä¹ ï¼Œä½†æˆ‘ä»¬å¹¶æ²¡æœ‰è¿›è¡Œä»»ä½•è¶…å‚æ•°è°ƒèŠ‚ã€‚é˜…è¯»å®Œæœ¬ç« åï¼Œæˆ‘ä»¬å»ºè®®ä½ é‡æ–°å®¡è§†è¿™äº›ç»ƒä¹ ï¼Œåº”ç”¨æ‰€å­¦çš„æŠ€å·§ï¼Œçœ‹çœ‹èƒ½å¦æå‡ç»“æœã€‚
- en: What Are Hyperparameters?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯è¶…å‚æ•°ï¼Ÿ
- en: Hyperparameters can be thought of as a set of dials and switches for each estimator
    that change how the estimator works to explain relationships in the data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°å¯ä»¥è¢«çœ‹ä½œæ˜¯æ¯ä¸ªä¼°ç®—å™¨çš„æ§åˆ¶æ—‹é’®å’Œå¼€å…³ï¼Œæ”¹å˜å®ƒä»¬ä¼šå½±å“ä¼°ç®—å™¨å¦‚ä½•å·¥ä½œï¼Œä»è€Œè§£é‡Šæ•°æ®ä¸­çš„å…³ç³»ã€‚
- en: 'Have a look at *Figure 8.1*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹*å›¾8.1*ï¼š
- en: '![Figure 8.1: How hyperparameters work'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾8.1ï¼šè¶…å‚æ•°å¦‚ä½•å·¥ä½œ'
- en: '](img/B15019_08_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_01.jpg)'
- en: 'Figure 8.1: How hyperparameters work'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.1ï¼šè¶…å‚æ•°å¦‚ä½•å·¥ä½œ
- en: If you read from left to right in the preceding figure, you can see that during
    the tuning process we change the value of the hyperparameter, which results in
    a change to the estimator. This in turn causes a change in model performance.
    Our objective is to find hyperparameterization that leads to the best model performance.
    This will be the *optimal* hyperparameterization.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»å·¦åˆ°å³é˜…è¯»å‰é¢çš„å›¾ç¤ºï¼Œä½ ä¼šçœ‹åˆ°åœ¨è°ƒèŠ‚è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ”¹å˜è¶…å‚æ•°çš„å€¼ï¼Œè¿™ä¼šå¯¼è‡´ä¼°ç®—å™¨çš„å˜åŒ–ã€‚è¿›è€Œï¼Œè¿™ä¼šå¼•èµ·æ¨¡å‹æ€§èƒ½çš„å˜åŒ–ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°èƒ½å¸¦æ¥æœ€ä½³æ¨¡å‹æ€§èƒ½çš„è¶…å‚æ•°é…ç½®ã€‚è¿™å°†æ˜¯*æœ€ä¼˜*çš„è¶…å‚æ•°é…ç½®ã€‚
- en: Estimators can have hyperparameters of varying quantities and types, which means
    that sometimes you can be faced with a very large number of possible hyperparameterizations
    to choose for an estimator.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼°ç®—å™¨å¯ä»¥å…·æœ‰ä¸åŒæ•°é‡å’Œç±»å‹çš„è¶…å‚æ•°ï¼Œè¿™æ„å‘³ç€æœ‰æ—¶ä½ å¯èƒ½ä¼šé¢ä¸´éœ€è¦é€‰æ‹©å¤§é‡å¯èƒ½çš„è¶…å‚æ•°é…ç½®çš„æƒ…å†µã€‚
- en: For instance, scikit-learn's implementation of the SVM classifier (`sklearn.svm.SVC`),
    which you will be introduced to later in the chapter, is an estimator that has
    multiple possible hyperparameterizations. We will test out only a small subset
    of these, namely using a linear kernel or a polynomial kernel of degree 2, 3,
    or 4.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œscikit-learn å®ç°çš„ SVM åˆ†ç±»å™¨ï¼ˆ`sklearn.svm.SVC`ï¼‰ï¼Œä½ å°†åœ¨æœ¬ç« ç¨åçœ‹åˆ°ï¼Œæ˜¯ä¸€ä¸ªå…·æœ‰å¤šç§å¯èƒ½è¶…å‚æ•°é…ç½®çš„ä¼°ç®—å™¨ã€‚æˆ‘ä»¬å°†ä»…æµ‹è¯•å…¶ä¸­çš„ä¸€å°éƒ¨åˆ†é…ç½®ï¼Œå³ä½¿ç”¨çº¿æ€§æ ¸æˆ–äºŒæ¬¡ã€ä¸‰æ¬¡æˆ–å››æ¬¡çš„å¤šé¡¹å¼æ ¸ã€‚
- en: Some of these hyperparameters are continuous in nature, while others are discrete,
    and the presence of continuous hyperparameters means that the number of possible
    hyperparameterizations is theoretically infinite. Of course, when it comes to
    producing a model with good predictive performance, some hyperparameterizations
    are much better than others, and it is your job as a data scientist to find them.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è¶…å‚æ•°ä¸­ï¼Œæœ‰äº›æ˜¯è¿ç»­å‹çš„ï¼Œè€Œæœ‰äº›æ˜¯ç¦»æ•£å‹çš„ï¼Œè¿ç»­å‹è¶…å‚æ•°çš„å­˜åœ¨æ„å‘³ç€ç†è®ºä¸Šå¯èƒ½çš„è¶…å‚æ•°åŒ–ç»„åˆæ˜¯æ— é™çš„ã€‚å½“ç„¶ï¼Œå½“æ¶‰åŠåˆ°ç”Ÿæˆå…·æœ‰è‰¯å¥½é¢„æµ‹æ€§èƒ½çš„æ¨¡å‹æ—¶ï¼Œä¸€äº›è¶…å‚æ•°åŒ–ç»„åˆè¦æ¯”å…¶ä»–çš„å¥½å¾—å¤šï¼Œä½œä¸ºæ•°æ®ç§‘å­¦å®¶ï¼Œä½ çš„å·¥ä½œå°±æ˜¯æ‰¾åˆ°è¿™äº›æ›´ä¼˜çš„è¶…å‚æ•°é…ç½®ã€‚
- en: In the next section, we will be looking at setting these hyperparameters in
    more detail. But first, some clarification of terms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°æ¢è®¨å¦‚ä½•è®¾ç½®è¿™äº›è¶…å‚æ•°ã€‚ä½†é¦–å…ˆï¼Œéœ€è¦æ¾„æ¸…ä¸€äº›æœ¯è¯­ã€‚
- en: Difference between Hyperparameters and Statistical Model Parameters
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°ä¸ç»Ÿè®¡æ¨¡å‹å‚æ•°çš„åŒºåˆ«
- en: 'In your reading on data science, particularly in the area of statistics, you
    will come across terms such as "model parameters," "parameter estimation," and
    "(non)-parametric models." These terms relate to the parameters that feature in
    the mathematical formulation of models. The simplest example is that of the single
    variable linear model with no intercept term that takes the following form:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ é˜…è¯»æ•°æ®ç§‘å­¦çš„ææ–™æ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»Ÿè®¡å­¦é¢†åŸŸï¼Œä½ ä¼šé‡åˆ°â€œæ¨¡å‹å‚æ•°â€ã€â€œå‚æ•°ä¼°è®¡â€å’Œâ€œï¼ˆéï¼‰å‚æ•°åŒ–æ¨¡å‹â€ç­‰æœ¯è¯­ã€‚è¿™äº›æœ¯è¯­ä¸æ¨¡å‹æ•°å­¦å…¬å¼ä¸­çš„å‚æ•°æœ‰å…³ã€‚æœ€ç®€å•çš„ä¾‹å­æ˜¯æ²¡æœ‰æˆªè·é¡¹çš„å•å˜é‡çº¿æ€§æ¨¡å‹ï¼Œå®ƒçš„å½¢å¼å¦‚ä¸‹ï¼š
- en: '![Figure 8.2: Equation for a single variable linear model'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.2ï¼šå•å˜é‡çº¿æ€§æ¨¡å‹çš„æ–¹ç¨‹'
- en: '](img/B15019_08_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_02.jpg)'
- en: 'Figure 8.2: Equation for a single variable linear model'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.2ï¼šå•å˜é‡çº¿æ€§æ¨¡å‹çš„æ–¹ç¨‹
- en: Here, ğ›½ is the statistical model parameter, and if this formulation is chosen,
    it is the data scientist's job to use data to estimate what value it takes. This
    could be achieved using **Ordinary Least Squares** (**OLS**) regression modeling,
    or it could be achieved through a method called median regression.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œğ›½ æ˜¯ç»Ÿè®¡æ¨¡å‹å‚æ•°ï¼Œå¦‚æœé€‰æ‹©è¿™ç§å½¢å¼ï¼Œæ•°æ®ç§‘å­¦å®¶çš„ä»»åŠ¡å°±æ˜¯ä½¿ç”¨æ•°æ®æ¥ä¼°è®¡å®ƒçš„å€¼ã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥é€šè¿‡**æ™®é€šæœ€å°äºŒä¹˜æ³•**ï¼ˆ**OLS**ï¼‰å›å½’å»ºæ¨¡å®ç°ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ä¸€ç§å«åšä¸­ä½æ•°å›å½’çš„æ–¹æ³•å®ç°ã€‚
- en: Hyperparameters are different in that they are external to the mathematical
    form. An example of a hyperparameter in this case is the way in which ğ›½ will be
    estimated (OLS, or median regression). In some cases, hyperparameters can change
    the algorithm completely (that is, generating a completely different mathematical
    form). You will see examples of this occurring throughout this chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°çš„ä¸åŒä¹‹å¤„åœ¨äºå®ƒä»¬æ˜¯å¤–éƒ¨äºæ•°å­¦æ¨¡å‹çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œè¶…å‚æ•°æ˜¯ä¼°è®¡ ğ›½ çš„æ–¹æ³•ï¼ˆå¦‚æœ€å°äºŒä¹˜æ³•ï¼ˆOLSï¼‰æˆ–ä¸­ä½æ•°å›å½’ï¼‰ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¶…å‚æ•°å¯èƒ½ä¼šå®Œå…¨æ”¹å˜ç®—æ³•ï¼ˆå³ï¼Œç”Ÿæˆä¸€ä¸ªå®Œå…¨ä¸åŒçš„æ•°å­¦æ¨¡å‹ï¼‰ã€‚ä½ å°†åœ¨æœ¬ç« ä¸­çœ‹åˆ°è¿™ç§æƒ…å†µçš„ä¾‹å­ã€‚
- en: In the next section, you will be looking at how to set a hyperparameter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•è®¾ç½®è¶…å‚æ•°ã€‚
- en: Setting Hyperparameters
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®è¶…å‚æ•°
- en: In *Chapter 7*, *The Generalization of Machine Learning Models*, you were introduced
    to the k-NN model for classification and you saw how varying k, the number of
    nearest neighbors, resulted in changes in model performance with respect to the
    prediction of class labels. Here, k is a hyperparameter, and the act of manually
    trying different values of k is a simple form of hyperparameter tuning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*ç¬¬7ç« *ï¼Œ*æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–*ä¸­ï¼Œä½ æ¥è§¦åˆ°äº†ç”¨äºåˆ†ç±»çš„ k-NN æ¨¡å‹ï¼Œå¹¶ä¸”ä½ çœ‹åˆ°äº†éšç€ kï¼ˆæœ€è¿‘é‚»çš„æ•°é‡ï¼‰çš„å˜åŒ–ï¼Œæ¨¡å‹æ€§èƒ½åœ¨é¢„æµ‹ç±»åˆ«æ ‡ç­¾æ—¶å‘ç”Ÿäº†å˜åŒ–ã€‚åœ¨è¿™é‡Œï¼Œk
    æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œæ‰‹åŠ¨å°è¯•ä¸åŒçš„ k å€¼å°±æ˜¯è¶…å‚æ•°è°ƒæ•´çš„ä¸€ä¸ªç®€å•å½¢å¼ã€‚
- en: Each time you initialize a scikit-learn estimator, it will take on a hyperparameterization
    as determined by the values you set for its arguments. If you specify no values,
    then the estimator will take on a default hyperparameterization. If you would
    like to see how the hyperparameters have been set for your estimator, and what
    hyperparameters you can adjust, simply print the output of the `estimator.get_params()`
    method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡åˆå§‹åŒ–ä¸€ä¸ª scikit-learn ä¼°è®¡å™¨æ—¶ï¼Œå®ƒå°†æ ¹æ®ä½ ä¸ºå…¶å‚æ•°è®¾ç½®çš„å€¼æ¥é‡‡ç”¨è¶…å‚æ•°åŒ–ã€‚å¦‚æœä½ æ²¡æœ‰æŒ‡å®šä»»ä½•å€¼ï¼Œé‚£ä¹ˆä¼°è®¡å™¨å°†é‡‡ç”¨é»˜è®¤çš„è¶…å‚æ•°åŒ–ã€‚å¦‚æœä½ æƒ³æŸ¥çœ‹ä¼°è®¡å™¨çš„è¶…å‚æ•°è®¾ç½®ä»¥åŠå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ï¼Œåªéœ€æ‰“å°`estimator.get_params()`æ–¹æ³•çš„è¾“å‡ºã€‚
- en: 'For instance, say we initialize a k-NN estimator without specifying any arguments
    (empty brackets). To see the default hyperparameterization, we can run:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸ª k-NN ä¼°è®¡å™¨ä½†æœªæŒ‡å®šä»»ä½•å‚æ•°ï¼ˆç©ºæ‹¬å·ï¼‰ã€‚è¦æŸ¥çœ‹é»˜è®¤çš„è¶…å‚æ•°è®¾ç½®ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œï¼š
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should get the following output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºï¼š
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A dictionary of all the hyperparameters is now printed to the screen, revealing
    their default settings. Notice `k`, our number of nearest neighbors, is set to
    `5`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ‰€æœ‰è¶…å‚æ•°çš„å­—å…¸å·²æ‰“å°åˆ°å±å¹•ä¸Šï¼Œæ˜¾ç¤ºå®ƒä»¬çš„é»˜è®¤è®¾ç½®ã€‚æ³¨æ„ï¼Œ`k`ï¼Œå³æˆ‘ä»¬æœ€è¿‘é‚»å±…çš„æ•°é‡ï¼Œè®¾ç½®ä¸º `5`ã€‚
- en: To get more information as to what these parameters mean, how they can be changed,
    and what their likely effect may be, you can run the following command and view
    the help file for the estimator in question.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è‹¥è¦äº†è§£è¿™äº›å‚æ•°çš„å«ä¹‰ï¼Œå¦‚ä½•æ›´æ”¹å®ƒä»¬ä»¥åŠå®ƒä»¬å¯èƒ½å¸¦æ¥çš„æ•ˆæœï¼Œä½ å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤å¹¶æŸ¥çœ‹ç›¸å…³ä¼°è®¡å™¨çš„å¸®åŠ©æ–‡ä»¶ã€‚
- en: 'For our k-NN estimator:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ k-NN ä¼°è®¡å™¨ï¼š
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output will be as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Figure 8.3: Help file for the k-NN estimator'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.3ï¼šk-NN ä¼°è®¡å™¨çš„å¸®åŠ©æ–‡ä»¶'
- en: '](img/B15019_08_03.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_03.jpg)'
- en: 'Figure 8.3: Help file for the k-NN estimator'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.3ï¼šk-NN ä¼°è®¡å™¨çš„å¸®åŠ©æ–‡ä»¶
- en: If you look closely at the help file, you will see the default hyperparameterization
    for the estimator under the `String form` heading, along with an explanation of
    what each hyperparameter means under the `Parameters` heading.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»”ç»†æŸ¥çœ‹å¸®åŠ©æ–‡ä»¶ï¼Œä½ ä¼šçœ‹åˆ°åœ¨ `String form` æ ‡é¢˜ä¸‹åˆ—å‡ºäº†ä¼°è®¡å™¨çš„é»˜è®¤è¶…å‚æ•°åŒ–ï¼Œå¹¶ä¸”åœ¨ `Parameters` æ ‡é¢˜ä¸‹æœ‰å¯¹æ¯ä¸ªè¶…å‚æ•°å«ä¹‰çš„è§£é‡Šã€‚
- en: 'Coming back to our example, if we want to change the hyperparameterization
    from `k = 5` to `k = 15`, just re-initialize the estimator and set the `n_neighbors`
    argument to `15`, which will override the default:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°æˆ‘ä»¬çš„ç¤ºä¾‹ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å°†è¶…å‚æ•°ä» `k = 5` æ›´æ”¹ä¸º `k = 15`ï¼Œåªéœ€é‡æ–°åˆå§‹åŒ–ä¼°è®¡å™¨å¹¶å°† `n_neighbors` å‚æ•°è®¾ç½®ä¸º `15`ï¼Œè¿™å°†è¦†ç›–é»˜è®¤è®¾ç½®ï¼š
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should get the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºï¼š
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You may have noticed that k is not the only hyperparameter available for k-NN
    classifiers. Setting multiple hyperparameters is as easy as specifying the relevant
    arguments. For example, let''s increase the number of neighbors from `5` to `15`
    and force the algorithm to take the distance of points in the neighborhood, rather
    than a simple majority vote, into account when training. For more information,
    see the description for the `weights` argument in the help file (`?knn`):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œk å¹¶ä¸æ˜¯ k-NN åˆ†ç±»å™¨å”¯ä¸€çš„è¶…å‚æ•°ã€‚è®¾ç½®å¤šä¸ªè¶…å‚æ•°å°±åƒæŒ‡å®šç›¸å…³å‚æ•°ä¸€æ ·ç®€å•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†é‚»å±…æ•°ä» `5` å¢åŠ åˆ° `15`ï¼Œå¹¶å¼ºåˆ¶ç®—æ³•åœ¨è®­ç»ƒæ—¶è€ƒè™‘é‚»åŸŸä¸­ç‚¹çš„è·ç¦»ï¼Œè€Œéç®€å•çš„å¤šæ•°æŠ•ç¥¨ã€‚æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…å¸®åŠ©æ–‡ä»¶ä¸­
    `weights` å‚æ•°çš„è¯´æ˜ï¼ˆ`?knn`ï¼‰ï¼š
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output will be as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the output, you can see `n_neighbors` (`k`) is now set to `15`, and `weights`
    is now set to `distance`, rather than `uniform`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ° `n_neighbors`ï¼ˆå³ `k`ï¼‰ç°åœ¨è¢«è®¾ç½®ä¸º `15`ï¼Œè€Œ `weights` è®¾ç½®ä¸º `distance`ï¼Œè€Œé `uniform`ã€‚
- en: Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The code for this section can be found at [https://packt.live/2tN5CH1](https://packt.live/2tN5CH1).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä»£ç å¯ä»¥åœ¨ [https://packt.live/2tN5CH1](https://packt.live/2tN5CH1) æ‰¾åˆ°ã€‚
- en: A Note on Defaults
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºé»˜è®¤å€¼çš„è¯´æ˜
- en: Generally, efforts have been made by the developers of machine learning libraries
    to set sensible default hyperparameters for estimators. That said, for certain
    datasets, significant performance improvements may be achieved through tuning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œæœºå™¨å­¦ä¹ åº“çš„å¼€å‘äººå‘˜å·²å°½åŠ›ä¸ºä¼°è®¡å™¨è®¾ç½®åˆç†çš„é»˜è®¤è¶…å‚æ•°ã€‚ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºæŸäº›æ•°æ®é›†ï¼Œé€šè¿‡è°ƒä¼˜å¯èƒ½ä¼šè·å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚
- en: Finding the Best Hyperparameterization
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯»æ‰¾æœ€ä½³è¶…å‚æ•°è®¾ç½®
- en: The best hyperparameterization depends on your overall objective in building
    a machine learning model in the first place. In most cases, this is to find the
    model that has the highest predictive performance on unseen data, as measured
    by its ability to correctly label data points (classification) or predict a number
    (regression).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³çš„è¶…å‚æ•°è®¾ç½®å–å†³äºä½ æœ€åˆæ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ•´ä½“ç›®æ ‡ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™å°±æ˜¯æ‰¾åˆ°å¯¹æœªè§æ•°æ®å…·æœ‰æœ€é«˜é¢„æµ‹æ€§èƒ½çš„æ¨¡å‹ï¼Œé€šå¸¸é€šè¿‡å…¶æ­£ç¡®æ ‡æ³¨æ•°æ®ç‚¹ï¼ˆåˆ†ç±»ï¼‰æˆ–é¢„æµ‹æ•°å­—ï¼ˆå›å½’ï¼‰çš„èƒ½åŠ›æ¥è¡¡é‡ã€‚
- en: The prediction of unseen data can be simulated using hold-out test sets or cross-validation,
    the former being the method used in this chapter. Performance is evaluated differently
    in each case, for instance, **Mean Squared Error** (**MSE**) for regression and
    accuracy for classification. We seek to reduce the MSE or increase the accuracy
    of our predictions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æœªè§æ•°æ®çš„é¢„æµ‹å¯ä»¥é€šè¿‡ä¿ç•™æµ‹è¯•é›†æˆ–äº¤å‰éªŒè¯æ¥æ¨¡æ‹Ÿï¼Œæœ¬ç« ä½¿ç”¨çš„æ˜¯å‰è€…æ–¹æ³•ã€‚æ¯ç§æƒ…å†µçš„æ€§èƒ½è¯„ä¼°æ–¹å¼ä¸åŒï¼Œä¾‹å¦‚ï¼Œå›å½’ä½¿ç”¨**å‡æ–¹è¯¯å·®**ï¼ˆ**MSE**ï¼‰ï¼Œåˆ†ç±»åˆ™ä½¿ç”¨å‡†ç¡®åº¦ã€‚æˆ‘ä»¬åŠ›æ±‚é™ä½MSEæˆ–æé«˜é¢„æµ‹çš„å‡†ç¡®åº¦ã€‚
- en: Let's implement manual hyperparameterization in the following exercise.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ä»¥ä¸‹ç»ƒä¹ ä¸­å®ç°æ‰‹åŠ¨è¶…å‚æ•°è°ƒä¼˜ã€‚
- en: 'Exercise 8.01: Manual Hyperparameter Tuning for a k-NN Classifier'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 8.01ï¼šæ‰‹åŠ¨è°ƒä¼˜k-NNåˆ†ç±»å™¨çš„è¶…å‚æ•°
- en: In this exercise, we will manually tune a k-NN classifier, which was covered
    in *Chapter 7, The Generalization of Machine Learning Models*, our goal being
    to predict incidences of malignant or benign breast cancer based on cell measurements
    sourced from the affected breast sample.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†æ‰‹åŠ¨è°ƒä¼˜ä¸€ä¸ªk-NNåˆ†ç±»å™¨ï¼Œè¯¥åˆ†ç±»å™¨åœ¨*ç¬¬7ç« ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–*ä¸­ä»‹ç»ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŸºäºæ¥è‡ªå—å½±å“ä¹³è…ºæ ·æœ¬çš„ç»†èƒæµ‹é‡å€¼é¢„æµ‹æ¶æ€§æˆ–è‰¯æ€§ä¹³è…ºç™Œçš„å‘ç”Ÿã€‚
- en: Note
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The dataset to be used in this exercise can be found on our GitHub repository
    at [https://packt.live/36dsxIF](https://packt.live/36dsxIF).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç»ƒä¹ ä¸­ä½¿ç”¨çš„æ•°æ®é›†å¯ä»¥åœ¨æˆ‘ä»¬çš„GitHubä»“åº“æ‰¾åˆ°ï¼Œç½‘å€ä¸º[https://packt.live/36dsxIF](https://packt.live/36dsxIF)ã€‚
- en: 'These are the important attributes of the dataset:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯æ•°æ®é›†çš„é‡è¦å±æ€§ï¼š
- en: ID number
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IDå·ç 
- en: Diagnosis (M = malignant, B = benign)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯Šæ–­ï¼ˆM = æ¶æ€§ï¼ŒB = è‰¯æ€§ï¼‰
- en: 3-32)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-32)
- en: '10 real-valued features are computed for each cell nucleus as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ¯ä¸ªç»†èƒæ ¸è®¡ç®—10ä¸ªå®æ•°å€¼ç‰¹å¾ï¼Œè®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š
- en: Radius (mean of distances from the center to points on the perimeter)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠå¾„ï¼ˆä»ä¸­å¿ƒåˆ°å‘¨é•¿ä¸Šç‚¹çš„å¹³å‡è·ç¦»ï¼‰
- en: Texture (standard deviation of grayscale values)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çº¹ç†ï¼ˆç°åº¦å€¼çš„æ ‡å‡†å·®ï¼‰
- en: Perimeter
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‘¨é•¿
- en: Area
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢ç§¯
- en: Smoothness (local variation in radius lengths)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹³æ»‘åº¦ï¼ˆå±€éƒ¨åŠå¾„é•¿åº¦çš„å˜åŒ–ï¼‰
- en: Compactness (perimeter^2 / area - 1.0)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç´§å‡‘æ€§ï¼ˆå‘¨é•¿^2 / é¢ç§¯ - 1.0ï¼‰
- en: Concavity (severity of concave portions of the contour)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡¹åº¦ï¼ˆè½®å»“å‡¹éƒ¨åˆ†çš„ä¸¥é‡ç¨‹åº¦ï¼‰
- en: Concave points (number of concave portions of the contour)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡¹ç‚¹ï¼ˆè½®å»“ä¸­å‡¹éƒ¨åˆ†çš„æ•°é‡ï¼‰
- en: Symmetry
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹ç§°æ€§
- en: Fractal dimension (refers to the complexity of the tissue architecture; "coastline
    approximation" - 1)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†å½¢ç»´åº¦ï¼ˆæŒ‡çš„æ˜¯ç»„ç»‡ç»“æ„çš„å¤æ‚æ€§ï¼›â€œæµ·å²¸çº¿è¿‘ä¼¼â€ - 1ï¼‰
- en: Note
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: Details on the attributes of the dataset can be found at [https://packt.live/30HzGQ6](https://packt.live/30HzGQ6).
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ•°æ®é›†çš„å±æ€§è¯¦æƒ…å¯ä»¥åœ¨[https://packt.live/30HzGQ6](https://packt.live/30HzGQ6)æ‰¾åˆ°ã€‚
- en: 'The following steps will help you complete this exercise:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ­¥éª¤å°†å¸®åŠ©ä½ å®Œæˆæœ¬ç»ƒä¹ ï¼š
- en: Create a new notebook in Google Colab.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨Google Colabä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„ç¬”è®°æœ¬ã€‚
- en: 'Next, import `neighbors`, `datasets`, and `model_selection` from scikit-learn:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä»scikit-learnå¯¼å…¥`neighbors`ã€`datasets`å’Œ`model_selection`ï¼š
- en: '[PRE7]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Load the data. We will call this object `cancer`, and isolate the target `y`,
    and the features, `X`:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®ã€‚æˆ‘ä»¬å°†è¿™ä¸ªå¯¹è±¡å‘½åä¸º`cancer`ï¼Œå¹¶éš”ç¦»ç›®æ ‡`y`å’Œç‰¹å¾`X`ï¼š
- en: '[PRE8]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Initialize a k-NN classifier with its default hyperparameterization:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ä¸€ä¸ªk-NNåˆ†ç±»å™¨ï¼Œä½¿ç”¨å…¶é»˜è®¤çš„è¶…å‚æ•°è®¾ç½®ï¼š
- en: '[PRE9]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Feed this classifier into a 10-fold cross-validation (`cv`), calculating the
    precision score for each fold. Assume that maximizing precision (the proportion
    of true positives in all positive classifications) is the primary objective of
    this exercise:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è¿™ä¸ªåˆ†ç±»å™¨è¾“å…¥åˆ°10æŠ˜äº¤å‰éªŒè¯ï¼ˆ`cv`ï¼‰ä¸­ï¼Œè®¡ç®—æ¯ä¸€æŠ˜çš„ç²¾ç¡®åº¦åˆ†æ•°ã€‚å‡è®¾æœ€å¤§åŒ–ç²¾ç¡®åº¦ï¼ˆåœ¨æ‰€æœ‰æ­£åˆ†ç±»ä¸­çœŸæ­£ä¾‹çš„æ¯”ä¾‹ï¼‰æ˜¯æœ¬ç»ƒä¹ çš„ä¸»è¦ç›®æ ‡ï¼š
- en: '[PRE10]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Printing `cv` shows the precision score calculated for each fold:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰“å°`cv`æ˜¾ç¤ºæ¯ä¸€æŠ˜è®¡ç®—å¾—åˆ°çš„ç²¾ç¡®åº¦åˆ†æ•°ï¼š
- en: '[PRE11]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You will see the following output:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ å°†çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºï¼š
- en: '[PRE12]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Calculate and print the mean precision score for all folds. This will give
    us an idea of the overall performance of the model, as shown in the following
    code snippet:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—å¹¶æ‰“å°æ‰€æœ‰æŠ˜çš„å¹³å‡ç²¾ç¡®åº¦åˆ†æ•°ã€‚è¿™å°†ç»™æˆ‘ä»¬ä¸€ä¸ªæ¨¡å‹æ•´ä½“è¡¨ç°çš„æ¦‚å¿µï¼Œå…·ä½“å¦‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ‰€ç¤ºï¼š
- en: '[PRE13]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You should get the following output:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥ä¼šå¾—åˆ°ä»¥ä¸‹è¾“å‡ºï¼š
- en: '[PRE14]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You should see the mean score is close to 94%. Can this be improved upon?
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥ä¼šçœ‹åˆ°å¹³å‡åˆ†æ•°æ¥è¿‘94%ã€‚èƒ½å¦è¿›ä¸€æ­¥æé«˜ï¼Ÿ
- en: 'Run everything again, this time setting hyperparameter `k` to `15`. You can
    see that the result is actually marginally worse (1% lower):'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å†æ¬¡è¿è¡Œæ‰€æœ‰å†…å®¹ï¼Œè¿™æ¬¡å°†è¶…å‚æ•°`k`è®¾ç½®ä¸º`15`ã€‚ä½ ä¼šçœ‹åˆ°ç»“æœå®é™…ä¸Šç¨å¾®æ›´å·®ï¼ˆä½äº†1%ï¼‰ï¼š
- en: '[PRE15]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output will be as follows:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE16]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Try again with `k` = `7`, `3`, and `1`. In this case, it seems reasonable that
    the default value of 5 is the best option. To avoid repetition, you may like to
    define and call a Python function as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å†å°è¯•ä¸€æ¬¡ï¼Œ`k` = `7`ã€`3` å’Œ `1`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé»˜è®¤å€¼5ä¼¼ä¹æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚ä¸ºäº†é¿å…é‡å¤ï¼Œä½ å¯ä»¥å®šä¹‰å¹¶è°ƒç”¨ä¸€ä¸ªPythonå‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE17]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output will be as follows:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE18]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Nothing beats 94%.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ²¡æœ‰ä»€ä¹ˆèƒ½è¶…è¿‡94%ã€‚
- en: 'Let''s alter a second hyperparameter. Setting `k = 5`, what happens if we change
    the k-NN weighing system to depend on `distance` rather than having `uniform`
    weights? Run all code again, this time with the following hyperparameterization:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¥æ”¹å˜ç¬¬äºŒä¸ªè¶…å‚æ•°ã€‚å°†`k = 5`ï¼Œå¦‚æœæˆ‘ä»¬æŠŠk-NNçš„åŠ æƒç³»ç»Ÿä»`uniform`æ”¹ä¸ºåŸºäº`distance`ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿé‡æ–°è¿è¡Œæ‰€æœ‰ä»£ç ï¼Œè¿™æ¬¡ä½¿ç”¨ä»¥ä¸‹è¶…å‚æ•°è®¾ç½®ï¼š
- en: '[PRE19]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Did performance improve?
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ€§èƒ½æœ‰æå‡å—ï¼Ÿ
- en: 'You should see no further improvement on the default hyperparameterization
    because the output is:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºé»˜è®¤çš„è¶…å‚æ•°è®¾ç½®ï¼Œä½ ä¸åº”çœ‹åˆ°è¿›ä¸€æ­¥çš„æå‡ï¼Œå› ä¸ºè¾“å‡ºç»“æœæ˜¯ï¼š
- en: '[PRE20]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We therefore conclude that the default hyperparameterization is the optimal
    one in this case.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé»˜è®¤çš„è¶…å‚æ•°è®¾ç½®æ˜¯æœ€ä¼˜çš„ã€‚
- en: Note
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: To access the source code for this specific section, please refer to [https://packt.live/322lWk4](https://packt.live/322lWk4).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¿é—®æ­¤éƒ¨åˆ†çš„æºä»£ç ï¼Œè¯·å‚é˜… [https://packt.live/322lWk4](https://packt.live/322lWk4)ã€‚
- en: You can also run this example online at [https://packt.live/3gbOyfU](https://packt.live/3gbOyfU).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥åœ¨çº¿è¿è¡Œè¿™ä¸ªç¤ºä¾‹ï¼Œç½‘å€ä¸º [https://packt.live/3gbOyfU](https://packt.live/3gbOyfU)ã€‚
- en: Advantages and Disadvantages of a Manual Search
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨æœç´¢çš„ä¼˜ç¼ºç‚¹
- en: Of all the strategies for hyperparameter tuning, the manual process gives you
    the most control. As you go through the process, you can get a feel for how your
    estimators might perform under different hyperparameterizations, and this means
    you can adjust them in line with your expectations without having to try a large
    number of possibilities unnecessarily. However, this strategy is feasible only
    when there is a small number of possibilities you would like to try. When the
    number of possibilities exceeds about five, this strategy becomes too labor-intensive
    to be practical.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰çš„è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥ä¸­ï¼Œæ‰‹åŠ¨è¿‡ç¨‹ç»™äº†ä½ æœ€å¤šçš„æ§åˆ¶æƒã€‚é€šè¿‡è¿™ä¸ªè¿‡ç¨‹ï¼Œä½ å¯ä»¥æ„Ÿå—åˆ°ä¸åŒè¶…å‚æ•°è®¾ç½®ä¸‹ï¼Œä¼°è®¡å™¨çš„è¡¨ç°å¦‚ä½•ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„æœŸæœ›è¿›è¡Œè°ƒæ•´ï¼Œè€Œæ— éœ€ä¸å¿…è¦åœ°å°è¯•å¤§é‡çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œè¿™ä¸ªç­–ç•¥åªæœ‰åœ¨ä½ æƒ³å°è¯•çš„å¯èƒ½æ€§è¾ƒå°‘æ—¶æ‰å¯è¡Œã€‚å½“å¯èƒ½æ€§è¶…è¿‡å¤§çº¦äº”ä¸ªæ—¶ï¼Œè¿™ä¸ªç­–ç•¥å°±å˜å¾—è¿‡äºç¹çï¼Œæ— æ³•å®é™…åº”ç”¨ã€‚
- en: In the following sections, we will introduce two strategies to better deal with
    this situation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„å‡ èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸¤ç§ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°å¤„ç†è¿™ç§æƒ…å†µã€‚
- en: Tuning Using Grid Search
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œè°ƒä¼˜
- en: In the context of machine learning, grid search refers to a strategy of systematically
    testing out every hyperparameterization from a pre-defined set of possibilities
    for your chosen estimator. You decide the criteria used to evaluate performance,
    and once the search is complete, you may manually examine the results and choose
    the best hyperparameterization, or let your computer automatically choose it for
    you.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œç½‘æ ¼æœç´¢æŒ‡çš„æ˜¯ä¸€ç§ç­–ç•¥ï¼Œå³ç³»ç»Ÿåœ°æµ‹è¯•ä»é¢„å®šä¹‰çš„è¶…å‚æ•°å¯èƒ½æ€§é›†åˆä¸­ï¼Œæ¯ä¸€ä¸ªè¶…å‚æ•°è®¾ç½®ã€‚ä½ å†³å®šç”¨äºè¯„ä¼°æ€§èƒ½çš„æ ‡å‡†ï¼Œæœç´¢å®Œæˆåï¼Œä½ å¯ä»¥æ‰‹åŠ¨æ£€æŸ¥ç»“æœå¹¶é€‰æ‹©æœ€ä½³çš„è¶…å‚æ•°è®¾ç½®ï¼Œæˆ–è€…è®©è®¡ç®—æœºä¸ºä½ è‡ªåŠ¨é€‰æ‹©ã€‚
- en: The overall objective is to try and find an optimal hyperparameterization that
    leads to improved performance when predicting unseen data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“ç›®æ ‡æ˜¯å°è¯•æ‰¾åˆ°ä¸€ä¸ªæœ€ä½³çš„è¶…å‚æ•°è®¾ç½®ï¼Œä»è€Œåœ¨é¢„æµ‹æœªè§æ•°æ®æ—¶æé«˜æ€§èƒ½ã€‚
- en: Before we get to the implementations of grid search in scikit-learn, let's first
    demonstrate the strategy using simple Python `for` loops.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å¼€å§‹ä»‹ç»scikit-learnä¸­çš„ç½‘æ ¼æœç´¢å®ç°ä¹‹å‰ï¼Œå…ˆç”¨ç®€å•çš„Python `for`å¾ªç¯æ¼”ç¤ºè¯¥ç­–ç•¥ã€‚
- en: Simple Demonstration of the Grid Search Strategy
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç½‘æ ¼æœç´¢ç­–ç•¥çš„ç®€å•æ¼”ç¤º
- en: In the following demonstration of the grid search strategy, we will use the
    breast cancer prediction dataset we saw in *Exercise 8.01*, *Manual Hyperparameter
    Tuning for a k-NN Classifier*, where we manually tuned the hyperparameters of
    the k-NN classifier to optimize for the precision of cancer predictions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç½‘æ ¼æœç´¢ç­–ç•¥æ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬åœ¨*ç»ƒä¹ 8.01*ä¸­çœ‹åˆ°çš„ä¹³è…ºç™Œé¢„æµ‹æ•°æ®é›†ï¼Œ*æ‰‹åŠ¨è°ƒèŠ‚k-NNåˆ†ç±»å™¨çš„è¶…å‚æ•°*ï¼Œå½“æ—¶æˆ‘ä»¬æ‰‹åŠ¨è°ƒæ•´äº†k-NNåˆ†ç±»å™¨çš„è¶…å‚æ•°ï¼Œä»¥ä¼˜åŒ–ç™Œç—‡é¢„æµ‹çš„ç²¾ç¡®åº¦ã€‚
- en: This time, instead of manually fitting models with different values of `k` we
    just define the `k` values we would like to try, that is, `k = 1, 3, 5, 7` in
    a Python dictionary. This dictionary will be the grid we will search through to
    find the optimal hyperparameterization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬ä¸å†æ‰‹åŠ¨æ‹Ÿåˆå…·æœ‰ä¸åŒ`k`å€¼çš„æ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡Pythonå­—å…¸å®šä¹‰æˆ‘ä»¬å¸Œæœ›å°è¯•çš„`k`å€¼ï¼Œå³`k = 1, 3, 5, 7`ã€‚è¿™ä¸ªå­—å…¸å°†ä½œä¸ºæˆ‘ä»¬è¿›è¡Œç½‘æ ¼æœç´¢çš„åŸºç¡€ï¼Œå¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°æœ€ä½³è¶…å‚æ•°è®¾ç½®ã€‚
- en: Note
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The code for this section can be found at [https://packt.live/2U1Y0Li](https://packt.live/2U1Y0Li).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬éƒ¨åˆ†ä»£ç å¯ä»¥åœ¨[https://packt.live/2U1Y0Li](https://packt.live/2U1Y0Li)æ‰¾åˆ°ã€‚
- en: 'The code will be as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç å¦‚ä¸‹ï¼š
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the code snippet, we have used a dictionary `{}` and set the `k` values in
    a Python dictionary.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå­—å…¸`{}`å¹¶å°†`k`å€¼è®¾ç½®åœ¨Pythonå­—å…¸ä¸­ã€‚
- en: In the next part of the code snippet, to conduct the search, we iterate through
    the grid, fitting a model for each value of `k`, each time evaluating the model
    through 10-fold cross-validation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»£ç ç‰‡æ®µçš„ä¸‹ä¸€éƒ¨åˆ†ï¼Œä¸ºäº†è¿›è¡Œæœç´¢ï¼Œæˆ‘ä»¬é€šè¿‡éå†ç½‘æ ¼æ¥æ‹Ÿåˆæ¯ä¸ª`k`å€¼çš„æ¨¡å‹ï¼Œå¹¶æ¯æ¬¡é€šè¿‡10æŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹ã€‚
- en: 'At the end of each iteration, we extract, format, and report back the mean
    precision score after cross-validation via the `print` method:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯æ¬¡è¿­ä»£ç»“æŸæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡`print`æ–¹æ³•æå–ã€æ ¼å¼åŒ–å¹¶æŠ¥å‘Šäº¤å‰éªŒè¯åçš„å¹³å‡ç²¾åº¦å¾—åˆ†ï¼š
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output will be as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '![Figure 8.4: Average precisions for all folds'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.4ï¼šæ‰€æœ‰æŠ˜å çš„å¹³å‡ç²¾åº¦](img/B15019_08_05.jpg)'
- en: '](img/B15019_08_04.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_04.jpg)'
- en: 'Figure 8.4: Average precisions for all folds'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.4ï¼šæ‰€æœ‰æŠ˜å çš„å¹³å‡ç²¾åº¦
- en: We can see from the output that `k = 5` is the best hyperparameterization found,
    with a mean precision score of roughly 94%. Increasing `k` to `7` didn't significantly
    improve performance. It is important to note that the only parameter we are changing
    here is k and that each time the k-NN estimator is initialized, it is done with
    the remaining hyperparameters set to their default values.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¾“å‡ºä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ`k = 5`æ˜¯æ‰¾åˆ°çš„æœ€ä½³è¶…å‚æ•°åŒ–ï¼Œå¹³å‡ç²¾åº¦å¤§çº¦ä¸º94%ã€‚å°†`k`å¢åŠ åˆ°`7`å¹¶æ²¡æœ‰æ˜¾è‘—æ”¹å–„æ€§èƒ½ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¿™é‡Œåªæ›´æ”¹äº†`k`è¿™ä¸ªå‚æ•°ï¼Œæ¯æ¬¡åˆå§‹åŒ–k-NNä¼°ç®—å™¨æ—¶ï¼Œå…¶ä»–è¶…å‚æ•°éƒ½ä¿æŒé»˜è®¤å€¼ã€‚
- en: 'To make this point clear, we can run the same loop, this time just printing
    the hyperparameterization that will be tried:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¸…æ™°åœ°è¡¨è¾¾è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œç›¸åŒçš„å¾ªç¯ï¼Œè¿™æ¬¡ä»…æ‰“å°å‡ºå°†è¦å°è¯•çš„è¶…å‚æ•°è®¾ç½®ï¼š
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output will be as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can see from the output that the only parameter we are changing is k; everything
    else remains the same in each iteration.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¾“å‡ºä¸­ä½ å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬å”¯ä¸€æ”¹å˜çš„å‚æ•°æ˜¯`k`ï¼Œæ¯æ¬¡è¿­ä»£ä¸­çš„å…¶ä»–è®¾ç½®éƒ½ä¿æŒä¸å˜ã€‚
- en: Simple, single-loop structures are fine for a grid search of a single hyperparameter,
    but what if we would like to try a second one? Remember that for k-NN we also
    have weights that can take values `uniform` or `distance`, the choice of which
    influences how k-NN learns how to classify points.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå•ä¸€è¶…å‚æ•°çš„ç½‘æ ¼æœç´¢ï¼Œç®€å•çš„å•å¾ªç¯ç»“æ„æ˜¯å¯ä»¥çš„ï¼Œä½†å¦‚æœæˆ‘ä»¬æƒ³å°è¯•ç¬¬äºŒä¸ªè¶…å‚æ•°å‘¢ï¼Ÿè®°ä½ï¼Œå¯¹äºk-NNï¼Œæˆ‘ä»¬è¿˜æœ‰å¯ä»¥å–å€¼ä¸º`uniform`æˆ–`distance`çš„æƒé‡ï¼Œé€‰æ‹©ä¸åŒçš„æƒé‡ä¼šå½±å“k-NNå¦‚ä½•å­¦ä¹ å¦‚ä½•åˆ†ç±»æ•°æ®ç‚¹ã€‚
- en: 'To proceed, all we need to do is create a dictionary containing both the values
    of k and the weight functions we would like to try as separate key/value pairs:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç»§ç»­ï¼Œæˆ‘ä»¬åªéœ€è¦åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«`k`çš„å€¼ä»¥åŠæˆ‘ä»¬å¸Œæœ›å°è¯•çš„æƒé‡å‡½æ•°ï¼Œä½œä¸ºå•ç‹¬çš„é”®/å€¼å¯¹ï¼š
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output will be as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '![Figure 8.5: Average precision values for all folds for different values of
    k'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.5ï¼šä¸åŒ`k`å€¼çš„æ‰€æœ‰æŠ˜å çš„å¹³å‡ç²¾åº¦å€¼](img/B15019_08_05.jpg)'
- en: '](img/B15019_08_05.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_05.jpg)'
- en: 'Figure 8.5: Average precision values for all folds for different values of
    k'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.5ï¼šä¸åŒ`k`å€¼çš„æ‰€æœ‰æŠ˜å çš„å¹³å‡ç²¾åº¦å€¼
- en: 'You can see that when `k = 5`, the weight function is not based on distance
    and all the other hyperparameters are kept as their default values, and the mean
    precision comes out highest. As we discussed earlier, if you would like to see
    the full set of hyperparameterizations evaluated for k-NN, just add `print(knn.get_params())`
    inside the `for` loop after the estimator is initialized:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°ï¼Œå½“`k = 5`æ—¶ï¼Œæƒé‡å‡½æ•°ä¸åŸºäºè·ç¦»ï¼Œæ‰€æœ‰å…¶ä»–è¶…å‚æ•°ä¿æŒé»˜è®¤å€¼ï¼Œå¹³å‡ç²¾åº¦æœ€é«˜ã€‚å¦‚å‰æ‰€è¿°ï¼Œå¦‚æœä½ æƒ³æŸ¥çœ‹k-NNçš„å®Œæ•´è¶…å‚æ•°åŒ–é›†ï¼Œåªéœ€åœ¨ä¼°ç®—å™¨åˆå§‹åŒ–åï¼Œåœ¨`for`å¾ªç¯ä¸­æ·»åŠ `print(knn.get_params())`ï¼š
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output will be as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This implementation, while great for demonstrating how the grid search process
    works, may not practical when trying to evaluate estimators that have `3`, `4`,
    or even `10` different types of hyperparameters, each with a multitude of possible
    settings.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®ç°è™½ç„¶éå¸¸é€‚åˆå±•ç¤ºç½‘æ ¼æœç´¢è¿‡ç¨‹çš„å·¥ä½œåŸç†ï¼Œä½†åœ¨å°è¯•è¯„ä¼°å…·æœ‰`3`ã€`4`ç”šè‡³`10`ç§ä¸åŒç±»å‹è¶…å‚æ•°ï¼ˆæ¯ç§éƒ½æœ‰å¤§é‡å¯èƒ½è®¾ç½®ï¼‰çš„ä¼°ç®—å™¨æ—¶ï¼Œå¯èƒ½å¹¶ä¸å®ç”¨ã€‚
- en: To carry on in this way will mean writing and keeping track of multiple `for`
    loops, which can be tedious. Thankfully, `scikit-learn`'s `model_selection` module
    gives us a method called `GridSearchCV` that is much more user-friendly. We will
    be looking at this in the topic ahead.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥è¿™ç§æ–¹å¼ç»§ç»­è¿›è¡Œæ„å‘³ç€éœ€è¦ç¼–å†™å’Œè·Ÿè¸ªå¤šä¸ª `for` å¾ªç¯ï¼Œè¿™å¯èƒ½ä¼šå¾ˆç¹çã€‚å¹¸è¿çš„æ˜¯ï¼Œ`scikit-learn` çš„ `model_selection`
    æ¨¡å—æä¾›äº†ä¸€ç§å«åš `GridSearchCV` çš„æ–¹æ³•ï¼Œå®ƒæ›´åŠ ç”¨æˆ·å‹å¥½ã€‚æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„ä¸»é¢˜ä¸­è®¨è®ºè¿™ä¸ªæ–¹æ³•ã€‚
- en: GridSearchCV
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GridSearchCV
- en: '`GridsearchCV` is a method of tuning wherein the model can be built by evaluating
    the combination of parameters mentioned in a grid. In the following figure, we
    will see how `GridSearchCV` is different from manual search and look at grid search
    in a muchdetailed way in a table format.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridsearchCV` æ˜¯ä¸€ç§è°ƒä¼˜æ–¹æ³•ï¼Œé€šè¿‡è¯„ä¼°ç½‘æ ¼ä¸­æåˆ°çš„å‚æ•°ç»„åˆæ¥æ„å»ºæ¨¡å‹ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ° `GridSearchCV` ä¸æ‰‹åŠ¨æœç´¢çš„ä¸åŒï¼Œå¹¶é€šè¿‡è¡¨æ ¼å½¢å¼è¯¦ç»†äº†è§£ç½‘æ ¼æœç´¢ã€‚'
- en: Tuning using GridSearchCV
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ GridSearchCV è¿›è¡Œè°ƒä¼˜
- en: We can conduct a grid search much more easily in practice by leveraging `model_selection.GridSearchCV`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ©ç”¨ `model_selection.GridSearchCV` æ›´è½»æ¾åœ°è¿›è¡Œç½‘æ ¼æœç´¢ã€‚
- en: 'For the sake of comparison, we will use the same breast cancer dataset and
    k-NN classifier as before:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åšå¯¹æ¯”ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¹‹å‰çš„åŒä¸€ä¸ªä¹³è…ºç™Œæ•°æ®é›†å’Œ k-NN åˆ†ç±»å™¨ï¼š
- en: '[PRE28]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The next thing we need to do after loading the data is to initialize the class
    of the estimator we would like to evaluate under different hyperparameterizations:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®åï¼Œæˆ‘ä»¬éœ€è¦åšçš„ä¸‹ä¸€ä»¶äº‹æ˜¯åˆå§‹åŒ–æˆ‘ä»¬å¸Œæœ›åœ¨ä¸åŒè¶…å‚æ•°åŒ–ä¸‹è¯„ä¼°çš„ä¼°è®¡å™¨ç±»ï¼š
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We then define the grid:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å®šä¹‰ç½‘æ ¼ï¼š
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: To set up the search, we pass the freshly initialized estimator and our grid
    of hyperparameters to `model_selection.GridSearchCV()`. We must also specify a
    scoring metric, which is the method that will be used to evaluate the performance
    of the various hyperparameterizations tried during the search.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®¾ç½®æœç´¢ï¼Œæˆ‘ä»¬å°†åˆšåˆå§‹åŒ–çš„ä¼°è®¡å™¨å’Œè¶…å‚æ•°ç½‘æ ¼ä¼ é€’ç»™ `model_selection.GridSearchCV()`ã€‚æˆ‘ä»¬è¿˜å¿…é¡»æŒ‡å®šä¸€ä¸ªè¯„åˆ†æŒ‡æ ‡ï¼Œè¿™ä¸ªæ–¹æ³•å°†ç”¨äºè¯„ä¼°åœ¨æœç´¢è¿‡ç¨‹ä¸­å°è¯•çš„å„ç§è¶…å‚æ•°åŒ–çš„è¡¨ç°ã€‚
- en: 'The last thing to do is set the number splits to be used using cross-validation
    via the `cv` argument. We will set this to `10`, thereby conducting 10-fold cross-validation:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ­¥æ˜¯ä½¿ç”¨äº¤å‰éªŒè¯é€šè¿‡ `cv` å‚æ•°è®¾ç½®å°†è¦ä½¿ç”¨çš„åˆ†å‰²æ•°ã€‚æˆ‘ä»¬å°†å…¶è®¾ç½®ä¸º `10`ï¼Œä»è€Œè¿›è¡Œ 10 æŠ˜äº¤å‰éªŒè¯ï¼š
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The last step is to feed data to this object via its `fit()` method. Once this
    has been done, the grid search process will be kick-started:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ­¥æ˜¯é€šè¿‡ `fit()` æ–¹æ³•å°†æ•°æ®ä¼ é€’ç»™è¿™ä¸ªå¯¹è±¡ã€‚ä¸€æ—¦å®Œæˆï¼Œç½‘æ ¼æœç´¢è¿‡ç¨‹å°†å¯åŠ¨ï¼š
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'By default, information relating to the search will be printed to the screen,
    allowing you to see the exact estimator parameterizations that will be evaluated
    for the k-NN estimator:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œå…³äºæœç´¢çš„ä¿¡æ¯å°†æ‰“å°åˆ°å±å¹•ä¸Šï¼Œå…è®¸ä½ çœ‹åˆ°å°†è¦è¯„ä¼°çš„ k-NN ä¼°è®¡å™¨çš„ç¡®åˆ‡ä¼°è®¡å™¨å‚æ•°åŒ–ï¼š
- en: '![Figure 8.6: Estimator parameterizations for the k-NN estimator'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.6: k-NN ä¼°è®¡å™¨çš„ä¼°è®¡å™¨å‚æ•°åŒ–'
- en: '](img/B15019_08_06.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_06.jpg)'
- en: 'Figure 8.6: Estimator parameterizations for the k-NN estimator'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 8.6: k-NN ä¼°è®¡å™¨çš„ä¼°è®¡å™¨å‚æ•°åŒ–'
- en: Once the search is complete, we can examine the results by accessing and printing
    the `cv_results_` attribute. `cv_results_` is a dictionary containing helpful
    information regarding model performance under each hyperparameterization, such
    as the mean test-set value of your scoring metric (`mean_test_score`, the lower
    the better), the complete list of hyperparameterizations tried (`params`), and
    the model ranks as they relate to the `mean_test_score` (`rank_test_score`).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æœç´¢å®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¿é—®å’Œæ‰“å° `cv_results_` å±æ€§æ¥æŸ¥çœ‹ç»“æœã€‚`cv_results_` æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«å…³äºåœ¨æ¯ä¸ªè¶…å‚æ•°åŒ–ä¸‹æ¨¡å‹è¡¨ç°çš„æœ‰ç”¨ä¿¡æ¯ï¼Œä¾‹å¦‚è¯„åˆ†æŒ‡æ ‡çš„å¹³å‡æµ‹è¯•é›†å€¼ï¼ˆ`mean_test_score`ï¼Œå€¼è¶Šä½è¶Šå¥½ï¼‰ï¼Œå°è¯•çš„æ‰€æœ‰è¶…å‚æ•°åŒ–çš„å®Œæ•´åˆ—è¡¨ï¼ˆ`params`ï¼‰ï¼Œä»¥åŠæ¨¡å‹ä¸
    `mean_test_score` ç›¸å…³çš„æ’åï¼ˆ`rank_test_score`ï¼‰ã€‚
- en: The best model found will have rank = 1, the second-best model will have rank
    = 2, and so on, as you can see in *Figure 8.8*. The model fitting times are reported
    through `mean_fit_time`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹çš„æ’åä¸º 1ï¼Œç¬¬äºŒå¥½çš„æ¨¡å‹æ’åä¸º 2ï¼Œä»¥æ­¤ç±»æ¨ï¼Œæ­£å¦‚ä½ åœ¨ *å›¾ 8.8* ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚æ¨¡å‹æ‹Ÿåˆæ—¶é—´é€šè¿‡ `mean_fit_time`
    æŠ¥å‘Šã€‚
- en: 'Although not usually a consideration for smaller datasets, this value can be
    important because in some cases you may find that a marginal increase in model
    performance through a certain hyperparameterization is associated with a significant
    increase in model fit time, which, depending on the computing resources you have
    available, may render that hyperparameterization infeasible because it will take
    too long to fit:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å¯¹äºè¾ƒå°çš„æ•°æ®é›†é€šå¸¸ä¸æ˜¯ä¸€ä¸ªè€ƒè™‘å› ç´ ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™ä¸ªå€¼å¯èƒ½å¾ˆé‡è¦ï¼Œå› ä¸ºæ‚¨å¯èƒ½ä¼šå‘ç°é€šè¿‡æŸäº›è¶…å‚æ•°åŒ–åœ¨æ¨¡å‹æ€§èƒ½ä¸Šçš„è¾¹é™…å¢åŠ ä¸æ¨¡å‹æ‹Ÿåˆæ—¶é—´çš„æ˜¾è‘—å¢åŠ ç›¸å…³è”ï¼Œè¿™å–å†³äºæ‚¨å¯ç”¨çš„è®¡ç®—èµ„æºï¼Œå¯èƒ½ä¼šå¯¼è‡´è¯¥è¶…å‚æ•°åŒ–å˜å¾—ä¸å¯è¡Œï¼Œå› ä¸ºå®ƒå°†èŠ±è´¹å¤ªé•¿æ—¶é—´æ¥æ‹Ÿåˆï¼š
- en: '[PRE33]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output will be as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Figure 8.7: GridsearchCV results'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.7ï¼šGridsearchCV ç»“æœ'
- en: '](img/B15019_08_07.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_07.jpg)'
- en: 'Figure 8.7: GridsearchCV results'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.7ï¼šGridsearchCV ç»“æœ
- en: 'The model ranks can be seen in the following image:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ’åå¯è§ä»¥ä¸‹å›¾ç‰‡ï¼š
- en: '![Figure 8.8: Model ranks'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.8ï¼šæ¨¡å‹æ’å'
- en: '](img/B15019_08_08.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_08.jpg)'
- en: 'Figure 8.8: Model ranks'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.8ï¼šæ¨¡å‹æ’å
- en: Note
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: 'For the purpose of presentation, the output has been truncated. You can see
    the complete output here: [https://packt.live/2uD12uP](https://packt.live/2uD12uP).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ºäºå±•ç¤ºç›®çš„ï¼Œè¾“å‡ºå·²ç»è¢«æˆªæ–­ã€‚æ‚¨å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºï¼š[https://packt.live/2uD12uP](https://packt.live/2uD12uP)ã€‚
- en: In the output, it is worth noting that this dictionary can be easily transformed
    into a pandas DataFrame, which makes information much clearer to read and allows
    us to selectively display the metrics we are interested in.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºä¸­ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ­¤å­—å…¸å¯ä»¥è½»æ¾è½¬æ¢ä¸º pandas DataFrameï¼Œè¿™æ ·ä½¿å¾—ä¿¡æ¯æ›´åŠ æ¸…æ™°æ˜“è¯»ï¼Œå¹¶å…è®¸æˆ‘ä»¬é€‰æ‹©æ€§åœ°æ˜¾ç¤ºæˆ‘ä»¬æ„Ÿå…´è¶£çš„æŒ‡æ ‡ã€‚
- en: 'For example, say we are only interested in each hyperparameterization (`params`)
    and mean cross-validated test score (`mean_test_score`) for the top five high
    - performing models:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬åªå¯¹å‰äº”ä¸ªè¡¨ç°æœ€ä½³æ¨¡å‹çš„æ¯ä¸ªè¶…å‚æ•°åŒ–ï¼ˆ`params`ï¼‰å’Œå¹³å‡äº¤å‰éªŒè¯æµ‹è¯•åˆ†æ•°ï¼ˆ`mean_test_score`ï¼‰æ„Ÿå…´è¶£ï¼š
- en: '[PRE34]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Running this code produces the following output:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ­¤ä»£ç å°†äº§ç”Ÿä»¥ä¸‹è¾“å‡ºï¼š
- en: '![Figure 8.9: mean_test_score for top 5 models'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.9ï¼šå‰ 5 ä¸ªæ¨¡å‹çš„ mean_test_score'
- en: '](img/B15019_08_09.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_09.jpg)'
- en: 'Figure 8.9: mean_test_score for top 5 models'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.9ï¼šå‰ 5 ä¸ªæ¨¡å‹çš„ mean_test_score
- en: 'We can also use pandas to produce visualizations of the result as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ pandas ç”Ÿæˆä»¥ä¸‹ç»“æœçš„å¯è§†åŒ–ï¼š
- en: '[PRE35]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output will be as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Figure 8.10: Using pandas to visualize the output'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.10ï¼šä½¿ç”¨ pandas å¯è§†åŒ–è¾“å‡º'
- en: '](img/B15019_08_10.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_10.jpg)'
- en: 'Figure 8.10: Using pandas to visualize the output'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.10ï¼šä½¿ç”¨ pandas å¯è§†åŒ–è¾“å‡º
- en: When you look at the preceding figure, you see that the best hyperparameterization
    found is where `n_neighbors = 5` and `weights = 'uniform'`, because this results
    in the highest mean test score (precision).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨æŸ¥çœ‹å‰è¿°å›¾è¡¨æ—¶ï¼Œæ‚¨ä¼šå‘ç°æ‰¾åˆ°çš„æœ€ä½³è¶…å‚æ•°è®¾ç½®æ˜¯ `n_neighbors = 5` å’Œ `weights = 'uniform'`ï¼Œå› ä¸ºè¿™ä¼šäº§ç”Ÿæœ€é«˜çš„å¹³å‡æµ‹è¯•åˆ†æ•°ï¼ˆç²¾åº¦ï¼‰ã€‚
- en: Note
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The code for this section can be found at [https://packt.live/2uD12uP](https://packt.live/2uD12uP).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤éƒ¨åˆ†çš„ä»£ç å¯ä»¥åœ¨ [https://packt.live/2uD12uP](https://packt.live/2uD12uP) æ‰¾åˆ°ã€‚
- en: Support Vector Machine (SVM) Classifiers
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ”¯æŒå‘é‡æœºï¼ˆSupport Vector Machineï¼ŒSVMï¼‰åˆ†ç±»å™¨
- en: The **SVM** classifier is basically a supervised machine learning model. It
    is a commonly used class of estimator that can be used for both binary and multi-class
    classification. It is known to perform well in cases where the data is limited,
    hence it is a reliable model. It is relatively fast to train compared to highly
    iterative or ensemble methods such as artificial neural networks or random forests,
    which makes it a good option if there is a limit on your computer's processing
    power.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVM** åˆ†ç±»å™¨åŸºæœ¬ä¸Šæ˜¯ä¸€ç§ç›‘ç£å­¦ä¹ æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ç§å¸¸ç”¨çš„ä¼°ç®—å™¨ç±»åˆ«ï¼Œå¯ç”¨äºäºŒå…ƒå’Œå¤šç±»åˆ†ç±»ã€‚å®ƒåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ï¼Œå› æ­¤æ˜¯ä¸€ä¸ªå¯é çš„æ¨¡å‹ã€‚ä¸é«˜åº¦è¿­ä»£æˆ–é›†æˆæ–¹æ³•ï¼ˆå¦‚äººå·¥ç¥ç»ç½‘ç»œæˆ–éšæœºæ£®æ—ï¼‰ç›¸æ¯”ï¼Œè®­ç»ƒé€Ÿåº¦ç›¸å¯¹è¾ƒå¿«ï¼Œè¿™ä½¿å…¶æˆä¸ºåœ¨è®¡ç®—æœºå¤„ç†èƒ½åŠ›æœ‰é™çš„æƒ…å†µä¸‹çš„è‰¯å¥½é€‰æ‹©ã€‚'
- en: It makes its predictions by leveraging a special mathematical formulation known
    as a kernel function. This function can take several forms with some functions,
    such as the polynomial kernel function with a degree (squared, cubed, and so on),
    that have their own adjustable parameters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒé€šè¿‡åˆ©ç”¨ä¸€ç§ç§°ä¸ºæ ¸å‡½æ•°çš„ç‰¹æ®Šæ•°å­¦å…¬å¼è¿›è¡Œé¢„æµ‹ã€‚è¿™ä¸ªå‡½æ•°å¯ä»¥é‡‡ç”¨å¤šç§å½¢å¼ï¼Œå…¶ä¸­ä¸€äº›å‡½æ•°ï¼ˆå¦‚å…·æœ‰å…¶è‡ªèº«å¯è°ƒå‚æ•°çš„å¤šé¡¹å¼æ ¸å‡½æ•°ï¼‰ã€‚
- en: SVMs have been shown to perform well in the context of image classification,
    which you will see in the following exercise.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: SVM åœ¨å›¾åƒåˆ†ç±»çš„èƒŒæ™¯ä¸‹è¡¨ç°è‰¯å¥½ï¼Œæ‚¨å°†åœ¨ä»¥ä¸‹ç»ƒä¹ ä¸­çœ‹åˆ°ã€‚
- en: Note
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: For more information on support vector machines, see [https://packt.live/37iDytw](https://packt.live/37iDytw)
    and also refer to [https://packt.live/38xaPkC](https://packt.live/38xaPkC).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ”¯æŒå‘é‡æœºçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[https://packt.live/37iDytw](https://packt.live/37iDytw)ï¼Œå¹¶å‚è€ƒ[https://packt.live/38xaPkC](https://packt.live/38xaPkC)ã€‚
- en: 'Exercise 8.02: Grid Search Hyperparameter Tuning for an SVM'
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'ç»ƒä¹  8.02: æ”¯æŒå‘é‡æœºçš„ç½‘æ ¼æœç´¢è¶…å‚æ•°è°ƒä¼˜'
- en: In this exercise, we will employ a class of estimator called an SVM classifier
    and tune its hyperparameters using a grid search strategy.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§å«åšSVMåˆ†ç±»å™¨çš„ä¼°è®¡å™¨ï¼Œå¹¶é€šè¿‡ç½‘æ ¼æœç´¢ç­–ç•¥æ¥è°ƒæ•´å…¶è¶…å‚æ•°ã€‚
- en: The supervised learning objective we will focus on here is the classification
    of handwritten digits (0-9) based solely on images. The dataset we will use contains
    1,797 labeled images of handwritten digits.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¿™é‡Œå…³æ³¨çš„ç›‘ç£å­¦ä¹ ç›®æ ‡æ˜¯åŸºäºå›¾åƒå¯¹æ‰‹å†™æ•°å­—ï¼ˆ0-9ï¼‰è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„æ•°æ®é›†åŒ…å«1,797ä¸ªæ ‡è®°çš„æ‰‹å†™æ•°å­—å›¾åƒã€‚
- en: Note
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The dataset to be used in this exercise can be found on our GitHub repository
    at [https://packt.live/2vdbHg9](https://packt.live/2vdbHg9).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç»ƒä¹ æ‰€ç”¨çš„æ•°æ®é›†å¯ä»¥åœ¨æˆ‘ä»¬çš„GitHubåº“ä¸­æ‰¾åˆ°ï¼š[https://packt.live/2vdbHg9](https://packt.live/2vdbHg9)ã€‚
- en: 'Details on the attributes of the dataset can be found on the original dataset''s
    URL: [https://packt.live/36cX35b](https://packt.live/36cX35b).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨åŸå§‹æ•°æ®é›†çš„ç½‘å€æ‰¾åˆ°ï¼š[https://packt.live/36cX35b](https://packt.live/36cX35b)ã€‚
- en: Create a new notebook in Google Colab.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨Google Colabä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„ç¬”è®°æœ¬ã€‚
- en: 'Import `datasets`, `svm`, and `model_selection` from scikit-learn:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»scikit-learnä¸­å¯¼å…¥`datasets`ã€`svm`å’Œ`model_selection`ï¼š
- en: '[PRE36]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Load the data. We will call this object images, and then we''ll isolate the
    target `y` and the features `X`. In the training step, the SVM classifier will
    learn how `y` relates to `X` and will therefore be able to predict new `y` values
    when given new `X` values:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®ã€‚æˆ‘ä»¬å°†è¿™ä¸ªå¯¹è±¡å‘½åä¸ºimagesï¼Œç„¶åå°†ç›®æ ‡`y`å’Œç‰¹å¾`X`åˆ†ç¦»ã€‚åœ¨è®­ç»ƒæ­¥éª¤ä¸­ï¼ŒSVMåˆ†ç±»å™¨å°†å­¦ä¹ `y`å¦‚ä½•ä¸`X`ç›¸å…³è”ï¼Œå› æ­¤ï¼Œå½“ç»™å®šæ–°çš„`X`å€¼æ—¶ï¼Œå®ƒèƒ½å¤Ÿé¢„æµ‹æ–°çš„`y`å€¼ï¼š
- en: '[PRE37]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Initialize the estimator as a multi-class SVM classifier and set the `gamma`
    argument to `scale`:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ä¼°è®¡å™¨åˆå§‹åŒ–ä¸ºå¤šç±»SVMåˆ†ç±»å™¨ï¼Œå¹¶å°†`gamma`å‚æ•°è®¾ç½®ä¸º`scale`ï¼š
- en: '[PRE38]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Note
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: For more information on the gamma argument, go to [https://packt.live/2Ga2l79](https://packt.live/2Ga2l79).
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…³äº`gamma`å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®[https://packt.live/2Ga2l79](https://packt.live/2Ga2l79)ã€‚
- en: 'Define our grid to cover four distinct hyperparameterizations of the classifier
    with a linear kernel and with a polynomial kernel of degrees `2`, `3,` and `4`.
    We want to see which of the four hyperparameterizations leads to more accurate
    predictions:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰æˆ‘ä»¬çš„ç½‘æ ¼ï¼Œæ¶µç›–å››ç§ä¸åŒçš„åˆ†ç±»å™¨è¶…å‚æ•°é…ç½®ï¼ŒåŒ…æ‹¬çº¿æ€§æ ¸å’Œå¤šé¡¹å¼æ ¸ï¼ˆåˆ†åˆ«ä¸º`2`ã€`3`å’Œ`4`åº¦ï¼‰ã€‚æˆ‘ä»¬å¸Œæœ›æŸ¥çœ‹å“ªç§è¶…å‚æ•°é…ç½®èƒ½å¤Ÿæä¾›æ›´å‡†ç¡®çš„é¢„æµ‹ï¼š
- en: '[PRE39]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Set up grid search k-fold cross-validation with `10` folds and a scoring measure
    of accuracy. Make sure it has our `grid` and `estimator` objects as inputs:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¾ç½®ç½‘æ ¼æœç´¢kæŠ˜äº¤å‰éªŒè¯ï¼Œä½¿ç”¨`10`æŠ˜å’Œå‡†ç¡®åº¦è¯„åˆ†æ–¹æ³•ã€‚ç¡®ä¿å®ƒçš„è¾“å…¥åŒ…å«æˆ‘ä»¬çš„`grid`å’Œ`estimator`å¯¹è±¡ï¼š
- en: '[PRE40]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Start the search by providing data to the `.fit()` method. Details of the process,
    including the hyperparameterizations tried and the scoring method selected, will
    be printed to the screen:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†æ•°æ®æä¾›ç»™`.fit()`æ–¹æ³•æ¥å¼€å§‹æœç´¢ã€‚è¿‡ç¨‹ä¸­ï¼Œå°†ä¼šæ‰“å°å‡ºè¶…å‚æ•°é…ç½®å°è¯•æƒ…å†µä»¥åŠé€‰æ‹©çš„è¯„åˆ†æ–¹æ³•ï¼š
- en: '[PRE41]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You should see the following output:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºï¼š
- en: '![Figure 8.11: Grid Search using the .fit() method'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![å›¾ 8.11: ä½¿ç”¨.fit()æ–¹æ³•è¿›è¡Œç½‘æ ¼æœç´¢'
- en: '](img/B15019_08_11.jpg)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_11.jpg)'
- en: 'Figure 8.11: Grid Search using the .fit() method'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'å›¾ 8.11: ä½¿ç”¨.fit()æ–¹æ³•è¿›è¡Œç½‘æ ¼æœç´¢'
- en: 'To examine all of the results, simply print `cv_spec.cv_results_` to the screen.
    You will see that the results are structured as a dictionary, allowing you to
    access the information you require using the keys:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¦æŸ¥çœ‹æ‰€æœ‰ç»“æœï¼Œåªéœ€å°†`cv_spec.cv_results_`æ‰“å°åˆ°å±å¹•ä¸Šã€‚ä½ ä¼šçœ‹åˆ°ç»“æœç»“æ„ä¸ºå­—å…¸ï¼Œè¿™æ ·ä½ å°±å¯ä»¥é€šè¿‡é”®è®¿é—®æ‰€éœ€çš„ä¿¡æ¯ï¼š
- en: '[PRE42]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You will see the following information:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ å°†çœ‹åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š
- en: '![Figure 8.12: Results as a dictionary'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![å›¾ 8.12: ç»“æœä½œä¸ºå­—å…¸'
- en: '](img/B15019_08_12.jpg)'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_12.jpg)'
- en: 'Figure 8.12: Results as a dictionary'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'å›¾ 8.12: ç»“æœä½œä¸ºå­—å…¸'
- en: For this exercise, we are primarily concerned with the test-set performance
    of each distinct hyperparameterization. You can see the first hyperparameterization
    through `cv_spec.cv_results_['mean_test_score']`, and the second through `cv_spec.cv_results_['params']`.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªç»ƒä¹ ï¼Œæˆ‘ä»¬ä¸»è¦å…³å¿ƒæ¯ç§ä¸åŒè¶…å‚æ•°é…ç½®åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ã€‚ä½ å¯ä»¥é€šè¿‡`cv_spec.cv_results_['mean_test_score']`æŸ¥çœ‹ç¬¬ä¸€ä¸ªè¶…å‚æ•°é…ç½®ï¼Œé€šè¿‡`cv_spec.cv_results_['params']`æŸ¥çœ‹ç¬¬äºŒä¸ªè¶…å‚æ•°é…ç½®ã€‚
- en: 'Let''s convert the results dictionary to a `pandas` DataFrame and find the
    best hyperparameterization:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç»“æœå­—å…¸è½¬æ¢ä¸º`pandas` DataFrameï¼Œå¹¶æ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°é…ç½®ï¼š
- en: '[PRE43]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You will see the following results:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ å°†çœ‹åˆ°ä»¥ä¸‹ç»“æœï¼š
- en: '![Figure 8.13: Parameterization results'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![å›¾ 8.13ï¼šå‚æ•°åŒ–ç»“æœ'
- en: '](img/B15019_08_13.jpg)'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_13.jpg)'
- en: 'Figure 8.13: Parameterization results'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å›¾ 8.13ï¼šå‚æ•°åŒ–ç»“æœ
- en: Note
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: You may get slightly different results. However, the values you obtain should
    largely agree with those in the preceding output.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šå¾—åˆ°ç•¥æœ‰ä¸åŒçš„ç»“æœã€‚ä¸è¿‡ï¼Œä½ è·å¾—çš„å€¼åº”ä¸å‰é¢çš„è¾“å‡ºå¤§è‡´ä¸€è‡´ã€‚
- en: 'It is best practice to visualize any results you produce. `pandas` makes this
    easy. Run the following code to produce a visualization:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€å¥½çš„åšæ³•æ˜¯å¯è§†åŒ–ä½ æ‰€å¾—åˆ°çš„ä»»ä½•ç»“æœã€‚`pandas` ä½¿è¿™ä¸€è¿‡ç¨‹å˜å¾—ç®€å•ã€‚è¿è¡Œä»¥ä¸‹ä»£ç æ¥ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼š
- en: '[PRE44]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output will be as follows:'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '![Figure 8.14: Using pandas to visualize the results'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![å›¾ 8.14ï¼šä½¿ç”¨ pandas å¯è§†åŒ–ç»“æœ'
- en: '](img/B15019_08_14.jpg)'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_14.jpg)'
- en: 'Figure 8.14: Using pandas to visualize the results'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.14ï¼šä½¿ç”¨ pandas å¯è§†åŒ–ç»“æœ
- en: Note
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: To access the source code for this specific section, please refer to [https://packt.live/36At2MO](https://packt.live/36At2MO).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¿é—®æ­¤ç‰¹å®šéƒ¨åˆ†çš„æºä»£ç ï¼Œè¯·å‚è€ƒ[https://packt.live/36At2MO](https://packt.live/36At2MO)ã€‚
- en: You can also run this example online at [https://packt.live/2YdQsGq](https://packt.live/2YdQsGq).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥åœ¨[https://packt.live/2YdQsGq](https://packt.live/2YdQsGq)ä¸Šåœ¨çº¿è¿è¡Œæ­¤ç¤ºä¾‹ã€‚
- en: We can see that an SVM classifier with a third-degree polynomial kernel function
    has the highest accuracy of all the hyperparameterizations evaluated in our search.
    Feel free to add more hyperparameterizations to the grid and see if you can improve
    on the score.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå…·æœ‰ä¸‰æ¬¡å¤šé¡¹å¼æ ¸å‡½æ•°çš„ SVM åˆ†ç±»å™¨åœ¨æ‰€æœ‰è¯„ä¼°çš„è¶…å‚æ•°ç»„åˆä¸­å…·æœ‰æœ€é«˜çš„å‡†ç¡®ç‡ã€‚ä½ å¯ä»¥éšæ„å‘ç½‘æ ¼ä¸­æ·»åŠ æ›´å¤šè¶…å‚æ•°ç»„åˆï¼Œçœ‹çœ‹æ˜¯å¦èƒ½æé«˜åˆ†æ•°ã€‚
- en: Advantages and Disadvantages of Grid Search
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç½‘æ ¼æœç´¢çš„ä¼˜ç¼ºç‚¹
- en: The primary advantage of the grid search compared to a manual search is that
    it is an automated process that one can simply set and forget. Additionally, you
    have the power to dictate the exact hyperparameterizations evaluated, which can
    be a good thing when you have prior knowledge of what kind of hyperparameterizations
    might work well in your context. It is also easy to understand exactly what will
    happen during the search thanks to the explicit definitions of the grid.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ‰‹åŠ¨æœç´¢ç›¸æ¯”ï¼Œç½‘æ ¼æœç´¢çš„ä¸»è¦ä¼˜ç‚¹åœ¨äºå®ƒæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–è¿‡ç¨‹ï¼Œç”¨æˆ·åªéœ€è®¾ç½®ä¸€æ¬¡å¹¶å¯å¿˜è®°å®ƒã€‚æ­¤å¤–ï¼Œä½ è¿˜å¯ä»¥ç²¾ç¡®æ§åˆ¶æ‰€è¯„ä¼°çš„è¶…å‚æ•°ç»„åˆï¼Œå½“ä½ å¯¹å“ªäº›è¶…å‚æ•°ç»„åˆå¯èƒ½åœ¨ä½ çš„åœºæ™¯ä¸­æœ‰æ•ˆæœ‰å…ˆéªŒçŸ¥è¯†æ—¶ï¼Œè¿™ç§ç²¾ç¡®æ€§æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¼˜åŠ¿ã€‚ç”±äºç½‘æ ¼çš„å®šä¹‰æ˜¯æ˜ç¡®çš„ï¼Œä½ ä¹Ÿå¾ˆå®¹æ˜“ç†è§£æœç´¢è¿‡ç¨‹ä¸­å°†ä¼šå‘ç”Ÿä»€ä¹ˆã€‚
- en: The major drawback of the grid search strategy is that it is computationally
    very expensive; that is, when the number of hyperparameterizations to try increases
    substantially, processing times can be very slow. Also, when you define your grid,
    you may inadvertently omit an hyperparameterization that would in fact be optimal.
    If it is not specified in your grid, it will never be tried
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘æ ¼æœç´¢ç­–ç•¥çš„ä¸»è¦ç¼ºç‚¹æ˜¯è®¡ç®—å¼€é”€éå¸¸å¤§ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“éœ€è¦å°è¯•çš„è¶…å‚æ•°ç»„åˆæ•°é‡å¤§å¹…å¢åŠ æ—¶ï¼Œå¤„ç†æ—¶é—´å¯èƒ½éå¸¸æ…¢ã€‚æ­¤å¤–ï¼Œåœ¨å®šä¹‰ç½‘æ ¼æ—¶ï¼Œä½ å¯èƒ½ä¼šæ— æ„ä¸­é—æ¼ä¸€ä¸ªå®é™…ä¸Šæ˜¯æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆã€‚å¦‚æœå®ƒæ²¡æœ‰å‡ºç°åœ¨ç½‘æ ¼ä¸­ï¼Œå®ƒå°†æ°¸è¿œä¸ä¼šè¢«å°è¯•ã€‚
- en: To overcome these drawbacks, we will be looking at random search in the next
    section.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†æ¢è®¨éšæœºæœç´¢ã€‚
- en: Random Search
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: éšæœºæœç´¢
- en: Instead of searching through every hyperparameterizations in a pre-defined set,
    as is the case with a grid search, in a random search we sample from a distribution
    of possibilities by assuming each hyperparameter to be a random variable. Before
    we go through the process in depth, it will be helpful to briefly review what
    random variables are and what we mean by a distribution.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç½‘æ ¼æœç´¢é€šè¿‡é¢„å®šä¹‰çš„è¶…å‚æ•°é›†åˆé€ä¸€è¿›è¡Œæœç´¢ä¸åŒï¼Œåœ¨éšæœºæœç´¢ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å‡è®¾æ¯ä¸ªè¶…å‚æ•°æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œä»å¯èƒ½æ€§çš„åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚åœ¨æ·±å…¥äº†è§£è¿™ä¸€è¿‡ç¨‹ä¹‹å‰ï¼Œç®€è¦å›é¡¾ä¸€ä¸‹éšæœºå˜é‡åŠå…¶åˆ†å¸ƒçš„å«ä¹‰æ˜¯æœ‰å¸®åŠ©çš„ã€‚
- en: Random Variables and Their Distributions
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ
- en: 'A random variable is non-constant (its value can change) and its variability
    can be described in terms of distribution. There are many different types of distributions,
    but each falls into one of two broad categories: discrete and continuous. We use
    discrete distributions to describe random variables whose values can take only
    whole numbers, such as counts.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºå˜é‡æ˜¯éæ’å®šçš„ï¼ˆå…¶å€¼å¯èƒ½å˜åŒ–ï¼‰ï¼Œå…¶å˜å¼‚æ€§å¯ä»¥é€šè¿‡åˆ†å¸ƒæ¥æè¿°ã€‚éšæœºå˜é‡çš„åˆ†å¸ƒæœ‰å¾ˆå¤šç§ç±»å‹ï¼Œä½†æ¯ç§éƒ½å±äºä¸¤å¤§ç±»ä¹‹ä¸€ï¼šç¦»æ•£å‹å’Œè¿ç»­å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ç¦»æ•£å‹åˆ†å¸ƒæ¥æè¿°å€¼åªèƒ½å–æ•´æ•°çš„éšæœºå˜é‡ï¼Œä¾‹å¦‚è®¡æ•°ã€‚
- en: An example is the count of visitors to a theme park in a day, or the number
    of attempted shots it takes a golfer to get a hole-in-one.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¾‹å­æ˜¯ä¸»é¢˜å…¬å›­ä¸€å¤©çš„æ¸¸å®¢æ•°é‡ï¼Œæˆ–è€…é«˜å°”å¤«çƒæ‰‹æ‰“å…¥ä¸€æ†æ‰€éœ€çš„å°è¯•æ¬¡æ•°ã€‚
- en: We use continuous distributions to describe random variables whose values lie
    along a continuum made up of infinitely small increments. Examples include human
    height or weight, or outside air temperature. Distributions often have parameters
    that control their shape.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨è¿ç»­åˆ†å¸ƒæ¥æè¿°é‚£äº›å€¼æ²¿ç€ç”±æ— é™å°å¢é‡ç»„æˆçš„è¿ç»­ä½“çš„éšæœºå˜é‡ã€‚ç¤ºä¾‹åŒ…æ‹¬äººç±»çš„èº«é«˜æˆ–ä½“é‡ï¼Œæˆ–è€…å¤–éƒ¨ç©ºæ°”æ¸©åº¦ã€‚åˆ†å¸ƒé€šå¸¸å…·æœ‰æ§åˆ¶å…¶å½¢çŠ¶çš„å‚æ•°ã€‚
- en: Discrete distributions can be described mathematically using what's called a
    probability mass function, which defines the exact probability of the random variable
    taking a certain value. Common notation for the left-hand side of this function
    is `P(X=x)`, which in plain English means that the probability that the random
    variable `X` equals a certain value `x` is `P`. Remember that probabilities range
    between `0` (impossible) and `1` (certain).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦»æ•£åˆ†å¸ƒå¯ä»¥é€šè¿‡æ‰€è°“çš„æ¦‚ç‡è´¨é‡å‡½æ•°æ¥æ•°å­¦æè¿°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†éšæœºå˜é‡å–æŸä¸€ç‰¹å®šå€¼çš„ç¡®åˆ‡æ¦‚ç‡ã€‚è¯¥å‡½æ•°å·¦ä¾§çš„å¸¸è§ç¬¦å·ä¸º`P(X=x)`ï¼Œå…¶è‹±æ–‡å«ä¹‰æ˜¯éšæœºå˜é‡`X`ç­‰äºæŸä¸€ç‰¹å®šå€¼`x`çš„æ¦‚ç‡ä¸º`P`ã€‚è¯·è®°ä½ï¼Œæ¦‚ç‡çš„èŒƒå›´ä»‹äº`0`ï¼ˆä¸å¯èƒ½ï¼‰å’Œ`1`ï¼ˆå¿…ç„¶ï¼‰ä¹‹é—´ã€‚
- en: By definition, the summation of each `P(X=x)` for all possible `x`'s will be
    equal to 1, or if expressed another way, the probability that `X` will take any
    value is 1\. A simple example of this kind of distribution is the discrete uniform
    distribution, where the random variable `X` will take only one of a finite range
    of values and the probability of it taking any particular value is the same for
    all values, hence the term uniform.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®å®šä¹‰ï¼Œæ‰€æœ‰å¯èƒ½çš„`x`å€¼å¯¹åº”çš„æ¯ä¸ª`P(X=x)`çš„æ€»å’Œå°†ç­‰äº1ï¼Œæˆ–è€…æ¢å¥è¯è¯´ï¼Œ`X`å–ä»»ä½•å€¼çš„æ¦‚ç‡ä¸º1ã€‚è¿™ç§åˆ†å¸ƒçš„ä¸€ä¸ªç®€å•ä¾‹å­æ˜¯ç¦»æ•£å‡åŒ€åˆ†å¸ƒï¼Œå…¶ä¸­éšæœºå˜é‡`X`åªèƒ½å–æœ‰é™èŒƒå›´å†…çš„ä¸€ä¸ªå€¼ï¼Œè€Œä¸”å®ƒå–ä»»ä½•ç‰¹å®šå€¼çš„æ¦‚ç‡å¯¹äºæ‰€æœ‰å€¼éƒ½æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤ç§°ä¹‹ä¸ºå‡åŒ€åˆ†å¸ƒã€‚
- en: 'For example, if there are 10 possible values the probability that `X` is any
    particular value is exactly 1/10\. If there were 6 possible values, as in the
    case of a standard 6-sided die, the probability would be 1/6, and so on. The probability
    mass function for the discrete uniform distribution is:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæœ‰10ä¸ªå¯èƒ½çš„å€¼ï¼Œ`X`å–ä»»ä½•ç‰¹å®šå€¼çš„æ¦‚ç‡æ°å¥½ä¸º1/10ã€‚å¦‚æœæœ‰6ä¸ªå¯èƒ½çš„å€¼ï¼Œå°±åƒæ ‡å‡†çš„å…­é¢éª°å­ä¸€æ ·ï¼Œæ¦‚ç‡å°†æ˜¯1/6ï¼Œä¾æ­¤ç±»æ¨ã€‚ç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°ä¸ºï¼š
- en: '![Figure 8.15: Probability mass function for the discrete uniform distribution'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.15: ç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°](img/B15019_08_16.jpg)'
- en: '](img/B15019_08_15.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_15.jpg)'
- en: 'Figure 8.15: Probability mass function for the discrete uniform distribution'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 8.15: ç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°'
- en: The following code will allow us to see the form of this distribution with 10
    possible values of X.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å°†å¸®åŠ©æˆ‘ä»¬æŸ¥çœ‹è¯¥åˆ†å¸ƒçš„å½¢å¼ï¼Œå…¶ä¸­`X`æœ‰10ä¸ªå¯èƒ½å€¼ã€‚
- en: 'First, we create a list of all the possible values `X` can take:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª`X`å¯èƒ½å–å€¼çš„æ‰€æœ‰å€¼çš„åˆ—è¡¨ï¼š
- en: '[PRE45]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output will be as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE46]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We then calculate the probability that `X` will take up any value of `x (P(X=x))`:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è®¡ç®—`X`å–ä»»æ„`x`å€¼çš„æ¦‚ç‡ï¼ˆ`P(X=x)`ï¼‰ï¼š
- en: '[PRE47]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As discussed, the summation of probabilities will equal 1, and this is the
    case with any distribution. We now have everything we need to visualize the distribution:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæ¦‚ç‡çš„æ€»å’Œå°†ç­‰äº1ï¼Œè€Œè¿™é€‚ç”¨äºä»»ä½•åˆ†å¸ƒã€‚ç°åœ¨æˆ‘ä»¬æ‹¥æœ‰äº†æ‰€æœ‰éœ€è¦çš„å†…å®¹æ¥å¯è§†åŒ–è¯¥åˆ†å¸ƒï¼š
- en: '[PRE48]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output will be as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '![Figure 8.16: Visualizing the bar chart'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.16: å¯è§†åŒ–æ¡å½¢å›¾](img/B15019_08_16.jpg)'
- en: '](img/B15019_08_16.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_16.jpg)'
- en: 'Figure 8.16: Visualizing the bar chart'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 8.16: å¯è§†åŒ–æ¡å½¢å›¾'
- en: In the visual output, we see that the probability of `X` being a specific whole
    number between 1 and 10 is equal to 1/10.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è§†è§‰è¾“å‡ºä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°`X`åœ¨1åˆ°10ä¹‹é—´çš„ç‰¹å®šæ•´æ•°çš„æ¦‚ç‡ä¸º1/10ã€‚
- en: Note
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: Other discrete distributions you commonly see include the binomial, negative
    binomial, geometric, and Poisson distributions, all of which we encourage you
    to investigate. Type these terms into a search engine to find out more.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¸¸è§çš„å…¶ä»–ç¦»æ•£åˆ†å¸ƒåŒ…æ‹¬äºŒé¡¹åˆ†å¸ƒã€è´ŸäºŒé¡¹åˆ†å¸ƒã€å‡ ä½•åˆ†å¸ƒå’Œæ³Šæ¾åˆ†å¸ƒï¼Œæ‰€æœ‰è¿™äº›æˆ‘ä»¬éƒ½é¼“åŠ±ä½ è¿›ä¸€æ­¥ç ”ç©¶ã€‚å¯ä»¥å°†è¿™äº›æœ¯è¯­è¾“å…¥æœç´¢å¼•æ“ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: Distributions of continuous random variables are a bit more challenging in that
    we cannot calculate an exact `P(X=x)` directly because `X` lies on a continuum.
    We can, however, use integration to approximate probabilities between a range
    of values, but this is beyond the scope of this book. The relationship between
    `X` and probability is described using a probability density function, `P(X)`.
    Perhaps the most well-known continuous distribution is the normal distribution,
    which visually takes the form of a bell.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ç»­éšæœºå˜é‡çš„åˆ†å¸ƒç¨å¾®å¤æ‚ä¸€ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•ç›´æ¥è®¡ç®— `P(X=x)`ï¼Œå› ä¸º `X` ä½äºä¸€ä¸ªè¿ç»­ä½“ä¸Šã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç§¯åˆ†æ¥è¿‘ä¼¼æŸä¸€èŒƒå›´å†…çš„æ¦‚ç‡ï¼Œä½†è¿™è¶…å‡ºäº†æœ¬ä¹¦çš„è®¨è®ºèŒƒå›´ã€‚`X`
    ä¸æ¦‚ç‡çš„å…³ç³»é€šè¿‡æ¦‚ç‡å¯†åº¦å‡½æ•° `P(X)` æ¥æè¿°ã€‚æˆ–è®¸æœ€è‘—åçš„è¿ç»­åˆ†å¸ƒå°±æ˜¯æ­£æ€åˆ†å¸ƒï¼Œå®ƒåœ¨è§†è§‰ä¸Šå‘ˆç°ä¸ºé’Ÿå½¢æ›²çº¿ã€‚
- en: 'The normal distribution has two parameters that describe its shape, mean (`ğœ‡`)
    and variance (`ğœ`2). The probability density function for the normal distribution
    is:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£æ€åˆ†å¸ƒæœ‰ä¸¤ä¸ªæè¿°å…¶å½¢çŠ¶çš„å‚æ•°ï¼šå‡å€¼ï¼ˆ`ğœ‡`ï¼‰å’Œæ–¹å·®ï¼ˆ`ğœ`2ï¼‰ã€‚æ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºï¼š
- en: '![Figure 8.17: Probability density function for the normal distribution'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.17ï¼šæ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°'
- en: '](img/B15019_08_17.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_17.jpg)'
- en: 'Figure 8.17: Probability density function for the normal distribution'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.17ï¼šæ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°
- en: 'The following code shows two normal distributions with the same mean (`ğœ‡` `=
    0`) but different variance parameters (`ğœ``2 = 1` and `ğœ``2 = 2.25`). Let''s first
    generate 100 evenly spaced values from `-10` to `10` using NumPy''s `.linspace`
    method:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å±•ç¤ºäº†ä¸¤ä¸ªæ­£æ€åˆ†å¸ƒï¼Œå®ƒä»¬çš„å‡å€¼ï¼ˆ`ğœ‡` `= 0`ï¼‰ç›¸åŒï¼Œä½†æ–¹å·®å‚æ•°ï¼ˆ`ğœ``2 = 1` å’Œ `ğœ``2 = 2.25`ï¼‰ä¸åŒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨
    NumPy çš„ `.linspace` æ–¹æ³•ç”Ÿæˆä» `-10` åˆ° `10` çš„ 100 ä¸ªå‡åŒ€åˆ†å¸ƒçš„å€¼ï¼š
- en: '[PRE49]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We then generate the approximate `X` probabilities for both normal distributions.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä¸ºè¿™ä¸¤ä¸ªæ­£æ€åˆ†å¸ƒç”Ÿæˆè¿‘ä¼¼çš„ `X` æ¦‚ç‡ã€‚
- en: 'Using `scipy.stats` is a good way to work with distributions, and its `pdf`
    method allows us to easily visualize the shape of probability density functions:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `scipy.stats` æ˜¯å¤„ç†åˆ†å¸ƒçš„å¥½æ–¹æ³•ï¼Œå®ƒçš„ `pdf` æ–¹æ³•ä½¿æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å¯è§†åŒ–æ¦‚ç‡å¯†åº¦å‡½æ•°çš„å½¢çŠ¶ï¼š
- en: '[PRE50]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: In this case, `loc` corresponds to ğœ‡, while `scale` corresponds to the standard
    deviation, which is the square root of `ğœ``2`, hence why we square the inputs.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`loc` å¯¹åº”äº ğœ‡ï¼Œè€Œ `scale` å¯¹åº”äºæ ‡å‡†å·®ï¼Œå®ƒæ˜¯ `ğœ``2` çš„å¹³æ–¹æ ¹ï¼Œå› æ­¤æˆ‘ä»¬è¦å¯¹è¾“å…¥è¿›è¡Œå¹³æ–¹å¤„ç†ã€‚
- en: 'We then visualize the result. Notice that `ğœ``2` controls how fat the distribution
    is and therefore how variable the random variable is:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯è§†åŒ–ç»“æœã€‚æ³¨æ„ï¼Œ`ğœ``2` æ§åˆ¶ç€åˆ†å¸ƒçš„å®½åº¦ï¼Œä¹Ÿå°±æ§åˆ¶äº†éšæœºå˜é‡çš„å˜åŒ–èŒƒå›´ï¼š
- en: '[PRE51]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output will be as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '![Figure 8.18: Visualizing the normal distribution'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.18ï¼šå¯è§†åŒ–æ­£æ€åˆ†å¸ƒ'
- en: '](img/B15019_08_18.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_18.jpg)'
- en: 'Figure 8.18: Visualizing the normal distribution'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.18ï¼šå¯è§†åŒ–æ­£æ€åˆ†å¸ƒ
- en: Other discrete distributions you commonly see include the gamma, exponential,
    and beta distributions, which we encourage you to investigate.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¸¸è§çš„å…¶ä»–ç¦»æ•£åˆ†å¸ƒåŒ…æ‹¬ä¼½ç›åˆ†å¸ƒã€æŒ‡æ•°åˆ†å¸ƒå’Œè´å¡”åˆ†å¸ƒï¼Œæˆ‘ä»¬é¼“åŠ±ä½ è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚
- en: Note
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The code for this section can be found at [https://packt.live/38Mfyzm](https://packt.live/38Mfyzm).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚çš„ä»£ç å¯ä»¥åœ¨[https://packt.live/38Mfyzm](https://packt.live/38Mfyzm)æ‰¾åˆ°ã€‚
- en: Simple Demonstration of the Random Search Process
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éšæœºæœç´¢è¿‡ç¨‹çš„ç®€å•æ¼”ç¤º
- en: Again, before we get to the scikit-learn implementation of random search parameter
    tuning, we will step through the process using simple Python tools. Up until this
    point, we have only been using classification problems to demonstrate tuning concepts,
    but now we will look at a regression problem. Can we find a model that's able
    to predict the progression of diabetes in patients based on characteristics such
    as BMI and age?
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡å¼ºè°ƒï¼Œåœ¨æˆ‘ä»¬å¼€å§‹è®²è§£ scikit-learn å®ç°çš„éšæœºæœç´¢å‚æ•°è°ƒä¼˜ä¹‹å‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç®€å•çš„ Python å·¥å…·æ¼”ç¤ºè¿™ä¸ªè¿‡ç¨‹ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸€ç›´ä½¿ç”¨åˆ†ç±»é—®é¢˜æ¥å±•ç¤ºè°ƒä¼˜æ¦‚å¿µï¼Œä½†æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ¢è®¨å›å½’é—®é¢˜ã€‚æˆ‘ä»¬èƒ½å¦æ‰¾åˆ°ä¸€ä¸ªæ¨¡å‹ï¼ŒåŸºäºæ‚£è€…çš„
    BMI å’Œå¹´é¾„ç­‰ç‰¹å¾é¢„æµ‹ç³–å°¿ç—…çš„å‘å±•ï¼Ÿ
- en: Note
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The original dataset can be found at [https://packt.live/2O4XN6v](https://packt.live/2O4XN6v).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹æ•°æ®é›†å¯ä»¥åœ¨[https://packt.live/2O4XN6v](https://packt.live/2O4XN6v)æ‰¾åˆ°ã€‚
- en: The code for this section can be found at [https://packt.live/3aOudvK](https://packt.live/3aOudvK).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚çš„ä»£ç å¯ä»¥åœ¨[https://packt.live/3aOudvK](https://packt.live/3aOudvK)æ‰¾åˆ°ã€‚
- en: 'We first load the data:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆåŠ è½½æ•°æ®ï¼š
- en: '[PRE52]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'To get a feel for the data, we can examine the disease progression for the
    first patient:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ç¬¬ä¸€ä¸ªæ‚£è€…çš„ç–¾ç—…è¿›å±•ï¼š
- en: '[PRE53]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output will be as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE54]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s now examine their characteristics:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ¥åˆ†æå®ƒä»¬çš„ç‰¹å¾ï¼š
- en: '[PRE55]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We should see the following:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åº”è¯¥çœ‹åˆ°ä»¥ä¸‹ç»“æœï¼š
- en: '![Figure 8.19: Dictionary for patient characteristics'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.19ï¼šæ‚£è€…ç‰¹å¾å­—å…¸'
- en: '](img/B15019_08_19.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_19.jpg)'
- en: 'Figure 8.19: Dictionary for patient characteristics'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.19ï¼šæ‚£è€…ç‰¹å¾å­—å…¸
- en: For this scenario, we will try a technique called ridge regression, which will
    fit a linear model to the data. Ridge regression is a special method that allows
    us to directly employ regularization to help mitigate the problem of overfitting.
    Ridge regression has one key hyperparameter, ğ›¼, which controls the level of regularization
    in the model fit. If ğ›¼ is set to 1, no regularization will be employed, which
    is actually a special case in which a ridge regression model fit will be exactly
    equal to the fit of an OLS' linear regression model. Increase the value of ğ›¼ and
    you increase the degree of regularization.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å°†å°è¯•ä¸€ç§å«åšå²­å›å½’çš„æŠ€æœ¯ï¼Œå®ƒå°†æ‹Ÿåˆä¸€ä¸ªçº¿æ€§æ¨¡å‹åˆ°æ•°æ®ä¸Šã€‚å²­å›å½’æ˜¯ä¸€ç§ç‰¹æ®Šçš„æ–¹æ³•ï¼Œå…è®¸æˆ‘ä»¬ç›´æ¥ä½¿ç”¨æ­£åˆ™åŒ–æ¥å¸®åŠ©å‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ã€‚å²­å›å½’æœ‰ä¸€ä¸ªå…³é”®è¶…å‚æ•°ğ›¼ï¼Œå®ƒæ§åˆ¶æ¨¡å‹æ‹Ÿåˆä¸­çš„æ­£åˆ™åŒ–ç¨‹åº¦ã€‚å¦‚æœğ›¼è®¾ç½®ä¸º1ï¼Œå°†ä¸ä½¿ç”¨æ­£åˆ™åŒ–ï¼Œè¿™å®é™…ä¸Šæ˜¯å²­å›å½’æ¨¡å‹æ‹Ÿåˆä¸OLSçº¿æ€§å›å½’æ¨¡å‹æ‹Ÿåˆç›¸ç­‰çš„ç‰¹æ®Šæƒ…å†µã€‚å¢å¤§ğ›¼çš„å€¼ï¼Œä¼šå¢åŠ æ­£åˆ™åŒ–çš„ç¨‹åº¦ã€‚
- en: Note
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: We covered ridge regression in *Chapter 7*, *The Generalization of Machine Learning
    Models*.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨*ç¬¬7ç« *ä¸­è®¨è®ºäº†å²­å›å½’ï¼Œ*æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–*ã€‚
- en: For more information on ridge regression and regularization, see [https://packt.live/2NR3GUq](https://packt.live/2NR3GUq).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå²­å›å½’å’Œæ­£åˆ™åŒ–çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[https://packt.live/2NR3GUq](https://packt.live/2NR3GUq)ã€‚
- en: In the context of random search parameter tuning, we assume ğ›¼ is a random variable
    and it is up to us to specify a likely distribution.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éšæœºæœç´¢è¶…å‚æ•°è°ƒæ•´çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å‡è®¾ğ›¼æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œæˆ‘ä»¬éœ€è¦æŒ‡å®šä¸€ä¸ªå¯èƒ½çš„åˆ†å¸ƒã€‚
- en: For this example, we will assume alpha follows a gamma distribution. This distribution
    takes two parameters, k and ğœƒ, which control the shape and scale of the distribution
    respectively.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å‡è®¾ğ›¼éµå¾ªä¼½é©¬åˆ†å¸ƒã€‚è¿™ä¸ªåˆ†å¸ƒæœ‰ä¸¤ä¸ªå‚æ•°ï¼Œkå’Œğœƒï¼Œåˆ†åˆ«æ§åˆ¶åˆ†å¸ƒçš„å½¢çŠ¶å’Œå°ºåº¦ã€‚
- en: 'For ridge regression, we believe the optimal ğ›¼ to be somewhere near 1, becoming
    less likely as you move away from 1\. A parameterization of the gamma distribution
    that reflects this idea is where k and ğœƒ are both equal to 1\. To visualize the
    form of this distribution, we can run the following:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå²­å›å½’ï¼Œæˆ‘ä»¬è®¤ä¸ºæœ€ä¼˜çš„ğ›¼å€¼æ¥è¿‘1ï¼Œå½“ğ›¼è¿œç¦»1æ—¶ï¼Œå…¶å¯èƒ½æ€§é€æ¸é™ä½ã€‚åæ˜ è¿™ä¸€æ€æƒ³çš„ä¼½é©¬åˆ†å¸ƒçš„å‚æ•°åŒ–æ–¹å¼æ˜¯kå’Œğœƒéƒ½ç­‰äº1ã€‚ä¸ºäº†å¯è§†åŒ–è¿™ç§åˆ†å¸ƒçš„å½¢å¼ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼š
- en: '[PRE56]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output will be as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Figure 8.20: Visualization of probabilities'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾8.20ï¼šæ¦‚ç‡çš„å¯è§†åŒ–'
- en: '](img/B15019_08_20.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_20.jpg)'
- en: 'Figure 8.20: Visualization of probabilities'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.20ï¼šæ¦‚ç‡çš„å¯è§†åŒ–
- en: In the graph, you can see how probability decays sharply for smaller values
    of ğ›¼, then decays more slowly for larger values.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºè¾ƒå°çš„ğ›¼å€¼ï¼Œæ¦‚ç‡æ€¥å‰§ä¸‹é™ï¼Œè€Œå¯¹äºè¾ƒå¤§çš„ğ›¼å€¼ï¼Œä¸‹é™é€Ÿåº¦è¾ƒæ…¢ã€‚
- en: 'The next step in the random search process is to sample n values from the chosen
    distribution. In this example, we will draw 100 ğ›¼ values. Remember that the probability
    of drawing out a particular value of ğ›¼ is related to its probability as defined
    by this distribution:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºæœç´¢è¿‡ç¨‹çš„ä¸‹ä¸€æ­¥æ˜¯ä»æ‰€é€‰åˆ†å¸ƒä¸­é‡‡æ ·nä¸ªå€¼ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ç»˜åˆ¶100ä¸ªğ›¼å€¼ã€‚è¯·è®°ä½ï¼ŒæŠ½å–æŸä¸ªç‰¹å®šğ›¼å€¼çš„æ¦‚ç‡ä¸è¯¥å€¼åœ¨æ­¤åˆ†å¸ƒä¸­å®šä¹‰çš„æ¦‚ç‡æœ‰å…³ï¼š
- en: '[PRE57]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: We set a random state to ensure reproducible results.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªéšæœºçŠ¶æ€ä»¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§ã€‚
- en: 'Plotting a histogram of the sample, as shown in the following figure, reveals
    a shape that approximately conforms to the distribution that we have sampled from.
    Note that as your sample sizes increases, the more the histogram conforms to the
    distribution:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ç»˜åˆ¶æ ·æœ¬çš„ç›´æ–¹å›¾ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå¯ä»¥æ­ç¤ºå‡ºä¸€ä¸ªå¤§è‡´ç¬¦åˆæˆ‘ä»¬æ‰€é‡‡æ ·çš„åˆ†å¸ƒçš„å½¢çŠ¶ã€‚è¯·æ³¨æ„ï¼Œéšç€æ ·æœ¬æ•°é‡çš„å¢åŠ ï¼Œç›´æ–¹å›¾å°†æ›´ç¬¦åˆè¯¥åˆ†å¸ƒï¼š
- en: '[PRE58]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output will be as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Figure 8.21: Visualization of the sample distribution'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾8.21ï¼šæ ·æœ¬åˆ†å¸ƒçš„å¯è§†åŒ–'
- en: '](img/B15019_08_21.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_21.jpg)'
- en: 'Figure 8.21: Visualization of the sample distribution'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.21ï¼šæ ·æœ¬åˆ†å¸ƒçš„å¯è§†åŒ–
- en: A model will then be fitted for each value of ğ›¼ sampled and assessed for performance.
    As we have seen with the other approaches to hyperparameter tuning in this chapter,
    performance will be assessed using k-fold cross-validation (with `k =10`) but
    because we are dealing with a regression problem, the performance metric will
    be the test-set negative MSE.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå°†ä¸ºæ¯ä¸ªé‡‡æ ·çš„ğ›¼å€¼æ‹Ÿåˆä¸€ä¸ªæ¨¡å‹å¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬ç« ä¸­å…¶ä»–è¶…å‚æ•°è°ƒæ•´æ–¹æ³•ä¸­çœ‹åˆ°çš„ï¼Œæ€§èƒ½å°†é€šè¿‡kæŠ˜äº¤å‰éªŒè¯ï¼ˆ`k = 10`ï¼‰æ¥è¯„ä¼°ï¼Œä½†å› ä¸ºæˆ‘ä»¬å¤„ç†çš„æ˜¯å›å½’é—®é¢˜ï¼Œæ‰€ä»¥è¯„ä¼°æŒ‡æ ‡å°†æ˜¯æµ‹è¯•é›†çš„è´Ÿå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚
- en: 'Using this metric means larger values are better. We will store the results
    in a dictionary with each ğ›¼ value as the key and the corresponding cross-validated
    negative MSE as the value:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ­¤åº¦é‡æ„å‘³ç€è¾ƒå¤§çš„å€¼æ›´å¥½ã€‚æˆ‘ä»¬å°†ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸ä¸­ï¼Œæ¯ä¸ª ğ›¼ å€¼ä½œä¸ºé”®ï¼Œå¯¹åº”çš„äº¤å‰éªŒè¯è´Ÿ MSE ä½œä¸ºå€¼ï¼š
- en: '[PRE59]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Instead of examining the raw dictionary of results, we will convert it to a
    pandas DataFrame, transpose it, and give the columns names. Sorting by descending
    negative mean squared error reveals that the optimal level of regularization for
    this problem is actually when ğ›¼ is approximately 1, meaning that we did not find
    evidence to suggest regularization is necessary for this problem and that the
    OLS linear model will suffice:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¸å†æ£€æŸ¥åŸå§‹çš„ç»“æœå­—å…¸ï¼Œè€Œæ˜¯å°†å…¶è½¬æ¢ä¸º pandas DataFrameï¼Œè½¬ç½®å¹¶ä¸ºåˆ—å‘½åã€‚æŒ‰è´Ÿå‡æ–¹è¯¯å·®é™åºæ’åºæ˜¾ç¤ºï¼Œå¯¹äºæ­¤é—®é¢˜ï¼Œæ­£åˆ™åŒ–çš„æœ€ä½³æ°´å¹³å®é™…ä¸Šæ˜¯åœ¨
    ğ›¼ çº¦ä¸º 1 æ—¶ï¼Œå³æˆ‘ä»¬æ²¡æœ‰æ‰¾åˆ°è¯æ®è¡¨æ˜è¯¥é—®é¢˜éœ€è¦æ­£åˆ™åŒ–ï¼ŒOLS çº¿æ€§æ¨¡å‹è¶³ä»¥è§£å†³è¯¥é—®é¢˜ï¼š
- en: '[PRE60]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output will be as follows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Figure 8.22: Output for the random search process'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.22ï¼šéšæœºæœç´¢è¿‡ç¨‹çš„è¾“å‡º'
- en: '](img/B15019_08_22.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_22.jpg)'
- en: 'Figure 8.22: Output for the random search process'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.22ï¼šéšæœºæœç´¢è¿‡ç¨‹çš„è¾“å‡º
- en: Note
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The results will be different, depending on the data used.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœä¼šæœ‰æ‰€ä¸åŒï¼Œå–å†³äºæ‰€ä½¿ç”¨çš„æ•°æ®ã€‚
- en: 'It is always beneficial to visualize results where possible. Plotting ğ›¼ by
    negative mean squared error as a scatter plot makes it clear that venturing away
    from ğ›¼ = 1 does not result in improvements in predictive performance:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: åªè¦å¯èƒ½ï¼Œè¿›è¡Œç»“æœå¯è§†åŒ–æ€»æ˜¯æœ‰ç›Šçš„ã€‚å°† ğ›¼ ä¸è´Ÿå‡æ–¹è¯¯å·®ç»˜åˆ¶æˆæ•£ç‚¹å›¾æ¸…æ™°åœ°è¡¨æ˜ï¼Œè¿œç¦» ğ›¼ = 1 å¹¶ä¸ä¼šæ”¹å–„é¢„æµ‹æ€§èƒ½ï¼š
- en: '[PRE61]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output will be as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Figure 8.23: Plotting the scatter plot'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.23ï¼šç»˜åˆ¶æ•£ç‚¹å›¾'
- en: '](img/B15019_08_23.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_23.jpg)'
- en: 'Figure 8.23: Plotting the scatter plot'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.23ï¼šç»˜åˆ¶æ•£ç‚¹å›¾
- en: The fact that we found the optimal ğ›¼ to be 1 (its default value) is a special
    case in hyperparameter tuning in that the optimal hyperparameterization is the
    default one.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘ç°æœ€ä¼˜çš„ ğ›¼ å€¼ä¸º 1ï¼ˆå…¶é»˜è®¤å€¼ï¼‰ï¼Œè¿™åœ¨è¶…å‚æ•°è°ƒä¼˜ä¸­æ˜¯ä¸€ä¸ªç‰¹æ®Šæƒ…å†µï¼Œå› ä¸ºæœ€ä¼˜çš„è¶…å‚æ•°é…ç½®æ°å¥½æ˜¯é»˜è®¤å€¼ã€‚
- en: Tuning Using RandomizedSearchCV
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ RandomizedSearchCV è¿›è¡Œè°ƒä¼˜
- en: 'In practice, we can use the `RandomizedSearchCV` method inside scikit-learn''s
    `model_selection` module to conduct the search. All you need to do is pass in
    your estimator, the hyperparameters you wish to tune along with their distributions,
    the number of samples you would like to sample from each distribution, and the
    metric by which you would like to assess model performance. These correspond to
    the `param_distributions`, `n_iter`, and `scoring` arguments respectively. For
    the sake of demonstration, let''s conduct the search we completed earlier using
    `RandomizedSearchCV`. First, we load the data and initialize our ridge regression
    estimator:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ scikit-learn çš„ `model_selection` æ¨¡å—ä¸­çš„ `RandomizedSearchCV` æ–¹æ³•è¿›è¡Œæœç´¢ã€‚ä½ åªéœ€è¦ä¼ å…¥ä¼°è®¡å™¨ã€ä½ å¸Œæœ›è°ƒä¼˜çš„è¶…å‚æ•°åŠå…¶åˆ†å¸ƒã€ä½ å¸Œæœ›ä»æ¯ä¸ªåˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬æ•°é‡ï¼Œä»¥åŠä½ å¸Œæœ›è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„åº¦é‡æ ‡å‡†ã€‚è¿™äº›å¯¹åº”äº
    `param_distributions`ã€`n_iter` å’Œ `scoring` å‚æ•°ã€‚ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨ `RandomizedSearchCV` è¿›è¡Œä¹‹å‰å®Œæˆçš„æœç´¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åŠ è½½æ•°æ®å¹¶åˆå§‹åŒ–æˆ‘ä»¬çš„å²­å›å½’ä¼°è®¡å™¨ï¼š
- en: '[PRE62]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We then specify that the hyperparameter we would like to tune is `alpha` and
    that we would like ğ›¼ to be distributed `gamma`, with `k = 1` and `ğœƒ` `= 1`:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æŒ‡å®šå¸Œæœ›è°ƒä¼˜çš„è¶…å‚æ•°æ˜¯ `alpha`ï¼Œå¹¶ä¸”æˆ‘ä»¬å¸Œæœ› ğ›¼ æŒ‰ `gamma` åˆ†å¸ƒï¼Œ`k = 1` ä¸” `ğœƒ` `= 1`ï¼š
- en: '[PRE63]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Next, we set up and run the random search process, which will sample 100 values
    from our `gamma(1,1)` distribution, fit the ridge regression, and evaluate its
    performance using cross-validation scored on the negative mean squared error metric:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¾ç½®å¹¶è¿è¡Œéšæœºæœç´¢è¿‡ç¨‹ï¼Œå®ƒå°†ä»æˆ‘ä»¬çš„ `gamma(1,1)` åˆ†å¸ƒä¸­æŠ½å– 100 ä¸ªå€¼ï¼Œæ‹Ÿåˆå²­å›å½’å¹¶ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°å…¶æ€§èƒ½ï¼Œè¯„åˆ†æ ‡å‡†æ˜¯è´Ÿå‡æ–¹è¯¯å·®ï¼š
- en: '[PRE64]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'After completing the search, we can extract the results and generate a pandas
    DataFrame, as we have done previously. Sorting by `rank_test_score` and viewing
    the first five rows aligns with our conclusion that alpha should be set to 1 and
    regularization does not seem to be required for this problem:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆæœç´¢åï¼Œæˆ‘ä»¬å¯ä»¥æå–ç»“æœå¹¶ç”Ÿæˆä¸€ä¸ª pandas DataFrameï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„é‚£æ ·ã€‚æŒ‰ `rank_test_score` æ’åºå¹¶æŸ¥çœ‹å‰äº”è¡Œä¸æˆ‘ä»¬çš„ç»“è®ºä¸€è‡´ï¼Œå³
    alpha åº”è®¾ç½®ä¸º 1ï¼Œå¹¶ä¸”è¯¥é—®é¢˜ä¼¼ä¹ä¸éœ€è¦æ­£åˆ™åŒ–ï¼š
- en: '[PRE65]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The output will be as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Figure 8.24: Output for tuning using RandomizedSearchCV'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.24ï¼šä½¿ç”¨ RandomizedSearchCV è°ƒä¼˜çš„è¾“å‡º'
- en: '](img/B15019_08_24.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_24.jpg)'
- en: 'Figure 8.24: Output for tuning using RandomizedSearchCV'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.24ï¼šä½¿ç”¨ RandomizedSearchCV è°ƒä¼˜çš„è¾“å‡º
- en: Note
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The preceding results may vary, depending on the data.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰çš„ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå–å†³äºæ•°æ®ã€‚
- en: 'Exercise 8.03: Random Search Hyperparameter Tuning for a Random Forest Classifier'
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»ƒä¹  8.03ï¼šéšæœºæœç´¢è¶…å‚æ•°è°ƒä¼˜ï¼Œåº”ç”¨äºéšæœºæ£®æ—åˆ†ç±»å™¨
- en: In this exercise, we will revisit the handwritten digit classification problem,
    this time using a random forest classifier with hyperparameters tuned using a
    random search strategy. The random forest is a popular method used for both single-class
    and multi-class classification problems. It learns by growing `n` simple tree
    models that each progressively split the dataset into areas that best separate
    the points of different classes.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†é‡æ–°å®¡è§†æ‰‹å†™æ•°å­—åˆ†ç±»é—®é¢˜ï¼Œè¿™æ¬¡ä½¿ç”¨éšæœºæ£®æ—åˆ†ç±»å™¨ï¼Œå¹¶é€šè¿‡éšæœºæœç´¢ç­–ç•¥è°ƒä¼˜è¶…å‚æ•°ã€‚éšæœºæ£®æ—æ˜¯ä¸€ç§å¸¸ç”¨çš„æ–¹æ³•ï¼Œé€‚ç”¨äºå•ç±»å’Œå¤šç±»åˆ†ç±»é—®é¢˜ã€‚å®ƒé€šè¿‡ç”Ÿæˆ
    `n` ä¸ªç®€å•çš„æ ‘æ¨¡å‹æ¥å­¦ä¹ ï¼Œæ¯æ£µæ ‘é€æ­¥å°†æ•°æ®é›†åˆ†å‰²æˆèƒ½å¤Ÿæœ€ä½³åˆ†ç¦»ä¸åŒç±»åˆ«æ•°æ®ç‚¹çš„åŒºåŸŸã€‚
- en: The final model produced can be thought of as the average of each of the n tree
    models. In this way, the random forest is an `ensemble` method. The parameters
    we will tune in this exercise are `criterion` and `max_features`.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆç”Ÿæˆçš„æ¨¡å‹å¯ä»¥çœ‹ä½œæ˜¯æ¯ä¸ªæ ‘æ¨¡å‹çš„å¹³å‡å€¼ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œéšæœºæ£®æ—æ˜¯ä¸€ç§ `é›†æˆ` æ–¹æ³•ã€‚æˆ‘ä»¬å°†åœ¨è¿™ä¸ªç»ƒä¹ ä¸­è°ƒä¼˜çš„å‚æ•°æ˜¯ `criterion` å’Œ `max_features`ã€‚
- en: '`criterion` refers to the way in which each split is evaluated from a class
    purity perspective (the purer the splits, the better) and `max_features` is the
    maximum number of features the random forest can use when finding the best splits.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '`criterion` æ˜¯æŒ‡ä»ç±»çº¯åº¦è§’åº¦è¯„ä¼°æ¯æ¬¡åˆ†è£‚çš„æ–¹å¼ï¼ˆåˆ†è£‚è¶Šçº¯å‡€è¶Šå¥½ï¼‰ï¼Œè€Œ `max_features` æ˜¯éšæœºæ£®æ—åœ¨å¯»æ‰¾æœ€ä½³åˆ†è£‚æ—¶å¯ä»¥ä½¿ç”¨çš„æœ€å¤§ç‰¹å¾æ•°é‡ã€‚'
- en: The following steps will help you complete the exercise.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ­¥éª¤å°†å¸®åŠ©ä½ å®Œæˆè¿™ä¸ªç»ƒä¹ ã€‚
- en: Create a new notebook in Google Colab.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ Google Colab ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„ç¬”è®°æœ¬ã€‚
- en: 'Import the data and isolate the features `X` and the target `y`:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¼å…¥æ•°æ®å¹¶æå–ç‰¹å¾ `X` å’Œç›®æ ‡ `y`ï¼š
- en: '[PRE66]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Initialize the random forest classifier estimator. We will set the `n_estimators`
    hyperparameter to `100`, which means the predictions of the final model will essentially
    be an average of `100` simple tree models. Note the use of a random state to ensure
    the reproducibility of results:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–éšæœºæ£®æ—åˆ†ç±»å™¨ä¼°ç®—å™¨ã€‚æˆ‘ä»¬å°† `n_estimators` è¶…å‚æ•°è®¾ç½®ä¸º `100`ï¼Œè¿™æ„å‘³ç€æœ€ç»ˆæ¨¡å‹çš„é¢„æµ‹åŸºæœ¬ä¸Šæ˜¯ `100` ä¸ªç®€å•æ ‘æ¨¡å‹çš„å¹³å‡å€¼ã€‚è¯·æ³¨æ„ä½¿ç”¨éšæœºçŠ¶æ€æ¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§ï¼š
- en: '[PRE67]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'One of the parameters we will be tuning is `max_features`. Let''s find out
    the maximum value this could take:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è°ƒä¼˜çš„å‚æ•°ä¹‹ä¸€æ˜¯ `max_features`ã€‚è®©æˆ‘ä»¬æ‰¾å‡ºå®ƒå¯ä»¥å–çš„æœ€å¤§å€¼ï¼š
- en: '[PRE68]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'You should see that we have 64 features:'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥çœ‹åˆ°æˆ‘ä»¬æœ‰ 64 ä¸ªç‰¹å¾ï¼š
- en: '[PRE69]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Now that we know the maximum value of `max_features` we are free to define our
    hyperparameter inputs to the randomized search process. At this point, we have
    no reason to believe any particular value of `max_features` is more optimal.
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çŸ¥é“äº† `max_features` çš„æœ€å¤§å€¼ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªç”±åœ°å®šä¹‰è¶…å‚æ•°è¾“å…¥åˆ°éšæœºæœç´¢è¿‡ç¨‹ä¸­ã€‚æ­¤æ—¶ï¼Œæˆ‘ä»¬æ²¡æœ‰ç†ç”±è®¤ä¸º `max_features`
    çš„æŸä¸ªç‰¹å®šå€¼æ›´åŠ ä¼˜åŒ–ã€‚
- en: 'Set a discrete uniform distribution covering the range `1` to `64`. Remember
    the probability mass function, `P(X=x) = 1/n`, for this distribution, so `P(X=x)
    = 1/64` in our case. Because `criterion` has only two discrete options, this will
    also be sampled as a discrete uniform distribution with `P(X=x) = Â½`:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¸€ä¸ªè¦†ç›– `1` åˆ° `64` èŒƒå›´çš„ç¦»æ•£å‡åŒ€åˆ†å¸ƒã€‚è®°ä½æ¦‚ç‡è´¨é‡å‡½æ•° `P(X=x) = 1/n`ï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ `P(X=x) = 1/64`ã€‚ç”±äº
    `criterion` åªæœ‰ä¸¤ä¸ªç¦»æ•£é€‰é¡¹ï¼Œå› æ­¤å®ƒä¹Ÿå°†ä½œä¸ºä¸€ä¸ªç¦»æ•£å‡åŒ€åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œ`P(X=x) = Â½`ï¼š
- en: '[PRE70]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We now have everything we need to set up the randomized search process. As
    before, we will use accuracy as the metric of model evaluation. Note the use of
    a random state:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æ‹¥æœ‰äº†è®¾ç½®éšæœºæœç´¢è¿‡ç¨‹æ‰€éœ€çš„ä¸€åˆ‡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºæ¨¡å‹è¯„ä¼°çš„æŒ‡æ ‡ã€‚è¯·æ³¨æ„ä½¿ç”¨éšæœºçŠ¶æ€ï¼š
- en: '[PRE71]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Let''s kick off the process with the. `fit` method. Please note that both fitting
    random forests and cross-validation are computationally expensive processes due
    to their internal processes of iteration. Generating a result may take some time:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ `fit` æ–¹æ³•å¯åŠ¨è¿‡ç¨‹ã€‚è¯·æ³¨æ„ï¼Œç”±äºéšæœºæ£®æ—çš„å†…éƒ¨è¿­ä»£è¿‡ç¨‹ï¼Œæ‹Ÿåˆéšæœºæ£®æ—å’Œäº¤å‰éªŒè¯éƒ½æ˜¯è®¡ç®—é‡å¤§çš„è¿‡ç¨‹ã€‚ç”Ÿæˆç»“æœå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼š
- en: '[PRE72]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'You should see the following:'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥çœ‹åˆ°å¦‚ä¸‹å†…å®¹ï¼š
- en: '![Figure 8.25: RandomizedSearchCV results'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![å›¾ 8.25ï¼šRandomizedSearchCV ç»“æœ'
- en: '](img/B15019_08_25.jpg)'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_25.jpg)'
- en: 'Figure 8.25: RandomizedSearchCV results'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å›¾ 8.25ï¼šRandomizedSearchCV ç»“æœ
- en: 'Next, you need to examine the results. Create a `pandas` DataFrame from the
    `results` attribute, order by the `rank_test_score`, and look at the top five
    model hyperparameterizations. Note that because the random search draws samples
    of hyperparameterizations at random, it is possible to have duplication. We remove
    the duplicate entries from the DataFrame:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦æ£€æŸ¥ç»“æœã€‚ä»`results`å±æ€§åˆ›å»ºä¸€ä¸ª`pandas` DataFrameï¼ŒæŒ‰`rank_test_score`æ’åºï¼Œå¹¶æŸ¥çœ‹æ’åå‰äº”çš„æ¨¡å‹è¶…å‚æ•°é…ç½®ã€‚è¯·æ³¨æ„ï¼Œç”±äºéšæœºæœç´¢æ˜¯éšæœºæŠ½å–è¶…å‚æ•°é…ç½®ï¼Œå› æ­¤å¯èƒ½ä¼šå‡ºç°é‡å¤é¡¹ã€‚æˆ‘ä»¬ä»DataFrameä¸­ç§»é™¤é‡å¤çš„æ¡ç›®ï¼š
- en: '[PRE73]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'You should get the following output:'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ‚¨åº”è¯¥ä¼šå¾—åˆ°ä»¥ä¸‹è¾“å‡ºï¼š
- en: '![Figure 8.26: Top five hyperparameterizations'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![å›¾8.26ï¼šå‰äº”åè¶…å‚æ•°é…ç½®'
- en: '](img/B15019_08_26.jpg)'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_26.jpg)'
- en: 'Figure 8.26: Top five hyperparameterizations'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å›¾8.26ï¼šå‰äº”åè¶…å‚æ•°é…ç½®
- en: Note
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: You may get slightly different results. However, the values you obtain should
    largely agree with those in the preceding output.
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½ä¼šå¾—åˆ°ç•¥æœ‰ä¸åŒçš„ç»“æœã€‚ä¸è¿‡ï¼Œæ‚¨è·å¾—çš„å€¼åº”è¯¥ä¸å‰è¿°è¾“å‡ºä¸­çš„å€¼å¤§ä½“ä¸€è‡´ã€‚
- en: 'The last step is to visualize the result. Including every parameterization
    will result in a cluttered plot, so we will filter on parameterizations that resulted
    in a mean test score > 0.93:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ­¥æ˜¯å¯è§†åŒ–ç»“æœã€‚åŒ…æ‹¬æ‰€æœ‰å‚æ•°é…ç½®ä¼šå¯¼è‡´å›¾è¡¨æ‚ä¹±ï¼Œå› æ­¤æˆ‘ä»¬å°†ç­›é€‰å‡ºé‚£äº›å¹³å‡æµ‹è¯•å¾—åˆ† > 0.93 çš„å‚æ•°é…ç½®ï¼š
- en: '[PRE74]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output will be as follows:'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Figure 8.27: Visualizing the test scores of the top-performing models'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![å›¾8.27ï¼šå¯è§†åŒ–è¡¨ç°æœ€ä½³æ¨¡å‹çš„æµ‹è¯•å¾—åˆ†'
- en: '](img/B15019_08_27.jpg)'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_27.jpg)'
- en: 'Figure 8.27: Visualizing the test scores of the top-performing models'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.27ï¼šå¯è§†åŒ–è¡¨ç°æœ€ä½³æ¨¡å‹çš„æµ‹è¯•å¾—åˆ†
- en: Note
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: To access the source code for this specific section, please refer to [https://packt.live/2uDVct8](https://packt.live/2uDVct8).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¿é—®æ­¤ç‰¹å®šéƒ¨åˆ†çš„æºä»£ç ï¼Œè¯·å‚è€ƒ[https://packt.live/2uDVct8](https://packt.live/2uDVct8)ã€‚
- en: You can also run this example online at [https://packt.live/3gbQMvw](https://packt.live/3gbQMvw).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥åœ¨ç½‘ä¸Šè¿è¡Œæ­¤ç¤ºä¾‹ï¼Œè®¿é—®[https://packt.live/3gbQMvw](https://packt.live/3gbQMvw)ã€‚
- en: We have found the best hyperparameterization to be a random forest classifier
    using the `gini` criterion with the maximum features set to 4.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘ç°æœ€å¥½çš„è¶…å‚æ•°é…ç½®æ˜¯ä½¿ç”¨`gini`å‡†åˆ™çš„éšæœºæ£®æ—åˆ†ç±»å™¨ï¼Œæœ€å¤§ç‰¹å¾æ•°è®¾ç½®ä¸º4ã€‚
- en: Advantages and Disadvantages of a Random Search
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éšæœºæœç´¢çš„ä¼˜ç¼ºç‚¹
- en: Because a random search takes a finite sample from a range of possible hyperparameterizations
    (`n_iter` in `model_selection.RandomizedSearchCV`), it is feasible to expand the
    range of your hyperparameter search beyond what would be practical with a grid
    search. This is because a grid search has to try everything in the range, and
    setting a large range of values may be too slow to process. Searching this wider
    range gives you the chance of discovering a truly optimal solution.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºéšæœºæœç´¢ä»å¯èƒ½çš„è¶…å‚æ•°é…ç½®èŒƒå›´ä¸­æŠ½å–æœ‰é™çš„æ ·æœ¬ï¼ˆåœ¨`model_selection.RandomizedSearchCV`ä¸­ä¸º`n_iter`ï¼‰ï¼Œå› æ­¤å¯ä»¥å°†è¶…å‚æ•°æœç´¢çš„èŒƒå›´æ‰©å±•åˆ°è¶…å‡ºç½‘æ ¼æœç´¢å®é™…å¯è¡Œçš„èŒƒå›´ã€‚è¿™æ˜¯å› ä¸ºç½‘æ ¼æœç´¢å¿…é¡»å°è¯•è¯¥èŒƒå›´å†…çš„æ‰€æœ‰å¯èƒ½æ€§ï¼Œè®¾ç½®ä¸€ä¸ªè¾ƒå¤§çš„å€¼èŒƒå›´å¯èƒ½ä¼šå¯¼è‡´å¤„ç†é€Ÿåº¦è¿‡æ…¢ã€‚æœç´¢æ›´å¹¿æ³›çš„èŒƒå›´ä½¿æ‚¨æœ‰æœºä¼šå‘ç°çœŸæ­£çš„æœ€ä¼˜è§£ã€‚
- en: Compared to the manual and grid search strategies, you do sacrifice a level
    of control to obtain this benefit. The other consideration is that setting up
    random search is a bit more involved than other options in that you have to specify
    distributions. There is always a chance of getting this wrong. That said, if you
    are unsure about what distributions to use, stick with discrete or continuous
    uniform for the respective variable types as this will assign an equal probability
    of selection to all options.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ‰‹åŠ¨æœç´¢å’Œç½‘æ ¼æœç´¢ç­–ç•¥ç›¸æ¯”ï¼Œæ‚¨ç¡®å®ç‰ºç‰²äº†ä¸€å®šçš„æ§åˆ¶åŠ›ä»¥è·å¾—è¿™ä¸€å¥½å¤„ã€‚å¦ä¸€ä¸ªéœ€è¦è€ƒè™‘çš„é—®é¢˜æ˜¯ï¼Œè®¾ç½®éšæœºæœç´¢æ¯”å…¶ä»–é€‰é¡¹ç¨å¾®å¤æ‚ä¸€äº›ï¼Œå› ä¸ºæ‚¨å¿…é¡»æŒ‡å®šåˆ†å¸ƒã€‚æ€»æ˜¯æœ‰å‡ºé”™çš„å¯èƒ½æ€§ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæ‚¨ä¸ç¡®å®šä½¿ç”¨ä»€ä¹ˆåˆ†å¸ƒï¼Œå¯ä»¥é€‰æ‹©ç¦»æ•£æˆ–è¿ç»­å‡åŒ€åˆ†å¸ƒï¼Œé’ˆå¯¹å„ä¸ªå˜é‡ç±»å‹ï¼Œè¿™å°†ä¸ºæ‰€æœ‰é€‰é¡¹åˆ†é…ç›¸ç­‰çš„é€‰æ‹©æ¦‚ç‡ã€‚
- en: 'Activity 8.01: Is the Mushroom Poisonous?'
  id: totrans-438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ´»åŠ¨ 8.01ï¼šè˜‘è‡æœ‰æ¯’å—ï¼Ÿ
- en: Imagine you are a data scientist working for the biology department at your
    local university. Your colleague who is a mycologist (a biologist who specializes
    in fungi) has requested that you help her develop a machine learning model capable
    of discerning whether a particular mushroom species is poisonous or not given
    attributes relating to its appearance.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨æ˜¯ä¸€åæ•°æ®ç§‘å­¦å®¶ï¼Œåœ¨å½“åœ°å¤§å­¦çš„ç”Ÿç‰©å­¦ç³»å·¥ä½œã€‚æ‚¨çš„åŒäº‹æ˜¯ä¸€ä½ä¸“é—¨ç ”ç©¶çœŸèŒçš„ç”Ÿç‰©å­¦å®¶ï¼ˆçœŸèŒå­¦å®¶ï¼‰ï¼Œå¥¹è¯·æ±‚æ‚¨å¸®åŠ©å¥¹å¼€å‘ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è˜‘è‡å¤–è§‚çš„ç›¸å…³ç‰¹å¾åˆ¤æ–­æŸä¸ªè˜‘è‡ç‰©ç§æ˜¯å¦æœ‰æ¯’ã€‚
- en: The objective of this activity is to employ the grid and randomized search strategies
    to find an optimal model for this purpose.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ´»åŠ¨çš„ç›®æ ‡æ˜¯ä½¿ç”¨ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢ç­–ç•¥æ¥æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜æ¨¡å‹ã€‚
- en: Note
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The dataset to be used in this exercise can be found on our GitHub repository
    at [https://packt.live/38zdhaB](https://packt.live/38zdhaB).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ¬¡ç»ƒä¹ æ‰€ç”¨çš„æ•°æ®é›†å¯ä»¥åœ¨æˆ‘ä»¬çš„GitHubä»“åº“æ‰¾åˆ°ï¼š[https://packt.live/38zdhaB](https://packt.live/38zdhaB)ã€‚
- en: 'Details on the attributes of the dataset can be found on the original dataset
    site: [https://packt.live/36j0jfA](https://packt.live/36j0jfA).'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†çš„å±æ€§è¯¦æƒ…å¯ä»¥åœ¨åŸå§‹æ•°æ®é›†ç½‘ç«™æ‰¾åˆ°ï¼š[https://packt.live/36j0jfA](https://packt.live/36j0jfA)ã€‚
- en: Load the data into Python using the `pandas.read_csv()` method, calling the
    object `mushrooms`.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`pandas.read_csv()`æ–¹æ³•å°†æ•°æ®åŠ è½½åˆ°Pythonä¸­ï¼Œå‘½åä¸º`mushrooms`å¯¹è±¡ã€‚
- en: 'Hint: The dataset is in CSV format and has no header. Set `header=None` in
    `pandas.read_csv()`.'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æç¤ºï¼šæ•°æ®é›†æ˜¯CSVæ ¼å¼ä¸”æ²¡æœ‰è¡¨å¤´ã€‚åœ¨`pandas.read_csv()`ä¸­è®¾ç½®`header=None`ã€‚
- en: Separate the target, `y` and features, `X` from the dataset.
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»æ•°æ®é›†ä¸­åˆ†ç¦»å‡ºç›®æ ‡å˜é‡`y`å’Œç‰¹å¾`X`ã€‚
- en: 'Hint: The target can be found in the first column (`mushrooms.iloc[:,0]`) and
    the features in the remaining columns (`mushrooms.iloc[:,1:]`).'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æç¤ºï¼šç›®æ ‡å˜é‡å¯ä»¥åœ¨ç¬¬ä¸€åˆ—ä¸­æ‰¾åˆ°ï¼ˆ`mushrooms.iloc[:,0]`ï¼‰ï¼Œç‰¹å¾åˆ™ä½äºå…¶ä½™åˆ—ï¼ˆ`mushrooms.iloc[:,1:]`ï¼‰ã€‚
- en: Recode the target, `y`, so that poisonous mushrooms are represented as `1` and
    edible mushrooms as `0`.
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡æ–°ç¼–ç ç›®æ ‡å˜é‡`y`ï¼Œä½¿æœ‰æ¯’è˜‘è‡è¡¨ç¤ºä¸º`1`ï¼Œå¯é£Ÿç”¨è˜‘è‡è¡¨ç¤ºä¸º`0`ã€‚
- en: Transform the columns of the feature set `X` into a `numpy` array with a binary
    representation. This is known as one-hot encoding.
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ç‰¹å¾é›†`X`çš„åˆ—è½¬æ¢ä¸ºå…·æœ‰äºŒè¿›åˆ¶è¡¨ç¤ºçš„`numpy`æ•°ç»„ã€‚è¿™ç§°ä¸ºç‹¬çƒ­ç¼–ç ï¼ˆone-hot encodingï¼‰ã€‚
- en: 'Hint: Use `preprocessing.OneHotEncoder()` to transform `X`.'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æç¤ºï¼šä½¿ç”¨`preprocessing.OneHotEncoder()`å°†`X`è½¬æ¢ã€‚
- en: Conduct both a grid and random search to find an optimal hyperparameterization
    for a random forest classifier. Use accuracy as your method of model evaluation.
    Make sure that when you initialize the classifier and when you conduct your random
    search, `random_state = 100`.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿›è¡Œç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢ï¼Œä»¥æ‰¾åˆ°éšæœºæ£®æ—åˆ†ç±»å™¨çš„æœ€ä½³è¶…å‚æ•°é…ç½®ã€‚ä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºæ¨¡å‹è¯„ä¼°æ–¹æ³•ã€‚ç¡®ä¿åœ¨åˆå§‹åŒ–åˆ†ç±»å™¨æ—¶ï¼Œä»¥åŠè¿›è¡Œéšæœºæœç´¢æ—¶ï¼Œ`random_state
    = 100`ã€‚
- en: 'For the grid search, use the following:'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºç½‘æ ¼æœç´¢ï¼Œä½¿ç”¨ä»¥ä¸‹å†…å®¹ï¼š
- en: '[PRE75]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'For the randomized search, use the following:'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºéšæœºæœç´¢ï¼Œä½¿ç”¨ä»¥ä¸‹å†…å®¹ï¼š
- en: '[PRE76]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Plot the mean test score versus hyperparameterization for the top 10 models
    found using random search.
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»˜åˆ¶éšæœºæœç´¢æ‰¾åˆ°çš„å‰10ä¸ªæ¨¡å‹çš„å¹³å‡æµ‹è¯•åˆ†æ•°ä¸è¶…å‚æ•°åŒ–çš„å…³ç³»å›¾ã€‚
- en: 'You should see a plot similar to the following:'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥çœ‹åˆ°ä¸€ä¸ªç±»ä¼¼ä¸‹é¢çš„å›¾è¡¨ï¼š
- en: '![Figure 8.28: Mean test score plot'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ 8.28ï¼šå¹³å‡æµ‹è¯•åˆ†æ•°å›¾'
- en: '](img/B15019_08_28.jpg)'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_28.jpg)'
- en: 'Figure 8.28: Mean test score plot'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.28ï¼šå¹³å‡æµ‹è¯•åˆ†æ•°å›¾
- en: Note
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: 'The solution to the activity can be found here: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: æ´»åŠ¨çš„è§£å†³æ–¹æ¡ˆå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š[https://packt.live/2GbJloz](https://packt.live/2GbJloz)ã€‚
- en: Summary
  id: totrans-463
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In this chapter, we have covered three strategies for hyperparameter tuning
    based on searching for estimator hyperparameterizations that improve performance.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸºäºæœç´¢ä¼°è®¡å™¨è¶…å‚æ•°åŒ–ä»¥æé«˜æ€§èƒ½çš„ä¸‰ç§è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥ã€‚
- en: The manual search is the most hands-on of the three but gives you a unique feel
    for the process. It is suitable for situations where the estimator in question
    is simple (a low number of hyperparameters).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨æœç´¢æ˜¯ä¸‰ç§æ–¹æ³•ä¸­æœ€å…·æ“ä½œæ€§çš„ï¼Œä½†å¯ä»¥è®©ä½ å¯¹æ•´ä¸ªè¿‡ç¨‹æœ‰ç‹¬ç‰¹çš„ç†è§£ã€‚å®ƒé€‚ç”¨äºä¼°ç®—å™¨è¾ƒç®€å•ï¼ˆè¶…å‚æ•°è¾ƒå°‘ï¼‰çš„æƒ…å†µã€‚
- en: The grid search is an automated method that is the most systematic of the three
    but can be very computationally intensive to run when the range of possible hyperparameterizations
    increases.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘æ ¼æœç´¢æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œæ˜¯ä¸‰ç§æ–¹æ³•ä¸­æœ€ç³»ç»ŸåŒ–çš„ï¼Œä½†å½“å¯èƒ½çš„è¶…å‚æ•°èŒƒå›´å¢å¤§æ—¶ï¼Œè¿è¡Œæ—¶å¯èƒ½ä¼šéå¸¸æ¶ˆè€—è®¡ç®—èµ„æºã€‚
- en: The random search, while the most complicated to set up, is based on sampling
    from distributions of hyperparameters, which allows you to expand the search range,
    thereby giving you the chance to discover a good solution that you may miss with
    the grid or manual search options.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºæœç´¢è™½ç„¶è®¾ç½®æœ€ä¸ºå¤æ‚ï¼Œä½†å®ƒåŸºäºä»è¶…å‚æ•°åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œè¿™ä½¿å¾—ä½ èƒ½å¤Ÿæ‰©å¤§æœç´¢èŒƒå›´ï¼Œä»è€Œå¢åŠ å‘ç°ä¼˜ç§€è§£çš„æœºä¼šï¼Œè€Œè¿™äº›å¯èƒ½åœ¨ç½‘æ ¼æœç´¢æˆ–æ‰‹åŠ¨æœç´¢ä¸­è¢«é”™è¿‡ã€‚
- en: In the next chapter, we will be looking at how to visualize results, summarize
    models, and articulate feature importance and weights.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å¯è§†åŒ–ç»“æœã€æ€»ç»“æ¨¡å‹å¹¶é˜è¿°ç‰¹å¾çš„é‡è¦æ€§å’Œæƒé‡ã€‚
