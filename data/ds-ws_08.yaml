- en: '8'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '8'
- en: 8\. Hyperparameter Tuning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. 超参数调节
- en: Overview
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 概览
- en: In this chapter, each hyperparameter tuning strategy will be first broken down
    into its key steps before any high-level scikit-learn implementations are demonstrated.
    This is to ensure that you fully understand the concept behind each of the strategies
    before jumping to the more automated methods.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，每种超参数调节策略将首先分解为其关键步骤，然后再展示任何高级的scikit-learn实现。这是为了确保在跳到更自动化的方法之前，你能完全理解每种策略背后的概念。
- en: By the end of this chapter, you will be able to find further predictive performance
    improvements via the systematic evaluation of estimators with different hyperparameters.
    You will successfully deploy manual, grid, and random search strategies to find
    the optimal hyperparameters. You will be able to parameterize **k-nearest neighbors**
    (**k-NN**), **support vector machines** (**SVMs**), ridge regression, and random
    forest classifiers to optimize model performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够通过系统地评估具有不同超参数的估算器，进一步提升预测性能。你将成功地部署手动搜索、网格搜索和随机搜索策略，找到最优超参数。你将能够对**k最近邻**（**k-NN**）、**支持向量机**（**SVMs**）、岭回归和随机森林分类器进行参数化，以优化模型性能。
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In previous chapters, we discussed several methods to arrive at a model that
    performs well. These include transforming the data via preprocessing, feature
    engineering and scaling, or simply choosing an appropriate estimator (algorithm)
    type from the large set of possible estimators made available to the users of
    scikit-learn.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了几种方法来得到一个表现良好的模型。这些方法包括通过预处理、特征工程和缩放来转换数据，或从大量可用的scikit-learn估算器中选择一个合适的估算器（算法）类型。
- en: Depending on which estimator you eventually select, there may be settings that
    can be adjusted to improve overall predictive performance. These settings are
    known as hyperparameters, and deriving the best hyperparameters is known as tuning
    or optimizing. Properly tuning your hyperparameters can result in performance
    improvements well into the double-digit percentages, so it is well worth doing
    in any modeling exercise.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最终选择的估算器，可能有一些设置可以调整，以提高整体预测性能。这些设置被称为超参数，推导出最佳超参数的过程称为调参或优化。正确调节超参数可以带来性能的显著提升，甚至达到两位数的百分比，因此在任何建模任务中都非常值得进行调参。
- en: This chapter will discuss the concept of hyperparameter tuning and will present
    some simple strategies that you can use to help find the best hyperparameters
    for your estimators.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论超参数调节的概念，并介绍一些简单的策略，帮助你为估算器找到最佳超参数。
- en: In previous chapters, we have seen some exercises that use a range of estimators,
    but we haven't conducted any hyperparameter tuning. After reading this chapter,
    we recommend you revisit these exercises, apply the techniques taught, and see
    if you can improve the results.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们看到了一些使用各种估算器的练习，但我们并没有进行任何超参数调节。阅读完本章后，我们建议你重新审视这些练习，应用所学的技巧，看看能否提升结果。
- en: What Are Hyperparameters?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是超参数？
- en: Hyperparameters can be thought of as a set of dials and switches for each estimator
    that change how the estimator works to explain relationships in the data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数可以被看作是每个估算器的控制旋钮和开关，改变它们会影响估算器如何工作，从而解释数据中的关系。
- en: 'Have a look at *Figure 8.1*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看*图8.1*：
- en: '![Figure 8.1: How hyperparameters work'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1：超参数如何工作'
- en: '](img/B15019_08_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_01.jpg)'
- en: 'Figure 8.1: How hyperparameters work'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：超参数如何工作
- en: If you read from left to right in the preceding figure, you can see that during
    the tuning process we change the value of the hyperparameter, which results in
    a change to the estimator. This in turn causes a change in model performance.
    Our objective is to find hyperparameterization that leads to the best model performance.
    This will be the *optimal* hyperparameterization.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从左到右阅读前面的图示，你会看到在调节过程中，我们改变超参数的值，这会导致估算器的变化。进而，这会引起模型性能的变化。我们的目标是找到能带来最佳模型性能的超参数配置。这将是*最优*的超参数配置。
- en: Estimators can have hyperparameters of varying quantities and types, which means
    that sometimes you can be faced with a very large number of possible hyperparameterizations
    to choose for an estimator.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 估算器可以具有不同数量和类型的超参数，这意味着有时你可能会面临需要选择大量可能的超参数配置的情况。
- en: For instance, scikit-learn's implementation of the SVM classifier (`sklearn.svm.SVC`),
    which you will be introduced to later in the chapter, is an estimator that has
    multiple possible hyperparameterizations. We will test out only a small subset
    of these, namely using a linear kernel or a polynomial kernel of degree 2, 3,
    or 4.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，scikit-learn 实现的 SVM 分类器（`sklearn.svm.SVC`），你将在本章稍后看到，是一个具有多种可能超参数配置的估算器。我们将仅测试其中的一小部分配置，即使用线性核或二次、三次或四次的多项式核。
- en: Some of these hyperparameters are continuous in nature, while others are discrete,
    and the presence of continuous hyperparameters means that the number of possible
    hyperparameterizations is theoretically infinite. Of course, when it comes to
    producing a model with good predictive performance, some hyperparameterizations
    are much better than others, and it is your job as a data scientist to find them.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些超参数中，有些是连续型的，而有些是离散型的，连续型超参数的存在意味着理论上可能的超参数化组合是无限的。当然，当涉及到生成具有良好预测性能的模型时，一些超参数化组合要比其他的好得多，作为数据科学家，你的工作就是找到这些更优的超参数配置。
- en: In the next section, we will be looking at setting these hyperparameters in
    more detail. But first, some clarification of terms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更详细地探讨如何设置这些超参数。但首先，需要澄清一些术语。
- en: Difference between Hyperparameters and Statistical Model Parameters
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数与统计模型参数的区别
- en: 'In your reading on data science, particularly in the area of statistics, you
    will come across terms such as "model parameters," "parameter estimation," and
    "(non)-parametric models." These terms relate to the parameters that feature in
    the mathematical formulation of models. The simplest example is that of the single
    variable linear model with no intercept term that takes the following form:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在你阅读数据科学的材料时，特别是在统计学领域，你会遇到“模型参数”、“参数估计”和“（非）参数化模型”等术语。这些术语与模型数学公式中的参数有关。最简单的例子是没有截距项的单变量线性模型，它的形式如下：
- en: '![Figure 8.2: Equation for a single variable linear model'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2：单变量线性模型的方程'
- en: '](img/B15019_08_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_02.jpg)'
- en: 'Figure 8.2: Equation for a single variable linear model'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：单变量线性模型的方程
- en: Here, 𝛽 is the statistical model parameter, and if this formulation is chosen,
    it is the data scientist's job to use data to estimate what value it takes. This
    could be achieved using **Ordinary Least Squares** (**OLS**) regression modeling,
    or it could be achieved through a method called median regression.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，𝛽 是统计模型参数，如果选择这种形式，数据科学家的任务就是使用数据来估计它的值。这个过程可以通过**普通最小二乘法**（**OLS**）回归建模实现，也可以通过一种叫做中位数回归的方法实现。
- en: Hyperparameters are different in that they are external to the mathematical
    form. An example of a hyperparameter in this case is the way in which 𝛽 will be
    estimated (OLS, or median regression). In some cases, hyperparameters can change
    the algorithm completely (that is, generating a completely different mathematical
    form). You will see examples of this occurring throughout this chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数的不同之处在于它们是外部于数学模型的。例如，在这个案例中，超参数是估计 𝛽 的方法（如最小二乘法（OLS）或中位数回归）。在某些情况下，超参数可能会完全改变算法（即，生成一个完全不同的数学模型）。你将在本章中看到这种情况的例子。
- en: In the next section, you will be looking at how to set a hyperparameter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习如何设置超参数。
- en: Setting Hyperparameters
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置超参数
- en: In *Chapter 7*, *The Generalization of Machine Learning Models*, you were introduced
    to the k-NN model for classification and you saw how varying k, the number of
    nearest neighbors, resulted in changes in model performance with respect to the
    prediction of class labels. Here, k is a hyperparameter, and the act of manually
    trying different values of k is a simple form of hyperparameter tuning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7章*，*机器学习模型的泛化*中，你接触到了用于分类的 k-NN 模型，并且你看到了随着 k（最近邻的数量）的变化，模型性能在预测类别标签时发生了变化。在这里，k
    是一个超参数，手动尝试不同的 k 值就是超参数调整的一个简单形式。
- en: Each time you initialize a scikit-learn estimator, it will take on a hyperparameterization
    as determined by the values you set for its arguments. If you specify no values,
    then the estimator will take on a default hyperparameterization. If you would
    like to see how the hyperparameters have been set for your estimator, and what
    hyperparameters you can adjust, simply print the output of the `estimator.get_params()`
    method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每次初始化一个 scikit-learn 估计器时，它将根据你为其参数设置的值来采用超参数化。如果你没有指定任何值，那么估计器将采用默认的超参数化。如果你想查看估计器的超参数设置以及可以调整的超参数，只需打印`estimator.get_params()`方法的输出。
- en: 'For instance, say we initialize a k-NN estimator without specifying any arguments
    (empty brackets). To see the default hyperparameterization, we can run:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们初始化一个 k-NN 估计器但未指定任何参数（空括号）。要查看默认的超参数设置，我们可以运行：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should get the following output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A dictionary of all the hyperparameters is now printed to the screen, revealing
    their default settings. Notice `k`, our number of nearest neighbors, is set to
    `5`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有超参数的字典已打印到屏幕上，显示它们的默认设置。注意，`k`，即我们最近邻居的数量，设置为 `5`。
- en: To get more information as to what these parameters mean, how they can be changed,
    and what their likely effect may be, you can run the following command and view
    the help file for the estimator in question.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解这些参数的含义，如何更改它们以及它们可能带来的效果，你可以运行以下命令并查看相关估计器的帮助文件。
- en: 'For our k-NN estimator:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 k-NN 估计器：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output will be as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 8.3: Help file for the k-NN estimator'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3：k-NN 估计器的帮助文件'
- en: '](img/B15019_08_03.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_03.jpg)'
- en: 'Figure 8.3: Help file for the k-NN estimator'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：k-NN 估计器的帮助文件
- en: If you look closely at the help file, you will see the default hyperparameterization
    for the estimator under the `String form` heading, along with an explanation of
    what each hyperparameter means under the `Parameters` heading.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细查看帮助文件，你会看到在 `String form` 标题下列出了估计器的默认超参数化，并且在 `Parameters` 标题下有对每个超参数含义的解释。
- en: 'Coming back to our example, if we want to change the hyperparameterization
    from `k = 5` to `k = 15`, just re-initialize the estimator and set the `n_neighbors`
    argument to `15`, which will override the default:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的示例，如果我们想将超参数从 `k = 5` 更改为 `k = 15`，只需重新初始化估计器并将 `n_neighbors` 参数设置为 `15`，这将覆盖默认设置：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should get the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You may have noticed that k is not the only hyperparameter available for k-NN
    classifiers. Setting multiple hyperparameters is as easy as specifying the relevant
    arguments. For example, let''s increase the number of neighbors from `5` to `15`
    and force the algorithm to take the distance of points in the neighborhood, rather
    than a simple majority vote, into account when training. For more information,
    see the description for the `weights` argument in the help file (`?knn`):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，k 并不是 k-NN 分类器唯一的超参数。设置多个超参数就像指定相关参数一样简单。例如，我们可以将邻居数从 `5` 增加到 `15`，并强制算法在训练时考虑邻域中点的距离，而非简单的多数投票。更多信息，请参阅帮助文件中
    `weights` 参数的说明（`?knn`）：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output will be as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the output, you can see `n_neighbors` (`k`) is now set to `15`, and `weights`
    is now set to `distance`, rather than `uniform`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，你可以看到 `n_neighbors`（即 `k`）现在被设置为 `15`，而 `weights` 设置为 `distance`，而非 `uniform`。
- en: Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The code for this section can be found at [https://packt.live/2tN5CH1](https://packt.live/2tN5CH1).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节代码可以在 [https://packt.live/2tN5CH1](https://packt.live/2tN5CH1) 找到。
- en: A Note on Defaults
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于默认值的说明
- en: Generally, efforts have been made by the developers of machine learning libraries
    to set sensible default hyperparameters for estimators. That said, for certain
    datasets, significant performance improvements may be achieved through tuning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器学习库的开发人员已尽力为估计器设置合理的默认超参数。但需要注意的是，对于某些数据集，通过调优可能会获得显著的性能提升。
- en: Finding the Best Hyperparameterization
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找最佳超参数设置
- en: The best hyperparameterization depends on your overall objective in building
    a machine learning model in the first place. In most cases, this is to find the
    model that has the highest predictive performance on unseen data, as measured
    by its ability to correctly label data points (classification) or predict a number
    (regression).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的超参数设置取决于你最初构建机器学习模型的整体目标。在大多数情况下，这就是找到对未见数据具有最高预测性能的模型，通常通过其正确标注数据点（分类）或预测数字（回归）的能力来衡量。
- en: The prediction of unseen data can be simulated using hold-out test sets or cross-validation,
    the former being the method used in this chapter. Performance is evaluated differently
    in each case, for instance, **Mean Squared Error** (**MSE**) for regression and
    accuracy for classification. We seek to reduce the MSE or increase the accuracy
    of our predictions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 未见数据的预测可以通过保留测试集或交叉验证来模拟，本章使用的是前者方法。每种情况的性能评估方式不同，例如，回归使用**均方误差**（**MSE**），分类则使用准确度。我们力求降低MSE或提高预测的准确度。
- en: Let's implement manual hyperparameterization in the following exercise.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在以下练习中实现手动超参数调优。
- en: 'Exercise 8.01: Manual Hyperparameter Tuning for a k-NN Classifier'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习8.01：手动调优k-NN分类器的超参数
- en: In this exercise, we will manually tune a k-NN classifier, which was covered
    in *Chapter 7, The Generalization of Machine Learning Models*, our goal being
    to predict incidences of malignant or benign breast cancer based on cell measurements
    sourced from the affected breast sample.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将手动调优一个k-NN分类器，该分类器在*第7章：机器学习模型的泛化*中介绍，我们的目标是基于来自受影响乳腺样本的细胞测量值预测恶性或良性乳腺癌的发生。
- en: Note
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The dataset to be used in this exercise can be found on our GitHub repository
    at [https://packt.live/36dsxIF](https://packt.live/36dsxIF).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习中使用的数据集可以在我们的GitHub仓库找到，网址为[https://packt.live/36dsxIF](https://packt.live/36dsxIF)。
- en: 'These are the important attributes of the dataset:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是数据集的重要属性：
- en: ID number
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ID号码
- en: Diagnosis (M = malignant, B = benign)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊断（M = 恶性，B = 良性）
- en: 3-32)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-32)
- en: '10 real-valued features are computed for each cell nucleus as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个细胞核计算10个实数值特征，计算方法如下：
- en: Radius (mean of distances from the center to points on the perimeter)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半径（从中心到周长上点的平均距离）
- en: Texture (standard deviation of grayscale values)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纹理（灰度值的标准差）
- en: Perimeter
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周长
- en: Area
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面积
- en: Smoothness (local variation in radius lengths)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平滑度（局部半径长度的变化）
- en: Compactness (perimeter^2 / area - 1.0)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧凑性（周长^2 / 面积 - 1.0）
- en: Concavity (severity of concave portions of the contour)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凹度（轮廓凹部分的严重程度）
- en: Concave points (number of concave portions of the contour)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凹点（轮廓中凹部分的数量）
- en: Symmetry
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对称性
- en: Fractal dimension (refers to the complexity of the tissue architecture; "coastline
    approximation" - 1)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分形维度（指的是组织结构的复杂性；“海岸线近似” - 1）
- en: Note
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Details on the attributes of the dataset can be found at [https://packt.live/30HzGQ6](https://packt.live/30HzGQ6).
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集的属性详情可以在[https://packt.live/30HzGQ6](https://packt.live/30HzGQ6)找到。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成本练习：
- en: Create a new notebook in Google Colab.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Colab中创建一个新的笔记本。
- en: 'Next, import `neighbors`, `datasets`, and `model_selection` from scikit-learn:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从scikit-learn导入`neighbors`、`datasets`和`model_selection`：
- en: '[PRE7]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Load the data. We will call this object `cancer`, and isolate the target `y`,
    and the features, `X`:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。我们将这个对象命名为`cancer`，并隔离目标`y`和特征`X`：
- en: '[PRE8]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Initialize a k-NN classifier with its default hyperparameterization:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个k-NN分类器，使用其默认的超参数设置：
- en: '[PRE9]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Feed this classifier into a 10-fold cross-validation (`cv`), calculating the
    precision score for each fold. Assume that maximizing precision (the proportion
    of true positives in all positive classifications) is the primary objective of
    this exercise:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个分类器输入到10折交叉验证（`cv`）中，计算每一折的精确度分数。假设最大化精确度（在所有正分类中真正例的比例）是本练习的主要目标：
- en: '[PRE10]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Printing `cv` shows the precision score calculated for each fold:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`cv`显示每一折计算得到的精确度分数：
- en: '[PRE11]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You will see the following output:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将看到以下输出：
- en: '[PRE12]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Calculate and print the mean precision score for all folds. This will give
    us an idea of the overall performance of the model, as shown in the following
    code snippet:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印所有折的平均精确度分数。这将给我们一个模型整体表现的概念，具体如下面的代码片段所示：
- en: '[PRE13]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You should get the following output:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '[PRE14]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You should see the mean score is close to 94%. Can this be improved upon?
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会看到平均分数接近94%。能否进一步提高？
- en: 'Run everything again, this time setting hyperparameter `k` to `15`. You can
    see that the result is actually marginally worse (1% lower):'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行所有内容，这次将超参数`k`设置为`15`。你会看到结果实际上稍微更差（低了1%）：
- en: '[PRE15]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output will be as follows:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE16]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Try again with `k` = `7`, `3`, and `1`. In this case, it seems reasonable that
    the default value of 5 is the best option. To avoid repetition, you may like to
    define and call a Python function as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再尝试一次，`k` = `7`、`3` 和 `1`。在这种情况下，默认值5似乎是最好的选择。为了避免重复，你可以定义并调用一个Python函数，如下所示：
- en: '[PRE17]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output will be as follows:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE18]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Nothing beats 94%.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 没有什么能超过94%。
- en: 'Let''s alter a second hyperparameter. Setting `k = 5`, what happens if we change
    the k-NN weighing system to depend on `distance` rather than having `uniform`
    weights? Run all code again, this time with the following hyperparameterization:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们来改变第二个超参数。将`k = 5`，如果我们把k-NN的加权系统从`uniform`改为基于`distance`，会发生什么呢？重新运行所有代码，这次使用以下超参数设置：
- en: '[PRE19]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Did performance improve?
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能有提升吗？
- en: 'You should see no further improvement on the default hyperparameterization
    because the output is:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于默认的超参数设置，你不应看到进一步的提升，因为输出结果是：
- en: '[PRE20]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We therefore conclude that the default hyperparameterization is the optimal
    one in this case.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们得出结论，在这种情况下，默认的超参数设置是最优的。
- en: Note
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/322lWk4](https://packt.live/322lWk4).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参阅 [https://packt.live/322lWk4](https://packt.live/322lWk4)。
- en: You can also run this example online at [https://packt.live/3gbOyfU](https://packt.live/3gbOyfU).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址为 [https://packt.live/3gbOyfU](https://packt.live/3gbOyfU)。
- en: Advantages and Disadvantages of a Manual Search
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动搜索的优缺点
- en: Of all the strategies for hyperparameter tuning, the manual process gives you
    the most control. As you go through the process, you can get a feel for how your
    estimators might perform under different hyperparameterizations, and this means
    you can adjust them in line with your expectations without having to try a large
    number of possibilities unnecessarily. However, this strategy is feasible only
    when there is a small number of possibilities you would like to try. When the
    number of possibilities exceeds about five, this strategy becomes too labor-intensive
    to be practical.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有的超参数调优策略中，手动过程给了你最多的控制权。通过这个过程，你可以感受到不同超参数设置下，估计器的表现如何，这意味着你可以根据自己的期望进行调整，而无需不必要地尝试大量的可能性。然而，这个策略只有在你想尝试的可能性较少时才可行。当可能性超过大约五个时，这个策略就变得过于繁琐，无法实际应用。
- en: In the following sections, we will introduce two strategies to better deal with
    this situation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将介绍两种策略，以更好地处理这种情况。
- en: Tuning Using Grid Search
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用网格搜索进行调优
- en: In the context of machine learning, grid search refers to a strategy of systematically
    testing out every hyperparameterization from a pre-defined set of possibilities
    for your chosen estimator. You decide the criteria used to evaluate performance,
    and once the search is complete, you may manually examine the results and choose
    the best hyperparameterization, or let your computer automatically choose it for
    you.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，网格搜索指的是一种策略，即系统地测试从预定义的超参数可能性集合中，每一个超参数设置。你决定用于评估性能的标准，搜索完成后，你可以手动检查结果并选择最佳的超参数设置，或者让计算机为你自动选择。
- en: The overall objective is to try and find an optimal hyperparameterization that
    leads to improved performance when predicting unseen data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 总体目标是尝试找到一个最佳的超参数设置，从而在预测未见数据时提高性能。
- en: Before we get to the implementations of grid search in scikit-learn, let's first
    demonstrate the strategy using simple Python `for` loops.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始介绍scikit-learn中的网格搜索实现之前，先用简单的Python `for`循环演示该策略。
- en: Simple Demonstration of the Grid Search Strategy
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索策略的简单演示
- en: In the following demonstration of the grid search strategy, we will use the
    breast cancer prediction dataset we saw in *Exercise 8.01*, *Manual Hyperparameter
    Tuning for a k-NN Classifier*, where we manually tuned the hyperparameters of
    the k-NN classifier to optimize for the precision of cancer predictions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的网格搜索策略演示中，我们将使用我们在*练习8.01*中看到的乳腺癌预测数据集，*手动调节k-NN分类器的超参数*，当时我们手动调整了k-NN分类器的超参数，以优化癌症预测的精确度。
- en: This time, instead of manually fitting models with different values of `k` we
    just define the `k` values we would like to try, that is, `k = 1, 3, 5, 7` in
    a Python dictionary. This dictionary will be the grid we will search through to
    find the optimal hyperparameterization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们不再手动拟合具有不同`k`值的模型，而是通过Python字典定义我们希望尝试的`k`值，即`k = 1, 3, 5, 7`。这个字典将作为我们进行网格搜索的基础，帮助我们找到最佳超参数设置。
- en: Note
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The code for this section can be found at [https://packt.live/2U1Y0Li](https://packt.live/2U1Y0Li).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分代码可以在[https://packt.live/2U1Y0Li](https://packt.live/2U1Y0Li)找到。
- en: 'The code will be as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the code snippet, we have used a dictionary `{}` and set the `k` values in
    a Python dictionary.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码片段中，我们使用了一个字典`{}`并将`k`值设置在Python字典中。
- en: In the next part of the code snippet, to conduct the search, we iterate through
    the grid, fitting a model for each value of `k`, each time evaluating the model
    through 10-fold cross-validation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码片段的下一部分，为了进行搜索，我们通过遍历网格来拟合每个`k`值的模型，并每次通过10折交叉验证评估模型。
- en: 'At the end of each iteration, we extract, format, and report back the mean
    precision score after cross-validation via the `print` method:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代结束时，我们通过`print`方法提取、格式化并报告交叉验证后的平均精度得分：
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output will be as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.4: Average precisions for all folds'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.4：所有折叠的平均精度](img/B15019_08_05.jpg)'
- en: '](img/B15019_08_04.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_04.jpg)'
- en: 'Figure 8.4: Average precisions for all folds'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：所有折叠的平均精度
- en: We can see from the output that `k = 5` is the best hyperparameterization found,
    with a mean precision score of roughly 94%. Increasing `k` to `7` didn't significantly
    improve performance. It is important to note that the only parameter we are changing
    here is k and that each time the k-NN estimator is initialized, it is done with
    the remaining hyperparameters set to their default values.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，`k = 5`是找到的最佳超参数化，平均精度大约为94%。将`k`增加到`7`并没有显著改善性能。需要注意的是，我们这里只更改了`k`这个参数，每次初始化k-NN估算器时，其他超参数都保持默认值。
- en: 'To make this point clear, we can run the same loop, this time just printing
    the hyperparameterization that will be tried:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰地表达这一点，我们可以运行相同的循环，这次仅打印出将要尝试的超参数设置：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output will be as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can see from the output that the only parameter we are changing is k; everything
    else remains the same in each iteration.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中你可以看到，我们唯一改变的参数是`k`，每次迭代中的其他设置都保持不变。
- en: Simple, single-loop structures are fine for a grid search of a single hyperparameter,
    but what if we would like to try a second one? Remember that for k-NN we also
    have weights that can take values `uniform` or `distance`, the choice of which
    influences how k-NN learns how to classify points.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单一超参数的网格搜索，简单的单循环结构是可以的，但如果我们想尝试第二个超参数呢？记住，对于k-NN，我们还有可以取值为`uniform`或`distance`的权重，选择不同的权重会影响k-NN如何学习如何分类数据点。
- en: 'To proceed, all we need to do is create a dictionary containing both the values
    of k and the weight functions we would like to try as separate key/value pairs:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续，我们只需要创建一个字典，包含`k`的值以及我们希望尝试的权重函数，作为单独的键/值对：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output will be as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.5: Average precision values for all folds for different values of
    k'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.5：不同`k`值的所有折叠的平均精度值](img/B15019_08_05.jpg)'
- en: '](img/B15019_08_05.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_05.jpg)'
- en: 'Figure 8.5: Average precision values for all folds for different values of
    k'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：不同`k`值的所有折叠的平均精度值
- en: 'You can see that when `k = 5`, the weight function is not based on distance
    and all the other hyperparameters are kept as their default values, and the mean
    precision comes out highest. As we discussed earlier, if you would like to see
    the full set of hyperparameterizations evaluated for k-NN, just add `print(knn.get_params())`
    inside the `for` loop after the estimator is initialized:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，当`k = 5`时，权重函数不基于距离，所有其他超参数保持默认值，平均精度最高。如前所述，如果你想查看k-NN的完整超参数化集，只需在估算器初始化后，在`for`循环中添加`print(knn.get_params())`：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output will be as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This implementation, while great for demonstrating how the grid search process
    works, may not practical when trying to evaluate estimators that have `3`, `4`,
    or even `10` different types of hyperparameters, each with a multitude of possible
    settings.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现虽然非常适合展示网格搜索过程的工作原理，但在尝试评估具有`3`、`4`甚至`10`种不同类型超参数（每种都有大量可能设置）的估算器时，可能并不实用。
- en: To carry on in this way will mean writing and keeping track of multiple `for`
    loops, which can be tedious. Thankfully, `scikit-learn`'s `model_selection` module
    gives us a method called `GridSearchCV` that is much more user-friendly. We will
    be looking at this in the topic ahead.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式继续进行意味着需要编写和跟踪多个 `for` 循环，这可能会很繁琐。幸运的是，`scikit-learn` 的 `model_selection`
    模块提供了一种叫做 `GridSearchCV` 的方法，它更加用户友好。我们将在接下来的主题中讨论这个方法。
- en: GridSearchCV
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GridSearchCV
- en: '`GridsearchCV` is a method of tuning wherein the model can be built by evaluating
    the combination of parameters mentioned in a grid. In the following figure, we
    will see how `GridSearchCV` is different from manual search and look at grid search
    in a muchdetailed way in a table format.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridsearchCV` 是一种调优方法，通过评估网格中提到的参数组合来构建模型。在下图中，我们将看到 `GridSearchCV` 与手动搜索的不同，并通过表格形式详细了解网格搜索。'
- en: Tuning using GridSearchCV
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GridSearchCV 进行调优
- en: We can conduct a grid search much more easily in practice by leveraging `model_selection.GridSearchCV`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过利用 `model_selection.GridSearchCV` 更轻松地进行网格搜索。
- en: 'For the sake of comparison, we will use the same breast cancer dataset and
    k-NN classifier as before:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做对比，我们将使用之前的同一个乳腺癌数据集和 k-NN 分类器：
- en: '[PRE28]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The next thing we need to do after loading the data is to initialize the class
    of the estimator we would like to evaluate under different hyperparameterizations:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据后，我们需要做的下一件事是初始化我们希望在不同超参数化下评估的估计器类：
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We then define the grid:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义网格：
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: To set up the search, we pass the freshly initialized estimator and our grid
    of hyperparameters to `model_selection.GridSearchCV()`. We must also specify a
    scoring metric, which is the method that will be used to evaluate the performance
    of the various hyperparameterizations tried during the search.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置搜索，我们将刚初始化的估计器和超参数网格传递给 `model_selection.GridSearchCV()`。我们还必须指定一个评分指标，这个方法将用于评估在搜索过程中尝试的各种超参数化的表现。
- en: 'The last thing to do is set the number splits to be used using cross-validation
    via the `cv` argument. We will set this to `10`, thereby conducting 10-fold cross-validation:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用交叉验证通过 `cv` 参数设置将要使用的分割数。我们将其设置为 `10`，从而进行 10 折交叉验证：
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The last step is to feed data to this object via its `fit()` method. Once this
    has been done, the grid search process will be kick-started:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是通过 `fit()` 方法将数据传递给这个对象。一旦完成，网格搜索过程将启动：
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'By default, information relating to the search will be printed to the screen,
    allowing you to see the exact estimator parameterizations that will be evaluated
    for the k-NN estimator:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，关于搜索的信息将打印到屏幕上，允许你看到将要评估的 k-NN 估计器的确切估计器参数化：
- en: '![Figure 8.6: Estimator parameterizations for the k-NN estimator'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.6: k-NN 估计器的估计器参数化'
- en: '](img/B15019_08_06.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_06.jpg)'
- en: 'Figure 8.6: Estimator parameterizations for the k-NN estimator'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.6: k-NN 估计器的估计器参数化'
- en: Once the search is complete, we can examine the results by accessing and printing
    the `cv_results_` attribute. `cv_results_` is a dictionary containing helpful
    information regarding model performance under each hyperparameterization, such
    as the mean test-set value of your scoring metric (`mean_test_score`, the lower
    the better), the complete list of hyperparameterizations tried (`params`), and
    the model ranks as they relate to the `mean_test_score` (`rank_test_score`).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦搜索完成，我们可以通过访问和打印 `cv_results_` 属性来查看结果。`cv_results_` 是一个字典，包含关于在每个超参数化下模型表现的有用信息，例如评分指标的平均测试集值（`mean_test_score`，值越低越好），尝试的所有超参数化的完整列表（`params`），以及模型与
    `mean_test_score` 相关的排名（`rank_test_score`）。
- en: The best model found will have rank = 1, the second-best model will have rank
    = 2, and so on, as you can see in *Figure 8.8*. The model fitting times are reported
    through `mean_fit_time`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 找到的最佳模型的排名为 1，第二好的模型排名为 2，以此类推，正如你在 *图 8.8* 中看到的那样。模型拟合时间通过 `mean_fit_time`
    报告。
- en: 'Although not usually a consideration for smaller datasets, this value can be
    important because in some cases you may find that a marginal increase in model
    performance through a certain hyperparameterization is associated with a significant
    increase in model fit time, which, depending on the computing resources you have
    available, may render that hyperparameterization infeasible because it will take
    too long to fit:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于较小的数据集通常不是一个考虑因素，但在某些情况下，这个值可能很重要，因为您可能会发现通过某些超参数化在模型性能上的边际增加与模型拟合时间的显著增加相关联，这取决于您可用的计算资源，可能会导致该超参数化变得不可行，因为它将花费太长时间来拟合：
- en: '[PRE33]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output will be as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 8.7: GridsearchCV results'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.7：GridsearchCV 结果'
- en: '](img/B15019_08_07.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_07.jpg)'
- en: 'Figure 8.7: GridsearchCV results'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：GridsearchCV 结果
- en: 'The model ranks can be seen in the following image:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 模型排名可见以下图片：
- en: '![Figure 8.8: Model ranks'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.8：模型排名'
- en: '](img/B15019_08_08.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_08.jpg)'
- en: 'Figure 8.8: Model ranks'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8：模型排名
- en: Note
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the purpose of presentation, the output has been truncated. You can see
    the complete output here: [https://packt.live/2uD12uP](https://packt.live/2uD12uP).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 出于展示目的，输出已经被截断。您可以在这里查看完整的输出：[https://packt.live/2uD12uP](https://packt.live/2uD12uP)。
- en: In the output, it is worth noting that this dictionary can be easily transformed
    into a pandas DataFrame, which makes information much clearer to read and allows
    us to selectively display the metrics we are interested in.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，值得注意的是，此字典可以轻松转换为 pandas DataFrame，这样使得信息更加清晰易读，并允许我们选择性地显示我们感兴趣的指标。
- en: 'For example, say we are only interested in each hyperparameterization (`params`)
    and mean cross-validated test score (`mean_test_score`) for the top five high
    - performing models:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们只对前五个表现最佳模型的每个超参数化（`params`）和平均交叉验证测试分数（`mean_test_score`）感兴趣：
- en: '[PRE34]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Running this code produces the following output:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将产生以下输出：
- en: '![Figure 8.9: mean_test_score for top 5 models'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.9：前 5 个模型的 mean_test_score'
- en: '](img/B15019_08_09.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_09.jpg)'
- en: 'Figure 8.9: mean_test_score for top 5 models'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9：前 5 个模型的 mean_test_score
- en: 'We can also use pandas to produce visualizations of the result as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 pandas 生成以下结果的可视化：
- en: '[PRE35]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output will be as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 8.10: Using pandas to visualize the output'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.10：使用 pandas 可视化输出'
- en: '](img/B15019_08_10.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_10.jpg)'
- en: 'Figure 8.10: Using pandas to visualize the output'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10：使用 pandas 可视化输出
- en: When you look at the preceding figure, you see that the best hyperparameterization
    found is where `n_neighbors = 5` and `weights = 'uniform'`, because this results
    in the highest mean test score (precision).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当您查看前述图表时，您会发现找到的最佳超参数设置是 `n_neighbors = 5` 和 `weights = 'uniform'`，因为这会产生最高的平均测试分数（精度）。
- en: Note
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The code for this section can be found at [https://packt.live/2uD12uP](https://packt.live/2uD12uP).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分的代码可以在 [https://packt.live/2uD12uP](https://packt.live/2uD12uP) 找到。
- en: Support Vector Machine (SVM) Classifiers
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机（Support Vector Machine，SVM）分类器
- en: The **SVM** classifier is basically a supervised machine learning model. It
    is a commonly used class of estimator that can be used for both binary and multi-class
    classification. It is known to perform well in cases where the data is limited,
    hence it is a reliable model. It is relatively fast to train compared to highly
    iterative or ensemble methods such as artificial neural networks or random forests,
    which makes it a good option if there is a limit on your computer's processing
    power.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVM** 分类器基本上是一种监督学习模型。它是一种常用的估算器类别，可用于二元和多类分类。它在数据有限的情况下表现良好，因此是一个可靠的模型。与高度迭代或集成方法（如人工神经网络或随机森林）相比，训练速度相对较快，这使其成为在计算机处理能力有限的情况下的良好选择。'
- en: It makes its predictions by leveraging a special mathematical formulation known
    as a kernel function. This function can take several forms with some functions,
    such as the polynomial kernel function with a degree (squared, cubed, and so on),
    that have their own adjustable parameters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过利用一种称为核函数的特殊数学公式进行预测。这个函数可以采用多种形式，其中一些函数（如具有其自身可调参数的多项式核函数）。
- en: SVMs have been shown to perform well in the context of image classification,
    which you will see in the following exercise.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 在图像分类的背景下表现良好，您将在以下练习中看到。
- en: Note
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on support vector machines, see [https://packt.live/37iDytw](https://packt.live/37iDytw)
    and also refer to [https://packt.live/38xaPkC](https://packt.live/38xaPkC).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 有关支持向量机的更多信息，请参见[https://packt.live/37iDytw](https://packt.live/37iDytw)，并参考[https://packt.live/38xaPkC](https://packt.live/38xaPkC)。
- en: 'Exercise 8.02: Grid Search Hyperparameter Tuning for an SVM'
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '练习 8.02: 支持向量机的网格搜索超参数调优'
- en: In this exercise, we will employ a class of estimator called an SVM classifier
    and tune its hyperparameters using a grid search strategy.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用一种叫做SVM分类器的估计器，并通过网格搜索策略来调整其超参数。
- en: The supervised learning objective we will focus on here is the classification
    of handwritten digits (0-9) based solely on images. The dataset we will use contains
    1,797 labeled images of handwritten digits.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里关注的监督学习目标是基于图像对手写数字（0-9）进行分类。我们将使用的数据集包含1,797个标记的手写数字图像。
- en: Note
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The dataset to be used in this exercise can be found on our GitHub repository
    at [https://packt.live/2vdbHg9](https://packt.live/2vdbHg9).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习所用的数据集可以在我们的GitHub库中找到：[https://packt.live/2vdbHg9](https://packt.live/2vdbHg9)。
- en: 'Details on the attributes of the dataset can be found on the original dataset''s
    URL: [https://packt.live/36cX35b](https://packt.live/36cX35b).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的详细信息可以在原始数据集的网址找到：[https://packt.live/36cX35b](https://packt.live/36cX35b)。
- en: Create a new notebook in Google Colab.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Colab中创建一个新的笔记本。
- en: 'Import `datasets`, `svm`, and `model_selection` from scikit-learn:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从scikit-learn中导入`datasets`、`svm`和`model_selection`：
- en: '[PRE36]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Load the data. We will call this object images, and then we''ll isolate the
    target `y` and the features `X`. In the training step, the SVM classifier will
    learn how `y` relates to `X` and will therefore be able to predict new `y` values
    when given new `X` values:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。我们将这个对象命名为images，然后将目标`y`和特征`X`分离。在训练步骤中，SVM分类器将学习`y`如何与`X`相关联，因此，当给定新的`X`值时，它能够预测新的`y`值：
- en: '[PRE37]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Initialize the estimator as a multi-class SVM classifier and set the `gamma`
    argument to `scale`:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将估计器初始化为多类SVM分类器，并将`gamma`参数设置为`scale`：
- en: '[PRE38]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Note
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on the gamma argument, go to [https://packt.live/2Ga2l79](https://packt.live/2Ga2l79).
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于`gamma`参数的更多信息，请访问[https://packt.live/2Ga2l79](https://packt.live/2Ga2l79)。
- en: 'Define our grid to cover four distinct hyperparameterizations of the classifier
    with a linear kernel and with a polynomial kernel of degrees `2`, `3,` and `4`.
    We want to see which of the four hyperparameterizations leads to more accurate
    predictions:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们的网格，涵盖四种不同的分类器超参数配置，包括线性核和多项式核（分别为`2`、`3`和`4`度）。我们希望查看哪种超参数配置能够提供更准确的预测：
- en: '[PRE39]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Set up grid search k-fold cross-validation with `10` folds and a scoring measure
    of accuracy. Make sure it has our `grid` and `estimator` objects as inputs:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置网格搜索k折交叉验证，使用`10`折和准确度评分方法。确保它的输入包含我们的`grid`和`estimator`对象：
- en: '[PRE40]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Start the search by providing data to the `.fit()` method. Details of the process,
    including the hyperparameterizations tried and the scoring method selected, will
    be printed to the screen:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将数据提供给`.fit()`方法来开始搜索。过程中，将会打印出超参数配置尝试情况以及选择的评分方法：
- en: '[PRE41]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You should see the following output:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 8.11: Grid Search using the .fit() method'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.11: 使用.fit()方法进行网格搜索'
- en: '](img/B15019_08_11.jpg)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_11.jpg)'
- en: 'Figure 8.11: Grid Search using the .fit() method'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 8.11: 使用.fit()方法进行网格搜索'
- en: 'To examine all of the results, simply print `cv_spec.cv_results_` to the screen.
    You will see that the results are structured as a dictionary, allowing you to
    access the information you require using the keys:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看所有结果，只需将`cv_spec.cv_results_`打印到屏幕上。你会看到结果结构为字典，这样你就可以通过键访问所需的信息：
- en: '[PRE42]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You will see the following information:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将看到以下信息：
- en: '![Figure 8.12: Results as a dictionary'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.12: 结果作为字典'
- en: '](img/B15019_08_12.jpg)'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_12.jpg)'
- en: 'Figure 8.12: Results as a dictionary'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 8.12: 结果作为字典'
- en: For this exercise, we are primarily concerned with the test-set performance
    of each distinct hyperparameterization. You can see the first hyperparameterization
    through `cv_spec.cv_results_['mean_test_score']`, and the second through `cv_spec.cv_results_['params']`.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个练习，我们主要关心每种不同超参数配置在测试集上的表现。你可以通过`cv_spec.cv_results_['mean_test_score']`查看第一个超参数配置，通过`cv_spec.cv_results_['params']`查看第二个超参数配置。
- en: 'Let''s convert the results dictionary to a `pandas` DataFrame and find the
    best hyperparameterization:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将结果字典转换为`pandas` DataFrame，并找到最佳的超参数配置：
- en: '[PRE43]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You will see the following results:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将看到以下结果：
- en: '![Figure 8.13: Parameterization results'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.13：参数化结果'
- en: '](img/B15019_08_13.jpg)'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_13.jpg)'
- en: 'Figure 8.13: Parameterization results'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.13：参数化结果
- en: Note
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You may get slightly different results. However, the values you obtain should
    largely agree with those in the preceding output.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可能会得到略有不同的结果。不过，你获得的值应与前面的输出大致一致。
- en: 'It is best practice to visualize any results you produce. `pandas` makes this
    easy. Run the following code to produce a visualization:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最好的做法是可视化你所得到的任何结果。`pandas` 使这一过程变得简单。运行以下代码来生成可视化图表：
- en: '[PRE44]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output will be as follows:'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.14: Using pandas to visualize the results'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.14：使用 pandas 可视化结果'
- en: '](img/B15019_08_14.jpg)'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_14.jpg)'
- en: 'Figure 8.14: Using pandas to visualize the results'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14：使用 pandas 可视化结果
- en: Note
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/36At2MO](https://packt.live/36At2MO).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/36At2MO](https://packt.live/36At2MO)。
- en: You can also run this example online at [https://packt.live/2YdQsGq](https://packt.live/2YdQsGq).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2YdQsGq](https://packt.live/2YdQsGq)上在线运行此示例。
- en: We can see that an SVM classifier with a third-degree polynomial kernel function
    has the highest accuracy of all the hyperparameterizations evaluated in our search.
    Feel free to add more hyperparameterizations to the grid and see if you can improve
    on the score.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，具有三次多项式核函数的 SVM 分类器在所有评估的超参数组合中具有最高的准确率。你可以随意向网格中添加更多超参数组合，看看是否能提高分数。
- en: Advantages and Disadvantages of Grid Search
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索的优缺点
- en: The primary advantage of the grid search compared to a manual search is that
    it is an automated process that one can simply set and forget. Additionally, you
    have the power to dictate the exact hyperparameterizations evaluated, which can
    be a good thing when you have prior knowledge of what kind of hyperparameterizations
    might work well in your context. It is also easy to understand exactly what will
    happen during the search thanks to the explicit definitions of the grid.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动搜索相比，网格搜索的主要优点在于它是一个自动化过程，用户只需设置一次并可忘记它。此外，你还可以精确控制所评估的超参数组合，当你对哪些超参数组合可能在你的场景中有效有先验知识时，这种精确性是一个很好的优势。由于网格的定义是明确的，你也很容易理解搜索过程中将会发生什么。
- en: The major drawback of the grid search strategy is that it is computationally
    very expensive; that is, when the number of hyperparameterizations to try increases
    substantially, processing times can be very slow. Also, when you define your grid,
    you may inadvertently omit an hyperparameterization that would in fact be optimal.
    If it is not specified in your grid, it will never be tried
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索策略的主要缺点是计算开销非常大；也就是说，当需要尝试的超参数组合数量大幅增加时，处理时间可能非常慢。此外，在定义网格时，你可能会无意中遗漏一个实际上是最优的超参数组合。如果它没有出现在网格中，它将永远不会被尝试。
- en: To overcome these drawbacks, we will be looking at random search in the next
    section.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些缺点，我们将在下一部分探讨随机搜索。
- en: Random Search
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机搜索
- en: Instead of searching through every hyperparameterizations in a pre-defined set,
    as is the case with a grid search, in a random search we sample from a distribution
    of possibilities by assuming each hyperparameter to be a random variable. Before
    we go through the process in depth, it will be helpful to briefly review what
    random variables are and what we mean by a distribution.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 与网格搜索通过预定义的超参数集合逐一进行搜索不同，在随机搜索中，我们通过假设每个超参数是一个随机变量，从可能性的分布中进行采样。在深入了解这一过程之前，简要回顾一下随机变量及其分布的含义是有帮助的。
- en: Random Variables and Their Distributions
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机变量及其分布
- en: 'A random variable is non-constant (its value can change) and its variability
    can be described in terms of distribution. There are many different types of distributions,
    but each falls into one of two broad categories: discrete and continuous. We use
    discrete distributions to describe random variables whose values can take only
    whole numbers, such as counts.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量是非恒定的（其值可能变化），其变异性可以通过分布来描述。随机变量的分布有很多种类型，但每种都属于两大类之一：离散型和连续型。我们使用离散型分布来描述值只能取整数的随机变量，例如计数。
- en: An example is the count of visitors to a theme park in a day, or the number
    of attempted shots it takes a golfer to get a hole-in-one.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是主题公园一天的游客数量，或者高尔夫球手打入一杆所需的尝试次数。
- en: We use continuous distributions to describe random variables whose values lie
    along a continuum made up of infinitely small increments. Examples include human
    height or weight, or outside air temperature. Distributions often have parameters
    that control their shape.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用连续分布来描述那些值沿着由无限小增量组成的连续体的随机变量。示例包括人类的身高或体重，或者外部空气温度。分布通常具有控制其形状的参数。
- en: Discrete distributions can be described mathematically using what's called a
    probability mass function, which defines the exact probability of the random variable
    taking a certain value. Common notation for the left-hand side of this function
    is `P(X=x)`, which in plain English means that the probability that the random
    variable `X` equals a certain value `x` is `P`. Remember that probabilities range
    between `0` (impossible) and `1` (certain).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 离散分布可以通过所谓的概率质量函数来数学描述，该函数定义了随机变量取某一特定值的确切概率。该函数左侧的常见符号为`P(X=x)`，其英文含义是随机变量`X`等于某一特定值`x`的概率为`P`。请记住，概率的范围介于`0`（不可能）和`1`（必然）之间。
- en: By definition, the summation of each `P(X=x)` for all possible `x`'s will be
    equal to 1, or if expressed another way, the probability that `X` will take any
    value is 1\. A simple example of this kind of distribution is the discrete uniform
    distribution, where the random variable `X` will take only one of a finite range
    of values and the probability of it taking any particular value is the same for
    all values, hence the term uniform.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，所有可能的`x`值对应的每个`P(X=x)`的总和将等于1，或者换句话说，`X`取任何值的概率为1。这种分布的一个简单例子是离散均匀分布，其中随机变量`X`只能取有限范围内的一个值，而且它取任何特定值的概率对于所有值都是相同的，因此称之为均匀分布。
- en: 'For example, if there are 10 possible values the probability that `X` is any
    particular value is exactly 1/10\. If there were 6 possible values, as in the
    case of a standard 6-sided die, the probability would be 1/6, and so on. The probability
    mass function for the discrete uniform distribution is:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果有10个可能的值，`X`取任何特定值的概率恰好为1/10。如果有6个可能的值，就像标准的六面骰子一样，概率将是1/6，依此类推。离散均匀分布的概率质量函数为：
- en: '![Figure 8.15: Probability mass function for the discrete uniform distribution'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.15: 离散均匀分布的概率质量函数](img/B15019_08_16.jpg)'
- en: '](img/B15019_08_15.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_15.jpg)'
- en: 'Figure 8.15: Probability mass function for the discrete uniform distribution'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.15: 离散均匀分布的概率质量函数'
- en: The following code will allow us to see the form of this distribution with 10
    possible values of X.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将帮助我们查看该分布的形式，其中`X`有10个可能值。
- en: 'First, we create a list of all the possible values `X` can take:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个`X`可能取值的所有值的列表：
- en: '[PRE45]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output will be as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE46]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We then calculate the probability that `X` will take up any value of `x (P(X=x))`:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算`X`取任意`x`值的概率（`P(X=x)`）：
- en: '[PRE47]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As discussed, the summation of probabilities will equal 1, and this is the
    case with any distribution. We now have everything we need to visualize the distribution:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，概率的总和将等于1，而这适用于任何分布。现在我们拥有了所有需要的内容来可视化该分布：
- en: '[PRE48]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output will be as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.16: Visualizing the bar chart'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.16: 可视化条形图](img/B15019_08_16.jpg)'
- en: '](img/B15019_08_16.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_16.jpg)'
- en: 'Figure 8.16: Visualizing the bar chart'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.16: 可视化条形图'
- en: In the visual output, we see that the probability of `X` being a specific whole
    number between 1 and 10 is equal to 1/10.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉输出中，我们看到`X`在1到10之间的特定整数的概率为1/10。
- en: Note
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Other discrete distributions you commonly see include the binomial, negative
    binomial, geometric, and Poisson distributions, all of which we encourage you
    to investigate. Type these terms into a search engine to find out more.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你常见的其他离散分布包括二项分布、负二项分布、几何分布和泊松分布，所有这些我们都鼓励你进一步研究。可以将这些术语输入搜索引擎以获取更多信息。
- en: Distributions of continuous random variables are a bit more challenging in that
    we cannot calculate an exact `P(X=x)` directly because `X` lies on a continuum.
    We can, however, use integration to approximate probabilities between a range
    of values, but this is beyond the scope of this book. The relationship between
    `X` and probability is described using a probability density function, `P(X)`.
    Perhaps the most well-known continuous distribution is the normal distribution,
    which visually takes the form of a bell.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 连续随机变量的分布稍微复杂一点，因为我们无法直接计算 `P(X=x)`，因为 `X` 位于一个连续体上。然而，我们可以使用积分来近似某一范围内的概率，但这超出了本书的讨论范围。`X`
    与概率的关系通过概率密度函数 `P(X)` 来描述。或许最著名的连续分布就是正态分布，它在视觉上呈现为钟形曲线。
- en: 'The normal distribution has two parameters that describe its shape, mean (`𝜇`)
    and variance (`𝜎`2). The probability density function for the normal distribution
    is:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布有两个描述其形状的参数：均值（`𝜇`）和方差（`𝜎`2）。正态分布的概率密度函数为：
- en: '![Figure 8.17: Probability density function for the normal distribution'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.17：正态分布的概率密度函数'
- en: '](img/B15019_08_17.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_17.jpg)'
- en: 'Figure 8.17: Probability density function for the normal distribution'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17：正态分布的概率密度函数
- en: 'The following code shows two normal distributions with the same mean (`𝜇` `=
    0`) but different variance parameters (`𝜎``2 = 1` and `𝜎``2 = 2.25`). Let''s first
    generate 100 evenly spaced values from `-10` to `10` using NumPy''s `.linspace`
    method:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了两个正态分布，它们的均值（`𝜇` `= 0`）相同，但方差参数（`𝜎``2 = 1` 和 `𝜎``2 = 2.25`）不同。首先，我们使用
    NumPy 的 `.linspace` 方法生成从 `-10` 到 `10` 的 100 个均匀分布的值：
- en: '[PRE49]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We then generate the approximate `X` probabilities for both normal distributions.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为这两个正态分布生成近似的 `X` 概率。
- en: 'Using `scipy.stats` is a good way to work with distributions, and its `pdf`
    method allows us to easily visualize the shape of probability density functions:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `scipy.stats` 是处理分布的好方法，它的 `pdf` 方法使我们可以轻松地可视化概率密度函数的形状：
- en: '[PRE50]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this case, `loc` corresponds to 𝜇, while `scale` corresponds to the standard
    deviation, which is the square root of `𝜎``2`, hence why we square the inputs.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`loc` 对应于 𝜇，而 `scale` 对应于标准差，它是 `𝜎``2` 的平方根，因此我们要对输入进行平方处理。
- en: 'We then visualize the result. Notice that `𝜎``2` controls how fat the distribution
    is and therefore how variable the random variable is:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可视化结果。注意，`𝜎``2` 控制着分布的宽度，也就控制了随机变量的变化范围：
- en: '[PRE51]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output will be as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.18: Visualizing the normal distribution'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.18：可视化正态分布'
- en: '](img/B15019_08_18.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_18.jpg)'
- en: 'Figure 8.18: Visualizing the normal distribution'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.18：可视化正态分布
- en: Other discrete distributions you commonly see include the gamma, exponential,
    and beta distributions, which we encourage you to investigate.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 你常见的其他离散分布包括伽玛分布、指数分布和贝塔分布，我们鼓励你进行进一步研究。
- en: Note
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The code for this section can be found at [https://packt.live/38Mfyzm](https://packt.live/38Mfyzm).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码可以在[https://packt.live/38Mfyzm](https://packt.live/38Mfyzm)找到。
- en: Simple Demonstration of the Random Search Process
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机搜索过程的简单演示
- en: Again, before we get to the scikit-learn implementation of random search parameter
    tuning, we will step through the process using simple Python tools. Up until this
    point, we have only been using classification problems to demonstrate tuning concepts,
    but now we will look at a regression problem. Can we find a model that's able
    to predict the progression of diabetes in patients based on characteristics such
    as BMI and age?
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，在我们开始讲解 scikit-learn 实现的随机搜索参数调优之前，我们将使用简单的 Python 工具演示这个过程。到目前为止，我们一直使用分类问题来展示调优概念，但接下来我们将探讨回归问题。我们能否找到一个模型，基于患者的
    BMI 和年龄等特征预测糖尿病的发展？
- en: Note
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The original dataset can be found at [https://packt.live/2O4XN6v](https://packt.live/2O4XN6v).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集可以在[https://packt.live/2O4XN6v](https://packt.live/2O4XN6v)找到。
- en: The code for this section can be found at [https://packt.live/3aOudvK](https://packt.live/3aOudvK).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码可以在[https://packt.live/3aOudvK](https://packt.live/3aOudvK)找到。
- en: 'We first load the data:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载数据：
- en: '[PRE52]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'To get a feel for the data, we can examine the disease progression for the
    first patient:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解数据，我们可以检查第一个患者的疾病进展：
- en: '[PRE53]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output will be as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE54]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s now examine their characteristics:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来分析它们的特征：
- en: '[PRE55]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We should see the following:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下结果：
- en: '![Figure 8.19: Dictionary for patient characteristics'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.19：患者特征字典'
- en: '](img/B15019_08_19.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_19.jpg)'
- en: 'Figure 8.19: Dictionary for patient characteristics'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19：患者特征字典
- en: For this scenario, we will try a technique called ridge regression, which will
    fit a linear model to the data. Ridge regression is a special method that allows
    us to directly employ regularization to help mitigate the problem of overfitting.
    Ridge regression has one key hyperparameter, 𝛼, which controls the level of regularization
    in the model fit. If 𝛼 is set to 1, no regularization will be employed, which
    is actually a special case in which a ridge regression model fit will be exactly
    equal to the fit of an OLS' linear regression model. Increase the value of 𝛼 and
    you increase the degree of regularization.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，我们将尝试一种叫做岭回归的技术，它将拟合一个线性模型到数据上。岭回归是一种特殊的方法，允许我们直接使用正则化来帮助减轻过拟合问题。岭回归有一个关键超参数𝛼，它控制模型拟合中的正则化程度。如果𝛼设置为1，将不使用正则化，这实际上是岭回归模型拟合与OLS线性回归模型拟合相等的特殊情况。增大𝛼的值，会增加正则化的程度。
- en: Note
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We covered ridge regression in *Chapter 7*, *The Generalization of Machine Learning
    Models*.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第7章*中讨论了岭回归，*机器学习模型的泛化*。
- en: For more information on ridge regression and regularization, see [https://packt.live/2NR3GUq](https://packt.live/2NR3GUq).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 关于岭回归和正则化的更多信息，请参见[https://packt.live/2NR3GUq](https://packt.live/2NR3GUq)。
- en: In the context of random search parameter tuning, we assume 𝛼 is a random variable
    and it is up to us to specify a likely distribution.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机搜索超参数调整的背景下，我们假设𝛼是一个随机变量，我们需要指定一个可能的分布。
- en: For this example, we will assume alpha follows a gamma distribution. This distribution
    takes two parameters, k and 𝜃, which control the shape and scale of the distribution
    respectively.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们假设𝛼遵循伽马分布。这个分布有两个参数，k和𝜃，分别控制分布的形状和尺度。
- en: 'For ridge regression, we believe the optimal 𝛼 to be somewhere near 1, becoming
    less likely as you move away from 1\. A parameterization of the gamma distribution
    that reflects this idea is where k and 𝜃 are both equal to 1\. To visualize the
    form of this distribution, we can run the following:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 对于岭回归，我们认为最优的𝛼值接近1，当𝛼远离1时，其可能性逐渐降低。反映这一思想的伽马分布的参数化方式是k和𝜃都等于1。为了可视化这种分布的形式，我们可以运行以下代码：
- en: '[PRE56]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output will be as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 8.20: Visualization of probabilities'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.20：概率的可视化'
- en: '](img/B15019_08_20.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_20.jpg)'
- en: 'Figure 8.20: Visualization of probabilities'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20：概率的可视化
- en: In the graph, you can see how probability decays sharply for smaller values
    of 𝛼, then decays more slowly for larger values.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，你可以看到，对于较小的𝛼值，概率急剧下降，而对于较大的𝛼值，下降速度较慢。
- en: 'The next step in the random search process is to sample n values from the chosen
    distribution. In this example, we will draw 100 𝛼 values. Remember that the probability
    of drawing out a particular value of 𝛼 is related to its probability as defined
    by this distribution:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索过程的下一步是从所选分布中采样n个值。在这个例子中，我们将绘制100个𝛼值。请记住，抽取某个特定𝛼值的概率与该值在此分布中定义的概率有关：
- en: '[PRE57]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We set a random state to ensure reproducible results.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置一个随机状态以确保结果的可重复性。
- en: 'Plotting a histogram of the sample, as shown in the following figure, reveals
    a shape that approximately conforms to the distribution that we have sampled from.
    Note that as your sample sizes increases, the more the histogram conforms to the
    distribution:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制样本的直方图，如下图所示，可以揭示出一个大致符合我们所采样的分布的形状。请注意，随着样本数量的增加，直方图将更符合该分布：
- en: '[PRE58]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output will be as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 8.21: Visualization of the sample distribution'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.21：样本分布的可视化'
- en: '](img/B15019_08_21.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_21.jpg)'
- en: 'Figure 8.21: Visualization of the sample distribution'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.21：样本分布的可视化
- en: A model will then be fitted for each value of 𝛼 sampled and assessed for performance.
    As we have seen with the other approaches to hyperparameter tuning in this chapter,
    performance will be assessed using k-fold cross-validation (with `k =10`) but
    because we are dealing with a regression problem, the performance metric will
    be the test-set negative MSE.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将为每个采样的𝛼值拟合一个模型并评估其性能。正如我们在本章中其他超参数调整方法中看到的，性能将通过k折交叉验证（`k = 10`）来评估，但因为我们处理的是回归问题，所以评估指标将是测试集的负均方误差（MSE）。
- en: 'Using this metric means larger values are better. We will store the results
    in a dictionary with each 𝛼 value as the key and the corresponding cross-validated
    negative MSE as the value:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此度量意味着较大的值更好。我们将结果存储在一个字典中，每个 𝛼 值作为键，对应的交叉验证负 MSE 作为值：
- en: '[PRE59]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Instead of examining the raw dictionary of results, we will convert it to a
    pandas DataFrame, transpose it, and give the columns names. Sorting by descending
    negative mean squared error reveals that the optimal level of regularization for
    this problem is actually when 𝛼 is approximately 1, meaning that we did not find
    evidence to suggest regularization is necessary for this problem and that the
    OLS linear model will suffice:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再检查原始的结果字典，而是将其转换为 pandas DataFrame，转置并为列命名。按负均方误差降序排序显示，对于此问题，正则化的最佳水平实际上是在
    𝛼 约为 1 时，即我们没有找到证据表明该问题需要正则化，OLS 线性模型足以解决该问题：
- en: '[PRE60]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output will be as follows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 8.22: Output for the random search process'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.22：随机搜索过程的输出'
- en: '](img/B15019_08_22.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_22.jpg)'
- en: 'Figure 8.22: Output for the random search process'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.22：随机搜索过程的输出
- en: Note
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The results will be different, depending on the data used.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 结果会有所不同，取决于所使用的数据。
- en: 'It is always beneficial to visualize results where possible. Plotting 𝛼 by
    negative mean squared error as a scatter plot makes it clear that venturing away
    from 𝛼 = 1 does not result in improvements in predictive performance:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 只要可能，进行结果可视化总是有益的。将 𝛼 与负均方误差绘制成散点图清晰地表明，远离 𝛼 = 1 并不会改善预测性能：
- en: '[PRE61]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output will be as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 8.23: Plotting the scatter plot'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.23：绘制散点图'
- en: '](img/B15019_08_23.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_23.jpg)'
- en: 'Figure 8.23: Plotting the scatter plot'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.23：绘制散点图
- en: The fact that we found the optimal 𝛼 to be 1 (its default value) is a special
    case in hyperparameter tuning in that the optimal hyperparameterization is the
    default one.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现最优的 𝛼 值为 1（其默认值），这在超参数调优中是一个特殊情况，因为最优的超参数配置恰好是默认值。
- en: Tuning Using RandomizedSearchCV
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 RandomizedSearchCV 进行调优
- en: 'In practice, we can use the `RandomizedSearchCV` method inside scikit-learn''s
    `model_selection` module to conduct the search. All you need to do is pass in
    your estimator, the hyperparameters you wish to tune along with their distributions,
    the number of samples you would like to sample from each distribution, and the
    metric by which you would like to assess model performance. These correspond to
    the `param_distributions`, `n_iter`, and `scoring` arguments respectively. For
    the sake of demonstration, let''s conduct the search we completed earlier using
    `RandomizedSearchCV`. First, we load the data and initialize our ridge regression
    estimator:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以使用 scikit-learn 的 `model_selection` 模块中的 `RandomizedSearchCV` 方法进行搜索。你只需要传入估计器、你希望调优的超参数及其分布、你希望从每个分布中抽取的样本数量，以及你希望评估模型性能的度量标准。这些对应于
    `param_distributions`、`n_iter` 和 `scoring` 参数。为了演示，我们使用 `RandomizedSearchCV` 进行之前完成的搜索。首先，我们加载数据并初始化我们的岭回归估计器：
- en: '[PRE62]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We then specify that the hyperparameter we would like to tune is `alpha` and
    that we would like 𝛼 to be distributed `gamma`, with `k = 1` and `𝜃` `= 1`:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们指定希望调优的超参数是 `alpha`，并且我们希望 𝛼 按 `gamma` 分布，`k = 1` 且 `𝜃` `= 1`：
- en: '[PRE63]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Next, we set up and run the random search process, which will sample 100 values
    from our `gamma(1,1)` distribution, fit the ridge regression, and evaluate its
    performance using cross-validation scored on the negative mean squared error metric:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置并运行随机搜索过程，它将从我们的 `gamma(1,1)` 分布中抽取 100 个值，拟合岭回归并使用交叉验证评估其性能，评分标准是负均方误差：
- en: '[PRE64]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'After completing the search, we can extract the results and generate a pandas
    DataFrame, as we have done previously. Sorting by `rank_test_score` and viewing
    the first five rows aligns with our conclusion that alpha should be set to 1 and
    regularization does not seem to be required for this problem:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 完成搜索后，我们可以提取结果并生成一个 pandas DataFrame，正如我们之前所做的那样。按 `rank_test_score` 排序并查看前五行与我们的结论一致，即
    alpha 应设置为 1，并且该问题似乎不需要正则化：
- en: '[PRE65]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The output will be as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 8.24: Output for tuning using RandomizedSearchCV'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.24：使用 RandomizedSearchCV 调优的输出'
- en: '](img/B15019_08_24.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_24.jpg)'
- en: 'Figure 8.24: Output for tuning using RandomizedSearchCV'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.24：使用 RandomizedSearchCV 调优的输出
- en: Note
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding results may vary, depending on the data.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的结果可能会有所不同，取决于数据。
- en: 'Exercise 8.03: Random Search Hyperparameter Tuning for a Random Forest Classifier'
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 8.03：随机搜索超参数调优，应用于随机森林分类器
- en: In this exercise, we will revisit the handwritten digit classification problem,
    this time using a random forest classifier with hyperparameters tuned using a
    random search strategy. The random forest is a popular method used for both single-class
    and multi-class classification problems. It learns by growing `n` simple tree
    models that each progressively split the dataset into areas that best separate
    the points of different classes.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将重新审视手写数字分类问题，这次使用随机森林分类器，并通过随机搜索策略调优超参数。随机森林是一种常用的方法，适用于单类和多类分类问题。它通过生成
    `n` 个简单的树模型来学习，每棵树逐步将数据集分割成能够最佳分离不同类别数据点的区域。
- en: The final model produced can be thought of as the average of each of the n tree
    models. In this way, the random forest is an `ensemble` method. The parameters
    we will tune in this exercise are `criterion` and `max_features`.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 最终生成的模型可以看作是每个树模型的平均值。通过这种方式，随机森林是一种 `集成` 方法。我们将在这个练习中调优的参数是 `criterion` 和 `max_features`。
- en: '`criterion` refers to the way in which each split is evaluated from a class
    purity perspective (the purer the splits, the better) and `max_features` is the
    maximum number of features the random forest can use when finding the best splits.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '`criterion` 是指从类纯度角度评估每次分裂的方式（分裂越纯净越好），而 `max_features` 是随机森林在寻找最佳分裂时可以使用的最大特征数量。'
- en: The following steps will help you complete the exercise.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习。
- en: Create a new notebook in Google Colab.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Google Colab 中创建一个新的笔记本。
- en: 'Import the data and isolate the features `X` and the target `y`:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据并提取特征 `X` 和目标 `y`：
- en: '[PRE66]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Initialize the random forest classifier estimator. We will set the `n_estimators`
    hyperparameter to `100`, which means the predictions of the final model will essentially
    be an average of `100` simple tree models. Note the use of a random state to ensure
    the reproducibility of results:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化随机森林分类器估算器。我们将 `n_estimators` 超参数设置为 `100`，这意味着最终模型的预测基本上是 `100` 个简单树模型的平均值。请注意使用随机状态来确保结果的可重复性：
- en: '[PRE67]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'One of the parameters we will be tuning is `max_features`. Let''s find out
    the maximum value this could take:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将调优的参数之一是 `max_features`。让我们找出它可以取的最大值：
- en: '[PRE68]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'You should see that we have 64 features:'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到我们有 64 个特征：
- en: '[PRE69]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Now that we know the maximum value of `max_features` we are free to define our
    hyperparameter inputs to the randomized search process. At this point, we have
    no reason to believe any particular value of `max_features` is more optimal.
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们知道了 `max_features` 的最大值，我们可以自由地定义超参数输入到随机搜索过程中。此时，我们没有理由认为 `max_features`
    的某个特定值更加优化。
- en: 'Set a discrete uniform distribution covering the range `1` to `64`. Remember
    the probability mass function, `P(X=x) = 1/n`, for this distribution, so `P(X=x)
    = 1/64` in our case. Because `criterion` has only two discrete options, this will
    also be sampled as a discrete uniform distribution with `P(X=x) = ½`:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个覆盖 `1` 到 `64` 范围的离散均匀分布。记住概率质量函数 `P(X=x) = 1/n`，因此在我们的情况下 `P(X=x) = 1/64`。由于
    `criterion` 只有两个离散选项，因此它也将作为一个离散均匀分布进行采样，`P(X=x) = ½`：
- en: '[PRE70]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We now have everything we need to set up the randomized search process. As
    before, we will use accuracy as the metric of model evaluation. Note the use of
    a random state:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了设置随机搜索过程所需的一切。如前所述，我们将使用准确率作为模型评估的指标。请注意使用随机状态：
- en: '[PRE71]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Let''s kick off the process with the. `fit` method. Please note that both fitting
    random forests and cross-validation are computationally expensive processes due
    to their internal processes of iteration. Generating a result may take some time:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过 `fit` 方法启动过程。请注意，由于随机森林的内部迭代过程，拟合随机森林和交叉验证都是计算量大的过程。生成结果可能需要一些时间：
- en: '[PRE72]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'You should see the following:'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到如下内容：
- en: '![Figure 8.25: RandomizedSearchCV results'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.25：RandomizedSearchCV 结果'
- en: '](img/B15019_08_25.jpg)'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_25.jpg)'
- en: 'Figure 8.25: RandomizedSearchCV results'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.25：RandomizedSearchCV 结果
- en: 'Next, you need to examine the results. Create a `pandas` DataFrame from the
    `results` attribute, order by the `rank_test_score`, and look at the top five
    model hyperparameterizations. Note that because the random search draws samples
    of hyperparameterizations at random, it is possible to have duplication. We remove
    the duplicate entries from the DataFrame:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您需要检查结果。从`results`属性创建一个`pandas` DataFrame，按`rank_test_score`排序，并查看排名前五的模型超参数配置。请注意，由于随机搜索是随机抽取超参数配置，因此可能会出现重复项。我们从DataFrame中移除重复的条目：
- en: '[PRE73]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'You should get the following output:'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该会得到以下输出：
- en: '![Figure 8.26: Top five hyperparameterizations'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.26：前五名超参数配置'
- en: '](img/B15019_08_26.jpg)'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_26.jpg)'
- en: 'Figure 8.26: Top five hyperparameterizations'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.26：前五名超参数配置
- en: Note
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You may get slightly different results. However, the values you obtain should
    largely agree with those in the preceding output.
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可能会得到略有不同的结果。不过，您获得的值应该与前述输出中的值大体一致。
- en: 'The last step is to visualize the result. Including every parameterization
    will result in a cluttered plot, so we will filter on parameterizations that resulted
    in a mean test score > 0.93:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是可视化结果。包括所有参数配置会导致图表杂乱，因此我们将筛选出那些平均测试得分 > 0.93 的参数配置：
- en: '[PRE74]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output will be as follows:'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 8.27: Visualizing the test scores of the top-performing models'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.27：可视化表现最佳模型的测试得分'
- en: '](img/B15019_08_27.jpg)'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_08_27.jpg)'
- en: 'Figure 8.27: Visualizing the test scores of the top-performing models'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.27：可视化表现最佳模型的测试得分
- en: Note
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2uDVct8](https://packt.live/2uDVct8).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2uDVct8](https://packt.live/2uDVct8)。
- en: You can also run this example online at [https://packt.live/3gbQMvw](https://packt.live/3gbQMvw).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在网上运行此示例，访问[https://packt.live/3gbQMvw](https://packt.live/3gbQMvw)。
- en: We have found the best hyperparameterization to be a random forest classifier
    using the `gini` criterion with the maximum features set to 4.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现最好的超参数配置是使用`gini`准则的随机森林分类器，最大特征数设置为4。
- en: Advantages and Disadvantages of a Random Search
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机搜索的优缺点
- en: Because a random search takes a finite sample from a range of possible hyperparameterizations
    (`n_iter` in `model_selection.RandomizedSearchCV`), it is feasible to expand the
    range of your hyperparameter search beyond what would be practical with a grid
    search. This is because a grid search has to try everything in the range, and
    setting a large range of values may be too slow to process. Searching this wider
    range gives you the chance of discovering a truly optimal solution.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机搜索从可能的超参数配置范围中抽取有限的样本（在`model_selection.RandomizedSearchCV`中为`n_iter`），因此可以将超参数搜索的范围扩展到超出网格搜索实际可行的范围。这是因为网格搜索必须尝试该范围内的所有可能性，设置一个较大的值范围可能会导致处理速度过慢。搜索更广泛的范围使您有机会发现真正的最优解。
- en: Compared to the manual and grid search strategies, you do sacrifice a level
    of control to obtain this benefit. The other consideration is that setting up
    random search is a bit more involved than other options in that you have to specify
    distributions. There is always a chance of getting this wrong. That said, if you
    are unsure about what distributions to use, stick with discrete or continuous
    uniform for the respective variable types as this will assign an equal probability
    of selection to all options.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动搜索和网格搜索策略相比，您确实牺牲了一定的控制力以获得这一好处。另一个需要考虑的问题是，设置随机搜索比其他选项稍微复杂一些，因为您必须指定分布。总是有出错的可能性。也就是说，如果您不确定使用什么分布，可以选择离散或连续均匀分布，针对各个变量类型，这将为所有选项分配相等的选择概率。
- en: 'Activity 8.01: Is the Mushroom Poisonous?'
  id: totrans-438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 8.01：蘑菇有毒吗？
- en: Imagine you are a data scientist working for the biology department at your
    local university. Your colleague who is a mycologist (a biologist who specializes
    in fungi) has requested that you help her develop a machine learning model capable
    of discerning whether a particular mushroom species is poisonous or not given
    attributes relating to its appearance.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您是一名数据科学家，在当地大学的生物学系工作。您的同事是一位专门研究真菌的生物学家（真菌学家），她请求您帮助她开发一个机器学习模型，能够根据蘑菇外观的相关特征判断某个蘑菇物种是否有毒。
- en: The objective of this activity is to employ the grid and randomized search strategies
    to find an optimal model for this purpose.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的目标是使用网格搜索和随机搜索策略来找到一个最优模型。
- en: Note
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The dataset to be used in this exercise can be found on our GitHub repository
    at [https://packt.live/38zdhaB](https://packt.live/38zdhaB).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习所用的数据集可以在我们的GitHub仓库找到：[https://packt.live/38zdhaB](https://packt.live/38zdhaB)。
- en: 'Details on the attributes of the dataset can be found on the original dataset
    site: [https://packt.live/36j0jfA](https://packt.live/36j0jfA).'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的属性详情可以在原始数据集网站找到：[https://packt.live/36j0jfA](https://packt.live/36j0jfA)。
- en: Load the data into Python using the `pandas.read_csv()` method, calling the
    object `mushrooms`.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas.read_csv()`方法将数据加载到Python中，命名为`mushrooms`对象。
- en: 'Hint: The dataset is in CSV format and has no header. Set `header=None` in
    `pandas.read_csv()`.'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示：数据集是CSV格式且没有表头。在`pandas.read_csv()`中设置`header=None`。
- en: Separate the target, `y` and features, `X` from the dataset.
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集中分离出目标变量`y`和特征`X`。
- en: 'Hint: The target can be found in the first column (`mushrooms.iloc[:,0]`) and
    the features in the remaining columns (`mushrooms.iloc[:,1:]`).'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示：目标变量可以在第一列中找到（`mushrooms.iloc[:,0]`），特征则位于其余列（`mushrooms.iloc[:,1:]`）。
- en: Recode the target, `y`, so that poisonous mushrooms are represented as `1` and
    edible mushrooms as `0`.
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新编码目标变量`y`，使有毒蘑菇表示为`1`，可食用蘑菇表示为`0`。
- en: Transform the columns of the feature set `X` into a `numpy` array with a binary
    representation. This is known as one-hot encoding.
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征集`X`的列转换为具有二进制表示的`numpy`数组。这称为独热编码（one-hot encoding）。
- en: 'Hint: Use `preprocessing.OneHotEncoder()` to transform `X`.'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示：使用`preprocessing.OneHotEncoder()`将`X`转换。
- en: Conduct both a grid and random search to find an optimal hyperparameterization
    for a random forest classifier. Use accuracy as your method of model evaluation.
    Make sure that when you initialize the classifier and when you conduct your random
    search, `random_state = 100`.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行网格搜索和随机搜索，以找到随机森林分类器的最佳超参数配置。使用准确率作为模型评估方法。确保在初始化分类器时，以及进行随机搜索时，`random_state
    = 100`。
- en: 'For the grid search, use the following:'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于网格搜索，使用以下内容：
- en: '[PRE75]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'For the randomized search, use the following:'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于随机搜索，使用以下内容：
- en: '[PRE76]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Plot the mean test score versus hyperparameterization for the top 10 models
    found using random search.
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制随机搜索找到的前10个模型的平均测试分数与超参数化的关系图。
- en: 'You should see a plot similar to the following:'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到一个类似下面的图表：
- en: '![Figure 8.28: Mean test score plot'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.28：平均测试分数图'
- en: '](img/B15019_08_28.jpg)'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_08_28.jpg)'
- en: 'Figure 8.28: Mean test score plot'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.28：平均测试分数图
- en: Note
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The solution to the activity can be found here: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 活动的解决方案可以在这里找到：[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。
- en: Summary
  id: totrans-463
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have covered three strategies for hyperparameter tuning
    based on searching for estimator hyperparameterizations that improve performance.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们介绍了基于搜索估计器超参数化以提高性能的三种超参数调优策略。
- en: The manual search is the most hands-on of the three but gives you a unique feel
    for the process. It is suitable for situations where the estimator in question
    is simple (a low number of hyperparameters).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 手动搜索是三种方法中最具操作性的，但可以让你对整个过程有独特的理解。它适用于估算器较简单（超参数较少）的情况。
- en: The grid search is an automated method that is the most systematic of the three
    but can be very computationally intensive to run when the range of possible hyperparameterizations
    increases.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索是一种自动化方法，是三种方法中最系统化的，但当可能的超参数范围增大时，运行时可能会非常消耗计算资源。
- en: The random search, while the most complicated to set up, is based on sampling
    from distributions of hyperparameters, which allows you to expand the search range,
    thereby giving you the chance to discover a good solution that you may miss with
    the grid or manual search options.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索虽然设置最为复杂，但它基于从超参数分布中进行采样，这使得你能够扩大搜索范围，从而增加发现优秀解的机会，而这些可能在网格搜索或手动搜索中被错过。
- en: In the next chapter, we will be looking at how to visualize results, summarize
    models, and articulate feature importance and weights.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习如何可视化结果、总结模型并阐述特征的重要性和权重。
