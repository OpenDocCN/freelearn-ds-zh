- en: Chapter 10. Modeling Stock Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。股票数据建模
- en: Automated stock analysis has gotten a lot of press recently. High-frequency
    trading firms are a flashpoint. People either believe that they're great for the
    markets and increasing liquidity, or that they're precursors to the apocalypse.
    Smaller traders have also gotten into the mix in a slower fashion. Some sites,
    such as Quantopian ([https://www.quantopian.com/](https://www.quantopian.com/))
    and AlgoTrader ([http://www.algotrader.ch/](http://www.algotrader.ch/)) provide
    services that allow you to create models for automated trading. Many others allow
    you to use automated analysis to inform your trading decisions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化股票分析最近受到了很多关注。高频交易公司是一个热点。人们要么认为它们对市场有利，增加了流动性，要么认为它们是末日的前兆。较小的交易者也以较慢的速度加入了进来。一些网站，如Quantopian
    ([https://www.quantopian.com/](https://www.quantopian.com/)) 和 AlgoTrader ([http://www.algotrader.ch/](http://www.algotrader.ch/))
    提供了允许您创建自动化交易模型的服务。许多其他服务允许您使用自动化分析来指导您的交易决策。
- en: Whatever your view of this phenomena, it's an area with a lot of data begging
    to be analyzed. It's also a nice domain in which to experiment with some analysis
    and machine learning techniques.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你对这种现象的看法如何，它都是一个数据量庞大、亟待分析的区域。它也是一个很好的领域，可以尝试一些分析和机器学习技术。
- en: For this chapter, we're going to look for relationships between news articles
    and stock prices in the future.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将寻找未来新闻文章和股票价格之间的关系。
- en: 'In the course of this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的进程中，我们将涵盖以下主题：
- en: Learn about financial data analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习金融数据分析
- en: Set up our project and acquire our data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置我们的项目和获取我们的数据
- en: Prepare the data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据
- en: Analyze the text
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析文本
- en: Analyze the stock prices
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析股票价格
- en: Learn patterns in both text and stock prices with neural networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络在文本和股票价格中学习模式
- en: Use this system to predict the future
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用此系统预测未来
- en: Talk about the limitations of these systems
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论这些系统的局限性
- en: Learning about financial data analysis
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解金融数据分析
- en: Finance has always relied heavily on data. Earnings statements, forecasting,
    and portfolio management are just some of the areas that make use of data to quantify
    their decisions. Because of this, financial data analysis and its related field,
    financial engineering, are extremely broad fields that are difficult to summarize
    in a short amount of space.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 金融业一直高度依赖数据。收益报告、预测和投资组合管理只是利用数据量化决策的一些领域。因此，金融数据分析及其相关领域——金融工程——是极其广泛的领域，难以在有限的空间内总结。
- en: However, lately, quantitative finance, high-frequency trading, and similar fields
    have gotten a lot of press and really come into their own. As I mentioned, some
    people hate them and the added volatility that the markets seem to have. Others
    maintain that they bring the necessary liquidity that helps the market function
    better.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最近，量化金融、高频交易和类似领域受到了很多关注，并真正地展现了自己的实力。正如我提到的，有些人讨厌它们以及市场似乎增加的波动性。其他人则认为，它们带来了必要的流动性，有助于市场更好地运行。
- en: All of these fields apply statistical or machine learning methods to financial
    data. Some of these techniques can be quite simple. Others are more sophisticated.
    Some of these analyses are used to inform a human analyst or manager to make better
    financial decisions. Others are used as inputs to automated algorithmic processes
    that operate with varying degrees of human oversight, but perhaps with little
    to no intervention.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些领域都将统计或机器学习方法应用于金融数据。其中一些技术可能相当简单。其他则更为复杂。这些分析中的一些被用于向分析师或管理者提供信息，以做出更好的金融决策。其他则作为输入，用于自动化算法流程，这些流程在人类监督下以不同的程度运行，但可能几乎没有干预。
- en: For this chapter, we'll focus on adding information to the human analyst's repertoire.
    We'll develop a simple machine learning system to look at past, current, and future
    stock prices, alongside the text of news articles, in order to identify potentially
    interesting articles that may indicate future fluctuations in stock price. These
    articles, with the possible future price vector, could provide important information
    to an investor or analyst attempting to decide how to shuffle his/her money around.
    We'll talk more about the purpose and limitations of this system toward the end
    of the chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将专注于向人类分析师的技能库中添加信息。我们将开发一个简单的机器学习系统，用于分析过去、现在和未来的股票价格，以及新闻文章的文本，以识别可能表明股票价格未来波动的有趣文章。这些文章，以及可能的价格向量，可以为试图决定如何调整资金的投资者或分析师提供重要信息。我们将在本章末尾更多地讨论这个系统的目的和局限性。
- en: Setting up the basics
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置基础
- en: Before we really dig into the project and the data, we need to prepare. We'll
    set up the code and the library, and then we'll download the data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们真正深入项目和数据之前，我们需要做好准备。我们将设置代码和库，然后下载数据。
- en: Setting up the library
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置库
- en: First, we'll need to initialize the library. We can do this using Leiningen
    2 ([http://leiningen.org/](http://leiningen.org/)) and Stuart Sierra's reloaded
    plugin for it ([https://github.com/stuartsierra/reloaded](http://https://github.com/stuartsierra/reloaded)).
    This will initialize the development environment and project.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要初始化库。我们可以使用Leiningen 2 ([http://leiningen.org/](http://leiningen.org/))和Stuart
    Sierra的reloaded插件来完成此操作([https://github.com/stuartsierra/reloaded](http://https://github.com/stuartsierra/reloaded))。这将初始化开发环境和项目。
- en: 'To do this, just execute the following command at the prompt (I''ve named the
    project `financial` in this case):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要这样做，只需在提示符下执行以下命令（在这个例子中，我将项目命名为`financial`）：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we can specify the libraries that we''ll need to use. We can do this in
    the `project.clj` file. Open it and replace its current contents with the following
    lines:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以指定我们将需要使用的库。我们可以在`project.clj`文件中这样做。打开它，并用以下行替换其当前内容：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The primary library that we'll use is Enclog ([https://github.com/jimpil/enclog](https://github.com/jimpil/enclog)).
    This is a Clojure wrapper around the Java library Encog ([http://www.heatonresearch.com/encog](http://www.heatonresearch.com/encog)),
    which is a machine learning library, including classes for artificial neural networks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的主要库是Enclog ([https://github.com/jimpil/enclog](https://github.com/jimpil/enclog))。这是一个围绕Java库Encog
    ([http://www.heatonresearch.com/encog](http://www.heatonresearch.com/encog))的Clojure包装器，Encog是一个机器学习库，包括人工神经网络的类。
- en: We now have the basics in place. We can get the data at this point.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经建立了基础。我们现在可以获取数据了。
- en: Getting the data
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据
- en: We'll need data from two different sources. To begin with, we'll focus on getting
    the stock data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从两个不同的来源获取数据。首先，我们将专注于获取股票数据。
- en: In this case, we're going to use the historical stock data for Dominion Resources,
    Inc. They're a power company that operates in the eastern United States. Their
    New York Stock Exchange symbol is D. Focusing on one stock like this will reduce
    possible noise and allow us to focus on the simple system that we'll be working
    on in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用多米尼恩资源公司（Dominion Resources, Inc.）的历史股票数据。他们是一家在美国东部运营的电力公司。他们的纽约证券交易所符号是D。专注于这样的单一股票将减少可能的噪声，并使我们能够专注于本章我们将要工作的简单系统。
- en: To download the stock data, I went to Google Finance ([https://finance.google.com/](https://finance.google.com/)).
    In the search box, I entered `NYSE:D`. On the left-hand side menu bar, there is
    an option to download **Historical prices**. Click on it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载股票数据，我去了谷歌财经([https://finance.google.com/](https://finance.google.com/))。在搜索框中，我输入了`NYSE:D`。在左侧菜单栏中，有一个下载**历史价格**的选项。点击它。
- en: 'In the table header, set the date range to be from `Sept 1, 1995` to `Jan 1,
    2001`. Refer to the following screenshot as an example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在表头中，设置日期范围为从`1995年9月1日`到`2001年1月1日`。参考以下截图作为示例：
- en: '![Getting the data](img/4139OS_10_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![获取数据](img/4139OS_10_01.jpg)'
- en: If you look at the lower-right corner of the screenshot, there's a link that
    reads **Download to spreadsheet**. Click on this link to download the data. By
    default, the filename is `d.csv`. I moved it into a directory named `d` inside
    my project folder and renamed it to `d-1995-2001.csv`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看截图的右下角，有一个链接，上面写着**下载到电子表格**。点击此链接以下载数据。默认文件名为`d.csv`。我将它移动到项目文件夹内的名为`d`的目录中，并将其重命名为`d-1995-2001.csv`。
- en: We'll also need some news article data to correlate with the stock data. Freely
    available news articles are difficult to come by. There are good corpora available
    for modest fees (several hundred dollars). However, in order to make this exercise
    as accessible as possible, I've limited the data to what's freely available.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一些新闻文章数据来与股票数据相关联。免费获取的新闻文章很难找到。有一些好的语料库可供适度付费（几百美元）。然而，为了尽可能使这个练习易于访问，我已经将数据限制为免费可用的。
- en: At the moment, the best collection appears to be the journalism segment of the
    Open American National Corpus ([http://www.anc.org/data/oanc/](http://www.anc.org/data/oanc/)).
    The **American National Corpus** (**ANC**) is a collection of texts from a variety
    of registers and genres that are assembled for linguistic research. The **Open
    ANC** (**OANC**) is the subset of the ANC that is available for open access downloading.
    The journalism genre is represented by articles from Slate ([http://www.slate.com/](http://www.slate.com/)).
    This has some benefits and introduces some problems. The primary benefit is that
    the data will be quite manageable. It means that we won't have a lot of documents
    to use for training and testing, and we'll need to be pickier about what features
    we pull from the documents. We'll see how we need to handle this later.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，最好的集合似乎是开放美国国家语料库（[http://www.anc.org/data/oanc/](http://www.anc.org/data/oanc/)）的新闻部分。**美国国家语料库**（**ANC**）是一个包含各种语域和体裁的文本集合，这些文本是为了语言学研究而汇编的。**开放
    ANC**（**OANC**）是可供公开下载的 ANC 的子集。新闻体裁由 Slate 的文章（[http://www.slate.com/](http://www.slate.com/)）代表。这有一些好处也带来了一些问题。主要好处是数据将非常易于管理。这意味着我们不会有很多文档用于训练和测试，我们需要对从文档中提取的特征更加挑剔。我们稍后会看到如何处理这个问题。
- en: To download the dataset, visit the download page at [http://www.anc.org/data/oanc/download/](http://www.anc.org/data/oanc/download/)
    and get the data in your preferred format, either a TAR ball or a ZIP file. I
    decompressed that data into the `d` directory. It created a directory named `OANC-GrAF`
    that contained the data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载数据集，请访问下载页面 [http://www.anc.org/data/oanc/download/](http://www.anc.org/data/oanc/download/)
    并以你喜欢的格式获取数据，无论是 TAR 包还是 ZIP 文件。我已经将数据解压缩到 `d` 目录中。它创建了一个名为 `OANC-GrAF` 的目录，其中包含数据。
- en: 'Your `d` directory should now look something as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 `d` 目录现在应该看起来像以下这样：
- en: '![Getting the data](img/4139OS_10_02.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![获取数据](img/4139OS_10_02.jpg)'
- en: Getting prepared with data
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: As usual, now we need to clean up the data and put it into a shape that we can
    work with. The news article dataset particularly will require some attention,
    so let's turn our attention to it first.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，现在我们需要清理数据并将其整理成我们可以处理的形式。特别是新闻文章数据集将需要一些关注，所以让我们首先关注它。
- en: Working with news articles
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理新闻文章
- en: 'The OANC is published in an XML format that includes a lot of information and
    annotations about the data. Specifically, this marks off:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: OANC 以 XML 格式发布，其中包含大量关于数据的信息和注释。具体来说，这包括：
- en: Sections and chapters
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分 和 章节
- en: Sentences
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子
- en: Words with part-of-speech lemma
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标有词性词元的单词
- en: Noun chunks
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名词块
- en: Verb chunks
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动词块
- en: Named entities
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体
- en: However, we want the option to use raw text later when the system is actually
    being used. Because of that, we will ignore the annotations and just extract the
    raw tokens. In fact, all we're really interested in is each document's text—either
    as a raw string or a feature vector—and the date it was published. Let's create
    a record type for this.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们希望有选项在系统实际使用时使用原始文本。因此，我们将忽略注释，只提取原始标记。实际上，我们真正感兴趣的是每篇文档的文本——无论是作为原始字符串还是特征向量——以及其发布日期。让我们为这个创建一个记录类型。
- en: 'We''ll put this into the `types.clj` file in `src/financial/`. Put this simple
    namespace header into the file:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个放入 `src/financial/` 目录下的 `types.clj` 文件中。将以下简单的命名空间标题放入文件中：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This data record will be similarly simple. It can be defined as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这条数据记录将同样简单。它可以定义为以下：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So let's see what the XML looks like and what we need to do to get it to work
    with the data structures we just defined.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 XML 的样子以及我们需要做什么才能让它与刚刚定义的数据结构一起工作。
- en: 'The Slate data is in the `OANC-GrAF/data/written_1/journal/slate/` directory.
    The data files are spread through 55 subdirectories as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Slate 数据位于 `OANC-GrAF/data/written_1/journal/slate/` 目录中。数据文件分布在以下 55 个子目录中：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Digging in deeper, each document is represented by a number of files. From
    the `slate` directory, we can see the following details:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 深入挖掘，每个文档都由多个文件表示。从 `slate` 目录中，我们可以看到以下详细信息：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So we can see the different annotations files are the files with the `xml` extension.
    The ANC file contains metadata about the file. We'll need to access that file
    for the date and other information. But most importantly, there's also a `.txt`
    file containing the raw text of the document. That will make working with this
    dataset much easier!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到不同的注释文件是具有 `xml` 扩展名的文件。ANC 文件包含有关文件的元数据。我们需要访问该文件以获取日期和其他信息。但最重要的是，还有一个包含文档原始文本的
    `.txt` 文件。这将使处理这个数据集变得容易得多！
- en: But let's take a minute to write some functions that will help us work with
    each document's text and its metadata as an entity. These will represent the knowledge
    we've just gained about the directory and file structure of the OANC corpus.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们花点时间编写一些函数，这些函数将帮助我们以实体形式处理每个文档的文本及其元数据。这些将代表我们刚刚获得的关于 OANC 语料库目录和文件结构的知识。
- en: 'We''ll call this file `src/financial/oanc.clj`, and its namespace header should
    look as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个文件命名为 `src/financial/oanc.clj`，其命名空间头部应该如下所示：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If we examine the directory structure that the OANC uses, we can see that it's
    divided into a clear hierarchy. Let's trace that structure in the `slate` directory
    that we discussed earlier, `OANC-GrAF/data/written_1/journal/slate/`. In this
    example, `written_1` represents a category, `journal` is a genre, and `slate`
    is a source. We can leverage this information as we walk the directory structure
    to get to the data files.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查 OANC 使用的目录结构，我们可以看到它被划分为一个清晰的层次结构。让我们在之前讨论的 `slate` 目录中追踪这个结构，`OANC-GrAF/data/written_1/journal/slate/`。在这个例子中，`written_1`
    代表一个类别，`journal` 是一个流派，而 `slate` 是一个源。我们可以利用这些信息在遍历目录结构时获取数据文件。
- en: 'Our first bit of code contains four functions. Let''s list them first, and
    then we can talk about them:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一段代码包含四个函数。让我们先列出它们，然后我们可以讨论它们：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The functions used in the preceding code are described as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 之前代码中使用的函数描述如下：
- en: The first of these functions, `list-category-genre`, takes a category directory
    (`OANC-GrAF/data/written_1/`) and returns the genres that it contains. This could
    be `journal`, as in our example here, or fiction, letters, or a number of other
    options. Each item returned is a hash map of the full directory and the name of
    the genre.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些函数中的第一个，`list-category-genre`，接受一个类别目录（`OANC-GrAF/data/written_1/`）并返回它包含的流派。这可能是我们的例子中的
    `journal`，也可能是小说、信件或其他许多选项之一。每个返回的项都是一个包含完整目录和流派名称的哈希表。
- en: The second function is `list-genres`. It lists all of the genres within the
    OANC data directory.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个函数是 `list-genres`。它列出 OANC 数据目录中的所有流派。
- en: The third function is `find-genre-dir`. It looks for one particular genre and
    returns the full directory for it.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个功能是 `find-genre-dir`。它查找一个特定的流派，并返回其完整目录。
- en: Finally, we have `find-source-data`. This takes a genre and source and lists
    all of the files with an `anc` extension.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们有 `find-source-data`。它接受一个流派和源，并列出所有具有 `anc` 扩展名的文件。
- en: 'Using these functions, we can iterate over the documents for a source. We can
    see how to do that in the next function, `find-slate-files`, which returns a sequence
    of maps pointing to each document''s metadata ANC file and to its raw text file,
    as shown in the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些函数，我们可以遍历源文档。我们可以在下一个函数 `find-slate-files` 中看到如何做到这一点，该函数返回指向每个文档的元数据 ANC
    文件和其原始文本文件的映射序列，如下面的代码所示：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we can get at the metadata in the ANC file. We''ll use the `clojure.data.xml`
    library to parse the file, and we''ll define a couple of utility functions to
    make descending into the file easier. Look at the following code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以获取 ANC 文件中的元数据。我们将使用 `clojure.data.xml` 库来解析文件，并定义几个实用函数以简化文件的下沉。看以下代码：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first utility function, `find-all`, lazily walks the XML document and returns
    all elements with a given tag name. The second function, `content-str`, returns
    all the text children of a tag.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实用函数 `find-all` 会懒加载地遍历 XML 文档，并返回所有具有给定标签名的元素。第二个函数 `content-str` 返回一个标签的所有文本子元素。
- en: Also, we'll need to parse the date from the `pubDate` elements. Some of these
    have a `value` attribute, but this isn't consistent. Instead, we'll parse the
    elements' content directly using the `clj-time` library ([https://github.com/clj-time/clj-time](https://github.com/clj-time/clj-time)),
    which is a wrapper over the Joda time library for Java ([http://joda-time.sourceforge.net/](http://joda-time.sourceforge.net/)).
    From our end, we'll use a few functions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要解析`pubDate`元素中的日期。其中一些有`value`属性，但并不一致。相反，我们将直接使用`clj-time`库（[https://github.com/clj-time/clj-time](https://github.com/clj-time/clj-time)）解析元素的内容，该库是Java的Joda时间库的包装器。从我们的角度来看，我们将使用一些函数。
- en: 'Before we do, though, we''ll need to define a date format string. The dates
    inside the `pubDate` functions look like *2/13/97 4:30:00 PM*. The formatting
    string, then, should look as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，我们需要定义一个日期格式字符串。`pubDate`函数中的日期看起来像*2/13/97 4:30:00 PM*。因此，格式化字符串应该如下所示：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can use this formatter to pull data out of a `pubDate` element and parse
    it into an `org.joda.time.DateTime` object as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个格式化器从`pubDate`元素中提取数据并将其解析为`org.joda.time.DateTime`对象，如下所示：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Unfortunately, some of these dates are about 2000 years off. We can normalize
    the dates and correct these errors fairly quickly, as shown in the following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，其中一些日期大约有2000年的误差。我们可以快速归一化日期并纠正这些错误，如下所示：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'With all of these parts in place, we can write a function that takes the XML
    from an ANC file and returns date and time for the publication date as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些部分就绪后，我们可以编写一个函数，该函数从ANC文件中的XML提取并返回出版日期，如下所示：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The other piece of data that we''ll load from the ANC metadata XML is the title.
    We get that from the `title` element, of course, as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从ANC元数据XML中加载的另一份数据是标题。当然，我们从`title`元素中获取它，如下所示：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, loading a `NewsArticle` object is straightforward. In fact, it''s so simple
    that we''ll also include a version of this that reads in the text from a plain
    file. Look at the following code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，加载`NewsArticle`对象很简单。事实上，它如此简单，我们还将包括一个从普通文件中读取文本的版本。看看以下代码：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And using these functions to load all of the Slate articles just involves repeating
    the earlier steps, as shown in the following commands:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些函数来加载所有Slate文章只需重复之前的步骤，如下所示：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The last command in the preceding code just prints the title, publication date,
    and the length of the text in the document.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中的最后一个命令只是打印标题、出版日期和文档中的文本长度。
- en: With these functions in place, we now have access to the article dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些函数就绪后，我们现在可以访问文章数据集。
- en: Working with stock data
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理股票数据
- en: Loading the news articles was complicated. Fortunately, the stock price data
    is in **comma-separated values** (**CSV**) format. Although not the richest data
    format, it is very popular, and `clojure.data.csv` ([https://github.com/clojure/data.csv/](https://github.com/clojure/data.csv/))
    is an excellent library for loading it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 加载新闻文章很复杂。幸运的是，股价数据是**逗号分隔值**（**CSV**）格式。虽然不是最丰富的数据格式，但它非常流行，`clojure.data.csv`（[https://github.com/clojure/data.csv/](https://github.com/clojure/data.csv/））是一个用于加载它的优秀库。
- en: As I just mentioned, though, CSV isn't the richest data format. We will want
    to convert this data into a richer format, so we'll still create a record type
    and some wrapper functions to make it easier to work with the data as we read
    it in.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我刚才提到的，CSV并不是最丰富的数据格式。我们希望将此数据转换为更丰富的格式，因此我们仍然会创建一个记录类型和一些包装函数，以便在读取数据时更容易处理。
- en: 'The data in this will closely follow the columns in the CSV file that we downloaded
    from Google Finance earlier. Open `src/financial/types.clj` again and add the
    following line to represent the data type for the stock data:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里包含的数据将紧密遵循我们从Google Finance下载的CSV文件中的列。再次打开`src/financial/types.clj`并添加以下行以表示股票数据的数据类型：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'For the rest of the code in this section, we''ll use a new namespace. Open
    the `src/financial/cvs_data.clj` file and add the following namespace declaration:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节剩余的代码中，我们将使用一个新的命名空间。打开`src/financial/cvs_data.clj`文件并添加以下命名空间声明：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Just like the Slate news article data, this data also has a field with a date,
    which we''ll need to parse. Unlike the Slate data, this value is formatted differently.
    Glancing at the first few lines of the file gives us all the information that
    we need, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Slate新闻文章数据一样，此数据也有一个包含日期的字段，我们需要解析。与Slate数据不同，此值格式不同。浏览文件的前几行就给我们提供了所需的所有信息，如下所示：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To parse dates in this format (29-Dec-00), we can use the following format
    specification:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解析这种格式的日期（29-Dec-00），我们可以使用以下格式规范：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we build on this and a few other function—which you can find in the code
    download in the file `src/financial/utils.clj`—to create a `StockData` instance
    from a row of data, as shown in the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在此基础上以及几个其他函数（你可以在文件`src/financial/utils.clj`中的代码下载中找到）的基础上，从数据行创建一个`StockData`实例，如下面的代码所示：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This is all straightforward. Basically, every value in the row must be converted
    to a native Clojure/Java type, and then all of those values are used to create
    the `StockData` instance.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都是直截了当的。基本上，行中的每个值都必须转换为Clojure/Java的本地类型，然后使用所有这些值来创建`StockData`实例。
- en: 'To read in an entire file, we just do this for every row returned by the CSV
    library as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了读取整个文件，我们只需对CSV库返回的每一行这样做：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The only wrinkle is that we have to drop the first row, since it's the header.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的问题是，我们必须删除第一行，因为它是标题行。
- en: 'And now, to load the data, we just call the following function (we''ve aliased
    the `financial.csv-data` namespace to `csvd`):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了加载数据，我们只需调用以下函数（我们将`financial.csv-data`命名空间别名为`csvd`）：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Everything appears to be working correctly. Let's turn our attention back to
    the news article dataset and begin analyzing it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一切似乎都在正常工作。让我们将注意力转回到新闻文章数据集，并开始分析它。
- en: Analyzing the text
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析文本
- en: Our goal for analyzing the news articles is to generate a vector space model
    of the collection of documents. This attempts to pull the salient features for
    the documents into a vector of floating-point numbers. Features can be words or
    information from the documents' metadata encoded for the vector. The feature values
    can be 0 or 1 for presence, an integer for raw frequency, or the frequency scaled
    in some form.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析新闻文章的目标是生成文档集合的向量空间模型。这试图将文档的显著特征拉入一个浮点数字的向量中。特征可以是单词或为向量编码的文档元数据中的信息。特征值可以是表示存在的0或1，原始频率的整数，或者以某种形式缩放的频率。
- en: In our case, we'll use the feature vector to represent a selection of the tokens
    in a document. Often, we can use all the tokens, or all the tokens that occur
    more than once or twice. However, in this case, we don't have a lot of data, so
    we'll need to be more selective in the features that we include. We'll consider
    how we select these in a few sections.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将使用特征向量来表示文档中一组标记的选择。通常，我们可以使用所有标记，或者所有出现一次或两次以上的标记。然而，在这种情况下，我们没有很多数据，因此我们需要在包含的特征上更加选择性地进行。我们将在几个部分中考虑如何选择这些特征。
- en: For the feature values, we'll use a scaled version of the token frequency called
    **term frequency-inverse document frequency** (**tf-idf**). There are good libraries
    for this, but this is a basic metric in working with free text data, so we'll
    take this algorithm apart and implement it ourselves for this chapter. That way,
    we'll understand it better.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征值，我们将使用称为**词频-逆文档频率**（**tf-idf**）的标记频率的缩放版本。有很好的库可以做到这一点，但这是在处理自由文本数据时的一个基本指标，因此我们将分解此算法并自行实现它，在本章中。这样，我们将更好地理解它。
- en: 'For the rest of this section, we''ll put the code into `src/financial/nlp.clj`.
    Open this file and add the following for the namespace header:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将代码放入`src/financial/nlp.clj`。打开此文件，并为命名空间头添加以下内容：
- en: '[PRE24]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: With this in place, we can now start to pick the documents apart.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们现在可以开始分解文档。
- en: Analyzing vocabulary
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析词汇
- en: The first step for analyzing a document, of course, is tokenizing. We'll use
    a simple tokenize function that just pulls out sequences of letters or numbers,
    including any single punctuation marks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 分析文档的第一步，当然是对其进行分词。我们将使用一个简单的分词函数，该函数仅提取字母或数字的序列，包括任何单个标点符号。
- en: Now, we can use this function to see what words are present in the text and
    how frequent they are. The core Clojure function, `frequencies`, makes this especially
    easy, but we do still need to pull out the data that we'll use.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用此函数查看文本中存在哪些单词以及它们的频率。Clojure的核心函数`frequencies`使这变得特别容易，但我们仍然需要提取我们将使用的数据。
- en: For each step, we'll first work on raw input, and then we'll write an additional
    utility function that modifies the `:text` property of the input `NewsArticle`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个步骤，我们首先处理原始输入，然后编写一个额外的实用函数来修改输入`NewsArticle`的`:text`属性。
- en: 'To tokenize the text, we''ll search for the matches for a regular expression
    and convert the output to lowercase. This won''t work well for a lot of cases—contractions,
    abbreviations, and hyphenations in English, for example—but it will take care
    of simple needs. Look at the following code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要标记化文本，我们将搜索正则表达式的匹配项，并将输出转换为小写。这对于许多情况（例如英语中的缩写、缩写和连字符）可能不起作用，但它将处理简单的需求。查看以下代码：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The actual tokenization is handled by the `tokenize` function. The `tokenize-text`
    function takes a `NewsArticle` instance and replaces its raw text property with
    the sequence of tokens generated from the text.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的标记化处理由`tokenize`函数负责。`tokenize-text`函数接受一个`NewsArticle`实例，并用从文本生成的标记序列替换其原始文本属性。
- en: 'The function `token-freqs` replaces the sequence of tokens with a mapping of
    their frequencies. It uses the Clojure core function frequencies as shown in the
    following code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`token-freqs`函数将标记序列替换为它们的频率映射。它使用以下代码中显示的Clojure核心函数frequencies：'
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can then take a sequence of `NewsArticle` instances that contain the token
    frequencies and generate the frequencies for the entire corpus. The function `corpus-freqs`
    takes care of that. Look at the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以取一个包含标记频率的`NewsArticle`实例序列，并为整个语料库生成频率。`corpus-freqs`函数负责这一点。查看以下代码：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s use the following functions to get the frequencies:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下函数来获取频率：
- en: 'We''ll get the tokens for each article. Then we''ll print out the first ten
    tokens from the first article, as follows:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将获取每篇文章的标记。然后，我们将按如下方式打印出第一篇文章的前十个标记：
- en: '[PRE28]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we''ll get the frequencies of the tokens in each document and print out
    ten of the token-frequency pairs from the first document, as follows:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们将获取每个文档中标记的频率，并按如下方式打印出第一个文档中的前十个标记-频率对：
- en: '[PRE29]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we can reduce those down into one set of frequencies over the entire
    collection. I''ve pretty-printed out the top ten most frequent tokens. Look at
    the following code:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们可以将这些合并成整个集合中的一组频率。我已经以易读的格式输出了最频繁的前十个标记。查看以下代码：
- en: '[PRE30]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can see that the most frequent words are common words with little semantic
    value. In the next section, we'll see what we need to do with them.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，最常见的词是语义价值很小的常见词。在下一节中，我们将看到我们需要对它们做什么。
- en: Stop lists
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停用词表
- en: The words identified as the most common words in the code in the previous section
    are often referred to as **function** words, because they're performing functions
    in the sentence, but not really carrying meaning. For some kinds of analyses,
    such as grammatical and stylistic analyses, these are vitally important. However,
    for this particular chapter, we're more interested in the documents' content words,
    or the words that carry semantic meaning.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中识别出的代码中最常见的词通常被称为**函数词**，因为它们在句子中执行功能，但并不真正承载意义。对于某些类型的分析，例如语法和风格分析，这些词至关重要。然而，对于这一特定章节，我们更感兴趣的是文档的内容词，即承载语义意义的词。
- en: To filter these out, the typical technique is to use a stop word list. This
    is a list of common words to remove from the list of tokens.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了过滤掉这些词，典型的技术是使用停用词表。这是一个从标记列表中移除的常见词列表。
- en: If you type `english stop list` into Google, you'll get a lot of workable stop
    lists. I've downloaded one from [http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop](http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop).
    Download this file too, and place it into the `d` directory along with the data
    files.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将`english stop list`输入到Google中，你会得到很多可用的停用词表。我已从[http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop](http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop)下载了一个。也请下载此文件，并将其与数据文件一起放入`d`目录中。
- en: To load the stop words, we'll use the following function. It simply takes the
    filename and returns a set of the tokens in it.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载停用词，我们将使用以下函数。它简单地接受文件名，并返回其中的一组标记。
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Using this set directly is easy enough on raw strings. However, we''ll want
    a function to make calling it on `NewsArticle` instances easier. Look at the following
    code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始字符串上直接使用这个集合很容易。然而，我们将需要一个函数来使在`NewsArticle`实例上调用它更容易。查看以下代码：
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we can load those words and remove them from the lists of tokens. We''ll
    start with the definition of tokens that we just created. Look at the following
    code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以加载这些单词并将它们从标记列表中移除。我们将从我们刚刚创建的标记定义开始。看看下面的代码：
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: First, we can tell that we've removed a number of tokens that weren't really
    adding much. `You`, `re`, and `s` were all taken out, along with others.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以看出我们移除了一些实际上并没有增加多少信息的标记。`You`、`re`和`s`都被移除了，以及其他一些。
- en: 'Now let''s regenerate the corpus frequencies with the following code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用以下代码重新生成语料库频率：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This list seems much more reasonable. It focuses on Bill Clinton, who was the
    US President during this period.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表看起来更加合理。它专注于这个时期的美国总统比尔·克林顿。
- en: 'Another way of dealing with this is to use a white list. This would be a set
    of words or features that represent the entire collection of those that we want
    to deal with. We could implement this as a simple function, `keep-white-list`,
    as shown in the following code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种处理方法是使用白名单。这将是一组单词或特征，代表我们想要处理的整个集合。我们可以通过以下代码中的简单函数`keep-white-list`来实现这一点：
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This function seems academic now, but we'll need it before we're done.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数现在看起来很学术，但我们完成之前需要它。
- en: Hapax and Dis Legomena
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单次词和双次词
- en: 'Now, let''s look at a graph of the frequencies:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看频率的图表：
- en: '![Hapax and Dis Legomena](img/4139OS_10_03.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![单次词和双次词](img/4139OS_10_03.jpg)'
- en: That's a lot of words that don't occur very much. This is actually expected.
    A few words occur a lot, but most just don't.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些很少出现的单词有很多。这实际上是可以预料的。一些单词出现频率很高，但大多数单词只是没有出现。
- en: 'We can get another view of the data by looking at the log-log plot of the frequencies
    and ranks. Functions that represent a value raised to a power should be linear
    in these line charts. We can see that the relationship isn''t quite on a line
    in this plot, but it''s very close. Look at the following graph:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看频率和排名的对数-对数图来获取数据的另一种视角。表示值被提升到幂的函数在这些线图中应该是线性的。我们可以看到，这个图中的关系并不是完全在线上，但它非常接近。看看下面的图表：
- en: '![Hapax and Dis Legomena](img/4139OS_10_04.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![单次词和双次词](img/4139OS_10_04.jpg)'
- en: 'In fact, let''s turn the frequency mapping around in the following code to
    look at how often different frequencies occur:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，让我们在下面的代码中反转频率映射，看看不同频率出现的频率：
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: So there are more than 23,000 words that only occur once and more than 8,000
    words that only occur twice. Words like these are very interesting for authorship
    studies. The words that are found only once are referred to as **hapax legomena**,
    from Greek for "said once", and words that occur only twice are **dis legomena**.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有超过23,000个单词只出现了一次，还有超过8,000个单词只出现了两次。这样的单词对于作者研究来说非常有趣。只出现一次的单词被称为**单次词**，源自希腊语“说一次”，而只出现两次的单词被称为**双次词**。
- en: 'Looking at a random 10 hapax legomena gives us a good indication of the types
    of words these are. The 10 words are: shanties, merrifield, cyberguru, alighting,
    roomfor, sciaretto, borisyeltsin, vermes, fugs, and gandhian. Some of these appear
    to be unusual or rare words. Others are mistakes or two words that were joined
    together for some reason, possibly by a dash.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下随机的10个单次词，我们可以很好地了解这些词的类型。这10个单词是：shanties、merrifield、cyberguru、alighting、roomfor、sciaretto、borisyeltsin、vermes、fugs和gandhian。其中一些看起来是不寻常或罕见的单词。其他的是错误，或者可能是由于某种原因（可能是破折号）将两个单词合并在一起。
- en: Unfortunately, they do not contribute much to our study, since they don't occur
    often enough to contribute to the results statistically. In fact, we'll just get
    rid of any words that occur less than 10 times. This will form a second stop list,
    this time of rare words. Let's generate this list. Another, probably better performing,
    option is to create a whitelist of the words that aren't rare, but we can easily
    integrate this with our existing stop-list infrastructure, so we'll do it by just
    creating another list here.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，它们对我们的研究贡献不大，因为它们出现得不够频繁，无法在统计上产生影响。事实上，我们将移除任何出现次数少于10次的单词。这将形成一个第二停止列表，这次是罕见单词的列表。让我们生成这个列表。另一个可能表现更好的选项是创建一个不罕见的单词白名单，但我们可以轻松地将它与现有的停止列表基础设施集成，所以我们在这里创建另一个列表。
- en: 'To create it from the frequencies, we''ll define a `make-rare-word-list` function.
    It takes a frequency mapping and returns the items with fewer than *n* occurrences,
    as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要从频率中创建它，我们将定义一个`make-rare-word-list`函数。它接受一个频率映射并返回出现次数少于*n*的项，如下所示：
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can now use this function to generate the `d/english.rare` file. We can
    use this file just like we used the stop list to remove items that aren''t common
    and to further clean up the tokens that we''ll have to deal with (you can also
    find this file in the code download for this chapter):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用这个函数来生成`d/english.rare`文件。我们可以像使用停用词列表一样使用这个文件来移除不常见的项目，并进一步清理我们将要处理的标记（你也可以在这个章节的代码下载中找到这个文件）：
- en: '[PRE38]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now, we have a list of more than 48,000 tokens that will get removed. For perspective,
    after removing the common stop words, there were more than 71,000 token types.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将移除超过48,000个标记的列表。为了有更直观的了解，在移除常见停用词后，有超过71,000个标记类型。
- en: 'We can now use that just like we did for the previous stop word list. Starting
    from `filtered`, which we defined in the earlier code after removing the common
    stop words, we''ll now define `filtered2` and recalculate the frequencies as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用它，就像我们之前对停用词列表所做的那样。从`filtered`开始，这是我们之前在移除常见停用词后定义的，我们现在将定义`filtered2`并重新计算频率，如下所示：
- en: '[PRE39]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: So we can see that the process has removed some uncommon words, such as `harmonic`
    and `convergences`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到这个过程移除了一些不常见的单词，例如`harmonic`和`convergences`。
- en: 'This process is pretty piecemeal so far, but it''s one that we would need to
    do multiple times, probably. Many natural language processing and text analysis
    tasks begin by taking a text, converting it to a sequence of features (tokenization,
    normalization, and filtering), and then counting them. Let''s package that into
    one function as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这个过程相当零散，但这是我们可能需要多次执行的过程。许多自然语言处理和文本分析任务都是从取一个文本，将其转换为一系列特征（分词、归一化和过滤），然后进行计数开始的。让我们将这个过程封装到一个函数中，如下所示：
- en: '[PRE40]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The preceding function allows us to call it with just a list of articles. We
    can also specify a list of stop word files. The entries in all the lists are added
    together to create a master list of stop words. Then the articles' text is tokenized,
    filtered by the stop words, and counted. Doing it this way should save on creating
    and possibly hanging on to multiple lists of intermediate processing stages that
    we won't ever use later.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数允许我们仅使用文章列表来调用它。我们还可以指定一个停用词文件列表。所有列表中的条目都会被添加到一起，以创建一个主停用词列表。然后对文章的文本进行分词，通过停用词进行过滤，并计数。这样操作应该可以节省创建和可能保留多个中间处理阶段的列表，而这些列表我们以后可能永远不会使用。
- en: 'Now we can skip to the document-level frequencies with the following command:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下命令跳转到文档级别的频率：
- en: '[PRE41]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now that we''ve filtered these out, let''s look at the graph of token frequencies
    again:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经过滤掉了这些内容，让我们再次查看词频的图表：
- en: '![Hapax and Dis Legomena](img/4139OS_10_05.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![Hapax and Dis Legomena](img/4139OS_10_05.jpg)'
- en: The distribution stayed the same, as we would expect, but the number of words
    should be more manageable.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 分布保持不变，正如我们所预期的，但单词的数量应该更容易管理。
- en: 'Again, we can see from the following log-log plot that the previous power relationship—almost,
    but not quite, linear—holds for this frequency distribution as well:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以从以下对数-对数图中看到，之前的幂关系——几乎但不是完全线性——也适用于这个频率分布：
- en: '![Hapax and Dis Legomena](img/4139OS_10_06.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![Hapax and Dis Legomena](img/4139OS_10_06.jpg)'
- en: 'Another way to approach this would be to use a whitelist, as we mentioned earlier.
    We could load files and keep only the tokens that we''ve seen before by using
    the following function:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用白名单，正如我们之前提到的。我们可以加载文件，并使用以下函数仅保留我们之前看到的标记：
- en: '[PRE42]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Again, this will come up later. We'll find this necessary when we need to load
    unseen documents to analyze.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这将在稍后出现。当我们需要加载未见文档进行分析时，我们会发现这是必要的。
- en: TF-IDF
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: The frequencies as we currently have them will present a couple of difficulties.
    For one thing, if we have a document with 100 words and one with 500 words, we
    can't really compare the frequencies. For another thing, if a word occurs three
    times in every document, say in a header, it's not as interesting as one that
    occurs in only a few documents three times and nowhere else.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前拥有的频率将带来一些困难。一方面，如果我们有一个包含100个单词的文档和一个包含500个单词的文档，我们实际上无法比较频率。另一方面，如果一个单词在每个文档中只出现三次，比如在标题中，它不如在一个文档中只出现几次且其他地方没有出现的单词有趣。
- en: In order to work around both of these, we'll use a metric called **term frequency-inverse
    document frequency** (**TF-IDF**). This combines some kind of document-term frequency
    with the log of the percentage of documents that contain that term.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这两个问题，我们将使用一个称为 **词频-逆文档频率**（**TF-IDF**）的度量标准。这结合了某种文档-词频与包含该术语的文档百分比的对数。
- en: 'For the first part, term frequency, we could use a number of metrics. We could
    use a boolean 0 or 1 to show absence or presence. We could use the raw frequency
    or the raw frequency scaled. In this case, we''re going to use an augmented frequency
    that scales the raw frequency by the maximum frequency of any word in the document.
    Look at the following code:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一部分，即词频，我们可以使用多种度量标准。我们可以使用布尔值 0 或 1 来表示缺失或存在。我们可以使用原始频率或缩放后的原始频率。在这种情况下，我们将使用增强频率，该频率将原始频率按文档中任何单词的最大频率进行缩放。请看以下代码：
- en: '[PRE43]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The first function in the preceding code, `tf`, is a basic augmented frequency
    equation and takes the raw values as parameters. The second function, `tf-article`,
    wraps `tf` but takes a `NewsArticle` instance and a word and generates the TF
    value for that pair.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中的第一个函数 `tf` 是一个基本的增强频率方程，它将原始值作为参数。第二个函数 `tf-article` 包装了 `tf`，但它接受一个 `NewsArticle`
    实例和一个单词，并为这对生成 TF 值。
- en: For the second part of this equation, the inverse document frequency, we'll
    use the log of the total number of documents divided by the number of documents
    containing that term. We'll also add one to the last number to protect against
    division-by-zero errors.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个公式的第二部分，即逆文档频率，我们将使用总文档数除以包含该术语的文档数的对数。我们还将最后一个数字加一，以防止除以零错误。
- en: 'The `idf` function calculates the inverse document frequency for a term over
    the given corpus, as shown in the following code:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`idf` 函数计算给定语料库中术语的逆文档频率，如下面的代码所示：'
- en: '[PRE44]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The IDF for a word won''t change between different documents. Because of this,
    we can calculate all of the IDF values for all the words represented in the corpus
    once and cache them. The following two functions take care of this scenario:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的 IDF 值在不同文档之间不会改变。正因为如此，我们可以一次性计算出语料库中所有单词的 IDF 值并缓存它们。以下两个函数负责处理这种情况：
- en: '[PRE45]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The first function in the preceding code, `get-vocabulary`, returns a set of
    all the words used in the corpus. The next function, `get-idf-cache`, iterates
    over the vocabulary set to construct a mapping of the cached IDF values. We'll
    use this cache to generate the TF-IDF values for each document.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中的第一个函数 `get-vocabulary` 返回语料库中使用的所有单词的集合。下一个函数 `get-idf-cache` 遍历词汇集来构建缓存
    IDF 值的映射。我们将使用此缓存为每个文档生成 TF-IDF 值。
- en: 'The `tf-idf` function combines the output of `tf` and `idf` (via `get-idf-cache`)
    to calculate the TF-IDF value. In this case, we simply take the raw frequencies
    and the IDF value and multiply them together as shown in the following code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf-idf` 函数通过 `get-idf-cache` 结合 `tf` 和 `idf` 的输出来计算 TF-IDF 值。在这种情况下，我们简单地取原始频率和
    IDF 值并将它们相乘，如下面的代码所示：'
- en: '[PRE46]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This works at the most basic level; however, we'll want some adapters to work
    with `NewsArticle` instances and higher-level Clojure data structures.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这在基本层面上是有效的；然而，我们希望有一些适配器来处理 `NewsArticle` 实例和更高级的 Clojure 数据结构。
- en: 'The first level up will take the IDF cache and a map of frequencies and return
    a new map of TF-IDF values based off of those frequencies. To do this, we have
    to find the maximum frequency represented in the mapping. Then we can calculate
    the TF-IDF for each token type in the frequency map as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 第一级将接受 IDF 缓存和频率映射，并返回基于这些频率的新 TF-IDF 值映射。为此，我们必须找到映射中表示的最大频率。然后我们可以计算频率映射中每个标记类型的
    TF-IDF，如下所示：
- en: '[PRE47]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The `tf-idf-freqs` function does most of the work for us. Now we can build
    on it further. First, we''ll write `tf-idf-over` to calculate the TF-IDF values
    for all the tokens in a `NewsArticle` instance. Then we''ll write `tf-idf-cached`,
    which takes a cache of IDF values for each word in a corpus. It returns those
    documents with their frequencies converted if TF-IDF. Finally, `tf-idf-all` will
    call this function on a collection of `NewsArticle` instances as shown in the
    following code:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf-idf-freqs` 函数为我们做了大部分工作。现在我们可以在此基础上进一步扩展。首先，我们将编写 `tf-idf-over` 函数来计算 `NewsArticle`
    实例中所有标记的 TF-IDF 值。然后我们将编写 `tf-idf-cached` 函数，它接受语料库中每个单词的 IDF 值缓存。它将返回那些如果转换成
    TF-IDF 的文档及其频率。最后，`tf-idf-all` 将在一系列 `NewsArticle` 实例上调用此函数，如下面的代码所示：'
- en: '[PRE48]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We've implemented TF-IDF, but now we should play with it some more to get a
    feel for how it works in practice.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了TF-IDF，但现在我们应该更多地玩弄它，以了解它在实际中的工作方式。
- en: We'll start with the definition of `filtered2` that we implemented in the *Hapax
    and Dis Legomena* section. This section contained the corpus of `NewsArticles`
    instances, and the `:text` property is the frequency of tokens without the tokens
    from the stop word lists of both rare and common words.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从我们在*Hapax and Dis Legomena*部分实现的`filtered2`的定义开始。这一部分包含了`NewsArticles`实例的语料库，而`:text`属性是不包含来自罕见和常见单词的停用词列表的标记的频率。
- en: 'Now we can generate the scaled TF-IDF frequencies for these articles by calling
    `tf-idf-all` on them. Once we have that, we can compare the frequencies for one
    article. Look at the following code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过在它们上调用`tf-idf-all`来生成这些文章的缩放TF-IDF频率。一旦我们有了这些，我们就可以比较一篇文章的频率。看看下面的代码：
- en: '[PRE49]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The table''s too long to reproduce here (176 tokens). Instead, I''ll just pick
    10 interesting terms to look at more closely. The following table includes not
    only each term''s raw frequencies and TF-IDF scores, but also the number of documents
    that they are found in:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表格太长了，无法在这里复制（176个标记）。相反，我将只挑选10个有趣的词来更仔细地查看。以下表格不仅包括每个词的原始频率和TF-IDF得分，还包括它们出现的文档数量：
- en: '| Token | Raw frequency | Document frequency | TF-IDF |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Token | Raw frequency | Document frequency | TF-IDF |'
- en: '| --- | --- | --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| sillier | 1 | 8 | 3.35002 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| sillier | 1 | 8 | 3.35002 |'
- en: '| politics | 1 | 749 | 0.96849 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| politics | 1 | 749 | 0.96849 |'
- en: '| british | 1 | 594 | 1.09315 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| british | 1 | 594 | 1.09315 |'
- en: '| reason | 2 | 851 | 0.96410 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| reason | 2 | 851 | 0.96410 |'
- en: '| make | 2 | 2,350 | 0.37852 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| make | 2 | 2,350 | 0.37852 |'
- en: '| military | 3 | 700 | 1.14842 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| military | 3 | 700 | 1.14842 |'
- en: '| time | 3 | 2,810 | 0.29378 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| time | 3 | 2,810 | 0.29378 |'
- en: '| mags | 4 | 18 | 3.57932 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| mags | 4 | 18 | 3.57932 |'
- en: '| women | 11 | 930 | 1.46071 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| women | 11 | 930 | 1.46071 |'
- en: '| men | 13 | 856 | 1.66526 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| men | 13 | 856 | 1.66526 |'
- en: The tokens in the preceding table are ordered by their raw frequencies. However,
    notice how badly that correlates with the TF-IDF.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的表格中的标记按其原始频率排序。然而，请注意这与TF-IDF的相关性有多差。
- en: First, notice the numbers for "sillier" and "politics". Both are found once
    in this document. But "sillier" probably doesn't occur much in the entire collection,
    and it has a TF-IDF score of more than 3\. However, "politics" is common, so it
    scores slightly less than 1.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，注意“sillier”和“politics”的数字。这两个词在这篇文档中都只出现了一次。但是，“sillier”可能在整个集合中不太常见，并且它的TF-IDF得分超过3。然而，“politics”很常见，所以它的得分略低于1。
- en: Next, notice the numbers for "time" (raw frequency of 3) and "mags" (4). "Time"
    is a very common word that kind of straddles the categories of function words
    and content words. On the one hand, you can be using expressions like "time after
    time", but you can also talk about "time" as an abstract concept. "Mags" is a
    slangy version of "magazines", and it occurs roughly the same number of times
    as "time". However, since "mags" is rarely found in the entire corpus (only 18
    times), it has the highest TF-IDF score of any word in this document.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，注意“时间”（原始频率为3）和“mags”（4）的数字。“时间”是一个非常常见的词，它在功能词和实词类别之间摇摆。一方面，你可以使用像“time
    after time”这样的表达，但你也可以谈论“时间”作为一个抽象概念。“mags”是“magazines”的俚语版本，其出现次数与“时间”大致相同。然而，由于“mags”在整个语料库中很少出现（只有18次），它在这个文档中任何单词的TF-IDF得分都是最高的。
- en: Finally, look at "women" and "men". These are the two most common words in this
    article. However, because they're found in so many documents, both are given TF-IDF
    scores of around 1.5.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，看看“women”和“men”。这是这篇文章中最常见的两个词。然而，由于它们出现在许多文档中，两者的TF-IDF得分都约为1.5。
- en: What we wind up with is a measure of how important a term is in that document.
    Words that are more common have to appear more to be considered significant. Words
    that are found in only a few documents can be important with just one mention.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到的是一个衡量一个词在那个文档中重要性的度量。更常见的词必须出现更多次才能被认为是重要的。只在少数文档中出现的词只需一次提及就可以是重要的。
- en: 'As a final step before we move on, we can also write a utility function that
    loads a set of articles, given a token whitelist and an IDF cache. This will be
    important }after we''ve trained the neural network when we''re actually using
    it. That''s because we will need to keep the same features, in the same order,
    and to scale between the two runs. Thus, it''s important to scale by the same
    IDF values. Look at the following code:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们还可以编写一个实用函数，它根据标记白名单和IDF缓存加载一系列文章。这在我们在训练神经网络后实际使用它时将非常重要。这是因为我们需要保持相同的特征，相同的顺序，并在两次运行之间进行缩放。因此，通过相同的IDF值进行缩放很重要。看看以下代码：
- en: '[PRE50]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The preceding code will allow us to analyze documents and actually use our neural
    network after we've trained it.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将允许我们在训练神经网络后分析文档并实际使用我们的神经网络。
- en: Inspecting the stock prices
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查股票价格
- en: 'Now that we have some hold on the textual data, let''s turn our attention to
    the stock prices. Previously, we loaded it from the CSV file using the `financial.csv-data/read-stock-prices`
    function. Let''s reload that data with the following commands:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对文本数据有了些了解，让我们把注意力转向股票价格。之前，我们使用`financial.csv-data/read-stock-prices`函数从CSV文件中加载了它。让我们用以下命令重新加载这些数据：
- en: '[PRE51]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s start with a graph that shows how the closing price has changed over
    the years:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一张显示收盘价随年份变化的图表开始：
- en: '![Inspecting the stock prices](img/4139OS_10_07.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![检查股票价格](img/4139OS_10_07.jpg)'
- en: So the price started in the low 30s, fluctuated a bit, and finished in the low
    20s. During that time, there were some periods where it climbed rapidly. Hopefully,
    we'll be able to capture and predict those changes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，价格从30多开始，略有波动，最终在20多结束。在这段时间里，有一些时期它迅速上升。希望我们能够捕捉并预测这些变化。
- en: Merging text and stock features
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并文本和股票特征
- en: Before we can start to train the neural network, however, we'll need to figure
    out how we need to represent the data and what information the neural network
    needs to have.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们开始训练神经网络之前，我们需要弄清楚我们需要如何表示数据以及神经网络需要哪些信息。
- en: 'The code for this section will be present in the `src/financial/nn.clj` file.
    Open it up and add the following namespace header:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部分的代码将在`src/financial/nn.clj`文件中。打开它并添加以下命名空间头：
- en: '[PRE52]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: However, we first need to be clear about what we're trying to do. That will
    allow us to properly format and present the data.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们首先需要明确我们试图做什么。这将使我们能够正确地格式化和展示数据。
- en: 'Let''s break it down like this: for each document, based on the previous stock
    prices and the tokens in a document, can we predict the direction of future stock
    prices.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们这样分解：对于每一份文档，基于之前的股票价格和文档中的标记，我们能否预测未来股票价格的方向。
- en: So one set of features will be the tokens in the document. We already have those
    identified earlier. Other features can represent the stock prices. Since we're
    actually interested in the direction of the future prices, we can actually use
    the difference between the stock prices of a point in the past and of the day
    the article was published. Offhand, we're not sure what time frames will be helpful,
    so we can select several and include them all.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一组特征将是文档中的标记。我们之前已经确定了这些。其他特征可以代表股票价格。因为我们实际上对未来的价格方向感兴趣，我们可以使用过去某一点和文章发布日的股票价格之间的差异。目前，我们不确定哪个时间段会有帮助，所以我们可以选择几个并包括它们。
- en: The output is another difference in stock prices. Again, we don't know at what
    difference in time we'll be able to get good results (if any!), so we'll try to
    look out into the future at various distances.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是股票价格的另一个差异。同样，我们不知道在什么时间差我们能够得到好的结果（如果有的话！），所以我们将尝试在未来的不同距离上观察。
- en: 'For the ranges of time, we''ll use some standard time periods, gradually getting
    further and further out: a day, two days, three days, four days, five days, two
    weeks, three weeks, one month, two months, six months, and one year. Days that
    fall on a weekend have the value of the previous business day. Months will be
    30 days, and a year is 365 days. This way, the time periods will be more or less
    regular.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对于时间范围，我们将使用一些标准的时间段，逐渐向外扩展：一天、两天、三天、四天、五天、两周、三周、一个月、两个月、六个月和一年。周末的日期将使用前一天的工作日价值。月份将是30天，一年是365天。这样，时间段将大致保持规律。
- en: 'We can represent those periods in Clojure using the `clj-time` library ([https://github.com/clj-time/clj-time](https://github.com/clj-time/clj-time))
    as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`clj-time`库（[https://github.com/clj-time/clj-time](https://github.com/clj-time/clj-time)）在Clojure中表示这些周期，如下所示：
- en: '[PRE53]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'For the features, we''ll use the difference in price over those periods. The
    easiest way to get at that information would be to index the stock prices by date
    and then access the prices from there using some utility functions. Let''s see
    what that would look like:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征，我们将使用这些周期内的价格差异。获取这些信息的最简单方法是将股票价格按日期索引，然后使用一些实用函数从那里访问价格。让我们看看这将是什么样子：
- en: '[PRE54]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We can use `index-by` to index a collection of anything into a map. The other
    function, `get-stock-date`, then attempts to get the `StockData` instance from
    the index. If it doesn't find one, it tries the previous day. If it ever works
    its way before 1990, it just returns nil.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`index-by`将任何集合索引到一个映射中。另一个函数`get-stock-date`尝试从索引中获取`StockData`实例。如果找不到，它尝试前一天。如果它最终在1990年之前找到，它就返回nil。
- en: Now let's get the input feature vector from a `NewsArticle` instance and the
    stock index.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从`NewsArticle`实例和股票指数中获取输入特征向量。
- en: 'The easy part of this will be getting the token vector. Getting the price vector
    will be more complicated, and we''ll be doing almost the same thing twice: once
    looking backward from the article for the input vector, and once looking forward
    from the article for the output vector. Since generating these two vectors will
    be mostly the same, we''ll write a function that does it and accepts function
    parameters for the differences, as shown in the following code:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分的最简单之处在于获取标记向量。获取价格向量将会更复杂，我们几乎会做两遍几乎相同的事情：一次是从文章向后查找输入向量，一次是从文章向前查找输出向量。由于生成这两个向量将大部分相同，我们将编写一个执行此操作并接受函数参数以显示差异的功能，如下面的代码所示：
- en: '[PRE55]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The `make-price-vector` function gets the base price from the day the article
    was published. It then gets the day offsets that we outlined previously and finds
    the closing stock price for each of those days. It finds the difference between
    the two prices.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`make-price-vector`函数从文章发布的当天获取基准价格。然后它获取我们之前概述的日偏移量，并找出这些天中每一天的收盘股票价格。它找出这两个价格之间的差异。'
- en: The parameter for this function is `date-op`, which gets the second day to find
    the stock price for. It will either add the period to the article's publish date
    or subtract it, depending on whether we're looking in the future or the past.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的参数是`date-op`，它获取用于查找股票价格的第二天。它将根据我们是在看未来还是过去，要么将周期加到文章的发布日期上，要么从中减去。
- en: 'We can build on this to make the input vector, which will contain the token
    vector and the price vector, as shown in the following code:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在此基础上构建输入向量，它将包含标记向量和价格向量，如下面的代码所示：
- en: '[PRE56]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: For the token vector, we get the frequencies from the `NewsArticle` instance
    in the order given by the vocab collection. This should be the same across all
    `NewsArticle` instances. We call `make-price-vector` to get the prices for the
    offset days. Then we concatenate all of them into one (Clojure) vector.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标记向量，我们从词汇集合给出的顺序中获取来自`NewsArticle`实例的频率。这应该在所有`NewsArticle`实例中都是相同的。我们调用`make-price-vector`来获取偏移天数的价格。然后我们将它们全部连接成一个（Clojure）向量。
- en: The following code gives us the input vector. However, we'll also want to have
    future prices as the output vector.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码给出了输入向量。然而，我们还将想要未来的价格作为输出向量。
- en: '[PRE57]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The preceding code is just a thin wrapper over `make-price-vector`. It calls
    this function with the appropriate arguments to get the future stock price.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码只是`make-price-vector`的一个薄包装。它使用适当的参数调用此函数以获取未来的股票价格。
- en: 'Finally, we''ll write a function that takes a stock index, a vocabulary, and
    a collection of articles. It will generate both the input vector and the expected
    output vector, and it will return both stored in a hash map. The code for this
    function is given as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写一个函数，它接受股票指数、词汇和文章集合。它将生成输入向量和预期输出向量，并将它们存储在哈希映射中。此函数的代码如下所示：
- en: '[PRE58]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This code will make it easy to generate a training set from the data that we've
    been working with.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将使我们能够轻松地从我们一直在处理的数据中生成训练集。
- en: Analyzing both text and stock features together with neural nets
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本和股票特征与神经网络一起分析
- en: We now have everything ready to perform the analysis, except for the engine
    that will actually attempt to learn the training data.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好进行分析了，除了实际尝试学习训练数据的引擎之外。
- en: In this instance, we're going to try to train an artificial neural network to
    learn the direction of change of the future prices of the input data. In other
    words, we'll try to train it to tell whether the price will go up or down in the
    near future. We want to create a simple binary classifier from the past price
    changes and the text of an article.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将尝试训练一个人工神经网络来学习输入数据未来价格变化的方向。换句话说，我们将尝试训练它来告诉价格在不久的将来是上涨还是下跌。我们希望从过去的价格变化和文章的文本中创建一个简单的二元分类器。
- en: Understanding neural nets
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解神经网络
- en: 'As the name implies, artificial neural networks are machine learning structures
    modeled on the architecture and behavior of neurons, such as the ones found in
    the human brain. Artificial neural networks come in many forms, but today we''re
    going to use one of the oldest and most common forms: the three-layer feed-forward
    network.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，人工神经网络是模仿人类大脑中神经元架构和行为的机器学习结构，例如在人类大脑中发现的那些。人工神经网络有多种形式，但今天我们将使用最古老和最常见的形式之一：三层前馈网络。
- en: 'We can see the structure of a unit outlined in the following figure:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图中看到单元的结构：
- en: '![Understanding neural nets](img/4139OS_10_08.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![理解神经网络](img/4139OS_10_08.jpg)'
- en: Each unit is able to realize linearly separable functions. That is, functions
    that divide their n-dimensional output space along a hyperplane. To emulate more
    complex functions, however, we have to go beyond a single unit and create a network
    of them.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单元都能够实现线性可分函数。也就是说，这些函数可以沿着一个超平面将它们的n维输出空间分割开来。然而，为了模拟更复杂的函数，我们必须超越单个单元，并创建一个由它们组成的网络。
- en: 'These networks have three layers: an input layer, a hidden layer, and an output
    layer. Each layer is made up of one or more neurons. Each neuron takes one or
    more inputs and produces an output, which is broadcast to one or more outputs.
    The inputs are weighted, and each input is weighted individually. All of the inputs
    are added together, and the sum is passed through an activation function that
    normalizes and scales the input. The inputs are *x*, the weights are *w*, and
    the outputs are *y*.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络有三个层次：一个输入层、一个隐藏层和一个输出层。每一层由一个或多个神经元组成。每个神经元接受一个或多个输入并产生一个输出，该输出被广播到一个或多个输出。输入是有权重的，每个输入都是单独加权的。所有输入加在一起，然后通过一个激活函数传递，该函数对输入进行归一化和缩放。输入是*x*，权重是*w*，输出是*y*。
- en: 'A simple schematic of this structure is shown as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 该结构的简单示意图如下所示：
- en: '![Understanding neural nets](img/4139OS_10_09.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![理解神经网络](img/4139OS_10_09.jpg)'
- en: 'The network operates in a fairly simple manner, following the process called
    feed forward activation. It is described as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 网络以相当简单的方式运行，遵循称为前向激活的过程。它被描述如下：
- en: The input vector is fed to the input layer of the network. Depending on how
    the network is set up, these may be passed through the activation function for
    each neuron. This determines the activation of each neuron, or the amount of signal
    coming into it from that channel and how excited it is.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入向量被送入网络的输入层。根据网络如何设置，这些输入可能需要通过每个神经元的激活函数。这决定了每个神经元的激活，或者说从该通道进入它的信号的量以及它的兴奋程度。
- en: The weighted connections between the input and hidden layers are then activated
    and used to excite the nodes in the hidden layer. This is done by getting the
    dot product of the input neurons with the weights going into each hidden node.
    These values are then passed through the activation function for the hidden neurons.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入层和隐藏层之间的加权连接被激活并用于激发隐藏层中的节点。这是通过获取输入神经元与进入每个隐藏节点的权重的点积来完成的。然后，这些值通过隐藏神经元的激活函数传递。
- en: The forward propagation process is repeated again between the hidden layer and
    the output layer.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前向传播过程在隐藏层和输出层之间再次重复。
- en: The activation of the neurons in the output layer is the output of the network.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出层中神经元的激活是网络的输出。
- en: Initially, the weights are usually randomly selected. Then the weights are trained
    using a variety of techniques. A common one is called backward propagation. This
    involves computing the error between the output neurons and the desired outputs.
    This error is then fed backward into the network. This is used to dampen some
    weights and increase others. The net effect is to nudge the output of the network
    slightly closer to the target.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，权重通常是随机选择的。然后使用各种技术训练权重。一个常见的方法称为反向传播。这涉及到计算输出神经元和期望输出之间的误差。然后，这个误差被反向输入到网络中。这用于减少一些权重并增加其他权重。最终的效果是稍微调整网络的输出，使其更接近目标。
- en: 'Other training methods work differently, but attempt to do the same thing:
    each tries to modify the weights so that the outputs are close to the targets
    for each input in the training set.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 其他训练方法工作方式不同，但试图做同样的事情：每个方法都试图修改权重，以便对于训练集中的每个输入，输出都接近目标。
- en: Note that I said *close to the targets*. When training a neural network, you
    don't want the outputs to align exactly. When this happens, the network is said
    to have memorized the training set. This means that the network will perform great
    for inputs that it has seen previously. But when it encounters new inputs, it
    is brittle and won't perform well. It has learned the training set too well, but
    it won't be able to generalize that information to new inputs.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我说的是**接近目标**。在训练神经网络时，你不想输出完全与目标对齐。当这种情况发生时，我们说网络已经记住了训练集。这意味着对于它之前见过的输入，网络将表现得很好。但是，当它遇到新的输入时，它会变得脆弱，表现不佳。它对训练集学得太好了，但无法将信息推广到新的输入。
- en: Setting up the neural net
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置神经网络
- en: Implementing neural networks isn't difficult—and doing so is a useful exercise—but
    there are good libraries for neural networks available for Java and Clojure, and
    we'll choose one of those here. For our case, we'll use the Encog Machine Learning
    Framework ([http://www.heatonresearch.com/encog](http://www.heatonresearch.com/encog)),
    which specializes in neural networks. But we'll primarily be using it through
    the Clojure wrapper library Enclog ([https://github.com/jimpil/enclog/](https://github.com/jimpil/enclog/)).
    We'll build on these to write some facade functions over Enclog to customize this
    library for our processes.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 实现神经网络并不困难——这样做是一个有用的练习——但是Java和Clojure都有可用的神经网络库，我们将选择其中之一。在我们的案例中，我们将使用Encog机器学习框架([http://www.heatonresearch.com/encog](http://www.heatonresearch.com/encog))，它专门从事神经网络。但我们将主要通过Clojure包装库Enclog([https://github.com/jimpil/enclog/](https://github.com/jimpil/enclog/))来使用它。我们将在此基础上编写一些封装函数，以定制这个库以适应我们的流程。
- en: 'The first step is to create the neural network. The `make-network` function
    takes the vocabulary size and the number of hidden nodes (the variables for our
    purposes), but it defines the rest of the parameters internally, as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建神经网络。`make-network`函数接受词汇大小和隐藏节点数量（我们目的的变量），但它在内部定义了其余的参数，如下所示：
- en: '[PRE59]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The number of input nodes is a function of the size of the vocabulary in addition
    to the number of periods. (Periods is a non-dynamic, namespace-level binding.
    We may want to rethink that and make it dynamic to provide a little more flexibility,
    but for our needs right now this is sufficient.) And since from the output node,
    we just want a single value indicating whether the stock went up or down, we hardcoded
    the number of output nodes to one. However, the number of hidden nodes that will
    perform best is an open question. We'll include that as a parameter so we can
    experiment with it.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 输入节点的数量是词汇大小和句点数量的函数。（句点是非动态的、命名空间级别的绑定。我们可能需要重新考虑这一点，使其动态化以提供更多的灵活性，但就我们现在的需求而言，这已经足够了。）而且，由于从输出节点来看，我们只想得到一个单一值，表示股票是上涨还是下跌，所以我们硬编码了输出节点的数量为1。然而，将表现最佳的隐藏节点数量是一个悬而未决的问题。我们将将其作为一个参数包括在内，这样我们就可以对其进行实验。
- en: 'For the output, we''ll need a way to take our expected output and run it through
    the same activation function as that output. That way, we can directly compare
    the two as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输出，我们需要一种方法来处理我们的预期输出，并使其通过与该输出相同的激活函数。这样，我们可以直接比较如下：
- en: '[PRE60]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The activated function takes an object that implements `org.encog.engine.network.activation.ActivationFunction`.
    We can get these from the neural network. It then puts the output for a period
    into a double array. The activation function scales this and then returns the
    array.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数接受一个实现了`org.encog.engine.network.activation.ActivationFunction`的对象。我们可以从神经网络中获取这些对象。然后，它将一段时间的输出放入一个双精度数组中。激活函数将这个数组进行缩放，然后返回该数组。
- en: 'We will also need to prepare the data and insert it into a data structure that
    Encog can work with. The primary transformation in the following code is pulling
    out the output prices for the period that we''re training for:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要准备数据并将其插入到Encog可以处理的数据结构中。以下代码中的主要转换是提取我们正在训练的时期的输出价格：
- en: '[PRE61]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: There's nothing particularly exciting here. We pull the inputs and the outputs
    for one time period out into two separate vectors and create a dataset with them.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有什么特别激动人心的。我们将一个时期的输入和输出分别提取到两个单独的向量中，并使用它们创建一个数据集。
- en: Training the neural net
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: Now we have a neural network, but it's been initialized to random weights, so
    it will perform very, very poorly. We'll need to train it immediately.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个神经网络，但它已经被初始化为随机权重，所以它的表现会非常、非常差。我们需要立即对其进行训练。
- en: To do this, we will put the training set together with the network in the following
    code. Like the previous functions, `train-for` accepts the parameters that we're
    interested in being able to change, uses reasonable defaults for ones that we'll
    probably leave alone, but hardcodes parameters that we won't touch. The function
    creates a trainer object and calls its `train` method. Finally, we return the
    neural network, which was modified in place.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们将在以下代码中将训练集与网络一起放入。像之前的函数一样，`train-for`接受我们感兴趣可以更改的参数，为那些我们可能不会更改的参数使用合理的默认值，但对于我们不会触及的参数则使用硬编码。该函数创建一个训练器对象并调用其`train`方法。最后，我们返回修改后的神经网络。
- en: '[PRE62]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'When it is time to validate a network, it will be a little easier to combine
    creating a network with training it into one function. We''ll do that with `make-train`
    as follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要验证网络时，将创建网络与训练结合成一个函数会稍微容易一些。我们将通过以下方式使用`make-train`：
- en: '[PRE63]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This allows us to train a new neural network in one call.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们在一次调用中训练一个新的神经网络。
- en: Running the neural net
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行神经网络
- en: 'Once we''ve trained the network, we''ll want to run it on new inputs, ones
    for which we don''t know the expected output. We can do that with the `run-network`
    function. This takes a trained network and an input collection and returns an
    array of the network''s output as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了网络，我们将在新的输入上运行它，这些输入的预期输出我们不知道。我们可以使用`run-network`函数来完成这个操作。这个函数接受一个训练好的网络和一个输入集合，并返回网络输出的数组，如下所示：
- en: '[PRE64]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We can use this function in one of two ways:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种方式使用此函数：
- en: We can pass it data that we don't know the output for to see how the network
    classifies it.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以传递给它我们不知道输出的数据，以查看网络如何对其进行分类。
- en: We can pass it input data that we do know the output for in order to evaluate
    how well this network performs against data it hasn't previously encountered.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以传递给它我们知道输出的输入数据，以便评估该网络在之前未遇到的数据上的表现如何。
- en: We'll see an example of the latter in the next section.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中看到后者的一个示例。
- en: Validating the neural net
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证神经网络
- en: We can build on all of these functions to validate the neural network, train
    it, test it against new data, and evaluate how it does.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以基于所有这些函数来验证神经网络，训练它，对新数据进行测试，并评估其表现。
- en: 'The `test-on` utility gets the **sum of squared errors** (**SSE**) for running
    the network on a test set for a given period. This trains and runs a neural network
    on the training set for a given period. It then returns the SSE for that run as
    follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`test-on`实用工具获取在给定时期运行网络上的**平方误差和**（**SSE**）。这将在给定时期对训练集上的神经网络进行训练和运行。然后，它返回该运行的SSE，如下所示：'
- en: '[PRE65]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Running this train-test combination once gives us a very rough idea of how the
    network will perform with those parameters. However, if we want a better idea,
    we can use K-fold cross-validation. This divides the data into *K* equally sized
    groups. It then runs the train-test combination *K* times. Each time, it holds
    out a different partition as a test group. It trains the network on the rest of
    the partitions and evaluates it on the test group. The errors returned by `test-on`
    can be averaged to get a better idea of how the network will perform with those
    parameters.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个训练-测试组合一次，我们可以对网络使用这些参数的性能有一个非常粗略的了解。然而，如果我们想要一个更好的了解，我们可以使用 K-fold 交叉验证。这会将数据分成
    *K* 个大小相等的组。然后它将运行训练-测试组合 *K* 次。每次，它都会保留一个不同的分区作为测试组。它将在其余的分区上训练网络，并在测试组上评估它。`test-on`
    返回的错误可以平均，以更好地了解网络使用这些参数的性能。
- en: 'For example, say we use K=4\. We''ll divide the training input into four groups:
    A, B, C, and D. This means that we''ll train the following four different classifiers:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们使用 K=4。我们将训练输入分成四个组：A、B、C 和 D。这意味着我们将训练以下四个不同的分类器：
- en: We'll use A as the test set and train on B, C, and D combined
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用 A 作为测试集，并在 B、C 和 D 的组合上进行训练。
- en: We'll use B as the test set and train on A, C, and D
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用 B 作为测试集，并在 A、C 和 D 上进行训练。
- en: We'll use C as the test set and train on A, B, and D
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用 C 作为测试集，并在 A、B 和 D 上进行训练。
- en: We'll use D as the test set and train on A, B, and C
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用 D 作为测试集，并在 A、B 和 C 上进行训练。
- en: For each classifier, we'll compute the SSE, and we'll take the mean of these
    to see how well the classification should perform with those parameters, on average.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个分类器，我们将计算 SSE，并将这些平均值作为参考，以了解使用这些参数时分类的平均性能。
- en: Note
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: I've defined the K-fold function in the `validate.clj` file at `src/financial/`.
    You can see how it's implemented in the source code download. I've also aliased
    that namespace to `v` in the current namespace.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 `src/financial/` 目录下的 `validate.clj` 文件中定义了 K-fold 函数。您可以在源代码下载中看到它的实现。我还在当前命名空间中将该命名空间别名为
    `v`。
- en: 'The `x-validate` function will perform the cross validation on the inputs.
    The other function, `accum`, is simply a small utility that accumulates the error
    values into a vector. The `v/k-fold` function expects the accumulator to return
    the base case (an empty vector) when called with no arguments, as shown in the
    following code:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '`x-validate` 函数将在输入上执行交叉验证。另一个函数 `accum` 只是一个简单的实用工具，它将错误值累加到一个向量中。`v/k-fold`
    函数期望在没有任何参数调用时返回基本案例（一个空向量），如下面的代码所示：'
- en: '[PRE66]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The `x-validate` function uses `make-train` to create a new network and train
    it. It tests that network using `test-on`, and it gathers the resulting error
    rates together with `accum`.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`x-validate` 函数使用 `make-train` 创建一个新的网络并对其进行训练。它使用 `test-on` 测试该网络，并使用 `accum`
    收集结果错误率。'
- en: Finding the best parameters
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找最佳参数
- en: We've defined this system to let us play with a couple of parameters. First,
    we can set the number of neurons in the hidden layer. Also, we can set the time
    period that we are to predict for into the future (one day, two days, three days,
    a month, a year, and so on).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了这个系统，以便我们可以玩一些参数。首先，我们可以设置隐藏层中的神经元数量。此外，我们还可以设置我们想要预测的未来时间段（一天、两天、三天、一个月、一年等等）。
- en: These parameters create a large space of possible solutions, some of which may
    perform better than others. We can make some educated guesses about some of the
    parameters—that it will predict the movement of the stock prices one day in the
    future better than it will the movement a year in the future—but we don't know
    that, and we should perhaps try it out.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数创建了一个可能解决方案的大空间，其中一些可能比其他解决方案表现更好。我们可以对一些参数做出一些有根据的猜测——它将比预测一年后的股价变动更好地预测未来一天的股价变动——但我们不知道这一点，也许我们应该尝试一下。
- en: These parameters present a search space. It would take too much time to try
    all the combinations, but we can try a number of them, just to see how they perform.
    This lets us tune the neural network to get the best results.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数构成了一个搜索空间。尝试所有组合将花费太多时间，但我们可以尝试其中的一些，以了解它们的性能。这使我们能够调整神经网络以获得最佳结果。
- en: 'To explore this search space, let''s first define what happens when we test
    one point, one combination of time period in the future, and a number of hidden
    nodes. The `explore-point` function will take care of this in the following code:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索这个搜索空间，让我们首先定义当我们测试一个点、未来某个时间段的一个组合以及隐藏节点数量时会发生什么。`explore-point` 函数将在下面的代码中处理这一点：
- en: '[PRE67]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The preceding code basically just takes the information and passes it to `x-validate`.
    It returns that function's return value (`error`) too. Along the way, it prints
    out a number of status messages. Then we need something that walks over the search
    space, calls `explore-point`, and collects the error rates returned for the output.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码基本上只是获取信息并将其传递给 `x-validate`。它还返回该函数的返回值（`error`）。在这个过程中，它打印出许多状态消息。然后我们需要某种东西来遍历搜索空间，调用
    `explore-point`，并收集返回的输出错误率。
- en: We'll define a dynamic global called `*hidden-counts*` that defines the range
    of hidden neuron counts that we're interested in exploring. The `periods` value
    that we bound earlier will define the search space for how far to look into the
    future.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个动态的全局变量 `*hidden-counts*`，它定义了我们感兴趣探索的隐藏神经元数量的范围。我们之前限制的 `periods` 值将定义搜索空间，以确定我们向未来看多远。
- en: To make sure that we don't train the networks too specifically on the data that
    we're using to find the best parameters, we'll first break the data into a development
    set and a test set. We'll use the development set to try out the different parameters,
    further breaking it up into a training set and a development-test set. At the
    end, we'll take the best set of parameters and test those against the test set
    that we originally held out. This will give us a better idea of how the neural
    network performs. The `final-eval` function will perform this last set and return
    the information that it creates.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们不会将网络训练得太具体，以至于使用我们用来寻找最佳参数的数据，我们首先将数据分成一个开发集和一个测试集。我们将使用开发集来尝试不同的参数，并将其进一步分成一个训练集和一个开发-测试集。最后，我们将使用最佳参数集来测试我们最初保留的测试集。这将给我们一个更好的想法，了解神经网络的表现。`final-eval`
    函数将执行这个最后的集合，并返回它创建的信息。
- en: 'The following function walks over these values and is named `explore-params`:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数遍历这些值，并命名为 `explore-params`：
- en: '[PRE68]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: I've made a slightly unusual design decision in writing `explore-params`. Instead
    of initializing a hash map to contain the period-hidden count pairs and their
    associated error rates, I need the caller to pass in a reference containing a
    hash map. During the course of the processing, `explore-params` fills the hash
    map and finally returns it.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写 `explore-params` 时，我做出了一个稍微不寻常的设计决策。不是初始化一个哈希表来包含周期-隐藏计数对及其相关的错误率，我需要调用者传递一个包含哈希表的引用。在处理过程中，`explore-params`
    填充这个哈希表，并最终返回它。
- en: 'I''ve done this for one reason: exploring this search space still takes a long
    time. Over the course of writing this chapter, I needed to stop the validation,
    tweak the possible parameter values, and start it again. Setting up the function
    this way allowed me to be able to stop the processing, but still have access to
    what''s happened thus far. I can look at the values, play around with them, and
    allow a more thorough examination of them to influence my decisions about what
    direction to take.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我这样做的原因是：探索这个搜索空间仍然需要很长时间。在撰写本章的过程中，我需要停止验证，调整可能的参数值，然后再次启动。以这种方式设置函数使我能够停止处理，但仍然可以访问到目前为止发生的事情。我可以查看这些值，对它们进行操作，并允许更彻底地检查它们，以影响我关于采取何种方向的决策。
- en: Predicting the future
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测未来
- en: Now is the time to bring together everything that we've assembled over the course
    of this chapter, so it seems appropriate to start over from scratch, just using
    the Clojure source code that we've written over the course of the chapter.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将本章过程中我们所组装的一切整合在一起了，因此从头开始使用本章中我们编写的 Clojure 源代码似乎是合适的。
- en: We'll take this one block at a time, loading and processing the data, creating
    training and test sets, training and validating the neural network, and finally
    viewing and analyzing its results.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一次处理一个代码块，加载和处理数据，创建训练集和测试集，训练和验证神经网络，最后查看和分析其结果。
- en: 'Before we do any of this, we''ll need to load the proper namespaces into the
    REPL. We can do that with the following `require` statement:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们做任何这些之前，我们需要将适当的命名空间加载到 REPL 中。我们可以使用以下 `require` 语句来完成：
- en: '[PRE69]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This will give us access to everything that we've implemented so far.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们能够访问到目前为止我们所实现的一切。
- en: Loading stock prices
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载股票价格
- en: 'First, we''ll load the stock prices with the following commands:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用以下命令加载股票价格：
- en: '[PRE70]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The preceding code loads the stock prices from the CSV file and indexes them
    by date. This will make it easy to integrate them with the new article data in
    a few steps.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码从CSV文件中加载股价，并按日期索引它们。这将使得将它们与新的文章数据集成变得容易。
- en: Loading news articles
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载新闻文章
- en: 'Now we can load the news articles. We''ll need two pieces of data from them:
    the TF-IDF scaled frequencies and the vocabulary list. Look at the following commands:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以加载新闻文章。我们需要从它们中获取两份数据：TF-IDF缩放频率和词汇表列表。看看以下命令：
- en: '[PRE71]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This code binds the frequencies as `freqs` and the vocabulary as `vocab`.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将频率绑定到`freqs`，将词汇表绑定到`vocab`。
- en: Creating training and test sets
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练集和测试集
- en: 'Since we bundled the entire process into one function, merging our two data
    sources together into one training set is simple, as shown in the following command:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将整个过程打包到一个函数中，所以将我们的两个数据源合并到一个训练集中很简单，如下所示：
- en: '[PRE72]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Now, for each article, we have an input vector and a series of output for different
    stock prices related to the data.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于每篇文章，我们都有一个输入向量以及一系列与数据相关的不同股价的输出。
- en: Finding the best parameters for the neural network
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找神经网络的最佳参数
- en: 'The training data and the parameters'' value ranges are the input for exploring
    the network parameter space. Look at the following commands:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据和参数值范围是探索网络参数空间的输入。看看以下命令：
- en: '[PRE73]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This takes a very long time to run. Actually, I looked at the output it was
    producing and realized that it wouldn't be able to predict well beyond a day or
    two, so I stopped it after that. Thanks to my decision to pass in a reference,
    I was able to stop it and still have access to the results generated by that point.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要非常长的时间来运行。实际上，我看了它产生的输出，并意识到它无法预测超过一两天，所以我停止了它。多亏了我决定传递一个参考值，我能够停止它，并且仍然可以访问到那个点生成的结果。
- en: 'The output is a mapping from the period and number of hidden nodes to a list
    of SSE values generated from each partition in the K-fold cross-validation. A
    more meaningful metric would be the average of the errors. We can generate that
    here and print out the results as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是从周期和隐藏节点数到每个K折交叉验证分区生成的SSE值列表的映射。一个更有意义的指标将是误差的平均值。我们可以在这里生成它并按以下方式打印结果：
- en: '[PRE74]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: So the squared sum of errors for predicting one day ahead go from about 1 for
    10 hidden units to 100 for 300 hidden units. So, based on that, we'll only train
    a network to predict one day into the future and to use 10 hidden nodes.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，预测未来一天的误差平方和从10个隐藏单元的约1增加到300个隐藏单元的100。所以，基于这一点，我们将只训练一个网络来预测未来一天，并使用10个隐藏节点。
- en: Training and validating the neural network
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和验证神经网络
- en: Actually, training the neural network is pretty easy from our end, but it does
    take a while. The following commands should somewhat produce better results than
    we saw before, but at the cost of some time. Remember that the training process
    may not take this long, but we should probably be prepared.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，从我们这边来看，训练神经网络相当简单，但确实需要一段时间。以下命令可能会产生比我们之前看到更好的结果，但代价是花费一些时间。记住，训练过程可能不会这么长，但我们可能需要做好准备。
- en: '[PRE75]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Well, that was quick.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这很快。
- en: This gives us a trained, ready-to-use neural network bound to the name `nn`.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了一个训练好的、准备好使用的神经网络，绑定到名称`nn`。
- en: Running the network on new data
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在新数据上运行网络
- en: We can now run our trained network on some new data. Just to have something
    to look at, I downloaded 10 articles off the Slate website and saved them to files
    in the directory `d/slate/`. I also downloaded the stock prices for Dominion,
    Inc.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在一些新数据上运行我们的训练好的网络。为了有所观察，我从Slate网站下载了10篇文章，并将它们保存在目录`d/slate/`中的文件里。我还下载了Dominion,
    Inc.的股价。
- en: Now, how would I analyze this data?
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我该如何分析这些数据？
- en: 'Before we really start, we''ll need to pull some data from the processes we''ve
    been using, and we''ll need to set up some reference values, such as the date
    of the documents. Look at the following code:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们真正开始之前，我们需要从我们一直在使用的过程中提取一些数据，并且我们需要设置一些参考值，例如文档的日期。看看以下代码：
- en: '[PRE76]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: So we get the IDF cache, the date the articles were downloaded on, and the vocabulary
    that we used in training. That vocabulary set will serve as the token whitelist
    for loading the news articles.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到了IDF缓存，文章下载的日期以及我们在训练中使用的词汇表。这个词汇表集将作为加载新闻文章的令牌白名单。
- en: 'Let''s see how to get the documents ready to analyze. Look at the following
    code:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何准备文档以便分析。看看以下代码：
- en: '[PRE77]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: This is a little more complicated than it was when we loaded them earlier. Basically,
    we just read the directory list and load the text from each one. Then we tokenize
    and filter it before determining the TF-IDF value for each token.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这比我们之前加载时要复杂一些。基本上，我们只是读取目录列表并从每个文件中加载文本。然后我们对文本进行分词和过滤，在确定每个标记的TF-IDF值之前。
- en: 'On the other hand, reading the stocks is very similar to what we just did.
    Look at the following code:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，阅读股票信息与我们刚才所做的是非常相似的。看以下代码：
- en: '[PRE78]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'With these in hand, we can put both together to make the input vectors as shown
    in the following code:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些，我们可以将它们结合起来，如以下代码所示，来制作输入向量：
- en: '[PRE79]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now let''s see how to run the network and see what happens. Look at the following:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何运行网络并观察会发生什么。看以下内容：
- en: '[PRE80]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: These items are very consistent. To quite a few decimal places, they're all
    clustered right around 0.5\. From the sigmoid function, this means that it doesn't
    really anticipate a stock change over the next day.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这些项目非常一致。它们都聚集在0.5左右，精确到很多小数位。从Sigmoid函数来看，这意味着它并不真正预测股票在第二天会发生变化。
- en: In fact, this tracks what actually happened fairly well. On March 20, the stock
    closed at $69.77, and on March 21, it closed at $70.06\. This was a gain of $0.29.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这很好地追踪了实际发生的情况。3月20日，股价收盘价为69.77美元，而3月21日收盘价为70.06美元。这增加了0.29美元。
- en: Taking it with a grain of salt
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采取谨慎的态度
- en: Any analysis like the one presented in this chapter has a number of things that
    we need to question. This chapter is no exception.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的任何分析都需要我们提出许多问题。本章也不例外。
- en: Related to this project
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与此项目相关
- en: 'The main weakness of this project was that it was carried out on far too little
    data. This cuts in several ways:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的主要弱点是它是在非常少的数据上进行的。这有几个方面的影响：
- en: We need articles from a number of data sources
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要来自多个数据源的文章
- en: We need articles from a wider range of time
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要来自更长时间范围的文章
- en: We need more density of articles in the time period
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要在特定时间段内增加文章的密度
- en: For all of these, there are reasons we didn't address the issues in this chapter.
    However, if you plan to take this further, you'd need to figure out some way around
    these.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有这些，我们之所以没有在本章中解决这些问题，都有其原因。然而，如果你打算进一步研究，你需要找出一些绕过这些问题的方法。
- en: There are several ways to look at the results too. The day we looked at, the
    results all clustered close to zero. In fact, this stock if relatively stable,
    so if it always indicated little change, then it would always have a fairly low
    SSE. Large changes seem to happen occasionally, and the error from not predicting
    them has a low impact on the SSE.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 看结果也有几种方式。我们查看的那天，结果都聚集在接近零的地方。事实上，这只股票相对稳定，所以如果它总是表示微小的变化，那么它总是会有一个相当低的SSE。大变化似乎偶尔会发生，而未能预测这些变化所带来的误差对SSE的影响很小。
- en: Related to machine learning and market modeling in general
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与机器学习和市场建模相关
- en: Second, and more importantly, simply putting some stock data into a jar with
    some machine learning and shaking it is a risky endeavor. This isn't a get-rich-quick
    scheme, and by approaching it so naively, you're asking for trouble. In this case,
    that means losing money.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，更重要的是，仅仅将一些股票数据放入一个带有一些机器学习的罐子中并摇动它是一种冒险的行为。这不是一个快速致富的计划，而且以如此天真的方式处理，你是在自找麻烦。在这种情况下，这意味着损失金钱。
- en: For one thing, there's not much noise in news articles, and the relationship
    between their content and stock prices is tenuous enough that in general, stock
    prices may not be predictable from news reports in the first place, whatever results
    we achieve is this study, particularly given how small it is.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，新闻文章中的噪声不多，它们的内容与股价之间的关系足够薄弱，以至于通常情况下，股价可能无法从新闻报道中预测，无论我们在这个研究中取得了什么结果，尤其是考虑到它的规模很小。
- en: 'Really, to do this well, you need to understand at least two things:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 真的，要做好这件事，你需要至少理解两件事：
- en: '**Financial modeling**: You need to understand how to model financial transactions
    and dynamics mathematically'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融建模**：你需要理解如何从数学上建模金融交易和动态'
- en: '**Machine learning**: You need to understand how machine learning works and
    how it models things'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习**：你需要理解机器学习是如何工作的以及它是如何建模事物的'
- en: With this knowledge, you should be able to formulate a better model of how the
    stock prices change and which prices you should pay attention to.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些知识，你应该能够制定出一个更好的模型来理解股价的变化以及你应该关注的哪些价格。
- en: But keep in mind, André Christoffer Andersen and Stian Mikelsen have published
    a master's thesis in 2012 showing that it's very, very difficult to do better
    than buying and holding index funds ([http://blog.andersen.im/wp-content/uploads/2012/12/ANovelAlgorithmicTradingFramework.pdf](http://blog.andersen.im/wp-content/uploads/2012/12/ANovelAlgorithmicTradingFramework.pdf)).
    So, if you do try this route, you have a hard, hard task in front of you.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 但请记住，安德烈·克里斯托弗·安德森和斯蒂安·米克尔斯伦在2012年发表了一篇硕士学位论文，表明要优于购买并持有指数基金是非常非常困难的([http://blog.andersen.im/wp-content/uploads/2012/12/ANovelAlgorithmicTradingFramework.pdf](http://blog.andersen.im/wp-content/uploads/2012/12/ANovelAlgorithmicTradingFramework.pdf))。因此，如果你尝试走这条路，你面前将是一个艰巨的任务。
- en: Summary
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Over the course of this chapter, we've gotten a hold of some news articles and
    some stock prices, and we've managed to train a neural network that projects just
    a little into the future. This is a risky thing to put into production, but we've
    also outlined what we'd need to learn to do this correctly.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的整个过程中，我们收集了一些新闻文章和一些股价，并成功训练了一个能够稍微预测未来的神经网络。将这个风险项目投入生产是有风险的，但我们也已经概述了我们需要学习的内容才能正确地完成这项工作。
- en: And this is also the end of this book. Thank you for staying with me this far.
    You've been a great reader. I hope that you've learned something as we've looked
    at the 10 data analysis projects that we've covered. If programming and data are
    both eating this world, hopefully you've seen how to have fun with both.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是本书的结尾。感谢你一直陪伴我走到这里。你是一位了不起的读者。我希望在我们回顾了所涵盖的10个数据分析项目之后，你已经学到了一些东西。如果编程和数据都正在吞噬这个世界，希望你已经看到了如何在这两者中找到乐趣。
