- en: Chapter 7. Machine Learning 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 机器学习 2
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下几种方法：
- en: Predicting real-valued numbers using regression
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用回归预测实值
- en: Learning regression with L2 shrinkage – ridge
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习带L2收缩的回归——岭回归
- en: Learning regression with L1 shrinkage – LASSO
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习带L1收缩的回归——LASSO
- en: Using cross-validation iterators with L1 and L2 shrinkage
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证迭代器与L1和L2收缩
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we will introduce regression techniques and how they can be
    coded in Python. We will follow it up with a discussion on some of the drawbacks
    that are inherent with regression methods, and discuss how to address the same
    using shrinkage methods. There are some parameters that need to be set in the
    shrinkage methods. We will discuss cross-validation techniques to find the optimal
    parameter values for the shrinkage methods.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍回归技术及其在Python中的实现方法。接着，我们将讨论回归方法固有的一些缺点，并探讨如何通过收缩方法来解决这些问题。收缩方法中需要设置一些参数。我们将讨论交叉验证技术，以找到收缩方法的最佳参数值。
- en: We saw classification problems in the previous chapter. In this chapter, let's
    turn our attention towards regression problems. In classification, the response
    variable `Y` was either binary or a set of discrete values (in the case of multiclass
    and multilabel problems). In contrast, the response variable in regression is
    a real-valued number.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了分类问题。在本章中，让我们将注意力转向回归问题。在分类问题中，响应变量`Y`要么是二元的，要么是一组离散值（在多类和多标签问题中）。而在回归中，响应变量是一个实值数字。
- en: Regression can be thought of as a function approximation. The job of regression
    is to find a function such that when `X`, a set of random variables, is provided
    as an input to that function, it should return `Y`, the response variable. `X`
    is also referred to as an independent variable and `Y` is referred as a dependent
    variable.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 回归可以看作是一种函数逼近。回归的任务是找到一个函数，使得当`X`（一组随机变量）作为输入传递给该函数时，它应该返回`Y`，即响应变量。`X`也被称为自变量，而`Y`则被称为因变量。
- en: We will leverage the techniques that we learnt in the previous chapter to divide
    our dataset into train, dev, and test sets, build our model iteratively on the
    train set, and validate it on dev. Finally, we will use our test set to get a
    good picture of the goodness of our model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用上一章中学到的技术，将数据集分为训练集、开发集和测试集，在训练集上迭代地构建模型，并在开发集上进行验证。最后，我们将使用测试集来全面评估模型的优劣。
- en: We will start the chapter with a recipe for simple linear regression using the
    least square estimation. At the beginning of the first recipe, we will provide
    a crisp introduction to the framework of regression, which is essential background
    information required to understand the other recipes in this chapter. Though very
    powerful, the simple regression framework suffers from a drawback. As there is
    no control over the upper and lower limits on the values that the coefficients
    of linear regression can take, they tend to overfit the given data. (The cost
    equation of linear regression is unconstrained. We will discuss more about it
    in the first recipe). The output regression model may not perform very well on
    unseen datasets. Shrinkage methods are used to address this problem. Shrinkage
    methods are also called regularization methods. In the next two recipes, we will
    cover two different shrinkage methods called LASSO and ridge. In our final recipe,
    we will introduce the concept of cross-validation and see how we can use it to
    our advantage in estimating the parameter, alpha, that is passed to ridge regression,
    a type of shrinkage method.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从使用最小二乘估计的简单线性回归方法开始。在第一种方法的开头，我们将简要介绍回归的框架，这是理解本章其他方法所必需的基础背景信息。尽管非常强大，简单回归框架也存在一个缺点。由于无法控制线性回归系数的上下限，它们往往会过拟合给定的数据。（线性回归的代价函数是无约束的，我们将在第一种方法中进一步讨论）。输出的回归模型可能在未见过的数据集上表现不佳。为了应对这个问题，使用了收缩方法。收缩方法也被称为正则化方法。在接下来的两种方法中，我们将介绍两种不同的收缩方法——LASSO和岭回归。在最后一种方法中，我们将介绍交叉验证的概念，并展示如何利用它来估计传递给岭回归（作为一种收缩方法）的参数alpha。
- en: Predicting real-valued numbers using regression
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用回归预测实值
- en: Before we delve into this recipe, let's quickly understand how regression generally
    operates. This introduction is essential for understanding this and subsequent
    recipes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨此方案之前，先快速了解回归一般是如何运作的。这个介绍对于理解当前方案以及后续方案非常重要。
- en: 'Regression is a special form of function approximation. Here are the set of
    predictors:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 回归是一种特殊形式的函数逼近。以下是预测变量集：
- en: '![Predicting real-valued numbers using regression](img/B04041_07_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![使用回归预测实值数](img/B04041_07_01.jpg)'
- en: 'With each instance, `xi` has `m` attributes:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个实例`xi`具有`m`个属性：
- en: '![Predicting real-valued numbers using regression](img/B04041_07_02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![使用回归预测实值数](img/B04041_07_02.jpg)'
- en: 'The job of regression is to find a function such that when X is provided as
    an input to that function, it should return a Y response variable. Y is a vector
    of real-valued entries:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 回归的任务是找到一个函数，使得当`X`作为输入提供给该函数时，应该返回一个`Y`响应变量。`Y`是一个实值向量：
- en: '![Predicting real-valued numbers using regression](img/B04041_07_03.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![使用回归预测实值数](img/B04041_07_03.jpg)'
- en: We will use the Boston housing dataset in order to explain the regression framework.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用波士顿房价数据集来解释回归框架。
- en: 'The following link provides a good introduction to the Boston housing dataset:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了波士顿房价数据集的良好介绍：
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)。'
- en: The response variable, `Y` in this case, is the median value of an owner-occupied
    home in the Boston area. There are 13 predictors. The preceding web link provides
    a good description of all the predictor variables.The regression problem is defined
    as finding a function, `F`, such that if we give a previously unseen predictor
    value to this function, it should be able to give us the median house price.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，响应变量`Y`是波士顿地区自有住房的中位数价值。有13个预测变量。前面的网页链接提供了所有预测变量的良好描述。回归问题的定义是找到一个函数`F`，使得如果我们给这个函数一个以前未见过的预测值，它应该能够给出中位数房价。
- en: 'The function, `F(X)`, which is the output of our linear regression model, is
    a linear combination of the input, `X`, hence the name linear regression:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`F(X)`是我们线性回归模型的输出，它是输入`X`的线性组合，因此得名线性回归：
- en: '![Predicting real-valued numbers using regression](img/B04041_07_04.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![使用回归预测实值数](img/B04041_07_04.jpg)'
- en: The `wi` variable is the unknown value in the preceding equation. The modeling
    exercise is about discovering the `wi` variable. Using our training data, we will
    find the value of `wi`; `wi` is called the coefficient of the regression model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`wi`变量是前述方程中的未知值。建模过程的关键在于发现`wi`变量。通过使用我们的训练数据，我们将找到`wi`的值；`wi`被称为回归模型的系数。'
- en: 'A linear regression modeling problem is framed as: using the training data
    to find the coefficients:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归建模问题可以表述为：使用训练数据来找到系数：
- en: '![Predicting real-valued numbers using regression](img/B04041_07_05.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![使用回归预测实值数](img/B04041_07_05.jpg)'
- en: 'Such that:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使得：
- en: '![Predicting real-valued numbers using regression](img/B04041_07_06.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![使用回归预测实值数](img/B04041_07_06.jpg)'
- en: The above formula is as low as possible.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式值尽可能小。
- en: The lower the value of this equation (called the cost function in optimization
    terminology), the better the linear regression model. So, the optimization problem
    is to minimize the preceding equation, that is, find the values for the `wi` coefficient
    so that it minimizes the equation. We will not delve into the details of the optimization
    routines that are used. However, it is good to know this objective function because
    the next two recipes expect you to understand it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程的值越低（在优化术语中称为代价函数），线性回归模型越好。因此，优化问题就是最小化前述方程，即找到`wi`系数的值，使其最小化该方程。我们不会深入探讨优化算法的细节，但了解这个目标函数是有必要的，因为接下来的两个方案需要你理解它。
- en: 'Now, the question is how do we know that the model that we built using the
    training data, that is, our newly found coefficients, `w1, w2,..wm` are good enough
    to accurately predict unseen records? Once again, we will leverage the cost function
    defined previously. When we apply the model in our dev set or test set, we find
    the average square of the difference between the actual and predicted values,
    as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是我们如何知道使用训练数据构建的模型，即我们新找到的系数`w1, w2,..wm`，是否足够准确地预测未见过的记录？我们将再次利用之前定义的成本函数。当我们将模型应用于开发集或测试集时，我们找到实际值和预测值之间差异平方的平均值，如下所示：
- en: '![Predicting real-valued numbers using regression](img/B04041_07_07.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![使用回归预测实数](img/B04041_07_07.jpg)'
- en: The preceding equation is called the mean squared error—the metric by which
    we can say that our regression model is worthy of use. We want an output model
    where the average square of the difference between the actual and predicted values
    is as low as possible. This method of finding the coefficients is called the least
    square estimation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程被称为均方误差（mean squared error）——这是我们用来判断回归模型是否值得使用的标准。我们希望得到一个输出模型，使得实际值和预测值之间的差异平方的平均值尽可能低。寻找系数的这个方法被称为最小二乘估计。
- en: 'We will use scikit-learn''s `LinearRegression` class. However, it internally
    uses the `scipy.linalg.lstsq` method. The method of least squares provides us
    with a closed form solution to the regression problem. Refer to the following
    links for more information on the method of least squares and the derivation for
    the least squares:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用scikit-learn的`LinearRegression`类。然而，它在内部使用`scipy.linalg.lstsq`方法。最小二乘法为我们提供了回归问题的闭式解。有关最小二乘法及其推导的更多信息，请参考以下链接：
- en: '[https://en.wikipedia.org/wiki/Least_squares](https://en.wikipedia.org/wiki/Least_squares).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Least_squares](https://en.wikipedia.org/wiki/Least_squares)。'
- en: '[https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics))。'
- en: We gave a very simple introduction to regression. Curious readers can refer
    to the following books [http://www.amazon.com/exec/obidos/ASIN/0387952845/trevorhastie-20](http://www.amazon.com/exec/obidos/ASIN/0387952845/trevorhastie-20).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对回归做了一个非常简单的介绍。感兴趣的读者可以参考以下书籍：[http://www.amazon.com/exec/obidos/ASIN/0387952845/trevorhastie-20](http://www.amazon.com/exec/obidos/ASIN/0387952845/trevorhastie-20)。
- en: '[http://www.amazon.com/Neural-Networks-Learning-Machines-Edition/dp/0131471392](http://www.amazon.com/Neural-Networks-Learning-Machines-Edition/dp/0131471392)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.amazon.com/Neural-Networks-Learning-Machines-Edition/dp/0131471392](http://www.amazon.com/Neural-Networks-Learning-Machines-Edition/dp/0131471392)'
- en: Getting ready
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: The Boston data has 13 attributes and 506 instances. The target variable is
    a real number and the median value of the houses is in the thousands.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 波士顿数据集有13个属性和506个实例。目标变量是一个实数，房屋的中位数值在千美元左右。
- en: 'Refer to the following UCI link for more information about the Boston dataset:
    [https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下 UCI 链接了解有关波士顿数据集的更多信息：[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)。
- en: 'We will provide the names of these predictor and response variables, as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提供这些预测变量和响应变量的名称，如下所示：
- en: '![Getting ready](img/B04041_07_08.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![准备就绪](img/B04041_07_08.jpg)'
- en: How to do it…
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We will start with loading all the necessary libraries. We will follow it up
    by defining our first function, `get_data()`. In this function, we will read the
    Boston dataset and return it as predictor `x` and response variable `y`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从加载所有必要的库开始。接下来，我们将定义第一个函数`get_data()`。在这个函数中，我们将读取波士顿数据集，并将其作为预测变量`x`和响应变量`y`返回：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In our `build_model` function, we will construct our linear regression model
    with the given data. The following two functions, `view_model` and `model_worth`,
    are used to introspect the model that we have built:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`build_model`函数中，我们将使用给定的数据构建线性回归模型。以下两个函数，`view_model`和`model_worth`，用于检查我们所构建的模型：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `plot_residual` function is used to plot the errors in our regression model:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot_residual`函数用于绘制我们回归模型中的误差：'
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we will write our `main` function, which is used to invoke all the
    preceding functions:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写我们的`main`函数，用于调用之前的所有函数：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works…
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Let''s start with the main module and follow the code. We will load the predictor
    `x` and response variable `y` using the `get_data` function:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从主模块开始，跟着代码走。我们将使用 `get_data` 函数加载预测变量 `x` 和响应变量 `y`：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The function invokes scikit-learn's convenient `load_boston()` function in order
    to retrieve the Boston house pricing dataset as NumPy arrays.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数调用了 scikit-learn 提供的便捷 `load_boston()` 函数，以便将波士顿房价数据集作为 NumPy 数组进行检索。
- en: 'We will proceed to divide the data into the train and test sets using the `train_test_split`
    function from the Scikit library. We will reserve 30 percent of our dataset to
    test:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Scikit 库中的 `train_test_split` 函数，将数据划分为训练集和测试集。我们将保留数据集的 30% 用于测试：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Out of which, we will extract the dev set in the next line:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一行中提取开发集：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the next line, we will proceed to build our model using the training dataset
    by calling the `build_model` method. This model creates an object of a `LinearRegression`
    type. The `LinearRegression` class encloses SciPy''s least squares method:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一行，我们将通过调用 `build_model` 方法使用训练数据集来构建我们的模型。该模型创建一个 `LinearRegression` 类型的对象。`LinearRegression`
    类封装了 SciPy 的最小二乘法：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let's look at the parameters passed when initializing this class.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看初始化该类时传递的参数。
- en: The `fit_intercept` parameter is set to `True`. This tells the linear regression
    class to center the data. By centering the data, the mean value of each of our
    predictors is set to zero. The linear regression methods require the data to be
    centered by its mean value for a better interpretation of the intercepts. In addition
    to centering each attribute by its mean, we will also normalize each attribute
    by its standard deviation. We will achieve this using the `normalize` parameter
    and setting it to `True`. Refer to the [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Scaling and data standardization* recipes on
    how to perform normalization by each column. With the `fit_intercept` parameter,
    we will instruct the algorithm to include an intercept in order to accommodate
    any constant shift in the response variable. Finally, we will fit the model by
    invoking the fit function with our response variable `y` and predictor `x`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit_intercept` 参数设置为 `True`。这告诉线性回归类进行数据中心化。通过数据中心化，我们将每个预测变量的均值设置为零。线性回归方法要求数据根据其均值进行中心化，以便更好地解释截距。除了按均值对每个特征进行中心化外，我们还将通过其标准差对每个特征进行归一化。我们将使用
    `normalize` 参数并将其设置为 `True` 来实现这一点。有关如何按列进行归一化的更多信息，请参考[第 3 章](ch03.xhtml "第 3
    章 数据分析 – 探索与清理")，*缩放与数据标准化* 的相关内容。通过 `fit_intercept` 参数，我们将指示算法包含一个截距，以适应响应变量中的任何常数偏移。最后，我们将通过调用
    fit 函数并使用响应变量 `y` 和预测变量 `x` 来拟合模型。'
- en: Note
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Refer to the book, The Elements of Statistical Learning by Trevor Hastie et
    al. for more information about linear regression methodologies.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有关线性回归方法的更多信息，请参考 Trevor Hastie 等人所著的《统计学习元素》一书。
- en: It is good practice to inspect the model that we built so that we can have a
    better understanding of the model for further improvement or interpretability.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 检查我们构建的模型是一种良好的做法，这样我们可以更好地理解模型，以便进一步改进或提升可解释性。
- en: 'Let''s now plot the residuals (the difference between the predicted `y` and
    actual `y`) and the predicted `y` values as a scatter plot. We will invoke the
    `plot_residual` method to do this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制残差图（预测的 `y` 与实际的 `y` 之间的差异），并将预测的 `y` 值作为散点图展示。我们将调用 `plot_residual`
    方法来实现：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s look at the following graph:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看下面的图表：
- en: '![How it works…](img/B04041_07_09.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/B04041_07_09.jpg)'
- en: We can validate the regression assumptions in our dataset using this scatter
    plot. We don't see any pattern and the points are scattered uniformly along zero
    residual values.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个散点图来验证数据集中的回归假设。我们没有看到任何模式，点均匀地分布在零残差值附近。
- en: Note
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Refer to the book, *Data Mining Methods and Models* by *Daniel. T. Larose* for
    more information about using residual plots in order to validate linear regression
    assumptions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用残差图来验证线性回归假设的更多信息，请参考 *Daniel. T. Larose* 所著的《数据挖掘方法与模型》一书。
- en: 'We will then inspect our model using the `view_model` method. In this method,
    we will print our intercept and coefficient values. The linear regression object
    has two attributes, one called `coef_`, which provides us with an array of coefficients,
    and one called `intercept_`, which gives the intercept value:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用`view_model`方法检查我们的模型。在这个方法中，我们将打印截距和系数值。线性回归对象有两个属性，一个叫做`coef_`，它提供了系数的数组，另一个叫做`intercept_`，它提供截距值：
- en: '![How it works…](img/B04041_07_10.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_10.jpg)'
- en: 'Let''s take `coefficient 6`, which is the number of livable rooms in the house.
    The coefficient value is interpreted as: for every additional room, the price
    moves up three times.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下`coefficient 6`，即房屋中可居住的房间数量。系数值的解释是：每增加一个房间，价格上升三倍。
- en: Finally, we will look at how good our model is by invoking the `model_worth`
    function with our predicted response values and actual response values, both from
    our training and dev sets.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过调用`model_worth`函数来评估我们的模型好坏，该函数使用我们预测的响应值和实际响应值，二者均来自训练集和开发集。
- en: 'This function prints out the mean squared error value, which is the average
    square of the difference between the actual and predicted values:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数打印出均方误差值，即实际值与预测值之间差值的平方的平均值：
- en: '![How it works…](img/B04041_07_11.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_11.jpg)'
- en: 'We have a lower value in our dev set, which is an indication of how good our
    model is. Let''s check whether we can improve our mean squared error. What if
    we provide more features to our model? Let''s create some features from our existing
    attributes. We will use the `PolynomialFeatures` class from scikit-learn to create
    second order polynomials:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在开发集中的值较低，这表明我们的模型表现良好。让我们看看是否可以改善均方误差。如果我们为模型提供更多特征会怎么样？让我们从现有的属性中创建一些特征。我们将使用scikit-learn中的`PolynomialFeatures`类来创建二阶多项式：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will pass `2` as a parameter to `PolynomialFeatures` to indicate that we
    need second order polynomials. `2` is also the default value used if the class
    is initialized as empty:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`2`作为参数传递给`PolynomialFeatures`，表示我们需要二阶多项式。如果类初始化为空，`2`也是默认值：
- en: '![How it works…](img/B04041_07_12.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_12.jpg)'
- en: 'A quick look at the shape of the new `x` reveals that we now have 105 attributes,
    compared with 13\. Let''s build the model using the new polynomial features and
    check out the model''s accuracy:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 快速查看新`x`的形状，我们现在有105个属性，而原本只有13个。让我们使用新的多项式特征来构建模型，并检查模型的准确性：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![How it works…](img/B04041_07_13.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_13.jpg)'
- en: Our model has fitted well with the training dataset. Both in the dev and training
    sets, our polynomial features performed better than the original features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型已经很好地拟合了训练数据集。在开发集和训练集中，我们的多项式特征表现优于原始特征。
- en: 'Let''s finally look at how the model with the polynomial features and the model
    with the regular features perform with our test set:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看使用多项式特征的模型和使用常规特征的模型在测试集上的表现：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![How it works…](img/B04041_07_14.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_14.jpg)'
- en: We can see that our polynomial features have fared better than our original
    set of features using the test dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在测试数据集上，使用多项式特征的表现优于我们原始特征的表现。
- en: That is all you need to know about how to do linear regression in Python. We
    looked at how linear regression works and how we can build models to predict real-valued
    numbers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你需要知道的如何在Python中进行线性回归。我们了解了线性回归的工作原理，以及如何构建模型来预测实数值。
- en: There's more...
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Before we move on, we will see one more parameter setting in the `PolynomialFeatures`
    class called `interaction_only`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们将查看`PolynomialFeatures`类中的另一个参数设置，叫做`interaction_only`：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: By setting `interaction_only` to `true—with x1` and `x2 attributes—only` the
    `x1*x2` attribute is created. The squares of `x1` and `x2` are not created, assuming
    that the degree is two.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`interaction_only`设置为`true-with x1`和`x2 attributes-only`，只会创建`x1*x2`属性。`x1`和`x2`的平方不会创建，假设阶数为二。
- en: Our test set results were not as good as our dev set results for both the normal
    and polynomial features. This is a known problem with linear regression. Linear
    regression is not well equipped to handle variance. The problem that we are facing
    is a high variance and low bias. As the model complexity increases, that is, the
    number of attributes presented to the model increases. The model tends to fit
    the training data very well—hence a low bias—but starts to give degrading outputs
    with the test data. There are several techniques available to handle this problem.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试集结果与开发集的结果不如预期，尤其是在普通特征和多项式特征上。这是线性回归的一个已知问题。线性回归对于处理方差的能力较弱。我们面临的问题是高方差和低偏差。随着模型复杂度的增加，也就是呈现给模型的特征数量增多，模型往往能够很好地拟合训练数据——因此偏差较低——但开始在测试数据上出现下降的结果。对此问题，有几种技术可以应对。
- en: 'Let''s look at a method called recursive feature selection. The number of required
    attributes is passed as a parameter to this method. It recursively filters the
    features. In the ith run, a linear model is fitted to the data and, based on the
    coefficients'' values, the attributes are filtered; the attributes with lower
    weights are left out. Thus, the iteration continues with the remaining set of
    attributes. Finally, when we have the required number of attributes, the iteration
    stops. Let''s look at a code example:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看一个叫做递归特征选择的方法。所需特征的数量作为参数传递给该方法。它递归地筛选特征。在第i次运行中，会对数据拟合一个线性模型，并根据系数的值来筛选特征；那些权重较低的特征会被剔除。如此一来，迭代就继续进行，直到我们得到所需数量的特征时，迭代才会停止。接下来，让我们看看一个代码示例：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This code is very similar to the preceding linear regression code, except for
    the `build_model` method:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与之前的线性回归代码非常相似，唯一不同的是`build_model`方法：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In addition to the predictor `x` and response variable `y`, `build_model` also
    accepts the number of features to retain `no_features` as a parameter. In this
    case, we passed a value of 20, asking recursive feature elimination to retain
    only 20 significant features. As you can see, we first created a linear regression
    object. This object is passed to the `RFE` class. RFE stands for recursive feature
    elimination, a class provided by scikit-learn to implement recursive feature elimination.
    Let''s now evaluate our model against the training, dev, and test datasets:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预测变量`x`和响应变量`y`，`build_model`方法还接受一个参数，即要保留的特征数量`no_features`。在这个例子中，我们传递了一个值20，要求递归特征消除方法只保留20个重要特征。如你所见，我们首先创建了一个线性回归对象。这个对象被传递给`RFE`类。RFE代表递归特征消除，这是scikit-learn提供的一个类，用于实现递归特征消除。现在，我们来评估一下我们的模型在训练集、开发集和测试集上的表现：
- en: '![There''s more...](img/B04041_07_15.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![更多内容...](img/B04041_07_15.jpg)'
- en: The mean squared error of the test dataset is 13.20, almost half of what we
    had earlier. Thus, we are able to use the recursive feature elimination method
    to perform feature selection effectively and hence improve our model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集的均方误差为13.20，几乎是之前的一半。因此，我们能够有效地使用递归特征消除方法进行特征选择，从而提升我们的模型。
- en: See also
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: '*Scaling the data* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data Analysis
    – Explore and Wrangle"), *Data Analysis – Explore and Wrangle*'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第3章](ch03.xhtml "第3章 数据分析 – 探索与整理")中的*数据标准化*方法，*数据分析 – 探索与整理*'
- en: '*Standardizing the data* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Data Analysis – Explore and Wrangle*'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第3章](ch03.xhtml "第3章 数据分析 – 探索与整理")中的*数据标准化*方法，*数据分析 – 探索与整理*'
- en: '*Preparing data for model building* recipe in [Chapter 6](ch06.xhtml "Chapter 6. Machine
    Learning 1"), *Machine Learning I*'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第6章](ch06.xhtml "第6章 机器学习 I")中的*为模型构建准备数据*方法，*机器学习 I*'
- en: Learning regression with L2 shrinkage – ridge
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用L2收缩学习回归 – 岭回归
- en: 'Let''s extend the regression technique discussed before to include regularization.
    While training a linear regression model, some of the coefficients may take very
    high values, leading to instability in the model. Regularization or shrinkage
    is a way of controlling the weights of the coefficients such that they don''t
    take very large values. Let''s look at the linear regression cost function once
    again to understand what issues are inherently present with regression, and what
    we mean by controlling the weights of the coefficients:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将之前讨论的回归技术扩展，以包括正则化。在训练线性回归模型时，一些系数可能会取非常高的值，导致模型的不稳定。正则化或收缩是一种控制系数权重的方法，目的是使它们不会取非常大的值。让我们再次看看线性回归的成本函数，以理解回归固有的问题，以及我们通过控制系数权重所指的内容：
- en: '![Learning regression with L2 shrinkage – ridge](img/B04041_07_05.jpg)![Learning
    regression with L2 shrinkage – ridge](img/B04041_07_06.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![学习回归与L2收缩 – 岭回归](img/B04041_07_05.jpg)![学习回归与L2收缩 – 岭回归](img/B04041_07_06.jpg)'
- en: Linear regression tries to find the coefficients, `w0…wm`, such that it minimizes
    the preceding equation. There are a few issues with linear regression.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归试图找到系数`w0…wm`，使得它最小化上述方程。线性回归存在一些问题。
- en: If the dataset contains many correlated predictors, very small changes in the
    data can lead to an unstable model. Additionally, we will face a problem with
    interpreting the model results. For example, if we have two variables that are
    negatively correlated, these variables will have an opposite effect on the response
    variable. We can manually look at these correlations and remove one of the variables
    that is responsible and then proceed with the model building. However, it will
    be helpful if we can handle these scenarios automatically.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集包含许多相关的预测变量，数据的非常小的变化可能导致模型不稳定。此外，我们还会面临解释模型结果的问题。例如，如果我们有两个负相关的变量，这些变量将对响应变量产生相反的影响。我们可以手动查看这些相关性，移除其中一个有影响的变量，然后继续构建模型。然而，如果我们能够自动处理这些场景，那将更有帮助。
- en: We introduced a method called recursive feature elimination in the previous
    recipe to keep the most informative attributes and discard the rest. However,
    in this approach, we either keep a variable or don't keep it; our decisions are
    binary. In this section, we will see a way by which we can control the weights
    associated with the variables in such a way that the unnecessary variables are
    penalized heavily and they receive extremely low weights.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的例子中介绍了一种方法，叫做递归特征消除，用来保留最具信息性的属性并丢弃其余的属性。然而，在这种方法中，我们要么保留一个变量，要么不保留它；我们的决策是二元的。在这一部分，我们将看到一种方法，能够以控制变量权重的方式，强烈惩罚不必要的变量，使它们的权重降到极低。
- en: 'We will change the cost function of linear regression to include the coefficients.
    As you know, the value of the cost function should be at a minimum for the best
    model. By including the coefficients in the cost function, we can heavily penalize
    the coefficients that take a very high value. In general, these techniques are
    known as shrinkage methods, as they try to shrink the value of the coefficients.
    In this recipe, we will see L2 shrinkage, most commonly called ridge regression.
    Let''s look at the cost function for ridge regression:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将修改线性回归的成本函数，以包含系数。如你所知，成本函数的值应该在最佳模型时达到最小。通过将系数包含在成本函数中，我们可以对取非常高值的系数进行严重惩罚。一般来说，这些技术被称为收缩方法，因为它们试图收缩系数的值。在这个例子中，我们将看到L2收缩，通常称为岭回归。让我们看看岭回归的成本函数：
- en: '![Learning regression with L2 shrinkage – ridge](img/B04041_07_16.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![学习回归与L2收缩 – 岭回归](img/B04041_07_16.jpg)'
- en: As you can see, the sum of the square of the coefficients is added to the cost
    function. This way, when the optimization routine tries to minimize the preceding
    function, it has to heavily reduce the value of the coefficients to attain its
    objective. The alpha parameter decides the amount of shrinkage. Greater the alpha
    value, greater the shrinkage. The coefficient values are reduced to zero.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，系数的平方和被加入到成本函数中。这样，当优化程序试图最小化上述函数时，它必须大幅减少系数的值才能达到目标。α参数决定了收缩的程度。α值越大，收缩越大。系数值会被减小至零。
- en: With this little math background, let's jump into our recipe to see ridge regression
    in action.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了这些数学背景后，让我们进入实际操作，看看岭回归的应用。
- en: Getting ready
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Once again, we will use the Boston dataset to demonstrate ridge regression.
    The Boston data has 13 attributes and 506 instances. The target variable is a
    real number and the median value of the houses is in the thousands. Refer to the
    following UCI link for more information about the Boston dataset:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用波士顿数据集来演示岭回归。波士顿数据集有13个属性和506个实例。目标变量是一个实数，房屋的中位数价格在几千美元之间。有关波士顿数据集的更多信息，请参考以下UCI链接：
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)'
- en: We intend to generate the polynomial features of degree two and consider only
    the interaction effects. At the end of this recipe, we will see how much the coefficients
    are penalized.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打算生成二次的多项式特征，并仅考虑交互效应。在本教程的最后，我们将看到系数如何受到惩罚。
- en: How to do it…
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'We will start by loading all the necessary libraries. We will follow it up
    by defining our first function, `get_data()`. In this function, we will read the
    Boston dataset and return it as predictor `x` and response variable `y`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先加载所有必要的库。接下来，我们将定义第一个函数`get_data()`。在这个函数中，我们将读取波士顿数据集，并将其返回为预测变量`x`和响应变量`y`：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In our next `build_model` function, we will construct our ridge regression
    model with the given data. The following two functions, `view_model` and `model_worth`,
    are used to introspect the model that we built:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一个`build_model`函数中，我们将使用给定的数据构建岭回归模型。以下两个函数，`view_model`和`model_worth`，用于检查我们构建的模型：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we will write our `main` function, which is used to invoke all the
    preceding functions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写我们的`main`函数，该函数用于调用所有前面的函数：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How it works…
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Let's start with the main module and follow the code. We loaded the predictor
    `x` and response variable `y` using the `get_data` function. This function invokes
    scikit-learn' s convenient `load_boston()` function to retrieve the Boston house
    pricing dataset as NumPy arrays.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从主模块开始，并跟随代码。我们使用`get_data`函数加载了预测变量`x`和响应变量`y`。该函数调用了scikit-learn的便捷函数`load_boston()`，将波士顿房价数据集作为NumPy数组获取。
- en: We will proceed to divide the data into train and test sets using the `train_test_split`
    function from the scikit-learn library. We will reserve 30 percent of our dataset
    to test. Out of this, we will extract the dev set in the next line.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用scikit-learn库中的`train_test_split`函数将数据集划分为训练集和测试集。我们将保留30%的数据集用于测试。在下一行，我们将从中提取开发集。
- en: 'We will then build the polynomial features:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将构建多项式特征：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you can see, we set `interaction_only` to true. By setting `interaction_only`
    to true—with `x1` and `x2` attributes—only the `x1*x2` attribute is created. The
    squares of `x1` and `x2` are not created, assuming that the degree is 2\. The
    default degree is two:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们将`interaction_only`设置为`true`。通过将`interaction_only`设置为`true`，对于`x1`和`x2`属性，仅创建`x1*x2`属性，而不创建`x1`和`x2`的平方，假设度数为2。默认的度数为2：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Using the `transform` function, we will transform our train, dev, and test datasets
    to include the polynomial features.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`transform`函数，我们将转换我们的训练集、开发集和测试集，以包含多项式特征。
- en: 'In the next line, we will build our ridge regression model using the training
    dataset by calling the `build_model` method:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一行，我们将通过调用`build_model`方法使用训练数据集构建我们的岭回归模型：
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The attributes in the dataset are centered by its mean and standardized by its
    standard deviation using the `normalize` parameter and setting it to `true`. `Alpha`
    controls the amount of shrinkage. Its value is set to `0.015`. We didn't arrive
    at this number magically, but by running the model several times. Later in this
    chapter, we will see how to empirically arrive at the right value for this parameter.
    We will also fit the intercept for this model using the `fit_intercept` parameter
    . However, by default, the `fit_intercept` parameter is set to `true` and hence
    we do not specify it explicitly.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的属性通过其均值进行中心化，并使用标准差进行标准化，方法是使用`normalize`参数并将其设置为`true`。`Alpha`控制收缩的程度，其值设置为`0.015`。我们不是凭空得出这个数字的，而是通过多次运行模型得出的。稍后在本章中，我们将看到如何通过经验得出此参数的正确值。我们还将使用`fit_intercept`参数来拟合该模型的截距。然而，默认情况下，`fit_intercept`参数设置为`true`，因此我们不会显式指定它。
- en: 'Let''s now see how the model has performed in the training set. We will call
    the `model_worth` method to get the mean square error. This method takes the predicted
    response variable and the actual response variable to return the mean square error:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看模型在训练集中的表现。我们将调用 `model_worth` 方法来获取均方误差。此方法需要预测的响应变量和实际的响应变量，以返回均方误差：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Our output looks as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输出如下所示：
- en: '![How it works…](img/B04041_07_17.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_17.jpg)'
- en: 'Before we apply our model to the test set, let''s look at the coefficients''
    weights. We will call a function called `view_model` to view the coefficient''s
    weight:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在将模型应用于测试集之前，让我们先看看系数的权重。我们将调用一个名为 `view_model` 的函数来查看系数的权重：
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![How it works…](img/B04041_07_18.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_18.jpg)'
- en: We have not shown all the coefficients. There are a total of 92\. However, looking
    at some of them, the shrinkage effect should be visible. For instance, Coefficient
    1 is almost 0 (remember that it is a very small value and we have shown only the
    first three decimal places here).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有展示所有的系数，共有 92 个。然而，从一些系数来看，收缩效应应该是显而易见的。例如，系数 1 几乎为 0（请记住，它是一个非常小的值，这里仅显示了前三位小数）。
- en: 'Let''s proceed to see how our model has performed in the dev set:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续看看模型在开发集中的表现：
- en: '[PRE23]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![How it works…](img/B04041_07_19.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_19.jpg)'
- en: 'Not bad, we have reached a mean square error lower than our training error.
    Finally, let''s look at our model performance on the test set:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 不错，我们已经达到了比训练误差更低的均方误差。最后，让我们来看一下我们模型在测试集上的表现：
- en: '![How it works…](img/B04041_07_20.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_20.jpg)'
- en: Compared with our linear regression model in the previous recipe, we performed
    better on our test set.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面配方中的线性回归模型相比，我们在测试集上的表现更好。
- en: There's more…
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'We mentioned earlier that linear regression models are very sensitive to even
    small changes in the dataset. Let''s see a small example that will demonstrate
    this:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，线性回归模型对数据集中的任何小变化都非常敏感。让我们看一个小例子，来演示这一点：
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this code, we will fit both the linear regression and ridge regression models
    on the original data using the `build_model` function:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们将使用 `build_model` 函数对原始数据进行线性回归和岭回归模型的拟合：
- en: '[PRE25]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will introduce a small noise in our original data, as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在原始数据中引入少量噪声，具体如下：
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once again, we will fit the models on the noisy dataset. Finally, we will compare
    the coefficients'' weights:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次在噪声数据集上拟合模型。最后，我们将比较系数的权重：
- en: '![There''s more…](img/B04041_07_21.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多…](img/B04041_07_21.jpg)'
- en: 'After adding a small noise, when we try to fit a model using linear regression,
    the weights assigned are very different to the the weights assigned by the previous
    model. Now, let''s see how ridge regression performs:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加了少量噪声后，当我们尝试使用线性回归拟合模型时，分配的权重与前一个模型分配的权重非常不同。现在，让我们看看岭回归的表现：
- en: '![There''s more…](img/B04041_07_22.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多…](img/B04041_07_22.jpg)'
- en: The weights have not varied starkly between the first and second model. Hopefully,
    this demonstrates the stability of ridge regression under noisy data conditions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 权重在第一个和第二个模型之间没有发生剧烈变化。希望这可以展示岭回归在噪声数据条件下的稳定性。
- en: It is always tricky to choose the appropriate alpha value. A brute force approach
    is to run it through multiple values and trace the path of the coefficients. From
    the path, choose the alpha value where the weights don't vary dramatically. We
    will plot the coefficients' weights using the `coeff_path` function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的 alpha 值总是很棘手。一个粗暴的方法是通过多个值来运行，并跟踪系数的路径。通过路径，选择一个系数变化不剧烈的 alpha 值。我们将使用
    `coeff_path` 函数绘制系数的权重。
- en: 'Let''s look at the `coeff_path` function. It first generates a list of the
    alpha values:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看 `coeff_path` 函数。它首先生成一组 alpha 值：
- en: '[PRE27]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In this case, we generated 300 uniformly spaced numbers between 10 and 100\.
    For each of these alpha values, we will build a model and save its coefficients:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们生成了从 10 到 100 之间均匀分布的 300 个数字。对于这些 alpha 值中的每一个，我们将构建一个模型并保存其系数：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we will plot these coefficient weights against the alpha value:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将绘制这些系数权重与 alpha 值的关系图：
- en: '![There''s more…](img/B04041_07_23.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多…](img/B04041_07_23.jpg)'
- en: As you can see, the values stabilize around the alpha value of 100\. You can
    further zoom into a range close to 100 and look for an ideal value.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，值在 alpha 值为 100 附近趋于稳定。您可以进一步缩放到接近 100 的范围，并寻找一个理想的值。
- en: See also
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Predicting real valued numbers using regression* recipe in [Chapter 7](ch07.xhtml
    "Chapter 7. Machine Learning 2"), *Machine Learning II*'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.xhtml "第7章 机器学习 2")中的*使用回归预测实数值*方法，*机器学习II*
- en: '*Scaling the data* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data Analysis
    – Explore and Wrangle"), *Data Analysis – Explore and Wrangle*'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml "第3章 数据分析 – 探索与整理")中的*数据缩放*方法，*数据分析 – 探索与整理*
- en: '*Standardizing the data* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Data Analysis – Explore and Wrangle*'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml "第3章 数据分析 – 探索与整理")中的*标准化数据*方法，*数据分析 – 探索与整理*
- en: '*Preparing data for model building* recipe in [Chapter 6](ch06.xhtml "Chapter 6. Machine
    Learning 1"), *Machine Learning I*'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml "第6章 机器学习 1")中的*准备数据以构建模型*方法，*机器学习I*
- en: Learning regression with L1 shrinkage – LASSO
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用L1收缩学习回归 – LASSO
- en: '**Least absolute shrinkage and selection operator** (**LASSO**) is another
    shrinkage method popularly used with regression problems. LASSO leads to sparse
    solutions compared with ridge. A solution is called sparse if most of the coefficients
    are reduced to zero. In LASSO, a lot of the coefficients are made zero. In the
    case of correlated variables, LASSO selects only one of them, whereas ridge assigns
    equal weights to the coefficients of both the variables. This attribute of LASSO
    can hence be leveraged for variable selection. In this recipe, let''s see how
    we can leverage LASSO for variable selection.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小绝对收缩与选择算子**（**LASSO**）是另一种常用的回归问题收缩方法。与岭回归相比，LASSO能得到稀疏解。所谓稀疏解，指的是大部分系数被缩减为零。在LASSO中，很多系数被设为零。对于相关变量，LASSO只选择其中一个，而岭回归则为两个变量的系数分配相等的权重。因此，LASSO的这一特性可以用来进行变量选择。在这个方法中，让我们看看如何利用LASSO进行变量选择。'
- en: 'Let''s look at the cost function of LASSO regression. If you followed through
    the previous two recipes, you can quickly identify the difference:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下LASSO回归的代价函数。如果你已经跟随前两个方法，你应该能很快识别出区别：
- en: '![Learning regression with L1 shrinkage – LASSO](img/B04041_07_24.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![学习使用L1收缩的回归 – LASSO](img/B04041_07_24.jpg)'
- en: The coefficients are penalized by the sum of the absolute value of the coefficients.
    Once again, the alpha controls the level of penalization. Let's try to understand
    the intuition behind why L1 shrinkage leads to a sparse solution.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 系数会受到系数绝对值之和的惩罚。再次强调，alpha控制惩罚的程度。我们来尝试理解为什么L1收缩会导致稀疏解的直观原因。
- en: 'We can rewrite the preceding equation as an unconstrained cost function and
    a constraint, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面的方程重写为一个无约束的代价函数和一个约束，如下所示：
- en: 'Minimize:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化：
- en: '![Learning regression with L1 shrinkage – LASSO](img/B04041_07_25.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![学习使用L1收缩的回归 – LASSO](img/B04041_07_25.jpg)'
- en: 'Subject to the constraint:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 受约束条件的影响：
- en: '![Learning regression with L1 shrinkage – LASSO](img/B04041_07_25b.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![学习使用L1收缩的回归 – LASSO](img/B04041_07_25b.jpg)'
- en: 'With this equation in mind, let''s plot the cost function values in the coefficient
    space for two coefficients, `w0` and `w1`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这个方程，我们在系数空间中绘制`w0`和`w1`的代价函数值：
- en: '![Learning regression with L1 shrinkage – LASSO](img/B04041_07_26.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![学习使用L1收缩的回归 – LASSO](img/B04041_07_26.jpg)'
- en: The blue lines represent the contours of the cost function (without constraint)
    values for the different values of `w0` and `w1`. The green region represents
    the constraint shape dictated by the eta value. The optimized value where both
    the regions meet is when `w0` is set to 0\. We depicted a two-dimensional space
    where our solution is made sparse with `w0` set to 0\. In a multidimensional space,
    we will have a rhomboid in the green region, and LASSO will give a sparse solution
    by reducing many of the coefficients to zero.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色线条表示不同`w0`和`w1`值下代价函数（无约束）的等高线。绿色区域表示由eta值决定的约束形状。两个区域交汇的优化值是`w0`设为0时的情况。我们展示了一个二维空间，在这个空间中，通过将`w0`设为0，我们的解变得稀疏。在多维空间中，我们将有一个菱形区域，LASSO通过将许多系数缩减为零，给出一个稀疏解。
- en: Getting ready
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: Once again, we will use the Boston dataset to demonstrate LASSO regression.
    The Boston data has 13 attributes and 506 instances. The target variable is a
    real number and the median value of the houses is in the thousands.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用波士顿数据集来演示LASSO回归。波士顿数据集有13个属性和506个实例。目标变量是一个实数，房屋的中位数价值在千元范围内。
- en: 'Refer to the following UCI link for more information on the Boston dataset:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多波士顿数据集的信息，请参考以下UCI链接：
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)。'
- en: We will see how we can use LASSO for variable selection.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如何使用 LASSO 进行变量选择。
- en: How to do it…
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'We will start by loading all the necessary libraries. We will follow it up
    by defining our first function, `get_data()`. In this function, we will read the
    Boston dataset and return it as a predictor `x` and response variable `y`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先加载所有必要的库。接下来我们定义我们的第一个函数`get_data()`。在此函数中，我们将读取波士顿数据集，并将其作为预测变量`x`和响应变量`y`返回：
- en: '[PRE29]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In our next `build_model` function, we will construct our LASSO regression
    model with the given data. The following two functions, `view_model` and `model_worth`,
    are used to introspect the model that we built:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们接下来的`build_model`函数中，我们将使用给定的数据构建 LASSO 回归模型。接下来的两个函数，`view_model`和`model_worth`，用于检查我们构建的模型：
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We will define two functions, `coeff_path` and `get_coeff`, to inspect our
    model coefficients. The `coeff_path` function is invoked from the `build_model`
    function to plot the weights of the coefficients for different alpha values. The
    `get_coeff` function is invoked from the main function:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义两个函数，`coeff_path`和`get_coeff`，来检查我们的模型系数。`coeff_path`函数由`build_model`函数调用，用于绘制不同
    alpha 值下的系数权重。`get_coeff`函数则由主函数调用：
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we will write our `main` function, which is used to invoke all the
    preceding functions:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写我们的`main`函数，用于调用之前所有的函数：
- en: '[PRE32]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: How it works…
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Let's start with the main module and follow the code. We will load the predictor
    `x` and response variable `y` using the `get_data` function. The function invokes
    scikit-learn's convenient `load_boston()` function to retrieve the Boston house
    pricing dataset as NumPy arrays.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从主模块开始，跟随代码进行。我们将使用`get_data`函数加载预测变量`x`和响应变量`y`。该函数调用了 scikit-learn 提供的便捷`load_boston()`函数，从而将波士顿房价数据集作为
    NumPy 数组载入。
- en: 'We will proceed by calling `build_models`. In `build_models`, we will construct
    multiple models for the different values of `alpha`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续调用`build_models`。在`build_models`中，我们将为不同的`alpha`值构建多个模型：
- en: '[PRE33]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you can see, in the for loop, we also store the coefficient values for different
    values of alpha in a list.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，在 for 循环中，我们还将不同 alpha 值的系数值存储在一个列表中。
- en: 'Let''s plot the coefficient values for different alpha values by calling the
    `coeff_path` function:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过调用`coeff_path`函数来绘制不同 alpha 值下的系数值：
- en: '[PRE34]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the `x` axis, you can see that we have the alpha values, and in the `y`
    axis, we will plot all the coefficients for a given alpha value. Let''s see the
    output plot:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在`x`轴上，你可以看到我们有 alpha 值，而在`y`轴上，我们将为给定的 alpha 值绘制所有系数。让我们看看输出的图表：
- en: '![How it works…](img/B04041_07_27.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的……](img/B04041_07_27.jpg)'
- en: The different colored lines represent different coefficient values. As you can
    see, as the value of alpha increases, the coefficient weights merge towards zero.
    From this plot, we can select the value of alpha.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 不同颜色的线条代表不同的系数值。正如你所看到的，随着 alpha 值的增大，系数权重趋向于零。从这个图表中，我们可以选择 alpha 的值。
- en: 'For our reference, let''s fit a simple linear regression model:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，我们先拟合一个简单的线性回归模型：
- en: '[PRE35]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s look at the mean square error when we try to predict using our newly
    built model:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们尝试使用新构建的模型进行预测时的均方误差：
- en: '![How it works…](img/B04041_07_28.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的……](img/B04041_07_28.jpg)'
- en: 'Let''s proceed to select the coefficients based on LASSO:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续根据 LASSO 来选择系数：
- en: '[PRE36]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Based on our preceding graph, we selected `0.22`, `0.08`, and `0.01` as the
    alpha values. In the loop, we will call the `get_coeff` method. This method fits
    a LASSO model with the given alpha values and returns only the non-zero coefficients''
    indices:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的图表，我们选择了`0.22`、`0.08`和`0.01`作为 alpha 值。在循环中，我们将调用`get_coeff`方法。该方法使用给定的
    alpha 值拟合 LASSO 模型，并仅返回非零系数的索引：
- en: '[PRE37]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Essentially, we are selecting only those attributes that have a non-zero coefficient
    value—feature selection. Let''s get back to our `for` loop where we will fit a
    linear regression model with the reduced coefficients:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们只选择那些系数值非零的属性——特征选择。让我们回到我们的`for`循环，在那里我们将使用减少后的系数拟合线性回归模型：
- en: '[PRE38]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'What we want to know is how good our models would be if we predicted them with
    the reduced set of attributes, compared with the model that we built initially
    using the whole dataset:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要了解的是，如果我们使用减少后的属性集来预测模型，与使用整个数据集最初构建的模型相比，模型的效果如何：
- en: '![How it works…](img/B04041_07_29.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的……](img/B04041_07_29.jpg)'
- en: Look at the first pass where our alpha value is `0.22`. There are only two coefficients
    with non-zero values, `5` and `12`. The mean squared error is `30.51`, which is
    only `9` more than the model fitted with all the variables.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 看看我们的第一次尝试，alpha值为`0.22`时。只有两个系数的非零值，分别为`5`和`12`。均方误差为`30.51`，仅比使用所有变量拟合的模型多了`9`。
- en: Similarly, for the alpha value of `0.08`, there are three non-zero coefficients.
    We can see some improvement in the mean squared error. Finally, with `0.01` alpha
    value, 9 out of 13 attributes are selected and the mean square error is very close
    to the model built with all the attributes.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于`0.08`的alpha值，存在三个非零系数。我们可以看到均方误差有所改善。最后，对于`0.01`的alpha值，13个属性中有9个被选择，且均方误差非常接近使用所有属性构建的模型。
- en: As you can see, we didn't fit the model with all the attributes. We are able
    to choose a subset of the attributes automatically using LASSO. Thus, we have
    seen how LASSO can be used for variable selection.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们并没有使用所有属性来拟合模型。我们能够使用LASSO自动选择一部分属性子集。因此，我们已经看到了LASSO如何用于变量选择。
- en: There's more…
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: By keeping only the most important variables, LASSO avoids overfitting. However,
    as you can see, the mean squared error values are not that good. We can see that
    there is a loss in the predictive power because of LASSO.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保留最重要的变量，LASSO避免了过拟合。然而，如你所见，均方误差值并不理想。我们可以看到，LASSO导致了预测能力的损失。
- en: As said before, in the case of the correlated variables, LASSO selects only
    one of them, whereas ridge assigns equal weights to the coefficients of both the
    variables. Hence, ridge has a higher predictive power compared with LASSO. However,
    LASSO can do variable selection, which Ridge is not capable of performing.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，对于相关变量，LASSO只选择其中一个，而岭回归则对两个变量的系数赋予相等的权重。因此，与LASSO相比，岭回归具有更高的预测能力。然而，LASSO能够进行变量选择，而岭回归则不能。
- en: Note
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Refer to the book, *Statistical learning with sparsity: The Lasso and generalization*
    by *Trevor Hastie et al.* for more information about the LASSO and ridge methodologies.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*Trevor Hastie等人*的《*稀疏统计学习：Lasso与泛化*》一书，了解更多关于LASSO和岭回归方法的信息。
- en: See also
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见：
- en: '*Scaling the data* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data Analysis
    – Explore and Wrangle"), *Data Analysis – Explore and Wrangle*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml "第3章. 数据分析 – 探索与清洗")中，*数据标准化*的内容，*数据分析 – 探索与清洗*
- en: '*Standardizing the data* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Data Analysis – Explore and Wrangle*'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml "第3章. 数据分析 – 探索与清洗")中，*数据标准化*的内容，*数据分析 – 探索与清洗*
- en: '*Preparing data for model building* recipe in [Chapter 6](ch06.xhtml "Chapter 6. Machine
    Learning 1"), *Machine Learning I*'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml "第6章. 机器学习 I")中，*模型构建数据准备*的内容，*机器学习 I*
- en: '*Regression with L2 Shrinkage – Ridge* recipe in [Chapter 7](ch07.xhtml "Chapter 7. Machine
    Learning 2"), *Machine Learning II*'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.xhtml "第7章. 机器学习 II")中，*L2收缩回归 – 岭回归*的内容，*机器学习 II*
- en: Using cross-validation iterators with L1 and L2 shrinkage
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交叉验证迭代器与L1和L2收缩
- en: In the previous chapter, we saw methods to divide the data into train and test
    sets. In the subsequent recipes, we again performed a split on the test dataset
    to arrive at a dev dataset. The idea was to keep the test set away from the model
    building cycle. However, as we need to improve our model continuously, we used
    the dev set to test the model accuracy in each iteration. Though it's a good approach,
    this method is difficult to implement if we don't have a large dataset. We want
    to provide as much data as possible to train our model but still need to hold
    some of the data for the evaluation and final testing. In many real-world scenarios,
    it is very rare to get a very large dataset.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了将数据划分为训练集和测试集的方法。在接下来的步骤中，我们再次对测试数据集进行划分，以得到开发数据集。其目的是将测试集从模型构建周期中分离出来。然而，由于我们需要不断改进模型，因此我们使用开发集来测试每次迭代中的模型准确性。虽然这是一个好的方法，但如果数据集不是很大，实施起来会比较困难。我们希望尽可能多地提供数据来训练模型，但仍然需要保留部分数据用于评估和最终测试。在许多实际场景中，获取一个非常大的数据集是非常罕见的。
- en: In this recipe, we will see a method called cross-validation to help us address
    this issue. This approach is typically called k-fold cross-validation. The training
    set is divided into k-folds. The model is trained on K-1 (K minus 1) folds and
    the left out fold is used to test. This way, we don't need a separate dev dataset.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将看到一种叫做交叉验证的方法，帮助我们解决这个问题。这个方法通常被称为k折交叉验证。训练集被分成k个折叠。模型在K-1（K减1）个折叠上进行训练，剩下的折叠用来测试。这样，我们就不需要单独的开发数据集了。
- en: Let's see some of the iterators provided by the scikit-learn library to perform
    the k-fold cross-validation effectively. Equipped with the knowledge of cross-validation,
    we will further see how we can leverage cross-validation for the selection of
    the alpha values in shrinkage methods.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看scikit-learn库提供的一些迭代器，以便有效地执行k折交叉验证。掌握了交叉验证的知识后，我们将进一步了解如何利用交叉验证选择收缩方法中的alpha值。
- en: Getting ready
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the Iris dataset to demonstrate the various cross-validation iterators
    concepts. We will return to our Boston housing dataset to demonstrate how cross-validation
    can be used successfully to find the ideal alpha values in shrinkage methods.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用鸢尾花数据集来演示各种交叉验证迭代器的概念。接着，我们会回到波士顿住房数据集，演示如何利用交叉验证成功找到收缩方法中的理想alpha值。
- en: How to do it…
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Let''s look at how to use the cross validation iterator:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用交叉验证迭代器：
- en: '[PRE39]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In our main function, we will call the `get_data` function to load the Iris
    dataset. We will then proceed to demonstrate a simple k-fold and stratified k-folds.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的主函数中，我们将调用`get_data`函数来加载鸢尾花数据集。然后我们将演示一个简单的k折和分层k折交叉验证。
- en: 'With the knowledge of k-fold cross-validation, let''s write a recipe to leverage
    this newly found knowledge in enhancing ridge regression:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掌握k折交叉验证的知识，让我们写一个食谱，利用这些新获得的知识来提升岭回归：
- en: '[PRE40]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We will start by loading all the necessary libraries. We will follow it up by
    defining our first function, `get_data()`. In this function, we will read the
    Boston dataset and return it as a predictor `x` and response variable `y`.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先加载所有必要的库。接下来，我们将定义我们的第一个函数`get_data()`。在这个函数中，我们将读取波士顿数据集，并将其返回为预测变量`x`和响应变量`y`。
- en: In our next `build_model` function, we will construct our ridge regression model
    with the given data. We will leverage the k-fold cross-validation.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一个`build_model`函数中，我们将使用给定的数据构建岭回归模型，并利用k折交叉验证。
- en: The following two functions, `view_model` and `model_worth`, are used to introspect
    the model that we built.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个函数，`view_model`和`model_worth`，用于 introspect（内省）我们构建的模型。
- en: 'Finally, we will write the `display_param_results` function to view the model
    errors in each fold:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写`display_param_results`函数，以查看每个折叠中的模型误差：
- en: '[PRE41]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, we will write our `main` function, which is used to invoke all the
    preceding functions:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写我们的`main`函数，用来调用所有前面的函数：
- en: '[PRE42]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: How it works…
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'Let''s start with our main method. We will start with the `KFold` class. This
    iterator class is instantiated with the number of instances in our dataset and
    the number of folds that we require:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从主方法开始。我们将从`KFold`类开始。这个迭代器类是通过实例化数据集中实例的数量和我们所需的折数来创建的：
- en: '[PRE43]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, we can iterate through the folds, as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按以下方式遍历这些折叠：
- en: '[PRE44]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s see the print statement output:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看打印语句的输出：
- en: '![How it works…](img/B04041_07_30.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理…](img/B04041_07_30.jpg)'
- en: We can see that the data is split into three parts, each with 100 instances
    to train and 50 instances to test.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据被分成三部分，每部分有100个实例用于训练，50个实例用于测试。
- en: We will move on next to `StratifiedKFold`. Recall our discussion on having a
    uniform class distribution in the train and test split from the previous chapter.
    `StratifiedKFold` achieves a uniform class distribution across the three folds.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转到`StratifiedKFold`。回想一下我们在上一章中讨论的关于训练集和测试集中的类分布均匀的问题。`StratifiedKFold`在三个折叠中实现了均匀的类分布。
- en: 'It is invoked as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 它是这样调用的：
- en: '[PRE45]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: As it needs to know the distribution of the class label in the dataset, this
    iterator object takes the response variable `y` as one of its parameters. The
    other parameter is the number of folds requested.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它需要知道数据集中类标签的分布，因此这个迭代器对象将响应变量`y`作为其参数之一。另一个参数是请求的折叠数。
- en: 'Let''s print the shape of our train and test sets in these three folds, along
    with their class distribution. We will use the `class_distribution` function to
    print the distribution of the classes in each of the folds:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出这三折数据集中训练集和测试集的形状，并查看它们的类分布。我们将使用`class_distribution`函数打印每一折中的类分布：
- en: '[PRE46]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![How it works…](img/B04041_07_31.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![How it works…](img/B04041_07_31.jpg)'
- en: You can see that the classes are distributed uniformly.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到类的分布是均匀的。
- en: Let's assume that you build a five-fold dataset, you fit five different models,
    and you have five different accuracy scores. You can now take the mean of these
    scores to evaluate how good your model has turned out to be. If you are not satisfied,
    you can go ahead and start rebuilding your model with a different set of parameters
    and again run it on the five-fold data and see the mean accuracy score. This way,
    you can continuously improve the model by finding the right parameter values only
    using the training dataset.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你构建了一个五折交叉验证的数据集，拟合了五个不同的模型，并得到了五个不同的准确度分数。现在，你可以取这些分数的平均值来评估模型的好坏。如果你不满意，你可以继续调整参数，重新构建模型，再在五折数据上运行并查看平均准确度分数。这样，你可以通过仅使用训练数据集找到合适的参数值，不断改进模型。
- en: Armed with this knowledge, let's revisit our old ridge regression problem.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握了这些知识后，我们来重新审视我们之前的岭回归问题。
- en: Let's start with the `main` module and follow the code. We will load the predictor
    `x` and response variable `y` using the `get_data` function. This function invokes
    scikit-learn's convenient `load_boston()` function to retrieve the Boston house
    pricing dataset as NumPy arrays.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`main`模块开始，跟着代码走。我们将使用`get_data`函数加载预测变量`x`和响应变量`y`。该函数调用scikit-learn便捷的`load_boston()`函数，将波士顿房价数据集作为NumPy数组提取出来。
- en: We will proceed to divide the data into train and test sets using the `train_test_split`
    function from the scikit-learn library. We will reserve 30 percent of our dataset
    to test.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用scikit-learn库中的`train_test_split`函数将数据分割为训练集和测试集。我们将保留30%的数据集用于测试。
- en: 'We will then to build the polynomial features:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将构建多项式特征：
- en: '[PRE47]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As you can see, we set `interaction_only` to `true`. By setting `interaction_only`
    to `true`—with `x1` and `x2` attributes—only the `x1*x2` attribute is created.
    The squares of `x1` and `x2` are not created, assuming that the degree is two.
    The default degree is two:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们将`interaction_only`设置为`true`。通过将`interaction_only`设置为`true`——并配合`x1`和`x2`属性——只会创建`x1*x2`属性。`x1`和`x2`的平方不会被创建，假设度数为二。默认度数为二：
- en: '[PRE48]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Using the transform function, we will transform our train and test dataset
    to include the polynomial features. Let''s call the `build_model` function. The
    first thing that we notice in the `build_model` function is the k-fold declaration.
    We will apply our knowledge of cross-validation here and create a five-fold dataset:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`transform`函数，我们将转换训练集和测试集，以包含多项式特征。我们来调用`build_model`函数。在`build_model`函数中，我们首先注意到的是k折交叉验证的声明。我们将在此应用交叉验证的知识，并创建一个五折数据集：
- en: '[PRE49]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We will then create our ridge object:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将创建我们的岭回归对象：
- en: '[PRE50]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Let's now see how we can leverage our k-folds to figure out the ideal alpha
    value for our ridge regression. In the next line, we will create an object out
    of `GridSearchCV:`
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何利用k折交叉验证来找出岭回归的理想alpha值。在接下来的行中，我们将使用`GridSearchCV`创建一个对象：
- en: '[PRE51]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`GridSearchCV` is a convenient function from scikit-learn that helps us train
    our models with a range of parameters. In this case, we want to find the ideal
    alpha value, and hence, would like to train our models with different alpha values.
    Let''s look at the parameters passed to `GridSearchCV`:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`是scikit-learn中的一个便捷函数，帮助我们使用一系列参数训练模型。在这种情况下，我们希望找到理想的alpha值，因此我们想用不同的alpha值训练我们的模型。让我们看看传递给`GridSearchCV`的参数：'
- en: 'Estimator: This is the type of model that should be run with the given parameter
    and data. In our case, we want to run ridge regression. Hence, we will create
    a ridge object and pass it to `GridSearchCV`.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 估算器：这是应该使用给定参数和数据运行的模型类型。在我们的例子中，我们希望运行岭回归。因此，我们将创建一个岭回归对象，并将其传递给`GridSearchCV`。
- en: 'Param-grid: This is a dictionary of parameters that we want to evaluate our
    model on. Let''s work this through in detail. We will first declare the range
    of alpha values that we want to build our model on:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 参数网格：这是我们希望用来评估模型的参数字典。让我们详细处理这个问题。我们将首先声明要用于构建模型的alpha值范围：
- en: '[PRE52]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This gives us a NumPy array of 30 uniformly spaced elements starting from 0.0015
    and ending at 0.0017\. We want to build a model for each of these values. We will
    create a dictionary object called `grid_param` and make an entry under the alpha
    key with the generated NumPy array of alpha values:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个NumPy数组，包含30个均匀间隔的元素，起始于0.0015，结束于0.0017。我们希望为每个这些值构建一个模型。我们将创建一个名为`grid_param`的字典对象，并在alpha键下添加生成的NumPy数组作为alpha值：
- en: '[PRE53]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We will pass this dictionary as a parameter to `GridSearchCV`. Look at the entry,
    `param_grid=grid_param`.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个字典作为参数传递给`GridSearchCV`。看一下条目`param_grid=grid_param`。
- en: 'Cv: This defines the kind of cross-validation that we are interested in. We
    will pass the k-fold (five-fold) iterator that we created before as the cv parameter.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Cv：这定义了我们感兴趣的交叉验证类型。我们将传递之前创建的k折（五折）迭代器作为cv参数。
- en: Finally, we need to define a scoring function. In our case, we are interested
    in finding out the squared error. This is the metric with which we will evaluate
    our model.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要定义一个评分函数。在我们的案例中，我们关注的是求得平方误差。这是我们用来评估模型的指标。
- en: So, internal `GridSearchCV` will build five models for each of our parameter
    values and return the mean score when tested in the left out folds. In our case,
    we have five folds of test data, so the average of the score values across these
    five folds of test data is returned.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，内部的`GridSearchCV`将为我们的每个参数值构建五个模型，并在排除的折中测试时返回平均分数。在我们的案例中，我们有五个测试折，因此会返回这五个测试折中分数的平均值。
- en: With this explained, we will then fit our model, that is, start our grid search
    activity.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 解释完这些后，我们将开始拟合我们的模型，也就是说，启动网格搜索活动。
- en: 'Finally, we want to see the output at the various parameter settings. We will
    use the `display_param_results` function to display the average mean squared error
    across the different folds:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想查看不同参数设置下的输出。我们将使用`display_param_results`函数来显示不同折中的平均均方误差：
- en: '![How it works…](img/B04041_07_32.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_32.jpg)'
- en: 'Each line in the output displays the parameter alpha value and average mean
    squared error from the test folds. We can see that as we move deep into the 0.0016
    range, the mean square error is increasing. Hence, we decide to stop at 0.0015\.
    We can query the grid object to get the best parameter and estimator:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的每一行显示了参数alpha值和来自测试折的平均均方误差。我们可以看到，当我们深入到0.0016的范围时，均方误差在增加。因此，我们决定停在0.0015。我们可以查询网格对象以获取最佳参数和估计器：
- en: '[PRE54]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This was not the first set of alpha values that we tested it with. Our initial
    alpha values were as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是我们测试的第一组alpha值。我们最初的alpha值如下：
- en: '[PRE55]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The following was our output:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们的输出：
- en: '![How it works…](img/B04041_07_33.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_33.jpg)'
- en: 'When our alpha values were above 0.01, the mean squared error was shooting
    up. Hence, we again gave a new range:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的alpha值超过0.01时，均方误差急剧上升。因此，我们又给出了一个新的范围：
- en: '[PRE56]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Our output was as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输出如下：
- en: '![How it works…](img/B04041_07_34.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_34.jpg)'
- en: This way, iteratively we arrived at the range starting at 0.0015 and ending
    at 0.0017.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们通过迭代的方式得到了一个范围，从0.0015开始，到0.0017结束。
- en: 'We will then get the best estimator from our grid search and apply it to our
    train and test data:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将从我们的网格搜索中获取最佳估计器，并将其应用于我们的训练和测试数据：
- en: '[PRE57]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Our `model_worth` function prints the mean squared error value in our training
    dataset:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`model_worth`函数打印出我们训练数据集中的均方误差值：
- en: '![How it works…](img/B04041_07_35.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_35.jpg)'
- en: 'Let''s view our coefficient weights:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看我们的系数权重：
- en: '![How it works…](img/B04041_07_36.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_36.jpg)'
- en: We have not displayed all of them but when you run the code, you can view all
    the values.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有显示所有的系数，但当你运行代码时，可以查看所有的值。
- en: 'Finally, let''s apply the model to our test dataset:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将模型应用到我们的测试数据集：
- en: '![How it works…](img/B04041_07_37.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_07_37.jpg)'
- en: Thus, we used cross-validation and grid search to arrive at an alpha value for
    our ridge regression successfully. Our model has resulted in a lower mean squared
    error compared with the value in the ridge regression recipe.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们通过交叉验证和网格搜索成功地得出了岭回归的alpha值。与岭回归配方中的值相比，我们的模型实现了较低的均方误差。
- en: There's more…
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: There are other cross-validation iterators available with scikit-learn. Of particular
    interest in this case is the leave-one-out iterator. You can read more about this
    at [http://scikit-learn.org/stable/modules/cross_validation.html#leave-one-out-loo](http://scikit-learn.org/stable/modules/cross_validation.html#leave-one-out-loo).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了其他交叉验证迭代器。在这种情况下，特别值得关注的是留一法迭代器。你可以在[http://scikit-learn.org/stable/modules/cross_validation.html#leave-one-out-loo](http://scikit-learn.org/stable/modules/cross_validation.html#leave-one-out-loo)上阅读更多关于它的信息。
- en: In this method, given the number of folds, it leaves one record to test and
    returns the rest to train. For example, if your input data has 100 instances and
    if we require five folds, we will get 99 instances to train and one to test in
    each fold.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，给定折数，它会留出一条记录进行测试，并将其余的记录用于训练。例如，如果你的输入数据有100个实例，并且我们要求五折交叉验证，那么每一折中我们将有99个实例用于训练，一个实例用于测试。
- en: 'In the grid search method that we used before, if we don''t provide a custom
    iterator to the **cross validation** (**cv**) parameter, it will by default use
    the leave-one-out cross-validation method:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前使用的网格搜索方法中，如果没有为**交叉验证**（**cv**）参数提供自定义迭代器，它将默认使用留一法交叉验证方法：
- en: '[PRE58]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: See also
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Scaling the data* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data Analysis
    – Explore and Wrangle"), *Data Analysis – Explore and Wrangle*'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第3章](ch03.xhtml "第3章 数据分析 – 探索与整理")中的*数据缩放*方法，*数据分析 – 探索与整理*'
- en: '*Standardizing the data* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Data Analysis – Explore and Wrangle*'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第3章](ch03.xhtml "第3章 数据分析 – 探索与整理")中的*标准化数据*方法，*数据分析 – 探索与整理*'
- en: '*Preparing data for model building* recipe in [Chapter 6](ch06.xhtml "Chapter 6. Machine
    Learning 1"), *Machine Learning I*'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第6章](ch06.xhtml "第6章 机器学习 1")中的*为模型构建准备数据*方法，*机器学习 I*'
- en: '*Regression with L2 Shrinkage – Ridge* recipe in [Chapter 7](ch07.xhtml "Chapter 7. Machine
    Learning 2"), *Machine Learning II*'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第7章](ch07.xhtml "第7章 机器学习 2")中的*L2缩减回归 – 岭回归*方法，*机器学习 II*'
- en: '*Regression with L2 Shrinkage – Lasso* recipe in [Chapter 7](ch07.xhtml "Chapter 7. Machine
    Learning 2"), *Machine Learning II*'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第7章](ch07.xhtml "第7章 机器学习 2")中的*L2缩减回归 – Lasso*方法，*机器学习 II*'
