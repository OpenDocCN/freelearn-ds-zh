- en: Chapter 7. Cassandra Partitioning, High Availability, and Consistency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。Cassandra分区、高可用性和一致性
- en: 'In this chapter, you will understand the internals of Cassandra to learn how
    data partitioning is implemented and you''ll learn about the hashing technique
    employed on Cassandra''s keyset distribution. We will also get an insight into
    replication and how it works, and the feature of hinted handoff. We will cover
    the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解Cassandra的内部，学习数据分区是如何实现的，你将了解Cassandra的键集分布上采用的哈希技术。我们还将深入了解复制以及它的工作原理，以及暗示的传递特性。我们将涵盖以下主题：
- en: Data partitioning and consistent hashing; we'll look at practical examples
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分区和一致性哈希；我们将看一些实际例子
- en: Replication, consistency, and high availability
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制、一致性和高可用性
- en: Consistent hashing
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一致性哈希
- en: Before you understand its implication and application in Cassandra, let's understand
    consistent hashing as a concept.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在你理解它在Cassandra中的含义和应用之前，让我们先了解一致性哈希作为一个概念。
- en: Consistent hashing works on the concept in its name—that is *hashing* and as
    we know, for a said hashing algorithm, the same key will always return the same
    hash code—thus, making the approach pretty deterministic by nature and implementation.
    When we use this approach for sharding or dividing the keys across the nodes in
    the cluster, consistent hashing is the technique that determines which node is
    stored in which node in the cluster.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性哈希按照其名称的概念工作——即*哈希*，正如我们所知，对于一个给定的哈希算法，相同的键将始终返回相同的哈希码——因此，这种方法在本质和实现上都是非常确定的。当我们将这种方法用于在集群中的节点之间进行分片或划分键时，一致性哈希是一种确定哪个节点存储在集群中的哪个节点的技术。
- en: Have a look at the following diagram to understand the concept of consistent
    hashing; imagine that the ring depicted in the following diagram represents the
    Cassandra ring and the nodes are marked here in letters along with the numerals
    that actually mark the objects (inverted triangles) to be mapped to the ring.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下下面的图表，理解一致性哈希的概念；想象一下下面图表中所描述的环代表Cassandra环，这里标记的节点是用字母标记的，实际上标记了要映射到环上的对象（倒三角形）。
- en: '![Consistent hashing](img/00048.jpeg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![一致性哈希](img/00048.jpeg)'
- en: Consistent hashing for the Cassandra cluster
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra集群的一致性哈希
- en: 'To compute the ownership of the object to the node it belongs to, all that''s
    required is traversal in clockwise to encounter the next node. The node that follows
    the data item, which is an inverted triangle, is the node that owns the object,
    for example:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算对象所属的节点的所有权，只需要顺时针遍历，遇到下一个节点即可。跟随数据项（倒三角形）的节点就是拥有该对象的节点，例如：
- en: '**1** belongs to node **A**'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1**属于节点**A**'
- en: '**2** belongs to node **B**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2**属于节点**B**'
- en: '**3** belongs to node **C**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3**属于节点**C**'
- en: '**4** belongs to node **C**'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4**属于节点**C**'
- en: '**5** belongs to node **D**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5**属于节点**D**'
- en: '**6** belongs to node **E**'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6**属于节点**E**'
- en: '**7** belongs to node **F**'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**7**属于节点**F**'
- en: '**8** belongs to node **H**'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**8**属于节点**H**'
- en: '**9** belongs to node **H**'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**9**属于节点**H**'
- en: So as you see, this uses simple hashing to compute the ownership of the key
    in a ring, based on owned token range.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你看，这使用简单的哈希来计算环中键的所有权，基于拥有的标记范围。
- en: Let's look at a practical example of consistent hashing; to explain this let's
    take a sample column family where the partition key value is the name.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个一致性哈希的实际例子；为了解释这一点，让我们以一个样本列族为例，其中分区键值是名称。
- en: 'Let''s say the following is the column value data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设以下是列值数据：
- en: '| Name | Gender |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 名字 | 性别 |'
- en: '| --- | --- |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Jammy | M |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Jammy | M |'
- en: '| Carry | F |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Carry | F |'
- en: '| Jesse | M |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Jesse | M |'
- en: '| Sammy | F |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Sammy | F |'
- en: 'Here is what the hash-mapping would look like:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是哈希映射的样子：
- en: '| Partition key | Hash value |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 分区键 | 哈希值 |'
- en: '| --- | --- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Jim | 2245462676723220000.00 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| Jim | 2245462676723220000.00 |'
- en: '| Carol | 7723358927203680000.00 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Carol | 7723358927203680000.00 |'
- en: '| Johnny | 6723372854036780000.00 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Johnny | 6723372854036780000.00 |'
- en: '| Suzy | 1168604627387940000.00 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| Suzy | 1168604627387940000.00 |'
- en: 'Let''s say I have four nodes with the following range; here is how the data
    would be distributed:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我有四个节点，具有以下范围；数据将如何分布：
- en: '| Node | Start range | End range | Partition key | Hash value |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 节点 | 起始范围 | 结束范围 | 分区键 | 哈希值 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| A | 9223372036854770000.00 | 4611686018427380000.00 | Jammy | 6723372854036780000.00
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| A | 9223372036854770000.00 | 4611686018427380000.00 | Jammy | 6723372854036780000.00
    |'
- en: '| B | 4611686018427380000.00 | 1.00 | Jesse | 2245462676723220000.00 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| B | 4611686018427380000.00 | 1.00 | Jesse | 2245462676723220000.00 |'
- en: '| C | 0.00 | 4611686018427380000.00 | suzy | 1168604627387940000.00 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| C | 0.00 | 4611686018427380000.00 | suzy | 1168604627387940000.00 |'
- en: '| D | 4611686018427380000.00 | 9223372036854770000.00 | Carry | 7723358927203680000.00
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| D | 4611686018427380000.00 | 9223372036854770000.00 | Carry | 7723358927203680000.00
    |'
- en: Now that you understand the concept of consistent hashing, let's look at the
    scenarios where the one or more node goes down and comes back up.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了一致性哈希的概念，让我们来看看一个或多个节点宕机并重新启动的情况。
- en: One or more node goes down
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个或多个节点宕机
- en: 'We are currently looking at a very common scenario where we envision that one
    node goes down; for instance, here we have captured two of them going down: **B**
    and **E**. What will happen now? Well nothing much, we''d follow the same pattern
    as before, which moves clockwise to find the next live node and allocate the values
    to that node.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前正在看一个非常常见的情况，即我们设想一个节点宕机；例如，在这里我们捕捉到两个节点宕机：**B**和**E**。现在会发生什么？嗯，没什么大不了的，我们会像以前一样按照相同的模式进行，顺时针移动以找到下一个活动节点，并将值分配给该节点。
- en: 'So in our case, the allocations would change to the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在我们的情况下，分配将改变如下：
- en: '![One or more node goes down](img/00049.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![一个或多个节点宕机](img/00049.jpeg)'
- en: 'The allocation in the preceding figure is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面图中的分配如下：
- en: '**1** belongs to **A**'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1**属于**A**'
- en: '**2**, **3**, and **4** belong to **C**'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2**，**3**和**4**属于**C**'
- en: '**5** belongs to **D**'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5**属于**D**'
- en: '**6**, and **7** belong to **F**'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6**，**7**属于**F**'
- en: '**8**, and **9** belong to **H**'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**8**，**9**属于**H**'
- en: One or more node comes back up
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个或多个节点重新上线
- en: 'Now let''s assume a scenario where node **2** comes back up; well, what happens
    then is again the same as on prior explanation, and the ownership is reestablished
    as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们假设一个场景，节点 **2** 再次上线；那么接下来的情况与之前的解释相同，所有权将重新建立如下：
- en: '**1** belongs to **A**'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1** 属于 **A**'
- en: '**2** belongs to **B**'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2** 属于 **B**'
- en: '**3**, and **4** belong to **C**'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3** 和 **4** 属于 **C**'
- en: '**5** belongs to **D**'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5** 属于 **D**'
- en: '**6**, and **7** belong to **F**'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6** 和 **7** 属于 **F**'
- en: '**8**, and **9** belong to **H**'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**8** 和 **9** 属于 **H**'
- en: So, we have demonstrated that this techniques works for all situations and that's
    why it is used.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经证明了这种技术适用于所有情况，这就是为什么它被使用的原因。
- en: Replication in Cassandra and strategies
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassandra中的复制和策略
- en: 'Replicating means to create a copy. This copy makes the data redundant and
    thus available even when one node fails or goes down. In Cassandra, you have the
    option to specify the replication factor as part of the creation of the keyspace
    or to later modify it. Attributes that need to be specified in this context are
    as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 复制意味着创建一个副本。这个副本使数据冗余，因此即使一个节点失败或宕机，数据也是可用的。在Cassandra中，您可以选择在创建keyspace的过程中指定复制因子，或者稍后修改它。在这种情况下需要指定的属性如下：
- en: '**Replication factor**: This is a numeric value specifying the number of replicas'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制因子**：这是指定副本数量的数字值'
- en: '**Strategy**: This could be simple strategy or topology strategy; this decides
    the placement of replicas across the cluster'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：这可以是简单策略或拓扑策略；这决定了在集群中的副本放置'
- en: 'Internally, Cassandra uses the row key to store replicas or copies of data
    across various nodes on the cluster. A replication factor of *n* means there are
    *n* copies of data stored on *n* different nodes. There are certain rules of thumb
    with replication, and they are as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，Cassandra使用行键在集群的各个节点上存储数据的副本或复制。复制因子 *n* 意味着数据在 *n* 个不同节点上有 *n* 个副本。复制有一些经验法则，它们如下：
- en: A replication factor should never be more than the number of nodes in a cluster,
    or you will run into exceptions due to not enough replicas and Cassandra will
    start rejecting the writes and reads, though replication factor would continue
    uninterrupted
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制因子不应该大于集群中节点的数量，否则由于副本不足，Cassandra将开始拒绝写入和读取，尽管复制因子将继续不间断地进行
- en: A replication factor should not be so small that data is lost forever if one
    odd node goes down
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果复制因子太小，那么如果一个奇数节点宕机，数据将永远丢失
- en: '**Snitch** is used to determine the physical location of nodes, attributes
    such as closeness to each other, and so on, which are of value when a vast amount
    of data is to be replicated and moved to and fro. In all such situations, network
    latency plays a very important part. The two strategies currently supported by
    Cassandra are as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**Snitch** 用于确定节点的物理位置，例如彼此的接近程度等，在大量数据需要复制和来回移动时具有价值。在所有这些情况下，网络延迟都起着非常重要的作用。Cassandra目前支持的两种策略如下：'
- en: '**Simple**: This is the default strategy provided by Cassandra for all keyspaces.
    It employs around a single data center. It''s pretty straightforward and simple
    in its operation; as the name suggests, the partitioner checks the key-value against
    the node range to determine the placement of the first replica. Thereon, the subsequent
    replicas are placed on the next nodes in a clockwise order. So if data item "A"
    has a replication of "3", and the partitioner decides the first node based on
    the key and ownership, on this node the subsequent replicas are created in a clockwise
    order.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单**：这是Cassandra为所有keyspaces提供的默认策略。它使用一个数据中心。它的操作非常简单直接；正如其名称所示，分区器检查键值对与节点范围的关系，以确定第一个副本的放置位置。然后，后续的副本按顺时针顺序放置在下一个节点上。因此，如果数据项
    "A" 的复制因子为 "3"，并且分区器根据键和所有权决定了第一个节点，那么在这个节点上，后续的副本将按顺时针顺序创建。'
- en: '**Network**: This is the topology that is used when we have the Cassandra cluster
    distributed across data centers. Here, we can plan our replica placement and define
    how many replicas we want to place in each data center. This approach makes the
    data geo-redundant and thus more fail-safe in cases where the entire data center
    crashes. The following are two things you should consider when making a choice
    on replica placement across data centers:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络**：这是当我们的Cassandra集群分布在多个数据中心时使用的拓扑。在这里，我们可以规划我们的副本放置，并定义我们想要在每个数据中心放置多少副本。这种方法使数据地理冗余，因此在整个数据中心崩溃的情况下更加安全。在选择跨数据中心放置副本时，应考虑以下两个因素：'
- en: Each data center should be self-sufficient to satisfy the requests
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据中心都应该是自给自足的，以满足请求
- en: Failover or crash situations
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障转移或崩溃情况
- en: If we have *2 replicas of datum in a data center*, then we have four copies
    of data and each data center has a tolerance for one node failure for the consistency
    `ONE`. If we get into the node of *3 replicas of datum in a data center*, then
    we have six copies of data and each data center has a tolerance for multiple node
    failures for the consistency of `ONE`. This strategy also permits asymmetrical
    replication.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在一个数据中心中有 *2 个数据副本*，那么我们就有四份数据副本，每个数据中心对一节点故障有一份数据的容忍度，以保持一致性 `ONE`。如果在一个数据中心中有
    *3 个数据副本*，那么我们就有六份数据副本，每个数据中心对多个节点故障有一份数据的容忍度，以保持一致性 `ONE`。这种策略也允许不对称复制。
- en: Cassandra consistency
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassandra一致性
- en: As we said in an earlier chapter, Cassandra eventually becomes consistent and
    follows the AP principal of the CAP theorem. Consistency refers to how up to date
    the information across all data replicas in a Cassandra cluster is. Cassandra
    does eventually guarantee consistency. Now let's have a closer look; well, let's
    say I have five node Cassandra clusters and a replication factor of 3\. This means
    if I have a *data item1*, it would be replicated to three nodes, let's say node1,
    node2, and node3; let's assume the key of this datum is *key1*. Now if the value
    of this key is to be rewritten and the write operation is performed on node1,
    then Cassandra internally replicates the values to other replicas, which are node2
    and node3\. But this update happens in the background and is not immediate; this
    is the mechanism of eventual consistency.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中所说，Cassandra最终变得一致，并遵循CAP定理的AP原则。一致性指的是Cassandra集群中所有数据副本的信息有多新。Cassandra最终保证一致性。现在让我们仔细看一下；假设我有一个由五个节点组成的Cassandra集群，复制因子为3。这意味着如果我有一个*数据项1*，它将被复制到三个节点，比如节点1、节点2和节点3；假设这个数据的键是*键1*。现在，如果要重写此键的值，并且在节点1上执行写操作，那么Cassandra会在内部将值复制到其他副本，即节点2和节点3。但此更新是在后台进行的，不是立即的；这就是最终一致性的机制。
- en: Cassandra provides the concept of offering the (read and write) client applications
    the decision of what consistency level they want to use to read and write to the
    data store.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra提供了向（读和写）客户端应用程序提供决定使用何种一致性级别来读取和写入数据存储的概念。
- en: Write consistency
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写一致性
- en: Let's inspect the write operation a little closely in Cassandra. Well, when
    a write operation is done in Cassandra, the client can specify the consistency
    at which the operation should be performed.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细检查一下Cassandra中的写操作。当在Cassandra中执行写操作时，客户端可以指定操作应执行的一致性级别。
- en: This means that if the replication factor is *x* and a write operation is performed
    with a consistency of *y* (where y is less than x), then Cassandra will wait for
    successful write to complete on *y* nodes before returning a successful acknowledgement
    to the client, marking the operation as complete. For the remaining *x-y* replicas,
    the data is propagated and replicated internally by the Cassandra processes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果复制因子为*x*，并且使用一致性为*y*（其中y小于x）执行写操作，那么Cassandra将在成功写入*y*个节点后，才向客户端返回成功的确认，并标记操作为完成。对于剩余的*x-y*个副本，数据将由Cassandra进程在内部传播和复制。
- en: 'The following table shows the various consistency levels and their implication
    where we have `ANY` that has the benefit of the highest availability with the
    lowest consistency, and `ALL` that offers the highest consistency but the lowest
    availability. So, being a client, one has to review the use case before deciding
    upon which consistency to choose. The following is a table with a few popular
    options and their implications:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了各种一致性级别及其含义，其中`ANY`具有最高可用性和最低一致性的优势，而`ALL`提供最高一致性但最低可用性。因此，作为客户端，在决定选择哪种一致性之前，必须审查使用情况。以下是一张包含一些常见选项及其含义的表格：
- en: '| Consistency level | Implication |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 一致性级别 | 含义 |'
- en: '| --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ANY | The write operation is returned as successful when the datum is written
    onto at least one node, where the node could either be a replica node or a non-replica
    node |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ANY | 当数据写入至少一个节点时，写操作将返回成功，其中节点可以是副本节点或非副本节点 |'
- en: '| ONE | The write operation is returned as successful when the datum is written
    onto at least one replica node |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ONE | 当数据写入至少一个副本节点时，写操作将返回成功 |'
- en: '| TWO | The write operation is returned as successful when the datum is written
    onto at least two replica nodes |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| TWO | 当数据写入至少两个副本节点时，写操作将返回成功 |'
- en: '| QUORUM | The write operation is returned as successful when the datum is
    written to the quorum of the replica node (where the quorum is n/2+1, and n is
    the replication factor) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| QUORUM | 当数据写入副本节点的法定副本数（法定副本数为n/2+1，n为复制因子）时，写操作将返回成功 |'
- en: '| ALL | The write operation is returned as successful when the datum is written
    onto all replica nodes |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ALL | 当数据写入所有副本节点时，写操作将返回成功 |'
- en: 'The following figure depicting the write operation on a four-node cluster,
    which has a replication factor of **3** and consistency of **2**:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了在具有复制因子**3**和一致性**2**的四节点集群上的写操作：
- en: '![Write consistency](img/00050.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![写一致性](img/00050.jpeg)'
- en: 'So as you see, the write operation is completed in three steps:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如您所看到的，写操作分为三个步骤：
- en: A write is issued from the client
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从客户端发出写操作
- en: The write is executed and completed on **replica1**
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写操作在**副本1**上执行并完成
- en: The write is executed and completed on **replica2**
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写操作在**副本2**上执行并完成
- en: An acknowledgement is issued to the client when a write is successfully completed
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当写操作成功完成时，向客户端发出确认
- en: Read consistency
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读一致性
- en: The read consistency is analogues to write consistency, it denotes how many
    replicas should respond or confirm their alignment to the data being returned
    to read operation before the results are returned to the client querying the Cassandra
    data store. This means if on an *N* node cluster with a replication factor of
    *x*, a read query is issued with a read consistency of *y* (y is less than x),
    then Cassandra would check the *y* replicas and then return the results. The results
    are validated on the basis that the most recent data is used to satisfy the request,
    and this is verified by the timestamp associated with each column.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 读一致性类似于写一致性，它表示在将结果返回给查询Cassandra数据存储的客户端之前，应有多少副本响应或确认其与返回的数据的一致性。这意味着，如果在具有复制因子*x*的*N*节点集群上，使用读一致性*y*（y小于x）发出读查询，则Cassandra将检查*y*个副本，然后返回结果。结果将根据使用最新数据来满足请求，并通过与每个列关联的时间戳进行验证。
- en: 'The following **Cassandra Query Language** (**CQL**), fetch the data from the
    column family with quorum consistency as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下**Cassandra查询语言**（**CQL**），使用四分一一致性从列族中获取数据如下：
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The functions of the CQL are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: CQL的功能如下：
- en: '| Consistency level | Implication |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 一致性级别 | 含义 |'
- en: '| --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ONE | A read request is serviced by the response from the closest replica
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ONE | 读请求由最近的副本的响应服务 |'
- en: '| TWO | A read request is serviced by the most recent response from one of
    the two closest replicas |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| TWO | 读请求由最近的两个副本中的一个最新响应服务 |'
- en: '| THREE | This level returns the most recent data from three of the closest
    replicas |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| THREE | 此级别从最近的三个副本返回最新的数据 |'
- en: '| QUORUM | A read request is serviced by the most recent responses from the
    quorum of replicas |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| QUORUM | 读请求由大多数副本的最新响应服务 |'
- en: '| ALL | A read request is serviced by the most recent response from all the
    replicas |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ALL | 读请求由所有副本的最新响应服务 |'
- en: Consistency maintenance features
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一致性维护功能
- en: 'In the previous section, we discussed read and write consistency in depth,
    and one thing that came clear is that Cassandra doesn''t provide or work towards
    total consistency at the time the read or write operation is performed; it executes
    and completes the request as per client''s consistency specifications. Another
    feature is *eventual consistency*, which highlights that there is some magic behind
    the veil that guarantees that eventually all data will be consistent. Now this
    magic is performed by certain components within Cassandra, and some are mentioned
    as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们深入讨论了读取和写入一致性，清楚的一点是Cassandra在执行读取或写入操作时不提供或不努力实现总一致性；它根据客户端的一致性规范执行并完成请求。另一个特性是*最终一致性*，它强调了在幕后有一些魔法，保证最终所有数据将是一致的。现在这个魔法是由Cassandra内部的某些组件执行的，其中一些如下所述：
- en: '**Read repair**: This service ensures that data across all the replicas is
    and up to date. This way, the row is consistent and has been updated with recent
    values across all replicas. This operation is executed by a job. Cassandra is
    running to execute repair read operation issued by the coordinator.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**读修复**：此服务确保所有副本之间的数据是最新的。这样，行就是一致的，并且已经使用最新的值更新了所有副本。此操作由作业执行。Cassandra正在运行以执行由协调员发出的读修复操作。'
- en: '**Anti-entropy repair service**: This service ensures that the data that''s
    not read very frequently, or when a downed host joins back, is in consistent a
    state. This is a regular cluster maintenance operation.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反熵修复服务**：此服务确保不经常读取的数据，或者当一个宕机的主机重新加入时，处于一致的状态。这是一个常规的集群维护操作。'
- en: '**Hinted handoff**: This is another unique and wonderful operation on Cassandra.
    When the write operation is executed, the coordinator issues a write operation
    to all replicas, irrespective of the consistency specified and waits for an acknowledgement.
    As soon as the acknowledgement count reaches the value mentioned on consistency
    of the operation, the thread is completed and the client is notified about its
    success. On the remaining replicas, the values are written using hinted handoffs.
    The hinted handoff approach is a savior when a few nodes are down. Let''s say
    one of the replicas is down and a write operation is executed with a consistency
    of `ANY`; in that case, one replica takes the write operation and hints to the
    neighboring replicas, which are currently down. When the downed replicas are revived,
    then the values are written back to them by taking hints from live replicas.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示性交接**：这是Cassandra上另一个独特而奇妙的操作。当执行写操作时，协调员向所有副本发出写操作，而不管指定的一致性，并等待确认。一旦确认计数达到操作的一致性上提到的值，线程就完成了，并且客户端被通知其成功。在剩余的副本上，使用提示性交接写入值。当一些节点宕机时，提示性交接方法是一个救世主。假设其中一个副本宕机，并且使用`ANY`的一致性执行写操作；在这种情况下，一个副本接受写操作并提示给当前宕机的相邻副本。当宕机的副本恢复时，然后从活动副本获取提示将值写回它们。'
- en: Quiz time
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测验时间
- en: 'Q.1\. State whether the following statements are true or false:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Q.1\. 判断以下陈述是真还是假：
- en: Cassandra has a default consistency of `ALL`.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cassandra有一个默认的`ALL`一致性。
- en: '`QUORUM` is the consistency level that provides the highest availability.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`QUORUM`是提供最高可用性的一致性级别。'
- en: Cassandra uses a snitch to identify the closeness of the nodes.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cassandra使用一个snitch来识别节点的接近程度。
- en: Cassandra reads and writes features have consistency level 1 by default.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cassandra的读写特性默认具有一致性级别1。
- en: 'Q.2\. Fill in the blanks:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Q.2\. 填空：
- en: _______________ is used to determine the physical closeness of the nodes.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: _______________ 用于确定节点的物理接近程度。
- en: _______________ is the consistency that provides the highest availability and
    lowest availability.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: _______________ 是提供最高可用性和最低可用性的一致性。
- en: The ___________ is the service that ensures that a node, which has been down
    for a while, is correctly updated with the latest changes.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: _______________ 是确保宕机一段时间的节点正确更新为最新更改的服务。
- en: 'Q.3\. Execute the following use case to see Cassandra high availability and
    replications:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Q.3\. 执行以下用例以查看Cassandra的高可用性和复制：
- en: Create a four-node Cassandra cluster.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个四节点的Cassandra集群。
- en: Create a keyspace with a replication factor of 3.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个副本因子为3的键空间。
- en: Add some data into a column family under this keyspace.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个键空间下的列族中添加一些数据。
- en: Attempt to retrieve data using read consistency with using `ALL` in select query.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用`ALL`在选择查询中使用读一致性来检索数据。
- en: Shut down the Cassandra daemon on one node and repeat step 4 from the other
    three live nodes.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭一个节点上的Cassandra守护程序，并从其他三个活动节点重复第4步。
- en: Shut down the Cassandra daemon on one node and repeat step 4 from the other
    three live nodes using the consistency `ANY`.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭一个节点上的Cassandra守护程序，并使用`ANY`的一致性从其他三个活动节点重复第4步。
- en: Shut down two nodes and update an existing value using a write consistency of
    `ANY`.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭两个节点并使用`ANY`的写一致性更新现有值。
- en: Attempt a read using `ANY`.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用`ANY`进行读取。
- en: Bring back the nodes that are down and execute `read` using the consistency
    `ALL` from all four nodes.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将宕机的节点恢复并从所有四个节点上使用一致性“ALL”执行“read”操作。
- en: Summary
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you have understood the concepts of replication and data partitioning
    in Cassandra. We also understood the replication strategy and the concept of eventual
    consistency. The exercise at the end of the chapter is a good hands-on exercise
    to help you understand the concepts covered in the chapter in a practical way.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经了解了Cassandra中的复制和数据分区的概念。我们还了解了复制策略和最终一致性的概念。本章末尾的练习是一个很好的实践练习，可以帮助您以实际方式理解本章涵盖的概念。
- en: In the next chapter, we will discuss the gossip protocols, Cassandra cluster
    maintenance, and management features.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论八卦协议、Cassandra集群维护和管理特性。
