- en: Chapter 7. RDBMS Cleaning Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：RDBMS清理技术
- en: Home refrigerators all come with shelves and most have one or two drawers for
    vegetables. But if you ever visit a home organization store or talk to a professional
    organizer, you will learn that there are also numerous additional storage options,
    including egg containers, cheese boxes, soda can dispensers, wine bottle holders,
    labeling systems for leftovers, and stackable, color-coded bins in a variety of
    sizes. But do we really need all these extras? To answer this, ask yourself these
    questions, are my frequently used foods easy to find? Are food items taking up
    more space than they should? Are leftovers clearly labeled so I remember what
    they are and when I made them? If our answers are *no*, organization experts say
    that containers and labels can help us optimize storage, reduce waste, and make
    our lives easier.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 家用冰箱通常配备架子，大多数还配有一两个蔬菜抽屉。但是，如果你曾经参观过家居整理商店或与专业的整理师交流过，你会发现还有许多额外的储存选项，包括蛋托、奶酪盒、饮料罐分配器、酒瓶架、剩菜标签系统以及各种尺寸的堆叠式、彩色编码的收纳盒。但我们真的需要这些额外的东西吗？要回答这个问题，你可以问自己以下几个问题：我常用的食物是否容易找到？食物是否占用了不应有的空间？剩菜是否清楚标注了内容和制作时间？如果我们的答案是*否*，整理专家表示，容器和标签可以帮助我们优化存储、减少浪费，并让生活更轻松。
- en: 'The same is true in our **Relational Database Management System** (**RDBMS**).
    As the classic long-term data storage solution, RDBMS is a standard part of the
    modern data science toolkit. But all too often, we are guilty of just depositing
    data in the database, with little thought about the details. In this chapter,
    we will learn how to design an RDBMS that goes beyond *two shelves and a drawer*.
    We will learn a few techniques that will ensure our RDBMS is optimizing storage,
    reducing waste, and making our lives easier. Specifically, we will:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们的**关系型数据库管理系统**（**RDBMS**）是一样的。作为经典的长期数据存储解决方案，RDBMS是现代数据科学工具包的标准部分。然而，我们常常犯的一个错误是，仅仅将数据存入数据库，却很少考虑细节。在本章中，我们将学习如何设计一个超越*两层架子和一个抽屉*的RDBMS。我们将学习一些技术，确保我们的RDBMS能够优化存储、减少浪费，并使我们的生活更轻松。具体来说，我们将：
- en: Learn how to find anomalies in our RDBMS data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何发现我们RDBMS数据中的异常
- en: Learn several strategies to clean different types of problematic data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习几种策略来清理不同类型的问题数据
- en: Learn when and how to create new tables for your cleaned data, including creating
    both child tables and lookup tables
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习何时以及如何为清理过的数据创建新表，包括创建子表和查找表
- en: Learn how to document the rules governing the changes you made
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何记录你所做更改的规则
- en: Getting ready
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To set up the examples in this chapter, we will be working with a popular dataset
    called **Sentiment140**. This dataset was created to help learn about the positive
    and negative sentiments in messages on Twitter. We are not really concerned with
    sentiment analysis in this book, but we are going to use this dataset to practice
    cleaning data after it has already been imported into a relational database.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的示例中，我们将使用一个流行的数据集——**Sentiment140**。该数据集的创建旨在帮助学习Twitter消息中的正面和负面情绪。我们在本书中并不专注于情感分析，但我们将使用这个数据集来练习在数据已导入关系型数据库后进行数据清理。
- en: To get started with the Sentiment140 dataset, you will need a MySQL server set
    up and ready to go, just like in the earlier Enron examples.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用Sentiment140数据集，你需要设置好MySQL服务器，和之前的Enron示例一样。
- en: Step one – download and examine Sentiment140
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步——下载并检查Sentiment140数据集
- en: The version of the Sentiment140 data that we want to use is the original set
    of two files available directly from the Sentiment140 project at [http://help.sentiment140.com/for-students](http://help.sentiment140.com/for-students).
    This ZIP file of tweets and their positive and negative polarity (or sentiment,
    on a scale of 0, 2, or 4) was created by some graduate students at Stanford University.
    Since this file was made publicly available, the original Sentiment140 files have
    been added by other websites and made available as part of many larger collections
    of tweets. For this chapter, we will use the original Sentiment140 text file,
    which is either available as a link from the preceding site or by following the
    precise path to [http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想使用的 Sentiment140 数据版本是来自 Sentiment140 项目的原始文件集，直接可以从[http://help.sentiment140.com/for-students](http://help.sentiment140.com/for-students)获取。这份包含推文及其积极与消极情感（或情绪，评分为
    0、2 或 4）的 ZIP 文件由斯坦福大学的研究生创建。自从这份文件公开发布后，其他网站也将原始的 Sentiment140 文件添加到其平台，并将其作为更大推文集合的一部分公开。对于本章内容，我们将使用原始的
    Sentiment140 文本文件，可以通过前面提到的链接或直接访问[http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip)来获取。
- en: Download the ZIP file, extract it, and take a look at the two CSV files inside
    using your text editor. Right away, you will notice that one file has many more
    lines than the other, but both these files have the same number of columns in
    them. The data is comma-delimited, and each column has been enclosed in double
    quotes. The description of each column can be found on the `for-students` page
    linked in the preceding section.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下载 ZIP 文件，解压缩并使用文本编辑器查看其中的两个 CSV 文件。你会立刻注意到，一个文件比另一个文件的行数多得多，但这两个文件的列数是相同的。数据是逗号分隔的，并且每一列都被双引号括起来。每一列的描述可以在前一部分链接的`for-students`页面中找到。
- en: Step two – clean for database import
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二步 – 清理以便数据库导入
- en: For our purposes—learning how to clean data—it will be sufficient to load the
    smaller of these files into a single MySQL database table. Everything we need
    to do to learn, we can accomplish with the smaller file, the one called `testdata.manual.2009.06.14.csv`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的——学习如何清理数据——将这些文件中较小的一个加载到单个 MySQL 数据库表中就足够了。我们所需要做的所有学习，都可以通过较小的文件来完成，这个文件叫做`testdata.manual.2009.06.14.csv`。
- en: 'As we are looking at the data, we may notice a few areas that will trip us
    up if we try to import this file directly into MySQL. One of the trouble spots
    is located at line 28 in the file:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看数据时，我们可能会注意到一些地方，如果我们直接将此文件导入 MySQL，可能会出现问题。其中一个问题出现在文件的第 28 行：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Do you see the triple quotation marks `"""` right before the `booz` keyword
    and after the word `allen`? The same issue comes up later on line 41 with double
    quotation marks around the song title `P.Y.T`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到在`booz`关键字前和`allen`一词后有三重引号`"""`吗？同样的问题出现在第 41 行，在歌曲标题`P.Y.T`周围有双引号：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The problem with these extra quotation marks is that the MySQL import routine
    will use the quotation marks to delimit the column text. This will produce an
    error, as MySQL will think that the line has more columns than there really are.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些额外的引号问题在于 MySQL 导入程序会使用引号来分隔列文本。这将导致错误，因为 MySQL 会认为这一行的列数比实际的多。
- en: To fix this, in our text editor, we can use **Find and Replace** to replace
    all instances of `"""` with `"` (double quote) and all instances of `""` with
    `'` (single quote).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，在文本编辑器中，我们可以使用**查找和替换**功能，将所有的`"""`替换为`"`（双引号），并将所有的`""`替换为`'`（单引号）。
- en: Tip
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: These `""` could also probably be removed entirely with very little negative
    effect on this cleaning exercise. To do this, we would simply search for `""`
    and replace it with nothing. But if you want to stick close to the original intent
    of the tweet, a single quote (or even an escaped double quote like this `\"`)
    is a safe choice for a replacement character.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些`""`可能也可以完全移除，对这个清理工作几乎没有负面影响。为此，我们只需要搜索`""`并将其替换为空。但如果你希望尽量接近推文的原始意图，使用单引号（甚至像这样转义的双引号`\"`）作为替代字符是一个安全的选择。
- en: Save this cleaned file to a new filename, something like `cleanedTestData.csv`.
    We are now ready to import it into MySQL.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个清理过的文件保存为新文件名，比如`cleanedTestData.csv`。现在我们准备将它导入到 MySQL 中。
- en: Step three – import the data into MySQL in a single table
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三步 – 将数据导入到 MySQL 的单一表中
- en: 'To load our somewhat cleaner data file into MySQL, we will need to revisit
    the CSV-to-SQL techniques from the *Importing spreadsheet data into MySQL* section
    in [Chapter 3](part0024.xhtml#aid-MSDG2 "Chapter 3. Workhorses of Clean Data –
    Spreadsheets and Text Editors"), *Workhorses of Clean Data – Spreadsheets and
    Text Editors*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们稍微清理过的数据文件加载到MySQL中，我们需要回顾一下[第3章](part0024.xhtml#aid-MSDG2 "第3章 工作中的清洁数据——电子表格和文本编辑器")中*导入电子表格数据到MySQL*部分的CSV到SQL技术：
- en: From the command line, navigate to the directory where you have saved the file
    you created in step two. This is the file we are going to import into MySQL.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行，导航到保存你在第二步中创建的文件的目录。这就是我们将要导入到MySQL中的文件。
- en: 'Then, launch your MySQL client, and connect to your database server:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，启动你的MySQL客户端，并连接到你的数据库服务器：
- en: '[PRE2]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Enter your password, and after you are logged in, create a database within
    MySQL to hold the table, as follows:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入你的密码，登录后，在MySQL中创建一个数据库来存储表格，方法如下：
- en: '[PRE3]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we need to create a table to hold the data. The data type and lengths
    for each column should represent our best attempt to match the data we have. Some
    of the columns will be varchars, and each of them will need a length. As we might
    not know what those lengths should be, we can use our cleaning tools to discern
    an appropriate range.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个表格来存储数据。每一列的数据类型和长度应该尽可能匹配我们所拥有的数据。某些列将是varchar类型的，每列都需要指定长度。由于我们可能不知道这些长度应该是多少，我们可以使用清理工具来确定一个合适的范围。
- en: If we open our CSV file in Excel (Google Spreadsheets will work just fine for
    this as well), we can run some simple functions to find the maximum lengths of
    some of our text fields. The `len()` function, for example, gives the length of
    a text string in characters, and the `max()` function can tell us the highest
    number in a range. With our CSV file open, we can apply these functions to see
    how long our varchar columns in MySQL should be.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们在Excel中打开CSV文件（Google电子表格同样可以很好地完成这项工作），我们可以运行一些简单的函数来找到某些文本字段的最大长度。例如，`len()`函数可以给出文本字符串的字符长度，`max()`函数则能告诉我们某个范围中的最大值。打开CSV文件后，我们可以应用这些函数来查看MySQL中varchar列的长度应是多少。
- en: The following screenshot shows a method to use functions to solve this problem.
    It shows the `length()` function applied to column **G**, and the `max()` function
    used in column **H** but applied to column **G**.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下截图展示了一种使用函数来解决这个问题的方法。它展示了`length()`函数应用于列**G**，并且`max()`函数应用于列**H**，但作用于列**G**。
- en: '![Step three – import the data into MySQL in a single table](img/image00294.jpeg)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![步骤三——将数据导入MySQL中的单一表格](img/image00294.jpeg)'
- en: Columns **G** and **H** show how to get the length of a text column in Excel
    and then get the maximum value.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列**G**和**H**展示了如何在Excel中获取文本列的长度，然后获取最大值。
- en: 'To calculate these maximum lengths more quickly, we can also take an Excel
    shortcut. The following array formula can work to quickly combine the maximum
    value with the length of a text column in a single cell—just make sure you press
    *Ctrl* + *Shift* + *Enter* after typing this nested function rather than just
    *Enter*:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更快速地计算这些最大长度，我们还可以使用Excel的快捷方式。以下数组公式可以快速将文本列的最大值和长度合并到一个单元格中——只需确保在输入此嵌套函数后按*Ctrl*
    + *Shift* + *Enter*，而不是仅按*Enter*：
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This nested function can be applied to any text column to get the maximum length
    of the text in that column, and it only uses a single cell to do this without
    requiring any intermediate length calculations.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个嵌套函数可以应用于任何文本列，以获取该列中文本的最大长度，它只使用一个单元格来完成这一操作，而不需要任何中间的长度计算。
- en: After we run these functions, it turns out that the maximum length for any of
    our tweets is 144 characters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行这些函数之后，结果显示我们任何一个推文的最大长度是144个字符。
- en: Detecting and cleaning abnormalities
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测和清理异常
- en: You might be wondering how a tweet in this dataset could possibly be 144 characters
    long as Twitter limits all tweets to a maximum length of 140 characters. It turns
    out that in the sentiment140 dataset, the **&** character was sometimes translated
    to the HTML equivalent code, `&amp`, but not always. Some other HTML code was
    used too, for instance, sometimes, the **<** character became `&lt;` and **>**
    became `&gt;`. So, for a few very long tweets, this addition of just a few more
    characters can easily push this tweet over the length limit of 140\. As we know
    that these HTML-coded characters were not what the original person tweeted, and
    because we see that they happen sometimes but not all the time, we call these
    **data abnormalities**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会好奇，为什么这个数据集中一条推文的长度会是144个字符，而Twitter限制所有推文的最大长度为140个字符。结果发现，在sentiment140数据集中，**&**字符有时被翻译成HTML等效代码`&amp`，但并不是每次都这样。有时也使用了其他HTML代码，例如，**<**字符变成了`&lt;`，**>**变成了`&gt;`。所以，对于一些非常长的推文，增加的几个字符很容易使这条推文超过140个字符的长度限制。我们知道，这些HTML编码的字符并不是原始用户推文的内容，而且我们发现这些情况并不是每次都会发生，因此我们称之为**数据异常**。
- en: 'To clean these, we have two choices. We can either go ahead and import the
    messy data into the database and try to clean it there, or we can attempt to clean
    it first in Excel or a text editor. To show the difference in these two techniques,
    we will do both here. First, we will use find and replace in our spreadsheet or
    text editor to try to convert the characters shown in the following table. We
    can import the CSV file into Excel and see how much cleaning we can do there:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要清理这些数据，我们有两种选择。我们可以选择直接将脏数据导入数据库并尝试在那里清理，或者先在Excel或文本编辑器中清理。为了展示这两种方法的不同，我们在这里会同时演示这两种做法。首先，我们将在电子表格或文本编辑器中使用查找和替换功能，尝试将下表中显示的字符转换。我们可以将CSV文件导入Excel，看看能在Excel中清理多少：
- en: '| HTML code | Replace with | The count of instances | The Excel function used
    to find count |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| HTML代码 | 替换为 | 实例计数 | 用来查找计数的Excel函数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `&lt;` | `<` | 6 | `=COUNTIF(F1:F498,"*&lt*")` |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `&lt;` | `<` | 6 | `=COUNTIF(F1:F498,"*&lt*")` |'
- en: '| `&gt;` | `>` | 5 | `=COUNTIF(F1:F498,"*&gt*")` |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| `&gt;` | `>` | 5 | `=COUNTIF(F1:F498,"*&gt*")` |'
- en: '| `&amp;` | `&` | 24 | `=COUNTIF(F1:F498,"*&amp*")` |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| `&amp;` | `&` | 24 | `=COUNTIF(F1:F498,"*&amp*")` |'
- en: 'The first two character swaps work fine with **Find and Replace** in Excel.
    The `&lt`; and `&gt`; HTML-encoded characters are changed. Take a look at the
    text like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个字符替换在Excel中的**查找和替换**功能中运行正常。`&lt;`和`&gt;`这些HTML编码字符已被替换。看看文本像这样：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding becomes text like this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容会变成如下文本：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'However, when we attempt to use Excel to find `&amp;` and replace it with `&`,
    you may run into an error, as shown:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们尝试在Excel中查找`&amp;`并将其替换为`&`时，可能会遇到一个错误，如下所示：
- en: '![Detecting and cleaning abnormalities](img/image00295.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![检测和清理异常](img/image00295.jpeg)'
- en: 'Some operating systems and versions of Excel have a problem with our selection
    of the **&** character as a replacement. At this point, if we run into this error,
    we could take a few different approaches:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一些操作系统和Excel版本存在我们选择**&**字符作为替代符时的一个问题。如果遇到这个错误，我们可以采取几种不同的方法：
- en: We could use our search engine of choice to attempt to find an Excel solution
    to this error
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用自己喜欢的搜索引擎，尝试找到一个Excel解决方案来修复这个错误。
- en: We could move our CSV text data into a text editor and perform the find and
    replace function in there
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将CSV文本数据移到文本编辑器中，并在那里执行查找和替换功能。
- en: We could forge ahead and throw the data into the database despite it having
    the weird `&amp`; characters in it and then attempt to clean it inside the database
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们也可以继续将数据导入数据库，即使其中包含了奇怪的`&amp;`字符，然后再尝试在数据库中清理这些数据。
- en: Normally, I would be in favor of not moving dirty data into the database if
    it is even remotely possible to clean it outside of the database. However, as
    this is a chapter about cleaning inside a database, let's go ahead and import
    the half-cleaned data into the database, and we will work on cleaning the `&amp`;
    issue once the data is inside the table.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我会倾向于不将脏数据导入数据库，除非可以在数据库外清理干净。然而，既然这章是关于在数据库内清理数据的，那我们就将半清理的数据导入数据库，等数据进入表格后，再清理`&amp;`问题。
- en: Creating our table
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建我们的表格
- en: 'To move our half-cleaned data into the database, we will first write our `CREATE`
    statement and then run it on our MySQL database. The `CREATE` statement is shown
    as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们半清理的数据导入数据库，我们首先需要编写`CREATE`语句，然后在MySQL数据库中运行它。`CREATE`语句如下所示：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'This statement uses the simple and fast MyISAM engine as we do not anticipate
    needing any InnoDB features such as row-level locking or transactions. For more
    on the difference between MyISAM and InnoDB, there is a handy discussion of when
    to use each storage engine located here: [http://stackoverflow.com/questions/20148/myisam-versus-innodb](http://stackoverflow.com/questions/20148/myisam-versus-innodb).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该语句使用简单快速的MyISAM引擎，因为我们预计不会需要任何InnoDB功能，例如行级锁定或事务处理。关于MyISAM与InnoDB的区别，这里有一个关于何时使用每种存储引擎的讨论：[http://stackoverflow.com/questions/20148/myisam-versus-innodb](http://stackoverflow.com/questions/20148/myisam-versus-innodb)。
- en: You might notice that the code still requires 144 for the length of the `tweet_text`
    column. This is because we were unable to clean these columns with the `&amp`;
    code in them. However, this does not bother me too much because I know that varchar
    columns will not use their extra space unless they need it. After all, this is
    why they are called varchar, or variable character, columns. But if this extra
    length really bothers you, you can alter the table later to only have 140 characters
    for that column.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，代码仍然要求`tweet_text`列的长度为144。这是因为我们无法清理包含`&amp;`代码的这些列。然而，这对我影响不大，因为我知道varchar列不会使用额外的空间，除非它们需要。毕竟，这就是它们被称为varchar（可变字符）列的原因。但是，如果这个额外的长度真的让你困扰，你可以稍后修改表格，只保留该列的140个字符。
- en: 'Next, we will use the MySQL command line to run the following import statement
    from the location:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用MySQL命令行从以下位置运行导入语句：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This command loads the data from our cleaned CSV file into the new table we
    created. A success message will look like this, indicating that all 498 rows were
    loaded into the table:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将从我们清理过的CSV文件中加载数据到我们创建的新表中。成功消息将显示如下，表明所有498行数据已经成功加载到表中：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Tip
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you have access to a browser-based interface such as phpMyAdmin (or a desktop
    application such as MySQL Workbench or Toad for MySQL), all of these SQL commands
    can be completed inside these tools very easily and without having to type on
    the command line, for example, in phpMyAdmin, you can use the Import tab and upload
    the CSV file there. Just make sure that the data file is cleaned following the
    procedures in *Step two – clean for database import*, or you may get errors about
    having too many columns in your file. This error is caused by quotation mark problems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你可以访问浏览器界面的工具，如phpMyAdmin（或者桌面应用程序如MySQL Workbench或Toad for MySQL），所有这些SQL命令都可以在这些工具中轻松完成，而无需在命令行中输入。例如，在phpMyAdmin中，你可以使用导入标签并在那里上传CSV文件。只要确保数据文件按照*第二步
    – 为数据库导入清理*中的步骤进行清理，否则你可能会遇到文件中列数过多的错误。这个错误是由于引号问题导致的。
- en: Step four – clean the &amp; character
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四步 – 清理`&amp;`字符
- en: 'In the last step, we decided to postpone cleaning the `&amp`; character because
    Excel was giving a weird error about it. Now that we have finished *Step three
    – import the data into MySQL in a single table* and our data is imported into
    MySQL, we can very easily clean the data using an `UPDATE` statement and the `replace()`string
    function. Here is the SQL query needed to take all instances of `&amp`; and replace
    them with `&`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们决定暂时不清理`&amp;`字符，因为Excel在处理时给出了一个奇怪的错误。现在我们已经完成了*第三步 – 将数据导入到MySQL的单一表格*，并且数据已经导入到MySQL中，我们可以非常轻松地使用`UPDATE`语句和`replace()`字符串函数来清理数据。以下是需要的SQL查询，用来将所有出现的`&amp;`替换为`&`：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `replace()`function works just like find and replace in Excel or in a text
    editor. We can see that tweet ID 594, which used to say `#at&amp;t is complete
    fail`, now reads `#at&t is complete fail`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`replace()`函数的工作方式就像在Excel或文本编辑器中的查找和替换一样。我们可以看到，推文ID 594曾经显示为`#at&amp;t is
    complete fail`，现在变成了`#at&t is complete fail`。'
- en: Step five – clean other mystery characters
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五步 – 清理其他神秘字符
- en: 'As we are perusing the `tweet_text` column, we may have noticed a few odd tweets,
    such as tweet IDs 613 and 2086:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们浏览`tweet_text`列时，可能会注意到一些奇怪的推文，例如推文ID 613和2086：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `?` character is what we should be concerned about. As with the HTML-encoded
    characters we saw earlier, this character issue is also very likely an artifact
    of a prior conversion between character sets. In this case, there was probably
    some kind of high-ASCII or Unicode apostrophe (sometimes called a **smart quote**)
    in the original tweet, but when the data was converted into a lower-order character
    set, such as plain ASCII, that particular flavor of apostrophe was simply changed
    to a `?`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`?`字符是我们应该关注的重点。和我们之前看到的HTML编码字符一样，这个字符问题也很可能是字符集转换中的一个副作用。在这种情况下，原始推文中可能有某种高ASCII或Unicode的撇号（有时称为**智能引号**），但当数据转换为低级字符集，如纯ASCII时，那个特定的撇号就被简单地更改为`?`。'
- en: Depending on what we plan to do with the data, we might not want to leave out
    the `?` character, for example, if we are performing word counting or text mining,
    it may be very important that we convert `I?ll` to `I'll` and `University?s` to
    `University's`. If we decide that this is important, then our job is to detect
    the tweets, where this error happened, and then devise a strategy to convert the
    question mark back to a single quote. The trick, of course, is that we cannot
    just replace every question mark in the `tweet_text` column with a single quote
    character, because some tweets have question marks in them that should be left
    alone.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们打算如何处理这些数据，我们可能不希望省略`?`字符。例如，如果我们进行词频统计或文本挖掘，可能非常重要的是将`I?ll`转换为`I'll`，并将`University?s`转换为`University's`。如果我们决定这很重要，那么我们的任务就是检测到发生错误的推文，然后制定策略将问号转换回单引号。当然，诀窍在于，我们不能仅仅把`tweet_text`列中的每个问号都替换成单引号，因为有些推文中的问号是应该保留的。
- en: 'To locate the problem characters, we can run some SQL queries that attempts
    to locate the problems using a regular expression. We are interested in question
    marks that appear in odd places, such as with an alphabetic character immediately
    following them. Here is an initial pass at a regular SQL expression using the
    MySQL `REGEXP` feature. Running this will give us a rough idea of where the problem
    question marks might reside:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定位问题字符，我们可以运行一些SQL查询，尝试使用正则表达式来查找问题。我们关注的是出现在奇怪位置的问号，例如紧跟其后的是字母字符。以下是使用MySQL
    `REGEXP` 功能的初步正则表达式。运行此查询将大致告诉我们问题问号可能所在的位置：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This SQL regular expression asks for any question mark characters that are
    immediately followed by one or more alphabetic characters. The SQL query yields
    six rows, four of which turn out to have odd question marks and two of which are
    **false positives**. False positives are tweets that matched our pattern but that
    should not actually be changed. The two false positives are tweets with IDs **234**
    and **2204**. These two included question marks as part of a legitimate URL. Tweets
    **139**, **224**, **613**, and **2086** are **true positives**, which means tweets
    that were correctly detected as anomalous and should be changed. All the results
    are shown in the following phpMyAdmin screenshot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个SQL正则表达式查找紧跟一个或多个字母字符的问号字符。SQL查询返回了六行结果，其中四行结果的问号是异常的，另外两行是**假阳性**。假阳性是指匹配了我们模式但实际上不应更改的推文。两个假阳性是推文ID为**234**和**2204**的推文，它们包含的问号是合法的URL的一部分。推文**139**、**224**、**613**和**2086**是**真阳性**，也就是说，这些推文被正确地检测为异常，需要进行修改。所有结果如下图所示，来自phpMyAdmin的截图：
- en: '![Step five – clean other mystery characters](img/image00296.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![步骤五 – 清理其他神秘字符](img/image00296.jpeg)'
- en: Tweet **139** is strange, though. It has a question mark before the word **Obama**,
    as if the name of a news article was being quoted, but there is no matching quote
    (or missing quote) at the end of the string. Was this supposed to be some other
    character? This might actually be a false positive too, or at least not enough
    of a true positive to actually make us fix it. While we are looking at the tweets
    closely, **224** also has an extra strange question mark in a place where it does
    not seem to belong.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，**139**号推文有点奇怪。它在**Obama**这个词前面有一个问号，就像是在引用某篇新闻文章的标题，但在字符串末尾没有匹配的引号（或者是丢失的引号）。这应该是某个其他字符吗？这实际上也可能是一个假阳性，或者至少它的阳性不足以让我们真正去修复它。在仔细检查推文时，**224**号推文也在一个看起来不该出现问号的地方多了一个奇怪的问号。
- en: If we are going to write a `replace()` function to change problematic question
    marks to single quotes, we will somehow need to write a regular expression that
    matches only the true positives and does not match any of the false positives.
    However, as this dataset is small, and there are only four true positives—or three
    if we decide **139** does not need to be cleaned—then we could just clean the
    true positives by hand. This is especially a good idea as we have a few questions
    about other possible issues such as the extra question mark in tweet **224**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要编写一个`replace()`函数，将问题问号替换为单引号，我们将需要编写一个正则表达式，仅匹配真正的问题，并且不匹配任何误报。然而，由于这个数据集很小，且只有四个真正的问题——如果我们认为**139**不需要清理的话，就是三个——那么我们完全可以手动清理这些问题。特别是因为我们对于其他可能存在的问题（例如推文**224**中的额外问号）还有一些疑问。
- en: 'In this case, as we only have three problem rows, it will be faster to simply
    run three small `UPDATE` commands on the data rather than attempting to craft
    the perfect regular expression. Here is the SQL query to take care of tweets **224**
    (first issue only), **613**, and **2086**:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于我们只有三行问题数据，直接对数据运行三个小的`UPDATE`命令会比尝试构建完美的正则表达式更快捷。以下是处理推文**224**（仅第一个问题）、**613**和**2086**的SQL查询：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that we had to escape our single quotes in these update statements. In
    MySQL, the escape character is either the backslash or single quote itself. These
    examples show the single quote as the escape character.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这些更新语句中，我们必须对单引号进行转义。在MySQL中，转义字符可以是反斜杠或单引号本身。这些示例中使用了单引号作为转义字符。
- en: Step six – clean the dates
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第六步——清理日期
- en: 'If we take a look at the `date_of_tweet` column, we see that we created it
    as a simple variable character field, `varchar(30)`. What is so wrong with that?
    Well, suppose we want to put the tweets in order from earliest to latest. Right
    now, we cannot use a simple SQL `ORDER BY` clause and get the proper date order,
    because we will get an alphabetical order instead. All Fridays will come before
    any Mondays, and May will always come after June. We can test this with the following
    SQL query:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`date_of_tweet`列，会发现我们将其创建为一个简单的可变字符字段，`varchar(30)`。那有什么问题呢？好吧，假设我们想按时间顺序排列这些推文。现在，我们不能使用简单的SQL
    `ORDER BY`语句来获取正确的日期顺序，因为我们得到的将是字母顺序。所有星期五都会排在任何星期一之前，五月总是在六月之后。我们可以用以下SQL查询来测试这一点：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The first few rows are in order but down near row 28, we start to see a problem:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的几行是按顺序排列的，但在第28行附近，我们开始看到问题：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`May 11` does not come after `May 15` or `June 8`. To fix this, we will need
    to create a new column that cleans these date strings and turns them into proper
    MySQL datetime data types. We learned in the *Converting between data types* section
    in [Chapter 2](part0020.xhtml#aid-J2B82 "Chapter 2. Fundamentals – Formats, Types,
    and Encodings"), *Fundamentals – Formats, Types, and Encodings*, that MySQL works
    best when dates and time are stored as native **date**, **time**, or **datetime**
    types. The format to insert a datetime type looks like this: `YYYY-MM-DD HH:MM:SS`.
    But this is not what our data looks like in the `date_of_tweet` column.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`5月11日`并不在`5月15日`或`6月8日`之后。为了解决这个问题，我们需要创建一个新列，清理这些日期字符串，并将它们转换为合适的MySQL日期时间数据类型。我们在[第2章](part0020.xhtml#aid-J2B82
    "第2章. 基础知识 – 格式、类型和编码")的*数据类型转换*部分中学到，MySQL在日期和时间作为原生**date**、**time**或**datetime**类型存储时效果最好。插入datetime类型的格式如下：`YYYY-MM-DD
    HH:MM:SS`。但我们在`date_of_tweet`列中的数据并不是这种格式。'
- en: There are numerous built-in MySQL functions that can help us format our messy
    date string into the preferred format. By doing this, we will be able to take
    advantage of MySQL's ability to perform math on the dates and time, for example,
    finding the difference between two dates or times or sorting items properly in
    the order of their dates or times.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL有许多内置函数可以帮助我们将杂乱的日期字符串格式化为首选格式。通过这样做，我们可以利用MySQL在日期和时间上的数学运算能力，例如，找出两个日期或时间之间的差异，或者按日期或时间正确地排序项目。
- en: 'To get our string into a MySQL-friendly datetime type, we will perform the
    following procedure:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的字符串转换为MySQL友好的日期时间类型，我们将执行以下操作：
- en: 'Alter the table to include a new column, the purpose of which is to hold the
    new datetime information. We can call this new column `date_of_tweet_new` or `date_clean`
    or some other name that clearly differentiates it from the original `date_of_tweet`
    column. The SQL query to perform this task is as follows:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改表格，增加一个新列，用于存储新的日期时间信息。我们可以将这个新列命名为`date_of_tweet_new`、`date_clean`，或者其他一个清晰区分于原`date_of_tweet`列的名称。执行此任务的SQL查询如下：
- en: '[PRE16]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Perform an update on each row, during which we format the old date string into
    a properly formatted datetime type instead of a string and add the new value into
    the newly created `date_clean` column. The SQL to perform this task is as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每一行执行更新操作，将旧的日期字符串格式化为正确格式的日期时间类型，而不是字符串，并将新值添加到新创建的`date_clean`列中。执行此任务的SQL语句如下：
- en: '[PRE17]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'At this point, we have a new column that has been populated with the clean
    datetime. Recall that the old `date_of_tweet` column was flawed in that it was
    not sorting dates properly. To test whether the dates are being sorted correctly
    now, we can select our data in the order of the new column:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经有了一个新列，里面填充了清理后的日期时间。回想一下，原来的`date_of_tweet`列有问题，因为它没有正确地对日期进行排序。为了测试日期是否现在已经正确排序，我们可以按新列的顺序选择数据：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We see that the rows are now perfectly sorted, with the May 11 date coming first,
    and no dates are out of order.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到现在的行已经完美排序，5月11日的日期排在最前面，且没有日期错乱。
- en: 'Should we remove the old `date` column? This is up to you. If you are worried
    that you may have made a mistake or that you might need to have the original data
    for some reason, then by all means, keep it. But if you feel like removing it,
    simply drop the column, as shown:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否应该删除旧的`date`列？这由你决定。如果你担心可能犯了错误，或者因为某些原因你可能需要原始数据，那么就保留它。但如果你觉得可以删除，直接删除该列，如下所示：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You could also create a copy of the Sentiment140 table that has the original
    columns in it as a backup.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以创建一个Sentiment140表的副本，里面包含原始列作为备份。
- en: Step seven – separate user mentions, hashtags, and URLs
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七步 – 分离用户提及、话题标签和URL
- en: Another problem with this data right now is that there are lots of interesting
    pieces of information hidden inside the `tweet_text` column, for example, consider
    all the times that a person directs a tweet to the attention of another person
    using the `@` symbol before their username. This is called a **mention** on Twitter.
    It might be interesting to count how many times a particular person is mentioned
    or how many times they are mentioned in conjunction with a particular keyword.
    Another interesting piece of data hidden in some of the tweets is **hashtags**;
    for example, the tweet with ID 2165 discusses the concepts of jobs and babysitting
    using the `#jobs` and `#sittercity` hashtags.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 目前这个数据的另一个问题是，`tweet_text`列中隐藏了很多有趣的信息，例如，考虑一个人使用`@`符号将推文指向另一个人的情况。这叫做Twitter上的**提及**。统计一个人被提及的次数，或者统计他们与特定关键词一起被提及的次数，可能会很有趣。另一个隐藏在部分推文中的有趣数据是**话题标签**；例如，ID为2165的推文使用了`#jobs`和`#sittercity`话题标签讨论工作和保姆的概念。
- en: This same tweet also includes an external, non-Twitter **URL**. We can extract
    each of these mentions, hashtags, and URLs and save them separately in the database.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这条推文还包含了一个外部的非Twitter**URL**。我们可以提取每个提及、话题标签和URL，并将它们单独保存到数据库中。
- en: 'This task will be similar to how we cleaned the dates, but with one important
    difference. In the case of the dates, we only had one possible corrected version
    of the date, so it was sufficient to add a single new column to hold the new,
    cleaned version. With mentions, hashtags, and URLs, however, we may have zero
    or more in a single `tweet_text` value, for example, the tweet we looked at earlier
    (ID 2165) had two hashtags in it, as does this tweet (ID 2223):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务与我们清理日期时的操作类似，但有一个重要的区别。在日期的情况下，我们只有一个可能的修正版本，因此只需添加一个新列来存储清理后的日期版本。然而，对于提及、话题标签和URL，我们在单个`tweet_text`值中可能会有零个或多个，例如我们之前查看的推文（ID
    2165）包含了两个话题标签，这条推文（ID 2223）也是如此：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This tweet has zero mentions, one URL, and two hashtags. Tweet 13078 includes
    three mentions but no hashtags or URLs:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这条推文没有提及、一个URL和两个话题标签。推文ID为13078的推文包含了三个提及，但没有话题标签或URL：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We will need to change our database structure to hold these new pieces of information—hashtags,
    URLs, and mentions—all the while keeping in mind that a given tweet can have a
    lot of these in it.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要更改数据库结构，以便存储这些新的信息——话题标签、URLs 和用户提及——同时要记住，一条推文中可能包含许多这样的内容。
- en: Create some new tables
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一些新表
- en: Following relational database theory, we should avoid creating columns that
    will store multivalue attributes, for example, if a tweet has three hashtags,
    we should not just deposit all three hashtags into a single column. The impact
    of this rule for us is that we cannot just copy the `ALTER` procedure we used
    for the date cleaning problem earlier.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据关系数据库理论，我们应避免创建用于存储多值属性的列。例如，如果一条推文有三个话题标签，我们不应该将这三个话题标签都存入同一列。对我们来说，这条规则意味着我们不能直接复制用于日期清理问题的`ALTER`过程。
- en: 'Instead, we need to create three new tables: `sentiment140_mentions`, `sentiment140_urls`,
    and `sentiment140_hashtags`. The primary key for each new table will be a synthetic
    ID column, and each table will include just two other columns: `tweet_id`, which
    ties this new table back to the original `sentiment140` table, and the actual
    extracted text of the hashtag, mention, or URL. Here are three `CREATE` statements
    to create the tables we need:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们需要创建三个新表：`sentiment140_mentions`、`sentiment140_urls` 和 `sentiment140_hashtags`。每个新表的主键将是一个合成ID列，每个表将包括另外两个列：`tweet_id`，它将该新表与原始`sentiment140`表联系起来，以及实际提取的标签、提及或URL文本。以下是创建这些表的三个`CREATE`语句：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: These tables do not use foreign keys back to the original `sentiment140` tweet
    table. If you would like to add these, that is certainly possible. For the purposes
    of learning how to clean this dataset, however, it is not necessary.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表不使用外键回到原始的`sentiment140`推文表。如果您想添加这些外键，这是完全可能的。但为了学习如何清理这个数据集，我们在此并不需要外键。
- en: Now that our tables are created, it is time to fill them with the data that
    we have carefully extracted from `tweet_text column`. We will work on each extraction
    case separately, starting with user mentions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了表格，是时候将我们从`tweet_text column`中仔细提取的数据填充到这些表格中了。我们将分别处理每个提取的案例，从用户提及开始。
- en: Extract user mentions
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取用户提及
- en: 'To design a procedure that can handle the extraction of the user mentions,
    let''s first review what we know about mentions in tweets:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设计一个能处理用户提及提取的程序，我们首先回顾一下我们已知的推文中关于提及的内容：
- en: The user mention always starts with the **@** sign
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户提及总是以**@**符号开始
- en: The user mention is the word that immediately follows the **@** sign
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户提及是紧跟在**@**符号后的单词
- en: If there is a space after **@**, it is not a user mention
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**@**后面有空格，则不是用户提及
- en: There are no spaces inside the user mention itself
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户提及本身内部没有空格
- en: As e-mail addresses also use **@**, we should be mindful of them
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于电子邮件地址也使用**@**符号，我们应该注意这一点
- en: 'Using these rules, we can construct some valid user mentions:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些规则，我们可以构造一些有效的用户提及：
- en: '@foo'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '@foo'
- en: '@foobar1'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '@foobar1'
- en: '@_1foobar_'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '@_1foobar_'
- en: 'We can construct some examples of invalid user mentions:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构造一些无效的用户提及示例：
- en: '@ foo (the space following the @ invalidates it)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '@ foo（@后面的空格使其无效）'
- en: foo@bar.com (bar.com is not recognized)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: foo@bar.com（bar.com未被识别）
- en: '@foo bar (only @foo will be recognized)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '@foo bar（只会识别@foo）'
- en: '@foo.bar (only @foo will be recognized)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '@foo.bar（只会识别@foo）'
- en: Note
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In this example, we assume that we do not care about the difference between
    a regular `@mention` and.`@mention`, sometimes called a dot-mention. These are
    tweets with a period in front of the `@` sign. They are designed to push a tweet
    to all of the user's followers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们假设我们不关心常规的`@mention`和`.`@mention`（有时称为点提及）之间的区别。这些是推文中在`@`符号前有一个句点的推文，目的是将推文推送到所有用户的粉丝。
- en: 'As this rule set is more complicated than what we can execute efficiently in
    SQL, it is preferable to write a simple little script to clean these tweets using
    some regular expressions. We can write this type of script in any language that
    can connect to our database, such as Python or PHP. As we used PHP to connect
    to the database in [Chapter 2](part0020.xhtml#aid-J2B82 "Chapter 2. Fundamentals
    – Formats, Types, and Encodings"), *Fundamentals – Formats, Types, and Encodings*,
    let''s use a quick PHP script here as well. This script connects to the database,
    searches for user mentions in the `tweet_text` column, and moves any found mentions
    into the new `sentiment140_mentions` table:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个规则集比我们能在 SQL 中高效执行的要复杂，因此更倾向于编写一个简单的小脚本，利用正则表达式来清理这些推文。我们可以用任何能连接到数据库的语言来编写这种类型的脚本，比如
    Python 或 PHP。由于我们在[第二章](part0020.xhtml#aid-J2B82 "第二章：基础知识 - 格式、类型和编码")中使用了 PHP
    连接数据库，*基础知识 - 格式、类型和编码*，我们在这里也使用一个简单的 PHP 脚本。这个脚本连接到数据库，搜索 `tweet_text` 列中的用户提及，并将找到的提及移动到新的
    `sentiment140_mentions` 表中：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After running this little script on the `sentiment140` table, we see that 124
    unique user mentions have been extracted out of the 498 original tweets. A few
    interesting things about this script include that it will handle Unicode characters
    in usernames, even though this dataset does not happen to have any. We can test
    this by quickly inserting a test row at the end of the `sentiment140` table, for
    example:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 `sentiment140` 表运行这个小脚本之后，我们发现从原始的 498 条推文中提取了 124 个独特的用户提及。这个脚本的几个有趣之处包括，它可以处理用户名中的
    Unicode 字符，即使这个数据集中没有这些字符。我们可以通过快速插入一行测试数据到 `sentiment140` 表的末尾来进行测试，例如：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Then, run the script again; you will see that a row has been added to the `sentiment140_mentions`
    table, with the `@тест` Unicode user mention successfully extracted. In the next
    section, we will build a similar script to extract hashtags.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，再次运行脚本；你会看到在 `sentiment140_mentions` 表中添加了一行，并成功提取了 `@тест` 的 Unicode 用户提及。在下一节中，我们将构建一个类似的脚本来提取标签。
- en: Extract hashtags
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取标签
- en: 'Hashtags have their own rules, which are slightly different to user mentions.
    Here is a list of some of the rules we can use to determine whether something
    is a hashtag:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 标签有其自身的规则，这些规则与用户提及略有不同。以下是一些我们可以用来判断是否为标签的规则：
- en: Hashtags start with the `#` sign
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签以 `#` 符号开头
- en: The hashtag is the word that immediately follows the `#` sign
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签是紧跟在 `#` 符号后面的单词
- en: Hashtags can have underscores in them but no spaces and no other punctuation
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签可以包含下划线，但不能有空格和其他标点符号
- en: 'The PHP code to extract hashtags into their own table is mostly identical to
    the user mentions code, with the exception of the regular expression in the middle
    of the code. We can simply change the `$mentions` variable to `$hashtags`, and
    then adjust the regular expression to look like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 用于提取标签的 PHP 代码与用户提及的代码几乎完全相同，唯一不同的是代码中间的正则表达式。我们只需将 `$mentions` 变量改为 `$hashtags`，然后调整正则表达式如下：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This regular expression says that we are interested in matching case-insensitive
    Unicode letter characters. Then, we need to change our `INSERT` line to use the
    correct table and column names like this:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个正则表达式表示我们对匹配大小写不敏感的 Unicode 字母字符感兴趣。然后，我们需要将 `INSERT` 行改为使用正确的表和列名，如下所示：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: When we successfully run this script, we see that 54 hashtags have been added
    to the `sentiment140_hashtags` table. Many more of the tweets have multiple hashtags,
    even more than the tweets that had multiple user mentions, for example, we can
    see right away that tweets 174 and 224 both have several embedded hashtags.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们成功运行这个脚本时，我们看到 54 个标签已被添加到 `sentiment140_hashtags` 表中。更多的推文中包含了多个标签，甚至比包含多个用户提及的推文还多。例如，我们可以立即看到推文
    174 和 224 都包含了多个嵌入的标签。
- en: Next, we will use this same skeleton script and modify it again to extract URLs.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用这个相同的骨架脚本，并再次修改它来提取 URLs。
- en: Extract URLs
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取 URLs
- en: 'Pulling out the URLs from the text can be as simple as looking for any string
    that starts with *http://* or *https://*, or it could get a lot more complex depending
    on what types of URLs the text string includes, for example, some strings might
    include *file://* URLs or torrent links, such as magnet URLs, or other types of
    unusual links. In the case of our Twitter data, we have it somewhat easier, as
    the URLs that were included in our dataset all start with HTTP. So, we could be
    lazy and just design a simple regular expression to extract any string that follows
    http:// or https://. This regular expression would just look like this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中提取 URL 可以像寻找任何以*http://*或*https://*开头的字符串一样简单，或者根据文本中包含的 URL 类型的不同，可能会变得更为复杂。例如，有些字符串可能包括*file://*
    URL 或者磁力链接（如磁力链接），或者其他类型的特殊链接。在我们的 Twitter 数据中，情况相对简单，因为数据集中包含的所有 URL 都以 HTTP
    开头。所以，我们可以偷懒，设计一个简单的正则表达式来提取任何以 http:// 或 https:// 开头的字符串。这个正则表达式看起来就是这样：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: However, if we do a bit of hunting on our favorite search engine, it turns out
    that we can easily find some pretty impressive and useful generic URL matching
    patterns that will handle more sophisticated link patterns. The reason that this
    is useful is that if we write our URL extraction to handle more sophisticated
    cases, then it will still work if our data changes in the future.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们在喜欢的搜索引擎上稍作搜索，实际上我们可以轻松找到一些相当印象深刻且实用的通用 URL 匹配模式，这些模式可以处理更复杂的链接格式。这样做的好处在于，如果我们编写的
    URL 提取程序能够处理这些更复杂的情况，那么即使未来我们的数据发生变化，它依然能够正常工作。
- en: 'A very well-documented URL pattern matching routine is given on the [http://daringfireball.net/2010/07/improved_regex_for_matching_urls](http://daringfireball.net/2010/07/improved_regex_for_matching_urls)
    website. The following code shows how to modify our PHP code to use this pattern
    for URL extraction in the Sentiment140 dataset:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常详细的 URL 匹配模式文档给出了[http://daringfireball.net/2010/07/improved_regex_for_matching_urls](http://daringfireball.net/2010/07/improved_regex_for_matching_urls)网站。以下代码展示了如何修改我们的
    PHP 代码，以便在 Sentiment140 数据集中使用该模式进行 URL 提取：
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This program is nearly identical to the mention extracting program we wrote
    earlier, with two exceptions. First, we stored the regular expression pattern
    in a variable called `$pattern`, as it was long and complicated. Second, we made
    small changes to our database `INSERT` command, just as we did for the hasthtag
    extraction.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序几乎与我们之前编写的提及提取程序相同，只有两个不同点。首先，我们将正则表达式模式存储在一个名为 `$pattern` 的变量中，因为它较长且复杂。其次，我们对数据库的
    `INSERT` 命令做了小的修改，就像我们在话题标签提取时做的那样。
- en: A full line-by-line explanation of the regular expression pattern is available
    on its original website, but the short explanation is that the pattern shown will
    match any URL protocol, such as http:// or file://, and it also attempts to match
    valid domain name patterns as well and directory/file patterns a few levels deep.
    The source website provides its own test dataset too if you want to see the variety
    of patterns that it will match and a few known patterns that will definitely *not*
    match.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式模式的逐行解释可以在原始网站上找到，但简短的解释是，所示的模式将匹配任何 URL 协议，如 http:// 或 file://，它还尝试匹配有效的域名模式以及几级深度的目录/文件模式。如果你想查看它匹配的多种模式以及一些肯定*不会*匹配的已知模式，源网站也提供了自己的测试数据集。
- en: Step eight – cleaning for lookup tables
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第八步——清理查找表
- en: In the *Step seven – Separate user mentions, hashtags, and URLs* section, we
    created new tables to hold the extracted hashtags, user mentions, and URLs, and
    then provided a way to link each row back to the original table via the `id` column.
    We followed the rules of database normalization by creating new tables that represent
    the one-to-many relationship between a tweet and user mentions, between a tweet
    and hashtags, or between a tweet and URLs. In this step, we will continue optimizing
    this table for performance and efficiency.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第七步——分离用户提及、话题标签和 URL*部分，我们创建了新的表格来存储提取的标签、用户提及和 URL，然后提供了一种方法，通过`id`列将每一行与原始表格关联起来。我们按照数据库规范化的规则，通过创建新的表格来表示推文与用户提及、推文与话题标签、推文与
    URL 之间的多对一关系。在这一步中，我们将继续优化此表格的性能和效率。
- en: 'The column we are concerned with now is the `query_phrase` column. Looking
    at the column data, we can see that it contains the same phrases repeated over
    and over. These were apparently the search phrases that were originally used to
    locate and select the tweets that now exist in this dataset. Of the 498 tweets
    in the `sentiment140` table, how many of the query phrases are repeated over and
    over? We can use the following SQL to detect this:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在关心的列是`query_phrase`列。查看该列数据，我们可以看到同样的短语反复出现。这些显然是最初用于定位和选择现在存在于数据集中的推文的搜索短语。在`sentiment140`表中的498条推文中，查询短语有多少次被反复使用？我们可以通过以下SQL来检测这一点：
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The query result shows that there are only 80 distinct query phrases, but these
    are used over and over in the 498 rows.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 查询结果显示，只有80个不同的查询短语，但它们在498行数据中反复出现。
- en: This may not seem like a problem in a table of 498 rows, but if we had an extremely
    large table, such as with hundreds of millions of rows, we should be concerned
    with two things about this column. First, duplicating these strings over and over
    takes up unnecessary space in the database, and second, searching for distinct
    string values is very slow.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这在498行数据的表中可能看起来不算问题，但如果我们有一个非常大的表，比如包含数亿行的表，我们就需要关注这个列的两个问题。首先，重复这些字符串占用了数据库中不必要的空间；其次，查找不同的字符串值会非常慢。
- en: 'To solve this problem, we will create a **lookup table** of query values. Each
    query string will exist only once in this new table, and we will also create an
    ID number for each one. Then, we will change the original table to use these new
    numeric values rather than the string values that it is using now. Our procedure
    to accomplish this is as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将创建一个**查找表**来存储查询值。每个查询字符串只会在这个新表中出现一次，我们还会为每个查询字符串创建一个ID号。接下来，我们将修改原始表，使用这些新的数字值，而不是目前使用的字符串值。我们的操作流程如下：
- en: 'Create a new lookup table:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的查找表：
- en: '[PRE30]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Populate the lookup table with the distinct query phrases and automatically
    give each one a `query_id` number:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用不同的查询短语填充查找表，并自动为每个短语分配一个`query_id`编号：
- en: '[PRE31]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create a new column in the original table to hold the query phrase number:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在原始表中创建一个新列，用于存储查询短语编号：
- en: '[PRE32]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Make a backup of the `sentiment140` table in case the next step goes wrong.
    Any time we perform `UPDATE` on a table, it is a good idea to make a backup. To
    create a copy of the `sentiment140` table, we can use a tool like phpMyAdmin to
    copy the table easily (use the **Operations** tab). Alternately, we can recreate
    a copy of the table and then import into it the rows from the original table,
    as shown in the following SQL:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步操作出错的情况下，备份`sentiment140`表。每次对表执行`UPDATE`操作时，最好先进行备份。我们可以使用像phpMyAdmin这样的工具轻松复制表（使用**操作**标签）。或者，我们可以重新创建一份表的副本，并将原始表中的行导入到副本中，如下所示的SQL：
- en: '[PRE33]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Populate the new column with the correct number. To do this, we join the two
    tables together on their text column, then look up to the correct number value
    from the lookup table, and insert it into the `sentiment140` table. In the following
    query, each table has been given an alias, `s` and `sq`:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用正确的数字填充新列。为此，我们通过它们的文本列将两个表连接起来，然后从查找表中查找正确的数字值，并将其插入到`sentiment140`表中。在以下查询中，每个表都被赋予了别名`s`和`sq`：
- en: '[PRE34]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Remove the old `query_phrase` column in the `sentiment140` table:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除`sentiment140`表中的旧`query_phrase`列：
- en: '[PRE35]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'At this point, we have an effective way to create a list of phrases, as follows.
    These are shown in the alphabetical order:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经找到了一种有效的方法来创建短语列表，具体如下。这些短语按字母顺序排列：
- en: '[PRE36]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can also find out the tweets with a given phrase (`baseball`) by performing
    a join between the two tables:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过对这两个表进行连接，查找包含给定短语（如`baseball`）的推文：
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: At this point, we have a cleaned `sentiment140` table and four new tables to
    hold various extracted and cleaned values, including hashtags, user mentions,
    URLs, and query phrases. Our `tweet_text` and `date_clean` columns are clean,
    and we have a lookup table for query phrases.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经清理了`sentiment140`表，并创建了四个新表来存储各种提取和清理后的值，包括话题标签、用户提及、网址和查询短语。我们的`tweet_text`和`date_clean`列已经清理干净，并且我们已经有了一个查询短语的查找表。
- en: Step nine – document what you did
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第九步 – 记录你所做的工作
- en: With nine steps of cleaning and multiple languages and tools in use, there is
    no doubt there'll be a point at which we will make a mistake and have to repeat
    a step. If we had to describe to someone else what we did, we will almost certainly
    have trouble remembering the exact steps and all the reasons why we did each thing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有九个清理步骤，并且使用了多种语言和工具，毫无疑问我们在某个环节会犯错并需要重复某个步骤。如果我们需要向别人描述我们做了什么，我们几乎肯定会记不清楚确切的步骤以及每个步骤背后的原因。
- en: 'To save ourselves mistakes along the way, it is essential that we keep a log
    of our cleaning steps. At a minimum, the log should contain these in the order
    in which they were performed:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过程中出现错误，我们必须保持清理步骤的日志记录。至少，日志应按照执行顺序包含以下内容：
- en: Every SQL statement
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个SQL语句
- en: Every Excel function or text editor routine, including screenshots if necessary
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个Excel函数或文本编辑器的操作流程，包括必要时的截图
- en: Every script
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个脚本
- en: Notes and comments about why you did each thing
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一步的备注和评论，解释为什么做每一件事
- en: Another excellent idea is to create a backup of the tables at each stage, for
    example, we created a backup just before we performed `UPDATE` on the `sentiment140`
    table, and we discussed performing backups after we created the new `date_clean`
    column. Backups are easy to do and you can always drop the backed-up table later
    if you decide you do not need it.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优秀的建议是，在每个阶段都创建表的备份，例如，我们在对`sentiment140`表执行`UPDATE`操作之前就创建了备份，我们还讨论了在创建新的`date_clean`列之后进行备份。备份操作很简单，如果以后决定不需要备份表，可以随时删除它。
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we used a sample dataset, a collection of tweets called Sentiment140,
    to learn how to clean and manipulate data in a relational database management
    system. We performed a few basic cleaning procedures in Excel, and then we reviewed
    how to get the data out of a CSV file and into the database. At this point, the
    rest of the cleaning procedures were performed inside the RDBMS itself. We learned
    how to manipulate strings into proper dates, and then we worked on extracting
    three kinds of data from within the tweet text, ultimately moving these extracted
    values to new, clean tables. Next, we learned how to create a lookup table of
    values that are currently stored inefficiently, thus allowing us to update the
    original table with efficient, numeric lookup values. Finally, because we performed
    a lot of steps and because there is always the potential for mistakes or miscommunication
    about what we did, we reviewed some strategies to document our cleaning procedures.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们使用了一个示例数据集——一个名为Sentiment140的推文集合，学习如何在关系数据库管理系统中清理和处理数据。我们在Excel中进行了几项基础清理操作，然后我们回顾了如何将数据从CSV文件导入到数据库中。在此之后，剩余的清理操作是在RDBMS内部进行的。我们学习了如何将字符串转换为正确的日期格式，然后我们提取了推文文本中的三种数据，并最终将这些提取的值移至新的干净表格中。接下来，我们学习了如何创建一个查找表，存储当前效率低下的数值，这样我们就能用高效的数字查找值更新原表。最后，由于我们执行了很多步骤，并且总有可能出现错误或关于我们所做操作的沟通不清晰，我们回顾了一些记录清理过程的策略。
- en: In the next chapter, we will switch our perspective away from cleaning what
    has been given to us toward preparing cleaned data for others to use. We will
    learn some best practices to create datasets that require the least amount of
    cleaning by others.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将改变视角，专注于为他人准备已清理的数据，而不是清理我们已经得到的数据。我们将学习一些最佳实践，创建需要他人最少清理的 数据集。
