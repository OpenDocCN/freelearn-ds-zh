- en: Chapter 14. Scalable Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章 可扩展算法
- en: '[PRE0]'
  id: totrans-1
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: L2 cache                                 7 ns
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: L2缓存 7 ns
- en: Main memory                       100 ns
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 主内存 100 ns
- en: Disk (random seek)              2,000,000 ns
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘（随机查找） 2,000,000 ns
- en: Fortunately, Spark provides in-memory processing capabilities, including many
    optimizations that take advantage of the fast caches available (L1/L2/L3 caches).
    Therefore, it can avoid unnecessarily reading from main memory or spilling to
    disk it's important that your analytics take full advantage of these efficiencies. This
    was introduced as part of Project Tungsten, [https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Spark提供了内存处理能力，包括许多利用快速缓存（L1/L2/L3缓存）的优化。因此，它可以避免不必要地从主内存读取或溢出到磁盘，重要的是你的分析要充分利用这些效率。这是作为Tungsten项目的一部分引入的，[https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)。
- en: '**Only optimize after observation**: There''s a famous saying by Donald Knuth,
    the legendary computer scientist and author, that *premature optimization is the
    root of all evil*. While this sounds extreme, what he means is that all performance-related
    tweaks or optimizations should be based on empirical evidence rather than preemptive
    intuition. As such predictions very often fail to correctly identify performance
    problems, and instead give rise to poor design choices that are later regretted.
    But contrary to what you might think, the suggestion here is not that you just
    forget about performance until the end, in fact quite the reverse. In an environment
    where the size of the data and hence the length of time any operation takes dictates
    everything, it''s fundamental to begin optimization early in the analytic design
    process. But isn''t this a contradiction of Knuth''s law? Well, no. In terms of
    performance, simplicity is often the key. The approach should be evidence-based
    so start simple, carefully observe the performance of your analytic at runtime
    (through the use of analytic tuning and code profiling, see the next section),
    perform targeted optimizations that correct the problems identified, and repeat.
    Over-engineering is usually as much to blame in poorly performing analytics as
    choosing slow algorithms, but it can be much harder to fix down the line.*   **Start
    small and scale-up**: Start with small data samples. While an analytic may *eventually*
    be required to run over a petabyte of data, starting with a small dataset is definitely
    advisable. Sometimes only a handful of rows are required to determine whether
    an analytic is working as expected. And more rows can be added to prove out the
    various test and edge cases. It''s more about breadth of coverage here rather
    than volume. The analytic design process is extremely iterative and judicious
    use of data sampling will pay dividends during this phase; while even a small
    dataset will allow you to measure the impact on performance as you incrementally
    increase the size of the data.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**只有在观察后进行优化**：有一句著名的话是由传奇计算机科学家和作家Donald Knuth说的，即*过早的优化是万恶之源*。虽然听起来很极端，但他的意思是所有与性能相关的调整或优化都应该基于经验证据，而不是预先的直觉。因为这样的预测往往无法正确识别性能问题，反而导致后来后悔的糟糕设计选择。但与你可能认为的相反，这里的建议并不是你直到最后才考虑性能，事实上恰恰相反。在数据的大小和因此任何操作所需的时间决定一切的环境中，从分析设计过程的早期开始优化是至关重要的。但这不是Knuth法则的矛盾吗？嗯，不是。在性能方面，简单通常是关键。这种方法应该是基于证据的，所以从简单开始，仔细观察你的分析在运行时的性能（通过分析调整和代码分析，见下一节），进行有针对性的优化来纠正所识别的问题，并重复。过度设计通常与选择缓慢的算法一样常见，但它可能更难以在后期修复。**从小开始，逐步扩大**：从小数据样本开始。虽然分析可能*最终*需要在一百万亿字节的数据上运行，但从一个小数据集开始绝对是明智的。有时只需要少数行就可以确定分析是否按预期工作。并且可以添加更多行来证明各种测试和边缘情况。这里更多的是关于覆盖面而不是数量。分析设计过程是极其迭代的，明智地使用数据抽样将在这个阶段产生回报；即使一个小数据集也能让你在逐渐增加数据大小时测量性能的影响。'
- en: The bottom line is that writing analytics, particularly over data you are unfamiliar
    with, can take time and there are no shortcuts.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 底线是，编写分析，特别是对你不熟悉的数据，可能需要时间，没有捷径。
- en: Now that we have some guidelines, let's focus on how they apply to Spark.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一些指导方针，让我们专注于它们如何适用于Spark。
- en: Spark architecture
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark架构
- en: Apache Spark is designed to simplify the laborious, and sometimes error prone
    task of highly-parallelized, distributed computing. To understand how it does
    this, let's explore its history and identify what Spark brings to the table.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark旨在简化费力且有时容易出错的高度并行分布式计算任务。为了了解它是如何做到这一点的，让我们探索其历史，并确定Spark带来了什么。
- en: History of Spark
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark的历史
- en: 'Apache Spark implements a type of *data parallelism* that seeks to improve
    upon the MapReduce paradigm popularized by Apache Hadoop. It extended MapReduce
    in four key areas:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark实现了一种*数据并行*，旨在改进Apache Hadoop所推广的MapReduce范式。它在四个关键领域扩展了MapReduce：
- en: '**Improved programming model**: Spark provides a higher level of abstraction
    through its APIs than Hadoop; creating a programming model that significantly
    reduces the amount of code that must be written. By introducing a fluent, side-effect-free,
    function-oriented API, Spark makes it possible to reason about an analytic in
    terms of its transformations and actions, rather than just sequences of mappers
    and reducers. This makes it easier to understand and debug.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进的编程模型**：Spark通过其API提供了比Hadoop更高级的抽象层；创建了一个编程模型，大大减少了必须编写的代码量。通过引入一个流畅的、无副作用的、面向函数的API，Spark使得可以根据其转换和操作来推理分析，而不仅仅是映射器和减速器的序列。这使得更容易理解和调试。'
- en: '**Introduces workflow**: Rather than chaining jobs together (by persisting
    results to disk and using a third-party workflow scheduler, as with traditional
    MapReduce), Spark allows analytics to be decomposed into tasks and expressed as
    **Directed Acyclic Graphs** (**DAGs**). This has the immediate effect of removing
    the need to materialize data, but also means it has much more control over how
    analytics are run, including enabling efficiencies such as cost-based query optimization
    (seen in the catalyst query planner).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引入工作流**：与传统的MapReduce通过将结果持久化到磁盘并使用第三方工作流调度程序来链接作业不同，Spark允许将分析分解为任务，并将其表示为**有向无环图**（**DAGs**）。这不仅立即消除了需要实现数据的需求，而且还意味着它对分析的运行方式有更多的控制，包括启用诸如基于成本的查询优化（在催化剂查询规划器中看到）等效率。'
- en: '**Better Memory Utilization**: Spark exploits the memory on each node for in-memory
    caching of datasets. It permits access to caches between operations to improve
    performance over basic MapReduce. This is particularly effective for iterative
    workloads, such as **stochastic gradient descent** (**SGD**), where a significant
    improvement in performance can usually be observed.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好的内存利用**：Spark利用每个节点上的内存来缓存数据集。它允许在操作之间访问缓存，以提高基本MapReduce的性能。这对于迭代工作负载（例如**随机梯度下降**（**SGD**））特别有效，通常可以观察到性能显着提高。'
- en: '**Integrated Approach**: With support for streaming, SQL execution, graph processing,
    machine learning, database integration, and much more, it offers one tool to rule
    them all! Before Spark, specialist tools were needed, for example, Storm, Pig,
    Giraph, Mahout, and so on. Although there are situations where the specialist
    tools can provide better results, Spark''s on-going commitment to integration
    is impressive.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成方法**：支持流处理、SQL执行、图处理、机器学习、数据库集成等，它提供了一个工具来统治它们所有！在Spark之前，需要专门的工具，例如Storm、Pig、Giraph、Mahout等。尽管在某些情况下，专门的工具可能会提供更好的结果，但Spark对集成的持续承诺令人印象深刻。'
- en: In addition to these general improvements, Spark offers many other features.
    Let's take a look inside the box.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些一般改进之外，Spark还提供了许多其他功能。让我们来看看里面的情况。
- en: Moving parts
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移动部件
- en: 'At a conceptual level, there are a number of key components inside Apache Spark,
    many of which you may know already, but let''s review them within the context
    of the scalability principles we''ve outlined:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念层面上，Apache Spark内部有许多关键组件，其中许多您可能已经了解，但让我们在我们已经概述的可伸缩性原则的背景下对它们进行审查：
- en: '![Moving parts](img/image_14_001.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![移动部件](img/image_14_001.jpg)'
- en: Driver
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 驱动程序
- en: The **Driver** is the main entry point for Spark. It's the program that you
    start, it runs in a single JVM, and it initiates and controls all of the operations
    in your job.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**驱动程序**是Spark的主要入口点。它是您启动的程序，它在单个JVM中运行，并启动和控制作业中的所有操作。'
- en: In terms of performance, it's likely that you'll want to avoid bringing large
    datasets back to the driver, as running such operations (such as `rdd.collect`)
    can often cause an `OutOfMemoryError`. This happens when the size of data being
    returned exceeds the JVM heap size of the driver, as specified by `--driver-memory`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，您可能希望避免将大型数据集带回驱动程序，因为运行此类操作（例如`rdd.collect`）通常会导致`OutOfMemoryError`。当返回的数据量超过由`--driver-memory`指定的驱动程序的JVM堆大小时，就会发生这种情况。
- en: SparkSession
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SparkSession
- en: As the driver is starting, the `SparkSession` class is initialized. The `SparkSession` class
    provides access to all of Spark's services, via the relevant context, such as
    `SQLContext`, `SparkContext`, and `StreamingContext` classes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当驱动程序启动时，`SparkSession`类被初始化。`SparkSession`类通过相关上下文（如`SQLContext`、`SparkContext`和`StreamingContext`类）提供对所有Spark服务的访问。
- en: It's also the place to tune Spark's runtime performance-related properties.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是调整Spark运行时与性能相关属性的地方。
- en: Resilient distributed datasets (RDDs)
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 弹性分布式数据集（RDD）
- en: An **Resilient Distributed Dataset** (**RDD**) is the underlying abstraction
    representing a distributed set of homogenous records.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集**（**RDD**）是表示分布式同类记录集的基础抽象。'
- en: 'Although data may be physically stored over many machines in the cluster, analytics
    are intentionally unaware of their actual location: they deal only with RDDs.
    Under the covers, RDDs consist of partitions, or contiguous blocks of data, like
    slices of cake. Each partition has one or more replicas, or copies, and Spark
    is able to determine the physical location of these replicas in order to decide
    where to run transformation tasks to ensure data locality.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据可能在集群中的许多机器上物理存储，但分析故意不知道它们的实际位置：它们只处理RDD。在幕后，RDD由分区或连续的数据块组成，就像蛋糕的切片。每个分区都有一个或多个副本，Spark能够确定这些副本的物理位置，以决定在哪里运行转换任务以确保数据局部性。
- en: Note
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For an example of how the physical location of replicas is determined, see
    `getPreferredLocations` in: [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有关副本的物理位置是如何确定的示例，请参见：[https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala)中的`getPreferredLocations`。
- en: RDDs are also responsible for ensuring that data is cached appropriately from
    the underlying block storage, for example, HDFS.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RDD还负责确保数据从底层块存储（例如HDFS）中适当地缓存。
- en: Executor
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行器
- en: '**Executors** are processes that run on the worker nodes of your cluster. When
    launched, each executor connects back to the driver and waits for instructions
    to run operations over data.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行器**是在集群的工作节点上运行的进程。启动时，每个执行器都会连接到驱动程序并等待运行数据操作的指令。'
- en: You decide on how many executors your analytic needs and this becomes your maximum
    level of parallelism.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您决定您的分析需要多少执行器，这将成为您的最大并行级别。
- en: Note
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Unless using dynamic allocation. In which case, the maximum level of parallelism
    is infinity until configured using `spark.dynamicAllocation.maxExecutors`. See
    Spark configuration for details.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除非使用动态分配。在这种情况下，最大的并行级别是无限的，直到使用`spark.dynamicAllocation.maxExecutors`进行配置。有关详细信息，请参阅Spark配置。
- en: Shuffle operation
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 洗牌操作
- en: The **shuffle** is the name given to the transfer of data between executors
    that occurs as part of an operation whenever data must be physically moved, in
    order to compute a calculation. It typically occurs when data is grouped so that
    all records with the same key are together on a single machine, but it can also
    be used strategically to repartition data for greater levels of parallelism.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**洗牌**是指作为操作的一部分发生的数据在执行器之间的传输。它通常发生在数据分组时，以便具有相同键的所有记录都在单个机器上，但也可以被战略性地用于重新分区数据以获得更高级别的并行性。'
- en: However, as it involves both (i) the movement of data over the network and (ii)
    its persistence to disk, it is generally considered a slow operation. And hence,
    the shuffle is an area of great significance to scalability more on this later.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于它涉及数据在网络上传输和持久性到磁盘，通常被认为是一个缓慢的操作。因此，洗牌对可扩展性非常重要，稍后会详细介绍。
- en: Cluster Manager
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群管理器
- en: The **Cluster Manager** sits outside of Spark, acting as a resource negotiator
    for the cluster. It controls the initial allocation of physical resources, so
    that Spark is able to start its executors on machines with the requisite number
    of cores and memory.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**集群管理器**位于Spark之外，充当集群的资源协商者。它控制物理资源的初始分配，以便Spark能够在具有所需核心数和内存的机器上启动其执行程序。'
- en: Although each cluster manager works in a different way, your choice is unlikely
    to have any measurable impact on algorithmic performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个集群管理器的工作方式都不同，但你的选择不太可能对算法性能产生任何可测量的影响。
- en: Task
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务
- en: A **Task** represents an instruction to run a set of operations over a single
    partition of data. Each task is serialized over to an executor by the driver and,
    is in effect, what is referred to by the expression moving the processing to the
    data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 任务代表对数据的单个分区运行一组操作的指令。每个任务都由驱动程序序列化到执行程序，并且实际上是指通过将处理移动到数据来实现的。
- en: DAG
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DAG
- en: A **DAG** represents the logical execution plan of all transformations involved
    in the execution of an action. Its optimization is fundamental to the performance
    of the analytic. In the case of SparkSQL and Datasets optimization is performed
    on your behalf by the catalyst optimizer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**DAG**代表执行操作所涉及的所有转换的逻辑执行计划。其优化对于分析的性能至关重要。在SparkSQL和数据集的情况下，优化是由催化剂优化器代表你执行的。'
- en: DAG scheduler
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DAG调度程序
- en: The **DAG scheduler** creates a physical plan, by dividing the DAG into stages
    and, for each stage, creating a corresponding set of tasks (one for each partition).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**DAG调度程序**创建一个物理计划，通过将DAG划分为阶段，并为每个阶段创建相应的任务集（每个分区一个任务）。'
- en: '![DAG scheduler](img/image_14_002.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![DAG调度程序](img/image_14_002.jpg)'
- en: Transformations
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换
- en: '**Transformations** are a type of operation. They typically apply a user-defined
    function to each record in an RDD. There are two kinds of transformation, *narrow*
    and *wide*.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换**是一种操作类型。它们通常将用户定义的函数应用于RDD中的每条记录。有两种转换，*窄*和*宽*。'
- en: 'Narrow transformations are operations that are applied locally to partitions
    and as such do not require data to be moved in order to compute correctly. They
    include: `filter`, `map`, `mapValues`, `flatMap`, `flatMapValues`, `glom`, `pipe`,
    `zipWithIndex`, `cartesian`, `union`, `mapPartitionsWithInputSplit`, `mapPartitions`,
    `mapPartitionsWithIndex`, `mapPartitionsWithContext`, `sample`, `randomSplit`.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 窄转换是应用于分区的本地操作，因此不需要移动数据才能正确计算。它们包括：`filter`，`map`，`mapValues`，`flatMap`，`flatMapValues`，`glom`，`pipe`，`zipWithIndex`，`cartesian`，`union`，`mapPartitionsWithInputSplit`，`mapPartitions`，`mapPartitionsWithIndex`，`mapPartitionsWithContext`，`sample`，`randomSplit`。
- en: In contrast, wide transformations are operations that require data to be moved
    in order to compute correctly. In other words, they require a shuffle. They include: `sortByKey`,
    `reduceByKey`, `groupByKey`, `join`, `cartesian`, `combineByKey`, `partitionBy`,
    `repartition`, `repartitionAndSortWithinPartitions`, `coalesce`, `subtractByKey`,
    `cogroup`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，宽转换是需要移动数据才能正确计算的操作。换句话说，它们需要进行洗牌。它们包括：`sortByKey`，`reduceByKey`，`groupByKey`，`join`，`cartesian`，`combineByKey`，`partitionBy`，`repartition`，`repartitionAndSortWithinPartitions`，`coalesce`，`subtractByKey`，`cogroup`。
- en: Note
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `coalesce`, `subtractByKey` and `cogroup` transformations could be narrow
    depending on where data is physically situated.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`coalesce`，`subtractByKey`和`cogroup`转换可能是窄的，具体取决于数据的物理位置。'
- en: In order to write scalable analytics, it's important to be aware of which type
    of transformation you are using.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编写可扩展的分析，重要的是要意识到你正在使用哪种类型的转换。
- en: Stages
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段
- en: 'A **stage** represents a group of operations that can be physically mapped
    to a task (one per partition). There are a couple of things to note about stages:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**阶段**代表可以物理映射到任务（每个分区一个任务）的一组操作。有几点需要注意：'
- en: Any sequence of narrow transformations appearing consecutively in a DAG are
    pipelined together into a single stage. In other words, they execute in order,
    on the same executor and hence against the same partition and do not need a shuffle.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在DAG中连续出现的一系列窄转换会被合并成一个阶段。换句话说，它们按顺序在同一个执行器上执行，因此针对同一个分区，不需要进行洗牌。
- en: Whenever a wide transformation is encountered in a DAG, a stage boundary is
    introduced. Two stages (or more in the case of join, and so on) now exist and
    the second cannot begin until the first has finished (see `ShuffledRDD` class
    for more details).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每当在DAG中遇到宽转换时，就会引入一个阶段边界。现在存在两个阶段（或更多，例如连接等），第二个阶段在第一个完成之前不能开始（有关详细信息，请参阅`ShuffledRDD`类）。
- en: Actions
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 操作
- en: '**Actions** are another type of operation within Spark. They''re typically
    used to perform a parallel write or transfer of data back to the driver. While
    other transformations are lazily evaluated, it is the action that triggers the
    execution of a DAG.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**操作**是Spark中的另一种操作类型。它们通常用于执行并行写入或将数据传输回驱动程序。虽然其他转换是惰性评估的，但是操作会触发DAG的执行。'
- en: Upon invoking an action, its parent RDD gets submitted to the `SparkSession` or `SparkContext` classes
    within the driver and the DAG scheduler generates a DAG for execution.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用操作时，其父RDD被提交给驱动程序中的`SparkSession`或`SparkContext`类，DAG调度程序生成用于执行的DAG。
- en: Task scheduler
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务调度程序
- en: The **task scheduler** receives a set of tasks determined by the DAG scheduler
    (one task per partition) and schedules each to run on an appropriate executor
    in conjunction with data locality.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务调度程序**接收由DAG调度程序确定的一组任务（每个分区一个任务），并安排每个任务在适当的执行程序上与数据局部性一起运行。'
- en: Challenges
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: Now that we have gained an understanding of the Spark architecture, let's prepare
    for writing scalable analytics by introducing some of the challenges, or *gotchas*
    that you might face if you're not careful. Without knowledge of these up-front,
    you could lose time trying to figure them out on your own!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Spark架构，让我们通过介绍一些可能会遇到的挑战或*陷阱*来为编写可扩展的分析做好准备。如果您不小心，没有提前了解这些问题，您可能会花费时间来自己解决这些问题！
- en: Algorithmic complexity
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法复杂度
- en: As well as the obvious effect of the size of your data, the performance of an
    analytic is highly dependent on the nature of the problem you're trying to solve.
    Even some seemingly simple problems, such as a depth first search of a graph,
    do not have well-defined algorithms that perform efficiently in distributed environments.
    This being the case, great care should be taken when designing analytics to ensure
    that they exploit patterns of processing that are readily parallelized. Taking
    the time to understand the nature of your problem in terms of complexity before
    you start, can pay off in the long term. In the next section, we'll show you how
    to do this.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据大小的明显影响外，分析的性能高度依赖于您尝试解决的问题的性质。即使是一些看似简单的问题，如图的深度优先搜索，在分布式环境中也没有效率高的明确定义的算法。在这种情况下，设计分析时应该非常小心，以确保它们利用可以轻松并行化的处理模式。在开始之前花时间了解问题的复杂性的本质，从长远来看会得到回报。在下一节中，我们将向您展示如何做到这一点。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Generally speaking, *NC-complete* problems are parallelizable, whereas P-complete
    problems are not: [https://en.wikipedia.org/wiki/NC_(complexity)](https://en.wikipedia.org/wiki/NC_(complexity)).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，*NC-complete*问题是可以并行化的，而P-complete问题则不行：[https://en.wikipedia.org/wiki/NC_(complexity)](https://en.wikipedia.org/wiki/NC_(complexity))。
- en: 'Another thing to note is that distributed algorithms will often be much slower
    than single-threaded applications when run on small data. It''s worth bearing
    in mind that in the scenarios where all of your data fits onto a single machine,
    the overhead of Spark: spawning processes, transferring data, and the latency
    introduced by interprocess communications, will rarely payoff. Investment in this
    approach only really starts to assist in the case where your datasets are large
    enough that they don''t fit comfortably into memory, then you will notice gains
    in throughput, the amount of data you can process in unit time, as a result of
    using Spark.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意的是，分布式算法在处理小数据时通常比单线程应用程序慢得多。值得注意的是，在所有数据都适合单台机器的情况下，Spark的开销：生成进程、传输数据以及进程间通信引入的延迟，很少会有回报。只有在数据集足够大，无法轻松放入内存时，才会注意到使用Spark会提高吞吐量，即单位时间内可以处理的数据量。
- en: Numerical anomalies
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值异常
- en: When processing large amounts of data, you might notice some strange effects
    with numbers. These oddities relate to the universal number representations of
    modern machines and specifically to the concept of *precision*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大量数据时，您可能会注意到一些数字的奇怪效果。这些奇异性与现代计算机的通用数字表示以及*精度*的概念有关。
- en: 'To demonstrate the effect, consider the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示效果，请考虑以下内容：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice how a positive number is turned into a negative number simply by adding
    one. This phenomenon is known as a **number overflow** and it occurs when a calculation
    results in a number that is too large for its type. In this case, an `Int` has
    a fixed-width of 32-bits, so when we attempt to store a 33-bit number, we get
    an overflow, resulting in a negative. This type of behavior can be demonstrated
    for any numeric type, and as a result of any arithmetic operation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到一个正数通过加一变成了负数。这种现象被称为**数字溢出**，当计算结果产生一个对于其类型来说太大的数字时就会发生。在这种情况下，一个`Int`有32位的固定宽度，所以当我们尝试存储一个33位的数字时，就会发生溢出，导致一个负数。这种行为可以针对任何数字类型进行演示，并且由于任何算术操作的结果而产生。
- en: Note
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This is due to the signed, fixed-width, two's complement number representations
    adopted by most modern processor manufacturers (and hence Java and Scala).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由于大多数现代处理器制造商（因此也包括Java和Scala）采用的有符号、固定宽度、二进制补码数表示。
- en: 'Although overflows occur in the course of normal programming, it''s much more
    apparent when dealing with large datasets. It can occur even when performing relatively
    simple calculations, such as summations or means. Let''s consider the most basic
    example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管溢出在正常编程过程中会发生，但在处理大型数据集时更为明显。即使在执行相对简单的计算时，如求和或平均值，也可能发生溢出。让我们考虑最基本的例子：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Datasets are not immune:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集也不是免疫的：
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Of course, there are strategies for handling this; for example by using alternative
    algorithms, different data types, or changing the unit of measurement. However,
    a plan for tackling these types of issues should always be taken into account
    in your design.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，有处理这个问题的策略；例如使用替代算法、不同的数据类型或更改测量单位。然而，在设计中应始终考虑解决这些问题的计划。
- en: 'Another similar effect is the loss of significance caused by rounding errors
    in calculations limited by their precision. For illustrative purposes, consider
    this really basic (and not very sophisticated!) example:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个类似的效果是由计算中的舍入误差引起的精度限制。为了说明问题，考虑这个非常基本（并不是非常复杂！）的例子：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we were expecting the answer `6.310887552645619145394993304824655E-30`,
    but instead we get zero. This is a clear loss of precision and significance, demonstrating
    another type of behavior that you need to be aware of when designing analytics.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们期望得到答案`6.310887552645619145394993304824655E-30`，但实际上得到了零。这是明显的精度和意义的损失，展示了在设计分析时需要注意的另一种行为。
- en: 'To cope with these issues, Welford and Chan devised an online algorithm for
    calculating the `mean` and `variance`. It seeks to avoid problems with precision.
    Under the covers, Spark implements this algorithm, and an example can be seen
    in the PySpark StatCounter:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些问题，Welford和Chan设计了一个在线算法来计算`mean`和`variance`。它试图避免精度问题。在Spark的内部，实现了这个算法，可以在PySpark
    StatCounter中看到一个例子：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s take a deeper look into how it''s calculating the mean and variance:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解它是如何计算平均值和方差的：
- en: '`delta`: The `delta` is the difference between mu (the current running average)
    and the new value under consideration. It measures the change in value between
    data points and because of this it''s always small. It''s basically a magic number
    that ensures that the calculation never involves summing all the values as this
    would potentially lead to an overflow.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delta`：`delta`是当前运行平均值mu和考虑中的新值之间的差异。它衡量了数据点之间的值的变化，因此始终很小。它基本上是一个魔术数字，确保计算永远不涉及对所有值进行求和，因为这可能导致溢出。'
- en: '`mu`: The mu represents the current running average. At any given time, it''s
    the total of the values seen so far, over the count of those values. The `mu`
    is calculated incrementally by continually applying the delta.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mu`：mu代表当前运行平均值。在任何给定时间，它是迄今为止看到的值的总和，除以这些值的计数。`mu`通过不断应用`delta`来逐渐计算。'
- en: '`m2`: The `m2` is the sum of the mean squared difference. It assists the algorithm
    in avoiding loss of significance by adjusting the precision during the calculation.
    This reduces the amount of information lost through rounding errors.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m2`：`m2`是均方差的总和。它通过在计算过程中调整精度来帮助算法避免精度损失。这减少了通过舍入误差丢失的信息量。'
- en: As it happens, this particular online algorithm is specifically for computing
    statistics, but the online approach may be adopted by the design of any analytic.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 恰好，这个特定的在线算法是专门用于计算统计数据的，但在线方法可能被任何分析的设计所采用。
- en: Shuffle
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 洗牌
- en: 'As we identified earlier in our section on principles, moving data around is
    expensive and this means that one of the main challenges when writing any scalable
    analytic is that of minimizing the transfer of data. The overhead of management
    and handling of data transfer is still, at this moment in time, a very costly
    operation. We''ll discuss more on how to tackle this later in the chapter, but
    for now we''ll build awareness of the challenges around data locality; knowing
    which operations are OK to use and which should be avoided, whilst also understanding
    the alternatives. Some of the key offenders are:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在原则部分中所指出的，数据的移动是昂贵的，这意味着编写任何可扩展分析的主要挑战之一是尽量减少数据传输。管理和处理数据传输的开销在目前仍然是一个非常昂贵的操作。我们将在本章后面讨论如何解决这个问题，但现在我们将意识到数据局部性周围的挑战；知道哪些操作是可以使用的，哪些应该避免，同时也了解替代方案。一些主要的问题是：
- en: '`cartesian()`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`笛卡尔()`'
- en: '`reduce()`'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce()`'
- en: '`PairRDDFunctions.groupByKey()`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PairRDDFunctions.groupByKey()`'
- en: But be aware, with a little forethought, using these can be avoided altogether.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 但要注意，经过一点思考，可以完全避免使用这些。
- en: Data schemes
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据方案
- en: 'Choosing a schema for your data will be critical to your analytic design. Obviously,
    often you have no choice about the format of your data; either a schema will be
    imposed on you or your data may not have a schema. Either way, with techniques
    such as "temporary tables" and schema-on-read (see [Chapter 3](ch03.xhtml "Chapter 3. Input
    Formats and Schema"), *Input Formats and Schema* for details), you still have
    control over how data is presented to your analytic - and you should take advantage
    of this. There are an enormous number of options here and selecting the right
    one is part of the challenge. Let''s discuss some common approaches and start
    with some that are not so good:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为数据选择一个模式对于分析设计至关重要。显然，通常你对数据的格式没有选择权；要么会有一个模式强加给你，要么你的数据可能没有模式。无论哪种情况，通过诸如“临时表”和“读时模式”（详见[第3章](ch03.xhtml
    "第3章。输入格式和模式")，“输入格式和模式”中的详细信息），你仍然可以控制数据如何呈现给你的分析 - 你应该利用这一点。这里有大量的选择，选择合适的选择是挑战的一部分。让我们讨论一些常见的方法，并从一些不太好的方法开始：
- en: '**OOP**: **Object-oriented programming** (**OOP**) is the general concept of
    programming by decomposing problems into classes that model real world concepts.
    Typically, definitions will group both data and behavior, making them a popular
    way to ensure that code is compact and understandable. In the context of Spark,
    however, creating complex object structures, particularly ones that includes rich
    behavior, is unlikely to benefit your analytic in terms of readability or maintenance.
    Instead, it is likely to vastly increase the number of objects requiring garbage
    collection and limit the scope for code reuse. Spark is designed using a *functional
    approach*, and while you should be careful about abandoning objects altogether,
    you should strive to keep them simple and reuse object references where it is
    safe to do so.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OOP**：**面向对象编程**（**OOP**）是将问题分解为模拟现实世界概念的类的一般编程概念。通常，定义将同时组织数据和行为，使其成为确保代码紧凑和可理解的一种流行方式。然而，在Spark的上下文中，创建复杂的对象结构，特别是包含丰富行为的对象结构，不太可能有助于您的分析，以便提高可读性或维护性。相反，这可能会大大增加需要垃圾回收的对象数量，并限制代码重用的范围。Spark是使用*功能方法*设计的，虽然您应该小心放弃对象，但应努力保持它们简单，并在安全的情况下重用对象引用。'
- en: '**3NF**: For decades, databases have been optimized for certain types of schema
    - relational, star, snowflake, and so on. And techniques such as **3rd Normal
    Form** (**3NF**) work well to ensure the correctness of traditional data models.
    However, within the context of Spark, forcing dynamic table joins, or/and joining
    facts with dimensions, results in shuffles, potentially many shuffles, which is
    ultimately bad for performance.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3NF**：几十年来，数据库一直针对某些类型的模式进行优化-关系型，星型，雪花等等。而**第三范式**（**3NF**）等技术可以很好地确保传统数据模型的正确性。然而，在Spark的上下文中，强制动态表连接或/和将事实与维度连接会导致洗牌，可能会有很多次洗牌，这对性能来说是不利的。'
- en: '**Denormalization**: Denormalization is a practical way to ensure that your
    analytic has all the data it needs without having to resort to a shuffle. Data
    can be arranged so that records processed together are also stored together. This
    has the added cost of having to store duplicates of much of the data, but it''s
    often a trade-off that pays off. Particularly as there are techniques and technologies
    that help overcome the cost of duplication, such as columnar-oriented storage,
    column-pruning, and so on. More on this later.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去规范化**：去规范化是确保您的分析具有所需数据而无需进行洗牌的实用方法。数据可以被安排成一起处理的记录也一起存储。这增加了存储大部分数据的重复成本，但通常是一种值得的权衡。特别是因为有技术和技术可以帮助克服重复成本，例如列式存储，列修剪等等。稍后再详细介绍。'
- en: Now that we understand some of the difficulties that you might encounter when
    designing analytics, let's get into the detail of how to apply patterns that address
    these and ensure that your analytics run well.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了在设计分析时可能遇到的一些困难，让我们深入了解如何应用解决这些问题的模式，并确保您的分析运行良好的细节。
- en: Plotting your course
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划你的路线
- en: It's easy to overlook planning and preparation when you're preoccupied with
    experimenting on the latest technologies and data! Nevertheless, the *process*
    of how you write scalable algorithms is just as important as the algorithms themselves.
    Therefore, it's crucial to understand the role of planning in your project and
    to choose an operating framework that allows you to respond to the demands of
    your goals. The first recommendation is to adopt an *agile development methodology*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当你专注于尝试最新的技术和数据时，很容易忽视规划和准备工作！然而，编写可扩展算法的*过程*与算法本身一样重要。因此，了解规划在项目中的作用并选择一个允许你应对目标需求的操作框架至关重要。第一个建议是采用*敏捷开发方法*。
- en: The distinctive ebb and flow of analytic authoring may mean that there is just
    no natural end to the project. By being disciplined and systematic with your approach,
    you can avoid many pitfalls that lead to an under performing project and poorly
    performing code. Conversely, no amount of innovative, open source software or
    copious corpus will rescue a project with no structure.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 分析创作的独特起伏可能意味着项目没有自然的结束。通过纪律和系统化的方法，您可以避免许多导致项目表现不佳和代码性能不佳的陷阱。相反，没有创新的开源软件或大量的语料库也无法拯救一个没有结构的项目。
- en: As every data science project is slightly different, there's no right or wrong
    answers when it comes to overall management. Here we offer a set of guidelines,
    or best practice, based on experience, that should help navigate the data minefield.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个数据科学项目都略有不同，因此在整体管理方面没有对错答案。在这里，我们提供一套基于经验的指导方针或最佳实践，应该有助于应对数据领域的挑战。
- en: When dealing with large quantities of data, even small mistakes in calculations
    may result in many lost hours - waiting for jobs to process without any certainty
    of when, or whether, they will finish. Therefore, generally speaking, one should
    approach analytic authoring with a similar level of rigor as one would the design
    of an experiment. The emphasis here should be on practicality and every care should
    be taken to anticipate the effect of changes on processing time.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大量数据时，即使在计算中出现小错误，也可能导致许多时间的浪费-等待作业处理，而不确定何时或是否会完成。因此，一般来说，应该以与设计实验相似的严谨程度来处理分析创作。这里的重点应该是实用性，并且应该注意预测更改对处理时间的影响。
- en: Here are some tips for staying out of trouble during the development process.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在开发过程中避免麻烦的一些提示。
- en: Be iterative
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代
- en: 'Take an iterative approach to your everyday work and build your analytics incrementally.
    Add functionality as you go, and use unit testing to ensure that you have a solid
    base before adding more features. For each code change you make, consider adopting
    an iterative cycle, such as the one shown in the following diagram:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 采取迭代方法处理日常工作，并逐步构建您的分析。随着工作的进行添加功能，并使用单元测试确保在添加更多功能之前有一个坚实的基础。对于您进行的每个代码更改，考虑采用迭代循环，例如下图所示的循环：
- en: '![Be iterative](img/image_14_003.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![迭代](img/image_14_003.jpg)'
- en: Let's discuss each of these steps in turn.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们依次讨论这些步骤。
- en: Data preparation
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: As always, the first step is to gain an understanding of the data you'll be
    processing. As discussed previously, it's likely that you'll have to attend to
    all the edge cases present in your corpus. You should consider starting with a
    basic data profile in order to understand whether the data meets your expectations,
    in terms of veracity and quality, where the potential risks are and how you might
    segment it into classes so that it can be processed. An approach to this is described
    in detail in [Chapter 4](ch04.xhtml "Chapter 4. Exploratory Data Analysis"), *Explorative
    Data Analysis*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，第一步是了解您将要处理的数据。如前所述，您可能需要处理语料库中存在的所有边缘情况。您应该考虑从基本数据概要开始，以了解数据是否符合您的期望，包括真实性和质量，潜在风险在哪里，以及如何将其分成类别以便进行处理。在[第4章](ch04.xhtml
    "第4章. 探索性数据分析")中详细描述了这种方法，*探索性数据分析*。
- en: In addition to **Exploratory data analysis** (**EDA**), understanding the shape
    of your data will allow you to reason about the design of your analytic and anticipate
    additional demands that you may have to cater for.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除了**探索性数据分析**（**EDA**）外，了解数据的形状将使您能够推断出您的分析设计，并预测您可能需要满足的额外需求。
- en: 'For example, here is a quick data profile to show the completeness of some
    GDELT news article downloads for a given day:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里是一个快速的数据概要，显示了给定日期的一些GDELT新闻文章下载的完整性：
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The results are in the following table:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下表所示：
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For this particular day, you'll see here that in fact the majority of GKG records
    surveyed have no associated news article content. Although this could be for a
    variety of reasons, the point to note is that these missing articles form a new
    class of records that will require different processing. We'll have to write an
    alternate flow for these records, and that flow might have different performance
    characteristics.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一天，您将看到实际上大多数GKG记录没有相关的新闻文章内容。尽管这可能是由于各种原因，但要注意的是这些缺失的文章形成了一类新的记录，需要不同的处理。我们将不得不为这些记录编写一个替代流程，并且该流程可能具有不同的性能特征。
- en: Scale up slowly
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缓慢扩大规模
- en: In terms of data, it is important to *start small and scale up*. Don't be afraid
    to start with a subset of your corpus. Consider choosing a subset identified as
    significant during the data profile stage, or in many cases it's beneficial to
    use a handful of records in each subset. What's important here is that the subset
    you choose is representative enough to prove the particular use case, function
    or feature, yet small enough to allow for *timely iterations*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据方面，*从小规模开始逐步扩大*是很重要的。不要害怕从语料库的子集开始。考虑在数据概要阶段选择一个重要的子集，或者在许多情况下，使用每个子集中的少量记录是有益的。重要的是所选择的子集足够代表特定的用例、功能或特性，同时又足够小以允许*及时迭代*。
- en: In the preceding GDELT example, we could temporarily ignore records with no
    content and deal only with the subset containing news articles. In this way, we'll
    filter out any troublesome cases and handle them in later iterations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的GDELT示例中，我们可以暂时忽略没有内容的记录，只处理包含新闻文章的子集。这样，我们将过滤掉任何麻烦的情况，并在后续迭代中处理它们。
- en: Having said that, eventually you'll definitely want to reintroduce all the subsets
    and edge cases present in your corpus. While it's fine to do this in a piecemeal
    way, by including more important classes first and leaving edge cases until later,
    it is necessary to ultimately understand the behavior of every record in your
    dataset, even outliers, because the chances are they won't be one offs. You will
    also need to understand the effect that any data has on your analytic when it
    is seen in production, regardless of how infrequently, in order to avoid an entire
    run failing due to a single rogue record.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这一点，最终你肯定会想要重新引入语料库中存在的所有子集和边缘情况。虽然可以逐步进行这样做，先包括更重要的类，然后留下边缘情况，但最终需要理解数据集中每条记录的行为，甚至是异常值，因为它们很可能不是一次性事件。您还需要了解任何数据在生产中的影响，无论频率如何，以避免由于单个异常记录而导致整个运行失败。
- en: Estimate performance
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 估算性能
- en: As you write each transformation, be aware of the time-cost in terms of complexity.
    For example, it's good to ask yourself, "how would the running time be affected
    if i doubled the input?". When considering this, it's helpful to think in terms
    of the **Big O Notation**. Big O will not give you an exact performance figure;
    it does not take into account practical factors, such as number of cores, available
    memory, or network speed. However, it can be useful as a guide in order to get
    an indicative measure of the processing complexity.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写每个转换时，要注意复杂性方面的时间成本。例如，问自己，“如果我将输入加倍，运行时间会受到什么影响？”。在考虑这一点时，考虑**大O符号**是有帮助的。大O不会给出确切的性能数字；它不考虑实际因素，如核心数量、可用内存或网络速度。然而，它可以作为指南，以便获得处理复杂性的指示性度量。
- en: 'As a reminder, here are some common notations, in order of time-complexity
    (preferred-first):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，以下是一些常见的符号，按时间复杂度顺序（首选-优先）：
- en: '| **Notation** | **Description** | **Example Operations** |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| **符号** | **描述** | **示例操作** |'
- en: '| O(1) | Constant (Quick)Not dependent on size | `broadcast.value``printSchema`
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| O(1) | 常数（快速）不依赖于大小 | `broadcast.value``printSchema` |'
- en: '| O(log n) | Logarithmic*Grows with the height of a balanced tree of n nodes*
    | `pregel``connectedComponents` |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| O(log n) | 对数*随n个节点的平衡树的高度增长* | `pregel``connectedComponents` |'
- en: '| O(n) | Linear*Grows proportionally with n (rows)* | `map``filter``count``reduceByKey``reduceGroups`
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| O(n) | 线性*与n（行）成比例增长* | `map``filter``count``reduceByKey``reduceGroups` |'
- en: '| O(n + m) | Linear*Grows proportionally with n and m (other dataset)* | `join``joinWith``groupWith``cogroup``fullOuterJoin`
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| O(n + m) | 线性*与n和m（其他数据集）成比例增长* | `join``joinWith``groupWith``cogroup``fullOuterJoin`
    |'
- en: '| O(n²) | Quadratic*Grows as the square of n* | `cartesian` |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| O(n²) | 二次方*随n的平方增长* | `笛卡尔积` |'
- en: '| O(n²c) | Polynomial (Slow)*Grows with n and c (columns)* | `LogisticRegression.fit`
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| O(n²c) | 多项式（慢）*与n和c（列）成比例增长* | `LogisticRegression.fit` |'
- en: Using this kind of notation can assist you when choosing the most efficient
    operation during the design phase of your analytic. For an example of how to replace
    a `cartesian` join [O(n2)] with `connectedComponents` [O(log n)], see [Chapter
    10](ch10.xhtml "Chapter 10. Story De-duplication and Mutation"), *Story De-duplication
    and Mutation*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计分析时，使用这种符号可以帮助你选择最有效的操作。例如，如何用`connectedComponents` [O(log n)] 替换`笛卡尔积` [O(n²)]，请参见[第10章](ch10.xhtml
    "第10章。故事去重和变异")，*故事去重和变异*。
- en: It also allows you to estimate your analytics performance characteristics prior
    to executing your job. You can use this information in conjunction with the parallelism
    and configuration of your cluster to ensure that when it's time to do a full-run
    of your job, maximum resources are employed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 它还让你在执行作业之前估计你的分析性能特征。你可以将这些信息与集群的并行性和配置结合使用，以确保在执行作业的时候，最大限度地利用资源。
- en: Step through carefully
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仔细地一步一步地进行
- en: Spark's fantastic, fluent, function-oriented API is designed to allow the *chaining
    together* of transformations. Indeed, this is one of its main benefits, and as
    we've seen it is especially convenient for building data science pipelines. However,
    it's because of this convenience that it is rather tempting to write a string
    of commands and then execute them all in one run. As you might have already found
    with this approach, if a failure occurs or you're not getting the results you
    expect, all processing up to that point is lost and must be replayed. As the development
    process is characteristically iterative, this results in an overly elongated cycle
    that can too often result in lost time.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的出色、流畅、面向函数的API旨在允许*链接*转换。事实上，这是它的主要优势之一，正如我们所见，它特别方便构建数据科学管道。然而，正是因为这种便利，很容易写一系列命令，然后一次性执行它们。正如你可能已经发现的那样，如果发生故障或者得不到预期的结果，那么到目前为止的所有处理都将丢失，并且必须重新执行。由于开发过程具有迭代特性，这导致了一个过长的周期，往往会导致时间的浪费。
- en: To avoid this problem, it's important to be able to **fail fast** during each
    iteration. Therefore, consider getting into the habit of running one step at a
    time on a small sample of data before proceeding. By issuing an action, say a
    count or small take, after each and every transformation, you can check for correctness
    and ensure that each step is successful before moving onto the next step. By investing
    in a little up-front care and attention, you'll make better use of your time and
    your development cycles will tend to be quicker.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，在每次迭代过程中能够**快速失败**是很重要的。因此，在继续之前，考虑养成在小样本数据上逐步运行一步的习惯。通过在每个转换后发出一个动作，比如计数或小的取样，你可以检查正确性，并确保每一步都成功后再进行下一步。通过在前期投入一点关注和注意，你将更好地利用你的时间，你的开发周期也会更快。
- en: In addition to this, and whenever possible during the development life cycle,
    consider persisting intermediate datasets to disk to avoid having to repeatedly
    recalculate, particularly if they are computationally heavy, or potentially reusable.
    This is a form of on-disk caching and it is a similar approach to *checkpointing*
    (as used in spark streaming when storing state). In fact, it's a common trade-off
    when writing CPU-intensive analytics, and it is especially useful when developing
    analytics that run over large datasets. However, it is a trade-off, so to decide
    whether or not it's worthwhile, evaluate the amount time taken to compute the
    dataset from scratch, versus the time taken to read it from disk.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，在开发生命周期中尽可能地考虑将中间数据集持久化到磁盘，以避免重复计算，特别是在计算量大或可重复使用的情况下。这是一种磁盘缓存的形式，类似于*检查点*（在spark流处理中存储状态时使用）。事实上，这是在编写CPU密集型分析时的常见权衡，特别是在开发运行在大型数据集上的分析时特别有用。然而，这是一个权衡，因此要决定是否值得，需要评估从头计算数据集所需的时间与从磁盘读取数据集所需的时间。
- en: If you decide to persist, be sure to use `ds.write.save` and format as `parquet`
    (default) to avoid a proliferation of bespoke classes and serialization version
    issues. This way you'll preserve the benefits of schema on read.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果决定持久化，确保使用`ds.write.save`并格式化为`parquet`（默认），以避免定制类和序列化版本问题的泛滥。这样你就能保留读取时的模式的好处。
- en: 'Furthermore, as you''re iterating through the analytic development lifecycle,
    writing your own highly-performant functions, it''s a good idea to maintain a
    **regression test pack**. This has a couple of benefits:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在迭代分析开发生命周期时，编写自己的高性能函数时，保持一个**回归测试包**是个好主意。这有几个好处：
- en: It allows you to ensure that as you introduce new classes of data, you haven't
    broken existing functionality.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它让你确保在引入新的数据类时，你没有破坏现有的功能。
- en: It gives you a level of confidence that your code is correct up to the step
    you're working on.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它给了你对代码正确性的一定程度的信心，直到你正在处理的步骤。
- en: 'You can easily create a regression test pack using unit tests. There are many
    unit testing frameworks out there to aid with this. One popular approach is to
    test each function by comparing the actual results with what you expected. In
    this way, you can build up a pack over time, by specifying tests, along with the
    commensurate data for each of your functions. Let''s explain how to do this with
    a simple example. Suppose we have the following model, taken from the GDELT GKG
    dataset:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用单元测试轻松创建回归测试包。有许多单元测试框架可帮助实现这一点。一个流行的方法是通过将实际结果与预期结果进行比较来测试每个函数。通过这种方式，您可以逐渐构建一个测试包，通过为每个函数指定测试和相应的数据来完成。让我们通过一个简单的例子来解释如何做到这一点。假设我们有以下模型，取自GDELT
    GKG数据集：
- en: '[PRE8]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We''d like to test that given a DataFrame of `PersonTone`''s, that the `averageNewsSentiment`
    function correctly computes the average tone for various people taken from all
    articles. For the purposes of writing this unit test, we''re not too interested
    in how the function works, just that it works as *expected*. Therefore, we''ll
    follow these steps:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望测试给定`PersonTone`的DataFrame，`averageNewsSentiment`函数是否正确计算了来自所有文章的各种人的平均语调。为了编写这个单元测试，我们对函数的工作原理不太感兴趣，只关心它是否按照*预期*工作。因此，我们将按照以下步骤进行：
- en: 'Import the required unit test frameworks. In this case, let''s use `ScalaTest`
    and a handy DataFrame-style, parsing framework called `product-collections`:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的单元测试框架。在这种情况下，让我们使用`ScalaTest`和一个方便的DataFrame风格的解析框架，称为`product-collections`：
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We'll also use a custom extension of the `ScalaTest FunSuite`, called `SparkFunSuite`,
    which we introduced in [Chapter 3](ch03.xhtml "Chapter 3. Input Formats and Schema"),
    *Input Formats and Schema*, which you can find in the code repository.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将使用`ScalaTest FunSuite`的自定义扩展，称为`SparkFunSuite`，我们在[第3章](ch03.xhtml "第3章
    输入格式和模式")中介绍了这一扩展，*输入格式和模式*，您可以在代码库中找到。
- en: Next, mock-up some input data and define the *expected* results.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，模拟一些输入数据并定义*预期*结果。
- en: 'Then, run the function on the input data using and collect the *actual* result.
    Note: this runs locally and does not require a cluster.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，对输入数据运行该函数并收集*实际*结果。注意：这在本地运行，不需要集群。
- en: Lastly, *verify* that the actual results match the expected results and if they
    don't, fail the test.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，*验证*实际结果是否与预期结果匹配，如果不匹配，则测试失败。
- en: 'The complete unit test looks like this:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的单元测试如下所示：
- en: '[PRE10]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Tune your analytic
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调优您的分析
- en: The purpose of analytic tuning is to ensure smooth running and maximum efficiency
    of your analytic within the practical limitations of your cluster. Most of the
    time, this means trying to confirm that memory is being used effectively on all
    machines, that your cluster is fully-utilized, and by ensuring that your analytic
    is not unduly IO-bound, CPU-bound, or network-bound. This can be difficult to
    achieve on a cluster due to the distributed nature of the processing and the sheer
    number of machines involved.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 分析调优的目的是确保在集群的实际限制内，您的分析能够平稳运行并实现最大效率。大多数情况下，这意味着尝试确认内存在所有机器上的有效使用，确保集群得到充分利用，并确保您的分析不会受到过多的IO、CPU或网络限制。由于处理的分布性质和涉及的机器数量众多，这在集群上可能很难实现。
- en: Thankfully, the Spark UI is designed to assist you in this task. It centralizes
    and provides a one-stop shop for useful information about the runtime performance
    and state of your analytic. It can help give pointers to resource bottlenecks
    and even tell you where your code is spending most of its time.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 值得庆幸的是，Spark UI 旨在帮助您完成这项任务。它集中并提供了有关运行时性能和分析状态的有用信息的一站式服务。它可以帮助指出资源瓶颈，并告诉您代码大部分时间都在哪里运行。
- en: 'Let''s take a closer look:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看一下：
- en: '**Input Size or Shuffle Read Size/Records**: Used both for narrow and wide
    transformations, in either case this is the total amount of data read by the task,
    regardless of its source (remote or local). If you''re seeing large input sizes
    or numbers of records, consider repartitioning or increasing the number of executors.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入大小或Shuffle Read Size/Records**：用于窄转换和宽转换，无论哪种情况，这都是任务读取的数据总量，无论其来源（远程或本地）。如果您看到大的输入大小或记录数，考虑重新分区或增加执行器的数量。'
- en: '![Tune your analytic](img/image_14_004.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![调优您的分析](img/image_14_004.jpg)'
- en: '**Duration**: The amount of time the task has been running. Although entirely
    dependent on the type of computational task underway, if you''re seeing small
    input sizes and long durations, you may be CPU-bound, consider using thread-dump
    to determine what the time is being spent.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续时间**：任务运行的时间。虽然完全取决于正在进行的计算任务的类型，但如果您看到小的输入大小和长时间运行，可能是CPU限制，考虑使用线程转储来确定时间花在哪里。'
- en: Pay particular attention to any variance in the duration. The Spark UI provides
    figures for the min, 25%, median, 75%, and max displayed on the **Stages** page.
    And from this it is possible to determine the profile of your cluster utilization.
    In other words, whether there is an even distribution of data across your tasks,
    meaning a fair distribution of computing responsibility, or whether you have a
    heavily skewed data distribution, meaning distorted processing with a long tail
    of tasks. If the latter is the case, review the section on handling data distribution.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 特别注意持续时间的任何变化。Spark UI 在**Stages**页面提供了最小值、25%、中位数、75%和最大值的数据。从中可以确定集群利用率的情况。换句话说，您的任务是否有数据的均匀分布，意味着计算责任的公平分配，或者您是否有严重偏斜的数据分布，意味着处理出现了长尾任务。如果是后者，查看处理数据分布的部分。
- en: '**Shuffle Write Size/Records**: The amount of data to be transferred as part
    of the shuffle. It may vary between tasks, but generally you''ll want to ensure
    that the total value is as low as possible.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Shuffle Write Size/Records**：作为洗牌的一部分要传输的数据量。可能会因任务而异，但通常您会希望确保总值尽可能低。'
- en: '**Locality Level**: A measure of data locality appears on the **Stages** page.
    Optimally, this should be PROCESS_LOCAL. However, you will see that it changes
    to any after a shuffle or wide transformation. This usually can''t be helped.
    However, if you''re seeing a lot of `NODE_LOCAL` or `RACK_LOCAL` for narrow transformations:
    consider increasing the number of executors, or in extreme cases confirm your
    storage system block size and replication factor or rebalance your data.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地性级别**：数据本地性的度量出现在**阶段**页面上。在最佳情况下，这应该是PROCESS_LOCAL。然而，您会看到在洗牌或宽转换后它会改变为任何。这通常是无法避免的。然而，如果您看到大量的`NODE_LOCAL`或`RACK_LOCAL`用于窄转换：考虑增加执行器的数量，或在极端情况下确认您的存储系统块大小和复制因子，或重新平衡您的数据。'
- en: '**GC time**: The amount of time each task spends garbage collecting, that is,
    cleaning-up no longer used objects in memory. It should be no more than around
    10% of the overall time (shown by **Duration**). If it''s excessively high, it''s
    probably an indication of an underlying problem. However, it''s worth reviewing
    the other areas of your analytic relating to data distribution (that is, number
    of executors, JVM heap size, number of partitions, parallelism, skew, and so on)
    before attempting to tune the garbage collector.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GC时间**：每个任务花费在垃圾回收上的时间，即清理内存中不再使用的对象。它不应超过总时间的大约10%（由**持续时间**显示）。如果它过高，这可能表明存在潜在问题。然而，在尝试调整垃圾收集器之前，值得审查与数据分发相关的分析的其他领域（即执行器数量、JVM堆大小、分区数量、并行性、偏斜等）。'
- en: '**Thread dump (per executor)**: Shown on the **Executors** page, the thread
    dump option allows you to take a peek at the inner workings of any of your executors,
    at any time. This can be invaluable when trying to gain an understanding of your
    analytic''s behavior. Helpfully, the thread dump is sorted and lists most interesting
    threads at the top of the list look for threads labeled **Executor task launch
    worker** as these are the threads that run your code.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程转储（每个执行器）**：在**执行器**页面上显示，线程转储选项允许您随时窥视任何执行器的内部工作。在尝试了解您的分析行为时，这可能非常有价值。线程转储被排序，并列出了列表顶部最有趣的线程，寻找标记为**Executor
    task launch worker**的线程，因为这些线程运行您的代码。'
- en: By repeatedly refreshing this view, and reviewing the stack trace for a single
    thread, it's possible to get a rough idea of where it's spending time and hence
    identify areas of concern.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反复刷新此视图，并查看单个线程的堆栈跟踪，可以大致了解它花费时间的地方，从而确定关注的领域。
- en: Note
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Alternatively, you can use a flame graph, for details see [https://www.paypal-engineering.com/2016/09/08/spark-in-flames-profiling-spark-applications-using-flame-graphs/](https://www.paypal-engineering.com/2016/09/08/spark-in-flames-profiling-spark-applications-using-flame-graphs/).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用火焰图，详情请参阅[https://www.paypal-engineering.com/2016/09/08/spark-in-flames-profiling-spark-applications-using-flame-graphs/](https://www.paypal-engineering.com/2016/09/08/spark-in-flames-profiling-spark-applications-using-flame-graphs/)。
- en: '![Tune your analytic](img/image_14_005.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![调整您的分析](img/image_14_005.jpg)'
- en: '**Skipped Stages**: The stages that were not required to run. Typically, when
    a stage is shown in this section on the Stages page, it means that a complete
    set of data for this section of the RDD lineage was found in the *cache*, which
    the DAG scheduler did not need to re-compute and instead skipped to the next stage.
    Generally, it is the sign of a good caching strategy.![Tune your analytic](img/image_14_006.jpg)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跳过的阶段**：不需要运行的阶段。通常，当一个阶段在**阶段**页面的此部分中显示时，这意味着在*缓存*中找到了RDD血统的这一部分的完整数据集，DAG调度程序不需要重新计算，而是跳过到下一个阶段。一般来说，这是一个良好缓存策略的标志。![调整您的分析](img/image_14_006.jpg)'
- en: '**Event Timeline**: Again, shown on the **Stages** page the event timeline
    provides a visual representation of your running tasks. It''s useful to see the
    level of parallelism, and how many tasks are executing on each executor at any
    given time.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件时间线**：同样，在**阶段**页面上显示事件时间线提供了您运行任务的可视化表示。它对于查看并行性的水平以及任何给定时间每个执行器上执行的任务数量非常有用。'
- en: 'If after initial investigations, should you need more in-depth information
    than the Spark UI provides, you can use any of the monitoring tools provided by
    your operating system in order to investigate the underlying conditions of your
    infrastructure. The following is a table of a selection of common Linux tools
    for this purpose:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在初步调查后，您需要比Spark UI提供的更深入的信息，您可以使用操作系统提供的任何监控工具来调查基础设施的情况。以下是用于此目的的一些常见Linux工具的表格：
- en: '| **Area Under Consideration** | **Tool** | **Description** | **Example Usage**
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **考虑的领域** | **工具** | **描述** | **示例用法** |'
- en: '| General / CPU | htop | Process activity monitor that refreshes to show near
    real-time CPU, memory and swap (among other things) utilization per process |
    htop -p <pid> |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 一般/CPU | htop | 进程活动监视器，刷新以显示接近实时的CPU、内存和交换（以及其他内容）利用率 | htop -p <pid> |'
- en: '|  | dstat | Highly-configurable reporting on system resource utilization |
    dstat -t -l -c -y -i -p -m -g -d -r -n 3 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | dstat | 高度可配置的系统资源利用率报告 | dstat -t -l -c -y -i -p -m -g -d -r -n 3 |'
- en: '|  | ganglia | Aggregating system resource monitor designed for use on distributed
    systems | Web-based |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | ganglia | 用于分布式系统的聚合系统资源监视器 | 基于Web的 |'
- en: '| Java Virtual Machine | jvmtop | Statistics about a JVM including resource
    utilization and a real-time view of its threads | jvmtop <pid> |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Java虚拟机 | jvmtop | 关于JVM的统计信息，包括资源利用率和实时查看其线程 | jvmtop <pid> |'
- en: '|  | jps | Lists all JVM processes | jps -l |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | jps | 列出所有JVM进程 | jps -l |'
- en: '|  | jmap | JVM internal memory map including breakdown of all objects allocated
    on the heap | jmap -histo <pid> &#124; head -20 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | jmap | 包括堆上分配的所有对象的JVM内部内存映射 | jmap -histo <pid> &#124; head -20 |'
- en: '|  | jstack | JVM Snapshot including full thread dump | jstack <pid> |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | jstack | 包括完整线程转储的JVM快照 | jstack <pid> |'
- en: '| Memory | free | Essential guide to memory utilization | free -m |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 内存 | free | 内存利用的基本指南 | free -m |'
- en: '|  | vmstat | Detailed system resource statistics based on sampling including
    breakdown of memory allocation | vmstat -s |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | vmstat | 基于采样的详细系统资源统计，包括内存分配的细分 | vmstat -s |'
- en: '| Disk I/O | iostat | Provides disk I/O statistics, including I/O wait | iostat
    -x 2 5 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 磁盘I/O | iostat | 提供磁盘I/O统计信息，包括I/O等待 | iostat -x 2 5 |'
- en: '|  | iotop | Disk I/O monitor, in a similar style to top. Show''s I/O at a
    process level | iotop |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | iotop | 磁盘I/O监视器，类似于top。显示进程级别的I/O | iotop |'
- en: '| Network | nettop | Network connection activity monitor including real-time
    I/O | nettop -Pd |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: 网络 | nettop | 包括实时I/O的网络连接活动监视器 | nettop -Pd |
- en: '|  | wireshark | Interactive network traffic analyzer | wireshark -i <iface>
    -ktshark -i <iface> |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | wireshark | 交互式网络流量分析器 | wireshark -i <iface> -ktshark -i <iface> |'
- en: Design patterns and techniques
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式和技术
- en: In this section, we'll outline some design patterns and general techniques for
    use when writing your own analytics. These are a collection of hints and tips
    that represent the accumulation of experiences working with Spark. They are offered
    up as guidelines for effective Spark analytic authoring. They also serve as a
    reference for when you encounter the inevitable scalability problems and don't
    know what to do.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将概述一些设计模式和一般技术，用于编写自己的分析。这些是一些提示和技巧的集合，代表了使用Spark的经验积累。它们被提供作为有效的Spark分析编写指南。当您遇到不可避免的可扩展性问题并不知道该怎么办时，它们也可以作为参考。
- en: Spark APIs
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark API
- en: Problem
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: With so many different sets of API's and functions to choose from, it's difficult
    to know which ones are the most performant.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有许多不同的API和函数可供选择，很难知道哪些是最有效的。
- en: Solution
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: Apache Spark currently has over one thousand contributors, many of whom are
    highly experienced world-class software professionals. It is a mature framework
    having been developed for over six years. Over that time, they have focused on
    refining and optimizing just about every part of the framework from the DataFrame-friendly
    APIs, through the Netty-based shuffle machinery, to the catalyst query plan optimizer.
    The great news is that it all comes for "free" - providing you use the newest
    APIs available in Spark 2.0.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark目前有一千多名贡献者，其中许多是经验丰富的世界级软件专业人士。它是一个成熟的框架，已经开发了六年多。在这段时间里，他们专注于从友好的DataFrame
    API，基于Netty的洗牌机制，到催化剂查询计划优化器的每个部分的完善和优化。令人振奋的消息是，这一切都是“免费”的-只要您使用Spark 2.0中提供的最新API。
- en: Recent optimizations (introduced by *project tungsten*), such as off-heap explicit
    memory management, cache-miss improvements, and dynamic stage generation, are
    only available with the newer `DataFrame` and `Dataset` APIs, and are not currently
    supported by the RDD API. In addition, the newly-introduced Encoders are significantly
    faster and more space-efficient than Kryo serialization or Java serialization.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的优化（由*project tungsten*引入），如离堆显式内存管理，缓存未命中改进和动态阶段生成，仅适用于较新的`DataFrame`和`Dataset`
    API，并且目前不受RDD API支持。此外，新引入的编码器比Kryo序列化或Java序列化快得多，占用的空间也更少。
- en: For the most part, this means that Datasets usually outperform RDDs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，这意味着数据集通常优于RDD。
- en: Example
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 例子
- en: 'Let''s illustrate using an informal example of a basic count of people mentioned
    in articles:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个简单的例子来说明基本的文章中提到的人数的计数：
- en: '[PRE11]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Summary pattern
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要模式
- en: Problem
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: My timeseries analytic must run operationally within strict **service level
    agreements** (**SLAs**) and there is not enough time to compute the required result
    over the entire dataset.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我的时间序列分析必须在严格的**服务级别协议**（**SLA**）内运行，并且没有足够的时间来计算整个数据集所需的结果。
- en: Solution
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: For real-time analytics, or ones with strict SLAs, running lengthy computations
    over large datasets can be impractical. Sometimes it's necessary to design analytics
    using a two-pass algorithm in order to compute results in a timely fashion. To
    do this, we'll need to introduce the concept of the *Summary* pattern.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实时分析或具有严格SLA的分析，运行大型数据集上的漫长计算可能是不切实际的。有时需要使用两遍算法来设计分析，以便及时计算结果。为此，我们需要引入*摘要*模式的概念。
- en: The Summary pattern is a two-pass algorithm where the end result is reconstructed
    from the aggregation of summaries only. Although only using summaries, and having
    never processed the entire dataset directly, the result of the aggregation is
    the same as if it were run over the entire raw dataset.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要模式是一种两遍算法，最终结果是仅从摘要的聚合中重建的。尽管只使用摘要，并且从未直接处理整个数据集，但聚合的结果与在整个原始数据集上运行时的结果相同。
- en: 'The basic steps are:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 基本步骤是：
- en: Calculate a summary over the appropriate interval (per minute, per day, per
    week, and so on).
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在适当的时间间隔（每分钟，每天，每周等）上计算摘要。
- en: Persist the summary data for later use.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将摘要数据持久化以供以后使用。
- en: Calculate an aggregate over the larger interval (per month, per year, and so
    on).
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在较大的时间间隔（每月，每年等）上计算聚合。
- en: This is a particularly useful approach when designing incremental or online
    algorithms for streaming analytics.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种在设计增量或在线算法进行流式分析时特别有用的方法。
- en: Example
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 例子
- en: The GDELT GKG dataset is a great example of a summary dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: GDELT GKG数据集是摘要数据集的一个很好的例子。
- en: Certainly, it would be impractical to perform sentiment analysis or named entity
    recognition over say, a month's worth of global media news articles every 15 minutes.
    Fortunately, GDELT produces those 15 minute summaries that we are able to aggregate
    making this entirely possible.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，每15分钟对全球媒体新闻文章进行情感分析或命名实体识别是不切实际的。幸运的是，GDELT产生了这些15分钟的摘要，我们能够对其进行聚合，使这完全成为可能。
- en: Expand and Conquer Pattern
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展和征服模式
- en: Problem
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: My analytic has a relatively small number of tasks, each with high *Input/Shuffle
    Size (Bytes)*. These tasks take a long time to complete, while sometimes there
    are idle executors.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我的分析有相对较少的任务，每个任务的*输入/洗牌大小（字节）*很大。这些任务需要很长时间才能完成，有时执行者是空闲的。
- en: Solution
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: The *Expand and Conquer* pattern tokenizes records for more efficient parallel
    execution by allowing you to increase parallelism. By decomposing or unpacking
    each record you enable them to be composed in different ways, spread over the
    cluster, and processed by different executors.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*扩展和征服*模式通过允许你增加并行性，将记录标记为更有效的并行执行。通过分解或解压每个记录，你使它们能够以不同的方式组合，分布在集群上，并由不同的执行者处理。'
- en: In this pattern, `flatMap` is used, usually in conjunction with shuffle or `repartition`,
    to *increase* the number of tasks and decrease the amount of data being processed
    by each task. This gives rise to an optimal situation whereby enough tasks are
    queued so that no executors are ever left idle. It can also help in the scenario
    where you're struggling to process large amounts of data in the memory of one
    machine, and hence receiving *out of memory errors*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式中，通常与洗牌或`repartition`一起使用`flatMap`，以*增加*任务数量并减少每个任务处理的数据量。这产生了一个最佳情况，足够多的任务排队，以便没有执行者会空闲。它还可以帮助处理在一个机器的内存中处理大量数据并因此收到*内存不足错误*的情况。
- en: This useful and versatile technique comes in handy in almost every situation
    where you have large datasets. It promotes the use of simple data structures and
    allows you to take full advantage of the distributed nature of Spark.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这种有用且多才多艺的技术几乎在每个需要处理大型数据集的情况下都派上用场。它促进了简单数据结构的使用，并允许你充分利用Spark的分布式特性。
- en: A word of caution, however, as `flatMap` can also cause performance problems
    because it has the potential to *increase* the time complexity of your analytic.
    By using `flatMap`, you are generating many records for each and every row, hence
    potentially adding another dimension of data that requires processing. Therefore,
    you should always consider the impact of this pattern on algorithmic complexity,
    using the Big O Notation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，`flatMap`也可能导致性能问题，因为它有可能*增加*你的分析的时间复杂度。通过使用`flatMap`，你为每一行生成了许多记录，因此可能添加了需要处理的数据的另一个维度。因此，你应该始终考虑这种模式对算法复杂度的影响，使用大O符号表示法。
- en: Lightweight Shuffle
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 轻量级洗牌
- en: Problem
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: The *Shuffle Read Blocked Time* of my analytic is a significant proportion of
    the overall processing time (>5%). What can I do to avoid having to wait for the
    shuffle to finish?
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我的分析的*洗牌读取阻塞时间*占整体处理时间的很大比例（>5%）。我该怎么做才能避免等待洗牌完成？
- en: Solution
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Although Spark''s shuffle is carefully engineered to minimize both network
    and disk I/O by using techniques such as data compression and merge file consolidation,
    it has the following two fundamental problems that mean it will often become a
    performance bottleneck:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Spark的洗牌经过精心设计，通过使用数据压缩和合并文件整合等技术来最小化网络和磁盘I/O，但它有以下两个根本问题，这意味着它经常会成为性能瓶颈：
- en: '**It''s I/O Intensive**: The shuffle relies on (i) moving data over a network
    and (ii) writing that data to disk on the target machine. Therefore, it''s much
    slower than local transformations. To illustrate how much slower, here are the
    relative timings for reading 1 MB sequentially from various devices: It''s I/O
    intensive: The shuffle relies on (i) moving data over a network and (ii) writing
    that data to disk on the target machine. Therefore, it''s much slower than local
    transformations. To illustrate how much slower, here are the relative timings
    for reading 1 MB sequentially from various devices:'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它的I/O密集**：洗牌依赖于（i）在网络上传输数据和（ii）将数据写入目标机器的磁盘。因此，它比本地转换要慢得多。为了说明慢了多少，这里是从各种设备顺序读取1MB数据的相对时间：它的I/O密集：洗牌依赖于（i）在网络上传输数据和（ii）将数据写入目标机器的磁盘。因此，它比本地转换要慢得多。为了说明慢了多少，这里是从各种设备顺序读取1MB数据的相对时间：'
- en: '*Memory                0.25ms*'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*内存                     0.25毫秒*'
- en: '*10 GbE              10ms*'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*10 GbE              10毫秒*'
- en: '*Disk                     20ms        *'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '*磁盘                     20毫秒        *'
- en: In this example, as a shuffle operation uses both network and disk, it would
    be around 120 times slower than one performed on a cached, local partition. Obviously,
    timings will vary depending on physical types and speeds of the devices used,
    figures here are provided as relative guidelines.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，由于洗牌操作同时使用了网络和磁盘，所以它的速度大约会比在缓存的本地分区上执行的速度慢120倍。显然，计时会根据使用的设备的物理类型和速度而有所不同，这里提供的数字只是相对指导。
- en: '**It''s a synchronization point for concurrency**: Every task in a stage must
    complete before the next stage can begin. Given that stage boundaries involve
    a shuffle (see `ShuffleMapStage`), it marks a point in the execution where tasks
    that otherwise would be ready to start, must wait until all tasks in that stage
    have finished. This gives rise to a synchronization barrier that can have a significant
    impact on performance.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它是并发的同步点**：一个阶段中的每个任务必须在下一个阶段开始之前完成。鉴于阶段边界涉及洗牌（参见`ShuffleMapStage`），它标志着执行中的一个点，任务必须等到该阶段的所有任务都完成才能开始。这产生了一个同步屏障，对性能有重大影响。'
- en: For these reasons, try to avoid the shuffle where possible, or at least minimize
    its impact.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，尽量避免洗牌是可能的，或者至少最小化它的影响。
- en: Sometimes it's possible to avoid a shuffle altogether, in fact there are patterns,
    such as *Broadcast Variables* or *Wide Table patterns*, that offer suggestions
    on how to do this, but often it's inevitable and all that can be done is to lessen
    the amount of data that is transferred, and hence the impact of the shuffle.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 有时可以完全避免洗牌，事实上有一些模式，比如*广播变量*或*宽表模式*，提供了如何做到这一点的建议，但通常是不可避免的，所有可以做的就是减少传输的数据量，从而减少洗牌的影响。
- en: In this case, try to construct a *Lightweight Shuffle* specifically minimizing
    data transfer - only necessary bytes should be transferred.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，尝试构建一个*轻量级洗牌*，专门最小化数据传输-只传输必要的字节。
- en: 'Once again, if you use the `Dataset` and `DataFrame` API, when the catalyst
    generates a logical query plan it will perform over 50 optimizations, including
    *pruning* any unused columns or partitions automatically (see [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala)).
    But if you''re using RDDs, you''ll have to do this yourself. There''s following few
    techniques that you can try:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，如果您使用`Dataset`和`DataFrame`API，当catalyst生成逻辑查询计划时，它将执行50多个优化，包括自动*修剪*任何未使用的列或分区（请参阅[https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala)）。但如果您使用RDD，您将不得不自己执行这些操作。以下是您可以尝试的一些技术：
- en: '**Use map to reduce data**: Call `map` on your data immediately prior to a
    shuffle in order to get rid of any data that is not used in follow on processing.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用map来减少数据**：在洗牌之前立即在数据上调用`map`，以便摆脱任何在后续处理中没有使用的数据。'
- en: '**Use keys only**: When you have key-value pairs, consider using `rdd.keys`
    instead of `rdd`. For operations such as counts or membership tests, this should
    be sufficient. Similarly, consider using `values` whenever appropriate.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅使用键**：当您有键值对时，考虑使用`rdd.keys`而不是`rdd`。对于计数或成员资格测试等操作，这应该足够了。同样，考虑在适当的时候使用`values`。'
- en: '**Adjust order of stages**: Should you join and then `groupBy` or `groupBy`
    and then join? In Spark, this is mainly about the size of the datasets. It should
    be fairly trivial to do cost-based assessments using the number of records before
    and after each transformation. Experiment to find which one is more efficient
    for your datasets.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整阶段顺序**：您应该先加入然后`groupBy`还是先`groupBy`然后加入？在Spark中，这主要取决于数据集的大小。使用每个转换前后的记录数量进行基于成本的评估应该是相当简单的。尝试找出哪种对您的数据集更有效。'
- en: '**Filter first**: Generally speaking, filtering rows prior to a shuffle is
    an advantage as it reduces the number of rows transferred. Consider filtering
    as early as possible, provided your revised analytic is functionally equivalent.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**首先过滤**：一般来说，在洗牌之前过滤行是有利的，因为它减少了传输的行数。尽可能早地进行过滤，前提是您修改后的分析功能上是等效的。'
- en: 'In some situations, you can also filter out entire partitions, like so:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您还可以过滤掉整个分区，就像这样：
- en: '[PRE13]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Use CoGroup**: If you have two or more RDDs all grouped by the same key,
    then `CoGroup` might be able to join them without instigating a shuffle. This
    ingenious little trick works because any `RDD[(K,V)]` using the same type `K`
    as a key, and grouped using a `HashPartitioner`, will always settle on the same
    node. Therefore, when joining by key `K`, no data needs to be moved.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用CoGroup**：如果您有两个或更多个RDD都按相同的键分组，那么`CoGroup`可能能够在不引发洗牌的情况下将它们连接起来。这个巧妙的小技巧之所以有效，是因为任何使用相同类型`K`作为键的`RDD[(K,V)]`，并使用`HashPartitioner`进行分组的数据，将始终落在同一个节点上。因此，当按键`K`进行连接时，不需要移动任何数据。'
- en: '**Try a different codec**: Another tip for decreasing the amount of bytes transferred
    is to change the compression algorithm.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尝试不同的编解码器**：减少传输字节数的另一个提示是更改压缩算法。'
- en: 'Spark provides three options: `lz4`, `lzf`, and `snappy`. Consider reviewing
    each one to determine which works best for your particular type of data:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了三个选项：`lz4`、`lzf`和`snappy`。考虑审查每个选项，以确定哪个对您特定类型的数据最有效：
- en: '[PRE14]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Wide Table pattern
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 宽表模式
- en: Problem
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: The one-to-many or many-to-many relationships in my datasets are producing many
    shuffles that ruin all my analytics' performance.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我的数据集中的一对多或多对多关系产生了许多破坏所有分析性能的洗牌。
- en: Solution
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: In order to optimize your data structures, we advocate denormalizing your data
    into a form that's useful for your particular type of processing. The approach,
    described here as the *Wide Table pattern*, involves combining data structures
    that are frequently used together, so that they are composed into a single record.
    This preserves data locality and removes the need to perform expensive joins.
    The more often a relationship is used, the more you benefit from this data locality.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化您的数据结构，我们建议将数据去规范化为对您特定类型的处理有用的形式。这种方法，这里描述为*宽表模式*，涉及将经常一起使用的数据结构组合在一起，以便它们组成单个记录。这样可以保留数据局部性并消除执行昂贵连接的需要。关系使用得越频繁，您就越能从这种数据局部性中受益。
- en: The process involves constructing a data representation, view, or table that
    contains everything you need to do for follow-on processing. You may construct
    this programmatically, or by standard *joins* SparkSQL statements. It is then
    materialized ahead of time and used directly inside your analytics whenever required.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程涉及构建一个包含您需要进行后续处理的所有内容的数据表示、视图或表。您可以通过编程方式构建这个，或者通过标准的*连接*SparkSQL语句。然后，它会提前实现，并在需要时直接在您的分析中使用。
- en: Where necessary, data is duplicated across each row to ensure self-sufficiency.
    You should resist the urge to factor out additional tables, like those found in
    third-normal form or in snowflake designs, and instead rely on columnar data formats,
    such as Parquet and ORC, to provide efficient storage mechanisms without sacrificing
    fast sequential access. They can do this by arranging data by column and compressing
    data within each column, which helps alleviate concerns when duplicating data.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在必要时，数据会在每一行之间复制，以确保自给自足。您应该抵制将额外的表因素出来的冲动，比如第三范式或雪花设计中的表，并依靠列式数据格式，如Parquet和ORC，提供高效的存储机制，而不会牺牲快速的顺序访问。它们可以通过按列排列数据并在每列内压缩数据来实现这一点，这有助于在复制数据时减轻担忧。
- en: Similarly, nested types, classes, or arrays can often be used to good effect
    inside a record to represent children or composite data classes. Again, avoid
    necessary dynamic joins at analytic runtime.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，嵌套类型、类或数组通常可以在记录内部有效地表示子类或复合数据类。同样，避免在分析运行时进行必要的动态连接。
- en: Example
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: For an example of how to use denormalized data structures, including nested
    types, see *[Chapter 3](ch03.xhtml "Chapter 3. Input Formats and Schema"), Input
    Formats and Schema*.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何使用非规范化数据结构（包括嵌套类型）的示例，请参见*[第3章](ch03.xhtml "第3章。输入格式和模式")，输入格式和模式*。
- en: Broadcast variables pattern
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广播变量模式
- en: Problem
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: My analytic requires many compact reference datasets and dimension tables that,
    despite their smaller size, cause costly shuffles of all data.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我的分析需要许多紧凑的参考数据集和维度表，尽管它们的大小较小，但会导致所有数据的昂贵洗牌。
- en: Solution
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: While some datasets - such as transaction logs or tweets - are theoretically
    infinitely large, others have natural limits and will never grow beyond a certain
    size. These are known as *bounded datasets*. Although they may change occasionally
    over time, they are reasonably stable and can be said to be held within a finite
    space. For example, the list of all the postcodes in the UK could be considered
    a bounded dataset.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然某些数据集-例如交易日志或推文-在理论上是无限大的，但其他数据集具有自然限制，永远不会超出一定大小。这些被称为*有界数据集*。尽管它们可能会随时间偶尔发生变化，但它们是相当稳定的，并且可以说被保存在有限的空间内。例如，英国所有邮政编码的列表可以被视为有界数据集。
- en: 'When joining to a bounded dataset or any small collection, there is an opportunity
    to take advantage of an efficiency pattern that Spark provides. Rather than using
    join as you would normally, which would instigate a shuffle that could potentially
    transfer all data, consider using a broadcast variable instead. Once assigned,
    the broadcast variable will be distributed and made available locally to all the
    executors in your cluster. You can use a broadcast variable like so:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当加入到有界数据集或任何小集合时，有机会利用Spark提供的效率模式。与通常使用连接不同，连接通常会引发可能传输所有数据的洗牌，考虑改用广播变量。一旦分配，广播变量将分发并在集群中的所有执行程序中提供本地可用。您可以这样使用广播变量：
- en: Creating a broadcast variable
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建广播变量
- en: '[PRE15]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure you collect any data to be broadcast.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 确保收集要广播的任何数据。
- en: Accessing a broadcast variable
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 访问广播变量
- en: '[PRE16]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Removing a broadcast variable
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 删除广播变量
- en: '[PRE17]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Broadcast variables can be used by the `RDD` API or the `Dataset` API. Also,
    you can still exploit broadcast variables in SparkSQL - it will handle it automatically.
    Just ensure that the threshold is set above the size of the table to join, like
    so:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量可以由`RDD` API或`Dataset` API使用。此外，您仍然可以在SparkSQL中利用广播变量-它将自动处理。只需确保阈值设置在要加入的表的大小以上，如下所示：
- en: '[PRE18]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Example
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 例子
- en: For examples of how to use broadcast variables to implement efficient joins
    and filters, see [Chapter 9](ch09.xhtml "Chapter 9.  News Dictionary and Real-Time
    Tagging System") *, News dictionary and Real-time Tagging System*.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何使用广播变量实现高效的连接和过滤的示例，请参见[第9章](ch09.xhtml "第9章。新闻词典和实时标记系统")*，新闻词典和实时标记系统*。
- en: Combiner pattern
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组合器模式
- en: Problem
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: My analytic is performing an aggregation based on a set of keys, and hence,
    is having to shuffle *all data for all keys*. Consequently, it's very slow.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我的分析正在根据一组键执行聚合，因此必须对*所有键的所有数据*进行洗牌。因此，速度非常慢。
- en: Solution
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'At the core of Apache Spark''s shuffling abilities is a powerful and flexible
    pattern, referred to here as the *Combiner* pattern, which offers a mechanism
    for greatly reducing the amount of data in the shuffle. The Combiner pattern is
    so important that examples of it can be found in multiple locations in the Spark
    code - to see it in action here are some of those examples:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark的洗牌能力的核心是一种强大而灵活的模式，这里称为*组合器*模式，它提供了一种大大减少洗牌数据量的机制。组合器模式非常重要，以至于可以在Spark代码的多个位置找到它的示例-要在此处看到它的实际效果，以下是其中一些示例：
- en: '`ExternalAppendOnlyMap`'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ExternalAppendOnlyMap`'
- en: '`CoGroupedRDD`'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CoGroupedRDD`'
- en: '`DeclarativeAggregate`'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`声明式聚合`'
- en: '`ReduceAggregator`'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReduceAggregator`'
- en: In fact, all high-level API's that use the shuffle operation, such as `groupBy`,
    `reduceByKey`, `combineByKey`, and so on, use this pattern as the core of their
    processing. However, there's some variation in the implementations mentioned previously,
    although the fundamental concept is the same. Let's take a closer look.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，所有使用洗牌操作的高级API，例如`groupBy`，`reduceByKey`，`combineByKey`等，都使用此模式作为其处理的核心。但是，先前提到的实现中存在一些变化，尽管基本概念是相同的。让我们仔细看看。
- en: The Combiner pattern provides an efficient approach to compute a function across
    sets of records in parallel and then combines their output in order to achieve
    an overall result.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 组合器模式提供了一种有效的方法，可以并行计算一组记录的函数，然后组合它们的输出以实现整体结果。
- en: '![Solution](img/B05261_14_04-1.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![解决方案](img/B05261_14_04-1.jpg)'
- en: 'Generally, it consists of three functions that must be provided by the caller:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，调用者必须提供三个函数：
- en: '**Initialize ***(e) -> C[0]: *Creates the initial *container*, otherwise known
    as `createCombiner`, `type` constructor, or `zero`.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始化***(e) -> C[0]: *创建初始*容器*，也称为`createCombiner`，`type`构造函数或`zero`。'
- en: In this function, you should create and initialize an instance that will serve
    as the container for all other combined values. Sometimes the first value from
    each key is also provided to pre-populate the container that will eventually hold
    all the combined values for that key. In this case, the function is known as *unit*.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在此函数中，您应该创建和初始化一个实例，该实例将作为所有其他组合值的容器。有时还会提供每个键的第一个值，以预先填充最终将保存该键的所有组合值的容器。在这种情况下，该函数称为*单元*。
- en: It's worth noting that this function is executed exactly once per key on every
    partition in your dataset. Therefore, it is potentially called multiple times
    for each key and consequently must not introduce any side-effects that would produce
    inconsistent results were the dataset to be distributed differently.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，此函数在数据集中的每个分区上每个键执行一次。因此，对于每个键可能会多次调用，因此不能引入任何可能在数据集分布不同的情况下产生不一致结果的副作用。
- en: '**Update***(C[0], e) -> C[i]: *Adds an element to the container. Otherwise
    known as `mergeValue`, `bind` *function*, or `reduce`.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新***(C[0], e) -> C[i]: *向容器中添加一个元素。也被称为`mergeValue`、`bind` *函数*或`reduce`。'
- en: In this function, you should add a record from the originating RDD into the
    container. This usually involves transforming or aggregating the value in some
    way and only the output of this calculation is taken forwards inside the container.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，您应该将源RDD中的记录添加到容器中。这通常涉及以某种方式转换或聚合值，只有这个计算的输出才会在容器内继续前进。
- en: As updates are executed in parallel and in any order, this function must be
    commutative and associative.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 更新以并行和任意顺序执行，因此此函数必须是可交换和可结合的。
- en: '**Merge***(C[i], C[j]) -> C[k]: *Combines together two containers. Otherwise
    known as `mergeCombiners` or `merge`.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合并***(C[i], C[j]) -> C[k]: *将两个容器合并在一起。也被称为`mergeCombiners`或`merge`。'
- en: In this function, you should combine the values represented by each container
    to form a new value, which is then taken forwards.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，您应该组合每个容器所代表的值，形成一个新的值，然后将其带入下一步。
- en: Again, because there are no guarantees on the order of merges, this function
    should be commutative and associative.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，由于合并顺序没有保证，这个函数应该是可交换和可结合的。
- en: You may have noticed a similarity between this pattern and the concept of *monads*.
    If you haven't encountered monads yet, they represent an abstract mathematical
    concept, used in functional programming as a way of expressing functions so that
    they are composable in a general way. They support many features, such as composition,
    side-effect free execution, repeatability, consistency, lazy evaluation, immutability,
    and provide many other benefits. We will not give a full explanation of monads
    here, there are plenty of great introductions already out there - for example
    [http://www.simononsoftware.com/a-short-introduction-to-monads/](http://www.simononsoftware.com/a-short-introduction-to-monads/),
    which takes a practical rather than a theoretical viewpoint. Instead, we will
    explain where the Combiner pattern is different and how it helps to understand
    Spark.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到了这种模式与*monads*的概念之间的相似性。如果您还没有遇到monads，它们代表一种抽象的数学概念，在函数式编程中用来表达函数，使得它们以一般的方式可组合。它们支持许多特性，比如组合、无副作用的执行、可重复性、一致性、惰性求值、不可变性，并提供许多其他好处。我们不会在这里对monads进行全面的解释，因为已经有很多很好的介绍了
    - 例如[http://www.simononsoftware.com/a-short-introduction-to-monads/](http://www.simononsoftware.com/a-short-introduction-to-monads/)，它采用的是实践而不是理论的观点。相反，我们将解释组合器模式的不同之处以及它如何帮助理解Spark。
- en: Spark executes the `update` function on every record in your dataset. Due to
    its distributed nature, this can happen in parallel. It also runs the *merge*
    function to combine results from the output of each partition. Again, because
    this function is applied in parallel and therefore could be combined in any order,
    Spark requires these functions to be *commutative*, meaning that the sequence
    in which they are applied should have no impact on the overall answer. It's this
    commutative, merge step that really provides the basis of the definition.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在数据集中的每条记录上执行`update`函数。由于其分布式性质，这可以并行进行。它还运行*merge*函数来合并每个分区输出的结果。同样，由于这个函数是并行应用的，因此可以以任何顺序组合，Spark要求这些函数是*可交换*的，这意味着它们被应用的顺序对最终答案没有影响。正是这个可交换的合并步骤真正提供了定义的基础。
- en: An understanding of this pattern is useful for reasoning about the behavior
    of any distributed aggregations. If you're interested in understanding this pattern
    further, a nice implementation can be found in [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这种模式对于推理任何分布式聚合的行为是有用的。如果您对这种模式感兴趣，可以在[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala)中找到一个不错的实现。
- en: In addition to this, it's useful when trying to determine which high-level API
    to use. With so many available, it's sometimes difficult to know which one to
    choose. By applying an understanding of *types* to the preceding descriptions,
    we can decide on the most fitting and performant API. For example, where the types
    of *e* and *C[n]* are the same, you should consider using `reduceByKey`. However,
    where the type of *e* is different to *C[n]*, then an operation such as `combineByKey`
    should be considered.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，在尝试确定使用哪个高级API时，这也是有用的。由于有这么多可用的API，有时很难知道选择哪一个。通过将*types*的理解应用到前面的描述中，我们可以决定使用最合适和性能最佳的API。例如，如果*e*和*C[n]*的类型相同，您应该考虑使用`reduceByKey`。然而，如果*e*的类型与*C[n]*不同，那么应该考虑使用`combineByKey`等操作。
- en: To illustrate, let's consider some different approaches using four of the most
    common operations available on the `RDD` API.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们考虑一些使用RDD API上四种最常见操作的不同方法。
- en: Example
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 例子
- en: 'To provide some context, let''s say we have an RDD of key-value pairs representing
    people mentioned in news articles, where the key is the name of the person referred
    to in the article, and the value is a pre-filtered, tokenized, bag-of-words, textual-version
    of the article:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一些背景，假设我们有一个RDD，其中包含表示新闻文章中提到的人的键值对，其中键是文章中提到的人的姓名，值是经过预过滤、标记化、词袋化的文章的文本版本：
- en: '[PRE19]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now suppose we want to find some statistics about articles in which a person
    is mentioned, for example, min and max length, most frequently used words (excluding
    stop-words), and so on. In this case, our result would be of the form, `(person:String,stats:ArticleStats)`,
    where `ArticleStats` is a case class designed to hold the required statistics:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想要找出提到某个人的文章的一些统计数据，例如最小和最大长度，最常用的单词（不包括停用词）等。在这种情况下，我们的结果将是`(person:String,stats:ArticleStats)`的形式，其中`ArticleStats`是一个用于保存所需统计数据的case类：
- en: '[PRE20]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s start with the definition of the three combiner functions, as described
    previously:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从之前描述的三个组合器函数的定义开始：
- en: '[PRE21]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As you might notice, these functions are really just the syntactic sugar of
    our pattern; the real logic is hidden away in the companion class and the semigroup:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到，这些函数实际上只是我们模式的语法糖；真正的逻辑隐藏在伴随类和半群中：
- en: '[PRE22]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: For our purposes, we won't cover these in detail, let's just assume that any
    computation necessary to calculate the statistics are carried out by the supporting
    code - including the logic for finding the extremities of two previously calculated
    metrics - and instead focus on the explanation of our different approaches.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们不会详细介绍这些，让我们假设计算统计数据所需的任何计算都是由支持代码执行的-包括查找两个先前计算的指标的极端的逻辑-而不是专注于解释我们的不同方法。
- en: '**GroupByKey approach**:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**GroupByKey方法**：'
- en: 'Our first approach is by far and away the slowest option because `groupByKey`
    doesn''t use the `update` function. Despite this obvious disadvantage, we can
    still achieve our result - by sandwiching the `groupByKey` between maps where
    the first map is used to convert into the desired type and the last to perform
    the reduce-side aggregation:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一种方法是迄今为止最慢的选项，因为`groupByKey`不使用`update`函数。尽管存在这种明显的劣势，我们仍然可以实现我们的结果-通过在第一个映射和最后一个执行减少侧面聚合的地方夹入`groupByKey`：
- en: '[PRE23]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: However, you will notice that it does not perform any map-side combining for
    efficiency, instead preferring to combine all values on the reduce-side, meaning
    that all values are copied across the network as part of the shuffle.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您会注意到它不执行任何map-side组合以提高效率，而是更喜欢在reduce-side上组合所有值，这意味着所有值都作为洗牌的一部分在网络上复制。
- en: For this reason, you should always consider the following alternatives before
    resorting to this approach.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在采用这种方法之前，您应该始终考虑以下替代方案。
- en: '**ReduceByKey approach**:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReduceByKey方法**：'
- en: 'To improve on this, we can use `reduceByKey`. Unlike `groupByKey`, `reduceByKey`
    provides map-side combining for efficiency by making use of the `update` function.
    In terms of performance, it offers an optimum approach. However, it still requires
    each value to be manually converted to the correct type prior to invocation:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进这一点，我们可以使用`reduceByKey`。与`groupByKey`不同，`reduceByKey`通过使用`update`函数提供了map-side组合以提高效率。在性能方面，它提供了最佳方法。但是，仍然需要在调用之前手动将每个值转换为正确的类型：
- en: '[PRE24]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The result is achieved in two steps by mapping records from the originating
    `RDD` into the desired type.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将源`RDD`中的记录映射到所需的类型，结果分两步实现。
- en: '**AggregateByKey approach**:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '**AggregateByKey方法**：'
- en: 'Again, `aggregateByKey` provides the same performance characteristics as `reduceByKey`
    - by implementing map-side combine - but this time as one operation:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，`aggregateByKey`提供了与`reduceByKey`相同的性能特征-通过实现map-side combine-但这次作为一个操作：
- en: '[PRE25]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**CombineByKey Approach**:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**CombineByKey方法**：'
- en: 'Generally, `combineByKey` is thought of as the most flexible key-based operation,
    giving you complete control over all three functions in the Combiner pattern:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，`combineByKey`被认为是最灵活的基于键的操作，可以完全控制组合器模式中的所有三个函数：
- en: '[PRE26]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: While providing `init` as a *function* rather than just a single value might
    give you more flexibility in select scenarios, in practice for most problems the
    relationship between `init`, `update`, and `merge` is such that you don't really
    gain anything in terms of functionality or performance between either approach.
    And regardless, all three are backed by `combineByKeyWithClassTag`, so in this
    instance feel free to choose whichever one that is a better syntactic fit for
    your problem, or just pick the one you prefer.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将`init`作为*函数*而不仅仅是单个值可能会在某些情况下为您提供更多的灵活性，但实际上对于大多数问题来说，`init`，`update`和`merge`之间的关系是这样的，以至于在功能或性能方面您并没有真正获得任何好处。而且无论如何，所有三个都由`combineByKeyWithClassTag`支持，因此在这种情况下，可以随意选择更适合您问题的语法或者只选择您更喜欢的语法。
- en: Optimized cluster
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化的集群
- en: Problem
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: I want to know how to configure my Spark job's executors in order to make full
    use of the resources of my cluster, but with so many options I'm confused.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我想知道如何配置我的Spark作业的执行器，以充分利用集群的资源，但是有这么多选项，我感到困惑。
- en: Solution
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: As Spark is designed to scale horizontally, generally speaking, you should prefer
    having *more* executors over *larger* executors. But with each executor comes
    the overhead of a JVM, so it's advisable to make full use of them by running *multiple*
    tasks inside each executor. As this seems like a bit of a contradiction, let's
    look at how to configure Spark to achieve this.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark设计为水平扩展，一般来说，您应该更喜欢*更多*的执行器而不是*更大*的执行器。但是每个执行器都带来了JVM的开销，因此最好通过在每个执行器内运行*多个*任务来充分利用它们。由于这似乎有点矛盾，让我们看看如何配置Spark以实现这一点。
- en: 'Spark provides the following options (specified on the command line or in configuration):'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供以下选项（在命令行或配置中指定）：
- en: '[PRE27]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Number of executors can be estimated using the following formula:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下公式估算执行器数量：
- en: '*number of executors = (total cores - cluster overhead) / cores per executor*'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '*执行器数量=（总核心数-集群开销）/每个执行器的核心数*'
- en: 'For example, when using a YARN-based cluster accessing HDFS and running in
    YARN-client mode, the equation would be as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当使用基于YARN的集群访问HDFS并在YARN客户端模式下运行时，方程如下：
- en: '*((T - (2*N + 6)) / 5)*'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '*((T-（2*N + 6）)/5)*'
- en: 'where:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '`T`: Total number of cores in the cluster.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`T`：集群中的总核心数。'
- en: '`N`: Total of nodes in the cluster.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`N`：集群中的节点总数。'
- en: '`2`: Removes the *per node* overhead of HDFS and YARN.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '`2`：去除HDFS和YARN的*每个节点*开销。'
- en: Assumes two HDFS processes on each node - `DataNode` and `NodeManager`.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 假设每个节点上有两个HDFS进程-`DataNode`和`NodeManager`。
- en: '`6`: Removes the *master process* overhead of HDFS and YARN.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '`6`：去除HDFS和YARN的*主进程*开销。'
- en: Assumes the average of six processes - `NameNode`, `ResourceManager`, `SecondaryNameNode`,
    `ProxyServer`, `HistoryServer`, and so on. Obviously, this is an example and in
    reality it depends on what other services are running the cluster, along with
    other factors such as Zookeeper quorum size, HA strategy, and so on.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 假设平均有六个进程-`NameNode`、`ResourceManager`、`SecondaryNameNode`、`ProxyServer`、`HistoryServer`等等。显然，这只是一个例子，实际上取决于集群中运行的其他服务，以及其他因素，如Zookeeper
    quorum大小、HA策略等等。
- en: '`5`: Anecdotally, the optimum number of cores for each executor to ensure optimal
    task concurrency without prohibitive disk I/O contention.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`5`：据说，每个执行器的最佳核心数，以确保最佳任务并发性而不会产生严重的磁盘I/O争用。'
- en: 'Memory allocation can be estimated using the following formula:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 内存分配可以使用以下公式估算：
- en: '*mem per executor = (mem per node / number of executors per node) * safety
    fraction*'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '*每个执行器的内存=（每个节点的内存/每个节点的执行器数）*安全系数*'
- en: 'For example, when using a YARN-based cluster running in YARN-client mode with
    64 GB per node, the equation would be as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当使用基于YARN的集群以YARN-client模式运行，每个节点为64 GB时，方程如下：
- en: '*(64 / E)* 0.9 => 57.6 / E*'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '*(64 / E)* 0.9 => 57.6 / E*'
- en: 'where:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '`E`:Number of executors per node (as calculated in the previous example).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`E`：每个节点的执行器数（如前面的示例中计算的）。'
- en: '`0.9`: Fraction of actual memory allocated to the heap after subtracting off-heap.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '`0.9`：在扣除堆外内存后分配给堆的实际内存的比例。'
- en: overhead (`spark.yarn.executor.memoryOverhead`, default 10%).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 开销（`spark.yarn.executor.memoryOverhead`，默认10%）。
- en: It's worth noting that while it is generally beneficial to allocate more memory
    to an executor (allowing more space for sorting, caching, and so on) increasing
    the memory also increases *garbage collection pressure*. The GC must sweep the
    entire heap for unreachable object references, therefore the larger the memory
    region it has to analyze, the more resources it must consume and at some point
    this leads to diminishing returns. Whilst there's no absolute figure as to at
    what point this happens, as a general rule of thumb, keep the memory per executor
    under 64 GB to avoid problems.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，虽然通常将更多的内存分配给执行器（允许更多的空间用于排序、缓存等）是有益的，但增加内存也会增加*垃圾收集压力*。GC必须扫描整个堆以查找不可达的对象引用，因此，它必须分析的内存区域越大，它消耗的资源就越多，而在某个时候，这会导致收益递减。虽然没有绝对的数字来说明在什么时候会发生这种情况，但作为一个经验法则，保持每个执行器的内存低于64
    GB可以避免问题。
- en: The preceding equations should provide a good starting-point estimation for
    sizing your cluster. For further tuning, you may wish to experiment by tweaking
    these settings and measuring the effect on performance using the Spark UI.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程应该为调整集群大小提供一个良好的起点估计。为了进一步调整，您可能希望通过调整这些设置并使用Spark UI来测量性能的影响来进行实验。
- en: Redistribution pattern
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新分配模式
- en: Problem
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: My analytic always runs on the same few executors. How do I increase the level
    of parallelism?
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我的分析总是在同几个执行器上运行。如何增加并行性？
- en: Solution
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: When `Datasets` and `RDDs` are relatively small to begin with, even if you then
    expand them using `flatMap`, any child in the lineage will take the parents number
    of partitions.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 当`Datasets`和`RDDs`相对较小时，即使使用`flatMap`扩展它们，衍生的任何子代都将采用父代的分区数。
- en: So, if some of your executors are idle, calling the `repartition` function could
    improve your level of parallelism. You will incur the immediate cost of moving
    data around, but this could pay-off overall.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您的一些执行器处于空闲状态，调用`repartition`函数可能会提高您的并行性。您将立即付出移动数据的成本，但这可能会在整体上得到回报。
- en: 'Use the following command to determine the number of partitions for your data
    and hence the parallelism:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令确定数据的分区数和并行性：
- en: '[PRE28]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: If the number of partitions is less than the maximum number of tasks allowable
    on your cluster, then you're not making full use of your executors.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分区数少于集群允许的最大任务数，则没有充分利用执行器。
- en: Conversely, if you have a large number of tasks (10,000+) and they aren't running
    for very long then you should probably call `coalesce` to make better use of your
    resources - starting and stopping tasks is relatively expensive!
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果有大量的任务（10,000+）并且运行时间不长，那么您可能应该调用`coalesce`来更好地利用您的资源-启动和停止任务相对昂贵！
- en: Example
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: 'Here, we increase the parallelism of a `Dataset` to `400`. The physical plan
    will show this as `RoundRobinPartitioning(400)`, like so:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将`Dataset`的并行性增加到`400`。物理计划将显示为`RoundRobinPartitioning(400)`，如下所示：
- en: '[PRE29]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And here''s the equivalent re-partitioning for an `RDD` performed by simply
    specifying the number of partitions to use in the `reduceByKey` function:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`RDD`的等效重新分区，只需在`reduceByKey`函数中指定要使用的分区数：
- en: '[PRE30]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Salting key pattern
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 盐键模式
- en: Problem
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: Most of my tasks finish in a reasonable time, but there's always one or two
    that take much longer (>10x) and repartitioning does not seem to have any beneficial
    effect.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我的大多数任务都在合理的时间内完成，但总会有一两个任务花费更长的时间（>10倍），重新分区似乎没有任何好处。
- en: Solution
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: If you're experiencing having to wait for a handful of slow tasks, then you
    could be suffering from a skew in your data distribution. Symptoms of this are
    that you're seeing some tasks taking far longer than others or that some tasks
    have far more input or output.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到必须等待少数慢任务的情况，那么您可能遭受数据分布不均的影响。这种情况的症状是，您会看到一些任务所花费的时间远远超过其他任务，或者一些任务的输入或输出要多得多。
- en: If this is the case, the first thing to do is check that the number of keys
    is greater than the number of executors, as coarse-grained grouping can limit
    parallelism. A quick way to find the number of keys in your `RDD` is to use `rdd.keys.count`.
    If this value is lower than the number of executors, then reconsider your key
    strategy. Patterns such as *Expand and Conquer* may be able to help out.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这种情况，首先要做的是检查键的数量是否大于执行器的数量，因为粗粒度分组可能会限制并行性。查找`RDD`中键的数量的快速方法是使用`rdd.keys.count`。如果这个值低于执行器的数量，那么请重新考虑您的键策略。例如，*扩展和征服*等模式可能会有所帮助。
- en: 'If the preceding things are in order, the next thing to review is key distribution.
    Where you find a small number of keys with large numbers of associated values,
    consider the *Salting Key* pattern. In this pattern, popular keys are subdivided
    by appending a random element. For example:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的事情都有序，需要审查的下一件事是键分布。当您发现少数键具有大量关联值时，考虑*盐化键*模式。在此模式中，通过附加一个随机元素来细分热门键。例如：
- en: '[PRE31]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This results in a more balanced key distribution because during the shuffle,
    the `HashPartitioner` sends the new keys to different executors. You can choose
    the value of n to suit the parallelism you need - greater skew in the data necessitates
    a greater range of salts.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了更加平衡的键分布，因为在洗牌过程中，`HashPartitioner`将新的键发送到不同的执行器。您可以选择n的值来适应您需要的并行性 - 数据中更大的偏斜需要更大范围的盐。
- en: Of course, all this salting does mean that you'll need to re-aggregate back
    onto the old keys to ensure that you ultimately calculate the correct answer.
    But, depending on the amount of skew in your data, a two-phase aggregation may
    still be faster.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，所有这些盐化都意味着您需要重新聚合到旧键上，以确保最终计算出正确的答案。但是，根据数据中的偏斜程度，两阶段聚合可能仍然更快。
- en: You can either apply this salting to all keys, or filter out as in the preceding
    example. The threshold at which you filter, decided by `isPopular` in the example,
    is also entirely your choice.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将这种盐应用于所有键，或者像前面的示例中那样进行过滤。您在示例中决定的过滤阈值，由`isPopular`决定，也完全由您自己选择。
- en: Secondary sort pattern
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二次排序模式
- en: Problem
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: 'When grouping by keys, my analytic has to explicitly sort the values *after*
    they are grouped. This sorting takes place in memory, therefore large value-sets
    take a long time, and they may involve spilling to disk and sometimes give an
    `OutOfMemoryError`. Here is an example of the problematic approach:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 当按键分组时，我的分析必须在它们分组之后显式对值进行排序。这种排序发生在内存中，因此大的值集需要很长时间，它们可能涉及溢出到磁盘，并且有时会出现`OutOfMemoryError`。以下是问题方法的示例：
- en: '[PRE32]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Instead, when grouping by key, values should be pre-ordered within each key
    for immediate and efficient follow-on processing.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当按键分组时，应该在每个键内预先排序值，以便进行即时和高效的后续处理。
- en: Solution
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the *Secondary Sort* pattern to order the list of items in a group efficiently
    by using the shuffle machinery. This approach will scale when handling even the
    largest of datasets.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*二次排序*模式通过使用洗牌机制高效地对组中的项目列表进行排序。这种方法在处理最大的数据集时也能够扩展。
- en: 'In order to sort efficiently, this pattern utilizes three concepts:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地进行排序，此模式利用了三个概念：
- en: '**Composite key**: Contains both the elements you want to group by *and* the
    elements you want to sort by.'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**复合键**：包含您想要按组进行分组的元素*和*您想要按排序进行排序的元素。'
- en: '**Grouping partitioner**: Understands which parts of the composite key are
    related to *grouping*.'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分组分区器**：了解复合键的哪些部分与*分组*相关。'
- en: '**Composite key ordering**: Understands which parts of the composite key are
    related to *ordering*.'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**复合键排序**：了解复合键的哪些部分与*排序*相关。'
- en: Each of these is injected into Spark so that the final dataset is presented
    as grouped and ordered.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念中的每一个都被注入到Spark中，以便最终的数据集呈现为分组和排序。
- en: Please note, in order to perform a secondary sort you need to use `RDDs`, as
    the new `Dataset` API is not currently supported. Track the progress on the following
    JIRA [https://issues.apache.org/jira/browse/SPARK-3655](https://issues.apache.org/jira/browse/SPARK-3655).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了执行二次排序，您需要使用`RDDs`，因为新的`Dataset` API目前不受支持。跟踪以下JIRA的进展[https://issues.apache.org/jira/browse/SPARK-3655](https://issues.apache.org/jira/browse/SPARK-3655)。
- en: Example
  id: totrans-403
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: 'Consider the following model:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下模型：
- en: '[PRE33]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here we have an entity representing the occasions where people are mentioned
    in news articles containing the person's name, the article they were mentioned
    in, and its publication date.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个实体，代表了人们在新闻文章中被提及的场合，包括人名、提及的文章以及其发布日期。
- en: 'Suppose we want to group together all the mentions of people with the same
    name, and order them by time. Let''s look at the three mechanisms we need:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要将所有提到相同名称的人的提及内容分组在一起，并按时间排序。让我们看看我们需要的三种机制：
- en: '**Composite key**:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '**复合键**：'
- en: '[PRE34]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '*Contains both name and published date.*'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '*包含名称和发布日期。*'
- en: '**Grouping partitioner**:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**分组分区器**：'
- en: '[PRE35]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '*It''s only grouping by name.*'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '*它只是按名称分组。*'
- en: '**Composite key ordering**:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '**复合键排序**：'
- en: '[PRE36]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '*It''s only sorting by published date.*'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '*它只是按发布日期排序。*'
- en: 'Once we have defined these, we can use them in the API, like so:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了这些，我们就可以在API中使用它们，就像这样：
- en: '[PRE37]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here the `SortKey` is used to pair the data, the `GroupingPartitioner` is used
    when partitioning the data, and the `Ordering` is used during the merge and, of
    course, it's found via Scala's `implicit` mechanism, which matches based on type.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用`SortKey`来配对数据，使用`GroupingPartitioner`在分区数据时使用，使用`Ordering`在合并时使用，当然，它是通过Scala的`implicit`机制找到的，该机制是基于类型匹配的。
- en: Filter overkill pattern
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤过度模式
- en: Problem
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: My analytic uses a *whitelist* in order to filter relevant data for processing.
    The filter happens early on in the pipeline so that my analytic only ever has
    to process the data I'm interested in, for maximum efficiency. However, the whitelist
    frequently changes meaning my analytic must be executed afresh, each time, against
    the new list.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我的分析使用*白名单*来过滤相关数据进行处理。过滤发生在管道的早期阶段，因此我的分析只需要处理我感兴趣的数据，以获得最大的效率。然而，白名单经常更改，这意味着我的分析必须针对新列表每次都要重新执行。
- en: Solution
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: Contrary to some of the other advice you'll read here, in some scenarios calculating
    results across all the data, by removing filters, can actually *increase* the
    overall efficiency of an analytic.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 与您在这里阅读的其他一些建议相反，在某些情况下，通过删除过滤器在所有数据上计算结果实际上可以*增加*分析的整体效率。
- en: If you are frequently rerunning your analytic over different segments of the
    dataset, then consider using a popular approach, described here as the *Filter
    Overkill pattern*. This involves omitting all filters in Spark and processing
    over the entire corpus. The results of this one-off processing will be much larger
    that the filtered version, but it can be easily indexed in a tabular data store
    and filtered dynamically at query time. This avoids having to apply different
    filters over multiple runs, and having to re-compute historical data when filters
    change.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您经常在数据集的不同部分上重新运行分析，请考虑使用一种流行的方法，这里描述为*Filter Overkill模式*。这涉及在Spark中省略所有过滤器，并在整个语料库上进行处理。这种一次性处理的结果将比经过过滤的版本大得多，但可以很容易地在表格数据存储中进行索引，并在查询时动态过滤。这避免了在多次运行中应用不同的过滤器，并且在过滤器更改时重新计算历史数据。
- en: Probabilistic algorithms
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率算法
- en: Problem
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: It takes too long to compute statistics over my dataset because it is too large.
    By the time the response is received, it's out of date or no longer relevant.
    Therefore, it's more important to receive a timely response, or at least provide
    a maximum bound to time-complexity, than a complete or correct answer. In fact,
    a well-timed estimate even with *a small probability of error* would be taken
    in preference to a correct answer where the running time is not known.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 计算我的数据集的统计数据需要太长时间，因为它太大了。到收到响应的时候，数据已经过时或不再相关。因此，及时收到响应，或者至少提供时间复杂度的最大限制，比完整或正确的答案更重要。事实上，即使有*小概率的错误*，及时的估计也会被优先考虑，而不是在运行时间未知的情况下得到正确的答案。
- en: Solution
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: Probabilistic algorithms use *randomization* to improve the time complexity
    of their algorithms and guarantee worst case performance. If you are time sensitive
    and just about right is good enough, you should consider using a probabilistic
    algorithm.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 概率算法使用*随机化*来改进其算法的时间复杂度，并保证最坏情况下的性能。如果您对时间敏感，而“差不多就行”的话，您应该考虑使用概率算法。
- en: 'In addition, the same can be said for the problem of memory usage. There are
    a set of Probabilistic algorithms that provide estimates inside restricted space-complexity.
    Examples include:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于内存使用的问题也可以同样适用。有一组概率算法可以在受限的空间复杂度内提供估计。例如：
- en: '**Bloom Filter **is a membership test that is guaranteed to never miss an element
    in a set, but could give you a false positive, that is, determine an element to
    be a member of a set when it is not. It''s useful for quickly reducing the amount
    of data in a problem-space prior to a more accurate calculation.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bloom Filter**是一种成员测试，保证不会错过集合中的任何元素，但可能会产生误报，即确定一个元素是集合的成员，而实际上不是。在更准确计算之前，它对快速减少问题空间中的数据非常有用。'
- en: '**HyperLogLog **counts the number of distinct values in a column, providing
    a very reasonable estimate using a fixed memory footprint.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HyperLogLog**计算列中不同值的数量，使用固定的内存占用量提供一个非常合理的估计。'
- en: '**CountMinSketch** provides a frequency table used for counting occurrences
    of events in a stream of data. Particularly useful in Spark streaming where a
    fixed memory footprint eliminates the potential for memory overflows.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CountMinSketch**提供了用于计算数据流中事件发生次数的频率表。在Spark流处理中特别有用，其中固定的内存占用量消除了内存溢出的可能性。'
- en: 'Spark provides implementations of these in `org.apache.spark.sql.DataFrameStatFunctions`
    and they can be used by accessing `df.stat`. Spark also includes some access via
    the `RDD` API:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在`org.apache.spark.sql.DataFrameStatFunctions`中提供了这些实现，可以通过访问`df.stat`来使用。Spark还通过`RDD`
    API包括一些访问：
- en: '[PRE38]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Example
  id: totrans-437
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: For an example of how to use a **Bloom Filter** see [Chapter 11](ch11.xhtml
    "Chapter 11. Anomaly Detection on Sentiment Analysis"), *Anomaly Detection on
    Sentiment Analysis*.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何使用**Bloom Filter**的示例，请参见[第11章](ch11.xhtml "第11章。情感分析中的异常检测")*情感分析中的异常检测*。
- en: Selective caching
  id: totrans-439
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有选择地缓存
- en: Problem
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: My analytic is caching datasets, but if anything, it's running slower than before.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 我的分析正在缓存数据集，但如果说有什么变化的话，它比以前运行得更慢。
- en: Solution
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Caching is key to getting the most performance out of Spark; however, when
    used incorrectly, it can have a detrimental effect. Caching is particularly useful
    whenever you intend to use an RDD more than once. This generally happens when
    you are: (i) using the data across stages, (ii) the data appears in the lineage
    of multiple child datasets, or (iii) during iterative processes, such as stochastic
    gradient descent.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存是提高Spark性能的关键；然而，使用不当时，它可能会产生不利影响。当您打算多次使用RDD时，缓存特别有用。这通常发生在以下情况下：（i）在不同阶段使用数据，（ii）数据出现在多个子数据集的谱系中，或者（iii）在迭代过程中，例如随机梯度下降。
- en: The problem occurs when you cache indiscriminately without considering reuse.
    This is because the cache adds overhead when it's created, updated and flushed,
    and then must be garbage collected when not used. Therefore, improper caching
    can actually *slow down your job*. So, the easiest way to improve caching is to
    stop doing it (selectively of course).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 当您不考虑重用而随意缓存时，问题就会出现。这是因为缓存在创建、更新和刷新时会增加开销，而在不使用时必须进行垃圾回收。因此，不正确的缓存实际上可能会*减慢您的作业*。因此，改进缓存的最简单方法是停止这样做（当然是有选择地）。
- en: Another consideration is whether there's enough memory allocated and available
    to efficiently cache your RDD. If your dataset won't fit into memory, Spark will
    either throw an `OutOfMemoryError` or swap data to disk (depending on the storage
    levels, this will be talked about shortly). In the latter case, this could have
    a performance impact due to both (i) the time taken to move extra data in and
    out of memory and (ii) having to wait for the availability of the disk (I/O wait).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个考虑因素是是否有足够的内存分配和可用来有效地缓存您的RDD。如果您的数据集无法放入内存，Spark将抛出`OutOfMemoryError`，或者将数据交换到磁盘（取决于存储级别，这将很快讨论）。在后一种情况下，这可能会因为（i）移动额外数据进出内存所花费的时间，以及（ii）等待磁盘的可用性（I/O等待）而产生性能影响。
- en: 'In order to determine whether you have enough memory allocated to your executors,
    first cache the dataset as follows:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定您的执行程序是否分配了足够的内存，首先将数据集缓存如下：
- en: '[PRE39]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Then, look at the *Storage* page in the Spark UI. For each RDD, this provides
    the fraction cached, its size, and the amount spilled to disk.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在Spark UI的*Storage*页面查看。对于每个RDD，这提供了缓存的比例、其大小以及溢出到磁盘的数量。
- en: '![Solution](img/B05261_14_07.jpg)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![Solution](img/B05261_14_07.jpg)'
- en: 'This should enable you to adjust the memory allocated to each executor in order
    to ensure that your data fits in memory. There are also the following caching
    options available:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该使您能够调整分配给每个执行程序的内存，以确保您的数据适合内存。还有以下可用的缓存选项：
- en: '**NONE**: No caching (default)'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NONE**：不缓存（默认）'
- en: '**MEMORY**: Used when `cache` is called'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MEMORY**：在调用`cache`时使用'
- en: '**DISK**: Spill to disk'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DISK**：溢出到磁盘'
- en: '**SER**: Same as MEMORY, but objects are stored in a byte array'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SER**：与MEMORY相同，但对象存储在字节数组中'
- en: '**2** (**REPLICATED**): Keep a cached copy on two different nodes'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2**（**REPLICATED**）：在两个不同的节点上保留缓存副本'
- en: 'The preceding options can be used in any combination, for example:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 上述选项可以以任何组合方式使用，例如：
- en: If you're experiencing `OutOfMemoryError` errors, try changing to `MEMORY_AND_DISK`
    to allow spilling of the cache to disk
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您遇到`OutOfMemoryError`错误，请尝试切换到`MEMORY_AND_DISK`以允许将缓存溢出到磁盘
- en: If you're experiencing high garbage collection times, consider trying one of
    the serialized byte buffer forms of cache, such as `MEMORY_AND_SER`, as this will
    circumvent the GC entirely (at the slight cost of increased serialization)
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您遇到高的垃圾回收时间，请考虑尝试一种序列化的字节缓冲形式的缓存，例如`MEMORY_AND_SER`，因为这将完全规避GC（稍微增加序列化的成本）。
- en: The goal here is to ensure that the *Fraction Cached* is at 100%, and where
    possible, minimize the *Size on Disk* to establish effective in-memory caching
    of your datasets.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是确保*Fraction Cached*为100%，并在可能的情况下，最小化*Size on Disk*，以建立数据集的有效内存缓存。
- en: Garbage collection
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垃圾回收
- en: Problem
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: The *GC time* of my analytic is a significant proportion of the overall processing
    time (>15%).
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我的分析的*GC时间*占整体处理时间的比例很大（>15%）。
- en: Solution
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: Spark's garbage collector works pretty efficiently out of the box, so you should
    only attempt to adjust it if you're sure that it's the cause and not the *symptom*
    of the problem. Before altering the GC settings, you should ensure that you have
    reviewed all other aspects of your analytic. Sometimes you might see high GC times
    in the Spark UI for reasons other than a poor GC configuration. Most of the time,
    it's worth investigating these first.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的垃圾收集器在开箱即用时效率非常高，因此只有在确定它是问题的原因而不是问题的*症状*时，才应尝试调整它。在更改GC设置之前，您应该确保已经审查了分析的所有其他方面。有时，您可能会在Spark
    UI中看到高的GC时间，原因并不一定是糟糕的GC配置。大多数情况下，首先调查这些情况是值得的。
- en: If you're seeing frequent or lengthy GC times, the first thing to do is confirm
    that your code is behaving sensibly and make sure that it's not at the root of
    excess/irregular memory consumption. For example, review your caching strategy
    (see the preceding section) or use the `unpersist` function to explicitly remove
    RDDs or Datasets that are no longer required.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到频繁或持续的GC时间，首先要确认您的代码是否表现得合理，并确保它不是过度/不规则内存消耗的根源。例如，审查您的缓存策略（参见上一节）或使用`unpersist`函数明确删除不再需要的RDD或数据集。
- en: Another factor for consideration is the number of objects you allocate within
    your job. Try to minimize the amount of objects you instantiate by (i) simplifying
    your domain model, or (ii) by reusing instances, or (iii) by preferring primitives
    where you can.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是您在作业中分配的对象数量。尝试通过（i）简化您的领域模型，或者（ii）通过重复使用实例，或者（iii）在可以的情况下优先使用基本类型来最小化您实例化的对象数量。
- en: Finally, if you're still seeing lengthy GC times, try tuning the GC. There's
    some great information provided by Oracle on how to do this ([https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/g1_gc_tuning.html](https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/g1_gc_tuning.html)),
    but specifically there is evidence to suggest that Spark can perform well using
    the G1 GC. It's possible to switch to this GC by adding `XX:UseG1GC` to the Spark
    command line.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您仍然看到持续的GC时间，请尝试调整GC。Oracle提供了一些关于如何做到这一点的很好的信息（[https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/g1_gc_tuning.html](https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/g1_gc_tuning.html)），但特别是有证据表明Spark可以使用G1
    GC表现良好。可以通过在Spark命令行中添加`XX:UseG1GC`来切换到此GC。
- en: 'When tuning the G1 GC, the two main options are:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整G1 GC时，主要的两个选项是：
- en: '**InitiatingHeapOccupancyPercent: **A threshold percent of how full the heap
    should be before the GC triggers a cycle. The lower the percentage, the more frequently
    the GC runs, but the less work it has to do on each run. Therefore, if you set
    it to *less than* 45% (the default value), you might see fewer pauses. It can
    be configured on the command line using `-XX:InitiatingHeapOccupancyPercent`.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**InitiatingHeapOccupancyPercent：**堆在触发GC周期之前应该有多满的阈值百分比。百分比越低，GC运行得越频繁，但每次运行的工作量就越小。因此，如果将其设置为*小于*45%（默认值），您可能会看到更少的暂停。可以在命令行上使用`-XX:InitiatingHeapOccupancyPercent`进行配置。'
- en: '**ConcGCThread**: The number of concurrent GC threads running in the background.
    The more threads, the quicker the garbage collection can complete. But it''s a
    trade-off as more GC threads means more CPU resource allocation. Can be configured
    on the command line using `-XX:ConcGCThread`.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ConcGCThread**：后台运行的并发GC线程数。线程越多，垃圾回收完成得越快。但这是一个权衡，因为更多的GC线程意味着更多的CPU资源分配。可以在命令行上使用`-XX:ConcGCThread`进行配置。'
- en: In summary, it's a matter of experimenting with these settings and tuning your
    analytic to find the optimum configuration.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这是一个通过调整这些设置并调整您的分析来找到最佳配置的实验过程。
- en: Graph traversal
  id: totrans-472
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图遍历
- en: Problem
  id: totrans-473
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: My analytic has an iterative step that only completes when a global condition
    is met, such as all keys report no more values to process, and consequently the
    running time can be slow and difficult to predict.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我的分析具有一个迭代步骤，只有当满足全局条件时才会完成，例如所有键都报告没有更多值要处理，因此运行时间可能会很慢，难以预测。
- en: Solution
  id: totrans-475
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Generally speaking, the efficiency of graph-based algorithms is such that,
    if you can represent your problem as a standard graph traversal problem, you probably
    should. Examples of problems with graph-based solutions include: shortest-path,
    depth-first search, and page-rank.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，基于图的算法的效率是这样的，如果您可以将问题表示为标准的图遍历问题，那么您可能应该这样做。基于图的解决方案的问题示例包括：最短路径、深度优先搜索和页面排名。
- en: Example
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: For an example of how to use the *Pregel* algorithm in `GraphX` and how to interpret
    a problem in terms of graph traversal, see [Chapter 7](ch07.xhtml "Chapter 7. Building
    Communities"), *Building Communities*.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何在`GraphX`中使用*Pregel*算法以及如何根据图遍历来解释问题的示例，请参见[第7章](ch07.xhtml "第7章。构建社区")
    *构建社区*。
- en: Summary
  id: totrans-479
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have concluded our journey by discussing aspects of distributed
    computing performance, and what to exploit when writing your own scalable analytics.
    Hopefully, you've come away with a sense of some of the challenges involved, and
    have a better understanding of how Spark works under the covers.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过讨论分布式计算性能的各个方面以及在编写可扩展分析时要利用的内容来结束了我们的旅程。希望您对涉及的一些挑战有所了解，并且对Spark在幕后的工作原理有了更好的理解。
- en: Apache Spark is a constantly evolving framework and new features and improvements
    are being added every day. No doubt it will become increasingly easier to use
    as continuous tweaks and refinements are intelligently applied into the framework,
    automating much of what must be done manually today.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个不断发展的框架，每天都在添加新功能和改进。毫无疑问，随着不断地对框架进行智能调整和改进，它将变得越来越容易使用，自动化许多今天必须手动完成的工作。
- en: In terms of what's next, who knows what's round the corner? But with Spark beating
    the competition yet again to win the 2016 CloudSort Benchmark ([http://sortbenchmark.org/](http://sortbenchmark.org/))
    and new versions set to be released every four months, one thing is for sure,
    it's going to be fast-paced. And hopefully, with the solid principles and methodical
    guidelines that you've learned in this chapter, you'll be developing scalable,
    performant algorithms for many years to come!
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 关于接下来的事情，谁知道下一个转角会是什么？但是，由于Spark再次击败竞争对手赢得了2016年CloudSort基准测试（[http://sortbenchmark.org/](http://sortbenchmark.org/)），并且新版本将每四个月发布一次，有一件事是肯定的，它将是快节奏的。希望通过本章学到的坚实原则和系统指南，您将能够开发可扩展、高性能的算法，为未来多年做好准备！
