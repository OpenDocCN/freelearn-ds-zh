- en: 'Chapter 7. Analytics Study: NLP and Big Data with Twitter Sentiment Analysis'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：分析研究：Twitter情感分析与NLP及大数据
- en: '|   | *"Data is the new oil."* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *“数据是新的石油。”* |   |'
- en: '|   | --*Unknown* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*未知* |'
- en: 'In this chapter we are going to look at two important fields of AI and data
    science: **natural language processing** (**NLP**) and big data analysis. For
    the supporting sample application, we re-implement the *Sentiment analysis of
    Twitter hashtags* project described in [Chapter 1](ch01.xhtml "Chapter 1. Programming
    and Data Science – A New Toolset"), *Programming and Data Science – A New Toolset*,
    but this time we leverage Jupyter Notebooks and PixieDust to build live dashboards
    that analyze data from a stream of tweets related to a particular entity, such
    as a product offered by a company, for example, to provide sentiment information
    as well as information about other trending entities extracted from the same tweets.
    At the end of this chapter, the reader will learn how to integrate cloud-based
    NLP services such as *IBM Watson Natural Language Understanding* into their application
    as well as perform data analysis at (Twitter) scale with frameworks such as Apache
    Spark.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探讨人工智能和数据科学的两个重要领域：**自然语言处理**（**NLP**）和大数据分析。为了支持示例应用程序，我们重新实现了[第1章](ch01.xhtml
    "第1章 编程与数据科学——一种新工具集")中描述的*Twitter标签情感分析*项目，*编程与数据科学——一种新工具集*，但这次我们利用Jupyter Notebooks和PixieDust构建实时仪表盘，分析来自与特定实体（例如公司提供的某个产品）相关的推文流中的数据，提供情感信息以及从相同推文中提取的其他趋势实体的信息。在本章的结尾，读者将学习如何将基于云的NLP服务，如*IBM
    Watson自然语言理解*，集成到他们的应用程序中，并使用像Apache Spark这样的框架在（Twitter）规模上执行数据分析。
- en: As always, we'll show how to operationalize the analytics by implementing a
    live dashboard as a PixieApp that runs directly in the Jupyter Notebook.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我们将展示如何通过实现一个作为PixieApp的实时仪表盘，直接在Jupyter Notebook中运行来使分析工作可操作化。
- en: Getting started with Apache Spark
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Apache Spark
- en: 'The term *big data* can rightly feel vague and imprecise. What is the cut-off
    for considering any dataset big data? Is it 10 GB, 100 GB, 1 TB or more? One definition
    that I like is: big data is when the data cannot fit into the memory available
    in a single machine. For years, data scientists have been forced to sample large
    datasets, so they could fit into a single machine, but that started to change
    as parallel computing frameworks that are able to distribute the data into a cluster
    of machines made it possible to work with the dataset in its entirety, provided
    of course that the cluster had enough machines. At the same time, advances in
    cloud technologies made it possible to provision on demand a cluster of machines
    that are adapted to the size of the dataset.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*大数据*这一术语常常给人模糊不清和不准确的感觉。什么样的数据集才算是大数据呢？是10 GB、100 GB、1 TB还是更多？我喜欢的一个定义是：大数据是当数据无法完全装入单个机器的内存时。多年来，数据科学家被迫对大数据集进行抽样处理，以便能够在单台机器上处理，但随着并行计算框架的出现，这些框架能够将数据分布到多台机器的集群中，使得可以在整个数据集上进行工作，当然，前提是集群有足够的机器。与此同时，云技术的进步使得可以按需提供适合数据集大小的机器集群。'
- en: Today, there are multiple frameworks (most of the time available as open source)
    that can provide robust, flexible parallel computing capabilities. Some of the
    most popular include Apache Hadoop ([http://hadoop.apache.org](http://hadoop.apache.org)),
    Apache Spark ([https://spark.apache.org](https://spark.apache.org)) and Dask ([https://dask.pydata.org](https://dask.pydata.org)).
    For our *Twitter Sentiment Analysis* application, we'll use Apache Spark, which
    provides excellent performances in the area of scalability, programmability, and
    speed. In addition, many cloud providers offer some flavor of Spark as a Service
    giving the ability to create on demand an appropriately sized Spark cluster in
    minutes.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有多种框架（大多数通常以开源形式提供）可以提供强大且灵活的并行计算能力。最受欢迎的一些包括Apache Hadoop ([http://hadoop.apache.org](http://hadoop.apache.org))、Apache
    Spark ([https://spark.apache.org](https://spark.apache.org)) 和 Dask ([https://dask.pydata.org](https://dask.pydata.org))。对于我们的*Twitter情感分析*应用程序，我们将使用Apache
    Spark，它在可扩展性、可编程性和速度方面表现出色。此外，许多云服务提供商提供了某种形式的Spark即服务，能够在几分钟内按需创建一个合适大小的Spark集群。
- en: 'Some Spark as a Service cloud providers include:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一些Spark即服务的云服务提供商包括：
- en: 'Microsoft Azure: [https://azure.microsoft.com/en-us/services/hdinsight/apache-spark](https://azure.microsoft.com/en-us/services/hdinsight/apache-spark)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Microsoft Azure: [https://azure.microsoft.com/en-us/services/hdinsight/apache-spark](https://azure.microsoft.com/en-us/services/hdinsight/apache-spark)'
- en: 'Amazon Web Services: [https://aws.amazon.com/emr/details/spark](https://aws.amazon.com/emr/details/spark)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊网络服务：[https://aws.amazon.com/emr/details/spark](https://aws.amazon.com/emr/details/spark)
- en: 'Google Cloud: [https://cloud.google.com/dataproc](https://cloud.google.com/dataproc)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud：[https://cloud.google.com/dataproc](https://cloud.google.com/dataproc)
- en: 'Databricks: [https://databricks.com](https://databricks.com)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks：[https://databricks.com](https://databricks.com)
- en: 'IBM Cloud: [https://www.ibm.com/cloud/analytics-engine](https://www.ibm.com/cloud/analytics-engine)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM Cloud：[https://www.ibm.com/cloud/analytics-engine](https://www.ibm.com/cloud/analytics-engine)
- en: Note
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: Apache Spark can also be easily installed on a local machine for testing
    purposes, in which case, the cluster nodes are simulated using threads.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：Apache Spark也可以轻松地在本地机器上安装用于测试，在这种情况下，集群节点通过线程进行模拟。'
- en: Apache Spark architecture
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark架构
- en: 'The following diagram shows the main components of the Apache Spark framework:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了Apache Spark框架的主要组件：
- en: '![Apache Spark architecture](img/B09699_07_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark架构](img/B09699_07_01.jpg)'
- en: Spark high level architecture
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark高层架构
- en: '**Spark SQL**: The core data structure of this component is the Spark DataFrame,
    which enables users who know the SQL language, to effortlessly work with structured
    data.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**：该组件的核心数据结构是Spark DataFrame，使得熟悉SQL语言的用户能够轻松地处理结构化数据。'
- en: '**Spark Streaming**: Module used to work with streaming data. As we''ll see
    later on, we''ll use this module and more specifically Structured Streaming (which
    was introduced in Spark 2.0) in our sample application.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**：用于处理流式数据的模块。正如我们稍后所看到的，我们将在示例应用中使用该模块，特别是Spark 2.0引入的结构化流处理（Structured
    Streaming）。'
- en: '**MLlib**: Module that provides a feature-rich machine learning library that
    works on a Spark scale.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib**：提供一个功能丰富的机器学习库，在Spark规模上运行。'
- en: '**GraphX**: Module used for performing the graph-parallel computation.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**：用于执行图并行计算的模块。'
- en: 'There are mainly two ways of working with a Spark cluster as illustrated in
    the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，主要有两种方式可以与Spark集群工作：
- en: '![Apache Spark architecture](img/B09699_07_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark架构](img/B09699_07_02.jpg)'
- en: Two ways to work with a Spark cluster
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与Spark集群工作的两种方式
- en: '**spark-submit**: Shell script used to launch Spark applications on a cluster'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**spark-submit**：用于在集群上启动Spark应用的Shell脚本'
- en: '**Notebooks**: Interactively execute code statements against a Spark cluster'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Notebooks**：与Spark集群交互式执行代码语句'
- en: 'Covering the `spark-submit` shell script is beyond the scope of this book,
    but official documentation can be found at: [https://spark.apache.org/docs/latest/submitting-applications.html](https://spark.apache.org/docs/latest/submitting-applications.html).
    For the rest of this chapter, we''ll focus on interacting with the Spark cluster
    via Jupyter Notebooks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不涵盖` spark-submit` shell脚本的内容，但可以在以下网址找到官方文档：[https://spark.apache.org/docs/latest/submitting-applications.html](https://spark.apache.org/docs/latest/submitting-applications.html)。在本章的其余部分，我们将重点介绍通过Jupyter
    Notebooks与Spark集群进行交互。
- en: Configuring Notebooks to work with Spark
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Notebooks以便与Spark一起使用
- en: The instructions in this section only cover installing Spark locally for development
    and testing. Manually installing Spark in a cluster is beyond the scope of this
    book. If a real cluster is needed, it is highly recommended to use a cloud-based
    service.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的说明仅涵盖在本地安装Spark用于开发和测试。手动在集群中安装Spark超出了本书的范围。如果需要真正的集群，强烈建议使用基于云的服务。
- en: 'By default, local Jupyter Notebooks are installed with plain Python Kernels.
    To work with Spark, users must use the following steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，本地Jupyter Notebooks会安装普通的Python内核。为了与Spark一起使用，用户必须执行以下步骤：
- en: Install Spark locally by downloading a binary distribution from [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)下载二进制分发包，安装Spark到本地。
- en: 'Generate a kernel specification in a temporary directory using the following
    command:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在临时目录中生成内核规范：
- en: '[PRE0]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: The preceding command may generate a warning message that can be
    safely ignored as long as the following message is stated:'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**注意**：上述命令可能会生成警告消息，只要显示以下信息，这些警告可以安全忽略：'
- en: '`Installed kernelspec python3 in /tmp/share/jupyter/kernels/python3`'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`已在/tmp/share/jupyter/kernels/python3中安装kernelspec python3`'
- en: 'Go to `/tmp/share/jupyter/kernels/python3`, and edit the `kernel.json` file
    to add the following key to the JSON object (replace `<<spark_root_path>>` with
    the directory path where you installed Spark and `<<py4j_version>>` with the version
    installed on your system):'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到 `/tmp/share/jupyter/kernels/python3`，编辑 `kernel.json` 文件，向 JSON 对象中添加以下键（将
    `<<spark_root_path>>` 替换为你安装 Spark 的目录路径，将 `<<py4j_version>>` 替换为你系统上安装的版本）：
- en: '[PRE1]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You may also want to customize the `display_name` key to make it unique and
    easily recognizable from the Juptyer UI. If you need to know the list of existing
    kernels, you can use the following command:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能还想自定义 `display_name` 键，以使其在 Juptyer 界面中独特且易于识别。如果你需要查看现有内核的列表，可以使用以下命令：
- en: '[PRE2]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding command will give you a list of kernel names and associated paths
    on the local filesystem. From the path, you can open the `kernel.json` file to
    access the `display_name` value. For example:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前述命令将为你提供内核名称和相关路径的列表。从路径中，你可以打开 `kernel.json` 文件，访问 `display_name` 值。例如：
- en: '[PRE3]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Install the kernel with the edited files using the following command:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装带有编辑文件的内核：
- en: '[PRE4]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: Depending on the environment, you may receive a "permission denied" error
    when running the preceding command. In this case, you may want to run the command
    with the admin privileges using `sudo` or use the `--user` switch as follows:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：根据环境不同，你可能在运行前述命令时会遇到“权限拒绝”的错误。在这种情况下，你可能需要使用管理员权限运行该命令，使用 `sudo` 或者按如下方式使用
    `--user` 开关：
- en: '`jupyter kernelspec install --user /tmp/share/jupyter/kernels/python3`'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`jupyter kernelspec install --user /tmp/share/jupyter/kernels/python3`'
- en: 'For more information about install ation options, you can use the `-h` switch.
    For example:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如需了解更多安装选项的信息，可以使用 `-h` 开关。例如：
- en: '[PRE5]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Restart the Notebook server and start using the new PySpark kernel.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重启 Notebook 服务器并开始使用新的 PySpark 内核。
- en: Fortunately, PixieDust provides an `install` script to automate the preceding
    manual steps.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，PixieDust 提供了一个 `install` 脚本来自动化前述的手动步骤。
- en: Note
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find detailed documentation for this script here:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到该脚本的详细文档：
- en: '[https://pixiedust.github.io/pixiedust/install.html](https://pixiedust.github.io/pixiedust/install.html)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://pixiedust.github.io/pixiedust/install.html](https://pixiedust.github.io/pixiedust/install.html)'
- en: 'In short, using the automated PixieDust `install` script requires the following
    command to be issued and the on-screen instructions to be followed:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用自动化 PixieDust `install` 脚本需要发出以下命令并按照屏幕上的说明操作：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We'll dive deeper into the Spark programming model later in this chapter, but
    for now, let's define in the next section, the MVP requirements for our *Twitter
    Sentiment Analysis* application.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本章稍后会深入探讨 Spark 编程模型，但现在让我们在下一节定义我们 *Twitter 情感分析* 应用的 MVP 要求。
- en: Twitter sentiment analysis application
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter 情感分析应用
- en: 'As always, we start by defining the requirements for our MVP version:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们首先定义 MVP 版本的要求：
- en: Connect to Twitter to get a stream of real-time tweets filtered by a query string
    provided by the user
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接 Twitter，获取由用户提供的查询字符串过滤的实时推文流
- en: Enrich the tweets to add sentiment information and relevant entities extracted
    from the text
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丰富推文，添加情感信息和从文本中提取的相关实体
- en: Display a dashboard with various statistics about the data using live charts that
    are updated at specified intervals
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用实时图表显示有关数据的各种统计信息，并在指定的时间间隔内更新图表
- en: The system should be able to scale up to Twitter data size
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统应该能够扩展到 Twitter 数据规模
- en: 'The following diagram shows the first version of our application architecture:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了我们应用架构的第一个版本：
- en: '![Twitter sentiment analysis application](img/B09699_07_03.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![Twitter 情感分析应用](img/B09699_07_03.jpg)'
- en: Twitter sentiment architecture version 1
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 情感分析架构版本 1
- en: For version 1, the application will be entirely implemented in a single Python
    Notebook and will call out to an external service for the NLP part. To be able
    to scale, we will certainly have to externalize some of the processing outside
    of the Notebook, but for development and testing, I found that being able to contain
    the whole application in a single Notebook significantly increases productivity.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个版本，应用将完全在一个 Python Notebook 中实现，并调用外部服务处理 NLP 部分。为了能够扩展，我们肯定需要将一些处理外部化，但对于开发和测试，我发现能够将整个应用封装在一个
    Notebook 中显著提高了生产力。
- en: As for libraries and frameworks, we'll use Tweepy ([http://www.tweepy.org](http://www.tweepy.org))
    for connecting to Twitter, Apache Spark Structured Streaming ([https://spark.apache.org/streaming](https://spark.apache.org/streaming))
    for processing the streaming data in a distributed cluster and the Watson Developer
    Cloud Python SDK ([https://github.com/watson-developer-cloud/python-sdk](https://github.com/watson-developer-cloud/python-sdk))
    to access the IBM Watson Natural Language Understanding ([https://www.ibm.com/watson/services/natural-language-understanding](https://www.ibm.com/watson/services/natural-language-understanding))
    service.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 至于库和框架，我们将使用Tweepy（[http://www.tweepy.org](http://www.tweepy.org)）连接到Twitter，使用Apache
    Spark结构化流处理（[https://spark.apache.org/streaming](https://spark.apache.org/streaming)）处理分布式集群中的流数据，使用Watson
    Developer Cloud Python SDK（[https://github.com/watson-developer-cloud/python-sdk](https://github.com/watson-developer-cloud/python-sdk)）访问IBM
    Watson自然语言理解（[https://www.ibm.com/watson/services/natural-language-understanding](https://www.ibm.com/watson/services/natural-language-understanding)）服务。
- en: Part 1 – Acquiring the data with Spark Structured Streaming
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1部分 – 使用Spark结构化流处理获取数据
- en: 'To acquire the data, we use Tweepy which provides an elegant Python client
    library to access the Twitter APIs. The APIs covered by Tweepy are very extensive
    and covering them in detail is beyond the scope of this book, but you can find
    the complete API reference at the Tweepy official website: [http://tweepy.readthedocs.io/en/v3.6.0/cursor_tutorial.html](http://tweepy.readthedocs.io/en/v3.6.0/cursor_tutorial.html).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取数据，我们使用Tweepy，它提供了一个优雅的Python客户端库来访问Twitter API。Tweepy支持的API非常广泛，详细介绍超出了本书的范围，但你可以在Tweepy官方网站找到完整的API参考：[http://tweepy.readthedocs.io/en/v3.6.0/cursor_tutorial.html](http://tweepy.readthedocs.io/en/v3.6.0/cursor_tutorial.html)。
- en: 'You can install the Tweepy library directly from PyPi using the `pip install`
    command. The following command shows how to install it from a Notebook using the
    `!` directive:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接通过PyPi安装Tweepy库，使用`pip install`命令。以下命令展示了如何通过Notebook使用`!`指令安装：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: The current Tweepy version used is 3.6.0\. Do not forget to restart
    the kernel after installing the library.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：当前使用的Tweepy版本是3.6.0。安装完库后，别忘了重启内核。'
- en: Architecture diagram for the data pipeline
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管道架构图
- en: Before we start diving into each component of the data pipeline, it would be
    good to take a look at its overall architecture and understand the computation
    flow.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解数据管道的每个组件之前，最好先了解其整体架构并理解计算流。
- en: 'As shown in the following diagram, we start by creating a Tweepy stream that
    writes raw data in CSV files. We then create a Spark Streaming DataFrame that
    reads the CSV files and is periodically updated with new data. From the Spark
    Streaming DataFrame, we create a Spark structured query using SQL and store its
    results in a Parquet database:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们首先创建一个Tweepy流，将原始数据写入CSV文件。然后，我们创建一个Spark Streaming数据框，读取CSV文件并定期更新新数据。从Spark
    Streaming数据框中，我们使用SQL创建一个Spark结构化查询，并将其结果存储在Parquet数据库中：
- en: '![Architecture diagram for the data pipeline](img/B09699_07_04.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![数据管道架构图](img/B09699_07_04.jpg)'
- en: Streaming computation flow
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 流计算流程
- en: Authentication with Twitter
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Twitter身份验证
- en: 'Before using any of the Twitter APIs, it is recommended to authenticate with
    the system. One of the most commonly used authentication mechanism is the OAuth
    2.0 protocol ([https://oauth.net](https://oauth.net)) which enables third-party
    applications to access a service on the web. The first thing you need to do is
    acquire a set of key strings that are used by the OAuth protocol to authenticate
    you:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用任何Twitter API之前，建议先进行身份验证。最常用的身份验证机制之一是OAuth 2.0协议（[https://oauth.net](https://oauth.net)），该协议使第三方应用程序能够访问网络服务。你需要做的第一件事是获取一组密钥字符串，这些字符串由OAuth协议用于对你进行身份验证：
- en: '**Consumer key**: String that uniquely identifies the client app (a.k.a. the
    API Key).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者密钥**：唯一标识客户端应用程序的字符串（即API密钥）。'
- en: '**Consumer secret**: Secret string known only to the application and the Twitter
    OAuth server. It can be thought of like a password.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者密钥**：仅应用程序和Twitter OAuth服务器知道的密钥字符串。可以将其视为密码。'
- en: '**Access token**: String used to authenticate your requests. This token is
    also used during the authorization phase to determine the level of access for
    the application.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问令牌**：用于验证请求的字符串。该令牌也在授权阶段用于确定应用程序的访问级别。'
- en: '**Access token secret**: Similar to the consumer secret, this is a secret string
    sent with the access token to be used as a password.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问令牌密钥**：与消费者密钥类似，这是与访问令牌一起发送的密码字符串，用作密码。'
- en: 'To generate the preceding key strings, you need to go to [http://apps.twitter.com](http://apps.twitter.com),
    provide authentication with your regular Twitter user ID and password and follow
    these steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成前面的密钥字符串，您需要访问[http://apps.twitter.com](http://apps.twitter.com)，使用您的常规Twitter用户ID和密码进行身份验证，并按照以下步骤操作：
- en: Create a new Twitter app using the **Create New App** button.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**创建新应用**按钮创建一个新的Twitter应用。
- en: Fill out the application details, agree to the Developer agreement and click
    on **Create your Twitter application** button.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写应用程序详情，同意开发者协议，然后点击**创建您的Twitter应用**按钮。
- en: Tip
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Note**: Make sure that your mobile phone number is added to your profile or
    you''ll get an error when creating the Twitter application.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**注意**：确保您的手机号码已添加到个人资料中，否则在创建Twitter应用时会出现错误。'
- en: You can provide a random URL for the mandatory **Website** input and leave the
    **URL** input blank as this is an optional callback URL.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以为必填项**网站**输入提供一个随机URL，并将**URL**输入留空，因为这是一个可选的回调URL。
- en: Click on the **Keys and Access Tokens** tab to get the consumer and access token.
    At any time, you can regenerate these tokens using the buttons available on this
    page. If you do so, you'll need to also update the value in your application code.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**密钥和访问令牌**标签以获取消费者和访问令牌。您可以随时使用页面上提供的按钮重新生成这些令牌。如果您这么做，您还需要在应用程序代码中更新这些值。
- en: 'For easier code maintenance, let''s put these tokens in their own variables
    at the top of the Notebook and create the `tweepy.OAuthHandler` class that we''ll
    use later on:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易进行代码维护，我们将把这些令牌放在Notebook顶部的单独变量中，并创建我们稍后将使用的`tweepy.OAuthHandler`类：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Creating the Twitter stream
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Twitter流
- en: 'For implementing our application, we only need to use the Twitter streaming
    API that is documented here: [http://tweepy.readthedocs.io/en/v3.5.0/streaming_how_to.html](http://tweepy.readthedocs.io/en/v3.5.0/streaming_how_to.html).
    In this step, we create a Twitter stream that stores the incoming data into CSV
    files on the local filesystem. This is done using a custom `RawTweetsListener`
    class that inherits from `tweepy.streaming.StreamListener`. Custom processing
    of the incoming data is done by overriding the `on_data` method.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我们的应用程序，我们只需要使用这里文档化的Twitter流API：[http://tweepy.readthedocs.io/en/v3.5.0/streaming_how_to.html](http://tweepy.readthedocs.io/en/v3.5.0/streaming_how_to.html)。在此步骤中，我们创建一个Twitter流，将传入的数据存储到本地文件系统中的CSV文件中。通过继承自`tweepy.streaming.StreamListener`的自定义`RawTweetsListener`类完成此操作。通过重写`on_data`方法来处理传入数据的自定义处理。
- en: In our case, we want to transform the incoming data from JSON to CSV using `DictWriter`
    from the standard Python `csv` module. Because the Spark Streaming file input
    source triggers only when new files are created in the input directory, we can't
    simply append the data into an existing file. Instead, we buffer the data into
    an array and write it to disk once the buffer has reached capacity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们希望使用标准Python `csv`模块中的`DictWriter`将传入的JSON数据转换为CSV格式。由于Spark Streaming文件输入源仅在输入目录中创建新文件时触发，因此我们不能简单地将数据追加到现有文件中。相反，我们将数据缓冲到一个数组中，并在缓冲区达到容量时将其写入磁盘。
- en: Note
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For simplicity, the implementation doesn't include cleaning up the files once
    they have been processed. Another minor limitation of this implementation is that
    we currently wait until the buffer is filled to write the file which theoretically
    could take a long time if no new tweets appear.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，实施中没有包括处理完文件后的清理工作。另一个小的限制是，我们目前等待缓冲区填满后再写入文件，理论上如果没有新推文出现，这可能需要很长时间。
- en: 'The code for the `RawTweetsListener` is shown here:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`RawTweetsListener`的代码如下所示：'
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode1.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode1.py)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode1.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode1.py)'
- en: 'A few important things to notice from the preceding code are:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中有几个重要的地方需要注意：
- en: 'Each tweet coming from the Twitter API contains a lot of data, and we pick which
    field to keep using the `field_metadata` variable. We also define a global variable
    `fieldnames` that holds the list of fields to capture from the stream, and a `transforms`
    variable that contains a dictionary with all the field names that have a transform
    function as a key and the transform function itself as a value:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每条来自Twitter API的推文都包含大量数据，我们使用`field_metadata`变量选择保留的字段。我们还定义了一个全局变量`fieldnames`，它保存了要从流中捕获的字段列表，以及一个`transforms`变量，它包含一个字典，字典的键是所有具有变换函数的字段名，值是变换函数本身：
- en: '[PRE10]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode2.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode2.py)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode2.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode2.py)'
- en: 'The CSV files are written in `output_dir` which is defined in its own variable.
    At start time, we first remove the directory and its contents:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV文件被写入定义在自己的变量中的`output_dir`目录。在启动时，我们首先删除该目录及其内容：
- en: '[PRE11]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode3.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode3.py)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode3.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode3.py)'
- en: The `field_metadata` contains the Spark DataType that we'll use later on to build
    the schema when creating the Spark streaming query.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`field_metadata`包含了Spark DataType，我们稍后将在创建Spark流查询时使用它来构建模式。'
- en: 'The `field_metadata` also contains an optional transform `lambda` function
    to cleanse the value before being written to disk. For reference, a lambda function
    in Python is an anonymous function defined inline (see [https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)).
    We use it for the source field that is often returned as an HTML fragment. In
    this lambda function, we use the BeautifulSoup library (which was also used in
    the previous chapter) to extract only the text as shown in the following snippet:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`field_metadata`还包含一个可选的变换`lambda`函数，用于在将值写入磁盘之前清理数据。作为参考，Python中的lambda函数是一个内联定义的匿名函数（请参见[https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)）。我们在此使用它来处理常常以HTML片段形式返回的源字段。在这个lambda函数中，我们使用了BeautifulSoup库（它也在上一章中使用过）来提取只有文本的内容，如以下代码片段所示：'
- en: '[PRE12]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that the `RawTweetsListener` is created, we define a `start_stream` function
    that we''ll use later on in the PixieApp. This function takes an array of search
    terms as input and starts a new stream using the `filter` method:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`RawTweetsListener`已经创建，我们定义了一个`start_stream`函数，稍后将在PixieApp中使用。此函数接受一个搜索词数组作为输入，并使用`filter`方法启动一个新的流：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Notice the `async=True` parameter passed to `stream.filter`. This is needed
    to make sure that the function doesn't block, which would prevent us from running
    any other code in the Notebook.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到传递给`stream.filter`的`async=True`参数。这是必要的，确保该函数不会阻塞，这样我们就可以在Notebook中运行其他代码。
- en: 'You can find the code file here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode4.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode4.py)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode4.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode4.py)'
- en: 'The following code starts the stream that will receive tweets containing the
    word `baseball` in it:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码启动了一个流，它将接收包含单词`baseball`的推文：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When running the preceding code, no output is generated in the Notebook. However,
    you can see the files (that is, `tweets0.csv`, `tweets1.csv`, and so on.) being generated
    in the output directory (that is, `../output/raw`) from the path where the Notebook
    is being run.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行上述代码时，Notebook中不会生成任何输出。然而，你可以在输出目录（即`../output/raw`）中看到生成的文件（如`tweets0.csv`、`tweets1.csv`等），这些文件位于Notebook运行的路径下。
- en: 'To stop the stream, we simply call the `disconnect` method, as shown here:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止流，我们只需调用`disconnect`方法，如下所示：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Creating a Spark Streaming DataFrame
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个 Spark Streaming DataFrame
- en: Referring to the architecture diagram, the next step is to create a Spark Streaming
    DataFrame `tweets_sdf` that uses the `output_dir` as the source file input. We
    can think of a Streaming DataFrame as an unbounded table where new rows are continuously
    added as new data arrives from the stream.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 根据架构图，下一步是创建一个 Spark Streaming DataFrame `tweets_sdf`，该 DataFrame 使用 `output_dir`
    作为源文件输入。我们可以把 Streaming DataFrame 看作一个没有边界的表格，随着新数据从流中到达，新的行会不断被添加进来。
- en: Note
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: Spark Structured Streaming supports multiple types of input source including
    File, Kafka, Socket, and Rate. (Both Socket and Rate are used only for testing.)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：Spark Structured Streaming 支持多种类型的输入源，包括文件、Kafka、Socket 和 Rate。（Socket
    和 Rate 仅用于测试。）'
- en: 'The following diagram is taken from the Spark website and does a great job explaining
    how new data is appended to the Streaming DataFrame:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表摘自 Spark 网站，能够很好地解释新数据是如何被添加到 Streaming DataFrame 中的：
- en: '![Creating a Spark Streaming DataFrame](img/B09699_07_05.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![创建 Spark Streaming DataFrame](img/B09699_07_05.jpg)'
- en: Streaming DataFrame flow
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Streaming DataFrame 流程
- en: 'Source: [https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)'
- en: The Spark Streaming Python API provides an elegant way to create the Streaming
    DataFrame using the `spark.readStream` property which creates a new `pyspark.sql.streamingreamReader`
    object that conveniently lets you chain method calls with the added benefit of
    creating clearer code (see [https://en.wikipedia.org/wiki/Method_chaining](https://en.wikipedia.org/wiki/Method_chaining)
    for more details on this pattern).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming Python API 提供了一种优雅的方式来使用 `spark.readStream` 属性创建 Streaming DataFrame，该属性会创建一个新的
    `pyspark.sql.streamingreamReader` 对象，方便你链式调用方法，并能让代码更加清晰（有关此模式的更多细节，请参见 [https://en.wikipedia.org/wiki/Method_chaining](https://en.wikipedia.org/wiki/Method_chaining)）。
- en: 'For example, to create a CSV file stream, we call the format method with `csv`,
    chain the applicable options and call the `load` method with the path of the directory:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要创建一个 CSV 文件流，我们调用 `format` 方法并传入 `csv`，接着链式调用适用的选项，并通过指定目录路径调用 `load` 方法：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此处找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode5.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode5.py)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode5.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode5.py)'
- en: '`spark.readStream` also provides a convenient high-level `csv` method that
    takes the path as the first argument and keyword arguments for the options:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.readStream` 还提供了一个方便的高阶 `csv` 方法，它将路径作为第一个参数，并为选项提供关键字参数：'
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此处找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode6.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode6.py)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode6.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode6.py)'
- en: 'You can verify that the `csv_sdf` DataFrame is indeed a Streaming DataFrame
    by calling the `isStreaming` method which should return `true`. The following
    code also adds a call to `printSchema` to verify that the schema follows the `field_metadata`
    configuration as expected:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过调用 `isStreaming` 方法来验证 `csv_sdf` DataFrame 是否真的是一个 Streaming DataFrame，返回值应为
    `true`。以下代码还添加了 `printSchema` 方法的调用，以验证 schema 是否按照 `field_metadata` 配置如预期那样：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Returns:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Before continuing to the next step, it is important to understand how the `csv_sdf`
    Streaming DataFrame fits in the Structured Streaming programming model and what
    limitations it has. At its core, the Spark low-level APIs define the **Resilient
    Distributed Dataset** (**RDD**) data structure which encapsulates all the underlying
    complexity of managing the distributed data. Features like fault-tolerance (cluster
    nodes that crashes for any reason are transparently restarted with no intervention
    from the developer) are automatically handled by the framework. There are two
    types of RDD operations: transformations and actions. **Transformations** are
    logical operations on an existing RDD that are not immediately executed on the
    cluster until an action is invoked (lazy execution). The output of a transformation
    is a new RDD. Internally, Spark maintains an RDD acyclic directed graph that keeps
    track of all the lineage resulting in the creation of the RDD, which is useful
    when recovering from server failure. Example transformations include `map`, `flatMap`,
    `filter`, `sample`, and `distinct`. The same goes for transformations on DataFrames
    (which internally are backed by RDDs) that have the benefit of including SQL queries.
    On the other hand, **actions** do not produce other RDDs, but rather perform an
    operation on the actual distributed data to return a non-RDD value. Examples of
    actions include `reduce`, `collect`, `count`, and `take`.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续下一步之前，理解`csv_sdf`流数据框如何适应结构化流编程模型及其局限性非常重要。从本质上讲，Spark的低级API定义了**弹性分布式数据集**（**RDD**）数据结构，它封装了管理分布式数据的所有底层复杂性。像容错（集群节点因任何原因崩溃时，框架会自动重启节点，无需开发者干预）等特性都由框架自动处理。RDD操作有两种类型：转换和动作。**转换**是对现有RDD的逻辑操作，直到调用动作操作时，转换才会在集群上立即执行（懒执行）。转换的输出是一个新的RDD。内部，Spark维护一个RDD有向无环图（DAG），记录所有生成RDD的血统，这在从服务器故障恢复时非常有用。常见的转换操作包括`map`、`flatMap`、`filter`、`sample`和`distinct`。对数据框的转换（数据框在内部由RDD支持）也适用，且它们具有包括SQL查询的优点。另一方面，**动作**不会生成其他RDD，而是对实际分布式数据执行操作，返回非RDD值。常见的动作操作包括`reduce`、`collect`、`count`和`take`。
- en: As mentioned before, `csv_sdf` is a Streaming DataFrame, which means that the
    data is continuously added to it and as such we are only able to apply transformations
    to it, not actions. To circumvent this problem, we must first create a streaming
    query using `csv_sdf.writeStream` which is a `pyspark.sql.streaming.DataStreamWriter`
    object. The streaming query is responsible for sending the results to an output
    sink. We can then run the streaming query using the `start()` method.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`csv_sdf`是一个流式数据框（Streaming DataFrame），这意味着数据会持续被添加到其中，因此我们只能对其应用转换，而不能执行操作。为了解决这个问题，我们必须先使用`csv_sdf.writeStream`创建一个流查询，这是一个`pyspark.sql.streaming.DataStreamWriter`对象。流查询负责将结果发送到输出接收器。然后，我们可以通过`start()`方法运行流查询。
- en: 'Spark Streaming supports multiple output sink types:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming支持多种输出接收器类型：
- en: '**File**: All the classic file formats are supported, including JSON, CSV,
    and Parquet'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件**：支持所有经典文件格式，包括JSON、CSV和Parquet'
- en: '**Kafka**: Write directly to one or more Kafka topics'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka**：直接写入一个或多个Kafka主题'
- en: '**Foreach**: Run arbitrary computations on each element in the collection'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Foreach**：对集合中的每个元素执行任意计算'
- en: '**Console**: Prints the output to the system console (used mainly for debugging)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制台**：将输出打印到系统控制台（主要用于调试）'
- en: '**Memory**: Output is stored in memory'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存**：输出存储在内存中'
- en: In the next section, we'll create and run a structured query on `csv_sdf` with
    an output sink that stores the output in Parquet format.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将创建并运行一个结构化查询，针对`csv_sdf`使用输出接收器将结果存储为Parquet格式。
- en: Creating and running a structured query
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建并运行结构化查询
- en: Using the `tweets_sdf` Streaming DataFrame, we create a streaming query `tweet_streaming_query`
    that writes the data into a Parquet format using the *append* output mode.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tweets_sdf`流数据框，我们创建一个流查询`tweet_streaming_query`，该查询将数据以*append*输出模式写入Parquet格式。
- en: Note
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: Spark streaming queries support three output modes: **complete**
    where the entire table is written at each trigger, **append** where only the delta
    rows since the last trigger are written, and **update** where only the rows that
    were modified are written.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：Spark流查询支持三种输出模式：**complete**，每次触发时写入整个表；**append**，只写入自上次触发以来的增量行；以及**update**，只写入已修改的行。'
- en: 'Parquet is a columnar database format that provides an efficient, scalable
    storage for distributed analytics. You can find more information about the Parquet
    format at: [https://parquet.apache.org](https://parquet.apache.org).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是一种列式数据库格式，提供了高效、可扩展的分布式分析存储。你可以在这里找到有关 Parquet 格式的更多信息：[https://parquet.apache.org](https://parquet.apache.org)。
- en: 'The following code creates and starts the `tweet_streaming_query` streaming
    query:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建并启动 `tweet_streaming_query` 流式查询：
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode7.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode7.py)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode7.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode7.py)'
- en: 'Similarly, you can stop the streaming query by using the `stop()` method as
    follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，你可以使用 `stop()` 方法来停止流式查询，如下所示：
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code, we use the `path` option to specify the location of the
    Parquet files, and the `checkpointLocation` to specify the location of the recovery
    data that would be used in case of a server failure. We also specify the trigger
    interval for new data to be read from the stream and new rows to be added to the
    Parquet database.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们使用 `path` 选项指定 Parquet 文件的位置，并使用 `checkpointLocation` 指定在服务器故障时用于恢复的数据位置。我们还指定了从流中读取新数据并将新行添加到
    Parquet 数据库的触发间隔。
- en: 'For testing purpose, you can also use the `console` sink to see the new rows
    being read every time a new raw CSV file is generated in the `output_dir` directory:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 出于测试目的，你也可以使用 `console` sink 来查看每次生成新原始 CSV 文件时从 `output_dir` 目录读取的新行：
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode8.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode8.py)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode8.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode8.py)'
- en: You can see the results in the system output of the master node of your Spark
    cluster (you will need to physically access the master node machine and look at
    the log files, since, unfortunately, the output is not printed into the Notebook
    itself because the operation is executed in a different process. Location of the
    log files depends on the cluster management software; please refer to the specific
    documentation for more information).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Spark 集群主节点的系统输出中查看结果（你需要物理访问主节点机器并查看日志文件，因为不幸的是，由于操作在不同的进程中执行，输出不会显示在笔记本中。日志文件的位置取决于集群管理软件；有关更多信息，请参阅具体的文档）。
- en: 'Here are sample results displayed for a particular batch (identifiers have
    been masked):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是特定批次显示的示例结果（标识符已被屏蔽）：
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Monitoring active streaming queries
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控活动流式查询
- en: 'When a streaming query is started, cluster resources are allocated by Spark.
    Therefore, it is important to manage and monitor these queries to make sure that
    you don''t run out of cluster resources. At any time, you can get a list of all
    the running queries as shown in the following code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当流式查询启动时，Spark 会分配集群资源。因此，管理和监控这些查询非常重要，以确保你不会耗尽集群资源。随时可以通过以下代码获取所有正在运行的查询列表：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Results:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '[PRE25]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can then dive into the details of each query by using the following query
    monitoring properties:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以通过使用以下查询监控属性来深入了解每个查询的细节：
- en: '`id`: Returns a unique identifier for the query that persists across restarts
    from checkpoint data'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id`：返回查询的唯一标识符，该标识符在重启时仍会保留（从检查点数据恢复）'
- en: '`runId`: Returns a unique ID generated for the current session'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runId`：返回为当前会话生成的唯一 ID'
- en: '`explain()`: Prints detailed explanations of the query'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`explain()`：打印查询的详细解释'
- en: '`recentProgress`: Returns an array of the most recent progress updates'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`recentProgress`：返回最近的进度更新数组'
- en: '`lastProgress:` Returns the most recent progress'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastProgress`：返回最新的进度'
- en: 'The following code prints the most recent progress for each active query:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码打印每个活动查询的最新进度：
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode9.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode9.py)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode9.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode9.py)'
- en: 'Results for the first query are shown here:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个查询的结果显示如下：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As an exercise for the reader, it would be useful to build a PixieApp that provides
    a live dashboard with updated details about each active streaming query.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 作为读者的练习，构建一个PixieApp，它提供一个实时仪表盘，显示每个活跃流查询的更新详情，会很有帮助。
- en: Note
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: We''ll show how to build this PixieApp in *Part 3 – Create a real-time
    dashboard PixieApp*.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：我们将在*第3部分 – 创建实时仪表盘PixieApp*中展示如何构建这个PixieApp。'
- en: Creating a batch DataFrame from the Parquet files
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Parquet文件创建批处理DataFrame
- en: Note
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: For the rest of this chapter, we define a batch Spark DataFrame as
    a classic Spark DataFrame, that is non-streaming.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：在本章的其余部分，我们将批处理Spark DataFrame定义为经典Spark DataFrame，即非流式的。'
- en: The last step of this streaming computation flow is to create one or more batch
    DataFrames that we can use for building our analytics and data visualizations.
    We can think of this last step as taking a snapshot of the data for deeper analysis.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个流计算流程的最后一步是创建一个或多个批处理DataFrame，我们可以用来构建分析和数据可视化。我们可以将这最后一步视为对数据进行快照，以便进行更深层次的分析。
- en: 'There are two ways to programmatically load a batch DataFrame from a Parquet
    file:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以通过编程方式从Parquet文件加载批处理DataFrame：
- en: 'Using `spark.read` (notice that we don''t use `spark.readStream` as we did earlier):'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`spark.read`（注意，我们不再像之前那样使用`spark.readStream`）：
- en: '[PRE28]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Using `spark.sql`:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`spark.sql`：
- en: '[PRE29]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode10.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode10.py)'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode10.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode10.py)'
- en: The benefit of this method is that we can use any ANSI SQL query to load the
    data, instead of using the equivalent low-level DataFrame APIs that we would have
    to use in the first method.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的好处是，我们可以使用任何ANSI SQL查询来加载数据，而不必像第一种方法那样使用等效的低级DataFrame API。
- en: 'We can then periodically refresh the data by rerunning the preceding code and
    recreating the DataFrame. We are now ready to create further analysis on the data
    by, for example, running the PixieDust `display()` method on it in order to create
    visualizations:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过重新运行前面的代码并重新创建DataFrame来定期刷新数据。我们现在可以为数据创建进一步的分析，例如，通过在数据上运行PixieDust的`display()`方法来生成可视化图表：
- en: '[PRE30]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We select the **Bar Chart** menu and drag and drop the `source` field in the
    **Keys** field area. Since we want to show only the top 10 tweets, we set this
    value in the **# of Rows to Display** field. The following screenshot shows the
    PixieDust options dialog:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择**条形图**菜单，并将`source`字段拖到**Keys**字段区域。由于我们只想显示前10条推文，因此我们在**要显示的行数**字段中设置这个值。下图显示了PixieDust选项对话框：
- en: '![Creating a batch DataFrame from the Parquet files](img/B09699_07_06.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![从Parquet文件创建批处理DataFrame](img/B09699_07_06.jpg)'
- en: Options dialog for showing the top 10 sources of tweets
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 显示前10个推文来源的选项对话框
- en: 'After clicking **OK**, we see the following results:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**确定**后，我们会看到以下结果：
- en: '![Creating a batch DataFrame from the Parquet files](img/B09699_07_07.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![从Parquet文件创建批处理DataFrame](img/B09699_07_07.jpg)'
- en: Chart showing the number of tweets related to baseball by source
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 展示与棒球相关的推文数量按来源分类的图表
- en: In this section, we've seen how to use the Tweepy library to create a Twitter
    stream, clean the raw data and store it in CSV files, create a Spark Streaming
    DataFrame, run streaming queries on it and store the output in a Parquet database,
    create a batch DataFrame from the Parquet file, and visualize the data using PixieDust
    `display()`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们已经展示了如何使用Tweepy库创建Twitter流，清洗原始数据并将其存储在CSV文件中，创建Spark Streaming DataFrame，在其上运行流查询并将输出存储在Parquet数据库中，从Parquet文件创建批处理DataFrame，并使用PixieDust的`display()`方法进行数据可视化。
- en: Note
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The complete notebook for *Part 1 – Acquiring the data with Spark Structured
    Streaming* can be found here:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*第1部分 – 使用Spark结构化流获取数据*的完整笔记本可以在这里找到：'
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%201.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%201.ipynb)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%201.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%201.ipynb)'
- en: In the next part, we'll look at enriching the data with sentiment and entity
    extraction using the IBM Watson Natural Language Understanding service.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将探讨如何使用IBM Watson自然语言理解服务，丰富数据中的情感分析和实体提取。
- en: Part 2 – Enriching the data with sentiment and most relevant extracted entity
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 部分 - 使用情感和最相关的提取实体丰富数据
- en: In this part, we enrich the Twitter data with sentiment information, for example,
    *positive*, *negative*, and *neutral*. We also want to extract the most relevant
    entity from the tweet, for example, sport, organization, and location. This extra
    information will be analyzed and visualized by the real-time dashboard that we'll
    build in the next section. The algorithms used to extract sentiment and entity
    from an unstructured text belong to a field of computer science and artificial
    intelligence called **natural language processing** (**NLP**). There are plenty
    of tutorials available on the web that provide algorithm examples on how to extract
    sentiment. For example, you can find a comprehensive text analytic tutorial on
    the scikit-learn repo at [https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/working_with_text_data.rst](https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/working_with_text_data.rst).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将推特数据与情感信息进行丰富处理，例如，*正面*，*负面*和*中性*。我们还希望从推文中提取出最相关的实体，例如，运动，组织和地点。这些额外的信息将通过我们在下一部分构建的实时仪表板进行分析和可视化。从非结构化文本中提取情感和实体所使用的算法属于计算机科学和人工智能领域，称为**自然语言处理**（**NLP**）。网上有许多教程提供了提取情感的算法示例。例如，您可以在
    scikit-learn 的 GitHub 仓库找到一个全面的文本分析教程，链接为[https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/working_with_text_data.rst](https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/working_with_text_data.rst)。
- en: However, for this sample application, we are not going to build our own NLP
    algorithm. Instead, we'll choose a cloud-based service that provides text analytics
    such as sentiment and entity extraction. This approach works very well when you
    have generic requirements such as do not require training custom models, but even
    then, most of the service providers now provide tooling to do so. There are major
    advantages to use a cloud-based provider over creating your own model such as
    saving on the development time and much better accuracy and performance. With
    a simple REST call, we'll be able to generate the data we need and integrate it
    into the flow of our application. Also, it would be very easy to change providers
    if needed as the code responsible for interfacing with the service is well isolated.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于这个示例应用程序，我们不会构建自己的 NLP 算法。而是选择一个提供文本分析（如情感和实体提取）的云服务。当您的需求比较通用，不需要训练自定义模型时，这种方法效果很好，尽管即便如此，许多服务提供商现在也提供了相关工具来完成此类任务。使用云服务提供商相比自己创建模型具有显著优势，比如节省开发时间、提高准确性和性能。通过简单的
    REST 调用，我们可以生成所需数据并将其集成到应用程序流程中。如果需要，切换服务提供商也非常容易，因为与服务接口的代码已经很好地隔离。
- en: For this sample application, we'll use the **IBM Watson Natural Language Understanding**
    (**NLU**) service which is a part of the IBM Watson family of cognitive services,
    and available on IBM Cloud.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例应用程序，我们将使用**IBM Watson 自然语言理解**（**NLU**）服务，它是 IBM Watson 认知服务家族的一部分，并且可以在
    IBM Cloud 上使用。
- en: Getting started with the IBM Watson Natural Language Understanding service
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用 IBM Watson 自然语言理解服务
- en: The process of provisioning a new service is usually the same for every cloud
    provider. After logging in, you go to a service catalog page where you can search for
    a particular service.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为新服务提供资源的过程对于每个云服务提供商通常都是相同的。登录后，您将进入服务目录页面，在那里可以搜索特定的服务。
- en: 'To log in to the IBM Cloud, just go to [https://console.bluemix.net](https://console.bluemix.net)
    and create a free IBM account if you don''t already have one. Once in the dashboard,
    there are multiple ways to search for the IBM Watson NLU service:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要登录到 IBM Cloud，只需访问[https://console.bluemix.net](https://console.bluemix.net)，如果还没有
    IBM 账户，可以创建一个免费的账户。进入仪表板后，有多种方式可以搜索 IBM Watson NLU 服务：
- en: Click on the top left-hand menu, and select **Watson**, select **Browse services**,
    and find the **Natural Language Understanding** entry in the list of services.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击左上角菜单，选择**Watson**，点击**浏览服务**，然后在服务列表中找到**自然语言理解**条目。
- en: Click on the **Create Resource** button in the top-right corner to get to the
    catalog. Once in the catalog, you can search for `Natural Language Understanding`
    in the search bar as shown in the following screenshot:![Getting started with
    the IBM Watson Natural Language Understanding service](img/B09699_07_08.jpg)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击右上角的**创建资源**按钮进入目录。一旦进入目录，你可以在搜索栏中搜索`Natural Language Understanding`，如以下截图所示：![开始使用
    IBM Watson 自然语言理解服务](img/B09699_07_08.jpg)
- en: Searching for Watson NLU in the service catalog
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在服务目录中搜索 Watson NLU
- en: You can then click on **Natural Language Understanding** to provision a new
    instance. It is not unusual that cloud providers offer a free or trial-based plan
    for some services and fortunately Watson NLU provides one of these, with the limitation
    that you can train only one custom model with a maximum of 30,000 NLU items processed
    per month (which is adequate for our sample application). After selecting the
    **Lite** (free) plan and clicking on the **Create** button, the newly provisioned
    instance will appear on the dashboard and is ready to accept requests.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以点击**自然语言理解**来配置一个新实例。云服务提供商通常会为一些服务提供免费的或基于试用的计划，幸运的是，Watson NLU 也提供了这样的计划，但有限制，你只能训练一个自定义模型，每月最多处理
    30,000 个 NLU 项目（对于我们的示例应用足够了）。选择**Lite**（免费）计划并点击**创建**按钮后，新配置的实例将出现在仪表盘上，并准备好接受请求。
- en: Note
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: After creating the service, you may be redirected to the NLU service *getting
    started document*. If so, simply navigate back to the dashboard where you should
    see the new service instance listed.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：创建服务后，你可能会被重定向到 NLU 服务的*入门文档*。如果是这种情况，只需返回仪表盘，应该能看到新创建的服务实例。'
- en: The next step is to test the service from our Notebook by making a REST call.
    Every service provides detailed documentation on how to use it including the API
    reference. From the Notebook, we could use the requests package to make GET, POST,
    PUT, or DELETE calls according to the API reference, but it is highly recommended
    to check whether the service offers SDKs with high-level programmatic access to
    the APIs.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是通过在笔记本中发出 REST 调用来测试服务。每个服务都会提供详细的文档，说明如何使用，包括 API 参考。在笔记本中，我们可以使用 requests
    包根据 API 参考发出 GET、POST、PUT 或 DELETE 请求，但强烈建议检查服务是否提供具有高级编程访问功能的 SDK。
- en: 'Fortunately, IBM Watson provides the `watson_developer_cloud` open source library
    which includes multiple open source SDKs supporting some of the most popular languages,
    including Java, Python, and Node.js. For this project, we''ll use the Python SDK
    with source code and code examples located here: [https://github.com/watson-developer-cloud/python-sdk](https://github.com/watson-developer-cloud/python-sdk).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，IBM Watson 提供了`watson_developer_cloud`开源库，其中包含多个支持流行编程语言（包括 Java、Python
    和 Node.js）的开源 SDK。对于本项目，我们将使用 Python SDK，源代码和示例代码可以在此找到：[https://github.com/watson-developer-cloud/python-sdk](https://github.com/watson-developer-cloud/python-sdk)。
- en: 'The following `pip` command installs the `watson_developer_cloud` package directly from
    the Jupyter Notebook:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`pip`命令直接从 Jupyter Notebook 安装`watson_developer_cloud`包：
- en: '[PRE31]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Notice the `!` in front of the command that signifies that it's a shell command.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意命令前的`!`，它表示这是一个 shell 命令。
- en: '**Note**: Don''t forget to restart the kernel once installation is complete.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：安装完成后，别忘了重新启动内核。'
- en: Most cloud service providers use a common pattern to let consumers authenticate
    with the service, which consists of generating a set of credentials from the service
    console dashboard that will be embedded in the client application. To generate
    the credentials, simply click on the **Service credentials** tab of your Watson
    NLU instance and click on the **New credential** button.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数云服务提供商使用一种通用模式，允许用户通过服务控制台仪表盘生成一组凭证，然后将其嵌入到客户端应用程序中。要生成凭证，只需点击 Watson NLU
    实例的**服务凭证**标签，然后点击**新建凭证**按钮。
- en: 'This will generate a new set of credentials in JSON format as shown in the
    following screenshot:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一组新的凭证，格式为 JSON，如下截图所示：
- en: '![Getting started with the IBM Watson Natural Language Understanding service](img/B09699_07_09.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![开始使用 IBM Watson 自然语言理解服务](img/B09699_07_09.jpg)'
- en: Generating new credentials for the Watson NLU service
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Watson NLU 服务生成新凭证
- en: 'Now that we have the credentials to our service, we can create a `NaturalLanguageUnderstandingV1`
    object that will provide programmatic access to the REST APIs, as shown in the
    following code:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了服务的凭据，我们可以创建一个 `NaturalLanguageUnderstandingV1` 对象，它将提供对 REST API 的编程访问，如下所示的代码所示：
- en: '[PRE32]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode11.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode11.py)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode11.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode11.py)'
- en: '**Note**: In the preceding code, replace the `XXXX` text with the appropriate
    username and password from the service credentials.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：在前面的代码中，将 `XXXX` 文本替换为服务凭据中的适当用户名和密码。'
- en: 'The `version` argument refers to a specific version of the API. To know the
    latest version, go to the official documentation page located here:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`version` 参数指的是 API 的特定版本。要了解最新版本，请访问此处的官方文档页面：'
- en: '[https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1](https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1](https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1)'
- en: 'Before continuing with building the application, let''s take a moment to understand
    the text analytics capabilities offered by the Watson Natural Language service
    which include:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续构建应用程序之前，让我们花点时间了解 Watson 自然语言服务所提供的文本分析功能，包括：
- en: Sentiment
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感
- en: Entities
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体
- en: Concepts
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念
- en: Categories
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别
- en: Emotion
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感
- en: Keywords
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键词
- en: Relations
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系
- en: Semantic roles
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义角色
- en: 'In our application, enriching the Twitter data happens in the `RawTweetsListener`
    where we create an `enrich` method that will be invoked from the `on_data` handler
    method. In this method, we call the `nlu.analyze` method with the Twitter data
    and a feature list that includes sentiment and entities only as shown in the following
    code:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的应用程序中，Twitter 数据的丰富化发生在 `RawTweetsListener` 中，我们在其中创建了一个 `enrich` 方法，该方法将从
    `on_data` 处理程序方法中调用。在这个方法中，我们使用 Twitter 数据和仅包含情感和实体的特征列表调用 `nlu.analyze` 方法，如下所示的代码所示：
- en: Note
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: The `[[RawTweetsListener]]` notation means that the following code
    is part of a class called `RawTweetsListener` and that the user should not attempt
    to run the code as is without the complete class. As always, you can always refer
    to the complete notebook for reference.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：`[[RawTweetsListener]]` 符号表示以下代码是一个名为 `RawTweetsListener` 的类的一部分，用户不应尝试在没有完整类的情况下直接运行代码。像往常一样，您可以参考完整的笔记本进行查看。'
- en: '[PRE33]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode12.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode12.py)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode12.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode12.py)'
- en: The results are then stored in the `data` object which will be written to the
    CSV files. We also guard against unexpected exceptions skipping the current tweet
    and logging a warning message instead of letting the exception bubble up which
    would stop the Twitter stream.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将存储在 `data` 对象中，随后会写入 CSV 文件。我们还会防范意外异常，跳过当前推文并记录警告信息，而不是让异常冒泡，从而停止 Twitter
    流。
- en: Note
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: The most common exception happens when the tweet data is in a language
    that is not supported by the service.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：最常见的异常发生在推文数据使用该服务不支持的语言时。'
- en: We use the `@Logger` decorator described in [Chapter 5](ch05.xhtml "Chapter 5. Python
    and PixieDust Best Practices and Advanced Concepts"), *Python and PixieDust Best
    Practices and Advanced Concepts* to log messages against the PixieDust logging
    framework. As a reminder, you can use the `%pixiedustLog` magic from another cell
    to view the log messages.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用在[第 5 章](ch05.xhtml "第 5 章. Python 和 PixieDust 最佳实践与高级概念")中描述的 `@Logger`
    装饰器，*Python 和 PixieDust 最佳实践与高级概念*，通过 PixieDust 日志框架记录日志消息。提醒一下，您可以使用来自另一个单元的
    `%pixiedustLog` 魔法命令来查看日志消息。
- en: 'We still need to change the schema metadata to include the new fields as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要更改模式元数据以包括新的字段，如下所示：
- en: '[PRE34]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode13.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode13.py)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode13.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode13.py)'
- en: 'Finally, we update `on_data` handler to invoke the `enrich` method as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们更新`on_data`处理程序以调用`enrich`方法，如下所示：
- en: '[PRE35]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode14.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode14.py)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode14.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode14.py)'
- en: 'When we restart the Twitter stream and create the Spark Streaming DataFrame,
    we can verify that we have the correct schema using the following code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们重新启动Twitter流并创建Spark Streaming DataFrame时，我们可以通过以下代码验证我们是否有正确的模式：
- en: '[PRE36]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode15.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode15.py)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode15.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode15.py)'
- en: 'Which shows the following results as expected:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示如下结果，如预期：
- en: '[PRE37]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Similarly, when we run the structured query with the `console` sink, data is
    displayed in batches in the console of the Spark master node as shown here:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当我们使用`console`接收器运行结构化查询时，数据将按批次显示在Spark主节点的控制台中，如下所示：
- en: '[PRE38]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, we run the structured query with the Parquet `output` sink, create
    a batch DataFrame, and explore the data using the PixieDust `display()` to show,
    for example, a count of tweets by sentiment (`positive`, `negative`, `neutral`)
    clustered by the entity as shown in the following chart:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用Parquet的`output`接收器运行结构化查询，创建一个批量DataFrame，并使用PixieDust的`display()`探索数据，展示例如按情感（`positive`，`negative`，`neutral`）聚类的推文计数，如下图所示：
- en: '![Getting started with the IBM Watson Natural Language Understanding service](img/B09699_07_10.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![开始使用IBM Watson自然语言理解服务](img/B09699_07_10.jpg)'
- en: Bar chart showing the number of tweets by sentiment clustered by entities
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 显示按情感分类的推文数的条形图，按实体聚类
- en: Note
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The complete notebook for *Part 2 – Enrich the data with sentiment and most relevant
    extracted entity* is located here:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的笔记本《*第二部分——通过情感和最相关的提取实体丰富数据*》位于此处：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%202.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%202.ipynb)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%202.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%202.ipynb)'
- en: If you are running it, I encourage you to experiment by adding more fields to
    the schema, run different SQL queries, and visualize the data with PixieDust `display()`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在运行它，我鼓励你通过向模式添加更多字段、运行不同的SQL查询，并使用PixieDust的`display()`来可视化数据进行实验。
- en: In the next section, we'll build a dashboard that displays multiple metrics
    about the Twitter data.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将构建一个展示Twitter数据多个指标的仪表盘。
- en: Part 3 – Creating a real-time dashboard PixieApp
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分——创建实时仪表盘PixieApp
- en: 'As always, we first need to define the requirements for the MVP version of
    the dashboard. This time we''ll borrow a tool from the agile methodology called
    a **user story** which describes the features we want to build from the perspective
    of the user. The agile methodology also prescribes fully understanding the context
    of the different users that will interact with the software by categorizing them
    into personas. In our case, we will only use one persona: *Frank the marketing
    director who wants to get real-time insights from what consumers are talking about
    on social media*.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我们首先需要定义MVP版本仪表盘的需求。这次我们将借用敏捷方法中的一个工具，称为**用户故事**，它从用户的角度描述我们希望构建的功能。敏捷方法还要求我们通过将不同的用户分类为角色，充分理解与软件互动的用户的背景。在我们的案例中，我们只使用一个角色：*Frank，市场营销总监，想要实时了解消费者在社交媒体上讨论的内容*。
- en: 'The user story goes like this:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 用户故事是这样的：
- en: Frank enters a search query like for example a product name
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frank输入类似产品名称的搜索查询
- en: A dashboard is then presented that displays a set of charts showing metrics about
    user sentiments (positive, negative, neutral)
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，展示一个仪表板，显示一组图表，展示有关用户情绪（正面、负面、中立）的度量
- en: The dashboard also contains a word cloud of all the entities being uttered in the
    tweets
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仪表板还包含一个展示所有在推文中提到的实体的词云
- en: Additionally, the dashboard has an option to display the real-time progress
    of all the Spark Streaming queries that are currently active
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，仪表板还提供了一个选项，可以显示当前所有活跃的Spark Streaming查询的实时进度
- en: Note
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: The last feature is not really needed for Frank, but we show it here
    anyway as an example implementation of the exercise given earlier.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：最后一个功能对于Frank来说并不是必需的，但我们还是在这里展示它，作为之前练习的示例实现。'
- en: Refactoring the analytics into their own methods
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将分析功能重构为独立的方法
- en: Before we start, we need to refactor the code that starts the Twitter stream
    and creates the Spark Streaming DataFrame into their own method that we will invoke
    in the PixieApp.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要将启动Twitter流和创建Spark Streaming数据框的代码重构为独立的方法，并在PixieApp中调用这些方法。
- en: 'The `start_stream,` `start_streaming_dataframe`, and `start_parquet_streaming_query`
    methods are as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`start_stream,` `start_streaming_dataframe` 和 `start_parquet_streaming_query`
    方法如下：'
- en: '[PRE39]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode16.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode16.py)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode16.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode16.py)'
- en: '[PRE40]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode17.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode17.py)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode17.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode17.py)'
- en: '[PRE41]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode18.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode18.py)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode18.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode18.py)'
- en: As part of the preparation work, we also need to manage the life cycle of the
    different streams that will be created by the PixieApp and make sure that the
    underlying resources are correctly stopped when the user restarts the dashboard.
    To help with that, we create a `StreamsManager` class that encapsulates the Tweepy
    `twitter_stream` and the CSV Streaming DataFrame. This class has a `reset` method
    that will stop the `twitter_stream`, stop all the active streaming queries, delete
    all the output files created from the previous queries, and start a new one with
    a new query string. If the `reset` method is called without a query string, then
    we don't start new streams.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 作为准备工作的一部分，我们还需要管理PixieApp将要创建的不同流的生命周期，并确保在用户重新启动仪表板时，底层资源被正确停止。为此，我们创建了一个`StreamsManager`类，封装了Tweepy的`twitter_stream`和CSV流数据框。这个类有一个`reset`方法，它会停止`twitter_stream`，停止所有活动的流查询，删除先前查询创建的所有输出文件，并使用新的查询字符串启动一个新的流。如果`reset`方法在没有查询字符串的情况下被调用，我们将不会启动新的流。
- en: We also create a global `streams_manager` instance that will keep track of the
    current state even if the dashboard is restarted. Since the user can rerun the
    cell that contains the global `streams_manager` we need to make sure that the
    `reset` method is automatically invoked when the current global instance is deleted.
    For that, we override the object's `__del__` method, which is Python's way of
    implementing a destructor and call `reset`.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了一个全局的`streams_manager`实例，它将跟踪当前状态，即使仪表板被重新启动。由于用户可以重新运行包含全局`streams_manager`的单元，我们需要确保在当前全局实例被删除时，`reset`方法会自动调用。为此，我们重写了对象的`__del__`方法，这是Python实现析构函数的一种方式，并调用`reset`。
- en: 'The code for `StreamsManager` is shown here:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamsManager`的代码如下：'
- en: '[PRE42]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode19.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode19.py)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode19.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode19.py)'
- en: Creating the PixieApp
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建PixieApp
- en: 'Like in [Chapter 6](ch06.xhtml "Chapter 6. Analytics Study: AI and Image Recognition
    with TensorFlow"), *Analytics Study: AI and Image Recognition with TensorFlow*,
    we''ll use the `TemplateTabbedApp` class again to create a tab layout with two
    PixieApps:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在[第 6 章](ch06.xhtml "第 6 章 分析研究：TensorFlow 下的 AI 和图像识别")，*分析研究：TensorFlow
    下的 AI 和图像识别*，我们再次使用 `TemplateTabbedApp` 类来创建一个包含两个 PixieApp 的标签布局：
- en: '`TweetInsightApp`: Lets the user specify a query string and shows the real-time
    dashboard associated with it'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TweetInsightApp`：允许用户指定查询字符串并显示与之关联的实时仪表盘'
- en: '`StreamingQueriesApp`: Monitors the progress of the active structured queries'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamingQueriesApp`：监控活动结构化查询的进度'
- en: 'In the default route of the `TweetInsightApp`, we return a fragment that asks
    the user for the query string as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `TweetInsightApp` 的默认路由中，我们返回一个片段，提示用户输入查询字符串，如下所示：
- en: '[PRE43]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode20.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode20.py)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode20.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode20.py)'
- en: 'The following screenshot shows the results of running the preceding code:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了运行上述代码后的结果：
- en: Note
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: We''ll create the main `TwitterSentimentApp` PixieApp that has the
    tabbed layout and includes this class later on in this section. For now, we are
    only showing the `TweetInsightApp` child app in isolation.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：稍后我们会创建主 `TwitterSentimentApp` PixieApp，它具有标签布局，并包含此类。在此之前，我们只展示 `TweetInsightApp`
    子应用程序的独立功能。'
- en: '![Creating the PixieApp](img/B09699_07_11.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![创建 PixieApp](img/B09699_07_11.jpg)'
- en: Welcome screen for the Twitter Sentiment Dashboard
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 情感仪表盘的欢迎界面
- en: 'In the `Go` button, we invoke the `search_query` route with the query string
    provided by the user. In this route, we first start the various streams and create
    a batch DataFrame stored in a class variable called `parquet_df` from the output
    directory where the Parquet database is located. We then return the HTML fragment
    that is composed of three widgets showing the following metrics:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Go` 按钮中，我们通过用户提供的查询字符串调用 `search_query` 路由。在这个路由中，我们首先启动各种流并创建一个批量数据框，该数据框从
    Parquet 数据库所在的输出目录中存储为一个类变量，命名为 `parquet_df`。然后，我们返回由三个小部件组成的 HTML 片段，展示以下指标：
- en: Bar chart for each of the three sentiments clustered by entities
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照实体分组的三种情感的柱状图
- en: Line chart subplots showing the distribution of the tweets by sentiment
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示推文情感分布的折线图子图
- en: A word cloud for the entities
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于实体的词云
- en: Each of the widgets is calling a specific route at a regular interval using
    the `pd_refresh_rate` attribute documented in [Chapter 5](ch05.xhtml "Chapter 5. Python
    and PixieDust Best Practices and Advanced Concepts"), *Python and PixieDust Best
    Practices and Advanced Concepts*. We also make sure to reload the `parquet_df`
    variable to pick up the new data that has arrived since the last time. This variable
    is then referenced in the `pd_entity` attribute for displaying the chart.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 每个小部件都在使用 `pd_refresh_rate` 属性定期调用特定的路由，相关文档可以参考[第 5 章](ch05.xhtml "第 5 章 Python
    和 PixieDust 最佳实践与高级概念")，*Python 和 PixieDust 最佳实践与高级概念*。我们还确保重新加载 `parquet_df`
    变量，以获取自上次加载以来到达的新数据。该变量随后在 `pd_entity` 属性中引用，用于显示图表。
- en: 'The following code shows the implementation for the `search_query` route:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了 `search_query` 路由的实现：
- en: '[PRE44]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode21.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode21.py)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode21.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode21.py)'
- en: 'There are multiple things to notice from the preceding code:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述代码中有多个需要注意的地方：
- en: The output directory for the Parquet files may not be ready when we try to load
    the `parquet_df` batch DataFrame, which would cause an exception. To solve this
    timing issue, we wrap the code into a `try...except` statement and wait for 5
    seconds using `time.sleep(5)`.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们尝试加载 `parquet_df` 批数据框时，Parquet 文件的输出目录可能尚未准备好，这会导致异常。为了解决这个时序问题，我们将代码包裹在
    `try...except` 语句中，并使用 `time.sleep(5)` 等待 5 秒钟。
- en: 'We also display the current count of tweets in the header. To do this we add
    a `<div>` element that refreshes every 5 seconds, with a `<pd_script>` that prints
    the current count of tweets using `streams_manager.twitter_stream.listener.tweet_count`
    which is a variable we added to the `RawTweetsListener` class. We also updated
    the `on_data()` method to increment the `tweet_count` variable every time a new
    tweet arrives as shown in the following code:'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还在页头显示当前推文的数量。为此，我们添加了一个每5秒刷新一次的`<div>`元素，并且在该元素中使用 `<pd_script>` 来打印当前的推文数量，使用
    `streams_manager.twitter_stream.listener.tweet_count` 变量，它是我们在 `RawTweetsListener`
    类中添加的变量。我们还更新了 `on_data()` 方法，以便每次新推文到达时增加 `tweet_count` 变量，以下代码展示了这一过程：
- en: '[PRE45]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Also, to avoid flickering, we prevent the displaying of the *loading spinner*
    image using `class="no_loading_msg"` in the `<div>` element.
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同时，为了避免闪烁，我们通过在 `<div>` 元素中使用 `class="no_loading_msg"` 来阻止显示 *加载旋转图标* 图像。
- en: We invoke three different routes (`display_metric1`, `display_metric2`, and
    `display_wc`) that are responsible for displaying the three widgets respectively.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们调用了三个不同的路由（`display_metric1`，`display_metric2` 和 `display_wc`），它们分别负责显示三个小部件。
- en: The `display_metric1` and `display_metric2` routes are very similar. They return
    a div with `parquet_df` as the `pd_entity` and a custom `<pd_options>` child element
    that contains the JSON configuration passed to the PixieDust `display()` layer.
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`display_metric1` 和 `display_metric2` 路由非常相似。它们返回一个包含`parquet_df`作为`pd_entity`的`div`，以及一个自定义的
    `<pd_options>` 子元素，该元素包含传递给 PixieDust `display()` 层的 JSON 配置。'
- en: 'The following code shows the implementation for the `display_metric1` route:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了 `display_metric1` 路由的实现：
- en: '[PRE46]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode22.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode22.py)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode22.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode22.py)'
- en: The `display_metric2` route follows a similar pattern but with a different set
    of `pd_options` attributes.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '`display_metric2` 路由遵循类似的模式，但使用了不同的一组 `pd_options` 属性。'
- en: 'The last route is `display_wc` and is responsible for displaying the word cloud
    for the entities. This route uses the `wordcloud` Python library that you can
    install with the following command:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一条路由是 `display_wc`，负责显示实体的词云。该路由使用 `wordcloud` Python 库，你可以通过以下命令安装它：
- en: '[PRE47]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Note
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: As always, don''t forget to restart the kernel once installation
    is complete.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：一如既往，安装完成后不要忘记重启内核。'
- en: 'We use the `@captureOutput` decorator documented in [Chapter 5](ch05.xhtml
    "Chapter 5. Python and PixieDust Best Practices and Advanced Concepts"), *Python
    and PixieDust Best Practices and Advanced Concepts* as shown here:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了在[第5章](ch05.xhtml "第5章. Python 和 PixieDust 最佳实践与高级概念")中记录的 `@captureOutput`
    装饰器，*Python 和 PixieDust 最佳实践与高级概念*，如以下所示：
- en: '[PRE48]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Note
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode23.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode23.py)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode23.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode23.py)'
- en: The text passed to the `WordCloud` class is generated from collecting all the
    entities in the `parquet_df` batch DataFrame.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 `WordCloud` 类的文本是通过收集 `parquet_df` 批处理 DataFrame 中的所有实体生成的。
- en: 'The following screenshot shows the dashboard after letting a Twitter stream,
    created with the search query `baseball`, run for a little while:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在使用搜索查询 `baseball` 创建的 Twitter 流运行一段时间后的仪表盘：
- en: '![Creating the PixieApp](img/B09699_07_12.jpg)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![创建 PixieApp](img/B09699_07_12.jpg)'
- en: Twitter Sentiment Dashboard for the search query "baseball"
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 用于搜索查询“baseball”的 Twitter 情感仪表盘
- en: 'The second PixieApp is used to monitor the streaming queries that are actively
    running. The main route returns an HTML fragment that has a `<div>` element that invokes
    the `show_progress` route at regular intervals (5000 ms) as shown in the following
    code:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个 PixieApp 用于监控正在积极运行的流查询。主路由返回一个 HTML 片段，该片段包含一个 `<div>` 元素，该元素定期（每5000毫秒）调用
    `show_progress` 路由，如以下代码所示：
- en: '[PRE49]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Note
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode24.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode24.py)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode24.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode24.py)'
- en: 'In the `show_progress` route we use the `query.lastProgress` monitoring API described
    earlier in this chapter, iterate over the JSON object using Jinja2 `{%for%}` loop
    and display the results in a table as shown in the following code:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在`show_progress`路由中，我们使用了本章之前描述的`query.lastProgress`监控 API，通过 Jinja2 `{%for%}`
    循环遍历 JSON 对象，并如以下代码所示在表格中显示结果：
- en: '[PRE50]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode25.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode25.py)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode25.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode25.py)'
- en: 'The following screenshot shows the streaming query monitoring PixieApp:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了 PixieApp 的流查询监控：
- en: '![Creating the PixieApp](img/B09699_07_13.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![创建 PixieApp](img/B09699_07_13.jpg)'
- en: Live monitoring of the active Spark streaming queries
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 实时监控活动的 Spark 流查询
- en: 'The last step is to put together the complete application using the `TemplateTabbedApp`
    class as shown in the following code:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用`TemplateTabbedApp`类来集成完整的应用程序，如下所示的代码：
- en: '[PRE51]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Note
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode26.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode26.py)'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode26.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode26.py)'
- en: 'Part 3 of our sample application is now complete; you can find the fully-built
    Notebook here:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例应用程序第三部分现已完成；您可以在这里找到完整的 Notebook：
- en: Note
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%203.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%203.ipynb)'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%203.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%203.ipynb)'
- en: In the next section, we discuss ways to make the data pipeline of our application
    more scalable by using Apache Kafka for event streaming and IBM Streams Designer for
    data enrichment of the streaming data.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论如何通过使用 Apache Kafka 进行事件流处理和 IBM Streams Designer 对流数据进行数据增强来使应用程序的数据管道更加可扩展。
- en: Part 4 – Adding scalability with Apache Kafka and IBM Streams Designer
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分 – 使用 Apache Kafka 和 IBM Streams Designer 增加可扩展性
- en: Note
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: This section is optional. It demonstrates how to re-implement parts
    of the data pipeline with cloud-based streaming services to achieve greater scalability'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本节为可选部分。它演示了如何通过使用基于云的流服务重新实现数据管道的部分，以实现更大的可扩展性。'
- en: Implementing the entire data pipeline in a single Notebook gave us high productivity
    during development and testing. We can experiment with the code and test the changes
    very rapidly with a very small footprint. Also, performances have been reasonable
    because we have been working with a relatively small amount of data. However,
    it is quite obvious that we wouldn't use this architecture in production and the
    next question we need to ask ourselves is where are the bottlenecks that would
    prevent the application from scaling as the quantity of streaming data coming
    from Twitter increases dramatically.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个 Notebook 中实现整个数据管道使我们在开发和测试过程中具有很高的生产力。我们可以快速实验代码并测试更改，且占用的资源非常小。由于我们使用的是相对较小的数据量，性能也很合理。然而，显然我们不会在生产环境中使用这种架构，接下来我们需要问自己的是，随着来自
    Twitter 的流数据量急剧增加，哪些瓶颈会阻碍应用程序的扩展。
- en: 'In this section, we identify two areas for improvement:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们确定了两个改进的方向：
- en: In the Tweepy stream, the incoming data is sent to the `RawTweetsListener` instance
    for processing using the `on_data` method. We need to make sure to spend as little
    time as possible in this method otherwise the system will fall behind as the amount
    of incoming data increases. In the current implementation, the data is enriched
    synchronously by making an external call to the Watson NLU service; it is then
    buffered and eventually written to disk. To fix this issue, we send the data to
    a Kafka service, which is a highly scalable, fault tolerant streaming platform
    using a publish/subscribe pattern for processing a high volume of data. We also
    use the Streaming Analytics service, which will consume data from Kafka and enrich
    it by invoking the Watson NLU service. Both services are available on the IBM
    Cloud.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Tweepy 流中，传入的数据会通过 `on_data` 方法发送到 `RawTweetsListener` 实例进行处理。我们需要确保在此方法中尽量减少时间消耗，否则随着传入数据量的增加，系统将会落后。在当前的实现中，数据是通过外部调用
    Watson NLU 服务同步丰富的，然后将数据缓冲，最终写入磁盘。为了解决这个问题，我们将数据发送到 Kafka 服务，这是一个高可扩展性、容错的流平台，使用发布/订阅模式来处理大量数据。我们还使用了
    Streaming Analytics 服务，它将从 Kafka 消费数据并通过调用 Watson NLU 服务来丰富数据。两个服务都可以在 IBM Cloud
    上使用。
- en: Note
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: There are alternative open source frameworks that we could have used
    for processing the streaming data, such as, for example, Apache Flink ([https://flink.apache.org](https://flink.apache.org))
    or Apache Storm ([http://storm.apache.org](http://storm.apache.org)).'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**注意**：我们可以使用其他开源框架来处理流数据，例如 Apache Flink（[https://flink.apache.org](https://flink.apache.org)）或
    Apache Storm（[http://storm.apache.org](http://storm.apache.org)）。'
- en: In the current implementation, the data is stored as CSV files, and we create a
    Spark Streaming DataFrame with the output directory as the source. This step consumes
    time and resources on the Notebook and the local environment. Instead, we can
    have the Streaming Analytics write back the enriched events in a different topic
    and create a Spark Streaming DataFrame with the Message Hub service as the Kafka
    input source.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在当前实现中，数据以 CSV 文件形式存储，我们使用输出目录作为源创建一个 Spark Streaming DataFrame。这个步骤会消耗 Notebook
    和本地环境的时间和资源。相反，我们可以让 Streaming Analytics 将丰富后的事件写回到不同的主题，并创建一个以 Message Hub 服务作为
    Kafka 输入源的 Spark Streaming DataFrame。
- en: 'The following diagram shows the updated architecture for our sample application:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们示例应用程序的更新架构：
- en: '![Part 4 – Adding scalability with Apache Kafka and IBM Streams Designer](img/B09699_07_14.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![Part 4 – 使用 Apache Kafka 和 IBM Streams Designer 添加可扩展性](img/B09699_07_14.jpg)'
- en: Scaling the architecture with Kafka and Streams Designer
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Kafka 和 Streams Designer 扩展架构
- en: In the next few sections, we will implement the updated architecture, starting
    with streaming the tweets to Kafka.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个部分中，我们将实现更新后的架构，首先将推文流式传输到 Kafka。
- en: Streaming the raw tweets to Kafka
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将原始推文流式传输到 Kafka
- en: 'Provisioning a Kafka / Message Hub service instance on IBM Cloud follows the
    same pattern as the steps we used to provision the Watson NLU service. We first
    locate and select the service in the catalog, pick a pricing plan and click **Create**.
    We then open the service dashboard and select the **Service credentials** tab
    to create new credentials as shown in the following screenshot:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在 IBM Cloud 上配置 Kafka / Message Hub 服务实例的过程与我们配置 Watson NLU 服务时的步骤相同。首先，我们在目录中找到并选择该服务，选择定价计划后点击
    **创建**。然后，我们打开服务仪表板，选择 **服务凭证** 标签以创建新的凭证，如下图所示：
- en: '![Streaming the raw tweets to Kafka](img/B09699_07_15.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![将原始推文流式传输到 Kafka](img/B09699_07_15.jpg)'
- en: Creating new credentials for the Message Hub service
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Message Hub 服务创建新的凭证
- en: 'As is the case for all the services available on IBM Cloud, the credentials
    come in the form of a JSON object that we''ll need to store in its own variable
    in the Notebook as shown in the following code (again, don''t forget to replace
    the `XXXX` text with your username and password from the service credentials):'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 与 IBM Cloud 上的所有服务一样，凭证以 JSON 对象的形式提供，我们需要将其存储在 Notebook 中的一个变量里，代码如下所示（同样，别忘了将
    `XXXX` 替换为您的用户名和服务凭证中的密码）：
- en: '[PRE52]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode27.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode27.py)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode27.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode27.py)'
- en: As for interfacing with Kafka, we have a choice between multiple good client
    libraries. I have tried many of them, but the one I ended up using most often
    is `kafka-python` ([https://github.com/dpkp/kafka-python](https://github.com/dpkp/kafka-python))
    which has the advantage of being a pure Python implementation and is thereby easier
    to install.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 关于与Kafka的接口，我们可以选择多个优秀的客户端库。我尝试了很多，但最终我使用得最多的是`kafka-python`（[https://github.com/dpkp/kafka-python](https://github.com/dpkp/kafka-python)），它的优势是纯Python实现，因此更容易安装。
- en: 'To install it from the Notebook, use the following command:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Notebook安装它，请使用以下命令：
- en: '[PRE53]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: '**Note**: As always, do not forget to restart the kernel after installing any
    libraries.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '**注**：像往常一样，在安装任何库之后，不要忘记重启内核。'
- en: The `kafka-python` library provides a `KafkaProducer` class for writing the
    data as messages into the service, which we'll need to configure with the credentials
    we created earlier. There are multiple Kafka configuration options available and
    going over all of them is beyond the scope of this book. The required options
    are related to authentication, host servers, and API version.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '`kafka-python`库提供了一个`KafkaProducer`类，用于将数据作为消息写入服务，我们需要用之前创建的凭证来配置它。Kafka有多个配置选项，涵盖所有选项超出了本书的范围。所需的选项与身份验证、主机服务器和API版本相关。'
- en: 'The following code is implemented in the `__init__` constructor of `RawTweetsListener`
    class. It creates a `KafkaProducer` instance and stores it as a class variable:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现了`RawTweetsListener`类的`__init__`构造函数。它创建了一个`KafkaProducer`实例并将其存储为类变量：
- en: '[PRE54]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Note
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: 'You can find the code file here:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode28.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode28.py)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode28.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode28.py)'
- en: We configure a lambda function for the `value_serializer` key that serializes
    JSON objects which is the format we'll be using for our data.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为`value_serializer`键配置了一个lambda函数，用于序列化JSON对象，这是我们将用于数据的格式。
- en: Note
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: '**Note**: We need to specify the `api_version` key because otherwise, the library
    would try to autodiscover its value which would cause a `NoBrokerAvailable` exception
    to be raised due to a bug in the `kafka-python` library reproducible only on Macs.
    A fix for this bug has not yet been provided at the time of writing this book.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '**注**：我们需要指定`api_version`键，否则库会尝试自动发现其值，这会导致由于`kafka-python`库中的一个bug（只在Mac上可复现）引发`NoBrokerAvailable`异常。编写本书时，尚未提供该bug的修复。'
- en: 'We now need to update the `on_data` method to send the tweets data to Kafka
    using the `tweets` topic. A Kafka topic is like a channel that applications can
    publish or subscribe to. It is important to have the topic already created before
    attempting to write into it otherwise an exception will be raised. This is done
    in the following `ensure_topic_exists` method:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要更新`on_data`方法，通过使用`tweets`主题将推文数据发送到Kafka。Kafka主题就像一个频道，应用程序可以发布或订阅它。在尝试向主题写入之前，确保该主题已经创建，否则会引发异常。此操作在以下`ensure_topic_exists`方法中完成：
- en: '[PRE55]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Note
  id: totrans-452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: 'You can find the code file here:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode29.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode29.py)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode29.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode29.py)'
- en: In the preceding code, we make a POST request into the path `/admin/topic` with
    a JSON payload that contains the name of the topic we want to create. The request
    must be authenticated using the API key provided in the credentials and the `X-Auth-Token`
    header. We also make sure to ignore HTTP error codes 422 and 403 which indicate
    that the topic already exists.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们向路径`/admin/topic`发出了一个POST请求，载荷为包含我们想要创建的主题名称的JSON数据。请求必须使用凭证中提供的API密钥和`X-Auth-Token`头进行身份验证。我们还确保忽略HTTP错误码422和403，它们表示该主题已经存在。
- en: 'The code for the `on_data` method now looks much simpler as shown here:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '`on_data`方法的代码现在看起来简单得多，如下所示：'
- en: '[PRE56]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Note
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: 'You can find the code file here:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode30.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode30.py)'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode30.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode30.py)'
- en: As we can see, with this new code, we're spending as little time as possible
    in the `on_data` method, which is the goal we wanted to achieve. The tweet data
    is now flowing into the Kafka `tweets` topic, ready to be enriched by the Streaming
    Analytics service which we'll discuss in the next section.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，通过这段新代码，我们在`on_data`方法中所花费的时间最少，这是我们想要实现的目标。推文数据现在正在流入Kafka的`tweets`主题，准备通过我们将在下一节讨论的流式分析服务进行丰富化。
- en: Enriching the tweets data with the Streaming Analytics service
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用流式分析服务丰富推文数据
- en: For this step, we'll need to use Watson Studio which is an integrated cloud-based
    IDE that provides various tools for working with data, including machine learning
    / deep learning models, Jupyter Notebooks, stream flows, and more. Watson Studio
    is a companion tool to IBM Cloud accessible at [https://datascience.ibm.com](https://datascience.ibm.com),
    and therefore no extra sign up is required.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们需要使用Watson Studio，这是一个集成的基于云的IDE，提供多种数据处理工具，包括机器学习/深度学习模型、Jupyter Notebooks、流式数据流等。Watson
    Studio是IBM Cloud的一个配套工具，可以通过[https://datascience.ibm.com](https://datascience.ibm.com)访问，因此无需额外注册。
- en: Once logged in to Watson Studio, we create a new project which we'll call `Thoughtful
    Data Science`.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 登录到Watson Studio后，我们创建一个新的项目，命名为`Thoughtful Data Science`。
- en: Note
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: It is OK to select the default options when creating a project.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：创建项目时，选择默认选项是可以的。'
- en: We then go to the **Settings** tab to create a Streaming Analytics service,
    which will be the engine that powers our enrichment process and associate it with
    the project. Note that we could also have created the service in the IBM Cloud
    catalog as we did for the other services used in this chapter, but since we still
    have to associate it with the project, we might as well do the creation in Watson
    Studio too.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们进入**设置**标签页创建一个流式分析服务，它将成为驱动我们丰富化过程的引擎，并将其与项目关联。请注意，我们也可以像为本章中其他服务一样，在IBM
    Cloud目录中创建该服务，但由于我们仍然需要将其与项目关联，最好也在Watson Studio中进行创建。
- en: 'In the **Settings** tab, we scroll to the **Associated services** section and
    click on the **Add service** drop-down to select **Streaming Analytics**. In the
    next page, you have the choice between **Existing** and **New**. Select **New**
    and follow the steps to create the service. Once done, the newly created service
    should be associated with the project as shown in the following screenshot:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在**设置**标签页中，我们向下滚动到**关联服务**部分，点击**添加服务**下拉菜单，选择**流式分析**。在接下来的页面中，您可以选择**现有**和**新建**。选择**新建**并按照步骤创建服务。创建完成后，新创建的服务应已与项目关联，如下图所示：
- en: Note
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: If there are multiple free options, it is OK to pick any one of them.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：如果有多个免费选项，可以任选其一。'
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_16.jpg)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![使用流式分析服务丰富推文数据](img/B09699_07_16.jpg)'
- en: Associating the Streaming Analytics service with the project
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 将流式分析服务与项目关联
- en: We are now ready to create the stream flow that defines the enrichment processing
    of our tweet data.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备创建定义推文数据丰富处理的流式数据流。
- en: We go to the **Assets** tab, scroll down to the **Streams flows** section and
    click on the **New streams flow** button. In the next page, we give a name, select
    the Streaming Analytics service, select **Manually** and click on the **Create**
    button.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入**资源**标签页，向下滚动到**流式数据流**部分，点击**新建流式数据流**按钮。在接下来的页面中，我们为其命名，选择流式分析服务，选择**手动**并点击**创建**按钮。
- en: 'We are now in the Streams Designer which is composed of a palette of operators
    on the left and a canvas where we can graphically build our stream flow. For our
    sample application, we''ll need to pick three operators from the palette and drag
    and drop them into the canvas:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在流式设计器中，它由左侧的操作符调色板和一个可以用来图形化构建流式数据流的画布组成。对于我们的示例应用程序，我们需要从调色板中选择三个操作符并将它们拖放到画布上：
- en: '**Message Hub from the Sources section of the palette**: Input source for our
    data. Once in the canvas, we rename it `Source Message Hub` (by double- clicking
    on it to enter edit mode).'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调色板中的源部分的消息中心**：我们数据的输入源。进入画布后，我们将其重命名为`Source Message Hub`（通过双击进入编辑模式）。'
- en: '**Code from the Processing and analytics section**: It will contain the data
    enrichment Python code that invokes the Watson NLU service. We rename the operator
    to `Enrichment`.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理和分析部分的代码**：它将包含调用Watson NLU服务的数据丰富化Python代码。我们将操作符重命名为`Enrichment`。'
- en: '**Message Hub from the Targets section of the palette**: Output source for the enriched
    data. We rename it to `Target Message Hub`.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来自调色板的目标部分中的Message Hub**：丰富数据的输出源。我们将其重命名为`目标Message Hub`。'
- en: Next, we create a connection between the **Source Message Hub** and **Enrichment**
    and between **Enrichment** and the **Target Message Hub**. To create a connection
    between two operators, simply grab the output port at the end of the first operator
    and drag it to the input port of the other operator. Notice that a source operator
    has only one output port on the right of the box to denote that it only supports
    outgoing connections, while a target operator has only one input port on the left
    to denote that it only supports incoming connections. Any operator from the **PROCESSING
    AND ANALYTICS** section has two ports on the left and right as they accept both
    incoming and outgoing connections.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建**源Message Hub**与**丰富**之间，以及**丰富**与**目标Message Hub**之间的连接。要创建两个操作符之间的连接，只需将第一个操作符末尾的输出端口拖动到另一个操作符的输入端口。请注意，源操作符右侧只有一个输出端口，表示它仅支持外部连接，而目标操作符左侧只有一个输入端口，表示它仅支持内部连接。**处理与分析**部分的任何操作符都有左右两个端口，因为它们同时接受和发送连接。
- en: 'The following screenshot shows the fully completed canvas:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了完整的画布：
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_17.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![使用流分析服务丰富推文数据](img/B09699_07_17.jpg)'
- en: Tweet enrichment stream flow
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 推文丰富流处理
- en: Let's now look at the configuration of each of these three operators.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下这三个操作符的配置。
- en: Note
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: To complete this section, make sure to run the code that generates
    topics to the Message Hub instance that we discussed in the previous section.
    Otherwise, the Message Hub instance will be empty, and no schema will be detected.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：要完成此部分，请确保运行生成主题的代码，并将其发送到我们在前一部分讨论过的Message Hub实例。否则，Message Hub实例将为空，且无法检测到任何模式。'
- en: 'Click on the source Message Hub. An animated pane on the right appears with
    the options to select the Message Hub instance that contains the tweets. The first
    time, you''ll need to create a connection to the Message Hub instance. Select
    `tweets` as the topic. Click on the **Edit Output Schema** and then **Detect Schema**
    to have the schema autopopulated from the data. You can also preview the live
    streaming data using the **Show Preview** button as shown in the following screenshot:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 点击源Message Hub。右侧会出现一个动画窗格，提供选择包含推文的Message Hub实例的选项。第一次使用时，您需要创建与Message Hub实例的连接。选择`tweets`作为主题。点击**编辑输出模式**，然后点击**检测模式**，以从数据中自动填充模式。您还可以使用**显示预览**按钮预览实时流数据，如下图所示：
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_18.jpg)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![使用流分析服务丰富推文数据](img/B09699_07_18.jpg)'
- en: Setting the schema and previewing the live streaming data
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 设置模式并预览实时流数据
- en: Now select the **Code** operator to implement the code that invokes the Watson
    NLU. The animated contextual right-hand pane contains a Python code editor with
    boilerplate code that includes the required functions to implement, namely `init(state)`
    and `process(event, state)`.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 现在选择**代码**操作符，执行调用Watson NLU的代码。右侧的动画上下文窗格包含一个Python代码编辑器，其中包含所需实现的模板代码，分别是`init(state)`和`process(event,
    state)`函数。
- en: 'In the `init` method, we instantiate the `NaturalLanguageUnderstandingV1` instance
    as shown in the following code:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在`init`方法中，我们实例化了`NaturalLanguageUnderstandingV1`实例，如下代码所示：
- en: '[PRE57]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note
  id: totrans-492
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode31.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode31.py)'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode31.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode31.py)'
- en: '**Note**: We need to install the `Watson_developer_cloud` library via the **Python
    packages** link located above the Python editor window in the right-hand contextual
    pane as shown in the following screenshot:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：我们需要通过位于Python编辑器窗口上方的**Python包**链接安装`Watson_developer_cloud`库，如下图所示：'
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_19.jpg)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![使用流分析服务丰富推文数据](img/B09699_07_19.jpg)'
- en: Adding the watson_cloud_developer package to the stream flow
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 将watson_cloud_developer包添加到流处理中
- en: 'The process method is invoked on every event data. We use it to invoke the
    Watson NLU and add the extra information to the event object as shown in the following
    code:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 每次事件数据都会调用该过程方法。我们使用它来调用 Watson NLU，并将额外的信息添加到事件对象中，如下代码所示：
- en: '[PRE58]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Note
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode32.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode32.py)'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode32.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode32.py)'
- en: '**Note**: We must also declare all output variables by using the **Edit Output Schema**
    link as shown in the following screenshot:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：我们还必须通过使用**编辑输出架构**链接声明所有输出变量，如下截图所示：'
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_20.jpg)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![通过流分析服务丰富推文数据](img/B09699_07_20.jpg)'
- en: Declaring all output variables for the Code operator
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 声明所有输出变量用于代码操作符
- en: Finally, we configure the target Message Hub to use the `enriched_tweets` topic.
    Note that you'll need to manually create the topic the first time by going into
    the dashboard of the Message Hub instance on the IBM Cloud and clicking on the
    **Add Topic** button.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们配置目标 Message Hub 以使用`enriched_tweets`主题。请注意，首次需要手动创建该主题，方法是进入 IBM Cloud
    上的 Message Hub 实例的仪表板并点击**添加主题**按钮。
- en: We then save the stream flow using the **Save** button in the main toolbar.
    Any errors in the flow, whether it be a compile error in the code, a service configuration
    error or any other errors, will be shown in the notification pane. After we make
    sure that there is no error, we can run the flow using the **Run** button which
    takes us to the streams flow live monitoring screen. This screen is composed of
    multiple panes. The main pane shows the different operators with the data represented
    as little balls flowing in a virtual pipe between operators. We can click on a
    pipe to show the events payload in a pane on the right. This is really useful
    for debugging as we can visualize how the data is transformed through each operator.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用主工具栏中的**保存**按钮保存流。流中的任何错误，无论是代码中的编译错误、服务配置错误还是其他任何错误，都将在通知面板中显示。在确保没有错误后，我们可以使用**运行**按钮运行流，该按钮将带我们进入流数据监控屏幕。此屏幕由多个面板组成。主面板显示不同的操作符，数据以小球的形式流动在操作符之间的虚拟管道中。我们可以点击管道，在右侧面板中显示事件负载。这对于调试非常有用，因为我们可以可视化数据如何在每个操作符中进行转换。
- en: Note
  id: totrans-508
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: Streams Designer also supports adding Python logging messages in the
    code operator which can then be downloaded on your local machine for analysis.
    You can learn more about this functionality here:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：Streams Designer 还支持在代码操作符中添加 Python 日志消息，然后可以将其下载到本地机器进行分析。你可以在这里了解更多关于此功能的信息：'
- en: '[https://dataplatform.cloud.ibm.com/docs/content/streaming-pipelines/downloading_logs.html](https://dataplatform.cloud.ibm.com/docs/content/streaming-pipelines/downloading_logs.html)'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://dataplatform.cloud.ibm.com/docs/content/streaming-pipelines/downloading_logs.html](https://dataplatform.cloud.ibm.com/docs/content/streaming-pipelines/downloading_logs.html)'
- en: 'The following screenshot shows the stream flow live monitoring screen:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了流式数据监控屏幕：
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_21.jpg)'
  id: totrans-512
  prefs: []
  type: TYPE_IMG
  zh: '![通过流分析服务丰富推文数据](img/B09699_07_21.jpg)'
- en: Live monitoring screen for the Twitter Sentiment Analysis stream flow
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 情感分析流数据的实时监控屏幕
- en: We now have our enriched tweets flowing in the Message Hub instance using the
    `enriched_tweets` topic. In the next section, we show how to create a Spark Streaming
    DataFrame using the Message Hub instance as the input source.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的丰富推文数据已经通过`enriched_tweets`主题流入 Message Hub 实例。在下一节中，我们将展示如何使用 Message
    Hub 实例作为输入源创建 Spark Streaming DataFrame。
- en: Creating a Spark Streaming DataFrame with a Kafka input source
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Kafka 输入源创建 Spark Streaming DataFrame
- en: In this final step, we create a Spark Streaming DataFrame that consumes the
    enriched tweets from the `enriched_tweets` Kafka topic of the Message Hub service.
    For this, we use the built-in Spark Kafka connector specifying the topic we want
    to subscribe to in the `subscribe` option. We also need to specify the list of
    Kafka servers in the `kafka.bootstrap.servers` option, by reading it from the
    global `message_hub_creds` variable that we created earlier.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步中，我们创建一个 Spark Streaming DataFrame，它从 `enriched_tweets` Kafka 主题中消费经过增强的推文，这个主题属于
    Message Hub 服务。为此，我们使用内置的 Spark Kafka 连接器，并在 `subscribe` 选项中指定我们想要订阅的主题。同时，我们还需要在
    `kafka.bootstrap.servers` 选项中指定 Kafka 服务器的列表，这些信息通过读取我们之前创建的全局 `message_hub_creds`
    变量来获取。
- en: Note
  id: totrans-517
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: You have probably noticed that different systems use different names
    for this option making it more error prone. Fortunately, in case of a misspelling,
    an exception with an explicit root cause message will be displayed.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：你可能已经注意到，不同的系统为此选项使用不同的名称，这使得它更容易出错。幸运的是，如果拼写错误，异常将显示一个明确的根本原因信息。'
- en: 'The preceding options are for Spark Streaming, and we still need to configure
    the Kafka credentials so that the lower level Kafka consumer can be properly authenticated
    with the Message Hub service. To properly pass these consumer properties to Kafka,
    we do not use the `.option` method, but rather we create a `kafka_options` dictionary
    that we pass to the load method as shown in the following code:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 上述选项是针对 Spark Streaming 的，我们仍然需要配置 Kafka 凭证，以便较低级别的 Kafka 消费者可以与 Message Hub
    服务进行正确的身份验证。为了正确地将这些消费者属性传递给 Kafka，我们不使用 `.option` 方法，而是创建一个 `kafka_options` 字典，并将其作为参数传递给加载方法，代码如下所示：
- en: '[PRE59]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Note
  id: totrans-521
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode33.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode33.py)'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode33.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode33.py)'
- en: You would think that we're done with the code at this point since the rest of
    the Notebook should work unchanged from *Part 3 – Create a real-time dashboard
    PixieApp*. This would be correct until we run the Notebook and start seeing exceptions
    with Spark complaining that the Kafka connector cannot be found. This is because
    the Kafka connector is not included in the core distribution of Spark and must
    be installed separately.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为代码到此为止就完成了，因为 Notebook 的其他部分应该与 *第 3 部分 – 创建实时仪表板 PixieApp* 一致。这个想法是正确的，直到我们运行
    Notebook 并开始看到 Spark 抛出异常，提示 Kafka 连接器无法找到。这是因为 Kafka 连接器并不包含在 Spark 的核心发行版中，必须单独安装。
- en: 'Unfortunately, these types of problems which are infrastructural in nature
    and are not directly related to the task at hand, happen all the time and we end
    up spending a lot of time trying to fix them. Searching on Stack Overflow or any
    other technical site usually yields a solution rapidly, but in some cases, the
    answer is not obvious. In this case, because we are running in a Notebook and
    not in a `spark-submit` script, there isn''t much help available, and we have
    to experiment ourselves until we find the solution. To install the `spark-sql-kafka`,
    we need to edit the `kernel.json` file discussed earlier in this chapter, and
    add the following option to the `"PYSPARK_SUBMIT_ARGS"` entry:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这类基础设施层面的问题并不直接与手头的任务相关，然而它们经常发生，我们最终花费大量时间去修复它们。在 Stack Overflow 或其他技术网站搜索通常能够快速找到解决方案，但有时答案并不显而易见。在这种情况下，由于我们是在
    Notebook 中运行，而不是在 `spark-submit` 脚本中运行，因此没有太多现成的帮助，我们只能自己尝试直到找到解决方法。要安装 `spark-sql-kafka`，我们需要编辑本章前面讨论过的
    `kernel.json` 文件，并将以下选项添加到 `"PYSPARK_SUBMIT_ARGS"` 项中：
- en: '[PRE60]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: When the kernel restarts, this configuration will automatically download the
    dependencies and cache them locally.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 当内核重启时，这个配置将自动下载依赖并将其缓存到本地。
- en: It should all work now right? Well, not yet. We still have to configure Kafka
    security to use the credentials to our Message Hub service which uses SASL as
    the security protocol. For that, we need to provide a **JAAS** (short for, **Java
    Authentication and Authorization Service**) configuration file that will contain
    the username and password for the service. The latest version of Kafka provides
    a flexible mechanism to programmatically configure the security using a consumer
    property called `sasl.jaas.config`. Unfortunately, the latest version of Spark
    (2.3.0 as of the time of writing) has not yet updated to the latest version of
    Kafka. So, we have to fall back to the other way of configuring JAAS which is
    to set a JVM system property called `java.security.auth.login.config` with the
    path to a `jaas.conf` configuration file.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该可以正常工作了吧？嗯，暂时还不行。我们仍然需要配置Kafka的安全性，以使用我们Message Hub服务的凭证，而该服务使用SASL作为安全协议。为此，我们需要提供一个**JAAS**（即**Java认证和授权服务**）配置文件，其中包含服务的用户名和密码。Kafka的最新版本提供了一种灵活的机制，允许使用名为`sasl.jaas.config`的消费者属性以编程方式配置安全性。不幸的是，Spark的最新版本（截至写作时为2.3.0）尚未更新为Kafka的最新版本。因此，我们必须退回到另一种配置JAAS的方式，即设置一个名为`java.security.auth.login.config`的JVM系统属性，并指向一个`jaas.conf`配置文件的路径。
- en: 'We first create the `jaas.conf` in a directory of our choice and add the following
    content to it:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在选择的目录中创建`jaas.conf`文件，并将以下内容添加到其中：
- en: '[PRE61]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: In the preceding content, replace the `XXXX` text with the username and password
    taken from the Message Hub service credentials.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述内容中，将`XXXX`替换为从Message Hub服务凭证中获得的用户名和密码。
- en: 'We then add the following configuration to the `"PYSPARK_SUBMIT_ARGS"` entry
    of `kernel.json`:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将以下配置添加到`kernel.json`中的`"PYSPARK_SUBMIT_ARGS"`条目：
- en: '[PRE62]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'For reference, here is a sample `kernel.json` that contains these configurations:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，这里是一个包含这些配置的示例`kernel.json`：
- en: '[PRE63]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Note
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode34.json](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode34.json)'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode34.json](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode34.json)'
- en: '**Note**: We should always restart the Notebook server when modifying `kernel.json`
    to make sure that all new configurations are properly reloaded.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：我们在修改`kernel.json`时，应该始终重启Notebook服务器，以确保所有新配置能够正确重新加载。'
- en: The rest of the Notebook code doesn't change, and the PixieApp dashboard should
    work the same.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的Notebook代码没有变化，PixieApp仪表盘应该依然能够正常工作。
- en: Note
  id: totrans-541
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'We have now completed Part 4 of our sample application; you can find the complete
    notebook here:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了示例应用的第4部分；您可以在这里找到完整的笔记本：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%204.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%204.ipynb)'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%204.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%204.ipynb)'
- en: 'The extra code we had to write at the end of this section reminds us that the
    journey of working with data is never a straight line. We have to be prepared
    to deal with obstacles that can be different in nature: a bug in a dependency
    library or a limitation in an external service. Surmounting these obstacles doesn''t
    have to stop the project for a long time. Since we''re using mostly open-source
    components, we can leverage a large community of like-minded developers on social
    sites such as Stack Overflow, get new ideas and code samples, and experiment quickly
    on a Jupyter Notebook.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节末尾我们编写的额外代码提醒我们，与数据打交道的旅程永远不会是一条直线。我们必须准备好应对不同性质的障碍：可能是依赖库中的错误，或外部服务的限制。克服这些障碍不必让项目停滞太久。由于我们主要使用开源组件，我们可以借助像Stack
    Overflow这样的社交网站上志同道合的开发者社区，获取新的想法和代码示例，并在Jupyter Notebook中快速实验。
- en: Summary
  id: totrans-545
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've built a data pipeline that analyzes large quantities
    of streaming data containing unstructured text and applies NLP algorithms coming
    from external cloud services to extract sentiment and other important entities
    found in the text. We also built a PixieApp dashboard that displays live metrics
    with insights extracted from the tweets. We've also discussed various techniques
    for analyzing data at scale, including Apache Spark Structured Streaming, Apache
    Kafka, and IBM Streaming Analytics. As always, the goal of these sample applications
    is to show the art of the possible in building data pipelines with a special focus
    on leveraging existing frameworks, libraries, and cloud services.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们构建了一个数据管道，用于分析包含非结构化文本的大量流数据，并应用来自外部云服务的NLP算法来提取情感和文本中发现的其他重要实体。我们还构建了一个PixieApp仪表板，显示从推文中提取的实时指标和洞察。我们还讨论了多种分析大规模数据的技术，包括Apache
    Spark结构化流处理、Apache Kafka和IBM Streaming Analytics。像往常一样，这些示例应用程序的目标是展示如何构建数据管道，特别关注如何利用现有框架、库和云服务的可能性。
- en: In the next chapter, we'll discuss time series analysis, which is another great
    data science topic with a lot of industry applications, which we'll illustrate
    by building a *Financial Portfolio* analysis application.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论时间序列分析，这是另一个具有广泛行业应用的数据科学话题，我们将通过构建一个*金融投资组合*分析应用程序来说明这一点。
