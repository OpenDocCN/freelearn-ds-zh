- en: Building a Classification Model with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark构建分类模型
- en: In this chapter, you will learn the basics of classification models, and how
    they can be used in a variety of contexts. Classification generically refers to
    classifying things into distinct categories or classes. In the case of a classification
    model, we typically wish to assign classes based on a set of features. The features
    might represent variables related to an item or object, an event or context, or
    some combination of these.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习分类模型的基础知识，以及它们在各种情境中的应用。分类通常指将事物分类到不同的类别中。在分类模型的情况下，我们通常希望基于一组特征分配类别。这些特征可能代表与物品或对象、事件或背景相关的变量，或者这些变量的组合。
- en: The simplest form of classification is when we have two classes; this is referred
    to as **binary classification**. One of the classes is usually labeled as the
    **positive class** (assigned a label of 1), while the other is labeled as the
    **negative class** (assigned a label of -1, or, sometimes, 0). A simple example
    with two classes is shown in the following figure. The input features, in this
    case, have two dimensions, and the feature values are represented on the *x* and y-axes
    in the figure. Our task is to train a model that can classify new data points
    in this two-dimensional space as either one class (red) or the other (blue).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的分类形式是当我们有两个类别时；这被称为二元分类。其中一个类通常被标记为正类（分配标签1），而另一个被标记为负类（分配标签-1，有时为0）。下图显示了一个具有两个类的简单示例。在这种情况下，输入特征具有两个维度，并且特征值在图中的x和y轴上表示。我们的任务是训练一个模型，可以将这个二维空间中的新数据点分类为一个类（红色）或另一个类（蓝色）。
- en: '![](img/image_06_001.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_001.png)'
- en: A simple binary classification problem
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的二元分类问题
- en: 'If we have more than two classes, we would refer to multiclass classification,
    and classes are typically labeled using integer numbers starting at 0 (for example,
    five different classes would range from label 0 to 4). An example is shown in
    the following figure. Again, the input features are assumed to be two-dimensional
    for ease of illustration:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有超过两个类别，我们将称之为多类分类，类别通常使用从0开始的整数编号（例如，五个不同的类别的标签范围从0到4）。示例如下图所示。再次强调，为了便于说明，假设输入特征是二维的：
- en: '![](img/image_06_002.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_002.png)'
- en: A simple multiclass classification problem
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的多类分类问题
- en: 'Classification is a form of supervised learning, where we train a model with
    training examples that include known targets or outcomes of interest (that is,
    the model is supervised with these example outcomes). Classification models can
    be used in many situations, but a few common examples include the ones listed
    next:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是一种监督学习的形式，我们通过包含已知目标或感兴趣结果的训练示例来训练模型（即，模型受这些示例结果的监督）。分类模型可以在许多情况下使用，但一些常见的例子包括以下几种：
- en: Predicting the probability of Internet users clicking on an online advert; here,
    the classes are binary in nature (that is, click or no click)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测互联网用户点击在线广告的概率；在这里，类别的性质是二元的（即点击或不点击）
- en: Detecting fraud; again, in this case, the classes are commonly binary (fraud
    or no fraud)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测欺诈；同样，在这种情况下，类别通常是二元的（欺诈或无欺诈）
- en: Predicting defaults on loans (binary)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测贷款违约（二元）
- en: Classifying images, video, or sounds (most often multiclass, with potentially
    very many different classes)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图像、视频或声音进行分类（通常是多类，可能有很多不同的类）
- en: Assigning categories or tags to news articles, web pages, or other content (multiclass)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新闻文章、网页或其他内容分配到类别或标签中（多类）
- en: Discovering e-mail and web spam, network intrusions, and other malicious behavior
    (binary or multiclass)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现电子邮件和网络垃圾邮件、网络入侵和其他恶意行为（二元或多类）
- en: Detecting failure situations, for example, in computer systems or networks
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测故障情况，例如计算机系统或网络中的故障
- en: Ranking customers or users in order of probability that they might purchase
    a product or use a service
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照客户或用户购买产品或使用服务的概率对其进行排名
- en: Predicting customers or users who might stop using a product, service, or provider
    (called churn)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测可能停止使用产品、服务或提供者的客户或用户（称为流失）
- en: These are just a few possible use cases. In fact, it is probably safe to say
    that classification is one of the most widely used machine learning and statistical
    techniques in modern businesses, especially, online businesses.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一些可能的用例。事实上，可以说分类是现代企业中最广泛使用的机器学习和统计技术之一，尤其是在线企业。
- en: 'In this chapter, we will do the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将进行以下操作：
- en: Discuss the types of classification models available in ML library
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论ML库中可用的分类模型类型
- en: Use Spark to extract appropriate features from raw input data
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark从原始输入数据中提取适当的特征
- en: Train a number of classification models using ML library
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ML库训练多个分类模型
- en: Make predictions with our classification models
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们的分类模型进行预测
- en: Apply a number of standard evaluation techniques to assess the predictive performance
    of our models
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用多种标准评估技术来评估我们模型的预测性能
- en: Illustrate how to improve model performance using some of the feature extraction
    approaches from [Chapter 4](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml), *Obtaining,
    Processing, and Preparing Data with Spark*
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 说明如何使用[第4章](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml)中的一些特征提取方法来改善模型性能，*使用Spark获取、处理和准备数据*
- en: Explore the impact of parameter tuning on model performance, and learn how to
    use cross-validation to select the most optimal model parameters
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索参数调整对模型性能的影响，并学习如何使用交叉验证来选择最优的模型参数
- en: Types of classification models
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类模型的类型
- en: 'We will explore three common classification models available in Spark: linear
    models, decision trees, and naive Bayes models. Linear models, while less complex,
    are relatively easier to scale to very large datasets. Decision tree is a powerful
    non-linear technique, which can be a little more difficult to scale up (fortunately,
    ML library takes care of this for us!) and more computationally intensive to train,
    but delivers leading performance in many situations. The naive Bayes models are
    more simple, but are easy to train efficiently and parallelize (in fact, they
    require only one pass over the dataset). They can also give reasonable performance
    in many cases where appropriate feature engineering is used. A naive Bayes model
    also provides a good baseline model against which we can measure the performance
    of other models.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨Spark中可用的三种常见分类模型：线性模型、决策树和朴素贝叶斯模型。线性模型虽然较为简单，但相对容易扩展到非常大的数据集。决策树是一种强大的非线性技术，可能更难扩展（幸运的是，ML库会为我们处理这个问题！）并且训练时计算量更大，但在许多情况下提供领先的性能。朴素贝叶斯模型更简单，但易于高效训练和并行化（事实上，它们只需要对数据集进行一次遍历）。在适当的特征工程使用的情况下，它们也可以在许多情况下提供合理的性能。朴素贝叶斯模型还提供了一个良好的基准模型，可以用来衡量其他模型的性能。
- en: Currently, Spark's ML library supports binary classification for linear models,
    decision trees, and naive Bayes models, and multiclass classification for decision
    trees and naive Bayes models. In this book, for simplicity in illustrating the
    examples, we will focus on the binary case.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Spark的ML库支持线性模型、决策树和朴素贝叶斯模型的二元分类，以及决策树和朴素贝叶斯模型的多类分类。在本书中，为了简化示例，我们将专注于二元情况。
- en: Linear models
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型
- en: The core idea of linear models (or generalized linear models) is that we model
    the predicted outcome of interest (often called the **target** or **dependent
    variable**) as a function of a simple linear predictor applied to the input variables
    (also referred to as features or independent variables).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型（或广义线性模型）的核心思想是，我们将感兴趣的预测结果（通常称为**目标**或**因变量**）建模为应用于输入变量（也称为特征或自变量）的简单线性预测器的函数。
- en: '*y = f(W^Tx)*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = f(W^Tx)*'
- en: Here, *y* is the target variable, *w* is the vector of parameters (known as
    the **weight vector**), and *x* is the vector of input features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y*是目标变量，*w*是参数向量（称为**权重向量**），*x*是输入特征向量。
- en: '*wTx* is the linear predictor (or vector dot product) of the weight vector
    *w* and feature vector *x*. To this linear predictor, we applied a function *f*
    (called the **link function**).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*wTx*是权重向量*w*和特征向量*x*的线性预测器（或向量点积）。对于这个线性预测器，我们应用了一个函数*f*（称为**链接函数**）。'
- en: Linear models can, in fact, be used for both classification and regression,
    simply by changing the link function. Standard linear regression (covered in the
    next chapter) uses an identity link (that is, *y =W^Tx* directly), while binary
    classification uses alternative link functions as discussed here.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型实际上可以用于分类和回归，只需改变链接函数。标准线性回归（在下一章中介绍）使用恒等链接（即*y =W^Tx*直接），而二元分类使用本文讨论的替代链接函数。
- en: Let's take a look at the example of online advertising. In this case, the target
    variable would be 0 (often assigned the class label of -1 in mathematical treatments)
    if no click was observed for a given advert displayed on a web page (called an
    **impression**). The target variable would be 1 if a click occurred. The feature
    vector for each impression would consist of variables related to the impression
    event (such as features relating to the user, web page, advert and advertiser,
    and various other factors relating to the context of the event, such as the type
    of device used, time of the day, and geolocation).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下在线广告的例子。在这种情况下，如果在网页上显示的广告（称为**曝光**）没有观察到点击，则目标变量将为0（在数学处理中通常被分配为-1的类标签）。如果发生了点击，则目标变量将为1。每个曝光的特征向量将由与曝光事件相关的变量组成（例如与用户、网页、广告和广告商相关的特征，以及与事件背景相关的各种其他因素，如使用的设备类型、时间和地理位置）。
- en: Thus, we would like to find a model that maps a given input feature vector (advert
    impression) to a predicted outcome (click or not). To make a prediction for a
    new data point, we will take the new feature vector (which is unseen, and hence,
    we do not know what the target variable is), and compute the dot product with
    our weight vector. We will then apply the relevant link function, and the result
    is our predicted outcome (after applying a threshold to the prediction, in the
    case of some models).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望找到一个模型，将给定的输入特征向量（广告曝光）映射到预测结果（点击或未点击）。为了对新数据点进行预测，我们将采用新的特征向量（未见过，因此我们不知道目标变量是什么），并计算与我们的权重向量的点积。然后应用相关的链接函数，结果就是我们的预测结果（在某些模型的情况下，应用阈值到预测结果）。
- en: Given a set of input data in the form of feature vectors and target variables,
    we would like to find the weight vector that is the best fit for the data, in
    the sense that we minimize some error between what our model predicts and the
    actual outcomes observed. This process is called model fitting, training, or optimization.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组以特征向量和目标变量形式的输入数据，我们希望找到最适合数据的权重向量，即我们最小化模型预测和实际观察结果之间的某种误差。这个过程称为模型拟合、训练或优化。
- en: More formally, we seek to find the weight vector that minimizes the sum, over
    all the training examples, of the loss (or error) computed from some loss function.
    The loss function takes the weight vector, feature vector, and the actual outcome
    for a given training example as input, and outputs the loss. In fact, the loss
    function itself is effectively specified by the link function; hence, for a given
    type of classification or regression (that is, a given link function), there is
    a corresponding loss function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，我们试图找到最小化所有训练示例的损失（或错误）的权重向量，该损失是从某个损失函数计算出来的。损失函数将权重向量、特征向量和给定训练示例的实际结果作为输入，并输出损失。实际上，损失函数本身是由链接函数有效地指定的；因此，对于给定类型的分类或回归（即给定链接函数），存在相应的损失函数。
- en: For further details on linear models and loss functions, see the linear methods
    section related to binary classification in the *Spark Programming Guide* at [http://spark.apache.org/docs/latest/mllib-linear-methods.html#binary-classification](http://spark.apache.org/docs/latest/mllib-linear-methods.html#binary-classification)
    and [http://spark.apache.org/docs/latest/ml-classification-regression.html#linear-methods](http://spark.apache.org/docs/latest/ml-classification-regression.html#linear-methods).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有关线性模型和损失函数的更多细节，请参阅*Spark编程指南*中与二元分类相关的线性方法部分[http://spark.apache.org/docs/latest/mllib-linear-methods.html#binary-classification](http://spark.apache.org/docs/latest/mllib-linear-methods.html#binary-classification)和[http://spark.apache.org/docs/latest/ml-classification-regression.html#linear-methods](http://spark.apache.org/docs/latest/ml-classification-regression.html#linear-methods)。
- en: Also, see the Wikipedia entry for generalized linear models at [http://en.wikipedia.org/wiki/Generalized_linear_model](http://en.wikipedia.org/wiki/Generalized_linear_model).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另请参阅维基百科关于广义线性模型的条目[http://en.wikipedia.org/wiki/Generalized_linear_model](http://en.wikipedia.org/wiki/Generalized_linear_model)。
- en: While a detailed treatment of linear models and loss functions is beyond the
    scope of this book, Spark ML provides two loss functions suitable to binary classification
    (you can learn more about them from the Spark documentation). The first one is
    a logistic loss, which equates to a model known as **logistic regression**, while
    the second one is the hinge loss, which is equivalent to a linear **Support Vector
    Machine** (**SVM**). Note that the SVM does not strictly fall into the statistical
    framework of generalized linear models, but can be used in the same way as it
    essentially specifies a loss and link function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对线性模型和损失函数的详细处理超出了本书的范围，但Spark ML提供了两个适用于二元分类的损失函数（您可以从Spark文档中了解更多信息）。第一个是逻辑损失，它等同于一个称为**逻辑回归**的模型，而第二个是铰链损失，它等同于线性**支持向量机**（**SVM**）。请注意，SVM并不严格属于广义线性模型的统计框架，但可以像它一样使用，因为它本质上指定了损失和链接函数。
- en: In the following figure, we show the logistic loss and hinge loss relative to
    the actual zero-one loss. The zero-one loss is the true loss for binary classification--it
    is either zero if the model predicts correctly, or one if the model predicts incorrectly.
    The reason it is not actually used is that it is not a differentiable loss function,
    so it is not possible to easily compute a gradient and, thus, very difficult to
    optimize.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们展示了逻辑损失和铰链损失相对于实际零一损失的情况。零一损失是二元分类的真实损失--如果模型预测正确，则为零，如果模型预测错误，则为一。它实际上没有被使用的原因是它不是一个可微的损失函数，因此不可能轻松地计算梯度，因此非常难以优化。
- en: 'The other loss functions are approximations to the zero-one loss, which make
    optimization possible:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其他损失函数是零一损失的近似，这使得优化成为可能：
- en: '![](img/image_06_003.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_003.png)'
- en: The logistic, hinge, and zero-one loss functions
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑、铰链和零一损失函数
- en: The preceding loss diagram is adapted from the scikit-learn example at [http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html](http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的损失图是从scikit-learn示例调整而来的[http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html](http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html)。
- en: Logistic regression
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is a probabilistic model that is, its predictions are bounded
    between 0 and 1, and for binary classification, equate to the model's estimate
    of the probability of the data point belonging to the positive class. Logistic
    regression is one of the most widely used linear classification models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一个概率模型，也就是说，它的预测值介于0和1之间，对于二元分类，等同于模型对数据点属于正类的概率的估计。逻辑回归是最广泛使用的线性分类模型之一。
- en: 'As mentioned earlier, the link function used in logistic regression is this
    logit link:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，逻辑回归中使用的链接函数是logit链接：
- en: '*1 / (1 + exp(- W^Tx))   a*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*1 / (1 + exp(- W^Tx)) a*'
- en: 'The related loss function for logistic regression is the logistic loss:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的相关损失函数是逻辑损失：
- en: '*log(1 + exp(-y W^Tx)) *'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*log(1 + exp(-y W^Tx)) *'
- en: Here, *y* is the actual target variable (either 1 for the positive class or
    -1 for the negative class).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*y*是实际的目标变量（正类别为1，负类别为-1）。
- en: Multinomial logistic regression
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式逻辑回归
- en: Multinomial logistic regression generalizes to multiclass problems; it allows
    for more than two categories of the outcome variable. Just like binary logistic
    regression, multinomial logistic regression also uses maximum likelihood estimation
    to evaluate the probability.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式逻辑回归推广到多类问题；它允许结果变量有两个以上的类别。与二元逻辑回归一样，多项式逻辑回归也使用最大似然估计来评估概率。
- en: Multinomial logistic regression is mainly used when the dependent variable in
    question is nominal. Multinomial logistic regression is a classification problem
    in which a linear combination of the observed features and parameters can be utilized
    to calculate the probability of each particular outcome of the dependent variable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式逻辑回归主要用于被解释变量是名义的情况。多项式逻辑回归是一个分类问题，其中观察到的特征和参数的线性组合可以用来计算依赖变量的每个特定结果的概率。
- en: In this chapter, we will use a different dataset from the one we used for our
    recommendation model, as the MovieLens data doesn't have much for us to work with
    in terms of a classification problem. We will use a dataset from a competition
    on Kaggle. The dataset was provided by StumbleUpon, and the problem relates to
    classifying whether a given web page is ephemeral (that is, short-lived, and will
    cease being popular soon), or evergreen (that is, persistently popular) on their
    web content recommendation pages.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个不同的数据集，而不是我们用于推荐模型的数据集，因为MovieLens数据对于我们解决分类问题并不多。我们将使用Kaggle上的一项竞赛数据集。该数据集由StumbleUpon提供，问题涉及对给定网页进行分类，判断其是短暂的（即短暂存在，很快就不再流行）还是长青的（即持续流行）在他们的网页内容推荐页面上。
- en: The dataset used here can be downloaded from [http://www.kaggle.com/c/stumbleupon/data](http://www.kaggle.com/c/stumbleupon/data).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此处使用的数据集可以从[http://www.kaggle.com/c/stumbleupon/data](http://www.kaggle.com/c/stumbleupon/data)下载。
- en: Download the training data (`train.tsv`)-you will need to accept the terms and
    conditions before downloading the dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下载训练数据（`train.tsv`）-您需要在下载数据集之前接受条款和条件。
- en: You can find more information about the competition at [http://www.kaggle.com/c/stumbleupon](http://www.kaggle.com/c/stumbleupon).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[http://www.kaggle.com/c/stumbleupon](http://www.kaggle.com/c/stumbleupon)找到有关该竞赛的更多信息。
- en: The code listing to get started is available at [https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_06/2.0.0/src/scala/org/sparksamples/classification/stumbleupon](https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_06/2.0.0/src/scala/org/sparksamples/classification/stumbleupon).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_06/2.0.0/src/scala/org/sparksamples/classification/stumbleupon](https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_06/2.0.0/src/scala/org/sparksamples/classification/stumbleupon)找到开始的代码列表。
- en: 'A glimpse of the StumbleUpon dataset stored as a temporary table using Spark
    SQLContext is given in the following screenshot:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用Spark SQLContext存储为临时表的StumbleUpon数据集的一瞥：
- en: '![](img/image_06_004.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_004.png)'
- en: Visualizing the StumbleUpon dataset
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化StumbleUpon数据集
- en: We ran custom logic to reduce the number of features to two, so that we can
    visualize the dataset in a two-dimensional plane, keeping the lines in the dataset
    constant.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行了自定义逻辑，将特征数量减少到两个，以便我们可以在二维平面上可视化数据集，保持数据集中的线条不变。
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once we have the data in a two-dimensional format, log scale is applied to
    both *x* and *y* for plotting convenience. In our case, we used D3.js for plotting
    as shown next. This data will be classified into two classes, and we will use
    the same base image to show the classification:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据转换为二维格式，就会对*x*和*y*应用对数尺度以方便绘图。在我们的情况下，我们使用D3.js进行绘图，如下所示。这些数据将被分类为两类，并且我们将使用相同的基础图像来显示分类：
- en: '![](img/image_06_005.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_005.png)'
- en: Extracting features from the Kaggle/StumbleUpon evergreen classification dataset
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Kaggle/StumbleUpon长青分类数据集中提取特征
- en: 'Before we begin, we will remove the column name header from the first line
    of the file to make it easier for us to work with the data in Spark. Change to
    the directory in which you downloaded the data (referred to as `PATH` here), run
    the following command to remove the first line, and pipe the result to a new file
    called `train_noheader.tsv`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们将删除文件的第一行列名标题，以便我们更容易在Spark中处理数据。切换到您下载数据的目录（这里称为`PATH`），运行以下命令以删除第一行，并将结果导出到一个名为`train_noheader.tsv`的新文件中：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we are ready to start up our Spark shell (remember to run this command
    from your Spark installation directory):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备启动我们的Spark shell（记得从您的Spark安装目录运行此命令）：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can type in the code that follows for the remainder of this chapter directly
    into your Spark shell.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接在Spark shell中输入本章剩余部分的代码。
- en: 'In a manner similar to what we did in the earlier chapters, we will load the
    raw training data into an RDD, and inspect it as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前章节类似，我们将将原始训练数据加载到RDD中，并进行检查：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will see the following on the screen:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕上会看到以下内容：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can check the fields that are available by reading through the overview
    on the dataset page as mentioned earlier. The first two columns contain the URL
    and ID of the page. The next column contains some raw textual content. The next
    column contains the category assigned to the page. The next 22 columns contain
    numeric or categorical features of various kinds. The final column contains the
    target--1 is evergreen, while 0 is non-evergreen.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过阅读数据集页面上的概述来检查可用的字段，如前面提到的。前两列包含页面的URL和ID。下一列包含一些原始文本内容。下一列包含分配给页面的类别。接下来的22列包含各种数字或分类特征。最后一列包含目标--1表示长青，而0表示非长青。
- en: We'll start off with a simple approach of using only the available numeric features
    directly. As each categorical variable is binary, we already have a *1-of-k* encoding
    for these variables, so we don't need to do any further feature extraction.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从直接使用可用的数字特征开始。由于每个分类变量都是二进制的，我们已经对这些变量进行了*1-of-k*编码，因此我们不需要进行进一步的特征提取。
- en: Due to the way the data is formatted, we will have to do a bit of data cleaning
    during our initial processing by trimming out the extra quotation characters (`"`).
    There are also missing values in the dataset; they are denoted by the `"?"` character.
    In this case, we will simply assign a zero value to these missing values.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据格式的原因，在初始处理过程中，我们将不得不进行一些数据清理，去除额外的引号字符（`"`）。数据集中还存在缺失值；它们由`"?"`字符表示。在这种情况下，我们将简单地为这些缺失值分配一个零值。
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, we extracted the `label` variable from the last column,
    and an array of `features` for columns 5 to 25 after cleaning and dealing with
    missing values. We converted the `label` variable to an integer value, and the
    `features` variable to an `Array[Double]`. Finally, we wrapped `label` and `features`
    in a `LabeledPoint` instance, converting the features into an MLlib vector.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们从最后一列中提取了`label`变量，并在清理和处理缺失值后，提取了列5到25的`features`数组。我们将`label`变量转换为整数值，将`features`变量转换为`Array[Double]`。最后，我们将`label`和`features`包装在`LabeledPoint`实例中，将特征转换为MLlib向量。
- en: 'We will also cache the data and count the number of data points as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将缓存数据并计算数据点的数量如下：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You will see that the value of `numData` is `7395`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到`numData`的值为`7395`。
- en: We will explore the dataset in more detail a little later, but we will tell
    you now that there are some negative feature values in the numeric data. As we
    saw earlier, the naive Bayes model requires non-negative features, and will throw
    an error if it encounters negative values. So, for now, we will create a version
    of our input feature vectors for the naive Bayes model by setting any negative
    feature values to zero.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后我们将更详细地探索数据集，但现在我们会告诉您数值数据中有一些负特征值。正如我们之前看到的，朴素贝叶斯模型需要非负特征，并且如果遇到负值，将会抛出错误。因此，现在我们将通过将任何负特征值设置为零来为朴素贝叶斯模型创建我们输入特征向量的版本。
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: StumbleUponExecutor
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StumbleUponExecutor
- en: 'The StumbleUponExecutor ([https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/StumbleUponExecutor.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/StumbleUponExecutor.scala))
    object can be used to choose and run the respective classification model; for
    example, to run `LogisiticRegression` and to execute the logistic regression pipeline,
    set program argument as LR. For other commands, refer to the following code snippet:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: StumbleUponExecutor ([https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/StumbleUponExecutor.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/StumbleUponExecutor.scala))
    对象可用于选择和运行相应的分类模型；例如，要运行`LogisiticRegression`并执行逻辑回归管道，将程序参数设置为LR。有关其他命令，请参考以下代码片段：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s train the StumbleUpon dataset by splitting it into 80% training and
    20% testing; use `LogisticRegression` with `TrainValidationSplit` from Spark to
    build the model, and get the evaluation metrics around test data:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将StumbleUpon数据集分成80%的训练集和20%的测试集来进行训练；使用`LogisticRegression`和`TrainValidationSplit`从Spark构建模型，并获得关于测试数据的评估指标：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To create a pipeline object, we will use `ParamGridBuilder`. `ParamGridBuilder`
    is used to build the param grid, which is a list of parameters to choose from
    or search over by the estimator for best model selection. You can find more details
    about it at the following link:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个管道对象，我们将使用`ParamGridBuilder`。`ParamGridBuilder`用于构建参数网格，这是一个供估计器选择或搜索的参数列表，以便进行最佳模型选择。您可以在以下链接找到更多详细信息：
- en: '[https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/ml/tuning/ParamGridBuilder.html](https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/ml/tuning/ParamGridBuilder.html)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/ml/tuning/ParamGridBuilder.html](https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/ml/tuning/ParamGridBuilder.html)'
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will use `TrainValidationSplit` for hyperparameter tuning. It evaluates each
    combination of parameters once as opposed to*k* times in the case of `CrossValidator`.
    It creates a single training, test dataset pair, and splits between the training
    and testing is done based on the `trainRatio` parameter.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`TrainValidationSplit`进行超参数调整。与`CrossValidator`相比，它对每个参数组合进行一次评估，而不是*k*次。它创建一个单一的训练、测试数据集对，并且基于`trainRatio`参数进行训练和测试的拆分。
- en: '`Trainvalidationsplit` takes `Estimator`, a set of `ParamMaps` provided in
    the `estimatorParamMaps` parameter, and `Evaluator`. Refer to the following link
    for more information:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`Trainvalidationsplit`接受`Estimator`，在`estimatorParamMaps`参数中提供的一组`ParamMaps`，以及`Evaluator`。有关更多信息，请参考以下链接：'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.tuning.TrainValidationSplit](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.tuning.TrainValidationSplit)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.tuning.TrainValidationSplit](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.tuning.TrainValidationSplit)'
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You will see the following output displayed:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出显示：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The code listing can be found at this link: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/LogisticRegressionPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/LogisticRegressionPipeline.scala)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单可以在此链接找到：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/LogisticRegressionPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/LogisticRegressionPipeline.scala)
- en: 'The visualization of predicted and actual data in a two-dimensional scatter
    plot is shown in these two screenshots:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个截图中显示了预测和实际数据的二维散点图可视化：
- en: '![](img/image_06_006.png)![](img/image_06_007.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_006.png)![](img/image_06_007.png)'
- en: Linear support vector machines
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性支持向量机
- en: SVM is a powerful and popular technique for regression and classification. Unlike
    logistic regression, it is not a probabilistic model but predicts classes based
    on whether the model evaluation is positive or negative.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是回归和分类的强大且流行的技术。与逻辑回归不同，它不是概率模型，而是根据模型评估是正面还是负面来预测类别。
- en: 'The SVM link function is the identity link, so the predicted outcome is as
    follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: SVM链接函数是恒等链接，因此预测的结果如下：
- en: '*y = w^Tx*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = w^Tx*'
- en: Hence, if the evaluation of *wTx* is greater than or equal to a threshold of
    0, the SVM will assign the data point to class 1; otherwise, the SVM will assign
    it to class 0
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果*wTx*的评估大于或等于阈值0，SVM将把数据点分配给类1；否则，SVM将把它分配给类0。
- en: (this threshold is a model parameter of SVM, and can be adjusted).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: （此阈值是SVM的模型参数，可以进行调整）。
- en: 'The loss function for SVM is known as the hinge loss and is defined as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的损失函数称为铰链损失，定义如下：
- en: '*max(0, 1 - yw^Tx)*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*max(0, 1 - yw^Tx)*'
- en: SVM is a maximum margin classifier--it tries to find a weight vector such that
    the classes are separated as much as possible. It has been shown to perform well
    on many classification tasks, and the linear variant can scale to very large datasets.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是最大间隔分类器--它试图找到一个权重向量，使得类尽可能分开。已经证明在许多分类任务上表现良好，线性变体可以扩展到非常大的数据集。
- en: SVMs have a large amount of theory behind them, which is beyond the scope of
    this book, but you can visit [http://en.wikipedia.org/wiki/Support_vector_machine](http://en.wikipedia.org/wiki/Support_vector_machine)
    and [http://www.support-vector-machines.org/](http://www.support-vector-machines.org/)
    for more details.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: SVM有大量的理论支持，超出了本书的范围，但您可以访问[http://en.wikipedia.org/wiki/Support_vector_machine](http://en.wikipedia.org/wiki/Support_vector_machine)和[http://www.support-vector-machines.org/](http://www.support-vector-machines.org/)了解更多详情。
- en: In the following figure, we have plotted the different decision functions for
    logistic regression (the blue line) and linear SVM (the red line) based on the
    simple binary classification example explained earlier.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们根据之前解释的简单二元分类示例，绘制了逻辑回归（蓝线）和线性SVM（红线）的不同决策函数。
- en: 'You can see that the SVM effectively focuses on the points that lie closest
    to the decision function (the margin lines are shown with red dashes):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到SVM有效地聚焦于距离决策函数最近的点（边际线用红色虚线显示）：
- en: '![](img/image_06_008.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_008.png)'
- en: Decision functions for logistic regression and linear SVM for binary classification
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归和线性SVM的二元分类决策函数
- en: 'Let''s train the StumbleUpon dataset by splitting it into 80% training and
    20% testing, use SVM from Spark to build the model, and get evaluation metrics
    around the test data:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将StumbleUpon数据集分为80%的训练集和20%的测试集，使用Spark中的SVM构建模型，并在测试数据周围获取评估指标：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You will see the following output displayed:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出显示：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The code listing is available at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/SVMPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/SVMPipeline.scala).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单可在[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/SVMPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/SVMPipeline.scala)找到。
- en: The naive Bayes model
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型
- en: Naive Bayes is a probabilistic model, which makes predictions by computing the
    probability of a data point that belongs to a given class. A naive Bayes model
    assumes that each feature makes an independent contribution to the probability
    assigned to a class (it assumes conditional independence between features).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一个概率模型，通过计算数据点属于给定类的概率来进行预测。朴素贝叶斯模型假设每个特征对分配给类的概率做出独立贡献（假设特征之间具有条件独立性）。
- en: Due to this assumption, the probability of each class becomes a function of
    the product of the probability of a feature occurring, given the class, as well
    as the probability of this class. This makes training the model tractable and
    relatively straightforward. The class prior probabilities and feature conditional
    probabilities are all estimated from the frequencies present in the dataset. Classification
    is performed by selecting the most probable class, given the features and class
    probabilities.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个假设，每个类的概率成为特征出现给定类的概率以及该类的概率的乘积函数。这使得训练模型变得可行且相对简单。类先验概率和特征条件概率都是从数据集中的频率估计得出的。分类是通过选择最可能的类来执行的，给定特征和类概率。
- en: An assumption is also made about the feature distributions (the parameters of
    which are estimated from the data). Spark ML implements multinomial naive Bayes,
    which assumes that the feature distribution is a multinomial distribution that
    represents non-negative frequency counts of the features.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 还对特征分布做出了假设（其参数是从数据中估计得出的）。Spark ML实现了多项式朴素贝叶斯，假设特征分布是代表特征的非负频率计数的多项式分布。
- en: It is suitable for binary features (for example, 1-of-k encoded categorical
    features), and is commonly used for text and document classification (where, as
    we have seen in [Chapter 4](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml), *Obtaining,
    Processing, and Preparing Data with Spark*, the bag-of-words vector is a typical
    feature representation).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 它适用于二元特征（例如，1-of-k编码的分类特征），通常用于文本和文档分类（正如我们在[第4章](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml)中看到的，*使用Spark获取、处理和准备数据*，词袋向量是典型的特征表示）。
- en: Take a look at the *ML - Naive Bayes* section in the Spark documentation at
    [http://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes](http://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes)
    for more information.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark文档的*ML - 朴素贝叶斯*部分查看更多信息，网址为[http://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes](http://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes)。
- en: The Wikipedia page at [http://en.wikipedia.org/wiki/Naive_Bayes_classifier](http://en.wikipedia.org/wiki/Naive_Bayes_classifier)
    has a more detailed explanation of the mathematical formulation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科页面[http://en.wikipedia.org/wiki/Naive_Bayes_classifier](http://en.wikipedia.org/wiki/Naive_Bayes_classifier)对数学公式有更详细的解释。
- en: 'In the following figure, we have shown the decision function of naive Bayes
    on our simple binary classification example:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们展示了朴素贝叶斯在我们简单的二元分类示例上的决策函数：
- en: '![](img/image_06_009.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_009.png)'
- en: Decision function of naive Bayes for binary classification
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯的决策函数用于二元分类
- en: 'Let''s train the StumbleUpon dataset by splitting it into 80% training and
    20% testing, use naive Bayes from Spark to build the model, and get evaluation
    metrics around the test data as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将StumbleUpon数据集分成80%的训练集和20%的测试集，使用Spark中的朴素贝叶斯构建模型，并在测试数据周围获取评估指标如下：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You will see the following output displayed:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出显示：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The complete code listing is available at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/NaiveBayesPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/NaiveBayesPipeline.scala).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码清单可在[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/NaiveBayesPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/NaiveBayesPipeline.scala)找到。
- en: 'The visualization of predicted and actual data in a two-dimensional scatter
    plot is shown here:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里显示了预测和实际数据的二维散点图的可视化：
- en: '![](img/image_06_010.png)![](img/image_06_011.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_010.png)![](img/image_06_011.png)'
- en: Decision trees
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: The Decision tree model is a powerful, non-probabilistic technique, which can
    capture more complex non-linear patterns and feature interactions. They have been
    shown to perform well on many tasks, are relatively easy to understand and interpret,
    can handle categorical and numerical features, and do not require input data to
    be scaled or standardized. They are well-suited to be included in ensemble methods
    (for example, ensembles of decision tree models, which are called decision forests).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树模型是一种强大的非概率技术，可以捕捉更复杂的非线性模式和特征交互。已经证明在许多任务上表现良好，相对容易理解和解释，可以处理分类和数值特征，并且不需要输入数据进行缩放或标准化。它们非常适合包含在集成方法中（例如，决策树模型的集成，称为决策森林）。
- en: The decision tree model constructs a tree, where the leaves represent a class
    assignment to class 0 or 1, and the branches are a set of features. In the following
    figure, we show a simple decision tree where the binary outcome is **Stay at home**
    or **Go to beach!**. The features are the weather outside.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树模型构建了一棵树，其中叶子表示对类0或1的类分配，分支是一组特征。在下图中，我们展示了一个简单的决策树，其中二元结果是**呆在家里**或**去海滩**。特征是外面的天气。
- en: '![](img/image_06_012.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_012.png)'
- en: A simple decision tree
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的决策树
- en: The decision tree algorithm is a top-down approach, which begins with a root
    node (or feature), and then selects a feature at each step that gives the best
    split of the dataset as measured by the information gain of this split. The information
    gain is computed from the node impurity (which is the extent to which the labels
    at the node are similar, or homogenous) minus the weighted sum of the impurities
    for the two child nodes that would be created by the split. For classification
    tasks, there are two measures that can be used to select the best split. These
    are Gini impurity and entropy.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法是自顶向下的方法，从根节点（或特征）开始，然后在每一步选择一个特征，该特征通过信息增益来衡量数据集的最佳分割。信息增益是从节点不纯度（标签在节点上相似或同质的程度）减去由分割创建的两个子节点的不纯度的加权和计算而来。对于分类任务，有两种可以用来选择最佳分割的度量。这些是基尼不纯度和熵。
- en: See the *ML Library - Decision Tree* section in the *Spark Programming Guide*
    at [http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier](http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier)
    for further details on the decision tree algorithm and impurity measures for classification.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有关决策树算法和分类的不纯度度量的更多细节，请参阅[http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier](http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier)中的*ML
    Library - Decision Tree*部分的*Spark编程指南*。
- en: 'In the following screenshot, we have plotted the decision boundary for the
    decision tree model, as we did for the other models earlier. We can see that the
    decision tree is able to fit complex, non-linear models:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，我们已经绘制了决策树模型的决策边界，就像我们之前对其他模型所做的那样。我们可以看到决策树能够拟合复杂的非线性模型：
- en: '![](img/image_06_013.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_013.png)'
- en: Decision function for a decision tree for binary classification
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类的决策树的决策函数
- en: 'Let''s train the StumbleUpon dataset by splitting it into 80% training and
    20% testing, use decision trees from Spark to build the model, and get evaluation
    metrics around the test data as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将StumbleUpon数据集分成80%的训练集和20%的测试集，使用Spark中的决策树构建模型，并在测试数据周围获取评估指标如下：
- en: '[PRE17]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You will see the following output displayed:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出显示：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The code listing is available at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/DecisionTreePipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/DecisionTreePipeline.scala).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单可在[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/DecisionTreePipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/DecisionTreePipeline.scala)找到。
- en: 'The visualization of predicted and actual data in a two-dimensional scatter
    plot is shown in the following figures:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中显示了二维散点图中预测和实际数据的可视化：
- en: '![](img/image_06_014.png)![](img/image_06_015.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_014.png)![](img/image_06_015.png)'
- en: Ensembles of trees
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树的集成
- en: 'The ensemble method is a machine learning algorithm that creates a model composed
    of a set of other base models. Spark machine learning supports two major ensemble
    algorithms: RandomForest and GradientBoostedTrees.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法是一种机器学习算法，它创建由一组其他基本模型组成的模型。Spark机器学习支持两种主要的集成算法：随机森林和梯度提升树。
- en: Random Forests
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: Random Forests are known as ensembles of decision trees, formed by combining
    many decision trees. Like decision trees, random forests can handle categorical
    features, support multiclass classification, and don't require feature scaling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林被称为决策树的集成，由许多决策树组成。与决策树一样，随机森林可以处理分类特征，支持多类别分类，并且不需要特征缩放。
- en: Spark ML supports random forests for both binary and multiclass classification
    and regression using both continuous and categorical features.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML支持随机森林用于二元和多类别分类以及使用连续和分类特征进行回归。
- en: Let's train the sample lib SVM data by splitting it into 80% training and 20%
    testing, use Random Forest Classifier from Spark to build the model, and get evaluation
    metrics around the test data. The model can be persisted and loaded for later
    use.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将样本lib SVM数据分为80%的训练和20%的测试，使用Spark中的随机森林分类器来构建模型，并获得关于测试数据的评估指标。模型可以被持久化并加载以供以后使用。
- en: 'Let''s train the StumbleUpon dataset by splitting it into 80% training and
    20% testing, use Random Forest Trees from Spark to build the model, and get evaluation
    metrics around the test data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将StumbleUpon数据集分为80%的训练和20%的测试，使用Spark中的随机森林树来构建模型，并获得关于测试数据的评估指标：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You will see the following output displayed:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出显示：
- en: '[PRE20]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The complete code listing is available at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/RandomForestPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/RandomForestPipeline.scala).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码清单可在[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/RandomForestPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/RandomForestPipeline.scala)找到。
- en: 'The visualization of predicted and actual data in a two-dimensional scatter
    plot is shown here:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里显示了二维散点图中的预测和实际数据的可视化：
- en: '![](img/image_06_016.png)![](img/image_06_017.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_016.png)![](img/image_06_017.png)'
- en: Gradient-Boosted Trees
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: Gradient-Boosted Trees are ensembles of decision trees. Gradient-Boosted Trees
    iteratively train decision trees to minimize loss function. Gradient-Boosted Trees
    handle categorical features, support multiclass classification, and don't require
    feature scaling.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升树是决策树的集成。梯度提升树迭代训练决策树以最小化损失函数。梯度提升树处理分类特征，支持多类别分类，并且不需要特征缩放。
- en: Spark ML implements Gradient-Boosted Trees using the existing decision tree
    implementation. It supports both classification and regression.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML使用现有的决策树实现梯度提升树。它支持分类和回归。
- en: 'Let''s train the StumbleUpon dataset by splitting it into 80% training and
    20% testing, use Gradient-Boosted Trees from Spark to build the model, and get
    evaluation metrics around the test data as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将StumbleUpon数据集分为80%的训练和20%的测试，使用Spark中的梯度提升树来构建模型，并获得关于测试数据的评估指标：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You will see the following output displayed:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出显示：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The code listing can be found at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/GradientBoostedTreePipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/GradientBoostedTreePipeline.scala).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单可以在[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/GradientBoostedTreePipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/GradientBoostedTreePipeline.scala)找到。
- en: 'The visualization of predictions in a two-dimensional scatter plot is shown
    in the following figures:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图表中显示了二维散点图中的预测可视化：
- en: '![](img/image_06_018.png)![](img/image_06_019.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_018.png)![](img/image_06_019.png)'
- en: Multilayer perceptron classifier
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器分类器
- en: The neural network is a complex adaptive system, which changes its internal
    structure based on the information flowing through it using weights. Optimizing
    the weights of a multilayered neural network is called **backpropagation**. Backpropagation
    is a bit beyond the scope of this book, and involves an activation function and
    basic calculus.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一个复杂的自适应系统，它根据通过它流动的信息改变其内部结构，使用权重。优化多层神经网络的权重称为**反向传播**。反向传播略微超出了本书的范围，并涉及激活函数和基本微积分。
- en: The multilayer perceptron classifier is based on a feed-forward artificial neural
    network. It consists of multiple layers. Each neural layer is completely connected
    to the next neural layer in the network, and nodes in the input layer denote the
    input data. All other nodes map inputs to the outputs by performing a linear combination
    of the inputs with the nodes' weights and bias, and by applying an activation
    or link function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器分类器基于前馈人工神经网络。它由多个层组成。每个神经层与网络中的下一个神经层完全连接，输入层中的节点表示输入数据。所有其他节点通过使用节点的权重和偏差执行输入的线性组合，并应用激活或链接函数将输入映射到输出。
- en: 'Let''s train the sample `libsvm` data by splitting it into 80% training and
    20% testing, use Multi Layer Perceptron classifier from Spark to build the model,
    and get evaluation metrics around the test data:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将样本`libsvm`数据分为80%的训练和20%的测试，使用Spark中的多层感知器分类器来构建模型，并获得关于测试数据的评估指标：
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You will see the following output displayed:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出显示：
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The code listing is available at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/MultilayerPerceptronClassifierExample.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/MultilayerPerceptronClassifierExample.scala).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单可在[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/MultilayerPerceptronClassifierExample.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/stumbleupon/MultilayerPerceptronClassifierExample.scala)找到。
- en: Before we proceed further, please note that the following examples for Feature
    Extraction and Classification use the MLLib package from Spark v1.6\. Kindly follow
    the code listing mentioned earlier to use Spark v2.0 Dataframe-based APIs. As
    of Spark 2.0, RDD-based APIs have entered maintenance mode.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步进行之前，请注意以下特征提取和分类的示例使用Spark v1.6中的MLLib包。请按照之前提到的代码清单来使用Spark v2.0基于Dataframe的API。截至Spark
    2.0，基于RDD的API已进入维护模式。
- en: Extracting the right features from your data
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据中提取正确的特征
- en: You might recall from [Chapter 4](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml),
    *Obtaining, Processing, and Preparing Data with Spark*, that the majority of machine
    learning models operate on numerical data in the form of feature vectors. In addition,
    for supervised learning methods such as classification and regression, we need
    to provide the target variable (or variables in the case of multiclass situations)
    together with the feature vector.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得[第4章](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml)中的内容，*使用Spark获取、处理和准备数据*，大多数机器学习模型都是在特征向量的数值数据上操作的。此外，对于监督学习方法，如分类和回归，我们需要提供目标变量（或在多类情况下的变量）以及特征向量。
- en: Classification models in MLlib operate on instances of `LabeledPoint`, which
    is a wrapper around the target variable (called `label`) and the `feature` vector.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib中的分类模型操作`LabeledPoint`的实例，它是目标变量（称为`label`）和`feature`向量的包装器。
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: While in most examples of using classification, you will come across existing
    datasets that are already in the vector format, in practice, you will usually
    start with raw data that needs to be transformed into features. As we have already
    seen, this can involve preprocessing and transformation such as binning numerical
    features, scaling and normalizing features, and using 1-of-k encodings for categorical
    features.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数分类使用的示例中，您将遇到已经以向量格式存在的现有数据集，但在实践中，您通常会从需要转换为特征的原始数据开始。正如我们已经看到的，这可能涉及预处理和转换，如对数值特征进行分箱处理，对特征进行缩放和归一化，以及对分类特征使用1-of-k编码。
- en: Training classification models
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练分类模型
- en: Now that we have extracted some basic features from our dataset and created
    our input RDD, we are ready to train a number of models. To compare the performance
    and use of different models, we will train a model using logistic regression,
    SVM, naive Bayes, and a decision tree. You will notice that training each model
    looks nearly identical, although each has its own specific model parameters, which
    can be set. Spark ML sets sensible defaults in most cases, but in practice, the
    best parameter setting should be selected using evaluation techniques, which we
    will cover later in this chapter.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从数据集中提取了一些基本特征并创建了我们的输入RDD，我们已经准备好训练多个模型了。为了比较不同模型的性能和使用情况，我们将使用逻辑回归、SVM、朴素贝叶斯和决策树来训练一个模型。您会注意到，训练每个模型看起来几乎相同，尽管每个模型都有自己特定的模型参数，可以设置。Spark
    ML在大多数情况下设置了合理的默认值，但在实践中，最佳参数设置应该使用评估技术来选择，我们将在本章后面介绍。
- en: Training a classification model on the Kaggle/StumbleUpon evergreen classification
    dataset
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kaggle/StumbleUpon永久分类数据集上训练分类模型
- en: We can now apply the models from Spark ML to our input data. First, we need
    to import the required classes, and set up some minimal input parameters for each
    model. For logistic regression and SVM, this is the number of iterations while,
    for the decision tree model, it is the maximum tree depth.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将Spark ML中的模型应用于我们的输入数据。首先，我们需要导入所需的类，并为每个模型设置一些最小输入参数。对于逻辑回归和SVM，这是迭代次数，而对于决策树模型，这是最大树深度。
- en: '[PRE26]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, train each model in turn. First, we will train logistic regression as
    follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，依次训练每个模型。首先，我们将训练逻辑回归模型如下：
- en: '[PRE27]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You will see the following output:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出：
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next up, we will train an SVM model like this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将这样训练一个SVM模型：
- en: '[PRE29]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You will now see the following output:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在将看到以下输出：
- en: '[PRE30]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, we will train the naive Bayes model; remember to use your special non-negative
    feature dataset:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将训练朴素贝叶斯模型；请记住使用您特殊的非负特征数据集：
- en: '[PRE31]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is the output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Finally, we will train our decision tree.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将训练我们的决策树。
- en: '[PRE33]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Notice that we set the mode or Algo of the decision tree to `Classification`,
    and we used the `Entropy` impurity measure.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将决策树的模式或算法设置为`Classification`，并使用`Entropy`不纯度度量。
- en: Using classification models
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分类模型
- en: We now have four models trained on our input labels and features. We will now
    see how to use these models to make predictions on our dataset. For now, we will
    use the same training data to illustrate the predict method of each model.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经对我们的输入标签和特征进行了四个模型的训练。现在我们将看到如何使用这些模型对我们的数据集进行预测。目前，我们将使用相同的训练数据来说明每个模型的预测方法。
- en: Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为Kaggle/StumbleUpon永久分类数据集生成预测
- en: 'We will use our logistic regression model as an example (the other models are
    used in the same way):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以逻辑回归模型为例（其他模型使用方式相同）：
- en: '[PRE35]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following is the output:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE36]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We saw that, for the first data point in our training dataset, the model predicted
    a label of 1 (that is, evergreen). Let's examine the true label for this data
    point.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在我们的训练数据集中，第一个数据点的模型预测标签为1（即永久）。让我们检查这个数据点的真实标签。
- en: '[PRE37]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You can see the following output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到以下输出：
- en: '[PRE38]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: So, in this case, our model got it wrong!
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这种情况下，我们的模型出错了！
- en: 'We can also make predictions in bulk by passing in an `RDD[Vector]` as input:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过传入`RDD[Vector]`来批量进行预测：
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The following is the output:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE40]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Evaluating the performance of classification models
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估分类模型的性能
- en: When we make predictions using our model, as we did earlier, how do we know
    whether the predictions are good or not? We need to be able to evaluate how well
    our model performs. Evaluation metrics commonly used in binary classification
    include prediction accuracy and error, precision and recall, the area under the
    precision-recall curve, the receiver operating characteristic (ROC) curve, the
    area under ROC curve (AUC), and the F-measure.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用我们的模型进行预测时，就像我们之前做的那样，我们如何知道预测是好还是不好？我们需要能够评估我们的模型的表现如何。在二元分类中常用的评估指标包括预测准确度和错误、精确度和召回率、精确-召回曲线下面积、接收器操作特征（ROC）曲线、ROC曲线下面积（AUC）和F-度量。
- en: Accuracy and prediction error
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率和预测错误
- en: The prediction error for binary classification is possibly the simplest measure
    available. It is the number of training examples that are misclassified, divided
    by the total number of examples. Similarly, accuracy is the number of correctly
    classified examples divided by the total examples.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类的预测错误可能是可用的最简单的度量。它是被错误分类的训练示例数除以总示例数。同样，准确率是正确分类的示例数除以总示例数。
- en: We can calculate the accuracy of our models in our training data by making predictions
    on each input feature and comparing them to the true label. We will sum up the
    number of correctly classified instances, and divide this by the total number
    of data points to get the average classification accuracy.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对每个输入特征进行预测并将其与真实标签进行比较，我们可以计算我们在训练数据中模型的准确率。我们将总结正确分类的实例数，并将其除以数据点的总数以获得平均分类准确率。
- en: '[PRE41]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE42]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This gives us 51.5 percent accuracy, which doesn't look particularly impressive!
    Our model got only half of the training examples correct, which seems to be about
    as good as a random chance.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了51.5％的准确率，看起来并不特别令人印象深刻！我们的模型只正确分类了一半的训练示例，这似乎与随机机会一样好。
- en: The predictions made by the model are not naturally exactly 1 or 0\. The output
    is usually a real number that must be turned into a class prediction. This is
    done through the use of a threshold in the classifier's decision or scoring function.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 模型做出的预测通常不是完全1或0。输出通常是一个实数，必须转换为类预测。这是通过分类器的决策或评分函数中的阈值来实现的。
- en: For example, binary logistic regression is a probabilistic model, which returns
    the estimated probability of class 1 in its scoring function. Thus, a decision
    threshold of 0.5 is typical. That is, if the estimated probability of being in
    class 1 is higher than 50 percent, the model decides to classify the point as
    class 1; otherwise, it will be classified as class 0.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，二元逻辑回归是一个概率模型，它在评分函数中返回类1的估计概率。因此，典型的决策阈值为0.5。也就是说，如果被估计为类1的概率高于50％，模型决定将该点分类为类1；否则，它将被分类为类0。
- en: The threshold itself is effectively a model parameter that can be tuned in some
    models. It also plays a role in evaluation measures, as we will see now.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值本身实际上是一种可以在某些模型中进行调整的模型参数。它还在评估度量中发挥作用，我们现在将看到。
- en: 'What about the other models? Let''s compute the accuracy for the other three:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 其他模型呢？让我们计算其他三个的准确率：
- en: '[PRE43]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Note that the decision tree prediction threshold needs to be specified explicitly,
    as highlighted here:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，决策树预测阈值需要明确指定，如下所示：
- en: '[PRE44]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can now inspect the accuracy for the other three models. First, the SVM
    model, which is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以检查其他三个模型的准确性。首先是SVM模型，如下所示：
- en: '[PRE45]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here is the output for the SVM model:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是SVM模型的输出：
- en: '[PRE46]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Next, is our naive Bayes model.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的朴素贝叶斯模型。
- en: '[PRE47]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE48]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Finally, we compute the accuracy for the decision tree:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算决策树的准确率：
- en: '[PRE49]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is this:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE50]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We can see that both SVM and naive Bayes also performed quite poorly. The decision
    tree model is better with 65% accuracy, but this is still not particularly high.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到SVM和朴素贝叶斯的表现也相当糟糕。决策树模型的准确率为65％，但这仍然不是特别高。
- en: Precision and recall
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: In information retrieval, precision is a commonly used measure of the quality
    of the results, while recall is a measure of the completeness of the results.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息检索中，精确度是结果质量的常用度量，而召回率是结果完整性的度量。
- en: In the binary classification context, precision is defined as the number of
    true positives (that is, the number of examples correctly predicted as class 1)
    divided by the sum of true positives and false positives (that is, the number
    of examples that were incorrectly predicted as class 1). Thus, we can see that
    a precision of 1.0 (or 100%) is achieved if every example predicted by the classifier
    to be class 1 is, in fact, in class 1 (that is, there are no false positives).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类环境中，精确度被定义为真正例数（即被正确预测为类1的示例数）除以真正例数和假正例数之和（即被错误预测为类1的示例数）。因此，我们可以看到，如果分类器预测为类1的每个示例实际上都是类1（即没有假正例），则可以实现1.0（或100％）的精确度。
- en: Recall is defined as the number of true positives divided by the sum of true
    positives and false negatives (that is, the number of examples that were in class
    1, but were predicted as class 0 by the model). We can see that a recall of 1.0
    (or 100%) is achieved if the model doesn't miss any examples that were in class
    1 (that is, there are no false negatives).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率被定义为真正例数除以真正例数和假反例数之和（即模型错误预测为类0的实例数）。我们可以看到，如果模型没有错过任何属于类1的示例（即没有假反例），则可以实现1.0（或100％）的召回率。
- en: Generally, precision and recall are inversely related; often, higher precision
    is related to lower recall and vice versa. To illustrate this, assume that we
    built a model that always predicted class 1\. In this case, the model predictions
    would have no false negatives, because the model always predicts 1; it will not
    miss any of class 1\. Thus, the recall will be 1.0 for this model. On the other
    hand, the false positive rate could be very high, meaning precision would be low
    (this depends on the exact distribution of the classes in the dataset).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，精确度和召回率是相互关联的；通常，较高的精确度与较低的召回率相关，反之亦然。为了说明这一点，假设我们构建了一个总是预测类别1的模型。在这种情况下，模型预测将没有假阴性，因为模型总是预测1；它不会错过任何类别1。因此，对于这个模型，召回率将为1.0。另一方面，假阳性率可能非常高，这意味着精确度会很低（这取决于数据集中类的确切分布）。
- en: Precision and recall are not particularly useful as standalone metrics, but
    are typically used together to form an aggregate or averaged metric. Precision
    and recall are also dependent on the threshold selected for the model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度和召回率作为独立的度量并不特别有用，但通常一起使用以形成一个聚合或平均度量。精确度和召回率也依赖于模型选择的阈值。
- en: Intuitively, the following are some threshold levels where a model will always
    predict class 1\. Hence, it will have a recall of 1, but most likely, it will
    have low precision. At a high enough threshold, the model will always predict
    class 0\. The model will then have a recall of 0, since it cannot achieve any
    true positives, and will likely have many false negatives. Furthermore, its precision
    score will be undefined, as it will achieve zero true positives and zero false
    positives.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，以下是一些模型将始终预测类别1的阈值水平。因此，它将具有召回率为1，但很可能精确度较低。在足够高的阈值下，模型将始终预测类别0。然后，模型将具有召回率为0，因为它无法实现任何真阳性，并且可能有许多假阴性。此外，其精确度得分将是未定义的，因为它将实现零真阳性和零假阳性。
- en: '**The precision-recall** (**PR**) curve shown in the following figure plots
    precision against the recall outcomes for a given model, as the decision threshold
    of the classifier is changed. The area under this PR curve is referred to as the
    average precision. Intuitively, an area under the PR curve of 1.0 will equate
    to a perfect classifier that will achieve 100 percent in both precision and recall.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度-召回率**（**PR**）曲线在下图中绘制了给定模型的精确度与召回率结果，随着分类器的决策阈值的改变。这个PR曲线下的面积被称为平均精度。直观地，PR曲线下的面积为1.0将等同于一个完美的分类器，将实现100%的精确度和召回率。'
- en: '![](img/image_06_020.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_020.png)'
- en: Precision-recall curve
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度-召回率曲线
- en: See [http://en.wikipedia.org/wiki/Precision_and_recall](http://en.wikipedia.org/wiki/Precision_and_recall)
    and [http://en.wikipedia.org/wiki/Average_precision#Average_precision](http://en.wikipedia.org/wiki/Average_precision)
    for more details on precision, recall, and area under the PR curve.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[http://en.wikipedia.org/wiki/Precision_and_recall](http://en.wikipedia.org/wiki/Precision_and_recall)和[http://en.wikipedia.org/wiki/Average_precision#Average_precision](http://en.wikipedia.org/wiki/Average_precision)以获取有关精确度、召回率和PR曲线下面积的更多详细信息。
- en: ROC curve and AUC
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC曲线和AUC
- en: The ROC curve is a concept similar to the PR curve. It is a graphical illustration
    of the true positive rate against the false positive rate for a classifier.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线是与PR曲线类似的概念。它是分类器的真阳性率与假阳性率的图形表示。
- en: '**The true positive rate** (**TPR**) is the number of true positives divided
    by the sum of true positives and false negatives. In other words, it is the ratio
    of true positives to all positive examples. This is the same as the recall we
    saw earlier, and is also commonly referred to as sensitivity.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**真阳性率**（**TPR**）是真阳性的数量除以真阳性和假阴性的总和。换句话说，它是真阳性与所有正例的比率。这与我们之前看到的召回率相同，通常也被称为灵敏度。'
- en: '**The false positive rate** (**FPR**) is the number of false positives divided
    by the sum of false positives and true negatives (that is, the number of examples
    correctly predicted as class 0). In other words, it is the ratio of false positives
    to all negative examples.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**假阳性率**（**FPR**）是假阳性的数量除以假阳性和真阴性的总和（即正确预测为类别0的示例数量）。换句话说，它是假阳性与所有负例的比率。'
- en: In a manner similar to precision and recall, the ROC curve (plotted in the following
    figure) represents the classifier's performance trade-off of TPR against FPR,
    for different decision thresholds. Each point on the curve represents a different
    threshold in the decision function for the classifier.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 与精确度和召回率类似，ROC曲线（在下图中绘制）表示分类器在不同决策阈值下TPR与FPR的性能折衷。曲线上的每个点代表分类器决策函数中的不同阈值。
- en: '![](img/image_06_021.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_021.png)'
- en: The ROC curve
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线
- en: The area under the ROC curve (commonly referred to as AUC) represents an average
    value. Again, an AUC of 1.0 will represent a perfect classifier. An area of 0.5
    is referred to as the random score. Thus, a model that achieves an AUC of 0.5
    is no better than guessing randomly.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线下的面积（通常称为AUC）代表了一个平均值。同样，AUC为1.0将代表一个完美的分类器。面积为0.5被称为随机分数。因此，实现AUC为0.5的模型不比随机猜测更好。
- en: As both, the area under the PR curve and the area under the ROC curve, are effectively
    normalized (with a minimum of 0 and maximum of 1), we can use these measures to
    compare models with differing parameter settings, and even compare completely
    different models. Thus, these metrics are popular for model evaluation and selection
    purposes.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PR曲线下面积和ROC曲线下面积都被有效地归一化（最小为0，最大为1），我们可以使用这些度量来比较具有不同参数设置的模型，甚至比较完全不同的模型。因此，这些指标在模型评估和选择方面很受欢迎。
- en: 'MLlib comes with a set of built-in routines to compute the area under the PR
    and ROC curves for binary classification. Here, we will compute these metrics
    for each of our models:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib带有一组内置例程，用于计算二元分类的PR曲线和ROC曲线下的面积。在这里，我们将为我们的每个模型计算这些度量：
- en: '[PRE51]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As we did previously to train the naive Bayes model and computing accuracy,
    we need to use the special `nbData` version of the dataset that we created to
    compute the classification metrics.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前训练朴素贝叶斯模型和计算准确率一样，我们需要使用我们创建的`nbData`版本的数据集来计算分类指标。
- en: '[PRE52]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Note that because the `DecisionTreeModel` model does not implement the `ClassificationModel`
    interface that is implemented by the other three models, we need to compute the
    results separately for this model in the following code:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，因为`DecisionTreeModel`模型没有实现其他三个模型实现的`ClassificationModel`接口，我们需要在以下代码中单独计算该模型的结果：
- en: '[PRE53]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Your output will look similar to the one here:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你的输出将类似于这里的输出：
- en: '[PRE54]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We can see that all models achieve broadly similar results for the average precision
    metric.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，所有模型在平均精度指标上取得了大致相似的结果。
- en: Logistic regression and SVM achieve results of around 0.5 for AUC. This indicates
    that they do no better than random chance! Our naive Bayes and decision tree models
    fare a little better, achieving an AUC of 0.58 and 0.65, respectively. Still,
    this is not a very good result in terms of binary classification performance.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归和支持向量机的AUC结果约为0.5。这表明它们的表现甚至不如随机机会！我们的朴素贝叶斯和决策树模型稍微好一些，分别达到了0.58和0.65的AUC。但就二元分类性能而言，这仍然不是一个很好的结果。
- en: While we don't cover multiclass classification here, MLlib provides a similar
    evaluation class called `MulticlassMetrics`, which provides averaged versions
    of many common metrics.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这里没有涉及多类分类，但MLlib提供了一个类似的评估类，称为`MulticlassMetrics`，它提供了许多常见指标的平均版本。
- en: Improving model performance and tuning parameters
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进模型性能和调整参数
- en: So, what went wrong? Why have our sophisticated models achieved nothing better
    than random chance? Is there a problem with our models?
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，出了什么问题？为什么我们复杂的模型的表现甚至不如随机机会？我们的模型有问题吗？
- en: Recall that we started out by just throwing the data at our model. In fact,
    we didn't even throw all our data at the model, just the numeric columns that
    were easy to use. Furthermore, we didn't do a lot of analysis on these numeric
    features.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们最初只是将数据投放到我们的模型中。事实上，我们甚至没有将所有数据都投放到模型中，只是那些易于使用的数值列。此外，我们对这些数值特征没有进行大量分析。
- en: Feature standardization
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征标准化
- en: Many models that we employ make inherent assumptions about the distribution
    or scale of input data. One of the most common forms of assumption is about normally-distributed
    features. Let's take a deeper look at the distribution of our features.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的许多模型对输入数据的分布或规模做出了固有的假设。其中最常见的假设形式之一是关于正态分布特征的。让我们更深入地研究一下我们特征的分布。
- en: To do this, we can represent the feature vectors as a distributed matrix in
    MLlib, using the `RowMatrix` class. `RowMatrix` is an RDD made up of vectors,
    where each vector is a row of our matrix.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以将特征向量表示为MLlib中的分布矩阵，使用`RowMatrix`类。`RowMatrix`是由向量组成的RDD，其中每个向量是矩阵的一行。
- en: The `RowMatrix` class comes with some useful methods to operate on the matrix,
    one of which is a utility to compute statistics on the columns of the matrix.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`RowMatrix`类带有一些有用的方法来操作矩阵，其中之一是在矩阵的列上计算统计数据的实用程序。'
- en: '[PRE55]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The following code statement will print the mean of the matrix:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码语句将打印矩阵的均值：
- en: '[PRE56]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Here is the output:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE57]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The following code statement will print the minimum value of the matrix:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码语句将打印矩阵的最小值：
- en: '[PRE58]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Here is the output:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE59]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The following code statement will print the maximum value of the matrix:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码语句将打印矩阵的最大值：
- en: '[PRE60]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output is as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE61]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The following code statement will print the variance of the matrix:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码语句将打印矩阵的方差：
- en: '[PRE62]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output of the variance is:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 方差的输出是：
- en: '[PRE63]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The following code statement will print the non-zero number of the matrix:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码语句将打印矩阵的非零数：
- en: '[PRE64]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Here is the output:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE65]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The `computeColumnSummaryStatistics` method computes a number of statistics
    over each column of features including the mean and variance, storing each of
    these in a vector with one entry per column (that is, one entry per feature in
    our case).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '`computeColumnSummaryStatistics`方法计算特征的各列统计数据，包括均值和方差，并将每个统计数据存储在一个向量中，每列一个条目（也就是在我们的情况下，每个特征一个条目）。'
- en: 'Looking at the preceding output for mean and variance, we can see quite clearly
    that the second feature has a much higher mean and variance than some of the other
    features (you will find a few other features that are similar, and a few others
    that are more extreme). So, our data definitely does not conform to a standard
    Gaussian distribution in its raw form. To get the data in a more suitable form
    for our models, we can standardize each feature such that it has zero mean and
    unit standard deviation. We can do this by subtracting the column mean from each
    feature value, and then scaling it by dividing it by the column standard deviation
    for the feature as follows:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的均值和方差输出中，我们可以清楚地看到第二个特征的均值和方差比其他一些特征要高得多（你会发现还有一些其他类似的特征，还有一些更极端的特征）。因此，我们的数据在原始形式下明显不符合标准的高斯分布。为了使数据更适合我们的模型，我们可以对每个特征进行标准化，使其均值为零，标准差为单位。我们可以通过以下方式实现：从每个特征值中减去列均值，然后除以特征的列标准差。
- en: '*(x - μ) / sqrt(variance)*'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*(x - μ) / sqrt(variance)*'
- en: Practically, for each feature vector in our input dataset, we can simply perform
    an element-wise subtraction of the preceding mean vector from the feature vector,
    and then perform an element-wise division of the feature vector by the vector
    of feature standard deviations. The standard deviation vector itself can be obtained
    by performing an element-wise square root operation on the variance vector.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对于输入数据集中的每个特征向量，我们可以简单地对先前的均值向量进行逐元素减法运算，然后对特征向量进行逐元素除法运算，除以特征标准差向量。标准差向量本身可以通过对方差向量进行逐元素平方根运算得到。
- en: As we mentioned in [Chapter 4](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml),
    *Obtaining, Processing, and Preparing Data with Spark*, we fortunately have access
    to a convenience method from Spark's `StandardScaler` to accomplish this.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml)中提到的，*使用Spark获取、处理和准备数据*，我们幸运地可以访问Spark的`StandardScaler`的便利方法来完成这个任务。
- en: '`StandardScaler` works in much the same way as the Normalizer feature we used
    in that chapter. We will instantiate it by passing in two arguments that tell
    it whether to subtract the mean from the data, and whether to apply standard deviation
    scaling. We will then fit `StandardScaler` on our input vectors. Finally, we will
    pass in an input vector to the transform function, which will then return a normalized
    vector. We will do this within the following `map` function to preserve the `label`
    from our dataset:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`StandardScaler`的工作方式与我们在该章节中使用的Normalizer特征基本相同。我们将通过传入两个参数来实例化它，告诉它是否从数据中减去平均值，以及是否应用标准差缩放。然后，我们将在我们的输入向量上拟合`StandardScaler`。最后，我们将在`transform`函数中传入一个输入向量，然后返回一个标准化向量。我们将在以下`map`函数中执行此操作，以保留数据集中的`label`：'
- en: '[PRE66]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Our data should now be standardized. Let's inspect the first row of the original
    and standardized features.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在应该是标准化的。让我们检查原始和标准化特征的第一行。
- en: '[PRE67]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output of the preceding line of code is as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 前面一行代码的输出如下：
- en: '[PRE68]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The following code will be the first row of the standardized features:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将是标准化特征的第一行：
- en: '[PRE69]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output is as follows:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE70]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: As we can see, the first feature has been transformed by applying the standardization
    formula. We can check this by subtracting the mean (which we computed earlier)
    from the first feature, and dividing the result by the square root of the variance
    (which we computed earlier).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，通过应用标准化公式，第一个特征已经被转换。我们可以通过从第一个特征中减去平均值（我们之前计算过的）并将结果除以方差的平方根（我们之前计算过的）来检查这一点。
- en: '[PRE71]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The result should be equal to the first element of our scaled vector:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该等于我们缩放向量的第一个元素：
- en: '[PRE72]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: We can now retrain our model using the standardized data. We will use only the
    logistic regression model to illustrate the impact of feature standardization
    (since the decision tree and naive Bayes are not impacted by this).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用标准化的数据重新训练我们的模型。我们将仅使用逻辑回归模型来说明特征标准化的影响（因为决策树和朴素贝叶斯不受此影响）。
- en: '[PRE73]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The result should look similar to this:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该看起来类似于这样：
- en: '[PRE74]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Simply through standardizing our features, we have improved the logistic regression
    performance for accuracy and AUC from 50%, no better than random, to 62%.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅通过对特征进行标准化，我们已经将逻辑回归的准确性和AUC从50%（不比随机好）提高到了62%。
- en: Additional features
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的特征
- en: We have seen that we need to be careful about standardizing and potentially
    normalizing our features, and the impact on model performance can be serious.
    In this case, we used only a portion of the features available. For example, we
    completely ignored the category variable and the textual content in the boilerplate
    variable column.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，我们需要小心地对特征进行标准化和可能的归一化，对模型性能的影响可能很严重。在这种情况下，我们仅使用了部分可用的特征。例如，我们完全忽略了类别变量和boilerplate变量列中的文本内容。
- en: This was done for ease of illustration, but let's assess the impact of adding
    an additional feature such as the category feature.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为了便于说明而做的，但让我们评估添加额外特征（如类别特征）的影响。
- en: 'First, we will inspect the categories, and form a mapping of index to category,
    which you might recognize as the basis for a 1-of-k encoding of this categorical
    feature:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将检查类别，并形成一个索引到类别的映射，您可能会认识到这是对这个分类特征进行1-of-k编码的基础：
- en: '[PRE75]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output of the different categories is as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类别的输出如下：
- en: '[PRE76]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The following code will print the number of categories:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将打印类别的数量：
- en: '[PRE77]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Here is the output:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE78]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'So, we will need to create a vector of length 14 to represent this feature,
    and assign a value of 1 for the index of the relevant category for each data point.
    We can then prepend this new feature vector to the vector of other numerical features,
    as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要创建一个长度为14的向量来表示这个特征，并为每个数据点的相关类别的索引分配一个值为1。然后，我们可以将这个新的特征向量放在其他数值特征向量的前面，如下所示：
- en: '[PRE79]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: You should see output similar to what is shown here. You can see that the first
    part of our feature vector is now a vector of length 14 with one nonzero entry
    at the relevant category index.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似于这里显示的输出。您可以看到我们特征向量的第一部分现在是一个长度为14的向量，其中在相关类别索引处有一个非零条目。
- en: '[PRE80]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Again, since our raw features are not standardized, we should perform this
    transformation using the same `StandardScaler` approach that we used earlier before
    training a new model on this expanded dataset:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，由于我们的原始特征没有标准化，我们应该在对这个扩展数据集进行新模型训练之前，使用与之前相同的`StandardScaler`方法进行转换：
- en: '[PRE81]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: We can inspect the features before and after scaling as we did earlier.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像之前一样检查缩放前后的特征。
- en: '[PRE82]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output is as follows:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE83]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The following code will print the features after scaling:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将打印缩放后的特征：
- en: '[PRE84]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'You will see the following on the screen:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在屏幕上看到以下内容：
- en: '[PRE85]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: While the original raw features were sparse (that is, there are many entries
    that are zero), if we subtract the mean from each entry, we would end up with
    a non-sparse (dense) representation, as can be seen in the preceding example.
    This is not a problem in this case as the data size is small, but often large-scale
    real-world problems have extremely sparse input data with many features (online
    advertising and text classification are good examples). In this case, it is not
    advisable to lose this sparsity, as the memory and processing requirements for
    the equivalent dense representation can quickly explode with many millions of
    features. We can use `StandardScaler` and set `withMean` to `false` to avoid this.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然原始的原始特征是稀疏的（即有许多条目为零），但如果我们从每个条目中减去平均值，我们将得到一个非稀疏（密集）表示，就像前面的例子中所示的那样。在这种情况下，这并不是一个问题，因为数据规模很小，但通常大规模的现实世界问题具有极其稀疏的输入数据和许多特征（在线广告和文本分类是很好的例子）。在这种情况下，不建议失去这种稀疏性，因为等效的密集表示的内存和处理要求可能会随着许多百万特征的增加而迅速增加。我们可以使用`StandardScaler`并将`withMean`设置为`false`来避免这种情况。
- en: We're now ready to train a new logistic regression model with our expanded feature
    set, and then we will evaluate the performance.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备使用扩展的特征集训练一个新的逻辑回归模型，然后我们将评估其性能。
- en: '[PRE86]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'You should see output similar to this one:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似于这样的输出：
- en: '[PRE87]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: By applying a feature standardization transformation to our data, we improved
    both the accuracy and AUC measures from 50% to 62%, and then, we achieved a further
    boost to 66% by adding the category feature into our model (remember to apply
    the standardization to our new feature set).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对我们的数据应用特征标准化转换，我们将准确度和AUC指标从50%提高到62%，然后通过将类别特征添加到我们的模型中，我们进一步提高到了66%（记得对我们的新特征集应用标准化）。
- en: The best model performance in the competition was an AUC of 0.88906 (see [http://www.kaggle.com/c/stumbleupon/leaderboard/private](http://www.kaggle.com/c/stumbleupon/leaderboard/private)).
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛中最佳的模型性能是AUC为0.88906（请参阅[http://www.kaggle.com/c/stumbleupon/leaderboard/private](http://www.kaggle.com/c/stumbleupon/leaderboard/private)）。
- en: One approach to achieving performance almost as high is outlined at [http://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878](http://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在[http://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878](http://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878)中概述了实现几乎与最高性能相当的方法。
- en: Notice that there are still features that we have not yet used; most notably,
    the text features in the boilerplate variable. The leading competition submissions
    predominantly use the boilerplate features and features based on the raw textual
    content to achieve their performance. As we saw earlier, while adding category-improved
    performance, it appears that most of the variables are not very useful as predictors,
    while the textual content turned out to be highly predictive.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们尚未使用的特征仍然存在；尤其是在boilerplate变量中的文本特征。领先的竞赛提交主要使用boilerplate特征和基于原始文本内容的特征来实现他们的性能。正如我们之前看到的，虽然添加类别可以提高性能，但大多数变量并不是很有用作预测因子，而文本内容却具有很高的预测性。
- en: Going through some of the best performing approaches for these competitions
    can give you a good idea as to how feature extraction and engineering play a critical
    role in model performance.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 研究一些在这些比赛中表现最佳的方法可以让您了解特征提取和工程在模型性能中起到了关键作用。
- en: Using the correct form of data
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正确的数据形式
- en: Another critical aspect of model performance is using the correct form of data
    for each model. Previously, we saw that applying a naive Bayes model to our numerical
    features resulted in very poor performance. Is this because the model itself is
    deficient?
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能的另一个关键方面是使用每个模型的正确数据形式。之前我们看到，将朴素贝叶斯模型应用于我们的数值特征会导致性能非常差。这是因为模型本身存在缺陷吗？
- en: In this case, recall that MLlib implements a multinomial model. This model works
    on input in the form of non-zero count data. This can include a binary representation
    of categorical features (such as the 1-of-k encoding covered previously) or frequency
    data (such as the frequency of occurrences of words in a document). The numerical
    features we used initially do not conform to this assumed input distribution,
    so it is probably unsurprising that the model did so poorly.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，请记住MLlib实现了一个多项式模型。该模型适用于非零计数数据的输入形式。这可以包括分类特征的二进制表示（例如之前介绍的1-of-k编码）或频率数据（例如文档中单词出现的频率）。我们最初使用的数值特征不符合这种假定的输入分布，因此模型表现不佳可能并不奇怪。
- en: 'To illustrate this, we''ll use only the category feature, which, when 1-of-k
    encoded, is of the correct form for the model. We will create a new dataset as
    follows:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们将仅使用类别特征，当进行1-of-k编码时，这符合模型的正确形式。我们将创建一个新的数据集，如下所示：
- en: '[PRE88]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Next, we will train a new naive Bayes model and evaluate its performance.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练一个新的朴素贝叶斯模型并评估其性能。
- en: '[PRE89]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'You should see the following output:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '[PRE90]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: So, by ensuring that we use the correct form of input, we have improved the
    performance of the naive Bayes model slightly from 58% to 60%.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过确保我们使用正确形式的输入，我们将朴素贝叶斯模型的性能略微从58%提高到60%。
- en: Tuning model parameters
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整模型参数
- en: The previous section showed the impact of feature extraction and selection on
    model performance as well as the form of input data and a model's assumptions
    around data distributions. So far, we have discussed model parameters only in
    passing, but they also play a significant role in model performance.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的部分展示了特征提取和选择对模型性能的影响，以及输入数据的形式和模型对数据分布的假设。到目前为止，我们只是简单地讨论了模型参数，但它们在模型性能中也起着重要作用。
- en: MLlib's default train methods use default values for the parameters of each
    model. Let's take a deeper look at them.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib的默认训练方法使用每个模型参数的默认值。让我们更深入地研究一下它们。
- en: Linear models
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型
- en: 'Both logistic regression and SVM share the same parameters, because they use
    the same underlying optimization technique of **stochastic gradient descent**
    (**SGD**). They differ only in the loss function applied. If we take a look at
    the class definition for logistic regression in MLlib, we will see the following
    definition:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归和支持向量机共享相同的参数，因为它们使用相同的**随机梯度下降**（**SGD**）的优化技术。它们只在应用的损失函数上有所不同。如果我们看一下MLlib中逻辑回归的类定义，我们会看到以下定义：
- en: '[PRE91]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: We can see that the arguments that can be passed to the constructor are `stepSize`,
    `numIterations`, `regParam`, and `miniBatchFraction`. Of these, all except `regParam`
    are related to the underlying optimization technique.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到可以传递给构造函数的参数是`stepSize`、`numIterations`、`regParam`和`miniBatchFraction`。其中，除了`regParam`之外，所有参数都与底层优化技术有关。
- en: The instantiation code for logistic regression initializes the `gradient`, `updater`,
    and `optimizer`, and sets the relevant arguments for `optimizer` (`GradientDescent`
    in this case).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的实例化代码初始化了`gradient`、`updater`和`optimizer`，并为`optimizer`（在本例中为`GradientDescent`）设置了相关参数。
- en: '[PRE92]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '`LogisticGradient` sets up the logistic loss function that defines our logistic
    regression model.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '`LogisticGradient`设置了定义我们逻辑回归模型的逻辑损失函数。'
- en: 'While a detailed treatment of optimization techniques is beyond the scope of
    this book, MLlib provides two optimizers for linear models: SGD and L-BFGS. L-BFGS
    is often more accurate, and has fewer parameters to tune.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对优化技术的详细处理超出了本书的范围，但MLlib为线性模型提供了两种优化器：SGD和L-BFGS。L-BFGS通常更准确，并且参数更少需要调整。
- en: SGD is the default, while L-BGFS can currently only be used directly for logistic
    regression via `LogisticRegressionWithLBFGS`. Try it out yourself, and compare
    the results to those found with SGD.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: SGD是默认值，而L-BGFS目前只能通过`LogisticRegressionWithLBFGS`直接用于逻辑回归。自己试一试，并将结果与SGD找到的结果进行比较。
- en: See [http://spark.apache.org/docs/latest/mllib-optimization.html](http://spark.apache.org/docs/latest/mllib-optimization.html)
    for further details.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参阅[http://spark.apache.org/docs/latest/mllib-optimization.html](http://spark.apache.org/docs/latest/mllib-optimization.html)。
- en: 'To investigate the impact of the remaining parameter settings, we will create
    a helper function, which will train a logistic regression model given a set of
    parameter inputs. First, we will import the required classes:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查剩余参数设置的影响，我们将创建一个辅助函数，它将根据一组参数输入训练逻辑回归模型。首先，我们将导入所需的类：
- en: '[PRE93]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Next, we will define our helper function to train a model given a set of inputs:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个辅助函数来训练给定一组输入的模型：
- en: '[PRE94]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Finally, we will create a second helper function to take the input data and
    a classification model, and generate the relevant AUC metrics:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将创建第二个辅助函数，以获取输入数据和分类模型，并生成相关的AUC指标：
- en: '[PRE95]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: We will also cache our scaled dataset, including categories, to speed up the
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将缓存我们的缩放数据集，包括类别，以加快速度
- en: 'multiple model training runs that we will be using to explore these different
    parameter settings, as follows:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用多个模型训练运行来探索这些不同的参数设置，如下所示：
- en: '[PRE96]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Iterations
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代
- en: 'Many machine learning methods are iterative in nature, converging to a solution
    (the optimal weight vector that minimizes the chosen loss function) over a number
    of iteration steps. SGD typically requires relatively few iterations to converge
    to a reasonable solution, but can be run for more iterations to improve the solution.
    We can see this by trying a few different settings for the `numIterations` parameter,
    and comparing the AUC results like this:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习方法都是迭代的，通过多次迭代收敛到一个解（最小化所选损失函数的最优权重向量）。SGD通常需要相对较少的迭代次数才能收敛到一个合理的解，但可以运行更多次迭代来改善解。我们可以通过尝试一些不同的`numIterations`参数设置，并像这样比较AUC结果来看到这一点：
- en: '[PRE97]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Your output should look like this:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 你的输出应该是这样的：
- en: '[PRE98]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: So, we can see that the number of iterations has a minor impact on the results
    once a certain number of iterations have been completed.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到一旦完成了一定数量的迭代，迭代次数对结果的影响很小。
- en: Step size
  id: totrans-420
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步长
- en: In SGD, the step size parameter controls how far in the direction of the steepest
    gradient the algorithm takes a step when updating the model weight vector after
    each training example. A larger step size might speed up convergence, but a step
    size that is too large might cause problems with convergence, as good solutions
    are overshot. The learning rate determines the size of the steps we take to reach
    a (local or global) minimum. In other words, we follow the direction of the slope
    of the surface created by the objective function downhill until we reach a valley.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在SGD中，步长参数控制算法在更新模型权重向量之后每个训练样本时所采取的步骤方向的梯度。较大的步长可能加快收敛，但步长太大可能会导致收敛问题，因为好的解决方案被超越。学习率确定我们采取的步骤大小，以达到（局部或全局）最小值。换句话说，我们沿着目标函数创建的表面的斜率方向向下走，直到我们到达一个山谷。
- en: 'We can see the impact of changing the step size here:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到改变步长的影响在这里：
- en: '[PRE99]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'This will give us the following results, which show that increasing the step
    size too much can begin to negatively impact performance:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下结果，显示增加步长太多可能开始对性能产生负面影响：
- en: '[PRE100]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Regularization
  id: totrans-426
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: We briefly touched on the `Updater` class in the preceding logistic regression
    code. An `Updater` class in MLlib implements regularization. Regularization can
    help avoid over-fitting of a model to training data by effectively penalizing
    model complexity. This can be done by adding a term to the loss function, which
    acts to increase the loss as a function of the model weight vector.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的逻辑回归代码中，我们简要介绍了`Updater`类。MLlib中的`Updater`类实现了正则化。正则化可以通过有效地惩罚模型复杂性来帮助避免模型对训练数据的过度拟合。这可以通过向损失函数添加一个项来实现，该项作用是随着模型权重向量的函数增加损失。
- en: Regularization is almost always required in real use cases, but is of particular
    importance when the feature dimension is very high (that is, the effective number
    of variable weights that can be learned is high) relative to the number of training
    examples.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际使用情况下，几乎总是需要正则化，但当特征维度非常高（即可以学习的有效变量权重数量很高）相对于训练样本数量时，正则化尤为重要。
- en: When regularization is absent or low, models can tend to overfit. Without regularization,
    most models will overfit on a training dataset. This is a key reason behind the
    use of cross-validation techniques for model fitting (which we will cover now).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有或很低的正则化时，模型可能会过拟合。没有正则化时，大多数模型会在训练数据集上过拟合。这是使用交叉验证技术进行模型拟合的一个关键原因（我们现在将介绍）。
- en: Before we proceed further, let's define what it means to overfit and underfit
    data. Overfitting occurs when a model learns details and the noise in training
    data to an extent that negatively impacts the performance of the model on new
    data. The model should not follow the training dataset very rigorously, and in
    underfitting, a model can neither model the training data nor generalize to new
    data.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步进行之前，让我们定义一下过拟合和欠拟合数据的含义。过拟合发生在模型学习训练数据中的细节和噪音，从而对新数据的性能产生负面影响的程度。模型不应该过于严格地遵循训练数据集，在欠拟合中，模型既不能对训练数据建模，也不能推广到新数据。
- en: Conversely, since applying regularization encourages simpler models, model performance
    can suffer when regularization is high through underfitting the data.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当应用正则化时，鼓励简化模型，当正则化很高时，模型性能可能会受到影响，导致数据欠拟合。
- en: 'The forms of regularization available in MLlib are the following:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib中可用的正则化形式如下：
- en: '`SimpleUpdater`: This equates to no regularization, and is the default for
    logistic regression'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SimpleUpdater`：这等同于没有正则化，是逻辑回归的默认值'
- en: '`SquaredL2Updater`: This implements a regularizer based on the squared L2-norm
    of the weight vector; this is the default for SVM models'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SquaredL2Updater`：这实现了基于权重向量的平方L2范数的正则化器；这是SVM模型的默认值'
- en: '`L1Updater`: This applies a regularizer based on the L1-norm of the weight
    vector; this can lead to sparse solutions in the weight vector (as less important
    weights are pulled towards zero)'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`L1Updater`：这应用基于权重向量的L1范数的正则化器；这可能导致权重向量中的稀疏解（因为不太重要的权重被拉向零）'
- en: 'Regularization and its relation to optimization is a broad and heavily researched
    area. Some more information is available from the following links:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化及其与优化的关系是一个广泛而深入研究的领域。有关更多信息，请参考以下链接：
- en: 'General regularization overview: [http://en.wikipedia.org/wiki/Regularization_(mathematics)](http://en.wikipedia.org/wiki/Regularization_(mathematics))'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般正则化概述：[http://en.wikipedia.org/wiki/Regularization_(mathematics)](http://en.wikipedia.org/wiki/Regularization_(mathematics))
- en: 'L2 regularization: [http://en.wikipedia.org/wiki/Tikhonov_regularization](http://en.wikipedia.org/wiki/Tikhonov_regularization)'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2正则化：[http://en.wikipedia.org/wiki/Tikhonov_regularization](http://en.wikipedia.org/wiki/Tikhonov_regularization)
- en: 'Overfitting and underfitting: [http://en.wikipedia.org/wiki/Overfitting](http://en.wikipedia.org/wiki/Overfitting)
    Detailed overview of overfitting and L1 versus L2 regularization: [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.9860&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.9860&rep=rep1&type=pdf)'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合：[http://en.wikipedia.org/wiki/Overfitting](http://en.wikipedia.org/wiki/Overfitting)
    过拟合和L1与L2正则化的详细概述：[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.9860&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.9860&rep=rep1&type=pdf)
- en: 'Let''s explore the impact of a range of regularization parameters using `SquaredL2Updater`:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`SquaredL2Updater`来探索一系列正则化参数的影响。
- en: '[PRE101]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Your output should look like this:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 你的输出应该像这样：
- en: '[PRE102]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: As we can see, at low levels of regularization, there is not much impact in
    model performance. However, as we increase regularization, we can see the impact
    of under-fitting on our model evaluation.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，在正则化水平较低时，模型性能没有太大影响。然而，随着正则化的增加，我们可以看到欠拟合对我们模型评估的影响。
- en: You will find similar results when using the L1 regularization. Give it a try
    by performing the same evaluation of regularization parameter against the AUC
    measure for `L1Updater`.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用L1正则化时，您将会得到类似的结果。通过对AUC指标进行相同的正则化参数评估，尝试使用L1Updater。
- en: Decision trees
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: The decision trees, which controls the maximum depth of the tree and, thus,
    the complexity of the model. Deeper trees result in more complex models that will
    be able to fit the data better.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树控制树的最大深度，从而控制模型的复杂性。更深的树会导致更复杂的模型，能够更好地拟合数据。
- en: 'For classification problems, we can also select between two measures of impurity:
    `Gini` and `Entropy`.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，我们还可以在“Gini”和“Entropy”之间选择两种不纯度度量。
- en: Tuning tree depth and impurity
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整树深度和不纯度
- en: We will illustrate the impact of tree depth in a similar manner as we did for
    our logistic regression model.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以与逻辑回归模型相似的方式来说明树深度的影响。
- en: 'First, we will need to create another helper function in the Spark shell as
    follows:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在Spark shell中创建另一个辅助函数，如下所示：
- en: '[PRE103]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Now, we're ready to compute our AUC metric for different settings of tree depth.
    We will simply use our original dataset in this example, since we do not need
    the data to be standardized.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备计算不同树深度设置下的AUC指标。在这个例子中，我们将简单地使用我们的原始数据集，因为我们不需要数据被标准化。
- en: Note that decision tree models generally do not require features to be standardized
    or normalized, nor do they require categorical features to be binary-encoded.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，决策树模型通常不需要特征被标准化或归一化，也不需要分类特征被二进制编码。
- en: 'First, train the model using the `Entropy` impurity measure and varying tree
    depths like this:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用“Entropy”不纯度度量和不同的树深度来训练模型，如下所示：
- en: '[PRE104]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'This preceding code should output the results shown here:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应该输出以下结果：
- en: '[PRE105]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Next, we will perform the same computation using the `Gini` impurity measure
    (we omitted the code as it is very similar, but it can be found in the code bundle).
    Your results should look something like this:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`Gini`不纯度度量执行相同的计算（我们省略了代码，因为它非常相似，但可以在代码包中找到）。你的结果应该看起来像这样：
- en: '[PRE106]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: As you can see from the preceding results, increasing the tree depth parameter
    results in a more accurate model (as expected, since the model is allowed to get
    more complex with greater tree depth). It is very likely that at higher tree depths,
    the model will overfit the dataset significantly. As the tree depth increases,
    the generalization capability reduces, where generalization is how the concepts
    learned by a machine learning model apply to an example which has not ever been
    seen by the model.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的结果中可以看出，增加树深度参数会导致更准确的模型（正如预期的那样，因为模型允许在更大的树深度下变得更复杂）。很可能在更高的树深度下，模型会显著地过度拟合数据集。随着树深度的增加，泛化能力会降低，泛化是指机器学习模型学习的概念如何适用于模型从未见过的示例。
- en: There is very little difference in performance between the two impurity measures.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种不纯度度量的性能几乎没有什么区别。
- en: The naive Bayes model
  id: totrans-463
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型
- en: Finally, let's see the impact of changing the `lambda` parameter for naive Bayes.
    This parameter controls additive smoothing, which handles the case when a `class`
    and `feature` value do not occur together in the dataset.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看改变朴素贝叶斯的`lambda`参数会产生什么影响。这个参数控制加法平滑，处理当`class`和`feature`值在数据集中没有同时出现的情况。
- en: See [http://en.wikipedia.org/wiki/Additive_smoothing](http://en.wikipedia.org/wiki/Additive_smoothing)
    for more details on additive smoothing.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于加法平滑的细节，请参见[http://en.wikipedia.org/wiki/Additive_smoothing](http://en.wikipedia.org/wiki/Additive_smoothing)。
- en: 'We will take the same approach as we did earlier, first creating a convenience
    training function and training the model with varying levels of `lambda` as follows:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用与之前相同的方法，首先创建一个方便的训练函数，然后使用不同水平的`lambda`来训练模型，如下所示：
- en: '[PRE107]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'The results of the training are as follows:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的结果如下：
- en: '[PRE108]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: We can see that `lambda` has no impact in this case, since it will not be a
    problem if the combination of feature and class label do not occur together in
    the dataset.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在这种情况下`lambda`没有影响，因为如果特征和类标签的组合在数据集中没有出现在一起，这不会成为问题。
- en: Cross-validation
  id: totrans-471
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: So far in this book, we have only briefly mentioned the idea of cross-validation
    and out-of-sample testing. Cross-validation is a critical part of real-world machine
    learning, and is central to many model selection and parameter tuning pipelines.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们只是简要提到了交叉验证和样本外测试的概念。交叉验证是现实世界机器学习的关键部分，是许多模型选择和参数调整流程的核心。
- en: The general idea behind cross-validation is that we want to know how our model
    will perform on unseen data. Evaluating this on real, live data (for example,
    in a production system) is risky, because we don't really know whether the trained
    model is the best in the sense of being able to make accurate predictions on new
    data. As we saw previously with regard to regularization, our model might have
    overfit the training data, and be poor at making predictions on data it has not
    been trained on.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证的基本思想是我们想知道我们的模型在未见数据上的表现如何。在真实的、实时数据上评估这一点（例如在生产系统中）是有风险的，因为我们并不真正知道训练好的模型是否是最佳的，能够对新数据进行准确的预测。正如我们之前在正则化方面看到的那样，我们的模型可能已经过度拟合了训练数据，在未经训练的数据上做出预测可能很差。
- en: Cross-validation provides a mechanism where we use part of our available dataset
    to train our model, and another part to evaluate the performance of this model.
    As the model is tested on data that it has not seen during the training phase,
    its performance, when evaluated on this part of the dataset, gives us an estimate
    as to how well our model generalizes for the new data points.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证提供了一种机制，我们可以使用可用数据集的一部分来训练我们的模型，另一部分来评估这个模型的性能。由于模型在训练阶段没有见过这部分数据，当在数据集的这部分上评估模型的性能时，可以给我们一个关于我们的模型在新数据点上的泛化能力的估计。
- en: Here, we will implement a simple cross-validation evaluation approach using
    a train-test split. We will divide our dataset into two non-overlapping parts.
    The first dataset is used to train our model, and is called the **training set**.
    The second dataset, called the **test set** or **hold-out set**, is used to evaluate
    the performance of our model using our chosen evaluation measure. Common splits
    used in practice include 50/50, 60/40, and 80/20 splits, but you can use any split
    as long as the training set is not too small for the model to learn (generally,
    at least 50% is a practical minimum).
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用训练-测试分离来实现一个简单的交叉验证评估方法。我们将把我们的数据集分成两个不重叠的部分。第一个数据集用于训练我们的模型，称为**训练集**。第二个数据集，称为**测试集**或**留出集**，用于使用我们选择的评估指标评估我们的模型的性能。实际使用的常见分割包括50/50、60/40和80/20的分割，但只要训练集不太小以至于模型无法学习（通常至少50%是一个实际的最小值），你可以使用任何分割。
- en: 'In many cases, three sets are created: a training set, an evaluation set (which
    is used like the aforementioned test set to tune the model parameters such as
    lambda and step size), and a test set (which is never used to train a model or
    tune any parameters, but is only used to generate an estimated true performance
    on completely unseen data).'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，会创建三组数据：一个训练集，一个评估集（类似于前面提到的测试集，用于调整模型参数，如lambda和步长），以及一个测试集（从不用于训练模型或调整任何参数，只用于生成对完全未见数据的估计真实性能）。
- en: Here, we will explore a simple train-test split approach. There are many cross-validation
    techniques that are more exhaustive and complex.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探讨一个简单的训练-测试分离方法。还有许多更详尽和复杂的交叉验证技术。
- en: One popular example is the **K-fold cross-validation**, where the dataset is
    split into *K* non-overlapping folds. The model is trained on *K-1* folds of data,
    and tested on the remaining, held-out fold. This is repeated *K* times, and the
    results are averaged to give the cross-validation score. The train-test split
    is effectively like a two-fold cross-validation.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的例子是**K折交叉验证**，其中数据集被分成*K*个不重叠的折叠。模型在*K-1*个数据折叠上进行训练，并在剩下的保留的折叠上进行测试。这个过程重复*K*次，然后对结果进行平均以得到交叉验证分数。训练-测试分割实际上就像是两折交叉验证。
- en: Other approaches include leave-one-out cross-validation and random sampling.
    See the article at [http://en.wikipedia.org/wiki/Cross-validation_(statistics)](http://en.wikipedia.org/wiki/Cross-validation_(statistics))
    for further details.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法包括留一交叉验证和随机抽样。更多细节请参见[http://en.wikipedia.org/wiki/Cross-validation_(statistics)](http://en.wikipedia.org/wiki/Cross-validation_(statistics))的文章。
- en: First, we will split our dataset into a 60% training set and a 40% test set
    (we will use a constant random seed of 123 here to ensure that we get the same
    results for ease of illustration).
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将把数据集分成60%的训练集和40%的测试集（我们将在这里使用一个常数随机种子123，以确保我们获得相同的结果以便进行说明）。
- en: '[PRE109]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Next, we will compute the evaluation metric of interest (again, we will use
    AUC) for a range of regularization parameter settings. Note that here we will
    use a finer-grained step size between the evaluated regularization parameters
    to better illustrate the differences in AUC, which are very small in this case.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算感兴趣的评估指标（再次，我们将使用AUC）的一系列正则化参数设置。请注意，这里我们将使用更精细的步长在评估的正则化参数之间，以更好地说明AUC的差异，在这种情况下差异非常小。
- en: '[PRE110]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'This preceding code will compute the results of training on the training set,
    and the results of evaluating on the test set, as shown here:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算在训练集上训练的结果，以及在测试集上评估的结果，如下所示：
- en: '[PRE111]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Now, let's compare this to the results of training and testing on the training
    set
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这与在训练集上进行训练和测试的结果进行比较。
- en: '(this is what we were doing previously by training and testing on all data).
    Again, we will omit the code, as it is very similar (but it is available in the
    code bundle):'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: （这是我们之前在所有数据上进行训练和测试的做法）。同样，我们将省略代码，因为它非常相似（但它在代码包中是可用的）：
- en: '[PRE112]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: So, we can see that when we train and evaluate our model on the same dataset,
    we generally achieve the highest performance when regularization is lower. This
    is because our model has seen all the data points, and with low levels of regularization,
    it can overfit the data set and achieve higher performance.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到当我们在相同的数据集上训练和评估我们的模型时，通常在正则化较低时会获得最高的性能。这是因为我们的模型已经看到了所有的数据点，并且在低水平的正则化下，它可以过度拟合数据集并获得更高的性能。
- en: In contrast, when we train on one dataset and test on another, we see that,
    generally, a slightly higher level of regularization results in better test set
    performance.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，当我们在一个数据集上训练并在另一个数据集上测试时，通常略高水平的正则化会导致更好的测试集性能。
- en: In cross-validation, we would typically find the parameter settings (including
    regularization as well as the various other parameters, such as step size and
    so on) that result in the best test set performance. We would then use these parameter
    settings to retrain the model on all of our data in order to use it to make predictions
    on new data.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证中，我们通常会找到参数设置（包括正则化以及其他各种参数，如步长等），以获得最佳的测试集性能。然后我们将使用这些参数设置在所有数据上重新训练模型，以便在新数据上进行预测。
- en: Recall from [Chapter 5](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml), *Building
    a Recommendation Engine with Spark* that we did not cover cross-validation. You
    can apply the same techniques we used earlier to split the ratings dataset from
    that chapter into a training and test dataset. You can then try out different
    parameter settings on the training set while evaluating the MSE and MAP performance
    metrics on the test set in a manner similar to what we did earlier. Give it a
    try!
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第5章](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml)，*使用Spark构建推荐引擎*，我们没有涉及交叉验证。您可以应用我们之前使用的相同技术，将该章节中的评分数据集分成训练集和测试集。然后，您可以尝试在训练集上尝试不同的参数设置，同时在测试集上评估MSE和MAP性能指标，方式类似于我们之前所做的。试一试吧！
- en: Summary
  id: totrans-493
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the various classification models available in Spark
    MLlib, and we saw how to train models on input data, and how to evaluate their
    performance using standard metrics and measures. We also explored how to apply
    some of the techniques previously introduced to transform our features. Finally,
    we investigated the impact of using the correct input data format or distribution
    on model performance, and we also saw the impact of adding more data to our model,
    tuning model parameters and implementing cross-validation.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了Spark MLlib中可用的各种分类模型，并且我们看到了如何在输入数据上训练模型，以及如何使用标准指标和度量来评估它们的性能。我们还探讨了如何应用一些先前介绍的技术来转换我们的特征。最后，我们调查了使用正确的输入数据格式或分布对模型性能的影响，以及增加更多数据对我们的模型，调整模型参数和实施交叉验证的影响。
- en: In the next chapter, we will take a similar approach to delve into MLlib's regression
    models.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将采用类似的方法来深入MLlib的回归模型。
