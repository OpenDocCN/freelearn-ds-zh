- en: Chapter 5. Spark for Geographic Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。地理分析的Spark
- en: Geographic processing is a powerful use case for Spark and therefore the aim
    of this chapter is to explain how data scientists can process geographic data
    using Spark to produce powerful, map-based views of very large datasets. We will
    demonstrate how to process spatio-temporal datasets easily via Spark integrations
    with GeoMesa, which helps turn Spark into a sophisticated geographic processing
    engine. As the **Internet of Things** (**IoT**) and other location-aware datasets
    become ever more common, and *moving objects* data volumes climb, Spark will become
    a critical tool that closes the geoprocessing gap that exists between spatial
    functionality and processing scalability. This chapter reveals how to conduct
    advanced geopolitical analysis of global news with a view to leveraging the data
    to analyze and perform data science on oil prices.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 地理处理是Spark的一个强大用例，因此本章的目的是解释数据科学家如何使用Spark处理地理数据，以产生强大的基于地图的大型数据集视图。我们将演示如何通过Spark与GeoMesa集成轻松处理时空数据集，这有助于将Spark转变为一个复杂的地理处理引擎。随着**物联网**（**IoT**）和其他位置感知数据变得越来越普遍，以及*移动对象*数据量的增加，Spark将成为一个重要的工具，弥合空间功能和处理可伸缩性之间的地理处理差距。本章揭示了如何通过全球新闻进行高级地缘政治分析，以利用数据分析和进行石油价格数据科学。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Using Spark to ingest and preprocess geolocated data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark摄取和预处理地理定位数据
- en: Storing geodata which is appropriately indexed, using Geohash indexing inside
    GeoMesa
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储适当索引的地理数据，使用GeoMesa内部的Geohash索引
- en: Running complex spatio-temporal queries, filtering data across time and space
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行复杂的时空查询，跨时间和空间过滤数据
- en: Using Spark and GeoMesa together to perform advanced geographic processing in
    order to study change over time
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark和GeoMesa一起执行高级地理处理，以研究随时间的变化
- en: Using Spark to calculate density maps and to visualize changes in these maps
    over time
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark计算密度地图，并可视化这些地图随时间的变化
- en: Querying and integrating spatial data across map layers to build new insights
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询和整合跨地图层的空间数据以建立新的见解
- en: GDELT and oil
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GDELT和石油
- en: The premise of this chapter is that we can manipulate GDELT data to determine,
    to a greater or lesser extent, the price of oil based on historic events. The
    accuracy of our predictor will depend on many variables including the detail of
    our events, the number used and our hypotheses surrounding the nature of the relationship
    between oil and these events.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前提是我们可以操纵GDELT数据，以更大或更小的程度确定石油价格基于历史事件。我们的预测准确性将取决于许多变量，包括我们事件的细节，使用的数量以及我们关于石油和这些事件之间关系性质的假设。
- en: The oil industry is very complex and is driven by many factors. It has been
    found however, that most major oil price fluctuations are largely explained by
    shifts in the demand of crude oil. The price also increases during times of greater
    demand for stock, and historically has been high in times of geopolitical tension
    in the Middle East. In particular, political events have a strong influence on
    the oil price and it is this aspect that we will concentrate on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 石油行业非常复杂，受到许多因素的驱动。然而，研究发现，大多数主要的石油价格波动主要是由原油需求的变化所解释的。价格在股票需求增加时也会上涨，并且在中东地区的地缘政治紧张时期价格历史上也很高。特别是政治事件对石油价格有很大影响，我们将集中讨论这一方面。
- en: 'Crude oil is produced by many countries around the world; there are however,
    three main benchmarks that are used by producers for pricing:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 世界各地有许多国家生产原油；然而，有三个主要的基准价格，供应商用于定价：
- en: 'Brent: Produced by various entities in the North Sea'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布伦特：由北海的各种实体生产
- en: 'WTI: **West Texas Intermediate** (**WTI**) covering entities in the mid-west
    and Gulf Coast regions of North America'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WTI：**西德克萨斯中质原油**（**WTI**）覆盖北美中西部和墨西哥湾沿岸地区的实体
- en: 'OPEC: Produced by members of OPEC:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧佩克：由欧佩克成员国生产：
- en: Algeria, Angola, Ecuador, Gabon, Indonesia, Iran, Iraq, Kuwait, Libya, Nigeria,
    Qatar, Saudi Arabia, UAE, and Venezuela
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 阿尔及利亚，安哥拉，厄瓜多尔，加蓬，印度尼西亚，伊朗，伊拉克，科威特，利比亚，尼日利亚，卡塔尔，沙特阿拉伯，阿联酋和委内瑞拉
- en: 'It becomes clear that the first thing we need to do is to obtain the historical
    pricing data for the three baselines. By searching the Internet, downloadable
    data can be found in many places, for example:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们需要做的第一件事是获取三个基准的历史定价数据。通过搜索互联网，可以在许多地方找到可下载的数据，例如：
- en: Brent: [https://fred.stlouisfed.org/](https://fred.stlouisfed.org/series/DCOILBRENTEU)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布伦特：[https://fred.stlouisfed.org/](https://fred.stlouisfed.org/series/DCOILBRENTEU)
- en: WTI: [https://fred.stlouisfed.org/](https://fred.stlouisfed.org/series/DCOILBRENTEU)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WTI：[https://fred.stlouisfed.org/](https://fred.stlouisfed.org/series/DCOILBRENTEU)
- en: OPEC: [http://opec.org](http://opec.org)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧佩克：[http://opec.org](http://opec.org)
- en: Now we know that oil prices are primarily determined by supply and demand, our
    first hypothesis will be that the supply and demand is affected, to a greater
    extent, by world events and thus we can predict what that supply and demand is
    likely to be.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道，石油价格主要由供需决定，我们的第一个假设是供需受世界事件的影响更大，因此我们可以预测供需可能是什么。
- en: We want to try and determine whether the oil price will rise or fall during
    the next day, week, or month and, as we have used GDELT throughout the book, we
    will take that knowledge and expand it to run some very large processing jobs.
    Before we start, it's worth discussing the path we are going to take, and the
    reasons for the decisions made. The first area of concern is how GDELT relates
    to oil; this will define the scope of the initial work, and provide a base upon
    which we can build later. It is important here that we decide how to leverage
    GDELT and what the consequences of that decision will be; for example, we could
    decide to use all of the data for all of the time, but the processing time required
    for that is very large indeed since just one day of GDELT events data can average
    15 MB, and 1.5 GB for GKG. Therefore, we should analyze the contents of the two
    sets and try to establish what our initial data input will be.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要确定原油价格在接下来的一天、一周或一个月内是上涨还是下跌，由于我们在整本书中都使用了GDELT，我们将利用这些知识来运行一些非常大的处理任务。在开始之前，值得讨论我们将采取的路径以及决定的原因。首要关注的是GDELT与石油的关系；这将定义最初工作的范围，并为我们以后的工作奠定基础。在这里很重要的是我们决定如何利用GDELT以及这个决定的后果；例如，我们可以决定使用所有时间的所有数据，但是所需的处理时间确实非常大，因为仅一天的GDELT事件数据平均为15
    MB，GKG为1.5 GB。因此，我们应该分析这两组数据的内容，并尝试确定我们的初始数据输入将是什么。
- en: GDELT events
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GDELT事件
- en: 'Looking through the GDELT schema, there are a number of points that could be
    useful; the events schema primarily revolves around identifying the two primary
    actors in a story and relating an event to them. There is also the ability to
    look at events at different levels, so we will have good flexibility to work at
    higher or lower levels of complexity, depending upon how our results work out.
    For example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看GDELT模式，有一些可能有用的要点；事件模式主要围绕着识别故事中的两个主要参与者并将事件与他们联系起来。还可以查看不同级别的事件，因此我们将有很好的灵活性，可以根据我们的结果在更高或更低的复杂性水平上工作。例如：
- en: 'The `EventCode` field is a CAMEO action code: 0251 (Appeal for easing of administrative
    sanctions) and can also be used at the levels 02 (Appeal) and 025 (Appeal to yield).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`EventCode`字段是一个CAMEO动作代码：0251（呼吁放宽行政制裁），也可以在02（呼吁）和025（呼吁让步）级别使用。'
- en: Our second hypothesis is therefore, that the level of detail of the event will
    provide better or worse accuracy from our algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的第二个假设是，事件的详细程度将从我们的算法中提供更好或更差的准确性。
- en: Other interesting labels are `GoldsteinScale`, `NumMentions` and `Lat`/`Lon`.
    The `GoldsteinScale` label is a number from -10 to +10 and it attempts to capture
    the theoretical potential impact that type of event can have on the stability
    of a country; a great match based on what we have already established about the
    stability of oil prices. The `NumMentions` label gives us an indication of how
    often the event has appeared across all source documents; this could help us to
    assign an importance to events if we find that we need to reduce the number of
    assessed events in our processing. For example, we could process the data and
    find the top 10, 100, or 1000 events in the last hour, day, or week based upon
    how often they have been mentioned. Finally, the `lat`/`lon` label information
    attempts to assign a geographical point of reference for the event, making this
    very useful for when we want to produce maps in GeoMesa.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其他有趣的标签包括`GoldsteinScale`、`NumMentions`和`Lat`/`Lon`。`GoldsteinScale`标签是一个从-10到+10的数字，它试图捕捉该类型事件对一个国家稳定性的理论潜在影响；这与我们已经确定的关于石油价格稳定性的情况非常匹配。`NumMentions`标签给出了事件在所有来源文件中出现的频率的指示；如果我们发现需要减少我们处理的事件数量，这可能有助于我们为事件分配重要性。例如，我们可以处理数据并找出在过去一小时、一天或一周中出现频率最高的10、100或1000个事件。最后，`lat`/`lon`标签信息试图为事件分配地理参考点，这在我们想要在GeoMesa中制作地图时非常有用。
- en: GDELT GKG
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GDELT GKG
- en: 'The GKG schema is related to summarizing the content of the events and providing
    enhanced information specific to that content. Areas of interest for our purposes
    include `Counts`, `Themes`, `GCAM`, and `Locations`; the `Counts` field maps any
    numeric mention, thus potentially allowing us to calculate a severity, for example
    KILLS=47\. The `Themes` field lists all of the themes based on the GDELT category
    list; this could help us to machine learn particular areas, over time, which affect
    oil prices. The `GCAM` field is the result of content analysis of the event; a
    quick perusal of the GCAM list shows us that there are some possibly useful dimensions
    to look out for:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GKG模式与总结事件内容和提供特定于该内容的增强信息有关。我们感兴趣的领域包括`Counts`、`Themes`、`GCAM`和`Locations`；`Counts`字段映射任何数字提及，因此可能允许我们计算严重性，例如KILLS=47。`Themes`字段列出了基于GDELT类别列表的所有主题；这可能有助于我们随着时间的推移学习影响石油价格的特定领域。`GCAM`字段是对事件内容的内容分析的结果；快速浏览GCAM列表，我们发现有一些可能有用的维度需要注意：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And finally, we have the `Locations` field, which provides similar information
    to the Events, and thus also can be used for visualization of maps.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有`Locations`字段，它提供了与事件类似的信息，因此也可以用于制作地图的可视化。
- en: Formulating a plan of action
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 制定行动计划
- en: 'Having inspected the GDELT schemas, we now need to make some decisions around
    what data we are going to use, and make sure we justify that usage based on our
    hypotheses. This is a critical stage as there are many areas to consider, and
    at the very least we need to:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查了GDELT模式之后，我们现在需要做一些决定，确定我们将使用哪些数据，并确保我们根据我们的假设来证明这种用法。这是一个关键阶段，因为有许多方面需要考虑，至少我们需要：
- en: Ensure that our hypotheses are clear so that we have a known starting point
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保我们的假设清晰，这样我们就有一个已知的起点
- en: Ensure that we are clear about how we are going to implement the hypotheses,
    and determine an action plan
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保我们清楚地了解如何实施假设，并确定一个行动计划
- en: Ensure that we use enough appropriate data to meet our action plan; scope the
    data usage to ensure we can produce a conclusion within a given time frame, for
    example, using all GDELT data would be great, but is probably not reasonable unless
    a large processing cluster is available. On the other hand using one day is clearly
    not enough to gauge any patterns over time
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保我们使用足够的适当数据来满足我们的行动计划；限定数据使用范围，以确保我们能够在给定的时间范围内得出结论，例如，使用所有GDELT数据将是很好的，但除非有一个大型处理集群可用，否则可能不太合理。另一方面，仅使用一天显然不足以评估任何时间段内的任何模式
- en: Formulate a plan B in case our initial results are not conclusive
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定B计划，以防我们的初始结果不具有决定性
- en: Our second hypothesis is about the detail of the events; for the purposes of
    clarity, in this chapter, we are going to choose just one of the data sources
    initially, with a view to adding further complexity if our model does not perform
    well. Therefore, we can choose the GDELT events as the fields mentioned above
    provide for an excellent base upon which to prove our algorithms; in particular,
    the `gcam` field will be very useful to determine the nature of an event and the
    `NumMentions` field will be quick to implement when considering the importance
    of an event. While the GKG data also looks useful, we want to try and use general
    events at this stage; so, the GCAM oil data, for example, is considered too specific
    as there is a good chance that articles related to these fields will often be
    about the reaction to oil price change, and therefore too late to consider for
    our model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个假设是关于事件的细节；为了清晰起见，在本章中，我们将首先选择一个数据源，以便在模型表现不佳时添加更多复杂性。因此，我们可以选择GDELT事件作为上述提到的字段，这些字段为我们的算法提供了一个很好的基础；特别是`gcam`字段将非常有用于确定事件的性质，而`NumMentions`字段在考虑事件的重要性时将很快实施。虽然GKG数据看起来也很有用，但我们希望在这个阶段尝试使用一般事件；因此，例如GCAM油数据被认为太具体，因为这些领域的文章很可能经常涉及对油价变化的反应，因此对于我们的模型来说考虑太晚了。
- en: 'Our initial processing flow (action plan) will involve the following steps:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始处理流程（行动计划）将涉及以下步骤：
- en: Obtain oil price data for the last 5 years
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取过去5年的油价数据
- en: Obtain GDELT events for the last 5 years
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取过去5年的GDELT事件
- en: Install GeoMesa and related tools
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装GeoMesa和相关工具
- en: Load the GDELT data to GeoMesa
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将GDELT数据加载到GeoMesa
- en: Build a visualization to show some of the events on a world map
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个可视化，显示世界地图上的一些事件
- en: Use an appropriate machine learning algorithm to learn event types against oil
    price rise/fall
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用适当的机器学习算法来学习事件类型与油价的涨跌
- en: Use the model to predict the rise or fall in price of oil
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型预测油价的涨跌
- en: GeoMesa
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GeoMesa
- en: GeoMesa is an open source product designed to leverage the distributed nature
    of storage systems, such as Accumulo and Cassandra, to hold a distributed spatio-temporal
    database. With this design, GeoMesa is capable of running the large-scale geospatial
    analytics that are required for very large data sets, including GDELT.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: GeoMesa是一个开源产品，旨在利用存储系统的分布式特性，如Accumulo和Cassandra，来保存分布式时空数据库。有了这个设计，GeoMesa能够运行大规模的地理空间分析，这对于非常大的数据集，包括GDELT，是必需的。
- en: We are going to use GeoMesa to store GDELT data and run our analytics across
    a large proportion of that data; this should give us access to enough data to
    train our model so that we can predict the future rise and fall of oil prices.
    Also, GeoMesa will enable us to plot large amounts of points on a map, so that
    we can visualize GDELT and any other useful data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用GeoMesa来存储GDELT数据，并在其中的大部分数据上运行我们的分析；这应该为我们提供足够的数据来训练我们的模型，以便我们可以预测未来油价的涨跌。此外，GeoMesa还将使我们能够在地图上绘制大量点，以便我们可以可视化GDELT和其他有用的数据。
- en: Installing
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'There is a very good tutorial on the GeoMesa website ([www.geomesa.org](http://www.geomesa.org))
    that guides the user through the installation process. Therefore, it is not our
    intention here to produce another how-to guide; there are, however, a few points
    worth noting that may save you time in getting everything up and running:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GeoMesa网站（[www.geomesa.org](http://www.geomesa.org)）上有一个非常好的教程，指导用户完成安装过程。因此，我们在这里并不打算制作另一个操作指南；然而，有几点值得注意，可能会节省您在启动一切时的时间。
- en: GeoMesa has a lot of components, and many of these have a lot of versions. It
    is very important to ensure that all of the versions of the software stack match
    exactly with the versions specified in the GeoMesa maven POMs. Of particular interest
    are Hadoop, Zookeeper, and Accumulo; the version locations can be found in the
    root `pom.xml` file in the GeoMesa tutorial and other related downloads.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GeoMesa有很多组件，其中许多组件有很多版本。确保软件堆栈的所有版本与GeoMesa maven POMs中指定的版本完全匹配非常重要。特别感兴趣的是Hadoop、Zookeeper和Accumulo；版本位置可以在GeoMesa教程和其他相关下载的根`pom.xml`文件中找到。
- en: At the time of writing, there are some additional issues when integrating GeoMesa
    with some of the Hadoop vendor stacks. If you are able, use GeoMesa with your
    own stack of Hadoop/Accumulo and so on, to ensure version compatibility.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在撰写本文时，将GeoMesa与某些Hadoop供应商堆栈集成时存在一些额外问题。如果可能的话，使用GeoMesa与您自己的Hadoop/Accumulo等堆栈，以确保版本兼容性。
- en: The GeoMesa version dependency labeling has changed from version 1.3.0\. It
    is very important that you ensure all of the versions line up with your chosen
    version of GeoMesa; if there are any conflicting classes then there will definitely
    be problems at some point down the line.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GeoMesa版本依赖标签已从版本1.3.0更改。确保所有版本与您选择的GeoMesa版本完全匹配非常重要；如果有任何冲突的类，那么在某个时候肯定会出现问题。
- en: If you have not used Accumulo before, we have discussed it in detail in other
    chapters within this book. An initial familiarization will help greatly when using
    GeoMesa (see [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"), *Building
    Communities*).
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您以前没有使用过Accumulo，我们在本书的其他章节中已经详细讨论过它。初步熟悉将在使用GeoMesa时大有裨益（参见[第7章](ch07.xhtml
    "第7章。建立社区")，“建立社区”）。
- en: When using Accumulo 1.6 or greater with GeoMesa, there is the option to use
    Accumulo namespaces. If you are unfamiliar with this, then opt to not use namespaces
    and simply copy the GeoMesa runtime JAR into `/lib/text` in your Accumulo root
    folder.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用Accumulo 1.6或更高版本与GeoMesa时，有使用Accumulo命名空间的选项。如果您对此不熟悉，则选择不使用命名空间，并将GeoMesa运行时JAR简单地复制到Accumulo根文件夹中的`/lib/text`中。
- en: GeoMesa uses a few shell scripts; due to the nature of operating systems there
    may be the odd problem with running these scripts, depending upon your platform.
    The issues are minor and can be fixed with some quick Internet searches; for example
    when running `jai-image.sh` there was a minor issue with user confirmation on
    an Mac OSX.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GeoMesa使用一些shell脚本；由于操作系统的性质，运行这些脚本可能会出现一些问题，这取决于您的平台。这些问题很小，可以通过一些快速的互联网搜索来解决；例如，在运行`jai-image.sh`时，在Mac
    OSX上会出现用户确认的小问题。
- en: The GeoMesa maven repository can be found at [https://repo.locationtech.org/content/repositories/releases/org/locationtech/geomesa/](https://repo.locationtech.org/content/repositories/releases/org/locationtech/geomesa/)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GeoMesa的maven仓库可以在[https://repo.locationtech.org/content/repositories/releases/org/locationtech/geomesa/](https://repo.locationtech.org/content/repositories/releases/org/locationtech/geomesa/)找到
- en: Once you are able to successfully run GeoMesa from the command line, we can
    move on to the next section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您能够成功地从命令行运行GeoMesa，我们就可以继续下一节了。
- en: GDELT Ingest
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GDELT摄入
- en: 'The next stage is to obtain the GDELT data and load it into GeoMesa. There
    are a number of options here, depending upon how you plan to proceed; if you are
    just working through this chapter, then you can use a script to download the data
    in one go:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下一阶段是获取GDELT数据并将其加载到GeoMesa中。这里有许多选择，取决于您打算如何进行；如果您只是在阅读本章，那么可以使用脚本一次性下载数据：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will download and verify all of the GDELT events data for 2015 and 2016\.
    The amount of data required is something we need to estimate at this stage, as
    we do not know how our algorithm is going to work out, so we have chosen two years
    worth to start with.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载并验证2015年和2016年的所有GDELT事件数据。在这个阶段，我们需要估计所需的数据量，因为我们不知道我们的算法将如何运行，所以我们选择了两年的数据来开始。
- en: 'An alternative to the script is to read [Chapter 2](ch02.xhtml "Chapter 2. Data
    Acquisition"), *Data Acquisitio*n, which explains in detail how to configure Apache
    NiFi to download the GDELT data in real time, and further it loads it to HDFS
    ready for use. Otherwise, a script to allow the preceding data to be transferred
    to HDFS is shown as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的替代方法是阅读[第2章](ch02.xhtml "第2章。数据获取")，*数据获取*，其中详细解释了如何配置Apache NiFi以实时下载GDELT数据，并将其加载到HDFS以供使用。否则，可以使用脚本将前述数据传输到HDFS，如下所示：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: HDFS uses data blocks; we want to ensure that files are stored as efficiently
    as possible. Writing a method to aggregate files to the HDFS block size (64 MB
    by default) will ensure the NameNode memory is not filled with many entries for
    lots of small files, and will make processing more efficient also. Large files
    that use more than one block (file size > 64 MB) are known as split files.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS使用数据块；我们希望确保文件存储尽可能高效。编写一个方法来将文件聚合到HDFS块大小（默认为64 MB）将确保NameNode内存不会被许多小文件的条目填满，并且还将使处理更加高效。使用多个块（文件大小>
    64 MB）的大文件称为分割文件。
- en: We have a substantial amount of data in HDFS (approximately 48 GB for 2015/16).
    Now, we will load this to Accumulo via GeoMesa.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在HDFS中有大量的数据（大约为2015/16年的48 GB）。现在，我们将通过GeoMesa将其加载到Accumulo中。
- en: GeoMesa Ingest
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GeoMesa摄入
- en: The GeoMesa tutorials discuss the idea of loading the data from HDFS to Accumulo
    using a `MapReduce` job. Let's take a look at this and create a Spark equivalent.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: GeoMesa教程讨论了使用`MapReduce`作业从HDFS加载数据到Accumulo的想法。让我们来看看这个，并创建一个Spark等价物。
- en: MapReduce to Spark
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MapReduce到Spark
- en: Since **MapReduce** (**MR**) is generally considered dead, or at least dying,
    it is very useful to know how to create Spark jobs from those existing in MR.
    The following method can be applied to any MR job. We will consider the GeoMesa
    Accumulo loading job described in the GeoMesa tutorial (`geomesa-examples-gdelt`)
    for this case.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于**MapReduce**（**MR**）通常被认为已经死亡，或者至少正在消亡，因此了解如何从MR中创建Spark作业非常有用。以下方法可以应用于任何MR作业。我们将考虑GeoMesa教程中描述的GeoMesa
    Accumulo加载作业（`geomesa-examples-gdelt`）。
- en: 'An MR job is typically made up of three parts: the mapper, the reducer, and
    the driver. The GeoMesa example is a map-only job and therefore requires no reducer.
    The job takes a GDELT input line, creates a (Key,Value) pair from an empty `Text`
    object and the created GeoMesa `SimpleFeature`, and uses the `GeoMesaOutputFormat`
    to load the data to Accumulo. The full code of the MR job can be found in our
    repository; next this we will work through the key parts and suggest the changes
    required for Spark.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: MR作业通常由三部分组成：mapper、reducer和driver。GeoMesa示例是一个仅包含mapper的作业，因此不需要reducer。该作业接收GDELT输入行，从空的`Text`对象和创建的GeoMesa
    `SimpleFeature`创建一个（Key,Value）对，并使用`GeoMesaOutputFormat`将数据加载到Accumulo。MR作业的完整代码可以在我们的仓库中找到；接下来，我们将逐步介绍关键部分并建议Spark所需的更改。
- en: 'The job is initiated from the `main` method; the first few lines are related
    to parsing the required options from the command line, such as the Accumulo username
    and password. We then reach:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作业是从`main`方法启动的；前几行与从命令行解析所需选项有关，例如Accumulo用户名和密码。然后我们到达：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The GeoMesa `SimpleFeatureType` is the primary mechanism used to store data
    in a GeoMesa data store and it needs to be initialized once, along with the data
    store initialization. Once this is done we execute the MR job itself. In Spark,
    we can pass the arguments via the command line as before, and then do the one-off
    setup:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: GeoMesa `SimpleFeatureType`是用于在GeoMesa数据存储中存储数据的主要机制，需要初始化一次，以及数据存储初始化。完成这些后，我们执行MR作业本身。在Spark中，我们可以像以前一样通过命令行传递参数，然后进行一次性设置：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The contents of the jar contain a standard Spark job:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: jar文件的内容包含了一个标准的Spark作业：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Parse the command line arguments as before, as well as performing the initialization:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 像以前一样解析命令行参数，并执行初始化：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we can load the data from HDFS, using wildcards if required. This creates
    one partition for each block of the file (64 MB default), resulting in an `RDD[String]`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以从HDFS加载数据，如果需要可以使用通配符。这将为文件的每个块（默认为64 MB）创建一个分区，从而产生一个`RDD[String]`：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Or we can fix the number of partitions, depending upon our available resources:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可以根据可用资源来固定分区的数量：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we can perform the map, where we can embed the function to replace the
    process in the original MR `map` method. We create a tuple (Text,SimpleFeatureType)
    to replicate a (Key, Value) pair so that we can use the `OutputFormat` in the
    next step. When Scala Tuples are created in this way, the resulting RDD gains
    extra methods, such as `ReduceByKey`, which is functionally equivalent to the
    MR Reducer (see below for further information on what we should really be using,
    `mapPartitions`):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以执行map，其中我们可以嵌入函数来替换原始MR`map`方法中的过程。我们创建一个元组（Text，SimpleFeatureType）来复制一个（Key，Value）对，以便我们可以在下一步中使用`OutputFormat`。当以这种方式创建Scala元组时，生成的RDD会获得额外的方法，比如`ReduceByKey`，它在功能上等同于MR
    Reducer（有关我们真正应该使用的`mapPartitions`的更多信息，请参见下文）：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we can finally output to Accumulo using the `GeomesaOutputFormat` from
    the original job:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们最终可以使用原始作业中的`GeomesaOutputFormat`输出到Accumulo：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: At this stage, we have not mentioned the `setup` method in the MR job; this
    method is called before any input is processed to allocate an expensive resource
    like a database connection, or in our case, a reusable object, and a `cleanup`
    method is then used to release that resource if it were to persist when out of
    scope. In our case, the `setup` method is used to create a `SimpleFeatureBuilder`
    which can be reused during each call of the mapper to build `SimpleFeatures` for
    output; there is no `cleanup` method as the memory is automatically released when
    the object is out of scope (the code has completed).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们还没有提到MR作业中的`setup`方法；这个方法在处理任何输入之前被调用，用来分配一个昂贵的资源，比如数据库连接，或者在我们的情况下，一个可重用的对象，然后使用`cleanup`方法来释放资源，如果它在作用域外持续存在的话。在我们的情况下，`setup`方法用来创建一个`SimpleFeatureBuilder`，它可以在每次调用mapper时重复使用来构建输出的`SimpleFeatures`；没有`cleanup`方法，因为当对象超出作用域时，内存会自动释放（代码已经完成）。
- en: 'The Spark `map` function only operates on one input at a time, and provides
    no means to execute code before or after transforming a batch of values. It looks
    reasonable to simply put the setup and cleanup code before and after a call to
    `map`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`map`函数一次只对一个输入进行操作，并且没有办法在转换一批值之前或之后执行代码。在调用`map`之前和之后放置设置和清理代码似乎是合理的。
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'But, this fails for several reasons:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这失败的原因有几个：
- en: It puts any objects used in `map` into the map function's closure, which requires
    that it be serializable (for example, by implementing `java.io.Serializable`).
    Not all objects will be serializable, thus exceptions may be thrown.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将`map`中使用的任何对象放入map函数的闭包中，这要求它是可序列化的（例如，通过实现`java.io.Serializable`）。并非所有对象都是可序列化的，因此可能会抛出异常。
- en: The `map` function is a transformation, rather than an operation, and is lazily
    evaluated. Thus, instructions after the `map` function are not guaranteed to be
    executed immediately.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map`函数是一个转换，而不是一个操作，它是惰性评估的。因此，在`map`函数之后的指令不能保证立即执行。'
- en: Even if the preceding issues were covered for a particular implementation, we
    would only be executing code on the driver, not necessarily freeing resources
    allocated by serialized copies.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使前面的问题针对特定的实现进行了处理，我们只会在驱动程序上执行代码，而不一定会释放由序列化副本分配的资源。
- en: 'The closest counterpart to a mapper in Spark is the `mapPartitions` method.
    This method does not map just one value to another value, but maps an Iterator
    of values to an Iterator of other values, akin to a bulk-map method. This means
    that the `mapPartitions` can allocate resources locally at its start:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中最接近mapper的方法是`mapPartitions`方法。这个方法不仅仅是将一个值映射到另一个值，而是将一个值的迭代器映射到另一个值的迭代器，类似于批量映射方法。这意味着`mapPartitions`可以在开始时在本地分配资源：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'However, releasing resources (`cleanup`) is not straightforward as we still
    experience the lazy evaluation problem; if resources are freed after the `map`,
    then the iterator may not have evaluated before the disappearance of those resources.
    One solution to this is as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，释放资源（`cleanup`）并不简单，因为我们仍然遇到了惰性评估的问题；如果资源在`map`之后被释放，那么在这些资源消失之前，迭代器可能还没有被评估。解决这个问题的一个方法如下：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we have the Spark code for ingest, there is an additional change that
    we could make, which is to add a `Geohash` field (see the following for more information
    on how to produce this field). To insert this field into the code, we will need
    an additional entry at the end of the GDELT attributes list:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了用于摄取的Spark代码，我们可以进行额外的更改，即添加一个`Geohash`字段（有关如何生成此字段的更多信息，请参见以下内容）。要将此字段插入代码，我们需要在GDELT属性列表的末尾添加一个额外的条目：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And a line to set the value of the `simpleFeature` type:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 并设置`simpleFeature`类型的值的一行：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we can run our Spark job to load the GeoMesa Accumulo instance with
    the GDELT data from HDFS. The two years of GDELT is around 100 million entries!
    You can check how much data is in Accumulo by using the Accumulo shell, run from
    the `accumulo/bin` directory:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以运行我们的Spark作业，从HDFS加载GDELT数据到GeoMesa Accumulo实例。GDELT的两年数据大约有1亿条目！您可以通过使用Accumulo
    shell来检查Accumulo中有多少数据，从`accumulo/bin`目录运行：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Geohash
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 地理哈希
- en: Geohash is a geocoding system invented by Gustavo Niemeyer. It is a hierarchical,
    spatial data structure that subdivides space into buckets of grid shape, which
    is one of the many applications of what is known as a Z-order curve and generally
    space-filling curves.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 地理哈希是由Gustavo Niemeyer发明的地理编码系统。它是一种分层的空间数据结构，将空间细分为网格形状的桶，这是所谓的Z顺序曲线和一般空间填充曲线的许多应用之一。
- en: Geohashes offer properties like arbitrary precision and the possibility of gradually
    removing characters from the end of the code to reduce its size (and gradually
    lose precision).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 地理哈希提供了诸如任意精度和逐渐删除代码末尾的字符以减小其大小（逐渐失去精度）等属性。
- en: As a consequence of the gradual precision degradation, nearby geographical locations
    will often (but not always) present similar prefixes. The longer a shared prefix
    is, the closer the two locations are; this is very useful in GeoMesa should we
    want to use points from a particular area, as we can use the `Geohash` field added
    in the preceding ingest code .
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于逐渐精度下降的结果，附近的地理位置通常（但并非总是）会呈现相似的前缀。共享前缀越长，两个位置越接近；这在GeoMesa中非常有用，因为我们可以使用前面摄入代码中添加的`Geohash`字段，如果我们想要使用特定区域的点。
- en: 'The main usages of Geohashes are:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Geohashes的主要用途是：
- en: As a unique identifier
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为唯一标识符
- en: To represent point data, for example, in databases
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，在数据库中表示点数据
- en: 'When used in a database, the structure of geo-hashed data has two advantages.
    First, data indexed by Geohash will have all points for a given rectangular area
    in contiguous slices (the number of slices depends on the precision required and
    the presence of Geohash *fault lines*). This is especially useful in database
    systems where queries on a single index are much easier or faster than multiple-index
    queries: Accumulo, for example. Second, this index structure can be used for a
    quick-and-dirty proximity search: the closest points are often among the closest
    Geohashes. These advantages make Geohashes ideal for use in GeoMesa. The following
    is an extract of code from David Allsopp''s excellent Geohash scala implementation
    [https://github.com/davidallsopp/geohash-scala](https://github.com/davidallsopp/geohash-scala).
    This code can be used to produce Geohashes based on a `lat`/`lon` input:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库中使用时，地理哈希数据的结构具有两个优点。首先，通过Geohash索引的数据将在给定矩形区域的所有点在连续的切片中（切片数量取决于所需的精度和Geohash
    *故障线*的存在）。这在数据库系统中特别有用，因为单个索引上的查询比多个索引查询更容易或更快：例如，Accumulo。其次，这种索引结构可以用于快速的近似搜索：最接近的点通常是最接近的Geohashes。这些优势使Geohashes非常适合在GeoMesa中使用。以下是David
    Allsopp出色的Geohash scala实现的代码摘录[https://github.com/davidallsopp/geohash-scala](https://github.com/davidallsopp/geohash-scala)。此代码可用于基于`lat`/`lon`输入生成Geohashes：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: One limitation of the Geohash algorithm is in attempting to utilize it to find
    points in proximity to each other based on a common prefix. Edge case locations
    that are close to each other, but on opposite sides of the 180 degrees meridian,
    will result in Geohash codes with no common prefix (different longitudes for near
    physical locations). Points that are close by at the North and South poles will
    have very different Geohashes (different longitudes for near physical locations).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Geohash算法的一个局限性在于试图利用它来找到具有共同前缀的相邻点。接近的边缘情况位置，它们彼此靠近，但位于180度子午线的对立面，将导致没有共同前缀的Geohash代码（接近物理位置的不同经度）。在北极和南极附近的点将具有非常不同的Geohashes（接近物理位置的不同经度）。
- en: Also, two close locations on either side of the equator (or Greenwich meridian)
    will not have a long common prefix since they belong to different halves of the
    world; one location's binary latitude (or longitude) will be 011111... and the
    other 100000... so they will not have a common prefix and most bits will be flipped.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，赤道（或格林威治子午线）两侧的两个接近位置将不会有长的公共前缀，因为它们属于世界的不同半球；一个位置的二进制纬度（或经度）将是011111...，另一个位置将是100000...，因此它们不会有共同的前缀，大多数位将被翻转。
- en: In order to do a proximity search, we could compute the southwest corner (low
    Geohash with low latitude and longitude) and northeast corner (high Geohash with
    high latitude and longitude) of a bounding box and search for Geohashes between
    those two. This will retrieve all points in the Z-order curve between the two
    corners; this also breaks down at the 180 meridians and the poles.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行近似搜索，我们可以计算一个边界框的西南角（低纬度和经度的低Geohash）和东北角（高纬度和经度的高Geohash），并搜索这两者之间的Geohashes。这将检索两个角之间Z顺序曲线上的所有点；这在180子午线和极点处也会中断。
- en: 'Finally, since a Geohash (in this implementation) is based on coordinates of
    longitude and latitude, the distance between two Geohashes reflects the distance
    in latitude/longitude coordinates between two points, which does not translate
    to actual distance. In this case, we can use the **Haversine** formula:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于Geohash（在此实现中）是基于经度和纬度坐标的，两个Geohashes之间的距离反映了两点之间纬度/经度坐标的距离，这并不等同于实际距离。在这种情况下，我们可以使用**Haversine**公式：
- en: '![Geohash](img/B05261_05_03.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![Geohash](img/B05261_05_03.jpg)'
- en: 'This gives us the actual distance between the two points taking into account
    the curvature of the earth, where:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了考虑到地球曲率的两点之间的实际距离，其中：
- en: '**r** is the radius of the sphere,'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**r**是球体的半径，'
- en: '**φ1**, **φ2**: latitude of point 1 and latitude of point 2, in radians'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**φ1**，**φ2**：点1的纬度和点2的纬度，以弧度表示'
- en: '**λ1**, **λ2**: longitude of point 1 and longitude of point 2, in radians'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**λ1**，**λ2**：点1的经度和点2的经度，以弧度表示'
- en: GeoServer
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GeoServer
- en: Now that we have successfully loaded GDELT data to Accumulo via GeoMesa, we
    can work towards visualizing that data on a map; this feature is very useful for
    plotting results of analytics on world maps, for example. GeoMesa integrates well
    with GeoServer for this purpose. GeoServer is an **Open Geospatial Consortium**
    (**OGC**) compliant with the implementation of a number of standards including
    **Web Feature Service** (**WFS**) and **Web Map Service** (**WMS**). "It publishes
    data from any major spatial data source".
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功通过GeoMesa将GDELT数据加载到Accumulo中，我们可以开始在地图上可视化这些数据；例如，这个功能对于在世界地图上绘制分析结果非常有用。GeoMesa与GeoServer很好地集成在一起。GeoServer是一个符合**开放地理空间联盟**（**OGC**）标准的实现，包括**Web要素服务**（**WFS**）和**Web地图服务**（**WMS**）。"它可以发布来自任何主要空间数据源的数据"。
- en: 'We are going to use GeoServer to view the results from our analytics in a clean,
    presentable way. Again, we are not going to delve into getting GeoServer up and
    running, as there is a very good tutorial in the GeoMesa documentation that enables
    the integration of the two. A couple of common points to watch out for are as
    follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用GeoServer以清晰、可呈现的方式查看我们分析结果。同样，我们不会深入研究如何启动和运行GeoServer，因为GeoMesa文档中有一个非常好的教程，可以实现两者的集成。需要注意的一些常见点如下：
- en: 'The system uses **Java Advanced Imaging** (**JAI**) libraries; if you have
    issues with these, specifically on a Mac, then these can often be fixed by removing
    the libraries from the default Java installation:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统使用**Java高级图像**（**JAI**）库；如果您在Mac上遇到问题，通常可以通过从默认Java安装中删除库来解决这些问题：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This will then allow the GeoServer versions to be used, located in `$GEOSERVER_HOME/webapps/geoserver/WEB-INF/lib/`
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用GeoServer版本，位于“$GEOSERVER_HOME/webapps/geoserver/WEB-INF/lib/”
- en: Again, we cannot stress the importance of versions. You must be very clear about
    which versions of the main modules you are using, for example, Hadoop, Accumulo,
    Zookeeper, and most importantly, GeoMesa. If you mix versions you will see problems
    and the stack traces often mask the true issue. If you do have exceptions, check
    and double-check your versions.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次强调版本的重要性。您必须非常清楚您正在使用的主要模块的版本，例如Hadoop，Accumulo，Zookeeper，最重要的是GeoMesa。如果混合使用不同版本，您将遇到问题，而堆栈跟踪通常会掩盖真正的问题。如果确实遇到异常，请检查并反复检查您的版本。
- en: Map layers
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 地图图层
- en: 'Once GeoServer is running, we can create a layer for visualization. GeoServer
    enables us to publish a single or a group of layers to produce a graphic. When
    we create a layer, we can specify the bounding box, view the feature (which is
    the `SimpleFeature` we created in the Spark code previously), and even run a **Common
    Query Language** (**CQL**) query to filter the data (more about this as follows).
    After a layer has been created, selecting layer preview and the JPG option will
    produce a URL with a graphic similar to the following; temporal bounding here
    is for January 2016 so that the map is not overcrowded:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦GeoServer运行，我们就可以创建一个用于可视化的图层。GeoServer使我们能够发布单个或一组图层以生成图形。创建图层时，我们可以指定边界框，查看要素（这是我们之前在Spark代码中创建的“SimpleFeature”），甚至运行**通用查询语言**（**CQL**）查询来过滤数据（后面将更多介绍）。创建图层后，选择图层预览和JPG选项将生成一个类似以下的图形的URL；这里的时间边界是2016年1月，以便地图不会过于拥挤：
- en: '![Map layers](img/image_05_002.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![地图图层](img/image_05_002.jpg)'
- en: 'The URL can be used to produce other graphics, simply by manipulating the arguments.
    A brief breakdown of the URL is given as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: URL可以用于通过操作参数生成其他图形。以下是URL的简要分解：
- en: 'The `geoserver` URL with the standard:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 具有标准的“geoserver”URL：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `request` type:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: “请求”类型：
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `layers` and `styles`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: “图层”和“样式”：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Set the layer `transparency`, if required:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，设置图层的“透明度”：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `cql` statement, in this case any row that has an entry with `GoldsteinScale>8`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，“cql”语句是任何具有“GoldsteinScale>8”条目的行：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The bounding box `bbox`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框“bbox”：
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `height` and `width` of the graphic:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图形的“高度”和“宽度”：
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Source and `image` type:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 源和“图像”类型：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Filter the content by temporal query bounds:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过时间查询边界过滤内容：
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The final step for this section is to attach a world map to this layer so that
    the image becomes more readable. If you search the Internet for world map shape
    files, there are a number of options; we have used one from [http://thematicmapping.org](http://thematicmapping.org).
    Adding one of these into GeoServer as a shape-file store, and then creating and
    publishing a layer before creating a layer group of our GDELT data and the shape-file,
    will produce an image similar to this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的最后一步是将世界地图附加到此图层，以使图像更易读。如果您在互联网上搜索世界地图形状文件，会有许多选项；我们使用了[http://thematicmapping.org](http://thematicmapping.org)上的一个选项。将其中一个添加到GeoServer作为形状文件存储，然后创建和发布一个图层，再创建我们的GDELT数据和形状文件的图层组，将产生类似于以下图像的图像：
- en: '![Map layers](img/image_05_003.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![地图图层](img/image_05_003.jpg)'
- en: To make things a bit more interesting, we have filtered the events based on
    the `GoldsteinScale` field in the `FeatureType`. By adding `cql_filter=GoldsteinScale
    > 8` to the URL, we can plot all of the points where the `GoldsteinScale` score
    was greater than eight; so essentially, the above image shows us where the highest
    levels of positive sentiment were located in the world, in January 2016!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使事情更有趣，我们根据“FeatureType”中的“GoldsteinScale”字段过滤了事件。通过在URL中添加“cql_filter=GoldsteinScale
    > 8”，我们可以绘制所有“GoldsteinScale”分数大于八的点；因此，上面的图像向我们展示了2016年1月世界上积极情绪水平最高的地方在哪里！
- en: CQL
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CQL
- en: '**Common Query Language** (**CQL**) is a plain text query language created
    by the OGC for the [Catalogue Web Services specification](http://www.opengeospatial.org/standards/cat).
    It is a human-readable query language (unlike, for example, [OGC filters](http://www.opengeospatial.org/standards/filter))
    and uses a similar syntax to SQL. Although similar to SQL, CQL has much less functionality;
    for example, it is quite strict in requiring an attribute to be on the left side
    of any comparison operator.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用查询语言**（**CQL**）是由OGC为[目录Web服务规范](http://www.opengeospatial.org/standards/cat)创建的一种纯文本查询语言。它是一种人类可读的查询语言（不像，例如，[OGC过滤器](http://www.opengeospatial.org/standards/filter)），并且使用与SQL类似的语法。尽管与SQL类似，但CQL的功能要少得多；例如，它在要求属性在任何比较运算符的左侧时非常严格。'
- en: 'The following lists the CQL supported operators:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出了CQL支持的运算符：
- en: 'Comparison operators: =, <>, >, >=, <, <='
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较运算符：=，<>，>，>=，<，<=
- en: 'ID, list and other operators: BETWEEN, BEFORE, AFTER, LIKE, IS, EXISTS, NOT,
    IN'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ID、列表和其他运算符：BETWEEN，BEFORE，AFTER，LIKE，IS，EXISTS，NOT，IN
- en: 'Arithmetic expression operators: +, -, *, /'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算术表达式运算符：+，-，*，/
- en: 'Geometric operators: EQUALS, DISJOINT, INTERSECTS, TOUCHES, CROSSES, WITHIN,
    CONTAINS, OVERLAPS, RELATE, DWITHIN, BEYOND'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几何运算符：EQUALS，DISJOINT，INTERSECTS，TOUCHES，CROSSES，WITHIN，CONTAINS，OVERLAPS，RELATE，DWITHIN，BEYOND
- en: Due to the limitations of CQL, GeoServer provides an extended version of CQL
    called ECQL. ECQL provides much of the missing functionality of CQL, providing
    a more flexible language that has more in common with SQL. GeoServer supports
    the use of both CQL and ECQL in WMS and WFS requests.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CQL的限制，GeoServer提供了一个名为ECQL的CQL扩展版本。ECQL提供了CQL的许多缺失功能，提供了一种更灵活的语言，与SQL更相似。GeoServer支持在WMS和WFS请求中使用CQL和ECQL。
- en: The quickest way to test CQL queries is to amend the URL of a layer such as
    the one we created above, when using JPGs for example, or to use the CQL box at
    the bottom of the layer option within GeoMesa.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 测试CQL查询的最快方法是修改图层的URL，例如我们上面创建的图层，例如使用JPG，或者在GeoMesa的图层选项底部使用CQL框。
- en: 'If we have several layers defined in one WMS request, such as:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在一个WMS请求中定义了几个图层，比如：
- en: '[PRE28]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then we may want to filter just one of those layers with the CQL query. In
    this case, CQL filters must be ordered in the same way that the layers are; we
    use the `INCLUDE` keyword for the layers that we don''t want to filter and delimit
    them using a ";". For example, to filter only `layer2` in our example, the WMS
    request would appear thus:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可能想要使用CQL查询过滤其中一个图层。在这种情况下，CQL过滤器必须按照图层的顺序进行排序；我们使用`INCLUDE`关键字来表示我们不想过滤的图层，并使用“;”进行分隔。例如，在我们的示例中，要仅过滤`layer2`，WMS请求将如下所示：
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Be aware when using columns of type `Date`; we need to determine their format
    before attempting any CQL with them. Usually they will be in ISO8601 format; 2012-01-01T00:00:00Z.
    However, different formats may be present depending upon how the data was loaded.
    In our example, we have ensured the SQLDATE is in the correct format.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`Date`类型的列时要注意；我们需要确定它们的格式，然后再尝试使用CQL。通常它们将采用ISO8601格式；2012-01-01T00:00:00Z。然而，根据数据加载的方式，可能会出现不同的格式。在我们的示例中，我们已确保SQLDATE的格式是正确的。
- en: Gauging oil prices
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量油价
- en: Now that we have a substantial amount of data in our data store (we can always
    add more data using the preceding Spark job) we will proceed to query that data,
    using the GeoMesa API, to get the rows ready for application to our learning algorithm.
    We could of course use raw GDELT files, but the following method is a useful tool
    to have available.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据存储中有大量数据（我们可以始终使用前面的Spark作业添加更多数据），我们将继续查询这些数据，使用GeoMesa API，准备好行以应用于我们的学习算法。当然，我们可以使用原始GDELT文件，但以下方法是一个有用的工具。
- en: Using the GeoMesa query API
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GeoMesa查询API
- en: The GeoMesa query API enables us to query for results based upon spatio-temporal
    attributes, whilst also leveraging the parallelization of the data store, in this
    case Accumulo with its iterators. We can use the API to build `SimpleFeatureCollections`,
    which we can then parse to realize GeoMesa `SimpleFeatures` and ultimately the
    raw data that matches our query.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: GeoMesa查询API使我们能够基于时空属性查询结果，同时利用数据存储的并行化，本例中是Accumulo和其迭代器。我们可以使用API构建`SimpleFeatureCollections`，然后解析以实现GeoMesa`SimpleFeatures`，最终匹配我们查询的原始数据。
- en: 'At this stage we should build code that is generic, such that we can change
    it easily should we decide later that we have not used enough data, or perhaps
    if we need to change the output fields. Initially, we will extract a few fields;
    `SQLDATE`, `Actor1Name`, `Actor2Name`, and `EventCode`. We should also decide
    on the bounding box for our queries; as we are looking at three different oil
    indexes there is a decision to be made about how we suppose the geographical influence
    of events relates to the oil price itself. This is one of the most difficult variables
    to evaluate, as there are so many factors involved in the price determination;
    arguably the bounding box is the whole world. However, as we are using three indexes,
    we are going to make the assumption that each index has its own geographic limitations,
    based on research regarding the areas of oil supply and the areas of demand. We
    can always vary these bounds later should we have more relevant information, or
    if the results are not favorable and we need to re-evaluate. The proposed initial
    bounding boxes are:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们应该构建通用的代码，这样我们可以很容易地改变它，如果我们决定以后没有使用足够的数据，或者也许如果我们需要改变输出字段。最初，我们将提取一些字段；`SQLDATE`，`Actor1Name`，`Actor2Name`和`EventCode`。我们还应该决定我们查询的边界框；因为我们正在查看三种不同的石油指数，所以我们需要决定事件的地理影响如何与石油价格本身相关。这是最难评估的变量之一，因为在价格确定中涉及了很多因素；可以说边界框是整个世界。然而，由于我们使用了三个指数，我们将假设每个指数都有自己的地理限制，这是基于有关石油供应地区和需求地区的研究。如果我们有更多相关信息，或者结果不理想并且需要重新评估，我们随时可以稍后改变这些边界。建议的初始边界框是：
- en: 'Brent: North Sea and the UK (Supply) and Central Europe (Demand): 34.515610,
    -21.445313 - 69.744748, 36.914063'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布伦特：北海和英国（供应）和中欧（需求）：34.515610，-21.445313 - 69.744748，36.914063
- en: 'WTI: America (Supply) and Western Europe (Demand): -58.130121, -162.070313,
    71.381635, -30.585938'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WTI：美国（供应）和西欧（需求）：-58.130121，-162.070313，71.381635，-30.585938
- en: 'OPEC: The Middle East (Supply) and Europe (Demand): -38.350273, -20.390625,
    38.195022, 149.414063'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧佩克：中东（供应）和欧洲（需求）：-38.350273，-20.390625，38.195022，149.414063
- en: 'The code to extract our results from GeoMesa is as follows (Brent Oil):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从GeoMesa提取结果的代码如下（布伦特原油）：
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `RDD[Row]` collection can be written to disk for future use as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`RDD[Row]`集合可以按以下方式写入磁盘以供将来使用：'
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We should read in as much data as possible at this point in order to provide
    our algorithm with a large amount of training data. We will split our input data
    between training and test data at a later stage. Therefore, there is no need to
    hold any data back.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在这一点上尽可能多地读取数据，以便为我们的算法提供大量的训练数据。我们将在以后的阶段将我们的输入数据分为训练和测试数据。因此，没有必要保留任何数据。
- en: Data preparation
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'At this stage, we have obtained our data from GeoMesa based on the bounding
    box, and the date range, for a particular oil index. The output has been organized
    such that we have a collection of rows, each one containing the supposed important
    details for one event. We are not sure whether the fields we have chosen for each
    event are entirely relevant in providing enough information to build a reliable
    model so, depending upon our results, this is something that we may have to experiment
    with at a later date. We next need to transform the data into something that can
    be used by our learning process. In this case, we will aggregate the data into
    one-week blocks and transform the data into a typical *bag of words*, starting
    by loading the data from the previous step:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经根据边界框和日期范围从GeoMesa获取了我们的数据，用于特定的石油指数。输出已经被组织起来，以便我们有一系列行，每一行包含一个事件的所谓重要细节。我们不确定我们为每个事件选择的字段是否完全相关，能够提供足够的信息来构建可靠的模型，因此，根据我们的结果，这是我们可能需要在以后进行实验的事情。接下来，我们需要将数据转换为可以被我们的学习过程使用的形式。在这种情况下，我们将数据聚合成为一周的数据块，并将数据转换为典型的“词袋”，首先从上一步加载数据开始：
- en: '[PRE32]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Within this RDD, we have the `EventCodes` (CAMEO codes): these will need to
    be transformed into their respective descriptions, so that the bag of words can
    be built. By downloading the CAMEO codes from [http://gdeltproject.org/data/lookups/CAMEO.eventcodes.txt](http://gdeltproject.org/data/lookups/CAMEO.eventcodes.txt),
    we can create a `Map` object for use in the next step:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个RDD中，我们有“EventCodes”（CAMEO代码）：这些将需要转换为它们各自的描述，以便构建词袋。通过从[http://gdeltproject.org/data/lookups/CAMEO.eventcodes.txt](http://gdeltproject.org/data/lookups/CAMEO.eventcodes.txt)下载CAMEO代码，我们可以为下一步创建一个“Map”对象：
- en: '[PRE33]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note that we normalize the output by removing any non-standard characters; the
    aim of this is to try and avoid erroneous characters affecting our training model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们通过删除任何非标准字符来规范化输出；这样做的目的是尝试避免错误字符影响我们的训练模型。
- en: 'We can now create our `bagOfWordsRDD` by appending the actor codes either side
    of the `EventCode` mapped description, and create a DataFrame from the date and
    formed sentence:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过在“EventCode”映射描述的两侧附加演员代码来创建我们的`bagOfWordsRDD`，并从日期和形成的句子创建一个DataFrame：
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We have previously mentioned that we could work with our data at a daily, weekly,
    or even yearly level; by choosing weekly, we will next need to group our DataFrame
    by week. In Spark 2.0, we can achieve this easily using window functions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，我们可以在每日、每周甚至每年的水平上处理我们的数据；通过选择每周，我们接下来需要按周对我们的DataFrame进行分组。在Spark 2.0中，我们可以使用窗口函数轻松实现这一点：
- en: '[PRE35]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As we will produce the oil price data for the end of each week, we should ensure
    that our sentence data is grouped for the days Friday to Thursday, so that we
    can later join this with the price data for that Friday. This is achieved by altering
    the fourth argument of the `window` function; in this case, one day provided the
    correct grouping. If we run the command `sentencesDF.printSchema`, we will see
    that the `sentenceArray` column is an array of strings, while we need just a `String`
    for the input to our learning algorithms. The next code extract demonstrates this
    change, as well as producing the column `commonFriday`, which gives us a reference
    for the date we are working around for each row, as well as a unique key that
    we can join with later:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将为每周末生成石油价格数据，因此我们应确保我们的句子数据在周五到周四之间分组，以便稍后可以将其与该周五的价格数据进行连接。这是通过更改“window”函数的第四个参数来实现的；在这种情况下，一天提供了正确的分组。如果我们运行命令“sentencesDF.printSchema”，我们将看到“sentenceArray”列是一个字符串数组，而我们需要的是学习算法的输入的一个字符串。下一个代码片段演示了这种变化，以及生成“commonFriday”列，它为我们每一行工作的日期提供了一个参考，以及一个我们稍后可以连接的唯一键：
- en: '[PRE36]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The next step is to collect our data and label it for use in the next stage.
    In order to label it, we must normalize the oil price data we downloaded. Earlier
    in this chapter we mentioned the frequency of data points; at the moment the data
    contains a date and the price at the end of that day. We need to transform our
    data into tuples of (Date, change) where the Date is the Friday of that week and
    the change is a rise or fall based on the average of the daily prices from the
    previous Monday onwards; if the price stays the same, we'll take this to be a
    fall so that we can implement binary value learning algorithms later.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是收集我们的数据并为下一阶段的使用进行标记。为了对其进行标记，我们必须对下载的油价数据进行归一化处理。在本章的前面部分，我们提到了数据点的频率；目前数据包含日期和当天结束时的价格。我们需要将我们的数据转换为元组（日期，变化），其中日期是该周五的日期，变化是基于从上周一开始的每日价格的平均值的上升或下降；如果价格保持不变，我们将把这视为下降，以便稍后可以实现二进制值学习算法。
- en: 'We can again use the window feature in Spark DataFrames to easily group the
    data by week; we will also reformat the date as follows, so that the window group
    function performs correctly:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次使用Spark DataFrames中的窗口功能轻松地按周对数据进行分组；我们还将重新格式化日期，以便窗口组函数正确执行：
- en: '[PRE37]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This will produce something similar to this:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生类似于这样的东西：
- en: '[PRE38]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now we can calculate the rise or fall from the previous week; first by adding
    the previous week''s `last(PRICE)` to each row (using the Spark `lag` function),
    and then by calculating the result:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算上周的涨跌幅；首先通过将上周的“last(PRICE)”添加到每一行（使用Spark的“lag”函数），然后计算结果：
- en: '[PRE39]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You will notice the use of the `signum` function; this is very useful for comparison
    as it produces the following outcomes:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到使用了“signum”函数；这对于比较非常有用，因为它产生以下结果：
- en: If the first value is less than the second, output -1
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果第一个值小于第二个值，则输出-1
- en: If the first value is greater than the second, output +1
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果第一个值大于第二个值，则输出+1
- en: If the two values are equal, output 0
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个值相等，则输出0
- en: 'Now that we have the two DataFrames, `aggSentenceDF` and `oilPriceChangeDF`,
    we can join the two using the `commonFriday` column to produce a labeled dataset:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个DataFrame，“aggSentenceDF”和“oilPriceChangeDF”，我们可以使用“commonFriday”列将这两个数据集连接起来，以产生一个带标签的数据集：
- en: '[PRE40]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We also drop the window and `sentenceArray` columns, as well as add an ID column,
    so that we can uniquely reference each row:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还删除窗口和`sentenceArray`列，并添加一个ID列，以便我们可以唯一引用每一行：
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Machine learning
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习
- en: 'We now have input data and the weekly price change; next, we will turn our
    GeoMesa data into numerical vectors that a machine-learning model can work with.
    The Spark machine learning library, MLlib, has a utility called `HashingTF` to
    do just that. `HashingTF` transforms a bag of words into a vector of term frequencies
    by applying a hash function to each term. Because the vector has a finite number
    of elements, it''s possible that two terms will map to the same, hashed term;
    the hashed, vectorized features may not exactly represent the actual content of
    the input text. So, we''ll set up a relatively large feature vector, accommodating
    10,000 different hashed values, to reduce the chance of these collisions. The
    logic behind this is that there are only so many possible events (regardless of
    their size) and therefore a repeat of a previously seen event should produce a
    similar outcome. Of course, the combination of events may change this, which is
    accounted for by initially taking one-week blocks. To format the input data correctly
    for `HashingTF`, we will also execute a `Tokenizer` over the input text:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了输入数据和每周的价格变动；接下来，我们将把我们的GeoMesa数据转换成机器学习模型可以处理的数值向量。Spark机器学习库MLlib有一个叫做`HashingTF`的实用程序来做到这一点。`HashingTF`通过对每个术语应用哈希函数，将词袋转换为术语频率向量。因为向量有有限数量的元素，可能会出现两个术语映射到相同的哈希术语；哈希化的向量特征可能不完全代表输入文本的实际内容。因此，我们将设置一个相对较大的特征向量，容纳10,000个不同的哈希值，以减少这些碰撞的机会。这背后的逻辑是，可能事件只有那么多（不管它们的大小），因此先前看到的事件的重复应该产生类似的结果。当然，事件的组合可能会改变这一点，这是通过最初采取一周的时间块来考虑的。为了正确格式化输入数据以供`HashingTF`使用，我们还将在输入文本上执行一个`Tokenizer`：
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The final preparation step is to implement an **Inverse Document Frequency**
    (**IDF**), this is a numerical measure of how much information each term provides:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的准备步骤是实现**逆文档频率**（**IDF**），这是每个术语提供多少信息的数值度量：
- en: '[PRE43]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: For the purposes of this exercise, we will implement a Naive Bayes implementation
    to perform the machine learning part of our functionality. This algorithm is a
    good initial fit to learn outcomes from a series of inputs; in our case, we hope
    to learn an increase or decrease in oil price given a set of events from the previous
    week.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个练习的目的，我们将实现一个朴素贝叶斯实现来执行我们功能的机器学习部分。这个算法是一个很好的初始拟合，可以从一系列输入中学习结果；在我们的情况下，我们希望学习在给定上周一系列事件的情况下，油价的增加或减少。
- en: Naive Bayes
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: 'Naive Bayes is a simple technique for constructing classifiers: models that
    assign class labels to problem instances, represented as vectors of feature values,
    where the class labels are drawn from some finite set. Naive Bayes is available
    in Spark MLlib, thus:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一种简单的构建分类器的技术：模型将类标签分配给问题实例，表示为特征值向量，其中类标签来自某个有限集合。朴素贝叶斯在Spark MLlib中可用，因此：
- en: '[PRE44]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can tie all of the above steps together using an MLlib Pipeline; a Pipeline
    can be thought of as a workflow that simplifies the combination of multiple algorithms.
    From the Spark documentation some definitions are as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用MLlib Pipeline将所有上述步骤绑在一起；Pipeline可以被认为是一个简化多个算法组合的工作流程。从Spark文档中，一些定义如下：
- en: 'DataFrame: This ML API uses DataFrames from Spark SQL as an ML dataset, which
    can hold a variety of data types. For example, a DataFrame could have different
    columns storing text, feature vectors, true labels, and predictions.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame：这个ML API使用来自Spark SQL的DataFrame作为ML数据集，可以容纳各种数据类型。例如，一个DataFrame可以有不同的列存储文本、特征向量、真实标签和预测。
- en: 'Transformer: A Transformer is an algorithm that can transform one DataFrame
    into another DataFrame. For example, an ML model is a Transformer that transforms
    a DataFrame with features into a DataFrame with predictions.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器：转换器是一种可以将一个DataFrame转换为另一个DataFrame的算法。例如，一个ML模型是一个将带有特征的DataFrame转换为带有预测的DataFrame的转换器。
- en: 'Estimator: An Estimator is an algorithm that can "fit" a DataFrame to produce
    a Transformer. For example, a learning algorithm is an Estimator that trains on
    a DataFrame and produces a model.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计器：估计器是一种可以“拟合”DataFrame以产生转换器的算法。例如，学习算法是一个可以在DataFrame上进行训练并产生模型的估计器。
- en: 'Pipeline: A Pipeline chains multiple Transformers and Estimators together to
    specify an ML workflow.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pipeline：Pipeline将多个转换器和估计器链接在一起，以指定一个ML工作流程。
- en: 'The `pipeline` is declared thus:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipeline`被声明如下：'
- en: '[PRE45]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We noted previously, that all of the available data should be read from GeoMesa,
    as we would split the data at a later stage in order to provide training and test
    data sets. This is performed here:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前注意到，所有可用的数据都应该从GeoMesa中读取，因为我们将在后期分割数据，以提供训练和测试数据集。这是在这里执行的：
- en: '[PRE46]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'And finally, we can execute the full model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以执行完整的模型：
- en: '[PRE47]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The model can be saved and loaded easily:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以轻松保存和加载：
- en: '[PRE48]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Results
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'To test our model, we should execute the `model` transformer, mentioned as
    follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的模型，我们应该执行`model`转换器，如下所述：
- en: '[PRE49]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This provides a prediction for each of the input rows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这为每个输入行提供了一个预测：
- en: '[PRE50]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The results, having been taken from the resultant DataFrame (`model.transform(testDF).select("rawPrediction",
    "probability", "prediction").show`), are as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，从结果DataFrame中取出（`model.transform(testDF).select("rawPrediction", "probability",
    "prediction").show`），如下所示：
- en: '[PRE51]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Analysis
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析
- en: In a problem space such as oil price prediction, it is always going to be very
    difficult/near impossible to create a truly successful algorithm, so this chapter
    was always geared towards more of a demonstration piece. However, we have results
    and their legitimacy is not irrelevant; we trained the above algorithms with several
    years of data from the oil indexes and GDELT, and then gleaned the results from
    the outcome of the model execution before comparing it to the correct label.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在像石油价格预测这样的问题领域中，要创建一个真正成功的算法总是非常困难/几乎不可能的，因此本章始终是更多地向演示性质靠拢。然而，我们有了结果，它们的合法性并不无关紧要；我们用石油指数和GDELT的几年数据训练了上述算法，然后从模型执行的结果中获取了结果，再将其与正确的标签进行比较。
- en: In tests, the previous model showed a 51% accuracy. This is marginally better
    than what we would expect from simply selecting results at random, but provides
    a firm base upon which to make improvements. With the ability to save data sets
    and models, it would be straightforward to make changes to the model during efforts
    to improve accuracy.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试中，先前的模型显示了51%的准确性。这比我们从简单地随机选择结果所期望的稍微好一点，但为改进提供了坚实的基础。通过保存数据集和模型的能力，在努力提高准确性的过程中，对模型进行更改将是直截了当的。
- en: There are many areas of improvement that can be made and we have already mentioned
    some of them during this chapter. In order to improve our model, we should address
    the specific areas in a systematic manner. As we can only make an educated guess
    as to which changes will affect an improvement, it is important to try and address
    the areas of greatest concern first. Following, is a brief summary of how we might
    approach these changes. We should always visit our hypotheses and determine whether
    they are still valid, or where changes should be made.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可以改进的地方，我们在本章已经提到了其中一些。为了改进我们的模型，我们应该以系统化的方式解决特定领域的问题。由于我们只能就哪些改变会带来改进做出合理猜测，因此重要的是首先尝试解决最关键的问题领域。接下来，我们简要总结一下我们可能如何处理这些改变。我们应该经常检查我们的假设，确定它们是否仍然有效，或者需要做出哪些改变。
- en: 'Hypothesis 1: *"The supply and demand [of oil] is affected, to a greater extent,
    by world events and thus we can predict what that supply and demand is likely
    to be."* Our initial attempt at a model has shown 51% accuracy; although this
    is not enough to determine that this hypothesis is valid, it is worth continuing
    with other areas of the model and to attempt to improve accuracy before discounting
    the hypothesis altogether.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 假设1：“石油的供需受世界事件的影响更大，因此我们可以预测供需可能会是什么样。”我们初步尝试建立的模型显示了51%的准确性；虽然这还不足以确定这个假设是否有效，但在放弃这个假设之前，继续改进模型的其他方面是值得的。
- en: 'Hypothesis 2: *"The level of detail of the event will provide better or worse
    accuracy from our algorithm."* We have huge scope for change here; there are several
    areas where we could amend the code and re-run the model quickly, for example:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 假设2：“事件的详细程度将从我们的算法中提供更好或更差的准确性。”在这里，我们有很大的改变空间；有几个领域我们可以修改代码并快速重新运行模型，例如：
- en: 'Number of events: does an increase affect accuracy?'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件数量：增加是否会影响准确性？
- en: 'Daily/Weekly/Monthly data roundups: weekly round-ups may not ever give good
    results'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每日/每周/每月的数据汇总：每周汇总可能永远不会产生良好的结果
- en: 'Limited data sets: we currently only use a few fields from GDELT, would more
    fields help with the accuracy?'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有限的数据集：我们目前只使用了GDELT的少数字段，增加更多字段是否有助于提高准确性？
- en: 'Preclusion of any other types of data: would the introduction of GKG data help
    with accuracy?'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除其他类型的数据：引入GKG数据是否有助于提高准确性？
- en: In conclusion, we perhaps have more questions than we started with; however,
    we have now done the ground work to produce an initial model upon which we can
    build, hopefully improving accuracy and leading to a further understanding of
    the data and its potential effect on oil prices.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们可能比开始时有更多的问题；然而，我们现在已经做好了基础工作，建立了一个初步模型，希望能够提高准确性，并进一步了解数据及其对石油价格的潜在影响。
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have introduced the concepts of storing data in a spatio-temporal
    way so that we can use GeoMesa and GeoServer to create and run queries. We have
    shown these queries executed in both the tools themselves and in a programmatic
    way, leveraging GeoServer to display results. Further, we have demonstrated how
    to merge different artifacts to create insights purely from the raw GDELT events,
    before any follow-on processing. Following on from GeoMesa, we have touched upon
    the highly complex world of oil pricing and worked on a simple algorithm to estimate
    weekly oil changes. Whilst it is not reasonable to create an accurate model with
    the time and resources available, we have explored a number of areas of concern
    and attempted to address these, at least at a high level, in order to give an
    insight into possible approaches that can be made in this problem space.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了将数据以时空方式存储的概念，以便我们可以使用GeoMesa和GeoServer来创建和运行查询。我们展示了这些查询在这些工具本身以及以编程方式执行的情况，利用GeoServer来显示结果。此外，我们还演示了如何合并不同的工件，纯粹从原始的GDELT事件中创建见解，而不需要任何后续处理。在GeoMesa之后，我们涉及了高度复杂的石油定价世界，并致力于一个简单的算法来估计每周的石油变化。虽然在现有的时间和资源下创建一个准确的模型是不合理的，但我们已经探讨了许多关注领域，并试图至少在高层次上解决这些问题，以便提供可能在这个问题领域中可以采取的方法的见解。
- en: Throughout the chapter, we have introduced a number of key Spark libraries and
    functions, the key area being MLlib which we will see in further detail during
    the course of the rest of this book.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些关键的Spark库和函数，其中关键的领域是MLlib，我们将在本书的其余部分中更详细地了解它。
- en: In the next chapter, [Chapter 6](ch06.xhtml "Chapter 6. Scraping Link-Based
    External Data"), *Scraping Link-Based External Data*, we further implement the
    GDELT dataset to build a web scale news scanner for tracking trends.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，第6章，“抓取基于链接的外部数据”，我们进一步实施GDELT数据集，构建一个用于跟踪趋势的网络规模新闻扫描器。
