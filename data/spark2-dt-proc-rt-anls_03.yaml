- en: Structured Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: As you might already have understood from the previous chapters, Apache Spark
    is currently in transition from RDD-based data processing to a more structured
    one, backed by DataFrames and Datasets in order to let Catalyst and Tungsten kick
    in for performance optimizations. This means that the community currently uses
    a double-tracked approach. While the unstructured APIs are still supported--they
    haven't even been marked as deprecated yet ,and it is questionable if they ever
    will--a new set of structured APIs has been introduced for various components
    with Apache Spark V 2.0, and this is also true for Spark Streaming. Structured
    Steaming was marked stable in Apache Spark V 2.2\. Note that, as of Apache Spark
    V 2.1 when ...
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经从前几章理解的那样，Apache Spark 目前正从基于 RDD 的数据处理过渡到更结构化的处理，背后有 DataFrames 和 Datasets
    支持，以便让 Catalyst 和 Tungsten 发挥作用，进行性能优化。这意味着社区目前采用双轨制。虽然非结构化 API 仍然得到支持——它们甚至还没有被标记为已弃用，而且它们是否会这样做也值得怀疑——但在
    Apache Spark V 2.0 中为各种组件引入了一套新的结构化 API，这也适用于 Spark Streaming。Structured Steaming
    在 Apache Spark V 2.2 中被标记为稳定。请注意，截至 Apache Spark V 2.1 时...
- en: The concept of continuous applications
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续应用的概念
- en: Streaming apps tend to grow in complexity. Streaming computations don't run
    in isolation; they interact with storage systems, batch applications, and machine
    learning libraries. Therefore, the notion of continuous applications--in contrast
    to batch processing--emerged, and basically means the composite of batch processing
    and real-time stream processing with a clear focus of the streaming part being
    the main driver of the application, and just accessing the data created or processed
    by batch processes for further augmentation. Continuous applications never stop
    and continuously produce data as new data arrives.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 流应用程序往往变得复杂。流计算不是孤立运行的；它们与存储系统、批处理应用程序和机器学习库交互。因此，与批处理相对的连续应用的概念应运而生，基本上意味着批处理和实时流处理的组合，其中流处理部分是应用程序的主要驱动力，并且仅访问由批处理过程创建或处理的数据以进行进一步增强。连续应用程序永不停止，并且随着新数据的到达而持续产生数据。
- en: True unification - same code, same engine
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真正的统一 - 相同的代码，相同的引擎
- en: So a continuous application could also be implemented on top of RDDs and DStreams
    but would require the use of use two different APIs. In Apache Spark Structured
    Streaming the APIs are unified. This unification is achieved by seeing a structured
    stream as a relational table without boundaries where new data is continuously
    appended to the bottom of it. In batch processing on DataFrames using the relational
    API or SQL, intermediate DataFrames are created. As stream and batch computing
    are unified on top of the Apache SparkSQL engine, when working with structured
    streams, intermediate relational tables without boundaries are created.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个连续的应用程序也可以基于 RDD 和 DStreams 实现，但需要使用两种不同的 API。在 Apache Spark Structured
    Streaming 中，API 得到了统一。这种统一是通过将结构化流视为一张无边界的关系表来实现的，其中新数据不断追加到表的底部。在批处理中使用关系 API
    或 SQL 处理 DataFrames 时，会创建中间 DataFrames。由于流和批处理在 Apache SparkSQL 引擎上得到了统一，当处理结构化流时，会创建无边界的中间关系表。
- en: It is important to note that one can mix (join) static and incremental ...
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，可以混合（连接）静态和增量...
- en: Windowing
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口化
- en: Open source and commercial streaming engines such as IBM Streams, Apache Storm,
    or Apache Flink are using the concept of windows.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 开源和商业流处理引擎，如 IBM Streams、Apache Storm 或 Apache Flink，都在使用窗口的概念。
- en: Windows specify the granularity or number of subsequent records, which are taken
    into account when executing aggregation functions on streams.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 指定了粒度或后续记录的数量，这些记录在执行流上的聚合函数时会被考虑。
- en: How streaming engines use windowing
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流处理引擎如何使用窗口化
- en: There exist five different properties in two dimensions, which is how windows
    can be defined, where each window definition needs to use one property of each
    dimension.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 存在五个不同的属性，分为两个维度，这就是窗口如何被定义的方式，其中每个窗口定义都需要使用每个维度的一个属性。
- en: 'The first property is the mode in which subsequent windows of a continuous
    stream of tuples can be created: sliding and tumbling.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个属性是连续流中元组的后续窗口可以创建的模式：滑动和翻滚。
- en: 'The second is that the number of tuples that fall into a window has to be specified:
    either count-based, time-based or session-based.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个是必须指定落入窗口的元组数量：基于计数、基于时间或基于会话。
- en: 'Let''s take a look at what they mean:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看它们的含义：
- en: '**Sliding windows**: A sliding window removes a tuple from it whenever a new
    tuple is eligible to be included.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滑动窗口**：每当有新元组符合条件被纳入时，滑动窗口就会移除一个元组。'
- en: '**Tumbling windows**: A tumbling window removes all tuples from it whenever
    there are enough tuples arriving to create a new window.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻滚窗口**：每当有足够多的元组到达以创建新窗口时，翻滚窗口就会移除所有元组。'
- en: '**Count-based ...**'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于计数的...**'
- en: How Apache Spark improves windowing
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark如何优化窗口操作
- en: Apache Spark structured streaming is significantly more flexible in the window-processing
    model. As streams are virtually treated as continuously appended tables, and every
    row in such a table has a timestamp, operations on windows can be specified in
    the query itself and each query can define different windows. In addition, if
    there is a timestamp present in static data, window operations can also be defined,
    leading to a very flexible stream-processing model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark结构化流在窗口处理模型中展现出显著的灵活性。由于流被视为持续追加的表，且表中每行都带有时间戳，窗口操作可以在查询中直接指定，每个查询可以定义不同的窗口。此外，如果静态数据中存在时间戳，窗口操作也可以定义，从而形成一个非常灵活的流处理模型。
- en: In other words, Apache Spark windowing is just a sort of special type of grouping
    on the timestamp column. This makes it really easy to handle late arriving data
    as well because Apache Spark can include it in the appropriate window and rerun
    the computation on that window when a certain data item arrives late. This feature
    is highly configurable.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 换言之，Apache Spark的窗口操作本质上是对时间戳列的一种特殊分组。这使得处理迟到数据变得非常简单，因为Apache Spark可以将迟到数据纳入适当的窗口，并在特定数据项迟到时重新计算该窗口。此功能高度可配置。
- en: '**Event time versus processing time**: In time series analysis and especially
    in stream computing, each record is assigned to a particular timestamp. One way
    of creating such a timestamp is the arrival time at the stream-processing engine.
    Often, this is not what you want. Usually, you want to assign an event time for
    each record at that particular point in time when it was created, for example,
    when a measurement on an IoT device took place. This allows coping with latency
    between creating and processing of an event, for example, when an IoT sensor was
    offline for a certain amount of time, or network congestion caused a delay of
    data delivery.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**事件时间与处理时间对比**：在时间序列分析中，尤其是在流计算中，每个记录都会被分配一个特定的时戳。一种创建这种时戳的方法是记录到达流处理引擎的时间。然而，这往往并非所需。通常，我们希望为每个记录分配一个事件时间，即该记录创建时的特定时间点，例如，当物联网设备进行测量时。这有助于处理事件创建与处理之间的延迟，例如，当物联网传感器离线一段时间，或网络拥堵导致数据交付延迟时。'
- en: The concept of late data is interesting when using event time instead of processing
    time to assign a unique timestamp to each tuple. Event time is the timestamp when
    a particular measurement took place, for example. Apache Spark structured streaming
    can automatically cope with subsets of data arriving at a later point in time
    transparently.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用事件时间而非处理时间为每个元组分配唯一时戳时，迟到数据的概念颇具趣味。事件时间是指特定测量发生的时间戳。Apache Spark结构化流能够自动透明地处理在稍后时间点到达的数据子集。
- en: '**Late data**: If a record arrives at any streaming engine, it is processed
    immediately. Here, Apache Spark streaming doesn''t differ from other engines.
    However, Apache Spark has the capability of determining the corresponding windows
    a certain tuple belongs to at any time. If for whatever reason, a tuple arrives
    late, all affected windows will be updated and all affected aggregate operations
    based on these updated windows are rerun. This means that results are allowed
    to change over time in case late data arrives. This is supported out of the box
    without the programmer worrying about it. Finally, since Apache Spark V2.1, it
    is possible to specify the amount of time that the system accepts late data using
    the `withWatermark` method.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**迟到数据**：无论记录何时到达任何流引擎，都会立即处理。在此方面，Apache Spark流处理与其他引擎并无二致。然而，Apache Spark具备在任何时间确定特定元组所属窗口的能力。如果由于任何原因元组迟到，所有受影响的窗口将被更新，基于这些更新窗口的所有受影响聚合操作将重新运行。这意味着，如果迟到数据到达，结果允许随时间变化，而无需程序员为此担忧。最后，自Apache
    Spark V2.1起，可以使用`withWatermark`方法指定系统接受迟到数据的时间量。'
- en: The watermark is basically the threshold, used to define how old a late arriving
    data point is allowed to be in order to still be included in the respective window.
    Again, consider the HTTP server log file working over a minute length window.
    If, for whatever reason, a data tuple arrives which is more than 4 hours old it
    might not make sense to include it in the windows if, for example, this application
    is used to create a time-series forecast model on an hourly basis to provision
    or de-provision additional HTTP servers to a cluster. A four-hour-old data point
    just wouldn't make sense to process, even if it could change the decision, as
    the decision has already been made.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 水印基本上是阈值，用于定义延迟到达的数据点允许有多旧，以便仍能被包含在相应的窗口中。再次考虑HTTP服务器日志文件在超过一分钟长度的窗口上工作。如果，由于任何原因，一个数据元组到达，它超过4小时旧，如果这个应用程序用于创建基于小时的时间序列预测模型来为集群提供或取消提供额外的HTTP服务器，那么它可能没有意义将其包含在窗口中。一个四小时前的数据点就没有意义处理，即使它可能改变决策，因为决策已经做出。
- en: Increased performance with good old friends
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与老朋友一起提升性能
- en: As in Apache SparkSQL for batch processing and, as Apache Spark structured streaming
    is part of Apache SparkSQL, the Planner (Catalyst) creates incremental execution
    plans as well for mini-batches. This means that the whole streaming model is based
    on batches. This is the reason why a unified API for streams and batch processing
    could be achieved. The price we pay is that Apache Spark streaming sometimes has
    drawbacks when it comes to very low latency requirements (sub-second, in the range
    of tens of ms). As the name Structured Streaming and the usage of DataFrames and
    Datasets implies, we are also benefiting from performance improvements due to
    project Tungsten, which has been introduced in a previous ...
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在Apache SparkSQL中用于批处理，以及作为Apache Spark结构化流的一部分，Catalyst Planner也为微批创建增量执行计划。这意味着整个流模型基于批处理。这也是为什么能够实现流处理和批处理的统一API的原因。我们付出的代价是，Apache
    Spark流处理在面对极低延迟要求（亚秒级，在几十毫秒范围内）时有时会有缺点。正如结构化流和使用DataFrame及Dataset所暗示的，我们也因Tungsten项目带来的性能提升而受益，该项目在之前的...
- en: How transparent fault tolerance and exactly-once delivery guarantee is achieved
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现透明的容错和精确一次投递保证
- en: Apache Spark structured streaming supports full crash fault tolerance and exactly-once
    delivery guarantee without the user taking care of any specific error handling
    routines. Isn't this amazing? So how is this achieved?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark结构化流支持完全崩溃容错和精确一次投递保证，而无需用户处理任何特定的错误处理例程。这不是很神奇吗？那么这是如何实现的呢？
- en: Full crash fault tolerance and exactly-once delivery guarantee are terms of
    systems theory. Full crash fault tolerance means that you can basically pull the
    power plug of the whole data center at any point in time, and no data is lost
    or left in an inconsistent state. Exactly-once delivery guarantee means, even
    if the same power plug is pulled, it is guaranteed that each tuple- end-to-end
    from the data source to the data sink - is delivered - only, and exactly, once.
    Not zero times and also not more than one time. Of course, those concepts must
    also hold in case a single node fails or misbehaves (for example- starts throttling).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 完全崩溃容错和精确一次投递保证是系统理论中的术语。完全崩溃容错意味着你可以在任何时间点拔掉整个数据中心的电源，而不会丢失任何数据或留下不一致的状态。精确一次投递保证意味着，即使拔掉同一个电源插头，也能确保每个元组——从数据源到数据汇——仅且仅一次被投递。既不是零次，也不会超过一次。当然，这些概念也必须在一个节点失败或行为异常（例如开始限流）的情况下成立。
- en: First of all, states between individual batches and offset ranges (position
    in a source stream) are kept in-memory but are backed by a **Write Ahead Log**
    (**WAL**) in a fault-tolerant filesystem such as HDFS. A WAL is basically a log
    file reflecting the overall stream processing state in a pro-active fashion. This
    means before data is transformed through an operator, it is first persistently
    stored in the WAL in a way it can be recovered after a crash. So, in other words,
    during the processing of an individual mini batch, the regions of the worker memory,
    as well as the position offset of the streaming source, are persisted to disk.
    In case the system fails and has to recover, it can re-request chunks of data
    from the source. Of course, this is only possible if the source supports this
    semantics.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，各个批次和偏移量范围（源流中的位置）之间的状态保持在内存中，但由**预写日志**（**WAL**）在如 HDFS 这样的容错文件系统中支持。WAL
    基本上是一个日志文件，以主动的方式反映整个流处理状态。这意味着在数据通过操作符转换之前，它首先以一种可以在崩溃后恢复的方式持久存储在 WAL 中。因此，换句话说，在处理单个迷你批次期间，工作者内存的区域以及流源的偏移位置都被持久化到磁盘。如果系统失败并需要恢复，它可以重新请求源中的数据块。当然，这只在源支持这种语义的情况下才可能。
- en: Replayable sources can replay streams from a given offset
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可重放源可以从给定的偏移量重放流
- en: End-to-end exactly-once delivery guarantee requires the streaming source to
    support some sort of stream replay at a requested position. This is true for file
    sources and Apache Kafka, for example, as well as the IBM Watson Internet of Things
    platform, where the following example in this chapter will be based on.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端的一次性交付保证要求流源支持在请求位置进行某种流重放。这对于文件源和 Apache Kafka 等是正确的，例如，以及本章中示例将基于的 IBM
    Watson 物联网平台。
- en: Idempotent sinks prevent data duplication
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幂等接收器防止数据重复
- en: Another key to end-to-end exactly-once delivery guarantee is idempotent sinks.
    This basically means that sinks are aware of which particular write operation
    has succeeded in the past. This means that such a smart sink can re-request data
    in case of a failure and also drop data in case the same data has been sent multiple
    times.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端一次性交付保证的另一个关键是幂等接收器。这基本上意味着接收器知道过去哪些特定的写操作已经成功。这意味着这样的智能接收器可以在失败时重新请求数据，并在相同数据被发送多次时丢弃数据。
- en: State versioning guarantees consistent results after reruns
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态版本化确保重跑后结果一致
- en: What about the state? Imagine that a machine learning algorithm maintains a
    count variable on all the workers. If you replay the exact same data twice, you
    will end up counting the data multiple times. Therefore, the query planner also
    maintains a versioned key-value map within the workers, which are persisting their
    state in turn to HDFS--which is by design fault tolerant.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 那么状态呢？设想一个机器学习算法在所有工作者上维护一个计数变量。如果你将完全相同的数据重放两次，你最终会多次计数这些数据。因此，查询计划器也在工作者内部维护一个版本化的键值映射，这些工作者依次将其状态持久化到
    HDFS——这是设计上的容错机制。
- en: So, in case of a failure, if data has to be replaced, the planner makes sure
    that the correct version of the key-value map is used by the workers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在发生故障时，如果数据需要被替换，计划器确保工作者使用正确的键值映射版本。
- en: Example - connection to a MQTT message broker
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 - 连接到 MQTT 消息代理
- en: So, let's start with a sample use case. Let's connect to an **Internet of Things**
    (**IoT**) sensor data stream. As we haven't covered machine learning so far, we
    don't analyze the data, we just showcase the concept.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们从一个示例用例开始。让我们连接到一个**物联网**（**IoT**）传感器数据流。由于我们到目前为止还没有涉及机器学习，我们不分析数据，我们只是展示概念。
- en: We are using the IBM Watson IoT platform as a streaming source. At its core,
    the Watson IoT platform is backed by an **MQTT** (**Message Queue Telemetry Transport**)
    message broker. MQTT is a lightweight telemetry protocol invented by IBM in 1999
    and became-- an **OASIS** (**Organization for the Advancement of Structured Information
    Standards**, a global nonprofit consortium that works on the development, convergence,
    and adoption of standards for security, Internet of Things, energy, content technologies,
    emergency management, and other areas) standard in 2013--the de facto standard
    for IoT data integration.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用IBM Watson物联网平台作为流数据源。在其核心，Watson物联网平台由**MQTT**（**消息队列遥测传输**）消息代理支持。MQTT是IBM于1999年发明的一种轻量级遥测协议，并于2013年成为**OASIS**（**结构化信息标准促进组织**，一个全球非营利性联盟，致力于安全、物联网、能源、内容技术、应急管理等领域的标准开发、融合和采纳）的标准——物联网数据集成的实际标准。
- en: Messaging between applications can be backed by a message queue which is a middleware
    system supporting asynchronous point to point channels in various delivery modes
    like **first-in-first-out** (**FIFO**), **last-in-first-out** (**LIFO**) or **Priority
    Queue** (where each message can be re-ordered by certain criteria).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序间的消息传递可以由消息队列支持，这是一种支持各种交付模式的异步点对点通道的中间件系统，如**先进先出**（**FIFO**）、**后进先出**（**LIFO**）或**优先级队列**（其中每条消息可以根据特定标准重新排序）。
- en: This is already a very nice feature, but still, couples applications in a certain
    way because, once a message is read, it is made unavailable to others.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经是一个非常棒的功能，但仍然以某种方式耦合了应用程序，因为一旦消息被读取，它就对其他应用程序不可用了。
- en: This way N to N communication is hard (but not impossible) to achieve. In a
    publish/subscribe model applications are completely de-coupled. There doesn't
    exist any queues anymore but the notion of topics is introduced. Data providers
    publish messages on specific topics and data consumers subscribe to those topics.
    This way N to N communication is very straightforward to achieve since it is reflected
    by the underlying message delivery model. Such a middleware is called a Message
    Broker in contrast to a Message Queue.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种N对N通信实现起来较为困难（但并非不可能）。在发布/订阅模型中，应用程序完全解耦。不再存在任何队列，而是引入了主题的概念。数据提供者在特定主题上发布消息，而数据消费者则订阅这些主题。这样一来，N对N通信的实现就变得非常直接，因为它反映了底层的消息传递模型。这种中间件被称为消息代理，与消息队列相对。
- en: 'As cloud services tend to change constantly, and cloud, in general, is introduced
    later in this book, the following tutorial explains how to set up the test data
    generator in the cloud and connect to the remote MQTT message broker. In this
    example, we will use the IBM Watson IoT Platform, which is an MQTT message broker
    available in the cloud. Alternatively one can install an open source message broker
    like MOSQUITTO which also provides a publicly available test installation on the
    following URL: [http://test.mosquitto.org/](http://test.mosquitto.org/).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于云服务不断变化，且本书稍后才会介绍云，以下教程解释了如何在云中设置测试数据生成器并连接到远程MQTT消息代理。在本例中，我们将使用IBM Watson
    IoT平台，这是一个在云中可用的MQTT消息代理。或者，也可以安装开源消息代理如MOSQUITTO，它还提供了一个公开可用的测试安装，网址如下：[http://test.mosquitto.org/](http://test.mosquitto.org/)。
- en: 'In order to replicate the example, the following steps (1) and (2) are necessary
    as described in the following tutorial: [https://www.ibm.com/developerworks/library/iot-cognitive-iot-app-machine-learning/index.html](https://www.ibm.com/developerworks/library/iot-cognitive-iot-app-machine-learning/index.html).
    Please make sure to note down `http_host`, `org` , `apiKey`, and `apiToken` during
    execution of the tutorial. Those are needed later in order to subscribe to data
    using Apache Spark Structured Streaming.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了复现示例，以下步骤（1）和（2）是必要的，如以下教程所述：[https://www.ibm.com/developerworks/library/iot-cognitive-iot-app-machine-learning/index.html](https://www.ibm.com/developerworks/library/iot-cognitive-iot-app-machine-learning/index.html)。请确保在执行教程时记下`http_host`、`org`、`apiKey`和`apiToken`。这些信息稍后用于通过Apache
    Spark结构化流订阅数据。
- en: As the IBM Watson IoT platform uses the open MQTT standard, no special IBM component
    is necessary to connect to the platform. Instead, we are using MQTT and Apache
    Bahir as a connector between MQTT and Apache Spark structured streaming.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于IBM Watson物联网平台采用开放的MQTT标准，因此无需特殊的IBM组件即可连接到该平台。相反，我们使用MQTT和Apache Bahir作为MQTT与Apache
    Spark结构化流之间的连接器。
- en: The goal of the Apache Bahir project is to provide a set of source and sink
    connectors for various data processing engines including Apache Spark and Apache
    Flink since they are lacking those connectors. In this case, we will use the Apache
    Bahir MQTT data source for MQTT.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Bahir项目的目标是为包括Apache Spark和Apache Flink在内的各种数据处理引擎提供一组源和汇连接器，因为它们缺乏这些连接器。在这种情况下，我们将使用Apache
    Bahir MQTT数据源进行MQTT通信。
- en: 'In order to use Apache Bahir, we need to add two dependencies to our local
    maven repository. A complete `pom.xml` file is provided in the download section
    of this chapter. Let''s have a look at the dependency section of `pom.xml`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用Apache Bahir，我们需要向本地maven仓库添加两个依赖项。本章下载部分提供了一个完整的`pom.xml`文件。让我们看一下`pom.xml`的依赖部分：
- en: '![](img/3408ec51-5ead-4efd-902c-37efc6735a9f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3408ec51-5ead-4efd-902c-37efc6735a9f.png)'
- en: We are basically getting the MQTT Apache structured streaming adapter of Apache
    Bahir and a dependent package for low-level MQTT processing. A simple `mvn dependency:resolve`
    command in the directory of the `pom.xml` file pulls the required dependencies
    into our local maven repository, where they can be accessed by the Apache Spark
    driver and transferred to the Apache Spark workers automatically.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上是在获取Apache Bahir的MQTT Apache结构化流适配器以及一个用于低级MQTT处理的依赖包。在`pom.xml`文件所在的目录中执行简单的`mvn
    dependency:resolve`命令，会将所需的依赖项拉取到我们的本地maven仓库，在那里它们可以被Apache Spark驱动程序访问并自动传输到Apache
    Spark工作节点。
- en: 'Another way of resolving the dependencies is when using the following command
    in order to start a spark-shell (spark-submit works the same way); the necessary
    dependencies are automatically distributed to the workers:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决依赖关系的方法是在启动spark-shell（spark-submit同样适用）时使用以下命令；必要的依赖项会自动分发给工作节点：
- en: '![](img/929fb112-1ce2-45d8-a721-ebd6b7db7a15.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/929fb112-1ce2-45d8-a721-ebd6b7db7a15.png)'
- en: 'Now we need the MQTT credentials that we''ve obtained earlier. Let''s set the
    values here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要之前获取的MQTT凭证。让我们在这里设置值：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can start creating a stream connecting to an MQTT message broker. We
    are telling Apache Spark to use the Apache Bahir MQTT streaming source:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始创建一个连接到MQTT消息代理的流。我们告诉Apache Spark使用Apache Bahir MQTT流源：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We need to specify credentials such as `username`, `password`, and `clientId` in
    order to pull data from the MQTT message broker; the link to the tutorial mentioned
    earlier explains how to obtain these:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从MQTT消息代理拉取数据，我们需要指定凭证，如`username`、`password`和`clientId`；前面提到的教程链接解释了如何获取这些凭证：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As we are using a publish/subscribe messaging model, we have to provide the
    topic that we are subscribing to--this topic is used by the test data generator
    that you''ve deployed to the cloud before:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是发布/订阅消息模型，我们必须提供我们正在订阅的主题——这个主题由您之前部署到云端的测试数据生成器使用：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once everything is set on the configuration side, we have to provide the endpoint
    host and port in order to create the stream:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置方面一切就绪，我们就必须提供端点主机和端口以创建流：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Interestingly, as can be seen in the following screenshot, this leads to the
    creation of a DataFrame:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，正如以下截图所示，这导致了DataFrame的创建：
- en: '![](img/7303999c-963f-42ee-935a-149985204527.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7303999c-963f-42ee-935a-149985204527.png)'
- en: Note that the schema is fixed to `[String, Timestamp]` and cannot be changed
    during stream creation--this is a limitation of the Apache Bahir library. However,
    using the rich DataFrame API, you can parse the value, a JSON string for example,
    and create new columns.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模式固定为`[String, Timestamp]`，并且在流创建过程中无法更改——这是Apache Bahir库的一个限制。然而，使用丰富的DataFrame
    API，您可以解析值（例如，JSON字符串）并创建新列。
- en: 'As discussed before, this is one of the powerful features of Apache Spark structured
    streaming, as the very same DataFrame (and Dataset) API now can be used to process
    historic and real-time data. So let''s take a look at the contents of this stream
    by writing it to the console:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这是Apache Spark结构化流的一个强大功能，因为相同的DataFrame（和Dataset）API现在可以用于处理历史和实时数据。因此，让我们通过将其写入控制台来查看此流的
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As output mode, we choose `append` to enforce incremental display and avoid
    having the complete contents of the historic stream being written to the console
    again and again. As `format`, we specify `console` as we just want to debug what''s
    happening on the stream:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输出模式，我们选择`append`以强制增量显示，并避免历史流的内容被反复写入控制台。作为`格式`，我们指定`console`，因为我们只想调试流上发生的情况：
- en: '![](img/7a3c0353-00f4-43e3-92fa-1d3b9e656b7e.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a3c0353-00f4-43e3-92fa-1d3b9e656b7e.png)'
- en: 'Finally, the `start` method initiates query processing, as can be seen here:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`start` 方法启动查询处理，如这里所示：
- en: '![](img/af7770d2-b680-4aaf-b6c3-e7c683163054.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af7770d2-b680-4aaf-b6c3-e7c683163054.png)'
- en: Controlling continuous applications
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制连续应用程序
- en: 'Once a continuous application (even a simple one, not taking historic data
    into account) is started and running, it has to be controlled somehow as the call
    to the `start` method immediately starts processing, but also returns without
    blocking. In case you want your program to block at this stage until the application
    has finished, one can use the `awaitTermination` method as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦连续应用程序（即使是简单的，不考虑历史数据）启动并运行，它就必须以某种方式进行控制，因为调用 `start` 方法立即开始处理，但也不会阻塞返回。如果您希望程序在此阶段阻塞，直到应用程序完成，可以使用
    `awaitTermination` 方法，如下所示：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is particularly important when precompiling code and using the `spark-submit`
    command. When using `spark-shell`, the application is not terminated anyway.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这在预编译代码并使用 `spark-submit` 命令时尤为重要。当使用 `spark-shell` 时，应用程序无论如何都不会终止。
- en: More on stream life cycle management
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多关于流生命周期管理
- en: 'Streaming tends to be used in the creation of continuous applications. This
    means that the process is running in the background and, in contrast to batch
    processing, doesn''t have a clear stop time; therefore, DataFrames and Datasets
    backed by a streaming source, support various methods for stream life cycle management,
    which are explained as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 流式传输通常用于创建连续应用程序。这意味着该过程在后台运行，与批处理不同，它没有明确的停止时间；因此，由流式源支持的 DataFrames 和 Datasets
    支持各种流生命周期管理方法，如下所述：
- en: '`start`: This starts the continuous application. This method doesn''t block.
    If this is not what you want, use `awaitTermination`.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start`：这启动了连续应用程序。此方法不会阻塞。如果这不是您想要的，请使用 `awaitTermination`。'
- en: '`stop` : This terminates the continuous application.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop`：这终止了连续应用程序。'
- en: '`awaitTermination` : As mentioned earlier, starting a stream using the `start`
    method immediately returns, which means that the call is not blocking. Sometimes
    you want to wait until the stream is terminated, either by someone else calling
    `stop` on it or by an error.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`awaitTermination`：如前所述，使用 `start` 方法启动流立即返回，这意味着调用不会阻塞。有时您希望等待直到流被终止，无论是由其他人调用
    `stop` 还是由于错误。'
- en: '`exception`: In case a stream stopped because of an error, the cause can be
    read using this method.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exception`：如果流因错误而停止，可以使用此方法读取原因。'
- en: '`sourceStatus`: This is to obtain real-time meta information on the streaming
    source.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sourceStatus`：这是为了获取流式源的实时元信息。'
- en: '`sinkStatus` : This is to obtain real-time meta information on the streaming
    sink.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sinkStatus`：这是为了获取流式接收器的实时元信息。'
- en: Sinks in Apache Spark streaming are smart in the sense that they support fault
    tolerance and end-to-end exactly-once delivery guarantee as mentioned before.
    In addition, Apache Spark needs them to support different output methods. Currently,
    the following three output methods, `append`, `update`, and `complete`, significantly
    change the underlying semantics. The following paragraph contains more details
    about the different output methods.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 流式传输中的接收器很智能，因为它们支持故障恢复和端到端的一次性交付保证，如前所述。此外，Apache Spark 需要它们支持不同的输出方法。目前，以下三种输出方法
    `append`、`update` 和 `complete` 显著改变了底层语义。以下段落包含有关不同输出方法的更多详细信息。
- en: 'Different output modes on sinks: Sinks can be specified to handle output in
    different ways. This is known as `outputMode`. The naive choice would use an incremental
    approach as we are processing incremental data with streaming anyway. This mode
    is referred to as `append`. However, there exist requirements where data already
    processed by the sink has to be changed. One example is the late arrival problem
    of missing data in a certain time window, which can lead to changing results once
    the computation for that particular time window is recomputed. This mode is called
    `complete`.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的输出模式在接收器上：接收器可以指定以不同方式处理输出。这称为 `outputMode`。最简单的选择是使用增量方法，因为我们无论如何都在处理增量数据流。此模式称为
    `append`。然而，存在一些需求，其中已经由接收器处理的数据必须更改。一个例子是特定时间窗口中缺失数据的延迟到达问题，一旦为该特定时间窗口重新计算，就可能导致结果改变。此模式称为
    `complete`。
- en: Since Version 2.1 of Apache Spark, the `update` mode was introduced that behaves
    similarly to the `complete` mode but only changes rows that have been altered,
    therefore saving processing resources and improving speed. Some types of modes
    do not support all query types. As this is constantly changing, it is best to
    refer to the latest documentation at [http://spark.apache.org/docs/latest/streaming-programming-guide.html.](http://spark.apache.org/docs/latest/streaming-programming-guide.html)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 自Apache Spark 2.1版本起，引入了`update`模式，其行为类似于`complete`模式，但仅更改已修改的行，从而节省处理资源并提高速度。某些模式不支持所有查询类型。由于这不断变化，最好参考[http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)上的最新文档。
- en: Summary
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'So why do we have two different streaming engines within the same data processing
    framework? We hope that after reading this chapter, you''ll agree that the main
    pain points of the classical DStream based engine have been addressed. Formerly,
    event time-based processing was not possible and only the arrival time of data
    was considered. Then, late data has simply been processed with the wrong timestamp
    as only processing time could be used. Also, batch and stream processing required
    using two different APIs: RDDs and DStreams. Although the API is similar, it is
    not exactly the same; therefore, the rewriting of code when going back and forth
    between the two paradigms was necessary. Finally, an end-to-end delivery guarantee
    was hard to ...'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么在同一个数据处理框架内会有两种不同的流处理引擎呢？我们希望在阅读本章后，您会认同经典DStream引擎的主要痛点已得到解决。以前，基于事件时间的处理是不可能的，只考虑了数据的到达时间。随后，延迟数据仅以错误的时戳进行处理，因为只能使用处理时间。此外，批处理和流处理需要使用两种不同的API：RDD和DStreams。尽管API相似，但并不完全相同；因此，在两种范式之间来回切换时重写代码是必要的。最后，端到端的交付保证难以实现...
