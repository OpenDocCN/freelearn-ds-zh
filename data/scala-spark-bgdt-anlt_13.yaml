- en: My Name is Bayes, Naive Bayes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我的名字是贝叶斯，朴素贝叶斯
- en: '"Prediction is very difficult, especially if it''s about the future"'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “预测是非常困难的，尤其是关于未来的预测”
- en: -Niels Bohr
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: -尼尔斯·玻尔
- en: '**Machine learning (ML)** in combination with big data is a radical combination
    that has created some great impacts in the field of research in Academia and Industry.
    Moreover, many research areas are also entering into big data since datasets are
    being generated and produced in an unprecedented way from diverse sources and
    technologies, commonly referred as the **Data Deluge**. This imposes great challenges
    on ML, data analytics tools, and algorithms to find the real **VALUE** out of
    big data criteria such as volume, velocity, and variety. However, making predictions
    from these huge dataset has never been easy.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习（ML）**与大数据的结合是一种革命性的组合，对学术界和工业界的研究产生了巨大影响。此外，许多研究领域也进入了大数据领域，因为数据集以前所未有的方式从各种来源和技术产生和生成，通常被称为**数据洪流**。这给机器学习、数据分析工具和算法带来了巨大挑战，以从大数据的诸如容量、速度和多样性等标准中找到真正的**价值**。然而，从这些庞大数据集中进行预测从来都不容易。'
- en: 'Considering this challenge, in this chapter we will dig deeper into ML and
    find out how to use a simple yet powerful method to build a scalable classification
    model and even more. In a nutshell, the following topics will be covered throughout
    this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一挑战，在本章中我们将深入探讨机器学习，并了解如何使用一种简单而强大的方法来构建可扩展的分类模型，甚至更多。简而言之，本章将涵盖以下主题：
- en: Multinomial classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式分类
- en: Bayesian inference
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯推断
- en: Naive Bayes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Decision trees
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Naive Bayes versus decision trees
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯与决策树
- en: Multinomial classification
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式分类
- en: In ML, **multinomial** (also known as multiclass) classification is the task
    of classifying data objects or instances into more than two classes, that is,
    having more than two labels or classes. Classifying data objects or instances
    into two classes is called **binary classification**. More technically, in multinomial
    classification, each training instance belongs to one of N different classes subject
    to `N >=2`. The goal is then to construct a model that correctly predicts the
    classes to which the new instances belong. There may be numerous scenarios having
    multiple categories in which the data points belong. However, if a given point
    belongs to multiple categories, this problem decomposes trivially into a set of
    unlinked binary problems, which can be solved naturally using a binary classification
    algorithm.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，多项式（也称为多类）分类是将数据对象或实例分类为两个以上类别的任务，即具有两个以上标签或类别。将数据对象或实例分类为两个类别称为二进制分类。更具体地说，在多项式分类中，每个训练实例属于N个不同类别中的一个，其中`N
    >=2`。目标是构建一个能够正确预测新实例所属类别的模型。可能存在许多情景，其中数据点属于多个类别。然而，如果给定点属于多个类别，这个问题可以轻松地分解为一组不相关的二进制问题，可以使用二进制分类算法自然地解决。
- en: Readers are suggested not be confused distinguishing the multiclass classification
    with multilabel classification, where multiple labels are to be predicted for
    each instance. For more on Spark-based implementation for the multilabel classification,
    interested readers should refer to [https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#multilabel-classification](https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#multilabel-classification).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 建议读者不要混淆多类分类和多标签分类，多标签分类是要为每个实例预测多个标签。对于基于Spark的多标签分类的实现，感兴趣的读者应参考[https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#multilabel-classification](https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#multilabel-classification)。
- en: 'Multiclass classification techniques can be divided into several categories
    as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类技术可以分为以下几类：
- en: Transformation to binary
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换为二进制
- en: Extension from binary
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从二进制扩展
- en: Hierarchical classification
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层分类
- en: Transformation to binary
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换为二进制
- en: Using the transformation to binary technique, a multiclass classification problem
    can be transformed into an equivalent strategy for multiple binary classification
    problems. In other words, this technique can be referred to as a *problem transformation
    techniques*. A detailed discussion from the theoretical and practical perspectives
    is out of the scope of this chapter. Therefore, here we will discuss only one
    example of the problem transformation technique called **One-Vs-The-Rest** (**OVTR**)
    algorithm as the representative of this category.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用转换为二进制的技术，多类分类问题可以转化为多个二进制分类问题的等效策略。换句话说，这种技术可以称为*问题转换技术*。从理论和实践角度进行详细讨论超出了本章的范围。因此，这里我们只讨论问题转换技术的一个例子，即代表这一类别的**一对多**（OVTR）算法。
- en: Classification using One-Vs-The-Rest approach
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用一对多方法进行分类
- en: In this subsection, we will describe an example of performing multiclass classification
    using the OVTR algorithm by converting the problem into equivalent multiple binary
    classification problems. The OVTR strategy breaks down the problem and trains
    each binary classifier per class. In other words, the OVTR classifier strategy
    consists of fitting one binary classifier per class. It then treats all the samples
    of the current class as positive samples, and consequently other samples of other
    classifiers are treated as negatives samples.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将通过将问题转化为等效的多个二进制分类问题，来描述使用OVTR算法进行多类分类的示例。OVTR策略将问题分解，并针对每个类训练每个二进制分类器。换句话说，OVTR分类器策略包括为每个类拟合一个二进制分类器。然后将当前类的所有样本视为正样本，因此其他分类器的样本被视为负样本。
- en: This is a modular machine learning technique no doubt. However, on the downside,
    this strategy requires a base classifier from the multiclass family. The reason
    is that the classifier must produce a real value also called *confidence scores*
    instead of a prediction of the actual labels. The second disadvantage of this
    strategy is that if the dataset (aka training set) contains discrete class labels,
    these eventually lead to vague prediction results. In that case, multiple classes
    can be predicted for a single sample. To make the preceding discussion clearer,
    now let's see an example as follows.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，这是一种模块化的机器学习技术。然而，这种策略的缺点是需要来自多类家族的基本分类器。原因是分类器必须产生一个实值，也称为*置信分数*，而不是实际标签的预测。这种策略的第二个缺点是，如果数据集（也称为训练集）包含离散的类标签，这最终会导致模糊的预测结果。在这种情况下，一个样本可能被预测为多个类。为了使前面的讨论更清晰，现在让我们看一个例子。
- en: 'Suppose that we have a set of 50 observations divided into three classes. Thus,
    we will use the same logic as before for selecting the negative examples too.
    For the training phase, let''s have the following setting:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组50个观察结果，分为三类。因此，我们将使用与之前相同的逻辑来选择负例。对于训练阶段，让我们有以下设置：
- en: '**Classifier 1** has 30 positive examples and 20 negative examples'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器1**有30个正例和20个负例'
- en: '**Classifier 2** has 36 positive examples and 14 negative examples'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器2**有36个正例和14个负例'
- en: '**Classifier 3** has 14 positive examples and 24 negative examples'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器3**有14个正例和24个负例'
- en: On the other hand, for the testing phase, suppose I have a new instance that
    need to be classified into one of the previous classes. Each of the three classifiers,
    of course, produces a probability with respect to the estimation This is an estimation
    of how low an instance belongs to the negative or positive examples in the classifier?
    In this case, we should always compare the probabilities of positive class in
    one versus the rest. Now that for *N* classes, we will have *N* probability estimates
    of the positive class for one test sample. Compare them, and whichever probability
    is the maximum of *N* probabilities belongs to that particular class. Spark provides
    multiclass to binary reduction with the OVTR algorithm, where the **Logistic Regression**
    algorithm is used as the base classifier.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在测试阶段，假设我有一个新实例需要分类到之前的某个类别中。当然，每个分类器都会产生一个关于估计的概率。这是一个实例属于分类器中的负面或正面示例的估计？在这种情况下，我们应该总是比较一个类中的正面概率与其他类。现在对于*N*个类，我们将有*N*个正面类的概率估计值。比较它们，无论哪个概率是*N*个概率中的最大值，都属于那个特定的类。Spark提供了OVTR算法的多类到二进制的缩减，其中**逻辑回归**算法被用作基本分类器。
- en: Now let's see another example of a real dataset to demonstrate how Spark classifies
    all the features using OVTR algorithm. The OVTR classifier eventually predicts
    handwritten characters from the **Optical Character Reader** (**OCR**) dataset.
    However, before diving into the demonstration, let's explore the OCR dataset first
    to get the exploratory nature of the data. It is to be noted that when OCR software
    first processes a document, it divides the paper or any object into a matrix such
    that each cell in the grid contains a single glyph (also known different graphical
    shapes), which is just an elaborate way of referring to a letter, symbol, or number
    or any contextual information from the paper or the object.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看另一个真实数据集的例子，以演示Spark如何使用OVTR算法对所有特征进行分类。OVTR分类器最终预测来自**光学字符识别**（OCR）数据集的手写字符。然而，在深入演示之前，让我们先探索OCR数据集，以了解数据的探索性质。需要注意的是，当OCR软件首次处理文档时，它将纸张或任何对象分成一个矩阵，以便网格中的每个单元格包含一个单一的字形（也称为不同的图形形状），这只是一种指代字母、符号、数字或来自纸张或对象的任何上下文信息的复杂方式。
- en: To demonstrate the OCR pipeline, let's assume that the document contains only
    alpha characters in English that match glyphs to one of the 26 capital letters,
    that is, *A* to *Z*. We will use the OCR letter dataset from the *UCI Machine
    Learning Data Repository*. The dataset was denoted by W*. Frey* and *D. J. Slate.*
    While exploring the dataset, you should observe 20,000 examples of 26 English
    capital letters. Letter written in capital letters are available as printed using
    20 different, randomly reshaped and distorted black and white fonts as glyphs
    of different shapes. In short, predicting all the characters from 26 alphabets
    turns the problem itself into a multiclass classification problem with 26 classes.
    Consequently, a binary classifier will not be able to serve our purpose.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示OCR管道，假设文档只包含与26个大写字母中的一个匹配的英文alpha字符，即*A*到*Z*。我们将使用来自*UCI机器学习数据存储库*的OCR字母数据集。该数据集由W*.
    Frey*和*D. J. Slate.*标记。在探索数据集时，您应该观察到20,000个例子，其中包含26个英文大写字母。大写字母以20种不同的、随机重塑和扭曲的黑白字体作为不同形状的字形打印。简而言之，从26个字母中预测所有字符将问题本身转变为一个具有26个类的多类分类问题。因此，二元分类器将无法满足我们的目的。
- en: '![](img/00362.gif)**Figure 1**: Some of the printed glyphs (Source: Letter
    recognition using Holland-style adaptive classifiers, ML, V. 6, p. 161-182, by
    W. Frey and D.J. Slate [1991])'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00362.gif)**图1：** 一些印刷字形（来源：使用Holland风格自适应分类器进行字母识别，ML，V. 6，p. 161-182，作者W.
    Frey和D.J. Slate [1991])'
- en: 'The preceding figure shows the images that I explained earlier.*The dataset*
    provides an example of some of the printed glyphs distorted in this way; therefore,
    the letters are computationally challenging for a computer to identify. Yet, these
    glyphs are easily recognized by a human being. The following figure shows the
    statistical attributes of the top 20 rows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了我之前解释过的图像。*数据集*提供了一些以这种方式扭曲的印刷字形的示例；因此，这些字母对计算机来说是具有挑战性的。然而，这些字形对人类来说很容易识别。下图显示了前20行的统计属性：
- en: '![](img/00020.jpeg)**Figure 2:** The snapshot of the dataset shown as the data
    frame'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00020.jpeg)**图2：** 数据框架显示的数据集快照'
- en: Exploration and preparation of the OCR dataset
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OCR数据集的探索和准备
- en: According to the dataset description, glyphs are scanned using an OCR reader
    on to the computer then they are automatically converted into pixels. Consequently,
    all the 16 statistical attributes (in **figure 2**) are recorded to the computer
    too. The the concentration of black pixels across various areas of the box provide
    a way to differentiate 26 letters using OCR or a machine learning algorithm to
    be trained.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集描述，字形是使用OCR阅读器扫描到计算机上，然后它们自动转换为像素。因此，所有16个统计属性（在**图2**中）也记录到计算机中。盒子各个区域的黑色像素的浓度提供了一种区分26个字母的方法，使用OCR或机器学习算法进行训练。
- en: Recall that **support vector machines** (**SVM**), Logistic Regression, Naive
    Bayesian-based classifier, or any other classifier algorithms (along with their
    associated learners) require all the features to be numeric. LIBSVM allows you
    to use a sparse training dataset in an unconventional format. While transforming
    the normal training dataset to the LIBSVM format. Only the nonzero values that
    are also included in the dataset are stored in a sparse array/matrix form. The
    index specifies the column of the instance data (feature index). However, any
    missing data is taken as holding zero value too. The index serves as a way to
    distinguish between the features/parameters. For example, for three features,
    indices 1, 2, and 3 would correspond to the *x*, *y*, and *z* coordinates, respectively.
    The correspondence between the same index values of different data instances is
    merely mathematical when constructing the hyperplane; these serve as coordinates.
    If you skip any index in between, it should be assigned a default value of zero.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，**支持向量机**（**SVM**），逻辑回归，朴素贝叶斯分类器，或者任何其他分类器算法（以及它们关联的学习器）都要求所有特征都是数字。LIBSVM允许您使用非常规格式的稀疏训练数据集。在将正常训练数据集转换为LIBSVM格式时，只有数据集中包含的非零值存储在稀疏数组/矩阵形式中。索引指定实例数据的列（特征索引）。但是，任何缺失的数据也被视为零值。索引用作区分特征/参数的一种方式。例如，对于三个特征，索引1、2和3分别对应于*x*、*y*和*z*坐标。不同数据实例的相同索引值之间的对应仅在构建超平面时是数学的；这些用作坐标。如果您在中间跳过任何索引，它应该被分配一个默认值为零。
- en: 'In most practical cases, we might need to normalize the data against all the
    features points. In short, we need to convert the current tab-separated OCR data
    into LIBSVM format to make the training step easier. Thus, I''m assuming you have
    downloaded the data and converted into LIBSVM format using their own script. The
    resulting dataset that is transformed into LIBSVM format consisting of labels
    and features is shown in the following figure:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实际情况下，我们可能需要对所有特征点进行数据归一化。简而言之，我们需要将当前的制表符分隔的OCR数据转换为LIBSVM格式，以使训练步骤更容易。因此，我假设您已经下载了数据并使用它们自己的脚本转换为LIBSVM格式。转换为LIBSVM格式的结果数据集包括标签和特征，如下图所示：
- en: '![](img/00223.gif)**Figure 3:** A snapshot of 20 rows of the OCR dataset in
    LIBSVM format'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00223.gif)**图3：**LIBSVM格式的OCR数据集的20行快照'
- en: 'Interested readers can refer to the following research article for gaining
    in-depth knowledge: *Chih-Chung Chang* and *Chih-Jen Lin*, *LIBSVM: a library
    for support vector machines*, *ACM Transactions on Intelligent Systems and Technology*,
    2:27:1--27:27, 2011\. You can also refer to a public script provided on my GitHub
    repository at [https://github.com/rezacsedu/RandomForestSpark/](https://github.com/rezacsedu/RandomForestSpark/)
    that directly converts the OCR data in CSV into LIBSVM format. I read the data
    about all the letters and assigned a unique numeric value to each. All you need
    is to show the input and output file path and run the script.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的读者可以参考以下研究文章以获得深入的知识：*Chih-Chung Chang*和*Chih-Jen Lin*，*LIBSVM：支持向量机库*，*ACM智能系统与技术交易*，2:27:1--27:27，2011年。您还可以参考我在GitHub存储库上提供的公共脚本，该脚本直接将CSV中的OCR数据转换为LIBSVM格式。我读取了所有字母的数据，并为每个字母分配了唯一的数值。您只需要显示输入和输出文件路径并运行脚本。
- en: Now let's dive into the example. The example that I will be demonstrating has
    11 steps including data parsing, Spark session creation, model building, and model
    evaluation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一个例子。我将演示的例子包括11个步骤，包括数据解析、Spark会话创建、模型构建和模型评估。
- en: '**Step 1\. Creating Spark session** - Create a Spark session by specifying
    master URL, Spark SQL warehouse, and application name as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1. 创建Spark会话** - 通过指定主URL、Spark SQL仓库和应用程序名称来创建Spark会话，如下所示：'
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2\. Loading, parsing, and creating the data frame** - Load the data
    file from the HDFS or local disk and create a data frame, and finally show the
    data frame structure as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2. 加载、解析和创建数据框** - 从HDFS或本地磁盘加载数据文件，并创建数据框，最后显示数据框结构如下：'
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3\. Generating training and test set to train the model** - Let''s generate
    the training and test set by splitting 70% for training and 30% for the test:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3. 生成训练和测试集以训练模型** - 让我们通过将70%用于训练和30%用于测试来生成训练和测试集：'
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Step 4\. Instantiate the base classifier** - Here the base classifier acts
    as the multiclass classifier. For this case, it is the Logistic Regression algorithm
    that can be instantiated by specifying parameters such as the number of max iterations,
    tolerance, regression parameter, and Elastic Net parameters.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4. 实例化基本分类器** - 这里基本分类器充当多类分类器。在这种情况下，可以通过指定最大迭代次数、容差、回归参数和弹性网参数来实例化逻辑回归算法。'
- en: Note that Logistic Regression is an appropriate regression analysis to conduct
    when the dependent variable is dichotomous (binary). Like all regression analyses,
    Logistic Regression is a predictive analysis. Logistic regression is used to describe
    data and to explain the relationship between one dependent binary variable and
    one or more nominal, ordinal, interval, or ratio level independent variables.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当因变量是二元的时，逻辑回归是适当的回归分析。与所有回归分析一样，逻辑回归是一种预测性分析。逻辑回归用于描述数据并解释一个因变量二进制变量和一个或多个名义，有序，间隔或比率水平自变量之间的关系。
- en: For a a Spark-based implementation of the Logistic Regression algorithm, interested
    readers can refer to [https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于Spark的逻辑回归算法的实现，感兴趣的读者可以参考[https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression)。
- en: 'In brief, the following parameters are used to training a Logistic Regression
    classifier:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，以下参数用于训练逻辑回归分类器：
- en: '`MaxIter`: This specifies the number of maximum iterations. In general, more
    is better.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxIter`：这指定了最大迭代次数。一般来说，越多越好。'
- en: '`Tol`: This is the tolerance for the stopping criteria. In general, less is
    better, which helps the model to be trained more intensively. The default value
    is 1E-4.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tol`：这是停止标准的公差。一般来说，越少越好，这有助于更加强烈地训练模型。默认值为1E-4。'
- en: '`FirIntercept`: This signifies if you want to intercept the decision function
    while generating the probabilistic interpretation.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FirIntercept`：这表示是否在生成概率解释时拦截决策函数。'
- en: '`Standardization`: This signifies a Boolean value depending upon if would like
    to standardize the training or not.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Standardization`：这表示一个布尔值，取决于是否要对训练进行标准化。'
- en: '`AggregationDepth`: More is better.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AggregationDepth`：越多越好。'
- en: '`RegParam`: This signifies the regression params. Less is better for most cases.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RegParam`：这表示回归参数。在大多数情况下，越少越好。'
- en: '`ElasticNetParam`: This signifies more advanced regression params. Less is
    better for most cases.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ElasticNetParam`：这表示更先进的回归参数。在大多数情况下，越少越好。'
- en: 'Nevertheless, you can specify the fitting intercept as a `Boolean` value as
    true or false depending upon your problem type and dataset properties:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您可以指定拟合拦截作为`Boolean`值，取决于您的问题类型和数据集属性：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 5\. Instantiate the OVTR classifier** - Now instantiate an OVTR classifier
    to convert the multiclass classification problem into multiple binary classifications
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步。 实例化OVTR分类器 - 现在实例化一个OVTR分类器，将多类分类问题转换为多个二进制分类问题如下：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here `classifier` is the Logistic Regression estimator. Now it's time to train
    the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`classifier`是逻辑回归估计器。现在是训练模型的时候了。
- en: '**Step 6\. Train the multiclass model** - Let''s train the model using the
    training set as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步。 训练多类模型 - 让我们使用训练集来训练模型如下：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Step 7\. Score the model on the test set** - We can score the model on test
    data using the transformer (that is, `ovrModel`) as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第7步。 在测试集上对模型进行评分 - 我们可以使用转换器（即`ovrModel`）对测试数据进行评分如下：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Step 8\. Evaluate the model** - In this step, we will predict the labels
    for the characters in the first column. But before that we need instantiate an
    `evaluator` to compute the classification performance metrics such as accuracy,
    precision, recall, and `f1` measure as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第8步。 评估模型 - 在这一步中，我们将预测第一列中字符的标签。但在此之前，我们需要实例化一个`evaluator`来计算分类性能指标，如准确性，精确度，召回率和`f1`度量如下：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Step 9\. Compute performance metrics** - Compute the classification accuracy,
    precision, recall, `f1` measure, and error on test data as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步。 计算性能指标 - 计算测试数据的分类准确性，精确度，召回率，`f1`度量和错误如下：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Step 10.** Print the performance metrics:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第10步。 打印性能指标：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should observe the value as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该观察到以下值：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Step 11.** Stop the Spark session:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 第11步。 停止Spark会话：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This way, we can convert a multinomial classification problem into multiple
    binary classifications problem without sacrificing the problem types. However,
    from step 10, we can observe that the classification accuracy is not good at all.
    It might be because of several reasons such as the nature of the dataset we used
    to train the model. Also even more importantly, we did not tune the hyperparameters
    while training the Logistic Regression model. Moreover, while performing the transformation,
    the OVTR had to sacrifice some accuracy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以将多项分类问题转换为多个二进制分类问题，而不会牺牲问题类型。然而，从第10步可以观察到分类准确性并不好。这可能是由于多种原因，例如我们用来训练模型的数据集的性质。而且更重要的是，在训练逻辑回归模型时，我们没有调整超参数。此外，在执行转换时，OVTR不得不牺牲一些准确性。
- en: Hierarchical classification
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层分类
- en: In a hierarchical classification task, the classification problem can be resolved
    by dividing the output space into a tree. In that tree, parent nodes are divided
    into multiple child nodes. The process persists until each child node depicts
    a single class. Several methods have been proposed based on the hierarchical classification
    technique. Computer vision is an example of such areas where recognizing pictures
    or written text are something that use hierarchical processing does. An extensive
    discussion on this classifier is out of the scope of this chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在分层分类任务中，分类问题可以通过将输出空间划分为树来解决。在该树中，父节点被划分为多个子节点。该过程持续进行，直到每个子节点表示一个单一类别。基于分层分类技术提出了几种方法。计算机视觉是这样的领域的一个例子，其中识别图片或书面文本是使用分层处理的内容。本章对这个分类器的广泛讨论超出了范围。
- en: Extension from binary
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从二进制扩展
- en: This is a technique for extending existing binary classifiers to solve multiclass
    classification problems. To address multiclass classification problems, several
    algorithms have been proposed and developed based on neural networks, DTs, Random
    forest, k-nearest neighbors, Naive Bayes, and SVM. In the following sections,
    we will discuss the Naive Bayes and the DT algorithm as two representatives of
    this category.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种将现有的二元分类器扩展为解决多类分类问题的技术。为了解决多类分类问题，基于神经网络、决策树、随机森林、k-最近邻、朴素贝叶斯和支持向量机等算法已经被提出和发展。在接下来的部分中，我们将讨论朴素贝叶斯和决策树算法作为这一类别的代表。
- en: Now, before starting to solve multiclass classification problems using Naive
    Bayes algorithms, let's have a brief overview of Bayesian inference in the next
    section.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在开始使用朴素贝叶斯算法解决多类分类问题之前，让我们在下一节简要概述贝叶斯推断。
- en: Bayesian inference
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯推断
- en: In this section, we will briefly discuss **Bayesian inference** (**BI**) and
    its underlying theory. Readers will be familiar with this concept from the theoretical
    and computational viewpoints.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要讨论**贝叶斯推断**（**BI**）及其基本理论。读者将从理论和计算的角度熟悉这个概念。
- en: An overview of Bayesian inference
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯推断概述
- en: Bayesian inference is a statistical method based on Bayes theorem. It is used
    to update the probability of a hypothesis (as a strong statistical proof) so that
    statistical models can repeatedly update towards more accurate learning. In other
    words, all types of uncertainty are revealed in terms of statistical probability
    in the Bayesian inference approach. This is an important technique in theoretical
    as well as mathematical statistics. We will discuss the Bayes theorem broadly
    in a later section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯推断是一种基于贝叶斯定理的统计方法。它用于更新假设的概率（作为强有力的统计证据），以便统计模型可以反复更新以实现更准确的学习。换句话说，在贝叶斯推断方法中，所有类型的不确定性都以统计概率的形式显现出来。这是理论统计学和数学统计学中的重要技术。我们将在后面的部分广泛讨论贝叶斯定理。
- en: Furthermore, Bayesian updating is predominantly foremost in the incremental
    learning and dynamic analysis of the sequence of the dataset. For example time
    series analysis, genome sequencing in biomedical data analytics, science, engineering,
    philosophy, and law are some example where Bayesian inference is used widely.
    From the philosophical perspective and decision theory, Bayesian inference is
    strongly correlated to predictive probability. This theory, however, is more formally
    known as the **Bayesian probability**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，贝叶斯更新在数据集序列的增量学习和动态分析中占据主导地位。例如，在时间序列分析、生物医学数据分析中的基因组测序、科学、工程、哲学和法律等领域，广泛使用贝叶斯推断。从哲学和决策理论的角度来看，贝叶斯推断与预测概率密切相关。然而，这个理论更正式地被称为**贝叶斯概率**。
- en: What is inference?
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是推断？
- en: Inference or model evaluation is the process of updating probabilities of the
    denouement derived from the model at the end. As a result, all the probabilistic
    evidence is eventually known against the observation at hand so that observations
    can be updated while using the Bayesian model for classification analysis. Later
    on, this information is fetched to the Bayesian model by instantiating the consistency
    against all the observations in the dataset. The rules that are fetched to the
    model are referred to as prior probabilities where a probability is assessed before
    making reference to certain relevant observations, especially subjectively or
    on the assumption that all possible outcomes be given the same probability. Then
    beliefs are computed when all the evidence is known as posterior probabilities.
    These posterior probabilities reflect the levels of hypothesis computed based
    on updated evidence.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 推断或模型评估是更新模型得出的结果的概率的过程。因此，所有的概率证据最终都会根据手头的观察结果得知，以便在使用贝叶斯模型进行分类分析时更新观察结果。随后，这些信息通过将一致性实例化到数据集中的所有观察结果中，被提取到贝叶斯模型中。被提取到模型中的规则被称为先验概率，其中在参考某些相关观察结果之前评估概率，特别是主观地或者假设所有可能的结果具有相同的概率。然后，当所有证据都已知时，信念就会被计算为后验概率。这些后验概率反映了基于更新的证据计算出的假设水平。
- en: The Bayes theorem is used to compute the posterior probabilities that signify
    a consequence of two antecedents. Based on these antecedents, a prior probability
    and a likelihood function are derived from a statistical model for the new data
    for model adaptability. We will further discuss the Bayes theorem in a later section.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理用于计算表示两个前提的结果的后验概率。基于这些前提，从统计模型中推导出先验概率和似然函数，用于新数据的模型适应性。我们将在后面的部分进一步讨论贝叶斯定理。
- en: How does it work?
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: Here we discuss a general setup for a statistical inference problem. At the
    first place, from the data, we estimate the desired quantity and there might be
    unknown quantities too that we would like to estimate. It could be simply a response
    variable or predicted variable, a class, a label, or simply a number. If you are
    familiar with the *frequentist* approach, you might know that in this approach
    the unknown quantity say `θ` is assumed to be a fixed (nonrandom) quantity that
    is to be estimated by the observed data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了统计推断问题的一般设置。首先，从数据中估计所需的数量，可能还有一些未知的数量，我们也想要估计。它可能只是一个响应变量或预测变量，一个类别，一个标签，或者只是一个数字。如果您熟悉*频率主义*方法，您可能知道在这种方法中，假设未知的数量θ被假定为一个固定的（非随机的）数量，它将由观察到的数据来估计。
- en: However, in the Bayesian framework, an unknown quantity say `θ` is treated as
    a random variable. More specifically, it is assumed that we have an initial guess
    about the distribution of `θ`, which is commonly referred to as the **prior distribution**.
    Now, after observing some data, the distribution of `θ` is updated. This step
    is usually performed using Bayes' rule (for more details, refer to the next section).
    This is why this approach is called the Bayesian approach. However, in short,
    from the prior distribution, we can compute predictive distributions for future
    observations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在贝叶斯框架中，一个未知的量θ被视为一个随机变量。更具体地说，假设我们对θ的分布有一个初始猜测，通常称为**先验分布**。现在，在观察到一些数据后，θ的分布被更新。通常使用贝叶斯定理来执行这一步骤（有关更多细节，请参阅下一节）。这就是为什么这种方法被称为贝叶斯方法。然而，简而言之，从先验分布中，我们可以计算未来观察的预测分布。
- en: This unpretentious process can be justified as the appropriate methodology to
    uncertain inference with the help of numerous arguments. However, the consistency
    is maintained with the clear principles of the rationality of these arguments.
    In spite of this strong mathematical evidence, many machine learning practitioners
    are uncomfortable with, and a bit reluctant of, using the Bayesian approach. The
    reason behind this is that often they view the selection of a posterior probability
    or prior as being arbitrary and subjective; however, in reality, this is subjective
    but not arbitrary.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不矫揉造作的过程可以通过许多论据来证明是不确定推理的适当方法。然而，这些论据的合理性原则是保持一致的。尽管有这些强有力的数学证据，许多机器学习从业者对使用贝叶斯方法感到不舒服，有些不情愿。其背后的原因是他们经常认为选择后验概率或先验是任意和主观的；然而，实际上这是主观的但不是任意的。
- en: Inappropriately, many Bayesians don't really think in true Bayesian terms. One
    can, therefore, find many pseudo-Bayesian procedures in the literature, in which
    models and priors are used that cannot be taken seriously as expressions of prior
    belief. There may also be computational difficulties with the Bayesian approach.
    Many of these can be addressed using **Markov chain Monte Carlo** methods, which
    are another main focus of my research. The details of this approach will be clearer
    as you go through this chapter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 不恰当地，许多贝叶斯派并不真正以真正的贝叶斯方式思考。因此，人们可以在文献中找到许多伪贝叶斯程序，其中使用的模型和先验不能被认真地看作是先验信念的表达。贝叶斯方法也可能存在计算困难。其中许多可以通过**马尔可夫链蒙特卡洛**方法来解决，这也是我研究的另一个主要焦点。随着您阅读本章，这种方法的细节将更加清晰。
- en: Naive Bayes
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: In ML, **Naive Bayes** (**NB**) is an example of the probabilistic classifier
    based on the well-known Bayes' theorem with strong independence assumptions between
    the features. We will discuss Naive Bayes in detail in this section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**朴素贝叶斯**（**NB**）是一个基于著名的贝叶斯定理和特征之间强独立假设的概率分类器的例子。我们将在本节详细讨论朴素贝叶斯。
- en: An overview of Bayes' theorem
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理概述
- en: In probability theory, **Bayes' theorem** describes the probability of an event
    based on a prior knowledge of conditions that is related to that certain event.
    This is a theorem of probability originally stated by the Reverend Thomas Bayes.
    In other words, it can be seen as a way of understanding how the probability theory
    is true and affected by a new piece of information. For example, if cancer is
    related to age, the information about *age* can be used to assess the probability
    that a person might have cancer more accurately*.*
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，**贝叶斯定理**描述了基于与某一事件相关的先验条件的先验知识来计算该事件的概率。这是由托马斯·贝叶斯牧师最初陈述的概率定理。换句话说，它可以被看作是一种理解概率论如何受新信息影响的方式。例如，如果癌症与年龄有关，关于*年龄*的信息可以用来更准确地评估一个人可能患癌症的概率*。*
- en: 'Bayes'' theorem is stated mathematically as the following equation:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理在数学上陈述如下方程：
- en: '![](img/00241.gif)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00241.gif)'
- en: 'In the preceding equation, *A* and *B* are events with *P (B) ≠ 0,* and the
    other terms can be described as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，*A*和*B*是具有*P (B) ≠ 0*的事件，其他项可以描述如下：
- en: '*P*(*A*) and *P*(*B*) are the probabilities of observing *A* and *B* without
    regard to each other (that is, independence)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*A*)和*P*(*B*)是观察到*A*和*B*的概率，而不考虑彼此（即独立性）'
- en: '*P*(*A* | *B*) is the conditional probability of observing event *A* given
    that *B* is true'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*A* | *B*)是在*B*为真的情况下观察到事件*A*的条件概率'
- en: '*P*(*B*| *A*) is the conditional probability of observing event *B* given that
    *A* is true'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*B*| *A*)是在*A*为真的情况下观察到事件*B*的条件概率'
- en: 'As you probably know, a well-known Harvard study shows that only 10% of happy
    people are rich. However, you might think that this statistic is very compelling
    but you might be somewhat interested in knowing the percentage of rich people
    are also really happy*.* Bayes'' theorem helps you out on how to calculate this
    reserving statistic using two additional clues:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能知道，一项著名的哈佛大学研究显示，只有10%的快乐人群是富裕的。然而，您可能认为这个统计数据非常有说服力，但您可能对知道富裕人群中也真的很快乐的百分比感兴趣*。*贝叶斯定理可以帮助您计算这个逆转统计，使用两个额外线索：
- en: The percentage of people overall who are happy, that is, *P(A).*
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总体上快乐的人的百分比，即*P(A).*
- en: The percentage of people overall who are rich, that is *P(B).*
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总体上富裕的人的百分比，即*P(B).*
- en: 'The key idea behind Bayes'' theorem is reversing the statistic considering
    the overall rates**.** Suppose that the following pieces of information are available
    as a prior:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理背后的关键思想是逆转统计考虑整体比率**。**假设以下信息作为先验可用：
- en: 40% of people are happy and *=> P(A).*
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 40%的人是快乐的*=> P(A).*
- en: 5% of people are rich *=> P(B).*
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5%的人是富裕的*=> P(B).*
- en: 'Now let''s consider that the Harvard study is correct, that is, *P(B|A) = 10%*.
    Now the fraction of rich people who are happy, that is, *P(A | B),* can be calculated
    as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们假设哈佛大学的研究是正确的，即*P(B|A) = 10%*。现在富裕人群中快乐的人的比例，即*P(A | B),* 可以计算如下：
- en: '*P(A|B) = {P(A)* P(B| A)}/ P(B) = (40%*10%)/5% = 80%*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A|B) = {P(A)* P(B| A)}/ P(B) = (40%*10%)/5% = 80%*'
- en: 'Consequently, a majority of the people are also happy! Nice. To make it clearer,
    now let''s assume the population of the whole world is 1,000 for simplicity. Then,
    according to our calculation, there are two facts that exist:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，大多数人也很高兴！很好。为了更清楚，现在让我们假设整个世界的人口为1,000，以便简化。然后，根据我们的计算，存在两个事实：
- en: 'Fact 1: This tells us 400 people are happy, and the Harvard study tells us
    that 40 of these happy people are also rich.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事实1：这告诉我们有400人很高兴，哈佛的研究告诉我们这些快乐的人中有40个也很富有。
- en: 'Fact 2: There are 50 rich people altogether, and so the fraction who are happy
    is 40/50 = 80%.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事实2：总共有50个富人，所以快乐的比例是40/50 = 80%。
- en: This proves the Bayes theorem and its effectiveness. However, more comprehensive
    examples can be found at [https://onlinecourses.science.psu.edu/stat414/node/43](https://onlinecourses.science.psu.edu/stat414/node/43).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了贝叶斯定理及其有效性。然而，更全面的例子可以在[https://onlinecourses.science.psu.edu/stat414/node/43](https://onlinecourses.science.psu.edu/stat414/node/43)找到。
- en: My name is Bayes, Naive Bayes
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我的名字是贝叶斯，朴素贝叶斯
- en: I'm Bayes, Naive Bayes (NB). I'm a successful classifier based upon the principle
    of **maximum a posteriori** (**MAP**). As a classifier, I am highly scalable,
    requiring a number of parameters linear in the number of variables (features/predictors)
    in a learning problem. I have several properties, for example, I am computationally
    faster, if you can hire me to classify something I'm simple to implement, and
    I can work well with high-dimensional datasets. Moreover, I can handle missing
    values in your dataset. Nevertheless, I'm adaptable since the model can be modified
    with new training data without rebuilding the model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我是贝叶斯，朴素贝叶斯（NB）。我是一个成功的分类器，基于**最大后验概率**（**MAP**）原理。作为一个分类器，我具有高度可扩展性，需要的参数数量与学习问题中的变量（特征/预测器）数量成正比。我有几个特性，例如，我在计算上更快，如果你雇佣我来分类一些东西，我很容易实现，并且我可以很好地处理高维数据集。此外，我可以处理数据集中的缺失值。然而，我是适应性的，因为模型可以通过新的训练数据进行修改而无需重建模型。
- en: In Bayesian statistics, a MAP estimate is an estimate of an unknown quantity
    that equals the mode of the posterior distribution. The MAP estimate can be used
    to obtain a point estimate of an unobserved quantity on the basis of empirical
    data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯统计学中，MAP估计是未知数量的估计，等于后验分布的模。MAP估计可用于根据经验数据获得未观察到的数量的点估计。
- en: 'Sounds something similar to James Bond movies? Well, you/we can think a classifer
    as agent 007, right? Just kidding. I believe I am not as the parameters of the
    Naive Bayes classifier such as priori and conditional probabilities are learned
    or rather determined using a deterministic set of steps: this involves two very
    trivial operations that can be blindingly fast on modern computers, that is, counting
    and dividing. There is no *iteration*. There is no *epoch*. There is *no optimization
    of a cost equation* (which can be complex, of cubic order on an average or at
    least of square order complexity). There is no *error back-propagation*. There
    is no operation(s) involving *solving a matrix equation*. These make Naive Bayes
    and its overall training faster.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来有点像詹姆斯·邦德电影？好吧，你/我们可以把分类器看作是007特工，对吧？开玩笑。我相信我不像朴素贝叶斯分类器的参数，例如先验和条件概率是通过一组确定的步骤学习或确定的：这涉及两个非常微不足道的操作，在现代计算机上可以非常快速，即计数和除法。没有*迭代*。没有*时代*。没有*优化成本方程*（这可能是复杂的，平均为三次方或至少为二次方复杂度）。没有*错误反向传播*。没有涉及*解矩阵方程*的操作。这使得朴素贝叶斯及其整体训练更快。
- en: 'However, before hiring this agent, you/we can discover his pros and cons so
    that we can use this agent like a trump card by utilizing it''s best only. Well,
    here''s table summarizing the pros and cons of this agent:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在雇佣这个代理之前，你/我们可以发现他的优缺点，这样我们才能像使用王牌一样利用它的优势。好吧，下面是总结这个代理的优缺点的表格：
- en: '| **Agent** | **Pros** | **Cons** | **Better at** |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **代理** | **优点** | **缺点** | **擅长** |'
- en: '| **Naive Bayes (NB)** | - Computationally fast- Simple to implement- Works
    well with high dimensions- Can handle missing values- Requires a small amount
    of data to train the model - It is scalable- Is adaptable since the model can
    be modified with new training data without rebuilding the model | - Relies on
    independence assumptions and so performs badly if the assumption does not meet-
    Relatively low accuracy- If you have no occurrences of a class label and a certain
    attribute value together then the frequency-based probability estimate will be
    zero | - When data has lots of missing values- When dependencies of features from
    each other are similar between features- Spam filtering and classification- Classifying
    a news article about technology, politics, sports, and so on.- Text mining |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **朴素贝叶斯（NB）** | - 计算速度快- 实现简单- 在高维度下工作良好- 可处理缺失值- 需要少量数据来训练模型- 可扩展- 适应性强，因为模型可以通过新的训练数据进行修改而无需重建模型
    | - 依赖独立假设，如果假设不成立则性能较差- 相对较低的准确性- 如果类标签和某个属性值没有出现在一起，则基于频率的概率估计将为零 | - 当数据有很多缺失值时-
    当特征之间的依赖关系相似- 垃圾邮件过滤和分类- 对科技、政治、体育等新闻文章进行分类- 文本挖掘 |'
- en: '**Table 1:** Pros and the cons of the Naive Bayes algorithm'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**表1：**朴素贝叶斯算法的优缺点'
- en: Building a scalable classifier with NB
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NB构建可扩展的分类器
- en: In this section, we will see a step-by-step example using **Naive Bayes** (**NB**)
    algorithm. As already stated, NB is highly scalable, requiring a number of parameters
    linear in the number of variables (features/predictors) in a learning problem.
    This scalability has enabled the Spark community to make predictive analytics
    on large-scale datasets using this algorithm. The current implementation of NB
    in Spark MLlib supports both the multinomial NB and Bernoulli NB.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看到使用**朴素贝叶斯**（**NB**）算法的逐步示例。如前所述，NB具有高度可扩展性，需要的参数数量与学习问题中的变量（特征/预测器）数量成正比。这种可扩展性使得Spark社区能够使用这种算法对大规模数据集进行预测分析。Spark
    MLlib中NB的当前实现支持多项式NB和伯努利NB。
- en: Bernoulli NB is useful if the feature vectors are binary. One application would
    be text classification with a bag of words (BOW) approach. On the other hand,
    multinomial NB is typically used for discrete counts. For example, if we have
    a text classification problem, we can take the idea of Bernoulli trials one step
    further and instead of BOW in a document we can use the frequency count in a document.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征向量是二进制的，伯努利NB是有用的。一个应用可能是使用词袋（BOW）方法进行文本分类。另一方面，多项式NB通常用于离散计数。例如，如果我们有一个文本分类问题，我们可以进一步采用伯努利试验的想法，而不是在文档中使用BOW，我们可以使用文档中的频率计数。
- en: 'In this section, we will see how to predict the digits from the **Pen-Based
    Recognition of Handwritten Digits** dataset by incorporating Spark machine learning
    APIs including Spark MLlib, Spark ML, and Spark SQL:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何通过整合Spark机器学习API（包括Spark MLlib、Spark ML和Spark SQL）来预测**基于笔的手写数字识别**数据集中的数字：
- en: '**Step 1\. Data collection, preprocessing, and exploration** - The Pen-based
    recognition of handwritten digits dataset was downloaded from the UCI Machine
    Learning Repository at [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits.](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits)
    This dataset was generated after collecting around 250 digit samples each from
    44 writers, correlated to the location of the pen at fixed time intervals of 100
    milliseconds. Each digit was then written inside a 500 x 500 pixel box. Finally,
    those images were scaled to an integer value between 0 and 100 to create consistent
    scaling between each observation. A well-known spatial resampling technique was
    used to obtain 3 and 8 regularly spaced points on an arc trajectory. A sample
    image along with the lines from point to point can be visualized by plotting the
    3 or 8 sampled points based on their (x, y) coordinates; it looks like what is
    shown in the following table:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1. 数据收集、预处理和探索** - 从UCI机器学习库[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits)下载了基于笔的手写数字数据集。该数据集是在从44位作者那里收集了大约250个数字样本后生成的，这些数字样本与笔在100毫秒的固定时间间隔内的位置相关。然后，每个数字都写在一个500
    x 500像素的框内。最后，这些图像被缩放到0到100之间的整数值，以创建每个观察之间的一致缩放。一个众所周知的空间重采样技术被用来获得弧轨迹上的3和8个等间距点。可以通过根据它们的（x，y）坐标绘制3或8个采样点来可视化一个样本图像以及点与点之间的线；它看起来像下表所示：'
- en: '| Set | ''0'' | ''1'' | ''2'' | ''3'' | ''4'' | ''5'' | ''6'' | ''7'' | ''8''
    | ''9'' | Total |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 集合 | ''0'' | ''1'' | ''2'' | ''3'' | ''4'' | ''5'' | ''6'' | ''7'' | ''8''
    | ''9'' | 总计 |'
- en: '| Training | 780 | 779 | 780 | 719 | 780 | 720 | 720 | 778 | 718 | 719 | 7493
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 780 | 779 | 780 | 719 | 780 | 720 | 720 | 778 | 718 | 719 | 7493 |'
- en: '| Test | 363 | 364 | 364 | 336 | 364 | 335 | 336 | 364 | 335 | 336 | 3497 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 363 | 364 | 364 | 336 | 364 | 335 | 336 | 364 | 335 | 336 | 3497 |'
- en: 'Table 2: Number of digits used for the training and the test set'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：用于训练和测试集的数字数量
- en: As shown in the preceding table, the training set consists of samples written
    by 30 writers and the testing set consists of samples written by 14 writers.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如前表所示，训练集由30位作者撰写的样本组成，测试集由14位作者撰写的样本组成。
- en: '![](img/00130.jpeg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00130.jpeg)'
- en: 'Figure 4: Example of digit 3 and 8 respectively'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：数字3和8的示例
- en: 'More on this dataset can be found at [http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.names](http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.names).
    A digital representation of a sample snapshot of the dataset is shown in the following
    figure:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有关该数据集的更多信息可以在[http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.names](http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.names)找到。数据集的一个样本快照的数字表示如下图所示：
- en: '![](img/00149.gif)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00149.gif)'
- en: 'Figure 5: A snap of the 20 rows of the hand-written digit dataset'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：手写数字数据集的20行快照
- en: Now to predict the dependent variable (that is, label) using the independent
    variables (that is, features), we need to train a multiclass classifier since,
    as shown previously, the dataset now has nine classes, that is, nine handwritten
    digits. For the prediction, we will use the Naive Bayes classifier and evaluate
    the model's performance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了使用独立变量（即特征）预测因变量（即标签），我们需要训练一个多类分类器，因为如前所示，数据集现在有九个类别，即九个手写数字。对于预测，我们将使用朴素贝叶斯分类器并评估模型的性能。
- en: '**Step 2.** Load the required library and packages:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2.** 加载所需的库和包：'
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Step 3.** Create an active Spark session:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3.** 创建一个活跃的Spark会话：'
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that here the master URL has been set as `local[*]`, which means all the
    cores of your machine will be used for processing the Spark job. You should set
    SQL warehouse accordingly and other configuration parameter based on the requirements.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的主URL已设置为“local[*]”，这意味着您的计算机的所有核心将用于处理Spark作业。您应该根据要求相应地设置SQL数据仓库和其他配置参数。
- en: '**Step 4\. Create the DataFrame** - Load the data stored in LIBSVM format as
    a DataFrame:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4. 创建DataFrame** - 将以LIBSVM格式存储的数据加载为DataFrame：'
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For digits classification, the input feature vectors are usually sparse, and
    sparse vectors should be supplied as input to take advantage of sparsity. Since
    the training data is only used once, and moreover the size of the dataset is relatively
    smaller (that is, few MBs), we can cache it if you use the DataFrame more than
    once.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数字分类，输入特征向量通常是稀疏的，应该将稀疏向量作为输入以利用稀疏性。由于训练数据只使用一次，而且数据集的大小相对较小（即几MB），如果您多次使用DataFrame，可以将其缓存。
- en: '**Step 5\. Prepare the training and test set** - Split the data into training
    and test sets (25% held out for testing):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5. 准备训练和测试集** - 将数据分割为训练集和测试集（25%用于测试）：'
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Step 6\. Train the Naive Bayes model** - Train a Naive Bayes model using
    the training set as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6. 训练朴素贝叶斯模型** - 使用训练集训练朴素贝叶斯模型如下：'
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Step 7\. Calculate the prediction on the test set** - Calculate the prediction
    using the model transformer and finally show the prediction against each label
    as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：** 计算测试集上的预测 - 使用模型变换器计算预测，最后显示针对每个标签的预测，如下所示：'
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00189.jpeg)**Figure 6:** Prediction against each label (that is, each
    digit)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00189.jpeg)**图6：** 针对每个标签（即每个数字）的预测'
- en: As you can see in the preceding figure, some labels were predicted accurately
    and some of them were wrongly. Again we need to know the weighted accuracy, precision,
    recall and f1 measures without evaluating the model naively.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，一些标签被准确预测，而另一些标签则错误。再次，我们需要了解加权准确性、精确度、召回率和F1度量，而不是简单地评估模型。
- en: '**Step 8\. Evaluate the model** - Select the prediction and the true label
    to compute test error and classification performance metrics such as accuracy,
    precision, recall, and f1 measure as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：** 评估模型 - 选择预测和真实标签来计算测试错误和分类性能指标，如准确性、精确度、召回率和F1度量，如下所示：'
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Step 9\. Compute the performance metrics** - Compute the classification accuracy,
    precision, recall, f1 measure, and error on test data as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9：** 计算性能指标 - 计算测试数据的分类准确性、精确度、召回率、F1度量和错误，如下所示：'
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Step 10.** Print the performance metrics:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10：** 打印性能指标：'
- en: '[PRE20]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should observe values as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该观察到以下值：
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The performance is not that bad. However, you can still increase the classification
    accuracy by performing hyperparameter tuning. There are further opportunities
    to improve the prediction accuracy by selecting appropriate algorithms (that is,
    classifier or regressor) through cross-validation and train split, which will
    be discussed in the following section.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 性能并不是那么糟糕。但是，您仍然可以通过进行超参数调整来提高分类准确性。通过交叉验证和训练集拆分，可以进一步提高预测准确性，这将在下一节中讨论。
- en: Tune me up!
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整我！
- en: You already know my pros and cons, I have a con that is, my classification accuracy
    is relatively low. However, if you tune me up, I can perform much better. Well,
    should we trust Naive Bayes? If so, shouldn't we look at how to increase the prediction
    performance of this guy? Let's say using the WebSpam dataset. At first, we should
    observe the performance of the NB model, and after that we will see how to increase
    the performance using the cross-validation technique.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解我的优缺点，我的一个缺点是，我的分类准确性相对较低。但是，如果您调整我，我可以表现得更好。好吧，我们应该相信朴素贝叶斯吗？如果是这样，我们不应该看看如何提高这家伙的预测性能吗？比如使用WebSpam数据集。首先，我们应该观察NB模型的性能，然后再看如何使用交叉验证技术提高性能。
- en: 'The WebSpam dataset that downloaded from [http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2)
    contains features and corresponding labels, that is, spam or ham. Therefore, this
    is a supervised machine learning problem, and the task here is to predict whether
    a given message is spam or ham (that is, not spam). The original dataset size
    is 23.5 GB, where the classes are labeled as +1 or -1 (that is, a binary classification
    problem). Later on, we replaced -1 with 0.0 and +1 with 1.0 since Naive Bayes
    does not permit using signed integers. The modified dataset is shown in the following
    figure:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 从[http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2)下载的WebSpam数据集包含特征和相应的标签，即垃圾邮件或正常邮件。因此，这是一个监督式机器学习问题，这里的任务是预测给定消息是垃圾邮件还是正常邮件（即非垃圾邮件）。原始数据集大小为23.5
    GB，类别标签为+1或-1（即二元分类问题）。后来，我们将-1替换为0.0，+1替换为1.0，因为朴素贝叶斯不允许使用有符号整数。修改后的数据集如下图所示：
- en: '![](img/00054.gif)**Figure 7:** A snapshot of the 20 rows of the WebSpam dataset'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00054.gif)**图7：** WebSpam数据集的20行快照'
- en: 'At first, we need to import necessary packages as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入必要的包，如下所示：
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now create the Spark Session as the entry point to the code as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建Spark会话作为代码的入口点，如下所示：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s load the WebSpam dataset and prepare the training set to train the Naive
    Bayes model as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载WebSpam数据集并准备训练集来训练朴素贝叶斯模型，如下所示：
- en: '[PRE24]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the preceding code, setting the seed is required for reproducibility. Now
    let''s make the prediction on the validation set as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，设置种子是为了可重现性。现在让我们在验证集上进行预测，如下所示：
- en: '[PRE25]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now let''s obtain `evaluator` and compute the classification performance metrics
    like accuracy, precision, recall, and `f1` measure as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们获取`evaluator`并计算分类性能指标，如准确性、精确度、召回率和`f1`度量，如下所示：
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now let''s compute and print the performance metrics:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算并打印性能指标：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should receive the following output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该收到以下输出：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Although the accuracy is at a satisfactory level, we can further improve it
    by applying the cross-validation technique. The technique goes as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管准确性达到了令人满意的水平，但我们可以通过应用交叉验证技术进一步提高它。该技术的步骤如下：
- en: Create a pipeline by chaining an NB estimator as the only stage of the pipeline
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过链接一个NB估计器作为管道的唯一阶段来创建管道
- en: Now prepare the param grid for tuning
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在为调整准备参数网格
- en: Perform the 10-fold cross-validation
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行10折交叉验证
- en: Now fit the model using the training set
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在使用训练集拟合模型
- en: Compute the prediction on the validation set
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算验证集上的预测
- en: The first step in model tuning techniques such as cross-validation is pipeline
    creation. A pipeline can be created by chaining a transformer, an estimator, and
    related parameters.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如交叉验证之类的模型调整技术的第一步是创建管道。可以通过链接变换器、估计器和相关参数来创建管道。
- en: '**Step 1\. Pipeline creation** - Let''s create a Naive Bayes estimator (`nb`
    is an estimator in the following case) and create a pipeline by chaining the estimator
    as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：** 创建管道 - 让我们创建一个朴素贝叶斯估计器（在下面的情况中`nb`是一个估计器），并通过链接估计器来创建管道，如下所示：'
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: A pipeline can be considered as the data workflow system for training and prediction
    using the model. ML pipelines provide a uniform set of high-level APIs built on
    top of [DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html)
    that help users create and tune practical machine learning pipelines. DataFrame,
    transformer, estimator, pipeline, and parameter are the five most important components
    in Pipeline creation. For more on Pipeline, interested readers should refer to
    [https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 管道可以被视为用于训练和预测的数据工作流系统。ML管道提供了一组统一的高级API，构建在[DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html)之上，帮助用户创建和调整实用的机器学习管道。DataFrame、转换器、估计器、管道和参数是管道创建中最重要的五个组件。有兴趣的读者可以参考[https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)了解更多关于管道的信息。
- en: In the earlier case, the only stage in our pipeline is an estimator that is
    an algorithm for fitting on a DataFrame to produce a transformer to make sure
    the training is carried out successfully.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期情况下，我们管道中的唯一阶段是一个估计器，它是用于在DataFrame上拟合的算法，以产生一个转换器，以确保训练成功进行。
- en: '**Step 2\. Creating grid parameters** - Let''s use `ParamGridBuilder` to construct
    a grid of parameters to search over:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2. 创建网格参数** - 让我们使用`ParamGridBuilder`构建一个参数网格进行搜索：'
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Step 3\. Performing 10-fold cross-validation** - We now treat the pipeline
    as an estimator, wrapping it in a cross-validator instance. This will allow us
    to jointly choose parameters for all Pipeline stages. A `CrossValidator` requires
    an estimator, a set of estimator `ParamMaps`, and an evaluator. Note that the
    evaluator here is a `BinaryClassificationEvaluator`, and its default metric is
    `areaUnderROC`. However, if you use the evaluator as `MultiClassClassificationEvaluator`,
    you will be able to use the other performance metrics as well:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3. 执行10折交叉验证** - 现在我们将管道视为一个估计器，将其包装在一个交叉验证实例中。这将允许我们共同选择所有管道阶段的参数。`CrossValidator`需要一个估计器、一组估计器`ParamMaps`和一个评估器。请注意，这里的评估器是`BinaryClassificationEvaluator`，其默认指标是`areaUnderROC`。但是，如果您将评估器用作`MultiClassClassificationEvaluator`，您将能够使用其他性能指标：'
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Step 4.** Fit the cross-validation model with the training set as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4.** 按以下方式使用训练集拟合交叉验证模型：'
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '**Step 5.** Compute performance as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5.** 按以下方式计算性能：'
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Step 6.** Obtain the evaluator, compute the performance metrics, and display
    the results. Now let''s obtain `evaluator` and compute the classification performance
    metrics such as accuracy, precision, recall, and f1 measure. Here `MultiClassClassificationEvaluator`
    will be used for accuracy, precision, recall, and f1 measure:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 6.** 获取评估器，计算性能指标并显示结果。现在让我们获取`evaluator`并计算分类性能指标，如准确度、精确度、召回率和f1度量。这里将使用`MultiClassClassificationEvaluator`来计算准确度、精确度、召回率和f1度量：'
- en: '[PRE34]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now compute the classification accuracy, precision, recall, f1 measure, and
    error on test data as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在按照以下步骤计算测试数据的分类准确度、精确度、召回率、f1度量和错误：
- en: '[PRE35]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now let''s print the performance metrics:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印性能指标：
- en: '[PRE36]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You should now receive the results as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该收到以下结果：
- en: '[PRE37]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now this is much better compared to the previous one, right? Please note that
    you might receive a slightly different result due to the random split of the dataset
    and your platform.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这比之前的好多了，对吧？请注意，由于数据集的随机分割和您的平台，您可能会收到略有不同的结果。
- en: The decision trees
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: In this section, we will discuss the DT algorithm in detail. A comparative analysis
    of Naive Bayes and DT will be discussed too. DTs are commonly considered as a
    supervised learning technique used for solving classification and regression tasks.
    A DT is simply a decision support tool that uses a tree-like graph (or a model
    of decisions) and their possible consequences, including chance event outcomes,
    resource costs, and utility. More technically, each branch in a DT represents
    a possible decision, occurrence, or reaction in terms of statistical probability.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细讨论决策树算法。还将讨论朴素贝叶斯和决策树的比较分析。决策树通常被认为是一种用于解决分类和回归任务的监督学习技术。决策树简单地说是一种决策支持工具，它使用树状图（或决策模型）及其可能的后果，包括机会事件结果、资源成本和效用。更技术性地说，决策树中的每个分支代表了一个可能的决策、发生或反应，以统计概率的形式。
- en: Compared to Naive Bayes, DT is a far more robust classification technique. The
    reason is that at first DT splits the features into training and test set. Then
    it produces a good generalization to infer the predicted labels or classes. Most
    interestingly, DT algorithm can handle both binary and multiclass classification
    problems.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 与朴素贝叶斯相比，决策树是一种更加健壮的分类技术。原因在于决策树首先将特征分为训练集和测试集。然后它产生了一个很好的泛化来推断预测的标签或类。最有趣的是，决策树算法可以处理二元和多类分类问题。
- en: '![](img/00081.jpeg)**Figure 8:** A sample decision tree on the admission test
    dataset using the Rattle package of R'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00081.jpeg)**图 8：** 使用Rattle软件包在入学测试数据集上的一个样本决策树'
- en: For instance, in the preceding example figure, DTs learn from the admission
    data to approximate a sine curve with a set of `if...else` decision rules. The
    dataset contains the record of each student who applied for admission, say to
    an American university. Each record contains the graduate record exam score, CGPA
    score, and the rank of the column. Now we will have to predict who is competent
    based on these three features (variables). DTs can be used to solve this kind
    of problem after training the DT model and pruning unwanted branches of the tree.
    In general, a deeper tree signifies more complex decision rules and a better fitted
    model. Therefore, the deeper the tree, the more complex the decision rules and
    the more fitted the model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在前面的示例图中，决策树从入学数据中学习，用一组“if...else”决策规则来逼近正弦曲线。数据集包含每个申请入学的学生的记录，比如申请美国大学。每条记录包含研究生入学考试成绩、CGPA成绩和列的排名。现在我们需要根据这三个特征（变量）来预测谁是胜任的。在训练决策树模型并修剪树的不需要的分支后，决策树可以用来解决这种问题。一般来说，树越深，决策规则越复杂，模型拟合得越好。因此，树越深，决策规则越复杂，模型拟合得越好。
- en: If you would like to draw the preceding figure, just run my R script, execute
    it on RStudio, and feed the admission dataset. The script and the dataset can
    be found in my GitHub repository at [https://github.com/rezacsedu/AdmissionUsingDecisionTree](https://github.com/rezacsedu/AdmissionUsingDecisionTree).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想绘制前面的图，只需运行我的R脚本，在RStudio上执行，并提供入学数据。脚本和数据集可以在我的GitHub存储库中找到[https://github.com/rezacsedu/AdmissionUsingDecisionTree](https://github.com/rezacsedu/AdmissionUsingDecisionTree)。
- en: Advantages and disadvantages of using DTs
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树的优缺点
- en: Before hiring me, you can discover my pros and cons and when I work best from
    Table 3 so that you don't have any late regrets!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在雇佣我之前，你可以从表3中了解我的优缺点以及我最擅长的工作时间，这样你就不会有任何迟来的后悔！
- en: '| **Agent** | **Pros** | **Cons** | **Better at** |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **代理** | **优点** | **缺点** | **擅长** |'
- en: '| **Decision trees (DTs)** | -Simple to implement, train, and interpret-Trees
    can be visualized-Requires little data preparation-Less model building and prediction
    time-Can handle both numeric and categorical data-Possible of validating the model
    using the statistical tests-Robust against noise and missing values-High accuracy
    | -Interpretation is hard with large and complex trees-Duplication may occur within
    the same subtree-Possible issues with diagonal decision boundaries-DT learners
    can create overcomplex trees that do not generalize data well-Sometimes DTs can
    be unstable because of small variants in the data-Learning the DTs itself an NP-complete
    problem (aka. nondeterministic polynomial time -complete problem)-DTs learners
    create biased trees if some classes dominate | -Targeting highly accurate classification-Medical
    diagnosis and prognosis-Credit risk analytics |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **决策树（DTs）** | -简单实现、训练和解释-树可以可视化-准备数据很少-模型构建和预测时间少-可以处理数值和分类数据-可以使用统计测试验证模型-对噪声和缺失值很健壮-高准确性
    | -大型和复杂树的解释很困难-同一子树内可能会出现重复-可能出现对角决策边界问题-DT学习者可能会创建不能很好泛化数据的过于复杂的树-有时由于数据的微小变化，决策树可能不稳定-学习决策树本身是一个NP完全问题-如果某些类占主导地位，DT学习者会创建有偏见的树
    | -针对高准确性分类-医学诊断和预后-信用风险分析 |'
- en: '**Table 3:** Pros and cons of the decision tree'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**表3：** 决策树的优缺点'
- en: Decision tree versus Naive Bayes
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树与朴素贝叶斯
- en: As stated in the preceding table, DTs are very easy to understand and debug
    because of their flexibility for training datasets. They will work with both classification
    as well as regression problems.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如前表所述，由于其对训练数据的灵活性，决策树非常容易理解和调试。它们可以处理分类问题和回归问题。
- en: If you are trying to predict values out of categorical or continuous values,
    DTs will handle both problems. Consequently, if you just have tabular data, feed
    it to the DT and it will build the model toward classifying your data without
    any additional requirement for upfront or manual interventions. In summary, DTs
    are very simple to implement, train, and interpret. With very little data preparation,
    DTs can build the model with much less prediction time. As said earlier, they
    can handle both numeric and categorical data and are very robust against noise
    and missing values. They are very easy to validate the model using statistical
    tests. More interestingly, the constructed trees can be visualized. Overall, they
    provide very high accuracy.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要预测分类值或连续值，决策树都可以处理。因此，如果你只有表格数据，将其提供给决策树，它将构建模型以对数据进行分类，而无需任何额外的前期或手动干预。总之，决策树非常简单实现、训练和解释。准备数据很少，决策树就可以用更少的预测时间构建模型。正如前面所说，它们可以处理数值和分类数据，并且对噪声和缺失值非常健壮。使用统计测试非常容易验证模型。更有趣的是，构建的树可以可视化。总的来说，它们提供了非常高的准确性。
- en: However, on the downside, DTs sometimes tend to the overfitting problem for
    the training data. This means that you generally have to prune the tree and find
    an optimal one for better classification or regression accuracy. Moreover, duplication
    may occur within the same subtree. Sometimes it also creates issues with diagonal
    decision boundaries towards overfitting and underfitting. Furthermore, DT learners
    can create over-complex trees that do not generalize the data well this makes
    overall interpretation hard. DTs can be unstable because of small variants in
    the data, and as a result learning DT is itself an NP-complete problem. Finally,
    DT learners create biased trees if some classes dominate over others.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，决策树有时倾向于过拟合训练数据的问题。这意味着通常需要修剪树，并找到一个更好的分类或回归准确性的最佳树。此外，同一子树内可能会出现重复。有时它还会在对角决策边界问题上出现问题，导致过拟合和欠拟合。此外，DT学习者可能会创建不能很好泛化数据的过于复杂的树，这使得整体解释很困难。由于数据的微小变化，决策树可能不稳定，因此学习决策树本身是一个NP完全问题。最后，如果某些类占主导地位，DT学习者会创建有偏见的树。
- en: Readers are suggested to refer to *Tables 1* and *3* to get a comparative summary
    between Naive Bayes and DTs.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 建议读者参考*表1*和*3*，以获得朴素贝叶斯和DT之间的比较摘要。
- en: 'On the other hand, there is a saying while using Naive Bayes: *NB requires
    you build a classification by hand*. There''s no way to feed a bunch of tabular
    data to it, and it picks the best features for the classification. In this case,
    however, choosing the right features and features that matter is up to the user,
    that is, you. On the other hand, DTs will pick the best features from tabular
    data. Given this fact, you probably need to combine Naive Bayes with other statistical
    techniques to help toward best feature extraction and classify them later on.
    Alternatively, use DTs to get better accuracy in terms of precision, recall, and
    f1 measure. Another positive thing about Naive Bayes is that it will answer as
    a continuous classifier. However, the downside is that they are harder to debug
    and understand. Naive Bayes does quite well when the training data doesn''t have
    good features with low amounts of data.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在使用朴素贝叶斯时有一句话：*NB需要您手动构建分类*。无法将大量表格数据输入其中，然后选择最佳的特征进行分类。然而，在这种情况下，选择正确的特征和重要的特征取决于用户，也就是您。另一方面，DT将从表格数据中选择最佳的特征。鉴于这一事实，您可能需要将朴素贝叶斯与其他统计技术结合起来，以帮助进行最佳特征提取并稍后对其进行分类。或者，使用DT以获得更好的精度、召回率和f1度量的准确性。朴素贝叶斯的另一个优点是它将作为连续分类器进行回答。然而，缺点是它们更难调试和理解。当训练数据没有良好特征且数据量较小时，朴素贝叶斯表现得相当不错。
- en: In summary, if you are trying to choose the better classifier from these two
    often times it is best to test each one to solve a problem. My recommendation
    would be to build a DT as well as a Naive Bayes classifier using the training
    data you have and then compare the performance using available performance metrics
    and then decide which one best solves your problem subject to the dataset nature.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，如果您试图从这两者中选择更好的分类器，通常最好的方法是测试每个来解决问题。我的建议是使用您拥有的训练数据构建DT和朴素贝叶斯分类器，然后使用可用的性能指标比较性能，然后决定哪一个最适合解决您的问题，取决于数据集的性质。
- en: Building a scalable classifier with DT algorithm
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DT算法构建可扩展分类器
- en: 'As you have already seen, using the OVTR classifier we observed the following
    values of the performance metrics on the OCR dataset:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您已经看到的，使用OVTR分类器，我们观察到OCR数据集上性能指标的以下值：
- en: '[PRE38]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This signifies that the accuracy of the model on that dataset is very low. In
    this section, we will see how we could improve the performance using the DT classifier.
    An example with Spark 2.1.0 will be shown using the same OCR dataset. The example
    will have several steps including data loading, parsing, model training, and,
    finally, model evaluation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明该数据集上模型的准确性非常低。在本节中，我们将看到如何使用DT分类器来提高性能。将使用相同的OCR数据集展示Spark 2.1.0的示例。该示例将包括数据加载、解析、模型训练以及最终的模型评估等多个步骤。
- en: 'Since we will be using the same dataset, to avoid redundancy, we will escape
    the dataset exploration step and will enter into the example:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用相同的数据集，为了避免冗余，我们将跳过数据集探索步骤，直接进入示例：
- en: '**Step 1.** Load the required library and packages as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1.** 加载所需的库和包如下：'
- en: '[PRE39]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Step 2.** Create an active Spark session as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2.** 创建一个活跃的Spark会话如下：'
- en: '[PRE40]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note that here the master URL has been set as `local[*]`, which means all the
    cores of your machine will be used for processing the Spark job. You should set
    SQL warehouse accordingly and other configuration parameter based on requirements.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里将主URL设置为`local[*]`，这意味着您的计算机的所有核心将用于处理Spark作业。您应该根据要求设置SQL仓库和其他配置参数。
- en: '**Step 3\. Create the DataFrame** - Load the data stored in LIBSVM format as
    a DataFrame as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3.** 创建DataFrame - 加载以LIBSVM格式存储的数据作为DataFrame如下：'
- en: '[PRE41]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: For the classification of digits, the input feature vectors are usually sparse,
    and sparse vectors should be supplied as input to take advantage of the sparsity.
    Since the training data is only used once, and moreover the size of the dataset
    is relatively small (that is, a few MBs), we can cache it if you use the DataFrame
    more than once.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数字的分类，输入特征向量通常是稀疏的，应该提供稀疏向量作为输入以利用稀疏性。由于训练数据只使用一次，而且数据集的大小相对较小（即几MB），如果您多次使用DataFrame，可以将其缓存起来。
- en: '**Step 4\. Label indexing** - Index the labels, adding metadata to the label
    column. Then let''s fit on the whole dataset to include all labels in the index:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4.** 标签索引 - 对标签进行索引，为标签列添加元数据。然后让我们在整个数据集上进行拟合，以包含索引中的所有标签：'
- en: '[PRE42]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '**Step 5\. Identifying categorical features** - The following code segment
    automatically identifies categorical features and indexes them:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5.** 识别分类特征 - 以下代码段自动识别分类特征并对其进行索引：'
- en: '[PRE43]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: For this case, if the number of features is more than four distinct values,
    they will be treated as continuous.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，如果特征的数量超过四个不同的值，它们将被视为连续的。
- en: '**Step 6\. Prepare the training and test sets** - Split the data into training
    and test sets (25% held out for testing):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6.** 准备训练和测试集 - 将数据分割为训练集和测试集（25%用于测试）：'
- en: '[PRE44]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Step 7.** Train the DT model as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7.** 训练DT模型如下：'
- en: '[PRE45]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Step 8.** Convert the indexed labels back to original labels as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8.** 将索引的标签转换回原始标签如下：'
- en: '[PRE46]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 9\. Create a DT pipeline** - Let''s create a DT pipeline by changing
    the indexers, label converter and tree together:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9.** 创建DT管道 - 让我们通过更改索引器、标签转换器和树来创建一个DT管道：'
- en: '[PRE47]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '**Step 10\. Running the indexers** - Train the model using the transformer
    and run the indexers:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10.** 运行索引器 - 使用转换器训练模型并运行索引器：'
- en: '[PRE48]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '**Step 11\. Calculate the prediction on the test set** - Calculate the prediction
    using the model transformer and finally show the prediction against each label
    as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤11.** 计算测试集上的预测 - 使用模型转换器计算预测，最后显示每个标签的预测如下：'
- en: '[PRE49]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](img/00344.jpeg)**Figure 9:** Prediction against each label (that is, each
    letter)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00344.jpeg)**图9：** 预测与每个标签（即每个字母）相对应'
- en: As you can see from the preceding figure, some labels were predicted accurately
    and some of them were predicted wrongly. However, we know the weighted accuracy,
    precision, recall, and f1 measures, but we need to evaluate the model first.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图可以看出，一些标签被准确预测，而另一些则被错误预测。然而，我们知道加权准确性、精确度、召回率和f1度量，但我们需要先评估模型。
- en: '**Step 12\. Evaluate the model** - Select the prediction and the true label
    to compute test error and classification performance metrics such as accuracy,
    precision, recall, and f1 measure as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤12. 评估模型** - 选择预测和真实标签来计算测试错误和分类性能指标，如准确性、精确度、召回率和f1度量，如下所示：'
- en: '[PRE50]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '**Step 13\. Compute the performance metrics** - Compute the classification
    accuracy, precision, recall, f1 measure, and error on test data as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤13. 计算性能指标** - 计算测试数据的分类准确性、精确度、召回率、f1度量和错误，如下所示：'
- en: '[PRE51]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '**Step 14.** Print the performance metrics:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤14.** 打印性能指标：'
- en: '[PRE52]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'You should observe values as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该按以下数值观察：
- en: '[PRE53]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Now the performance is excellent, right? However, you can still increase the
    classification accuracy by performing hyperparameter tuning. There are further
    opportunities to improve the prediction accuracy by selecting appropriate algorithms
    (that is, classifier or regressor) through cross-validation and train split.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在性能很好，对吧？然而，您仍然可以通过执行超参数调整来提高分类准确性。通过交叉验证和训练集拆分，可以进一步提高预测准确性，选择适当的算法（即分类器或回归器）。
- en: '**Step 15.** Print the DT nodes:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤15.** 打印决策树节点：'
- en: '[PRE54]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Finally, we will print a few nodes in the DT, as shown in the following figure:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将打印决策树中的一些节点，如下图所示：
- en: '![](img/00199.gif)**Figure 10:** A few decision tree nodes that were generated
    during the model building'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00199.gif)**图10：** 在模型构建过程中生成的一些决策树节点'
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed some advanced algorithms in ML and found out how
    to use a simple yet powerful method of Bayesian inference to build another kind
    of classification model, multinomial classification algorithms. Moreover, the
    Naive Bayes algorithm was discussed broadly from the theoretical and technical
    perspectives. At the last pace, a comparative analysis between the DT and Naive
    Bayes algorithms was discussed and a few guidelines were provided.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一些机器学习中的高级算法，并发现了如何使用一种简单而强大的贝叶斯推断方法来构建另一种分类模型，即多项式分类算法。此外，从理论和技术角度广泛讨论了朴素贝叶斯算法。最后，讨论了决策树和朴素贝叶斯算法之间的比较分析，并提供了一些指导方针。
- en: In the next chapter*,* we will dig even deeper into ML and find out how we can
    take advantage of ML to cluster records belonging to a dataset of unsupervised
    observations.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地研究机器学习，并找出如何利用机器学习来对属于无监督观测数据集的记录进行聚类。
