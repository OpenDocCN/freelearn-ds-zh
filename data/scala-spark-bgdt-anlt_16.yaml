- en: Spark Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark调优
- en: '*"Harpists spend 90 percent of their lives tuning their harps and 10 percent
    playing out of tune."*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “竖琴手的90%的时间都在调琴，10%的时间在弹走音。”
- en: '- Igor Stravinsky'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 伊戈尔·斯特拉文斯基'
- en: 'In this chapter, we will dig deeper into Apache Spark internals and see that
    while Spark is great in making us feel like we are using just another Scala collection,
    we don''t have to forget that Spark actually runs in a distributed system. Therefore,
    some extra care should be taken. In a nutshell, the following topics will be covered
    in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入了解Apache Spark的内部，并看到，虽然Spark在让我们感觉像是在使用另一个Scala集合方面做得很好，但我们不必忘记Spark实际上是在分布式系统中运行的。因此，需要额外小心。简而言之，本章将涵盖以下主题：
- en: Monitoring Spark jobs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监视Spark作业
- en: Spark configuration
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark配置
- en: Common mistakes in Spark app development
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark应用程序开发中的常见错误
- en: Optimization techniques
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化技术
- en: Monitoring Spark jobs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监视Spark作业
- en: Spark provides web UI for monitoring all the jobs running or completed on computing
    nodes (drivers or executors). In this section, we will discuss in brief how to
    monitor Spark jobs using Spark web UI with appropriate examples. We will see how
    to monitor the progress of jobs (including submitted, queued, and running jobs).
    All the tabs in the Spark web UI will be discussed briefly. Finally, we will discuss
    the logging procedure in Spark for better tuning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Spark为监视计算节点（驱动程序或执行程序）上运行或已完成的所有作业提供了Web UI。在本节中，我们将简要讨论如何使用适当的示例使用Spark Web
    UI监视Spark作业的进度。我们将看到如何监视作业的进度（包括已提交、排队和运行的作业）。将简要讨论Spark Web UI中的所有选项卡。最后，我们将讨论Spark的日志记录过程，以便更好地进行调优。
- en: Spark web interface
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Web界面
- en: The web UI (also known as Spark UI) is the web interface for running Spark applications
    to monitor the execution of jobs on a web browser such as Firefox or Google Chrome.
    When a SparkContext launches, a web UI that displays useful information about
    the application gets started on port 4040 in standalone mode. The Spark web UI
    is available in different ways depending on whether the application is still running
    or has finished its execution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Web UI（也称为Spark UI）是用于在Web浏览器（如Firefox或Google Chrome）上监视Spark应用程序的执行的Web界面。当SparkContext启动时，独立模式下将在端口4040上启动显示有关应用程序的有用信息的Web
    UI。Spark Web UI的可用性取决于应用程序是否仍在运行或已完成执行。
- en: 'Also, you can use the web UI after the application has finished its execution
    by persisting all the events using `EventLoggingListener`. The `EventLoggingListener`,
    however, cannot work alone, and the incorporation of the Spark history server
    is required. Combining these two features, the following facilities can be achieved:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以在应用程序完成执行后使用Web UI，方法是使用`EventLoggingListener`持久化所有事件。但是，`EventLoggingListener`不能单独工作，需要结合Spark历史服务器。结合这两个功能，可以实现以下功能：
- en: A list of scheduler stages and tasks
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度程序阶段和任务的列表
- en: A summary of RDD sizes
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD大小的摘要
- en: Memory usage
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存使用情况
- en: Environmental information
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境信息
- en: Information about the running executors
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关正在运行的执行程序的信息
- en: You can access the UI at `http://<driver-node>:4040` in a web browser. For example,
    a Spark job submitted and running as a standalone mode can be accessed at `http://localhost:4040`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Web浏览器中访问UI，网址为`http://<driver-node>:4040`。例如，以独立模式提交并运行的Spark作业可以在`http://localhost:4040`上访问。
- en: Note that if multiple SparkContexts are running on the same host, they will
    bind to successive ports beginning with 4040, 4041, 4042, and so on. By default,
    this information will be available for the duration of your Spark application
    only. This means that when your Spark job finishes its execution, the binding
    will no longer be valid or accessible.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果同一主机上运行多个SparkContext，则它们将绑定到从4040开始的连续端口，如4040、4041、4042等。默认情况下，此信息仅在您的Spark应用程序运行期间可用。这意味着当您的Spark作业完成执行时，绑定将不再有效或可访问。
- en: As long as the job is running, stages can be observed on Spark UI. However,
    to view the web UI after the job has finished the execution, you could try setting
    `spark.eventLog.enabled` as true before submitting your Spark jobs. This forces
    Spark to log all the events to be displayed in the UI that are already persisted
    on storage such as local filesystem or HDFS.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 只要作业正在运行，就可以在Spark UI上观察到阶段。但是，要在作业完成执行后查看Web UI，可以尝试在提交Spark作业之前将`spark.eventLog.enabled`设置为true。这将强制Spark记录所有已在存储中持久化的事件，以便在UI中显示。
- en: 'In the previous chapter, we saw how to submit a Spark job to a cluster. Let''s
    reuse one of the commands for submitting the k-means clustering, as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到如何将Spark作业提交到集群。让我们重用提交k均值聚类的命令之一，如下所示：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you submit the job using the preceding command, you will not be able to
    see the status of the jobs that have finished the execution, so to make the changes
    permanent, use the following two options:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用上述命令提交作业，则将无法查看已完成执行的作业的状态，因此要使更改永久生效，请使用以下两个选项：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By setting the preceding two configuration variables, we asked the Spark driver
    to make the event logging enabled to be saved at `file:///home/username/log`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置前两个配置变量，我们要求Spark驱动程序启用事件记录以保存在`file:///home/username/log`。
- en: 'In summary, with the following changes, your submitting command will be as
    follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，通过以下更改，您的提交命令将如下所示：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/00201.jpeg)**Figure 1:** Spark web UI'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Spark Web UI
- en: 'As shown in the preceding screenshot, Spark web UI provides the following tabs:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的屏幕截图所示，Spark Web UI提供以下选项卡：
- en: Jobs
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业
- en: Stages
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阶段
- en: Storage
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储
- en: Environment
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境
- en: Executors
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行程序
- en: SQL
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL
- en: It is to be noted that all the features may not be visible at once as they are
    lazily created on demand, for example, while running a streaming job.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，并非所有功能都可以一次性显示，因为它们是按需懒惰创建的，例如，在运行流式作业时。
- en: Jobs
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作业
- en: 'Depending upon the SparkContext, the Jobs tab shows the status of all the Spark
    jobs in a Spark application. When you access the Jobs tab on the Spark UI using
    a web browser at `http://localhost:4040` (for standalone mode), you should observe
    the following options:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据SparkContext的不同，作业选项卡显示了Spark应用程序中所有Spark作业的状态。当您在Spark UI上使用Web浏览器访问`http://localhost:4040`的作业选项卡（对于独立模式），您应该观察以下选项：
- en: 'User: This shows the active user who has submitted the Spark job'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户：显示已提交Spark作业的活跃用户
- en: 'Total Uptime: This shows the total uptime for the jobs'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总正常运行时间：显示作业的总正常运行时间
- en: 'Scheduling Mode: In most cases, it is first-in-first-out (aka FIFO)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度模式：在大多数情况下，它是先进先出（FIFO）模式
- en: 'Active Jobs: This shows the number of active jobs'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 活跃作业：显示活跃作业的数量
- en: 'Completed Jobs: This shows the number of completed jobs'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已完成的作业：显示已完成的作业数量
- en: 'Event Timeline: This shows the timeline of a job that has completed its execution'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件时间轴：显示已完成执行的作业的时间轴
- en: 'Internally, the Jobs tab is represented by the `JobsTab` class, which is a
    custom SparkUI tab with the jobs prefix. The Jobs tab uses `JobProgressListener`
    to access statistics about the Spark jobs to display the above information on
    the page. Take a look at the following screenshot:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，作业选项卡由`JobsTab`类表示，它是一个带有作业前缀的自定义SparkUI选项卡。作业选项卡使用`JobProgressListener`来访问有关Spark作业的统计信息，以在页面上显示上述信息。请查看以下屏幕截图：
- en: '![](img/00135.jpeg)**Figure 2:** The jobs tab in the Spark web UI'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00135.jpeg)**图2：**Spark Web UI中的作业选项卡'
- en: 'If you further expand the Active Jobs option in the Jobs tab, you will be able
    to see the execution plan, status, number of completed stages, and the job ID
    of that particular job as DAG Visualization, as shown in the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在作业选项卡中进一步展开“Active Jobs”选项，您将能够看到执行计划、状态、已完成阶段的数量以及该特定作业的作业ID，如下所示：
- en: '![](img/00185.jpeg)**Figure 3:** The DAG visualization for task in the Spark
    web UI (abridged)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00185.jpeg)**图3：**Spark Web UI中任务的DAG可视化（摘要）'
- en: When a user enters the code in the Spark console (for example, Spark shell or
    using Spark submit), Spark Core creates an operator graph. This is basically what
    happens when a user executes an action (for example, reduce, collect, count, first,
    take, countByKey, saveAsTextFile) or transformation (for example, map, flatMap,
    filter, mapPartitions, sample, union, intersection, distinct) on an RDD (which
    are immutable objects) at a particular node.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户在Spark控制台（例如，Spark shell或使用Spark submit）中输入代码时，Spark Core会创建一个操作符图。这基本上是当用户在特定节点上执行操作（例如，reduce、collect、count、first、take、countByKey、saveAsTextFile）或转换（例如，map、flatMap、filter、mapPartitions、sample、union、intersection、distinct）时发生的情况，这些操作是在RDD上进行的（它们是不可变对象）。
- en: '![](img/00187.jpeg)**Figure 4:** DAG scheduler transforming RDD lineage into
    stage DAG'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00187.jpeg)**图4：**DAG调度程序将RDD谱系转换为阶段DAG'
- en: During the transformation or action, **Directed Acyclic Graph** (**DAG**) information
    is used to restore the node to last transformation and actions (refer to *Figure
    4* and *Figure 5* for a clearer picture) to maintain the data resiliency. Finally,
    the graph is submitted to a DAG scheduler.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换或操作期间，使用**有向无环图**（**DAG**）信息来将节点恢复到最后的转换和操作（参见*图4*和*图5*以获得更清晰的图像），以维护数据的弹性。最后，图被提交给DAG调度程序。
- en: How does Spark compute the DAG from the RDD and subsequently execute the task?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark如何从RDD计算DAG，然后执行任务？
- en: At a high level, when any action is called on the RDD, Spark creates the DAG
    and submits it to the DAG scheduler. The DAG scheduler divides operators into
    stages of tasks. A stage comprises tasks based on partitions of the input data.
    The DAG scheduler pipelines operators together. For example, many map operators
    can be scheduled in a single stage. The final result of a DAG scheduler is a set
    of stages. The stages are passed on to the task scheduler. The task scheduler
    launches tasks through the cluster manager (Spark Standalone/YARN/Mesos). The
    task scheduler doesn't know about the dependencies of the stages. The worker executes
    the tasks on the stage.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，当RDD上调用任何操作时，Spark会创建DAG并将其提交给DAG调度程序。DAG调度程序将操作符划分为任务阶段。一个阶段包括基于输入数据的分区的任务。DAG调度程序将操作符进行流水线处理。例如，可以在单个阶段中安排多个map操作符。DAG调度程序的最终结果是一组阶段。这些阶段被传递给任务调度程序。任务调度程序通过集群管理器（Spark
    Standalone/YARN/Mesos）启动任务。任务调度程序不知道阶段的依赖关系。工作节点在阶段上执行任务。
- en: The DAG scheduler then keeps track of which RDDs the stage outputs materialized
    from. It then finds a minimal schedule to run jobs and divides the related operators
    into stages of tasks. Based on partitions of the input data, a stage comprises
    multiple tasks. Then, operators are pipelined together with the DAG scheduler.
    Practically, more than one map or reduce operator (for example) can be scheduled
    in a single stage.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，DAG调度程序跟踪阶段输出的RDDs。然后，它找到运行作业的最小调度，并将相关的操作符划分为任务阶段。基于输入数据的分区，一个阶段包括多个任务。然后，操作符与DAG调度程序一起进行流水线处理。实际上，可以在单个阶段中安排多个map或reduce操作符（例如）。
- en: '![](img/00191.jpeg)**Figure 5:** Executing action leads to new ResultStage
    and ActiveJob in DAGScheduler'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00191.jpeg)**图5：**执行操作导致DAGScheduler中的新ResultStage和ActiveJob'
- en: Two fundamental concepts in DAG scheduler are jobs and stages. Thus, it has
    to track them through internal registries and counters. Technically speaking,
    DAG scheduler is a part of SparkContext's initialization that works exclusively
    on the driver (immediately after the task scheduler and scheduler backend are
    ready). DAG scheduler is responsible for three major tasks in Spark execution.
    It computes an execution DAG, that is, DAG of stages, for a job. It determines
    the preferred node to run each task on and handles failures due to shuffle output
    files being lost.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: DAG调度程序中的两个基本概念是作业和阶段。因此，它必须通过内部注册表和计数器来跟踪它们。从技术上讲，DAG调度程序是SparkContext初始化的一部分，它专门在驱动程序上工作（在任务调度程序和调度程序后端准备就绪后立即进行）。DAG调度程序在Spark执行中负责三项主要任务。它计算作业的执行DAG，即阶段的DAG。它确定每个任务运行的首选节点，并处理由于丢失洗牌输出文件而导致的故障。
- en: '![](img/00193.jpeg)**Figure 6:** DAGScheduler as created by SparkContext with
    other services'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00193.jpeg)**图6：** 由SparkContext创建的DAGScheduler与其他服务'
- en: The final result of a DAG scheduler is a set of stages. Therefore, most of the
    statistics and the status of the job can be seen using this visualization, for
    example, execution plan, status, number of completed stages, and the job ID of
    that particular job.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: DAG调度程序的最终结果是一组阶段。因此，大部分统计信息和作业的状态可以使用此可视化来查看，例如执行计划、状态、已完成阶段的数量以及该特定作业的作业ID。
- en: Stages
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阶段
- en: 'The Stages tab in Spark UI shows the current status of all stages of all jobs
    in a Spark application, including two optional pages for the tasks and statistics
    for a stage and pool details. Note that this information is available only when
    the application works in a fair scheduling mode. You should be able to access
    the Stages tab at `http://localhost:4040/stages`. Note that when there are no
    jobs submitted, the tab shows nothing but the title. The Stages tab shows the
    stages in a Spark application. The following stages can be seen in this tab:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Spark UI中的阶段选项卡显示了Spark应用程序中所有阶段的当前状态，包括任务和阶段的统计信息以及池详细信息的两个可选页面。请注意，此信息仅在应用程序以公平调度模式运行时才可用。您应该能够在`http://localhost:4040/stages`上访问阶段选项卡。请注意，当没有提交作业时，该选项卡除了标题外什么也不显示。阶段选项卡显示了Spark应用程序中的阶段。该选项卡中可以看到以下阶段：
- en: Active Stages
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 活动阶段
- en: Pending Stages
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 待处理的阶段
- en: Completed Stages
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已完成的阶段
- en: 'For example, when you submit a Spark job locally, you should be able to see
    the following status:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当您在本地提交一个Spark作业时，您应该能够看到以下状态：
- en: '![](img/00265.jpeg)**Figure 7:** The stages for all jobs in the Spark web UI'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00265.jpeg)**图7：** Spark Web UI中所有作业的阶段'
- en: In this case, there's only one stage that is an active stage. However, in the
    upcoming chapters, we will be able to observe other stages when we will submit
    our Spark jobs to AWS EC2 clusters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，只有一个处于活动状态的阶段。然而，在接下来的章节中，当我们将Spark作业提交到AWS EC2集群时，我们将能够观察到其他阶段。
- en: 'To further dig down to the summary of the completed jobs, click on any link
    contained in the Description column and you should find the related statistics
    on execution time as metrics. An approximate time of min, median, 25th percentile,
    75th percentile, and max for the metrics can also be seen in the following figure:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步了解已完成作业的摘要，请单击描述列中包含的任何链接，您应该能够找到与执行时间相关的统计信息。在以下图中还可以看到指标的最小值、中位数、25th百分位数、75th百分位数和最大值：
- en: '![](img/00378.jpeg)**Figure 8:** The summary for completed jobs on the Spark
    web UI'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00378.jpeg)**图8：** Spark Web UI上已完成作业的摘要'
- en: Your case might be different as I have executed and submitted only two jobs
    for demonstration purposes during the writing of this book. You can see other
    statistics on the executors as well. For my case, I submitted these jobs in the
    standalone mode by utilizing 8 cores and 32 GB of RAM. In addition to these, information
    related to the executor, such as ID, IP address with the associated port number,
    task completion time, number of tasks (including number of failed tasks, killed
    tasks, and succeeded tasks), and input size of the dataset per records are shown.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您的情况可能不同，因为在撰写本书期间，我只执行和提交了两个作业以进行演示。您还可以查看有关执行程序的其他统计信息。对于我的情况，我使用8个核心和32GB的RAM在独立模式下提交了这些作业。此外，还显示了与执行程序相关的信息，例如ID、关联端口号的IP地址、任务完成时间、任务数量（包括失败任务、被杀任务和成功任务的数量）以及数据集每条记录的输入大小。
- en: The other section in the image shows other information related to these two
    tasks, for example, index, ID, attempts, status, locality level, host information,
    launch time, duration, **Garbage Collection** (**GC**) time, and so on.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的另一部分显示了与这两个任务相关的其他信息，例如索引、ID、尝试次数、状态、本地级别、主机信息、启动时间、持续时间，垃圾收集（GC）时间等。
- en: Storage
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储
- en: 'The Storage tab shows the size and memory use for each RDD, DataFrame, or Dataset.
    You should be able to see the storage-related information of RDDs, DataFrames,
    or Datasets. The following figure shows storage metadata such as RDD name, storage
    level, the number of cache partitions, the percentage of a fraction of the data
    that was cached, and the size of the RDD in the main memory:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 存储选项卡显示了每个RDD、DataFrame或Dataset的大小和内存使用情况。您应该能够看到RDD、DataFrame或Dataset的存储相关信息。以下图显示了存储元数据，如RDD名称、存储级别、缓存分区的数量、缓存的数据比例的百分比以及RDD在主内存中的大小：
- en: '![](img/00229.jpeg)**Figure 9:** Storage tab shows space consumed by an RDD
    in disk'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00229.jpeg)**图9：** 存储选项卡显示RDD在磁盘中消耗的空间'
- en: Note that if the RDD cannot be cached in the main memory, disk space will be
    used instead. A more detailed discussion will be carried out in a later section
    of this chapter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果RDD无法缓存在主内存中，则将使用磁盘空间。本章的后续部分将进行更详细的讨论。
- en: '![](img/00067.jpeg)**Figure 10:** Data distribution and the storage used by
    the RDD in disk'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00067.jpeg)**图10：** 数据分布和RDD在磁盘中使用的存储'
- en: Environment
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境
- en: The Environment tab shows the environmental variables that are currently set
    on your machine (that is, driver). More specifically, runtime information such
    as Java Home, Java Version, and Scala Version can be seen under Runtime Information.
    Spark properties such as Spark application ID, app name, and driver host information,
    driver port, executor ID, master URL, and the schedule mode can be seen. Furthermore,
    other system-related properties and job properties such as AWT toolkit version,
    file encoding type (for example, UTF-8), and file encoding package information
    (for example, sun.io) can be seen under System Properties.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 环境选项卡显示了当前设置在您的机器（即驱动程序）上的环境变量。更具体地说，可以在运行时信息下看到Java Home、Java Version和Scala
    Version等运行时信息。还可以看到Spark属性，如Spark应用程序ID、应用程序名称、驱动程序主机信息、驱动程序端口、执行程序ID、主URL和调度模式。此外，还可以在系统属性下看到其他与系统相关的属性和作业属性，例如AWT工具包版本、文件编码类型（例如UTF-8）和文件编码包信息（例如sun.io）。
- en: '![](img/00249.jpeg)**Figure 11:** Environment tab on Spark web UI'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00249.jpeg)**图11：** Spark Web UI上的环境选项卡'
- en: Executors
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行程序
- en: 'The Executors tab uses `ExecutorsListener` to collect information about executors
    for a Spark application. An executor is a distributed agent that is responsible
    for executing tasks. Executors are instantiated in different ways. For example,
    they can be instantiated when `CoarseGrainedExecutorBackend` receives `RegisteredExecutor`
    message for Spark Standalone and YARN. The second case is when a Spark job is
    submitted to Mesos. The Mesos''s `MesosExecutorBackend` gets registered. The third
    case is when you run your Spark jobs locally, that is, `LocalEndpoint` is created.
    An executor typically runs for the entire lifetime of a Spark application, which
    is called static allocation of executors, although you can also opt in for dynamic
    allocation. The executor backends exclusively manage all the executors in a computing
    node or clusters. An executor reports heartbeat and partial metrics for active
    tasks to the **HeartbeatReceiver** RPC endpoint on the driver periodically and
    the results are sent to the driver. They also provide in-memory storage for RDDs
    that are cached by user programs through block manager. Refer to the following
    figure for a clearer idea on this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器选项卡使用`ExecutorsListener`收集有关Spark应用程序的执行器信息。执行器是负责执行任务的分布式代理。执行器以不同的方式实例化。例如，当`CoarseGrainedExecutorBackend`接收到Spark
    Standalone和YARN的`RegisteredExecutor`消息时，它们可以被实例化。第二种情况是当Spark作业提交到Mesos时。Mesos的`MesosExecutorBackend`会被注册。第三种情况是当您在本地运行Spark作业时，即创建`LocalEndpoint`。执行器通常在Spark应用程序的整个生命周期内运行，这称为执行器的静态分配，尽管您也可以选择动态分配。执行器后端专门管理计算节点或集群中的所有执行器。执行器定期向驱动程序的**HeartbeatReceiver**
    RPC端点报告活动任务的心跳和部分指标，并将结果发送给驱动程序。它们还通过块管理器为用户程序缓存的RDD提供内存存储。有关此内容的更清晰的想法，请参考以下图：
- en: '![](img/00071.jpeg)**Figure 12:** Spark driver instantiates an executor that
    is responsible for HeartbeatReceiver''s Heartbeat message handler'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：Spark驱动程序实例化一个执行器，负责处理HeartbeatReceiver的心跳消息处理程序
- en: 'When an executor starts, it first registers with the driver and communicates
    directly to execute tasks, as shown in the following figure:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行器启动时，它首先向驱动程序注册，并直接通信以执行任务，如下图所示：
- en: '![](img/00009.jpeg)**Figure 13:** Launching tasks on executor using TaskRunners'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：使用TaskRunners在执行器上启动任务
- en: You should be able to access the Executors tab at `http://localhost:4040/executors`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该能够在`http://localhost:4040/executors`访问执行器选项卡。
- en: '![](img/00084.jpeg)**Figure 14:** Executor tab on Spark web UI'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：Spark Web UI上的执行器选项卡
- en: As shown in the preceding figure, Executor ID, Address, Status, RDD Blocks,
    Storage Memory, Disk Used, Cores, Active Tasks, Failed Tasks, Complete Tasks,
    Total Tasks, Task Time (GC Time), Input, Shuffle Read, Shuffle Write, and Thread
    Dump about the executor can be seen.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，可以看到有关执行器的执行器ID、地址、状态、RDD块、存储内存、已使用磁盘、核心、活动任务、失败任务、完成任务、总任务、任务时间（GC时间）、输入、Shuffle读取、Shuffle写入以及线程转储的信息。
- en: SQL
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL
- en: The SQL tab in the Spark UI displays all the accumulator values per operator.
    You should be able to access the SQL tab at `http://localhost:4040/SQL/`. It displays
    all the SQL query executions and underlying information by default. However, the
    SQL tab displays the details of the SQL query execution only after a query has
    been selected.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Spark UI中的SQL选项卡显示每个操作符的所有累加器值。您应该能够在`http://localhost:4040/SQL/`访问SQL选项卡。它默认显示所有SQL查询执行和底层信息。但是，只有在选择查询后，SQL选项卡才会显示SQL查询执行的详细信息。
- en: A detailed discussion on SQL is out of the scope of this chapter. Interested
    readers should refer to [http://spark.apache.org/docs/latest/sql-programming-guide.html#sql](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)
    for more on how to submit an SQL query and see its result output.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SQL的详细讨论超出了本章的范围。感兴趣的读者应参考[http://spark.apache.org/docs/latest/sql-programming-guide.html#sql](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)了解如何提交SQL查询并查看其结果输出。
- en: Visualizing Spark application using web UI
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Web UI可视化Spark应用程序
- en: 'When a Spark job is submitted for execution, a web application UI is launched
    that displays useful information about the application. An event timeline displays
    the relative ordering and interleaving of application events. The timeline view
    is available on three levels: across all jobs, within one job, and within one
    stage. The timeline also shows executor allocation and deallocation.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当提交Spark作业进行执行时，将启动一个Web应用程序UI，显示有关应用程序的有用信息。事件时间轴显示应用程序事件的相对顺序和交错。时间轴视图可在三个级别上使用：跨所有作业、在一个作业内以及在一个阶段内。时间轴还显示执行器的分配和释放。
- en: '![](img/00325.jpeg)**Figure 15:** Spark jobs executed as DAG on Spark web UI'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：Spark作业在Spark Web UI上以DAG形式执行
- en: Observing the running and completed Spark jobs
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察正在运行和已完成的Spark作业
- en: To access and observe the running and the completed Spark jobs, open `http://spark_driver_host:4040`
    in a web browser. Note that you will have to replace `spark_driver_host` with
    an IP address or hostname accordingly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问和观察正在运行和已完成的Spark作业，请在Web浏览器中打开`http://spark_driver_host:4040`。请注意，您将需要相应地用IP地址或主机名替换`spark_driver_host`。
- en: Note that if multiple SparkContexts are running on the same host, they will
    bind to successive ports beginning with 4040, 4041, 4042, and so on. By default,
    this information will be available for the duration of your Spark application
    only. This means that when your Spark job finishes its execution, the binding
    will no longer be valid or accessible.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果同一主机上运行多个SparkContexts，它们将绑定到从4040开始的连续端口，4040、4041、4042等。默认情况下，此信息仅在您的Spark应用程序运行期间可用。这意味着当您的Spark作业完成执行时，绑定将不再有效或可访问。
- en: Now, to access the active jobs that are still executing, click on the Active
    Jobs link and you will see the related information of those jobs. On the other
    hand, to access the status of the completed jobs, click on Completed Jobs and
    you will see the information as DAG style as discussed in the preceding section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要访问仍在执行的活动作业，请单击Active Jobs链接，您将看到这些作业的相关信息。另一方面，要访问已完成作业的状态，请单击Completed
    Jobs，您将看到信息以DAG样式显示，如前一节所述。
- en: '![](img/00129.jpeg)**Figure 16:** Observing the running and completed Spark
    jobs'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00129.jpeg)**图16：**观察正在运行和已完成的Spark作业'
- en: You can achieve these by clicking on the job description link under the Active
    Jobs or Completed Jobs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过单击Active Jobs或Completed Jobs下的作业描述链接来实现这些。
- en: Debugging Spark applications using logs
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用日志调试Spark应用程序
- en: 'Seeing the information about all running Spark applications depends on which
    cluster manager you are using. You should follow these instructions while debugging
    your Spark application:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 查看所有正在运行的Spark应用程序的信息取决于您使用的集群管理器。在调试Spark应用程序时，应遵循这些说明：
- en: '**Spark Standalone**: Go to the Spark master UI at `http://master:18080`. The
    master and each worker show cluster and the related job statistics. In addition,
    a detailed log output for each job is also written to the working directory of
    each worker. We will discuss how to enable the logging manually using the `log4j`
    with Spark.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Standalone**：转到`http://master:18080`上的Spark主UI。主节点和每个工作节点显示集群和相关作业统计信息。此外，每个作业的详细日志输出也写入到每个工作节点的工作目录中。我们将讨论如何使用`log4j`手动启用日志记录。'
- en: '**YARN**: If your cluster manager is YARN, and suppose that you are running
    your Spark jobs on the Cloudera (or any other YARN-based platform), then go to
    the YARN applications page in the Cloudera Manager Admin Console. Now, to debug
    Spark applications running on YARN, view the logs for the Node Manager role. To
    make this happen, open the log event viewer and then filter the event stream to
    choose a time window and log level and to display the Node Manager source. You
    can access logs through the command as well. The format of the command is as follows:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YARN**：如果您的集群管理器是YARN，并且假设您正在Cloudera（或任何其他基于YARN的平台）上运行Spark作业，则转到Cloudera
    Manager管理控制台中的YARN应用程序页面。现在，要调试在YARN上运行的Spark应用程序，请查看Node Manager角色的日志。要实现这一点，打开日志事件查看器，然后过滤事件流以选择时间窗口和日志级别，并显示Node
    Manager源。您也可以通过命令访问日志。命令的格式如下：'
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For example, the following are the valid commands for these IDs:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下是这些ID的有效命令：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that the user IDs are different. However, this is only true if `yarn.log-aggregation-enable`
    is true in `yarn-site.xml` and the application has already finished the execution.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，用户ID是不同的。但是，只有在`yarn-site.xml`中的`yarn.log-aggregation-enable`为true并且应用程序已经完成执行时，才是真的。
- en: Logging with log4j with Spark
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用log4j记录Spark
- en: Spark uses `log4j` for its own logging. All the operations that happen backend
    get logged to the Spark shell console (which is already configured to the underlying
    storage). Spark provides a template of `log4j` as a property file, and we can
    extend and modify that file for logging in Spark. Move to the `SPARK_HOME/conf`
    directory and you should see the `log4j.properties.template` file. This could
    help us as the starting point for our own logging system.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Spark使用`log4j`进行自身的日志记录。发生在后端的所有操作都会记录到Spark shell控制台（已配置为基础存储）。Spark提供了`log4j`的属性文件模板，我们可以扩展和修改该文件以记录Spark中的日志。转到`SPARK_HOME/conf`目录，您应该看到`log4j.properties.template`文件。这可以作为我们自己日志系统的起点。
- en: 'Now, let''s create our own custom logging system while running a Spark job.
    When you are done, rename the file as `log4j.properties` and put it under the
    same directory (that is, project tree). A sample snapshot of the file can be seen
    as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在运行Spark作业时创建自己的自定义日志系统。完成后，将文件重命名为`log4j.properties`并将其放在相同的目录（即项目树）下。文件的示例快照如下：
- en: '![](img/00112.jpeg)**Figure 17:** A snap of the log4j.properties file'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00112.jpeg)**图17：**log4j.properties文件的快照'
- en: 'By default, everything goes to console and file. However, if you want to bypass
    all the noiser logs to a system file located at, say, `/var/log/sparkU.log`, then
    you can set these properties in the `log4j.properties` file as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有内容都会输出到控制台和文件。但是，如果您想将所有噪音日志绕过并记录到系统文件中，例如`/var/log/sparkU.log`，则可以在`log4j.properties`文件中设置这些属性如下：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Basically, we want to hide all logs Spark generates so that we don't have to
    deal with them in the shell. We redirect them to be logged in the filesystem.
    On the other hand, we want our own logs to be logged in the shell and a separate
    file so that they don't get mixed up with the ones from Spark. From here, we will
    point Splunk to the files where our own logs are, which in this particular case
    is `/var/log/sparkU.log`*.*
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们希望隐藏Spark生成的所有日志，以便我们不必在shell中处理它们。我们将它们重定向到文件系统中进行记录。另一方面，我们希望我们自己的日志记录在shell和单独的文件中进行记录，以便它们不会与Spark的日志混在一起。从这里，我们将指向Splunk的文件，其中我们自己的日志记录，特别是`/var/log/sparkU.log`*.*
- en: Then the`log4j.properties` file is picked up by Spark when the application starts,
    so we don't have to do anything aside from placing it in the mentioned location.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当应用程序启动时，Spark会读取`log4j.properties`文件，因此我们除了将其放在指定位置外，无需进行其他操作。
- en: 'Now let''s see how we can create our own logging system. Look at the following
    code and try to understand what is happening here:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何创建我们自己的日志记录系统。看看以下代码，并尝试理解这里发生了什么：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding code conceptually logs only the warning message. It first prints
    the warning message and then creates an RDD by parallelizing numbers from 1 to
    100,000\. Once the RDD job is finished, it prints another warning log. However,
    there is a problem we haven't noticed yet with the earlier code segment.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码概念上仅记录警告消息。它首先打印警告消息，然后通过并行化从1到100,000的数字创建RDD。一旦RDD作业完成，它会打印另一个警告日志。但是，我们尚未注意到先前代码段中的问题。
- en: 'One drawback of the `org.apache.log4j.Logger` class is that it is not serializable
    (refer to the optimization technique section for more details), which implies
    that we cannot use it inside a *closure* while doing operations on some parts
    of the Spark API. For example, if you try to execute the following code, you should
    experience an exception that says Task not serializable:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`org.apache.log4j.Logger`类的一个缺点是它不可序列化（有关更多详细信息，请参阅优化技术部分），这意味着我们不能在对Spark
    API的某些部分进行操作时在*闭包*内使用它。例如，如果尝试执行以下代码，则应该会遇到一个说任务不可序列化的异常：'
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To solve this problem is also easy; just declare the Scala object with `extends
    Serializable` and now the code looks like the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题也很容易；只需声明带有`extends Serializable`的Scala对象，现在代码看起来如下：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So what is happening in the preceding code is that the closure can't be neatly
    distributed to all partitions since it can't close on the logger; hence, the whole
    instance of type `MyMapper` is distributed to all partitions; once this is done,
    each partition creates a new logger and uses it for logging.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中发生的情况是，闭包无法整洁地分布到所有分区，因为它无法关闭记录器；因此，类型为`MyMapper`的整个实例分布到所有分区；一旦完成此操作，每个分区都会创建一个新的记录器并用于记录。
- en: 'In summary, the following is the complete code that helps us to get rid of
    this problem:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，以下是帮助我们摆脱这个问题的完整代码：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will discuss the built-in logging of Spark in the next section.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节讨论Spark的内置日志记录。
- en: Spark configuration
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark配置
- en: 'There are a number of ways to configure your Spark jobs. In this section, we
    will discuss these ways. More specifically, according to Spark 2.x release, there
    are three locations to configure the system:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以配置您的Spark作业。在本节中，我们将讨论这些方法。更具体地说，根据Spark 2.x版本，有三个位置可以配置系统：
- en: Spark properties
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark属性
- en: Environmental variables
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境变量
- en: Logging
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录
- en: Spark properties
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark属性
- en: 'As discussed previously, Spark properties control most of the application-specific
    parameters and can be set using a `SparkConf` object of Spark. Alternatively,
    these parameters can be set through the Java system properties. `SparkConf` allows
    you to configure some of the common properties as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Spark属性控制大部分应用程序特定的参数，并且可以使用Spark的`SparkConf`对象进行设置。或者，这些参数可以通过Java系统属性进行设置。`SparkConf`允许您配置一些常见属性，如下所示：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'An application can be configured to use a number of available cores on your
    machine. For example, we could initialize an application with two threads as follows.
    Note that we run with `local [2]`, meaning two threads, which represents minimal
    parallelism and using `local [*]`, which utilizes all the available cores in your
    machine. Alternatively, you can specify the number of executors while submitting
    Spark jobs with the following spark-submit script:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以配置为使用计算机上的多个可用核心。例如，我们可以初始化一个具有两个线程的应用程序如下。请注意，我们使用`local [2]`运行，表示两个线程，这代表最小的并行性，并使用`local
    [*]`，它利用计算机上所有可用的核心。或者，您可以在提交Spark作业时使用以下spark-submit脚本指定执行程序的数量：
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There might be some special cases where you need to load Spark properties dynamically
    when required. You can do this while submitting a Spark job through the spark-submit
    script. More specifically, you may want to avoid hardcoding certain configurations
    in `SparkConf`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会有一些特殊情况，您需要在需要时动态加载Spark属性。您可以在通过spark-submit脚本提交Spark作业时执行此操作。更具体地说，您可能希望避免在`SparkConf`中硬编码某些配置。
- en: 'Apache Spark precedence:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark优先级：
- en: 'Spark has the following precedence on the submitted jobs: configs coming from
    a config file have the lowest priority. The configs coming from the actual code
    have higher priority with respect to configs coming from a config file, and configs
    coming from the CLI through the Spark-submit script have higher priority.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Spark对提交的作业具有以下优先级：来自配置文件的配置具有最低优先级。来自实际代码的配置相对于来自配置文件的配置具有更高的优先级，而通过Spark-submit脚本通过CLI传递的配置具有更高的优先级。
- en: 'For instance, if you want to run your application with different masters, executors,
    or different amounts of memory, Spark allows you to simply create an empty configuration
    object, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果要使用不同的主节点、执行程序或不同数量的内存运行应用程序，Spark允许您简单地创建一个空配置对象，如下所示：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then you can provide the configuration for your Spark job at runtime as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以在运行时为您的Spark作业提供配置，如下所示：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`SPARK_HOME/bin/spark-submit` will also read configuration options from `SPARK_HOME
    /conf/spark-defaults.conf`, in which each line consists of a key and a value separated
    by whitespace. An example is as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`SPARK_HOME/bin/spark-submit`还将从`SPARK_HOME /conf/spark-defaults.conf`中读取配置选项，其中每行由空格分隔的键和值组成。示例如下：'
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Values that are specified as flags in the properties file will be passed to
    the application and merged with those ones specified through `SparkConf`. Finally,
    as discussed earlier, the application web UI at `http://<driver>:4040` lists all
    the Spark properties under the Environment tab.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在属性文件中指定为标志的值将传递给应用程序，并与通过`SparkConf`指定的值合并。最后，如前所述，应用程序Web UI在`http://<driver>:4040`下的环境选项卡下列出所有Spark属性。
- en: Environmental variables
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境变量
- en: 'Environment variables can be used to set the setting in the computing nodes
    or machine settings. For example, IP address can be set through the `conf/spark-env.sh`
    script on each computing node. The following table lists the name and the functionality
    of the environmental variables that need to be set:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 环境变量可用于设置计算节点或机器设置中的设置。例如，IP地址可以通过每个计算节点上的`conf/spark-env.sh`脚本进行设置。以下表列出了需要设置的环境变量的名称和功能：
- en: '![](img/00116.gif)**Figure 18:** Environmental variables and their meaning'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00116.gif)**图18：**环境变量及其含义'
- en: Logging
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录
- en: 'Finally, logging can be configured through the `log4j.properties` file under
    your Spark application tree, as discussed in the preceding section. Spark uses
    log4j for logging. There are several valid logging levels supported by log4j with
    Spark; they are as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以通过在Spark应用程序树下的`log4j.properties`文件中配置日志记录，如前一节所述。Spark使用log4j进行日志记录。log4j支持几个有效的日志记录级别，它们如下：
- en: '| **Log Level** | **Usages** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **日志级别** | **用途** |'
- en: '| OFF | This is the most specific, which allows no logging at all |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| OFF | 这是最具体的，完全不允许记录日志 |'
- en: '| FATAL | This is the most specific one that shows fatal errors with little
    data |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| FATAL | 这是最具体的，显示了有很少数据的严重错误 |'
- en: '| ERROR | This shows only the general errors |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| ERROR | 这只显示一般错误 |'
- en: '| WARN | This shows warnings that are recommended to be fixed but not mandatory
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| WARN | 这显示了建议修复但不是强制的警告 |'
- en: '| INFO | This shows information required for your Spark job |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| INFO | 这显示了您的Spark作业所需的信息 |'
- en: '| DEBUG | While debugging, those logs will be printed |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| DEBUG | 在调试时，这些日志将被打印 |'
- en: '| TRACE | This provides the least specific error trace with a lot of data |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| TRACE | 这提供了具有大量数据的最不具体的错误跟踪 |'
- en: '| ALL | Least specific message with all data |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| ALL | 具有所有数据的最不具体的消息 |'
- en: '**Table 1:** Log level with log4j and Spark'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**表1：** 使用log4j和Spark的日志级别'
- en: 'You can set up the default logging for Spark shell in `conf/log4j.properties`.
    In standalone Spark applications or while in a Spark Shell session, use `conf/log4j.properties.template`
    as a starting point. In an earlier section of this chapter, we suggested you put
    the `log4j.properties` file under your project directory while working on an IDE-based
    environment like Eclipse. However, to disable logging completely, you should use
    the following `conf/log4j.properties.template` as `log4j.properties` . Just set
    the `log4j.logger.org` flags as OFF, as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在`conf/log4j.properties`中设置Spark shell的默认日志记录。在独立的Spark应用程序中或在Spark Shell会话中，可以使用`conf/log4j.properties.template`作为起点。在本章的前一节中，我们建议您在像Eclipse这样的基于IDE的环境中将`log4j.properties`文件放在项目目录下。但是，要完全禁用日志记录，您应该将以下`conf/log4j.properties.template`设置为`log4j.properties`。只需将`log4j.logger.org`标志设置为OFF，如下所示：
- en: '[PRE16]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the next section, we will discuss some common mistakes made by the developer
    or programmer while developing and submitting Spark jobs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论开发和提交Spark作业时开发人员或程序员常犯的一些常见错误。
- en: Common mistakes in Spark app development
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark应用程序开发中的常见错误
- en: Common mistakes that happen often are application failure, a slow job that gets
    stuck due to numerous factors, mistakes in the aggregation, actions or transformations,
    an exception in the main thread and, of course, **Out Of Memory** (**OOM**).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 经常发生的常见错误包括应用程序失败、由于多种因素而卡住的作业运行缓慢、聚合操作中的错误、动作或转换中的错误、主线程中的异常，当然还有**内存不足**（**OOM**）。
- en: Application failure
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序失败
- en: 'Most of the time, application failure happens because one or more stages fail
    eventually. As discussed earlier in this chapter, Spark jobs comprise several
    stages. Stages aren''t executed independently: for instance, a processing stage
    can''t take place before the relevant input-reading stage. So, suppose that stage
    1 executes successfully but stage 2 fails to execute, the whole application fails
    eventually. This can be shown as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，应用程序失败是因为一个或多个阶段最终失败。如本章前面所述，Spark作业包括多个阶段。阶段不是独立执行的：例如，处理阶段无法在相关的输入读取阶段之前发生。因此，假设阶段1成功执行，但阶段2无法执行，整个应用程序最终将失败。可以如下所示：
- en: '![](img/00119.jpeg)**Figure 19:** Two stages in a typical Spark job'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00119.jpeg)**图19：** 典型Spark作业中的两个阶段'
- en: 'To show an example, suppose you have the following three RDD operations as
    stages. The same can be visualized as shown in *Figure 20*, *Figure 21*, and *Figure
    22*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设您有以下三个RDD操作作为阶段。可以将其可视化为*图20*、*图21*和*图22*所示：
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00157.jpeg)**Figure 20:** Stage 1 for rdd1'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00157.jpeg)**图20：** rdd1的第1阶段'
- en: '[PRE18]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Conceptually, this can be shown in *Figure 21*, which first parses the data
    using the `hadoopFile()` method, groups it using the `groupByKey()` method, and
    finally, maps it:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，可以如*图21*所示，首先使用`hadoopFile()`方法解析数据，然后使用`groupByKey()`方法对其进行分组，最后对其进行映射：
- en: '![](img/00090.jpeg)**Figure 21:** Stage 2 for rdd2'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00090.jpeg)**图21：** rdd2的第2阶段'
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Conceptually, this can be shown in *Figure 22*, which first parses the data,
    joins it, and finally, maps it:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，可以如*图22*所示，首先解析数据，然后将其连接，最后映射：
- en: '![](img/00144.jpeg)**Figure 22:** Stage 3 for rdd3'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00144.jpeg)**图22：** rdd3的第3阶段'
- en: 'Now you can perform an aggregation function, for example, collect, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以执行聚合函数，例如collect，如下所示：
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Well! You have developed a Spark job consisting of three stages. Conceptually,
    this can be shown as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 噢！您已经开发了一个包含三个阶段的Spark作业。从概念上讲，可以如下所示：
- en: '![](img/00041.jpeg)**Figure 23:** three stages for the rdd3.collect() operation'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00041.jpeg)**图23：** rdd3.collect()操作的三个阶段'
- en: 'Now, if one of the stages fails, your job will fail eventually. As a result,
    the final `rdd3.collect()` statement will throw an exception about stage failure.
    Moreover, you may have issues with the following four factors:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果其中一个阶段失败，您的作业最终将失败。因此，最终的`rdd3.collect()`语句将抛出有关阶段失败的异常。此外，您可能会遇到以下四个因素的问题：
- en: Mistakes in the aggregation operation
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合操作中的错误
- en: Exceptions in the main thread
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主线程中的异常
- en: OOP
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OOP
- en: Class not found exception while submitting jobs using the `spark-submit` script
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`spark-submit`脚本提交作业时出现类找不到异常
- en: Misconception about some API/methods in Spark core library
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark核心库中一些API/方法的误解
- en: 'To get rid of the aforementioned issues, our general suggestion is to ensure
    that you have not made any mistakes while performing any map, flatMap, or aggregate
    operations. Second, ensure that there are no flaws in the main method while developing
    your application with Java or Scala. Sometimes you don''t see any syntax error
    in your code, but it''s important that you have developed some small test cases
    for your application. Most common exceptions that occur in the main method are
    as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了摆脱上述问题，我们的一般建议是确保在执行任何map、flatMap或aggregate操作时没有犯任何错误。其次，在使用Java或Scala开发应用程序的主方法中确保没有缺陷。有时您在代码中看不到任何语法错误，但重要的是您为应用程序开发了一些小的测试用例。主方法中最常见的异常如下：
- en: '`java.lang.noclassdeffounderror`'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`java.lang.noclassdeffounderror`'
- en: '`java.lang.nullpointerexception`'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`java.lang.nullpointerexception`'
- en: '`java.lang.arrayindexoutofboundsexception`'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`java.lang.arrayindexoutofboundsexception`'
- en: '`java.lang.stackoverflowerror`'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`java.lang.stackoverflowerror`'
- en: '`java.lang.classnotfoundexception`'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`java.lang.classnotfoundexception`'
- en: '`java.util.inputmismatchexception`'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`java.util.inputmismatchexception`'
- en: These exceptions can be avoided with the careful coding of your Spark application.
    Alternatively, use Eclipse's (or any other IDEs) code debugging features extensively
    to get rid of the semantic error to avoid the exception. For the third problem,
    that is, OOM, it's a very common problem. It is to be noted that Spark requires
    at least 8 GB of main memory, with sufficient disk space available for the standalone
    mode. On the other hand, to get the full cluster computing facilities, this requirement
    is often high.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通过谨慎编写Spark应用程序可以避免这些异常。或者，广泛使用Eclipse（或任何其他IDE）的代码调试功能来消除语义错误以避免异常。对于第三个问题，即OOM，这是一个非常常见的问题。需要注意的是，Spark至少需要8GB的主内存，并且独立模式下需要足够的磁盘空间。另一方面，为了获得完整的集群计算功能，这个要求通常很高。
- en: Preparing a JAR file including all the dependencies to execute Spark jobs is
    of paramount importance. Many practitioners use Google's Guava; it is included
    in most distributions, yet it doesn't guarantee backward compatibility. This means
    that sometimes your Spark job won't find a Guava class even if you explicitly
    provided it; this happens because one of the two versions of the Guava libraries
    takes precedence over the other, and this version might not include a required
    class. In order to overcome this issue, you usually resort to shading.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 准备一个包含所有依赖项的JAR文件来执行Spark作业非常重要。许多从业者使用谷歌的Guava；它包含在大多数发行版中，但不能保证向后兼容。这意味着有时即使您明确提供了Guava类，您的Spark作业也找不到Guava类；这是因为Guava库的两个版本中的一个优先于另一个，而这个版本可能不包括所需的类。为了解决这个问题，通常会使用shading。
- en: 'Make sure that you have set the Java heap space with –Xmx parameter with a
    sufficiently large value if you''re coding using IntelliJ, Vim, Eclipse, Notepad,
    and so on. While working with cluster mode, you should specify the executor memory
    while submitting Spark jobs using the Spark-submit script. Suppose you have a
    CSV to be parsed and do some predictive analytics using a random forest classifier,
    you might need to specify the right amount of memory, say 20 GB, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 确保如果您使用IntelliJ、Vim、Eclipse、记事本等编码，已使用-Xmx参数设置了Java堆空间，并设置了足够大的值。在集群模式下工作时，应在使用Spark-submit脚本提交Spark作业时指定执行器内存。假设您有一个要解析的CSV文件，并使用随机森林分类器进行一些预测分析，您可能需要指定正确的内存量，比如20GB，如下所示：
- en: '[PRE21]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Even if you receive the OOM error, you can increase this amount to, say, 32
    GB or more. Since random forest is computationally intensive, requiring larger
    memory, this is just an example of random forest. You might experience similar
    issues while just parsing your data. Even a particular stage may fail due to this
    OOM error. Therefore, make sure that you are aware of this error.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 即使收到OOM错误，您也可以将此金额增加到32GB或更多。由于随机森林计算密集，需要更大的内存，这只是随机森林的一个例子。您可能在仅解析数据时遇到类似的问题。甚至由于此OOM错误，特定阶段可能会失败。因此，请确保您知道这个错误。
- en: For the `class not found exception`, make sure that you have included your main
    class in the resulting JAR file. The JAR file should be prepared with all the
    dependencies to execute your Spark job on the cluster nodes. We will provide a
    step-by-step JAR preparation guideline in [Chapter 17](part0511.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c),
    *Time to Go to ClusterLand - Deploying Spark on a Cluster.*
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`class not found exception`，请确保您已经在生成的JAR文件中包含了主类。JAR文件应该准备好包含所有依赖项，以便在集群节点上执行您的Spark作业。我们将在[第17章](part0511.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c)中提供一份逐步的JAR准备指南，*时候去集群
    - 在集群上部署Spark*。
- en: For the last issue, we can provide some examples of some misconceptions about
    Spark Core library. For example, when you use the `wholeTextFiles` method to prepare
    RDDs or DataFrames from multiple files, Spark does not run in parallel; in cluster
    mode for YARN, it may run out of memory sometimes.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最后一个问题，我们可以提供一些关于Spark Core库的一些误解的例子。例如，当您使用`wholeTextFiles`方法从多个文件准备RDDs或DataFrames时，Spark不会并行运行；在YARN的集群模式下，有时可能会耗尽内存。
- en: 'Once, I experienced an issue where, at first, I copied six files in my S3 storage
    to HDFS. Then, I tried to create an RDD, as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有一次，我遇到了一个问题，首先我将六个文件从我的S3存储复制到HDFS。然后，我尝试创建一个RDD，如下所示：
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, I tried to process those files line by line using a UDF. When I looked
    at my computing nodes, I saw that only one executor was running per file. However,
    I then got an error message saying that YARN had run out of memory. Why so? The
    reasons are as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我尝试使用UDF逐行处理这些文件。当我查看我的计算节点时，我发现每个文件只有一个执行器在运行。然而，后来我收到了一条错误消息，说YARN已经耗尽了内存。为什么呢？原因如下：
- en: The goal of `wholeTextFiles` is to have only one executor for each file to be
    processed
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wholeTextFiles`的目标是每个要处理的文件只有一个执行器'
- en: If you use `.gz` files, for example, you will have only one executor per file,
    maximum
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用`.gz`文件，例如，您将每个文件只有一个执行器，最多
- en: Slow jobs or unresponsiveness
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 慢作业或无响应
- en: 'Sometimes, if the SparkContext cannot connect to a Spark standalone master,
    then the driver may display errors such as the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，如果SparkContext无法连接到Spark独立主节点，那么驱动程序可能会显示以下错误：
- en: '[PRE23]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: At other times, the driver is able to connect to the master node but the master
    is unable to communicate back to the driver. Then, multiple attempts to connect
    are made even though the driver will report that it could not connect to the Master's
    log directory.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他时候，驱动程序能够连接到主节点，但主节点无法与驱动程序进行通信。然后，尝试多次连接，即使驱动程序会报告无法连接到主节点的日志目录。
- en: 'Furthermore, you might often experience very slow performance and progress
    in your Spark jobs. This happens because your driver program is not that fast
    to compute your jobs. As discussed earlier, sometimes a particular stage may take
    a longer time than usual because there might be a shuffle, map, join, or aggregation
    operation involved. Even if the computer is running out of disk storage or main
    memory, you may experience these issues. For example, if your master node does
    not respond or you experience unresponsiveness from the computing nodes for a
    certain period of time, you might think that your Spark job has halted and become
    stagnant at a certain stage:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可能经常会在Spark作业中经历非常缓慢的性能和进展。这是因为您的驱动程序程序计算速度不够快。正如前面讨论的，有时特定阶段可能需要比平常更长的时间，因为可能涉及洗牌、映射、连接或聚合操作。即使计算机的磁盘存储或主内存用尽，您也可能会遇到这些问题。例如，如果您的主节点没有响应，或者在一定时间内计算节点出现不响应，您可能会认为您的Spark作业在某个阶段停滞不前。
- en: '![](img/00224.jpeg)**Figure 24:** An example log for executor/driver unresponsiveness'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00224.jpeg)**图24：**执行器/驱动程序不响应的示例日志'
- en: 'Potential solutions could be several, including the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的解决方案可能有几种，包括以下内容：
- en: 'Check to make sure that workers and drivers are correctly configured to connect
    to the Spark master on the exact address listed in the Spark master web UI/logs.
    Then, explicitly supply the Spark cluster''s master URL when starting your Spark
    shell:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请确保工作节点和驱动程序正确配置为连接到Spark主节点上的确切地址，该地址在Spark主节点web UI/logs中列出。然后，在启动Spark shell时明确提供Spark集群的主URL：
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Set `SPARK_LOCAL_IP` to a cluster-addressable hostname for the driver, master,
    and worker processes.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`SPARK_LOCAL_IP`设置为驱动程序、主节点和工作进程的集群可寻址主机名。
- en: Sometimes, we experience some issues due to hardware failure. For example, if
    the filesystem in a computing node closes unexpectedly, that is, an I/O exception,
    your Spark job will eventually fail too. This is obvious because your Spark job
    cannot write the resulting RDDs or data to store to the local filesystem or HDFS.
    This also implies that DAG operations cannot be performed due to the stage failures.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，由于硬件故障，我们会遇到一些问题。例如，如果计算节点中的文件系统意外关闭，即I/O异常，您的Spark作业最终也会失败。这是显而易见的，因为您的Spark作业无法将生成的RDD或数据写入本地文件系统或HDFS。这也意味着由于阶段失败，DAG操作无法执行。
- en: 'Sometimes, this I/O exception occurs due to an underlying disk failure or other
    hardware failures. This often provides logs, as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，这种I/O异常是由底层磁盘故障或其他硬件故障引起的。这通常会提供日志，如下所示：
- en: '![](img/00048.jpeg)**Figure 25:** An example filesystem closed'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00048.jpeg)**图25：**文件系统关闭示例'
- en: Nevertheless, you often experience slow job computing performance because your
    Java GC is somewhat busy with, or cannot do, the GC fast. For example, the following
    figure shows that for task 0, it took 10 hours to finish the GC! I experienced
    this issue in 2014, when I was new to Spark. Control of these types of issues,
    however, is not in our hands. Therefore, our recommendation is that you should
    make the JVM free and try submitting the jobs again.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您经常会遇到作业计算性能较慢的问题，因为您的Java GC有些忙碌，或者无法快速进行GC。例如，以下图显示了对于任务0，完成GC花了10个小时！我在2014年遇到了这个问题，当时我刚开始使用Spark。然而，这些问题的控制并不在我们手中。因此，我们的建议是您应该释放JVM并尝试重新提交作业。
- en: '![](img/00352.jpeg)**Figure 26:** An example where GC stalled in between'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00352.jpeg)**图26：**GC在中间停滞的示例'
- en: The fourth factor could be the slow response or slow job performance is due
    to the lack of data serialization. This will be discussed in the next section.
    The fifth factor could be the memory leak in the code that will tend to make your
    application consume more memory, leaving the files or logical devices open. Therefore,
    make sure that there is no option that tends to be a memory leak. For example,
    it is a good practice to finish your Spark application by calling `sc.stop()`
    or `spark.stop()`. This will make sure that one SparkContext is still open and
    active. Otherwise, you might get unwanted exceptions or issues. The sixth issue
    is that we often keep too many open files, and this sometimes creates `FileNotFoundException`
    in the shuffle or merge stage.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个因素可能是响应缓慢或作业性能较慢是由于数据序列化不足。这将在下一节中讨论。第五个因素可能是代码中的内存泄漏，这将导致应用程序消耗更多内存，使文件或逻辑设备保持打开状态。因此，请确保没有导致内存泄漏的选项。例如，通过调用`sc.stop()`或`spark.stop()`完成您的Spark应用程序是一个好习惯。这将确保一个SparkContext仍然是打开和活动的。否则，您可能会遇到不必要的异常或问题。第六个问题是我们经常保持太多的打开文件，有时会在洗牌或合并阶段中创建`FileNotFoundException`。
- en: Optimization techniques
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化技术
- en: There are several aspects of tuning Spark applications toward better optimization
    techniques. In this section, we will discuss how we can further optimize our Spark
    applications by applying data serialization by tuning the main memory with better
    memory management. We can also optimize performance by tuning the data structure
    in your Scala code while developing Spark applications. The storage, on the other
    hand, can be maintained well by utilizing serialized RDD storage.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个方面可以调整Spark应用程序以实现更好的优化技术。在本节中，我们将讨论如何通过调整主内存和更好的内存管理来进一步优化我们的Spark应用程序，通过应用数据序列化来优化性能。另一方面，通过在开发Spark应用程序时调整Scala代码中的数据结构，也可以优化性能。另外，通过利用序列化RDD存储，可以很好地维护存储。
- en: One of the most important aspects is garbage collection, and it's tuning if
    you have written your Spark application using Java or Scala. We will look at how
    we can also tune this for optimized performance. For distributed environment-
    and cluster-based system, a level of parallelism and data locality has to be ensured.
    Moreover, performance could further be improved by using broadcast variables.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的一个方面是垃圾收集，以及如果您使用Java或Scala编写了Spark应用程序，则需要调整。我们将看看如何为优化性能调整这一点。对于分布式环境和基于集群的系统，必须确保一定程度的并行性和数据局部性。此外，通过使用广播变量，性能还可以进一步提高。
- en: Data serialization
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据序列化
- en: Serialization is an important tuning for performance improvement and optimization
    in any distributed computing environment. Spark is not an exception, but Spark
    jobs are often data and computing extensive. Therefore, if your data objects are
    not in a good format, then you first need to convert them into serialized data
    objects. This demands a large number of bytes of your memory. Eventually, the
    whole process will slow down the entire processing and computation drastically.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化是任何分布式计算环境中性能改进和优化的重要调整。Spark也不例外，但Spark作业通常涉及数据和计算。因此，如果您的数据对象格式不好，那么您首先需要将它们转换为序列化数据对象。这需要大量的内存字节。最终，整个过程将严重减慢整个处理和计算的速度。
- en: As a result, you often experience a slow response from the computing nodes.
    This means that we sometimes fail to make 100% utilization of the computing resources.
    It is true that Spark tries to keep a balance between convenience and performance.
    This also implies that data serialization should be the first step in Spark tuning
    for better performance.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您经常会发现计算节点的响应速度很慢。这意味着我们有时无法充分利用计算资源。事实上，Spark试图在便利性和性能之间保持平衡。这也意味着数据序列化应该是Spark调整性能的第一步。
- en: 'Spark provides two options for data serialization: Java serialization and Kryo
    serialization libraries:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了两种数据序列化选项：Java序列化和Kryo序列化库：
- en: '**Java serialization:** Spark serializes objects using Java''s `ObjectOutputStream`
    framework. You handle the serialization by creating any class that implements
    `java.io.Serializable`. Java serialization is very flexible but often quite slow,
    which is not suitable for large data object serialization.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Java序列化：** Spark使用Java的`ObjectOutputStream`框架对对象进行序列化。您可以通过创建任何实现`java.io.Serializable`的类来处理序列化。Java序列化非常灵活，但通常相当慢，不适合大数据对象序列化。'
- en: '**Kryo serialization:** You can also use Kryo library to serialize your data
    objects more quickly. Compared to Java serialization, Kryo serialization is much
    faster, with 10x speedup and is compact than that of Java. However, it has one
    issue, that is, it does not support all the serializable types, but you need to
    require your classes to be registered.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kryo序列化：** 您还可以使用Kryo库更快地序列化数据对象。与Java序列化相比，Kryo序列化速度更快，速度提高了10倍，比Java更紧凑。但是，它有一个问题，即它不支持所有可序列化类型，但您需要要求您的类进行注册。'
- en: 'You can start using Kryo by initializing your Spark job with a `SparkConf`
    and calling `conf.set(spark.serializer, org.apache.spark.serializer.KryoSerializer)`.
    To register your own custom classes with Kryo, use the `registerKryoClasses` method,
    as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过初始化Spark作业并调用`conf.set(spark.serializer, org.apache.spark.serializer.KryoSerializer)`来开始使用Kryo。要使用Kryo注册自定义类，请使用`registerKryoClasses`方法，如下所示：
- en: '[PRE25]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If your objects are large, you may also need to increase the `spark.kryoserializer.buffer`
    config. This value needs to be large enough to hold the largest object you serialize.
    Finally, if you don't register your custom classes, Kryo still works; however,
    the full class name with each object needs to be stored, which is wasteful indeed.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的对象很大，您可能还需要增加`spark.kryoserializer.buffer`配置。这个值需要足够大，以容纳您序列化的最大对象。最后，如果您没有注册自定义类，Kryo仍然可以工作；但是，每个对象的完整类名都需要被存储，这实际上是浪费的。
- en: 'For example, in the logging subsection at the end of the monitoring Spark jobs
    section, the logging and computing can be optimized using the `Kryo` serialization.
    At first, just create the `MyMapper` class as a normal class (that is, without
    any serialization), as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在监控Spark作业部分的日志子部分中，可以使用`Kryo`序列化来优化日志记录和计算。首先，只需创建`MyMapper`类作为普通类（即，不进行任何序列化），如下所示：
- en: '[PRE26]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, let''s register this class as a `Kyro` serialization class and then set
    the `Kyro` serialization as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这个类注册为`Kyro`序列化类，然后设置`Kyro`序列化如下：
- en: '[PRE27]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'That''s all you need. The full source code of this example is given in the
    following. You should be able to run and observe the same output, but an optimized
    one as compared to the previous example:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你需要的全部内容。此示例的完整源代码如下。您应该能够运行并观察相同的输出，但与上一个示例相比是优化的：
- en: '[PRE28]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE29]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Well done! Now let's have a quick look at how to tune the memory. We will look
    at some advanced strategies to make sure the efficient use of the main memory
    in the next section.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！现在让我们快速看一下如何调整内存。我们将在下一节中看一些高级策略，以确保主内存的有效使用。
- en: Memory tuning
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存调整
- en: In this section, we will discuss some advanced strategies that can be used by
    users like you to make sure that an efficient use of memory is carried out while
    executing your Spark jobs. More specifically, we will show how to calculate the
    memory usages of your objects. We will suggest some advanced ways to improve it
    by optimizing your data structures or by converting your data objects in a serialized
    format using Kryo or Java serializer. Finally, we will look at how to tune Spark's
    Java heap size, cache size, and the Java garbage collector.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些高级策略，用户可以使用这些策略来确保在执行Spark作业时对内存的有效使用。更具体地说，我们将展示如何计算对象的内存使用情况。我们将建议一些高级方法来通过优化数据结构或将数据对象转换为使用Kryo或Java序列化的序列化格式来改进它。最后，我们将看看如何调整Spark的Java堆大小、缓存大小和Java垃圾收集器。
- en: 'There are three considerations in tuning memory usage:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 调整内存使用时有三个考虑因素：
- en: 'The amount of memory used by your objects: You may even want your entire dataset
    to fit in the memory'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的对象使用的内存量：甚至可能希望整个数据集都能放入内存中
- en: The cost of accessing those objects
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问这些对象的成本
- en: 'The overhead of garbage collection: If you have a high turnover in terms of
    objects'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垃圾收集的开销：如果对象的周转率很高
- en: Although Java objects are fast enough to access, they can easily consume a factor
    of 2 to 5x more space than the actual (aka raw) data in their original fields.
    For example, each distinct Java object has 16 bytes of overhead with an object
    header. A Java string, for example, has almost 40 bytes of extra overhead over
    the raw string. Furthermore, Java collection classes like `Set`, `List`, `Queue`,
    `ArrayList`, `Vector`, `LinkedList`, `PriorityQueue`, `HashSet`, `LinkedHashSet`,
    `TreeSet`, and so on, are also used. The linked data structures, on the other
    hand, are too complex, occupying too much extra space since there is a wrapper
    object for each entry in the data structure. Finally, the collections of primitive
    types frequently store them in the memory as boxed objects, such as `java.lang.Double`
    and `java.lang.Integer`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Java对象访问速度足够快，但它们很容易消耗实际数据字段的2到5倍的空间。例如，每个不同的Java对象都有16字节的开销与对象头。例如，Java字符串比原始字符串多出近40字节的额外开销。此外，还使用了Java集合类，如`Set`、`List`、`Queue`、`ArrayList`、`Vector`、`LinkedList`、`PriorityQueue`、`HashSet`、`LinkedHashSet`、`TreeSet`等。另一方面，链式数据结构过于复杂，占用了太多额外的空间，因为数据结构中的每个条目都有一个包装对象。最后，基本类型的集合经常将它们存储在内存中作为装箱对象，例如`java.lang.Double`和`java.lang.Integer`。
- en: Memory usage and management
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存使用和管理
- en: Memory usages by your Spark application and underlying computing nodes can be
    categorized as execution and storage. Execution memory is used during the computation
    in merge, shuffles, joins, sorts, and aggregations. On the other hand, storage
    memory is used for caching and propagating internal data across the cluster. In
    short, this is due to the large amount of I/O across the network.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 您的Spark应用程序和底层计算节点的内存使用可以分为执行和存储。执行内存用于合并、洗牌、连接、排序和聚合计算过程中。另一方面，存储内存用于在集群中缓存和传播内部数据。简而言之，这是由于网络上的大量I/O。
- en: Technically, Spark caches network data locally. While working with Spark iteratively
    or interactively, caching or persistence are optimization techniques in Spark.
    These two help in saving interim partial results so that they can be reused in
    subsequent stages. Then these interim results (as RDDs) can be kept in memory
    (default) or more solid storage, such as disk, and/or replicated. Furthermore,
    RDDs can be cached using cache operations too. They can also be persisted using
    a persist operation. The difference between cache and persist operations is purely
    syntactic. The cache is a synonym of persisting or persists (`MEMORY_ONLY`), that
    is, cache is merely persisted with the default storage level `MEMORY_ONLY`.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，Spark将网络数据缓存在本地。在使用Spark进行迭代或交互式工作时，缓存或持久化是Spark中的优化技术。这两种技术有助于保存中间部分结果，以便它们可以在后续阶段重复使用。然后这些中间结果（作为RDD）可以保存在内存（默认）或更可靠的存储介质，如磁盘，并/或复制。此外，RDD也可以使用缓存操作进行缓存。它们也可以使用持久化操作进行持久化。缓存和持久化操作之间的区别纯粹是语法上的。缓存是持久化或持久化（`MEMORY_ONLY`）的同义词，即缓存仅以默认存储级别`MEMORY_ONLY`进行持久化。
- en: If you go under the Storage tab in your Spark web UI, you should observe the
    memory/storage used by an RDD, DataFrame, or Dataset object, as shown in *Figure
    10*. Although there are two relevant configurations for tuning memory in Spark,
    users do not need to readjust them. The reason is that the default values set
    in the configuration files are enough for your requirements and workloads.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Spark web UI中转到存储选项卡，您应该观察RDD、DataFrame或Dataset对象使用的内存/存储，如*图10*所示。尽管在Spark中有两个相关的内存调整配置，用户不需要重新调整它们。原因是配置文件中设置的默认值足以满足您的需求和工作负载。
- en: spark.memory.fraction is the size of the unified region as a fraction of (JVM
    heap space - 300 MB) (default 0.6). The rest of the space (40%) is reserved for
    user data structures, internal metadata in Spark, and safeguarding against OOM
    errors in case of sparse and unusually large records. On the other hand, `spark.memory.storageFraction`
    expresses the size of R storage space as a fraction of the unified region (default
    is 0.5). The default value of this parameter is 50% of Java heap space, that is,
    300 MB.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: spark.memory.fraction是统一区域大小占(JVM堆空间-300 MB)的比例（默认为0.6）。其余空间（40%）用于用户数据结构、Spark内部元数据和防止在稀疏和异常大记录的情况下发生OOM错误。另一方面，`spark.memory.storageFraction`表示R存储空间大小占统一区域的比例（默认为0.5）。该参数的默认值是Java堆空间的50%，即300
    MB。
- en: A more detailed discussion on memory usage and storage is given in [Chapter
    15](part0458.html#DKP1K1-21aec46d8593429cacea59dbdcd64e1c), *Text Analytics Using
    Spark ML*.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 有关内存使用和存储的更详细讨论，请参阅[第15章](part0458.html#DKP1K1-21aec46d8593429cacea59dbdcd64e1c)，*使用Spark
    ML进行文本分析*。
- en: 'Now, one question might arise in your mind: which storage level to choose?
    To answer this question, Spark storage levels provide you with different trade-offs
    between memory usage and CPU efficiency. If your RDDs fit comfortably with the
    default storage level (MEMORY_ONLY), let your Spark driver or master go with it.
    This is the most memory-efficient option, allowing operations on the RDDs to run
    as fast as possible. You should let it go with this, because this is the most
    memory-efficient option. This also allows numerous operations on the RDDs to be
    done as fast as possible.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可能会想到一个问题：选择哪种存储级别？为了回答这个问题，Spark存储级别为您提供了在内存使用和CPU效率之间的不同权衡。如果您的RDD与默认存储级别（MEMORY_ONLY）相适应，请让您的Spark
    driver或master使用它。这是最节省内存的选项，允许对RDD进行的操作尽可能快地运行。您应该让它使用这个选项，因为这是最节省内存的选项。这也允许对RDD进行的众多操作尽可能快地完成。
- en: If your RDDs do not fit the main memory, that is, if `MEMORY_ONLY` does not
    work out, you should try using `MEMORY_ONLY_SER`. It is strongly recommended to
    not spill your RDDs to disk unless your **UDF** (aka **user-defined function**
    that you have defined for processing your dataset) is too expensive. This also
    applies if your UDF filters a large amount of the data during the execution stages.
    In other cases, recomputing a partition, that is, repartition, may be faster for
    reading data objects from disk. Finally, if you want fast fault recovery, use
    the replicated storage levels.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的RDD不适合主内存，也就是说，如果`MEMORY_ONLY`不起作用，您应该尝试使用`MEMORY_ONLY_SER`。强烈建议不要将RDD溢出到磁盘，除非您的**UDF**（即为处理数据集定义的**用户定义函数**）太昂贵。如果您的UDF在执行阶段过滤了大量数据，也适用于此。在其他情况下，重新计算分区，即重新分区，可能更快地从磁盘读取数据对象。最后，如果您希望快速故障恢复，请使用复制的存储级别。
- en: 'In summary, there are the following StorageLevels available and supported in
    Spark 2.x: (number _2 in the name denotes 2 replicas):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在Spark 2.x中支持以下StorageLevels：（名称中的数字_2表示2个副本）：
- en: '`DISK_ONLY`: This is for disk-based operation for RDDs'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DISK_ONLY`: 这是RDD的基于磁盘的操作'
- en: '`DISK_ONLY_2`: This is for disk-based operation for RDDs for 2 replicas'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DISK_ONLY_2`: 这是RDD的基于磁盘的操作，带有2个副本'
- en: '`MEMORY_ONLY`: This is the default for cache operation in memory for RDDs'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_ONLY`: 这是RDD的内存缓存操作的默认值'
- en: '`MEMORY_ONLY_2`: This is the default for cache operation in memory for RDDs
    with 2 replicas'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_ONLY_2`: 这是RDD具有2个副本的内存缓存操作的默认值'
- en: '`MEMORY_ONLY_SER`: If your RDDs do not fit the main memory, that is, if `MEMORY_ONLY`
    does not work out, this option particularly helps in storing data objects in a
    serialized form'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_ONLY_SER`: 如果您的RDD不适合主内存，也就是说，如果`MEMORY_ONLY`不起作用，这个选项特别有助于以序列化形式存储数据对象'
- en: '`MEMORY_ONLY_SER_2`: If your RDDs do not fit the main memory, that is, if `MEMORY_ONLY`
    does not work out with 2 replicas, this option also helps in storing data objects
    in a serialized form'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_ONLY_SER_2`: 如果您的RDD不适合主内存，也就是说，如果`MEMORY_ONLY`不适合2个副本，这个选项也有助于以序列化形式存储数据对象'
- en: '`MEMORY_AND_DISK`: Memory and disk (aka combined) based RDD persistence'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_AND_DISK`: 基于内存和磁盘（也称为组合）的RDD持久性'
- en: '`MEMORY_AND_DISK_2`: Memory and disk (aka combined) based RDD persistence with
    2 replicas'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_AND_DISK_2`: 基于内存和磁盘（也称为组合）的RDD持久性，带有2个副本'
- en: '`MEMORY_AND_DISK_SER`: If `MEMORY_AND_DISK` does not work, it can be used'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_AND_DISK_SER`: 如果`MEMORY_AND_DISK`不起作用，可以使用它'
- en: '`MEMORY_AND_DISK_SER_2`: If `MEMORY_AND_DISK` does not work with 2 replicas,
    this option can be used'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_AND_DISK_SER_2`: 如果`MEMORY_AND_DISK`不适用于2个副本，可以使用此选项'
- en: '`OFF_HEAP`: Does not allow writing into Java heap space'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OFF_HEAP`: 不允许写入Java堆空间'
- en: Note that cache is a synonym of persist (`MEMORY_ONLY`). This means that cache
    is solely persist with the default storage level, that is, `MEMORY_ONLY`. Detailed
    information can be found at [https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-StorageLevel.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-StorageLevel.html).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，缓存是持久化的同义词（`MEMORY_ONLY`）。这意味着缓存仅使用默认存储级别，即`MEMORY_ONLY`。详细信息可以在[https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-StorageLevel.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-StorageLevel.html)找到。
- en: Tuning the data structures
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整数据结构
- en: The first way to reduce extra memory usage is to avoid some features in the
    Java data structure that impose extra overheads. For example, pointer-based data
    structures and wrapper objects contribute to nontrivial overheads. To tune your
    source code with a better data structure, we provide some suggestions here, which
    can be useful.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 减少额外内存使用的第一种方法是避免Java数据结构中的一些特性，这些特性会带来额外的开销。例如，基于指针的数据结构和包装对象会导致非常大的开销。为了调整您的源代码以使用更好的数据结构，我们在这里提供了一些建议，这可能会有所帮助。
- en: First, design your data structures such that you use arrays of objects and primitive
    types more. Thus, this also suggests using standard Java or Scala collection classes
    like `Set`, `List`, `Queue`, `ArrayList`, `Vector`, `LinkedList`, `PriorityQueue`,
    `HashSet`, `LinkedHashSet`, and `TreeSet` more frequently.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，设计您的数据结构，以便更多地使用对象和基本类型的数组。因此，这也建议更频繁地使用标准的Java或Scala集合类，如`Set`，`List`，`Queue`，`ArrayList`，`Vector`，`LinkedList`，`PriorityQueue`，`HashSet`，`LinkedHashSet`和`TreeSet`。
- en: Second, when possible, avoid using nested structures with a lot of small objects
    and pointers so that your source code becomes more optimized and concise. Third,
    when possible, consider using numeric IDs and sometimes using enumeration objects
    rather than using strings for keys. This is recommended because, as we have already
    stated, a single Java string object creates an extra overhead of 40 bytes. Finally,
    if you have less than 32 GB of main memory (that is, RAM), set the JVM flag `-XX:+UseCompressedOops`
    to make pointers 4 bytes instead of 8.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，尽可能避免使用具有大量小对象和指针的嵌套结构，以使您的源代码更加优化和简洁。第三，尽可能考虑使用数字ID，有时使用枚举对象而不是使用字符串作为键。这是因为，正如我们已经提到的，单个Java字符串对象会产生额外的40字节开销。最后，如果您的主内存（即RAM）少于32GB，请设置JVM标志`-XX:+UseCompressedOops`，以使指针为4字节而不是8字节。
- en: The earlier option can be set in the `SPARK_HOME/conf/spark-env.sh.template`.
    Just rename the file as `spark-env.sh` and set the value straight away!
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的选项可以在`SPARK_HOME/conf/spark-env.sh.template`中设置。只需将文件重命名为`spark-env.sh`并立即设置值！
- en: Serialized RDD storage
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列化RDD存储
- en: As discussed already, despite other types of memory tuning, when your objects
    are too large to fit in the main memory or disk efficiently, a simpler and better
    way of reducing memory usage is storing them in a serialized form.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面讨论的，尽管有其他类型的内存调整，但当您的对象太大而无法有效地适应主内存或磁盘时，减少内存使用的一个更简单和更好的方法是以序列化形式存储它们。
- en: This can be done using the serialized storage levels in the RDD persistence
    API, such as `MEMORY_ONLY_SER`. For more information, refer to the previous section
    on memory management and start exploring available options.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过RDD持久性API中的序列化存储级别（如`MEMORY_ONLY_SER`）来实现。有关更多信息，请参阅前一节关于内存管理的内容，并开始探索可用的选项。
- en: If you specify using `MEMORY_ONLY_SER`, Spark will then store each RDD partition
    as one large byte array. However, the only downside of this approach is that it
    can slow down data access times. This is reasonable and obvious too; fairly speaking,
    there's no way to avoid it since each object needs to deserialize on the fly back
    while reusing.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您指定使用`MEMORY_ONLY_SER`，Spark将把每个RDD分区存储为一个大的字节数组。然而，这种方法的唯一缺点是它可能会减慢数据访问速度。这是合理的，也很明显；公平地说，没有办法避免它，因为每个对象都需要在重用时动态反序列化。
- en: As discussed previously, we highly recommend using Kryo serialization instead
    of Java serialization to make data access a bit faster.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们强烈建议使用Kryo序列化而不是Java序列化，以使数据访问速度更快。
- en: Garbage collection tuning
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 垃圾收集调优
- en: Although it is not a major problem in your Java or Scala programs that just
    read an RDD sequentially or randomly once and then execute numerous operations
    on it, **Java Virtual Machine** (**JVM**) GC can be problematic and complex if
    you have a large amount of data objects w.r.t RDDs stored in your driver program.
    When the JVM needs to remove obsolete and unused objects from the old objects
    to make space for the newer ones, it is mandatory to identify them and remove
    them from the memory eventually. However, this is a costly operation in terms
    of processing time and storage. You might be wondering that the cost of GC is
    proportional to the number of Java objects stored in your main memory. Therefore,
    we strongly suggest you tune your data structure. Also, having fewer objects stored
    in your memory is recommended.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在您的Java或Scala程序中，只是顺序或随机读取RDD一次，然后对其执行大量操作并不是一个主要问题，但是如果您的驱动程序中存储了大量数据对象，就会导致**Java虚拟机**（**JVM**）GC变得棘手和复杂。当JVM需要从旧对象中删除过时和未使用的对象以为新对象腾出空间时，有必要识别它们并最终从内存中删除它们。然而，这在处理时间和存储方面是一项昂贵的操作。您可能会想到GC的成本与存储在主内存中的Java对象数量成正比。因此，我们强烈建议您调整数据结构。此外，建议减少存储在内存中的对象数量。
- en: 'The first step in GC tuning is collecting the related statistics on how frequently
    garbage collection by JVM occurs on your machine. The second statistic needed
    in this regard is the amount of time spent on GC by JVM on your machine or computing
    nodes. This can be achieved by adding `-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps`
    to the Java options in your IDE, such as Eclipse, in the JVM startup arguments
    and specifying a name and location for our GC log file, as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: GC调优的第一步是收集有关JVM在您的计算机上频繁进行垃圾收集的相关统计信息。在这方面需要的第二个统计数据是JVM在您的计算机或计算节点上花费在GC上的时间。这可以通过在您的IDE（如Eclipse）中的JVM启动参数中添加`-verbose:gc
    -XX:+PrintGCDetails -XX:+PrintGCTimeStamps`来实现，并指定GC日志文件的名称和位置，如下所示：
- en: '![](img/00213.jpeg)**Figure 27:** Setting GC verbose on Eclipse'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00213.jpeg)**图27：**在Eclipse上设置GC详细信息'
- en: 'Alternatively, you can specify `verbose:gc` while submitting your Spark jobs
    using the Spark-submit script, as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在使用Spark-submit脚本提交Spark作业时指定`verbose:gc`，如下所示：
- en: '[PRE30]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In short, when specifying GC options for Spark, you must determine where you
    want the GC options specified, on the executors or on the driver. When you submit
    your jobs, specify `--driver-java-options -XX:+PrintFlagsFinal -verbose:gc` and
    so on. For the executor, specify `--conf spark.executor.extraJavaOptions=-XX:+PrintFlagsFinal
    -verbose:gc` and so on.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在为Spark指定GC选项时，您必须确定要在执行程序或驱动程序上指定GC选项。当您提交作业时，指定`--driver-java-options
    -XX:+PrintFlagsFinal -verbose:gc`等。对于执行程序，指定`--conf spark.executor.extraJavaOptions=-XX:+PrintFlagsFinal
    -verbose:gc`等。
- en: Now, when your Spark job is executed, you will be able to see the logs and messages
    printed in the worker's node at `/var/log/logs` each time a GC occurs. The downside
    of this approach is that these logs will not be on your driver program but on
    your cluster's worker nodes.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当执行您的Spark作业时，每次发生GC时，您都可以在工作节点的`/var/log/logs`中看到打印的日志和消息。这种方法的缺点是这些日志不会出现在您的驱动程序上，而是出现在集群的工作节点上。
- en: It is to be noted that `verbose:gc` only prints appropriate message or logs
    after each GC collection. Correspondingly, it prints details about memory. However,
    if you are interested in looking for more critical issues, such as a memory leak,
    `verbose:gc` may not be enough. In that case, you can use some visualization tools,
    such as jhat and VisualVM. A better way of GC tuning in your Spark application
    can be read at [https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，`verbose:gc`只会在每次GC收集后打印适当的消息或日志。相应地，它会打印有关内存的详细信息。但是，如果您对寻找更严重的问题（如内存泄漏）感兴趣，`verbose:gc`可能不够。在这种情况下，您可以使用一些可视化工具，如jhat和VisualVM。您可以在[https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html)上阅读有关在Spark应用程序中进行更好的GC调优的信息。
- en: Level of parallelism
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行级别
- en: Although you can control the number of map tasks to be executed through optional
    parameters to the `SparkContext.text` file, Spark sets the same on each file according
    to its size automatically. In addition to this, for a distributed `reduce` operation
    such as `groupByKey` and `reduceByKey`, Spark uses the largest parent RDD's number
    of partitions. However, sometimes, we make one mistake, that is, not utilizing
    the full computing resources for your nodes in a computing cluster. As a result,
    the full computing resources will not be fully exploited unless you set and specify
    the level of parallelism for your Spark job explicitly. Therefore, you should
    set the level of parallelism as the second argument.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管您可以通过`SparkContext.text`文件的可选参数来控制要执行的映射任务的数量，但Spark会根据文件的大小自动设置每个文件的映射任务数量。此外，对于分布式的`reduce`操作，如`groupByKey`和`reduceByKey`，Spark会使用最大父RDD的分区数。然而，有时我们会犯一个错误，即未充分利用计算集群中节点的全部计算资源。因此，除非您明确设置和指定Spark作业的并行级别，否则将无法充分利用全部计算资源。因此，您应该将并行级别设置为第二个参数。
- en: For more on this option, please refer to [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions.](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此选项的更多信息，请参阅[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions.](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)
- en: Alternatively, you can do it by setting the config property spark.default.parallelism
    to change the default. For operations such as parallelizing with no parent RDDs,
    the level of parallelism depends on the cluster manager, that is, standalone,
    Mesos, or YARN. For the local mode, set the level of parallelism equal to the
    number of cores on the local machine. For Mesos or YARN, set fine-grained mode
    to 8\. In other cases, the total number of cores on all executor nodes or 2, whichever
    is larger, and in general, 2-3 tasks per CPU core in your cluster is recommended.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以通过设置配置属性spark.default.parallelism来更改默认设置。对于没有父RDD的并行操作，并行级别取决于集群管理器，即独立模式、Mesos或YARN。对于本地模式，将并行级别设置为本地机器上的核心数。对于Mesos或YARN，将细粒度模式设置为8。在其他情况下，所有执行程序节点上的总核心数或2，以较大者为准，通常建议在集群中每个CPU核心使用2-3个任务。
- en: Broadcasting
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播
- en: A broadcast variable enables a Spark developer to keep a read-only copy of an
    instance or class variable cached on each driver program, rather than transferring
    a copy of its own with the dependent tasks. However, an explicit creation of a
    broadcast variable is useful only when tasks across multiple stages need the same
    data in deserialize form.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量使Spark开发人员能够在每个驱动程序程序上缓存一个实例或类变量的只读副本，而不是将其自己的副本与依赖任务一起传输。但是，只有当多个阶段的任务需要以反序列化形式的相同数据时，显式创建广播变量才有用。
- en: In Spark application development, using the broadcasting option of SparkContext
    can reduce the size of each serialized task greatly. This also helps to reduce
    the cost of initiating a Spark job in a cluster. If you have a certain task in
    your Spark job that uses large objects from the driver program, you should turn
    it into a broadcast variable.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark应用程序开发中，使用SparkContext的广播选项可以大大减小每个序列化任务的大小。这也有助于减少在集群中启动Spark作业的成本。如果您的Spark作业中有某个任务使用了驱动程序中的大对象，您应该将其转换为广播变量。
- en: 'To use a broadcast variable in a Spark application, you can instantiate it
    using `SparkContext.broadcast`. Later on, use the value method from the class
    to access the shared value as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Spark应用程序中使用广播变量，可以使用`SparkContext.broadcast`进行实例化。然后，使用该类的value方法来访问共享值，如下所示：
- en: '[PRE31]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Output/log: `bv: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(0)`'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '输出/日志：`bv: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(0)`'
- en: '[PRE32]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Output/log: `res0: Int = 1`'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '输出/日志：`res0: Int = 1`'
- en: '![](img/00359.jpeg)**Figure 28:** Broadcasting a value from driver to executors'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00359.jpeg)**图28：** 从驱动程序向执行程序广播一个值'
- en: 'The Broadcast feature of Spark uses the **SparkContext** to create broadcast
    values. After that, the **BroadcastManager** and **ContextCleaner** are used to
    control their life cycle, as shown in the following figure:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的广播功能使用**SparkContext**创建广播值。之后，**BroadcastManager**和**ContextCleaner**用于控制它们的生命周期，如下图所示：
- en: '![](img/00368.jpeg)**Figure 29:** SparkContext broadcasts the variable/value
    using BroadcastManager and ContextCleaner'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00368.jpeg)**图29：** SparkContext使用BroadcastManager和ContextCleaner广播变量/值'
- en: Spark application in the driver program automatically prints the serialized
    size of each task on the driver. Therefore, you can decide whether your tasks
    are too large to make it parallel. If your task is larger than 20 KB, it's probably
    worth optimizing.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序中的Spark应用程序会自动打印每个任务在驱动程序上的序列化大小。因此，您可以决定您的任务是否过大而无法并行。如果您的任务大于20 KB，可能值得优化。
- en: Data locality
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据本地性
- en: Data locality means how close the data is to the code to be processed. Technically,
    data locality can have a nontrivial impact on the performance of a Spark job to
    be executed locally or in cluster mode. As a result, if the data and the code
    to be processed are tied together, computation is supposed to be much faster.
    Usually, shipping a serialized code from a driver to an executor is much faster
    since the code size is much smaller than that of data.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 数据本地性意味着数据与要处理的代码的接近程度。从技术上讲，数据本地性对于在本地或集群模式下执行的Spark作业的性能可能会产生重大影响。因此，如果数据和要处理的代码是绑定在一起的，计算速度应该会更快。通常情况下，从驱动程序向执行程序发送序列化代码要快得多，因为代码大小比数据小得多。
- en: 'In Spark application development and job execution, there are several levels
    of locality. In order from closest to farthest, the level depends on the current
    location of the data you have to process:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark应用程序开发和作业执行中，存在几个级别的本地性。从最接近到最远，级别取决于您需要处理的数据的当前位置：
- en: '| **Data Locality** | **Meaning** | **Special Notes** |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| **数据本地性** | **含义** | **特殊说明** |'
- en: '| `PROCESS_LOCAL` | Data and code are in the same location | Best locality
    possible |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '`PROCESS_LOCAL` | 数据和代码位于同一位置 | 最佳的位置可能'
- en: '| `NODE_LOCAL` | Data and the code are on the same node, for example, data
    stored on HDFS | A bit slower than `PROCESS_LOCAL` since the data has to propagate
    across the processes and network |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '`NODE_LOCAL` | 数据和代码位于同一节点上，例如，存储在HDFS上的数据 | 比`PROCESS_LOCAL`慢一点，因为数据必须在进程和网络之间传播'
- en: '| `NO_PREF` | The data is accessed equally from somewhere else | Has no locality
    preference |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '`NO_PREF` | 数据可以从其他地方平等访问 | 没有位置偏好'
- en: '| `RACK_LOCAL` | The data is on the same rack of servers over the network |
    Suitable for large-scale data processing |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '`RACK_LOCAL` | 数据位于同一机架上的服务器上 | 适用于大规模数据处理'
- en: '| `ANY` | The data is elsewhere on the network and not in the same rack | Not
    recommended unless there are no other options available |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '`ANY` | 数据在网络的其他地方，不在同一机架上 | 除非没有其他选择，否则不建议使用'
- en: '**Table 2:** Data locality and Spark'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**表2：** 数据位置和Spark'
- en: 'Spark is developed such that it prefers to schedule all tasks at the best locality
    level, but this is not guaranteed and not always possible either. As a result,
    based on the situation in the computing nodes, Spark switches to lower locality
    levels if available computing resources are too occupied. Moreover, if you would
    like to have the best data locality, there are two choices for you:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Spark被开发成优先在最佳位置调度所有任务，但这并不是保证的，也并非总是可能的。因此，基于计算节点的情况，如果可用的计算资源过于繁忙，Spark会切换到较低的位置级别。此外，如果您想要最佳的数据位置，有两种选择：
- en: Wait until a busy CPU gets free to start a task on your data on the same server
    or same node
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待繁忙的CPU空闲下来，在同一台服务器或同一节点上启动任务
- en: Immediately start a new one, which requires moving data there
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 立即开始一个新的任务，需要将数据移动到那里
- en: Summary
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed some advanced topics of Spark toward making your
    Spark job's performance better. We discussed some basic techniques to tune your
    Spark jobs. We discussed how to monitor your jobs by accessing Spark web UI. We
    discussed how to set Spark configuration parameters. We also discussed some common
    mistakes made by Spark users and provided some recommendations. Finally, we discussed
    some optimization techniques that help tune Spark applications.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一些关于Spark的高级主题，以使您的Spark作业性能更好。我们讨论了一些调整Spark作业的基本技术。我们讨论了如何通过访问Spark
    web UI来监视您的作业。我们讨论了如何设置Spark配置参数。我们还讨论了一些Spark用户常见的错误，并提供了一些建议。最后，我们讨论了一些优化技术，帮助调整Spark应用程序。
- en: In the next chapter, you will see how to test Spark applications and debug to
    solve most common issues.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将看到如何测试Spark应用程序并调试以解决最常见的问题。
