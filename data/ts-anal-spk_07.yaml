- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Building and Testing Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建与测试模型
- en: Having covered the data preparation and exploratory data analysis stages of
    time series analysis, we will now direct our focus to constructing predictive
    models for time series data. We will cover the diverse types of models and how
    to decide which one to choose. We will also learn how to train, tune, and evaluate
    models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在覆盖了时间序列分析的数据准备和探索性数据分析阶段后，我们现在将重点转向为时间序列数据构建预测模型。我们将涵盖多种类型的模型以及如何决定选择哪个模型。我们还将学习如何训练、调整和评估模型。
- en: The concepts covered in this chapter will act as a practical guide to model
    development, providing essential building blocks for effective time series models
    and facilitating accurate predictions and insightful analyses. We will factor
    in common execution constraints faced in real-life projects and conclude with
    a comparison of the outcome of the different models to solve a forecasting problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所涉及的概念将作为模型开发的实用指南，为有效的时间序列模型提供基本构建块，并促进准确的预测和深入的分析。我们将考虑在实际项目中常见的执行约束，并最终对不同模型解决预测问题的结果进行比较。
- en: 'We are going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主要主题：
- en: Model selection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型选择
- en: Development and testing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发与测试
- en: Model comparison
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型比较
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code for this chapter, which will be covered in the *Development and testing*
    section, can be found in the `ch7` folder of the book’s GitHub repository at this
    URL:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码将在*开发与测试*部分中讲解，可以在本书的GitHub仓库的`ch7`文件夹中找到，网址如下：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch7](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch7).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch7](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch7)。'
- en: Model selection
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型选择
- en: The first step before developing a time series analysis model is to select which
    model to use. As discussed in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016),
    one of the key challenges of time series analysis is using the right model. This
    choice impacts, among other things, the accuracy, reliability, efficiency, and
    scalability of the analysis. This, in turn, ensures that the analysis leads to
    better-informed decisions and more effective outcomes while being scientifically
    robust and practically useful.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发时间序列分析模型之前的第一步是选择使用哪个模型。正如在[*第1章*](B18568_01.xhtml#_idTextAnchor016)中讨论的那样，时间序列分析的一个关键挑战是选择合适的模型。这个选择会影响分析的准确性、可靠性、效率和可扩展性等多个方面。反过来，这确保了分析能够得出更有根据的决策和更有效的结果，同时具有科学的严谨性和实际的实用性。
- en: There are different types of models, each with its own characteristics.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的模型各有其特点。
- en: Types of models
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型类型
- en: 'Time series analysis models can be categorized into statistical, classical
    Machine Learning (ML), and Deep Learning (DL) models:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列分析模型可以分为统计模型、经典机器学习（ML）模型和深度学习（DL）模型：
- en: '**Statistical models** for time series analysis are based on statistical theories
    with assumptions about the characteristics of the time series, such as linearity
    and stationarity. Examples of classical models include **Autoregressive Moving
    Average** (**ARIMA**), **Seasonal Autoregressive Integrated Moving Average Exogenous**
    (**SARIMAX**), **Exponential Smoothing** (**ETS**), **Generalized Autoregressive
    Conditional Heteroskedasticity** (**GARCH**), and state-space models.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计模型**用于时间序列分析，它们基于统计理论并假设时间序列的特征，如线性和平稳性。经典模型的例子包括**自回归滑动平均模型**（**ARIMA**）、**季节性自回归积分滑动平均外生模型**（**SARIMAX**）、**指数平滑法**（**ETS**）、**广义自回归条件异方差模型**（**GARCH**）和状态空间模型。'
- en: '**Classical Machine Learning models** for time series analysis use algorithms
    that can learn from data without explicit programming. These models can handle
    non-linear relationships. However, they often require more data for training compared
    to classical models. Examples of Machine Learning models include linear regression,
    **Support Vector Machines** (**SVMs**), **k-Nearest Neighbors** (**kNN**), random
    forests, and gradient boosting machines.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经典机器学习模型**用于时间序列分析，采用无需显式编程的算法从数据中学习。这些模型能够处理非线性关系。然而，它们通常需要比经典模型更多的数据来进行训练。机器学习模型的例子包括线性回归、**支持向量机**（**SVMs**）、**k近邻**（**kNN**）、随机森林和梯度提升机。'
- en: '**Deep Learning models** use neural networks with multiple layers to learn
    complex patterns in time series data. These models can handle non-linear relationships
    and long-term dependencies. They, however, require large datasets for training
    and significant computational resources. Examples of Deep Learning models include
    **Long Short-Term Memory (LSTM) Networks**, **Convolutional Neural Networks**
    (**CNNs**), **Temporal Convolutional Networks** (**TCNs**), transformers, and
    autoencoders.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习模型**使用具有多个层次的神经网络来学习时间序列数据中的复杂模式。这些模型能够处理非线性关系和长期依赖性。然而，它们需要大量的训练数据和显著的计算资源。深度学习模型的例子包括**长短期记忆网络（LSTM）**、**卷积神经网络（CNN）**、**时间卷积网络（TCN）**、变换器（transformers）和自编码器（autoencoders）。'
- en: Machine Learning and Deep Learning
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与深度学习
- en: Deep Learning is a subset of Machine Learning that uses deep neural networks.
    As is common practice, we are using the term classical Machine Learning here to
    refer to approaches and models that are not neural-network-based. The term Deep
    Learning is used for approaches and models using neural networks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子集，它使用深度神经网络。按照常规做法，我们在这里使用“经典机器学习”这一术语，指的是那些不是基于神经网络的方法和模型。深度学习一词则用于描述使用神经网络的方法和模型。
- en: Each of the preceding categories and models has distinct characteristics and
    approaches that determine their applicability, which we will explore next.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的每个类别和模型都有独特的特点和方法，这决定了它们的适用性，我们接下来将进一步探讨这些内容。
- en: Selection criteria
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择标准
- en: When to use which model is based on several criteria. We touched on this briefly
    in the section on using the right model in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016)
    and when initially discussing model selection in [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087).
    The applicability of a model to solve a time series analysis problem is dependent
    on factors such as the objectives of the analysis, the characteristics of the
    data, and the computation power and time available.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 选择使用哪种模型是基于几个标准的。在[*第1章*](B18568_01.xhtml#_idTextAnchor016)中关于选择正确模型的部分以及在[*第4章*](B18568_04.xhtml#_idTextAnchor087)中初步讨论模型选择时，我们简要提到了这一点。模型在解决时间序列分析问题中的适用性取决于分析目标、数据特征、计算能力和可用时间等因素。
- en: We will now dive deep into the details of these and other important factors
    for model selection.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将深入探讨模型选择中的这些及其他重要因素。
- en: Types of use cases
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用场景类型
- en: Time series analysis broadly falls into use cases for forecasting, classification,
    and anomaly detection, as discussed in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044).
    We will briefly recap these types of use cases here, highlighting the frequently
    used models. We will go into further detail in the rest of the chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列分析大体上可分为预测、分类和异常检测等使用场景，如在[*第2章*](B18568_02.xhtml#_idTextAnchor044)中讨论的那样。我们将在这里简要回顾这些使用场景，并突出常用的模型。接下来的章节将深入讨论这些内容。
- en: '**Forecasting**’s goal is to predict future values based on patterns learned
    by the model from past values. As presented in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044),
    forecasting can be single or multi-steps, based on a single (**univariate**) or
    multiple (**multivariate**) time series. Commonly used models such as ARIMA, SARIMA,
    and **Exponential Smoothing** (**ETS**) are chosen for their simplicity while
    giving strong performance in forecasting tasks. LSTM and Prophet, introduced in
    previous examples in the book, are preferred for more complex forecasting requirements
    where they can be more effective.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测**的目标是基于模型从过去的值中学习到的模式来预测未来的值。如在[*第2章*](B18568_02.xhtml#_idTextAnchor044)中所述，预测可以是单步或多步，基于单一（**单变量**）或多个（**多变量**）时间序列。常用的模型如ARIMA、SARIMA和**指数平滑法**（**ETS**）因其简单性而被选择，并在预测任务中表现出强劲的性能。LSTM和在本书前面章节中介绍的Prophet，适用于更复杂的预测需求，并在这些场景下能更有效地工作。'
- en: '**Pattern recognition** and **classification** are used to identify and understand
    patterns and classify time series accordingly. Commonly used models are based
    on decomposition methods, such as **Seasonal-Trend decomposition using LOESS**
    (**STL**) and **Multiple STL** (**MSTL**), and Fourier analysis. We spent some
    time on decomposition in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016) and [*Chapter
    6*](B18568_06.xhtml#_idTextAnchor116). We briefly discussed Fourier analysis in
    [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044), in addition to distance-based
    approaches, shapelets analysis, ensemble methods and Deep Learning.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式识别**和**分类**用于识别和理解模式，并相应地对时间序列进行分类。常用的模型基于分解方法，例如**使用LOESS的季节-趋势分解**（**STL**）和**多重STL**（**MSTL**），以及傅里叶分析。我们在[*第一章*](B18568_01.xhtml#_idTextAnchor016)和[*第六章*](B18568_06.xhtml#_idTextAnchor116)中花了一些时间讨论分解方法。我们在[*第二章*](B18568_02.xhtml#_idTextAnchor044)中简要讨论了傅里叶分析，此外还讨论了基于距离的方法、形状分析、集成方法和深度学习。'
- en: '**Anomaly detection** aims to identify outliers or anomalies in the time series.
    As presented in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044), this detection
    can be based on univariate or multivariate series and point, collective, or contextual
    analysis. What is initially flagged as an anomaly can turn out to be a novelty,
    in the sense of a new non-problematic pattern. Commonly used models are based
    on their capabilities for residual analysis, such as ARIMA. Machine learning models
    are frequently used as well, such as Isolation Forest or, when there is a high
    percentage of anomalies, specialized methods such as **Seasonal Hybrid Extreme
    Studentized Deviate** (**SH-ESD**). We saw a code example in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044)
    of Isolation Forest for anomaly detection, in addition to discussing supervised,
    unsupervised, semi-supervised, and hybrid approaches.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**旨在识别时间序列中的异常值或异常。正如在[*第二章*](B18568_02.xhtml#_idTextAnchor044)中所展示的，这种检测可以基于单变量或多变量序列，以及点、集体或上下文分析。最初标记为异常的点可能最终被认为是新颖的，即一种非问题的全新模式。常用的模型基于残差分析的能力，例如ARIMA。机器学习模型也经常被使用，例如孤立森林，或者在异常值比例较高时，使用专门的方法，如**季节性混合极端学生化偏差**（**SH-ESD**）。我们在[*第二章*](B18568_02.xhtml#_idTextAnchor044)中看到过使用孤立森林进行异常检测的代码示例，并讨论了监督、无监督、半监督和混合方法。'
- en: Another model selection criterion, which we will look at next, is the statistical
    nature of the time series.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个模型选择标准是我们接下来将讨论的时间序列的统计性质。
- en: Nature of time series
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间序列的性质
- en: 'The nature of the time series, that is, its statistical properties, influences
    the choice of model. Models are researched and developed to work well, if at all,
    based on specific assumptions about the nature of the time series, which then
    determines their applicability. We will focus in this section on applicability
    and skip definitions, assuming that, by now, you are familiar with the terms we
    will use in this section, based on the introduction in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016)
    and code examples in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列的性质，即其统计特性，影响模型的选择。模型的研究和开发通常是基于对时间序列特性的特定假设，从而决定它们的适用性。本节将专注于适用性，并跳过定义，假设到目前为止，你已经熟悉我们将在本节中使用的术语，这些术语基于[*第一章*](B18568_01.xhtml#_idTextAnchor016)的介绍和[*第六章*](B18568_06.xhtml#_idTextAnchor116)中的代码示例：
- en: '**Stationary** time series can be modeled with ARIMA, which assumes stationarity.
    An example of a stationary time series is the daily percentage returns of a stock
    over a 3-year period. Assuming no significant structural changes in the market,
    stock returns tend to fluctuate around a stable mean with consistent variance.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平稳**时间序列可以使用ARIMA建模，ARIMA假设序列是平稳的。平稳时间序列的一个例子是某股票在三年期间的日收益百分比。假设市场没有发生显著的结构性变化，股票收益通常围绕稳定的均值波动，并具有一致的方差。'
- en: Non-stationary time series can be converted to stationary, for example, by differencing,
    as seen in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116). The differenced series
    can then be used with such models. Alternatively, use Prophet, or Machine Learning
    models for non-stationary series. An example of a non-stationary time series is
    the monthly unemployment rate, which possibly has a trend, and cyclical patterns
    related to economic conditions.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非平稳时间序列可以通过差分转化为平稳序列，正如在[*第六章*](B18568_06.xhtml#_idTextAnchor116)中看到的那样。差分后的序列可以与这些模型一起使用。或者，使用Prophet或机器学习模型来处理非平稳序列。非平稳时间序列的一个例子是月度失业率，它可能具有趋势，并与经济状况相关的周期性模式。
- en: '**Seasonal** time series require models that handle seasonality, such as SARIMA,
    ETS, Prophet, or Machine Learning models. We have seen this in action with the
    coding example in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044) to forecast
    temperature using Prophet.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**季节性**时间序列需要能够处理季节性的模型，如SARIMA、ETS、Prophet或机器学习模型。我们在[*第2章*](B18568_02.xhtml#_idTextAnchor044)中的编码示例中已经看到，使用Prophet预测温度的应用。'
- en: '**Trends** in time series can impact the performance of certain models, such
    as ARIMA. In this case, similarly to stationarity, we can remove the trend component
    by differencing, as per the code example in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116).
    ARIMA can then be used. Alternatively, use models that can handle trends, such
    as trend models, ETS, Prophet, or Machine Learning.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列中的**趋势**可能会影响某些模型的表现，比如ARIMA。在这种情况下，类似于平稳性，我们可以通过差分去除趋势成分，如[*第6章*](B18568_06.xhtml#_idTextAnchor116)中的代码示例所示。然后可以使用ARIMA模型。或者，使用能够处理趋势的模型，如趋势模型、ETS、Prophet或机器学习。
- en: '**Volatility** in time series can be handled with models such as **Generalized
    Autoregressive Conditional Heteroskedasticity** (**GARCH**), **Stochastic Volatility
    GARCH** (**SV-GARCH**), or Machine Learning. Common use cases for these models
    are forecasting and risk management in highly volatile financial markets and other
    domains.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**波动性**时间序列可以通过模型处理，如**广义自回归条件异方差性**（**GARCH**）、**随机波动性GARCH**（**SV-GARCH**）或机器学习。这些模型常用于高度波动的金融市场以及其他领域的预测和风险管理。'
- en: '**Linearity** of the relationship in the data means that linear models such
    as ARIMA are suitable. An example of a linear time series is daily temperature,
    where today’s temperature can be predicted by a linear combination of the temperatures
    from the previous two days plus some random error.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据关系的**线性**意味着线性模型（如ARIMA）适用。一个线性时间序列的例子是每日气温，其中今天的气温可以通过前两天气温的线性组合加上一些随机误差来预测。
- en: In the case of non-linear patterns, Machine Learning models with neural networks
    are preferable. An example of a non-linear time series is if a stock price follows
    one relationship if below a certain threshold (say 100) and follows a different
    relationship if it’s above that threshold.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在非线性模式的情况下，具有神经网络的机器学习模型更为合适。一个非线性时间序列的例子是，当股票价格低于某个阈值（比如100）时遵循一种关系，而当其高于该阈值时则遵循另一种关系。
- en: The volume and frequency of data to analyze, discussed next, is another property
    of time series that influences model selection.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析的量和频率（接下来会讨论）是影响模型选择的时间序列的另一个特性。
- en: Volume and frequency of data
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据的量和频率
- en: 'The volume and frequency of data impact the computational power required and
    the duration of the analysis. The combination of these factors determines the
    choice of model to use. We will discuss volume and frequency here, and the other
    two factors in the following section:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的量和频率会影响所需的计算能力和分析所需的时间。它们的组合决定了选择何种模型进行分析。我们将在这里讨论数据的量和频率，而另外两个因素将在下一个部分进行讨论：
- en: '**Small datasets** can be analyzed with statistical models such as ARIMA and
    ETS. These are simple models that work well with smaller datasets. An example
    of a small dataset is the daily sales for a store over the past few years.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小数据集**可以使用统计模型进行分析，例如ARIMA和ETS。这些是适用于较小数据集的简单模型。一个小数据集的例子是过去几年某商店的每日销售数据。'
- en: '**Large datasets** are a good match for Machine Learning models such as gradient
    boosting and LSTM. This works in both ways: in terms of processing capability
    and scalability of ML models for large datasets, and the substantial amount of
    data needed for model training to avoid overfitting. ML models can learn complex
    patterns present in large datasets at the cost of more computational resources.
    Examples of large datasets are minute-by-minute stock prices or sensor data over,
    say, the past 5 years.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大数据集**非常适合机器学习模型，如梯度提升和LSTM。这两者之间是相辅相成的：一方面是机器学习模型在处理大数据集时的计算能力和可扩展性，另一方面是需要大量数据进行模型训练以避免过拟合。机器学习模型可以学习大数据集中的复杂模式，但需要更多的计算资源。大数据集的例子包括分钟级股票价格或过去五年的传感器数据。'
- en: 'As we will see in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151), we can scale
    models to large datasets by using the distributed computing capabilities of Apache
    Spark:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第8章*](B18568_08.xhtml#_idTextAnchor151)中将看到的，我们可以通过利用Apache Spark的分布式计算能力，将模型扩展到大数据集：
- en: '**Low-frequency** time series, such as daily, weekly, monthly, quarterly, or
    annual, are usually small in size. As discussed before about small datasets, ARIMA
    and ETS are usually good choices for such datasets.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低频率**时间序列，如日度、周度、月度、季度或年度，通常规模较小。如前所述关于小数据集，ARIMA和ETS通常是这类数据集的好选择。'
- en: '**High-frequency** time series are likely to have rapid changes, noise, volatility,
    and heteroskedasticity, which can be handled with models such as GARCH, often
    used for financial time series.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高频率**时间序列可能具有快速变化、噪声、波动性和异方差性，可以使用诸如GARCH这样的模型来处理，这通常用于金融时间序列。'
- en: If the analysis is required at a lower frequency than the data arrival rate,
    the high-frequency series can be converted to low frequency by resampling and
    aggregation, as discussed in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116).
    Resampling decreases the size of the dataset while smoothing out the noise and
    volatility. This opens the possibility of using models suited for low-frequency
    time series, as discussed earlier.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要在数据到达速率以下的较低频率上进行分析，则可以通过重新采样和聚合将高频序列转换为低频序列，如[*第6章*](B18568_06.xhtml#_idTextAnchor116)中所讨论的。重新采样会减少数据集的大小，同时平滑噪声和波动性。这打开了使用适合低频时间序列模型的可能性，正如之前讨论的那样。
- en: Diminishing value of high-frequency data
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 高频率数据的减值
- en: We discussed frequency here as pertaining to the time interval between consecutive
    data points in the time series, also referred to as granularity. Another consideration
    for high-frequency data is the requirement that the analysis also be done at high
    frequency. This is due to the quickly diminishing value of high-frequency data
    over time. Consider how real-time stock tick changes are critical at the moment
    they occur but become less relevant just a few hours later. In this scenario,
    the model must be capable of performing extremely rapid computations, potentially
    in real time.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论频率，指的是时间序列中连续数据点之间的时间间隔，也称为粒度。高频数据的另一个考虑因素是分析也需要高频率进行。这是由于高频数据随时间的快速减值。考虑到实时股票tick变化在发生时的关键性，但几小时后变得不那么相关。在这种情况下，模型必须能够进行极快的计算，潜在地实时进行。
- en: Higher volume and frequency of data require more computational resources, which
    we will cover next.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的数据量和频率需要更多的计算资源，这将在接下来进行讨论。
- en: Computational constraints
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算约束
- en: 'Like any other project, time series analysis occurs within a budget. This means
    that the amount of resources available, including the computing power to execute
    the analysis process, is constrained. At the same time, we know that higher volume
    and frequency of data require more computational resources. We also must factor
    in how fast the analysis needs to be completed for the outcome to be useful. With
    these constraints in mind, let’s investigate the choice of model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 像任何其他项目一样，时间序列分析也是在预算内进行的。这意味着可用的资源量，包括执行分析过程的计算能力，是受限制的。同时，我们知道更高的数据量和频率需要更多的计算资源。我们还必须考虑分析需要多快才能完成，以使结果有用。在考虑这些约束条件的同时，让我们来探讨模型的选择：
- en: '**Limited computation** resources mean that we may have to consider a combination
    of dataset size reduction, with resampling, and simpler models such as ARIMA or
    ETS. Machine learning models, while better at detecting complex patterns and with
    larger datasets, usually require more computation resources.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的计算**资源意味着我们可能需要考虑通过重新采样来减少数据集的大小，并使用ARIMA或ETS等简单模型的组合。机器学习模型虽然能更好地检测复杂模式和更大的数据集，但通常需要更多的计算资源。'
- en: '**Fast analysis** requires using faster models for training and prediction.
    Models such as ARIMA or ETS are, again, good candidates for smaller datasets.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速分析**需要使用更快的模型进行训练和预测。像ARIMA或ETS这样的模型，对于较小的数据集再次是很好的选择。'
- en: 'If fast analysis is required for a large dataset, options include the following:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果需要对大型数据集进行快速分析，则可以选择以下选项：
- en: Scaling out using the distributed processing of Apache Spark clusters on large
    datasets, which we will cover in the next chapter.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍使用Apache Spark集群的分布式处理来扩展大型数据集。
- en: Resampling to convert to a smaller dataset size, with the use of simpler models
    such as ARIMA or ETS.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新采样以将数据集大小转换为更小，并使用ARIMA或ETS等简单模型。
- en: Using Machine Learning models with the following caveats. The training and tuning
    stage will be slower for larger datasets. The prediction speed can be improved
    by using more computation resources, which of course comes at a higher cost. Note
    that training, tuning, and prediction speed can also be improved by using the
    distributed processing of Apache Spark, as we will see in the next chapter.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习模型时的注意事项：对于较大的数据集，训练和调优阶段会变慢。通过使用更多的计算资源可以提高预测速度，但这当然会带来更高的成本。值得注意的是，训练、调优和预测速度也可以通过使用Apache
    Spark的分布式处理来提高，正如我们将在下一章看到的那样。
- en: '**Cost of compute resources** is another important factor that may limit the
    use of compute-intensive models. While the simpler statistical models can run
    on cheaper standard resources, Deep Learning models may require more expensive
    GPUs on high-performance hardware.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算资源成本**是另一个可能限制使用计算密集型模型的重要因素。虽然较简单的统计模型可以在较便宜的标准资源上运行，但深度学习模型可能需要在高性能硬件上使用更昂贵的GPU。'
- en: After considering how computational requirements influence the choice of models,
    we will now consider how model accuracy, complexity, and interpretability determine
    which model to use.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑了计算需求如何影响模型选择后，我们将进一步考虑模型准确性、复杂性和可解释性如何决定使用哪个模型。
- en: Model accuracy, complexity, and interpretability
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型准确性、复杂性和可解释性
- en: 'Some of the other factors that are considered for model selection are model
    accuracy, complexity, and interpretability:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型选择时需要考虑的其他因素包括模型准确性、复杂性和可解释性：
- en: '**Model accuracy** is wrongly seen as the determining factor for model selection
    in many cases. Accuracy has been presented at the end of the list of selection
    criteria on purpose to highlight the importance of considering other factors as
    well. The best model is not always the most accurate one. It is the one that delivers
    the most ROI for the use case.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型准确性**在许多情况下被错误地视为模型选择的决定性因素。准确性被故意列在选择标准的末尾，目的是强调同时考虑其他因素的重要性。最好的模型不一定是最准确的模型，而是为特定应用场景带来最大投资回报的模型。'
- en: When high accuracy is needed, especially in forecasting, more complex models
    such as SARIMAX or Deep Learning may be necessary. Hyperparameter tuning is used
    as part of the development process to further improve accuracy, but this comes
    at the cost of additional computations.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当需要高准确性时，特别是在预测中，可能需要更复杂的模型，如SARIMAX或深度学习。超参数调优作为开发过程的一部分，用于进一步提高准确性，但这会增加计算开销。
- en: '**Complexity** and **interpretability** usually conflict. The need for higher
    accuracy leads to the use of more complex models, which are then harder to interpret
    and often referred to as black boxes.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**和**可解释性**通常是相互冲突的。对更高准确性的需求会导致使用更复杂的模型，而这些模型通常更难以解释，常被称为黑箱模型。'
- en: If interpretability is crucial, prefer simpler models such as ARIMA or ETS,
    which have the added benefit of lower compute requirements. Tree-based models
    such as GBM or **Tree-Based Pipelines for Time Series** (**TSPi**) offer a good
    balance of accuracy and compute requirement, while simpler tree-based models offer
    interpretability.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果可解释性至关重要，可以选择较简单的模型，如ARIMA或ETS，这些模型还具有计算资源需求较低的额外优势。基于树的模型，如GBM或**时间序列树形管道**（**TSPi**），在准确性和计算需求之间提供了良好的平衡，而较简单的树形模型则提供了可解释性。
- en: If the data exhibits complex patterns and high accuracy is crucial, there may
    not be many options, and we may have to use complex models, with a trade-off on
    compute and interpretability.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据表现出复杂的模式且高准确性至关重要，可能没有太多选择，我们可能需要使用复杂的模型，这会在计算资源和可解释性上做出权衡。
- en: Overview of model selection
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型选择概述
- en: 'To conclude on model selection, there are a few points worth noting:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型选择，有几个要点值得注意：
- en: Statistical models such as ARIMA are based on assumptions about the nature of
    the time series, requiring statistical tests and possibly additional pre-processing
    to convert the series before using the model.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计模型如ARIMA基于对时间序列的假设，需进行统计检验，并可能需要额外的预处理，以便在使用模型之前转换序列。
- en: Prophet and Machine Learning models are more broadly applicable but have additional
    complexity and compute requirements.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prophet 和机器学习模型更广泛适用，但具有额外的复杂性和计算要求。
- en: The models mentioned in this section are provided as examples applicable to
    the criteria discussed. Other models, from a growing list of publicly available
    models and approaches, can and should be tested. Finding the best model is an
    experimentation and iterative process, dependent on one’s context.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本节中提到的模型作为示例，适用于讨论的标准。其他模型，来自不断增长的公开可用模型和方法列表，可以并且应该被测试。找到最佳模型是一个实验和迭代的过程，取决于具体的应用场景。
- en: As we have seen in this section on selection criteria, several factors influence
    the choice of models and determine which ones to invest more effort in. Which
    factors are most important depends on the project context and the use case. The
    best model to choose is the one resulting in the highest ROI, requiring a trade-off
    between the different factors discussed here.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在选择标准部分所看到的，多个因素会影响模型的选择，并决定在哪些方面投入更多的精力。哪些因素最为重要取决于项目的背景和使用场景。最佳的模型选择是能带来最高投资回报率的模型，这需要在这里讨论的不同因素之间进行权衡。
- en: At this point, with the models selected, we are ready to move on to the next
    development step, which is to train the model on our time series data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在此时，选定了模型后，我们已准备好进入下一开发步骤，即在我们的时间序列数据上训练模型。
- en: Development and testing
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发与测试
- en: 'In this section, we will compare forecasting performance across different categories
    of models: statistical, classical Machine Learning, and Deep Learning. We will
    use six different models: SARIMA, LightGBM, LSTM, NBEATS, NHITS, and NeuralProphet.
    These models are chosen for their wide and proven adoption and ease of access
    and use.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将比较不同类别模型的预测性能：统计模型、经典机器学习模型和深度学习模型。我们将使用六种不同的模型：SARIMA、LightGBM、LSTM、NBEATS、NHITS和NeuralProphet。这些模型因其广泛且经过验证的应用以及易于访问和使用而被选中。
- en: 'We will proceed with the following constraints:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续执行以下限制条件：
- en: Use of the default model hyperparameters whenever possible for comparison and
    minimize tuning to a few cases, which will be explained
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能使用默认模型超参数进行比较，并将调整限制在少数几个案例中，具体内容将在后文说明。
- en: The complete execution, from data loading to model training, testing, and forecasting,
    will be limited to under 15 minutes
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的执行过程，从数据加载到模型训练、测试和预测，将限制在15分钟以内。
- en: The computing resource used will also be constrained to the Databricks Community
    Edition compute as per *Figure 7**.1*, with 15.3 GB of memory and 2 CPU cores
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所使用的计算资源将受限于Databricks社区版计算资源，如*图7.1*所示，具有15.3 GB的内存和2个CPU核心。
- en: '![](img/B18568_07_01.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_01.jpg)'
- en: 'Figure 7.1: Databricks Community Edition compute resource'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：Databricks社区版计算资源
- en: We all commonly face time and resource constraints in our real-life projects.
    This section also aims to give you the tools to work within these limits.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实际项目中，我们常常面临时间和资源的限制。本节还旨在为您提供在这些限制条件下工作的工具。
- en: Single-threaded, multi-threaded, and clustering
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 单线程、多线程和集群
- en: We will use `Pandas` and `NumPy` in the code examples in this chapter. `Pandas`
    is single-threaded in terms of the use of a CPU core. `NumPy` is multi-threaded
    by default, so it makes use of multiple CPU cores in parallel. Both are bound
    to a single machine and do not leverage the multi-machine Spark clustering capability.
    We will address this limitation in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151),
    which covers scaling. As a lot of the existing code examples, you will find use
    `Pandas` and `NumPy`, it is important to start with these libraries as a foundation.
    We will then, in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151), cover how to
    convert the single-machine code to leverage Spark clustering capabilities.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的代码示例中，我们将使用`Pandas`和`NumPy`。`Pandas`在使用CPU核心时是单线程的，`NumPy`默认是多线程的，因此它会并行使用多个CPU核心。两者都绑定到单一机器上，无法利用多机器的Spark集群能力。我们将在[*第8章*](B18568_08.xhtml#_idTextAnchor151)中讨论如何解决这一限制，该章节涉及扩展。在很多现有的代码示例中，你会发现使用了`Pandas`和`NumPy`，因此从这些库开始作为基础非常重要。然后，在[*第8章*](B18568_08.xhtml#_idTextAnchor151)中，我们将讨论如何将单机代码转换为利用Spark集群能力的代码。
- en: The time series data that will be used for this section is an extended version
    of that used in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044) for the energy
    consumption of a household. We will use the same time series for all the models
    we develop in the rest of this chapter. The dataset is in `ts-spark_ch7_ds1_25mb.csv`
    in the `ch7` folder. As this is a new dataset, we will go through the steps of
    exploring the data as part of the next section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用的时间序列数据是[**第 2 章**](B18568_02.xhtml#_idTextAnchor044)中用于家庭能量消耗的扩展版本。我们将在本章余下的所有模型中使用相同的时间序列。数据集位于`ch7`文件夹中的`ts-spark_ch7_ds1_25mb.csv`。由于这是一个新数据集，我们将在下一节中通过探索数据的步骤进行介绍。
- en: Data exploration
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据探索
- en: In this section, we want to check the stationarity, seasonality, and autocorrelation
    in the dataset. This is a crucial step in our understanding of the nature of the
    time series.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们要检查数据集中的平稳性、季节性和自相关。这是理解时间序列特性的重要步骤。
- en: 'The code for this section is in `ts-spark_ch7_1e_sarima_comm.dbc`. We import
    the code into Databricks Community Edition, as per the approach explained in the
    *Hands-on: Loading and visualizing time series* section of [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码位于`ts-spark_ch7_1e_sarima_comm.dbc`。我们按照[**第 1 章**](B18568_01.xhtml#_idTextAnchor016)中“实践操作：加载和可视化时间序列”部分的说明，将代码导入Databricks社区版。
- en: 'The code URL is as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的URL如下：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc)'
- en: The first part of the code loads and prepares the data. We will not go into
    the details of this part here as we already covered data preparation in [*Chapter
    5*](B18568_05.xhtml#_idTextAnchor103), and you can refer to the code in the notebook.
    The data exploration part is, however, pertinent to this chapter, so let’s explore
    this further next, starting with the stationarity check.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一部分加载并准备数据。我们在这里不详细讲解这部分内容，因为我们已经在[**第 5 章**](B18568_05.xhtml#_idTextAnchor103)中涵盖了数据准备的内容，你可以参考笔记本中的代码。然而，数据探索部分与本章相关，因此让我们接下来进一步探索，从平稳性检查开始。
- en: Stationarity
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平稳性
- en: 'We can check whether the energy consumption time series is stationary by running
    the **Augmented Dickey-Fuller** (**ADF**) test on the data with the following
    code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行**增强型迪基-富勒**（**ADF**）测试，使用以下代码来检查能量消耗时间序列是否平稳：
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This gives the following ADF statistics:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下ADF统计量：
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As the ADF statistic is less than the critical values and the p-value is less
    than 0.05, we can conclude that the time series is stationary.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ADF统计量小于临界值，且p值小于0.05，我们可以得出结论，时间序列是平稳的。
- en: Seasonality
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 季节性
- en: 'We can check on the seasonality with the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码检查季节性：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This gives the seasonal decomposition in *Figure 7**.2*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了*图 7**.2*中的季节性分解。
- en: '![](img/B18568_07_02.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_02.jpg)'
- en: 'Figure 7.2: Seasonal decomposition'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：季节性分解
- en: As the pattern repeats every 24 hours, we can conclude that the time series
    has a daily seasonality.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模式每24小时重复一次，我们可以得出结论，时间序列具有日常季节性。
- en: Autocorrelation
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自相关
- en: 'We can check on the autocorrelation and partial autocorrelation with the following
    code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码检查自相关和偏自相关：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This gives the autocorrelation plot in *Figure 7**.3*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了*图 7**.3*中的自相关图。
- en: '![](img/B18568_07_03.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_03.jpg)'
- en: 'Figure 7.3: Autocorrelation (y axis) at different lags (x axis)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：不同滞后（x轴）下的自相关（y轴）
- en: 'We can see the high autocorrelation at the lower lag values, including lag
    1, and at lag 12, as well as the effect of seasonality at lag 24\. This makes
    sense when we consider the following typical patterns of energy consumption in
    a household:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在较低的滞后值（包括滞后1）和滞后12时有较高的自相关性，以及在滞后24时季节性的影响。考虑到家庭中典型的能量消耗模式，这一点是合理的：
- en: Moments of active energy use, for example for cooking, washing, or use of the
    television, are likely to go over an hour (lag 1)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比如做饭、洗衣或看电视等活跃能量使用的时段，很可能会超过一个小时（滞后1）
- en: The mornings and evenings (lag 12) are usually peaks in activity
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 早晨和晚上（滞后12）通常是活动的高峰期
- en: Daily routines mean that we have similar periods of activities every 24 hours
    (lag 24)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日常例行活动意味着我们每24小时会有相似的活动周期（滞后24）
- en: '![](img/B18568_07_04.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_04.jpg)'
- en: 'Figure 7.4: Partial autocorrelation'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：偏自相关
- en: The PACF plot shows high partial autocorrelation at lag 1 and noticeable partial
    autocorrelation around lag 10 and lag 23\. This is in line with the typical patterns
    of energy consumption in a household we mentioned.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: PACF 图显示在滞后 1 时有较高的偏自相关，并在滞后 10 和滞后 23 附近有明显的偏自相关。这与我们提到的家庭能源消费的典型模式一致。
- en: Statistical model – SARIMA
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计模型 – SARIMA
- en: The first model we will cover is SARIMA, which extends the ARIMA model by incorporating
    seasonal components. While ARIMA models address aspects such as autocorrelation,
    differencing for stationarity, and moving averages, SARIMA adds the handling of
    seasonal patterns in the data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第一个模型是 SARIMA，它通过加入季节性组件扩展了 ARIMA 模型。虽然 ARIMA 模型解决了自相关、差分平稳性和移动平均等问题，SARIMA
    在此基础上还考虑了数据中的季节性模式。
- en: 'The code for this section is in `ts-spark_ch7_1e_sarima_comm.dbc`. We import
    the code into Databricks Community Edition, as per the approach explained in the
    *Hands-on: Loading and visualizing time series* section of [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码位于 `ts-spark_ch7_1e_sarima_comm.dbc` 文件中。我们按照 [*第一章*](B18568_01.xhtml#_idTextAnchor016)
    中 *动手实践：加载和可视化时间序列* 部分的说明，将代码导入 Databricks 社区版。
- en: 'The code URL is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 URL 如下：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc)'
- en: Development and tuning
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发与调优
- en: 'For model development, we separated the last 48 hours of the dataset from the
    training data with the following code. This will be used for testing afterward.
    We use the rest for training:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型开发过程中，我们使用以下代码将数据集的最后 48 小时与训练数据分开。这将用于后续的测试，其他部分将用于训练：
- en: '[PRE4]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will discuss two methods combining training and tuning to train the model
    and find the best parameters: `auto_arima` and `ParameterGrid`.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论两种结合训练和调优的方法，用于训练模型并找到最佳参数：`auto_arima` 和 `ParameterGrid`。
- en: Auto ARIMA
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Auto ARIMA
- en: With the auto ARIMA approach, we want to automatically find the model parameters
    that minimize the `pmdarima` library to demonstrate the auto ARIMA approach. As
    this is a compute-intensive operation, and we want to keep to the time (15 minutes)
    and resource (Databricks Community Edition) constraints explained previously,
    we will limit the dataset to the last `300` data points.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 auto ARIMA 方法时，我们希望自动找到最小化 `pmdarima` 库的模型参数，以演示 auto ARIMA 方法。由于这是一个计算密集型操作，我们希望保持之前解释的时间（15
    分钟）和资源（Databricks 社区版）限制，因此我们将数据集限制为最后 `300` 个数据点。
- en: 'The code to use `pmdarima` is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pmdarima` 的代码如下：
- en: '[PRE5]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following code output shows the step-by-step search for the parameters
    minimizing the AIC. This will be the best set of model parameters to use with
    the ARIMA model to forecast this household’s energy consumption:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码输出展示了逐步搜索最小化 AIC 的参数集。这将是用于 ARIMA 模型的最佳参数集，用于预测家庭的能源消费：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that while this is the best set of model parameters, we may find, given
    the time and resource constraints, that we may be able to find a better model
    with a longer run of the algorithm.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然这是一组最佳的模型参数，但考虑到时间和资源的限制，我们可能会发现，通过更长时间运行算法，我们能够找到更好的模型。
- en: ParameterGrid
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ParameterGrid
- en: With the `ParameterGrid` approach, we will sweep one by one through a list of
    parameter combinations to find the model parameters that minimize the AIC.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `ParameterGrid` 方法时，我们将逐一遍历参数组合列表，以找到最小化 AIC 的模型参数。
- en: 'The code to use `ParameterGrid` is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `ParameterGrid` 的代码如下：
- en: '[PRE7]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: While both auto ARIMA and `ParamaeterGrid` are similar in terms of minimizing
    AIC, auto ARIMA is much simpler to use with only 1 line of code.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 auto ARIMA 和 `ParamaeterGrid` 在最小化 AIC 方面相似，但 auto ARIMA 使用起来要简单得多，仅需一行代码。
- en: After the SARIMA model is trained, we will next test the model forecasting.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SARIMA 模型训练完成后，我们将接下来进行模型预测测试。
- en: Testing and forecasting
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试与预测
- en: 'We use the model to forecast the test dataset with the `predict` function,
    one period at a time, updating the model with the actual value after every time
    forecast. This iterative approach converts single-step forecasting in `forecast_step`
    into multi-step forecasting:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用模型通过 `predict` 函数预测测试数据集，每次预测一个周期，每次预测后更新模型的实际值。这种迭代方法将 `forecast_step`
    中的单步预测转化为多步预测：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can then plot the forecast against the actual values in *Figures 7.5* and
    *7.6*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以在*图 7.5*和*图 7.6*中绘制预测值与实际值的对比图。
- en: '![](img/B18568_07_05.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_05.jpg)'
- en: 'Figure 7.5: SARIMA Forecast vs Actuals (training and testing)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：SARIMA 预测与实际值（训练与测试）
- en: We zoom in on the testing period in *Figure 7**.6* for a visual comparison of
    the forecast and actuals.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图 7.6*中放大了测试期，以便直观比较预测值与实际值。
- en: '![](img/B18568_07_06.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_06.jpg)'
- en: 'Figure 7.6: SARIMA Forecast vs Actuals (zoom on test data)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6：SARIMA 预测与实际值（缩放至测试数据）
- en: While visualizing the graphs gives us an idea of the forecasting capability
    of the model, we need quantifiable metrics on how good the model is. These metrics
    will also allow us to compare forecasting accuracy with other models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可视化图表可以帮助我们了解模型的预测能力，但我们仍然需要定量的指标来评估模型的好坏。这些指标还将帮助我们与其他模型进行预测准确度的比较。
- en: 'There are several metrics available for time series forecasting. We will show
    the use of the following three in this chapter, to highlight how different metrics
    serve different objectives:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测有多种可用的评估指标。本章将展示以下三种指标的使用，突出它们如何服务于不同的目标：
- en: '**Mean Squared Error** (**MSE**) measures the average squared differences between
    the forecasted (F) and actual (A) values. It works well when we want to penalize
    large errors. However, it is sensitive to outliers because the squaring of errors
    gives importance to large discrepancies.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）度量了预测值（F）与实际值（A）之间差值的平方平均值。当我们希望惩罚较大误差时，它效果很好。然而，由于平方误差会赋予较大差异更大的权重，因此它对异常值敏感。'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>M</mi><mi>S</mi><mi>E</mi><mo
    mathvariant="italic">=</mo><mfrac><mn mathvariant="italic">1</mn><mi>n</mi></mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>t</mi><mo
    mathvariant="italic">=</mo><mn mathvariant="italic">1</mn></mrow><mi>n</mi></msubsup><msup><mrow><mo>(</mo><msub><mi>F</mi><mi>t</mi></msub><mo
    mathvariant="italic">−</mo><msub><mi>A</mi><mi>t</mi></msub><mo>)</mo></mrow><mn
    mathvariant="italic">2</mn></msup></mrow></mrow></mrow></math>](img/2.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>M</mi><mi>S</mi><mi>E</mi><mo
    mathvariant="italic">=</mo><mfrac><mn mathvariant="italic">1</mn><mi>n</mi></mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>t</mi><mo
    mathvariant="italic">=</mo><mn mathvariant="italic">1</mn></mrow><mi>n</mi></msubsup><msup><mrow><mo>(</mo><msub><mi>F</mi><mi>t</mi></msub><mo
    mathvariant="italic">−</mo><msub><mi>A</mi><mi>t</mi></msub><mo>)</mo></mrow><mn
    mathvariant="italic">2</mn></msup></mrow></mrow></mrow></math>](img/2.png)'
- en: '**Symmetric Mean Absolute Percentage Error** (**SMAPE**) is the average of
    the absolute differences between forecasted (F) and actual (A) values. It is expressed
    as a percentage over half of the sum of absolute values of actual and forecasted
    values. SMAPE adjusts to the scale of the data, making it suitable for comparisons
    across different datasets. Due to its symmetric scaling, it is less sensitive
    to extreme values.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对称平均绝对百分比误差**（**SMAPE**）是预测值（F）与实际值（A）之间绝对差值的平均值。它以百分比的形式表示，基于实际值和预测值的绝对值之和的一半。SMAPE
    可调整数据的尺度，使其适用于不同数据集之间的比较。由于其对称缩放，它对极端值的敏感度较低。'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>S</mi><mi>M</mi><mi>A</mi><mi>P</mi><mi>E</mi><mo
    mathvariant="italic">=</mo><mfrac><mrow><mn mathvariant="italic">100</mn><mi>%</mi></mrow><mi>n</mi></mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>t</mi><mo
    mathvariant="italic">=</mo><mn mathvariant="italic">1</mn></mrow><mi>n</mi></msubsup><mfrac><mrow><mo>|</mo><msub><mi>F</mi><mi>t</mi></msub><mo
    mathvariant="italic">−</mo><msub><mi>A</mi><mi>t</mi></msub><mo>|</mo></mrow><mrow><mo>(</mo><mo>|</mo><msub><mi>A</mi><mi>t</mi></msub><mo>|</mo><mo
    mathvariant="italic">+</mo><mo>|</mo><msub><mi>F</mi><mi>t</mi></msub><mo>|</mo><mo>)</mo><mo
    mathvariant="italic">/</mo><mn mathvariant="italic">2</mn></mrow></mfrac></mrow></mrow></mrow></math>](img/3.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>S</mi><mi>M</mi><mi>A</mi><mi>P</mi><mi>E</mi><mo
    mathvariant="italic">=</mo><mfrac><mrow><mn mathvariant="italic">100</mn><mi>%</mi></mrow><mi>n</mi></mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>t</mi><mo
    mathvariant="italic">=</mo><mn mathvariant="italic">1</mn></mrow><mi>n</mi></msubsup><mfrac><mrow><mo>|</mo><msub><mi>F</mi><mi>t</mi></msub><mo
    mathvariant="italic">−</mo><msub><mi>A</mi><mi>t</mi></msub><mo>|</mo></mrow><mrow><mo>(</mo><mo>|</mo><msub><mi>A</mi><mi>t</mi></msub><mo>|</mo><mo
    mathvariant="italic">+</mo><mo>|</mo><msub><mi>F</mi><mi>t</mi></msub><mo>|</mo><mo>)</mo><mo
    mathvariant="italic">/</mo><mn mathvariant="italic">2</mn></mrow></mfrac></mrow></mrow></mrow></math>](img/3.png)'
- en: '**Weighted Absolute Percentage Error** (**WAPE**) is a normalized measure of
    error, weighing the absolute errors by the actual values. It works well when dealing
    with data of varying magnitudes but is sensitive to high-value errors.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权绝对百分比误差**（**WAPE**）是一个归一化的误差度量，通过实际值加权绝对误差。当处理具有不同大小的数据时，它表现良好，但对大值误差敏感。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>W</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mi
    mathvariant="normal">%</mml:mi></mml:math>](img/4.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>W</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mi
    mathvariant="normal">%</mml:mi></mml:math>](img/4.png)'
- en: 'We will see two different approaches to metrics calculation: metrics calculation
    functions included in the model library, and a separate dedicated metrics calculation
    library.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到两种不同的度量计算方法：模型库中包含的度量计算函数，以及一个独立的专门度量计算库。
- en: Metric functions from the model library
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型库中的度量函数
- en: 'In this approach, we want to use the functions for metrics calculations already
    included in the model library. We will use the `sklearn` and `pmdarima` libraries
    for the metric calculations to demonstrate this in the following code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们希望使用模型库中已经包含的度量计算函数。我们将使用`sklearn`和`pmdarima`库进行度量计算，并在以下代码中演示：
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This gives the following results:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下结果：
- en: '[PRE10]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Separate metrics library
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单独的度量库
- en: 'In this second approach for metrics calculation, we use the `SeqMetrics` library,
    as in the following code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种第二种度量计算方法中，我们使用`SeqMetrics`库，如以下代码所示：
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This gives the following results:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下结果：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This library also provides a nice visualization of all the metrics calculated,
    as in *Figures 7.7* and *7.8*.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 该库还提供了所有计算的度量的可视化，如*图 7.7*和*7.8*所示。
- en: '![](img/B18568_07_07.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_07.jpg)'
- en: 'Figure 7.7: SeqMetrics display of WAPE'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7：WAPE的SeqMetrics显示
- en: '![](img/B18568_07_08.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_08.jpg)'
- en: 'Figure 7.8: SeqMetrics display of SMAPE'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8：SMAPE的SeqMetrics显示
- en: After training and testing our first model, we can move on to the next model,
    which is a classical Machine Learning model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和测试完我们的第一个模型后，我们可以进入下一个模型，这是一个经典的机器学习模型。
- en: Classical Machine Learning model – LightGBM
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经典机器学习模型 – LightGBM
- en: The second model we will cover is **Light Gradient Boosting Machine** (**LightGBM**),
    which is a free open source gradient boosting model. It is based on the tree learning
    algorithm, designed to be efficient and distributed.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍的第二个模型是**Light Gradient Boosting Machine**（**LightGBM**），这是一个免费的开源梯度提升模型。它基于树学习算法，旨在高效且分布式。
- en: The code for this section is in `ts-spark_ch7_1e_lgbm_comm.dbc`. We import the
    code into Databricks Community Edition, as per the approach explained in [*Chapter
    1*](B18568_01.xhtml#_idTextAnchor016).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码在`ts-spark_ch7_1e_lgbm_comm.dbc`中。我们将代码导入到Databricks社区版中，按照[*第1章*](B18568_01.xhtml#_idTextAnchor016)中解释的方法进行操作。
- en: 'The code URL is as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的URL如下：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_lgbm_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_lgbm_comm.dbc)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_lgbm_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_lgbm_comm.dbc)'
- en: Development and tuning
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发与调优
- en: 'For model development, we separated the last 48 hours of the dataset from the
    training data with the following code. This will be used for testing afterward.
    We use the rest for training:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型开发，我们使用以下代码将数据集的最后 48 小时从训练数据中分离出来，用于后续测试。其余部分用于训练：
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will use the `GridSearchCV` method to find the best parameters for the `LGBMRegressor`
    model. `TimeSeriesSplit` is used to split the training dataset for cross-validation,
    respecting the time series nature of the dataset:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `GridSearchCV` 方法为 `LGBMRegressor` 模型寻找最佳参数。`TimeSeriesSplit` 用于根据时间序列特性将训练数据集进行交叉验证划分：
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We find the following best parameters:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们找到了以下最佳参数：
- en: '[PRE15]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Based on the training dataset, this will be the best set of parameters to use
    with the LightGBM model to forecast this household’s energy consumption. We can
    then train the final model with these parameters:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于训练数据集，这将是与 LightGBM 模型预测该家庭能耗时使用的最佳参数集。然后，我们可以用这些参数训练最终模型：
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After the LightGBM model is trained, we will test the model forecasting next.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练好 LightGBM 模型后，我们将进行模型预测测试。
- en: Testing and forecasting
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试与预测
- en: 'We use the model to forecast the test dataset with the `predict` function.
    Note that in this case, we have not had the need to use iterative multi-step forecasting
    code. We have instead used the lag values as input features to the model:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用模型通过 `predict` 函数对测试数据集进行预测。请注意，在此情况下，我们并没有使用迭代式多步预测代码，而是使用了滞后值作为模型的输入特征：
- en: '[PRE17]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can then plot the forecast against the actual values in *Figures 7.8* and
    *7.9*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以在 *图 7.8* 和 *图 7.9* 中将预测值与实际值进行对比。
- en: '![](img/B18568_07_09.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_09.jpg)'
- en: 'Figure 7.9: LightGBM Forecast vs Actuals (training and testing)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9：LightGBM 预测与实际值对比（训练与测试）
- en: We zoom in on the testing period in *Figure 7**.9* for a visual comparison of
    the forecast and actuals.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *图 7.9* 中放大测试期，以便直观比较预测值与实际值。
- en: '![](img/B18568_07_10.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_10.jpg)'
- en: 'Figure 7.10: LightGBM Forecast vs Actuals (zoom on test data)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10：LightGBM 预测与实际值对比（测试数据放大）
- en: 'Based on the forecast and actuals, we can then measure the SMAPE and WAPE,
    getting the following values:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 根据预测值与实际值，我们可以计算 SMAPE 和 WAPE，得到以下值：
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we have trained and tested statistical and classical Machine Learning
    models, we can move on to a third type of model, which is a Deep Learning model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练并测试了统计学和经典机器学习模型，可以进入第三种模型类型——深度学习模型。
- en: Deep Learning model – NeuralProphet
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习模型 - NeuralProphet
- en: The third model we will cover is NeuralProphet, which is a free open source
    Deep Learning model inspired by Prophet, which we used in previous chapters, and
    AR-Net. NeuralProphet is built on PyTorch.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍的第三个模型是 NeuralProphet，它是一个免费的开源深度学习模型，灵感来自于我们在前几章使用过的 Prophet 和 AR-Net。NeuralProphet
    基于 PyTorch 构建。
- en: The code for this section is in `ts-spark_ch7_1e_nprophet_comm.dbc`. We import
    the code into Databricks Community Edition, as per the approach explained in [*Chapter
    1*](B18568_01.xhtml#_idTextAnchor016).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本节代码位于 `ts-spark_ch7_1e_nprophet_comm.dbc` 文件中。我们按照 [*第一章*](B18568_01.xhtml#_idTextAnchor016)
    中的方式将代码导入 Databricks Community Edition。
- en: 'The code URL is as follows: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_nprophet_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_nprophet_comm.dbc)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 代码链接如下：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_nprophet_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_nprophet_comm.dbc)
- en: Note
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the notebook for this example requires Databricks compute DBR 13.3
    LTS ML.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此示例的笔记本需要 Databricks 计算 DBR 13.3 LTS ML。
- en: Development
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发
- en: 'We instantiate a `NeuralProphet` model, specifying with `n_lag` that we want
    to use the past 24 hours for the forecasting. We then train (the `fit` method)
    the model on the training dataset:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化了一个 `NeuralProphet` 模型，并通过 `n_lag` 参数指定我们希望使用过去 24 小时的数据进行预测。然后，我们在训练数据集上训练（`fit`
    方法）该模型：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With these two lines of code sufficient to train the model, we will next test
    the model forecasting.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 只需这两行代码即可训练模型，接下来我们将进行模型预测测试。
- en: Testing and forecasting
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试与预测
- en: 'Before using the model to forecast the test dataset, we need to prepare the
    data for NeuralProphet, similar to how we did previously for Prophet. The required
    format is to have a `ds` column for the date/time and `y` for the forecasting
    target. We can then use the `predict` method. Note that in this case, we have
    not had the need to use iterative multi-step forecasting code. With the lag of
    24 specified as a parameter in the previous code section, NeuralProphet uses a
    sliding window of the past 24 values to forecast the next values:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用模型对测试数据集进行预测之前，我们需要为 NeuralProphet 准备数据，类似于之前为 Prophet 所做的准备。所需的格式是有一个`ds`列用于日期/时间，另一个`y`列用于预测目标。然后，我们可以使用`predict`方法。请注意，在此情况下，我们没有使用迭代的多步预测代码。在前一段代码中指定了滞后
    24 作为参数，NeuralProphet 使用过去 24 个值的滑动窗口来预测下一个值：
- en: '[PRE20]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We plot the forecast against the actual values in *Figures 7.12* and *7.13*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图 7.12*和*图 7.13*中将预测值与实际值进行对比。
- en: '![](img/B18568_07_11.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_11.jpg)'
- en: 'Figure 7.11: NeuralProphet Forecast vs Actuals (training and testing)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：NeuralProphet 预测与实际值对比（训练与测试）
- en: We zoom in on the testing period in *Figure 7**.13* for a visual comparison
    of the forecast and actuals.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图 7.13*中放大测试期，以便进行预测与实际值的视觉对比。
- en: '![](img/B18568_07_12.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_12.jpg)'
- en: 'Figure 7.12: NeuralProphet Forecast vs Actuals (zoom on test data)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12：NeuralProphet 预测与实际值对比（放大测试数据）
- en: 'Based on the forecast and actuals, we can then measure the SMAPE and WAPE,
    getting the following values as a measurement of the accuracy of the model:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 基于预测和实际值，我们可以计算 SMAPE 和 WAPE，并获得以下值来衡量模型的准确性：
- en: '[PRE21]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We will use these metrics to compare the different models we have used in this
    chapter, in the later *Model* *comparison* section.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续的*模型* *比较*部分使用这些指标来比较本章中使用的不同模型。
- en: 'So far, we have trained and tested each type of model: a statistical, a classical
    Machine Learning, and a Deep Learning model. Other examples of commonly used models
    for time series are provided in the book’s GitHub repository:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经训练并测试了每种类型的模型：统计模型、经典机器学习模型和深度学习模型。书中 GitHub 仓库中提供了其他一些常用的时间序列模型示例：
- en: 'Prophet: `ts-spark_ch7_1e_prophet_comm.dbc`'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Prophet: `ts-spark_ch7_1e_prophet_comm.dbc`'
- en: 'LSTM: `ts-spark_ch7_1e_lstm_comm1-cpu.dbc`'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LSTM: `ts-spark_ch7_1e_lstm_comm1-cpu.dbc`'
- en: 'NBEATS and NHITS: `ts-spark_ch7_1e_nbeats-nhits_comm.dbc`'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NBEATS 和 NHITS: `ts-spark_ch7_1e_nbeats-nhits_comm.dbc`'
- en: We encourage you to explore these further.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你进一步探索这些内容。
- en: Having a working model is great but is not sufficient. We also need to be able
    to explain the model we are working with. We will cover this next.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个有效的模型很重要，但还不够。我们还需要能够解释我们使用的模型。接下来我们将介绍这一部分内容。
- en: Explainability
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: Explainability is a key requirement in many cases, such as for financial and
    regulated industries. We will look at how to do this now using a widely used method
    called **Shapley Additive Explanations** (**SHAP**) to explain how the different
    features of the dataset contributed to the prediction.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性在许多情况下都是一个关键要求，例如金融和受监管行业。我们将通过一种广泛使用的方法——**Shapley 加法解释**（**SHAP**）来解释数据集的不同特征如何影响预测结果。
- en: We will use the `TreeExplainer` function of the `shap` library on the final
    model from the *Classical Machine Learning model – LightGBM* section to compute
    the SHAP values, which will give us the impact of each feature on the model output.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`shap`库中的`TreeExplainer`函数，应用于*经典机器学习模型 – LightGBM*部分的最终模型，计算 SHAP 值，从而了解每个特征对模型输出的影响。
- en: '[PRE22]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can then plot the feature importance in *Figure 7**.10*. As expected from
    the data exploration we did in the earlier section, lag 1 and lag 24 are the features
    contributing the most to the forecasting.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以在*图 7.10*中绘制特征重要性。正如我们在前一部分的数据探索中所预期的那样，滞后 1 和滞后 24 是对预测贡献最大的特征。
- en: '![](img/B18568_07_13.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_13.jpg)'
- en: 'Figure 7.13: SHAP – feature importance'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：SHAP – 特征重要性
- en: 'We can go further in the analysis by focusing on a specific forecast with the
    following code, where we want to explain the forecasting for the first value:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码进一步分析，聚焦于某个特定的预测值，在此我们要解释第一个预测值的情况：
- en: '[PRE23]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can see in *Figure 7**.11* the relative contributions of the features, again
    with pre-dominance of lag 1 and 24, and to a lesser extent lag 12\. This is coherent
    with our analysis in the *Data exploration* section, where we established the
    pertinence of these lags in forecasting the energy consumption of a household.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图 7.11*中看到特征的相对贡献，再次呈现滞后1和滞后24的主导地位，滞后12的贡献相对较小。这与我们在*数据探索*部分中的分析一致，在该部分中我们确认了这些滞后项在预测家庭能源消耗中的重要性。
- en: '![](img/B18568_07_14.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_07_14.jpg)'
- en: 'Figure 7.14: SHAP – feature importance (first observation)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14：SHAP—特征重要性（首次观测）
- en: Model comparison
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型比较
- en: Before concluding this chapter, we will compare all the models we have tested
    based on the metrics we measured and the code execution time. The results are
    shown in *Table 7.1*.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，我们将根据我们所测量的指标和代码执行时间对所有测试过的模型进行比较。结果显示在*表 7.1*中。
- en: '| **Model** | **Type** | **SMAPE** | **WAPE** | **Training** | **Tuning** |
    **Testing** | **Total incl. data** **prep.** |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **类型** | **SMAPE** | **WAPE** | **训练** | **调优** | **测试** | **总计（包括数据预处理）**
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| NeuralProphet | DL/Mixed | 41.19 | 0.35 | 60s | - | 1s | 90s |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| NeuralProphet | 深度学习/混合 | 41.19 | 0.35 | 60秒 | - | 1秒 | 90秒 |'
- en: '| LightGBM | Classical ML | 41.46 | 0.39 | 60s | Included | Included | 137s
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| LightGBM | 经典机器学习 | 41.46 | 0.39 | 60秒 | 包含 | 包含 | 137秒 |'
- en: '| SARIMA | Statistical | 43.78 | 0.42 | Included | 420s | 180s | 662s |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| SARIMA | 统计模型 | 43.78 | 0.42 | 包含 | 420秒 | 180秒 | 662秒 |'
- en: '| Prophet | Statistical/Mixed | 47.60 | 0.41 | 2s | - | 1s | 70s |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Prophet | 统计/混合 | 47.60 | 0.41 | 2秒 | - | 1秒 | 70秒 |'
- en: '| NHITS | DL | 54.43 | 0.47 | 35s | - | Included | 433s |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| NHITS | 深度学习 | 54.43 | 0.47 | 35秒 | - | 包含 | 433秒 |'
- en: '| NBEATS | DL | 54.91 | 0.48 | 35s | - | Included | 433s |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| NBEATS | 深度学习 | 54.91 | 0.48 | 35秒 | - | 包含 | 433秒 |'
- en: '| LSTM | DL | 55.08 | 0.48 | 722s | - | 4s | 794s |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 深度学习 | 55.08 | 0.48 | 722秒 | - | 4秒 | 794秒 |'
- en: 'Table 7.1: Model results comparison'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.1：模型结果比较
- en: 'Here are a few observations on the model accuracy:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关于模型准确性的观察：
- en: NeuralProphet and LightGBM gave the best forecasting accuracy with both the
    SMAPE and WAPE metrics. SARIMA was not very far behind.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NeuralProphet和LightGBM在SMAPE和WAPE指标下提供了最佳的预测准确性。SARIMA的表现也不算差。
- en: The Deep Learning models, NBEATS, NHITS, and LSTM, did not have good forecasting
    accuracy when used as single-input models. We encourage you to explore further
    how they can be improved with multiple inputs.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型NBEATS、NHITS和LSTM作为单输入模型时预测准确性较差。我们建议进一步探索如何通过多输入来提升它们的表现。
- en: 'The following is in regard to execution time:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容涉及执行时间：
- en: In all the cases, we kept within the constraint of a total execution of 900s
    (15 minutes) with the 2 CPU cores on a single-node Databricks Community Edition
    cluster. This worked with the 25 MB dataset. We will see in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151)
    how to scale for larger datasets.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有情况下，我们都保持在900秒（15分钟）的总执行时间限制内，使用2个CPU核心在单节点的Databricks社区版集群上运行。这对于25MB的数据集来说是可行的。我们将在[*第8章*](B18568_08.xhtml#_idTextAnchor151)中看到如何为更大的数据集进行扩展。
- en: Prophet, NBEATS, and NHITS had the best execution time, with NeuralProphet and
    LightGBM coming after, still within 1 minute for training, tuning, and testing.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prophet、NBEATS和NHITS的执行时间最佳，NeuralProphet和LightGBM紧随其后，训练、调优和测试时间仍在1分钟以内。
- en: SARIMA had a relatively high execution time, even if we limited the dataset
    to the last 300 observations. This was due to the Auto ARIMA algorithm searching
    for the best hyperparameter, and then the multi-step iterative forecasting code.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使我们将数据集限制为最后300个观测值，SARIMA的执行时间仍然相对较高。这是由于Auto ARIMA算法在搜索最佳超参数时以及多步迭代预测代码的执行。
- en: LSTM had the longest execution time, which can be explained by the use of CPUs
    instead of GPUs, which are much faster for Deep Learning.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM的执行时间最长，这可以通过使用CPU而非GPU来解释，GPU对于深度学习来说要快得多。
- en: The overall conclusion from this model comparison is that NeuralProphet and
    LightGBM are the best choices for the dataset we used, with minimal tuning, and
    for the compute and execution time constraint that we set.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 从这次模型比较的整体结论来看，NeuralProphet和LightGBM是我们使用的数据集的最佳选择，几乎不需要调优，并且符合我们设定的计算和执行时间限制。
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have focused on the core topic of this book, which is the
    development of models for time series analysis, more specifically for forecasting.
    Starting with a review of the different types of models, we then looked at the
    important criteria guiding the choice of the right model to use. In the second
    part of the chapter, we put into practice the development and testing of several
    models, which we then compared on accuracy and execution time.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们重点讨论了本书的核心主题，即时间序列分析模型的开发，特别是预测模型。从回顾不同类型的模型开始，然后介绍了选择合适模型的关键标准。在本章的第二部分，我们实践了多个模型的开发和测试，并根据准确性和执行时间进行了比较。
- en: 'In the next chapter, we will expand on a topic where Apache Spark shines: scaling
    time series analysis to big data.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将扩展一个Apache Spark的优势领域：将时间序列分析扩展到大数据。
- en: Join our community on Discord
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/ds](https://packt.link/ds)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/ds](https://packt.link/ds)'
- en: '![](img/ds_(1).jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ds_(1).jpg)'
- en: 'Part 3: Scaling to Production and Beyond'
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：扩展到生产环境及更远发展
- en: In this last part, we will cover the considerations and practical examples of
    scaling and bringing to production the solutions covered in *Part 2*. We then
    conclude the book with techniques to go further with Apache Spark and time series
    analysis. This guides you to using Databricks and generative AI as part of your
    solutions.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一部分，我们将讨论将第二部分中涉及的解决方案扩展和投入生产时需要考虑的因素和实际案例。随后，我们将以使用Databricks和生成式AI作为解决方案的一部分，来结束本书，并介绍如何进一步推进Apache
    Spark和时间序列分析的应用。
- en: 'This part has the following chapters:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 8*](B18568_08.xhtml#_idTextAnchor151), *Going at Scale*'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B18568_08.xhtml#_idTextAnchor151)，*大规模处理*'
- en: '[*Chapter 9*](B18568_09.xhtml#_idTextAnchor169), *Going to Production*'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B18568_09.xhtml#_idTextAnchor169)，*投入生产*'
- en: '[*Chapter 10*](B18568_10.xhtml#_idTextAnchor190), *Going Further with Apache
    Spark*'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B18568_10.xhtml#_idTextAnchor190)，*进一步使用Apache Spark*'
- en: '[*Chapter 11*](B18568_11.xhtml#_idTextAnchor211), *Recent Developments in Time
    Series Analysis*'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B18568_11.xhtml#_idTextAnchor211)，*时间序列分析的最新发展*'
