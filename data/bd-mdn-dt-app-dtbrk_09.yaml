- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Leveraging Databricks Asset Bundles to Streamline Data Pipeline Deployment
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用 Databricks 资产包简化数据管道部署
- en: This chapter explores a relatively new **continuous integration and continuous
    deployment** ( **CI/CD** ) tool called **Databricks Asset Bundles** ( **DABs**
    ), which can be leveraged to streamline the development and deployment of data
    analytical projects across various Databricks workspaces. In this chapter, we’ll
    dive into the core concept of **DABs** . We’ll demonstrate the practical use of
    DABs through several hands-on exercises so that you feel comfortable developing
    your next data analytics projects as a DAB. Lastly, we’ll cover how DABs can be
    used to increase cross-team collaboration through version control systems such
    as GitHub, and how DABs can be used to simplify even the most complex data analytical
    deployments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了一种相对较新的 **持续集成与持续部署** (**CI/CD**) 工具，称为 **Databricks 资产包** (**DABs**)，它可以用来简化数据分析项目在不同
    Databricks 工作区中的开发和部署。在本章中，我们将深入了解 **DABs** 的核心概念。我们将通过几个实践操作示例来演示 DAB 的实际应用，帮助你熟悉作为
    DAB 开发下一个数据分析项目。最后，我们将介绍如何通过版本控制系统（如 GitHub）使用 DAB 来促进跨团队协作，以及 DAB 如何简化即使是最复杂的数据分析部署。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要内容：
- en: Introduction to Databricks Asset Bundles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 资产包简介
- en: Databricks Asset Bundles in action
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 资产包的应用
- en: Simplifying cross-team collaboration
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化跨团队协作
- en: Versioning and maintenance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本控制与维护
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To follow the examples in this chapter, it’s recommended that you have Databricks
    workspace administrative privileges so that you can deploy DABs to target workspaces.
    You’ll also need to download and install version 0.218.0 or higher of the Databricks
    CLI. All the code samples can be downloaded from this chapter’s GitHub repository
    at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter09](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter09)
    . In this chapter, we will deploy several new workflows, DLT pipelines, notebooks,
    and clusters. It’s estimated that this will consume around 5-10 **Databricks**
    **Units** ( **DBUs** ).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本章中的示例，建议你拥有 Databricks 工作区的管理员权限，以便能够将 DAB 部署到目标工作区。你还需要下载并安装版本为 0.218.0
    或更高版本的 Databricks CLI。所有的代码示例可以从本章的 GitHub 仓库中下载，地址为 [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter09](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter09)。在本章中，我们将部署多个新的工作流、DLT
    管道、笔记本和集群。预计这将消耗大约 5-10 **Databricks** **Units** (**DBUs**)。
- en: Introduction to Databricks Asset Bundles
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks 资产包简介
- en: DABs provide an easy and convenient way to develop your data and **artificial
    intelligence** ( **AI** ) projects together with YAML metadata for declaring the
    infrastructure that goes along with it – just like a bundle. DABs provide data
    engineers with a way to programmatically validate, deploy, and test Databricks
    resources in target workspaces. This may include deploying workspace assets such
    as **Delta Live Tables** ( **DLT** ) pipelines, workflows, notebooks, and more.
    DABs also provide a convenient way to develop, package, and deploy machine learning
    workloads using reusable templates (we’ll cover DAB templates later in the *Initializing
    an asset bundle using templates* section), called MLOps Stacks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DAB 提供了一种简单便捷的方式，可以与 YAML 元数据一起开发你的数据和 **人工智能** (**AI**) 项目，用于声明相关的基础设施—就像一个捆绑包。DAB
    为数据工程师提供了一种程序化验证、部署和测试 Databricks 资源到目标工作区的方法。这可能包括部署工作区资产，如 **Delta Live Tables**
    (**DLT**) 管道、工作流、笔记本等。DAB 还提供了一种便捷的方式来开发、打包和部署机器学习工作负载，使用可重用的模板（我们将在 *使用模板初始化资产包*
    部分介绍 DAB 模板），这些模板称为 MLOps 堆栈。
- en: DABs were designed around the principles of expressing **Infrastructure as Code**
    ( **IaC** ) and benefit from using configuration to drive the deployment of architectural
    components of your data applications. DABs provide a way to check in IaC configuration
    along with data assets such as Python files, Notebooks, and other dependencies.
    DABs can also be an alternative if you feel Terraform (covered in [*Chapter 8*](B22011_08.xhtml#_idTextAnchor185)
    ) is too advanced for your organization’s needs within the context of the Databricks
    Data Intelligence Platform.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: DAB是围绕表达**基础设施即代码**（**IaC**）的原则设计的，利用配置来驱动数据应用程序架构组件的部署。DAB提供了一种将IaC配置与数据资产（如Python文件、笔记本和其他依赖项）一起管理的方式。如果你觉得Terraform（在[*第8章*](B22011_08.xhtml#_idTextAnchor185)中有介绍）对于你组织在Databricks数据智能平台中的需求过于复杂，DAB也可以作为一种替代方案。
- en: 'DABs share some similarities with Terraform in that both are IaC tools that
    give users the ability to define cloud resources and deploy those resources in
    a cloud-agnostic manner. However, there are many differences as well. Let’s compare
    a few of the similarities and differences between DABs and Terraform to get a
    better feeling of when to choose which tool over the other for your organization’s
    needs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DAB与Terraform有一些相似之处，两者都是IaC工具，能够让用户定义云资源并以云中立的方式部署这些资源。然而，它们也有许多不同之处。让我们比较一下DAB和Terraform之间的一些相似性和差异性，以便更好地了解在组织的需求下，何时选择哪个工具：
- en: '![Figure 9.1 – DABs and Terraform are both IaC tools, but they meet very different
    needs](img/B22011_09_001.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – DAB和Terraform都是IaC工具，但它们满足非常不同的需求](img/B22011_09_001.jpg)'
- en: Figure 9.1 – DABs and Terraform are both IaC tools, but they meet very different
    needs
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – DAB和Terraform都是IaC工具，但它们满足的是非常不同的需求
- en: Before we start writing our very first DAB, let’s spend some time getting to
    know the major building blocks of what makes up a DAB configuration file.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编写第一个DAB之前，先花点时间了解一下构成DAB配置文件的主要构建块。
- en: Elements of a DAB configuration file
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DAB配置文件的元素
- en: At the center of a DAB is a YAML configuration file, named **databricks.yml**
    . This configuration file provides engineers with an entry point for configuring
    the deployment of their project’s resources. The file consists of many composable
    building blocks that tell the Databricks **command-line interface** ( **CLI**
    ) what to deploy to a target Databricks workspace and how to configure each resource.
    Each building block accepts different parameters for configuring that component.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: DAB的核心是一个YAML配置文件，名为**databricks.yml**。该配置文件为工程师提供了一个配置其项目资源部署的入口点。该文件由许多可组合的构建块组成，这些构建块告诉Databricks的**命令行界面**（**CLI**）将什么资源部署到目标Databricks工作区，并如何配置每个资源。每个构建块都接受不同的参数来配置该组件。
- en: Later in this chapter, we’ll cover how to decompose the configuration file into
    many YAML files, but for simplicity’s sake, we’ll start with a single YAML file.
    Within this YAML configuration file, we’ll declare our Databricks resources, as
    well as other metadata. These building blocks, or **mappings** , tell the DAB
    tool what Databricks resource to create, and more importantly, what Databricks
    REST API to manipulate to create and configure a Databricks resource.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章稍后，我们将介绍如何将配置文件分解为多个YAML文件，但为了简单起见，我们从单个YAML文件开始。在这个YAML配置文件中，我们将声明我们的Databricks资源以及其他元数据。这些构建块，或**映射**，告诉DAB工具创建什么Databricks资源，更重要的是，告诉DAB工具操作哪个Databricks
    REST API来创建和配置Databricks资源。
- en: 'These mappings can be a variety of Databricks resources. For example, a DAB
    configuration file can contain any combination of the following mappings:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些映射可以是各种Databricks资源。例如，DAB配置文件可以包含以下映射的任意组合：
- en: '| **Mapping Name** | **Required?** | **Description** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **映射名称** | **是否必需？** | **描述** |'
- en: '| **bundle** | Yes | Contains top-level information about the current asset
    bundle, including the Databricks CLI version, existing cluster identifier, and
    git settings. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **bundle** | 是 | 包含有关当前资产包的顶级信息，包括Databricks CLI版本、现有集群标识符和git设置。 |'
- en: '| **variables** | No | Contains global variables that will be dynamically populated
    during the execution of a DAB deployment. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **variables** | 否 | 包含在DAB部署执行过程中将动态填充的全局变量。 |'
- en: '| **workspace** | No | Used to specify non-default workspace locations, such
    as the root storage, artifact storage, and file paths. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **workspace** | 否 | 用于指定非默认工作区位置，例如根存储、工件存储和文件路径。 |'
- en: '| **permissions** | No | Contains information about what permissions to grant
    to the deployed resources. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **permissions** | 否 | 包含有关要授予已部署资源的权限的信息。 |'
- en: '| **resources** | Yes | Specifies what Databricks resources to deploy and how
    to configure them. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **resources** | 是 | 指定要部署的 Databricks 资源以及如何配置它们。 |'
- en: '| **artifacts** | No | Specifies deployment artifacts, such as Python **.whl**
    files, that will be generated during the deployment process. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **artifacts** | 否 | 指定部署工件，如 Python **.whl** 文件，这些文件将在部署过程中生成。 |'
- en: '| **include** | No | Specifies a list of relative file path globs to additional
    configuration files. This is a great way to separate a DAB configuration file
    into several child configuration files. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **include** | 否 | 指定一个相对文件路径模式列表，以包括其他配置文件。这是将 DAB 配置文件分割成多个子配置文件的一个好方法。
    |'
- en: '| **sync** | No | Specifies a list of relative file path globs to include or
    exclude in the deployment process. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **sync** | 否 | 指定在部署过程中要包含或排除的相对文件路径模式列表。 |'
- en: '| **targets** | Yes | Specifies information about the context in addition to
    the Databricks workspace and details about the workflow, pipeline, and artifacts.
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **targets** | 是 | 除了 Databricks 工作区外，指定有关工作流、管道和工件的上下文信息。 |'
- en: Table 9.1 – Mappings in a databricks.yml file
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1 – databricks.yml 文件中的映射
- en: 'Let’s look at a simple DAB configuration file so that we’re familiar with some
    of the basics. The following example will create a new Databricks workflow called
    **Hello, World!** that will run a notebook that prints the simple yet popular
    expression **Hello, World!** :'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的 DAB 配置文件，以便我们熟悉一些基本概念。以下示例将创建一个名为**Hello, World!**的新 Databricks 工作流，该工作流将运行一个打印简单而流行表达式
    **Hello, World!** 的笔记本：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this simple example, our DAB configuration file consists of three main sections:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的示例中，我们的 DAB 配置文件由三个主要部分组成：
- en: '**bundle** : This section contains high-level information about the current
    DAB – in this case, its name.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bundle** : 该部分包含有关当前 DAB 的高级信息——在此案例中是其名称。'
- en: '**resources** : This defines a new Databricks workflow with a single notebook
    task that should be run on an existing cluster.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**resources** : 这定义了一个新的 Databricks 工作流，包含一个单一的笔记本任务，应该在现有集群上运行。'
- en: '**targets** : This specifies information about the target Databricks workspace
    the workflow and notebook should be deployed to.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**targets** : 这指定了有关目标 Databricks 工作区的信息，工作流和笔记本应该部署到该工作区。'
- en: Now that we have a strong understanding of the basics of a DAB configuration
    file, let’s look at how we can deploy our Databricks resources under different
    deployment scenarios.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对 DAB 配置文件的基础知识有了充分了解，接下来让我们看看如何在不同的部署场景下部署 Databricks 资源。
- en: Specifying a deployment mode
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指定部署模式
- en: 'One attribute that’s available from within a DAB configuration file is a deployment
    mode, which allows us to specify an operating mode when we’re deploying resources.
    There are two types of deployment modes available: development and production.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: DAB 配置文件中有一个可用的属性是部署模式，它允许我们在部署资源时指定操作模式。部署模式有两种类型：开发模式和生产模式。
- en: In *development* mode, all resources are marked with a special prefix, **[dev
    <username>]** , to indicate that the resources are in development. Furthermore,
    all resources, when available, are deployed with the **dev** metadata tag, to
    also indicate that the resources are in development. As you may recall from [*Chapter
    2*](B22011_02.xhtml#_idTextAnchor052) , DLT also has a development mode available.
    When DLT pipelines are deployed using DABs in development mode, all deployed DLT
    pipelines will be deployed to the target workspace with this development mode
    enabled.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *开发* 模式下，所有资源都以特殊前缀 **[dev <用户名>]** 标记，表示这些资源处于开发阶段。此外，所有可用资源在部署时都会带有 **dev**
    元数据标签，进一步表明这些资源处于开发阶段。正如你可能从 [*第 2 章*](B22011_02.xhtml#_idTextAnchor052) 回忆起的那样，DLT
    也有一个开发模式可用。当 DLT 管道在开发模式下使用 DAB 部署时，所有已部署的 DLT 管道将在目标工作区启用开发模式的情况下进行部署。
- en: During the development life cycle, it’s also expected that engineers will want
    to experiment with changes and quickly iterate on design changes. As a result,
    development mode will also pause all Databricks workflow schedules and enable
    concurrent runs of the same workflow, allowing engineers to run the workflow in
    an ad hoc fashion directly from the Databricks CLI. Similarly, development mode
    gives you the option to specify an existing all-purpose cluster to use for the
    deployment process, either by specifying the cluster ID as an argument from the
    Databricks CLI via **--** **compute-id <cluster_id>** or by adding the cluster
    ID to the top-level **bundle** mapping of the YAML configuration file.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发生命周期中，工程师通常需要尝试更改并快速迭代设计变更。因此，开发模式也会暂停所有 Databricks 工作流计划，并允许相同工作流的并行运行，使工程师能够直接从
    Databricks CLI 以临时方式运行工作流。同样，开发模式也允许您指定一个现有的通用集群用于部署过程，您可以通过 **--** **compute-id
    <cluster_id>** 参数从 Databricks CLI 指定集群 ID，或者将集群 ID 添加到 YAML 配置文件的顶级 **bundle**
    映射中。
- en: 'Let’s look at how we might be able to specify a target workspace so that it
    can be used as a development environment and override all clusters with a default,
    existing all-purpose cluster:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何指定一个目标工作区，以便将其用作开发环境，并使用默认的现有通用集群覆盖所有集群：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Conversely, you can also specify a production mode. In *production* mode, resources
    won’t be prepended with a special naming prefix and no tags will be applied. However,
    production mode will validate the resources before they’re deployed to the target
    workspace. For example, it will be ensured that all DLT pipelines have been set
    to production mode and resources that specify cloud storage locations or workspace
    paths don’t point to user-specific locations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，您还可以指定生产模式。在 *生产* 模式下，资源不会添加特殊的命名前缀，也不会应用标签。然而，生产模式会在将资源部署到目标工作区之前验证这些资源。例如，它会确保所有
    DLT 管道已设置为生产模式，并且指定云存储位置或工作区路径的资源不会指向用户特定的位置。
- en: In the next section, we’ll roll up our sleeves and dive into using the Databricks
    CLI to experiment with asset bundles and see them in action.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将卷起袖子，深入使用 Databricks CLI 来实验资源包，并查看它们的实际应用。
- en: Databricks Asset Bundles in action
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks 资源包在实际应用中的效果
- en: 'DABs depend entirely on the Databricks CLI tool (see [*Chapter 8*](B22011_08.xhtml#_idTextAnchor185)
    for installation instructions) to create new bundles from templates, deploy bundles
    to target workspaces, and even remove previously deployed resource bundles from
    workspaces. For this section, you’ll need version 0.218.0 or higher of the Databricks
    CLI. You can quickly check the version of your local Databricks CLI by passing
    the **--** **version** argument:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: DAB 完全依赖于 Databricks CLI 工具（请参阅[*第 8 章*](B22011_08.xhtml#_idTextAnchor185)获取安装说明）来从模板创建新的资源包，将资源包部署到目标工作区，甚至从工作区中删除先前部署的资源包。对于本节内容，您需要使用版本
    0.218.0 或更高版本的 Databricks CLI。您可以通过传递 **--** **version** 参数快速检查本地 Databricks CLI
    的版本：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should get a similar output as shown in the following *Figure 9* *.2* :'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会得到类似于以下 *图 9* *.2* 的输出：
- en: "![Figure 9.\uFEFF2 - Checking the version of a previously installed Databricks\
    \ CLI](img/B22011_09_002.jpg)"
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 - 检查已安装的 Databricks CLI 版本](img/B22011_09_002.jpg)'
- en: Figure 9.2 - Checking the version of a previously installed Databricks CLI
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 - 检查已安装的 Databricks CLI 版本
- en: 'Once you’ve successfully installed the recommended version of the Databricks
    CLI, you can test that the installation was successful by displaying the manual
    page for the **bundle** command. Enter the following command to display the available
    arguments and descriptions from the CLI:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您成功安装了推荐版本的 Databricks CLI，您可以通过显示 **bundle** 命令的手册页来测试安装是否成功。输入以下命令，以从 CLI
    显示可用的参数和说明：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We’ll get the following manual page:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如下的手册页：
- en: "![Figure 9.\uFEFF3 – The manual page for the bundle command in the Databricks\
    \ CLI](img/B22011_09_003.jpg)"
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – Databricks CLI 中 **bundle** 命令的手册页](img/B22011_09_003.jpg)'
- en: Figure 9.3 – The manual page for the bundle command in the Databricks CLI
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – Databricks CLI 中 **bundle** 命令的手册页
- en: Before we can begin authoring DABs and deploying resources to Databricks workspaces,
    we will nee d to authenticate with the target Databricks workspaces so that we
    can deploy resources. DABs leverage OAuth toke ns to authenticate with Databricks
    workspaces. Two types of OAuth authentication can be used with DABs – **user-to-machine**
    ( **U2M** ) authentication and **machine-to-machine** ( **M2M** ) authentication.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编写 DABs 并将资源部署到 Databricks 工作区之前，我们需要先进行 Databricks 工作区的认证，以便我们可以部署资源。DABs
    使用 OAuth 令牌与 Databricks 工作区进行认证。DABs 可以使用两种类型的 OAuth 认证——**用户与机器**（**U2M**）认证和**机器与机器**（**M2M**）认证。
- en: User-to-machine authentication
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户与机器认证
- en: U2M authentication involves a human in the loop generating an OAuth token that
    can be used when you’re deploying new resources to a target workspace. This type
    of authentication involves a user who will log in via a web browser when prompted
    by the CLI tool. This type of authentication is good for development scenarios
    where users want to experiment with DABs and deploy resources in non-critical
    development workspaces.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: U2M 认证涉及一个人在循环中生成一个 OAuth 令牌，该令牌可以在将新资源部署到目标工作区时使用。这种认证类型涉及一个用户，当 CLI 工具提示时，用户将通过
    web 浏览器登录。此认证类型适用于开发场景，用户希望在非关键的开发工作区中尝试 DAB 并部署资源。
- en: 'U2M is the easiest way to authenticate with your Databricks workspace and can
    be done directly from the Databricks CLI:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: U2M 是与 Databricks 工作区认证的最简单方法，可以直接通过 Databricks CLI 完成：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Workspace information such as the workspace’s URL, nickname, and authentication
    details are stored in a hidden file under your user directory on your local machine.
    For example, on Mac and Linux systems, this information will be written to a local
    **~/.databrickscfg** file under the user’s home directory:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 工作区的信息，如工作区的 URL、昵称和认证详情，都会保存在本地机器的用户目录下的隐藏文件中。例如，在 Mac 和 Linux 系统中，这些信息将被写入用户主目录下的本地
    **~/.databrickscfg** 文件：
- en: "![Figure 9.\uFEFF4 – Example of multiple Databricks profiles saved to a local\
    \ configuration file](img/B22011_09_004.jpg)"
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 多个 Databricks 配置文件保存到本地配置文件的示例](img/B22011_09_004.jpg)'
- en: Figure 9.4 – Example of multiple Databricks profiles saved to a local configuration
    file
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 多个 Databricks 配置文件保存到本地配置文件的示例
- en: 'You can quickly switch between different Databricks workspaces by passing the
    **--profile <profile_nickname>** argument using CLI commands. For example, the
    following command will apply a DAB to a workspace saved under the **TEST_ENV**
    profile:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用 CLI 命令传递 **--profile <profile_nickname>** 参数来快速切换不同的 Databricks 工作区。例如，以下命令将把
    DAB 应用到 **TEST_ENV** 配置文件下保存的工作区：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: U2M authentication was designed strictly for development purposes. For production
    scenarios, this type of authentication is not recommended as it can’t be automated
    and doesn’t restrict access to the least set of privileges necessary. In these
    cases, M2M authentication is recommended.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: U2M 认证严格设计用于开发目的。对于生产环境，不推荐使用这种认证类型，因为它无法自动化，且无法限制访问到最低必要权限。在这种情况下，推荐使用 M2M
    认证。
- en: Let’s take a look at this alternative authentication type for when you’re automating
    your DAB deployment in production scenarios.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这种替代认证类型，特别是当你在生产环境中自动化 DAB 部署时。
- en: Machine-to-machine authentication
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器与机器认证
- en: M2M authentication doesn’t involve a human, per se. This type of authentication
    was designed for fully automated CI/CD workflows. Furthermore, this type of authentication
    pairs well with version control systems such as GitHub, Bitbucket, and Azure DevOps.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: M2M 认证本身并不涉及人类。这种认证类型是为完全自动化的 CI/CD 工作流设计的。此外，这种认证类型与版本控制系统如 GitHub、Bitbucket
    和 Azure DevOps 配合良好。
- en: M2M requires the us e of service principals to abstract the generation of OAuth
    tokens. Furthermore, service principals give automated tools and scripts API-only
    access to Databricks resources, providing greater security than using users or
    groups. For this reason, service principals are an ideal scenario for production
    environments.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: M2M 需要使用服务主体来抽象化 OAuth 令牌的生成。此外，服务主体使自动化工具和脚本仅通过 API 访问 Databricks 资源，相较于使用用户或组，提供了更高的安全性。因此，服务主体是生产环境的理想选择。
- en: M2M requires a Databricks account admin to crea te a service principal and generate
    an OAuth token from the Databricks account console. Once an OAuth token has been
    generated under the service principal’s identity, the token can be used to populate
    environment variables such as **DATABRICKS_HOST** , **DATABRICKS_CLIENT_ID** ,
    and **DATABRICKS_CLIENT_SECRET** , which are used in automated build and deployment
    tools such as GitHub Actions or Azure DevOps.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: M2M 需要 Databricks 账户管理员创建一个服务主体，并从 Databricks 账户控制台生成 OAuth 令牌。一旦 OAuth 令牌在服务主体的身份下生成，该令牌就可以用来填充环境变量，如**DATABRICKS_HOST**、**DATABRICKS_CLIENT_ID**
    和 **DATABRICKS_CLIENT_SECRET**，这些环境变量用于 GitHub Actions 或 Azure DevOps 等自动化构建和部署工具。
- en: Initializing an asset bundle using templates
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模板初始化资产包
- en: 'DABs also come with project templates, which allow developers to quickly create
    a new bundle using predefined settings. DAB templates contain predefined artifacts
    and settings for commonly deployed Databricks projects. For example, the following
    command will initialize a local DAB project:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: DAB 还提供项目模板，允许开发人员使用预定义设置快速创建新包。DAB 模板包含预定义的工件和常用的 Databricks 项目设置。例如，以下命令将初始化一个本地
    DAB 项目：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'From the CLI, the user is prompted to choose a DAB template:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从 CLI 进行操作时，用户将被提示选择一个 DAB 模板：
- en: "![Figure 9.\uFEFF5 – Initializing a new DAB project using templates from the\
    \ Databricks CLI](img/B22011_09_005.jpg)"
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – 使用 Databricks CLI 从模板初始化一个新的 DAB 项目](img/B22011_09_005.jpg)'
- en: Figure 9.5 – Initializing a new DAB project using templates from the Databricks
    CLI
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 使用 Databricks CLI 从模板初始化一个新的 DAB 项目
- en: 'At the time of writing, DABs come with four templates to choose from: **default-python**
    , **default-sql** , **dbt-sql** , and **mlops-stacks** ( [https://docs.databricks.com/en/dev-tools/bundles/templates.html](https://docs.databricks.com/en/dev-tools/bundles/templates.html)
    ). However, you also have the option to create organization templates and generate
    artifacts as a reusable project bundle.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，DAB 提供四个模板供选择：**default-python**，**default-sql**，**dbt-sql** 和 **mlops-stacks**（[https://docs.databricks.com/en/dev-tools/bundles/templates.html](https://docs.databricks.com/en/dev-tools/bundles/templates.html)）。不过，你也可以选择创建组织模板并生成可重用的项目包。
- en: Now that we have a good understanding of the basics of DABs, let’s put together
    everything that we’ve learned so far and deploy a few resources to a Databricks
    workspace.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 DAB 的基础知识有了清晰的了解，让我们将迄今为止学到的内容整合起来，并将一些资源部署到 Databricks 工作区。
- en: Hands-on exercise – deploying your first DAB
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实操练习 – 部署你的第一个 DAB
- en: In this hands-on exercise, we’re going to create a Python-based asset bundle
    and deploy a simple Databricks workflow that runs a DLT pipeline in a target workspace.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实操练习中，我们将创建一个基于 Python 的资产包，并在目标工作区中部署一个简单的 Databricks 工作流，该工作流运行一个 DLT 管道。
- en: 'Let’s begin by creating a local directory that we’ll be using to create the
    project scaffolding for our DAB. For example, the following command will create
    a new directory under the user’s home directory:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从创建一个本地目录开始，稍后将在该目录中创建我们 DAB 项目的框架。例如，以下命令将在用户的主目录下创建一个新目录：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, navigate to the newly created project directory:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，导航到新创建的项目目录：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Generate a new OAuth token using U2M authentication by entering the following
    command and completing the **single sign-on** ( **SSO** ) login when you’re redirected
    to a browser window:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过输入以下命令并在重定向到浏览器窗口时完成**单点登录**（**SSO**）登录，生成一个新的 OAuth 令牌，使用 U2M 认证：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that our directory has been created and we’ve authenticated with our target
    workspace, let’s use the Databricks CLI to initialize an empty DAB project. Enter
    the following command to bring up the prompt for choosing a DAB template:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了目录，并且与目标工作区进行了身份验证，接下来让我们使用 Databricks CLI 初始化一个空的 DAB 项目。输入以下命令以弹出选择
    DAB 模板的提示：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, choose **default-python** from the template chooser prompt. Enter a meaningful
    name for your project, such as **my_first_dab** . When prompted to select a notebook
    stub, select **No** . Select **Yes** when you’re prompted to include a sample
    DLT pipeline. Finally, select **No** when you’re prompted to add a sample Python
    library. The project scaffolding will be created, at which point you can list
    the directory’s contents so that you can have a glance at the gene rated artifacts:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从模板选择提示中选择**default-python**。为你的项目输入一个有意义的名称，例如**my_first_dab**。当提示你选择一个笔记本模板时，选择**No**。当提示你是否包含一个示例DLT管道时，选择**Yes**。最后，当提示你是否添加示例Python库时，选择**No**。项目框架将被创建，此时你可以列出目录内容，以便查看生成的构件：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To navigate to the newly created project files more easily, open the project
    directory using your favorite code editor, such as VS Code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更方便地导航到新创建的项目文件，使用你喜欢的代码编辑器（如VS Code）打开项目目录：
- en: '![Figure 9.6 – Generated DAB project scaffolding using the default-python template,
    viewed from VS Code](img/B22011_09_006.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6——使用default-python模板生成的DAB项目框架，从VS Code查看](img/B22011_09_006.jpg)'
- en: Figure 9.6 – Generated DAB project scaffolding using the default-python template,
    viewed from VS Code
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6——使用default-python模板生成的DAB项目框架，从VS Code查看
- en: 'Go ahead and explore the subdirectories of the generated DAB project for yourself.
    You should notice a couple of important directories and files:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 继续探索生成的DAB项目的子目录。你应该会注意到几个重要的目录和文件：
- en: '**src** : This directory contains the DLT pipeline definition as a notebook
    file.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**src**：此目录包含作为笔记本文件的DLT管道定义。'
- en: '**resources** : DABs can be decomposed into multiple YAML files that pertain
    to a single resource or a subset of resources. This directory contains the resource
    definitions for a DLT pipeline and a workflow definition for running the pipeline,
    including the schedule, notebook task definition, and job cluster attributes.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**resources**：DAB可以被分解为多个与单个资源或资源子集相关的YAML文件。此目录包含DLT管道的资源定义以及运行管道的工作流定义，包括调度、笔记本任务定义和作业集群属性。'
- en: '**databricks.yml** : This is the main entry point and definition of our DAB.
    This tells the Databricks CLI what resources to deploy and how to deploy them,
    and specifies target workspace information.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**databricks.yml**：这是我们DAB的主要入口点和定义。它告诉Databricks CLI部署哪些资源以及如何部署它们，并指定目标工作空间信息。'
- en: '**README.md** : This is the project README file and contains helpful information
    on the different sections of the project, as well as instructions on how to deploy
    or undeploy the resources.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**README.md**：这是项目的README文件，包含有关项目不同部分的有用信息，以及如何部署或撤销资源的说明。'
- en: Open the **dlt_pipeline.ipynb** notebook contained under the **src** directory.
    Notice that the notebook defines two datasets – a view that reads raw, unprocessed
    JSON files from the NYC Taxi dataset and a table that filters the view based on
    rows with a **fare_amount** value of less than 30.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 打开**src**目录下包含的**dlt_pipeline.ipynb**笔记本。注意，这个笔记本定义了两个数据集——一个是读取来自NYC Taxi数据集的原始、未处理JSON文件的视图，另一个是基于**fare_amount**值小于30的行过滤视图的表。
- en: 'Next, open the **databricks.yml** file. You’ll notice that this file has three
    main sections: **bundle** , **include** , and **targets** .'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开**databricks.yml**文件。你会注意到该文件有三个主要部分：**bundle**、**include**和**targets**。
- en: For simplicity’s sake, under the **targets** mapping, remove all sections except
    for the **dev** section. We’ll only be deploying to a development environment
    for this exercise.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，在**targets**映射下，删除所有部分，保留**dev**部分。我们将在此练习中只部署到开发环境。
- en: 'Finally, ensure that the **dev** target is pointing to the correct development
    workspace. Your **databricks.yml** file should look similar to this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保**dev**目标指向正确的开发工作空间。你的**databricks.yml**文件应该类似于下面的内容：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Save the changes to the **databricks.yml** file and return to your Terminal
    window. Let’s validate the changes to our DAB project by executing the **validate**
    command from the Databricks CLI:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 保存对**databricks.yml**文件的更改，并返回到终端窗口。让我们通过从Databricks CLI执行**validate**命令来验证我们对DAB项目所做的更改：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that our project has been modified to our liking, it’s time to deploy the
    bundle to our development workspace. Execute the following command from your Databricks
    CLI:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的项目已经根据我们的需求进行了修改，是时候将bundle部署到我们的开发工作空间了。从你的Databricks CLI中执行以下命令：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The Databricks CLI will parse our DAB definition and deploy the resources to
    our development target. Log in to the development workspace and verify that a
    new workflow titled **[dev <username>] my_first_dab_job** has been created and
    your Databricks user is listed as the owner.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks CLI将解析我们的DAB定义，并将资源部署到我们的开发目标。登录到开发工作区并验证是否已创建一个名为**[dev <username>]
    my_first_dab_job**的工作流，并且您的Databricks用户被列为所有者。
- en: Congratulations! You’ve just created your first DAB and deployed it to a development
    workspace. You’re well on your way to automating the deployment of data pipelines
    and other Databricks resources.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您刚刚创建了第一个DAB并将其部署到开发工作区。您已经在自动化部署数据管道和其他Databricks资源的路上迈出了重要一步。
- en: 'Let’s test that the deployment was successful by executing a new run of the
    deployed workflow. From the same Databricks CLI, enter the following command.
    This will start an execution run of the newly created workflow and trigger an
    update of the DLT pipeline:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行已部署工作流的新运行来测试部署是否成功。从同一个Databricks CLI中，输入以下命令。这将启动新创建的工作流的执行，并触发DLT管道的更新：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You may be prompted to select which resource to run. For this, select **my_first_dab_job**
    . If success ful, you should see a confirmation message from the CLI that the
    workflow is currently running. Return to your Databricks workspace and verify
    that an execution run has indeed been started.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会被提示选择要运行的资源。在这里，请选择**my_first_dab_job**。如果成功，您应该会看到CLI的确认消息，说明工作流正在运行。返回到您的Databricks工作区，验证确实已开始执行运行。
- en: 'There may be certain scenarios where you need to undeploy resources from a
    target workspace. To undeploy the workflow and DLT pipeline definitions that were
    created earlier, we can use the **destroy** command in the Databricks CLI. Enter
    the following command to revert all changes that were created in this hands-on
    exercise. You’ll need to confirm that you would like to permanently delete all
    resources:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能需要从目标工作区中撤销部署资源。要撤销先前创建的工作流和DLT管道定义，我们可以使用Databricks CLI中的**destroy**命令。输入以下命令以恢复在本次实践中所做的所有更改。您需要确认是否永久删除所有资源：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: So far, we’ve created a simple workflow and DLT pipeline defined in a source
    notebook in a target Databricks workspace. We’ve used a local code editor to author
    the DAB project and deployed the changes from our local machine. However, in production
    scenarios, you’ll be collaborating with teams within your organization to author
    data pipelines and other Databricks resources that all work together to generate
    data products for your organization.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在目标Databricks工作区中创建了一个简单的工作流和DLT管道，并在源笔记本中定义了它们。我们使用本地代码编辑器编写了DAB项目，并将更改从本地计算机部署出去。然而，在生产环境中，您将与组织内的团队合作，共同编写数据管道和其他Databricks资源，这些资源协同工作，为您的组织生成数据产品。
- en: In the next section, we’ll look at how we expand upon this simple exercise and
    work with team members to deploy Databricks resources such as workflows, notebooks,
    or DLT pipelines using automation tools.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何在这个简单的练习基础上进行扩展，并与团队成员一起使用自动化工具部署Databricks资源，如工作流、笔记本或DLT管道。
- en: Hands-on exercise – simplifying cross-team collaboration with GitHub Actions
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践练习 – 使用GitHub Actions简化跨团队协作
- en: Often, you’ll be working across a team of data engineers working to deploy Databricks
    assets such as DLT pipelines, all-purpose clusters, or workflows, to name a few.
    In these scenarios, you’ll likely be using a version control system such as GitHub,
    Bitbucket, or Azure DevOps to collaborate with members of a team.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您将与一个由数据工程师组成的团队一起工作，部署Databricks资产，如DLT管道、通用集群或工作流等。在这些情况下，您可能会使用版本控制系统（如GitHub、Bitbucket或Azure
    DevOps）与团队成员协作。
- en: DABs can be easily incorporated into your CI/CD pipelines. Let’s look at how
    we can use GitHub Actions to automatically deploy changes made to our main branch
    of the code repository and automatically deploy the resource changes to our production
    Databricks workspace.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: DAB可以轻松地集成到您的CI/CD管道中。让我们看看如何使用GitHub Actions自动部署代码库主分支所做的更改，并将资源更改自动部署到生产Databricks工作区。
- en: GitHub Actions is a feature in GitHub that allows users to implement a CI/CD
    workflow directly from a GitHub repository, making it simple to declare a workflow
    of actions to perform based on some triggering event, such as merging a feature
    branch into a master branch. Together with DABs, we can implement a robust, fully
    automated CI/CD pipeline that deploys changes that have been made to our Databricks
    code base. This enables our teams to be more agile, deploying changes as soon
    as they are available, allowing them to speed up the iterative development life
    cycle and quickly test changes.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Actions 是 GitHub 中的一项功能，允许用户直接从 GitHub 仓库实现 CI/CD 工作流，使得基于某些触发事件（例如将功能分支合并到主分支）声明要执行的工作流变得简单。结合
    DABs，我们可以实现一个强大、完全自动化的 CI/CD 管道，将对 Databricks 代码库所做的更改自动部署。这使得我们的团队能够更加灵活，尽快部署可用的更改，从而加速迭代开发周期并快速测试更改。
- en: Setting up the environment
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置环境
- en: 'In this hands-on exercise, we’ll be creating a GitHub Action to automatically
    deploy changes to our Databricks workspace as soon as a branch is merged in our
    GitHub repository. Let’s return to the example from earlier in this chapter. If
    you haven’t already done so, you can clone the example from this chapter’s GitHub
    repository: [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter09](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter09)
    .'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个动手实践中，我们将创建一个 GitHub Action，在我们的 GitHub 仓库中分支合并后，自动将更改部署到 Databricks 工作空间。让我们回到本章早些时候的示例。如果你还没有这样做，可以从本章的
    GitHub 仓库克隆示例：[https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter09](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter09)。
- en: First, let’s create a new private folder in the root of our repository – that
    is, **.github** . Within this folder, let’s create another child folder called
    **workflows** . This nested directory structure is a special pattern whose presence
    will be automatically picked up by the GitHub repository and parsed as a GitHub
    Actions workflow. Within this folder, we’ll define our GitHub Actions workflow,
    which also uses a YAML configuration file to declare a CI/CD workflow. Create
    a new YAML file called **dab_deployment_workflow.yml** within the **.** **github/workflows**
    folder.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在仓库的根目录下创建一个新的私有文件夹——即 **.github**。在此文件夹中，让我们创建另一个名为 **workflows** 的子文件夹。这个嵌套目录结构是一个特殊的模式，其存在将被
    GitHub 仓库自动识别并解析为 GitHub Actions 工作流。在这个文件夹中，我们将定义 GitHub Actions 工作流，工作流同样使用
    YAML 配置文件声明 CI/CD 工作流。在 **.github/workflows** 文件夹中创建一个名为 **dab_deployment_workflow.yml**
    的新 YAML 文件。
- en: Next, we’ll open the workflow file in our favorite code editor so that it’s
    easier to manipulate.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在我们喜欢的代码编辑器中打开工作流文件，以便更方便地操作。
- en: Configuring the GitHub Action
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 GitHub Action
- en: 'Let’s begin by adding the basic structure to our GitHub Actions workflow file.
    Within the YAML file, we’ll give the GitHub Actions workflow a user-friendly name,
    such as **DABs in Action** . Within this file, we’ll also specify that whenever
    an approved pull request is merged into the main branch of our code repository,
    our CI/CD pipeline should be run. Copy and paste the following contents into the
    newly created file, **dab_deployment_workflow** **.yml** :'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过在 GitHub Actions 工作流文件中添加基本结构来开始。我们将在 YAML 文件中为 GitHub Actions 工作流指定一个用户友好的名称，例如
    **DABs in Action**。在这个文件中，我们还将指定每当一个批准的拉取请求被合并到我们的代码仓库的主分支时，CI/CD 管道就应当运行。将以下内容复制并粘贴到新创建的文件
    **dab_deployment_workflow** **.yml** 中：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, let’s define a job within our GitHub Actions YAML file that will clone
    the GitHub repository, download the Databricks CLI, and deploy our DAB to our
    target Databricks workspace. Add the following job definition to the workflow
    file:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在 GitHub Actions 的 YAML 文件中定义一个任务，该任务将克隆 GitHub 仓库、下载 Databricks CLI，并将我们的
    DAB 部署到目标 Databricks 工作空间。将以下任务定义添加到工作流文件中：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You’ll also notice that we’ve used the same Databricks CLI **bundle** command
    to deploy our Databricks resources as we did in the earlier example, using our
    local installation to deploy resources. Furthermore, under the **working-directory**
    parameter, we’ve specified that our DAB configuration file will be found at the
    root of our GitHub repository under the **dabs** folder. We’ve also leveraged
    GitHub Secrets ( [https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions#creating-secrets-for-a-repository](https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions#creating-secrets-for-a-repository)
    ) to securely store the API token for authenticating with our target Databricks
    workspace, as well as followed the best practice of using a service principal
    (see the *User-to-machine authentication* section) to automate the deployment
    of our resources.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到，我们在之前的示例中使用了相同的 Databricks CLI **bundle** 命令来部署我们的 Databricks 资源，使用本地安装来部署资源。此外，在
    **working-directory** 参数下，我们指定了 DAB 配置文件将位于 GitHub 仓库根目录下的 **dabs** 文件夹中。我们还利用了
    GitHub Secrets（[https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions#creating-secrets-for-a-repository](https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions#creating-secrets-for-a-repository)）安全地存储了用于与目标
    Databricks 工作空间进行身份验证的 API 令牌，并遵循了使用服务主体的最佳实践（请参阅 *用户到机器身份验证* 部分）来自动化资源的部署。
- en: You’ll recall that service principals are restricted to a subset of API calls
    and follow the best practice of least privilege, whereas a user account would
    provide more privileges than are necessary. Furthermore, our users can come and
    go from our organization, making maintenance activities such as user deprovisioning
    a headache.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，服务主体只能调用部分 API，并遵循最小权限的最佳实践，而用户账户则会提供比必要的更多权限。此外，我们的用户可能会在组织中进出，这使得诸如用户注销等维护任务变得头疼。
- en: Testing the workflow
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试工作流
- en: Now that we’ve defined when our CI/CD pipeline should be triggered and the workflow
    job responsible for deploying our DAB to our target workspace, we can test the
    GitHub Actions workflow.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了何时触发 CI/CD 流水线以及负责将 DAB 部署到目标工作空间的工作流作业，我们可以测试 GitHub Actions 工作流。
- en: 'Let’s add a section to our existing GitHub Actions workflow file that will
    trigger the **my_first_dab_job** Databricks workflow that we created in the previous
    example. You’ll also notice that, under the **needs** parameter, we declare a
    dependency on **DAB Deployment Job** , which must be completed before we can execute
    a run of the Databricks workflow. In other words, we can’t test the changes without
    deploying them first. Add the following job definition below the **bundle-and-deploy**
    job in the workflow file:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在现有的 GitHub Actions 工作流文件中添加一个部分，该部分将触发我们在前一个示例中创建的 **my_first_dab_job**
    Databricks 工作流。你还会注意到，在 **needs** 参数下，我们声明了对 **DAB Deployment Job** 的依赖关系，必须先完成该作业，才能执行
    Databricks 工作流的运行。换句话说，在部署更改之前，我们不能测试这些更改。将以下作业定义添加到工作流文件中的 **bundle-and-deploy**
    作业下：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Save the GitHub Actions workflow file. Now, let’s test the changes by opening
    a new pull request on our GitHub repository and merging the pull request into
    the main branch of the repository.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 保存 GitHub Actions 工作流文件。现在，通过在 GitHub 仓库中打开一个新的拉取请求并将其合并到主分支来测试更改。
- en: 'First, create a new feature branch using **git** :'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用 **git** 创建一个新的功能分支：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, open the DAB configuration file for the Databricks workflow in a code
    editor. Update the autoscaling size of our job cluster from four worker nodes
    to five. Save the file and commit the changes to the branch. Finally, push the
    changes to the remote repository. Using a web browser, navigate to the GitHub
    repository and create a new pull request in GitHub. Approve the changes and merge
    the branch into the main branch. Ensure that the GitHub Actions workflow is triggered
    and that the code changes have been deployed to the target Databricks workspac
    e. You should also see that a new run of the **my_first_dab_job** Databricks workflow
    has been executed by the GitHub Actions workflow.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在代码编辑器中打开 Databricks 工作流的 DAB 配置文件。将我们的作业集群的自动缩放大小从四个工作节点更新为五个。保存文件并提交更改到分支。最后，将更改推送到远程仓库。使用
    Web 浏览器导航到 GitHub 仓库并创建一个新的拉取请求。批准更改并将分支合并到主分支。确保触发 GitHub Actions 工作流，并且代码更改已部署到目标
    Databricks 工作空间。你还应该看到 **my_first_dab_job** Databricks 工作流的一个新运行已由 GitHub Actions
    工作流执行。
- en: Now that we’ve seen how easy it is to incorporate our DABs into a CI/CD pipeline,
    let’s expand on this example to see how DABs can assist us when we want to deploy
    different versions of our code base to a Databricks workspace.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了将我们的 DABs 融入 CI/CD 管道中有多么容易，让我们扩展这个例子，看看当我们希望将我们的代码库的不同版本部署到 Databricks
    工作空间时，DABs 如何帮助我们。
- en: Versioning and maintenance
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 版本控制和维护
- en: 'DABs make it simple to deploy changes to different environments iteratively.
    There may be scenarios where you might want to experiment with different changes
    and document that those changes come from a particular version of your repository.
    The top-level **bundle** mapping permits users to specify a repository URL and
    branch name to annotate different versions of your code base that are deployed
    to target Databricks workspaces. This is a great way to document that a bundle
    deployment comes from a particular repository and feature branch. For example,
    the following code annotates that an asset bundle uses an experimental feature
    branch as the project source:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: DABs 可以简化迭代地将更改部署到不同的环境。可能会有场景，您希望尝试不同的更改，并记录这些更改来自存储库的特定版本。顶级**bundle**映射允许用户指定存储库
    URL 和分支名称，以注释部署到目标 Databricks 工作空间的代码库的不同版本。这是一个很好的方式来记录 bundle 部署来自特定存储库和功能分支的情况。例如，以下代码注释说明了一个资产
    bundle 使用了实验性功能分支作为项目来源：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As another example, DABs make it simple to automate and document regular maintenance
    activities such as upgrading Databricks runtimes to the latest release. This is
    a great way to experiment with beta versions of the runtime and test compatibility
    with existing Databricks workflows. DABs can be used to automate the manual deployment
    and testing process, and even roll back changes if workflows begin to fail, for
    example.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以另一个例子来说，DABs使得自动化和文档化常规维护活动变得简单，例如将 Databricks 运行时升级到最新版本。这是一个很好的方式来试验运行时的
    beta 版本，并测试与现有 Databricks 工作流的兼容性。例如，如果工作流开始失败，DABs 可用于自动化手动部署和测试过程，甚至回滚更改。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we covered how to automate the deployment of your Databricks
    resources u sing DABs. We saw how integral the Databricks CLI was in creating
    new bundles from preconfigured templates, authenticating the CLI tool with target
    Databricks workspaces, triggering Databricks workflow runs, and managing the end-to-end
    bundle life cycle. We also saw how we can quickly iterate on design and testing
    by using a development mode inside of our DAB configuration file.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何使用 DABs 自动化部署您的 Databricks 资源。我们看到了 Databricks CLI 在从预配置模板创建新 bundle、将
    CLI 工具与目标 Databricks 工作空间进行身份验证、触发 Databricks 工作流运行以及管理端到端 bundle 生命周期中的重要性。我们还看到了通过在
    DAB 配置文件中使用开发模式来快速迭代设计和测试的方法。
- en: In the next chapter, we’ll conclude with the skills necessary to monitor your
    data applications in a production environment. We’ll touch on key features in
    the Databricks Data Intelligence Platform, including alerting, viewing the pipeline
    event log, and measuring statistical metrics using L akehouse M onitoring.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将总结在生产环境中监控数据应用所需的技能。我们将涉及 Databricks 数据智能平台的关键特性，包括警报、查看管道事件日志以及使用
    Lakehouse Monitoring 测量统计指标。
