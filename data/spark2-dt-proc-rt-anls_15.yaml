- en: Implementing Text Analytics with Spark 2.0 ML Library
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0 ML库实现文本分析
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将涵盖以下内容：
- en: Doing term frequency with Spark - everything that counts
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行词频统计 - 一切计数
- en: Displaying similar words with Spark using Word2Vec
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark通过Word2Vec显示相似词
- en: Downloading a complete dump of Wikipedia for a real-life Spark ML project
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为实际的Spark ML项目下载维基百科的完整数据集
- en: Using Latent Semantic Analysis for text analytics with Spark 2.0
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行文本分析的潜在语义分析
- en: Topic modeling with Latent Dirichlet allocation in Spark 2.0
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0中的主题建模与潜在狄利克雷分配
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Text analytics is at the intersection of machine learning, mathematics, linguistics,
    and natural language processing. Text analytics, referred to as text mining in
    older literature, attempts to extract information and infer higher level concepts,
    sentiment, and semantic details from unstructured and semi-structured data. It
    is important to note that the traditional keyword searches are insufficient to
    deal with noisy, ambiguous, and irrelevant tokens and concepts that need to be
    filtered out based on the actual context.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析处于机器学习、数学、语言学和自然语言处理的交叉点。文本分析，在较早的文献中称为文本挖掘，试图从未结构化和半结构化数据中提取信息并推断更高级别的概念、情感和语义细节。值得注意的是，传统的关键词搜索不足以处理需要根据实际上下文过滤掉的噪声、模糊和无关的词条和概念。
- en: Ultimately, what we are trying to do is for a given set of documents (text,
    tweets, web, and social media), is determine what the gist of the communication
    is and what concepts it is trying to convey (topics and ...
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们试图做的是对于一组给定的文档（文本、推文、网页和社交媒体），确定沟通的要点以及它试图传达的概念（主题和...
- en: Doing term frequency with Spark - everything that counts
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行词频统计 - 一切计数
- en: For this recipe, we will download a book in text format from Project Gutenberg,
    from [http://www.gutenberg.org/cache/epub/62/pg62.txt](http://www.gutenberg.org/cache/epub/62/pg62.txt).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此示例，我们将从古腾堡项目下载一本文本格式的书籍，网址为[http://www.gutenberg.org/cache/epub/62/pg62.txt](http://www.gutenberg.org/cache/epub/62/pg62.txt)。
- en: Project Gutenberg offers over 50,000 free eBooks in various formats for human
    consumption. Please read their terms of use; let us not use command-line tools
    to download any books.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 古腾堡项目提供超过50,000种格式的免费电子书供人类消费。请阅读他们的使用条款；让我们不要使用命令行工具下载任何书籍。
- en: When you look at the contents of the file, you will notice the title and author
    of the book is *The Project Gutenberg EBook of A Princess of Mars* by Edgar Rice
    Burroughs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看文件内容时，会注意到该书的标题和作者是*《火星公主》项目古腾堡电子书》*，作者是埃德加·赖斯·巴勒斯。
- en: This eBook is for the use of anyone, anywhere, at no cost, and with almost no
    restrictions whatsoever. You may copy it, give it away, or reuse it under the
    terms of the Project Gutenberg License included with this eBook online at [http://www.gutenberg.org/](http://www.gutenberg.org/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这本电子书可供任何人免费使用，几乎没有任何限制。你可以复制它、赠送它，或根据古腾堡项目许可证中包含的条款重新使用它，该许可证可在[http://www.gutenberg.org/](http://www.gutenberg.org/)在线获取。
- en: We then use the downloaded book to demonstrate the classic word count program
    with Scala and Spark. The example may seem somewhat simple at first, but we are
    beginning the process of feature extraction for text processing. Also, a general
    understanding of counting word occurrences in a document will go a long way to
    help us understand the concept of TF-IDF.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用下载的书籍来演示使用Scala和Spark的经典词频统计程序。这个例子起初可能看起来有些简单，但我们正在开始文本处理的特征提取过程。此外，对文档中词频计数的一般理解将大大有助于我们理解TF-IDF的概念。
- en: How to do it...
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'The `package` statement for the recipe is as follows:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该示例的`包`声明如下：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for Scala, Spark, and JFreeChart:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Scala、Spark和JFreeChart所需的必要包：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will define a function to display our JFreeChart within a window:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将定义一个函数，在窗口中显示我们的JFreeChart：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works...
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其工作原理...
- en: We began by loading the downloaded book and tokenizing it via a regular expression.
    The next step was to convert all tokens to lowercase and exclude stop words from
    our token list, followed by filtering out any words less than two characters long.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载下载的书籍，并通过正则表达式对其进行分词。接下来的步骤是将所有词条转换为小写，并从词条列表中排除停用词，然后过滤掉任何长度小于两个字符的词。
- en: The removal of stop words and words of a certain length reduce the number of
    features we have to process. It may not seem obvious, but the removal of particular
    words based on various processing criteria reduce the number of dimensions our
    machine learning algorithms will later process.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 去除停用词和特定长度的词减少了我们需要处理的特征数量。这可能不明显，但根据各种处理标准去除特定单词会减少机器学习算法后续处理的维度数量。
- en: Finally, we sorted the resulting word count in descending order, taking the
    top 25, which we displayed a bar chart for.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对结果的词频进行了降序排序，取前25个，并为其展示了一个条形图。
- en: There's more...
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In this recipe, we have the base of what a keyword search would do. It is important
    to understand the difference between topic modelling and keyword search. In a
    keyword search, we try to associate a phrase with a given document based on the
    occurrences. In this case, we will point the user to a set of documents that has
    the most number of occurrences.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们有了关键词搜索的基础。理解主题建模和关键词搜索之间的区别很重要。在关键词搜索中，我们试图根据出现次数将短语与给定文档关联。在这种情况下，我们将向用户指向出现次数最多的文档集。
- en: See also
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: The next step in the evolution of this algorithm, that a developer can try as
    an extension, would be to add weights and come up with a weighted average, but
    then Spark provides a facility which we explore in the upcoming recipes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的下一步演化，开发者可以尝试作为扩展，将是添加权重并计算加权平均值，但Spark提供了我们将在接下来的菜谱中探讨的设施。
- en: Displaying similar words with Spark using Word2Vec
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark的Word2Vec展示相似词
- en: In this recipe, we will explore Word2Vec, which is Spark's tool for assessing
    word similarity. The Word2Vec algorithm is inspired by the *distributional hypothesis*
    in general linguistics. At the core, what it tries to say is that the tokens which
    occur in the same context (that is, distance from the target) tend to support
    the same primitive concept/meaning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们将探讨Word2Vec，这是Spark评估词相似性的工具。Word2Vec算法受到普通语言学中*分布假设*的启发。其核心思想是，出现在相同上下文（即与目标的距离）中的词倾向于支持相同的原始概念/意义。
- en: The Word2Vec algorithm was invented by a team of researchers at Google. Please
    refer to a white paper mentioned in the *There's more...* section of this recipe
    which describes Word2Vec in more detail.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的一组研究人员发明了Word2Vec算法。请参考本菜谱中*还有更多...*部分提到的白皮书，该白皮书对Word2Vec进行了更详细的描述。
- en: How to do it...
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'The `package` statement for the recipe is as follows:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本菜谱的`package`语句如下：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Import the necessary packages for Scala and Spark:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Scala和Spark所需的必要包：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let us define the location of our book file:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义书籍文件的位置：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create a Spark session with configurations using the factory builder pattern:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用工厂构建器模式创建具有配置的Spark会话：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We should set the logging level to warning, otherwise output will be difficult
    to follow:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该将日志级别设置为警告，否则输出将难以跟踪：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We load in the book and convert it to a DataFrame:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载书籍并将其转换为DataFrame：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We now transform each line into a bag of words utilizing Spark''s regular expression
    tokenizer, converting each term into lowercase and filtering away any term which
    has a character length of less than four:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在利用Spark的正则表达式分词器将每行转换为词袋，将每个词转换为小写，并过滤掉任何字符长度小于四的词：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We remove stop words by using Spark''s `StopWordRemover` class:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用Spark的`StopWordRemover`类去除停用词：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We apply the Word2Vec machine learning algorithm to extract features:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应用Word2Vec机器学习算法提取特征：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We find ten synonyms from the book for *martian*:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从书中找出*martian*的十个同义词：
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Display the results of ten synonyms found by the model:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展示模型找到的十个同义词的结果：
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/7d3b2158-a771-4656-af50-1a4492e191f7.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d3b2158-a771-4656-af50-1a4492e191f7.png)'
- en: 'We close the program by stopping the SparkContext:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止SparkContext来关闭程序：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Word2Vec in Spark uses skip-gram and not **Continuous Bag of Words** (**CBOW**)
    which is more suitable for a **Neural Net** (**NN**). At its core, we are attempting
    to compute the representation of the words. It is highly recommended for the user
    to understand the difference between local representation versus distributed presentation,
    which is very different to the apparent meaning of the words themselves.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，Word2Vec使用的是跳字模型而非**连续词袋模型**（**CBOW**），后者更适合**神经网络**（**NN**）。其核心在于尝试计算词的表示。强烈建议用户理解局部表示与分布式表示之间的差异，这与词本身的表面意义截然不同。
- en: If we use distributed vector representation for words, it is natural that similar
    words will fall close together in the vector space, which is a desirable generalization
    technique for pattern abstraction and manipulation (that is, we reduce the problem
    to vector arithmetic).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用分布式向量表示词，自然地，相似的词在向量空间中会彼此靠近，这是一种理想的模式抽象和操作泛化技术（即，我们将问题简化为向量运算）。
- en: What we want to do for a given set of words *{Word[1,] Word[2, .... ...]*
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的一组词*{Word[1,] Word[2, .... ...]*，我们想要做的是
- en: There's more...
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: How would you find similar words anyhow? How many algorithms are there that
    can solve this problem, and how do they vary? The Word2Vec algorithm has been
    around for a while and has a counterpart called CBOW. Please bear in mind that
    Spark provides the skip-gram method as the implementation technique.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，你如何找到相似的词？有多少算法能解决这个问题，它们之间有何不同？Word2Vec算法已经存在一段时间，并且有一个对应的模型称为CBOW。请记住，Spark提供的实现技术是跳字模型。
- en: 'The variations of the Word2Vec algorithm are as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec算法的变化如下：
- en: '**Continuous Bag of Words (CBOW)**: Given a central word, what are the surrounding
    words?'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续词袋模型（CBOW）**：给定一个中心词，周围的词是什么？'
- en: '**Skip-gram**: If we know the words surrounding, can we guess the missing word?'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跳字模型**：如果我们知道周围的词，能否猜出缺失的词？'
- en: There is a variation of the algorithm that is called **skip-gram model with
    negative sampling** (**SGNS**), which seems to outperform other variants.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种算法变体称为**带负采样的跳字模型**（**SGNS**），它似乎优于其他变体。
- en: The co-occurrence is the fundamental concept underlying both CBOW and skip-gram.
    Even though the skip-gram does not directly use a co-occurrence matrix, it is
    using it indirectly.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 共现是CBOW和跳字模型背后的基本概念。尽管跳字模型并不直接使用共现矩阵，但它间接地使用了它。
- en: In this recipe, we used the *stop words* techniques from NLP to have a cleaner
    corpus before running our algorithm. The stop words are English words such as
    "*the*" that need to be removed since they are not contributing to any improvement
    in the outcome.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本方法中，我们使用了NLP中的*停用词*技术，在运行算法前对语料库进行净化。停用词如英语中的"*the*"需要被移除，因为它们对结果的改进没有贡献。
- en: Another important concept is* stemming*, which is not covered here but will
    be demonstrated in later recipes. Stemming removes extra language artifacts and
    reduces the word to its root (for example, *Engineering*, *Engineer*, and *Engineers*
    become *Engin* which is the root).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要概念是*词干提取*，这里未涉及，但将在后续方法中展示。词干提取去除额外的语言特征，将词还原为其词根（例如，*Engineering*、*Engineer*和*Engineers*变为*Engin*，即词根）。
- en: 'The white paper found at the following URL should provide a deeper explanation
    for Word2Vec:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下URL的白皮书应提供对Word2Vec更深入的解释：
- en: '[http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf)'
- en: See also
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'Documentation for the Word2Vec recipe:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec方法的文档：
- en: '`Word2Vec()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Word2Vec()`：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec)'
- en: '`Word2VecModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Word2VecModel()`：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel)'
- en: '`StopWordsRemover()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StopWordsRemover()`：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover)'
- en: Downloading a complete dump of Wikipedia for a real-life Spark ML project
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为实际的Spark ML项目下载完整的维基百科转储
- en: In this recipe, we will be downloading and exploring a dump of Wikipedia so
    we can have a real-life example. The dataset that we will be downloading in this
    recipe is a dump of Wikipedia articles. You will either need the command-line
    tool **curl**, or a browser to retrieve a compressed file, which is about 13.6
    GB at this time. Due to the size, we recommend the curl command-line tool.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将下载并探索维基百科的转储，以便我们有一个实际的示例。本示例中我们将下载的数据集是维基百科文章的转储。您将需要命令行工具**curl**或浏览器来检索压缩文件，目前该文件大小约为13.6
    GB。由于文件较大，我们建议使用curl命令行工具。
- en: How to do it...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'You can start with downloading the dataset using the following command:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下命令开始下载数据集：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now you want to decompress the ZIP file:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您想要解压缩ZIP文件：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This should create an uncompressed file which is named `enwiki-latest-pages-articles-multistream.xml`
    and is about 56 GB.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个未压缩的文件，名为`enwiki-latest-pages-articles-multistream.xml`，大小约为56 GB。
- en: 'Let us take a look at the Wikipedia XML file:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看维基百科XML文件：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There's more...
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We recommend working with the XML file in chunks, and using sampling for your
    experiments until you are ready for a final job to submit. It will save a tremendous
    amount of time and effort.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议将XML文件分块处理，并在准备好提交最终作业之前使用抽样进行实验。这将节省大量时间和精力。
- en: See also
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for Wiki download is available at [https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 维基下载文档可在[https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download)找到。
- en: Using Latent Semantic Analysis for text analytics with Spark 2.0
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行文本分析的潜在语义分析
- en: In this recipe, we will explore LSA utilizing a data dump of articles from Wikipedia.
    LSA translates into analyzing a corpus of documents to find hidden meaning or
    concepts in those documents.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将利用维基百科文章的数据转储来探索LSA。LSA意味着分析文档集合以发现这些文档中的隐藏含义或概念。
- en: In the first recipe of this chapter, we covered the basics of the TF (that is,
    term frequency) technique. In this recipe, we use HashingTF for calculating TF
    and use IDF to fit a model into the calculated TF. At its core, LSA uses **singular
    value decomposition** (**SVD**) on the term frequency document to reduce dimensionality
    and therefore extract the most important concepts. There are other cleanup steps
    that we need to do (for example, stop words and stemming) that will clean up the
    bag of words before we start analyzing it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本章第一个示例中，我们介绍了TF（即词频）技术的基本概念。在本示例中，我们使用HashingTF计算TF，并使用IDF将模型拟合到计算出的TF上。LSA的核心在于对词频文档进行**奇异值分解**（**SVD**），以降低维度并提取最重要的概念。我们还需要进行其他清理步骤（例如，停用词和词干提取），以在开始分析之前清理词袋。
- en: How to do it...
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'The package statement for the recipe is as follows:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本示例的包声明如下：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Import the necessary packages for Scala and Spark:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Scala和Spark导入必要的包：
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How it works...
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The example starts off by loading a dump of Wikipedia XML using Cloud9 Hadoop
    XML streaming tools to process the enormous XML document. Once we have parsed
    out the page text, the tokenization phase invokes turning our stream of Wikipedia
    page text into tokens. We used the Porter stemmer during the tokenization phase
    to help reduce words to a common base form.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 示例首先通过使用Cloud9 Hadoop XML流工具加载维基百科XML转储来处理庞大的XML文档。一旦我们解析出页面文本，分词阶段就会将我们的维基百科页面文本流转换为令牌。在分词阶段，我们使用了Porter词干提取器来帮助将单词简化为共同的基本形式。
- en: More details on stemming are available at [https://en.wikipedia.org/wiki/Stemming](https://en.wikipedia.org/wiki/Stemming).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 关于词干提取的更多详细信息，请访问[https://en.wikipedia.org/wiki/Stemming](https://en.wikipedia.org/wiki/Stemming)。
- en: The next step was to use Spark HashingTF on each page token to compute the term
    frequency. After this phase was completed, we utilized Spark's IDF to generate
    the inverse document frequency.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用Spark的HashingTF对每个页面令牌计算词频。在此阶段完成后，我们利用Spark的IDF生成逆文档频率。
- en: Finally, we took the TF-IDF API and applied a singular value decomposition to
    handle factorization and dimensionality reduction.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们采用了TF-IDF API，并应用奇异值分解来处理因式分解和降维。
- en: 'The following screenshot shows the steps and flow of the recipe:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了该方法的步骤和流程：
- en: '![](img/ff9337d4-120e-4985-a45a-d4a273732f96.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: 图片：![](img/ff9337d4-120e-4985-a45a-d4a273732f96.png)
- en: 'The Cloud9 Hadoop XML tools and several other required dependencies can be
    found at:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 可在以下链接找到Cloud9 Hadoop XML工具及其他所需依赖：
- en: '`bliki-core-3.0.19.jar`: [http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bliki-core-3.0.19.jar`：[http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
- en: '`cloud9-2.0.1.jar`: [http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud9-2.0.1.jar`：[http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
- en: '`hadoop-streaming-2.7.4.jar`: [http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar](http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hadoop-streaming-2.7.4.jar`：[http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar](http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar)'
- en: '`lucene-snowball-3.0.3.jar`: [http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar](http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lucene-snowball-3.0.3.jar`：[http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar](http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar)'
- en: There's more...
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: It should be obvious by now that even though Spark does not provide a direct
    LSA implementation, the combination of TF-IDF and SVD will let us construct and
    then decompose the large corpus matrix into three matrices, which can help us
    interpret the results by applying the dimensionality reduction via SVD. We can
    concentrate on the most meaningful clusters (similar to a recommendation algorithm).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该很明显，尽管Spark没有直接提供LSA实现，但TF-IDF与SVD的结合将使我们能够构建并分解大型语料库矩阵为三个矩阵，这有助于我们通过SVD进行降维来解释结果。我们可以专注于最有意义的集群（类似于推荐算法）。
- en: SVD will factor the term frequency document (that is, documents by attributes)
    to three distinct matrices that are much more efficient to extract to *N* concepts
    (that is, *N=27* in our example) from a large matrix that is hard and expensive
    to handle. In ML, we always prefer the tall and skinny matrices (that is, *U*
    matrix in this case) ...
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: SVD将词频文档（即按属性划分的文档）分解为三个不同的矩阵，这些矩阵更便于从难以处理且成本高昂的大型矩阵中提取*N*个概念（例如，在我们的例子中*N=27*）。在机器学习中，我们总是偏好高瘦矩阵（即本例中的*U*矩阵）...
- en: See also
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: More details on `SingularValueDecomposition()` can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`SingularValueDecomposition()`的更多详情，请参阅[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition)。
- en: Please refer to [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix) for
    more details on `RowMatrix()`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`RowMatrix()`的更多详情，请参考[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix)。
- en: Topic modeling with Latent Dirichlet allocation in Spark 2.0
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行主题建模与潜在狄利克雷分配
- en: In this recipe, we will be demonstrating topic model generation by utilizing
    Latent Dirichlet Allocation to infer topics from a collection of documents.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本方法中，我们将展示如何利用潜在狄利克雷分配（Latent Dirichlet Allocation）从文档集合中推断主题模型。
- en: We have covered LDA in previous chapters as it applies to clustering and topic
    modelling, but in this chapter, we demonstrate a more elaborate example to show
    its application to text analytics using more real-life and complex datasets.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的章节中已经介绍了LDA，因为它适用于聚类和主题建模，但在本章中，我们展示了一个更复杂的示例，以展示它如何应用于使用更真实和复杂数据集的文本分析。
- en: We also apply NLP techniques such as stemming and stop words to provide a more
    realistic approach to LDA problem-solving. What we are trying to do is to discover
    a set of latent factors (that is, different from the original) that can solve
    and describe the solution in a more efficient way in a reduced ...
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应用了诸如词干提取和停用词等NLP技术，以提供更真实的LDA问题解决方法。我们试图做的是发现一组潜在因素（即与原始因素不同），这些因素可以在减少的...中以更高效的方式解决问题并描述解决方案。
- en: How to do it...
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'The `package` statement for the recipe is as follows:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该配方中的`package`声明如下：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Import the necessary packages for Scala and Spark:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Scala和Spark所需的包：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We define a function to parse a Wikipedia page and return the title and content
    text of the page:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个函数来解析Wikipedia页面并返回页面的标题和内容文本：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let us define the location of the Wikipedia data dump:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义Wikipedia数据转储的位置：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We create a job configuration for Hadoop XML streaming:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为Hadoop XML流创建作业配置：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We set up the data path for Hadoop XML streaming processing:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为Hadoop XML流处理设置数据路径：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create a `SparkSession` with configurations using the factory builder pattern:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用工厂构建器模式创建具有配置的`SparkSession`：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We should set the logging level to warning, otherwise, output will be difficult
    to follow:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该将日志级别设置为警告，否则输出将难以跟踪：
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We begin to process the huge Wikipedia data dump into article pages taking
    a sample of the file:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们开始处理巨大的Wikipedia数据转储成文章页面，从文件中抽取样本：
- en: '[PRE28]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we process our sample data into an RDD containing a tuple of title and
    page context text to finally generate a DataFrame:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将示例数据处理成包含标题和页面上下文文本的元组的RDD，最终生成一个DataFrame：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We now transform the text column of the DataFrame into raw words using Spark''s
    `RegexTokenizer` for each Wikipedia page:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在使用Spark的`RegexTokenizer`将DataFrame的文本列转换为每个Wikipedia页面的原始单词：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The next step is to filter raw words by removing all stop words from the tokens:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是过滤原始单词，通过去除令牌中的所有停用词：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We generate term counts for the filtered tokens by using Spark''s `CountVectorizer`
    class, resulting in a new DataFrame containing the column features:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用Spark的`CountVectorizer`类为过滤后的令牌生成词频，从而产生包含列特征的新DataFrame：
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The "MinDF" specifies the minimum number of different document terms that must
    appear in order to be included in the vocabulary.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '"MinDF"指定必须出现在词汇表中的不同文档术语的最小数量。'
- en: 'We now invoke Spark''s LDA class to generate topics and the distributions of
    tokens to topics:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在调用Spark的LDA类来生成主题以及令牌到主题的分布：
- en: '[PRE33]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The "K" refers to how many topics and "MaxIter" maximum iterations to execute.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '"K"指的是主题数量，"MaxIter"是执行的最大迭代次数。'
- en: 'We finally describe the top five generated topics and display:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们最终描述了生成的五个顶级主题并显示：
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/65c0cc6a-8015-4bdb-85e2-c753056bb79c.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65c0cc6a-8015-4bdb-85e2-c753056bb79c.png)'
- en: 'Now display, topics and terms associated with them:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在显示与主题相关联的主题和术语：
- en: '[PRE35]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The console output will be as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出将如下所示：
- en: '![](img/d6907078-900d-4d60-9db9-7697c384eb19.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6907078-900d-4d60-9db9-7697c384eb19.png)'
- en: 'We close the program by stopping the SparkContext:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止SparkContext来关闭程序：
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We began with loading the dump of Wikipedia articles and parsed the page text
    into tokens using Hadoop XML leveraging streaming facilities API. The feature
    extraction process utilized several classes to set up the final processing by
    the LDA class, letting the tokens flow from Spark's `RegexTokenize`, `StopwordsRemover`,
    and `HashingTF`. Once we had the term frequencies, the data was passed to the
    LDA class for clustering the articles together under several topics.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载Wikipedia文章的转储，并使用Hadoop XML利用流API将页面文本解析为令牌。特征提取过程利用了几个类来设置最终由LDA类处理的流程，让令牌从Spark的`RegexTokenize`、`StopwordsRemover`和`HashingTF`流过。一旦我们有了词频，数据就被传递给LDA类，以便在几个主题下将文章聚类在一起。
- en: 'The Hadoop XML tools and several other required dependencies can be found at:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop XML工具和其他几个必需的依赖项可以在以下位置找到：
- en: '`bliki-core-3.0.19.jar`: [http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bliki-core-3.0.19.jar`：[http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
- en: '`cloud9-2.0.1.jar`: [http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar
    ...](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud9-2.0.1.jar`：[http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar
    ...](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
- en: There's more...
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Please see the recipe LDA to classify documents and text into topics in Chapter
    8, *Unsupervised Clustering with Apache Spark 2.0* for a more detailed explanation
    of the LDA algorithm itself.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅第8章《Apache Spark 2.0 无监督聚类》中的*LDA食谱*，以获取有关LDA算法本身的更详细解释，该章节介绍了如何将文档和文本分类为主题。
- en: The following white paper from the *Journal of Machine Learning Research (JMLR)*
    provides a comprehensive treatment for those who would like to do an extensive
    analysis. It is a well-written paper, and a person with a basic background in
    stat and math should be able to follow it without any problems.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 《机器学习研究杂志》(JMLR) 的以下白皮书为希望进行深入分析的人提供了全面的论述。这是一篇写得很好的论文，具有统计和数学基础的人应该能够毫无困难地理解它。
- en: Refer to the [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) link
    for more details of JMLR; an alternative link is [https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于JMLR的详情，请参考[http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)链接；另有一替代链接为[https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf)。
- en: See also
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for the constructor is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)找到。
- en: Documentation for LDAModel is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDAModel的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)找到。
- en: 'See also Spark''s Scala API documentation for the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 亦可参阅Spark的Scala API文档，了解以下内容：
- en: DistributedLDAModel
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistributedLDAModel
- en: EMLDAOptimizer
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMLDAOptimizer
- en: LDAOptimizer
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDAOptimizer
- en: LocalLDAModel
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LocalLDAModel
- en: OnlineLDAOptimizer
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OnlineLDAOptimizer
