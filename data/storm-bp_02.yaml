- en: Chapter 2. Configuring Storm Clusters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。配置Storm集群
- en: In this chapter, you'll take a deeper look at the Storm technology stack, its
    software dependencies, and the process of setting up and deploying it to a Storm
    cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，您将深入了解Storm技术栈、其软件依赖关系以及设置和部署到Storm集群的过程。
- en: We will begin by installing Storm in the pseudo-distributed mode where all components
    are collocated on the same machine, rather than distributed across multiple machines.
    Once you have an understanding of the basic steps involved in installing and configuring
    Storm, we will move on to automating these processes using the Puppet provisioning
    tool, which will greatly reduce the time and effort required to set up a multi-node
    cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从在伪分布模式下安装Storm开始，其中所有组件都位于同一台机器上，而不是分布在多台机器上。一旦您了解了安装和配置Storm所涉及的基本步骤，我们将继续使用Puppet配置工具自动化这些过程，这将大大减少设置多节点集群所需的时间和精力。
- en: 'Specifically, we will cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖：
- en: The various components and services that compose a cluster
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组成集群的各种组件和服务
- en: The Storm technology stack
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm技术栈
- en: Installing and configuring Storm on Linux
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Linux上安装和配置Storm
- en: Storm's configuration parameters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm的配置参数
- en: Storm's command-line interface
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm的命令行界面
- en: Using the Puppet provisioning tool to automate the installation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Puppet配置工具自动化安装
- en: Introducing the anatomy of a Storm cluster
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Storm集群的解剖
- en: Storm clusters follow a master/slave architecture similar to distributed computing
    technologies such as Hadoop but with slightly different semantics. In a master/slave
    architecture, there is typically a master node that is either statically assigned
    through configuration or dynamically elected at runtime. Storm uses the former
    approach. While the master/slave architecture can be criticized as a setup that
    introduces a single point of failure, we'll show that Storm is semi-tolerant of
    a master node failure.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Storm集群遵循类似于Hadoop等分布式计算技术的主/从架构，但语义略有不同。在主/从架构中，通常有一个主节点，可以通过配置静态分配或在运行时动态选举。Storm使用前一种方法。虽然主/从架构可能会被批评为引入单点故障的设置，但我们将展示Storm对主节点故障具有一定的容错性。
- en: 'A Storm cluster consists of one master node (called **nimbus**) and one or
    more worker nodes (called **supervisors**). In addition to the nimbus and supervisor
    nodes, Storm also requires an instance of Apache ZooKeeper, which itself may consist
    of one or more nodes as shown in the following diagram:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Storm集群由一个主节点（称为**nimbus**）和一个或多个工作节点（称为**supervisors**）组成。除了nimbus和supervisor节点外，Storm还需要一个Apache
    ZooKeeper的实例，它本身可能由一个或多个节点组成，如下图所示：
- en: '![Introducing the anatomy of a Storm cluster](img/8294OS_02_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![介绍Storm集群的解剖](img/8294OS_02_01.jpg)'
- en: Both the nimbus and supervisor processes are daemon processes provided by Storm
    and do not need to be isolated from individual machines. In fact, it is possible
    to create a single-node pseudo-cluster with the nimbus, supervisor, and ZooKeeper
    processes all running on the same machine.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: nimbus和supervisor进程都是Storm提供的守护进程，不需要从单独的机器中隔离出来。事实上，可以在同一台机器上运行nimbus、supervisor和ZooKeeper进程，从而创建一个单节点伪集群。
- en: Understanding the nimbus daemon
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解nimbus守护程序
- en: The nimbus daemon's primary responsibility is to manage, coordinate, and monitor
    topologies running on a cluster, including topology deployment, task assignment,
    and task reassignment in the event of a failure.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: nimbus守护程序的主要责任是管理、协调和监视在集群上运行的拓扑，包括拓扑部署、任务分配以及在失败时重新分配任务。
- en: Deploying a topology to a Storm cluster involves *submitting* the prepackaged
    topology JAR file to the nimbus server along with topology configuration information.
    Once nimbus has received the topology archive, it in turn distributes the JAR
    file to the necessary number of supervisor nodes. When the supervisor nodes receive
    the topology archive, nimbus then assigns tasks (spout and bolt instances) to
    each supervisor and signals them to spawn the necessary workers to perform the
    assigned tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将拓扑部署到Storm集群涉及*提交*预打包的拓扑JAR文件到nimbus服务器以及拓扑配置信息。一旦nimbus收到拓扑归档，它会将JAR文件分发给必要数量的supervisor节点。当supervisor节点收到拓扑归档时，nimbus会为每个supervisor分配任务（spout和bolt实例）并向它们发出信号，以生成执行分配任务所需的工作节点。
- en: Nimbus tracks the status of all supervisor nodes and the tasks assigned to each.
    If nimbus detects that a specific supervisor node has failed to heartbeat or has
    become unavailable, it will reassign that supervisor's tasks to other supervisor
    nodes in the cluster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Nimbus跟踪所有supervisor节点的状态以及分配给每个节点的任务。如果nimbus检测到特定的supervisor节点未能心跳或变得不可用，它将重新分配该supervisor的任务到集群中的其他supervisor节点。
- en: As mentioned earlier, nimbus is not a single point of failure in the strictest
    sense. This quality is due to the fact that nimbus does not take part in topology
    data processing, rather it merely manages the initial deployment, task assignment,
    and monitoring of a topology. In fact, if a nimbus daemon dies while a topology
    is running, the topology will continue to process data as long as the supervisors
    and workers assigned with tasks remain healthy. The main caveat is that if a supervisor
    fails while nimbus is down, data processing will fail since there is no nimbus
    daemon to reassign the failed supervisor's tasks to another node.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，nimbus在严格意义上并不是单点故障。这一特性是因为nimbus并不参与拓扑数据处理，而仅仅管理拓扑的初始部署、任务分配和监视。事实上，如果nimbus守护程序在拓扑运行时死机，只要分配任务的supervisors和workers保持健康，拓扑将继续处理数据。主要的警告是，如果nimbus宕机时supervisor失败，数据处理将失败，因为没有nimbus守护程序将失败的supervisor任务重新分配到另一个节点。
- en: Working with the supervisor daemon
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与监督守护程序一起工作
- en: The supervisor daemon waits for task assignments from nimbus and spawns and
    monitors workers (JVM processes) to execute tasks. Both the supervisor daemon
    and the workers it spawns are separate JVM processes. If a worker process spawned
    by a supervisor exits unexpectedly due to an error (or even if the process is
    being forcibly terminated with the UNIX `kill -9` or Windows `taskkill` command),
    the supervisor daemon will attempt to respawn the worker process.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: supervisor守护程序等待来自nimbus的任务分配，并生成和监视工作进程（JVM进程）来执行任务。supervisor守护程序和它生成的工作进程都是单独的JVM进程。如果由supervisor生成的工作进程由于错误意外退出（甚至如果进程被强制使用UNIX的`kill
    -9`或Windows的`taskkill`命令终止），supervisor守护程序将尝试重新生成工作进程。
- en: At this point, you may be wondering how Storm's guaranteed delivery features
    fit into its fault tolerance model. If a worker or even an entire supervisor node
    fails, how does Storm guarantee the delivery of the tuples that were in process
    at the time of failure?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可能想知道Storm的可靠交付功能如何适应其容错模型。如果一个worker甚至整个supervisor节点失败，Storm如何保证在故障发生时正在处理的元组的交付？
- en: The answer lies in Storm's tuple anchoring and acknowledgement mechanism. When
    reliable delivery is enabled, tuples routed to the task on the failed node will
    not be acknowledged, and the original tuple will eventually be replayed by the
    spout after it is timed out. This process will repeat until the topology has recovered
    and normal processing has resumed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 答案在于Storm的元组锚定和确认机制。启用可靠交付后，路由到失败节点上的任务的元组将不会被确认，并且原始元组最终将在超时后由spout重新播放。这个过程将重复，直到拓扑已经恢复并且正常处理已经恢复。
- en: Introducing Apache ZooKeeper
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Apache ZooKeeper
- en: ZooKeeper provides a service for maintaining centralized information in a distributed
    environment using a small set of primitives and group services. It has a simple
    yet powerful distributed synchronization mechanism that allows client applications
    to watch or subscribe to individual data or sets of data and receive notifications
    when that data is created, updated, or modified. Using common ZooKeeper patterns
    or recipes, developers can implement a number of different constructs needed by
    distributed applications such as leader election, distributed locks and queues.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper提供了在分布式环境中维护集中信息的服务，使用一小组基本原语和组服务。它具有简单而强大的分布式同步机制，允许客户端应用程序监视或订阅单个数据或数据集，并在创建、更新或修改数据时接收通知。使用常见的ZooKeeper模式或配方，开发人员可以实现分布式应用程序所需的许多不同构造，如领导者选举、分布式锁和队列。
- en: Storm uses ZooKeeper primarily to coordinate state information such as task
    assignments, worker status, and topology metrics between nimbus and supervisors
    in a cluster. Nimbus and supervisor node communication is largely handled through
    a combination of ZooKeeper's state modifications and watch notifications.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Storm主要使用ZooKeeper来协调任务分配、worker状态和集群中nimbus和supervisor之间的拓扑指标等状态信息。Nimbus和supervisor节点之间的通信主要通过ZooKeeper的状态修改和监视通知来处理。
- en: Storm's use of ZooKeeper is relatively lightweight by design and does not incur
    a heavy resource burden. For heavier-weight data transfer operations, such as
    a one-time (at deployment time) transfer of topology JAR files, Storm relies on
    Thrift for communication. And as we'll see, data transfer operations between components
    in a topology—where performance matters most—is handled at a low level and optimized
    for performance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Storm对ZooKeeper的使用设计上相对轻量，并不会产生沉重的资源负担。对于较重的数据传输操作，例如拓扑JAR文件的一次性（在部署时）传输，Storm依赖于Thrift进行通信。正如我们将看到的，拓扑中组件之间的数据传输操作——在性能最重要的地方——是在低级别处理并针对性能进行了优化。
- en: Working with Storm's DRPC server
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Storm的DRPC服务器
- en: A common pattern among Storm applications involves the desire to leverage Storm's
    parallelization and distributed computation capabilities within a request-response
    paradigm where a client process or application submits a request and waits for
    a response synchronously. While such a paradigm may seem to counter the highly
    asynchronous, long-lived nature of a typical Storm topology, Storm includes a
    transactional capability that enables such a use case.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Storm应用程序中常见的模式涉及利用Storm的并行化和分布式计算能力，其中客户端进程或应用程序在请求-响应范式中提交请求并同步等待响应。虽然这样的范式似乎与典型Storm拓扑的高度异步、长寿命的特性相悖，但Storm包括了一种事务能力，可以实现这样的用例。
- en: '![Working with Storm''s DRPC server](img/8294OS_02_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: 使用Storm的DRPC服务器
- en: To enable this functionality, Storm uses the combination of an extra service
    (Storm DRPC) and a specialized spout and bolt that work together to provide a
    highly scalable Distributed RPC capability.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用这个功能，Storm使用了额外的服务（Storm DRPC）和一个专门的spout和bolt，它们共同提供了高度可扩展的分布式RPC功能。
- en: The use of Storm's DRPC capability is entirely optional. DRPC server nodes are
    only necessary when a Storm application leverages this functionality.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Storm的DRPC功能的使用是完全可选的。只有当Storm应用程序利用此功能时，才需要DRPC服务器节点。
- en: Introducing the Storm UI
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Storm UI
- en: Storm UI is an optional, but very useful, service that provides a web-based
    GUI to monitor Storm clusters and manage the running topologies to a certain degree.
    The Storm UI provides statistics for a given Storm cluster and its deployed topologies
    and is very useful when monitoring and tuning cluster and topology performance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Storm UI是一个可选的，但非常有用的服务，它提供了一个基于Web的GUI，用于监视Storm集群并在一定程度上管理运行中的拓扑。Storm UI为给定的Storm集群及其部署的拓扑提供统计信息，在监视和调整集群和拓扑性能时非常有用。
- en: '![Introducing the Storm UI](img/8294OS_02_03.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: 介绍Storm UI
- en: Storm UI only reports information gleaned from the nimbus thrift API and does
    not impart any other functionality to a Storm cluster. The Storm UI service can
    be started and stopped at any time without affecting any topology or cluster functionality
    and is in that respect completely stateless. It can also be configurated to start,
    stop, pause, and rebalance topologies for easy management.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Storm UI只报告从nimbus thrift API获取的信息，并不向Storm集群提供任何其他功能。Storm UI服务可以随时启动和停止，而不会影响任何拓扑或集群功能，在这方面它是完全无状态的。它还可以配置为启动、停止、暂停和重新平衡拓扑，以便进行简单的管理。
- en: Introducing the Storm technology stack
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Storm技术栈
- en: Before we jump into installing Storm, let's take a look at the technologies
    with which Storm and topologies are built.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始安装Storm之前，让我们先看看Storm和拓扑构建的技术。
- en: Java and Clojure
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Java和Clojure
- en: Storm runs on the Java Virtual Machine and is written with a roughly equal combination
    of Java and Clojure. Storm's primary interfaces are defined in Java, with the
    core logic being implemented mostly in Clojure. In addition to JVM languages,
    Storm uses Python to implement the Storm executable. Beyond those languages, Storm
    is a highly polyglot-friendly technology due in part to the fact that a number
    of its interfaces use Apache Thrift.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Storm在Java虚拟机上运行，并且大致上由Java和Clojure的组合编写。Storm的主要接口是用Java定义的，核心逻辑大部分是用Clojure实现的。除了JVM语言，Storm还使用Python来实现Storm可执行文件。除了这些语言，Storm还是一种高度多语言友好的技术，部分原因是它的一些接口使用了Apache
    Thrift。
- en: The components of Storm topologies (spouts and bolts) can be written in virtually
    any programming language supported by the operating system on which it's installed.
    JVM language implementations can run natively, and other implementations are possible
    through JNI and Storm's multilang protocol.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Storm拓扑的组件（spouts和bolts）可以用安装它的操作系统支持的几乎任何编程语言编写。JVM语言实现可以本地运行，其他实现可以通过JNI和Storm的多语言协议实现。
- en: Python
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python
- en: All Storm daemons and management commands are run from a single executable file
    written in Python. This includes the nimbus and supervisor daemons, and as we'll
    see, all the commands to deploy and manage topologies. It is for this reason that
    a properly configured Python interpreter be installed on all machines participating
    in a Storm cluster as well as any workstation used for management purposes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Storm守护程序和管理命令都是从一个用Python编写的单个可执行文件运行的。这包括nimbus和supervisor守护程序，以及我们将看到的所有部署和管理拓扑的命令。因此，在参与Storm集群的所有机器上以及用于管理目的的任何工作站上都需要安装一个正确配置的Python解释器。
- en: Installing Storm on Linux
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Linux上安装Storm
- en: Storm was originally designed to run on Unix-style operating systems, but as
    of Version 0.9.1, it supports deployment on Windows as well.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Storm最初设计为在类Unix操作系统上运行，但从版本0.9.1开始，它也支持在Windows上部署。
- en: For our purposes, we will be using Ubuntu 12.04 LTS for its relative ease of
    use. We'll use the server version which by default does not include a graphical
    user interface since we won't need or use it. The Ubuntu 12.04 LTS server can
    be downloaded from [http://releases.ubuntu.com/precise/ubuntu-12.04.2-server-i386.iso](http://releases.ubuntu.com/precise/ubuntu-12.04.2-server-i386.iso).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们的目的，我们将使用Ubuntu 12.04 LTS，因为它相对容易使用。我们将使用服务器版本，默认情况下不包括图形用户界面，因为我们不需要也不会使用它。Ubuntu
    12.04 LTS服务器可以从[http://releases.ubuntu.com/precise/ubuntu-12.04.2-server-i386.iso](http://releases.ubuntu.com/precise/ubuntu-12.04.2-server-i386.iso)下载。
- en: The instructions that follow the command work equally well on both the actual
    hardware as well as virtual machines. For the purpose of learning and development,
    you will likely find it much more convenient to work with virtual machines, especially
    if you don't have several networked computers readily available.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的指令在实际硬件和虚拟机上同样有效。为了学习和开发的目的，如果你没有准备好的网络计算机，使用虚拟机会更加方便。
- en: 'Virtualization software is readily available for OSX, Linux, and Windows. We
    recommend any one of the following software options:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化软件可以在OSX，Linux和Windows上轻松获得。我们推荐以下任何一种软件选项：
- en: VMWare (OSX, Linux, and Windows)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMWare（OSX，Linux和Windows）
- en: This software would need to be purchased. It is available at [http://www.vmware.com](http://www.vmware.com).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个软件需要购买。它可以在[http://www.vmware.com](http://www.vmware.com)上获得。
- en: VirtualBox (OSX, Linux, and Windows)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VirtualBox（OSX，Linux和Windows）
- en: This software is available for free. It is available at [https://www.virtualbox.org](https://www.virtualbox.org).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个软件是免费提供的。它可以在[https://www.virtualbox.org](https://www.virtualbox.org)上获得。
- en: Parallels Desktop (OSX)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parallels Desktop（OSX）
- en: This software would need to be purchased. It is available at [http://www.parallels.com](http://www.parallels.com).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个软件需要购买。它可以在[http://www.parallels.com](http://www.parallels.com)上获得。
- en: Installing the base operating system
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装基本操作系统
- en: You can begin by booting from the Ubuntu installation disk (or disk image) and
    follow the onscreen instructions for a basic installation. When the **Package
    Selection** screen comes up, choose the option to install OpenSSH Server. This
    package will allow you to use `ssh` to remotely log into the server. In all other
    cases, you can simply accept the default options unless you choose to make modifications
    specific to your hardware.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从Ubuntu安装光盘（或光盘镜像）启动，并按照屏幕上的指示进行基本安装。当**Package Selection**屏幕出现时，选择安装OpenSSH
    Server选项。这个软件包将允许你使用`ssh`远程登录服务器。在其他情况下，除非你选择对硬件进行特定修改，否则可以接受默认选项。
- en: By default, the primary user under Ubuntu will have administrative (sudo) privileges.
    If you are using a different user account or Linux distribution, make sure your
    account has administration privileges.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ubuntu下，默认情况下，主要用户将具有管理（sudo）权限。如果你使用不同的用户账户或Linux发行版，请确保你的账户具有管理权限。
- en: Installing Java
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Java
- en: 'First, install a JVM. Storm is known to work with Java 1.6 and 1.7 JVMs from
    both the open source OpenJDK and Oracle. In this example, we''ll update the apt
    repository information and install the OpenJDK distribution of Java 1.6:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，安装JVM。已知Storm可以与来自开源OpenJDK和Oracle的Java 1.6和1.7 JVM一起工作。在这个例子中，我们将更新apt存储库信息并安装Java
    1.6的OpenJDK发行版：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ZooKeeper installation
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ZooKeeper安装
- en: 'For our single-node pseudo-cluster, we''ll install ZooKeeper alongside all
    other Storm components. Storm currently requires Version 3.3.x, so we''ll install
    that version rather than the latest one using the following command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的单节点伪集群，我们将在所有其他Storm组件旁边安装ZooKeeper。Storm目前需要版本3.3.x，因此我们将安装该版本而不是最新版本，使用以下命令：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This command will install both the ZooKeeper binaries as well as the service
    scripts to start and stop ZooKeeper. It will also create a cron job that will
    periodically purge old ZooKeeper transaction logs and snapshot files, which will
    quickly consume large amounts of disk space if not purged on a regular basis as
    this is ZooKeeper's default behavior.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将安装ZooKeeper二进制文件以及启动和停止ZooKeeper的服务脚本。它还将创建一个定期清除旧的ZooKeeper事务日志和快照文件的cron作业，如果不定期清除，这些文件将迅速占用大量磁盘空间，因为这是ZooKeeper的默认行为。
- en: Storm installation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风暴安装
- en: Storm's binary release distributions can be downloaded from the Storm website
    ([http://storm.incubator.apache.org](http://storm.incubator.apache.org)). The
    layout of the binary archives is geared more toward development activities than
    running a production system, so we'll make a few modifications to more closely
    follow UNIX conventions (such as logging to `/var/log` rather than Storm's home
    directory).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Storm的二进制发行版可以从Storm网站([http://storm.incubator.apache.org](http://storm.incubator.apache.org))下载。二进制存档的布局更适合开发活动，而不是运行生产系统，因此我们将对其进行一些修改，以更紧密地遵循UNIX约定（例如将日志记录到`/var/log`而不是Storm的主目录）。
- en: 'We begin by creating a Storm user and group. This will allow us to run the
    Storm daemons as a specific user rather than the default or root users:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个Storm用户和组。这将允许我们以特定用户而不是默认或根用户运行Storm守护进程：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, download and unzip the Storm distribution. We''ll install Storm in `/usr/share`
    and symlink the version-specific directory to `/usr/share/storm`. This approach
    will allow us to easily install other versions and activate (or revert) the new
    version by changing a single symbolic link. We''ll also link the Storm executable
    to `/usr/bin/storm`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，下载并解压Storm分发版。我们将在`/usr/share`中安装Storm，并将特定版本的目录链接到`/usr/share/storm`。这种方法可以让我们轻松安装其他版本，并通过更改单个符号链接来激活（或恢复）新版本。我们还将Storm可执行文件链接到`/usr/bin/storm`：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'By default, Storm will log information to `$STORM_HOME/logs` rather than the
    `/var/log` directory that most UNIX services use. To change this, execute the
    following commands to create the `storm` directory under `/var/log/` and configure
    Storm to write its log data there:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Storm将日志信息记录到`$STORM_HOME/logs`而不是大多数UNIX服务使用的`/var/log`目录。要更改这一点，执行以下命令在`/var/log/`下创建`storm`目录，并配置Storm将其日志数据写入那里：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we''ll move Storm''s configuration file to `/etc/storm` and create
    a symbolic link so Storm can find it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将Storm的配置文件移动到`/etc/storm`并创建一个符号链接，以便Storm可以找到它：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With Storm installed, we're now ready to configure Storm and set up the Storm
    daemons so they start automatically.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了Storm后，我们现在可以配置Storm并设置Storm守护进程，使它们可以自动启动。
- en: Running the Storm daemons
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行Storm守护进程
- en: All of the Storm daemons are fail-fast by design, meaning the process will halt
    whenever an unexpected error occurs. This allows individual components to safely
    fail and successfully recover without affecting the rest of the system.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Storm守护进程都是设计为失败快速的，这意味着每当发生意外错误时，进程将停止。这允许各个组件安全失败并成功恢复，而不影响系统的其他部分。
- en: This means that the Storm daemons need to be restarted immediately whenever
    they die unexpectedly. The technique for this is known as running a process under
    *supervision*, and fortunately there are a number of utilities available to perform
    this function. In fact, ZooKeeper is also a fail-fast system, and the upstart-based
    `init` scripts included in the ZooKeeper Debian distributions (Ubuntu is a Debian-based
    distribution) provide just that functionality—if the ZooKeeper process exits abnormally
    at any time, upstart will ensure it is restarted so the cluster can recover.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着Storm守护进程需要在它们意外死机时立即重新启动。这种技术称为在*监督*下运行进程，幸运的是有许多可用的实用程序来执行这个功能。事实上，ZooKeeper也是一个失败快速的系统，而ZooKeeper
    Debian发行版（Ubuntu是基于Debian的发行版）中包含的基于upstart的`init`脚本提供了这个功能——如果ZooKeeper进程在任何时候异常退出，upstart将确保它重新启动，以便集群可以恢复。
- en: While the Debian upstart system is perfect for this situation, there are simpler
    options that are also available on other Linux distributions. To keep things simple,
    we'll use the supervisor package that's readily available on most distributions.
    Unfortunately, the supervisor name collides with the name of Storm's supervisor
    daemon. To clarify this distinction, we'll refer to the non-Storm process supervision
    daemon as *supervisord* (note the added *d* at the end) in the text, even though
    sample code and commands will use the proper name without the added *d*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Debian的upstart系统非常适合这种情况，但其他Linux发行版上也有更简单的选择。为了简化事情，我们将使用大多数发行版上都可以找到的supervisor软件包。不幸的是，supervisor名称与Storm的supervisor守护进程的名称冲突。为了澄清这一区别，我们将在文本中将非Storm进程监督守护进程称为*supervisord*（注意末尾添加的*d*），即使示例代码和命令将使用正确的名称而不添加*d*。
- en: 'Under Debian-based Linux distributions, the `supervisord` package is named
    supervisor, while other distributions such as Red Hat use the name supervisord.
    To install it on Ubuntu, use the following command:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于Debian的Linux发行版中，`supervisord`软件包被命名为supervisor，而其他发行版如Red Hat使用supervisord这个名字。要在Ubuntu上安装它，请使用以下命令：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will install and start the supervisord service. The main configuration
    file will be located at `/etc/supervisor/supervisord.conf`. Supervisord's configuration
    file will automatically include any files matching the pattern `*.conf` in the
    `/etc/supervisord/conf.d/` directory, and this is where we'll place our `config`
    files for to run the Storm daemons under supervision.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装并启动supervisord服务。主配置文件将位于`/etc/supervisor/supervisord.conf`。Supervisord的配置文件将自动包括`/etc/supervisord/conf.d/`目录中与模式`*.conf`匹配的任何文件，并且这就是我们将放置`config`文件以便在supervision下运行Storm守护进程的地方。
- en: 'For each Storm daemon command we want to run under supervision, we''ll create
    a configuration file that contains the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们想要在监督下运行的每个Storm守护进程命令，我们将创建一个包含以下内容的配置文件：
- en: A unique (within the supervisord configuration) name for the service under supervision.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于监督服务的唯一（在supervisord配置中）名称。
- en: The command to run.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行的命令。
- en: The working directory in which to run the command.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行命令的工作目录。
- en: Whether or not the command/service should be automatically restarted if it exits.
    For fail-fast services, this should always be true.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令/服务是否应在退出时自动重新启动。对于失败快速的服务，这应该始终为true。
- en: The user that will own the process. In this case, we will run all Storm daemons
    with the Storm user as the process owner.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将拥有该进程的用户。在这种情况下，我们将使用Storm用户运行所有Storm守护进程作为进程所有者。
- en: 'Create the following three files to set up the Storm daemons to be automatically
    started (and restarted in the event of unexpected failure) by the supervisord
    service:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 创建以下三个文件以设置Storm守护进程自动启动（并在意外故障时重新启动）：
- en: '`/etc/supervisord/conf.d/storm-nimbus.conf`'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/etc/supervisord/conf.d/storm-nimbus.conf`'
- en: 'Use the following code to create the file:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码创建文件：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`/etc/supervisord/conf.d/storm-supervisor.conf`'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/etc/supervisord/conf.d/storm-supervisor.conf`'
- en: 'Use the following code to create the file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码创建文件：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`/etc/supervisord/conf.d/storm-ui.conf`'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/etc/supervisord/conf.d/storm-ui.conf`'
- en: 'Use the following code to create the file:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码创建文件：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once those files have been created, stop and start the supervisord service
    with the following commands:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了这些文件后，使用以下命令停止并启动supervisord服务：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The supervisord service will load the new configurations and start the Storm
    daemons. Wait a moment or two for the Storm services to start and then verify
    the Storm pseudo-cluster is up and running by visiting the following URL in a
    web browser (replace `localhost` with the host name or IP address of the actual
    machine):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: supervisord服务将加载新的配置并启动Storm守护进程。等待一两分钟，然后通过在Web浏览器中访问以下URL（用实际机器的主机名或IP地址替换`localhost`）来验证Storm伪集群是否已启动并运行：
- en: '`http://localhost:8080`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://localhost:8080`'
- en: This will bring up the Storm UI graphical interface. It should indicate that
    the cluster is up with one supervisor node running with four available worker
    slots and no topologies are running (we'll deploy a topology to the cluster later).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动Storm UI图形界面。它应指示集群已经启动，有一个监督节点正在运行，有四个可用的工作槽，并且没有拓扑正在运行（我们稍后将向集群部署拓扑）。
- en: 'If for some reason the Storm UI does not come up or fails to show an active
    supervisor in the cluster, check the following log files for errors:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于某种原因Storm UI没有启动或未显示集群中的活动监督员，请检查以下日志文件以查找错误：
- en: '**Storm UI**: Check the `ui.log` file under `/var/log/storm` to check for errors'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Storm UI**：检查`/var/log/storm`下的`ui.log`文件以查找错误'
- en: '**Nimbus**: Check the `nimbus.log` file under `/var/log/storm` to check for
    errors'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nimbus**：检查`/var/log/storm`下的`nimbus.log`文件以查找错误'
- en: '**Supervisor**: Check the `supervisor.log` file under `/var/log/storm` to check
    for errors'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Supervisor**：检查`/var/log/storm`下的`supervisor.log`文件以查找错误'
- en: So far, we've relied on the default Storm configuration that defaults to using
    `localhost` for many cluster hostname parameters such as the ZooKeeper hosts as
    well as the location of the nimbus master. This is fine for a single-node pseudo-cluster
    where everything runs on the same machine, but setting up a real multi-node cluster
    requires overriding the default values. Next, we'll explore the various configuration
    options Storm provides and how they affect the behavior of a cluster and its topologies.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直依赖默认的Storm配置，该配置默认使用`localhost`作为许多集群主机名参数的值，例如ZooKeeper主机以及nimbus主节点的位置。这对于单节点伪集群是可以的，其中所有内容都在同一台机器上运行，但是设置真正的多节点集群需要覆盖默认值。接下来，我们将探讨Storm提供的各种配置选项以及它们对集群及其拓扑行为的影响。
- en: Configuring Storm
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Storm
- en: Storm's configuration consists of a series of YAML properties. When a Storm
    daemon starts, it loads the default values and then loads the `storm.yaml` (which
    we've symlinked to `/etc/storm/storm.yaml`) file under `$STORM_HOME/conf/`, substituting
    any values found there with the defaults.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Storm的配置由一系列YAML属性组成。当Storm守护进程启动时，它会加载默认值，然后加载`storm.yaml`（我们已经将其符号链接到`/etc/storm/storm.yaml`）文件在`$STORM_HOME/conf/`下，用默认值替换找到的任何值。
- en: 'The listing below provides a minimal `storm.yaml` file with entries that you
    must override:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表提供了一个最小的`storm.yaml`文件，其中包含您必须覆盖的条目：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Mandatory settings
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强制设置
- en: The following settings are mandatory for configuring working, multihost Storm
    clusters.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下设置是配置工作的多主机Storm集群的强制设置。
- en: '`storm.zookeeper.servers`: This setting is a list of the hostnames in the ZooKeeper
    cluster. Since we''re running a single node ZooKeeper on the same machine as the
    other Storm daemons, the default value of localhost is acceptable.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`storm.zookeeper.servers`：此设置是ZooKeeper集群中主机名的列表。由于我们在与其他Storm守护进程相同的机器上运行单节点ZooKeeper，因此`localhost`的默认值是可以接受的。'
- en: '`nimbus.host`: This is the hostname of the cluster''s nimbus node. Workers
    need to know which node is the master in order to download topology JAR files
    and configurations.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nimbus.host`：这是集群nimbus节点的主机名。工作节点需要知道哪个节点是主节点，以便下载拓扑JAR文件和配置。'
- en: '`supervisor.slots.ports`: This setting controls how many worker processes run
    on a supervisor node. It is defined as a list of port numbers that the workers
    will listen on, and the number of port numbers listed will control how many worker
    slots are available on the supervisor node. For example, if we have a cluster
    with three supervisor nodes, and each node is configured with three ports, the
    cluster will have a total of nine (3 * 3 = 9) worker slots. By default, Storm
    will use ports 6700-6703, a total of four slots per supervisor node.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`supervisor.slots.ports`: 此设置控制在supervisor节点上运行多少个工作进程。它被定义为工作进程将监听的端口号列表，列出的端口号数量将控制supervisor节点上可用的工作槽位数量。例如，如果我们有一个配置了三个端口的三个supervisor节点的集群，那么集群将有总共九个（3
    * 3 = 9）工作槽位。默认情况下，Storm将使用端口6700-6703，每个supervisor节点有四个槽位。'
- en: '`storm.local.dir`: Both the nimbus and supervisor daemons store a small amount
    of transient state information as well as JAR and configuration files required
    by workers. This setting determines where the nimbus and supervisor processes
    will store that information. The directory specified here must exist with appropriate
    permissions so the process owner (in our case, the Storm user) can read and write
    to the directory. The contents of this directory must persist as long as the cluster
    is running, so it is best to avoid using `/tmp` where the contents might be deleted
    by the operating system.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`storm.local.dir`: nimbus和supervisor守护程序都存储少量临时状态信息以及工作进程所需的JAR和配置文件。此设置确定nimbus和supervisor进程将存储该信息的位置。此处指定的目录必须存在，并具有适当的权限，以便进程所有者（在我们的情况下是Storm用户）可以读取和写入该目录。该目录的内容必须在集群运行期间持久存在，因此最好避免使用`/tmp`，因为其中的内容可能会被操作系统删除。'
- en: Optional settings
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可选设置
- en: 'In addition to the settings that are mandatory for an operational cluster,
    there are several other settings that you may find necessary to override. Storm
    configuration settings follow a dotted naming convention where the prefix identifies
    the category of the setting; this is shown in the following table:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对操作集群必需的设置之外，还有一些其他设置可能需要覆盖。Storm配置设置遵循点分命名约定，其中前缀标识了设置的类别；这在下表中有所体现：
- en: '| Prefix | Category |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 前缀 | 类别 |'
- en: '| --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `storm.*` | General configuration |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `storm.*` | 通用配置 |'
- en: '| `nimbus.*` | Nimbus configuration |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| `nimbus.*` | Nimbus 配置 |'
- en: '| `ui.*` | Storm UI configuration |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| `ui.*` | Storm UI 配置 |'
- en: '| `drpc.*` | DRPC server configuration |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| `drpc.*` | DRPC 服务器配置 |'
- en: '| `supervisor.*` | Supervisor configuration |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| `supervisor.*` | Supervisor 配置 |'
- en: '| `worker.*` | Worker configuration |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `worker.*` | Worker 配置 |'
- en: '| `zmq.*` | ZeroMQ configuration |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| `zmq.*` | ZeroMQ 配置 |'
- en: '| `topology.*` | Topology configuration |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| `topology.*` | 拓扑配置 |'
- en: 'For a complete list of the default configuration settings that are available,
    take a look at the `defaults.yaml` file in the Storm source code ([https://github.com/nathanmarz/storm/blob/master/conf/defaults.yaml](https://github.com/nathanmarz/storm/blob/master/conf/defaults.yaml)).
    Some of the more frequently overridden settings are outlined as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看可用的默认配置设置的完整列表，请查看Storm源代码中的`defaults.yaml`文件（[https://github.com/nathanmarz/storm/blob/master/conf/defaults.yaml](https://github.com/nathanmarz/storm/blob/master/conf/defaults.yaml)）。以下是一些经常被覆盖的设置：
- en: '`nimbus.childopts` (default: "-Xmx1024m"): This setting is a list of JVM options
    that will be added to the Java command line when starting the nimbus daemon.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nimbus.childopts` (默认值: "-Xmx1024m"): 这是在启动nimbus守护程序时将添加到Java命令行的JVM选项列表。'
- en: '`ui.port` (default: 8080): This specifies the listening port for the Storm
    UI web server.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ui.port` (默认值: 8080): 这指定了Storm UI web服务器的监听端口。'
- en: '`ui.childopts` (default: "-Xmx1024m"): This specifies the JVM options that
    will be added to the Java command line when starting the Storm UI service.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ui.childopts` (默认值: "-Xmx1024m"): 这指定了在启动Storm UI服务时将添加到Java命令行的JVM选项。'
- en: '`supervisor.childopts` (default: "-Xmx1024m"): This specifies the JVM options
    that will be added to the Java command line when starting the supervisor daemon.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`supervisor.childopts` (默认值: "-Xmx1024m"): 这指定了在启动supervisor守护程序时将添加到Java命令行的JVM选项。'
- en: '`worker.childopts` (default: "-Xmx768m"): This specifies the JVM options that
    will be added to the Java command line when starting worker processes.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`worker.childopts` (默认值: "-Xmx768m"): 这指定了在启动worker进程时将添加到Java命令行的JVM选项。'
- en: '`topology.message.timeout.secs` (default: 30): This configures the maximum
    amount of time (in seconds) for a tuple''s tree to be acknowledged (fully processed)
    before it is considered failed (timed out). Setting this value too low may cause
    tuples to be replayed repeatedly. For this setting to take effect, a spout must
    be configured to emit anchored tuples.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topology.message.timeout.secs` (默认值: 30): 这配置了元组在被确认（完全处理）之前的最长时间（以秒为单位），在此时间内未确认的元组将被视为失败（超时）。将此值设置得太低可能会导致元组被重复重放。要使此设置生效，必须配置spout以发出锚定元组。'
- en: '`topology.max.spout.pending` (default: null): With the default value of null,
    Storm will stream tuples from a spout as fast as the spout can produce them. Depending
    on the execute latency of downstream bolts, the default behavior can overwhelm
    the topology, leading to message timeouts. Setting this value to a non-null number
    greater than 0 will cause Storm to pause streaming tuples from spouts until the
    number of outstanding tuples falls below that number, essentially throttling the
    spout. This setting, along with `topology.message.timeout.secs`, are two of the
    most important parameters when tuning a topology for performance.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topology.max.spout.pending` (默认值: null): 默认值为null时，Storm将从spout尽可能快地流出元组。根据下游bolt的执行延迟，默认行为可能会使拓扑不堪重负，导致消息超时。将此值设置为大于0的非null数字将导致Storm暂停从spout流出元组，直到未完成的元组数量下降到该数字以下，从而限制了spout的流量。在调整拓扑性能时，此设置与`topology.message.timeout.secs`一起是最重要的两个参数之一。'
- en: '`topology.enable.message.timeouts` (default: true): This sets the timeout behavior
    for anchored tuples. If false, anchored tuples will not time out. Use this setting
    with care. Consider altering `topology.message.timeout.secs` before setting this
    to false. For this setting to take effect, a spout must be configured to emit
    anchored tuples.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topology.enable.message.timeouts`（默认值：true）：这设置了锚定元组的超时行为。如果为false，则锚定元组不会超时。谨慎使用此设置。在将其设置为false之前，请考虑修改`topology.message.timeout.secs`。要使此设置生效，必须配置一个spout以发射锚定元组。'
- en: The Storm executable
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Storm可执行文件
- en: The Storm executable is a multipurpose command used for everything from launching
    Storm daemons to performing topology management functions, such as deploying new
    topologies to a cluster, or simply running a topology in local mode during development
    and testing phases.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Storm可执行文件是一个多用途命令，用于从启动Storm守护程序到执行拓扑管理功能，例如将新的拓扑部署到集群中，或者在开发和测试阶段以本地模式运行拓扑。
- en: 'The basic syntax for the Storm command is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Storm命令的基本语法如下：
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Setting up the Storm executable on a workstation
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在工作站上设置Storm可执行文件
- en: 'For running Storm commands that connect to a remote cluster, you will need
    to have the Storm distribution installed locally. Installing the distribution
    on a workstation is simple; just unzip the Storm distribution archive and add
    the Storm bin directory (`$STORM_HOME/bin`) to your `PATH` environment variable.
    Next, create the `storm.yaml` file under `~/.storm/` with a single line that tells
    Storm where to find the nimbus server for the cluster with which you want to interact:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于运行连接到远程集群的Storm命令，您需要在本地安装Storm分发版。在工作站上安装分发版很简单；只需解压Storm分发版存档，并将Storm bin目录（`$STORM_HOME/bin`）添加到您的`PATH`环境变量中。接下来，在`~/.storm/`下创建`storm.yaml`文件，其中包含一行告诉Storm在哪里找到要与之交互的集群的nimbus服务器：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In order for a Storm cluster to operate properly, it is imperative that the
    IP address name resolution be set up properly, either through the DNS system or
    entries in the `hosts` file under `/etc`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使Storm集群正常运行，必须正确设置IP地址名称解析，可以通过DNS系统或`/etc`下的`hosts`文件进行设置。
- en: While it is possible to use IP addresses instead of hostnames throughout Storm's
    configuration, using the DNS system is preferred.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在Storm的配置中可以使用IP地址代替主机名，但最好使用DNS系统。
- en: The daemon commands
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 守护程序命令
- en: Storm's daemon commands are used to launch Storm services, and should be run
    under supervision so they are relaunched in the event of unexpected failures.
    When starting, Storm daemons read configuration from `$STORM_HOME/conf/storm.yaml`.
    Any configuration parameters in this file will override Storm's built-in defaults.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Storm的守护程序命令用于启动Storm服务，并且应该在监督下运行，以便在发生意外故障时重新启动。启动时，Storm守护程序从`$STORM_HOME/conf/storm.yaml`读取配置。此文件中的任何配置参数都将覆盖Storm的内置默认值。
- en: Nimbus
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Nimbus
- en: 'Usage: `storm nimbus`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm nimbus`
- en: This launches the nimbus daemon.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动nimbus守护程序。
- en: Supervisor
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Supervisor
- en: 'Usage: `storm supervisor`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm supervisor`
- en: This launches the supervisor daemon.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动监督守护程序。
- en: UI
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: UI
- en: 'Usage: `storm ui`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm ui`
- en: This launches the Storm UI daemon that provides a web-based UI for monitoring
    Storm clusters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动提供用于监视Storm集群的基于Web的UI的Storm UI守护程序。
- en: DRPC
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DRPC
- en: 'Usage: `storm drpc`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm drpc`
- en: This launches the DRPC daemon.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动DRPC守护程序。
- en: The management commands
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理命令
- en: 'Storm''s management commands are used to deploy and manage topologies running
    in a cluster. Management commands typically, but not necessarily, run from a workstation
    outside of the Storm cluster. They communicate to the nimbus Thrift API and thus
    need to know the hostname of the nimbus node. The management commands look for
    configuration from the `~/.storm/storm.yaml` file, and Storm''s jars are appended
    to the classpath. The only required configuration parameter is the hostname of
    the nimbus node:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Storm的管理命令用于部署和管理在集群中运行的拓扑。管理命令通常从Storm集群外的工作站运行。它们与nimbus Thrift API通信，因此需要知道nimbus节点的主机名。管理命令从`~/.storm/storm.yaml`文件中查找配置，并将Storm的jar附加到类路径上。唯一必需的配置参数是nimbus节点的主机名：
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Jar
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jar
- en: 'Usage: `storm jar topology_jar topology_class [arguments...]`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm jar topology_jar topology_class [arguments...]`
- en: The `jar` command is used to submit a topology to a cluster. It runs the `main()`
    method of `topology_class` with the specified arguments and uploads the `topology_jar`
    file to nimbus for distribution to the cluster. Once submitted, Storm will activate
    the topology and start processing.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`jar`命令用于将拓扑提交到集群。它运行`topology_class`的`main()`方法，并使用指定的参数上传`topology_jar`文件到nimbus以分发到集群。一旦提交，Storm将激活拓扑并开始处理。'
- en: The `main()` method in the topology class is responsible for calling the `StormSubmitter.submitTopology()`
    method and supplying a unique (within the cluster) name for the topology. If a
    topology with that name already exists on the cluster, the `jar` command will
    fail. It is common practice to specify the topology name in the command-line arguments
    so that the topology can be named at the time of submission.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑类中的`main()`方法负责调用`StormSubmitter.submitTopology()`方法，并为拓扑提供一个在集群中唯一的名称。如果集群中已经存在具有该名称的拓扑，则`jar`命令将失败。通常的做法是在命令行参数中指定拓扑名称，以便在提交时为拓扑命名。
- en: Kill
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kill
- en: 'Usage: `storm kill topology_name [-w wait_time]`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm kill topology_name [-w wait_time]`
- en: The `kill` command is used to undeploy. It kills the topology with the name
    `topology_name`. Storm will first deactivate the topology's spouts for the duration
    of the topology's configured `topology.message.timeout.secs` to allow all tuples
    actively being processed to complete. Storm will then halt the workers and attempt
    to clean up any saved states. Specifying a wait time with the `-w` switch will
    override `topology.message.timeout.secs` with the specified interval.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`kill`命令用于取消部署。它会杀死名为`topology_name`的拓扑。Storm将首先停用拓扑的喷口，持续时间为拓扑配置的`topology.message.timeout.secs`，以允许所有正在处理的元组完成。然后，Storm将停止工作进程，并尝试清理任何保存的状态。使用`-w`开关指定等待时间将覆盖`topology.message.timeout.secs`为指定的间隔。'
- en: The functionality of the `kill` command is also available in the Storm UI.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`kill`命令的功能也可以在Storm UI中使用。'
- en: Deactivate
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停用
- en: 'Usage: `storm deactivate topology_name`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm deactivate topology_name`
- en: The `deactivate` command tells Storm to stop streaming tuples from the specified
    topology's spouts.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`deactivate`命令告诉Storm停止从指定拓扑的喷口流元组。'
- en: Topologies can also be deactivated from the Storm UI.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以从Storm UI停用拓扑。
- en: Activate
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活
- en: 'Usage: `storm activate topology_name`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm activate topology_name`
- en: The `activate` command tells Storm to resume streaming tuples from the specified
    topology's spouts.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`activate`命令告诉Storm从指定拓扑的喷口恢复流元组。'
- en: Topologies can also be reactivated from the Storm UI.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以从Storm UI重新激活拓扑。
- en: Rebalance
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新平衡
- en: 'Usage: `storm rebalance topology_name [-w wait_time] [-n worker_count] [-e
    component_name=executer_count]`...'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm rebalance topology_name [-w wait_time] [-n worker_count] [-e component_name=executer_count]`...
- en: The `rebalance` command instructs Storm to redistribute tasks among workers
    in a cluster without killing and resubmitting the topology. For example, this
    might be necessary when a new supervisor node has been added to a cluster—since
    it is a new node, none of the tasks of existing topologies would have been assigned
    to workers on that node.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`rebalance`命令指示Storm在集群中重新分配任务，而无需杀死和重新提交拓扑。例如，当向集群添加新的监督节点时，可能需要这样做——因为它是一个新节点，现有拓扑的任何任务都不会分配给该节点上的工作进程。'
- en: The `rebalance` command also allows you to alter the number of workers assigned
    to a topology and change the number of executors assigned to a given task with
    the `-n` and `-e` switches respectively.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`rebalance`命令还允许您使用`-n`和`-e`开关更改分配给拓扑的工作进程数量，并分别更改分配给给定任务的执行器数量。'
- en: When the `rebalance` command is run, Storm will first deactivate the topology,
    wait for the configured time for outstanding tuples to finish processing, then
    redistribute workers evenly among supervisor nodes. After rebalancing, Storm will
    return the topology to its previous activation state (that is, if it was activated,
    Storm will reactivate it and vice versa).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`rebalance`命令时，Storm将首先停用拓扑，等待配置的时间以完成未完成的元组处理，然后在监督节点之间均匀重新分配工作进程。重新平衡后，Storm将拓扑返回到其先前的激活状态（也就是说，如果它被激活了，Storm将重新激活它，反之亦然）。
- en: 'The following example will rebalance the topology with the name `wordcount-topology`
    with a waiting time of 15 seconds, assign five workers to the topology, and set
    `sentence-spout` and `split-bolt` to use 4 and 8 executor threads respectively:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例将使用等待时间为15秒重新平衡名为`wordcount-topology`的拓扑，为该拓扑分配五个工作进程，并分别设置`sentence-spout`和`split-bolt`使用4和8个执行线程：
- en: '[PRE15]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Remoteconfvalue
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Remoteconfvalue
- en: 'Usage: `storm remoteconfvalue conf-name`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm remoteconfvalue conf-name`
- en: The `remoteconfvalue` command is used to look up a configuration parameter on
    a remote cluster. Note that this applies to the global cluster configuration and
    does not take into account individual overrides made at the topology level.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`remoteconfvalue`命令用于查找远程集群上的配置参数。请注意，这适用于全局集群配置，并不考虑在拓扑级别进行的个别覆盖。'
- en: Local debug/development commands
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地调试/开发命令
- en: Storm's local commands are utilities for debugging and testing. Like the management
    commands, Storm's debug commands read `~/.storm/storm.yaml` and use those values
    to override Storm's built-in defaults.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Storm的本地命令是用于调试和测试的实用程序。与管理命令一样，Storm的调试命令读取`~/.storm/storm.yaml`并使用这些值来覆盖Storm的内置默认值。
- en: REPL
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: REPL
- en: 'Usage: `storm repl`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm repl`
- en: The `repl` command opens a Clojure REPL session configured with Storm's local
    classpath.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`repl`命令打开一个配置了Storm本地类路径的Clojure REPL会话。'
- en: Classpath
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类路径
- en: 'Usage: `storm classpath`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm classpath`
- en: The `classpath` command prints the classpath used by the Storm client.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`classpath`命令打印Storm客户端使用的类路径。'
- en: Localconfvalue
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地配置值
- en: 'Usage: `storm localconfvalue conf-name`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：`storm localconfvalue conf-name`
- en: The `localconfvalue` command looks up a configuration key from the consolidated
    configuration, that is, from `~/.storm/storm.yaml` and Storm's built-in defaults.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`localconfvalue`命令从合并配置中查找配置键，即从`~/.storm/storm.yaml`和Storm的内置默认值中查找。'
- en: Submitting topologies to a Storm cluster
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向Storm集群提交拓扑
- en: 'Now that we have a running cluster, let''s revisit our earlier word count example
    and modify it so we can deploy it to a cluster as well as run it in local mode.
    The previous example used Storm''s `LocalCluster` class to run in local mode:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个运行中的集群，让我们重新审视之前的单词计数示例，并修改它，以便我们可以将其部署到集群，并在本地模式下运行。之前的示例使用了Storm的`LocalCluster`类在本地模式下运行：
- en: '[PRE16]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Submitting a topology to a remote cluster is simply a matter of using Storm''s
    `StormSubmitter` class, which exposes a method with the same name and signature:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 向远程集群提交拓扑只是使用Storm的`StormSubmitter`类的方法，该方法具有相同的名称和签名：
- en: '[PRE17]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When developing Storm topologies, you usually aren''t going to want to change
    code and recompile them to switch between running in local mode and deploying
    to a cluster. The standard way to handle this is to add an if/else block that
    makes that determination based on a command-line argument. In our updated example,
    if there are no command line arguments, we run the topology in local mode; otherwise,
    we use the first argument as the topology name and submit it to the cluster, as
    shown in the following code:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发Storm拓扑时，通常不希望更改代码并重新编译它们以在本地模式和部署到集群之间切换。处理这种情况的标准方法是添加一个if/else块，根据命令行参数来确定。在我们更新的示例中，如果没有命令行参数，我们在本地模式下运行拓扑；否则，我们使用第一个参数作为拓扑名称并将其提交到集群，如下面的代码所示：
- en: '[PRE18]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To deploy the updated word count topology to a running cluster, first perform
    a Maven build in the `Chapter 2` source code directory:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要将更新的单词计数拓扑部署到运行的集群中，首先在`第2章`源代码目录中执行Maven构建：
- en: '[PRE19]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, run the `storm jar` command to deploy the topology:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，运行`storm jar`命令来部署拓扑：
- en: '[PRE20]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When the command completes, you should see the topology become active in the
    Storm UI and be able to click on the topology name to drill down and view the
    topology statistics.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 当命令完成时，您应该在Storm UI中看到拓扑变为活动状态，并能够点击拓扑名称进行详细查看和查看拓扑统计信息。
- en: '![Submitting topologies to a Storm cluster](img/8294OS_02_04.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![将拓扑提交到Storm集群](img/8294OS_02_04.jpg)'
- en: Automating the cluster configuration
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化集群配置
- en: So far, we've configured a single-node pseudo-cluster manually from the command
    line. While this approach certainly works with small clusters, it will quickly
    become untenable as the cluster size increases. Consider the situation where one
    needs to configure clusters consisting of tens, hundreds, or even thousands of
    nodes. The configuration tasks can be automated using shell scripts, but even
    a shell script-based automation solution is questionable in terms of scalability.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从命令行手动配置了单节点伪集群。虽然这种方法在小集群中当然有效，但随着集群规模的增加，它将很快变得不可行。考虑需要配置由数十、数百甚至数千个节点组成的集群的情况。配置任务可以使用shell脚本自动化，但即使是基于shell脚本的自动化解决方案在可扩展性方面也是值得怀疑的。
- en: Fortunately, there are a number of technologies available to help address the
    issue of configuration and provisioning of large numbers of managed servers. Both
    Chef and Puppet offer a declarative approach to configuration that allows you
    to define **states** (that is, what packages are installed and how they are configured)
    as well as **classes** of machines (for example, an *Apache web server* class
    machine needs to have the Apache `httpd` daemon installed).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有许多技术可用于解决大量受管服务器的配置和配置问题。Chef和Puppet都提供了一种声明性的配置方法，允许您定义**状态**（即安装了哪些软件包以及它们如何配置）以及机器的**类**（例如，*Apache
    web服务器*类机器需要安装Apache `httpd`守护程序）。
- en: Automating the process of provisioning and configuring servers is a very broad
    topic that is far beyond the scope of this book. For our purposes, we will use
    Puppet and leverage a subset of its functionality in the hope that it will provide
    a basic introduction to the topic and encourage further exploration.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化服务器的配置和配置过程是一个非常广泛的主题，远远超出了本书的范围。为了我们的目的，我们将使用Puppet并利用其功能的一个子集，希望它能够提供对该主题的基本介绍，并鼓励进一步探索。
- en: A rapid introduction to Puppet
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Puppet的快速介绍
- en: Puppet ([https://puppetlabs.com](https://puppetlabs.com)) is an IT automation
    framework that helps system administrators manage large network infrastructure
    resources using a flexible, declarative approach to IT automation.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet ([https://puppetlabs.com](https://puppetlabs.com))是一个IT自动化框架，它帮助系统管理员使用灵活的声明性方法管理大型网络基础设施资源。
- en: 'At the heart of Puppet is the concept of a *manifest* that describes the desired
    *state* of an infrastructure resource. In Puppet terms, a state can include the
    following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet的核心是描述基础设施资源期望状态的*清单*概念。在Puppet术语中，状态可以包括以下内容：
- en: Which software packages are installed
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了哪些软件包
- en: Which services are running and which aren't
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些服务正在运行，哪些没有
- en: Software configuration details
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件配置细节
- en: Puppet manifests
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Puppet清单
- en: 'Puppet uses a declarative Ruby-based DSL to describe system configuration in
    collections of files known as manifests. An example Puppet manifest for ZooKeeper
    is listed as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet使用声明性基于Ruby的DSL来描述文件集合中的系统配置，这些文件集合称为清单。ZooKeeper的一个示例Puppet清单如下所示：
- en: '[PRE21]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This simple manifest can be used to make sure ZooKeeper is installed as a service
    and that the service is running. The first package block tells Puppet to use the
    operating system's package manager (for example, apt-get for Ubuntu/Debian, yum
    for Red Hat, and so on) to ensure that the Version 3.3.5 of the zookeeper package
    is installed. The second package block ensures that the zookeeperd package is
    installed; it requires that the zookeeper package is already installed. Finally,
    the `service` block tells Puppet that it should ensure that the zookeeperd system
    service is running and that the service requires the zookeeperd package to be
    installed.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的清单可以用来确保ZooKeeper作为服务安装并且服务正在运行。第一个软件包块告诉Puppet使用操作系统的软件包管理器（例如，Ubuntu/Debian的apt-get，Red
    Hat的yum等）来确保安装zookeeper软件包的3.3.5版本。第二个软件包块确保安装了zookeeperd软件包；它要求zookeeper软件包已经安装。最后，`service`块告诉Puppet应该确保zookeeperd系统服务正在运行，并且该服务需要zookeeperd软件包已安装。
- en: To illustrate how Puppet manifests translate to installed software and system's
    state, let's install Puppet and use the preceding example to install and start
    the zookeeperd service.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明Puppet清单如何转换为已安装的软件和系统状态，让我们安装Puppet并使用前面的示例来安装和启动zookeeperd服务。
- en: 'To get the latest version of Puppet, we need to configure apt-get to use the
    Puppet labs repository. Execute the following commands to do so and install the
    latest version of puppet:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取Puppet的最新版本，我们需要配置apt-get以使用Puppet实验室存储库。执行以下命令来这样做并安装最新版本的puppet：
- en: '[PRE22]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, save the preceding example manifest to a file named `init.pp` and use
    Puppet to apply the manifest:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将前面的示例清单保存到名为`init.pp`的文件中，并使用Puppet应用该清单：
- en: '[PRE23]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When the command completes, check to see whether the zookeeper service is in
    fact running:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 命令完成后，检查zookeeper服务是否实际在运行：
- en: '[PRE24]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If we were to manually stop the zookeeper service and rerun the `puppet apply`
    command, Puppet would not install the packages again (since they are already there);
    however, it would restart the zookeeper service since the state defined in the
    manifest defines the service as *running*.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们手动停止zookeeper服务并重新运行`puppet apply`命令，Puppet不会再次安装包（因为它们已经存在）；然而，它会重新启动zookeeper服务，因为清单中定义的状态将服务定义为*运行*。
- en: Puppet classes and modules
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Puppet类和模块
- en: While standalone Puppet manifests make it easy to define the state of an individual
    resource, such an approach can quickly become unwieldy when the number of resources
    you're managing increases.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然独立的Puppet清单使得定义单个资源的状态变得容易，但当您管理的资源数量增加时，这种方法很快就会变得难以控制。
- en: Fortunately, Puppet has the concept of classes and modules that can be leveraged
    to better organize and isolate specific configuration details.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Puppet有类和模块的概念，可以更好地组织和隔离特定的配置细节。
- en: Consider a situation with Storm where we have multiple classes of nodes. For
    example, a node in a Storm cluster may be a nimbus node, a supervisor node, or
    both. Puppet classes and modules provide a way to distinguish between multiple
    configuration roles that you can mix and match to easily define a network resource
    that performs multiple roles.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一种Storm的情况，我们有多个节点类。例如，Storm集群中的一个节点可能是nimbus节点、supervisor节点或两者兼有。Puppet类和模块提供了一种区分多个配置角色的方法，您可以混合和匹配以轻松定义执行多个角色的网络资源。
- en: 'To illustrate this capability, let''s revisit the manifest we used to install
    the zookeeper package and redefine it as a class that can be reused and included
    in multiple class types and manifests:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种能力，让我们重新审视一下我们用来安装zookeeper包的清单，并重新定义它为一个可以被重复使用并包含在多个类类型和清单中的类：
- en: '[PRE25]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding example, we've redefined the zookeeper manifest to be a `puppet`
    class that can be used in other classes and manifests. On the second line, the
    `zookeeper` class includes another class, `jdk`, which will include the class
    definition for a resource that will include the state necessary for a machine
    that requires a Java JDK.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们重新定义了zookeeper清单为一个`puppet`类，可以在其他类和清单中使用。在第二行，`zookeeper`类包含另一个类`jdk`，它将包含一个资源的类定义，该资源将包含需要Java
    JDK的机器的状态。
- en: Puppet templates
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Puppet模板
- en: Puppet also leverages the Ruby ERB templating system that allows you to define
    templates for various files that will be populated when Puppet applies a manifest
    file. Placeholders in Puppet ERB templates are Ruby expressions and constructs
    that will be evaluated and replaced when Puppet runs. The Ruby code in ERB templates
    has full access to the Puppet variables defined in manifest files.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet还利用了Ruby ERB模板系统，允许您为将在Puppet应用清单文件时填充的各种文件定义模板。Puppet ERB模板中的占位符是将在Puppet运行时评估和替换的Ruby表达式和结构。ERB模板中的Ruby代码可以完全访问清单文件中定义的Puppet变量。
- en: 'Consider the following Puppet file declaration that''s used to generate the
    `storm.yaml` configuration file:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下Puppet文件声明，用于生成`storm.yaml`配置文件：
- en: '[PRE26]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This declaration tells Puppet to create the file, `storm.yaml`, under `/etc/storm/`
    from the `storm.yaml.erb` template:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 此声明告诉Puppet从`storm.yaml.erb`模板创建文件`storm.yaml`，放在`/etc/storm/`下：
- en: '[PRE27]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The conditional logic and variable expansion in the template allow us to define
    a single file that can be used for many environments. For example, if the environment
    we're configuring does not have any Storm DRPC servers, then the `drpc.servers`
    section of the generated `storm.yaml` file will be omitted.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 模板中的条件逻辑和变量扩展允许我们定义一个可以用于许多环境的单个文件。例如，如果我们正在配置的环境没有任何Storm DRPC服务器，那么生成的`storm.yaml`文件的`drpc.servers`部分将被省略。
- en: Managing environments with Puppet Hiera
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Puppet Hiera管理环境
- en: 'We''ve briefly introduced the concepts of Puppet manifests, classes, and templates.
    At this point, you''re probably wondering how to define variables in a puppet
    class or manifest. Defining a variable within a `puppet` class or manifest is
    easy; simply define it at the beginning of the manifest or class definition as
    follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要介绍了Puppet清单、类和模板的概念。此时，您可能想知道如何在puppet类或清单中定义变量。在`puppet`类或清单中定义变量非常简单；只需在清单或类定义的开头定义如下：
- en: '[PRE28]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Once defined, the `java_version` variable will be available throughout the class
    or manifest definition as well as any ERB templates; however, there is a drawback
    here in terms of reusability. If we hard-code information such as version numbers,
    we're effectively limiting the reuse of our class by pinning it to a hard-coded
    value. It would be better if we could externalize all potentially frequently changing
    variables to make configuration management more maintainable. This is where Hiera
    comes into play.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义，`java_version`变量将在整个类或清单定义以及任何ERB模板中可用；然而，这里存在一个可重用性的缺点。如果我们硬编码诸如版本号之类的信息，实际上就限制了我们的类的重用，使其固定在一个硬编码的值上。如果我们能够将所有可能频繁更改的变量外部化，使配置管理更易于维护，那将更好。这就是Hiera发挥作用的地方。
- en: Introducing Hiera
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Hiera
- en: Hiera is a key-value lookup tool that has been integrated into the latest version
    of the Puppet framework. Hiera allows you to define key-value hierarchies (hence
    the name) such that keys in a parent definition source can be overridden by child
    definition sources.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Hiera是一个键值查找工具，已集成到Puppet框架的最新版本中。Hiera允许您定义键值层次结构（因此得名），使得父定义源中的键可以被子定义源覆盖。
- en: For example, consider a situation where we are defining configuration parameters
    for a number of machines that will participate in a Storm cluster. All machines
    will share a common set of key-values such as the version of Java we'd like to
    use. So, we'd define those values in a file called "`common.yaml`.`"`
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑这样一种情况，我们正在为将参与Storm集群的多台机器定义配置参数。所有机器将共享一组常见的键值，例如我们想要使用的Java版本。因此，我们将在一个名为“`common.yaml`”的文件中定义这些值。
- en: From there on, things start to diverge. We may have environments that are single-node
    pseudo-clusters, and we may have environments that are multi-node. For that, we'd
    like to store environment-specific configuration values in separate files such
    as "`single-node.yaml"` and "`cluster.yaml`."
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里开始，事情开始分歧。我们可能有单节点伪集群的环境，也可能有多节点的环境。因此，我们希望将特定于环境的配置值存储在诸如“`single-node.yaml`”和“`cluster.yaml`”之类的单独文件中。
- en: Finally, we'd like to store true host-specific information in files that follow
    the naming convenion "**[hostname].yaml**."
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望将真实的特定于主机的信息存储在遵循命名约定“**[hostname].yaml**”的文件中。
- en: '![Introducing Hiera](img/8294OS_02_05.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![介绍Hiera](img/8294OS_02_05.jpg)'
- en: Puppet's Hiera integration allows you to do just that and use built-in Puppet
    variables to resolve filenames appropriately.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet的Hiera集成允许您这样做，并使用内置的Puppet变量来适当地解析文件名。
- en: The examples in the `Chapter 2` source code directory demonstrate how to implement
    this type of organization.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`第2章`源代码目录中的示例演示了如何实现这种组织形式。'
- en: 'A typical `common.yaml` file might define global properties common to all hosts
    and looks like the following :'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的`common.yaml`文件可能定义了所有主机共有的全局属性，如下所示：
- en: '[PRE29]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'At the environment level, we may want to distinguish between *standalone* and
    *cluster* configurations, in which case a `cluster.yaml` file might look like
    this:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境级别，我们可能希望区分*独立*和*集群*配置，这种情况下，`cluster.yaml`文件可能如下所示：
- en: '[PRE30]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Finally, we may want to define host-specific parameters in files that use the
    naming convention [hostname].yaml, and define the Puppet classes that should be
    applied for that node.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可能希望在使用命名约定[hostname].yaml的文件中定义特定于主机的参数，并定义应该应用于该节点的Puppet类。
- en: 'For `nimbus01.yaml`, use the following code:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`nimbus01.yaml`，请使用以下代码：
- en: '[PRE31]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For `zookeeper01.yaml`, use the following code:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`zookeeper01.yaml`，请使用以下代码：
- en: '[PRE32]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We've only scratched the surface of what's possible with Puppet and Hiera. The
    `Chapter 2` source code directory contains additional examples and documentation
    on how to use Puppet to automate deployment and configuration tasks.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是触及了Puppet和Hiera可能性的表面。`第2章`源代码目录包含了有关如何使用Puppet自动化部署和配置任务的其他示例和文档。
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've covered the steps necessary to install and configure
    Storm in both a single-node (pseudo-distributed) configuration as well as a fully
    distributed multi-node configuration. We've also introduced you to the Storm daemons
    and command line utilities used to deploy and manage running topologies.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们已经介绍了在单节点（伪分布式）配置以及完全分布式多节点配置中安装和配置Storm所需的步骤。我们还向您介绍了用于部署和管理运行拓扑的Storm守护程序和命令行实用程序。
- en: Finally, we offered a brief introduction to the Puppet framework and showed
    how it can be used to manage multiple environment configurations.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要介绍了Puppet框架，并展示了如何使用它来管理多个环境配置。
- en: We'd encourage you to explore the additional code and documentation included
    in the accompanied downloads.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您探索附带下载中包含的附加代码和文档。
- en: In the next chapter, we will introduce Trident, which is a high-level abstraction
    layer on top of Storm for transactions and state management.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍Trident，这是一个在Storm之上用于事务和状态管理的高级抽象层。
