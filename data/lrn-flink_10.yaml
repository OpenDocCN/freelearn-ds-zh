- en: Chapter 10. Best Practices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。最佳实践
- en: So far in this book, we have learned various things about Flink. We started
    with Flink's architecture and the various APIs it supports. We also learned how
    we use graph and machine learning APIs provided by Flink. Now in this concluding
    chapter, we are going to talk about some best practices you should follow in order
    to create production quality maintainable Flink applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们已经学习了关于Flink的各种知识。我们从Flink的架构和它支持的各种API开始。我们还学习了如何使用Flink提供的图形和机器学习API。现在在这个总结性的章节中，我们将讨论一些最佳实践，您应该遵循以创建高质量可维护的Flink应用程序。
- en: 'We will be discussing about the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论以下主题：
- en: Logging best practices
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志最佳实践
- en: Using custom serializers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义序列化器
- en: Using and monitoring the REST API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用和监控REST API
- en: Back pressure monitoring
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 背压监控
- en: So let's get started.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们开始吧。
- en: Logging best practices
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志最佳实践
- en: It is very important to have logs configured in any software application. Logs
    help in debugging the issues.  We don’t follow these logging practices, it would
    be very diffuclt to understand the progress of the job or if any issues with it.
    There are couple of libraries we can use for better logging experience.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何软件应用程序中配置日志非常重要。日志有助于调试问题。如果我们不遵循这些日志记录实践，将很难理解作业的进度或是否存在任何问题。我们可以使用一些库来获得更好的日志记录体验。
- en: Configuring Log4j
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Log4j
- en: Log4j, as we know, is one the most widely used logging libraries. We can configure
    it in any Flink application with very little effort. We have only to include a
    `log4j.properties` file. We can pass the `log4j.properties` file by passing it
    as an `Dlog4j.configuration=/path/to/log4j.properties` argument.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，Log4j是最广泛使用的日志记录库之一。我们可以在任何Flink应用程序中配置它，只需很少的工作。我们只需要包含一个`log4j.properties`文件。我们可以通过将其作为`Dlog4j.configuration=/path/to/log4j.properties`参数传递来传递`log4j.properties`文件。
- en: 'Flink supports the following default property files:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持以下默认属性文件：
- en: '`log4j-cli.properties`: This file is used by the Flink command line tool. Here
    is the exact file at [https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-cli.properties](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-cli.properties).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log4j-cli.properties`：此文件由Flink命令行工具使用。以下是该文件的确切文件：[https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-cli.properties](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-cli.properties)。'
- en: '`log4j-yarn-session.properties`: This file is used by the Flink YARN session.
    Here is the exact file at [https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-yarn-session.properties](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-yarn-session.properties).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log4j-yarn-session.properties`：此文件由Flink YARN会话使用。以下是该文件的确切文件：[https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-yarn-session.properties](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-yarn-session.properties)。'
- en: '`log4j.properties`: This file is used by the Flink Job Manager and Task Manager.
    Here is the exact file at [https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j.properties](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j.properties).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log4j.properties`：此文件由Flink作业管理器和任务管理器使用。以下是该文件的确切文件：[https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j.properties](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j.properties)。'
- en: Configuring Logback
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Logback
- en: These days a lot of people prefer using Logback over Log4j because of it features.
    Logback provides faster I/O, thoroughly tested libraries, extensive documentation
    etc.  Flink also supports configuring Logback for an application.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，很多人更喜欢Logback而不是Log4j，因为它具有更多的功能。Logback提供更快的I/O、经过彻底测试的库、广泛的文档等。Flink也支持为应用程序配置Logback。
- en: 'We need to use the same property to configure `logback.xml`. `Dlogback.configurationFile=<file>`,
    or we can also put the `logback.xml` file in the class path. A sample `logback.xml`
    would look like this:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用相同的属性来配置`logback.xml`。`Dlogback.configurationFile=<file>`，或者我们也可以将`logback.xml`文件放在类路径中。示例`logback.xml`如下所示：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can always change the `logback.xml` file and set the logging level according
    to our preferences.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以随时更改`logback.xml`文件，并根据我们的偏好设置日志级别。
- en: Logging in applications
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用程序中的日志记录
- en: 'While using SLF4J in any Flink application, we need to import the following
    package and classes, and initiate the logger with the class name:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何Flink应用程序中使用SLF4J时，我们需要导入以下包和类，并使用类名初始化记录器：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It is also a best practice to use a placeholder mechanism for logging instead
    of using a string formatter. The placeholder mechanism helps to avoid unnecessary
    string formations instead it only does string concatenation. The following code
    snippet shows how to use a placeholder:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用占位符机制而不是使用字符串格式化也是最佳实践。占位符机制有助于避免不必要的字符串形成，而只进行字符串连接。以下代码片段显示了如何使用占位符：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also use placeholder logging in exception handling:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在异常处理中使用占位符日志记录：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using ParameterTool
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ParameterTool
- en: Since Flink 0.9, we have a built-in `ParameterTool` in Flink, which helps to
    get parameters from external sources such as arguments, system properties, or
    from property files. Internally, it is a map of strings which keeps the key as
    the parameter name and the value as the parameter value.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自Flink 0.9以来，我们在Flink中有一个内置的`ParameterTool`，它有助于从外部源（如参数、系统属性或属性文件）获取参数。在内部，它是一个字符串映射，它将键保留为参数名称，将值保留为参数值。
- en: 'For example, we can think of using ParameterTool in our DataStream API example,
    where we need to set Kafka properties:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以考虑在我们的DataStream API示例中使用ParameterTool，其中我们需要设置Kafka属性：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: From system properties
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从系统属性
- en: We can read properties defined in system variables. We need to pass the system
    properties file before initializing them by setting `Dinput=hdfs://myfile`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以读取系统变量中定义的属性。我们需要在初始化之前通过设置`Dinput=hdfs://myfile`来传递系统属性文件。
- en: 'Now we can read all those properties in `ParameterTool` as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以按以下方式在`ParameterTool`中读取所有这些属性：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: From command line arguments
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从命令行参数
- en: We can also read the parameters from command line arguments. We have to set
    `--elements` before invoking the application.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从命令行参数中读取参数。在调用应用程序之前，我们必须设置`--elements`。
- en: 'The following code shows how to read parameters from command line arguments:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了如何从命令行参数中读取参数：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: From .properties file
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自.properties文件
- en: 'We can also read the parameters from the `.properties` file. The following
    is the code for this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从`.properties`文件中读取参数。以下是此代码：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can read the parameters in the Flink program. The following shows how we
    get the parameters:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Flink程序中读取参数。以下显示了我们如何获取参数：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Naming large TupleX types
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名大型TupleX类型
- en: As we know, a tuple is a complex data type used to represent complex data structures.
    It is a combination of various primitive data types. Generally, it is recommended
    not to use large tuples; instead it is recommended to use Java POJOs. If you want
    to use a tuple, it is recommended to name it with some custom POJO type.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，元组是用于表示复杂数据结构的复杂数据类型。它是各种原始数据类型的组合。通常建议不要使用大型元组；而是建议使用Java POJOs。如果要使用元组，建议使用一些自定义POJO类型来命名它。
- en: 'It is very easy to create a custom type for a large tuple. For example, if
    we want to use `Tuple8` then we can define it as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为大型元组创建自定义类型非常容易。例如，如果我们想要使用`Tuple8`，则可以定义如下：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Registering a custom serializer
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注册自定义序列化程序
- en: 'In the distributed computing world, it is very important to take care of each
    and every small thing. Serialization is one of them. By default, Flink uses the
    Kryo serializer. Flink also allows us to write custom serializers in case you
    think the default one is not good enough. We need to register the custom serializer
    in order for Flink to understand it. Registering the custom serializer is very
    simple; we just need to register its class type in the Flink execution environment.
    The following code snippet shows how we do that:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式计算世界中，非常重要的是要注意每一个小细节。序列化就是其中之一。默认情况下，Flink使用Kryo序列化程序。Flink还允许我们编写自定义序列化程序，以防您认为默认的序列化程序不够好。我们需要注册自定义序列化程序，以便Flink能够理解它。注册自定义序列化程序非常简单；我们只需要在Flink执行环境中注册其类类型。以下代码片段显示了我们如何做到这一点：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here is a complete sample class for Custom Serializer at [https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/RecordSerializer.java](https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/RecordSerializer.java).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个完整的自定义序列化程序示例类，网址为[https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/RecordSerializer.java](https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/RecordSerializer.java)。
- en: And the custom type at [https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/Record.java](https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/Record.java).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以及自定义类型在[https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/Record.java](https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/Record.java)。
- en: We need to make sure that the custom serializer has to extend the Kryo's serializer
    class. With Google Protobuf and Apache Thrift, this has been done already.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保自定义序列化程序必须扩展Kryo的序列化程序类。使用Google Protobuf和Apache Thrift，这已经完成了。
- en: Note
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can read more about Google Protobuf at [https://github.com/google/protobuf](https://github.com/google/protobuf).
    Details on Apache Thrift can be read at [https://thrift.apache.org/](https://thrift.apache.org/).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/google/protobuf](https://github.com/google/protobuf)了解更多关于Google
    Protobuf的信息。有关Apache Thrift的详细信息，请访问[https://thrift.apache.org/](https://thrift.apache.org/)。
- en: 'In order to use Google Protobuf, you can add the following Maven dependency:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用Google Protobuf，您可以添加以下Maven依赖项：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Metrics
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 度量
- en: Flink supports a metrics system which allows users to know more about the Flink
    setup and the applications running on it. This would be very useful if you are
    using Flink in a very big production system where a huge number of jobs are running
    and we need to get details of each. We can also use these to feed external monitoring
    systems. So let's try to understand what is available and how to use them.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持一个度量系统，允许用户了解有关Flink设置和在其上运行的应用程序的更多信息。如果您在一个非常庞大的生产系统中使用Flink，那将非常有用，其中运行了大量作业，我们需要获取每个作业的详细信息。我们还可以使用这些来提供给外部监控系统。因此，让我们尝试了解可用的内容以及如何使用它们。
- en: Registering metrics
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注册度量
- en: Metric functions are available for use from any user function which extends
    `RichFunction` by calling `getRuntimeContext().getMetricGroup()`. These methods
    return a `MetricGroup` object, which can be used to create and register a new
    metric.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 度量函数可以从任何扩展`RichFunction`的用户函数中使用，方法是调用`getRuntimeContext().getMetricGroup()`。这些方法返回一个`MetricGroup`对象，可用于创建和注册新的度量。
- en: 'Flink supports various metrics types, such as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持各种度量类型，例如：
- en: Counters
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数器
- en: Gauges
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计量表
- en: Histograms
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直方图
- en: Meters
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 米
- en: Counters
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计数器
- en: 'A counter can be used to count certain things while processing. A simple use
    of a counter can be to count invalid records in the data. You can choose to either
    increment or decrement the counter, based on the conditions. The following code
    snippet shows this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 计数器可以用于在处理过程中计算某些事物。计数器的一个简单用途可以是计算数据中的无效记录。您可以选择根据条件增加或减少计数器。以下代码片段显示了这一点：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Gauges
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计量表
- en: A gauge can provide any value whenever required. In order to use a gauge, first
    we need to create a class that implements `org.apache.flink.metrics.Gauge`. Later,
    you can register that with `MetricGroup`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 计量表可以在需要时提供任何值。为了使用计量表，首先我们需要创建一个实现`org.apache.flink.metrics.Gauge`的类。稍后，您可以将其注册到`MetricGroup`中。
- en: 'The following code snippet shows the use of a gauge in the Flink application:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示了在Flink应用程序中使用计量表：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Histograms
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直方图
- en: 'A histogram provides for the distribution of long values over a metric. This
    can be used to monitor certain metrics over time. The following code snippet shows
    how to use this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图提供了长值在度量上的分布。这可用于随时间监视某些度量。以下代码片段显示了如何使用它：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Meters
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 米
- en: 'A meter is used for monitoring a specific parameter''s average throughput.
    The occurrence of an event is registered using the `markEvent()` method. We can
    register a meter using the `meter(String name, Meter meter)` method on `MeterGroup`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 米用于监视特定参数的平均吞吐量。使用`markEvent()`方法注册事件的发生。我们可以使用`MeterGroup`上的`meter(String name,
    Meter meter)`方法注册米：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Reporters
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 报告者
- en: 'Metrics can be displayed to the external system by configuring one or more
    reporters in the `conf/flink-conf.yaml` file. Most of you might be aware of systems
    such as JMX, which help in monitoring many systems. We can consider configuring
    JMX reporting in Flink. A reporter should have certain properties, as listed in
    the following table:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`conf/flink-conf.yaml`文件中配置一个或多个报告者，可以将指标显示到外部系统中。大多数人可能知道诸如JMX之类的系统，这些系统有助于监视许多系统。我们可以考虑在Flink中配置JMX报告。报告者应具有以下表中列出的某些属性：
- en: '| **Configuration** | **Description** |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **配置** | **描述** |'
- en: '| `metrics.reporters` | The list of named reporters |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `metrics.reporters` | 命名报告者的列表 |'
- en: '| `metrics.reporter.<name>.<config>` | Configuration for reporter with `<name>`
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `metrics.reporter.<name>.<config>` | 用于名为`<name>`的报告者的配置 |'
- en: '| `metrics.reporter.<name>.class` | Reporter class used for reporter named
    `<name>` |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| `metrics.reporter.<name>.class` | 用于名为`<name>`的报告者的报告者类 |'
- en: '| `metrics.reporter.<name>.interval` | Interval time for reporter with name
    `<name>` |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `metrics.reporter.<name>.interval` | 名为`<name>`的报告者的间隔时间 |'
- en: '| `metrics.reporter.<name>.scope.delimiter` | Scope of reporter with name `<name>`
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `metrics.reporter.<name>.scope.delimiter` | 名为`<name>`的报告者的范围 |'
- en: 'The following is an example of a reported configuration for the JMX reporter:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是JMX报告者的报告配置示例：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once we add the preceding given configuration in `config/flink-conf.yaml`, we
    need to start Flink Job Manager process. Now Flink will start exposing these variables
    to JMX port `8789.` We can use JConsole to monitor the reports published by Flink.
    JConsole comes by default with JDK installation. We just need to go to JDK installation
    directory and start `JConsole.exe`. Once the JConsole is running, we need to select
    the Flink Job Manager process to monitor and we can see various values that can
    be monitored. Following is a sample screenshot of a JConsole screen monitoring
    Flink.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在`config/flink-conf.yaml`中添加了上述给定的配置，我们需要启动Flink作业管理器进程。现在，Flink将开始将这些变量暴露给JMX端口`8789`。我们可以使用JConsole来监视Flink发布的报告。JConsole默认随JDK安装。我们只需要转到JDK安装目录并启动`JConsole.exe`。一旦JConsole运行，我们需要选择Flink作业管理器进程进行监视，我们可以看到可以监视的各种值。以下是监视Flink的JConsole屏幕的示例截图。
- en: '![Reporters](img/B05653_10_01-1024x318.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![报告者](img/B05653_10_01-1024x318.jpg)'
- en: Note
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Apart from JMX, Flink supports reporters such as Ganglia, Graphite and StasD.
    More information on those can be found at [https://ci.apache.org/projects/flink/flink-docs-release-1.2/monitoring/metrics.html#reporter](https://ci.apache.org/projects/flink/flink-docs-release-1.2/monitoring/metrics.html#reporter).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了JMX，Flink还支持Ganglia、Graphite和StasD等报告者。有关这些报告者的更多信息，请访问[https://ci.apache.org/projects/flink/flink-docs-release-1.2/monitoring/metrics.html#reporter](https://ci.apache.org/projects/flink/flink-docs-release-1.2/monitoring/metrics.html#reporter)。
- en: Monitoring REST API
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控REST API
- en: Flink supports the monitoring of the status of running and completed apps. These
    APIs are also used by Flink's own job dashboard. The status APIs support the `get`
    method which returns JSON objects giving information of the job. Currently, monitoring
    APIs is by default started within the Flink Job Manager dashboard. This information
    can also be accessed with Job Manager Dashboard.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持监视正在运行和已完成应用程序的状态。这些API也被Flink自己的作业仪表板使用。状态API支持`get`方法，该方法返回给定作业的信息的JSON对象。目前，默认情况下在Flink作业管理器仪表板中启动监控API。这些信息也可以通过作业管理器仪表板访问。
- en: There are many APIs available in Flink. Let's start understanding some of them.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Flink中有许多可用的API。让我们开始了解其中一些。
- en: Config API
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置API
- en: 'This gives configuration details of the API:  `http://localhost:8081/config`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了API的配置详细信息：`http://localhost:8081/config`
- en: 'The following is  the response:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是响应：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Overview API
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述API
- en: 'This gives an overview of the Flink cluster: `http://localhost:8081/overview`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了Flink集群的概述：`http://localhost:8081/overview`
- en: 'The following is the response:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是响应：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Overview of the jobs
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作业概述
- en: 'This gives an overview of the jobs which have run recently and are currently
    running: ` http://localhost:8081/jobs`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了最近运行并当前正在运行的作业的概述：`http://localhost:8081/jobs`
- en: 'The following is the response:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是响应：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`http://localhost:8081/joboverview` API gives the complete overview of a Flink
    job. It contains job ID, start and end times, duration of run, no. of tasks and
    their states. A state could be started, running, killed or finished.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://localhost:8081/joboverview` API提供了Flink作业的完整概述。它包含作业ID、开始和结束时间、运行持续时间、任务数量及其状态。状态可以是已启动、运行中、已终止或已完成。'
- en: 'The following is the response:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是响应：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Details of a specific job
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特定作业的详细信息
- en: This gives details of the specific job. We need to provide the job ID returned
    by the previous API. When a job is submitted, Flink creates a Directed Acyclic
    Job (DAG) for that job. This graph contains vertices as the tasks of the job and
    the execution plan. Following output shows the same details.   `http://localhost:8081/jobs/<jobid>`
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了特定作业的详细信息。我们需要提供上一个API返回的作业ID。当提交作业时，Flink为该作业创建一个有向无环作业（DAG）。该图包含作业的任务和执行计划的顶点。以下输出显示了相同的细节。
    `http://localhost:8081/jobs/<jobid>`
- en: 'The following is the response:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是响应：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: User defined job configuration
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户定义的作业配置
- en: 'This gives the user defined job configuration used by a specific job:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了特定作业使用的用户定义作业配置的概述：
- en: '`http://localhost:8081/jobs/<jobid>/config`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://localhost:8081/jobs/<jobid>/config`'
- en: 'The following is the response:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是响应：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Similarly, you can explore all the following listed APIs on your own setup:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以在自己的设置中探索以下列出的所有 API：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Back pressure monitoring
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背压监控
- en: Back pressure is a special situation in Flink applications where the downstream
    operators are not able to consume data with the same speed of the upstream operator
    that is pushing the data. This starts building pressure on the pipeline and the
    data flow starts in the opposite direction. Generally, if this happens, Flink
    gives us warnings in the logs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 背压是 Flink 应用程序中的一种特殊情况，其中下游运算符无法以与推送数据的上游运算符相同的速度消耗数据。这开始在管道上施加压力，并且数据流开始朝相反方向流动。一般来说，如果发生这种情况，Flink
    会在日志中警告我们。
- en: In a source sink scenario, if we see a warning to the source, then it means
    sink is consuming data slower than the source is producing it.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在源汇场景中，如果我们看到对源的警告，那么这意味着汇正在以比源产生数据更慢的速度消耗数据。
- en: It is very important to monitor back pressure in all streaming jobs, as a high
    back pressuring job may fail or give the wrong results. The backpressure can be
    monitored from the Flink dashboard.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 监控所有流作业的背压非常重要，因为高背压的作业可能会失败或产生错误的结果。可以从 Flink 仪表板监控背压。
- en: Flink handles back pressure monitoring continuously, taking sample stack traces
    of the running tasks. If the sample shows that the task is stuck in an internal
    method, this indicates that there is a back pressure.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 不断处理背压监控，对运行任务进行采样堆栈跟踪。如果采样显示任务卡在内部方法中，这表明存在背压。
- en: 'On an average, the Job Manager triggers 100 stack traces every 50 milliseconds.
    Based on the number of tasks stuck in the internal process, the back pressure
    warning level is decided, as shown in the following table:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，作业管理器每 50 毫秒触发 100 个堆栈跟踪。根据卡在内部过程中的任务数量，决定背压警告级别，如下表所示：
- en: '| **Ratio** | **Back pressure level** |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **比率** | **背压级别** |'
- en: '| 0 to 0.10 | ok |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 0 到 0.10 | 正常 |'
- en: '| 0.10 to 0.5 | low |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 0.10 到 0.5 | 低 |'
- en: '| 0.5 to 1 | high |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 到 1 | 高 |'
- en: 'You can also configure the number of samples and their intervals by setting
    the following parameters:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过设置以下参数来配置样本的数量和间隔：
- en: '| **Parameter** | **Description** |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **描述** |'
- en: '| `jobmanager.web.backpressure.refresh-interval` | Refresh interval to reset
    available stats. Default is `60,000`, 1 min. |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| `jobmanager.web.backpressure.refresh-interval` | 重置可用统计信息的刷新间隔。默认为 `60,000`，1
    分钟。 |'
- en: '| `jobmanager.web.backpressure.delay-between-samples` | Interval for delay
    between the samples. Default is `50` ms. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| `jobmanager.web.backpressure.delay-between-samples` | 样本之间的延迟间隔。默认为 `50`
    毫秒。 |'
- en: '| `jobmanager.web.backpressure.num-samples` | Number of samples to determine
    the back pressure. Default is `100`. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| `jobmanager.web.backpressure.num-samples` | 用于确定背压的样本数量。默认为 `100`。 |'
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this final chapter, we looked at some best practices you should follow in
    order to achieve the best of Flink's performance. We also looked at various monitoring
    APIs and metrics which can be used for the detailed monitoring of Flink applications.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这最后一章中，我们看了一些应该遵循的最佳实践，以实现 Flink 的最佳性能。我们还研究了各种监控 API 和指标，这些可以用于详细监控 Flink
    应用程序。
- en: For Flink, I would say the journey has just started and I am sure over the years,
    the community and support is going to get stronger and better. After all, Flink
    is called the **fourth generation** (**4G**) of big data!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Flink，我想说旅程刚刚开始，我相信多年来，社区和支持会变得更加强大和更好。毕竟，Flink 被称为大数据的**第四代**（**4G**）！
